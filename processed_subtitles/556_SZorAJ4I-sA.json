{
  "text": "[MUSIC PLAYING]\nDALE MARKOWITZ: The\nneat thing about working\nin machine learning is that\nevery few years, somebody\ninvents something crazy that\nmakes you totally reconsider\nwhat's possible, like\nmodels that can play\nGo or generate\nhyper-realistic faces.\nAnd today, the\nmind-blowing discovery\nthat's rocking\neveryone's world is\na type of neural network\ncalled a transformer.\nTransformers are models that\ncan translate text, write\npoems and op-eds, and even\ngenerate computer code.\nThey could be used in biology\nto solve the protein folding\nproblem.\nTransformers are like\nthis magical machine\nlearning hammer that seems to\nmake every problem into a nail.\nIf you've heard of the\ntrendy new ML models\nBERT, or GPT-3, or T5,\nall of these models\nare based on transformers.\nSo if you want to stay\nhip in machine learning\nand especially in natural\nlanguage processing,\nyou have to know\nabout the transformer.\nSo in this video,\nI'm going to tell you\nabout what transformers\nare, how they work,\nand why they've\nbeen so impactful.\nLet's get to it.\nSo what is a transformer?\nIt's a type of neural\nnetwork architecture.\nTo recap, neural networks\nare a very effective type\nof model for analyzing\ncomplicated data\ntypes, like images,\nvideos, audio, and text.\nBut there are different types\nof neural networks optimized\nfor different types of data.\nLike if you're analyzing\nimages, you would typically\nuse a convolutional\nneural network,\nwhich is designed\nto vaguely mimic\nthe way that the human\nbrain processes vision.\nAnd since around\n2012, neural networks\nhave been really good\nat solving vision tasks,\nlike identifying\nobjects in photos.\nBut for a long time, we didn't\nhave anything comparably\ngood for analyzing language,\nwhether for translation,\nor text summarization,\nor text generation.\nAnd this is a problem, because\nlanguage is the primary way\nthat humans communicate.\nYou see, until transformers\ncame around, the way\nwe used deep learning\nto understand text\nwas with a type of model called\na Recurrent Neural Network,\nor an RNN, that looked\nsomething like this.\nLet's say you wanted\nto translate a sentence\nfrom English to French.\nAn RNN would take as\ninput an English sentence\nand process the\nwords one at a time,\nand then sequentially spit\nout their French counterparts.\nThe keyword here is sequential.\nIn language, the order\nof words matters,\nand you can't just\nshuffle them around.\nFor example, the sentence\nJane went looking for trouble\nmeans something very different\nthan the sentence Trouble\nwent looking for Jane.\nSo any model that's going\nto deal with language\nhas to capture word order,\nand recurrent neural networks\ndo this by looking at one\nword at a time sequentially.\nBut RNNs had a lot of problems.\nFirst, they never\nreally did well\nat handling large sequences\nof text, like long paragraphs\nor essays.\nBy the time they were analyzing\nthe end of a paragraph,\nthey'd forget what\nhappened in the beginning.\nAnd even worse, RNNs were\npretty hard to train.\nBecause they process\nwords sequentially,\nthey couldn't\nparalellize well, which\nmeans that you couldn't just\nspeed them up by throwing\nlots of GPUs at them.\nAnd when you have a model\nthat's slow to train,\nyou can't train it on\nall that much data.\nThis is where the transformer\nchanged everything.\nThey're a model developed in\n2017 by researchers at Google\nand the University of Toronto,\nand they were initially\ndesigned to do translation.\nBut unlike recurrent\nneural networks,\nyou could really efficiently\nparalellize transformers.\nAnd that meant that\nwith the right hardware,\nyou could train some\nreally big models.\nHow big?\nReally big.\nRemember GPT-3, that model\nthat writes poetry and code,\nand has conversations?\nThat was trained on almost\n45 terabytes of text data,\nincluding almost the\nentire public web.\n[WHISTLES] So if you remember\nanything about transformers,\nlet it be this.\nCombine a model that scales\nreally well with a huge data\nset and the results will\nprobably blow your mind.\nSo how do these\nthings actually work?\nFrom the diagram in the paper,\nit should be pretty clear.\nOr maybe not.\nActually, it's simpler\nthan you might think.\nThere are three main\ninnovations that\nmake this model work so well.\nPositional encodings and\nattention, and specifically,\na type of attention\ncalled self-attention.\nLet's start by talking\nabout the first one,\npositional encodings.\nLet's say we're trying\nto translate text\nfrom English to French.\nPositional encodings is\nthe idea that instead\nof looking at\nwords sequentially,\nyou take each word\nin your sentence,\nand before you feed it\ninto the neural network,\nyou slap a number on it--\n1, 2, 3, depending\non what number\nthe word is in the sentence.\nIn other words, you\nstore information\nabout word order\nin the data itself,\nrather than in the\nstructure of the network.\nThen as you train the\nnetwork on lots of text data,\nit learns how to interpret\nthose positional encodings.\nIn this way, the neural\nnetwork learns the importance\nof word order from the data.\nThis is a high level\nway to understand\npositional encodings,\nbut it's an innovation\nthat really helped make\ntransformers easier\nto train than RNNs.\nThe next innovation\nin this paper\nis a concept called\nattention, which\nyou'll see used everywhere in\nmachine learning these days.\nIn fact, the title of the\noriginal transformer paper\nis \"Attention Is All You Need.\"\nSo the agreement on the\nEuropean economic area\nwas signed in August 1992.\nDid you know that?\nThat's the example sentence\ngiven in the original paper.\nAnd remember, the\noriginal transformer\nwas designed for translation.\nNow imagine trying to translate\nthat sentence to French.\nOne bad way to translate text\nis to try to translate each word\none for one.\nBut in French, some\nwords are flipped,\nlike in the French translation,\nEuropean comes before economic.\nPlus, French is a\nlanguage that has gendered\nagreement between words.\nSo the word [FRENCH] needs\nto be in the feminine form\nto match with [FRENCH].\nThe attention mechanism is\na neural network structure\nthat allows a text model to\nlook at every single word\nin the original\nsentence when making\na decision about how to\ntranslate a word in the output\nsentence.\nIn fact, here's a\nnice visualization\nfrom that paper that shows what\nwords in the input sentence\nthe model is\nattending to when it\nmakes predictions about a\nword for the output sentence.\nSo when the model outputs\nthe word [FRENCH],,\nit's looking at the input\nwords European and economic.\nYou can think of this\ndiagram as a sort\nof heat map for attention.\nAnd how does the\nmodel know which words\nit should be attending to?\nIt's something that's\nlearned over time from data.\nBy seeing thousands of examples\nof French and English sentence\npairs, the model\nlearns about gender,\nand word order, and\nplurality, and all\nof that grammatical stuff.\nSo we talked about two key\ntransformer innovations,\npositional encoding\nand attention.\nBut actually, attention had\nbeen invented before this paper.\nThe real innovation in\ntransformers was something\ncalled self-attention, a twist\non traditional attention.\nThe type of attention\nwe just talked about\nhad to do with aligning\nwords in English and French,\nwhich is really important\nfor translation.\nBut what if you're just\ntrying to understand\nthe underlying meaning\nin language so that you\ncan build a network that can do\nany number of language tasks?\nWhat's incredible\nabout neural networks,\nlike transformers, is that as\nthey analyze tons of text data,\nthey begin to build up this\ninternal representation\nor understanding of\nlanguage automatically.\nThey might learn, for example,\nthat the words programmer,\nand software engineer,\nand software developer\nare all synonymous.\nAnd they might also naturally\nlearn the rules of grammar,\nand gender, and\ntense, and so on.\nThe better this internal\nrepresentation of language\nthe neural network\nlearns, the better it\nwill be at any language task.\nAnd it turns out that attention\ncan be a very effective way\nto get a neural network\nto understand language\nif it's turned on the\ninput text itself.\nLet me give you an example.\nTake these two sentences--\nServer, can I have the check?\nVersus, Looks like I\njust crashed the server.\nThe word server here means\ntwo very different things.\nAnd I know that,\nbecause I'm looking\nat the context of the\nsurrounding words.\nSelf-attention allows\na neural network\nto understand a word in the\ncontext of the words around it.\nSo when a model\nprocesses the word server\nin the first\nsentence, it might be\nattending to the\nword check, which\nhelps it disambiguate from a\nhuman server versus a mail one.\nIn the second\nsentence, the model\nmight be attending to the\nword crashed to determine\nthat the server is a machine.\nSelf-attention can also\nhelp neural networks\ndisambiguate words,\nrecognize parts of speech,\nand even identify word tense.\nThis, in a nutshell, is the\nvalue of self-attention.\nSo to summarize,\ntransformers boil down\nto positional encodings,\nattention, and self-attention.\nOf course, this is a 10,000-foot\nlook at transformers.\nBut how are they\nactually useful?\nOne of the most popular\ntransformer-based models\nis called BERT, which was\ninvented just around the time\nthat I joined Google in 2018.\nBERT was trained on\na massive text corpus\nand has become this sort\nof general pocketknife\nfor NLP that can be adapted\nto a bunch of different tasks,\nlike text summarization,\nquestion answering,\nclassification, and\nfinding similar sentences.\nIt's used in Google Search to\nhelp understand search queries,\nand it powers a lot of\nGoogle Cloud's NLP tools,\nlike Google Cloud\nAutoML Natural Language.\nBERT also proved that you\ncould build very good models\non unlabeled data,\nlike text scraped\nfrom Wikipedia or Reddit.\nThis is called\nsemi-supervised learning,\nand it's a big trend in\nmachine learning right now.\nSo if I've sold you about\nhow cool transformers are,\nyou might want to start\nusing them in your app.\nNo problem.\nTensorFlow Hub is a great place\nto grab pretrained transformer\nmodels, like BERT.\nYou can download them for\nfree in multiple language\nand drop them straight\ninto your app.\nYou can also check out the\npopular transformers Python\nlibrary, built by the\ncompany Hugging Face.\nThat's one of the\ncommunity's favorite ways\nto train and use\ntransformer models.\nFor more transformer\ntips, check out\nmy blog post linked below,\nand thanks for watching.\n[MUSIC PLAYING]\n",
  "words": [
    "music",
    "playing",
    "dale",
    "markowitz",
    "neat",
    "thing",
    "working",
    "machine",
    "learning",
    "every",
    "years",
    "somebody",
    "invents",
    "something",
    "crazy",
    "makes",
    "totally",
    "reconsider",
    "possible",
    "like",
    "models",
    "play",
    "go",
    "generate",
    "faces",
    "today",
    "discovery",
    "rocking",
    "everyone",
    "world",
    "type",
    "neural",
    "network",
    "called",
    "transformer",
    "transformers",
    "models",
    "translate",
    "text",
    "write",
    "poems",
    "even",
    "generate",
    "computer",
    "code",
    "could",
    "used",
    "biology",
    "solve",
    "protein",
    "folding",
    "problem",
    "transformers",
    "like",
    "magical",
    "machine",
    "learning",
    "hammer",
    "seems",
    "make",
    "every",
    "problem",
    "nail",
    "heard",
    "trendy",
    "new",
    "ml",
    "models",
    "bert",
    "t5",
    "models",
    "based",
    "transformers",
    "want",
    "stay",
    "hip",
    "machine",
    "learning",
    "especially",
    "natural",
    "language",
    "processing",
    "know",
    "transformer",
    "video",
    "going",
    "tell",
    "transformers",
    "work",
    "impactful",
    "let",
    "get",
    "transformer",
    "type",
    "neural",
    "network",
    "architecture",
    "recap",
    "neural",
    "networks",
    "effective",
    "type",
    "model",
    "analyzing",
    "complicated",
    "data",
    "types",
    "like",
    "images",
    "videos",
    "audio",
    "text",
    "different",
    "types",
    "neural",
    "networks",
    "optimized",
    "different",
    "types",
    "data",
    "like",
    "analyzing",
    "images",
    "would",
    "typically",
    "use",
    "convolutional",
    "neural",
    "network",
    "designed",
    "vaguely",
    "mimic",
    "way",
    "human",
    "brain",
    "processes",
    "vision",
    "since",
    "around",
    "2012",
    "neural",
    "networks",
    "really",
    "good",
    "solving",
    "vision",
    "tasks",
    "like",
    "identifying",
    "objects",
    "photos",
    "long",
    "time",
    "anything",
    "comparably",
    "good",
    "analyzing",
    "language",
    "whether",
    "translation",
    "text",
    "summarization",
    "text",
    "generation",
    "problem",
    "language",
    "primary",
    "way",
    "humans",
    "communicate",
    "see",
    "transformers",
    "came",
    "around",
    "way",
    "used",
    "deep",
    "learning",
    "understand",
    "text",
    "type",
    "model",
    "called",
    "recurrent",
    "neural",
    "network",
    "rnn",
    "looked",
    "something",
    "like",
    "let",
    "say",
    "wanted",
    "translate",
    "sentence",
    "english",
    "french",
    "rnn",
    "would",
    "take",
    "input",
    "english",
    "sentence",
    "process",
    "words",
    "one",
    "time",
    "sequentially",
    "spit",
    "french",
    "counterparts",
    "keyword",
    "sequential",
    "language",
    "order",
    "words",
    "matters",
    "ca",
    "shuffle",
    "around",
    "example",
    "sentence",
    "jane",
    "went",
    "looking",
    "trouble",
    "means",
    "something",
    "different",
    "sentence",
    "trouble",
    "went",
    "looking",
    "jane",
    "model",
    "going",
    "deal",
    "language",
    "capture",
    "word",
    "order",
    "recurrent",
    "neural",
    "networks",
    "looking",
    "one",
    "word",
    "time",
    "sequentially",
    "rnns",
    "lot",
    "problems",
    "first",
    "never",
    "really",
    "well",
    "handling",
    "large",
    "sequences",
    "text",
    "like",
    "long",
    "paragraphs",
    "essays",
    "time",
    "analyzing",
    "end",
    "paragraph",
    "forget",
    "happened",
    "beginning",
    "even",
    "worse",
    "rnns",
    "pretty",
    "hard",
    "train",
    "process",
    "words",
    "sequentially",
    "could",
    "paralellize",
    "well",
    "means",
    "could",
    "speed",
    "throwing",
    "lots",
    "gpus",
    "model",
    "slow",
    "train",
    "ca",
    "train",
    "much",
    "data",
    "transformer",
    "changed",
    "everything",
    "model",
    "developed",
    "2017",
    "researchers",
    "google",
    "university",
    "toronto",
    "initially",
    "designed",
    "translation",
    "unlike",
    "recurrent",
    "neural",
    "networks",
    "could",
    "really",
    "efficiently",
    "paralellize",
    "transformers",
    "meant",
    "right",
    "hardware",
    "could",
    "train",
    "really",
    "big",
    "models",
    "big",
    "really",
    "big",
    "remember",
    "model",
    "writes",
    "poetry",
    "code",
    "conversations",
    "trained",
    "almost",
    "45",
    "terabytes",
    "text",
    "data",
    "including",
    "almost",
    "entire",
    "public",
    "web",
    "whistles",
    "remember",
    "anything",
    "transformers",
    "let",
    "combine",
    "model",
    "scales",
    "really",
    "well",
    "huge",
    "data",
    "set",
    "results",
    "probably",
    "blow",
    "mind",
    "things",
    "actually",
    "work",
    "diagram",
    "paper",
    "pretty",
    "clear",
    "maybe",
    "actually",
    "simpler",
    "might",
    "think",
    "three",
    "main",
    "innovations",
    "make",
    "model",
    "work",
    "well",
    "positional",
    "encodings",
    "attention",
    "specifically",
    "type",
    "attention",
    "called",
    "let",
    "start",
    "talking",
    "first",
    "one",
    "positional",
    "encodings",
    "let",
    "say",
    "trying",
    "translate",
    "text",
    "english",
    "french",
    "positional",
    "encodings",
    "idea",
    "instead",
    "looking",
    "words",
    "sequentially",
    "take",
    "word",
    "sentence",
    "feed",
    "neural",
    "network",
    "slap",
    "number",
    "1",
    "2",
    "3",
    "depending",
    "number",
    "word",
    "sentence",
    "words",
    "store",
    "information",
    "word",
    "order",
    "data",
    "rather",
    "structure",
    "network",
    "train",
    "network",
    "lots",
    "text",
    "data",
    "learns",
    "interpret",
    "positional",
    "encodings",
    "way",
    "neural",
    "network",
    "learns",
    "importance",
    "word",
    "order",
    "data",
    "high",
    "level",
    "way",
    "understand",
    "positional",
    "encodings",
    "innovation",
    "really",
    "helped",
    "make",
    "transformers",
    "easier",
    "train",
    "rnns",
    "next",
    "innovation",
    "paper",
    "concept",
    "called",
    "attention",
    "see",
    "used",
    "everywhere",
    "machine",
    "learning",
    "days",
    "fact",
    "title",
    "original",
    "transformer",
    "paper",
    "attention",
    "need",
    "agreement",
    "european",
    "economic",
    "area",
    "signed",
    "august",
    "know",
    "example",
    "sentence",
    "given",
    "original",
    "paper",
    "remember",
    "original",
    "transformer",
    "designed",
    "translation",
    "imagine",
    "trying",
    "translate",
    "sentence",
    "french",
    "one",
    "bad",
    "way",
    "translate",
    "text",
    "try",
    "translate",
    "word",
    "one",
    "one",
    "french",
    "words",
    "flipped",
    "like",
    "french",
    "translation",
    "european",
    "comes",
    "economic",
    "plus",
    "french",
    "language",
    "gendered",
    "agreement",
    "words",
    "word",
    "french",
    "needs",
    "feminine",
    "form",
    "match",
    "french",
    "attention",
    "mechanism",
    "neural",
    "network",
    "structure",
    "allows",
    "text",
    "model",
    "look",
    "every",
    "single",
    "word",
    "original",
    "sentence",
    "making",
    "decision",
    "translate",
    "word",
    "output",
    "sentence",
    "fact",
    "nice",
    "visualization",
    "paper",
    "shows",
    "words",
    "input",
    "sentence",
    "model",
    "attending",
    "makes",
    "predictions",
    "word",
    "output",
    "sentence",
    "model",
    "outputs",
    "word",
    "french",
    "looking",
    "input",
    "words",
    "european",
    "economic",
    "think",
    "diagram",
    "sort",
    "heat",
    "map",
    "attention",
    "model",
    "know",
    "words",
    "attending",
    "something",
    "learned",
    "time",
    "data",
    "seeing",
    "thousands",
    "examples",
    "french",
    "english",
    "sentence",
    "pairs",
    "model",
    "learns",
    "gender",
    "word",
    "order",
    "plurality",
    "grammatical",
    "stuff",
    "talked",
    "two",
    "key",
    "transformer",
    "innovations",
    "positional",
    "encoding",
    "attention",
    "actually",
    "attention",
    "invented",
    "paper",
    "real",
    "innovation",
    "transformers",
    "something",
    "called",
    "twist",
    "traditional",
    "attention",
    "type",
    "attention",
    "talked",
    "aligning",
    "words",
    "english",
    "french",
    "really",
    "important",
    "translation",
    "trying",
    "understand",
    "underlying",
    "meaning",
    "language",
    "build",
    "network",
    "number",
    "language",
    "tasks",
    "incredible",
    "neural",
    "networks",
    "like",
    "transformers",
    "analyze",
    "tons",
    "text",
    "data",
    "begin",
    "build",
    "internal",
    "representation",
    "understanding",
    "language",
    "automatically",
    "might",
    "learn",
    "example",
    "words",
    "programmer",
    "software",
    "engineer",
    "software",
    "developer",
    "synonymous",
    "might",
    "also",
    "naturally",
    "learn",
    "rules",
    "grammar",
    "gender",
    "tense",
    "better",
    "internal",
    "representation",
    "language",
    "neural",
    "network",
    "learns",
    "better",
    "language",
    "task",
    "turns",
    "attention",
    "effective",
    "way",
    "get",
    "neural",
    "network",
    "understand",
    "language",
    "turned",
    "input",
    "text",
    "let",
    "give",
    "example",
    "take",
    "two",
    "sentences",
    "server",
    "check",
    "versus",
    "looks",
    "like",
    "crashed",
    "server",
    "word",
    "server",
    "means",
    "two",
    "different",
    "things",
    "know",
    "looking",
    "context",
    "surrounding",
    "words",
    "allows",
    "neural",
    "network",
    "understand",
    "word",
    "context",
    "words",
    "around",
    "model",
    "processes",
    "word",
    "server",
    "first",
    "sentence",
    "might",
    "attending",
    "word",
    "check",
    "helps",
    "disambiguate",
    "human",
    "server",
    "versus",
    "mail",
    "one",
    "second",
    "sentence",
    "model",
    "might",
    "attending",
    "word",
    "crashed",
    "determine",
    "server",
    "machine",
    "also",
    "help",
    "neural",
    "networks",
    "disambiguate",
    "words",
    "recognize",
    "parts",
    "speech",
    "even",
    "identify",
    "word",
    "tense",
    "nutshell",
    "value",
    "summarize",
    "transformers",
    "boil",
    "positional",
    "encodings",
    "attention",
    "course",
    "look",
    "transformers",
    "actually",
    "useful",
    "one",
    "popular",
    "models",
    "called",
    "bert",
    "invented",
    "around",
    "time",
    "joined",
    "google",
    "bert",
    "trained",
    "massive",
    "text",
    "corpus",
    "become",
    "sort",
    "general",
    "pocketknife",
    "nlp",
    "adapted",
    "bunch",
    "different",
    "tasks",
    "like",
    "text",
    "summarization",
    "question",
    "answering",
    "classification",
    "finding",
    "similar",
    "sentences",
    "used",
    "google",
    "search",
    "help",
    "understand",
    "search",
    "queries",
    "powers",
    "lot",
    "google",
    "cloud",
    "nlp",
    "tools",
    "like",
    "google",
    "cloud",
    "automl",
    "natural",
    "language",
    "bert",
    "also",
    "proved",
    "could",
    "build",
    "good",
    "models",
    "unlabeled",
    "data",
    "like",
    "text",
    "scraped",
    "wikipedia",
    "reddit",
    "called",
    "learning",
    "big",
    "trend",
    "machine",
    "learning",
    "right",
    "sold",
    "cool",
    "transformers",
    "might",
    "want",
    "start",
    "using",
    "app",
    "problem",
    "tensorflow",
    "hub",
    "great",
    "place",
    "grab",
    "pretrained",
    "transformer",
    "models",
    "like",
    "bert",
    "download",
    "free",
    "multiple",
    "language",
    "drop",
    "straight",
    "app",
    "also",
    "check",
    "popular",
    "transformers",
    "python",
    "library",
    "built",
    "company",
    "hugging",
    "face",
    "one",
    "community",
    "favorite",
    "ways",
    "train",
    "use",
    "transformer",
    "models",
    "transformer",
    "tips",
    "check",
    "blog",
    "post",
    "linked",
    "thanks",
    "watching",
    "music",
    "playing"
  ],
  "keywords": [
    "machine",
    "learning",
    "every",
    "something",
    "like",
    "models",
    "type",
    "neural",
    "network",
    "called",
    "transformer",
    "transformers",
    "translate",
    "text",
    "even",
    "could",
    "used",
    "problem",
    "make",
    "bert",
    "language",
    "know",
    "work",
    "let",
    "networks",
    "model",
    "analyzing",
    "data",
    "types",
    "different",
    "designed",
    "way",
    "around",
    "really",
    "good",
    "tasks",
    "time",
    "translation",
    "understand",
    "recurrent",
    "sentence",
    "english",
    "french",
    "take",
    "input",
    "words",
    "one",
    "sequentially",
    "order",
    "example",
    "looking",
    "means",
    "word",
    "rnns",
    "first",
    "well",
    "train",
    "google",
    "big",
    "remember",
    "actually",
    "paper",
    "might",
    "positional",
    "encodings",
    "attention",
    "trying",
    "number",
    "learns",
    "innovation",
    "original",
    "european",
    "economic",
    "attending",
    "two",
    "build",
    "also",
    "server",
    "check"
  ]
}