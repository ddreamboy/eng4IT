{
  "text": "today I'd like to overview the exciting\nfield of deep reinforcement learning\nintroduced overview and provide you some\nof the basics I think it's one of the\nmost exciting fields in artificial\nintelligence it's marrying the power and\nthe ability of deep neural networks to\nrepresent and comprehend the world with\nthe ability to act on that understanding\non that representation taking as a whole\nthat's really what the creation of\nintelligent beings is understand the\nworld and act and the exciting\nbreakthroughs that recently have\nhappened captivate our imagination about\nwhat's possible and that's why this is\nmy favorite area of deep learning and\nartificial intelligence in general and I\nhope you feel the same so what is deep\nreinforcement learning we've talked\nabout deep learning which is taking\nsamples of data being able to in a\nsupervised way compress encode the\nrepresentation that data in the way that\nyou can reason about it I would take\nthat power and apply it to the world\nwhere sequential decisions are to be\nmade so it's looking at problems and\nformulations of tasks where an agent an\nintelligent system has to make a\nsequence of decisions and the decisions\nthat are made have an effect on the\nworld around the agent how how do all of\nus any intelligent being that it's\ntasked with operating in the world how\ndid he learn anything especially when\nyou know very little in the beginning\nit's trial and error is the fundamental\nprocess by which reinforcement learning\nagents learn and the deep part of deep\nreinforcement learning is neural\nnetworks as using the frameworks and\nreinforcement learning where the neural\nnetwork is doing the representation of\nthe world based on which the actions are\nmade\nand we have to take a step back when we\nlook at the types of learning sometimes\nthe terminology itself can confuse us to\nthe fundamentals there are supervised\nlearning there semi-supervised learning\nthere's unsupervised learning there's\nreinforcement learning and there's this\nfeeling that supervised learning is\nreally the only one where you have to\nperform the manual annotation where you\nhave to do the large-scale supervision\nthat's not the case every type of\nmachine learning is supervised learning\nit's supervised by a loss function or a\nfunction that tells you what's good and\nwhat's bad you know even looking at our\nown existence is how we humans figure\nout what's good and bad there's all\nkinds of sources direct and indirect by\nwhich our morals and ethics we figure\nout what's good and bad the difference\nwe supervised and unsupervised and\nreinforcement learning is the source of\nthat supervision what's implied when you\nsay unsupervised is that the cost of\nhuman labor required to attain the\nsupervision is low but it's never\nTurtles all the way down it's Turtles\nand then there's a human at the bottom\nthere at some point there needs to be\nhuman intervention human input to\nprovide what's good and what's bad and\nthis will arise in reinforcement\nlearning as well I have to remember that\nbecause the challenges and the exciting\nopportunities of reinforcement learning\nlie in the fact of how do we get that\nsupervision in the most efficient way\npossible but supervision nevertheless is\nrequired for any system that has an\ninput and an output that's trying to\nlearn like a neural network does to\nprovide an output that's good he needs\nsomebody to say what's good and what's\nbad for you curious about that there's\nbeen a few books a couple written\nthroughout the last few centuries from\nSocrates to Nietzsche I recommend the\nlatter especially so let's look at\nsupervised learning and reinforcement\nlearning let like to propose a way to\nthink about the difference\nthat is illustrative and useful when we\nstart talking about the techniques so\nsupervised learning is taking a bunch of\nexamples of data and learning from those\nexamples where a ground truth provides\nyou the compressed semantic meaning of\nwhat's in that data and from those\nexamples one by one whether it's\nsequences or single samples we learn\nwhat how to then few take future such\nsamples and interpret them reinforcement\nlearning is teaching what we teach an\nagent through experience not by showing\na singular sample of a data set but by\nputting them out into the world the\ndistinction there the essential element\nof reinforcement learning then for us\nnow we'll talk about a bunch of\nalgorithms but the essential design step\nis to provide the world in which to\nexperience the agent learns from the\nworld the from the world it gets the\ndynamics of that world the physics of\nthe world from that world that gets the\nrewards what's good and bad and us as\ndesigners of that agent do not just have\nto do the algorithm we have to do design\nthe the world in which that agent is\ntrying to solve a task the design of the\nworld is the process of reinforcement\nlearning the design of examples the\nannotation of examples is the world of\nsupervised learning and the essential\nperhaps the most difficult element of\nreinforcement learning is the reward the\ngood versus bad here a baby starts\nwalking across the room we want to\ndefine success as a baby walking across\nthe room and reaching the destination\nthat's success and failure is the\ninability to reach that destination\nsimple and reinforcement learning in\nhumans\nthe way we learn from these very few\nexamples appear to learn from very few\nexamples of trial and error is a mystery\na beautiful mystery full of open\nquestions it could be from the huge\namount of data 230 million years worth\nof bipedal data there who've been\nwalking what mammals walking ability to\nwalk or 500 million years the ability to\nsee having eyes so that's the the\nhardware side somehow genetically\nencoded in us is the ability to\ncomprehend this world extremely\nefficiently it could be through not the\nhardware not the five hundred million\nyears but the the few minutes hours days\nmonths maybe even years in the very\nbeginning were born the ability to learn\nreally quickly through observation to\naggregate that information filter all\nthe junk that you don't need and be able\nto learn really quickly through\nimitation learning through observation\nthe way for walking that might mean\nobserving others talk the idea there is\nif there was no other around we would\nnever be able to learn this the\nfundamentals of this walking or as\nefficiently it's through observation and\nthen it could be the algorithm totally\nnot understood is the algorithm that our\nbrain uses to learn the backpropagation\nthat's an artificial neural networks the\nsame kind of processes not understood in\nthe brain that could be the key so I\nwant you to think about that as we talk\nabout the very trivial by comparison\naccomplishments and reinforcement\nlearning and how do we take the next\nsteps but it nevertheless is exciting to\nhave machines that learn how to act in\nthe world the process of learning for\nthose who have fallen in love with\nartificial intelligence the process of\nlearning is thought of as intelligence\nit's the ability to know very little and\nthrough experience examples interaction\nwith the world in whatever medium\nwhether it's data or simulation so on\nbe able to form much richer and\ninteresting representations of that\nworld be able to act in that world\nthat's that's the dream\nso let's look at this stack of what an\nage what it means to be an agent in this\nworld from top the input to the bottom\nthe output is the there's an environment\nwe have to sense that environment we\nhave just a few tools as humans have\nseveral sensory systems on cars you can\nhave lidar camera\nstereo vision audio microphone\nnetworking GPS IMU sensor so on whatever\nrobot you can think about there's a way\nto sense that world and you have this\nraw sensory data and then once you have\nthe raw sensory data you're tasked with\nrepresenting that data in such a way\nthat you can make sense of it as opposed\nto all the the raw sensors and the I the\ncones and so on that taking just giant\nstream of high bandwidth information we\nhave to be able to form higher\nabstractions of features based on which\nwe can reason from edges to corners to\nfaces and so on that's exactly what deep\nlearning neural networks have stepped in\nto be able to in an automated fashion\nwith as little human input as possible\nbe able to form higher-order\nrepresentations of that information then\nthere is the the learning aspect\nbuilding on top of the greater\nabstractions form through the\nrepresentations be able to accomplish\nsomething useful well--there's\ndiscriminative tasks a generative task\nand so on based on the representation be\nable to make sense of the data be able\nto generate new data and so on from\nsequence the sequence to sequence the\nsample from Sam of the sequence and so\non and so forth to actions as we'll talk\nabout and then there is the ability to\naggregate all the information has been\nreceived in the past to the useful\ninformation that's pertinent to the task\nat hand it's the thing the old it looks\nlike a duck quacks like a duck swims\nlike a duck three different data sets\nI'm sure there's state-of-the-art\nalgorithms for the three image class\neducation audio recognition video\nclassification - activity recognition so\non aggregating those three together is\nstill an open problem and that could be\nthe last piece again I want you to think\nabout as we think about reinforcement\nlearning agents how do we play how do we\ntransfer from the game of Atari to the\ngame of go to the game of dota to the\ngame of a robot navigating an uncertain\nenvironment in the real world and once\nyou have that once you sense the raw\nworld once you have a representation of\nthat world then we need to act which is\nprovide actions within the constraints\nof the world in such a way that we\nbelieve can get us towards success the\npromise excitement of deep learning is\nis the part of the stack that converts\nraw data into meaningful representations\nthe promise the dream of deeper\nenforcement learning is going beyond and\nbuilding an agent that uses that\nrepresentation and acts achieve success\nin the world that's super exciting the\nframework and the formulation\nreinforcement learning at its simplest\nis that there's an environment and\nthere's an agent that acts in that\nenvironment the agent senses the\nenvironment by a by some observation\nwell there's partial or complete\nobservation of the environment and it\ngives the environment and action it acts\nin that environment and through the\naction the environment changes in some\nway and then a new observation occurs\nand then also as you provide they\nactually make the observations you\nreceive a reward in most formulations of\nthis of this framework this entire\nsystem has no memory that the the only\nthing you two could be concerned about\nas a state you came from the state you\narrived in and the reward received the\nopen question here is what can't be\nmodeled in this kind of way can we model\nall of it\nfrom from human life to the game of go\ncan all this be model in this way and\nwhat are is this a good way to formulate\nthe learning problem of robotic systems\nin the real world in simulated world\nthose are the open questions the\nenvironment could be fully observable or\npartially observable like in poker\nit could be single agent or multi agent\nAtari versus driving like deep traffic\ndeterministic or stochastic static\nversus dynamic static is in chess\ndynamic again and driving in most\nreal-world applications the screen\nversus continuous like games chess or\ncontinuous and carpal balancing a polo\non a cart\nthe challenge for RL in real world\napplications is that as a reminder\nsupervised learning is teaching by\nexample learning by example teaching\nfrom our perspective reinforcement\nlearning is teaching by experience and\nthe way we provide experience the\nreinforcement learning agents currently\nfor the most part is through simulation\nor through highly constrained real-world\nscenarios so the challenge is in the\nfact that most of the successes is with\nsystems environments that are simulated\nso there's two ways to then close this\ngap to directions of research and work\none is to improve the algorithms improve\nthe ability of the algorithm student to\nform policies that are transferable\nacross all kinds of domains including\nthe real world including especially in\nthe real world so train and simulation\ntransfer to the real world\nor is we improve the simulation in such\na way that the fidelity of the\nsimulation increased increases to the\npoint where the gap between reality and\nsimulation is is minimal to a degree\nthat things learn the simulation are\ndirectly trivially transferable to the\nto the real world\nokay the major components of an RL agent\nan agent operates based on a strategy\ncalled the policy it sees the world it\nmakes a decision that's a policy makes a\ndecision how to act sees the reward sees\na new state acts sees a reward\nshe's new States and acts and this\nrepeats forever until a terminal state\nthe value function is the estimate of\nhow good a state is or how good a state\naction pair is meaning taking an action\nin a particular state how good is that\nability to evaluate that and then the\nmodel different from the environment\nfrom the perspective the agent so the\nenvironment has a model based on which\nit operates and then the agent has a\nrepresentation best understanding of\nthat model so the purpose for an RL\nagent in this simply formulated\nframework is to maximize reward the way\nthat the reward mathematically and\npractically is talked about is with a\ndiscounted framework so we discount\nfurther and further future award so the\nreward that's farther into the future is\nmeans less to us in terms of\nmaximization than reward that's in the\nnear term and so why do we discount it\nso first a lot of it is a math trick to\nbe able to prove certain aspects analyze\ncertain aspects of convergence and in\ngeneral on a more philosophical sense\nbecause environments either are or can\nbe thought of a stochastic random it's\nvery difficult to there's a degree of\nuncertainty\nwhich makes it difficult to really\nestimate the the the reward they'll be\nin the future because of the ripple\neffect of the uncertainty let's look at\nan example a simple one helps us\nunderstand policy's rewards actions\nthere's a robot in the room there's 12\ncells in which you can step it starts in\nthe bottom left it tries to get rewards\non the on the top right there's a plus\none it's a really good thing at the top\nright wants to get there by walking\naround there's a negative 1 which is\nreally bad you wants to avoid that\nSquare and the choice of action is this\nup-down left-right for actions so you\ncould think of there being a negative\nreward of point 0 4 for each step so\nthere's a cost to each step and there's\na stochastic nature to this world\npotentially we'll talk about both\ndeterministic stochastic so in the in\nthe stochastic case when you choose the\naction up with an 80% probability with\nan 80% chance you move up but with 10%\nchance to move left another 10 move\nright so that's the Catholic nature even\nthough you try to go up you might end up\nin a blocks to the left into the right\nso for a deterministic world the optimal\npolicy here given that we always start\nin the bottom left is really shortest\npath is you know you can't ever because\nthere's no stochasticity you're never\ngonna screw up and just fall into the\nhole negative 1 hole that you just\ncompute the shortest path and walk along\nthat shortest path why shortest path\nbecause every single step hurts there's\na negative a reward to it point 0 4\nso shortest path is the thing that\nminimizes the reward shortest path to\nthe to the plus 1 block ok let's look at\nit stochastic world like I mentioned the\n80% up and then split to 20 10 % to left\nand right how does the policy change\nwell first of all we need to have we\nneed to have a plan for every single\nblock in the area because you might end\nup there due to this the castus 'ti of\nthe world ok the the basic\naddition there is that we're trying to\ngo avoid up the closer you get to the\nnegative one hole so just try to avoid\nup because up the stochastic nature of\nup means that you might fall into the\nhole with a 10% chance and given the\npoint zero for step reward you're\nwilling to take the long way home\nin some cases in order to avoid that\npossibility the negative one possibility\nnow let's look at a reward for each step\nif it decreases to negative two it\nreally hurts to take every step then\nagain we go to the shortest path despite\nthe fact that there's a stochastic\nnature in fact you don't really care\nthat you step into the negative one hole\nbecause every step really hurts you just\nwant to get home and then you can play\nwith this reward structure right yes\ninstead of negative 2 or negative point\n0 4 you can look at negative 0.1 and you\ncan see immediately that the structure\nof the policy it changes so with a\nhigher value the higher negative reward\nfree step immediately the urgency of the\nagent increases versus the less urgency\nthe lower the negative reward and when\nthe reward flips so it's positive the\nevery step is a positive so the entire\nsystem which is actually quite common in\nreinforcement learning the entire system\nis full of positive rewards and so that\nthen the optimal policy becomes the\nlongest path is grad school taking as\nlong as possible never reaching the\ndestination so what lessons do we draw\nfrom robot in the room two things the\nenvironment model the dynamics is just\nthere in the trivial example the\nstochastic nature the difference between\n80 percent 100 percent and 50 percent\nthe model of the world the environment\nhas a big impact on what the optimal\npolicy is\nand the reward structure most\nimportantly the thing we can often\ncontrol more in our constructs of the\ntask we try to solve them enforcement is\nthe what is good and what is bad and how\nbad is it and how good is it\nthe reward structure is a big impact and\nthat has a complete change like like\nRobert Frost say the complete change on\nthe policy the choices the agent makes\nso at when you formulate a reinforcement\nlearning framework as researchers as\nstudents what you often do is you design\nthe environment you design the world in\nwhich the system learns even when your\nultimate goal is the physical robot it\ndoes still there's a lot of work still\ndone simulation so you design the world\nthe parameters of that world and you\nalso design the reward structure and it\ncan have a transformative results slight\nvariations in those parameters going to\nhuge results on huge differences on the\npolicy that's arrived and of course the\nexample I've shown before I really love\nis the impact of the the changing reward\nstructure might have unintended\nconsequences and those consequences for\nreal-world system can have obviously\nhighly detrimental costs that are more\nthan just a failed game of Atari so here\nis a human performing the task gate\nplaying the game of coast runners racing\naround the track and so it's when you\nfinish first and you finish fast you get\na lot of points and so it's natural to\nthen okay let's do an RL agent and then\noptimize this for those points and will\nyou find out in the game is that you\nalso get points by picking up the little\ngreen turbo things and with agent\nfigures out is that you can actually get\na lot more points even\nby simply focusing on the green turbos\nfocusing on the green turbos just\nrotating over and over slamming into the\nwall fire and everything just picking it\nup especially because ability to pick up\nthose turbos can avoid the terminal\nstate at the end of finishing the race\nin fact finishing the race means you\nstop collecting positive reward so you\nnever want to finish collected turbos\nand though that's a trivial example it's\nnot actually easy to find such examples\nbut they're out there of unintended\nconsequences that can have highly\nnegative detrimental effects when put in\nthe real world we'll talk about a little\nbit of robotics when you put robots for\nwheeled ones like autonomous vehicles\ninto the real world and you have\nobjective functions that have to\nnavigate difficult intersections full of\npedestrians you have to form intent\nmodels those pedestrians here you see\ncars asserting themselves through dense\nintersections taking risks and within\nthose risks that are taking by us humans\nwill drive vehicles we have to then\nencode that ability to take subtle risk\ninto into AI based control algorithms\nperception then you have to think about\nat the end of the day there's an\nobjective function and if that objective\nfunction does not anticipate the green\nturbos that are to be collected and then\nresult in some understand the\nconsequences could have very negative\neffects especially in situations that\ninvolve human life that's the field of\nAI safety and some of the folks will\ntalk about deep mind and open AI that\nare doing incredible work in RL also\nhave groups that are working on a AI\nsafety for a very good reason this is a\nproblem that I believe that artificial\nintelligent will define some of the most\nimpactful positive things in the 21st\ncentury\nbut I also believe we are nowhere close\nto solving some of the fundamental\nproblems of AI safety that we also need\nto address as we\nthose algorithms okay examples and\nreinforcement learning systems all of it\nhas to do with formulation or rewards\nformulation of states and actions you\nhave the traditional the often used\nbenchmark of a cart balancing a poll\ncontinuous so the action is the\nhorizontal force to the cart the goal is\nto balance the poll so stays top and the\nmoving cart and the reward is one in\neach time step if the poll is upright in\nthe state measured by the cart by the\nagent is the pole angle angular speed\nand of course self sensing of the cart\nposition and the horizontal velocity\nanother example here didn't want to\ninclude the video because it's really\ndisturbing but I do want to include the\nslide because it's really important to\nthink about is by sensing the the raw\npixels learning and teaching an agent to\nplay a game of doom so the goal there is\nto eliminate all opponents the state is\nthe raw game pixels the action is\nup/down shoot reload and so on and the\npositive reward is when an opponent is\neliminated and negative one the agent is\neliminated simple I added it here\nbecause again on the topic of AI safety\nwe have to think about objective\nfunctions and how that translate into\nthe world of not just autonomous\nvehicles but things that even more\ndirectly have harm like autonomous\nweapon systems and we have a lecture on\nthis in the AGI series and on the\nrobotics platform the manipulate object\nmanipulation and grasping objects\nthere's a few benchmarks there's a few\ninteresting applications learning the\nproblem of grabbing objects moving\nobjects manipulating objects rotating\nand so on especially when those objects\ndon't have have complicated shapes and\nso the goal is to pick up an object in\nthe purely in the grasping objects\nallenge the state is the visual\nracial slurs visual visual base the raw\npixels of the objects the actions is to\nmove the arm grasp the object pick it up\nand obviously it's positive when the\npickup is successful the reason I'm\npersonally excited by this is because\nit'll finally allow us to solve the\nproblem of the claw which has been\ntorturing me for many years\nI don't know that's not at all why I'm\nexcited by it okay and then we have to\nthink about as we get greater and\ngreater degree of application in the\nreal world with robotics\nlike cars the the main focus of my\npassion in terms of robotics is how do\nwe encode some of the things that us\nhumans encode how do we you know we have\nto think about our own objective\nfunction our own reward structure our\nown model of the environment about which\nwe perceive and reasonable in order to\nthen encode machines that are doing the\nsame\nand I believe autonomous driving is in\nthat category but to ask questions of\nethics we have to ask questions of of\nrisk value of human life value of\nefficiency money and so on all these in\nfront of ethical questions that an\nautonomous vehicle unfortunately has to\nsolve before it becomes fully autonomous\nso here are the key takeaways of the\nreal-world impact of reinforcement\nlearning agents on the deep learning\nside okay these neural networks that\nform high representation the fun part is\nthe algorithms all the different\narchitectures the different\nencoder/decoder structures all the\nattentions self attention recurrent\nSallust Engr use all the fun\narchitectures and the data so that and\nthe ability to leverage different data\nsets in order to discriminate better\nthan perform this Crematory tasks better\nthan you know MIT does better than stand\nfor that kind of thing that's the fun\npart the hard part is asking good\nquestions and collecting huge amounts of\ndata that's representative over the task\nthat's for real world impact not cvpr\npublication real-world impact\na huge amount of data on a deeper\nenforcement learning side the key\nchallenge the fun part again is the\nalgorithms how do we learn from data\nsome of the stuff I'll talk about today\nthe hard part is defining the\nenvironment defining the acts of space\nand the reward structure as I mentioned\nthis is the big challenge and the\nhardest part is how to crack the gap\nbetween simulation in the real world the\nleaping lizard that's the hardest part\nwe don't even know how to solve that\ntransfer learning problem yet for the\nreal world in fact the three types of\nreinforcement learning there's countless\nalgorithms and there's a lot of ways to\neconomize them but at the highest level\nthere's model-based and there's model\nfree model based algorithms learn the\nmodel of the world so as you interact\nwith the world you construct your\nestimate of how you believe the dynamics\nof that world operates the nice thing\nabout doing that is once you have a\nmodel or an estimate of a model you're\nable to anticipate you're able to plan\ninto the future you're able to use the\nmodel to in a branching way predict how\nyour actions will change the world\nso you can plan far into the future this\nis the mechanism by which you can you\ncan do chess in the simplest form\nbecause in chess you don't even need to\nlearn the model the models learnt is\ngiven to you chess go and so on\nthe most important way in which they're\ndifferent I think is the sample\nefficiency is how many examples of data\nare needed to be able to successfully\noperate in the world and so model based\nmethods because they're constructing a\nmodel if they can are extremely simple\nefficient because once you have a model\nyou can do all kinds of reasoning that\ndoesn't require experiencing every\npossibility of that model you can unroll\nthe model to see how the world changes\nbased on your actions value based\nmethods are ones that look to estimate\nthe quality of states the quality of\ntaking a certain action in the certain\nstate so they're called off policy\nversus the last category that's on\npolicy what does it mean to be off\npolicy it means that they constantly\nvalue based agents constantly update how\ngood is taken action in a state and they\nhave this model of that goodness of\ntaking action in a state and they use\nthat to pick them optimal action they\ndon't directly learn a policy a strategy\nof how to act they learn how good it is\nto be in a state and use that goodness\ninformation to then pick the best one\nand then every once in a while flip a\ncoin in order to explore and then policy\nbased methods our ones that directly\nlearn a policy function so they take as\ninput the the world representation of\nthat world neural networks and this\noutput a action where the action is\nstochastic so okay that's the range of\nmodel-based value based and policy based\nhere's an image from open AI that I\nreally like I encourage you to as we\nfurther explore here to look up spinning\nup in deeper enforcement learning from\nopen AI here's an image that texana\nmises in the way that I described some\nof the recent developments in RL so at\nthe very top the distinction between\nmodel free RL and model-based RL in\nmodel free RL which is what we'll focus\non today there is a distinction between\npolicy optimization so on policy methods\nand q-learning\nwhich is all policy methods pause\noptimizations methods that directly\noptimize the policy they'll directly\nlearn the policy in some way and then\nq-learning off policy methods learn like\nI mentioned the value of taking a\ncertain action in the state and from\nthat learned that learned Q value be\nable to\nchoose how to act in the world so let's\nlook at a few sample representative\napproaches in this space let's start\nwith the with the one that really was\none of the first great breakthroughs\nfrom google deepmind on the deep IRL\nside and solving atari games dqn deep\nqueue learning networks deep queue\nnetworks and let's take a step back and\nthink about what cue learning is\nq-learning looks at the state action\nvalue function queue that estimates\nbased on a particular policy or based on\nan optimal policy how good is it to take\nan action in this state the estimated\nreward if I take an action in this state\nand continue operating under an optimal\noptimal policy it gives you directly a\nway to say amongst all the actions I\nhave which action should that take to\nmaximize the reward now in the beginning\nyou know nothing you know you don't have\nthis value estimation you don't have\nthis cue function so you have to learn\nit and you learn it with a bellman\nequation of updating it you take your\ncurrent estimate and update it with the\nreward you seed received after you take\nan action here it's off policy and model\nfree you don't have to have any estimate\nor knowledge of the world you don't have\nto have any policy whatsoever all you're\ndoing is roaming about the world\ncollecting data when you took a certain\naction here award you received and\nyou're updating gradually this table\nwhere the table has state states on the\ny-axis and actions on the x-axis and the\nkey part there is because you always\nhave an estimate of what of to take an\naction of the value of taking that\naction so you can always take the\noptimal one but because you know very\nlittle in the beginning that optimal is\ngoing to you have no way of knowing\nthat's good or not so there's some\ndegree of expiration the fundamental\naspect of value based methods or ami are\nall methods like I said it's trial and\nerror is exploration so for value based\nmethods that q-learning\nthe way that's done is with the flip of\na coin epsilon greedy with a flip of a\ncoin\nyou can choose to just take a random\naction and you slowly decrease epsilon\nto zero as your agent learns more and\nmore and more so in the beginning you\nexplore a lot with epsilon 1 and epsilon\nof zero in the end when you're just\nacting greedy based on the your\nunderstanding of the world as\nrepresented by the q-value function for\nnon neural network approaches this is\nsimply a table the Q this Q function is\na table like I said on the Y State X\nactions and in each cell you have a\nreward that's at this counter reward\nthat you estimated to be received there\nand as you walk around with this bellami\nequation you can update that table but\nit's a table nevertheless number of\nstates times number of actions now if\nyou look at any practical real-world\nproblem and an arcade game with raw\nsensory input is a very crude first step\ntowards the real world so raw sensor\ninformation this kind of value iteration\nand updating a table is impractical\nbecause here's for a game of break out\nif we look at four consecutive frames of\na game of breakout size of the of the\nraw sensory input is 84 by 84 pixels\ngrayscale every pixel has 256 values\nthat's 256 to the power of whatever 84\ntimes 84 times 4 is whatever it is it's\nsignificantly larger the number of atoms\nin the universe so the size of this cue\ntable if we use the traditional approach\nis intractable\nyou'll know it's to the rescue deep RL\nis rl+ neural networks where the neural\nnetworks is tasked with taking this in\nValley based methods taking this cue\ntable and learning a compress\nrepresentation of it learning an\napproximator for the function from state\naction to the value that's what\npreviously talked about the ability the\npowerful ability of neural networks to\nform representations from extremely high\ndimensional complex raw sensory\ninformation so it's simple the framework\nremains for the most part the same in\nreinforcement learning\nit's just that this cue function for\nvalue based methods becomes a neural\nnetwork and becomes an approximator\nwhere the hope is as you navigate the\nworld and you pick up new knowledge\nthrough the back propagating the\ngradient and the loss function that\nyou're able to form a good\nrepresentation of the optimal q function\nso using your networks with you'll know\nit's a good at which is function\napproximator x' and that's DQ 1 deep Q\nNetwork was used to have the initial\nincredible nice results on our K games\nwhere the input is the raw sensory\npixels with a few convolutional layers\nfor the connected layers and the output\nis a set of actions you know probability\nof taking that action and then you\nsample that and you choose the best\naction and so this simple agent whether\nthe neural network that estimates that Q\nfunction very simple network is able to\nachieve superhuman performance on many\nof these arcade games that excited the\nworld because it's taking raw sensory\ninformation with a pretty simple network\nthat doesn't in the beginning understand\nany of the physics of the world any of\nthe dynamics of the environment and\nthrough that intractable space the\nintractable state space is able to learn\nhow to actually do pretty well the loss\nfunction for DQ n has to Q functions one\nis the expected the predicted Q value of\na taking an action in a particular state\nand the other is the target against\nwhich the loss function is calculated\nwhich is what is the value that you got\nonce you actually take in that action\nand once you've taken that action the\nway you calculate the value is by\nlooking at the next step and choosing\nthe max to Singh if you take the best\naction in the next state what is going\nto be the Q function so there's two\nestimators going on with in terms of\nneural networks those two forward passes\nhere there's two Q's in this equation so\nin traditional DQ n that's just that's\ndone by a single neural network with a\nfew tricks and double DQ n that's done\nby two neural networks and I mentioned\ntricks because with this and with most\nof RL tricks tell a lot of the story a\nlot of what makes\nsystems work is the details in in games\nand robotic systems in these cases the\ntwo biggest tricks for DQ n that will\nreappear and a lot of value based\nmethods is experience replay so think of\nan agent that plays through these games\nas also collecting memories you collect\nthis bank of memories that can then be\nreplayed the power of that one of the\ncentral elements of what makes value\nbased methods attractive is that because\nyou're not directly estimating the\npolicy but are learning the quality of\ntaking an action in a particular state\nthe you're able to then jump around\nthrough your memory and and play\ndifferent aspects of that memory so\nlearn train the network through the\nhistorical data and then the other trick\nsimple is like I said that there is so\nthe loss function has two queues\nso you're it's it's a dragon chasing its\nown tail it's easy for the loss function\nto become unstable so the training does\nnot converge so the trick of fixing a\ntarget Network is taking one of the\nqueues and only updating in every X\nsteps every thousand steps and so on and\ntaking the same kind of network\nit's just fixing it so for the target\nnetwork that defines the loss function\njust keeping it fixed and only updating\nany regulator so you're chasing a fixed\ntarget with a loss function as opposed\nto a dynamic one so you can solve a lot\nof the Atari games with minimal effort\ncome up with some creative solutions\nhere break out here after 10 minutes of\ntraining on the left after a to have 2\nhours of training on the right is coming\nup with some creative solutions again\nit's pretty cool because this is raw\npixels right we're now like there's been\na few years since this breakthrough so\nkind of take it for granted but I still\nfor the most part captivated by just how\nbeautiful it is that from the raw\nsensory information\nneural networks are able to learn to act\nin a way that actually supersedes humans\nin terms of creativity in terms of in\nterms of actual raw performance it's\nreally exciting and games of simple form\nis the cleanest way to demonstrate that\nand you the the same kind of DQ and\nnetwork is able to achieve superhuman\nperformance and a bunch of different\ngames\nthere's improvements to this like dual\nDQ one again the q function can be\ndecomposed which is useful in to the\nvalue estimate of being in that state\nand what's called and in future slides\nthat we called advantage\nso the advantage of taking action in\nthat state the nice thing of the\nadvantage as a measure is that it's a\nmeasure of the action quality relative\nto the average action that could be\ntaken there so if it's very useful\nadvantage versus sort of raw reward is\nthat if all the actions you have to take\nare pretty good you want to know well\nhow much better it is in terms of\noptimism\nthat's a better measure for choosing\nactions in a value-based sense so when\nyou have these two estimates you have\nthese two streams for neural networking\nthe dueling DQ n DG QM where one\nestimates the value the other the\nadvantage and that's again that dueling\nnature is useful for also on the there\nare many states in which the action is\ndecoupled the quality of the actions is\ndecouple from the state so many states\nit doesn't matter which action you take\nso you don't need to learn all the\ndifferent complexities all the topology\nof different actions when you in a\nparticular state and another one is\nprioritize experience for play like I\nsaid experience replay is really key to\nthese algorithms and the thing that\nsinks some of the policy optimization\nmethods and experiments replay is\ncollecting different memories but if you\njust sample randomly in those memories\nyou're now affected the sampled\nexperiences are really affected by the\nfrequency of those experience occurred\nnot their importance so prioritize\nexperience replay assigns a priority a\nvalue based on the magnitude of the\ntemporal difference learned error so the\nthe stuff you have learned the most from\nis given a higher priority and therefore\nyou get to see through the experience\nreplay process that that particular\nexperience more often okay moving on to\npolicy gradients this is on policy\nversus q-learning off policy policy\ngradient\nis directly optimizing the policy where\nthe input is the raw pixels and the\npolicy network represents the forms of\nrepresentations of that environment\nspace and as output produces a\nstochastic estimate a probability of the\ndifferent actions here in the pong the\npixels a single output that produces the\nprobability of moving the paddle up so\nhow do pause gradients vanilla policy\ngrading the very basic works is you\nunroll the environment you play through\nthe environment here pong moving the\npaddle up and down and so on collecting\nno rewards and only collecting reward at\nthe very end based on whether you win or\nlose every single action you're taking\nalong the way gets either punished or\nrewarded based on whether it led to\nvictory or defeat this also is\nremarkable that this works at all\nbecause the credit assignment there's a\nis I mean every single thing you did\nalong the way is averaged out it's like\nmuddied it's the reason that policy\ngradient methods are more inefficient\nbut it's still very surprising that it\nworks at all so the pros versus DQ one\nthe value based methods is that if the\nworld is so messy that you can't learn a\nq function the nice thing about policy\ngradient because it's learning the\npolicy directly that it will at least\nlearn a pretty good policy usually in\nmany cases faster convergence it's able\nto deal with stochastic policies so\nvalue based methods can out learners the\ngassing policies and it's much more\nnaturally able to deal with continuous\nactions the cons is it's inefficient\nversus dqn it's it can become highly\nunstable as we'll talk about some\nsolutions to this during the training\nprocess and the credit assignment so if\nwe look at the chain of actions that\nlead to a positive reward some might be\nawesome action some may be good action\nsome might be terrible actions but that\ndoesn't matter as long as the death\nthe nation was good and that's then\nevery single action along the way gets a\npositive reinforcement that's the\ndownside and there's now improvements to\nthat advantage actor critic methods a to\nsee combining the best of value based\nmethods and policy base methods so\nhaving an actor two networks an actor\nwhich is policy based and that's the one\nthat's takes the actions samples the\nactions from the policy Network and the\ncritic that measures how good those\nactions are and the critic is value\nbased all right so as opposed to in the\npolicy update the first equation there\nthe reward coming from the destination\nthe that our war being from whether you\nwon the game or not every single step\nalong the way you now learn a Q value\nfunction Q s a state and action using\nthe critic Network so you're able to now\nlearn about the environment about\nevaluating your own actions at every\nstep so you're much more sample\nefficient there's a synchronous from\ndeep mind and synchronous from open AI\nvariants of this but of the actor\nadvantage actor critic framework but\nboth are highly parallelizable the\ndifference with a three C the\nasynchronous one is that every single\nagency just throw these agents operating\nin the environment and they're learning\nthey're rolling out the games and\ngetting the reward they're updating the\noriginal Network asynchronously the\nglobal network parameters asynchronously\nand as a result they're also operating\nconstantly an outdated versions of that\nnetwork the open AI approach that fixes\nthis is that there's a coordinator that\nthere's these rounds where everybody all\nthe agents in parallel are rolling out\nthe episode but then the coordinator\nwaits for everybody to finish in order\nto make the update to the global network\nand then distributes all the same\nparameter\nto all the agents and so that means that\nevery iteration starts with the same\nglobal parameters and that has really\nnice properties in terms of conversions\nand stability of the training process\nokay from google deepmind the deep\ndeterministic policy gradient is\ncombining the ideas of dqn but dealing\nwith continuous action spaces so taking\na policy network but instead of the\nactor actor critic framework but instead\nof picking a stochastic policy having\nthe actor operator on the since the\ncasting nature is picking the best\npicking a deterministic policy so it's\nalways choosing the best action but ok\nwith that the problem quite naturally is\nthat when the policy is now\ndeterministic it's able to do continuous\naction space but because it's termina\nstick it's never exploring so the way we\ninject exploration into the system is by\nadding noise either adding noise into\nthe action space on the output or adding\nnoise into the parameters of the network\nthat have then that create perturbations\nand the actions such that the final\nresult is that you try different kinds\nof things and the the scale of the noise\njust like well the epsilon greedy in the\nexploration for DQ on the scale of the\nnoise decreases as you learn more and\nmore so on the policy optimization side\nfrom open ai and others\nwe'll do a lecture just on this there's\nbeen a lot of exciting work here the\nbasic idea of optimization on policy\noptimization with PPO and TRP au is\nfirst of all we want to formulate\nreinforcement learning as purely an\noptimization problem and second of all\nif policy optimization the actions you\ntake influences the rest of your the\noptimization process you have to be very\ncareful about the actions you take in\nparticular you have to avoid taking\nreally bad actions when you're\nconvergence the the training performance\nin general collapses so how do we do\nthat\nthere's the line search methods which is\nwhere gradient descent or gradient\ndescent falls under which which is the\nhow we train deep neural networks is you\nfirst pick a direction of the gradient\nand then pick the step size the problem\nwith that is that can get you into\ntrouble here there's a nice\nvisualization walking along a ridge is\nit can it can result in you stepping off\nthat Ridge again the collapsing of the\ntraining process the performance the\ntrust region is is the underlying idea\nhere for the for the policy optimization\nmethods that first pick the step size so\nthat constrain in various kinds of ways\nthe the magnitude of the difference to\nthe weights that's applied and then the\ndirection so it placing a much higher\npriority not choosing bad actions that\ncan throw you off the optimization path\nshould actually we should take to that\npath and finally the on the model-based\nmethods and we'll also talk about them\nin the robotics side there's a lot of\ninteresting approaches now where deep\nlearning is starting to be used for a\nmodel-based methods when the model has\nto be learned but of course when the\nmodel doesn't have to be learned it's\ngiven inherent to the game you know the\nmodel like Ingo and chess and so on out\nzero has really done incredible stuff so\nwhat's wise what is the model here so\nthe way that a lot of these games are\napproached you know game of Go it's\nturn-based one person goes and then\nanother person goes and there's this\ngame tree at every point as a set of\nactions that could be taken and quickly\nif you look at that game tree it's it\nbecomes you know a girl's exponentially\nso it becomes huge a game of go is the\nhugest of all in terms of because the\nnumber of choices you have is the\nlargest and there's chess and then you\nknow it gets the checkers and then\ntic-tac-toe and it's just the the degree\nat every step increases decreased based\non the game structure and so the task\nfor a neural network there is to learn\nthe quality of the board it's that it's\nto learn which boards which game\npositions are most likely to result in a\nare most useful to explore and a result\nin a highly successful state so that\nchoice of what's good to explore what's\nwhat branch is good to go down is where\nwe can have neural network step in and\nwithout phago it was pre trained the\nfirst success that beat the world\nchampion was pre trained on expert games\nthen with alphago zero\nit was no pre training on expert systems\nso no imitation learning is just purely\nthrough self play through suggesting\nthrough playing itself new board\npositions many of these systems use\nMonte Carlo tree search and during the\nsearch balancing exploitation\nexploration so going deep on promising\npositions based on the estimation then\nyou'll network or with a flip of a coin\nplaying under play positions and so this\nkind of here you can think of as an\nintuition of looking at a board and\nestimating how good that board is and\nalso estimating how good that board is\nlikely to lead to victory down the end\nso as to mean just general quality and\nprobability of leading to victory then\nthe next step forward is alpha zero\nusing the same similar architecture with\nMCTS what do you call it research but\napplying it to different games and\napplying it and competing against other\nengines state-of-the-art engines and go\nand shogi in chess and outperforming\nthem with very few very few steps so\nhere's this model-based approaches which\nare really extremely simple efficient if\nyou can construct us such a model and in\nin the robotics if you can learn such a\nmodel I can be exceptionally powerful\nhere beating the the engines which are\nfar superior to humans already stockfish\ncan destroy most humans on earth at the\ngame of chess the ability through\nlearning through through estimating the\nquality of a board to be able to defeat\nthese engines is incredible and the the\nexciting aspect here versus engines that\ndon't use neural networks is that the\nnumber its it really has to do with\nbased on the neural network you explore\ncertain positions you explore certain\nparts of the tree and if you look at\ngrandmasters human players in chess they\nseem to explore very few moves they have\na really good neural network at\nestimating which are the likely branches\nwhich would provide value to explore and\non the other side stock fish and so on\nare much more brute force in their\nestimation for the MCTS and then alpha\nzero is a step towards the Grandmaster\nis the number of branches need to be\nexplored as much much fewer a lot of the\nwork is done in the representation form\nby the neural network it's just super\nexciting and then it's able to uh\nperform stockfish in chess it's able to\noutperform Elmo and shogi and it's\nitself in go or the previous iterations\nof alphago zero and so on now the\nchallenge here the sobering truth is\nthat majority of real world application\nof agents that have to act in this world\nperceive the world and act in this world\nare for the most part not based have no\nRL involved so the action is not learned\nuse neural networks to perceive certain\naspects of the world but ultimately the\naction is not is not learned from data\nthat's true for all most of the\nautonomous vehicle companies are all of\nthe autonomous vehicle companies\noperating today and it's true for\nrobotic manipulation in the industrial\nrobotics and any of the humanoid robots\nhave to navigate in this world under\nuncertain conditions all the work from\nBoston Dynamics doesn't involve any\nmachine learning as far as we know now\nthat's beginning to change here with\nanimal the the recent development where\nthe certain aspects of the control a\nrobotic could be learned\nyou're trying to learn more efficient\nmovement you're trying to learn more\nrobust movement on top of the other\ncontrollers so it's quite exciting\nthrough RL to be able to learn some of\nthe control dynamics here that's able to\nteach this particular robot to be able\nto get up from arbitrary positions so\nit's less hard coding in order to be\nable to deal with unexpected nishal\nconditions and unexpected perturbations\nso that's exciting there in terms of\nlearning the control dynamics and some\nof the driving policy\nso maybe behavioral driving behavior\ndecisions\nchanging lanes turning and so on that if\nyou if you were here last week heard\nfrom way moe\nthey they're starting to use some RL in\nterms of the driving policy in order to\nespecially predict the future they're\ntrying to anticipate\nintent modeling predict what the\npedestrians the cars are going to be\nbased on environment that are trying to\nunroll what's happened recently into the\nfuture and beginning to move beyond sort\nof pure end to end on NVIDIA and to end\nlearning approach of the control\ndecisions are actually moving to\nRL and making long-term planning\ndecisions but again the challenge is the\nthe gap the leap needed to go from\nsimulation to real-world all most the\nwork is done from the design of the\nenvironment and the design and the\nreward structure and because most of\nthat work now is in simulation we need\nto either develop better algorithms for\ntransfer learning or close the distance\nbetween simulation in the real world and\nalso we could think outside the box a\nlittle bit at the conversation with\nPeter bill recently one of the leading\nresearchers in deep RL it kind of on the\nside quickly mentioned the the idea is\nthat we don't need to make simulation\nmore realistic what we could do is just\ncreate an infinite number of simulations\nor very large number of simulations and\nthe naturally the regularization aspect\nof having all those simulations will\nmake it so that our our reality is just\nanother sample from those simulations\nand so maybe the solution isn't to\ncreate higher fidelity simulation or to\ncreate transfer learning algorithms\nmaybe it's to build a arbitrary number\nof simulations so then that step towards\ncreating a agent that work that works in\nthe real world is a trivial one and\nmaybe that's exactly whoever created the\nsimulation we're living in and the\nmultiverse that we're living in did next\nsteps the lecture videos will have\nseveral in RL will be made all available\non deep learning that MIT ID you will\nhave several tutorials in RL on github\nthe link is there and I really like the\nessay from open AI on spinning up as a\ndeep our researcher you know if you're\ninterested in getting into research in\nRL what are the steps need to take from\nthe background of developing the\nmathematical background prop stat and\nmultivariate calculus to some of the\nbasics like it's covered last week on\ndeep learning some\nthe basics ideas in RL just terminology\nand so on some basic concepts then\npicking a framework tends to flow our PI\ntorch and learn by doing i implemented\nguram as i mentioned today those are the\ncore RL algorithms so implement all isms\nfrom scratch it should only take about\ntwo hundred three hundred lines of code\nthere actually when you put it down on\npaper quite simple intuitive algorithms\nand then read papers about those\nalgorithms that follow after looking not\nfor the big waving performance the hand\nwaving performance but for the tricks\nthat were used to change these\nalgorithms the tricks tell a lot of the\nstory and that's the useful parts that\nthey need to learn and iterate fast on\nsimple benchmark environments so open\nthe I Jim has provided a lot of easy to\nuse environments that you can play with\nthat you can train an agent in minutes\nhours as opposed to days and weeks and\nso iterating fast is the best way to\nlearn these algorithms and then on the\nresearch side there's three ways to get\na best paper award right two to publish\nand to contribute and have an impact in\nthe research community in in RL one is\nimproving existing approach given us a\nparticular benchmarks there's a few\nbenchmark datasets environments that are\nemerging so you want to improve on the\nexisting approach some aspect of the\nconvergence in the performance you can\nfocus on an unsolved task there's\ncertain games that just haven't been\nsolved through their RL formulation or\nyou can come up with a totally new\nproblem that hasn't been addressed by RL\nbefore so with that I'd like to thank\nyou very much\ntomorrow I'll hope to see you here for\ndeep traffic Thanks\nyou\nyou\n",
  "words": [
    "today",
    "like",
    "overview",
    "exciting",
    "field",
    "deep",
    "reinforcement",
    "learning",
    "introduced",
    "overview",
    "provide",
    "basics",
    "think",
    "one",
    "exciting",
    "fields",
    "artificial",
    "intelligence",
    "marrying",
    "power",
    "ability",
    "deep",
    "neural",
    "networks",
    "represent",
    "comprehend",
    "world",
    "ability",
    "act",
    "understanding",
    "representation",
    "taking",
    "whole",
    "really",
    "creation",
    "intelligent",
    "beings",
    "understand",
    "world",
    "act",
    "exciting",
    "breakthroughs",
    "recently",
    "happened",
    "captivate",
    "imagination",
    "possible",
    "favorite",
    "area",
    "deep",
    "learning",
    "artificial",
    "intelligence",
    "general",
    "hope",
    "feel",
    "deep",
    "reinforcement",
    "learning",
    "talked",
    "deep",
    "learning",
    "taking",
    "samples",
    "data",
    "able",
    "supervised",
    "way",
    "compress",
    "encode",
    "representation",
    "data",
    "way",
    "reason",
    "would",
    "take",
    "power",
    "apply",
    "world",
    "sequential",
    "decisions",
    "made",
    "looking",
    "problems",
    "formulations",
    "tasks",
    "agent",
    "intelligent",
    "system",
    "make",
    "sequence",
    "decisions",
    "decisions",
    "made",
    "effect",
    "world",
    "around",
    "agent",
    "us",
    "intelligent",
    "tasked",
    "operating",
    "world",
    "learn",
    "anything",
    "especially",
    "know",
    "little",
    "beginning",
    "trial",
    "error",
    "fundamental",
    "process",
    "reinforcement",
    "learning",
    "agents",
    "learn",
    "deep",
    "part",
    "deep",
    "reinforcement",
    "learning",
    "neural",
    "networks",
    "using",
    "frameworks",
    "reinforcement",
    "learning",
    "neural",
    "network",
    "representation",
    "world",
    "based",
    "actions",
    "made",
    "take",
    "step",
    "back",
    "look",
    "types",
    "learning",
    "sometimes",
    "terminology",
    "confuse",
    "us",
    "fundamentals",
    "supervised",
    "learning",
    "learning",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "feeling",
    "supervised",
    "learning",
    "really",
    "one",
    "perform",
    "manual",
    "annotation",
    "supervision",
    "case",
    "every",
    "type",
    "machine",
    "learning",
    "supervised",
    "learning",
    "supervised",
    "loss",
    "function",
    "function",
    "tells",
    "good",
    "bad",
    "know",
    "even",
    "looking",
    "existence",
    "humans",
    "figure",
    "good",
    "bad",
    "kinds",
    "sources",
    "direct",
    "indirect",
    "morals",
    "ethics",
    "figure",
    "good",
    "bad",
    "difference",
    "supervised",
    "unsupervised",
    "reinforcement",
    "learning",
    "source",
    "supervision",
    "implied",
    "say",
    "unsupervised",
    "cost",
    "human",
    "labor",
    "required",
    "attain",
    "supervision",
    "low",
    "never",
    "turtles",
    "way",
    "turtles",
    "human",
    "bottom",
    "point",
    "needs",
    "human",
    "intervention",
    "human",
    "input",
    "provide",
    "good",
    "bad",
    "arise",
    "reinforcement",
    "learning",
    "well",
    "remember",
    "challenges",
    "exciting",
    "opportunities",
    "reinforcement",
    "learning",
    "lie",
    "fact",
    "get",
    "supervision",
    "efficient",
    "way",
    "possible",
    "supervision",
    "nevertheless",
    "required",
    "system",
    "input",
    "output",
    "trying",
    "learn",
    "like",
    "neural",
    "network",
    "provide",
    "output",
    "good",
    "needs",
    "somebody",
    "say",
    "good",
    "bad",
    "curious",
    "books",
    "couple",
    "written",
    "throughout",
    "last",
    "centuries",
    "socrates",
    "nietzsche",
    "recommend",
    "latter",
    "especially",
    "let",
    "look",
    "supervised",
    "learning",
    "reinforcement",
    "learning",
    "let",
    "like",
    "propose",
    "way",
    "think",
    "difference",
    "illustrative",
    "useful",
    "start",
    "talking",
    "techniques",
    "supervised",
    "learning",
    "taking",
    "bunch",
    "examples",
    "data",
    "learning",
    "examples",
    "ground",
    "truth",
    "provides",
    "compressed",
    "semantic",
    "meaning",
    "data",
    "examples",
    "one",
    "one",
    "whether",
    "sequences",
    "single",
    "samples",
    "learn",
    "take",
    "future",
    "samples",
    "interpret",
    "reinforcement",
    "learning",
    "teaching",
    "teach",
    "agent",
    "experience",
    "showing",
    "singular",
    "sample",
    "data",
    "set",
    "putting",
    "world",
    "distinction",
    "essential",
    "element",
    "reinforcement",
    "learning",
    "us",
    "talk",
    "bunch",
    "algorithms",
    "essential",
    "design",
    "step",
    "provide",
    "world",
    "experience",
    "agent",
    "learns",
    "world",
    "world",
    "gets",
    "dynamics",
    "world",
    "physics",
    "world",
    "world",
    "gets",
    "rewards",
    "good",
    "bad",
    "us",
    "designers",
    "agent",
    "algorithm",
    "design",
    "world",
    "agent",
    "trying",
    "solve",
    "task",
    "design",
    "world",
    "process",
    "reinforcement",
    "learning",
    "design",
    "examples",
    "annotation",
    "examples",
    "world",
    "supervised",
    "learning",
    "essential",
    "perhaps",
    "difficult",
    "element",
    "reinforcement",
    "learning",
    "reward",
    "good",
    "versus",
    "bad",
    "baby",
    "starts",
    "walking",
    "across",
    "room",
    "want",
    "define",
    "success",
    "baby",
    "walking",
    "across",
    "room",
    "reaching",
    "destination",
    "success",
    "failure",
    "inability",
    "reach",
    "destination",
    "simple",
    "reinforcement",
    "learning",
    "humans",
    "way",
    "learn",
    "examples",
    "appear",
    "learn",
    "examples",
    "trial",
    "error",
    "mystery",
    "beautiful",
    "mystery",
    "full",
    "open",
    "questions",
    "could",
    "huge",
    "amount",
    "data",
    "230",
    "million",
    "years",
    "worth",
    "bipedal",
    "data",
    "walking",
    "mammals",
    "walking",
    "ability",
    "walk",
    "500",
    "million",
    "years",
    "ability",
    "see",
    "eyes",
    "hardware",
    "side",
    "somehow",
    "genetically",
    "encoded",
    "us",
    "ability",
    "comprehend",
    "world",
    "extremely",
    "efficiently",
    "could",
    "hardware",
    "five",
    "hundred",
    "million",
    "years",
    "minutes",
    "hours",
    "days",
    "months",
    "maybe",
    "even",
    "years",
    "beginning",
    "born",
    "ability",
    "learn",
    "really",
    "quickly",
    "observation",
    "aggregate",
    "information",
    "filter",
    "junk",
    "need",
    "able",
    "learn",
    "really",
    "quickly",
    "imitation",
    "learning",
    "observation",
    "way",
    "walking",
    "might",
    "mean",
    "observing",
    "others",
    "talk",
    "idea",
    "around",
    "would",
    "never",
    "able",
    "learn",
    "fundamentals",
    "walking",
    "efficiently",
    "observation",
    "could",
    "algorithm",
    "totally",
    "understood",
    "algorithm",
    "brain",
    "uses",
    "learn",
    "backpropagation",
    "artificial",
    "neural",
    "networks",
    "kind",
    "processes",
    "understood",
    "brain",
    "could",
    "key",
    "want",
    "think",
    "talk",
    "trivial",
    "comparison",
    "accomplishments",
    "reinforcement",
    "learning",
    "take",
    "next",
    "steps",
    "nevertheless",
    "exciting",
    "machines",
    "learn",
    "act",
    "world",
    "process",
    "learning",
    "fallen",
    "love",
    "artificial",
    "intelligence",
    "process",
    "learning",
    "thought",
    "intelligence",
    "ability",
    "know",
    "little",
    "experience",
    "examples",
    "interaction",
    "world",
    "whatever",
    "medium",
    "whether",
    "data",
    "simulation",
    "able",
    "form",
    "much",
    "richer",
    "interesting",
    "representations",
    "world",
    "able",
    "act",
    "world",
    "dream",
    "let",
    "look",
    "stack",
    "age",
    "means",
    "agent",
    "world",
    "top",
    "input",
    "bottom",
    "output",
    "environment",
    "sense",
    "environment",
    "tools",
    "humans",
    "several",
    "sensory",
    "systems",
    "cars",
    "lidar",
    "camera",
    "stereo",
    "vision",
    "audio",
    "microphone",
    "networking",
    "gps",
    "imu",
    "sensor",
    "whatever",
    "robot",
    "think",
    "way",
    "sense",
    "world",
    "raw",
    "sensory",
    "data",
    "raw",
    "sensory",
    "data",
    "tasked",
    "representing",
    "data",
    "way",
    "make",
    "sense",
    "opposed",
    "raw",
    "sensors",
    "cones",
    "taking",
    "giant",
    "stream",
    "high",
    "bandwidth",
    "information",
    "able",
    "form",
    "higher",
    "abstractions",
    "features",
    "based",
    "reason",
    "edges",
    "corners",
    "faces",
    "exactly",
    "deep",
    "learning",
    "neural",
    "networks",
    "stepped",
    "able",
    "automated",
    "fashion",
    "little",
    "human",
    "input",
    "possible",
    "able",
    "form",
    "representations",
    "information",
    "learning",
    "aspect",
    "building",
    "top",
    "greater",
    "abstractions",
    "form",
    "representations",
    "able",
    "accomplish",
    "something",
    "useful",
    "well",
    "discriminative",
    "tasks",
    "generative",
    "task",
    "based",
    "representation",
    "able",
    "make",
    "sense",
    "data",
    "able",
    "generate",
    "new",
    "data",
    "sequence",
    "sequence",
    "sequence",
    "sample",
    "sam",
    "sequence",
    "forth",
    "actions",
    "talk",
    "ability",
    "aggregate",
    "information",
    "received",
    "past",
    "useful",
    "information",
    "pertinent",
    "task",
    "hand",
    "thing",
    "old",
    "looks",
    "like",
    "duck",
    "quacks",
    "like",
    "duck",
    "swims",
    "like",
    "duck",
    "three",
    "different",
    "data",
    "sets",
    "sure",
    "algorithms",
    "three",
    "image",
    "class",
    "education",
    "audio",
    "recognition",
    "video",
    "classification",
    "activity",
    "recognition",
    "aggregating",
    "three",
    "together",
    "still",
    "open",
    "problem",
    "could",
    "last",
    "piece",
    "want",
    "think",
    "think",
    "reinforcement",
    "learning",
    "agents",
    "play",
    "transfer",
    "game",
    "atari",
    "game",
    "go",
    "game",
    "dota",
    "game",
    "robot",
    "navigating",
    "uncertain",
    "environment",
    "real",
    "world",
    "sense",
    "raw",
    "world",
    "representation",
    "world",
    "need",
    "act",
    "provide",
    "actions",
    "within",
    "constraints",
    "world",
    "way",
    "believe",
    "get",
    "us",
    "towards",
    "success",
    "promise",
    "excitement",
    "deep",
    "learning",
    "part",
    "stack",
    "converts",
    "raw",
    "data",
    "meaningful",
    "representations",
    "promise",
    "dream",
    "deeper",
    "enforcement",
    "learning",
    "going",
    "beyond",
    "building",
    "agent",
    "uses",
    "representation",
    "acts",
    "achieve",
    "success",
    "world",
    "super",
    "exciting",
    "framework",
    "formulation",
    "reinforcement",
    "learning",
    "simplest",
    "environment",
    "agent",
    "acts",
    "environment",
    "agent",
    "senses",
    "environment",
    "observation",
    "well",
    "partial",
    "complete",
    "observation",
    "environment",
    "gives",
    "environment",
    "action",
    "acts",
    "environment",
    "action",
    "environment",
    "changes",
    "way",
    "new",
    "observation",
    "occurs",
    "also",
    "provide",
    "actually",
    "make",
    "observations",
    "receive",
    "reward",
    "formulations",
    "framework",
    "entire",
    "system",
    "memory",
    "thing",
    "two",
    "could",
    "concerned",
    "state",
    "came",
    "state",
    "arrived",
    "reward",
    "received",
    "open",
    "question",
    "ca",
    "modeled",
    "kind",
    "way",
    "model",
    "human",
    "life",
    "game",
    "go",
    "model",
    "way",
    "good",
    "way",
    "formulate",
    "learning",
    "problem",
    "robotic",
    "systems",
    "real",
    "world",
    "simulated",
    "world",
    "open",
    "questions",
    "environment",
    "could",
    "fully",
    "observable",
    "partially",
    "observable",
    "like",
    "poker",
    "could",
    "single",
    "agent",
    "multi",
    "agent",
    "atari",
    "versus",
    "driving",
    "like",
    "deep",
    "traffic",
    "deterministic",
    "stochastic",
    "static",
    "versus",
    "dynamic",
    "static",
    "chess",
    "dynamic",
    "driving",
    "applications",
    "screen",
    "versus",
    "continuous",
    "like",
    "games",
    "chess",
    "continuous",
    "carpal",
    "balancing",
    "polo",
    "cart",
    "challenge",
    "rl",
    "real",
    "world",
    "applications",
    "reminder",
    "supervised",
    "learning",
    "teaching",
    "example",
    "learning",
    "example",
    "teaching",
    "perspective",
    "reinforcement",
    "learning",
    "teaching",
    "experience",
    "way",
    "provide",
    "experience",
    "reinforcement",
    "learning",
    "agents",
    "currently",
    "part",
    "simulation",
    "highly",
    "constrained",
    "scenarios",
    "challenge",
    "fact",
    "successes",
    "systems",
    "environments",
    "simulated",
    "two",
    "ways",
    "close",
    "gap",
    "directions",
    "research",
    "work",
    "one",
    "improve",
    "algorithms",
    "improve",
    "ability",
    "algorithm",
    "student",
    "form",
    "policies",
    "transferable",
    "across",
    "kinds",
    "domains",
    "including",
    "real",
    "world",
    "including",
    "especially",
    "real",
    "world",
    "train",
    "simulation",
    "transfer",
    "real",
    "world",
    "improve",
    "simulation",
    "way",
    "fidelity",
    "simulation",
    "increased",
    "increases",
    "point",
    "gap",
    "reality",
    "simulation",
    "minimal",
    "degree",
    "things",
    "learn",
    "simulation",
    "directly",
    "trivially",
    "transferable",
    "real",
    "world",
    "okay",
    "major",
    "components",
    "rl",
    "agent",
    "agent",
    "operates",
    "based",
    "strategy",
    "called",
    "policy",
    "sees",
    "world",
    "makes",
    "decision",
    "policy",
    "makes",
    "decision",
    "act",
    "sees",
    "reward",
    "sees",
    "new",
    "state",
    "acts",
    "sees",
    "reward",
    "new",
    "states",
    "acts",
    "repeats",
    "forever",
    "terminal",
    "state",
    "value",
    "function",
    "estimate",
    "good",
    "state",
    "good",
    "state",
    "action",
    "pair",
    "meaning",
    "taking",
    "action",
    "particular",
    "state",
    "good",
    "ability",
    "evaluate",
    "model",
    "different",
    "environment",
    "perspective",
    "agent",
    "environment",
    "model",
    "based",
    "operates",
    "agent",
    "representation",
    "best",
    "understanding",
    "model",
    "purpose",
    "rl",
    "agent",
    "simply",
    "formulated",
    "framework",
    "maximize",
    "reward",
    "way",
    "reward",
    "mathematically",
    "practically",
    "talked",
    "discounted",
    "framework",
    "discount",
    "future",
    "award",
    "reward",
    "farther",
    "future",
    "means",
    "less",
    "us",
    "terms",
    "maximization",
    "reward",
    "near",
    "term",
    "discount",
    "first",
    "lot",
    "math",
    "trick",
    "able",
    "prove",
    "certain",
    "aspects",
    "analyze",
    "certain",
    "aspects",
    "convergence",
    "general",
    "philosophical",
    "sense",
    "environments",
    "either",
    "thought",
    "stochastic",
    "random",
    "difficult",
    "degree",
    "uncertainty",
    "makes",
    "difficult",
    "really",
    "estimate",
    "reward",
    "future",
    "ripple",
    "effect",
    "uncertainty",
    "let",
    "look",
    "example",
    "simple",
    "one",
    "helps",
    "us",
    "understand",
    "policy",
    "rewards",
    "actions",
    "robot",
    "room",
    "12",
    "cells",
    "step",
    "starts",
    "bottom",
    "left",
    "tries",
    "get",
    "rewards",
    "top",
    "right",
    "plus",
    "one",
    "really",
    "good",
    "thing",
    "top",
    "right",
    "wants",
    "get",
    "walking",
    "around",
    "negative",
    "1",
    "really",
    "bad",
    "wants",
    "avoid",
    "square",
    "choice",
    "action",
    "actions",
    "could",
    "think",
    "negative",
    "reward",
    "point",
    "0",
    "4",
    "step",
    "cost",
    "step",
    "stochastic",
    "nature",
    "world",
    "potentially",
    "talk",
    "deterministic",
    "stochastic",
    "stochastic",
    "case",
    "choose",
    "action",
    "80",
    "probability",
    "80",
    "chance",
    "move",
    "10",
    "chance",
    "move",
    "left",
    "another",
    "10",
    "move",
    "right",
    "catholic",
    "nature",
    "even",
    "though",
    "try",
    "go",
    "might",
    "end",
    "blocks",
    "left",
    "right",
    "deterministic",
    "world",
    "optimal",
    "policy",
    "given",
    "always",
    "start",
    "bottom",
    "left",
    "really",
    "shortest",
    "path",
    "know",
    "ca",
    "ever",
    "stochasticity",
    "never",
    "gon",
    "na",
    "screw",
    "fall",
    "hole",
    "negative",
    "1",
    "hole",
    "compute",
    "shortest",
    "path",
    "walk",
    "along",
    "shortest",
    "path",
    "shortest",
    "path",
    "every",
    "single",
    "step",
    "hurts",
    "negative",
    "reward",
    "point",
    "0",
    "4",
    "shortest",
    "path",
    "thing",
    "minimizes",
    "reward",
    "shortest",
    "path",
    "plus",
    "1",
    "block",
    "ok",
    "let",
    "look",
    "stochastic",
    "world",
    "like",
    "mentioned",
    "80",
    "split",
    "20",
    "10",
    "left",
    "right",
    "policy",
    "change",
    "well",
    "first",
    "need",
    "need",
    "plan",
    "every",
    "single",
    "block",
    "area",
    "might",
    "end",
    "due",
    "castus",
    "world",
    "ok",
    "basic",
    "addition",
    "trying",
    "go",
    "avoid",
    "closer",
    "get",
    "negative",
    "one",
    "hole",
    "try",
    "avoid",
    "stochastic",
    "nature",
    "means",
    "might",
    "fall",
    "hole",
    "10",
    "chance",
    "given",
    "point",
    "zero",
    "step",
    "reward",
    "willing",
    "take",
    "long",
    "way",
    "home",
    "cases",
    "order",
    "avoid",
    "possibility",
    "negative",
    "one",
    "possibility",
    "let",
    "look",
    "reward",
    "step",
    "decreases",
    "negative",
    "two",
    "really",
    "hurts",
    "take",
    "every",
    "step",
    "go",
    "shortest",
    "path",
    "despite",
    "fact",
    "stochastic",
    "nature",
    "fact",
    "really",
    "care",
    "step",
    "negative",
    "one",
    "hole",
    "every",
    "step",
    "really",
    "hurts",
    "want",
    "get",
    "home",
    "play",
    "reward",
    "structure",
    "right",
    "yes",
    "instead",
    "negative",
    "2",
    "negative",
    "point",
    "0",
    "4",
    "look",
    "negative",
    "see",
    "immediately",
    "structure",
    "policy",
    "changes",
    "higher",
    "value",
    "higher",
    "negative",
    "reward",
    "free",
    "step",
    "immediately",
    "urgency",
    "agent",
    "increases",
    "versus",
    "less",
    "urgency",
    "lower",
    "negative",
    "reward",
    "reward",
    "flips",
    "positive",
    "every",
    "step",
    "positive",
    "entire",
    "system",
    "actually",
    "quite",
    "common",
    "reinforcement",
    "learning",
    "entire",
    "system",
    "full",
    "positive",
    "rewards",
    "optimal",
    "policy",
    "becomes",
    "longest",
    "path",
    "grad",
    "school",
    "taking",
    "long",
    "possible",
    "never",
    "reaching",
    "destination",
    "lessons",
    "draw",
    "robot",
    "room",
    "two",
    "things",
    "environment",
    "model",
    "dynamics",
    "trivial",
    "example",
    "stochastic",
    "nature",
    "difference",
    "80",
    "percent",
    "100",
    "percent",
    "50",
    "percent",
    "model",
    "world",
    "environment",
    "big",
    "impact",
    "optimal",
    "policy",
    "reward",
    "structure",
    "importantly",
    "thing",
    "often",
    "control",
    "constructs",
    "task",
    "try",
    "solve",
    "enforcement",
    "good",
    "bad",
    "bad",
    "good",
    "reward",
    "structure",
    "big",
    "impact",
    "complete",
    "change",
    "like",
    "like",
    "robert",
    "frost",
    "say",
    "complete",
    "change",
    "policy",
    "choices",
    "agent",
    "makes",
    "formulate",
    "reinforcement",
    "learning",
    "framework",
    "researchers",
    "students",
    "often",
    "design",
    "environment",
    "design",
    "world",
    "system",
    "learns",
    "even",
    "ultimate",
    "goal",
    "physical",
    "robot",
    "still",
    "lot",
    "work",
    "still",
    "done",
    "simulation",
    "design",
    "world",
    "parameters",
    "world",
    "also",
    "design",
    "reward",
    "structure",
    "transformative",
    "results",
    "slight",
    "variations",
    "parameters",
    "going",
    "huge",
    "results",
    "huge",
    "differences",
    "policy",
    "arrived",
    "course",
    "example",
    "shown",
    "really",
    "love",
    "impact",
    "changing",
    "reward",
    "structure",
    "might",
    "unintended",
    "consequences",
    "consequences",
    "system",
    "obviously",
    "highly",
    "detrimental",
    "costs",
    "failed",
    "game",
    "atari",
    "human",
    "performing",
    "task",
    "gate",
    "playing",
    "game",
    "coast",
    "runners",
    "racing",
    "around",
    "track",
    "finish",
    "first",
    "finish",
    "fast",
    "get",
    "lot",
    "points",
    "natural",
    "okay",
    "let",
    "rl",
    "agent",
    "optimize",
    "points",
    "find",
    "game",
    "also",
    "get",
    "points",
    "picking",
    "little",
    "green",
    "turbo",
    "things",
    "agent",
    "figures",
    "actually",
    "get",
    "lot",
    "points",
    "even",
    "simply",
    "focusing",
    "green",
    "turbos",
    "focusing",
    "green",
    "turbos",
    "rotating",
    "slamming",
    "wall",
    "fire",
    "everything",
    "picking",
    "especially",
    "ability",
    "pick",
    "turbos",
    "avoid",
    "terminal",
    "state",
    "end",
    "finishing",
    "race",
    "fact",
    "finishing",
    "race",
    "means",
    "stop",
    "collecting",
    "positive",
    "reward",
    "never",
    "want",
    "finish",
    "collected",
    "turbos",
    "though",
    "trivial",
    "example",
    "actually",
    "easy",
    "find",
    "examples",
    "unintended",
    "consequences",
    "highly",
    "negative",
    "detrimental",
    "effects",
    "put",
    "real",
    "world",
    "talk",
    "little",
    "bit",
    "robotics",
    "put",
    "robots",
    "wheeled",
    "ones",
    "like",
    "autonomous",
    "vehicles",
    "real",
    "world",
    "objective",
    "functions",
    "navigate",
    "difficult",
    "intersections",
    "full",
    "pedestrians",
    "form",
    "intent",
    "models",
    "pedestrians",
    "see",
    "cars",
    "asserting",
    "dense",
    "intersections",
    "taking",
    "risks",
    "within",
    "risks",
    "taking",
    "us",
    "humans",
    "drive",
    "vehicles",
    "encode",
    "ability",
    "take",
    "subtle",
    "risk",
    "ai",
    "based",
    "control",
    "algorithms",
    "perception",
    "think",
    "end",
    "day",
    "objective",
    "function",
    "objective",
    "function",
    "anticipate",
    "green",
    "turbos",
    "collected",
    "result",
    "understand",
    "consequences",
    "could",
    "negative",
    "effects",
    "especially",
    "situations",
    "involve",
    "human",
    "life",
    "field",
    "ai",
    "safety",
    "folks",
    "talk",
    "deep",
    "mind",
    "open",
    "ai",
    "incredible",
    "work",
    "rl",
    "also",
    "groups",
    "working",
    "ai",
    "safety",
    "good",
    "reason",
    "problem",
    "believe",
    "artificial",
    "intelligent",
    "define",
    "impactful",
    "positive",
    "things",
    "21st",
    "century",
    "also",
    "believe",
    "nowhere",
    "close",
    "solving",
    "fundamental",
    "problems",
    "ai",
    "safety",
    "also",
    "need",
    "address",
    "algorithms",
    "okay",
    "examples",
    "reinforcement",
    "learning",
    "systems",
    "formulation",
    "rewards",
    "formulation",
    "states",
    "actions",
    "traditional",
    "often",
    "used",
    "benchmark",
    "cart",
    "balancing",
    "poll",
    "continuous",
    "action",
    "horizontal",
    "force",
    "cart",
    "goal",
    "balance",
    "poll",
    "stays",
    "top",
    "moving",
    "cart",
    "reward",
    "one",
    "time",
    "step",
    "poll",
    "upright",
    "state",
    "measured",
    "cart",
    "agent",
    "pole",
    "angle",
    "angular",
    "speed",
    "course",
    "self",
    "sensing",
    "cart",
    "position",
    "horizontal",
    "velocity",
    "another",
    "example",
    "want",
    "include",
    "video",
    "really",
    "disturbing",
    "want",
    "include",
    "slide",
    "really",
    "important",
    "think",
    "sensing",
    "raw",
    "pixels",
    "learning",
    "teaching",
    "agent",
    "play",
    "game",
    "doom",
    "goal",
    "eliminate",
    "opponents",
    "state",
    "raw",
    "game",
    "pixels",
    "action",
    "shoot",
    "reload",
    "positive",
    "reward",
    "opponent",
    "eliminated",
    "negative",
    "one",
    "agent",
    "eliminated",
    "simple",
    "added",
    "topic",
    "ai",
    "safety",
    "think",
    "objective",
    "functions",
    "translate",
    "world",
    "autonomous",
    "vehicles",
    "things",
    "even",
    "directly",
    "harm",
    "like",
    "autonomous",
    "weapon",
    "systems",
    "lecture",
    "agi",
    "series",
    "robotics",
    "platform",
    "manipulate",
    "object",
    "manipulation",
    "grasping",
    "objects",
    "benchmarks",
    "interesting",
    "applications",
    "learning",
    "problem",
    "grabbing",
    "objects",
    "moving",
    "objects",
    "manipulating",
    "objects",
    "rotating",
    "especially",
    "objects",
    "complicated",
    "shapes",
    "goal",
    "pick",
    "object",
    "purely",
    "grasping",
    "objects",
    "allenge",
    "state",
    "visual",
    "racial",
    "slurs",
    "visual",
    "visual",
    "base",
    "raw",
    "pixels",
    "objects",
    "actions",
    "move",
    "arm",
    "grasp",
    "object",
    "pick",
    "obviously",
    "positive",
    "pickup",
    "successful",
    "reason",
    "personally",
    "excited",
    "finally",
    "allow",
    "us",
    "solve",
    "problem",
    "claw",
    "torturing",
    "many",
    "years",
    "know",
    "excited",
    "okay",
    "think",
    "get",
    "greater",
    "greater",
    "degree",
    "application",
    "real",
    "world",
    "robotics",
    "like",
    "cars",
    "main",
    "focus",
    "passion",
    "terms",
    "robotics",
    "encode",
    "things",
    "us",
    "humans",
    "encode",
    "know",
    "think",
    "objective",
    "function",
    "reward",
    "structure",
    "model",
    "environment",
    "perceive",
    "reasonable",
    "order",
    "encode",
    "machines",
    "believe",
    "autonomous",
    "driving",
    "category",
    "ask",
    "questions",
    "ethics",
    "ask",
    "questions",
    "risk",
    "value",
    "human",
    "life",
    "value",
    "efficiency",
    "money",
    "front",
    "ethical",
    "questions",
    "autonomous",
    "vehicle",
    "unfortunately",
    "solve",
    "becomes",
    "fully",
    "autonomous",
    "key",
    "takeaways",
    "impact",
    "reinforcement",
    "learning",
    "agents",
    "deep",
    "learning",
    "side",
    "okay",
    "neural",
    "networks",
    "form",
    "high",
    "representation",
    "fun",
    "part",
    "algorithms",
    "different",
    "architectures",
    "different",
    "structures",
    "attentions",
    "self",
    "attention",
    "recurrent",
    "sallust",
    "engr",
    "use",
    "fun",
    "architectures",
    "data",
    "ability",
    "leverage",
    "different",
    "data",
    "sets",
    "order",
    "discriminate",
    "better",
    "perform",
    "crematory",
    "tasks",
    "better",
    "know",
    "mit",
    "better",
    "stand",
    "kind",
    "thing",
    "fun",
    "part",
    "hard",
    "part",
    "asking",
    "good",
    "questions",
    "collecting",
    "huge",
    "amounts",
    "data",
    "representative",
    "task",
    "real",
    "world",
    "impact",
    "cvpr",
    "publication",
    "impact",
    "huge",
    "amount",
    "data",
    "deeper",
    "enforcement",
    "learning",
    "side",
    "key",
    "challenge",
    "fun",
    "part",
    "algorithms",
    "learn",
    "data",
    "stuff",
    "talk",
    "today",
    "hard",
    "part",
    "defining",
    "environment",
    "defining",
    "acts",
    "space",
    "reward",
    "structure",
    "mentioned",
    "big",
    "challenge",
    "hardest",
    "part",
    "crack",
    "gap",
    "simulation",
    "real",
    "world",
    "leaping",
    "lizard",
    "hardest",
    "part",
    "even",
    "know",
    "solve",
    "transfer",
    "learning",
    "problem",
    "yet",
    "real",
    "world",
    "fact",
    "three",
    "types",
    "reinforcement",
    "learning",
    "countless",
    "algorithms",
    "lot",
    "ways",
    "economize",
    "highest",
    "level",
    "model",
    "free",
    "model",
    "based",
    "algorithms",
    "learn",
    "model",
    "world",
    "interact",
    "world",
    "construct",
    "estimate",
    "believe",
    "dynamics",
    "world",
    "operates",
    "nice",
    "thing",
    "model",
    "estimate",
    "model",
    "able",
    "anticipate",
    "able",
    "plan",
    "future",
    "able",
    "use",
    "model",
    "branching",
    "way",
    "predict",
    "actions",
    "change",
    "world",
    "plan",
    "far",
    "future",
    "mechanism",
    "chess",
    "simplest",
    "form",
    "chess",
    "even",
    "need",
    "learn",
    "model",
    "models",
    "learnt",
    "given",
    "chess",
    "go",
    "important",
    "way",
    "different",
    "think",
    "sample",
    "efficiency",
    "many",
    "examples",
    "data",
    "needed",
    "able",
    "successfully",
    "operate",
    "world",
    "model",
    "based",
    "methods",
    "constructing",
    "model",
    "extremely",
    "simple",
    "efficient",
    "model",
    "kinds",
    "reasoning",
    "require",
    "experiencing",
    "every",
    "possibility",
    "model",
    "unroll",
    "model",
    "see",
    "world",
    "changes",
    "based",
    "actions",
    "value",
    "based",
    "methods",
    "ones",
    "look",
    "estimate",
    "quality",
    "states",
    "quality",
    "taking",
    "certain",
    "action",
    "certain",
    "state",
    "called",
    "policy",
    "versus",
    "last",
    "category",
    "policy",
    "mean",
    "policy",
    "means",
    "constantly",
    "value",
    "based",
    "agents",
    "constantly",
    "update",
    "good",
    "taken",
    "action",
    "state",
    "model",
    "goodness",
    "taking",
    "action",
    "state",
    "use",
    "pick",
    "optimal",
    "action",
    "directly",
    "learn",
    "policy",
    "strategy",
    "act",
    "learn",
    "good",
    "state",
    "use",
    "goodness",
    "information",
    "pick",
    "best",
    "one",
    "every",
    "flip",
    "coin",
    "order",
    "explore",
    "policy",
    "based",
    "methods",
    "ones",
    "directly",
    "learn",
    "policy",
    "function",
    "take",
    "input",
    "world",
    "representation",
    "world",
    "neural",
    "networks",
    "output",
    "action",
    "action",
    "stochastic",
    "okay",
    "range",
    "value",
    "based",
    "policy",
    "based",
    "image",
    "open",
    "ai",
    "really",
    "like",
    "encourage",
    "explore",
    "look",
    "spinning",
    "deeper",
    "enforcement",
    "learning",
    "open",
    "ai",
    "image",
    "texana",
    "mises",
    "way",
    "described",
    "recent",
    "developments",
    "rl",
    "top",
    "distinction",
    "model",
    "free",
    "rl",
    "rl",
    "model",
    "free",
    "rl",
    "focus",
    "today",
    "distinction",
    "policy",
    "optimization",
    "policy",
    "methods",
    "policy",
    "methods",
    "pause",
    "optimizations",
    "methods",
    "directly",
    "optimize",
    "policy",
    "directly",
    "learn",
    "policy",
    "way",
    "policy",
    "methods",
    "learn",
    "like",
    "mentioned",
    "value",
    "taking",
    "certain",
    "action",
    "state",
    "learned",
    "learned",
    "q",
    "value",
    "able",
    "choose",
    "act",
    "world",
    "let",
    "look",
    "sample",
    "representative",
    "approaches",
    "space",
    "let",
    "start",
    "one",
    "really",
    "one",
    "first",
    "great",
    "breakthroughs",
    "google",
    "deepmind",
    "deep",
    "irl",
    "side",
    "solving",
    "atari",
    "games",
    "dqn",
    "deep",
    "queue",
    "learning",
    "networks",
    "deep",
    "queue",
    "networks",
    "let",
    "take",
    "step",
    "back",
    "think",
    "cue",
    "learning",
    "looks",
    "state",
    "action",
    "value",
    "function",
    "queue",
    "estimates",
    "based",
    "particular",
    "policy",
    "based",
    "optimal",
    "policy",
    "good",
    "take",
    "action",
    "state",
    "estimated",
    "reward",
    "take",
    "action",
    "state",
    "continue",
    "operating",
    "optimal",
    "optimal",
    "policy",
    "gives",
    "directly",
    "way",
    "say",
    "amongst",
    "actions",
    "action",
    "take",
    "maximize",
    "reward",
    "beginning",
    "know",
    "nothing",
    "know",
    "value",
    "estimation",
    "cue",
    "function",
    "learn",
    "learn",
    "bellman",
    "equation",
    "updating",
    "take",
    "current",
    "estimate",
    "update",
    "reward",
    "seed",
    "received",
    "take",
    "action",
    "policy",
    "model",
    "free",
    "estimate",
    "knowledge",
    "world",
    "policy",
    "whatsoever",
    "roaming",
    "world",
    "collecting",
    "data",
    "took",
    "certain",
    "action",
    "award",
    "received",
    "updating",
    "gradually",
    "table",
    "table",
    "state",
    "states",
    "actions",
    "key",
    "part",
    "always",
    "estimate",
    "take",
    "action",
    "value",
    "taking",
    "action",
    "always",
    "take",
    "optimal",
    "one",
    "know",
    "little",
    "beginning",
    "optimal",
    "going",
    "way",
    "knowing",
    "good",
    "degree",
    "expiration",
    "fundamental",
    "aspect",
    "value",
    "based",
    "methods",
    "ami",
    "methods",
    "like",
    "said",
    "trial",
    "error",
    "exploration",
    "value",
    "based",
    "methods",
    "way",
    "done",
    "flip",
    "coin",
    "epsilon",
    "greedy",
    "flip",
    "coin",
    "choose",
    "take",
    "random",
    "action",
    "slowly",
    "decrease",
    "epsilon",
    "zero",
    "agent",
    "learns",
    "beginning",
    "explore",
    "lot",
    "epsilon",
    "1",
    "epsilon",
    "zero",
    "end",
    "acting",
    "greedy",
    "based",
    "understanding",
    "world",
    "represented",
    "function",
    "non",
    "neural",
    "network",
    "approaches",
    "simply",
    "table",
    "q",
    "q",
    "function",
    "table",
    "like",
    "said",
    "state",
    "x",
    "actions",
    "cell",
    "reward",
    "counter",
    "reward",
    "estimated",
    "received",
    "walk",
    "around",
    "bellami",
    "equation",
    "update",
    "table",
    "table",
    "nevertheless",
    "number",
    "states",
    "times",
    "number",
    "actions",
    "look",
    "practical",
    "problem",
    "arcade",
    "game",
    "raw",
    "sensory",
    "input",
    "crude",
    "first",
    "step",
    "towards",
    "real",
    "world",
    "raw",
    "sensor",
    "information",
    "kind",
    "value",
    "iteration",
    "updating",
    "table",
    "impractical",
    "game",
    "break",
    "look",
    "four",
    "consecutive",
    "frames",
    "game",
    "breakout",
    "size",
    "raw",
    "sensory",
    "input",
    "84",
    "84",
    "pixels",
    "grayscale",
    "every",
    "pixel",
    "256",
    "values",
    "256",
    "power",
    "whatever",
    "84",
    "times",
    "84",
    "times",
    "4",
    "whatever",
    "significantly",
    "larger",
    "number",
    "atoms",
    "universe",
    "size",
    "cue",
    "table",
    "use",
    "traditional",
    "approach",
    "intractable",
    "know",
    "rescue",
    "deep",
    "rl",
    "neural",
    "networks",
    "neural",
    "networks",
    "tasked",
    "taking",
    "valley",
    "based",
    "methods",
    "taking",
    "cue",
    "table",
    "learning",
    "compress",
    "representation",
    "learning",
    "approximator",
    "function",
    "state",
    "action",
    "value",
    "previously",
    "talked",
    "ability",
    "powerful",
    "ability",
    "neural",
    "networks",
    "form",
    "representations",
    "extremely",
    "high",
    "dimensional",
    "complex",
    "raw",
    "sensory",
    "information",
    "simple",
    "framework",
    "remains",
    "part",
    "reinforcement",
    "learning",
    "cue",
    "function",
    "value",
    "based",
    "methods",
    "becomes",
    "neural",
    "network",
    "becomes",
    "approximator",
    "hope",
    "navigate",
    "world",
    "pick",
    "new",
    "knowledge",
    "back",
    "propagating",
    "gradient",
    "loss",
    "function",
    "able",
    "form",
    "good",
    "representation",
    "optimal",
    "q",
    "function",
    "using",
    "networks",
    "know",
    "good",
    "function",
    "approximator",
    "x",
    "dq",
    "1",
    "deep",
    "q",
    "network",
    "used",
    "initial",
    "incredible",
    "nice",
    "results",
    "k",
    "games",
    "input",
    "raw",
    "sensory",
    "pixels",
    "convolutional",
    "layers",
    "connected",
    "layers",
    "output",
    "set",
    "actions",
    "know",
    "probability",
    "taking",
    "action",
    "sample",
    "choose",
    "best",
    "action",
    "simple",
    "agent",
    "whether",
    "neural",
    "network",
    "estimates",
    "q",
    "function",
    "simple",
    "network",
    "able",
    "achieve",
    "superhuman",
    "performance",
    "many",
    "arcade",
    "games",
    "excited",
    "world",
    "taking",
    "raw",
    "sensory",
    "information",
    "pretty",
    "simple",
    "network",
    "beginning",
    "understand",
    "physics",
    "world",
    "dynamics",
    "environment",
    "intractable",
    "space",
    "intractable",
    "state",
    "space",
    "able",
    "learn",
    "actually",
    "pretty",
    "well",
    "loss",
    "function",
    "dq",
    "n",
    "q",
    "functions",
    "one",
    "expected",
    "predicted",
    "q",
    "value",
    "taking",
    "action",
    "particular",
    "state",
    "target",
    "loss",
    "function",
    "calculated",
    "value",
    "got",
    "actually",
    "take",
    "action",
    "taken",
    "action",
    "way",
    "calculate",
    "value",
    "looking",
    "next",
    "step",
    "choosing",
    "max",
    "singh",
    "take",
    "best",
    "action",
    "next",
    "state",
    "going",
    "q",
    "function",
    "two",
    "estimators",
    "going",
    "terms",
    "neural",
    "networks",
    "two",
    "forward",
    "passes",
    "two",
    "q",
    "equation",
    "traditional",
    "dq",
    "n",
    "done",
    "single",
    "neural",
    "network",
    "tricks",
    "double",
    "dq",
    "n",
    "done",
    "two",
    "neural",
    "networks",
    "mentioned",
    "tricks",
    "rl",
    "tricks",
    "tell",
    "lot",
    "story",
    "lot",
    "makes",
    "systems",
    "work",
    "details",
    "games",
    "robotic",
    "systems",
    "cases",
    "two",
    "biggest",
    "tricks",
    "dq",
    "n",
    "reappear",
    "lot",
    "value",
    "based",
    "methods",
    "experience",
    "replay",
    "think",
    "agent",
    "plays",
    "games",
    "also",
    "collecting",
    "memories",
    "collect",
    "bank",
    "memories",
    "replayed",
    "power",
    "one",
    "central",
    "elements",
    "makes",
    "value",
    "based",
    "methods",
    "attractive",
    "directly",
    "estimating",
    "policy",
    "learning",
    "quality",
    "taking",
    "action",
    "particular",
    "state",
    "able",
    "jump",
    "around",
    "memory",
    "play",
    "different",
    "aspects",
    "memory",
    "learn",
    "train",
    "network",
    "historical",
    "data",
    "trick",
    "simple",
    "like",
    "said",
    "loss",
    "function",
    "two",
    "queues",
    "dragon",
    "chasing",
    "tail",
    "easy",
    "loss",
    "function",
    "become",
    "unstable",
    "training",
    "converge",
    "trick",
    "fixing",
    "target",
    "network",
    "taking",
    "one",
    "queues",
    "updating",
    "every",
    "x",
    "steps",
    "every",
    "thousand",
    "steps",
    "taking",
    "kind",
    "network",
    "fixing",
    "target",
    "network",
    "defines",
    "loss",
    "function",
    "keeping",
    "fixed",
    "updating",
    "regulator",
    "chasing",
    "fixed",
    "target",
    "loss",
    "function",
    "opposed",
    "dynamic",
    "one",
    "solve",
    "lot",
    "atari",
    "games",
    "minimal",
    "effort",
    "come",
    "creative",
    "solutions",
    "break",
    "10",
    "minutes",
    "training",
    "left",
    "2",
    "hours",
    "training",
    "right",
    "coming",
    "creative",
    "solutions",
    "pretty",
    "cool",
    "raw",
    "pixels",
    "right",
    "like",
    "years",
    "since",
    "breakthrough",
    "kind",
    "take",
    "granted",
    "still",
    "part",
    "captivated",
    "beautiful",
    "raw",
    "sensory",
    "information",
    "neural",
    "networks",
    "able",
    "learn",
    "act",
    "way",
    "actually",
    "supersedes",
    "humans",
    "terms",
    "creativity",
    "terms",
    "terms",
    "actual",
    "raw",
    "performance",
    "really",
    "exciting",
    "games",
    "simple",
    "form",
    "cleanest",
    "way",
    "demonstrate",
    "kind",
    "dq",
    "network",
    "able",
    "achieve",
    "superhuman",
    "performance",
    "bunch",
    "different",
    "games",
    "improvements",
    "like",
    "dual",
    "dq",
    "one",
    "q",
    "function",
    "decomposed",
    "useful",
    "value",
    "estimate",
    "state",
    "called",
    "future",
    "slides",
    "called",
    "advantage",
    "advantage",
    "taking",
    "action",
    "state",
    "nice",
    "thing",
    "advantage",
    "measure",
    "measure",
    "action",
    "quality",
    "relative",
    "average",
    "action",
    "could",
    "taken",
    "useful",
    "advantage",
    "versus",
    "sort",
    "raw",
    "reward",
    "actions",
    "take",
    "pretty",
    "good",
    "want",
    "know",
    "well",
    "much",
    "better",
    "terms",
    "optimism",
    "better",
    "measure",
    "choosing",
    "actions",
    "sense",
    "two",
    "estimates",
    "two",
    "streams",
    "neural",
    "networking",
    "dueling",
    "dq",
    "n",
    "dg",
    "qm",
    "one",
    "estimates",
    "value",
    "advantage",
    "dueling",
    "nature",
    "useful",
    "also",
    "many",
    "states",
    "action",
    "decoupled",
    "quality",
    "actions",
    "decouple",
    "state",
    "many",
    "states",
    "matter",
    "action",
    "take",
    "need",
    "learn",
    "different",
    "complexities",
    "topology",
    "different",
    "actions",
    "particular",
    "state",
    "another",
    "one",
    "prioritize",
    "experience",
    "play",
    "like",
    "said",
    "experience",
    "replay",
    "really",
    "key",
    "algorithms",
    "thing",
    "sinks",
    "policy",
    "optimization",
    "methods",
    "experiments",
    "replay",
    "collecting",
    "different",
    "memories",
    "sample",
    "randomly",
    "memories",
    "affected",
    "sampled",
    "experiences",
    "really",
    "affected",
    "frequency",
    "experience",
    "occurred",
    "importance",
    "prioritize",
    "experience",
    "replay",
    "assigns",
    "priority",
    "value",
    "based",
    "magnitude",
    "temporal",
    "difference",
    "learned",
    "error",
    "stuff",
    "learned",
    "given",
    "higher",
    "priority",
    "therefore",
    "get",
    "see",
    "experience",
    "replay",
    "process",
    "particular",
    "experience",
    "often",
    "okay",
    "moving",
    "policy",
    "gradients",
    "policy",
    "versus",
    "policy",
    "policy",
    "gradient",
    "directly",
    "optimizing",
    "policy",
    "input",
    "raw",
    "pixels",
    "policy",
    "network",
    "represents",
    "forms",
    "representations",
    "environment",
    "space",
    "output",
    "produces",
    "stochastic",
    "estimate",
    "probability",
    "different",
    "actions",
    "pong",
    "pixels",
    "single",
    "output",
    "produces",
    "probability",
    "moving",
    "paddle",
    "pause",
    "gradients",
    "vanilla",
    "policy",
    "grading",
    "basic",
    "works",
    "unroll",
    "environment",
    "play",
    "environment",
    "pong",
    "moving",
    "paddle",
    "collecting",
    "rewards",
    "collecting",
    "reward",
    "end",
    "based",
    "whether",
    "win",
    "lose",
    "every",
    "single",
    "action",
    "taking",
    "along",
    "way",
    "gets",
    "either",
    "punished",
    "rewarded",
    "based",
    "whether",
    "led",
    "victory",
    "defeat",
    "also",
    "remarkable",
    "works",
    "credit",
    "assignment",
    "mean",
    "every",
    "single",
    "thing",
    "along",
    "way",
    "averaged",
    "like",
    "muddied",
    "reason",
    "policy",
    "gradient",
    "methods",
    "inefficient",
    "still",
    "surprising",
    "works",
    "pros",
    "versus",
    "dq",
    "one",
    "value",
    "based",
    "methods",
    "world",
    "messy",
    "ca",
    "learn",
    "q",
    "function",
    "nice",
    "thing",
    "policy",
    "gradient",
    "learning",
    "policy",
    "directly",
    "least",
    "learn",
    "pretty",
    "good",
    "policy",
    "usually",
    "many",
    "cases",
    "faster",
    "convergence",
    "able",
    "deal",
    "stochastic",
    "policies",
    "value",
    "based",
    "methods",
    "learners",
    "gassing",
    "policies",
    "much",
    "naturally",
    "able",
    "deal",
    "continuous",
    "actions",
    "cons",
    "inefficient",
    "versus",
    "dqn",
    "become",
    "highly",
    "unstable",
    "talk",
    "solutions",
    "training",
    "process",
    "credit",
    "assignment",
    "look",
    "chain",
    "actions",
    "lead",
    "positive",
    "reward",
    "might",
    "awesome",
    "action",
    "may",
    "good",
    "action",
    "might",
    "terrible",
    "actions",
    "matter",
    "long",
    "death",
    "nation",
    "good",
    "every",
    "single",
    "action",
    "along",
    "way",
    "gets",
    "positive",
    "reinforcement",
    "downside",
    "improvements",
    "advantage",
    "actor",
    "critic",
    "methods",
    "see",
    "combining",
    "best",
    "value",
    "based",
    "methods",
    "policy",
    "base",
    "methods",
    "actor",
    "two",
    "networks",
    "actor",
    "policy",
    "based",
    "one",
    "takes",
    "actions",
    "samples",
    "actions",
    "policy",
    "network",
    "critic",
    "measures",
    "good",
    "actions",
    "critic",
    "value",
    "based",
    "right",
    "opposed",
    "policy",
    "update",
    "first",
    "equation",
    "reward",
    "coming",
    "destination",
    "war",
    "whether",
    "game",
    "every",
    "single",
    "step",
    "along",
    "way",
    "learn",
    "q",
    "value",
    "function",
    "q",
    "state",
    "action",
    "using",
    "critic",
    "network",
    "able",
    "learn",
    "environment",
    "evaluating",
    "actions",
    "every",
    "step",
    "much",
    "sample",
    "efficient",
    "synchronous",
    "deep",
    "mind",
    "synchronous",
    "open",
    "ai",
    "variants",
    "actor",
    "advantage",
    "actor",
    "critic",
    "framework",
    "highly",
    "parallelizable",
    "difference",
    "three",
    "c",
    "asynchronous",
    "one",
    "every",
    "single",
    "agency",
    "throw",
    "agents",
    "operating",
    "environment",
    "learning",
    "rolling",
    "games",
    "getting",
    "reward",
    "updating",
    "original",
    "network",
    "asynchronously",
    "global",
    "network",
    "parameters",
    "asynchronously",
    "result",
    "also",
    "operating",
    "constantly",
    "outdated",
    "versions",
    "network",
    "open",
    "ai",
    "approach",
    "fixes",
    "coordinator",
    "rounds",
    "everybody",
    "agents",
    "parallel",
    "rolling",
    "episode",
    "coordinator",
    "waits",
    "everybody",
    "finish",
    "order",
    "make",
    "update",
    "global",
    "network",
    "distributes",
    "parameter",
    "agents",
    "means",
    "every",
    "iteration",
    "starts",
    "global",
    "parameters",
    "really",
    "nice",
    "properties",
    "terms",
    "conversions",
    "stability",
    "training",
    "process",
    "okay",
    "google",
    "deepmind",
    "deep",
    "deterministic",
    "policy",
    "gradient",
    "combining",
    "ideas",
    "dqn",
    "dealing",
    "continuous",
    "action",
    "spaces",
    "taking",
    "policy",
    "network",
    "instead",
    "actor",
    "actor",
    "critic",
    "framework",
    "instead",
    "picking",
    "stochastic",
    "policy",
    "actor",
    "operator",
    "since",
    "casting",
    "nature",
    "picking",
    "best",
    "picking",
    "deterministic",
    "policy",
    "always",
    "choosing",
    "best",
    "action",
    "ok",
    "problem",
    "quite",
    "naturally",
    "policy",
    "deterministic",
    "able",
    "continuous",
    "action",
    "space",
    "termina",
    "stick",
    "never",
    "exploring",
    "way",
    "inject",
    "exploration",
    "system",
    "adding",
    "noise",
    "either",
    "adding",
    "noise",
    "action",
    "space",
    "output",
    "adding",
    "noise",
    "parameters",
    "network",
    "create",
    "perturbations",
    "actions",
    "final",
    "result",
    "try",
    "different",
    "kinds",
    "things",
    "scale",
    "noise",
    "like",
    "well",
    "epsilon",
    "greedy",
    "exploration",
    "dq",
    "scale",
    "noise",
    "decreases",
    "learn",
    "policy",
    "optimization",
    "side",
    "open",
    "ai",
    "others",
    "lecture",
    "lot",
    "exciting",
    "work",
    "basic",
    "idea",
    "optimization",
    "policy",
    "optimization",
    "ppo",
    "trp",
    "au",
    "first",
    "want",
    "formulate",
    "reinforcement",
    "learning",
    "purely",
    "optimization",
    "problem",
    "second",
    "policy",
    "optimization",
    "actions",
    "take",
    "influences",
    "rest",
    "optimization",
    "process",
    "careful",
    "actions",
    "take",
    "particular",
    "avoid",
    "taking",
    "really",
    "bad",
    "actions",
    "convergence",
    "training",
    "performance",
    "general",
    "collapses",
    "line",
    "search",
    "methods",
    "gradient",
    "descent",
    "gradient",
    "descent",
    "falls",
    "train",
    "deep",
    "neural",
    "networks",
    "first",
    "pick",
    "direction",
    "gradient",
    "pick",
    "step",
    "size",
    "problem",
    "get",
    "trouble",
    "nice",
    "visualization",
    "walking",
    "along",
    "ridge",
    "result",
    "stepping",
    "ridge",
    "collapsing",
    "training",
    "process",
    "performance",
    "trust",
    "region",
    "underlying",
    "idea",
    "policy",
    "optimization",
    "methods",
    "first",
    "pick",
    "step",
    "size",
    "constrain",
    "various",
    "kinds",
    "ways",
    "magnitude",
    "difference",
    "weights",
    "applied",
    "direction",
    "placing",
    "much",
    "higher",
    "priority",
    "choosing",
    "bad",
    "actions",
    "throw",
    "optimization",
    "path",
    "actually",
    "take",
    "path",
    "finally",
    "methods",
    "also",
    "talk",
    "robotics",
    "side",
    "lot",
    "interesting",
    "approaches",
    "deep",
    "learning",
    "starting",
    "used",
    "methods",
    "model",
    "learned",
    "course",
    "model",
    "learned",
    "given",
    "inherent",
    "game",
    "know",
    "model",
    "like",
    "ingo",
    "chess",
    "zero",
    "really",
    "done",
    "incredible",
    "stuff",
    "wise",
    "model",
    "way",
    "lot",
    "games",
    "approached",
    "know",
    "game",
    "go",
    "one",
    "person",
    "goes",
    "another",
    "person",
    "goes",
    "game",
    "tree",
    "every",
    "point",
    "set",
    "actions",
    "could",
    "taken",
    "quickly",
    "look",
    "game",
    "tree",
    "becomes",
    "know",
    "girl",
    "exponentially",
    "becomes",
    "huge",
    "game",
    "go",
    "hugest",
    "terms",
    "number",
    "choices",
    "largest",
    "chess",
    "know",
    "gets",
    "checkers",
    "degree",
    "every",
    "step",
    "increases",
    "decreased",
    "based",
    "game",
    "structure",
    "task",
    "neural",
    "network",
    "learn",
    "quality",
    "board",
    "learn",
    "boards",
    "game",
    "positions",
    "likely",
    "result",
    "useful",
    "explore",
    "result",
    "highly",
    "successful",
    "state",
    "choice",
    "good",
    "explore",
    "branch",
    "good",
    "go",
    "neural",
    "network",
    "step",
    "without",
    "phago",
    "pre",
    "trained",
    "first",
    "success",
    "beat",
    "world",
    "champion",
    "pre",
    "trained",
    "expert",
    "games",
    "alphago",
    "zero",
    "pre",
    "training",
    "expert",
    "systems",
    "imitation",
    "learning",
    "purely",
    "self",
    "play",
    "suggesting",
    "playing",
    "new",
    "board",
    "positions",
    "many",
    "systems",
    "use",
    "monte",
    "carlo",
    "tree",
    "search",
    "search",
    "balancing",
    "exploitation",
    "exploration",
    "going",
    "deep",
    "promising",
    "positions",
    "based",
    "estimation",
    "network",
    "flip",
    "coin",
    "playing",
    "play",
    "positions",
    "kind",
    "think",
    "intuition",
    "looking",
    "board",
    "estimating",
    "good",
    "board",
    "also",
    "estimating",
    "good",
    "board",
    "likely",
    "lead",
    "victory",
    "end",
    "mean",
    "general",
    "quality",
    "probability",
    "leading",
    "victory",
    "next",
    "step",
    "forward",
    "alpha",
    "zero",
    "using",
    "similar",
    "architecture",
    "mcts",
    "call",
    "research",
    "applying",
    "different",
    "games",
    "applying",
    "competing",
    "engines",
    "engines",
    "go",
    "shogi",
    "chess",
    "outperforming",
    "steps",
    "approaches",
    "really",
    "extremely",
    "simple",
    "efficient",
    "construct",
    "us",
    "model",
    "robotics",
    "learn",
    "model",
    "exceptionally",
    "powerful",
    "beating",
    "engines",
    "far",
    "superior",
    "humans",
    "already",
    "stockfish",
    "destroy",
    "humans",
    "earth",
    "game",
    "chess",
    "ability",
    "learning",
    "estimating",
    "quality",
    "board",
    "able",
    "defeat",
    "engines",
    "incredible",
    "exciting",
    "aspect",
    "versus",
    "engines",
    "use",
    "neural",
    "networks",
    "number",
    "really",
    "based",
    "neural",
    "network",
    "explore",
    "certain",
    "positions",
    "explore",
    "certain",
    "parts",
    "tree",
    "look",
    "grandmasters",
    "human",
    "players",
    "chess",
    "seem",
    "explore",
    "moves",
    "really",
    "good",
    "neural",
    "network",
    "estimating",
    "likely",
    "branches",
    "would",
    "provide",
    "value",
    "explore",
    "side",
    "stock",
    "fish",
    "much",
    "brute",
    "force",
    "estimation",
    "mcts",
    "alpha",
    "zero",
    "step",
    "towards",
    "grandmaster",
    "number",
    "branches",
    "need",
    "explored",
    "much",
    "much",
    "fewer",
    "lot",
    "work",
    "done",
    "representation",
    "form",
    "neural",
    "network",
    "super",
    "exciting",
    "able",
    "uh",
    "perform",
    "stockfish",
    "chess",
    "able",
    "outperform",
    "elmo",
    "shogi",
    "go",
    "previous",
    "iterations",
    "alphago",
    "zero",
    "challenge",
    "sobering",
    "truth",
    "majority",
    "real",
    "world",
    "application",
    "agents",
    "act",
    "world",
    "perceive",
    "world",
    "act",
    "world",
    "part",
    "based",
    "rl",
    "involved",
    "action",
    "learned",
    "use",
    "neural",
    "networks",
    "perceive",
    "certain",
    "aspects",
    "world",
    "ultimately",
    "action",
    "learned",
    "data",
    "true",
    "autonomous",
    "vehicle",
    "companies",
    "autonomous",
    "vehicle",
    "companies",
    "operating",
    "today",
    "true",
    "robotic",
    "manipulation",
    "industrial",
    "robotics",
    "humanoid",
    "robots",
    "navigate",
    "world",
    "uncertain",
    "conditions",
    "work",
    "boston",
    "dynamics",
    "involve",
    "machine",
    "learning",
    "far",
    "know",
    "beginning",
    "change",
    "animal",
    "recent",
    "development",
    "certain",
    "aspects",
    "control",
    "robotic",
    "could",
    "learned",
    "trying",
    "learn",
    "efficient",
    "movement",
    "trying",
    "learn",
    "robust",
    "movement",
    "top",
    "controllers",
    "quite",
    "exciting",
    "rl",
    "able",
    "learn",
    "control",
    "dynamics",
    "able",
    "teach",
    "particular",
    "robot",
    "able",
    "get",
    "arbitrary",
    "positions",
    "less",
    "hard",
    "coding",
    "order",
    "able",
    "deal",
    "unexpected",
    "nishal",
    "conditions",
    "unexpected",
    "perturbations",
    "exciting",
    "terms",
    "learning",
    "control",
    "dynamics",
    "driving",
    "policy",
    "maybe",
    "behavioral",
    "driving",
    "behavior",
    "decisions",
    "changing",
    "lanes",
    "turning",
    "last",
    "week",
    "heard",
    "way",
    "moe",
    "starting",
    "use",
    "rl",
    "terms",
    "driving",
    "policy",
    "order",
    "especially",
    "predict",
    "future",
    "trying",
    "anticipate",
    "intent",
    "modeling",
    "predict",
    "pedestrians",
    "cars",
    "going",
    "based",
    "environment",
    "trying",
    "unroll",
    "happened",
    "recently",
    "future",
    "beginning",
    "move",
    "beyond",
    "sort",
    "pure",
    "end",
    "end",
    "nvidia",
    "end",
    "learning",
    "approach",
    "control",
    "decisions",
    "actually",
    "moving",
    "rl",
    "making",
    "planning",
    "decisions",
    "challenge",
    "gap",
    "leap",
    "needed",
    "go",
    "simulation",
    "work",
    "done",
    "design",
    "environment",
    "design",
    "reward",
    "structure",
    "work",
    "simulation",
    "need",
    "either",
    "develop",
    "better",
    "algorithms",
    "transfer",
    "learning",
    "close",
    "distance",
    "simulation",
    "real",
    "world",
    "also",
    "could",
    "think",
    "outside",
    "box",
    "little",
    "bit",
    "conversation",
    "peter",
    "bill",
    "recently",
    "one",
    "leading",
    "researchers",
    "deep",
    "rl",
    "kind",
    "side",
    "quickly",
    "mentioned",
    "idea",
    "need",
    "make",
    "simulation",
    "realistic",
    "could",
    "create",
    "infinite",
    "number",
    "simulations",
    "large",
    "number",
    "simulations",
    "naturally",
    "regularization",
    "aspect",
    "simulations",
    "make",
    "reality",
    "another",
    "sample",
    "simulations",
    "maybe",
    "solution",
    "create",
    "higher",
    "fidelity",
    "simulation",
    "create",
    "transfer",
    "learning",
    "algorithms",
    "maybe",
    "build",
    "arbitrary",
    "number",
    "simulations",
    "step",
    "towards",
    "creating",
    "agent",
    "work",
    "works",
    "real",
    "world",
    "trivial",
    "one",
    "maybe",
    "exactly",
    "whoever",
    "created",
    "simulation",
    "living",
    "multiverse",
    "living",
    "next",
    "steps",
    "lecture",
    "videos",
    "several",
    "rl",
    "made",
    "available",
    "deep",
    "learning",
    "mit",
    "id",
    "several",
    "tutorials",
    "rl",
    "github",
    "link",
    "really",
    "like",
    "essay",
    "open",
    "ai",
    "spinning",
    "deep",
    "researcher",
    "know",
    "interested",
    "getting",
    "research",
    "rl",
    "steps",
    "need",
    "take",
    "background",
    "developing",
    "mathematical",
    "background",
    "prop",
    "stat",
    "multivariate",
    "calculus",
    "basics",
    "like",
    "covered",
    "last",
    "week",
    "deep",
    "learning",
    "basics",
    "ideas",
    "rl",
    "terminology",
    "basic",
    "concepts",
    "picking",
    "framework",
    "tends",
    "flow",
    "pi",
    "torch",
    "learn",
    "implemented",
    "guram",
    "mentioned",
    "today",
    "core",
    "rl",
    "algorithms",
    "implement",
    "isms",
    "scratch",
    "take",
    "two",
    "hundred",
    "three",
    "hundred",
    "lines",
    "code",
    "actually",
    "put",
    "paper",
    "quite",
    "simple",
    "intuitive",
    "algorithms",
    "read",
    "papers",
    "algorithms",
    "follow",
    "looking",
    "big",
    "waving",
    "performance",
    "hand",
    "waving",
    "performance",
    "tricks",
    "used",
    "change",
    "algorithms",
    "tricks",
    "tell",
    "lot",
    "story",
    "useful",
    "parts",
    "need",
    "learn",
    "iterate",
    "fast",
    "simple",
    "benchmark",
    "environments",
    "open",
    "jim",
    "provided",
    "lot",
    "easy",
    "use",
    "environments",
    "play",
    "train",
    "agent",
    "minutes",
    "hours",
    "opposed",
    "days",
    "weeks",
    "iterating",
    "fast",
    "best",
    "way",
    "learn",
    "algorithms",
    "research",
    "side",
    "three",
    "ways",
    "get",
    "best",
    "paper",
    "award",
    "right",
    "two",
    "publish",
    "contribute",
    "impact",
    "research",
    "community",
    "rl",
    "one",
    "improving",
    "existing",
    "approach",
    "given",
    "us",
    "particular",
    "benchmarks",
    "benchmark",
    "datasets",
    "environments",
    "emerging",
    "want",
    "improve",
    "existing",
    "approach",
    "aspect",
    "convergence",
    "performance",
    "focus",
    "unsolved",
    "task",
    "certain",
    "games",
    "solved",
    "rl",
    "formulation",
    "come",
    "totally",
    "new",
    "problem",
    "addressed",
    "rl",
    "like",
    "thank",
    "much",
    "tomorrow",
    "hope",
    "see",
    "deep",
    "traffic",
    "thanks"
  ],
  "keywords": [
    "today",
    "like",
    "exciting",
    "deep",
    "reinforcement",
    "learning",
    "provide",
    "think",
    "one",
    "artificial",
    "intelligence",
    "power",
    "ability",
    "neural",
    "networks",
    "world",
    "act",
    "representation",
    "taking",
    "really",
    "intelligent",
    "understand",
    "possible",
    "general",
    "samples",
    "data",
    "able",
    "supervised",
    "way",
    "encode",
    "reason",
    "take",
    "decisions",
    "made",
    "looking",
    "agent",
    "system",
    "make",
    "sequence",
    "around",
    "us",
    "operating",
    "learn",
    "especially",
    "know",
    "little",
    "beginning",
    "error",
    "process",
    "agents",
    "part",
    "using",
    "network",
    "based",
    "actions",
    "step",
    "look",
    "supervision",
    "every",
    "loss",
    "function",
    "good",
    "bad",
    "even",
    "humans",
    "kinds",
    "difference",
    "say",
    "human",
    "never",
    "bottom",
    "point",
    "input",
    "well",
    "fact",
    "get",
    "efficient",
    "output",
    "trying",
    "last",
    "let",
    "useful",
    "examples",
    "whether",
    "single",
    "future",
    "teaching",
    "experience",
    "sample",
    "talk",
    "algorithms",
    "design",
    "gets",
    "dynamics",
    "rewards",
    "algorithm",
    "solve",
    "task",
    "difficult",
    "reward",
    "versus",
    "walking",
    "room",
    "want",
    "success",
    "destination",
    "simple",
    "open",
    "questions",
    "could",
    "huge",
    "years",
    "see",
    "side",
    "extremely",
    "maybe",
    "quickly",
    "observation",
    "information",
    "need",
    "might",
    "mean",
    "idea",
    "kind",
    "key",
    "trivial",
    "next",
    "steps",
    "whatever",
    "simulation",
    "form",
    "much",
    "representations",
    "means",
    "top",
    "environment",
    "sense",
    "sensory",
    "systems",
    "cars",
    "robot",
    "raw",
    "opposed",
    "higher",
    "aspect",
    "new",
    "received",
    "thing",
    "three",
    "different",
    "still",
    "problem",
    "play",
    "transfer",
    "game",
    "atari",
    "go",
    "real",
    "believe",
    "towards",
    "enforcement",
    "going",
    "acts",
    "framework",
    "formulation",
    "action",
    "also",
    "actually",
    "two",
    "state",
    "model",
    "robotic",
    "driving",
    "deterministic",
    "stochastic",
    "chess",
    "continuous",
    "games",
    "cart",
    "challenge",
    "rl",
    "example",
    "highly",
    "environments",
    "ways",
    "gap",
    "research",
    "work",
    "improve",
    "train",
    "degree",
    "things",
    "directly",
    "okay",
    "called",
    "policy",
    "sees",
    "makes",
    "states",
    "value",
    "estimate",
    "particular",
    "best",
    "terms",
    "first",
    "lot",
    "certain",
    "aspects",
    "convergence",
    "either",
    "left",
    "right",
    "negative",
    "1",
    "avoid",
    "4",
    "nature",
    "choose",
    "80",
    "probability",
    "move",
    "10",
    "another",
    "try",
    "end",
    "optimal",
    "given",
    "always",
    "shortest",
    "path",
    "hole",
    "along",
    "mentioned",
    "change",
    "basic",
    "zero",
    "order",
    "structure",
    "free",
    "positive",
    "quite",
    "becomes",
    "big",
    "impact",
    "often",
    "control",
    "goal",
    "done",
    "parameters",
    "consequences",
    "finish",
    "points",
    "picking",
    "green",
    "turbos",
    "pick",
    "collecting",
    "robotics",
    "autonomous",
    "objective",
    "ai",
    "result",
    "safety",
    "incredible",
    "used",
    "moving",
    "pixels",
    "objects",
    "many",
    "fun",
    "use",
    "better",
    "space",
    "nice",
    "methods",
    "quality",
    "update",
    "taken",
    "flip",
    "coin",
    "explore",
    "optimization",
    "learned",
    "q",
    "approaches",
    "cue",
    "estimates",
    "equation",
    "updating",
    "table",
    "said",
    "exploration",
    "epsilon",
    "number",
    "size",
    "84",
    "approach",
    "gradient",
    "dq",
    "performance",
    "pretty",
    "n",
    "target",
    "choosing",
    "tricks",
    "replay",
    "memories",
    "estimating",
    "training",
    "advantage",
    "works",
    "actor",
    "critic",
    "noise",
    "create",
    "tree",
    "board",
    "positions",
    "engines",
    "simulations"
  ]
}