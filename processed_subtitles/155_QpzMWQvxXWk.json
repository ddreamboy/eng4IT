{
  "text": "in today's video we're going to walk\nthrough a natural language processing\nproject from start to finish by doing\nsentiment analysis on amazon reviews\nsentiment analysis it's the use of\nnatural language processing to identify\nthe motions behind text we're going to\nwalk through a traditional approach to\nsentiment analysis using python's\nnatural language toolkit or nltk\nand then we'll implement\na more complex model called roberta\nthat's provided by hugging face\nwe'll do some analysis of how the\ndifferent models perform and we'll even\nexplore using some pre-trained pipelines\nfor making sentiment analysis\nreally quick and easy hi my name is rob\nand i make videos about data science\nmachine learning and coding in python\ni'm going to share everything we do\ntoday in a kaggle notebook so you can\nfind the link in the description\ncopy that notebook and explore all the\nstuff that we would do today if you do\nenjoy this video please consider\nsubscribing liking and follow me on\ntwitch where i stream live coding all\nright let's get to the code okay so here\nwe are in a kaggle notebook\nyou can see this is the basic outline of\nwhat we're going to be talking about\ntoday we're going to be doing some\nsentiment analysis in python and we're\ngoing to use two main techniques the\nfirst one\nis the older kind of way of approaching\nsentiment analysis with a model called\nvader and this uses a bag of words\napproach and then we're gonna look at a\npre-trained model from hugging face\nthat's a roberta type model and this is\na more advanced transformer model that\nwe're going to see how the results\ncompare between those two models\nand then we're going to also explore a\nhugging face pipeline\nbut before we get into that let's talk\nabout the data and do some basic\nanalysis with the natural language\ntoolkit which is a great library for\npython i'm going to show you over here\nto the right side that we are going to\nuse this data set that is a bunch of\namazon fine food reviews so these are\ntext reviews for food on amazon as well\nas the rating that out of five stars\nthat the reviewer gave them\nand all of this is in csv format that\nwe're going to pull in\nso before we get too far into it\nlet's do some of our imports we are\ngoing to let's give it some space here\nwe're going to import pandas as pd we're\ngoing to import numpy\nit for some plotting we'll import\nmatplotlib\npi plot as plt\nand we're going to import\nseaborn as sns\nlet's\nset a style sheet that we'll use for our\nplots\nand then we're going to just to start\nout here import\nnltk which is that natural language\ntoolkit we'll be using for the start\nlet's go ahead and read in our data so\nlet's comment here and say read in\ndata and we're going to read in from the\ninput directory\nthere is this reviews.csv\nand after i read this in\ni can show you here\njust in head command on this\nthat in this data set we have each row\nis the unique id we have the product id\nuser id profile name and then the real\ninteresting stuff here is the score so\nthis is out of a one to five star rating\nhow many stars the reviewer gave this\nitem and then the text it's a little\nsmall to see here\nbut you can see\nif i just do the text row and i show us\nthe first one it's actual text with the\nreview that was written by the reviewer\nfor this product and we're going to be\nrunning our sentiment analysis on this\nrow of data in this entire data set\nbut actually this data set if i print\nthe shape\nis quite large there are almost half a\nmillion reviews here and just for time's\nsake let's down sample this data set so\ni can do that pretty simply by just\ndoing a head command on this and taking\nthe first 500 rows\nand then if we print the data frame\nshape after that we'll see that it's\n500 rows but you could scale up this\nproject very easily to all half a\nmillion\nproducts if you wanted to\nrun a more intense analysis so then i'll\njust put the data frame head command\nhere so we can see and reference back to\nwhat columns we have available to us\nnow let's do some\nquick\neda just to get an idea of what this\ndata set looks like so we'll take this\nscore column which we know to be a value\nbetween 1 and 5\nand we're going to do a value counts on\nthis\nthis gives us the number of times each\nscore occurs\nand then we'll sort the index\nand we'll do just a bar plot of this\nkind will be a bar plot and the title is\ngoing to be count of reviews\nby stars\nand let's also do a fig size of\n10 by five\nplot that\nand let's add a label to it so i'm going\nto do a\nsome line breaks here to clean this up\nand we're going to call this our axis\nand then i'm going to\nset the x label\nas\nreview stars\nand we will do plt show\nall right so we can see here that\nmost of the reviews are actually 5 stars\nbut then\nit kind of goes down it has a little\nuptick in the number of one star reviews\nwe have so\nthe\nvery biased towards positive reviews in\nour data set that's good to know before\nwe get any further\nnow um the next thing we'll do is just\nsome basic and nltk stuff\nand we'll start by just taking one\nexample\nreview so let's do example equals this\ntext column and just pick the 50th value\nas an example and we'll print this\nexample\nall right so what did this person say\nthey said this oatmeal is not good it's\nmushy soft i don't like it quaker oats\nis where you go okay so\nuh seems to be negative sentiment here\nbut before we get into that let's just\nsee some of the stuff that nltk can do\nout of the box um nltk can t tokenize\nthis sentence\nso\nlet's taste this example and run l ntk\nword tokenizer and all that basically\ndoes is splits this\ninto\nuh the parts of each word\nin the sentence now it looks pretty\nclear clean at the beginning but then\nyou can see that\num i don't\ndo an n apostrophe t is split so this is\na little bit smarter than just splitting\non spaces in the\nthe text and this will give us our\nactual tokenized\nresults\nin natural language processing\noften you need to convert the text into\nsome format that the computer can\ninterpret and tokenizing that is the way\nthat you do it so let's make this the\ntokens\nand then take the tokens and let's just\nshow the first 10 so we can remember\nwhat this looks\nlike all right so\nnow once we have these\ntokens another thing nltk can do out of\nthe box is actually find the part of\nspeech\nfor\neach of these words so let's run nltk's\npos tag for part of speech tagging and\nwe'll run this on each of these tokens\nnow we can see that we have each token\nand we also have its part of speech so\nthese part of speech values\nare\ncodes and we can actually load up an\nexample page that has\nsome examples of\nwhat each abbreviation means so\nlet's go back here to our example not\nhere\nso you can see that oatmeal is nnn\nand in our part of speech tagging that\nmeans it's a noun a singular noun\nso each of the values in this text has\nnow been given its part of speech so\nlet's call this tagged\nand then let's just show the first 10\nagain\nas our example\num now it can actually we can take it\nthe next step from this and take these\ntags part of speech and put them into\nentities so\nnltk\nwe can do a chunk on this\nand then n e chunk so\nthis\nuh takes the recommended name entity\nchunker to chunk the given list of\ntokens so it takes these tokens and\nactually will group them into chunks of\ntext so let's run it on this to see what\nit looks like\n[Music]\nwhat are we getting here\noh yeah so we need to\nstore this\nbecause we're running in a notebook and\nthen we'll actually run entities dot p\nprint\nfor pretty print of this\nso you can see that it's\nchunked this into a sentence\nand\nnoted here that this is an organization\nsome other\ninteresting stuff about the\ntext that can be extracted out\nautomatically using an nltk\nall right so that's just a basic primer\nabout nltk\nbut we want to get into sentiment\nanalysis so we're going to start by\nusing\nvader vader stands for\nwhat does vader stand for\nit stands for i wrote up here balance\naware dictionary and sentiment reasoner\nso this approach\nessentially just takes\nall the words in our sentence\nand it has a value of either positive\nnegative or neutral for each of those\nwords\nand it combines up it just does a math\nequation and\nfor all the words it'll add up to tell\nyou how positive negative or neutral\nthat the statement is\nbased on all those words now one thing\nto keep in mind is this approach does\nnot account for relationships between\nwords which in\nhuman speech is very important\nbut at least is a good start so\nwe also remove something called stop\nwords stop words are just words like\nand and the and words that really don't\nhave a positive or negative feeling\nuh to them they're just for the\nstructure of the sentence\nall right so let's do\nsome\nuh sentiment analysis using this\nvader approach we're gonna do from nltk\nsentiment\nsentiment\nimport sentiment intensity\nanalyzer\nand then we're going to also import from\ntqdm notebook\nimport tqdm this is just a progress bar\ntracker for when we're going to do some\nloops on this data i also made a video\nabout tqdm that you can watch if you're\ninterested\nand then we're going to make our\nsentiment\nanalyzer object by calling this\nsentiment\nintensity analyzer creating it and\ncalling it s i a and that's going to be\nwhat we're\nthe object\nuh let me make sure\noh yeah this needs to be from\ntqdm notebook input cdm and now we have\nour sentiment\nintensity\nanalyzer object we can run this on text\nand see\nuh what the sentiment is based on the\nwords so let's run just on some examples\nlet's say\ni am so happy an exclamation point\nwe can see that\nthis vader approach has made this has\ntagged this negative as zero\nthis these are scales from zero to one\nso\nzero negative\nneutral point three and positive point\nsix eight two so mostly\npositive now there's also this compound\nscore which is an aggregation of\nnegative neutral and positive this count\ncom\ncompound value is from negative one to\npositive one representing how negative\nto positive it is\nbut if you want more detail you can take\nthe breakdown of this negative neutral\nand positive\nso it did a good job it made this\nit tagged this as being mostly positive\nlet's try the opposite so s-i-a\npolarity scores of this is the worst\nthing ever\nall right now we see that the polarity\nscore\npolarity score for this is\nmostly negative and neutral\nand nothing positive\nand then this compound score is\nnet point\nnegative 0.62 so more on the negative\nside than positive\nvery interesting now we can run sia on\nour example\nlike that we had before remember our\nexample which was\nthis oatmeal comment let's run that on\nthe oatmeal comment\nand see what it is okay so\nit's pretty high neutral but also some\nnegative\nand the overall compound score is\nnegative no positive score\nso we want to run this\npolarity score\nrun the polarity score\non the entire data set so basically\nlooping through this data frame we have\nevery text field we wanna\nrun this and grab the polarity scores\nand we can do that with a simple\nloop so\nwe're gonna do\nfor\nd which is going to be just our row or\nwe can just say four row and t qdm\ndf dot itter\nrows\nand then our total is going to be yeah\nso then this should work i think and\nthen we're gonna take\num\nthe row\ntext\nand this will be\ntech our text\nand then we're also gonna take our we're\ngoing to call it my id which is the rows\nid\ncolumn and then let's just break here to\nmake sure we have this correctly\noh that's correct this is going to be\nfor i row in tqdm iter tuples uh it\narrows and then we'll also make the\ntotal of this the length of the data\nframe so that when we see our progress\nbar it's out of 500\nwe're going to want some way to store\nthese results so let's make a dictionary\ncalled\nres for results and every time we loop\nthrough\nwe'll take my id we'll store in the my\nid part of the dictionary\nthe polarity score score\npolarity score of the text\nright\nand then\nyeah that's it let's run this\nso really fast it ran it's done\nand um now we have this result\nuh dictionary with each id the negative\nneutral positive and compound score of\neach\nbut we want to store this\ninto a pandas data frame because that's\neasier to work with\nlet's\ndo that really quickly by just running\npd.dataframe on this dictionary pandas\ncan take in a dictionary pretty easily\nexcept for it's oriented the wrong way\nso let's just quickly run a dot t on\nthis which will flip everything\nhorizontally and now we have an index\nwhich is our id and then our negative\nneutral positive and our compound\nscore for the sentiment for each of\nthose\nvalues\nall right let's call this vader's\nthat's our vader's result\nand then let's also\nlet's take this vaders\nand let's reset the index\nand rename that index\nas our id so we can merge this onto our\noriginal data frame\nand then we're going to take vader's\nso\nvader's\nwill now be this and then we'll also\ntake vader's and we're going to merge it\non our original data frame and how we'll\ndo a left merge\nso now basically we have our data frame\nbut with our\nscores\nand we also have all the other values\nfrom our original data set including\nthe text so if i run a head on this we\ncan see\nnow we have\nsentiment\nscore\nand metadata\nall right so let's see\num\nlet's see\nif this in general\nis in line with what we would expect so\nwe're gonna make some assumptions here\nabout our data\nthat if the score of the item that the\nreviewer gave it is a five star review\nit's probably going to be more positive\nof text than if it was a score of one\none star review is going to have more\nnegative connotation than\na five star\nreview and we can do that by just doing\na simple bar plot so let's use seaborne\ni think i imported seaborne yeah i\nimported keyboard before and we're going\nto do a bar plot of this data\nwhere our data is vader's\nlet's call this\nplot\nvader results\nand our x value is going to be the score\nwhich remember is the\nstar review of the\nthe person and then compound\nis going to be our y value and that's\nthe\nnegative one to positive one\noverall\num\nsentiment of\nthe of the um\ntext\nthen let's set the title to be\ncompound\nscore\nby amazon\nstars\nreview\nand then we'll show this\nwhat did i do wrong here\ni spelled compound wrong\ncomp\nbound\nthere we go okay so\none star review has\nlower compound score and a five star\nview is higher and it's actually exactly\nwhat we would expect the more\nuh positive that the compound becomes\nthat's the more\num\nwell by each score that was given it's\nmore and more positive of\ntext\nrespectively and that's uh that's good\nthat just kind of validates what we're\nlooking for\nwe can even break this down instead of\nlooking at the compound score we can\nlook at the positive\nneutral and negative scores for each so\nwe're going to do that by doing\nsomething like sns bar plot\ndata is vader's again\nx is score again and then let's do\nthe positive\nand\nsee what this looks like all right so\nthis is the positive score and let's\nactually make three of these side by\nside uh left being positive neutral and\nthen the negative to the right and we'll\ndo that with\nmatplotlib subplots\nso this will make a 1 by 3 grid\nof our results\nand we will\ncall this axes\nput this first one here which will be\nour\npositive then we want our\nneutral\nand then we want the negative and this\nis going to be in position one two and\nthree and then let's also\nset the title so we remember what these\nare positive\nneutral\nand\nnegative\nand plots show this\n[Music]\noh this needs to be ax equals\nand i need to change each of these\nthere we go\nnow we have\num\nlet's see what we have here let's make\nthis a little\nless wide\nwe have the positive\npositivity is higher as the score is\nhigher in terms of stars the neutral is\nkind of flat and the negative goes down\nit becomes less negative of a comet as\nthe star review becomes higher great\nthis just confirms what we would hope to\nsee and shows that vader is valuable\nin having this connection between\nthe\nscore\nof the text and sentiment score and that\nit does relate to the actual um\nthe actual\nrating review of the reviewers uh let's\ndo a tight\nlayout\njust because i see some overlapping here\nof\nof the review of the\ny-axis labels but i think this is good\nall right\nso now we're going to take it up a notch\nour previous model just looked at each\nword in the sentence or in the review\nand scored each word individually but\nlike we mentioned before\nhuman language depends a lot about a lot\non context so if i say\nsomething uh we'll see a sentence that\ncould have negative words actually could\nbe sarcastic or related to other words\nin which way it makes it a positive\nstatement\nso\nthis uh vader model wouldn't pick up on\nthat sort of\nrelationship between words but more and\nmore\nrecently\nthese transformer based deep learning\nmodels have become\nvery popular because they can pick up on\nthat context so we're going to use from\nhugging face which is one of the leaders\nin these types of models and gathering\nthem and making them easily available\nwe're going to import from transformers\nnow this is hugging faces library you\ncould pip install transformers to get\nthis on your local machine\nor\num of course you can just\nrun it in a\nkaggle notebook like we are right now\nso let me make sure this works from\ntransformers we're gonna\nimport our auto tokenizer now this is\ngonna tokenize similar to what we showed\nnltk can do\nand then from transformers\nwe're gonna import\nauto\nmodel\nfor\nlet's auto complete here\nfor sequence\nclassification you can see that there\nare a lot of different types of models\nthat hugging face has and then we're\nalso going to import from sci pi\nspecial\num\nsoft max which we will apply to the\noutputs because they don't have soft max\napplied and this will smooth out between\nzero and one all right special spell\nthat\ncorrectly right and then we're going to\npull in a very specific model that has\nbeen pre-trained on a bunch of data\num\nfor\nsentiment exactly like we're trying to\ndo\nthis is provided by hugging face and\nwhen we\nrun the auto tokenizer in the auto model\nsequence classification methods and load\nit from a pre-trained model it'll pull\ndown\nthe model weights that have been stored\nand this is really great because we're\nessentially doing transfer learning this\nmodel was trained on a bunch of twitter\ncomments that were labeled and we don't\nhave to retrain the model at all we can\njust use these trained weights and apply\nit to our data set and see what comes\nout\nso\nanytime you do this the first time you\nwill see that it needs to download all\nof the weights this is expected\nand now that's finished now we have a\nmodel and a tokenizer that we can apply\nto the text so let's remember what our\nexample was before now this is this\noatmeal comment when our polarity score\nfrom the old type of model\nlook like this\nlet's call this the\nvader results\non example\nremember negative neutral\nand we want to run this though\non\nusing the roberta model that we've\npulled so we just need to take a few\nsteps so before we can run it on\nroberta\nmodel and that's uh first thing is\nencoding the text so we're going to take\nour tokenizer that we pulled in we're\ngoing to apply it to this example\nand return\nreturn\ntensors\nis going to be pt for pi torch and then\nencoded let's call this\nyou can see here this is the encoded\ntext so this is taking\nthat text and putting it into\nones and zeroes that embeddings that the\nmodel will understand we'll call this\nour ink coded text\nthen we're going to take that and we're\ngoing to run our model on it it's just\nthat simple so we're going to take this\nencoded text\nrun our model on it and this will be our\noutput\nand then you remember how we\nso this is what the output looks like\nit's a tensor\nwith our results\nand then we're gonna take that output\ntake it from being a tensor\nand\nmake it into numpy so that we can\nstore it\nlocally so let's detach this\nand then numpy\nand\nstore this\nas scores\nand then the last thing we're going to\ndo is just apply that soft max\nto the these scores that we imported\nsoft max layer\num\nlike this\nnow if we print our scores\nwe see that we have three different\nvalues in a numpy array now these are\nsimilar to the last type of\nmodel that we ran so basically this is\nthe negative the neutral and the\npositive score for this text so let's\nmake a scores dictionary where we will\nstore this\nand we'll put in\nroberta\nnegative is going to be the first value\nand then we'll just do\nlike this\nnegative\nneutral\nand positive and this will be 0 1 2\nand we'll print this\nscores\ndictionary\nneed a comma here\nthere we go all right so the roberta\nmodel\nmuch more than the vader model thinks\nthat this comment is negative which from\nreading it\nseems to make sense this is a very\nnegative\nreview of this product\nso\nalready here we can see\nsort of how much more powerful roberta\nis than just a vader model\nlet's go ahead and run this on the\nentire data set like we did before with\nthe vader model so we can do this pretty\neasily by just making a function out of\nthe code that we did before called\npolarity\nscores roberta\nwhere it takes an example like our code\ndid before and it runs all of this and\nit just returns\nscores\ndictionary\nso now we could run this on one example\nof text and get this\nscores dictionary like we had written\nthe code for\nabove and we're going to enter iterate\nover the data set just like we did\nbefore so let's take this\ncode from abort above where we iterated\nand we have our we're going to call this\nour\nvader result because we'll still run the\nvader text on this\nand then we'll also have\nour roberto results\nwhich is gonna be\nthe polarity scores roberta function\nthat we had written\non this text\nand we'll break here after the first one\njust to see how it ran on the first\niteration through so we see we have our\nveda results and we also have\nour roberta results it's exactly what we\nwanted and we also want to combine these\nso the way we can combine two\ndictionaries\nis\nthere's a way to do it with the newer\nversion of python but we're running an\nolder version so we'll just do it like\nthis\nand we'll call this both\nthat's both\nresults let's also go and rename this\nfrom negative neutral and positive to be\nexplicitly named that they're vader\nvader results and i'm just going to copy\nthis code which will basically\nrename these\nto vader and then the key name instead\nof\nright now it's just negative positive\nand then we will\ncombine these two okay so running that\nfor one\niteration it looks like our results look\ngood and we want to now just run it\nthrough all 500 examples so i'm going to\ntake out this break\nand it's gonna run through oh we also\nneed to\num\nactually store this into the dictionary\nthat we're gonna store with the id and\nwe're gonna store both\nnow here's i know this is gonna break\nuh because i ran this before but i want\nto just show as an example when it does\nbreak\nall right it did break\non one of these examples\nbecause the text had some issues with it\nand it wasn't able to run through\nthe roberta model so instead of\ndebugging this all right now and it has\nto do with the size of the text itself\nthere's certain size of text that's just\ntoo big for the model to handle\nwe will skip those and we will skip\nthose by adding a try accept clause here\nso it'll run through except for when\nthis runtime error occurs\nin that case we'll just print out a\nmessage so we we know\nthat it broke\nfor id\nthis\nid now we're going to rerun and let it\ngo through all 500.\nokay so that's done running and you can\nsee that did break for two examples um\nwe could we could have\nlowered the amount of size of those and\nand it would have worked but this gets\nus a good result for now\nnow it was pretty slow running keep in\nmind that's because i was running it\nonly on a cpu these roberta models and\ntransformer models are\noptimized to be run on a gpu and if i\nwas to go here and turn on the gpu and\nthe preferences\ni could have run a lot faster\nbut it works for this case just to run\nit on a cpu\nand now let's actually take the results\nof this\nand make the results dictionary similar\nto what we did before so\nthis\nline of code which takes these results\nruns of transforms on\nit let's call this\nresults data frame\nand it will merge back on the main data\nset\nnow if i do a head command on this\nwe can see now we have our vader\nscores\nall four of them and our roberta scores\nfor each row in the data set\nuh really quickly let's\ncome\ncompare\nscores\nacross\nor\nbetween models\nand we can do this using seaborne's pair\nplot i think this would be a nice way to\nlook at it so\nuh pair plot lets us see\ncomparison between each observation and\nwhat each feature looks like so i'll\nshow you here by\nrunning it on this results data frame\nand then we're just going to provide it\nthe variables we want it to\nlook at\nand those will be this vader negative\nneutral positive and\nthe\nlet's remove the vader compound because\ni don't think that's really needed to\ncompare\nand we're also provided the roberta\ncolumns and we're saying these are the\nones variables we want to compare\nuh let's also color it so the hue\ncolor of each dot is going to be by the\nscore which is that one to five star\nscores and let's also give it a\npalette something where we can\neasily see the difference between each\nvalues\num okay so this is vares i think\nand actually this combine and compare\nis what we're doing now\nokay so a lot going on here but one\nthing that we notice here is the five\nstar reviews are this purplish color\nand if we look at\nvader the positive reviews are more so\nto the right on for these five star\nreviews\nfor the\nroberta model you can see it's way over\nto the right\nand then we can see that there are some\ncorrelations between the roberta model\nand the vader model it's a little hard\nto see exactly if there are correlations\nbut one thing that becomes very clear is\nthat the vader model is a little bit\nless confident\nin all of its predictions compared to\nthe roberta model which really separates\nthe positivity\nand neutral and negative scores for each\nof these predicted values but if you\nlook here this um positive and neutral\nlike the\nroberta model has very high scores for\nthe five stars and most of these one\nstars are very low in positivity um\nsentiment scoring\nso that's pretty cool\nlet's also\nreview some examples this is going gonna\nbe pretty cool because now that we have\nuh sentiment score and uh\nwe know the five-star ranking of the\nreview we can look and see where the\nmodel maybe does the opposite of what we\nthink it should so\none way we can do that is we just take\nthis results data frame\nand we query where there's a one star\nreview\nso score equals one these are all our\none star\nreviews and then we'll sort the values\nby this roberta\npositivity score and for ascending\nlet's make this false so the highest\npositive\nscore positivity score with\nthe\nrank rating of the value being 1\nwill appear at the top\nand then we'll take the text of that and\njust do a values\nand print out the top value so what\nwe're looking at here is\na text that is said to be positive by\nthe model but is one score by what the\nactual reviewer gave it\nit says i felt energized within five\nminutes but it lasts about 45 minutes i\npaid 3.99 for this drink i should have\njust drunk a cup of coffee and save my\nmoney so this is very nuanced sentence\nand you can see that\nit starts off being sort of positive i\nfelt energized\nit lasted 45 minutes\nthe model is getting confused and\nthinking this is more of a positive\nstatement than\nwe can tell that this is saying negative\nby the end of the statement so that's\ninteresting and it makes sense let's do\nthe same thing with the\nwith the vader score so look at the most\npositive\nscore\nfor a once\nrating\nit says so we canceled the order it was\ncancelled without any problem\nthat is a positive note\nso they actually were used the word\npositive\nand\nwithout any problem\nseems positive\nbut it is a negative review\nand it's being a little sarcastic i\nguess a little tongue-in-cheek and the\nmodel does not pick on up on that\nespecially the vader type of model which\nis only looking at a bag of words\num for all of this sentence and and the\nscore of each word individually\nlet's also look at\nuh\nnegative\nsentiment\nfive star review\nand let's do this with the roberta model\nfirst\nso we'll switch to a five-star review\nand we'll look at the top negative\nsentence\nit says this was so delicious but too\nbad i ate them too fast and gained two\npounds my fault okay so it is sort of a\nnegative sentiment but a positive review\nthat's kind of funny that that one came\nup and then we'll do the exact same\nthing for the\nvader\nand it happens to be the exact same one\nso the both models got\ni guess you could say confused but maybe\nthis actually is a negative sentiment\nfor a positive review so maybe it did a\nbetter job than what we would expect to\nsee\nall right so we've explored a lot of\nthings with sentiment analysis so far\nthe one extra bonus piece that i want to\nshow you is just using the hugging face\ntransformers pipelines and this just\nmakes everything really\nsimple and i want to make sure i noted\nthis you can read about it on their\nwebsite\nbut you basically can just\nimport from transformers library\na pipeline\num and you need a spell pipeline\ncorrectly\nthere we go and then we can make a\npipeline\nwith\ncalled sentiment pipeline\nwith this pipeline\nand there are a handful of things that\nwe could feed it\num\nthat it it will automatically be set up\ntasks that it's automatically set up\nfor and we want to do\nsentiment analysis this will\nautomatically\ndownload their default model\nand embeddings\nfor\nuh this pipeline and you can just run\nsentiment analysis with two lines of\ncode super quick\nand easy uh you can also go in here and\nchange the model that it uses in the\ndifferent tokenizer but the nice part\nabout this is you don't have to set\nanything up you just do this it'll\ndownload the model\nit'll give us our default sentiment\npipeline and then we can just run text\non it\nokay so that's done running so i could\nsay i love sentiment analysis\nyou can see it's a different format and\nthe output it gives by default but it's\nsaying this is positive with a very high\nconfidence\num\ni'm going to type in make sure\nto like and subscribe\nhere\nand that also is positive\njust to do a bad example we'll say boo\nand that does show as negative so it's\nworking\nall right so\nthat's it\nfor our sentiment\nanalysis project tutorial\nso there we have it we walked through\ntwo different types of models that you\ncan use for sentiment analysis explored\nsome of the differences between them we\nactually ran it on a whole corpus of\ndata 500 different reviews from amazon\nyou could scale this up and run it on\nall half a million\nexamples and see what insights you can\nfind\nso thanks again for watching my videos\nmake sure you subscribe so that next\ntime i release a video you'll be\nnotified and i'll see you in the next\none bye\n",
  "words": [
    "today",
    "video",
    "going",
    "walk",
    "natural",
    "language",
    "processing",
    "project",
    "start",
    "finish",
    "sentiment",
    "analysis",
    "amazon",
    "reviews",
    "sentiment",
    "analysis",
    "use",
    "natural",
    "language",
    "processing",
    "identify",
    "motions",
    "behind",
    "text",
    "going",
    "walk",
    "traditional",
    "approach",
    "sentiment",
    "analysis",
    "using",
    "python",
    "natural",
    "language",
    "toolkit",
    "nltk",
    "implement",
    "complex",
    "model",
    "called",
    "roberta",
    "provided",
    "hugging",
    "face",
    "analysis",
    "different",
    "models",
    "perform",
    "even",
    "explore",
    "using",
    "pipelines",
    "making",
    "sentiment",
    "analysis",
    "really",
    "quick",
    "easy",
    "hi",
    "name",
    "rob",
    "make",
    "videos",
    "data",
    "science",
    "machine",
    "learning",
    "coding",
    "python",
    "going",
    "share",
    "everything",
    "today",
    "kaggle",
    "notebook",
    "find",
    "link",
    "description",
    "copy",
    "notebook",
    "explore",
    "stuff",
    "would",
    "today",
    "enjoy",
    "video",
    "please",
    "consider",
    "subscribing",
    "liking",
    "follow",
    "twitch",
    "stream",
    "live",
    "coding",
    "right",
    "let",
    "get",
    "code",
    "okay",
    "kaggle",
    "notebook",
    "see",
    "basic",
    "outline",
    "going",
    "talking",
    "today",
    "going",
    "sentiment",
    "analysis",
    "python",
    "going",
    "use",
    "two",
    "main",
    "techniques",
    "first",
    "one",
    "older",
    "kind",
    "way",
    "approaching",
    "sentiment",
    "analysis",
    "model",
    "called",
    "vader",
    "uses",
    "bag",
    "words",
    "approach",
    "gon",
    "na",
    "look",
    "model",
    "hugging",
    "face",
    "roberta",
    "type",
    "model",
    "advanced",
    "transformer",
    "model",
    "going",
    "see",
    "results",
    "compare",
    "two",
    "models",
    "going",
    "also",
    "explore",
    "hugging",
    "face",
    "pipeline",
    "get",
    "let",
    "talk",
    "data",
    "basic",
    "analysis",
    "natural",
    "language",
    "toolkit",
    "great",
    "library",
    "python",
    "going",
    "show",
    "right",
    "side",
    "going",
    "use",
    "data",
    "set",
    "bunch",
    "amazon",
    "fine",
    "food",
    "reviews",
    "text",
    "reviews",
    "food",
    "amazon",
    "well",
    "rating",
    "five",
    "stars",
    "reviewer",
    "gave",
    "csv",
    "format",
    "going",
    "pull",
    "get",
    "far",
    "let",
    "imports",
    "going",
    "let",
    "give",
    "space",
    "going",
    "import",
    "pandas",
    "pd",
    "going",
    "import",
    "numpy",
    "plotting",
    "import",
    "matplotlib",
    "pi",
    "plot",
    "plt",
    "going",
    "import",
    "seaborn",
    "sns",
    "let",
    "set",
    "style",
    "sheet",
    "use",
    "plots",
    "going",
    "start",
    "import",
    "nltk",
    "natural",
    "language",
    "toolkit",
    "using",
    "start",
    "let",
    "go",
    "ahead",
    "read",
    "data",
    "let",
    "comment",
    "say",
    "read",
    "data",
    "going",
    "read",
    "input",
    "directory",
    "read",
    "show",
    "head",
    "command",
    "data",
    "set",
    "row",
    "unique",
    "id",
    "product",
    "id",
    "user",
    "id",
    "profile",
    "name",
    "real",
    "interesting",
    "stuff",
    "score",
    "one",
    "five",
    "star",
    "rating",
    "many",
    "stars",
    "reviewer",
    "gave",
    "item",
    "text",
    "little",
    "small",
    "see",
    "see",
    "text",
    "row",
    "show",
    "us",
    "first",
    "one",
    "actual",
    "text",
    "review",
    "written",
    "reviewer",
    "product",
    "going",
    "running",
    "sentiment",
    "analysis",
    "row",
    "data",
    "entire",
    "data",
    "set",
    "actually",
    "data",
    "set",
    "print",
    "shape",
    "quite",
    "large",
    "almost",
    "half",
    "million",
    "reviews",
    "time",
    "sake",
    "let",
    "sample",
    "data",
    "set",
    "pretty",
    "simply",
    "head",
    "command",
    "taking",
    "first",
    "500",
    "rows",
    "print",
    "data",
    "frame",
    "shape",
    "see",
    "500",
    "rows",
    "could",
    "scale",
    "project",
    "easily",
    "half",
    "million",
    "products",
    "wanted",
    "run",
    "intense",
    "analysis",
    "put",
    "data",
    "frame",
    "head",
    "command",
    "see",
    "reference",
    "back",
    "columns",
    "available",
    "us",
    "let",
    "quick",
    "eda",
    "get",
    "idea",
    "data",
    "set",
    "looks",
    "like",
    "take",
    "score",
    "column",
    "know",
    "value",
    "1",
    "5",
    "going",
    "value",
    "counts",
    "gives",
    "us",
    "number",
    "times",
    "score",
    "occurs",
    "sort",
    "index",
    "bar",
    "plot",
    "kind",
    "bar",
    "plot",
    "title",
    "going",
    "count",
    "reviews",
    "stars",
    "let",
    "also",
    "fig",
    "size",
    "10",
    "five",
    "plot",
    "let",
    "add",
    "label",
    "going",
    "line",
    "breaks",
    "clean",
    "going",
    "call",
    "axis",
    "going",
    "set",
    "x",
    "label",
    "review",
    "stars",
    "plt",
    "show",
    "right",
    "see",
    "reviews",
    "actually",
    "5",
    "stars",
    "kind",
    "goes",
    "little",
    "uptick",
    "number",
    "one",
    "star",
    "reviews",
    "biased",
    "towards",
    "positive",
    "reviews",
    "data",
    "set",
    "good",
    "know",
    "get",
    "um",
    "next",
    "thing",
    "basic",
    "nltk",
    "stuff",
    "start",
    "taking",
    "one",
    "example",
    "review",
    "let",
    "example",
    "equals",
    "text",
    "column",
    "pick",
    "50th",
    "value",
    "example",
    "print",
    "example",
    "right",
    "person",
    "say",
    "said",
    "oatmeal",
    "good",
    "mushy",
    "soft",
    "like",
    "quaker",
    "oats",
    "go",
    "okay",
    "uh",
    "seems",
    "negative",
    "sentiment",
    "get",
    "let",
    "see",
    "stuff",
    "nltk",
    "box",
    "um",
    "nltk",
    "tokenize",
    "sentence",
    "let",
    "taste",
    "example",
    "run",
    "l",
    "ntk",
    "word",
    "tokenizer",
    "basically",
    "splits",
    "uh",
    "parts",
    "word",
    "sentence",
    "looks",
    "pretty",
    "clear",
    "clean",
    "beginning",
    "see",
    "um",
    "n",
    "apostrophe",
    "split",
    "little",
    "bit",
    "smarter",
    "splitting",
    "spaces",
    "text",
    "give",
    "us",
    "actual",
    "tokenized",
    "results",
    "natural",
    "language",
    "processing",
    "often",
    "need",
    "convert",
    "text",
    "format",
    "computer",
    "interpret",
    "tokenizing",
    "way",
    "let",
    "make",
    "tokens",
    "take",
    "tokens",
    "let",
    "show",
    "first",
    "10",
    "remember",
    "looks",
    "like",
    "right",
    "tokens",
    "another",
    "thing",
    "nltk",
    "box",
    "actually",
    "find",
    "part",
    "speech",
    "words",
    "let",
    "run",
    "nltk",
    "pos",
    "tag",
    "part",
    "speech",
    "tagging",
    "run",
    "tokens",
    "see",
    "token",
    "also",
    "part",
    "speech",
    "part",
    "speech",
    "values",
    "codes",
    "actually",
    "load",
    "example",
    "page",
    "examples",
    "abbreviation",
    "means",
    "let",
    "go",
    "back",
    "example",
    "see",
    "oatmeal",
    "nnn",
    "part",
    "speech",
    "tagging",
    "means",
    "noun",
    "singular",
    "noun",
    "values",
    "text",
    "given",
    "part",
    "speech",
    "let",
    "call",
    "tagged",
    "let",
    "show",
    "first",
    "10",
    "example",
    "um",
    "actually",
    "take",
    "next",
    "step",
    "take",
    "tags",
    "part",
    "speech",
    "put",
    "entities",
    "nltk",
    "chunk",
    "n",
    "e",
    "chunk",
    "uh",
    "takes",
    "recommended",
    "name",
    "entity",
    "chunker",
    "chunk",
    "given",
    "list",
    "tokens",
    "takes",
    "tokens",
    "actually",
    "group",
    "chunks",
    "text",
    "let",
    "run",
    "see",
    "looks",
    "like",
    "music",
    "getting",
    "oh",
    "yeah",
    "need",
    "store",
    "running",
    "notebook",
    "actually",
    "run",
    "entities",
    "dot",
    "p",
    "print",
    "pretty",
    "print",
    "see",
    "chunked",
    "sentence",
    "noted",
    "organization",
    "interesting",
    "stuff",
    "text",
    "extracted",
    "automatically",
    "using",
    "nltk",
    "right",
    "basic",
    "primer",
    "nltk",
    "want",
    "get",
    "sentiment",
    "analysis",
    "going",
    "start",
    "using",
    "vader",
    "vader",
    "stands",
    "vader",
    "stand",
    "stands",
    "wrote",
    "balance",
    "aware",
    "dictionary",
    "sentiment",
    "reasoner",
    "approach",
    "essentially",
    "takes",
    "words",
    "sentence",
    "value",
    "either",
    "positive",
    "negative",
    "neutral",
    "words",
    "combines",
    "math",
    "equation",
    "words",
    "add",
    "tell",
    "positive",
    "negative",
    "neutral",
    "statement",
    "based",
    "words",
    "one",
    "thing",
    "keep",
    "mind",
    "approach",
    "account",
    "relationships",
    "words",
    "human",
    "speech",
    "important",
    "least",
    "good",
    "start",
    "also",
    "remove",
    "something",
    "called",
    "stop",
    "words",
    "stop",
    "words",
    "words",
    "like",
    "words",
    "really",
    "positive",
    "negative",
    "feeling",
    "uh",
    "structure",
    "sentence",
    "right",
    "let",
    "uh",
    "sentiment",
    "analysis",
    "using",
    "vader",
    "approach",
    "gon",
    "na",
    "nltk",
    "sentiment",
    "sentiment",
    "import",
    "sentiment",
    "intensity",
    "analyzer",
    "going",
    "also",
    "import",
    "tqdm",
    "notebook",
    "import",
    "tqdm",
    "progress",
    "bar",
    "tracker",
    "going",
    "loops",
    "data",
    "also",
    "made",
    "video",
    "tqdm",
    "watch",
    "interested",
    "going",
    "make",
    "sentiment",
    "analyzer",
    "object",
    "calling",
    "sentiment",
    "intensity",
    "analyzer",
    "creating",
    "calling",
    "going",
    "object",
    "uh",
    "let",
    "make",
    "sure",
    "oh",
    "yeah",
    "needs",
    "tqdm",
    "notebook",
    "input",
    "cdm",
    "sentiment",
    "intensity",
    "analyzer",
    "object",
    "run",
    "text",
    "see",
    "uh",
    "sentiment",
    "based",
    "words",
    "let",
    "run",
    "examples",
    "let",
    "say",
    "happy",
    "exclamation",
    "point",
    "see",
    "vader",
    "approach",
    "made",
    "tagged",
    "negative",
    "zero",
    "scales",
    "zero",
    "one",
    "zero",
    "negative",
    "neutral",
    "point",
    "three",
    "positive",
    "point",
    "six",
    "eight",
    "two",
    "mostly",
    "positive",
    "also",
    "compound",
    "score",
    "aggregation",
    "negative",
    "neutral",
    "positive",
    "count",
    "com",
    "compound",
    "value",
    "negative",
    "one",
    "positive",
    "one",
    "representing",
    "negative",
    "positive",
    "want",
    "detail",
    "take",
    "breakdown",
    "negative",
    "neutral",
    "positive",
    "good",
    "job",
    "made",
    "tagged",
    "mostly",
    "positive",
    "let",
    "try",
    "opposite",
    "polarity",
    "scores",
    "worst",
    "thing",
    "ever",
    "right",
    "see",
    "polarity",
    "score",
    "polarity",
    "score",
    "mostly",
    "negative",
    "neutral",
    "nothing",
    "positive",
    "compound",
    "score",
    "net",
    "point",
    "negative",
    "negative",
    "side",
    "positive",
    "interesting",
    "run",
    "sia",
    "example",
    "like",
    "remember",
    "example",
    "oatmeal",
    "comment",
    "let",
    "run",
    "oatmeal",
    "comment",
    "see",
    "okay",
    "pretty",
    "high",
    "neutral",
    "also",
    "negative",
    "overall",
    "compound",
    "score",
    "negative",
    "positive",
    "score",
    "want",
    "run",
    "polarity",
    "score",
    "run",
    "polarity",
    "score",
    "entire",
    "data",
    "set",
    "basically",
    "looping",
    "data",
    "frame",
    "every",
    "text",
    "field",
    "wan",
    "na",
    "run",
    "grab",
    "polarity",
    "scores",
    "simple",
    "loop",
    "gon",
    "na",
    "going",
    "row",
    "say",
    "four",
    "row",
    "qdm",
    "df",
    "dot",
    "itter",
    "rows",
    "total",
    "going",
    "yeah",
    "work",
    "think",
    "gon",
    "na",
    "take",
    "um",
    "row",
    "text",
    "tech",
    "text",
    "also",
    "gon",
    "na",
    "take",
    "going",
    "call",
    "id",
    "rows",
    "id",
    "column",
    "let",
    "break",
    "make",
    "sure",
    "correctly",
    "oh",
    "correct",
    "going",
    "row",
    "tqdm",
    "iter",
    "tuples",
    "uh",
    "arrows",
    "also",
    "make",
    "total",
    "length",
    "data",
    "frame",
    "see",
    "progress",
    "bar",
    "500",
    "going",
    "want",
    "way",
    "store",
    "results",
    "let",
    "make",
    "dictionary",
    "called",
    "res",
    "results",
    "every",
    "time",
    "loop",
    "take",
    "id",
    "store",
    "id",
    "part",
    "dictionary",
    "polarity",
    "score",
    "score",
    "polarity",
    "score",
    "text",
    "right",
    "yeah",
    "let",
    "run",
    "really",
    "fast",
    "ran",
    "done",
    "um",
    "result",
    "uh",
    "dictionary",
    "id",
    "negative",
    "neutral",
    "positive",
    "compound",
    "score",
    "want",
    "store",
    "pandas",
    "data",
    "frame",
    "easier",
    "work",
    "let",
    "really",
    "quickly",
    "running",
    "dictionary",
    "pandas",
    "take",
    "dictionary",
    "pretty",
    "easily",
    "except",
    "oriented",
    "wrong",
    "way",
    "let",
    "quickly",
    "run",
    "dot",
    "flip",
    "everything",
    "horizontally",
    "index",
    "id",
    "negative",
    "neutral",
    "positive",
    "compound",
    "score",
    "sentiment",
    "values",
    "right",
    "let",
    "call",
    "vader",
    "vader",
    "result",
    "let",
    "also",
    "let",
    "take",
    "vaders",
    "let",
    "reset",
    "index",
    "rename",
    "index",
    "id",
    "merge",
    "onto",
    "original",
    "data",
    "frame",
    "going",
    "take",
    "vader",
    "vader",
    "also",
    "take",
    "vader",
    "going",
    "merge",
    "original",
    "data",
    "frame",
    "left",
    "merge",
    "basically",
    "data",
    "frame",
    "scores",
    "also",
    "values",
    "original",
    "data",
    "set",
    "including",
    "text",
    "run",
    "head",
    "see",
    "sentiment",
    "score",
    "metadata",
    "right",
    "let",
    "see",
    "um",
    "let",
    "see",
    "general",
    "line",
    "would",
    "expect",
    "gon",
    "na",
    "make",
    "assumptions",
    "data",
    "score",
    "item",
    "reviewer",
    "gave",
    "five",
    "star",
    "review",
    "probably",
    "going",
    "positive",
    "text",
    "score",
    "one",
    "one",
    "star",
    "review",
    "going",
    "negative",
    "connotation",
    "five",
    "star",
    "review",
    "simple",
    "bar",
    "plot",
    "let",
    "use",
    "seaborne",
    "think",
    "imported",
    "seaborne",
    "yeah",
    "imported",
    "keyboard",
    "going",
    "bar",
    "plot",
    "data",
    "data",
    "vader",
    "let",
    "call",
    "plot",
    "vader",
    "results",
    "x",
    "value",
    "going",
    "score",
    "remember",
    "star",
    "review",
    "person",
    "compound",
    "going",
    "value",
    "negative",
    "one",
    "positive",
    "one",
    "overall",
    "um",
    "sentiment",
    "um",
    "text",
    "let",
    "set",
    "title",
    "compound",
    "score",
    "amazon",
    "stars",
    "review",
    "show",
    "wrong",
    "spelled",
    "compound",
    "wrong",
    "comp",
    "bound",
    "go",
    "okay",
    "one",
    "star",
    "review",
    "lower",
    "compound",
    "score",
    "five",
    "star",
    "view",
    "higher",
    "actually",
    "exactly",
    "would",
    "expect",
    "uh",
    "positive",
    "compound",
    "becomes",
    "um",
    "well",
    "score",
    "given",
    "positive",
    "text",
    "respectively",
    "uh",
    "good",
    "kind",
    "validates",
    "looking",
    "even",
    "break",
    "instead",
    "looking",
    "compound",
    "score",
    "look",
    "positive",
    "neutral",
    "negative",
    "scores",
    "going",
    "something",
    "like",
    "sns",
    "bar",
    "plot",
    "data",
    "vader",
    "x",
    "score",
    "let",
    "positive",
    "see",
    "looks",
    "like",
    "right",
    "positive",
    "score",
    "let",
    "actually",
    "make",
    "three",
    "side",
    "side",
    "uh",
    "left",
    "positive",
    "neutral",
    "negative",
    "right",
    "matplotlib",
    "subplots",
    "make",
    "1",
    "3",
    "grid",
    "results",
    "call",
    "axes",
    "put",
    "first",
    "one",
    "positive",
    "want",
    "neutral",
    "want",
    "negative",
    "going",
    "position",
    "one",
    "two",
    "three",
    "let",
    "also",
    "set",
    "title",
    "remember",
    "positive",
    "neutral",
    "negative",
    "plots",
    "show",
    "music",
    "oh",
    "needs",
    "ax",
    "equals",
    "need",
    "change",
    "go",
    "um",
    "let",
    "see",
    "let",
    "make",
    "little",
    "less",
    "wide",
    "positive",
    "positivity",
    "higher",
    "score",
    "higher",
    "terms",
    "stars",
    "neutral",
    "kind",
    "flat",
    "negative",
    "goes",
    "becomes",
    "less",
    "negative",
    "comet",
    "star",
    "review",
    "becomes",
    "higher",
    "great",
    "confirms",
    "would",
    "hope",
    "see",
    "shows",
    "vader",
    "valuable",
    "connection",
    "score",
    "text",
    "sentiment",
    "score",
    "relate",
    "actual",
    "um",
    "actual",
    "rating",
    "review",
    "reviewers",
    "uh",
    "let",
    "tight",
    "layout",
    "see",
    "overlapping",
    "review",
    "labels",
    "think",
    "good",
    "right",
    "going",
    "take",
    "notch",
    "previous",
    "model",
    "looked",
    "word",
    "sentence",
    "review",
    "scored",
    "word",
    "individually",
    "like",
    "mentioned",
    "human",
    "language",
    "depends",
    "lot",
    "lot",
    "context",
    "say",
    "something",
    "uh",
    "see",
    "sentence",
    "could",
    "negative",
    "words",
    "actually",
    "could",
    "sarcastic",
    "related",
    "words",
    "way",
    "makes",
    "positive",
    "statement",
    "uh",
    "vader",
    "model",
    "would",
    "pick",
    "sort",
    "relationship",
    "words",
    "recently",
    "transformer",
    "based",
    "deep",
    "learning",
    "models",
    "become",
    "popular",
    "pick",
    "context",
    "going",
    "use",
    "hugging",
    "face",
    "one",
    "leaders",
    "types",
    "models",
    "gathering",
    "making",
    "easily",
    "available",
    "going",
    "import",
    "transformers",
    "hugging",
    "faces",
    "library",
    "could",
    "pip",
    "install",
    "transformers",
    "get",
    "local",
    "machine",
    "um",
    "course",
    "run",
    "kaggle",
    "notebook",
    "like",
    "right",
    "let",
    "make",
    "sure",
    "works",
    "transformers",
    "gon",
    "na",
    "import",
    "auto",
    "tokenizer",
    "gon",
    "na",
    "tokenize",
    "similar",
    "showed",
    "nltk",
    "transformers",
    "gon",
    "na",
    "import",
    "auto",
    "model",
    "let",
    "auto",
    "complete",
    "sequence",
    "classification",
    "see",
    "lot",
    "different",
    "types",
    "models",
    "hugging",
    "face",
    "also",
    "going",
    "import",
    "sci",
    "pi",
    "special",
    "um",
    "soft",
    "max",
    "apply",
    "outputs",
    "soft",
    "max",
    "applied",
    "smooth",
    "zero",
    "one",
    "right",
    "special",
    "spell",
    "correctly",
    "right",
    "going",
    "pull",
    "specific",
    "model",
    "bunch",
    "data",
    "um",
    "sentiment",
    "exactly",
    "like",
    "trying",
    "provided",
    "hugging",
    "face",
    "run",
    "auto",
    "tokenizer",
    "auto",
    "model",
    "sequence",
    "classification",
    "methods",
    "load",
    "model",
    "pull",
    "model",
    "weights",
    "stored",
    "really",
    "great",
    "essentially",
    "transfer",
    "learning",
    "model",
    "trained",
    "bunch",
    "twitter",
    "comments",
    "labeled",
    "retrain",
    "model",
    "use",
    "trained",
    "weights",
    "apply",
    "data",
    "set",
    "see",
    "comes",
    "anytime",
    "first",
    "time",
    "see",
    "needs",
    "download",
    "weights",
    "expected",
    "finished",
    "model",
    "tokenizer",
    "apply",
    "text",
    "let",
    "remember",
    "example",
    "oatmeal",
    "comment",
    "polarity",
    "score",
    "old",
    "type",
    "model",
    "look",
    "like",
    "let",
    "call",
    "vader",
    "results",
    "example",
    "remember",
    "negative",
    "neutral",
    "want",
    "run",
    "though",
    "using",
    "roberta",
    "model",
    "pulled",
    "need",
    "take",
    "steps",
    "run",
    "roberta",
    "model",
    "uh",
    "first",
    "thing",
    "encoding",
    "text",
    "going",
    "take",
    "tokenizer",
    "pulled",
    "going",
    "apply",
    "example",
    "return",
    "return",
    "tensors",
    "going",
    "pt",
    "pi",
    "torch",
    "encoded",
    "let",
    "call",
    "see",
    "encoded",
    "text",
    "taking",
    "text",
    "putting",
    "ones",
    "zeroes",
    "embeddings",
    "model",
    "understand",
    "call",
    "ink",
    "coded",
    "text",
    "going",
    "take",
    "going",
    "run",
    "model",
    "simple",
    "going",
    "take",
    "encoded",
    "text",
    "run",
    "model",
    "output",
    "remember",
    "output",
    "looks",
    "like",
    "tensor",
    "results",
    "gon",
    "na",
    "take",
    "output",
    "take",
    "tensor",
    "make",
    "numpy",
    "store",
    "locally",
    "let",
    "detach",
    "numpy",
    "store",
    "scores",
    "last",
    "thing",
    "going",
    "apply",
    "soft",
    "max",
    "scores",
    "imported",
    "soft",
    "max",
    "layer",
    "um",
    "like",
    "print",
    "scores",
    "see",
    "three",
    "different",
    "values",
    "numpy",
    "array",
    "similar",
    "last",
    "type",
    "model",
    "ran",
    "basically",
    "negative",
    "neutral",
    "positive",
    "score",
    "text",
    "let",
    "make",
    "scores",
    "dictionary",
    "store",
    "put",
    "roberta",
    "negative",
    "going",
    "first",
    "value",
    "like",
    "negative",
    "neutral",
    "positive",
    "0",
    "1",
    "2",
    "print",
    "scores",
    "dictionary",
    "need",
    "comma",
    "go",
    "right",
    "roberta",
    "model",
    "much",
    "vader",
    "model",
    "thinks",
    "comment",
    "negative",
    "reading",
    "seems",
    "make",
    "sense",
    "negative",
    "review",
    "product",
    "already",
    "see",
    "sort",
    "much",
    "powerful",
    "roberta",
    "vader",
    "model",
    "let",
    "go",
    "ahead",
    "run",
    "entire",
    "data",
    "set",
    "like",
    "vader",
    "model",
    "pretty",
    "easily",
    "making",
    "function",
    "code",
    "called",
    "polarity",
    "scores",
    "roberta",
    "takes",
    "example",
    "like",
    "code",
    "runs",
    "returns",
    "scores",
    "dictionary",
    "could",
    "run",
    "one",
    "example",
    "text",
    "get",
    "scores",
    "dictionary",
    "like",
    "written",
    "code",
    "going",
    "enter",
    "iterate",
    "data",
    "set",
    "like",
    "let",
    "take",
    "code",
    "abort",
    "iterated",
    "going",
    "call",
    "vader",
    "result",
    "still",
    "run",
    "vader",
    "text",
    "also",
    "roberto",
    "results",
    "gon",
    "na",
    "polarity",
    "scores",
    "roberta",
    "function",
    "written",
    "text",
    "break",
    "first",
    "one",
    "see",
    "ran",
    "first",
    "iteration",
    "see",
    "veda",
    "results",
    "also",
    "roberta",
    "results",
    "exactly",
    "wanted",
    "also",
    "want",
    "combine",
    "way",
    "combine",
    "two",
    "dictionaries",
    "way",
    "newer",
    "version",
    "python",
    "running",
    "older",
    "version",
    "like",
    "call",
    "results",
    "let",
    "also",
    "go",
    "rename",
    "negative",
    "neutral",
    "positive",
    "explicitly",
    "named",
    "vader",
    "vader",
    "results",
    "going",
    "copy",
    "code",
    "basically",
    "rename",
    "vader",
    "key",
    "name",
    "instead",
    "right",
    "negative",
    "positive",
    "combine",
    "two",
    "okay",
    "running",
    "one",
    "iteration",
    "looks",
    "like",
    "results",
    "look",
    "good",
    "want",
    "run",
    "500",
    "examples",
    "going",
    "take",
    "break",
    "gon",
    "na",
    "run",
    "oh",
    "also",
    "need",
    "um",
    "actually",
    "store",
    "dictionary",
    "gon",
    "na",
    "store",
    "id",
    "gon",
    "na",
    "store",
    "know",
    "gon",
    "na",
    "break",
    "uh",
    "ran",
    "want",
    "show",
    "example",
    "break",
    "right",
    "break",
    "one",
    "examples",
    "text",
    "issues",
    "able",
    "run",
    "roberta",
    "model",
    "instead",
    "debugging",
    "right",
    "size",
    "text",
    "certain",
    "size",
    "text",
    "big",
    "model",
    "handle",
    "skip",
    "skip",
    "adding",
    "try",
    "accept",
    "clause",
    "run",
    "except",
    "runtime",
    "error",
    "occurs",
    "case",
    "print",
    "message",
    "know",
    "broke",
    "id",
    "id",
    "going",
    "rerun",
    "let",
    "go",
    "okay",
    "done",
    "running",
    "see",
    "break",
    "two",
    "examples",
    "um",
    "could",
    "could",
    "lowered",
    "amount",
    "size",
    "would",
    "worked",
    "gets",
    "us",
    "good",
    "result",
    "pretty",
    "slow",
    "running",
    "keep",
    "mind",
    "running",
    "cpu",
    "roberta",
    "models",
    "transformer",
    "models",
    "optimized",
    "run",
    "gpu",
    "go",
    "turn",
    "gpu",
    "preferences",
    "could",
    "run",
    "lot",
    "faster",
    "works",
    "case",
    "run",
    "cpu",
    "let",
    "actually",
    "take",
    "results",
    "make",
    "results",
    "dictionary",
    "similar",
    "line",
    "code",
    "takes",
    "results",
    "runs",
    "transforms",
    "let",
    "call",
    "results",
    "data",
    "frame",
    "merge",
    "back",
    "main",
    "data",
    "set",
    "head",
    "command",
    "see",
    "vader",
    "scores",
    "four",
    "roberta",
    "scores",
    "row",
    "data",
    "set",
    "uh",
    "really",
    "quickly",
    "let",
    "come",
    "compare",
    "scores",
    "across",
    "models",
    "using",
    "seaborne",
    "pair",
    "plot",
    "think",
    "would",
    "nice",
    "way",
    "look",
    "uh",
    "pair",
    "plot",
    "lets",
    "us",
    "see",
    "comparison",
    "observation",
    "feature",
    "looks",
    "like",
    "show",
    "running",
    "results",
    "data",
    "frame",
    "going",
    "provide",
    "variables",
    "want",
    "look",
    "vader",
    "negative",
    "neutral",
    "positive",
    "let",
    "remove",
    "vader",
    "compound",
    "think",
    "really",
    "needed",
    "compare",
    "also",
    "provided",
    "roberta",
    "columns",
    "saying",
    "ones",
    "variables",
    "want",
    "compare",
    "uh",
    "let",
    "also",
    "color",
    "hue",
    "color",
    "dot",
    "going",
    "score",
    "one",
    "five",
    "star",
    "scores",
    "let",
    "also",
    "give",
    "palette",
    "something",
    "easily",
    "see",
    "difference",
    "values",
    "um",
    "okay",
    "vares",
    "think",
    "actually",
    "combine",
    "compare",
    "okay",
    "lot",
    "going",
    "one",
    "thing",
    "notice",
    "five",
    "star",
    "reviews",
    "purplish",
    "color",
    "look",
    "vader",
    "positive",
    "reviews",
    "right",
    "five",
    "star",
    "reviews",
    "roberta",
    "model",
    "see",
    "way",
    "right",
    "see",
    "correlations",
    "roberta",
    "model",
    "vader",
    "model",
    "little",
    "hard",
    "see",
    "exactly",
    "correlations",
    "one",
    "thing",
    "becomes",
    "clear",
    "vader",
    "model",
    "little",
    "bit",
    "less",
    "confident",
    "predictions",
    "compared",
    "roberta",
    "model",
    "really",
    "separates",
    "positivity",
    "neutral",
    "negative",
    "scores",
    "predicted",
    "values",
    "look",
    "um",
    "positive",
    "neutral",
    "like",
    "roberta",
    "model",
    "high",
    "scores",
    "five",
    "stars",
    "one",
    "stars",
    "low",
    "positivity",
    "um",
    "sentiment",
    "scoring",
    "pretty",
    "cool",
    "let",
    "also",
    "review",
    "examples",
    "going",
    "gon",
    "na",
    "pretty",
    "cool",
    "uh",
    "sentiment",
    "score",
    "uh",
    "know",
    "ranking",
    "review",
    "look",
    "see",
    "model",
    "maybe",
    "opposite",
    "think",
    "one",
    "way",
    "take",
    "results",
    "data",
    "frame",
    "query",
    "one",
    "star",
    "review",
    "score",
    "equals",
    "one",
    "one",
    "star",
    "reviews",
    "sort",
    "values",
    "roberta",
    "positivity",
    "score",
    "ascending",
    "let",
    "make",
    "false",
    "highest",
    "positive",
    "score",
    "positivity",
    "score",
    "rank",
    "rating",
    "value",
    "1",
    "appear",
    "top",
    "take",
    "text",
    "values",
    "print",
    "top",
    "value",
    "looking",
    "text",
    "said",
    "positive",
    "model",
    "one",
    "score",
    "actual",
    "reviewer",
    "gave",
    "says",
    "felt",
    "energized",
    "within",
    "five",
    "minutes",
    "lasts",
    "45",
    "minutes",
    "paid",
    "drink",
    "drunk",
    "cup",
    "coffee",
    "save",
    "money",
    "nuanced",
    "sentence",
    "see",
    "starts",
    "sort",
    "positive",
    "felt",
    "energized",
    "lasted",
    "45",
    "minutes",
    "model",
    "getting",
    "confused",
    "thinking",
    "positive",
    "statement",
    "tell",
    "saying",
    "negative",
    "end",
    "statement",
    "interesting",
    "makes",
    "sense",
    "let",
    "thing",
    "vader",
    "score",
    "look",
    "positive",
    "score",
    "rating",
    "says",
    "canceled",
    "order",
    "cancelled",
    "without",
    "problem",
    "positive",
    "note",
    "actually",
    "used",
    "word",
    "positive",
    "without",
    "problem",
    "seems",
    "positive",
    "negative",
    "review",
    "little",
    "sarcastic",
    "guess",
    "little",
    "model",
    "pick",
    "especially",
    "vader",
    "type",
    "model",
    "looking",
    "bag",
    "words",
    "um",
    "sentence",
    "score",
    "word",
    "individually",
    "let",
    "also",
    "look",
    "uh",
    "negative",
    "sentiment",
    "five",
    "star",
    "review",
    "let",
    "roberta",
    "model",
    "first",
    "switch",
    "review",
    "look",
    "top",
    "negative",
    "sentence",
    "says",
    "delicious",
    "bad",
    "ate",
    "fast",
    "gained",
    "two",
    "pounds",
    "fault",
    "okay",
    "sort",
    "negative",
    "sentiment",
    "positive",
    "review",
    "kind",
    "funny",
    "one",
    "came",
    "exact",
    "thing",
    "vader",
    "happens",
    "exact",
    "one",
    "models",
    "got",
    "guess",
    "could",
    "say",
    "confused",
    "maybe",
    "actually",
    "negative",
    "sentiment",
    "positive",
    "review",
    "maybe",
    "better",
    "job",
    "would",
    "expect",
    "see",
    "right",
    "explored",
    "lot",
    "things",
    "sentiment",
    "analysis",
    "far",
    "one",
    "extra",
    "bonus",
    "piece",
    "want",
    "show",
    "using",
    "hugging",
    "face",
    "transformers",
    "pipelines",
    "makes",
    "everything",
    "really",
    "simple",
    "want",
    "make",
    "sure",
    "noted",
    "read",
    "website",
    "basically",
    "import",
    "transformers",
    "library",
    "pipeline",
    "um",
    "need",
    "spell",
    "pipeline",
    "correctly",
    "go",
    "make",
    "pipeline",
    "called",
    "sentiment",
    "pipeline",
    "pipeline",
    "handful",
    "things",
    "could",
    "feed",
    "um",
    "automatically",
    "set",
    "tasks",
    "automatically",
    "set",
    "want",
    "sentiment",
    "analysis",
    "automatically",
    "download",
    "default",
    "model",
    "embeddings",
    "uh",
    "pipeline",
    "run",
    "sentiment",
    "analysis",
    "two",
    "lines",
    "code",
    "super",
    "quick",
    "easy",
    "uh",
    "also",
    "go",
    "change",
    "model",
    "uses",
    "different",
    "tokenizer",
    "nice",
    "part",
    "set",
    "anything",
    "download",
    "model",
    "give",
    "us",
    "default",
    "sentiment",
    "pipeline",
    "run",
    "text",
    "okay",
    "done",
    "running",
    "could",
    "say",
    "love",
    "sentiment",
    "analysis",
    "see",
    "different",
    "format",
    "output",
    "gives",
    "default",
    "saying",
    "positive",
    "high",
    "confidence",
    "um",
    "going",
    "type",
    "make",
    "sure",
    "like",
    "subscribe",
    "also",
    "positive",
    "bad",
    "example",
    "say",
    "boo",
    "show",
    "negative",
    "working",
    "right",
    "sentiment",
    "analysis",
    "project",
    "tutorial",
    "walked",
    "two",
    "different",
    "types",
    "models",
    "use",
    "sentiment",
    "analysis",
    "explored",
    "differences",
    "actually",
    "ran",
    "whole",
    "corpus",
    "data",
    "500",
    "different",
    "reviews",
    "amazon",
    "could",
    "scale",
    "run",
    "half",
    "million",
    "examples",
    "see",
    "insights",
    "find",
    "thanks",
    "watching",
    "videos",
    "make",
    "sure",
    "subscribe",
    "next",
    "time",
    "release",
    "video",
    "notified",
    "see",
    "next",
    "one",
    "bye"
  ],
  "keywords": [
    "today",
    "video",
    "going",
    "natural",
    "language",
    "start",
    "sentiment",
    "analysis",
    "amazon",
    "reviews",
    "use",
    "text",
    "approach",
    "using",
    "python",
    "nltk",
    "model",
    "called",
    "roberta",
    "hugging",
    "face",
    "different",
    "models",
    "really",
    "name",
    "make",
    "data",
    "notebook",
    "stuff",
    "would",
    "right",
    "let",
    "get",
    "code",
    "okay",
    "see",
    "basic",
    "two",
    "first",
    "one",
    "kind",
    "way",
    "vader",
    "words",
    "gon",
    "na",
    "look",
    "type",
    "results",
    "compare",
    "also",
    "pipeline",
    "show",
    "side",
    "set",
    "rating",
    "five",
    "stars",
    "reviewer",
    "gave",
    "give",
    "import",
    "numpy",
    "plot",
    "go",
    "read",
    "comment",
    "say",
    "head",
    "command",
    "row",
    "id",
    "interesting",
    "score",
    "star",
    "little",
    "us",
    "actual",
    "review",
    "running",
    "actually",
    "print",
    "time",
    "pretty",
    "500",
    "rows",
    "frame",
    "could",
    "easily",
    "run",
    "put",
    "looks",
    "like",
    "take",
    "know",
    "value",
    "1",
    "sort",
    "index",
    "bar",
    "size",
    "call",
    "positive",
    "good",
    "um",
    "next",
    "thing",
    "example",
    "pick",
    "oatmeal",
    "soft",
    "uh",
    "negative",
    "sentence",
    "word",
    "tokenizer",
    "basically",
    "need",
    "tokens",
    "remember",
    "part",
    "speech",
    "values",
    "examples",
    "takes",
    "oh",
    "yeah",
    "store",
    "dot",
    "automatically",
    "want",
    "dictionary",
    "neutral",
    "statement",
    "something",
    "analyzer",
    "tqdm",
    "sure",
    "point",
    "zero",
    "three",
    "compound",
    "polarity",
    "scores",
    "simple",
    "think",
    "break",
    "ran",
    "result",
    "merge",
    "higher",
    "exactly",
    "becomes",
    "looking",
    "positivity",
    "lot",
    "transformers",
    "auto",
    "max",
    "apply",
    "output",
    "combine"
  ]
}