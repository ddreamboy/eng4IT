{
  "text": "Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about\nrecurrent neural networks, and they're going to be clearly explained!\nLightning and Grid are totally cool. Check them out when you've got some time.\nNOTE: This StatQuest assumes that you are already familiar with the main ideas behind\nneural networks, backpropagation and the ReLU activation function. If not, check\nout the 'Quest. ALSO NOTE: Although basic, or vanilla recurrent neural networks are\nawesome, they are usually thought of as a stepping stone to understanding fancier\nthings like Long Short-Term Memory Networks and Transformers, which we will talk\nabout in future StatQuests. In other words, every Quest worth taking take steps,\nand this is the first step. So with that said, let's say Hi to StatSquatch. Hi! And StatSquatch says,\nHello! The other day I bought stock in a company called Get Rich Quick, but the next\nday their stock price went down and I lost money. Bummer. So, I was thinking, maybe\nwe could create a neural network to predict stock prices. Wouldn't that be cool?\nThat sure would be cool 'Squatch, unfortunately the actual stock market is crazy complicated\nand we'd probably both get in a lot of trouble if we offered advice on how to make\nmoney with it, but if we go to that mystical place called StatLand things are much\nsimpler and there are far fewer lawyers. So let's build a neural network that predicts\nstock prices in StatLand. However, first let's just talk about stock market data\nin general. When we look at stock prices, they tend to change over time. For example,\nthe price of this stock went up for four days before going down. Also, the longer\na company has been traded on the stock market, the more data we'll have for it. For\nexample, we have more time points for the company represented by the blue line then\nwe have for the company represented by the red line. What that means is, if we want\nto use a neural network to predict stock prices, then we need a neural network that\nworks with different amounts of sequential data. In other words, if we want to predict\nthe stock price for the Blue Line company on day 10, then we might want to use the\ndata from all nine of the preceding days. In contrast, if we wanted to predict the\nstock price for the Red Line company on day 10, then we would only have data for\nthe preceding five days. So we need the neural network to be flexible in terms of\nhow much sequential data we use to make a prediction.\nThis is a big difference compared to the other neural networks we've looked at in\nthis series. For example, in Neural Networks Clearly Explained, we examined a neural\nnetwork that made predictions using one input value, no more and no less. And if\nyou saw the StatQuest on neural networks with multiple inputs and outputs, you saw\nthis neural network that made predictions using two input values, no more and no less.\nAnd in the StatQuest on Deep Learning Image Classification, you saw a neural network\nthat made a prediction using an image that was six pixels by six pixels, no bigger\nand no smaller. However, now we need a neural network that can make a prediction\nusing the nine values we have for the blue company and make a prediction using the\nfive values we have for the red company. The good news is that one way to deal with\nthe problem of having different amounts of input values is to use a Recurrent Neural\nNetwork. Just like the other neural networks that we've seen before, recurrent neural\nnetworks have weights, biases, layers and activation functions. The big difference\nis that recurrent neural networks also have feedback loops. And, although this neural\nnetwork may look like it only takes a single input value, the feedback loop makes\nit possible to use sequential input values, like stock market prices collected over time, to make predictions.\nTo understand how, exactly, this recurrent neural network can make predictions with\nsequential input values, let's run some of StatLand's stock market data through it.\nIn StatLand, if the price of a stock is low for two days in a row, then, more often\nthan not, the price remains low on the next day. In other words, if yesterday and\ntoday's stock price is low, then tomorrow's price should also be low. In contrast,\nif yesterday's price was low and today's price is medium, then tomorrow's price should\nbe even higher. And when the price decreases from high to medium, then tomorrow's\nprice will be even lower. Lastly, if the price stays high for two days in a row,\nthen the price will be high tomorrow. Now that we see the general trends in stock\nprices in StatLand, we can talk about how to run yesterday and today's data through\na recurrent neural network to predict tomorrow's price. The first thing we'll do\nis scale the prices so that low equals 0, medium equals 0.5, and high equals 1.\nNow let's run the values for yesterday and today through this recurrent neural network\nand see if it can correctly predict tomorrow's value. Now, because the recurrent\nneural network has a feedback loop, we can enter yesterday and today's values into\nthe input sequentially. We'll start by plugging yesterday's value into the input.\nNow we can do the math just like we would for any other neural network. Beep. Boop.\nBeep. Boop. Boop. At this point, the output from the activation function, the y axis\ncoordinate that we will call Y sub 1, can go two places. First Y sub 1 can go towards\nthe output. And if we go that way and do the math beep, boop, boop then the output\nis the predicted value for today. However, we're not interested in the predicted\nvalue for today because we already have the actual value for today. Instead, we want\nto use both yesterday and today's value to predict tomorrow's value. So, for now,\nwe'll ignore this output, and instead, focus on what happens with this feedback loop.\nThe key to understanding how the feedback loop works is this summation. The summation\nallows us to add Y sub 1 times W sub 2, which is based on yesterday's value, to the\nvalue from today times W sub 1. In other words, the feedback loop allows both yesterday\nand today's values to influence the prediction.\nHey, this feedback loop has got me all turned around. Is there an easier way to see how this works?\nYes! There's an easier way to see what's going on. Instead of having to remember which\nvalue is in the loop, and which value is in the input, we can unroll the feedback\nloop by making a copy of the neural network for each input value. Now, instead of\npointing the feedback loop to the sum in the first copy, we can point it to the sum\nin the second copy. By unrolling the recurrent neural network, we end up with a new\nnetwork that has two inputs and two outputs. The first input is for yesterday's value,\nand if we do the math straight through to the first output like we did earlier, we\nget the predicted value for today. However, as we saw earlier, we can ignore this\noutput. This second input is for today's value and the connection between the first\nactivation function and the  second summation allows both yesterday and today's values\nto influence the final output, which gives us the predicted value for tomorrow. Now,\nwhen we put yesterday's value into the first input and we do the math just like before\nbeep boop beep boop then we follow the connection from the first activation function\nto the summation in the second copy of the neural network. Now we put today's value\ninto the second input and keep doing the math beep boop beep boop beep boop beep\nand that gives us the predicted value for tomorrow, zero,\nwhich is consistent with the original observation. In other words, the recurrent neural\nnetwork correctly predicted tomorrow's value.\nLikewise,\nwhen we run yesterday and today's values for the other scenarios through the recurrent\nneural network we predict a correct values for tomorrow.\nThis recurrent neural network performs great with two days worth of data, but what if we have three days of data?\nWhen we want to use three days of data to make a prediction about tomorrow's price,\nlike this, then we just keep unrolling the recurrent neural network until we have\nan input for each day of data. Then we plug the values into the inputs, always from\nthe oldest to the newest. In this case, that means we start by plugging in the value\nfor the day before yesterday, then we plug in yesterday's value, and then we plug\nin today's value. And when we do the math, the last output gives us the prediction\nfor tomorrow. NOTE: Regardless of how many times we unroll a recurrent neural network,\nthe weights and biases are shared across every input. In other words, even though\nthis unrolled network has three inputs the weight, W sub 1, is the same for all three\ninputs. And the bias, B sub 1, is also the same for all three inputs. Likewise, all\nof the other weights and biases are shared. So, no matter how many times we unroll\na recurrent neural network, we never increase the number of weights and biases that we have to train.\nOkay, now that we've talked about what makes basic recurrent neural networks so cool,\nlet's briefly talk about why they are not used very often. One big problem is that\nthe more we unroll a recurrent neural network, the harder it is to train. This problem\nis called The Vanishing / Exploding Gradient Problem. Which is also known as the \"hey wait, where the gradient go?\"\nproblem. In our example, The Vanishing / Exploding Gradient Problem has to do\nwith the weight along the squiggle that we copy each time we unroll the network. NOTE:\nTo make it easier to understand the Vanishing / Exploding Gradient Problem, we're\ngoing to ignore the other weights and biases in this network and just focus on W\nsub 2. Also, just to remind you when we optimize neural networks with backpropagation,\nwe first find the derivatives, or gradients, for each parameter. We then plug those\ngradients into the gradient descent algorithm to find the parameter values that minimize\na loss function, like the sum of the squared residuals. Bam. Now, even though the\nVanishing / Exploding Gradient Problem starts with Vanishing, we're going to start by showing how a gradient can explode.\nIn our example, the gradient will explode when we set W sub 2 to any value larger than one.\nSo let's set W sub 2 equal to 2. Now, the first input value, input sub 1, will be\nmultiplied by 2 on the first squiggle and then multiplied by 2 on the next squiggle\nand again on the next squiggle and again  on the last squiggle. In other words, since\nwe unrolled the recurrent neural network four times,\nwe multiply the input value by W sub 2, which is 2, raised to the number of times\nwe unrolled, which is 4. And that means the first input value is amplified 16 times\nbefore it gets to the final copy of the network. Now, if we had 50 sequential days\nof stock market data, which to be honest, really isn't that much data, then we would\nunroll the network 50 times, and 2 raised to the 50 power is a huge number. And this\nhuge number is why they call this an Exploding Gradient Problem.\nIf we tried to train this recurrent neural network with backpropagation, this huge\nnumber would find its way into some of the gradients,\nand that would make it hard to take small steps to find the optimal weights and biases.\nIn other words, in order to find the parameter values that give us the lowest value\nfor the loss function, we usually want to take relatively small steps. Bam. However,\nwhen the gradient contains a huge number, then we'll end up taking relatively large\nsteps and instead of finding the optimal parameter we will just bounce around a lot.\nBummer. One way to prevent the Exploding Gradient Problem would be to limit W Sub\n2 to values less than 1. However, this results in the Vanishing Gradient Problem. \"Hey, wait, where the gradient go?\"\nTo illustrate the Vanishing Gradient Problem, let's set W sub 2 to 0.5. Now, just\nlike before, we multiply the first input by W sub 2 raised to the number of times\nwe unroll the network. So if we have 50 sequential input values, that means multiplying\ninput sub 1 by 0.5 raised to the 50th power and 0.5 raised to the 50th power is a number super close to zero.\nBecause this number is super close to zero. This is called The Vanishing Gradient Problem.\nNow when optimizing a parameter, instead of taking steps that are too large, we end\nup taking steps that are too small. And as a result, we end up hitting the maximum\nnumber of steps we are allowed to take before we find the optimal value.\nHey, Josh, these Vanishing / Exploding Gradients are a total bummer. Is there anything we can do about them?\nYes, and we'll talk about a popular solution called Long Short-Term Memory Networks in the next StatQuest.\nNow, it's time for some Shameless Self Promotion.\nIf you want to review statistics and machine learning offline, check out my book.\nThe StatQuest Illustrated Guide to Machine Learning at statquest.org. It's over 300 pages of total awesomeness.\nHooray! We've made it to the the end of another exciting StatQuest. If you liked this\nStatQuest and want to see more, please subscribe. And if you want to support StatQuest,\nconsider contributing to my patreon campaign, becoming a channel member buying one\nor two of my original songs or a t-shirt or a hoodie, or just donate. The links are\nin the description below. Alright, until next time Quest on!\n",
  "words": [
    "hello",
    "josh",
    "starmer",
    "welcome",
    "statquest",
    "today",
    "going",
    "talk",
    "recurrent",
    "neural",
    "networks",
    "going",
    "clearly",
    "explained",
    "lightning",
    "grid",
    "totally",
    "cool",
    "check",
    "got",
    "time",
    "note",
    "statquest",
    "assumes",
    "already",
    "familiar",
    "main",
    "ideas",
    "behind",
    "neural",
    "networks",
    "backpropagation",
    "relu",
    "activation",
    "function",
    "check",
    "also",
    "note",
    "although",
    "basic",
    "vanilla",
    "recurrent",
    "neural",
    "networks",
    "awesome",
    "usually",
    "thought",
    "stepping",
    "stone",
    "understanding",
    "fancier",
    "things",
    "like",
    "long",
    "memory",
    "networks",
    "transformers",
    "talk",
    "future",
    "statquests",
    "words",
    "every",
    "quest",
    "worth",
    "taking",
    "take",
    "steps",
    "first",
    "step",
    "said",
    "let",
    "say",
    "hi",
    "statsquatch",
    "hi",
    "statsquatch",
    "says",
    "hello",
    "day",
    "bought",
    "stock",
    "company",
    "called",
    "get",
    "rich",
    "quick",
    "next",
    "day",
    "stock",
    "price",
    "went",
    "lost",
    "money",
    "bummer",
    "thinking",
    "maybe",
    "could",
    "create",
    "neural",
    "network",
    "predict",
    "stock",
    "prices",
    "would",
    "cool",
    "sure",
    "would",
    "cool",
    "unfortunately",
    "actual",
    "stock",
    "market",
    "crazy",
    "complicated",
    "probably",
    "get",
    "lot",
    "trouble",
    "offered",
    "advice",
    "make",
    "money",
    "go",
    "mystical",
    "place",
    "called",
    "statland",
    "things",
    "much",
    "simpler",
    "far",
    "fewer",
    "lawyers",
    "let",
    "build",
    "neural",
    "network",
    "predicts",
    "stock",
    "prices",
    "statland",
    "however",
    "first",
    "let",
    "talk",
    "stock",
    "market",
    "data",
    "general",
    "look",
    "stock",
    "prices",
    "tend",
    "change",
    "time",
    "example",
    "price",
    "stock",
    "went",
    "four",
    "days",
    "going",
    "also",
    "longer",
    "company",
    "traded",
    "stock",
    "market",
    "data",
    "example",
    "time",
    "points",
    "company",
    "represented",
    "blue",
    "line",
    "company",
    "represented",
    "red",
    "line",
    "means",
    "want",
    "use",
    "neural",
    "network",
    "predict",
    "stock",
    "prices",
    "need",
    "neural",
    "network",
    "works",
    "different",
    "amounts",
    "sequential",
    "data",
    "words",
    "want",
    "predict",
    "stock",
    "price",
    "blue",
    "line",
    "company",
    "day",
    "10",
    "might",
    "want",
    "use",
    "data",
    "nine",
    "preceding",
    "days",
    "contrast",
    "wanted",
    "predict",
    "stock",
    "price",
    "red",
    "line",
    "company",
    "day",
    "10",
    "would",
    "data",
    "preceding",
    "five",
    "days",
    "need",
    "neural",
    "network",
    "flexible",
    "terms",
    "much",
    "sequential",
    "data",
    "use",
    "make",
    "prediction",
    "big",
    "difference",
    "compared",
    "neural",
    "networks",
    "looked",
    "series",
    "example",
    "neural",
    "networks",
    "clearly",
    "explained",
    "examined",
    "neural",
    "network",
    "made",
    "predictions",
    "using",
    "one",
    "input",
    "value",
    "less",
    "saw",
    "statquest",
    "neural",
    "networks",
    "multiple",
    "inputs",
    "outputs",
    "saw",
    "neural",
    "network",
    "made",
    "predictions",
    "using",
    "two",
    "input",
    "values",
    "less",
    "statquest",
    "deep",
    "learning",
    "image",
    "classification",
    "saw",
    "neural",
    "network",
    "made",
    "prediction",
    "using",
    "image",
    "six",
    "pixels",
    "six",
    "pixels",
    "bigger",
    "smaller",
    "however",
    "need",
    "neural",
    "network",
    "make",
    "prediction",
    "using",
    "nine",
    "values",
    "blue",
    "company",
    "make",
    "prediction",
    "using",
    "five",
    "values",
    "red",
    "company",
    "good",
    "news",
    "one",
    "way",
    "deal",
    "problem",
    "different",
    "amounts",
    "input",
    "values",
    "use",
    "recurrent",
    "neural",
    "network",
    "like",
    "neural",
    "networks",
    "seen",
    "recurrent",
    "neural",
    "networks",
    "weights",
    "biases",
    "layers",
    "activation",
    "functions",
    "big",
    "difference",
    "recurrent",
    "neural",
    "networks",
    "also",
    "feedback",
    "loops",
    "although",
    "neural",
    "network",
    "may",
    "look",
    "like",
    "takes",
    "single",
    "input",
    "value",
    "feedback",
    "loop",
    "makes",
    "possible",
    "use",
    "sequential",
    "input",
    "values",
    "like",
    "stock",
    "market",
    "prices",
    "collected",
    "time",
    "make",
    "predictions",
    "understand",
    "exactly",
    "recurrent",
    "neural",
    "network",
    "make",
    "predictions",
    "sequential",
    "input",
    "values",
    "let",
    "run",
    "statland",
    "stock",
    "market",
    "data",
    "statland",
    "price",
    "stock",
    "low",
    "two",
    "days",
    "row",
    "often",
    "price",
    "remains",
    "low",
    "next",
    "day",
    "words",
    "yesterday",
    "today",
    "stock",
    "price",
    "low",
    "tomorrow",
    "price",
    "also",
    "low",
    "contrast",
    "yesterday",
    "price",
    "low",
    "today",
    "price",
    "medium",
    "tomorrow",
    "price",
    "even",
    "higher",
    "price",
    "decreases",
    "high",
    "medium",
    "tomorrow",
    "price",
    "even",
    "lower",
    "lastly",
    "price",
    "stays",
    "high",
    "two",
    "days",
    "row",
    "price",
    "high",
    "tomorrow",
    "see",
    "general",
    "trends",
    "stock",
    "prices",
    "statland",
    "talk",
    "run",
    "yesterday",
    "today",
    "data",
    "recurrent",
    "neural",
    "network",
    "predict",
    "tomorrow",
    "price",
    "first",
    "thing",
    "scale",
    "prices",
    "low",
    "equals",
    "0",
    "medium",
    "equals",
    "high",
    "equals",
    "let",
    "run",
    "values",
    "yesterday",
    "today",
    "recurrent",
    "neural",
    "network",
    "see",
    "correctly",
    "predict",
    "tomorrow",
    "value",
    "recurrent",
    "neural",
    "network",
    "feedback",
    "loop",
    "enter",
    "yesterday",
    "today",
    "values",
    "input",
    "sequentially",
    "start",
    "plugging",
    "yesterday",
    "value",
    "input",
    "math",
    "like",
    "would",
    "neural",
    "network",
    "beep",
    "boop",
    "beep",
    "boop",
    "boop",
    "point",
    "output",
    "activation",
    "function",
    "axis",
    "coordinate",
    "call",
    "sub",
    "1",
    "go",
    "two",
    "places",
    "first",
    "sub",
    "1",
    "go",
    "towards",
    "output",
    "go",
    "way",
    "math",
    "beep",
    "boop",
    "boop",
    "output",
    "predicted",
    "value",
    "today",
    "however",
    "interested",
    "predicted",
    "value",
    "today",
    "already",
    "actual",
    "value",
    "today",
    "instead",
    "want",
    "use",
    "yesterday",
    "today",
    "value",
    "predict",
    "tomorrow",
    "value",
    "ignore",
    "output",
    "instead",
    "focus",
    "happens",
    "feedback",
    "loop",
    "key",
    "understanding",
    "feedback",
    "loop",
    "works",
    "summation",
    "summation",
    "allows",
    "us",
    "add",
    "sub",
    "1",
    "times",
    "w",
    "sub",
    "2",
    "based",
    "yesterday",
    "value",
    "value",
    "today",
    "times",
    "w",
    "sub",
    "words",
    "feedback",
    "loop",
    "allows",
    "yesterday",
    "today",
    "values",
    "influence",
    "prediction",
    "hey",
    "feedback",
    "loop",
    "got",
    "turned",
    "around",
    "easier",
    "way",
    "see",
    "works",
    "yes",
    "easier",
    "way",
    "see",
    "going",
    "instead",
    "remember",
    "value",
    "loop",
    "value",
    "input",
    "unroll",
    "feedback",
    "loop",
    "making",
    "copy",
    "neural",
    "network",
    "input",
    "value",
    "instead",
    "pointing",
    "feedback",
    "loop",
    "sum",
    "first",
    "copy",
    "point",
    "sum",
    "second",
    "copy",
    "unrolling",
    "recurrent",
    "neural",
    "network",
    "end",
    "new",
    "network",
    "two",
    "inputs",
    "two",
    "outputs",
    "first",
    "input",
    "yesterday",
    "value",
    "math",
    "straight",
    "first",
    "output",
    "like",
    "earlier",
    "get",
    "predicted",
    "value",
    "today",
    "however",
    "saw",
    "earlier",
    "ignore",
    "output",
    "second",
    "input",
    "today",
    "value",
    "connection",
    "first",
    "activation",
    "function",
    "second",
    "summation",
    "allows",
    "yesterday",
    "today",
    "values",
    "influence",
    "final",
    "output",
    "gives",
    "us",
    "predicted",
    "value",
    "tomorrow",
    "put",
    "yesterday",
    "value",
    "first",
    "input",
    "math",
    "like",
    "beep",
    "boop",
    "beep",
    "boop",
    "follow",
    "connection",
    "first",
    "activation",
    "function",
    "summation",
    "second",
    "copy",
    "neural",
    "network",
    "put",
    "today",
    "value",
    "second",
    "input",
    "keep",
    "math",
    "beep",
    "boop",
    "beep",
    "boop",
    "beep",
    "boop",
    "beep",
    "gives",
    "us",
    "predicted",
    "value",
    "tomorrow",
    "zero",
    "consistent",
    "original",
    "observation",
    "words",
    "recurrent",
    "neural",
    "network",
    "correctly",
    "predicted",
    "tomorrow",
    "value",
    "likewise",
    "run",
    "yesterday",
    "today",
    "values",
    "scenarios",
    "recurrent",
    "neural",
    "network",
    "predict",
    "correct",
    "values",
    "tomorrow",
    "recurrent",
    "neural",
    "network",
    "performs",
    "great",
    "two",
    "days",
    "worth",
    "data",
    "three",
    "days",
    "data",
    "want",
    "use",
    "three",
    "days",
    "data",
    "make",
    "prediction",
    "tomorrow",
    "price",
    "like",
    "keep",
    "unrolling",
    "recurrent",
    "neural",
    "network",
    "input",
    "day",
    "data",
    "plug",
    "values",
    "inputs",
    "always",
    "oldest",
    "newest",
    "case",
    "means",
    "start",
    "plugging",
    "value",
    "day",
    "yesterday",
    "plug",
    "yesterday",
    "value",
    "plug",
    "today",
    "value",
    "math",
    "last",
    "output",
    "gives",
    "us",
    "prediction",
    "tomorrow",
    "note",
    "regardless",
    "many",
    "times",
    "unroll",
    "recurrent",
    "neural",
    "network",
    "weights",
    "biases",
    "shared",
    "across",
    "every",
    "input",
    "words",
    "even",
    "though",
    "unrolled",
    "network",
    "three",
    "inputs",
    "weight",
    "w",
    "sub",
    "1",
    "three",
    "inputs",
    "bias",
    "b",
    "sub",
    "1",
    "also",
    "three",
    "inputs",
    "likewise",
    "weights",
    "biases",
    "shared",
    "matter",
    "many",
    "times",
    "unroll",
    "recurrent",
    "neural",
    "network",
    "never",
    "increase",
    "number",
    "weights",
    "biases",
    "train",
    "okay",
    "talked",
    "makes",
    "basic",
    "recurrent",
    "neural",
    "networks",
    "cool",
    "let",
    "briefly",
    "talk",
    "used",
    "often",
    "one",
    "big",
    "problem",
    "unroll",
    "recurrent",
    "neural",
    "network",
    "harder",
    "train",
    "problem",
    "called",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "also",
    "known",
    "hey",
    "wait",
    "gradient",
    "go",
    "problem",
    "example",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "weight",
    "along",
    "squiggle",
    "copy",
    "time",
    "unroll",
    "network",
    "note",
    "make",
    "easier",
    "understand",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "going",
    "ignore",
    "weights",
    "biases",
    "network",
    "focus",
    "w",
    "sub",
    "also",
    "remind",
    "optimize",
    "neural",
    "networks",
    "backpropagation",
    "first",
    "find",
    "derivatives",
    "gradients",
    "parameter",
    "plug",
    "gradients",
    "gradient",
    "descent",
    "algorithm",
    "find",
    "parameter",
    "values",
    "minimize",
    "loss",
    "function",
    "like",
    "sum",
    "squared",
    "residuals",
    "bam",
    "even",
    "though",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "starts",
    "vanishing",
    "going",
    "start",
    "showing",
    "gradient",
    "explode",
    "example",
    "gradient",
    "explode",
    "set",
    "w",
    "sub",
    "2",
    "value",
    "larger",
    "one",
    "let",
    "set",
    "w",
    "sub",
    "2",
    "equal",
    "first",
    "input",
    "value",
    "input",
    "sub",
    "1",
    "multiplied",
    "2",
    "first",
    "squiggle",
    "multiplied",
    "2",
    "next",
    "squiggle",
    "next",
    "squiggle",
    "last",
    "squiggle",
    "words",
    "since",
    "unrolled",
    "recurrent",
    "neural",
    "network",
    "four",
    "times",
    "multiply",
    "input",
    "value",
    "w",
    "sub",
    "2",
    "2",
    "raised",
    "number",
    "times",
    "unrolled",
    "means",
    "first",
    "input",
    "value",
    "amplified",
    "16",
    "times",
    "gets",
    "final",
    "copy",
    "network",
    "50",
    "sequential",
    "days",
    "stock",
    "market",
    "data",
    "honest",
    "really",
    "much",
    "data",
    "would",
    "unroll",
    "network",
    "50",
    "times",
    "2",
    "raised",
    "50",
    "power",
    "huge",
    "number",
    "huge",
    "number",
    "call",
    "exploding",
    "gradient",
    "problem",
    "tried",
    "train",
    "recurrent",
    "neural",
    "network",
    "backpropagation",
    "huge",
    "number",
    "would",
    "find",
    "way",
    "gradients",
    "would",
    "make",
    "hard",
    "take",
    "small",
    "steps",
    "find",
    "optimal",
    "weights",
    "biases",
    "words",
    "order",
    "find",
    "parameter",
    "values",
    "give",
    "us",
    "lowest",
    "value",
    "loss",
    "function",
    "usually",
    "want",
    "take",
    "relatively",
    "small",
    "steps",
    "bam",
    "however",
    "gradient",
    "contains",
    "huge",
    "number",
    "end",
    "taking",
    "relatively",
    "large",
    "steps",
    "instead",
    "finding",
    "optimal",
    "parameter",
    "bounce",
    "around",
    "lot",
    "bummer",
    "one",
    "way",
    "prevent",
    "exploding",
    "gradient",
    "problem",
    "would",
    "limit",
    "w",
    "sub",
    "2",
    "values",
    "less",
    "however",
    "results",
    "vanishing",
    "gradient",
    "problem",
    "hey",
    "wait",
    "gradient",
    "go",
    "illustrate",
    "vanishing",
    "gradient",
    "problem",
    "let",
    "set",
    "w",
    "sub",
    "2",
    "like",
    "multiply",
    "first",
    "input",
    "w",
    "sub",
    "2",
    "raised",
    "number",
    "times",
    "unroll",
    "network",
    "50",
    "sequential",
    "input",
    "values",
    "means",
    "multiplying",
    "input",
    "sub",
    "1",
    "raised",
    "50th",
    "power",
    "raised",
    "50th",
    "power",
    "number",
    "super",
    "close",
    "zero",
    "number",
    "super",
    "close",
    "zero",
    "called",
    "vanishing",
    "gradient",
    "problem",
    "optimizing",
    "parameter",
    "instead",
    "taking",
    "steps",
    "large",
    "end",
    "taking",
    "steps",
    "small",
    "result",
    "end",
    "hitting",
    "maximum",
    "number",
    "steps",
    "allowed",
    "take",
    "find",
    "optimal",
    "value",
    "hey",
    "josh",
    "vanishing",
    "exploding",
    "gradients",
    "total",
    "bummer",
    "anything",
    "yes",
    "talk",
    "popular",
    "solution",
    "called",
    "long",
    "memory",
    "networks",
    "next",
    "statquest",
    "time",
    "shameless",
    "self",
    "promotion",
    "want",
    "review",
    "statistics",
    "machine",
    "learning",
    "offline",
    "check",
    "book",
    "statquest",
    "illustrated",
    "guide",
    "machine",
    "learning",
    "300",
    "pages",
    "total",
    "awesomeness",
    "hooray",
    "made",
    "end",
    "another",
    "exciting",
    "statquest",
    "liked",
    "statquest",
    "want",
    "see",
    "please",
    "subscribe",
    "want",
    "support",
    "statquest",
    "consider",
    "contributing",
    "patreon",
    "campaign",
    "becoming",
    "channel",
    "member",
    "buying",
    "one",
    "two",
    "original",
    "songs",
    "hoodie",
    "donate",
    "links",
    "description",
    "alright",
    "next",
    "time",
    "quest"
  ],
  "keywords": [
    "statquest",
    "today",
    "going",
    "talk",
    "recurrent",
    "neural",
    "networks",
    "cool",
    "time",
    "note",
    "activation",
    "function",
    "also",
    "like",
    "words",
    "taking",
    "take",
    "steps",
    "first",
    "let",
    "day",
    "stock",
    "company",
    "called",
    "next",
    "price",
    "network",
    "predict",
    "prices",
    "would",
    "market",
    "make",
    "go",
    "statland",
    "however",
    "data",
    "example",
    "days",
    "line",
    "means",
    "want",
    "use",
    "sequential",
    "prediction",
    "made",
    "predictions",
    "using",
    "one",
    "input",
    "value",
    "saw",
    "inputs",
    "two",
    "values",
    "way",
    "problem",
    "weights",
    "biases",
    "feedback",
    "loop",
    "run",
    "low",
    "yesterday",
    "tomorrow",
    "even",
    "high",
    "see",
    "math",
    "beep",
    "boop",
    "output",
    "sub",
    "1",
    "predicted",
    "instead",
    "summation",
    "us",
    "times",
    "w",
    "2",
    "hey",
    "unroll",
    "copy",
    "second",
    "end",
    "three",
    "plug",
    "number",
    "vanishing",
    "exploding",
    "gradient",
    "squiggle",
    "find",
    "gradients",
    "parameter",
    "raised",
    "50",
    "huge"
  ]
}