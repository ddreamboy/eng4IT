{
  "text": "applications of machine learning have\ngotten a lot of traction in the last few\nyears there's a couple of big categories\nthat have had wins one is identifying\npictures the equivalent of finding cats\non the internet and any problem that can\nbe made to look like that and the other\nis sequence to sequence translation this\ncan be speech to text or one language to\nanother most of the former are done with\nconvolutional neural networks most of\nthe latter are done with recurrent\nneural networks uh particularly long\nshort-term memory to give an example of\nhow long short-term memory works we will\nconsider the question of what's For\nDinner let's say for a minute that you\nare a very lucky apartment dweller and\nyou have a flatmate who loves to cook\ndinner every night he cooks one of three\nthings Sushi waffles or\npizza and you would like to be able to\npredict what you're going to have on a\ngiven night so you can plan the rest of\nyour days eating accordingly in order to\npredict what you're going to have for\ndinner you set up a neural network the\ninputs to this neural network are a\nbunch of items like the day of the week\nthe month of the year whether or not\nyour flatmate was in a late meeting\nvariables that might reasonably affect\nwhat you're going to have for\ndinner now if you're new to neural\nnetworks I highly recommend you take a\nminute and stop to watch the how neural\nnetworks work tutorial there's a link\ndown in the comments section if you'd\nrather not do that right now and you're\nstill not familiar with neural networks\nyou can think of them as a voting\nprocess and so in the neural network\nthat you set up there's a complicated\nvoting process and all of the inputs\nlike day of the week and month of the\nyear go into it and then you train it on\nyour history of what you've had for\ndinner and you learn how to predict\nwhat's going to be for dinner\ntonight the trouble is that your network\ndoesn't work very well despite carefully\nchoosing your inputs and training it\nthoroughly you still can't get much\nbetter than chance predictions on dinner\nas is often the case with complicated\nmachine learning problems it's useful to\ntake a step back and just look at the\ndata and when you do that you notice a\npattern your flat make makes pizza then\nSushi then waffles then pizza again in a\ncycle it doesn't depend on the day of\nthe week or anything else it's in a\nregular\ncycle so knowing this we can make a new\nneural\nnetwork in our new one the only inputs\nthat matter are what we had for dinner\nyesterday so if we know if we had pizza\nfor dinner yesterday it'll be sushi\ntonight Sushi yesterday waffles tonight\nand Waffles yesterday pizza tonight it\nbecomes a very simple voting process\nprocess and uh and it's right all the\ntime because your flat ma is incredibly\nconsistent now if you happen to be gone\non a given night let's say yesterday you\nwere out you don't know what was for\ndinner\nyesterday you can still predict what's\ngoing to be for dinner tonight by\nthinking back two days ago think what\nwas for dinner\nthen so what would be predicted for you\nlast night and then you can use that\nprediction in turn to make a prediction\nfor\ntonight so we make use of not only our\nactual information from yesterday but\nalso what our prediction was yesterday\nso at this point it's helpful to take a\nlittle detour and talk about vectors a\nvector is just a fancy word for a list\nof numbers if I wanted to describe the\nweather to you for a given day I could\nsay the high is 76Â° fah the low is 43\nthe wind's 13 mil an hour there's going\nto be a quarter inch of rain and the\nrelative humidity is\n83% that's all a vector is uh the reason\nthat it's useful is vectors list of\nnumbers are computer's native language\nif you want to get something into a\nformat that it's natural for a computer\nto compute to do operations on to do\nstatistical machine learning lists of\nnumbers are the way to go everything\ngets reduced to a list of numbers before\nit goes through an algorithm we can also\nhave a vector for statements like it's\nTuesday in order to encode this kind of\ninformation what we do is we make a list\nof all the possible values it could have\nin this case all the days of the week\nand we assign a number to each and then\nwe go through and set them all equal to\nzero except for the one that is true\nright now uh this format is called one\nhot en coding and it's very common to\nsee a long Vector of zeros with just one\nelement being one it seems inefficient\nbut for a computer this is a lot easier\nway to ingest that\ninformation so we can make a one hot\nVector for our prediction for dinner\ntonight we said everything equal to zero\nexcept for the dinner item that we\npredict so in this case we'll be\npredicting\nSushi now we can group together our uh\nwe can group together our inputs and\noutputs into vectors separate lists of\nnumbers and it becomes a use ful\nshorthand for describing this neural\nnetwork so we can have our dinner\nyesterday Vector our predictions for\nyesterday vector and our prediction for\ntoday\nvector and the neural network is just\nconnections between every element in\neach of those input vectors to every\nelement in the output\nvector and to complete our picture we\ncan show how the prediction for today\nwill get recycled the dotted line there\nmeans hold on to it for a day and then\nreuse it tomorrow and it becomes our\nyesterday's predictions tomorrow now we\ncan see how if we were lacking some\ninformation let's say we were out of\ntown for two weeks we can still make a\ngood guess about what's going to be for\ndinner tonight we just ignore the new\ninformation part and we can unwrap or\nunwind this Vector in time until we do\nhave some information to base it on and\nthen just play it\nforward and when it's unwrapped it looks\nlike this and we can go back as far as\nwe need to and see what was for dinner\nand then just trace it forward and play\nout our menu over the last two weeks\nuntil we find out what's for dinner\ntonight so this was a nice simple\nexample that showed recurrent neural\nnetworks now to show how they don't meet\nall of our needs we're going to write a\nchildren's book it'll have sentences of\nthe format Doug saw Jane period Jane saw\nspot period\nspot saw Doug period and so\non so our dictionary is small just the\nwords Doug Jane spot saw and a\nperiod and the task of the neural\nnetwork is to put these together in the\nright order to make a good children's\nbook so to do this we replace our food\nvectors with our dictionary vectors here\nagain it's just a list of numbers\nrepresenting each of the words so for\ninstance if Doug was the most recent\nword that I saw my new information\nVector would be all zeros except for a\none in the Doug\nposition and we similarly can represent\nour predictions and our predictions from\nyesterday now after training this neural\nnetwork and teaching it what to do we\nwould expect to see certain patterns for\ninstance anytime a name comes up Jane\nDoug or spot we would expect that to\nvote heavily for the word saw or for a\nperiod because those are the two words\nin our dictionary that can follow a\nname similarly if we had predicted a\nname on the previous time step we would\nexpect those to vote also for the word\nsaw or for a\nperiod and then by a similar method\nanytime we come across the word saw or a\nperiod we know that a name has to come\nafter that so it will learn to vote very\nstrongly for a name Jane Doug or\nspot so in this form in this formulation\nwe have a recurrent neural network for\nSimplicity I'll take the vectors and the\nweights and collapse them down to that\nlittle symbol with the dots and the\narrows the dots and the lines connecting\nthem and there's one more symbol we\nhaven't talked about yet this is a\nsquashing function and it just helps the\nnetwork to\nbehave how it works is you take all of\nyour\nvotes coming out and you subject them to\nthis squashing function for instance if\nsomething received a total vote of 0.\nfive you draw a vertical line up where\nit crosses the function you draw a\nhorizontal line over to the Y AIS and\nthere is your squashed version out for\nsmall numbers the squashed version is\npretty close to the original version but\nas your number gets larger the number\nthat comes out is closer and closer to\none and similarly if you put in a big\nnegative number then what you'll get out\nwill be very close to minus one no\nmatter what you put in what comes out is\nbetween minus one and\none so this is really helpful when you\nhave a loop like this where the same\nvalues get processed again and again day\nafter day um it is possible you can\nimagine if in the course of that\nprocessing say something got voted for\ntwice it got multiplied by two in that\ncase it would get twice as big every\ntime and very soon blow up to be\nastronomical by ensuring that it's\nalways less than one but more than minus\none you can multiply it as many times as\nyou want you can go through that Loop\nand it won't explode in a feedback loop\nthis is an example of Nega negative\nfeedback or attenuating\nfeedback so you may have noticed our\nneural network in its current state is\nsubject to some\nmistakes we could get a sentence for\ninstance of the form Doug saw Doug\nperiod because Doug strongly votes for\nthe word saw which in turn strongly\nvotes for a name any name which could be\nDoug similarly we could get something\nlike Doug saw Jane saw spot saw Doug\nbecause each of our predictions only\nlooks back one time Step It has very\nshortterm memory then it doesn't use the\ninformation from further back and it's\nsubject to these types of\nmistakes in order to overcome this we\ntake our recurrent neural network and we\nexpand it and we add some more pieces to\nit the critical part that we add to the\nmiddle here is memory we want to be able\nto remember what happened many time\nsteps\nago so in order to explain how this\nworks I'll have to describe a few new\nsymbols that we've introduced here one\nis another squashing function this one\nwith a flat\nbottom one is an X in a circle and one\nis a cross in a\ncircle so the cross in a circle is\nelement by element\naddition the way it works is you start\nwith two vectors of equal size and you\ngo down each one you add the first\nelement of one vector to the first\nelement of another vector and then the\ntotal goes into the first element of the\noutput Vector so 3 + 6 = 99 then you go\nto the next element 4 + 7al 11 and so\nyour output Vector is the same size of\neach of your input vectors just a list\nof numbers same length but it's the sum\nelement by element of the\ntwo and very closely related to this\nyou've probably guessed the X in the\ncircle is element by element\nmultiplication it's just like addition\nexcept instead of adding you multiply\nfor instance three * 6 gives you a first\nelement of 18 4 * 7 gets you 28 again\nthe output Vector is the same size of\neach of the input\nvectors now element wi multiplication\nlets you do something pretty\ncool um you imagine that you have a\nsignal and it's like a bunch of pipes\nand they have a certain amount of water\ntrying to flow down them in this case\nwe'll just assign the number to that of\n08 it's like a\nsignal now on each of those pipes we\nhave a faucet and we can open it all the\nway close it all the way or keep it\nsomewhere in the middle to either let\nthat signal come through or block it\nso in this case an open gate an open\nfaucet would be a one and a closed\nfaucet would be a zero and the way this\nworks with element wise multiplication\nwe get 8 * 1 = 8 that signal passed\nright through into the output Vector but\nthe last element 8 * 0 equals z that\nsignal the original signal was\neffectively\nblocked and then with the gating value\nof 0.5 the signal was passed through but\nit's smaller it's\nattenuated so gating lets us control\nwhat passes through and what gets\nblocked which is really\nuseful now in order to do gating it's\nnice to have a value that you know is\nalways between zero and one so we\nintroduce another squashing function\nthis will represent with a circle with a\nflat bottom and this is It's called The\nlogistic function it's very similar to\nthe other squashing function the\nhyperbolic tangent except that it just\ngoes between zero and one instead of\nminus one and\none now when we introduce all of these\ntogether what we get we still have the\ncombination of our previous predictions\nand our new information those vectors\nget pass and we make predictions based\non them those predictions get get passed\nthrough but the other thing that happens\nis a copy of those\npredictions is held on to for the next\ntime step the next pass through the\nnetwork and some of them here's a gate\nright here some of them are forgotten\nsome of them are remembered the ones\nthat are remembered are added back into\nthe prediction so now we have not just\nprediction but predictions plus the\nmemories that we've accumulated and that\nwe haven't chosen to forget\nyet now there's an entirely separate\nneural network here that learns when to\nforget what based on what we're seeing\nright now what do we want to remember\nwhat do we want to\nforget so you can see this is powerful\nthis will let us hold on to things for\nas long as we\nwant now you've probably noticed though\num when we are combining our predictions\nwith our memories we may not necessarily\nwant to release all of those memories\nout as new predictions each time so we\nwant a little filter to keep our\nmemories inside and let our predictions\nget out and that's we add another gate\nfor that to do selection it has its own\nneural network so its own voting process\nso that our new information and our\nprevious predictions can be used to vote\non what all the gates should be what\nshould be kept internal and what should\nbe released as a\nprediction we've also introduced another\nsquashing function here since we do an\naddition here it's possible that things\ncould become greater than one or smaller\nthan minus one so we just squash it to\nbe careful to make sure it never gets\nout of\ncontrol and now when we bring in new\npredictions we make a lot of\npossibilities and then we collect those\nwith memory over time and of all of\nthose possible predictions at each time\nstep we select just a few to release as\nthe prediction for that\nmoment each of these things when to\nforget and when to let things out of our\nmemory are learned by their own neural\nnetworks and the only other piece we\nneed to add to complete our picture here\nis yet another set of gates this lets us\nactually ignore uh possible predictions\npossibilities as they come in this is an\nattention mechanism it lets things that\naren't immediately relevant be set aside\nso they don't Cloud the predictions in\nmemory going\nforward it has its own neural network\nand its own logistic squashing function\nand its own gating activity right\nhere now long short-term memory has a\nlot of pieces a lot of bits that work\ntogether and it's a little much to wrap\nyour head around it all at once so what\nwe'll do is take a very simple example\nand step through it just to illustrate\nhow a couple of these pieces work it's\nadmittedly an overly simplistic example\nand feel free to poke holes at it later\nwhen you get to that point then you know\nyou're ready to move on to the next\nlevel of material so we are now in the\nprocess of writing our children's\nbook and for the purposes of\ndemonstration we'll assume that this\nlstm has been trained\non our children's books examples that we\nwant to\nmimic and all of the appropriate votes\nand weights in those neural networks\nhave been learned now we'll show it in\naction so so far our story so far is\nJane saw spot period Doug so Doug is the\nmost recent word that's occurred in our\nstory and also not surprisingly for this\ntime step um the names Doug Jane and\nspot were all predicted as viable\noptions this makes sense we had just\nwrapped up a sentence with a period the\nnew sentence can start with any name so\nthese are all great predictions so we\nhave our new information which is the\nword Doug we have our recent prediction\nwhich is Doug Jane and spot and we\npassed these two vectors together to all\nfour of our neural networks which are\nlearning to make predictions\nto do it ignoring to do forgetting and\nto do\nselection so the first one of these\nmakes some\npredictions given that the word Doug\njust occurred this is learned that the\nword saw is a great guess to make for a\nnext word but it's also learned that\nhaving seen the word\nDoug that it should not see the word\nDoug again\nvery soon seeing the word Doug at the\nbeginning of a sentence so it makes a\npositive prediction for Saul and a\nnegative prediction for Doug it says I\ndo not expect to see Doug in the near\nfuture so that's why Doug is in Black so\nthis example is so simple we don't need\nto focus on attention or ignoring so\nwe'll skip over it for now and this\nprediction of Saw not Doug is passed\nforward and again for the purposes of\nSimplicity let's say that there's no\nmemory at the moment so saw and Doug get\npassed forward and then the selection\nmechanism here has learned that when the\nmost recent word was a name then what\ncomes next is either going to be the\nword saw or a period so it blocks any\nother names from coming out so the fact\nthat there's a vote for not Doug gets\nblocked here and the word saw gets sent\nout as the prediction for the next time\nstep so we take a step forward in time\nnow the word saw is our most recent word\nand our most recent prediction they get\npassed forward to all of these neural\nnetworks and we get a new set of\npredictions because the word saw just\noccurred we now predict that the words\nDoug Jane or spot might come\nnext we'll pass over ignoring and\nattention in this example and we'll take\nthose predictions\nforward now the other thing that\nhappened is our previous set of uh\npossibilities the word saw and not Doug\nthat we were maintaining\ninternally get passed to a forgetting\ngate now the forgetting gate says hey my\nlast word that came uh that occurred was\nthe word saw based on my past experience\nthen I for can forget about you know I\nknow that it occurred I can forget that\nit happened but I want to keep any\npredictions having to do with names so\nit forgets saw holds on to the vote for\nnot Doug and now at this ele element by\nelement addition we have a positive vote\nfor Doug a negative vote for Doug and so\nthey cancel each other out so now we\njust have votes for Jane and\nspot those get passed forward our\nselection gate it knows that the word\nsaw just occurred and based on\nexperience a name will happen next and\nso it passes through these predictions\nfor names and for the next time step\nthen we get predictions of only Jane and\nspot not Doug this avoids the Doug saw\nDoug period type of error and the other\nerrors that we\nsaw what do shows is that long\nshort-term memory can look back two\nthree many any time steps and use that\ninformation to make good predictions\nabout what's going to happen next now to\nbe fair to vanilla recurrent neural\nnetworks they can actually look back\nseveral time steps as well but not very\nmany lstm can look back many time steps\nand has shown that\nsuccessfully this is really useful in\nsome surprisingly practical applications\nif I have text in one language and I\nwant to translate it to text to another\nlanguage lstms work very well even\nthough translation is not a word tow\nprocess it's a phrase to phrase or even\nin some cases a sentence to sentence\nprocess lstms are able to represent\nthose grammar structures that are\nspecific to each language\nand what it looks like is that they\nfind the higher level idea and translate\nit from mo one mode of expression to\nanother just using the bits and pieces\nthat we just walked\nthrough another thing that they do well\nis translating speech to\ntext speech is just some signals that\nvary in time it takes them and uses that\nthen to predict what text what word is\nbeing spoken and it can\nuse the history the recent history of of\nwords to make a better guess for what's\ngoing to come\nnext lstms are a great fit for any\ninformation that's embedded in time\naudio video uh my favorite application\nof all of course is robotics robotics is\nnothing more than uh an agent taking in\ninformation from a set of sensors and\nthen based on that information making a\ndecision and carrying out an action it's\ninherently sequential and actions taken\nnow can influence what is sensed and\nwhat should be done many time steps down\nthe\nline if you're curious what lstms look\nlike in math this is it this is lifted\nstraight from the Wikipedia page I won't\nstep through it but it's encouraging\nthat something that looks so complex\nexpressed mathematically uh actually\nmakes it fairly straightforward picture\nand story and if you'd like to dig into\nit more I encourage you to go to the\nWikipedia page also there are a\ncollection of really good tutorials and\ndiscussions other ways of explaining\nlstms that you may find helpful as well\nI'd also strongly encourage you to visit\nAndre kathi's blog post showing examples\nof what lstms can do in\ntext and if you haven't seen it yet take\na look at the video on how neural\nnetworks work to get some more details\non exactly how you go about implementing\nsomething like this in code thanks for\ntuning in I wish you a lot of luck on\nyour next project building with\nrecurrent neural networks\n",
  "words": [
    "applications",
    "machine",
    "learning",
    "gotten",
    "lot",
    "traction",
    "last",
    "years",
    "couple",
    "big",
    "categories",
    "wins",
    "one",
    "identifying",
    "pictures",
    "equivalent",
    "finding",
    "cats",
    "internet",
    "problem",
    "made",
    "look",
    "like",
    "sequence",
    "sequence",
    "translation",
    "speech",
    "text",
    "one",
    "language",
    "another",
    "former",
    "done",
    "convolutional",
    "neural",
    "networks",
    "latter",
    "done",
    "recurrent",
    "neural",
    "networks",
    "uh",
    "particularly",
    "long",
    "memory",
    "give",
    "example",
    "long",
    "memory",
    "works",
    "consider",
    "question",
    "dinner",
    "let",
    "say",
    "minute",
    "lucky",
    "apartment",
    "dweller",
    "flatmate",
    "loves",
    "cook",
    "dinner",
    "every",
    "night",
    "cooks",
    "one",
    "three",
    "things",
    "sushi",
    "waffles",
    "pizza",
    "would",
    "like",
    "able",
    "predict",
    "going",
    "given",
    "night",
    "plan",
    "rest",
    "days",
    "eating",
    "accordingly",
    "order",
    "predict",
    "going",
    "dinner",
    "set",
    "neural",
    "network",
    "inputs",
    "neural",
    "network",
    "bunch",
    "items",
    "like",
    "day",
    "week",
    "month",
    "year",
    "whether",
    "flatmate",
    "late",
    "meeting",
    "variables",
    "might",
    "reasonably",
    "affect",
    "going",
    "dinner",
    "new",
    "neural",
    "networks",
    "highly",
    "recommend",
    "take",
    "minute",
    "stop",
    "watch",
    "neural",
    "networks",
    "work",
    "tutorial",
    "link",
    "comments",
    "section",
    "rather",
    "right",
    "still",
    "familiar",
    "neural",
    "networks",
    "think",
    "voting",
    "process",
    "neural",
    "network",
    "set",
    "complicated",
    "voting",
    "process",
    "inputs",
    "like",
    "day",
    "week",
    "month",
    "year",
    "go",
    "train",
    "history",
    "dinner",
    "learn",
    "predict",
    "going",
    "dinner",
    "tonight",
    "trouble",
    "network",
    "work",
    "well",
    "despite",
    "carefully",
    "choosing",
    "inputs",
    "training",
    "thoroughly",
    "still",
    "ca",
    "get",
    "much",
    "better",
    "chance",
    "predictions",
    "dinner",
    "often",
    "case",
    "complicated",
    "machine",
    "learning",
    "problems",
    "useful",
    "take",
    "step",
    "back",
    "look",
    "data",
    "notice",
    "pattern",
    "flat",
    "make",
    "makes",
    "pizza",
    "sushi",
    "waffles",
    "pizza",
    "cycle",
    "depend",
    "day",
    "week",
    "anything",
    "else",
    "regular",
    "cycle",
    "knowing",
    "make",
    "new",
    "neural",
    "network",
    "new",
    "one",
    "inputs",
    "matter",
    "dinner",
    "yesterday",
    "know",
    "pizza",
    "dinner",
    "yesterday",
    "sushi",
    "tonight",
    "sushi",
    "yesterday",
    "waffles",
    "tonight",
    "waffles",
    "yesterday",
    "pizza",
    "tonight",
    "becomes",
    "simple",
    "voting",
    "process",
    "process",
    "uh",
    "right",
    "time",
    "flat",
    "incredibly",
    "consistent",
    "happen",
    "gone",
    "given",
    "night",
    "let",
    "say",
    "yesterday",
    "know",
    "dinner",
    "yesterday",
    "still",
    "predict",
    "going",
    "dinner",
    "tonight",
    "thinking",
    "back",
    "two",
    "days",
    "ago",
    "think",
    "dinner",
    "would",
    "predicted",
    "last",
    "night",
    "use",
    "prediction",
    "turn",
    "make",
    "prediction",
    "tonight",
    "make",
    "use",
    "actual",
    "information",
    "yesterday",
    "also",
    "prediction",
    "yesterday",
    "point",
    "helpful",
    "take",
    "little",
    "detour",
    "talk",
    "vectors",
    "vector",
    "fancy",
    "word",
    "list",
    "numbers",
    "wanted",
    "describe",
    "weather",
    "given",
    "day",
    "could",
    "say",
    "high",
    "fah",
    "low",
    "43",
    "wind",
    "13",
    "mil",
    "hour",
    "going",
    "quarter",
    "inch",
    "rain",
    "relative",
    "humidity",
    "83",
    "vector",
    "uh",
    "reason",
    "useful",
    "vectors",
    "list",
    "numbers",
    "computer",
    "native",
    "language",
    "want",
    "get",
    "something",
    "format",
    "natural",
    "computer",
    "compute",
    "operations",
    "statistical",
    "machine",
    "learning",
    "lists",
    "numbers",
    "way",
    "go",
    "everything",
    "gets",
    "reduced",
    "list",
    "numbers",
    "goes",
    "algorithm",
    "also",
    "vector",
    "statements",
    "like",
    "tuesday",
    "order",
    "encode",
    "kind",
    "information",
    "make",
    "list",
    "possible",
    "values",
    "could",
    "case",
    "days",
    "week",
    "assign",
    "number",
    "go",
    "set",
    "equal",
    "zero",
    "except",
    "one",
    "true",
    "right",
    "uh",
    "format",
    "called",
    "one",
    "hot",
    "en",
    "coding",
    "common",
    "see",
    "long",
    "vector",
    "zeros",
    "one",
    "element",
    "one",
    "seems",
    "inefficient",
    "computer",
    "lot",
    "easier",
    "way",
    "ingest",
    "information",
    "make",
    "one",
    "hot",
    "vector",
    "prediction",
    "dinner",
    "tonight",
    "said",
    "everything",
    "equal",
    "zero",
    "except",
    "dinner",
    "item",
    "predict",
    "case",
    "predicting",
    "sushi",
    "group",
    "together",
    "uh",
    "group",
    "together",
    "inputs",
    "outputs",
    "vectors",
    "separate",
    "lists",
    "numbers",
    "becomes",
    "use",
    "ful",
    "shorthand",
    "describing",
    "neural",
    "network",
    "dinner",
    "yesterday",
    "vector",
    "predictions",
    "yesterday",
    "vector",
    "prediction",
    "today",
    "vector",
    "neural",
    "network",
    "connections",
    "every",
    "element",
    "input",
    "vectors",
    "every",
    "element",
    "output",
    "vector",
    "complete",
    "picture",
    "show",
    "prediction",
    "today",
    "get",
    "recycled",
    "dotted",
    "line",
    "means",
    "hold",
    "day",
    "reuse",
    "tomorrow",
    "becomes",
    "yesterday",
    "predictions",
    "tomorrow",
    "see",
    "lacking",
    "information",
    "let",
    "say",
    "town",
    "two",
    "weeks",
    "still",
    "make",
    "good",
    "guess",
    "going",
    "dinner",
    "tonight",
    "ignore",
    "new",
    "information",
    "part",
    "unwrap",
    "unwind",
    "vector",
    "time",
    "information",
    "base",
    "play",
    "forward",
    "unwrapped",
    "looks",
    "like",
    "go",
    "back",
    "far",
    "need",
    "see",
    "dinner",
    "trace",
    "forward",
    "play",
    "menu",
    "last",
    "two",
    "weeks",
    "find",
    "dinner",
    "tonight",
    "nice",
    "simple",
    "example",
    "showed",
    "recurrent",
    "neural",
    "networks",
    "show",
    "meet",
    "needs",
    "going",
    "write",
    "children",
    "book",
    "sentences",
    "format",
    "doug",
    "saw",
    "jane",
    "period",
    "jane",
    "saw",
    "spot",
    "period",
    "spot",
    "saw",
    "doug",
    "period",
    "dictionary",
    "small",
    "words",
    "doug",
    "jane",
    "spot",
    "saw",
    "period",
    "task",
    "neural",
    "network",
    "put",
    "together",
    "right",
    "order",
    "make",
    "good",
    "children",
    "book",
    "replace",
    "food",
    "vectors",
    "dictionary",
    "vectors",
    "list",
    "numbers",
    "representing",
    "words",
    "instance",
    "doug",
    "recent",
    "word",
    "saw",
    "new",
    "information",
    "vector",
    "would",
    "zeros",
    "except",
    "one",
    "doug",
    "position",
    "similarly",
    "represent",
    "predictions",
    "predictions",
    "yesterday",
    "training",
    "neural",
    "network",
    "teaching",
    "would",
    "expect",
    "see",
    "certain",
    "patterns",
    "instance",
    "anytime",
    "name",
    "comes",
    "jane",
    "doug",
    "spot",
    "would",
    "expect",
    "vote",
    "heavily",
    "word",
    "saw",
    "period",
    "two",
    "words",
    "dictionary",
    "follow",
    "name",
    "similarly",
    "predicted",
    "name",
    "previous",
    "time",
    "step",
    "would",
    "expect",
    "vote",
    "also",
    "word",
    "saw",
    "period",
    "similar",
    "method",
    "anytime",
    "come",
    "across",
    "word",
    "saw",
    "period",
    "know",
    "name",
    "come",
    "learn",
    "vote",
    "strongly",
    "name",
    "jane",
    "doug",
    "spot",
    "form",
    "formulation",
    "recurrent",
    "neural",
    "network",
    "simplicity",
    "take",
    "vectors",
    "weights",
    "collapse",
    "little",
    "symbol",
    "dots",
    "arrows",
    "dots",
    "lines",
    "connecting",
    "one",
    "symbol",
    "talked",
    "yet",
    "squashing",
    "function",
    "helps",
    "network",
    "behave",
    "works",
    "take",
    "votes",
    "coming",
    "subject",
    "squashing",
    "function",
    "instance",
    "something",
    "received",
    "total",
    "vote",
    "five",
    "draw",
    "vertical",
    "line",
    "crosses",
    "function",
    "draw",
    "horizontal",
    "line",
    "ais",
    "squashed",
    "version",
    "small",
    "numbers",
    "squashed",
    "version",
    "pretty",
    "close",
    "original",
    "version",
    "number",
    "gets",
    "larger",
    "number",
    "comes",
    "closer",
    "closer",
    "one",
    "similarly",
    "put",
    "big",
    "negative",
    "number",
    "get",
    "close",
    "minus",
    "one",
    "matter",
    "put",
    "comes",
    "minus",
    "one",
    "one",
    "really",
    "helpful",
    "loop",
    "like",
    "values",
    "get",
    "processed",
    "day",
    "day",
    "um",
    "possible",
    "imagine",
    "course",
    "processing",
    "say",
    "something",
    "got",
    "voted",
    "twice",
    "got",
    "multiplied",
    "two",
    "case",
    "would",
    "get",
    "twice",
    "big",
    "every",
    "time",
    "soon",
    "blow",
    "astronomical",
    "ensuring",
    "always",
    "less",
    "one",
    "minus",
    "one",
    "multiply",
    "many",
    "times",
    "want",
    "go",
    "loop",
    "wo",
    "explode",
    "feedback",
    "loop",
    "example",
    "nega",
    "negative",
    "feedback",
    "attenuating",
    "feedback",
    "may",
    "noticed",
    "neural",
    "network",
    "current",
    "state",
    "subject",
    "mistakes",
    "could",
    "get",
    "sentence",
    "instance",
    "form",
    "doug",
    "saw",
    "doug",
    "period",
    "doug",
    "strongly",
    "votes",
    "word",
    "saw",
    "turn",
    "strongly",
    "votes",
    "name",
    "name",
    "could",
    "doug",
    "similarly",
    "could",
    "get",
    "something",
    "like",
    "doug",
    "saw",
    "jane",
    "saw",
    "spot",
    "saw",
    "doug",
    "predictions",
    "looks",
    "back",
    "one",
    "time",
    "step",
    "shortterm",
    "memory",
    "use",
    "information",
    "back",
    "subject",
    "types",
    "mistakes",
    "order",
    "overcome",
    "take",
    "recurrent",
    "neural",
    "network",
    "expand",
    "add",
    "pieces",
    "critical",
    "part",
    "add",
    "middle",
    "memory",
    "want",
    "able",
    "remember",
    "happened",
    "many",
    "time",
    "steps",
    "ago",
    "order",
    "explain",
    "works",
    "describe",
    "new",
    "symbols",
    "introduced",
    "one",
    "another",
    "squashing",
    "function",
    "one",
    "flat",
    "bottom",
    "one",
    "x",
    "circle",
    "one",
    "cross",
    "circle",
    "cross",
    "circle",
    "element",
    "element",
    "addition",
    "way",
    "works",
    "start",
    "two",
    "vectors",
    "equal",
    "size",
    "go",
    "one",
    "add",
    "first",
    "element",
    "one",
    "vector",
    "first",
    "element",
    "another",
    "vector",
    "total",
    "goes",
    "first",
    "element",
    "output",
    "vector",
    "3",
    "6",
    "99",
    "go",
    "next",
    "element",
    "4",
    "7al",
    "11",
    "output",
    "vector",
    "size",
    "input",
    "vectors",
    "list",
    "numbers",
    "length",
    "sum",
    "element",
    "element",
    "two",
    "closely",
    "related",
    "probably",
    "guessed",
    "x",
    "circle",
    "element",
    "element",
    "multiplication",
    "like",
    "addition",
    "except",
    "instead",
    "adding",
    "multiply",
    "instance",
    "three",
    "6",
    "gives",
    "first",
    "element",
    "18",
    "4",
    "7",
    "gets",
    "28",
    "output",
    "vector",
    "size",
    "input",
    "vectors",
    "element",
    "wi",
    "multiplication",
    "lets",
    "something",
    "pretty",
    "cool",
    "um",
    "imagine",
    "signal",
    "like",
    "bunch",
    "pipes",
    "certain",
    "amount",
    "water",
    "trying",
    "flow",
    "case",
    "assign",
    "number",
    "08",
    "like",
    "signal",
    "pipes",
    "faucet",
    "open",
    "way",
    "close",
    "way",
    "keep",
    "somewhere",
    "middle",
    "either",
    "let",
    "signal",
    "come",
    "block",
    "case",
    "open",
    "gate",
    "open",
    "faucet",
    "would",
    "one",
    "closed",
    "faucet",
    "would",
    "zero",
    "way",
    "works",
    "element",
    "wise",
    "multiplication",
    "get",
    "8",
    "1",
    "8",
    "signal",
    "passed",
    "right",
    "output",
    "vector",
    "last",
    "element",
    "8",
    "0",
    "equals",
    "z",
    "signal",
    "original",
    "signal",
    "effectively",
    "blocked",
    "gating",
    "value",
    "signal",
    "passed",
    "smaller",
    "attenuated",
    "gating",
    "lets",
    "us",
    "control",
    "passes",
    "gets",
    "blocked",
    "really",
    "useful",
    "order",
    "gating",
    "nice",
    "value",
    "know",
    "always",
    "zero",
    "one",
    "introduce",
    "another",
    "squashing",
    "function",
    "represent",
    "circle",
    "flat",
    "bottom",
    "called",
    "logistic",
    "function",
    "similar",
    "squashing",
    "function",
    "hyperbolic",
    "tangent",
    "except",
    "goes",
    "zero",
    "one",
    "instead",
    "minus",
    "one",
    "one",
    "introduce",
    "together",
    "get",
    "still",
    "combination",
    "previous",
    "predictions",
    "new",
    "information",
    "vectors",
    "get",
    "pass",
    "make",
    "predictions",
    "based",
    "predictions",
    "get",
    "get",
    "passed",
    "thing",
    "happens",
    "copy",
    "predictions",
    "held",
    "next",
    "time",
    "step",
    "next",
    "pass",
    "network",
    "gate",
    "right",
    "forgotten",
    "remembered",
    "ones",
    "remembered",
    "added",
    "back",
    "prediction",
    "prediction",
    "predictions",
    "plus",
    "memories",
    "accumulated",
    "chosen",
    "forget",
    "yet",
    "entirely",
    "separate",
    "neural",
    "network",
    "learns",
    "forget",
    "based",
    "seeing",
    "right",
    "want",
    "remember",
    "want",
    "forget",
    "see",
    "powerful",
    "let",
    "us",
    "hold",
    "things",
    "long",
    "want",
    "probably",
    "noticed",
    "though",
    "um",
    "combining",
    "predictions",
    "memories",
    "may",
    "necessarily",
    "want",
    "release",
    "memories",
    "new",
    "predictions",
    "time",
    "want",
    "little",
    "filter",
    "keep",
    "memories",
    "inside",
    "let",
    "predictions",
    "get",
    "add",
    "another",
    "gate",
    "selection",
    "neural",
    "network",
    "voting",
    "process",
    "new",
    "information",
    "previous",
    "predictions",
    "used",
    "vote",
    "gates",
    "kept",
    "internal",
    "released",
    "prediction",
    "also",
    "introduced",
    "another",
    "squashing",
    "function",
    "since",
    "addition",
    "possible",
    "things",
    "could",
    "become",
    "greater",
    "one",
    "smaller",
    "minus",
    "one",
    "squash",
    "careful",
    "make",
    "sure",
    "never",
    "gets",
    "control",
    "bring",
    "new",
    "predictions",
    "make",
    "lot",
    "possibilities",
    "collect",
    "memory",
    "time",
    "possible",
    "predictions",
    "time",
    "step",
    "select",
    "release",
    "prediction",
    "moment",
    "things",
    "forget",
    "let",
    "things",
    "memory",
    "learned",
    "neural",
    "networks",
    "piece",
    "need",
    "add",
    "complete",
    "picture",
    "yet",
    "another",
    "set",
    "gates",
    "lets",
    "us",
    "actually",
    "ignore",
    "uh",
    "possible",
    "predictions",
    "possibilities",
    "come",
    "attention",
    "mechanism",
    "lets",
    "things",
    "immediately",
    "relevant",
    "set",
    "aside",
    "cloud",
    "predictions",
    "memory",
    "going",
    "forward",
    "neural",
    "network",
    "logistic",
    "squashing",
    "function",
    "gating",
    "activity",
    "right",
    "long",
    "memory",
    "lot",
    "pieces",
    "lot",
    "bits",
    "work",
    "together",
    "little",
    "much",
    "wrap",
    "head",
    "around",
    "take",
    "simple",
    "example",
    "step",
    "illustrate",
    "couple",
    "pieces",
    "work",
    "admittedly",
    "overly",
    "simplistic",
    "example",
    "feel",
    "free",
    "poke",
    "holes",
    "later",
    "get",
    "point",
    "know",
    "ready",
    "move",
    "next",
    "level",
    "material",
    "process",
    "writing",
    "children",
    "book",
    "purposes",
    "demonstration",
    "assume",
    "lstm",
    "trained",
    "children",
    "books",
    "examples",
    "want",
    "mimic",
    "appropriate",
    "votes",
    "weights",
    "neural",
    "networks",
    "learned",
    "show",
    "action",
    "far",
    "story",
    "far",
    "jane",
    "saw",
    "spot",
    "period",
    "doug",
    "doug",
    "recent",
    "word",
    "occurred",
    "story",
    "also",
    "surprisingly",
    "time",
    "step",
    "um",
    "names",
    "doug",
    "jane",
    "spot",
    "predicted",
    "viable",
    "options",
    "makes",
    "sense",
    "wrapped",
    "sentence",
    "period",
    "new",
    "sentence",
    "start",
    "name",
    "great",
    "predictions",
    "new",
    "information",
    "word",
    "doug",
    "recent",
    "prediction",
    "doug",
    "jane",
    "spot",
    "passed",
    "two",
    "vectors",
    "together",
    "four",
    "neural",
    "networks",
    "learning",
    "make",
    "predictions",
    "ignoring",
    "forgetting",
    "selection",
    "first",
    "one",
    "makes",
    "predictions",
    "given",
    "word",
    "doug",
    "occurred",
    "learned",
    "word",
    "saw",
    "great",
    "guess",
    "make",
    "next",
    "word",
    "also",
    "learned",
    "seen",
    "word",
    "doug",
    "see",
    "word",
    "doug",
    "soon",
    "seeing",
    "word",
    "doug",
    "beginning",
    "sentence",
    "makes",
    "positive",
    "prediction",
    "saul",
    "negative",
    "prediction",
    "doug",
    "says",
    "expect",
    "see",
    "doug",
    "near",
    "future",
    "doug",
    "black",
    "example",
    "simple",
    "need",
    "focus",
    "attention",
    "ignoring",
    "skip",
    "prediction",
    "saw",
    "doug",
    "passed",
    "forward",
    "purposes",
    "simplicity",
    "let",
    "say",
    "memory",
    "moment",
    "saw",
    "doug",
    "get",
    "passed",
    "forward",
    "selection",
    "mechanism",
    "learned",
    "recent",
    "word",
    "name",
    "comes",
    "next",
    "either",
    "going",
    "word",
    "saw",
    "period",
    "blocks",
    "names",
    "coming",
    "fact",
    "vote",
    "doug",
    "gets",
    "blocked",
    "word",
    "saw",
    "gets",
    "sent",
    "prediction",
    "next",
    "time",
    "step",
    "take",
    "step",
    "forward",
    "time",
    "word",
    "saw",
    "recent",
    "word",
    "recent",
    "prediction",
    "get",
    "passed",
    "forward",
    "neural",
    "networks",
    "get",
    "new",
    "set",
    "predictions",
    "word",
    "saw",
    "occurred",
    "predict",
    "words",
    "doug",
    "jane",
    "spot",
    "might",
    "come",
    "next",
    "pass",
    "ignoring",
    "attention",
    "example",
    "take",
    "predictions",
    "forward",
    "thing",
    "happened",
    "previous",
    "set",
    "uh",
    "possibilities",
    "word",
    "saw",
    "doug",
    "maintaining",
    "internally",
    "get",
    "passed",
    "forgetting",
    "gate",
    "forgetting",
    "gate",
    "says",
    "hey",
    "last",
    "word",
    "came",
    "uh",
    "occurred",
    "word",
    "saw",
    "based",
    "past",
    "experience",
    "forget",
    "know",
    "know",
    "occurred",
    "forget",
    "happened",
    "want",
    "keep",
    "predictions",
    "names",
    "forgets",
    "saw",
    "holds",
    "vote",
    "doug",
    "ele",
    "element",
    "element",
    "addition",
    "positive",
    "vote",
    "doug",
    "negative",
    "vote",
    "doug",
    "cancel",
    "votes",
    "jane",
    "spot",
    "get",
    "passed",
    "forward",
    "selection",
    "gate",
    "knows",
    "word",
    "saw",
    "occurred",
    "based",
    "experience",
    "name",
    "happen",
    "next",
    "passes",
    "predictions",
    "names",
    "next",
    "time",
    "step",
    "get",
    "predictions",
    "jane",
    "spot",
    "doug",
    "avoids",
    "doug",
    "saw",
    "doug",
    "period",
    "type",
    "error",
    "errors",
    "saw",
    "shows",
    "long",
    "memory",
    "look",
    "back",
    "two",
    "three",
    "many",
    "time",
    "steps",
    "use",
    "information",
    "make",
    "good",
    "predictions",
    "going",
    "happen",
    "next",
    "fair",
    "vanilla",
    "recurrent",
    "neural",
    "networks",
    "actually",
    "look",
    "back",
    "several",
    "time",
    "steps",
    "well",
    "many",
    "lstm",
    "look",
    "back",
    "many",
    "time",
    "steps",
    "shown",
    "successfully",
    "really",
    "useful",
    "surprisingly",
    "practical",
    "applications",
    "text",
    "one",
    "language",
    "want",
    "translate",
    "text",
    "another",
    "language",
    "lstms",
    "work",
    "well",
    "even",
    "though",
    "translation",
    "word",
    "tow",
    "process",
    "phrase",
    "phrase",
    "even",
    "cases",
    "sentence",
    "sentence",
    "process",
    "lstms",
    "able",
    "represent",
    "grammar",
    "structures",
    "specific",
    "language",
    "looks",
    "like",
    "find",
    "higher",
    "level",
    "idea",
    "translate",
    "mo",
    "one",
    "mode",
    "expression",
    "another",
    "using",
    "bits",
    "pieces",
    "walked",
    "another",
    "thing",
    "well",
    "translating",
    "speech",
    "text",
    "speech",
    "signals",
    "vary",
    "time",
    "takes",
    "uses",
    "predict",
    "text",
    "word",
    "spoken",
    "use",
    "history",
    "recent",
    "history",
    "words",
    "make",
    "better",
    "guess",
    "going",
    "come",
    "next",
    "lstms",
    "great",
    "fit",
    "information",
    "embedded",
    "time",
    "audio",
    "video",
    "uh",
    "favorite",
    "application",
    "course",
    "robotics",
    "robotics",
    "nothing",
    "uh",
    "agent",
    "taking",
    "information",
    "set",
    "sensors",
    "based",
    "information",
    "making",
    "decision",
    "carrying",
    "action",
    "inherently",
    "sequential",
    "actions",
    "taken",
    "influence",
    "sensed",
    "done",
    "many",
    "time",
    "steps",
    "line",
    "curious",
    "lstms",
    "look",
    "like",
    "math",
    "lifted",
    "straight",
    "wikipedia",
    "page",
    "wo",
    "step",
    "encouraging",
    "something",
    "looks",
    "complex",
    "expressed",
    "mathematically",
    "uh",
    "actually",
    "makes",
    "fairly",
    "straightforward",
    "picture",
    "story",
    "like",
    "dig",
    "encourage",
    "go",
    "wikipedia",
    "page",
    "also",
    "collection",
    "really",
    "good",
    "tutorials",
    "discussions",
    "ways",
    "explaining",
    "lstms",
    "may",
    "find",
    "helpful",
    "well",
    "also",
    "strongly",
    "encourage",
    "visit",
    "andre",
    "kathi",
    "blog",
    "post",
    "showing",
    "examples",
    "lstms",
    "text",
    "seen",
    "yet",
    "take",
    "look",
    "video",
    "neural",
    "networks",
    "work",
    "get",
    "details",
    "exactly",
    "go",
    "implementing",
    "something",
    "like",
    "code",
    "thanks",
    "tuning",
    "wish",
    "lot",
    "luck",
    "next",
    "project",
    "building",
    "recurrent",
    "neural",
    "networks"
  ],
  "keywords": [
    "machine",
    "learning",
    "lot",
    "last",
    "big",
    "one",
    "look",
    "like",
    "speech",
    "text",
    "language",
    "another",
    "done",
    "neural",
    "networks",
    "recurrent",
    "uh",
    "long",
    "memory",
    "example",
    "works",
    "dinner",
    "let",
    "say",
    "every",
    "night",
    "three",
    "things",
    "sushi",
    "waffles",
    "pizza",
    "would",
    "able",
    "predict",
    "going",
    "given",
    "days",
    "order",
    "set",
    "network",
    "inputs",
    "day",
    "week",
    "new",
    "take",
    "work",
    "right",
    "still",
    "voting",
    "process",
    "go",
    "history",
    "tonight",
    "well",
    "get",
    "predictions",
    "case",
    "useful",
    "step",
    "back",
    "flat",
    "make",
    "makes",
    "yesterday",
    "know",
    "becomes",
    "simple",
    "time",
    "happen",
    "two",
    "predicted",
    "use",
    "prediction",
    "information",
    "also",
    "helpful",
    "little",
    "vectors",
    "vector",
    "word",
    "list",
    "numbers",
    "could",
    "computer",
    "want",
    "something",
    "format",
    "way",
    "gets",
    "goes",
    "possible",
    "number",
    "equal",
    "zero",
    "except",
    "see",
    "element",
    "together",
    "input",
    "output",
    "picture",
    "show",
    "line",
    "good",
    "guess",
    "forward",
    "looks",
    "far",
    "need",
    "find",
    "children",
    "book",
    "doug",
    "saw",
    "jane",
    "period",
    "spot",
    "dictionary",
    "words",
    "put",
    "instance",
    "recent",
    "similarly",
    "represent",
    "expect",
    "name",
    "comes",
    "vote",
    "previous",
    "come",
    "strongly",
    "yet",
    "squashing",
    "function",
    "votes",
    "subject",
    "version",
    "close",
    "negative",
    "minus",
    "really",
    "loop",
    "um",
    "many",
    "feedback",
    "may",
    "sentence",
    "add",
    "pieces",
    "happened",
    "steps",
    "circle",
    "addition",
    "size",
    "first",
    "next",
    "multiplication",
    "lets",
    "signal",
    "faucet",
    "open",
    "keep",
    "gate",
    "8",
    "passed",
    "blocked",
    "gating",
    "us",
    "pass",
    "based",
    "thing",
    "memories",
    "forget",
    "selection",
    "possibilities",
    "learned",
    "actually",
    "attention",
    "story",
    "occurred",
    "names",
    "great",
    "ignoring",
    "forgetting",
    "lstms"
  ]
}