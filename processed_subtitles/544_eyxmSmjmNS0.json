{
  "text": "hi there today we'll look at generative\nadversarial Nets by Ian J good fellow at\nall so this one is another installment\nin our series of historical papers that\nhad great impact Ganz nowadays or\ngeneral generative adversarial Nets back\nthen were sort of this was the starting\nshot in a long line of research that is\nstill continuing today so I remember\nwhen I started my PhD in 2015 Ganz were\njust about spiking I remember new rips\nor back then nips in 2016 and every\nother paper was about Ganz it was there\nwas also this famous schmidhuber\nGoodfellow moment at the tutorial it was\nit was a wild time and this is the paper\nthat started it all and the paper is\nquite well written it's very kind of\nfocused on convincing you that this is a\nsound method mathematically that it does\ndo you know that it doesn't just do wild\nthings and also it is already quite has\na lot of the it has a lot of sort of the\nmodern tricks for Gans already sort of\nbuilt into it so astounding how how much\nforesight there was already in this\npaper but of course Gans have come like\na super long way since then and today\nwe'll just go through the paper and look\nat how it looked back then and what this\npaper was like so yeah join me in this\nif you like it please share it out let\nme know in the comments what you think\nof historic paper reviews this is not\ngoing to be like in a beginners tutorial\nin Gans this is really going to be we'll\ngo through the paper you see right here\nthe paper is from 2014 so it would still\nbe another like two years or so until\nGans really take off from this point on\nbut the introduction of course was\nreally important okay\nso abstract here we go we propose a\nframework for estimating generative\nmodels via an adversarial process in\nwhich we simultaneously train two models\na generative model G that captures the\ndata distribution and a discriminative\nmodel D that estimates the probability\nthat a sample came from the training\ndata rather than G okay this was sort of\na new thing now\nI know I know people disagree with this\nbeing a new thing but this was a new\nthing and specifically this this was the\nfirst paper that made something like\nthis really work for data so to have a\ndiscriminator D and the words generator\nand discriminator were also introduced\nin this paper so you train this D model\nwhich is the discriminator and the D\nmodel basically decides whether or not a\ngiven data point comes from data or\ncomes from the fake distribution and\nthen you have a generative model G that\nis supposed to just create this data X\nrather and then coming from the database\nso you want to sample a couple of times\nfrom the data and sometimes you sample\nfrom this model G and then the\ndiscriminator is supposed to decide\nwhether or not it comes from the dataset\nor from your count from your\ncounterfeiter like from it is generator\nG and it's supposed to see it say\nwhether its data fake so you train the D\nmodel as a simple image classifier so\npeople already knew how to build image\nclassifiers this was this was shortly as\nyou can see before ResNet came on the\nscene so people already kind of knew how\nto build cnn's build really good image\nclassifiers and the thought here was\nreally generative models weren't really\na thing until then so people were in\nlanguage models work too vac was kind of\ncoming up but they were would still be\ndoing like RN ends using these word to\nvac vectors for generating language in\nimages distant AK generative models\nweren't really much of a thing so\nyou you would do like compositional\nmodels or you would do auto-encoders\nwhich were just either really blurry or\nreally really are the factory and there\nare also approaches like deep belief\nnetworks and so on but they have their\nown problems so there wasn't really a\nsatisfactory way to do image generation\nthat resulted in here really high\nquality images and now here I think the\nentire fault and this is not really\nspelled out but the entire thought here\nis that hey we know how to train really\nreally good image classifiers right this\nhas been evident in these since since\nalex net so for two years this was\nevident how to build really good image\nclassifiers and the question here is to\nsay that rather than also building\nreally good generators can't we like\nharness the power of building really\ngood classifiers\nfor training and generator right and\nthis this is this idea right here this\nwasn't the one before as you know in\nlike an autoencoder what you do is you\ninput a sample into some kind of auto\nbottleneck thing whatever and then at\nthe end you train your output sample to\nmatch the input sample as close as\npossible and then in here after you've\ntrained this this part here is your\ngenerative model and then here in here\nyou'd input like MCMC sampling or\nwhatnot and then of course variational\nautoencoders came up and so on but still\nwhat you always would do is you would\nsomehow use the data directly so this is\ndata in order to train your model so you\nwould somehow say ah the output here\nshould probably match the input in some\nway or in some at least distributional\nway right this this was a new thing as\nyou can see right here there is no\ndirect connection between the data and\nthe generator and I think this this was\nthe success of this model the fact that\nthe generator did not it wasn't trained\nfrom the data like you would do if you\nwere just approaching this problem but\nthe philosophy here is let's use the\npower of discriminative\nmodels which we know how to build in\norder to train this generator right so\nthe generators task now isn't to match\nany sort of data point the generators\ntask is to produce images that the\ndiscriminator would classify as data and\nyou can do that by simply back\npropagating through the discriminator to\nthe generator okay so I think that's\nthat's the only thing that's kind of\nunstated in this paper the the reasoning\nbehind why this is new why this might\nwork but everything else is spelled out\nvery well in this paper I have to say if\nyou read through it so the training\nprocedure for G is to maximize the\nprobability of D making a mistake\nthis framework corresponds to a minimax\ntwo-player game so as I said the paper\nis very much focused on convincing you\nthat there's something sound happening\nhere because at that time if you were to\nlook at this you'd say something like\nthere is no way right\nyou would be like yeah so so I can\nunderstand the motivation here to really\nconvince people that you know something\nsomething good is happening also on the\non the theoretical side in the spaces\nare in the space of arbitrary functions\nG and D a unique solution exists with G\nrecovering the training data\ndistribution D equals to 1/2 everywhere\nin the case where G and D are defined by\nmulti-layer perceptrons the entire\nsystem can be trained with\nbackpropagation there is no need for any\nmark of canes or unrolled approximate\ninference networks during either\ntraining or generation of samples ok so\nthe point here is that it's much easier\nthan current methods of producing of\ngenerative models and also it does\nsomething sound now let's jump into the\nloss function right here so they say G\nand D play the following two-player\nminimax game with value function V and\nthis is you know still understood\nuntil today that it was already like if\nthis was a pure engineering paper they\ncould simply build the architecture and\nsay oh we let these networks fight and\nthey they are kind of adversarial and\nthey they pump each other up and so on\nand and this here was more much more\ninto the direction of kind of a a\ntheoretical reasoning into why something\nlike this would work of course there is\nstill a lot of engineering going on to\nactually make it work so they they have\nthere is this value function right here\nokay and the value function is the\nfollowing so what you have is you have\nthe log probability of data and you have\none the log 1 minus D of the generated\nsamples so here you can see and this was\nintroduced this seems also obvious now\nright but you have a prior on what this\nis called the noise distribution okay\nthey have a prior on your input noise to\nthe generator because the generator is\nsupposed to come up with very many\ndifferent data points and if it is a if\nit is a you know non stochastic function\nlike a neural network then you need some\nway to make to produce different images\nso there is this prior distribution over\nthe noise you feed that noise into the\ngenerator the generator will produce an\noutput you put that into the\ndiscriminator and then this right here\nas you can see the discriminator is\ntrying to maximize this objective so the\ndiscriminator is trying to maximize the\nprobability of real data and it is\ntrying to minimize the probability of\nfake data okay it is this is simply a\ntwo-way classification problem at the\nsame time the generator as you can see\nis trying to minimize the objective in\nfact the order here is quite important\nso the generator as you can see is\ntrying to minimize whatever this here is\nso the generator is sort of is trying\nto minimize against the best possible\ndiscriminator and so this is one one\nobservation right here is that the\nformulation is always with respect to a\nperfect discriminator now we know that\nthis doesn't work with if you have a\nperfect discriminator than generator\ncannot catch up because you have\ninsufficient gradients and so on and\nthis was already recognized in this\npaper as well but the formulation is\nwith respect to a min Max game and not a\nmax min game so the other point I want\nto make here is that you can see the\ndiscriminator appears in both in both\nterms right here\nhowever the generator only appears right\nhere okay and this this basically means\nthat the objective for the generator is\nonly this part here because the other\npart is constant so the generator is\njust trying to make the discriminator\nthink that fake data is real so it is\ntrying to make the discriminator the\nclass of fake data as small as possible\nfor the data that it outputs while the\ndiscriminator is trying to make the\nclass of fake data more than the class\nof sorry real data yeah it's trying to\nmake it it's trying to classify fake\ndata as fake and real data as real\nwhereas the generator as only this part\non the right this is I feel this is um\nit's quite important um why because\nalready in this paper they recognize\nthat this might not be the best\npractical objective and for the\ngenerator they can actually exchange\nthis part here on the right to simply\nsay we want to so we want to instead of\n1 minus D instead of log 1 minus D we\nsimply want to use minus log D as an\nobjective for the generator so you can\nkind of play around with this and as you\nknow lots of formulations have played\naround with this loss right here and\nyeah that's why we have like a billion\nbillion billion billion gam variations\nthe\nintroduce the reasoning behind this so\nthat there's an intuition right here and\nyou can see already in practice equation\none may not provide sufficient gradient\nfor GE to learn well early in learning\nwhen G is poor D can reject samples with\nhigh confidence because they are clearly\ndifferent from the training data in this\ncase this saturates rather than training\nG to minimize that we can train G to\nmaximize log D this objective function\nresults in the same fixed point for the\ndynamic but provides much stronger\ngradients in early much stronger\ngradients early in learning this is in\ncontrast to like other papers that\nsimply say oh we do this and they at\nleast say it provides the same fixed\npoint right yeah so again they're trying\nto convince you that this is doing\nsomething useful and that this is easier\nokay so this strategy is analogous to\nother things\ntraining maintain samples from a Markov\nchain from one learning step in the next\ntwo order to avoid burning in the Markov\nchain in another loop of learning sorry\nokay this is from another paper so their\npoint here is that it's analogous to\nother papers that use these markov\nchains where you always do one step in\nGE and one step in d we alternate\nbetween K steps of optimizing D and one\nstep of optimizing G because you have\nthis inner maximization over D and then\nthe outer maxim is it in the outer\nminimization over G so this has already\nbeen around the fact that you kind of\nhave to have these optimizations in\nlockstep but the difference here is you\ndon't need any sort of like Markov chain\nin the inner loop and so on you simply\nneed back propagation so here's an\nillustration of how that might work so\nat the beginning here you have your z\nspace and this is always sampled\nuniformly as you can see right here this\nis from a prior distribution and through\nthe mapping so this here is from Z to X\nis G so this is the mapping G you can\nsee that the uniform distribution is now\nmapped to something non uniform which\nresults in the green thing\nso G is the Greenline while as this is\ndata the black dots are data and if you\nhave a discriminator the discriminator\nis supposed to tell you where there's\ndata and where there's fake data now so\ngreen here is fake now this blue line is\nsort of a half-trained discriminator now\nyou train d right you max maximize d the\ndiscriminator and that gives you this\nblue line right here so this this is a\nperfect discriminator for these two data\ndistributions it tells you it's\nbasically the the ratio of green to\nblack at each point and now you train\nthe generator according to this and you\ncan see that the gradient of the\ndiscriminator yes so the gradient of the\ndiscriminator is in this direction okay\nso it's like up this hill and that's why\nyou want to shift your green curve over\nhere according to the gradient of the\ndiscriminator note that you know we\nfirst trained the discriminator and now\nin the second step we mean we optimize\nthe generator so now we shift this green\ncurve over in order to in along the\ngradient of the blue curve so it's\nimportant the green curve doesn't see\nthe black curve ever the generator\ndoesn't see the data the generator\nsimply sees that blue curve and it goes\nalong the gradient of that blue curve of\nthe discriminator ok and then if you do\nthis many many steps actually there are\ndots right here you will end up with a\ndiscriminator that has no clue what's\nwhere this is 1/2 probability everywhere\nbecause the ratio is the same and you\nend up with the probability of data\nequal to the probability of the output\ngenerated samples and this can happen if\nthe generator simply remembers the\ntraining data\nbut there are a number of things that\ncounter that for example the generator\nis continuous while the training data is\nof course a discrete so there is this in\nbetween things right here where there is\nno training data in fact you hit a\nexactly training data is very very\nunlikely but of course you can still you\ncan still peek at the training data but\nalso the there I think there are two\nthings while the generator doesn't\nsimply remember the training data first\nbecause it doesn't ever see the training\ndata directly so it can only see it\nthrough the discriminator and second of\nall because it is built as these\nmulti-layer neural networks it doesn't\nhave the power to just remember this\nbecause as there is kind of this notion\nof continuous function so and the these\nneural networks are rather smooth\nfunctions often and therefore I think\nthat is something that helps the\ngenerator avoid remembering the training\ndata of course there is still this\nproblem of mode collapse that was really\nbig in Gantz so even if it doesn't\nremember the training data it might\nfocus on the easiest part of the\ntraining data and forget all other parts\nand that was a direct result actually of\nthis objective so where it was it so\nthis objective directly led to mode\ncollapse in some in some form because it\npenalizes different errors differently\nso of course people have come up with\nways to to solve that okay now here is\nthe algorithm as you can see this was\nalready quite it was already quite the\nalgorithm we use nowadays so for K steps\nthis is the inner maximization and here\nthey say that we use K equals 1 so all\nthis is this is pretty much what we use\ntoday the early days of Gann were still\nlike how much do I need to discriminator\nper generator and so on nowadays\neveryone's just using one step here one\nstep there or even training and jointly\nworks in some cases so you want to\nsample a mini batch of noise samples and\nyou will sample a mini batch of em\nexamples from training data generation\nso from this data you want to update the\ndiscriminator by ascending it's\nstochastic gradient and this is simply\nthe gradient of the objective and then\nafter those K steps you're going to\nsample another mini batch of noise\nsamples and update the generator by\ndescending it's stochastic gradient and\nyou can see right here already there is\nthis reduced objective that doesn't\ninclude this because it falls away in\nthe gradient right and they say the\ngradient based updates can use any\nstandard learning based rule we use\nmomentum in our experiments very cool so\nI believe they already also say that it\nis somewhere here it's pretty it's\npretty fun that they say oh in our\ngenerator we only input noise at the\nlowest layer this is also something that\nif you think that G here is a\nmulti-layer network so it's kind of a\nmulti-layer network that outputs an\nimage right and if you ask yourself if I\nhave noise how would I input that into\nthere it's so clear nowadays that you\nknow we just put it here but this was\nnot clear at all this was kind of an\ninvention of this paper because you\ncould you know put it pretty much at all\nlayers you could distribute it and so on\nyou could add some right here it it was\nthis paper that already established the\nfact that we input noise kind of as a\nvector at the very beginning and then\njust let the neural network produce the\nimage from that so yeah pretty pretty\ncool it's pretty sneaky how many things\nare hidden in these initial papers how\nmany decisions that are made there then\nare just taken over and you know this\none I guess turned out to be fairly\nfairly good okay so here they go for\nsome theoretical analysis and the first\nthey want to convince you that if the\nthe generator if this all works well\nif this if both parties this generator\nand the discriminator optimized their\nobjective to the optimum then the\ngenerator will have captured the data\ndistribution so the global optimality of\nthis and they go about convincing you of\nthat so the first thing that they\nconvince you of is that if you fix the\ngenerator the optimal discriminator is\nthis and we've already seen this in this\ndrawing right here so the optimal\ndiscriminator is simply the ratio of the\ndata of the likelihood of data versus\nthe likelihood of the generated data\nokay so you train you're always trained\neat this discriminator in the inner loop\nand that's simply the consequence of\nthis of a point wise this is true point\nwise therefore it's true over the entire\ndata distribution in the next thing they\nconvince you that the global minimum of\nthe virtual training criterion and this\nis the value function this min max game\nis achieved if and only if this holds at\nthat point the training criterion\nachieves the value of negative log four\nand this again this was already already\nhere the fact that this has a global\nminimum and it is achieved when the\ngenerator matches the data distribution\nwhich is pretty cool so in the proof\nit's pretty simple actually they first\nsay look if this is the case we just\nsimply plug that in this the\ndiscriminator will be confused so if the\ngenerator exactly captures the data the\ndiscriminator will have no clue what's\ngoing on right because it can't because\nthey're equal so it must basically\noutput the probability of 1/2 everywhere\nand then your objective becomes a\nconstant negative log for now if you\nthen plug that into the other equation\nyou'll see that the training criterion\nends\nbeing negative log four plus twice the\nJensen Shannon divergence between the\ndata and the generated distribution and\nsince this term here is always positive\nthat means that this thing here can\nnever be less than negative log four and\ntherefore the negative log four is the\noptimum okay that's it's that the proof\nis is pretty cool I have to say to show\nthat this has the optimum at that place\nand the last thing they convinced you of\nis that this algorithm actually\nconverges and the converges is simply\npredicated on the fact that if you look\nat each of these problems individually\nthey are convex so like here is convex\nand X for every alpha so each of these\nare sort of convex problems and then it\nwill naturally converge to the two their\nminimum however in practice adversarial\nnets represent a limited family of\ndistributions via the function and we\noptimize the parameters rather than the\ndistribution itself using a multi-layer\nperceptron to define G introduces\nmultiple critical points in parameter\nspace however the excellent performance\nof the multi-layer perceptrons in\npractice suggests that they are a\nreasonable model to use despite their\nlack of theoretical guarantees so they\nsay if we could optimize this\nprobability distribution directly it is\na convex problem and we always converge\nbut in practice of course we only\noptimize the parameters of an MLP or a\nCNN and that doesn't always converge but\nwe have reasonable hopes that it will\nconverge okay so again it's very much\nfocused on convincing me that this is\ndoing something sensible which I hope\nnow you are convinced so there is a\nglobal optimum point it's when the\ngenerator captures the data distribution\nperfectly this is this can be achieved\nand we\nwe'll be achieved if you can optimize\nthese probability distributions with a\nreasonable degree of freedom and the\nneural networks provide that reasonable\ndegree of freedom and you know give us\ngood hope that in practice it will work\nso they apply this to data sets namely M\nnest the Toronto phase database and C\n410 the generator Nets use the mixture\nof rectified linear activations and\nsigmoid activation z' while the\ndiscriminator net used maxout\nactivations that was still a thing\ndropout was applied in training and the\ndiscriminator net while our theoretical\nframework misused other data yeah while\nour theoretical framework permits the\nuse of dropout and other noise at\nintermediate layers of the generator we\nused noise as the input to only the\nbottom most layer of the generator\nNetwork again this wasn't kind of clear\nat the beginning and also the fact that\nto leave out dropout and so on in the\ngenerator was I guess they found that\nempirically and then there was of course\nno way to evaluate these things like how\ndo we evaluate generative models\nnowadays we have these inception\ndistances and so on but then we estimate\nprobability of the test set under P on\nthe regenerated data by fitting a\nGaussian parson window to the samples\ngenerated with G and reporting the\nlog-likelihood under this distribution\nthe theta parameter yada-yada results\nare reported this method of estimating\nthe likelihood has somewhat high\nvariance and does not perform well in\nhigh dimensional spaces but it is the\nbest method available to our knowledge\nadvances in generative models that can\nsample but not estimate likelihood\ndirectly motivated further research into\ninto how to evaluate such models they\nwere absolutely right in this and there\nwas a lot of research into Europe into\nhow to evaluate these models however I\nit is my opinion that we still have very\nvery limited methods of evaluating\nmodels like this like we have better\nmethods but it's yeah it's not really\nit's not really satisfactory how it is\nright now so you see that these models\nthese adversarial Nets by the way\nthey're always called adversarial Nets\nright here well I think we call them\nlike most people would call them\nadversarial networks but it's just\ninteresting to see the nets and also in\nthe title right it says I think it says\nNets\ndoes it I think it does we'll look at it\nafter so the T out they outperform these\nother models in especially these these\nbelief networks were kind of popular at\nthe time and you can see the samples\nright here were in no way comparable to\nexamples that you get from the modern\nGanz but this was already very very very\ngood especially the emne stand then here\nyou could ask actually recognize so the\nonce what the yellow are always from the\ntraining data set they're like the\nnearest neighbors of the things on the\nleft so they want to show that it\ndoesn't some simply remember the\ntraining data though I'm not so sure\nlike this seems like it has some surge\nsomehow remember the training data a\nlittle bit also this one right here and\nthere was already a way so this was also\nvery farsighted so these a to see were\nfully connected networks which might be\none of the reasons why worked moderately\nwell right but the last one was a\nconvolutional discriminator and AD\nconvolutional a generator so already\nusing kind of d convolutions that are\nused everywhere today so they are used\nin in ganz in whatnot we VA is to\nupsample anything if you want to do\npixel wise classification you use d\nconvolutions so again this this paper\nsort of introduced a lot of things that\nlater that we still use in guns today\nnow I'm sure D convolutions weren't\ninvented here but you know we still we\nstill use them so legit they were the\nfirst gaen paper to use the convolutions\nhaha yeah they also say we make no claim\nthat these samples are better than\nsamples generated by existing methods we\nbelieve that these samples are at least\ncompetitive with the better generative\nmodels in the literature and highlight\nthe potential of the adversary framework\ntoday this paper would be so rejected\nlike hey wait you're not better get out\nof here\nyou can't claim you can't claim this\nanymore\ndoesn't work anymore I'm sorry\nyours has always has to be better than\neverything else nowadays otherwise it's\na it's it's a weak rejecter experimental\nevidence doesn't doesn't convince me you\ncan't simply say something's cool also\nalready introduced in this paper digits\nobtained by linearly interpolating\nbetween coordinates in z space of the\nfull more like this thing here every\nsingle gantt paper had interpolations in\nthe like in this in the ganz bike and it\ncame all came from here so already this\nis just like this is a like every gam\npaper then had like rows of these like\nof these interpolations and I should\nknow I've I've written the paper on it\nand introduced right here who knows if\nthey hadn't done this yeah I guess it's\nit's kind of an obvious thing but still\nyou know very very cool to see that this\nwas already done and here ganz compared\nto other different methods like deep\ndirect graphical models generative\nauto-encoders\nand compared in very many ways so this\nis a actually good reference if you want\nto learn about these different kinds of\nmodels and they make the claim here that\nthere are advantages and disadvantages\nso disadvantages mainly come with\ntraining these things because you have\nto train them in lockstep but then also\nthe disadvantage is that you don't\nhave an explicit representation so there\nthere is no explicit representation of\nthis probability distribution you never\nbuild the data distribution you can only\nsample from it however the advantages\nare that Markov chains are never needed\nonly back prop is used to obtain\ngradients no inference is needed during\nlearning and a wide variety of functions\ncan be incorporated into the model this\nyou know I hadn't read this paper in a\nwhile and I just have to laugh nowadays\nbecause you know now all the people are\ntrying to reintroduce like there are as\nmany papers like reintroducing Markov\nchains into Gans being like oh ma Gans\nwould be so much better if they had an\nMC MC sampler somewhere you're like no\nthis it the point was to get rid of it\nand like no inference is needed during\nlearning which you know for some of\nthese other models you actually need an\ninference during training right so this\nis very very costly and how many models\nare there nowadays where it's like oh if\nwe just do this inference during\ntraining yeah so it it's quite it's\nquite funny to see people kind of trying\nto to just combine everything with\neverything and in the process sort of\nreverse reverse\nwhatever these methods were originally\nmeant to get rid of now I'm not saying\nanything against these methods but it's\njust kind of funny yeah so they had a\nlot of conclusions and future work they\nalready say you know conditional Gans\nare very easy to do straightforward\nlearned approximate inference can be\nperformed by training an auxiliary\nnetwork to predict Z given X and this of\ncourse as you know has come you know it\nhas come to fruit very often early\npapers already introduced the D so if\nyou have the G network producing some\nproducing an X and then the D network\ndiscriminating that you would also have\nlike a encoder right here\nto produce back the Z noise to give you\nthe latent encoding sort of like a\nvariational encoder but not really it's\nmore like a reverse generator\nyou know this models nowadays are big by\nGann and things like this that employ\nthis exact thing that was sort of\npredicted right here of course there are\nmuch earlier models also using this as\nlong as I can remember people have\nattempted to bring encoders into Gans\neasy they have a bunch of other things\nlike semi-supervised learning you can\nuse this to do to do get more data for a\nclassifier which is also done so a lot\nof things here already foresight in this\npapers it's pretty cool and the coolest\nthing look at that savages Goodfellow\nnot even using the full eight pages just\nnot dropping this on the world\nabsolutely cool\nmad respect yeah so yeah this was kind\nof my take on general yeah it is\ngenerative adversarial Nets and yeah\nyou'd please tell me if you like\nhistoric paper overviews it's more kind\nof a rant than it really is a paper\nexplanation but I do enjoy going through\nthis papers and kind of looking at them\nin hindsight all right that was it for\nme I wish you nice day bye bye\n",
  "words": [
    "hi",
    "today",
    "look",
    "generative",
    "adversarial",
    "nets",
    "ian",
    "j",
    "good",
    "fellow",
    "one",
    "another",
    "installment",
    "series",
    "historical",
    "papers",
    "great",
    "impact",
    "ganz",
    "nowadays",
    "general",
    "generative",
    "adversarial",
    "nets",
    "back",
    "sort",
    "starting",
    "shot",
    "long",
    "line",
    "research",
    "still",
    "continuing",
    "today",
    "remember",
    "started",
    "phd",
    "2015",
    "ganz",
    "spiking",
    "remember",
    "new",
    "rips",
    "back",
    "nips",
    "2016",
    "every",
    "paper",
    "ganz",
    "also",
    "famous",
    "schmidhuber",
    "goodfellow",
    "moment",
    "tutorial",
    "wild",
    "time",
    "paper",
    "started",
    "paper",
    "quite",
    "well",
    "written",
    "kind",
    "focused",
    "convincing",
    "sound",
    "method",
    "mathematically",
    "know",
    "wild",
    "things",
    "also",
    "already",
    "quite",
    "lot",
    "lot",
    "sort",
    "modern",
    "tricks",
    "gans",
    "already",
    "sort",
    "built",
    "astounding",
    "much",
    "foresight",
    "already",
    "paper",
    "course",
    "gans",
    "come",
    "like",
    "super",
    "long",
    "way",
    "since",
    "today",
    "go",
    "paper",
    "look",
    "looked",
    "back",
    "paper",
    "like",
    "yeah",
    "join",
    "like",
    "please",
    "share",
    "let",
    "know",
    "comments",
    "think",
    "historic",
    "paper",
    "reviews",
    "going",
    "like",
    "beginners",
    "tutorial",
    "gans",
    "really",
    "going",
    "go",
    "paper",
    "see",
    "right",
    "paper",
    "2014",
    "would",
    "still",
    "another",
    "like",
    "two",
    "years",
    "gans",
    "really",
    "take",
    "point",
    "introduction",
    "course",
    "really",
    "important",
    "okay",
    "abstract",
    "go",
    "propose",
    "framework",
    "estimating",
    "generative",
    "models",
    "via",
    "adversarial",
    "process",
    "simultaneously",
    "train",
    "two",
    "models",
    "generative",
    "model",
    "g",
    "captures",
    "data",
    "distribution",
    "discriminative",
    "model",
    "estimates",
    "probability",
    "sample",
    "came",
    "training",
    "data",
    "rather",
    "g",
    "okay",
    "sort",
    "new",
    "thing",
    "know",
    "know",
    "people",
    "disagree",
    "new",
    "thing",
    "new",
    "thing",
    "specifically",
    "first",
    "paper",
    "made",
    "something",
    "like",
    "really",
    "work",
    "data",
    "discriminator",
    "words",
    "generator",
    "discriminator",
    "also",
    "introduced",
    "paper",
    "train",
    "model",
    "discriminator",
    "model",
    "basically",
    "decides",
    "whether",
    "given",
    "data",
    "point",
    "comes",
    "data",
    "comes",
    "fake",
    "distribution",
    "generative",
    "model",
    "g",
    "supposed",
    "create",
    "data",
    "x",
    "rather",
    "coming",
    "database",
    "want",
    "sample",
    "couple",
    "times",
    "data",
    "sometimes",
    "sample",
    "model",
    "g",
    "discriminator",
    "supposed",
    "decide",
    "whether",
    "comes",
    "dataset",
    "count",
    "counterfeiter",
    "like",
    "generator",
    "g",
    "supposed",
    "see",
    "say",
    "whether",
    "data",
    "fake",
    "train",
    "model",
    "simple",
    "image",
    "classifier",
    "people",
    "already",
    "knew",
    "build",
    "image",
    "classifiers",
    "shortly",
    "see",
    "resnet",
    "came",
    "scene",
    "people",
    "already",
    "kind",
    "knew",
    "build",
    "cnn",
    "build",
    "really",
    "good",
    "image",
    "classifiers",
    "thought",
    "really",
    "generative",
    "models",
    "really",
    "thing",
    "people",
    "language",
    "models",
    "work",
    "vac",
    "kind",
    "coming",
    "would",
    "still",
    "like",
    "rn",
    "ends",
    "using",
    "word",
    "vac",
    "vectors",
    "generating",
    "language",
    "images",
    "distant",
    "ak",
    "generative",
    "models",
    "really",
    "much",
    "thing",
    "would",
    "like",
    "compositional",
    "models",
    "would",
    "either",
    "really",
    "blurry",
    "really",
    "really",
    "factory",
    "also",
    "approaches",
    "like",
    "deep",
    "belief",
    "networks",
    "problems",
    "really",
    "satisfactory",
    "way",
    "image",
    "generation",
    "resulted",
    "really",
    "high",
    "quality",
    "images",
    "think",
    "entire",
    "fault",
    "really",
    "spelled",
    "entire",
    "thought",
    "hey",
    "know",
    "train",
    "really",
    "really",
    "good",
    "image",
    "classifiers",
    "right",
    "evident",
    "since",
    "since",
    "alex",
    "net",
    "two",
    "years",
    "evident",
    "build",
    "really",
    "good",
    "image",
    "classifiers",
    "question",
    "say",
    "rather",
    "also",
    "building",
    "really",
    "good",
    "generators",
    "ca",
    "like",
    "harness",
    "power",
    "building",
    "really",
    "good",
    "classifiers",
    "training",
    "generator",
    "right",
    "idea",
    "right",
    "one",
    "know",
    "like",
    "autoencoder",
    "input",
    "sample",
    "kind",
    "auto",
    "bottleneck",
    "thing",
    "whatever",
    "end",
    "train",
    "output",
    "sample",
    "match",
    "input",
    "sample",
    "close",
    "possible",
    "trained",
    "part",
    "generative",
    "model",
    "input",
    "like",
    "mcmc",
    "sampling",
    "whatnot",
    "course",
    "variational",
    "autoencoders",
    "came",
    "still",
    "always",
    "would",
    "would",
    "somehow",
    "use",
    "data",
    "directly",
    "data",
    "order",
    "train",
    "model",
    "would",
    "somehow",
    "say",
    "ah",
    "output",
    "probably",
    "match",
    "input",
    "way",
    "least",
    "distributional",
    "way",
    "right",
    "new",
    "thing",
    "see",
    "right",
    "direct",
    "connection",
    "data",
    "generator",
    "think",
    "success",
    "model",
    "fact",
    "generator",
    "trained",
    "data",
    "like",
    "would",
    "approaching",
    "problem",
    "philosophy",
    "let",
    "use",
    "power",
    "discriminative",
    "models",
    "know",
    "build",
    "order",
    "train",
    "generator",
    "right",
    "generators",
    "task",
    "match",
    "sort",
    "data",
    "point",
    "generators",
    "task",
    "produce",
    "images",
    "discriminator",
    "would",
    "classify",
    "data",
    "simply",
    "back",
    "propagating",
    "discriminator",
    "generator",
    "okay",
    "think",
    "thing",
    "kind",
    "unstated",
    "paper",
    "reasoning",
    "behind",
    "new",
    "might",
    "work",
    "everything",
    "else",
    "spelled",
    "well",
    "paper",
    "say",
    "read",
    "training",
    "procedure",
    "g",
    "maximize",
    "probability",
    "making",
    "mistake",
    "framework",
    "corresponds",
    "minimax",
    "game",
    "said",
    "paper",
    "much",
    "focused",
    "convincing",
    "something",
    "sound",
    "happening",
    "time",
    "look",
    "say",
    "something",
    "like",
    "way",
    "right",
    "would",
    "like",
    "yeah",
    "understand",
    "motivation",
    "really",
    "convince",
    "people",
    "know",
    "something",
    "something",
    "good",
    "happening",
    "also",
    "theoretical",
    "side",
    "spaces",
    "space",
    "arbitrary",
    "functions",
    "g",
    "unique",
    "solution",
    "exists",
    "g",
    "recovering",
    "training",
    "data",
    "distribution",
    "equals",
    "everywhere",
    "case",
    "g",
    "defined",
    "perceptrons",
    "entire",
    "system",
    "trained",
    "backpropagation",
    "need",
    "mark",
    "canes",
    "unrolled",
    "approximate",
    "inference",
    "networks",
    "either",
    "training",
    "generation",
    "samples",
    "ok",
    "point",
    "much",
    "easier",
    "current",
    "methods",
    "producing",
    "generative",
    "models",
    "also",
    "something",
    "sound",
    "let",
    "jump",
    "loss",
    "function",
    "right",
    "say",
    "g",
    "play",
    "following",
    "minimax",
    "game",
    "value",
    "function",
    "v",
    "know",
    "still",
    "understood",
    "today",
    "already",
    "like",
    "pure",
    "engineering",
    "paper",
    "could",
    "simply",
    "build",
    "architecture",
    "say",
    "oh",
    "let",
    "networks",
    "fight",
    "kind",
    "adversarial",
    "pump",
    "much",
    "direction",
    "kind",
    "theoretical",
    "reasoning",
    "something",
    "like",
    "would",
    "work",
    "course",
    "still",
    "lot",
    "engineering",
    "going",
    "actually",
    "make",
    "work",
    "value",
    "function",
    "right",
    "okay",
    "value",
    "function",
    "following",
    "log",
    "probability",
    "data",
    "one",
    "log",
    "1",
    "minus",
    "generated",
    "samples",
    "see",
    "introduced",
    "seems",
    "also",
    "obvious",
    "right",
    "prior",
    "called",
    "noise",
    "distribution",
    "okay",
    "prior",
    "input",
    "noise",
    "generator",
    "generator",
    "supposed",
    "come",
    "many",
    "different",
    "data",
    "points",
    "know",
    "non",
    "stochastic",
    "function",
    "like",
    "neural",
    "network",
    "need",
    "way",
    "make",
    "produce",
    "different",
    "images",
    "prior",
    "distribution",
    "noise",
    "feed",
    "noise",
    "generator",
    "generator",
    "produce",
    "output",
    "put",
    "discriminator",
    "right",
    "see",
    "discriminator",
    "trying",
    "maximize",
    "objective",
    "discriminator",
    "trying",
    "maximize",
    "probability",
    "real",
    "data",
    "trying",
    "minimize",
    "probability",
    "fake",
    "data",
    "okay",
    "simply",
    "classification",
    "problem",
    "time",
    "generator",
    "see",
    "trying",
    "minimize",
    "objective",
    "fact",
    "order",
    "quite",
    "important",
    "generator",
    "see",
    "trying",
    "minimize",
    "whatever",
    "generator",
    "sort",
    "trying",
    "minimize",
    "best",
    "possible",
    "discriminator",
    "one",
    "one",
    "observation",
    "right",
    "formulation",
    "always",
    "respect",
    "perfect",
    "discriminator",
    "know",
    "work",
    "perfect",
    "discriminator",
    "generator",
    "catch",
    "insufficient",
    "gradients",
    "already",
    "recognized",
    "paper",
    "well",
    "formulation",
    "respect",
    "min",
    "max",
    "game",
    "max",
    "min",
    "game",
    "point",
    "want",
    "make",
    "see",
    "discriminator",
    "appears",
    "terms",
    "right",
    "however",
    "generator",
    "appears",
    "right",
    "okay",
    "basically",
    "means",
    "objective",
    "generator",
    "part",
    "part",
    "constant",
    "generator",
    "trying",
    "make",
    "discriminator",
    "think",
    "fake",
    "data",
    "real",
    "trying",
    "make",
    "discriminator",
    "class",
    "fake",
    "data",
    "small",
    "possible",
    "data",
    "outputs",
    "discriminator",
    "trying",
    "make",
    "class",
    "fake",
    "data",
    "class",
    "sorry",
    "real",
    "data",
    "yeah",
    "trying",
    "make",
    "trying",
    "classify",
    "fake",
    "data",
    "fake",
    "real",
    "data",
    "real",
    "whereas",
    "generator",
    "part",
    "right",
    "feel",
    "um",
    "quite",
    "important",
    "um",
    "already",
    "paper",
    "recognize",
    "might",
    "best",
    "practical",
    "objective",
    "generator",
    "actually",
    "exchange",
    "part",
    "right",
    "simply",
    "say",
    "want",
    "want",
    "instead",
    "1",
    "minus",
    "instead",
    "log",
    "1",
    "minus",
    "simply",
    "want",
    "use",
    "minus",
    "log",
    "objective",
    "generator",
    "kind",
    "play",
    "around",
    "know",
    "lots",
    "formulations",
    "played",
    "around",
    "loss",
    "right",
    "yeah",
    "like",
    "billion",
    "billion",
    "billion",
    "billion",
    "gam",
    "variations",
    "introduce",
    "reasoning",
    "behind",
    "intuition",
    "right",
    "see",
    "already",
    "practice",
    "equation",
    "one",
    "may",
    "provide",
    "sufficient",
    "gradient",
    "ge",
    "learn",
    "well",
    "early",
    "learning",
    "g",
    "poor",
    "reject",
    "samples",
    "high",
    "confidence",
    "clearly",
    "different",
    "training",
    "data",
    "case",
    "saturates",
    "rather",
    "training",
    "g",
    "minimize",
    "train",
    "g",
    "maximize",
    "log",
    "objective",
    "function",
    "results",
    "fixed",
    "point",
    "dynamic",
    "provides",
    "much",
    "stronger",
    "gradients",
    "early",
    "much",
    "stronger",
    "gradients",
    "early",
    "learning",
    "contrast",
    "like",
    "papers",
    "simply",
    "say",
    "oh",
    "least",
    "say",
    "provides",
    "fixed",
    "point",
    "right",
    "yeah",
    "trying",
    "convince",
    "something",
    "useful",
    "easier",
    "okay",
    "strategy",
    "analogous",
    "things",
    "training",
    "maintain",
    "samples",
    "markov",
    "chain",
    "one",
    "learning",
    "step",
    "next",
    "two",
    "order",
    "avoid",
    "burning",
    "markov",
    "chain",
    "another",
    "loop",
    "learning",
    "sorry",
    "okay",
    "another",
    "paper",
    "point",
    "analogous",
    "papers",
    "use",
    "markov",
    "chains",
    "always",
    "one",
    "step",
    "ge",
    "one",
    "step",
    "alternate",
    "k",
    "steps",
    "optimizing",
    "one",
    "step",
    "optimizing",
    "g",
    "inner",
    "maximization",
    "outer",
    "maxim",
    "outer",
    "minimization",
    "g",
    "already",
    "around",
    "fact",
    "kind",
    "optimizations",
    "lockstep",
    "difference",
    "need",
    "sort",
    "like",
    "markov",
    "chain",
    "inner",
    "loop",
    "simply",
    "need",
    "back",
    "propagation",
    "illustration",
    "might",
    "work",
    "beginning",
    "z",
    "space",
    "always",
    "sampled",
    "uniformly",
    "see",
    "right",
    "prior",
    "distribution",
    "mapping",
    "z",
    "x",
    "g",
    "mapping",
    "g",
    "see",
    "uniform",
    "distribution",
    "mapped",
    "something",
    "non",
    "uniform",
    "results",
    "green",
    "thing",
    "g",
    "greenline",
    "data",
    "black",
    "dots",
    "data",
    "discriminator",
    "discriminator",
    "supposed",
    "tell",
    "data",
    "fake",
    "data",
    "green",
    "fake",
    "blue",
    "line",
    "sort",
    "discriminator",
    "train",
    "right",
    "max",
    "maximize",
    "discriminator",
    "gives",
    "blue",
    "line",
    "right",
    "perfect",
    "discriminator",
    "two",
    "data",
    "distributions",
    "tells",
    "basically",
    "ratio",
    "green",
    "black",
    "point",
    "train",
    "generator",
    "according",
    "see",
    "gradient",
    "discriminator",
    "yes",
    "gradient",
    "discriminator",
    "direction",
    "okay",
    "like",
    "hill",
    "want",
    "shift",
    "green",
    "curve",
    "according",
    "gradient",
    "discriminator",
    "note",
    "know",
    "first",
    "trained",
    "discriminator",
    "second",
    "step",
    "mean",
    "optimize",
    "generator",
    "shift",
    "green",
    "curve",
    "order",
    "along",
    "gradient",
    "blue",
    "curve",
    "important",
    "green",
    "curve",
    "see",
    "black",
    "curve",
    "ever",
    "generator",
    "see",
    "data",
    "generator",
    "simply",
    "sees",
    "blue",
    "curve",
    "goes",
    "along",
    "gradient",
    "blue",
    "curve",
    "discriminator",
    "ok",
    "many",
    "many",
    "steps",
    "actually",
    "dots",
    "right",
    "end",
    "discriminator",
    "clue",
    "probability",
    "everywhere",
    "ratio",
    "end",
    "probability",
    "data",
    "equal",
    "probability",
    "output",
    "generated",
    "samples",
    "happen",
    "generator",
    "simply",
    "remembers",
    "training",
    "data",
    "number",
    "things",
    "counter",
    "example",
    "generator",
    "continuous",
    "training",
    "data",
    "course",
    "discrete",
    "things",
    "right",
    "training",
    "data",
    "fact",
    "hit",
    "exactly",
    "training",
    "data",
    "unlikely",
    "course",
    "still",
    "still",
    "peek",
    "training",
    "data",
    "also",
    "think",
    "two",
    "things",
    "generator",
    "simply",
    "remember",
    "training",
    "data",
    "first",
    "ever",
    "see",
    "training",
    "data",
    "directly",
    "see",
    "discriminator",
    "second",
    "built",
    "neural",
    "networks",
    "power",
    "remember",
    "kind",
    "notion",
    "continuous",
    "function",
    "neural",
    "networks",
    "rather",
    "smooth",
    "functions",
    "often",
    "therefore",
    "think",
    "something",
    "helps",
    "generator",
    "avoid",
    "remembering",
    "training",
    "data",
    "course",
    "still",
    "problem",
    "mode",
    "collapse",
    "really",
    "big",
    "gantz",
    "even",
    "remember",
    "training",
    "data",
    "might",
    "focus",
    "easiest",
    "part",
    "training",
    "data",
    "forget",
    "parts",
    "direct",
    "result",
    "actually",
    "objective",
    "objective",
    "directly",
    "led",
    "mode",
    "collapse",
    "form",
    "penalizes",
    "different",
    "errors",
    "differently",
    "course",
    "people",
    "come",
    "ways",
    "solve",
    "okay",
    "algorithm",
    "see",
    "already",
    "quite",
    "already",
    "quite",
    "algorithm",
    "use",
    "nowadays",
    "k",
    "steps",
    "inner",
    "maximization",
    "say",
    "use",
    "k",
    "equals",
    "1",
    "pretty",
    "much",
    "use",
    "today",
    "early",
    "days",
    "gann",
    "still",
    "like",
    "much",
    "need",
    "discriminator",
    "per",
    "generator",
    "nowadays",
    "everyone",
    "using",
    "one",
    "step",
    "one",
    "step",
    "even",
    "training",
    "jointly",
    "works",
    "cases",
    "want",
    "sample",
    "mini",
    "batch",
    "noise",
    "samples",
    "sample",
    "mini",
    "batch",
    "em",
    "examples",
    "training",
    "data",
    "generation",
    "data",
    "want",
    "update",
    "discriminator",
    "ascending",
    "stochastic",
    "gradient",
    "simply",
    "gradient",
    "objective",
    "k",
    "steps",
    "going",
    "sample",
    "another",
    "mini",
    "batch",
    "noise",
    "samples",
    "update",
    "generator",
    "descending",
    "stochastic",
    "gradient",
    "see",
    "right",
    "already",
    "reduced",
    "objective",
    "include",
    "falls",
    "away",
    "gradient",
    "right",
    "say",
    "gradient",
    "based",
    "updates",
    "use",
    "standard",
    "learning",
    "based",
    "rule",
    "use",
    "momentum",
    "experiments",
    "cool",
    "believe",
    "already",
    "also",
    "say",
    "somewhere",
    "pretty",
    "pretty",
    "fun",
    "say",
    "oh",
    "generator",
    "input",
    "noise",
    "lowest",
    "layer",
    "also",
    "something",
    "think",
    "g",
    "network",
    "kind",
    "network",
    "outputs",
    "image",
    "right",
    "ask",
    "noise",
    "would",
    "input",
    "clear",
    "nowadays",
    "know",
    "put",
    "clear",
    "kind",
    "invention",
    "paper",
    "could",
    "know",
    "put",
    "pretty",
    "much",
    "layers",
    "could",
    "distribute",
    "could",
    "add",
    "right",
    "paper",
    "already",
    "established",
    "fact",
    "input",
    "noise",
    "kind",
    "vector",
    "beginning",
    "let",
    "neural",
    "network",
    "produce",
    "image",
    "yeah",
    "pretty",
    "pretty",
    "cool",
    "pretty",
    "sneaky",
    "many",
    "things",
    "hidden",
    "initial",
    "papers",
    "many",
    "decisions",
    "made",
    "taken",
    "know",
    "one",
    "guess",
    "turned",
    "fairly",
    "fairly",
    "good",
    "okay",
    "go",
    "theoretical",
    "analysis",
    "first",
    "want",
    "convince",
    "generator",
    "works",
    "well",
    "parties",
    "generator",
    "discriminator",
    "optimized",
    "objective",
    "optimum",
    "generator",
    "captured",
    "data",
    "distribution",
    "global",
    "optimality",
    "go",
    "convincing",
    "first",
    "thing",
    "convince",
    "fix",
    "generator",
    "optimal",
    "discriminator",
    "already",
    "seen",
    "drawing",
    "right",
    "optimal",
    "discriminator",
    "simply",
    "ratio",
    "data",
    "likelihood",
    "data",
    "versus",
    "likelihood",
    "generated",
    "data",
    "okay",
    "train",
    "always",
    "trained",
    "eat",
    "discriminator",
    "inner",
    "loop",
    "simply",
    "consequence",
    "point",
    "wise",
    "true",
    "point",
    "wise",
    "therefore",
    "true",
    "entire",
    "data",
    "distribution",
    "next",
    "thing",
    "convince",
    "global",
    "minimum",
    "virtual",
    "training",
    "criterion",
    "value",
    "function",
    "min",
    "max",
    "game",
    "achieved",
    "holds",
    "point",
    "training",
    "criterion",
    "achieves",
    "value",
    "negative",
    "log",
    "four",
    "already",
    "already",
    "fact",
    "global",
    "minimum",
    "achieved",
    "generator",
    "matches",
    "data",
    "distribution",
    "pretty",
    "cool",
    "proof",
    "pretty",
    "simple",
    "actually",
    "first",
    "say",
    "look",
    "case",
    "simply",
    "plug",
    "discriminator",
    "confused",
    "generator",
    "exactly",
    "captures",
    "data",
    "discriminator",
    "clue",
    "going",
    "right",
    "ca",
    "equal",
    "must",
    "basically",
    "output",
    "probability",
    "everywhere",
    "objective",
    "becomes",
    "constant",
    "negative",
    "log",
    "plug",
    "equation",
    "see",
    "training",
    "criterion",
    "ends",
    "negative",
    "log",
    "four",
    "plus",
    "twice",
    "jensen",
    "shannon",
    "divergence",
    "data",
    "generated",
    "distribution",
    "since",
    "term",
    "always",
    "positive",
    "means",
    "thing",
    "never",
    "less",
    "negative",
    "log",
    "four",
    "therefore",
    "negative",
    "log",
    "four",
    "optimum",
    "okay",
    "proof",
    "pretty",
    "cool",
    "say",
    "show",
    "optimum",
    "place",
    "last",
    "thing",
    "convinced",
    "algorithm",
    "actually",
    "converges",
    "converges",
    "simply",
    "predicated",
    "fact",
    "look",
    "problems",
    "individually",
    "convex",
    "like",
    "convex",
    "x",
    "every",
    "alpha",
    "sort",
    "convex",
    "problems",
    "naturally",
    "converge",
    "two",
    "minimum",
    "however",
    "practice",
    "adversarial",
    "nets",
    "represent",
    "limited",
    "family",
    "distributions",
    "via",
    "function",
    "optimize",
    "parameters",
    "rather",
    "distribution",
    "using",
    "perceptron",
    "define",
    "g",
    "introduces",
    "multiple",
    "critical",
    "points",
    "parameter",
    "space",
    "however",
    "excellent",
    "performance",
    "perceptrons",
    "practice",
    "suggests",
    "reasonable",
    "model",
    "use",
    "despite",
    "lack",
    "theoretical",
    "guarantees",
    "say",
    "could",
    "optimize",
    "probability",
    "distribution",
    "directly",
    "convex",
    "problem",
    "always",
    "converge",
    "practice",
    "course",
    "optimize",
    "parameters",
    "mlp",
    "cnn",
    "always",
    "converge",
    "reasonable",
    "hopes",
    "converge",
    "okay",
    "much",
    "focused",
    "convincing",
    "something",
    "sensible",
    "hope",
    "convinced",
    "global",
    "optimum",
    "point",
    "generator",
    "captures",
    "data",
    "distribution",
    "perfectly",
    "achieved",
    "achieved",
    "optimize",
    "probability",
    "distributions",
    "reasonable",
    "degree",
    "freedom",
    "neural",
    "networks",
    "provide",
    "reasonable",
    "degree",
    "freedom",
    "know",
    "give",
    "us",
    "good",
    "hope",
    "practice",
    "work",
    "apply",
    "data",
    "sets",
    "namely",
    "nest",
    "toronto",
    "phase",
    "database",
    "c",
    "410",
    "generator",
    "nets",
    "use",
    "mixture",
    "rectified",
    "linear",
    "activations",
    "sigmoid",
    "activation",
    "z",
    "discriminator",
    "net",
    "used",
    "maxout",
    "activations",
    "still",
    "thing",
    "dropout",
    "applied",
    "training",
    "discriminator",
    "net",
    "theoretical",
    "framework",
    "misused",
    "data",
    "yeah",
    "theoretical",
    "framework",
    "permits",
    "use",
    "dropout",
    "noise",
    "intermediate",
    "layers",
    "generator",
    "used",
    "noise",
    "input",
    "bottom",
    "layer",
    "generator",
    "network",
    "kind",
    "clear",
    "beginning",
    "also",
    "fact",
    "leave",
    "dropout",
    "generator",
    "guess",
    "found",
    "empirically",
    "course",
    "way",
    "evaluate",
    "things",
    "like",
    "evaluate",
    "generative",
    "models",
    "nowadays",
    "inception",
    "distances",
    "estimate",
    "probability",
    "test",
    "set",
    "p",
    "regenerated",
    "data",
    "fitting",
    "gaussian",
    "parson",
    "window",
    "samples",
    "generated",
    "g",
    "reporting",
    "distribution",
    "theta",
    "parameter",
    "results",
    "reported",
    "method",
    "estimating",
    "likelihood",
    "somewhat",
    "high",
    "variance",
    "perform",
    "well",
    "high",
    "dimensional",
    "spaces",
    "best",
    "method",
    "available",
    "knowledge",
    "advances",
    "generative",
    "models",
    "sample",
    "estimate",
    "likelihood",
    "directly",
    "motivated",
    "research",
    "evaluate",
    "models",
    "absolutely",
    "right",
    "lot",
    "research",
    "europe",
    "evaluate",
    "models",
    "however",
    "opinion",
    "still",
    "limited",
    "methods",
    "evaluating",
    "models",
    "like",
    "like",
    "better",
    "methods",
    "yeah",
    "really",
    "really",
    "satisfactory",
    "right",
    "see",
    "models",
    "adversarial",
    "nets",
    "way",
    "always",
    "called",
    "adversarial",
    "nets",
    "right",
    "well",
    "think",
    "call",
    "like",
    "people",
    "would",
    "call",
    "adversarial",
    "networks",
    "interesting",
    "see",
    "nets",
    "also",
    "title",
    "right",
    "says",
    "think",
    "says",
    "nets",
    "think",
    "look",
    "outperform",
    "models",
    "especially",
    "belief",
    "networks",
    "kind",
    "popular",
    "time",
    "see",
    "samples",
    "right",
    "way",
    "comparable",
    "examples",
    "get",
    "modern",
    "ganz",
    "already",
    "good",
    "especially",
    "emne",
    "stand",
    "could",
    "ask",
    "actually",
    "recognize",
    "yellow",
    "always",
    "training",
    "data",
    "set",
    "like",
    "nearest",
    "neighbors",
    "things",
    "left",
    "want",
    "show",
    "simply",
    "remember",
    "training",
    "data",
    "though",
    "sure",
    "like",
    "seems",
    "like",
    "surge",
    "somehow",
    "remember",
    "training",
    "data",
    "little",
    "bit",
    "also",
    "one",
    "right",
    "already",
    "way",
    "also",
    "farsighted",
    "see",
    "fully",
    "connected",
    "networks",
    "might",
    "one",
    "reasons",
    "worked",
    "moderately",
    "well",
    "right",
    "last",
    "one",
    "convolutional",
    "discriminator",
    "ad",
    "convolutional",
    "generator",
    "already",
    "using",
    "kind",
    "convolutions",
    "used",
    "everywhere",
    "today",
    "used",
    "ganz",
    "whatnot",
    "va",
    "upsample",
    "anything",
    "want",
    "pixel",
    "wise",
    "classification",
    "use",
    "convolutions",
    "paper",
    "sort",
    "introduced",
    "lot",
    "things",
    "later",
    "still",
    "use",
    "guns",
    "today",
    "sure",
    "convolutions",
    "invented",
    "know",
    "still",
    "still",
    "use",
    "legit",
    "first",
    "gaen",
    "paper",
    "use",
    "convolutions",
    "haha",
    "yeah",
    "also",
    "say",
    "make",
    "claim",
    "samples",
    "better",
    "samples",
    "generated",
    "existing",
    "methods",
    "believe",
    "samples",
    "least",
    "competitive",
    "better",
    "generative",
    "models",
    "literature",
    "highlight",
    "potential",
    "adversary",
    "framework",
    "today",
    "paper",
    "would",
    "rejected",
    "like",
    "hey",
    "wait",
    "better",
    "get",
    "ca",
    "claim",
    "ca",
    "claim",
    "anymore",
    "work",
    "anymore",
    "sorry",
    "always",
    "better",
    "everything",
    "else",
    "nowadays",
    "otherwise",
    "weak",
    "rejecter",
    "experimental",
    "evidence",
    "convince",
    "ca",
    "simply",
    "say",
    "something",
    "cool",
    "also",
    "already",
    "introduced",
    "paper",
    "digits",
    "obtained",
    "linearly",
    "interpolating",
    "coordinates",
    "z",
    "space",
    "full",
    "like",
    "thing",
    "every",
    "single",
    "gantt",
    "paper",
    "interpolations",
    "like",
    "ganz",
    "bike",
    "came",
    "came",
    "already",
    "like",
    "like",
    "every",
    "gam",
    "paper",
    "like",
    "rows",
    "like",
    "interpolations",
    "know",
    "written",
    "paper",
    "introduced",
    "right",
    "knows",
    "done",
    "yeah",
    "guess",
    "kind",
    "obvious",
    "thing",
    "still",
    "know",
    "cool",
    "see",
    "already",
    "done",
    "ganz",
    "compared",
    "different",
    "methods",
    "like",
    "deep",
    "direct",
    "graphical",
    "models",
    "generative",
    "compared",
    "many",
    "ways",
    "actually",
    "good",
    "reference",
    "want",
    "learn",
    "different",
    "kinds",
    "models",
    "make",
    "claim",
    "advantages",
    "disadvantages",
    "disadvantages",
    "mainly",
    "come",
    "training",
    "things",
    "train",
    "lockstep",
    "also",
    "disadvantage",
    "explicit",
    "representation",
    "explicit",
    "representation",
    "probability",
    "distribution",
    "never",
    "build",
    "data",
    "distribution",
    "sample",
    "however",
    "advantages",
    "markov",
    "chains",
    "never",
    "needed",
    "back",
    "prop",
    "used",
    "obtain",
    "gradients",
    "inference",
    "needed",
    "learning",
    "wide",
    "variety",
    "functions",
    "incorporated",
    "model",
    "know",
    "read",
    "paper",
    "laugh",
    "nowadays",
    "know",
    "people",
    "trying",
    "reintroduce",
    "like",
    "many",
    "papers",
    "like",
    "reintroducing",
    "markov",
    "chains",
    "gans",
    "like",
    "oh",
    "gans",
    "would",
    "much",
    "better",
    "mc",
    "mc",
    "sampler",
    "somewhere",
    "like",
    "point",
    "get",
    "rid",
    "like",
    "inference",
    "needed",
    "learning",
    "know",
    "models",
    "actually",
    "need",
    "inference",
    "training",
    "right",
    "costly",
    "many",
    "models",
    "nowadays",
    "like",
    "oh",
    "inference",
    "training",
    "yeah",
    "quite",
    "quite",
    "funny",
    "see",
    "people",
    "kind",
    "trying",
    "combine",
    "everything",
    "everything",
    "process",
    "sort",
    "reverse",
    "reverse",
    "whatever",
    "methods",
    "originally",
    "meant",
    "get",
    "rid",
    "saying",
    "anything",
    "methods",
    "kind",
    "funny",
    "yeah",
    "lot",
    "conclusions",
    "future",
    "work",
    "already",
    "say",
    "know",
    "conditional",
    "gans",
    "easy",
    "straightforward",
    "learned",
    "approximate",
    "inference",
    "performed",
    "training",
    "auxiliary",
    "network",
    "predict",
    "z",
    "given",
    "x",
    "course",
    "know",
    "come",
    "know",
    "come",
    "fruit",
    "often",
    "early",
    "papers",
    "already",
    "introduced",
    "g",
    "network",
    "producing",
    "producing",
    "x",
    "network",
    "discriminating",
    "would",
    "also",
    "like",
    "encoder",
    "right",
    "produce",
    "back",
    "z",
    "noise",
    "give",
    "latent",
    "encoding",
    "sort",
    "like",
    "variational",
    "encoder",
    "really",
    "like",
    "reverse",
    "generator",
    "know",
    "models",
    "nowadays",
    "big",
    "gann",
    "things",
    "like",
    "employ",
    "exact",
    "thing",
    "sort",
    "predicted",
    "right",
    "course",
    "much",
    "earlier",
    "models",
    "also",
    "using",
    "long",
    "remember",
    "people",
    "attempted",
    "bring",
    "encoders",
    "gans",
    "easy",
    "bunch",
    "things",
    "like",
    "learning",
    "use",
    "get",
    "data",
    "classifier",
    "also",
    "done",
    "lot",
    "things",
    "already",
    "foresight",
    "papers",
    "pretty",
    "cool",
    "coolest",
    "thing",
    "look",
    "savages",
    "goodfellow",
    "even",
    "using",
    "full",
    "eight",
    "pages",
    "dropping",
    "world",
    "absolutely",
    "cool",
    "mad",
    "respect",
    "yeah",
    "yeah",
    "kind",
    "take",
    "general",
    "yeah",
    "generative",
    "adversarial",
    "nets",
    "yeah",
    "please",
    "tell",
    "like",
    "historic",
    "paper",
    "overviews",
    "kind",
    "rant",
    "really",
    "paper",
    "explanation",
    "enjoy",
    "going",
    "papers",
    "kind",
    "looking",
    "hindsight",
    "right",
    "wish",
    "nice",
    "day",
    "bye",
    "bye"
  ],
  "keywords": [
    "today",
    "look",
    "generative",
    "adversarial",
    "nets",
    "good",
    "one",
    "another",
    "papers",
    "ganz",
    "nowadays",
    "back",
    "sort",
    "still",
    "remember",
    "new",
    "every",
    "paper",
    "also",
    "time",
    "quite",
    "well",
    "kind",
    "convincing",
    "know",
    "things",
    "already",
    "lot",
    "gans",
    "much",
    "course",
    "come",
    "like",
    "way",
    "since",
    "go",
    "yeah",
    "let",
    "think",
    "going",
    "really",
    "see",
    "right",
    "would",
    "two",
    "point",
    "important",
    "okay",
    "framework",
    "models",
    "train",
    "model",
    "g",
    "data",
    "distribution",
    "probability",
    "sample",
    "came",
    "training",
    "rather",
    "thing",
    "people",
    "first",
    "something",
    "work",
    "discriminator",
    "generator",
    "introduced",
    "basically",
    "fake",
    "supposed",
    "x",
    "want",
    "say",
    "image",
    "build",
    "classifiers",
    "using",
    "images",
    "networks",
    "high",
    "entire",
    "ca",
    "input",
    "output",
    "trained",
    "part",
    "always",
    "use",
    "directly",
    "order",
    "fact",
    "problem",
    "produce",
    "simply",
    "might",
    "everything",
    "maximize",
    "game",
    "convince",
    "theoretical",
    "space",
    "everywhere",
    "need",
    "inference",
    "samples",
    "methods",
    "function",
    "value",
    "could",
    "oh",
    "actually",
    "make",
    "log",
    "1",
    "minus",
    "generated",
    "prior",
    "noise",
    "many",
    "different",
    "neural",
    "network",
    "trying",
    "objective",
    "real",
    "minimize",
    "gradients",
    "max",
    "however",
    "billion",
    "practice",
    "gradient",
    "early",
    "learning",
    "markov",
    "step",
    "k",
    "steps",
    "inner",
    "z",
    "green",
    "blue",
    "curve",
    "optimize",
    "pretty",
    "cool",
    "optimum",
    "global",
    "likelihood",
    "achieved",
    "negative",
    "four",
    "convex",
    "converge",
    "reasonable",
    "used",
    "evaluate",
    "better",
    "get",
    "convolutions",
    "claim"
  ]
}