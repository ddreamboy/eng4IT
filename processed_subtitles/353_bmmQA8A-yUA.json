{
  "text": "this machine learning course is created\nfor beginners who are learning in 2024\nthe course begins with a machine\nlearning road map for 2024 emphasizing\ncareer paths and beginner-friendly\nTheory then the course moves on to\nHands-On practical applications and a\ncomprehensive end to-end project using\npython Todd have created this course she\nis an experienced data science\nprofessional her aim is to demystify\nmachine learning Concepts making them\naccessible and actionable for newcomers\nand to bridge the gap in existing\neducational resources setting you on a\npath to success in the evolving field of\nmachine learning looking to step into\nmachine learning or data science it's\nabout starting somewhere practical yet\npowerful in this introductory course\nmachine learning for beginners we are\ngoing to cover the basics of machine\nlearning and we're going to put that\ninto practice by implementing it in a\nreal world case study I'm d founder of\nLun Tech where we are making data\nscience and AI more accessible for\nindividuals and businesses if you're\nlooking for machine learning deep\nlearning data science or AI resources\nthen check out the free resources\nsection in\nlunch. or our YouTube channel where you\ncan find more content and you can dive\ninto machine learning and in AI we're\ngoing to start with machine learning\nroad map we in this detailed section we\nare going to discuss the exact skill set\nthat you need to get into machine\nlearning we're also going to cover the\ndefinition of machine learning what is a\ncommon career path and lot of resources\nthat you can use in order to get into\nmachine learning then we are going to\nstart with the actual Theory we are\ngoing to touch base the basics we're\ngoing to learn what are those different\nfundamentals in machine learning once we\nhave learned the theory and we have also\nlooked into the machine learning road\nmap we're going to put our Theory into\npractice we are going to conduct an\nendtoend a basic yet powerful case study\nwhere we're are going to implement the\nlinear aggression model we're going to\nuse it both for caal analysis and for\nPredictive Analytics for Californian\nhouse prices we're are going to find out\nthe features that drive the Californian\nhouse values and we are going to discuss\nthe stepbystep approach for conducting a\nreal world data science project at the\nend of this course you are going to to\nknow the exact machine learning road map\nfor\n2024 what are the exact skill set and\nthe action plan that you can use to get\ninto machine learning and in data\nscience you are going to learn the\nbasics when it comes to machine learning\nyou're going to implement it into actual\nmachine learning project end to end\nincluding implementing pandas numai\npsychic learn touch models medal tap and\ncurn in Python for a real world data\nscience project dive into machine\nlearning with us start Simple Start\nstrong let's get\nstarted hi there in this video we are\ngoing to talk about how you can get into\nmachine learning in\n2024 first we are going to start with\nall the skills that you need in order to\nget into machine learning step by step\nwhat are the topics that you need to\ncover and what are the topics that you\nneed to study in order to to get into\nmachine learning we are going to talk\nabout what is machine learning then we\nare going to cover step by step what are\nthe exact topics and the skills that you\nneed in order to become a machine\nlearning researcher or just get into\nmachine learning then we're going to\ncover the type of exact projects you can\ncomplete so examples of portfolio\nprojects in order to put it on your\nresume and to start to apply for machine\nlearning related jobs and then we are\ngoing to also talk about the type of\nindustries that you can get into once\nyou have all the skills and you want to\nget into machine learning so the exact\ncareer path and what kind of business\ntitles are usually related to machine\nlearning we are also going to talk about\nthe average salary that you can expect\nfor each of those different machine\nlearning related positions at the end of\nthis video you are going to know what\nexactly machine learning is where is it\nused what kind of skills are there that\nyou need in order to get into to machine\nlearning in 2024 and what kind of career\npath with what kind of compensation you\ncan expect with the corresponding\nbusiness titles when you want to start\nyour career in machine learning so we\nwill first start with the definition of\nmachine learning what machine learning\nis and what are the different sorts of\napplications of machine learning that\nyou most likely have heard of but you\ndidn't know that it was based on machine\nlearning so what is machine learning\nmachine learning is a brand of\nartificial intelligence of AI that helps\nto uh build models based on the data and\nthen learn from this data in order to\nmake different decisions and it's being\nused across different Industries uh\nstarting from healthare till\nentertainment in order to improve uh the\ncustomer uh experience custom identify\ncustomer behavior um improve the sales\nfor the businesses uh and it also helps\num governments to make decisions so it's\nreally has a wide range of applications\nso let's start with the healthcare for\ninstance machine learning is being used\nin the healthcare to help with the uh\ndiagnosis of diseases it can help to uh\ndiagnose cancer uh during the co it\nhelped many hospitals to identify\nwhether people are getting more uh\nsevere side effects or they are getting\np uh pneumonia um based on those\npictures and that was all based on\nmachine learning and specifically comp\ncomputer\nvision uh in the healthcare is also\nbeing used for drug Discovery it's being\nused for personalized medicine for\npersonalizing treatment plans to improve\nthe operations of the hospitals to\nunderstand what is the amount of uh\npeople and uh patients that hospital can\nexpect in each of those uh uh days per\nweek and also to estimate the amount of\ndoctors that need to be available the\namount of uh people uh that the hospital\ncan expect in the emergency room based\non the day or the time of the day and\nthis is basically not a machine learning\napplication then we have uh machine\nlearning in finance machine learning is\nbeing largely used in finance for\ndifferent applications starting from\nfraud detection in credit cards or in\nother sorts of banking operations um\nit's also being used in trading uh with\nspecifically in combination with\nquantitative Finance to help traders to\nmake decisions with they need to go\nshort or long into different stocks or\nbonds or different assets just in\ngeneral to estimate the price that those\ntalks will happen Assets in the real\ntime in the most accurate way uh it's\nalso being used in uh retail uh it helps\nyou understand an estimated demand for\ncertain products in certain warehouses\nit also helps you understand what is the\nmost appropriate or closest uh uh\nwarehouses that the items for that\ncorresponding customer should be shipped\nso it's uh optimizing the operations\nit's also being used to build different\ndirect Commander systems and search\nengines like the famous Amazon is doing\nso every time when you go to Amazon and\nyou are searching for project or product\nyou will most likely see many article\nrecommenders and that's based on machine\nlearning because Amazon is uh Gathering\nthe data and comparing your behavior So\nbased on what you have bought based on\nwhat you are searching uh to other\ncustomers and those items to other items\nin order to understand what are the\nitems that you will most likely will be\ninterested in and eventually will buy it\nand that's exactly based on machine\nlearning and specifically different\nsorts of recommended system\nalgorithm and then we have uh marketing\nwhere machine learning is being heavily\nused because this can help to understand\nuh what are these different tactics and\nspecific targeting uh groups that that\nyou belong and how retailers can Target\nyou uh in order to reduce their\nmarketing cost and to result in high\nconversion rates so to ensure that you\nbuy their product then we have machine\nlearning in autonomous vehicles that's\nbased on machine learning and\nspecifically uh deep learning\napplications uh and then we have also um\nuh natural language Pro processing which\nis highly related to the famous Chad GPT\nI'm sure you are using it and that's\nthat's based on the machine learning and\nspecifically the large language models\nso the Transformers large language\nmodels where you are going and providing\nyour text and then question and the chat\nGPT will provide answer to you or in\nfact any other uh virtual assistant or\nchat boats those are all based on\nmachine\nlearning and then we have also uh smart\nhome devices so Alexa is based on\nmachine learning also in agriculture uh\nmachine learning is being used heavily\nthese days to estimate what the weather\nconditions will be uh to understand what\nwill be the uh production of different\nplants uh what will be the um outcome of\nthis uh to understand and to make\ndecisions uh also how they can optimize\nthose uh crop uh yields to monitor uh\nsoil health and for different sorts of\napplications that can just in general uh\nimprove the uh revenue for the farmer\nthen we have of course in the\nentertainment so the Vivid example is\nNetflix that uses the uh data uh that\nyou are providing uh related to the\nmovies and also based on what kind of\nmovies you are watching Netflix is uh\nbuilding this super smart recommender\nsystem to recommend you movies that you\nmost likely will be interested in and\nyou will also like it so in all this\nmachine learning is being used and it's\nactually super powerful topic and super\npowerful uh field to get into and in the\nupcoming 10 years this is only going to\ngrow so if you have made that decision\nor you are about to make that decision\nto get into machine learning continue\nwatching this video because I'm going to\ntell you exactly what kind of skills you\nneed and what kind of uh practice\nprojects you can complete in order to\nget into machine learning in\n2024 so you first need to start with\nmathematics you Al also need to know\npython you also need to know statistics\nyou will need to know machine learning\nand you will need to know some NLP to\nget into machine learning so let's now\nunpack each of those skill sets so\nindependent the type of machine learning\nyou are going to do you need to know\nmathematics and specifically you need to\nknow linear algebra so you need to know\nwhat is matrix multiplication what are\nthe vectors matrices dot product you\nneed to know how you can uh multiply\nthose different matrices Matrix with\nvectors what are these different rules\nthe dimensions also what does it mean to\ntransform a matrix the inverse of the\nMatrix identity Matrix diagonal matrix\nuh those are all Concepts as part of\nlinear algebra that you need to know as\npart of your mathematical skill set in\norder to understand those different\nmachine learning\nalgorithms then as part of your\nmathematics you also need to to know\ncalculus and specifically differential\nTheory so you need to know these\ndifferent theorems such as chain rule\nthe rule of uh differentiating when you\nhave sum of instances when you have\nconstant multiply with an instance when\nyou have um uh sum but also subtraction\ndivision multiplication of two items and\nthen you need to take the uh derivative\nof that what is this idea of derivative\nwhat is the idea of partial derivative\nwhat is the idea of Haitian so first\norder derivative second order derivative\nand it would be also great to know a\nbasic integration Theory so we have\ndifferentiation and the opposite of it\nis integration Theory so this is kind of\nbasic you don't need to know uh too much\nwhen it comes to calculus but those are\nbasic things that you need to know uh in\norder to succeed in machine learning uh\nthen the next Concepts uh such as\ndiscrete mathematics so you need to know\nuh what is this idea of uh graph Theory\nuh what are this uh combinations\ncombinators uh what is uh this idea of\ncomplexity which is important when you\nwant to become a machine learning\nengineer because you need to understand\nwhat is this Big O notation so you need\nto understand what is this complexity of\nuh n s complexity of n complexity of n\nlog n um and about that you need to know\nuh some basic um mathematics when it\ncomes which comes from usually high\nschool so you need to know\nmultiplication division you need to\nunderstand uh multiplying uh uh amounts\nwhich are within the parentheses you\nneed to understand um different symbols\nthat represent mathematical um values\nyou need to know this idea of using X's\ny's uh and then what is X2 what is y^ 2\nWhat is X to ^ 3 so different exponents\nof the different VAR variables then you\nneed to know what is logarithm what is\nlogarithm at the base of two what is\nlogarithm at the base of e and then at\nthe base of 10 uh what is the idea of e\nso what is the idea of Pi uh what is\nthis idea of uh exponent logarithm and\nhow does those uh transform when it\ncomes to taking derivative of the\nlogarithm taking the derivative of the\nuh exponent those are all values and all\nuh topics that are actually quite basic\nthey might sound complicated but they\nare actually not so if someone explains\nyou uh clearly then you will definitely\nunderstand it from the first goal and uh\nfor this uh to understand all those\ndifferent mathematical Concepts so\nlinear algebra calculus differential\nTheory and then discrete mathematics and\nthose different symbols you need to uh\ngo for instance uh and look for courses\nor um YouTube tutorials that are about\nuh basic mathematics uh for machine\nlearning and AI uh don't go and look\nfurther you can check for instance Can\nAcademy which is uh quite favorite when\nit comes to learning math uh both for\nuni students and also for just people\nwho want to learn mathematics and this\nwill be your guide um or you can check\nour resources at Lear tech. cuz we are\ngoing also to uh provide this resources\nfor you uh in case you want to learn\nmathematics for your machine Learning\nJourney the next skill set that you need\nto gain in order to break into machine\nlearning is the statistics so you need\nto know this is a must statistics if you\nwant to get into machine learning and in\nAI in general so there are few topics\nthat you must um study when it comes to\nstatistics and uh those are descriptive\nstatistics multivariate statistics\ninferential statistics probability\ndistribution and some bial\nthinking so let's start with descriptive\nstatistics when it comes to descriptive\nstatistics you need to know what is side\nof mean uh median standard deviation\nvariance and uh just in general how you\ncan uh analyze the data with using this\ndescriptive measures so distance\nmeasures but also variational measures\nthen the next topic area that you need\nto know as part of your statistical\nJourney is the inferential statistics so\nyou need to know those INF famous\ntheories such as Central limit theorem\nthe law of a large numbers uh and how\nyou can um relate to this idea of\npopulation sample unbiased sample and\nalso uh a hypothesis testing confidence\ninterval statistical significance uh and\nuh how you can test different theories\nby using uh this idea of statistical\nsignificance uh what what is the power\nof the test what is type one error what\nis type two error so uh this is super\nimportant for understanding different\nSES of machine learning applications if\nyou want to get into machine learning\nthen you have probability distributions\nand this idea of probabilities so to\nunderstand those different machine\nlearning Concepts you need to know what\nare probabilities so what is this idea\nof probability what is this idea of\nSample versus\npopulation uh what is what does it mean\nto estimate probability what are those\ndifferent rules of probability so\nconditional\nprobability uh and um those uh\nprobability uh values and rules that\nusually you can uh apply when you have\nuh probability of um multipliers\nprobability of two sums um and then uh\nyou need to understand some uh popular\nand you need to know some popular\nprobability distribution function\nand those are perno distribution\nbinomial distribution uh normal\ndistribution uniform distribution\nexponential distribution so those are\nall super important distributions that\nyou need to know in order to understand\nuh this idea of normality\nnormalization uh also uh this idea of\nbare noly trials and uh relating uh\ndifferent probability distributions to\ndifferent uh uh higher level statistical\nconcept steps so rolling a dice the\nprobability of it how it is related to\nbero distribution or to binomial\ndistribution and those are super\nimportant when it comes to hypothesis\ntesting but also for uh many other\nmachine learning\napplications so then we have the ban\nthinking this is super important when it\ncomes to more advanced machine learning\nbut also some basic machine learning you\nneed to know what is the Bas theorem\nwhich arguably is one of the most\npopular statistical theorems out there\ncomparable also to the central limit\ntheorem you need to know what is\nconditional probability what is this\nbias theorem and how does it relate to\nconditional probability uh what is this\nuh bation uh statistics Ide at very high\nlevel you don't need to know everything\nin uh super detailed but you need to\nknow um the these Concepts at least at\nhigh level in order to understand\nmachine learning so to learn statistics\nand fundamental concepts of Statistics\nyou can check out the fundamentals to\nstatistics course at\nlunch. here you can learn all this\nrequired Concepts and topics and you can\npractice it in order to get into machine\nlearning and to gain the statistical\nskills the next skill set that you must\nknow is the fundamentals to machine\nlearning so this covers not only the\nbasics of machine learning but also the\nmost popular machine learning algorithms\nso you need to know this uh different um\nmathematical side of these algorithms\nstep by step how they work what are the\nbenefits of them what are the demores\nand and which one to use for what type\nof applications so you need to know this\nuh categorization of supervised versus\nunsupervised versus\nsemi-supervised then you need to know\nwhat is this idea of classification\nregression or uh clustering then you\nneed to know uh also time series\nanalysis uh you also need to know uh\nthese different popular algorithms\nincluding linear\nregression also logistic regression LDA\nso linear discriminant analysis you need\nto know KNN you uh need to know uh\ndecision treats both classification and\nregression case you need to know uh\nrandom Forest begging but also boosting\nso popular boosting algorithms like uh\nlight GBM GBM uh so gradient boosting\nmodels and you need to know uh HG boost\nuh you uh also need to know um some\nsupervised learning algorithm such as K\nmeans uh usually Ed for class string you\nneed to know DB scan which becomes more\nand more popular in uh class string\nalgorithms you also need to know\nhierarchal class string um and um for\nall this type of uh models you need to\nunderstand the idea behind them what are\nthe advantages and disadvantages whether\nthey can be applied for unsupervised\nversus supervised versus semi-supervised\nyou need to know whether they are for\nregression classification or for uh\nclass stre beside of this popular\nalgorithms and models you also need to\nknow the basics of uh training a machine\nlearning model so you need to know uh\nthis process behind training validating\nand testing your machine learning\nalgorithms so you need to know uh what\ndoes it mean to uh perform\nhyperparameter tuning\nwhat are those different optimization\nalgorithms that can be used to optimize\nyour parameters such as uh GD SGD SGD\nwith momentum Adam and Adam V you also\nneed to know the testing process this\nidea of splitting the data into train\nvalidation and then test you need to\nknow resampling techniques why are they\nused including the um bootstrapping and\nuh cross viation and there's different\nsorts of cross viation techniques such\nas one out cross validation kful cross\nvalidation validation set approach uh\nyou also need to know um this uh idea of\nuh Matrix and how you can use different\nMatrix to evaluate your machine learning\nmodels such as uh classification type of\nmetrics like F1 score\nFB Precision recall um cross entropy um\nand also you need to know some Matrix\nthat can be used to evaluate regression\ntype of problems like the uh me squared\nerror so MC root me squared error R MC\nuh MAA so the absolute uh version of\nthose different sorts of Errors um and\num or the residual sum of squares for\nall these cases you not only need to\nknow higher level what the those\nalgorithms or those uh topics or\nconcepts are doing but you actually need\nto know the uh mathematics behind it\ntheir benefits the uh disadvantage ages\nbecause during the interviews you can\ndefinitely expect questions that will\ntest uh not only your high level\nunderstanding but also this uh\nbackground knowledge if you want to\nlearn machine learning and you want to\ngain those skills then uh feel free to\ncheck out my uh fundamentals to machine\nlearning course at\nlunch. or you can also check out and\ndownload for free the fundamentals to\nmachine learning handbook that I\npublished with free cord Camp then the\nnext skill set that you definitely need\nto gain is a knowledge in python python\nis actually one of the most popular\nprogramming languages out there and it's\nbeing used across software Engineers uh\nAI Engineers machine learning Engineers\ndata scientists so this this is the\nuniversal language I would say when it\ncomes to programming so if you're\nconsidering getting into machine\nlearning in 2024 then python will be\nyour friend so knowing the theory is one\nthing then uh implementing it uh in in\nthe actual job is another and that's\nexactly where python comes in handy so\nyou need to know python in order to\nperform uh descriptive statistics in\norder to trade machine learning model or\nmore advanced machine learning models so\ndeep learning models you can use for\ntraining validation and uh for testing\nof your models and uh also for building\ndifferent sorts of applications so\npython is super powerful therefore it's\nalso gaining such a high uh popularity\nacross the globe because it has so many\nuh libraries it has uh taner flow pie\ntorch both that uh are must if you want\nto not only get into machine learning\nbut also the advanced uh levels of\nmachine learning so if you are\nconsidering the AI engineering jobs or\nmachine learning engineering jobs and uh\nyou want to train for instance deep\nlearning models uh or you want to build\nlarge l W models or generative AI models\nthen you definitely need to learn uh\npytorch and tens flow which are\nFrameworks that I use in order to uh\nImplement different deep learning uh\nwhich are Advanced machine learning\nmodels here are few libraries that you\nneed to know in order to uh get into\nmachine learning so you definitely need\nto know pandas napai you need to know\npsyit learn scipi you also need to know\nuh nltk for the TX data you also need to\nknow tensor flow and Pythor for bit more\nadvanced machine learning and um beside\nthis there are also data visualization\nlibraries that I would definitely\nsuggest you to practice with which are\nthe Met plot lip and specifically the\nPIP plot and also the\ncurn when it comes to python beside\nknowing how to use libraries you also\nneed to know some basic data structures\nso you need to know what are these\nvariables how you can create variables\nwhat what are the matrices arrays how\nthe indexing works and also uh what are\nthe lists what are the sets so unique\nlists uh What uh are the ways that you\ncan what are the different operations\nyou can perform uh how does the Sorting\nfor instance work I would definitely\nsuggest you know um some basic data\nstructures and algorithms such as binary\nsort so in optimal way to sort your\narrays you also need to know uh the data\nprocessing in Python so you need to\nunderstand how to identify missing data\nhow to uh identify uh duplicating your\ndata how to clean this how to perform\nfeature engineering so how to combine uh\nmultiple variables or to perform\noperations to create new\nvariables um you also need to know uh\nhow you can aggregate your data how you\ncan filter your data how you can sort\nyour data and of course you also need to\nknow how you can form AB testing in your\nPython and how you can train machine\nlearning models how you can test it and\nhow you can evaluate them and also\nvisualize the performance of it if you\nwant to Learn Python then the easiest\nthing you can do is just to Google for\nuh python for data science or python for\nmachine learning tutorials or blogs or\nyou can even try out the python for data\nscience course at Learner tech. in order\nto learn all these Basics and usage of\nthese libraries and some practical\nexamples when it comes to python for\nmachine learning the next skill set that\nyou need to gain in order to get into\nmachine learning is the basic\nintroduction to NLP natural language\nprocessing so you need to know how to\nwork with text Data given that these\ndays the text data is the Cornerstone of\nall these different Advanced algorithms\nsuch as uh gpts Transformers the\nattention mechanisms so those uh\napplications that you see as part of\nbuilding chat boat or this uh p I uh\napplications based on Tex data they are\nall based on NLP so therefore you need\nto know this basics of NLP to just get\nstarted with machine learning so you\nneed to know uh this idea of text Data\nwhat are those strings uh how you can\nclean Text data so how you can clean uh\nthose um dirty data that you get and\nwhat are the steps involved such as\nlower casing uh removing punctuation\ntokenization uh also what is this idea\nof stemming lemmatization stop wordss\nhow you can use the nltk in Pyon in\norder to perform this cleaning you also\nneed to know uh this idea of embeddings\nand uh you can also learn this idea of\nuh the uh tfidf which is a basic uh NLP\nalgorithm uh you also uh can learn this\nidea of word and Bings uh the sub word\nembeddings uh and the character\nembeddings if you want to learn the\nbasics of NLP you can check out those\nConcepts and learn them as part of the\nblogs there are many tutorials on\nYouTube you can also try the\nintroduction to uh NLP course at lunch.\nin order to learn this uh different\nBasics that form the NLP if you want to\ngo beyond this uh intro till medium\nlevel machine learning and you also want\nto learn more advanced machine learning\nand this is something that you need to\nknow after you have gained all these\npreview skills that I mentioned then you\ncan gain uh this uh knowledge and the\nskill set by learning deep learning and\nalso uh you can consider uh getting into\ngenerative AI topics so you can for\ninstance learn what are the rnns what\nare the Ann what are the CNN you can\nlearn what is this uh out encoder\nconcept what are the variational outen\ncoders what what are the uh generative\nadversarial networks so gens uh you can\nunderstand what is this idea of\nreconstruction error uh you can\nunderstand this um these different sorts\nof neural networks what is this idea of\nback propagation the optimization of\nthese algorithms by using the different\noptimization algorithms such as GD HGD\num HGD momentum Adam adamw RMS prop uh\nyou uh can also go One Step Beyond and\nyou can uh get into gener AI topics such\nas um uh the uh variational Auto\nencoders like I just mentioned but also\nthe large language models so if you want\nto move towards the NLP side of\ngenerative Ai and you want to know how\nthe ched GPT has been invented how the\ngpts work or the birth model uh then you\nwill definitely need to uh get into this\ntopic of language model so what are the\nend grams what is the attention\nmechanism what is the difference between\nthe self attention and attention what is\nuh one head self attention mechanism\nwhat is multi-ad self attention\nmechanism you also need to know at high\nlevel this uh encoder decoder\narchitecture of Transformers so you need\nto know the architecture of Transformers\nand how they solve different problems of\nuh reur neuron networks or RNN and\nlstms uh you can also look into uh this\nuh uh encoder based or decoder based\nalgorithm such as\nuh gpts or Birch model and those all\nwill help you to not only get into\nmachine learning but also stand out from\nall the other candidates by having this\nAdvanced knowledge let's now talk about\ndifferent sorts of projects that you can\ncomplete in order to train your machine\nlearning skill set that you just learned\nuh so there are few projects that I\nsuggest you to complete and you can put\nit this on your resume to start to apply\nfor machine learning roles the first\napplication the project that I would\nsuggest you to do is building a basic\nrecommender system whether it's a job\nrecommender system or a movie\nrecommender system in this way you can\nshowcase how you can use for instance\ntext Data from those job advertisement\nhow you can use numeric data such as the\nratings of the movies in order to build\na topend recommender system this will\nshowcase your understanding of the\ndistance measures such as cosign\nsimilarity this Cann algorithm idea and\nthis will help you to uh uh tackle this\nspecific uh area of data science and\nmachine learning the next project I\nwould suggest you to do will be to build\na regression based model so in this way\nyou will showcase that you understand\nthis idea of regression how to work with\na Predictive Analytics and predictive\nmodel that has a dependent variable\nresponse variable that is in the numeric\nformat so here for instance you can uh\nestimate the salaries of the jobs based\non the uh characteristics of the uh job\nbased on this data which you can get for\ninstance from uh open source uh web\npages such as keegle and you can then uh\nuse different sorts of regression\nalgorithms to perform your predictions\nof the salaries evaluate the model and\nthen compare the uh performance of the\ndifferent machine learning regression\nbased algorithms for instance you can\nuse the uh linear regression you can use\nthe decision trees regression version\nyou can use the um uh random Forest you\ncan use uh GBM xgo in order to Showcase\nand then in one uh graph to compare this\nuh performance of these different\nalgorithms by using single regression uh\nml modal metrics so for instance the\nrmsc this project will showcase that you\nunderstand how you can train a\nregression model how you can test it and\nvalidate it and it will showcase your\nunderstanding of optimization of this\nregression algorithm you understand this\nconcept of hyperparameter unit the next\nproject that I would suggest you to do\nin order to Showcase your classification\nknowledge so when it comes to uh\npredicting a class for an observation\ngiven uh the feature space would be uh\nto uh build a classification model that\nwould classify emails being a Spam or\nnot a Spam so you can use a publicly\navailable data that will be uh\ndescribing a specific email and then you\nwill have multiple emails and the idea\nis to uh build a machine learning model\nthat would classify the email to the\nclass zero and class one where class\nzero for instance can be your uh not\nbeing a Spam and one being a Spam so\nwith this binary classification you will\nshowcase that you know how to train a\nmachine learning model for\nclassification purposes and you can here\nuse for instance logistic regression you\ncan use also the decision Trea for\nclassification case you can also use\nrandom Forest the uh EG she Bo for\nclassification GBM for classification\nand uh with all these models you can\nthen obtain the performance metrics such\nas uh F1 score or you can put the rck\ncurve uh or the uh area under the Curve\nmetrics and you can also compare those\ndifferent classification models so in\nthis way you will also tackle another\narea of expertise when it comes to the\nmachine\nlearning then a final project that I\nwould suggest you to do would be uh from\nthe unsupervised learning to Showcase\nanother area of expertise and here you\ncan for instance use data to your\ncustomers into good better and best\ncustomers based on their transaction\nhistory the amount of uh money that they\nare spending in the store so uh in this\ncase you can for instance use K means uh\nDB scan hierarchy clustering and then\nyou can evaluate your uh clustering\nalgorithms and then select the one that\nperforms the best so you will then in\nthis case cover yet another area of\nmachine learning which would be super\nimportant to show case that you can not\nonly handle recommended systems or\nsupervised learning but also\nunsupervised learning and the reason why\nI suggest you to uh cover all these\ndifferent areas and complete this four\ndifferent projects is because in this\nway you will be covering different\nexpertise and areas of machine learning\nso you will be also putting projects on\nyour uh resume that are covering\ndifferent sorts of algorithms different\nsorts of uh Matrix and approaches and it\nwill show case that you actually know a\nlot from machine\nlearning now if you want to go beyond\nthe basic or medium level and you want\nto be considered for medium or Advanced\nmachine learning uh levels and positions\nyou also need to know bit more advanced\nwhich means that you need to complete\nbit more advanced projects for instance\nif you want to apply for generative AI\nrelated or large language models related\npositions I would suggest you to\ncomplete a project where you are\nbuilding a very basic uh large language\nmodel and specifically the pre-training\nprocess which is the most difficult one\nso in this case uh for instance you can\nbuild a baby GPT and I'll put a here\nlink that you can follow where I'm\nbuilding a baby GPT a basic pre-trained\nGPT algorithm where uh I am using a text\nData uh publicly available data in order\nto uh uh process data in the same way\nlike GPT is doing and the encoded part\nof the Transformer\nin this way you will showcase to your um\nhiring managers that you understand this\narchitecture behind Transformers\narchitecture behind the um uh large\nlanguage models and the gpts and you\nunderstand how you can use pytorch in\nPython in order to do this Advanced NLP\nand generative AI task and finally let's\nnow talk about the common career path\nand the business titles that you can\nexpect from a career in machine learning\nso assuming that you have gained all the\nskills uh that are must for breaking\ninto machine learning there are\ndifferent sorts of business titles that\nyou can apply in order to get into\nmachine learning so when it comes to\nmachine learning uh you can uh get into\nmachine learning uh and there are\ndifferent fields that are covered as\npart of this so uh first we have the\ngeneral machine learning researcher\nmachine learning researcher is basically\ndoing a research so training testing\nevaluating different machine learning\nalgorithms they are usually people who\ncome from academic background but it\ndoesn't mean that you cannot get into\nmachine learning research without\ngetting a degree in statistics\nmathematics or in um um machine learning\nspecifically not at all so uh if you\nhave this um desire and this passion for\nreading doing research uh and you don't\nmind reading uh research papers then\nmachine learning res researcher job\nwould be a good fit for you so machine\nlearning combined with research then\nsets you uh for the machine learning\nresearcher role then we have the machine\nlearning engineer so machine learning\nengineer is the engineering version of\nthe machine learning uh expertise which\nmeans that we are combining machine\nlearning skills with the engineering\nskills such as\nproductionizing pipelines or end to end\nrobust pipeline scalability of the m\nmodel considering all these different\naspects of the model not only from the\nperformance side when it comes to the\nquality of the algorithm but also the uh\nscalability of it and when putting it in\nfront of many users so when it comes to\ncombining engineering with machine\nlearning then you get machine learning\nengineering so if you are someone who is\na software engineer and you want to get\ninto machine learning then machine\nlearning engineering would be the best\nfit for you so so for machine learning\nengineering you not only need to have\nall these different skills that I\nalready mentioned but you also need to\nhave this good grasp of uh uh\nscalability of algorithms the uh uh data\nstructures and algorithms type of um\nskill set uh the uh complexity of the\nmoral uh also system design so this one\nuh converges more towards and similar to\nthe software engineering position\ncombined with machine learning red than\nyour pure machine learning or AI role\nthen we have the AI research versus AI\nengineering position so uh the uh AI\nresearch position is similar to The\nMachine learning uh research position\nand the AI engineer position is similar\nto The Machine learning engineer\nposition with only single difference\nwhen it comes to machine learning we are\nspecifically talking about the\ntraditional machine learning so linear\nregression logistic regression and also\nuh random Forest exy boost begging and\nwhen it comes to AI research and AI\nengineer position here we are tackling\nmore the advanced machine learning so\nhere we are talking about deep learning\nmodels such as RNN lstms grus CNN or\ncomputer vision applications and we are\nalso talking about uh generative AI\nmodels large language models so uh we\nare talking about um the Transformers\nimplementation of Transformers the gbts\nT5 all these different algorithms that\nare from uh more advanced uh AI topics\nrather than traditional machine learning\nuh for those you will then be applying\nfor AI research and AI engineering\npositions and finally you have these\ndifferent sorts of obervations niches\nfrom AI for instance NLP research NLP\nengineer or even data science positions\nfor which you will need to know machine\nlearning and knowing machine learning\nwill set you apart for the source of\npositions so also the business titles\nsuch as data science or technical data\nscience positions NLP researcher NLP\nengineer for this all uh you will need\nto know machine learning and knowing\nmachine learning will help you to break\ninto those positions and those career\npaths if you want to prepare for your\ndeep learning interviews for instance\nand you want to get into AI engineering\nor AI research then I have recently\npublished for free a full course with\n100 interview questions with answers for\na span of 7.5 hours that will help you\nto prepare for your deep learning\ninterviews and for your machine learning\ninterviews you can check out my uh\nfundamentals to machine learning course\nat lunch. or uh you can download the\nmachine learning fundamentals handbook\nfrom free Cod camp and check out my\nblogs and also free resources at lunch.\nAI in order to prepare for your\ninterviews and in order to get into\nmachine learning let's not talk talk\nabout the list of resources that you can\nuse in order to get into machine\nlearning in\n2024 so to learn statistics and the\nfundamental concepts of Statistics you\ncan check out the fundamental statistics\ncourse at\nlunch. here you can learn all this\nrequired Concepts and topics and you can\npractice it in order to get into machine\nlearning and to gain this statistical\nskills then when you want to learn\nmachine learning you can check the\nfundamentals to a learning course at\nlunch. to get all these basic concepts\nthe fundamentals to machine learning and\nthe list of comprehensive and the most\ncomprehensive list of machine learning\nalgorithms out there as part of this\ncourse then you can also check out the\nintroduction to NLP course at the lunch.\na in order to learn the basic concepts\nbehind natural language\npreprocessing and finally if you want to\nLearn Python and specifically python for\nRal learning you can check out the\npython for data science course at\nlunch. and if you want to get access to\nthis different projects that you can\npractice your machine learning skills\nthat you just learned you can either\ncheck out the ultimate data science boot\ncamp that covers a specific course the\nuh data science uh project portfolio\ncourse covering multiple of these\nprojects that you can train your machine\nlearning skills and put on your resume\nor you can also check my GitHub account\nor my LinkedIn account where I cover\nmany case studies including the baby GPT\nand I will also put the link to this\ncourse and to this uh case study in the\nlink below and once you have gained all\nthe skills you are ready to get into\nmachine learning in\n2024 in this lecture we will go through\nthe basic concepts in machine learning\nthat is needed to understand and follow\nconversations and solve main problems\nusing machine learning strong\nunderstanding of machine learning Basics\nis an important step for anyone looking\nto learn more about or work with machine\nlearning we'll be looking at the three\nconcepts in this tutorial we will Define\nand look into the difference between\nsupervised and unsupervised machine\nlearning models then we will look into\nthe difference between the regression\nand classification type of machine\nlearning models after this we will look\ninto the process of training machine\nlearning models from scratch and how to\nevaluate them by introducing performance\nmetrics what you can use depending on\nthe type of machine learning model or\nproblem you are dealing with so whether\nit's a supervised or unsupervised\nwhether it's regression versus\nclassification type of\nproblem machine learning methods are\ncategorized into two types depending on\nthe existence of the label data in the\ntraining data set which is especially\nimportant in the training process so we\nare talking about the So-Cal dependent\nvariable that we so in the section of\nfundamental Su statistics supervised and\nunsupervised machine learning models are\ntwo main type of machine learning\nalgorithms one key difference between\nthe two is the level of supervision\nduring the training phase supervised\nmachine learning algorithms are Guided\nby the labeled examples while as\nsupervised algorithms are not as\nlearning model is more reliable but it\nalso requires a larger amount of labeled\ndata which can be timec consuming and\nquite expensive to\nobtain\nexamples of supervised machine learning\nmodels include regression and\nclassification type of\nmodels on the other hand unsupervised\nmachine learning algorithms are trained\non unlabeled data the model must find\npatterns and relationships in the data\nwithout the guidance of correct outputs\nso we no longer have a dependent\nvariable so unsupervised ml models\nrequire training data that consists only\nof independent variables or the features\nand there is no dependent variable or\nlabel data that can supervise the\nalgorithm when learning from the\ndata examples of unsupervised models are\nclust string models and outlier\ndetection\ntechniques supervised machine learning\nmethods are categorized into two types\ndepending on the type of dependent\nvariable they are predicting so we have\nregression type and we have\nclassification type some key differences\nbetween regression and classification\ninclude output type so the regression\nalgorithms predict continuous values\nwhile the classification algorithms\npredict categorized values some key\ndifference between regression and\nclassification include the output type\nthe evaluation metrics and their\napplications so with regards to the\noutput type regression algorithms\npredict continuous values while\nclassification algorithms predict\ncategorical\nvalues with regard to the evaluation\nmetric different evaluation metrics are\nbeing used for regression and\nclassification tasks for example mean\nsquare is commonly used to evaluate\nregression models while accuracy is\ncommonly used to evaluate classification\nmodels when it comes to Applications\nregression and classification models are\nused in entirely different types of\napplications regression models are often\nused for prediction tests while\nclassifications are used for decision\nmaking tasks progression algorithms are\nused to predict the continuous value\nsuch as price or probability for example\na regression model might be used to\npredict the price of a house based on\nits size location or other\nfeatures examples of regression type of\nmachine learning models are linear\nregression fixed effect regression exus\nregression\nEtc classification algorithms on the\nother hand are used to predict the\ncategorical value these algorithms take\nan input and classify it to one of the\nseveral predetermined categories for\nexample a classification model might be\nused to classify emails as a Spam or as\nnot a Spam or to identify the type of\nanimal in an\nimage examples of classification type of\nmachine learning models are logistic\nregression exus classification random\nForest\nclassification let us now look into\ndifferent typee of performance metrics\nwe can use in order to evaluate\ndifferent type of machine learning\nmodels for aggression models common\nevaluation Matrix includes residual sum\nof squared which is the RSS mean squared\nerror which is the msse the root mean\nsquared error or rmsc and the mean\nabsolute error which is the m AE this\nmetrix measure the difference between\nthe predicted values and the True Values\nwith a lower value indicating a better\nfeed for the model so let's go through\nthis metrics one by one the first one is\nthe RSS or the residual sum of squares\nthis is a matrix commonly used in the\nsetting of linear regression when we are\nevaluating the performance of the model\nin estimating the different coefficients\nand here the beta is a coefficient and\nthe Yi is our dependent variable value\nand the Y head is the predicted value as\nyou can see the RSS or the residual sum\nof square or the beta is equal to sum of\nall the squ of Y IUS y hat across all I\nis equal to 1 n where I is the index of\nthe each r or the individual or the\nobservation included in the data the\nsecond Matrix is the m or the mean\nsquared error which is the average of\nthe squared differences between the\npredicted values and the True Values so\nas you can see m is equal to 1 / to n\nand then sum across all i y i minus y\nhead squ as you can see the RSS and the\nmsse are quite similar in terms of their\nuh formulas the only difference is that\nwe are adding a 1 / to n and then this\nmakes it the average across all the\nsquare differences between the predicted\nvalue and the actual true valum a lower\nvalue of msse indicates a better fit the\nrmsc which is the root mean squared\nerror is the square root of the msse so\nas you can see it has the same formula\nas msse only with the difference that we\nare adding a square roof on the top of\nthat formula a lower value of rmsc\nindicates a better fit and finally the\nMae or the mean absolute error is the\naverage absolute difference between the\npredicted values so the Y hat and the\nTrue Values or y i a lower value of this\nindicates a better\nfit the choice of a regression metrics\ndepends on the specific problem you are\ntrying to solve and the nature of your\ndata for instance the MSE is commonly\nused when you want to penalize large\nerrors more than the small ones MSE is\nsensitive to outliers which means that\nit may not be the best choice when your\ndata contains many outliers or extreme\nvalues rmsc on the other hand which is\nthe square root of the MSC makes it\neasier to interpret so it's easier\ninterpretable because it's in the same\nunits as Target variable it is commonly\nused when you want to compare the\nperformance of different models or when\nyou want to report the error in a way\nthat it's easier to understand and to\nexplain the Mia is commonly used when\nyou want to penalize all errors equally\nregardless of their magnitude and Mia is\nless sensitive to outliers compared to\nmsse for classification models common\nevaluation metrics include accuracy\nprecision recall and F1 score this\nmetrics measure the ability of the\nmachine learning model to correctly\nclassify instances into the correct\ncategories let's briefly look into this\nmetrix individually so the accuracy is a\nproportion of correct predictions made\nby the model it's calculated by taking\nthe correct predictions so the correct\nnumber of predictions and divide two all\nnumber of predictions which means\ncorrect predictions plus incorrect\npredictions next we will look into the\nPrecision so Precision is the proportion\nof true positive predictions among all\npositive predictions made by the model\nand it's equal to True positive divided\nto True positive plus false positive so\nall number of positives true positives\nare cases where the model correctly\npredict a positive outcome while false\npositives are the cases where the model\nincorrectly predict a positive outcome\nnext Matrix is recall recall is a\nproportion of true positive predictions\namong all actual positive instances it's\ncalculated as the number of true\npositive predictions divided by the\ntotal number of actual positive\ninstances which means dividing the true\npositive to True positive plus false\nnegative so for example let's say we are\nlooking into medical test a true\npositive would be a case where it has\ncorrectly identifies a patient as having\na disease while a false positive would\nbe a case where the test incorrectly\nidentifies a healthy patient as having\nthe\ndisease and the final score is the F1\nscore the F1 score is the harmonic mean\nor the usual mean of the Precision and\nrecall with a higher value indicating a\nbetter balance between precision and\nrecall and it's calculated as the\ntwo times recall times Precision divided\nto recall plus\nPrecision for unsupervised models such\nas class string models whose performance\nis typically evaluated using metrics\nthat measure the similarity of the data\npoints within a cluster and the dis\nsimilarity of the data points between\ndifferent clusters we have three type of\nmetrics that we can use homogeneity is a\nmeasure of the degree to which all of\nthe data points within a single cluster\nbelong to the same class A Higher value\nindicates a more homogeneous cluster so\nas you can see homogeneity of age where\nage is the simply the short way of\ndescribing homogeneity is equal to one\nminus conditional entropy given cluster\nassignments divided to the entropy or\npredicted class if you wondering what\nthis entropy is then stay tuned as we\nare going to discuss this entropy\nwhenever we will discuss the clustering\nas well as decision trees X Matrix is\nthe silid score silid score is a measure\nof the similarity of the data point to\nits own cluster compared to the other\nclusters a higher silid score indicates\nthat the data point is well matched to\nits own cluster this is usually used for\nDB scan or k me so here the silhouette\nscore can be represented by this formula\nso the S so or the silhouette score is\nequal to B minus AO divided to the\nmaximum of AO and B where s o is The\nSilo coefficient of the data point\ncharacterized by o AO is the average\ndistance between o and all the other\ndata points in the cluster to which o\nbelongs and the B is the minimum average\ndistance from o to all the Clusters to\nwhich o does not\nbelong the final metrix we look to is\nthe completeness completeness is another\nmeasure of the degree to which all of\nthe data points that belongs to a\nparticular class are assigned to the\nsame cluster a higher value indicates a\nmore compete\ncluster let's conclude this lecture by\ngoing through the step-by-step process\nof evaluating a machine learning model\nat a very simplified version since there\nare many additional considerations and\ntechniques that may be needed depending\non a specific task and the\ncharacteristics of the data knowing how\nto properly train machine learning model\nis really important since this defines\nthe accuracy of the results and\nconclusions you will\nmake the training Pro process starts\nwith the preparing of the data this\nincludes splitting the data into\ntraining and test sets or if you are\nusing more advanced resampling\ntechniques that we will talk about later\nthan splitting your data into multiple\nsets the training set of your data is\nused to feed the model if you have also\na validation set then this validation\nset is used to optimize your\nhyperparameters and to pick the best\nmodel while the test set is to use to\nevaluate the model\nperformance when when we will approach\nmore lectures in this section we will\ntalk in detail about these different\ntechniques as well as what the training\nmeans what the test means what\nvalidation means as well as what the\nhyperparameter tuning\nmeans secondly we need to choose an\nalgorithm or set of algorithms and train\nthe model on the training data and save\nthe fitted model there are many\ndifferent algorithms to choose from and\nthe appropriate algorithm will depend on\nthe specific test task and the\ncharacteristics of the data as a third\nstep we need to adjust the model\nparameters to minimize the error on the\ntraining set by performing\nhyperparameter tuning for this we need\nto use validation data and then we can\nselect the best model that results in\nthe least possible validation error rate\nin this step we want to look for the\noptimal set of parameters that are\nincluded as part of our model to end up\nwith a model that has the least possible\nerror so it performs in the best\npossible way in the final two steps we\nneed to evaluate the model we are always\ninterested in a test a rate and not the\ntraining or the validation error rates\nbecause we have not used a test set but\nwe have used the training and validation\nsets so this test error rate will give\nyou an idea of how well the model will\ngeneralize to the new unseen data we\nneed to use the optimal set of\nparameters from hyperparameter tuning\nstage and the training data to train the\nmodel again with this hyper parameters\nand with the best\nmodel so we can use the best fitted\nmodel to get the predictions on the test\ndata and this will help us to calculate\nour test error\nrate once we have calculated the test\nerror rate and we have also obtained our\nbest model we are ready to save the\npredictions so once we are satisfied\nwith the model performance and we have\ntuned the parameters we can use it to\nmake predictions on a new unseen data on\nthe test data and compute the\nperformance metrics for the model us the\npredictions and the real values of the\ntarget variable from the test data and\nthis complete this lecture so in this\nlecture we have spoken about the basics\nof machine learning we have discussed\nthe difference between the the\nunsupervised and supervised learning\nmodels as well as regression versus\nclassification we have discussed in\ndetails the different type of\nperformance metrics we can use to\nevaluate different type of machine\nlearning models as well as we have\nlooked into the simplified version of\nthe step-by-step process to train the\nmachine machine learning model in this\nlecture lecture number two we will\ndiscuss a very important Concepts which\nyou need to know before considering and\napplying any statistical or machine\nlearning model here I'm talking about\nthe bias of the model and the variance\nof the model and the trade of between\nthe two which we call bias various trade\nof whenever you are using a statistical\neconometrical or a machine learning\nmodel no matter how simple the model is\nyou should always evaluate your model\nand check its error rate in all this\ncases it comes down to the trade-off you\nmake between the variance of the model\nand the bias of your model because there\nis always a catch when it comes to the\nmodel choice and the\nperformance let us firstly Define what\nbias and the variant of the machine\nlearning model are the inability of the\nmodel to capture the true relationship\nin the data is called bias hence the\nmachine learning models that are able to\ndetect the true relationship in the data\nhave low\nbias usually complex models or more\nflexible models tend to have a lower\nbias than simpler models so\nmathematically the bias of the model can\nbe expressed as the expectation of the\ndifference between the\nestimate and the True\nValue let us also Define the variance of\nthe model the variance of the model is\nthe inconsistency level or the\nvariability of the model performance\nwhen applying the model to different\ndata sets when the same model that is\ntrained using training data performs\nentirely differently than on the test\ndata this means that there is a large\nvariation or variance in the model\ncomplex models or more flexible models\ntend to have a higher variance than\nsimpler\nmodels in order to evaluate the\nperformance of the model we need to look\nat the amount of error that the model is\nmaking for Simplicity let's assume we\nhave the following simple regression\nmodel which aims to use a single\nindependent variable X to model the\nnumeric y dependent\nvariable that is we fit our model on our\ntraining observations where we have a\npair of independent and dependent\nvariables X1 y1 X2 Y2 up to xn YN and we\nobtain an estimate for our training\nobservations\nfhe we can then compute this let's say\nfhe X1 fhe X2 up to fhe xn which are the\nestimat for our dependent variable y1 Y2\nup to YN and if these are approximately\nequal to this actual values so one head\nis approximately equal to y1 Y2 head is\napproximately equal to Y2 head Etc then\nthe training error rate would be small\nhowever if we are really interested in\nwhether our model is predicting the\ndependent variable appropriately we want\nto instead of looking at the training\nerror rate we want to look at our test\nerror rate so so the error rate of the\nmodel is the expected Square difference\nbetween the real test values and their\nprediction\nwhere the predictions are made using the\nmachine learning model we can rewrite\nthis aor rate as a sum of two quantities\nwhere as you can see the left part is\nthe amount of FX minus F hat x^ squared\nand the second entity is the variance of\nthe error term so the accuracy of Y head\nas a prediction for y depends on the two\nquantities which we can call the\nreducible error and the irreducible\nerror so this is the reducible error\nequal to FX minus f x s and then we have\nour irreducible error or the variance of\nEpsilon so the accuracy of Y head as a\nprediction for y depends on the two\nquantities which we can call the\nreducible error and the irreducible\nerror in general The Fad will not be a\nperfect estimate for f and this\ninaccuracy will introduce some errors\nthis error is reducible since we can\npotentially improve the accuracy of fad\nby using the most appropriate machine\nlearning model and the best version of\nit to estimate the F however even if it\nwas possible to find a model that would\nestimate F perfectly so that the\nestimated response took the form of Y\nhead is equal to FX our prediction would\nstill have some error in it this happens\nbecause Y is also a function of the\nerror rate Epsilon which by definition\ncannot be predicted by using our feature\nX so there will always be some error\nthat is not predictable so variability\nassociated with the error Epsilon also\naffects the accuracy of the predictions\nand this is known as the irreducible\nerror because no matter how well we will\nestimate F we cannot reduce the error\nintroduced by the Epsilon this error\ncontains all the features that are not\nincluded in our model so all the unknown\nfactors that have an influence on our\ndependent variable but are not included\nas part of our\ndata but we can't reduce the reducible\nerror rate which is based on two values\nthe variance of the estimate and the\nbias of the model if we were to simplify\nthe mathematical expression describing\nthe error rate that we got then it's\nequal to the variance of our model plus\nsquared bias of our model plus the\nirreducible error so even if we cannot\nreduce the irreducible error we can\nreduce the reducible error rate which is\nbased on the two values the variance and\nthe squared bias so though the\nmathematical derivation is out of the\nscope of this course just keep in mind\nthat the reducible error of the model\ncan be described as the sum of the\nvariance of the model and a squared bias\nof the\nmodel so mathematically the error in the\nsupervised machine learning model is\nequal to the squared bias in the model\nthe variance of the model and the\nirreducible error therefore in order to\nminimize the expected test error rate so\non the Unseen data we need to select the\nmachine learning meod that\nsimultaneously achieves low variance and\nlow\nbias and that's exactly what we call\ncalled bias variance tradeoff the\nproblem is is that there is a negative\ncorrelation between the variance and the\nbias of the model another thing that is\nhighly related to the bias and the\nvariance of the model is the flexibility\nof the machine learning model so\nflexibility of the machine learning\nmodel has a direct impact on its\nvariance and on its\nbias let's look at this relationships\none by one so complex models or more\nflexible models tend to have a lower\nbias but at the same time complex models\nor flexible models tend to have higher\nvariance than simpler models so as the\nflexibility of the model increases the\nmodel finds the true patterns in the\ndata easier which reduces the bias of\nthe model at the same time the variance\nof such models increases so as the\nflexibility of the model decreases model\nfinds it more difficult to find the true\nparents in the data which then increases\nthe bias of the morel but also decreases\nthe variance of the model keep this\ntopic in mind and we will continue this\ntopic in the next next lecture when we\nwill be discussing the topic of\noverfitting and how to solve the\noverfitting problem by using\nregularization in this lecture lecture\nnumber three we will talk about very\nimportant concept called overfitting and\nhow we can solve overfitting by using\ndifferent techniques including\nregularization this topic is related to\nthe previous lecture and to the topics\nof error of the model train error rate\ntest error rate bias and a variance of\nthe machine learning model overfitting\nis important to know and also how to\nsolve it with\nregularization because this topic can\nlead to inaccurate predictions and the\nlack of generalization of the model to\nnew\ndata knowing how to detect and prevent\noverfitting is crucial in building\neffective machine learning models\nquestions about this topic are almost\nguaranteed to appear during every single\ndata science\ninterview in the previous lecture we\ndiscuss the relationship between model\nflexibility and the variance as well as\nthe bias of the model we saw that as the\nflexibility of the model increases model\nfinds the true pattern in the data\neasier which reduces the bias of the\nmodel but at the same time the variance\nof such models\nincreases so as the flexibility of the\nmodel decreases model finds it more\ndifficult to find the true patterns in\nthe data which then increases the bias\nof the model and decreases the variance\nof the model\nlet's first formally Define what the\noverfitting problem is as well as what\nthe underfitting is so overfitting\noccurs when the model performs well in\nthe training while the model performs\nworse on the test data so you end up\nhaving a low training error rate but a\nhigh test error rate and in the ideal\nworld we want our test error rate to be\nlow or at least that the training a rate\nis equal to the test error rate\noverfitting is a common problem in\nmachine learning where a model learns\nthe detail and noise in training data to\nthe point where it negatively impacts\nthe performance of the model on this new\ndata so the model follows the data too\nclosely closer than it should this means\nthat the noise or random fluctuations of\nthe training data is picked up and\nlearned as concepts by the model which\nit should actually\nignore the problem is that the noise or\nrandom component of the training data\nwill be very different from the noise in\nthe new data the model will therefore be\nless effective in making predictions on\nnew data overfitting is caused by having\ntoo many features too complex of a model\nor too little of the\ndata when the model is overfitting then\nalso the model has high variance and low\nbias usually the higher is the model\nflexibility the higher is the risk of\noverfitting because then we have higher\nrisk of having a model following the\ndata too closely and following the noise\nso underfitting is the other way around\nunderfitting occurs when our test error\nrate is much lower than our training\nerror\nrate given that overfitting is much\nbigger of a problem and we want ideally\nto fix the case when our test theate is\nlarge we will only focus on the\noverfitting and this also the topic that\nyou can expect during your data science\ninterviews as well as something that you\nneed to be aware of whenever you are\ntraining a machine learning model all\nright so now we we know what overfitting\nis we should now talk about how we can\nfix this problem there are several ways\nof fixing or preventing overfitting\nfirst you can reduce the complexity of\nthe model we saw that higher the\ncomplexity of the model higher is the\nchance of the following the data\nincluding the noise too closely\nresulting in overfitting therefore\nreducing the flexibility of the model\nwill reduce the overfitting as well this\ncan be done by using a simpler model\nwith fewer parameters or by applying a\nregularization techniques such as L1 or\nL2 regularization that we will talk in a\nbit kind solution is to collect more\ndata the more data you have the less\nlikely your model will overfit third and\nanother solution is using resampling\ntechniques one of which is cross\nvalidation this is a technique that\nallows you to train and test your model\non different subsets of your data which\ncan help you to identify if your model\nis overfitting we will discuss cross\nvalidation as well as other re sampling\ntechniques later in the section another\nsolution is to apply early stopping\nearly stopping is a technique where you\nmonitor the performance of the model on\na validation set during the training\nprocess and stop the training when the\nperformance starts to decrease another\nsolution is to use assemble methods by\ncombining multiple models such as\ndecision trees overfitting can be\nreduced we will be covering many popular\nemble techniques in this course as\nwell finally you can is what we call\ndropout dropout is a regularization\ntechnique for reducing overfitting in\nnarrow networks by dropping out or\nsetting to zero some of the neurons\nduring the training process because from\ntime to time Dropout related questions\ndo appear during the data science\ninterviews for people with no experience\nso if someone asks you about Dropout\nthen at least you will remember that\nit's a technique used to solve\noverfitting in the setting of deep\nlearning it's worth noting that there is\nno one solution that works for all types\nof overfitting and often a group of\nthese techniques that we just talk about\nshould be used to address the\nproblem we saw that when the model is\noverfitting then the model has high\nvariance and low bias by definition\nregularization or what we also call\nshrinkage is a method that shrinks some\nof the estimated coefficients toward\nzero to penalize unimportant variables\nfor increasing the variance of the model\nthis is a technique used to solve the\noverfitting problem by introducing the\nlethal bias in the model was\nsignificantly decreasing its\nvariance there are three types of\nregularization techniques that are\nwidely known in the industry the first\none is to reach regression or L2\nregularization the second one is the ler\nregression or the L1 regularization and\nfinally the third one is the Dropout\nwhich is a regularization technique used\nin deep learning we will cover the first\ntwo types in this\nlecture let's now talk about re\nregression or L2 regularization so re\nregression or L2 regularization is a\nshrinkage technique that aims to solve\noverfitting by shrinking some of the\nmodor coefficients towards zero\nretrogression introduces latal bias into\nthe model while significantly reducing\nthe model\nvariance R regression is a variation of\nlinear regression but instead of trying\nto minimize the sum of squared\nresiduales that linear regression does\nit aims to minimize the sum of squared\nresiduales added on the top of the\nsquared coefficients what we call L2\nregularization term let's look at a\nmultiple linear regression example with\nP independent variables or predictors\nthat are used to model the dependent\nvariable\ny if you have followed the statistical\nsection of this course\nyou might also recall that the most\npopular estimation technique to estimate\nthe parameter of the linear regression\nassuming its assumptions are satisfied\nis the ordinary Le squares or the OLS\nwhich finds the optimal coefficients by\nminimizing the sum of squared residuales\nor the\nRSS so re regression is pretty similar\nto the OS except that the coefficients\nare estimated by minimizing a slightly\ndifferent cost or loss\nfunction this is the loss function of\nthe re regession where beta J is the\ncoefficient of the model for variable J\nbeta0 is the intercept and x i j is the\ninput value for the variable J and\nobservation I Yi is a target variable or\nthe dependent variable for observation Y\nand N is the number of samples and\nLambda is what we call regularization\nparameter of the r\nregression so this is the loss function\nof OLS that you can see here and added a\npenalization term so it's combined the\nwhat we call RSS so if you check out the\nvery initial lecture in this section\nwhere we spoke about different metrics\nthat can be used to evaluate regression\ntype of models you can see RSS and the\ndefinition of RSS well if you compare\nthis expression then you can easily find\nthat this is the exact formula for the\nRSS added with an intercept and this\nright term is what we called a penalty\namount which basically represents the\nLambda times the sum of the squar of the\ncoefficients included in our model here\nLambda which is always positive so it's\nalways larger than equal zero is the\ntuning parameter or the penalty\nparameter this expression of the sum\nsquared coefficients is called L2 Norm\nwhich is why we call this L2 penalty\nbased regression or L2\nregularization in this way regression\nassigns a penalty by shrinking their\ncoefficients towards zero reduces the\noverall model variance but this\ncoefficient will never become exactly\nzero so the model parameters are never\nsaid to exactly zero which means that\nall P predictors of the model are still\nintact this one is a key property of\nretrogression to keep in mind that it\nshrinks the parameters towards zero but\nnever exactly sets them equal to\nzero L2 Norm is a mathematical term\ncoming from linear algebra and it's\nstanding for alian\nNorm we spoke about the penalty\nparameter lum LDA what we also call the\ntuning parameter Lambda which serves to\ncontrol the relative impact of the\npenalty on the regression coefficient\nestimates when the Lambda is equal to\nzero the penalty term has no effect and\nthe re regression will introduce the\nordinary Le squares estimates but as the\nLambda increases the impact of the\nshrinkage penalty grows and the r\nregression coefficient estimates\napproach to zero what is important to\nkeep in mind which you can also see from\nthis graph is that in r agression large\nLambda will assign a penalty to some\nvariables by shrinking their\ncoefficients towards zero but they will\nnever become exactly zero which becomes\na problem when you are dealing with a\nmodel that has a large number of\nfeatures and your model has a low\ninterpretability retrogressions\nadvantage over ordinarily squares is\ncoming from the earlier introduced bias\nVarian trade of phenomenon so as in\nLambda the penalty parameter increases\nthe flexibility of the retrogression F\ndecreases leading to decreased variance\nbut increased\nbias the main advantages of\nretrogression are solving overfitting\nwhich regression can shrink the\nregression coefficient of less important\npredictors towards zero it can improve\nthe prediction accuracy as well by\nreducing the variance and increasing the\nbias of the model Rich repression is\nless sensitive to outliers in the data\ncompared to linear regression Rich\nregression is computationally less\nexpensive compared to class or\nregression the main disadvantage of R\naggression is the low modal\ninterpretability as the P so the number\nof features your model is\nlarge let's now look into another\nregularization technique called l or\nregression or L1 regularization by\ndefinition l or regression or L1\nregularization is a shrinkage technique\nthat aims to solve overfitting by\nshrinking some of the modal coefficients\ntowards zero and setting some to exactly\nzero l or regression like retrogression\nintroduces later bias into the model\nwhile significantly reducing model\nvariance there is however small\ndifference between the two regression\ntechniques that makes a huge difference\nin their results we saw that one of the\nbiggest disadvantages of R regression is\nthat it will always include all the\npredictors or all the p predictors in\nthe final\nmodel whereas in case of lasso it\novercomes this disadvantage so large\nLambda or penalty parameter will assign\na penalty to some variables by shrinking\ntheir coefficients towards zero in case\nof Rich aggression they will never\nbecome exactly zero which becomes a\nproblem when your model has a large\nnumber of features and it has a low\ninterpretability and L or regression\novercomes this disadvantage of\nretrogression let's have a look at the\nloss function of L\nregularization so this is the loss\nfunction of OLS which is a left part of\nthe formula called RSS combined with a\npenalty amount which is the right hand\nside of the expression the Lambda times\nsome of the absolute values of the\ncoefficients beta\nJ as you can see this is the RSS that we\njust saw which is exactly the same as\nthe loss function of the OLS and then we\nare adding the second term which\nbasically is the Lambda the penalization\nparameter multiplied by the sum of the\nabsolute value of the coefficient beta J\nwhere J goes from one till p and the p\nis number of predictors included in now\nmodel here once again the Lambda which\nis always positive larger than equal Z\nis a tuning parameter or the penalty\nparameter this expression of the sum of\nsquared coefficients is called L1 Norm\nwhich is why we call this L1 penalty\nbased regression or L1 regularization in\nthis way L of regression assigns a\npenalty to some of the variables by\nshrinking their coefficients towards\nzero and setting some of these\nparameters to exactly zero\nso this means that some of the\ncoefficients will end up being exactly\nequal to zero which is a key difference\nbetween the L regression versus the reg\nregression the L1 Norm is a mathematical\nterm coming from the linear alra and\nit's standing for man had Norm or\ndistance you might see here a key\ndifference when comparing the visual\nrepresentation of the L regression\ncompared to the visual representation of\nthe reg agression so if you look at this\npoint you can see that there will be\ncases where our coefficients will be set\nto exactly zero this is where we have\nthis intersection whereas in case of R\nregression you can recall that there was\nnot a single intersection so the numbers\nwhere the circle was closed to the\nintersection points but there was not a\nsingle point when there was an\nintersection and the coefficients were\nput to zero and that's the key\ndifference between two regression type\nof models between the two regularization\nTech\ntechniques the main advantages of loss\nor regression are solving overfitting so\nloss or regression can shrink the\nregression coefficient of less important\npredictors toward zero and some to\nexactly zero as the model filters some\nvariables out L indirectly performs also\nwhat we call feature selection such that\nthe resulted model is highly\ninterpretable and with less features and\nmuch more interpretable compared to the\nreg aggression laso can also improve the\npredi accuracy of the model by reducing\nthe variance and increasing the bias of\nthe model but not as much as the\nretrogression earlier when speaking\nabout correlation we also briefly\ndiscussed the concept of causation we\ndiscuss that correlation is not a\ncausation and we also briefly spoke the\nmethod used to determine whether there\nis a causation or not that model is the\ninfamous linear aggression and even if\nthis model is recognized as a simple\napproach it's one one of the few methods\nthat allows identifying features that\nhave an impact or statistically\nsignificant impact on a variable that we\nare interested in and we want to explain\nand it also helps you identify how and\nhow much there is a change in the Target\nvariable when changing the independent\nvariable\nvalues to understand the concept of\nlinear aggression you should also know\nand understand the concepts of dependent\nvariable independent variable linearity\nand statistical significant effect\ndependent variables are often referred\nto as response variables or explained\nvariables by definition dependent\nvariable is a variable that is being\nmeasured or tested it's called the\ndependent variable because it's thought\nto depend on the independent variables\nso you can have one or multiple\nindependent variables but you can have\nonly one dependent variable that you are\ninterested in that is your target\nvariable let's now look into the\nindependent variable definition so\nindependent variables are often referred\nas regressors or explanatory variables\nand by definition independent variable\nis the variable that is being\nmanipulated or controlled in the\nexperiment and is believed to have an\neffect on the dependent variable put it\ndifferently the value of the dependent\nvariable is s to depend on the value of\nthe independent variable for example in\nan experiment to test the effect of\nhaving a degree on the wage the degree\nvariable would be your independent\nvariable and the wage would be your\ndependent variable finally let's look\ninto the very important concept of\nstatistical significance we call the\neffect statistically significant if it's\nunlikely to have occurred by random\nchance in other words a statistically\nsignificant effect is one that is likely\nto be real and not due to a random\nchance let's now Define the linear\nregression model formally and then we\nwill dive deep into the theoretical and\npractical details\nby definition V regression is a\nstatistical or machine learning method\nthat can help to model the impact of a\nunit change in the variable the\nindependent variable on the values of\nanother Target variable or the dependent\nvariable when the relationship between\nthe two variables is assumed to be\nlinear when the linear regression model\nis based on a single independent\nvariable then we call this model simple\nlinear\nregression when the model is based on\nmultiple independent variables we call\nit multiple linear\nregression let's look at the\nmathematical expression describing\nlinear regression you can recall that\nwhen the linear regression model is\nbased on a single independent variable\nwe just call it a simple linear\nregression this expression that you see\nhere is the most common mathematical\nexpression describing simple linear\nregression so you can see that we are\nsaying that the Yi is equal to Beta 0\nplus beta 1 x i plus\nUI in this expression the Yi is the\ndependent variable and the I that you\nsee here is the index corresponding to\nthe E row so whenever you are getting\nthe data and you want to analyze this\ndata you will have multiple rows and if\nyour multiple rows describe the\nobservations that you have in your data\nso it can be people it can be\nobservation describing uh your data then\nthe each characterizes the specific roow\nthe each roow that you have in your data\nand the Yi is then variables value\ncorresponding to that each show then the\nsame holds for the XI so the XI is then\nthe independent variable or the\nexplanatory variable or the regressor\nthat you have in your model which is the\nvariable that we are testing so we want\nto manipulate it to see whether this\nvariable has a statistically significant\nimpact on the dependent variable y so we\nwant to see whether the unit change in\nthe X will result in a specific change\nin the Y and what kind of change is\nthat so beta Z that you see here is not\na variable and it's called intercept or\nconstant something that is unknown so we\ndon't have that in our data and it's one\nof the parameters of linear regression\nit's an unknown number which the linear\nregression model should estimate so we\nwant to use the linear regression model\nto find out this uh unknown value as\nwell as the second unknown value which\nis a beta one as well as we can estimate\nthe error terms which are represented by\nthe UR\nso beta one next to the XI so next to\nthe independent variable is also not a\nvariable so like beta zero is an unknown\nparameter in linear regression model an\nunknown number which the linear\nregression model should estimate beta\none is often referred as a slope\ncoefficient of variable X which is the\nnumber that quantifies how much\ndependent variable y will change if the\nindependent variable X will change by\none unit so that's EX exactly what we\nare most interested in the beta one\nbecause this is the coefficient and this\nis the unknown number that will help us\nto understand and answer the question\nwhether our independent variable X has a\nstatistically significant impact on our\ndependent variable y finally the U that\nyou see here or the UI in the expression\nis the error term or the amount of\nmistake that the model makes when\nexplaining the target variable we add\nthis value since we know that we can\nnever exactly and accurately estimate\nthe Target variable so we will always\nmake some amount of estimation error and\nwe can never estimate the exact value of\ny hence we need to account for this\nmistake that we are going to make and we\nknow in advance that we are going to\nhave this mistake by adding an error\nterm to our\nmodel let's also have a brief look at\nhow multiple linear regression is\nusually expressed in mathematical terms\nso you might recall that difference\nbetween the simple linear regression and\nmultiple linear regression is that the\nfirst one has a a single independent\nvariable in it whereas the letter or the\nmultiple linear regression like the name\nsuggest has multiple independent\nvariables in it so more than\none knowing this type of Expressions is\ncritical since they not only appear a\nlot in the interviews but also in\ngeneral you will see them in the data\nscience blogs in presentations in books\nand also in papers so being able to\nquickly identify and say ah I remember\nsaying this at once then it will help\nyou to easier understand and follow the\nprocess and the story\nline so uh what you see here you can\nread as Yi is equal to Beta 0 plus beta\n1 * X1 I plus beta 2 * X2 I plus beta 3\n* X3 I plus UI so this is the most\ncommon mathematical expression\ndescribing multiple linear regression in\nthis case with three independent\nvariables so if you were to have more\nindependent variables you should add\nthem with their corresponding indices\nand coefficients so in this case the\nmethod will aim to estimate the model\nparameters which are beta 0 beta 1 beta\n2 and beta Tre so like before Yi is our\ndependent variable which is always a\nsingle one so we only have one dependent\nvariable then we have beta 0 which is\nour intercept or the constant then we\nhave our first slope coefficient which\nis beta 1 corresponding to our first\nindependent variable X1 then we have X1\nI which stands for the independent\nvariable the first independent variable\nwith an index one and the I stands for\nthe index corresponding to the row so\nwhenever we have multiple linear\nregression we always need to specify two\nindices and not only one like we had in\nour uh single linear regression the\nindex cor that characterizes which\nindependent variable we are referring to\nso whether it's independent variable one\ntwo or three and then we need to specify\nwhich row we are referring to which is\nthe index I so you might notice that\nthat in this case all the indices are\nthe same because we are uh looking into\none specific role and we are\nrepresenting this role by using the\nindependent variables the error term and\ndependent variable so then we are adding\nour third term which is beta 2 * x2i so\nthe beta 2 is our third unknown\nparameter in the model and the second\nslope coefficient corresponding to our\nsecond independent variable and then we\nhave our third independent variable with\nthe corresponding slope coefficient beta\n3 as well as we also add like always an\nerror term to account for the error that\nwe know that we are going to\nmake so now when we know what the linear\nregression is and how to express it in\nthe mathematical terms you might be\nasking the next logical question well we\nknow that when we know what the linear\nregression is and how to express it in\nthe mathematical terms you might be\nasking the next logical question how do\nwe find those unknown parameters in the\nmodel in order to find out how the\nindependent variables in impacted the\ndependent variable finding this unknown\nparameters is called estimating in data\nscience and in general so we are\ninterested in finding out the possible\nvalues or the values that the best\napproximate the unknown values in our\nmodel and we call this process\nestimation and one technique used to\nestimate linear regression parameters is\ncalled oils or ordinary Le\nsquares so domain idea behind this\napproach the OLS is to find the best\nfitting straight line so the regression\nline through a set of paired X and y's\nso our independent variables and\ndependent variables values by minimizing\nthe sum of squared\nerrors so to minimize the sum of squares\nof the differences between the observed\ndependent variable and its values which\nare the predicted values that we are\npredicted by our model that's exactly\nwhat we want to do by by using this\nlinear function of the independent\nvariables the residuals so this is too\nmuch information let's go it step by\nstep so in linear regression we just so\nwhen we are expressing our simple linear\nregression we have this error term and\nwe can never know what is the actual\nerror term but what we can do is to\nestimate the value of the error term\nwhich we call residual so we want to\nminimize the sum of squ residuales\nbecause we don't know the errors so we\nwant to find a line\nthat will best fit our data in such way\nthat the error that we are making or the\nsum of squared errors is as small as\npossible and since we don't know the\nerrors we can estimate the Errors By\neach time looking at the predicted value\nthat is predicted by our model and the\nTrue Value and then we can subtract them\nfrom each other and we can see how good\nour model is estimating the values that\nwe have so how good is our model\nestimating the unknown\nparameters so to minimize the sum of\nsquar of the differences between the\nobserved dependent variable and its\nvalues predicted by the linear function\nof the independent variables so the\nminimizing the sum of squared\nresiduales so uh we Define the estimate\nof a parameters and variables by adding\na hge on the top of the variables or\nparameters so in this case you can see\nthat y I had is equal to Beta Z head\nplus beta 1 head XI so you can see that\nwe no longer have a error term this and\nwe say that Yi head is the estimated\nvalue of Yi and beta zero head is the\nestimated value of beta 0 beta 1 head is\nthe estimated value of our beta 1 and\nthe XI is still our data so the values\nthat we have in our data and therefore\nwe don't have a hat since that does not\nneed to be estimated so what we want to\ndo is to estimate our dependent variable\nand we want to compare our estimated\nvalue that we got using our OLS with the\nactual with the real value such that we\ncan calculate our errors or the estimate\nof the error which is represented by the\nUI head so the UI head is equal to Yi\nminus Yi head where UI head is simply\nthe estimate of the error term or the\nresidual so this predicted error is\nalways referred as residual so make sure\nthat you do not confuse the error with\nthe residual so error can never be\nobserved error you can never calculate\nand you will never know but what you can\ndo is to predict the error and you can\nwhen you predict the error then you get\na recal and what oil is trying to do is\nto minimize the amount of airor that\nit's\nmaking therefore it looks at the sum of\nsquared residuales across all the\nobservation and it tries to find the\nline that will minimize this value\ntherefore we are saying that the O tries\nto find the best fitting straight line\nsuch that it minimizes the sum of\nsquared\nresiduals we have discussed this model\nwhen we were talking about this model\nmainly from the perspective of causal\nanalysis in order to identify features\nthat have a statistically significant\nimpact on the response variable but\nlinear regression can also be used as a\nprediction model for modeling linear\nrelationship so let's refresh our memory\nwith the definition of linear regression\nmodel by definition linear regression is\na statistical or a machine learning\nmethod that can help to modrow the\nimpact of a unit change in a variable\nthe independent variable on the values\nof another Target variable the dependent\nvariable when the relationship between\ntwo variables is linear we also\ndiscussed how mathematically we can\nexpress what we call Simple linear\nregression and a multiple linear\nregression so this how the uh simple\nlinear regression can be represented so\nuh in case of simple linear regression\nyou might recorde that we are dealing\nwith just a single independent variable\nand we always have just one dependent\nvariable both in the single linear\nregression and in the multiple linear\nregression so here you can see that Yi\nis equal to Beta 0 plus beta 1 * XI plus\nUI where Y is the dependent variable and\nI is basically the index of each\nobservation or the row and then the beta\n0 is The Intercept which is also known\nas constant and then the beta 1 is the\nslope coefficient or a parameter\ncorresponding to the independent\nvariable X which is unnown and a\nconstant which want to estimate along to\nthe beta zero and then the XI is the\nindependent variable corresponding to\nthe observation I and then finally the\nUI is the error term corresponding to\nthe observation I do keep in mind that\nthis error term we are adding because we\ndo know that we always are going to make\na mistake and we can never perfectly\nestimate the dependent variable\ntherefore to account for this mistake we\nare adding this\nUI so let's also recall the estimation\ntechnique that we use to estimate the\nparameter of the linear regression model\nso the beta 0 and beta 1 and to predict\nthe response variable so we call this\nestimation technique ORS or the ordinary\nLe squares NS is an estimation technique\nfor estimating the unknown parameters in\nthe linear regression model to predict\nthe response or the dependent variable\nso we need to estimate the beta Z so we\nneed to get the beta zero head and we\nneed to estimate the beta one or the\nbeta 1 head in order to obtain the Y I\nhead so Yi head is equal to Beta Z head\nplus beta 1 head time x i where the um\ndifference between the Yi head and the\nYi so the true value of the dependent\nvariable and the predicted value they\nare different will then produce our\nestimate of the error or what we also\ncall residual the main idea behind this\napproach is to find the best fitting\nstraight line so the regression line\nthrough a set of paired X and Y values\nby minimizing the sum of squared\nresiduales so we want to minimize our\nerrors as much as possible therefore we\nare taking their squared version and we\nare trying to sum them up and we want to\nminimize this entire error so to\nminimize the sum of squar residual so\nthe difference between the observed\ndependent variable and its values\npredicted by the linear function of the\nindependent variables we need to use the\nOLS one of the most common questions\nrelated to linear regression that comes\ntime and time again in the uh data\nscience related interviews is a topic of\nthe Assumption of the linear regression\nmodel so you need to know each of these\nfive fundamental assumptions of the\nlinear regression and the OLS and also\nyou need to know how to test whether\neach of these assumptions are\nsatisfied so the first assumption is the\nlinearity Assumption which states that\nthe relationship between the independent\nvariables and the dependent variable is\nlinear we also say that the model is\nlinear in parameters you can also check\nwhether the linearity assumption is\nSatisfied by plotting the residuals to\nthe fitted values if the pattern is not\nlinear then the estimat will be biased\nin this case we say that the linearity\nassumption is violated and we need to\nuse more flexible models such as tree\nbased models that we will discuss in a\nbit that are able to model these\nnonlinear\nrelationships the second assumption in\nthe linear regression is the Assumption\nabout randomness of the sample which\nmeans that the data is randomly sampled\nand which basically means that the\nerrors or the residuales of the\ndifferent observations in the data are\nindependent of each other you can also\ncheck whether the second assumption so\nthis assumption about random sample is\nSatisfied by plotting the residuals you\ncan then check whether the mean of this\nresiduales is around zero and if not\nthen the OLS estimate will be biased and\nthe second assumption is violated this\nmeans that you are systematically over\nor under predicting the dependent\nvariable the third assumption is the\nexogeneity Assumption which is a really\nimportant assumption often as during the\ndata science interviews exogeneity means\nthat each independent variable is\nuncorrelated with the error terms\nexogeneity refers to the assumption that\nthe independent variables are not\naffected by the error term in the model\nin other words the independent variables\nare assumed to be determined\nindependently of the erors in the model\nexogeneity is a key Assumption of the\nnew regression model as it allows us to\ninterpret the estimated coefficient as\nrepresenting the true causal effect of\nthe independent variables on the\ndependent variable\nif the independent variables are not\nexogeneous then the estimated\ncoefficients may be biased and the\ninterpretation of the results may be\ninvalid in this case we call this\nproblem an endogeneity problem and we\nsay that the independent variable is not\nexogeneous but it's endogeneous it's\nimportant to carefully consider the\nexogeneity Assumption when building a\nlinear regression model as violation of\nthis assumption can lead to invalid or\nmisleading results if this assumption is\nsatisfied for an independent variable in\nthe linear model we call this\nindependent variable exogeneous so\notherwise we call it endogeneous and we\nsay that we have a problem of\nendogenity endogenity refers to the\nsituation in which the independent\nvariables in the linear regression model\nare correlated with the error terms in\nthe model in other words the errors are\nnot independent of the independent\nvariables endogeneity is a violation of\none of the key assumptions of the linear\nregression model which is that the\nindependent variables are EX geners or\nnot affected by the errors in the model\nendogenity can arise in a number of ways\nfor example it can be caused by omitted\nvariable bias in which an important\npredictor of the dependent variable is\nnot included in the model it can also be\ncaused by the reverse causality in which\nthe dependent variable affects the\nindependent variable so those two are a\nvery popular examples of the case when\nwe can get an endogenity problem and\nthose are things that you should know\nwhenever you are interest in for data\nscience roles especially when it's\nrelated to machine learning because\nthose questions are uh being asked to\nyou in order to test whether you\nunderstand the concept of exogeneity\nversus endogenity and also in which\ncases you can get endogenity and also\nhow you can solve it so uh in case of\nomitted variable bias let's say you are\nestimating a person's salary and you are\nusing as independent variable their\neducation their number of years of\nexperience and uh some other factors but\nyou are not including for instance in\nyour model a feature that would describe\nthe uh intelligence of a person or uh\nfor instance IQ of the person well given\nthat those are a very important\nindicator for a person in order to\nperform in their uh field and this can\ndefinitely have um indirect impact on\ntheir salary not including these\nvariables will result in omitted\nvariable bias because this will then be\nuh Incorporated in your um error term\nand uh this can also relate to the other\nindependent variables because then your\nuh IQ is also related to the um to the\neducation that you have higher is your\nIQ usually higher is your education so\nin this way you will have an error term\nthat includes an important variable so\nthis is the omitted variable which is\nthen uh correlated with your uh one of\nyour or multiple of your independent\nvariables include in your model so the\nother example other cause of the\nendogenity problem is the reverse\ncausality and um what reverse causality\nmeans is basically that not only the\nindependent variable has an impact on\nthe dependent variable but also the\ndependent variable has an impact on the\nindependent variable so there is a\nreverse relationship which is something\nthat we want to avoid we want to have\nour features that include in our model\nthat have only an impact on dependent\nvariable so they are explaining the\ndependent variable but not the other way\naround because if you have the um the\nother way so you have the dependent\nvariable impacting your independent\nvariable then you will have the error\nterm being related to this independent\nvariable because there are some\ncomponents that also Define your\ndependent variable so knowing the uh few\nexamples such as those that can cause uh\nendogenity so they can violate the\nexogeneity assumption is really\nimportant then uh you can also check for\nthe exogeneity Assumption by conducting\na formal statistical test this is called\nhouse one test so this is an\neconometrical test that helps to\nunderstand whether you have an\nexogeneity uh violation or not but this\nis out of the scope of this course I\nwill however include uh many resources\nrelated the exogeneity endogenity the\nomitted variable bias as well as the\nreverse cality and also how the house\none test can be conducted so for that\ncheck out the interation guide where you\ncan also find the corresponding free\nyour\nresources the fourth assumption linear\nregression is the Assumption about homos\nskes homos refers to the assumption that\nthe variance of the errors is constant\nacross all predicted values this\nassumption is also known as the\nhomogeneity of the variance homosa is an\nimportant Assumption of linear\nregression model as it allows us to use\ncertain statistical techniques and make\ninferences about parameters of the model\nif the errors are not homoskedastic then\nthe result of these techniques may be\ninvalid or misleading if this assumption\nis violated then we say that we have\nheteroscedasticity hecticity refers to\nthe situation in which the variance of\nthe error terms in the linear regression\nmodel is not constant across all the\npredicted values so we have a variating\nvariant in other words the Assumption of\nhomos skas testing in that case is\nviolated and we say we have a problem of\nheos heteros can be a real problem in V\nregression nurses because it can lead to\ninvalid or misleading results for\nexample the standard estimates and the\nconfidence intervals for the parameters\nmay be incorrect which means that also\nthe statistical test may have incorrect\ntype one error rates so you might recall\nwhen we were discussing the linear\nregression as part of the fundamental\nstatis section of this course is that we\nuh looked into the output that comes\nfrom a python and we saw that we are\ngetting uh estimates as part of the\noutput as well as standard errors then\nthe T Test so the student T test and\nthen the corresponding P values and the\n95% confidence intervals so whenever\nthere is a heos problem the um\ncoefficient might still be accurate but\nthen the corresponding standard error\nthe U student T Test which is based on\nthe standard error and then the P value\nas well as the uh confidence intervals\nmay not be accurate so you might get the\nuh good and reasonable coefficient but\nthen you don't know how to correctly\nevaluate them you might end up\ndiscovering that um you might end up\nstating that certain uh independent\nvariables are statistically significant\nbecause their coefficients are\nstatistically significant since their P\nvalues are small but in the reality\nthose P values are misleading because\nthey are based on the wrong statistical\nuh test and they are based on the wrong\nstandard errors you can check for this\nassumption by plotting the residual and\nsee whether there is a funnel like graph\nif there's Fel like gra then you have a\na constant variance but if there is not\nthen you won't see this fenel like this\nshape that indicates that your variances\nare constant and if not then we say we\nhave a problem of heos skos if you have\na heteros system you can no longer use\nthe OS and the linear regression and\ninstead you need to look for other more\nadvanced econometrical regression\ntechniques that do not make such a\nstrong assumption regarding the variance\nof your um residuals so you can for\ninstance use the GLS the fgs the GMM and\nthis type of solutions will um help to\nsolve the hoscar problem and they will\nnot make a strong assumptions regarding\nthe variance in your\nmodel the fifth and the final assumption\nin linear regression is the Assumption\nabout no perfect multicolinearity this\nassumption states that there are no\nexactly new relationships between the\nindependent variables multicolinearity\nrefers to the case when two or more\nindependent variables in your linear\nregression model are highly correlated\nwith each other this can be a problem\nbecause it can lead to unstable and\nunreliable estimate of the parameters in\nthe model perfect multicolinearity\nhappens when the independent variables\nare perfectly correlated with each other\nmeaning that one variable can be\nperfectly predicted from the other ones\nand this can cause the estimated\ncoefficient your linear regression model\nto be infinite or undefined and can lead\nyour errors to be uh entirely misleading\nwhen making a predictions using this\nmodel if perfect multicolinearity is\ndetected it may be necessary to remove\none if not more problematic variables\nsuch that you will avoid having\ncorrelated variables in your model and\neven if the perfect multicolinearity is\nnot present multicolinearity at a high\nlevel can still be a problem if the\ncorrelations between the independent\nvariables are high in this case the\nestimate of the parameters may be\nimprecise and the model may be uh\nentirely misleading and will results in\nless reliable uh\npredictions so uh to test for the\nmulticolinearity Assumption you have\ndifferent solutions you have different\noptions the first way uh you can do that\nis by using the uh di test De test is a\nformal statistical and econometrical\ntest that will help you to identify\nwhich variables cause a problem and\nwhether you have a perfect\nmulticolinearity in your linear\nregression model you can PL heat map\nwhich will be based on the uh\ncorrelation metrix corresponding to your\nfeatures then you will have your uh\ncorrelations per pair of independent\nvariables plotted as a part of your heat\nmap and then you can identify all the um\npair of features that are highly\ncorrelated with each other and those are\nproblematic features one of which should\nbe removed from your model and in this\nway by uh showing the heat map you can\nalso showcase your stakeholders why you\nhave remove certain variables from your\nmodel whereas explaining a Diller test\nis much more complex because it involves\nmore advanced econometrics and linear uh\nregression um\nexplanation so if you're wondering how\nyou can perform this de FL test and you\nwant to prepare the uh questions related\nto perfect multicolinearity as well as\nhow you can solve the perfect\nmulticolinearity problem in your linear\nregression model then head towards the\ninterview preparation guide included in\nthis part of of the course in order to\nanswer such questions and also to see\nthe 30 most popular interview questions\nyou can expect from this section in the\ninterview preparation guide now let's\nlook into an example coming from the\nlinear regression in order to see how\nall those pieces of the puzzle come\ntogether so let's say we have collected\na data on a class size and a test course\nfor of students and we want to model the\nlinear relationship between the class\nsize and the test course using the\nlinear regression model so as we have\njust one independent variable we are\ndealing with a simple linear regession\nand the model equation would be as\nfollows so you can see that the test\ncourse is equal to beta0 plus beta 1\nmultip by class size plus Epsilon so\nhere the class size is the single\nindependent variable that we got in our\nmodel the test score is the dependent\nvariable the beta0 is is The Intercept\nor the constant the beta one is the\ncoefficient of Interest as this the\ncoefficient corresponding to our\nindependent variable and this will help\nus to understand what is the uh impact\nof a unit change in the class size on\nthe test score and then finally we are\nincluding in our model our error term to\naccount for the mistakes that we are\ndefinitely going to make when estimating\nthe uh dependent variable that has\ncourse the goal is to estimate the\ncoefficient 0 and beta 1 from the data\nand use the estimated model to predict\nthe test course based on the class size\nso once we have the estimates we can\nthen interpret them as follows the Y\nintercept the beta zero represents the\nexpected test course when the class size\nis zero it represents the base score\nthat the student would have obtained if\nthe class size would have been zero then\nthe coefficient for the class size the\nbeta one represents the change in the\ntest course associated with the one unit\nchange in the class size the positive\ncoefficient would imply that one unit\nchange in the class size would increase\nthe test course whereas the negative\ncoefficient would uh imply that the one\nunit change in the class size will\ndecrease the test course uh\ncorrespondingly we can then use this\nmodel with OLS estimate in order to\npredict the test course for any given\nclass\nsize so let's go ahead and Implement\nthat in Python if you're wondering how\nthis can be done then head towards the\nresources section as as well as the part\nof the Python for data science where you\ncan learn more about how to work with\npendant data frames how to import the\ndata as well as how to fit a linear\nregression model so the problem is as\nfollows we have collected data on the\nclass size and we have this independent\nvariable so as you can see here we have\nthe students uncore data and then we\nhave the class size and this our feature\nand then we want to estimate the Y which\nis the test SC so uh here is the code a\nsample code that will fit a linear\nregression model we are keeping here\neverything very simple we are not\nsplitting our data into training test\nand then fitting the model on the\ntraining data and making the predictions\nwith the test score but we just want to\nsee how we can interpret the uh\ncoefficients so keeping everything very\nsimple so you can see here that we are\ngetting an intercept equal to\n63.7 and the coefficient corresponding\nto our single independent variable class\nsize is equal to minus\n0.40 what this means is that so each\nincrease of the uh class size by one\nunit will result in the decrease of the\ntest scores with 0.4 so there is a\nnegative relationship between the two\nnow the next question is whether there\nis a statistical significance whether\nthe uh coefficient is actually\nsignificant and where the class size has\nactually statistically significant imp\nimpact on the dependent variable but all\nthose are things that we have discussed\nas part of the fundamental statistic\nsection of this course as well as we are\ngoing to look into a linear regression\nexample when we are going to discuss the\nhypothesis testing so I would highly\nsuggest you to uh stop in here to\nrevisit the fundamentals to statistic\nsection of this course to refresh your\nmemory in terms of linear regression and\nthen um check also the hypothesis test\nuh section of the course in order to\nlook into a specific example of linear\nregression when we are discussing the\nstandard errors how you can evaluate\nyour OLS estimation results how you can\nuse the student T Test the P value and\nthe confidence intervals and how you can\nestimate them in this way you will learn\nfor now only the theory related to the\ncoefficients and then you can um add on\nthe top of this Theory once you have\nlearned all the other sections and the\nother topics in this course let's\nfinally discuss the advantages and the\ndisadvantages of the linear regression\nmodel so some of the advantages of the\nlinear regression model are the\nfollowing the linear regression is\nrelatively simple and easy to understand\nand to implement linear regression\nmodels are well suited for understanding\nthe relationship between a single\nindependent variable and a dependent\nvariable also linear regression can help\nto handle multiple independent variables\nand can estimate the unique relationship\nbetween each independent variable and\nthe corresponding dependent variable\nthear regression model can also be\nextended to handle more complex models\nsuch as pooms interaction terms allowing\nfor more flexibility in the modeling the\ndata also linear aggression model can be\neasily regularized to prevent\noverfitting which is a common problem in\nmodeling as we saw uh in the beginning\nof this section so you can use for\ninstance retrogression which is an\nextension of Vue regression you can use\nler regression which is also an\nextension of Vue regression model and\nthen finally linear regression models\nare widely supported by software\npackages and libraries making it easy to\nimplement and to analyze and some of the\ndisadvantages of the linear aggression\nare the following so the linear\naggression models make a lot of strong\nassumptions regarding for instance the\nlinearity between independent variables\nand independent variables while the true\nrelationship can actually be also\nnonlinear so the model will not then be\nable to capture the complexity of the\ndata so nonlinearity and the predictions\nwill be inaccurate therefore it's really\nimportant to have a data that has a\nlinear relationship for linear\nregression to work linear regression\nalso assumes that the error terms are\nnormally distributed and also\nhomoskedastic error terms are\nindependent across observations\nviolations of the strong assumption will\nlead to bias and inefficient estimates\nlinear regression is also sensitive to\noutliers which can have a\ndisproportionate effect on the estimate\nof the regression coefficients linear\nregression does not easily handle\ncategorical independent variables which\noften require additional data\npreparation or the use of indicator\nvariables or using\nencodings finally linear regression also\nassumes that the independent variables\nare exogeneous and not affected by the\nerror terms if this assumption is\nviolated then the result of the model\nmay be\nmisleading\nin this lecture lecture number five we\nwill discuss another simple machine\nlearning technique called logistic\nregression which is simple but very\nimportant classification model useful\nwhen dealing with a problem where the\noutput should be a probability so the\nname regression in logistic regression\nmight be confusing since this is\nactually a classification model logistic\nregression is widely used in a variety\nof fields such as social sciences\nmedicine and\nEngineering so let us firstly Define the\nlogistic regression model the logistic\nregression is a supervised\nclassification technique that models the\nconditional probability of an event\noccurring or observation belonging to a\ncertain class given a data set of\nindependent variables and those are our\nfeatures the class can have two\ncategories or more but later on we will\nlearn that logistic regression Works\nideally when we have just two classes\nthis is is another very important and\nvery popular machine learning technique\nwhich though named regression is\nactually a supervised classification\ntechnique so when the relationship\nbetween two variables is linear the\ndependent variable is a categorical\nvariable and you want to predict a\nvariable in the form of a probability so\na number between zero and one then\nlogistic regression comes in very handy\nthis is because during the prediction\nprocess in logistic regression the\nclassifier predicts the probability\nility a value between Z and one of each\nobservation belonging to a certain\nclass for instance if you want to\npredict the probability or the\nlikelihood of a candidate being elected\nor Not Elected during the election\nprocess given the set of characteristics\nthat you got about your candidate let's\nsay the popularity score the past\nsuccesses and other descriptive\nvariables about this candidate then\nlogistic regression comes in very handy\nto model this probability\nso rather than predicting the response\nvariable logistic regression models the\nprobability that y belongs to a\nparticular\ncategory similar to the linear\nregression with a difference that\ninstead of Y it predicts the log odds so\nwe will come about this definition of\nlog odds and odds in a bit in\nstatistical terminology what we are\ntrying to do is to model the conditional\ndistribution of the response y given the\npredictors X\ntherefore logistic regression helps to\npredict the probability of Y belonging\nto a certain class given the feature\nspace what we call probability of Y\ngiven X if you're wondering what is the\nconcept of probability what is this\nconditional probability then make sure\nto head towards the section of\nfundamentals to statistics as we are\ngoing to in detail about this Concepts\nas well as we are looking into different\nexamples these definitions and this\nConcepts will help you to better follow\nthis lecture\nso here we see the probability X which\nis what we are interested in modeling\nand it's equal to e to the power beta 0\n+ beta 1 * x / to 1 + e^ beta 0 + beta 1\n* X let's now look into the formulas for\nthe odds and log ODS both these formulas\nare really important because you can\nexpect them during your data science\ninterviews so sometimes you will be\nasked to explicitly write down the odds\nand log ODS formulas and those are\nhighly related to the log likelihood and\nlikelihood functions which are the base\nfor the estimation technique mle or the\nmaximum likelihood estimation used to\nestimate the unknown parameters in the\nlogistic\nagression so the log odds and the odds\nare highly related to each other and in\nlogistic regression we use the odds and\nlog ODS to describe the probability of\nan event occurring the odds is a ratio\nof the probability of an event occurring\nto the probability of the event not\noccurring so as you can see the odd is\nequal to PX / to 1 - PX where PX is the\nprobability of event occurring and 1 -\nPX is the probability of the event not\noccurring so this formula is equal to E\npower beta 0 + beta 1 * X in our formula\nwhere we only have one independent\nvariable and the E simply is the ERS\nnumber or the 2.72 which is a constant\nso we won't derive this formula by\nourselves because that's out of the\nscope of this course but feel free to\nhead out to the PX formula that we just\nsaw in the previous slide and take this\nformula divide it to one minus 2 exactly\nthe same expression and you can verify\nthat you will end up with this\nexpression that you see\nhere for example if the probability of a\nperson having a heart attack is 0.2 then\nthe ads of having a heart attack will be\n0.2 / to 1 - 0.2 which is equal to\n0.25 the low OD also known as the logit\nfunction is a natural logarithm of the\nOD so as you can see here the log of px/\nto 1 minus PX and this is equal to Beta\n0 plus beta 1 * X so you can see that we\nare getting rid of this e and this is\nsimply because of a mathematical\nexpression that says if we take the log\nof the e to the power something then we\nend up with only the exponent part in it\nthough this is out of the scope of this\ncourse to look into the mathematical\nderivation of this formula I will\ninclude many resources regarding this\nlogarithm the Transformations and the\nmathematics behind it just in case you\nwant to look into those details and do\nsome uh extra\nlearning so logistic regression uses the\nlog ODS as the dependent variable and\nthe independent variables are used to\npredict this log ODS the coefficient of\nthe independent varibles represent then\nthe change in the log OD for a one unit\nchange in the independent variable so\nyou might that in the linear regression\nwe were modeling the actual dependent\nvariable in case of logistic regression\nthe difference is that we are modeling\nthe\nlogas another important Concept in\nlogistic regression is the likelihood\nfunction the likelihood function is used\nto estimate the parameters of the model\ngiven the observed data sometimes during\nthe interviews you might also be asked\nto write down the exact likelihood\nformula or the log likelihood function\nso I would definitely suggest you to\nmemorize this one and to understand all\nthe components included in this formula\nthe likelihood function describes the\nprobability of The observed data given\nthe parameters of the model and if you\nfollow the lecture of the probability\ndensity functions in the section of\nfundamentals to statistics you might\nhere even recognize the bar noly PDF\nsince the likelihood function here is\nbased on the probability Mass function\nof a Baro distribution which is a\ndistribution of a binary outcome So This\nis highly applicable to the case where\nwe have only two categories in our\ndependent variable and we are trying to\nestimate the probability of observation\nto belonging to one of those two classes\nso this is the L likelihood function and\nthis is the likelihood function we start\nwith the likelihood function and the L\nthe capital letter L stands for the\nlikelihood function the L is equal the\nlikelihood function L is equal to\nproduct across all pair of these\nmultipliers so we have Peak side to the\npower Yi multiplied by 1 - pxi to^ 1 - y\ni where pxi is the PX that we just so\nonly for observation I and the Yi is\nsimply the class so Yi will either be\nequal to zero or one so Yi is equal to 1\nthen 1 minus Yi is equal to zero so we\nevery time we are looking into the\nprobability of observation belonging to\nthe first class multiply by the\nprobability of observation not belonging\nto that plus and we take this cross\nmultiplications and we do that for all\nthe observations that are included in\nour data and this also comes from\nmathematics so this stands for the\nproduct so uh given that it's harder to\nwork with products compared to the sums\nwe then apply the Lo likelihood uh\ntransformation in order to obtain the Lo\nlikelihood function instead of\nlikelihood function so when we apply\nthis log transformation so we take the L\nlogarithm of this expression we end up\nwith this log likelihood expression and\nhere again one more time we are making\nuse of a mathematical property which\nsays that if we take the logarithm of\nthe products we end up with the sum of\nthe logarithms so we go from the\nproducts to the sums I will also include\nresources regarding that such that you\ncan also learn the mathematics Behind\nThese\nTransformations so the L likelihood with\na lowercase L is equal to logarithm of\nthe products p i^ y i * 1 - PX i^ 1 - Yi\nand when we apply that mathematical\ntransformation then the L is equal to\nsum across all observation I is equal to\n1 till M and then y i so the power the\nexponent comes to the front Yi *\nlogarithm of the pxi plus 1 - Yi *\nlogarithm of 1 -\npxi while for linear regression we use\nOLS as estimation technique for logis\nregression in other estimation technique\nshould be used the reason why we cannot\nuse OLS in logistic regression to find\nthe best fitting line is because the\nerrors can become very large or very\nsmall and sometimes even negative in\ncase of logistic aggression while for\nlogistic regression we aim for predicted\nvalue between zero and\none therefore for logistic regression we\nneed to use estimation technique called\nmaximum likelihood estimation or in\nshort mle where the likelihood function\ncalculates the probability of observing\nthe data outcome given the input data in\nthe model we just saw the likel function\nin the previous slide this function is\nthen optimized to find the set of\nparameters that result in the largest\nsum likelihood so the maximum likelihood\nover the training data\nset logistic function will always\nproduce this s-shaped curve regardless\nof the value of independent variable\nable X resulting in sensible estimation\nmost of the time so value between 0 and\none so as you can see this s-shaped cure\nis what characterizes the maximum\nlikelihood estimation corresponding to\nthe logistic regression and it will\nalways provide not come between zero and\none then the idea behind the maximum\nlikelihood estimation is to find a set\nof estimates that would maximize the\nlikelihood function\nso let's go through the maximum\nlikelihood estimation step by step what\nwe need to do first is to define a\nlikelihood function the first step is to\nalways Define this function for the\nmodel secondly we need to write the log\nlikelihood function so the next step is\nto take the natural logarithm of the\nlikelihood function to obtain the log\nlikelihood\nfunction so I'm talking about this\none the L likel function is a more\nconvenient and computationally efficient\nfunction to work with and what we need\nto do next is to find the maximum of\nthis L like function so this step\nconsists of finding the values of the\nparameters beta 0 and beta 1 that\nmaximize the L lik function there are\nmany optimization algorithms that can be\nused to find the maximum but these are\nout of the scope of this course and you\ndon't need to know them as part of\nbecoming a data scientist and entering\ndata science field in the fourth step we\nneed to estimate the parameters so we\nare talking about the beta 0 and beta 1\nonce the maximum of the log likel\nfunction is found the values of the\nparameters that correspond to the\nmaximum are considered the maximum\nlikelihood estimate of the\nparameters and then in the next step we\nneed to check the model Feit so once the\nmaximum likelihood estimates are\nobtained we can check the goodness of\nfit of the model by calculating\ninformation criteria such as AIC B Bic\nor R squ where AIC stands for akas\ninformation criteria Bic stands for bi\ninformation criteria and r s refers to\nthe same evaluation value that we use\nfor evaluating linear\nregression in the final step we need to\nmake predictions and evaluate the model\nusing the maximum likelihood estimates\nthe model can be used to make\npredictions on a new unseen data and the\nperformance of the model can be then\nevaluated using various evaluation\nmetrics such as accuracy precision and\nrecall those are metrics that we have\nRevisited as part of the very initial\nlecture in the section and those are\nmetrics that you need to know so unlike\nthe AIC Bic that we just spoke about\nthat evaluates the goodness of feed of\nthe very initial estimates that come\nfrom the maximum likelihood the accuracy\nand precision and the recall evaluate\nthe final model so the values that we\nget for the nuan in data when we make\nthe predictions and we get the\nclasses and those are metrics that you\nneed to know if you're wondering what\nthis accuracy is what this Precision\nrecall is as well as the F1 score make\nsure to head towards the very initial\nlecture in this section where we talked\nabout the exact definition of this\nmetrix let's finally discuss the\nadvantages and the disadvantages of the\nlogistic regression so some of the\nadvantages of logistic regressions are\nthat it's a simple model it has a low\nvariance it has a low bias and it\nprovides probabilities some of the\ndisadvantages of logistic regressions\nare logistic regression is unable to\nmodel nonlinear relationship so one of\nthe key assumptions that logistic\nregression is making is that there is a\nlinear relationship between your\nindependent variable and your dependent\nvariable logistic regression is also\nunstable when your classes are well\nseparable as well logistic agression\nbecomes very unstable when you have more\nthan two classes so this means whenever\nyou have more than two categories in\nyour dependent variable or whenever your\nclasses are well separable using\nlogistic regression for classification\npurposes will not be very smart so\ninstead you should look for other models\nthat you can use for this task and one\nof such models is linear discriminate\nanalysis so the LDA that we will\nintroduce in the next lecture so this is\nall for this lecture where we have\nlooked into the logistic regression and\nthe maximum likelihood estimation in the\nnext lecture we will look into the LDA\nso stay tuned and I will see you in the\nnext\nlecture looking to step into machine\nlearning or data science it's about\nstarting somewhere practical yet\npowerful and as the simple yet most\npopular machine learning algorithm\nlinear regression linear aggression\nisn't just a jargon it's a tool that is\nused both for a finding out what are the\nmost important features in your data as\nwell as being used to forecast the\nfuture that's your starting point in the\nJourney of data science and Hands-On\nmachine learning uh work embark on a\nhandson data science and machine\nlearning project where we are going to\nfind what are the drivers of Californian\nhouse prices you will clean the data\nvisualize the key trends you will learn\nhow to process your data and how to use\ndifferent python libraries to understand\nwhat are those drivers of Californian\nhouse values you're are going to learn\nhow to implement linear regression in\nPython and learn all these fundamental\nsteps that you need in order to conduct\na proper handson data science project at\nthe end of this project you will not\nonly learn those different python\nlibraries when it comes to data science\nand machine learning such as pandas\npsyit learn tou models medf Le curn but\nyou will also be able to put this\nproject on your person website and on\nyour resume a point size stepbystep case\nstudy and approach to build your\nconfidence and expertise in machine\nlearning and in data science in this\npart we are going to talk about a case\nstudy in the field of Predictive\nAnalytics and causal analysis so we are\ngoing to use this simple yet powerful\nregression technique called your\nregression in order to perform causal\nanalysis and Predictive Analytics so by\ncausal analysis I mean that we are going\nto look into this correlations clation\nand we're trying to figure out what are\nthe features that have an impact on the\nhousing price on the house value so what\nare these features that are describing\nthe house that Define and cause the\nvariation in the uh house prices the\ngoal of this case study is to uh\npractice linear regression model and to\nget this first feeling of how uh you can\nuse a machine learning model a simple\nmachine learning model in order to\nperform uh model training model\nevaluation and also use it for causal\nanalysis where you are trying to\nidentify features that have a\nstatistically significant impact on your\nresponse variable so on your dependent\nvariable so here is the step-by-step\nprocess that we are going to follow in\norder to find out what are the features\nthat Define the Californian house values\nso first we are going to understand what\nare the set of independent variables\nthat we have we're also going to\nunderstand what is the response variable\nthat we have so for our multiple linear\nregression model we are going to\nunderstand what are this uh techniques\nthat we uh need and what are the\nlibraries in Python that we need to load\nin order to be able to conduct this case\nstudy so first we are going to load all\nthese libraries and we are going to\nunderstand why we need them then we are\ngoing to conduct data loading and data\npreprocessing this is a very important\nstep and I deliberately didn't want you\nto skip this and didn't want you to give\nyou the clean data cuz uh usually in\nnormal real Hands-On data science job\nyou won't get a clean data you will get\na dirty data which will contain missing\nvalues which will contain outliers and\nthose are things that you need to handle\nbefore you proceed to the actual and F\npart which is the modeling and the uh\nanalysis so therefore we are going to do\nmissing data analysis we are going to\nremove the missing data from our\nCalifornian house price data we are\ngoing to conduct outlier detection so we\nare going to identify outliers we are\ngoing to learn different techniques that\nyou can use visualization uh techniques\nuh in Python that you can use in order\nto identify outliers and then remove\nthem from your data then we are going to\nperform data visualization so we are\ngoing to explore the data and we are\ngoing to do different plots to learn\nmore about the data to learn more about\nthis outliers and different statistical\ntechniques uh combined with python so\nthen we are going to do correlation\nanalysis to identify some problematic\nfeatures which is something that I would\nsuggest you to do independent the nature\nof your case study to understand\nunderstand what kind of variables you\nhave what is the relationship between\nthem and whether you are dealing with\nsome potentially problematic\nvariables so then we will be uh moving\ntowards the fun part which is performing\nthe uh multiple theine regression in\norder to perform the caal NES which\nmeans identifying the features in the\nCalifornian house blocks that Define the\nvalue of the Californian\nhouses so uh finally we will do very\nquickly another uh implementation of the\nsame multiple uh multiple linear\nregression in order to uh give you not\nonly one but two different ways of\nconducting linear regression because\nlinear regression can be used not only\nfor caal analysis but also as a\nstandalone a common machine learning\nregression type of model therefore I\nwill also tell you how you can use psych\nlearn as a second way of training and\nthen predicting the C for house\nvalues so without further Ado let's get\nstarted once you become a DAT a\nscientist or machine learning researcher\nor machine learning engineer there will\nbe some cases some Hands-On uh data\nscience projects where the business will\ncome to you and we'll tell you well here\nwe have this data and we want to\nunderstand what are these features that\nhave the biggest influence on this Auto\nfactor in this specific case in our case\nstudy um let's assume we have a client\nthat uh is interested in identifying\nwhat are the features that uh Define the\nhouse price so maybe it's someone who\nwants to um uh invest in uh houses so\nit's someone who is interested in buying\nhouses and maybe even renovating them\nand then reselling them and making a\nprofit in that way or maybe in the\nlong-term uh investment Market when uh\npeople are buying real estate in a way\nof uh in inting in it and then longing\nfor uh holding it for a long time and\nthen uh selling it later or for some\nother purposes the end goal in this\nspecific case uh for a person is to\nidentify what are this features of the\nhouse that makes this house um to be\npriced at a certain level so what are\nthe features of the house that are\ncausing the price and the value of the\nhouse so we are going to make use of\nthis very popular data set that is\navailable on kagal and it's originally\ncoming from psyit learn and is called\nCalifornia housing prices I'll also make\nsure to put the link uh of this uh\nspecific um data set uh both in my\nGitHub account uh under this repository\nthat will be dedicated for this specific\ncase study as well as um I will also\npoint out the additional links that you\ncan use to learn more about this data\nset so uh this data set is derived from\n1990 um US Census so United uh States\ncensus using one row Paris sensus block\nso a Blog group or block is the smallest\nuh geographical unit for which the US\ncus Bureau publishes sample data so a\nBlog group typically has a population of\n600 to 3,000 people who are living there\nso a household is a group of people\nresiding with within a single home uh\nsince the average number of rooms and\nbedrooms in this data set are provided\nper household this conss may be um May\ntake surprisingly large values for blog\ngroups with few households and many\nempty houses such as Vacation\nResorts so\num let's now look into uh the variables\nthat are available in this specific data\nset so uh what we have here is the med\nInc which is the median income in blog\ngroup so uh this um touches the uh\nfinancial side and uh Financial level of\nthe uh block uh block of\nhouseholds then we have House age so\nthis is the median house age in the\nblock group uh then we have average\nrooms which is the average number of\nrooms uh per\nhousehold and then we have average\nbedroom which is the average number of\nbedrooms per household then we have\npopulation which is the uh blog group\npopulation so that's basically like we\njust saw that's the number of people who\nlive in that\nblock then we have a uh o OU uh which is\nbasically the average number of\nhousehold\nmembers uh then we have latitude and\nlongitude which are the latitude and\nlongitude of this uh block group that we\nare looking into so as you can see here\nwe are dealing with aggregate data so we\ndon't have the uh the data per household\nbut rather the data is calculated and\naverage aggregated based on a block so\nthis very common in data science uh when\nwe uh want to reduce the dimension of\nthe data and when we want to have some\nsensible numbers and create this\ncrosssection data and uh cross-section\ndata means that we have multiple\nobservations for which we have data on a\nsingle time period period in this case\nwe are using as an aggregation unit the\nblock and uh we have already learned as\npart of the uh Theory lectures this idea\nof median so we have seen that there are\ndifferent descriptive measures that we\ncan use in order to aggregate our data\none of them is the mean but the other\none is the median and often times\nespecially if we are dealing with skute\ndistribution so if we have a\ndistribution that is not symmetric but\nit's rather right cuute or left skewed\nthen we need to use this idea of median\nbecause median is then better\nrepresentation of this um uh scale of\nthe data um compared to the mean and um\nin this case we will soon see when\nrepresenting and visualizing this data\nthat we are indeed dealing with a skewed\ndata so um this basically a very simple\na very basic data set with not too many\nfeatures so great um way to uh get your\nhands uh uh on with actual machine\nlearning use case uh we will be keeping\nit simple but yet we will be learning\nthe basics and the fundamentals uh in a\nvery good way such that uh learning more\num difficult and more advanced machine\nlearning models will be much more easier\nfor you so let's now get into the actual\ncoding part so uh here I will be using\nthe Google clap so I will be sharing the\nlink to this notebook uh combined with\nthe data in my python for data science\nrepository and you can make use of it in\norder to uh follow this uh tutorial uh\nwith me so uh we always start with\nimporting uh libraries we can run a l\nregression uh manually without using\nlibraries by using matrix\nmultiplication uh but I would suggest\nyou not to do that you can do it for fun\nor to understand this metrix\nmultiplication the linear algebra behind\nthe linear regression but uh if you want\nto um get handson and uh understand how\nyou can use the new regression like you\nexpect to do it on your day-to-day job\nthen you expect to use um instead\nlibraries such as psychic learn or you\ncan also use the statsmodels.api\nlibraries in order to understand uh this\ntopic and also to get handson I decided\nto uh showcase this example not only in\none library in Cy thir but also the\nstarts models and uh the reason for this\nis because many people use linear\nregression uh just for Predictive\nAnalytics and for that using psyit learn\nthis is the go-to option but um if you\nwant to use linear regression for causal\nanalysis so to identify and interpret\nthis uh features the independent\nvariables that have a statistically\nsignificant impact on your response\nvariable and then you will need to uh\nuse another Library a very handy one for\nlinear regression which is called uh\nstats models. API and from there you\nneed to import the SM uh functionality\nand this will help you to do exactly\nthat so later on we will see how nicely\nthis Library will provide you the\noutcome exactly like you will learn on\nyour uh traditional econometrics or\nintroduction to linear regression uh\nclass so I'm going to give you all this\nbackground information like no one\nbefore and we're going to interpret and\nlearn everything such that um you start\nyour machine Learning Journey in a very\nproper and uh in a very um uh high\nquality way so uh in this case uh first\nthing we are going to import is the\npendence library so we are importing\npendis Library as PD and then non pile\nLibrary as NP we are going to need\npendes uh just to uh create a pendis\ndata frame to read the data and then to\nperform data wrangling to identify the\nmissing data outliers so common data\nwrangling and data prosessing steps and\nthen we are going to use npy and npy is\na common way to uh use whenever you are\nvisualizing data or whenever you are\ndealing with metrices or with arrays so\npandas and nonp are being used\ninterchangeably so then we are going to\nuse meth plot lip and specifically the\nPIP plat from it uh and this library is\nvery important um when you want to\nvisualize a data uh then we have cburn\num which uh is another handy data\nvisualization library in Python so\nwhenever you want to visualize data in\nPython then methot leip and Cy uh cburn\nthere are two uh very handy data\nvisualization techniques that you must\nknow if you like this um cooler\nundertone of colors the Seaburn will be\nyour go-to option because then the\nvisualizations that you are creating are\nmuch more appealing compared to the med\nplot Le but the underlying way of\nworking so plotting scatter plot or\nlines or um heat map they are the\nsame so then we have the STS mods. API\nuh which is the library from which we\nwill be importing the uh as uh that is\nthe temple uh linear regression model\nthat we will be using uh for our caal\nanalysis uh here I'm also importing the\nuh from Psychic learn um linear model\nand specifically the linear regression\nmodel and um this one uh is basically\nsimilar to this one you can uh use both\nof them but um it is a common um way of\nworking with machine learning model so\nwhenever you are dealing with Predictive\nAnalytics so we you are using the data\nnot for uh identifying features that\nhave a statistically significant impact\non the response variable so features\nthat have an influence and are causing\nthe dependent variable but rather you\nare just interested to use the data to\ntrain the model on this data and then um\ntest it on an unseen data then uh you\ncan use pyit learn so psyit learn will\nuh will be something that you will be\nusing not only for linear regression but\nalso for a machine learning model I\nthink of uh Canon um logistic regression\num random Forest decision trees um\nboosting techniques such as light GBM\nGBM um also clustering techniques like K\nmeans DB scan anything that you can\nthink of uh that fits in in this\ncategory of traditional machine learning\nmodel you will be able to find Ayler\ntherefore I didn't want you to limit\nthis tutorial only to the S models which\nwe could do uh if we wanted to use um if\nwe wanted to have this case study for uh\nspecifically for linear regression which\nwe are doing but instead I wanted to\nShowcase also this usage of psychic\nlearn because pyic learn is something\nthat you can use Beyond linear\nregression so for all these added type\nof machine learning models and given\nthat this course is designed to\nintroduce you to the world of machine\nlearning I thought that we will combine\nthis uh also with psychic learning\nsomething that you are going to see time\nand time again when you are uh using\npython combined with machine\nlearning so then I'm also uh importing\nthe uh training test plate uh from the\npsychic learn model selection such that\nwe can uh split our data into train and\ntest now uh before we move into uh the\nuh actual training and testing we need\nto first load our data so so therefore\nuh what I did was to uh here uh in this\nsample data so in a folder in Google\ncollab I uh put it this housing. CSV\ndata that's the data that you can\ndownload uh when you go to this specific\nuh page so uh when you go here um then\nuh you can also uh download here that\ndata so download 49 kab of this uh\nhousing data and that's exactly what I'm\nuh downloading and then uploading here\nin Google clap so this housing. CSV in\nthis folder so I'm copying the path and\nI'm putting it here and I'm creating a\nvariable that holds this um name so the\npath of the data so the file uncore path\nis the variable string variable that\nholds the path of the data and then what\nI need to do is that I need to uh take\nthis file uncore path and and I need to\nput it in the pd. read CSV uh which is a\nfunction that we can use in order to uh\nload data so PD stands for pandas the\nshort way of uh naming pandas uh PD do\nand then read uncore CSV is the function\nthat we are taking from Panda's library\nand then within the parentheses we are\nputting the file uncore path if you want\nto learn more about this Basics or\nvariable different data structures some\nbasic python for data science then um to\nensure that we are keeping this specific\ntutorial structured I will not be\ntalking about that but feel free to\ncheck the python for data science course\nand I will put the link um in the\ncomments below such that you can uh\nlearn that if you don't know yet and\nthen you can come back to this tutorial\nto learn how you can use python in\ncombination with linear regression so uh\nthe first thing that I tend to do before\nmoving on to the the actual execution\nstage is to um look into the data to\nperform data exploration so what I tend\nto do is to look at the data field so\nthe name of the variables that are\navailable in the data and that you can\ndo by doing data. columns so you will\nthen look into the columns in your data\nthis will be the name of your uh uh data\nfields so let's go ahead and do command\nenter so we see that we have longitude\nlike attitude housing unor median age we\nhave total rooms we have total bedrooms\npopulation so basically the the um\namount of people who are living in the\nin those households and in those houses\nthen we have households then we have\nmedian income we have median housecore\nvalue and we have ocean proximity now\nyou might notice that the name of these\nvariables are a bit different than in\nthe actual um documentation of the\nCalifornia house so you see here the\nnaming is different but the underlying\nuh explanation is the same so here they\nare just trying to make it uh nicer and\nuh represent it in a better uh naming\nbut uh it is a common um thing to see in\nPython when we are dealing with uh data\nthat uh we have this underscores in the\nname approvation so we have housing\nuncore median AG which in this case you\ncan see that it says house um age so bit\ndifferent but their meaning is the same\nthis is still the median house age in\nthe block group so uh one thing uh that\nyou can also uh notice here is that the\num in the official uh documentation we\ndon't have this um one extra variable\nthat we have here which is the ocean\nproximity and this basically uh\ndescribes the uh Clos cless of the house\nfrom the ocean which of course uh for\nsome people can definitely mean a\nincrease or decrease in the house price\nso I basically um we have all these\nvariables and next thing that I tend to\ndo is to look into the actual data and\none thing that we can do is just to look\nat the um top 10 rows of the data\ninstead of printing the entire uh data\nframe so when we go and uh execute this\nspecific part of the code and the\ncommand you can see that here we have\nthe top 10 rows uh of our data so we\nhave the longitude the latitude we have\nthe housing median age you can see we\nare see some 41 year 21 year 52 year\nbasically the number of years that a\nhouse the median age of the house is 41\n21 52 and this is per\nblock then we have the number of total\nbedrooms so we see that uh we have um in\nthis blog uh the total number of rooms\nthat this houses have is\n7,99 so we are already seeing a data\nthat consists of these large numbers\nwhich is something to take into account\nwhen uh you are dealing with machine\nlearning models and especially with line\nregression then we have total bedrooms\num and we have then population\nhouseholds median income median house\nvalue and the ocean\nproximity one thing that you can see\nright of the bed is that uh we have\nlongitude and latitude uh which have\nsome uh unique uh\ncharacteristics um and longitude is with\nminuses latitude is with pluses uh but\nthat's fine for the linear regression\nbecause what it is basically looking is\nuh whether a variation in certain\nindependent variables in this case\nlongitude and latitude but that will\ncause a change in the dependent variable\nso just to refresh our memory what this\nlinear regression will do in this case\num so we are dealing with multiple inine\nregression because we have more than one\nindependent variables so we have as\nindependent variables those different\nfeatures that describe the house except\nof the house price because median house\nvalue is the dependent variable so\nthat's basically what we are trying to\nfigure out we want to see what are the\nfeatures of the house that cause so\nDefine the house price we want to\nidentify what are um the features that\ncause a change in our dependent variable\nand specifically uh what is the uh\nchange in our median house price uh\nvolue if we apply a one unit change in\nour independent feature so if we have a\nmultiple linear regession we have\nlearned during the theory lecture that\nwhat linear regression tries to use\nduring causal analysis is that it tries\nto keep all the independent variables\nconstant and then investigate for a\nspecific independent variable what is\nthis one unit uh change uh increase uh\nin the specific independent variable\nwill result in what kind of change in\nour dependent variable so if we for\ninstance change by one unit our uh\nhousing median age um then what will be\nthe correspond in change in our median\nhousehold value keeping everything else\nconcent so that's basically the idea\nbehind multi multiple linear regression\nand using that for this specific use\ncase and in here um what we also want to\ndo is to find out what are the uh data\ntypes and whether we can learn bit more\nabout our data before proceeding to the\nnext step and for that I tend to use\nthis uh info uh function in panel\nso given that the data is a penis data\nframe I will just do data. info and then\nparentheses and then this will uh show\nus what is the data type and what is the\nnumber of new values per\nvariable so um as we have already\nnoticed from this header which we can\nalso see here being confirmed that ocean\nproximity is a variable that is not a\nnumeric value so here you can see nearby\num also a value for that variable which\nunlike all the other values is\nrepresented by a string so this is\nsomething that we need to take into\naccount because later on when we uh will\nbe doing the data prop processing and we\nwill actually uh\nactually run this model we will need to\ndo something with this specific variable\nwe need to process it so um for the rest\nwe are dealing with numeric variables so\nyou can see here that longitude latitude\nor all the other variables including our\ndependent variable is a numeric variable\nso float\n64 the only variable that needs to be\ntaken care of is this ocean uncore\nproximity uh which um we can actually\nlater on also see that is um categorical\nstring variable and what this basically\nmeans is that it has these different\ncategories so um for instance uh let us\nactually do that in here very quickly\nso let's see what are all the unique\nvalues for this variable so if we take\nthe name of this variable so we copied\nfrom this overview in here and we do\nunique then this should give us the\nunique values for this categorical\nvariable so here we go so we have\nactually five different unique values\nfor this categorical string variable so\nthis means that this ocean proximity can\ntake uh five different values and it can\nbe either near Bay it can be less than 1\nhour from the ocean it can be Inland it\ncan be near Ocean and it can be uh in\nthe Iceland what this means is that we\nare dealing with a feature that\ndescribes the distance uh of the block\nfrom the ocean and here the underlying\nidea is that maybe this specific feature\nhas a statistically significant impact\non the house value meaning that it might\nbe possible that for some people um in\ncertain areas or in certain countries\nliving in the uh nearby the ocean uh\nwill be increasing the value of the\nhouse so if there is a huge demand for\nhouses which are near the ocean so\npeople prefer to uh leave near the ocean\nthen most likely there will be a\npositive relationship\nif there is a uh negative relationship\nthen it means that uh people uh if uh in\nthat area in California for instance\npeople do not prefer to live near the\nocean then uh we will see this negative\nrelationship so we can see that um if we\nincrease uh the uh if if people uh if\nthe house is in the uh um area that is\nuh not close to Ocean so further from\nthe ocean then the house value will be\nhigher so this is something that we want\nto figure out with this line regression\nwe want to understand what are the\nfeatures that uh Define the value of the\nhouse and we can say that um if the\nhouse has those characteristics then\nmost likely the house price will be\nhigher or the house price will be lower\nand uh linear aggression helps us to not\nonly understand what are those features\nbut also to understand how much higher\nor how much lower will be the value of\nthe house if we have the certain\ncharacteristics and if we increase the\ncertain characteristics by one unit so\nnext we are going to look into uh the\nmissing data in our data so in order to\nhave a proper machine learning model we\nneed to do some uh data processing so\nfor that what we need to do is we need\nto check for the uh missing values in\nour data and we need to understand what\nis this amount of new values per data\nfield and this will help us to\nunderstand whether uh we can uh remove\nsome of those missing values or we need\nto do\nimputation so depending on the amount of\nmissing data that we got in our data we\ncan then understand which all those\nSolutions we need to take so here we can\nsee that uh we don't have any n values\nwhen it comes to longitude latitude\nhousing median age and all the other\nvariables except of one variable one\nindependent variable and that's the\ntotal bedrooms so we can see that um out\nof all the observations that we got the\ntotal uh underscore bedrooms variable\nhas 207 cases when we do not have the\ncorresponding uh\ninformation so when it comes to\nrepresenting this numbers in percentages\nwhich is something that you should do as\nyour next step we can see that um out of\nuh the entire data set uh for total\nunderscore bedrooms variable um only 1.\nn n uh 3% is missing now this is really\nimportant because by simply looking at\nthe number of times the uh number of\nmissing uh observations perir data field\nthis won't be helpful for you because\nyou will not be able to understand\nrelatively how much of the data is\nmissing now if you have for a certain\nvariable 50% missing or 80% missing then\nit means that for majority of your house\nblocks you don't have that information\nand including that will not be\nbeneficial for your Morel nor will be it\naccurate to include it and it will\nresult in biased uh Morel because if you\nhave for the majority of observations uh\nno information and for certain\nobservations you do that inform you have\nthat information then you will\nautomatically skew your results and you\nwill have biased results\ntherefore if you have uh for the\nmajority of your um data set that\nspecific uh variable missing then I\nwould suggest you choose just to drop\nthat independent variable in this case\nwe have just one uh% uh of the uh house\nblocks missing that information which\nmeans that this gives me confidence that\nuh I would rather keep this independent\nvariable and just to drop those\nobservations that do not have a total uh\nunderscore bedrooms uh information now\nanother solution could also be is uh to\ninstead of dropping that entire\nindependent variable is just to uh use\nsome sort of imputation technique so uh\nwhat this means is that uh we will uh\ntry to find a way to systematically find\na replacement for that missing value so\nwe can use mean imputation median imput\nation or more model based more advanced\nstatistical or econometrical approaches\nto perform imputation so for now this\nout of the scope of this problem but I\nwould say look at the uh percentage of\nuh observations that for which this uh\nindependent variable has missing uh\nvalues if this is uh low like less than\n10% and you have a large data set then\nuh you should uh be comfortable dropping\nthose observations but if you have a\nsmall data set so you got only 100\nobservations and for them like 20% or\n40% is missing then consider from\nimputation so try to find the values\nthat can be um used in order to replace\nthose missing\nvalues now uh once we have this\ninformation and we have identified the\nmissing values the next thing is to uh\nclean the data so here what I'm doing is\nthat I'm using the data that we got and\nI'm using the function drop na which\nmeans drop the um uh observations where\nthe uh value is missing so I'm dropping\nall the observations for which the total\nunderscore bedrooms has a null value so\nI'm getting rid of my missing\nobservations so after doing that I'm\nchecking whether I got rid of my missing\nobservations and you can see here that\nwhen I'm printing data do is n do sum so\nI'm summing up the number of uh Missing\nobservations no values per uh variable\nthen uh now I no longer have any missing\nobservations so I successfully deleted\nall the missing\nobservations now the next state is to\ndescribe the data uh through some\ndescriptive statistics and through data\nvisualization so before moving on\ntowards the caal analysis or predictive\nanalysis in any sort of machine learning\ntraditional machine learning approach\ntry to First Look Into the data try to\nunderstand the data and see uh whether\nyou are seeing some patterns uh what is\nthe mean uh of different um numeric data\nfields uh do you have certain uh\ncategorical values that cause an un\nunbalanced data those are things that\nyou can discover uh early on uh before\nmoving on to uh the model training and\ntesting and blindly believing to the\nnumbers so data visualization techniques\nand data exploration are great way to\nunderstand uh this uh data that you got\nbefore using that uh in order to train\nin t machine learning model so here I'm\nusing the uh traditional describe\nfunction of pendas so data. describe\nparentheses and then this will give me\nthe descriptive statistics of my data so\nhere what we can see is that in total we\ngot uh 20 , 640\nobservations uh and then uh we also have\na mean of uh all the variables so you\ncan see that per variable I have the\nsame count which basically means that\nfor all variables I have the same number\nof rows and then uh here I have the mean\nwhich means that um here we have the\nmean of the uh variables so per variable\nwe have their mean and then we have\ntheir standard deviation so the square\nroot of the variance we have the minimum\nwe have the maximum but we also have the\n25th percentile the 15 percentile and\nthe 75th percentile so the uh percentile\nuh and quantiles those are uh\nstatistical terms that we oftenly use\nand the 25th percentile is the first\nquantile the 15 percentile is the second\nquantile or the uh median and the 75th\npercentile is the\nthird quantile so uh what this basically\nmeans is that uh this percentiles help\nus to understand what is this threshold\nwhen it comes to looking at the um\nobservations uh that fall under the\n25% uh and then above the 25% so when we\nlook at this uh standard deviation\nstandard deviation helps us to interpret\nthe variation in the data at the unit so\nscale of that variable so in this case\nthe variable is median house value and\nwe have that the mean is equal to\n206 ,000 approximately so more or less\nthat uh range 206 K and then the\nstandard deviation is\n115k what this means is that uh in the\ndata set we will find blocks that will\nhave the median house value that will be\nuh 200 uh 6K 206k plus 115k which is\naround\n321k so there will be blocks where the\nmedian house value is around\n321k and there will also be blocks where\nthe um median house value will be around\nuh 91k so 206,000 minus\n115k so this the idea behind standard\ndeviation this variation your data so\nnext we can interpret the idea of this\nuh minimum and the maximum of your data\nin your data fields the minimum will\nhelp you to understand what is this\nminimum value that you have per data\nfield numeric data field and what is the\nmaximum value so what is the range of\nvalues that you are looking into in case\nof the median house value this means\nwhat um are the uh what is this minimum\nmedian house value per uh block and uh\nin case of Maximum what is this um\nhighest value per block when it comes to\nMedan house value so this can uh help\nyou to understand um when we look at\nthis aggregated data so the median house\nvalue what are the blocks that have the\nuh cheapest uh houses when it comes to\ntheir valuation and what are the most\nexpensive uh blocks of\nhouses so we can see that uh the\ncheapest um block uh where in that block\nthe median house value is uh 15K so\n14,999\nand the house block with the um highest\nvaluation when it comes to the median\nhouse value so uh the median um\nvaluation of the houses is equal to\n$500,000\nAnd1 which means that when we look at\nour blocks of houses um that uh the\nmedian house value in this most\nexpensive blocks will be a maximum\n500k so uh next thing that I tend to do\nis to visualize the data I tend to start\nwith the dependent variable so this is\nthe variable of interest the target\nvariable or the response variable which\nis in our case the median house value so\nthis will serve us as our dependent\nvariable and what I want you to do is to\nupload this histogram uh in order to\nunderstand what is the distribution of\nmedian house values so I want to see\nthat when when looking at the data what\nare the um most frequently appearing\nmedian house values and uh what are this\nuh type of blocks that have um unique\nless frequently um appearing uh meded\nhouse\nvalues by plotting this type of plots\nyou can see some outliers some um\nfrequently appearing values but also\nsome values that uh go uh and uh are\nlying outside of the range and this will\nhelp you to identify and learn more\nabout your data and toid identify\noutliers in your data so in here I'm\nusing the uh curn uh Library so given\nthat earlier I already imported this\nlibraries there is no need to import\nhere what I'm doing is that I'm setting\nthe the GD so which basically means that\nI'm saying the background should be\nwhite and I also want discrete so this\nmeans those discrete behind then I'm\ninitializing the size of the figure so\nPLT this comes from met plotly P plot\nand then I'm setting the figure the\nfigure size should be 10x 6 so um this\nis the 10 and this is the six then we\nhave the main plot so I'm um using the\nuh his plot function from curn and then\nI'm taking from the uh clean data so\nfrom which we have removed the missing\ndata I'm picking the uh variable of\ninterest which is the median house value\nand then I'm saying upload this um\nhistogram using the fors green color and\nthen uh I'm saying uh the title of this\nfigure is distribution of p and house\nvalues then um I'm also mentioning what\nis the X label which basically means\nwhat is the name of this variable that\nI'm putting on the xaxis which is a\nmedian house value and what is the Y\nlabel so what is the name of the\nvariable that I need to put on the Y AIS\nand then I'm saying pl. show which means\nshow me the figure so that's basically\nhow in Python the visualization works we\nuh first need to write down the the\nactual uh figure size uh and then we\nneed to uh Set uh the function uh in the\nright variable so provide data to the\nvisualization then we need to put the\ntitle we need to put the X label y label\nand then we need to say show me the\nvisualization and uh if you want to\nlearn more about this visualization\ntechniques uh make sure to check the\npython for data science course cuz that\none will help you to understand slowly\nuh and in detail how you can uh\nvisualize your data so in here what we\nare visualizing is the frequency of\nthese median house values in the entire\ndata set what this means is that we are\nlooking at the um number of times each\nof those median house values appear in\nthe data set so uh we want to understand\nare there uh certain uh median house\nvalues that appear very often and are\nthere certain house values that do not\nappear that often so those can be may be\nconsidered\noutliers uh because we want in our data\nonly to keep those uh most relevant and\nrepresentative data points we want to\nderive conclusions that hold for the\nmajority of our uh uh observations and\nnot for outliers we will be then using\nthat uh representative data in order to\nrun our linear regression and then make\nconclusions when looking at this graph\nwhat we can see is that uh we have a\ncertain cluster of um median house\nvalues that appear quite often and those\nare the cases when this frequency is\nhigh so you can see that uh we have for\ninstance houses in here in all this\nblock that appear um very often so for\ninstance the median house value U of A\nabout 160 170k this appears very\nfrequently so you can see that the\nfrequency is above 1,000 those are the\nmost frequently appearing Medan house\nvalues and um there are cases when the\num so you can see in here and you can\nsee in here houses that uh whose median\nhouse value is not appearing very often\nso you can see that their frequency is\nlow so um roughly speaking those houses\nthey are unusual houses they can be\nconsidered as outliers and the same\nholds also for these houses because you\ncan see that for those the frequency is\nvery low which means that in our\npopulation of houses so California house\nprices you'll most likely see houses uh\nblocks of houses whose medium value is\nbetween let's say um 17K up to to uh\nlet's say uh 300 or\n350k but anything below and above this\nis considered as unusual so you don't\noften see a houses that are um so house\nblocks that have a median house value\nless than uh 70 or 60k and then uh also\nuh houses that are above um 370 or 400k\nso do consider that uh we are dealing\nwith\n1990 um a year data and not the current\nuh prices because nowadays uh\nCalifornian houses are much more\nexpensive but this is the data coming\nfrom 1990 so uh do take that into\naccount when interpreting this type of\ndata\nvisualizations so uh what we can then do\nis to use this idea of inter quantile\nrange to remove this outl what this\nbasically means is that we are looking\nat the lowest 25th uh% percentile so uh\nwe are looking at this first quantile so\n0.25 which is a 25th percentile and we\nare looking at this upper 25th um\npercent which means the third quantile\nor the 75th percentile and then we want\nto basically remove those uh by using\nthis idea of 25th percentile and 75th\npercentile so the first quantile and the\nthird quantile we can then identify what\nare the um uh observations so the blocks\nthat have a median house value that is\nbelow the uh 25th per H and above the\n75% he so basically we want to uh get\nthe middle part of our data so we want\nto get this data for which the median\nhouse uh value is above the 25th\npercentile so U above all the uh median\nhouse values that is above the uh lowest\n25% uh percent\nand then we also want to remove this\nvery large median house\nvalues so we want to uh keep in our data\nthe so-called normal uh and\nrepresentative blocks blocks where the\nMedan house uh value is above the lowest\n25% and smaller than the largest 25%\nwhat we are using is this statistical uh\nterm called inter Quan range you don't\nneed to know the name but I think it\nwould be just work to understand it\nbecause this is a very popular way of uh\nmaking a datadriven uh removal of the\noutliers so I'm selecting the um 25th\npercentile by using the quantile\nfunction from pandas uh so I'm saying\nfind for me the um value that divides my\nentire uh block of observations so block\nobservations to observations for which\nthe Medan house value is below the um\nthe um 25th percentile and above the\n25th percentile so what are the largest\n75% and what are the smallest\n25% when it comes to the median house\nvalue and we will then be removing this\n25% so that I will do by using this q1\nand then uh we will be using the uh Q3\nin order to remove the very large median\nhouse Valu so the uh upper 25th\npercentile and then uh in order to um\ncalculate the inter quanti range we need\nto uh pick the Q3 and subtract from it\nthe q1 so just to understand this idea\nof q1 and Q3 so the Quantas better let's\nactually print this uh\nq1 and this uh\nQ3 so let's actually remove this part\nfor now and they run\nit\nso as you can see here what we are\nfinding is that the uh q1 so the 25th\npercentile or first quantile is equal to\n19,500 so it basically is a number in\nhere what it means is that um we have uh\n25% um of the um\nobservations the smallest observations\nhave a median house value that is below\nthe uh $119,500\nand the remaining 75 uh% of our\nobservations have a meeting house value\nthat is above the\n$190,500\nand then the uh Q3 which is the third\nquantile or the 75th percentile it\ndescribes this threshold the volume\nwhere we make a distinction between the\num uh lowest median house values the\nfirst 75th uh% of the lowest uh median\nhouse values versus the uh most\nexpensive so the highest median house\nvalues so what is this upper uh\n25% uh when it comes to the median house\nvalue so we see that that distinction is\n264,000 save\n$700 so it is somewhere in here which\nbasically means that when it comes to\nthis uh to this blocks of uh houses the\nmost expensive ones with the highest\nvaluation so the 25% top rated median\nhouse values they are above\n264,000 that's something that we want to\nremove so we want to remove the\nobservations that have a smallest median\nhouse value and the largest median house\nvalues and and usually it's a common\npractice when it comes to the inter\nquantile uh range approach to multiply\nthe inter quantile range by 1.5 in order\nto um obtain the lower bound and the\nupper bound so to understand what are\nthe um thresholds that we need to use in\norder to remove the uh blocks uh so\nobservation from our data where the med\nhouse value is very small or very large\nso for that we will be multiply the IQR\nso inter quanti range by 1.5 and when we\nuh subtract this value from q1 then we\nwill be getting our lower bound when uh\nwe will be adding this value to Q3 then\nwe will be using and getting this\nthreshold when it comes to the uh upper\nbound and we will be seeing that um\nafter we uh clean this uh outliers from\nour data we end up uh getting um smaller\ndata so this means that uh previously we\nhad uh\n20K so\n20,43 3 observations and now we have\n9,369 observations so we have roughly\nremoved um like about 1,000 or bit over\n1,000 observations from our\ndata so uh next let's look into some\nother variables for instance the median\nuh income and um one other technique\nthat we can use in order to identify\noutliers in the data is by using the box\nplots so I wanted to showcase the\ndifferent approaches that we can use in\norder to visualize the data and to\nidentify outliers such that you will be\nfamiliar with uh different techniques so\nlet's go ahead and plot the uh box plot\nand box plot is a statistical um way to\nrepresent your data uh the central boook\nuh represents the inter Quant range so\num that is is the IQR uh and with the uh\nwith the bottom and the top edges they\nindicate the 25th percentile so the\nfirst quantile and the 75% H so the\nthird quantile respectively the length\nof this box that you see here uh this\ndark part is basically the 50% of your\ndata for the median\nincome and uh this uh median uh line\ninside this box um this is the uh the\none with uh contrast in color that\nrepresents the median of the data set so\nthe median is the middle value when data\nis sorted in an ascending order then we\nhave this whiskers in our box Flo and\nthis line of whiskers extends from the\ntop and the bottom of the box and\nindicate this range for the rest of the\ndata set excluding the\noutliers they are typically this 1.5 IQR\nabove and 1.5 times um IQR uh below the\nq1 something that we also saw uh just\npreviously when we were removing the\noutliers from the median house volum so\nin order to um identify the outliers you\ncan quickly see that we have all these\npoints that um lie above the 1.5 time\nIQR above the um third quantile so the\n75% H and um that's something that you\ncan also see here and this means that\nthose are uh blocks of houses that have\nunusually high median income that's\nsomething that we want to remove from\nour data and therefore we can use the uh\nexactly the same approach that we used\npreviously for the median house value so\nwe will then identify the uh 25th\npercentile or the first quantile so q1\nand then Q3 so the third quantile or the\n75th percentile then we will compute the\nIQR um and then we will be obtaining the\nlower bound and the upper upper bound\nusing this\n1.5 um as a scale and then we will be\nusing that this lower bound and upper\nbound to then um use this filters in\norder to remove from the data all the\nobservations where the medium income is\nabove the lower bound and all the\nobservations that have a median income\nbelow the upper bound so we are using\nlower bound and upper bound to perform\ndouble filtering we are using two\nfilters in the same row as you can see\nand we are using this parenthesis and\nthis end functionality to tell to python\nwell first look that this condition is\nsatisfied so the observations have a\nmedian income that is above this lower\nbound and at the same time it should\nhold that the observation so the block\nshould have a median income that is\nbelow the upper bound and if this uh\nblock this observation in the data\nsatisfies to two of this criteria then\nwe are dealing with a good point a\nnormal point and we can keep this and we\nare saying that this is our new data so\nlet's actually go ahead and execute this\ncode in this case we can see too high as\nall our out layers lie in this part of\nthe box putot and then we will end up\nwith the clean data I'm taking this\nclean data and then I'm putting it under\ndata just for\nSimplicity and uh this data now uh is\nmuch more clean and uh it's better\nrepresentation of the population\nsomething that ideally we want because\nwe want to find out what are the\nfeatures that uh describe and Define the\nhouse value not based on this unique and\nrare houses which are too expensive or\nwhich are in the blogs that have uh very\nhigh income uh people but rather we want\nto see the uh the uh true representation\nso the most frequently appearing data\nwhat are the features that Define the\nhouse value of the prices uh for common\nuh houses and for common areas for\npeople with average or with normal\nincome that's what we want to uh find so\nuh the next thing that I tend to do uh\nwhen it comes to especially regression\nnases and caal nases is to plot the\ncorrelation heat map so this means that\nuh we are getting the um uh correlation\nMatrix pairwise correlation score uh for\neach of this pair of variables in our\ndata when it comes to the linear\nregression one of the uh assumptions of\nthe linear regression that we learned\nduring the theory part is that we should\nnot have a perfect multicolinearity what\nthis means is that there should not be a\nhigh correlation between pair of\nindependent variables so knowing one\nshould not help us to automatically\nDefine the value of the other\nindependent variable and if the\ncorrelation between the two independent\nvariables is very high it means that we\nmight potentially be dealing with\nmulticolinearity that's something that\nwe do want to avoid so hit map is a\ngreat way to identify whether we have\nthis type of problematic independent\nvariables and whether we need to drop\nany of them or maybe multiple of them to\nensure that we are dealing with proper\nlinear regression model and the\nassumptions lession model is satisfied\nnow when we look at this correlation\nheat map um and uh here we use the curn\nin order to plot this as you can see\nhere the colors can be from very light\nso white from till very dark green where\nuh the light means um there is a\nnegative strong negative correlation and\nvery dark uh green means that there is a\nvery strong\npositive correlation\nso uh we know that correlation a value\nPearson correlation can take values\nbetween minus one and 1 minus one means\nuh very strong negative correlation one\nmeans very strong positive\ncorrelation and um usually when uh we\nare dealing with correlation of the\nvariable with itself so a correlation\nbetween longitude and longitude then uh\nthis correlation is equal to one so as\nyou can see on the diagonal we have\nthere for all the ones because those are\nthe pairwise correlation of the\nvariables with themselves and then um in\nhere uh all the values under the\ndiagonal are actually equal to the uh\nmirror of them in the upper diagonal\nbecause the variable between so the\ncorrelation between uh the same two\nvariables independent of how we put it\nso which one we put first and which one\nthe second is going to be the same so\nbasically correlation between longitude\nand ltitude and correlation latitude and\nlongitude is the same so um now we have\nrefreshed our memory on this let's now\nlook into the actual number and this\nheat map so as we can see here we have\nthis section where we um have uh\nvariables independent variables um that\nhave a low uh positive correlation with\nthe uh remaining independent variables\nso you can see here that we have this\nlight green uh values which indicate a\nlow positive relationship between pair\nof\nvariables one thing that is very\ninteresting here is the middle part of\nthis heat map where we have this dark\nnumbers so the numbers uh below the\ndiagonals are something we can interpret\nand remember that below diagonal and\nabove diagonal is basically the mirror\nwe here already see a problem because we\nare dealing with variables which are\ngoing to be independent variables in our\nmodel that have a high correlation\nnow why is this a problem because one of\nthe assumptions of linear regression\nlike we saw during the theory section is\nthat we should not have a multiple uh\ncolinearity so multicolinearity problem\nwhen we have perfect multicolinearity it\nmeans that we are dealing with\nindependent variables that have a high\ncorrelation knowing a value one variable\nwill help us to know automatically what\nis the value of the other one and when\nwe have a correlation of 0.93 which is\nvery high or\n0.98 this means that those two variables\nthose two independent variables they\nhave a super high positive relationship\nthis is a problem because this might\ncause our model to result in uh very\nlarge standard errors and also not\naccurate and not generalizable model\nthat's something we want to avoid and\nand uh we want to ensure that the\nassumptions of our model are\nsatisfied now um we are dealing with\nindependent variable which is total\nunderscore bedrooms and households which\nmeans that number of total\nbedrooms uh pair block and the uh\nhouseholds is highly correlated\npositively correlated and this a problem\nso ideally what we want to do is to drop\none of those two independent variables\nand and uh the reason why we can't do\nthat is because uh those two variables\ngiven that they are highly correlated\nthey already uh explain similar type of\ninformation so they contain similar type\nof variation which means that including\nthe two just it doesn't make sense on\none hand it's uh violating the moral\nassumptions potentially and on the other\nhand it's not even adding too much volum\nbecause the other one already shows\nsimilar variation so\num the total underscore bedrooms\nbasically contains similar type of\ninformation as the households so we can\nas well um so we can better just drop\none of those uh two independent\nvariables now uh the question is which\none and that's something that we can uh\nDefine by also looking at other\ncorrelations in here because we uh have\na total bedrooms uh having a high\ncorrelation with households but we can\nalso see that the total underscore rooms\nhas a very high correlation with our\nhouseholds so this means that there is\nyet another independent variable that\nhas a high correlation with our\nhouseholds\nvariable and then this total underscore\nrooms has also High uh correlation with\nthe total underscore bedroom so this\nmeans that um we can decide which one is\nhas um more frequently uh High\ncorrelation with the rest of independent\nvariables and in this case it seems like\nthat the largest two numbers in here are\nthe um this one and this one so we see\nthat the total bedroom has a 0.93 as\ncorrelation with the total underscore\nrooms and uh at the same time we also\nsee that hotel bedrooms has also um very\nhigh correlation with the household so\n0.98 which means that total underscore\nbedrooms has the highest correlation\nwith the remaining independent variables\nso we might as well drop this\nindependent variable but before you do\nthat I would suggest to do one more\nquick visual check and it is to look\ninto the total uncore bedroom\ncorrelation with the dependent variable\nto understand how strong of a\nrelationship does this have on the\nresponse variable that we are looking\ninto so we see that the uh total\nunderscore\nbedroom uh has this one\n0.05 correlation with the response\nvariable so the median house value when\nit comes to the total rooms that one has\nmuch higher so I'm already seeing from\nhere that uh we can feel comfortable uh\nexcluding and dropping the total\nunderscore bedroom from our data in\norder to ensure that we are not dealing\nwith perfect multicolinearity\nso this exactly what I'm doing here so\nI'm dropping the um total\nbedrooms so after doing that we no\nlonger have this uh total bedrooms as\nthe column so before moving on to the\nactual CA analysis there is one more\nstep that I wanted to uh show you uh\nwhich is super important when it comes\nto the POS analysis and some uh\nintroductory econometrical stuff so uh\nwhen you have a string categorical\nvariable there are a few ways that you\ncan deal with them one easy way that you\nwill see um on the web is to perform one\nH encoding which basically means\ntransforming all this uh string values\nso um we have a near Bay less than 1\nhour ocean uh Inland near Ocean Iceland\nto transform all these values to some\nnumbers such that we we have for the\nocean proximity variable values such as\n1 2 3 4 5 one way of doing that can be\nuh something like this but better way\nwhen it comes to using this type of\nvariables in linear regression is to\ntransform this a string uh category type\nof variable to what we're calling dami\nvariables so dami variable means that\nthis variable takes two possible values\nand usually uh it is a binary Boolean\nvariable\nwhich means that it can take two\npossible values zero and one where one\nmeans that the condition is satisfied\nand zero means condition is not\nsatisfied so let me give you an example\nin this specific case we have that the\nocean proximity has five different\nvalues and ocean proximity is just a\nsingle\nvariable then uh what we will do is we\nwill use the uh get underscore D\nfunction in Python from pandas in order\nto uh go from this one variable to a\nfive different variable per each of this\ncategory which means that now we will\nhave new variables that uh will uh\nbasically be uh whether uh it is uh\nnearby or not whether it's less than 1\nhour uh uh from the ocean uh variable\nwhether it's Inland whether it's near\nOcean or whether is an island this will\nbe a separate binary variable a dummy\nvariable that will take value 0 and one\nwhich means that we are going from one\nstring categorical variable to five\ndifferent dami variables and in this\ncase um each of those dami variables\nthat you can see here we are creating\nfive dami variables each of each for uh\neach of those five categories and then\nuh we are combining them and uh from the\noriginal data we will then be dropping\nthe ocean prox IM data so on one hand we\nare getting rid of this string variable\nwhich is a problematic variable for\nlinear regression when combined with the\npyler library because cyler cannot\nhandle this type of um data when it\ncomes to linear regression and B we are\nmaking our job easier when it comes to\ninterpreting the results so uh\ninterpreting linear regression for CER\nnazes uh is much more easy when we have\ndami variables then when we have a one\nstring categorical variable so just to\ngive you an example if we are creating\nfrom this string variable uh five\ndifferent dami variables and those are\nthose five different dami variables that\nyou can see in here so this means that\nif we are looking at this one category\nso let's say uh ocean _ proximity under\nInland it means that for all the rows\nwhere we have the value equal to zero it\nmeans this criteria is not satisfied\nwhich means that uh ocean proximity uh\nunderscore Inland is equal to zero which\nmeans that the house blob we are dealing\nwith is not from\nInland so that criteria is not satisfied\nand otherwise if this value is equal to\none so for all these rows when the ocean\nproximity Inland is equal to one It\nmeans that the criteria is satisfied and\nwe are dealing with house blocks that\nare indeed in the Inland one thing thing\nto keep in mind uh when it comes to uh\ntransforming a string categorical\nvariable to um set of DS is that you\nalways need to drop at least one of the\ncategories and the reason for this is\nbecause we learned during the theory\nthat uh we should have no perfect\nmulticolinearity this means that um we\ncannot have five different variables\nthat are perfectly\ncorrelated and if we include all these\nvalues and this variables it means that\num when uh we know that the uh uh block\nof houses is not near the bay is not\nless than 1 hour ocean is not Inland is\nnot near the ocean automatically we know\nthat it should be the remaining category\nwhich is Inland so we know that for all\nthose blocks the um uh ocean proximity\nunderscore uh uh irand uh Iceland will\nbe equal to one and that's something\nthat we want to avoid because because\nthat is the definition of perfect\nmulticolinearity So to avoid one of the\noils assumptions to be violated we need\nto drop one of those\ncategories so uh we can see in here uh\nthat's exactly uh what I'm doing I'm\nsaying so let's go ahead and actually\ndrop one of those variables so let's see\nfirst what is the set of all variables\nwe got so we got less than one hour uh\nocean Inland Iceland new bay and then uh\nnew ocean let's actually drop one of\nthem so let's drop the\nIceland and uh that we can do very\nsimply by let me\nsee I is not allowing me to add a code\nin here so we are doing data is equal\nto uh and\nthen data do drop and then the name of\nthe variable we within the uh quotation\nmarks and then uh X is = to 1 so in this\nway I'm basically dropping one of the uh\ndaming variables that uh I created in\norder to avoid the perfect\nmulticolinearity assumption to be\nviolated and once I go ahead and print\nthe columns now we should see uh this uh\ncolumn uh\ndisappearing here we go so we\nsuccessfully deleted that variable let's\ngo ahead and actually get the head so\nnow you can see that we no longer have a\nstring in our data but instead we got\nfour additional binary variable out of a\nstring categorical variable with five\ncategories all right now we are ready to\ndo the actual work uh when it comes to\nthe training a machine learning model uh\nor statistical model we learn during the\nuh theory that we always need to split\nthat data into train uh and test set\nthat is the minimum in some cases we\nalso need to do train validation and\ntest such that we can train the model on\nthe training data and then optimize the\nmodel on validation data and find out\nwhat is the optimal set of\nhyperparameters and then uh use this\ninformation to uh apply this fitted and\noptimized model on an unseen test data\nwe are going to skip the validation set\nfor Simplicity especially given that we\nare dealing with a very simple machine\nlearning model as linear regression\nand we're going to split our data into\ntrain and test and here uh what I'm\ngoing to do is first I'm creating this\nlist of the name or variables that we\nare going to use in order to um train\nour machine learning bottle so uh we\nhave a set of independent variables and\na set of dependent variable so in our\nmultiple linear\nregression here is the set of uh\nindependent variables that we will have\nso we have long itude latitude housing\nmedian Edge total rooms population\nhouseholds median income median house\nvalue and the four different categorical\ndami uh four different uh dami variables\nthat we built from the categorical\nvariable then um I am specifying that\nthe uh Target variable is so the target\nso the response variable or the\ndependent variable is the um median\nhouse value this is the value that we\nwant to uh uh Target because we want to\nsee what are the features and what are\nthe independent variables out of the set\nof all features that have a\nstatistically significant impact on the\nuh dependent variable which is the\nmedian house value because we want to\nfind out what are these features um\ndescribing the houses in the block that\ncause a change cause a variation in the\num t Target variable such as the Medan\nhouse value so here we have X is equal\nto and then uh from the data we are\ntaking all the features that have the\nfollowing names and then we have the uh\nTarget which is a midin house uh house\nvalue and that's uh the column that we\nare going to subtract and select from\nthe data so we are doing data\nfiltering so here we are then selecting\nand what I'm using here is the train\ntest complete function from the psych\nlearn so you might recall that in the\nbeginning we spoke and imported this uh\nmodel selection um library and from the\ncyler model selection we imported the\ntrain _ testore Spate function now this\nis a function that you are going to need\nquite a lot in machine learning because\nthis a very easy way to uh split your\ndata so um in here uh the arguments of\nthe this function is first the uh Matrix\nor the data frame that contains the\nindependent variables in our case X so\nhere you fill in X and then the second\nuh argument is the dependent variable so\nuh the Y and then we have test size\nwhich means um what is the uh proportion\nof um observations that you want to put\nin the test and what is the proportion\nof observation that you um don't want to\nput basically in the training if you are\nputting 0.2 it means that you want your\ntest size to be uh 20% of your entire\n100% of data and the remaining 80% will\nbe your training data so if you provide\nyour point two to this argument then the\nfunction automatically understands that\nyou want this 80 20 division so 80%\ntraining and then 20% test size and then\nfinally you can also uh add the random\nState because the split is going to be\nrandom so the data is going to be\nrandomly selected from the entire data\nand to ensure that your results are\nreproducible and uh the next time you\nare running this um notebook you will\nget the same results and also to ensure\nthat me and you get the same results we\nwill be using a random State and a\nrandom state of 111 is just um random\nnumber there I liked and decided to use\nhere so uh when we go in um use this and\nrun this command you can see that we\nhave a training set size 15K and then\ntest size uh 38k so when you look at\nthese numbers you will then get a\nverification that you are dealing with\n20% versus 80%\nthresholds so then we go and we do the\ntraining one thing to keep in mind is\nthat here we are using the SM Library uh\nNSM function that we imported from the\nuh stats model. API so this is one one\nuh function that we can use in order to\nconduct our uh Cal analysis and to train\nLe regression model so uh for that what\nwe need to do so uh when we\nare using this Library uh this Library\ndoesn't automatically add the uh first\nuh column of ones uh in your uh set of\nindependent variables which means that\nit only goes and looks at what are the\nfeatures that you have provided and\nthose are all the independent variables\nbut we learned from the theory that uh\nwhen it comes to linear regression we\nalways are adding this intercept so the\nbeta0 if you go back to the theory\nlectures you can see this beta0 to be\nadded to both to the simple linear\nregression and to the multipar\nregression this ensures that we look at\nthis intercept and we see what is this\naverage uh in this case median house\nvalue if all the other features are um\nequal to\nzero so um therefore given that the this\nspecific stats models. API is not adding\nthis uh constant um column to the\nbeginning for intercept it means that we\nneed to add this manually therefore we\nare saying sm. addore constant to the\nexrain which means that U now our uh x\nuh table or X data frame uh add a column\nof ones uh to the features so let me\nactually show you uh before doing the uh\ntraining because I think this also\nsomething that you should be aware of so\nif we do here a pause so I'm going to do\nxcore train underscore uh constant and\nthen I'm also going to print um the same\num\nfeature data frame before adding this\nconstant such that you see what I mean\nso as you can see here this is just the\nsame set of all columns that form the\nindependent variables the features so\nthen when we add the constant now after\ndoing that you can see that now we have\nthis initial column of ones this is th\nsuch that we can have uh uh beta Z at\nthe end which is the intercept and we\ncan then perform a valid multiple linear\nregression otherwise you don't have an\nintercept and this is just not what\nyou're looking for now the psychic learn\nLibrary does this automatically\ntherefore when you are using uh this tou\nmodels. API you should add this constant\nand then I use the pyit learn without\nadding the constant and if you're\nwondering why to use this specific model\nas uh we already discussed about this\njust to refresh your memory we are using\nthe T models. API because this one has\nthis nice property of visualizing the\nsummary of your result results so your P\nvalues your test your standard errors\nsomething that you definitely are\nlooking for when you are performing a\nproper causal analysis and you want to\nidentify the features that have a\nstatistically significant impact on your\ndependent variable if you are using a\nmachine learning model including linear\nregression only for Predictive Analytics\nso in that case you can use the psychic\nlearn without worrying about using STS\nmodels.\nAPI so this is about adding constant uh\nnow we are ready to actually uh fit our\nmodel or train our model therefore what\nwe need to do is to use sm. OLS so OS is\nthe ordinar squares estimation technique\nthat we also discussed as part of the\ntheory and we need to provide first the\ndependent variable so Yore train and\nthen the um feature set which is xcore\ntrain uncore constant so then what we\nneed to do is to do that feed par\nparesis which means that take the OS\nmodel and use the Yore train as my\ndependent variable and xcore Trainor\nconstant as my independent variable set\nand then fit the OLS algorithm and\nlinear regression on this specific data\nif you're wondering why y train or X\ntrain and what is the differ between\ntrain and test and sure to go and\nrevisit the training um Theory lectures\nbecause there I go in detail into this\nconcept of training and testing and how\nwe can divide the data into train and\ntest and uh this Y and X as we have\nalready discussed during this tutorial\nis simply this distinction between\nindependent variables defined by X and\nthe dependent variable defined by y so y\ntrain y test is the dependent variable\ndata for the training data and test data\nand then EXT train ex uh test is simply\nthe training data features so ex train\nand then test data features X test we\nneed to use x train and Y train to fit\nour data to learn from the data and then\nonce it comes down to evaluating the\nmodel we need to uh use the fitted model\nfrom which we have learned using both\nthe dependent variable and the\nindependent variable set so y train X\ntrain and then uh once we have this\nmodel uh that is fitted we can apply\nthis to unseen data exore test we have\ncan obtain the predictions and we can\ncompare this to the true y so Yore test\nand to see how\ndifferent the Y uh underscore test is\nfrom the Y predictions for this unseen\ndata and to evaluate how moral uh is\nperforming this prediction so how moral\nis uh managing to identify the median uh\nhouse values and predict median house uh\nvalues based on the uh um fitted model\nand on an unseen data so exore test so\nthis is just a background info and some\nrefreshment and now um in this case we\nare just uh fitting the data on the\ntraining uh dependent variable and then\ntraining uh independent variable edit a\nconstant and then we are ready to print\nthe summary now let's now interpret\nthose results first thing that we can\nsee is that uh all the coefficients and\nall the independent variables are\nstatistically significant and how can I\nsay this well um if we look in here we\ncan see the column of P values this is\nthe first thing that you need to look at\nwhen you are getting this results of a\ncaal analysis in linear oppression so\nhere we are seeing that the P value is\nvery small and just to refresh our\nmemory P value says what is this\nprobability that you have obtained too\nhigh of a test statistics uh given that\nthis is just by a random chance so you\nare seeing statistically significant\nresults which is just by random chance\nand not because your uh n hypothesis is\nfalse and you need to reject it so\nthat's one thing in here you can see you\ncan see that we are getting much more so\nfirst thing that you can do is to verify\nthat you have used the correct dependent\nvariable so you can see here that the\ndependent variable is a median house\nvalue the model that is used to estimate\nthose coefficients in your model is the\nOS the method is the Le squares so Le\nsquares is simply uh the uh technique\nthat is the underlying approach of\nminimizing the sum of uh uh squared\nresiduals so the least squares the date\nthat we are running this analysis is the\n26th of January of\n2024 uh so we have the number of\nobservations which is the number of\ntraining observations so the 80% of our\noriginal data we have R squ which is the\num Matrix that showcases what is the um\ngoodness of fat of your model so r s is\na matrix that is commonly used in linear\nregression specifically to identify how\ngood your model is able to fit your data\nwith this linear regression line and the\nr squ uh the maximum of R squ is one and\nthe minimum is zero\n0.58 uh in this case approximately 59 it\nmeans that uh all your data that you got\nand all your independent variables so\nthose are all the independent variables\nthat you have included they are able to\nexplain\n59% so\n0.59 out of the entire set of variation\nso 59% of variation in your response\nvariable which is the median house value\nyou are able to explain with a set of\nindependent variables that you have\nprovided to the model now what does this\nmean on one hand it means that you have\na reasonable enough information so\nanything above 0.5 is quite good which\nmeans that more than half of the uh\nentire variation in your median house\nvalue you are able to explain but on the\nother hand it means also that there is\napproximately 40% of variation so\ninformation about your house values that\nyou don't have in your data this means\nthat you might consider going and\nlooking for extra additional information\nso additional independent variables to\nadd on the top of the existing\nindependent variables in order to\nincrease this amount and to increase the\namount of information and variation that\nyou are able to explain with your model\nso the r squ this is like the best way\nto uh explain what is the quality of\nyour regression model another thing that\nwe have is the adjusted R squ adjusted R\nsqu and R squ in this specific case as\nyou can see they are the same so 0 um 59\nthis usually means that uh you're fine\nwhen it comes amount of features that\nyou are using once you overwhelm your\nmodel with too many features you will\nnotice that the adjusted R squ will be\ndifferent than your R squ so adjusted R\nsqu helps you to understand whether your\nMotel is performing well only because\nyou are adding so many of you of those\nvariables or because really they contain\nsome useful information CU sometimes the\nr squ it will automatically increase\njust because you are adding too many\nindependent variables but in some cases\nthose independ variables they are not\nuseful so they are just adding to the\ncomplexity of the model and possibly\noverfitting your model but not providing\nany edit\ninformation then we have the F\nstatistics here which corresponds to the\nF test and uh F test um it comes from\nstatistics uh you don't need to know it\nbut I would say uh check out the\nfundamentals to statistics course if you\ndo want to know it because it means that\nuh you are testing whether all these\nindependent variables Al together\nwhether they are helping to explain your\nuh dependent variable so the median\nhouse value and uh if the F statistics\nis very large or the P value of your F\nstatistics is very small so\n0.0 it means that all your independent\nvariables jointly are statistically\nsignificant which means that all of them\ntogether helped you explain your uh uh\nmedian house value and have a\nstatistically significant impact or your\nmedian house value which means that you\nhave a good set of independent\nvariables so then we have the log\nlikelihood not super relevant in this\ncase you have the AIC Bic which stand\nfor AAS information criteria and bation\ninformation criteria those are also not\nnecessary to know for now but once you\nadvance in your career in machine\nlearning it might be useful to know at\nhigher level for now think of it like um\nvalue that helps to understand this uh\ninformation that you gain when you are\nadding this set of independent variables\nto your model but this is just optional\nignore it if you don't know it for now\nokay let's now go into the fun part so\nin this Mata uh part of the summary uh\ntable we got first the set of uh\nindependent variables so we have our\nconstant which is The Intercept we have\nthe longitude latitude housing median\nage total roles population households\nmedian income and the four dami\nvariables that we have created\nthen we have the coefficients\ncorresponding to those independent\nvariables those are basically the beta0\nbeta 1 head beta 2 head Etc which are\nthe um parameters of the linear\nregression model that our oils method\nhas estimated based on the data that we\nhave\nprovided now before interpreting this\nindependent variables the first thing\nyou need to do as I mentioned in the\nbeginning is to look at this P value\ncolumn this showcases the set of all\nindependent variables that are\nstatistically significant and usually\nthis table that you will get from a Sato\nAPI is at 5% significance level so the\nalpha the threshold of statistical\nsignificance is equal to 5% and any P\nvalue that is smaller than 0.05 it means\nyou are dealing with a statistically\nsignificant independent variable now the\nnext thing that you can see here in the\nleft is the T statistics this P value is\nbased on a t test so this T Test is\nsimply stating as we have learned during\nthe theory and you can also check the\nfundamental to statistics course from\nlunar tech for more detailed\nunderstanding of this test but for now\nthis T Test um States a hypothesis\nwhether um each of these independent\nvariables individually has a\nstatistically significant impact on the\ndependent variable and whenever this uh\nT Test has a p value that is smaller\nthan the 0.05 it means you are dealing\nwith statistically significant uh\nindependent variable in this case we are\nsuper lucky all our independent\nvariables are statistically significant\nthen the question is whether we have a\npositive statistical significant or\nnegative that's something that you can\nsee by the signs of these numbers so you\ncan see that longitude has a negative\ncoefficient latitude negative\ncoefficient housing median age positive\ncoefficient\nEtc negative coefficient means that this\nindependent variable causes a negative\nchange in the dependent variable so more\nspecifically when we look for instance\nthe um let's say which one should we\nlook uh let's say the uh total uh\nunderscore rooms when we look at the\ntotal underscore rooms and it's minus\n2.67 it means that when we look at this\ntotal number of rooms and we increase\nthe number of\nrooms uh by uh one additional unit so\none more room added to the total\nunderscore rooms then the uh house value\nuh decreases by minus\n2.67 now you might be wondering but how\nis this possible well first of all the\nvalue the coefficient is quite small so\nin one hand it's it's not super relevant\nas we can see the uh relationship\nbetween them is not super strong because\nthe U margin of this um coefficient is\nquite small but on the other hand you\ncan explain that at some point when you\nare adding more rooms it just doesn't\nadd any value and in fact in some cases\njust decreases the value of the house\nthis might be the case at least this is\nthe case based on this data we can see\nthat if there is a negative coefficient\nthen one unit increase in that specific\nindependent variables all else constant\nwill result in um uh in this case for\ninstance in case of the total rooms uh\n2.67\ndecrease in the median house value\neverything else constant we are also\nreferring to this ass set that is parus\nin econometric which means that\neverything else constant so one more\ntime let's refresh our memory on this so\nensure that we are clear on this if we\nadd one more room to the total number of\nrooms then the median house value will\ndecrease by\n$267 and this when the longitude\nlatitude house median age population\nhouseholds median income and all the\nother criterias are the same so if we\nhave uh for instance this negative value\nthis means that we are getting a\ndecrease in the median house value if we\nhave an increase by one unit in our uh\ntotal number of roles now let's look at\nthe op opposite uh case when the\ncoefficient is actually positive and\nlarge which is the hous in median age\nthis means is if we have two houses they\nhave uh exactly the same characteristics\nso they have the same longitude latitude\nthey have the same total number of rooms\npopulation housing households median\nincome they are uh the same in terms of\nthe distance from the ocean then um if\none of these houses has one more\nadditional year added on the uh median\nage so housing median age so it's one\nyear older then the house value of this\nspecific house is higher by\n$846 so this house which has one more\nadditional median age has $\n846 higher median house value compared\nto the one that has all these\ncharacteris ICS except it has just the\num uh house median age that uh is one\nyear less so one more additional uh year\nin the median age will result in\n846 uh increase in the mediate house\nvalue everything else\nconstant so this is regarding this idea\nof negative and imp positive and then\nthe margin of coefficient now let's look\nat one dami variable and um explain the\nidea behind it and how we can interpret\nit and uh it's it's a good way to\nunderstand how the dond variables can be\ninterpreted in the context of linear\nregression so one of the independent\nvariables is the ocean proximity Inland\nand the coefficient is equal to-\n2108 e plus 0.5 this simply means -\n210 K uh approximately and um what this\nmeans is that if we have two houses\nthey have exactly the same\ncharacteristics so their longitude\nlatitude is the same house median age is\nthe same they have the same total number\nof rooms population households median\nincome all these characteristics for\nthis two blocks of houses is the same\nwith a single difference that one block\nis located in the um Inland when it\ncomes to Ocean proximity and the other\nblock of houses is not located in the\nInland so in this case the reference so\nthe\num category that we have removed from\nhere was the Iceland you might recall uh\nso if the block of houses is in the\nInland that their value is on average uh\nsmaller and less by\n210k when it comes to the median house\nvalue compared to the block of houses\nthat has exactly the same\ncharacteristics but it's not in the\nInland so for instance is in the uh\nIceland so uh when it comes to this dumi\nvariables where there is also an\nunderlying reference variable which you\nhave deleted as part of your string\ncategorical variable then you need to\nreference your dami variable to that\nspecific category this might sound\ncomplex it is actually not I would say\nuh it's just a matter of practicing and\ntrying to understand what is this\napproach of D variable it means that you\neither have that criteria or not in this\nspecific case it means that if you have\ntwo blocks of houses with exactly the\nsame characteristics and one block of\nhouses is in the Inland and the other\none is not in the Inland for instance is\nin the Iceland then the block of houses\nin the Inland will have on average\n210,000 less uh median house value\ncompared to the block of houses that is\nthe IND for instance in the Iceland uh\nwhen it comes to the ocean\nproximity which kind of uh makes sense\nbecause in California people might\nprefer living uh in the isoland location\nin the houses might have more demand\nwhen it comes to the Iceland location\ncompared to the um Inland locations so\nthe longitude uh has a statistically\nsignificant impact on the uh median\nhouse value latitude house median age\nhas an impact and causes a a\nstatistically significant difference in\nthe Medan house value if there is a\nchange in median age the total number of\nrooms have an impact on the median house\nvolume and the population has an impact\nhouseholds median income as well as the\nuh proximity from the ocean and this is\nbecause all their P values is uh zero\nwhich means that they are smaller than\n0.05 and this means that they all have a\nstatistically significant impact on the\nmedian house value in the Californian\nhouse in market now when it comes to the\nuh interpretation of all of them uh we\nhave interpreted just few uh for the\nsake of Simplicity and ensuring that\nthis uh this entire case study doesn't\ntake too long but what I would suggest\nyou to do is to uh interpret all of the\nuh coefficients here because we have\ninterpreted just the housing median age\nand the um the total number of rooms but\nyou can also interpret the population uh\nas well as the median income and uh we\nhave also interpreted one of those Dy\nvariables but feel free also to\ninterpret all the other ones so by doing\nthis you can also uh Even build an\nentire case study paper in which you can\nexplain in one or two pages the results\nthat you have obtained and this will\nshowcase that you have an understanding\nof how you can interpret the linear\ngressional results another thing that I\nwould suggest you to do is to uh add a\ncomment on the standard error so let's\nnow look into the standard errors we can\nsee a huge standard error that we are um\nmaking and this is the direct result of\nthe fourth assumption that was violated\nnow this case study is super important\nand useful in a way that it showcases\nwhat happens if some of your um\nassumptions are satisfied and if some of\nthose assumptions are violated so in\nthis specific specific case the\nAssumption related to the uh uh the\nerrors having a constant variance is\nviolated so we have a heos SK assist the\nissue and that's something that we are\nseeing back in our results and this is a\nvery good example of the case that even\nwithout checking the assumptions you can\nalready see that the standard error is\nvery large and uh you can see here that\ngiven that the standard ER is large this\nalready gives a hint that most most\nlikely our\nheteroscedasticity uh is present and our\nhomoscedasticity assumption is violated\nyou keep in mind this um idea of um\nlarge standard errors that we just saw\nbecause we are going to see that this\nbecomes a problem also for the um\nperformance of the model and we will see\nthat we are obtaining a large error due\nto this and uh one more comment when it\ncomes to the total rooms and the housing\nmedian age in some cases the linear\nregression results might not seem\nlogical but sometimes they actually is\nan underlying explanation that can be\nprovided or maybe your model is just\noverfitting or biased that's also\npossible and uh that's\nsomething that uh you can do by checking\nyour ois assumptions and uh before uh\ngoing to that stage I wanted to briefly\nshowcase to you this um idea of\npredictions so we have now fitted our\nmodel on the uh uh training data and we\nare ready to perform the predictions so\nwe can then use our fitted model and we\ncan then uh use the test data so ex test\nin order to perform the predictions so\nto uh use a data to get new house\nmediate house values for the um blocks\nof houses for which we are not providing\nthe uh corresponding Medan house price\nso on aning data we are uh re um\napplying our model that we have already\nfitted and we want to see what are these\npredicted median house values and then\nwe can compare these predictions to the\ntrue median house values that we have\nbut we are not yet exposing them and we\nwant to see how good our model is doing\na job of estimating and finding these\nunknown median house values for the test\ndata so for all the blocks of houses for\nwhich we provided the characteristics in\nthe X test but we are not providing the\nY test so uh as usual like in case of\ntraining we are adding a constant with\nthis library and then we are saying\nmodel. fitted model uncore fitted so the\nfitted model and then that predict and\nproviding the test data and those are\nthe test\npredictions now uh once we do this we\ncan then get the test predictions and uh\nif we print those you can see that we\nare getting a least of house values\nthose are the house values for the um um\nblocks of houses which were included as\npart of the testing data so the 20% of\nour entire data\nset uh like I mentioned just before in\norder to ensure that your model is\nperforming well you need to check the OS\nassumptions so uh during the um Theory\nsection we learned that there are a\ncouple of assumptions that your model\nshould satisfy and your data should\nsatisfy for OLS to provide uh B unbiased\nand um efficient uh estimates which\nmeans that they are accurate their\nstandard error is low something that um\nwe are also seeing as part of the\nsummary results and uh your estimates\nare accurate so the standard error is a\nmeasure that showcases how efficient\nyour estimat are which means um do you\nhave a high variation uh can the\ncoefficients that you are showing in\nthis table very a lot which means that\nyou don't have accurate um coefficient\nand your coefficient can be all the way\nfrom one place to the other so the range\nis very L large which means that your\nstandard error will be very large and\nthis is a bad sign or you are dealing\nwith an accurate estimation and uh it's\nmore precise estimation and in that case\nthe standard there will be low uh and\nunbias estimate means that your\nestimates are are a true representation\nof the pattern between each pair of\nindependent variable and the response\nvariable if you want to learn more about\nthis IDE of bias unbias and then\nefficiency and sure to check the U\nfundamental statistics course at lunar\nTech because it explains very clearly\nthis Concepts in detail so here I'm\nassuming that you know or maybe you\ndon't even need it but I would suggest\nyou to know at higher level at least\nthen uh let's quickly do the checking of\noiless assumption so the first\nassumption is the linearity Assumption\nwhich means that your model is linear in\nparameters one way of checking that is\nby using your already fitted model and\nyour uh predicted model so the Y uh uh\ntest which are your true house median\nhouse values for your test data and then\ntest predictions which are your uh\npredicted median house values for nonen\ndata so you are using the uh True Values\nand the predicted values in order to um\nplot them and then to also plot the best\nfitted line in an ideal situation when\nyou would make no error and your model\nwould give you the exact True Values um\nand then see how well your um uh how\nlinear is this relationship do we\nactually have a linear\nrelationship now if the observed versus\npredicted values where the observed\nmeans the uh real uh test test wise and\nthe predicted means the test predictions\nif this pattern is kind of linear and\nmatching this perfect linear line then\nyou have um assumption one that is\nsatisfied your linearity assumption is\nsatisfied and you can say that your uh\ndata uh and your model is indeed linear\nin\nparameters then uh we have the second\nassumption which states that your uh\nsample should be random and this\nbasically translates that the uh\nexpectation of your error terms should\nbe equal to zero and uh one way of\nchecking this is by simply taking the\nresiduales from your fitted model so\nmodel on score fitted and then that's\nresidual so you take the residuales you\nobtain the average which is a good\nestimate of your expectation of errors\nand then this is the mean of residuales\nso the average uh\nresiduales where the residual is the\nestimate of your true error terms and\nthen uh here what I do is just I just\nround up uh to the two decimals behind\nuh the uh the point this means that uh\nwe are getting uh this average amount of\nuh errors or the estimate of the errors\nwhich we are referring as residuales and\nif this number is equal to zero which is\nthe case so the mean of the residuales\nin our model is zero it means that\nindeed the um uh expectation of the uh\nerror terms at least the estimate of it\nexpectation of the residuales is inde\nequal to zero another way of checking\nthe um uh second assumption which is\nthat the um moral uh has a is based on\nthe random sample and the sample we are\nusing is random which means that the\nexpectation of the error terms is equal\nto zero is by plotting the residuales\nversus fitted values so uh we are taking\nthe resid from the fitted model and we\nare comparing to the fitted values that\ncomes from the model uh and we are\nlooking at this um graph this scatter\nplot which you can see in here and we're\nlooking where this um pattern is\nuh\nsymmetric uh around the uh threshold of\nzero so you can see this line kind of\ncomes right in the middle of this\npattern which means that on average we\nhave residuales that are across zero so\nthe mean of the residuales is equal to\nzero and that's exactly what we were\ncalculating also here therefore we can\nsay that we are indeed dealing with a\nrandom\nsample this FL is also super useful when\nit comes to the fourth assumption that\nwe will come a bit later so for now\nlet's check the third assumption which\nis the Assumption of exogeneity so\nexogeneity means that uh each of our\nindependent variables should be\nuncorrelated from the error terms so\nthere is no omitted variable bias there\nis no um reverse causality which means\nthat the uh independent variable has an\nimpact on the dependent variable but not\nthe other way around so dependent\nvariable should not have an impact and\nshould not cause the independent\nvariable so for that there are few ways\nthat we can deal with uh with this uh\none way is just straightforward to\ncompute the uh correlation coefficient\nbetween between each of these\nindependent variables and the residuales\nthat you have obtained from your fitted\nmodel the just simple uh technique that\nyou can use in a very uh quick way to\nunderstand what is this uh correlation\nbetween each pair of independent\nvariable and the residuals which are the\nbest estimates of your error terms and\nin this way you can understand that\nthere is a correlation between your\nindependent variables and your error\nterms another way you can do that and\nthis is more advanced and bit more um\ntowards the econometrical side is by\nusing this test which is called the\nDurban uh view housan test so this uh\nDurban view housan test is um a more\nprofessional more advanced way of uh\nusing an econometrical test to find out\nwhether you have um exogene so\nexogeneity sup is satisfied or you have\nendogenity which means that one or\nmultiple of your your independent\nvariables is potentially correlated with\nyour error terms uh I won't go into\ndetail of this test uh I'll put some\nexplanation here and also feel free to\nuh check any uh introductory to\neconometrics course to understand more\non this Duran Vu housan test for\nexogeneity assumption the fourth\nassumption that we will talk about is\nthe homos skasis homosa assumption\nstates that the error terms should have\na variance that is constant which means\nthat when we are looking at this\nvariation that uh the model is making uh\nacross uh different observations that uh\nwhen we look at them the variation is\nkind of constant so uh we have all these\nuh cases when the uh in observations for\nwhich the residuals are bit small in\nsome cases bit large we have this miror\nwhen it comes to this figure with what\nwe are calling heteros skos which means\nmeans that homos assumption is violated\nour error terms do not have a variation\nthat is constant across all the\nobservations and we have a high\nvariation and different variations for\ndifferent observations so we have the\nheteros issue we should consider a bit\nmore um flexible approaches like uh GLS\nfgs GMM all bit more advanced\neconometrical\nalgorithms so uh the final part of this\ncase study will be to show you how you\ncan do uh this all but for machine\nlearning traditional machine learning\nsite by using the psychic learn so uh in\nhere um I'm using the um standard scaler\nfunction in order to uh scale my data\nbecause we saw uh in the summary of the\ntable um that we got from the stats uh\nmos. API that our data is at a very high\nscale because the uh median house values\nare those large numbers the uh age uh\nthe median age of the house is in this\nvery large numbers that's something that\nyou want to avoid when you are using the\nlinear regression as a Predictive\nAnalytics model when you are using it\nfor interpreting purposes then you\nshould keep the skilles because it's\neasier to interpret those values and to\nunderstand uh what is the difference in\nthe median price uh of the house when\nyou compare different characteristics of\nthe box of houses but when it comes to\nusing it for Predictive Analytics\npurposes which means that you really\ncare about the accuracy of your\npredictions then you need to uh scale\nyour data and ensure that your data is\nstandardized one way of doing that is by\nusing the standard scaler function uh in\nthe pyit learn.\npreprocessing uh and uh the way I do it\nis that I initialize the scaler by using\nthe standard scaler and then parenthesis\nwhich are just import from this psychic\nlearn library and then uh I am uh taking\nthis scaler I'm doing that fitore\ntransform exrain which basically means\ntake the independent variables and\nensure that we scale and standardize the\ndata and standardization simply means\nthat uh we are\nstandardizing the data that we have to\nensure that um some large values do not\nwrongly influence the predictive power\nof the model so the the model is not\nconfused by the large numbers and finds\na wrong variation but instead it focuses\non the a true variation in the data\nbased on how much the change in one\nindependent variable causes a change in\nthe dependent\nvariable here given that we are dealing\nwith the supervised learning algorithm\nuh the exrain uh scaled will be then\ncontaining our standardized uh features\nso independent variables and then each\ntest SC will contain our standardized\ntest features so the Unseen data that\nthe model will not see during the\ntraining but only during prediction and\nthen what we will be doing is that we\nwill also use the um y train and Y train\nuh is the dependent variable in now\nsupervised model and why train\ncorresponds to the training data so we\nwill then first initialize the linear\nregression here so linear regression\nmodel from pyit learn\nand then uh we will initialize the model\nthis is just the empty linear regression\nmodel and then we will take this\ninitialized uh model and then we will\nfit them on the uh training data so\nexore trained uncore scale so this is\nthe trained features and then the um uh\ndependent variable from training data so\nwhy\ntrain uh do you knowe that I'm not\nscaling the dependent variable this is a\ncommon practice cuz you don't want to uh\nstandardize your dependent variable\nrather than you want to ensure that your\nfeatures are standardized because what\nyou care is about the variation in your\nfeatures and to ensure that the model\ndoesn't mess up when it's learning from\nthose features less when it comes to\nlooking into the impact of those\nfeatures on your dependent\nvariable so then uh I am fitting the uh\nmodel on this training data so uh\nfeatures and independent variable and\nthen I'm using this fitted uh model the\nLR which already has learned from this\nfeatures and dependent variable during\nsupervised training and then I'm using\nthe X test scale so the test\nstandardized uh data in order to uh\nperform the prediction so to predict the\nimmediate house values for the test data\nunseen data and you can notice that here\nin no places I'm using white test white\ntest I'm keeping to myself which is the\ndependent variable True Values such that\nI can then compare to this predicted\nvalues and see how well my motor was\nable to actually get the predictions now\nuh let's actually also do one more step\nI'm importing from the psyit learn the\nMatrix such as mean squared error uh and\nI'm using the mean squared error to find\nout how well my motel was able to\npredict those house prices so this means\nthat uh we have on average we are making\nan error of\n59,000 of dollars when it comes to the\nmedian house prices which uh dependent\non what we consider as large or small\nthis is something that we can look into\nso um like I mentioned in the beginning\nthe uh idea behind linear regression\nusing IND specific uh course is not to\nuh use it in terms of pure traditional\nmachine learning but rather than to\nperform um causal analysis and to see\nhow we can interpret it when it comes to\nthe quality of the predictive power of\nthe model then uh if you want to improve\nthis model this can be considered as the\nnext step you can understand whether\nyour model is overfitting and then the\nnext step could be to apply for instance\nthe um lasso regularization so lasso\nregression which addresses the\noverfitting you can also consider going\nback and removing more outliers from the\ndata Maybe the outliers that we have\nremoved was not enough so you can also\napply that factor then another thing\nthat you can do is to consider bit more\nadvanced machine learning algorithms\nbecause it can be that um although the\num regression assumption is satisfied\nbut still um using bit more flexible\nMotors like random Forest decision trees\nor boosting techniques will be bit more\nmore appropriate and this will give you\nhigher predictive power consider also uh\nuh working more with this uh scaled uh\nversion or normalization of your data as\nthe next step in your machine Learning\nJourney you can consider learning bit\nmore advanced machine learning models so\nnow when you know in detail what is\nlinear regression and how you can use it\nhow you can train and test a machine\nlearning Model A simple one yet very\npopular one and you also know what is\nlogistic progression and all these\nBasics you're ready to go on to the next\nstep which is learning all the other\npopular traditional machine learning\nmodels think about learning decision\ntrees for modeling nonlinear\nrelationships think about learning\nbagging boosting random forest and\ndifferent sours of optimization\nalgorithms like gradi and descent HGD\nHGD with momentum Adam Adam V RMS prop\nand what is the difference between them\nand how you can Implement them and also\nconsider a learning clustering\napproaches like K means uh DB skin\nhierarchial clust string doing this will\nhelp you uh to get more hands on and go\nto this next step when it comes to the\nmachine learning once you have covered\nall these fundamentals you are ready to\ngo one step further which is getting\ninto deep\nLe thank you for watching this video If\nyou like this content make sure to check\nall the other videos available on this\nchannel and don't forget to subscribe\nlike and comment to help the algorithm\nto make this content more accessible to\neveryone across the world and if you\nwant to get free resources make sure to\ncheck the free resources section at\nlunch. and if you want to become a job\nready data scientist and you are looking\nfor this accessible boot camp that will\nhelp you to make your job ready data\nscientist consider enrolling to the data\nscience boot camp the ultimate data\nscience boot camp at\nl. you will learn all the theory the\nfundamentals to become a jbre data\nscientist you will also implement the\nlearn theory into real world multiple\ndata science projects beside this after\nlearning the theory and practicing it\nwith a real world case studies you will\nalso prepare for your data science\ninterviews and if you want to stay up to\ndate with do recent developments in Tech\nwhat are the headlines that you have\nmissed in the last week what are the\nopen positions currently in the market\nacross the globe and what are the tech\nstartups that are making waves in the\ntech and sure to subscribe to the data\nscience Nai newsletter from\n[Music]\nlunarch\n",
  "words": [
    "machine",
    "learning",
    "course",
    "created",
    "beginners",
    "learning",
    "2024",
    "course",
    "begins",
    "machine",
    "learning",
    "road",
    "map",
    "2024",
    "emphasizing",
    "career",
    "paths",
    "theory",
    "course",
    "moves",
    "practical",
    "applications",
    "comprehensive",
    "end",
    "project",
    "using",
    "python",
    "todd",
    "created",
    "course",
    "experienced",
    "data",
    "science",
    "professional",
    "aim",
    "demystify",
    "machine",
    "learning",
    "concepts",
    "making",
    "accessible",
    "actionable",
    "newcomers",
    "bridge",
    "gap",
    "existing",
    "educational",
    "resources",
    "setting",
    "path",
    "success",
    "evolving",
    "field",
    "machine",
    "learning",
    "looking",
    "step",
    "machine",
    "learning",
    "data",
    "science",
    "starting",
    "somewhere",
    "practical",
    "yet",
    "powerful",
    "introductory",
    "course",
    "machine",
    "learning",
    "beginners",
    "going",
    "cover",
    "basics",
    "machine",
    "learning",
    "going",
    "put",
    "practice",
    "implementing",
    "real",
    "world",
    "case",
    "study",
    "founder",
    "lun",
    "tech",
    "making",
    "data",
    "science",
    "ai",
    "accessible",
    "individuals",
    "businesses",
    "looking",
    "machine",
    "learning",
    "deep",
    "learning",
    "data",
    "science",
    "ai",
    "resources",
    "check",
    "free",
    "resources",
    "section",
    "lunch",
    "youtube",
    "channel",
    "find",
    "content",
    "dive",
    "machine",
    "learning",
    "ai",
    "going",
    "start",
    "machine",
    "learning",
    "road",
    "map",
    "detailed",
    "section",
    "going",
    "discuss",
    "exact",
    "skill",
    "set",
    "need",
    "get",
    "machine",
    "learning",
    "also",
    "going",
    "cover",
    "definition",
    "machine",
    "learning",
    "common",
    "career",
    "path",
    "lot",
    "resources",
    "use",
    "order",
    "get",
    "machine",
    "learning",
    "going",
    "start",
    "actual",
    "theory",
    "going",
    "touch",
    "base",
    "basics",
    "going",
    "learn",
    "different",
    "fundamentals",
    "machine",
    "learning",
    "learned",
    "theory",
    "also",
    "looked",
    "machine",
    "learning",
    "road",
    "map",
    "going",
    "put",
    "theory",
    "practice",
    "going",
    "conduct",
    "endtoend",
    "basic",
    "yet",
    "powerful",
    "case",
    "study",
    "going",
    "implement",
    "linear",
    "aggression",
    "model",
    "going",
    "use",
    "caal",
    "analysis",
    "predictive",
    "analytics",
    "californian",
    "house",
    "prices",
    "going",
    "find",
    "features",
    "drive",
    "californian",
    "house",
    "values",
    "going",
    "discuss",
    "stepbystep",
    "approach",
    "conducting",
    "real",
    "world",
    "data",
    "science",
    "project",
    "end",
    "course",
    "going",
    "know",
    "exact",
    "machine",
    "learning",
    "road",
    "map",
    "2024",
    "exact",
    "skill",
    "set",
    "action",
    "plan",
    "use",
    "get",
    "machine",
    "learning",
    "data",
    "science",
    "going",
    "learn",
    "basics",
    "comes",
    "machine",
    "learning",
    "going",
    "implement",
    "actual",
    "machine",
    "learning",
    "project",
    "end",
    "end",
    "including",
    "implementing",
    "pandas",
    "numai",
    "psychic",
    "learn",
    "touch",
    "models",
    "medal",
    "tap",
    "curn",
    "python",
    "real",
    "world",
    "data",
    "science",
    "project",
    "dive",
    "machine",
    "learning",
    "us",
    "start",
    "simple",
    "start",
    "strong",
    "let",
    "get",
    "started",
    "hi",
    "video",
    "going",
    "talk",
    "get",
    "machine",
    "learning",
    "2024",
    "first",
    "going",
    "start",
    "skills",
    "need",
    "order",
    "get",
    "machine",
    "learning",
    "step",
    "step",
    "topics",
    "need",
    "cover",
    "topics",
    "need",
    "study",
    "order",
    "get",
    "machine",
    "learning",
    "going",
    "talk",
    "machine",
    "learning",
    "going",
    "cover",
    "step",
    "step",
    "exact",
    "topics",
    "skills",
    "need",
    "order",
    "become",
    "machine",
    "learning",
    "researcher",
    "get",
    "machine",
    "learning",
    "going",
    "cover",
    "type",
    "exact",
    "projects",
    "complete",
    "examples",
    "portfolio",
    "projects",
    "order",
    "put",
    "resume",
    "start",
    "apply",
    "machine",
    "learning",
    "related",
    "jobs",
    "going",
    "also",
    "talk",
    "type",
    "industries",
    "get",
    "skills",
    "want",
    "get",
    "machine",
    "learning",
    "exact",
    "career",
    "path",
    "kind",
    "business",
    "titles",
    "usually",
    "related",
    "machine",
    "learning",
    "also",
    "going",
    "talk",
    "average",
    "salary",
    "expect",
    "different",
    "machine",
    "learning",
    "related",
    "positions",
    "end",
    "video",
    "going",
    "know",
    "exactly",
    "machine",
    "learning",
    "used",
    "kind",
    "skills",
    "need",
    "order",
    "get",
    "machine",
    "learning",
    "2024",
    "kind",
    "career",
    "path",
    "kind",
    "compensation",
    "expect",
    "corresponding",
    "business",
    "titles",
    "want",
    "start",
    "career",
    "machine",
    "learning",
    "first",
    "start",
    "definition",
    "machine",
    "learning",
    "machine",
    "learning",
    "different",
    "sorts",
    "applications",
    "machine",
    "learning",
    "likely",
    "heard",
    "know",
    "based",
    "machine",
    "learning",
    "machine",
    "learning",
    "machine",
    "learning",
    "brand",
    "artificial",
    "intelligence",
    "ai",
    "helps",
    "uh",
    "build",
    "models",
    "based",
    "data",
    "learn",
    "data",
    "order",
    "make",
    "different",
    "decisions",
    "used",
    "across",
    "different",
    "industries",
    "uh",
    "starting",
    "healthare",
    "till",
    "entertainment",
    "order",
    "improve",
    "uh",
    "customer",
    "uh",
    "experience",
    "custom",
    "identify",
    "customer",
    "behavior",
    "um",
    "improve",
    "sales",
    "businesses",
    "uh",
    "also",
    "helps",
    "um",
    "governments",
    "make",
    "decisions",
    "really",
    "wide",
    "range",
    "applications",
    "let",
    "start",
    "healthcare",
    "instance",
    "machine",
    "learning",
    "used",
    "healthcare",
    "help",
    "uh",
    "diagnosis",
    "diseases",
    "help",
    "uh",
    "diagnose",
    "cancer",
    "uh",
    "co",
    "helped",
    "many",
    "hospitals",
    "identify",
    "whether",
    "people",
    "getting",
    "uh",
    "severe",
    "side",
    "effects",
    "getting",
    "p",
    "uh",
    "pneumonia",
    "um",
    "based",
    "pictures",
    "based",
    "machine",
    "learning",
    "specifically",
    "comp",
    "computer",
    "vision",
    "uh",
    "healthcare",
    "also",
    "used",
    "drug",
    "discovery",
    "used",
    "personalized",
    "medicine",
    "personalizing",
    "treatment",
    "plans",
    "improve",
    "operations",
    "hospitals",
    "understand",
    "amount",
    "uh",
    "people",
    "uh",
    "patients",
    "hospital",
    "expect",
    "uh",
    "uh",
    "days",
    "per",
    "week",
    "also",
    "estimate",
    "amount",
    "doctors",
    "need",
    "available",
    "amount",
    "uh",
    "people",
    "uh",
    "hospital",
    "expect",
    "emergency",
    "room",
    "based",
    "day",
    "time",
    "day",
    "basically",
    "machine",
    "learning",
    "application",
    "uh",
    "machine",
    "learning",
    "finance",
    "machine",
    "learning",
    "largely",
    "used",
    "finance",
    "different",
    "applications",
    "starting",
    "fraud",
    "detection",
    "credit",
    "cards",
    "sorts",
    "banking",
    "operations",
    "um",
    "also",
    "used",
    "trading",
    "uh",
    "specifically",
    "combination",
    "quantitative",
    "finance",
    "help",
    "traders",
    "make",
    "decisions",
    "need",
    "go",
    "short",
    "long",
    "different",
    "stocks",
    "bonds",
    "different",
    "assets",
    "general",
    "estimate",
    "price",
    "talks",
    "happen",
    "assets",
    "real",
    "time",
    "accurate",
    "way",
    "uh",
    "also",
    "used",
    "uh",
    "retail",
    "uh",
    "helps",
    "understand",
    "estimated",
    "demand",
    "certain",
    "products",
    "certain",
    "warehouses",
    "also",
    "helps",
    "understand",
    "appropriate",
    "closest",
    "uh",
    "uh",
    "warehouses",
    "items",
    "corresponding",
    "customer",
    "shipped",
    "uh",
    "optimizing",
    "operations",
    "also",
    "used",
    "build",
    "different",
    "direct",
    "commander",
    "systems",
    "search",
    "engines",
    "like",
    "famous",
    "amazon",
    "every",
    "time",
    "go",
    "amazon",
    "searching",
    "project",
    "product",
    "likely",
    "see",
    "many",
    "article",
    "recommenders",
    "based",
    "machine",
    "learning",
    "amazon",
    "uh",
    "gathering",
    "data",
    "comparing",
    "behavior",
    "based",
    "bought",
    "based",
    "searching",
    "uh",
    "customers",
    "items",
    "items",
    "order",
    "understand",
    "items",
    "likely",
    "interested",
    "eventually",
    "buy",
    "exactly",
    "based",
    "machine",
    "learning",
    "specifically",
    "different",
    "sorts",
    "recommended",
    "system",
    "algorithm",
    "uh",
    "marketing",
    "machine",
    "learning",
    "heavily",
    "used",
    "help",
    "understand",
    "uh",
    "different",
    "tactics",
    "specific",
    "targeting",
    "uh",
    "groups",
    "belong",
    "retailers",
    "target",
    "uh",
    "order",
    "reduce",
    "marketing",
    "cost",
    "result",
    "high",
    "conversion",
    "rates",
    "ensure",
    "buy",
    "product",
    "machine",
    "learning",
    "autonomous",
    "vehicles",
    "based",
    "machine",
    "learning",
    "specifically",
    "uh",
    "deep",
    "learning",
    "applications",
    "uh",
    "also",
    "um",
    "uh",
    "natural",
    "language",
    "pro",
    "processing",
    "highly",
    "related",
    "famous",
    "chad",
    "gpt",
    "sure",
    "using",
    "based",
    "machine",
    "learning",
    "specifically",
    "large",
    "language",
    "models",
    "transformers",
    "large",
    "language",
    "models",
    "going",
    "providing",
    "text",
    "question",
    "chat",
    "gpt",
    "provide",
    "answer",
    "fact",
    "uh",
    "virtual",
    "assistant",
    "chat",
    "boats",
    "based",
    "machine",
    "learning",
    "also",
    "uh",
    "smart",
    "home",
    "devices",
    "alexa",
    "based",
    "machine",
    "learning",
    "also",
    "agriculture",
    "uh",
    "machine",
    "learning",
    "used",
    "heavily",
    "days",
    "estimate",
    "weather",
    "conditions",
    "uh",
    "understand",
    "uh",
    "production",
    "different",
    "plants",
    "uh",
    "um",
    "outcome",
    "uh",
    "understand",
    "make",
    "decisions",
    "uh",
    "also",
    "optimize",
    "uh",
    "crop",
    "uh",
    "yields",
    "monitor",
    "uh",
    "soil",
    "health",
    "different",
    "sorts",
    "applications",
    "general",
    "uh",
    "improve",
    "uh",
    "revenue",
    "farmer",
    "course",
    "entertainment",
    "vivid",
    "example",
    "netflix",
    "uses",
    "uh",
    "data",
    "uh",
    "providing",
    "uh",
    "related",
    "movies",
    "also",
    "based",
    "kind",
    "movies",
    "watching",
    "netflix",
    "uh",
    "building",
    "super",
    "smart",
    "recommender",
    "system",
    "recommend",
    "movies",
    "likely",
    "interested",
    "also",
    "like",
    "machine",
    "learning",
    "used",
    "actually",
    "super",
    "powerful",
    "topic",
    "super",
    "powerful",
    "uh",
    "field",
    "get",
    "upcoming",
    "10",
    "years",
    "going",
    "grow",
    "made",
    "decision",
    "make",
    "decision",
    "get",
    "machine",
    "learning",
    "continue",
    "watching",
    "video",
    "going",
    "tell",
    "exactly",
    "kind",
    "skills",
    "need",
    "kind",
    "uh",
    "practice",
    "projects",
    "complete",
    "order",
    "get",
    "machine",
    "learning",
    "2024",
    "first",
    "need",
    "start",
    "mathematics",
    "al",
    "also",
    "need",
    "know",
    "python",
    "also",
    "need",
    "know",
    "statistics",
    "need",
    "know",
    "machine",
    "learning",
    "need",
    "know",
    "nlp",
    "get",
    "machine",
    "learning",
    "let",
    "unpack",
    "skill",
    "sets",
    "independent",
    "type",
    "machine",
    "learning",
    "going",
    "need",
    "know",
    "mathematics",
    "specifically",
    "need",
    "know",
    "linear",
    "algebra",
    "need",
    "know",
    "matrix",
    "multiplication",
    "vectors",
    "matrices",
    "dot",
    "product",
    "need",
    "know",
    "uh",
    "multiply",
    "different",
    "matrices",
    "matrix",
    "vectors",
    "different",
    "rules",
    "dimensions",
    "also",
    "mean",
    "transform",
    "matrix",
    "inverse",
    "matrix",
    "identity",
    "matrix",
    "diagonal",
    "matrix",
    "uh",
    "concepts",
    "part",
    "linear",
    "algebra",
    "need",
    "know",
    "part",
    "mathematical",
    "skill",
    "set",
    "order",
    "understand",
    "different",
    "machine",
    "learning",
    "algorithms",
    "part",
    "mathematics",
    "also",
    "need",
    "know",
    "calculus",
    "specifically",
    "differential",
    "theory",
    "need",
    "know",
    "different",
    "theorems",
    "chain",
    "rule",
    "rule",
    "uh",
    "differentiating",
    "sum",
    "instances",
    "constant",
    "multiply",
    "instance",
    "um",
    "uh",
    "sum",
    "also",
    "subtraction",
    "division",
    "multiplication",
    "two",
    "items",
    "need",
    "take",
    "uh",
    "derivative",
    "idea",
    "derivative",
    "idea",
    "partial",
    "derivative",
    "idea",
    "haitian",
    "first",
    "order",
    "derivative",
    "second",
    "order",
    "derivative",
    "would",
    "also",
    "great",
    "know",
    "basic",
    "integration",
    "theory",
    "differentiation",
    "opposite",
    "integration",
    "theory",
    "kind",
    "basic",
    "need",
    "know",
    "uh",
    "much",
    "comes",
    "calculus",
    "basic",
    "things",
    "need",
    "know",
    "uh",
    "order",
    "succeed",
    "machine",
    "learning",
    "uh",
    "next",
    "concepts",
    "uh",
    "discrete",
    "mathematics",
    "need",
    "know",
    "uh",
    "idea",
    "uh",
    "graph",
    "theory",
    "uh",
    "uh",
    "combinations",
    "combinators",
    "uh",
    "uh",
    "idea",
    "complexity",
    "important",
    "want",
    "become",
    "machine",
    "learning",
    "engineer",
    "need",
    "understand",
    "big",
    "notation",
    "need",
    "understand",
    "complexity",
    "uh",
    "n",
    "complexity",
    "n",
    "complexity",
    "n",
    "log",
    "n",
    "um",
    "need",
    "know",
    "uh",
    "basic",
    "um",
    "mathematics",
    "comes",
    "comes",
    "usually",
    "high",
    "school",
    "need",
    "know",
    "multiplication",
    "division",
    "need",
    "understand",
    "uh",
    "multiplying",
    "uh",
    "uh",
    "amounts",
    "within",
    "parentheses",
    "need",
    "understand",
    "um",
    "different",
    "symbols",
    "represent",
    "mathematical",
    "um",
    "values",
    "need",
    "know",
    "idea",
    "using",
    "x",
    "uh",
    "x2",
    "2",
    "x",
    "3",
    "different",
    "exponents",
    "different",
    "var",
    "variables",
    "need",
    "know",
    "logarithm",
    "logarithm",
    "base",
    "two",
    "logarithm",
    "base",
    "e",
    "base",
    "10",
    "uh",
    "idea",
    "e",
    "idea",
    "pi",
    "uh",
    "idea",
    "uh",
    "exponent",
    "logarithm",
    "uh",
    "transform",
    "comes",
    "taking",
    "derivative",
    "logarithm",
    "taking",
    "derivative",
    "uh",
    "exponent",
    "values",
    "uh",
    "topics",
    "actually",
    "quite",
    "basic",
    "might",
    "sound",
    "complicated",
    "actually",
    "someone",
    "explains",
    "uh",
    "clearly",
    "definitely",
    "understand",
    "first",
    "goal",
    "uh",
    "uh",
    "understand",
    "different",
    "mathematical",
    "concepts",
    "linear",
    "algebra",
    "calculus",
    "differential",
    "theory",
    "discrete",
    "mathematics",
    "different",
    "symbols",
    "need",
    "uh",
    "go",
    "instance",
    "uh",
    "look",
    "courses",
    "um",
    "youtube",
    "tutorials",
    "uh",
    "basic",
    "mathematics",
    "uh",
    "machine",
    "learning",
    "ai",
    "uh",
    "go",
    "look",
    "check",
    "instance",
    "academy",
    "uh",
    "quite",
    "favorite",
    "comes",
    "learning",
    "math",
    "uh",
    "uni",
    "students",
    "also",
    "people",
    "want",
    "learn",
    "mathematics",
    "guide",
    "um",
    "check",
    "resources",
    "lear",
    "tech",
    "cuz",
    "going",
    "also",
    "uh",
    "provide",
    "resources",
    "uh",
    "case",
    "want",
    "learn",
    "mathematics",
    "machine",
    "learning",
    "journey",
    "next",
    "skill",
    "set",
    "need",
    "gain",
    "order",
    "break",
    "machine",
    "learning",
    "statistics",
    "need",
    "know",
    "must",
    "statistics",
    "want",
    "get",
    "machine",
    "learning",
    "ai",
    "general",
    "topics",
    "must",
    "um",
    "study",
    "comes",
    "statistics",
    "uh",
    "descriptive",
    "statistics",
    "multivariate",
    "statistics",
    "inferential",
    "statistics",
    "probability",
    "distribution",
    "bial",
    "thinking",
    "let",
    "start",
    "descriptive",
    "statistics",
    "comes",
    "descriptive",
    "statistics",
    "need",
    "know",
    "side",
    "mean",
    "uh",
    "median",
    "standard",
    "deviation",
    "variance",
    "uh",
    "general",
    "uh",
    "analyze",
    "data",
    "using",
    "descriptive",
    "measures",
    "distance",
    "measures",
    "also",
    "variational",
    "measures",
    "next",
    "topic",
    "area",
    "need",
    "know",
    "part",
    "statistical",
    "journey",
    "inferential",
    "statistics",
    "need",
    "know",
    "inf",
    "famous",
    "theories",
    "central",
    "limit",
    "theorem",
    "law",
    "large",
    "numbers",
    "uh",
    "um",
    "relate",
    "idea",
    "population",
    "sample",
    "unbiased",
    "sample",
    "also",
    "uh",
    "hypothesis",
    "testing",
    "confidence",
    "interval",
    "statistical",
    "significance",
    "uh",
    "uh",
    "test",
    "different",
    "theories",
    "using",
    "uh",
    "idea",
    "statistical",
    "significance",
    "uh",
    "power",
    "test",
    "type",
    "one",
    "error",
    "type",
    "two",
    "error",
    "uh",
    "super",
    "important",
    "understanding",
    "different",
    "ses",
    "machine",
    "learning",
    "applications",
    "want",
    "get",
    "machine",
    "learning",
    "probability",
    "distributions",
    "idea",
    "probabilities",
    "understand",
    "different",
    "machine",
    "learning",
    "concepts",
    "need",
    "know",
    "probabilities",
    "idea",
    "probability",
    "idea",
    "sample",
    "versus",
    "population",
    "uh",
    "mean",
    "estimate",
    "probability",
    "different",
    "rules",
    "probability",
    "conditional",
    "probability",
    "uh",
    "um",
    "uh",
    "probability",
    "uh",
    "values",
    "rules",
    "usually",
    "uh",
    "apply",
    "uh",
    "probability",
    "um",
    "multipliers",
    "probability",
    "two",
    "sums",
    "um",
    "uh",
    "need",
    "understand",
    "uh",
    "popular",
    "need",
    "know",
    "popular",
    "probability",
    "distribution",
    "function",
    "perno",
    "distribution",
    "binomial",
    "distribution",
    "uh",
    "normal",
    "distribution",
    "uniform",
    "distribution",
    "exponential",
    "distribution",
    "super",
    "important",
    "distributions",
    "need",
    "know",
    "order",
    "understand",
    "uh",
    "idea",
    "normality",
    "normalization",
    "uh",
    "also",
    "uh",
    "idea",
    "bare",
    "noly",
    "trials",
    "uh",
    "relating",
    "uh",
    "different",
    "probability",
    "distributions",
    "different",
    "uh",
    "uh",
    "higher",
    "level",
    "statistical",
    "concept",
    "steps",
    "rolling",
    "dice",
    "probability",
    "related",
    "bero",
    "distribution",
    "binomial",
    "distribution",
    "super",
    "important",
    "comes",
    "hypothesis",
    "testing",
    "also",
    "uh",
    "many",
    "machine",
    "learning",
    "applications",
    "ban",
    "thinking",
    "super",
    "important",
    "comes",
    "advanced",
    "machine",
    "learning",
    "also",
    "basic",
    "machine",
    "learning",
    "need",
    "know",
    "bas",
    "theorem",
    "arguably",
    "one",
    "popular",
    "statistical",
    "theorems",
    "comparable",
    "also",
    "central",
    "limit",
    "theorem",
    "need",
    "know",
    "conditional",
    "probability",
    "bias",
    "theorem",
    "relate",
    "conditional",
    "probability",
    "uh",
    "uh",
    "bation",
    "uh",
    "statistics",
    "ide",
    "high",
    "level",
    "need",
    "know",
    "everything",
    "uh",
    "super",
    "detailed",
    "need",
    "know",
    "um",
    "concepts",
    "least",
    "high",
    "level",
    "order",
    "understand",
    "machine",
    "learning",
    "learn",
    "statistics",
    "fundamental",
    "concepts",
    "statistics",
    "check",
    "fundamentals",
    "statistics",
    "course",
    "lunch",
    "learn",
    "required",
    "concepts",
    "topics",
    "practice",
    "order",
    "get",
    "machine",
    "learning",
    "gain",
    "statistical",
    "skills",
    "next",
    "skill",
    "set",
    "must",
    "know",
    "fundamentals",
    "machine",
    "learning",
    "covers",
    "basics",
    "machine",
    "learning",
    "also",
    "popular",
    "machine",
    "learning",
    "algorithms",
    "need",
    "know",
    "uh",
    "different",
    "um",
    "mathematical",
    "side",
    "algorithms",
    "step",
    "step",
    "work",
    "benefits",
    "demores",
    "one",
    "use",
    "type",
    "applications",
    "need",
    "know",
    "uh",
    "categorization",
    "supervised",
    "versus",
    "unsupervised",
    "versus",
    "need",
    "know",
    "idea",
    "classification",
    "regression",
    "uh",
    "clustering",
    "need",
    "know",
    "uh",
    "also",
    "time",
    "series",
    "analysis",
    "uh",
    "also",
    "need",
    "know",
    "uh",
    "different",
    "popular",
    "algorithms",
    "including",
    "linear",
    "regression",
    "also",
    "logistic",
    "regression",
    "lda",
    "linear",
    "discriminant",
    "analysis",
    "need",
    "know",
    "knn",
    "uh",
    "need",
    "know",
    "uh",
    "decision",
    "treats",
    "classification",
    "regression",
    "case",
    "need",
    "know",
    "uh",
    "random",
    "forest",
    "begging",
    "also",
    "boosting",
    "popular",
    "boosting",
    "algorithms",
    "like",
    "uh",
    "light",
    "gbm",
    "gbm",
    "uh",
    "gradient",
    "boosting",
    "models",
    "need",
    "know",
    "uh",
    "hg",
    "boost",
    "uh",
    "uh",
    "also",
    "need",
    "know",
    "um",
    "supervised",
    "learning",
    "algorithm",
    "k",
    "means",
    "uh",
    "usually",
    "ed",
    "class",
    "string",
    "need",
    "know",
    "db",
    "scan",
    "becomes",
    "popular",
    "uh",
    "class",
    "string",
    "algorithms",
    "also",
    "need",
    "know",
    "hierarchal",
    "class",
    "string",
    "um",
    "um",
    "type",
    "uh",
    "models",
    "need",
    "understand",
    "idea",
    "behind",
    "advantages",
    "disadvantages",
    "whether",
    "applied",
    "unsupervised",
    "versus",
    "supervised",
    "versus",
    "need",
    "know",
    "whether",
    "regression",
    "classification",
    "uh",
    "class",
    "stre",
    "beside",
    "popular",
    "algorithms",
    "models",
    "also",
    "need",
    "know",
    "basics",
    "uh",
    "training",
    "machine",
    "learning",
    "model",
    "need",
    "know",
    "uh",
    "process",
    "behind",
    "training",
    "validating",
    "testing",
    "machine",
    "learning",
    "algorithms",
    "need",
    "know",
    "uh",
    "mean",
    "uh",
    "perform",
    "hyperparameter",
    "tuning",
    "different",
    "optimization",
    "algorithms",
    "used",
    "optimize",
    "parameters",
    "uh",
    "gd",
    "sgd",
    "sgd",
    "momentum",
    "adam",
    "adam",
    "v",
    "also",
    "need",
    "know",
    "testing",
    "process",
    "idea",
    "splitting",
    "data",
    "train",
    "validation",
    "test",
    "need",
    "know",
    "resampling",
    "techniques",
    "used",
    "including",
    "um",
    "bootstrapping",
    "uh",
    "cross",
    "viation",
    "different",
    "sorts",
    "cross",
    "viation",
    "techniques",
    "one",
    "cross",
    "validation",
    "kful",
    "cross",
    "validation",
    "validation",
    "set",
    "approach",
    "uh",
    "also",
    "need",
    "know",
    "um",
    "uh",
    "idea",
    "uh",
    "matrix",
    "use",
    "different",
    "matrix",
    "evaluate",
    "machine",
    "learning",
    "models",
    "uh",
    "classification",
    "type",
    "metrics",
    "like",
    "f1",
    "score",
    "fb",
    "precision",
    "recall",
    "um",
    "cross",
    "entropy",
    "um",
    "also",
    "need",
    "know",
    "matrix",
    "used",
    "evaluate",
    "regression",
    "type",
    "problems",
    "like",
    "uh",
    "squared",
    "error",
    "mc",
    "root",
    "squared",
    "error",
    "r",
    "mc",
    "uh",
    "maa",
    "absolute",
    "uh",
    "version",
    "different",
    "sorts",
    "errors",
    "um",
    "um",
    "residual",
    "sum",
    "squares",
    "cases",
    "need",
    "know",
    "higher",
    "level",
    "algorithms",
    "uh",
    "topics",
    "concepts",
    "actually",
    "need",
    "know",
    "uh",
    "mathematics",
    "behind",
    "benefits",
    "uh",
    "disadvantage",
    "ages",
    "interviews",
    "definitely",
    "expect",
    "questions",
    "test",
    "uh",
    "high",
    "level",
    "understanding",
    "also",
    "uh",
    "background",
    "knowledge",
    "want",
    "learn",
    "machine",
    "learning",
    "want",
    "gain",
    "skills",
    "uh",
    "feel",
    "free",
    "check",
    "uh",
    "fundamentals",
    "machine",
    "learning",
    "course",
    "lunch",
    "also",
    "check",
    "download",
    "free",
    "fundamentals",
    "machine",
    "learning",
    "handbook",
    "published",
    "free",
    "cord",
    "camp",
    "next",
    "skill",
    "set",
    "definitely",
    "need",
    "gain",
    "knowledge",
    "python",
    "python",
    "actually",
    "one",
    "popular",
    "programming",
    "languages",
    "used",
    "across",
    "software",
    "engineers",
    "uh",
    "ai",
    "engineers",
    "machine",
    "learning",
    "engineers",
    "data",
    "scientists",
    "universal",
    "language",
    "would",
    "say",
    "comes",
    "programming",
    "considering",
    "getting",
    "machine",
    "learning",
    "2024",
    "python",
    "friend",
    "knowing",
    "theory",
    "one",
    "thing",
    "uh",
    "implementing",
    "uh",
    "actual",
    "job",
    "another",
    "exactly",
    "python",
    "comes",
    "handy",
    "need",
    "know",
    "python",
    "order",
    "perform",
    "uh",
    "descriptive",
    "statistics",
    "order",
    "trade",
    "machine",
    "learning",
    "model",
    "advanced",
    "machine",
    "learning",
    "models",
    "deep",
    "learning",
    "models",
    "use",
    "training",
    "validation",
    "uh",
    "testing",
    "models",
    "uh",
    "also",
    "building",
    "different",
    "sorts",
    "applications",
    "python",
    "super",
    "powerful",
    "therefore",
    "also",
    "gaining",
    "high",
    "uh",
    "popularity",
    "across",
    "globe",
    "many",
    "uh",
    "libraries",
    "uh",
    "taner",
    "flow",
    "pie",
    "torch",
    "uh",
    "must",
    "want",
    "get",
    "machine",
    "learning",
    "also",
    "advanced",
    "uh",
    "levels",
    "machine",
    "learning",
    "considering",
    "ai",
    "engineering",
    "jobs",
    "machine",
    "learning",
    "engineering",
    "jobs",
    "uh",
    "want",
    "train",
    "instance",
    "deep",
    "learning",
    "models",
    "uh",
    "want",
    "build",
    "large",
    "l",
    "w",
    "models",
    "generative",
    "ai",
    "models",
    "definitely",
    "need",
    "learn",
    "uh",
    "pytorch",
    "tens",
    "flow",
    "frameworks",
    "use",
    "order",
    "uh",
    "implement",
    "different",
    "deep",
    "learning",
    "uh",
    "advanced",
    "machine",
    "learning",
    "models",
    "libraries",
    "need",
    "know",
    "order",
    "uh",
    "get",
    "machine",
    "learning",
    "definitely",
    "need",
    "know",
    "pandas",
    "napai",
    "need",
    "know",
    "psyit",
    "learn",
    "scipi",
    "also",
    "need",
    "know",
    "uh",
    "nltk",
    "tx",
    "data",
    "also",
    "need",
    "know",
    "tensor",
    "flow",
    "pythor",
    "bit",
    "advanced",
    "machine",
    "learning",
    "um",
    "beside",
    "also",
    "data",
    "visualization",
    "libraries",
    "would",
    "definitely",
    "suggest",
    "practice",
    "met",
    "plot",
    "lip",
    "specifically",
    "pip",
    "plot",
    "also",
    "curn",
    "comes",
    "python",
    "beside",
    "knowing",
    "use",
    "libraries",
    "also",
    "need",
    "know",
    "basic",
    "data",
    "structures",
    "need",
    "know",
    "variables",
    "create",
    "variables",
    "matrices",
    "arrays",
    "indexing",
    "works",
    "also",
    "uh",
    "lists",
    "sets",
    "unique",
    "lists",
    "uh",
    "uh",
    "ways",
    "different",
    "operations",
    "perform",
    "uh",
    "sorting",
    "instance",
    "work",
    "would",
    "definitely",
    "suggest",
    "know",
    "um",
    "basic",
    "data",
    "structures",
    "algorithms",
    "binary",
    "sort",
    "optimal",
    "way",
    "sort",
    "arrays",
    "also",
    "need",
    "know",
    "uh",
    "data",
    "processing",
    "python",
    "need",
    "understand",
    "identify",
    "missing",
    "data",
    "uh",
    "identify",
    "uh",
    "duplicating",
    "data",
    "clean",
    "perform",
    "feature",
    "engineering",
    "combine",
    "uh",
    "multiple",
    "variables",
    "perform",
    "operations",
    "create",
    "new",
    "variables",
    "um",
    "also",
    "need",
    "know",
    "uh",
    "aggregate",
    "data",
    "filter",
    "data",
    "sort",
    "data",
    "course",
    "also",
    "need",
    "know",
    "form",
    "ab",
    "testing",
    "python",
    "train",
    "machine",
    "learning",
    "models",
    "test",
    "evaluate",
    "also",
    "visualize",
    "performance",
    "want",
    "learn",
    "python",
    "easiest",
    "thing",
    "google",
    "uh",
    "python",
    "data",
    "science",
    "python",
    "machine",
    "learning",
    "tutorials",
    "blogs",
    "even",
    "try",
    "python",
    "data",
    "science",
    "course",
    "learner",
    "tech",
    "order",
    "learn",
    "basics",
    "usage",
    "libraries",
    "practical",
    "examples",
    "comes",
    "python",
    "machine",
    "learning",
    "next",
    "skill",
    "set",
    "need",
    "gain",
    "order",
    "get",
    "machine",
    "learning",
    "basic",
    "introduction",
    "nlp",
    "natural",
    "language",
    "processing",
    "need",
    "know",
    "work",
    "text",
    "data",
    "given",
    "days",
    "text",
    "data",
    "cornerstone",
    "different",
    "advanced",
    "algorithms",
    "uh",
    "gpts",
    "transformers",
    "attention",
    "mechanisms",
    "uh",
    "applications",
    "see",
    "part",
    "building",
    "chat",
    "boat",
    "uh",
    "p",
    "uh",
    "applications",
    "based",
    "tex",
    "data",
    "based",
    "nlp",
    "therefore",
    "need",
    "know",
    "basics",
    "nlp",
    "get",
    "started",
    "machine",
    "learning",
    "need",
    "know",
    "uh",
    "idea",
    "text",
    "data",
    "strings",
    "uh",
    "clean",
    "text",
    "data",
    "clean",
    "uh",
    "um",
    "dirty",
    "data",
    "get",
    "steps",
    "involved",
    "lower",
    "casing",
    "uh",
    "removing",
    "punctuation",
    "tokenization",
    "uh",
    "also",
    "idea",
    "stemming",
    "lemmatization",
    "stop",
    "wordss",
    "use",
    "nltk",
    "pyon",
    "order",
    "perform",
    "cleaning",
    "also",
    "need",
    "know",
    "uh",
    "idea",
    "embeddings",
    "uh",
    "also",
    "learn",
    "idea",
    "uh",
    "uh",
    "tfidf",
    "basic",
    "uh",
    "nlp",
    "algorithm",
    "uh",
    "also",
    "uh",
    "learn",
    "idea",
    "word",
    "bings",
    "uh",
    "sub",
    "word",
    "embeddings",
    "uh",
    "character",
    "embeddings",
    "want",
    "learn",
    "basics",
    "nlp",
    "check",
    "concepts",
    "learn",
    "part",
    "blogs",
    "many",
    "tutorials",
    "youtube",
    "also",
    "try",
    "introduction",
    "uh",
    "nlp",
    "course",
    "lunch",
    "order",
    "learn",
    "uh",
    "different",
    "basics",
    "form",
    "nlp",
    "want",
    "go",
    "beyond",
    "uh",
    "intro",
    "till",
    "medium",
    "level",
    "machine",
    "learning",
    "also",
    "want",
    "learn",
    "advanced",
    "machine",
    "learning",
    "something",
    "need",
    "know",
    "gained",
    "preview",
    "skills",
    "mentioned",
    "gain",
    "uh",
    "uh",
    "knowledge",
    "skill",
    "set",
    "learning",
    "deep",
    "learning",
    "also",
    "uh",
    "consider",
    "uh",
    "getting",
    "generative",
    "ai",
    "topics",
    "instance",
    "learn",
    "rnns",
    "ann",
    "cnn",
    "learn",
    "uh",
    "encoder",
    "concept",
    "variational",
    "outen",
    "coders",
    "uh",
    "generative",
    "adversarial",
    "networks",
    "gens",
    "uh",
    "understand",
    "idea",
    "reconstruction",
    "error",
    "uh",
    "understand",
    "um",
    "different",
    "sorts",
    "neural",
    "networks",
    "idea",
    "back",
    "propagation",
    "optimization",
    "algorithms",
    "using",
    "different",
    "optimization",
    "algorithms",
    "gd",
    "hgd",
    "um",
    "hgd",
    "momentum",
    "adam",
    "adamw",
    "rms",
    "prop",
    "uh",
    "uh",
    "also",
    "go",
    "one",
    "step",
    "beyond",
    "uh",
    "get",
    "gener",
    "ai",
    "topics",
    "um",
    "uh",
    "uh",
    "variational",
    "auto",
    "encoders",
    "like",
    "mentioned",
    "also",
    "large",
    "language",
    "models",
    "want",
    "move",
    "towards",
    "nlp",
    "side",
    "generative",
    "ai",
    "want",
    "know",
    "ched",
    "gpt",
    "invented",
    "gpts",
    "work",
    "birth",
    "model",
    "uh",
    "definitely",
    "need",
    "uh",
    "get",
    "topic",
    "language",
    "model",
    "end",
    "grams",
    "attention",
    "mechanism",
    "difference",
    "self",
    "attention",
    "attention",
    "uh",
    "one",
    "head",
    "self",
    "attention",
    "mechanism",
    "self",
    "attention",
    "mechanism",
    "also",
    "need",
    "know",
    "high",
    "level",
    "uh",
    "encoder",
    "decoder",
    "architecture",
    "transformers",
    "need",
    "know",
    "architecture",
    "transformers",
    "solve",
    "different",
    "problems",
    "uh",
    "reur",
    "neuron",
    "networks",
    "rnn",
    "lstms",
    "uh",
    "also",
    "look",
    "uh",
    "uh",
    "uh",
    "encoder",
    "based",
    "decoder",
    "based",
    "algorithm",
    "uh",
    "gpts",
    "birch",
    "model",
    "help",
    "get",
    "machine",
    "learning",
    "also",
    "stand",
    "candidates",
    "advanced",
    "knowledge",
    "let",
    "talk",
    "different",
    "sorts",
    "projects",
    "complete",
    "order",
    "train",
    "machine",
    "learning",
    "skill",
    "set",
    "learned",
    "uh",
    "projects",
    "suggest",
    "complete",
    "put",
    "resume",
    "start",
    "apply",
    "machine",
    "learning",
    "roles",
    "first",
    "application",
    "project",
    "would",
    "suggest",
    "building",
    "basic",
    "recommender",
    "system",
    "whether",
    "job",
    "recommender",
    "system",
    "movie",
    "recommender",
    "system",
    "way",
    "showcase",
    "use",
    "instance",
    "text",
    "data",
    "job",
    "advertisement",
    "use",
    "numeric",
    "data",
    "ratings",
    "movies",
    "order",
    "build",
    "topend",
    "recommender",
    "system",
    "showcase",
    "understanding",
    "distance",
    "measures",
    "cosign",
    "similarity",
    "cann",
    "algorithm",
    "idea",
    "help",
    "uh",
    "uh",
    "tackle",
    "specific",
    "uh",
    "area",
    "data",
    "science",
    "machine",
    "learning",
    "next",
    "project",
    "would",
    "suggest",
    "build",
    "regression",
    "based",
    "model",
    "way",
    "showcase",
    "understand",
    "idea",
    "regression",
    "work",
    "predictive",
    "analytics",
    "predictive",
    "model",
    "dependent",
    "variable",
    "response",
    "variable",
    "numeric",
    "format",
    "instance",
    "uh",
    "estimate",
    "salaries",
    "jobs",
    "based",
    "uh",
    "characteristics",
    "uh",
    "job",
    "based",
    "data",
    "get",
    "instance",
    "uh",
    "open",
    "source",
    "uh",
    "web",
    "pages",
    "keegle",
    "uh",
    "use",
    "different",
    "sorts",
    "regression",
    "algorithms",
    "perform",
    "predictions",
    "salaries",
    "evaluate",
    "model",
    "compare",
    "uh",
    "performance",
    "different",
    "machine",
    "learning",
    "regression",
    "based",
    "algorithms",
    "instance",
    "use",
    "uh",
    "linear",
    "regression",
    "use",
    "decision",
    "trees",
    "regression",
    "version",
    "use",
    "um",
    "uh",
    "random",
    "forest",
    "use",
    "uh",
    "gbm",
    "xgo",
    "order",
    "showcase",
    "one",
    "uh",
    "graph",
    "compare",
    "uh",
    "performance",
    "different",
    "algorithms",
    "using",
    "single",
    "regression",
    "uh",
    "ml",
    "modal",
    "metrics",
    "instance",
    "rmsc",
    "project",
    "showcase",
    "understand",
    "train",
    "regression",
    "model",
    "test",
    "validate",
    "showcase",
    "understanding",
    "optimization",
    "regression",
    "algorithm",
    "understand",
    "concept",
    "hyperparameter",
    "unit",
    "next",
    "project",
    "would",
    "suggest",
    "order",
    "showcase",
    "classification",
    "knowledge",
    "comes",
    "uh",
    "predicting",
    "class",
    "observation",
    "given",
    "uh",
    "feature",
    "space",
    "would",
    "uh",
    "uh",
    "build",
    "classification",
    "model",
    "would",
    "classify",
    "emails",
    "spam",
    "spam",
    "use",
    "publicly",
    "available",
    "data",
    "uh",
    "describing",
    "specific",
    "email",
    "multiple",
    "emails",
    "idea",
    "uh",
    "build",
    "machine",
    "learning",
    "model",
    "would",
    "classify",
    "email",
    "class",
    "zero",
    "class",
    "one",
    "class",
    "zero",
    "instance",
    "uh",
    "spam",
    "one",
    "spam",
    "binary",
    "classification",
    "showcase",
    "know",
    "train",
    "machine",
    "learning",
    "model",
    "classification",
    "purposes",
    "use",
    "instance",
    "logistic",
    "regression",
    "use",
    "also",
    "decision",
    "trea",
    "classification",
    "case",
    "also",
    "use",
    "random",
    "forest",
    "uh",
    "eg",
    "bo",
    "classification",
    "gbm",
    "classification",
    "uh",
    "models",
    "obtain",
    "performance",
    "metrics",
    "uh",
    "f1",
    "score",
    "put",
    "rck",
    "curve",
    "uh",
    "uh",
    "area",
    "curve",
    "metrics",
    "also",
    "compare",
    "different",
    "classification",
    "models",
    "way",
    "also",
    "tackle",
    "another",
    "area",
    "expertise",
    "comes",
    "machine",
    "learning",
    "final",
    "project",
    "would",
    "suggest",
    "would",
    "uh",
    "unsupervised",
    "learning",
    "showcase",
    "another",
    "area",
    "expertise",
    "instance",
    "use",
    "data",
    "customers",
    "good",
    "better",
    "best",
    "customers",
    "based",
    "transaction",
    "history",
    "amount",
    "uh",
    "money",
    "spending",
    "store",
    "uh",
    "case",
    "instance",
    "use",
    "k",
    "means",
    "uh",
    "db",
    "scan",
    "hierarchy",
    "clustering",
    "evaluate",
    "uh",
    "clustering",
    "algorithms",
    "select",
    "one",
    "performs",
    "best",
    "case",
    "cover",
    "yet",
    "another",
    "area",
    "machine",
    "learning",
    "would",
    "super",
    "important",
    "show",
    "case",
    "handle",
    "recommended",
    "systems",
    "supervised",
    "learning",
    "also",
    "unsupervised",
    "learning",
    "reason",
    "suggest",
    "uh",
    "cover",
    "different",
    "areas",
    "complete",
    "four",
    "different",
    "projects",
    "way",
    "covering",
    "different",
    "expertise",
    "areas",
    "machine",
    "learning",
    "also",
    "putting",
    "projects",
    "uh",
    "resume",
    "covering",
    "different",
    "sorts",
    "algorithms",
    "different",
    "sorts",
    "uh",
    "matrix",
    "approaches",
    "show",
    "case",
    "actually",
    "know",
    "lot",
    "machine",
    "learning",
    "want",
    "go",
    "beyond",
    "basic",
    "medium",
    "level",
    "want",
    "considered",
    "medium",
    "advanced",
    "machine",
    "learning",
    "uh",
    "levels",
    "positions",
    "also",
    "need",
    "know",
    "bit",
    "advanced",
    "means",
    "need",
    "complete",
    "bit",
    "advanced",
    "projects",
    "instance",
    "want",
    "apply",
    "generative",
    "ai",
    "related",
    "large",
    "language",
    "models",
    "related",
    "positions",
    "would",
    "suggest",
    "complete",
    "project",
    "building",
    "basic",
    "uh",
    "large",
    "language",
    "model",
    "specifically",
    "process",
    "difficult",
    "one",
    "case",
    "uh",
    "instance",
    "build",
    "baby",
    "gpt",
    "put",
    "link",
    "follow",
    "building",
    "baby",
    "gpt",
    "basic",
    "gpt",
    "algorithm",
    "uh",
    "using",
    "text",
    "data",
    "uh",
    "publicly",
    "available",
    "data",
    "order",
    "uh",
    "uh",
    "process",
    "data",
    "way",
    "like",
    "gpt",
    "encoded",
    "part",
    "transformer",
    "way",
    "showcase",
    "um",
    "hiring",
    "managers",
    "understand",
    "architecture",
    "behind",
    "transformers",
    "architecture",
    "behind",
    "um",
    "uh",
    "large",
    "language",
    "models",
    "gpts",
    "understand",
    "use",
    "pytorch",
    "python",
    "order",
    "advanced",
    "nlp",
    "generative",
    "ai",
    "task",
    "finally",
    "let",
    "talk",
    "common",
    "career",
    "path",
    "business",
    "titles",
    "expect",
    "career",
    "machine",
    "learning",
    "assuming",
    "gained",
    "skills",
    "uh",
    "must",
    "breaking",
    "machine",
    "learning",
    "different",
    "sorts",
    "business",
    "titles",
    "apply",
    "order",
    "get",
    "machine",
    "learning",
    "comes",
    "machine",
    "learning",
    "uh",
    "uh",
    "get",
    "machine",
    "learning",
    "uh",
    "different",
    "fields",
    "covered",
    "part",
    "uh",
    "first",
    "general",
    "machine",
    "learning",
    "researcher",
    "machine",
    "learning",
    "researcher",
    "basically",
    "research",
    "training",
    "testing",
    "evaluating",
    "different",
    "machine",
    "learning",
    "algorithms",
    "usually",
    "people",
    "come",
    "academic",
    "background",
    "mean",
    "get",
    "machine",
    "learning",
    "research",
    "without",
    "getting",
    "degree",
    "statistics",
    "mathematics",
    "um",
    "um",
    "machine",
    "learning",
    "specifically",
    "uh",
    "um",
    "desire",
    "passion",
    "reading",
    "research",
    "uh",
    "mind",
    "reading",
    "uh",
    "research",
    "papers",
    "machine",
    "learning",
    "res",
    "researcher",
    "job",
    "would",
    "good",
    "fit",
    "machine",
    "learning",
    "combined",
    "research",
    "sets",
    "uh",
    "machine",
    "learning",
    "researcher",
    "role",
    "machine",
    "learning",
    "engineer",
    "machine",
    "learning",
    "engineer",
    "engineering",
    "version",
    "machine",
    "learning",
    "uh",
    "expertise",
    "means",
    "combining",
    "machine",
    "learning",
    "skills",
    "engineering",
    "skills",
    "productionizing",
    "pipelines",
    "end",
    "end",
    "robust",
    "pipeline",
    "scalability",
    "model",
    "considering",
    "different",
    "aspects",
    "model",
    "performance",
    "side",
    "comes",
    "quality",
    "algorithm",
    "also",
    "uh",
    "scalability",
    "putting",
    "front",
    "many",
    "users",
    "comes",
    "combining",
    "engineering",
    "machine",
    "learning",
    "get",
    "machine",
    "learning",
    "engineering",
    "someone",
    "software",
    "engineer",
    "want",
    "get",
    "machine",
    "learning",
    "machine",
    "learning",
    "engineering",
    "would",
    "best",
    "fit",
    "machine",
    "learning",
    "engineering",
    "need",
    "different",
    "skills",
    "already",
    "mentioned",
    "also",
    "need",
    "good",
    "grasp",
    "uh",
    "uh",
    "scalability",
    "algorithms",
    "uh",
    "uh",
    "data",
    "structures",
    "algorithms",
    "type",
    "um",
    "skill",
    "set",
    "uh",
    "uh",
    "complexity",
    "moral",
    "uh",
    "also",
    "system",
    "design",
    "one",
    "uh",
    "converges",
    "towards",
    "similar",
    "software",
    "engineering",
    "position",
    "combined",
    "machine",
    "learning",
    "red",
    "pure",
    "machine",
    "learning",
    "ai",
    "role",
    "ai",
    "research",
    "versus",
    "ai",
    "engineering",
    "position",
    "uh",
    "uh",
    "ai",
    "research",
    "position",
    "similar",
    "machine",
    "learning",
    "uh",
    "research",
    "position",
    "ai",
    "engineer",
    "position",
    "similar",
    "machine",
    "learning",
    "engineer",
    "position",
    "single",
    "difference",
    "comes",
    "machine",
    "learning",
    "specifically",
    "talking",
    "traditional",
    "machine",
    "learning",
    "linear",
    "regression",
    "logistic",
    "regression",
    "also",
    "uh",
    "random",
    "forest",
    "exy",
    "boost",
    "begging",
    "comes",
    "ai",
    "research",
    "ai",
    "engineer",
    "position",
    "tackling",
    "advanced",
    "machine",
    "learning",
    "talking",
    "deep",
    "learning",
    "models",
    "rnn",
    "lstms",
    "grus",
    "cnn",
    "computer",
    "vision",
    "applications",
    "also",
    "talking",
    "uh",
    "generative",
    "ai",
    "models",
    "large",
    "language",
    "models",
    "uh",
    "talking",
    "um",
    "transformers",
    "implementation",
    "transformers",
    "gbts",
    "t5",
    "different",
    "algorithms",
    "uh",
    "advanced",
    "uh",
    "ai",
    "topics",
    "rather",
    "traditional",
    "machine",
    "learning",
    "uh",
    "applying",
    "ai",
    "research",
    "ai",
    "engineering",
    "positions",
    "finally",
    "different",
    "sorts",
    "obervations",
    "niches",
    "ai",
    "instance",
    "nlp",
    "research",
    "nlp",
    "engineer",
    "even",
    "data",
    "science",
    "positions",
    "need",
    "know",
    "machine",
    "learning",
    "knowing",
    "machine",
    "learning",
    "set",
    "apart",
    "source",
    "positions",
    "also",
    "business",
    "titles",
    "data",
    "science",
    "technical",
    "data",
    "science",
    "positions",
    "nlp",
    "researcher",
    "nlp",
    "engineer",
    "uh",
    "need",
    "know",
    "machine",
    "learning",
    "knowing",
    "machine",
    "learning",
    "help",
    "break",
    "positions",
    "career",
    "paths",
    "want",
    "prepare",
    "deep",
    "learning",
    "interviews",
    "instance",
    "want",
    "get",
    "ai",
    "engineering",
    "ai",
    "research",
    "recently",
    "published",
    "free",
    "full",
    "course",
    "100",
    "interview",
    "questions",
    "answers",
    "span",
    "hours",
    "help",
    "prepare",
    "deep",
    "learning",
    "interviews",
    "machine",
    "learning",
    "interviews",
    "check",
    "uh",
    "fundamentals",
    "machine",
    "learning",
    "course",
    "lunch",
    "uh",
    "download",
    "machine",
    "learning",
    "fundamentals",
    "handbook",
    "free",
    "cod",
    "camp",
    "check",
    "blogs",
    "also",
    "free",
    "resources",
    "lunch",
    "ai",
    "order",
    "prepare",
    "interviews",
    "order",
    "get",
    "machine",
    "learning",
    "let",
    "talk",
    "talk",
    "list",
    "resources",
    "use",
    "order",
    "get",
    "machine",
    "learning",
    "2024",
    "learn",
    "statistics",
    "fundamental",
    "concepts",
    "statistics",
    "check",
    "fundamental",
    "statistics",
    "course",
    "lunch",
    "learn",
    "required",
    "concepts",
    "topics",
    "practice",
    "order",
    "get",
    "machine",
    "learning",
    "gain",
    "statistical",
    "skills",
    "want",
    "learn",
    "machine",
    "learning",
    "check",
    "fundamentals",
    "learning",
    "course",
    "lunch",
    "get",
    "basic",
    "concepts",
    "fundamentals",
    "machine",
    "learning",
    "list",
    "comprehensive",
    "comprehensive",
    "list",
    "machine",
    "learning",
    "algorithms",
    "part",
    "course",
    "also",
    "check",
    "introduction",
    "nlp",
    "course",
    "lunch",
    "order",
    "learn",
    "basic",
    "concepts",
    "behind",
    "natural",
    "language",
    "preprocessing",
    "finally",
    "want",
    "learn",
    "python",
    "specifically",
    "python",
    "ral",
    "learning",
    "check",
    "python",
    "data",
    "science",
    "course",
    "lunch",
    "want",
    "get",
    "access",
    "different",
    "projects",
    "practice",
    "machine",
    "learning",
    "skills",
    "learned",
    "either",
    "check",
    "ultimate",
    "data",
    "science",
    "boot",
    "camp",
    "covers",
    "specific",
    "course",
    "uh",
    "data",
    "science",
    "uh",
    "project",
    "portfolio",
    "course",
    "covering",
    "multiple",
    "projects",
    "train",
    "machine",
    "learning",
    "skills",
    "put",
    "resume",
    "also",
    "check",
    "github",
    "account",
    "linkedin",
    "account",
    "cover",
    "many",
    "case",
    "studies",
    "including",
    "baby",
    "gpt",
    "also",
    "put",
    "link",
    "course",
    "uh",
    "case",
    "study",
    "link",
    "gained",
    "skills",
    "ready",
    "get",
    "machine",
    "learning",
    "2024",
    "lecture",
    "go",
    "basic",
    "concepts",
    "machine",
    "learning",
    "needed",
    "understand",
    "follow",
    "conversations",
    "solve",
    "main",
    "problems",
    "using",
    "machine",
    "learning",
    "strong",
    "understanding",
    "machine",
    "learning",
    "basics",
    "important",
    "step",
    "anyone",
    "looking",
    "learn",
    "work",
    "machine",
    "learning",
    "looking",
    "three",
    "concepts",
    "tutorial",
    "define",
    "look",
    "difference",
    "supervised",
    "unsupervised",
    "machine",
    "learning",
    "models",
    "look",
    "difference",
    "regression",
    "classification",
    "type",
    "machine",
    "learning",
    "models",
    "look",
    "process",
    "training",
    "machine",
    "learning",
    "models",
    "scratch",
    "evaluate",
    "introducing",
    "performance",
    "metrics",
    "use",
    "depending",
    "type",
    "machine",
    "learning",
    "model",
    "problem",
    "dealing",
    "whether",
    "supervised",
    "unsupervised",
    "whether",
    "regression",
    "versus",
    "classification",
    "type",
    "problem",
    "machine",
    "learning",
    "methods",
    "categorized",
    "two",
    "types",
    "depending",
    "existence",
    "label",
    "data",
    "training",
    "data",
    "set",
    "especially",
    "important",
    "training",
    "process",
    "talking",
    "dependent",
    "variable",
    "section",
    "fundamental",
    "su",
    "statistics",
    "supervised",
    "unsupervised",
    "machine",
    "learning",
    "models",
    "two",
    "main",
    "type",
    "machine",
    "learning",
    "algorithms",
    "one",
    "key",
    "difference",
    "two",
    "level",
    "supervision",
    "training",
    "phase",
    "supervised",
    "machine",
    "learning",
    "algorithms",
    "guided",
    "labeled",
    "examples",
    "supervised",
    "algorithms",
    "learning",
    "model",
    "reliable",
    "also",
    "requires",
    "larger",
    "amount",
    "labeled",
    "data",
    "timec",
    "consuming",
    "quite",
    "expensive",
    "obtain",
    "examples",
    "supervised",
    "machine",
    "learning",
    "models",
    "include",
    "regression",
    "classification",
    "type",
    "models",
    "hand",
    "unsupervised",
    "machine",
    "learning",
    "algorithms",
    "trained",
    "unlabeled",
    "data",
    "model",
    "must",
    "find",
    "patterns",
    "relationships",
    "data",
    "without",
    "guidance",
    "correct",
    "outputs",
    "longer",
    "dependent",
    "variable",
    "unsupervised",
    "ml",
    "models",
    "require",
    "training",
    "data",
    "consists",
    "independent",
    "variables",
    "features",
    "dependent",
    "variable",
    "label",
    "data",
    "supervise",
    "algorithm",
    "learning",
    "data",
    "examples",
    "unsupervised",
    "models",
    "clust",
    "string",
    "models",
    "outlier",
    "detection",
    "techniques",
    "supervised",
    "machine",
    "learning",
    "methods",
    "categorized",
    "two",
    "types",
    "depending",
    "type",
    "dependent",
    "variable",
    "predicting",
    "regression",
    "type",
    "classification",
    "type",
    "key",
    "differences",
    "regression",
    "classification",
    "include",
    "output",
    "type",
    "regression",
    "algorithms",
    "predict",
    "continuous",
    "values",
    "classification",
    "algorithms",
    "predict",
    "categorized",
    "values",
    "key",
    "difference",
    "regression",
    "classification",
    "include",
    "output",
    "type",
    "evaluation",
    "metrics",
    "applications",
    "regards",
    "output",
    "type",
    "regression",
    "algorithms",
    "predict",
    "continuous",
    "values",
    "classification",
    "algorithms",
    "predict",
    "categorical",
    "values",
    "regard",
    "evaluation",
    "metric",
    "different",
    "evaluation",
    "metrics",
    "used",
    "regression",
    "classification",
    "tasks",
    "example",
    "mean",
    "square",
    "commonly",
    "used",
    "evaluate",
    "regression",
    "models",
    "accuracy",
    "commonly",
    "used",
    "evaluate",
    "classification",
    "models",
    "comes",
    "applications",
    "regression",
    "classification",
    "models",
    "used",
    "entirely",
    "different",
    "types",
    "applications",
    "regression",
    "models",
    "often",
    "used",
    "prediction",
    "tests",
    "classifications",
    "used",
    "decision",
    "making",
    "tasks",
    "progression",
    "algorithms",
    "used",
    "predict",
    "continuous",
    "value",
    "price",
    "probability",
    "example",
    "regression",
    "model",
    "might",
    "used",
    "predict",
    "price",
    "house",
    "based",
    "size",
    "location",
    "features",
    "examples",
    "regression",
    "type",
    "machine",
    "learning",
    "models",
    "linear",
    "regression",
    "fixed",
    "effect",
    "regression",
    "exus",
    "regression",
    "etc",
    "classification",
    "algorithms",
    "hand",
    "used",
    "predict",
    "categorical",
    "value",
    "algorithms",
    "take",
    "input",
    "classify",
    "one",
    "several",
    "predetermined",
    "categories",
    "example",
    "classification",
    "model",
    "might",
    "used",
    "classify",
    "emails",
    "spam",
    "spam",
    "identify",
    "type",
    "animal",
    "image",
    "examples",
    "classification",
    "type",
    "machine",
    "learning",
    "models",
    "logistic",
    "regression",
    "exus",
    "classification",
    "random",
    "forest",
    "classification",
    "let",
    "us",
    "look",
    "different",
    "typee",
    "performance",
    "metrics",
    "use",
    "order",
    "evaluate",
    "different",
    "type",
    "machine",
    "learning",
    "models",
    "aggression",
    "models",
    "common",
    "evaluation",
    "matrix",
    "includes",
    "residual",
    "sum",
    "squared",
    "rss",
    "mean",
    "squared",
    "error",
    "msse",
    "root",
    "mean",
    "squared",
    "error",
    "rmsc",
    "mean",
    "absolute",
    "error",
    "ae",
    "metrix",
    "measure",
    "difference",
    "predicted",
    "values",
    "true",
    "values",
    "lower",
    "value",
    "indicating",
    "better",
    "feed",
    "model",
    "let",
    "go",
    "metrics",
    "one",
    "one",
    "first",
    "one",
    "rss",
    "residual",
    "sum",
    "squares",
    "matrix",
    "commonly",
    "used",
    "setting",
    "linear",
    "regression",
    "evaluating",
    "performance",
    "model",
    "estimating",
    "different",
    "coefficients",
    "beta",
    "coefficient",
    "yi",
    "dependent",
    "variable",
    "value",
    "head",
    "predicted",
    "value",
    "see",
    "rss",
    "residual",
    "sum",
    "square",
    "beta",
    "equal",
    "sum",
    "squ",
    "ius",
    "hat",
    "across",
    "equal",
    "1",
    "n",
    "index",
    "r",
    "individual",
    "observation",
    "included",
    "data",
    "second",
    "matrix",
    "mean",
    "squared",
    "error",
    "average",
    "squared",
    "differences",
    "predicted",
    "values",
    "true",
    "values",
    "see",
    "equal",
    "1",
    "n",
    "sum",
    "across",
    "minus",
    "head",
    "squ",
    "see",
    "rss",
    "msse",
    "quite",
    "similar",
    "terms",
    "uh",
    "formulas",
    "difference",
    "adding",
    "1",
    "n",
    "makes",
    "average",
    "across",
    "square",
    "differences",
    "predicted",
    "value",
    "actual",
    "true",
    "valum",
    "lower",
    "value",
    "msse",
    "indicates",
    "better",
    "fit",
    "rmsc",
    "root",
    "mean",
    "squared",
    "error",
    "square",
    "root",
    "msse",
    "see",
    "formula",
    "msse",
    "difference",
    "adding",
    "square",
    "roof",
    "top",
    "formula",
    "lower",
    "value",
    "rmsc",
    "indicates",
    "better",
    "fit",
    "finally",
    "mae",
    "mean",
    "absolute",
    "error",
    "average",
    "absolute",
    "difference",
    "predicted",
    "values",
    "hat",
    "true",
    "values",
    "lower",
    "value",
    "indicates",
    "better",
    "fit",
    "choice",
    "regression",
    "metrics",
    "depends",
    "specific",
    "problem",
    "trying",
    "solve",
    "nature",
    "data",
    "instance",
    "mse",
    "commonly",
    "used",
    "want",
    "penalize",
    "large",
    "errors",
    "small",
    "ones",
    "mse",
    "sensitive",
    "outliers",
    "means",
    "may",
    "best",
    "choice",
    "data",
    "contains",
    "many",
    "outliers",
    "extreme",
    "values",
    "rmsc",
    "hand",
    "square",
    "root",
    "msc",
    "makes",
    "easier",
    "interpret",
    "easier",
    "interpretable",
    "units",
    "target",
    "variable",
    "commonly",
    "used",
    "want",
    "compare",
    "performance",
    "different",
    "models",
    "want",
    "report",
    "error",
    "way",
    "easier",
    "understand",
    "explain",
    "mia",
    "commonly",
    "used",
    "want",
    "penalize",
    "errors",
    "equally",
    "regardless",
    "magnitude",
    "mia",
    "less",
    "sensitive",
    "outliers",
    "compared",
    "msse",
    "classification",
    "models",
    "common",
    "evaluation",
    "metrics",
    "include",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "metrics",
    "measure",
    "ability",
    "machine",
    "learning",
    "model",
    "correctly",
    "classify",
    "instances",
    "correct",
    "categories",
    "let",
    "briefly",
    "look",
    "metrix",
    "individually",
    "accuracy",
    "proportion",
    "correct",
    "predictions",
    "made",
    "model",
    "calculated",
    "taking",
    "correct",
    "predictions",
    "correct",
    "number",
    "predictions",
    "divide",
    "two",
    "number",
    "predictions",
    "means",
    "correct",
    "predictions",
    "plus",
    "incorrect",
    "predictions",
    "next",
    "look",
    "precision",
    "precision",
    "proportion",
    "true",
    "positive",
    "predictions",
    "among",
    "positive",
    "predictions",
    "made",
    "model",
    "equal",
    "true",
    "positive",
    "divided",
    "true",
    "positive",
    "plus",
    "false",
    "positive",
    "number",
    "positives",
    "true",
    "positives",
    "cases",
    "model",
    "correctly",
    "predict",
    "positive",
    "outcome",
    "false",
    "positives",
    "cases",
    "model",
    "incorrectly",
    "predict",
    "positive",
    "outcome",
    "next",
    "matrix",
    "recall",
    "recall",
    "proportion",
    "true",
    "positive",
    "predictions",
    "among",
    "actual",
    "positive",
    "instances",
    "calculated",
    "number",
    "true",
    "positive",
    "predictions",
    "divided",
    "total",
    "number",
    "actual",
    "positive",
    "instances",
    "means",
    "dividing",
    "true",
    "positive",
    "true",
    "positive",
    "plus",
    "false",
    "negative",
    "example",
    "let",
    "say",
    "looking",
    "medical",
    "test",
    "true",
    "positive",
    "would",
    "case",
    "correctly",
    "identifies",
    "patient",
    "disease",
    "false",
    "positive",
    "would",
    "case",
    "test",
    "incorrectly",
    "identifies",
    "healthy",
    "patient",
    "disease",
    "final",
    "score",
    "f1",
    "score",
    "f1",
    "score",
    "harmonic",
    "mean",
    "usual",
    "mean",
    "precision",
    "recall",
    "higher",
    "value",
    "indicating",
    "better",
    "balance",
    "precision",
    "recall",
    "calculated",
    "two",
    "times",
    "recall",
    "times",
    "precision",
    "divided",
    "recall",
    "plus",
    "precision",
    "unsupervised",
    "models",
    "class",
    "string",
    "models",
    "whose",
    "performance",
    "typically",
    "evaluated",
    "using",
    "metrics",
    "measure",
    "similarity",
    "data",
    "points",
    "within",
    "cluster",
    "dis",
    "similarity",
    "data",
    "points",
    "different",
    "clusters",
    "three",
    "type",
    "metrics",
    "use",
    "homogeneity",
    "measure",
    "degree",
    "data",
    "points",
    "within",
    "single",
    "cluster",
    "belong",
    "class",
    "higher",
    "value",
    "indicates",
    "homogeneous",
    "cluster",
    "see",
    "homogeneity",
    "age",
    "age",
    "simply",
    "short",
    "way",
    "describing",
    "homogeneity",
    "equal",
    "one",
    "minus",
    "conditional",
    "entropy",
    "given",
    "cluster",
    "assignments",
    "divided",
    "entropy",
    "predicted",
    "class",
    "wondering",
    "entropy",
    "stay",
    "tuned",
    "going",
    "discuss",
    "entropy",
    "whenever",
    "discuss",
    "clustering",
    "well",
    "decision",
    "trees",
    "x",
    "matrix",
    "silid",
    "score",
    "silid",
    "score",
    "measure",
    "similarity",
    "data",
    "point",
    "cluster",
    "compared",
    "clusters",
    "higher",
    "silid",
    "score",
    "indicates",
    "data",
    "point",
    "well",
    "matched",
    "cluster",
    "usually",
    "used",
    "db",
    "scan",
    "k",
    "silhouette",
    "score",
    "represented",
    "formula",
    "silhouette",
    "score",
    "equal",
    "b",
    "minus",
    "ao",
    "divided",
    "maximum",
    "ao",
    "b",
    "silo",
    "coefficient",
    "data",
    "point",
    "characterized",
    "ao",
    "average",
    "distance",
    "data",
    "points",
    "cluster",
    "belongs",
    "b",
    "minimum",
    "average",
    "distance",
    "clusters",
    "belong",
    "final",
    "metrix",
    "look",
    "completeness",
    "completeness",
    "another",
    "measure",
    "degree",
    "data",
    "points",
    "belongs",
    "particular",
    "class",
    "assigned",
    "cluster",
    "higher",
    "value",
    "indicates",
    "compete",
    "cluster",
    "let",
    "conclude",
    "lecture",
    "going",
    "process",
    "evaluating",
    "machine",
    "learning",
    "model",
    "simplified",
    "version",
    "since",
    "many",
    "additional",
    "considerations",
    "techniques",
    "may",
    "needed",
    "depending",
    "specific",
    "task",
    "characteristics",
    "data",
    "knowing",
    "properly",
    "train",
    "machine",
    "learning",
    "model",
    "really",
    "important",
    "since",
    "defines",
    "accuracy",
    "results",
    "conclusions",
    "make",
    "training",
    "pro",
    "process",
    "starts",
    "preparing",
    "data",
    "includes",
    "splitting",
    "data",
    "training",
    "test",
    "sets",
    "using",
    "advanced",
    "resampling",
    "techniques",
    "talk",
    "later",
    "splitting",
    "data",
    "multiple",
    "sets",
    "training",
    "set",
    "data",
    "used",
    "feed",
    "model",
    "also",
    "validation",
    "set",
    "validation",
    "set",
    "used",
    "optimize",
    "hyperparameters",
    "pick",
    "best",
    "model",
    "test",
    "set",
    "use",
    "evaluate",
    "model",
    "performance",
    "approach",
    "lectures",
    "section",
    "talk",
    "detail",
    "different",
    "techniques",
    "well",
    "training",
    "means",
    "test",
    "means",
    "validation",
    "means",
    "well",
    "hyperparameter",
    "tuning",
    "means",
    "secondly",
    "need",
    "choose",
    "algorithm",
    "set",
    "algorithms",
    "train",
    "model",
    "training",
    "data",
    "save",
    "fitted",
    "model",
    "many",
    "different",
    "algorithms",
    "choose",
    "appropriate",
    "algorithm",
    "depend",
    "specific",
    "test",
    "task",
    "characteristics",
    "data",
    "third",
    "step",
    "need",
    "adjust",
    "model",
    "parameters",
    "minimize",
    "error",
    "training",
    "set",
    "performing",
    "hyperparameter",
    "tuning",
    "need",
    "use",
    "validation",
    "data",
    "select",
    "best",
    "model",
    "results",
    "least",
    "possible",
    "validation",
    "error",
    "rate",
    "step",
    "want",
    "look",
    "optimal",
    "set",
    "parameters",
    "included",
    "part",
    "model",
    "end",
    "model",
    "least",
    "possible",
    "error",
    "performs",
    "best",
    "possible",
    "way",
    "final",
    "two",
    "steps",
    "need",
    "evaluate",
    "model",
    "always",
    "interested",
    "test",
    "rate",
    "training",
    "validation",
    "error",
    "rates",
    "used",
    "test",
    "set",
    "used",
    "training",
    "validation",
    "sets",
    "test",
    "error",
    "rate",
    "give",
    "idea",
    "well",
    "model",
    "generalize",
    "new",
    "unseen",
    "data",
    "need",
    "use",
    "optimal",
    "set",
    "parameters",
    "hyperparameter",
    "tuning",
    "stage",
    "training",
    "data",
    "train",
    "model",
    "hyper",
    "parameters",
    "best",
    "model",
    "use",
    "best",
    "fitted",
    "model",
    "get",
    "predictions",
    "test",
    "data",
    "help",
    "us",
    "calculate",
    "test",
    "error",
    "rate",
    "calculated",
    "test",
    "error",
    "rate",
    "also",
    "obtained",
    "best",
    "model",
    "ready",
    "save",
    "predictions",
    "satisfied",
    "model",
    "performance",
    "tuned",
    "parameters",
    "use",
    "make",
    "predictions",
    "new",
    "unseen",
    "data",
    "test",
    "data",
    "compute",
    "performance",
    "metrics",
    "model",
    "us",
    "predictions",
    "real",
    "values",
    "target",
    "variable",
    "test",
    "data",
    "complete",
    "lecture",
    "lecture",
    "spoken",
    "basics",
    "machine",
    "learning",
    "discussed",
    "difference",
    "unsupervised",
    "supervised",
    "learning",
    "models",
    "well",
    "regression",
    "versus",
    "classification",
    "discussed",
    "details",
    "different",
    "type",
    "performance",
    "metrics",
    "use",
    "evaluate",
    "different",
    "type",
    "machine",
    "learning",
    "models",
    "well",
    "looked",
    "simplified",
    "version",
    "process",
    "train",
    "machine",
    "machine",
    "learning",
    "model",
    "lecture",
    "lecture",
    "number",
    "two",
    "discuss",
    "important",
    "concepts",
    "need",
    "know",
    "considering",
    "applying",
    "statistical",
    "machine",
    "learning",
    "model",
    "talking",
    "bias",
    "model",
    "variance",
    "model",
    "trade",
    "two",
    "call",
    "bias",
    "various",
    "trade",
    "whenever",
    "using",
    "statistical",
    "econometrical",
    "machine",
    "learning",
    "model",
    "matter",
    "simple",
    "model",
    "always",
    "evaluate",
    "model",
    "check",
    "error",
    "rate",
    "cases",
    "comes",
    "make",
    "variance",
    "model",
    "bias",
    "model",
    "always",
    "catch",
    "comes",
    "model",
    "choice",
    "performance",
    "let",
    "us",
    "firstly",
    "define",
    "bias",
    "variant",
    "machine",
    "learning",
    "model",
    "inability",
    "model",
    "capture",
    "true",
    "relationship",
    "data",
    "called",
    "bias",
    "hence",
    "machine",
    "learning",
    "models",
    "able",
    "detect",
    "true",
    "relationship",
    "data",
    "low",
    "bias",
    "usually",
    "complex",
    "models",
    "flexible",
    "models",
    "tend",
    "lower",
    "bias",
    "simpler",
    "models",
    "mathematically",
    "bias",
    "model",
    "expressed",
    "expectation",
    "difference",
    "estimate",
    "true",
    "value",
    "let",
    "us",
    "also",
    "define",
    "variance",
    "model",
    "variance",
    "model",
    "inconsistency",
    "level",
    "variability",
    "model",
    "performance",
    "applying",
    "model",
    "different",
    "data",
    "sets",
    "model",
    "trained",
    "using",
    "training",
    "data",
    "performs",
    "entirely",
    "differently",
    "test",
    "data",
    "means",
    "large",
    "variation",
    "variance",
    "model",
    "complex",
    "models",
    "flexible",
    "models",
    "tend",
    "higher",
    "variance",
    "simpler",
    "models",
    "order",
    "evaluate",
    "performance",
    "model",
    "need",
    "look",
    "amount",
    "error",
    "model",
    "making",
    "simplicity",
    "let",
    "assume",
    "following",
    "simple",
    "regression",
    "model",
    "aims",
    "use",
    "single",
    "independent",
    "variable",
    "x",
    "model",
    "numeric",
    "dependent",
    "variable",
    "fit",
    "model",
    "training",
    "observations",
    "pair",
    "independent",
    "dependent",
    "variables",
    "x1",
    "y1",
    "x2",
    "y2",
    "xn",
    "yn",
    "obtain",
    "estimate",
    "training",
    "observations",
    "fhe",
    "compute",
    "let",
    "say",
    "fhe",
    "x1",
    "fhe",
    "x2",
    "fhe",
    "xn",
    "estimat",
    "dependent",
    "variable",
    "y1",
    "y2",
    "yn",
    "approximately",
    "equal",
    "actual",
    "values",
    "one",
    "head",
    "approximately",
    "equal",
    "y1",
    "y2",
    "head",
    "approximately",
    "equal",
    "y2",
    "head",
    "etc",
    "training",
    "error",
    "rate",
    "would",
    "small",
    "however",
    "really",
    "interested",
    "whether",
    "model",
    "predicting",
    "dependent",
    "variable",
    "appropriately",
    "want",
    "instead",
    "looking",
    "training",
    "error",
    "rate",
    "want",
    "look",
    "test",
    "error",
    "rate",
    "error",
    "rate",
    "model",
    "expected",
    "square",
    "difference",
    "real",
    "test",
    "values",
    "prediction",
    "predictions",
    "made",
    "using",
    "machine",
    "learning",
    "model",
    "rewrite",
    "aor",
    "rate",
    "sum",
    "two",
    "quantities",
    "see",
    "left",
    "part",
    "amount",
    "fx",
    "minus",
    "f",
    "hat",
    "squared",
    "second",
    "entity",
    "variance",
    "error",
    "term",
    "accuracy",
    "head",
    "prediction",
    "depends",
    "two",
    "quantities",
    "call",
    "reducible",
    "error",
    "irreducible",
    "error",
    "reducible",
    "error",
    "equal",
    "fx",
    "minus",
    "f",
    "x",
    "irreducible",
    "error",
    "variance",
    "epsilon",
    "accuracy",
    "head",
    "prediction",
    "depends",
    "two",
    "quantities",
    "call",
    "reducible",
    "error",
    "irreducible",
    "error",
    "general",
    "fad",
    "perfect",
    "estimate",
    "f",
    "inaccuracy",
    "introduce",
    "errors",
    "error",
    "reducible",
    "since",
    "potentially",
    "improve",
    "accuracy",
    "fad",
    "using",
    "appropriate",
    "machine",
    "learning",
    "model",
    "best",
    "version",
    "estimate",
    "f",
    "however",
    "even",
    "possible",
    "find",
    "model",
    "would",
    "estimate",
    "f",
    "perfectly",
    "estimated",
    "response",
    "took",
    "form",
    "head",
    "equal",
    "fx",
    "prediction",
    "would",
    "still",
    "error",
    "happens",
    "also",
    "function",
    "error",
    "rate",
    "epsilon",
    "definition",
    "predicted",
    "using",
    "feature",
    "x",
    "always",
    "error",
    "predictable",
    "variability",
    "associated",
    "error",
    "epsilon",
    "also",
    "affects",
    "accuracy",
    "predictions",
    "known",
    "irreducible",
    "error",
    "matter",
    "well",
    "estimate",
    "f",
    "reduce",
    "error",
    "introduced",
    "epsilon",
    "error",
    "contains",
    "features",
    "included",
    "model",
    "unknown",
    "factors",
    "influence",
    "dependent",
    "variable",
    "included",
    "part",
    "data",
    "ca",
    "reduce",
    "reducible",
    "error",
    "rate",
    "based",
    "two",
    "values",
    "variance",
    "estimate",
    "bias",
    "model",
    "simplify",
    "mathematical",
    "expression",
    "describing",
    "error",
    "rate",
    "got",
    "equal",
    "variance",
    "model",
    "plus",
    "squared",
    "bias",
    "model",
    "plus",
    "irreducible",
    "error",
    "even",
    "reduce",
    "irreducible",
    "error",
    "reduce",
    "reducible",
    "error",
    "rate",
    "based",
    "two",
    "values",
    "variance",
    "squared",
    "bias",
    "though",
    "mathematical",
    "derivation",
    "scope",
    "course",
    "keep",
    "mind",
    "reducible",
    "error",
    "model",
    "described",
    "sum",
    "variance",
    "model",
    "squared",
    "bias",
    "model",
    "mathematically",
    "error",
    "supervised",
    "machine",
    "learning",
    "model",
    "equal",
    "squared",
    "bias",
    "model",
    "variance",
    "model",
    "irreducible",
    "error",
    "therefore",
    "order",
    "minimize",
    "expected",
    "test",
    "error",
    "rate",
    "unseen",
    "data",
    "need",
    "select",
    "machine",
    "learning",
    "meod",
    "simultaneously",
    "achieves",
    "low",
    "variance",
    "low",
    "bias",
    "exactly",
    "call",
    "called",
    "bias",
    "variance",
    "tradeoff",
    "problem",
    "negative",
    "correlation",
    "variance",
    "bias",
    "model",
    "another",
    "thing",
    "highly",
    "related",
    "bias",
    "variance",
    "model",
    "flexibility",
    "machine",
    "learning",
    "model",
    "flexibility",
    "machine",
    "learning",
    "model",
    "direct",
    "impact",
    "variance",
    "bias",
    "let",
    "look",
    "relationships",
    "one",
    "one",
    "complex",
    "models",
    "flexible",
    "models",
    "tend",
    "lower",
    "bias",
    "time",
    "complex",
    "models",
    "flexible",
    "models",
    "tend",
    "higher",
    "variance",
    "simpler",
    "models",
    "flexibility",
    "model",
    "increases",
    "model",
    "finds",
    "true",
    "patterns",
    "data",
    "easier",
    "reduces",
    "bias",
    "model",
    "time",
    "variance",
    "models",
    "increases",
    "flexibility",
    "model",
    "decreases",
    "model",
    "finds",
    "difficult",
    "find",
    "true",
    "parents",
    "data",
    "increases",
    "bias",
    "morel",
    "also",
    "decreases",
    "variance",
    "model",
    "keep",
    "topic",
    "mind",
    "continue",
    "topic",
    "next",
    "next",
    "lecture",
    "discussing",
    "topic",
    "overfitting",
    "solve",
    "overfitting",
    "problem",
    "using",
    "regularization",
    "lecture",
    "lecture",
    "number",
    "three",
    "talk",
    "important",
    "concept",
    "called",
    "overfitting",
    "solve",
    "overfitting",
    "using",
    "different",
    "techniques",
    "including",
    "regularization",
    "topic",
    "related",
    "previous",
    "lecture",
    "topics",
    "error",
    "model",
    "train",
    "error",
    "rate",
    "test",
    "error",
    "rate",
    "bias",
    "variance",
    "machine",
    "learning",
    "model",
    "overfitting",
    "important",
    "know",
    "also",
    "solve",
    "regularization",
    "topic",
    "lead",
    "inaccurate",
    "predictions",
    "lack",
    "generalization",
    "model",
    "new",
    "data",
    "knowing",
    "detect",
    "prevent",
    "overfitting",
    "crucial",
    "building",
    "effective",
    "machine",
    "learning",
    "models",
    "questions",
    "topic",
    "almost",
    "guaranteed",
    "appear",
    "every",
    "single",
    "data",
    "science",
    "interview",
    "previous",
    "lecture",
    "discuss",
    "relationship",
    "model",
    "flexibility",
    "variance",
    "well",
    "bias",
    "model",
    "saw",
    "flexibility",
    "model",
    "increases",
    "model",
    "finds",
    "true",
    "pattern",
    "data",
    "easier",
    "reduces",
    "bias",
    "model",
    "time",
    "variance",
    "models",
    "increases",
    "flexibility",
    "model",
    "decreases",
    "model",
    "finds",
    "difficult",
    "find",
    "true",
    "patterns",
    "data",
    "increases",
    "bias",
    "model",
    "decreases",
    "variance",
    "model",
    "let",
    "first",
    "formally",
    "define",
    "overfitting",
    "problem",
    "well",
    "underfitting",
    "overfitting",
    "occurs",
    "model",
    "performs",
    "well",
    "training",
    "model",
    "performs",
    "worse",
    "test",
    "data",
    "end",
    "low",
    "training",
    "error",
    "rate",
    "high",
    "test",
    "error",
    "rate",
    "ideal",
    "world",
    "want",
    "test",
    "error",
    "rate",
    "low",
    "least",
    "training",
    "rate",
    "equal",
    "test",
    "error",
    "rate",
    "overfitting",
    "common",
    "problem",
    "machine",
    "learning",
    "model",
    "learns",
    "detail",
    "noise",
    "training",
    "data",
    "point",
    "negatively",
    "impacts",
    "performance",
    "model",
    "new",
    "data",
    "model",
    "follows",
    "data",
    "closely",
    "closer",
    "means",
    "noise",
    "random",
    "fluctuations",
    "training",
    "data",
    "picked",
    "learned",
    "concepts",
    "model",
    "actually",
    "ignore",
    "problem",
    "noise",
    "random",
    "component",
    "training",
    "data",
    "different",
    "noise",
    "new",
    "data",
    "model",
    "therefore",
    "less",
    "effective",
    "making",
    "predictions",
    "new",
    "data",
    "overfitting",
    "caused",
    "many",
    "features",
    "complex",
    "model",
    "little",
    "data",
    "model",
    "overfitting",
    "also",
    "model",
    "high",
    "variance",
    "low",
    "bias",
    "usually",
    "higher",
    "model",
    "flexibility",
    "higher",
    "risk",
    "overfitting",
    "higher",
    "risk",
    "model",
    "following",
    "data",
    "closely",
    "following",
    "noise",
    "underfitting",
    "way",
    "around",
    "underfitting",
    "occurs",
    "test",
    "error",
    "rate",
    "much",
    "lower",
    "training",
    "error",
    "rate",
    "given",
    "overfitting",
    "much",
    "bigger",
    "problem",
    "want",
    "ideally",
    "fix",
    "case",
    "test",
    "theate",
    "large",
    "focus",
    "overfitting",
    "also",
    "topic",
    "expect",
    "data",
    "science",
    "interviews",
    "well",
    "something",
    "need",
    "aware",
    "whenever",
    "training",
    "machine",
    "learning",
    "model",
    "right",
    "know",
    "overfitting",
    "talk",
    "fix",
    "problem",
    "several",
    "ways",
    "fixing",
    "preventing",
    "overfitting",
    "first",
    "reduce",
    "complexity",
    "model",
    "saw",
    "higher",
    "complexity",
    "model",
    "higher",
    "chance",
    "following",
    "data",
    "including",
    "noise",
    "closely",
    "resulting",
    "overfitting",
    "therefore",
    "reducing",
    "flexibility",
    "model",
    "reduce",
    "overfitting",
    "well",
    "done",
    "using",
    "simpler",
    "model",
    "fewer",
    "parameters",
    "applying",
    "regularization",
    "techniques",
    "l1",
    "l2",
    "regularization",
    "talk",
    "bit",
    "kind",
    "solution",
    "collect",
    "data",
    "data",
    "less",
    "likely",
    "model",
    "overfit",
    "third",
    "another",
    "solution",
    "using",
    "resampling",
    "techniques",
    "one",
    "cross",
    "validation",
    "technique",
    "allows",
    "train",
    "test",
    "model",
    "different",
    "subsets",
    "data",
    "help",
    "identify",
    "model",
    "overfitting",
    "discuss",
    "cross",
    "validation",
    "well",
    "sampling",
    "techniques",
    "later",
    "section",
    "another",
    "solution",
    "apply",
    "early",
    "stopping",
    "early",
    "stopping",
    "technique",
    "monitor",
    "performance",
    "model",
    "validation",
    "set",
    "training",
    "process",
    "stop",
    "training",
    "performance",
    "starts",
    "decrease",
    "another",
    "solution",
    "use",
    "assemble",
    "methods",
    "combining",
    "multiple",
    "models",
    "decision",
    "trees",
    "overfitting",
    "reduced",
    "covering",
    "many",
    "popular",
    "emble",
    "techniques",
    "course",
    "well",
    "finally",
    "call",
    "dropout",
    "dropout",
    "regularization",
    "technique",
    "reducing",
    "overfitting",
    "narrow",
    "networks",
    "dropping",
    "setting",
    "zero",
    "neurons",
    "training",
    "process",
    "time",
    "time",
    "dropout",
    "related",
    "questions",
    "appear",
    "data",
    "science",
    "interviews",
    "people",
    "experience",
    "someone",
    "asks",
    "dropout",
    "least",
    "remember",
    "technique",
    "used",
    "solve",
    "overfitting",
    "setting",
    "deep",
    "learning",
    "worth",
    "noting",
    "one",
    "solution",
    "works",
    "types",
    "overfitting",
    "often",
    "group",
    "techniques",
    "talk",
    "used",
    "address",
    "problem",
    "saw",
    "model",
    "overfitting",
    "model",
    "high",
    "variance",
    "low",
    "bias",
    "definition",
    "regularization",
    "also",
    "call",
    "shrinkage",
    "method",
    "shrinks",
    "estimated",
    "coefficients",
    "toward",
    "zero",
    "penalize",
    "unimportant",
    "variables",
    "increasing",
    "variance",
    "model",
    "technique",
    "used",
    "solve",
    "overfitting",
    "problem",
    "introducing",
    "lethal",
    "bias",
    "model",
    "significantly",
    "decreasing",
    "variance",
    "three",
    "types",
    "regularization",
    "techniques",
    "widely",
    "known",
    "industry",
    "first",
    "one",
    "reach",
    "regression",
    "l2",
    "regularization",
    "second",
    "one",
    "ler",
    "regression",
    "l1",
    "regularization",
    "finally",
    "third",
    "one",
    "dropout",
    "regularization",
    "technique",
    "used",
    "deep",
    "learning",
    "cover",
    "first",
    "two",
    "types",
    "lecture",
    "let",
    "talk",
    "regression",
    "l2",
    "regularization",
    "regression",
    "l2",
    "regularization",
    "shrinkage",
    "technique",
    "aims",
    "solve",
    "overfitting",
    "shrinking",
    "modor",
    "coefficients",
    "towards",
    "zero",
    "retrogression",
    "introduces",
    "latal",
    "bias",
    "model",
    "significantly",
    "reducing",
    "model",
    "variance",
    "r",
    "regression",
    "variation",
    "linear",
    "regression",
    "instead",
    "trying",
    "minimize",
    "sum",
    "squared",
    "residuales",
    "linear",
    "regression",
    "aims",
    "minimize",
    "sum",
    "squared",
    "residuales",
    "added",
    "top",
    "squared",
    "coefficients",
    "call",
    "l2",
    "regularization",
    "term",
    "let",
    "look",
    "multiple",
    "linear",
    "regression",
    "example",
    "p",
    "independent",
    "variables",
    "predictors",
    "used",
    "model",
    "dependent",
    "variable",
    "followed",
    "statistical",
    "section",
    "course",
    "might",
    "also",
    "recall",
    "popular",
    "estimation",
    "technique",
    "estimate",
    "parameter",
    "linear",
    "regression",
    "assuming",
    "assumptions",
    "satisfied",
    "ordinary",
    "le",
    "squares",
    "ols",
    "finds",
    "optimal",
    "coefficients",
    "minimizing",
    "sum",
    "squared",
    "residuales",
    "rss",
    "regression",
    "pretty",
    "similar",
    "os",
    "except",
    "coefficients",
    "estimated",
    "minimizing",
    "slightly",
    "different",
    "cost",
    "loss",
    "function",
    "loss",
    "function",
    "regession",
    "beta",
    "j",
    "coefficient",
    "model",
    "variable",
    "j",
    "beta0",
    "intercept",
    "x",
    "j",
    "input",
    "value",
    "variable",
    "j",
    "observation",
    "yi",
    "target",
    "variable",
    "dependent",
    "variable",
    "observation",
    "n",
    "number",
    "samples",
    "lambda",
    "call",
    "regularization",
    "parameter",
    "r",
    "regression",
    "loss",
    "function",
    "ols",
    "see",
    "added",
    "penalization",
    "term",
    "combined",
    "call",
    "rss",
    "check",
    "initial",
    "lecture",
    "section",
    "spoke",
    "different",
    "metrics",
    "used",
    "evaluate",
    "regression",
    "type",
    "models",
    "see",
    "rss",
    "definition",
    "rss",
    "well",
    "compare",
    "expression",
    "easily",
    "find",
    "exact",
    "formula",
    "rss",
    "added",
    "intercept",
    "right",
    "term",
    "called",
    "penalty",
    "amount",
    "basically",
    "represents",
    "lambda",
    "times",
    "sum",
    "squar",
    "coefficients",
    "included",
    "model",
    "lambda",
    "always",
    "positive",
    "always",
    "larger",
    "equal",
    "zero",
    "tuning",
    "parameter",
    "penalty",
    "parameter",
    "expression",
    "sum",
    "squared",
    "coefficients",
    "called",
    "l2",
    "norm",
    "call",
    "l2",
    "penalty",
    "based",
    "regression",
    "l2",
    "regularization",
    "way",
    "regression",
    "assigns",
    "penalty",
    "shrinking",
    "coefficients",
    "towards",
    "zero",
    "reduces",
    "overall",
    "model",
    "variance",
    "coefficient",
    "never",
    "become",
    "exactly",
    "zero",
    "model",
    "parameters",
    "never",
    "said",
    "exactly",
    "zero",
    "means",
    "p",
    "predictors",
    "model",
    "still",
    "intact",
    "one",
    "key",
    "property",
    "retrogression",
    "keep",
    "mind",
    "shrinks",
    "parameters",
    "towards",
    "zero",
    "never",
    "exactly",
    "sets",
    "equal",
    "zero",
    "l2",
    "norm",
    "mathematical",
    "term",
    "coming",
    "linear",
    "algebra",
    "standing",
    "alian",
    "norm",
    "spoke",
    "penalty",
    "parameter",
    "lum",
    "lda",
    "also",
    "call",
    "tuning",
    "parameter",
    "lambda",
    "serves",
    "control",
    "relative",
    "impact",
    "penalty",
    "regression",
    "coefficient",
    "estimates",
    "lambda",
    "equal",
    "zero",
    "penalty",
    "term",
    "effect",
    "regression",
    "introduce",
    "ordinary",
    "le",
    "squares",
    "estimates",
    "lambda",
    "increases",
    "impact",
    "shrinkage",
    "penalty",
    "grows",
    "r",
    "regression",
    "coefficient",
    "estimates",
    "approach",
    "zero",
    "important",
    "keep",
    "mind",
    "also",
    "see",
    "graph",
    "r",
    "agression",
    "large",
    "lambda",
    "assign",
    "penalty",
    "variables",
    "shrinking",
    "coefficients",
    "towards",
    "zero",
    "never",
    "become",
    "exactly",
    "zero",
    "becomes",
    "problem",
    "dealing",
    "model",
    "large",
    "number",
    "features",
    "model",
    "low",
    "interpretability",
    "retrogressions",
    "advantage",
    "ordinarily",
    "squares",
    "coming",
    "earlier",
    "introduced",
    "bias",
    "varian",
    "trade",
    "phenomenon",
    "lambda",
    "penalty",
    "parameter",
    "increases",
    "flexibility",
    "retrogression",
    "f",
    "decreases",
    "leading",
    "decreased",
    "variance",
    "increased",
    "bias",
    "main",
    "advantages",
    "retrogression",
    "solving",
    "overfitting",
    "regression",
    "shrink",
    "regression",
    "coefficient",
    "less",
    "important",
    "predictors",
    "towards",
    "zero",
    "improve",
    "prediction",
    "accuracy",
    "well",
    "reducing",
    "variance",
    "increasing",
    "bias",
    "model",
    "rich",
    "repression",
    "less",
    "sensitive",
    "outliers",
    "data",
    "compared",
    "linear",
    "regression",
    "rich",
    "regression",
    "computationally",
    "less",
    "expensive",
    "compared",
    "class",
    "regression",
    "main",
    "disadvantage",
    "r",
    "aggression",
    "low",
    "modal",
    "interpretability",
    "p",
    "number",
    "features",
    "model",
    "large",
    "let",
    "look",
    "another",
    "regularization",
    "technique",
    "called",
    "l",
    "regression",
    "l1",
    "regularization",
    "definition",
    "l",
    "regression",
    "l1",
    "regularization",
    "shrinkage",
    "technique",
    "aims",
    "solve",
    "overfitting",
    "shrinking",
    "modal",
    "coefficients",
    "towards",
    "zero",
    "setting",
    "exactly",
    "zero",
    "l",
    "regression",
    "like",
    "retrogression",
    "introduces",
    "later",
    "bias",
    "model",
    "significantly",
    "reducing",
    "model",
    "variance",
    "however",
    "small",
    "difference",
    "two",
    "regression",
    "techniques",
    "makes",
    "huge",
    "difference",
    "results",
    "saw",
    "one",
    "biggest",
    "disadvantages",
    "r",
    "regression",
    "always",
    "include",
    "predictors",
    "p",
    "predictors",
    "final",
    "model",
    "whereas",
    "case",
    "lasso",
    "overcomes",
    "disadvantage",
    "large",
    "lambda",
    "penalty",
    "parameter",
    "assign",
    "penalty",
    "variables",
    "shrinking",
    "coefficients",
    "towards",
    "zero",
    "case",
    "rich",
    "aggression",
    "never",
    "become",
    "exactly",
    "zero",
    "becomes",
    "problem",
    "model",
    "large",
    "number",
    "features",
    "low",
    "interpretability",
    "l",
    "regression",
    "overcomes",
    "disadvantage",
    "retrogression",
    "let",
    "look",
    "loss",
    "function",
    "l",
    "regularization",
    "loss",
    "function",
    "ols",
    "left",
    "part",
    "formula",
    "called",
    "rss",
    "combined",
    "penalty",
    "amount",
    "right",
    "hand",
    "side",
    "expression",
    "lambda",
    "times",
    "absolute",
    "values",
    "coefficients",
    "beta",
    "j",
    "see",
    "rss",
    "saw",
    "exactly",
    "loss",
    "function",
    "ols",
    "adding",
    "second",
    "term",
    "basically",
    "lambda",
    "penalization",
    "parameter",
    "multiplied",
    "sum",
    "absolute",
    "value",
    "coefficient",
    "beta",
    "j",
    "j",
    "goes",
    "one",
    "till",
    "p",
    "p",
    "number",
    "predictors",
    "included",
    "model",
    "lambda",
    "always",
    "positive",
    "larger",
    "equal",
    "z",
    "tuning",
    "parameter",
    "penalty",
    "parameter",
    "expression",
    "sum",
    "squared",
    "coefficients",
    "called",
    "l1",
    "norm",
    "call",
    "l1",
    "penalty",
    "based",
    "regression",
    "l1",
    "regularization",
    "way",
    "l",
    "regression",
    "assigns",
    "penalty",
    "variables",
    "shrinking",
    "coefficients",
    "towards",
    "zero",
    "setting",
    "parameters",
    "exactly",
    "zero",
    "means",
    "coefficients",
    "end",
    "exactly",
    "equal",
    "zero",
    "key",
    "difference",
    "l",
    "regression",
    "versus",
    "reg",
    "regression",
    "l1",
    "norm",
    "mathematical",
    "term",
    "coming",
    "linear",
    "alra",
    "standing",
    "man",
    "norm",
    "distance",
    "might",
    "see",
    "key",
    "difference",
    "comparing",
    "visual",
    "representation",
    "l",
    "regression",
    "compared",
    "visual",
    "representation",
    "reg",
    "agression",
    "look",
    "point",
    "see",
    "cases",
    "coefficients",
    "set",
    "exactly",
    "zero",
    "intersection",
    "whereas",
    "case",
    "r",
    "regression",
    "recall",
    "single",
    "intersection",
    "numbers",
    "circle",
    "closed",
    "intersection",
    "points",
    "single",
    "point",
    "intersection",
    "coefficients",
    "put",
    "zero",
    "key",
    "difference",
    "two",
    "regression",
    "type",
    "models",
    "two",
    "regularization",
    "tech",
    "techniques",
    "main",
    "advantages",
    "loss",
    "regression",
    "solving",
    "overfitting",
    "loss",
    "regression",
    "shrink",
    "regression",
    "coefficient",
    "less",
    "important",
    "predictors",
    "toward",
    "zero",
    "exactly",
    "zero",
    "model",
    "filters",
    "variables",
    "l",
    "indirectly",
    "performs",
    "also",
    "call",
    "feature",
    "selection",
    "resulted",
    "model",
    "highly",
    "interpretable",
    "less",
    "features",
    "much",
    "interpretable",
    "compared",
    "reg",
    "aggression",
    "laso",
    "also",
    "improve",
    "predi",
    "accuracy",
    "model",
    "reducing",
    "variance",
    "increasing",
    "bias",
    "model",
    "much",
    "retrogression",
    "earlier",
    "speaking",
    "correlation",
    "also",
    "briefly",
    "discussed",
    "concept",
    "causation",
    "discuss",
    "correlation",
    "causation",
    "also",
    "briefly",
    "spoke",
    "method",
    "used",
    "determine",
    "whether",
    "causation",
    "model",
    "infamous",
    "linear",
    "aggression",
    "even",
    "model",
    "recognized",
    "simple",
    "approach",
    "one",
    "one",
    "methods",
    "allows",
    "identifying",
    "features",
    "impact",
    "statistically",
    "significant",
    "impact",
    "variable",
    "interested",
    "want",
    "explain",
    "also",
    "helps",
    "identify",
    "much",
    "change",
    "target",
    "variable",
    "changing",
    "independent",
    "variable",
    "values",
    "understand",
    "concept",
    "linear",
    "aggression",
    "also",
    "know",
    "understand",
    "concepts",
    "dependent",
    "variable",
    "independent",
    "variable",
    "linearity",
    "statistical",
    "significant",
    "effect",
    "dependent",
    "variables",
    "often",
    "referred",
    "response",
    "variables",
    "explained",
    "variables",
    "definition",
    "dependent",
    "variable",
    "variable",
    "measured",
    "tested",
    "called",
    "dependent",
    "variable",
    "thought",
    "depend",
    "independent",
    "variables",
    "one",
    "multiple",
    "independent",
    "variables",
    "one",
    "dependent",
    "variable",
    "interested",
    "target",
    "variable",
    "let",
    "look",
    "independent",
    "variable",
    "definition",
    "independent",
    "variables",
    "often",
    "referred",
    "regressors",
    "explanatory",
    "variables",
    "definition",
    "independent",
    "variable",
    "variable",
    "manipulated",
    "controlled",
    "experiment",
    "believed",
    "effect",
    "dependent",
    "variable",
    "put",
    "differently",
    "value",
    "dependent",
    "variable",
    "depend",
    "value",
    "independent",
    "variable",
    "example",
    "experiment",
    "test",
    "effect",
    "degree",
    "wage",
    "degree",
    "variable",
    "would",
    "independent",
    "variable",
    "wage",
    "would",
    "dependent",
    "variable",
    "finally",
    "let",
    "look",
    "important",
    "concept",
    "statistical",
    "significance",
    "call",
    "effect",
    "statistically",
    "significant",
    "unlikely",
    "occurred",
    "random",
    "chance",
    "words",
    "statistically",
    "significant",
    "effect",
    "one",
    "likely",
    "real",
    "due",
    "random",
    "chance",
    "let",
    "define",
    "linear",
    "regression",
    "model",
    "formally",
    "dive",
    "deep",
    "theoretical",
    "practical",
    "details",
    "definition",
    "v",
    "regression",
    "statistical",
    "machine",
    "learning",
    "method",
    "help",
    "model",
    "impact",
    "unit",
    "change",
    "variable",
    "independent",
    "variable",
    "values",
    "another",
    "target",
    "variable",
    "dependent",
    "variable",
    "relationship",
    "two",
    "variables",
    "assumed",
    "linear",
    "linear",
    "regression",
    "model",
    "based",
    "single",
    "independent",
    "variable",
    "call",
    "model",
    "simple",
    "linear",
    "regression",
    "model",
    "based",
    "multiple",
    "independent",
    "variables",
    "call",
    "multiple",
    "linear",
    "regression",
    "let",
    "look",
    "mathematical",
    "expression",
    "describing",
    "linear",
    "regression",
    "recall",
    "linear",
    "regression",
    "model",
    "based",
    "single",
    "independent",
    "variable",
    "call",
    "simple",
    "linear",
    "regression",
    "expression",
    "see",
    "common",
    "mathematical",
    "expression",
    "describing",
    "simple",
    "linear",
    "regression",
    "see",
    "saying",
    "yi",
    "equal",
    "beta",
    "0",
    "plus",
    "beta",
    "1",
    "x",
    "plus",
    "ui",
    "expression",
    "yi",
    "dependent",
    "variable",
    "see",
    "index",
    "corresponding",
    "e",
    "row",
    "whenever",
    "getting",
    "data",
    "want",
    "analyze",
    "data",
    "multiple",
    "rows",
    "multiple",
    "rows",
    "describe",
    "observations",
    "data",
    "people",
    "observation",
    "describing",
    "uh",
    "data",
    "characterizes",
    "specific",
    "roow",
    "roow",
    "data",
    "yi",
    "variables",
    "value",
    "corresponding",
    "show",
    "holds",
    "xi",
    "xi",
    "independent",
    "variable",
    "explanatory",
    "variable",
    "regressor",
    "model",
    "variable",
    "testing",
    "want",
    "manipulate",
    "see",
    "whether",
    "variable",
    "statistically",
    "significant",
    "impact",
    "dependent",
    "variable",
    "want",
    "see",
    "whether",
    "unit",
    "change",
    "x",
    "result",
    "specific",
    "change",
    "kind",
    "change",
    "beta",
    "z",
    "see",
    "variable",
    "called",
    "intercept",
    "constant",
    "something",
    "unknown",
    "data",
    "one",
    "parameters",
    "linear",
    "regression",
    "unknown",
    "number",
    "linear",
    "regression",
    "model",
    "estimate",
    "want",
    "use",
    "linear",
    "regression",
    "model",
    "find",
    "uh",
    "unknown",
    "value",
    "well",
    "second",
    "unknown",
    "value",
    "beta",
    "one",
    "well",
    "estimate",
    "error",
    "terms",
    "represented",
    "ur",
    "beta",
    "one",
    "next",
    "xi",
    "next",
    "independent",
    "variable",
    "also",
    "variable",
    "like",
    "beta",
    "zero",
    "unknown",
    "parameter",
    "linear",
    "regression",
    "model",
    "unknown",
    "number",
    "linear",
    "regression",
    "model",
    "estimate",
    "beta",
    "one",
    "often",
    "referred",
    "slope",
    "coefficient",
    "variable",
    "x",
    "number",
    "quantifies",
    "much",
    "dependent",
    "variable",
    "change",
    "independent",
    "variable",
    "x",
    "change",
    "one",
    "unit",
    "ex",
    "exactly",
    "interested",
    "beta",
    "one",
    "coefficient",
    "unknown",
    "number",
    "help",
    "us",
    "understand",
    "answer",
    "question",
    "whether",
    "independent",
    "variable",
    "x",
    "statistically",
    "significant",
    "impact",
    "dependent",
    "variable",
    "finally",
    "u",
    "see",
    "ui",
    "expression",
    "error",
    "term",
    "amount",
    "mistake",
    "model",
    "makes",
    "explaining",
    "target",
    "variable",
    "add",
    "value",
    "since",
    "know",
    "never",
    "exactly",
    "accurately",
    "estimate",
    "target",
    "variable",
    "always",
    "make",
    "amount",
    "estimation",
    "error",
    "never",
    "estimate",
    "exact",
    "value",
    "hence",
    "need",
    "account",
    "mistake",
    "going",
    "make",
    "know",
    "advance",
    "going",
    "mistake",
    "adding",
    "error",
    "term",
    "model",
    "let",
    "also",
    "brief",
    "look",
    "multiple",
    "linear",
    "regression",
    "usually",
    "expressed",
    "mathematical",
    "terms",
    "might",
    "recall",
    "difference",
    "simple",
    "linear",
    "regression",
    "multiple",
    "linear",
    "regression",
    "first",
    "one",
    "single",
    "independent",
    "variable",
    "whereas",
    "letter",
    "multiple",
    "linear",
    "regression",
    "like",
    "name",
    "suggest",
    "multiple",
    "independent",
    "variables",
    "one",
    "knowing",
    "type",
    "expressions",
    "critical",
    "since",
    "appear",
    "lot",
    "interviews",
    "also",
    "general",
    "see",
    "data",
    "science",
    "blogs",
    "presentations",
    "books",
    "also",
    "papers",
    "able",
    "quickly",
    "identify",
    "say",
    "ah",
    "remember",
    "saying",
    "help",
    "easier",
    "understand",
    "follow",
    "process",
    "story",
    "line",
    "uh",
    "see",
    "read",
    "yi",
    "equal",
    "beta",
    "0",
    "plus",
    "beta",
    "1",
    "x1",
    "plus",
    "beta",
    "2",
    "x2",
    "plus",
    "beta",
    "3",
    "x3",
    "plus",
    "ui",
    "common",
    "mathematical",
    "expression",
    "describing",
    "multiple",
    "linear",
    "regression",
    "case",
    "three",
    "independent",
    "variables",
    "independent",
    "variables",
    "add",
    "corresponding",
    "indices",
    "coefficients",
    "case",
    "method",
    "aim",
    "estimate",
    "model",
    "parameters",
    "beta",
    "0",
    "beta",
    "1",
    "beta",
    "2",
    "beta",
    "tre",
    "like",
    "yi",
    "dependent",
    "variable",
    "always",
    "single",
    "one",
    "one",
    "dependent",
    "variable",
    "beta",
    "0",
    "intercept",
    "constant",
    "first",
    "slope",
    "coefficient",
    "beta",
    "1",
    "corresponding",
    "first",
    "independent",
    "variable",
    "x1",
    "x1",
    "stands",
    "independent",
    "variable",
    "first",
    "independent",
    "variable",
    "index",
    "one",
    "stands",
    "index",
    "corresponding",
    "row",
    "whenever",
    "multiple",
    "linear",
    "regression",
    "always",
    "need",
    "specify",
    "two",
    "indices",
    "one",
    "like",
    "uh",
    "single",
    "linear",
    "regression",
    "index",
    "cor",
    "characterizes",
    "independent",
    "variable",
    "referring",
    "whether",
    "independent",
    "variable",
    "one",
    "two",
    "three",
    "need",
    "specify",
    "row",
    "referring",
    "index",
    "might",
    "notice",
    "case",
    "indices",
    "uh",
    "looking",
    "one",
    "specific",
    "role",
    "representing",
    "role",
    "using",
    "independent",
    "variables",
    "error",
    "term",
    "dependent",
    "variable",
    "adding",
    "third",
    "term",
    "beta",
    "2",
    "x2i",
    "beta",
    "2",
    "third",
    "unknown",
    "parameter",
    "model",
    "second",
    "slope",
    "coefficient",
    "corresponding",
    "second",
    "independent",
    "variable",
    "third",
    "independent",
    "variable",
    "corresponding",
    "slope",
    "coefficient",
    "beta",
    "3",
    "well",
    "also",
    "add",
    "like",
    "always",
    "error",
    "term",
    "account",
    "error",
    "know",
    "going",
    "make",
    "know",
    "linear",
    "regression",
    "express",
    "mathematical",
    "terms",
    "might",
    "asking",
    "next",
    "logical",
    "question",
    "well",
    "know",
    "know",
    "linear",
    "regression",
    "express",
    "mathematical",
    "terms",
    "might",
    "asking",
    "next",
    "logical",
    "question",
    "find",
    "unknown",
    "parameters",
    "model",
    "order",
    "find",
    "independent",
    "variables",
    "impacted",
    "dependent",
    "variable",
    "finding",
    "unknown",
    "parameters",
    "called",
    "estimating",
    "data",
    "science",
    "general",
    "interested",
    "finding",
    "possible",
    "values",
    "values",
    "best",
    "approximate",
    "unknown",
    "values",
    "model",
    "call",
    "process",
    "estimation",
    "one",
    "technique",
    "used",
    "estimate",
    "linear",
    "regression",
    "parameters",
    "called",
    "oils",
    "ordinary",
    "le",
    "squares",
    "domain",
    "idea",
    "behind",
    "approach",
    "ols",
    "find",
    "best",
    "fitting",
    "straight",
    "line",
    "regression",
    "line",
    "set",
    "paired",
    "x",
    "independent",
    "variables",
    "dependent",
    "variables",
    "values",
    "minimizing",
    "sum",
    "squared",
    "errors",
    "minimize",
    "sum",
    "squares",
    "differences",
    "observed",
    "dependent",
    "variable",
    "values",
    "predicted",
    "values",
    "predicted",
    "model",
    "exactly",
    "want",
    "using",
    "linear",
    "function",
    "independent",
    "variables",
    "residuals",
    "much",
    "information",
    "let",
    "go",
    "step",
    "step",
    "linear",
    "regression",
    "expressing",
    "simple",
    "linear",
    "regression",
    "error",
    "term",
    "never",
    "know",
    "actual",
    "error",
    "term",
    "estimate",
    "value",
    "error",
    "term",
    "call",
    "residual",
    "want",
    "minimize",
    "sum",
    "squ",
    "residuales",
    "know",
    "errors",
    "want",
    "find",
    "line",
    "best",
    "fit",
    "data",
    "way",
    "error",
    "making",
    "sum",
    "squared",
    "errors",
    "small",
    "possible",
    "since",
    "know",
    "errors",
    "estimate",
    "errors",
    "time",
    "looking",
    "predicted",
    "value",
    "predicted",
    "model",
    "true",
    "value",
    "subtract",
    "see",
    "good",
    "model",
    "estimating",
    "values",
    "good",
    "model",
    "estimating",
    "unknown",
    "parameters",
    "minimize",
    "sum",
    "squar",
    "differences",
    "observed",
    "dependent",
    "variable",
    "values",
    "predicted",
    "linear",
    "function",
    "independent",
    "variables",
    "minimizing",
    "sum",
    "squared",
    "residuales",
    "uh",
    "define",
    "estimate",
    "parameters",
    "variables",
    "adding",
    "hge",
    "top",
    "variables",
    "parameters",
    "case",
    "see",
    "equal",
    "beta",
    "z",
    "head",
    "plus",
    "beta",
    "1",
    "head",
    "xi",
    "see",
    "longer",
    "error",
    "term",
    "say",
    "yi",
    "head",
    "estimated",
    "value",
    "yi",
    "beta",
    "zero",
    "head",
    "estimated",
    "value",
    "beta",
    "0",
    "beta",
    "1",
    "head",
    "estimated",
    "value",
    "beta",
    "1",
    "xi",
    "still",
    "data",
    "values",
    "data",
    "therefore",
    "hat",
    "since",
    "need",
    "estimated",
    "want",
    "estimate",
    "dependent",
    "variable",
    "want",
    "compare",
    "estimated",
    "value",
    "got",
    "using",
    "ols",
    "actual",
    "real",
    "value",
    "calculate",
    "errors",
    "estimate",
    "error",
    "represented",
    "ui",
    "head",
    "ui",
    "head",
    "equal",
    "yi",
    "minus",
    "yi",
    "head",
    "ui",
    "head",
    "simply",
    "estimate",
    "error",
    "term",
    "residual",
    "predicted",
    "error",
    "always",
    "referred",
    "residual",
    "make",
    "sure",
    "confuse",
    "error",
    "residual",
    "error",
    "never",
    "observed",
    "error",
    "never",
    "calculate",
    "never",
    "know",
    "predict",
    "error",
    "predict",
    "error",
    "get",
    "recal",
    "oil",
    "trying",
    "minimize",
    "amount",
    "airor",
    "making",
    "therefore",
    "looks",
    "sum",
    "squared",
    "residuales",
    "across",
    "observation",
    "tries",
    "find",
    "line",
    "minimize",
    "value",
    "therefore",
    "saying",
    "tries",
    "find",
    "best",
    "fitting",
    "straight",
    "line",
    "minimizes",
    "sum",
    "squared",
    "residuals",
    "discussed",
    "model",
    "talking",
    "model",
    "mainly",
    "perspective",
    "causal",
    "analysis",
    "order",
    "identify",
    "features",
    "statistically",
    "significant",
    "impact",
    "response",
    "variable",
    "linear",
    "regression",
    "also",
    "used",
    "prediction",
    "model",
    "modeling",
    "linear",
    "relationship",
    "let",
    "refresh",
    "memory",
    "definition",
    "linear",
    "regression",
    "model",
    "definition",
    "linear",
    "regression",
    "statistical",
    "machine",
    "learning",
    "method",
    "help",
    "modrow",
    "impact",
    "unit",
    "change",
    "variable",
    "independent",
    "variable",
    "values",
    "another",
    "target",
    "variable",
    "dependent",
    "variable",
    "relationship",
    "two",
    "variables",
    "linear",
    "also",
    "discussed",
    "mathematically",
    "express",
    "call",
    "simple",
    "linear",
    "regression",
    "multiple",
    "linear",
    "regression",
    "uh",
    "simple",
    "linear",
    "regression",
    "represented",
    "uh",
    "case",
    "simple",
    "linear",
    "regression",
    "might",
    "recorde",
    "dealing",
    "single",
    "independent",
    "variable",
    "always",
    "one",
    "dependent",
    "variable",
    "single",
    "linear",
    "regression",
    "multiple",
    "linear",
    "regression",
    "see",
    "yi",
    "equal",
    "beta",
    "0",
    "plus",
    "beta",
    "1",
    "xi",
    "plus",
    "ui",
    "dependent",
    "variable",
    "basically",
    "index",
    "observation",
    "row",
    "beta",
    "0",
    "intercept",
    "also",
    "known",
    "constant",
    "beta",
    "1",
    "slope",
    "coefficient",
    "parameter",
    "corresponding",
    "independent",
    "variable",
    "x",
    "unnown",
    "constant",
    "want",
    "estimate",
    "along",
    "beta",
    "zero",
    "xi",
    "independent",
    "variable",
    "corresponding",
    "observation",
    "finally",
    "ui",
    "error",
    "term",
    "corresponding",
    "observation",
    "keep",
    "mind",
    "error",
    "term",
    "adding",
    "know",
    "always",
    "going",
    "make",
    "mistake",
    "never",
    "perfectly",
    "estimate",
    "dependent",
    "variable",
    "therefore",
    "account",
    "mistake",
    "adding",
    "ui",
    "let",
    "also",
    "recall",
    "estimation",
    "technique",
    "use",
    "estimate",
    "parameter",
    "linear",
    "regression",
    "model",
    "beta",
    "0",
    "beta",
    "1",
    "predict",
    "response",
    "variable",
    "call",
    "estimation",
    "technique",
    "ors",
    "ordinary",
    "le",
    "squares",
    "ns",
    "estimation",
    "technique",
    "estimating",
    "unknown",
    "parameters",
    "linear",
    "regression",
    "model",
    "predict",
    "response",
    "dependent",
    "variable",
    "need",
    "estimate",
    "beta",
    "z",
    "need",
    "get",
    "beta",
    "zero",
    "head",
    "need",
    "estimate",
    "beta",
    "one",
    "beta",
    "1",
    "head",
    "order",
    "obtain",
    "head",
    "yi",
    "head",
    "equal",
    "beta",
    "z",
    "head",
    "plus",
    "beta",
    "1",
    "head",
    "time",
    "x",
    "um",
    "difference",
    "yi",
    "head",
    "yi",
    "true",
    "value",
    "dependent",
    "variable",
    "predicted",
    "value",
    "different",
    "produce",
    "estimate",
    "error",
    "also",
    "call",
    "residual",
    "main",
    "idea",
    "behind",
    "approach",
    "find",
    "best",
    "fitting",
    "straight",
    "line",
    "regression",
    "line",
    "set",
    "paired",
    "x",
    "values",
    "minimizing",
    "sum",
    "squared",
    "residuales",
    "want",
    "minimize",
    "errors",
    "much",
    "possible",
    "therefore",
    "taking",
    "squared",
    "version",
    "trying",
    "sum",
    "want",
    "minimize",
    "entire",
    "error",
    "minimize",
    "sum",
    "squar",
    "residual",
    "difference",
    "observed",
    "dependent",
    "variable",
    "values",
    "predicted",
    "linear",
    "function",
    "independent",
    "variables",
    "need",
    "use",
    "ols",
    "one",
    "common",
    "questions",
    "related",
    "linear",
    "regression",
    "comes",
    "time",
    "time",
    "uh",
    "data",
    "science",
    "related",
    "interviews",
    "topic",
    "assumption",
    "linear",
    "regression",
    "model",
    "need",
    "know",
    "five",
    "fundamental",
    "assumptions",
    "linear",
    "regression",
    "ols",
    "also",
    "need",
    "know",
    "test",
    "whether",
    "assumptions",
    "satisfied",
    "first",
    "assumption",
    "linearity",
    "assumption",
    "states",
    "relationship",
    "independent",
    "variables",
    "dependent",
    "variable",
    "linear",
    "also",
    "say",
    "model",
    "linear",
    "parameters",
    "also",
    "check",
    "whether",
    "linearity",
    "assumption",
    "satisfied",
    "plotting",
    "residuals",
    "fitted",
    "values",
    "pattern",
    "linear",
    "estimat",
    "biased",
    "case",
    "say",
    "linearity",
    "assumption",
    "violated",
    "need",
    "use",
    "flexible",
    "models",
    "tree",
    "based",
    "models",
    "discuss",
    "bit",
    "able",
    "model",
    "nonlinear",
    "relationships",
    "second",
    "assumption",
    "linear",
    "regression",
    "assumption",
    "randomness",
    "sample",
    "means",
    "data",
    "randomly",
    "sampled",
    "basically",
    "means",
    "errors",
    "residuales",
    "different",
    "observations",
    "data",
    "independent",
    "also",
    "check",
    "whether",
    "second",
    "assumption",
    "assumption",
    "random",
    "sample",
    "satisfied",
    "plotting",
    "residuals",
    "check",
    "whether",
    "mean",
    "residuales",
    "around",
    "zero",
    "ols",
    "estimate",
    "biased",
    "second",
    "assumption",
    "violated",
    "means",
    "systematically",
    "predicting",
    "dependent",
    "variable",
    "third",
    "assumption",
    "exogeneity",
    "assumption",
    "really",
    "important",
    "assumption",
    "often",
    "data",
    "science",
    "interviews",
    "exogeneity",
    "means",
    "independent",
    "variable",
    "uncorrelated",
    "error",
    "terms",
    "exogeneity",
    "refers",
    "assumption",
    "independent",
    "variables",
    "affected",
    "error",
    "term",
    "model",
    "words",
    "independent",
    "variables",
    "assumed",
    "determined",
    "independently",
    "erors",
    "model",
    "exogeneity",
    "key",
    "assumption",
    "new",
    "regression",
    "model",
    "allows",
    "us",
    "interpret",
    "estimated",
    "coefficient",
    "representing",
    "true",
    "causal",
    "effect",
    "independent",
    "variables",
    "dependent",
    "variable",
    "independent",
    "variables",
    "exogeneous",
    "estimated",
    "coefficients",
    "may",
    "biased",
    "interpretation",
    "results",
    "may",
    "invalid",
    "case",
    "call",
    "problem",
    "endogeneity",
    "problem",
    "say",
    "independent",
    "variable",
    "exogeneous",
    "endogeneous",
    "important",
    "carefully",
    "consider",
    "exogeneity",
    "assumption",
    "building",
    "linear",
    "regression",
    "model",
    "violation",
    "assumption",
    "lead",
    "invalid",
    "misleading",
    "results",
    "assumption",
    "satisfied",
    "independent",
    "variable",
    "linear",
    "model",
    "call",
    "independent",
    "variable",
    "exogeneous",
    "otherwise",
    "call",
    "endogeneous",
    "say",
    "problem",
    "endogenity",
    "endogenity",
    "refers",
    "situation",
    "independent",
    "variables",
    "linear",
    "regression",
    "model",
    "correlated",
    "error",
    "terms",
    "model",
    "words",
    "errors",
    "independent",
    "independent",
    "variables",
    "endogeneity",
    "violation",
    "one",
    "key",
    "assumptions",
    "linear",
    "regression",
    "model",
    "independent",
    "variables",
    "ex",
    "geners",
    "affected",
    "errors",
    "model",
    "endogenity",
    "arise",
    "number",
    "ways",
    "example",
    "caused",
    "omitted",
    "variable",
    "bias",
    "important",
    "predictor",
    "dependent",
    "variable",
    "included",
    "model",
    "also",
    "caused",
    "reverse",
    "causality",
    "dependent",
    "variable",
    "affects",
    "independent",
    "variable",
    "two",
    "popular",
    "examples",
    "case",
    "get",
    "endogenity",
    "problem",
    "things",
    "know",
    "whenever",
    "interest",
    "data",
    "science",
    "roles",
    "especially",
    "related",
    "machine",
    "learning",
    "questions",
    "uh",
    "asked",
    "order",
    "test",
    "whether",
    "understand",
    "concept",
    "exogeneity",
    "versus",
    "endogenity",
    "also",
    "cases",
    "get",
    "endogenity",
    "also",
    "solve",
    "uh",
    "case",
    "omitted",
    "variable",
    "bias",
    "let",
    "say",
    "estimating",
    "person",
    "salary",
    "using",
    "independent",
    "variable",
    "education",
    "number",
    "years",
    "experience",
    "uh",
    "factors",
    "including",
    "instance",
    "model",
    "feature",
    "would",
    "describe",
    "uh",
    "intelligence",
    "person",
    "uh",
    "instance",
    "iq",
    "person",
    "well",
    "given",
    "important",
    "indicator",
    "person",
    "order",
    "perform",
    "uh",
    "field",
    "definitely",
    "um",
    "indirect",
    "impact",
    "salary",
    "including",
    "variables",
    "result",
    "omitted",
    "variable",
    "bias",
    "uh",
    "incorporated",
    "um",
    "error",
    "term",
    "uh",
    "also",
    "relate",
    "independent",
    "variables",
    "uh",
    "iq",
    "also",
    "related",
    "um",
    "education",
    "higher",
    "iq",
    "usually",
    "higher",
    "education",
    "way",
    "error",
    "term",
    "includes",
    "important",
    "variable",
    "omitted",
    "variable",
    "uh",
    "correlated",
    "uh",
    "one",
    "multiple",
    "independent",
    "variables",
    "include",
    "model",
    "example",
    "cause",
    "endogenity",
    "problem",
    "reverse",
    "causality",
    "um",
    "reverse",
    "causality",
    "means",
    "basically",
    "independent",
    "variable",
    "impact",
    "dependent",
    "variable",
    "also",
    "dependent",
    "variable",
    "impact",
    "independent",
    "variable",
    "reverse",
    "relationship",
    "something",
    "want",
    "avoid",
    "want",
    "features",
    "include",
    "model",
    "impact",
    "dependent",
    "variable",
    "explaining",
    "dependent",
    "variable",
    "way",
    "around",
    "um",
    "way",
    "dependent",
    "variable",
    "impacting",
    "independent",
    "variable",
    "error",
    "term",
    "related",
    "independent",
    "variable",
    "components",
    "also",
    "define",
    "dependent",
    "variable",
    "knowing",
    "uh",
    "examples",
    "cause",
    "uh",
    "endogenity",
    "violate",
    "exogeneity",
    "assumption",
    "really",
    "important",
    "uh",
    "also",
    "check",
    "exogeneity",
    "assumption",
    "conducting",
    "formal",
    "statistical",
    "test",
    "called",
    "house",
    "one",
    "test",
    "econometrical",
    "test",
    "helps",
    "understand",
    "whether",
    "exogeneity",
    "uh",
    "violation",
    "scope",
    "course",
    "however",
    "include",
    "uh",
    "many",
    "resources",
    "related",
    "exogeneity",
    "endogenity",
    "omitted",
    "variable",
    "bias",
    "well",
    "reverse",
    "cality",
    "also",
    "house",
    "one",
    "test",
    "conducted",
    "check",
    "interation",
    "guide",
    "also",
    "find",
    "corresponding",
    "free",
    "resources",
    "fourth",
    "assumption",
    "linear",
    "regression",
    "assumption",
    "homos",
    "skes",
    "homos",
    "refers",
    "assumption",
    "variance",
    "errors",
    "constant",
    "across",
    "predicted",
    "values",
    "assumption",
    "also",
    "known",
    "homogeneity",
    "variance",
    "homosa",
    "important",
    "assumption",
    "linear",
    "regression",
    "model",
    "allows",
    "us",
    "use",
    "certain",
    "statistical",
    "techniques",
    "make",
    "inferences",
    "parameters",
    "model",
    "errors",
    "homoskedastic",
    "result",
    "techniques",
    "may",
    "invalid",
    "misleading",
    "assumption",
    "violated",
    "say",
    "heteroscedasticity",
    "hecticity",
    "refers",
    "situation",
    "variance",
    "error",
    "terms",
    "linear",
    "regression",
    "model",
    "constant",
    "across",
    "predicted",
    "values",
    "variating",
    "variant",
    "words",
    "assumption",
    "homos",
    "skas",
    "testing",
    "case",
    "violated",
    "say",
    "problem",
    "heos",
    "heteros",
    "real",
    "problem",
    "v",
    "regression",
    "nurses",
    "lead",
    "invalid",
    "misleading",
    "results",
    "example",
    "standard",
    "estimates",
    "confidence",
    "intervals",
    "parameters",
    "may",
    "incorrect",
    "means",
    "also",
    "statistical",
    "test",
    "may",
    "incorrect",
    "type",
    "one",
    "error",
    "rates",
    "might",
    "recall",
    "discussing",
    "linear",
    "regression",
    "part",
    "fundamental",
    "statis",
    "section",
    "course",
    "uh",
    "looked",
    "output",
    "comes",
    "python",
    "saw",
    "getting",
    "uh",
    "estimates",
    "part",
    "output",
    "well",
    "standard",
    "errors",
    "test",
    "student",
    "test",
    "corresponding",
    "p",
    "values",
    "95",
    "confidence",
    "intervals",
    "whenever",
    "heos",
    "problem",
    "um",
    "coefficient",
    "might",
    "still",
    "accurate",
    "corresponding",
    "standard",
    "error",
    "u",
    "student",
    "test",
    "based",
    "standard",
    "error",
    "p",
    "value",
    "well",
    "uh",
    "confidence",
    "intervals",
    "may",
    "accurate",
    "might",
    "get",
    "uh",
    "good",
    "reasonable",
    "coefficient",
    "know",
    "correctly",
    "evaluate",
    "might",
    "end",
    "discovering",
    "um",
    "might",
    "end",
    "stating",
    "certain",
    "uh",
    "independent",
    "variables",
    "statistically",
    "significant",
    "coefficients",
    "statistically",
    "significant",
    "since",
    "p",
    "values",
    "small",
    "reality",
    "p",
    "values",
    "misleading",
    "based",
    "wrong",
    "statistical",
    "uh",
    "test",
    "based",
    "wrong",
    "standard",
    "errors",
    "check",
    "assumption",
    "plotting",
    "residual",
    "see",
    "whether",
    "funnel",
    "like",
    "graph",
    "fel",
    "like",
    "gra",
    "constant",
    "variance",
    "wo",
    "see",
    "fenel",
    "like",
    "shape",
    "indicates",
    "variances",
    "constant",
    "say",
    "problem",
    "heos",
    "skos",
    "heteros",
    "system",
    "longer",
    "use",
    "os",
    "linear",
    "regression",
    "instead",
    "need",
    "look",
    "advanced",
    "econometrical",
    "regression",
    "techniques",
    "make",
    "strong",
    "assumption",
    "regarding",
    "variance",
    "um",
    "residuals",
    "instance",
    "use",
    "gls",
    "fgs",
    "gmm",
    "type",
    "solutions",
    "um",
    "help",
    "solve",
    "hoscar",
    "problem",
    "make",
    "strong",
    "assumptions",
    "regarding",
    "variance",
    "model",
    "fifth",
    "final",
    "assumption",
    "linear",
    "regression",
    "assumption",
    "perfect",
    "multicolinearity",
    "assumption",
    "states",
    "exactly",
    "new",
    "relationships",
    "independent",
    "variables",
    "multicolinearity",
    "refers",
    "case",
    "two",
    "independent",
    "variables",
    "linear",
    "regression",
    "model",
    "highly",
    "correlated",
    "problem",
    "lead",
    "unstable",
    "unreliable",
    "estimate",
    "parameters",
    "model",
    "perfect",
    "multicolinearity",
    "happens",
    "independent",
    "variables",
    "perfectly",
    "correlated",
    "meaning",
    "one",
    "variable",
    "perfectly",
    "predicted",
    "ones",
    "cause",
    "estimated",
    "coefficient",
    "linear",
    "regression",
    "model",
    "infinite",
    "undefined",
    "lead",
    "errors",
    "uh",
    "entirely",
    "misleading",
    "making",
    "predictions",
    "using",
    "model",
    "perfect",
    "multicolinearity",
    "detected",
    "may",
    "necessary",
    "remove",
    "one",
    "problematic",
    "variables",
    "avoid",
    "correlated",
    "variables",
    "model",
    "even",
    "perfect",
    "multicolinearity",
    "present",
    "multicolinearity",
    "high",
    "level",
    "still",
    "problem",
    "correlations",
    "independent",
    "variables",
    "high",
    "case",
    "estimate",
    "parameters",
    "may",
    "imprecise",
    "model",
    "may",
    "uh",
    "entirely",
    "misleading",
    "results",
    "less",
    "reliable",
    "uh",
    "predictions",
    "uh",
    "test",
    "multicolinearity",
    "assumption",
    "different",
    "solutions",
    "different",
    "options",
    "first",
    "way",
    "uh",
    "using",
    "uh",
    "di",
    "test",
    "de",
    "test",
    "formal",
    "statistical",
    "econometrical",
    "test",
    "help",
    "identify",
    "variables",
    "cause",
    "problem",
    "whether",
    "perfect",
    "multicolinearity",
    "linear",
    "regression",
    "model",
    "pl",
    "heat",
    "map",
    "based",
    "uh",
    "correlation",
    "metrix",
    "corresponding",
    "features",
    "uh",
    "correlations",
    "per",
    "pair",
    "independent",
    "variables",
    "plotted",
    "part",
    "heat",
    "map",
    "identify",
    "um",
    "pair",
    "features",
    "highly",
    "correlated",
    "problematic",
    "features",
    "one",
    "removed",
    "model",
    "way",
    "uh",
    "showing",
    "heat",
    "map",
    "also",
    "showcase",
    "stakeholders",
    "remove",
    "certain",
    "variables",
    "model",
    "whereas",
    "explaining",
    "diller",
    "test",
    "much",
    "complex",
    "involves",
    "advanced",
    "econometrics",
    "linear",
    "uh",
    "regression",
    "um",
    "explanation",
    "wondering",
    "perform",
    "de",
    "fl",
    "test",
    "want",
    "prepare",
    "uh",
    "questions",
    "related",
    "perfect",
    "multicolinearity",
    "well",
    "solve",
    "perfect",
    "multicolinearity",
    "problem",
    "linear",
    "regression",
    "model",
    "head",
    "towards",
    "interview",
    "preparation",
    "guide",
    "included",
    "part",
    "course",
    "order",
    "answer",
    "questions",
    "also",
    "see",
    "30",
    "popular",
    "interview",
    "questions",
    "expect",
    "section",
    "interview",
    "preparation",
    "guide",
    "let",
    "look",
    "example",
    "coming",
    "linear",
    "regression",
    "order",
    "see",
    "pieces",
    "puzzle",
    "come",
    "together",
    "let",
    "say",
    "collected",
    "data",
    "class",
    "size",
    "test",
    "course",
    "students",
    "want",
    "model",
    "linear",
    "relationship",
    "class",
    "size",
    "test",
    "course",
    "using",
    "linear",
    "regression",
    "model",
    "one",
    "independent",
    "variable",
    "dealing",
    "simple",
    "linear",
    "regession",
    "model",
    "equation",
    "would",
    "follows",
    "see",
    "test",
    "course",
    "equal",
    "beta0",
    "plus",
    "beta",
    "1",
    "multip",
    "class",
    "size",
    "plus",
    "epsilon",
    "class",
    "size",
    "single",
    "independent",
    "variable",
    "got",
    "model",
    "test",
    "score",
    "dependent",
    "variable",
    "beta0",
    "intercept",
    "constant",
    "beta",
    "one",
    "coefficient",
    "interest",
    "coefficient",
    "corresponding",
    "independent",
    "variable",
    "help",
    "us",
    "understand",
    "uh",
    "impact",
    "unit",
    "change",
    "class",
    "size",
    "test",
    "score",
    "finally",
    "including",
    "model",
    "error",
    "term",
    "account",
    "mistakes",
    "definitely",
    "going",
    "make",
    "estimating",
    "uh",
    "dependent",
    "variable",
    "course",
    "goal",
    "estimate",
    "coefficient",
    "0",
    "beta",
    "1",
    "data",
    "use",
    "estimated",
    "model",
    "predict",
    "test",
    "course",
    "based",
    "class",
    "size",
    "estimates",
    "interpret",
    "follows",
    "intercept",
    "beta",
    "zero",
    "represents",
    "expected",
    "test",
    "course",
    "class",
    "size",
    "zero",
    "represents",
    "base",
    "score",
    "student",
    "would",
    "obtained",
    "class",
    "size",
    "would",
    "zero",
    "coefficient",
    "class",
    "size",
    "beta",
    "one",
    "represents",
    "change",
    "test",
    "course",
    "associated",
    "one",
    "unit",
    "change",
    "class",
    "size",
    "positive",
    "coefficient",
    "would",
    "imply",
    "one",
    "unit",
    "change",
    "class",
    "size",
    "would",
    "increase",
    "test",
    "course",
    "whereas",
    "negative",
    "coefficient",
    "would",
    "uh",
    "imply",
    "one",
    "unit",
    "change",
    "class",
    "size",
    "decrease",
    "test",
    "course",
    "uh",
    "correspondingly",
    "use",
    "model",
    "ols",
    "estimate",
    "order",
    "predict",
    "test",
    "course",
    "given",
    "class",
    "size",
    "let",
    "go",
    "ahead",
    "implement",
    "python",
    "wondering",
    "done",
    "head",
    "towards",
    "resources",
    "section",
    "well",
    "part",
    "python",
    "data",
    "science",
    "learn",
    "work",
    "pendant",
    "data",
    "frames",
    "import",
    "data",
    "well",
    "fit",
    "linear",
    "regression",
    "model",
    "problem",
    "follows",
    "collected",
    "data",
    "class",
    "size",
    "independent",
    "variable",
    "see",
    "students",
    "uncore",
    "data",
    "class",
    "size",
    "feature",
    "want",
    "estimate",
    "test",
    "sc",
    "uh",
    "code",
    "sample",
    "code",
    "fit",
    "linear",
    "regression",
    "model",
    "keeping",
    "everything",
    "simple",
    "splitting",
    "data",
    "training",
    "test",
    "fitting",
    "model",
    "training",
    "data",
    "making",
    "predictions",
    "test",
    "score",
    "want",
    "see",
    "interpret",
    "uh",
    "coefficients",
    "keeping",
    "everything",
    "simple",
    "see",
    "getting",
    "intercept",
    "equal",
    "coefficient",
    "corresponding",
    "single",
    "independent",
    "variable",
    "class",
    "size",
    "equal",
    "minus",
    "means",
    "increase",
    "uh",
    "class",
    "size",
    "one",
    "unit",
    "result",
    "decrease",
    "test",
    "scores",
    "negative",
    "relationship",
    "two",
    "next",
    "question",
    "whether",
    "statistical",
    "significance",
    "whether",
    "uh",
    "coefficient",
    "actually",
    "significant",
    "class",
    "size",
    "actually",
    "statistically",
    "significant",
    "imp",
    "impact",
    "dependent",
    "variable",
    "things",
    "discussed",
    "part",
    "fundamental",
    "statistic",
    "section",
    "course",
    "well",
    "going",
    "look",
    "linear",
    "regression",
    "example",
    "going",
    "discuss",
    "hypothesis",
    "testing",
    "would",
    "highly",
    "suggest",
    "uh",
    "stop",
    "revisit",
    "fundamentals",
    "statistic",
    "section",
    "course",
    "refresh",
    "memory",
    "terms",
    "linear",
    "regression",
    "um",
    "check",
    "also",
    "hypothesis",
    "test",
    "uh",
    "section",
    "course",
    "order",
    "look",
    "specific",
    "example",
    "linear",
    "regression",
    "discussing",
    "standard",
    "errors",
    "evaluate",
    "ols",
    "estimation",
    "results",
    "use",
    "student",
    "test",
    "p",
    "value",
    "confidence",
    "intervals",
    "estimate",
    "way",
    "learn",
    "theory",
    "related",
    "coefficients",
    "um",
    "add",
    "top",
    "theory",
    "learned",
    "sections",
    "topics",
    "course",
    "let",
    "finally",
    "discuss",
    "advantages",
    "disadvantages",
    "linear",
    "regression",
    "model",
    "advantages",
    "linear",
    "regression",
    "model",
    "following",
    "linear",
    "regression",
    "relatively",
    "simple",
    "easy",
    "understand",
    "implement",
    "linear",
    "regression",
    "models",
    "well",
    "suited",
    "understanding",
    "relationship",
    "single",
    "independent",
    "variable",
    "dependent",
    "variable",
    "also",
    "linear",
    "regression",
    "help",
    "handle",
    "multiple",
    "independent",
    "variables",
    "estimate",
    "unique",
    "relationship",
    "independent",
    "variable",
    "corresponding",
    "dependent",
    "variable",
    "thear",
    "regression",
    "model",
    "also",
    "extended",
    "handle",
    "complex",
    "models",
    "pooms",
    "interaction",
    "terms",
    "allowing",
    "flexibility",
    "modeling",
    "data",
    "also",
    "linear",
    "aggression",
    "model",
    "easily",
    "regularized",
    "prevent",
    "overfitting",
    "common",
    "problem",
    "modeling",
    "saw",
    "uh",
    "beginning",
    "section",
    "use",
    "instance",
    "retrogression",
    "extension",
    "vue",
    "regression",
    "use",
    "ler",
    "regression",
    "also",
    "extension",
    "vue",
    "regression",
    "model",
    "finally",
    "linear",
    "regression",
    "models",
    "widely",
    "supported",
    "software",
    "packages",
    "libraries",
    "making",
    "easy",
    "implement",
    "analyze",
    "disadvantages",
    "linear",
    "aggression",
    "following",
    "linear",
    "aggression",
    "models",
    "make",
    "lot",
    "strong",
    "assumptions",
    "regarding",
    "instance",
    "linearity",
    "independent",
    "variables",
    "independent",
    "variables",
    "true",
    "relationship",
    "actually",
    "also",
    "nonlinear",
    "model",
    "able",
    "capture",
    "complexity",
    "data",
    "nonlinearity",
    "predictions",
    "inaccurate",
    "therefore",
    "really",
    "important",
    "data",
    "linear",
    "relationship",
    "linear",
    "regression",
    "work",
    "linear",
    "regression",
    "also",
    "assumes",
    "error",
    "terms",
    "normally",
    "distributed",
    "also",
    "homoskedastic",
    "error",
    "terms",
    "independent",
    "across",
    "observations",
    "violations",
    "strong",
    "assumption",
    "lead",
    "bias",
    "inefficient",
    "estimates",
    "linear",
    "regression",
    "also",
    "sensitive",
    "outliers",
    "disproportionate",
    "effect",
    "estimate",
    "regression",
    "coefficients",
    "linear",
    "regression",
    "easily",
    "handle",
    "categorical",
    "independent",
    "variables",
    "often",
    "require",
    "additional",
    "data",
    "preparation",
    "use",
    "indicator",
    "variables",
    "using",
    "encodings",
    "finally",
    "linear",
    "regression",
    "also",
    "assumes",
    "independent",
    "variables",
    "exogeneous",
    "affected",
    "error",
    "terms",
    "assumption",
    "violated",
    "result",
    "model",
    "may",
    "misleading",
    "lecture",
    "lecture",
    "number",
    "five",
    "discuss",
    "another",
    "simple",
    "machine",
    "learning",
    "technique",
    "called",
    "logistic",
    "regression",
    "simple",
    "important",
    "classification",
    "model",
    "useful",
    "dealing",
    "problem",
    "output",
    "probability",
    "name",
    "regression",
    "logistic",
    "regression",
    "might",
    "confusing",
    "since",
    "actually",
    "classification",
    "model",
    "logistic",
    "regression",
    "widely",
    "used",
    "variety",
    "fields",
    "social",
    "sciences",
    "medicine",
    "engineering",
    "let",
    "us",
    "firstly",
    "define",
    "logistic",
    "regression",
    "model",
    "logistic",
    "regression",
    "supervised",
    "classification",
    "technique",
    "models",
    "conditional",
    "probability",
    "event",
    "occurring",
    "observation",
    "belonging",
    "certain",
    "class",
    "given",
    "data",
    "set",
    "independent",
    "variables",
    "features",
    "class",
    "two",
    "categories",
    "later",
    "learn",
    "logistic",
    "regression",
    "works",
    "ideally",
    "two",
    "classes",
    "another",
    "important",
    "popular",
    "machine",
    "learning",
    "technique",
    "though",
    "named",
    "regression",
    "actually",
    "supervised",
    "classification",
    "technique",
    "relationship",
    "two",
    "variables",
    "linear",
    "dependent",
    "variable",
    "categorical",
    "variable",
    "want",
    "predict",
    "variable",
    "form",
    "probability",
    "number",
    "zero",
    "one",
    "logistic",
    "regression",
    "comes",
    "handy",
    "prediction",
    "process",
    "logistic",
    "regression",
    "classifier",
    "predicts",
    "probability",
    "ility",
    "value",
    "z",
    "one",
    "observation",
    "belonging",
    "certain",
    "class",
    "instance",
    "want",
    "predict",
    "probability",
    "likelihood",
    "candidate",
    "elected",
    "elected",
    "election",
    "process",
    "given",
    "set",
    "characteristics",
    "got",
    "candidate",
    "let",
    "say",
    "popularity",
    "score",
    "past",
    "successes",
    "descriptive",
    "variables",
    "candidate",
    "logistic",
    "regression",
    "comes",
    "handy",
    "model",
    "probability",
    "rather",
    "predicting",
    "response",
    "variable",
    "logistic",
    "regression",
    "models",
    "probability",
    "belongs",
    "particular",
    "category",
    "similar",
    "linear",
    "regression",
    "difference",
    "instead",
    "predicts",
    "log",
    "odds",
    "come",
    "definition",
    "log",
    "odds",
    "odds",
    "bit",
    "statistical",
    "terminology",
    "trying",
    "model",
    "conditional",
    "distribution",
    "response",
    "given",
    "predictors",
    "x",
    "therefore",
    "logistic",
    "regression",
    "helps",
    "predict",
    "probability",
    "belonging",
    "certain",
    "class",
    "given",
    "feature",
    "space",
    "call",
    "probability",
    "given",
    "x",
    "wondering",
    "concept",
    "probability",
    "conditional",
    "probability",
    "make",
    "sure",
    "head",
    "towards",
    "section",
    "fundamentals",
    "statistics",
    "going",
    "detail",
    "concepts",
    "well",
    "looking",
    "different",
    "examples",
    "definitions",
    "concepts",
    "help",
    "better",
    "follow",
    "lecture",
    "see",
    "probability",
    "x",
    "interested",
    "modeling",
    "equal",
    "e",
    "power",
    "beta",
    "0",
    "beta",
    "1",
    "x",
    "1",
    "beta",
    "0",
    "beta",
    "1",
    "x",
    "let",
    "look",
    "formulas",
    "odds",
    "log",
    "ods",
    "formulas",
    "really",
    "important",
    "expect",
    "data",
    "science",
    "interviews",
    "sometimes",
    "asked",
    "explicitly",
    "write",
    "odds",
    "log",
    "ods",
    "formulas",
    "highly",
    "related",
    "log",
    "likelihood",
    "likelihood",
    "functions",
    "base",
    "estimation",
    "technique",
    "mle",
    "maximum",
    "likelihood",
    "estimation",
    "used",
    "estimate",
    "unknown",
    "parameters",
    "logistic",
    "agression",
    "log",
    "odds",
    "odds",
    "highly",
    "related",
    "logistic",
    "regression",
    "use",
    "odds",
    "log",
    "ods",
    "describe",
    "probability",
    "event",
    "occurring",
    "odds",
    "ratio",
    "probability",
    "event",
    "occurring",
    "probability",
    "event",
    "occurring",
    "see",
    "odd",
    "equal",
    "px",
    "1",
    "px",
    "px",
    "probability",
    "event",
    "occurring",
    "1",
    "px",
    "probability",
    "event",
    "occurring",
    "formula",
    "equal",
    "e",
    "power",
    "beta",
    "0",
    "beta",
    "1",
    "x",
    "formula",
    "one",
    "independent",
    "variable",
    "e",
    "simply",
    "ers",
    "number",
    "constant",
    "wo",
    "derive",
    "formula",
    "scope",
    "course",
    "feel",
    "free",
    "head",
    "px",
    "formula",
    "saw",
    "previous",
    "slide",
    "take",
    "formula",
    "divide",
    "one",
    "minus",
    "2",
    "exactly",
    "expression",
    "verify",
    "end",
    "expression",
    "see",
    "example",
    "probability",
    "person",
    "heart",
    "attack",
    "ads",
    "heart",
    "attack",
    "1",
    "equal",
    "low",
    "od",
    "also",
    "known",
    "logit",
    "function",
    "natural",
    "logarithm",
    "od",
    "see",
    "log",
    "1",
    "minus",
    "px",
    "equal",
    "beta",
    "0",
    "plus",
    "beta",
    "1",
    "x",
    "see",
    "getting",
    "rid",
    "e",
    "simply",
    "mathematical",
    "expression",
    "says",
    "take",
    "log",
    "e",
    "power",
    "something",
    "end",
    "exponent",
    "part",
    "though",
    "scope",
    "course",
    "look",
    "mathematical",
    "derivation",
    "formula",
    "include",
    "many",
    "resources",
    "regarding",
    "logarithm",
    "transformations",
    "mathematics",
    "behind",
    "case",
    "want",
    "look",
    "details",
    "uh",
    "extra",
    "learning",
    "logistic",
    "regression",
    "uses",
    "log",
    "ods",
    "dependent",
    "variable",
    "independent",
    "variables",
    "used",
    "predict",
    "log",
    "ods",
    "coefficient",
    "independent",
    "varibles",
    "represent",
    "change",
    "log",
    "od",
    "one",
    "unit",
    "change",
    "independent",
    "variable",
    "might",
    "linear",
    "regression",
    "modeling",
    "actual",
    "dependent",
    "variable",
    "case",
    "logistic",
    "regression",
    "difference",
    "modeling",
    "logas",
    "another",
    "important",
    "concept",
    "logistic",
    "regression",
    "likelihood",
    "function",
    "likelihood",
    "function",
    "used",
    "estimate",
    "parameters",
    "model",
    "given",
    "observed",
    "data",
    "sometimes",
    "interviews",
    "might",
    "also",
    "asked",
    "write",
    "exact",
    "likelihood",
    "formula",
    "log",
    "likelihood",
    "function",
    "would",
    "definitely",
    "suggest",
    "memorize",
    "one",
    "understand",
    "components",
    "included",
    "formula",
    "likelihood",
    "function",
    "describes",
    "probability",
    "observed",
    "data",
    "given",
    "parameters",
    "model",
    "follow",
    "lecture",
    "probability",
    "density",
    "functions",
    "section",
    "fundamentals",
    "statistics",
    "might",
    "even",
    "recognize",
    "bar",
    "noly",
    "pdf",
    "since",
    "likelihood",
    "function",
    "based",
    "probability",
    "mass",
    "function",
    "baro",
    "distribution",
    "distribution",
    "binary",
    "outcome",
    "highly",
    "applicable",
    "case",
    "two",
    "categories",
    "dependent",
    "variable",
    "trying",
    "estimate",
    "probability",
    "observation",
    "belonging",
    "one",
    "two",
    "classes",
    "l",
    "likelihood",
    "function",
    "likelihood",
    "function",
    "start",
    "likelihood",
    "function",
    "l",
    "capital",
    "letter",
    "l",
    "stands",
    "likelihood",
    "function",
    "l",
    "equal",
    "likelihood",
    "function",
    "l",
    "equal",
    "product",
    "across",
    "pair",
    "multipliers",
    "peak",
    "side",
    "power",
    "yi",
    "multiplied",
    "1",
    "pxi",
    "1",
    "pxi",
    "px",
    "observation",
    "yi",
    "simply",
    "class",
    "yi",
    "either",
    "equal",
    "zero",
    "one",
    "yi",
    "equal",
    "1",
    "1",
    "minus",
    "yi",
    "equal",
    "zero",
    "every",
    "time",
    "looking",
    "probability",
    "observation",
    "belonging",
    "first",
    "class",
    "multiply",
    "probability",
    "observation",
    "belonging",
    "plus",
    "take",
    "cross",
    "multiplications",
    "observations",
    "included",
    "data",
    "also",
    "comes",
    "mathematics",
    "stands",
    "product",
    "uh",
    "given",
    "harder",
    "work",
    "products",
    "compared",
    "sums",
    "apply",
    "lo",
    "likelihood",
    "uh",
    "transformation",
    "order",
    "obtain",
    "lo",
    "likelihood",
    "function",
    "instead",
    "likelihood",
    "function",
    "apply",
    "log",
    "transformation",
    "take",
    "l",
    "logarithm",
    "expression",
    "end",
    "log",
    "likelihood",
    "expression",
    "one",
    "time",
    "making",
    "use",
    "mathematical",
    "property",
    "says",
    "take",
    "logarithm",
    "products",
    "end",
    "sum",
    "logarithms",
    "go",
    "products",
    "sums",
    "also",
    "include",
    "resources",
    "regarding",
    "also",
    "learn",
    "mathematics",
    "behind",
    "transformations",
    "l",
    "likelihood",
    "lowercase",
    "l",
    "equal",
    "logarithm",
    "products",
    "p",
    "1",
    "px",
    "1",
    "yi",
    "apply",
    "mathematical",
    "transformation",
    "l",
    "equal",
    "sum",
    "across",
    "observation",
    "equal",
    "1",
    "till",
    "power",
    "exponent",
    "comes",
    "front",
    "yi",
    "logarithm",
    "pxi",
    "plus",
    "1",
    "yi",
    "logarithm",
    "1",
    "pxi",
    "linear",
    "regression",
    "use",
    "ols",
    "estimation",
    "technique",
    "logis",
    "regression",
    "estimation",
    "technique",
    "used",
    "reason",
    "use",
    "ols",
    "logistic",
    "regression",
    "find",
    "best",
    "fitting",
    "line",
    "errors",
    "become",
    "large",
    "small",
    "sometimes",
    "even",
    "negative",
    "case",
    "logistic",
    "aggression",
    "logistic",
    "regression",
    "aim",
    "predicted",
    "value",
    "zero",
    "one",
    "therefore",
    "logistic",
    "regression",
    "need",
    "use",
    "estimation",
    "technique",
    "called",
    "maximum",
    "likelihood",
    "estimation",
    "short",
    "mle",
    "likelihood",
    "function",
    "calculates",
    "probability",
    "observing",
    "data",
    "outcome",
    "given",
    "input",
    "data",
    "model",
    "saw",
    "likel",
    "function",
    "previous",
    "slide",
    "function",
    "optimized",
    "find",
    "set",
    "parameters",
    "result",
    "largest",
    "sum",
    "likelihood",
    "maximum",
    "likelihood",
    "training",
    "data",
    "set",
    "logistic",
    "function",
    "always",
    "produce",
    "curve",
    "regardless",
    "value",
    "independent",
    "variable",
    "able",
    "x",
    "resulting",
    "sensible",
    "estimation",
    "time",
    "value",
    "0",
    "one",
    "see",
    "cure",
    "characterizes",
    "maximum",
    "likelihood",
    "estimation",
    "corresponding",
    "logistic",
    "regression",
    "always",
    "provide",
    "come",
    "zero",
    "one",
    "idea",
    "behind",
    "maximum",
    "likelihood",
    "estimation",
    "find",
    "set",
    "estimates",
    "would",
    "maximize",
    "likelihood",
    "function",
    "let",
    "go",
    "maximum",
    "likelihood",
    "estimation",
    "step",
    "step",
    "need",
    "first",
    "define",
    "likelihood",
    "function",
    "first",
    "step",
    "always",
    "define",
    "function",
    "model",
    "secondly",
    "need",
    "write",
    "log",
    "likelihood",
    "function",
    "next",
    "step",
    "take",
    "natural",
    "logarithm",
    "likelihood",
    "function",
    "obtain",
    "log",
    "likelihood",
    "function",
    "talking",
    "one",
    "l",
    "likel",
    "function",
    "convenient",
    "computationally",
    "efficient",
    "function",
    "work",
    "need",
    "next",
    "find",
    "maximum",
    "l",
    "like",
    "function",
    "step",
    "consists",
    "finding",
    "values",
    "parameters",
    "beta",
    "0",
    "beta",
    "1",
    "maximize",
    "l",
    "lik",
    "function",
    "many",
    "optimization",
    "algorithms",
    "used",
    "find",
    "maximum",
    "scope",
    "course",
    "need",
    "know",
    "part",
    "becoming",
    "data",
    "scientist",
    "entering",
    "data",
    "science",
    "field",
    "fourth",
    "step",
    "need",
    "estimate",
    "parameters",
    "talking",
    "beta",
    "0",
    "beta",
    "1",
    "maximum",
    "log",
    "likel",
    "function",
    "found",
    "values",
    "parameters",
    "correspond",
    "maximum",
    "considered",
    "maximum",
    "likelihood",
    "estimate",
    "parameters",
    "next",
    "step",
    "need",
    "check",
    "model",
    "feit",
    "maximum",
    "likelihood",
    "estimates",
    "obtained",
    "check",
    "goodness",
    "fit",
    "model",
    "calculating",
    "information",
    "criteria",
    "aic",
    "b",
    "bic",
    "r",
    "squ",
    "aic",
    "stands",
    "akas",
    "information",
    "criteria",
    "bic",
    "stands",
    "bi",
    "information",
    "criteria",
    "r",
    "refers",
    "evaluation",
    "value",
    "use",
    "evaluating",
    "linear",
    "regression",
    "final",
    "step",
    "need",
    "make",
    "predictions",
    "evaluate",
    "model",
    "using",
    "maximum",
    "likelihood",
    "estimates",
    "model",
    "used",
    "make",
    "predictions",
    "new",
    "unseen",
    "data",
    "performance",
    "model",
    "evaluated",
    "using",
    "various",
    "evaluation",
    "metrics",
    "accuracy",
    "precision",
    "recall",
    "metrics",
    "revisited",
    "part",
    "initial",
    "lecture",
    "section",
    "metrics",
    "need",
    "know",
    "unlike",
    "aic",
    "bic",
    "spoke",
    "evaluates",
    "goodness",
    "feed",
    "initial",
    "estimates",
    "come",
    "maximum",
    "likelihood",
    "accuracy",
    "precision",
    "recall",
    "evaluate",
    "final",
    "model",
    "values",
    "get",
    "nuan",
    "data",
    "make",
    "predictions",
    "get",
    "classes",
    "metrics",
    "need",
    "know",
    "wondering",
    "accuracy",
    "precision",
    "recall",
    "well",
    "f1",
    "score",
    "make",
    "sure",
    "head",
    "towards",
    "initial",
    "lecture",
    "section",
    "talked",
    "exact",
    "definition",
    "metrix",
    "let",
    "finally",
    "discuss",
    "advantages",
    "disadvantages",
    "logistic",
    "regression",
    "advantages",
    "logistic",
    "regressions",
    "simple",
    "model",
    "low",
    "variance",
    "low",
    "bias",
    "provides",
    "probabilities",
    "disadvantages",
    "logistic",
    "regressions",
    "logistic",
    "regression",
    "unable",
    "model",
    "nonlinear",
    "relationship",
    "one",
    "key",
    "assumptions",
    "logistic",
    "regression",
    "making",
    "linear",
    "relationship",
    "independent",
    "variable",
    "dependent",
    "variable",
    "logistic",
    "regression",
    "also",
    "unstable",
    "classes",
    "well",
    "separable",
    "well",
    "logistic",
    "agression",
    "becomes",
    "unstable",
    "two",
    "classes",
    "means",
    "whenever",
    "two",
    "categories",
    "dependent",
    "variable",
    "whenever",
    "classes",
    "well",
    "separable",
    "using",
    "logistic",
    "regression",
    "classification",
    "purposes",
    "smart",
    "instead",
    "look",
    "models",
    "use",
    "task",
    "one",
    "models",
    "linear",
    "discriminate",
    "analysis",
    "lda",
    "introduce",
    "next",
    "lecture",
    "lecture",
    "looked",
    "logistic",
    "regression",
    "maximum",
    "likelihood",
    "estimation",
    "next",
    "lecture",
    "look",
    "lda",
    "stay",
    "tuned",
    "see",
    "next",
    "lecture",
    "looking",
    "step",
    "machine",
    "learning",
    "data",
    "science",
    "starting",
    "somewhere",
    "practical",
    "yet",
    "powerful",
    "simple",
    "yet",
    "popular",
    "machine",
    "learning",
    "algorithm",
    "linear",
    "regression",
    "linear",
    "aggression",
    "jargon",
    "tool",
    "used",
    "finding",
    "important",
    "features",
    "data",
    "well",
    "used",
    "forecast",
    "future",
    "starting",
    "point",
    "journey",
    "data",
    "science",
    "machine",
    "learning",
    "uh",
    "work",
    "embark",
    "handson",
    "data",
    "science",
    "machine",
    "learning",
    "project",
    "going",
    "find",
    "drivers",
    "californian",
    "house",
    "prices",
    "clean",
    "data",
    "visualize",
    "key",
    "trends",
    "learn",
    "process",
    "data",
    "use",
    "different",
    "python",
    "libraries",
    "understand",
    "drivers",
    "californian",
    "house",
    "values",
    "going",
    "learn",
    "implement",
    "linear",
    "regression",
    "python",
    "learn",
    "fundamental",
    "steps",
    "need",
    "order",
    "conduct",
    "proper",
    "handson",
    "data",
    "science",
    "project",
    "end",
    "project",
    "learn",
    "different",
    "python",
    "libraries",
    "comes",
    "data",
    "science",
    "machine",
    "learning",
    "pandas",
    "psyit",
    "learn",
    "tou",
    "models",
    "medf",
    "le",
    "curn",
    "also",
    "able",
    "put",
    "project",
    "person",
    "website",
    "resume",
    "point",
    "size",
    "stepbystep",
    "case",
    "study",
    "approach",
    "build",
    "confidence",
    "expertise",
    "machine",
    "learning",
    "data",
    "science",
    "part",
    "going",
    "talk",
    "case",
    "study",
    "field",
    "predictive",
    "analytics",
    "causal",
    "analysis",
    "going",
    "use",
    "simple",
    "yet",
    "powerful",
    "regression",
    "technique",
    "called",
    "regression",
    "order",
    "perform",
    "causal",
    "analysis",
    "predictive",
    "analytics",
    "causal",
    "analysis",
    "mean",
    "going",
    "look",
    "correlations",
    "clation",
    "trying",
    "figure",
    "features",
    "impact",
    "housing",
    "price",
    "house",
    "value",
    "features",
    "describing",
    "house",
    "define",
    "cause",
    "variation",
    "uh",
    "house",
    "prices",
    "goal",
    "case",
    "study",
    "uh",
    "practice",
    "linear",
    "regression",
    "model",
    "get",
    "first",
    "feeling",
    "uh",
    "use",
    "machine",
    "learning",
    "model",
    "simple",
    "machine",
    "learning",
    "model",
    "order",
    "perform",
    "uh",
    "model",
    "training",
    "model",
    "evaluation",
    "also",
    "use",
    "causal",
    "analysis",
    "trying",
    "identify",
    "features",
    "statistically",
    "significant",
    "impact",
    "response",
    "variable",
    "dependent",
    "variable",
    "process",
    "going",
    "follow",
    "order",
    "find",
    "features",
    "define",
    "californian",
    "house",
    "values",
    "first",
    "going",
    "understand",
    "set",
    "independent",
    "variables",
    "also",
    "going",
    "understand",
    "response",
    "variable",
    "multiple",
    "linear",
    "regression",
    "model",
    "going",
    "understand",
    "uh",
    "techniques",
    "uh",
    "need",
    "libraries",
    "python",
    "need",
    "load",
    "order",
    "able",
    "conduct",
    "case",
    "study",
    "first",
    "going",
    "load",
    "libraries",
    "going",
    "understand",
    "need",
    "going",
    "conduct",
    "data",
    "loading",
    "data",
    "preprocessing",
    "important",
    "step",
    "deliberately",
    "want",
    "skip",
    "want",
    "give",
    "clean",
    "data",
    "cuz",
    "uh",
    "usually",
    "normal",
    "real",
    "data",
    "science",
    "job",
    "wo",
    "get",
    "clean",
    "data",
    "get",
    "dirty",
    "data",
    "contain",
    "missing",
    "values",
    "contain",
    "outliers",
    "things",
    "need",
    "handle",
    "proceed",
    "actual",
    "f",
    "part",
    "modeling",
    "uh",
    "analysis",
    "therefore",
    "going",
    "missing",
    "data",
    "analysis",
    "going",
    "remove",
    "missing",
    "data",
    "californian",
    "house",
    "price",
    "data",
    "going",
    "conduct",
    "outlier",
    "detection",
    "going",
    "identify",
    "outliers",
    "going",
    "learn",
    "different",
    "techniques",
    "use",
    "visualization",
    "uh",
    "techniques",
    "uh",
    "python",
    "use",
    "order",
    "identify",
    "outliers",
    "remove",
    "data",
    "going",
    "perform",
    "data",
    "visualization",
    "going",
    "explore",
    "data",
    "going",
    "different",
    "plots",
    "learn",
    "data",
    "learn",
    "outliers",
    "different",
    "statistical",
    "techniques",
    "uh",
    "combined",
    "python",
    "going",
    "correlation",
    "analysis",
    "identify",
    "problematic",
    "features",
    "something",
    "would",
    "suggest",
    "independent",
    "nature",
    "case",
    "study",
    "understand",
    "understand",
    "kind",
    "variables",
    "relationship",
    "whether",
    "dealing",
    "potentially",
    "problematic",
    "variables",
    "uh",
    "moving",
    "towards",
    "fun",
    "part",
    "performing",
    "uh",
    "multiple",
    "theine",
    "regression",
    "order",
    "perform",
    "caal",
    "nes",
    "means",
    "identifying",
    "features",
    "californian",
    "house",
    "blocks",
    "define",
    "value",
    "californian",
    "houses",
    "uh",
    "finally",
    "quickly",
    "another",
    "uh",
    "implementation",
    "multiple",
    "uh",
    "multiple",
    "linear",
    "regression",
    "order",
    "uh",
    "give",
    "one",
    "two",
    "different",
    "ways",
    "conducting",
    "linear",
    "regression",
    "linear",
    "regression",
    "used",
    "caal",
    "analysis",
    "also",
    "standalone",
    "common",
    "machine",
    "learning",
    "regression",
    "type",
    "model",
    "therefore",
    "also",
    "tell",
    "use",
    "psych",
    "learn",
    "second",
    "way",
    "training",
    "predicting",
    "c",
    "house",
    "values",
    "without",
    "ado",
    "let",
    "get",
    "started",
    "become",
    "dat",
    "scientist",
    "machine",
    "learning",
    "researcher",
    "machine",
    "learning",
    "engineer",
    "cases",
    "uh",
    "data",
    "science",
    "projects",
    "business",
    "come",
    "tell",
    "well",
    "data",
    "want",
    "understand",
    "features",
    "biggest",
    "influence",
    "auto",
    "factor",
    "specific",
    "case",
    "case",
    "study",
    "um",
    "let",
    "assume",
    "client",
    "uh",
    "interested",
    "identifying",
    "features",
    "uh",
    "define",
    "house",
    "price",
    "maybe",
    "someone",
    "wants",
    "um",
    "uh",
    "invest",
    "uh",
    "houses",
    "someone",
    "interested",
    "buying",
    "houses",
    "maybe",
    "even",
    "renovating",
    "reselling",
    "making",
    "profit",
    "way",
    "maybe",
    "uh",
    "investment",
    "market",
    "uh",
    "people",
    "buying",
    "real",
    "estate",
    "way",
    "uh",
    "inting",
    "longing",
    "uh",
    "holding",
    "long",
    "time",
    "uh",
    "selling",
    "later",
    "purposes",
    "end",
    "goal",
    "specific",
    "case",
    "uh",
    "person",
    "identify",
    "features",
    "house",
    "makes",
    "house",
    "um",
    "priced",
    "certain",
    "level",
    "features",
    "house",
    "causing",
    "price",
    "value",
    "house",
    "going",
    "make",
    "use",
    "popular",
    "data",
    "set",
    "available",
    "kagal",
    "originally",
    "coming",
    "psyit",
    "learn",
    "called",
    "california",
    "housing",
    "prices",
    "also",
    "make",
    "sure",
    "put",
    "link",
    "uh",
    "uh",
    "specific",
    "um",
    "data",
    "set",
    "uh",
    "github",
    "account",
    "uh",
    "repository",
    "dedicated",
    "specific",
    "case",
    "study",
    "well",
    "um",
    "also",
    "point",
    "additional",
    "links",
    "use",
    "learn",
    "data",
    "set",
    "uh",
    "data",
    "set",
    "derived",
    "1990",
    "um",
    "us",
    "census",
    "united",
    "uh",
    "states",
    "census",
    "using",
    "one",
    "row",
    "paris",
    "sensus",
    "block",
    "blog",
    "group",
    "block",
    "smallest",
    "uh",
    "geographical",
    "unit",
    "us",
    "cus",
    "bureau",
    "publishes",
    "sample",
    "data",
    "blog",
    "group",
    "typically",
    "population",
    "600",
    "people",
    "living",
    "household",
    "group",
    "people",
    "residing",
    "within",
    "single",
    "home",
    "uh",
    "since",
    "average",
    "number",
    "rooms",
    "bedrooms",
    "data",
    "set",
    "provided",
    "per",
    "household",
    "conss",
    "may",
    "um",
    "may",
    "take",
    "surprisingly",
    "large",
    "values",
    "blog",
    "groups",
    "households",
    "many",
    "empty",
    "houses",
    "vacation",
    "resorts",
    "um",
    "let",
    "look",
    "uh",
    "variables",
    "available",
    "specific",
    "data",
    "set",
    "uh",
    "med",
    "inc",
    "median",
    "income",
    "blog",
    "group",
    "uh",
    "um",
    "touches",
    "uh",
    "financial",
    "side",
    "uh",
    "financial",
    "level",
    "uh",
    "block",
    "uh",
    "block",
    "households",
    "house",
    "age",
    "median",
    "house",
    "age",
    "block",
    "group",
    "uh",
    "average",
    "rooms",
    "average",
    "number",
    "rooms",
    "uh",
    "per",
    "household",
    "average",
    "bedroom",
    "average",
    "number",
    "bedrooms",
    "per",
    "household",
    "population",
    "uh",
    "blog",
    "group",
    "population",
    "basically",
    "like",
    "saw",
    "number",
    "people",
    "live",
    "block",
    "uh",
    "ou",
    "uh",
    "basically",
    "average",
    "number",
    "household",
    "members",
    "uh",
    "latitude",
    "longitude",
    "latitude",
    "longitude",
    "uh",
    "block",
    "group",
    "looking",
    "see",
    "dealing",
    "aggregate",
    "data",
    "uh",
    "data",
    "per",
    "household",
    "rather",
    "data",
    "calculated",
    "average",
    "aggregated",
    "based",
    "block",
    "common",
    "data",
    "science",
    "uh",
    "uh",
    "want",
    "reduce",
    "dimension",
    "data",
    "want",
    "sensible",
    "numbers",
    "create",
    "crosssection",
    "data",
    "uh",
    "data",
    "means",
    "multiple",
    "observations",
    "data",
    "single",
    "time",
    "period",
    "period",
    "case",
    "using",
    "aggregation",
    "unit",
    "block",
    "uh",
    "already",
    "learned",
    "part",
    "uh",
    "theory",
    "lectures",
    "idea",
    "median",
    "seen",
    "different",
    "descriptive",
    "measures",
    "use",
    "order",
    "aggregate",
    "data",
    "one",
    "mean",
    "one",
    "median",
    "often",
    "times",
    "especially",
    "dealing",
    "skute",
    "distribution",
    "distribution",
    "symmetric",
    "rather",
    "right",
    "cuute",
    "left",
    "skewed",
    "need",
    "use",
    "idea",
    "median",
    "median",
    "better",
    "representation",
    "um",
    "uh",
    "scale",
    "data",
    "um",
    "compared",
    "mean",
    "um",
    "case",
    "soon",
    "see",
    "representing",
    "visualizing",
    "data",
    "indeed",
    "dealing",
    "skewed",
    "data",
    "um",
    "basically",
    "simple",
    "basic",
    "data",
    "set",
    "many",
    "features",
    "great",
    "um",
    "way",
    "uh",
    "get",
    "hands",
    "uh",
    "uh",
    "actual",
    "machine",
    "learning",
    "use",
    "case",
    "uh",
    "keeping",
    "simple",
    "yet",
    "learning",
    "basics",
    "fundamentals",
    "uh",
    "good",
    "way",
    "uh",
    "learning",
    "um",
    "difficult",
    "advanced",
    "machine",
    "learning",
    "models",
    "much",
    "easier",
    "let",
    "get",
    "actual",
    "coding",
    "part",
    "uh",
    "using",
    "google",
    "clap",
    "sharing",
    "link",
    "notebook",
    "uh",
    "combined",
    "data",
    "python",
    "data",
    "science",
    "repository",
    "make",
    "use",
    "order",
    "uh",
    "follow",
    "uh",
    "tutorial",
    "uh",
    "uh",
    "always",
    "start",
    "importing",
    "uh",
    "libraries",
    "run",
    "l",
    "regression",
    "uh",
    "manually",
    "without",
    "using",
    "libraries",
    "using",
    "matrix",
    "multiplication",
    "uh",
    "would",
    "suggest",
    "fun",
    "understand",
    "metrix",
    "multiplication",
    "linear",
    "algebra",
    "behind",
    "linear",
    "regression",
    "uh",
    "want",
    "um",
    "get",
    "handson",
    "uh",
    "understand",
    "use",
    "new",
    "regression",
    "like",
    "expect",
    "job",
    "expect",
    "use",
    "um",
    "instead",
    "libraries",
    "psychic",
    "learn",
    "also",
    "use",
    "libraries",
    "order",
    "understand",
    "uh",
    "topic",
    "also",
    "get",
    "handson",
    "decided",
    "uh",
    "showcase",
    "example",
    "one",
    "library",
    "cy",
    "thir",
    "also",
    "starts",
    "models",
    "uh",
    "reason",
    "many",
    "people",
    "use",
    "linear",
    "regression",
    "uh",
    "predictive",
    "analytics",
    "using",
    "psyit",
    "learn",
    "option",
    "um",
    "want",
    "use",
    "linear",
    "regression",
    "causal",
    "analysis",
    "identify",
    "interpret",
    "uh",
    "features",
    "independent",
    "variables",
    "statistically",
    "significant",
    "impact",
    "response",
    "variable",
    "need",
    "uh",
    "use",
    "another",
    "library",
    "handy",
    "one",
    "linear",
    "regression",
    "called",
    "uh",
    "stats",
    "models",
    "api",
    "need",
    "import",
    "sm",
    "uh",
    "functionality",
    "help",
    "exactly",
    "later",
    "see",
    "nicely",
    "library",
    "provide",
    "outcome",
    "exactly",
    "like",
    "learn",
    "uh",
    "traditional",
    "econometrics",
    "introduction",
    "linear",
    "regression",
    "uh",
    "class",
    "going",
    "give",
    "background",
    "information",
    "like",
    "one",
    "going",
    "interpret",
    "learn",
    "everything",
    "um",
    "start",
    "machine",
    "learning",
    "journey",
    "proper",
    "uh",
    "um",
    "uh",
    "high",
    "quality",
    "way",
    "uh",
    "case",
    "uh",
    "first",
    "thing",
    "going",
    "import",
    "pendence",
    "library",
    "importing",
    "pendis",
    "library",
    "pd",
    "non",
    "pile",
    "library",
    "np",
    "going",
    "need",
    "pendes",
    "uh",
    "uh",
    "create",
    "pendis",
    "data",
    "frame",
    "read",
    "data",
    "perform",
    "data",
    "wrangling",
    "identify",
    "missing",
    "data",
    "outliers",
    "common",
    "data",
    "wrangling",
    "data",
    "prosessing",
    "steps",
    "going",
    "use",
    "npy",
    "npy",
    "common",
    "way",
    "uh",
    "use",
    "whenever",
    "visualizing",
    "data",
    "whenever",
    "dealing",
    "metrices",
    "arrays",
    "pandas",
    "nonp",
    "used",
    "interchangeably",
    "going",
    "use",
    "meth",
    "plot",
    "lip",
    "specifically",
    "pip",
    "plat",
    "uh",
    "library",
    "important",
    "um",
    "want",
    "visualize",
    "data",
    "uh",
    "cburn",
    "um",
    "uh",
    "another",
    "handy",
    "data",
    "visualization",
    "library",
    "python",
    "whenever",
    "want",
    "visualize",
    "data",
    "python",
    "methot",
    "leip",
    "cy",
    "uh",
    "cburn",
    "two",
    "uh",
    "handy",
    "data",
    "visualization",
    "techniques",
    "must",
    "know",
    "like",
    "um",
    "cooler",
    "undertone",
    "colors",
    "seaburn",
    "option",
    "visualizations",
    "creating",
    "much",
    "appealing",
    "compared",
    "med",
    "plot",
    "le",
    "underlying",
    "way",
    "working",
    "plotting",
    "scatter",
    "plot",
    "lines",
    "um",
    "heat",
    "map",
    "sts",
    "mods",
    "api",
    "uh",
    "library",
    "importing",
    "uh",
    "uh",
    "temple",
    "uh",
    "linear",
    "regression",
    "model",
    "using",
    "uh",
    "caal",
    "analysis",
    "uh",
    "also",
    "importing",
    "uh",
    "psychic",
    "learn",
    "um",
    "linear",
    "model",
    "specifically",
    "linear",
    "regression",
    "model",
    "um",
    "one",
    "uh",
    "basically",
    "similar",
    "one",
    "uh",
    "use",
    "um",
    "common",
    "um",
    "way",
    "working",
    "machine",
    "learning",
    "model",
    "whenever",
    "dealing",
    "predictive",
    "analytics",
    "using",
    "data",
    "uh",
    "identifying",
    "features",
    "statistically",
    "significant",
    "impact",
    "response",
    "variable",
    "features",
    "influence",
    "causing",
    "dependent",
    "variable",
    "rather",
    "interested",
    "use",
    "data",
    "train",
    "model",
    "data",
    "um",
    "test",
    "unseen",
    "data",
    "uh",
    "use",
    "pyit",
    "learn",
    "psyit",
    "learn",
    "uh",
    "something",
    "using",
    "linear",
    "regression",
    "also",
    "machine",
    "learning",
    "model",
    "think",
    "uh",
    "canon",
    "um",
    "logistic",
    "regression",
    "um",
    "random",
    "forest",
    "decision",
    "trees",
    "um",
    "boosting",
    "techniques",
    "light",
    "gbm",
    "gbm",
    "um",
    "also",
    "clustering",
    "techniques",
    "like",
    "k",
    "means",
    "db",
    "scan",
    "anything",
    "think",
    "uh",
    "fits",
    "category",
    "traditional",
    "machine",
    "learning",
    "model",
    "able",
    "find",
    "ayler",
    "therefore",
    "want",
    "limit",
    "tutorial",
    "models",
    "could",
    "uh",
    "wanted",
    "use",
    "um",
    "wanted",
    "case",
    "study",
    "uh",
    "specifically",
    "linear",
    "regression",
    "instead",
    "wanted",
    "showcase",
    "also",
    "usage",
    "psychic",
    "learn",
    "pyic",
    "learn",
    "something",
    "use",
    "beyond",
    "linear",
    "regression",
    "added",
    "type",
    "machine",
    "learning",
    "models",
    "given",
    "course",
    "designed",
    "introduce",
    "world",
    "machine",
    "learning",
    "thought",
    "combine",
    "uh",
    "also",
    "psychic",
    "learning",
    "something",
    "going",
    "see",
    "time",
    "time",
    "uh",
    "using",
    "python",
    "combined",
    "machine",
    "learning",
    "also",
    "uh",
    "importing",
    "uh",
    "training",
    "test",
    "plate",
    "uh",
    "psychic",
    "learn",
    "model",
    "selection",
    "uh",
    "split",
    "data",
    "train",
    "test",
    "uh",
    "move",
    "uh",
    "uh",
    "actual",
    "training",
    "testing",
    "need",
    "first",
    "load",
    "data",
    "therefore",
    "uh",
    "uh",
    "uh",
    "sample",
    "data",
    "folder",
    "google",
    "collab",
    "uh",
    "put",
    "housing",
    "csv",
    "data",
    "data",
    "download",
    "uh",
    "go",
    "specific",
    "uh",
    "page",
    "uh",
    "go",
    "um",
    "uh",
    "also",
    "uh",
    "download",
    "data",
    "download",
    "49",
    "kab",
    "uh",
    "housing",
    "data",
    "exactly",
    "uh",
    "downloading",
    "uploading",
    "google",
    "clap",
    "housing",
    "csv",
    "folder",
    "copying",
    "path",
    "putting",
    "creating",
    "variable",
    "holds",
    "um",
    "name",
    "path",
    "data",
    "file",
    "uncore",
    "path",
    "variable",
    "string",
    "variable",
    "holds",
    "path",
    "data",
    "need",
    "need",
    "uh",
    "take",
    "file",
    "uncore",
    "path",
    "need",
    "put",
    "pd",
    "read",
    "csv",
    "uh",
    "function",
    "use",
    "order",
    "uh",
    "load",
    "data",
    "pd",
    "stands",
    "pandas",
    "short",
    "way",
    "uh",
    "naming",
    "pandas",
    "uh",
    "pd",
    "read",
    "uncore",
    "csv",
    "function",
    "taking",
    "panda",
    "library",
    "within",
    "parentheses",
    "putting",
    "file",
    "uncore",
    "path",
    "want",
    "learn",
    "basics",
    "variable",
    "different",
    "data",
    "structures",
    "basic",
    "python",
    "data",
    "science",
    "um",
    "ensure",
    "keeping",
    "specific",
    "tutorial",
    "structured",
    "talking",
    "feel",
    "free",
    "check",
    "python",
    "data",
    "science",
    "course",
    "put",
    "link",
    "um",
    "comments",
    "uh",
    "learn",
    "know",
    "yet",
    "come",
    "back",
    "tutorial",
    "learn",
    "use",
    "python",
    "combination",
    "linear",
    "regression",
    "uh",
    "first",
    "thing",
    "tend",
    "moving",
    "actual",
    "execution",
    "stage",
    "um",
    "look",
    "data",
    "perform",
    "data",
    "exploration",
    "tend",
    "look",
    "data",
    "field",
    "name",
    "variables",
    "available",
    "data",
    "data",
    "columns",
    "look",
    "columns",
    "data",
    "name",
    "uh",
    "uh",
    "data",
    "fields",
    "let",
    "go",
    "ahead",
    "command",
    "enter",
    "see",
    "longitude",
    "like",
    "attitude",
    "housing",
    "unor",
    "median",
    "age",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "population",
    "basically",
    "um",
    "amount",
    "people",
    "living",
    "households",
    "houses",
    "households",
    "median",
    "income",
    "median",
    "housecore",
    "value",
    "ocean",
    "proximity",
    "might",
    "notice",
    "name",
    "variables",
    "bit",
    "different",
    "actual",
    "um",
    "documentation",
    "california",
    "house",
    "see",
    "naming",
    "different",
    "underlying",
    "uh",
    "explanation",
    "trying",
    "make",
    "uh",
    "nicer",
    "uh",
    "represent",
    "better",
    "uh",
    "naming",
    "uh",
    "common",
    "um",
    "thing",
    "see",
    "python",
    "dealing",
    "uh",
    "data",
    "uh",
    "underscores",
    "name",
    "approvation",
    "housing",
    "uncore",
    "median",
    "ag",
    "case",
    "see",
    "says",
    "house",
    "um",
    "age",
    "bit",
    "different",
    "meaning",
    "still",
    "median",
    "house",
    "age",
    "block",
    "group",
    "uh",
    "one",
    "thing",
    "uh",
    "also",
    "uh",
    "notice",
    "um",
    "official",
    "uh",
    "documentation",
    "um",
    "one",
    "extra",
    "variable",
    "ocean",
    "proximity",
    "basically",
    "uh",
    "describes",
    "uh",
    "clos",
    "cless",
    "house",
    "ocean",
    "course",
    "uh",
    "people",
    "definitely",
    "mean",
    "increase",
    "decrease",
    "house",
    "price",
    "basically",
    "um",
    "variables",
    "next",
    "thing",
    "tend",
    "look",
    "actual",
    "data",
    "one",
    "thing",
    "look",
    "um",
    "top",
    "10",
    "rows",
    "data",
    "instead",
    "printing",
    "entire",
    "uh",
    "data",
    "frame",
    "go",
    "uh",
    "execute",
    "specific",
    "part",
    "code",
    "command",
    "see",
    "top",
    "10",
    "rows",
    "uh",
    "data",
    "longitude",
    "latitude",
    "housing",
    "median",
    "age",
    "see",
    "see",
    "41",
    "year",
    "21",
    "year",
    "52",
    "year",
    "basically",
    "number",
    "years",
    "house",
    "median",
    "age",
    "house",
    "41",
    "21",
    "52",
    "per",
    "block",
    "number",
    "total",
    "bedrooms",
    "see",
    "uh",
    "um",
    "blog",
    "uh",
    "total",
    "number",
    "rooms",
    "houses",
    "already",
    "seeing",
    "data",
    "consists",
    "large",
    "numbers",
    "something",
    "take",
    "account",
    "uh",
    "dealing",
    "machine",
    "learning",
    "models",
    "especially",
    "line",
    "regression",
    "total",
    "bedrooms",
    "um",
    "population",
    "households",
    "median",
    "income",
    "median",
    "house",
    "value",
    "ocean",
    "proximity",
    "one",
    "thing",
    "see",
    "right",
    "bed",
    "uh",
    "longitude",
    "latitude",
    "uh",
    "uh",
    "unique",
    "uh",
    "characteristics",
    "um",
    "longitude",
    "minuses",
    "latitude",
    "pluses",
    "uh",
    "fine",
    "linear",
    "regression",
    "basically",
    "looking",
    "uh",
    "whether",
    "variation",
    "certain",
    "independent",
    "variables",
    "case",
    "longitude",
    "latitude",
    "cause",
    "change",
    "dependent",
    "variable",
    "refresh",
    "memory",
    "linear",
    "regression",
    "case",
    "um",
    "dealing",
    "multiple",
    "inine",
    "regression",
    "one",
    "independent",
    "variables",
    "independent",
    "variables",
    "different",
    "features",
    "describe",
    "house",
    "except",
    "house",
    "price",
    "median",
    "house",
    "value",
    "dependent",
    "variable",
    "basically",
    "trying",
    "figure",
    "want",
    "see",
    "features",
    "house",
    "cause",
    "define",
    "house",
    "price",
    "want",
    "identify",
    "um",
    "features",
    "cause",
    "change",
    "dependent",
    "variable",
    "specifically",
    "uh",
    "uh",
    "change",
    "median",
    "house",
    "price",
    "uh",
    "volue",
    "apply",
    "one",
    "unit",
    "change",
    "independent",
    "feature",
    "multiple",
    "linear",
    "regession",
    "learned",
    "theory",
    "lecture",
    "linear",
    "regression",
    "tries",
    "use",
    "causal",
    "analysis",
    "tries",
    "keep",
    "independent",
    "variables",
    "constant",
    "investigate",
    "specific",
    "independent",
    "variable",
    "one",
    "unit",
    "uh",
    "change",
    "uh",
    "increase",
    "uh",
    "specific",
    "independent",
    "variable",
    "result",
    "kind",
    "change",
    "dependent",
    "variable",
    "instance",
    "change",
    "one",
    "unit",
    "uh",
    "housing",
    "median",
    "age",
    "um",
    "correspond",
    "change",
    "median",
    "household",
    "value",
    "keeping",
    "everything",
    "else",
    "concent",
    "basically",
    "idea",
    "behind",
    "multi",
    "multiple",
    "linear",
    "regression",
    "using",
    "specific",
    "use",
    "case",
    "um",
    "also",
    "want",
    "find",
    "uh",
    "data",
    "types",
    "whether",
    "learn",
    "bit",
    "data",
    "proceeding",
    "next",
    "step",
    "tend",
    "use",
    "uh",
    "info",
    "uh",
    "function",
    "panel",
    "given",
    "data",
    "penis",
    "data",
    "frame",
    "data",
    "info",
    "parentheses",
    "uh",
    "show",
    "us",
    "data",
    "type",
    "number",
    "new",
    "values",
    "per",
    "variable",
    "um",
    "already",
    "noticed",
    "header",
    "also",
    "see",
    "confirmed",
    "ocean",
    "proximity",
    "variable",
    "numeric",
    "value",
    "see",
    "nearby",
    "um",
    "also",
    "value",
    "variable",
    "unlike",
    "values",
    "represented",
    "string",
    "something",
    "need",
    "take",
    "account",
    "later",
    "uh",
    "data",
    "prop",
    "processing",
    "actually",
    "uh",
    "actually",
    "run",
    "model",
    "need",
    "something",
    "specific",
    "variable",
    "need",
    "process",
    "um",
    "rest",
    "dealing",
    "numeric",
    "variables",
    "see",
    "longitude",
    "latitude",
    "variables",
    "including",
    "dependent",
    "variable",
    "numeric",
    "variable",
    "float",
    "64",
    "variable",
    "needs",
    "taken",
    "care",
    "ocean",
    "uncore",
    "proximity",
    "uh",
    "um",
    "actually",
    "later",
    "also",
    "see",
    "um",
    "categorical",
    "string",
    "variable",
    "basically",
    "means",
    "different",
    "categories",
    "um",
    "instance",
    "uh",
    "let",
    "us",
    "actually",
    "quickly",
    "let",
    "see",
    "unique",
    "values",
    "variable",
    "take",
    "name",
    "variable",
    "copied",
    "overview",
    "unique",
    "give",
    "us",
    "unique",
    "values",
    "categorical",
    "variable",
    "go",
    "actually",
    "five",
    "different",
    "unique",
    "values",
    "categorical",
    "string",
    "variable",
    "means",
    "ocean",
    "proximity",
    "take",
    "uh",
    "five",
    "different",
    "values",
    "either",
    "near",
    "bay",
    "less",
    "1",
    "hour",
    "ocean",
    "inland",
    "near",
    "ocean",
    "uh",
    "iceland",
    "means",
    "dealing",
    "feature",
    "describes",
    "distance",
    "uh",
    "block",
    "ocean",
    "underlying",
    "idea",
    "maybe",
    "specific",
    "feature",
    "statistically",
    "significant",
    "impact",
    "house",
    "value",
    "meaning",
    "might",
    "possible",
    "people",
    "um",
    "certain",
    "areas",
    "certain",
    "countries",
    "living",
    "uh",
    "nearby",
    "ocean",
    "uh",
    "increasing",
    "value",
    "house",
    "huge",
    "demand",
    "houses",
    "near",
    "ocean",
    "people",
    "prefer",
    "uh",
    "leave",
    "near",
    "ocean",
    "likely",
    "positive",
    "relationship",
    "uh",
    "negative",
    "relationship",
    "means",
    "uh",
    "people",
    "uh",
    "uh",
    "area",
    "california",
    "instance",
    "people",
    "prefer",
    "live",
    "near",
    "ocean",
    "uh",
    "see",
    "negative",
    "relationship",
    "see",
    "um",
    "increase",
    "uh",
    "uh",
    "people",
    "uh",
    "house",
    "uh",
    "um",
    "area",
    "uh",
    "close",
    "ocean",
    "ocean",
    "house",
    "value",
    "higher",
    "something",
    "want",
    "figure",
    "line",
    "regression",
    "want",
    "understand",
    "features",
    "uh",
    "define",
    "value",
    "house",
    "say",
    "um",
    "house",
    "characteristics",
    "likely",
    "house",
    "price",
    "higher",
    "house",
    "price",
    "lower",
    "uh",
    "linear",
    "aggression",
    "helps",
    "us",
    "understand",
    "features",
    "also",
    "understand",
    "much",
    "higher",
    "much",
    "lower",
    "value",
    "house",
    "certain",
    "characteristics",
    "increase",
    "certain",
    "characteristics",
    "one",
    "unit",
    "next",
    "going",
    "look",
    "uh",
    "missing",
    "data",
    "data",
    "order",
    "proper",
    "machine",
    "learning",
    "model",
    "need",
    "uh",
    "data",
    "processing",
    "need",
    "need",
    "check",
    "uh",
    "missing",
    "values",
    "data",
    "need",
    "understand",
    "amount",
    "new",
    "values",
    "per",
    "data",
    "field",
    "help",
    "us",
    "understand",
    "whether",
    "uh",
    "uh",
    "remove",
    "missing",
    "values",
    "need",
    "imputation",
    "depending",
    "amount",
    "missing",
    "data",
    "got",
    "data",
    "understand",
    "solutions",
    "need",
    "take",
    "see",
    "uh",
    "n",
    "values",
    "comes",
    "longitude",
    "latitude",
    "housing",
    "median",
    "age",
    "variables",
    "except",
    "one",
    "variable",
    "one",
    "independent",
    "variable",
    "total",
    "bedrooms",
    "see",
    "um",
    "observations",
    "got",
    "total",
    "uh",
    "underscore",
    "bedrooms",
    "variable",
    "207",
    "cases",
    "corresponding",
    "uh",
    "information",
    "comes",
    "representing",
    "numbers",
    "percentages",
    "something",
    "next",
    "step",
    "see",
    "um",
    "uh",
    "entire",
    "data",
    "set",
    "uh",
    "total",
    "underscore",
    "bedrooms",
    "variable",
    "um",
    "n",
    "n",
    "uh",
    "3",
    "missing",
    "really",
    "important",
    "simply",
    "looking",
    "number",
    "times",
    "uh",
    "number",
    "missing",
    "uh",
    "observations",
    "perir",
    "data",
    "field",
    "wo",
    "helpful",
    "able",
    "understand",
    "relatively",
    "much",
    "data",
    "missing",
    "certain",
    "variable",
    "50",
    "missing",
    "80",
    "missing",
    "means",
    "majority",
    "house",
    "blocks",
    "information",
    "including",
    "beneficial",
    "morel",
    "accurate",
    "include",
    "result",
    "biased",
    "uh",
    "morel",
    "majority",
    "observations",
    "uh",
    "information",
    "certain",
    "observations",
    "inform",
    "information",
    "automatically",
    "skew",
    "results",
    "biased",
    "results",
    "therefore",
    "uh",
    "majority",
    "um",
    "data",
    "set",
    "specific",
    "uh",
    "variable",
    "missing",
    "would",
    "suggest",
    "choose",
    "drop",
    "independent",
    "variable",
    "case",
    "one",
    "uh",
    "uh",
    "uh",
    "house",
    "blocks",
    "missing",
    "information",
    "means",
    "gives",
    "confidence",
    "uh",
    "would",
    "rather",
    "keep",
    "independent",
    "variable",
    "drop",
    "observations",
    "total",
    "uh",
    "underscore",
    "bedrooms",
    "uh",
    "information",
    "another",
    "solution",
    "could",
    "also",
    "uh",
    "instead",
    "dropping",
    "entire",
    "independent",
    "variable",
    "uh",
    "use",
    "sort",
    "imputation",
    "technique",
    "uh",
    "means",
    "uh",
    "uh",
    "try",
    "find",
    "way",
    "systematically",
    "find",
    "replacement",
    "missing",
    "value",
    "use",
    "mean",
    "imputation",
    "median",
    "imput",
    "ation",
    "model",
    "based",
    "advanced",
    "statistical",
    "econometrical",
    "approaches",
    "perform",
    "imputation",
    "scope",
    "problem",
    "would",
    "say",
    "look",
    "uh",
    "percentage",
    "uh",
    "observations",
    "uh",
    "independent",
    "variable",
    "missing",
    "uh",
    "values",
    "uh",
    "low",
    "like",
    "less",
    "10",
    "large",
    "data",
    "set",
    "uh",
    "uh",
    "comfortable",
    "dropping",
    "observations",
    "small",
    "data",
    "set",
    "got",
    "100",
    "observations",
    "like",
    "20",
    "40",
    "missing",
    "consider",
    "imputation",
    "try",
    "find",
    "values",
    "um",
    "used",
    "order",
    "replace",
    "missing",
    "values",
    "uh",
    "information",
    "identified",
    "missing",
    "values",
    "next",
    "thing",
    "uh",
    "clean",
    "data",
    "using",
    "data",
    "got",
    "using",
    "function",
    "drop",
    "na",
    "means",
    "drop",
    "um",
    "uh",
    "observations",
    "uh",
    "value",
    "missing",
    "dropping",
    "observations",
    "total",
    "underscore",
    "bedrooms",
    "null",
    "value",
    "getting",
    "rid",
    "missing",
    "observations",
    "checking",
    "whether",
    "got",
    "rid",
    "missing",
    "observations",
    "see",
    "printing",
    "data",
    "n",
    "sum",
    "summing",
    "number",
    "uh",
    "missing",
    "observations",
    "values",
    "per",
    "uh",
    "variable",
    "uh",
    "longer",
    "missing",
    "observations",
    "successfully",
    "deleted",
    "missing",
    "observations",
    "next",
    "state",
    "describe",
    "data",
    "uh",
    "descriptive",
    "statistics",
    "data",
    "visualization",
    "moving",
    "towards",
    "caal",
    "analysis",
    "predictive",
    "analysis",
    "sort",
    "machine",
    "learning",
    "traditional",
    "machine",
    "learning",
    "approach",
    "try",
    "first",
    "look",
    "data",
    "try",
    "understand",
    "data",
    "see",
    "uh",
    "whether",
    "seeing",
    "patterns",
    "uh",
    "mean",
    "uh",
    "different",
    "um",
    "numeric",
    "data",
    "fields",
    "uh",
    "certain",
    "uh",
    "categorical",
    "values",
    "cause",
    "un",
    "unbalanced",
    "data",
    "things",
    "discover",
    "uh",
    "early",
    "uh",
    "moving",
    "uh",
    "model",
    "training",
    "testing",
    "blindly",
    "believing",
    "numbers",
    "data",
    "visualization",
    "techniques",
    "data",
    "exploration",
    "great",
    "way",
    "understand",
    "uh",
    "uh",
    "data",
    "got",
    "using",
    "uh",
    "order",
    "train",
    "machine",
    "learning",
    "model",
    "using",
    "uh",
    "traditional",
    "describe",
    "function",
    "pendas",
    "data",
    "describe",
    "parentheses",
    "give",
    "descriptive",
    "statistics",
    "data",
    "see",
    "total",
    "got",
    "uh",
    "20",
    "640",
    "observations",
    "uh",
    "uh",
    "also",
    "mean",
    "uh",
    "variables",
    "see",
    "per",
    "variable",
    "count",
    "basically",
    "means",
    "variables",
    "number",
    "rows",
    "uh",
    "mean",
    "means",
    "um",
    "mean",
    "uh",
    "variables",
    "per",
    "variable",
    "mean",
    "standard",
    "deviation",
    "square",
    "root",
    "variance",
    "minimum",
    "maximum",
    "also",
    "25th",
    "percentile",
    "15",
    "percentile",
    "75th",
    "percentile",
    "uh",
    "percentile",
    "uh",
    "quantiles",
    "uh",
    "statistical",
    "terms",
    "oftenly",
    "use",
    "25th",
    "percentile",
    "first",
    "quantile",
    "15",
    "percentile",
    "second",
    "quantile",
    "uh",
    "median",
    "75th",
    "percentile",
    "third",
    "quantile",
    "uh",
    "basically",
    "means",
    "uh",
    "percentiles",
    "help",
    "us",
    "understand",
    "threshold",
    "comes",
    "looking",
    "um",
    "observations",
    "uh",
    "fall",
    "25",
    "uh",
    "25",
    "look",
    "uh",
    "standard",
    "deviation",
    "standard",
    "deviation",
    "helps",
    "us",
    "interpret",
    "variation",
    "data",
    "unit",
    "scale",
    "variable",
    "case",
    "variable",
    "median",
    "house",
    "value",
    "mean",
    "equal",
    "206",
    "approximately",
    "less",
    "uh",
    "range",
    "206",
    "k",
    "standard",
    "deviation",
    "115k",
    "means",
    "uh",
    "data",
    "set",
    "find",
    "blocks",
    "median",
    "house",
    "value",
    "uh",
    "200",
    "uh",
    "6k",
    "206k",
    "plus",
    "115k",
    "around",
    "321k",
    "blocks",
    "median",
    "house",
    "value",
    "around",
    "321k",
    "also",
    "blocks",
    "um",
    "median",
    "house",
    "value",
    "around",
    "uh",
    "91k",
    "minus",
    "115k",
    "idea",
    "behind",
    "standard",
    "deviation",
    "variation",
    "data",
    "next",
    "interpret",
    "idea",
    "uh",
    "minimum",
    "maximum",
    "data",
    "data",
    "fields",
    "minimum",
    "help",
    "understand",
    "minimum",
    "value",
    "per",
    "data",
    "field",
    "numeric",
    "data",
    "field",
    "maximum",
    "value",
    "range",
    "values",
    "looking",
    "case",
    "median",
    "house",
    "value",
    "means",
    "um",
    "uh",
    "minimum",
    "median",
    "house",
    "value",
    "per",
    "uh",
    "block",
    "uh",
    "case",
    "maximum",
    "um",
    "highest",
    "value",
    "per",
    "block",
    "comes",
    "medan",
    "house",
    "value",
    "uh",
    "help",
    "understand",
    "um",
    "look",
    "aggregated",
    "data",
    "median",
    "house",
    "value",
    "blocks",
    "uh",
    "cheapest",
    "uh",
    "houses",
    "comes",
    "valuation",
    "expensive",
    "uh",
    "blocks",
    "houses",
    "see",
    "uh",
    "cheapest",
    "um",
    "block",
    "uh",
    "block",
    "median",
    "house",
    "value",
    "uh",
    "15k",
    "house",
    "block",
    "um",
    "highest",
    "valuation",
    "comes",
    "median",
    "house",
    "value",
    "uh",
    "median",
    "um",
    "valuation",
    "houses",
    "equal",
    "and1",
    "means",
    "look",
    "blocks",
    "houses",
    "um",
    "uh",
    "median",
    "house",
    "value",
    "expensive",
    "blocks",
    "maximum",
    "500k",
    "uh",
    "next",
    "thing",
    "tend",
    "visualize",
    "data",
    "tend",
    "start",
    "dependent",
    "variable",
    "variable",
    "interest",
    "target",
    "variable",
    "response",
    "variable",
    "case",
    "median",
    "house",
    "value",
    "serve",
    "us",
    "dependent",
    "variable",
    "want",
    "upload",
    "histogram",
    "uh",
    "order",
    "understand",
    "distribution",
    "median",
    "house",
    "values",
    "want",
    "see",
    "looking",
    "data",
    "um",
    "frequently",
    "appearing",
    "median",
    "house",
    "values",
    "uh",
    "uh",
    "type",
    "blocks",
    "um",
    "unique",
    "less",
    "frequently",
    "um",
    "appearing",
    "uh",
    "meded",
    "house",
    "values",
    "plotting",
    "type",
    "plots",
    "see",
    "outliers",
    "um",
    "frequently",
    "appearing",
    "values",
    "also",
    "values",
    "uh",
    "go",
    "uh",
    "uh",
    "lying",
    "outside",
    "range",
    "help",
    "identify",
    "learn",
    "data",
    "toid",
    "identify",
    "outliers",
    "data",
    "using",
    "uh",
    "curn",
    "uh",
    "library",
    "given",
    "earlier",
    "already",
    "imported",
    "libraries",
    "need",
    "import",
    "setting",
    "gd",
    "basically",
    "means",
    "saying",
    "background",
    "white",
    "also",
    "want",
    "discrete",
    "means",
    "discrete",
    "behind",
    "initializing",
    "size",
    "figure",
    "plt",
    "comes",
    "met",
    "plotly",
    "p",
    "plot",
    "setting",
    "figure",
    "figure",
    "size",
    "10x",
    "6",
    "um",
    "10",
    "six",
    "main",
    "plot",
    "um",
    "using",
    "uh",
    "plot",
    "function",
    "curn",
    "taking",
    "uh",
    "clean",
    "data",
    "removed",
    "missing",
    "data",
    "picking",
    "uh",
    "variable",
    "interest",
    "median",
    "house",
    "value",
    "saying",
    "upload",
    "um",
    "histogram",
    "using",
    "fors",
    "green",
    "color",
    "uh",
    "saying",
    "uh",
    "title",
    "figure",
    "distribution",
    "p",
    "house",
    "values",
    "um",
    "also",
    "mentioning",
    "x",
    "label",
    "basically",
    "means",
    "name",
    "variable",
    "putting",
    "xaxis",
    "median",
    "house",
    "value",
    "label",
    "name",
    "variable",
    "need",
    "put",
    "ais",
    "saying",
    "pl",
    "show",
    "means",
    "show",
    "figure",
    "basically",
    "python",
    "visualization",
    "works",
    "uh",
    "first",
    "need",
    "write",
    "actual",
    "uh",
    "figure",
    "size",
    "uh",
    "need",
    "uh",
    "set",
    "uh",
    "function",
    "uh",
    "right",
    "variable",
    "provide",
    "data",
    "visualization",
    "need",
    "put",
    "title",
    "need",
    "put",
    "x",
    "label",
    "label",
    "need",
    "say",
    "show",
    "visualization",
    "uh",
    "want",
    "learn",
    "visualization",
    "techniques",
    "uh",
    "make",
    "sure",
    "check",
    "python",
    "data",
    "science",
    "course",
    "cuz",
    "one",
    "help",
    "understand",
    "slowly",
    "uh",
    "detail",
    "uh",
    "visualize",
    "data",
    "visualizing",
    "frequency",
    "median",
    "house",
    "values",
    "entire",
    "data",
    "set",
    "means",
    "looking",
    "um",
    "number",
    "times",
    "median",
    "house",
    "values",
    "appear",
    "data",
    "set",
    "uh",
    "want",
    "understand",
    "uh",
    "certain",
    "uh",
    "median",
    "house",
    "values",
    "appear",
    "often",
    "certain",
    "house",
    "values",
    "appear",
    "often",
    "may",
    "considered",
    "outliers",
    "uh",
    "want",
    "data",
    "keep",
    "uh",
    "relevant",
    "representative",
    "data",
    "points",
    "want",
    "derive",
    "conclusions",
    "hold",
    "majority",
    "uh",
    "uh",
    "observations",
    "outliers",
    "using",
    "uh",
    "representative",
    "data",
    "order",
    "run",
    "linear",
    "regression",
    "make",
    "conclusions",
    "looking",
    "graph",
    "see",
    "uh",
    "certain",
    "cluster",
    "um",
    "median",
    "house",
    "values",
    "appear",
    "quite",
    "often",
    "cases",
    "frequency",
    "high",
    "see",
    "uh",
    "instance",
    "houses",
    "block",
    "appear",
    "um",
    "often",
    "instance",
    "median",
    "house",
    "value",
    "u",
    "160",
    "170k",
    "appears",
    "frequently",
    "see",
    "frequency",
    "frequently",
    "appearing",
    "medan",
    "house",
    "values",
    "um",
    "cases",
    "um",
    "see",
    "see",
    "houses",
    "uh",
    "whose",
    "median",
    "house",
    "value",
    "appearing",
    "often",
    "see",
    "frequency",
    "low",
    "um",
    "roughly",
    "speaking",
    "houses",
    "unusual",
    "houses",
    "considered",
    "outliers",
    "holds",
    "also",
    "houses",
    "see",
    "frequency",
    "low",
    "means",
    "population",
    "houses",
    "california",
    "house",
    "prices",
    "likely",
    "see",
    "houses",
    "uh",
    "blocks",
    "houses",
    "whose",
    "medium",
    "value",
    "let",
    "say",
    "um",
    "17k",
    "uh",
    "let",
    "say",
    "uh",
    "300",
    "350k",
    "anything",
    "considered",
    "unusual",
    "often",
    "see",
    "houses",
    "um",
    "house",
    "blocks",
    "median",
    "house",
    "value",
    "less",
    "uh",
    "70",
    "60k",
    "uh",
    "also",
    "uh",
    "houses",
    "um",
    "370",
    "400k",
    "consider",
    "uh",
    "dealing",
    "1990",
    "um",
    "year",
    "data",
    "current",
    "uh",
    "prices",
    "nowadays",
    "uh",
    "californian",
    "houses",
    "much",
    "expensive",
    "data",
    "coming",
    "1990",
    "uh",
    "take",
    "account",
    "interpreting",
    "type",
    "data",
    "visualizations",
    "uh",
    "use",
    "idea",
    "inter",
    "quantile",
    "range",
    "remove",
    "outl",
    "basically",
    "means",
    "looking",
    "lowest",
    "25th",
    "uh",
    "percentile",
    "uh",
    "looking",
    "first",
    "quantile",
    "25th",
    "percentile",
    "looking",
    "upper",
    "25th",
    "um",
    "percent",
    "means",
    "third",
    "quantile",
    "75th",
    "percentile",
    "want",
    "basically",
    "remove",
    "uh",
    "using",
    "idea",
    "25th",
    "percentile",
    "75th",
    "percentile",
    "first",
    "quantile",
    "third",
    "quantile",
    "identify",
    "um",
    "uh",
    "observations",
    "blocks",
    "median",
    "house",
    "value",
    "uh",
    "25th",
    "per",
    "h",
    "75",
    "basically",
    "want",
    "uh",
    "get",
    "middle",
    "part",
    "data",
    "want",
    "get",
    "data",
    "median",
    "house",
    "uh",
    "value",
    "25th",
    "percentile",
    "u",
    "uh",
    "median",
    "house",
    "values",
    "uh",
    "lowest",
    "25",
    "uh",
    "percent",
    "also",
    "want",
    "remove",
    "large",
    "median",
    "house",
    "values",
    "want",
    "uh",
    "keep",
    "data",
    "normal",
    "uh",
    "representative",
    "blocks",
    "blocks",
    "medan",
    "house",
    "uh",
    "value",
    "lowest",
    "25",
    "smaller",
    "largest",
    "25",
    "using",
    "statistical",
    "uh",
    "term",
    "called",
    "inter",
    "quan",
    "range",
    "need",
    "know",
    "name",
    "think",
    "would",
    "work",
    "understand",
    "popular",
    "way",
    "uh",
    "making",
    "datadriven",
    "uh",
    "removal",
    "outliers",
    "selecting",
    "um",
    "25th",
    "percentile",
    "using",
    "quantile",
    "function",
    "pandas",
    "uh",
    "saying",
    "find",
    "um",
    "value",
    "divides",
    "entire",
    "uh",
    "block",
    "observations",
    "block",
    "observations",
    "observations",
    "medan",
    "house",
    "value",
    "um",
    "um",
    "25th",
    "percentile",
    "25th",
    "percentile",
    "largest",
    "75",
    "smallest",
    "25",
    "comes",
    "median",
    "house",
    "value",
    "removing",
    "25",
    "using",
    "q1",
    "uh",
    "using",
    "uh",
    "q3",
    "order",
    "remove",
    "large",
    "median",
    "house",
    "valu",
    "uh",
    "upper",
    "25th",
    "percentile",
    "uh",
    "order",
    "um",
    "calculate",
    "inter",
    "quanti",
    "range",
    "need",
    "uh",
    "pick",
    "q3",
    "subtract",
    "q1",
    "understand",
    "idea",
    "q1",
    "q3",
    "quantas",
    "better",
    "let",
    "actually",
    "print",
    "uh",
    "q1",
    "uh",
    "q3",
    "let",
    "actually",
    "remove",
    "part",
    "run",
    "see",
    "finding",
    "uh",
    "q1",
    "25th",
    "percentile",
    "first",
    "quantile",
    "equal",
    "basically",
    "number",
    "means",
    "um",
    "uh",
    "25",
    "um",
    "um",
    "observations",
    "smallest",
    "observations",
    "median",
    "house",
    "value",
    "uh",
    "remaining",
    "75",
    "uh",
    "observations",
    "meeting",
    "house",
    "value",
    "uh",
    "q3",
    "third",
    "quantile",
    "75th",
    "percentile",
    "describes",
    "threshold",
    "volume",
    "make",
    "distinction",
    "um",
    "uh",
    "lowest",
    "median",
    "house",
    "values",
    "first",
    "75th",
    "uh",
    "lowest",
    "uh",
    "median",
    "house",
    "values",
    "versus",
    "uh",
    "expensive",
    "highest",
    "median",
    "house",
    "values",
    "upper",
    "uh",
    "25",
    "uh",
    "comes",
    "median",
    "house",
    "value",
    "see",
    "distinction",
    "save",
    "700",
    "somewhere",
    "basically",
    "means",
    "comes",
    "uh",
    "blocks",
    "uh",
    "houses",
    "expensive",
    "ones",
    "highest",
    "valuation",
    "25",
    "top",
    "rated",
    "median",
    "house",
    "values",
    "something",
    "want",
    "remove",
    "want",
    "remove",
    "observations",
    "smallest",
    "median",
    "house",
    "value",
    "largest",
    "median",
    "house",
    "values",
    "usually",
    "common",
    "practice",
    "comes",
    "inter",
    "quantile",
    "uh",
    "range",
    "approach",
    "multiply",
    "inter",
    "quantile",
    "range",
    "order",
    "um",
    "obtain",
    "lower",
    "bound",
    "upper",
    "bound",
    "understand",
    "um",
    "thresholds",
    "need",
    "use",
    "order",
    "remove",
    "uh",
    "blocks",
    "uh",
    "observation",
    "data",
    "med",
    "house",
    "value",
    "small",
    "large",
    "multiply",
    "iqr",
    "inter",
    "quanti",
    "range",
    "uh",
    "subtract",
    "value",
    "q1",
    "getting",
    "lower",
    "bound",
    "uh",
    "adding",
    "value",
    "q3",
    "using",
    "getting",
    "threshold",
    "comes",
    "uh",
    "upper",
    "bound",
    "seeing",
    "um",
    "uh",
    "clean",
    "uh",
    "outliers",
    "data",
    "end",
    "uh",
    "getting",
    "um",
    "smaller",
    "data",
    "means",
    "uh",
    "previously",
    "uh",
    "20k",
    "3",
    "observations",
    "observations",
    "roughly",
    "removed",
    "um",
    "like",
    "bit",
    "observations",
    "data",
    "uh",
    "next",
    "let",
    "look",
    "variables",
    "instance",
    "median",
    "uh",
    "income",
    "um",
    "one",
    "technique",
    "use",
    "order",
    "identify",
    "outliers",
    "data",
    "using",
    "box",
    "plots",
    "wanted",
    "showcase",
    "different",
    "approaches",
    "use",
    "order",
    "visualize",
    "data",
    "identify",
    "outliers",
    "familiar",
    "uh",
    "different",
    "techniques",
    "let",
    "go",
    "ahead",
    "plot",
    "uh",
    "box",
    "plot",
    "box",
    "plot",
    "statistical",
    "um",
    "way",
    "represent",
    "data",
    "uh",
    "central",
    "boook",
    "uh",
    "represents",
    "inter",
    "quant",
    "range",
    "um",
    "iqr",
    "uh",
    "uh",
    "bottom",
    "top",
    "edges",
    "indicate",
    "25th",
    "percentile",
    "first",
    "quantile",
    "75",
    "h",
    "third",
    "quantile",
    "respectively",
    "length",
    "box",
    "see",
    "uh",
    "dark",
    "part",
    "basically",
    "50",
    "data",
    "median",
    "income",
    "uh",
    "uh",
    "median",
    "uh",
    "line",
    "inside",
    "box",
    "um",
    "uh",
    "one",
    "uh",
    "contrast",
    "color",
    "represents",
    "median",
    "data",
    "set",
    "median",
    "middle",
    "value",
    "data",
    "sorted",
    "ascending",
    "order",
    "whiskers",
    "box",
    "flo",
    "line",
    "whiskers",
    "extends",
    "top",
    "bottom",
    "box",
    "indicate",
    "range",
    "rest",
    "data",
    "set",
    "excluding",
    "outliers",
    "typically",
    "iqr",
    "times",
    "um",
    "iqr",
    "uh",
    "q1",
    "something",
    "also",
    "saw",
    "uh",
    "previously",
    "removing",
    "outliers",
    "median",
    "house",
    "volum",
    "order",
    "um",
    "identify",
    "outliers",
    "quickly",
    "see",
    "points",
    "um",
    "lie",
    "time",
    "iqr",
    "um",
    "third",
    "quantile",
    "75",
    "h",
    "um",
    "something",
    "also",
    "see",
    "means",
    "uh",
    "blocks",
    "houses",
    "unusually",
    "high",
    "median",
    "income",
    "something",
    "want",
    "remove",
    "data",
    "therefore",
    "use",
    "uh",
    "exactly",
    "approach",
    "used",
    "previously",
    "median",
    "house",
    "value",
    "identify",
    "uh",
    "25th",
    "percentile",
    "first",
    "quantile",
    "q1",
    "q3",
    "third",
    "quantile",
    "75th",
    "percentile",
    "compute",
    "iqr",
    "um",
    "obtaining",
    "lower",
    "bound",
    "upper",
    "upper",
    "bound",
    "using",
    "um",
    "scale",
    "using",
    "lower",
    "bound",
    "upper",
    "bound",
    "um",
    "use",
    "filters",
    "order",
    "remove",
    "data",
    "observations",
    "medium",
    "income",
    "lower",
    "bound",
    "observations",
    "median",
    "income",
    "upper",
    "bound",
    "using",
    "lower",
    "bound",
    "upper",
    "bound",
    "perform",
    "double",
    "filtering",
    "using",
    "two",
    "filters",
    "row",
    "see",
    "using",
    "parenthesis",
    "end",
    "functionality",
    "tell",
    "python",
    "well",
    "first",
    "look",
    "condition",
    "satisfied",
    "observations",
    "median",
    "income",
    "lower",
    "bound",
    "time",
    "hold",
    "observation",
    "block",
    "median",
    "income",
    "upper",
    "bound",
    "uh",
    "block",
    "observation",
    "data",
    "satisfies",
    "two",
    "criteria",
    "dealing",
    "good",
    "point",
    "normal",
    "point",
    "keep",
    "saying",
    "new",
    "data",
    "let",
    "actually",
    "go",
    "ahead",
    "execute",
    "code",
    "case",
    "see",
    "high",
    "layers",
    "lie",
    "part",
    "box",
    "putot",
    "end",
    "clean",
    "data",
    "taking",
    "clean",
    "data",
    "putting",
    "data",
    "simplicity",
    "uh",
    "data",
    "uh",
    "much",
    "clean",
    "uh",
    "better",
    "representation",
    "population",
    "something",
    "ideally",
    "want",
    "want",
    "find",
    "features",
    "uh",
    "describe",
    "define",
    "house",
    "value",
    "based",
    "unique",
    "rare",
    "houses",
    "expensive",
    "blogs",
    "uh",
    "high",
    "income",
    "uh",
    "people",
    "rather",
    "want",
    "see",
    "uh",
    "uh",
    "true",
    "representation",
    "frequently",
    "appearing",
    "data",
    "features",
    "define",
    "house",
    "value",
    "prices",
    "uh",
    "common",
    "uh",
    "houses",
    "common",
    "areas",
    "people",
    "average",
    "normal",
    "income",
    "want",
    "uh",
    "find",
    "uh",
    "next",
    "thing",
    "tend",
    "uh",
    "comes",
    "especially",
    "regression",
    "nases",
    "caal",
    "nases",
    "plot",
    "correlation",
    "heat",
    "map",
    "means",
    "uh",
    "getting",
    "um",
    "uh",
    "correlation",
    "matrix",
    "pairwise",
    "correlation",
    "score",
    "uh",
    "pair",
    "variables",
    "data",
    "comes",
    "linear",
    "regression",
    "one",
    "uh",
    "assumptions",
    "linear",
    "regression",
    "learned",
    "theory",
    "part",
    "perfect",
    "multicolinearity",
    "means",
    "high",
    "correlation",
    "pair",
    "independent",
    "variables",
    "knowing",
    "one",
    "help",
    "us",
    "automatically",
    "define",
    "value",
    "independent",
    "variable",
    "correlation",
    "two",
    "independent",
    "variables",
    "high",
    "means",
    "might",
    "potentially",
    "dealing",
    "multicolinearity",
    "something",
    "want",
    "avoid",
    "hit",
    "map",
    "great",
    "way",
    "identify",
    "whether",
    "type",
    "problematic",
    "independent",
    "variables",
    "whether",
    "need",
    "drop",
    "maybe",
    "multiple",
    "ensure",
    "dealing",
    "proper",
    "linear",
    "regression",
    "model",
    "assumptions",
    "lession",
    "model",
    "satisfied",
    "look",
    "correlation",
    "heat",
    "map",
    "um",
    "uh",
    "use",
    "curn",
    "order",
    "plot",
    "see",
    "colors",
    "light",
    "white",
    "till",
    "dark",
    "green",
    "uh",
    "light",
    "means",
    "um",
    "negative",
    "strong",
    "negative",
    "correlation",
    "dark",
    "uh",
    "green",
    "means",
    "strong",
    "positive",
    "correlation",
    "uh",
    "know",
    "correlation",
    "value",
    "pearson",
    "correlation",
    "take",
    "values",
    "minus",
    "one",
    "1",
    "minus",
    "one",
    "means",
    "uh",
    "strong",
    "negative",
    "correlation",
    "one",
    "means",
    "strong",
    "positive",
    "correlation",
    "um",
    "usually",
    "uh",
    "dealing",
    "correlation",
    "variable",
    "correlation",
    "longitude",
    "longitude",
    "uh",
    "correlation",
    "equal",
    "one",
    "see",
    "diagonal",
    "ones",
    "pairwise",
    "correlation",
    "variables",
    "um",
    "uh",
    "values",
    "diagonal",
    "actually",
    "equal",
    "uh",
    "mirror",
    "upper",
    "diagonal",
    "variable",
    "correlation",
    "uh",
    "two",
    "variables",
    "independent",
    "put",
    "one",
    "put",
    "first",
    "one",
    "second",
    "going",
    "basically",
    "correlation",
    "longitude",
    "ltitude",
    "correlation",
    "latitude",
    "longitude",
    "um",
    "refreshed",
    "memory",
    "let",
    "look",
    "actual",
    "number",
    "heat",
    "map",
    "see",
    "section",
    "um",
    "uh",
    "variables",
    "independent",
    "variables",
    "um",
    "low",
    "uh",
    "positive",
    "correlation",
    "uh",
    "remaining",
    "independent",
    "variables",
    "see",
    "light",
    "green",
    "uh",
    "values",
    "indicate",
    "low",
    "positive",
    "relationship",
    "pair",
    "variables",
    "one",
    "thing",
    "interesting",
    "middle",
    "part",
    "heat",
    "map",
    "dark",
    "numbers",
    "numbers",
    "uh",
    "diagonals",
    "something",
    "interpret",
    "remember",
    "diagonal",
    "diagonal",
    "basically",
    "mirror",
    "already",
    "see",
    "problem",
    "dealing",
    "variables",
    "going",
    "independent",
    "variables",
    "model",
    "high",
    "correlation",
    "problem",
    "one",
    "assumptions",
    "linear",
    "regression",
    "like",
    "saw",
    "theory",
    "section",
    "multiple",
    "uh",
    "colinearity",
    "multicolinearity",
    "problem",
    "perfect",
    "multicolinearity",
    "means",
    "dealing",
    "independent",
    "variables",
    "high",
    "correlation",
    "knowing",
    "value",
    "one",
    "variable",
    "help",
    "us",
    "know",
    "automatically",
    "value",
    "one",
    "correlation",
    "high",
    "means",
    "two",
    "variables",
    "two",
    "independent",
    "variables",
    "super",
    "high",
    "positive",
    "relationship",
    "problem",
    "might",
    "cause",
    "model",
    "result",
    "uh",
    "large",
    "standard",
    "errors",
    "also",
    "accurate",
    "generalizable",
    "model",
    "something",
    "want",
    "avoid",
    "uh",
    "want",
    "ensure",
    "assumptions",
    "model",
    "satisfied",
    "um",
    "dealing",
    "independent",
    "variable",
    "total",
    "underscore",
    "bedrooms",
    "households",
    "means",
    "number",
    "total",
    "bedrooms",
    "uh",
    "pair",
    "block",
    "uh",
    "households",
    "highly",
    "correlated",
    "positively",
    "correlated",
    "problem",
    "ideally",
    "want",
    "drop",
    "one",
    "two",
    "independent",
    "variables",
    "uh",
    "reason",
    "ca",
    "uh",
    "two",
    "variables",
    "given",
    "highly",
    "correlated",
    "already",
    "uh",
    "explain",
    "similar",
    "type",
    "information",
    "contain",
    "similar",
    "type",
    "variation",
    "means",
    "including",
    "two",
    "make",
    "sense",
    "one",
    "hand",
    "uh",
    "violating",
    "moral",
    "assumptions",
    "potentially",
    "hand",
    "even",
    "adding",
    "much",
    "volum",
    "one",
    "already",
    "shows",
    "similar",
    "variation",
    "um",
    "total",
    "underscore",
    "bedrooms",
    "basically",
    "contains",
    "similar",
    "type",
    "information",
    "households",
    "well",
    "um",
    "better",
    "drop",
    "one",
    "uh",
    "two",
    "independent",
    "variables",
    "uh",
    "question",
    "one",
    "something",
    "uh",
    "define",
    "also",
    "looking",
    "correlations",
    "uh",
    "total",
    "bedrooms",
    "uh",
    "high",
    "correlation",
    "households",
    "also",
    "see",
    "total",
    "underscore",
    "rooms",
    "high",
    "correlation",
    "households",
    "means",
    "yet",
    "another",
    "independent",
    "variable",
    "high",
    "correlation",
    "households",
    "variable",
    "total",
    "underscore",
    "rooms",
    "also",
    "high",
    "uh",
    "correlation",
    "total",
    "underscore",
    "bedroom",
    "means",
    "um",
    "decide",
    "one",
    "um",
    "frequently",
    "uh",
    "high",
    "correlation",
    "rest",
    "independent",
    "variables",
    "case",
    "seems",
    "like",
    "largest",
    "two",
    "numbers",
    "um",
    "one",
    "one",
    "see",
    "total",
    "bedroom",
    "correlation",
    "total",
    "underscore",
    "rooms",
    "uh",
    "time",
    "also",
    "see",
    "hotel",
    "bedrooms",
    "also",
    "um",
    "high",
    "correlation",
    "household",
    "means",
    "total",
    "underscore",
    "bedrooms",
    "highest",
    "correlation",
    "remaining",
    "independent",
    "variables",
    "might",
    "well",
    "drop",
    "independent",
    "variable",
    "would",
    "suggest",
    "one",
    "quick",
    "visual",
    "check",
    "look",
    "total",
    "uncore",
    "bedroom",
    "correlation",
    "dependent",
    "variable",
    "understand",
    "strong",
    "relationship",
    "response",
    "variable",
    "looking",
    "see",
    "uh",
    "total",
    "underscore",
    "bedroom",
    "uh",
    "one",
    "correlation",
    "response",
    "variable",
    "median",
    "house",
    "value",
    "comes",
    "total",
    "rooms",
    "one",
    "much",
    "higher",
    "already",
    "seeing",
    "uh",
    "feel",
    "comfortable",
    "uh",
    "excluding",
    "dropping",
    "total",
    "underscore",
    "bedroom",
    "data",
    "order",
    "ensure",
    "dealing",
    "perfect",
    "multicolinearity",
    "exactly",
    "dropping",
    "um",
    "total",
    "bedrooms",
    "longer",
    "uh",
    "total",
    "bedrooms",
    "column",
    "moving",
    "actual",
    "ca",
    "analysis",
    "one",
    "step",
    "wanted",
    "uh",
    "show",
    "uh",
    "super",
    "important",
    "comes",
    "pos",
    "analysis",
    "uh",
    "introductory",
    "econometrical",
    "stuff",
    "uh",
    "string",
    "categorical",
    "variable",
    "ways",
    "deal",
    "one",
    "easy",
    "way",
    "see",
    "um",
    "web",
    "perform",
    "one",
    "h",
    "encoding",
    "basically",
    "means",
    "transforming",
    "uh",
    "string",
    "values",
    "um",
    "near",
    "bay",
    "less",
    "1",
    "hour",
    "ocean",
    "uh",
    "inland",
    "near",
    "ocean",
    "iceland",
    "transform",
    "values",
    "numbers",
    "ocean",
    "proximity",
    "variable",
    "values",
    "1",
    "2",
    "3",
    "4",
    "5",
    "one",
    "way",
    "uh",
    "something",
    "like",
    "better",
    "way",
    "comes",
    "using",
    "type",
    "variables",
    "linear",
    "regression",
    "transform",
    "string",
    "uh",
    "category",
    "type",
    "variable",
    "calling",
    "dami",
    "variables",
    "dami",
    "variable",
    "means",
    "variable",
    "takes",
    "two",
    "possible",
    "values",
    "usually",
    "uh",
    "binary",
    "boolean",
    "variable",
    "means",
    "take",
    "two",
    "possible",
    "values",
    "zero",
    "one",
    "one",
    "means",
    "condition",
    "satisfied",
    "zero",
    "means",
    "condition",
    "satisfied",
    "let",
    "give",
    "example",
    "specific",
    "case",
    "ocean",
    "proximity",
    "five",
    "different",
    "values",
    "ocean",
    "proximity",
    "single",
    "variable",
    "uh",
    "use",
    "uh",
    "get",
    "underscore",
    "function",
    "python",
    "pandas",
    "order",
    "uh",
    "go",
    "one",
    "variable",
    "five",
    "different",
    "variable",
    "per",
    "category",
    "means",
    "new",
    "variables",
    "uh",
    "uh",
    "basically",
    "uh",
    "whether",
    "uh",
    "uh",
    "nearby",
    "whether",
    "less",
    "1",
    "hour",
    "uh",
    "uh",
    "ocean",
    "uh",
    "variable",
    "whether",
    "inland",
    "whether",
    "near",
    "ocean",
    "whether",
    "island",
    "separate",
    "binary",
    "variable",
    "dummy",
    "variable",
    "take",
    "value",
    "0",
    "one",
    "means",
    "going",
    "one",
    "string",
    "categorical",
    "variable",
    "five",
    "different",
    "dami",
    "variables",
    "case",
    "um",
    "dami",
    "variables",
    "see",
    "creating",
    "five",
    "dami",
    "variables",
    "uh",
    "five",
    "categories",
    "uh",
    "combining",
    "uh",
    "original",
    "data",
    "dropping",
    "ocean",
    "prox",
    "im",
    "data",
    "one",
    "hand",
    "getting",
    "rid",
    "string",
    "variable",
    "problematic",
    "variable",
    "linear",
    "regression",
    "combined",
    "pyler",
    "library",
    "cyler",
    "handle",
    "type",
    "um",
    "data",
    "comes",
    "linear",
    "regression",
    "b",
    "making",
    "job",
    "easier",
    "comes",
    "interpreting",
    "results",
    "uh",
    "interpreting",
    "linear",
    "regression",
    "cer",
    "nazes",
    "uh",
    "much",
    "easy",
    "dami",
    "variables",
    "one",
    "string",
    "categorical",
    "variable",
    "give",
    "example",
    "creating",
    "string",
    "variable",
    "uh",
    "five",
    "different",
    "dami",
    "variables",
    "five",
    "different",
    "dami",
    "variables",
    "see",
    "means",
    "looking",
    "one",
    "category",
    "let",
    "say",
    "uh",
    "ocean",
    "proximity",
    "inland",
    "means",
    "rows",
    "value",
    "equal",
    "zero",
    "means",
    "criteria",
    "satisfied",
    "means",
    "uh",
    "ocean",
    "proximity",
    "uh",
    "underscore",
    "inland",
    "equal",
    "zero",
    "means",
    "house",
    "blob",
    "dealing",
    "inland",
    "criteria",
    "satisfied",
    "otherwise",
    "value",
    "equal",
    "one",
    "rows",
    "ocean",
    "proximity",
    "inland",
    "equal",
    "one",
    "means",
    "criteria",
    "satisfied",
    "dealing",
    "house",
    "blocks",
    "indeed",
    "inland",
    "one",
    "thing",
    "thing",
    "keep",
    "mind",
    "uh",
    "comes",
    "uh",
    "transforming",
    "string",
    "categorical",
    "variable",
    "um",
    "set",
    "ds",
    "always",
    "need",
    "drop",
    "least",
    "one",
    "categories",
    "reason",
    "learned",
    "theory",
    "uh",
    "perfect",
    "multicolinearity",
    "means",
    "um",
    "five",
    "different",
    "variables",
    "perfectly",
    "correlated",
    "include",
    "values",
    "variables",
    "means",
    "um",
    "uh",
    "know",
    "uh",
    "uh",
    "block",
    "houses",
    "near",
    "bay",
    "less",
    "1",
    "hour",
    "ocean",
    "inland",
    "near",
    "ocean",
    "automatically",
    "know",
    "remaining",
    "category",
    "inland",
    "know",
    "blocks",
    "um",
    "uh",
    "ocean",
    "proximity",
    "underscore",
    "uh",
    "uh",
    "irand",
    "uh",
    "iceland",
    "equal",
    "one",
    "something",
    "want",
    "avoid",
    "definition",
    "perfect",
    "multicolinearity",
    "avoid",
    "one",
    "oils",
    "assumptions",
    "violated",
    "need",
    "drop",
    "one",
    "categories",
    "uh",
    "see",
    "uh",
    "exactly",
    "uh",
    "saying",
    "let",
    "go",
    "ahead",
    "actually",
    "drop",
    "one",
    "variables",
    "let",
    "see",
    "first",
    "set",
    "variables",
    "got",
    "got",
    "less",
    "one",
    "hour",
    "uh",
    "ocean",
    "inland",
    "iceland",
    "new",
    "bay",
    "uh",
    "new",
    "ocean",
    "let",
    "actually",
    "drop",
    "one",
    "let",
    "drop",
    "iceland",
    "uh",
    "simply",
    "let",
    "see",
    "allowing",
    "add",
    "code",
    "data",
    "equal",
    "uh",
    "data",
    "drop",
    "name",
    "variable",
    "within",
    "uh",
    "quotation",
    "marks",
    "uh",
    "x",
    "1",
    "way",
    "basically",
    "dropping",
    "one",
    "uh",
    "daming",
    "variables",
    "uh",
    "created",
    "order",
    "avoid",
    "perfect",
    "multicolinearity",
    "assumption",
    "violated",
    "go",
    "ahead",
    "print",
    "columns",
    "see",
    "uh",
    "uh",
    "column",
    "uh",
    "disappearing",
    "go",
    "successfully",
    "deleted",
    "variable",
    "let",
    "go",
    "ahead",
    "actually",
    "get",
    "head",
    "see",
    "longer",
    "string",
    "data",
    "instead",
    "got",
    "four",
    "additional",
    "binary",
    "variable",
    "string",
    "categorical",
    "variable",
    "five",
    "categories",
    "right",
    "ready",
    "actual",
    "work",
    "uh",
    "comes",
    "training",
    "machine",
    "learning",
    "model",
    "uh",
    "statistical",
    "model",
    "learn",
    "uh",
    "theory",
    "always",
    "need",
    "split",
    "data",
    "train",
    "uh",
    "test",
    "set",
    "minimum",
    "cases",
    "also",
    "need",
    "train",
    "validation",
    "test",
    "train",
    "model",
    "training",
    "data",
    "optimize",
    "model",
    "validation",
    "data",
    "find",
    "optimal",
    "set",
    "hyperparameters",
    "uh",
    "use",
    "information",
    "uh",
    "apply",
    "fitted",
    "optimized",
    "model",
    "unseen",
    "test",
    "data",
    "going",
    "skip",
    "validation",
    "set",
    "simplicity",
    "especially",
    "given",
    "dealing",
    "simple",
    "machine",
    "learning",
    "model",
    "linear",
    "regression",
    "going",
    "split",
    "data",
    "train",
    "test",
    "uh",
    "going",
    "first",
    "creating",
    "list",
    "name",
    "variables",
    "going",
    "use",
    "order",
    "um",
    "train",
    "machine",
    "learning",
    "bottle",
    "uh",
    "set",
    "independent",
    "variables",
    "set",
    "dependent",
    "variable",
    "multiple",
    "linear",
    "regression",
    "set",
    "uh",
    "independent",
    "variables",
    "long",
    "itude",
    "latitude",
    "housing",
    "median",
    "edge",
    "total",
    "rooms",
    "population",
    "households",
    "median",
    "income",
    "median",
    "house",
    "value",
    "four",
    "different",
    "categorical",
    "dami",
    "uh",
    "four",
    "different",
    "uh",
    "dami",
    "variables",
    "built",
    "categorical",
    "variable",
    "um",
    "specifying",
    "uh",
    "target",
    "variable",
    "target",
    "response",
    "variable",
    "dependent",
    "variable",
    "um",
    "median",
    "house",
    "value",
    "value",
    "want",
    "uh",
    "uh",
    "target",
    "want",
    "see",
    "features",
    "independent",
    "variables",
    "set",
    "features",
    "statistically",
    "significant",
    "impact",
    "uh",
    "dependent",
    "variable",
    "median",
    "house",
    "value",
    "want",
    "find",
    "features",
    "um",
    "describing",
    "houses",
    "block",
    "cause",
    "change",
    "cause",
    "variation",
    "um",
    "target",
    "variable",
    "medan",
    "house",
    "value",
    "x",
    "equal",
    "uh",
    "data",
    "taking",
    "features",
    "following",
    "names",
    "uh",
    "target",
    "midin",
    "house",
    "uh",
    "house",
    "value",
    "uh",
    "column",
    "going",
    "subtract",
    "select",
    "data",
    "data",
    "filtering",
    "selecting",
    "using",
    "train",
    "test",
    "complete",
    "function",
    "psych",
    "learn",
    "might",
    "recall",
    "beginning",
    "spoke",
    "imported",
    "uh",
    "model",
    "selection",
    "um",
    "library",
    "cyler",
    "model",
    "selection",
    "imported",
    "train",
    "testore",
    "spate",
    "function",
    "function",
    "going",
    "need",
    "quite",
    "lot",
    "machine",
    "learning",
    "easy",
    "way",
    "uh",
    "split",
    "data",
    "um",
    "uh",
    "arguments",
    "function",
    "first",
    "uh",
    "matrix",
    "data",
    "frame",
    "contains",
    "independent",
    "variables",
    "case",
    "x",
    "fill",
    "x",
    "second",
    "uh",
    "argument",
    "dependent",
    "variable",
    "uh",
    "test",
    "size",
    "means",
    "um",
    "uh",
    "proportion",
    "um",
    "observations",
    "want",
    "put",
    "test",
    "proportion",
    "observation",
    "um",
    "want",
    "put",
    "basically",
    "training",
    "putting",
    "means",
    "want",
    "test",
    "size",
    "uh",
    "20",
    "entire",
    "100",
    "data",
    "remaining",
    "80",
    "training",
    "data",
    "provide",
    "point",
    "two",
    "argument",
    "function",
    "automatically",
    "understands",
    "want",
    "80",
    "20",
    "division",
    "80",
    "training",
    "20",
    "test",
    "size",
    "finally",
    "also",
    "uh",
    "add",
    "random",
    "state",
    "split",
    "going",
    "random",
    "data",
    "going",
    "randomly",
    "selected",
    "entire",
    "data",
    "ensure",
    "results",
    "reproducible",
    "uh",
    "next",
    "time",
    "running",
    "um",
    "notebook",
    "get",
    "results",
    "also",
    "ensure",
    "get",
    "results",
    "using",
    "random",
    "state",
    "random",
    "state",
    "111",
    "um",
    "random",
    "number",
    "liked",
    "decided",
    "use",
    "uh",
    "go",
    "um",
    "use",
    "run",
    "command",
    "see",
    "training",
    "set",
    "size",
    "15k",
    "test",
    "size",
    "uh",
    "38k",
    "look",
    "numbers",
    "get",
    "verification",
    "dealing",
    "20",
    "versus",
    "80",
    "thresholds",
    "go",
    "training",
    "one",
    "thing",
    "keep",
    "mind",
    "using",
    "sm",
    "library",
    "uh",
    "nsm",
    "function",
    "imported",
    "uh",
    "stats",
    "model",
    "api",
    "one",
    "one",
    "uh",
    "function",
    "use",
    "order",
    "conduct",
    "uh",
    "cal",
    "analysis",
    "train",
    "le",
    "regression",
    "model",
    "uh",
    "need",
    "uh",
    "using",
    "library",
    "uh",
    "library",
    "automatically",
    "add",
    "uh",
    "first",
    "uh",
    "column",
    "ones",
    "uh",
    "uh",
    "set",
    "independent",
    "variables",
    "means",
    "goes",
    "looks",
    "features",
    "provided",
    "independent",
    "variables",
    "learned",
    "theory",
    "uh",
    "comes",
    "linear",
    "regression",
    "always",
    "adding",
    "intercept",
    "beta0",
    "go",
    "back",
    "theory",
    "lectures",
    "see",
    "beta0",
    "added",
    "simple",
    "linear",
    "regression",
    "multipar",
    "regression",
    "ensures",
    "look",
    "intercept",
    "see",
    "average",
    "uh",
    "case",
    "median",
    "house",
    "value",
    "features",
    "um",
    "equal",
    "zero",
    "um",
    "therefore",
    "given",
    "specific",
    "stats",
    "models",
    "api",
    "adding",
    "uh",
    "constant",
    "um",
    "column",
    "beginning",
    "intercept",
    "means",
    "need",
    "add",
    "manually",
    "therefore",
    "saying",
    "sm",
    "addore",
    "constant",
    "exrain",
    "means",
    "u",
    "uh",
    "x",
    "uh",
    "table",
    "x",
    "data",
    "frame",
    "uh",
    "add",
    "column",
    "ones",
    "uh",
    "features",
    "let",
    "actually",
    "show",
    "uh",
    "uh",
    "training",
    "think",
    "also",
    "something",
    "aware",
    "pause",
    "going",
    "xcore",
    "train",
    "underscore",
    "uh",
    "constant",
    "also",
    "going",
    "print",
    "um",
    "um",
    "feature",
    "data",
    "frame",
    "adding",
    "constant",
    "see",
    "mean",
    "see",
    "set",
    "columns",
    "form",
    "independent",
    "variables",
    "features",
    "add",
    "constant",
    "see",
    "initial",
    "column",
    "ones",
    "th",
    "uh",
    "uh",
    "beta",
    "z",
    "end",
    "intercept",
    "perform",
    "valid",
    "multiple",
    "linear",
    "regression",
    "otherwise",
    "intercept",
    "looking",
    "psychic",
    "learn",
    "library",
    "automatically",
    "therefore",
    "using",
    "uh",
    "tou",
    "models",
    "api",
    "add",
    "constant",
    "use",
    "pyit",
    "learn",
    "without",
    "adding",
    "constant",
    "wondering",
    "use",
    "specific",
    "model",
    "uh",
    "already",
    "discussed",
    "refresh",
    "memory",
    "using",
    "models",
    "api",
    "one",
    "nice",
    "property",
    "visualizing",
    "summary",
    "result",
    "results",
    "p",
    "values",
    "test",
    "standard",
    "errors",
    "something",
    "definitely",
    "looking",
    "performing",
    "proper",
    "causal",
    "analysis",
    "want",
    "identify",
    "features",
    "statistically",
    "significant",
    "impact",
    "dependent",
    "variable",
    "using",
    "machine",
    "learning",
    "model",
    "including",
    "linear",
    "regression",
    "predictive",
    "analytics",
    "case",
    "use",
    "psychic",
    "learn",
    "without",
    "worrying",
    "using",
    "sts",
    "models",
    "api",
    "adding",
    "constant",
    "uh",
    "ready",
    "actually",
    "uh",
    "fit",
    "model",
    "train",
    "model",
    "therefore",
    "need",
    "use",
    "sm",
    "ols",
    "os",
    "ordinar",
    "squares",
    "estimation",
    "technique",
    "also",
    "discussed",
    "part",
    "theory",
    "need",
    "provide",
    "first",
    "dependent",
    "variable",
    "yore",
    "train",
    "um",
    "feature",
    "set",
    "xcore",
    "train",
    "uncore",
    "constant",
    "need",
    "feed",
    "par",
    "paresis",
    "means",
    "take",
    "os",
    "model",
    "use",
    "yore",
    "train",
    "dependent",
    "variable",
    "xcore",
    "trainor",
    "constant",
    "independent",
    "variable",
    "set",
    "fit",
    "ols",
    "algorithm",
    "linear",
    "regression",
    "specific",
    "data",
    "wondering",
    "train",
    "x",
    "train",
    "differ",
    "train",
    "test",
    "sure",
    "go",
    "revisit",
    "training",
    "um",
    "theory",
    "lectures",
    "go",
    "detail",
    "concept",
    "training",
    "testing",
    "divide",
    "data",
    "train",
    "test",
    "uh",
    "x",
    "already",
    "discussed",
    "tutorial",
    "simply",
    "distinction",
    "independent",
    "variables",
    "defined",
    "x",
    "dependent",
    "variable",
    "defined",
    "train",
    "test",
    "dependent",
    "variable",
    "data",
    "training",
    "data",
    "test",
    "data",
    "ext",
    "train",
    "ex",
    "uh",
    "test",
    "simply",
    "training",
    "data",
    "features",
    "ex",
    "train",
    "test",
    "data",
    "features",
    "x",
    "test",
    "need",
    "use",
    "x",
    "train",
    "train",
    "fit",
    "data",
    "learn",
    "data",
    "comes",
    "evaluating",
    "model",
    "need",
    "uh",
    "use",
    "fitted",
    "model",
    "learned",
    "using",
    "dependent",
    "variable",
    "independent",
    "variable",
    "set",
    "train",
    "x",
    "train",
    "uh",
    "model",
    "uh",
    "fitted",
    "apply",
    "unseen",
    "data",
    "exore",
    "test",
    "obtain",
    "predictions",
    "compare",
    "true",
    "yore",
    "test",
    "see",
    "different",
    "uh",
    "underscore",
    "test",
    "predictions",
    "unseen",
    "data",
    "evaluate",
    "moral",
    "uh",
    "performing",
    "prediction",
    "moral",
    "uh",
    "managing",
    "identify",
    "median",
    "uh",
    "house",
    "values",
    "predict",
    "median",
    "house",
    "uh",
    "values",
    "based",
    "uh",
    "um",
    "fitted",
    "model",
    "unseen",
    "data",
    "exore",
    "test",
    "background",
    "info",
    "refreshment",
    "um",
    "case",
    "uh",
    "fitting",
    "data",
    "training",
    "uh",
    "dependent",
    "variable",
    "training",
    "uh",
    "independent",
    "variable",
    "edit",
    "constant",
    "ready",
    "print",
    "summary",
    "let",
    "interpret",
    "results",
    "first",
    "thing",
    "see",
    "uh",
    "coefficients",
    "independent",
    "variables",
    "statistically",
    "significant",
    "say",
    "well",
    "um",
    "look",
    "see",
    "column",
    "p",
    "values",
    "first",
    "thing",
    "need",
    "look",
    "getting",
    "results",
    "caal",
    "analysis",
    "linear",
    "oppression",
    "seeing",
    "p",
    "value",
    "small",
    "refresh",
    "memory",
    "p",
    "value",
    "says",
    "probability",
    "obtained",
    "high",
    "test",
    "statistics",
    "uh",
    "given",
    "random",
    "chance",
    "seeing",
    "statistically",
    "significant",
    "results",
    "random",
    "chance",
    "uh",
    "n",
    "hypothesis",
    "false",
    "need",
    "reject",
    "one",
    "thing",
    "see",
    "see",
    "getting",
    "much",
    "first",
    "thing",
    "verify",
    "used",
    "correct",
    "dependent",
    "variable",
    "see",
    "dependent",
    "variable",
    "median",
    "house",
    "value",
    "model",
    "used",
    "estimate",
    "coefficients",
    "model",
    "os",
    "method",
    "le",
    "squares",
    "le",
    "squares",
    "simply",
    "uh",
    "uh",
    "technique",
    "underlying",
    "approach",
    "minimizing",
    "sum",
    "uh",
    "uh",
    "squared",
    "residuals",
    "least",
    "squares",
    "date",
    "running",
    "analysis",
    "26th",
    "january",
    "2024",
    "uh",
    "number",
    "observations",
    "number",
    "training",
    "observations",
    "80",
    "original",
    "data",
    "r",
    "squ",
    "um",
    "matrix",
    "showcases",
    "um",
    "goodness",
    "fat",
    "model",
    "r",
    "matrix",
    "commonly",
    "used",
    "linear",
    "regression",
    "specifically",
    "identify",
    "good",
    "model",
    "able",
    "fit",
    "data",
    "linear",
    "regression",
    "line",
    "r",
    "squ",
    "uh",
    "maximum",
    "r",
    "squ",
    "one",
    "minimum",
    "zero",
    "uh",
    "case",
    "approximately",
    "59",
    "means",
    "uh",
    "data",
    "got",
    "independent",
    "variables",
    "independent",
    "variables",
    "included",
    "able",
    "explain",
    "59",
    "entire",
    "set",
    "variation",
    "59",
    "variation",
    "response",
    "variable",
    "median",
    "house",
    "value",
    "able",
    "explain",
    "set",
    "independent",
    "variables",
    "provided",
    "model",
    "mean",
    "one",
    "hand",
    "means",
    "reasonable",
    "enough",
    "information",
    "anything",
    "quite",
    "good",
    "means",
    "half",
    "uh",
    "entire",
    "variation",
    "median",
    "house",
    "value",
    "able",
    "explain",
    "hand",
    "means",
    "also",
    "approximately",
    "40",
    "variation",
    "information",
    "house",
    "values",
    "data",
    "means",
    "might",
    "consider",
    "going",
    "looking",
    "extra",
    "additional",
    "information",
    "additional",
    "independent",
    "variables",
    "add",
    "top",
    "existing",
    "independent",
    "variables",
    "order",
    "increase",
    "amount",
    "increase",
    "amount",
    "information",
    "variation",
    "able",
    "explain",
    "model",
    "r",
    "squ",
    "like",
    "best",
    "way",
    "uh",
    "explain",
    "quality",
    "regression",
    "model",
    "another",
    "thing",
    "adjusted",
    "r",
    "squ",
    "adjusted",
    "r",
    "squ",
    "r",
    "squ",
    "specific",
    "case",
    "see",
    "0",
    "um",
    "59",
    "usually",
    "means",
    "uh",
    "fine",
    "comes",
    "amount",
    "features",
    "using",
    "overwhelm",
    "model",
    "many",
    "features",
    "notice",
    "adjusted",
    "r",
    "squ",
    "different",
    "r",
    "squ",
    "adjusted",
    "r",
    "squ",
    "helps",
    "understand",
    "whether",
    "motel",
    "performing",
    "well",
    "adding",
    "many",
    "variables",
    "really",
    "contain",
    "useful",
    "information",
    "cu",
    "sometimes",
    "r",
    "squ",
    "automatically",
    "increase",
    "adding",
    "many",
    "independent",
    "variables",
    "cases",
    "independ",
    "variables",
    "useful",
    "adding",
    "complexity",
    "model",
    "possibly",
    "overfitting",
    "model",
    "providing",
    "edit",
    "information",
    "f",
    "statistics",
    "corresponds",
    "f",
    "test",
    "uh",
    "f",
    "test",
    "um",
    "comes",
    "statistics",
    "uh",
    "need",
    "know",
    "would",
    "say",
    "uh",
    "check",
    "fundamentals",
    "statistics",
    "course",
    "want",
    "know",
    "means",
    "uh",
    "testing",
    "whether",
    "independent",
    "variables",
    "al",
    "together",
    "whether",
    "helping",
    "explain",
    "uh",
    "dependent",
    "variable",
    "median",
    "house",
    "value",
    "uh",
    "f",
    "statistics",
    "large",
    "p",
    "value",
    "f",
    "statistics",
    "small",
    "means",
    "independent",
    "variables",
    "jointly",
    "statistically",
    "significant",
    "means",
    "together",
    "helped",
    "explain",
    "uh",
    "uh",
    "median",
    "house",
    "value",
    "statistically",
    "significant",
    "impact",
    "median",
    "house",
    "value",
    "means",
    "good",
    "set",
    "independent",
    "variables",
    "log",
    "likelihood",
    "super",
    "relevant",
    "case",
    "aic",
    "bic",
    "stand",
    "aas",
    "information",
    "criteria",
    "bation",
    "information",
    "criteria",
    "also",
    "necessary",
    "know",
    "advance",
    "career",
    "machine",
    "learning",
    "might",
    "useful",
    "know",
    "higher",
    "level",
    "think",
    "like",
    "um",
    "value",
    "helps",
    "understand",
    "uh",
    "information",
    "gain",
    "adding",
    "set",
    "independent",
    "variables",
    "model",
    "optional",
    "ignore",
    "know",
    "okay",
    "let",
    "go",
    "fun",
    "part",
    "mata",
    "uh",
    "part",
    "summary",
    "uh",
    "table",
    "got",
    "first",
    "set",
    "uh",
    "independent",
    "variables",
    "constant",
    "intercept",
    "longitude",
    "latitude",
    "housing",
    "median",
    "age",
    "total",
    "roles",
    "population",
    "households",
    "median",
    "income",
    "four",
    "dami",
    "variables",
    "created",
    "coefficients",
    "corresponding",
    "independent",
    "variables",
    "basically",
    "beta0",
    "beta",
    "1",
    "head",
    "beta",
    "2",
    "head",
    "etc",
    "um",
    "parameters",
    "linear",
    "regression",
    "model",
    "oils",
    "method",
    "estimated",
    "based",
    "data",
    "provided",
    "interpreting",
    "independent",
    "variables",
    "first",
    "thing",
    "need",
    "mentioned",
    "beginning",
    "look",
    "p",
    "value",
    "column",
    "showcases",
    "set",
    "independent",
    "variables",
    "statistically",
    "significant",
    "usually",
    "table",
    "get",
    "sato",
    "api",
    "5",
    "significance",
    "level",
    "alpha",
    "threshold",
    "statistical",
    "significance",
    "equal",
    "5",
    "p",
    "value",
    "smaller",
    "means",
    "dealing",
    "statistically",
    "significant",
    "independent",
    "variable",
    "next",
    "thing",
    "see",
    "left",
    "statistics",
    "p",
    "value",
    "based",
    "test",
    "test",
    "simply",
    "stating",
    "learned",
    "theory",
    "also",
    "check",
    "fundamental",
    "statistics",
    "course",
    "lunar",
    "tech",
    "detailed",
    "understanding",
    "test",
    "test",
    "um",
    "states",
    "hypothesis",
    "whether",
    "um",
    "independent",
    "variables",
    "individually",
    "statistically",
    "significant",
    "impact",
    "dependent",
    "variable",
    "whenever",
    "uh",
    "test",
    "p",
    "value",
    "smaller",
    "means",
    "dealing",
    "statistically",
    "significant",
    "uh",
    "independent",
    "variable",
    "case",
    "super",
    "lucky",
    "independent",
    "variables",
    "statistically",
    "significant",
    "question",
    "whether",
    "positive",
    "statistical",
    "significant",
    "negative",
    "something",
    "see",
    "signs",
    "numbers",
    "see",
    "longitude",
    "negative",
    "coefficient",
    "latitude",
    "negative",
    "coefficient",
    "housing",
    "median",
    "age",
    "positive",
    "coefficient",
    "etc",
    "negative",
    "coefficient",
    "means",
    "independent",
    "variable",
    "causes",
    "negative",
    "change",
    "dependent",
    "variable",
    "specifically",
    "look",
    "instance",
    "um",
    "let",
    "say",
    "one",
    "look",
    "uh",
    "let",
    "say",
    "uh",
    "total",
    "uh",
    "underscore",
    "rooms",
    "look",
    "total",
    "underscore",
    "rooms",
    "minus",
    "means",
    "look",
    "total",
    "number",
    "rooms",
    "increase",
    "number",
    "rooms",
    "uh",
    "uh",
    "one",
    "additional",
    "unit",
    "one",
    "room",
    "added",
    "total",
    "underscore",
    "rooms",
    "uh",
    "house",
    "value",
    "uh",
    "decreases",
    "minus",
    "might",
    "wondering",
    "possible",
    "well",
    "first",
    "value",
    "coefficient",
    "quite",
    "small",
    "one",
    "hand",
    "super",
    "relevant",
    "see",
    "uh",
    "relationship",
    "super",
    "strong",
    "u",
    "margin",
    "um",
    "coefficient",
    "quite",
    "small",
    "hand",
    "explain",
    "point",
    "adding",
    "rooms",
    "add",
    "value",
    "fact",
    "cases",
    "decreases",
    "value",
    "house",
    "might",
    "case",
    "least",
    "case",
    "based",
    "data",
    "see",
    "negative",
    "coefficient",
    "one",
    "unit",
    "increase",
    "specific",
    "independent",
    "variables",
    "else",
    "constant",
    "result",
    "um",
    "uh",
    "case",
    "instance",
    "case",
    "total",
    "rooms",
    "uh",
    "decrease",
    "median",
    "house",
    "value",
    "everything",
    "else",
    "constant",
    "also",
    "referring",
    "ass",
    "set",
    "parus",
    "econometric",
    "means",
    "everything",
    "else",
    "constant",
    "one",
    "time",
    "let",
    "refresh",
    "memory",
    "ensure",
    "clear",
    "add",
    "one",
    "room",
    "total",
    "number",
    "rooms",
    "median",
    "house",
    "value",
    "decrease",
    "267",
    "longitude",
    "latitude",
    "house",
    "median",
    "age",
    "population",
    "households",
    "median",
    "income",
    "criterias",
    "uh",
    "instance",
    "negative",
    "value",
    "means",
    "getting",
    "decrease",
    "median",
    "house",
    "value",
    "increase",
    "one",
    "unit",
    "uh",
    "total",
    "number",
    "roles",
    "let",
    "look",
    "op",
    "opposite",
    "uh",
    "case",
    "coefficient",
    "actually",
    "positive",
    "large",
    "hous",
    "median",
    "age",
    "means",
    "two",
    "houses",
    "uh",
    "exactly",
    "characteristics",
    "longitude",
    "latitude",
    "total",
    "number",
    "rooms",
    "population",
    "housing",
    "households",
    "median",
    "income",
    "uh",
    "terms",
    "distance",
    "ocean",
    "um",
    "one",
    "houses",
    "one",
    "additional",
    "year",
    "added",
    "uh",
    "median",
    "age",
    "housing",
    "median",
    "age",
    "one",
    "year",
    "older",
    "house",
    "value",
    "specific",
    "house",
    "higher",
    "846",
    "house",
    "one",
    "additional",
    "median",
    "age",
    "846",
    "higher",
    "median",
    "house",
    "value",
    "compared",
    "one",
    "characteris",
    "ics",
    "except",
    "um",
    "uh",
    "house",
    "median",
    "age",
    "uh",
    "one",
    "year",
    "less",
    "one",
    "additional",
    "uh",
    "year",
    "median",
    "age",
    "result",
    "846",
    "uh",
    "increase",
    "mediate",
    "house",
    "value",
    "everything",
    "else",
    "constant",
    "regarding",
    "idea",
    "negative",
    "imp",
    "positive",
    "margin",
    "coefficient",
    "let",
    "look",
    "one",
    "dami",
    "variable",
    "um",
    "explain",
    "idea",
    "behind",
    "interpret",
    "uh",
    "good",
    "way",
    "understand",
    "dond",
    "variables",
    "interpreted",
    "context",
    "linear",
    "regression",
    "one",
    "independent",
    "variables",
    "ocean",
    "proximity",
    "inland",
    "coefficient",
    "equal",
    "2108",
    "e",
    "plus",
    "simply",
    "means",
    "210",
    "k",
    "uh",
    "approximately",
    "um",
    "means",
    "two",
    "houses",
    "exactly",
    "characteristics",
    "longitude",
    "latitude",
    "house",
    "median",
    "age",
    "total",
    "number",
    "rooms",
    "population",
    "households",
    "median",
    "income",
    "characteristics",
    "two",
    "blocks",
    "houses",
    "single",
    "difference",
    "one",
    "block",
    "located",
    "um",
    "inland",
    "comes",
    "ocean",
    "proximity",
    "block",
    "houses",
    "located",
    "inland",
    "case",
    "reference",
    "um",
    "category",
    "removed",
    "iceland",
    "might",
    "recall",
    "uh",
    "block",
    "houses",
    "inland",
    "value",
    "average",
    "uh",
    "smaller",
    "less",
    "210k",
    "comes",
    "median",
    "house",
    "value",
    "compared",
    "block",
    "houses",
    "exactly",
    "characteristics",
    "inland",
    "instance",
    "uh",
    "iceland",
    "uh",
    "comes",
    "dumi",
    "variables",
    "also",
    "underlying",
    "reference",
    "variable",
    "deleted",
    "part",
    "string",
    "categorical",
    "variable",
    "need",
    "reference",
    "dami",
    "variable",
    "specific",
    "category",
    "might",
    "sound",
    "complex",
    "actually",
    "would",
    "say",
    "uh",
    "matter",
    "practicing",
    "trying",
    "understand",
    "approach",
    "variable",
    "means",
    "either",
    "criteria",
    "specific",
    "case",
    "means",
    "two",
    "blocks",
    "houses",
    "exactly",
    "characteristics",
    "one",
    "block",
    "houses",
    "inland",
    "one",
    "inland",
    "instance",
    "iceland",
    "block",
    "houses",
    "inland",
    "average",
    "less",
    "uh",
    "median",
    "house",
    "value",
    "compared",
    "block",
    "houses",
    "ind",
    "instance",
    "iceland",
    "uh",
    "comes",
    "ocean",
    "proximity",
    "kind",
    "uh",
    "makes",
    "sense",
    "california",
    "people",
    "might",
    "prefer",
    "living",
    "uh",
    "isoland",
    "location",
    "houses",
    "might",
    "demand",
    "comes",
    "iceland",
    "location",
    "compared",
    "um",
    "inland",
    "locations",
    "longitude",
    "uh",
    "statistically",
    "significant",
    "impact",
    "uh",
    "median",
    "house",
    "value",
    "latitude",
    "house",
    "median",
    "age",
    "impact",
    "causes",
    "statistically",
    "significant",
    "difference",
    "medan",
    "house",
    "value",
    "change",
    "median",
    "age",
    "total",
    "number",
    "rooms",
    "impact",
    "median",
    "house",
    "volume",
    "population",
    "impact",
    "households",
    "median",
    "income",
    "well",
    "uh",
    "proximity",
    "ocean",
    "p",
    "values",
    "uh",
    "zero",
    "means",
    "smaller",
    "means",
    "statistically",
    "significant",
    "impact",
    "median",
    "house",
    "value",
    "californian",
    "house",
    "market",
    "comes",
    "uh",
    "interpretation",
    "uh",
    "interpreted",
    "uh",
    "sake",
    "simplicity",
    "ensuring",
    "uh",
    "entire",
    "case",
    "study",
    "take",
    "long",
    "would",
    "suggest",
    "uh",
    "interpret",
    "uh",
    "coefficients",
    "interpreted",
    "housing",
    "median",
    "age",
    "um",
    "total",
    "number",
    "rooms",
    "also",
    "interpret",
    "population",
    "uh",
    "well",
    "median",
    "income",
    "uh",
    "also",
    "interpreted",
    "one",
    "dy",
    "variables",
    "feel",
    "free",
    "also",
    "interpret",
    "ones",
    "also",
    "uh",
    "even",
    "build",
    "entire",
    "case",
    "study",
    "paper",
    "explain",
    "one",
    "two",
    "pages",
    "results",
    "obtained",
    "showcase",
    "understanding",
    "interpret",
    "linear",
    "gressional",
    "results",
    "another",
    "thing",
    "would",
    "suggest",
    "uh",
    "add",
    "comment",
    "standard",
    "error",
    "let",
    "look",
    "standard",
    "errors",
    "see",
    "huge",
    "standard",
    "error",
    "um",
    "making",
    "direct",
    "result",
    "fourth",
    "assumption",
    "violated",
    "case",
    "study",
    "super",
    "important",
    "useful",
    "way",
    "showcases",
    "happens",
    "um",
    "assumptions",
    "satisfied",
    "assumptions",
    "violated",
    "specific",
    "specific",
    "case",
    "assumption",
    "related",
    "uh",
    "uh",
    "errors",
    "constant",
    "variance",
    "violated",
    "heos",
    "sk",
    "assist",
    "issue",
    "something",
    "seeing",
    "back",
    "results",
    "good",
    "example",
    "case",
    "even",
    "without",
    "checking",
    "assumptions",
    "already",
    "see",
    "standard",
    "error",
    "large",
    "uh",
    "see",
    "given",
    "standard",
    "er",
    "large",
    "already",
    "gives",
    "hint",
    "likely",
    "heteroscedasticity",
    "uh",
    "present",
    "homoscedasticity",
    "assumption",
    "violated",
    "keep",
    "mind",
    "um",
    "idea",
    "um",
    "large",
    "standard",
    "errors",
    "saw",
    "going",
    "see",
    "becomes",
    "problem",
    "also",
    "um",
    "performance",
    "model",
    "see",
    "obtaining",
    "large",
    "error",
    "due",
    "uh",
    "one",
    "comment",
    "comes",
    "total",
    "rooms",
    "housing",
    "median",
    "age",
    "cases",
    "linear",
    "regression",
    "results",
    "might",
    "seem",
    "logical",
    "sometimes",
    "actually",
    "underlying",
    "explanation",
    "provided",
    "maybe",
    "model",
    "overfitting",
    "biased",
    "also",
    "possible",
    "uh",
    "something",
    "uh",
    "checking",
    "ois",
    "assumptions",
    "uh",
    "uh",
    "going",
    "stage",
    "wanted",
    "briefly",
    "showcase",
    "um",
    "idea",
    "predictions",
    "fitted",
    "model",
    "uh",
    "uh",
    "training",
    "data",
    "ready",
    "perform",
    "predictions",
    "use",
    "fitted",
    "model",
    "uh",
    "use",
    "test",
    "data",
    "ex",
    "test",
    "order",
    "perform",
    "predictions",
    "uh",
    "use",
    "data",
    "get",
    "new",
    "house",
    "mediate",
    "house",
    "values",
    "um",
    "blocks",
    "houses",
    "providing",
    "uh",
    "corresponding",
    "medan",
    "house",
    "price",
    "aning",
    "data",
    "uh",
    "um",
    "applying",
    "model",
    "already",
    "fitted",
    "want",
    "see",
    "predicted",
    "median",
    "house",
    "values",
    "compare",
    "predictions",
    "true",
    "median",
    "house",
    "values",
    "yet",
    "exposing",
    "want",
    "see",
    "good",
    "model",
    "job",
    "estimating",
    "finding",
    "unknown",
    "median",
    "house",
    "values",
    "test",
    "data",
    "blocks",
    "houses",
    "provided",
    "characteristics",
    "x",
    "test",
    "providing",
    "test",
    "uh",
    "usual",
    "like",
    "case",
    "training",
    "adding",
    "constant",
    "library",
    "saying",
    "model",
    "fitted",
    "model",
    "uncore",
    "fitted",
    "fitted",
    "model",
    "predict",
    "providing",
    "test",
    "data",
    "test",
    "predictions",
    "uh",
    "get",
    "test",
    "predictions",
    "uh",
    "print",
    "see",
    "getting",
    "least",
    "house",
    "values",
    "house",
    "values",
    "um",
    "um",
    "blocks",
    "houses",
    "included",
    "part",
    "testing",
    "data",
    "20",
    "entire",
    "data",
    "set",
    "uh",
    "like",
    "mentioned",
    "order",
    "ensure",
    "model",
    "performing",
    "well",
    "need",
    "check",
    "os",
    "assumptions",
    "uh",
    "um",
    "theory",
    "section",
    "learned",
    "couple",
    "assumptions",
    "model",
    "satisfy",
    "data",
    "satisfy",
    "ols",
    "provide",
    "uh",
    "b",
    "unbiased",
    "um",
    "efficient",
    "uh",
    "estimates",
    "means",
    "accurate",
    "standard",
    "error",
    "low",
    "something",
    "um",
    "also",
    "seeing",
    "part",
    "summary",
    "results",
    "uh",
    "estimates",
    "accurate",
    "standard",
    "error",
    "measure",
    "showcases",
    "efficient",
    "estimat",
    "means",
    "um",
    "high",
    "variation",
    "uh",
    "coefficients",
    "showing",
    "table",
    "lot",
    "means",
    "accurate",
    "um",
    "coefficient",
    "coefficient",
    "way",
    "one",
    "place",
    "range",
    "l",
    "large",
    "means",
    "standard",
    "error",
    "large",
    "bad",
    "sign",
    "dealing",
    "accurate",
    "estimation",
    "uh",
    "precise",
    "estimation",
    "case",
    "standard",
    "low",
    "uh",
    "unbias",
    "estimate",
    "means",
    "estimates",
    "true",
    "representation",
    "pattern",
    "pair",
    "independent",
    "variable",
    "response",
    "variable",
    "want",
    "learn",
    "ide",
    "bias",
    "unbias",
    "efficiency",
    "sure",
    "check",
    "u",
    "fundamental",
    "statistics",
    "course",
    "lunar",
    "tech",
    "explains",
    "clearly",
    "concepts",
    "detail",
    "assuming",
    "know",
    "maybe",
    "even",
    "need",
    "would",
    "suggest",
    "know",
    "higher",
    "level",
    "least",
    "uh",
    "let",
    "quickly",
    "checking",
    "oiless",
    "assumption",
    "first",
    "assumption",
    "linearity",
    "assumption",
    "means",
    "model",
    "linear",
    "parameters",
    "one",
    "way",
    "checking",
    "using",
    "already",
    "fitted",
    "model",
    "uh",
    "predicted",
    "model",
    "uh",
    "uh",
    "test",
    "true",
    "house",
    "median",
    "house",
    "values",
    "test",
    "data",
    "test",
    "predictions",
    "uh",
    "predicted",
    "median",
    "house",
    "values",
    "nonen",
    "data",
    "using",
    "uh",
    "true",
    "values",
    "predicted",
    "values",
    "order",
    "um",
    "plot",
    "also",
    "plot",
    "best",
    "fitted",
    "line",
    "ideal",
    "situation",
    "would",
    "make",
    "error",
    "model",
    "would",
    "give",
    "exact",
    "true",
    "values",
    "um",
    "see",
    "well",
    "um",
    "uh",
    "linear",
    "relationship",
    "actually",
    "linear",
    "relationship",
    "observed",
    "versus",
    "predicted",
    "values",
    "observed",
    "means",
    "uh",
    "real",
    "uh",
    "test",
    "test",
    "wise",
    "predicted",
    "means",
    "test",
    "predictions",
    "pattern",
    "kind",
    "linear",
    "matching",
    "perfect",
    "linear",
    "line",
    "um",
    "assumption",
    "one",
    "satisfied",
    "linearity",
    "assumption",
    "satisfied",
    "say",
    "uh",
    "data",
    "uh",
    "model",
    "indeed",
    "linear",
    "parameters",
    "uh",
    "second",
    "assumption",
    "states",
    "uh",
    "sample",
    "random",
    "basically",
    "translates",
    "uh",
    "expectation",
    "error",
    "terms",
    "equal",
    "zero",
    "uh",
    "one",
    "way",
    "checking",
    "simply",
    "taking",
    "residuales",
    "fitted",
    "model",
    "model",
    "score",
    "fitted",
    "residual",
    "take",
    "residuales",
    "obtain",
    "average",
    "good",
    "estimate",
    "expectation",
    "errors",
    "mean",
    "residuales",
    "average",
    "uh",
    "residuales",
    "residual",
    "estimate",
    "true",
    "error",
    "terms",
    "uh",
    "round",
    "uh",
    "two",
    "decimals",
    "behind",
    "uh",
    "uh",
    "point",
    "means",
    "uh",
    "getting",
    "uh",
    "average",
    "amount",
    "uh",
    "errors",
    "estimate",
    "errors",
    "referring",
    "residuales",
    "number",
    "equal",
    "zero",
    "case",
    "mean",
    "residuales",
    "model",
    "zero",
    "means",
    "indeed",
    "um",
    "uh",
    "expectation",
    "uh",
    "error",
    "terms",
    "least",
    "estimate",
    "expectation",
    "residuales",
    "inde",
    "equal",
    "zero",
    "another",
    "way",
    "checking",
    "um",
    "uh",
    "second",
    "assumption",
    "um",
    "moral",
    "uh",
    "based",
    "random",
    "sample",
    "sample",
    "using",
    "random",
    "means",
    "expectation",
    "error",
    "terms",
    "equal",
    "zero",
    "plotting",
    "residuales",
    "versus",
    "fitted",
    "values",
    "uh",
    "taking",
    "resid",
    "fitted",
    "model",
    "comparing",
    "fitted",
    "values",
    "comes",
    "model",
    "uh",
    "looking",
    "um",
    "graph",
    "scatter",
    "plot",
    "see",
    "looking",
    "um",
    "pattern",
    "uh",
    "symmetric",
    "uh",
    "around",
    "uh",
    "threshold",
    "zero",
    "see",
    "line",
    "kind",
    "comes",
    "right",
    "middle",
    "pattern",
    "means",
    "average",
    "residuales",
    "across",
    "zero",
    "mean",
    "residuales",
    "equal",
    "zero",
    "exactly",
    "calculating",
    "also",
    "therefore",
    "say",
    "indeed",
    "dealing",
    "random",
    "sample",
    "fl",
    "also",
    "super",
    "useful",
    "comes",
    "fourth",
    "assumption",
    "come",
    "bit",
    "later",
    "let",
    "check",
    "third",
    "assumption",
    "assumption",
    "exogeneity",
    "exogeneity",
    "means",
    "uh",
    "independent",
    "variables",
    "uncorrelated",
    "error",
    "terms",
    "omitted",
    "variable",
    "bias",
    "um",
    "reverse",
    "causality",
    "means",
    "uh",
    "independent",
    "variable",
    "impact",
    "dependent",
    "variable",
    "way",
    "around",
    "dependent",
    "variable",
    "impact",
    "cause",
    "independent",
    "variable",
    "ways",
    "deal",
    "uh",
    "uh",
    "one",
    "way",
    "straightforward",
    "compute",
    "uh",
    "correlation",
    "coefficient",
    "independent",
    "variables",
    "residuales",
    "obtained",
    "fitted",
    "model",
    "simple",
    "uh",
    "technique",
    "use",
    "uh",
    "quick",
    "way",
    "understand",
    "uh",
    "correlation",
    "pair",
    "independent",
    "variable",
    "residuals",
    "best",
    "estimates",
    "error",
    "terms",
    "way",
    "understand",
    "correlation",
    "independent",
    "variables",
    "error",
    "terms",
    "another",
    "way",
    "advanced",
    "bit",
    "um",
    "towards",
    "econometrical",
    "side",
    "using",
    "test",
    "called",
    "durban",
    "uh",
    "view",
    "housan",
    "test",
    "uh",
    "durban",
    "view",
    "housan",
    "test",
    "um",
    "professional",
    "advanced",
    "way",
    "uh",
    "using",
    "econometrical",
    "test",
    "find",
    "whether",
    "um",
    "exogene",
    "exogeneity",
    "sup",
    "satisfied",
    "endogenity",
    "means",
    "one",
    "multiple",
    "independent",
    "variables",
    "potentially",
    "correlated",
    "error",
    "terms",
    "uh",
    "wo",
    "go",
    "detail",
    "test",
    "uh",
    "put",
    "explanation",
    "also",
    "feel",
    "free",
    "uh",
    "check",
    "uh",
    "introductory",
    "econometrics",
    "course",
    "understand",
    "duran",
    "vu",
    "housan",
    "test",
    "exogeneity",
    "assumption",
    "fourth",
    "assumption",
    "talk",
    "homos",
    "skasis",
    "homosa",
    "assumption",
    "states",
    "error",
    "terms",
    "variance",
    "constant",
    "means",
    "looking",
    "variation",
    "uh",
    "model",
    "making",
    "uh",
    "across",
    "uh",
    "different",
    "observations",
    "uh",
    "look",
    "variation",
    "kind",
    "constant",
    "uh",
    "uh",
    "cases",
    "uh",
    "observations",
    "residuals",
    "bit",
    "small",
    "cases",
    "bit",
    "large",
    "miror",
    "comes",
    "figure",
    "calling",
    "heteros",
    "skos",
    "means",
    "means",
    "homos",
    "assumption",
    "violated",
    "error",
    "terms",
    "variation",
    "constant",
    "across",
    "observations",
    "high",
    "variation",
    "different",
    "variations",
    "different",
    "observations",
    "heteros",
    "issue",
    "consider",
    "bit",
    "um",
    "flexible",
    "approaches",
    "like",
    "uh",
    "gls",
    "fgs",
    "gmm",
    "bit",
    "advanced",
    "econometrical",
    "algorithms",
    "uh",
    "final",
    "part",
    "case",
    "study",
    "show",
    "uh",
    "machine",
    "learning",
    "traditional",
    "machine",
    "learning",
    "site",
    "using",
    "psychic",
    "learn",
    "uh",
    "um",
    "using",
    "um",
    "standard",
    "scaler",
    "function",
    "order",
    "uh",
    "scale",
    "data",
    "saw",
    "uh",
    "summary",
    "table",
    "um",
    "got",
    "stats",
    "uh",
    "mos",
    "api",
    "data",
    "high",
    "scale",
    "uh",
    "median",
    "house",
    "values",
    "large",
    "numbers",
    "uh",
    "age",
    "uh",
    "median",
    "age",
    "house",
    "large",
    "numbers",
    "something",
    "want",
    "avoid",
    "using",
    "linear",
    "regression",
    "predictive",
    "analytics",
    "model",
    "using",
    "interpreting",
    "purposes",
    "keep",
    "skilles",
    "easier",
    "interpret",
    "values",
    "understand",
    "uh",
    "difference",
    "median",
    "price",
    "uh",
    "house",
    "compare",
    "different",
    "characteristics",
    "box",
    "houses",
    "comes",
    "using",
    "predictive",
    "analytics",
    "purposes",
    "means",
    "really",
    "care",
    "accuracy",
    "predictions",
    "need",
    "uh",
    "scale",
    "data",
    "ensure",
    "data",
    "standardized",
    "one",
    "way",
    "using",
    "standard",
    "scaler",
    "function",
    "uh",
    "pyit",
    "learn",
    "preprocessing",
    "uh",
    "uh",
    "way",
    "initialize",
    "scaler",
    "using",
    "standard",
    "scaler",
    "parenthesis",
    "import",
    "psychic",
    "learn",
    "library",
    "uh",
    "uh",
    "taking",
    "scaler",
    "fitore",
    "transform",
    "exrain",
    "basically",
    "means",
    "take",
    "independent",
    "variables",
    "ensure",
    "scale",
    "standardize",
    "data",
    "standardization",
    "simply",
    "means",
    "uh",
    "standardizing",
    "data",
    "ensure",
    "um",
    "large",
    "values",
    "wrongly",
    "influence",
    "predictive",
    "power",
    "model",
    "model",
    "confused",
    "large",
    "numbers",
    "finds",
    "wrong",
    "variation",
    "instead",
    "focuses",
    "true",
    "variation",
    "data",
    "based",
    "much",
    "change",
    "one",
    "independent",
    "variable",
    "causes",
    "change",
    "dependent",
    "variable",
    "given",
    "dealing",
    "supervised",
    "learning",
    "algorithm",
    "uh",
    "exrain",
    "uh",
    "scaled",
    "containing",
    "standardized",
    "uh",
    "features",
    "independent",
    "variables",
    "test",
    "sc",
    "contain",
    "standardized",
    "test",
    "features",
    "unseen",
    "data",
    "model",
    "see",
    "training",
    "prediction",
    "also",
    "use",
    "um",
    "train",
    "train",
    "uh",
    "dependent",
    "variable",
    "supervised",
    "model",
    "train",
    "corresponds",
    "training",
    "data",
    "first",
    "initialize",
    "linear",
    "regression",
    "linear",
    "regression",
    "model",
    "pyit",
    "learn",
    "uh",
    "initialize",
    "model",
    "empty",
    "linear",
    "regression",
    "model",
    "take",
    "initialized",
    "uh",
    "model",
    "fit",
    "uh",
    "training",
    "data",
    "exore",
    "trained",
    "uncore",
    "scale",
    "trained",
    "features",
    "um",
    "uh",
    "dependent",
    "variable",
    "training",
    "data",
    "train",
    "uh",
    "knowe",
    "scaling",
    "dependent",
    "variable",
    "common",
    "practice",
    "cuz",
    "want",
    "uh",
    "standardize",
    "dependent",
    "variable",
    "rather",
    "want",
    "ensure",
    "features",
    "standardized",
    "care",
    "variation",
    "features",
    "ensure",
    "model",
    "mess",
    "learning",
    "features",
    "less",
    "comes",
    "looking",
    "impact",
    "features",
    "dependent",
    "variable",
    "uh",
    "fitting",
    "uh",
    "model",
    "training",
    "data",
    "uh",
    "features",
    "independent",
    "variable",
    "using",
    "fitted",
    "uh",
    "model",
    "lr",
    "already",
    "learned",
    "features",
    "dependent",
    "variable",
    "supervised",
    "training",
    "using",
    "x",
    "test",
    "scale",
    "test",
    "standardized",
    "uh",
    "data",
    "order",
    "uh",
    "perform",
    "prediction",
    "predict",
    "immediate",
    "house",
    "values",
    "test",
    "data",
    "unseen",
    "data",
    "notice",
    "places",
    "using",
    "white",
    "test",
    "white",
    "test",
    "keeping",
    "dependent",
    "variable",
    "true",
    "values",
    "compare",
    "predicted",
    "values",
    "see",
    "well",
    "motor",
    "able",
    "actually",
    "get",
    "predictions",
    "uh",
    "let",
    "actually",
    "also",
    "one",
    "step",
    "importing",
    "psyit",
    "learn",
    "matrix",
    "mean",
    "squared",
    "error",
    "uh",
    "using",
    "mean",
    "squared",
    "error",
    "find",
    "well",
    "motel",
    "able",
    "predict",
    "house",
    "prices",
    "means",
    "uh",
    "average",
    "making",
    "error",
    "dollars",
    "comes",
    "median",
    "house",
    "prices",
    "uh",
    "dependent",
    "consider",
    "large",
    "small",
    "something",
    "look",
    "um",
    "like",
    "mentioned",
    "beginning",
    "uh",
    "idea",
    "behind",
    "linear",
    "regression",
    "using",
    "ind",
    "specific",
    "uh",
    "course",
    "uh",
    "use",
    "terms",
    "pure",
    "traditional",
    "machine",
    "learning",
    "rather",
    "perform",
    "um",
    "causal",
    "analysis",
    "see",
    "interpret",
    "comes",
    "quality",
    "predictive",
    "power",
    "model",
    "uh",
    "want",
    "improve",
    "model",
    "considered",
    "next",
    "step",
    "understand",
    "whether",
    "model",
    "overfitting",
    "next",
    "step",
    "could",
    "apply",
    "instance",
    "um",
    "lasso",
    "regularization",
    "lasso",
    "regression",
    "addresses",
    "overfitting",
    "also",
    "consider",
    "going",
    "back",
    "removing",
    "outliers",
    "data",
    "maybe",
    "outliers",
    "removed",
    "enough",
    "also",
    "apply",
    "factor",
    "another",
    "thing",
    "consider",
    "bit",
    "advanced",
    "machine",
    "learning",
    "algorithms",
    "um",
    "although",
    "um",
    "regression",
    "assumption",
    "satisfied",
    "still",
    "um",
    "using",
    "bit",
    "flexible",
    "motors",
    "like",
    "random",
    "forest",
    "decision",
    "trees",
    "boosting",
    "techniques",
    "bit",
    "appropriate",
    "give",
    "higher",
    "predictive",
    "power",
    "consider",
    "also",
    "uh",
    "uh",
    "working",
    "uh",
    "scaled",
    "uh",
    "version",
    "normalization",
    "data",
    "next",
    "step",
    "machine",
    "learning",
    "journey",
    "consider",
    "learning",
    "bit",
    "advanced",
    "machine",
    "learning",
    "models",
    "know",
    "detail",
    "linear",
    "regression",
    "use",
    "train",
    "test",
    "machine",
    "learning",
    "model",
    "simple",
    "one",
    "yet",
    "popular",
    "one",
    "also",
    "know",
    "logistic",
    "progression",
    "basics",
    "ready",
    "go",
    "next",
    "step",
    "learning",
    "popular",
    "traditional",
    "machine",
    "learning",
    "models",
    "think",
    "learning",
    "decision",
    "trees",
    "modeling",
    "nonlinear",
    "relationships",
    "think",
    "learning",
    "bagging",
    "boosting",
    "random",
    "forest",
    "different",
    "sours",
    "optimization",
    "algorithms",
    "like",
    "gradi",
    "descent",
    "hgd",
    "hgd",
    "momentum",
    "adam",
    "adam",
    "v",
    "rms",
    "prop",
    "difference",
    "implement",
    "also",
    "consider",
    "learning",
    "clustering",
    "approaches",
    "like",
    "k",
    "means",
    "uh",
    "db",
    "skin",
    "hierarchial",
    "clust",
    "string",
    "help",
    "uh",
    "get",
    "hands",
    "go",
    "next",
    "step",
    "comes",
    "machine",
    "learning",
    "covered",
    "fundamentals",
    "ready",
    "go",
    "one",
    "step",
    "getting",
    "deep",
    "le",
    "thank",
    "watching",
    "video",
    "like",
    "content",
    "make",
    "sure",
    "check",
    "videos",
    "available",
    "channel",
    "forget",
    "subscribe",
    "like",
    "comment",
    "help",
    "algorithm",
    "make",
    "content",
    "accessible",
    "everyone",
    "across",
    "world",
    "want",
    "get",
    "free",
    "resources",
    "make",
    "sure",
    "check",
    "free",
    "resources",
    "section",
    "lunch",
    "want",
    "become",
    "job",
    "ready",
    "data",
    "scientist",
    "looking",
    "accessible",
    "boot",
    "camp",
    "help",
    "make",
    "job",
    "ready",
    "data",
    "scientist",
    "consider",
    "enrolling",
    "data",
    "science",
    "boot",
    "camp",
    "ultimate",
    "data",
    "science",
    "boot",
    "camp",
    "learn",
    "theory",
    "fundamentals",
    "become",
    "jbre",
    "data",
    "scientist",
    "also",
    "implement",
    "learn",
    "theory",
    "real",
    "world",
    "multiple",
    "data",
    "science",
    "projects",
    "beside",
    "learning",
    "theory",
    "practicing",
    "real",
    "world",
    "case",
    "studies",
    "also",
    "prepare",
    "data",
    "science",
    "interviews",
    "want",
    "stay",
    "date",
    "recent",
    "developments",
    "tech",
    "headlines",
    "missed",
    "last",
    "week",
    "open",
    "positions",
    "currently",
    "market",
    "across",
    "globe",
    "tech",
    "startups",
    "making",
    "waves",
    "tech",
    "sure",
    "subscribe",
    "data",
    "science",
    "nai",
    "newsletter",
    "music",
    "lunarch"
  ],
  "keywords": [
    "machine",
    "learning",
    "course",
    "2024",
    "map",
    "career",
    "theory",
    "applications",
    "end",
    "project",
    "using",
    "python",
    "data",
    "science",
    "concepts",
    "making",
    "resources",
    "path",
    "field",
    "looking",
    "step",
    "yet",
    "going",
    "cover",
    "basics",
    "put",
    "practice",
    "real",
    "case",
    "study",
    "tech",
    "ai",
    "deep",
    "check",
    "free",
    "section",
    "lunch",
    "find",
    "start",
    "discuss",
    "exact",
    "skill",
    "set",
    "need",
    "get",
    "also",
    "definition",
    "common",
    "use",
    "order",
    "actual",
    "learn",
    "different",
    "fundamentals",
    "learned",
    "basic",
    "implement",
    "linear",
    "aggression",
    "model",
    "analysis",
    "predictive",
    "analytics",
    "californian",
    "house",
    "prices",
    "features",
    "values",
    "approach",
    "know",
    "comes",
    "including",
    "psychic",
    "models",
    "us",
    "simple",
    "strong",
    "let",
    "talk",
    "first",
    "skills",
    "topics",
    "become",
    "type",
    "projects",
    "complete",
    "examples",
    "apply",
    "related",
    "want",
    "kind",
    "usually",
    "average",
    "expect",
    "positions",
    "exactly",
    "used",
    "corresponding",
    "sorts",
    "likely",
    "based",
    "helps",
    "uh",
    "build",
    "make",
    "across",
    "identify",
    "um",
    "really",
    "range",
    "instance",
    "help",
    "many",
    "whether",
    "people",
    "getting",
    "side",
    "p",
    "specifically",
    "understand",
    "amount",
    "per",
    "estimate",
    "time",
    "basically",
    "go",
    "price",
    "accurate",
    "way",
    "estimated",
    "certain",
    "like",
    "see",
    "interested",
    "algorithm",
    "specific",
    "target",
    "result",
    "high",
    "ensure",
    "language",
    "highly",
    "sure",
    "large",
    "example",
    "super",
    "actually",
    "topic",
    "decision",
    "mathematics",
    "statistics",
    "nlp",
    "independent",
    "matrix",
    "mean",
    "part",
    "mathematical",
    "algorithms",
    "sum",
    "constant",
    "two",
    "take",
    "idea",
    "second",
    "would",
    "much",
    "next",
    "complexity",
    "important",
    "engineer",
    "n",
    "log",
    "x",
    "variables",
    "logarithm",
    "e",
    "taking",
    "quite",
    "might",
    "definitely",
    "look",
    "descriptive",
    "probability",
    "distribution",
    "median",
    "standard",
    "variance",
    "statistical",
    "numbers",
    "population",
    "sample",
    "testing",
    "test",
    "power",
    "one",
    "error",
    "versus",
    "popular",
    "function",
    "higher",
    "level",
    "concept",
    "advanced",
    "bias",
    "least",
    "fundamental",
    "work",
    "supervised",
    "unsupervised",
    "classification",
    "regression",
    "logistic",
    "random",
    "means",
    "class",
    "string",
    "behind",
    "training",
    "process",
    "perform",
    "parameters",
    "train",
    "validation",
    "techniques",
    "evaluate",
    "metrics",
    "score",
    "precision",
    "recall",
    "squared",
    "r",
    "errors",
    "residual",
    "squares",
    "cases",
    "interviews",
    "questions",
    "say",
    "knowing",
    "thing",
    "job",
    "another",
    "therefore",
    "libraries",
    "engineering",
    "l",
    "bit",
    "visualization",
    "suggest",
    "plot",
    "unique",
    "missing",
    "clean",
    "feature",
    "multiple",
    "new",
    "performance",
    "even",
    "given",
    "lower",
    "something",
    "consider",
    "towards",
    "difference",
    "head",
    "solve",
    "showcase",
    "dependent",
    "variable",
    "response",
    "characteristics",
    "predictions",
    "compare",
    "single",
    "unit",
    "observation",
    "describing",
    "zero",
    "obtain",
    "final",
    "good",
    "better",
    "best",
    "show",
    "finally",
    "research",
    "mind",
    "fit",
    "already",
    "similar",
    "talking",
    "traditional",
    "rather",
    "account",
    "ready",
    "lecture",
    "define",
    "problem",
    "dealing",
    "key",
    "include",
    "hand",
    "predict",
    "categorical",
    "accuracy",
    "often",
    "prediction",
    "value",
    "size",
    "effect",
    "categories",
    "rss",
    "predicted",
    "true",
    "coefficients",
    "beta",
    "coefficient",
    "yi",
    "equal",
    "squ",
    "1",
    "included",
    "minus",
    "terms",
    "adding",
    "formula",
    "top",
    "trying",
    "small",
    "outliers",
    "may",
    "easier",
    "interpret",
    "explain",
    "less",
    "compared",
    "number",
    "plus",
    "positive",
    "total",
    "negative",
    "cluster",
    "age",
    "simply",
    "whenever",
    "well",
    "point",
    "maximum",
    "since",
    "additional",
    "results",
    "later",
    "fitted",
    "third",
    "minimize",
    "possible",
    "rate",
    "always",
    "give",
    "unseen",
    "satisfied",
    "discussed",
    "call",
    "econometrical",
    "relationship",
    "called",
    "able",
    "low",
    "tend",
    "variation",
    "observations",
    "pair",
    "instead",
    "f",
    "term",
    "perfect",
    "unknown",
    "expression",
    "got",
    "keep",
    "correlation",
    "flexibility",
    "impact",
    "overfitting",
    "regularization",
    "saw",
    "l2",
    "technique",
    "group",
    "residuales",
    "estimation",
    "parameter",
    "assumptions",
    "le",
    "ols",
    "intercept",
    "lambda",
    "penalty",
    "never",
    "estimates",
    "statistically",
    "significant",
    "change",
    "saying",
    "0",
    "ui",
    "add",
    "name",
    "line",
    "information",
    "causal",
    "entire",
    "assumption",
    "five",
    "violated",
    "exogeneity",
    "endogenity",
    "correlated",
    "cause",
    "multicolinearity",
    "remove",
    "increase",
    "uncore",
    "likelihood",
    "odds",
    "criteria",
    "figure",
    "housing",
    "blocks",
    "houses",
    "block",
    "rooms",
    "bedrooms",
    "households",
    "income",
    "latitude",
    "longitude",
    "scale",
    "library",
    "api",
    "ocean",
    "proximity",
    "near",
    "inland",
    "iceland",
    "underscore",
    "drop",
    "25th",
    "percentile",
    "quantile",
    "25",
    "upper",
    "bound",
    "box",
    "column",
    "dami"
  ]
}