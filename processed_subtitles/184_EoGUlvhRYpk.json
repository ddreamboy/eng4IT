{
  "text": "what's going on guys hope you're doing\nawesome so in this tutorial we're gonna\ndo the coolest project ever basically\nwhat we're gonna do is we're gonna\ncreate our own Google Translate well\nsort of specifically we're gonna\nimplement a sequence of sequence model\nusing LS themes on a data set with\nGerman and English sentences but before\nthat we do that let's play that\nbeautiful intro so I went through the\nsequence sequence paper in my last video\nbut for those of you who didn't watch\nthat one let me do a quick 30 second\nrecap and let me bring out this figure\nright here so this beautiful figure what\nwe're gonna have is we're gonna have two\nLS DM s so we're gonna have this left\none this left one right here which is\ngonna which is called the encoder and\nit's gonna take the German sentence and\nit's gonna produce this this red box\nright here which we're gonna call the\ncontacts vector or Zed and this is gonna\nbe input to the other LS diem which is\ngoing to be called the decoder and\nessentially this one is gonna take a\nstar token and then it's going to start\npredicting these words these translated\nwords and cell by cell it's gonna\nproduce the output sentence alright that\nwas a bit longer than 30 seconds but\nnever mind let's get started with the\ncode alright so let's start with our\nimports and I'm gonna cheat don't be mad\nI'm gonna copy those in right here so we\njust have you know torch and we have\ntorch text which I've made tutorials of\nif you wanna watch those and we're gonna\nimport the multi 30k data set which is\nas I said in an intro a German to\nEnglish data set and then we're going to\nimport field and bucket iterator for the\npre-processing we're gonna have numpy\nwe're gonna have Spacey for a tokenizer\nand then we're gonna have tensor board\njust to get some nice cost plots or loss\nloss plots so anyways let's first do the\nspace eager to be Spacey dot load of\nde which is gonna load the the German\ntokenizer and sorry for those who watch\nmy tour text tutorials I'm gonna go\nthrough basically the same thing\nso Spacey English its Spacey that load\nin the air so this is gonna be our\nEnglish tokenizer and we're gonna define\ntokenizer in German it's gonna take some\ninput as text then we're gonna return\ntokenizer text for toke in space see\ngerm tokenizer of some text\nalright so it's just gonna take some\ntext I don't know some text in German\nand it's gonna output so basically if I\nhave hello my name is and it's gonna\noutput basically a list of hello my name\nit's something like this so it's a\ntokenizer and that's what it does so I'm\ngonna do the same thing for the English\none also gonna take aim put some text\nand let's just copy this in right here\nso taupe text for Spacey underscore\nEnglish tokenizer alright so let's now\nconstruct our field which is gonna we're\ngonna define how the pre-processing of\nthe text is done so we're gonna do field\nand we're gonna do tokenize equals\ntokenize German and then we're gonna do\nthe lower equals true to make everything\nlowercase and we're gonna do in its\ntoken to be start-up sentence like this\nand and token to be end of sentence like\nthis and then we're gonna need one for\nour English text as well so we're gonna\ndo English equals field\nyes I'm rushing this a bit because I\nhave made this in my previous tutorial\nwatch this if you want watch those if\nyou want more info about this this is\nkind of just coding it for the sake of\ncoding it I guess so then we're gonna do\nlower equals true and we're gonna do\ntokenize English so now that we have\nthose we can use the multi 30k dataset\nso we're going to use train data\nvalidation data and test data to be\nMulti 30k dot split and then extensions\nare we're gonna have dot de for the\nGerman one and then I'm gonna have dot\nen for the English one and then we gotta\ndefine our field and our fields are\ngonna be German first to match the first\nin the extension which is de for German\nand then we're gonna have English okay\nnow that we have our training data all\nthat we need to do or ASR two things we\nneed to do first we need to build our\nvocabulary alright easy enough we're\njust gonna do German that build vocab on\nour training data and we're also gonna\ndefine sort of a max size of the\nvocabulary so I don't know ten thousand\nsomething like that and we're gonna use\na minimum frequency of two so\nessentially if a word is only repeated\nonce in the entire data set it's we're\nnot gonna add it to a vocabulary it\nneeds to be at least two times and then\nwe're gonna do English dot build vocab\nof training data and max size is equal\nto a thousand yeah it should have copy\npasted this so minimum frequency is two\nand yeah alright now we come to the fun\npart we have one more thing to do with\nthe torch text which is define our\nbucket iterator but we're gonna do that\nlater on and that's the only thing we\nhave left so what we what we we have is\nactually you know implementing the model\nso we're gonna have class encoder and\nthen module let me just do a pass so we\ncan see sort of the overview of what\nwe're gonna do we're gonna have an\nencoder right the first lsdm and then\nwe're gonna have decoder which is also\ngonna hurt an in module that's the\nsecond ellis IAM and then we're gonna\nhave another so sequence to sequence\nit's also gonna import and in module and\nso essentially this sequence of sequence\nis gonna combine the encoder and decoder\nokay and is there anything else no so\nokay that's this is gonna take a little\nbit of time but we're gonna go through\nit step by step so first thing we're\ngonna do is we're gonna have in it self\nand we're gonna define\nwe're gonna have some input size and\nlet's see the input size yeah so the\ninput size is basically going to be the\nsize of the vocabulary in this case the\nGerman vocabulary right because this is\ngoing to be the input to the encoder\nthen we're gonna have some embedding\nsize so that each word is mapped to some\nD dimensional space okay basically and\nthen we're gonna have hidden sighs and\nwe're gonna have a number of layers of\nthis lsdm and then we're gonna have\ndropout and so first gotta call super of\nencoder self in it like this and so I\nguess we can do self that hidden size is\nequal to hidden size and self letting on\nlayers is just equal to num layers and\nthen we can sort of define all of the\nmodules that we're gonna use so we're\ngonna use dropout we're gonna do self\nthat dropout is and then that dropout\nwith some value empty of the parameter\nof how many nodes to drop so we're just\ngonna send that in as a dropout that's\nmaybe a bad name\nmaybe we should name it like I don't\nknow like P or something and then self\ndot embedding is gonna be an endowed\nembedding of some input size and it's\ngonna be mapped to some embedding size\nall right and then we're gonna have self\nat RN n it's gonna be N&L STM and the\nEllis Amy is gonna take his input so\nwe're gonna run the embedding on the\ninput and then we're gonna run the RN n\non those embeddings so this side the\ninput of the other serums is the\nembedding size and then the output let's\nsee so the the output from the LS M is\njust gonna be the hidden size and we're\ngonna have num layers and then we're\ngonna have dropout equals dropout which\nwell dropout equals P so this is a what\nis it called key a key argument yeah key\nargument so then we're going to define\nforward self comma X so we're gonna\nwrite we're gonna send him\nI guess a long vector of of you know all\nof the indexes of the word that is in\nthe vocabulary well that was a poor\nexplanation so basically we're gonna\nhave a sentence first of all it's gonna\nbe tokenized and it's gonna be mapped to\nsome index corresponding to where it is\nin the vocabulary and then that's gonna\nthat vector is gonna be sending to the\nlsdm so this is a vector of indices so\nwhat we're gonna do first so basically\nthis shape of X is sequence length comma\nn right where where n is the batch size\nand then we're gonna do the embedding to\nbe self dot drop out of self dot\nembedding of X and so when we have the\nembedding we're just gonna call the RN n\non this embedding like this and so we're\ngonna have some outputs and we're gonna\nhave some hidden and cell output and the\nonly thing that we care about is the\nhidden and the cell right if you\nremember that figure I showed in the\nbeginning all we care about is a context\nvector and the context vector is the\nhidden in the cell so the output from\nthis is a irrelevant and so we're gonna\njust return hidden comma cell and let me\njust also explain the shapes of this so\nnothing is missing in this tutorial so\nwe're gonna say like embed the shape of\nthe embedding is gonna be sequence\nlength right comma n which is the shape\nof X but then each of those so each word\nis also going to be mapped to some so to\nsome embedding size okay so we basically\nadded another another dimension to our\ntensor so that I know we can define the\nembedding size but let's say it's 300 so\nfor each word there's now a mapping to\nsome 300 dimensional space okay so now\nthat we have the let's see we have the\nencoder and we know that we got the\nhidden in the cell we're gonna start\nwith the decoder and let's do the same\nthing define in\nand we're also gonna take self input\nsize the embedding size hidden size and\nthen we're also gonna have some input\noutput size which so you know the the\ninput size here is going to be the size\nof the the English vocabulary right but\nif you think about it the output size is\ngoing to be the same as input size okay\nso we're gonna output some some sort of\nsome vector which corresponds to values\nfor each word which is in our vocabulary\nso basically if we have a I don't know\nlike if we have 10,000 in our vocabulary\nin English vocabulary\nso we're gonna have like a like a a\noutput which is 10,000 dimensional where\neach node represents sort of the\nprobability that it is it's it's that\nword in the vocabulary\nokay so then then the input size is\ngonna be the same as the output size but\nso we could have just written this as a\nseed like output size or input size but\njust to be very clear and then we're\ngonna have the same the number of layers\nand we're gonna have some drop out and\nas usual we gotta call the super and\nself dot in it and so let's see what did\nwe do with the self dot hidden size to\nbe hidden size and then cellphone on\nlayers to be num layers and self that\ndrop out like this is gonna be an entire\nop out and yeah so we did and drop drop\nout of P and and yeah so we got to do\nthe embedding which is gonna be a nun\ndot embedding of some input size and map\nto some embedding size and yeah so let\nme also well actually let's do that in\nthe forward let's just define all of the\nthe methods that we're going to use\nor the modules and then separate RNN is\ngonna be n n dot L STM and the input to\nthe LCM is gonna be the embedding size\nmap to some hidden size number of layers\nand then dropout equals P and yeah also\none thing that's important is that the\nhidden size of the encoder and the\ndecoder are the same so just one thing\nto keep in mind and then we're gonna\nhave also a fully connected which is\ngoing to be a nun linear of hidden size\nto some output size and again the output\nsize is just going to be the length of\nour in this case or English vocabulary\nwe're all gonna define all of those\nlater\nbut we're gonna do this forward self dot\nX but it's also now gonna take in the\nhidden and the cell okay so the context\nvector we talked about and then one\nthing to I need to say here is that the\nshape of X is going to be n like this\nbut we want it to be you know so but we\nwant one comma N because what the\ndecoder is gonna do it's gonna predict\none word at a time\nokay so given the the previous hidden\nand the cell state and sort of the the\nprevious word it's gonna or I guess yeah\nthe previous predicted value the\nprevious predicted words let me bring\nout this figure again so sort of it\npredicts good and then it's gonna take\nin good right here and then sort of also\nthe previous hidden cell and then it's\ngoing to output a one word so sort of\nthis is gonna work a word by word and if\nso this one here represents that it's\njust a single word we have n batches of\na single word at the same time and this\nis in contrast to you know the encoder\nwhich had sequence length of words at a\ntime because in the encoder we can just\nsending the entire German sentence but\nwhen we actually do the prediction we're\ndoing it one word at a time\nso so this is easy in pytho we're just\ngoing to do X ton squeeze of zero so we\njust added one dimension and now we can\nsend it to the the embedding so we can\nyourself that dropout of self dot\nembedding of X and this is gonna just be\ncalled our embedding and then yeah so I\ncan do these shapes again so embedding\nshape it's just gonna be 1 comma n coma\nand\n[Music]\nembedding sighs yeah then we're ready to\nsend it in through the lsdm so we're\ngonna do self dot RNN and we're gonna do\nand betting it's gonna take the\nembedding as input and then it's also\ngoing to take the hidden and the cell\nright the hidden in the cell we sent in\nhere so this is gonna be we're gonna get\nthe output but then we're also gonna get\nthe hidden ins and the cell and we got\nto keep track of all because all of\nthese are important now because we're\ngonna use the hidden in the cell for the\nnext prediction sort of the next word we\npredict and the outputs of course is\njust the the output for this well we\nthink this next word should be so the\nMcGinn shape of outputs is gonna be 1\ncomma n comma hidden size hidden side\nand is there anything else I want to say\nhere\nno we're gonna do one more so we're\ngonna do self-thought fully connected of\noutputs and this is gonna be called our\npredictions and so basically so put the\nshape of predictions is gonna be 1 comma\nn comma you know the the length of\nvocabulary so our English vocabulary and\nthis is then gonna be sent to you know\nour loss function later on but what we\nwant to have the shapes is that we want\nto remove this one now so so you know\nlet's just do predictions dot squeeze of\n0 and then call this our prediction and\nthen we can just return let's see why\ndid we want to remove this dimension I\nthink it was because when we send it to\nthe C when we send it to the\n[Music]\nwhen we send it to the loss function\nlater on let's see let's keep that in\nmind let's see if we if I remember why\nwe wanted to remove that one but\nessentially you know all of these things\nare like we need to make sure that your\nshapes are the way that PI torch wants\nit and sort of it's kind of a guess and\ntry like you you try something in the\nshape scene and work ok what does it\nwant and then you just adapt it until it\nworks so that's yeah so there's nothing\nlike major major thinking behind these\nshapes right here I'm just trying to\nexplain the top pattern behind why I'm\ndoing the shapes and what the shapes are\ncurrently at so you sort of follow every\nstep yeah I just wanted to add some\nwords on this why we squeeze the first\ndimension so it is nothing to do with\nloss function directly\nso essentially you know we're predicting\none word at a time\nbut we of course want to predict all of\nthe words in the target sentence so what\nwe're gonna do later on when we well\nyou're going to see in the sequence the\nsequence model we're going to sort of\nadd the output from the decoder one step\nat a time and then having this shape\nmakes sense yeah that's the best\nexplanation I can have it's gonna make\nsense later on anyways moving on anyways\nthen we're going to return the\npredictions but we're also going to\nreturn the hidden and the cell and let's\nsee so now that we have we have our\nencoder and we have our decoder now\nwe're ready to combine those two so\nwe're ready to create our model\nessentially and our model is going to be\nthis sequence of sequence so we're going\nto define in it I'm going to self and\nwe're gonna send the encoder and the\ndecoder then we're going to do super of\nsequence sequence of self dot in it and\nwe're gonna do self that encoder is just\ngonna be encoder so if the decoder it's\ngonna be you guessed at decoder and then\nwe're just gonna do the forward one so\nlet's see what are we gonna send in here\nso we're gonna send in our we're gonna\nsend in the source sentence like they\ndid the German sentence and then we're\nalso going to send the\nthe the correct target sentence and I'm\ngonna explain that a little bit more so\nwe're gonna first we're just gonna send\nin the source and we're gonna send you\nthe target so the correct translation\nand then I'm gonna use one more thing so\nI'm gonna use teacher force ratio we're\ngonna set this to 0.5 so let me explain\nthis what this is used for so you know\nas I said let's bring out this figure\nagain so as I said it's going to run at\nthe beginning it's gonna have a star\ntoken and it's gonna predict one word\nand then it's gonna you know take this\nword and it's gonna be the input to the\nnext I guess\ncell okay for then so that it can\npredict in the next word well sometimes\nyou wanted to use you know this\nprediction is not always correct and you\ndon't want it to send in the like the\nincorrect one for the next cell because\nthat would just like that would ruin a\nlot of sentences so for training phase\nso what you want is you probably want to\ndo some prediction right here that can\nlater be on used for creating the you\nknow calculating the loss and so on but\nthen for the next cell you actually send\nin the target one like that the real one\nor what it should translate to and\nthat's why we send in the target and\nthen for the let's say the teacher force\nratio is that sometimes we're gonna use\nthe correct one right here and sometimes\nwe're gonna use a fake one I mean not a\nfake one so the predicted one the\npredicted one from the previous cell\nokay so sometimes we're gonna take the\nprediction some and sometimes fifty\npercent of the cases we're gonna choose\nthe target translated word but it should\nhave been so sort of the reason why we\ndo this is we don't want to talk teacher\nforce ratio to be one I to be the\ncorrect target words all the time\nbecause then a test time the words that\nit predicts might be completely\ndifferent than what it sees a training\ntime all right I talked too long about\nthat one so anyways we're gonna do batch\nsize and this is going to be sourced\nthat shape of one\nand yeah that's just shape of it so\nwe're gonna send him let's see\nsource I think yes the source is gonna\nbe don't quote me on this but I think\nit's gonna be target link , the the\nbatch size if I remember correctly or\ncomma n so yeah anyways the batch size\nis gonna be the the second dimension of\nthe source shape and then target length\nis gonna be target dot shape of 0 and\nunless you target a vocabulary size is\ngonna be the length of the English\nvocabulary all right and we created B\nlet's see let me scroll up here we\ncreated the English vocabulary so we can\njust do length of English vocabulary and\nthen let us start with doing the output\nto be well actually it's 2 so remember\nwhat we're gonna do now is that we have\nan encoder and we have the decoder and\nwe're gonna run through the encoder to\nget our hidden and the cell and that's\ngonna be the input to the decoder and so\nthe first thing we're gonna do is we're\ngonna kind of build up those hidden and\ncell state so we can do you know hidden\ncomma cell to be self that encoder of\nsource that my cell phone right now\nalright I'll be right back alright so\ndon't people know when I'm making videos\nanyway so where were we\nwe have target vocabulary size and then\nhidden itself from the self dot encoder\nalright what's the next step well the\nfirst thing we're gonna do is we're\ngonna have the star token so we're gonna\nhave so we're gonna do x equals target\nof 0 so that this will be the grab start\ntoken and then what we're gonna do is\nwe're gonna send in this to the decoder\nright and we're gonna do this word by\nword so it's the give 40 in range of 1\ncomma target Len and target length and\nthen so we're gonna do output he didn't\ncall my cell so we're gonna call the\ndecoder\nsafe the decoder of X and we're gonna\nuse the hidden and the cell from the\nencoder so we're gonna also input hidden\nand the cell and then so the output from\nthis is gonna be the next hidden in the\ncell which is going to be reused in the\nloop right so then we're gonna do\nactually I forgot one thing alright so\nwhat we're gonna do also is we're gonna\nadd here outputs to be torched at zeroes\nof target length batch size and then\ntarget vocabulary size then we're going\nto also do dot two device so essentially\nwe're gonna predict one word at a time\nright and for each word we we predict\nwe're gonna do it for an entire batch\nand then every prediction is gonna be\nyou know sort of that that vector of of\nthe entire vocabulary size so when we do\none output right here it's gonna be\nbatch size comma and then target\nvocabulary size that's going to be the\nsize of the output tensor and then we're\ngoing just gonna sort of put that on\nthis dimension right here alright so\nthat was a poor explanation it's gonna\nmake more sense so we're gonna do output\nof T and then we're gonna equal that to\noutput all right and we're adding it\nalong that dimension the first dimension\nokay and then we're gonna do so we're\ngonna get the best guess which is going\nto be the output and then the Arg max of\none right since it's outputting in the\nsecond dimension right so it will be so\nthe output will be wait I didn't mean to\nrun that so output will be n comma and\nthen English vocabulary size and so for\neach batch if we take the Arg max of\nthis dimension we'll get the index of\nthe best guess that the decoder output\nit all right so then what we're going to\ndo we're going to do\nso the next input to the decoder is\ngoing to be target t if random dot\nrandom is less than teacher force ratio\nelse best guess\nokay so we're gonna the next input to\nthe decoder will be the target word if\nthis random dot random which is between\nwill be between zero and one is less\nthan this this teacher force ratio that\nwe input to the model so 50% of the\ntimes it's gonna get the actual one and\nthe rest is just gonna get the guests\nfrom the decoder then at the end of this\nloop we're just gonna return outputs\nokay is there anything else I think\nthat's it's actually so now we've come\nto a to the point where we have the\nmodel and we have done the encoder and\ndecoder now we just gotta set up the the\ntraining phase right so let's do now\nwe're ready to do the training\nwell that was alive we need to define\neverything first so we need to do\ntraining hyper parameters we gotta do\nmotto\nyeah model high parameters I guess so\nwe're gonna do like the number of epochs\nlet's do like 20 learning rate its to\n0.001 and then batch size let's do 64\nit gets pretty standard so far and then\nwe could do like load model equals false\nand then the device torch that device\nCUDA if torch CUDA is available else\nwe're gonna use the CPU and yeah so the\ninput size to the encoder it's just\ngonna be the length of the German\nvocabulary and then the input size to\nthe decoder will be just the length of\nthe English vocabulary the output size\nis gonna be the length of the English\nvocabulary as well and then we can\ndefine the encoder the embedding size so\nlet's say I don't know 300 and then the\ndecoder embedding size 300 as well I\nthink 300 might be actually a large\ndimension for the embedding size so I've\nseen people use like a hundred for the\nembedding size but I think that's on the\nsmaller end so I guess somewhere around\n100 to 300 is a good number I know\nyou're gonna do the hidden size let's do\na thousand 24 and I think so the paper\nreview on the sequence is sequence\nthough they use the hidden size of a\nthousand twenty-four thousand or\nsomething like that\nbut I think that's small for today\nstandard since that paper was from 2014\nI don't know it's I guess a medium size\ntoday so then we're gonna do the number\nof layers and let's do just two layers\nthinking the paper they did for let's\njust do two so it doesn't take too long\nand then let's do encoder drop out to be\n0.5 decoder drop out to be 0.5 and one\nmore thing let's do something for the\ntensor board so we're going to do\nwriter by the way check out my video on\ntensor board if you're unsure about this\nbut we're just gonna do summary writer\nruns loss plot so what we're gonna do is\nwe're just gonna essentially we're gonna\njust do it to get a loss plot so that we\ncan see sort of how the loss is\ndecreasing and also we're gonna do step\nequals zero all right all right that was\na lot to do and then we got also do the\nget iterator from what I said in the\nbeginning of the video we also got to\nuse the bucket iterator to split the\ndata so we're gonna do training iterator\nvalidation iterator and then that this\ntest iterator is bucket iterator dot\nsplit and we're gonna send in the train\ndata validation data and the test data\nand one note on this make sure that they\nmatch the training reduced first and\nthen this will be the train data\nvalidation iterator is second and the\nvalidation data a second okay so then\nwe're gonna do the batch size which is\njust going to be equal to batch size and\nI guess one thing that I didn't talk\nabout that much in my torch text\ntutorials is that you can do sort within\nbatch we're gonna use that since it will\nbe true and let me write this line and\nI'll explain what it does we're gonna in\nshort key is lambda X and then we're\ngonna do the length of X dot source\nalright so what this does is that we\nhave all of these sentences right and\nthey vary in length what their what the\nbucket area is gonna do when we have\nsort within batch true and then they\nsort key right here is that it's gonna\nprioritize to have examples that are of\nsimilar length in a batch right and why\nwe want to do this is because we want to\nminimize the number of padding so that\nwe can sort of save on compute all right\nand then we're going to do device equals\ndevice and all right and then I think\nwe're just going to call so we're going\nto encoder net to be the encoder and\nwe're gonna send all these arguments in\nso we're gonna send an input size of the\nencoder encoder embedding\nwe're also attending the hidden sighs\nI'm gonna sing the number of layers\nwe're gonna send encoder dropout and\nthen we're gonna make it to CUDA if it's\navailable and let me just copy this\nbecause we're gonna do I guess pretty\nmuch the same C decoder net and then\nwe're gonna call the decoder\nthis should be the coda right here and\nalso the coder embedding sighs hidden\nsighs this should be the output size and\nthe number of layers and then decoder\ndropout dot two device all right we can\ndo the model we're gonna do sequence to\nsequence and we're just gonna send them\nthe encoder and a decoder and lastly\nwe're just gonna make it to device all\nright we're almost there I promise we\ngot some more things to do so we're then\ngonna do this is gonna be a long video I\nthink anyways we're going to pad index\nto be English that vocabulary dot string\nto index of the pad in the end the pad\nstring and then we're gonna do the\ncriterion and we're just gonna use the\ncross entropy loss and we're gonna input\none keyword argument here we're gonna\nuse ignore index is equal to pad index\nso that we don't if we have padded it\ncuz it's gonna so that all the examples\nin the batch are saying similar lengths\nthen we don't want to pay anything for\nthat in our cost function all right and\nthen we're gonna use if load model we're\ngonna do load checkpoint torch that load\nand then my checkpoint dot P th star and\nthen model optimizer right I think so\nyeah I think I forgot let's see I got a\nload that as well and so I'm gonna do\nfrom utils and this is a a separate file\nthat I created and I just brought some I\nguess nice functions to have like we're\ngonna do translate sentence we're gonna\ncalculate a blue score I'm gonna do save\ncheckpoint and load checkpoint\nso the to save the model and load the\nmodel I've made a entire separate video\non that if you wanna check that out and\nthen for the blue score and the\ntranslate sentence that's just too nice\nutil functions that we're gonna we're\ngonna use I'm gonna probably talk about\nshow the implementation in a separate\nvideo just now to make this like over an\nhour because I don't know how long this\nvideo is gonna be right now so then if\nit's we're gonna use false yeah the\nbeginnings we don't have a model then\nwe're gonna do for epoch in range of non\na pox and we're gonna do print we're\ngoing to be Park epoch out of number of\nepochs like like this\nand then we're going to do checkpoint is\nstate licked and we're gonna do model\nthat state dick and then optimizer and\nthis is just standard code too so that\nwe can save the model state ticked I'm\ngonna have all the parameters and then\nwe're just gonna call it to the save\ncheckpoint of checkpoint alright then\nwhat we're going to do is we're going to\ndo for batch sit for patch in fact comma\nbatch in enumerate train iterator\nthen we're gonna do input data is equal\nto batch source and then we're gonna\njust make it to CUDA and then the target\nis gonna be the batch dot target dot 2\ndevice and so one thing we got to do now\nis that the we gotta just send this to\nour model write the input data and the\ntarget and then the output right here is\ngonna be so the output shape is gonna be\nthe target length and it's gonna be bash\nsize and then output dimension okay\nso there's a problem here is that in\nthat cross-entropy loss wants to have\nsort of a matrix this would be a\nthree-dimensional tensor but so for\nexample let's say that we would have the\nM nice data set we would have n comma 10\nto be the output sort of n examples at\nthe same time in each example outputs 10\nfor you know each digit and the output\nso and and then and then the you know\ntargets would be would be just this\nshape right and we got to do similar\nhere in that so you can sort of view\nthat we have a batch right and for each\nbatch we sort of have 20 outputs from\nthis model well one thing we can do is\nthat we can concatenate or we can yeah I\nguess it's called catenate we can sort\nof put these dimensions together so that\neach batch has I guess like the target\nlength amount of output that's going to\nbe sending to cross-entropy loss yeah I\ndon't know if I confused you right there\nbut anyways all we're gonna do is we're\njust gonna do output and we do some\nreshaping so we're gonna do upward and\nwe're gonna do dot reshape then we're\ngonna do minus 1 and then output shape\nof 2 right so we're going to keep the\noutput dimension which would be the size\nof the vocabulary and\njust gonna sort of put everything else\ntogether so we're just gonna I don't\nwhat is the word not concatenate but\nlike anyway so I lost the word and then\nwe're gonna do target is target oh one\nmore thing actually we're gonna do one\nsee we're gonna do one and then rest so\nessentially the first output right here\nis gonna be the the star token but we\ndon't want to send that to our to our\nmodel so we're just gonna remove that\nand we're gonna do the same for the\ntarget and we're gonna do reshape for\nthis one as well and so that this one is\njust a single dimension so that this\nwould take target length times the batch\nsize and that would be the shape of\ntarget then we can do optimizer dot 0\ngrad loss is going to be the criterion\nof output and target then we can do law\nstart backward we can also do another\nthing which is sort of to to avoid that\nthe gradients become too large so that\nwe avoid exploding gradient problems we\ncan do torch and then you tailstock clip\ngrad norm and then model dot parameters\nand then we can do max norm equals 1\nthis is just to sort of make sure that\nthe gradients are in a healthy range I\nguess and then we do optimizer dot step\nand yeah I promise we're almost done we\njust have like two more lines so we're\ngonna do brighter dot add scalar\ntraining loss loss and then the global\nstep is equal to step and then we're\njust gonna just step plus equals one all\nright\nso now believe it or not we've created\nthe entire thing we've basically created\ngoogle translate all that we're gonna do\nnow is we're gonna run this for twenty\nparks we're just gonna take a while but\nthat's the best part\nyou know when you've created everything\nnow it's just time to relax it back\ndrink some coffee just have a good time\nand so I'm gonna press the button and\nwhoa okay I thought I was gonna have a\ngood time\ntokenized Jerry not to find that's like\nin the beginning to see tokenized dirt\nright here tokenizer alright tokenizer\nif that was the only air then I'm very\nhappy right now\nand it seems to work alright I'll see\nyou guys in a bit\nin about one hour be right back okay so\nwe did did we did get one more error\nwhich is that this is not valid ish this\nis gonna be validation they don't know\nthe value data all right\nlet's rerun this and believe it or not\nwe got one more error so let's see we\nget we've got to do our optimizer as\nwell so we're gonna do optimizer equals\nopt-in Adam model dot parameters and\nthen the learning rate will just be\nlearning rate and hopefully that was the\nlast one unless we run this all right so\nit's it's training now and it seems to\nwork but I got one more idea that I\nwould like to do so I took one sentence\nhere from the see right here sentence so\nI took this quite a long sentence right\nhere and let's keep it on one line or we\ncould do like yeah we can keep it all in\none line anyways this very long sentence\nhere I don't know it was something like\nI don't know German but it's like a boat\nwas pulled by a large team of horses or\nsomething like that so this is in the\ntraining data and I thought it would be\ninteresting to to just like see how how\nthis translated sentence would improve\nas epochs go on so I was thinking we\ncould do like we can do model dot\nevaluate and then we can do this\ntranslate sentence that I used that I\ncreated and I'm gonna show that one\nprobably the next video but anyways we\ncan send it to translate sentence and we\ncan do model and then this sentence and\nwe gotta send in the the fields as well\nand then the device and then the maximum\nlength of the prediction so this will be\njust purely a we're just gonna sending\nthis string and we're just gonna get\nwhat does the model predict that it's\ngonna be and that we can do sort of like\nprint translated example sentence and we\ncan do a new line and then we can do\ntranslated sentence and\nthen we can do mala train again alright\nso the model that eval is just gonna I\nguess turn off the dropout in this case\nyeah I just thought that would be\ninteresting let's now run this and let\nit rain alright I gotta just show you\nwhat this sentence was actually saying\nso the center's were here that I took\nfrom the train data it said about with\nother men is pulled to the shore by a\nlarge team of horses okay and I trained\nthe model so I'm gonna bring this over\nhere and yeah I trained it on a couple\noccasions so I just printed it too I\njust wrote it or not nope add plus plus\nfile so anyways on the first epoch it\nsaid actually there's another epoch that\nI forgot to copy and that was\nessentially like just random words and I\nguess this is also random words but it\nwas more random words so it didn't have\nthis end of sentence token but yeah so I\nguess this is the second epoch\ntechnically so then it was like a young\nplayer is a AAA and you you'll see a lot\nof these a and their or is or those\nwords since they they are very similar\nin the data so it's not uncommon to just\nsee like a AAA and that's all the model\npredict alright and then let's let's see\nfor the next epoch a runner in a with a\nin the background alright it doesn't\nreally make sense like there's nothing\nto do with boats or anything like that\nbut at least sort of doing something\nthat makes I guess some sort of sense a\nbaseball player with a is is the the I\nguess that doesn't make any sense a\nfootball player in a is being ready to\nAAA okay and then it comes a both with\nwith his men is being pulled by a large\nbuilding I mean it got something's right\nin this one and then a boat with a full\nof a is being pulled by a large so we\ncan sort of see you know the model is\nactually progressing he's doing\nsomething that makes sense right hey\nboth both is being pulled by a large\nlarge large large large in the sentence\nand then a both both is being pulled by\na large building building again\nboth with with a a is pulled by a large\nbeing pulled by a large okay they both\nboth with two men being pulled by a\nlarge large okay both with no passengers\nis pulled by a large Ford by a large\nokay I mean what is interesting here is\nthat it's being pulled by something and\nthen I guess by something again a both\nboat with two men being pulled by a\nlarge cable I'm not sure how it arrived\nat that one but anyways you know you can\nsort of see let's see this last one here\na boat boat with several men pulled\npulled by a large cable all right so\nit's sort of getting there right and I\nshould have copied all of them but like\nI trained it for like 50 bucks more and\nI didn't copy it so we don't see it but\nat least we can see how it does now so\nwhen I rerun it now with these saved\nparameters it's a boat carrying several\nmen is pulled by shore by a large team\nof horses and alright you might be\nsaying like well this data isn't it's in\nthe training data it's of course going\nto learn those sentences all right it\ndoesn't prove anything\nwell I'm gonna prove hopefully I'm gonna\nprove you wrong so we're gonna do\nsomething like we're gonna use Google\nTranslate we're gonna write an English\nsentence and we're in translated in\nGerman and see if we get the English\nsentence back one thing to keep in mind\nis like this sentence has to be like a\ndescription of something because there's\nthere's like I don't\nI tried first like writing like I really\nlove peanuts and it didn't get it at all\nbecause there's like no sentences that\nwritten in that sort of that I guess uh\nwhat is it called in that form like\nsomething that I like it's more like a\ndescription like that person walked\nthere and then yada yada yada to people\nwalking walk in and to the store to buy\nvery cold ice cream sort of something\nlike that and then we can copy this we\ncan go back right here and we can do our\nscent\nis going to be we're gonna copy that in\nand we can do I have never done this\nlet's see if this makes sense\nso either you're right and this makes no\nsense or we just created Google\nTranslate to people going to a cross\ngetting ready to to end of sentence all\nright I'm gonna defend myself right here\nit's two people and that was exactly\nwhat we wrote and they're going\nsomewhere and they're not getting ready\nbut at least like it's getting some\nthings right we have to just train this\nfor a longer time and it would make\nperfect sense\nprobably so one thing you could also do\nas they did in the in the paper we we\nread is that you can calculate this blue\nscore and I'm gonna copy that in so we\ncan probably do that at the end of\ntraining like this and I'm gonna do that\nand I'm gonna see so we can do like this\nand we're gonna see what kind of blue\nscore we get on the test data that it\nhasn't seen before right and yeah I'm\ngonna run this it's gonna take a while\nand I'll get back when it's done so\nthere was something wrong with the\nkernel and I had to restart it but\nrunning it again we get a blue score of\nabout 21 which is you know it's not that\nbad it's all right it's not that good\neither but it's not terrible so you know\nwe need to you know we're not creating\nGoogle Translate yet right we have some\nwork to do which is awesome because then\nwe got some more stuff to learn so\nbasically I guess in the next video I'm\ngonna review sequence sequence with\nattention which is AI an idea that you\nlike when you see it you're like oh of\ncourse\nbut then again every idea is I guess\neasy or simple in retrospect so in the\nnext video we're gonna do sequence\nsequence with attention probably review\nthe paper and then we're gonna do\nimplementation of that and we'll see how\nmuch does that improve the blue score\nand yeah so this will be the start of a\nof a nice series I guess so hopefully\nyou enjoyed the video if you have any\nquestions and leave them in the comment\nsection below\nI think I said thank you for watching\nbut yeah thank you for watching hope to\nsee you in the next video\n[Music]\n",
  "words": [
    "going",
    "guys",
    "hope",
    "awesome",
    "tutorial",
    "gon",
    "na",
    "coolest",
    "project",
    "ever",
    "basically",
    "gon",
    "na",
    "gon",
    "na",
    "create",
    "google",
    "translate",
    "well",
    "sort",
    "specifically",
    "gon",
    "na",
    "implement",
    "sequence",
    "sequence",
    "model",
    "using",
    "ls",
    "themes",
    "data",
    "set",
    "german",
    "english",
    "sentences",
    "let",
    "play",
    "beautiful",
    "intro",
    "went",
    "sequence",
    "sequence",
    "paper",
    "last",
    "video",
    "watch",
    "one",
    "let",
    "quick",
    "30",
    "second",
    "recap",
    "let",
    "bring",
    "figure",
    "right",
    "beautiful",
    "figure",
    "gon",
    "na",
    "gon",
    "na",
    "two",
    "ls",
    "dm",
    "gon",
    "na",
    "left",
    "one",
    "left",
    "one",
    "right",
    "gon",
    "na",
    "called",
    "encoder",
    "gon",
    "na",
    "take",
    "german",
    "sentence",
    "gon",
    "na",
    "produce",
    "red",
    "box",
    "right",
    "gon",
    "na",
    "call",
    "contacts",
    "vector",
    "zed",
    "gon",
    "na",
    "input",
    "ls",
    "diem",
    "going",
    "called",
    "decoder",
    "essentially",
    "one",
    "gon",
    "na",
    "take",
    "star",
    "token",
    "going",
    "start",
    "predicting",
    "words",
    "translated",
    "words",
    "cell",
    "cell",
    "gon",
    "na",
    "produce",
    "output",
    "sentence",
    "alright",
    "bit",
    "longer",
    "30",
    "seconds",
    "never",
    "mind",
    "let",
    "get",
    "started",
    "code",
    "alright",
    "let",
    "start",
    "imports",
    "gon",
    "na",
    "cheat",
    "mad",
    "gon",
    "na",
    "copy",
    "right",
    "know",
    "torch",
    "torch",
    "text",
    "made",
    "tutorials",
    "wan",
    "na",
    "watch",
    "gon",
    "na",
    "import",
    "multi",
    "30k",
    "data",
    "set",
    "said",
    "intro",
    "german",
    "english",
    "data",
    "set",
    "going",
    "import",
    "field",
    "bucket",
    "iterator",
    "gon",
    "na",
    "numpy",
    "gon",
    "na",
    "spacey",
    "tokenizer",
    "gon",
    "na",
    "tensor",
    "board",
    "get",
    "nice",
    "cost",
    "plots",
    "loss",
    "loss",
    "plots",
    "anyways",
    "let",
    "first",
    "space",
    "eager",
    "spacey",
    "dot",
    "load",
    "de",
    "gon",
    "na",
    "load",
    "german",
    "tokenizer",
    "sorry",
    "watch",
    "tour",
    "text",
    "tutorials",
    "gon",
    "na",
    "go",
    "basically",
    "thing",
    "spacey",
    "english",
    "spacey",
    "load",
    "air",
    "gon",
    "na",
    "english",
    "tokenizer",
    "gon",
    "na",
    "define",
    "tokenizer",
    "german",
    "gon",
    "na",
    "take",
    "input",
    "text",
    "gon",
    "na",
    "return",
    "tokenizer",
    "text",
    "toke",
    "space",
    "see",
    "germ",
    "tokenizer",
    "text",
    "alright",
    "gon",
    "na",
    "take",
    "text",
    "know",
    "text",
    "german",
    "gon",
    "na",
    "output",
    "basically",
    "hello",
    "name",
    "gon",
    "na",
    "output",
    "basically",
    "list",
    "hello",
    "name",
    "something",
    "like",
    "tokenizer",
    "gon",
    "na",
    "thing",
    "english",
    "one",
    "also",
    "gon",
    "na",
    "take",
    "aim",
    "put",
    "text",
    "let",
    "copy",
    "right",
    "taupe",
    "text",
    "spacey",
    "underscore",
    "english",
    "tokenizer",
    "alright",
    "let",
    "construct",
    "field",
    "gon",
    "na",
    "gon",
    "na",
    "define",
    "text",
    "done",
    "gon",
    "na",
    "field",
    "gon",
    "na",
    "tokenize",
    "equals",
    "tokenize",
    "german",
    "gon",
    "na",
    "lower",
    "equals",
    "true",
    "make",
    "everything",
    "lowercase",
    "gon",
    "na",
    "token",
    "sentence",
    "like",
    "token",
    "end",
    "sentence",
    "like",
    "gon",
    "na",
    "need",
    "one",
    "english",
    "text",
    "well",
    "gon",
    "na",
    "english",
    "equals",
    "field",
    "yes",
    "rushing",
    "bit",
    "made",
    "previous",
    "tutorial",
    "watch",
    "want",
    "watch",
    "want",
    "info",
    "kind",
    "coding",
    "sake",
    "coding",
    "guess",
    "gon",
    "na",
    "lower",
    "equals",
    "true",
    "gon",
    "na",
    "tokenize",
    "english",
    "use",
    "multi",
    "30k",
    "dataset",
    "going",
    "use",
    "train",
    "data",
    "validation",
    "data",
    "test",
    "data",
    "multi",
    "30k",
    "dot",
    "split",
    "extensions",
    "gon",
    "na",
    "dot",
    "de",
    "german",
    "one",
    "gon",
    "na",
    "dot",
    "en",
    "english",
    "one",
    "got",
    "ta",
    "define",
    "field",
    "fields",
    "gon",
    "na",
    "german",
    "first",
    "match",
    "first",
    "extension",
    "de",
    "german",
    "gon",
    "na",
    "english",
    "okay",
    "training",
    "data",
    "need",
    "asr",
    "two",
    "things",
    "need",
    "first",
    "need",
    "build",
    "vocabulary",
    "alright",
    "easy",
    "enough",
    "gon",
    "na",
    "german",
    "build",
    "vocab",
    "training",
    "data",
    "also",
    "gon",
    "na",
    "define",
    "sort",
    "max",
    "size",
    "vocabulary",
    "know",
    "ten",
    "thousand",
    "something",
    "like",
    "gon",
    "na",
    "use",
    "minimum",
    "frequency",
    "two",
    "essentially",
    "word",
    "repeated",
    "entire",
    "data",
    "set",
    "gon",
    "na",
    "add",
    "vocabulary",
    "needs",
    "least",
    "two",
    "times",
    "gon",
    "na",
    "english",
    "dot",
    "build",
    "vocab",
    "training",
    "data",
    "max",
    "size",
    "equal",
    "thousand",
    "yeah",
    "copy",
    "pasted",
    "minimum",
    "frequency",
    "two",
    "yeah",
    "alright",
    "come",
    "fun",
    "part",
    "one",
    "thing",
    "torch",
    "text",
    "define",
    "bucket",
    "iterator",
    "gon",
    "na",
    "later",
    "thing",
    "left",
    "actually",
    "know",
    "implementing",
    "model",
    "gon",
    "na",
    "class",
    "encoder",
    "module",
    "let",
    "pass",
    "see",
    "sort",
    "overview",
    "gon",
    "na",
    "gon",
    "na",
    "encoder",
    "right",
    "first",
    "lsdm",
    "gon",
    "na",
    "decoder",
    "also",
    "gon",
    "na",
    "hurt",
    "module",
    "second",
    "ellis",
    "iam",
    "gon",
    "na",
    "another",
    "sequence",
    "sequence",
    "also",
    "gon",
    "na",
    "import",
    "module",
    "essentially",
    "sequence",
    "sequence",
    "gon",
    "na",
    "combine",
    "encoder",
    "decoder",
    "okay",
    "anything",
    "else",
    "okay",
    "gon",
    "na",
    "take",
    "little",
    "bit",
    "time",
    "gon",
    "na",
    "go",
    "step",
    "step",
    "first",
    "thing",
    "gon",
    "na",
    "gon",
    "na",
    "self",
    "gon",
    "na",
    "define",
    "gon",
    "na",
    "input",
    "size",
    "let",
    "see",
    "input",
    "size",
    "yeah",
    "input",
    "size",
    "basically",
    "going",
    "size",
    "vocabulary",
    "case",
    "german",
    "vocabulary",
    "right",
    "going",
    "input",
    "encoder",
    "gon",
    "na",
    "embedding",
    "size",
    "word",
    "mapped",
    "dimensional",
    "space",
    "okay",
    "basically",
    "gon",
    "na",
    "hidden",
    "sighs",
    "gon",
    "na",
    "number",
    "layers",
    "lsdm",
    "gon",
    "na",
    "dropout",
    "first",
    "got",
    "ta",
    "call",
    "super",
    "encoder",
    "self",
    "like",
    "guess",
    "self",
    "hidden",
    "size",
    "equal",
    "hidden",
    "size",
    "self",
    "letting",
    "layers",
    "equal",
    "num",
    "layers",
    "sort",
    "define",
    "modules",
    "gon",
    "na",
    "use",
    "gon",
    "na",
    "use",
    "dropout",
    "gon",
    "na",
    "self",
    "dropout",
    "dropout",
    "value",
    "empty",
    "parameter",
    "many",
    "nodes",
    "drop",
    "gon",
    "na",
    "send",
    "dropout",
    "maybe",
    "bad",
    "name",
    "maybe",
    "name",
    "like",
    "know",
    "like",
    "p",
    "something",
    "self",
    "dot",
    "embedding",
    "gon",
    "na",
    "endowed",
    "embedding",
    "input",
    "size",
    "gon",
    "na",
    "mapped",
    "embedding",
    "size",
    "right",
    "gon",
    "na",
    "self",
    "rn",
    "n",
    "gon",
    "na",
    "n",
    "l",
    "stm",
    "ellis",
    "amy",
    "gon",
    "na",
    "take",
    "input",
    "gon",
    "na",
    "run",
    "embedding",
    "input",
    "gon",
    "na",
    "run",
    "rn",
    "n",
    "embeddings",
    "side",
    "input",
    "serums",
    "embedding",
    "size",
    "output",
    "let",
    "see",
    "output",
    "ls",
    "gon",
    "na",
    "hidden",
    "size",
    "gon",
    "na",
    "num",
    "layers",
    "gon",
    "na",
    "dropout",
    "equals",
    "dropout",
    "well",
    "dropout",
    "equals",
    "p",
    "called",
    "key",
    "key",
    "argument",
    "yeah",
    "key",
    "argument",
    "going",
    "define",
    "forward",
    "self",
    "comma",
    "x",
    "gon",
    "na",
    "write",
    "gon",
    "na",
    "send",
    "guess",
    "long",
    "vector",
    "know",
    "indexes",
    "word",
    "vocabulary",
    "well",
    "poor",
    "explanation",
    "basically",
    "gon",
    "na",
    "sentence",
    "first",
    "gon",
    "na",
    "tokenized",
    "gon",
    "na",
    "mapped",
    "index",
    "corresponding",
    "vocabulary",
    "gon",
    "na",
    "vector",
    "gon",
    "na",
    "sending",
    "lsdm",
    "vector",
    "indices",
    "gon",
    "na",
    "first",
    "basically",
    "shape",
    "x",
    "sequence",
    "length",
    "comma",
    "n",
    "right",
    "n",
    "batch",
    "size",
    "gon",
    "na",
    "embedding",
    "self",
    "dot",
    "drop",
    "self",
    "dot",
    "embedding",
    "x",
    "embedding",
    "gon",
    "na",
    "call",
    "rn",
    "n",
    "embedding",
    "like",
    "gon",
    "na",
    "outputs",
    "gon",
    "na",
    "hidden",
    "cell",
    "output",
    "thing",
    "care",
    "hidden",
    "cell",
    "right",
    "remember",
    "figure",
    "showed",
    "beginning",
    "care",
    "context",
    "vector",
    "context",
    "vector",
    "hidden",
    "cell",
    "output",
    "irrelevant",
    "gon",
    "na",
    "return",
    "hidden",
    "comma",
    "cell",
    "let",
    "also",
    "explain",
    "shapes",
    "nothing",
    "missing",
    "tutorial",
    "gon",
    "na",
    "say",
    "like",
    "embed",
    "shape",
    "embedding",
    "gon",
    "na",
    "sequence",
    "length",
    "right",
    "comma",
    "n",
    "shape",
    "x",
    "word",
    "also",
    "going",
    "mapped",
    "embedding",
    "size",
    "okay",
    "basically",
    "added",
    "another",
    "another",
    "dimension",
    "tensor",
    "know",
    "define",
    "embedding",
    "size",
    "let",
    "say",
    "300",
    "word",
    "mapping",
    "300",
    "dimensional",
    "space",
    "okay",
    "let",
    "see",
    "encoder",
    "know",
    "got",
    "hidden",
    "cell",
    "gon",
    "na",
    "start",
    "decoder",
    "let",
    "thing",
    "define",
    "also",
    "gon",
    "na",
    "take",
    "self",
    "input",
    "size",
    "embedding",
    "size",
    "hidden",
    "size",
    "also",
    "gon",
    "na",
    "input",
    "output",
    "size",
    "know",
    "input",
    "size",
    "going",
    "size",
    "english",
    "vocabulary",
    "right",
    "think",
    "output",
    "size",
    "going",
    "input",
    "size",
    "okay",
    "gon",
    "na",
    "output",
    "sort",
    "vector",
    "corresponds",
    "values",
    "word",
    "vocabulary",
    "basically",
    "know",
    "like",
    "vocabulary",
    "english",
    "vocabulary",
    "gon",
    "na",
    "like",
    "like",
    "output",
    "dimensional",
    "node",
    "represents",
    "sort",
    "probability",
    "word",
    "vocabulary",
    "okay",
    "input",
    "size",
    "gon",
    "na",
    "output",
    "size",
    "could",
    "written",
    "seed",
    "like",
    "output",
    "size",
    "input",
    "size",
    "clear",
    "gon",
    "na",
    "number",
    "layers",
    "gon",
    "na",
    "drop",
    "usual",
    "got",
    "ta",
    "call",
    "super",
    "self",
    "dot",
    "let",
    "see",
    "self",
    "dot",
    "hidden",
    "size",
    "hidden",
    "size",
    "cellphone",
    "layers",
    "num",
    "layers",
    "self",
    "drop",
    "like",
    "gon",
    "na",
    "entire",
    "op",
    "yeah",
    "drop",
    "drop",
    "p",
    "yeah",
    "got",
    "embedding",
    "gon",
    "na",
    "nun",
    "dot",
    "embedding",
    "input",
    "size",
    "map",
    "embedding",
    "size",
    "yeah",
    "let",
    "also",
    "well",
    "actually",
    "let",
    "forward",
    "let",
    "define",
    "methods",
    "going",
    "use",
    "modules",
    "separate",
    "rnn",
    "gon",
    "na",
    "n",
    "n",
    "dot",
    "l",
    "stm",
    "input",
    "lcm",
    "gon",
    "na",
    "embedding",
    "size",
    "map",
    "hidden",
    "size",
    "number",
    "layers",
    "dropout",
    "equals",
    "p",
    "yeah",
    "also",
    "one",
    "thing",
    "important",
    "hidden",
    "size",
    "encoder",
    "decoder",
    "one",
    "thing",
    "keep",
    "mind",
    "gon",
    "na",
    "also",
    "fully",
    "connected",
    "going",
    "nun",
    "linear",
    "hidden",
    "size",
    "output",
    "size",
    "output",
    "size",
    "going",
    "length",
    "case",
    "english",
    "vocabulary",
    "gon",
    "na",
    "define",
    "later",
    "gon",
    "na",
    "forward",
    "self",
    "dot",
    "x",
    "also",
    "gon",
    "na",
    "take",
    "hidden",
    "cell",
    "okay",
    "context",
    "vector",
    "talked",
    "one",
    "thing",
    "need",
    "say",
    "shape",
    "x",
    "going",
    "n",
    "like",
    "want",
    "know",
    "want",
    "one",
    "comma",
    "n",
    "decoder",
    "gon",
    "na",
    "gon",
    "na",
    "predict",
    "one",
    "word",
    "time",
    "okay",
    "given",
    "previous",
    "hidden",
    "cell",
    "state",
    "sort",
    "previous",
    "word",
    "gon",
    "na",
    "guess",
    "yeah",
    "previous",
    "predicted",
    "value",
    "previous",
    "predicted",
    "words",
    "let",
    "bring",
    "figure",
    "sort",
    "predicts",
    "good",
    "gon",
    "na",
    "take",
    "good",
    "right",
    "sort",
    "also",
    "previous",
    "hidden",
    "cell",
    "going",
    "output",
    "one",
    "word",
    "sort",
    "gon",
    "na",
    "work",
    "word",
    "word",
    "one",
    "represents",
    "single",
    "word",
    "n",
    "batches",
    "single",
    "word",
    "time",
    "contrast",
    "know",
    "encoder",
    "sequence",
    "length",
    "words",
    "time",
    "encoder",
    "sending",
    "entire",
    "german",
    "sentence",
    "actually",
    "prediction",
    "one",
    "word",
    "time",
    "easy",
    "pytho",
    "going",
    "x",
    "ton",
    "squeeze",
    "zero",
    "added",
    "one",
    "dimension",
    "send",
    "embedding",
    "dropout",
    "self",
    "dot",
    "embedding",
    "x",
    "gon",
    "na",
    "called",
    "embedding",
    "yeah",
    "shapes",
    "embedding",
    "shape",
    "gon",
    "na",
    "1",
    "comma",
    "n",
    "coma",
    "music",
    "embedding",
    "sighs",
    "yeah",
    "ready",
    "send",
    "lsdm",
    "gon",
    "na",
    "self",
    "dot",
    "rnn",
    "gon",
    "na",
    "betting",
    "gon",
    "na",
    "take",
    "embedding",
    "input",
    "also",
    "going",
    "take",
    "hidden",
    "cell",
    "right",
    "hidden",
    "cell",
    "sent",
    "gon",
    "na",
    "gon",
    "na",
    "get",
    "output",
    "also",
    "gon",
    "na",
    "get",
    "hidden",
    "ins",
    "cell",
    "got",
    "keep",
    "track",
    "important",
    "gon",
    "na",
    "use",
    "hidden",
    "cell",
    "next",
    "prediction",
    "sort",
    "next",
    "word",
    "predict",
    "outputs",
    "course",
    "output",
    "well",
    "think",
    "next",
    "word",
    "mcginn",
    "shape",
    "outputs",
    "gon",
    "na",
    "1",
    "comma",
    "n",
    "comma",
    "hidden",
    "size",
    "hidden",
    "side",
    "anything",
    "else",
    "want",
    "say",
    "gon",
    "na",
    "one",
    "gon",
    "na",
    "fully",
    "connected",
    "outputs",
    "gon",
    "na",
    "called",
    "predictions",
    "basically",
    "put",
    "shape",
    "predictions",
    "gon",
    "na",
    "1",
    "comma",
    "n",
    "comma",
    "know",
    "length",
    "vocabulary",
    "english",
    "vocabulary",
    "gon",
    "na",
    "sent",
    "know",
    "loss",
    "function",
    "later",
    "want",
    "shapes",
    "want",
    "remove",
    "one",
    "know",
    "let",
    "predictions",
    "dot",
    "squeeze",
    "0",
    "call",
    "prediction",
    "return",
    "let",
    "see",
    "want",
    "remove",
    "dimension",
    "think",
    "send",
    "c",
    "send",
    "music",
    "send",
    "loss",
    "function",
    "later",
    "let",
    "see",
    "let",
    "keep",
    "mind",
    "let",
    "see",
    "remember",
    "wanted",
    "remove",
    "one",
    "essentially",
    "know",
    "things",
    "like",
    "need",
    "make",
    "sure",
    "shapes",
    "way",
    "pi",
    "torch",
    "wants",
    "sort",
    "kind",
    "guess",
    "try",
    "like",
    "try",
    "something",
    "shape",
    "scene",
    "work",
    "ok",
    "want",
    "adapt",
    "works",
    "yeah",
    "nothing",
    "like",
    "major",
    "major",
    "thinking",
    "behind",
    "shapes",
    "right",
    "trying",
    "explain",
    "top",
    "pattern",
    "behind",
    "shapes",
    "shapes",
    "currently",
    "sort",
    "follow",
    "every",
    "step",
    "yeah",
    "wanted",
    "add",
    "words",
    "squeeze",
    "first",
    "dimension",
    "nothing",
    "loss",
    "function",
    "directly",
    "essentially",
    "know",
    "predicting",
    "one",
    "word",
    "time",
    "course",
    "want",
    "predict",
    "words",
    "target",
    "sentence",
    "gon",
    "na",
    "later",
    "well",
    "going",
    "see",
    "sequence",
    "sequence",
    "model",
    "going",
    "sort",
    "add",
    "output",
    "decoder",
    "one",
    "step",
    "time",
    "shape",
    "makes",
    "sense",
    "yeah",
    "best",
    "explanation",
    "gon",
    "na",
    "make",
    "sense",
    "later",
    "anyways",
    "moving",
    "anyways",
    "going",
    "return",
    "predictions",
    "also",
    "going",
    "return",
    "hidden",
    "cell",
    "let",
    "see",
    "encoder",
    "decoder",
    "ready",
    "combine",
    "two",
    "ready",
    "create",
    "model",
    "essentially",
    "model",
    "going",
    "sequence",
    "sequence",
    "going",
    "define",
    "going",
    "self",
    "gon",
    "na",
    "send",
    "encoder",
    "decoder",
    "going",
    "super",
    "sequence",
    "sequence",
    "self",
    "dot",
    "gon",
    "na",
    "self",
    "encoder",
    "gon",
    "na",
    "encoder",
    "decoder",
    "gon",
    "na",
    "guessed",
    "decoder",
    "gon",
    "na",
    "forward",
    "one",
    "let",
    "see",
    "gon",
    "na",
    "send",
    "gon",
    "na",
    "send",
    "gon",
    "na",
    "send",
    "source",
    "sentence",
    "like",
    "german",
    "sentence",
    "also",
    "going",
    "send",
    "correct",
    "target",
    "sentence",
    "gon",
    "na",
    "explain",
    "little",
    "bit",
    "gon",
    "na",
    "first",
    "gon",
    "na",
    "send",
    "source",
    "gon",
    "na",
    "send",
    "target",
    "correct",
    "translation",
    "gon",
    "na",
    "use",
    "one",
    "thing",
    "gon",
    "na",
    "use",
    "teacher",
    "force",
    "ratio",
    "gon",
    "na",
    "set",
    "let",
    "explain",
    "used",
    "know",
    "said",
    "let",
    "bring",
    "figure",
    "said",
    "going",
    "run",
    "beginning",
    "gon",
    "na",
    "star",
    "token",
    "gon",
    "na",
    "predict",
    "one",
    "word",
    "gon",
    "na",
    "know",
    "take",
    "word",
    "gon",
    "na",
    "input",
    "next",
    "guess",
    "cell",
    "okay",
    "predict",
    "next",
    "word",
    "well",
    "sometimes",
    "wanted",
    "use",
    "know",
    "prediction",
    "always",
    "correct",
    "want",
    "send",
    "like",
    "incorrect",
    "one",
    "next",
    "cell",
    "would",
    "like",
    "would",
    "ruin",
    "lot",
    "sentences",
    "training",
    "phase",
    "want",
    "probably",
    "want",
    "prediction",
    "right",
    "later",
    "used",
    "creating",
    "know",
    "calculating",
    "loss",
    "next",
    "cell",
    "actually",
    "send",
    "target",
    "one",
    "like",
    "real",
    "one",
    "translate",
    "send",
    "target",
    "let",
    "say",
    "teacher",
    "force",
    "ratio",
    "sometimes",
    "gon",
    "na",
    "use",
    "correct",
    "one",
    "right",
    "sometimes",
    "gon",
    "na",
    "use",
    "fake",
    "one",
    "mean",
    "fake",
    "one",
    "predicted",
    "one",
    "predicted",
    "one",
    "previous",
    "cell",
    "okay",
    "sometimes",
    "gon",
    "na",
    "take",
    "prediction",
    "sometimes",
    "fifty",
    "percent",
    "cases",
    "gon",
    "na",
    "choose",
    "target",
    "translated",
    "word",
    "sort",
    "reason",
    "want",
    "talk",
    "teacher",
    "force",
    "ratio",
    "one",
    "correct",
    "target",
    "words",
    "time",
    "test",
    "time",
    "words",
    "predicts",
    "might",
    "completely",
    "different",
    "sees",
    "training",
    "time",
    "right",
    "talked",
    "long",
    "one",
    "anyways",
    "gon",
    "na",
    "batch",
    "size",
    "going",
    "sourced",
    "shape",
    "one",
    "yeah",
    "shape",
    "gon",
    "na",
    "send",
    "let",
    "see",
    "source",
    "think",
    "yes",
    "source",
    "gon",
    "na",
    "quote",
    "think",
    "gon",
    "na",
    "target",
    "link",
    "batch",
    "size",
    "remember",
    "correctly",
    "comma",
    "n",
    "yeah",
    "anyways",
    "batch",
    "size",
    "gon",
    "na",
    "second",
    "dimension",
    "source",
    "shape",
    "target",
    "length",
    "gon",
    "na",
    "target",
    "dot",
    "shape",
    "0",
    "unless",
    "target",
    "vocabulary",
    "size",
    "gon",
    "na",
    "length",
    "english",
    "vocabulary",
    "right",
    "created",
    "b",
    "let",
    "see",
    "let",
    "scroll",
    "created",
    "english",
    "vocabulary",
    "length",
    "english",
    "vocabulary",
    "let",
    "us",
    "start",
    "output",
    "well",
    "actually",
    "2",
    "remember",
    "gon",
    "na",
    "encoder",
    "decoder",
    "gon",
    "na",
    "run",
    "encoder",
    "get",
    "hidden",
    "cell",
    "gon",
    "na",
    "input",
    "decoder",
    "first",
    "thing",
    "gon",
    "na",
    "gon",
    "na",
    "kind",
    "build",
    "hidden",
    "cell",
    "state",
    "know",
    "hidden",
    "comma",
    "cell",
    "self",
    "encoder",
    "source",
    "cell",
    "phone",
    "right",
    "alright",
    "right",
    "back",
    "alright",
    "people",
    "know",
    "making",
    "videos",
    "anyway",
    "target",
    "vocabulary",
    "size",
    "hidden",
    "self",
    "dot",
    "encoder",
    "alright",
    "next",
    "step",
    "well",
    "first",
    "thing",
    "gon",
    "na",
    "gon",
    "na",
    "star",
    "token",
    "gon",
    "na",
    "gon",
    "na",
    "x",
    "equals",
    "target",
    "0",
    "grab",
    "start",
    "token",
    "gon",
    "na",
    "gon",
    "na",
    "send",
    "decoder",
    "right",
    "gon",
    "na",
    "word",
    "word",
    "give",
    "40",
    "range",
    "1",
    "comma",
    "target",
    "len",
    "target",
    "length",
    "gon",
    "na",
    "output",
    "call",
    "cell",
    "gon",
    "na",
    "call",
    "decoder",
    "safe",
    "decoder",
    "x",
    "gon",
    "na",
    "use",
    "hidden",
    "cell",
    "encoder",
    "gon",
    "na",
    "also",
    "input",
    "hidden",
    "cell",
    "output",
    "gon",
    "na",
    "next",
    "hidden",
    "cell",
    "going",
    "reused",
    "loop",
    "right",
    "gon",
    "na",
    "actually",
    "forgot",
    "one",
    "thing",
    "alright",
    "gon",
    "na",
    "also",
    "gon",
    "na",
    "add",
    "outputs",
    "torched",
    "zeroes",
    "target",
    "length",
    "batch",
    "size",
    "target",
    "vocabulary",
    "size",
    "going",
    "also",
    "dot",
    "two",
    "device",
    "essentially",
    "gon",
    "na",
    "predict",
    "one",
    "word",
    "time",
    "right",
    "word",
    "predict",
    "gon",
    "na",
    "entire",
    "batch",
    "every",
    "prediction",
    "gon",
    "na",
    "know",
    "sort",
    "vector",
    "entire",
    "vocabulary",
    "size",
    "one",
    "output",
    "right",
    "gon",
    "na",
    "batch",
    "size",
    "comma",
    "target",
    "vocabulary",
    "size",
    "going",
    "size",
    "output",
    "tensor",
    "going",
    "gon",
    "na",
    "sort",
    "put",
    "dimension",
    "right",
    "alright",
    "poor",
    "explanation",
    "gon",
    "na",
    "make",
    "sense",
    "gon",
    "na",
    "output",
    "gon",
    "na",
    "equal",
    "output",
    "right",
    "adding",
    "along",
    "dimension",
    "first",
    "dimension",
    "okay",
    "gon",
    "na",
    "gon",
    "na",
    "get",
    "best",
    "guess",
    "going",
    "output",
    "arg",
    "max",
    "one",
    "right",
    "since",
    "outputting",
    "second",
    "dimension",
    "right",
    "output",
    "wait",
    "mean",
    "run",
    "output",
    "n",
    "comma",
    "english",
    "vocabulary",
    "size",
    "batch",
    "take",
    "arg",
    "max",
    "dimension",
    "get",
    "index",
    "best",
    "guess",
    "decoder",
    "output",
    "right",
    "going",
    "going",
    "next",
    "input",
    "decoder",
    "going",
    "target",
    "random",
    "dot",
    "random",
    "less",
    "teacher",
    "force",
    "ratio",
    "else",
    "best",
    "guess",
    "okay",
    "gon",
    "na",
    "next",
    "input",
    "decoder",
    "target",
    "word",
    "random",
    "dot",
    "random",
    "zero",
    "one",
    "less",
    "teacher",
    "force",
    "ratio",
    "input",
    "model",
    "50",
    "times",
    "gon",
    "na",
    "get",
    "actual",
    "one",
    "rest",
    "gon",
    "na",
    "get",
    "guests",
    "decoder",
    "end",
    "loop",
    "gon",
    "na",
    "return",
    "outputs",
    "okay",
    "anything",
    "else",
    "think",
    "actually",
    "come",
    "point",
    "model",
    "done",
    "encoder",
    "decoder",
    "got",
    "ta",
    "set",
    "training",
    "phase",
    "right",
    "let",
    "ready",
    "training",
    "well",
    "alive",
    "need",
    "define",
    "everything",
    "first",
    "need",
    "training",
    "hyper",
    "parameters",
    "got",
    "ta",
    "motto",
    "yeah",
    "model",
    "high",
    "parameters",
    "guess",
    "gon",
    "na",
    "like",
    "number",
    "epochs",
    "let",
    "like",
    "20",
    "learning",
    "rate",
    "batch",
    "size",
    "let",
    "64",
    "gets",
    "pretty",
    "standard",
    "far",
    "could",
    "like",
    "load",
    "model",
    "equals",
    "false",
    "device",
    "torch",
    "device",
    "cuda",
    "torch",
    "cuda",
    "available",
    "else",
    "gon",
    "na",
    "use",
    "cpu",
    "yeah",
    "input",
    "size",
    "encoder",
    "gon",
    "na",
    "length",
    "german",
    "vocabulary",
    "input",
    "size",
    "decoder",
    "length",
    "english",
    "vocabulary",
    "output",
    "size",
    "gon",
    "na",
    "length",
    "english",
    "vocabulary",
    "well",
    "define",
    "encoder",
    "embedding",
    "size",
    "let",
    "say",
    "know",
    "300",
    "decoder",
    "embedding",
    "size",
    "300",
    "well",
    "think",
    "300",
    "might",
    "actually",
    "large",
    "dimension",
    "embedding",
    "size",
    "seen",
    "people",
    "use",
    "like",
    "hundred",
    "embedding",
    "size",
    "think",
    "smaller",
    "end",
    "guess",
    "somewhere",
    "around",
    "100",
    "300",
    "good",
    "number",
    "know",
    "gon",
    "na",
    "hidden",
    "size",
    "let",
    "thousand",
    "24",
    "think",
    "paper",
    "review",
    "sequence",
    "sequence",
    "though",
    "use",
    "hidden",
    "size",
    "thousand",
    "thousand",
    "something",
    "like",
    "think",
    "small",
    "today",
    "standard",
    "since",
    "paper",
    "2014",
    "know",
    "guess",
    "medium",
    "size",
    "today",
    "gon",
    "na",
    "number",
    "layers",
    "let",
    "two",
    "layers",
    "thinking",
    "paper",
    "let",
    "two",
    "take",
    "long",
    "let",
    "encoder",
    "drop",
    "decoder",
    "drop",
    "one",
    "thing",
    "let",
    "something",
    "tensor",
    "board",
    "going",
    "writer",
    "way",
    "check",
    "video",
    "tensor",
    "board",
    "unsure",
    "gon",
    "na",
    "summary",
    "writer",
    "runs",
    "loss",
    "plot",
    "gon",
    "na",
    "gon",
    "na",
    "essentially",
    "gon",
    "na",
    "get",
    "loss",
    "plot",
    "see",
    "sort",
    "loss",
    "decreasing",
    "also",
    "gon",
    "na",
    "step",
    "equals",
    "zero",
    "right",
    "right",
    "lot",
    "got",
    "also",
    "get",
    "iterator",
    "said",
    "beginning",
    "video",
    "also",
    "got",
    "use",
    "bucket",
    "iterator",
    "split",
    "data",
    "gon",
    "na",
    "training",
    "iterator",
    "validation",
    "iterator",
    "test",
    "iterator",
    "bucket",
    "iterator",
    "dot",
    "split",
    "gon",
    "na",
    "send",
    "train",
    "data",
    "validation",
    "data",
    "test",
    "data",
    "one",
    "note",
    "make",
    "sure",
    "match",
    "training",
    "reduced",
    "first",
    "train",
    "data",
    "validation",
    "iterator",
    "second",
    "validation",
    "data",
    "second",
    "okay",
    "gon",
    "na",
    "batch",
    "size",
    "going",
    "equal",
    "batch",
    "size",
    "guess",
    "one",
    "thing",
    "talk",
    "much",
    "torch",
    "text",
    "tutorials",
    "sort",
    "within",
    "batch",
    "gon",
    "na",
    "use",
    "since",
    "true",
    "let",
    "write",
    "line",
    "explain",
    "gon",
    "na",
    "short",
    "key",
    "lambda",
    "x",
    "gon",
    "na",
    "length",
    "x",
    "dot",
    "source",
    "alright",
    "sentences",
    "right",
    "vary",
    "length",
    "bucket",
    "area",
    "gon",
    "na",
    "sort",
    "within",
    "batch",
    "true",
    "sort",
    "key",
    "right",
    "gon",
    "na",
    "prioritize",
    "examples",
    "similar",
    "length",
    "batch",
    "right",
    "want",
    "want",
    "minimize",
    "number",
    "padding",
    "sort",
    "save",
    "compute",
    "right",
    "going",
    "device",
    "equals",
    "device",
    "right",
    "think",
    "going",
    "call",
    "going",
    "encoder",
    "net",
    "encoder",
    "gon",
    "na",
    "send",
    "arguments",
    "gon",
    "na",
    "send",
    "input",
    "size",
    "encoder",
    "encoder",
    "embedding",
    "also",
    "attending",
    "hidden",
    "sighs",
    "gon",
    "na",
    "sing",
    "number",
    "layers",
    "gon",
    "na",
    "send",
    "encoder",
    "dropout",
    "gon",
    "na",
    "make",
    "cuda",
    "available",
    "let",
    "copy",
    "gon",
    "na",
    "guess",
    "pretty",
    "much",
    "c",
    "decoder",
    "net",
    "gon",
    "na",
    "call",
    "decoder",
    "coda",
    "right",
    "also",
    "coder",
    "embedding",
    "sighs",
    "hidden",
    "sighs",
    "output",
    "size",
    "number",
    "layers",
    "decoder",
    "dropout",
    "dot",
    "two",
    "device",
    "right",
    "model",
    "gon",
    "na",
    "sequence",
    "sequence",
    "gon",
    "na",
    "send",
    "encoder",
    "decoder",
    "lastly",
    "gon",
    "na",
    "make",
    "device",
    "right",
    "almost",
    "promise",
    "got",
    "things",
    "gon",
    "na",
    "gon",
    "na",
    "long",
    "video",
    "think",
    "anyways",
    "going",
    "pad",
    "index",
    "english",
    "vocabulary",
    "dot",
    "string",
    "index",
    "pad",
    "end",
    "pad",
    "string",
    "gon",
    "na",
    "criterion",
    "gon",
    "na",
    "use",
    "cross",
    "entropy",
    "loss",
    "gon",
    "na",
    "input",
    "one",
    "keyword",
    "argument",
    "gon",
    "na",
    "use",
    "ignore",
    "index",
    "equal",
    "pad",
    "index",
    "padded",
    "cuz",
    "gon",
    "na",
    "examples",
    "batch",
    "saying",
    "similar",
    "lengths",
    "want",
    "pay",
    "anything",
    "cost",
    "function",
    "right",
    "gon",
    "na",
    "use",
    "load",
    "model",
    "gon",
    "na",
    "load",
    "checkpoint",
    "torch",
    "load",
    "checkpoint",
    "dot",
    "p",
    "th",
    "star",
    "model",
    "optimizer",
    "right",
    "think",
    "yeah",
    "think",
    "forgot",
    "let",
    "see",
    "got",
    "load",
    "well",
    "gon",
    "na",
    "utils",
    "separate",
    "file",
    "created",
    "brought",
    "guess",
    "nice",
    "functions",
    "like",
    "gon",
    "na",
    "translate",
    "sentence",
    "gon",
    "na",
    "calculate",
    "blue",
    "score",
    "gon",
    "na",
    "save",
    "checkpoint",
    "load",
    "checkpoint",
    "save",
    "model",
    "load",
    "model",
    "made",
    "entire",
    "separate",
    "video",
    "wan",
    "na",
    "check",
    "blue",
    "score",
    "translate",
    "sentence",
    "nice",
    "util",
    "functions",
    "gon",
    "na",
    "gon",
    "na",
    "use",
    "gon",
    "na",
    "probably",
    "talk",
    "show",
    "implementation",
    "separate",
    "video",
    "make",
    "like",
    "hour",
    "know",
    "long",
    "video",
    "gon",
    "na",
    "right",
    "gon",
    "na",
    "use",
    "false",
    "yeah",
    "beginnings",
    "model",
    "gon",
    "na",
    "epoch",
    "range",
    "non",
    "pox",
    "gon",
    "na",
    "print",
    "going",
    "park",
    "epoch",
    "number",
    "epochs",
    "like",
    "like",
    "going",
    "checkpoint",
    "state",
    "licked",
    "gon",
    "na",
    "model",
    "state",
    "dick",
    "optimizer",
    "standard",
    "code",
    "save",
    "model",
    "state",
    "ticked",
    "gon",
    "na",
    "parameters",
    "gon",
    "na",
    "call",
    "save",
    "checkpoint",
    "checkpoint",
    "alright",
    "going",
    "going",
    "batch",
    "sit",
    "patch",
    "fact",
    "comma",
    "batch",
    "enumerate",
    "train",
    "iterator",
    "gon",
    "na",
    "input",
    "data",
    "equal",
    "batch",
    "source",
    "gon",
    "na",
    "make",
    "cuda",
    "target",
    "gon",
    "na",
    "batch",
    "dot",
    "target",
    "dot",
    "2",
    "device",
    "one",
    "thing",
    "got",
    "got",
    "ta",
    "send",
    "model",
    "write",
    "input",
    "data",
    "target",
    "output",
    "right",
    "gon",
    "na",
    "output",
    "shape",
    "gon",
    "na",
    "target",
    "length",
    "gon",
    "na",
    "bash",
    "size",
    "output",
    "dimension",
    "okay",
    "problem",
    "loss",
    "wants",
    "sort",
    "matrix",
    "would",
    "tensor",
    "example",
    "let",
    "say",
    "would",
    "nice",
    "data",
    "set",
    "would",
    "n",
    "comma",
    "10",
    "output",
    "sort",
    "n",
    "examples",
    "time",
    "example",
    "outputs",
    "10",
    "know",
    "digit",
    "output",
    "know",
    "targets",
    "would",
    "would",
    "shape",
    "right",
    "got",
    "similar",
    "sort",
    "view",
    "batch",
    "right",
    "batch",
    "sort",
    "20",
    "outputs",
    "model",
    "well",
    "one",
    "thing",
    "concatenate",
    "yeah",
    "guess",
    "called",
    "catenate",
    "sort",
    "put",
    "dimensions",
    "together",
    "batch",
    "guess",
    "like",
    "target",
    "length",
    "amount",
    "output",
    "going",
    "sending",
    "loss",
    "yeah",
    "know",
    "confused",
    "right",
    "anyways",
    "gon",
    "na",
    "gon",
    "na",
    "output",
    "reshaping",
    "gon",
    "na",
    "upward",
    "gon",
    "na",
    "dot",
    "reshape",
    "gon",
    "na",
    "minus",
    "1",
    "output",
    "shape",
    "2",
    "right",
    "going",
    "keep",
    "output",
    "dimension",
    "would",
    "size",
    "vocabulary",
    "gon",
    "na",
    "sort",
    "put",
    "everything",
    "else",
    "together",
    "gon",
    "na",
    "word",
    "concatenate",
    "like",
    "anyway",
    "lost",
    "word",
    "gon",
    "na",
    "target",
    "target",
    "oh",
    "one",
    "thing",
    "actually",
    "gon",
    "na",
    "one",
    "see",
    "gon",
    "na",
    "one",
    "rest",
    "essentially",
    "first",
    "output",
    "right",
    "gon",
    "na",
    "star",
    "token",
    "want",
    "send",
    "model",
    "gon",
    "na",
    "remove",
    "gon",
    "na",
    "target",
    "gon",
    "na",
    "reshape",
    "one",
    "well",
    "one",
    "single",
    "dimension",
    "would",
    "take",
    "target",
    "length",
    "times",
    "batch",
    "size",
    "would",
    "shape",
    "target",
    "optimizer",
    "dot",
    "0",
    "grad",
    "loss",
    "going",
    "criterion",
    "output",
    "target",
    "law",
    "start",
    "backward",
    "also",
    "another",
    "thing",
    "sort",
    "avoid",
    "gradients",
    "become",
    "large",
    "avoid",
    "exploding",
    "gradient",
    "problems",
    "torch",
    "tailstock",
    "clip",
    "grad",
    "norm",
    "model",
    "dot",
    "parameters",
    "max",
    "norm",
    "equals",
    "1",
    "sort",
    "make",
    "sure",
    "gradients",
    "healthy",
    "range",
    "guess",
    "optimizer",
    "dot",
    "step",
    "yeah",
    "promise",
    "almost",
    "done",
    "like",
    "two",
    "lines",
    "gon",
    "na",
    "brighter",
    "dot",
    "add",
    "scalar",
    "training",
    "loss",
    "loss",
    "global",
    "step",
    "equal",
    "step",
    "gon",
    "na",
    "step",
    "plus",
    "equals",
    "one",
    "right",
    "believe",
    "created",
    "entire",
    "thing",
    "basically",
    "created",
    "google",
    "translate",
    "gon",
    "na",
    "gon",
    "na",
    "run",
    "twenty",
    "parks",
    "gon",
    "na",
    "take",
    "best",
    "part",
    "know",
    "created",
    "everything",
    "time",
    "relax",
    "back",
    "drink",
    "coffee",
    "good",
    "time",
    "gon",
    "na",
    "press",
    "button",
    "whoa",
    "okay",
    "thought",
    "gon",
    "na",
    "good",
    "time",
    "tokenized",
    "jerry",
    "find",
    "like",
    "beginning",
    "see",
    "tokenized",
    "dirt",
    "right",
    "tokenizer",
    "alright",
    "tokenizer",
    "air",
    "happy",
    "right",
    "seems",
    "work",
    "alright",
    "see",
    "guys",
    "bit",
    "one",
    "hour",
    "right",
    "back",
    "okay",
    "get",
    "one",
    "error",
    "valid",
    "ish",
    "gon",
    "na",
    "validation",
    "know",
    "value",
    "data",
    "right",
    "let",
    "rerun",
    "believe",
    "got",
    "one",
    "error",
    "let",
    "see",
    "get",
    "got",
    "optimizer",
    "well",
    "gon",
    "na",
    "optimizer",
    "equals",
    "adam",
    "model",
    "dot",
    "parameters",
    "learning",
    "rate",
    "learning",
    "rate",
    "hopefully",
    "last",
    "one",
    "unless",
    "run",
    "right",
    "training",
    "seems",
    "work",
    "got",
    "one",
    "idea",
    "would",
    "like",
    "took",
    "one",
    "sentence",
    "see",
    "right",
    "sentence",
    "took",
    "quite",
    "long",
    "sentence",
    "right",
    "let",
    "keep",
    "one",
    "line",
    "could",
    "like",
    "yeah",
    "keep",
    "one",
    "line",
    "anyways",
    "long",
    "sentence",
    "know",
    "something",
    "like",
    "know",
    "german",
    "like",
    "boat",
    "pulled",
    "large",
    "team",
    "horses",
    "something",
    "like",
    "training",
    "data",
    "thought",
    "would",
    "interesting",
    "like",
    "see",
    "translated",
    "sentence",
    "would",
    "improve",
    "epochs",
    "go",
    "thinking",
    "could",
    "like",
    "model",
    "dot",
    "evaluate",
    "translate",
    "sentence",
    "used",
    "created",
    "gon",
    "na",
    "show",
    "one",
    "probably",
    "next",
    "video",
    "anyways",
    "send",
    "translate",
    "sentence",
    "model",
    "sentence",
    "got",
    "ta",
    "send",
    "fields",
    "well",
    "device",
    "maximum",
    "length",
    "prediction",
    "purely",
    "gon",
    "na",
    "sending",
    "string",
    "gon",
    "na",
    "get",
    "model",
    "predict",
    "gon",
    "na",
    "sort",
    "like",
    "print",
    "translated",
    "example",
    "sentence",
    "new",
    "line",
    "translated",
    "sentence",
    "mala",
    "train",
    "alright",
    "model",
    "eval",
    "gon",
    "na",
    "guess",
    "turn",
    "dropout",
    "case",
    "yeah",
    "thought",
    "would",
    "interesting",
    "let",
    "run",
    "let",
    "rain",
    "alright",
    "got",
    "ta",
    "show",
    "sentence",
    "actually",
    "saying",
    "center",
    "took",
    "train",
    "data",
    "said",
    "men",
    "pulled",
    "shore",
    "large",
    "team",
    "horses",
    "okay",
    "trained",
    "model",
    "gon",
    "na",
    "bring",
    "yeah",
    "trained",
    "couple",
    "occasions",
    "printed",
    "wrote",
    "nope",
    "add",
    "plus",
    "plus",
    "file",
    "anyways",
    "first",
    "epoch",
    "said",
    "actually",
    "another",
    "epoch",
    "forgot",
    "copy",
    "essentially",
    "like",
    "random",
    "words",
    "guess",
    "also",
    "random",
    "words",
    "random",
    "words",
    "end",
    "sentence",
    "token",
    "yeah",
    "guess",
    "second",
    "epoch",
    "technically",
    "like",
    "young",
    "player",
    "aaa",
    "see",
    "lot",
    "words",
    "since",
    "similar",
    "data",
    "uncommon",
    "see",
    "like",
    "aaa",
    "model",
    "predict",
    "alright",
    "let",
    "let",
    "see",
    "next",
    "epoch",
    "runner",
    "background",
    "alright",
    "really",
    "make",
    "sense",
    "like",
    "nothing",
    "boats",
    "anything",
    "like",
    "least",
    "sort",
    "something",
    "makes",
    "guess",
    "sort",
    "sense",
    "baseball",
    "player",
    "guess",
    "make",
    "sense",
    "football",
    "player",
    "ready",
    "aaa",
    "okay",
    "comes",
    "men",
    "pulled",
    "large",
    "building",
    "mean",
    "got",
    "something",
    "right",
    "one",
    "boat",
    "full",
    "pulled",
    "large",
    "sort",
    "see",
    "know",
    "model",
    "actually",
    "progressing",
    "something",
    "makes",
    "sense",
    "right",
    "hey",
    "pulled",
    "large",
    "large",
    "large",
    "large",
    "large",
    "sentence",
    "pulled",
    "large",
    "building",
    "building",
    "pulled",
    "large",
    "pulled",
    "large",
    "okay",
    "two",
    "men",
    "pulled",
    "large",
    "large",
    "okay",
    "passengers",
    "pulled",
    "large",
    "ford",
    "large",
    "okay",
    "mean",
    "interesting",
    "pulled",
    "something",
    "guess",
    "something",
    "boat",
    "two",
    "men",
    "pulled",
    "large",
    "cable",
    "sure",
    "arrived",
    "one",
    "anyways",
    "know",
    "sort",
    "see",
    "let",
    "see",
    "last",
    "one",
    "boat",
    "boat",
    "several",
    "men",
    "pulled",
    "pulled",
    "large",
    "cable",
    "right",
    "sort",
    "getting",
    "right",
    "copied",
    "like",
    "trained",
    "like",
    "50",
    "bucks",
    "copy",
    "see",
    "least",
    "see",
    "rerun",
    "saved",
    "parameters",
    "boat",
    "carrying",
    "several",
    "men",
    "pulled",
    "shore",
    "large",
    "team",
    "horses",
    "alright",
    "might",
    "saying",
    "like",
    "well",
    "data",
    "training",
    "data",
    "course",
    "going",
    "learn",
    "sentences",
    "right",
    "prove",
    "anything",
    "well",
    "gon",
    "na",
    "prove",
    "hopefully",
    "gon",
    "na",
    "prove",
    "wrong",
    "gon",
    "na",
    "something",
    "like",
    "gon",
    "na",
    "use",
    "google",
    "translate",
    "gon",
    "na",
    "write",
    "english",
    "sentence",
    "translated",
    "german",
    "see",
    "get",
    "english",
    "sentence",
    "back",
    "one",
    "thing",
    "keep",
    "mind",
    "like",
    "sentence",
    "like",
    "description",
    "something",
    "like",
    "tried",
    "first",
    "like",
    "writing",
    "like",
    "really",
    "love",
    "peanuts",
    "get",
    "like",
    "sentences",
    "written",
    "sort",
    "guess",
    "uh",
    "called",
    "form",
    "like",
    "something",
    "like",
    "like",
    "description",
    "like",
    "person",
    "walked",
    "yada",
    "yada",
    "yada",
    "people",
    "walking",
    "walk",
    "store",
    "buy",
    "cold",
    "ice",
    "cream",
    "sort",
    "something",
    "like",
    "copy",
    "go",
    "back",
    "right",
    "scent",
    "going",
    "gon",
    "na",
    "copy",
    "never",
    "done",
    "let",
    "see",
    "makes",
    "sense",
    "either",
    "right",
    "makes",
    "sense",
    "created",
    "google",
    "translate",
    "people",
    "going",
    "cross",
    "getting",
    "ready",
    "end",
    "sentence",
    "right",
    "gon",
    "na",
    "defend",
    "right",
    "two",
    "people",
    "exactly",
    "wrote",
    "going",
    "somewhere",
    "getting",
    "ready",
    "least",
    "like",
    "getting",
    "things",
    "right",
    "train",
    "longer",
    "time",
    "would",
    "make",
    "perfect",
    "sense",
    "probably",
    "one",
    "thing",
    "could",
    "also",
    "paper",
    "read",
    "calculate",
    "blue",
    "score",
    "gon",
    "na",
    "copy",
    "probably",
    "end",
    "training",
    "like",
    "gon",
    "na",
    "gon",
    "na",
    "see",
    "like",
    "gon",
    "na",
    "see",
    "kind",
    "blue",
    "score",
    "get",
    "test",
    "data",
    "seen",
    "right",
    "yeah",
    "gon",
    "na",
    "run",
    "gon",
    "na",
    "take",
    "get",
    "back",
    "done",
    "something",
    "wrong",
    "kernel",
    "restart",
    "running",
    "get",
    "blue",
    "score",
    "21",
    "know",
    "bad",
    "right",
    "good",
    "either",
    "terrible",
    "know",
    "need",
    "know",
    "creating",
    "google",
    "translate",
    "yet",
    "right",
    "work",
    "awesome",
    "got",
    "stuff",
    "learn",
    "basically",
    "guess",
    "next",
    "video",
    "gon",
    "na",
    "review",
    "sequence",
    "sequence",
    "attention",
    "ai",
    "idea",
    "like",
    "see",
    "like",
    "oh",
    "course",
    "every",
    "idea",
    "guess",
    "easy",
    "simple",
    "retrospect",
    "next",
    "video",
    "gon",
    "na",
    "sequence",
    "sequence",
    "attention",
    "probably",
    "review",
    "paper",
    "gon",
    "na",
    "implementation",
    "see",
    "much",
    "improve",
    "blue",
    "score",
    "yeah",
    "start",
    "nice",
    "series",
    "guess",
    "hopefully",
    "enjoyed",
    "video",
    "questions",
    "leave",
    "comment",
    "section",
    "think",
    "said",
    "thank",
    "watching",
    "yeah",
    "thank",
    "watching",
    "hope",
    "see",
    "next",
    "video",
    "music"
  ],
  "keywords": [
    "going",
    "gon",
    "na",
    "basically",
    "translate",
    "well",
    "sort",
    "sequence",
    "model",
    "data",
    "set",
    "german",
    "english",
    "let",
    "paper",
    "video",
    "one",
    "second",
    "right",
    "two",
    "called",
    "encoder",
    "take",
    "sentence",
    "call",
    "vector",
    "input",
    "decoder",
    "essentially",
    "token",
    "start",
    "words",
    "translated",
    "cell",
    "output",
    "alright",
    "get",
    "copy",
    "know",
    "torch",
    "text",
    "said",
    "iterator",
    "tokenizer",
    "tensor",
    "loss",
    "anyways",
    "first",
    "dot",
    "load",
    "thing",
    "define",
    "return",
    "see",
    "something",
    "like",
    "also",
    "equals",
    "make",
    "end",
    "need",
    "previous",
    "want",
    "guess",
    "use",
    "train",
    "validation",
    "got",
    "ta",
    "okay",
    "training",
    "vocabulary",
    "size",
    "word",
    "entire",
    "add",
    "equal",
    "yeah",
    "later",
    "actually",
    "anything",
    "else",
    "time",
    "step",
    "self",
    "embedding",
    "hidden",
    "number",
    "layers",
    "dropout",
    "drop",
    "send",
    "n",
    "run",
    "comma",
    "x",
    "long",
    "index",
    "shape",
    "length",
    "batch",
    "outputs",
    "shapes",
    "say",
    "dimension",
    "300",
    "think",
    "keep",
    "predict",
    "good",
    "prediction",
    "1",
    "ready",
    "next",
    "target",
    "sense",
    "source",
    "would",
    "probably",
    "created",
    "back",
    "device",
    "random",
    "parameters",
    "large",
    "checkpoint",
    "optimizer",
    "blue",
    "score",
    "epoch",
    "boat",
    "pulled",
    "men"
  ]
}