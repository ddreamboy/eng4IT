{
  "text": "uh hi guys my name is mohammed\nkhalid and the topic of my presentation\nis\nintroduction to semi supervised learning\nso i'm going to give you guys\na 50 000 feet view of what semi\nsupervised learning\nactually is so let's get into the\npresentation\nso what i aim to do in this presentation\nis tell you guys what semi supervised\nlearning is\nhow it is different from supervised and\nunsupervised learning methods\nwhich are conventional machine learning\nmethods right\nwhy do we use semi-supervised learning\nlike we have two perfectly good methods\nright\nwhy would you why would we fall to\nsemi-supervised learning right\nwhat are some of the types of semi\nsupervised learning there are\na lot of types of semi supervised\nlearning rights there are a lot there\nare even a lot of sub types of semi\nsupervised learning\nbut i would just touch on a very few of\nthem because of the time restriction\n25 minutes is not a lot of time to\nexplain the math of semi-supervised\nsupport vector machines and\nsemi-supervised graphs\nso i'll just get into the theory and\ntheory of most of them\nand just touch upon the graphs a little\nbit touch upon the statistics of them a\nlittle bit okay\nand at the end we will just discuss a\nfew problems that\nactually use semi-supervised learning\nokay so\num let's start with the presentation\nso um firstly let's have an overview\nlet's have a revision of what's\nsupervised and unsupervised learning are\nso\nin supervised learning we have\nuh label data like we have some\ndata we have some sort of data and we\nhave a label against it right so if uh\nif i\nfor instance let's say i have a data\nimages as you can see\nin this example right so if i have a\ndata of images\ni will have a label against each of that\nimage if i have this image\ni'll say that yep this is the dog if i\nhave this image i'll say yep this\nis a cat so that so when we train our\nmodel using\nlabel data and then we can make uh\npredictions on some future data that was\nnot a part of our test data\nthat is not necessary but for the sake\nof understanding\nthat is called as supervised learning\nright so we\nwe uh train a function\nf of x using our training data x in our\nlabels uh\nwe train a function f of x that predicts\nthe label\ny on some future data x right so that is\nthe gist of supervised learning\nand for unsupervised learning it's the\nopposite for example if\nif we consider this data right we have\njust four images\nand we have no way to differentiate them\nlike we have no\nlabels that we can say are against these\nimages to differentiate them\nfrom each other right so unsupervised\nlearning in\nfor example if i feed this to an\nunsupervised learning algorithm\ni will ask ask this uh ask the method to\nmake\nclusters of my data and it will divide\nthe data into clusters based on the\ninformation of each image\nright it will say yep there is a lot of\nblack and white in this image\nin these images so i'll put them in one\ncluster and there's a lot of brown black\nand white in these images\nso it'll just put them in another\ncluster right so when we're classifying\ndata without any labels that is called\nunsupervised learning okay\nso moving on to our topic what semi\nsupervised learning is\nso uh as we just talked about supervised\nand unsupervised learning that\nit uses both label data and unlabeled\ndata\nuh it uses a sorry supervised learning\nuses level data and unsupervised\nuh learning uses unlabeled data semi\nsupervised learning uses both\nlike for instance our data set would be\nsomething like\nthis right so it will have\nsome information about the weight of the\nanimal\nand it won't have uh information on some\nother\nimages like some images will have labels\nattached to them and some will not\nright and we will train our model based\non that data set\nright so that is called semi supervised\nlearning right\nso uh what i would like to do it i would\nlike you guys to imagine\num a spectrum right so imagine\nthis is a spectrum in front of you okay\nand\nthe these are points on that spectrum on\nthe very far left\nwe have supervised learning and on the\nvery far right we have unsupervised\nlearning right\nand this area is semi supervised\nlearning okay and\nlet's consider this the very center just\nfor the sake of understanding consider\nthis\nan equator of the spectrum okay so\nthis area as you can see is very close\nto supervised learning\nand this area is very close to\nunsupervised learning so that means\nthere are\nbranches of semi-supervised learnings\nsome of them stem from supervised\nlearning\nand some of them stem from unsupervised\nlearning okay\nand the type of the type that usually\nstems from supervised learning we call\nit semi-supervised classification\nor induction is something we use and\nsome and what stems from unsupervised\nlearning it is called constrained\nclustering or transaction uh\ntransduction sorry as we're gonna\nlearn later on in our presentation okay\nso before\nuh getting into what induction is what\ntransduction is\nuh one question that arises is what is\nthe need of semi supervised learning\nright we have perfect we have perfectly\ngood machine learning methods to\nuh to do everything like we have um\na supervised learning in case we have\nlabels we can just label our data and\nyou know\nfeed it in the machine learning\nalgorithm right why won't it work\nlike that is what we usually do right\nwhat is stopping us from doing that\nso i'll give you guys an example of a\ndata\nthat i have personally used right so\nthis is from a spacex data set like\nthese images these are from\na space x data set right and the\nresolution\nof the image that this sub image is from\nwas 5000 into 5000 right\nthat means it had 25\nmillion pixels in one image okay and it\nthe data set had\nthousands and thousands of image right\nand\nwhen i wanted to semantically segment\nthe image that means i had to assign a\nlabel\nto each and every one of those pixels\nit wasn't possible for me or it wouldn't\nwouldn't have been possible for\nanyone to create a label against every\nsingle one of the\nimage so for reference let's consider\nthis right\nso if we're talking about our images um\nthis is the total number of images in\none pixels that is 25 million if i'm not\nwrong\nright now consider i have a relatively\nsmall data set right consider i have\nonly\num 1500 images okay so this\nis 3 billion and 750\nmillion i think right that is the total\nnumber of pixels\nthat you would have to manually label in\norder to get a label data set right\nand that is not a very realistic thing\nto expect right that is not something\nthat i would think anyone can anyone\nwould like to do or it would be\nfeasible to do right so in those\nscenarios we work with\nboth labeled and unlabeled data and then\nwe mold it according to our needs right\nso that is why we use semi supervised\nlearning the primary reason of using\nsemi supervised learning right\nso um let's talk about induction and\ntransduction\nokay so when we're talking about\ninduction let's first discuss induction\nright\ninduction is the part of uh semi\nsupervised\nsemi supervised learning it is that\nportion that stems\nfrom supervised learning right so what\nwe do in induction is that we use our\nmodel\nuh the model that we have created right\nit then\nwe generate this model\nthen using this model we can predict on\nnewer images we can predict our newer\ndata like\nfor instance if i have used if i go back\nto this example\nif i used these four images for training\nthen i can use any fifth images for\nprediction\nlike if my f x was trained using these\nfour images then the x i i will i am\ngoing to use to predict\nthe label y that is going to be from a\nfifth image right\nthat is known as induction okay\nso and transduction is its counterpart\ntransaction is what stems from semi\nsupervised learning\ni'm sorry transduction is what uh\noriginates from\nunsupervised learning okay so in\ntransduction what we have\nis what we what we do is\nwe make predictions on our training data\nset so\nuh what we have for instance let's say i\nhave five\nmillion images okay and 1.2 million\nof those are labeled okay this is just\nan example\nso what i do is i run the model on the\n1.2 million\nand after that i make predictions on the\nrest of the 3.8 million of our images\nokay so now i have labels\nagainst five million of those images\nright and now\nin the model that i've trained what i'm\ngonna do\nis i'm gonna take an image out of the\nset of the five million images that i\nhad right i will take\none one image or one the point from the\ndata\nand predict on that point like uh my\ntest\ndata is gonna come from the data that i\nused for training right\nthat is called as transduction okay so\nthat is the difference between induction\nand\ntransduction okay another thing that uh\nsome other differences between induction\nand transduction or inductive learning\nversus\ntransductive learning would be that um\nin inductive law inductive learning we\nwe test the\ntrain the model with vmware right so we\ncan test the model\non data that we have never seen like uh\ngoing back to the example in the first\nslide\nif we had four images we we trained our\nneural network or trained our machine\nlearning algorithm\nusing four images then we may test on a\nfifth image right\nsame goes for uh the uh opposite goes\nfor transductive learning\nthat we will make a prediction on one of\nthe\nuh one of the points that is already\npresent in our database right\nso inductive learning we build a\npredictive model and transductive\nlearning does not make a predictive\nmodel\nthat means that we can we can use uh\noutsider data in inductive learning but\nwe cannot do so introspective learning\num inductive learning can predict any\npoint like\nit doesn't matter if it's in the space\nor not it can\num we can just feed it to the model that\nwe have trained\nand it will give us a prediction whereas\nin translative learning it can only\nuh predict points that we have actually\nused in the training algorithm right\nso inductive learning has less\ncomputational cost as you can guess that\nif i have trained the model once i will\nnot have to train it again but\num so we can make predictions on\nthe model that was trained once but\ntransductive learning can become costly\nwhy because let's say i have a data set\nof 600 images okay and i trained my\nmodel on that\nlike i had 250 label data\nokay and i had uh 350\nuh 250 and 350 or 600 yen so i had 350\nunlabeled data right\nso what i'm going to do is i'm going to\nfirstly train the model\nthis on the 350 250 images then i'm\ngoing to\nmake a prediction on these 350 images\nthen i'm going to clean the model on\nthose 600\nlabeled images and then i'm going to\nmake a prediction on one of those images\nright\nbut let's say i get 10 or 20 more images\nthen i'll have to do this process again\nlike i'll have to first train the data\non those 20 images as well\nand then i'm going to make a prediction\nso that can become\nvery computationally intensive right\nso going into a bit more detail about\nthe types of\nuh models that are\nused in semi-supervised learning one of\nthem is\nself training models right so okay\none thing that i think we just skipped\nis\nwhile we're using semi supervised\nlearning we have to make some assumption\nright so these without these assumptions\nsemi-supervised learning is\nnot possible so without going into too\nmuch detail of\nwhat these mean the gist of the main\nidea of these assumptions\nis the data that the data that we have\nis similar\nright the data that we uh if we are\ngiven\na cluster or something like that\nsimilar like data that is closer to each\nother will be similar\nthat is the gist of the assumptions that\nwe have to make\nwhen we're using semi supervised\nlearning okay so\nwhatever algorithms we're gonna discuss\nwhatever methodologies we're gonna\ndiscuss\nthis is the main idea that we're gonna\nkeep in mind\nokay so moving on to self training\nmodels\num so we're using the same assumption\nthat each cluster that we're going to\nget will have\nonly similar data right and what we're\ngonna have is\na very trace amount of label data and a\nhuge amount of unlabeled data right a\nvery\ntrace amount of label data and a very\nhuge amount of unlabeled data right\nso i'm gonna show this to you guys i'm\ngonna explain it to you guys better with\nan example\nright consider that you have to identify\nthe gender of a hundred people\nright so you were given just\nthe two uh genders of two people\nand you have to find out the gender of\n98 people\nright so what are you gonna do you're\ngonna apply one nearest neighbor okay\nso i know that this one point that i was\ngiven\nis a female this one point that i was\ngiven is a male\nright this is what i know now the model\nand now i'm gonna apply one nearest\nneighbor to this\nand the model is gonna train itself\nokay so what does it do in every\ninstance it\nlook at its most nearest neighbor and it\nwill assign it the same label\nas it has like\nlike it's its own label to its nearest\npaper right now\nif our assumption was correct that the\ndata that is closer\nuh like uh the data that is closer on\nthe plane\nis uh gonna be similar like it's gonna\nbelong to the same class right\nso if we just like having a very of\nhaving just a look on this data we can\nsee that this data is very close to each\nother\nthis data is very close to each other\nthis is very close to each other and\nthis is very close to each other\nright so and if we had to divide it\nin if we decrease the number of cluster\nthis is going to be one cluster and this\nis going to be\nanother cluster right so assuming that\nthis\nthese were all the females and this was\nall the males\nor this uh or vice versa this means that\nour did that our prediction was totally\ncorrect when we use self-training models\nright but if\nif our assumption was correct let's\nassume that our assumption was incorrect\nand there was one outlier somewhere\nuh let me just change the color of the\npen there was one outlier\nsomewhere here and one outlier somewhere\nhere\nright so that means that anything beyond\nthese points upward is going to be\npredicted\nincorrectly right so that is one\nuh limitation of cell training models\nthat we cannot have outliers in it\nif we have outliers that is just gonna\nmess things up for us\nokay so another thing\nanother very interesting method that we\nuse in semi supervised learning is\nco-training\nso co-training it uses the same\nassumption\nthat um that our data is going to be\nindependent\nand it will um similar\nsimilar data as we're not using graphs\nwe're talking we're going to primarily\ntalk about\ntext mining and everything so i don't\nthink that it'll matter for us that the\ndata is\nin similar clusters or not but another\nassumption that we're going to make is\nthe data is going to be conditionally\nindependent given the class label\nokay so\nthat two views are conditioned uh\nconditionally independent given the\nclass therefore right\nso this is the formula for conditional\nindependence we did that in the very\nfirst\num lecture that our math review was so\ni'm just not\ni'm not gonna go into the details okay\nso let's\nuh consider an example of code training\nokay so um let's just consider\nthat we have\ntwo point like we have a huge data set\nright\nand we have just two of these points\nthat are labeled\nright so this is consider this a text\ncorpus right\nso what it says is we will have will\ndivide the data set based on commas or\nsomething\nand our x1 uh and x2 are obviously\nindependent\nso what we're gonna do is we're gonna\nconsider washington state\nis headquartered in locate that tells us\nthat it is a location\nright so that makes our prediction for\nnumber three pretty easy\nlike it's it sees this headquartered and\nit sees this headquartered in\nit sees that the sentence structure is\nthe same so it can pretty easily\nuh guess that yep this is a location\nright so that is uh\nhow this is going to go right now if we\nmove on to example four\nso it says kazakhstan flute\nokay like it the sentence is gonna be\nlike it flew to kazakhstan right now our\noriginal\nlabeled data set it neither did have\nhad flew to nor did it have kazakhstan\nright but we just made a prediction that\nkazakhstan is a location\nso using that it will say that yep it is\na location as well\nokay so then our last one as\nmr smith is a partner right now the\nthe three that we just discussed does\nnot have do not have anything\nthat is uh related to that right but if\nwe look at example number two what\nmr washington and it sees mr smith\nright so it can make and make a\nprediction based on that\nthat this is a person right\nso that is easy for it okay now let's\nconsider that we have an example that is\nuh someone\nflew to china right if we consider if we\ncome to this text here\nnow then what happens now if we look at\nour original\ndata set or even if we look at our\npredicted data\nsorry if if you look at our label data\nset\nwe don't have flu to anywhere we don't\nhave china anywhere right\nbut on if we see line number four\nwe see instance number four we actually\num inferred that as flew to\nas with kazakhstan and kazakhstan was a\nlocation\nso china is also going to be a location\nright\nso that is the concept of core training\nright okay so uh\nthat was co-training now i'm just gonna\ntouch on graph page semi-supervised\nlearning uh because\nuh as i said at the very beginning of\nthe presentation that\ngraph based supervised learning and um\nthis svm based semi supervised learning\nlight\nthe semi supervised support vector\nmachines as we call them\nthey involve a lot of maths and\nexplaining that would have not have been\nfeasible in the 25 minute presentation\nso i'm just gonna go to a very basic\nuh basic overview of what graph base any\nsupervised learning is okay\nso what we do is we construct a graph\nfrom our training data right and as i\nas i said at the very start of the\npresentation that um\nthe gist of semi supervised learning is\ngoing to be the same thing\nthat edges uh that sorry\nthe some of the data will be labeled and\nsome of that\nwill be unlabeled right so that is what\nwe're going to assume here as well\nthat some of the edges of the graph like\nat the very beginning\nthey were labeled right and using those\nlabels\nwe will connect the edges that do not\nhave labels\nright so um let's consider\nan example for this i will just uh\nshow you an example and come back to\nthis slide\nif we see this slide right so let's\nconsider that there was this little girl\nright and she was reading a book\nthat had labels in it okay\nand some some of those labels like\nsome of those uh\nafter a few articles it stopped having\nlabels\nright and it was very hard for the\nlittle girl\nwho was reading the book to understand\nwhat\nuh like if she saw this topic comet\nlight curve she had\nshe had no idea what it belonged to\nright so\nmaking the assumption that the book had\nonly\nuh the book had topics about only two\nthings\nlike it was either sky or earth and she\nshe read only these two labels\nlike bright sky asteroid\nso she read bright asteroid this had a\nlabel and she read airport bike rental\nthis had the label right so and\nnone of these things had a label right\nor what should what did she do\nokay she made an assumption that if\nthis word has asteroid and it belongs to\nthe sky then\nevery other graph that we're gonna see\nthat will have uh that will have a word\nuh that was con in that topic or in that\nheading\nit will uh be related to sky right so\nasteroid and comet\nif we go here then this has comet it is\nalready like connect pretty connected\nand same boat goes for the ground right\nthat if it is\num this had the word airport in it so it\nbelonged to the ground like that\nthat much she knew so she said that yep\nthis has airport so this is on the\nground this has denali and this also is\ndenali\nso just like that you can label the\nwhole graph based on some assumptions\nthat you made at the very start okay so\num that is the gist of graph based\nsupervised learning now getting into the\nformula of graph based supervised\nlearning\nis we have a fully connected graph right\nwhere x i\nand x j are the vertices of the graph\nright so\nevery single word is every single pair\nof vertices has to be connected by an\nedge right and the weight of\neach edge decreases depending on\nhow far away we are from our original\nweight\nright well like that we can use any\nweight function right\none of the weight functions that we can\nuse as a\nis explained by this w i j is equals to\nexponent of x i minus x j mod square\nover two\nuh bandwidth square right where uh this\nis a bandwidth parameter and controls\nhow quickly the weight decreases okay so\nthis is just one\none way one way of deciding how our\nhow we can use our weight function how\nour edge weight\nincreases or decreases based on how far\nwe are from our original data\nright so uh ending\non the note uh we'll just talk about\nwhere ssl\nis being used so speech analysis we just\nlooked at an example as well that um\nthat washington and headquarters and\neverything it is not possible\nto like predict or\nprovide labels to each and every single\nword that a person is speaking\nor a person is writing right so we use\nsemi supervised learning we use some of\nthe data is labeled\nand some of it is not another thing that\nwe use\nas ssl and as internet content\nclassification\nof course there is huge amounts of data\non the internet right\nso we can uh even google uses a semi\nsupervised training\nto classify its website it is not\npossible for it to classify every single\nsite differently right\nthere is humongous amounts of data on\nthe internet so that is also a place\nwhere\nuh ssl is used protein sequence\nclassification\nlike we know that one a single fiber of\nprotein\nhas like thousands and thousands of\njoins and\neverything so classifying every\nevery single protein sequence\nindividually\nthat is not going to be possible for us\nright so we use protein we use ssl here\nas well\nright and some other types that we\ndidn't discuss\nwere semi-supervised\nbased svm like semi supervised support\nvector machines\nmen card which is a graph based semi\nsupervised learning method and harmonic\nfunction which is also a graph based\nmethod right\nso if any of you want to discuss with\nthem with me i have a paper on that i\nactually read it but i didn't think i\nhad the time and\nalso representation time is finished as\nwell these are the references that i use\nand with that i will continue my\npresentation\nand your questions are welcome thank you\nguys\n",
  "words": [
    "uh",
    "hi",
    "guys",
    "name",
    "mohammed",
    "khalid",
    "topic",
    "presentation",
    "introduction",
    "semi",
    "supervised",
    "learning",
    "going",
    "give",
    "guys",
    "50",
    "000",
    "feet",
    "view",
    "semi",
    "supervised",
    "learning",
    "actually",
    "let",
    "get",
    "presentation",
    "aim",
    "presentation",
    "tell",
    "guys",
    "semi",
    "supervised",
    "learning",
    "different",
    "supervised",
    "unsupervised",
    "learning",
    "methods",
    "conventional",
    "machine",
    "learning",
    "methods",
    "right",
    "use",
    "learning",
    "like",
    "two",
    "perfectly",
    "good",
    "methods",
    "right",
    "would",
    "would",
    "fall",
    "learning",
    "right",
    "types",
    "semi",
    "supervised",
    "learning",
    "lot",
    "types",
    "semi",
    "supervised",
    "learning",
    "rights",
    "lot",
    "even",
    "lot",
    "sub",
    "types",
    "semi",
    "supervised",
    "learning",
    "would",
    "touch",
    "time",
    "restriction",
    "25",
    "minutes",
    "lot",
    "time",
    "explain",
    "math",
    "support",
    "vector",
    "machines",
    "graphs",
    "get",
    "theory",
    "theory",
    "touch",
    "upon",
    "graphs",
    "little",
    "bit",
    "touch",
    "upon",
    "statistics",
    "little",
    "bit",
    "okay",
    "end",
    "discuss",
    "problems",
    "actually",
    "use",
    "learning",
    "okay",
    "um",
    "let",
    "start",
    "presentation",
    "um",
    "firstly",
    "let",
    "overview",
    "let",
    "revision",
    "supervised",
    "unsupervised",
    "learning",
    "supervised",
    "learning",
    "uh",
    "label",
    "data",
    "like",
    "data",
    "sort",
    "data",
    "label",
    "right",
    "uh",
    "instance",
    "let",
    "say",
    "data",
    "images",
    "see",
    "example",
    "right",
    "data",
    "images",
    "label",
    "image",
    "image",
    "say",
    "yep",
    "dog",
    "image",
    "say",
    "yep",
    "cat",
    "train",
    "model",
    "using",
    "label",
    "data",
    "make",
    "uh",
    "predictions",
    "future",
    "data",
    "part",
    "test",
    "data",
    "necessary",
    "sake",
    "understanding",
    "called",
    "supervised",
    "learning",
    "right",
    "uh",
    "train",
    "function",
    "f",
    "x",
    "using",
    "training",
    "data",
    "x",
    "labels",
    "uh",
    "train",
    "function",
    "f",
    "x",
    "predicts",
    "label",
    "future",
    "data",
    "x",
    "right",
    "gist",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "opposite",
    "example",
    "consider",
    "data",
    "right",
    "four",
    "images",
    "way",
    "differentiate",
    "like",
    "labels",
    "say",
    "images",
    "differentiate",
    "right",
    "unsupervised",
    "learning",
    "example",
    "feed",
    "unsupervised",
    "learning",
    "algorithm",
    "ask",
    "ask",
    "uh",
    "ask",
    "method",
    "make",
    "clusters",
    "data",
    "divide",
    "data",
    "clusters",
    "based",
    "information",
    "image",
    "right",
    "say",
    "yep",
    "lot",
    "black",
    "white",
    "image",
    "images",
    "put",
    "one",
    "cluster",
    "lot",
    "brown",
    "black",
    "white",
    "images",
    "put",
    "another",
    "cluster",
    "right",
    "classifying",
    "data",
    "without",
    "labels",
    "called",
    "unsupervised",
    "learning",
    "okay",
    "moving",
    "topic",
    "semi",
    "supervised",
    "learning",
    "uh",
    "talked",
    "supervised",
    "unsupervised",
    "learning",
    "uses",
    "label",
    "data",
    "unlabeled",
    "data",
    "uh",
    "uses",
    "sorry",
    "supervised",
    "learning",
    "uses",
    "level",
    "data",
    "unsupervised",
    "uh",
    "learning",
    "uses",
    "unlabeled",
    "data",
    "semi",
    "supervised",
    "learning",
    "uses",
    "like",
    "instance",
    "data",
    "set",
    "would",
    "something",
    "like",
    "right",
    "information",
    "weight",
    "animal",
    "wo",
    "uh",
    "information",
    "images",
    "like",
    "images",
    "labels",
    "attached",
    "right",
    "train",
    "model",
    "based",
    "data",
    "set",
    "right",
    "called",
    "semi",
    "supervised",
    "learning",
    "right",
    "uh",
    "would",
    "like",
    "would",
    "like",
    "guys",
    "imagine",
    "um",
    "spectrum",
    "right",
    "imagine",
    "spectrum",
    "front",
    "okay",
    "points",
    "spectrum",
    "far",
    "left",
    "supervised",
    "learning",
    "far",
    "right",
    "unsupervised",
    "learning",
    "right",
    "area",
    "semi",
    "supervised",
    "learning",
    "okay",
    "let",
    "consider",
    "center",
    "sake",
    "understanding",
    "consider",
    "equator",
    "spectrum",
    "okay",
    "area",
    "see",
    "close",
    "supervised",
    "learning",
    "area",
    "close",
    "unsupervised",
    "learning",
    "means",
    "branches",
    "learnings",
    "stem",
    "supervised",
    "learning",
    "stem",
    "unsupervised",
    "learning",
    "okay",
    "type",
    "type",
    "usually",
    "stems",
    "supervised",
    "learning",
    "call",
    "classification",
    "induction",
    "something",
    "use",
    "stems",
    "unsupervised",
    "learning",
    "called",
    "constrained",
    "clustering",
    "transaction",
    "uh",
    "transduction",
    "sorry",
    "gon",
    "na",
    "learn",
    "later",
    "presentation",
    "okay",
    "uh",
    "getting",
    "induction",
    "transduction",
    "uh",
    "one",
    "question",
    "arises",
    "need",
    "semi",
    "supervised",
    "learning",
    "right",
    "perfect",
    "perfectly",
    "good",
    "machine",
    "learning",
    "methods",
    "uh",
    "everything",
    "like",
    "um",
    "supervised",
    "learning",
    "case",
    "labels",
    "label",
    "data",
    "know",
    "feed",
    "machine",
    "learning",
    "algorithm",
    "right",
    "wo",
    "work",
    "like",
    "usually",
    "right",
    "stopping",
    "us",
    "give",
    "guys",
    "example",
    "data",
    "personally",
    "used",
    "right",
    "spacex",
    "data",
    "set",
    "like",
    "images",
    "space",
    "x",
    "data",
    "set",
    "right",
    "resolution",
    "image",
    "sub",
    "image",
    "5000",
    "5000",
    "right",
    "means",
    "25",
    "million",
    "pixels",
    "one",
    "image",
    "okay",
    "data",
    "set",
    "thousands",
    "thousands",
    "image",
    "right",
    "wanted",
    "semantically",
    "segment",
    "image",
    "means",
    "assign",
    "label",
    "every",
    "one",
    "pixels",
    "possible",
    "would",
    "would",
    "possible",
    "anyone",
    "create",
    "label",
    "every",
    "single",
    "one",
    "image",
    "reference",
    "let",
    "consider",
    "right",
    "talking",
    "images",
    "um",
    "total",
    "number",
    "images",
    "one",
    "pixels",
    "25",
    "million",
    "wrong",
    "right",
    "consider",
    "relatively",
    "small",
    "data",
    "set",
    "right",
    "consider",
    "um",
    "1500",
    "images",
    "okay",
    "3",
    "billion",
    "750",
    "million",
    "think",
    "right",
    "total",
    "number",
    "pixels",
    "would",
    "manually",
    "label",
    "order",
    "get",
    "label",
    "data",
    "set",
    "right",
    "realistic",
    "thing",
    "expect",
    "right",
    "something",
    "would",
    "think",
    "anyone",
    "anyone",
    "would",
    "like",
    "would",
    "feasible",
    "right",
    "scenarios",
    "work",
    "labeled",
    "unlabeled",
    "data",
    "mold",
    "according",
    "needs",
    "right",
    "use",
    "semi",
    "supervised",
    "learning",
    "primary",
    "reason",
    "using",
    "semi",
    "supervised",
    "learning",
    "right",
    "um",
    "let",
    "talk",
    "induction",
    "transduction",
    "okay",
    "talking",
    "induction",
    "let",
    "first",
    "discuss",
    "induction",
    "right",
    "induction",
    "part",
    "uh",
    "semi",
    "supervised",
    "semi",
    "supervised",
    "learning",
    "portion",
    "stems",
    "supervised",
    "learning",
    "right",
    "induction",
    "use",
    "model",
    "uh",
    "model",
    "created",
    "right",
    "generate",
    "model",
    "using",
    "model",
    "predict",
    "newer",
    "images",
    "predict",
    "newer",
    "data",
    "like",
    "instance",
    "used",
    "go",
    "back",
    "example",
    "used",
    "four",
    "images",
    "training",
    "use",
    "fifth",
    "images",
    "prediction",
    "like",
    "f",
    "x",
    "trained",
    "using",
    "four",
    "images",
    "x",
    "going",
    "use",
    "predict",
    "label",
    "going",
    "fifth",
    "image",
    "right",
    "known",
    "induction",
    "okay",
    "transduction",
    "counterpart",
    "transaction",
    "stems",
    "semi",
    "supervised",
    "learning",
    "sorry",
    "transduction",
    "uh",
    "originates",
    "unsupervised",
    "learning",
    "okay",
    "transduction",
    "make",
    "predictions",
    "training",
    "data",
    "set",
    "uh",
    "instance",
    "let",
    "say",
    "five",
    "million",
    "images",
    "okay",
    "million",
    "labeled",
    "okay",
    "example",
    "run",
    "model",
    "million",
    "make",
    "predictions",
    "rest",
    "million",
    "images",
    "okay",
    "labels",
    "five",
    "million",
    "images",
    "right",
    "model",
    "trained",
    "gon",
    "na",
    "gon",
    "na",
    "take",
    "image",
    "set",
    "five",
    "million",
    "images",
    "right",
    "take",
    "one",
    "one",
    "image",
    "one",
    "point",
    "data",
    "predict",
    "point",
    "like",
    "uh",
    "test",
    "data",
    "gon",
    "na",
    "come",
    "data",
    "used",
    "training",
    "right",
    "called",
    "transduction",
    "okay",
    "difference",
    "induction",
    "transduction",
    "okay",
    "another",
    "thing",
    "uh",
    "differences",
    "induction",
    "transduction",
    "inductive",
    "learning",
    "versus",
    "transductive",
    "learning",
    "would",
    "um",
    "inductive",
    "law",
    "inductive",
    "learning",
    "test",
    "train",
    "model",
    "vmware",
    "right",
    "test",
    "model",
    "data",
    "never",
    "seen",
    "like",
    "uh",
    "going",
    "back",
    "example",
    "first",
    "slide",
    "four",
    "images",
    "trained",
    "neural",
    "network",
    "trained",
    "machine",
    "learning",
    "algorithm",
    "using",
    "four",
    "images",
    "may",
    "test",
    "fifth",
    "image",
    "right",
    "goes",
    "uh",
    "uh",
    "opposite",
    "goes",
    "transductive",
    "learning",
    "make",
    "prediction",
    "one",
    "uh",
    "one",
    "points",
    "already",
    "present",
    "database",
    "right",
    "inductive",
    "learning",
    "build",
    "predictive",
    "model",
    "transductive",
    "learning",
    "make",
    "predictive",
    "model",
    "means",
    "use",
    "uh",
    "outsider",
    "data",
    "inductive",
    "learning",
    "introspective",
    "learning",
    "um",
    "inductive",
    "learning",
    "predict",
    "point",
    "like",
    "matter",
    "space",
    "um",
    "feed",
    "model",
    "trained",
    "give",
    "us",
    "prediction",
    "whereas",
    "translative",
    "learning",
    "uh",
    "predict",
    "points",
    "actually",
    "used",
    "training",
    "algorithm",
    "right",
    "inductive",
    "learning",
    "less",
    "computational",
    "cost",
    "guess",
    "trained",
    "model",
    "train",
    "um",
    "make",
    "predictions",
    "model",
    "trained",
    "transductive",
    "learning",
    "become",
    "costly",
    "let",
    "say",
    "data",
    "set",
    "600",
    "images",
    "okay",
    "trained",
    "model",
    "like",
    "250",
    "label",
    "data",
    "okay",
    "uh",
    "350",
    "uh",
    "250",
    "350",
    "600",
    "yen",
    "350",
    "unlabeled",
    "data",
    "right",
    "going",
    "going",
    "firstly",
    "train",
    "model",
    "350",
    "250",
    "images",
    "going",
    "make",
    "prediction",
    "350",
    "images",
    "going",
    "clean",
    "model",
    "600",
    "labeled",
    "images",
    "going",
    "make",
    "prediction",
    "one",
    "images",
    "right",
    "let",
    "say",
    "get",
    "10",
    "20",
    "images",
    "process",
    "like",
    "first",
    "train",
    "data",
    "20",
    "images",
    "well",
    "going",
    "make",
    "prediction",
    "become",
    "computationally",
    "intensive",
    "right",
    "going",
    "bit",
    "detail",
    "types",
    "uh",
    "models",
    "used",
    "learning",
    "one",
    "self",
    "training",
    "models",
    "right",
    "okay",
    "one",
    "thing",
    "think",
    "skipped",
    "using",
    "semi",
    "supervised",
    "learning",
    "make",
    "assumption",
    "right",
    "without",
    "assumptions",
    "learning",
    "possible",
    "without",
    "going",
    "much",
    "detail",
    "mean",
    "gist",
    "main",
    "idea",
    "assumptions",
    "data",
    "data",
    "similar",
    "right",
    "data",
    "uh",
    "given",
    "cluster",
    "something",
    "like",
    "similar",
    "like",
    "data",
    "closer",
    "similar",
    "gist",
    "assumptions",
    "make",
    "using",
    "semi",
    "supervised",
    "learning",
    "okay",
    "whatever",
    "algorithms",
    "gon",
    "na",
    "discuss",
    "whatever",
    "methodologies",
    "gon",
    "na",
    "discuss",
    "main",
    "idea",
    "gon",
    "na",
    "keep",
    "mind",
    "okay",
    "moving",
    "self",
    "training",
    "models",
    "um",
    "using",
    "assumption",
    "cluster",
    "going",
    "get",
    "similar",
    "data",
    "right",
    "gon",
    "na",
    "trace",
    "amount",
    "label",
    "data",
    "huge",
    "amount",
    "unlabeled",
    "data",
    "right",
    "trace",
    "amount",
    "label",
    "data",
    "huge",
    "amount",
    "unlabeled",
    "data",
    "right",
    "gon",
    "na",
    "show",
    "guys",
    "gon",
    "na",
    "explain",
    "guys",
    "better",
    "example",
    "right",
    "consider",
    "identify",
    "gender",
    "hundred",
    "people",
    "right",
    "given",
    "two",
    "uh",
    "genders",
    "two",
    "people",
    "find",
    "gender",
    "98",
    "people",
    "right",
    "gon",
    "na",
    "gon",
    "na",
    "apply",
    "one",
    "nearest",
    "neighbor",
    "okay",
    "know",
    "one",
    "point",
    "given",
    "female",
    "one",
    "point",
    "given",
    "male",
    "right",
    "know",
    "model",
    "gon",
    "na",
    "apply",
    "one",
    "nearest",
    "neighbor",
    "model",
    "gon",
    "na",
    "train",
    "okay",
    "every",
    "instance",
    "look",
    "nearest",
    "neighbor",
    "assign",
    "label",
    "like",
    "like",
    "label",
    "nearest",
    "paper",
    "right",
    "assumption",
    "correct",
    "data",
    "closer",
    "uh",
    "like",
    "uh",
    "data",
    "closer",
    "plane",
    "uh",
    "gon",
    "na",
    "similar",
    "like",
    "gon",
    "na",
    "belong",
    "class",
    "right",
    "like",
    "look",
    "data",
    "see",
    "data",
    "close",
    "data",
    "close",
    "close",
    "close",
    "right",
    "divide",
    "decrease",
    "number",
    "cluster",
    "going",
    "one",
    "cluster",
    "going",
    "another",
    "cluster",
    "right",
    "assuming",
    "females",
    "males",
    "uh",
    "vice",
    "versa",
    "means",
    "prediction",
    "totally",
    "correct",
    "use",
    "models",
    "right",
    "assumption",
    "correct",
    "let",
    "assume",
    "assumption",
    "incorrect",
    "one",
    "outlier",
    "somewhere",
    "uh",
    "let",
    "change",
    "color",
    "pen",
    "one",
    "outlier",
    "somewhere",
    "one",
    "outlier",
    "somewhere",
    "right",
    "means",
    "anything",
    "beyond",
    "points",
    "upward",
    "going",
    "predicted",
    "incorrectly",
    "right",
    "one",
    "uh",
    "limitation",
    "cell",
    "training",
    "models",
    "outliers",
    "outliers",
    "gon",
    "na",
    "mess",
    "things",
    "us",
    "okay",
    "another",
    "thing",
    "another",
    "interesting",
    "method",
    "use",
    "semi",
    "supervised",
    "learning",
    "uses",
    "assumption",
    "um",
    "data",
    "going",
    "independent",
    "um",
    "similar",
    "similar",
    "data",
    "using",
    "graphs",
    "talking",
    "going",
    "primarily",
    "talk",
    "text",
    "mining",
    "everything",
    "think",
    "matter",
    "us",
    "data",
    "similar",
    "clusters",
    "another",
    "assumption",
    "going",
    "make",
    "data",
    "going",
    "conditionally",
    "independent",
    "given",
    "class",
    "label",
    "okay",
    "two",
    "views",
    "conditioned",
    "uh",
    "conditionally",
    "independent",
    "given",
    "class",
    "therefore",
    "right",
    "formula",
    "conditional",
    "independence",
    "first",
    "um",
    "lecture",
    "math",
    "review",
    "gon",
    "na",
    "go",
    "details",
    "okay",
    "let",
    "uh",
    "consider",
    "example",
    "code",
    "training",
    "okay",
    "um",
    "let",
    "consider",
    "two",
    "point",
    "like",
    "huge",
    "data",
    "set",
    "right",
    "two",
    "points",
    "labeled",
    "right",
    "consider",
    "text",
    "corpus",
    "right",
    "says",
    "divide",
    "data",
    "set",
    "based",
    "commas",
    "something",
    "x1",
    "uh",
    "x2",
    "obviously",
    "independent",
    "gon",
    "na",
    "gon",
    "na",
    "consider",
    "washington",
    "state",
    "headquartered",
    "locate",
    "tells",
    "us",
    "location",
    "right",
    "makes",
    "prediction",
    "number",
    "three",
    "pretty",
    "easy",
    "like",
    "sees",
    "headquartered",
    "sees",
    "headquartered",
    "sees",
    "sentence",
    "structure",
    "pretty",
    "easily",
    "uh",
    "guess",
    "yep",
    "location",
    "right",
    "uh",
    "going",
    "go",
    "right",
    "move",
    "example",
    "four",
    "says",
    "kazakhstan",
    "flute",
    "okay",
    "like",
    "sentence",
    "gon",
    "na",
    "like",
    "flew",
    "kazakhstan",
    "right",
    "original",
    "labeled",
    "data",
    "set",
    "neither",
    "flew",
    "kazakhstan",
    "right",
    "made",
    "prediction",
    "kazakhstan",
    "location",
    "using",
    "say",
    "yep",
    "location",
    "well",
    "okay",
    "last",
    "one",
    "mr",
    "smith",
    "partner",
    "right",
    "three",
    "discussed",
    "anything",
    "uh",
    "related",
    "right",
    "look",
    "example",
    "number",
    "two",
    "mr",
    "washington",
    "sees",
    "mr",
    "smith",
    "right",
    "make",
    "make",
    "prediction",
    "based",
    "person",
    "right",
    "easy",
    "okay",
    "let",
    "consider",
    "example",
    "uh",
    "someone",
    "flew",
    "china",
    "right",
    "consider",
    "come",
    "text",
    "happens",
    "look",
    "original",
    "data",
    "set",
    "even",
    "look",
    "predicted",
    "data",
    "sorry",
    "look",
    "label",
    "data",
    "set",
    "flu",
    "anywhere",
    "china",
    "anywhere",
    "right",
    "see",
    "line",
    "number",
    "four",
    "see",
    "instance",
    "number",
    "four",
    "actually",
    "um",
    "inferred",
    "flew",
    "kazakhstan",
    "kazakhstan",
    "location",
    "china",
    "also",
    "going",
    "location",
    "right",
    "concept",
    "core",
    "training",
    "right",
    "okay",
    "uh",
    "gon",
    "na",
    "touch",
    "graph",
    "page",
    "learning",
    "uh",
    "uh",
    "said",
    "beginning",
    "presentation",
    "graph",
    "based",
    "supervised",
    "learning",
    "um",
    "svm",
    "based",
    "semi",
    "supervised",
    "learning",
    "light",
    "semi",
    "supervised",
    "support",
    "vector",
    "machines",
    "call",
    "involve",
    "lot",
    "maths",
    "explaining",
    "would",
    "feasible",
    "25",
    "minute",
    "presentation",
    "gon",
    "na",
    "go",
    "basic",
    "uh",
    "basic",
    "overview",
    "graph",
    "base",
    "supervised",
    "learning",
    "okay",
    "construct",
    "graph",
    "training",
    "data",
    "right",
    "said",
    "start",
    "presentation",
    "um",
    "gist",
    "semi",
    "supervised",
    "learning",
    "going",
    "thing",
    "edges",
    "uh",
    "sorry",
    "data",
    "labeled",
    "unlabeled",
    "right",
    "going",
    "assume",
    "well",
    "edges",
    "graph",
    "like",
    "beginning",
    "labeled",
    "right",
    "using",
    "labels",
    "connect",
    "edges",
    "labels",
    "right",
    "um",
    "let",
    "consider",
    "example",
    "uh",
    "show",
    "example",
    "come",
    "back",
    "slide",
    "see",
    "slide",
    "right",
    "let",
    "consider",
    "little",
    "girl",
    "right",
    "reading",
    "book",
    "labels",
    "okay",
    "labels",
    "like",
    "uh",
    "articles",
    "stopped",
    "labels",
    "right",
    "hard",
    "little",
    "girl",
    "reading",
    "book",
    "understand",
    "uh",
    "like",
    "saw",
    "topic",
    "comet",
    "light",
    "curve",
    "idea",
    "belonged",
    "right",
    "making",
    "assumption",
    "book",
    "uh",
    "book",
    "topics",
    "two",
    "things",
    "like",
    "either",
    "sky",
    "earth",
    "read",
    "two",
    "labels",
    "like",
    "bright",
    "sky",
    "asteroid",
    "read",
    "bright",
    "asteroid",
    "label",
    "read",
    "airport",
    "bike",
    "rental",
    "label",
    "right",
    "none",
    "things",
    "label",
    "right",
    "okay",
    "made",
    "assumption",
    "word",
    "asteroid",
    "belongs",
    "sky",
    "every",
    "graph",
    "gon",
    "na",
    "see",
    "uh",
    "word",
    "uh",
    "con",
    "topic",
    "heading",
    "uh",
    "related",
    "sky",
    "right",
    "asteroid",
    "comet",
    "go",
    "comet",
    "already",
    "like",
    "connect",
    "pretty",
    "connected",
    "boat",
    "goes",
    "ground",
    "right",
    "um",
    "word",
    "airport",
    "belonged",
    "ground",
    "like",
    "much",
    "knew",
    "said",
    "yep",
    "airport",
    "ground",
    "denali",
    "also",
    "denali",
    "like",
    "label",
    "whole",
    "graph",
    "based",
    "assumptions",
    "made",
    "start",
    "okay",
    "um",
    "gist",
    "graph",
    "based",
    "supervised",
    "learning",
    "getting",
    "formula",
    "graph",
    "based",
    "supervised",
    "learning",
    "fully",
    "connected",
    "graph",
    "right",
    "x",
    "x",
    "j",
    "vertices",
    "graph",
    "right",
    "every",
    "single",
    "word",
    "every",
    "single",
    "pair",
    "vertices",
    "connected",
    "edge",
    "right",
    "weight",
    "edge",
    "decreases",
    "depending",
    "far",
    "away",
    "original",
    "weight",
    "right",
    "well",
    "like",
    "use",
    "weight",
    "function",
    "right",
    "one",
    "weight",
    "functions",
    "use",
    "explained",
    "w",
    "j",
    "equals",
    "exponent",
    "x",
    "minus",
    "x",
    "j",
    "mod",
    "square",
    "two",
    "uh",
    "bandwidth",
    "square",
    "right",
    "uh",
    "bandwidth",
    "parameter",
    "controls",
    "quickly",
    "weight",
    "decreases",
    "okay",
    "one",
    "one",
    "way",
    "one",
    "way",
    "deciding",
    "use",
    "weight",
    "function",
    "edge",
    "weight",
    "increases",
    "decreases",
    "based",
    "far",
    "original",
    "data",
    "right",
    "uh",
    "ending",
    "note",
    "uh",
    "talk",
    "ssl",
    "used",
    "speech",
    "analysis",
    "looked",
    "example",
    "well",
    "um",
    "washington",
    "headquarters",
    "everything",
    "possible",
    "like",
    "predict",
    "provide",
    "labels",
    "every",
    "single",
    "word",
    "person",
    "speaking",
    "person",
    "writing",
    "right",
    "use",
    "semi",
    "supervised",
    "learning",
    "use",
    "data",
    "labeled",
    "another",
    "thing",
    "use",
    "ssl",
    "internet",
    "content",
    "classification",
    "course",
    "huge",
    "amounts",
    "data",
    "internet",
    "right",
    "uh",
    "even",
    "google",
    "uses",
    "semi",
    "supervised",
    "training",
    "classify",
    "website",
    "possible",
    "classify",
    "every",
    "single",
    "site",
    "differently",
    "right",
    "humongous",
    "amounts",
    "data",
    "internet",
    "also",
    "place",
    "uh",
    "ssl",
    "used",
    "protein",
    "sequence",
    "classification",
    "like",
    "know",
    "one",
    "single",
    "fiber",
    "protein",
    "like",
    "thousands",
    "thousands",
    "joins",
    "everything",
    "classifying",
    "every",
    "every",
    "single",
    "protein",
    "sequence",
    "individually",
    "going",
    "possible",
    "us",
    "right",
    "use",
    "protein",
    "use",
    "ssl",
    "well",
    "right",
    "types",
    "discuss",
    "based",
    "svm",
    "like",
    "semi",
    "supervised",
    "support",
    "vector",
    "machines",
    "men",
    "card",
    "graph",
    "based",
    "semi",
    "supervised",
    "learning",
    "method",
    "harmonic",
    "function",
    "also",
    "graph",
    "based",
    "method",
    "right",
    "want",
    "discuss",
    "paper",
    "actually",
    "read",
    "think",
    "time",
    "also",
    "representation",
    "time",
    "finished",
    "well",
    "references",
    "use",
    "continue",
    "presentation",
    "questions",
    "welcome",
    "thank",
    "guys"
  ],
  "keywords": [
    "uh",
    "guys",
    "topic",
    "presentation",
    "semi",
    "supervised",
    "learning",
    "going",
    "actually",
    "let",
    "get",
    "unsupervised",
    "methods",
    "machine",
    "right",
    "use",
    "like",
    "two",
    "would",
    "types",
    "lot",
    "touch",
    "time",
    "25",
    "little",
    "okay",
    "discuss",
    "um",
    "label",
    "data",
    "instance",
    "say",
    "images",
    "see",
    "example",
    "image",
    "yep",
    "train",
    "model",
    "using",
    "make",
    "predictions",
    "test",
    "called",
    "function",
    "x",
    "training",
    "labels",
    "gist",
    "consider",
    "four",
    "algorithm",
    "method",
    "based",
    "one",
    "cluster",
    "another",
    "uses",
    "unlabeled",
    "sorry",
    "set",
    "something",
    "weight",
    "spectrum",
    "points",
    "far",
    "close",
    "means",
    "stems",
    "induction",
    "transduction",
    "gon",
    "na",
    "everything",
    "know",
    "us",
    "used",
    "million",
    "pixels",
    "thousands",
    "every",
    "possible",
    "single",
    "number",
    "think",
    "thing",
    "labeled",
    "first",
    "predict",
    "go",
    "prediction",
    "trained",
    "point",
    "inductive",
    "transductive",
    "350",
    "well",
    "models",
    "assumption",
    "assumptions",
    "similar",
    "given",
    "amount",
    "huge",
    "nearest",
    "look",
    "independent",
    "location",
    "sees",
    "kazakhstan",
    "flew",
    "original",
    "also",
    "graph",
    "book",
    "sky",
    "read",
    "asteroid",
    "word",
    "ssl",
    "protein"
  ]
}