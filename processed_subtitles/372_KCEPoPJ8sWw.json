{
  "text": "big data is a term i'm sure you're all\nfamiliar with\nbig data as a technology has grown\nmassively over a decade and a half when\ninternet users boomed and companies\nstarted generating vast amounts of data\nit became more popular with the advent\nof ai machine learning mobile technology\nand the internet of things\nbig data analytics helps companies from\ndifferent sectors such as automobile\nmanufacturing e-commerce and logistics\nto manage processes streamline use data\nsets in real time as well as improve\ntheir organization's decision-making\ncapability\nbig data and analytics can enable\norganizations to get a better\nunderstanding of their customers and\nhelp to narrow down their targeted\naudience\nthus helping to improve companies\nmarketing campaigns\nnow there is no doubt that the big data\nmarket is steadily growing and as per\nfortune business insights\nthe global big data market is expected\nto grow from 231.43\nbillion dollars\nin 2021 to\n549.73 billion us dollars in 2028 at a\ncagr of 13.2 percent\nas per monster.com annual trends report\nbig data analytics is most likely going\nto be the top in demand skill for 2022.\nat least 96 percent of companies are\ndefinitely planning or likely to plan to\nhire new staff with relevant skills\nto fill future big data analytics\nrelated roles in 2022\nthis indicates that the career\nopportunities and big data are very high\nand the scope is really good\nnow if you are looking for a career in\nbig data in 2022 on the screen you can\nsee some of the top big data companies\nfor 2022. so we have oracle\necommerce leader amazon hpe\ntech giant ibm and salesforce\nnow\nlet's look at the agenda for today's\nvideo on big data full course 2022 so we\nwill start off by learning how to become\na big data engineer and then we will see\nthe crucial skills\nfor big data\nnext we will understand big data\nanalytics and look at the top\napplications in big data\nafter that\nwe are going to learn about\nbig data tutorial and understand the\nmost popular big data framework that is\nhadoop\nnext we are going to look at the\ndifferent tools that are part of the\nhadoop ecosystem\nthen we are going to learn about apache\nspark and understand the spark\narchitecture\nfinally we are going to close our\nfull course video session by learning\nthe top hadoop interview questions so\nlet's begin\ntoday i'm going to tell you how you can\nbecome a big data engineer now it should\ncome as no surprise for you guys to know\nthat organizations generate as well as\nuse a whole lot of data this vast volume\nof data is called big data companies use\nbig data to draw meaningful insights and\ntake business decisions and big data\nengineers are the people who can make\nsense out of this enormous amount of\ndata now let's find out who is a big\ndata engineer a big data engineer is a\nprofessional who develops maintains\ntests and evaluates the company's big\ndata infrastructure in other words\ndevelop big data solutions based on a\ncompany's requirements they maintain\nthese solutions they test out these\nsolutions as to the company's\nrequirements they integrate this\nsolution with the various tools and\nsystems of the organization and finally\nthey evaluate how well the solution is\nworking to fulfill the company's\nrequirements next up let's have a look\nat the responsibilities of a big data\nengineer now they need to be able to\ndesign implement verify and maintain\nsoftware systems now for the process of\ningesting data as well as processing it\nthey need to be able to build highly\nscalable as well as robust systems they\nneed to be able to extract data from one\ndatabase transform it as well as load it\ninto another data store with the process\nof etl or the extract transform load\nprocess and they need to research as\nwell as propose new ways to acquire data\nimprove the overall data quality and the\nefficiency of the system now to ensure\nthat all the business requirements are\nmet they need to build a suitable data\narchitecture they need to be able to\nintegrate several programming languages\nand tools together so that they can\ngenerate a structured solution they need\nto build models that reduce the overall\ncomplexity and increase the efficiency\nof the whole system by mining data from\nvarious sources and finally they need to\nwork well with other teams ones that\ninclude data architects data analysts\nand data scientists next up let's have a\nlook at the skills required to become a\nbig data engineer the first step is to\nhave programming knowledge one of the\nmost important skills required to become\na big data engineer is that of\nexperience with programming languages\nespecially hands-on experience now the\nbig data solutions that organizations\nwould want you to create will not be\npossible without experience in\nprogramming languages i can even tell\nyou an easy way through which you can\nget experience with programming\nlanguages practice practice and more\npractice some of the most commonly used\nprogramming languages used for big data\nengineering are python java and c plus\nthe second skill that you require is to\nhave in-depth knowledge about dbms and\nsql you need to know how data is\nmaintained as well as managed in a\ndatabase you need to know how sql can be\nused to transform as well as perform\nactions on a database and by extension\nknow how to write sql queries for any\nrelational database management systems\nsome of the commonly used database\nmanagement systems for big data\nengineering mysql oracle database and\nthe microsoft sql server the third skill\nthat you require is to have experience\nworking with etl and warehousing tools\nnow you need to know how to construct as\nwell as use a data warehouse so that you\ncan perform the etl operation or the\nextract transform and load operations\nnow as a big data engineer you'll be\nconstantly tasked with extracting\nunstructured data from a number of\ndifferent sources transforming it into\nmeaningful information and loading it\ninto other data storages databases or\ndata warehouses now what this is is\nbasically aggregating unstructured data\nfrom multiple sources analyzing it so\nthat you can take better business\ndecisions some of the tools used for\nthis purpose are talent ibm data stage\npentaho and informatica next up we have\nthe fourth skill that you require which\nis knowledge about operating systems now\nsince most big data tools have unique\ndemands such as root access to operating\nsystem functionality as well as hardware\nhaving a strong understanding of\noperating systems like linux and unix is\nabsolutely mandatory some of the\noperating systems used by big data\nengineers are unix linux and solaris now\nthe fifth skill that you require to have\nexperience with hadoop based analytics\nsince hadoop is one of the most commonly\nused tools when it comes to big data\nengineering it's understood that you\nneed to have experience with apache\nhadoop based technologies technologies\nlike hdfs hadoop mapreduce apache edge\nbase hive and pig the sixth skill that\nyou require is to have worked with\nreal-time processing frameworks like\napache spark now as a big data engineer\nyou'll have to deal with vast volumes of\ndata so for this data you need an\nanalytics engine like spark which can be\nused for large-scale real-time data\nprocessing now spark can process live\nstreaming data from a number of\ndifferent sources like facebook\ninstagram twitter and so on it can also\nperform interactive analysis and data\nintegration and now we're at our final\nskill requirement which is to have\nexperience with data mining and modeling\nso as a data engineer you'll have to\nexamine massive pre-existing data so\nthat you can discover patterns as well\nas new information with this you will\ncreate predictive models for your\nbusiness so that you can make better\ninformed decisions some of the tools\nused for this are r rapidminer weka and\nnow let's talk about a big data\nengineer's salary as well as other roles\nthey can look forward to now the average\nsalary of a big data engineer in the\nunited states is approximately ninety\nthousand dollars per year now this\nranges from sixty six thousand dollars\nall the way to hundred and thirty\nthousand dollars per annum in india the\naverage salary is around seven lakh\nrupees and ranges from four lakhs to 13\nlakhs per annum after you become a big\ndata engineer some of the job roles that\nyou can look forward to are that of a\nsenior big data engineer a business\nintelligence architect and a data\narchitect now let's talk about\ncertifications that a big data engineer\ncan opt for first off we have the\ncloudera ccp data engineer a cloudera\ncertified professional data engineer\npossesses the skills to develop reliable\nand scalable data pipelines that result\nin optimized data sets for a variety of\nworkloads it is one of the industry's\nmost demanding performance based\ncertification ccp evaluates and\nrecognizes a candidate's mastery of the\ntechnical skills most sought after by\nemployers the time limit for this exam\nis 240 minutes and it costs 400\nnext we have the ibm certified data\narchitect big data certification an ibm\ncertified big data architect understands\nthe complexity of data and can design\nsystems and models to handle different\ndata variety including structured\nsemi-structured unstructured volume\nvelocity veracity and so on a big data\narchitect is also able to effectively\naddress information governance and\nsecurity challenges associated with the\nsystem this exam is 75 minutes long and\nfinally we have the google cloud\ncertified data engineer a google\ncertified data engineer enables data\ndriven decision making by collecting\ntransforming and publishing data they\nshould also be able to leverage deploy\nand continuously train pre-existing\nmachine learning models the length of\nthe certification exam is 2 hours and\nits registration fee is 200 now let's\nhave a look at how simply learn can help\nyou become a big data engineer\nsimply learn provides the big data\narchitect masters program this includes\na number of different courses like big\ndata hadoop and spark developer apache\nspark and scala mongodb developer and\nadministrator big data and hadoop\nadministrator and so much more this\ncourse goes through 50 plus in-demand\nskills and tools 12 plus real life\nprojects and the possibility of an\nannual average salary of 19 to 26 lakh\nrupees per annum it will also help you\nget noticed by the top hiring companies\nthis course will also go through some\nmajor tools like kafka apache spark\nflume edge base mongodb hive pig\nmapreduce java scala and much more now\nwhy don't you head over to\nsimplylearn.com and get started on your\njourney to get certified and get ahead\nwe all use smartphones but have you ever\nwondered how much data it generates in\nthe form of texts phone calls emails\nphotos\nvideos searches and music approximately\n40 exabytes of data gets generated every\nmonth by a single smartphone user\nnow imagine this number multiplied by 5\nbillion smartphone users that's a lot\nfor our mind even process isn't it in\nfact this amount of data is quite a lot\nfor traditional computing systems to\nhandle and this massive amount of data\nis what we term as big data let's have a\nlook at the data generated per minute on\nthe internet\n2.1 million snaps are shared on snapchat\n3.8 million search queries are made on\ngoogle one million people log on to\nfacebook 4.5 million videos are watched\non youtube\n188 million emails are sent that's a lot\nof data so how do you classify any data\nas big data this is possible with the\nconcept of five v's\nvolume velocity\nvariety\nveracity and value\nlet us understand this with an example\nfrom the healthcare industry hospitals\nand clinics across the world generate\nmassive volumes of data 2\n314 exabytes of data are collected\nannually in the form of patient records\nand test results all this data is\ngenerated at a very high speed which\nattributes to the velocity of big data\nvariety refers to the various data types\nsuch as structured semi-structured and\nunstructured data examples include excel\nrecords log files and x-ray images\naccuracy and trustworthiness of the\ngenerated data is termed as veracity\nanalyzing all this data will benefit the\nmedical sector by enabling faster\ndisease detection\nbetter treatment and reduced cost\nthis is known as the value of big data\nbut how do we store and process this big\ndata to do this job we have various\nframeworks such as cassandra hadoop and\nspark let us take hadoop as an example\nand see how hadoop stores and processes\nbig data\nhadoop uses a distributed file system\nknown as hadoop distributed file system\nto store big data if you have a huge\nfile your file will be broken down into\nsmaller chunks and stored in various\nmachines not only that when you break\nthe file you also make copies of it\nwhich goes into different nodes this way\nyou store your big data in a distributed\nway and make sure that even if one\nmachine fails your data is safe on\nanother\nmapreduce technique is used to process\nbig data a lengthy task a is broken into\nsmaller tasks\nb\nc\nand d\nnow instead of one machine three\nmachines take up each task and complete\nit in a parallel fashion and assemble\nthe results at the end thanks to this\nthe processing becomes easy and fast\nthis is known as parallel processing\nnow that we have stored and processed\nour big data we can analyze this data\nfor numerous applications\nin games like halo 3 and call of duty\ndesigners analyze user data to\nunderstand at which stage most of the\nusers pause restart or quit playing this\ninsight can help them rework on the\nstoryline of the game and improve the\nuser experience\nwhich in turn reduces the customer churn\nrate\nbig data also helped with disaster\nmanagement during hurricane sandy in\n2012 it was used to gain a better\nunderstanding of the storm's effect on\nthe east coast of the us and necessary\nmeasures were taken it could predict the\nhurricane's landfall five days in\nadvance which wasn't possible earlier\nthese are some of the clear indications\nof how valuable big data can be once it\nis accurately processed and analyzed\nback in the early 2000s there was\nrelatively less data generated but with\nthe rise of various social media\nplatforms and multinational companies\nacross the globe the generation of data\nhas increased by leaps and bounds\naccording to the idc the total volume of\ndata is expected to reach 175 zettabytes\nin 2025 that's a lot of data\nso we can define big data as massive\namount of data which cannot be stored\nprocessed and analyzed using the\ntraditional ways let's now have a look\nat the challenges with respect to big\ndata\nthe first challenge with big data is its\nstorage storing big data is not easy as\nthe data generation is endless in\naddition to this storing unstructured\ndata in our traditional databases is a\ngreat challenge unstructured data refers\nto data such as photographs and videos\nthe second challenge is processing big\ndata data is only useful to us if it is\nprocessed and analyzed processing big\ndata consumes a lot of time due to its\nsize and structure\nto overcome these challenges of big data\nwe have various frameworks such as\nhadoop cassandra and spark let us have a\nquick look at hadoop and spark\nso what is hadoop it is a framework that\nstores big data in a distributed way and\nprocesses it parallely how do you think\nhadoop does this\nwell hadoop uses a distributed file\nsystem known as hadoop distributed file\nsystem to store big data if you have a\nhuge file your file will be broken out\ninto smaller chunks and stored in\nvarious machines this is why it is\ntermed as distributed storage\nmapreduce is the processing unit of\nhadoop here we have multiple machines\nworking parallelly to process big data\nthanks to this technique the processing\nbecomes easy and fast\nlet's now move on to spark spark is a\nframework that is responsible for\nprocessing data both in batches and in\nreal time spark is used to analyze data\nacross various clusters of computers\nnow that you have understood big data\nhadoop and spark let's look into few of\nthe different job roles in this domain\ncareer opportunities in this field are\nlimitless as organizations are using big\ndata to enhance their products business\ndecisions and marketing effectiveness\nso here we will understand in depth as\nto how to become a big data engineer in\naddition to this we will also look into\nthe various uh skills required to become\na hadoop developer spark developer and a\nbig data architect\nstarting off with a big data engineer\nrole\ndo you know who a big data engineer is\nwell big data engineers are\nprofessionals who develop maintain test\nand evaluate a company's big data\ninfrastructure\nthey have several responsibilities\nfirstly they are responsible for\ndesigning and implementing software\nsystems they verify and maintain these\nsystems for the ingestion and processing\nof data they built robust systems\nextract transform load operations known\nas the etl process is carried out by big\ndata engineers they also research\nvarious new methods to obtain data and\nimprove its quality\nbig data engineers are also responsible\nfor building data architectures that\nmeet the business requirements they\nprovide a solution by integrating\nvarious tools and programming languages\nin addition to the above\nresponsibilities their primary\nresponsibility is to mine data from\nplenty of different sources to build\nefficient business models lastly they\nalso work closely with data architects\ndata scientists and data analysts\nthose acquire a lot of responsibilities\nlet us now have a look at the skills\nrequired to achieve these\nresponsibilities\nas you can see on your screens we have\nlisted the top\ntop seven skills needed to be possessed\nby a big data engineer\nstarting off the essential skill is\nprogramming\na big data engineer needs to have\nhands-on experience in any one of the\nprogramming languages such as java\nc-plus plus or python\nas a big data engineer you should also\nhave in-depth knowledge of dbms and sql\nthis is because you have to understand\nhow data is managed and maintained in a\ndatabase hence you need to know how to\nwrite sql queries for any rdbms systems\nsome of the commonly used database\nmanagement systems for big data\nengineering are mysql oracle database\nand the microsoft sql server\nas mentioned earlier carrying out an e\ncarrying out an etl operation is a big\ndata engineer's responsibility now you\nneed to know how to construct as well as\nuse a data warehouse so that you can\nperform these etl operations as a big\ndata engineer you will be continuously\ntasked with extracting data from various\nsources transforming them into\nmeaningful information and loading it\ninto other data storages some of the\ntools used for this purpose are talent\nibm data stage pentaho and informatica\nnext up we have the fourth skill that\nyou require which is knowledge about\noperating systems\nbig data tools run on operating systems\nhence a sound understanding of unix\nlinux windows and solaris is mandatory\nthe fifth skill that you require is to\nhave experience with hadoop based\nanalytics\nsince haroop is one of the most commonly\nused big data engineering tools it's\nunderstood that you need to have\nexperience with apache hadoop based\ntechnologies like hdfs mapreduce apache\npig hive and apache hedge base\nthe sixth skill that you require is to\nhave worked with real-time processing\nframeworks like apache spark\nas a big data engineer you would deal\nwith large volumes of data\nso for this you need an analytics engine\nlike spark which can be used for both\nbatch and real-time processing\nspark can process live streaming data\nfrom several different sources like\nfacebook instagram twitter and so on\nand now we are at our final skill\nrequirement which is to have experience\nwith data mining and data modeling as a\nbig data engineer you would have to\nexamine massive pre-existing data so\nthat you can discover patterns and new\ninformation this will help you create\npredictive models that will help you\nmake various business business decisions\nsome of the tools used for this are are\nrapidminer becca and nime\nnow let's talk about a big data engineer\nsalary in the u.s a big data engineer's\naverage annual salary is around hundred\nand two thousand dollars per annum in\nindia a big data engineer makes about\nseven lakh rupees per annum\nthat was all about our first role big\ndata engineer moving on to our second\nrole we have hadoop developer\nas the name suggests a hadoop developer\nlooks into the coding and programming of\nthe various hadoop applications this job\nrole is more or less similar to that of\na software developer\nmoving on to the skills required to\nbecome a hadoop developer at first it is\na knowledge of the hadoop ecosystem a\nhadoop developer should have in-depth\nknowledge about the hadoop ecosystem and\nits components which include hedge base\npig scoop flume uzi etc you should also\nhave data modeling experience with olap\nand oltp knowledge of sql is required\nlike a big data engineer a hadoop\ndeveloper must also know popular tools\nlike pentaho informatica and talent\nfinally you should also be well versed\nin writing pig latin scripts and\nmap-reduce jobs\nthe average annual salary of a hadoop\ndeveloper in the u.s is nearly 76 000\nper annum in india a hadoop developer\nmakes approximately 4 lakh 57 000 rupees\nper annum\nmoving to our third job role we have\nspark developer\nwe saw what spark is now let's\nunderstand what a spark developer does\nspark developers create spark jobs using\npython or scala for data aggregation and\ntransformation they also write analytics\ncode and design data processing\npipelines\na spark developer needs to have the\nfollowing skills they need to know spark\nand its components such as spark core\nspark streaming spark machine learning\nlibrary etc\nthey should also know the scripting\nlanguages like python or perl\njust like the previous job roles a spark\ndeveloper should also have basic\nknowledge of sequel queries and a\ndatabase structure\nin addition to the above a spark\ndeveloper is also expected to have a\nfairly good understanding of linux and\nits commands\nmoving to the average annual salary of a\nspark developer it is nearly 81 000\ndollars per annum in the us in india a\nspark developer earns nearly 5 lakh 87\n000 rupees per annum\nlet's now move on to our final job role\nthat is big data architect\nso let's understand who a big data\narchitect is\nwell a big data architect is a\nprofessional who is responsible for\ndesigning and planning big data systems\nthey also manage large scale development\nand deployment of hadoop applications\nmoving on to the skills required to\nbecome a big data architect first up the\nindividual must have advanced data\nmining and data analysis skills big data\narchitects should also be able to\nimplement and use a nosql database and\ncloud computing techniques\nthey must also have an idea of various\nbig data technologies like hadoop\nmapreduce hbase hype and so on\nfinally big data architects must hold\nexperience with agile and scrum\nframeworks\nlet's now have a look at the average\nannual salary of a big data architect in\nthe united states and india in the u.s a\nbig data architect earns a whopping 118\nthousand dollars per annum meanwhile in\nindia a big data architect makes nearly\n19 lakh rupees per annum these are huge\nnumbers right\nso now that we have understood the job\nroles of a big data engineer hadoop\ndeveloper spark developer and a big data\narchitect let us have a look at the\ncompanies hiring these professionals\nas you all can see on your screens we\nhave ibm amazon american express netflix\nmicrosoft and bosch to name a few\nlet us now understand why big data\nanalytics is required with an example so\nall of you listen to music online isn't\nit here we will take an example of\nspotify which is a swedish audio\nstreaming platform and see how big data\nanalytics is used here spotify has\nnearly 96 plus million users and all\nthese users generate a tremendous amount\nof data data like the songs which are\nplayed repeatedly the numerous likes\nshares and the user search history all\nthese data can be termed as big data\nhere with respect to spotify have you\never wondered what spotify does with all\nthis big data well spotify analyzes this\nbig data for suggesting songs to its\nusers i'm sure all of you might have\ncome across the recommendation list\nwhich is made available to each one of\nyou by spotify each one of you will have\na totally different recommendation list\nthis is based on your likes your past\nhistory like the songs you like\nlistening to and your playlists this\nworks on something known as the\nrecommendation system recommendation\nsystems are nothing but data filtering\ntools they collect all the data and then\nfilter them using various algorithms\nthis system has the ability to\naccurately predict what a user would\nlike to listen to next with the help of\nbig data analytics this prediction helps\nthe users stay on the page for a longer\ntime and by doing so spotify engages all\nits users the users don't have the need\nto go on searching for different songs\nthis is because spotify readily provides\nthem with a variety of songs according\nto their taste well this is how big data\nanalytics is used by spotify now that we\nhave understood why big data analytics\nis required let us move on to our next\ntopic that is what is big data analytics\nbut before that there is another term we\nneed to understand that is big data we\nall hear the term big data many times\nbut do you know what exactly big data\nmeans well we will understand the term\nbig data clearly right now big data is a\nterm for data sets that cannot be\nhandled by traditional computers or\ntools due to their value volume velocity\nand veracity it is defined as massive\namount of data which cannot be stored\nprocess and analyzed using various\ntraditional methods do you know how much\ndata is being generated every day every\nsecond even as i talk right now there\nare millions of data sources which\ngenerate data at a very rapid rate these\ndata sources are present across the\nworld as you know social media sites\ngenerate a lot of data let's take an\nexample of facebook facebook generates\nover 500 plus terabytes of data every\nday this data is mainly generated in\nterms of your photographs videos\nmessages etc big data also contains data\nof different formats like structured\ndata semi-structured data and\nunstructured data data like your excel\nsheets all fall under structured data\nthis data has a definite format your\nemails fall under semi-structured and\nyour pictures and videos all fall under\nunstructured data all these data\ntogether make up for big data it is very\ntough to store process and analyze big\ndata using rtbms if you have looked into\nour previous videos you would know that\nhadoop is the solution to this hadoop is\na framework that stores and processes\nbig data it stores big data using the\ndistributed storage system and it\nprocesses big data using the parallel\nprocessing method hence storing and\nprocessing big data is no more a problem\nusing hadoop big data in its raw form is\nof no use to us we must try to derive\nmeaningful information from it in order\nto benefit from this big data do you\nknow amazon uses big data to monitor its\nitems that are in its fulfillment\ncenters across the globe how do you\nthink amazon does this well it is done\nby analyzing big data which is known as\nbig data analytics what is big data\nanalytics in simple terms big data\nanalytics is defined as the process\nwhich is used to extract meaningful\ninformation from big data this\ninformation could be your hidden\npatterns unknown correlations market\ntrends and so on by using big data\nanalytics there are many advantages it\ncan be used for better decision making\nto prevent fraudulent activities and\nmany others we will look into four of\nthem step by step i will first start off\nwith big data analytics which is used\nfor risk management bdo which is a\nphilippine banking company uses big data\nanalytics for risk management risk\nmanagement is an important aspect for\nany organization especially in the field\nof banking risk management analysis\ncomprises a series of measures which are\nemployed to prevent any sort of\nunauthorized activities identifying\nfraudulent activities was a main concern\nfor bdo it was difficult for the bank to\nidentify the fraudster from a long list\nof suspects bdo adopted big data\nanalytics which held the bank to narrow\ndown the entire list of suspects thus\nthe organization was able to identify\nthe fraudster in a very short time this\nis how big data analytics is used in the\nfield of banking for risk management let\nus now see how big data analytics is\nused for product development and\ninnovations with an example all of you\nare aware of rolls-royce cars right do\nyou also know that they manufacture jet\nengines which are used across the world\nwhat is more interesting is that they\nuse big data analytics for developing\nand innovating this engine a new product\nis always developed by trial and error\nmethod big data analytics is used here\nto analyze if an engine design is good\nor bad it is also used to analyze if\nthere can be more scope for improvement\nbased on the previous models and on the\nfuture demands this way big data\nanalytics is used in designing a product\nwhich is of higher quality using big\ndata analytics the company can save a\nlot of time if the team is struggling to\narrive at the right conclusion big data\ncan be used to zero in on the right data\nwhich have to be studied and thus the\ntime spent on the product development is\nless big data analytics helps in quicker\nand better decision making in\norganizations the process of selecting a\ncourse of action from various other\nalternatives is known as decision making\nlot of organizations take important\ndecisions based on the data that they\nhave data driven business decisions can\nmake or break a company hence it is\nimportant to analyze all the\npossibilities thoroughly and quickly\nbefore making important decisions let us\nnow try to understand this with an use\ncase starbucks uses big data analytics\nfor making important decisions they\ndecide the location of their new outlet\nusing big data analytics choosing the\nright location is an important factor\nfor any organization the wrong location\nwill not be able to attract the required\namount of customers positioning of a new\noutlet a few miles here or there can\nalways make a huge difference for an\noutlet especially a one like starbucks\nvarious factors are involved in choosing\nthe right location for a new outlet for\nexample parking adequacy has to be taken\ninto consideration it would be\ninconvenient for people to go to a store\nwhich has no parking facility similarly\nthe other factors that have to be\nconsidered are the visibility of the\nlocation the accessibility the economic\nfactors the population of that\nparticular location and also we would\nhave to look into the competition in the\nvicinity all these factors have to be\nthoroughly analyzed before making a\ndecision as to where the new outlet must\nbe started without analyzing these\nfactors it would be impossible for us to\nmake a wise decision using big data\nanalytics we can consider all these\nfactors and analyze them quickly and\nthoroughly thus starbucks makes use of\nbig data analytics to understand if\ntheir new location would be fruitful or\nnot finally we will look into how big\ndata analytics is used to improve\ncustomer experience using an example\ndelta an american airline uses big data\nanalytics to improve its customer\nexperiences with the increase in global\nair travel it is necessary that an\nairline does everything they can in\norder to provide good service and\nexperience to its customers delta\nairlines improves its customer\nexperience by making use of big data\nanalytics this is done by monitoring\ntweets which will give them an idea as\nto how their customers journey was if\nthe airline comes across a negative\ntweet and if it is found to be the\nairline's fault the airline goes ahead\nand upgrades that particular customer's\nticket when this happens the customer is\nable to trust the airlines and without a\ndoubt the customer will choose delta for\ntheir next journey by doing so the\ncustomer is happy and the airlines will\nbe able to build a good brand\nrecognition thus we see here that by\nusing analysis delta airlines was able\nto improve its customer experience\nmoving on to our next topic that is life\ncycle of big data analytics here we will\nlook into the various stages as to how\ndata is analyzed from scratch the first\nstage is the business case evaluation\nstage here the motive behind the\nanalysis is identified we need to\nunderstand why we are analyzing so that\nwe know how to do it and what are the\ndifferent parameters that have to be\nlooked into once this is done it is\nclear for us and it becomes much easier\nfor us to proceed with the rest after\nwhich we will look into the various data\nsources from where we can gather all the\ndata which will be required for analysis\nonce we get the required data we will\nhave to see if the data that we received\nis fit for analysis or not not all the\ndata that we receive will have\nmeaningful information some of it will\nsurely just be corrupt data to remove\nthis corrupt data we will pass this\nentire data through a filtering stage in\nthis stage all the corrupt data will be\nremoved now we have the data minus the\ncorrupt data do you think our data is\nnow fit for analysis well it is not we\nstill have to figure out which data will\nbe compatible with the tool that we will\nbe using for analysis if we find data\nwhich is incompatible we first extract\nit and then transform it to a compatible\nform depending on the tool that we use\nin the next stage all the data with the\nsame fields will be integrated this is\nknown as the data aggregation stage the\nnext stage which is the analysis stage\nis a very important stage in the life\ncycle of big data analytics right here\nin this step the entire process of\nevaluating your data using various\nanalytical and statistical tools to\ndiscover meaningful information is done\nlike we have discussed before the entire\nprocess of deriving meaningful\ninformation from data which is known as\nanalysis is done here in this stage the\nresult of the data analysis stage is\nthen graphically communicated using\ntools like tableau power bi click view\nthis analysis result will then be made\navailable to different business\nstakeholders for various decision making\nthis was the entire life cycle of big\ndata analytics we just saw how data is\nanalyzed from scratch now we will move\non to a very important topic that is the\ndifferent types of big data analytics\nwell we have four different types of big\ndata analytics as you see here these are\nthe types and below this are the\nquestions that each type tries to answer\nwe have descriptive analytics which asks\nthe question what has happened then we\nhave diagnostic analytics which asks why\ndid it happen predictive analytics\nasking what will happen and prescriptive\nanalytics which questions by asking what\nis the solution we will look into all\nthese four one by one with an use case\nfor each we will first start off with\ndescriptive analytics as mentioned\nearlier descriptive analytics asks the\nquestion what has happened it can be\ndefined as the type that summarizes past\ndata into a form that can be understood\nby humans in this type we will look into\nthe past data and arrive at various\nconclusions for example an organization\ncan review its performance using\ndescriptive analytics that is it\nanalyzes its past data such as revenue\nover the years and arrives at a\nconclusion with the profit by looking at\nthis graph we can understand if the\ncompany is running at a profit or not\nthus descriptive analytics helps us\nunderstand this easily we can simply say\nthat descriptive analytics is used for\ncreating various reports for companies\nand also for tabulating various social\nmedia metrics like facebook likes tweets\netc now that we have seen what is\ndescriptive analytics let us look into\nand use case of descriptive analytics\nthe dow chemical company analyzed all\nits past data using descriptive\nanalytics and by doing so they were able\nto identify the under utilized space in\ntheir facility descriptive analytics\nhelped them in this space consolidation\non the whole the company was able to\nsave nearly 4 million dollars annually\nso we now see here that descriptive\nanalytics not only helps us derive\nmeaningful information from the past\ndata but it can also help companies in\ncost reduction if it is used wisely let\nus now move on to our next type that is\ndiagnostic analytics diagnostic\nanalytics asks the question why a\nparticular problem has occurred as you\ncan see it will always ask the question\nwhy did it happen it will look into the\nroot cause of a problem and try to\nunderstand why it has occurred\ndiagnostic analytics makes use of\nvarious techniques such as data mining\ndata discovery and drill down companies\nbenefit from this type of analytics\nbecause it helps them look into the root\ncause of a problem by doing so the next\ntime the same problem will not arise as\nthe company already knows why it has\nhappened and they will arrive at a\nparticular solution for it initsoft's bi\nquery tool is an example of diagnostic\nanalytics we can use query tool for\ndiagnostic analytics now that you know\nwhy diagnostic analytics is required and\nwhat diagnostic analytics is i will run\nyou through an example which shows where\ndiagnostic analytics can be used all of\nus shop on e-commerce sites right have\nyou ever added items to your cart but\nended up not buying it yes all of us\nmight have done that at some point an\norganization tries to understand why its\ncustomers don't end up buying their\nproducts although it has been added to\ntheir carts and this understanding is\ndone with the help of diagnostic\nanalytics an e-commerce site wonders why\nthey have made few online sales although\nthey have had a very good marketing\nstrategy there could have been various\nfactors as to why this has happened\nfactors like the shipping fee which was\ntoo high or the page that didn't load\ncorrectly not enough payment options\navailable and so on all these factors\nare analyzed using diagnostic analytics\nand the company comes to a conclusion as\nto why this has happened thus we see\nhere that the root cause is identified\nso that in future the same problem\ndoesn't occur again let us now move on\nto the third type that is predictive\nanalytics as the name suggests\npredictive analytics makes predictions\nof the future it analyzes the current\nand historical facts to make predictions\nabout future it always asks the question\nwhat will happen next predictive\nanalytics is used with the help of\nartificial intelligence machine learning\ndata mining to analyze the data it can\nbe used for predicting customer trends\nmarket trends customer behavior etc it\nsolely works on probability it always\ntries to understand what can happen next\nwith the help of all the past and\ncurrent information a company like\npaypal which has 260 plus million\naccounts always has the need to ensure\nthat their online fraudulent and\nunauthorized activities are brought down\nto nil fear of constant fraudulent\nactivities have always been a major\nconcern for paypal when a fraudulent\nactivity occurs people lose trust in the\ncompany and this brings in a very bad\nname for the brand it is inevitable that\nfraudulent activities will happen in a\ncompany like paypal which is one of the\nlargest online payment processors in the\nworld but paypal uses analytics wisely\nhere to prevent such fraudulent\nactivities and to minimize them it uses\npredictive analytics to do so the\norganization is able to analyze past\ndata which includes a customer's\nhistorical payment data a customer's\nbehavior trend and then it builds an\nalgorithm which works on predicting what\nis likely to happen next with respect to\ntheir transaction with the use of big\ndata and algorithms the system can gauge\nwhich of the transactions are valid and\nwhich could be potentially a fraudulent\nactivity by doing so paypal is always\nready with precautions that they have to\ntake to protect all their clients\nagainst fraudulent transactions we will\nnow move on to our last type that is\nprescriptive analytics prescriptive\nanalytics as the name suggests always\nprescribes a solution to a particular\nproblem the problem can be something\nwhich is happening currently hence it\ncan be termed as the type that always\nasks the question what is the solution\nprescriptive analytics is related to\nboth predictive and descriptive\nanalytics as we saw earlier descriptive\nanalytics always asks the question what\nhas happened and predictive analytics\nhelps you understand what can happen\nnext with the help of artificial\nintelligence and machine learning\nprescriptive analytics helps you arrive\nat the best solution for a particular\nproblem various business rules\nalgorithms and computational modeling\nprocedures are used in prescriptive\nanalytics let us now have a look at how\nand where prescriptive analytics is used\nwith an example here we will understand\nhow prescriptive analytics is used by an\nairline for its profit do you know that\nwhen you book a flight ticket the price\nof it depends on various factors both\ninternal and external factors apart from\ntaxes seed selection there are other\nfactors like oil prices customer demand\nwhich are all taken into consideration\nbefore the flight sphere is displayed\nprices change due to availability and\ndemand holiday seasons are a time when\nthe rates are much higher than the\nnormal days seasons like christmas and\nschool vacations also weekends the rates\nwill be much higher than weekdays\nanother factor which determines a\nflight's fair is your destination\ndepending on the place where you are\ntraveling to the flight fair will be\nadjusted accordingly this is because\nthere are quite a few places where the\nair traffic is less and in such places\nthe flight sphere will also be less so\nprescriptive analytics analyzes all\nthese factors that are discussed and it\nbuilds an algorithm which will\nautomatically adjust a flight sphere by\ndoing so the airline is able to maximize\nits profit these were the four types of\nanalytics now let us understand how we\nachieve these with the use of big data\ntools our next topic will be the various\ntools used in big data analytics these\nare few of the tools that i will be\ntalking about today we have hadoop\nmongodb talindi kafka cassandra spark\nand storm we will look into each one of\nthem one by one we will first start off\nwith hadoop when you speak of big data\nthe first framework that comes into\neveryone's mind is hadoop isn't it as i\nmentioned earlier apache hadoop is used\nto store and process big data in a\ndistributed and parallel fashion it\nallows us to process data very fast\nhadoop uses mapreduce big and high for\nanalyzing this big data hadoop is easily\none of the most famous big data tools\nnow let us move on to the next one that\nis mongodb mongodb is a cross-platform\ndocument oriented database it has the\nability to deal with large amount of\nunstructured data processing of data\nwhich is unstructured and processing of\ndata sets that change very frequently is\ndone using mongodb talendi provides\nsoftware and services for data\nintegration data management and cloud\nstorage it specializes in big data\nintegration talandi open studio is a\nfree open source tool for processing\ndata easily on a big data environment\ncassandra is used widely for an\neffective management of large amounts of\ndata it is similar to hadoop in its\nfeature of fault tolerance where data is\nautomatically replicated to multiple\nnodes cassandra is preferred for\nreal-time processing spark is another\ntool that is used for data processing\nthis data processing engine is developed\nto process data way faster than hadoop\nmapreduce this is done in a way because\nspark does all the processing in the\nmain memory of the data nodes and thus\nit prevents unnecessary input output\noverheads with the disk whereas\nmapreduce is disk based and hence spark\nproves to be faster than hadoop\nmapreduce storm is a free big data\ncomputational system which is done in\nreal time it is one of the easiest tools\nfor big data analysis it can be used\nwith any programming language this\nfeature makes storm very simple to use\nfinally we will look into another big\ndata tool which is known as kafka kafka\nis a distributed streaming platform\nwhich was developed by linkedin and\nlater given to apache software\nfoundation it is used to provide\nreal-time analytics result and it is\nalso used for fault tolerant storage\nthese were few of the big data analytics\ntools now let us move on to our last\ntopic for today that is big data\napplication domains here we will look\ninto the various sectors where big data\nanalytics is actively used the first\nsector is e-commerce merely 45 percent\nof the world is online and they create a\nlot of data every second big data can be\nused smartly in the field of e-commerce\nby predicting customer trend forecasting\ndemands adjusting the price and so on\nonline retailers have the opportunity to\ncreate better shopping experience and\ngenerate higher sales if big data\nanalytics is used correctly having big\ndata doesn't automatically lead to a\nbetter marketing strategy meaningful\ninsights need to be derived from it in\norder to make right decisions by\nanalyzing big data we can have\npersonalized marketing campaigns which\ncan result in better and higher sales in\nthe field of education depending on the\nmarket requirements new courses are\ndeveloped the market requirement needs\nto be analyzed correctly with respect to\nthe scope of a course and accordingly a\nscope needs to be developed there is no\npoint in developing a course which has\nno scope in the future hence to analyze\nthe market requirement and to develop\nnew courses we use big data analytics\nhere there are a number of uses of big\ndata analytics in the field of health\ncare and one of it is to predict a\npatient's health issue that is with the\nhelp of their previous medical history\nbig data analytics can determine how\nlikely they are to have a particular\nhealth issue in the future the example\nof spotify that we saw previously showed\nhow big data analytics is used to\nprovide a personalized recommendation\nlist to all its users similarly in the\nfield of media and entertainment big\ndata analytics is used to understand the\ndemands of shows songs movies and so on\nto deliver personalized recommendation\nlist as we saw with spotify big data\nanalytics is used in the field of\nbanking as we saw previously with a few\nuse cases big data analytics was used\nfor risk management in addition to risk\nmanagement\nit is also used to analyze a customer's\nincome and spend patterns and to help\nthe bank predict if a particular\ncustomer is going to choose any of the\nbank offers such as loans credit card\nschemes and so on this way the bank is\nable to identify the right customer who\nis interested in its offers it has\nnoticed that telecom companies have\nbegun to embrace big data to gain profit\nbig data analytics helps in analyzing\nnetwork traffic and call data records it\ncan also improve its service quality and\nimprove its customer experience let us\nnow look into how big data analytics is\nused by governments across the world in\nthe field of law enforcement big data\nanalytics can be applied to analyze all\nthe available data to understand crime\npatterns intelligence services can use\npredictive analytics to focus the crime\nwhich could be committed\nin durham the police department was able\nto reduce the crime rate using big data\nanalytics with the help of data police\ncould identify whom to target where to\ngo when to petrol and how to investigate\ncrimes big data analytics help them to\ndiscover patterns of crime emerging in\nthe area before we move on to the\napplications let's have a quick look at\nthe big data market revenue forecast\nworldwide from 2011 to 2027.\nso here's a graph in which the y-axis\nrepresents the revenue in billion us\ndollars and the x-axis represents the\nyears as it is seen clearly from the\ngraph big data has grown until 2019 and\nstatistics predict that this growth will\ncontinue even in the future this growth\nis made possible as numerous companies\nuse big data in various domains to boost\ntheir revenue we will look into few of\nsuch applications the first big data\napplication we will look into is weather\nforecast imagine there is a sudden storm\nand you're not even prepared that would\nbe a terrifying situation isn't it\ndealing with any calamities such as\nhurricane storms floods would be very\ninconvenient if we are caught off guard\nthe solution is to have a tool that\npredicts the weather of the coming days\nwell in advance this tool needs to be\naccurate and to make such a tool big\ndata is used so how does big data help\nhere well it allows us to gather all the\ninformation required to predict the\nweather information such as the climate\nchange details wind direction\nprecipitation previous weather reports\nand so on after all this data is\ncollected it becomes easier for us to\nspot a trend and identify what's going\nto happen next by analyzing all of this\nbig data a weather prediction engine\nworks on this analysis it predicts the\nweather of every region across the world\nfor any given time by using such a tool\nwe can be well prepared to face any\nclimate change or any natural calamity\nlet's take an example of a landslide and\ntry to understand how big data is used\nto tackle such a situation predicting a\nlandslide is very difficult with just\nthe basic warning signs lack of this\nprediction can cause a huge damage to\nlife and property this challenge was\nstudied by the university of melbourne\nand they developed a tool which is\ncapable of predicting a landslide this\ntool predicts the boundary where a\nlandslide is likely to occur two weeks\nbefore this magical tool works on both\nbig data and applied mathematics an\naccurate prediction like this which is\nmade two weeks before can save lives and\nhelp in relocating people in that\nparticular region it also gives us an\ninsight into the magnitude of the\nupcoming destruction this is how big\ndata is used in weather forecast and in\npredicting any natural calamities across\nthe world let us now move on to our next\napplication that is big data application\nin the field of media and entertainment\nthe media and the entertainment industry\nis a massive one leveraging big data\nhere can produce sky-high results and\nboost the revenue for any company let us\nsee the different ways in which big data\nis used in this industry have you ever\nnoticed that you come across relevant\nadvertisements in your social media\nsites and in your mailboxes well this is\ndone by analyzing all your data such as\nyour previous browsing history and your\npurchase data publishers then display\nwhat you like in the form of ads which\nwill in turn catch your interest in\nlooking into it next up is customer\nsentiment analysis customers are very\nimportant for a company the happier the\ncustomer the greater the company's\nrevenue big data helps in gathering all\nthe emotions of a customer through their\nposts messages conversations etc these\nemotions are then analyzed to arrive at\na conclusion regarding the customer\nsatisfaction if the customer is unhappy\nthe company strives to do better the\nnext time and provides their customers a\nbetter experience while purchasing an\nitem from an e-commerce site or while\nwatching videos on an entertainment site\nyou might have noticed a segment which\nsays most recommended list for you this\nlist is a personalized list which is\nmade available to you by analyzing all\nthe data such as your previous watch\nhistory your subscriptions your likes\nand so on recommendation engine is a\ntool that filters and analyzes all this\ndata and provides you with a list that\nyou would most likely be interested in\nby doing so the site is able to retain\nand engage its customer for a longer\ntime next is customer churn analysis in\nsimple words customer churn happens when\na customer stops a subscription with a\nservice predicting and preventing this\nis of paramount importance to any\norganization by analyzing the behavioral\npatterns of previously churned customers\nan organization can identify which of\ntheir current customers are likely to\nchurn by analyzing all of this data the\norganization can then implement\neffective programs for customer\nretention let us now look into an use\ncase of starbucks big data is\neffectively used by the starbucks app 17\nmillion users use this app and you can\nimagine how much data they generate data\nin the form of their coffee buying\nhabits the store they visit and to the\ntime they purchase all of this data is\nfed into the app so when a customer\nenters a new starbucks location the\nsystem analyzes all their data and they\nare provided with their preferred order\nthis app also suggests new products to\nthe customer in addition to this they\nalso provide personalized offer and\ndiscounts on special occasions moving on\nto our next sector which is healthcare\nit is one of the most important sectors\nbig data is widely used here to save\nlives with all the available big data\nmedical researchers are done very\neffectively they are performed\naccurately by analyzing all the previous\nmedical histories and new treatments and\nmedicines are discovered cure can be\nfound out even for few of the incurable\ndiseases there are cases when one\nmedication need not be effective for\nevery patient hence personal care is\nvery important personal care is provided\nto each patient depending on their past\nmedical history and individuals medical\nhistory along with their body parameters\nare analyzed and personal attention is\ngiven to each of them as we all know\nmedical treatments are not very pocket\nfriendly every time a medical treatment\nis taken the amount increases this can\nbe reduced if readmissions are brought\ndown analyzing all the data precisely\nwill deliver a long-term efficient\nresult which will in turn prevent a\npatient's readmission frequently with\nglobalization came an increase in the\nease for infectious diseases to spread\nwidely based on geography and\ndemographics big data helps in\npredicting where an outbreak of epidemic\nviruses are most likely to occur an\namerican healthcare company united\nhealthcare uses big data to detect any\nonline medical fraud activities such as\npayment of unauthorized benefits\nintentional misrepresentation of data\nand so on the healthcare company runs\ndisease management programs the success\nrates of these programs are predicted\nusing big data depending on how patients\nrespond to it the next sector we will\nlook into is logistics logistics looks\ninto the process of transportation and\nstorage of goods the movement of a\nproduct from its supplier to a consumer\nis very important big data is used to\nmake this process faster and efficient\nthe most important factor in logistics\nis the time taken for the products to\nreach their destination to achieve\nminimum time sensors within the vehicle\nanalyze the fastest route this analysis\nis based on various data such as the\nweather traffic the list of orders and\nso on by doing so the fastest route is\nobtained and the delivery time is\nreduced capacity planning is another\nfactor which needs to be taken into\nconsideration details regarding the\nworkforce and the number of vehicles are\nanalyzed thoroughly and each vehicle is\nallocated a different route this is done\nas there is no need for many trucks to\ntravel in the same direction which will\nbe pointless depending on the analysis\nof the available workforce and resources\nthis decision is taken big data\nanalytics also finds its use in managing\nwarehouses efficiently this analysis\nalong with tracking sensors provide\ninformation regarding the underutilized\nspace which results in efficient\nresource allocation and eventually\nreduces the cost customer satisfaction\nis important in logistics just like it\nis in any other sector customer\nreactions are analyzed from the\navailable data which will eventually\ncreate an instant feedback loop a happy\ncustomer will always help the company\ngain more customers let us now look into\na use case of ups as you know ups is one\nof the biggest shipping company in the\nworld they have a huge customer database\nand they work on data every minute ups\nuses big data to gather different kinds\nof data regarding the weather the\ntraffic jams the geography the locations\nand so on after collecting all this data\nthey analyze it to discover the best and\nthe fastest route to the destination in\naddition to this they also use big data\nto change the routes in real time this\nis how efficiently ups leverages big\ndata next up we have a very interesting\nsector that is the travel and tourism\nsector the global tourism market is\nexpected to grow in the near future big\ndata is used in various ways in this\nsector let us look into a few of them\nhotels can increase their revenue by\nadjusting the room tariffs depending on\nthe peak seasons such as holiday seasons\nfestive seasons and so on the tourism\nindustry uses all of this data to\nanticipate the demand and maximize their\nrevenue big data is also used by resorts\nand hotels to analyze various details\nregarding their competitors this\nanalysis result helps them to\nincorporate all the good facilities\ntheir competitors are providing and by\ndoing so the hotel is able to flourish\nfurther a customer always comes back if\nthey are offered good packages which are\nmore than just the basic ones looking\ninto a customer's past travel history\nlikes and preferences hotels can provide\nits customers with personalized\nexperiences which will interest them\nhighly investing in an area which could\nbe the hub of tourism is very wise few\ncountries use big data to examine the\ntourism activities in their country and\nthis in turn helps them discover new and\nfruitful investment opportunities let us\nlook into one of the best online\nhomestay networks airbnb and see how big\ndata is used by them airbnb undoubtedly\nprovides its customers with the best\naccommodation across the world big data\nis used by it to analyze the different\nkinds of available properties depending\non the customer's preferences the\npricing the keywords previous customers\nratings and experiences airbnb filters\nout the best result big data works its\nmagic yet again now we will move on to\nour final sector which is the government\nand law enforcement sector maintaining\nlaw and order is of utmost importance to\nany government it is a huge task by\nitself big data plays an active role\nhere and in addition to this it also\nhelps governments bring in new policies\nand schemes for the welfare of its\ncitizens the police department is able\nto predict criminal activities way\nbefore it happens by analyzing big data\ninformation such as the previous crime\nrecords in a particular region the\nsafety aspect in that region and so on\nby analyzing these factors they are able\nto predict any activity which breaks the\nlaw and order of the region governments\nare able to tackle unemployment to a\ngreat extent by using big data by\nanalyzing the number of students\ngraduating every year to the number of\nrelevant job openings the government can\nhave an idea of the unemployment rate in\nthe country and then take necessary\nmeasures to tackle it our next factor is\npoverty in large countries it is\ndifficult to analyze which area requires\nattention and development big data\nanalytics makes it easier for\ngovernments to discover such areas\npoverty gradually decreases once these\nareas begin to develop governments have\nto always be on the lookout for better\ndevelopment a public survey voices the\nopinion of a country's citizens\nanalyzing all the data collected from\nsuch surveys can help governments build\nbetter policies and services which will\nbenefit its citizens let us now move on\nto our use case did you know that the\nnew york police department uses big data\nanalytics to protect its citizens the\ndepartment prevents and identifies\ncrimes by analyzing a huge amount of\ndata which includes fingerprints certain\nemails and records from previous police\ninvestigations and so on after analyzing\nall of this data meaningful insights are\ndrawn from it which will help the police\nin taking the required preventive\nmeasures against crimes now when we talk\nabout evolution of big data we have\nknown that data has evolved in last five\nyears like never before now in fact\nbefore going to big data or before\nunderstanding these solutions and the\nneed and why there is a rush towards big\ndata technology and solution i would\nlike to ask a question take a couple of\nminutes and think why are organizations\ninterested in big data why is there\ncertain rush in industry where everyone\nwould want to ramp up their current\ninfrastructure or would want to be\nworking on technologies which allow them\nto use this big data think about it what\nis happening and why are organizations\ninterested in this and if you think on\nthis\nyou will start thinking about what\norganizations have been doing in past\nwhat organizations have not done and why\nare organizations interested in big data\nnow before we learn on big data we can\nalways look into internet and check for\nuse cases where organizations have\nfailed to use legacy systems or\nrelational databases to work on their\ndata requirements now over in recent or\nover past five years or in recent decade\nwhat has happened is organizations have\nstarted understanding the value of data\nand they have decided\nnot to ignore any data as being\nuneconomical now we can talk about\ndifferent platforms through which data\nis generated take an example of social\nmedia like twitter facebook instagram\nwhatsapp youtube you have e-commerce and\nvarious portals say ebay amazon flipkart\nalibaba.com and then you have various\ntech giants such as google oracle sap\namazon microsoft and so on so lots of\ndata is getting generated every day in\nevery business sector the point here is\nthat organizations have slowly started\nrealizing that they would be interested\nin working on all the data now the\nquestion which i asked was why are\norganizations interested in big data and\nsome of you might have already answered\nor thought about that organizations are\ninterested in doing precise analysis or\nthey want to work on different formats\nof data such as structured unstructured\nsemi-structured data organizations are\ninterested in gaining insights or\nfinding the hidden treasure in the\nso-called big data and this is the main\nreason where organizations are\ninterested in big data now there are\nvarious use cases there are various use\ncases we can compare that organizations\nfrom past 50 or more than 50 years have\nbeen handling huge amount of data they\nhave been working on huge volume of data\nbut the question here is have they\nworked on all the data or have they\nworked on some portion of it what have\nthey used to store this data and if they\nhave used something to store this data\nwhat is happening what is what is\nchanging now when we talk about the\nbusinesses we cannot avoid talking about\nthe dynamism involved now any\norganization would want to have a\nsolution which allows them to store data\nand store huge amount of data capture it\nprocess it analyze it and also look into\nthe data to give more value to the data\norganizations have then been looking for\nsolutions now let's look at some facts\nthat can convince you or that would\nconvince you that data is exploding and\nneeds your attention right 55 billion\nmessages and 4.5 billion photos are sent\neach day on whatsapp 300 hours of video\nare uploaded every minute on youtube did\nyou guys know that youtube is the second\nlargest search engine after google every\nminute users send\n31.25 million messages and watch 2.77\nmillion videos on facebook walmart\nhandles more than 1 million customer\ntransactions every hour google 40 000\nsearch queries are performed on google\nper second that is 3.46 million searches\na day in fact you could also say that a\nlot of times people when they are\nloading up the google page is basically\njust to check their internet connection\nhowever that is also generating data idc\nreports that by 2025 real-time data will\nbe more than a quarter of all the data\nand by 2025 the volume of digital data\nwill increase to 163 zeta bytes that is\nwe are not even talking about gigabytes\nor terabytes anymore we are talking\nabout petabytes exabytes and zeta bytes\nand zeta bytes means 10 to the power 21\nbytes so this is how data has evolved\nnow you can talk about different\ncompanies which would want to use their\ndata to take business decisions they\nwould want to collect the data store it\nand analyze it and that's how they would\nbe interested in drawing insights for\nthe business now this is just a simple\nexample about facebook and what it does\nto work on the data now before we go to\nfacebook you could always check in\ngoogle by just typing in companies using\nbig data and if we say companies using\nbig data we should be able to find a\nlist of different companies which are\nusing big data for different use cases\nthere are various sources from where you\ncan find we could also search for\nsolution that is hadoop which we'll\ndiscuss later but you could always say\ncompanies using hadoop and that should\ntake you to the wiki page which will\nbasically help you know what are the\ndifferent companies which are using this\nso so-called solution called hadoop okay\nnow coming back to what we were\ndiscussing about so organizations are\ninterested in big data as we discussed\nin gaining insights they would want to\nuse the data to find hidden information\nwhich probably they ignored earlier now\ntake an example of rdbms what is biggest\ndrawback in using an rdbms now you might\nthink that rdbms is known for stability\nand consistency and organizations would\nbe interested in storing their data in\noracle or db2 or mysql or microsoft sql\nserver and they have been doing that for\nmany years now so what has changed now\nnow when we talk about rdbms the first\nquestion which i would ask is do we have\naccess to 100 of data being online in\nrdbms the answer is no we would only\nhave 10 or 20 or 30 percent of data\nonline and rest of the data would be\narchived which means that if an\norganization is interested in working on\nall the data they would have to move the\ndata from the archived storage to the\nprocessing layer and that would involve\nbandwidth consumption now this is one of\nthe biggest drawbacks of rdbms you do\nnot have access to 100 of data online in\nmany of the cases organizations started\nrealizing that the data which they were\nignoring as being uneconomical had\nhidden value which they had never\nexploited i had read a presentation\nsomewhere which said torture the data\nand it will confess to anything now\nthat's the value of data which\norganizations have realized in recent\npast take an example of facebook now\nthis shows what facebook does with its\nbig data and we'll come to what is big\ndata but let's understand the use case\nnow facebook collects huge volumes of\nuser data whether that is sms whether\nthat is likes whether that is\nadvertisements whether that is features\nwhich people are liking or photographs\nor even user profiles now by collecting\nthis data and providing a portal which\npeople can use to connect facebook is\nalso accumulating huge volume of data\nand that's way beyond petabytes they\nwould also be interested in analyzing\nthis data and one of the reasons would\nbe they would want to personalize the\nexperience take an example of\npersonalized news feed depending on a\nuser behavior depending on what a user\nlikes what a user would want to know\nabout they can recommend a personalized\nnews feed to every particular user\nthat's just one example of what facebook\ndoes with its data take an example of\nphoto tag suggestions now when you log\ninto facebook account you could also get\nsuggestions on different friends whom\nyou would like to connect to or you\nwould want to tag so that they could be\nknown by others some more examples which\nshow how facebook uses its data are as\nfollows so the flashback collection of\nphotos and posts that receive the most\ncomments and likes okay there was\nsomething called as i voted that was\nused for 2016 elections with reminders\nand directions to tell users their time\nand place of polling also something\ncalled as safety checks in incidents\nsuch as earthquake hurricane or mass\nshooting facebook gives you safety\nchecks now these are some examples where\nfacebook is using big data and that\nbrings us to the question what is big\ndata this was just an example where we\ndiscussed about one company which is\nmaking use of that data which has been\naccumulated and it's not only for\ncompanies which are social media\noriented like facebook where data is\nimportant take an example of ibm take an\nexample of jpmorgan chase take an\nexample of ge or any other organization\nwhich is collecting huge amount of data\nthey would all want to gather insights\nthey would want to analyze the data they\nwould want to be more precise in\nbuilding their services or solutions\nwhich can take care of their customers\nso what is big data big data is\nbasically a term it is used to describe\nthe data that is too large and complex\nto store in traditional databases and as\ni gave an example it's not just about\nstoring the data it is also about what\nyou can do with the data it also means\nthat if there is a lot of dynamism\ninvolved can you change the underlying\nstorage and handle any kind of data that\ncomes in now before we get into that\nlet's just understand what is big data\nso big data is basically a term which\nhas been given to categorize the data if\nit has different characteristics\norganizations would want to have the big\ndata stored processed and then analyzed\nto get whatever useful information they\ncan get from this data now there are\nfive v's of big data volume velocity\nvariety value velocity although these\nare five v's but then there are other\nv's which also categorize the data as\nbig data such as volatility validity\nviscosity virality of data okay so these\nare five v's of big data and if the data\nhas one or all of these characteristics\nthen it can be considered as big data\nincluding the other ways which i just\nmentioned so volume basically means\nincredible amount of data huge volumes\nof data data generated every second now\nthat could be used for batch processing\nthat could be used for real-time stream\nprocessing okay you might have data\nbeing generated from different kind of\ndevices like your cell phones your\nsocial media websites online\ntransactions variable devices servers\nand these days with iot we are also\ntalking about data of getting generated\nvia internet of things that is you could\nhave different devices which could be\ncommunicating to each other you could be\ngetting data from radars or leaders or\neven camera sensors so there is a huge\nvolume of data which is getting\ngenerated and if we are talking about\ndata which has huge volume which is\ngetting generated constantly or has been\naccumulated over a period of time we\nwould say that is big data velocity now\nthis is one more important aspect of big\ndata speed with which the data is\ngetting generated think about stock\nmarkets think about social media\nwebsites think about online surveys or\nmarketing campaigns or airline industry\nso if the data is getting generated with\na lot of speed where it becomes\ndifficult to capture collect process\ncure\nmine or analyze the data then we are\ncertainly talking about big data the\nnext aspect of big data is variety now\nthis is where we talk about structured\ndata semi-structured data or\nunstructured data and here i would like\nto ask a question what is the difference\nwhen do you call the data is structured\nsemi-structured or unstructured now\nlet's look at an example before we\ntheoretically discuss about this i\nalways would like to use some examples\nlet's look at a log file and let's see\nwhat is it so if i look at this log file\nand if i would say what kind of data is\nthis which is the highlighted one the\nanswer would be\nit is structured data it has specific\ndelimiters such as space it has data\nwhich is separated by space and if i had\na hundred or thousand or million rows\nwhich had similar kind of data i could\ncertainly store that in a table i could\nhave a predefined schema to store this\ndata so i would call the one which is\nhighlighted and structured but if i look\nat this portion where i would look at a\ncombination of this kind of data where\nsome data has a pattern and some data\ndoesn't now this is an example of\nsemi-structured data so if i would have\na predefined structure to store this\ndata probably the pattern of data would\nbreak the structure and if i look at all\nthe data then i would certainly call it\nunstructured data because there is no\nclear schema which can define this data\nnow this is what i mean by variety of\ndata that is structured data which\nbasically has a schema or has a format\nwhich could be easily understood you\nhave semi-structured which could be like\nan xml or json or even your excel sheets\nwhere you could have some data which is\nstructured and the other is unstructured\nand when we talk about unstructured we\nare talking about absence of schema it\ndoes not have a format it does not have\na schema and it is hard to analyze which\nbrings its own challenges the next\naspect is value now value refers to the\nability to turn your data useful for\nbusiness you would have lot of data\nwhich is being collected as we mentioned\nin previous slides right there would be\nlot of data wrangling or data\npre-processing or cleaning up of data\nhappening and then finally you would\nwant to draw value from that data but\nfrom all the data collected what\npercentage of data gives us value and if\nall my data can give me value then why\nwouldn't i use it this is an aspect of\nbig data right veracity now this means\nthe quality of data billions of dollars\nare lost every year by organizations\nbecause the data which was collected was\nnot of good quality or probably they\ncollected a lot of data and then it was\nerroneous take an example of autonomous\ndriving projects which are\nhappening in europe or u.s where there\nare car fleets which are on the road\ncollecting data via radar sensors and\ncamera sensors and when this data has to\nbe processed to train algorithms it has\nrealized that sometimes the data which\nwas collected was missing in some values\nmight be was not appropriate or had a\nlot of errors and all this process of\ncollecting the data becomes a repetitive\ntask because the quality of data was not\ngood this is just one example we can\ntake example from healthcare industry or\nstock markets or financial institutions\nand so on so extracting loads of data is\nnot useful if the data is messy or poor\nin quality and that basically means that\nvelocity is a very important v of big\ndata now apart from veracity volume\nvariety velocity and value we have the\nother v such as viscosity how dense the\ndata is\nvalidity is the data still valid\nvolatility is my data volatile or\nvirality is the data viral now all of\nthese different v's categorize the data\nas big data now here we would like to\ntalk on a big data case study and we\nhave taken an example of google which\nobviously is one of the companies which\nis\nchurning and working on huge amount of\ndata now it's actually said that if you\ncompare one grain of sand with one byte\nof data then google is processing or\ngoogle is handling whole worlds sand\nevery week that is the kind of data\nwhich google is processing now in early\n2000 and since then when the number of\ninternet users started growing google\nalso faced lot of problems in storing\nincreasing user data and\nusing the traditional servers to manage\nthat now that was a challenge which\ngoogle started facing could they use\ntraditional data server to store the\ndata well yes they could right storage\ndevices have been getting cheaper day by\nday but then how much time does it take\nto retrieve that data what is the seek\ntime what is the time taken to read and\nprocess that data thousands of search\nqueries were raised per second no doubt\nnow we could say millions and billions\nof queries are raised per second every\nquery read 100 mbs\nof data and consumed tens of billions of\ncpu cycles based on these queries so the\nrequirement was that they wanted to have\na large\ndistributed highly fault tolerant file\nsystem large to store to capture process\nhuge amount of data distributed because\nthey could not rely just on one server\neven if that had multiple disks stacked\nup that was not an efficient choice what\nwould happen if this particular machine\nfailed what would happen if the whole\nserver was down so they needed a\ndistributed storage and a distributed\ncomputing environment they needed\nsomething which can be highly fault\ntolerant right so this was the\nrequirement which google had and the\nsolution which came out as a result was\ngfs google file system now let's look at\nhow gfs works so normally in any\nparticular linux system or linux server\nyou would have a file system you would\nhave set of processes you would have set\nup files and directories which could\nstore the data gfs was different so to\nfacilitate gfs which could store huge\namount of data there was an architecture\nan architecture which had one master and\nmultiple chunk servers or you could say\nslave servers or slave machines\nmaster machine was\nto contain metadata was to contain data\nabout data when we say metadata we are\ntalking about information about data and\nthen you have the chung servers or the\nslave machines which could be storing\ndata in a distributed fashion now any\nclient or an api or an application which\nwould want to read the data would first\ncontact the master server it would\ncontact the machine where the master\nprocess was running and client would\nplace a request of reading the data or\nshowing an interest of reading the data\ninternally what it is doing is it is\nrequesting for metadata your api or an\napplication would want to know from\nwhere it can read the data master server\nwhich has metadata whether that is in\nram or disk we can discuss that later\nbut then master server would have the\nmetadata and it would know which are the\nchunk servers or the slave machines\nwhere the data was stored in a\ndistributed fashion master would respond\nback with the metadata information to\nthe client and then client could use\nthat information to read or write to\nthese slave machines where actually the\ndata was stored now this is what the\nprocess or set of processes work\ntogether to make gfs so when you say a\nchunk server we would basically have the\nfiles getting divided into fixed size\nchunks now how would they get divided so\nthere would be some kind of chunk size\nor a block size which would determine\nthat if the file is bigger than the\npre-decided chunk size then it would be\nsplit into smaller chunks and be\ndistributed across the chunk servers or\nthe slave machines if the file was\nsmaller then it would still use one\nchunk or a block to get stored on the\nunderlying slave machines so these junk\nservers or slave machines are the ones\nwhich actually store the data on local\ndisks as your linux files client which\nis interacting with master for metadata\nand then interacting with chunk servers\nfor read write operations would be the\none which would be externally connecting\nto the cluster so this is how it would\nlook so you have a master which would\nobviously be receiving some kind of\nheartbeats from the chunk servers to\nknow their status and receive\ninformation in the form of packets which\nwould let the master know which machines\nwere available for storage which\nmachines already had data and master\nwould build up the metadata within\nitself the files would be broken down\ninto chunks for example we can look at\nfile one it is broken down into chunk\none and chunk two and file two has one\nchunk which is one portion of it and\nthen you have file two residing on some\nother chunk server which also lets us\nknow that there is some kind of auto\nreplication for this file system right\nand the data which is getting stored in\nthe chunk could have a data of 64 mb now\nthat chunk size could be changed based\non the data size but google file system\nhad the basic size of the chunk as 64 mb\neach chunk would be replicated on\nmultiple servers the default replication\nwas 3 and that could again be increased\nor decreased as per requirement this\nwould also mean that if a particular\nslave machine or a chunk server would\ndie or would get killed or would crash\nthere would never be any data loss\nbecause a replica of data residing on\nthe failed machine would still be\navailable on some other slave server\nchunk server or slave machine now this\nhelped google to store and process huge\nvolumes of data in a distributed manner\nand does have a fault tolerant\ndistributed scalable storage which could\nallow them to store a huge amount of\ndata now that was just one example which\nactually led to the solution which today\nwe call as hadoop now when we talk about\nbig data here i would like to ask you\nsome questions that if we were talking\nabout the rdbms case take an example of\nsomething like nasa which was working on\na project called set eye search of\nextraterrestrial intelligence now this\nwas a project where they were looking\nfor a solution to take care of their\nproblem the problem was that they would\nroughly send some waves in space capture\nthose waves back and then analyze this\ndata to find if there was any\nextraterrestrial object in space now\nthey had two options for it they could\neither have a huge server built which\ncould take care of storing the data and\nprocessing it or they could go for\nvolunteer computing now volunteer\ncomputing basically means that you could\nhave a lot of people volunteering and\nbeing part of this project and what they\nwould in turn do is they would be\ndonating their ram and storage from\ntheir machines when they are not using\nit how would that happen basically\ndownload some kind of patch on their\nmachine which would run as a screen\nsaver and if the user is not using his\nmachine some portion of data could be\ntransferred to these machines for\nintermittent storage and processing\nusing ram now this sounds very\ninteresting and this sounds very easy\nhowever it would have its own challenges\nright think about security think about\nintegrity but those those problems are\nnot bigger as much as is the requirement\nof bandwidth and this is the same thing\nwhich happens in rdbms if you would have\nto move data from archived solution to\nthe processing layer that would consume\nhuge amount of bandwidth big data brings\nits own challenges\nhuge amount of data is getting generated\nevery day now the biggest challenge is\nstoring this huge volume of data and\nespecially when this data is getting\ngenerated with a lot of variety where it\ncan have different kind of formats where\nit could be viral it could be having a\nlot of value and nobody has looked into\nthe veracity of data but the primary\nproblem would be handling this huge\nvolume of data variety of the data would\nbring in challenges of storing it in\nlegacy systems if processing of the data\nwas required now here again i would\nsuggest you need to think what is the\ndifference between reading a data and\nprocessing a data so reading might just\nmean bringing in the data from disk and\ndoing some io operations and processing\nwould mean reading the data probably\ndoing some transformations on it\nextracting some useful information from\nit and then storing it in the same\nformat or probably in a different format\nso processing this massive volume of\ndata is the second challenge\norganizations don't just store their big\ndata they would eventually want to use\nit to process it to gather some insights\nnow processing and extracting insights\nfrom big data would take huge amount of\ntime unless and until there was an\nefficient solution to handle and process\nthis big data securing the data that's\nagain a concern for organizations right\nencryption of big data is difficult to\nperform if you would think about\ndifferent compression mechanisms then\nthat would also mean decompressing of\ndata which would also mean that you\ncould take a hit on the cpu cycles or on\ndisk usage providing user authentication\nfor every team member now that could\nalso be dangerous so that\nled to hadoop as a solution so big data\nbrings its own challenges big data\nbrings its own benefits and here we have\na solution which is hadoop now what is\nhadoop it's an open source framework for\nstoring data and running applications on\nclusters of commodity hardware hadoop is\nan open source framework and before we\ndiscuss on two main components of hadoop\nit would be good to look into the link\nwhich i was suggesting earlier that is\ncompanies using hadoop and any person\nwho would be interested in learning big\ndata should start somewhere here where\nyou could list down different companies\nwhat kind of setup they have why are\nthey having hadoop what kind of\nprocessing they are doing and how are\nthey using these so called hadoop\nclusters to process and in fact store\ncapture and process huge amount of data\nanother link which i would suggest is\nlooking at different distributions\nof hadoop any person who is interested\nin learning in big data should know\nabout different distributions of hadoop\nnow in linux we have different\ndistributions like ubuntu centos red hat\nsusie debian in the same way you have\ndifferent distributions of hadoop which\nwe can look on the wiki page and this is\nthe link which talks about products that\ninclude apache hadoop or derivative\nworks and commercial support which\nbasically means that apache hadoop the\nsole products that can be called a\nrelease of apache hadoop come from\napache.org that's an open source\ncommunity and then you have various\nvendor-specific distributions like\namazon web services you have cloud era\nyou have hortonworks you have ibm's big\ninside you have mapper all these are\ndifferent distributions of hadoop so\nbasically all of these vendor-specific\ndistributions are depending on using on\ncore apache hadoop in brief we can say\nthat these are the vendors which take up\nthe apache hadoop package it within a\ncluster management solution so that\nusers who intend to use apache hadoop\nwould not have difficulties of setting\nup a cluster setting up a framework they\ncould just use a vendor-specific\ndistribution with its cluster\ninstallation solutions cluster\nmanagement solution and easily plan\ndeploy install and manage your cluster\nlet's rewind to the days before the\nworld turned digital\nback then miniscule amounts of data were\ngenerated at a relatively sluggish pace\nall the data was mostly documents and in\nthe form of rows and columns storing or\nprocessing this data wasn't much trouble\nas a single storage unit and processor\ncombination would do the job but as\nyears passed by the internet took the\nworld by storm giving rise to tons of\ndata generated in a multitude of forms\nand formats every microsecond\nsemi-structured and unstructured data\nwas available now in the form of emails\nimages audio and video to name a few\nall this data became collectively known\nas big data\nalthough fascinating it became nearly\nimpossible to handle this big data and a\nstorage unit processor combination was\nobviously not enough\nso what was the solution\nmultiple storage units and processors\nwere undoubtedly the need of the hour\nthis concept was incorporated in the\nframework of hadoop that could store and\nprocess vast amounts of any data\nefficiently using a cluster of commodity\nhardware hadoop consisted of three\ncomponents that were specifically\ndesigned to work on big data in order to\ncapitalize on data the first step is\nstoring it the first component of hadoop\nis its storage unit the hadoop\ndistributed file system or hdfs\nstoring massive data on one computer is\nunfeasible hence data is distributed\namongst many computers and stored in\nblocks\nso if you have 600 megabytes of data to\nbe stored hdfs splits the data into\nmultiple blocks of data that are then\nstored on several data nodes in the\ncluster 128 megabytes is the default\nsize of each block\nhence 600 megabytes will be split into\nfour blocks a b c and d of 128 megabytes\neach and the remaining 88 megabytes in\nthe last block e\nso now you might be wondering what if\none data node crashes\ndo we lose that specific piece of data\nwell\nno that's the beauty of hdfs\nhdfs makes copies of the data and stores\nit across multiple systems\nfor example when block a is created it\nis replicated with a replication factor\nof three and stored on different data\nnodes\nthis is termed the replication method by\ndoing so data is not lost at any cost\neven if one data node crashes making\nhdfs fault tolerant after storing the\ndata successfully it needs to be\nprocessed this is where the second\ncomponent of hadoop mapreduce comes into\nplay\nin the traditional data processing\nmethod entire data would be processed on\na single machine having a single\nprocessor this consumed time and was\ninefficient especially when processing\nlarge volumes of a variety of data to\novercome this mapreduce splits data into\nparts and processes each of them\nseparately on different data nodes\nthe individual results are then\naggregated to give the final output\nlet's try to count the number of\noccurrences of words taking this example\nfirst the input is split into five\nseparate parts based on full stops the\nnext step is the mapper phase where the\noccurrence of each word is counted and\nallocated a number\nafter that depending on the words\nsimilar words are shuffled sorted and\ngrouped following which in the reducer\nphase all the grouped words are given\naccount\nfinally the output is displayed by\naggregating the results all this is done\nby writing a simple program\nsimilarly mapreduce processes each part\nof big data individually and then sums\nthe result at the end\nthis improves load balancing and saves a\nconsiderable amount of time\nnow that we have our mapreduce job ready\nit is time for us to run it on the\nhadoop cluster\nthis is done with the help of a set of\nresources such as ram network bandwidth\nand cpu\nmultiple jobs are run on hadoop\nsimultaneously and each of them needs\nsome resources to complete the task\nsuccessfully\nto efficiently manage these resources we\nhave the third component of hadoop which\nis yarn\nyet another resource negotiator or yarn\nconsists of a resource manager node\nmanager application master and\ncontainers the resource manager assigns\nresources node managers handle the nodes\nand monitor the resource usage in the\nnode the containers hold a collection of\nphysical resources\nsuppose we want to process the mapreduce\njob we had created\nfirst the application master requests\nthe container from the node manager\nonce the node manager gets the resources\nit sends them to the resource manager\nthis way yarn processes job requests and\nmanages cluster resources in hadoop\nin addition to these components hadoop\nalso has various big data tools and\nframeworks dedicated to managing\nprocessing and analyzing data the hadoop\necosystem comprises several other\ncomponents like hive pig apache spark\nflume and scoop to name a few\nthe hadoop ecosystem works together on\nbig data management before we dive into\nthe technical side of hadoop we're going\nto take a little detour to try to give\nyou a visual understanding and relate it\nto maybe a more life setup and we're\ngoing to go to the farm in this case so\nwe have in a farm far away i almost wish\nthey'd put far far away it does remind\nme a little bit of a star wars theme so\nwe're going to look at fruit at a farm\nwe have jack who harvests his grapes and\nthen sells it in the nearby town after\nharvesting he stores his produce in a\nstorage shed or a storage room in this\ncase we found out though is there was a\nhigh demand for other fruits so he\nstarted harvesting apples and oranges as\nwell hopefully he has a couple fills\nwith these different fruit trees growing\nand he set up there and you can see that\nhe's working hard to harvest all these\ndifferent fruits but he has a problem\nhere because there's only one of him so\nhe can't really do more work so what he\nneeds to do then is hire two more people\nto work with him with this harvesting is\ndone simultaneously so instead of him\ntrying to harvest all this different\nfruit he now has two more people in\nthere who are putting their food away\nand harvesting it for them now the\nstorage room becomes a bottleneck to\nstore and access all the fruits in a\nsingle storage area so they can't fit\nall the fruit in one place so jack\ndecides to distribute the storage area\nand give each one of them a separate\nstorage and you can look at this\ncomputer terms we have our people that\nare the processors we have our fruit\nthat's a data and you can see it's\nstoring it in the different storage\nrooms so you can see me popping up there\ngetting my hello i want fruit basket of\nthree grapes two apples and three\noranges i'm getting ready for a\nbreakfast with family a little large\nfamily my family's not that large to\ncomplete the order on time all of them\nwork parallelly with their own storage\nspace so here we have a process of\nretrieving or querying the data and you\ncan see from the one storage space he\npulls out three grapes she pulls out two\napples and then another storage room he\npulls out three oranges and we complete\na nice fruit basket and this solution\nhelps them to complete the order on time\nwithout any hassle all of them are happy\nand they're prepared for an increase in\ndemand in the future so they now have\nthis growth system where you can just\nkeep hiring on new people they can\ncontinue to grow and develop a very\nlarge farm so how does this story relate\nto big data and i hinted at that a\nlittle bit earlier the limited data only\none processor one storage unit was\nneeded i remember back in the 90s they\nwould just upgrade the computer instead\nof having a small computer you would\nthen spend money for a huge mainframe\nwith all the flashing lights on it then\nthe cray computers were really massive\nnowadays a lot of the computers that\nsits on our desktop are powerful as the\nmainframes they had back then so it's\npretty amazing how time has changed but\nused to be able to do everything on one\ncomputer and you had structured data and\na database you stored your structured\ndata in so most of the time you're\nacquiring databases sql queries just\nthink of it as a giant spreadsheet with\nrows and columns where everything has a\nvery specific size and fits neatly in\nthat rows and columns and back in the\n90s this was a nice setup you just\nupgraded your computer you would get\nyourself a nice big sun computer or\nmainframe if you had a lot of data and a\nlot of stuff going on and it was very\neasy to do soon though the data\ngeneration increased leading to high\nvolume of data along with different data\nformats and so you can imagine in\ntoday's world this year we will generate\nmore data than all the previous years\nsummed together we will generate more\ndata just this year than all the\nprevious years some together and that's\nthe way it's been going for some time\nand you can see we have a variety of\ndata we have our structured data which\nis what we're you think about a database\nwith rows and columns and easy to look\nat nice spreadsheet we have our\nsemi-structured data they have emails as\nan example here that would be one\nexample your xml your html web pages and\nwe have unstructured data if you ever\nlook through your folder on photos i\nhave photos that were taken on my phone\nwith high quality i've got photos from a\nlong time ago i got web photos low\nquality so just in my pictures alone\nnone of them are the same you know there\ncertainly are groups of them that are\nbut overall there are a lot of variety\nin size and setup so a single processor\nwas not enough to process such high\nvolume of different kinds of data as it\nwas very time consuming you can imagine\nthat if you are twitter with millions of\ntwitter feeds you're not going to be\nable to do a query across one server\nthere's just no way that's going to\nhappen unless people don't mind waiting\na year to get the history of their\ntweets or look something up hence we\nstart doing multiple processors so\nthey're used to process high volume of\ndata and this saved time so we're moving\nforward we got multiple processors the\nsingle storage unit became the\nbottleneck due to which network overhead\nwas generated so now you have your\nnetwork coming in and each one of these\nservers has to wait before it can grab\nthe data from the\nsingle stored unit maybe you have a sql\nserver there with a nice setup or a file\nsystem going the solution was to use\ndistributed storage for each processor\nthis enabled easy access to storage and\naccess to data so this makes a lot of\nsense you have multiple workers multiple\nstorage units just like we had our\nstorage room and the different fruit\ncoming in your variety you can see that\nnice parallel to working on a farm now\nwe're dealing with a lot of data it's\nall about the data now this method\nworked and there were no network\noverhead generated you're not getting a\nbottleneck somewhere where people are\njust waiting for data being pulled or\nbeing processed this is known as\nparallel processing with distributed\nstorage so parallel processing\ndistributed storage and you can see here\nthe parallel processing is your\ndifferent computers running the\nprocesses and distributed storage here\nis a quick demo on setting up cloudera\nquick start vm in case you are\ninterested in working on a standalone\ncluster you can download the cloudera\nquick start vm so you can just type in\ndownload cloudera quick start vm and you\ncan search for package now this can be\nused to set up a quick start vm which\nwould be a single node cloudera based\ncluster so you can click on this link\nand then basically based on the platform\nwhich you would be choosing to install\nsuch as using a vm box or which version\nof cloudera you would install so here i\ncan select a platform so i can choose\nbox and then you can click on get it now\nso give your details and basically then\nit should allow you to download the\nquick start vm which would look\nsomething like this and once you have\nthe zip file which is downloaded you can\nunzip it which can then be used to set\nup a single node cloud error cluster so\nonce you have downloaded the zip file\nthat would look something like this so\nyou would have a quick start virtual box\nand then a virtual boss disk now this\ncan be used to set up a cluster ignore\nthese files which are related to amazon\nmachines and we you don't need to have\nthat so you would just have this and\nthis can be used to set up a cloud error\ncluster so for this to be set up you can\nclick on file import appliance and here\nyou can choose your quick start vm by\nlooking into downloads quick start vm\nselect this and click on open now you\ncan click on next and that shows you the\nspecifications of cpu ram which we can\nthen change later and click on import\nthis will start importing virtual disk\nimage dot vmdk file into your vm box\nonce this is done we will have to change\nthe specifications or machines to use\ntwo cpu cores minimum and give a little\nmore ram because cloudera quickstart vm\nis very cpu intensive and it needs good\namount of ram so to survive i will give\ntwo cpu cores and 5gb ram and that\nshould be enough for us to bring up a\nquick start vm which gives us a cloudera\ndistribution of hadoop in a single node\ncluster setup which can be used for\nworking learning about different\ndistributions in cloudera clusters\nworking with sdfs and other hadoop\necosystem components let's just wait for\nthis importing to finish and then we\nwill go ahead and set up a quick start\nvm for our practice here the importing\nof appliance is done and we see cloudera\nquickstart machine is added to my list\nof machines i can click on this and\nclick on settings as mentioned i would\nlike to give it more ram and more cpu\ncores so click on system and here let's\nincrease the ram to at least\nfive and click on processor and let's\ngive it two cpu cores which would at\nleast be better than using one cpu core\nnetwork it goes for nat and that's fine\nclick on ok and we would want to start\nthis machine so that it uses two cpu\ncores 5gb ram and it should bring up my\ncloudera quick start vm now let's go\nahead and start this machine which has\nour quick start vm it might take\ninitially some time to start up because\ninternally there will be various\ncloudera services which will be starting\nup and those services need to be up for\nour cloud era quick start vm to be\naccessible so unlike your apache hadoop\ncluster where we start our cluster and\nwe will be starting all our processes in\ncase of cloudera it is your cloudera scm\nserver and agents which take care of\nstarting up of your services and\nstarting up of your different roles for\nthose services i explained in my\nprevious session that for a cloud era\ncluster it would be these services let\nme just show you that so in case of\napache cluster we start our services\nthat is we start our cluster by running\nscript and then basically those scripts\nwill individually start the different\nprocesses on different nodes in case of\ncloud era we would always have a\ncloudera scm server which would be\nrunning on one machine and then\nincluding that machine we would have\nclouded icm agents which would be\nrunning on multiple machines similarly\nif we had a hortonworks cluster we would\nhave ambari server starting up on the\nfirst machine and then embody agents\nrunning on other machines so your server\ncomponent knows what are the services\nwhich are set up what are their\nconfigurations and agents running on\nevery node are responsible to send\nheartbeats to the server receive\ninstructions and then take care of\nstarting and stopping off of individual\nroles on different machines in case of\nour single node cluster setup in quick\nstart vm we would just have one scm\nserver and one scm agent which will\nstart on the machine which will then\ntake care of all the roles which need to\nbe started for your different services\nso we will just wait for our machine to\ncome up and basically have cloud sem\nserver and agent running and once we\nhave that we need to follow few steps so\nthat we can have the cloudera admin\nconsole accessible which allows you to\nbrowse the cluster look at different\nservices look at the roles for different\nservices and also work with your cluster\neither using command line or using the\nweb interface that is\nnow that my machine has come up and it\nalready is connected to the internet\nwhich we can see here we need to do\ncertain things so that we can have our\nadmin console accessible at this point\nof time you can click on terminal and\ncheck if you have access to the cluster\nso here type in host name and that shows\nyou your host name which is\nquickstart.cloudera we can also type in\nhdfs command to see if we have access\nand if my cluster is working these\ncommands are same as you would give them\nin a apache hadoop cluster or in any\nother distribution of a loop sometimes\nwhen your cluster is up and you have\naccess to the terminal it might take few\nseconds or few minutes before there is a\nconnection established between cloudera\ncm server and cloudera cm agent running\nin the background which takes care of\nyour cluster i have given a sdfs dfs\nlist command which basically should show\nme what by default exists on my sdfs\nlet's just give it a couple of seconds\nbefore it shows us the output we can\nalso check by giving a service cloudera\nscm server status and here it tells me\nthat if you would want to use cloudera\nexpress free run this command it needs 8\ngb of ram and it leads to virtual cpu\ncores and it also mentions it may take\nseveral minutes before cloudera manager\nhas started i can login as root here and\nthen give the command service cloud\nerror scm server status remember the\npassword for root is cloudera so it\nbasically says that if you would want to\ncheck the settings it is good to have\nexpress edition running so we can close\nthis my sdfs access is working fine\nlet's close the terminal and here we\nhave launch cloud error express click on\nthis and that will give you that you\nneed to give a command which is force\nlet's copy this command let's open a\ndifferent terminal and let's give this\ncommand like this which will then go\nahead and shut down your cloudera based\nservices and then it will restart it\nonly after which you will be able to\naccess your admin console so let's just\ngive it a couple of minutes before it\ndoes this and then we will have access\nto our admin console here if you see it\nis starting the cloudera manager server\nagain it is waiting for cloudera manager\napi then starting the cloudera manager\nagents and then configuring the\ndeployment as per the new settings which\nwe have given as to use the express\nedition of cloudera once all this is\ndone it will say the cluster has been\nrestarted and the admin console can be\naccessed by id and password as cloudera\nwe'll give it a couple of more minutes\nand once this is done we are ready to\nuse our admin console now that\ndeployment has been configured client\nconfigurations have also been deployed\nand it has restarted the cloudera\nmanagement service it gives you an\naccess to quick start admin console\nusing username and password as cloud\nerror let's try accessing it so we can\nopen up the browser here and let's\nchange this to 7180\nthat's the default port and that shows\nthe admin console which is coming up now\nhere we can log in as cloud error cloud\nerror and then let's click on login now\nas i said cloudera is very cpu intensive\nand memory intensive so it would slow\ndown since we have not given enough gb\nram to our cloud error cluster and thus\nit will be advisable to stop or even\nremove the services which we don't need\nnow as of now if we look at the services\nall of them look in a stop status and\nthat's good in one way because we can\nthen go ahead and remove the services\nwhich we will not use in the beginning\nand later we can anytime add services to\nthe cluster so for example i can click\non key value store here and then i can\nscroll down where it says delete to\nremove this service from the admin\nconsole now anytime you are removing a\nparticular service it will only remove\nthe service from the management by\ncloudera manager all the role groups\nunder this service will be removed from\nhost templates so we can click on delete\nnow if this service was depending on\nsome other service it would have\nprompted me with a message that remove\nthe relevant services on which this\nparticular service depends if the\nservice was already running then it\nwould have given me a message that the\nservice has to be stopped before it can\nbe deleted from the cloudera admin\nconsole now this is my admin console\nwhich allows you to click on services\nlook at the different roles and\nprocesses which are running for this\nservice we anyways have access to our\ncloudera cluster from the terminal using\nour regular sdfs or yarn or maplet\ncommands now i removed a service i will\nalso remove solar which we will not be\nusing for the beginning but then it\ndepends on your choice so we can here\nscroll down to delete it and that says\nthat before deleting the solar service\nyou must remove the dependencies on the\nservice from the configuration of\nfollowing services that is hue now hue\nis a web interface which allows you to\nwork with your sdfs and that is\ndepending on this so click on configure\nservice dependency and here we can make\nsure that our hue service does not\ndepend on a particular service we are\nremoving so that then we can have a\nclean removal of the service so i'll\nclick on none and i will say save\nchanges once this is done then we can go\nahead and try removing the solar service\nfrom our admin console which will reduce\nsome load on my management console which\nwill also allow me to work faster on my\ncluster now here we have removed the\ndependency of hue on solar so we can\nclick on this and then we can delete it\nremember i'm only doing this so that my\ncluster becomes little lighter and i can\nwork on my focus services at any point\nof time if you want to add more services\nto your cluster you can anytime do that\nyou can fix different configuration\nissues like what we see here with\ndifferent warning messages and here we\nhave these services which are already\nexisting now if we don't need any of the\nservice i can click on the drop down and\nclick on delete again this says that\nscoop 2 also has\nrelevance to hue so hue as a web\ninterface also depends on scope 2. as of\nnow we'll make it none at any point of\ntime later you can add the services by\nclicking the add service option now this\nis a cluster to which you have admin\naccess and this is a quick start vm\nwhich gives you a single node cloud\nerror cluster which you can use for\nlearning and practicing so here we will\nclick on scope 2 and then we will say\ndelete as we have configured the\ndependency now and we will remove scope\n2 also from the list of services which\nyour admin console is managing right so\nonce this is done we have removed three\nservices which we did not need we can\neven remove scope as a client and if we\nneed we can add that later now there are\nvarious other alerts which your cloudera\nadmin console shows and we can always\nfix them by clicking on the health\nissues or configuration issues we can\nclick here and see what is the health\nissue it is pointing to if that is a\ncritical one or if that can be ignored\nso it says there is an issue with a\nclock offset which basically relates to\nan ntp service network time protocol\nwhich makes sure that one or multiple\nmachines are in the same time zone and\nare in sync so for now we can click on\nsuppress and we can just say suppress\nfor all hosts and we can say look into\nit later and confirm so now we will not\nhave that health issue reported that\nprobably the ntp service and the\nmachines might not be in sync now that\ndoes not have an impact for our use case\nas of now but if we have a kerberos kind\nof setup which is for security then\nbasically this offset and time zone\nbecomes important so we can ignore this\nmessage and we are still good to use the\ncluster we also have other configuration\nissues and you can click on this which\nmight talk about the heap size or the\nram which is available for machines it\ntalks about zookeeper should be in odd\nnumbers q does not have a load balancer\nsdfs only has one data node but all of\nthese issues are not to be worried upon\nbecause this is a single node cluster\nsetup so if you want to avoid all of\nthese warnings you can always click on\nsuppress and you can avoid and let your\ncluster be in all green status but\nthat's nothing to worry so we can click\non cluster and basically we can look at\nthe services so we have removed some\nservices which we don't intend to use\nnow i have also suppressed a offset\nwarning which is not very critical for\nmy use case and basically i am good to\nstart the cluster at any point of time\nas i said if you would want to add\nservices this is the actions button\nwhich you can use to add service so we\nwill just say restart my cluster which\nwill restart all the services one by one\nstarting from zookeeper as the first\nservice to come up we can always click\non this arrow mark and see what is\nhappening in the services what services\nare coming up and in which order if you\nhave any issues you can always click on\nthe link next to it which will take you\nto the logs and we can click on close to\nlet it happen in the background so this\nwill basically let my services restart\none by one and my cluster will then\nbecome completely accessible either\nusing hue as a web interface or quick\nstart terminal which allows you to give\nyour commands now while my machines are\ncoming up you can click on hosts and you\ncan have a look at all the hosts we have\nas of now only one which will also tell\nyou how many roles or processes are\nrunning on this machine so that is 25\nrolls it tells you what is the disk\nusage it tells you what is the physical\nmemory being used and using this host\ntab we can add new host to the cluster\nwe can check the configuration we can\ncheck all the hosts in diagnostics you\ncan look at the logs which will give you\naccess to all the logs you can even\nselect the sources from which you would\nwant to have the logs or you can give\nthe host name you can click on search\nyou can build your own charts you can\nalso do the admin stuff by adding\ndifferent users or enabling security\nusing the administration tab so since we\nhave clicked on restart of a cluster we\nwill slowly start seeing all the\nservices one by one coming up starting\nwith zookeeper to begin with and once we\nhave our cluster up and running whether\nthat is showing all services in green or\nin a different status we still should be\nable to access the service now as we saw\nin apache hadoop cluster even here we\ncan click on sdfs and we can access the\nweb ui once our sdfs service is up by\nclicking on quick links so the service\nis not yet up once it is up we should be\nable to see the web ui link which will\nallow you to check things from sdfs web\ninterface similarly yarn as a service\nalso has a web interface so as soon as\nthe service comes up under your quick\nlinks we will have access to the yarn ui\nand similarly once the service comes up\nwe will have access to hue which will\ngive you the web interface which allows\nyou to work with your sdfs which allows\nyou to work with your different other\ncomponents within the cluster without\neven using the command line tools or\ncommand line options so we will have to\ngive it some time while the cloud error\nscm agent on every machine will be able\nto restart the roles which are\nresponsible for your cluster to come up\nwe can always click here with tells that\nthere are some running commands in the\nbackground which are trying to start my\ncluster we can go to the terminal and we\ncan switch as hdfs user remember sdfs\nuser is the admin user and it does not\nhave a password unless you have set one\nso you can just log in as sdfs which\nmight ask you for a password initially\nwhich we do not have so the best way to\ndo this is by logging in as root where\nthe password is cloud error and then you\ncan log in as hdfs so that then onwards\nyou can give your sdfs commands to work\nwith your file system now since my\nservices are coming up right now when i\ntry to give a sdfs dfs command it might\nnot work or it might also say that it is\ntrying to connect to the name node which\nis not up yet so we will have to give it\nsome time and only once the name node is\nup we will be able to access our sdfs\nusing commands so this is how you can\nquickly set up your quick start and then\nyou can be working using the command\nline options from the terminal like what\nyou would do in apache hadoop cluster\nyou could use the web interfaces which\nallow you to work with your cluster now\nthis usually takes more time so you will\nhave to give it some time before your\nservices are up and running and for any\nreason if you have issues it might\nrequire you to restart your cluster\nseveral times in the beginning before it\ngets accustomed to the settings what you\nhave given and it starts up the services\nat any point of time if you have any\nerror message then you can always go\nback and look in logs and see what is\nhappening and try starting your cluster\nso this is how we set up a quick start\nvm and you can be using this to work\nwith your cloud error clusters we'll\nstart with the big data challenges and\nthe first thing with the big data is you\ncan see here we have a nice chaotic\nimage with all these different inputs\nserver racks all over the place graphs\nbeing generated just about everything\nyou can imagine and so the problems that\ncome up with big data is one storing it\nhow do you store this massive amount of\ndata and we're not talking about a\nterabyte or 10 terabytes we're talking a\nminimal of 10 terabytes up to petabytes\nof data and then the next question is\nprocessing and so the two go hand in\nhand because when you're storing the\ndata that might take up a huge amount of\nspace or you might have a small amount\nof data that takes a lot of processing\nand so either one will drive a series of\ndata or processing into the big data\narena so with storing data storing big\ndata was a problem due to its massive\nvolume just straight up people would\nhave huge backup tapes and then you'd\nhave to go through the backup tapes for\nhours to go find your data and a simple\nquery could take days and then\nprocessing processing big data consumed\nmore time and so the hadoop came up with\na cheap way to process the data it used\nto be like i've had some processes that\nif i ran on my computer without trying\nto use multiple cores and multiple\nthreads would take years to process just\na simple data analysis can get that\nheavy in the data processing and so\nprocessing can be as big of a problem as\nthe size of the data itself so hadoop as\na solution this is the solution to big\ndata and big data storage storing big\ndata was a problem due to its massive\nvolume so we take the hadoop file system\nor the hdfs and now we're able to store\nhuge data across a large number of\nmachines and access it like this one\nfile system and processing big data\nconsumed more time we talked about some\nprocesses you can't even do on your\ncomputer because it would take years now\nthe hadoop with the map reduce\nprocessing big data was faster and i'm\ngoing to add a little notation right\nhere that's really important to note\nthat hadoop is the beginning when we\ntalk about data processing they've added\nnew processes on top of the map reduce\nthat even accelerate it your spark setup\nand some other different functionalities\nbut really the basis of all of it where\nit all starts with the most basic\nconcept is your hadoop mapreduce let us\nnow look into the hdfs in detail in the\ntraditional approach all the data was\nstored in a single central database with\nthe rise of big data a single database\nwas not enough for storage and i\nremember the old sun computers or the\nhuge ibm machines with all the flashing\nlights now all the data on one of those\ncan be stored on your phone it's almost\na bit of humor how much this has\naccelerated over the years the same\nthing with our rise of big data no\nlonger can you just store it on one\nmachine no longer can you go out and buy\na sun computer and put it on that one\nsun computer no more can you buy a craig\nmachine or an enterprise ibm server it's\nnot going to work you're not going to\nfit it all into one server the solution\nwas to use distributed approach to store\nthe massive amount of data data was\ndivided and distributed amongst many\nindividual databases and you can see\nhere where we have three different\ndatabases going on so you might actually\nsaw this in one where they divided up\nthe user accounts a through g so on by\nthe letter and so the first query would\nsay what's the first letter of this uh\nwhatever id it was and then it would go\ninto that database to find it so i had a\ndatabase telling it which database to\nlook for stuff that was a long time ago\nand to be honest it didn't work really\nwell nowadays so that was a distributed\ndatabase you'd have to track which\ndatabase you put it in so what is hdfs\nhadoop distributed file system hdfs is a\nspecially designed file system for\nstoring huge data sets in commodity\nhardware and commodities an interesting\nterm because i mentioned enterprise\nversus commodity and i'll touch back\nupon that it has two core components\nname node and data node name node is the\nmaster\ndaemon there's only one active name node\nit manages the data nodes and stores all\nthe metadata so it stores all the\nmapping of where the data is on your\nhadoop file system now the name node is\nusually an enterprise machine you spend\na lot of extra money on it so you have a\nvery solid name known machine and so\nthen we have our data nodes our data\nnode data node data node we have three\nhere the data node is the slave there\ncan be multiple data nodes and it stores\nthe actual data and this is where your\ncommodity hardware comes in the best\ndefinition i've heard of commodity\nhardware is the cheap knockoffs this is\nwhere you buy you can buy 10 of these\nand you expect one of them not to work\nbecause you know when they come in\nthey're going to break right away so\nyou're looking at hardware it's not as\nhigh-end so where you might have your\nmain master node is your enterprise\nserver then your data nodes are just as\ncheap as you can get them with all the\ndifferent features you need on them as\nsaid earlier name node stores the\nmetadata metadata gives information\nregarding the file location block size\nand so on so our metadata in the hdfs is\nmaintained by using two files it has the\nedit log and the fs image the edit log\nkeeps track of the recent changes made\non the hadoop file system only recent\nchanges are tracked here the fs image\nkeeps track of every change made on the\nhdfs since the beginning now what\nhappens when the edit log file size\nincreases the name node fails these are\nbig ones so what happens we get our edit\nlog it just keeps so big until it's too\nbig or our main enterprise computer that\nwe spent all that money on so it would\nbreak actually fails because they still\nfail the solution we make copies of the\nedit log and the fs image files so\nthat's pretty straightforward you just\ncopy them over so you have both the most\nrecent edits going on and the long term\nimage of your file system and then we\nalso create a secondary name node is a\nnode that maintains a copies of the edit\nlog and the fs image it combines them\nboth to get an updated version of the fs\nimage now the secondary name node only\ncame in the last oh i guess two three\nyears where it became as part of the\nmain system and usually your secondary\nname node is also an enterprise computer\nand you'll put them on separate racks so\nif you have three racks of computers you\nwould have maybe the first two racks\nwould have the name node and the second\nrack would have the secondary name note\nand the reason you put them on different\nracks is you can have a whole rack go\ndown you could have somebody literally\ntrip over the power cable or the switch\nthat goes between the racks is most\ncommon goes down well if the switch goes\ndown you can easily switch to the\nsecondary name node and while you're\ngetting your switch replaced and\nreplacing that hardware because of the\nway the hdfs works it still is\ncompletely functional so let's take a\nlook at the name node we have our edit\nlog our fs image and we have our\nsecondary name node and you take that it\ncopies the edit log over and it copies\nthe fs image and you can see right here\nyou have all the different contents on\nyour main name node now also on your\nsecondary name node and then your\nsecondary name node will actually take\nthese two your edit log and your fs\nimage and it will make a copy so you\nhave a full fs image that contains its\ncurrent it's up to date the secondary\nname node creates a periodic checkpoint\nof the files and then it updates the new\nfs image into the name node now used to\nbe this all occurred on the name node\nbefore you had a secondary name note so\nnow you can use your secondary node to\nboth back up everything going on the\nname node and it does that lifting in\nthe back where you're combining your\nedit log and bringing your fs image so\nit's current and then you end up with a\nnew edit log and a new you have your fs\nimage updated and you start a fresh edit\nlog this process of updating happens\nevery hour and that's how it's scheduled\nyou can actually change those schedules\nbut that is the standard is to update\nevery hour so let's we took a look at\nthe the master node and the you know the\nname node and the secondary name node\nlet's take a look at the cluster\narchitecture of our hadoop file system\nthe hdfs cluster architecture so we have\nour name node and it stores the metadata\nand we have our block location so we\nhave our fs image plus our edit log and\nthen we have the backup fs image and\nedit log and then we have so you have\nyour rack we have our switch on top\nremember i was talking about the switch\nthat's the most common thing to go in\nthe rack is the switches and underneath\nthe rack you have your different data\nnodes you have your data node 1 2\nfour five maybe have 10 15 on this rack\nyou can stack them pretty high nowadays\nuh used to be you only get about 10\nservers on there but now you see racks\nthat contain a lot more and then you\nhave multiple racks so we're not talking\nabout just one rack we also have you\nknow rack 2 rack 3 4 5 6 and so on until\nyou have rack in so if you had a 100\ndata nodes we would be looking at 10\nracks of 10 data nodes each and that is\nliterally 10 commodity server computers\nhardware and we have a core switch which\nmaintains network bandwidth and connects\nthe name node to the data nodes so just\nlike each rack has a switch that\nconnects all your nodes on the rack you\nnow have core switches that connect all\nthe racks together and these also\nconnect into our name node setup so now\ni can look up your fs image and your\nedit log and pull that information your\nmetadata out so we've looked at the\narchitecture from the name node coming\ndown and you have your metadata your\nblock locations this then sorts it out\nyou have your core switches which\nconnect everything all your different\nracks and then each individual rack has\ntheir own switches which connect all the\ndifferent nodes and to the core switches\nso now let's talk about the actual data\nblocks what's actually sitting on those\ncommodity machines and so the hadoop\nfile system splits massive files into\nsmall chunks these chunks are known as\ndata blocks each file in the hadoop file\nsystem is stored as a data block and we\nhave a nice picture here where it looks\nlike a lego if you ever played with the\nlegos as a kid it's a good example we\njust stack that data right on top of\neach other but each block has to be the\nsame symmetry has to be the same size so\nthat it can track it easily and the\ndefault size of one data block is\nusually 128 megabytes now you can go in\nand change that this standard is pretty\nsolid as far as most data is concerned\nwhen we're loading up huge amounts of\ndata and there's certainly reasons to\nchange it 128 megabytes is pretty\nstandard block so why 128 megabytes if\nthe block size is smaller then there\nwill be too many data blocks along with\nlots of metadata which will create\noverhead so that's why you don't really\nwant to go smaller on these data blocks\nunless you have a very certain kind of\ndata similarly if the block size is very\nlarge then the processing time for each\nblock increases then as i pointed out\nearlier each block is the same size just\nlike your lego blocks are all the same\nbut the last block can be the same size\nor less so you might only be storing 100\nmegabytes in the last block and you can\nthink of this as if you had a terabyte\nof data that you're storing on here it's\nnot going to be exactly divided into 128\nmegabytes we just store all of it 128\nmegabytes except for the last one which\ncould have anywhere between one and 128\nmegabytes depending on how evenly your\ndata is divided now let's look into how\nfiles are stored in the hadoop file\nsystem so we have a file a text let's\nsay it's 520 megabytes we have block a\nso we take 128 megabytes out of the 520\nand we store it in block a and then we\nhave block b again we're taking 128\nmegabytes out of our 520 storing it\nthere and so on block c block d and then\nblock e we only have eight megabytes\nleft so when you add up 128 plus 128\nplus 128 plus 128 you only get 512. and\nso the last eight megabytes goes into\nits own block the final block uses only\nthe remaining space for storage data\nnode failure and replication and this is\nreally where hadoop shines this is what\nmakes it this is why you can use it with\ncommodity computers this is why you can\nhave multiple racks and have something\ngo down all the data blocks are stored\nin various data notes you take each\nblock you store it at 128 megabytes and\nthen we're gonna put it on different\nnodes so here's our block a block b\nblock c from our last example and we\nhave node one node two node three node\nfour node five node six and so each one\nof these represents a different computer\nit literally splits the data up into\ndifferent machines so what happens if\nnode five crashes well that's a big deal\ni mean we might not even have just node\nfive you might have a whole rack go down\nand if you're a company that's building\nyour whole business off of that you're\ngoing to lose a lot of money so what\ndoes happen when node 5 crashes or the\nfirst rack goes down the data stored in\nnode 5 will be unavailable as there is\nno copy stored elsewhere in this\nparticular image so the hadoop file\nsystem overcomes the issue of data node\nfailure by creating copies of the data\nthis is known as the replication method\nand you can see here we have our six\nnodes here's our block a but instead of\nstoring it just on the first machine\nwe're actually going to store it on the\nsecond and fourth nodes so now it's\nspread across three different computers\nand in this if these are on a rack one\nof these is always on a different rack\nso you might have two copies on the same\nrack but you never have all three on the\nsame rack and you always have you never\nhave more than two copies on one node\nthere's no reason to have more than one\ncopy per node and you can see we do the\nsame thing with block b block c is then\nalso spread out across the different\nmachines and same with block d and block\ne node five crashes will the data blocks\nb d and e be lost well in this example\nno because we have backups of all three\nof those on different machines the\nblocks have their copies in the other\nnodes due to which the data is not lost\neven if the node 5 crashes and again\nbecause they're also stored on different\nracks even if the whole rack goes down\nyou are still up and live with your\nhadoop file system the default\nreplication factor is three in total\nwe'll have three copies of each data\nblock now that can be changed for\ndifferent reasons or purposes but you\ngot to remember when you're looking at a\ndata center this is all in one huge room\nthese switches are connecting all these\nservers so they can shuffle the data\nback and forth really quick and that is\nvery important when you're dealing with\nbig data and you can see here each block\nis by default replicated three times\nthat's the standard there is very rare\noccasions to do four and there's even\nfewer reasons to do two blocks i've only\nseen four used once and it was because\nthey had two data centers and so each\ndata center kept two different copies\nrack awareness in the hadoop file system\nrack is a collection of 30 to 40 data\nnodes rack awareness is a concept that\nhelps to decide where the replica of the\ndata block should be stored so here we\nhave rack one we have our data node one\nto four remember saying that it used to\nbe only put ten machines and then it\nwent to twenty now it's thirty to forty\nso you can have a rack with forty\nservers on it then we have rack two and\nrack three and we put block one on there\nreplicas of block a cannot be in the\nsame rack and so i'll put the replicas\nonto a different rack and notice that\nthese are actually these two are on the\nsame rack but you'll never have all\nthree stored on the same rack in case\nthe whole rack goes down and replicas of\nblock a are created in rack two and they\nactually do they do by default create\nthe replicas onto the same rack and that\nhas to do with the data exchange and\nmaximizing your processing time and then\nwe have of course our block b and it's\nreplicated onto rack 3 and block c which\nwill then replicate onto rack 1 and so\non for all of your data all the way up\nto block d or whatever how much ever\ndata you have on there hdfs architecture\nso let's look over the architecture as a\nbigger picture we looked at the name\nnode and we store some metadata names\nreplicas home food data three so it has\nall your different infra your metadata\nstored on there and then we have our\ndata nodes and you can see our data\nnodes are each on different racks with\nour different machines and we have our\nname node and you're going to see we\nhave a heartbeat or pulse here and a lot\nof times one of the things that confuses\npeople sometimes in classes they talk\nabout nodes versus machines so you could\nhave a data node that's a hadoop data\nnode and you could also have a spark\nnode on there sparks a different\narchitecture and these are each daemons\nthat are running on these computers\nthat's why you refer to them as nodes\nand not just always as servers and\nmachines so even though i use them\ninterchangeable be aware that these\nnodes you can even have virtual machines\nif you're testing something out it\ndoesn't make sense to have 10 virtual\nnodes on one machine and deploy it\nbecause you might as well just run your\ncode on the machine so we have our\nheartbeat going on here and the\nheartbeat is a signal that data nodes\ncontinuously send to the name nodes this\nsignal shows the status of the data node\nso there's a continual pulse going up\nand saying hey i'm here i'm ready for\nwhatever instructions or data you want\nto send me and you can see here we've\ndivided up into rack 1 and rack 2 and\nour different data nodes and it also\nhave the replications we talked about\nhow to replicate data and replicate it\nin three different locations and then we\nhave a client machine and the client\nfirst requests the name node to read the\ndata now if you're not familiar with the\nclient machine the client is you the\nprogrammer the client is you've logged\nin external to this hadoop file system\nand you're sending it instructions and\nso the client whatever instructions or\nscript you're sending is first request\nthe name node to read the data the name\nnode allows the client to read the\nrequested data from the data nodes the\ndata is read from the data nodes and\nsent to the client and so you can see\nhere that it basically the the name node\nconnects the client up and says here\nhere's a data stream and now you have\nthe query that you sent out returning\nthe data you asked for and then of\ncourse it goes and finishes it and says\noh metadata operations and it goes in\nthere and finalizes your request the\nother thing the name node does is you're\nsending your information because the\nclient sends the information in there is\nyour block operations so your block\noperations are performs creation of data\nso you're going to create new files and\nfolders you're going to delete the\nfolders and also it covers the\nreplication of the folders which goes on\nin the background and so we can see here\nthat we have a nice full picture you can\nsee where the client machine comes in it\ncues the metadata the metadata goes into\nit stores a metadata and then it goes\ninto block operations and maybe you're\nsending the data to the hadoop file\nsystem maybe you're querying maybe\nyou're asking it to delete if you're\nsending data in there it then does the\nreplication on there it goes back so you\nhave your data client which is writing\nthe data into the data node and of\ncourse replicating it and that's all\npart of the block operations and so\nlet's talk a little bit about read\nmechanisms in the hadoop file system the\nhadoop file system read mechanism we\nhave our hadoop file system client\nthat's you on your computer and the\nclient jvm on the client node so we have\nour client jvm or the java virtual\nmachine that is going through and then\nyour client node and we're zooming in on\nthe read mechanism so we're looking at\nthis picture here as you can guess your\nclient is reading the data and i also\nhave another client down here writing\ndata we're going to look a little closer\nat that and so we have our name node up\nhere we have our racks of your data\nnodes and your racks of computers down\nhere and so our client the first thing\nit does is it opens a connection up with\nthe distributed file system to the hdfs\nand it goes hey can i get the block\nlocations and so it goes by using the\nrpc remote procedure call it gets those\nlocations and the name node first checks\nif the client is authorized to access\nthe requested file and if yes it then\nprovides a block location and a token to\nthe client which is showing to the slave\nfor authentication so here's the name\nnode it tells a client hey here's the\ntoken the client's going to come in and\nget this information from you and it\ntells the client oh hey here's where the\ninformation is so this is what is\ntelling your script you sent to query\nyour data whether you're writing your\nscript in one of the many setups that\nyou have available through the hadoop\nfile system or a connection through your\ncode and so you have your client machine\nat this point then reads so your fs data\ninput stream comes through and you have\nand you can see right here we did one\nand two which is verify who you are and\ngive you all the information you need\nthen three you're going to read it from\nthe input stream and then the input\nstream is going to grab it from the\ndifferent nodes where it's at and it'll\nsupply the tokens to those machines\nsaying hey this client needs this data\nhere's a token for that we're good to go\nlet me have the data the client will\nshow the authentication token to the\ndata nodes for the read process to begin\nso after reaching the end of the data\nblock the connection is closed and we\ncan see here where we've gone through\nthe different steps get block locations\nwe have step one you open up your\nconnection you get the block locations\nby using the rpc then you actively go\nthrough the fs data input stream to grab\nall those different data brings it back\ninto the client and then once it's done\nit closes down that connection and then\nonce the client or in this case the\nprogrammer you know manager has gone in\nand pig script you can do that there's\nan actual coding in each in hadoop we're\npulling data called pig or hive once you\nget that data back we close the\nconnection delete all those randomly\nhuge series of tokens so they can't be\nused anymore and then it's done with\nthat query and we can go ahead and zoom\nin just a little bit more here and let's\nlook at this even a little closer here's\nour hadoop file system client and our\nclient jvm our java virtual machine on\nthe client node and we have the data to\nbe read block a block b and so we\nrequest to read block a and b and it\ngoes into the name node two then it\nsends a location in this case um ip\naddresses of the blocks for the dn1 and\ndn2 where those blocks are stored then\nthe client interacts with the data nodes\nthrough the switches and so you have\nhere the core switch so your client node\ncomes in three and it goes to the core\nswitch and that then goes to rack switch\n1 rack switch 2 and rack switch 3. now\nif you're looking at this you'll\nautomatically see a point of failure in\nthe core switch and certainly you want a\nhigh end switch mechanism for your core\nswitch you want to use enterprise\nhardware for that and then when you get\nto the racks that's all commodity all\nyour rack switches so if one of those\ngoes down you don't care as much you\njust have to get in there and swap it in\nand out really quick you can see here we\nhave block a which is replicated three\ntimes and so is block b and it'll pull\nfrom there so we come in here and here\nthe data is read from the dn1 and dn2 as\nthey are the closest to each other and\nso you can see here that it's not going\nto read from two different racks it's\ngoing to read from one rack whatever the\nclosest setup is for that query the\nreason for this is if you have 10 other\nqueries going on you want this one to\npull all the data through one setup and\nit minimizes that traffic so the\nresponse from the data nodes to the\nclient that read the operation was\nsuccessful it says ah we've read the\ndata we're successful which is always\ngood we like to be successful i don't\nknow about you i like to be successful\nso if we look at the read mechanism\nlet's go ahead and zoom in and look at\nthe write mechanism for the hadoop file\nsystem so our hdfs write mechanism and\nso when we have the hdfs write mechanism\nhere's our client machine this is again\nthe programmer on their in computer and\nit's going through the client java\nmachine or java virtual machine the jvm\nthis is all occurring on the client node\nsomebody's office or maybe it's on the\nlocal server for the office so we have\nour name node we have data nodes and we\nhave the distributed file system so the\nclient first executes create file on the\ndistributed file system says hey i'm\ngoing to create this file over here then\nit goes through the rpc call just like\nour read did the client first executes\ncreate file on the distributed file\nsystem then the dfs interacts with the\nname node to create a file name node\nthen provides a location to write the\ndata and so here we have our hdfs client\nand the fs data output stream so this\ntime instead of the data going to the\nclient it's coming from the client and\nso here the client writes the data\nthrough the fs data output stream keep\nin mind that this output stream the\nclient could be a streaming code it\ncould be i mean you know i always refer\nto the client as just being this\ncomputer that your programmer is writing\non it could be you have your sql server\nthere where your data is that's current\nwith all your current sales and it's\narchiving all that information through\nscoop one of the tools in the hadoop\nfile system it could be streaming data\nit could be a connection to the stock\nservers and you're pulling stock data\ndown from those servers at a regular\ntime and that's all controlled you can\nactually set that code up to be\ncontrolled in many of the different\nfeatures in the hadoop file system and\nsome of the different resources you have\nthat sit on top of it so here the client\nwrites the data through the fs data\noutput stream and the fs data output\nstream as you can see goes into right\npacket so it divides it up into packets\n128 megabytes and the data is written\nand the slave further replicates it so\nhere's our data coming in and then if\nyou remember correctly that's part of\nthe fs data setup is it tells it where\nto replicate it out but the data node\nitself is like oh hey okay i've got the\ndata coming in and it's also given the\ntokens of where to send the replications\nto and then acknowledgement is sent\nafter the required replicas are made so\nthen that goes back up saying hey\nsuccessful i've written data and made\nthree replications on this as far as our\nyou know going through the pipeline of\nthe data nodes and then that goes back\nand says after the date is written the\nclient performs the close method so the\nclient's done it says okay i'm done\nhere's the end of the data we're\nfinished and after the date is written\nthe client performs a close method and\nwe can see here just a quick reshape we\ngo in there just like we did with the\nread we create the connection this\ncreates a name node which lets it know\nwhat's going on step two step three that\nalso includes uh the tokens and\neverything and then we go into step\nthree where we're now writing through\nthe fs data output stream and that sorts\nit out into\nwhatever data node it's going to go to\nand which also tells it how to replicate\nit so that data node then sends it to\nother data nodes so we have a\nreplication and of course you finalize\nit and close everything up marks it\ncomplete to the name node and it deletes\nall those magical tokens in the\nbackground so that they can't be reused\nand we can go ahead and just do this\nwith an example the same setups and\nwhether you're doing a read or write\nthey're very similar as we come in here\nfrom our client and our name node and\nyou can see right here we actually\ndepicting these as the actual rack and\nthe switch is going on and so as the\ndata comes in you have your request like\nwe saw earlier it sends the location of\nthe data nodes and this actually turns\nyour your ip addresses your dynamic node\nconnections they come back to your\nclient your hdfs client and at that\npoint with the tokens it then goes into\nthe core switch and the core switch says\nhey here it goes the client interacts\nwith the data nodes through the switches\nand you can see here where you're\nwriting in block a replication of block\na and a second replication of block a on\nthe second server so blocking is\nreplicated on the second server and the\nthird server at this point you now have\nthree replications of block a and it\ncomes back and says it acknowledges and\nsays hey we're done and that goes back\ninto your hadoop file system to the\nclient and says okay we're done and\nfinally the\nsuccess written to the name node and it\njust closes everything down a quick\nrecap of the hadoop file system and the\nadvantages and so the first one is\nprobably one of the all of these are\nhuge one you have multiple data copies\nare available so it's very fault\ntolerant whole racks can go down\nswitches can go down even your main name\nnode could go down if you have a\nsecondary name node something we didn't\ntalk too much about is how scalable it\nis because it uses distributed storage\nyou run into then you're oh my gosh i'm\nout of space or i need to do some\nheavier processing let me just add\nanother rack of computers so you can\nscale it up very quickly it's a linear\nscalability where it used to be if you\nbought a server you would have to pay a\nlot of money to get that the craig\ncomputer remember the big craig\ncomputers coming out the craig computer\nruns 2 million a year just the\nmaintenance to liquid cool it that's\nvery expensive compared to just adding\nmore racks of computer and extending\nyour data center so it's very cost\neffective since commodity hardware is\nused we're talking cheap knockoff\ncomputers you still need your high-end\nenterprise uh for the name node but the\nrest of them literally it is a tenth of\nthe cost of storing data on more\ntraditional high-end computers and then\nthe data is secure so it has a very\nhigh-end data security provides data\nsecurity for your data so all theory and\nno play doesn't make for much fun so\nlet's go ahead and show you what it\nlooks like as far as some of the\ndimensions when you're getting into\npulling data or putting data into the\nhadoop file system and you can see here\ni have oracle virtual machine virtual\nbox manager i have a couple different\nthings loaded on there cloudera is one\nof them so we should probably explain\nsome of these things if you're new to\nvirtual machines the oracle virtualbox\nallows you to spin up a machine as if it\nis a separate computer so in this case\nthis is running i believe centos linux\nand it creates like a box on my computer\nso the centos is running on my machine\nwhile i'm running my windows 10 this\nhappens to be a windows 10 computer and\nthen underneath here i can actually go\nunder let me just open up the general\nmight be hard to see there and you can\nsee i can actually go down to system\nprocessor i happen to be on an 8 core it\nhas 16 dedicated threads registers are\n16 cpus but 8 cores and i've only\ndesignated this for one cpu so it's only\ngoing to use one of my dedicated threads\non my computer and this the oracle\nvirtual machine is open source you can\nsee right here we're on the oracle\nwww.oracle.com i usually just do a\nsearch for downloading virtualbox if you\nsearch for virtualbox all one word it\nwill come up with this page and then you\ncan download it for whatever operating\nsystem you're working with there\ncertainly are a number of different\noptions and let me go and point those\nout if you're setting this up as a for\ndemoing for yourself the first thing to\nnote for doing a virtual box and doing a\ncloudera or horton setup on that virtual\nbox for doing the hadoop system to try\nit out you need a minimum of 12\ngigabytes it cannot be a windows 10 home\nedition because you'll have problems\nwith your virtual setup and sometimes\nyou have to go turn on the virtual\nsettings so it knows it's in there so if\nyou're on a home setup there are other\nsources there's cloudera and we'll talk\na little bit about clare dara here in\njust a second but they have the cloudera\nonline live we can go try the cloudera\nsetup i've never used it but cloudera is\na pretty good company cloudera and\nhortonworks are two of the common ones\nout there and we'll actually be running\na cloudera hadoop cluster on on our demo\nhere so you have oracle virtual machine\nyou also have the option of doing it on\ndifferent vmware that's another one like\nvirtual machine this is more of a paid\nservice there is a free setup for just\ndoing it for yourself which will work\nfine for this and then in the cloudera\nlike again they have the new online\nsetup where you can go in there to the\nonline and for cloudera you want to go\nunderneath the cloudera quick start if\nyou type in a search for cloudera\nquickstart it'll bring you to this\nwebsite and then you can select your\nplatform in this case i did virtualbox\nthere's vmware we just talked about\ndocker docker is a very high-end virtual\nsetup unless you already know it you\nreally don't want to mess with it then\nyour kvm is if you're on a linux\ncomputer that sets up multiple systems\non that computer so the two you really\nwant to use are usually the virtual box\nor do an online setup and you can see\nhere with the download if you're going\ninto the horton version it's called\nhortonworks and they call it sandbox so\nyou'll see the term hortonworks sandbox\nand these are all test demos you're not\ngoing to deploy a single node hadoop\nsystems that would just be kind of\nridiculous and defeat the whole purpose\nof having a horton or having a hadoop\nsystem if it's only installed on one\ncomputer in a virtual node so a lot of\ndifferent options if you're not on a\nprofessional windows version or you\ndon't have at least 12 gigabytes ram to\nrun this\nyou'll want to try and see if you can\nfind an online version and of course\nsimplylearn has our own labs if you sign\nup for classes we set you up i don't\nknow what it is now but last time i was\nin there was a five node uh setup so you\ncan get around and see what's going on\nwhether you're studying for the admin\nside or for the programming side in\nscript writing and if i go into my\noracle virtual box and i go under my\ncloudera and i start this up and each\none has their own flavors um horton uses\njust a login so you log everything in\nthrough\na local host through your internet\nexplorer or i might use chrome cloudera\nactually opens up a full interface so\nyou actually are in that setup and you\ncan see when i uh started it let me go\nback here once i downloaded this this is\na big download by the way i had to\nimport the appliance in virtualbox the\nfirst time i ran it it takes a long time\nto configure the setup and the second\ntime it comes up pretty quick and with\nthe cloudera quick start again this is a\npretend single node it opens up and\nyou'll see that it actually has firefox\nhere so here's my web browser i don't\nhave to go to localhost i'm actually\nalready in the quickstart for cloudera\nand if we come down here you can see\ngetting started i have some information\nanalyze your data manage your cluster\nyour general information on there and\nwhat i always want to start to do is to\ngo ahead and open up a terminal window\nso we'll open up a terminal widen this a\nlittle bit let me just maximize this out\nhere so you can see so we are now in a\nvirtual machine this virtual machine is\ncentos linux so i'm on a linux computer\non my windows computer and so when i'm\non this terminal window this is your\nbasic terminal if i do list you'll see\ndocuments eclipse these are the\ndifferent things that are installed with\nthe quick start guide on the linux\nsystem so this is a linux computer and\nthen hadoop is running on here so now i\nhave hadoop single node so it has both\nthe name node and the data node and\neverything squished together in one\nvirtual machine we can then do let's do\nhdfs telling it that it's a hadoop file\nsystem dfs minus ls now notice the ls is\nthe same i have ls for list and ls for\nlist and i click on here it'll take you\njust a second reading the hadoop file\nsystem and it comes up with nothing so a\nquick recap let's go back over this\nthree different environments i have this\none out here let's just put this in a\nbright red so you can actually see it i\nhave this environment out here which is\nmy slides i have this environment here\nor i did a list that's looking at the\nfiles on the linuxcentos computer and\nthen we have this system here which is\nlooking at the files on the hadoop file\nsystem so three completely separate\nenvironments and then we connect them\nand so right now we have i have whatever\nfiles i have my personal files and the\num of course we're also looking at the\nscreen for my windows 10 and then we're\nlooking at the screen here uh here's our\nlist there let's look at the files and\nthis is the screen for centos linux and\nthen this is looking at the files right\nhere for the hadoop file system so three\ncompletely separate files this one here\nwhich is the linux is running in a\nvirtual box\nso this is a virtual box i'm using one\ncore to run it or one cpu and everything\nin there is it has its own file system\nyou can see we have our desktop and\ndocuments and whatever in there and then\nyou can see here we right now have no\nfiles and our hadoop file system and\nthis hadoop file system currently is\nstored on the linux machine but it could\nbe stored across 10 linux machines 20\n100 this could be stored across in\npetabytes i mean it could be really huge\nor it could just be in this case just a\ndemo where we're putting it on just one\ncomputer and then once we're in here let\nme just see real quick if i can go under\nview zoom in view zoom in this is just a\nstandard browser so i can use any like\nthe control plus and stuff like that to\nzoom in and this is very common to be in\na browser window with the hadoop file\nsystem so right now i'm in a linux and\ni'm going to do oh let's just create a\nfile go file my new file and i'm going\nto use vi and this is a vi editor just a\nbasic editor and we go and type\nsomething in here\none two three four maybe it's columns 44\n66 77. of course a new file system just\nlike your regular computer can also so\nin our vi editor you hit your colon i\nactually work with a lot of different\nother editors and we'll write quit vi so\nlet's take a look and see what happened\nhere i'm in my linux system i type in ls\nfor list and we should see my new file\nand sure enough we do over here there it\nis let me just highlight that my new\nfile and if i then go into the hadoop\nsystem hdfs dfs minus ls for list we\nstill show nothing it's still empty so\nwhat i can simply do is i can go hdfs\ndfs\nminus put and i'm going to put my new\nfile and this is just going to move it\nfrom the linux system because i'm in\nthis file folder into the hadoop file\nsystem and now if we go in and we type\nin our list for a new file system you\nwill see in here that i now have just\nthe one file on there which is my new\nfile and very similar to linux we can do\ncat and the cat command simply uh evokes\nreading the file so hdfs dfs minus cat\nand i had to look it up remember\ncloudera the the format is going to be a\nuser uh then of course the path location\nand the file name and in here when we\ndid the list here's our list so you can\nsee it lists our file here and we\nrealize that this is under uh user\ncloudera and so i can now go user\ncloudera my new file and the minus cat\nand we'll be able to read the file in\nhere and you can see right here this is\nthe file that was in the linux system is\nnow copied into the cloudera system and\nit's one three four five that what i\nentered in there and if we go back to\nthe linux and do list you'll still see\nit in here my new file and we can also\ndo something like this in our hdfs\nminus mv i will do my new file and we're\ngoing to change it to my\nnew new file and if we do that\nunderneath our hadoop file system the\nminus mv will rename it so if i go back\nhere to our hadoop file system ls you'll\nnow see instead of my new file it has my\nnew new file coming up and there it is\nmy new new files we've renamed it we can\nalso go in here and\ndelete this so i can now come in here so\nin our hdfs dfs we can also do a remove\nand this will remove the file and so if\nwe come in here we run this we'll see\nthat when i come back and do the list\nthe file is gone and now we just have\nanother empty\nfolder with our hadoop file system and\njust like any file system we can take\nthis and we can go ahead and make\ndirectory create a new directory so mk\nfor make directory we'll call this my\ndir so we're going to make a directory\nmy dir it'll take it just a second and\nof course if we do\nthe list command you'll see that we now\nhave the directory in there give it just\na second there it comes mydir and just\nlike we did before i can go in here and\nwe're going to put the file and if you\nremember correctly from our files in the\nsetup i called it my new file so this is\ncoming from the linux system and we're\ngoing to put that into my dir that's the\ntarget in my hadoop setup and so if i\nhit enter on there i can now do\nthe hadoop list and that's not going to\nshow the file because remember i put it\nin a sub-folder so if i should do the\nkadroop just this will show my directory\nand i can do list and then i can do my\ndur for my directory and you'll see\nunderneath the my directory in the\nhadoop file system it now has my new\nfile put in there and with any good\noperating system we need a minus help so\njust like you can type in help in your\nlinux you can now come in here and type\nin hdfs help and it shows you a lot of\nthe commands in there underneath the\nhadoop file system most of them should\nbe very similar to the linux on here and\nwe can also do something like this a\nhadoop version and the hadoop version\nshows up that we're in hadoop 2.60\ncdh is it we're cloudera 5 and compiled\nby jenkins and it has a date and all the\ndifferent information on our hadoop file\nsystem so this is some basics in the\nterminal window let me go ahead and\nclose this out because if you're going\nto play with this you should really come\nin here let me just maximize the cloud\ndata it opens up in a browser window and\nso once we're in here again this is a\nbrowser window which you could access\nmight look like any access for a hadoop\nfile system one of the fun things to do\nwhen you're first starting is to go\nunder hue you'll see it up here at the\ntop has cloudera hue hadoop your hbase\nyour impala your spark these are\nstandard installs now and hue is\nbasically an overview of the file system\nand so come up here and you can see\nwhere you can do queries as far as if\nyou have a hbase or a hive the hive\ndatabase we go over here to the top\nwhere it says file browser and if we go\nunder file browser now this is a hadoop\nfile system we're looking at and once we\nopen up the file browser you can now see\nthere's my directory which we created\nand if i click on my directory there's\nmy new file which is in here and if i\nclick on my new file it actually opens\nit up and you can see from our hadoop\nfile system this is in the hadoop file\nsystem the file that we created so we\ncovered the terminal window you can see\nhere's a terminal window up here it\nmight be if you were in a web browser\nit'll look a little different because it\nactually opens up as a web browser\nterminal window and we've looked a\nlittle bit at hue which is one of the\nmost basic components of hadoop one of\nthe original components for going\nthrough and looking at your data and\nyour databases of course now they're up\nto the hue4 it's gone through a number\nof changes and you can see there's a lot\nof different choices in here for other\ndifferent tools in the hadoop file\nsystem and i'll go ahead and just close\nout of this and one of the cool things\nwith the virtual uh box i can either\nsave the machine state send the shutdown\nsignal or power off the machine i'll go\nand just power off the machine\ncompletely now suppose you have a\nlibrary that has a collection of huge\nnumber of books on each floor and you\nwant to count the total number of books\npresent on each floor what would be your\napproach you could say i will do it\nmyself but then don't you think that\nwill take a lot of time and that's\nobviously not an efficient way of\ncounting the number of books in this\nhuge collection on every floor by\nyourself now there could be a different\napproach or an alternative to that you\ncould think of asking three of your\nfriends or three of your colleagues and\nyou could then say if each friend could\ncount the books on every floor then\nobviously that would make your work\nfaster and easier to count the books on\nevery floor now this is what we mean by\nparallel processing so when you say\nparallel processing in technical terms\nyou are talking about using multiple\nmachines and each machine would be\ncontributing its ram and cpu cores for\nprocessing and your data would be\nprocessed on multiple machines at the\nsame time now this type of process\ninvolves parallel processing in our case\nor in our library example where you\nwould have person 1 who would be taking\ncare of books on floor 1 and counting\nthem person 2 on floor 2 then you have\nsomeone on floor 3 and someone on floor\n4. so every individual would be counting\nthe books on every floor in parallel so\nthat reduces the time consumed for this\nactivity and then there should be some\nmechanism where all these counts from\nevery floor can be aggregated so what is\neach person doing here each person is\nmapping the data of a particular floor\nor you can say each person is doing a\nkind of activity or basically a task on\nevery floor and the task is counting the\nbooks on every floor now then you could\nhave some aggregation mechanism that\ncould basically reduce or summarize this\ntotal count and in terms of map reduce\nwe would say that's the work of reducer\nso when you talk about hadoop map reduce\nit processes data on different node\nmachines now this is the whole concept\nof hadoop framework right that you not\nonly have your data stored across\nmachines but you would also want to\nprocess the data locally so instead of\ntransferring the data from one machine\nto other machine or bringing all the\ndata together into some central\nprocessing unit and then processing it\nyou would rather have the data processed\non the machines wherever that is stored\nso we know in case of hadoop cluster we\nwould have our data stored on multiple\ndata nodes on their multiple disks and\nthat is the data which needs to be\nprocessed but the requirement is that we\nwant to process this data as fast as\npossible and that could be achieved by\nusing parallel processing now in case of\nmapreduce we basically have the first\nphase which is your mapping phase so in\ncase of mapreduce programming model you\nbasically have two phases one is mapping\nand one is reducing now who takes care\nof things in mapping phase it is a\nmapper class and this mapper class has\nthe function which is provided by the\ndeveloper which takes care of these\nindividual map tasks which will work on\nmultiple nodes in parallel your reducer\nclass belongs to the reducing phase so a\nreducing phase basically uses a reducer\nclass which provides a function that\nwill aggregate and reduce the output of\ndifferent data nodes to generate the\nfinal output now that's how your\nmapreduce works using mapping and then\nobviously reducing now you could have\nsome other kind of jobs which are map\nonly jobs wherein there is no reducing\nrequired but we are not talking about\nthose we are talking about our\nrequirement where we would want to\nprocess the data using mapping and\nreducing especially when data is huge\nwhen data is stored across multiple\nmachines and you would want to process\nthe data in parallel so when you talk\nabout mapreduce you could say it's a\nprogramming model you could say\ninternally it's a processing engine of\nhadoop that allows you to process and\ncompute huge volumes of data and when we\nsay huge volumes of data we can talk\nabout terabytes we can talk about\npetabytes exabytes and that amount of\ndata which needs to be processed on a\nhuge cluster we could also use mapreduce\nprogramming model and run a mapreduce\nalgorithm in a local mode but what does\nthat mean if you would go for a local\nmode it basically means it would do all\nthe mapping and reducing on the same\nnode using the processing capacity that\nis ram and cpu cores on the same machine\nwhich is not really efficient in fact we\nwould want to have our map reduce work\non multiple nodes which would obviously\nhave mapping phase followed by a\nreducing phase and intermittently there\nwould be data generated there would be\ndifferent other phases which help this\nwhole processing so when you talk about\nhadoop map reduce you are mainly talking\nabout two main components or two main\nphases that is mapping and reducing\nmapping taking care of map tasks\nreducing taking care of reduced tasks so\nyou would have your data which would be\nstored on multiple machines now when we\ntalk about data data could be in\ndifferent formats we could or the\ndeveloper could specify what is the\nformat which needs to be used to\nunderstand the data which is coming in\nthat data then goes through the mapping\ninternally there would be some shuffling\nsorting and then reducing which gives\nyou your final output so the way we\naccess data from sdfs or the way our\ndata is getting stored on sdfs we have\nour input data which would have one or\nmultiple files in one or multiple\ndirectories and your final output is\nalso stored on sdfs to be accessed to be\nlooked into and to see if the processing\nwas done correctly so this is how it\nlooks so you have the input data which\nwould then be worked upon by multiple\nmap tasks now how many map tasks that\nbasically depends on the file that\ndepends on the input format so normally\nwe know that in a hadoop cluster you\nwould have a file which is broken down\ninto blocks depending on its size so the\ndefault block size is 128 mb which can\nthen still be customized based on your\naverage size of data which is getting\nstored on the cluster so if i have\nreally huge files which are getting\nstored on the cluster i would certainly\nset a higher block size so that my every\nfile does not have huge number of blocks\ncreating a load on name nodes ram\nbecause that's tracking the number of\nelements in your cluster or number of\nobjects in your cluster so depending on\nyour file size your file would be split\ninto multiple chunks and for every chunk\nwe would have a map task running now\nwhat is this map task doing that is\nspecified within the mapper class so\nwithin the mapper class you have the\nmapper function which basically says\nwhat each of these map tasks has to do\non each of the chunks which has to be\nprocessed this data intermittently is\nwritten to sdfs where it is sorted and\nshuffled and then you have internal\nphases such as partitioner which decides\nhow many reduce tasks would be used or\nwhat data goes to which reducer you\ncould also have a combiner phase which\nis like a mini reducer doing the same\nreduce operation before it reaches\nreduce then you have your reducing phase\nwhich is taken care by a reducer class\nand internally the reducer function\nprovided by developer which would have\nreduced task running on the data which\ncomes as an output from map tasks\nfinally your output is then generated\nwhich is stored on sdfs now in case of\nhadoop it accepts data in different\nformats your data could be in compressed\nformat your data could be in part k your\ndata could be in afro text csv psv\nbinary format and all of these formats\nare supported however remember if you\nare talking about data being compressed\nthen you have to also look into what\nkind of split ability the compression\nmechanism supports otherwise when\nmapreduce processing happens it would\ntake the complete file as one chunk to\nbe processed so sdfs accepts input data\nin different formats this data is stored\nin sdfs and that is basically our input\nwhich is then passing through the\nmapping phase now what is mapping phase\ndoing as i said it reads record by\nrecord depending on the input format it\nreads the data so we have multiple map\ntasks running on multiple chunks once\nthis data is being read this is broken\ndown into individual elements and when i\nsay individual element i could say this\nis my list of key value pairs so your\nrecords based on some kind of delimiter\nor without delimiter are broken down\ninto individual elements and thus your\nmac creates key value pairs now these\nkey value pairs are not my final output\nthese key value pairs are basically a\nlist of elements which will then be\nsubjected to further processing so you\nwould have internally shuffling and\nsorting of data so that all the relevant\nkey value pairs are brought together\nwhich basically benefits the processing\nand then you have your reducing which\naggregates the key value pairs into set\nof smaller tuples or tuples as you would\nsay finally your output is getting\nstored in the\ndesignated directory as a list of\naggregated key value pairs which gives\nyou your output so when we talk about\nmapreduce one of the key factors here is\nthe parallel processing which it can\noffer so we know that we have a data is\ngetting stored across multiple data\nnodes and you would have huge volume of\ndata which is split and randomly\ndistributed across data nodes and this\nis the data which needs to be processed\nand the best way would be parallel\nprocessing so you could have your data\ngetting stored on multiple data nodes or\nmultiple slave nodes and each slave node\nwould have again one or multiple disks\nto process this data basically we have\nto go for parallel processing approach\nwe have to use the mapreduce now let's\nlook at the mapreduce workflow to\nunderstand how it works so basically you\nhave your input data stored on sdfs now\nthis is the data which needs to be\nprocessed it is stored in input files\nand the processing which you want can be\ndone on one single file or it can be\ndone on a directory which has multiple\nfiles you could also later have multiple\noutputs merged which we achieve by using\nsomething called as chaining of mappers\nso here you have your data getting\nstored on sdfs now input format is\nbasically something to define the input\nspecification and how the input files\nwill be split so there are various input\nformats now we can search for that so we\ncan go to google and we can basically\nsearch for\nhadoop map reduce\nyahoo tutorial this is one of the good\nlinks and if i look into this link i can\nsearch for different input formats and\noutput formats so let's search for input\nformat so when we talk about input\nformat you basically have something to\ndefine how input files are split so\ninput files are split up and read based\non what input format is specified so\nthis is a class that provides following\nfunctionality it selects the files or\nother objects that should be used for\ninput it defines the input split that\nbreak a file into tasks provides a\nfactory for record reader objects that\nread the file so there are different\nformats if you look in the table here\nand you can see that the text input\nformat is the default format which reads\nlines of a text file and each line is\nconsidered as a record here the key is\nthe byte offset of the line and the\nvalue is the line content it says you\ncan have key value input format which\npasses lines into key value pairs\neverything up to the first tab character\nis the key and the remainder is the line\nyou could also have sequence file input\nformat which basically works on binary\nformat so you have input format and in\nthe same way you can also search for\noutput format which takes care of how\nthe data is handled after the processing\nis done so the key value pairs provided\nto this output collector are then\nwritten to output files the way they are\nwritten is governed by output format so\nit functions pretty much like input\nformat as described in earlier right so\nwe could set what is the output format\nto be followed and again you have text\noutput sequence file output format null\noutput format and so on so these are\ndifferent classes which take care of how\nyour data is handled when it is being\nread for processing or how is the data\nbeing written when the processing is\ndone so based on the input format the\nfile is broken down into splits and this\nlogically represents the data to be\nprocessed by individual map tasks or you\ncould say individual mapper functions so\nyou could have one or multiple splits\nwhich need to be processed depending on\nthe file size depending on what\nproperties have been set now once this\nis done you have your input splits which\nare subjected to mapping phase\ninternally you have a record reader\nwhich communicates with the input split\nand converts the data into key value\npairs suitable to be read by mapper and\nwhat is mapper doing it is basically\nworking on these key value pairs the map\ntask giving you an intermittent output\nwhich would then be forwarded for\nfurther processing now once that is done\nand we have these key value pairs which\nis being worked upon my map your map\ntasks as a part of your mapper function\nare generating your key value pairs\nwhich are your intermediate outputs to\nbe processed further now you could have\nas i said a combiner face or internally\na mini radio surface now combiner does\nnot have its own class so combiner\nbasically uses the same class as the\nreducer class provided by the developer\nand its main work is to do the reducing\nor its main work is to do some kind of\nmini aggregation on the key value pairs\nwhich were generated by map so once the\ndata is coming in from the combiner then\nwe have internally a partitioner phase\nwhich decides how outputs from combiners\nare sent to the reducers or you could\nalso say that even if i did not have a\ncombiner partitioner would decide based\non the keys and values based on the type\nof keys how many reducers would be\nrequired or how many reduced tasks would\nbe required to work on your output which\nwas generated by map task now once\npartitioner has decided that then your\ndata would be then sorted and shuffled\nwhich is then fed into the reducer so\nwhen you talk about your reducer it\nwould basically have one or multiple\nreduced tasks now that depends on what\nor what partitioner decided or\ndetermined for your data to be processed\nit can also depend on the configuration\nproperties which have been set to decide\nhow many radio stars should be used now\ninternally all this data is obviously\ngoing through sorting and shuffling so\nthat your reducing or aggregation\nbecomes an easier task once we have this\ndone we basically have the reducer which\nis the code for the reducer is provided\nby the developer and all the\nintermediate data has then to be\naggregated to give you a final output\nwhich would then be stored on sdfs and\nwho does this you have an internal\nrecord writer which writes these output\nkey value pairs from reducer to the\noutput files now this is how your\nmapreduce works wherein the final output\ndata can be not only stored but then\nread or accessed from sdfs or even used\nas an input for further mapreduce kind\nof processing so this is how it overall\nlooks so you basically have your data\nstored on sdfs based on input format you\nhave the splits then you have record\nreader which gives your data to the\nmapping phase which is then taken care\nby your mapper function and mapper\nfunction basically means one or multiple\nmap tasks working on your chunks of data\nyou could have a combiner phase which is\noptional which is not mandatory then you\nhave a partitioner phase which decides\non how many reduced tasks or how many\nreducers would be used to work on your\ndata internally there is sorting and\nshuffling of data happening and then\nbasically based on your output format\nyour record reader will write the output\nto sdfs directory now internally you\ncould also remember that data is being\nprocessed locally so you would have the\noutput of each task which is being\nworked upon stored locally however we do\nnot access the data directly from data\nnodes we access it from sdfs so our\noutput is stored on sdfs so that is your\nmapreduce workflow when you talk about\nmapreduce architecture now this is how\nit would look so you would have\nbasically a edge node or a client\nprogram or an api which intends to\nprocess some data so it submits the job\nto the job tracker or you can say\nresource manager in case of hadoop yarn\nframework right now before this step we\ncan also say that an interaction with\nname node would have already happened\nwhich would have given information of\ndata nodes which have the relevant data\nstored then your master processor so in\nhadoop version 1 we had job tracker and\nthen the slaves were called task\ntrackers in hadoop version 2 instead of\njob tracker you have resource manager\ninstead of task trackers you have node\nmanagers so basically your resource\nmanager has to assign the job to the\ntask trackers or node managers so your\nnode managers as we discussed in yarn\nare basically taking care of processing\nwhich happens on every note so\ninternally there is all of this work\nhappening by resource manager node\nmanagers and application master then you\ncan refer to the yarn based tutorial to\nunderstand more about that so here your\nprocessing master is basically breaking\ndown the application into tasks what it\ndoes internally is once your application\nis submitted your application to be run\non yarn processing framework is handled\nby resource manager now forget about the\nyarn part as of now i mean who does the\nnegotiating of resources who allocates\nthem how does the processing happen on\nthe nodes right so that's all to do with\nhow yarn handles the processing request\nso you have your data which is stored in\nsdfs broken down into one or multiple\nsplits depending on the input format\nwhich has been specified by the\ndeveloper your input splits are to be\nworked upon by your one or multiple map\ntasks which will be running within the\ncontainer on the nodes basically you\nhave the resources which are being\nutilized so for each map task you would\nhave some amount of ram which will be\nutilized and then further the same data\nwhich has to go through reducing phase\nthat is your reduced task will also be\nutilizing some ram and cpu cores now\ninternally you have these functions\nwhich take care of deciding on number of\nreducers doing a mini reduce and\nbasically reading and processing the\ndata from multiple data nodes now this\nis how your mapreduce programming model\nmakes parallel processing work or\nprocesses your data which is stored\nacross multiple machines finally you\nhave your output which is getting stored\non its dfs\nso let's have a quick demo on mapreduce\nand see how it works on a hadoop cluster\nnow we have discussed briefly about\nmapreduce which contains mainly two\nphases that is your mapping phase and\nyour reducing phase and mapping phase is\ntaken care by your mapper function and\nyour reducing phase is taken care by\nyour reducer function now in between we\nalso have sorting and shuffling and then\nyou have other phases which is\npartitioner and combiner and we will\ndiscuss about all those in detail in\nlater sessions but let's have a quick\ndemo on how we can run a mapreduce which\nis already existing as a package jar\nfile within your apache hadoop cluster\nor even in your cloudera cluster now we\ncan build our own mapreduce programs we\ncan package them as jar transfer them to\nthe cluster and then run it on a hadoop\ncluster on yarn or we could be using\nalready provided default program so\nlet's see where they are now these are\nmy two machines which i have brought up\nand basically this would have my apache\nhadoop cluster running now we can just\ndo a simple start hyphen all dot sh now\ni know that this script is deprecated\nand it says instead use start dfs and\nstart yarn but then it will still take\ncare of starting off my cluster on these\ntwo nodes where i would have one single\nname node two data nodes one secondary\nname node one resource manager and two\nnode managers now if you have any doubt\nin how this cluster came up you can\nalways look at the previous sessions\nwhere we had a walkthrough in setting up\na cluster on apache and then you could\nalso have your cluster running using\nless than 3 gb of your total machine ram\nand you could have apache cluster\nrunning on your machine now once this\ncluster comes up we will also have a\nlook at the web ui which is available\nfor name node and resource manager now\nbased on the settings what we have given\nour uis will show us details of our\ncluster but remember the ui is only to\nbrowse now here my cluster has come up i\ncan just do a jps to look at java\nrelated processes and that will show me\nwhat are the processes which are running\non c1 which is your data node resource\nmanager node manager and name node and\non my m1 machine which is my second\nmachine which i have configured here i\ncan always do a jps and that shows me\nthe processes running which also means\nthat my cluster is up with two data\nnodes with two node managers and here i\ncan have a look at my web ui so i can\njust do a refresh and the same thing\nwith this one just to refresh so i had\nalready opened the web pages so you can\nalways access the web ui using your name\nnotes hostname and 570 port it tells me\nwhat is my cluster id what is my block\npull id it gives you information of what\nis the space usage how many live nodes\nyou have and you can even browse your\nfile system so i have put in a lot of\ndata here i can click on browse the file\nsystem and this basically shows me\nmultiple directories and these\ndirectories have one or multiple files\nwhich we will use for our mapreduce\nexample now if you see here these are my\ndirectories which have some sample files\nalthough these files are very small like\n8.7 kilobytes if you look into this\ndirectory if you look into this i have\njust pulled in some of my hadoop logs\nand i have put it on my sdfs these are a\nlittle bigger files and then we also\nhave some other data which we can see\nhere and this is data which i have\ndownloaded from web now we can either\nrun a mapreduce on a single file or in a\ndirectory which contains multiple files\nlet's look at that before looking at\ndemon mapreduce also remember mapreduce\nwill create a output directory and we\nneed to have that directory created plus\nwe need to have the permissions to run\nthe mapreduce job so by default since\ni'm running it using admin id i should\nnot have any problem but then if you\nintend to run map reduce with a\ndifferent user then obviously you will\nhave to ask the admin or you will have\nto give the user permission to read and\nwrite from sdfs so this is the directory\nwhich i have created which will contain\nmy output once the mapreduce job\nfinishes and this is my cluster file\nsystem if you look on this ui this shows\nme about my yarn which is available for\ntaking care of any processing it as of\nnow shows that i have total of 8 gb\nmemory and i have 8 v cores now that can\nbe depending on what configuration we\nhave set or how many nodes are available\nwe can look at nodes which are available\nand that shows me i have two node\nmanagers running each has 8 gb memory\nand 8 v cores now that's not true\nactually but then we have not set the\nconfigurations for node managers and\nthat's why it takes the default\nproperties that is 8gb laminate vcos now\nthis is my yarn ui we can also look at\nscheduler which basically shows me the\ndifferent cues if they have been\nconfigured where you will have to run\nthe jobs we'll discuss about all these\nin later in detail now let's go back to\nour terminal and let's see where we can\nfind some sample applications which we\ncan run on the cluster so once i go to\nthe terminal i can well submit the\nmapreduce job from any terminal now here\ni know that my hadoop related directory\nis here and within hadoop you have\nvarious directories we have discussed\nthat in binaries you have the commands\nwhich you can run in s bin you basically\nhave the startup scripts and here you\nalso notice there is a share directory\nin the end if you look in the shared\ndirectory you would find hadoop and\nwithin hadoop you have various sub\ndirectories in which we will look for\nmap reduce now this mapreduce directory\nhas some sample jar files which we can\nuse to run a mapreduce on the cluster\nsimilarly if you are working on a\ncloudera cluster you would have to go\ninto opt cloudera parcel cdh slash lib\nand in that you would have directories\nfor sdfs mapreduce or sdfs yarn where\nyou can still find the same jars it is\nbasically a package which contains your\nmultiple applications now how do we run\na mapreduce we can just type in hadoop\nand hit enter and that shows me that i\nhave an option called jar which can be\nused to run a jar file now at any point\nof time if you would want to see what\nare the different classes which are\navailable in a particular\njar you could always do a jar minus xvf\nfor example i could say jar xv f and i\ncould say user local hadoop\nshare hadoop mapreduce and then list\ndown your jar file so i'll say hadoop\nmapreduce examples and if i do this this\nshould basically unpack it to show me\nwhat classes are available within this\nparticular jar and it has done this it\nhas created a meta file and it has\ncreated a org directory we can see that\nby doing a ls and here if you look in ls\norg since i ran the command from your\nphone directory i can look into org\npatchy hadoop\nexamples which shows me the classes\nwhich i have and those classes contain\nwhich mapper or reducer classes so it\nmight not be just mapper and reducer but\nyou can always have a look so for\nexample i am targeting to use word count\nprogram which does a word count on files\nand gives me a list of words and how\nmany times they occur in a particular\nfile or in set of files and this shows\nme that what are the classes which\nbelong to word count so we have a int\nsum reducer so this is my reducer class\ni have tokenizer mapper that is my\nmapper class right and basically this is\nwhat is used these classes are used if\nyou run a word code now there are many\nother programs which are part of this\njar file and we can expand and see that\nso i can say hadoop jar and give your\npath so i'll say hadoop jar user local\nhadoop share hadoop mapreduce hadoop\nmapreduce examples and if i hit on enter\nthat will show me what are the inbuilt\nclasses which are already available now\nthese are certain things which we can\nuse now there are other jar files also\nfor example i can look at hadoop and\nhere we can look at the jar files which\nwe have in this particular path so this\nis one hadoop mapreduce examples which\nyou can use you can always look in other\njar files like you can look for\nhadoop mapreduce client job client and\nthen you can look at the test one so\nthat is also an interesting one so you\ncan always look into hadoop mapreduce\nclient job client and then you have\nsomething ending with this so if i would\nhave tried this one using my hadoop jar\ncommand so in my previous example when\nwe did this it was showing me all the\nclasses which are available and that\nalready has a word count now there are\nother good programs which you can try\nlike teragen to generate dummy data\nterasort to check your sorting\nperformance and so on and terra validate\nto validate the results similarly we can\nalso do a hadoop jar as i said on hadoop\nmapreduce i think that was client and\nthen we have\njob client and then test star now this\nhas a lot of other classes which can be\nused or programs which can be used for\ndoing a stress testing or checking your\ncluster status and so on one of them\ninteresting one is test dfs io but let's\nnot get into all the details in first\ninstance let's see how we can run a\nmapreduce now if i would want to run a\nmapreduce i need to give hadoop jar and\nthen my jar file and if i hit on enter\nit would say it needs the input and\noutput it needs which class you want to\nrun so for example i would say word\ncount and again if i hit on enter it\ntells me that you need to give me some\ninput and output to process and\nobviously this processing will be\nhappening on cluster that is our yarn\nprocessing framework unless and until\nyou would want to run this job in a\nlocal mode so there is a possibility\nthat you can run the job in local mode\nbut let's first try how it runs on the\ncluster so how do we do that now here i\ncan do a hdfs\nls slash command to see what i have on\nmy sdfs now through my ui i was already\nshowing you that we have set of files\nand directories which we can use to\nprocess now we can take up one single\nfile so for example if i pick up new\ndata and i can look into the files here\nwhat we have and we can basically run a\nmapreduce on a single file or multiple\nfiles so let's take this file whatever\nthat contains and i would like to do a\nword count so that i get a list of words\nand their occurrence in this file so let\nme just copy this now i also need my\noutput to be written and that will be\nwritten here so here if i want to run a\nmapreduce i can say hadoop which we can\npull out from history so hadoop jar word\ncount now i need to give my input so\nthat will be new data and then i will\ngive this file which we just copied now\ni am going to run the word count only on\na single file and i will basically have\nmy output which will be stored in this\ndirectory the directory which i have\ncreated already mr output so let's do\nthis\noutput and this is fair enough now you\ncan give many other properties you can\nspecify how many map jobs you want to\nrun how many reduced jobs you want to\nrun do you want your output to be\ncompressed do you want your output to be\nmerged or many other properties can be\ndefined when you are specifying word\ncount and then you can pass in an\nargument to pass properties from the\ncommand line which will affect your\noutput now once i go ahead and submit\nthis this is basically running a simple\ninbuilt mapreduce job on our hadoop\ncluster now obviously internally it will\nbe looking for name node now we have\nsome issue here and it says the output\nalready exists what does that mean so it\nbasically means that hadoop will create\nan output for you you just need to give\na name but then you don't need to create\nit so let's give let's append the output\nwith number one and then let's go ahead\nand run this so i've submitted this\ncommand now this can also be done in\nbackground if you would want to run\nmultiple jobs on your cluster at the\nsame time so it takes total input paths\nto process one so that is there is only\none split on which your job has to work\nnow it will internally try to contact\nyour resource manager and basically this\nis done so here we can have a look and\nwe can see some counters here now what i\nalso see is for some property which it\nis missing it has run the job but it has\nrun in a local mode it has run in a\nlocal mode so although we have submitted\nso this might be related to my yarn\nsettings and we can check that\nso if i do a refresh when i have run my\napplication it has completed it would\nhave created an output but the only\nthing is it did not interact with your\nyarn it did not interact with your\nresource manager we can check those\nproperties and here if we look into the\njob it basically tells me that it went\nfor mapping and reducing it would have\ncreated an output it worked on my file\nbut then it ran in a local mode it ran\nin a local mode so mapreduce remember is\na programming model right now if you run\nit on yarn you get the facilities of\nrunning it on a cluster where yarn takes\ncare of resource management if you don't\nrun it on yarn and run it on a local\nmode it will use your machines ram and\ncpu cores for processing but then we can\nquickly look at the output and then we\ncan also try running this on yarn so if\ni look into my hdfs and if i look into\nmy output mr output that's the directory\nwhich was not used actually let's look\ninto the other directory which is ending\nwith one and that should show me the\noutput created by this mapreduce\nalthough it ran in the local mode it\nfetched an input file from usdfs and it\nwould have created output in hdfs now\nthat's my part file which is created and\nif you look at part minus r minus these\nzeros if you would have more than one\nreducer running then you would have\nmultiple such files created we can look\ninto this what does this file contain\nwhich should have my word count and here\ni can say cat which basically shows me\nwhat is the output created by my\nmapreduce let's have a look into this so\nthe file which we gave for processing\nhas been broken down and now we have the\nlist of words which occur in the file\nplus a count of those words so if there\nis some word which is in is more then it\nshows me the count so this is a list of\nmy words and the count for that so this\nis how we run a sample mapreduce job i\nwill also show you how we can run it on\nyeah now let's run mapreduce on yarn and\ninitially when we tried running a\nmapreduce it did not hit yarn but it ran\nin a local mode and that was because\nthere was a property which had to be\nchanged in mapreduce hyphen site file so\nbasically if you look into this file the\nerror was that i had given a property\nwhich says mapred dot framework dot name\nand that was not the right property name\nand it was ignored and that's why it ran\nin a local mode so i changed the\nproperty to mapreduce.framework.name\nrestarted my cluster and everything\nshould be fine now and that map red\nhyphen site file has also been copied\nacross the nodes now to run a mapreduce\non a hadoop cluster so that it uses yarn\nand yarn takes care of resource\nallocation on one or multiple machines\nso i'm just changing the output here and\nnow i will submit this job which should\nfirst connect to the resource manager\nand if it connects to the resource\nmanager that means our job will be run\nusing yarn on the cluster rather than in\na local mode so now we have to wait for\nthis application to internally connect\nto resource manager and once it starts\nthere we can always go back to the web\nui and check if our application has\nreached yarn so it shows me that there\nis one input part to be processed that's\nmy job id that's my application id which\nyou can even monitor status from the\ncommand line now here the job has been\nsubmitted so let's go back here and just\ndo a refresh on my yarn ui which should\nshow me the new application which is\nsubmitted it tells me that it is an\naccepted state application master has\nalready started and if you click on this\nlink it will also give you more details\nof how many map and reduce tasks would\nrun so as of now it says the application\nmaster is running it would be using this\nnode which is m1 we can always look into\nthe logs we can see that there is a one\ntask attempt which is being made and now\nif i go back to my terminal i will see\nthat it is waiting to get some resources\nfrom the cluster and once it gets the\nresources it will first start with the\nmapping phase where the mapper function\nruns it does the map tasks one or\nmultiple depending on the splits so\nright now we have one file and one split\nso we will have just one map task\nrunning and once the mapping phase\ncompletes then it will get into reducing\nwhich will finally give me my output so\nwe can be toggling through these\nsessions so here i can just do a refresh\nto see what is happening with my\napplication is it proceeding is it still\nwaiting for resource manager to allocate\nsome resources now just couple of\nminutes back i tested this application\non yarn and we can see that my first\napplication completed successfully and\nhere we will have to give some time so\nthat yarn can allocate the resources now\nif the resources were used by some other\napplication they will have to be freed\nup now internally yan takes care of all\nthat which we will learn more detail in\nyarn or you might have already followed\nthe yarn based session now here we will\nhave to just give it some more time and\nlet's see if my application proceeds\nwith the resources what yarn can\nallocate to it sometimes you can also\nsee a slowness in what web ui shows up\nand that can be related to the amount of\nmemory you have allocated to your nodes\nnow for apache we can have less amount\nof memory and we can still run the\ncluster and as i said the memory which\nshows up here 16 gb and 16 cores is not\nthe true one those are the default\nsettings right but then my yarn should\nbe able to facilitate running of this\napplication let's just give it a couple\nof seconds and then let's look into the\noutput here again i had to make some\nchanges in the settings because our\napplication was not getting enough\nresources and then basically i restarted\nmy cluster now let's submit the\napplication again to the cluster which\nfirst should contact the resource\nmanager and then basically the map and\nreduce process should start so here i\nhave submitted an application it is\nconnecting to the resource manager and\nthen basically it will start internally\nin app master that is application master\nit is looking for the number of splits\nwhich is one it's getting the\napplication id and it basically then\nstarts running the job it also gives you\na tracking url to look at the output and\nnow we should go back and look at our\nyarn ui if our application shows up here\nand we will have to give it a couple of\nseconds when it can get the final status\nchange to running and that's where my\napplication will be getting resources\nnow if you closely notice here i have\nallocated specific amount of memory that\nis 1.5 gb for node manager on every node\nand i have basically given two cores\neach which my machines also have and my\nyarn should be utilizing these resources\nrather than going for default now the\napplication has started moving\nand we can see the progress bar here\nwhich basically will show what is\nhappening and if we go back to the\nterminal it will show that first it went\nin deciding map and reduce it goes for\nmap once the mapping phase completes\nthen the reducing phase will come into\nexistence and here my job has completed\nso now it has basically used we can\nalways look at how many map and reduce\nsas were run it shows me that there was\none map and one reduced task now with\nthe number of map tasks depends on the\nnumber of splits and we had just one\nfile which is less than 128 mb so that\nwas one split to be processed and\nreduced task is internally decided by\nthe reducer or depending on what kind of\nproperty has been set in hadoop config\nfiles now it also tells me how many\ninput records were read which basically\nmeans these were the number of lines in\nthe file it tells me output records\nwhich gives me the number of total words\nin the file now there might be\nduplicates and that which is processed\nby internal combiner further processing\nor forwarding that information to\nreducer and basically reducer works on\n335 records gives us a list of words and\ntheir account now if i do a refresh here\nthis would obviously show my application\nis completed it says succeeded you can\nalways click on the application to look\nfor more information it tells me where\nit ran now we do not have a history\nserver running as of now otherwise we\ncan always access more information so\nthis leads to history server where all\nyour applications are stored but i can\nclick on this attempt tasks and this\nwill basically show me the history url\nor you can always look into the logs so\nthis is how you can submit a sample\napplication which is inbuilt which is\navailable in the jar on your hadoop\ncluster and that will utilize your\ncluster to run now you could always as i\nsaid when you are running a particular\njob remember to change the output\ndirectory and if you would not want it\nto be processing is a single individual\nfile you could also point it to a\ndirectory that basically means it will\nhave multiple files and depending on the\nfile sizes there would be multiple\nsplits and according to that multiple\nmap tasks will be selected so if i click\non this this would submit my second\napplication to the cluster which should\nfirst connect to resource manager then\nresource manager has to start an\napplication master now here we are\ntargeting 10 splits now you have to\nsometimes give couple of seconds in your\nmachines so that the resources which\nwere used are internally already freed\nup so that your cluster can pick it up\nand then your yarn can take care of\nresources so right now my application is\nan undefined status but then as soon as\nmy yarn provides it the resources we\nwill have the application running on our\nyarn cluster so it has already started\nif you see it is going further then it\nwould launch 10 map tasks and it would\nthe number of reduced stars would be\ndecided on either the way your data is\nor based on the properties which have\nbeen set at your cluster level let's\njust do a quick refresh here on my yarn\nui to show me the progress also take\ncare that when you are submitting your\napplication you need to have the output\ndirectory mentioned however do not\ncreate it hadoop will create that for\nyou now this is how you run a mapreduce\nwithout specifying properties but then\nyou can specify more properties you can\nlook into what are the things which can\nbe changed for your mapper and reducer\nor basically having a combiner class\nwhich can do a mini reducing and all\nthose things can be done so we will\nlearn about that in the later sessions\nnow we will compare hadoop version one\nthat is with mapreduce version one we\nwill understand and learn about the\nlimitations of hadoop version one what\nis the need of yarn what is yarn what\nkind of workloads can be running on yarn\nwhat are yarn components what is yarn\narchitecture and finally we will see a\ndemo on yarn so hadoop version one or\nmapreduce version one well that's\noutdated now and nobody is using hadoop\nversion one but it would be good to\nunderstand what was in hadoop version\none and what were the limitations of\nhadoop version one which brought in the\nthought for the future processing layer\nthat is yarn now when we talk about\nhadoop we already know that hadoop is a\nframework and hadoop has two layers one\nis your storage layer that is your sdfs\nhadoop distributed file system which\nallows for distributed storage and\nprocessing which allows fault tolerance\nby inbuilt replication and which\nbasically allows you to store huge\namount of data across multiple commodity\nmachines when we talk about processing\nwe know that mapreduce is the oldest and\nthe most mature processing programming\nmodel which basically takes care of your\ndata processing on your distributed file\nsystem so in hadoop version 1 mapreduce\nperformed both data processing and\nresource management and that's how it\nwas problematic in mapreduce we had\nbasically when we talked about the\nprocessing layer we had the master which\nwas called job tracker and then you had\nthe slaves which were the task records\nso your job tracker was taking care of\nallocating resources it was performing\nscheduling and even monitoring the jobs\nit basically was taking care of\nassigning map and reduced tasks to the\njobs running on task trackers and task\ntrackers which were co-located with data\nnodes were responsible for processing\nthe jobs so task trackers were the\nslaves for the processing layer which\nreported their progress to the job\ntracker so this is what was happening in\nhadoop version 1. now when we talk about\nhadoop version 1 we would have say\nclient machines or an api or an\napplication which basically submits the\njob to the master that is job tracker\nnow obviously we cannot forget that\nthere would already be an involvement\nfrom name node which basically tells\nwhich are the machines or which are the\ndata nodes where the data is already\nstored now once the job submission\nhappens to the job tracker job tracker\nbeing the master demon for taking care\nof your processing request and also\nresource management job scheduling would\nthen be interacting with your multiple\ntask trackers which would be running on\nmultiple machines so each machine would\nhave a task tracker running and that\ntask tracker which is a processing slave\nwould be co-located with the data nodes\nnow we know that in case of hadoop you\nhave the concept of moving the\nprocessing to wherever the data is\nstored rather than moving the data to\nthe processing layer so we would have\ntask trackers which would be running on\nmultiple machines and these task\ntrackers would be responsible for\nhandling the tasks what are these tasks\nthese are the application which is\nbroken down into smaller tasks which\nwould work on the data which is\nrespectively stored on that particular\nnode now these were your slave demons\nright so your job tracker was not only\ntracking the resources so your task\ntrackers were sending heartbeats they\nwere sending in packets and information\nto the job tracker which would then be\nknowing how many resources and when we\ntalk about resources we are talking\nabout the cpu cores we are talking about\nthe ram which would be available on\nevery node so task trackers would be\nsending in their resource information to\njob tracker and your job tracker would\nbe already aware of what amount of\nresources are available on a particular\nnode how loaded a particular node is\nwhat kind of work could be given to the\ntask tracker so job tracker was taking\ncare of resource management and it was\nalso breaking the application into tasks\nand doing the job scheduling part assign\ndifferent tasks to these slave demons\nthat is your task trackers so job\ntracker was\neventually overburdened right because it\nwas managing jobs it was tracking the\nresources from multiple task trackers\nand basically it was taking care of job\nscheduling so job tracker would be\noverburdened and in a case if job\ntracker would fail then it would affect\nthe overall processing so if the master\nis killed if the master demand dies then\nthe processing cannot proceed now this\nwas one of the limitations of hadoop\nversion one so when you talk about\nscalability that is the capability to\nscale due to a single job tracker\nscalability would be hitting a\nbottleneck you cannot have a cluster\nsize of more than 4000 nodes and cannot\nrun more than 40 000 concurrent tasks\nnow that's just a number we could always\nlook into the individual resources which\neach machine was having and then we can\ncome up with an appropriate number\nhowever with a single job tracker there\nwas no horizontal scalability for the\nprocessing layer because we had single\nprocessing master now when we talk about\navailability job tracker as i mentioned\nwould be a single point of failure now\nany failure kills all the queued and\nrunning jobs and jobs would have to be\nresubmitted now why would we want that\nin a distributed platform in a cluster\nwhich has hundreds and thousands of\nmachines we would want a processing\nlayer which can handle huge amount of\nprocessing which could be more scalable\nwhich could be more available and could\nhandle different kind of workloads when\nit comes to resource utilization now if\nyou would have a predefined number of\nmap and reduce slots for each task\ntracker you would have issues which\nwould relate to resource utilization and\nthat again is putting a burden on the\nmaster which is tracking these resources\nwhich has to assign jobs which can run\non multiple machines in parallel so\nlimitations in running non-map reduce\napplications now that was one more\nlimitation of hadoop version one and\nmapreduce that the only kind of\nprocessing you could do is mapreduce and\nmapreduce programming model although it\nis good it is oldest it has matured over\na period of time but then it is very\nrigid you will have to go for mapping\nand reducing approach and that was the\nonly kind of processing which could be\ndone in hadoop person one so when it\ncomes to doing a real time analysis or\ndoing ad hoc query or doing a graph\nbased processing or massive parallel\nprocessing there were limitations\nbecause that could not be done in hadoop\nversion 1 which was having mapreduce\nversion 1 as the processing component\nnow that brings us to the need for yarn\nso yarn it stands for yet another\nresource negotiator so as i mentioned\nbefore yarn in hadoop version one well\nyou could have applications which could\nbe written in different programming\nlanguages but then the only kind of\nprocessing which was possible was\nmapreduce we had the storage layer we\nhad the processing but then kind of\nlimited processing which could be done\nnow this was one thing which brought in\na thought that why shouldn't we have a\nprocessing layer which can handle\ndifferent kind of workloads as i\nmentioned might be graph processing\nmight be real-time processing might be\nmassive parallel processing or any other\nkind of processing which would be a\nrequirement of an organization now\ndesigned to run map-reduce jobs only and\nhaving issues in scalability resource\nutilization job tracking etc that led to\nthe need of something what we call as\nyarn now from hadoop version 2 onwards\nwe have the two main layers have changed\na little bit you have the storage layer\nwhich is intact that is your hdfs and\nthen you have the processing layer which\nis called yarn yet another resource\nnegotiator now we will understand how\nyarn works but then yarn is taking care\nof your processing layer it does support\nmapreduce so mapreduce processing can\nstill be done but then now you can have\na support to other processing frameworks\nyarn can be used to solve the issues\nwhich hadoop version 1 was posing\nsomething like resource management\nsomething like different kind of\nworkload processing something like\nscalability resource utilization all\nthat is now taken care by yarn now when\nwe talk about yarn we can have now a\ncluster size of more than 10 000 nodes\nand can run more than 100 000 concurrent\ntasks that's just to take care of your\nscalability when you talk about\ncompatibility applications which were\ndeveloped for hadoop version 1 which\nwere primarily map reduce kind of\nprocessing can run on yarn without any\ndisruption or availability issues when\nyou talk about resource utilization\nthere is a mechanism which takes care of\ndynamic allocation of cluster resources\nand this basically improves the resource\nutilization when we talk about\nmulti-tenancy so basically now the\ncluster can handle different kind of\nworkloads so you can use open source and\nproprietary data access engines you can\nperform real-time analysis you can be\ndoing graph processing you can be doing\nad hoc querying and this can be\nsupported for multiple workloads which\ncan run in parallel so this is what yarn\noffers so what is yarn as i mentioned\nyarn stands for yet another resource\nnegotiator so it is the cluster resource\nmanagement layer for your apache hadoop\necosystem which takes care of scheduling\njobs and assigning resources now just\nimagine when you would want to run a\nparticular application you would\nbasically be telling the cust cluster\nthat i would want resources to run my\napplications that application might be a\nmapreduce application that might be a\nhive query which is triggering a\nmapreduce that might be a big script\nwhich is triggering a mapreduce that\ncould be hive with days as an execution\nengine that could be a spark application\nthat could be a graph processing\napplication in any of these cases you\nwould still you as in in sense client or\nbasically an api or the application\nwould be requesting for resources yan\nwould take care of that so yarn would\nprovide the desired resources now when\nwe talk about resources we are mainly\ntalking about the network related\nresources we are talking about the cpu\ncores or as in terms of yarn we say\nvirtual cpu course we would talk about\nram that is in gb or mb or in terabytes\nwhich would be offered from multiple\nmachines and yarn would take care of\nthis so with yarn you could basically\nhandle different workloads now these are\nsome of the workloads which are showing\nup here you have the traditional\nmapreduce which is\nmainly batch oriented you could have an\ninteractive execution engine something\nas space you could have hbase which is a\ncolumn oriented or a four dimensional\ndatabase and that would be not only\nstoring data on sdfs but would also need\nsome kind of processing you could have\nstreaming functionalities which would be\nfrom storm or kafka or spark you could\nhave graph processing you could have\nin-memory processing such as spark and\nits components and you could have many\nothers so these are different frameworks\nwhich could now run and which can run on\ntop of yarn so how does the undo that\nnow when we talk about yarn this is how\na overall yarn architecture looks so at\none end you have the client now client\ncould be basically your edge node where\nyou have some applications which are\nrunning it could be an api which would\nwant to interact with your cluster it\ncould be a user triggered application\nwhich wants to run some jobs which are\ndoing some processing so this client\nwould submit a job request now what is\nresource manager doing resource manager\nis the master of your processing layer\nin hadoop version 1 we basically had job\ntracker and then we had task trackers\nwhich were running on individual nodes\nso your task trackers were sending your\nheart beats to the job tracker your task\ntrackers were sending it their resource\ninformation and job tracker was the one\nwhich was tracking the resources and it\nwas doing the job scheduling and that's\nhow as i mentioned earlier job tracker\nwas overburdened so job tracker is now\nreplaced by your resource manager which\nis the master for your processing layer\nyour task trackers are replaced by node\nmanagers which would be then running on\nevery node and we have a temporary demon\nwhich you see here in blue and that's\nyour app master so this is what we\nmentioned when we say yet another\nresource negotiator so appmaster would\nbe existing in a hadoop version 2. now\nwhen we talk about your resource manager\nresource manager is the master for\nprocessing layer so it would already be\nreceiving heartbeats and you can say\nresource information from multiple node\nmanagers which would be running on one\nor multiple machines and these node\nmanagers are not only updating their\nstatus but they are also giving an\ninformation of the amount of resources\nthey have now when we talk about\nresources we should understand that if\ni'm talking about this node manager then\nthis has been allocated some amount of\nram for processing and some amount of\ncpu cores and that is just a portion of\nwhat the complete node has so if my node\nhas say imagine my node has around 100\ngb ram and i have saved 60 cores all of\nthat cannot be allocated to node manager\nso node manager is just one of the\ncomponents of hadoop ecosystem it is the\nslave of the processing layer so we\ncould say keeping in all the aspects\nsuch as different services which are\nrunning might be cloudera or hortonworks\nrelated services running system\nprocesses running on a particular node\nsome portion of this would be assigned\nto node manager for processing so we\ncould say for example say 60 gb ram per\nnode and say 40 cpu cores so this is\nwhat is allocated for the node manager\non every machine similarly we would have\nhere similarly we would have here so\nnode manager is constantly giving an\nupdate to resource manager about the\nresources what it has probably there\nmight be some other applications running\nand node manager is already occupied so\nit gives an update now we also have a\nconcept of containers which is basically\nwe will we will talk about which is\nabout these resources being broken down\ninto smaller parts so resource manager\nis keeping a track of the resources\nwhich every node manager has and it is\nalso responsible for taking care of the\njob request how do these things happen\nnow as we see here resource manager at a\nhigher level you can always say this is\nthe processing master which does\neverything but in reality it is not the\nresource manager which is doing it but\nit has internally different services or\ncomponents which are helping it to do\nwhat it is supposed to do now let's look\nfurther now as i mentioned your resource\nmanager has these\nservices or components which basically\nhelps it to do the things it is\nbasically a an architecture where\nmultiple components are working together\nto achieve what yarn allows so resource\nmanager has mainly two components that\nis your scheduler and applications\nmanager and these are at high level four\nmain components here so we talk about\nresource manager which is the processing\nmaster you have node managers which are\nthe processing slaves which are running\non every nodes you have the concept of\ncontainer and you have the concept of\napplication master how do all these\nthings work now let's look at yarn\ncomponents so resource manager basically\nhas two main components you can say\nwhich assist resource manager in doing\nwhat it is capable of so you have\nscheduler and applications manager now\nthere is when you talk about\nresources there is always a requirement\nfor the applications which need to run\non cluster of resources so your\napplication which has to run which was\nsubmitted by client needs resources and\nthese resources are coming in from\nmultiple machines wherever the relevant\ndata is stored and a node manager is\nrunning so we always know that node\nmanager is co-located with data nodes\nnow what does the scheduler do so we\nhave different kind of schedulers here\nwe have basically a capacity scheduler\nwe have a fair scheduler or we could\nhave a fifo scheduler so there are\ndifferent schedulers which take care of\nresource allocation so your scheduler is\nresponsible for allocating resources to\nvarious running applications now imagine\na particular environment where you have\ndifferent teams or different departments\nwhich are working on the same cluster so\nwe would call the cluster as a\nmulti-tenant cluster and on the\nmulti-tenant cluster you would have\ndifferent applications which would want\nto run simultaneously accessing the\nresources of the cluster how is that\nmanaged so there has to be some\ncomponent which has a concept of pooling\nor queuing so that different departments\nor different users can get dedicated\nresources or can share resources on the\ncluster so scheduler is responsible for\nallocating resources to various running\napplications now it does not perform\nmonitoring or tracking of the status of\napplications that's not the part of\nscheduler it does not offer any\nguarantee about restarting the failed\ntasks due to hardware or network or any\nother failures scheduler is mainly\nresponsible for allocating resources now\nas i mentioned you could have different\nkind of schedulers you could have a fifo\nscheduler which was mainly in older\nversion of hadoop which stands for first\nin first out you could have a fair\nscheduler which basically means\nmultiple applications could be running\nin the cluster and they would have a\nfair share of the resources you could\nhave a capacity scheduler which would\nbasically have dedicated or fixed amount\nof resources across the cluster now\nwhichever scheduler is being used\nscheduler is mainly responsible for\nallocating resources then it's your\napplications manager now this is\nresponsible for accepting job\nsubmissions now as i said at higher\nlevel we could always say resource\nmanager state doing everything it is\nallocating the resources it is\nnegotiating the resources it is also\ntaking care of listening to the clients\nand taking care of job submissions but\nwho is doing it in real it is these\ncomponents so you have applications\nmanager which is responsible for\naccepting job submissions it negotiates\nthe first container for executing the\napplication specific application master\nit provides the service for restarting\nthe application master now how does this\nwork how do these things happen in\ncoordination now as i said your node\nmanager is the slave process which would\nbe running on every machine slave is\ntracking the resources what it has it is\ntracking the processes it is taking care\nof running the jobs and basically it is\ntracking each container resource\nutilization so let's understand what is\nthis container so normally when you talk\nabout a application request which comes\nfrom a client so let's say this is my\nclient which is requesting or which is\ncoming up with an application which\nneeds to run on the cluster now this\napplication could be anything it first\ncontacts your master that's your\nresource manager which is the master for\nyour processing layer now as i mentioned\nand as we already know that your name\nnode which is the master of your cluster\nhas the metadata in its ram which is\naware of the data being split into\nblocks the blocks when stored on\nmultiple machines and other information\nso obviously there was a interaction\nwith the master which has given this\ninformation of the relevant nodes where\nthe data exists now for the processing\nneed your client basically the\napplication which needs to run on the\ncluster so your resource manager which\nbasically has the scheduler which takes\ncare of allocating resources resource\nmanager has mainly these two components\nwhich are helping it to do its work now\nfor a particular application which might\nbe needing data from multiple machines\nnow we know that we would have multiple\nmachines where we would have node\nmanager running we would have a data\nnode running and data nodes are\nresponsible for storing the data on disk\nso your resource manager has to\nnegotiate the resources now when i say\nnegotiating the resources it could\nbasically ask each of these node\nmanagers for some amount of resources\nfor example it would be saying can i\nhave 1 gb of ram and 1 cpu core from you\nbecause there is some data residing on\nyour machine and that needs to be\nprocessed as part of my application can\ni again have one gb and one cpu core\nfrom you and this is again because some\nrelevant data is stored and this request\nwhich resource manager makes of holding\nthe resources of total resources which\nthe node manager has your resource\nmanager is negotiating or is asking for\nresources from the processing slave so\nthis request of holding resources can be\nconsidered as a container so resource\nmanager now we know it is not actually\nthe resource manager but it is the\napplication manager which is negotiating\nthe resources so it negotiates the\nresources which\nare called container so this request of\nholding resource can be considered as a\ncontainer so basically a container can\nbe of different sizes we will talk about\nthat so resource manager negotiates the\nresources with node manager now node\nmanager which is already giving an\nupdate of the resources it has\nwhat amount of resources it holds how\nmuch busy it is can basically approve or\ndeny this request so node manager would\nbasically approve in saying yes i could\nhold these resources i could give you\nthis container of this particular size\nnow once the container has been approved\nor allocated or you can say granted by\nyour node manager resource manager now\nknows that resources to process the\napplication are available and guaranteed\nby the node manager so resource manager\nstarts a temporary demon called\nappmaster so this is a piece of code\nwhich would also be running in one of\nthe containers it would be running in\none of the containers which would then\ntake care of execution of tasks in other\ncontainers so your application master is\nper application so if i would have 10\ndifferent applications coming in from\nthe client then we would have 10 app\nmasters one app master being responsible\nfor per application now what does this\napp master do it basically is a piece of\ncode which is responsible for execution\nof the application so your app master\nwould run in one of the containers and\nthen it would use the other containers\nwhich node manager is guaranteed that it\nwill give when the request application\nrequest comes to it and using these\ncontainers the app master will run the\nprocessing tasks within these designated\nresources so it is mainly the\nresponsibility of application master to\nget the execution done and then\ncommunicate it to the master so resource\nmanager is tracking the resources it is\nnegotiating the resources and once the\nresources have been negotiated it\nbasically gives the control to\napplication master application master is\nthen running within one of the\ncontainers on one of the nodes and using\nthe other containers to take care of\nexecution this is how it looks so\nbasically container as i said is a\ncollection of resources like cpu memory\nyour disk which would be used or which\nalready has the data and network so your\nnode manager is basically looking into\nthe request from application master\nand it basically is granting this\nrequest or basically is allocating these\ncontainers now again we could have\ndifferent sizing of the containers let's\ntake an example here so as i mentioned\nfrom the total resources which are\navailable for a particular node some\nportion of resources are allocated to\nthe node manager so let's imagine this\nis my node where node manager as a\nprocessing slave is running so from the\ntotal resources which the node has some\nportion of ram and cpu cores is\nbasically allocated to the node manager\nso i could say out of total 100 gb ram\nwe can say around\n60 cores which the particular node has\nso this is my ram which the node has and\nthese are the cpu cores which the node\nhas some portion of it right so we can\nsay might be\n70 or 60 percent of the total resources\nso we could say around 60 gb ram and\nthen we could say around 40 v cores have\nbeen allocated to node manager so there\nare these settings which are given in\nthe yarn hyphen site file now apart from\nthis allocation that is 60 gb ram and\n40v cores we also have some properties\nwhich say what will be the container\nsizes so for example we could have a\nsmall container setting which could say\nmy every container could have 2gb ram\nand say one virtual cpu core so this is\nmy smallest container so based on the\ntotal resources you could calculate how\nmany such small containers could be\nrunning so if i say 2gb ram then i could\nhave around 30 containers but then i am\ntalking about one virtual cpu core so\ntotally i could have around 30\nsmall\ncontainers which could be running in\nparallel on a particular node and as of\nthat calculation you would say 10 cpu\ncores are not being utilized you could\nhave a bigger container size which could\nsay i would go for 2 cpu cores and 3gb\nram\nso 3gb ram and 2 cpu cores so that would\ngive me around 20 containers of bigger\nsize so this is the container sizing\nwhich is again defined in the yarn\nhyphen site file so what we know is on a\nparticular node which has this kind of\nallocation either we could have 30 small\ncontainers running or we could have 20\nbig containers running and same would\napply to multiple nodes so node manager\nbased on the request from application\nmaster can allocate these containers now\nremember it is within this one of these\ncontainers you would have an application\nmaster running and other containers\ncould be used for your processing\nrequirement application master which is\nper application it is the one which uses\nthese resources it basically manages or\nuses these resources for individual\napplication so remember if we have 10\napplications running on yarn then it\nwould be 10 application masters one\nresponsible for each application your\napplication master is the one which also\ninteracts with the scheduler to\nbasically know how much amount of\nresources could be allocated for one\napplication and your application master\nis the one which uses these resources\nbut it can never negotiate for more\nresources to node manager application\nmaster cannot do that application master\nhas to always go back to resource\nmanager if it needs more resources so it\nis always the resource manager an\ninternally resource manager component\nthat is application manager which\nnegotiates the resources at any point of\ntime due to some node failures or due to\nany other requirements if application\nmaster\nneeds\nmore resources on one or multiple nodes\nit will always be contacting the\nresource manager internally the\napplications manager for more containers\nnow this is how it looks so your client\nsubmits the job request to resource\nmanager now we know that resource\nmanager internally has scheduler an\napplications manager node managers which\nare running on multiple machines are the\nones which are tracking their resources\ngiving this information to the source\nmanager so that resource manager or i\nwould say its component applications\nmanager could request resources from\nmultiple node managers when i say\nrequest resources it is these containers\nso your resource manager basically will\nrequest for the resources on one or\nmultiple nodes node manager is the one\nwhich approves these containers and once\nthe container has been approved your\nresource manager triggers a piece of\ncode that is application master which\nobviously needs some resources so it\nwould run in one of the containers and\nwill use other containers to do the\nexecution so your client submits an\napplication to resource manager resource\nmanager allocates a container or i would\nsay this is at a high level right\nresource manager is negotiating the\nresources and internally who is\nnegotiating the resources it is your\napplications manager who is granting\nthis request it is node manager and\nthat's how we can say resource manager\nlocates a container application master\nbasically contacts the related node\nmanager because it needs to use the\ncontainers node manager is the one which\nlaunches the container or basically\ngives those resources within which an\napplication can run an application\nmaster itself will then accommodate\nitself in one of the containers and then\nuse other containers for the processing\nand it is within these containers the\nactual execution happens now that could\nbe a map task that could be a reduced\ntask that could be a spark executor\ntaking care of spark tasks and many\nother processing\nso before we look into the demo on how\nyarn works i would suggest looking into\none of the blogs from cloudera so you\ncan just look for yarn untangling and\nthis is really a good blog which\nbasically talks about the overall\nfunctionality which i explained just now\nso as we mentioned here so you basically\nhave the master process you have the\nworker process which basically takes\ncare of your processing your resource\nmanager being the master and node\nmanager being the slave this also talks\nabout the resources which each node\nmanager has\nit talks about the yarn configuration\nfile where you give all these properties\nit basically shows you node manager\nwhich reports the amount of resources it\nhas to resource manager now remember if\nworker node shows 18 to 8 cpu cores and\n128 gb ram and if your node manager says\n64 v cores and ram 128 gb then that's\nnot the total capacity of your node it\nis some portion of your node which is\nallocated to node manager now once your\nnode manager reports that your resource\nmanager is requesting for containers\nbased on the application what is a\ncontainer it is basically a logical name\ngiven to a combination of vcore and\nram\nit is within this container where you\nwould have basically the process running\nso once your application starts and once\nnode manager is guaranteed these\ncontainers your application or your\nresource manager has basically already\nstarted an application master within the\ncontainer and what does that application\nmaster do it uses the other containers\nwhere the tasks would run so this is a\nvery good blog which you can refer to\nand this also talks about mapreduce if\nyou have already followed the mapreduce\ntutorials in past then you would know\nabout the different kind of tasks that\nis map and reduce and these map and\nreduce tasks could be running within the\ncontainer in one or multiple as i said\nit could be map task it could be reduced\ntask it could be a spark based task\nwhich would be running within the\ncontainer now once the task finishes\nbasically that resources can be freed up\nso the container is released and the\nresources are given back to yarn so that\nit can take care of further processing\nif you'll further look in this blog you\ncan also look into the part two of it\nwhere you talk mainly about\nconfiguration settings you can always\nlook into this which talks about why and\nhow much resources are allocated to the\nnode manager it basically talks about\nyour operating system overhead it talks\nabout other services it talks about\nclouded our hotend works related\nservices running and other processes\nwhich might be running and based on that\nsome portion of ram and cpu cores would\nbe allocated to node manager so that's\nhow it would be done in the yarn hyphen\nsite file and this basically shows you\nwhat is the total amount of memory and\ncpu cores which is allocated to node\nmanager then within every machine where\nyou have a node manager running on every\nmachine in the yarn hyphen site file you\nwould have such properties which would\nsay what is the minimum container size\nwhat is the maximum container size in\nterms of ram what is the minimum for cpu\ncores what is the maximum for cpu cores\nand what is the incremental size in\nwhere ram and cpu cores can increment so\nthese are some of the properties which\ndefine how containers are allocated for\nyour application request so have a look\nat this and this could be a good\ninformation which talks about different\nproperties now you can look further\nwhich talks about scheduling if you look\nin this particular blog which also talks\nabout scheduling where it talks about\nscheduling in yarn which talks about\nfair scheduler or you basically having\ndifferent cues in which allocations can\nbe done you also have different ways in\nwhich queues can be managed and\ndifferent schedulers can be used so you\ncan always look at this series of blog\nyou can also be checking for yarn\nschedulers and then search for a\nhadoop\ndefinitive\nguide and that could give you some\ninformation on how it looks when you\nlook for hadoop definitive guide so if\nyou look into this book which talks\nabout the different resources as i\nmentioned so you could have a fee for\nscheduler that is first in first out\nwhich basically means if a long running\napplication is submitted to the cluster\nall other small running applications\nwill have to wait there is no other way\nbut that would not be a preferred option\nif you look in fifo scheduler if you\nlook for capacity scheduler it basically\nmeans that you could have different\nqueues created and those queues would\nhave resources allocated so then you\ncould have a production queue where\nproduction jobs are running in a\nparticular queue which has fixed amount\nof resources allocated you could have a\ndevelopment queue where development jobs\nare running and both of them are running\nin parallel you could then also look\ninto fair scheduler which basically\nmeans again multiple applications could\nbe running on the cluster however they\nwould have a fair share so when i say\nfair share in brief what it means is\nif i had given 50 percent of resources\nto a queue for production and 50 percent\nof resources for a queue of development\nand if both of them are running in\nparallel then they would have access to\n50 percent of cluster resources however\nif one of the queue is unutilized then\nsecond queue can utilize all cluster\nresources so look into the fair\nscheduling part it also shows you about\nhow allocations can be given and you can\nlearn more about schedulers and how cues\ncan be used for managing multiple\napplications now we will spend some time\nin looking into few ways or few quick\nways in interacting with yarn in the\nform of a demo to understand and learn\non how\nyarn works we can look into a particular\ncluster now here we have a designated\ncluster which can be used you could be\nusing the similar kind of commands on\nyour apache based cluster or a cloudera\nquick start vm if you already have or if\nyou have a cloud error or a hortonworks\ncluster running there are different ways\nin which we can interact with yarn and\nwe can look at the information one is\nbasically looking into the admin console\nso if i would look into cloud data\nmanager which is basically an admin\nconsole for a cloudera's distribution of\nhadoop similarly you could have a\nhortonworks cluster than access to the\nadmin console so if you have even read\naccess for your cluster and if you have\nthe admin console then you can search\nfor yarn as a service which is running\nyou can click on yarn as a service and\nthat gives you different tabs so you\nhave the instances which tells basically\nwhat are the different roles for your\nyarn service running so we have here\nmultiple node managers now some of them\nshowing stop status but that's nothing\nto worry so we have three and six node\nmanagers we have resource manager which\nis one but then that can also be in a\nhigh availability where you can have\nactive and standby you also have a job\nhistory server which would show you the\napplications once they have completed\nnow you can look at the yarn\nconfigurations and as i was explaining\nyou can always look for the properties\nwhich are related to the allocation so\nyou can here search for\ncourse and that should show you the\nproperties which talk about the\nallocations so here if we see we can be\nlooking for\nyarn app mapreduce application master\nresource cpu course what is the cpu\ncourse allocated for mapreduce map task\nreduce task you can be looking at yarn\nnode manager resource cpu course which\nbasically says every node manager on\nevery node would be allocated with six\ncpu cores and the container sizing is\nwith minimum allocation of one cpu core\nand the maximum could be two cpu cores\nsimilarly you could also be searching\nfor memory allocation and here you could\nthen scroll down to see what kind of\nmemory allocation has been done for the\nnode manager so if we look further it\nshould give me information of node\nmanager which basically says here that\nthe container minimum allocation is 2 gb\nthe maximum is 3 gb and we can look at\nnode manager which has been given 25 gb\nper node so it's a combination of this\nmemory and cpu cores which is the total\namount of resources which have been\nallocated to every node manager now we\ncan always look into applications tab\nthat would show us different\napplications which are submitted on yarn\nfor example right now we see there is a\nspark application running which is\nbasically a user who is using spark\nshell which has triggered a application\non spark and that is running on yarn you\ncan look at different applications\nworkload information you can always do a\nsearch based on the number of days how\nmany applications have run and so on you\ncan always go to the web ui and you can\nbe searching for the resource manager\nweb ui and if you have access to that it\nwill give you overall information of\nyour cluster so this basically says that\nhere we have 100 gb memory allocated so\nthat could be say 25 gb per node and if\nwe have four node managers running and\nwe have 24 cores which is six cores per\nnode if we look further here into nodes\ni could get more information so this\ntells me that i have four node managers\nrunning and node managers basically have\n25 gb memory allocated per node and six\ncores out of which some portion is being\nutilized we can always look at the\nscheduler here which can give us\ninformation what kind of scheduler has\nbeen allocated so we basically see that\nthere is just a root cue and within root\nyou have default queue and you have\nbasically user's queue based on\ndifferent users we can always scroll\nhere and that can give us information if\nit is a fair share so here we see that\nmy root dot default has 50 percent of\nresources and the other queue also has\n50 percent of resources which also gives\nme an idea that a fair scheduler is\nbeing used we can always confirm that if\nwe are using a fair scheduler or a\ncapacity scheduler which takes care of a\nlocation so search for scheduler\nand that should give you some\nunderstanding of what kind of scheduler\nis being used and what are the\nallocations given for that particular\nscheduler so here we have fair scheduler\nit shows me you have under root you have\nthe root q which has been given hundred\n100 capacity and then you have within\nthat default which also takes 100\nso this is how you can understand about\nyarn by looking into the yarn web ui you\ncan be looking into the configurations\nyou can look at applications you can\nalways look at different actions now\nsince we do not have admin access the\nonly information we have is to download\nthe client configuration we can always\nlook at the history server which can\ngive us information of all the\napplications which have successfully\ncompleted now this is from your yarn ui\nwhat i can also do is i can be going\ninto hue which is the web interface and\nyour web interface also basically allows\nyou to look into the jobs so you can\nclick on hue web ui and if you have\naccess to that it should show up or you\nshould have a way to get to your hue\nwhich is a graphical user interface\nmainly comes with your cloud error you\ncan also configure that with apache\nhortonworks has a different way of\ngiving you the web ui access you can\nclick and get into hue and that is also\none way where you can look at yarn you\ncan look at the jobs which are running\nif there are some issues with it and\nthese these are your web interfaces so\neither you look from\nyarn web ui or here in hue you have\nsomething called as job browser which\ncan also give you information of your\ndifferent applications which might have\nrun so here i can just remove this one\nwhich would basically give me a list of\nall the different kind of jobs or\nworkflows which were run so either it\nwas a spark based application or it was\na map reduce or it was coming from hive\nso here i have list of all the\napplications and it says this was a\nmapreduce this was a spark something was\nkilled something was successful\nand this was basically a probably a hive\nquery which triggered a mapreduce job\nyou can click on the application and\nthat tells you how many tasks were run\nfor it so there was a map task which ran\nfor it you can get into the metadata\ninformation which you can obviously you\ncan also look from the yarn ui to look\ninto your applications which can give\nyou a detailed information of if it was\na map reduce how many map and reduce\ntasks were run what were the different\ncounters if it was a spark application\nit can let you follow through spark\nhistory server or job history server so\nyou can always use the web ui to look\ninto the jobs you can be finding in lot\nof useful information here you can also\nbe looking at how many resources were\nused and what happened to the job was it\nsuccessful did it fail and what was the\njob status now apart from\nweb ui which always you might not have\naccess to so in a particular cluster in\na production cluster there might be\nrestrictions and the organization might\nnot have\naccess given to all the users to\ngraphical user interface like you or\nmight be you would not have access to\nthe cloud era manager or admin console\nbecause probably organization is\nmanaging multiple clusters using this\nadmin console so the one way which you\nwould have access is your web console or\nbasically your edge node or client\nmachine from where you can connect to\nthe cluster and then you can be working\nso let's log in here and now here we can\ngive different commands so this is the\ncommand line from where you can have\naccess to different details you can\nalways check by just typing in map red\nwhich gives you different options where\nyou can look at the map reduce related\njobs you can look at different queues if\nthere are queues configured you can look\nat the history server or you can also be\ndoing some admin stuff provided you have\naccess so for example if i just say map\nred and q here this basically gives me\nan option says what would you want to do\nwould you want to list all the queues do\nyou want information on a particular\nqueue so let's try a list\nand that should give you different\nqueues which were being used now here we\nknow that per user a queue dynamically\ngets created which is under root dot\nusers and that gives me what is the\nstatus of the queue what is the capacity\nhas there been any kind of maximum\ncapacity or capping done so we get to\nsee a huge list of queues which\ndynamically get configured in this\nenvironment and then you also look at\nyour root dot default i could have also\npicked up one particular queue and i\ncould have said show me the jobs so i\ncould do that now here we can also give\na yarn command so let me just clear the\nscreen and i will say yarn and that\nshows me different options so apart from\nyour web interface\nsomething like web ui apart from your\nyarn's web ui you could also be looking\nfor information using yarn commands here\nso these are some list of commands which\nwe can check now you can just type in\nyarn and version if you would want to\nsee the version which basically gives\nyou information of what is the hadoop\nversion being used and what is the\nvendor-specific distribution version so\nhere we see we are working on cloudera's\ndistribution 5.14 which is internally\nusing hadoop 2.6 now similarly you can\nbe doing a yarn application list so if\nyou give this that could be an\nexhaustive list of all the applications\nwhich are running or applications which\nhave completed so here we don't see any\napplications because right now probably\nthere are no applications which are\nrunning it also shows you you could be\npulling out different status such as\nsubmitted accepted or running now you\ncould also say i would want to see the\nservices that i finished running so i\ncould say yarn application list and app\nstate says finished so here we could be\nusing our command so i could say yarn\nlist\nand then i would want to see the app\nstates which gives me the applications\nwhich have finished and we would want to\nlist all the applications which finished\nnow there that might be applications\nwhich succeeded right and there is a\nhuge list of application which is coming\nin from the history server which is\nbasically showing you the huge list of\napplications which have completed so\nthis is one way and then you could also\nbe searching for one particular\napplication if you would want to search\na particular application if you have the\napplication id you could always be doing\na grip that's a simple way i could say\nbasically let's pick up this one and if\ni would want to search for this if i\nwould want more details on this i could\nobviously do that\nby calling in my previous command and\nyou could do a grip if that's what you\nwant to do and if you would want to\nsearch is there any application which is\nin the list of my applications that\nshows my application i could pull out\nmore information about my application so\ni could look at the log files for a\nparticular application by giving the\napplication id so i could say yarn logs\nnow that's an option and every time\nanytime you have a doubt just hit enter\nit will always give you options what you\nneed to give with a particular command\nso i can say yarn logs application\nid now we copied an application id and\nwe could just give it here we could give\nother options like app owner or if you\nwould want to get into the container\ndetails or if you would want to check on\na particular node now here i am giving\nyarn logs and then i'm pointing it to\nan application id and it says the log\naggregation has not completed might be\nthis was might be this was an\napplication which was triggered based on\na particular interactive shell or based\non a particular query so there is no log\nexisting for this particular application\nyou can always look at the status of an\napplication you can kill an application\nso here you can be saying yan yan\napplication and then what would you want\nto do with an application hit and enter\nit shows you the different options so we\njust tried app states you could always\nlook at the last one which says status\nand then for my status i could be giving\nmy application id so that tells me what\nis the status of this application it\nconnects to the resource manager it\ntells me what's the application id what\nkind of application it was who ran it\nwhich was the queue where the job was\nrunning what was the start and end time\nwhat is the progress the status of it if\nit is finished or if it has succeeded\nand then it basically gives me also an\ninformation of where the application\nmaster was running it gives me the\ninformation where you can find this job\ndetails in history server if you are\ninterested in looking into it also gives\nyou a aggregate resource allocation\nwhich tells how much gb memory and how\nmany c core seconds it used so this is\nbasically looking out at the application\ndetails now i could kill an application\nif the application was already running i\ncould always do a yarn application\nminus skill and then i could be giving\nmy application now i could try killing\nthis however it would say the\napplication is already finished if i had\nan application running and if my\napplication was already given an\napplication id by resource manager i\ncould just kill it i can also say yarn\nnode list which should give me a list of\nthe node managers now this is what we\nwere looking from the yarn web ui and we\nwere pulling out the information so we\ncan get this and kind of information\nfrom your command line always remember\nand always try to be well accustomed\nwith the command line so you can do\nvarious things from the command line and\nthen obviously you have the web uis\nwhich can help you with a graphical\ninterface easily able to access things\nnow you could be also starting the\nresource manager which we would not be\ndoing here because we are already\nrunning in a cluster so you could give a\nyarn resource manager you could get the\nlogs of resource manager if you would\nwant by giving yarn demon so we can try\nthat so you can say yarn and then demon\nso it says it does not find the demon so\nyou can give something like this get\nlevel and here i will have to give the\nnode and the ip address where you want\nto check the logs of resource manager so\nyou could be giving this for which we\nwill have to then get into cloud error\nmanager to look into the nodes and the\nip address you could be giving a command\nsomething like this which basically\ngives you the level of the log which you\nhave and i got this resource manager\naddress from the web ui now i can be\ngiving in this command to look into the\ndemand log and it basically says you\nwould want to look at the resource\nmanager related log and you have the log\n4j which is being used for logging the\nkind of level which has been set as info\nwhich can again be changed in the way\nyou're logging the information now you\ncan try any other commands also from\nyarn for example looking at the yarn rm\nadmin so you can always do a yarn rm\nadmin and this basically gives you a lot\nof other informations like refreshing\nthe cues or refreshing the nodes or\nbasically looking at the admin acls or\ngetting groups so you could always get\ngroup names for a particular user now we\ncould search for a particular user such\nas yarn or hdfs itself so i could just\nsay here i would want\nget\ngroups and then i could be searching for\nsay username hdfs so that tells me sdfs\nbelongs to a hadoop group similarly you\ncould search for say map red or you\ncould search for yarn so these are\nservice related users which\nautomatically get created and you can\npull out information related to these\nyou can always do a refresh nodes kind\nof command and that is mainly done\ninternally this can be useful when\nyou're doing commissioning\ndecommissioning but then in case of\ncloudera or hortonworks kind of cluster\nyou would not be manually giving this\ncommand because if you are doing a\ncommissioning decommissioning from an\nadmin console and if you are an\nadministrator then you could just\nrestart the services which are affected\nand that will take care of this but if\nyou were working in an apache cluster\nand if you were doing commissioning\ndecommissioning then you would be using\nin two commands refresh nodes and\nbasically that's for refreshing the\nnotes which should not be used for\nprocessing and similarly you could have\na command refresh notes which comes with\nsdfs so these are different options\nwhich you can use with your yarn on the\ncommand line you could also be using\ncurl commands to get more information\nabout your cluster by giving curl minus\nx and then basically pointing out to\nyour resource manager web ui address now\nhere i would like to print out the\ncluster related metrics and i could just\nsimply do this which basically gives me\na high level information of how many\napplications were submitted how many are\npending what is the reserved resources\nwhat is the available amount of memory\nor cpu cores and all the information\nsimilarly you can be using the same curl\ncommands to get more information like\nscheduler information so you would just\nreplace the metrics with scheduler and\nyou could get the information of the\ndifferent queues now that's a huge list\nwe can cancel this and that would give\nme a list of all the queues which are\nallocated and what are the resources\nallocated for each queue you could also\nget cluster information on application\nids and status running of applications\nrunning in yarn so you would have to\nreplace the last bit of it and you would\nsay i would want to look at the\napplications and that gives me a huge\nlist of applications then you can do a\ngrip and you can be filtering out\nspecific application related information\nsimilarly you can be looking at the\nnodes so you can always be looking at\nnode specific information which gives\nyou how many nodes you have but this\ncould be mainly used when you have an\napplication which wants to or a web\napplication which wants to use a curl\ncommand and would want to get\ninformation about your cluster from an\nhttp interface now when it comes to\napplication we can basically try running\na simple or a sample mapreduce job which\ncould then be triggered on yarn and it\nwould use the resources now i can look\nat my application here and i can be\nlooking into my specific directory which\nis this one which should have lot of\nfiles and directories which we have here\nnow i could pick up one of these and i\ncould be using a simple example to do\nsome processing let's take up this file\nso there is a file and i could run a\nsimple word count or i could be running\na hive query which triggers a mapreduce\njob i could even run a spark application\nwhich would then show that the\napplication is running on the cluster so\nfor example if i would say spark to\nshell now i know that this is an\ninteractive way of working with spark\nbut this internally triggers a spark\nsubmit and this runs an application so\nhere when you do a spark to shell by\ndefault it will contact yarn so it gets\nan application id it is running on yarn\nwith the master being yarn and now i\nhave access to the interactive way of\nworking with spark now if i go and look\ninto applications i should be able to\nsee my application which has been\nstarted here and it shows up here so\nthis is my application 3827\nwhich has been started on yarn and as of\nnow we can also look into the yarn ui\nand that shows me the application which\nhas been started which basically has one\nrunning container which has one cpu core\nallocated 2gb ram and it's in progress\nalthough we are not doing anything there\nso we can always look at our application\nfrom the yarn ui or as i mentioned from\nyour applications tab within yarn\nservices which gives us the information\nand you can even click on this\napplication to follow and see more\ninformation but you should be given\naccess to that now this is just a simple\napplication which i triggered using\nspark shell similarly i can basically be\nrunning a mapreduce now to run a\nmapreduce i can say hadoop jar and that\nbasically needs a class so we can look\nfor the default path which is opt cloud\nerror parcels cdh\nlib hadoop mapreduce hadoop mapreduce\nexamples and then we can look at this\nparticular jar file and if i hit on\nenter it shows me the different classes\nwhich are part of this jar and here i\nwould like to use word count so i could\njust give this i could say word count\nnow remember i could run the job in a\nparticular queue by giving in an\nargument here so i could say minus d map\nred dot job dot q\ndot name and then i can point my job to\na particular queue i can even give\ndifferent arguments in saying i would\nwant my mapreduce output to be\ncompressed or i want it to be stored in\na particular directory and so on so here\ni have the word count and then basically\nwhat i can be doing is i can be pointing\nit to a particular input path\nand then i can have my output which can\nbe getting stored here again a directory\nwhich we need to choose and i will say\noutput\nnew and i can submit my job now once i\nhave submitted my job it connects to\nresource manager it basically gets a job\nid it gets an application id it shows\nyou from where you can track your\napplication you can always go to the\nyarn ui and you can be looking at your\napplication and the resources it is\nusing so my application was not a big\none and it has already completed it\ntriggered one map task it launched one\nreduced task it was working on around 12\n466 records where you have then the\noutput of map which is these many number\nof output records which was then taken\nby a combiner and finally by a reducer\nwhich basically gives you the output so\nthis is my on application which has\ncompleted now i could be looking into\nthe yarn ui and if my job has completed\nyou might not see your application here\nso as of now it shows up here the word\ncount which i ran it also shows me my\nprevious spark shell job it shows me my\napplication is completed and if you\nwould want further information on this\nyou can click and go to the history\nserver if you have been given access to\nit or directly go to the history server\nweb ui where your application shows up\nit shows how many map and reduce tasks\nit was running you can click on this\nparticular application which basically\ngives you information of your map and\nreduce tasks you can look at different\ncounters for your application right you\ncan always look at map specific tasks\nyou can always look into one particular\ntask what it did on which node it was\nrunning or you can be looking at the\ncomplete application log so you can\nalways click on the logs and here you\nhave click here for full log which gives\nyou the information and you can always\nlook for your application which can give\nyou information of app master being\nlaunched or you could have\nsearch for the word container so you\ncould see a job which needs one or\nmultiple containers and then you could\nsay container is being requested then\nyou could see container is being\nallocated then you can see what is the\ncontainer size and then basically your\ntask moves from initializing to running\nin the container and finally you can\neven search for release which will tell\nyou that the container was released so\nyou can always look into the log for\nmore information so this is how you can\ninteract with yarn this is how you can\ninteract with your command line to look\nfor more information or using your yarn\nweb ui or you can also be looking into\nyour hue for more information welcome to\nscoop tutorial one of the many features\nof the hadoop ecosystem for the hadoop\nfile system what's in it for you today\nwe're going to cover the need for scoop\nwhat is scoop scoop features scoop\narchitecture scoop import scoop export\nscoop processing and then finally we'll\nhave a little hands-on demo on scoop so\nyou can see what it looks like so where\ndoes the need for scoop come in in our\nbig data hadoop file system processing\nhuge volumes of data requires loading\ndata from diverse sources into hadoop\ncluster you can see here we have our\ndata processing and this process of\nloading data from the heterogeneous\nsources comes with a set of challenges\nso what are the challenges maintaining\ndata consistency ensuring efficient\nutilization of resources especially when\nyou're talking about big data we can\ncertainly use up the resources when\nimporting terabytes and petabytes of\ndata over the course of time loading\nbulk data to hadoop was not possible\nit's one of the big challenges that came\nup when they first had the hadoop file\nsystem going and loading data using\nscript was very slow in other words\nyou'd write a script in whatever\nlanguage you're in and then it would\nvery slowly load each piece and parse it\nin so the solution scoop scooped helped\nin overcoming all the challenges to\ntraditional approach and could lead bulk\ndata from rdbms to hadoop very easily so\nthank your enterprise server you want to\ntake the from mysql or your sql and you\nwant to bring that data into your hadoop\nwarehouse your data filing system and\nthat's where scoop comes in so what\nexactly is scoop scoop is a tool used to\ntransfer bulk of data between hadoop and\nexternal data stores such as relational\ndatabases and my sql server or the\nmicrosoft sql server or my sql server so\nscoop equals sql plus hadoop and you can\nsee here we have our rdbms all the data\nwe have stored on there and then your\nscoop is the middle ground and brings\nthe import into the hadoop file system\nit also is one of the features that goes\nout and grabs the data from hadoop and\nexports it back out into an rdbms let's\ntake a look at scoop features sku\nfeatures has parallel import and export\nit has import results of sql query\nconnectors for all major rdbms databases\nkerberos security integration provides\nfull and incremental load so we look at\nparallel import and export scoop uses\nyarn yet another resource negotiator\nframework to import and export data this\nprovides fault tolerance on a top of\nparallelism scoop allows us to import\nthe result returned from an sql carry\ninto the hadoop file system or the hdfs\nand you can see here where the import\nresults of sql query come in scoop\nprovides connectors for multiple\nrelational database management system\nrdbms's databases such as mysql and\nmicrosoft sql server and has connectors\nfor all major rdbms databases scoop\nsupports kerberos computer network\nauthentication protocol that allows\nnodes communicating over a non-secure\nnetwork to prove their identity to one\nanother in a secure manner scoop can\nload the whole table or parts of the\ntable by a single command hence it\nsupports full and incremental load let's\ndig a little deeper into the scoop\narchitecture we have our client in this\ncase a hooded wizard behind his laptop\nyou never know who's going to be\naccessing the hadoop cluster and the\nclient comes in and sends their command\nwhich goes into scoop the client submits\nthe import export command to import or\nexport data data from different\ndatabases is fetched by scoop and so we\nhave\nenterprise data warehouse document based\nsystems you have connect connector for\nyour data warehouse a connector for\ndocument based systems which reaches out\nto those two entities and we have our\nconnector for the rdbms so connectors\nhelp in working with a range of popular\ndatabases multiple mappers perform map\ntasks to load the data onto hdfs the\nhedo file system and you can see here we\nhave the map task if you remember from\nhadoop hadoop is based on map reduce\nbecause we're not reducing the data\nwe're just mapping it over it only\naccesses the mappers and it opens up\nmultiple mappers to do parallel\nprocessing and you can see here the hdfs\nhbase hive is where the target is for\nthis particular one similarly multiple\nmap tests will export the data from hdfs\nonto rdbms using scoop export command so\njust like you can import it you can now\nexport it using the multiple map\nroutines scoop import so here we have\nour dbms data store and we have the\nfolders on there so maybe it's your\ncompany's database maybe it's an archive\nat google with all the searches going on\nwhatever it is usually you think with\nscoop you think sql you think mysql\nserver or microsoft sql server that kind\nof setup\nso it gathers the metadata and you see\nthe scoop import so introspect database\ntogether metadata primary key\ninformation and then it submits uh so\nyou can see submits map only job\nremember we talked about mapreduce it\nonly needs the map side of it because\nwe're not reducing the data we're just\nmapping it over scoop divides the input\ndata set into splits and uses individual\nmap tests to push the splits into hdfs\nso right into the hadoop file system and\nyou can see down on the right is kind of\na small depiction of a hadoop cluster\nand then you have scoop export so we're\ngoing to go the other direction and with\nthe other direction you have your hadoop\nfile system storage which is your hadoop\ncluster you have your scoop job and each\none of those clusters then gets a map\nmapper comes out to each one of the\ncomputers that has data on it\nso the first step is you've got to\ngather the metadata so step one you\ngather the metadata step two submits map\nonly job introspect database together\nmetadata primary key information scoop\ndivides the input data set into splits\nand uses individual map tests to push\nthe splits to rdbms scoop will export\nhadoop files back to rdms tables you can\nthink of this in a number of different\nmanners one of them would be if you're\nrestoring a backup from the hadoop file\nsystem into your enterprise machines\nthere's certainly many others as far as\nexploring data and data science so as we\ndig a little deeper into scoop input we\nhave our connect our jdbc and our url so\nspecify the jdbc connect string\nconnecting manager we specify the\nconnection manager class to use you can\nsee here driver with the class name\nmanually specify the jdbc driver class\nto use hadoop mapreduce home directory\noverride hadoop mapped home username set\nauthentication username and of course\nhelp print usage instructions and with\nthe export you'll see that we can\nspecify the jdbc connect string specify\nthe connection manager class to use\nmanually specify jdbc driver class to\nuse you do have to let it know to\noverride the hadoop map reduce home and\nthat's true on both of these and set\nauthentication username and finally you\ncan print out all your help setup so you\ncan see the format for scoop is pretty\nstraightforward both import and export\nso let's uh continue on our path and\nlook at scoop processing and what the\ncomputer goes through for that and we\ntalk about scoop processing first scoop\nruns in the hadoop cluster it imports\ndata from the rdbms the nosql database\nto the hadoop file system so remember we\nmight not be importing the data from a\nrdbms it might actually be coming from\nan osql sql there's many out there it\nuses mappers to slice the incoming data\ninto multiple formats and load the data\ninto hdfs it exports data back into an\nrdbms while making sure that the schema\nof the data in the database is\nmaintained so now that we've looked at\nthe basic commands in our scoop in the\nscoop processing or at least the basics\nas far as theory is concerned let's just\njump in and take a look at a demo on\nscoop\nfor this demo i'm going to use our\ncloudera quick start if you've been\nwatching our other demos we've done\nyou'll see that we've been using that\npretty consistently certainly this will\nwork in any of your\nyour horton sandbox which is also a\nsingle node testing machine cloudera is\none of um there's a docker version\ninstead of virtualbox and you can also\nset up your own hadoop cluster plan a\nlittle extra time if you're not an admin\nit's actually a pretty\nsignificant endeavor for an admin if\nyou've been admitting linux machines for\na very long time and you know a lot of\nthe commands i find for most admins it\ntakes them about two to four hours the\nfirst time they go in and create a\nvirtual machine and set up their own\nhadoop in this case though when you're\njust learning and getting set up best to\nstart with cloudera cloudera also\nincludes an installed version of mysql\nthat way you don't have to install the\nthe sql version for importing data from\nand 2. once you're in the cloudera quick\nstart you'll see it opens a nice centos\nlinux\ninterface and it has the desktop setup\non there this is really nice for\nlearning so you're not just looking at\ncommand lines and from in here it should\nopen up by default to hue if not you can\nclick on hue here's a kind of a fun\nlittle uh web-based interface under hue\ni can go under query i can pick an\neditor and we'll go right down to scoop\nso now i'm just going to load the scoop\neditor inner hue now i'm going to switch\nover and do this all in command line i\njust want to show that you can actually\ndo this in a hue through the web-based\ninterface the reason i like to do the\ncommand line is specifically on my\ncomputer it runs much quicker or if i do\nthe command line here and i run it it\ntends to have an extra lag or an added\nlayer in it so for this we're going to\ngo ahead and open our command line the\nsecond reason i do this is we're going\nto need to go ahead and edit our\nmysql so we have something to scoop in\notherwise i don't have anything going in\nthere and of course we zoom in we'll\nzoom in this and increase the size of\nour screen so for this demo our hands-on\ni'm going to use oracle virtualbox\nmanager and the cloudera quick start if\nyou're not familiar with this we do have\nanother tutorial we put out and you can\nsend a note in the youtube video below\nand let our team know and they'll send\nyou a link or come visit\nwww.simplylearn.com now this creates a\nlinux box on my windows computer so\nwe're going to be in linux and it'll be\nthe cloudera version\nwith scoop and we'll also be using mysql\nmysql server once inside the cloudera\nvirtualbox we'll go under the hue editor\nnow we're going to do everything in\nterminal window but i just want you to\nbe aware that under the hue editor you\ncan go under query editor and you'll see\nas we come down here here's our scoop on\nthis so you can run your scoop from in\nhere now before we do this we have to do\na little exploration in my sql and my\nsql server that way we know what data is\ncoming in so let me go ahead and open up\na terminal window in cloudera you have a\nterminal window at the top here that you\ncan just click on it open it up and let\nme just go ahead and zoom in on here go\nview and zoom in now to get into my sql\nserver you simply type in mysql and this\npart will depend on your setup now the\ncloudera quickstart comes up that the\nusername is root and the password is\ncloudera kind of a strange quirk is that\nyou can put a space between the minus u\nand the root but not between the minus p\nand the cloudera usually you'd put in a\nminus capital p and then it prompts you\nfor your password on here for this demo\ni don't worry too much about you knowing\nthe password on that so we'll just go\nright into my sql server since this is\nthe standard password for this quick\nstart and you can see we're now into\nmysql and we're going to do just a\ncouple of quick\ncommands in here there's show databases\nand you follow by the semicolon that's\nstandard in most of these shell commands\nso it knows it's the end of your shell\ncommand and you'll see in here in the\nquick start cloudera quickstart the\nmysql comes with a standard set of\ndatabases these are just\nsome of these have to do like with the\nuzi which is the uzi part of hadoop\nwhere others of these like customers and\nemployees and stuff like that those are\njust for demo purposes they come as a\nstandard setup in there so that people\ngoing in for the first time have a\ndatabase to play with which is really\ngood for us so we don't have to recreate\nthose databases and you will see in the\nlist here we have a retail underscore db\nand then we can simply do uh use\nretail underscore db this will set that\nas a default in mysql and then we want\nto go ahead and show the tables and if\nwe show the tables you can see under the\ndatabase the retail db database we have\ncategories customers departments order\nitems orders products so there's a\nnumber of tables in here and we're going\nto go ahead and just use a standard\nsql command and if you did our hive\nlanguage you'll note remember it's the\nsame for hql also on this we're just\ngoing to select star everything from\ndepartments\nso there's our departments table and\nwe're going to list everything on the\ndepartment's table and you'll see we've\ngot six lines in here and it has a\ndepartment id and a department name\ntwo for fitness three for footwear so on\nand so forth now at this point i can\njust go ahead and exit but it's kind of\nnice to have this data up here so we can\nlook at it and flip back and forth\nbetween the screens so i'm going to open\nup another terminal window and we'll go\nahead and zoom in on this also and it\nisn't too important for this particular\nsetup but it's always kind of fun fun to\nknow what your setup you're working with\nwhat is your host name and so we'll go\nahead and just type that in this is a\nlinux command and it's a\nhost name minus f and you see we're on\nquick start cloudera no surprise there\nnow this next command is going to be a\nlittle bit longer because we're going to\nbe doing our first scoop command and i'm\ngoing to do two of them we're going to\nlist databases and list tables it's\ngoing to take just a moment to get\nthrough this\nbecause there's a bunch of stuff going\non here so we have scoop we have list\ndatabases we have connect and under the\nconnect command we need to let it know\nhow we're connecting we're going to use\nthe jdbc this is a very standard one\njdbc mysql so you'll see that if you're\ndoing an sql database that's how you\nstarted off with and then the next part\nthis is where you have to go look it up\nit's however it was created so if your\nadmin created a mysql server with a\ncertain setup that's what you have to go\nby and you'll see that usually they list\nthis as localhost so you'll see\nsomething like localhost sometimes\nthere's a lot of different formats but\nthe most common is either localhost or\nthe actual connection so in this case we\nwant to go ahead and do quickstart 3306.\nand so quick start use the name of the\nlocal host database and how it's hosted\non here and when you set up the quick\nstart for um for hadoop under cloudera\nit's port 3306 is where that's coming in\nso that's where all that's coming from\nand so there's our path for that and\nthen we have to put in our password we\ntypically type password if you look it\nup password on the cloudera quick start\nis cloudera and we have to also let it\nknow the username and again if you're\ndoing this you'd probably put in a minus\ncapital you can actually just do it for\na prompt\nfor the password so if you leave that\nout it'll prompt you but for this\ndoesn't really matter i don't care if\nyou see my password it's the default one\nfor cloudera quickstart and then the\nusername on here is simply root and then\nwe're going to put our semicolon at the\nend and so we have here our full setup\nand we go ahead and list the databases\nand you'll see you might get some\nwarnings on here i haven't run the\nupdates on the quick start i suggest\nyou're not running the updates either if\nyou're doing this for the first time\nbecause it'll do some reformatting on\nthere and it quickly pass up and you can\nsee here's all of our the tables we went\nin there and if we go back to\non the previous window we should see\nthat these tables match so here we come\nin and here we have our databases and\nyou can see back up here where we had\nthe cm customers employees and so on so\nthe databases match and then we want to\ngo ahead and list the\ntables for a specific database so let's\ngo ahead and do that i'm a very lazy\ntypist so i'll put the up arrow in and\nyou can see here scoop list databases\nwe're just going to go back and change\nthis from databases to list tables so we\nwant to list the tables in here same\nconnection so most the connection is the\nsame except we need to know which tables\nwe're listing an interesting fact is you\ncan create a table without being under a\ndatabase so if you left this blank it\nwill show the open tables that aren't\nconnected directly to a database or\nunder a database but what we want to do\nis right past this last slash on the\n3306 we want to put that retail\nunderscore db because that's the\ndatabase we're going to be working with\nand this will go in there and show the\ntables listed under that database and\nhere we go we got categories customers\ndepartments order items and products if\nwe flip back here real quick there it is\nthe same thing we had we had categories\ncustomers departments order items and so\non and so let's go ahead and run our\nfirst import command and again i'm that\nlazy typer so we're going to do scoop\nand instead of list tables we want to go\nahead and import so there's our import\ncommand and so once we have our import\ncommand in there then we need to tell it\nexactly what we're going to import so\neverything else is the same we're\nimporting from the retail db so we keep\nthat and then at the very end we're\ngoing to tag on dash dash table that\ntells us we can tell it what table we're\nimporting from and we're going to import\ndepartments there we go so this is\npretty straightforward because it what's\nnice about this is you can see the\ncommands are the same i got the same\nconnection um i change it for the\nwhatever database i'm in\nthen i come in here our password the\nusername are going to be the same that's\nall under the mysql server setup and\nthen we let it know what table we're\nentering in we run this and this is\ngoing to actually go through the mapper\nprocess in hadoop so this is a mapping\nprocess it takes the data and it maps it\nup to different parts in the setup in\nhadoop on there and then saves that data\ninto the hadoop file system and it does\ntake it a moment to zip through which i\nkind of skipped over for you since it is\nrunning a you know it's designed to run\nacross the cluster not on a single node\nso when you're running on a single node\nit's going to run slow even if you\ndedicate a couple cores to it i think i\nput dedicated four cores to this one uh\nand so you can see right down here we\nget to the end it's now mapped in that\ninformation and then we can go in here\nwe can go under can flip back to our hue\nand under hue on the top i have there's\ndatabases and the second icon over is\nyour hadoop file system and we can go in\nhere and look at the hadoop file system\nand you'll see it show up underneath our\ndocuments there it is departments\ncloudera departments and you can see\nthere's always a delay when i'm working\nin hue which i don't like\nand that's the quick start issue that's\nnot necessarily running out on a server\nwhen i'm running it on a server you\npretty much have to run through some\nkind of server interface i still prefer\nthe terminal window it still runs a lot\nquicker but we'll flip back on over here\nto the command line and we can do the\nhadoop type in the hadoop fs and then\nlist minus ls and if we run this you'll\nsee underneath our hadoop file system\nthere is our departments which has been\nadded in and we can also do uh hadoop\nfs and this is kind of interesting for\nthose who've gone through the hadoop\nfile system everything you'll you'll\nrecognize this on here i'm going to list\nit the contents of departments and\nyou'll see underneath departments uh we\nhave part part\nm0001002003 and so this is interesting\nbecause this is how hadoop saves these\nfiles this is in the file system this is\nnot in hive so we didn't directly import\nthis into hive we put this in the hadoop\nfile system depending on what you're\ndoing you would then write the schema\nfor hive to look at the hadoop file\nsystem certainly visit our hive tutorial\nfor more information on hive specific\nso you can see in here are different\nfiles that it forms that are part of\ndepartments and we can do something like\nthis we can look at the contents of one\nof these files fs minus ls or a number\nof the files and we'll simply do the\nfull path which is user cloudera and\nthen we already know the next one is\ndepartments\nand then after departments we're going\nto put slash part star so this is going\nto say anything that has part in it\nso we have part dash m 0 0 0 and so on\nwe can go ahead and cat use that cut\ncommand or that list command to bring\nthose up and then we can use the cat\ncommand to actually display the contents\nand that's a linux command hadoop linux\ncommand the cat captinate not to be\nconfused with catatonic catastrophic\nthere's a lot of cat got your tongue and\nwe see here fitness footwear apparel\nthat should look really familiar because\nthat's what we had in our mysql server\nwe went in here we did a select all on\nhere there it is fitness footwear\napparel golf outdoors and fan shop and\nthen of course it's really important\nlet's look back on over here to be able\nto tell it where to put the data so we\ngo back to our import command so here's\nour scoop import we have our connect we\nhave the db underneath our connection\nour my sql server we have our password\nour username the table going where it's\ngoing to i mean the table where it's\ncoming from uh and then we can add a\ntarget on here we can put in uh target\ndash directory and you do have to put\nthe full path that's a hadoop thing it's\na good practice to be in and we're gonna\nadd it to department uh we'll just do\ndepartment one and so here we now add a\ntarget directory in here and user\ncloudera department one and this will\ntake just a moment before so i'll go\nahead and skip over the process since\nit's going to run very slowly it's only\nrunning on like i said a couple cores\nand it's also on a single node and now\nwe can do the\nhadoop let's just do the up arrow file\nsystem list we want just straight list\nand when we do the hadoop file system\nminus ls or list you'll see that we now\nhave department one and we can of course\ndo list department one and you can see\nwe have the files inside department one\nand they mirrored what we saw before\nwith the same files in there and the\npart m o zero zero and so on if we want\nto look at them it'd be the same thing\nwe did before with the cat so except\ninstead of departments\nwe'd be department one there we go\nsomething that's going to come up with\nthe same data we had before now one of\nthe important things when you're\nimporting data and it's always a\nquestion to ask is do you filter the\ndata before it comes in do we want to\nfilter this data as it comes in so we're\nnot storing everything in our file\nsystem you would think hadoop big data\nput it all in there i know from\nexperience that putting it all in there\ncan turn a couple hundred terabytes into\na petabyte very rapidly and suddenly\nyou're having to really add on to that\ndata store and you're storing duplicate\ndata sometimes so you really need to be\nable to filter your data out and so\nlet's go ahead and use our up arrow to\ngo to our last import since it's still a\nlot the same stuff\nso we have all of our commands under\nimport we have the target\nwe're going to change this to department\n2 so we're going to create a new\ndirectory for this one and then after\ndepartments there's another command that\nwe didn't really slide in here and\nthat's our mapping and i'll show you\nwhat this looks like in a minute but\nwe're going to put m3 in there that\ndoesn't have nothing to do with the\nfiltering i'll show you that in a second\nthough what that's for and we just want\nto put in where uh so where and what is\nthe where in this case we want to know\nwhere department\nid and if you want to know where that\ncame from we can flip back on over here\nwe have department underscore ids this\nis where that's coming from that's just\nthe name of the column on here so we\ncome in here to department id\nis greater than four simple um logic\nthere you can see where you'd use that\nfor maybe creating buckets for ages uh\nyou know age from 10 to 15 20 to 30. you\nmight be looking for i mean there's all\nkinds of reasons why you could use the\nwhere command on here and filter\ninformation out maybe you're doing word\ncounting and you want to know words that\nare used less than 100 times you want to\nget rid of the and is and and all the\nstuff that's used over and over again uh\nso we'll go ahead and put the where and\nthen department id is greater than four\nwe'll go ahead and hit enter on here and\nthis will create our department two set\nup on this uh and i'll go ahead and skip\nover some of the runtime again it runs\nreally slow on a single node real quick\npage through our commands\nlet's see here we go\nour list and we should see underneath\nthe list the department 2 on here now\nand there it is department 2 and then i\ncan go ahead and do list department two\nyou'll see the contents in here and\nyou'll see that there is only three maps\nand it could be that the data created\nthree maps but remember i set it up to\nonly use three mappers uh so there's\nzero one and two and we can go ahead and\ndo a cat on there remember this is\ndepartment two so we want to look at all\nthe contents of these three different\nfiles and there it is it's greater than\nfour so we have golf is five outdoor six\nuh fan shop is seven so we've\neffectively filtered out our data and\njust storing the data we want on our\nfile system so if you're going to store\ndata on here the next stage is to export\nthe data remember a lot of times you\nhave my sql server and we're continually\ndumping that data into our long term\nstorage and access a hadoop file system\nbut what happens when you need to pull\nthat data out and\nrestore a database or\nmaybe you have\nyou just merged with a new company a\nfavorite topic merging companies and\nmerging databases that's listed under\nnightmare and how many different names\nfor company can you have so you can see\nwhere being able to export is also\nequally important and let's go ahead and\ndo that and i'm going to flip back over\nto my sql server here and we'll need to\ngo ahead and create our database we're\ngoing to export into now i'm not going\nto go too much in detail on this command\nwe're simply creating a table and the\ntable is going to have\nit's pretty much the same table we\nalready have in here from departments\nbut in this case we're going to create a\ntable called dept\nso it's the same setup but it's it's\njust going to we're just giving a\ndifferent name a different schema and so\nwe've done that and we'll go ahead and\ndo a select star from e p t\nthere we go and it's empty that's what\nwe expect a new database a new data\ntable and it's empty in there uh so now\nwe need to go ahead and export our data\nthat we just filtered out into there so\nlet's flip back on over here to our\nscoop setup which is just our linux\nterminal window and let's go back up to\none of our commands here's scoop import\nin this case instead of import we're\ngoing to take the scoop and we're going\nto export so we're going to just change\nthat export and the connection is going\nto remain the same so same connect same\ndatabase we're also we're still doing\nthe retail db uh we have the same\npassword so none of that changes uh the\nbig change here is going to be the table\ninstead of departments uh remember we\nchanged it and gave it a new name and so\nwe want to change it here also d-e-p-t\nso department we're not going to worry\nabout the mapper count and the where was\npart of our import there we go and then\nfinally it needs to know where to export\nfrom so instead of target directory we\nhave an export directory that's where\nit's coming from uh still user cloudera\nand we'll keep it as department 2. just\nso you can see how that data is coming\nback with that we filtered in and let's\ngo ahead and run this it'll take it just\na moment to go through with steps and\nagain because it's low i'm just going to\ngo and skip this so you don't have to\nsit through it and once we've wrapped up\nour export we'll flip back on over here\nto mysql use the up arrow and this time\nwe're going to select star from\ndepartment and we can see that there it\nis it exported the golf outdoors and fan\nshop and you can imagine also that you\nmight have to use the where command in\nyour export also so there's a lot of\nmixing the command line for scoop is\npretty straightforward you're changing\nthe different variables in there whether\nyou're creating a table listing a table\nlisting databases very powerful tool for\nbringing your data into the hadoop file\nsystem and exporting it so now that\nwe've wrapped up our demo on scoop and\ngone through a lot of basic commands\nlet's dive in with a brief history of\nhive so the history of hive begins with\nfacebook facebook began using hadoop as\na solution to handle the growing big\ndata and we're not talking about a data\nthat fits on one or two or even five\ncomputers we're talking due to the fits\non if you've looked at any of our other\nhadoop tutorials you'll know we're\ntalking about very big data and data\npools and facebook certainly has a lot\nof data it tracks as we know the hadoop\nuses mapreduce for processing data\nmapreduce required users to write long\ncodes and so you'd have these really\nextensive java codes very complicated\nfor the average person to use not all\nusers were versed in java and other\ncoding languages this proved to be a\ndisadvantage for them users were\ncomfortable with writing queries in sql\nsql has been around for a long time the\nstandard sql query language hive was\ndeveloped with the vision to incorporate\nthe concepts of tables columns just like\nsql so why hive well the problem was for\nprocessing and analyzing data users\nfound it difficult to code as not all of\nthem were well-versed with the coding\nlanguages you have your processing ever\nanalyzing and so the solution was\nrequired a language similar to sql which\nwas well known to all the users and thus\nthe hive or hql language evolved what is\nhive hive is a data warehouse system\nwhich is used for querying and analyzing\nlarge data sets stored in the hdfs or\nthe hadoop file system hive uses a query\nlanguage that we call hive ql or hql\nwhich is similar to sql so if we take\nour user the user sends out their hive\nqueries and then that is converted into\na map reduce tasks and then accesses the\nhadoop mapreduce system let's take a\nlook at the architecture of hive\narchitecture of hive we have the hive\nclient\nso that could be the programmer or maybe\nit's a manager who knows enough sql to\ndo a basic query to look up the data\nthey need the hive client supports\ndifferent types of client applications\nin different languages prefer for\nperforming queries and so we have our\nthrift application in the hive thrift\nclient thrift is a software framework\nhive server is based on thrift so it can\nserve the request from all programming\nlanguage that support thrift and then we\nhave our jdbc application and the hive\njdbc driver jdbc java database\nconnectivity jdbc application is\nconnected through the jdbc driver and\nthen you have the odbc application or\nthe hive odbc driver the odbc or open\ndatabase connectivity the odbc\napplication is connected through the\nodbc driver with the growing development\nof all of our different scripting\nlanguages python c plus plus spar\njava you can find just about any\nconnection in any of the main scripting\nlanguages and so we have our hive\nservices as we look at deeper into the\narchitecture hive supports various\nservices\nso you have your hive server basically\nyour thrift application or your hive\nthrift client or your jdbc or your hive\njdbc driver your odbc application or\nyour hive odbc driver they all connect\ninto the hive server and you have your\nhive web interface you also have your\ncli now the hive web interface is a gui\nis provided to execute hive queries and\nwe'll actually be using that later on\ntoday so you can see kind of what that\nlooks like and get a feel for what that\nmeans commands are executed directly in\ncli and then the cli is a direct\nterminal window and i'll also show you\nthat too so you can see how those two\ndifferent interfaces work these then\npush the code into the hive driver hive\ndriver is responsible for all the\nqueries submitted so everything goes\nthrough that driver let's take a closer\nlook at the hive driver the hive driver\nnow performs three steps internally one\nis a compiler hive driver passes query\nto compiler where it is checked and\nanalyzed then the optimizer kicks in and\nthe optimize logical plan in the form of\na graph of mapreduce and hdfs tasks is\nobtained and then finally in the\nexecutor in the final step the tasks are\nexecuted we look at the architecture we\nalso have to note the meta store\nmetastore is a repository for hive\nmetadata stores metadata for hive tables\nand you can think of this as your schema\nand where is it located and it's stored\non the apache derby db processing and\nresource management is all handled by\nthe mapreduce v1 you'll see mapreduce v2\nthe yarn and the tez these are all\ndifferent ways of managing these\nresources depending on what version of\nhadoop you're in hive uses mapreduce\nframework to process queries and then we\nhave our distributed storage which is\nthe hdfs and if you looked at our hadoop\ntutorials you'll know that these are on\ncommodity machines and are linearly\nscalable that means they're very\naffordable a lot of time when you're\ntalking about big data you're talking\nabout a tenth of the price of storing it\non enterprise computers and then we look\nat the data flow and hive\nso in our data flow and hive we have our\nhive in the hadoop system and underneath\nthe user interface or the ui we have our\ndriver our compiler our execution engine\nand our meta store that all goes into\nthe mapreduce and the hadoop file system\nso when we execute a query you see it\ncoming in here goes into the driver step\none step two we get a plan what are we\ngoing to do refers to the query\nexecution uh then we go to the metadata\nit's like well what kind of metadata are\nwe actually looking at where is this\ndata located what is the schema on it\nthen this comes back with the metadata\ninto the compiler then the compiler\ntakes all that information and the syn\nplan returns it to the driver the driver\nthen sends the execute plan to the\nexecution engine once it's in the\nexecution engine the execution engine\nacts as a bridge between hive and hadoop\nto process the query and that's going\ninto your mapreduce in your hadoop file\nsystem or your hdfs and then we come\nback with the metadata operations it\ngoes back into the metastore to update\nor let it know what's going on which\nalso goes to the between it's a\ncommunication between the execution\nengine and the meta store execution\nengine communications is\nbi-directionally with the metastore to\nperform operations like create drop\ntables metastore stores information\nabout tables and columns so again we're\ntalking about the schema of your\ndatabase and once we have that we have a\nbi-directional\nsend results communication back into the\ndriver and then we have the fetch\nresults which goes back to the client so\nlet's take a little bit look at the hive\ndata modeling hive data modeling so you\nhave your high data modeling you have\nyour tables you have your partitions and\nyou have buckets the tables in hive are\ncreated the same way it is done in rdbms\nso when you're looking at your\ntraditional sql server or mysql server\nwhere you might have enterprise\nequipment and a lot of\npeople pulling and removing stuff off of\nthere the tables are going to look very\nsimilar and this makes it very easy to\ntake that information and let's say you\nneed to keep current information but you\nneed to store all of your years of\ntransactions back into the hadoop hive\nso you match those those all kind of\nlook the same the tables are the same\nyour databases look very similar and you\ncan easily import them back you can\neasily store them into the hive system\npartitions here tables are organized\ninto partitions for grouping same type\nof data based on partition key this can\nbecome very important for speeding up\nthe process of doing queries so if\nyou're looking at dates as far as like\nyour employment dates of employees if\nthat's what you're tracking you might\nadd a partition there because that might\nbe one of the key things that you're\nalways looking up as far as employees\nare concerned and finally we have\nbuckets uh data present in partitions\ncan be further divided into buckets for\nefficient querying again there's that\nefficiency at this level a lot of times\nyou're taught you're working with the\nprogrammer and the admin of your hadoop\nfile system to maximize the efficiency\nof that file system so it's usually a\ntwo-person job but we're talking about\nhive data modeling you want to make sure\nthat they work together and you're\nmaximizing your resources hive data\ntypes so we're talking about hive data\ntypes we have our primitive data types\nand our complex data types a lot of this\nwill look familiar because it mirrors a\nlot of stuff in sql in our primitive\ndata types we have the numerical data\ntypes string data type date time data\ntype and\nmiscellaneous data type and these should\nbe very they're kind of self-explanatory\nbut just in case numerical data is your\nfloats your integers your short integers\nall of that numerical data comes in as a\nnumber a string of course is characters\nand numbers and then you have your date\ntime stamp and then we have kind of a\ngeneral way of pulling your own created\ndata types in there that's our\nmiscellaneous data type and we have\ncomplex data types so you can store\narrays you can store maps you can store\nstructures\nand even units in there as we dig into\nhive data types\nand we have the primitive data types and\nthe complex data types so we look at\nprimitive data types and we're looking\nat numeric data types data types like an\ninteger a float a decimal those are all\nstored as numbers in the hive data\nsystem a string data type data types\nlike characters and strings you store\nthe name of the person you're working\nwith uh you know john doe the city\nmemphis the state tennessee maybe it's\nboulder colorado usa or maybe it's hyper\nbad\nindia that's all going to be string and\nstored as a string character and of\ncourse we have our date time data type\ndata types like timestamp date interval\nthose are very common as far as tracking\nsales anything like that you just think\nif you can type a stamp of time on it or\nmaybe you're dealing with the race and\nyou want to know the interval how long\ndid the person take to complete whatever\ntask it was all that is date time data\ntype and then we talk miscellaneous data\ntype these are like boolean and binary\nand when you get into boolean binary you\ncan actually almost create anything in\nthere but your yes knows zero one now\nlet's take a look at complex data types\na little closer uh we have arrays so\nyour syntax is of data type and it's an\narray and you can just think of an array\nas a collection of same\nentities one two three four if they're\nall numbers and you have maps this is a\ncollection of key value pairs\nso understanding maps is so central to\nhadoop\nso we store maps you have a key which is\na set you only have one key per mapped\nvalue and so you in hadoop of course you\ncollect uh the same keys and you can add\nthem all up or do something with all the\ncontents of the same key but this is our\nmap as a primitive type data type in our\ncollection of key value pairs and then\ncollection of complex data with comment\nso we can have a structure we have a\ncolumn name data type comment call a\ncolumn comment so you can get very\ncomplicated structures in here with your\ncollection of data and your commented\nsetup and then we have units and this is\na collection of heterogeneous data types\nso the syntax for this is union type\ndata type data type and so on so it's\nall going to be the same a little bit\ndifferent than the arrays where you can\nactually mix and match different modes\nof hive hive operates in two modes\ndepending on the number and size of data\nnodes we have our local mode and our map\nreduce mode we talk about the local mode\nit is used when hadoop is having one\ndata node and the data is small\nprocessing will be very fast on a\nsmaller data sets which are present in\nlocal machine and this might be that you\nhave a local file stuff you're uploading\ninto the hive and you need to do some\nprocesses in there you can go ahead and\nrun those high processes and queries on\nit usually you don't see much in the way\nof a single node hadoop system if you're\ngoing to do that you might as well just\nuse like an sql database or even a java\nsqlite or something python and sqlite so\nyou don't really see a lot of single\nnode hadoop databases but you do see the\nlocal mode in hive where you're working\nwith a small amount of data that's going\nto be integrated into the larger\ndatabase and then we have the map reduce\nmode this is used when hadoop is having\nmultiple data nodes and the data is\nspread across various data nodes\nprocessing large data sets can be more\nefficient using this mode and this you\ncan think of instead of it being one two\nthree or even five computers we're\nusually talking with the hadoop file\nsystem we're looking at 10 computers 15\n100 where this data is spread across all\nthose different hadoop nodes difference\nbetween hive and\nrdbms remember rdbms stands for the\nrelational database management system\nlet's take a look at the difference\nbetween hive and the rdbms with hive\nhive enforces schema on read and it's\nvery important that whatever is coming\nin that's when hive's looking at it and\nmaking sure that it fits the model\nthe rdbms enforces a schema when it\nactually writes the data into the\ndatabase so it's read the data and then\nonce it starts to write it that's where\nit's going to give you the error tell\nyou something's incorrect about your\nscheme hive data size is in petabytes\nthat is hard to imagine um you know\nwe're looking at your personal computer\non your desk maybe you have 10 terabytes\nif it's a high-end computer but we're\ntalking petabytes so that's hundreds of\ncomputers grouped together when a rdbms\ndata size is in terabytes very rarely do\nyou see an rdbms system that's spread\nover more than five computers and\nthere's a lot of reasons for that with\nthe rdbms it actually has a high end\namount of writes to the hard drive\nthere's a lot more going on there you're\nwriting and pulling stuff so you really\ndon't want to get too big with an rd bms\nor you're going to run into a lot of\nproblems with hive you can take it as\nbig as you want hive is based on the\nnotion of write once and read many times\nthis is so important and they call it\nworm which is write w wants o read are\nmany times m they refer to it as worm\nand that's true of any of a lot of your\nhadoop setup it's it's altered a little\nbit but in general we're looking at\narchiving data that you want to do data\nanalysis on we're looking at pulling all\nthat stuff off your rd bms from years\nand years and years of business or\nwhatever your company does or scientific\nresearch and putting that into a huge\ndata pool so that you can now do queries\non it and get that information out of it\nwith the rdbms it's based on the notion\nof read and write many times so you're\ncontinually updating this database\nyou're continually bringing up new stuff\nnew sales\nthe account changes because they have a\ndifferent licensing now whatever\nsoftware you're selling all that kind of\nstuff where the data is continually\nfluctuating and then hive resembles a\ntraditional database by supporting sql\nbut it is not a database it is a data\nwarehouse this is very important it goes\nwith all the other stuff we've talked\nabout that we're not looking at a\ndatabase but a data warehouse to store\nthe data and still have fast and easy\naccess to it for doing queries you can\nthink of\ntwitter and facebook they have so many\nposts that are archived back\nhistorically those posts aren't going to\nchange they made the post they're posted\nthey're there and they're in their\ndatabase but they have to store it in a\nwarehouse in case they want to pull it\nback up with the rdbms it's a type of\ndatabase management system which is\nbased on the relational model of data\nand then with hive easily scalable at a\nlow cost again we're talking maybe a\nthousand dollars per terabyte um the\nrdbms is not scalable at a low cost when\nyou first start on the lower end you're\ntalking about 10 000 per terabyte of\ndata including all the backup on the\nmodels and all the added necessities to\nsupport it as you scale it up you have\nto scale those computers and hardware up\nso you might start off with a basic\nserver and then you upgrade to a sun\ncomputer to run it and you spend you\nknow tens of thousands of dollars for\nthat hardware upgrade with hive you just\nput another computer into your hadoop\nfile system so let's look at some of the\nfeatures of hive\nwhen we're looking at the features of\nhive we're talking about the use of sql\nlike language called hive ql a lot of\ntimes you'll see that as hql which is\neasier than long codes this is nice if\nyou're working with your shareholders\nyou come to them and you say hey you can\ndo a basic sql query on here and pull up\nthe information you need this way you\ndon't have to take off have your\nprogrammers jump in every time they want\nto look up something in the database\nthey actually now can easily do that if\nthey're not\nskilled in programming and script\nwriting tables are used which are\nsimilar to the rdbms hence easier to\nunderstand and one of the things i like\nabout this is when i'm bringing tables\nin from a mysql server or sql server\nthere's almost a direct reflection\nbetween the two so when you're looking\nat one which is the data which is\ncontinually changing and then you're\ngoing into the archive database it's not\nthis huge jump where you have to learn a\nwhole new language\nyou mirror that same schema into the\nhdfs into the hive making it very easy\nto go between the two and then using\nhive ql multiple users can\nsimultaneously query data so again you\nhave multiple clients in there and they\nsend in their query that's also true\nwith the rdbms which kind of queues them\nup because it's running so fast you\ndon't notice the lag time well you get\nthat also with the hql as you add more\ncomputers and the query can go very\nquickly depending on how many computers\nand how much resources each machine has\nto pull the information and hive\nsupports a variety of data types\nso with hive it's designed to be on the\nhadoop system which you can put almost\nanything into the hadoop file system so\nwith all that let's take a look at a\ndemo on hive ql or hql before i dive\ninto the hands-on demo let's take a look\nat the website hive.apache.org\nthat's the main website since apache\nit's an apache open source\nsoftware this is the main software for\nthe main site for the build and if you\ngo in here you'll see that they're\nslowly migrating hive into beehive and\nso if you see beehive versus hive note\nthe beehive as the new release is coming\nout that's all it is it reflects a lot\nof the same functionality of hive it's\nthe same thing and then we like to pull\nup some kind of documentation on\ncommands and for this i'm actually going\nto go to hortonworks hive cheat sheet\nand that's because hortonworks and\ncloudera are two of the most common used\nbuilds for hadoop and four which include\nhive and all the different tools in\nthere and so hortonworks has a pretty\ngood pdf you can download cheat sheet on\nthere i believe cloudera does too but\nwe'll go ahead and just look at the\nhorton one because it's the one that\ncomes up really good and you can see\nwhen we look at the query language it\ncompares my sql server to hive ql or hql\nand you can see the basic select we\nselect from columns from table where\nconditions exist the most basic command\non there and they have different things\nyou can do with it just like you do with\nyour sql and if you scroll down you'll\nsee\ndata types so here's your integer your\nflow your binary double string timestamp\nand all the different data types you can\nuse some different semantics different\nkeys features functions uh for running a\nhive query command line setup and of\ncourse a hive shell uh set up in here so\nyou can see right here if we loop\nthrough it has a lot of your basic stuff\nand it is we're basically looking at sql\nacross a horton database we're going to\ngo ahead and run our hadoop cluster hive\ndemo and i'm going to go ahead and use\nthe cloudera quick start this is in the\nvirtual box so again we have an oracle\nvirtual box which is open source and\nthen we have our cloudera quick start\nwhich is the hadoop setup on a single\nnode now obviously hadoop and hive are\ndesigned to run across a cluster of\ncomputers so we talk about a single node\nis for education testing that kind of\nthing and if you have a chance you can\nalways go back and look at our demo we\nhad on\nsetting up a hadoop system in a single\ncluster just set a note down below in\nthe youtube video and our team will get\nin contact with you and send you that\nlink if you don't already have it or you\ncan contact us at the\nwww.simplylearn.com now in here it's\nalways important to note that you do\nneed\non your computer if you're running on\nwindows because i'm on a windows machine\nyou're going to need probably about 12\ngigabytes to actually run this it used\nto be goodbye with a lot less but as\nthings have evolved they take up more\nand more resources and you need the\nprofessional version if you have the\nhome version i was able to get that to\nrun but boy did it take a lot of extra\nwork to get the home version to let me\nuse the virtual\nsetup on there and we'll simply click on\nthe cloudera quick start and i'm going\nto go and just start that up and this is\nstarting up our linux so we have our\nwindows 10 which is a computer i'm on\nand then i have the virtual box which is\ngoing to have a linux operating system\nin it and we'll skip ahead so you don't\nhave to watch the whole install\nsomething interesting to know about the\ncloudera is that it's running on\nlinuxcentos and for whatever reason i've\nalways had to click on it and hit the\nescape button for it to spin up and then\nyou'll see the dos come in here now that\nour cloudera spun up on our virtual\nmachine with the linux on uh we can see\nhere we have our it uses the thunderbird\nbrowser on here by default and\nautomatically opens up a number of\ndifferent tabs for us and a quick note\nbecause i mentioned like the\nrestrictions on getting set up on your\nown computer if you have a home edition\ncomputer and you're worried about\nsetting it up on there you can also go\nin there and spin up a one month free\nservice on amazon web service to play\nwith this so there's other options\nyou're not stuck with just doing it on\nthe quick start menu you can spin this\nup in many other ways now the first\nthing we want to note is that we've come\nin here into cloudera and i'm going to\naccess this in two ways\nthe first one is we're going to use hue\nand i'm going to open up hue and i'll\ntake it a moment to load from the setup\non here and hue is nice if i go in and\nuse hue as an editor into hive or into\nthe hadoop setup usually i'm doing it as\na\nfrom an admin side because it has a lot\nmore information a lot of visuals less\nto do with you know actually diving in\nthere and just executing code and you\ncan also write this code into files and\nscripts and there's other things you can\notherwise you can upload it into hive\nbut today we're going to look at the\ncommand lines and we'll upload it into\nhue and then we'll go into and actually\ndo our work in a terminal window under\nthe hive shell now in the hue browser\nwindow if you go under query and click\non the pull down menu and then you go\nunder editor and you'll see hive there\nwe go there's our hive setup i go and\nclick on hive and this will open up our\nquery down here and now it has a nice\nlittle b that shows our hive going and\nwe can go something very simple down\nhere like show\ndatabases and we follow it with the\nsemicolon and that's the standard in\nhive is you always add our\npunctuation at the end there and i'll go\nahead and run this and the query will\nshow up underneath and you'll see down\nhere since this is a new quick start i\njust put on here you'll see it has the\ndefault down here for the databases\nthat's the database name i haven't\nactually created any databases on here\nand then there's a lot of other like\nassistant function tables\nyour databases up here there's all kinds\nof things you can research you can look\nat through hue as far as a bigger\npicture the downside of this is it\nalways seems to lag for me whenever i'm\ndoing this i always seem to run slow so\nif you're in cloudera you can open up a\nterminal window they actually have an\nicon at the top you can also go under\napplications and under applications\nsystem tools and terminal either one\nwill work it's just a regular terminal\nwindow and this terminal window is now\nrunning underneath our linux so this is\na linux terminal window or on our\nvirtual machine which is resting on our\nregular windows 10 machine and we'll go\nahead and zoom this in so you can see\nthe text better on your own video i\nsimply just clicked on view and zoom in\nand then all we have to do is type in\nhive and this will open up the shell on\nhere and it takes it just a moment to\nload when starting up hive i also want\nto note that depending on your rights on\nthe computer you're on in your action\nyou might have to do pseudohyme and put\nin your password and username most\ncomputers are usually set up with the\nhive login again it just depends on how\nyou're accessing the linux system and\nthe hive shell once we're in here we can\ngo ahead and do a simple uh hql command\nshow databases and if we do that we'll\nsee here that we don't have any\ndatabases so we can go ahead and create\na database and we'll just call it office\nfor today for this moment now if i do\nshow we'll just do the up arrow up arrow\nis a hotkey that works in both linux and\nin hive so i can go back and paste\nthrough all the commands i've typed in\nand we can see now that i have my\nthere's of course a default database and\nthen there's the office database so now\nwe've created a database it's pretty\nquick and easy and we can go ahead and\ndrop the database we can do drop\ndatabase\noffice now this will work on this\ndatabase because it's empty if your\ndatabase was not empty you would have to\ndo cascade\nand that drops all the tables in the\ndatabase and the database itself now if\nwe do show database and we'll go ahead\nand recreate our database because we're\ngoing to use the office database for the\nrest of this hands-on demo a really\nhandy command now\nset with the sql or hql is to use office\nand what that does is that sets office\nas a default database so instead of\nhaving to reference the database every\ntime we work with a table it now\nautomatically assumes that's the\ndatabase being used whatever tables\nwe're working on the difference is you\nput the database name period table and\ni'll show you in just a minute what that\nlooks like and how that's different if\nwe're going to have a table and a\ndatabase we should probably load some\ndata into it so let me go ahead and\nswitch gears here and open up a terminal\nwindow you can just open another\nterminal window and it'll open up right\non top of the one that you have hive\nshell running in and when we're in this\nterminal window first we're going to go\nahead and just do a list which is of\ncourse a linux command you can see all\nthe files i have in here this is the\ndefault load we can change directory to\ndocuments we can list in documents and\nwe're actually going to be looking at\nemployee.csv a linux command is the cat\nyou can use this actually to combine\ndocuments there's all kinds of things\nthat cat does but if we want to just\ndisplay the contents of our employee.csv\nfile we can simply do cat employee csv\nand when we're looking at this we want\nto know a couple things one there's a\nline at the top okay so the very first\nthing we notice is that we have a header\nline the next thing we notice is that\nthe data is comma separated and in this\nparticular case you'll see a space here\ngenerally with these you've got to be\nreal careful with spaces there's all\nkinds of things you got to watch out for\nbecause it can cause issues these spaces\nwon't because these are all strings that\nthe space is connected to if this was a\nspace next to the integer you would get\na null value that comes into the\ndatabase without doing something extra\nin there now with most of hadoop that's\nimportant to know that you're writing\nthe data once reading it many times and\nthat's true of almost all your hadoop\nthings coming in so you really want to\nprocess the data before it gets into the\ndatabase and for those who of you have\nstudied uh data transformation that's\nthe etyl where you extract transfer form\nand then load the data so you really\nwant to extract and transform before\nputting it into the hive then you load\nit into the hive with the transform data\nand of course we also want to note the\nschema we have an integer string string\ninteger integer so we kept it pretty\nsimple in here as far as the way the\ndata is set up the last thing that\nyou're going to want to look up\nis the source since we're doing local\nuploads we want to know what the path is\nwe have the whole path in this case it's\nhome slash cloudera slash documents and\nthese are just text documents we're\nworking with right now we're not doing\nanything fancy so we can do a simple get\nedit employee.csv\nand you'll see it comes up here it's\njust a text document so i can easily\nremove these added spaces there we go\nand then we go and just save it and so\nnow it has a new setup in there we've\nedited it the g edit is usually one of\nthe default that loads into linux so any\ntext editor will do back to the hive\nshell so let's go ahead and create a\ntable employee and what i want you to\nnote here is i did not put the semicolon\non the end here semicolon tells it to\nexecute that line so this is kind of\nnice if you're you can actually just\npaste it in if you have it written on\nanother sheet and you can see right here\nwhere i have create table employee and\nit goes into the next line on there so i\ncan do all of my commands at once now\njust so i don't have any typo errors i\nwent ahead and just pasted the next\nthree lines in and the next one is our\nschema if you remember correctly from\nthe other side we had uh the different\nvalues in here which was id name\ndepartment year of joining and salary\nand the id is an integer name is a\nstring department string air joining\nenergy salary an integer and they're in\nbrackets we put close brackets around\nthem and you could do this all as one\nline and then we have row format\ndelimited fields terminated by comma and\nthis is important because the default is\ntabs so if i do it now it won't find any\nterminated fields so you'll get a bunch\nof null values loaded into your table\nand then finally our table properties we\nwant to skip the header line count\nequals one now this is a lot of work for\nuploading a single file it's kind of\ngoofy when you're uploading a single\nfile that you have to put all this in\nhere but keep in mind hive and hadoop is\ndesigned for writing many files into the\ndatabase you write them all in there and\nthen you can they're saved it's an\narchive it's a data warehouse and then\nyou're able to do all your queries on\nthem so a lot of times we're not looking\nat just the one file coming up we're\nloading hundreds of files you have your\nreports coming off of your main database\nall those reports are being loaded and\nyou have your log files you have i mean\nall this different data is being dumped\ninto hadoop and in this case hive on top\nof hadoop and so we need to let it know\nhey how do i handle these files coming\nin and then we have the semicolon at the\nend which lets us know to go ahead and\nrun this line and so we'll go ahead and\nrun that and now if we do a show tables\nyou can see there's our employee on\nthere we can also describe if we do\ndescribe employee\nyou can see that we have our id integer\nname string department string year of\njoining integer and salary integer and\nthen finally let's just do a select star\nfrom employee very basic sql nhql\ncommand selecting data it's going to\ncome up and we haven't put anything in\nit so as we expect there's no data in it\nso if we flip back to our\nlinux terminal window you can see where\nwe did the cat\nemployee.csv and you can see all the\ndata we expect to come into it and we\nalso did our pwd and right here you see\nthe path you need that full path when\nyou are loading data you know you can do\na browse and if i did it right now with\njust the employee.csv as a name it will\nwork but that is a really bad habit in\ngeneral when you're loading data because\nit's you don't know what else is going\non in the computer you want to do the\nfull path almost in all your data loads\nso let's go ahead and flip back over\nhere to our hive shell we're working in\nand the command for this is load data so\nthat says hey we're loading data that's\na hive command hql and we want local\ndata so you got to put down local in\npath so now it needs to know where the\npath is now to make this more legible\ni'm just going to go ahead and hit enter\nthen we'll just paste the full path in\nthere which i have stored over on the\nside like a good prepared demo and\nyou'll see here we have home cloudera\ndocuments employee.csv so it's a whole\npath for this text document in here and\nwe go ahead and hit enter in there and\nthen we have to let it know where the\ndata is going so now we have a source\nand we need a destination and it's going\nto go into the table and we'll just call\nit employee we'll just match the table\nin there and because i want it to\nexecute we put the semicolon on the end\nit goes ahead and executes all three\nlines now if we go back if you remember\nwe did the select star from employee\njust using the up error to page through\nmy different commands i've already typed\nin you can see right here we have as we\nexpect we have rows sam mike and nick\nand we have all their information\nshowing in our four rows and then let's\ngo ahead and do uh select\nand count let's look at a couple of\nthese different select options you can\ndo we're going to count everything from\nemployee now this is kind of interesting\nbecause the first one just pops up with\nthe basic select because it doesn't need\nto go through the full map reduce phase\nbut when you start doing a count it does\ngo through the full map redo setup in\nthe hive in hadoop and because i'm doing\nthis demo on a single node cloudera\nvirtual box on top of a windows 10 all\nthe benefits of running it on a cluster\nare gone and instead it's now going\nthrough all those added layers so it\ntakes longer to run you know like i said\nwhen you do a single node as i said\nearlier it doesn't do any good as an\nactual distribution because you're only\nrunning it on one computer and then\nyou've added all these different layers\nto run it and we see it comes up with\nfour and that's what we expect we have\nfour rows we expect four at the end and\nif you remember from\nour cheat sheet which we brought up here\nfrom hortons it's a pretty good one\nthere's all these different commands we\ncan do we'll look at one more command\nwhere we do the uh what they call sub\nqueries right down here because that's\nreally common to do a lot of sub queries\nand so we'll do select\nstar or all different columns from\nemployee now if we weren't using the\noffice database it would look like this\nfrom office dot employee and either one\nwill work on this particular one because\nwe have office set as a default on there\nso from office employee and then the\ncommand where creates a subset and in\nthis case we want to know where the\nsalary is greater than 25\n000. there we go and of course we end\nwith our semicolon and if we run this\nquery you can see it pops up and there's\nour salaries of people top earners we\nhave rose and i t and mike and hr kudos\nto them of course they're fictional i\ndon't actually we don't actually have a\nrose and a mic in those positions or\nmaybe we do so finally we want to go\nahead and do is we're done with this\ntable now remember you're dealing with a\ndata warehouse so you usually don't do a\nlot of dropping of tables and\ndatabases but we're going to go ahead\nand drop this table here before we drop\nit one more quick note is we can change\nit so what we're going to do is we're\ngoing to alter table office employee and\nwe want to go ahead and rename it\nthere's some other commands you can do\nin here but rename is pretty common and\nwe're going to rename it to\nand it's going to stay in office and\nit turns out one of our\nshareholders really doesn't like the\nword employee he wants employees plural\nit's a big deal to him so let's go ahead\nand change that name for the table it's\nthat easy because it's just changing the\nmetadata on there and now if we do show\ntables you'll see we now have employees\nnot employee and then at this point\nmaybe we're doing some house cleaning\nbecause this is all practice so we're\ngoing to go ahead and drop table and\nwe'll drop table employees because we\nchanged the name in there so if we did\nemployee just give us an error and now\nif we do show tables you'll see all the\ntables are gone now the next thing we\nwant to go and take a look at and we're\ngoing to walk back through the loading\nof data just real quick because we're\ngoing to load two tables in here and let\nme just float back to our terminal\nwindow so we can see what those tables\nare that we're loading and so up here we\nhave customer we have a customer\nfile and we have an order file we want\nto go ahead and put the customers and\nthe orders into here so those are the\ntwo we're doing and of course it's\nalways nice to see what you're working\nwith\nso let's do our cat\ncustomer.csv we could always do g edit\nbut we don't really need to edit these\nwe just want to take a look at the data\nin customer and important in here is\nagain we have a header so we have to\nskip a line comma separated\nnothing odd with the data we have our\nschema which is\ninteger string integer string integer so\nyou'd want to take that note that down\nor flip back and forth when you're doing\nit and then let's go ahead and do cat\norder.csv and we can see we have oid\nwhich i'm guessing is the order id we\nhave a date up something new we've done\nintegers and strings but we haven't done\ndate when you're importing new and you\nnever worked with the date date's always\none of the more trickier fields to port\nin when that's true of just about any\nscripting language i've worked with all\nof them have their own idea of how\ndate's supposed to be formatted what the\ndefault is this particular format or its\nyear and it has all four\ndigits dash month two digits dash day is\nthe standard import for the hive so\nyou'll have to look up and see what the\ndifferent formats are if you're going to\ndo a different format in there coming in\nor you're not able to pre-process the\ndata but this would be a pre-processing\nof the data thing coming in if you\nremember correctly from our edel which\nis uh e just in case you weren't able to\nhear me last time etl which stands for\nextract transform then load so you want\nto make sure you're transforming this\ndata before it gets into here and so\nwe're going to go ahead and bring\nboth this data in here and really we're\ndoing this so we can show you the basic\njoin there is if you remember from our\nsetup merge join all kinds of different\nthings you can do but joining different\ndata sets is so common so it's really\nimportant to know how to do this we need\nto go ahead and bring in these two data\nsets and you can see where i just\ncreated a table customer here's our\nschema the integer name age address\nsalary here's our eliminated by commas\nand our table properties where we skip a\nline well let's go ahead and load the\ndata first and then we'll do that with\nour order and let's go ahead and put\nthat in here and i've got it split into\nthree lines so you can see it easily\nwe've got load data local in path so we\nknow we're loading data we know it's\nlocal and we have the path here's the\ncomplete path for\noops this is supposed to be order csv\ngrab the wrong one of course it's going\nto give me errors because you can't\nrecreate the same table on there and\nhere we go create table here's our\ninteger date customer the basic setup\nthat we had coming in here for our\nschema row format commas table\nproperties skip header line and then\nfinally let's load the data into\nour order table load data local in path\nhome cloudera documents order.csv into\ntable order now if we did everything\nright we should be able to do select\nstar from customer and you can see we\nhave all seven customers and then we can\ndo select star from order and we have uh\nfour orders uh so this is just like a\nquick frame we have a lot of times when\nyou have your customer databases in\nbusiness you have thousands of customers\nfrom years and years and some of them\nyou know they move they close their\nbusiness they change names all kinds of\nthings happen uh so we want to do is we\nwant to go ahead and find just the\ninformation connected to these orders\nand who's connected to them and so let's\ngo ahead and do it's a select because\nwe're going to display information so\nselect and this is kind of interesting\nwe're going to do c dot id\nand i'm going to define c as customer as\na customer table in just a minute then\nwe're going to do c dot name and again\nwe're going to define the c c dot age so\nthis means from the customer we want to\nknow their id their name their age and\nthen you know i'd also like to know the\norder amount uh so let's do o for dot\namount and then this is where we need to\ngo ahead and define uh what we're doing\nand i'm going to capitalize from\ncustomer so we're going to take the\ncustomer table in here and we're going\nto name it c that's where the c comes\nfrom so that's the customer table c and\nwe want to join order as o that's where\nour o comes from so the o dot amount is\nwhat we're joining in there and then we\nwant to do this on we got to tell it how\nto connect the two tables c dot id\nequals o dot customer underscore id so\nnow we know how they're joined and\nremember we have seven customers in here\nwe have four orders and as a processes\nwe should get a return of four different\nnames joined together and they're joined\nbased on of course the orders on there\nand once we're done we now have the\norder number the person who made the\norder their age and the amount of the\norder which came from the order table uh\nso you have your different information\nand you can see how the join works here\nvery common use of tables and hql and\nsql and let's do one more thing with our\ndatabase and then i'll show you a couple\nother hive commands and let's go ahead\nand do a drop and we're going to drop\ndatabase office\nand if you're looking at this and you\nremember from earlier this will give me\nan error and let's just see what that\nlooks like it says fill to execute\nexception one or more tables exist so if\nyou remember from before you can't just\ndrop a database unless you tell it to\ncascade that lets it know i don't care\nhow many tables are in it let's get rid\nof it and in hadoop since it's an art\nit's a warehouse a data warehouse you\nusually don't do a lot of dropping maybe\nat the beginning when you're developing\nthe schemas and you realize you messed\nup you might drop some stuff but down\nthe road you're really just adding\ncommodity machines to pick up so you can\nstore more stuff on it so you usually\ndon't do a lot of database dropping and\nsome other fun commands to know is you\ncan do select round 2.3 is round value\nyou can do a round off in\nhive we can do as floor value which is\ngoing to give us a 2 so it turns it into\nan integer versus a float it goes down\nyou know basically truncates it but it\ngoes down and we can also do ceiling\nwhich is going to round it up so we're\nlooking for the next integer above\nthere's a few commands we didn't show in\nhere because we're on a single node as\nas an admin to help spediate the process\nyou usually add in partitions for the\ndata and buckets you can't do that on a\nsingle node because the when you add a\npartition it partitions it across\nseparate nodes but beyond that you can\nsee that it's very straightforward we\nhave sql coming in and all your basic\nqueries that are in sql are very similar\nto hql let's get started with pig why\npig what is pig mapreduce versus hive\nversus pig hopefully you've had a chance\nto do our hive tutorial in our mapreduce\ntutorial if you haven't send a note over\nto simplylearn and we'll follow up with\na link to you we'll look at pig\narchitecture working a pig pig latin\ndata model pig execution modes a use\ncase twitter and features a pig and then\nwe'll tag on a short demo so you can see\npig in action so why pig as we all know\nhadoop uses mapreduce to analyze and\nprocess big data processing big data\nconsumed more time so before we had the\nhadoop system they'd have to spend a lot\nof money on a huge set of computers and\nenterprise machines so he introduced the\nhadoop map reduce and so afterwards\nprocessing big data was faster using\nmapreduce then what is the problem with\nmap reduce prior to 2006 all mapreduce\nprograms were written in java\nnon-programmers found it difficult to\nwrite lengthy java codes they faced\nissues in incorporating map sort reduced\nto fundamentals of mapreduce while\ncreating a program you can see here map\nface shuffle and sort reduce phase\neventually it became a difficult task to\nmaintain and optimize a code due to\nwhich the processing time increased you\ncan imagine a manager trying to go in\nthere and needing a simple query to find\nout data and he has to go talk to the\nprogrammers anytime he wants anything so\nthat was a big problem not everybody\nwants to have a on-call programmer for\nevery manager on their team yahoo faced\nproblems to process and analyze large\ndata sets using java as the codes were\ncomplex and lengthy there was a\nnecessity to develop an easier way to\nanalyze large datasets without using\ntime-consuming complex java modes and\ncodes and scripts and all that fun stuff\napache pig was developed by yahoo it was\ndeveloped with the vision to analyze and\nprocess large datasets without using\ncomplex java codes pig was developed\nespecially for non-programmers pig used\nsimple steps to analyze data sets which\nwas time efficient so what exactly is\npik pig is a scripting platform that\nruns on hadoop clusters designed to\nprocess and analyze large data sets and\nso you have your pig which uses sql like\nqueries they're definitely not sql but\nsome of them resemble sql queries and\nthen we use that to analyze our data pig\noperates on various types of data like\nstructured semi-structured and\nunstructured data let's take a closer\nlook at mapreduce versus hive versus pig\nso we start with a compiled language\nyour mapreduce and we have hive which is\nyour sql like query and then we have pig\nwhich is a scripting language it has\nsome similarities to sql but it has a\nlot of its own stuff remember sql like\nquery which is what hive is based off\nlooks for structured data and so we get\ninto scripting languages like pig now\nwe're dealing more with semi-structured\nand even unstructured data with a hadoop\nmap reduced we have a need to write long\ncomplex codes with hive no need to write\ncomplex codes you could just put it in a\nsimple sql query or hql hive ql and in\npig no need to write complex codes as we\nhave piglet now remember in the map\nreduce it can produce structured\nsemi-structured and unstructured data\nand as i mentioned before hive can\nprocess only structured data think rows\nand columns where pig can process\nstructured semi-structured and\nunstructured data you can think of\nstructured data as rows and columns\nsemi-structured as your html xml\ndocuments like you have on your web\npages and unstructured could be anything\nfrom groups of documents and written\nformat twitter tweets any of those\nthings come in as very unstructured data\nand with our hadoop map reduce we have a\nlower level of abstraction with both\nhive and pig we have a higher level\nabstraction so it's much more easy for\nsomeone to use without having to dive in\ndeep and write a very lengthy map reduce\ncode and those map and reduce codes can\ntake 70 80 lines of code when you can do\nthe same thing in one or two lines with\nhigh ever pig this is the advantage pig\nhas over hive it can process only\nstructured data and hive while in pig it\ncan process structured semi-structured\nand unstructured data some other\nfeatures to know that separates the\ndifferent query languages as we look at\nmap and reduce mapreduce supports\npartitioning features as does hive pig\nno concept of partitioning in pix it\ndoesn't support your partitioning\nfeature your partitioning features allow\nyou to partition the data in such a way\nthat it can be queried quicker you're\nnot able to do that in pig mapreduce\nuses java and python while hive uses an\nsql like query language known as hive ql\nor hql pig latin is used which is a\nprocedural data language mapreduce is\nused by programmers pretty much as\nstraightforward on java hive is used by\ndata analysts pig is used by researchers\nand programmers certainly there's a lot\nof mix between all three programmers\nhave been known to go in and use a hive\nfor quick query and anybody's been able\nto use pig for quick query or research\nunder map and reduce code performance is\nreally good under hive code performance\nis lesser than map and reduce in pig\nunder pig code performance is lesser\nthan mapreduce but better than hive so\nif we're going to look at speed and time\nthe map reduce is going to be the\nfastest performance on all of those\nwhere pig will have second and high\nfollows in the back let's look at\ncomponents of pig pig has two main\ncomponents we have pig latin pig latin\nis the procedural data flow language\nused in pig to analyze data it is easy\nto program using piglet and it is\nsimilar to sql and then we have the\nruntime engine runtime engine represents\nthe execution environment created to run\npig latin programs it is also a compiler\nthat produces mapreduce programs uses\nhdfs or your hadoop file system for\nstoring and retrieving data and as we\ndig deeper into the pig architecture\nwe'll see that we have pig latin scripts\nprogrammers write a script in piglet to\nanalyze data using pig then you have the\ngrunt shell and it actually says grunt\nwhen we start it up and we'll show you\nthat here in a little bit which goes\ninto the pig server and this is where we\nhave our parser parser checks the syntax\nof the pig script after checking the\noutput will be a dag directed acylic\ngraph and then we have an optimizer\nwhich optimizes after your dag your\nlogical plan is passed to the logical\noptimizer where an optimization takes\nplace finally the compiler converts the\ndag into mapreduce jobs and then that is\nexecuted on the map reduce under the\nexecution engine the results are\ndisplayed using dump statement and\nstored in hdfs using store statement and\nagain we'll show you that\nthe kind of end you always want to\nexecute everything once you've created\nit and so dump is kind of our execution\nstatement and you can see right here as\nwe were talking about earlier once we\nget to the execution engine and it's\ncoded into mapreduce then the mapreduce\nprocesses it onto the hdfs\nworking of pig pig latin script is\nwritten by the users so you have low\ndata and write pig script and pig\noperations so when we look at the\nworking of pig pig latin script is\nwritten by the users there's step one we\nload data and write pig script and step\ntwo in this step all the pig operations\nare performed by parser optimizer and\ncompiler so we go into the pig\noperations and then we get to step three\nexecution of the plan in this days the\nresults are shown on the screen\notherwise stored in the hdfs as per the\ncode so it might be of a small amount of\ndata you're reducing it to and you want\nto put that on the screen or you might\nbe converting a huge amount of data\nwhich you want to put back into the\nhadoop file system for other use let's\ntake a look at the pig latin data model\nthe data model of pig latin helps pig to\nhandle various types of data for example\nwe have adam rob or 50. atom represents\nany single value of primitive data type\nin pig latin like integer float string\nit is stored as a string tuple so we go\nfrom our atom which are most basic\nthings so if you look at just rob or\njust 50 that's an atom that's our most\nbasic object we have in pig latin then\nyou have a tuple tuple represents\nsequence of fields that can be of any\ndata type it is the same as a row in\nrdbms for example a set of data from a\nsingle row and you can see here we have\nrob comma five and you can imagine with\nmany of our other examples we've used\nyou might have the id number the name\nwhere they live their age their date of\nstarting the job that would all be one\nrow and stored as a tuple and then we\ncreate a bag a bag is a collection of\ntuples it is the same as a table in\nrdbms and is represented by brackets and\nyou can see here we have our table with\nrob5 mic 10 and we also have a map a map\nis a set of key value pairs key is of\ncharacter array type and a value can be\nof any type it is represented by the\nbrackets and so we have name and age\nwhere the key value is mic in 10. pig\nlatin has a fully nestable data model\nthat means one data type can be nested\nwithin another here's a diagram\nrepresentation of pig latin data model\nand in this particular example we have\nbasically an id number a name and age\nand a place and we break this apart we\nlook at this model from pig latin\nperspective we start with our field and\nif you remember a field contains\nbasically an atom it is one particular\ndata type and the atom is stored as a\nstring which then converts it into\neither an integer or number or character\nstring next we have our tuple and in\nthis case you can see that it represents\na row so our tuple would be three comma\njoe comma 29 comma california and\nfinally we have our bag which contains\nthree rows in it in this particular\nexample let's take a quick look at pig\nexecution modes pig works in two\nexecution modes depending on where the\ndata is reciting and where the pig\nscript is going to run we have local\nmode here the pig engine takes input\nfrom the linux file system and the\noutput is stored in the same file system\nlocal mold local mode is useful in\nanalyzing small data sets using pig and\nwe have the map reduce mode here the pig\nengine directly interacts and executes\nin hdfs and mapreduce in the map reduce\nmode queries written in pig latin are\ntranslated into mapreduce jobs and are\nrun on a hadoop cluster by default pig\nruns in this mode there are three modes\nin pig depending on how a pig latin code\ncan be written we have our interactive\nmode batch mode and embedded mode the\ninteractive mode means coding and\nexecuting the script line by line when\nwe do our example we'll be in the\ninteractive mode in batch mode all\nscripts are coded in a file with the\nextension.pig\nand the file is directly executed and\nthen there's embedded mode pig lets its\nusers define their own functions udfss\nin a programming language such as java\nso let's take a look and see how this\nworks in a use case in this case use\ncase twitter users on twitter generate\nabout 500 million tweets on a daily\nbasis the hadoop mapreduce was used to\nprocess and analyze this data analyzing\nthe number of tweets created by a user\nin the tweet table was done using\nmapreduce and java programming language\nand you can see the problem it was\ndifficult to perform map reduce\noperations as users were not well versed\nwith written complex java codes so\ntwitter used apache pig to overcome\nthese problems and let's see how let's\nstart with the problem statement analyze\nthe user table and tweet table and find\nout how many tweets are created by a\nperson and here you can see we have a\nuser table we have alice tim and john\nwith their id numbers one two three and\nwe have a tweet table in the tweet table\nyou have your um the id of the user and\nthen what they tweeted\ngoogle was a good whatever it was tennis\ndot spacecraft olympics politics\nwhatever they're tweeting about the\nfollowing operations were performed for\nanalyzing given data first the twitter\ndata is loaded into the pig storage\nusing load command and you can see here\nwe have our data coming in and then\nthat's going into pig storage and this\ndata is probably on an enterprise\ncomputer so this is actually active\ntwitter's going on and then it goes into\nhadoop file system remember the hadoop\nfile system is a data warehouse for\nstoring data and so the first step is we\nwant to go ahead and load it into the\npig storage into our data storage system\nthe remaining operations performed are\nshown below in join and group operation\nthe tweet and user tables are joined and\ngrouped using co-group command and you\ncan see here where we add a whole column\nwhen we go from\nuser names and tweet to the id link\ndirectly to the name so alice was user 1\n10 was 2 and john 3. and so now they're\nlisted with their actual tweet the next\noperation is the aggregation the tweets\nare counted according to the names the\ncommand used is count so it's very\nstraightforward we just want to count\nhow many tweets each user is doing and\nfinally the result after the count\noperation is joined with the user table\nto find out the username and you can see\nhere where alice had three tim two and\njohn 1. pig reduces the complexity of\nthe operations which would have been\nlengthy using mapreduce in joining group\noperation the tweet and user tables are\njoined and grouped using co-group\ncommand the next operation is the\naggregation the tweets are counted\naccording to the names the command used\nis count the result after the count\noperation is joined with the user table\nto find out the username and you can see\nwe're talking about three lines of\nscript versus a mapreduce code of about\n80 lines finally we could find out the\nnumber of tweets created by a user in a\nsimple way so let's go quickly over some\nof the features of pig that we already\nwent through most of these\nfirst ease of programming as pig latin\nis similar to sql lesser lines of code\nneed to be written short development\ntime as the code is simpler so we can\nget our queries out rather quickly\ninstead of having to have a programmer\nspend hours on it handles all kinds of\ndata like structured semi-structured and\nunstructured pig lets us create user\ndefined functions pig offers a large set\nof operators such as join filter and so\non it allows for multiple queries to\nprocess on parallel and optimization and\ncompilation is easy as it is done\nautomatically and internally\nso\nenough theory let's dive in and show you\na quick demo on some of the commands you\ncan do in pick today's setup will\ncontinue as we have in the last three\ndemos to go and use cloudera quick start\nand we'll be doing this in virtual box\nwe do have a tutorial in setting that up\nyou can send a note to our simply learn\nteam and then get that linked to you\nonce your cloudera quickstart has spun\nup and remember this is virtualbox we've\ncreated a virtual machine and this\nvirtual machine is centos linux once\nit's spun up you'll be in a full linux\nsystem here and as you can see we have\nthunderbird browser which opens up to\nthe hadoop basic system browser and we\ncan go underneath the hue where it comes\nup by default if you click on the pull\ndown menu and go under editor you can\nsee there's our impala our hive a pig\nalong with a bunch of other query\nlanguages you can use and we're going\nunder pig and then once you're in pig we\ncan go ahead and use our command line\nhere and just click that little blue\nbutton to start it up and running we\nwill actually be working in terminal\nwindow and so if you're in the cloudera\nquick start you can open up the terminal\nwindow up top or if you're in your own\nsetup and you're logged in you can\neasily use all of your commands here in\nterminal window and we'll zoom in that\nway you get a nice view of what's going\non there we go now for our first command\nwe're going to do a hadoop command and\nimport some data into the hadoop system\nin this case a pig input and let's take\na look at this we have a hadoop that\nlisten no it's going to be a hadoop\ncommand dfs there's actually four\nvariations of dfs so if you have hdfs or\nwhatever that's fine all four of them\npoint used to be different setups\nunderneath different things and now they\nall do the same thing and we want to put\nthis file which in this case is under\nhome cloudera documents and sample and\nwe just want to take that and put it\ninto the pig input now let's take a look\nat that file if i go under my document\nbrowsers and open this up you'll see\nit's got a simple id name profession and\nage we have one jack engineer 25 and\nthat was in one of our earlier things we\nhad in there and so let's go ahead and\nhit enter and execute this and now we've\nuploaded that data and it's gone into\nour pig input and then a lot of the\nhadoop commands mimic the linux commands\nand so you'll see we have cat as one of\nour commands or it has a hyphen before\nit so we execute that with hadoop dfs\nhyphen cat slash pig input because\nthat's what we called it that's where we\nput our sample csv at and we execute\nthis you can see from our hadoop system\nit's going to go in and pull that up and\nsure enough it pulls out the data file\nwe just put in there and then we can\nsimply enter the pig latin or pig editor\nmode by typing in pig and we can see\nhere uh by our grunt i told you that's\nhow it was going to tell you you were in\npig latin there's our grunt command line\nso we are now in the pig shell and then\nwe'll go ahead and put our load command\nin here and the way this works is i'm\ngoing to have office equals load and\nhere's my load in this case it's going\nto be pig input we have that in single\nbrackets you remember that's where the\ndata is in the hadoop file system where\nwe dumped it into there we're going to\nusing pig storage our data was separated\nas with a comma so there's our comma\nseparator and then we have as in this\ncase we have an id character array name\ncharacter array profession character a\nand age character ray and we're just\ngoing to do them all as character arrays\njust to keep this simple for this one\nand then when i hit put this all in here\nyou can see that's our full command line\ngoing in and we have our semicolon at\nthe end so when i hit enter it's now set\noffice up but it hasn't actually done\nanything yet it doesn't do anything\nuntil we do dump office so there's our\ncommand to execute whatever we've loaded\nor whatever setup we have in here and we\nrun that you can see it go through the\ndifferent languages and this is going\nthrough the map reduce remember we're\nnot doing this locally we're doing this\non the hadoop setup and once we finished\nour dump you can see we have id name\nprofession age and all the information\nthat we just dumped into our pick oh we\ncan now do let's say oh let's say we\nhave a request just for we'll keep it\nsimple in here but just for the name and\nage and so we can go office we'll call\nit each as our variable underscore each\nand we'll say for each office generate\nname comma h and for each means that\nwe're going to do this for each row and\nif you're thinking map reduce you know\nthat this is a map function because it's\nmapping each row and generating name and\nage on here and of course we want to go\nahead and close it with a semicolon and\nthen once we've created our query or the\ncommand line in here let's go ahead and\ndump office underscore each in with our\nsemicolon and this will go through our\nmap reduce\nsetup on here and if we were on a large\ncluster the same processing time would\nhappen in fact it's really slow because\ni have multiple things on this computer\nand this particular virtual box is only\nusing a quarter of my processor it's\nonly dedicated to this and you can see\nhere there it is name and age and it\nalso included the top row since we\ndidn't delete that out of there or tell\nit not to and that's fine for this\nexample but you need to be aware of\nthose things when you're processing a\nsignificantly large amount of data or\nany data and we can also do office and\nwe'll call this dsc for descending so\nmaybe the boss comes to you and says hey\ncan we order office by id\ndescending and of course your boss\nyou've taught him how to uh your\nshareholder it sounds a little\ndruggatory and say boss you've talked to\nthe shareholder and you said and you've\ntaught them a little bit of pig latin\nand they know that they can now create\noffice description and we can order\noffice by id description and of course\nonce we do that we have to dump office\nunderscore description so that it'll\nactually execute and there goes into our\nmap reduce we'll take just a moment for\nit to come up because again i'm running\non only a quarter of my processor and\nyou can see we now have our ids in\ndescending order returned let's also\nlook at and this is so important with\nanytime you're dealing with big data\nlet's create office with a limit and you\ncan of course do any of this instead of\nwith office we could do this with office\ndescending so you get just the top two\nids on there we're going to limit just\nto two and of course to execute that we\nhave to dump office underscore limit you\ncan just think of dumping your garbage\ninto the pig pen for the pig to eat\nthere we go dump office limit two and\nthat's going to just limit our office to\nthe top two and for our output we get\nour first row which had our id name\nprofession and age and our second row\nwhich is jack who's an engineer now\nlet's do an filter we'll call it office\nunderscore\nfilter you guessed it equals filter\noffice by profession equals and keep\nnote this is uh similar to how python\ndoes it with the double equal signs for\nequal for doing a true false statement\nso for your logic statement remember to\nuse two equal signs in pig and we're\ngoing to say it equals doctor so we want\nto find out how many doctors do we have\non our list and we'll go ahead and do\nour dump we're dumping all our garbage\ninto the pig pen and we're letting pig\ntake over and see what it can find out\nand see who's a doctor on our list and\nwe find uh employee id number two bob is\na doctor 30 years old for this next\nsection we're going to cover something\nwe see it a lot nowadays in data\nanalysis and that's word counting\ntokenization that is one of the next big\nsteps as we move forward in our data\nanalysis where we go from say stock\nmarket analysis of highs and lows and\nall the numbers to what are people\nsaying about companies on twitter what\nare they saying on the web pages and on\nfacebook suddenly you need to start\ncounting words and finding out how many\nwords are totaled how many are in the\nfirst part of the document and so on\nwe're going to cover a very basic word\ncount example and in this case i've\ncreated a document called wordrose.txt\nand you can see here we have simplylearn\nis a company supporting online learning\nsimplylearn helps people attain their\ncertifications simplylearn is an online\ncommunity i love simply learn i love\nprogramming i love data analysis and i\nwent and saved this into my documents\nfolder so we could use it and let me go\nahead and open up a new terminal window\nfor our word count let me go and close\nthe old one so we're going to go in here\nand instead of doing this as pig we're\ngoing to do pig minus x local and what\ni'm doing is i'm telling the pig to\nstart the pig shell but we're going to\nbe looking at files local to our virtual\nbox or this centos machine and let me go\nahead and hit enter on there just\nmaximize this up there we go and it will\nload pig up and it's going to look just\nthe same as the pig we were doing which\nwas defaulted to hi to our hadoop system\nto our hdfs this is now defaulted to the\nlocal system now we're going to create\nlines we're going to load it straight\nfrom the file remember last time we took\nthe hdfs and loaded it into there and\nthen loaded it into pig since we're\ngoing to local we're just going to run a\nlocal script we have lines equals load\nhome the actual full path home cloud\narea documents and i called it\nwordrose.txt\nand as line is a character array so each\nline and i've actually you can change\nthis to read each document i certainly\nhave done a lot of document analysis and\nthen you go through and do word counts\nand different kind of counts in there so\nonce we go ahead and create our line\ninstead of doing the dump we're going to\ngo ahead and start entering all of our\ndifferent setups for each of our steps\nwe want to go through and let's just\ntake a look at this next one because the\nload is straightforward we're loading\nfrom this particular file since we're\nlocals loading it directly from here\ninstead of going into the hadoop file\nsystem and it says as and then each line\nis read as a character array now we're\ngoing to do words equal for each of the\nlines generate flat tokenize line space\nas word now there's a lot of ways to do\nthis this is if you're a programmer\nyou're just splitting the line up by\nspaces there's actual ways to tokenize\nit you gotta look for periods\ncapitalization there's all kinds of\nother things you play with with this but\nfor the most basic word count we're just\ngoing to separate it by spaces the\nflatten takes the line and just creates\na\nit flattens each of the words out so\nthis is uh we're just going to generate\na bunch of words for each line and then\neach each of those words is as a word a\nlittle confusing in there but if you\nreally think about it we're just going\ndown each line separating it out and\nwe're generating a list of words one\nthing to note is the default for\ntokenize you can just do tokenized line\nwithout the space in there if you do\nthat it'll automatically tokenize it by\nspace you can do either one and then\nwe're going to do group we're going to\ngroup it by words so we're going to\ngroup words by word so when we we split\nit up each token is a word and it's a\nlist of words and so we're going to\ngrouped equals group words by word so\nwe're going to group all the same words\ntogether and if we're going to group\nthem then we want to go ahead and count\nthem and so for count we'll go ahead and\ncreate a word count variable and here's\nour four each so for each group grouped\nis our line where we group all the words\nin the line that are similar we're going\nto generate a group and then we're going\nto count the words for each group so for\neach line we group the words together\nwe're going to generate a group and\nthat's going to count the words we want\nto know the word count in each of those\nand that comes back in our word count\nand finally we want to take this and we\nwant to go ahead and dump word count and\nthis is a little bit more what you see\nwhen you start looking at grunt scripts\nyou'll see right here these these lines\nright here we have each of the steps you\ntake to get there so we load our file\nfor each of our\nlines we're going to generate and\ntokenize it into words then we're going\nto take the words and we're going to\ngroup them by same words\nfor each group we're going to generate a\ngroup and we're just going to count the\nwords so we're going to summarize all\nthe words in here and let's go ahead and\ndo our dump word count which executes\nall this and it goes through our\nmapreduce it's actually a local runner\nyou'll see down here you start seeing\nwhere they still have mapreduce but as a\nspecial runner we're mapping it that's a\npart of each row being counted and\ngrouped and then when we do the word\ncount that's a reducer the reducer\ncreates these keys and you can see i is\nused three times a came up once and came\nup once is to continue on down here to\nattain online people company analysis\nsimply learn they took the top rating\nwith four certification so all these\nthings are encountered in the how many\nwords are used uh in in data analysis\nthis is probably the very the beginnings\nof data analysis where you might look at\nit and say oh they mentioned love\nthree times so whatever's going on in\nthis post it's about love and uh what do\nthey love and then you might attach that\nto the different objects in here so you\ncan see that uh pig latin is fairly easy\nto use there's nothing really you know\nmight it takes a little bit to learn the\nscript uh depending on how good your\nmemory is as i get older my memory leaks\na little bit more so i don't memorize it\nas much but that was pretty\nstraightforward the script we put in\nthere and then it goes through the full\nmap reduce localized run comes out and\nlike i said it's very easy to use that's\nwhy people like pig latin is because\nit's intuitive one of the things i like\nabout pig latin is when i'm\ntroubleshooting when we're\ntroubleshooting a lot of times you're\nworking with a small amount of data and\nyou start doing one line at a time and\nso i can go lines equal load and there's\nmy loaded text and maybe i'll just dump\nlines and then it's going to run it's\ngoing to show me all the lines that i'm\nworking on in the small amount of data\nand that way i can test that if i got an\nerror on there that said oh this isn't\nworking maybe they'll be oh my gosh i'm\nin map reduce or i'm in the basic grunt\nshell instead of the local path grunt so\nmaybe it'll generate an error on there\nand you can see here it just shows each\nof the lines going down hive versus pig\non one side we'll have our sharp stinger\non our black and yellow friend and on\nthe other side our thick hide on our pig\nlet's start with an introduction to\nhbase back in the days data used to be\nless and was mostly structured you see\nwe have structured data here we usually\nhad it like in a database where you had\nuh every field was exactly the correct\nlength so if you had a name field that\nis exactly 32 characters remember the\nold access database in microsoft the\nfiles are small if we had you know\nhundreds of people in one database that\nwas considered big data this data could\nbe easily stored in relational database\nor rdbms when we talk about relational\ndatabase you might think of oracle you\nmight think of sql microsoft sql mysql\nall of these have evolved even from back\nthen to do a lot more today than they\ndid but they still fall short in a lot\nof ways and they're all examples of an\nrdms or relationship database then\ninternet evolved and huge volumes of\nstructured and semi-structured data got\ngenerated and you can see here with the\nsemi-structured data we have email if\nyou look at my spam filter you know\nwe're talking about all the html pages\nxml which is a lot of time is displayed\non our html and help desk pages json all\nof this really has just even in the last\neach year it almost doubles from the\nyear before how much of this is\ngenerated so storing and processing this\ndata on an rdbms has become a major\nproblem and so the solution is we use\napache hbase apache hbase was the\nsolution for this let's take a look at\nthe history the hbase history and we\nlook at the hbase history we're going to\nstart back in 2006 november google\nreleased the paper on big table and then\nin 2017 just a few months later hbase\nprototype was created as a hadoop\ncontribution later on in the year 2007\nin october first usable hbase along with\nthe hadoop 0.15 was released and then in\njanuary of 2008 hbase became the\nsubproject of hadoop and later on that\nyear in october all the way into\nseptember the next year hbase was\nreleased the 0.81 version the 0.19\nversion and 0.20 and finally in may of\n2010 hbase became apache top level\nproject and so you can see in the course\nof about four years hbase started off as\njust an idea on paper and has evolved\nall the way till 2010 as a solid project\nunder the apache and since 2010 has\ncontinued to evolve and grow as a major\nsource for storing data in\nsemi-structured data so what is hbase\nhbase is a column-oriented database\nmanagement system derived from google's\nnosql database big table that runs on\ntop of the hadoop file system or the\nhdfs it's an open source project that is\nhorizontally scalable and that's very\nimportant to understand that you don't\nhave to buy a bunch of huge expensive\ncomputers you're expanding it by\ncontinually adding commodity machines\nand so it's a linear cost expansion as\nopposed to being exponential no sql\ndatabase written in java which permits\nfaster querying so java is the backend\nfor the hbase setup and it's well suited\nfor sparse data sets so it can contain\nmissing or n a values and this doesn't\nboggle it down like it would another\ndatabase companies using hbase so let's\ntake a look and see who is using this\nnosql database for their servers and for\nstoring their data and we have\nhortonworks which isn't a surprise\nbecause they're one of the like cloudera\nhortonworks they are behind hadoop and\none of the big developments and backing\nof it and of course apache hbase is the\nopen source behind it and we have\ncapital one as banks you also see bank\nof america where they're collecting\ninformation on people and tracking it so\ntheir information might be very sparse\nthey might have one bank way back when\nthey collected information as far as the\nperson's family and what their income\nfor the whole family is and their\npersonal income and maybe another one\ndoesn't collect the family income as you\nstart seeing where you have data that is\nvery difficult to store or it's missing\na bunch of data hubspot's using it\nfacebook\ncertainly all of your facebook twitter\nmost of your social medias are using it\nand then of course there's jpmorgan\nchase and company another bank that uses\nthe hbase as their data warehouse for\nnosql let's take a look at an hbase use\ncase so we can dig a little bit more\ninto it to see how it functions\ntelecommunication company that provides\nmobile voice and multimedia services\nacross china the china mobile and china\nmobile they generate billions of call\ndetailed records or cdr and so these\ncdrs and all these records are these\ncalls and how long they are and\ndifferent aspects of the call maybe the\ntower they're broadcasted from all that\nis being recorded so they can track it a\ntraditional database systems were unable\nto scale up to the vast volumes of data\nand provide a cost-effective solution no\ngood so storing and real-time analysis\nof billions of call records was a major\nproblem for this company solution apache\nhbase hbase stores billions of rows of\ndetailed call records hbc performs fast\nprocessing of records using sql queries\nso you can mix your sql and no sql\nqueries and usually just say no sql\nqueries because of the way the query\nworks applications of hbase one of them\nwould be in the medical industry hbase\nis used for storing genome sequences\nstoring disease history of people of an\narea and you can imagine how sparse that\nis as far as both of those a genome\nsequence might be only have pieces to it\nthat each person is unique or is unique\nto different people and the same thing\nwith disease you really don't need a\ncolumn for every possible disease a\nperson could get you just want to know\nwhat those diseases those people have\nhad to deal with in that area e-commerce\nhbase is used for storing logs about\ncustomer search history performs\nanalytics and target advertisement for\nbetter business insights sports hba\nstores match details in the history of\neach match uses this data for better\nprediction so when we look at hbase we\nall want to know what's the difference\nbetween hbase versus rdbms that is a\nrelational database management system\nhbase versus rdbms so the hbase does not\nhave a fixed schema it's schema-less\ndefines only column families and we'll\nshow you what that means later on rdbms\nhas a fixed schema which describes the\nstructure of the tables and you can\nthink of this as you have a row and you\nhave columns and each column is a very\nspecific structure how much data can go\nin there and what it does with the hbase\nit works well with structured and\nsemi-structured data with the rdbms it\nworks only well with structured data\nwith the hbase it can have denormalized\ndata it can contain missing or null\nvalues with the rdbms it can store only\nnormalized data now you can still store\na null value in the rdbms but it still\ntakes up the same space as if you're\nstoring a regular value in many cases\nand it also for the hbase is built for y\ntables it can be scaled horizontally for\ninstance if you were doing a tokenizer\nof words and word clusters you might\nhave 1.4 million different words that\nyou're pulling up and combinations of\nwords so with an rdbms it's built for\nthin tables that are hard to scale you\ndon't want to store 1.4 million columns\nin your sql it's going to crash and it's\ngoing to be very hard to do searches\nwith the age base it only stores that\ndata which is part of whatever row\nyou're working on let's look at some of\nthe features of the hbase it's scalable\ndata can be scaled across various nodes\nas it is stored in the hdfs and i always\nthink about this it's a linear add-on\nfor each terabyte of data i'm adding on\nroughly a thousand dollars in commodity\ncomputing with an enterprise machine\nwe're looking at about 10 000 at the\nlower end for each terabyte of data that\nincludes all your backup and redundancy\nso it's a big difference it's like a\ntenth of the cost to store it across the\nhbase it has automatic failure support\nright ahead log across clusters which\nprovides automatic support against\nfailure consistent read and write hbase\nprovides consistent read and write of\nthe data it's a java api for client\naccess provides easy to use java api for\nclients block cache and bloom filters so\nthe hbase supports block caching and\nbloom filters for high volume query\noptimization let's dig a little deeper\ninto the hbase storage hbase column\noriented storage and i told you we're\ngoing to look into this to see how it\nstores the data and here you can see you\nhave a row key this is really one of the\nimportant references is each row has to\nhave its own key or your row id and then\nyou have your column family and in here\nyou can see we have column family one\ncolumn family two column family three\nand you have your column qualifiers so\nyou can have in column family one you\ncan have three columns in there and\nthere might not be any data in that so\nwhen you go into column family one and\ndo a query for every column that\ncontains a certain thing that row might\nnot have anything in there and not be\nqueried where in column family two maybe\nyou have column 1 filled out and column\n3 filled out and so on and so forth and\nthen each cell is connected to the row\nwhere the data is actually stored let's\ntake a look at this and what it looks\nlike when you fill the data in so in\nhere we have a row key with a row id and\nwe have our employee id one two three\nthat's pretty straightforward you\nprobably would even have that on an sql\nserver and then you have your column\nfamily this is where it starts really\nseparating out your column family you\nmight have personal data and under\npersonal data you would have name city\nage you might have a lot more than just\nthat you might have number of children\nyou might have degree all those kinds of\ndifferent things that go into personal\ndata and some of them might be missing\nyou might only have the name and the age\nof an employee you might only have the\nname the city and how many children and\nnot the age and so you can see with the\npersonal data you can now collect a\nlarge variety of data and store it in\nthe hbase very easily and then maybe you\nhave a family of professional data your\ndesignation your salary all the stuff\nthat the employee is doing for you in\nthat company let's dig a little deeper\ninto the hbase architecture and so you\ncan see here what looks to be a\ncomplicated chart it's not as\ncomplicated as you think from the apache\na space we have the zookeeper which is\nused for monitoring what's going on and\nyou have your h master this is the hbase\nmaster assigns regions and load\nbalancing and then underneath the region\nor the hbase master then under the h\nmaster or hbase master you have your\nreader server serves data for read and\nwrite and the region server which is all\nyour different computers you have in\nyour hadoop cluster you'll have a region\nan h log you'll have a store memory\nstore and then you have your different\nfiles for h file that are stored on\nthere and those are separated across the\ndifferent computers and that's all part\nof the hdfs storage system so we look at\nthe architectural components or regions\nand we're looking at we're drilling down\na little bit hbase tables are divided\nhorizontally by a row so you have a key\nrange into regions so each of those ids\nyou might have ids 1 to 20 21 to 50 or\nwhatever they are regions are assigned\nto the nodes in the cluster called\nregion servers a region contains all\nrows in the table between the region's\nstart key and the end key again 1 to 10\n11 to 20 and so forth these servers\nserve data for read and write and you\ncan see here we have the client and the\nget and then git sends it out and it\nfinds out where that start if it's\nbetween which start keys and n keys and\nthen it pulls the data from that\ndifferent region server and so the\nregion sign data definition language\noperation create delete are handled by\nthe h master so the h-master is telling\nit what are we doing with this data\nwhat's going out there assigning and\nreassigning regions for recovery or load\nbalancing and monitoring all servers so\nthat's also part of it so you know if\nyour ids if you have 500 ids across\nthree servers you're not going to put\n400 ids on server 1 and 100 on the\nserver 2 and leaves region 3 and region\n4 empty you're going to split that up\nand that's all handled by the h master\nand you can see here monitors region\nservers assigns regions to region\nservers assigns regions to recent\nservers and so forth and so forth hbase\nhas a distributed environment where\nh-master alone is not sufficient to\nmanage everything hence zookeeper was\nintroduced it works with h-master so you\nhave an active h-master which sends a\nheartbeat signal to zookeeper indicating\nthat it's active and the zookeeper also\nhas a heartbeat to the region servers so\nthe region servers send their status to\nzoo keeper indicating they are ready for\nread and write operation inactive server\nacts as a backup if the active hmaster\nfails it'll come to the rescue active\nhmaster and region servers connect with\na session to zookeeper so you see your\nactive hmaster selection region server\nsession they're all looking at the\nzookeeper keeping that pulse an active\nh-master region server connects with a\nsession to the zoo keeper and you can\nsee here where we have ephemeral nodes\nfor active sessions via heartbeats to\nindicate that the region servers are up\nand running so let's take a look at\nhbase read or write going on there's a\nspecial hbase catalog table called the\nmeta table which holds the location of\nthe regions in the cluster here's what\nhappens the first time a client reads or\nwrites data to hbase the client gets the\nregion server the host the meta table\nfrom zookeeper and you can see right\nhere the client it has a request for\nyour region server and goes hey\nzookeeper can you handle this the\nzookeeper takes a look at it and goes ah\nmiddle location is stored in zookeeper\nso it looks at his meta data on there\nand then the metadata table location is\nsent back to the client the client will\nquery the meta server to get the region\nserver corresponding to the row key if\nit wants to access the client caches\nthis information along with the minute\ntable location and you can see here the\nclient going back and forth to the\nregion server with the information and\nit might be going across multiple region\nservers depending on what you're\nquerying so we get the region server for\nrow key from the meta table that's where\nthat row key comes in and says hey this\nis where we're going with this and so\nonce it gets a row key from the\ncorresponding region server we can now\nput row or git row from that region\nserver let's take a look at the hbase\nmeta table special hbase catalog table\nthat maintains a list of all the region\nservers in the hbase storage system so\nyou see here we have the meta table we\nhave a row key and a value table key\nregion region server so the meta table\nis used to find the region for the given\ntable key and you can see down here you\nknow meta table comes in is going to\nfire out where it's going with the\nregion server and we look a little\ncloser at the right mechanism in hbase\nwe have right ahead log or wall as you\nabbreviate it kind of a way to remember\nwall is right ahead log is a file used\nto store new data that is yet to be put\non permanent storage it is used for\nrecovery in the case of failure so you\ncan see here where the client comes in\nand it literally puts the new data\ncoming in into this kind of temporary\nstorage or the wall on there once it's\ngone into the wall then the memory store\nmemstor is the right cache that stores a\nnew data that's not yet been written to\ndisk there is one mems store per column\nfamily per region and once we've done\nthat we have three ack once the data is\nplaced in mems store the client then\nreceives the acknowledgement when the\nmems store reaches the threshold it\ndumps or commits the data into h file as\nyou can see right here we've taken our\ngun into the wall the wall then source\nit into the different memory stores uh\nand then the memory stores it says hey\nwe've reached we're ready to dump that\ninto our h files and then it moves it\ninto the h files h files store the rows\nas data as stored key value on disk so\nhere we've done a lot of theory let's\ndive in and just take a look and see\nwhat some of these commands look like\nand what happens in our age base when\nwe're manipulating a nosql setup\n[Music]\nso if you're learning a new setup it's\nalways good to start with where is this\ncoming from it's open source by apache\nand you can go to\nhbase.apache.org and you'll see that it\nhas a lot of information you can\nactually download the hbase separate\nfrom the hadoop although most people\njust install the hadoop because it's\nbundled with it and if you go in here\nyou'll find a reference guide and you\ncan go through the apache reference\nguide and there's a number of things to\nlook at but we're going to be going\nthrough apache hbase shell that's what\nwe're going to be working with and\nthere's a lot of other interfaces on the\nsetup and you can look up a lot of the\ndifferent commands on here so we go into\nthe apache hbase reference guide we can\ngo down to read hbase shell commands\nfrom a command file you can see here\nwhere it gives you different options of\nformats for putting the data in and\nlisting the data certainly can also\ncreate files and scripts to do this too\nbut we're going to look at the basics\nwe're going to go through this on a\nbasic hbase shell and one last thing to\nlook at is of course if you continue\ndown the setup you can see here where\nthey have more detail as far as how to\ncreate and how to get to your data on\nyour hbase now i will be working in a\nvirtual box and this is by oracle you\ncan download the oracle virtual box you\ncan put a note in below for the youtube\nas we did have a previous session on\nsetting up virtual setup to run your\nhadoop system in there i'm using the\ncloudera quick start installed in here\nthere's hortons you can also use the\namazon web service there's a number of\noptions for trying this out in this case\nwe have cloudera on the oracle\nvirtualbox the virtual box has linux\ncentos installed on it and then the\nhadoop that has all the different hadoop\nflavors including hbase and i bring this\nup because my computer is a windows 10\nthe operating system of the virtual box\nis linux and we're looking at the hbase\ndata warehouse and so we have three very\ndifferent entities all running on my\ncomputer and that can be confusing if\nit's the first time in and working with\nthis kind of setup now you'll notice in\nour cloudera setup they actually have\nsome hbase monitoring so i can go\nunderneath here and click on hbase and\nmaster and it'll tell me what's going on\nwith my region servers it'll tell me\nwhat's going on with our backup tables\nright now i don't have any user tables\nbecause we haven't created any and this\nis only a single node and a single hbase\ntour so you're not gonna expect anything\ntoo extensive in here since this is for\npractice and education and perhaps\ntesting out package you're working on\nit's not for really you can deploy\ncloudera of course but when you talk\nabout a quick start or a single node\nsetup that's what it's really for so we\ncan go through all the different hbase\nand you'll see all kinds of different\ninformation with zookeeper if you saw it\nflash by down here what version we're\nworking in since zookeeper is part of\nthe hbase setup where we want to go is\nwe want to open up a terminal window and\nin cloudera it happens to be up at the\ntop and when you click on here you'll\nsee your cloudera terminal window open\nand let me just expand this we have a\nnice full screen and then i'm also going\nto zoom in that way you have a nice big\npicture and you can see what i'm typing\nwhat's going on and to open up your\nhbase shell simply type hbase shell to\nget in and hit enter and you'll see it\ntakes just a moment to load and we'll be\nin our age based shell for doing hbase\ncommands once we've gotten into our\nhbase shell you'll see it'll have the\nhbase prompt information ahead of it we\ncan do something simple like list this\nis going to list whatever tables we have\nit so happens that there's a base table\nthat comes with hbase now we can go\nahead and create and i'm going to type\nin just create what's nice about this is\nit's going to throw me kind of a it's\ngoing to say hey there's no just\nstraight create but it does come up and\ntell me all these different formats we\ncan use for create so we can create our\ntable and one of our families and add\nsplits names versions all kinds of\nthings you can do with this let's just\nstart with a very basic one on here and\nlet's go ahead and create and we'll call\nit new\ntable now let's just call it new tbl for\ntable new table and then we also want to\ndo let's do knowledge so let's take a\nlook at this i'm creating a new table\nand it's going to have a family of\nknowledge in it and let me hit enter\nit's going to come up it's going to take\nit a second to go ahead and create it\nnow we have our new table in here so if\ni go list you'll now see table and new\ntable so you can now see that we have\nthe new table and of course the default\ntable that's set up in here and we can\ndo something like uh describe we can\ndescribe and then we're going to do new\ntbl and when we describe it it's going\nto come up it's going to say hey name i\nhave knowledge data block encoding none\nbloom filter row or replication go\nversion all the different information\nyou need new minimum version zero\nforever deleted cells false block size\nin memory you can look this stuff up on\napache.org to really track it down one\nof the things that's important to note\nis versions so you have your different\nversions of the data that's stored and\nthat's always important to understand\nthat we might talk about that a little\nbit later on and then we have to\ndescribe it we can also do a status the\nstatus says i have one active master\ngoing on that's our hbase as a whole we\ncan do status\nsummary it should do the same thing as\nstatus so we got the same thing coming\nup and now that we've created let's go\nahead and put something in it so we're\ngoing to put new tbl and then we want\nrow one you know what before i even do\nthis let's just type in put and you can\nsee when i type in put it gives us like\na lot of different options of how it\nworks and different ways of formatting\nour data as it goes in and all of them\nusually begin with the new table new tbl\nthen we have in this case we'll call it\nrow one and then we'll have knowledge\nremember we created knowledge already\nand we'll do knowledge\nsports and then in knowledge and sports\nwe're gonna set that equal to cricket so\nwe're gonna put underneath this uh our\nknowledge setup that we have a thing\ncalled sports in there and we'll see\nwhat this looks like in just a second\nlet's go ahead and put in we'll do a\ncouple of these let's see let's do\nanother row one and this time set of\nsports let's do\nscience you know this person not only\nyou know we have row one which is both\nknowledgeable and cricket and also in\nchemistry so it's a chemist who plays\ncricket in row one and uh let's see if\nwe have let's do another row one just to\nkeep it going and we'll do science in\nthis case let's do physics not only in\nchemistry but also a physicist i have\nquite a joy in physics myself so here we\ngo we have uh row one there we go and\nthen let's do uh row two let's see what\nthat looks like we start putting in row\ntwo and in row two this person is has\nknowledge in economics this is a master\nof business and how or maybe it's global\neconomics maybe it's just for the\nbusiness and how it fits in with the\ncountry's economics and that we call it\nmacro economics so i guess it is for the\nwhole country there so we have knowledge\neconomics macroeconomics and then let's\njust do one more we'll keep it as row\ntwo and this time our economist is also\na musician so we'll put music and they\nhappen to have knowledge and they enjoy\noh let's do pop music they're into the\ncurrent pop music going on so we've\nloaded our database and you'll see we\nhave two rows row one and row two in\nhere and we can do is we can list the\ncontents of our database by simply doing\nscan scan and then let's just do scan by\nitself so you can see how that looks you\ncan always just type in there and it\ntells you all the different setups you\ncan do with scan and how it works in\nthis case we want to do scan new tbl and\nin our scan new tbl we have row one row\none row two row two and you'll see row\none has a column called knowledge\nscience time step value crickets value\nphysics so it has information as when it\nwas created when the time stamp is row\none also has knowledge sports and a\nvalue of cricut so we have sports and\nscience and this is interesting because\nif you remember up here we also gave it\noriginally we told it to come in here\nand have chemistry we had science\nchemistry and science physics and we\ncome down here i don't see the chemistry\nwhy because we've now replaced chemistry\nwith physics so the new value is physics\non here let me go ahead and clear down a\nlittle bit and in this we're going to\nask the question is enabled new\ntable when i hit enter in here you're\ngoing to see it comes out true and then\nwe'll go ahead and disable it let's go\nahead and disable new\ntable make sure i have our quotes around\nit and now that we've disabled it what\nhappens when we do the scan when we do\nthe scan new table and hit enter you're\ngonna see that we get an error coming up\nso once it's disabled you can't do\nanything with it until we re-enable it\nnow before we enable the table let's do\nan alteration on it and here's our new\ntable and this should look a little\nfamiliar because it's very similar to\ncreate we'll call this test info we'll\nhit enter in there it'll take just a\nmoment for updating and then we want to\ngo ahead and enable it so let's go ahead\nand enable our new table so it's back up\nand running and then we want to describe\ndescribe\nnew table and we come in here you'll now\nsee we have name knowledge and under\nthere we have our data encoding and all\nthe information under knowledge and then\nwe also have down below test info so now\nwe have the name test info and all the\ninformation concerning the test info on\nhere and we'll simply enable it new\ntable so now it's enabled oops already\ndid that i guess we'll enable it twice\nand so let's start looking at well we\nhad scan new table and you can see here\nwhere it brings up the information like\nthis what if we want to go ahead and get\na row so we'll do r1 and when we do\nhbase r1 you can see we have knowledge\nscience and it has a timestamp value\nphysics and we have knowledge sports and\nit has a time stamp on it and value\ncricket and then let's see what happens\nwhen you put into our\nnew\ntable and in here we want row one and if\nyou can guess from earlier because we\ndid something similar uh we're going to\ndo knowledge economics and then it's\ngoing to be instead of i think it was\nwhat macroeconomics is now market\neconomics and we'll go back and do our\nget command and now see what it looks\nlike and we can see here where we have\nknowledge economics it has a timestamp\nvalue market economics physics and\ncricket and this is because we have\neconomics science and sports those are\nthe three different columns that we have\nand then each one has different\ninformation in it and so if you managed\nto go through all these commands and\nlook at basics on here you'll now have\nthe ability to create a very basic hbase\nsetup nosql setup based on your columns\nand your rows and just for fun we'll go\nback to the cloudera where they have the\nwebsite up for the hbase master status\nand i'll go ahead and refresh it and\nthen we can go down here and you'll see\nuser tables table set one and we can\nclick on details and here's what we just\ndid it goes through uh so if you're the\nadmin looking at this and go oh someone\njust created new tbl and this is what\nthey have underneath of it and their new\ntable in there here we will learn on\napache spark history of spark what is\nspark hadoop which is a framework again\nwes spark components of apache spark\nthat is spark core spark sql spark\nstreaming spark ml lab and graphics then\nwe will learn on spark architecture\napplications of spark spark use cases so\nlet's begin with understanding about\nhistory of apache spark it all started\nin 2009 as a project at uc berkeley amp\nlabs by mate\nin 2010 it was open source under a bsd\nlicense in 2013 spark became an apache\ntop level project and in 2014 used by\ndata bricks to sort large-scale data\nsets and it set a new world record so\nthat's how apache spark started and\ntoday it is one of the most in demand\nprocessing framework or i would say in\nmemory computing framework which is used\nacross the big data industry so what is\napache spark let's learn about this\napache spark is a open source in-memory\ncomputing framework or you could say\ndata processing engine which is used to\nprocess data in batch and also in real\ntime across various cluster computers\nand it has a very simple programming\nlanguage behind the scenes that is scala\nwhich is used although if users would\nwant to work on spark they can work with\npython they can work with scala they can\nwork with java and so on even r for that\nmatter so it supports all these\nprogramming languages and that's one of\nthe reasons that it is called polyglot\nwherein you have good set of libraries\nand support from all the programming\nlanguages and developers and data\nscientists incorporate spark into their\napplications or build spark based\napplications to process analyze query\nand transform data at a very large scale\nso these are key features of apache\nspark now if you compare hadoop west\nspark we know that hadoop is a framework\nand it basically has map reduce which\ncomes with hadoop for processing data\nhowever processing data using mapreduce\nin hadoop is quite slow because it is a\nbatch oriented operation and it is time\nconsuming if you if you talk about spark\nspark can process the same data 100\ntimes faster than mapreduce as it is a\nin-memory computing framework well there\ncan always be conflicting ideas saying\nwhat if my spark application is not\nreally efficiently coded and my\nmapreduce application has been very\nefficiently coded well then it's a\ndifferent case however\nnormally if you talk about code which is\nefficiently written for mapreduce or for\nspark based processing spark will win\nthe battle by doing almost 100 times\nfaster than mapreduce so as i mentioned\nhadoop performs batch processing and\nthat is one of the paradigms of map\nreduced programming model which involves\nmapping and reducing and that's quite\nrigid so it performs batch processing\nthe intermittent data is written to sdfs\nand written red back from sdfs and that\nmakes hadoop's map reduce processing\nslower in case of spark it can perform\nboth batch and real-time processing\nhowever lot of use cases are based on\nreal-time processing take an example of\nmacy's take an example of retail giant\nsuch as walmart and there are many use\ncases who would prefer to do real time\nprocessing or i would say near real time\nprocessing so when we say real time or\nnear real time it is about processing\nthe data as it comes in or you are\ntalking about streaming kind of data now\nhadoop or hadoop's mapreduce obviously\nwas started to be written in java now\nyou could also write it in scala or in\npython however if you talk about\nmapreduce it will have more lines of\ncode since it is written in java and it\nwill take more times to execute you have\nto manage the dependencies you have to\ndo the right declarations you have to\ncreate your mapper and reducer and\ndriver classes however if you compare\nspark it has few lines of code as it is\nimplemented in scala and scala is a\nstatically typed dynamically inferred\nlanguage it's very very concise and the\nbenefit is it has features from both\nfunctional programming and\nobject-oriented language and in case of\nscala whatever code is written that is\nconverted into byte codes and then it\nruns in the jvm now hadoop supports\nkerberos authentication there are\ndifferent kind of authentication\nmechanisms kerberos is one of the\nwell-known ones and it can really get\ndifficult to manage now spark supports\nauthentication via a shared secret it\ncan also run on yarn leveraging the\ncapability of kerberos so what are spark\nfeatures which really makes it unique or\nin demand processing framework when we\ntalk about spark features one of the key\nfeatures is fast processing so spark\ncontains resilient distributed data sets\nso rdds are the building blocks for\nspark and we'll learn more about rdds\nlater so spark contains rdds which saves\nhuge time taken in reading and writing\noperations so it can be 100 times or you\ncan say 10 to 100 times faster than\nhadoop when we say in memory computing\nhere i would like to make a note that\nthere is a difference between caching\nand in memory computing think about it\ncaching is mainly to support read ahead\nmechanism where you have your data\npre-loaded so that it can benefit\nfurther queries however when we say in\nmemory computing we are talking about\nlazy evaluation we are talking about\ndata being loaded into memory only and\nonly when a specific kind of action is\ninvoked so data is stored in ram so here\nwe can say ram is not only used for\nprocessing but it can also be used for\nstorage and we can again decide whether\nwe would want that ram to be used for\npersistence or just for computing so it\ncan access the data quickly and\naccelerate the speed of analytics now\nspark is quite flexible it supports\nmultiple languages as i already\nmentioned and it allows the developers\nto write applications in java scala r or\npython it's quite fault tolerance so\nspark contains these rdds or you could\nsay execution logic or you could say\ntemporary data sets which initially do\nnot have any data loaded\nand the data will be loaded into rdds\nonly when execution is happening so\nthese can be fault tolerant as\nthese rdds are distributed across\nmultiple nodes so failure of one worker\nnode in the cluster will really not\naffect the rdds because that portion can\nbe recomputed so it ensures loss of data\nit ensures that there is no data loss\nand it is absolutely fault tolerant it\nis for better than analytics so spark\nhas\nrich set of sql queries machine learning\nalgorithms complex analytics all of this\nsupported by various par components\nwhich we will learn in coming slides\nwith all these functionalities analytics\ncan be performed better in terms of\nspark so these are some of the key\nfeatures of spark however there are many\nmore features which are related to\ndifferent components of spark and we\nwill learn about them so what are these\ncomponents of spark which i'm talking\nabout spark core so this is the core\ncomponent which basically has rdds which\nhas a core engine which takes care of\nyour processing now you also have spark\nsql so people who would be interested in\nworking on structured data or data which\ncan be structuralized would want to\nprefer using spark sql and spark sql\ninternally has components or features\nlike data frames and data sets which can\nbe used to process your structured data\nin a much much faster way you have spark\nstreaming now that's again an important\ncomponent of spark which allows you to\ncreate your spark streaming applications\nwhich not only works on data which is\nbeing streamed in or data which is\nconstantly getting generated but you\nwould also or you could also transform\nthe data you could analyze or process\nthe data as it comes in in smaller\nchunks you have sparks mlib now this is\nbasically a set of libraries which\nallows developers or data scientists to\nbuild their machine learning algorithms\nso that they can do predictive analytics\nor prescriptive descriptive pre-emptive\nanalytics or they could build their\nrecommendation systems or bigger smarter\nmachine learning algorithms using these\nlibraries and then you have graphics so\nthink about organizations like linkedin\nor say twitter where you have data which\nnaturally has a network kind of flow so\ndata which could be represented in the\nform of graphs now here when i talk\nabout graphs i'm not talking about pie\ncharts or bar charts but i'm talking\nabout network related data that is data\nwhich can be networked together which\ncan have some kind of relationship think\nabout facebook think about linkedin\nwhere you have one person connected to\nother person or one company connected to\nother companies so if we have our data\nwhich can be represented in the form of\nnetwork graphs then spark has a\ncomponent called graphics which allows\nyou to do graph based processing so\nthese are some of the components of\napache spark spark core spark sql spark\nstreaming spark mlib and graphics so to\nlearn more about components of spark\nlet's learn here about spark core now\nthis is the base engine and this is used\nfor large scale parallel and distributed\ndata processing so when you work with\nspark at least and the minimum you would\nwork with a spark core which has rdds as\nthe building blocks of your spark so it\nis responsible for your memory\nmanagement your fault recovery\nscheduling distributing and monitoring\njobs on a cluster and interacting with\nstorage systems so here i would like to\nmake a key point that spark by itself\ndoes not have its own storage\nit relies on storage now that storage\ncould be your sdfs that is hadoop's\ndistributed file system it could be\na database like nosql database such as\nhbase or it could be any other database\nsay rdbms from where you could connect\nyour spark and then fetch the data\nextract the data process it analyze it\nso let's learn a little bit about your\nrdds resilient distributed data sets now\nspark core which is the base engine or\nthe core engine is embedded with the\nbuilding blocks of spark which is\nnothing but your resilient distributed\ndata set so as the name says it is\nresilient so it is existing for a\nshorter period of time distributed so it\nis distributed across nodes and it is a\ndata set where the data will be loaded\nor where the data will be existing for\nprocessing so it is immutable fault\ntolerant distributed collection of\nobjects so that's what your rdd is and\nthere are mainly two operations which\ncan be performed on an rdd now to take\nan example of this\nsay i want to process a particular file\nnow here i could write a simple code\nin\nscala and that would basically mean\nsomething like this so if i say val\nwhich is to declare a variable i would\nsay val x and then i could use what we\ncall a spark context which is basically\nthe most important entry point of your\napplication so then i could use a method\nof spark context for example that is\ntext file and then i could point it to a\nparticular file so this is just a method\nof your spark context and spark context\nis the entry point of your application\nnow here i could just give a path in\nthis method so what does this step do it\ndoes not do any evaluation so when i say\nval x i'm creating an immutable variable\nand to that variable i'm assigning a\nfile now what this step does is it\nactually creates a rdd resilient\ndistributed data set so we can imagine\nthis as a simple execution logic a empty\ndata set which is created in memory of\nyour node so if i would say i have\nmultiple nodes in which my data is split\nand stored imagining that your yarn your\nspark is working with hadoop so i have\nhadoop which is using say two nodes and\nthis is my distributed file system sdfs\nwhich basically means my file is written\nto htfs and it also means that the file\nrelated blocks are stored in the\nunderlying disk of these machines so\nwhen i say val x equals sc.text file\nthat is using a method of spark context\nnow there are various other methods like\nhold text files parallel eyes and so on\nthis step will create an rdd so you can\nimagine this as a logical data set which\nis created in memory across these nodes\nbecause these nodes have the data\nhowever no data is loaded here so this\nis the first rdd and i can say first\nstep in what we call as a tag a dag\nwhich will have series of steps which\nwill get executed at later stage now\nlater i could do further processing on\nthis i could say val y and then i could\ndo something on x so i could say x dot\nmap and i would want to apply a function\nto every record or every element in this\nfile and i could give a logic here x dot\nmap now this second step is again\ncreating an rdd a resilient distributed\ndata set you can say second step in my\ndag okay and here you have a external\nrdd one more rdd created which depends\non the first rtd so my first rdd becomes\nthe base rdd or parent rtd and the\nresultant rtd becomes the child rdd then\nwe can go further and we could say val\nzed and i would say okay now i would\nwant to do some filter on y so this\nfilter which i am doing here and then i\ncould give a logic might be i'm\nsearching for a word i am searching for\nsome pattern so i could say val z equals\ny dot filter which again creates one\nmore rdd a resilient distributed data\nset in memory and a you can say this is\nnothing but one more step in the dag so\nthis is my tag which is a series of\nsteps which will be executed now here\nwhen does the execution happen when the\ndata get when will the data get loaded\ninto these rdds so all of this that is\nusing a method using a transformation\nlike map using a transformation like\nfilter or flat map or anything else\nthese are your\ntransformations\nso the operations such as map filter\njoin union and many others will only\ncreate rdds which basically means it is\nonly creating execution logic no data is\nevaluated no operation is happening\nright now only and only when you invoke\nan action that is might be you want to\nprint some result might be you want to\ntake some elements and see that might be\nyou want to do a count so those are\nactions which will actually trigger the\nexecution of this dag right from the\nbeginning so if i here say z\ndot count where i would want to just\ncount the number of words which i am\nfiltering this is an action which is\ninvoked and this will trigger the\nexecution of dag right from the\nbeginning so this is what happens in a\nspark now if i do a z dot count again\nit will start the whole execution of dag\nagain right from the beginning so my z\ndot count second time in action is\ninvoked again the data will be loaded in\nthe first rtd then you will have map\nthen you will have filter and finally\nyou will have result\nso this is the core concept of your rdds\nand this is how rtd works so mainly in\nspark there are two kind of operations\none is your transformations and one is\nyour actions transformations or using a\nmethod of spark context will always and\nalways create an rtd or you could say a\nstep in the tag actions are something\nwhich will invoke the execution which\nwill invoke the execution from the first\nrdd till the last rdd where you can get\nyour result so this is how your rdds\nwork now when we talk about components\nof spark let's learn a little bit about\nspark sql so spark sql is a component\ntype processing framework which is used\nfor structured and semi-structured data\nprocessing so usually people might have\ntheir structured data stored in rdbms or\nin files where data is structured with\nbut particular delimiters and has a\npattern and if one wants to process this\nstructured data if one wants to\nuse spark to do in-memory processing and\nwork on this structured data they would\nprefer to use spark sql so you can work\non different data formats say csv json\nyou can even work on smarter formats\nlike avro parquet even your binary files\nor sequence files you could have your\ndata coming in from an rdbms which can\nthen be extracted using a jdbc\nconnection so at the bottom level when\nyou talk about spark sql it has a data\nsource api which basically allows you to\nget the data in whichever format it is\nnow spark sql has something called as\ndata frame api so what are data frames\ndata frames in short you can visualize\nor imagine as rows and columns or if\nyour data can be represented in the form\nof rows and columns with some column\nheadings so data frame api allows you to\ncreate data frames so like my previous\nexample when you work on a file when you\nwant to process it you would convert\nthat into an rdd using a method of smart\ncontext or by doing some transformations\nso in the similar way when you use data\nframes or when you want to use spark sql\nyou would use sparks context which is\nsql context or hive context or spark\nwhich allows you to work with data\nframes so like in my earlier example we\nwere saying val x equals sc dot text\nfile now in case of data frames instead\nof sc you would be using say spark dot\nsomething so spark context is available\nfor your data frames api to be used in\nolder versions like spark 1.6 and so on\nwe were using hive context or sql\ncontext so if you were working with\nspark 1.6 you would be saying val x\nequals sql context dot here we would be\nusing spark dot so data frame api\nbasically allows you to create data\nframes out of your structured data which\nalso lets spark know that data is\nalready in a particular structure it\nfollows a format and based on that your\nsparks back-end dag scheduler right so\nwhen i say about that i talk about your\nsequence of steps so spark is already\naware of\nwhat are the different steps involved in\nyour application so your data frame api\nbasically allows you to create data\nframes out of your data and data frames\nwhen i say i'm talking about rows and\ncolumns with some headings\nand then you have your data frame dsl\nlanguage or you can use spark sql or\nhive query language any of these options\ncan be used to work with your data\nframes so to learn more about data\nframes follow in the next sessions when\nyou talk about spark streaming now this\nis very interesting for organizations\nwho would want to work on streaming data\nimagine a store like macy's where they\nwould want to have machine learning\nalgorithms now what would these machine\nlearning algorithms do suppose you have\na lot of customers walking in the store\nand they are\nsearching for particular product or\nparticular item so there could be\ncameras placed in the store and this is\nbeing already done there are cameras\nplaced in the store which will keep\nmonitoring in which corner of the store\nthere are more customers now once camera\ncaptures this information this\ninformation can be streamed in to be\nprocessed by algorithms and those\nalgorithms will see which product or\nwhich series of product customers might\nbe interested in and if this algorithm\nin real time can process based on the\nnumber of customers based on the\navailable product in the store it can\ncome up with a attractive alternative\nprice so that which the price can be\ndisplayed on the screen and probably\ncustomers would buy the product now this\nis a real-time processing where the data\ncomes in algorithms work on it do some\ncomputation and give out some result and\nwhich can then result in customers\nbuying a particular product so the whole\nessence of this machine learning and\nreal-time processing will really hold\ngood if and when customers are in the\nstore or this could relate to even an\nonline shopping portal where there might\nbe machine learning algorithms which\nmight be doing real time processing\nbased on the clicks which customer is\ndoing based on the clicks based on\ncustomer history based on customer\nbehavior algorithms can come up with\nrecommendation of products or better\naltered price so that the sale happens\nnow in this case we would be\nseeing the essence of real time\nprocessing only in a fixed or in a\nparticular duration of time and this\nalso means that you should have\nsomething which can process the data as\nit comes in so spark streaming is a\nlightweight api that allows developers\nto perform batch processing and also\nreal-time streaming and processing of\ndata so it provides secure reliable fast\nprocessing of live data streams so what\nhappens here in spark streaming in brief\nso you have a input data stream now that\ndata stream could be a file which is\nconstantly getting appended it could be\nsome kind of metrics it could be some\nkind of events based on the clicks which\ncustomers are doing or based on the\nproducts which they are choosing in a\nstore this input data stream is then\npushed in through a spark streaming\napplication now spark streaming\napplication will broke break this\ncontent into smaller streams what we\ncall as disk criticized streams or\nbatches of smaller data on which\nprocessing can happen in frames so you\ncould say process my file every five\nseconds for the latest data which has\ncome in now there are also some windows\nbased uh\noptions like when i say windows i mean a\nwindow of past three events window of\npast three events each event being of\nfive seconds so your batches of smaller\ndata is processed by spark engine and\nthis process data can then be stored or\ncan be used for further processing so\nthat's what spark streaming does when\nyou talk about mlib it's a low level\nmachine learning library that is simple\nto use scalable and compatible with\nvarious programming languages now hadoop\nalso has some libraries like you have\napache mahout which can be used for\nmachine learning algorithms however in\nterms of spark we are talking about\nmachine learning algorithms which can be\nbuilt using ml labs libraries and then\nspark can be used for processing so mlib\neases the deployment and development of\nscalable machine learning algorithms i\nmean think about your clustering\ntechniques\nso think about your classification where\nyou would want to classify the data\nwhere you would want to do supervised or\nunsupervised learning think about\ncollaborative filtering and many other\ndata science related techniques or\ntechniques which are required to build\nyour recommendation engines or machine\nlearning algorithms can be built using\nsparks mlip graphics is spark's own\ngraph computation engine so this is\nmainly if you are interested in doing a\ngraph based processing think about\nfacebook think about linkedin where you\ncan have your data which can be stored\nand that data\nhas some kind of network connections or\nyou could say it is well networked i\ncould say x is connected to y y is\nconnected to z z is connected to a so x\ny z a all of these are in terms of graph\nterminologies we call as vertices or\nvertex which are basically being\nconnected and the connection between\nthese\nare called edges so i could say a is\nfriend to b so a and b are vertices and\nfriend a relation between them is the\nedge now if i have my data which can be\nrepresented in the form of graphs if i\nwould want to do a processing in such\nway this could be not only for social\nmedia it could be for your network\ndevices it could be a cloud platform it\ncould be about different applications\nwhich are connected in a particular\nenvironment so if you have data which\ncan be represented in the form of graph\nthen graphics can be used to do etl that\nis extraction transformation load to do\nyour data analysis and also do\ninteractive graph computation so\ngraphics is quite powerful now when you\ntalk about spark your spark can work\nwith your different clustering\ntechnologies so it can work with apache\nmesos that's how spark came in where it\nwas initially to prove the credibility\nof apache mesos spark can work with yarn\nwhich is usually you will see in\ndifferent working environments spark can\nalso work as standalone that means\nwithout hadoop spark can have its own\nsetup with master and worker processes\nso usually or you can say technically\nspark uses a master slave architecture\nnow that consists of a driver program\nthat can run on a master node it can\nalso run on a client node it depends on\nhow you have configured or what your\napplication is\nand then you have multiple executors\nwhich can run on worker nodes so your\nmaster node has a driver program and\nthis driver program internally has the\nspark context so your spark every spark\napplication will have a driver program\nand that's driver program has a inbuilt\nor internally used spark context which\nis basically your entry point of\napplication for any spark functionality\nso your driver or your driver program\ninteracts with your cluster manager now\nwhen i say interacts with cluster\nmanager so you have your spark context\nwhich is the entry point that takes your\napplication request to the cluster\nmanager now as i said your cluster\nmanager could be say apache mesos it\ncould be yarn it could be spark\nstandalone master itself so your cluster\nmanager in terms of yarn is your\nresource manager so your spark\napplication internally runs as series or\nset of tasks and processes your driver\nprogram wherever that is run will have a\nspark context and spark context will\ntake care of your application execution\nhow does that do it spark context will\ntalk to cluster manager so your cluster\nmanager could be on and in terms of when\ni say cluster manager for yarn would be\nresource manager so at high level we can\nsay a job is split into multiple tasks\nand those tasks will be distributed over\nthe slave nodes or worker nodes so\nwhenever you do some kind of\ntransformation or you use a method of\nspark context and rdd is created and\nthis rdd is distributed across multiple\nnodes as i explained earlier worker\nnodes are the slaves that run different\ntasks so this is how a spark\narchitecture looks like now we can learn\nmore about spark architecture and its\ninteraction with yarn so usually what\nhappens when your spark context\ninteracts with the cluster manager so in\nterms of yarn i could say resource\nmanager now we already know about yarn\nso you would have say node managers\nrunning on multiple machines and each\nmachine has some ram and cpu cores\nallocated for your node manager on the\nsame machine you have the data nodes\nrunning which obviously are there to\nhave the hadoop related data so whenever\nthe application wants to process the\ndata your application via spark context\ncontacts the cluster managers that is\nresource manager now what does resource\nmanager do resource manager\nmakes a request so resource manager\nmakes requests to the node manager of\nthe machines wherever the relevant data\nresides asking for containers so your\nresource manager is negotiating or\nasking for containers from node manager\nsaying hey can i have a container of 1gb\nram and one cpu core can i have a\ncontainer of 1gb ram and 1 cpu core and\nyour node manager based on the kind of\nprocessing it is doing will approve or\ndeny it so node manager would say fine i\ncan give you the container and once this\ncontainer is allocated or approved by\nnode manager resource manager will\nbasically start an extra piece of code\ncalled appmaster so appmaster is\nresponsible for execution of your\napplications whether those are spark\napplications or mapreduce so your\napplication master which is a piece of\ncode will run in one of the containers\nthat is it will use the ram and cpu core\nand then it will use the other\ncontainers which were allocated by node\nmanager to run the tasks so it is within\nthis container which can take care of\nexecution so what is a container a\ncombination of ram and cpu core so it is\nwithin this container we will have a\nexecutable process which would run and\nthis executor process is taking care of\nyour application related tasks so that's\nhow overall spark works in integration\nwith yarn now let's learn about this\nspark cluster managers as i said spark\ncan work in a standalone mode so that is\nwithout hadoop so by default application\nsubmitted to spark standalone mode\ncluster will run in fifo order and each\napplication will try to use all the\navailable nodes so you could have a\nspark standalone cluster which basically\nmeans you could have multiple nodes on\none of the nodes you would have the\nmaster process running and on the other\nnodes you would have the spark worker\nprocesses running so here we would not\nhave any distributed file system because\nspark is standalone and it will rely on\nan external storage to get the data or\nprobably the file system of the nodes\nwhere the data is stored and processing\nwill happen across the nodes where your\nworker processes are running you could\nhave spark working with apache mesos now\nas i said apache mesos is an open source\nproject to manage your computer clusters\nand can also run hadoop applications\napache mesos was introduced earlier and\nspark came in and as existence to prove\nthe credibility of apache missiles you\ncan have spark working with hadoop's\nyarn this is something which widely you\nwill see in different working\nenvironments so yarn which takes care of\nyour processing and can take care of\ndifferent processing frameworks also\nsupports spark you could have kubernetes\nnow that is something which is making a\nlot of news in today's world it is an\nopen source system for automating\ndeployment scaling and management of\ncontainerized applications so where you\ncould have multiple docker based images\nwhich can be connecting to each other so\nspark also works with kubernetes now\nlet's look at some applications of spark\nso jpmorgan chase and company uses spark\nto detect fraudulent transactions\nanalyze the business spends of an\nindividual to suggest offers and\nidentify patterns to decide how much to\ninvest and where to invest so this this\nis one of the examples of banking a lot\nof banking environments are using spark\ndue to its real-time processing\ncapabilities and in-memory faster\nprocessing where they could be working\non fraud detection or credit analysis or\npattern identification and many other\nuse cases alibaba group that uses also\nspark to analyze large data sets of data\nsuch as real-time transaction details\nnow that might be based online or in the\nstores of looking at the browsing\nhistory in the form of spark jobs and\nthen provides recommendations to its\nusers so alibaba group is using spark in\nits e-commerce domain you have iq here\nnow this is a leading healthcare company\nthat uses spark to analyze patients data\nidentify possible health issues and\ndiagnose it based on their medical\nhistory so there is a lot of work\nhappening in healthcare industry where\nreal time processing is finding a lot of\nimportance and real time and faster\nprocessing is what is required so health\ncare industry and iqvi is also using\nspark you have netflix which is known\nand you have riot games so entertainment\nand gaming companies like netflix and\nride games use apache spark to showcase\nrelevant advertisements to their users\nbased on the videos that they have\nwatched shared or liked so these are few\ndomains which find use cases of spark\nthat is banking e-commerce health care\nentertainment and then there are many\nmore which are using spark in their day\nto day activities for real time in\nmemory faster processing now let's\ndiscuss about the sparks use case and\nlet's talk about conviva which is\nworld's leading video streaming\ncompanies so video streaming is a\nchallenge now if you talk about youtube\nwhich has data you could always read\nabout it so youtube has data which is\nworth watching 10 years so there is huge\namount of data where people are\nuploading their videos or companies are\ndoing advertisements and this\nvideos are streamed in or can be watched\nby users so video streaming is a\nchallenge and especially with increasing\ndemand for high quality streaming\nexperiences conviva collects data about\nvideo streaming quality to give their\ncustomers visibility into the end user\nexperience they are delivering now\nhow do they do it apache spark again\nusing apache spark conviva delivers a\nbetter quality of service to its\ncustomers by removing the screen\nbuffering and learning in detail about\nnetwork conditions in real time this\ninformation is then stored in the video\nplayer to manage live video traffic\ncoming in from 4 billion video feeds\nevery month to ensure maximum retention\nnow using apache spark conviva has\ncreated an auto diagnostics alert it\nautomatically detects anomalies along\nthe video streaming pipeline and\ndiagnoses the root cause of the issue\nnow this really makes it one of the\nleading video streaming companies based\non auto diagnostic alerts it reduces\nwaiting time before the video starts it\navoids buffering and recovers the video\nfrom a technical error and the whole\ngoal is to maximize the viewer\nengagement so this is sparks use case\nwhere conviva is using spark in\ndifferent ways to stay ahead in video\nstreaming related deliveries if we have\nunderstood and learned about your spark\ncomponents your spark architecture we\ncan also see running a spark application\nnow a spark application can run in a\nstandalone mode that is you could set up\nyour ide such as eclipse with a scala\nplug-in and then you could have your\ncoded application which is written in\neclipse to be run in local mode now here\ni have an example of application which\ncan be run in a local mode or on a\ncluster so this application is importing\nsome packages that is spark context\nspark conf i have created an object\nwhich is first app it is the main class\nof your project and other classes can\njust be extending app rather than having\nmain here we declare a variable called x\nwhich is pointing to a file in my\nproject directory it looks for a file\nwhich is abc1.txt\nand this file basically has some content\nwhat are we doing in the application so\nwe create a variable where we assign the\nfile then we define or initialize our\nspark context so remember whenever you\nwork with ide you don't have spark\ncontext or spark available implicitly\nthat has to be defined so here we create\na configuration object we set our\napplication name we set the master as\nlocal if you want to run it in a local\nmode if you want to run it say on your\nwindows machine or if you would want to\nrun on a spark standalone cluster if i\nwould be running it on yarn then i would\nremove this property that is set master\nnow once you have defined your\nconfiguration object you can be\nbasically using spark context you can\nuse method of it which will result in\nrdp that is what is happening in line 13\nval y and then finally i can create a\nvariable where i would want to do a flat\nmap transformation on y which would\nresult in an internal rdd followed by a\nmap transformation which would again\nresult in an rdd and finally reduce by\nkey which is doing an aggregation once\nall these steps are done i can decide to\ndisplay the result on the screen or i\ncan even use save as text file point it\nto a particular location and then have\nmy application run now this is my\neclipse refer to other sessions where\ni've explained how you can set up ide on\nwindows with different environment\nvariables and then you can run your\napplications on a particular cluster if\ni would want to run this application on\na cluster then i will have to give a\nparticular path so here in this case i\ncan say let's do this and i will say my\nfile is\nabc1.txt let's save it and i'll also say\nmy output will be getting stored in a\ndefault location as i intend to run it i\nintend to run this application on a\ncluster now in that case the cluster\nwould usually be set up on linux\nmachines would have a hadoop cluster\nwhere you can run this application so if\ni would want to run this application on\na cluster but not locally on the machine\ni can just delete this part i can keep\nmy application now if you see my\napplication already compiles and does\nnot show any error message and that is\nbecause in my project's build path i\nhave added all the spark related jars\nall the spark related jars you can get\nfrom your spark directory or you can be\ngetting in manually all the dependencies\nsome people would prefer to use maven or\nsay sbt for packaging your application\nas jar and that can also be done so here\nmy code compiles code is fine it is\npointing to a file and then it is also\ncreating an output which will have word\ncount now what i can do is since i have\nmy code already written i know that i\nhave sbt installed on this machine\nbecause i would want to package this\ncode as jar and run it on a cluster so\nfor that we can look in\nour command prompt and here i can go\ninto workspace i can go into my scala\nproject and if you see here we have your\nbuild dot sbt file you have your\nbinaries and you have your source so you\nmight not have these spark related\ndirectories now these exist in my case\nbecause i have been using spark and i've\ndone some previous runs now this is what\nwe have within my spark apps so to build\nmy package i need a build dot sbt file\nwe can see what does that contain so\nyour build.sbt contains your name\nversion of your package jar scala\nversion spark version and then\nrepository which spark will refer to\nwhen it wants to have the dependencies\nfor all of your components such as\nparkour spark sql mlib and so on so this\nis my build.spd file which exists in the\nproject folder and if you are intending\nto use sbt to package your code as jar\nand then run it on the cluster in that\ncase i can even skip adding spark\nrelated charge to the build path so that\nis only done to make sure that your code\ncompiles now once i have my code written\ni have sbt installed i have made the\nfile path changes i can just go to the\ncommand line within my project folder\nand i can say sbt package\nnow this will basically resolve all the\ndependencies based on whatever you have\ngiven in the code it will create a jar\nfile and place it in a particular\nlocation and then we can use the same\njar file to run on a particular cluster\nso sbd package is busy in creating the\njar now meanwhile what i can do is i can\nopen up a lab content and what i can do\nis\ni can just say for example\nsimply learn\nand here i already have a lab set up so\nyou could have your own spark standalone\ncluster where you could also run this\njar file you could have spark with\nhadoop which is what i have here and i\nwill use that to submit an application\non the yarn cluster and let's go to the\nweb console let me copy my link i'll\nclose this i'll say launch lab and then\ni can log in here i can just to paste my\npassword\nand i'm logged in so i can just say\nspark 2 shell now that's how it has been\nconfigured here to work with your spark\nversion 2. so this is an interactive way\nof bringing your spark shell and running\nyour application but what we are\ninterested in is running the application\nas a jar file so let's go and see here\nwhere we have our code and let's see if\nsbt has done the packaging yes it has\ndone and it has created a jar file in\nthis location so what i can do is i need\nto get this jar file from this location\nonto my cluster so what i can do is i\ncan come in here i can do a ftp\nso this basically allows me to push\nwhatever jars i have on my web console\nso i'll go in here i'll say connect now\ni'll search if there is already existing\njar file which might create a conflict\nso i have something here\nso i can for now just delete it it's\ndone i will say upload file and i am\ninterested in getting the jar file so\nhere we can\nclick on users win10 i have my workspace\nproject i can get into target scala 2.11\nand take my jar file and say open so\nthis will upload my jar file on the web\nconsole or the terminal wherein i can\nconnect to my cluster now let's go in\nhere let's quit from spark shell as we\nwant to run an application on the\ncluster how do i do it so i can search\nif i have my jar file existing it's here\nso i'll say spark to submit and then i\nwill point to my jar file and then i can\nsimply say class and i know my code has\na package and a class name so this is\npackage and this is my object or class\nname so i can say main dot scala dot\nfirst app now it will be good if we\ncheck if the file exists which our code\npoints to so i'll just for now comment\nit out i will check in my htfs ls in my\ndefault user directory this is where it\nwill search for a file and here we can\nsearch if i have a file called abc so i\ndon't see anything here so let's do this\nwhat i can do is i can again go to ftp\nand basically i can do a upload file\nlike what we did earlier and this time\ni'll pick up this existing abc1 file\nwhich i showed you and upload it once\nthis is done i can put it on my cluster\nby just saying sdfs dfs put let's take\nthe abc1 file and let's put it in my\ndirectory so this will be my input so\ni'm putting in a file there and now i\ncan do a spark submit to submit the\napplication on the cluster so if you see\nhere it has basically started the\napplication it contacts the resource\nmanager it gets an application id and\nnow it is doing the processing of my\nfile where i would want to get once this\nis done it will be completed and the\ntemporary directory will be deleted so\nthis is how i run a spark application on\na cluster now once this is done i can\nalso go to spark's history server by\nsaying this is the path for my spark\nhistory server it shows an application\nwhich was run today which was doing a\nword count you can click on the\napplication it says it ended with save\nas text file you can click on this and\nthen it shows you the dag visualization\nit says we started with text file we did\na flat map we did a map and then there\nwas some shuffling because we wanted to\ndo reduce by key so as i said every rdd\nby default has two partitions and if you\nwant to do some aggregate or wider\ntransformations like reduce by key sort\nby key group by key count by key and so\non in that case\nsimilar key related data has to be\nshuffled and brought into one partition\nthat's where we see shuffling happening\nhere and it also tells what are the\nnumber of tasks which have run per\npartition so here we ran a spark\napplication by packaging it as jar using\nyour sbt so sbt created the jar file and\nthen basically we brought our jar file\nonto the cluster and then submitted it\nusing spark submit so this is how a\nspark application runs on the cluster so\nas i said your application has a driver\nprogram now your spark applications run\nas independent processes\nand they can run on a cluster across the\nnodes so we just saw how we can run a\nspark application on a cluster now we\ncan always look at spark applications\nprogress or after it has completed by\nlooking into the spark ui your spark\napplication as i mentioned has a driver\nprogram and when you run your\napplication on the\ncluster you can always specify where\nwould you want your driver program to\nrun so in our case when we ran our spark\napplication here we basically just did a\nsimple spark submit and we gave our jar\nand the class name i could also say\nmaster and then i could specify if i\nwould want my application to run in a\nlocal mode or i could say yarn but that\nis default or if it was a spark\nstandalone cluster then i could be\ngiving something like this your host\nname and then your port so you could do\nall of these options by specifying minus\nminus master i could also specify how\nmany executors i need how much memory\nper executor how much course per\nexecutor i need and i could also say\ndeploy mode as client which basically\nmeans my driver will run on this machine\nhowever my execution of application will\nhappen on cluster nodes i can also say\ndeploy mode as cluster which basically\nmeans my driver will run on one of the\nnodes of the cluster you could submit\nyour application and then based on\nwhatever arguments you have given your\napplication will be submitted on the\ncluster and it will run so you can have\nyour application running so as i said\nyou have an application which has a\ndriver and it also has either spark\nsession or spark context which takes\nyour application request to the resource\nmanager now if your application is\ncompleted you can always come back and\nlook into the spark history server or\nspark ui if i choose this as my\napplication which i have run i can go to\nexecutors and that shows me that there\nwas\none for your driver now that was running\non this particular node which is my\nclient node then i have my executors on\nmy other nodes which are nodes of my\ncluster which used one core which ran\ntwo tasks for the partitions which they\nwere working on and there was some\nshuffling involved as we were doing a\nword count which uses reduce by key so\nwhen you run your application your yarn\nor your cluster manager such as resource\nmanager negotiates the resources to the\nslave processes your worker node\nbasically will have the resources\navailable for any kind of execution now\nas i said your resource manager will\nrequest for containers your worker nodes\nwill approve those containers and then\nwithin those containers you will have\nthe executor which will run which will\ntake care of the task to process the\ndata so what is a task it is a unit of\nwork for the data set which has to be\nworked upon so your rdds which get\ncreated have partitions and for every\npartition you have a task which is taken\ncare by the executor so the data is\nloaded into the rdd when the action is\ninvoked and then your task is worked\nupon by the executor so what does it do\nit basically gets the data into the\npartition of rdd and then does the\nexecution as a task on that particular\npartition results are then sent back to\nthe driver and you can also have your\noutput saved on the disk so this is how\nyour application run whenever you run a\nparticular application you can always go\nto your web console and scroll or look\ninto the logs for more information so\nhere our application was run and we can\nsee right from the beginning here when\nwe talk about spark submit which is done\nhere it says running spark version 2.2\nit basically then will see that there\nwill be a driver which will be started\nyou can further see that it does some\nmemory calculations it then starts a\nspark ui and here we can see that\nrequesting a new application from\ncluster with four node managers verify\nour application has not requested more\nthan maximum memory capability of the\ncluster so the container sizing at the\ncluster level is 3 gb per container and\nyour application should not be\nrequesting for a container bigger than\nthat it starts a application master\ncontainer with specific amount of memory\nand then finally your execution starts\nif we scroll down and look into the logs\nwe will see where my driver runs it will\nalso see show you how much memory was\nutilized and it will talk about back\nscheduler which is taking care of\nexecution of your tag that is series of\nyour rdds and finally you will see the\nresult getting generated or saved so\nthis is how you run your spark\napplication this is how you can see your\nspark applications in history server or\nspark ui and this is how overall apache\nspark works utilizing your cluster\nmanager help getting the resources from\nthe worker processes and then running\nthe executors within those here we will\nlearn on what is spark streaming spark\nstreaming data sources\nfeatures of spark streaming working of\nspark streaming disk criticized streams\ncaching or persistence as we call check\npointing in spark streaming and we will\nthen have a demo on spark streaming so\nlet's learn on what is spark streaming\nand what it is capable of so it's an\nextension of core spark api that\nbasically enables scalable high\nthroughput fault tolerant stream\nprocessing of live data streams now data\ncan be ingested from different sources\nlike kafka flume kinases or tcp sockets\nand then it can be processed using\ncomplex algorithms expressed with\ndifferent kind of functions such as map\nreduce join in window and when we have\nthe data processed that data can be\npushed out to your file systems\ndatabases and live dashboards so if you\nlook on this image we can clearly see\nthat we would be working on a input data\nstream which goes into spark streaming\ncomponent of spark which gives us\nbatches of input data or you could say\nthe streaming data is broken down into\nsmaller patches which would then be\nworked upon by spark core engine and you\nhave finally batches of processed data\nnow when we talk about spark streaming\nand the data sources you could have\nstreaming data sources coming in from\nkafka or flume or say twitter api or\nalso with different formats such as\nparquet you could also have static data\nsources coming in from mongodb hbase\nmysql and postgres so when we talk about\nspark streaming it receives the input\ndata streams divides the data into\npatches which can then be processed by\nspark engine to generate your final\nstream of results again in batches so\nyour spark streaming actually provides a\nhigh level abstraction called disk\ncriticize stream and we will learn about\nthat or what we call as d stream which\nrepresents a continuous stream of data\nnow when we look at your different data\nsources from where the data can come in\nspark streaming would take in this input\nand then you could have even your mlib\ncomponent which could be used that is\nsparks component to build machine\nlearning algorithms wherein you can\ntrain your models with live data and you\ncan even use your trained model you\ncould be also going for structured\nprocessing or processing the data which\nis structuralized and that could be done\nby using sparks components that is spark\nsql which has data frames and data sets\nso you could process your data with data\nframes interactively query with sql when\nspark streaming is working on the data\nwhich is constantly flowing in finally\nthe data which is processed can be\nstored in your distributed file system\nsuch as sdfs or any other nosql or sql\nbased database now when we talk about\nspark streaming it is good to know some\nof the features of spark streaming and\nhere you have some of the features so it\nenables fast recovery from failures\nwhile it is working on streaming data\nyou have better load balancing and\nresource usage and you can also combine\nthe streaming data with static data sets\nand perform interactive queries now your\nspark streaming also supports native\nintegration with advanced processing\nlibraries and that is one of the\nbenefits users can have when they are\nusing spark streaming let's learn about\nworking of spark streaming so as i\nmentioned earlier at one end you have\nyour data streams now those data streams\nare then caught or perceived by your\nreceivers which we have to enable in our\napplication your data streams or\nstreaming data which is constantly\ngetting generated and flowing in is\nbroken down into smaller patches which\nwill then be processed by spark so you\nwould have your final processed results\nnow if we look at the bigger picture\nhere we will talk about live input data\nstreams that could be divided into\nsmaller batches and when we say batches\nthese are batches of input data as rdds\nso spark streaming performs computation\nexpressed using these streams it\ngenerates rdd transformations so you\nwould have your spark pad jobs to\nexecute rdd transformations which would\ngive you your final processed result now\nthere are various examples and we'll\nlook at examples later so once your\nspark streaming works on breaking the\ninput into smaller streams it can then\nprocess the data finally giving you\nbatches of your result and that is again\non the streaming data now what is these\nd streams let's understand about these\nstreams or disk criticized streams so\nthat's the basic abstraction provided by\nspark streaming now it represents a\ncontinuous stream of data either the\ninput data stream received from the\nsource or the process data stream\ngenerated by transforming the input\nstream now here if we look at the d\nstream we would say you would have\nseries of rdds or series of\ntransformations applied on the data\nwhich is flowing in for a particular\ntime frame now here we say data from\ntime 0 to 1 and that would result in\nsome of the transformations which you\nwould perform on the data when it has\ncome in between this time zone or time\nframe you would have again data from\ntime 1 to 2 and so on so that's how your\nspark streaming works so if we look at\nyour different transformations now there\ncould be various transformations which\ncould be applied on your data so this is\nsomething like your streaming data which\ncomes in you would want to say have a\nreceiver which monitors a particular\nsocket or a particular port and looks\nfor data coming in we would also define\nthe time interval and for that time\ninterval the data is taken so that's a\nsmaller batch or a d stream on which you\ncould have your processing done now\nwithin your application you would have\nseries of steps which are nothing but\ntransformations which would be performed\non this data within this time frame\ngiving you a result which could be\nstored which could be seen on your\nconsole or which could just be pushed\nfurther for processing and this keeps\nhappening at regular time intervals\nwhatever you have specified till the\nspark streaming application continues\nnow when we talk about spark we already\nknow that there are different kind of\ntransformations which can be applied so\nyou have map transformation wherein you\nhave map and then you pass in a function\nwhich basically says you would want to\nperform a function on every element so\nwhen we say map function that would\nreturn a new d stream by passing each\nelement of the source d stream through a\nfunction which is passed similarly it is\nfor flat map where you would be passing\nin a function you would want to perform\na flat map transformation on the data\nstream or t stream which comes in so\neach input item can be mapped to zero or\nmore output elements\nyou could be doing a filter wherein you\nreturn a d stream by selecting only the\nrecords of the source d stream on which\nthe function returns true so filtering\nis used when you want to run a\ntransformation where you would want to\nlook at the input data for a particular\ntime interval as i mentioned earlier and\nyou would want to filter that data as\nper whatever your function is applied\nnow you could be doing a union where you\ncould basically be having a union of\nmultiple d streams so that this would\nreturn a new d stream that contains\nunion of elements in the source tree\nstream and a other d string you could be\ndoing a transform function that contains\nthe union of elements you could be doing\na count you could be doing a join so\nthese are some of the transformations\nwhich can be performed on your d streams\nnow there is also a concept of windowing\nand that is basically to process the\ndata for a series of time intervals so\nwhen i mention windowed stream\nprocessing i'm saying spark streaming\nwould allow you to apply transformations\nover a sliding window of data now this\noperation is called as windowed\ncomputation let's see how it looks like\nso if you have your original d stream\nwhich is being which is basically your\ndata coming in now that would be looked\nupon for specific time intervals such as\ntime one time two and then you could be\ndoing a windowed computation which could\nbasically mean that i could have a\nwindow which is a series of these time\nintervals on which you would want to\nperform series of your rdd\ntransformations so you have a window d\nstream at time one then at time two and\ntime three so that could be one window\nwherein you would want to get an output\nso here we see window at time three now\nyou could also have a another sliding\nwindow which would take your time series\nand we have time 3 time 4 and time 5\nwhere you could be performing the series\nof transformations so this is usually\nhelpful where you would not only want to\nprocess the data at particular time\ninterval but you would want a\nconsolidated processing for series of\nintervals and that's what we mean by\nwindowed computation now before we\nunderstand about caching and persistence\nwe can talk a little bit more on\nwindowing so if one would want to\nunderstand the window stream processing\nor as we say spark streaming's feature\nof windowed computations we need to\nthink it as applying transformations\nover a sliding window of data now as we\nsee here every time the window slides\nover a source d stream the source rdds\nthat fall within the window are combined\nand operated upon to produce the rdds of\nwindowed stream now in this specific\ncase we can say the operation is applied\nover last three time units of data and\nslides by two time units this shows that\nany window operation needs to specify\ntwo parameters one is the window length\nwhich is basically the duration of\nwindow so for example we can say three\nas we see in the figure here and then\nyou have a sliding interval which is\nbasically the interval at which window\noperation is performed so these two\nparameters must be multiples of the\nbatch interval of source d stream so\nthat's what we do when we talk about\nwindow now there are various other\ntransformations which can be applied or\nwindow based operations which can be\ndone on your d streams here to talk a\nlittle bit more on these streams which\nis as i said it's a basic abstraction\nprovided by spark streaming it\nrepresents a just remember it has a\ncontinuous stream of data now either the\ninput data stream received from source\nor the process data stream which is\ngenerated by transforming the input\nstream so a d stream is represented by\nyou could say a continuous series of\nrdds which is sparks abstraction of a\nimmutable distributed data set so any\noperation applied on a d stream it is\nbasically translating to operations on\nthe underlying rdds now for example we\ncan say converting a stream of lines to\nword the flat map operation is applied\non each rdd in the lines now d stream to\ngenerate the rtds of your words t stream\nso when we talk about your discretized\nstreams understand that you would have a\nseries of transformations which would be\napplied for this time interval whatever\nhas been specified now when you talk\nabout your streaming or spark streaming\narchitecture as i mentioned here that is\nyour receivers now that plays a very\nimportant role here so your input d\nstreams or data streams are representing\nthe stream of input data that is\nreceived from your streaming sources now\nwe could have different kind of data and\nwe could be doing different kind of\ntransformations so your receiver is\nbasically an object which receives the\ndata from a source and stores it in\nsparks memory for processing and that's\nthe main role of your receiver now spark\nstreaming provides two categories of\nbuilding streaming sources so you have\nbasic sources that is sources directly\navailable in streaming context which is\na class we learn about it such as your\nfile systems or socket connections so\nthose could be your basic sources from\nwhere the data is coming in you could\nhave advanced sources like your kafka\nflume kinases etc and they would be\navailable through extra utility classes\nso your receiver is going to be looking\ninto the data which is constantly\ngetting generated and then basically\nforwarding it for processing by your\nspark streaming you know when we talk\nabout your spark streaming one more\nimportant aspect is basically\nunderstanding the caching and\npersistence now as we know from the spar\ncore engine or as you should know that\nrdds are say your logical steps or rdds\nare created when you perform some\ntransformations and these\ntransformations or these computations or\nthese rdds can be cached so that it can\nimprove the performance of your\napplication so the computed rdds or the\nrdds which are result of some performing\nsome transformations they can be cached\nso that they can be reutilized down your\napplication for your further processing\nso when we talk about your caching and\npersistence these streams also allows\ndevelopers to persist the stream data in\nmemory so similar to your concept of\nrdds these streams can allow you to\npersist the particular streams data in\nmemory now that is by doing or using\nyour persist method on a d stream which\nwill automatically persist every rdd of\nthat d stream in memory it could be\nevery rdd or it could be specifically\nchosen rdds now this is really useful if\nthe data in d stream has to be or would\nbe computed multiple times say in your\napplication so for example if we say\nwindow based operations like reduce by\nwindow reduce by key and window wherein\nyou have group of operations being done\nor you could have state based operations\nlike update state by key now in any of\nthese cases your d streams generated by\nsay window based operations are\nautomatically persistent in memory\nwithout the developer calling for the\npersist method now for input streams\nthat receive data over networks such as\nkafka flume sockets etc the default\npersistence level is set to replicate\nthe data to two nodes for fault\ntolerance now one thing which we should\nremember is unlike your rdds the default\npersistence level of d streams keeps the\ndata data serialized in memory and that\nwe can discuss again further about your\nserializations or deserialization now\none important aspect which takes care of\nyour fault recovery is check pointing\nmechanism in spark streaming now when i\nsay check pointing a streaming\napplication as uh in real scenario we\nwould want must operate 24 bar 7 and if\nthe streaming application is constantly\nrunning then there has to be a mechanism\nwhich can make your streaming\napplication resilient to failures which\ncan be unrelated to your application\nlogic so spark streaming needs to do the\ncheck pointing it needs to checkpoint\nenough information to a fault tolerant\nunderlying storage system such that it\ncan recover from failures so your check\npointing is the process to make\nstreaming applications more fault\ntolerant or resilient to your failures\nnow this is usually used when you would\nwant to recover from failure of a node\nrunning the driver of streaming\napplication now we know driver is\nexisting for every application and it is\nbasically one which knows the flow of\nyour application driver also has the\ncontext in case of streaming application\nwe would have say the spark streaming\ncontext which is the entry point of your\napplication now when we talk about your\ncheck pointing to ensure your streaming\napplication is more fault tolerant you\nhave two kinds of checkpointing here so\nyou could have a metadata checkpointing\nand you could have data checkpointing\nnow when we talk about metadata what\ndoes that include so metadata includes\nconfiguration d stream operations or\neven incomplete batches so when we talk\nabout metadata it has configurations\nconfiguration that was used to create\nthe streaming application you could have\nd stream operations that is set of d\nstream operations that define the\nstreaming application that is your\nseries of rdds and then you have\nincomplete batches or batches whose jobs\nare queued but not have completed so\nthis would have to be check pointed so\nmetadata check pointing is used for\nrecovering from a node failure running\nthe streaming application driver now\nyour metadata which has your\nconfiguration incomplete batches and d\nstream operations need to be saved in an\nunderlying storage system now when you\ntalk about data check pointing it is\nmainly about saving the generated rtds\nto reliable storage that is whatever\nrdds are computed so saving the\ninformation which is saving the\ncomputations to a storage like sdfs now\nthat is used in stateful transformations\ncombining data across various batches so\nwhen we talk about transformations\nwhatever rtds are generated that depend\non rdds of previous batches if we are\ntalking about stateful and that can\ncause the length of dependency chain to\nincrease or keep increasing with time\nnow to avoid any kind of increase in the\nrecovery time intermediate rdds can be\nperiodically checkpointed and that could\nbe done to a reliable storage to\nbasically cut off the growing dependency\nchanges so if we would want to summarize\nwe would say metadata checkpointing is\nprimarily needed for recovery from\ndriver failures whereas data or rtd\ncheckpointing is necessary even for\nbasic functioning if stateful\ntransformations are used and we should\nremember we are talking about stateful\ntransformations here so when we talk\nabout check pointing now the question is\nwhen would you enable checkpointing or\nwhat would you do to enable\ncheckpointing so checkpointing must be\nenabled for applications with different\nkind of requirements so for example if\nyou are using stateful transformations\nwhere one series of rdds depend on the\nresult of your previous batches so\nsomething like update state by key or\nreduce by key in window now if these\nkind of operations are used in your\napplication then checkpoint directory\nmust be provided to allow for periodic\nrdd checkpoint when we say about\nrequirements then recovering from\nfailures of the driver which is taking\ncare of your application metadata\ncheckpoints should be used when we talk\nabout simple streaming applications\nwithout the stateful transformations\nthey could be run without enabling\ncheckpointing because your one batch of\nrdds really does not depend on the\nprevious set of rdds which were done in\nthe previous time frame now recovery\nfrom driver failures will also be\npartial in that case when you talk about\nyour stateless so some might be received\nand your unprocessed data might be lost\nbut then that's pretty much acceptable\nwhen you talk about spark streaming\napplications so here we look at your\ncheckpointing so we say start now you\nhave a checkpointing now whenever your\napplication is creating a streaming\ncontext it would create it would set a\ncheckpoint path and then you would\ndefine your d stream which is nothing\nbut series of your rdd transformations\nyour streaming context starts which is\nbasically your application entry point\ninto the cluster for processing and at\nany point of time if there is any\nfailure you could always recover using\nthe checkpoint which was created in case\nof uh spark streaming one more thing we\nneed to remember is that your\ncheckpointing can be enabled by setting\nit directly in a fault tolerant reliable\nfile system such as sdfs wherein the\ncheckpointing information will be saved\nso we will have to add\nsome methods within your application\nlike streaming context with a checkpoint\nand then pointing it to a checkpoint\ndirectory and in that way we can have\nour stateful transformations or metadata\ninformation stored in the underlying\nstorage whichever we have chosen now\nthat we have discussed about spark\nstreaming some basics of spark streaming\nlet's also understand about shared\nvariables or what we call as accumulator\nand broadcast variables so normally when\nyou talk about your spark operations\nsuch as your map or reduce these are\nexecuted on one of the node of your\ncluster and what happens is when you\ntalk about your operations they work on\nseparate copies of all variables used in\nthe function now in this case variables\nare copied to each machine and no\nupdates to the variables on remote\nmachines are propagated back to the\ndriver program now when we talk about\nread write shared variables across tasks\nthat would be inefficient so spark\nactually provides two limited types of\nshared variables for common usage\npatterns and those are your broadcast\nvariables and your accumulators now when\nyou talk about your accumulators these\nare variables that are only added\nthrough an associative or commutative\noperation so spark natively supports\naccumulators of numeric types and\nprogrammers can add support for your new\ntypes so when we talk about accumulators\nthey can be used to implement counters\nsuch as your mapreduce or sums so as a\nuser you can create named or unnamed\naccumulators now as we see in the image\nhere in named accumulator in this\ninstance counter will display in web ui\nfor the stage that modifies that\naccumulator spark displays the value for\neach accumulator modified by a task in\nthe tasks table now tracking\naccumulators in the ui can be useful for\nunderstanding the progress of your\nrunning stages but we should remember\nthat as of now this is not supported in\npython might be in future the support\nfor python will also be added now when\nyou talk about your accumulators you can\nhave say a numeric accumulator created\nby calling a spark context and its\nmethod such as long accumulator or you\ncould have double accumulator to\naccumulate values of type long or double\nrespectively so tasks running on a\ncluster can then add to it using the add\nmethod now we cannot i mean in this case\nthe value cannot be read but driver\nprogram can read the accumulator's value\nusing its value method we can look at\nsome examples to understand this later\nnow when you talk about your broadcast\nvariables that is one more type of\nvariables which allows the programmers\nto keep a read only variable cached on\neach machine rather than shipping a copy\nof it with tasks now what we know is\nsometimes you might be doing very\ncostlier operations like joins where you\nmight be working on multiple rdds they\nneed to be joined and these rtds could\nalso be pair rdds which could be key\nvalue pairs now whenever you are\nperforming a join there would be kind of\ntwo level of shuffling one within a\nparticular rdd so if you are joining two\nrdds the first rdd which might have data\nin the form of key value pairs that rdd\nwould have to have some shuffling where\nall the similar keys can be brought into\none partition and this would happen on\nthe second rdd also which has again key\nvalue pairs and then if you are doing a\njoin these rtds will be shipped to the\nnode or basically the data would be\nloaded in the memory and then basically\nyour transformations will happen this\ncan be a costlier affair so what can be\ndone is you could create broadcast\nvariables so for example if we have two\nrdds and we want to perform a join\noperation and if one rdd is known to be\nsmaller then we can create a broadcast\nvariable of the smaller rdd so that this\nvariable itself can be shipped to each\nmachine and then this variable can be\nused for your join operations with the\nother rdd which is existing in the\nmemory of those nodes so that saves time\nand that improves your performance so\nspark basically attempts to distribute\nthe broadcast variable using efficient\nbroadcast algorithms to reduce the\ncommunication cost now if for example\nyou have multiple nodes in a cluster and\nif you would want to give every node a\ncopy of large input data set in an\nefficient manner so spark actions are\nexecuted through a set of stages now we\nknow that and stages are separated by\nyour shuffle operations so whenever we\ntalk about narrow dependencies such as\nmap flat map filter you would not have\nany shuffling involved but if you go for\ngroup by key reduce by key and such\noperations to bring the similar keys\ntogether there would be shuffling\ninvolved and also this applies when\nyou're doing some join operations so in\nthat case broadcast variables could be a\nreally plus where one set of data or rdd\nwhich is already computed that could be\nbroadcasted to other nodes so the data\nbroadcasted will be cached in serialized\nform and deserialized before running\neach task this means that explicitly\ncreating broadcast variables can be\nuseful when tasks across multiple stages\nneed the same data or when caching the\ndata in the serialized form is important\nso we can create broadcast variables\nusing spark context and its method\ncalled broadcast and your broadcast\nvariable can then be shipped to other\nnodes which can be used for your other\noperations now when you talk about smart\nstreaming it is used in various use\ncases i mean you could talk about speech\nrecognition you could talk about\nsentiment analysis you could talk about\nstreaming applications which would be\nperforming some kind of\nanalytics on the data which is coming in\nit is also used widely in retail chain\ncompanies now if you look at the example\nhere so big retail chain companies would\nwant to build real-time dashboards so\nthat they can keep a track of their\ninventory and operations and for this\nthey would need one streaming data or\ndata which is constantly getting\ngenerated at source which needs to be\nprocessed on and this information can\nthen be populating your dashboards to\ngive you a real-time scenario or\nreal-time information of what is\nhappening so in case of an inventory\ndashboard you could use then these\ninteractive dashboards uh wherein you\ncould draw insights about the business\nand that's what retail companies are\ndoing so how many products are being\npurchased or products that have been\nshipped or how many products have been\ndelivered to customers and this kind of\ninformation would be good to capture in\nreal time so when the streaming data is\nbasically being processed so at one end\nyou have your data which is getting\ngenerated that might be based on the\nsales which are happening that might be\nbased on the products which are being\nshipped or that might be based on the\nacknowledgement that the products have\nbeen received now while this data is\ngetting generated at various sources it\ncan be subjected to a smart streaming\napplication which will look into the\nstreaming data perform series of\ntransformations which you would want to\nprocess the data at regular time\nintervals and then pushing it to your\ndashboards or to your storage layer\nwherein it could then be used to answer\nsuch questions so when we talk about\nspark streaming it's an ideal choice to\nprocess this kind of data in real time\nand there are various use cases so at\none end if you see you have a input\nstream which shows the product status\nthat is how many products were purchased\nor shipped or delivered now this would\nbe then handled by your spark streaming\nand also your sparkcore engine which\nwould then process the data to give you\nan output stream which gives you a\nstatus\nsuch as what was the total count of\nproducts which were purchased products\nwhich were shipped and products which\nwere delivered so this was a quick and\nbrief introduction to spark streaming\nand how it works now we can also see how\nspark streaming works or how we create\nan application for that what we can do\nis we can basically set up our eclipse\nto have your scala based spark\napplications and for that what you could\ndo is you could have your eclipse which\ncan then be having a scala plug-in added\nto it now if somebody would want to look\nat the scala plugin then you can always\ngo to scala.ibe.org\nand this is the place where you can\nscroll towards the bottom you can click\non stable and that basically also shows\nyour video how scala plugin can be added\nto your eclipse it also tells from where\nyou can get the scala latest release and\nthis can be added to your eclipse so in\nmy case it has already been added to the\neclipse i i'm bringing it up here and\nthen you can have your applications\nbuilt using your ide people would prefer\nto use intellij and that's also fine so\nyou could also look for videos on\nsetting up intellij with your scala\nplugin now there are two ways one you\ncan build your application run it on\nyour windows machine when you could have\nsome kind of utility like netcat which\ncan be used to send in some messages in\na streaming fashion you could have a\nreceiver which looks at a particular\nsocket at a particular port and you\ncould build a streaming application\nwithin your ide run it on your windows\nmachine in a local mode which would then\nbe looking at your source where the data\nis getting generated that's one option\nthe second option is you could build\nyour application package it as jar and\nthen run it on a cluster now that could\nbe a spark standalone cluster or spark\nwith yarn and then you could be\npackaging your application using tools\nlike sbt and then use spark submit to\nsubmit your application now i'll show\nyou both the ways in which you can work\nwith streaming applications so first is\nget your eclipse and make sure that\nscala plug-in is already added now in my\ncase on the right top corner i see scala\nsymbol that says i can be using scholar\nperspective now here i have some\nprojects so what you can do is you can\ncreate a project by saying new scala\nproject you can give it a name so for\nexample i could say my apps3 and then\nyou can say finish so that creates a\nproject now within your project what you\ncan do is you can create a package so\nfor example i could say something like\nmain dot scala and then say finish so\nthat creates your package now also one\nthing to remember is it would be good to\nchange your compiler instead of 2.12 to\n2.11 normally for different environments\nyou might have spark with different\nversions and scala with two point turner\n2.11 so it would be good to have the\ncompiler change to the bundle 2.11 so we\ncan select this project i can do a right\nclick i can go to the build path i can\nsay configure build path and then i can\nclick on scala compiler where i will use\nproject settings and i will choose to\n2.11 bundle so that would make sure that\nyour applications can compile just say\nokay it might say the compiler settings\nhave changed a full rebuild is required\nfor changes to take effect just hit on\nok so that's the first thing you need to\ndo the second thing is for your code to\ncompile it would be good to have your\njars added to your build path now we\ncould do that or as i said you could be\nwriting your application which might not\ncompile within your ide but you could\npackage it using sbt and then run it as\njar so what i'm doing here is on my\nmachine within my c drive i already have\nspark distribution which i've downloaded\nso basically i have downloaded spark and\nthen untied it so spark 2.4.3 i have\nunzipped or untied it i have kept it in\nmy c and this basically has my spark now\nwithin which i have my jars and this has\nall my spark related jars so technically\nspeaking i can even use windows command\nline and i can start working on spark in\nan interactive way or packaging my\napplication and running it in a local\nmode so you could also do this so\nbasically have your spark and then if\nyou see on my desktop i have a hadoop\nfolder within which i have a bin folder\nand within which i have a win utils.exe\nand this will be basically required when\nyou want to try out your spark based\napplications whether that is streaming\nor data frames to be tested on your\nwindows machine so you can always search\nand download the winnutils.exe\nplace it in a hadoop folder within the\nbin folder on your machine so once you\nget your winnutels.exe\nonce you have your charts what you can\ndo is for your project you can just say\nright click you can go to the build path\nand you can say configure build path and\nhere you had add external jars so we can\nselect all of these jars here which i\nwould want to add to my build path so\nthat my code can compile and i can test\nit on the windows machine itself i can\nsay open and then just basically do\napply and okay so that has created my\nproject with a package my compiler has\nchanged to 2.11 i have added the\nexternal chars the spark related jars\nand that is good enough for my code now\nwhat we need is a streaming application\nwhich we need to build so let me show\nyou from my existing project how it\nlooks like so within your source the\nsame way i have main package and here i\nhave a streaming application so this\nstreaming application is to test or to\nwork on capturing the data which is\ngenerated at this particular stream on a\nparticular ip and a particular port and\ni would want to do some series of\ntransformations on that so this gives me\nan example of doing a word count and\nthen i would want to print the results i\ncould also be saving the output in a\nparticular location so for this\napplication we need basically to import\ncertain packages that is say spark\nstreaming spark context and also you\nhave spark conf so these are the\npackages which we need to import now\nhere i have created an object i have\ncalled this fifth app and i'm saying\nextends app and this is because within\nmy project i already have an application\nwhich has been defined as main so if\nyour application has been defined as\nmain one application is already existing\nthen you can just have your new objects\nwith extending app so we don't need to\ndefine the main method here now here i'm\nsaying val conf new spark conf so i'm\ncreating a configuration object i'm\nsetting my application name and to test\nit on windows we will have to set the\nmaster as local and it is advisable to\ngive it more than one thread because you\nwould be creating a receiver within your\napplication that would utilize one\nthread so here i am saying set master\nlocal and then i am saying two threads i\nam also creating my spark context which\nwe need to initialize based on the\nconfiguration object we just created in\nthe previous step then we need to create\na spark streaming context and this\nstreaming context depends on spark\ncontext and i've given a time interval\nof 10 seconds so this is the time\ninterval which i am setting for which i\nwould want to work on the stream of data\nwhich comes in every 10 seconds on a\nparticular socket now here we are\nsetting up a receiver so i say stream\nrdd which is basically be spark\nstreaming context and then you use your\nsocket text stream now there are various\nmethods of spark streaming context for\nexample if i just go here and if i just\ndo a dot it shows me what are the\ndifferent options you have file stream\nyou have q stream you have socket stream\nreceiver stream and so on so i am using\nsocket text stream and i would want to\npoint it to this machine so i will say\n127.0.0.1\ni will also specify port 2222 okay and\nhere once i have created my\nconfiguration object my spark context my\nspark streaming context with the time\ninterval and i have set my receiver to\nuse the socket text stream method on\nthis particular ip on this particular\nport now then i am specifying what i\nwould want to do on the data which gets\ngenerated on this machine at this\nparticular port so i am saying val word\ncount so i would want to work on the\nstream rdd that is the d stream i would\nwant to do a flat map on it to split the\ndata based on space i would want to map\nevery word to word comma 1 and then i\nwould want to do a reduce by key now\nreduce by key you can pass in\nspecifically the function you would want\nto do i could do a count of this i could\nprint the result or i could even save it\nwith an output using the java method to\ncreate a random string attached to the\noutput once you're done with this here\ni'm mentioning spark streaming context\nstart now this will basically trigger my\nspark streaming context to start and it\nwill run till we terminate this\napplication now here to run this on\nwindows machine you can always look into\nyour run configuration and what i have\ndone is in my environment i basically\nwant to use\nlet's say okay now let's look at the\nconfiguration what you need to set so\nhere i have\nmy streaming application and let's look\nat the run configuration which basically\nshows my application has started and\nit's running on the environment so if\nyou see here i have added hadoop\nunderscore home pointing to this hadoop\ndirectory which has been and when utils\nx i have also given spark local ip which\nis\n127.0.0.1 so that is my run\nconfiguration now if you see here my\nstreaming application has started my\nstreaming application has not yet\nstarted probably it is running a\ndifferent application so we can come\nback and check this so we can go to run\nas scala application and let's see what\nit does so it tries to connect to\n127.0.0.1\nso receiver is trying to do that but it\ndoes not find anything on that\nparticular machine on that particular\nport so my receiver is not able to find\nit is not able to establish a connection\nnow what i can do is i can go to my\ncommand line and here i will go into\ndownloads so i've already downloaded the\nnetcat utility for windows and here we\ncan basically search for something which\ni have on my machine within downloads\nand that is your netcat utility so what\ni would do is i will come back here and\ni would say nc.exe\nlvp and then i could say my port which i\nhave specified in my streaming\napplication and i can just start this\nnetcat utility now here it says\nlistening on 222 it says connection to\n127.0.0.1\nand then if i look in the background my\nreceiver will now be able to establish a\nconnection establish a connection with\nthis netcat utility now whatever i type\nhere will be taken for processing every\n10 seconds and we would see a word count\nwhile my application is running so let's\ntest this so i'll say this is a test\ntest\nis\nbeing done and as soon as i pass in this\nmessage we see that there is a word\ncount happening for the stream of data\nwhich is coming in we can say winters\nare coming winters will be cold and once\ni given these messages we will see our\nstreaming application which will try to\nwork on the data which is coming in and\nprocess it and show us the result so we\ncan say this is a test\nof winters and let's see if it continues\nto do the processing and shows us that\nright so my streaming application is\nrunning fine it is looking at the words\nwhich we are passing in and we can say\nif it is able to process my data and\nevery 10 seconds the tech stock socket\nstream is looking on this machine at\nthis particular port doing a series of\ntransformations and these series of\ntransformations and then are seen within\nyour console now in my application if i\nhad used word counts dot save as text\nfile then i could also have my output\ngetting generated at every 10 second\ninterval and that would get saved here i\ncould also decide to have my output\nsaved and stored on a sdfs or any other\nstorage so this is a simple streaming\napplication what we saw which is using\nsocket tech stream it is looking at this\nmachine on a particular port where we\nare running a netcat utility it does\nseries of your rdd transformations that\nis flat map map and then you reduce by\nkey and finally we are invoking an\naction such as print which basically\ntriggers these rdds which work on my\nstreaming data so this is one example\nnow what i could also be doing is this\nis particularly my application now i\nalready have sbt for windows installed\nso i can be going into my project space\nso i could say workspace i could go into\nmy apps and i'm in my project folder so\ni could say sbt package and this will\nbasically then based on my build file\nwhich i already have within my project\nfolder so if you see here this is my\nbuild file which says name the version\nscala version spark version and the\nrepository with the spark to manage the\ndependencies for all its components like\nspark core sql ml streaming and hive so\nyou need to have this build dot sbt file\nwithin your project folder which can be\nused to package your application as jar\nnow we have done the packaging and it\nshows me that it has created a jar of my\napplications within this particular\nfolder now once i have this i can\nbasically then import this package into\nmy cluster where i can run it on a\ncluster using spark submit now to run\nthe spark streaming application on a\ncluster here i have a two node cluster\nwhich will then have my spark stand\nalone cluster you could look at the\nprevious videos which wherein i explain\nhow to set up a spark standalone cluster\nnow here i can go into the spark\ndirectory and then i could just say s\nbin start all dot sh which will start by\nspark master and worker note now i do\nhave hadoop cluster also set up on this\nbut right now i'm running a spark\nstandalone cluster which shows me i have\na master and worker process on this\nmachine and i have a worker process on\nthis machine so my spark standalone\ncluster is running we can always bring\nup the ui to see how it looks like so\nhere i can just be giving http\nslash um one eight zero eight zero so\nthat's my spark standalone cluster with\ntwo worker nodes right now there is no\napplication running we can come back\nhere and we can check if we have my jar\nfile so i've already placed my jar here\nnow at any point of time if you have\nyour jar which has been packaged you can\nalways go and do a jar minus xvf and you\ncan choose your jar to see what is\nwithin your jar which i've already done\non this machine so if i look in the main\nfolder i have scala and within scala if\nyou see these are the different classes\nwhat we have so my code was packaged as\na jar it has a fifth app class which we\nwould want to run here so what i can do\nis i can just be saying spark\nsubmit because now i would want my\napplication to run on the cluster so i\ncan say class main dot scala and then my\nobject name so i will say fifth app now\nthat's my class my jar is this one and\nwe will also say master which is going\nto be spark which is running on um one\nand it listens on 7077 port so that\nwould basically start my streaming\napplication but what we would also need\nis like our windows we would also need\nsome utility like netcat so basically\nwhat i can do is i can here use a netcat\nutility to basically connect or send in\nsome messages for our streaming\napplication so let's say let me search\nif i have already the netcat which i\nmight have used here so let's see so we\non your window machine you will have to\ninstall your netcat utility by saying a\napt-get netcat so here i can say nc\nminus l and then i can say port which is\ntwo two two two so we can see if this is\nworking so this is a test and let's see\nif it works so let's start our\napplication streaming application which\nthen needs the receiver to establish a\nconnection with the particular machine\nand let's see if that is done so here i\ncould also be canceling this and give it\na particular port so i can say 192 168\n0.18 now the only problem is when we\npackaged our application when we\npackaged our application we should be\nspecifically giving the ip and also\ncomment out the local so when i package\nthis to run on the cluster i commented\nout this part and i obviously would not\nbe giving 127 but the ip where my net\ncat is running so right now let's test\nit so we will say this\nis a test new test for our application\nand i'm sending in some messages so it\nis already doing the word count as we\nexpected it to do and if while we are\ndoing it if you come back here so you\nwould see your application is now\nrunning on the cluster so it is my\napplication it is using four cores what\nis the memory per executor and you can\nbasically click on your application now\nhere you can look at the application\ndetail ui to see what is being done so\nfor our application we have streaming\njob running receiver which says this is\nmy application it says it is running you\ncan basically click at this click on\nthis and it says your dag visualization\nwhich will say the streaming job which\nis running you can look at the stages if\nyou would want to see if there are\nmultiple stages which we don't have\nbecause we are not doing any wider\ntransformation we can look at storage if\nwe have used some persisting or caching\nwe can look at the executors which are\nbeing used and one of the important\nthings when you talk about your\nstreaming is the streaming tab which\ngets activated now that does not show up\nwhen you run your batch applications but\nin your streaming applications you have\nthe ui shows you the streaming tab and\nthat basically says running batches of\n10 seconds for 2 minutes 9 seconds\nalready now we see the input rate we see\nthe receivers we see if there is any\ndelay in scheduling what is the\nprocessing time how many batches it has\ncompleted and it basically shows me what\nis the processing time how many tasks\nwere run and so on so my application is\nrunning fine and unless and until we go\nahead and cancel this my application\nwill continue to run it will keep\nlooking for messages which we keep\ntyping in so for example if i would want\nto just copy this and start again where\ni would keep passing in this is say test\none test one is this where\nwe test streaming application right and\ni can be seeing that my messages are\nbeing processed i'm getting a word count\nso we can say winter winter summer old\nwinter and you can always look at if\nyour streaming application is doing the\nword count so this is a simple way where\nyou can run your streaming application\neither on a windows machine in a local\nmode or you could be running on a spark\nstandalone cluster or you could also\ndeploy your application to run on a yarn\nbased cluster now that we have seen a\nstreaming application which does a word\ncount it will be interesting to also see\nthe window based computation or\nwindowing operation which is supported\nin spark streaming as i explained and\nfor that we can look at a different\napplication here which is streaming with\na window option let's look into this\nwhat this application does so here i am\nimporting some packages that is\nstreaming streaming context your spark\nin spark context spark configuration i\nam also using the spark api java\nfunction and then using spark streaming\napi and i am also using storage level\nbecause i also intend to have the\npersistence or caching done now here we\ncreate an application say streaming app\nextends app we create our configuration\nobject where we say app name and then we\nset master with appropriate number of\nthreads we create the spark streaming\ncontext that's we initialize it and then\nwe have the spark streaming context\nwhich depends on sc and we have given a\ntime interval of 10 seconds we then set\nup a receiver so we say stream rdd which\nwill use socket extreme like we did\nearlier with which will look for this\nparticular machine at this port wherein\ni'm also using storage level so what i\nwould want is that the data stream which\nwould get generated here every 10\nseconds should be cached in memory now\nwe could use different storage levels\nthat is we could use memory only disk\nonly memory and disk you could have\nmemory and disk with replication factor\nso you have different storage levels now\nhere we are then doing some series of\ntransformations like what we did earlier\nbut if you see in my previous example we\njust had reduce by key and here i am\nusing a transformation which is reduced\nby key and window now this one basically\ntakes the function as i said the\nwindowing takes associative or\ncommutative functions so instead of\nusing the shortcut wave we will be\nspecifying our function so we say it\ntakes a and b and basically it uses this\nfunction now here we have to give the\ninterval that is 30 seconds that's my\nwindow time frame and then within that\nwhat will be my each interval which we\nwould want to look at so these are the\ntwo main aspects one is using your\nwindow based transformation and then\nalso specifying your time interval for\nyour windowing which basically means we\nwould still do a word count but then\nhere we would basically not only be\ndoing a word count for every 10 seconds\nfor the data which is coming in here but\nwe would also want to have a\nconsolidated set of computation done\nevery 30 seconds so that is looking at\nthe last three time intervals now if you\nalso notice i am also doing a check\npointing here wherein i have not\nspecified a specific directory but what\ni am saying here is i would want to\ncheckpoint or i would want to basically\nsave my computations for any kind of\nfault recovery now this is a streaming\napplication with window based\ncomputation we are using storage level\nfor persistence we are also doing the\ncheck pointing which takes care of fault\nrecovery now to run this application we\nhave already set our environment\nvariables and so on so i can basically\nstart\nthis application by just doing a run as\nscala application now this would start\nmy streaming application with window\nbased computations now we can open up\nour command line where we would want to\nstart a netcat utility so i could go to\ndownloads let me just minimize this\nwindow here and what i can do is i can\nthen be starting my netcat utility so i\ncould say\nnc.exe minus lvp and on a port 222. as\nsoon as i do this in background my\napplication receiver will be able to\nestablish connection to this netcat\nutility running on this particular port\nnow that's done so we can see if the\nword count happens for our 10 second\ninterval so let's do that so i'll say\nthis is my first test and if i do this\nit should give me the word count for the\ndata which i have passed in now i can\nsay this is my second test this\nis my third test and if you see here it\nbasically is doing this now if i say\nthis is my first test again so since we\nare doing window based operation also we\nwill see a consolidated set of\ntransformations which happen for a\nwindow of three intervals so if you see\nhere now my this is giving me a count\nbut it is not showing me totally this as\n4 or say this because it is only looking\nat a window of last 2 or 3 time\nintervals so now it is back because it\nis looking at the last one so if i say\nthis is my test test is done test is\ndone so if you do this it will show you\nthe word count which is for whatever you\nhave passed in like test is done is\ntwice test is thrice but then since\nwindowing based operation is done you\nwill look at the time interval of the\nmessages which were passed within that\nparticular window now we can also since\nmy application is running and it is on\nwindows we can bring up your spark ui so\ni can go to http and slash slash i can\nbe looking at my machine and the port so\nfor example i could do this this shows\nmy streaming application which is\nrunning in wherein i can see the window\nbased application which is running but\nwhat would be interesting is to look at\nthe stages if there is any kind of\nshuffling which is happening looking at\nthe storage should show me the\npersistence which we are doing now if\nyou see here it shows me that we have\nbeen persisting in the memory what size\nit occupies how many partitions were\ncached and this is because of the\npersistence or the storage level what we\nhave used as memory only now if we\nchange that to disk we will see our rdds\nwill be cached on disk and then you can\nlook at your streaming tab which gives\nyou more information about your patches\nso this is a very simple example\nobviously you can change what kind of\nrtd computations you would want to do\nhere whether you want to do a word count\nor something else whether you want to\nsave your processed output now here we\nare able to do that you can always do a\nrefresh on your project and here what we\nsee is we see some entries of a folder\ngetting created right and we can decide\nwhether we would want our output to be\ncreated we can also see within this what\nwould be containing and since we have\nset check pointing in the project folder\nit would also be check pointing\ntemporarily the information for any kind\nof failure so this is a classic example\nof streaming application using window\nbased computations persisting your\ncomputations and also doing the check\npointing simultaneously in this tutorial\nhere we have some list of questions and\nan explanation of those so that you can\nbe well prepared for your hadoop\ninterviews now let's look at some\ngeneral hadoop questions so what are the\ndifferent vendor specific distributions\nof hadoop now all of you might be aware\nthat hadoop or apache hadoop is the core\ndistribution of hadoop and then you have\ndifferent vendors in the market which\nhave packaged the apache hadoop in a\ncluster management solution which allows\neveryone to easily deploy manage monitor\nupgrade your clusters so here are some\nvendor-specific distributions we have\ncloudera which is the dominant one in\nthe market we have hortonworks and now\nyou might be aware that cloudera and\nhortonworks have merged so it has become\na bigger entity you have map r you have\nmicrosoft azure ibm's infosphere and\namazon web services so these are some\npopularly known vendor-specific\ndistributions if you would want to know\nmore about the hadoop distributions you\nshould basically look into google and\nyou should check for hadoop different\ndistributions wiki page so if i type\nhadoop different distributions and then\ni check for the wiki page that will take\nme to the distributions and commercial\nsupport page and this basically says\nthat the sold products that can be\ncalled a release of apache hadoop come\nfrom apache.org so that's your open\nsource community and then you have\nvarious vendor-specific distributions\nwhich basically are running in one or\nthe other way apache hadoop but they\nhave packaged it as a solution like an\ninstaller so that you can easily set up\nclusters on set of machines so have a\nlook at this page and read through about\ndifferent distributions of hadoop coming\nback let's look at our next question so\nwhat are the different hadoop\nconfiguration files now whether you're\ntalking about apache hadoop cloudera\nhortonworks map r or no matter which\nother distribution these config files\nare the most important and existing in\nevery distribution of hadoop so you have\nhadoop environment dot sh wherein you\nwill have environment variables such as\nyour java path what would be your\nprocess id path where will your logs get\nstored what kind of metrics will be\ncollected and so on your core hyphen\nsite file has the hdfs path\nnow this has many other properties like\nenabling trash or enabling high\navailability or discussing or mentioning\nabout your zookeeper but this is one of\nthe most important file you have hdfs\nhyphen site file now this file will have\nother information related to your hadoop\ncluster such as your replication factor\nwhere will name node store its metadata\non disk if a data node is running where\nwould data node store its data if a\nsecondary name node is running where\nwould that store a copy of name node's\nmetadata and so on your mapred hyphen\nsite file is a file which will have\nproperties related to your mapreduce\nprocessing you also have masters and\nslaves now these might be deprecated in\na vendor-specific distribution and in\nfact you would have a yarn hyphen site\nfile which is based on the yarn\nprocessing framework which was\nintroduced in hadoop version 2 and this\nwould have all your resource allocation\nand resource manager and node manager\nrelated properties again if you would\nwant to look at default properties for\nany one of these for example let's say\nhdfs hyphen site file i could just go to\ngoogle and type in one of the properties\nfor example i would say\ndfs.namenode.name.directory and as i\nknow this property belongs to hdfs\nhyphen site file and if you search for\nthis it will take you to the first link\nwhich says sdfs default xml you can\nclick on this and this will show you all\nthe properties which can be given in\nyour sdfs hyphen site file it also shows\nyou which version you are looking at and\nyou can always change the version here\nso for example if i would want to look\nat 2.6.5\ni just need to change the version and\nthat should show me the properties\nsimilarly you can just give a property\nwhich belongs to say core hyphen site\nfile for example i would say fs dot\ndefault fs and that's a property which\nis in core hyphen site file and\nsomewhere here you would see core minus\ndefault.xml and this will show you all\nthe properties so similarly you could\nsearch for properties which are related\nto yarn hyphen site file or map red\nhyphen site file so i could say yarn dot\nresource manager and i could look at one\nof these properties which will directly\ntake me to yarn default xml and i can\nsee all the properties which can be\ngiven in yarn and similarly you could\nsay map reduce dot job dot reduces and i\nknow this property belongs to mapreduce\nhyphen site file and this takes you to\nthe default xml so these are important\nconfig files and no matter which\ndistribution of hadoop you're working on\nyou should be knowing about these config\nfiles whether you work as a hadoop admin\nor you work as a hadoop developer\nknowing these config properties would be\nvery important and that would also\nshowcase your internal knowledge about\nthe configs which drive your hadoop\ncluster let's look at the next question\nso what are the three modes in which\nhadoop can run so you can have hadoop\nrunning in a standalone mode now that's\nyour default mode it would basically use\na local file system in a single java\nprocess so when you say standalone mode\nit is as you downloading hadoop related\npackage on one single machine but you\nwould not have any process running that\nwould just be to test hadoop\nfunctionalities you could have a pseudo\ndistributed mode which basically means\nit's a single node hadoop deployment now\nhadoop as a framework has many many\nservices so it has a lot of services and\nthose services would be running\nirrespective of your distribution and\neach service would then have multiple\nprocesses so your pseudo-distributed\nmode is a mode of cluster where you\nwould have all the important processes\nbelonging to one or multiple services\nrunning on a single node if you would\nwant to work on a sudo distributed mode\nand using a cloudera you can always go\nto google and search for cloudera's\nquick start vm you can download it by\njust saying cloudera quick start vm and\nyou can search for this and that will\nallow you to download a quick start vm\nfollow the instructions and you can have\na single node cloudera cluster running\non your virtual machines for more\ninformation you can refer to the youtube\ntutorial where i have explained about\nhow to set up a quick start vm coming\nback you could have finally a production\nsetup or a fully distributed mode which\nbasically means that your hadoop\nframework and its components would be\nspread across multiple machines so you\nwould have multiple services such as\nsdfs yarn flume scope\nkafka hbase hive impala and for these\nservices there would be one or multiple\nprocesses distributed across multiple\nnodes so this is normally what is used\nin production environment so you could\nsay standalone would be good for testing\npseudo distributed could be good for\ntesting and development and fully\ndistributed would be mainly for your\nproduction setup now what are the\ndifferences between regular file system\nand hdfs so when you say regular file\nsystem you could be talking about a\nlinux file system or you could be\ntalking about a windows based operating\nsystem so in regular file system we\nwould have data maintained in a single\nsystem so the single system is where you\nhave all your files and directories so\nit is having low fault tolerance right\nso if the machine crashes your data\nrecovery would be very difficult unless\nand until you have a backup of that data\nthat also affects your processing so if\nthe machine crashes or if the machine\nfails then your processing would be\nblocked now the biggest challenge with\nregular file system is the seek time the\ntime taken to read the data so you might\nhave one single machine with huge amount\nof disks and huge amount of ram but then\nthe time taken to read that data when\nall the data is stored in one machine\nwould be very high and that would be\nwith least fault tolerance if you talk\nabout sdfs your data is distributed so\nsdfs stands for hadoop distributed file\nsystem so here your data is distributed\nand maintained on multiple systems so it\nis never one single machine it is also\nsupporting reliability so whatever is\nstored in hdfs say a file being stored\ndepending on its size is split into\nblocks and those blocks will be spread\nacross multiple nodes not only that\nevery block which is stored on a node\nwill have its replicas stored on other\nnodes replication factor depends but\nthis makes sdfs more reliable in cases\nof your slave nodes or data nodes\ncrashing you will rarely have data loss\nbecause of auto replication feature now\ntime taken to read the data is\ncomparatively more\nas you might have situations where your\ndata is distributed across the nodes and\neven if you are doing a parallel read\nyour data read might take more time\nbecause it needs coordination from\nmultiple machines however if you are\nworking with huge data which is getting\nstored it will still be beneficial in\ncomparison to reading from a single\nmachine so you should always think about\nits reliability through auto replication\nfeature its fault tolerance because of\nyour data getting stored across multiple\nmachines and its capability to scale so\nwhen you talk about sdfs we are talking\nabout horizontal scalability or scaling\nout when you talk about regular file\nsystem you are talking about vertical\nscalability which is scaling up\nnow let's look at some specific sdfs\nquestions what is this why is sdfs fault\ntolerant now as i just explained in\nprevious slides your sdfs is fault\ntolerant as it replicates data on\ndifferent data nodes so you have a\nmaster node and you have multiple slave\nnodes or data nodes where actually the\ndata is getting stored now we also have\na default block size of 128 mb that's\nthe minimum since hadoop version 2. so\nany file which is up to 128 mb would be\nusing one logical block and if the file\nsize is bigger than 128 mb then it will\nbe split into blocks and those blocks\nwill be stored across multiple machines\nnow since these blocks are stored across\nmultiple machines it makes it more fault\ntolerant because even if your machines\nfail you would still have a copy of your\nblock existing on some other machine now\nthere are two aspects here one we talked\nabout the first rule of replication\nwhich basically means you will never\nhave two identical blocks sitting on the\nsame machine and the second rule of\nreplication is in terms of rack\nawareness so if your machines are placed\nin racks as we see in the right image\nyou will never have all the replicas\nplaced on the same rack even if they are\non different machines so it has to be\nfault tolerant and it has to maintain\nredundancy so at least one replica will\nbe placed on some other node on some\nother rack that's how sdfs is fault\ntolerant now here let's understand the\narchitecture of sdfs now as i mentioned\nearlier you would in a hadoop cluster\nthe main service is your hdfs\nso for your sdfs service you would have\na name node which is your master process\nrunning on one of the machines and you\nwould have data nodes which are your\nslave machines getting stored across or\ngetting or the processes running across\nmultiple machines each one of these\nprocesses has an important role to play\nwhen you talk about sdfs whatever data\nis written to hdfs\nthat data is split into blocks depending\non its size and the blocks are randomly\ndistributed across nodes with auto\nreplication feature these blocks are\nalso auto replicated across multiple\nmachines with the first condition that\nno two identical blocks will sit on the\nsame machine now as soon as the cluster\ncomes up your data nodes which are part\nof the cluster and based on config files\nwould start sending their heartbeat to\nthe name node and this would be every\nthree seconds what does name node do\nwith that name node will store this\ninformation in its ram so name node\nstarts building a metadata in its ram\nand that metadata has information of\nwhat are the data nodes which are\navailable in the beginning now when a\ndata\nwriting activity starts and the blocks\nare distributed across data nodes data\nnodes every 10 seconds will also send a\nblock report to name node so name node\nis again adding up this information in\nits ram or the metadata in ram which\nearlier had only data node information\nnow name node will also have information\nabout what are the files the files are\nsplit in which blocks the blocks are\nstored on which machines and what are\nthe file permissions now while name node\nis maintaining this metadata in ram name\nnode is also maintaining metadata in\ndisk now that is what we see in the red\nbox which basically has information of\nwhatever information was written to hdfs\nso to summarize your name node has\nmetadata in ram and metadata in disk\nyour data nodes are the machines where\nyour blocks or data is actually getting\nstored and then there is a auto\nreplication feature which is always\nexisting unless and until you have\ndisabled it and your read and write\nactivity is a parallel activity however\nreplication is a sequential activity now\nthis is what i mentioned about when you\ntalk about name node which is the master\nprocess hosting metadata in disk and ram\nso when we talk about disk it basically\nhas a edit log which is your transaction\nlog and your fs image which is your file\nsystem image right from the time the\ncluster was started this metadata in\ndisk was existing and this gets appended\nevery time read write or any other\noperations happen on sdfs metadata in\nram is dynamically built every time the\ncluster comes up which basically means\nthat if your cluster is coming up name\nnode in the initial few seconds or few\nminutes would be in a safe mode which\nbasically means it is busy registering\nthe information from data nodes so name\nnode is one of the most critical\nprocesses if name node is down and if\nall other processes are running you will\nnot be able to access the cluster name\nnode's metadata in disk is very\nimportant for name node to come up and\nmaintain the cluster name node's\nmetadata in ram is basically for all or\nsatisfying all your client requests now\nwhen we look at data nodes as i\nmentioned data nodes hold the actual\ndata blocks and they are sending these\nblock reports every 10 seconds so the\nmetadata in name nodes ram is constantly\ngetting updated and metadata in disk is\nalso constantly getting updated based on\nany kind of write activity happening on\nthe cluster now data node which is\nstoring the block will also help in any\nkind of read activity whenever a client\nrequests so whenever a client on an\napplication or an api would want to read\nthe data it would first talk to name\nnode name node would look into its\nmetadata on ram and confirm to the\nclient which machines could be reached\nto get to that data that's where your\nclient would try to read the data from\nsdfs which is actually getting the data\nfrom data nodes and that's how your read\nwrite requests are satisfied now what\nare the two types of metadata in name\nnode server holds as i mentioned earlier\nmetadata in disk very important to\nremember edit log nfs image metadata in\nram which is information about your data\nnodes files files being split into\nblocks blocks residing on data nodes and\nfile permissions so i will share a very\ngood link on this and you can always\nlook for more detailed information about\nyour metadata so you can search for sdfs\nmetadata directories explained now this\nis from hortonworks however it talks\nabout the metadata in disk which name\nnode manages and details about this so\nhave a look at this link if you are more\ninterested in learning about metadata on\ndisk coming back\nlet's look at the next question what is\nthe difference between federation and\nhigh availability now these are the\nfeatures which were introduced in hadoop\nversion 2. both of these features are\nabout horizontal scalability of name\nnode so prior to version 2 the only\npossibility was that you could have one\nsingle master which basically me\nmeans that your cluster could become\nunavailable if name node would crash so\nhadoop version 2 introduced two new\nfeatures federation and high\navailability however high availability\nis a popular one so when you talk about\nfederation it basically means any number\nof name nodes so there is no limitation\nto the number of name nodes your name\nnodes are in a federated cluster which\nbasically means name nodes still belong\nto the same cluster but they are not\ncoordinating with each other so whenever\na write request comes in one of the name\nnode picks up that request and it guides\nthat request for the blocks to be\nwritten on data nodes but for this your\nname node does not have to coordinate\nwith other name node to find out if the\nblock id which was being assigned was\nthe same one as assigned by other name\nnode so all of them belong to a\nfederated cluster they are linked via a\ncluster id so whenever an application or\nan api is trying to talk to cluster it\nis always going via an cluster id and\none of the name node would pick up the\nread activity or write activity or\nprocessing activity so all the name\nnodes are sharing a pool of metadata in\nwhich each name node will have its own\ndedicated pool and we can remember that\nby a term called namespace or name\nservice so this also provides high fault\ntolerance supposed your one name node\ngoes down it will not affect or make\nyour cluster unavailable you will still\nhave your cluster reachable because\nthere are other name nodes running and\nthey are available now when it comes to\nheartbeats all your data nodes are\nsending their heart beats to all the\nname nodes and all the name nodes are\naware of all the data nodes when you\ntalk about high availability this is\nwhere you would only have two name nodes\nso you would have an active and you\nwould have a standby now normally in any\nenvironment you would see a\nhigh availability setup with zookeeper\nso zookeeper is a centralized\ncoordination service so when you talk\nabout your active and standby name notes\nelection of a name node to be made as\nactive and taking care of an automatic\nfailover is done by your zookeeper high\navailability can be set up without\nzookeeper but that would mean that the\nadmins intervention would be required to\nmake a name node as active from standby\nor also to take care of failover\nnow at any point of time in high\navailability a active name node would be\ntaking care of storing the edits about\nwhatever updates are happening on sdfs\nand it is also writing these edits to a\nshared location standby name node is the\none which is constantly looking for\nthese latest updates and applying to its\nmetadata which is actually a copy of\nwhatever your active name node has so in\nthis way your standby name node is\nalways in sync with the active name node\nand if for any reason active name node\nfails your standby name node will take\nover and become the active remember\nzookeeper plays a very important role\nhere it's a centralized coordination\nservice one more thing to remember here\nis that in your high availability\nsecondary name node will not be allowed\nso you would have a active name node and\nthen you will have a standby name node\nwhich will be configured on a separate\nmachine and both of these will be having\naccess to a shared location now that\nshared location could be nfs or it could\nbe a quorum of general nodes so for more\ninformation refer to the tutorial where\ni have explained about sdfs high\navailability in federation now let's\nlook at some logical question here so if\nyou have a input file of 350 mb which is\nobviously bigger than 128 mb how many\ninput splits would be created by sdfs\nand what would be the size of each input\nsplit so for this you need to remember\nthat by default the minimum block size\nis 128 mb now that's customizable if\nyour environment has more number of\nlarger files written on an average then\nobviously you have to go for a bigger\nblock size if your environment has a lot\nof files being written but these files\nare of smaller size you could be okay\nwith 128 mb remember in hadoop every\nentity\nthat is your directory on sdfs file on\nsdfs and a file having multiple blocks\neach of these are considered as objects\nand for each object hadoop's name nodes\nram 150 bytes is utilized so if your\nblock size is very small then you would\nhave more number of blocks which would\ndirectly affect the name node's ram if\nyou keep a block size very high that\nwill reduce the number of blocks but\nremember that might affect in processing\nbecause processing also depends on\nsplits more number of splits more the\nparallel processing so\nsetting of block size has to be done\nwith consideration about your\nparallelism requirement and your name\nnodes ram which is available now coming\nto the question if you have a file of\n350 mb that would be split into three\nblocks and here two blocks would have\n128 mb data and the third block although\nthe block size would still be 128 it\nwould have only 94 mb of data so this\nwould be the split of this particular\nfile now let's\nunderstand about rack awareness how does\nrack awareness work or why do we even\nhave racks so organizations always would\nwant to place their nodes or machines in\na systematic way there can be different\napproaches you could have a rack which\nwould have machines running on the\nmaster processes and the intention would\nbe that this particular rack could have\nhigher bandwidth more cooling dedicated\npower supply top of rack switch and so\non the second approach could be that you\ncould have one master process running on\none machine of every rack and then you\ncould have other slave processes running\nnow when you talk about your rack\nawareness one thing to understand is\nthat if your machines are placed within\nracks and we are aware that hadoop\nfollows auto replication the rule of\nreplication in a rack of air cluster\nwould be that you would never have all\nthe replicas placed on the same rack so\nif we look at this if we have block a in\nblue color you will never have all the\nthree blue boxes in the same rack even\nif they are on different nodes because\nthat makes us that makes it less fault\ntolerant so you would have at least one\ncopy of block which would be stored on a\ndifferent track on a different note now\nlet's look at this so basically here we\nare talking about replicas being placed\nin such a way now somebody could ask a\nquestion can i have my block and its\nreplicas spread across three lakhs and\nyes you can do that but then in order to\nmake it more redundant you are\nincreasing your bandwidth requirement so\nthe better approach would be two blocks\non the same rack on different machines\nand one copy on a different track now\nlet's proceed how can you restart name\nnode and all demons in hadoop so if you\nwere working on an apache hadoop cluster\nthen you could be doing a start and stop\nusing hadoop demon scripts so there are\nthese hadoop demon scripts which would\nbe used to start and stop your hadoop\nand this is when you talk about your\napache hadoop so let's look at one\nparticular file which i would like to\nshow you more information here and this\ntalks about your different clusters so\nlet's look into this and\nso let's look at the start and stop and\nhere i have a file let's look at this\none and this gives you highlight so if\nyou talk about apache hadoop this is how\nthe setup would be done so you would\nhave it download the adobe tar file you\nwould have to untie it edit the config\nfiles you would have to do formatting\nand then start your cluster and here i\nhave said using scripts so this is in\ncase of apache hadoop you could be using\na start all script that internally\ntriggers start dfs and start yarn and\nthese scripts start dfs internally would\nrun hadoop demon multiple times based on\nyour configs to start your different\nprocesses then your start yarn would run\nyarn demon script to start your\nprocessing related processes so this is\nhow it happens in apache hadoop now in\ncase of cloud era or hortonworks which\nis basically a vendor specific\ndistribution you would have say multiple\nservices which would have one or\nmultiple demons running across the\nmachines let's take an example here that\nyou would have machine 1 machine 2 and\nmachine 3 with your processor spread\nacross however in case of cloud era and\nhortonworks these are cluster management\nsolutions so you would never be involved\nin running a script individually to\nstart and stop your processes in fact in\ncase of cloud error you would have a\nclouded scm server running on one of the\nmachines and then clouded scm agents\nrunning on every machine if you talk\nabout hortonworks you would have ambari\nserver an ambari agent running so your\nagents which are running on every\nmachine are responsible to monitor the\nprocesses send also their heartbeat to\nthe master that is your server and your\nserver is the one or a service which\nbasically will give instructions to the\nagents so in case of vendor specific\ndistribution your start and stop of\nprocesses is automatically taken care by\nthese underlying services and these\nservices internally are still running\nthese commands however only in apache\nhadoop you have to manually follow these\nto start and stop coming back\nwe can look into\nsome command related questions so which\ncommand will help you find the status of\nblocks and file system health so you can\nalways go for a file system check\ncommand now that can show you the files\nfor a particular sdfs path it can show\nyou the blocks and it can also give you\ninformation on\nstatus such as under replicated blocks\nover replicated blocks misreplicated\nblocks default replication and so on so\nyour fsck file system check utility does\nnot repair if there is any problem with\nthe blocks but it can give you\ninformation of\nblocks related to the files on which\nmachines they are stored if they are\nreplicated as per the replication factor\nor if there is any problem with any\nparticular replica now what would happen\nif you store too many small files in a\ncluster and this relates to the block\ninformation which i gave some time back\nso remember hadoop is coded in java so\nhere every directory every file and file\nrelated block is considered as an object\nand for every object within your hadoop\ncluster name nodes ram gets utilized so\nmore number of blocks you have more\nwould be usage of name nodes ram and if\nyou're storing too many small files it\nwould not affect your disk it would\ndirectly affect your name nodes ram\nthat's why in production clusters admin\nguys or infrastructure specialist will\ntake care that everyone who is writing\ndata to hdfs follows a quota system so\nthat you could be controlled in the\namount of data you write plus the count\nof data and individual rights on hdfs\nnow how do you copy data from local\nsystem onto sdfs so you can use a put\ncommand or a copy from local and then\ngiven your local path which is your\nsource and then your destination which\nis your sdfs path remember you can\nalways do a copy from local using a\nminus f option that's a flag option and\nthat also helps you in writing the same\nfile or a new file to hdfs so with your\nminus f you have a chance of overwriting\nor rewriting the data which is existing\non sdfs so copy from local or minus put\nboth of them do the same thing and you\ncan also pass an argument when you're\ncopying to control your replication or\nother aspects of your file now when do\nyou use dfs admin refresh nodes or rm\nadmin refresh nodes so as the command\nsays this is basically to do with\nrefreshing the node information so your\nrefresh nodes is mainly used when say a\ncommissioning or decommissioning of\nnodes is done so when a node is added\ninto the cluster or when a node is\nremoved from the cluster you are\nactually informing hadoop master that\nthis particular node would not be used\nfor storage and would not be used for\nprocessing now in that case you would be\nonce you are done with the process of\ncommissioning or decommissioning you\nwould be giving these commands that is\nrefresh nodes and rm admin refresh nodes\nso internally when you talk about\ncommissioning decommissioning there are\ninclude and exclude files which are\nupdated and these include and exclude\nfiles will have entry of machines which\nare being added to the cluster or\nmachines which are being removed from\nthe cluster and while this is being done\nthe cluster is still running so you do\nnot have to restart your master process\nhowever you can just use these refresh\ncommands to take care of your\ncommissioning decommissioning activities\nnow is there any way to change\nreplication of files on sdfs after they\nare already written and the answer is of\ncourse yes so if you would want to set a\nreplication factor at a cluster level\nand if you have admin access then you\ncould edit your sdfs hyphen site file or\nyou could say hadoop hyphen site file\nand that would take care of replication\nfactor being set at a cluster level\nhowever if you would want to change the\nreplication after the data has been\nwritten you could always use a setrep\ncommand so setrep command is basically\nto change the replication after the data\nis written\nyou could also\nwrite the data with a different\nreplication and for that you could use a\nminus d dfs dot replication and give\nyour replication factor when you are\nwriting data to the cluster so in hadoop\nyou can let your data be replicated as\nper the property set in the config file\nyou could write the data with a\ndifferent replication\nyou could change the replication after\nthe data is written so all these options\nare available now who takes care of\nreplication consistency in a hadoop\ncluster and what do you mean by under\nover replicated blocks now as i\nmentioned your fsck command can give you\ninformation of over or under replicated\nblocks now in a cluster it is always an\nalways name node which takes care of\nreplication consistency so for example\nif you have set up a replication of\nthree and since we know the first rule\nof replication which basically means\nthat you cannot have two replicas\nresiding on the same node it would mean\nthat if your replication is three you\nwould need at least three data nodes\navailable now say for example you had a\ncluster with three nodes and replication\nwas set to three at one point of time\none of your name node crashed and if\nthat happens your blocks would be under\nreplicated that means there was a\nreplication factor set but now your\nblocks are not replicated or there are\nnot enough replicas as per the\nreplication factor set this is not a\nproblem your master process or name node\nwill wait for some time before it will\nstart the replication of data again so\nif a data road is not responding or if a\ndisk has crashed and if name node does\nnot get information of a replica name\nnode will wait for some time and then it\nwill start re-replication of those\nmissing blocks from the available nodes\nhowever while name node is doing it the\nblocks are in under replicated situation\nnow when you talk about over replicated\nthis is a situation where name node\nrealizes that there are extra copies of\nblock now this might be the case that\nyou had three nodes running with a\nreplication of three one of the node\nwent down due to a network failure or\nsome other issue within few minutes name\nnode re-replicated the data and then the\nfailed node is back with its set of\nblocks again name node is smart enough\nto understand that this is over\nreplication situation and it will delete\nset of blocks from one of the nodes it\nmight be the node which has been\nrecently added it might be your old node\nwhich has joined your cluster again or\nany node that depends on the load on a\nparticular node now we discussed about\nhadoop we discussed about sdfs now we\nwill discuss about mapreduce which is\nthe programming model and you can say\nprocessing framework what is distributed\ncache in mapreduce now we know that when\nwe talk about mapreduce the data which\nhas to be processed might be existing on\nmultiple nodes so when you would have\nyour mapreduce program running it would\nbasically read the data from the\nunderlying disks\nnow this could be a costly operation if\nevery time the data has to be read from\ndisk\nso distributed cache is a mechanism\nwherein\ndata set or data which is coming from\nthe disk can be cached\nand available for all worker nodes now\nhow will this benefit so when a map\nreduce is running instead of every time\nreading the data from disk it would pick\nup the data from distributed cache and\nthis this will benefit your mapreduce\nprocessing so distributed cache can be\nset in your jobconf where you can\nspecify that a file should be picked up\nfrom distributed cache\nnow let's understand about these roles\nso what is a record reader what is a\ncombiner what is a partitioner and what\nkind of roles do they play in a map\nreduce processing paradigm or map reduce\noperation so record reader communicates\nwith the input split and it basically\nconverts the data into key value pairs\nand these key value pairs are the ones\nwhich will be worked upon by the mapper\nyour combiner is an optional face it's\nlike mini radius so combiner does not\nhave its own class\nit relies on the reducer class\nbasically your combiner would receive\nthe data from your map tasks\nwhich would have completed works on it\nbased on whatever reducer class mentions\nand then passes its output to the\nreducer phase\npartitioner is basically a phase which\ndecides how many reduced tasks would be\nused aggregate or summarize your data so\npartitioner is a phase which would\ndecide based on the number of keys based\non the number of\nmap tasks your partitioner would decide\nif one or multiple reduced tasks would\nbe used to take care of processing so\neither it could be partitioner which\ndecides on how many reduced tasks would\nrun or it could be based on the\nproperties which we have set within the\ncluster which will take care of the\nnumber of reduced tasks which would be\nused always remember your partitioner\ndecides how outputs from combiner are\nsent to reducer and to how many reducers\nit controls the partitioning of keys of\nyour intermediate map outputs so map\nphase whatever output it generates is an\nintermediate output and that\nhas to be taken by your practitioner or\nby a combiner and then partitioner to be\nsent to one or multiple reduced tasks\nthis is one of the common questions\nwhich you might face\nwhy is mapreduce lower in processing so\nwe know mapreduce goes for parallel\nprocessing we know we can have multiple\nmap tasks running on multiple nodes at\nthe same time we also know that multiple\nreduced tasks could be running now why\ndoes then mapreduce become a slower\napproach\nfirst of all your mapreduce is a batch\noriented operation now mapreduce\nis very rigid and it strictly uses\nmapping and reducing phases so no matter\nwhat kind of processing you would want\nto do you would have to still provide\nthe mapper function and the reducer\nfunction to work on data not only this\nwhenever your map phase completes the\noutput of your map face which is an\nintermittent output would be written to\nhdfs and thereafter underlying disks and\nthis data would then be shuffled and\nsorted and picked up for reducing phase\nso every time your data being written to\nhdfs and retrieved from sdfs makes\nmapreduce a slower approach the question\nis for a mapreduce job is it possible to\nchange the number of mappers to be\ncreated\nnow\nby default you cannot change the number\nof map tasks because number of map tasks\ndepends on the input splits however\nthere are different ways in which you\ncan either set a property to have more\nnumber of map tasks which can be used or\nyou can customize your code or\nmake it use a different format which can\nthen control the number of map tasks\nby default number of map tasks\nare equal to the number of splits of\nfile you are processing\nso if you have a 1gb of file that is\nsplit into 8 blocks of 128 mb there\nwould be 8 map tasks running on the\ncluster these map tasks are basically\nrunning your mapper function\nif you have hard coded properties in\nyour map red hyphen site file to specify\nmore number of map tasks then you could\ncontrol the number of map tasks\nlet's also talk about some data types so\nwhen you prepare for hadoop when you\nwant to get into big data field you\nshould start learning about different\ndata formats now there are different\ndata formats such as\navro parquet\nyou have a sequence file or binary\nformat and these are different formats\nwhich are used now when you talk about\nyour data types\nin hadoop these are implementation of\nyour writable and writable comparable\ninterfaces so for every data type\nin java you have a equivalent in hadoop\nso ind in java would be intriguable in\nhadoop float would be float writable\nlong would be long writable double\nwritable boolean writable array writable\nmap writable and object credible so\nthese are your different data types that\ncould be used within your mapreduce\nprogram and these are implementation of\nwritable and writable comparable\ninterfaces what is speculative execution\nnow imagine you have a cluster which has\nhuge number of nodes and your data is\nspread across multiple slave machines or\nmultiple nodes now at a point of time\ndue to a disk degrade or network issues\nor machine heating up or more load being\non a particular node there can be a\nsituation where your data node will\nexecute task in a slower manner now in\nthis case if speculative execution is\nturned on there would be a shadow task\nor a another\nsimilar task running on some other node\nfor the same processing so whichever\ntask finishes first will be accepted and\nthe other task would be killed so\nspeculative execution could be good if\nyou are working in an intensive workload\nkind of environment where if a\nparticular node is slower you could\nbenefit from a unoccupied or a node\nwhich has less load to take care of your\nprocessing going further this is how we\ncan understand so node a which might be\nhaving a slower task you would have a\nscheduler which is maintaining or\nhaving knowledge of what are the\nresources available so if speculative\nexecution as a property is turned on\nthen the task which was running slow a\ncopy of that task or you can say shadow\ntask would run on some other node and\nwhichever task completes first will be\nconsidered this is what happens in your\nspeculative execution now how is\nidentity mapper different from chain\nmapper now this is where we are getting\ndeeper into mapreduce concepts so when\nyou talk about mapper identity mapper is\nthe default mapper which is chosen when\nno mapper is specified in mapreduce\ndriver class so for every mapreduce\nprogram you would have a map class which\nis taking care of your mapping phase\nwhich basically has a mapper function\nand which would run one or multiple map\ntasks right your programming your\nprogram would also have a reduce class\nwhich would be running a reducer\nfunction which takes care of reduced\ntasks running on multiple nodes\nnow if a mapper is not specified within\nyour driver class so driver class is\nsomething which has all information\nabout your flow what's your map class\nwhat is your reduced class what's your\ninput format what's your output format\nwhat are the job configurations and so\non so identity mapper is the default\nmapper which is chosen when no mapper\nclass is mentioned in your driver class\nit basically implements an identity\nfunction which directly writes all its\nkey pairs into output and it was defined\nin old mapreduce api in this particular\npackage but when you talk about chaining\nmappers or chain mapper this is\nbasically a class to run multiple\nmappers in a single map task or\nbasically you could say multiple map\ntasks would run as a part of your\nprocessing the output of first mapper\nwould become as an input to second\nmapper and so on and this can be defined\nin the under mentioned class or package\nwhat are the major configuration\nparameters required in a mapreduce\nprogram obviously we need to have the\ninput location we need to have the\noutput location so input location is\nwhere the files will be picked up from\nand this would preferably on sdfs\ndirectory output location is the path\nwhere your job output would be written\nby your mapreduce program you also need\nto specify input and output formats if\nyou don't specify the defaults are\nconsidered then we need to also have the\nclasses which have your map and reduce\nfunctions and if you intend to run the\ncode on a cluster you need to package\nyour class in a jar file export it to\nyour cluster and then this jar file\nwould have your mapper reducer and\ndriver classes so these are important\nconfiguration parameters\nwhich you need to consider for a map\nreduce program now what is the\ndifference or what do you mean by map\nside join and reduce side join map side\njoin is basically when the join is\nperformed at the mapping level or at the\nmapping phase or is performed by the\nmapper so each input data which is being\nworked upon has to be divided into same\nnumber of partitions\ninput to each map is in the form of a\nstructured partition and is in sorted\norder so map site join you can\nunderstand it in a simpler way that if\nyou compare it with rdbms concepts where\nyou had two tables which were being\njoined it will always be advisable to\ngive your bigger table as the left side\ntable or the first table for your join\ncondition and it would be your smaller\ntable on the left side and your bigger\ntable on the right side which basically\nmeans the smaller table could be loaded\nin memory and could be used for joining\nso map site drawing is a similar kind of\nmechanism where input data is divided\ninto same number of partitions when you\ntalk about reduced side join here the\njoin is performed by the reducer so it\nis easier to implement than website join\nas all the sorting and shuffling will\nsend the values or send all the values\nhaving identical keys to the same\nreducer so you don't need to have your\ndata set in a structured form so look\ninto your map side join or reduce side\njoin and other joins just to understand\nhow mapreduce works however i would\nsuggest not to focus more on this\nbecause mapreduce is still being used\nfor processing but the amount of\nmapreduce based processing has decreased\noverall or across the industry now what\nis the role of output committer class in\na mapreduce job so output committer as\nthe name says describes the commit of\ntask output for a mapreduce job so we\ncould have this as mentioned or capacity\nhadoop mapreduce output committer you\ncould have a class which extends your\noutput committer class\nso mapreduce relies on this mapreduce\nrelies on the output committer of the\njob to\nset up the job initialization cleaning\nup the job after the job completion that\nmeans all the resources which were being\nused by a particular job setting up the\ntask temporary output checking whether a\ntask needs a commit committing the task\noutput and discarding the task out so\nthis is a very important class and can\nbe used within your mapreduce job what\nis the process of spilling in mapreduce\nwhat does that mean\nso spilling is basically a process of\ncopying the data from memory buffer to\ndisk when obviously the\nbuffer usage reaches a certain threshold\nso if there is not enough memory in your\nbuffer in your memory then the content\nwhich is stored in buffer or memory has\nto be flushed out so by default a\nbackground thread starts spilling the\ncontent from memory to disk after eighty\npercent of buffer size is filled now\nwhen is the buffer being used so when\nyour mapreduce processing is happening\nthe data from data is being read from\nthe disk loaded into the buffer and then\nsome processing happens\nsame thing also happens when you are\nwriting data to the cluster so you can\nimagine for a 100 megabyte size buffer\nthe spilling will start after the\ncontent of buffer reaches 80 megabytes\nthis is customizable how can you set the\nmappers and reducers for a mapreduce job\nso these are the properties so number of\nmappers and reducers as i mentioned\nearlier can be customized so by default\nyour number of map tasks depends on the\nsplit and number of reduced tasks\ndepends on the partitioning phase which\ndecides number of reduced tasks which\nwould be used depending on the keys\nhowever we can set these properties\neither in the config files or provide\nthem on the command line or also make\nthem part of our code and this can\ncontrol the number of map tasks or\nreduce tasks which would be run for a\nparticular job let's look at one more\ninteresting question what happens when a\nnode running a map task fails before\nsending the output to the reducer\nso there was a node which was running a\nmap task and we know that there could be\none or multiple map tasks running on one\nor multiple nodes and all the map tasks\nhave to be completed before the further\nstages that such as combiner or reducer\ncome into existence so in a case if a\nnode crashes where a map task was\nassigned to it the whole task will have\nto be run again on some other note so in\nhadoop version 2 yarn framework has a\ntemporary demon called application\nmaster so your application master is\ntaking care of execution of your\napplication\nand if a particular task on a particular\nnode failed due to unavailability of\nnode it is the role of application\nmaster to have this task scheduled on\nsome other node now can we write the\noutput of mapreduce in different formats\nof course we can so hadoop supports\nvarious input and output formats so you\ncan write the output of mapreduce in\ndifferent formats so you could have the\ndefault format that is text output\nformat wherein records are written as\nline of text you could have sequence\nfile which is basically to write\nsequence files or your binary format\nfiles where your output files need to be\nfed into another mapreduce jobs you\ncould go for a map file output format to\nwrite output as map files you could go\nfor a sequence file as a binary output\nformat so that's again a variant of your\nsequence file input format it basically\nwrites keys and values to a sequence\nfile so when we talk about binary format\nwe are talking about a non-human\nreadable format db output format now\nthis is basically used when you would\nwant to write data to say relational\ndatabases or say no sql databases such\nas hbase so this format also sends the\nreduce output to a sql table now let's\nlearn a little bit about yarn yarn which\nstands for yet another resource\nnegotiator it's the processing framework\nso what benefits did yan bring in hadoop\nversion 2 and how did it solve the\nissues of mapreduce version 1. so\nmapreduce version 1 had major issues\nwhen it comes to scalability or\navailability because sorry in hadoop\nversion 1 you had only one master\nprocess for processing layer and that is\nyour job tracker so your job tracker was\nlistening to all the task trackers which\nwere running on multiple machines so\nyour job tracker was responsible for\nresource tracking and job scheduling in\nyarn you still have a processing master\nbut that's called resource manager\ninstead of job tracker and now with\nhadoop version 2 you could even have\nresource manager running in high\navailability mode you have node managers\nwhich would be running on multiple\nmachines and then you have a temporary\ndemon called application master so in\ncase of hadoop version 2 your resource\nmanager or master is only handling the\nclient connections and taking care of\ntracking the resources the jobs\nscheduling or basically taking care of\nexecution across multiple nodes is\ncontrolled by application master till\nthe application completes\nso in yarn you can have different kind\nof resource allocations that could be\ndone and there is a concept of container\nso container is basically a combination\nof ram and cpu cores\nyarn can run different kind of workloads\nso it is not just mapreduce kind of\nworkload which can be run on adobe\nperson 2 but you would have graph\nprocessing massive parallel processing\nyou could have\nreal time processing and huge processing\napplications could run on a cluster\nbased on yarn so when we talk about\nscalability\nin case of your hadoop version 2 you can\nhave a cluster size of more than 10 000\nnodes and can run more than 100 000\nconcurrent tasks and this is because for\nevery application which is launched you\nhave this temporary demon called\napplication master so if i would have 10\napplications running i would have 10 app\nmasters running taking care of execution\nof these applications across multiple\nnodes compatibility so hadoop version 2\nis fully compatible with whatever was\ndeveloped as per hadoop version 1 and\nall your processing needs would be taken\ncare by yarn\nso dynamic allocation of cluster\nresources taking care of different\nworkloads\nallocating resources across multiple\nmachines and using them for execution\nall that is taken care by yarn multi\ntenancy which basically means you could\nhave multiple users or multiple teams\nyou could have open source and\nproprietary data access engines and all\nof these could be basically hosted using\nthe same cluster now how does yarn\nallocate resources to an application\nwith help of its architecture so\nbasically you have a client or an\napplication or an api which talks to\nresource manager resource manager is as\ni mentioned managing the resource\nallocation in the cluster when you talk\nabout resource manager you have its\ninternal two components one is your\nscheduler and one is your applications\nmanager so when we say resource manager\nbeing the master is tracking the\nresources the source manager is the one\nwhich is negotiating the resources with\nslave it is not actually resource\nmanager who is doing it but these\ninternal components\nso you have a scheduler which allocates\nresources to various running\napplications so scheduler is not\nbothered about tracking your resources\nor basically tracking your applications\nso we can have different kind of\nschedulers such as fifo which is first\nin first out you could have a fair\nscheduler or you could have a capacity\nscheduler and these schedulers basically\ncontrol how resources are allocated to\nmultiple applications when they are\nrunning in parallel so there is a queue\nmechanism so scheduler will schedule\nresources based on requirements of\napplication but it is not monitoring or\ntracking the status of applications\nyour applications manager is the one\nwhich is accepting the job submissions\nit is monitoring and restarting the\napplication masters so its application\nmanager which is basically then\nlaunching a application master which is\nresponsible for an application so this\nis how it looks\nso whenever a job submission happens we\nalready know that resource manager is\naware of the resources which are\navailable with every node manager so on\nevery node which has fixed amount of ram\nand cpu cores\nsome portion of resources that is your\nram and cpu cores are allocated to node\nmanager now resource manager is already\naware of how much resources are\navailable across nodes so whenever a\nclient request comes in resource manager\nwill make a request to node manager it\nwill basically request node manager to\nhold some resources for processing node\nmanager would basically approve or\ndisapprove this request of holding the\nsources\nand these resources that is a\ncombination of ram and cpu cores are\nnothing but containers we can allocate\ncontainers of different sizes within\nyarn hyphen site file so your node\nmanager based on a request from resource\nmanager guarantees the container which\nwould be available for processing that's\nwhen your resource manager starts a\ntemporary demon called application\nmaster to take care of your execution so\nyour app master which was launched by\nresource manager or we can say internal\naway applications manager will run in\none of the containers because\napplication master is also a piece of\ncode so it will run in one of the\ncontainers and then other containers\nwill be utilized for execution this is\nhow yarn is basically taking care of\nyour allocation your application master\nis managing resource needs it is the one\nwhich is interacting with scheduler and\nif\na particular\nnode crashes it is the responsibility of\napp master to go back to the master\nwhich is resource manager and negotiate\nfor more resources\nso your app master will never ever\nnegotiate resources with node manager\ndirectly it will always talk to resource\nmanager and the source manager is the\none which negotiates the resources\ncontainer as i said is a collection of\nresources like your ram cpu network\nbandwidth and your container is\nallocated based on the availability of\nresources on a particular node so which\nof the following has occupied the place\nof a job tracker of mapreduce\nso it is your resource manager so\nresource manager is the name of the\nmaster process in adobe portion 2. now\nif you would have to\nwrite yarn commands to check the status\nof an application so we could just say\nyarn application minus status and then\nthe application id and you could kill it\nalso from the command line remember your\nyarn has a ui and you can even look at\nyour applications from the ui you can\neven kill your applications from the ui\nhowever knowing the command line\ncommands would be very useful can we\nhave more than one resource manager in a\nyoung base cluster\nyes we can that is what hadoop version 2\nallows us to have so you can have a high\navailability yarn cluster where you have\na active and standby and the\ncoordination is taking care by your\nzookeeper at a particular time there can\nonly be one active resource manager and\nif active resource manager fails your\nstandby resource manager comes\nand becomes active however zookeeper is\nplaying a very important role remember\nzookeeper is the one which is\ncoordinating the server state and it is\ndoing the election of active to standby\nfailover what are the different\nschedulers available in yarn so you have\na fifo scheduler that is\nfirst in first out and this is not a\ndesirable option because in this case a\nlonger running application might block\nall other small running applications\nyour capacity scheduler is basically a\nscheduler where dedicated queues are\ncreated and they have fixed amount of\nresources so you can have multiple\napplications accessing the cluster at\nthe same time and they would be using\ntheir own cues\nand the resources allocated to them if\nyou talk about fair scheduler you don't\nneed to have a fixed amount of\nresources you can just have a percentage\nand you could decide what kind of\nfairness is to be followed which\nbasically means that if you were\nallocated 20 gigabytes of memory however\nthe cluster has 100 gigabytes and the\nother team was assigned 80 gigabytes of\nmemory then you have 20 percent access\nto the cluster another team has 80\npercent however if the other team does\nnot come up or does not use the cluster\nin a fair scheduler you can go up to\nmaximum 100\nof your cluster to find out more\ninformation about your schedulers you\ncould either look in hadoop definitive\nguide or what you could do is you could\njust go to google and you could type for\nexample\nyarn scheduler\nlet's search for yarn scheduler and then\nyou can look in\nhadoop definitive guide\nand so this is your hadoop definitive\nguide and it beautifully explains about\nyour different schedulers how do\nmultiple applications run and that could\nbe in your\nfifo kind of scheduling it could be in\ncapacity scheduler or it could be in a\nfair scheduling so have a look at this\nlink it's a very good link you can also\nsearch for yarn untangling and this is a\nblog of four or this is a series of four\nblocks where it's beautifully explained\nabout your yarn how it works how the\nresource allocation happens what is a\ncontainer and what runs within the\ncontainer so you can scroll down you can\nbe reading through this and you can then\nalso search for part two of it which\ntalks about allocation and so on so\ncoming back we basically have these\nschedulers what happens if a resource\nmanager fails while executing an\napplication in a high availability\ncluster so in a high availability\ncluster we know that we would have two\nresource managers one being active one\nbeing standby and zookeeper which is\nkeeping a track of the server states so\nif a rm fails in case of high\navailability the\nstandby will be elected as active and\nthen basically your resource manager or\nthe standby would become the active one\nand this one would instruct the\napplication master to abort in the\nbeginning then your resource manager\nrecovers its running state so there is\nsomething called as rm state store where\nall the applications which are running\ntheir status is stored so resource\nmanager recovers\nits running state by looking at your\nstate store\nby taking advantage of container\nstatuses and then continues to take care\nof your processing now in a cluster of\n10 data nodes each having 16 gb and 10\ncores what would be total processing\ncapacity of the cluster take a minute to\nthink 10 data nodes 16 gb ram per node\n10 cores\nso if you mention the answer as 160 gb\nram\nand 100 cores then\nyou went wrong now think of a cluster\nwhich has 10 data nodes each having 16\ngb ram and 10 cores remember on every\nnode in a hadoop cluster you would have\none or multiple processors running those\nprocesses would need ram the machine\nitself which has a linux file system\nwould have its own processes so that\nwould also be having some ram usage\nwhich basically means that if you talk\nabout 10 data nodes you should deduct at\nleast 20 to 30 percent towards the\noverheads towards the cloud database\nservices towards the other processes\nwhich are running and in that case i\ncould say that you could have 11 or 12\ngb available on every machine for\nprocessing and say 6 or 7 cores multiply\nthat by 10 and that's your processing\ncapacity remember the same thing applies\nto the disk usage also so if somebody\nasks you in a 10 data node cluster where\neach machine has 20 terabytes of disks\nwhat is my total storage capacity\navailable for sdfs so the answer would\nnot be 200 you have to consider the\noverheads and this is basically which\ngives you your processing capacity now\nlet's look at one more question so what\nhappens if requested memory or cpu cores\nbeyond or goes beyond the size of\ncontainer now as i said you can have\nyour configurations which can say that\nin a particular data node which has 100\ngb ram i could allocate say 50 gb\nfor the processing like out of 100 cores\ni could say 50 cores for processing so\nif you have 100 gb ram and 100 cores you\ncould ideally allocate 100 for\nprocessing but that's not ideally\npossible so if you have 100 gb ram you\nwould go for 50 gb and if you have 100\ncores you would go for 50 cores now\nwithin this ram and cpu course you have\nthe concept of containers right so\ncontainer is a combination of ram and\ncpu cores so you could have a minimum\nsize container and maximum size\ncontainer now at any point of time if\nyour application starts demanding\nmore memory or more cpu cores\nand this cannot fit into a container\nlocation your application will fail your\napplication will fail because you\nrequested for a memory or a combination\nof memory and cpu cores which is\nmore than the maximum container size so\nlook into this yarn tangling website\nwhich i mentioned and look for the\nsecond blog in those series which\nexplains about these allocations now\nhere we will discuss on hive peg hbase\nand these components of do which are\nbeing used in the industry for various\nuse cases let's look at some questions\nhere and let's look how you should\nprepare for them so first of all we will\nlearn on hive which is a data\nwarehousing package so the question is\nwhat are the different components of a\nhive architecture now when we talk about\nhive we already know that hive is a data\nwarehousing package which basically\nallows you to work on structured data or\ndata which can be structuralized so\nnormally people are well versed with\nquerying or basically processing the\ndata using sql queries a lot of people\ncome from database backgrounds and they\nwould find it comfortable if they know\nstructured query language hive is one of\nthe data warehousing package which\nresides within a hadoop ecosystem it\nuses hadoop's distributed file system to\nstore the data and it uses rdbms usually\nto store the metadata although metadata\ncan be stored locally also so what are\nthe different components of an hive\narchitecture so it has a user interface\nso user interface calls the execute\ninterface to the driver this creates a\nsession to the query and then it sends\nthe query to the compiler to generate an\nexecution plan for it usually whenever\nhive is set up it would have its\nmetadata stored in an rdbms now to\nestablish the connection between rdbms\nand hadoop we need odbc or jdbc\nconnector jar file and that connector\njar file has a driver class now this\ndriver class is mandatory to create a\nconnection between hive and hadoop so\nuser interface creates this interface\nusing the driver now we have metastore\nmetastore stores the metadata\ninformation so any object which you\ncreate such as database table indexes\ntheir metadata is stored in metastore\nand usually this meta store is stored in\nan rdbms so that multiple users can\nconnect to hive so your metastore stores\nthe metadata information and sends that\nto the compiler for execution of a query\nwhat does the compiler do it generates\nthe execution plan it has a dag now dag\nstands for direct cyclic graph so it has\na dag of stages where each stage is\neither a metadata operation a map or\nreduced job or an operation on sdfs and\nfinally we have execution engine that\nacts as a bridge between hive and hadoop\nto process the query so execution engine\ncommunicates bi-directionally with\nmetastore to perform operations like\ncreate or drop tables so these are four\nimportant components of hive\narchitecture\nnow what is the difference between\nexternal table and manage table and\nheight\nso we have various kinds of table in\nheight such as external table manage\ntable partition table the major\ndifference between your managed and\nexternal table is in respect to what\nhappens to the data if the table is\ndropped usually whenever we create a\ntable in hive it creates a managed table\nor we could also call that as an\ninternal table now this manages the data\nand moves it into warehouse directory by\ndefault whether you create a manage\ntable or external table usually the data\ncan reside in hive's default warehouse\ndirectory or it could be residing in a\nlocation chosen however when we talk\nabout manage table if one drops a\nmanaged table not only the metadata\ninformation is deleted but the table's\ndata is also deleted from his dfs if we\ntalk about external table it is created\nwith an external keyword explicitly and\nif an external table is dropped nothing\nhappens to the data which resides in\nsdfs so that's the main difference\nbetween your managed and external table\nwhat might be the use case if somebody\nasks you there might be a migration kind\nof activity or you are interested in\ncreating a lot of tables\nusing your queries so in that case you\ncould dump all the data on sdfs and then\nyou could create a table by pointing to\na particular directory or multiple\ndirectories now you could then do some\ntesting of your tables and would decide\nthat you might not need all the tables\nso in that case it would be advisable to\ncreate external tables so that even if\nthe table is later dropped the data on\nsdfs will be intact unlike your manage\ntable where dropping of table will\ndelete the data from sdfs also let's\nlearn a little bit on partition so what\nis partition and height and why is\npartitioning required in hive if\nsomebody asks you that now normally in\nworld of rdbms partition is the process\nto group similar type of data together\nand that is usually done on basis of a\ncolumn or what we call as partitioning\nkey now each table usually has one\ncolumn in context of rdbms which could\nbe used to partition the data and why do\nwe do that so that we can avoid scanning\nthe complete table for a query and\nrestrict the scan to\nset of data or to a particular partition\nin hive we can have any number of\npartition keys so partitioning provides\ngranularity in hive table it reduces the\nquery latency by scanning only relevant\npartition data instead of whole data set\nwe can partition at various levels now\nif i compare rdbms with hive in case of\nrdbms you could have\none column which could be used for\npartitioning and then you could be\nsquaring the specific partition so in\ncase of rdbms your partition column is\nusually a part of the table definition\nso for example if i have an employee\ntable i might have employee id employee\nname employee age and employee salary as\nfour columns and i would decide to\npartition the table based on salary\ncolumn now why would i partition it\nbecause i feel that employee table is\ngrowing very fast it is or it will have\nhuge amount of data and later when we\nquery the table we don't want to scan\nthe complete table so i could split my\ndata into multiple partition based on a\nsalary column giving some ranges in hive\nit is a little different\nin hive you can do partitioning and\nthere is a concept of static and dynamic\npartitioning but in hive the partition\ncolumn is not part of table definition\nso you might have an employee table with\nemployee id\nname age\nand that that's it that would be the\ntable definition but you could then have\npartitioning done based on salary column\nwhich will then create a specific folder\non sdfs in that case when we query the\ndata we can see the partition column\nalso showing up so we can partition the\ntransaction data for a bank for example\nbased on month like chan feb etc and any\noperation regarding a particular month\nwill then allow us to query that\nparticular folder that is where\npartitioning is useful now why does hive\nnot store metadata information in sdfs\nif somebody asks you so we know that\nhives data is stored in sdfs which is\nhadoop distributed file system however\nthe metadata is either stored locally\nand that mode of hive would be called as\nembedded mode or you could have hives\nmetadata stored in rdbms so that\nmultiple clients can initiate a\nconnection now this metadata which is\nvery important for hive would not be\nstored in sdfs so we already know that\nsdfs read and write operations are time\nconsuming it is a distributed file\nsystem and it can accommodate huge\namount of data so hive stores metadata\ninformation in metastore using rdbms\ninstead of sdfs so this allows to\nachieve low latency and faster data\naccess\nnow if somebody asks what are the\ncomponents used in hive query processor\nso usually we have the main components\nare your parser your execution engine\nlogical plan generation optimizer and\ntype checking so whenever a query is\nsubmitted it will go through a parser\nand parser would check the syntax it\nwould check for objects which are being\nqueried and other things to see if the\nquery is fine now internally you have a\nsemantic analyzer which will also look\nat the query you have an execution\nengine which basically will work on the\nexecution part that is the best\ngenerated execution plan which could be\nused to get the results for the query\nyou could also have user defined\nfunctions which a user would want to use\nand these are normally created in\njava or java programming language and\nthen basically these user defined\nfunctions are added to the class path\nnow you would have a logical plan\ngeneration which basically looks at your\nquery and then generates a logical plan\nor the best execution path which would\nbe required to get to the results\ninternally there is a physical plan\ngenerated which is then looked in by\noptimizer to get the best path to get to\nthe data and that might also be checking\nyour different operators which you are\nusing within your query finally we would\nalso have type checking so these are\nimportant components in hype so somebody\nmight ask you if you are querying your\ndata using hive what are the different\ncomponents involved or if you could\nexplain what are the different\ncomponents which\nwork\nwhen a query is submitted so these are\nthe components now let's look a scenario\nbased question suppose there are a lot\nof small csv files which are present in\na sdfs directory and you want to create\na single hive table from these files so\ndata in these files have fields like\nregistration number name email address\nso if this is what needs to be done what\nwill be your approach to solve it where\nwill you create a single hive table for\nlots of small files without degrading\nthe performance of the system so there\ncan be different approaches now we know\nthat there are a lot of small csv files\nwhich are present in a directory so we\nknow that when we create a table in hive\nwe can use a location parameter so i\ncould say create table give a table name\ngive the column and their data types i\ncould specify the delimiters and finally\ni could say location and then point it\nto a directory on sdfs and this\ndirectory might be the directory which\nhas lot of csv files so in this case i\nwill avoid loading the data in the table\nbecause table being point table pointing\nto the directory will directly pick up\nthe data from one or multiple files and\nwe also know that hive does schema check\non read so it does not do a schema check\non write so in case there were one or\ntwo files which did not follow the\nschema of the table it would not prevent\ndata loading data would anyways be\nloaded only when you query the data it\nmight show you null values if data which\nwas loaded does not follow the schema of\nthe table this is one approach what is\nthe other approach so let's look at that\nyou can think about sequence file format\nwhich is basically a smart format or a\nbinary format and you can group these\nsmall files together to form a sequence\nfile now this could be one other smarter\napproach so we could create a temporary\ntable so we could say create table give\na table name give the column names and\ntheir data types we could specify the\ndelimiters as it shows here that is row\nformat and fields terminated by and\nfinally we can store that as text file\nthen we can load data into this table by\ngiving a local file system path and then\nwe can create a table that will store\ndata in sequence file format so my point\none is storing the data in text text\nfile point three would be storing the\ndata in sequence file format so we say\ncreate table give the specifications we\nsay row format delimited fields are\nterminated by comma stored as sequence\nfile then we can move the data from test\ntable into test sequence file table so i\ncould just say insert overwrite my new\ntable as select star from other table\nremember in hive you cannot do insert\nupdate delete however if the table is\nexisting you can do a insert overwrite\nfrom an existing table into a new table\nso this could be one approach where we\ncould have lot of csv files or smaller\nfiles club together as one big sequence\nfile and then store it in the table now\nif somebody asks you write a query to\ninsert a new column that is\ninteger data type into a hive table and\nthe requirement might be that you would\nwant to insert this table at a position\nbefore an existing column now that's\npossible by doing an alter table giving\nyour table name and then specifying\nchange column giving you a new column\nwith the data type before an existing\ncolumn this is a simple way wherein you\ncan insert a new column into a hive\ntable what are the key differences\nbetween hive and pig\nnow some of you might have heard high\nvisit data where housing package and pig\nis more of a scripting language\nboth of them are used for data analysis\nor trend detection hypothesis testing\ndata transformation and many other use\ncases so if we compare hive and pig hive\nuses a declarative language called hive\nql that is hive querying language\nsimilar to sql and it is for reporting\nor for data analysis even for data\ntransformation or for your data\nextraction pig uses a high level\nprocedural language called pig latin for\nprogramming both of them remember use\nmapreduce processing framework so when\nwe run a query in hive to process the\ndata or when we create and submit a pick\nscript both of them trigger a mapreduce\njob unless and until we have set them to\nlocal mode hive operates on the server\nside of the cluster and basically works\non structured data or data which can be\nstructuralized pig usually works or\noperates on the client side of the\ncluster and allows both structured\nunstructured or even i could say\nsemi-structured data hive does not\nsupport avro file format by default\nhowever that can be done by using the\nwrite serializer d serializer so we can\nhave hive table related data stored in\navro format in sequence file format in\npart k format or even as a text file\nformat however when we are working on\nsmarter formats like avro or sequence\nfile or par k we might have to use\nspecific serializers d serializers for\navro this is the package which allows us\nto use avro format pig supports avro\nformat by default hive was developed by\nfacebook and it supports partitioning\nand pig was developed by yahoo and it\ndoes not support partitioning so these\nare high level differences there are\nlots and lots of differences remember\nhive is more of a data housing package\nand pig is more of a scripting language\nor a\nstrictly procedural flow following\nscripting language which allows us to\nprocess the data now let's get more and\nlet's get more deeper and learn about\npig which is as i mentioned a scripting\nlanguage which can be used for your data\nprocessing it also uses mapreduce\nalthough we can even have pig run in a\nlocal mode let's learn about pig in the\nnext section now let's learn on some\nquestions about pig which is a scripting\nlanguage and it is extensively used for\ndata processing and data analysis so the\nquestion is how is apache pig different\nfrom mapreduce now we all know that\nmapreduce is a programming model it is\nit's quite rigid when it comes to\nprocessing the data because you have to\ndo the mapping and reducing you have to\nwrite huge code usually mapreduce is\nwritten in java but now it can also be\nwritten in python it can be written in\nscala another programming languages so\nif we compare pig with mapreduce pig\nobviously is very concise it has less\nlines of code when compared to mapreduce\nnow we also know that pig script\ninternally will trigger a mapreduce job\nhowever user need not know about\nmapreduce programming model they can\nsimply write simple scripts in pig and\nthat will automatically be converted\ninto mapreduce however mapreduce has\nmore lines of code peak is high level\nlanguage which can easily perform join\noperations or other data processing\noperations mapreduce is a low level\nlanguage which cannot perform job join\noperations easily so we can do join\nusing mapreduce however it's not really\neasy in comparison to pick now as i said\non execution every pig operator is\nconverted internally into a mapreduce\njob so every pick script which is run\nwhich would be converted into mapreduce\njob now map reduce overall is a batch\noriented processing so it takes more\ntime to compile it takes more time to\nexecute either when you run a mapreduce\njob or when it is triggered by\npingscript pig works with all versions\nof hadoop and when we talk about\nmapreduce program which is written in\none hadoop version may not work with\nother versions it might work or it might\nnot it depends on what are the\ndependencies what is the compiler you\nare using what programming language you\nhave used and what version of hadoop you\nare working on so these are the main\ndifferences between apache pig and\nmapreduce what are the different ways of\nexecuting pig script so you could create\na script file store it in dot pic or dot\ntext and then you could execute it using\nthe pic command you could be bringing up\nthe grunt shell that is pig's shell now\nthat usually starts with mapreduce mode\nbut then we can also bring it up in a\nlocal mode and we can also run pic\nembedded as an embedded script in other\nprogramming language so these are the\ndifferent ways of executing your big\nscript now what are the major components\nof pig execution environment this is\nthis is a very common question\ninterviewers would always want to know\ndifferent components of hive different\ncomponents of pig even different\ncomponents which are involved in hadoop\necosystem so when we want to learn about\nmajor components of big execution\nenvironment here are some so you have\npig scripts now that is written in pig\nlatin using built-in operators and\nuser-defined functions and submitted to\nthe execution environment that's what\nhappens when you would want to process\nthe data using pic now there is a parser\nwhich does type checking and checks the\nsyntax of the script the output of\nparser is a tag direct cyclic graph so\nblock in wikipedia for dag so dag is\nbasically a sequence of steps which run\nin one direction then you have an\noptimizer now this optimizer performs\noptimization using merge transform split\netc it aims to reduce the amount of data\nin the pipeline that's the whole purpose\nof optimizer you have a internal\ncompiler so pic compiler converts the\noptimized code into a mapreduce job and\nhere user need not know the mapreduce\nprogramming model or how it works or how\nit is written they all need to know\nabout running the big script which would\nbe internally converted into a mapreduce\njob and finally we have an execution\nengine so mapreduce jobs are submitted\nto the execution engine to generate the\ndesired results so these are major\ncomponents of pig execution environment\nnow let's learn about different complex\ndata types in pig supports various data\ntypes the main ones are tuple bag and\nmap what is tuple or tuple as you might\nhave heard a tuple is an ordered set of\nfields which can contain different data\ntypes for each field so in array you\nwould have multiple elements but that\nwould be of same types list can also\nhave different types your tuple is a\ncollection which has different fields\nand each field can be of different type\nnow we could have an example as one\ncomma three or one comma three comma a\nstring or a float element and all of\nthat form a tuple bag is a set of tuples\nso that's represented by curly braces so\nyou could also imagine this like a\ndictionary which has various different\ncollection elements what is a map map is\na set of key value pairs used to\nrepresent data so when you work in big\ndata field you need to know about\ndifferent data types which are supported\nby pig which are supported by hive which\nare supported in other components of\nhadoop so tuple bag map array array\nbuffer you can think about list you can\nthink about dictionaries you can think\nabout map which is key value pair so\nthese are your different complex data\ntypes other than the primitive data type\nsuch as integer character string boolean\nfloat and so on now what are the various\ndiagnostic operators available in apache\npic so these are some of the operators\nor options which you can give in a pic\nscript you can do a dumb now dumb\noperator runs the pig latin scripts and\ndisplays the result on the screen so\neither i could do it dumb and see the\noutput on the screen or i can even do a\ndump into and i could store my output in\na particular file so we can load the\ndata using load operator and pick and\nthen pico also has different internal\nstorage like json loader or big storage\nwhich can be used if you are working on\nspecific kind of data and then you could\ndo a dump either before processing or\nafter processing and dump would produce\nthe result the result could be stored in\na file or\nseen on the screen you also have a\ndescribe operator now that is used to\nview the schema of a relation so you can\nload the data and then you can view the\nschema of relation using describe\noperator explain as we might already\nknow displays the physical logical and\nmapreduce execution plans so normally in\nrdbms when we use x-plane we would like\nto see what happens behind the scenes\nwhen a particular script or a query runs\nso we could load the data using load\noperator as in any other case and if we\nwould want to display the logical\nphysical and mapreduce execution plans\nwe could use explain operator there is\nalso an illustrate operator now that\ngives the step-by-step execution of\nsequence of statements so sometimes when\nwe would want to analyze our script to\nsee how good or bad they are or would\nthat really serve our purpose we could\nuse illustrate and again you can test\nthat by loading the data using load\noperator and you could just use a\nillustrate operator to have a look at\nthe step-by-step execution of the\nsequence of statements which you would\nwant to execute so these are different\ndiagnostic operators available in apache\npic now if somebody asks state the usage\nof group order by and distinct keywords\nin picscript so as i said pig is a\nscripting language so you could use\nvarious operators so group basically\ncollects various records with the same\nkey and groups the data in one or more\nrelations here is an example you could\ndo a group data so that is basically a\nvariable or you can give some other name\nand you can say group relation name by h\nnow say i have a file where i have field\nvarious fields and one of the field is\nrelation name so i could group that by a\ndifferent field order by is used to\ndisplay the contents of relation in a\nsorted order whether ascending or\ndescending so i could create a variable\ncalled relation 2 and then i could say\norder relation name 1 by ascending or\ndescending order distinct basically\nremoves the duplicate records and it is\nimplemented only on entire records\nnot on individual records so if you\nwould like want to find out the distinct\nvalues and relation name field i could\nuse distinct what are the relational\noperators in pig so you have various\nrelational operators which help data\nscientists or data analysts or\ndevelopers who are analyzing the data\nsuch as co-group which joins two or more\ntables and then performs group operation\non the join table result you have cross\nit is used to compute the cross product\nthat is a cartesian product of two or\nmore relations\nfor each is basically to do some\niteration so if it will iterate through\ntuples of a relation generating a data\ntransformation so for example if i say\nvariable a equals and then i load a file\nin a and then i could create a variable\ncalled b where i could say for each a i\nwould want to do something say group\njoin is to join two or more tables in a\nrelation limit is to limit the number of\noutput tuples or output results split is\nto split the relation into two or more\nrelations union is to get a combination\nthat's it will merge the contents of two\nor more relations and order is to get a\nsorted result so these are some\nrelational operators which are\nextensively used in pig for analysis\nwhat is the use of having filters in\napache pic\nnow say for example i have some data\nwhich has three fields year product\nquantity and this is my phone sales data\nso filter operator could be used to\nselect the required values from a\nrelation based on a condition it also\nallows you to remove unwanted records\nfrom data file so for example filter the\nproducts where quantity is greater than\nthousand so i see that i have one row\nwherein or multiple rows where the\nquantity is greater than thousand such\nas 1500 1700 1200 so i could create a\nvariable called a i would load my file\nusing pix storage as i explained earlier\npick storage is an internal parameter\nwhich can be used to\nspecify the delimiters now here my\ndelimiter is comma so i could say using\npick storage as and then i could specify\nthe data type for each field so here\nbeing integer product being character\narray and quantity being integer then b\ni could say filter a whatever we have in\na by quantity greater than thousand so\nit's very concise it's very simple and\nit allows us to extract and process data\nin a simpler way now suppose there is a\nfile called test.txt\nhaving 150 records in his dfs so this is\na file which is stored on his dfs and it\nhas 150 records where we can consider\nevery record being one line and if\nsomebody asks you to write a pick\ncommand to retrieve the first 10 records\nof the file first we will have to load\nthe data so i could create a variable\ncalled test underscore data and i would\nsay load my file using pick storage\nspecifying the delimiter as comma as and\nthen i could specify my fields whatever\nfields our file have and then i would\nwant to get only 10 records for which i\ncould use the limit operator so i could\nsay limit on test data and give me 10\nrecords this is very simple and we can\nextract 10 records from 150 records\nwhich are stored in the file on sdfs\nnow we have learned on pig we have\nlearned some questions on hive you could\nalways look more in books like\nprogramming in hive or programming in\npig and look for some more examples and\ntry out these examples on a existing\nhadoop setup now let's learn on hbase\nwhich is a nosql database now hbase is a\nfour dimensional database in comparison\nto your rdbms which usually are two\ndimensional so rdbms have rows and\ncolumns but hbase has four coordinates\nit has row key which is always unique\ncolumn family which can be any number\ncolumn qualifiers which can again be any\nnumber per column family and then you\nhave a version so these four coordinates\nmake edge base a four dimensional key\nvalue store or a column family store\nwhich is unique for storing huge amount\nof data and extracting data from hbase\nthere is a very good link which i would\nsuggest everyone can look at if you\nwould want to learn more on hbase and\nyou could just say edge base mapper and\nthis basically brings up a documentation\nwhich is from mapr but then that's not\nspecific to mapper and you can look at\nthis link which will give you a detailed\nexplanation of hbase how it works what\nare the architectural components and how\ndata is stored and how it makes hbase a\nvery powerful nosql database so let's\nlearn on some of the important or\ncritical questions on hbase which might\nbe asked by the interviewer in an\ninterview when you are applying for a\nbig data admin or a developer position\nrole so what are the key components of\nhbase now as i said this is one of the\nfavorite questions of interviewers where\nthey would want to understand your\nknowledge on different components for a\nparticular service hbase as i said is a\nnosql database and that comes as a part\nof service with cloudera or hortonworks\nand with apache hadoop you could also\nset up hbase as an independent package\nso what are the key components of hbase\nhbase has a region server now edge base\nfollows the similar kind of topology\nlike hadoop now hadoop has a master\nprocess that is named node and slave\nprocesses such as data nodes and\nsecondary name node in the same way\nhbase also has a master which is h\nmaster and the slave processes are\ncalled region servers so these region\nservers are usually co-located with data\nnodes however it is not mandatory that\nif you have 100 data nodes you would\nhave 100 region servers so it purely\ndepends on admin so what does this\nregion server contain so region server\ncontains hbase tables that are divided\nhorizontally into regions or you could\nsay group of rows is called regions so\nin edge base you have two aspects one is\ngroup of columns which is called column\nfamily and one is group of rows which is\ncalled regions now these regions or\nthese rows are grouped based on the key\nvalues or i would say row keys which are\nalways unique when you store your data\nin each base you would have data in the\nform of rows and columns so group of\nrows are called regions or you could say\nthese are horizontal partitions of the\ntable so a region server manages these\nregions on the node where a data node is\nrunning a region server can have up to\nthousand regions it runs on every node\nand decides the size of region so region\nserver as i said is a slave process\nwhich is responsible for managing hbase\ndata on the node each region server is a\nworker node or a worker process\nco-located with data node which will\ntake care of your read write update\ndelete request from the clients now when\nwe talk about more components of edge\nbase as i said you have hp h master so\nyou would always have a connection\ncoming in from a client or an\napplication what does hmaster do it\nassigns regions it monitors the region\nservers it assigns regions to region\nservers for load balancing and it cannot\ndo that without the help of zookeeper so\nif we talk about components of hbase\nthere are three main components you have\nzookeeper you have hmaster and you have\nregion server region server being the\nslave process your edge master being the\nmaster process which takes care of all\nyour table operations assigning regions\nto the region servers taking care of\nread and write requests which come from\nclient and for all of this edgemaster\nwill taken help of zookeeper which is a\ncentralized coordination service so\nwhenever a client wants to read or write\nor change the schema or any other\nmetadata operations it will contact\nhmaster edge master internally will\ncontact zookeeper so you could have\nhbase setup also in high availability\nmode where you could have a active edge\nmaster than a backup edge master you\nwould have a zookeeper quorum which is\nthe way zookeeper works so zookeeper is\na centralized coordination service which\nwill always run with a quorum of\nprocesses so zookeeper would always run\nwith odd number of processes such as 3 5\nand 7 because zookeeper works on the\nconcept of majority consensus now\nzookeeper which is a centralized\ncoordination service is keeping a track\nof all the servers which are alive\navailable and also keeps a track of\ntheir status for every server with\nzookeeper is monitoring zookeeper keeps\na session alive with that particular\nserver h master would always check with\nzookeeper which region servers are\navailable alive so that regions can be\nassigned to the region server at one end\nyou have region server which are sending\ntheir status to the zookeeper indicating\nif they are ready for any kind of read\nor write operation and at other end edge\nmaster is querying the zookeeper to\ncheck the status now zookeeper\ninternally manages a meta table now that\nmeta table will have information of\nwhich regions are residing on which\nregion server and what row keys those\nregions contain so in case of a read\nactivity hmaster will query zookeeper to\nfind out the region server which\ncontains that meta table once edge\nmaster gets the information of meta\ntable it can look into the meta table to\nfind out the row keys and the\ncorresponding region servers which\ncontain the regions for those row keys\nnow if we would want to understand row\nkey and column families in hbase let's\nlook at this and it would be good if we\ncould look this on an excel sheet so row\nkey is always unique it acts as a\nprimary key for any h base table it\nallows a logical grouping of cells and\nmake sure that all cells with the same\nrow key are co-located on the same\nserver so as i said you have four\ncoordinates for hbase you have a row key\nwhich is always unique you have column\nfamilies which is nothing but group of\ncolumns and when i say column families\none column family can have any number of\ncolumns so when i talk about h base h\nbase is four dimensional and in terms of\nedge base it is also called as a column\noriented database which basically means\nthat every row in one column could have\na different data type now you have a row\nkey which uniquely identifies the row\nyou have column families which could be\none or many depending on how the table\nhas been defined and a column family can\nhave any number of columns or i could\nsay for every row within a column family\nyou could have different number of\ncolumns so i could say for my row one i\ncould just have two columns such as name\nand city within the column family for my\nrow 2 i could have name city age\ndesignation salary for my third row i\ncould have thousand columns and all that\ncould belong to one column family so\nthis is a horizontally scalable database\nso column family consists of group of\ncolumns which is defined during table\ncreation and each column family can have\nany number of column qualifiers\nseparated by a delimiter now a\ncombination of row key column family\ncolumn qualifier such as name city age\nand the value within the cell is makes\nthe hbase a unique four dimensional\ndatabase for more information if you\nwould want to learn on hbase please\nrefer this link which is hbase mapper\nand this gives a complete hbase\narchitecture that is three components of\nname node\nthree components that is name node\nregion servers and zookeeper how it\nworks how each base edge master\ninteracts with zookeeper what zookeeper\ndoes in coordination how are the\ncomponents working together and how does\nhbs take care of read and write coming\nback and continuing why do we need to\ndisable a table so there are different\ntable operations what you can do in each\nbase and one of them is disabling a\ntable now if you would want to check the\nstatus of table you could check that by\nis disabled and giving the table name\norder is enabled and the table name so\nthe question is why do we need to\ndisable a table now if we would want to\nmodify a table or we are doing some kind\nof maintenance activity in that case we\ncan disable the table\nso that we can modify your changes\nsettings when a table is disabled it\ncannot be accessed through the scan\ncommand now if we have to write a code\nto open a connection in each base now to\ninteract with hbase one could either use\na graphical user interface such as hue\nor you could be using the command line\nhb shell or you could be using hbase\nadmin api if you are working with java\nor say happy base if you're working with\npython where you may want to open a\nconnection with hbase so that you can\nwork with it based programmatically in\nthat case we have to create a\nconfiguration object that is\nconfiguration my conf and then create a\nconfiguration object and then you can\nuse different classes like edge table\ninterface to work on a new table you\ncould use h column qualifier and many\nother classes which are available in\nhbase admin api what does replication\nmean in terms of hbase so edge base as i\nsaid works in a cluster way and when you\ntalk about cluster you could always set\nup a replication from one hbase cluster\nto other hbase cluster so this\nreplication feature in edge base\nprovides a mechanism to copy data\nbetween clusters or sync the data\nbetween different clusters this feature\ncan be used as a disaster recovery\nsolution that provides high availability\nfor hbase so if i have a hbase cluster\none where i have one master and multiple\nregion servers running in a hadoop\ncluster i could use the same hadoop\ncluster to create a hbase replica\ncluster or i could have a totally\ndifferent hbase replica cluster where my\nintention is that if things are changing\nin a particular table in cluster 1 i\nwould want them to be replicated across\ndifferent clusters so i could alter the\nhbase table and set the replication\nscope to one now a replication scope of\nzero indicates that table is not\nreplicated but if we set the replication\nto one we basically will have to set up\nah base cluster where we can replicate\nedge base tables data from cluster one\nto cluster so these are the commands\nwhich can be used to enable replication\nand then replicate the data of table\nacross clusters can we import and export\nin hbase of course we can it is possible\nto import and export tables from one\nhbase cluster to other hbase cluster or\neven within a cluster so we can use the\nhbase export utility which comes in this\nparticular package give a table name and\nthen a target location so that will\nexport the data of hbase table into a\ndirectory on sdfs then i could create a\ndifferent table which would follow some\nkind of same definition as the table\nwhich was exported and then i could use\nimport to import the data from the\ndirectory on sdfs to my table if you\nwould want to learn more on hbase import\nand export you could look at hbase\nimport operations let's search for the\nlink and this is the link where you\ncould learn more about hbase import\nexport utilities how you could do a bulk\nimport bulk export which internally uses\nmapreduce and then you could do a import\nand export into hbase tables moving\nfurther what do we mean by compaction in\nhbase now we all know that hbase is a\nnosql database which can be used to\nstore huge amount of data however\nwhenever a data is written in hbase it\nis first returned to what we call as\nright ahead log and also to mem store\nwhich is\nwrite cache now once the data is written\nin wall and your mem store it is\noffloaded to form an internal hbase\nformat file which is called h5 and\nusually these edge files are very small\nin nature so we also know that sdfs is\ngood when we talk about few number of\nlarger files in comparison to\nlarge number of smaller files due to the\nlimitation of name nodes memory\ncompaction is process of merging hbase\nfiles that is these smaller edge files\ninto a single large file this is done to\nreduce the amount of memory required to\nstore the files and number of disk seeks\nneeded so we could have lot of edge\nfiles which get created when the data is\nwritten to hbase and these smaller files\ncan then be compacted through a major or\nminor compaction creating one big edge\nfile which internally would then be\nwritten to sdfs and sdfs format of\nblocks that is the benefit of compaction\nthere is also a feature called bloom\nfilter so how does bloom filter work so\nbloom filter or hbase bloom filter is a\nmechanism to test whether a h file\ncontains a specific row or a row column\ncell bloom filter is named after its\ncreator burton hovered bloom it is a\ndata structure which predicts whether a\ngiven element is a member of a set of\ndata it provides an in-memory index\nstructure that reduces the disk reads\nand determines the probability of\nfinding a row in a particular file this\nis one of very useful features of hbase\nwhich allows for faster access and\navoids disk seeks\ndoes hbase have any concept of name\nspace so namespace is when you have\nsimilar elements grouped together so\nnamespace yes hb is support such name\nspace so namespace is a logical grouping\nof tables analogous to a database in\nrdbms so you can create hbs namespace to\nthe schema of rdbms database so you\ncould create a namespace by saying\ncreate namespace and giving it a name\nand then you could also list the tables\nwithin a namespace you could create\ntables within a specific namespace now\nthis is usually done in production\nenvironment where a cluster might be\nmulti-tenant cluster and there might be\ndifferent users of the same nosql\ndatabase in that case admin would create\nspecific namespace and for specific\nnamespace you would have different\ndirectories on htfs and users of a\nparticular business unit or a team can\nwork on their hbase objects within a\nspecific name space this is a question\nwhich is again very important to\nunderstand about the writes or reads so\nhow does right ahead log wall help when\na region server crashes now as i said\nwhen a write happens it will happen into\nmem store and wall that is your edit log\nor write ahead log so whenever a write\nhappens it will happen in two places mem\nstore which is the right cache and wall\nwhich is a edit log only when the data\nis written in both these places and\nbased on the limitation of mem store the\ndata will be flushed to create an\nedge-based format file called h-file\nthese files are then compacted and\ncreated into one bigger file which will\nthen be stored on sdfs and sdfs data as\nwe know is stored in the form of blocks\non the underlying data nodes so if a\nregion server hosting a mem store\ncrashes now where is region server\nrunning that would be co-located with\ndata node so if a data node crashes or\nif a region server which was hosting the\nmem store write cache crashes data in\nmemory the data that in memory which was\nnot persisted is lost now how does hbase\nrecover from this as i said your data is\nwritten into wall and mem store at the\nsame time hbase recovers against that by\nwriting to wall before the write\ncompletes so whenever a write happens it\nhappens in mem store and wall at the\nsame time hbase cluster keeps a wall to\nrecord changes as they happen and that's\nwhy we call it as also an edit log if\nhps goes down or the node that goes down\nthe data that was not flushed from mem\nstore to edge file can be recovered by\nreplaying the right ahead lock and\nthat's the benefit of your edit log or\nwrite ahead log now if we would have to\nwrite hbs command to list the contents\nand update the column families of a\ntable i could just do a scan and that\nwould give me complete data of a table\nif you are very specific and if you\nwould want to look at a particular row\nthen you could do a get table name and\nthen give the row key however you could\ndo a scan to get the complete data of a\nparticular table you could also do a\ndescribe to see what are the different\ncolumn families and if you would want to\nalter the table and add a new column\nfamily it is very simple you can just\nsay alter give the hvac table name and\nthen give you a new column family name\nwhich will then be added to the table\nwhat are catalog tables in each base so\nas i mentioned your zookeeper knows the\nlocation of this internal catalog table\nor what we call as the meta table now\ncatalog tables in edge base have two\ntables one is edge base meta table and\none is\nhyphen root the catalog table edge base\nmeta exists as an hbase table and is\nfiltered out of hbase shells list\ncommand so if i give a list command on\nedge base it would list all the tables\nwhich space contains but not the meta\ntable it's an internal table this meta\ntable keeps a list of all regions in the\nsystem and location of hbase meta stored\nin zookeeper so if somebody wants to\nfind out or look for particular rows\nthey need to know the regions which\ncontain that data and those regions are\nlocated on region server to get all this\ninformation one has to look into this\nmeta table however we will not be\nlooking into meta table directly we\nwould just be giving a write or a read\noperation internally uh based master\nqueries the zookeeper zookeeper has the\ninformation of where the meta table\nexists and that meta table which is\nexisting on region server contains\ninformation of row keys and the region\nservers where those rows can be found\nyour root table keeps a track of\nlocation of the meta table what is\nhotspotting in edge base and how to\navoid hot spotting now this is a common\nproblem and always admin guys or guys\nwho are managing the infrastructure\nwould think about it so one of the main\nidea is that edge base would be\nleveraging the benefit of sdfs your all\nread and write requests should be\nuniformly distributed across all of the\nregions in region servers otherwise\nwhat's the benefit of having a\ndistributed cluster so you would have\nyour data stored across region servers\nin the form of regions which are\nhorizontal partitions of the table and\nwhenever read and write requests happen\nthey should be uniformly distributed\nacross all the regions in the region\nservers now hot spotting occurs when a\ngiven region serviced by a region server\nreceives most or all of read write\nrequest which is basically a unbalanced\nway of read write operations now hotspot\ncan be avoided by designing the row key\nin such a way that data being written\nshould go to multiple regions across the\ncluster so you could do techniques such\nas salting hashing reversing the key and\nmany other techniques which are employed\nby users of hbase we need to just make\nsure that when the regions are\ndistributed across region servers they\nshould be spread across region servers\nso that your read and write request can\nbe satisfied from different region\nservers in parallel rather than all read\nwrite request hitting the same region\nserver overloading the region server\nwhich may also lead to the crashing of a\nparticular region server so these were\nsome of the important questions of hbase\nand then there are many more please\nrefer to the link which i specified in\nduring my discussion and that gives you\na detailed explanation of how each base\nworks you can also look into hp's\ndefinitive guide by o'reilly or hbase in\naction and these are really good books\nto understand about hbase internals and\nhow it works now that we have learned on\nhive which is a data warehousing package\nwe have learnt on pig which is a\nscripting or a scripting language which\nallows you to do data analysis and we\nhave learned some questions on a nosql\ndatabase just note it that there are\nmore than 225 nosql databases existing\nin market and if you would want to learn\nand know about more nosql databases you\ncan just go to google and type no sql\ndatabases\norg and that will take you to the link\nwhich is for nosql databases and this\nshows there are more than 225\nnosql databases existing in market and\nthese are for different use cases used\nby different users and for with\ndifferent features so have a look at\nthis link now when you talk about data\ningestion so let's look at data\ningestion and this is one good link\nwhich i would suggest to have a look at\nwhich lists down around 18 different\ningestion tools so when you talk about\ndifferent data injection tools some are\nfor structured data some are for\nstreaming data some are for data\ngovernance some are for data ingestion\nand transformation and so on so have a\nlook at this link which also gives you a\ncomparison of different data ingestion\ntools so here let's learn about some\nquestions on scope which is one of the\ndata injection tools mainly used for\nstructured data or you could say data\nwhich is coming in from rdbms or data\nwhich is already structured and you\nwould want to ingest that you would want\nto store that on sdfs which could then\nbe used for hive which could be used for\nany kind of processing using mapreduce\nor hive or pig or spark or any other\nprocessing frameworks or you would want\nto load that data into say high voltage\nbased tables scope is mainly for\nstructured data it is extensively used\nwhen organizations are migrating from\nrdbms to a big data platform and they\nwould be interested in ingesting the\ndata that is doing import and export of\ndata from rdbms to sdfs or vice versa so\nlet's learn about some important\nquestions on scope which you may be\nasked by an interviewer when you apply\nfor a big data related position how is\nscoop different from flume so this is a\nvery common question which is asked\nscoop which is mainly for structured\ndata so scope works with rdbms it also\nworks with nosql databases to import and\nexport data so you can import data into\nsdfs you can import data into data\nwarehousing package such as hive\ndirectly or also in hbase and you could\nalso export data from hadoop ecosystem\nto your rdbms however when it comes to\nflow flow is more of\na data injection tool which works with\nstreaming data or unstructured data so\ndata which is constantly getting\ngenerated for example log files or\nmetrics from server or some chat\nmessenger and so on so if you are\ninterested in working on capturing and\nstoring the streaming data in a storage\nlayer such as sdfs or hbase you could be\nusing flu there could be other tools\nalso like kafka or storm or chokwa or\nsamsa nifi and so on scoop however is\nmainly for structured data your loading\ndata in scope is not event driven so it\nis not based on event it basically works\non data which is already stored in rdbms\nin terms of flow it is completely event\ndriven that is as the messages or as the\nevents happen as the data is getting\ngenerated you can have that data\ningested using flow scope works with\nstructured data sources and you have\nvarious scope connectors which are used\nto fetch data from external data\nstructures or rdbms so for every rdbms\nsuch as mysql oracle db2 microsoft sql\nserver you have different connectors\nwhich are available flume it works on\nfetching streaming data such as tweets\nor log files or server metrics from your\ndifferent sources where the data is\ngetting generated and if you are\ninterested in not only ingesting that\ndata which is getting generated in a\nstreaming fashion but if you would be\ninterested in processing the data as it\narrives scoop can import data from rdbms\nonto sdfs and also export it back to\nrdbms flume is then used for streaming\ndata now you could have one to one one\ntoo many or many to one kind of relation\nso in terms of floom you have components\nsuch as your source sink and channel\nthat's the main difference between your\nscoop and flow what are the different\nfile formats to import data using scope\nwell there are lots and lots of formats\nin which you can import data into scope\nwhen you talk about scope you can have\ndelimited text file format now that's\nthe default import format it can be\nspecified explicitly using as text file\nargument so when i want to import data\nfrom an rdbms i could get that data in\nsdfs using different compression schemes\nor in different formats using the\nspecific arguments so i could specify an\nargument which will write string based\nrepresentation of each record to output\nfiles with delimiters between individual\ncolumns and rows so that is the default\nformat which is used to import data\nusing scope so to learn more about your\nscope and different arguments which are\navailable you can click on\nscoop.apache.org\nyou can look into the documentation and\ni would suggest choosing one of the\nversions and looking into the user guide\nand here you can search for arguments\nand look for specific control arguments\nwhich show how you can import data using\nscope so here we have common arguments\nand then you also have import control\narguments wherein we have different\noptions like getting data as avro as\nsequence file as text file or parquet\nfile these are different formats you can\nalso get data in default compression\nscheme that is gzip or you can specify\ncompression codec and then you can\nspecify what compression mechanism you\nwould want to use when you are importing\nyour data using scope when it comes to\ndefault format for flume we could say\nsequence file which is a binary format\nthat stores individual records in record\nspecific data types so these data types\nare manifested as java classes and scope\nwill automatically generate these data\ntypes for you so scoop does that when we\ntalk about your sequence file format in\nterms of your scope you could be\nextracting storage of all data in binary\nrepresentation so as i mentioned you can\nimport data in different formats such as\navro parquet sequence file that is\nbinary format or machine readable format\nand then you could also have data in\ndifferent compression schemes let me\njust show you some quick examples here\nso if i look in\nthe content and here i could search for\na scoop based file where i have listed\ndown some examples so if i would want to\nuse different compression schemes here\nare some examples have a look at these\nso i'm doing a scoop import i'm also\ngiving an argument so that scoop which\nalso triggers a map reduced job or i\nwould say map only job so when you run a\nscoop import it triggers a map only job\nno reduce happens here and you could\nspecify this parameter or this argument\non the command line\nmapreduce.framework.name\nso that you could run your map only job\nin a local mode to save time or that\nwould interact with yarn and run a\nfull-fledged map only job we can give\nthe connection and then connect to\nwhatever rdbms we are connecting\nmentioning the database name give your\nuser name and password give the table\nname give a target directory or it would\ncreate a directory same as the table\nname which would work only once and then\ni could say minus z to get data in a\ncompressed format that is gzip or i\ncould be specifying compression codec\nand then i could specify what\ncompression codec i would want to use\nsay snappy b lz4 default i could also\nrun a query by giving a scope import and\nwhen i'm specifying a query if you\nnotice i'm not given any table name\nbecause that would be included in the\nquery i can get my data as a sequence\nfile format which is a binary format\nwhich will create a huge file so we\ncould also have compression enabled and\nthen i could say the output of my map\njob should use a compression at record\nlevel for my data coming in sequence\nfile so sequence file or a binary format\nsupports compression at record level or\nat block level i could get my data in an\navro file where data has embedded schema\nwithin the file or a parquet file also\nso these are different ways in which you\ncan set up different compression schemes\nor you can even get data in different\nformats and you could be doing a simple\nscope import for these looking further\nwhat is the importance of eval tool in\nscope so there is something called as\neval tool so scoop eval tool allows\nusers to execute user defined queries\nagainst respective database servers and\npreview the result in the console so\neither i could be running a straight\naway query to import the data into my\nsdfs or i could just use scoop eval\nconnect to my external rdbms specify my\nusername and password\nand then i could be giving in a query to\nsee what would be the result of the\nquery which we intend to import now\nlet's learn about how scope imports and\nexports data between rdbms and sdfs with\nits architecture so rdbms as we know has\nyour database structures your tables\nwhich all of them are logical and\ninternally there is always metadata\nwhich is stored your scope import\nconnects to an external rdbms and for\nthis connection it uses an internal\nconnector jar file which has a driver\nclass so that's something which needs to\nbe set up by admin but they need to make\nsure that whichever rdbms you intend to\nconnect to they need to have the jdbc\nconnector for that particular rdbms\nstored within the scope lib folder so\nscope import gets the metadata and then\nfor your scoop command it converts that\ninto a map only job which might have one\nor multiple map tasks\nnow that depends on your scope command\nyou could be specifying that you would\nwant to do a import only in one task or\nin multiple tasks these multiple map\ntasks will then run on a section of data\nfrom rdbms and then store it in sdfs so\nat high level we could say scoop will\nintrospect database to get gathered the\nmetadata it divides the input data set\ninto splits and this division of data\ninto splits mainly happens on primary\nkey column of the table now if somebody\nmight ask what if my table in rdbms does\nnot have a primary key column then when\nyou are doing a scope import either you\nwill have to import it using one mapper\ntask by specifying hyphen hyphen m\nequals one or you would have to say\nsplit by parameter to specify a numeric\ncolumn from rdbms and that's how you can\nimport the data let me just show you a\nquick example on this so i could just\nlook in again into the scoop command\nfile and here we could be looking at an\nexample so if you see this one here we\nare specifying minus minus m equals 1\nwhich basically means i would want to\nimport the data using one map task now\nin this case whether the table has a\nprimary key column or does not have a\nprimary key column will not matter but\nif i say a minus minus ms6 where i am\nspecifying multiple map tasks to be\nimported then this will look for a\nprimary key column in the table which\nyou are importing now if the table does\nnot have a primary key column then i\ncould be specifying a split by and then\nspecify the column so that the data\ncould be split into multiple chunks and\nmultiple map tasks could take it\nnow if the second scenario is your table\ndoes not have a primary key column and\nit does not have a numeric column on\nwhich you could do a split by in that\ncase and if you would want to use\nmultiple mappers you could still say\nsplit by on a textual column but you\nwill have to add this property so that\nit allows splitting the data which is\nnon-numeric\nall of these options are given in the\nscoop apache.org link going further how\nscoop imports and exports data between\nrdbms and sdfs with its architecture so\nas i said it submits the map only job to\nthe cluster and then it basically does a\nimport or export so if we are exporting\nthe data from sdfs in that case again\nthere would be a map only job it would\nlook at multiple splits of the data\nwhich is existing which your map only\njob would process through one or one\ntable map task and then export it to\nrdbms suppose you have a database testdb\nin mysql we if somebody asked you to\nwrite a command to connect this database\nand import tables to scoop so here is a\nquick example as i showed you in the\ncommand file so you could say scope\nimport this is what we would want to do\nyou connect using jdbc now this will\nonly work if the jdbc connector already\nexists within your scope lib directory\nadmin has to set up that so you can\nconnect to your rdbms you can point to\nthe database so\nhere our database name is test\nunderscore db i could give user name and\nthen either i could give password on the\ncommand line or just say capital p so\nthat i could be prompted for the\npassword and then i could give the table\nname which i would want to import i\ncould also be specifying minus minus m\nand specify how many map tasks do i want\nto use for this import as i showed in\nprevious screen how to export a table\nback to rdbms now for this we need the\ndata in a directory on hdfs so for\nexample\nthere is a department's table in retail\ndatabase which is already imported into\nscoop and you need to export this table\nback to rdbms so this is the content of\nthe table now create a new department\ntable in rdbms so i could create a table\nspecifying the column names whether that\nsupports null or no if that has a\nprimary key column which is always\nrecommended and then i can do a scoop\nexport i can connect to the rdbms\nspecifying my username and password\nspecify the table into which you want to\nexport the data and then you give export\ndirectory pointing to a directory on\nsdfs which contains the data this is how\nyou can export data into table seeing\nexample on this so i could again look\ninto my file and here i have an example\nof import this is where you are\nimporting data directly into hive and\nyou have scope import where you are\nimporting data directly into hbase table\nand you can then query your hbase table\nto look at the data you could also do a\nexport by\nrunning your map only job in a local\nmode connecting to the rdbms specifying\nyour username specifying the table where\nyou would want to export and the\ndirectory on sdfs where you have kept\nthe relevant data this is a simple\nexample of export looking further\nwhat is the role of jdbc driver in scope\nsetup so as i said if you would want to\nuse scoop to connect to an external\nrdbms we need the jdbc odbc connector\njar file now one or admin could download\nthe jdbc connector jar file and then\nplace the jar file within the scoop lib\ndirectory wherever scoop is installed\nand this jdbc connector jar file\ncontains a driver now jdbc driver is a\nstandard java api which is used for\naccessing different databases in rdbms\nso this connector jar file is very much\nrequired and this connector jar file has\na driver class and this driver class\nenables the connection between your\nrdbms and your hadoop structure each\ndatabase vendor is responsible for\nwriting their own implementation that\nwill allow communication with the\ncorresponding database and we need to\ndownload the drivers which allow our\nscoop to connect to external rdbms so\nyour jdbc driver alone is not enough to\nconnect to scope we also need connectors\nto interact with different database so a\nconnector is a plugable piece that is\nused to fetch metadata and allow scoop\nto overcome the differences in sql\ndialects so this is how connection can\nbe established so normally your admins\nwould when they are setting up scope and\nhadoop they would download\nsay mysql jdbc connector and this is how\nthey would go to the mysql connectors if\nyou are connecting to mysql similarly\nfor your other rdbms you could be\nsay going in here you could be looking\nfor a previous version depending you\ncould be going for platform independent\nand then you could be downloading the\nconnected jar file now if you enter this\njar file you would see a mysql connector\njar and if we look in\n[Music]\nmysql.jdbc.driver\ncom.mysql.jdbcom.mysql.jdbc\nso this is the package which is within\nthe connector jar file and this\nhas the driver class which allows the\nconnection of your scope with your rdbms\nso these things will have to be done by\nyour admin so that you can have your\nscope connecting to an external rdbms\nnow how do you update the columns that\nare already exported so if i do a export\nand i put my data in rdbms can i really\nupdate the columns that are already\nexported yes i can using a update key\nparameter so scoop export command\nremains the same the only thing i will\nhave to specify now is the table name\nyour fields terminated by if you have a\nspecific delimiter and then you can say\nupdate key and then the column name so\nthis allows us to update the columns\nthat are already exported in rdbms\nwhat is code gen so scope commands\ntranslate into your mapreduce job or map\nonly job so code gen is basically a tool\nin scope that generates data access\nobjects dao java classes that\nencapsulate and interpret imported\nrecords so if i do a scoop code gen\nconnect to an rdbms using my username\nand give a table this will generate a\njava code for employee table in the test\ndatabase so this code gen can be useful\nfor us to understand what data we have\nin this particular table finally can\nscoop be used to convert data in\ndifferent formats i think i already\nanswered that right if no which tools\ncan be used for this purpose so scoop\ncan be used to convert data in different\nformats and that depends on the\ndifferent arguments which you use when\nyou do a import such as avro file\nparquet file binary format with record\nor block level compression so if you are\ninterested in knowing more on different\ndata formats then i think i can suggest\na link for that and we can say hadoop\nformats\ni think it is tech\nmaggie avro\nparque let's see you can find out the\nlink take mac e yeah this is a very good\nlink which specifies or talks about\ndifferent data formats which you should\nknow such as your text file format\ndifferent compression schemes how is\ndata organization what are the common\nformats what do you have in text file\nstructured binary sequence files with\ncompression without compression what is\nrecord level what is block level what is\na avro data file what is a sequence what\nis a\nparquet data file or a columnar format\nand other formats like orc rc and so on\nso please have a look at this\nthank you all for watching this full\ncourse video on big data for 2022 i hope\nit was useful and informative if you\nhave any queries please feel free to put\nthem in the comments section of the\nvideo we'll be happy to help you thanks\nagain stay safe and keep learning\n[Music]\nhi there if you like this video\nsubscribe to the simply learn youtube\nchannel and click here to watch similar\nvideos turn it up and get certified\nclick here\nyou\n",
  "words": [
    "big",
    "data",
    "term",
    "sure",
    "familiar",
    "big",
    "data",
    "technology",
    "grown",
    "massively",
    "decade",
    "half",
    "internet",
    "users",
    "boomed",
    "companies",
    "started",
    "generating",
    "vast",
    "amounts",
    "data",
    "became",
    "popular",
    "advent",
    "ai",
    "machine",
    "learning",
    "mobile",
    "technology",
    "internet",
    "things",
    "big",
    "data",
    "analytics",
    "helps",
    "companies",
    "different",
    "sectors",
    "automobile",
    "manufacturing",
    "logistics",
    "manage",
    "processes",
    "streamline",
    "use",
    "data",
    "sets",
    "real",
    "time",
    "well",
    "improve",
    "organization",
    "capability",
    "big",
    "data",
    "analytics",
    "enable",
    "organizations",
    "get",
    "better",
    "understanding",
    "customers",
    "help",
    "narrow",
    "targeted",
    "audience",
    "thus",
    "helping",
    "improve",
    "companies",
    "marketing",
    "campaigns",
    "doubt",
    "big",
    "data",
    "market",
    "steadily",
    "growing",
    "per",
    "fortune",
    "business",
    "insights",
    "global",
    "big",
    "data",
    "market",
    "expected",
    "grow",
    "billion",
    "dollars",
    "2021",
    "billion",
    "us",
    "dollars",
    "2028",
    "cagr",
    "percent",
    "per",
    "annual",
    "trends",
    "report",
    "big",
    "data",
    "analytics",
    "likely",
    "going",
    "top",
    "demand",
    "skill",
    "least",
    "96",
    "percent",
    "companies",
    "definitely",
    "planning",
    "likely",
    "plan",
    "hire",
    "new",
    "staff",
    "relevant",
    "skills",
    "fill",
    "future",
    "big",
    "data",
    "analytics",
    "related",
    "roles",
    "2022",
    "indicates",
    "career",
    "opportunities",
    "big",
    "data",
    "high",
    "scope",
    "really",
    "good",
    "looking",
    "career",
    "big",
    "data",
    "2022",
    "screen",
    "see",
    "top",
    "big",
    "data",
    "companies",
    "oracle",
    "ecommerce",
    "leader",
    "amazon",
    "hpe",
    "tech",
    "giant",
    "ibm",
    "salesforce",
    "let",
    "look",
    "agenda",
    "today",
    "video",
    "big",
    "data",
    "full",
    "course",
    "2022",
    "start",
    "learning",
    "become",
    "big",
    "data",
    "engineer",
    "see",
    "crucial",
    "skills",
    "big",
    "data",
    "next",
    "understand",
    "big",
    "data",
    "analytics",
    "look",
    "top",
    "applications",
    "big",
    "data",
    "going",
    "learn",
    "big",
    "data",
    "tutorial",
    "understand",
    "popular",
    "big",
    "data",
    "framework",
    "hadoop",
    "next",
    "going",
    "look",
    "different",
    "tools",
    "part",
    "hadoop",
    "ecosystem",
    "going",
    "learn",
    "apache",
    "spark",
    "understand",
    "spark",
    "architecture",
    "finally",
    "going",
    "close",
    "full",
    "course",
    "video",
    "session",
    "learning",
    "top",
    "hadoop",
    "interview",
    "questions",
    "let",
    "begin",
    "today",
    "going",
    "tell",
    "become",
    "big",
    "data",
    "engineer",
    "come",
    "surprise",
    "guys",
    "know",
    "organizations",
    "generate",
    "well",
    "use",
    "whole",
    "lot",
    "data",
    "vast",
    "volume",
    "data",
    "called",
    "big",
    "data",
    "companies",
    "use",
    "big",
    "data",
    "draw",
    "meaningful",
    "insights",
    "take",
    "business",
    "decisions",
    "big",
    "data",
    "engineers",
    "people",
    "make",
    "sense",
    "enormous",
    "amount",
    "data",
    "let",
    "find",
    "big",
    "data",
    "engineer",
    "big",
    "data",
    "engineer",
    "professional",
    "develops",
    "maintains",
    "tests",
    "evaluates",
    "company",
    "big",
    "data",
    "infrastructure",
    "words",
    "develop",
    "big",
    "data",
    "solutions",
    "based",
    "company",
    "requirements",
    "maintain",
    "solutions",
    "test",
    "solutions",
    "company",
    "requirements",
    "integrate",
    "solution",
    "various",
    "tools",
    "systems",
    "organization",
    "finally",
    "evaluate",
    "well",
    "solution",
    "working",
    "fulfill",
    "company",
    "requirements",
    "next",
    "let",
    "look",
    "responsibilities",
    "big",
    "data",
    "engineer",
    "need",
    "able",
    "design",
    "implement",
    "verify",
    "maintain",
    "software",
    "systems",
    "process",
    "ingesting",
    "data",
    "well",
    "processing",
    "need",
    "able",
    "build",
    "highly",
    "scalable",
    "well",
    "robust",
    "systems",
    "need",
    "able",
    "extract",
    "data",
    "one",
    "database",
    "transform",
    "well",
    "load",
    "another",
    "data",
    "store",
    "process",
    "etl",
    "extract",
    "transform",
    "load",
    "process",
    "need",
    "research",
    "well",
    "propose",
    "new",
    "ways",
    "acquire",
    "data",
    "improve",
    "overall",
    "data",
    "quality",
    "efficiency",
    "system",
    "ensure",
    "business",
    "requirements",
    "met",
    "need",
    "build",
    "suitable",
    "data",
    "architecture",
    "need",
    "able",
    "integrate",
    "several",
    "programming",
    "languages",
    "tools",
    "together",
    "generate",
    "structured",
    "solution",
    "need",
    "build",
    "models",
    "reduce",
    "overall",
    "complexity",
    "increase",
    "efficiency",
    "whole",
    "system",
    "mining",
    "data",
    "various",
    "sources",
    "finally",
    "need",
    "work",
    "well",
    "teams",
    "ones",
    "include",
    "data",
    "architects",
    "data",
    "analysts",
    "data",
    "scientists",
    "next",
    "let",
    "look",
    "skills",
    "required",
    "become",
    "big",
    "data",
    "engineer",
    "first",
    "step",
    "programming",
    "knowledge",
    "one",
    "important",
    "skills",
    "required",
    "become",
    "big",
    "data",
    "engineer",
    "experience",
    "programming",
    "languages",
    "especially",
    "experience",
    "big",
    "data",
    "solutions",
    "organizations",
    "would",
    "want",
    "create",
    "possible",
    "without",
    "experience",
    "programming",
    "languages",
    "even",
    "tell",
    "easy",
    "way",
    "get",
    "experience",
    "programming",
    "languages",
    "practice",
    "practice",
    "practice",
    "commonly",
    "used",
    "programming",
    "languages",
    "used",
    "big",
    "data",
    "engineering",
    "python",
    "java",
    "c",
    "plus",
    "second",
    "skill",
    "require",
    "knowledge",
    "dbms",
    "sql",
    "need",
    "know",
    "data",
    "maintained",
    "well",
    "managed",
    "database",
    "need",
    "know",
    "sql",
    "used",
    "transform",
    "well",
    "perform",
    "actions",
    "database",
    "extension",
    "know",
    "write",
    "sql",
    "queries",
    "relational",
    "database",
    "management",
    "systems",
    "commonly",
    "used",
    "database",
    "management",
    "systems",
    "big",
    "data",
    "engineering",
    "mysql",
    "oracle",
    "database",
    "microsoft",
    "sql",
    "server",
    "third",
    "skill",
    "require",
    "experience",
    "working",
    "etl",
    "warehousing",
    "tools",
    "need",
    "know",
    "construct",
    "well",
    "use",
    "data",
    "warehouse",
    "perform",
    "etl",
    "operation",
    "extract",
    "transform",
    "load",
    "operations",
    "big",
    "data",
    "engineer",
    "constantly",
    "tasked",
    "extracting",
    "unstructured",
    "data",
    "number",
    "different",
    "sources",
    "transforming",
    "meaningful",
    "information",
    "loading",
    "data",
    "storages",
    "databases",
    "data",
    "warehouses",
    "basically",
    "aggregating",
    "unstructured",
    "data",
    "multiple",
    "sources",
    "analyzing",
    "take",
    "better",
    "business",
    "decisions",
    "tools",
    "used",
    "purpose",
    "talent",
    "ibm",
    "data",
    "stage",
    "pentaho",
    "informatica",
    "next",
    "fourth",
    "skill",
    "require",
    "knowledge",
    "operating",
    "systems",
    "since",
    "big",
    "data",
    "tools",
    "unique",
    "demands",
    "root",
    "access",
    "operating",
    "system",
    "functionality",
    "well",
    "hardware",
    "strong",
    "understanding",
    "operating",
    "systems",
    "like",
    "linux",
    "unix",
    "absolutely",
    "mandatory",
    "operating",
    "systems",
    "used",
    "big",
    "data",
    "engineers",
    "unix",
    "linux",
    "solaris",
    "fifth",
    "skill",
    "require",
    "experience",
    "hadoop",
    "based",
    "analytics",
    "since",
    "hadoop",
    "one",
    "commonly",
    "used",
    "tools",
    "comes",
    "big",
    "data",
    "engineering",
    "understood",
    "need",
    "experience",
    "apache",
    "hadoop",
    "based",
    "technologies",
    "technologies",
    "like",
    "hdfs",
    "hadoop",
    "mapreduce",
    "apache",
    "edge",
    "base",
    "hive",
    "pig",
    "sixth",
    "skill",
    "require",
    "worked",
    "processing",
    "frameworks",
    "like",
    "apache",
    "spark",
    "big",
    "data",
    "engineer",
    "deal",
    "vast",
    "volumes",
    "data",
    "data",
    "need",
    "analytics",
    "engine",
    "like",
    "spark",
    "used",
    "data",
    "processing",
    "spark",
    "process",
    "live",
    "streaming",
    "data",
    "number",
    "different",
    "sources",
    "like",
    "facebook",
    "instagram",
    "twitter",
    "also",
    "perform",
    "interactive",
    "analysis",
    "data",
    "integration",
    "final",
    "skill",
    "requirement",
    "experience",
    "data",
    "mining",
    "modeling",
    "data",
    "engineer",
    "examine",
    "massive",
    "data",
    "discover",
    "patterns",
    "well",
    "new",
    "information",
    "create",
    "predictive",
    "models",
    "business",
    "make",
    "better",
    "informed",
    "decisions",
    "tools",
    "used",
    "r",
    "rapidminer",
    "weka",
    "let",
    "talk",
    "big",
    "data",
    "engineer",
    "salary",
    "well",
    "roles",
    "look",
    "forward",
    "average",
    "salary",
    "big",
    "data",
    "engineer",
    "united",
    "states",
    "approximately",
    "ninety",
    "thousand",
    "dollars",
    "per",
    "year",
    "ranges",
    "sixty",
    "six",
    "thousand",
    "dollars",
    "way",
    "hundred",
    "thirty",
    "thousand",
    "dollars",
    "per",
    "annum",
    "india",
    "average",
    "salary",
    "around",
    "seven",
    "lakh",
    "rupees",
    "ranges",
    "four",
    "lakhs",
    "13",
    "lakhs",
    "per",
    "annum",
    "become",
    "big",
    "data",
    "engineer",
    "job",
    "roles",
    "look",
    "forward",
    "senior",
    "big",
    "data",
    "engineer",
    "business",
    "intelligence",
    "architect",
    "data",
    "architect",
    "let",
    "talk",
    "certifications",
    "big",
    "data",
    "engineer",
    "opt",
    "first",
    "cloudera",
    "ccp",
    "data",
    "engineer",
    "cloudera",
    "certified",
    "professional",
    "data",
    "engineer",
    "possesses",
    "skills",
    "develop",
    "reliable",
    "scalable",
    "data",
    "pipelines",
    "result",
    "optimized",
    "data",
    "sets",
    "variety",
    "workloads",
    "one",
    "industry",
    "demanding",
    "performance",
    "based",
    "certification",
    "ccp",
    "evaluates",
    "recognizes",
    "candidate",
    "mastery",
    "technical",
    "skills",
    "sought",
    "employers",
    "time",
    "limit",
    "exam",
    "240",
    "minutes",
    "costs",
    "400",
    "next",
    "ibm",
    "certified",
    "data",
    "architect",
    "big",
    "data",
    "certification",
    "ibm",
    "certified",
    "big",
    "data",
    "architect",
    "understands",
    "complexity",
    "data",
    "design",
    "systems",
    "models",
    "handle",
    "different",
    "data",
    "variety",
    "including",
    "structured",
    "unstructured",
    "volume",
    "velocity",
    "veracity",
    "big",
    "data",
    "architect",
    "also",
    "able",
    "effectively",
    "address",
    "information",
    "governance",
    "security",
    "challenges",
    "associated",
    "system",
    "exam",
    "75",
    "minutes",
    "long",
    "finally",
    "google",
    "cloud",
    "certified",
    "data",
    "engineer",
    "google",
    "certified",
    "data",
    "engineer",
    "enables",
    "data",
    "driven",
    "decision",
    "making",
    "collecting",
    "transforming",
    "publishing",
    "data",
    "also",
    "able",
    "leverage",
    "deploy",
    "continuously",
    "train",
    "machine",
    "learning",
    "models",
    "length",
    "certification",
    "exam",
    "2",
    "hours",
    "registration",
    "fee",
    "200",
    "let",
    "look",
    "simply",
    "learn",
    "help",
    "become",
    "big",
    "data",
    "engineer",
    "simply",
    "learn",
    "provides",
    "big",
    "data",
    "architect",
    "masters",
    "program",
    "includes",
    "number",
    "different",
    "courses",
    "like",
    "big",
    "data",
    "hadoop",
    "spark",
    "developer",
    "apache",
    "spark",
    "scala",
    "mongodb",
    "developer",
    "administrator",
    "big",
    "data",
    "hadoop",
    "administrator",
    "much",
    "course",
    "goes",
    "50",
    "plus",
    "skills",
    "tools",
    "12",
    "plus",
    "real",
    "life",
    "projects",
    "possibility",
    "annual",
    "average",
    "salary",
    "19",
    "26",
    "lakh",
    "rupees",
    "per",
    "annum",
    "also",
    "help",
    "get",
    "noticed",
    "top",
    "hiring",
    "companies",
    "course",
    "also",
    "go",
    "major",
    "tools",
    "like",
    "kafka",
    "apache",
    "spark",
    "flume",
    "edge",
    "base",
    "mongodb",
    "hive",
    "pig",
    "mapreduce",
    "java",
    "scala",
    "much",
    "head",
    "get",
    "started",
    "journey",
    "get",
    "certified",
    "get",
    "ahead",
    "use",
    "smartphones",
    "ever",
    "wondered",
    "much",
    "data",
    "generates",
    "form",
    "texts",
    "phone",
    "calls",
    "emails",
    "photos",
    "videos",
    "searches",
    "music",
    "approximately",
    "40",
    "exabytes",
    "data",
    "gets",
    "generated",
    "every",
    "month",
    "single",
    "smartphone",
    "user",
    "imagine",
    "number",
    "multiplied",
    "5",
    "billion",
    "smartphone",
    "users",
    "lot",
    "mind",
    "even",
    "process",
    "fact",
    "amount",
    "data",
    "quite",
    "lot",
    "traditional",
    "computing",
    "systems",
    "handle",
    "massive",
    "amount",
    "data",
    "term",
    "big",
    "data",
    "let",
    "look",
    "data",
    "generated",
    "per",
    "minute",
    "internet",
    "million",
    "snaps",
    "shared",
    "snapchat",
    "million",
    "search",
    "queries",
    "made",
    "google",
    "one",
    "million",
    "people",
    "log",
    "facebook",
    "million",
    "videos",
    "watched",
    "youtube",
    "188",
    "million",
    "emails",
    "sent",
    "lot",
    "data",
    "classify",
    "data",
    "big",
    "data",
    "possible",
    "concept",
    "five",
    "v",
    "volume",
    "velocity",
    "variety",
    "veracity",
    "value",
    "let",
    "us",
    "understand",
    "example",
    "healthcare",
    "industry",
    "hospitals",
    "clinics",
    "across",
    "world",
    "generate",
    "massive",
    "volumes",
    "data",
    "2",
    "314",
    "exabytes",
    "data",
    "collected",
    "annually",
    "form",
    "patient",
    "records",
    "test",
    "results",
    "data",
    "generated",
    "high",
    "speed",
    "attributes",
    "velocity",
    "big",
    "data",
    "variety",
    "refers",
    "various",
    "data",
    "types",
    "structured",
    "unstructured",
    "data",
    "examples",
    "include",
    "excel",
    "records",
    "log",
    "files",
    "images",
    "accuracy",
    "trustworthiness",
    "generated",
    "data",
    "termed",
    "veracity",
    "analyzing",
    "data",
    "benefit",
    "medical",
    "sector",
    "enabling",
    "faster",
    "disease",
    "detection",
    "better",
    "treatment",
    "reduced",
    "cost",
    "known",
    "value",
    "big",
    "data",
    "store",
    "process",
    "big",
    "data",
    "job",
    "various",
    "frameworks",
    "cassandra",
    "hadoop",
    "spark",
    "let",
    "us",
    "take",
    "hadoop",
    "example",
    "see",
    "hadoop",
    "stores",
    "processes",
    "big",
    "data",
    "hadoop",
    "uses",
    "distributed",
    "file",
    "system",
    "known",
    "hadoop",
    "distributed",
    "file",
    "system",
    "store",
    "big",
    "data",
    "huge",
    "file",
    "file",
    "broken",
    "smaller",
    "chunks",
    "stored",
    "various",
    "machines",
    "break",
    "file",
    "also",
    "make",
    "copies",
    "goes",
    "different",
    "nodes",
    "way",
    "store",
    "big",
    "data",
    "distributed",
    "way",
    "make",
    "sure",
    "even",
    "one",
    "machine",
    "fails",
    "data",
    "safe",
    "another",
    "mapreduce",
    "technique",
    "used",
    "process",
    "big",
    "data",
    "lengthy",
    "task",
    "broken",
    "smaller",
    "tasks",
    "b",
    "c",
    "instead",
    "one",
    "machine",
    "three",
    "machines",
    "take",
    "task",
    "complete",
    "parallel",
    "fashion",
    "assemble",
    "results",
    "end",
    "thanks",
    "processing",
    "becomes",
    "easy",
    "fast",
    "known",
    "parallel",
    "processing",
    "stored",
    "processed",
    "big",
    "data",
    "analyze",
    "data",
    "numerous",
    "applications",
    "games",
    "like",
    "halo",
    "3",
    "call",
    "duty",
    "designers",
    "analyze",
    "user",
    "data",
    "understand",
    "stage",
    "users",
    "pause",
    "restart",
    "quit",
    "playing",
    "insight",
    "help",
    "rework",
    "storyline",
    "game",
    "improve",
    "user",
    "experience",
    "turn",
    "reduces",
    "customer",
    "churn",
    "rate",
    "big",
    "data",
    "also",
    "helped",
    "disaster",
    "management",
    "hurricane",
    "sandy",
    "2012",
    "used",
    "gain",
    "better",
    "understanding",
    "storm",
    "effect",
    "east",
    "coast",
    "us",
    "necessary",
    "measures",
    "taken",
    "could",
    "predict",
    "hurricane",
    "landfall",
    "five",
    "days",
    "advance",
    "possible",
    "earlier",
    "clear",
    "indications",
    "valuable",
    "big",
    "data",
    "accurately",
    "processed",
    "analyzed",
    "back",
    "early",
    "2000s",
    "relatively",
    "less",
    "data",
    "generated",
    "rise",
    "various",
    "social",
    "media",
    "platforms",
    "multinational",
    "companies",
    "across",
    "globe",
    "generation",
    "data",
    "increased",
    "leaps",
    "bounds",
    "according",
    "idc",
    "total",
    "volume",
    "data",
    "expected",
    "reach",
    "175",
    "zettabytes",
    "2025",
    "lot",
    "data",
    "define",
    "big",
    "data",
    "massive",
    "amount",
    "data",
    "stored",
    "processed",
    "analyzed",
    "using",
    "traditional",
    "ways",
    "let",
    "look",
    "challenges",
    "respect",
    "big",
    "data",
    "first",
    "challenge",
    "big",
    "data",
    "storage",
    "storing",
    "big",
    "data",
    "easy",
    "data",
    "generation",
    "endless",
    "addition",
    "storing",
    "unstructured",
    "data",
    "traditional",
    "databases",
    "great",
    "challenge",
    "unstructured",
    "data",
    "refers",
    "data",
    "photographs",
    "videos",
    "second",
    "challenge",
    "processing",
    "big",
    "data",
    "data",
    "useful",
    "us",
    "processed",
    "analyzed",
    "processing",
    "big",
    "data",
    "consumes",
    "lot",
    "time",
    "due",
    "size",
    "structure",
    "overcome",
    "challenges",
    "big",
    "data",
    "various",
    "frameworks",
    "hadoop",
    "cassandra",
    "spark",
    "let",
    "us",
    "quick",
    "look",
    "hadoop",
    "spark",
    "hadoop",
    "framework",
    "stores",
    "big",
    "data",
    "distributed",
    "way",
    "processes",
    "parallely",
    "think",
    "hadoop",
    "well",
    "hadoop",
    "uses",
    "distributed",
    "file",
    "system",
    "known",
    "hadoop",
    "distributed",
    "file",
    "system",
    "store",
    "big",
    "data",
    "huge",
    "file",
    "file",
    "broken",
    "smaller",
    "chunks",
    "stored",
    "various",
    "machines",
    "termed",
    "distributed",
    "storage",
    "mapreduce",
    "processing",
    "unit",
    "hadoop",
    "multiple",
    "machines",
    "working",
    "parallelly",
    "process",
    "big",
    "data",
    "thanks",
    "technique",
    "processing",
    "becomes",
    "easy",
    "fast",
    "let",
    "move",
    "spark",
    "spark",
    "framework",
    "responsible",
    "processing",
    "data",
    "batches",
    "real",
    "time",
    "spark",
    "used",
    "analyze",
    "data",
    "across",
    "various",
    "clusters",
    "computers",
    "understood",
    "big",
    "data",
    "hadoop",
    "spark",
    "let",
    "look",
    "different",
    "job",
    "roles",
    "domain",
    "career",
    "opportunities",
    "field",
    "limitless",
    "organizations",
    "using",
    "big",
    "data",
    "enhance",
    "products",
    "business",
    "decisions",
    "marketing",
    "effectiveness",
    "understand",
    "depth",
    "become",
    "big",
    "data",
    "engineer",
    "addition",
    "also",
    "look",
    "various",
    "uh",
    "skills",
    "required",
    "become",
    "hadoop",
    "developer",
    "spark",
    "developer",
    "big",
    "data",
    "architect",
    "starting",
    "big",
    "data",
    "engineer",
    "role",
    "know",
    "big",
    "data",
    "engineer",
    "well",
    "big",
    "data",
    "engineers",
    "professionals",
    "develop",
    "maintain",
    "test",
    "evaluate",
    "company",
    "big",
    "data",
    "infrastructure",
    "several",
    "responsibilities",
    "firstly",
    "responsible",
    "designing",
    "implementing",
    "software",
    "systems",
    "verify",
    "maintain",
    "systems",
    "ingestion",
    "processing",
    "data",
    "built",
    "robust",
    "systems",
    "extract",
    "transform",
    "load",
    "operations",
    "known",
    "etl",
    "process",
    "carried",
    "big",
    "data",
    "engineers",
    "also",
    "research",
    "various",
    "new",
    "methods",
    "obtain",
    "data",
    "improve",
    "quality",
    "big",
    "data",
    "engineers",
    "also",
    "responsible",
    "building",
    "data",
    "architectures",
    "meet",
    "business",
    "requirements",
    "provide",
    "solution",
    "integrating",
    "various",
    "tools",
    "programming",
    "languages",
    "addition",
    "responsibilities",
    "primary",
    "responsibility",
    "mine",
    "data",
    "plenty",
    "different",
    "sources",
    "build",
    "efficient",
    "business",
    "models",
    "lastly",
    "also",
    "work",
    "closely",
    "data",
    "architects",
    "data",
    "scientists",
    "data",
    "analysts",
    "acquire",
    "lot",
    "responsibilities",
    "let",
    "us",
    "look",
    "skills",
    "required",
    "achieve",
    "responsibilities",
    "see",
    "screens",
    "listed",
    "top",
    "top",
    "seven",
    "skills",
    "needed",
    "possessed",
    "big",
    "data",
    "engineer",
    "starting",
    "essential",
    "skill",
    "programming",
    "big",
    "data",
    "engineer",
    "needs",
    "experience",
    "one",
    "programming",
    "languages",
    "java",
    "plus",
    "python",
    "big",
    "data",
    "engineer",
    "also",
    "knowledge",
    "dbms",
    "sql",
    "understand",
    "data",
    "managed",
    "maintained",
    "database",
    "hence",
    "need",
    "know",
    "write",
    "sql",
    "queries",
    "rdbms",
    "systems",
    "commonly",
    "used",
    "database",
    "management",
    "systems",
    "big",
    "data",
    "engineering",
    "mysql",
    "oracle",
    "database",
    "microsoft",
    "sql",
    "server",
    "mentioned",
    "earlier",
    "carrying",
    "e",
    "carrying",
    "etl",
    "operation",
    "big",
    "data",
    "engineer",
    "responsibility",
    "need",
    "know",
    "construct",
    "well",
    "use",
    "data",
    "warehouse",
    "perform",
    "etl",
    "operations",
    "big",
    "data",
    "engineer",
    "continuously",
    "tasked",
    "extracting",
    "data",
    "various",
    "sources",
    "transforming",
    "meaningful",
    "information",
    "loading",
    "data",
    "storages",
    "tools",
    "used",
    "purpose",
    "talent",
    "ibm",
    "data",
    "stage",
    "pentaho",
    "informatica",
    "next",
    "fourth",
    "skill",
    "require",
    "knowledge",
    "operating",
    "systems",
    "big",
    "data",
    "tools",
    "run",
    "operating",
    "systems",
    "hence",
    "sound",
    "understanding",
    "unix",
    "linux",
    "windows",
    "solaris",
    "mandatory",
    "fifth",
    "skill",
    "require",
    "experience",
    "hadoop",
    "based",
    "analytics",
    "since",
    "haroop",
    "one",
    "commonly",
    "used",
    "big",
    "data",
    "engineering",
    "tools",
    "understood",
    "need",
    "experience",
    "apache",
    "hadoop",
    "based",
    "technologies",
    "like",
    "hdfs",
    "mapreduce",
    "apache",
    "pig",
    "hive",
    "apache",
    "hedge",
    "base",
    "sixth",
    "skill",
    "require",
    "worked",
    "processing",
    "frameworks",
    "like",
    "apache",
    "spark",
    "big",
    "data",
    "engineer",
    "would",
    "deal",
    "large",
    "volumes",
    "data",
    "need",
    "analytics",
    "engine",
    "like",
    "spark",
    "used",
    "batch",
    "processing",
    "spark",
    "process",
    "live",
    "streaming",
    "data",
    "several",
    "different",
    "sources",
    "like",
    "facebook",
    "instagram",
    "twitter",
    "final",
    "skill",
    "requirement",
    "experience",
    "data",
    "mining",
    "data",
    "modeling",
    "big",
    "data",
    "engineer",
    "would",
    "examine",
    "massive",
    "data",
    "discover",
    "patterns",
    "new",
    "information",
    "help",
    "create",
    "predictive",
    "models",
    "help",
    "make",
    "various",
    "business",
    "business",
    "decisions",
    "tools",
    "used",
    "rapidminer",
    "becca",
    "nime",
    "let",
    "talk",
    "big",
    "data",
    "engineer",
    "salary",
    "big",
    "data",
    "engineer",
    "average",
    "annual",
    "salary",
    "around",
    "hundred",
    "two",
    "thousand",
    "dollars",
    "per",
    "annum",
    "india",
    "big",
    "data",
    "engineer",
    "makes",
    "seven",
    "lakh",
    "rupees",
    "per",
    "annum",
    "first",
    "role",
    "big",
    "data",
    "engineer",
    "moving",
    "second",
    "role",
    "hadoop",
    "developer",
    "name",
    "suggests",
    "hadoop",
    "developer",
    "looks",
    "coding",
    "programming",
    "various",
    "hadoop",
    "applications",
    "job",
    "role",
    "less",
    "similar",
    "software",
    "developer",
    "moving",
    "skills",
    "required",
    "become",
    "hadoop",
    "developer",
    "first",
    "knowledge",
    "hadoop",
    "ecosystem",
    "hadoop",
    "developer",
    "knowledge",
    "hadoop",
    "ecosystem",
    "components",
    "include",
    "hedge",
    "base",
    "pig",
    "scoop",
    "flume",
    "uzi",
    "etc",
    "also",
    "data",
    "modeling",
    "experience",
    "olap",
    "oltp",
    "knowledge",
    "sql",
    "required",
    "like",
    "big",
    "data",
    "engineer",
    "hadoop",
    "developer",
    "must",
    "also",
    "know",
    "popular",
    "tools",
    "like",
    "pentaho",
    "informatica",
    "talent",
    "finally",
    "also",
    "well",
    "versed",
    "writing",
    "pig",
    "latin",
    "scripts",
    "jobs",
    "average",
    "annual",
    "salary",
    "hadoop",
    "developer",
    "nearly",
    "76",
    "000",
    "per",
    "annum",
    "india",
    "hadoop",
    "developer",
    "makes",
    "approximately",
    "4",
    "lakh",
    "57",
    "000",
    "rupees",
    "per",
    "annum",
    "moving",
    "third",
    "job",
    "role",
    "spark",
    "developer",
    "saw",
    "spark",
    "let",
    "understand",
    "spark",
    "developer",
    "spark",
    "developers",
    "create",
    "spark",
    "jobs",
    "using",
    "python",
    "scala",
    "data",
    "aggregation",
    "transformation",
    "also",
    "write",
    "analytics",
    "code",
    "design",
    "data",
    "processing",
    "pipelines",
    "spark",
    "developer",
    "needs",
    "following",
    "skills",
    "need",
    "know",
    "spark",
    "components",
    "spark",
    "core",
    "spark",
    "streaming",
    "spark",
    "machine",
    "learning",
    "library",
    "etc",
    "also",
    "know",
    "scripting",
    "languages",
    "like",
    "python",
    "perl",
    "like",
    "previous",
    "job",
    "roles",
    "spark",
    "developer",
    "also",
    "basic",
    "knowledge",
    "sequel",
    "queries",
    "database",
    "structure",
    "addition",
    "spark",
    "developer",
    "also",
    "expected",
    "fairly",
    "good",
    "understanding",
    "linux",
    "commands",
    "moving",
    "average",
    "annual",
    "salary",
    "spark",
    "developer",
    "nearly",
    "81",
    "000",
    "dollars",
    "per",
    "annum",
    "us",
    "india",
    "spark",
    "developer",
    "earns",
    "nearly",
    "5",
    "lakh",
    "87",
    "000",
    "rupees",
    "per",
    "annum",
    "let",
    "move",
    "final",
    "job",
    "role",
    "big",
    "data",
    "architect",
    "let",
    "understand",
    "big",
    "data",
    "architect",
    "well",
    "big",
    "data",
    "architect",
    "professional",
    "responsible",
    "designing",
    "planning",
    "big",
    "data",
    "systems",
    "also",
    "manage",
    "large",
    "scale",
    "development",
    "deployment",
    "hadoop",
    "applications",
    "moving",
    "skills",
    "required",
    "become",
    "big",
    "data",
    "architect",
    "first",
    "individual",
    "must",
    "advanced",
    "data",
    "mining",
    "data",
    "analysis",
    "skills",
    "big",
    "data",
    "architects",
    "also",
    "able",
    "implement",
    "use",
    "nosql",
    "database",
    "cloud",
    "computing",
    "techniques",
    "must",
    "also",
    "idea",
    "various",
    "big",
    "data",
    "technologies",
    "like",
    "hadoop",
    "mapreduce",
    "hbase",
    "hype",
    "finally",
    "big",
    "data",
    "architects",
    "must",
    "hold",
    "experience",
    "agile",
    "scrum",
    "frameworks",
    "let",
    "look",
    "average",
    "annual",
    "salary",
    "big",
    "data",
    "architect",
    "united",
    "states",
    "india",
    "big",
    "data",
    "architect",
    "earns",
    "whopping",
    "118",
    "thousand",
    "dollars",
    "per",
    "annum",
    "meanwhile",
    "india",
    "big",
    "data",
    "architect",
    "makes",
    "nearly",
    "19",
    "lakh",
    "rupees",
    "per",
    "annum",
    "huge",
    "numbers",
    "right",
    "understood",
    "job",
    "roles",
    "big",
    "data",
    "engineer",
    "hadoop",
    "developer",
    "spark",
    "developer",
    "big",
    "data",
    "architect",
    "let",
    "us",
    "look",
    "companies",
    "hiring",
    "professionals",
    "see",
    "screens",
    "ibm",
    "amazon",
    "american",
    "express",
    "netflix",
    "microsoft",
    "bosch",
    "name",
    "let",
    "us",
    "understand",
    "big",
    "data",
    "analytics",
    "required",
    "example",
    "listen",
    "music",
    "online",
    "take",
    "example",
    "spotify",
    "swedish",
    "audio",
    "streaming",
    "platform",
    "see",
    "big",
    "data",
    "analytics",
    "used",
    "spotify",
    "nearly",
    "96",
    "plus",
    "million",
    "users",
    "users",
    "generate",
    "tremendous",
    "amount",
    "data",
    "data",
    "like",
    "songs",
    "played",
    "repeatedly",
    "numerous",
    "likes",
    "shares",
    "user",
    "search",
    "history",
    "data",
    "termed",
    "big",
    "data",
    "respect",
    "spotify",
    "ever",
    "wondered",
    "spotify",
    "big",
    "data",
    "well",
    "spotify",
    "analyzes",
    "big",
    "data",
    "suggesting",
    "songs",
    "users",
    "sure",
    "might",
    "come",
    "across",
    "recommendation",
    "list",
    "made",
    "available",
    "one",
    "spotify",
    "one",
    "totally",
    "different",
    "recommendation",
    "list",
    "based",
    "likes",
    "past",
    "history",
    "like",
    "songs",
    "like",
    "listening",
    "playlists",
    "works",
    "something",
    "known",
    "recommendation",
    "system",
    "recommendation",
    "systems",
    "nothing",
    "data",
    "filtering",
    "tools",
    "collect",
    "data",
    "filter",
    "using",
    "various",
    "algorithms",
    "system",
    "ability",
    "accurately",
    "predict",
    "user",
    "would",
    "like",
    "listen",
    "next",
    "help",
    "big",
    "data",
    "analytics",
    "prediction",
    "helps",
    "users",
    "stay",
    "page",
    "longer",
    "time",
    "spotify",
    "engages",
    "users",
    "users",
    "need",
    "go",
    "searching",
    "different",
    "songs",
    "spotify",
    "readily",
    "provides",
    "variety",
    "songs",
    "according",
    "taste",
    "well",
    "big",
    "data",
    "analytics",
    "used",
    "spotify",
    "understood",
    "big",
    "data",
    "analytics",
    "required",
    "let",
    "us",
    "move",
    "next",
    "topic",
    "big",
    "data",
    "analytics",
    "another",
    "term",
    "need",
    "understand",
    "big",
    "data",
    "hear",
    "term",
    "big",
    "data",
    "many",
    "times",
    "know",
    "exactly",
    "big",
    "data",
    "means",
    "well",
    "understand",
    "term",
    "big",
    "data",
    "clearly",
    "right",
    "big",
    "data",
    "term",
    "data",
    "sets",
    "handled",
    "traditional",
    "computers",
    "tools",
    "due",
    "value",
    "volume",
    "velocity",
    "veracity",
    "defined",
    "massive",
    "amount",
    "data",
    "stored",
    "process",
    "analyzed",
    "using",
    "various",
    "traditional",
    "methods",
    "know",
    "much",
    "data",
    "generated",
    "every",
    "day",
    "every",
    "second",
    "even",
    "talk",
    "right",
    "millions",
    "data",
    "sources",
    "generate",
    "data",
    "rapid",
    "rate",
    "data",
    "sources",
    "present",
    "across",
    "world",
    "know",
    "social",
    "media",
    "sites",
    "generate",
    "lot",
    "data",
    "let",
    "take",
    "example",
    "facebook",
    "facebook",
    "generates",
    "500",
    "plus",
    "terabytes",
    "data",
    "every",
    "day",
    "data",
    "mainly",
    "generated",
    "terms",
    "photographs",
    "videos",
    "messages",
    "etc",
    "big",
    "data",
    "also",
    "contains",
    "data",
    "different",
    "formats",
    "like",
    "structured",
    "data",
    "data",
    "unstructured",
    "data",
    "data",
    "like",
    "excel",
    "sheets",
    "fall",
    "structured",
    "data",
    "data",
    "definite",
    "format",
    "emails",
    "fall",
    "pictures",
    "videos",
    "fall",
    "unstructured",
    "data",
    "data",
    "together",
    "make",
    "big",
    "data",
    "tough",
    "store",
    "process",
    "analyze",
    "big",
    "data",
    "using",
    "rtbms",
    "looked",
    "previous",
    "videos",
    "would",
    "know",
    "hadoop",
    "solution",
    "hadoop",
    "framework",
    "stores",
    "processes",
    "big",
    "data",
    "stores",
    "big",
    "data",
    "using",
    "distributed",
    "storage",
    "system",
    "processes",
    "big",
    "data",
    "using",
    "parallel",
    "processing",
    "method",
    "hence",
    "storing",
    "processing",
    "big",
    "data",
    "problem",
    "using",
    "hadoop",
    "big",
    "data",
    "raw",
    "form",
    "use",
    "us",
    "must",
    "try",
    "derive",
    "meaningful",
    "information",
    "order",
    "benefit",
    "big",
    "data",
    "know",
    "amazon",
    "uses",
    "big",
    "data",
    "monitor",
    "items",
    "fulfillment",
    "centers",
    "across",
    "globe",
    "think",
    "amazon",
    "well",
    "done",
    "analyzing",
    "big",
    "data",
    "known",
    "big",
    "data",
    "analytics",
    "big",
    "data",
    "analytics",
    "simple",
    "terms",
    "big",
    "data",
    "analytics",
    "defined",
    "process",
    "used",
    "extract",
    "meaningful",
    "information",
    "big",
    "data",
    "information",
    "could",
    "hidden",
    "patterns",
    "unknown",
    "correlations",
    "market",
    "trends",
    "using",
    "big",
    "data",
    "analytics",
    "many",
    "advantages",
    "used",
    "better",
    "decision",
    "making",
    "prevent",
    "fraudulent",
    "activities",
    "many",
    "others",
    "look",
    "four",
    "step",
    "step",
    "first",
    "start",
    "big",
    "data",
    "analytics",
    "used",
    "risk",
    "management",
    "bdo",
    "philippine",
    "banking",
    "company",
    "uses",
    "big",
    "data",
    "analytics",
    "risk",
    "management",
    "risk",
    "management",
    "important",
    "aspect",
    "organization",
    "especially",
    "field",
    "banking",
    "risk",
    "management",
    "analysis",
    "comprises",
    "series",
    "measures",
    "employed",
    "prevent",
    "sort",
    "unauthorized",
    "activities",
    "identifying",
    "fraudulent",
    "activities",
    "main",
    "concern",
    "bdo",
    "difficult",
    "bank",
    "identify",
    "fraudster",
    "long",
    "list",
    "suspects",
    "bdo",
    "adopted",
    "big",
    "data",
    "analytics",
    "held",
    "bank",
    "narrow",
    "entire",
    "list",
    "suspects",
    "thus",
    "organization",
    "able",
    "identify",
    "fraudster",
    "short",
    "time",
    "big",
    "data",
    "analytics",
    "used",
    "field",
    "banking",
    "risk",
    "management",
    "let",
    "us",
    "see",
    "big",
    "data",
    "analytics",
    "used",
    "product",
    "development",
    "innovations",
    "example",
    "aware",
    "cars",
    "right",
    "also",
    "know",
    "manufacture",
    "jet",
    "engines",
    "used",
    "across",
    "world",
    "interesting",
    "use",
    "big",
    "data",
    "analytics",
    "developing",
    "innovating",
    "engine",
    "new",
    "product",
    "always",
    "developed",
    "trial",
    "error",
    "method",
    "big",
    "data",
    "analytics",
    "used",
    "analyze",
    "engine",
    "design",
    "good",
    "bad",
    "also",
    "used",
    "analyze",
    "scope",
    "improvement",
    "based",
    "previous",
    "models",
    "future",
    "demands",
    "way",
    "big",
    "data",
    "analytics",
    "used",
    "designing",
    "product",
    "higher",
    "quality",
    "using",
    "big",
    "data",
    "analytics",
    "company",
    "save",
    "lot",
    "time",
    "team",
    "struggling",
    "arrive",
    "right",
    "conclusion",
    "big",
    "data",
    "used",
    "zero",
    "right",
    "data",
    "studied",
    "thus",
    "time",
    "spent",
    "product",
    "development",
    "less",
    "big",
    "data",
    "analytics",
    "helps",
    "quicker",
    "better",
    "decision",
    "making",
    "organizations",
    "process",
    "selecting",
    "course",
    "action",
    "various",
    "alternatives",
    "known",
    "decision",
    "making",
    "lot",
    "organizations",
    "take",
    "important",
    "decisions",
    "based",
    "data",
    "data",
    "driven",
    "business",
    "decisions",
    "make",
    "break",
    "company",
    "hence",
    "important",
    "analyze",
    "possibilities",
    "thoroughly",
    "quickly",
    "making",
    "important",
    "decisions",
    "let",
    "us",
    "try",
    "understand",
    "use",
    "case",
    "starbucks",
    "uses",
    "big",
    "data",
    "analytics",
    "making",
    "important",
    "decisions",
    "decide",
    "location",
    "new",
    "outlet",
    "using",
    "big",
    "data",
    "analytics",
    "choosing",
    "right",
    "location",
    "important",
    "factor",
    "organization",
    "wrong",
    "location",
    "able",
    "attract",
    "required",
    "amount",
    "customers",
    "positioning",
    "new",
    "outlet",
    "miles",
    "always",
    "make",
    "huge",
    "difference",
    "outlet",
    "especially",
    "one",
    "like",
    "starbucks",
    "various",
    "factors",
    "involved",
    "choosing",
    "right",
    "location",
    "new",
    "outlet",
    "example",
    "parking",
    "adequacy",
    "taken",
    "consideration",
    "would",
    "inconvenient",
    "people",
    "go",
    "store",
    "parking",
    "facility",
    "similarly",
    "factors",
    "considered",
    "visibility",
    "location",
    "accessibility",
    "economic",
    "factors",
    "population",
    "particular",
    "location",
    "also",
    "would",
    "look",
    "competition",
    "vicinity",
    "factors",
    "thoroughly",
    "analyzed",
    "making",
    "decision",
    "new",
    "outlet",
    "must",
    "started",
    "without",
    "analyzing",
    "factors",
    "would",
    "impossible",
    "us",
    "make",
    "wise",
    "decision",
    "using",
    "big",
    "data",
    "analytics",
    "consider",
    "factors",
    "analyze",
    "quickly",
    "thoroughly",
    "thus",
    "starbucks",
    "makes",
    "use",
    "big",
    "data",
    "analytics",
    "understand",
    "new",
    "location",
    "would",
    "fruitful",
    "finally",
    "look",
    "big",
    "data",
    "analytics",
    "used",
    "improve",
    "customer",
    "experience",
    "using",
    "example",
    "delta",
    "american",
    "airline",
    "uses",
    "big",
    "data",
    "analytics",
    "improve",
    "customer",
    "experiences",
    "increase",
    "global",
    "air",
    "travel",
    "necessary",
    "airline",
    "everything",
    "order",
    "provide",
    "good",
    "service",
    "experience",
    "customers",
    "delta",
    "airlines",
    "improves",
    "customer",
    "experience",
    "making",
    "use",
    "big",
    "data",
    "analytics",
    "done",
    "monitoring",
    "tweets",
    "give",
    "idea",
    "customers",
    "journey",
    "airline",
    "comes",
    "across",
    "negative",
    "tweet",
    "found",
    "airline",
    "fault",
    "airline",
    "goes",
    "ahead",
    "upgrades",
    "particular",
    "customer",
    "ticket",
    "happens",
    "customer",
    "able",
    "trust",
    "airlines",
    "without",
    "doubt",
    "customer",
    "choose",
    "delta",
    "next",
    "journey",
    "customer",
    "happy",
    "airlines",
    "able",
    "build",
    "good",
    "brand",
    "recognition",
    "thus",
    "see",
    "using",
    "analysis",
    "delta",
    "airlines",
    "able",
    "improve",
    "customer",
    "experience",
    "moving",
    "next",
    "topic",
    "life",
    "cycle",
    "big",
    "data",
    "analytics",
    "look",
    "various",
    "stages",
    "data",
    "analyzed",
    "scratch",
    "first",
    "stage",
    "business",
    "case",
    "evaluation",
    "stage",
    "motive",
    "behind",
    "analysis",
    "identified",
    "need",
    "understand",
    "analyzing",
    "know",
    "different",
    "parameters",
    "looked",
    "done",
    "clear",
    "us",
    "becomes",
    "much",
    "easier",
    "us",
    "proceed",
    "rest",
    "look",
    "various",
    "data",
    "sources",
    "gather",
    "data",
    "required",
    "analysis",
    "get",
    "required",
    "data",
    "see",
    "data",
    "received",
    "fit",
    "analysis",
    "data",
    "receive",
    "meaningful",
    "information",
    "surely",
    "corrupt",
    "data",
    "remove",
    "corrupt",
    "data",
    "pass",
    "entire",
    "data",
    "filtering",
    "stage",
    "stage",
    "corrupt",
    "data",
    "removed",
    "data",
    "minus",
    "corrupt",
    "data",
    "think",
    "data",
    "fit",
    "analysis",
    "well",
    "still",
    "figure",
    "data",
    "compatible",
    "tool",
    "using",
    "analysis",
    "find",
    "data",
    "incompatible",
    "first",
    "extract",
    "transform",
    "compatible",
    "form",
    "depending",
    "tool",
    "use",
    "next",
    "stage",
    "data",
    "fields",
    "integrated",
    "known",
    "data",
    "aggregation",
    "stage",
    "next",
    "stage",
    "analysis",
    "stage",
    "important",
    "stage",
    "life",
    "cycle",
    "big",
    "data",
    "analytics",
    "right",
    "step",
    "entire",
    "process",
    "evaluating",
    "data",
    "using",
    "various",
    "analytical",
    "statistical",
    "tools",
    "discover",
    "meaningful",
    "information",
    "done",
    "like",
    "discussed",
    "entire",
    "process",
    "deriving",
    "meaningful",
    "information",
    "data",
    "known",
    "analysis",
    "done",
    "stage",
    "result",
    "data",
    "analysis",
    "stage",
    "graphically",
    "communicated",
    "using",
    "tools",
    "like",
    "tableau",
    "power",
    "bi",
    "click",
    "view",
    "analysis",
    "result",
    "made",
    "available",
    "different",
    "business",
    "stakeholders",
    "various",
    "decision",
    "making",
    "entire",
    "life",
    "cycle",
    "big",
    "data",
    "analytics",
    "saw",
    "data",
    "analyzed",
    "scratch",
    "move",
    "important",
    "topic",
    "different",
    "types",
    "big",
    "data",
    "analytics",
    "well",
    "four",
    "different",
    "types",
    "big",
    "data",
    "analytics",
    "see",
    "types",
    "questions",
    "type",
    "tries",
    "answer",
    "descriptive",
    "analytics",
    "asks",
    "question",
    "happened",
    "diagnostic",
    "analytics",
    "asks",
    "happen",
    "predictive",
    "analytics",
    "asking",
    "happen",
    "prescriptive",
    "analytics",
    "questions",
    "asking",
    "solution",
    "look",
    "four",
    "one",
    "one",
    "use",
    "case",
    "first",
    "start",
    "descriptive",
    "analytics",
    "mentioned",
    "earlier",
    "descriptive",
    "analytics",
    "asks",
    "question",
    "happened",
    "defined",
    "type",
    "summarizes",
    "past",
    "data",
    "form",
    "understood",
    "humans",
    "type",
    "look",
    "past",
    "data",
    "arrive",
    "various",
    "conclusions",
    "example",
    "organization",
    "review",
    "performance",
    "using",
    "descriptive",
    "analytics",
    "analyzes",
    "past",
    "data",
    "revenue",
    "years",
    "arrives",
    "conclusion",
    "profit",
    "looking",
    "graph",
    "understand",
    "company",
    "running",
    "profit",
    "thus",
    "descriptive",
    "analytics",
    "helps",
    "us",
    "understand",
    "easily",
    "simply",
    "say",
    "descriptive",
    "analytics",
    "used",
    "creating",
    "various",
    "reports",
    "companies",
    "also",
    "tabulating",
    "various",
    "social",
    "media",
    "metrics",
    "like",
    "facebook",
    "likes",
    "tweets",
    "etc",
    "seen",
    "descriptive",
    "analytics",
    "let",
    "us",
    "look",
    "use",
    "case",
    "descriptive",
    "analytics",
    "dow",
    "chemical",
    "company",
    "analyzed",
    "past",
    "data",
    "using",
    "descriptive",
    "analytics",
    "able",
    "identify",
    "utilized",
    "space",
    "facility",
    "descriptive",
    "analytics",
    "helped",
    "space",
    "consolidation",
    "whole",
    "company",
    "able",
    "save",
    "nearly",
    "4",
    "million",
    "dollars",
    "annually",
    "see",
    "descriptive",
    "analytics",
    "helps",
    "us",
    "derive",
    "meaningful",
    "information",
    "past",
    "data",
    "also",
    "help",
    "companies",
    "cost",
    "reduction",
    "used",
    "wisely",
    "let",
    "us",
    "move",
    "next",
    "type",
    "diagnostic",
    "analytics",
    "diagnostic",
    "analytics",
    "asks",
    "question",
    "particular",
    "problem",
    "occurred",
    "see",
    "always",
    "ask",
    "question",
    "happen",
    "look",
    "root",
    "cause",
    "problem",
    "try",
    "understand",
    "occurred",
    "diagnostic",
    "analytics",
    "makes",
    "use",
    "various",
    "techniques",
    "data",
    "mining",
    "data",
    "discovery",
    "drill",
    "companies",
    "benefit",
    "type",
    "analytics",
    "helps",
    "look",
    "root",
    "cause",
    "problem",
    "next",
    "time",
    "problem",
    "arise",
    "company",
    "already",
    "knows",
    "happened",
    "arrive",
    "particular",
    "solution",
    "initsoft",
    "bi",
    "query",
    "tool",
    "example",
    "diagnostic",
    "analytics",
    "use",
    "query",
    "tool",
    "diagnostic",
    "analytics",
    "know",
    "diagnostic",
    "analytics",
    "required",
    "diagnostic",
    "analytics",
    "run",
    "example",
    "shows",
    "diagnostic",
    "analytics",
    "used",
    "us",
    "shop",
    "sites",
    "right",
    "ever",
    "added",
    "items",
    "cart",
    "ended",
    "buying",
    "yes",
    "us",
    "might",
    "done",
    "point",
    "organization",
    "tries",
    "understand",
    "customers",
    "end",
    "buying",
    "products",
    "although",
    "added",
    "carts",
    "understanding",
    "done",
    "help",
    "diagnostic",
    "analytics",
    "site",
    "wonders",
    "made",
    "online",
    "sales",
    "although",
    "good",
    "marketing",
    "strategy",
    "could",
    "various",
    "factors",
    "happened",
    "factors",
    "like",
    "shipping",
    "fee",
    "high",
    "page",
    "load",
    "correctly",
    "enough",
    "payment",
    "options",
    "available",
    "factors",
    "analyzed",
    "using",
    "diagnostic",
    "analytics",
    "company",
    "comes",
    "conclusion",
    "happened",
    "thus",
    "see",
    "root",
    "cause",
    "identified",
    "future",
    "problem",
    "occur",
    "let",
    "us",
    "move",
    "third",
    "type",
    "predictive",
    "analytics",
    "name",
    "suggests",
    "predictive",
    "analytics",
    "makes",
    "predictions",
    "future",
    "analyzes",
    "current",
    "historical",
    "facts",
    "make",
    "predictions",
    "future",
    "always",
    "asks",
    "question",
    "happen",
    "next",
    "predictive",
    "analytics",
    "used",
    "help",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "data",
    "mining",
    "analyze",
    "data",
    "used",
    "predicting",
    "customer",
    "trends",
    "market",
    "trends",
    "customer",
    "behavior",
    "etc",
    "solely",
    "works",
    "probability",
    "always",
    "tries",
    "understand",
    "happen",
    "next",
    "help",
    "past",
    "current",
    "information",
    "company",
    "like",
    "paypal",
    "260",
    "plus",
    "million",
    "accounts",
    "always",
    "need",
    "ensure",
    "online",
    "fraudulent",
    "unauthorized",
    "activities",
    "brought",
    "nil",
    "fear",
    "constant",
    "fraudulent",
    "activities",
    "always",
    "major",
    "concern",
    "paypal",
    "fraudulent",
    "activity",
    "occurs",
    "people",
    "lose",
    "trust",
    "company",
    "brings",
    "bad",
    "name",
    "brand",
    "inevitable",
    "fraudulent",
    "activities",
    "happen",
    "company",
    "like",
    "paypal",
    "one",
    "largest",
    "online",
    "payment",
    "processors",
    "world",
    "paypal",
    "uses",
    "analytics",
    "wisely",
    "prevent",
    "fraudulent",
    "activities",
    "minimize",
    "uses",
    "predictive",
    "analytics",
    "organization",
    "able",
    "analyze",
    "past",
    "data",
    "includes",
    "customer",
    "historical",
    "payment",
    "data",
    "customer",
    "behavior",
    "trend",
    "builds",
    "algorithm",
    "works",
    "predicting",
    "likely",
    "happen",
    "next",
    "respect",
    "transaction",
    "use",
    "big",
    "data",
    "algorithms",
    "system",
    "gauge",
    "transactions",
    "valid",
    "could",
    "potentially",
    "fraudulent",
    "activity",
    "paypal",
    "always",
    "ready",
    "precautions",
    "take",
    "protect",
    "clients",
    "fraudulent",
    "transactions",
    "move",
    "last",
    "type",
    "prescriptive",
    "analytics",
    "prescriptive",
    "analytics",
    "name",
    "suggests",
    "always",
    "prescribes",
    "solution",
    "particular",
    "problem",
    "problem",
    "something",
    "happening",
    "currently",
    "hence",
    "termed",
    "type",
    "always",
    "asks",
    "question",
    "solution",
    "prescriptive",
    "analytics",
    "related",
    "predictive",
    "descriptive",
    "analytics",
    "saw",
    "earlier",
    "descriptive",
    "analytics",
    "always",
    "asks",
    "question",
    "happened",
    "predictive",
    "analytics",
    "helps",
    "understand",
    "happen",
    "next",
    "help",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "prescriptive",
    "analytics",
    "helps",
    "arrive",
    "best",
    "solution",
    "particular",
    "problem",
    "various",
    "business",
    "rules",
    "algorithms",
    "computational",
    "modeling",
    "procedures",
    "used",
    "prescriptive",
    "analytics",
    "let",
    "us",
    "look",
    "prescriptive",
    "analytics",
    "used",
    "example",
    "understand",
    "prescriptive",
    "analytics",
    "used",
    "airline",
    "profit",
    "know",
    "book",
    "flight",
    "ticket",
    "price",
    "depends",
    "various",
    "factors",
    "internal",
    "external",
    "factors",
    "apart",
    "taxes",
    "seed",
    "selection",
    "factors",
    "like",
    "oil",
    "prices",
    "customer",
    "demand",
    "taken",
    "consideration",
    "flight",
    "sphere",
    "displayed",
    "prices",
    "change",
    "due",
    "availability",
    "demand",
    "holiday",
    "seasons",
    "time",
    "rates",
    "much",
    "higher",
    "normal",
    "days",
    "seasons",
    "like",
    "christmas",
    "school",
    "vacations",
    "also",
    "weekends",
    "rates",
    "much",
    "higher",
    "weekdays",
    "another",
    "factor",
    "determines",
    "flight",
    "fair",
    "destination",
    "depending",
    "place",
    "traveling",
    "flight",
    "fair",
    "adjusted",
    "accordingly",
    "quite",
    "places",
    "air",
    "traffic",
    "less",
    "places",
    "flight",
    "sphere",
    "also",
    "less",
    "prescriptive",
    "analytics",
    "analyzes",
    "factors",
    "discussed",
    "builds",
    "algorithm",
    "automatically",
    "adjust",
    "flight",
    "sphere",
    "airline",
    "able",
    "maximize",
    "profit",
    "four",
    "types",
    "analytics",
    "let",
    "us",
    "understand",
    "achieve",
    "use",
    "big",
    "data",
    "tools",
    "next",
    "topic",
    "various",
    "tools",
    "used",
    "big",
    "data",
    "analytics",
    "tools",
    "talking",
    "today",
    "hadoop",
    "mongodb",
    "talindi",
    "kafka",
    "cassandra",
    "spark",
    "storm",
    "look",
    "one",
    "one",
    "one",
    "first",
    "start",
    "hadoop",
    "speak",
    "big",
    "data",
    "first",
    "framework",
    "comes",
    "everyone",
    "mind",
    "hadoop",
    "mentioned",
    "earlier",
    "apache",
    "hadoop",
    "used",
    "store",
    "process",
    "big",
    "data",
    "distributed",
    "parallel",
    "fashion",
    "allows",
    "us",
    "process",
    "data",
    "fast",
    "hadoop",
    "uses",
    "mapreduce",
    "big",
    "high",
    "analyzing",
    "big",
    "data",
    "hadoop",
    "easily",
    "one",
    "famous",
    "big",
    "data",
    "tools",
    "let",
    "us",
    "move",
    "next",
    "one",
    "mongodb",
    "mongodb",
    "document",
    "oriented",
    "database",
    "ability",
    "deal",
    "large",
    "amount",
    "unstructured",
    "data",
    "processing",
    "data",
    "unstructured",
    "processing",
    "data",
    "sets",
    "change",
    "frequently",
    "done",
    "using",
    "mongodb",
    "talendi",
    "provides",
    "software",
    "services",
    "data",
    "integration",
    "data",
    "management",
    "cloud",
    "storage",
    "specializes",
    "big",
    "data",
    "integration",
    "talandi",
    "open",
    "studio",
    "free",
    "open",
    "source",
    "tool",
    "processing",
    "data",
    "easily",
    "big",
    "data",
    "environment",
    "cassandra",
    "used",
    "widely",
    "effective",
    "management",
    "large",
    "amounts",
    "data",
    "similar",
    "hadoop",
    "feature",
    "fault",
    "tolerance",
    "data",
    "automatically",
    "replicated",
    "multiple",
    "nodes",
    "cassandra",
    "preferred",
    "processing",
    "spark",
    "another",
    "tool",
    "used",
    "data",
    "processing",
    "data",
    "processing",
    "engine",
    "developed",
    "process",
    "data",
    "way",
    "faster",
    "hadoop",
    "mapreduce",
    "done",
    "way",
    "spark",
    "processing",
    "main",
    "memory",
    "data",
    "nodes",
    "thus",
    "prevents",
    "unnecessary",
    "input",
    "output",
    "overheads",
    "disk",
    "whereas",
    "mapreduce",
    "disk",
    "based",
    "hence",
    "spark",
    "proves",
    "faster",
    "hadoop",
    "mapreduce",
    "storm",
    "free",
    "big",
    "data",
    "computational",
    "system",
    "done",
    "real",
    "time",
    "one",
    "easiest",
    "tools",
    "big",
    "data",
    "analysis",
    "used",
    "programming",
    "language",
    "feature",
    "makes",
    "storm",
    "simple",
    "use",
    "finally",
    "look",
    "another",
    "big",
    "data",
    "tool",
    "known",
    "kafka",
    "kafka",
    "distributed",
    "streaming",
    "platform",
    "developed",
    "linkedin",
    "later",
    "given",
    "apache",
    "software",
    "foundation",
    "used",
    "provide",
    "analytics",
    "result",
    "also",
    "used",
    "fault",
    "tolerant",
    "storage",
    "big",
    "data",
    "analytics",
    "tools",
    "let",
    "us",
    "move",
    "last",
    "topic",
    "today",
    "big",
    "data",
    "application",
    "domains",
    "look",
    "various",
    "sectors",
    "big",
    "data",
    "analytics",
    "actively",
    "used",
    "first",
    "sector",
    "merely",
    "45",
    "percent",
    "world",
    "online",
    "create",
    "lot",
    "data",
    "every",
    "second",
    "big",
    "data",
    "used",
    "smartly",
    "field",
    "predicting",
    "customer",
    "trend",
    "forecasting",
    "demands",
    "adjusting",
    "price",
    "online",
    "retailers",
    "opportunity",
    "create",
    "better",
    "shopping",
    "experience",
    "generate",
    "higher",
    "sales",
    "big",
    "data",
    "analytics",
    "used",
    "correctly",
    "big",
    "data",
    "automatically",
    "lead",
    "better",
    "marketing",
    "strategy",
    "meaningful",
    "insights",
    "need",
    "derived",
    "order",
    "make",
    "right",
    "decisions",
    "analyzing",
    "big",
    "data",
    "personalized",
    "marketing",
    "campaigns",
    "result",
    "better",
    "higher",
    "sales",
    "field",
    "education",
    "depending",
    "market",
    "requirements",
    "new",
    "courses",
    "developed",
    "market",
    "requirement",
    "needs",
    "analyzed",
    "correctly",
    "respect",
    "scope",
    "course",
    "accordingly",
    "scope",
    "needs",
    "developed",
    "point",
    "developing",
    "course",
    "scope",
    "future",
    "hence",
    "analyze",
    "market",
    "requirement",
    "develop",
    "new",
    "courses",
    "use",
    "big",
    "data",
    "analytics",
    "number",
    "uses",
    "big",
    "data",
    "analytics",
    "field",
    "health",
    "care",
    "one",
    "predict",
    "patient",
    "health",
    "issue",
    "help",
    "previous",
    "medical",
    "history",
    "big",
    "data",
    "analytics",
    "determine",
    "likely",
    "particular",
    "health",
    "issue",
    "future",
    "example",
    "spotify",
    "saw",
    "previously",
    "showed",
    "big",
    "data",
    "analytics",
    "used",
    "provide",
    "personalized",
    "recommendation",
    "list",
    "users",
    "similarly",
    "field",
    "media",
    "entertainment",
    "big",
    "data",
    "analytics",
    "used",
    "understand",
    "demands",
    "shows",
    "songs",
    "movies",
    "deliver",
    "personalized",
    "recommendation",
    "list",
    "saw",
    "spotify",
    "big",
    "data",
    "analytics",
    "used",
    "field",
    "banking",
    "saw",
    "previously",
    "use",
    "cases",
    "big",
    "data",
    "analytics",
    "used",
    "risk",
    "management",
    "addition",
    "risk",
    "management",
    "also",
    "used",
    "analyze",
    "customer",
    "income",
    "spend",
    "patterns",
    "help",
    "bank",
    "predict",
    "particular",
    "customer",
    "going",
    "choose",
    "bank",
    "offers",
    "loans",
    "credit",
    "card",
    "schemes",
    "way",
    "bank",
    "able",
    "identify",
    "right",
    "customer",
    "interested",
    "offers",
    "noticed",
    "telecom",
    "companies",
    "begun",
    "embrace",
    "big",
    "data",
    "gain",
    "profit",
    "big",
    "data",
    "analytics",
    "helps",
    "analyzing",
    "network",
    "traffic",
    "call",
    "data",
    "records",
    "also",
    "improve",
    "service",
    "quality",
    "improve",
    "customer",
    "experience",
    "let",
    "us",
    "look",
    "big",
    "data",
    "analytics",
    "used",
    "governments",
    "across",
    "world",
    "field",
    "law",
    "enforcement",
    "big",
    "data",
    "analytics",
    "applied",
    "analyze",
    "available",
    "data",
    "understand",
    "crime",
    "patterns",
    "intelligence",
    "services",
    "use",
    "predictive",
    "analytics",
    "focus",
    "crime",
    "could",
    "committed",
    "durham",
    "police",
    "department",
    "able",
    "reduce",
    "crime",
    "rate",
    "using",
    "big",
    "data",
    "analytics",
    "help",
    "data",
    "police",
    "could",
    "identify",
    "target",
    "go",
    "petrol",
    "investigate",
    "crimes",
    "big",
    "data",
    "analytics",
    "help",
    "discover",
    "patterns",
    "crime",
    "emerging",
    "area",
    "move",
    "applications",
    "let",
    "quick",
    "look",
    "big",
    "data",
    "market",
    "revenue",
    "forecast",
    "worldwide",
    "2011",
    "graph",
    "represents",
    "revenue",
    "billion",
    "us",
    "dollars",
    "represents",
    "years",
    "seen",
    "clearly",
    "graph",
    "big",
    "data",
    "grown",
    "2019",
    "statistics",
    "predict",
    "growth",
    "continue",
    "even",
    "future",
    "growth",
    "made",
    "possible",
    "numerous",
    "companies",
    "use",
    "big",
    "data",
    "various",
    "domains",
    "boost",
    "revenue",
    "look",
    "applications",
    "first",
    "big",
    "data",
    "application",
    "look",
    "weather",
    "forecast",
    "imagine",
    "sudden",
    "storm",
    "even",
    "prepared",
    "would",
    "terrifying",
    "situation",
    "dealing",
    "calamities",
    "hurricane",
    "storms",
    "floods",
    "would",
    "inconvenient",
    "caught",
    "guard",
    "solution",
    "tool",
    "predicts",
    "weather",
    "coming",
    "days",
    "well",
    "advance",
    "tool",
    "needs",
    "accurate",
    "make",
    "tool",
    "big",
    "data",
    "used",
    "big",
    "data",
    "help",
    "well",
    "allows",
    "us",
    "gather",
    "information",
    "required",
    "predict",
    "weather",
    "information",
    "climate",
    "change",
    "details",
    "wind",
    "direction",
    "precipitation",
    "previous",
    "weather",
    "reports",
    "data",
    "collected",
    "becomes",
    "easier",
    "us",
    "spot",
    "trend",
    "identify",
    "going",
    "happen",
    "next",
    "analyzing",
    "big",
    "data",
    "weather",
    "prediction",
    "engine",
    "works",
    "analysis",
    "predicts",
    "weather",
    "every",
    "region",
    "across",
    "world",
    "given",
    "time",
    "using",
    "tool",
    "well",
    "prepared",
    "face",
    "climate",
    "change",
    "natural",
    "calamity",
    "let",
    "take",
    "example",
    "landslide",
    "try",
    "understand",
    "big",
    "data",
    "used",
    "tackle",
    "situation",
    "predicting",
    "landslide",
    "difficult",
    "basic",
    "warning",
    "signs",
    "lack",
    "prediction",
    "cause",
    "huge",
    "damage",
    "life",
    "property",
    "challenge",
    "studied",
    "university",
    "melbourne",
    "developed",
    "tool",
    "capable",
    "predicting",
    "landslide",
    "tool",
    "predicts",
    "boundary",
    "landslide",
    "likely",
    "occur",
    "two",
    "weeks",
    "magical",
    "tool",
    "works",
    "big",
    "data",
    "applied",
    "mathematics",
    "accurate",
    "prediction",
    "like",
    "made",
    "two",
    "weeks",
    "save",
    "lives",
    "help",
    "relocating",
    "people",
    "particular",
    "region",
    "also",
    "gives",
    "us",
    "insight",
    "magnitude",
    "upcoming",
    "destruction",
    "big",
    "data",
    "used",
    "weather",
    "forecast",
    "predicting",
    "natural",
    "calamities",
    "across",
    "world",
    "let",
    "us",
    "move",
    "next",
    "application",
    "big",
    "data",
    "application",
    "field",
    "media",
    "entertainment",
    "media",
    "entertainment",
    "industry",
    "massive",
    "one",
    "leveraging",
    "big",
    "data",
    "produce",
    "results",
    "boost",
    "revenue",
    "company",
    "let",
    "us",
    "see",
    "different",
    "ways",
    "big",
    "data",
    "used",
    "industry",
    "ever",
    "noticed",
    "come",
    "across",
    "relevant",
    "advertisements",
    "social",
    "media",
    "sites",
    "mailboxes",
    "well",
    "done",
    "analyzing",
    "data",
    "previous",
    "browsing",
    "history",
    "purchase",
    "data",
    "publishers",
    "display",
    "like",
    "form",
    "ads",
    "turn",
    "catch",
    "interest",
    "looking",
    "next",
    "customer",
    "sentiment",
    "analysis",
    "customers",
    "important",
    "company",
    "happier",
    "customer",
    "greater",
    "company",
    "revenue",
    "big",
    "data",
    "helps",
    "gathering",
    "emotions",
    "customer",
    "posts",
    "messages",
    "conversations",
    "etc",
    "emotions",
    "analyzed",
    "arrive",
    "conclusion",
    "regarding",
    "customer",
    "satisfaction",
    "customer",
    "unhappy",
    "company",
    "strives",
    "better",
    "next",
    "time",
    "provides",
    "customers",
    "better",
    "experience",
    "purchasing",
    "item",
    "site",
    "watching",
    "videos",
    "entertainment",
    "site",
    "might",
    "noticed",
    "segment",
    "says",
    "recommended",
    "list",
    "list",
    "personalized",
    "list",
    "made",
    "available",
    "analyzing",
    "data",
    "previous",
    "watch",
    "history",
    "subscriptions",
    "likes",
    "recommendation",
    "engine",
    "tool",
    "filters",
    "analyzes",
    "data",
    "provides",
    "list",
    "would",
    "likely",
    "interested",
    "site",
    "able",
    "retain",
    "engage",
    "customer",
    "longer",
    "time",
    "next",
    "customer",
    "churn",
    "analysis",
    "simple",
    "words",
    "customer",
    "churn",
    "happens",
    "customer",
    "stops",
    "subscription",
    "service",
    "predicting",
    "preventing",
    "paramount",
    "importance",
    "organization",
    "analyzing",
    "behavioral",
    "patterns",
    "previously",
    "churned",
    "customers",
    "organization",
    "identify",
    "current",
    "customers",
    "likely",
    "churn",
    "analyzing",
    "data",
    "organization",
    "implement",
    "effective",
    "programs",
    "customer",
    "retention",
    "let",
    "us",
    "look",
    "use",
    "case",
    "starbucks",
    "big",
    "data",
    "effectively",
    "used",
    "starbucks",
    "app",
    "17",
    "million",
    "users",
    "use",
    "app",
    "imagine",
    "much",
    "data",
    "generate",
    "data",
    "form",
    "coffee",
    "buying",
    "habits",
    "store",
    "visit",
    "time",
    "purchase",
    "data",
    "fed",
    "app",
    "customer",
    "enters",
    "new",
    "starbucks",
    "location",
    "system",
    "analyzes",
    "data",
    "provided",
    "preferred",
    "order",
    "app",
    "also",
    "suggests",
    "new",
    "products",
    "customer",
    "addition",
    "also",
    "provide",
    "personalized",
    "offer",
    "discounts",
    "special",
    "occasions",
    "moving",
    "next",
    "sector",
    "healthcare",
    "one",
    "important",
    "sectors",
    "big",
    "data",
    "widely",
    "used",
    "save",
    "lives",
    "available",
    "big",
    "data",
    "medical",
    "researchers",
    "done",
    "effectively",
    "performed",
    "accurately",
    "analyzing",
    "previous",
    "medical",
    "histories",
    "new",
    "treatments",
    "medicines",
    "discovered",
    "cure",
    "found",
    "even",
    "incurable",
    "diseases",
    "cases",
    "one",
    "medication",
    "need",
    "effective",
    "every",
    "patient",
    "hence",
    "personal",
    "care",
    "important",
    "personal",
    "care",
    "provided",
    "patient",
    "depending",
    "past",
    "medical",
    "history",
    "individuals",
    "medical",
    "history",
    "along",
    "body",
    "parameters",
    "analyzed",
    "personal",
    "attention",
    "given",
    "know",
    "medical",
    "treatments",
    "pocket",
    "friendly",
    "every",
    "time",
    "medical",
    "treatment",
    "taken",
    "amount",
    "increases",
    "reduced",
    "readmissions",
    "brought",
    "analyzing",
    "data",
    "precisely",
    "deliver",
    "efficient",
    "result",
    "turn",
    "prevent",
    "patient",
    "readmission",
    "frequently",
    "globalization",
    "came",
    "increase",
    "ease",
    "infectious",
    "diseases",
    "spread",
    "widely",
    "based",
    "geography",
    "demographics",
    "big",
    "data",
    "helps",
    "predicting",
    "outbreak",
    "epidemic",
    "viruses",
    "likely",
    "occur",
    "american",
    "healthcare",
    "company",
    "united",
    "healthcare",
    "uses",
    "big",
    "data",
    "detect",
    "online",
    "medical",
    "fraud",
    "activities",
    "payment",
    "unauthorized",
    "benefits",
    "intentional",
    "misrepresentation",
    "data",
    "healthcare",
    "company",
    "runs",
    "disease",
    "management",
    "programs",
    "success",
    "rates",
    "programs",
    "predicted",
    "using",
    "big",
    "data",
    "depending",
    "patients",
    "respond",
    "next",
    "sector",
    "look",
    "logistics",
    "logistics",
    "looks",
    "process",
    "transportation",
    "storage",
    "goods",
    "movement",
    "product",
    "supplier",
    "consumer",
    "important",
    "big",
    "data",
    "used",
    "make",
    "process",
    "faster",
    "efficient",
    "important",
    "factor",
    "logistics",
    "time",
    "taken",
    "products",
    "reach",
    "destination",
    "achieve",
    "minimum",
    "time",
    "sensors",
    "within",
    "vehicle",
    "analyze",
    "fastest",
    "route",
    "analysis",
    "based",
    "various",
    "data",
    "weather",
    "traffic",
    "list",
    "orders",
    "fastest",
    "route",
    "obtained",
    "delivery",
    "time",
    "reduced",
    "capacity",
    "planning",
    "another",
    "factor",
    "needs",
    "taken",
    "consideration",
    "details",
    "regarding",
    "workforce",
    "number",
    "vehicles",
    "analyzed",
    "thoroughly",
    "vehicle",
    "allocated",
    "different",
    "route",
    "done",
    "need",
    "many",
    "trucks",
    "travel",
    "direction",
    "pointless",
    "depending",
    "analysis",
    "available",
    "workforce",
    "resources",
    "decision",
    "taken",
    "big",
    "data",
    "analytics",
    "also",
    "finds",
    "use",
    "managing",
    "warehouses",
    "efficiently",
    "analysis",
    "along",
    "tracking",
    "sensors",
    "provide",
    "information",
    "regarding",
    "underutilized",
    "space",
    "results",
    "efficient",
    "resource",
    "allocation",
    "eventually",
    "reduces",
    "cost",
    "customer",
    "satisfaction",
    "important",
    "logistics",
    "like",
    "sector",
    "customer",
    "reactions",
    "analyzed",
    "available",
    "data",
    "eventually",
    "create",
    "instant",
    "feedback",
    "loop",
    "happy",
    "customer",
    "always",
    "help",
    "company",
    "gain",
    "customers",
    "let",
    "us",
    "look",
    "use",
    "case",
    "ups",
    "know",
    "ups",
    "one",
    "biggest",
    "shipping",
    "company",
    "world",
    "huge",
    "customer",
    "database",
    "work",
    "data",
    "every",
    "minute",
    "ups",
    "uses",
    "big",
    "data",
    "gather",
    "different",
    "kinds",
    "data",
    "regarding",
    "weather",
    "traffic",
    "jams",
    "geography",
    "locations",
    "collecting",
    "data",
    "analyze",
    "discover",
    "best",
    "fastest",
    "route",
    "destination",
    "addition",
    "also",
    "use",
    "big",
    "data",
    "change",
    "routes",
    "real",
    "time",
    "efficiently",
    "ups",
    "leverages",
    "big",
    "data",
    "next",
    "interesting",
    "sector",
    "travel",
    "tourism",
    "sector",
    "global",
    "tourism",
    "market",
    "expected",
    "grow",
    "near",
    "future",
    "big",
    "data",
    "used",
    "various",
    "ways",
    "sector",
    "let",
    "us",
    "look",
    "hotels",
    "increase",
    "revenue",
    "adjusting",
    "room",
    "tariffs",
    "depending",
    "peak",
    "seasons",
    "holiday",
    "seasons",
    "festive",
    "seasons",
    "tourism",
    "industry",
    "uses",
    "data",
    "anticipate",
    "demand",
    "maximize",
    "revenue",
    "big",
    "data",
    "also",
    "used",
    "resorts",
    "hotels",
    "analyze",
    "various",
    "details",
    "regarding",
    "competitors",
    "analysis",
    "result",
    "helps",
    "incorporate",
    "good",
    "facilities",
    "competitors",
    "providing",
    "hotel",
    "able",
    "flourish",
    "customer",
    "always",
    "comes",
    "back",
    "offered",
    "good",
    "packages",
    "basic",
    "ones",
    "looking",
    "customer",
    "past",
    "travel",
    "history",
    "likes",
    "preferences",
    "hotels",
    "provide",
    "customers",
    "personalized",
    "experiences",
    "interest",
    "highly",
    "investing",
    "area",
    "could",
    "hub",
    "tourism",
    "wise",
    "countries",
    "use",
    "big",
    "data",
    "examine",
    "tourism",
    "activities",
    "country",
    "turn",
    "helps",
    "discover",
    "new",
    "fruitful",
    "investment",
    "opportunities",
    "let",
    "us",
    "look",
    "one",
    "best",
    "online",
    "homestay",
    "networks",
    "airbnb",
    "see",
    "big",
    "data",
    "used",
    "airbnb",
    "undoubtedly",
    "provides",
    "customers",
    "best",
    "accommodation",
    "across",
    "world",
    "big",
    "data",
    "used",
    "analyze",
    "different",
    "kinds",
    "available",
    "properties",
    "depending",
    "customer",
    "preferences",
    "pricing",
    "keywords",
    "previous",
    "customers",
    "ratings",
    "experiences",
    "airbnb",
    "filters",
    "best",
    "result",
    "big",
    "data",
    "works",
    "magic",
    "yet",
    "move",
    "final",
    "sector",
    "government",
    "law",
    "enforcement",
    "sector",
    "maintaining",
    "law",
    "order",
    "utmost",
    "importance",
    "government",
    "huge",
    "task",
    "big",
    "data",
    "plays",
    "active",
    "role",
    "addition",
    "also",
    "helps",
    "governments",
    "bring",
    "new",
    "policies",
    "schemes",
    "welfare",
    "citizens",
    "police",
    "department",
    "able",
    "predict",
    "criminal",
    "activities",
    "way",
    "happens",
    "analyzing",
    "big",
    "data",
    "information",
    "previous",
    "crime",
    "records",
    "particular",
    "region",
    "safety",
    "aspect",
    "region",
    "analyzing",
    "factors",
    "able",
    "predict",
    "activity",
    "breaks",
    "law",
    "order",
    "region",
    "governments",
    "able",
    "tackle",
    "unemployment",
    "great",
    "extent",
    "using",
    "big",
    "data",
    "analyzing",
    "number",
    "students",
    "graduating",
    "every",
    "year",
    "number",
    "relevant",
    "job",
    "openings",
    "government",
    "idea",
    "unemployment",
    "rate",
    "country",
    "take",
    "necessary",
    "measures",
    "tackle",
    "next",
    "factor",
    "poverty",
    "large",
    "countries",
    "difficult",
    "analyze",
    "area",
    "requires",
    "attention",
    "development",
    "big",
    "data",
    "analytics",
    "makes",
    "easier",
    "governments",
    "discover",
    "areas",
    "poverty",
    "gradually",
    "decreases",
    "areas",
    "begin",
    "develop",
    "governments",
    "always",
    "lookout",
    "better",
    "development",
    "public",
    "survey",
    "voices",
    "opinion",
    "country",
    "citizens",
    "analyzing",
    "data",
    "collected",
    "surveys",
    "help",
    "governments",
    "build",
    "better",
    "policies",
    "services",
    "benefit",
    "citizens",
    "let",
    "us",
    "move",
    "use",
    "case",
    "know",
    "new",
    "york",
    "police",
    "department",
    "uses",
    "big",
    "data",
    "analytics",
    "protect",
    "citizens",
    "department",
    "prevents",
    "identifies",
    "crimes",
    "analyzing",
    "huge",
    "amount",
    "data",
    "includes",
    "fingerprints",
    "certain",
    "emails",
    "records",
    "previous",
    "police",
    "investigations",
    "analyzing",
    "data",
    "meaningful",
    "insights",
    "drawn",
    "help",
    "police",
    "taking",
    "required",
    "preventive",
    "measures",
    "crimes",
    "talk",
    "evolution",
    "big",
    "data",
    "known",
    "data",
    "evolved",
    "last",
    "five",
    "years",
    "like",
    "never",
    "fact",
    "going",
    "big",
    "data",
    "understanding",
    "solutions",
    "need",
    "rush",
    "towards",
    "big",
    "data",
    "technology",
    "solution",
    "would",
    "like",
    "ask",
    "question",
    "take",
    "couple",
    "minutes",
    "think",
    "organizations",
    "interested",
    "big",
    "data",
    "certain",
    "rush",
    "industry",
    "everyone",
    "would",
    "want",
    "ramp",
    "current",
    "infrastructure",
    "would",
    "want",
    "working",
    "technologies",
    "allow",
    "use",
    "big",
    "data",
    "think",
    "happening",
    "organizations",
    "interested",
    "think",
    "start",
    "thinking",
    "organizations",
    "past",
    "organizations",
    "done",
    "organizations",
    "interested",
    "big",
    "data",
    "learn",
    "big",
    "data",
    "always",
    "look",
    "internet",
    "check",
    "use",
    "cases",
    "organizations",
    "failed",
    "use",
    "legacy",
    "systems",
    "relational",
    "databases",
    "work",
    "data",
    "requirements",
    "recent",
    "past",
    "five",
    "years",
    "recent",
    "decade",
    "happened",
    "organizations",
    "started",
    "understanding",
    "value",
    "data",
    "decided",
    "ignore",
    "data",
    "uneconomical",
    "talk",
    "different",
    "platforms",
    "data",
    "generated",
    "take",
    "example",
    "social",
    "media",
    "like",
    "twitter",
    "facebook",
    "instagram",
    "whatsapp",
    "youtube",
    "various",
    "portals",
    "say",
    "ebay",
    "amazon",
    "flipkart",
    "various",
    "tech",
    "giants",
    "google",
    "oracle",
    "sap",
    "amazon",
    "microsoft",
    "lots",
    "data",
    "getting",
    "generated",
    "every",
    "day",
    "every",
    "business",
    "sector",
    "point",
    "organizations",
    "slowly",
    "started",
    "realizing",
    "would",
    "interested",
    "working",
    "data",
    "question",
    "asked",
    "organizations",
    "interested",
    "big",
    "data",
    "might",
    "already",
    "answered",
    "thought",
    "organizations",
    "interested",
    "precise",
    "analysis",
    "want",
    "work",
    "different",
    "formats",
    "data",
    "structured",
    "unstructured",
    "data",
    "organizations",
    "interested",
    "gaining",
    "insights",
    "finding",
    "hidden",
    "treasure",
    "big",
    "data",
    "main",
    "reason",
    "organizations",
    "interested",
    "big",
    "data",
    "various",
    "use",
    "cases",
    "various",
    "use",
    "cases",
    "compare",
    "organizations",
    "past",
    "50",
    "50",
    "years",
    "handling",
    "huge",
    "amount",
    "data",
    "working",
    "huge",
    "volume",
    "data",
    "question",
    "worked",
    "data",
    "worked",
    "portion",
    "used",
    "store",
    "data",
    "used",
    "something",
    "store",
    "data",
    "happening",
    "changing",
    "talk",
    "businesses",
    "avoid",
    "talking",
    "dynamism",
    "involved",
    "organization",
    "would",
    "want",
    "solution",
    "allows",
    "store",
    "data",
    "store",
    "huge",
    "amount",
    "data",
    "capture",
    "process",
    "analyze",
    "also",
    "look",
    "data",
    "give",
    "value",
    "data",
    "organizations",
    "looking",
    "solutions",
    "let",
    "look",
    "facts",
    "convince",
    "would",
    "convince",
    "data",
    "exploding",
    "needs",
    "attention",
    "right",
    "55",
    "billion",
    "messages",
    "billion",
    "photos",
    "sent",
    "day",
    "whatsapp",
    "300",
    "hours",
    "video",
    "uploaded",
    "every",
    "minute",
    "youtube",
    "guys",
    "know",
    "youtube",
    "second",
    "largest",
    "search",
    "engine",
    "google",
    "every",
    "minute",
    "users",
    "send",
    "million",
    "messages",
    "watch",
    "million",
    "videos",
    "facebook",
    "walmart",
    "handles",
    "1",
    "million",
    "customer",
    "transactions",
    "every",
    "hour",
    "google",
    "40",
    "000",
    "search",
    "queries",
    "performed",
    "google",
    "per",
    "second",
    "million",
    "searches",
    "day",
    "fact",
    "could",
    "also",
    "say",
    "lot",
    "times",
    "people",
    "loading",
    "google",
    "page",
    "basically",
    "check",
    "internet",
    "connection",
    "however",
    "also",
    "generating",
    "data",
    "idc",
    "reports",
    "2025",
    "data",
    "quarter",
    "data",
    "2025",
    "volume",
    "digital",
    "data",
    "increase",
    "163",
    "zeta",
    "bytes",
    "even",
    "talking",
    "gigabytes",
    "terabytes",
    "anymore",
    "talking",
    "petabytes",
    "exabytes",
    "zeta",
    "bytes",
    "zeta",
    "bytes",
    "means",
    "10",
    "power",
    "21",
    "bytes",
    "data",
    "evolved",
    "talk",
    "different",
    "companies",
    "would",
    "want",
    "use",
    "data",
    "take",
    "business",
    "decisions",
    "would",
    "want",
    "collect",
    "data",
    "store",
    "analyze",
    "would",
    "interested",
    "drawing",
    "insights",
    "business",
    "simple",
    "example",
    "facebook",
    "work",
    "data",
    "go",
    "facebook",
    "could",
    "always",
    "check",
    "google",
    "typing",
    "companies",
    "using",
    "big",
    "data",
    "say",
    "companies",
    "using",
    "big",
    "data",
    "able",
    "find",
    "list",
    "different",
    "companies",
    "using",
    "big",
    "data",
    "different",
    "use",
    "cases",
    "various",
    "sources",
    "find",
    "could",
    "also",
    "search",
    "solution",
    "hadoop",
    "discuss",
    "later",
    "could",
    "always",
    "say",
    "companies",
    "using",
    "hadoop",
    "take",
    "wiki",
    "page",
    "basically",
    "help",
    "know",
    "different",
    "companies",
    "using",
    "solution",
    "called",
    "hadoop",
    "okay",
    "coming",
    "back",
    "discussing",
    "organizations",
    "interested",
    "big",
    "data",
    "discussed",
    "gaining",
    "insights",
    "would",
    "want",
    "use",
    "data",
    "find",
    "hidden",
    "information",
    "probably",
    "ignored",
    "earlier",
    "take",
    "example",
    "rdbms",
    "biggest",
    "drawback",
    "using",
    "rdbms",
    "might",
    "think",
    "rdbms",
    "known",
    "stability",
    "consistency",
    "organizations",
    "would",
    "interested",
    "storing",
    "data",
    "oracle",
    "db2",
    "mysql",
    "microsoft",
    "sql",
    "server",
    "many",
    "years",
    "changed",
    "talk",
    "rdbms",
    "first",
    "question",
    "would",
    "ask",
    "access",
    "100",
    "data",
    "online",
    "rdbms",
    "answer",
    "would",
    "10",
    "20",
    "30",
    "percent",
    "data",
    "online",
    "rest",
    "data",
    "would",
    "archived",
    "means",
    "organization",
    "interested",
    "working",
    "data",
    "would",
    "move",
    "data",
    "archived",
    "storage",
    "processing",
    "layer",
    "would",
    "involve",
    "bandwidth",
    "consumption",
    "one",
    "biggest",
    "drawbacks",
    "rdbms",
    "access",
    "100",
    "data",
    "online",
    "many",
    "cases",
    "organizations",
    "started",
    "realizing",
    "data",
    "ignoring",
    "uneconomical",
    "hidden",
    "value",
    "never",
    "exploited",
    "read",
    "presentation",
    "somewhere",
    "said",
    "torture",
    "data",
    "confess",
    "anything",
    "value",
    "data",
    "organizations",
    "realized",
    "recent",
    "past",
    "take",
    "example",
    "facebook",
    "shows",
    "facebook",
    "big",
    "data",
    "come",
    "big",
    "data",
    "let",
    "understand",
    "use",
    "case",
    "facebook",
    "collects",
    "huge",
    "volumes",
    "user",
    "data",
    "whether",
    "sms",
    "whether",
    "likes",
    "whether",
    "advertisements",
    "whether",
    "features",
    "people",
    "liking",
    "photographs",
    "even",
    "user",
    "profiles",
    "collecting",
    "data",
    "providing",
    "portal",
    "people",
    "use",
    "connect",
    "facebook",
    "also",
    "accumulating",
    "huge",
    "volume",
    "data",
    "way",
    "beyond",
    "petabytes",
    "would",
    "also",
    "interested",
    "analyzing",
    "data",
    "one",
    "reasons",
    "would",
    "would",
    "want",
    "personalize",
    "experience",
    "take",
    "example",
    "personalized",
    "news",
    "feed",
    "depending",
    "user",
    "behavior",
    "depending",
    "user",
    "likes",
    "user",
    "would",
    "want",
    "know",
    "recommend",
    "personalized",
    "news",
    "feed",
    "every",
    "particular",
    "user",
    "one",
    "example",
    "facebook",
    "data",
    "take",
    "example",
    "photo",
    "tag",
    "suggestions",
    "log",
    "facebook",
    "account",
    "could",
    "also",
    "get",
    "suggestions",
    "different",
    "friends",
    "would",
    "like",
    "connect",
    "would",
    "want",
    "tag",
    "could",
    "known",
    "others",
    "examples",
    "show",
    "facebook",
    "uses",
    "data",
    "follows",
    "flashback",
    "collection",
    "photos",
    "posts",
    "receive",
    "comments",
    "likes",
    "okay",
    "something",
    "called",
    "voted",
    "used",
    "2016",
    "elections",
    "reminders",
    "directions",
    "tell",
    "users",
    "time",
    "place",
    "polling",
    "also",
    "something",
    "called",
    "safety",
    "checks",
    "incidents",
    "earthquake",
    "hurricane",
    "mass",
    "shooting",
    "facebook",
    "gives",
    "safety",
    "checks",
    "examples",
    "facebook",
    "using",
    "big",
    "data",
    "brings",
    "us",
    "question",
    "big",
    "data",
    "example",
    "discussed",
    "one",
    "company",
    "making",
    "use",
    "data",
    "accumulated",
    "companies",
    "social",
    "media",
    "oriented",
    "like",
    "facebook",
    "data",
    "important",
    "take",
    "example",
    "ibm",
    "take",
    "example",
    "jpmorgan",
    "chase",
    "take",
    "example",
    "ge",
    "organization",
    "collecting",
    "huge",
    "amount",
    "data",
    "would",
    "want",
    "gather",
    "insights",
    "would",
    "want",
    "analyze",
    "data",
    "would",
    "want",
    "precise",
    "building",
    "services",
    "solutions",
    "take",
    "care",
    "customers",
    "big",
    "data",
    "big",
    "data",
    "basically",
    "term",
    "used",
    "describe",
    "data",
    "large",
    "complex",
    "store",
    "traditional",
    "databases",
    "gave",
    "example",
    "storing",
    "data",
    "also",
    "data",
    "also",
    "means",
    "lot",
    "dynamism",
    "involved",
    "change",
    "underlying",
    "storage",
    "handle",
    "kind",
    "data",
    "comes",
    "get",
    "let",
    "understand",
    "big",
    "data",
    "big",
    "data",
    "basically",
    "term",
    "given",
    "categorize",
    "data",
    "different",
    "characteristics",
    "organizations",
    "would",
    "want",
    "big",
    "data",
    "stored",
    "processed",
    "analyzed",
    "get",
    "whatever",
    "useful",
    "information",
    "get",
    "data",
    "five",
    "v",
    "big",
    "data",
    "volume",
    "velocity",
    "variety",
    "value",
    "velocity",
    "although",
    "five",
    "v",
    "v",
    "also",
    "categorize",
    "data",
    "big",
    "data",
    "volatility",
    "validity",
    "viscosity",
    "virality",
    "data",
    "okay",
    "five",
    "v",
    "big",
    "data",
    "data",
    "one",
    "characteristics",
    "considered",
    "big",
    "data",
    "including",
    "ways",
    "mentioned",
    "volume",
    "basically",
    "means",
    "incredible",
    "amount",
    "data",
    "huge",
    "volumes",
    "data",
    "data",
    "generated",
    "every",
    "second",
    "could",
    "used",
    "batch",
    "processing",
    "could",
    "used",
    "stream",
    "processing",
    "okay",
    "might",
    "data",
    "generated",
    "different",
    "kind",
    "devices",
    "like",
    "cell",
    "phones",
    "social",
    "media",
    "websites",
    "online",
    "transactions",
    "variable",
    "devices",
    "servers",
    "days",
    "iot",
    "also",
    "talking",
    "data",
    "getting",
    "generated",
    "via",
    "internet",
    "things",
    "could",
    "different",
    "devices",
    "could",
    "communicating",
    "could",
    "getting",
    "data",
    "radars",
    "leaders",
    "even",
    "camera",
    "sensors",
    "huge",
    "volume",
    "data",
    "getting",
    "generated",
    "talking",
    "data",
    "huge",
    "volume",
    "getting",
    "generated",
    "constantly",
    "accumulated",
    "period",
    "time",
    "would",
    "say",
    "big",
    "data",
    "velocity",
    "one",
    "important",
    "aspect",
    "big",
    "data",
    "speed",
    "data",
    "getting",
    "generated",
    "think",
    "stock",
    "markets",
    "think",
    "social",
    "media",
    "websites",
    "think",
    "online",
    "surveys",
    "marketing",
    "campaigns",
    "airline",
    "industry",
    "data",
    "getting",
    "generated",
    "lot",
    "speed",
    "becomes",
    "difficult",
    "capture",
    "collect",
    "process",
    "cure",
    "mine",
    "analyze",
    "data",
    "certainly",
    "talking",
    "big",
    "data",
    "next",
    "aspect",
    "big",
    "data",
    "variety",
    "talk",
    "structured",
    "data",
    "data",
    "unstructured",
    "data",
    "would",
    "like",
    "ask",
    "question",
    "difference",
    "call",
    "data",
    "structured",
    "unstructured",
    "let",
    "look",
    "example",
    "theoretically",
    "discuss",
    "always",
    "would",
    "like",
    "use",
    "examples",
    "let",
    "look",
    "log",
    "file",
    "let",
    "see",
    "look",
    "log",
    "file",
    "would",
    "say",
    "kind",
    "data",
    "highlighted",
    "one",
    "answer",
    "would",
    "structured",
    "data",
    "specific",
    "delimiters",
    "space",
    "data",
    "separated",
    "space",
    "hundred",
    "thousand",
    "million",
    "rows",
    "similar",
    "kind",
    "data",
    "could",
    "certainly",
    "store",
    "table",
    "could",
    "predefined",
    "schema",
    "store",
    "data",
    "would",
    "call",
    "one",
    "highlighted",
    "structured",
    "look",
    "portion",
    "would",
    "look",
    "combination",
    "kind",
    "data",
    "data",
    "pattern",
    "data",
    "example",
    "data",
    "would",
    "predefined",
    "structure",
    "store",
    "data",
    "probably",
    "pattern",
    "data",
    "would",
    "break",
    "structure",
    "look",
    "data",
    "would",
    "certainly",
    "call",
    "unstructured",
    "data",
    "clear",
    "schema",
    "define",
    "data",
    "mean",
    "variety",
    "data",
    "structured",
    "data",
    "basically",
    "schema",
    "format",
    "could",
    "easily",
    "understood",
    "could",
    "like",
    "xml",
    "json",
    "even",
    "excel",
    "sheets",
    "could",
    "data",
    "structured",
    "unstructured",
    "talk",
    "unstructured",
    "talking",
    "absence",
    "schema",
    "format",
    "schema",
    "hard",
    "analyze",
    "brings",
    "challenges",
    "next",
    "aspect",
    "value",
    "value",
    "refers",
    "ability",
    "turn",
    "data",
    "useful",
    "business",
    "would",
    "lot",
    "data",
    "collected",
    "mentioned",
    "previous",
    "slides",
    "right",
    "would",
    "lot",
    "data",
    "wrangling",
    "data",
    "cleaning",
    "data",
    "happening",
    "finally",
    "would",
    "want",
    "draw",
    "value",
    "data",
    "data",
    "collected",
    "percentage",
    "data",
    "gives",
    "us",
    "value",
    "data",
    "give",
    "value",
    "would",
    "use",
    "aspect",
    "big",
    "data",
    "right",
    "veracity",
    "means",
    "quality",
    "data",
    "billions",
    "dollars",
    "lost",
    "every",
    "year",
    "organizations",
    "data",
    "collected",
    "good",
    "quality",
    "probably",
    "collected",
    "lot",
    "data",
    "erroneous",
    "take",
    "example",
    "autonomous",
    "driving",
    "projects",
    "happening",
    "europe",
    "car",
    "fleets",
    "road",
    "collecting",
    "data",
    "via",
    "radar",
    "sensors",
    "camera",
    "sensors",
    "data",
    "processed",
    "train",
    "algorithms",
    "realized",
    "sometimes",
    "data",
    "collected",
    "missing",
    "values",
    "might",
    "appropriate",
    "lot",
    "errors",
    "process",
    "collecting",
    "data",
    "becomes",
    "repetitive",
    "task",
    "quality",
    "data",
    "good",
    "one",
    "example",
    "take",
    "example",
    "healthcare",
    "industry",
    "stock",
    "markets",
    "financial",
    "institutions",
    "extracting",
    "loads",
    "data",
    "useful",
    "data",
    "messy",
    "poor",
    "quality",
    "basically",
    "means",
    "velocity",
    "important",
    "v",
    "big",
    "data",
    "apart",
    "veracity",
    "volume",
    "variety",
    "velocity",
    "value",
    "v",
    "viscosity",
    "dense",
    "data",
    "validity",
    "data",
    "still",
    "valid",
    "volatility",
    "data",
    "volatile",
    "virality",
    "data",
    "viral",
    "different",
    "v",
    "categorize",
    "data",
    "big",
    "data",
    "would",
    "like",
    "talk",
    "big",
    "data",
    "case",
    "study",
    "taken",
    "example",
    "google",
    "obviously",
    "one",
    "companies",
    "churning",
    "working",
    "huge",
    "amount",
    "data",
    "actually",
    "said",
    "compare",
    "one",
    "grain",
    "sand",
    "one",
    "byte",
    "data",
    "google",
    "processing",
    "google",
    "handling",
    "whole",
    "worlds",
    "sand",
    "every",
    "week",
    "kind",
    "data",
    "google",
    "processing",
    "early",
    "2000",
    "since",
    "number",
    "internet",
    "users",
    "started",
    "growing",
    "google",
    "also",
    "faced",
    "lot",
    "problems",
    "storing",
    "increasing",
    "user",
    "data",
    "using",
    "traditional",
    "servers",
    "manage",
    "challenge",
    "google",
    "started",
    "facing",
    "could",
    "use",
    "traditional",
    "data",
    "server",
    "store",
    "data",
    "well",
    "yes",
    "could",
    "right",
    "storage",
    "devices",
    "getting",
    "cheaper",
    "day",
    "day",
    "much",
    "time",
    "take",
    "retrieve",
    "data",
    "seek",
    "time",
    "time",
    "taken",
    "read",
    "process",
    "data",
    "thousands",
    "search",
    "queries",
    "raised",
    "per",
    "second",
    "doubt",
    "could",
    "say",
    "millions",
    "billions",
    "queries",
    "raised",
    "per",
    "second",
    "every",
    "query",
    "read",
    "100",
    "mbs",
    "data",
    "consumed",
    "tens",
    "billions",
    "cpu",
    "cycles",
    "based",
    "queries",
    "requirement",
    "wanted",
    "large",
    "distributed",
    "highly",
    "fault",
    "tolerant",
    "file",
    "system",
    "large",
    "store",
    "capture",
    "process",
    "huge",
    "amount",
    "data",
    "distributed",
    "could",
    "rely",
    "one",
    "server",
    "even",
    "multiple",
    "disks",
    "stacked",
    "efficient",
    "choice",
    "would",
    "happen",
    "particular",
    "machine",
    "failed",
    "would",
    "happen",
    "whole",
    "server",
    "needed",
    "distributed",
    "storage",
    "distributed",
    "computing",
    "environment",
    "needed",
    "something",
    "highly",
    "fault",
    "tolerant",
    "right",
    "requirement",
    "google",
    "solution",
    "came",
    "result",
    "gfs",
    "google",
    "file",
    "system",
    "let",
    "look",
    "gfs",
    "works",
    "normally",
    "particular",
    "linux",
    "system",
    "linux",
    "server",
    "would",
    "file",
    "system",
    "would",
    "set",
    "processes",
    "would",
    "set",
    "files",
    "directories",
    "could",
    "store",
    "data",
    "gfs",
    "different",
    "facilitate",
    "gfs",
    "could",
    "store",
    "huge",
    "amount",
    "data",
    "architecture",
    "architecture",
    "one",
    "master",
    "multiple",
    "chunk",
    "servers",
    "could",
    "say",
    "slave",
    "servers",
    "slave",
    "machines",
    "master",
    "machine",
    "contain",
    "metadata",
    "contain",
    "data",
    "data",
    "say",
    "metadata",
    "talking",
    "information",
    "data",
    "chung",
    "servers",
    "slave",
    "machines",
    "could",
    "storing",
    "data",
    "distributed",
    "fashion",
    "client",
    "api",
    "application",
    "would",
    "want",
    "read",
    "data",
    "would",
    "first",
    "contact",
    "master",
    "server",
    "would",
    "contact",
    "machine",
    "master",
    "process",
    "running",
    "client",
    "would",
    "place",
    "request",
    "reading",
    "data",
    "showing",
    "interest",
    "reading",
    "data",
    "internally",
    "requesting",
    "metadata",
    "api",
    "application",
    "would",
    "want",
    "know",
    "read",
    "data",
    "master",
    "server",
    "metadata",
    "whether",
    "ram",
    "disk",
    "discuss",
    "later",
    "master",
    "server",
    "would",
    "metadata",
    "would",
    "know",
    "chunk",
    "servers",
    "slave",
    "machines",
    "data",
    "stored",
    "distributed",
    "fashion",
    "master",
    "would",
    "respond",
    "back",
    "metadata",
    "information",
    "client",
    "client",
    "could",
    "use",
    "information",
    "read",
    "write",
    "slave",
    "machines",
    "actually",
    "data",
    "stored",
    "process",
    "set",
    "processes",
    "work",
    "together",
    "make",
    "gfs",
    "say",
    "chunk",
    "server",
    "would",
    "basically",
    "files",
    "getting",
    "divided",
    "fixed",
    "size",
    "chunks",
    "would",
    "get",
    "divided",
    "would",
    "kind",
    "chunk",
    "size",
    "block",
    "size",
    "would",
    "determine",
    "file",
    "bigger",
    "chunk",
    "size",
    "would",
    "split",
    "smaller",
    "chunks",
    "distributed",
    "across",
    "chunk",
    "servers",
    "slave",
    "machines",
    "file",
    "smaller",
    "would",
    "still",
    "use",
    "one",
    "chunk",
    "block",
    "get",
    "stored",
    "underlying",
    "slave",
    "machines",
    "junk",
    "servers",
    "slave",
    "machines",
    "ones",
    "actually",
    "store",
    "data",
    "local",
    "disks",
    "linux",
    "files",
    "client",
    "interacting",
    "master",
    "metadata",
    "interacting",
    "chunk",
    "servers",
    "read",
    "write",
    "operations",
    "would",
    "one",
    "would",
    "externally",
    "connecting",
    "cluster",
    "would",
    "look",
    "master",
    "would",
    "obviously",
    "receiving",
    "kind",
    "heartbeats",
    "chunk",
    "servers",
    "know",
    "status",
    "receive",
    "information",
    "form",
    "packets",
    "would",
    "let",
    "master",
    "know",
    "machines",
    "available",
    "storage",
    "machines",
    "already",
    "data",
    "master",
    "would",
    "build",
    "metadata",
    "within",
    "files",
    "would",
    "broken",
    "chunks",
    "example",
    "look",
    "file",
    "one",
    "broken",
    "chunk",
    "one",
    "chunk",
    "two",
    "file",
    "two",
    "one",
    "chunk",
    "one",
    "portion",
    "file",
    "two",
    "residing",
    "chunk",
    "server",
    "also",
    "lets",
    "us",
    "know",
    "kind",
    "auto",
    "replication",
    "file",
    "system",
    "right",
    "data",
    "getting",
    "stored",
    "chunk",
    "could",
    "data",
    "64",
    "mb",
    "chunk",
    "size",
    "could",
    "changed",
    "based",
    "data",
    "size",
    "google",
    "file",
    "system",
    "basic",
    "size",
    "chunk",
    "64",
    "mb",
    "chunk",
    "would",
    "replicated",
    "multiple",
    "servers",
    "default",
    "replication",
    "3",
    "could",
    "increased",
    "decreased",
    "per",
    "requirement",
    "would",
    "also",
    "mean",
    "particular",
    "slave",
    "machine",
    "chunk",
    "server",
    "would",
    "die",
    "would",
    "get",
    "killed",
    "would",
    "crash",
    "would",
    "never",
    "data",
    "loss",
    "replica",
    "data",
    "residing",
    "failed",
    "machine",
    "would",
    "still",
    "available",
    "slave",
    "server",
    "chunk",
    "server",
    "slave",
    "machine",
    "helped",
    "google",
    "store",
    "process",
    "huge",
    "volumes",
    "data",
    "distributed",
    "manner",
    "fault",
    "tolerant",
    "distributed",
    "scalable",
    "storage",
    "could",
    "allow",
    "store",
    "huge",
    "amount",
    "data",
    "one",
    "example",
    "actually",
    "led",
    "solution",
    "today",
    "call",
    "hadoop",
    "talk",
    "big",
    "data",
    "would",
    "like",
    "ask",
    "questions",
    "talking",
    "rdbms",
    "case",
    "take",
    "example",
    "something",
    "like",
    "nasa",
    "working",
    "project",
    "called",
    "set",
    "eye",
    "search",
    "extraterrestrial",
    "intelligence",
    "project",
    "looking",
    "solution",
    "take",
    "care",
    "problem",
    "problem",
    "would",
    "roughly",
    "send",
    "waves",
    "space",
    "capture",
    "waves",
    "back",
    "analyze",
    "data",
    "find",
    "extraterrestrial",
    "object",
    "space",
    "two",
    "options",
    "could",
    "either",
    "huge",
    "server",
    "built",
    "could",
    "take",
    "care",
    "storing",
    "data",
    "processing",
    "could",
    "go",
    "volunteer",
    "computing",
    "volunteer",
    "computing",
    "basically",
    "means",
    "could",
    "lot",
    "people",
    "volunteering",
    "part",
    "project",
    "would",
    "turn",
    "would",
    "donating",
    "ram",
    "storage",
    "machines",
    "using",
    "would",
    "happen",
    "basically",
    "download",
    "kind",
    "patch",
    "machine",
    "would",
    "run",
    "screen",
    "saver",
    "user",
    "using",
    "machine",
    "portion",
    "data",
    "could",
    "transferred",
    "machines",
    "intermittent",
    "storage",
    "processing",
    "using",
    "ram",
    "sounds",
    "interesting",
    "sounds",
    "easy",
    "however",
    "would",
    "challenges",
    "right",
    "think",
    "security",
    "think",
    "integrity",
    "problems",
    "bigger",
    "much",
    "requirement",
    "bandwidth",
    "thing",
    "happens",
    "rdbms",
    "would",
    "move",
    "data",
    "archived",
    "solution",
    "processing",
    "layer",
    "would",
    "consume",
    "huge",
    "amount",
    "bandwidth",
    "big",
    "data",
    "brings",
    "challenges",
    "huge",
    "amount",
    "data",
    "getting",
    "generated",
    "every",
    "day",
    "biggest",
    "challenge",
    "storing",
    "huge",
    "volume",
    "data",
    "especially",
    "data",
    "getting",
    "generated",
    "lot",
    "variety",
    "different",
    "kind",
    "formats",
    "could",
    "viral",
    "could",
    "lot",
    "value",
    "nobody",
    "looked",
    "veracity",
    "data",
    "primary",
    "problem",
    "would",
    "handling",
    "huge",
    "volume",
    "data",
    "variety",
    "data",
    "would",
    "bring",
    "challenges",
    "storing",
    "legacy",
    "systems",
    "processing",
    "data",
    "required",
    "would",
    "suggest",
    "need",
    "think",
    "difference",
    "reading",
    "data",
    "processing",
    "data",
    "reading",
    "might",
    "mean",
    "bringing",
    "data",
    "disk",
    "io",
    "operations",
    "processing",
    "would",
    "mean",
    "reading",
    "data",
    "probably",
    "transformations",
    "extracting",
    "useful",
    "information",
    "storing",
    "format",
    "probably",
    "different",
    "format",
    "processing",
    "massive",
    "volume",
    "data",
    "second",
    "challenge",
    "organizations",
    "store",
    "big",
    "data",
    "would",
    "eventually",
    "want",
    "use",
    "process",
    "gather",
    "insights",
    "processing",
    "extracting",
    "insights",
    "big",
    "data",
    "would",
    "take",
    "huge",
    "amount",
    "time",
    "unless",
    "efficient",
    "solution",
    "handle",
    "process",
    "big",
    "data",
    "securing",
    "data",
    "concern",
    "organizations",
    "right",
    "encryption",
    "big",
    "data",
    "difficult",
    "perform",
    "would",
    "think",
    "different",
    "compression",
    "mechanisms",
    "would",
    "also",
    "mean",
    "decompressing",
    "data",
    "would",
    "also",
    "mean",
    "could",
    "take",
    "hit",
    "cpu",
    "cycles",
    "disk",
    "usage",
    "providing",
    "user",
    "authentication",
    "every",
    "team",
    "member",
    "could",
    "also",
    "dangerous",
    "led",
    "hadoop",
    "solution",
    "big",
    "data",
    "brings",
    "challenges",
    "big",
    "data",
    "brings",
    "benefits",
    "solution",
    "hadoop",
    "hadoop",
    "open",
    "source",
    "framework",
    "storing",
    "data",
    "running",
    "applications",
    "clusters",
    "commodity",
    "hardware",
    "hadoop",
    "open",
    "source",
    "framework",
    "discuss",
    "two",
    "main",
    "components",
    "hadoop",
    "would",
    "good",
    "look",
    "link",
    "suggesting",
    "earlier",
    "companies",
    "using",
    "hadoop",
    "person",
    "would",
    "interested",
    "learning",
    "big",
    "data",
    "start",
    "somewhere",
    "could",
    "list",
    "different",
    "companies",
    "kind",
    "setup",
    "hadoop",
    "kind",
    "processing",
    "using",
    "called",
    "hadoop",
    "clusters",
    "process",
    "fact",
    "store",
    "capture",
    "process",
    "huge",
    "amount",
    "data",
    "another",
    "link",
    "would",
    "suggest",
    "looking",
    "different",
    "distributions",
    "hadoop",
    "person",
    "interested",
    "learning",
    "big",
    "data",
    "know",
    "different",
    "distributions",
    "hadoop",
    "linux",
    "different",
    "distributions",
    "like",
    "ubuntu",
    "centos",
    "red",
    "hat",
    "susie",
    "debian",
    "way",
    "different",
    "distributions",
    "hadoop",
    "look",
    "wiki",
    "page",
    "link",
    "talks",
    "products",
    "include",
    "apache",
    "hadoop",
    "derivative",
    "works",
    "commercial",
    "support",
    "basically",
    "means",
    "apache",
    "hadoop",
    "sole",
    "products",
    "called",
    "release",
    "apache",
    "hadoop",
    "come",
    "open",
    "source",
    "community",
    "various",
    "distributions",
    "like",
    "amazon",
    "web",
    "services",
    "cloud",
    "era",
    "hortonworks",
    "ibm",
    "big",
    "inside",
    "mapper",
    "different",
    "distributions",
    "hadoop",
    "basically",
    "distributions",
    "depending",
    "using",
    "core",
    "apache",
    "hadoop",
    "brief",
    "say",
    "vendors",
    "take",
    "apache",
    "hadoop",
    "package",
    "within",
    "cluster",
    "management",
    "solution",
    "users",
    "intend",
    "use",
    "apache",
    "hadoop",
    "would",
    "difficulties",
    "setting",
    "cluster",
    "setting",
    "framework",
    "could",
    "use",
    "distribution",
    "cluster",
    "installation",
    "solutions",
    "cluster",
    "management",
    "solution",
    "easily",
    "plan",
    "deploy",
    "install",
    "manage",
    "cluster",
    "let",
    "rewind",
    "days",
    "world",
    "turned",
    "digital",
    "back",
    "miniscule",
    "amounts",
    "data",
    "generated",
    "relatively",
    "sluggish",
    "pace",
    "data",
    "mostly",
    "documents",
    "form",
    "rows",
    "columns",
    "storing",
    "processing",
    "data",
    "much",
    "trouble",
    "single",
    "storage",
    "unit",
    "processor",
    "combination",
    "would",
    "job",
    "years",
    "passed",
    "internet",
    "took",
    "world",
    "storm",
    "giving",
    "rise",
    "tons",
    "data",
    "generated",
    "multitude",
    "forms",
    "formats",
    "every",
    "microsecond",
    "unstructured",
    "data",
    "available",
    "form",
    "emails",
    "images",
    "audio",
    "video",
    "name",
    "data",
    "became",
    "collectively",
    "known",
    "big",
    "data",
    "although",
    "fascinating",
    "became",
    "nearly",
    "impossible",
    "handle",
    "big",
    "data",
    "storage",
    "unit",
    "processor",
    "combination",
    "obviously",
    "enough",
    "solution",
    "multiple",
    "storage",
    "units",
    "processors",
    "undoubtedly",
    "need",
    "hour",
    "concept",
    "incorporated",
    "framework",
    "hadoop",
    "could",
    "store",
    "process",
    "vast",
    "amounts",
    "data",
    "efficiently",
    "using",
    "cluster",
    "commodity",
    "hardware",
    "hadoop",
    "consisted",
    "three",
    "components",
    "specifically",
    "designed",
    "work",
    "big",
    "data",
    "order",
    "capitalize",
    "data",
    "first",
    "step",
    "storing",
    "first",
    "component",
    "hadoop",
    "storage",
    "unit",
    "hadoop",
    "distributed",
    "file",
    "system",
    "hdfs",
    "storing",
    "massive",
    "data",
    "one",
    "computer",
    "unfeasible",
    "hence",
    "data",
    "distributed",
    "amongst",
    "many",
    "computers",
    "stored",
    "blocks",
    "600",
    "megabytes",
    "data",
    "stored",
    "hdfs",
    "splits",
    "data",
    "multiple",
    "blocks",
    "data",
    "stored",
    "several",
    "data",
    "nodes",
    "cluster",
    "128",
    "megabytes",
    "default",
    "size",
    "block",
    "hence",
    "600",
    "megabytes",
    "split",
    "four",
    "blocks",
    "b",
    "c",
    "128",
    "megabytes",
    "remaining",
    "88",
    "megabytes",
    "last",
    "block",
    "e",
    "might",
    "wondering",
    "one",
    "data",
    "node",
    "crashes",
    "lose",
    "specific",
    "piece",
    "data",
    "well",
    "beauty",
    "hdfs",
    "hdfs",
    "makes",
    "copies",
    "data",
    "stores",
    "across",
    "multiple",
    "systems",
    "example",
    "block",
    "created",
    "replicated",
    "replication",
    "factor",
    "three",
    "stored",
    "different",
    "data",
    "nodes",
    "termed",
    "replication",
    "method",
    "data",
    "lost",
    "cost",
    "even",
    "one",
    "data",
    "node",
    "crashes",
    "making",
    "hdfs",
    "fault",
    "tolerant",
    "storing",
    "data",
    "successfully",
    "needs",
    "processed",
    "second",
    "component",
    "hadoop",
    "mapreduce",
    "comes",
    "play",
    "traditional",
    "data",
    "processing",
    "method",
    "entire",
    "data",
    "would",
    "processed",
    "single",
    "machine",
    "single",
    "processor",
    "consumed",
    "time",
    "inefficient",
    "especially",
    "processing",
    "large",
    "volumes",
    "variety",
    "data",
    "overcome",
    "mapreduce",
    "splits",
    "data",
    "parts",
    "processes",
    "separately",
    "different",
    "data",
    "nodes",
    "individual",
    "results",
    "aggregated",
    "give",
    "final",
    "output",
    "let",
    "try",
    "count",
    "number",
    "occurrences",
    "words",
    "taking",
    "example",
    "first",
    "input",
    "split",
    "five",
    "separate",
    "parts",
    "based",
    "full",
    "stops",
    "next",
    "step",
    "mapper",
    "phase",
    "occurrence",
    "word",
    "counted",
    "allocated",
    "number",
    "depending",
    "words",
    "similar",
    "words",
    "shuffled",
    "sorted",
    "grouped",
    "following",
    "reducer",
    "phase",
    "grouped",
    "words",
    "given",
    "account",
    "finally",
    "output",
    "displayed",
    "aggregating",
    "results",
    "done",
    "writing",
    "simple",
    "program",
    "similarly",
    "mapreduce",
    "processes",
    "part",
    "big",
    "data",
    "individually",
    "sums",
    "result",
    "end",
    "improves",
    "load",
    "balancing",
    "saves",
    "considerable",
    "amount",
    "time",
    "mapreduce",
    "job",
    "ready",
    "time",
    "us",
    "run",
    "hadoop",
    "cluster",
    "done",
    "help",
    "set",
    "resources",
    "ram",
    "network",
    "bandwidth",
    "cpu",
    "multiple",
    "jobs",
    "run",
    "hadoop",
    "simultaneously",
    "needs",
    "resources",
    "complete",
    "task",
    "successfully",
    "efficiently",
    "manage",
    "resources",
    "third",
    "component",
    "hadoop",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "yarn",
    "consists",
    "resource",
    "manager",
    "node",
    "manager",
    "application",
    "master",
    "containers",
    "resource",
    "manager",
    "assigns",
    "resources",
    "node",
    "managers",
    "handle",
    "nodes",
    "monitor",
    "resource",
    "usage",
    "node",
    "containers",
    "hold",
    "collection",
    "physical",
    "resources",
    "suppose",
    "want",
    "process",
    "mapreduce",
    "job",
    "created",
    "first",
    "application",
    "master",
    "requests",
    "container",
    "node",
    "manager",
    "node",
    "manager",
    "gets",
    "resources",
    "sends",
    "resource",
    "manager",
    "way",
    "yarn",
    "processes",
    "job",
    "requests",
    "manages",
    "cluster",
    "resources",
    "hadoop",
    "addition",
    "components",
    "hadoop",
    "also",
    "various",
    "big",
    "data",
    "tools",
    "frameworks",
    "dedicated",
    "managing",
    "processing",
    "analyzing",
    "data",
    "hadoop",
    "ecosystem",
    "comprises",
    "several",
    "components",
    "like",
    "hive",
    "pig",
    "apache",
    "spark",
    "flume",
    "scoop",
    "name",
    "hadoop",
    "ecosystem",
    "works",
    "together",
    "big",
    "data",
    "management",
    "dive",
    "technical",
    "side",
    "hadoop",
    "going",
    "take",
    "little",
    "detour",
    "try",
    "give",
    "visual",
    "understanding",
    "relate",
    "maybe",
    "life",
    "setup",
    "going",
    "go",
    "farm",
    "case",
    "farm",
    "far",
    "away",
    "almost",
    "wish",
    "put",
    "far",
    "far",
    "away",
    "remind",
    "little",
    "bit",
    "star",
    "wars",
    "theme",
    "going",
    "look",
    "fruit",
    "farm",
    "jack",
    "harvests",
    "grapes",
    "sells",
    "nearby",
    "town",
    "harvesting",
    "stores",
    "produce",
    "storage",
    "shed",
    "storage",
    "room",
    "case",
    "found",
    "though",
    "high",
    "demand",
    "fruits",
    "started",
    "harvesting",
    "apples",
    "oranges",
    "well",
    "hopefully",
    "couple",
    "fills",
    "different",
    "fruit",
    "trees",
    "growing",
    "set",
    "see",
    "working",
    "hard",
    "harvest",
    "different",
    "fruits",
    "problem",
    "one",
    "ca",
    "really",
    "work",
    "needs",
    "hire",
    "two",
    "people",
    "work",
    "harvesting",
    "done",
    "simultaneously",
    "instead",
    "trying",
    "harvest",
    "different",
    "fruit",
    "two",
    "people",
    "putting",
    "food",
    "away",
    "harvesting",
    "storage",
    "room",
    "becomes",
    "bottleneck",
    "store",
    "access",
    "fruits",
    "single",
    "storage",
    "area",
    "ca",
    "fit",
    "fruit",
    "one",
    "place",
    "jack",
    "decides",
    "distribute",
    "storage",
    "area",
    "give",
    "one",
    "separate",
    "storage",
    "look",
    "computer",
    "terms",
    "people",
    "processors",
    "fruit",
    "data",
    "see",
    "storing",
    "different",
    "storage",
    "rooms",
    "see",
    "popping",
    "getting",
    "hello",
    "want",
    "fruit",
    "basket",
    "three",
    "grapes",
    "two",
    "apples",
    "three",
    "oranges",
    "getting",
    "ready",
    "breakfast",
    "family",
    "little",
    "large",
    "family",
    "family",
    "large",
    "complete",
    "order",
    "time",
    "work",
    "parallelly",
    "storage",
    "space",
    "process",
    "retrieving",
    "querying",
    "data",
    "see",
    "one",
    "storage",
    "space",
    "pulls",
    "three",
    "grapes",
    "pulls",
    "two",
    "apples",
    "another",
    "storage",
    "room",
    "pulls",
    "three",
    "oranges",
    "complete",
    "nice",
    "fruit",
    "basket",
    "solution",
    "helps",
    "complete",
    "order",
    "time",
    "without",
    "hassle",
    "happy",
    "prepared",
    "increase",
    "demand",
    "future",
    "growth",
    "system",
    "keep",
    "hiring",
    "new",
    "people",
    "continue",
    "grow",
    "develop",
    "large",
    "farm",
    "story",
    "relate",
    "big",
    "data",
    "hinted",
    "little",
    "bit",
    "earlier",
    "limited",
    "data",
    "one",
    "processor",
    "one",
    "storage",
    "unit",
    "needed",
    "remember",
    "back",
    "90s",
    "would",
    "upgrade",
    "computer",
    "instead",
    "small",
    "computer",
    "would",
    "spend",
    "money",
    "huge",
    "mainframe",
    "flashing",
    "lights",
    "cray",
    "computers",
    "really",
    "massive",
    "nowadays",
    "lot",
    "computers",
    "sits",
    "desktop",
    "powerful",
    "mainframes",
    "back",
    "pretty",
    "amazing",
    "time",
    "changed",
    "used",
    "able",
    "everything",
    "one",
    "computer",
    "structured",
    "data",
    "database",
    "stored",
    "structured",
    "data",
    "time",
    "acquiring",
    "databases",
    "sql",
    "queries",
    "think",
    "giant",
    "spreadsheet",
    "rows",
    "columns",
    "everything",
    "specific",
    "size",
    "fits",
    "neatly",
    "rows",
    "columns",
    "back",
    "90s",
    "nice",
    "setup",
    "upgraded",
    "computer",
    "would",
    "get",
    "nice",
    "big",
    "sun",
    "computer",
    "mainframe",
    "lot",
    "data",
    "lot",
    "stuff",
    "going",
    "easy",
    "soon",
    "though",
    "data",
    "generation",
    "increased",
    "leading",
    "high",
    "volume",
    "data",
    "along",
    "different",
    "data",
    "formats",
    "imagine",
    "today",
    "world",
    "year",
    "generate",
    "data",
    "previous",
    "years",
    "summed",
    "together",
    "generate",
    "data",
    "year",
    "previous",
    "years",
    "together",
    "way",
    "going",
    "time",
    "see",
    "variety",
    "data",
    "structured",
    "data",
    "think",
    "database",
    "rows",
    "columns",
    "easy",
    "look",
    "nice",
    "spreadsheet",
    "data",
    "emails",
    "example",
    "would",
    "one",
    "example",
    "xml",
    "html",
    "web",
    "pages",
    "unstructured",
    "data",
    "ever",
    "look",
    "folder",
    "photos",
    "photos",
    "taken",
    "phone",
    "high",
    "quality",
    "got",
    "photos",
    "long",
    "time",
    "ago",
    "got",
    "web",
    "photos",
    "low",
    "quality",
    "pictures",
    "alone",
    "none",
    "know",
    "certainly",
    "groups",
    "overall",
    "lot",
    "variety",
    "size",
    "setup",
    "single",
    "processor",
    "enough",
    "process",
    "high",
    "volume",
    "different",
    "kinds",
    "data",
    "time",
    "consuming",
    "imagine",
    "twitter",
    "millions",
    "twitter",
    "feeds",
    "going",
    "able",
    "query",
    "across",
    "one",
    "server",
    "way",
    "going",
    "happen",
    "unless",
    "people",
    "mind",
    "waiting",
    "year",
    "get",
    "history",
    "tweets",
    "look",
    "something",
    "hence",
    "start",
    "multiple",
    "processors",
    "used",
    "process",
    "high",
    "volume",
    "data",
    "saved",
    "time",
    "moving",
    "forward",
    "got",
    "multiple",
    "processors",
    "single",
    "storage",
    "unit",
    "became",
    "bottleneck",
    "due",
    "network",
    "overhead",
    "generated",
    "network",
    "coming",
    "one",
    "servers",
    "wait",
    "grab",
    "data",
    "single",
    "stored",
    "unit",
    "maybe",
    "sql",
    "server",
    "nice",
    "setup",
    "file",
    "system",
    "going",
    "solution",
    "use",
    "distributed",
    "storage",
    "processor",
    "enabled",
    "easy",
    "access",
    "storage",
    "access",
    "data",
    "makes",
    "lot",
    "sense",
    "multiple",
    "workers",
    "multiple",
    "storage",
    "units",
    "like",
    "storage",
    "room",
    "different",
    "fruit",
    "coming",
    "variety",
    "see",
    "nice",
    "parallel",
    "working",
    "farm",
    "dealing",
    "lot",
    "data",
    "data",
    "method",
    "worked",
    "network",
    "overhead",
    "generated",
    "getting",
    "bottleneck",
    "somewhere",
    "people",
    "waiting",
    "data",
    "pulled",
    "processed",
    "known",
    "parallel",
    "processing",
    "distributed",
    "storage",
    "parallel",
    "processing",
    "distributed",
    "storage",
    "see",
    "parallel",
    "processing",
    "different",
    "computers",
    "running",
    "processes",
    "distributed",
    "storage",
    "quick",
    "demo",
    "setting",
    "cloudera",
    "quick",
    "start",
    "vm",
    "case",
    "interested",
    "working",
    "standalone",
    "cluster",
    "download",
    "cloudera",
    "quick",
    "start",
    "vm",
    "type",
    "download",
    "cloudera",
    "quick",
    "start",
    "vm",
    "search",
    "package",
    "used",
    "set",
    "quick",
    "start",
    "vm",
    "would",
    "single",
    "node",
    "cloudera",
    "based",
    "cluster",
    "click",
    "link",
    "basically",
    "based",
    "platform",
    "would",
    "choosing",
    "install",
    "using",
    "vm",
    "box",
    "version",
    "cloudera",
    "would",
    "install",
    "select",
    "platform",
    "choose",
    "box",
    "click",
    "get",
    "give",
    "details",
    "basically",
    "allow",
    "download",
    "quick",
    "start",
    "vm",
    "would",
    "look",
    "something",
    "like",
    "zip",
    "file",
    "downloaded",
    "unzip",
    "used",
    "set",
    "single",
    "node",
    "cloud",
    "error",
    "cluster",
    "downloaded",
    "zip",
    "file",
    "would",
    "look",
    "something",
    "like",
    "would",
    "quick",
    "start",
    "virtual",
    "box",
    "virtual",
    "boss",
    "disk",
    "used",
    "set",
    "cluster",
    "ignore",
    "files",
    "related",
    "amazon",
    "machines",
    "need",
    "would",
    "used",
    "set",
    "cloud",
    "error",
    "cluster",
    "set",
    "click",
    "file",
    "import",
    "appliance",
    "choose",
    "quick",
    "start",
    "vm",
    "looking",
    "downloads",
    "quick",
    "start",
    "vm",
    "select",
    "click",
    "open",
    "click",
    "next",
    "shows",
    "specifications",
    "cpu",
    "ram",
    "change",
    "later",
    "click",
    "import",
    "start",
    "importing",
    "virtual",
    "disk",
    "image",
    "dot",
    "vmdk",
    "file",
    "vm",
    "box",
    "done",
    "change",
    "specifications",
    "machines",
    "use",
    "two",
    "cpu",
    "cores",
    "minimum",
    "give",
    "little",
    "ram",
    "cloudera",
    "quickstart",
    "vm",
    "cpu",
    "intensive",
    "needs",
    "good",
    "amount",
    "ram",
    "survive",
    "give",
    "two",
    "cpu",
    "cores",
    "5gb",
    "ram",
    "enough",
    "us",
    "bring",
    "quick",
    "start",
    "vm",
    "gives",
    "us",
    "cloudera",
    "distribution",
    "hadoop",
    "single",
    "node",
    "cluster",
    "setup",
    "used",
    "working",
    "learning",
    "different",
    "distributions",
    "cloudera",
    "clusters",
    "working",
    "sdfs",
    "hadoop",
    "ecosystem",
    "components",
    "let",
    "wait",
    "importing",
    "finish",
    "go",
    "ahead",
    "set",
    "quick",
    "start",
    "vm",
    "practice",
    "importing",
    "appliance",
    "done",
    "see",
    "cloudera",
    "quickstart",
    "machine",
    "added",
    "list",
    "machines",
    "click",
    "click",
    "settings",
    "mentioned",
    "would",
    "like",
    "give",
    "ram",
    "cpu",
    "cores",
    "click",
    "system",
    "let",
    "increase",
    "ram",
    "least",
    "five",
    "click",
    "processor",
    "let",
    "give",
    "two",
    "cpu",
    "cores",
    "would",
    "least",
    "better",
    "using",
    "one",
    "cpu",
    "core",
    "network",
    "goes",
    "nat",
    "fine",
    "click",
    "ok",
    "would",
    "want",
    "start",
    "machine",
    "uses",
    "two",
    "cpu",
    "cores",
    "5gb",
    "ram",
    "bring",
    "cloudera",
    "quick",
    "start",
    "vm",
    "let",
    "go",
    "ahead",
    "start",
    "machine",
    "quick",
    "start",
    "vm",
    "might",
    "take",
    "initially",
    "time",
    "start",
    "internally",
    "various",
    "cloudera",
    "services",
    "starting",
    "services",
    "need",
    "cloud",
    "era",
    "quick",
    "start",
    "vm",
    "accessible",
    "unlike",
    "apache",
    "hadoop",
    "cluster",
    "start",
    "cluster",
    "starting",
    "processes",
    "case",
    "cloudera",
    "cloudera",
    "scm",
    "server",
    "agents",
    "take",
    "care",
    "starting",
    "services",
    "starting",
    "different",
    "roles",
    "services",
    "explained",
    "previous",
    "session",
    "cloud",
    "era",
    "cluster",
    "would",
    "services",
    "let",
    "show",
    "case",
    "apache",
    "cluster",
    "start",
    "services",
    "start",
    "cluster",
    "running",
    "script",
    "basically",
    "scripts",
    "individually",
    "start",
    "different",
    "processes",
    "different",
    "nodes",
    "case",
    "cloud",
    "era",
    "would",
    "always",
    "cloudera",
    "scm",
    "server",
    "would",
    "running",
    "one",
    "machine",
    "including",
    "machine",
    "would",
    "clouded",
    "icm",
    "agents",
    "would",
    "running",
    "multiple",
    "machines",
    "similarly",
    "hortonworks",
    "cluster",
    "would",
    "ambari",
    "server",
    "starting",
    "first",
    "machine",
    "embody",
    "agents",
    "running",
    "machines",
    "server",
    "component",
    "knows",
    "services",
    "set",
    "configurations",
    "agents",
    "running",
    "every",
    "node",
    "responsible",
    "send",
    "heartbeats",
    "server",
    "receive",
    "instructions",
    "take",
    "care",
    "starting",
    "stopping",
    "individual",
    "roles",
    "different",
    "machines",
    "case",
    "single",
    "node",
    "cluster",
    "setup",
    "quick",
    "start",
    "vm",
    "would",
    "one",
    "scm",
    "server",
    "one",
    "scm",
    "agent",
    "start",
    "machine",
    "take",
    "care",
    "roles",
    "need",
    "started",
    "different",
    "services",
    "wait",
    "machine",
    "come",
    "basically",
    "cloud",
    "sem",
    "server",
    "agent",
    "running",
    "need",
    "follow",
    "steps",
    "cloudera",
    "admin",
    "console",
    "accessible",
    "allows",
    "browse",
    "cluster",
    "look",
    "different",
    "services",
    "look",
    "roles",
    "different",
    "services",
    "also",
    "work",
    "cluster",
    "either",
    "using",
    "command",
    "line",
    "using",
    "web",
    "interface",
    "machine",
    "come",
    "already",
    "connected",
    "internet",
    "see",
    "need",
    "certain",
    "things",
    "admin",
    "console",
    "accessible",
    "point",
    "time",
    "click",
    "terminal",
    "check",
    "access",
    "cluster",
    "type",
    "host",
    "name",
    "shows",
    "host",
    "name",
    "also",
    "type",
    "hdfs",
    "command",
    "see",
    "access",
    "cluster",
    "working",
    "commands",
    "would",
    "give",
    "apache",
    "hadoop",
    "cluster",
    "distribution",
    "loop",
    "sometimes",
    "cluster",
    "access",
    "terminal",
    "might",
    "take",
    "seconds",
    "minutes",
    "connection",
    "established",
    "cloudera",
    "cm",
    "server",
    "cloudera",
    "cm",
    "agent",
    "running",
    "background",
    "takes",
    "care",
    "cluster",
    "given",
    "sdfs",
    "dfs",
    "list",
    "command",
    "basically",
    "show",
    "default",
    "exists",
    "sdfs",
    "let",
    "give",
    "couple",
    "seconds",
    "shows",
    "us",
    "output",
    "also",
    "check",
    "giving",
    "service",
    "cloudera",
    "scm",
    "server",
    "status",
    "tells",
    "would",
    "want",
    "use",
    "cloudera",
    "express",
    "free",
    "run",
    "command",
    "needs",
    "8",
    "gb",
    "ram",
    "leads",
    "virtual",
    "cpu",
    "cores",
    "also",
    "mentions",
    "may",
    "take",
    "several",
    "minutes",
    "cloudera",
    "manager",
    "started",
    "login",
    "root",
    "give",
    "command",
    "service",
    "cloud",
    "error",
    "scm",
    "server",
    "status",
    "remember",
    "password",
    "root",
    "cloudera",
    "basically",
    "says",
    "would",
    "want",
    "check",
    "settings",
    "good",
    "express",
    "edition",
    "running",
    "close",
    "sdfs",
    "access",
    "working",
    "fine",
    "let",
    "close",
    "terminal",
    "launch",
    "cloud",
    "error",
    "express",
    "click",
    "give",
    "need",
    "give",
    "command",
    "force",
    "let",
    "copy",
    "command",
    "let",
    "open",
    "different",
    "terminal",
    "let",
    "give",
    "command",
    "like",
    "go",
    "ahead",
    "shut",
    "cloudera",
    "based",
    "services",
    "restart",
    "able",
    "access",
    "admin",
    "console",
    "let",
    "give",
    "couple",
    "minutes",
    "access",
    "admin",
    "console",
    "see",
    "starting",
    "cloudera",
    "manager",
    "server",
    "waiting",
    "cloudera",
    "manager",
    "api",
    "starting",
    "cloudera",
    "manager",
    "agents",
    "configuring",
    "deployment",
    "per",
    "new",
    "settings",
    "given",
    "use",
    "express",
    "edition",
    "cloudera",
    "done",
    "say",
    "cluster",
    "restarted",
    "admin",
    "console",
    "accessed",
    "id",
    "password",
    "cloudera",
    "give",
    "couple",
    "minutes",
    "done",
    "ready",
    "use",
    "admin",
    "console",
    "deployment",
    "configured",
    "client",
    "configurations",
    "also",
    "deployed",
    "restarted",
    "cloudera",
    "management",
    "service",
    "gives",
    "access",
    "quick",
    "start",
    "admin",
    "console",
    "using",
    "username",
    "password",
    "cloud",
    "error",
    "let",
    "try",
    "accessing",
    "open",
    "browser",
    "let",
    "change",
    "7180",
    "default",
    "port",
    "shows",
    "admin",
    "console",
    "coming",
    "log",
    "cloud",
    "error",
    "cloud",
    "error",
    "let",
    "click",
    "login",
    "said",
    "cloudera",
    "cpu",
    "intensive",
    "memory",
    "intensive",
    "would",
    "slow",
    "since",
    "given",
    "enough",
    "gb",
    "ram",
    "cloud",
    "error",
    "cluster",
    "thus",
    "advisable",
    "stop",
    "even",
    "remove",
    "services",
    "need",
    "look",
    "services",
    "look",
    "stop",
    "status",
    "good",
    "one",
    "way",
    "go",
    "ahead",
    "remove",
    "services",
    "use",
    "beginning",
    "later",
    "anytime",
    "add",
    "services",
    "cluster",
    "example",
    "click",
    "key",
    "value",
    "store",
    "scroll",
    "says",
    "delete",
    "remove",
    "service",
    "admin",
    "console",
    "anytime",
    "removing",
    "particular",
    "service",
    "remove",
    "service",
    "management",
    "cloudera",
    "manager",
    "role",
    "groups",
    "service",
    "removed",
    "host",
    "templates",
    "click",
    "delete",
    "service",
    "depending",
    "service",
    "would",
    "prompted",
    "message",
    "remove",
    "relevant",
    "services",
    "particular",
    "service",
    "depends",
    "service",
    "already",
    "running",
    "would",
    "given",
    "message",
    "service",
    "stopped",
    "deleted",
    "cloudera",
    "admin",
    "console",
    "admin",
    "console",
    "allows",
    "click",
    "services",
    "look",
    "different",
    "roles",
    "processes",
    "running",
    "service",
    "anyways",
    "access",
    "cloudera",
    "cluster",
    "terminal",
    "using",
    "regular",
    "sdfs",
    "yarn",
    "maplet",
    "commands",
    "removed",
    "service",
    "also",
    "remove",
    "solar",
    "using",
    "beginning",
    "depends",
    "choice",
    "scroll",
    "delete",
    "says",
    "deleting",
    "solar",
    "service",
    "must",
    "remove",
    "dependencies",
    "service",
    "configuration",
    "following",
    "services",
    "hue",
    "hue",
    "web",
    "interface",
    "allows",
    "work",
    "sdfs",
    "depending",
    "click",
    "configure",
    "service",
    "dependency",
    "make",
    "sure",
    "hue",
    "service",
    "depend",
    "particular",
    "service",
    "removing",
    "clean",
    "removal",
    "service",
    "click",
    "none",
    "say",
    "save",
    "changes",
    "done",
    "go",
    "ahead",
    "try",
    "removing",
    "solar",
    "service",
    "admin",
    "console",
    "reduce",
    "load",
    "management",
    "console",
    "also",
    "allow",
    "work",
    "faster",
    "cluster",
    "removed",
    "dependency",
    "hue",
    "solar",
    "click",
    "delete",
    "remember",
    "cluster",
    "becomes",
    "little",
    "lighter",
    "work",
    "focus",
    "services",
    "point",
    "time",
    "want",
    "add",
    "services",
    "cluster",
    "anytime",
    "fix",
    "different",
    "configuration",
    "issues",
    "like",
    "see",
    "different",
    "warning",
    "messages",
    "services",
    "already",
    "existing",
    "need",
    "service",
    "click",
    "drop",
    "click",
    "delete",
    "says",
    "scoop",
    "2",
    "also",
    "relevance",
    "hue",
    "hue",
    "web",
    "interface",
    "also",
    "depends",
    "scope",
    "make",
    "none",
    "point",
    "time",
    "later",
    "add",
    "services",
    "clicking",
    "add",
    "service",
    "option",
    "cluster",
    "admin",
    "access",
    "quick",
    "start",
    "vm",
    "gives",
    "single",
    "node",
    "cloud",
    "error",
    "cluster",
    "use",
    "learning",
    "practicing",
    "click",
    "scope",
    "2",
    "say",
    "delete",
    "configured",
    "dependency",
    "remove",
    "scope",
    "2",
    "also",
    "list",
    "services",
    "admin",
    "console",
    "managing",
    "right",
    "done",
    "removed",
    "three",
    "services",
    "need",
    "even",
    "remove",
    "scope",
    "client",
    "need",
    "add",
    "later",
    "various",
    "alerts",
    "cloudera",
    "admin",
    "console",
    "shows",
    "always",
    "fix",
    "clicking",
    "health",
    "issues",
    "configuration",
    "issues",
    "click",
    "see",
    "health",
    "issue",
    "pointing",
    "critical",
    "one",
    "ignored",
    "says",
    "issue",
    "clock",
    "offset",
    "basically",
    "relates",
    "ntp",
    "service",
    "network",
    "time",
    "protocol",
    "makes",
    "sure",
    "one",
    "multiple",
    "machines",
    "time",
    "zone",
    "sync",
    "click",
    "suppress",
    "say",
    "suppress",
    "hosts",
    "say",
    "look",
    "later",
    "confirm",
    "health",
    "issue",
    "reported",
    "probably",
    "ntp",
    "service",
    "machines",
    "might",
    "sync",
    "impact",
    "use",
    "case",
    "kerberos",
    "kind",
    "setup",
    "security",
    "basically",
    "offset",
    "time",
    "zone",
    "becomes",
    "important",
    "ignore",
    "message",
    "still",
    "good",
    "use",
    "cluster",
    "also",
    "configuration",
    "issues",
    "click",
    "might",
    "talk",
    "heap",
    "size",
    "ram",
    "available",
    "machines",
    "talks",
    "zookeeper",
    "odd",
    "numbers",
    "q",
    "load",
    "balancer",
    "sdfs",
    "one",
    "data",
    "node",
    "issues",
    "worried",
    "upon",
    "single",
    "node",
    "cluster",
    "setup",
    "want",
    "avoid",
    "warnings",
    "always",
    "click",
    "suppress",
    "avoid",
    "let",
    "cluster",
    "green",
    "status",
    "nothing",
    "worry",
    "click",
    "cluster",
    "basically",
    "look",
    "services",
    "removed",
    "services",
    "intend",
    "use",
    "also",
    "suppressed",
    "offset",
    "warning",
    "critical",
    "use",
    "case",
    "basically",
    "good",
    "start",
    "cluster",
    "point",
    "time",
    "said",
    "would",
    "want",
    "add",
    "services",
    "actions",
    "button",
    "use",
    "add",
    "service",
    "say",
    "restart",
    "cluster",
    "restart",
    "services",
    "one",
    "one",
    "starting",
    "zookeeper",
    "first",
    "service",
    "come",
    "always",
    "click",
    "arrow",
    "mark",
    "see",
    "happening",
    "services",
    "services",
    "coming",
    "order",
    "issues",
    "always",
    "click",
    "link",
    "next",
    "take",
    "logs",
    "click",
    "close",
    "let",
    "happen",
    "background",
    "basically",
    "let",
    "services",
    "restart",
    "one",
    "one",
    "cluster",
    "become",
    "completely",
    "accessible",
    "either",
    "using",
    "hue",
    "web",
    "interface",
    "quick",
    "start",
    "terminal",
    "allows",
    "give",
    "commands",
    "machines",
    "coming",
    "click",
    "hosts",
    "look",
    "hosts",
    "one",
    "also",
    "tell",
    "many",
    "roles",
    "processes",
    "running",
    "machine",
    "25",
    "rolls",
    "tells",
    "disk",
    "usage",
    "tells",
    "physical",
    "memory",
    "used",
    "using",
    "host",
    "tab",
    "add",
    "new",
    "host",
    "cluster",
    "check",
    "configuration",
    "check",
    "hosts",
    "diagnostics",
    "look",
    "logs",
    "give",
    "access",
    "logs",
    "even",
    "select",
    "sources",
    "would",
    "want",
    "logs",
    "give",
    "host",
    "name",
    "click",
    "search",
    "build",
    "charts",
    "also",
    "admin",
    "stuff",
    "adding",
    "different",
    "users",
    "enabling",
    "security",
    "using",
    "administration",
    "tab",
    "since",
    "clicked",
    "restart",
    "cluster",
    "slowly",
    "start",
    "seeing",
    "services",
    "one",
    "one",
    "coming",
    "starting",
    "zookeeper",
    "begin",
    "cluster",
    "running",
    "whether",
    "showing",
    "services",
    "green",
    "different",
    "status",
    "still",
    "able",
    "access",
    "service",
    "saw",
    "apache",
    "hadoop",
    "cluster",
    "even",
    "click",
    "sdfs",
    "access",
    "web",
    "ui",
    "sdfs",
    "service",
    "clicking",
    "quick",
    "links",
    "service",
    "yet",
    "able",
    "see",
    "web",
    "ui",
    "link",
    "allow",
    "check",
    "things",
    "sdfs",
    "web",
    "interface",
    "similarly",
    "yarn",
    "service",
    "also",
    "web",
    "interface",
    "soon",
    "service",
    "comes",
    "quick",
    "links",
    "access",
    "yarn",
    "ui",
    "similarly",
    "service",
    "comes",
    "access",
    "hue",
    "give",
    "web",
    "interface",
    "allows",
    "work",
    "sdfs",
    "allows",
    "work",
    "different",
    "components",
    "within",
    "cluster",
    "without",
    "even",
    "using",
    "command",
    "line",
    "tools",
    "command",
    "line",
    "options",
    "give",
    "time",
    "cloud",
    "error",
    "scm",
    "agent",
    "every",
    "machine",
    "able",
    "restart",
    "roles",
    "responsible",
    "cluster",
    "come",
    "always",
    "click",
    "tells",
    "running",
    "commands",
    "background",
    "trying",
    "start",
    "cluster",
    "go",
    "terminal",
    "switch",
    "hdfs",
    "user",
    "remember",
    "sdfs",
    "user",
    "admin",
    "user",
    "password",
    "unless",
    "set",
    "one",
    "log",
    "sdfs",
    "might",
    "ask",
    "password",
    "initially",
    "best",
    "way",
    "logging",
    "root",
    "password",
    "cloud",
    "error",
    "log",
    "hdfs",
    "onwards",
    "give",
    "sdfs",
    "commands",
    "work",
    "file",
    "system",
    "since",
    "services",
    "coming",
    "right",
    "try",
    "give",
    "sdfs",
    "dfs",
    "command",
    "might",
    "work",
    "might",
    "also",
    "say",
    "trying",
    "connect",
    "name",
    "node",
    "yet",
    "give",
    "time",
    "name",
    "node",
    "able",
    "access",
    "sdfs",
    "using",
    "commands",
    "quickly",
    "set",
    "quick",
    "start",
    "working",
    "using",
    "command",
    "line",
    "options",
    "terminal",
    "like",
    "would",
    "apache",
    "hadoop",
    "cluster",
    "could",
    "use",
    "web",
    "interfaces",
    "allow",
    "work",
    "cluster",
    "usually",
    "takes",
    "time",
    "give",
    "time",
    "services",
    "running",
    "reason",
    "issues",
    "might",
    "require",
    "restart",
    "cluster",
    "several",
    "times",
    "beginning",
    "gets",
    "accustomed",
    "settings",
    "given",
    "starts",
    "services",
    "point",
    "time",
    "error",
    "message",
    "always",
    "go",
    "back",
    "look",
    "logs",
    "see",
    "happening",
    "try",
    "starting",
    "cluster",
    "set",
    "quick",
    "start",
    "vm",
    "using",
    "work",
    "cloud",
    "error",
    "clusters",
    "start",
    "big",
    "data",
    "challenges",
    "first",
    "thing",
    "big",
    "data",
    "see",
    "nice",
    "chaotic",
    "image",
    "different",
    "inputs",
    "server",
    "racks",
    "place",
    "graphs",
    "generated",
    "everything",
    "imagine",
    "problems",
    "come",
    "big",
    "data",
    "one",
    "storing",
    "store",
    "massive",
    "amount",
    "data",
    "talking",
    "terabyte",
    "10",
    "terabytes",
    "talking",
    "minimal",
    "10",
    "terabytes",
    "petabytes",
    "data",
    "next",
    "question",
    "processing",
    "two",
    "go",
    "hand",
    "hand",
    "storing",
    "data",
    "might",
    "take",
    "huge",
    "amount",
    "space",
    "might",
    "small",
    "amount",
    "data",
    "takes",
    "lot",
    "processing",
    "either",
    "one",
    "drive",
    "series",
    "data",
    "processing",
    "big",
    "data",
    "arena",
    "storing",
    "data",
    "storing",
    "big",
    "data",
    "problem",
    "due",
    "massive",
    "volume",
    "straight",
    "people",
    "would",
    "huge",
    "backup",
    "tapes",
    "go",
    "backup",
    "tapes",
    "hours",
    "go",
    "find",
    "data",
    "simple",
    "query",
    "could",
    "take",
    "days",
    "processing",
    "processing",
    "big",
    "data",
    "consumed",
    "time",
    "hadoop",
    "came",
    "cheap",
    "way",
    "process",
    "data",
    "used",
    "like",
    "processes",
    "ran",
    "computer",
    "without",
    "trying",
    "use",
    "multiple",
    "cores",
    "multiple",
    "threads",
    "would",
    "take",
    "years",
    "process",
    "simple",
    "data",
    "analysis",
    "get",
    "heavy",
    "data",
    "processing",
    "processing",
    "big",
    "problem",
    "size",
    "data",
    "hadoop",
    "solution",
    "solution",
    "big",
    "data",
    "big",
    "data",
    "storage",
    "storing",
    "big",
    "data",
    "problem",
    "due",
    "massive",
    "volume",
    "take",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "able",
    "store",
    "huge",
    "data",
    "across",
    "large",
    "number",
    "machines",
    "access",
    "like",
    "one",
    "file",
    "system",
    "processing",
    "big",
    "data",
    "consumed",
    "time",
    "talked",
    "processes",
    "ca",
    "even",
    "computer",
    "would",
    "take",
    "years",
    "hadoop",
    "map",
    "reduce",
    "processing",
    "big",
    "data",
    "faster",
    "going",
    "add",
    "little",
    "notation",
    "right",
    "really",
    "important",
    "note",
    "hadoop",
    "beginning",
    "talk",
    "data",
    "processing",
    "added",
    "new",
    "processes",
    "top",
    "map",
    "reduce",
    "even",
    "accelerate",
    "spark",
    "setup",
    "different",
    "functionalities",
    "really",
    "basis",
    "starts",
    "basic",
    "concept",
    "hadoop",
    "mapreduce",
    "let",
    "us",
    "look",
    "hdfs",
    "detail",
    "traditional",
    "approach",
    "data",
    "stored",
    "single",
    "central",
    "database",
    "rise",
    "big",
    "data",
    "single",
    "database",
    "enough",
    "storage",
    "remember",
    "old",
    "sun",
    "computers",
    "huge",
    "ibm",
    "machines",
    "flashing",
    "lights",
    "data",
    "one",
    "stored",
    "phone",
    "almost",
    "bit",
    "humor",
    "much",
    "accelerated",
    "years",
    "thing",
    "rise",
    "big",
    "data",
    "longer",
    "store",
    "one",
    "machine",
    "longer",
    "go",
    "buy",
    "sun",
    "computer",
    "put",
    "one",
    "sun",
    "computer",
    "buy",
    "craig",
    "machine",
    "enterprise",
    "ibm",
    "server",
    "going",
    "work",
    "going",
    "fit",
    "one",
    "server",
    "solution",
    "use",
    "distributed",
    "approach",
    "store",
    "massive",
    "amount",
    "data",
    "data",
    "divided",
    "distributed",
    "amongst",
    "many",
    "individual",
    "databases",
    "see",
    "three",
    "different",
    "databases",
    "going",
    "might",
    "actually",
    "saw",
    "one",
    "divided",
    "user",
    "accounts",
    "g",
    "letter",
    "first",
    "query",
    "would",
    "say",
    "first",
    "letter",
    "uh",
    "whatever",
    "id",
    "would",
    "go",
    "database",
    "find",
    "database",
    "telling",
    "database",
    "look",
    "stuff",
    "long",
    "time",
    "ago",
    "honest",
    "work",
    "really",
    "well",
    "nowadays",
    "distributed",
    "database",
    "track",
    "database",
    "put",
    "hdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "hdfs",
    "specially",
    "designed",
    "file",
    "system",
    "storing",
    "huge",
    "data",
    "sets",
    "commodity",
    "hardware",
    "commodities",
    "interesting",
    "term",
    "mentioned",
    "enterprise",
    "versus",
    "commodity",
    "touch",
    "back",
    "upon",
    "two",
    "core",
    "components",
    "name",
    "node",
    "data",
    "node",
    "name",
    "node",
    "master",
    "daemon",
    "one",
    "active",
    "name",
    "node",
    "manages",
    "data",
    "nodes",
    "stores",
    "metadata",
    "stores",
    "mapping",
    "data",
    "hadoop",
    "file",
    "system",
    "name",
    "node",
    "usually",
    "enterprise",
    "machine",
    "spend",
    "lot",
    "extra",
    "money",
    "solid",
    "name",
    "known",
    "machine",
    "data",
    "nodes",
    "data",
    "node",
    "data",
    "node",
    "data",
    "node",
    "three",
    "data",
    "node",
    "slave",
    "multiple",
    "data",
    "nodes",
    "stores",
    "actual",
    "data",
    "commodity",
    "hardware",
    "comes",
    "best",
    "definition",
    "heard",
    "commodity",
    "hardware",
    "cheap",
    "knockoffs",
    "buy",
    "buy",
    "10",
    "expect",
    "one",
    "work",
    "know",
    "come",
    "going",
    "break",
    "right",
    "away",
    "looking",
    "hardware",
    "might",
    "main",
    "master",
    "node",
    "enterprise",
    "server",
    "data",
    "nodes",
    "cheap",
    "get",
    "different",
    "features",
    "need",
    "said",
    "earlier",
    "name",
    "node",
    "stores",
    "metadata",
    "metadata",
    "gives",
    "information",
    "regarding",
    "file",
    "location",
    "block",
    "size",
    "metadata",
    "hdfs",
    "maintained",
    "using",
    "two",
    "files",
    "edit",
    "log",
    "fs",
    "image",
    "edit",
    "log",
    "keeps",
    "track",
    "recent",
    "changes",
    "made",
    "hadoop",
    "file",
    "system",
    "recent",
    "changes",
    "tracked",
    "fs",
    "image",
    "keeps",
    "track",
    "every",
    "change",
    "made",
    "hdfs",
    "since",
    "beginning",
    "happens",
    "edit",
    "log",
    "file",
    "size",
    "increases",
    "name",
    "node",
    "fails",
    "big",
    "ones",
    "happens",
    "get",
    "edit",
    "log",
    "keeps",
    "big",
    "big",
    "main",
    "enterprise",
    "computer",
    "spent",
    "money",
    "would",
    "break",
    "actually",
    "fails",
    "still",
    "fail",
    "solution",
    "make",
    "copies",
    "edit",
    "log",
    "fs",
    "image",
    "files",
    "pretty",
    "straightforward",
    "copy",
    "recent",
    "edits",
    "going",
    "long",
    "term",
    "image",
    "file",
    "system",
    "also",
    "create",
    "secondary",
    "name",
    "node",
    "node",
    "maintains",
    "copies",
    "edit",
    "log",
    "fs",
    "image",
    "combines",
    "get",
    "updated",
    "version",
    "fs",
    "image",
    "secondary",
    "name",
    "node",
    "came",
    "last",
    "oh",
    "guess",
    "two",
    "three",
    "years",
    "became",
    "part",
    "main",
    "system",
    "usually",
    "secondary",
    "name",
    "node",
    "also",
    "enterprise",
    "computer",
    "put",
    "separate",
    "racks",
    "three",
    "racks",
    "computers",
    "would",
    "maybe",
    "first",
    "two",
    "racks",
    "would",
    "name",
    "node",
    "second",
    "rack",
    "would",
    "secondary",
    "name",
    "note",
    "reason",
    "put",
    "different",
    "racks",
    "whole",
    "rack",
    "go",
    "could",
    "somebody",
    "literally",
    "trip",
    "power",
    "cable",
    "switch",
    "goes",
    "racks",
    "common",
    "goes",
    "well",
    "switch",
    "goes",
    "easily",
    "switch",
    "secondary",
    "name",
    "node",
    "getting",
    "switch",
    "replaced",
    "replacing",
    "hardware",
    "way",
    "hdfs",
    "works",
    "still",
    "completely",
    "functional",
    "let",
    "take",
    "look",
    "name",
    "node",
    "edit",
    "log",
    "fs",
    "image",
    "secondary",
    "name",
    "node",
    "take",
    "copies",
    "edit",
    "log",
    "copies",
    "fs",
    "image",
    "see",
    "right",
    "different",
    "contents",
    "main",
    "name",
    "node",
    "also",
    "secondary",
    "name",
    "node",
    "secondary",
    "name",
    "node",
    "actually",
    "take",
    "two",
    "edit",
    "log",
    "fs",
    "image",
    "make",
    "copy",
    "full",
    "fs",
    "image",
    "contains",
    "current",
    "date",
    "secondary",
    "name",
    "node",
    "creates",
    "periodic",
    "checkpoint",
    "files",
    "updates",
    "new",
    "fs",
    "image",
    "name",
    "node",
    "used",
    "occurred",
    "name",
    "node",
    "secondary",
    "name",
    "note",
    "use",
    "secondary",
    "node",
    "back",
    "everything",
    "going",
    "name",
    "node",
    "lifting",
    "back",
    "combining",
    "edit",
    "log",
    "bringing",
    "fs",
    "image",
    "current",
    "end",
    "new",
    "edit",
    "log",
    "new",
    "fs",
    "image",
    "updated",
    "start",
    "fresh",
    "edit",
    "log",
    "process",
    "updating",
    "happens",
    "every",
    "hour",
    "scheduled",
    "actually",
    "change",
    "schedules",
    "standard",
    "update",
    "every",
    "hour",
    "let",
    "took",
    "look",
    "master",
    "node",
    "know",
    "name",
    "node",
    "secondary",
    "name",
    "node",
    "let",
    "take",
    "look",
    "cluster",
    "architecture",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "cluster",
    "architecture",
    "name",
    "node",
    "stores",
    "metadata",
    "block",
    "location",
    "fs",
    "image",
    "plus",
    "edit",
    "log",
    "backup",
    "fs",
    "image",
    "edit",
    "log",
    "rack",
    "switch",
    "top",
    "remember",
    "talking",
    "switch",
    "common",
    "thing",
    "go",
    "rack",
    "switches",
    "underneath",
    "rack",
    "different",
    "data",
    "nodes",
    "data",
    "node",
    "1",
    "2",
    "four",
    "five",
    "maybe",
    "10",
    "15",
    "rack",
    "stack",
    "pretty",
    "high",
    "nowadays",
    "uh",
    "used",
    "get",
    "10",
    "servers",
    "see",
    "racks",
    "contain",
    "lot",
    "multiple",
    "racks",
    "talking",
    "one",
    "rack",
    "also",
    "know",
    "rack",
    "2",
    "rack",
    "3",
    "4",
    "5",
    "6",
    "rack",
    "100",
    "data",
    "nodes",
    "would",
    "looking",
    "10",
    "racks",
    "10",
    "data",
    "nodes",
    "literally",
    "10",
    "commodity",
    "server",
    "computers",
    "hardware",
    "core",
    "switch",
    "maintains",
    "network",
    "bandwidth",
    "connects",
    "name",
    "node",
    "data",
    "nodes",
    "like",
    "rack",
    "switch",
    "connects",
    "nodes",
    "rack",
    "core",
    "switches",
    "connect",
    "racks",
    "together",
    "also",
    "connect",
    "name",
    "node",
    "setup",
    "look",
    "fs",
    "image",
    "edit",
    "log",
    "pull",
    "information",
    "metadata",
    "looked",
    "architecture",
    "name",
    "node",
    "coming",
    "metadata",
    "block",
    "locations",
    "sorts",
    "core",
    "switches",
    "connect",
    "everything",
    "different",
    "racks",
    "individual",
    "rack",
    "switches",
    "connect",
    "different",
    "nodes",
    "core",
    "switches",
    "let",
    "talk",
    "actual",
    "data",
    "blocks",
    "actually",
    "sitting",
    "commodity",
    "machines",
    "hadoop",
    "file",
    "system",
    "splits",
    "massive",
    "files",
    "small",
    "chunks",
    "chunks",
    "known",
    "data",
    "blocks",
    "file",
    "hadoop",
    "file",
    "system",
    "stored",
    "data",
    "block",
    "nice",
    "picture",
    "looks",
    "like",
    "lego",
    "ever",
    "played",
    "legos",
    "kid",
    "good",
    "example",
    "stack",
    "data",
    "right",
    "top",
    "block",
    "symmetry",
    "size",
    "track",
    "easily",
    "default",
    "size",
    "one",
    "data",
    "block",
    "usually",
    "128",
    "megabytes",
    "go",
    "change",
    "standard",
    "pretty",
    "solid",
    "far",
    "data",
    "concerned",
    "loading",
    "huge",
    "amounts",
    "data",
    "certainly",
    "reasons",
    "change",
    "128",
    "megabytes",
    "pretty",
    "standard",
    "block",
    "128",
    "megabytes",
    "block",
    "size",
    "smaller",
    "many",
    "data",
    "blocks",
    "along",
    "lots",
    "metadata",
    "create",
    "overhead",
    "really",
    "want",
    "go",
    "smaller",
    "data",
    "blocks",
    "unless",
    "certain",
    "kind",
    "data",
    "similarly",
    "block",
    "size",
    "large",
    "processing",
    "time",
    "block",
    "increases",
    "pointed",
    "earlier",
    "block",
    "size",
    "like",
    "lego",
    "blocks",
    "last",
    "block",
    "size",
    "less",
    "might",
    "storing",
    "100",
    "megabytes",
    "last",
    "block",
    "think",
    "terabyte",
    "data",
    "storing",
    "going",
    "exactly",
    "divided",
    "128",
    "megabytes",
    "store",
    "128",
    "megabytes",
    "except",
    "last",
    "one",
    "could",
    "anywhere",
    "one",
    "128",
    "megabytes",
    "depending",
    "evenly",
    "data",
    "divided",
    "let",
    "look",
    "files",
    "stored",
    "hadoop",
    "file",
    "system",
    "file",
    "text",
    "let",
    "say",
    "520",
    "megabytes",
    "block",
    "take",
    "128",
    "megabytes",
    "520",
    "store",
    "block",
    "block",
    "b",
    "taking",
    "128",
    "megabytes",
    "520",
    "storing",
    "block",
    "c",
    "block",
    "block",
    "e",
    "eight",
    "megabytes",
    "left",
    "add",
    "128",
    "plus",
    "128",
    "plus",
    "128",
    "plus",
    "128",
    "get",
    "last",
    "eight",
    "megabytes",
    "goes",
    "block",
    "final",
    "block",
    "uses",
    "remaining",
    "space",
    "storage",
    "data",
    "node",
    "failure",
    "replication",
    "really",
    "hadoop",
    "shines",
    "makes",
    "use",
    "commodity",
    "computers",
    "multiple",
    "racks",
    "something",
    "go",
    "data",
    "blocks",
    "stored",
    "various",
    "data",
    "notes",
    "take",
    "block",
    "store",
    "128",
    "megabytes",
    "gon",
    "na",
    "put",
    "different",
    "nodes",
    "block",
    "block",
    "b",
    "block",
    "c",
    "last",
    "example",
    "node",
    "one",
    "node",
    "two",
    "node",
    "three",
    "node",
    "four",
    "node",
    "five",
    "node",
    "six",
    "one",
    "represents",
    "different",
    "computer",
    "literally",
    "splits",
    "data",
    "different",
    "machines",
    "happens",
    "node",
    "five",
    "crashes",
    "well",
    "big",
    "deal",
    "mean",
    "might",
    "even",
    "node",
    "five",
    "might",
    "whole",
    "rack",
    "go",
    "company",
    "building",
    "whole",
    "business",
    "going",
    "lose",
    "lot",
    "money",
    "happen",
    "node",
    "5",
    "crashes",
    "first",
    "rack",
    "goes",
    "data",
    "stored",
    "node",
    "5",
    "unavailable",
    "copy",
    "stored",
    "elsewhere",
    "particular",
    "image",
    "hadoop",
    "file",
    "system",
    "overcomes",
    "issue",
    "data",
    "node",
    "failure",
    "creating",
    "copies",
    "data",
    "known",
    "replication",
    "method",
    "see",
    "six",
    "nodes",
    "block",
    "instead",
    "storing",
    "first",
    "machine",
    "actually",
    "going",
    "store",
    "second",
    "fourth",
    "nodes",
    "spread",
    "across",
    "three",
    "different",
    "computers",
    "rack",
    "one",
    "always",
    "different",
    "rack",
    "might",
    "two",
    "copies",
    "rack",
    "never",
    "three",
    "rack",
    "always",
    "never",
    "two",
    "copies",
    "one",
    "node",
    "reason",
    "one",
    "copy",
    "per",
    "node",
    "see",
    "thing",
    "block",
    "b",
    "block",
    "c",
    "also",
    "spread",
    "across",
    "different",
    "machines",
    "block",
    "block",
    "e",
    "node",
    "five",
    "crashes",
    "data",
    "blocks",
    "b",
    "e",
    "lost",
    "well",
    "example",
    "backups",
    "three",
    "different",
    "machines",
    "blocks",
    "copies",
    "nodes",
    "due",
    "data",
    "lost",
    "even",
    "node",
    "5",
    "crashes",
    "also",
    "stored",
    "different",
    "racks",
    "even",
    "whole",
    "rack",
    "goes",
    "still",
    "live",
    "hadoop",
    "file",
    "system",
    "default",
    "replication",
    "factor",
    "three",
    "total",
    "three",
    "copies",
    "data",
    "block",
    "changed",
    "different",
    "reasons",
    "purposes",
    "got",
    "remember",
    "looking",
    "data",
    "center",
    "one",
    "huge",
    "room",
    "switches",
    "connecting",
    "servers",
    "shuffle",
    "data",
    "back",
    "forth",
    "really",
    "quick",
    "important",
    "dealing",
    "big",
    "data",
    "see",
    "block",
    "default",
    "replicated",
    "three",
    "times",
    "standard",
    "rare",
    "occasions",
    "four",
    "even",
    "fewer",
    "reasons",
    "two",
    "blocks",
    "seen",
    "four",
    "used",
    "two",
    "data",
    "centers",
    "data",
    "center",
    "kept",
    "two",
    "different",
    "copies",
    "rack",
    "awareness",
    "hadoop",
    "file",
    "system",
    "rack",
    "collection",
    "30",
    "40",
    "data",
    "nodes",
    "rack",
    "awareness",
    "concept",
    "helps",
    "decide",
    "replica",
    "data",
    "block",
    "stored",
    "rack",
    "one",
    "data",
    "node",
    "one",
    "four",
    "remember",
    "saying",
    "used",
    "put",
    "ten",
    "machines",
    "went",
    "twenty",
    "thirty",
    "forty",
    "rack",
    "forty",
    "servers",
    "rack",
    "two",
    "rack",
    "three",
    "put",
    "block",
    "one",
    "replicas",
    "block",
    "rack",
    "put",
    "replicas",
    "onto",
    "different",
    "rack",
    "notice",
    "actually",
    "two",
    "rack",
    "never",
    "three",
    "stored",
    "rack",
    "case",
    "whole",
    "rack",
    "goes",
    "replicas",
    "block",
    "created",
    "rack",
    "two",
    "actually",
    "default",
    "create",
    "replicas",
    "onto",
    "rack",
    "data",
    "exchange",
    "maximizing",
    "processing",
    "time",
    "course",
    "block",
    "b",
    "replicated",
    "onto",
    "rack",
    "3",
    "block",
    "c",
    "replicate",
    "onto",
    "rack",
    "1",
    "data",
    "way",
    "block",
    "whatever",
    "much",
    "ever",
    "data",
    "hdfs",
    "architecture",
    "let",
    "look",
    "architecture",
    "bigger",
    "picture",
    "looked",
    "name",
    "node",
    "store",
    "metadata",
    "names",
    "replicas",
    "home",
    "food",
    "data",
    "three",
    "different",
    "infra",
    "metadata",
    "stored",
    "data",
    "nodes",
    "see",
    "data",
    "nodes",
    "different",
    "racks",
    "different",
    "machines",
    "name",
    "node",
    "going",
    "see",
    "heartbeat",
    "pulse",
    "lot",
    "times",
    "one",
    "things",
    "confuses",
    "people",
    "sometimes",
    "classes",
    "talk",
    "nodes",
    "versus",
    "machines",
    "could",
    "data",
    "node",
    "hadoop",
    "data",
    "node",
    "could",
    "also",
    "spark",
    "node",
    "sparks",
    "different",
    "architecture",
    "daemons",
    "running",
    "computers",
    "refer",
    "nodes",
    "always",
    "servers",
    "machines",
    "even",
    "though",
    "use",
    "interchangeable",
    "aware",
    "nodes",
    "even",
    "virtual",
    "machines",
    "testing",
    "something",
    "make",
    "sense",
    "10",
    "virtual",
    "nodes",
    "one",
    "machine",
    "deploy",
    "might",
    "well",
    "run",
    "code",
    "machine",
    "heartbeat",
    "going",
    "heartbeat",
    "signal",
    "data",
    "nodes",
    "continuously",
    "send",
    "name",
    "nodes",
    "signal",
    "shows",
    "status",
    "data",
    "node",
    "continual",
    "pulse",
    "going",
    "saying",
    "hey",
    "ready",
    "whatever",
    "instructions",
    "data",
    "want",
    "send",
    "see",
    "divided",
    "rack",
    "1",
    "rack",
    "2",
    "different",
    "data",
    "nodes",
    "also",
    "replications",
    "talked",
    "replicate",
    "data",
    "replicate",
    "three",
    "different",
    "locations",
    "client",
    "machine",
    "client",
    "first",
    "requests",
    "name",
    "node",
    "read",
    "data",
    "familiar",
    "client",
    "machine",
    "client",
    "programmer",
    "client",
    "logged",
    "external",
    "hadoop",
    "file",
    "system",
    "sending",
    "instructions",
    "client",
    "whatever",
    "instructions",
    "script",
    "sending",
    "first",
    "request",
    "name",
    "node",
    "read",
    "data",
    "name",
    "node",
    "allows",
    "client",
    "read",
    "requested",
    "data",
    "data",
    "nodes",
    "data",
    "read",
    "data",
    "nodes",
    "sent",
    "client",
    "see",
    "basically",
    "name",
    "node",
    "connects",
    "client",
    "says",
    "data",
    "stream",
    "query",
    "sent",
    "returning",
    "data",
    "asked",
    "course",
    "goes",
    "finishes",
    "says",
    "oh",
    "metadata",
    "operations",
    "goes",
    "finalizes",
    "request",
    "thing",
    "name",
    "node",
    "sending",
    "information",
    "client",
    "sends",
    "information",
    "block",
    "operations",
    "block",
    "operations",
    "performs",
    "creation",
    "data",
    "going",
    "create",
    "new",
    "files",
    "folders",
    "going",
    "delete",
    "folders",
    "also",
    "covers",
    "replication",
    "folders",
    "goes",
    "background",
    "see",
    "nice",
    "full",
    "picture",
    "see",
    "client",
    "machine",
    "comes",
    "cues",
    "metadata",
    "metadata",
    "goes",
    "stores",
    "metadata",
    "goes",
    "block",
    "operations",
    "maybe",
    "sending",
    "data",
    "hadoop",
    "file",
    "system",
    "maybe",
    "querying",
    "maybe",
    "asking",
    "delete",
    "sending",
    "data",
    "replication",
    "goes",
    "back",
    "data",
    "client",
    "writing",
    "data",
    "data",
    "node",
    "course",
    "replicating",
    "part",
    "block",
    "operations",
    "let",
    "talk",
    "little",
    "bit",
    "read",
    "mechanisms",
    "hadoop",
    "file",
    "system",
    "hadoop",
    "file",
    "system",
    "read",
    "mechanism",
    "hadoop",
    "file",
    "system",
    "client",
    "computer",
    "client",
    "jvm",
    "client",
    "node",
    "client",
    "jvm",
    "java",
    "virtual",
    "machine",
    "going",
    "client",
    "node",
    "zooming",
    "read",
    "mechanism",
    "looking",
    "picture",
    "guess",
    "client",
    "reading",
    "data",
    "also",
    "another",
    "client",
    "writing",
    "data",
    "going",
    "look",
    "little",
    "closer",
    "name",
    "node",
    "racks",
    "data",
    "nodes",
    "racks",
    "computers",
    "client",
    "first",
    "thing",
    "opens",
    "connection",
    "distributed",
    "file",
    "system",
    "hdfs",
    "goes",
    "hey",
    "get",
    "block",
    "locations",
    "goes",
    "using",
    "rpc",
    "remote",
    "procedure",
    "call",
    "gets",
    "locations",
    "name",
    "node",
    "first",
    "checks",
    "client",
    "authorized",
    "access",
    "requested",
    "file",
    "yes",
    "provides",
    "block",
    "location",
    "token",
    "client",
    "showing",
    "slave",
    "authentication",
    "name",
    "node",
    "tells",
    "client",
    "hey",
    "token",
    "client",
    "going",
    "come",
    "get",
    "information",
    "tells",
    "client",
    "oh",
    "hey",
    "information",
    "telling",
    "script",
    "sent",
    "query",
    "data",
    "whether",
    "writing",
    "script",
    "one",
    "many",
    "setups",
    "available",
    "hadoop",
    "file",
    "system",
    "connection",
    "code",
    "client",
    "machine",
    "point",
    "reads",
    "fs",
    "data",
    "input",
    "stream",
    "comes",
    "see",
    "right",
    "one",
    "two",
    "verify",
    "give",
    "information",
    "need",
    "three",
    "going",
    "read",
    "input",
    "stream",
    "input",
    "stream",
    "going",
    "grab",
    "different",
    "nodes",
    "supply",
    "tokens",
    "machines",
    "saying",
    "hey",
    "client",
    "needs",
    "data",
    "token",
    "good",
    "go",
    "let",
    "data",
    "client",
    "show",
    "authentication",
    "token",
    "data",
    "nodes",
    "read",
    "process",
    "begin",
    "reaching",
    "end",
    "data",
    "block",
    "connection",
    "closed",
    "see",
    "gone",
    "different",
    "steps",
    "get",
    "block",
    "locations",
    "step",
    "one",
    "open",
    "connection",
    "get",
    "block",
    "locations",
    "using",
    "rpc",
    "actively",
    "go",
    "fs",
    "data",
    "input",
    "stream",
    "grab",
    "different",
    "data",
    "brings",
    "back",
    "client",
    "done",
    "closes",
    "connection",
    "client",
    "case",
    "programmer",
    "know",
    "manager",
    "gone",
    "pig",
    "script",
    "actual",
    "coding",
    "hadoop",
    "pulling",
    "data",
    "called",
    "pig",
    "hive",
    "get",
    "data",
    "back",
    "close",
    "connection",
    "delete",
    "randomly",
    "huge",
    "series",
    "tokens",
    "ca",
    "used",
    "anymore",
    "done",
    "query",
    "go",
    "ahead",
    "zoom",
    "little",
    "bit",
    "let",
    "look",
    "even",
    "little",
    "closer",
    "hadoop",
    "file",
    "system",
    "client",
    "client",
    "jvm",
    "java",
    "virtual",
    "machine",
    "client",
    "node",
    "data",
    "read",
    "block",
    "block",
    "b",
    "request",
    "read",
    "block",
    "b",
    "goes",
    "name",
    "node",
    "two",
    "sends",
    "location",
    "case",
    "um",
    "ip",
    "addresses",
    "blocks",
    "dn1",
    "dn2",
    "blocks",
    "stored",
    "client",
    "interacts",
    "data",
    "nodes",
    "switches",
    "core",
    "switch",
    "client",
    "node",
    "comes",
    "three",
    "goes",
    "core",
    "switch",
    "goes",
    "rack",
    "switch",
    "1",
    "rack",
    "switch",
    "2",
    "rack",
    "switch",
    "looking",
    "automatically",
    "see",
    "point",
    "failure",
    "core",
    "switch",
    "certainly",
    "want",
    "high",
    "end",
    "switch",
    "mechanism",
    "core",
    "switch",
    "want",
    "use",
    "enterprise",
    "hardware",
    "get",
    "racks",
    "commodity",
    "rack",
    "switches",
    "one",
    "goes",
    "care",
    "much",
    "get",
    "swap",
    "really",
    "quick",
    "see",
    "block",
    "replicated",
    "three",
    "times",
    "block",
    "b",
    "pull",
    "come",
    "data",
    "read",
    "dn1",
    "dn2",
    "closest",
    "see",
    "going",
    "read",
    "two",
    "different",
    "racks",
    "going",
    "read",
    "one",
    "rack",
    "whatever",
    "closest",
    "setup",
    "query",
    "reason",
    "10",
    "queries",
    "going",
    "want",
    "one",
    "pull",
    "data",
    "one",
    "setup",
    "minimizes",
    "traffic",
    "response",
    "data",
    "nodes",
    "client",
    "read",
    "operation",
    "successful",
    "says",
    "ah",
    "read",
    "data",
    "successful",
    "always",
    "good",
    "like",
    "successful",
    "know",
    "like",
    "successful",
    "look",
    "read",
    "mechanism",
    "let",
    "go",
    "ahead",
    "zoom",
    "look",
    "write",
    "mechanism",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "write",
    "mechanism",
    "hdfs",
    "write",
    "mechanism",
    "client",
    "machine",
    "programmer",
    "computer",
    "going",
    "client",
    "java",
    "machine",
    "java",
    "virtual",
    "machine",
    "jvm",
    "occurring",
    "client",
    "node",
    "somebody",
    "office",
    "maybe",
    "local",
    "server",
    "office",
    "name",
    "node",
    "data",
    "nodes",
    "distributed",
    "file",
    "system",
    "client",
    "first",
    "executes",
    "create",
    "file",
    "distributed",
    "file",
    "system",
    "says",
    "hey",
    "going",
    "create",
    "file",
    "goes",
    "rpc",
    "call",
    "like",
    "read",
    "client",
    "first",
    "executes",
    "create",
    "file",
    "distributed",
    "file",
    "system",
    "dfs",
    "interacts",
    "name",
    "node",
    "create",
    "file",
    "name",
    "node",
    "provides",
    "location",
    "write",
    "data",
    "hdfs",
    "client",
    "fs",
    "data",
    "output",
    "stream",
    "time",
    "instead",
    "data",
    "going",
    "client",
    "coming",
    "client",
    "client",
    "writes",
    "data",
    "fs",
    "data",
    "output",
    "stream",
    "keep",
    "mind",
    "output",
    "stream",
    "client",
    "could",
    "streaming",
    "code",
    "could",
    "mean",
    "know",
    "always",
    "refer",
    "client",
    "computer",
    "programmer",
    "writing",
    "could",
    "sql",
    "server",
    "data",
    "current",
    "current",
    "sales",
    "archiving",
    "information",
    "scoop",
    "one",
    "tools",
    "hadoop",
    "file",
    "system",
    "could",
    "streaming",
    "data",
    "could",
    "connection",
    "stock",
    "servers",
    "pulling",
    "stock",
    "data",
    "servers",
    "regular",
    "time",
    "controlled",
    "actually",
    "set",
    "code",
    "controlled",
    "many",
    "different",
    "features",
    "hadoop",
    "file",
    "system",
    "different",
    "resources",
    "sit",
    "top",
    "client",
    "writes",
    "data",
    "fs",
    "data",
    "output",
    "stream",
    "fs",
    "data",
    "output",
    "stream",
    "see",
    "goes",
    "right",
    "packet",
    "divides",
    "packets",
    "128",
    "megabytes",
    "data",
    "written",
    "slave",
    "replicates",
    "data",
    "coming",
    "remember",
    "correctly",
    "part",
    "fs",
    "data",
    "setup",
    "tells",
    "replicate",
    "data",
    "node",
    "like",
    "oh",
    "hey",
    "okay",
    "got",
    "data",
    "coming",
    "also",
    "given",
    "tokens",
    "send",
    "replications",
    "acknowledgement",
    "sent",
    "required",
    "replicas",
    "made",
    "goes",
    "back",
    "saying",
    "hey",
    "successful",
    "written",
    "data",
    "made",
    "three",
    "replications",
    "far",
    "know",
    "going",
    "pipeline",
    "data",
    "nodes",
    "goes",
    "back",
    "says",
    "date",
    "written",
    "client",
    "performs",
    "close",
    "method",
    "client",
    "done",
    "says",
    "okay",
    "done",
    "end",
    "data",
    "finished",
    "date",
    "written",
    "client",
    "performs",
    "close",
    "method",
    "see",
    "quick",
    "reshape",
    "go",
    "like",
    "read",
    "create",
    "connection",
    "creates",
    "name",
    "node",
    "lets",
    "know",
    "going",
    "step",
    "two",
    "step",
    "three",
    "also",
    "includes",
    "uh",
    "tokens",
    "everything",
    "go",
    "step",
    "three",
    "writing",
    "fs",
    "data",
    "output",
    "stream",
    "sorts",
    "whatever",
    "data",
    "node",
    "going",
    "go",
    "also",
    "tells",
    "replicate",
    "data",
    "node",
    "sends",
    "data",
    "nodes",
    "replication",
    "course",
    "finalize",
    "close",
    "everything",
    "marks",
    "complete",
    "name",
    "node",
    "deletes",
    "magical",
    "tokens",
    "background",
    "ca",
    "reused",
    "go",
    "ahead",
    "example",
    "setups",
    "whether",
    "read",
    "write",
    "similar",
    "come",
    "client",
    "name",
    "node",
    "see",
    "right",
    "actually",
    "depicting",
    "actual",
    "rack",
    "switch",
    "going",
    "data",
    "comes",
    "request",
    "like",
    "saw",
    "earlier",
    "sends",
    "location",
    "data",
    "nodes",
    "actually",
    "turns",
    "ip",
    "addresses",
    "dynamic",
    "node",
    "connections",
    "come",
    "back",
    "client",
    "hdfs",
    "client",
    "point",
    "tokens",
    "goes",
    "core",
    "switch",
    "core",
    "switch",
    "says",
    "hey",
    "goes",
    "client",
    "interacts",
    "data",
    "nodes",
    "switches",
    "see",
    "writing",
    "block",
    "replication",
    "block",
    "second",
    "replication",
    "block",
    "second",
    "server",
    "blocking",
    "replicated",
    "second",
    "server",
    "third",
    "server",
    "point",
    "three",
    "replications",
    "block",
    "comes",
    "back",
    "says",
    "acknowledges",
    "says",
    "hey",
    "done",
    "goes",
    "back",
    "hadoop",
    "file",
    "system",
    "client",
    "says",
    "okay",
    "done",
    "finally",
    "success",
    "written",
    "name",
    "node",
    "closes",
    "everything",
    "quick",
    "recap",
    "hadoop",
    "file",
    "system",
    "advantages",
    "first",
    "one",
    "probably",
    "one",
    "huge",
    "one",
    "multiple",
    "data",
    "copies",
    "available",
    "fault",
    "tolerant",
    "whole",
    "racks",
    "go",
    "switches",
    "go",
    "even",
    "main",
    "name",
    "node",
    "could",
    "go",
    "secondary",
    "name",
    "node",
    "something",
    "talk",
    "much",
    "scalable",
    "uses",
    "distributed",
    "storage",
    "run",
    "oh",
    "gosh",
    "space",
    "need",
    "heavier",
    "processing",
    "let",
    "add",
    "another",
    "rack",
    "computers",
    "scale",
    "quickly",
    "linear",
    "scalability",
    "used",
    "bought",
    "server",
    "would",
    "pay",
    "lot",
    "money",
    "get",
    "craig",
    "computer",
    "remember",
    "big",
    "craig",
    "computers",
    "coming",
    "craig",
    "computer",
    "runs",
    "2",
    "million",
    "year",
    "maintenance",
    "liquid",
    "cool",
    "expensive",
    "compared",
    "adding",
    "racks",
    "computer",
    "extending",
    "data",
    "center",
    "cost",
    "effective",
    "since",
    "commodity",
    "hardware",
    "used",
    "talking",
    "cheap",
    "knockoff",
    "computers",
    "still",
    "need",
    "enterprise",
    "uh",
    "name",
    "node",
    "rest",
    "literally",
    "tenth",
    "cost",
    "storing",
    "data",
    "traditional",
    "computers",
    "data",
    "secure",
    "data",
    "security",
    "provides",
    "data",
    "security",
    "data",
    "theory",
    "play",
    "make",
    "much",
    "fun",
    "let",
    "go",
    "ahead",
    "show",
    "looks",
    "like",
    "far",
    "dimensions",
    "getting",
    "pulling",
    "data",
    "putting",
    "data",
    "hadoop",
    "file",
    "system",
    "see",
    "oracle",
    "virtual",
    "machine",
    "virtual",
    "box",
    "manager",
    "couple",
    "different",
    "things",
    "loaded",
    "cloudera",
    "one",
    "probably",
    "explain",
    "things",
    "new",
    "virtual",
    "machines",
    "oracle",
    "virtualbox",
    "allows",
    "spin",
    "machine",
    "separate",
    "computer",
    "case",
    "running",
    "believe",
    "centos",
    "linux",
    "creates",
    "like",
    "box",
    "computer",
    "centos",
    "running",
    "machine",
    "running",
    "windows",
    "10",
    "happens",
    "windows",
    "10",
    "computer",
    "underneath",
    "actually",
    "go",
    "let",
    "open",
    "general",
    "might",
    "hard",
    "see",
    "see",
    "actually",
    "go",
    "system",
    "processor",
    "happen",
    "8",
    "core",
    "16",
    "dedicated",
    "threads",
    "registers",
    "16",
    "cpus",
    "8",
    "cores",
    "designated",
    "one",
    "cpu",
    "going",
    "use",
    "one",
    "dedicated",
    "threads",
    "computer",
    "oracle",
    "virtual",
    "machine",
    "open",
    "source",
    "see",
    "right",
    "oracle",
    "usually",
    "search",
    "downloading",
    "virtualbox",
    "search",
    "virtualbox",
    "one",
    "word",
    "come",
    "page",
    "download",
    "whatever",
    "operating",
    "system",
    "working",
    "certainly",
    "number",
    "different",
    "options",
    "let",
    "go",
    "point",
    "setting",
    "demoing",
    "first",
    "thing",
    "note",
    "virtual",
    "box",
    "cloudera",
    "horton",
    "setup",
    "virtual",
    "box",
    "hadoop",
    "system",
    "try",
    "need",
    "minimum",
    "12",
    "gigabytes",
    "windows",
    "10",
    "home",
    "edition",
    "problems",
    "virtual",
    "setup",
    "sometimes",
    "go",
    "turn",
    "virtual",
    "settings",
    "knows",
    "home",
    "setup",
    "sources",
    "cloudera",
    "talk",
    "little",
    "bit",
    "clare",
    "dara",
    "second",
    "cloudera",
    "online",
    "live",
    "go",
    "try",
    "cloudera",
    "setup",
    "never",
    "used",
    "cloudera",
    "pretty",
    "good",
    "company",
    "cloudera",
    "hortonworks",
    "two",
    "common",
    "ones",
    "actually",
    "running",
    "cloudera",
    "hadoop",
    "cluster",
    "demo",
    "oracle",
    "virtual",
    "machine",
    "also",
    "option",
    "different",
    "vmware",
    "another",
    "one",
    "like",
    "virtual",
    "machine",
    "paid",
    "service",
    "free",
    "setup",
    "work",
    "fine",
    "cloudera",
    "like",
    "new",
    "online",
    "setup",
    "go",
    "online",
    "cloudera",
    "want",
    "go",
    "underneath",
    "cloudera",
    "quick",
    "start",
    "type",
    "search",
    "cloudera",
    "quickstart",
    "bring",
    "website",
    "select",
    "platform",
    "case",
    "virtualbox",
    "vmware",
    "talked",
    "docker",
    "docker",
    "virtual",
    "setup",
    "unless",
    "already",
    "know",
    "really",
    "want",
    "mess",
    "kvm",
    "linux",
    "computer",
    "sets",
    "multiple",
    "systems",
    "computer",
    "two",
    "really",
    "want",
    "use",
    "usually",
    "virtual",
    "box",
    "online",
    "setup",
    "see",
    "download",
    "going",
    "horton",
    "version",
    "called",
    "hortonworks",
    "call",
    "sandbox",
    "see",
    "term",
    "hortonworks",
    "sandbox",
    "test",
    "demos",
    "going",
    "deploy",
    "single",
    "node",
    "hadoop",
    "systems",
    "would",
    "kind",
    "ridiculous",
    "defeat",
    "whole",
    "purpose",
    "horton",
    "hadoop",
    "system",
    "installed",
    "one",
    "computer",
    "virtual",
    "node",
    "lot",
    "different",
    "options",
    "professional",
    "windows",
    "version",
    "least",
    "12",
    "gigabytes",
    "ram",
    "run",
    "want",
    "try",
    "see",
    "find",
    "online",
    "version",
    "course",
    "simplylearn",
    "labs",
    "sign",
    "classes",
    "set",
    "know",
    "last",
    "time",
    "five",
    "node",
    "uh",
    "setup",
    "get",
    "around",
    "see",
    "going",
    "whether",
    "studying",
    "admin",
    "side",
    "programming",
    "side",
    "script",
    "writing",
    "go",
    "oracle",
    "virtual",
    "box",
    "go",
    "cloudera",
    "start",
    "one",
    "flavors",
    "um",
    "horton",
    "uses",
    "login",
    "log",
    "everything",
    "local",
    "host",
    "internet",
    "explorer",
    "might",
    "use",
    "chrome",
    "cloudera",
    "actually",
    "opens",
    "full",
    "interface",
    "actually",
    "setup",
    "see",
    "uh",
    "started",
    "let",
    "go",
    "back",
    "downloaded",
    "big",
    "download",
    "way",
    "import",
    "appliance",
    "virtualbox",
    "first",
    "time",
    "ran",
    "takes",
    "long",
    "time",
    "configure",
    "setup",
    "second",
    "time",
    "comes",
    "pretty",
    "quick",
    "cloudera",
    "quick",
    "start",
    "pretend",
    "single",
    "node",
    "opens",
    "see",
    "actually",
    "firefox",
    "web",
    "browser",
    "go",
    "localhost",
    "actually",
    "already",
    "quickstart",
    "cloudera",
    "come",
    "see",
    "getting",
    "started",
    "information",
    "analyze",
    "data",
    "manage",
    "cluster",
    "general",
    "information",
    "always",
    "want",
    "start",
    "go",
    "ahead",
    "open",
    "terminal",
    "window",
    "open",
    "terminal",
    "widen",
    "little",
    "bit",
    "let",
    "maximize",
    "see",
    "virtual",
    "machine",
    "virtual",
    "machine",
    "centos",
    "linux",
    "linux",
    "computer",
    "windows",
    "computer",
    "terminal",
    "window",
    "basic",
    "terminal",
    "list",
    "see",
    "documents",
    "eclipse",
    "different",
    "things",
    "installed",
    "quick",
    "start",
    "guide",
    "linux",
    "system",
    "linux",
    "computer",
    "hadoop",
    "running",
    "hadoop",
    "single",
    "node",
    "name",
    "node",
    "data",
    "node",
    "everything",
    "squished",
    "together",
    "one",
    "virtual",
    "machine",
    "let",
    "hdfs",
    "telling",
    "hadoop",
    "file",
    "system",
    "dfs",
    "minus",
    "ls",
    "notice",
    "ls",
    "ls",
    "list",
    "ls",
    "list",
    "click",
    "take",
    "second",
    "reading",
    "hadoop",
    "file",
    "system",
    "comes",
    "nothing",
    "quick",
    "recap",
    "let",
    "go",
    "back",
    "three",
    "different",
    "environments",
    "one",
    "let",
    "put",
    "bright",
    "red",
    "actually",
    "see",
    "environment",
    "slides",
    "environment",
    "list",
    "looking",
    "files",
    "linuxcentos",
    "computer",
    "system",
    "looking",
    "files",
    "hadoop",
    "file",
    "system",
    "three",
    "completely",
    "separate",
    "environments",
    "connect",
    "right",
    "whatever",
    "files",
    "personal",
    "files",
    "um",
    "course",
    "also",
    "looking",
    "screen",
    "windows",
    "10",
    "looking",
    "screen",
    "uh",
    "list",
    "let",
    "look",
    "files",
    "screen",
    "centos",
    "linux",
    "looking",
    "files",
    "right",
    "hadoop",
    "file",
    "system",
    "three",
    "completely",
    "separate",
    "files",
    "one",
    "linux",
    "running",
    "virtual",
    "box",
    "virtual",
    "box",
    "using",
    "one",
    "core",
    "run",
    "one",
    "cpu",
    "everything",
    "file",
    "system",
    "see",
    "desktop",
    "documents",
    "whatever",
    "see",
    "right",
    "files",
    "hadoop",
    "file",
    "system",
    "hadoop",
    "file",
    "system",
    "currently",
    "stored",
    "linux",
    "machine",
    "could",
    "stored",
    "across",
    "10",
    "linux",
    "machines",
    "20",
    "100",
    "could",
    "stored",
    "across",
    "petabytes",
    "mean",
    "could",
    "really",
    "huge",
    "could",
    "case",
    "demo",
    "putting",
    "one",
    "computer",
    "let",
    "see",
    "real",
    "quick",
    "go",
    "view",
    "zoom",
    "view",
    "zoom",
    "standard",
    "browser",
    "use",
    "like",
    "control",
    "plus",
    "stuff",
    "like",
    "zoom",
    "common",
    "browser",
    "window",
    "hadoop",
    "file",
    "system",
    "right",
    "linux",
    "going",
    "oh",
    "let",
    "create",
    "file",
    "go",
    "file",
    "new",
    "file",
    "going",
    "use",
    "vi",
    "vi",
    "editor",
    "basic",
    "editor",
    "go",
    "type",
    "something",
    "one",
    "two",
    "three",
    "four",
    "maybe",
    "columns",
    "44",
    "66",
    "course",
    "new",
    "file",
    "system",
    "like",
    "regular",
    "computer",
    "also",
    "vi",
    "editor",
    "hit",
    "colon",
    "actually",
    "work",
    "lot",
    "different",
    "editors",
    "write",
    "quit",
    "vi",
    "let",
    "take",
    "look",
    "see",
    "happened",
    "linux",
    "system",
    "type",
    "ls",
    "list",
    "see",
    "new",
    "file",
    "sure",
    "enough",
    "let",
    "highlight",
    "new",
    "file",
    "go",
    "hadoop",
    "system",
    "hdfs",
    "dfs",
    "minus",
    "ls",
    "list",
    "still",
    "show",
    "nothing",
    "still",
    "empty",
    "simply",
    "go",
    "hdfs",
    "dfs",
    "minus",
    "put",
    "going",
    "put",
    "new",
    "file",
    "going",
    "move",
    "linux",
    "system",
    "file",
    "folder",
    "hadoop",
    "file",
    "system",
    "go",
    "type",
    "list",
    "new",
    "file",
    "system",
    "see",
    "one",
    "file",
    "new",
    "file",
    "similar",
    "linux",
    "cat",
    "cat",
    "command",
    "simply",
    "uh",
    "evokes",
    "reading",
    "file",
    "hdfs",
    "dfs",
    "minus",
    "cat",
    "look",
    "remember",
    "cloudera",
    "format",
    "going",
    "user",
    "uh",
    "course",
    "path",
    "location",
    "file",
    "name",
    "list",
    "list",
    "see",
    "lists",
    "file",
    "realize",
    "uh",
    "user",
    "cloudera",
    "go",
    "user",
    "cloudera",
    "new",
    "file",
    "minus",
    "cat",
    "able",
    "read",
    "file",
    "see",
    "right",
    "file",
    "linux",
    "system",
    "copied",
    "cloudera",
    "system",
    "one",
    "three",
    "four",
    "five",
    "entered",
    "go",
    "back",
    "linux",
    "list",
    "still",
    "see",
    "new",
    "file",
    "also",
    "something",
    "like",
    "hdfs",
    "minus",
    "mv",
    "new",
    "file",
    "going",
    "change",
    "new",
    "new",
    "file",
    "underneath",
    "hadoop",
    "file",
    "system",
    "minus",
    "mv",
    "rename",
    "go",
    "back",
    "hadoop",
    "file",
    "system",
    "ls",
    "see",
    "instead",
    "new",
    "file",
    "new",
    "new",
    "file",
    "coming",
    "new",
    "new",
    "files",
    "renamed",
    "also",
    "go",
    "delete",
    "come",
    "hdfs",
    "dfs",
    "also",
    "remove",
    "remove",
    "file",
    "come",
    "run",
    "see",
    "come",
    "back",
    "list",
    "file",
    "gone",
    "another",
    "empty",
    "folder",
    "hadoop",
    "file",
    "system",
    "like",
    "file",
    "system",
    "take",
    "go",
    "ahead",
    "make",
    "directory",
    "create",
    "new",
    "directory",
    "mk",
    "make",
    "directory",
    "call",
    "dir",
    "going",
    "make",
    "directory",
    "dir",
    "take",
    "second",
    "course",
    "list",
    "command",
    "see",
    "directory",
    "give",
    "second",
    "comes",
    "mydir",
    "like",
    "go",
    "going",
    "put",
    "file",
    "remember",
    "correctly",
    "files",
    "setup",
    "called",
    "new",
    "file",
    "coming",
    "linux",
    "system",
    "going",
    "put",
    "dir",
    "target",
    "hadoop",
    "setup",
    "hit",
    "enter",
    "hadoop",
    "list",
    "going",
    "show",
    "file",
    "remember",
    "put",
    "kadroop",
    "show",
    "directory",
    "list",
    "dur",
    "directory",
    "see",
    "underneath",
    "directory",
    "hadoop",
    "file",
    "system",
    "new",
    "file",
    "put",
    "good",
    "operating",
    "system",
    "need",
    "minus",
    "help",
    "like",
    "type",
    "help",
    "linux",
    "come",
    "type",
    "hdfs",
    "help",
    "shows",
    "lot",
    "commands",
    "underneath",
    "hadoop",
    "file",
    "system",
    "similar",
    "linux",
    "also",
    "something",
    "like",
    "hadoop",
    "version",
    "hadoop",
    "version",
    "shows",
    "hadoop",
    "cdh",
    "cloudera",
    "5",
    "compiled",
    "jenkins",
    "date",
    "different",
    "information",
    "hadoop",
    "file",
    "system",
    "basics",
    "terminal",
    "window",
    "let",
    "go",
    "ahead",
    "close",
    "going",
    "play",
    "really",
    "come",
    "let",
    "maximize",
    "cloud",
    "data",
    "opens",
    "browser",
    "window",
    "browser",
    "window",
    "could",
    "access",
    "might",
    "look",
    "like",
    "access",
    "hadoop",
    "file",
    "system",
    "one",
    "fun",
    "things",
    "first",
    "starting",
    "go",
    "hue",
    "see",
    "top",
    "cloudera",
    "hue",
    "hadoop",
    "hbase",
    "impala",
    "spark",
    "standard",
    "installs",
    "hue",
    "basically",
    "overview",
    "file",
    "system",
    "come",
    "see",
    "queries",
    "far",
    "hbase",
    "hive",
    "hive",
    "database",
    "go",
    "top",
    "says",
    "file",
    "browser",
    "go",
    "file",
    "browser",
    "hadoop",
    "file",
    "system",
    "looking",
    "open",
    "file",
    "browser",
    "see",
    "directory",
    "created",
    "click",
    "directory",
    "new",
    "file",
    "click",
    "new",
    "file",
    "actually",
    "opens",
    "see",
    "hadoop",
    "file",
    "system",
    "hadoop",
    "file",
    "system",
    "file",
    "created",
    "covered",
    "terminal",
    "window",
    "see",
    "terminal",
    "window",
    "might",
    "web",
    "browser",
    "look",
    "little",
    "different",
    "actually",
    "opens",
    "web",
    "browser",
    "terminal",
    "window",
    "looked",
    "little",
    "bit",
    "hue",
    "one",
    "basic",
    "components",
    "hadoop",
    "one",
    "original",
    "components",
    "going",
    "looking",
    "data",
    "databases",
    "course",
    "hue4",
    "gone",
    "number",
    "changes",
    "see",
    "lot",
    "different",
    "choices",
    "different",
    "tools",
    "hadoop",
    "file",
    "system",
    "go",
    "ahead",
    "close",
    "one",
    "cool",
    "things",
    "virtual",
    "uh",
    "box",
    "either",
    "save",
    "machine",
    "state",
    "send",
    "shutdown",
    "signal",
    "power",
    "machine",
    "go",
    "power",
    "machine",
    "completely",
    "suppose",
    "library",
    "collection",
    "huge",
    "number",
    "books",
    "floor",
    "want",
    "count",
    "total",
    "number",
    "books",
    "present",
    "floor",
    "would",
    "approach",
    "could",
    "say",
    "think",
    "take",
    "lot",
    "time",
    "obviously",
    "efficient",
    "way",
    "counting",
    "number",
    "books",
    "huge",
    "collection",
    "every",
    "floor",
    "could",
    "different",
    "approach",
    "alternative",
    "could",
    "think",
    "asking",
    "three",
    "friends",
    "three",
    "colleagues",
    "could",
    "say",
    "friend",
    "could",
    "count",
    "books",
    "every",
    "floor",
    "obviously",
    "would",
    "make",
    "work",
    "faster",
    "easier",
    "count",
    "books",
    "every",
    "floor",
    "mean",
    "parallel",
    "processing",
    "say",
    "parallel",
    "processing",
    "technical",
    "terms",
    "talking",
    "using",
    "multiple",
    "machines",
    "machine",
    "would",
    "contributing",
    "ram",
    "cpu",
    "cores",
    "processing",
    "data",
    "would",
    "processed",
    "multiple",
    "machines",
    "time",
    "type",
    "process",
    "involves",
    "parallel",
    "processing",
    "case",
    "library",
    "example",
    "would",
    "person",
    "1",
    "would",
    "taking",
    "care",
    "books",
    "floor",
    "1",
    "counting",
    "person",
    "2",
    "floor",
    "2",
    "someone",
    "floor",
    "3",
    "someone",
    "floor",
    "every",
    "individual",
    "would",
    "counting",
    "books",
    "every",
    "floor",
    "parallel",
    "reduces",
    "time",
    "consumed",
    "activity",
    "mechanism",
    "counts",
    "every",
    "floor",
    "aggregated",
    "person",
    "person",
    "mapping",
    "data",
    "particular",
    "floor",
    "say",
    "person",
    "kind",
    "activity",
    "basically",
    "task",
    "every",
    "floor",
    "task",
    "counting",
    "books",
    "every",
    "floor",
    "could",
    "aggregation",
    "mechanism",
    "could",
    "basically",
    "reduce",
    "summarize",
    "total",
    "count",
    "terms",
    "map",
    "reduce",
    "would",
    "say",
    "work",
    "reducer",
    "talk",
    "hadoop",
    "map",
    "reduce",
    "processes",
    "data",
    "different",
    "node",
    "machines",
    "whole",
    "concept",
    "hadoop",
    "framework",
    "right",
    "data",
    "stored",
    "across",
    "machines",
    "would",
    "also",
    "want",
    "process",
    "data",
    "locally",
    "instead",
    "transferring",
    "data",
    "one",
    "machine",
    "machine",
    "bringing",
    "data",
    "together",
    "central",
    "processing",
    "unit",
    "processing",
    "would",
    "rather",
    "data",
    "processed",
    "machines",
    "wherever",
    "stored",
    "know",
    "case",
    "hadoop",
    "cluster",
    "would",
    "data",
    "stored",
    "multiple",
    "data",
    "nodes",
    "multiple",
    "disks",
    "data",
    "needs",
    "processed",
    "requirement",
    "want",
    "process",
    "data",
    "fast",
    "possible",
    "could",
    "achieved",
    "using",
    "parallel",
    "processing",
    "case",
    "mapreduce",
    "basically",
    "first",
    "phase",
    "mapping",
    "phase",
    "case",
    "mapreduce",
    "programming",
    "model",
    "basically",
    "two",
    "phases",
    "one",
    "mapping",
    "one",
    "reducing",
    "takes",
    "care",
    "things",
    "mapping",
    "phase",
    "mapper",
    "class",
    "mapper",
    "class",
    "function",
    "provided",
    "developer",
    "takes",
    "care",
    "individual",
    "map",
    "tasks",
    "work",
    "multiple",
    "nodes",
    "parallel",
    "reducer",
    "class",
    "belongs",
    "reducing",
    "phase",
    "reducing",
    "phase",
    "basically",
    "uses",
    "reducer",
    "class",
    "provides",
    "function",
    "aggregate",
    "reduce",
    "output",
    "different",
    "data",
    "nodes",
    "generate",
    "final",
    "output",
    "mapreduce",
    "works",
    "using",
    "mapping",
    "obviously",
    "reducing",
    "could",
    "kind",
    "jobs",
    "map",
    "jobs",
    "wherein",
    "reducing",
    "required",
    "talking",
    "talking",
    "requirement",
    "would",
    "want",
    "process",
    "data",
    "using",
    "mapping",
    "reducing",
    "especially",
    "data",
    "huge",
    "data",
    "stored",
    "across",
    "multiple",
    "machines",
    "would",
    "want",
    "process",
    "data",
    "parallel",
    "talk",
    "mapreduce",
    "could",
    "say",
    "programming",
    "model",
    "could",
    "say",
    "internally",
    "processing",
    "engine",
    "hadoop",
    "allows",
    "process",
    "compute",
    "huge",
    "volumes",
    "data",
    "say",
    "huge",
    "volumes",
    "data",
    "talk",
    "terabytes",
    "talk",
    "petabytes",
    "exabytes",
    "amount",
    "data",
    "needs",
    "processed",
    "huge",
    "cluster",
    "could",
    "also",
    "use",
    "mapreduce",
    "programming",
    "model",
    "run",
    "mapreduce",
    "algorithm",
    "local",
    "mode",
    "mean",
    "would",
    "go",
    "local",
    "mode",
    "basically",
    "means",
    "would",
    "mapping",
    "reducing",
    "node",
    "using",
    "processing",
    "capacity",
    "ram",
    "cpu",
    "cores",
    "machine",
    "really",
    "efficient",
    "fact",
    "would",
    "want",
    "map",
    "reduce",
    "work",
    "multiple",
    "nodes",
    "would",
    "obviously",
    "mapping",
    "phase",
    "followed",
    "reducing",
    "phase",
    "intermittently",
    "would",
    "data",
    "generated",
    "would",
    "different",
    "phases",
    "help",
    "whole",
    "processing",
    "talk",
    "hadoop",
    "map",
    "reduce",
    "mainly",
    "talking",
    "two",
    "main",
    "components",
    "two",
    "main",
    "phases",
    "mapping",
    "reducing",
    "mapping",
    "taking",
    "care",
    "map",
    "tasks",
    "reducing",
    "taking",
    "care",
    "reduced",
    "tasks",
    "would",
    "data",
    "would",
    "stored",
    "multiple",
    "machines",
    "talk",
    "data",
    "data",
    "could",
    "different",
    "formats",
    "could",
    "developer",
    "could",
    "specify",
    "format",
    "needs",
    "used",
    "understand",
    "data",
    "coming",
    "data",
    "goes",
    "mapping",
    "internally",
    "would",
    "shuffling",
    "sorting",
    "reducing",
    "gives",
    "final",
    "output",
    "way",
    "access",
    "data",
    "sdfs",
    "way",
    "data",
    "getting",
    "stored",
    "sdfs",
    "input",
    "data",
    "would",
    "one",
    "multiple",
    "files",
    "one",
    "multiple",
    "directories",
    "final",
    "output",
    "also",
    "stored",
    "sdfs",
    "accessed",
    "looked",
    "see",
    "processing",
    "done",
    "correctly",
    "looks",
    "input",
    "data",
    "would",
    "worked",
    "upon",
    "multiple",
    "map",
    "tasks",
    "many",
    "map",
    "tasks",
    "basically",
    "depends",
    "file",
    "depends",
    "input",
    "format",
    "normally",
    "know",
    "hadoop",
    "cluster",
    "would",
    "file",
    "broken",
    "blocks",
    "depending",
    "size",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "still",
    "customized",
    "based",
    "average",
    "size",
    "data",
    "getting",
    "stored",
    "cluster",
    "really",
    "huge",
    "files",
    "getting",
    "stored",
    "cluster",
    "would",
    "certainly",
    "set",
    "higher",
    "block",
    "size",
    "every",
    "file",
    "huge",
    "number",
    "blocks",
    "creating",
    "load",
    "name",
    "nodes",
    "ram",
    "tracking",
    "number",
    "elements",
    "cluster",
    "number",
    "objects",
    "cluster",
    "depending",
    "file",
    "size",
    "file",
    "would",
    "split",
    "multiple",
    "chunks",
    "every",
    "chunk",
    "would",
    "map",
    "task",
    "running",
    "map",
    "task",
    "specified",
    "within",
    "mapper",
    "class",
    "within",
    "mapper",
    "class",
    "mapper",
    "function",
    "basically",
    "says",
    "map",
    "tasks",
    "chunks",
    "processed",
    "data",
    "intermittently",
    "written",
    "sdfs",
    "sorted",
    "shuffled",
    "internal",
    "phases",
    "partitioner",
    "decides",
    "many",
    "reduce",
    "tasks",
    "would",
    "used",
    "data",
    "goes",
    "reducer",
    "could",
    "also",
    "combiner",
    "phase",
    "like",
    "mini",
    "reducer",
    "reduce",
    "operation",
    "reaches",
    "reduce",
    "reducing",
    "phase",
    "taken",
    "care",
    "reducer",
    "class",
    "internally",
    "reducer",
    "function",
    "provided",
    "developer",
    "would",
    "reduced",
    "task",
    "running",
    "data",
    "comes",
    "output",
    "map",
    "tasks",
    "finally",
    "output",
    "generated",
    "stored",
    "sdfs",
    "case",
    "hadoop",
    "accepts",
    "data",
    "different",
    "formats",
    "data",
    "could",
    "compressed",
    "format",
    "data",
    "could",
    "part",
    "k",
    "data",
    "could",
    "afro",
    "text",
    "csv",
    "psv",
    "binary",
    "format",
    "formats",
    "supported",
    "however",
    "remember",
    "talking",
    "data",
    "compressed",
    "also",
    "look",
    "kind",
    "split",
    "ability",
    "compression",
    "mechanism",
    "supports",
    "otherwise",
    "mapreduce",
    "processing",
    "happens",
    "would",
    "take",
    "complete",
    "file",
    "one",
    "chunk",
    "processed",
    "sdfs",
    "accepts",
    "input",
    "data",
    "different",
    "formats",
    "data",
    "stored",
    "sdfs",
    "basically",
    "input",
    "passing",
    "mapping",
    "phase",
    "mapping",
    "phase",
    "said",
    "reads",
    "record",
    "record",
    "depending",
    "input",
    "format",
    "reads",
    "data",
    "multiple",
    "map",
    "tasks",
    "running",
    "multiple",
    "chunks",
    "data",
    "read",
    "broken",
    "individual",
    "elements",
    "say",
    "individual",
    "element",
    "could",
    "say",
    "list",
    "key",
    "value",
    "pairs",
    "records",
    "based",
    "kind",
    "delimiter",
    "without",
    "delimiter",
    "broken",
    "individual",
    "elements",
    "thus",
    "mac",
    "creates",
    "key",
    "value",
    "pairs",
    "key",
    "value",
    "pairs",
    "final",
    "output",
    "key",
    "value",
    "pairs",
    "basically",
    "list",
    "elements",
    "subjected",
    "processing",
    "would",
    "internally",
    "shuffling",
    "sorting",
    "data",
    "relevant",
    "key",
    "value",
    "pairs",
    "brought",
    "together",
    "basically",
    "benefits",
    "processing",
    "reducing",
    "aggregates",
    "key",
    "value",
    "pairs",
    "set",
    "smaller",
    "tuples",
    "tuples",
    "would",
    "say",
    "finally",
    "output",
    "getting",
    "stored",
    "designated",
    "directory",
    "list",
    "aggregated",
    "key",
    "value",
    "pairs",
    "gives",
    "output",
    "talk",
    "mapreduce",
    "one",
    "key",
    "factors",
    "parallel",
    "processing",
    "offer",
    "know",
    "data",
    "getting",
    "stored",
    "across",
    "multiple",
    "data",
    "nodes",
    "would",
    "huge",
    "volume",
    "data",
    "split",
    "randomly",
    "distributed",
    "across",
    "data",
    "nodes",
    "data",
    "needs",
    "processed",
    "best",
    "way",
    "would",
    "parallel",
    "processing",
    "could",
    "data",
    "getting",
    "stored",
    "multiple",
    "data",
    "nodes",
    "multiple",
    "slave",
    "nodes",
    "slave",
    "node",
    "would",
    "one",
    "multiple",
    "disks",
    "process",
    "data",
    "basically",
    "go",
    "parallel",
    "processing",
    "approach",
    "use",
    "mapreduce",
    "let",
    "look",
    "mapreduce",
    "workflow",
    "understand",
    "works",
    "basically",
    "input",
    "data",
    "stored",
    "sdfs",
    "data",
    "needs",
    "processed",
    "stored",
    "input",
    "files",
    "processing",
    "want",
    "done",
    "one",
    "single",
    "file",
    "done",
    "directory",
    "multiple",
    "files",
    "could",
    "also",
    "later",
    "multiple",
    "outputs",
    "merged",
    "achieve",
    "using",
    "something",
    "called",
    "chaining",
    "mappers",
    "data",
    "getting",
    "stored",
    "sdfs",
    "input",
    "format",
    "basically",
    "something",
    "define",
    "input",
    "specification",
    "input",
    "files",
    "split",
    "various",
    "input",
    "formats",
    "search",
    "go",
    "google",
    "basically",
    "search",
    "hadoop",
    "map",
    "reduce",
    "yahoo",
    "tutorial",
    "one",
    "good",
    "links",
    "look",
    "link",
    "search",
    "different",
    "input",
    "formats",
    "output",
    "formats",
    "let",
    "search",
    "input",
    "format",
    "talk",
    "input",
    "format",
    "basically",
    "something",
    "define",
    "input",
    "files",
    "split",
    "input",
    "files",
    "split",
    "read",
    "based",
    "input",
    "format",
    "specified",
    "class",
    "provides",
    "following",
    "functionality",
    "selects",
    "files",
    "objects",
    "used",
    "input",
    "defines",
    "input",
    "split",
    "break",
    "file",
    "tasks",
    "provides",
    "factory",
    "record",
    "reader",
    "objects",
    "read",
    "file",
    "different",
    "formats",
    "look",
    "table",
    "see",
    "text",
    "input",
    "format",
    "default",
    "format",
    "reads",
    "lines",
    "text",
    "file",
    "line",
    "considered",
    "record",
    "key",
    "byte",
    "offset",
    "line",
    "value",
    "line",
    "content",
    "says",
    "key",
    "value",
    "input",
    "format",
    "passes",
    "lines",
    "key",
    "value",
    "pairs",
    "everything",
    "first",
    "tab",
    "character",
    "key",
    "remainder",
    "line",
    "could",
    "also",
    "sequence",
    "file",
    "input",
    "format",
    "basically",
    "works",
    "binary",
    "format",
    "input",
    "format",
    "way",
    "also",
    "search",
    "output",
    "format",
    "takes",
    "care",
    "data",
    "handled",
    "processing",
    "done",
    "key",
    "value",
    "pairs",
    "provided",
    "output",
    "collector",
    "written",
    "output",
    "files",
    "way",
    "written",
    "governed",
    "output",
    "format",
    "functions",
    "pretty",
    "much",
    "like",
    "input",
    "format",
    "described",
    "earlier",
    "right",
    "could",
    "set",
    "output",
    "format",
    "followed",
    "text",
    "output",
    "sequence",
    "file",
    "output",
    "format",
    "null",
    "output",
    "format",
    "different",
    "classes",
    "take",
    "care",
    "data",
    "handled",
    "read",
    "processing",
    "data",
    "written",
    "processing",
    "done",
    "based",
    "input",
    "format",
    "file",
    "broken",
    "splits",
    "logically",
    "represents",
    "data",
    "processed",
    "individual",
    "map",
    "tasks",
    "could",
    "say",
    "individual",
    "mapper",
    "functions",
    "could",
    "one",
    "multiple",
    "splits",
    "need",
    "processed",
    "depending",
    "file",
    "size",
    "depending",
    "properties",
    "set",
    "done",
    "input",
    "splits",
    "subjected",
    "mapping",
    "phase",
    "internally",
    "record",
    "reader",
    "communicates",
    "input",
    "split",
    "converts",
    "data",
    "key",
    "value",
    "pairs",
    "suitable",
    "read",
    "mapper",
    "mapper",
    "basically",
    "working",
    "key",
    "value",
    "pairs",
    "map",
    "task",
    "giving",
    "intermittent",
    "output",
    "would",
    "forwarded",
    "processing",
    "done",
    "key",
    "value",
    "pairs",
    "worked",
    "upon",
    "map",
    "map",
    "tasks",
    "part",
    "mapper",
    "function",
    "generating",
    "key",
    "value",
    "pairs",
    "intermediate",
    "outputs",
    "processed",
    "could",
    "said",
    "combiner",
    "face",
    "internally",
    "mini",
    "radio",
    "surface",
    "combiner",
    "class",
    "combiner",
    "basically",
    "uses",
    "class",
    "reducer",
    "class",
    "provided",
    "developer",
    "main",
    "work",
    "reducing",
    "main",
    "work",
    "kind",
    "mini",
    "aggregation",
    "key",
    "value",
    "pairs",
    "generated",
    "map",
    "data",
    "coming",
    "combiner",
    "internally",
    "partitioner",
    "phase",
    "decides",
    "outputs",
    "combiners",
    "sent",
    "reducers",
    "could",
    "also",
    "say",
    "even",
    "combiner",
    "partitioner",
    "would",
    "decide",
    "based",
    "keys",
    "values",
    "based",
    "type",
    "keys",
    "many",
    "reducers",
    "would",
    "required",
    "many",
    "reduced",
    "tasks",
    "would",
    "required",
    "work",
    "output",
    "generated",
    "map",
    "task",
    "partitioner",
    "decided",
    "data",
    "would",
    "sorted",
    "shuffled",
    "fed",
    "reducer",
    "talk",
    "reducer",
    "would",
    "basically",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "depends",
    "partitioner",
    "decided",
    "determined",
    "data",
    "processed",
    "also",
    "depend",
    "configuration",
    "properties",
    "set",
    "decide",
    "many",
    "radio",
    "stars",
    "used",
    "internally",
    "data",
    "obviously",
    "going",
    "sorting",
    "shuffling",
    "reducing",
    "aggregation",
    "becomes",
    "easier",
    "task",
    "done",
    "basically",
    "reducer",
    "code",
    "reducer",
    "provided",
    "developer",
    "intermediate",
    "data",
    "aggregated",
    "give",
    "final",
    "output",
    "would",
    "stored",
    "sdfs",
    "internal",
    "record",
    "writer",
    "writes",
    "output",
    "key",
    "value",
    "pairs",
    "reducer",
    "output",
    "files",
    "mapreduce",
    "works",
    "wherein",
    "final",
    "output",
    "data",
    "stored",
    "read",
    "accessed",
    "sdfs",
    "even",
    "used",
    "input",
    "mapreduce",
    "kind",
    "processing",
    "overall",
    "looks",
    "basically",
    "data",
    "stored",
    "sdfs",
    "based",
    "input",
    "format",
    "splits",
    "record",
    "reader",
    "gives",
    "data",
    "mapping",
    "phase",
    "taken",
    "care",
    "mapper",
    "function",
    "mapper",
    "function",
    "basically",
    "means",
    "one",
    "multiple",
    "map",
    "tasks",
    "working",
    "chunks",
    "data",
    "could",
    "combiner",
    "phase",
    "optional",
    "mandatory",
    "partitioner",
    "phase",
    "decides",
    "many",
    "reduced",
    "tasks",
    "many",
    "reducers",
    "would",
    "used",
    "work",
    "data",
    "internally",
    "sorting",
    "shuffling",
    "data",
    "happening",
    "basically",
    "based",
    "output",
    "format",
    "record",
    "reader",
    "write",
    "output",
    "sdfs",
    "directory",
    "internally",
    "could",
    "also",
    "remember",
    "data",
    "processed",
    "locally",
    "would",
    "output",
    "task",
    "worked",
    "upon",
    "stored",
    "locally",
    "however",
    "access",
    "data",
    "directly",
    "data",
    "nodes",
    "access",
    "sdfs",
    "output",
    "stored",
    "sdfs",
    "mapreduce",
    "workflow",
    "talk",
    "mapreduce",
    "architecture",
    "would",
    "look",
    "would",
    "basically",
    "edge",
    "node",
    "client",
    "program",
    "api",
    "intends",
    "process",
    "data",
    "submits",
    "job",
    "job",
    "tracker",
    "say",
    "resource",
    "manager",
    "case",
    "hadoop",
    "yarn",
    "framework",
    "right",
    "step",
    "also",
    "say",
    "interaction",
    "name",
    "node",
    "would",
    "already",
    "happened",
    "would",
    "given",
    "information",
    "data",
    "nodes",
    "relevant",
    "data",
    "stored",
    "master",
    "processor",
    "hadoop",
    "version",
    "1",
    "job",
    "tracker",
    "slaves",
    "called",
    "task",
    "trackers",
    "hadoop",
    "version",
    "2",
    "instead",
    "job",
    "tracker",
    "resource",
    "manager",
    "instead",
    "task",
    "trackers",
    "node",
    "managers",
    "basically",
    "resource",
    "manager",
    "assign",
    "job",
    "task",
    "trackers",
    "node",
    "managers",
    "node",
    "managers",
    "discussed",
    "yarn",
    "basically",
    "taking",
    "care",
    "processing",
    "happens",
    "every",
    "note",
    "internally",
    "work",
    "happening",
    "resource",
    "manager",
    "node",
    "managers",
    "application",
    "master",
    "refer",
    "yarn",
    "based",
    "tutorial",
    "understand",
    "processing",
    "master",
    "basically",
    "breaking",
    "application",
    "tasks",
    "internally",
    "application",
    "submitted",
    "application",
    "run",
    "yarn",
    "processing",
    "framework",
    "handled",
    "resource",
    "manager",
    "forget",
    "yarn",
    "part",
    "mean",
    "negotiating",
    "resources",
    "allocates",
    "processing",
    "happen",
    "nodes",
    "right",
    "yarn",
    "handles",
    "processing",
    "request",
    "data",
    "stored",
    "sdfs",
    "broken",
    "one",
    "multiple",
    "splits",
    "depending",
    "input",
    "format",
    "specified",
    "developer",
    "input",
    "splits",
    "worked",
    "upon",
    "one",
    "multiple",
    "map",
    "tasks",
    "running",
    "within",
    "container",
    "nodes",
    "basically",
    "resources",
    "utilized",
    "map",
    "task",
    "would",
    "amount",
    "ram",
    "utilized",
    "data",
    "go",
    "reducing",
    "phase",
    "reduced",
    "task",
    "also",
    "utilizing",
    "ram",
    "cpu",
    "cores",
    "internally",
    "functions",
    "take",
    "care",
    "deciding",
    "number",
    "reducers",
    "mini",
    "reduce",
    "basically",
    "reading",
    "processing",
    "data",
    "multiple",
    "data",
    "nodes",
    "mapreduce",
    "programming",
    "model",
    "makes",
    "parallel",
    "processing",
    "work",
    "processes",
    "data",
    "stored",
    "across",
    "multiple",
    "machines",
    "finally",
    "output",
    "getting",
    "stored",
    "dfs",
    "let",
    "quick",
    "demo",
    "mapreduce",
    "see",
    "works",
    "hadoop",
    "cluster",
    "discussed",
    "briefly",
    "mapreduce",
    "contains",
    "mainly",
    "two",
    "phases",
    "mapping",
    "phase",
    "reducing",
    "phase",
    "mapping",
    "phase",
    "taken",
    "care",
    "mapper",
    "function",
    "reducing",
    "phase",
    "taken",
    "care",
    "reducer",
    "function",
    "also",
    "sorting",
    "shuffling",
    "phases",
    "partitioner",
    "combiner",
    "discuss",
    "detail",
    "later",
    "sessions",
    "let",
    "quick",
    "demo",
    "run",
    "mapreduce",
    "already",
    "existing",
    "package",
    "jar",
    "file",
    "within",
    "apache",
    "hadoop",
    "cluster",
    "even",
    "cloudera",
    "cluster",
    "build",
    "mapreduce",
    "programs",
    "package",
    "jar",
    "transfer",
    "cluster",
    "run",
    "hadoop",
    "cluster",
    "yarn",
    "could",
    "using",
    "already",
    "provided",
    "default",
    "program",
    "let",
    "see",
    "two",
    "machines",
    "brought",
    "basically",
    "would",
    "apache",
    "hadoop",
    "cluster",
    "running",
    "simple",
    "start",
    "hyphen",
    "dot",
    "sh",
    "know",
    "script",
    "deprecated",
    "says",
    "instead",
    "use",
    "start",
    "dfs",
    "start",
    "yarn",
    "still",
    "take",
    "care",
    "starting",
    "cluster",
    "two",
    "nodes",
    "would",
    "one",
    "single",
    "name",
    "node",
    "two",
    "data",
    "nodes",
    "one",
    "secondary",
    "name",
    "node",
    "one",
    "resource",
    "manager",
    "two",
    "node",
    "managers",
    "doubt",
    "cluster",
    "came",
    "always",
    "look",
    "previous",
    "sessions",
    "walkthrough",
    "setting",
    "cluster",
    "apache",
    "could",
    "also",
    "cluster",
    "running",
    "using",
    "less",
    "3",
    "gb",
    "total",
    "machine",
    "ram",
    "could",
    "apache",
    "cluster",
    "running",
    "machine",
    "cluster",
    "comes",
    "also",
    "look",
    "web",
    "ui",
    "available",
    "name",
    "node",
    "resource",
    "manager",
    "based",
    "settings",
    "given",
    "uis",
    "show",
    "us",
    "details",
    "cluster",
    "remember",
    "ui",
    "browse",
    "cluster",
    "come",
    "jps",
    "look",
    "java",
    "related",
    "processes",
    "show",
    "processes",
    "running",
    "c1",
    "data",
    "node",
    "resource",
    "manager",
    "node",
    "manager",
    "name",
    "node",
    "m1",
    "machine",
    "second",
    "machine",
    "configured",
    "always",
    "jps",
    "shows",
    "processes",
    "running",
    "also",
    "means",
    "cluster",
    "two",
    "data",
    "nodes",
    "two",
    "node",
    "managers",
    "look",
    "web",
    "ui",
    "refresh",
    "thing",
    "one",
    "refresh",
    "already",
    "opened",
    "web",
    "pages",
    "always",
    "access",
    "web",
    "ui",
    "using",
    "name",
    "notes",
    "hostname",
    "570",
    "port",
    "tells",
    "cluster",
    "id",
    "block",
    "pull",
    "id",
    "gives",
    "information",
    "space",
    "usage",
    "many",
    "live",
    "nodes",
    "even",
    "browse",
    "file",
    "system",
    "put",
    "lot",
    "data",
    "click",
    "browse",
    "file",
    "system",
    "basically",
    "shows",
    "multiple",
    "directories",
    "directories",
    "one",
    "multiple",
    "files",
    "use",
    "mapreduce",
    "example",
    "see",
    "directories",
    "sample",
    "files",
    "although",
    "files",
    "small",
    "like",
    "kilobytes",
    "look",
    "directory",
    "look",
    "pulled",
    "hadoop",
    "logs",
    "put",
    "sdfs",
    "little",
    "bigger",
    "files",
    "also",
    "data",
    "see",
    "data",
    "downloaded",
    "web",
    "either",
    "run",
    "mapreduce",
    "single",
    "file",
    "directory",
    "contains",
    "multiple",
    "files",
    "let",
    "look",
    "looking",
    "demon",
    "mapreduce",
    "also",
    "remember",
    "mapreduce",
    "create",
    "output",
    "directory",
    "need",
    "directory",
    "created",
    "plus",
    "need",
    "permissions",
    "run",
    "mapreduce",
    "job",
    "default",
    "since",
    "running",
    "using",
    "admin",
    "id",
    "problem",
    "intend",
    "run",
    "map",
    "reduce",
    "different",
    "user",
    "obviously",
    "ask",
    "admin",
    "give",
    "user",
    "permission",
    "read",
    "write",
    "sdfs",
    "directory",
    "created",
    "contain",
    "output",
    "mapreduce",
    "job",
    "finishes",
    "cluster",
    "file",
    "system",
    "look",
    "ui",
    "shows",
    "yarn",
    "available",
    "taking",
    "care",
    "processing",
    "shows",
    "total",
    "8",
    "gb",
    "memory",
    "8",
    "v",
    "cores",
    "depending",
    "configuration",
    "set",
    "many",
    "nodes",
    "available",
    "look",
    "nodes",
    "available",
    "shows",
    "two",
    "node",
    "managers",
    "running",
    "8",
    "gb",
    "memory",
    "8",
    "v",
    "cores",
    "true",
    "actually",
    "set",
    "configurations",
    "node",
    "managers",
    "takes",
    "default",
    "properties",
    "8gb",
    "laminate",
    "vcos",
    "yarn",
    "ui",
    "also",
    "look",
    "scheduler",
    "basically",
    "shows",
    "different",
    "cues",
    "configured",
    "run",
    "jobs",
    "discuss",
    "later",
    "detail",
    "let",
    "go",
    "back",
    "terminal",
    "let",
    "see",
    "find",
    "sample",
    "applications",
    "run",
    "cluster",
    "go",
    "terminal",
    "well",
    "submit",
    "mapreduce",
    "job",
    "terminal",
    "know",
    "hadoop",
    "related",
    "directory",
    "within",
    "hadoop",
    "various",
    "directories",
    "discussed",
    "binaries",
    "commands",
    "run",
    "bin",
    "basically",
    "startup",
    "scripts",
    "also",
    "notice",
    "share",
    "directory",
    "end",
    "look",
    "shared",
    "directory",
    "would",
    "find",
    "hadoop",
    "within",
    "hadoop",
    "various",
    "sub",
    "directories",
    "look",
    "map",
    "reduce",
    "mapreduce",
    "directory",
    "sample",
    "jar",
    "files",
    "use",
    "run",
    "mapreduce",
    "cluster",
    "similarly",
    "working",
    "cloudera",
    "cluster",
    "would",
    "go",
    "opt",
    "cloudera",
    "parcel",
    "cdh",
    "slash",
    "lib",
    "would",
    "directories",
    "sdfs",
    "mapreduce",
    "sdfs",
    "yarn",
    "still",
    "find",
    "jars",
    "basically",
    "package",
    "contains",
    "multiple",
    "applications",
    "run",
    "mapreduce",
    "type",
    "hadoop",
    "hit",
    "enter",
    "shows",
    "option",
    "called",
    "jar",
    "used",
    "run",
    "jar",
    "file",
    "point",
    "time",
    "would",
    "want",
    "see",
    "different",
    "classes",
    "available",
    "particular",
    "jar",
    "could",
    "always",
    "jar",
    "minus",
    "xvf",
    "example",
    "could",
    "say",
    "jar",
    "xv",
    "f",
    "could",
    "say",
    "user",
    "local",
    "hadoop",
    "share",
    "hadoop",
    "mapreduce",
    "list",
    "jar",
    "file",
    "say",
    "hadoop",
    "mapreduce",
    "examples",
    "basically",
    "unpack",
    "show",
    "classes",
    "available",
    "within",
    "particular",
    "jar",
    "done",
    "created",
    "meta",
    "file",
    "created",
    "org",
    "directory",
    "see",
    "ls",
    "look",
    "ls",
    "org",
    "since",
    "ran",
    "command",
    "phone",
    "directory",
    "look",
    "org",
    "patchy",
    "hadoop",
    "examples",
    "shows",
    "classes",
    "classes",
    "contain",
    "mapper",
    "reducer",
    "classes",
    "might",
    "mapper",
    "reducer",
    "always",
    "look",
    "example",
    "targeting",
    "use",
    "word",
    "count",
    "program",
    "word",
    "count",
    "files",
    "gives",
    "list",
    "words",
    "many",
    "times",
    "occur",
    "particular",
    "file",
    "set",
    "files",
    "shows",
    "classes",
    "belong",
    "word",
    "count",
    "int",
    "sum",
    "reducer",
    "reducer",
    "class",
    "tokenizer",
    "mapper",
    "mapper",
    "class",
    "right",
    "basically",
    "used",
    "classes",
    "used",
    "run",
    "word",
    "code",
    "many",
    "programs",
    "part",
    "jar",
    "file",
    "expand",
    "see",
    "say",
    "hadoop",
    "jar",
    "give",
    "path",
    "say",
    "hadoop",
    "jar",
    "user",
    "local",
    "hadoop",
    "share",
    "hadoop",
    "mapreduce",
    "hadoop",
    "mapreduce",
    "examples",
    "hit",
    "enter",
    "show",
    "inbuilt",
    "classes",
    "already",
    "available",
    "certain",
    "things",
    "use",
    "jar",
    "files",
    "also",
    "example",
    "look",
    "hadoop",
    "look",
    "jar",
    "files",
    "particular",
    "path",
    "one",
    "hadoop",
    "mapreduce",
    "examples",
    "use",
    "always",
    "look",
    "jar",
    "files",
    "like",
    "look",
    "hadoop",
    "mapreduce",
    "client",
    "job",
    "client",
    "look",
    "test",
    "one",
    "also",
    "interesting",
    "one",
    "always",
    "look",
    "hadoop",
    "mapreduce",
    "client",
    "job",
    "client",
    "something",
    "ending",
    "would",
    "tried",
    "one",
    "using",
    "hadoop",
    "jar",
    "command",
    "previous",
    "example",
    "showing",
    "classes",
    "available",
    "already",
    "word",
    "count",
    "good",
    "programs",
    "try",
    "like",
    "teragen",
    "generate",
    "dummy",
    "data",
    "terasort",
    "check",
    "sorting",
    "performance",
    "terra",
    "validate",
    "validate",
    "results",
    "similarly",
    "also",
    "hadoop",
    "jar",
    "said",
    "hadoop",
    "mapreduce",
    "think",
    "client",
    "job",
    "client",
    "test",
    "star",
    "lot",
    "classes",
    "used",
    "programs",
    "used",
    "stress",
    "testing",
    "checking",
    "cluster",
    "status",
    "one",
    "interesting",
    "one",
    "test",
    "dfs",
    "io",
    "let",
    "get",
    "details",
    "first",
    "instance",
    "let",
    "see",
    "run",
    "mapreduce",
    "would",
    "want",
    "run",
    "mapreduce",
    "need",
    "give",
    "hadoop",
    "jar",
    "jar",
    "file",
    "hit",
    "enter",
    "would",
    "say",
    "needs",
    "input",
    "output",
    "needs",
    "class",
    "want",
    "run",
    "example",
    "would",
    "say",
    "word",
    "count",
    "hit",
    "enter",
    "tells",
    "need",
    "give",
    "input",
    "output",
    "process",
    "obviously",
    "processing",
    "happening",
    "cluster",
    "yarn",
    "processing",
    "framework",
    "unless",
    "would",
    "want",
    "run",
    "job",
    "local",
    "mode",
    "possibility",
    "run",
    "job",
    "local",
    "mode",
    "let",
    "first",
    "try",
    "runs",
    "cluster",
    "hdfs",
    "ls",
    "slash",
    "command",
    "see",
    "sdfs",
    "ui",
    "already",
    "showing",
    "set",
    "files",
    "directories",
    "use",
    "process",
    "take",
    "one",
    "single",
    "file",
    "example",
    "pick",
    "new",
    "data",
    "look",
    "files",
    "basically",
    "run",
    "mapreduce",
    "single",
    "file",
    "multiple",
    "files",
    "let",
    "take",
    "file",
    "whatever",
    "contains",
    "would",
    "like",
    "word",
    "count",
    "get",
    "list",
    "words",
    "occurrence",
    "file",
    "let",
    "copy",
    "also",
    "need",
    "output",
    "written",
    "written",
    "want",
    "run",
    "mapreduce",
    "say",
    "hadoop",
    "pull",
    "history",
    "hadoop",
    "jar",
    "word",
    "count",
    "need",
    "give",
    "input",
    "new",
    "data",
    "give",
    "file",
    "copied",
    "going",
    "run",
    "word",
    "count",
    "single",
    "file",
    "basically",
    "output",
    "stored",
    "directory",
    "directory",
    "created",
    "already",
    "mr",
    "output",
    "let",
    "output",
    "fair",
    "enough",
    "give",
    "many",
    "properties",
    "specify",
    "many",
    "map",
    "jobs",
    "want",
    "run",
    "many",
    "reduced",
    "jobs",
    "want",
    "run",
    "want",
    "output",
    "compressed",
    "want",
    "output",
    "merged",
    "many",
    "properties",
    "defined",
    "specifying",
    "word",
    "count",
    "pass",
    "argument",
    "pass",
    "properties",
    "command",
    "line",
    "affect",
    "output",
    "go",
    "ahead",
    "submit",
    "basically",
    "running",
    "simple",
    "inbuilt",
    "mapreduce",
    "job",
    "hadoop",
    "cluster",
    "obviously",
    "internally",
    "looking",
    "name",
    "node",
    "issue",
    "says",
    "output",
    "already",
    "exists",
    "mean",
    "basically",
    "means",
    "hadoop",
    "create",
    "output",
    "need",
    "give",
    "name",
    "need",
    "create",
    "let",
    "give",
    "let",
    "append",
    "output",
    "number",
    "one",
    "let",
    "go",
    "ahead",
    "run",
    "submitted",
    "command",
    "also",
    "done",
    "background",
    "would",
    "want",
    "run",
    "multiple",
    "jobs",
    "cluster",
    "time",
    "takes",
    "total",
    "input",
    "paths",
    "process",
    "one",
    "one",
    "split",
    "job",
    "work",
    "internally",
    "try",
    "contact",
    "resource",
    "manager",
    "basically",
    "done",
    "look",
    "see",
    "counters",
    "also",
    "see",
    "property",
    "missing",
    "run",
    "job",
    "run",
    "local",
    "mode",
    "run",
    "local",
    "mode",
    "although",
    "submitted",
    "might",
    "related",
    "yarn",
    "settings",
    "check",
    "refresh",
    "run",
    "application",
    "completed",
    "would",
    "created",
    "output",
    "thing",
    "interact",
    "yarn",
    "interact",
    "resource",
    "manager",
    "check",
    "properties",
    "look",
    "job",
    "basically",
    "tells",
    "went",
    "mapping",
    "reducing",
    "would",
    "created",
    "output",
    "worked",
    "file",
    "ran",
    "local",
    "mode",
    "ran",
    "local",
    "mode",
    "mapreduce",
    "remember",
    "programming",
    "model",
    "right",
    "run",
    "yarn",
    "get",
    "facilities",
    "running",
    "cluster",
    "yarn",
    "takes",
    "care",
    "resource",
    "management",
    "run",
    "yarn",
    "run",
    "local",
    "mode",
    "use",
    "machines",
    "ram",
    "cpu",
    "cores",
    "processing",
    "quickly",
    "look",
    "output",
    "also",
    "try",
    "running",
    "yarn",
    "look",
    "hdfs",
    "look",
    "output",
    "mr",
    "output",
    "directory",
    "used",
    "actually",
    "let",
    "look",
    "directory",
    "ending",
    "one",
    "show",
    "output",
    "created",
    "mapreduce",
    "although",
    "ran",
    "local",
    "mode",
    "fetched",
    "input",
    "file",
    "usdfs",
    "would",
    "created",
    "output",
    "hdfs",
    "part",
    "file",
    "created",
    "look",
    "part",
    "minus",
    "r",
    "minus",
    "zeros",
    "would",
    "one",
    "reducer",
    "running",
    "would",
    "multiple",
    "files",
    "created",
    "look",
    "file",
    "contain",
    "word",
    "count",
    "say",
    "cat",
    "basically",
    "shows",
    "output",
    "created",
    "mapreduce",
    "let",
    "look",
    "file",
    "gave",
    "processing",
    "broken",
    "list",
    "words",
    "occur",
    "file",
    "plus",
    "count",
    "words",
    "word",
    "shows",
    "count",
    "list",
    "words",
    "count",
    "run",
    "sample",
    "mapreduce",
    "job",
    "also",
    "show",
    "run",
    "yeah",
    "let",
    "run",
    "mapreduce",
    "yarn",
    "initially",
    "tried",
    "running",
    "mapreduce",
    "hit",
    "yarn",
    "ran",
    "local",
    "mode",
    "property",
    "changed",
    "mapreduce",
    "hyphen",
    "site",
    "file",
    "basically",
    "look",
    "file",
    "error",
    "given",
    "property",
    "says",
    "mapred",
    "dot",
    "framework",
    "dot",
    "name",
    "right",
    "property",
    "name",
    "ignored",
    "ran",
    "local",
    "mode",
    "changed",
    "property",
    "restarted",
    "cluster",
    "everything",
    "fine",
    "map",
    "red",
    "hyphen",
    "site",
    "file",
    "also",
    "copied",
    "across",
    "nodes",
    "run",
    "mapreduce",
    "hadoop",
    "cluster",
    "uses",
    "yarn",
    "yarn",
    "takes",
    "care",
    "resource",
    "allocation",
    "one",
    "multiple",
    "machines",
    "changing",
    "output",
    "submit",
    "job",
    "first",
    "connect",
    "resource",
    "manager",
    "connects",
    "resource",
    "manager",
    "means",
    "job",
    "run",
    "using",
    "yarn",
    "cluster",
    "rather",
    "local",
    "mode",
    "wait",
    "application",
    "internally",
    "connect",
    "resource",
    "manager",
    "starts",
    "always",
    "go",
    "back",
    "web",
    "ui",
    "check",
    "application",
    "reached",
    "yarn",
    "shows",
    "one",
    "input",
    "part",
    "processed",
    "job",
    "id",
    "application",
    "id",
    "even",
    "monitor",
    "status",
    "command",
    "line",
    "job",
    "submitted",
    "let",
    "go",
    "back",
    "refresh",
    "yarn",
    "ui",
    "show",
    "new",
    "application",
    "submitted",
    "tells",
    "accepted",
    "state",
    "application",
    "master",
    "already",
    "started",
    "click",
    "link",
    "also",
    "give",
    "details",
    "many",
    "map",
    "reduce",
    "tasks",
    "would",
    "run",
    "says",
    "application",
    "master",
    "running",
    "would",
    "using",
    "node",
    "m1",
    "always",
    "look",
    "logs",
    "see",
    "one",
    "task",
    "attempt",
    "made",
    "go",
    "back",
    "terminal",
    "see",
    "waiting",
    "get",
    "resources",
    "cluster",
    "gets",
    "resources",
    "first",
    "start",
    "mapping",
    "phase",
    "mapper",
    "function",
    "runs",
    "map",
    "tasks",
    "one",
    "multiple",
    "depending",
    "splits",
    "right",
    "one",
    "file",
    "one",
    "split",
    "one",
    "map",
    "task",
    "running",
    "mapping",
    "phase",
    "completes",
    "get",
    "reducing",
    "finally",
    "give",
    "output",
    "toggling",
    "sessions",
    "refresh",
    "see",
    "happening",
    "application",
    "proceeding",
    "still",
    "waiting",
    "resource",
    "manager",
    "allocate",
    "resources",
    "couple",
    "minutes",
    "back",
    "tested",
    "application",
    "yarn",
    "see",
    "first",
    "application",
    "completed",
    "successfully",
    "give",
    "time",
    "yarn",
    "allocate",
    "resources",
    "resources",
    "used",
    "application",
    "freed",
    "internally",
    "yan",
    "takes",
    "care",
    "learn",
    "detail",
    "yarn",
    "might",
    "already",
    "followed",
    "yarn",
    "based",
    "session",
    "give",
    "time",
    "let",
    "see",
    "application",
    "proceeds",
    "resources",
    "yarn",
    "allocate",
    "sometimes",
    "also",
    "see",
    "slowness",
    "web",
    "ui",
    "shows",
    "related",
    "amount",
    "memory",
    "allocated",
    "nodes",
    "apache",
    "less",
    "amount",
    "memory",
    "still",
    "run",
    "cluster",
    "said",
    "memory",
    "shows",
    "16",
    "gb",
    "16",
    "cores",
    "true",
    "one",
    "default",
    "settings",
    "right",
    "yarn",
    "able",
    "facilitate",
    "running",
    "application",
    "let",
    "give",
    "couple",
    "seconds",
    "let",
    "look",
    "output",
    "make",
    "changes",
    "settings",
    "application",
    "getting",
    "enough",
    "resources",
    "basically",
    "restarted",
    "cluster",
    "let",
    "submit",
    "application",
    "cluster",
    "first",
    "contact",
    "resource",
    "manager",
    "basically",
    "map",
    "reduce",
    "process",
    "start",
    "submitted",
    "application",
    "connecting",
    "resource",
    "manager",
    "basically",
    "start",
    "internally",
    "app",
    "master",
    "application",
    "master",
    "looking",
    "number",
    "splits",
    "one",
    "getting",
    "application",
    "id",
    "basically",
    "starts",
    "running",
    "job",
    "also",
    "gives",
    "tracking",
    "url",
    "look",
    "output",
    "go",
    "back",
    "look",
    "yarn",
    "ui",
    "application",
    "shows",
    "give",
    "couple",
    "seconds",
    "get",
    "final",
    "status",
    "change",
    "running",
    "application",
    "getting",
    "resources",
    "closely",
    "notice",
    "allocated",
    "specific",
    "amount",
    "memory",
    "gb",
    "node",
    "manager",
    "every",
    "node",
    "basically",
    "given",
    "two",
    "cores",
    "machines",
    "also",
    "yarn",
    "utilizing",
    "resources",
    "rather",
    "going",
    "default",
    "application",
    "started",
    "moving",
    "see",
    "progress",
    "bar",
    "basically",
    "show",
    "happening",
    "go",
    "back",
    "terminal",
    "show",
    "first",
    "went",
    "deciding",
    "map",
    "reduce",
    "goes",
    "map",
    "mapping",
    "phase",
    "completes",
    "reducing",
    "phase",
    "come",
    "existence",
    "job",
    "completed",
    "basically",
    "used",
    "always",
    "look",
    "many",
    "map",
    "reduce",
    "sas",
    "run",
    "shows",
    "one",
    "map",
    "one",
    "reduced",
    "task",
    "number",
    "map",
    "tasks",
    "depends",
    "number",
    "splits",
    "one",
    "file",
    "less",
    "128",
    "mb",
    "one",
    "split",
    "processed",
    "reduced",
    "task",
    "internally",
    "decided",
    "reducer",
    "depending",
    "kind",
    "property",
    "set",
    "hadoop",
    "config",
    "files",
    "also",
    "tells",
    "many",
    "input",
    "records",
    "read",
    "basically",
    "means",
    "number",
    "lines",
    "file",
    "tells",
    "output",
    "records",
    "gives",
    "number",
    "total",
    "words",
    "file",
    "might",
    "duplicates",
    "processed",
    "internal",
    "combiner",
    "processing",
    "forwarding",
    "information",
    "reducer",
    "basically",
    "reducer",
    "works",
    "335",
    "records",
    "gives",
    "us",
    "list",
    "words",
    "account",
    "refresh",
    "would",
    "obviously",
    "show",
    "application",
    "completed",
    "says",
    "succeeded",
    "always",
    "click",
    "application",
    "look",
    "information",
    "tells",
    "ran",
    "history",
    "server",
    "running",
    "otherwise",
    "always",
    "access",
    "information",
    "leads",
    "history",
    "server",
    "applications",
    "stored",
    "click",
    "attempt",
    "tasks",
    "basically",
    "show",
    "history",
    "url",
    "always",
    "look",
    "logs",
    "submit",
    "sample",
    "application",
    "inbuilt",
    "available",
    "jar",
    "hadoop",
    "cluster",
    "utilize",
    "cluster",
    "run",
    "could",
    "always",
    "said",
    "running",
    "particular",
    "job",
    "remember",
    "change",
    "output",
    "directory",
    "would",
    "want",
    "processing",
    "single",
    "individual",
    "file",
    "could",
    "also",
    "point",
    "directory",
    "basically",
    "means",
    "multiple",
    "files",
    "depending",
    "file",
    "sizes",
    "would",
    "multiple",
    "splits",
    "according",
    "multiple",
    "map",
    "tasks",
    "selected",
    "click",
    "would",
    "submit",
    "second",
    "application",
    "cluster",
    "first",
    "connect",
    "resource",
    "manager",
    "resource",
    "manager",
    "start",
    "application",
    "master",
    "targeting",
    "10",
    "splits",
    "sometimes",
    "give",
    "couple",
    "seconds",
    "machines",
    "resources",
    "used",
    "internally",
    "already",
    "freed",
    "cluster",
    "pick",
    "yarn",
    "take",
    "care",
    "resources",
    "right",
    "application",
    "undefined",
    "status",
    "soon",
    "yarn",
    "provides",
    "resources",
    "application",
    "running",
    "yarn",
    "cluster",
    "already",
    "started",
    "see",
    "going",
    "would",
    "launch",
    "10",
    "map",
    "tasks",
    "would",
    "number",
    "reduced",
    "stars",
    "would",
    "decided",
    "either",
    "way",
    "data",
    "based",
    "properties",
    "set",
    "cluster",
    "level",
    "let",
    "quick",
    "refresh",
    "yarn",
    "ui",
    "show",
    "progress",
    "also",
    "take",
    "care",
    "submitting",
    "application",
    "need",
    "output",
    "directory",
    "mentioned",
    "however",
    "create",
    "hadoop",
    "create",
    "run",
    "mapreduce",
    "without",
    "specifying",
    "properties",
    "specify",
    "properties",
    "look",
    "things",
    "changed",
    "mapper",
    "reducer",
    "basically",
    "combiner",
    "class",
    "mini",
    "reducing",
    "things",
    "done",
    "learn",
    "later",
    "sessions",
    "compare",
    "hadoop",
    "version",
    "one",
    "mapreduce",
    "version",
    "one",
    "understand",
    "learn",
    "limitations",
    "hadoop",
    "version",
    "one",
    "need",
    "yarn",
    "yarn",
    "kind",
    "workloads",
    "running",
    "yarn",
    "yarn",
    "components",
    "yarn",
    "architecture",
    "finally",
    "see",
    "demo",
    "yarn",
    "hadoop",
    "version",
    "one",
    "mapreduce",
    "version",
    "one",
    "well",
    "outdated",
    "nobody",
    "using",
    "hadoop",
    "version",
    "one",
    "would",
    "good",
    "understand",
    "hadoop",
    "version",
    "one",
    "limitations",
    "hadoop",
    "version",
    "one",
    "brought",
    "thought",
    "future",
    "processing",
    "layer",
    "yarn",
    "talk",
    "hadoop",
    "already",
    "know",
    "hadoop",
    "framework",
    "hadoop",
    "two",
    "layers",
    "one",
    "storage",
    "layer",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "allows",
    "distributed",
    "storage",
    "processing",
    "allows",
    "fault",
    "tolerance",
    "inbuilt",
    "replication",
    "basically",
    "allows",
    "store",
    "huge",
    "amount",
    "data",
    "across",
    "multiple",
    "commodity",
    "machines",
    "talk",
    "processing",
    "know",
    "mapreduce",
    "oldest",
    "mature",
    "processing",
    "programming",
    "model",
    "basically",
    "takes",
    "care",
    "data",
    "processing",
    "distributed",
    "file",
    "system",
    "hadoop",
    "version",
    "1",
    "mapreduce",
    "performed",
    "data",
    "processing",
    "resource",
    "management",
    "problematic",
    "mapreduce",
    "basically",
    "talked",
    "processing",
    "layer",
    "master",
    "called",
    "job",
    "tracker",
    "slaves",
    "task",
    "records",
    "job",
    "tracker",
    "taking",
    "care",
    "allocating",
    "resources",
    "performing",
    "scheduling",
    "even",
    "monitoring",
    "jobs",
    "basically",
    "taking",
    "care",
    "assigning",
    "map",
    "reduced",
    "tasks",
    "jobs",
    "running",
    "task",
    "trackers",
    "task",
    "trackers",
    "data",
    "nodes",
    "responsible",
    "processing",
    "jobs",
    "task",
    "trackers",
    "slaves",
    "processing",
    "layer",
    "reported",
    "progress",
    "job",
    "tracker",
    "happening",
    "hadoop",
    "version",
    "talk",
    "hadoop",
    "version",
    "1",
    "would",
    "say",
    "client",
    "machines",
    "api",
    "application",
    "basically",
    "submits",
    "job",
    "master",
    "job",
    "tracker",
    "obviously",
    "forget",
    "would",
    "already",
    "involvement",
    "name",
    "node",
    "basically",
    "tells",
    "machines",
    "data",
    "nodes",
    "data",
    "already",
    "stored",
    "job",
    "submission",
    "happens",
    "job",
    "tracker",
    "job",
    "tracker",
    "master",
    "demon",
    "taking",
    "care",
    "processing",
    "request",
    "also",
    "resource",
    "management",
    "job",
    "scheduling",
    "would",
    "interacting",
    "multiple",
    "task",
    "trackers",
    "would",
    "running",
    "multiple",
    "machines",
    "machine",
    "would",
    "task",
    "tracker",
    "running",
    "task",
    "tracker",
    "processing",
    "slave",
    "would",
    "data",
    "nodes",
    "know",
    "case",
    "hadoop",
    "concept",
    "moving",
    "processing",
    "wherever",
    "data",
    "stored",
    "rather",
    "moving",
    "data",
    "processing",
    "layer",
    "would",
    "task",
    "trackers",
    "would",
    "running",
    "multiple",
    "machines",
    "task",
    "trackers",
    "would",
    "responsible",
    "handling",
    "tasks",
    "tasks",
    "application",
    "broken",
    "smaller",
    "tasks",
    "would",
    "work",
    "data",
    "respectively",
    "stored",
    "particular",
    "node",
    "slave",
    "demons",
    "right",
    "job",
    "tracker",
    "tracking",
    "resources",
    "task",
    "trackers",
    "sending",
    "heartbeats",
    "sending",
    "packets",
    "information",
    "job",
    "tracker",
    "would",
    "knowing",
    "many",
    "resources",
    "talk",
    "resources",
    "talking",
    "cpu",
    "cores",
    "talking",
    "ram",
    "would",
    "available",
    "every",
    "node",
    "task",
    "trackers",
    "would",
    "sending",
    "resource",
    "information",
    "job",
    "tracker",
    "job",
    "tracker",
    "would",
    "already",
    "aware",
    "amount",
    "resources",
    "available",
    "particular",
    "node",
    "loaded",
    "particular",
    "node",
    "kind",
    "work",
    "could",
    "given",
    "task",
    "tracker",
    "job",
    "tracker",
    "taking",
    "care",
    "resource",
    "management",
    "also",
    "breaking",
    "application",
    "tasks",
    "job",
    "scheduling",
    "part",
    "assign",
    "different",
    "tasks",
    "slave",
    "demons",
    "task",
    "trackers",
    "job",
    "tracker",
    "eventually",
    "overburdened",
    "right",
    "managing",
    "jobs",
    "tracking",
    "resources",
    "multiple",
    "task",
    "trackers",
    "basically",
    "taking",
    "care",
    "job",
    "scheduling",
    "job",
    "tracker",
    "would",
    "overburdened",
    "case",
    "job",
    "tracker",
    "would",
    "fail",
    "would",
    "affect",
    "overall",
    "processing",
    "master",
    "killed",
    "master",
    "demand",
    "dies",
    "processing",
    "proceed",
    "one",
    "limitations",
    "hadoop",
    "version",
    "one",
    "talk",
    "scalability",
    "capability",
    "scale",
    "due",
    "single",
    "job",
    "tracker",
    "scalability",
    "would",
    "hitting",
    "bottleneck",
    "cluster",
    "size",
    "4000",
    "nodes",
    "run",
    "40",
    "000",
    "concurrent",
    "tasks",
    "number",
    "could",
    "always",
    "look",
    "individual",
    "resources",
    "machine",
    "come",
    "appropriate",
    "number",
    "however",
    "single",
    "job",
    "tracker",
    "horizontal",
    "scalability",
    "processing",
    "layer",
    "single",
    "processing",
    "master",
    "talk",
    "availability",
    "job",
    "tracker",
    "mentioned",
    "would",
    "single",
    "point",
    "failure",
    "failure",
    "kills",
    "queued",
    "running",
    "jobs",
    "jobs",
    "would",
    "resubmitted",
    "would",
    "want",
    "distributed",
    "platform",
    "cluster",
    "hundreds",
    "thousands",
    "machines",
    "would",
    "want",
    "processing",
    "layer",
    "handle",
    "huge",
    "amount",
    "processing",
    "could",
    "scalable",
    "could",
    "available",
    "could",
    "handle",
    "different",
    "kind",
    "workloads",
    "comes",
    "resource",
    "utilization",
    "would",
    "predefined",
    "number",
    "map",
    "reduce",
    "slots",
    "task",
    "tracker",
    "would",
    "issues",
    "would",
    "relate",
    "resource",
    "utilization",
    "putting",
    "burden",
    "master",
    "tracking",
    "resources",
    "assign",
    "jobs",
    "run",
    "multiple",
    "machines",
    "parallel",
    "limitations",
    "running",
    "reduce",
    "applications",
    "one",
    "limitation",
    "hadoop",
    "version",
    "one",
    "mapreduce",
    "kind",
    "processing",
    "could",
    "mapreduce",
    "mapreduce",
    "programming",
    "model",
    "although",
    "good",
    "oldest",
    "matured",
    "period",
    "time",
    "rigid",
    "go",
    "mapping",
    "reducing",
    "approach",
    "kind",
    "processing",
    "could",
    "done",
    "hadoop",
    "person",
    "one",
    "comes",
    "real",
    "time",
    "analysis",
    "ad",
    "hoc",
    "query",
    "graph",
    "based",
    "processing",
    "massive",
    "parallel",
    "processing",
    "limitations",
    "could",
    "done",
    "hadoop",
    "version",
    "1",
    "mapreduce",
    "version",
    "1",
    "processing",
    "component",
    "brings",
    "us",
    "need",
    "yarn",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "mentioned",
    "yarn",
    "hadoop",
    "version",
    "one",
    "well",
    "could",
    "applications",
    "could",
    "written",
    "different",
    "programming",
    "languages",
    "kind",
    "processing",
    "possible",
    "mapreduce",
    "storage",
    "layer",
    "processing",
    "kind",
    "limited",
    "processing",
    "could",
    "done",
    "one",
    "thing",
    "brought",
    "thought",
    "processing",
    "layer",
    "handle",
    "different",
    "kind",
    "workloads",
    "mentioned",
    "might",
    "graph",
    "processing",
    "might",
    "processing",
    "might",
    "massive",
    "parallel",
    "processing",
    "kind",
    "processing",
    "would",
    "requirement",
    "organization",
    "designed",
    "run",
    "jobs",
    "issues",
    "scalability",
    "resource",
    "utilization",
    "job",
    "tracking",
    "etc",
    "led",
    "need",
    "something",
    "call",
    "yarn",
    "hadoop",
    "version",
    "2",
    "onwards",
    "two",
    "main",
    "layers",
    "changed",
    "little",
    "bit",
    "storage",
    "layer",
    "intact",
    "hdfs",
    "processing",
    "layer",
    "called",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "understand",
    "yarn",
    "works",
    "yarn",
    "taking",
    "care",
    "processing",
    "layer",
    "support",
    "mapreduce",
    "mapreduce",
    "processing",
    "still",
    "done",
    "support",
    "processing",
    "frameworks",
    "yarn",
    "used",
    "solve",
    "issues",
    "hadoop",
    "version",
    "1",
    "posing",
    "something",
    "like",
    "resource",
    "management",
    "something",
    "like",
    "different",
    "kind",
    "workload",
    "processing",
    "something",
    "like",
    "scalability",
    "resource",
    "utilization",
    "taken",
    "care",
    "yarn",
    "talk",
    "yarn",
    "cluster",
    "size",
    "10",
    "000",
    "nodes",
    "run",
    "100",
    "000",
    "concurrent",
    "tasks",
    "take",
    "care",
    "scalability",
    "talk",
    "compatibility",
    "applications",
    "developed",
    "hadoop",
    "version",
    "1",
    "primarily",
    "map",
    "reduce",
    "kind",
    "processing",
    "run",
    "yarn",
    "without",
    "disruption",
    "availability",
    "issues",
    "talk",
    "resource",
    "utilization",
    "mechanism",
    "takes",
    "care",
    "dynamic",
    "allocation",
    "cluster",
    "resources",
    "basically",
    "improves",
    "resource",
    "utilization",
    "talk",
    "basically",
    "cluster",
    "handle",
    "different",
    "kind",
    "workloads",
    "use",
    "open",
    "source",
    "proprietary",
    "data",
    "access",
    "engines",
    "perform",
    "analysis",
    "graph",
    "processing",
    "ad",
    "hoc",
    "querying",
    "supported",
    "multiple",
    "workloads",
    "run",
    "parallel",
    "yarn",
    "offers",
    "yarn",
    "mentioned",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "cluster",
    "resource",
    "management",
    "layer",
    "apache",
    "hadoop",
    "ecosystem",
    "takes",
    "care",
    "scheduling",
    "jobs",
    "assigning",
    "resources",
    "imagine",
    "would",
    "want",
    "run",
    "particular",
    "application",
    "would",
    "basically",
    "telling",
    "cust",
    "cluster",
    "would",
    "want",
    "resources",
    "run",
    "applications",
    "application",
    "might",
    "mapreduce",
    "application",
    "might",
    "hive",
    "query",
    "triggering",
    "mapreduce",
    "might",
    "big",
    "script",
    "triggering",
    "mapreduce",
    "could",
    "hive",
    "days",
    "execution",
    "engine",
    "could",
    "spark",
    "application",
    "could",
    "graph",
    "processing",
    "application",
    "cases",
    "would",
    "still",
    "sense",
    "client",
    "basically",
    "api",
    "application",
    "would",
    "requesting",
    "resources",
    "yan",
    "would",
    "take",
    "care",
    "yarn",
    "would",
    "provide",
    "desired",
    "resources",
    "talk",
    "resources",
    "mainly",
    "talking",
    "network",
    "related",
    "resources",
    "talking",
    "cpu",
    "cores",
    "terms",
    "yarn",
    "say",
    "virtual",
    "cpu",
    "course",
    "would",
    "talk",
    "ram",
    "gb",
    "mb",
    "terabytes",
    "would",
    "offered",
    "multiple",
    "machines",
    "yarn",
    "would",
    "take",
    "care",
    "yarn",
    "could",
    "basically",
    "handle",
    "different",
    "workloads",
    "workloads",
    "showing",
    "traditional",
    "mapreduce",
    "mainly",
    "batch",
    "oriented",
    "could",
    "interactive",
    "execution",
    "engine",
    "something",
    "space",
    "could",
    "hbase",
    "column",
    "oriented",
    "four",
    "dimensional",
    "database",
    "would",
    "storing",
    "data",
    "sdfs",
    "would",
    "also",
    "need",
    "kind",
    "processing",
    "could",
    "streaming",
    "functionalities",
    "would",
    "storm",
    "kafka",
    "spark",
    "could",
    "graph",
    "processing",
    "could",
    "processing",
    "spark",
    "components",
    "could",
    "many",
    "others",
    "different",
    "frameworks",
    "could",
    "run",
    "run",
    "top",
    "yarn",
    "undo",
    "talk",
    "yarn",
    "overall",
    "yarn",
    "architecture",
    "looks",
    "one",
    "end",
    "client",
    "client",
    "could",
    "basically",
    "edge",
    "node",
    "applications",
    "running",
    "could",
    "api",
    "would",
    "want",
    "interact",
    "cluster",
    "could",
    "user",
    "triggered",
    "application",
    "wants",
    "run",
    "jobs",
    "processing",
    "client",
    "would",
    "submit",
    "job",
    "request",
    "resource",
    "manager",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "hadoop",
    "version",
    "1",
    "basically",
    "job",
    "tracker",
    "task",
    "trackers",
    "running",
    "individual",
    "nodes",
    "task",
    "trackers",
    "sending",
    "heart",
    "beats",
    "job",
    "tracker",
    "task",
    "trackers",
    "sending",
    "resource",
    "information",
    "job",
    "tracker",
    "one",
    "tracking",
    "resources",
    "job",
    "scheduling",
    "mentioned",
    "earlier",
    "job",
    "tracker",
    "overburdened",
    "job",
    "tracker",
    "replaced",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "task",
    "trackers",
    "replaced",
    "node",
    "managers",
    "would",
    "running",
    "every",
    "node",
    "temporary",
    "demon",
    "see",
    "blue",
    "app",
    "master",
    "mentioned",
    "say",
    "yet",
    "another",
    "resource",
    "negotiator",
    "appmaster",
    "would",
    "existing",
    "hadoop",
    "version",
    "talk",
    "resource",
    "manager",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "would",
    "already",
    "receiving",
    "heartbeats",
    "say",
    "resource",
    "information",
    "multiple",
    "node",
    "managers",
    "would",
    "running",
    "one",
    "multiple",
    "machines",
    "node",
    "managers",
    "updating",
    "status",
    "also",
    "giving",
    "information",
    "amount",
    "resources",
    "talk",
    "resources",
    "understand",
    "talking",
    "node",
    "manager",
    "allocated",
    "amount",
    "ram",
    "processing",
    "amount",
    "cpu",
    "cores",
    "portion",
    "complete",
    "node",
    "node",
    "say",
    "imagine",
    "node",
    "around",
    "100",
    "gb",
    "ram",
    "saved",
    "60",
    "cores",
    "allocated",
    "node",
    "manager",
    "node",
    "manager",
    "one",
    "components",
    "hadoop",
    "ecosystem",
    "slave",
    "processing",
    "layer",
    "could",
    "say",
    "keeping",
    "aspects",
    "different",
    "services",
    "running",
    "might",
    "cloudera",
    "hortonworks",
    "related",
    "services",
    "running",
    "system",
    "processes",
    "running",
    "particular",
    "node",
    "portion",
    "would",
    "assigned",
    "node",
    "manager",
    "processing",
    "could",
    "say",
    "example",
    "say",
    "60",
    "gb",
    "ram",
    "per",
    "node",
    "say",
    "40",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "every",
    "machine",
    "similarly",
    "would",
    "similarly",
    "would",
    "node",
    "manager",
    "constantly",
    "giving",
    "update",
    "resource",
    "manager",
    "resources",
    "probably",
    "might",
    "applications",
    "running",
    "node",
    "manager",
    "already",
    "occupied",
    "gives",
    "update",
    "also",
    "concept",
    "containers",
    "basically",
    "talk",
    "resources",
    "broken",
    "smaller",
    "parts",
    "resource",
    "manager",
    "keeping",
    "track",
    "resources",
    "every",
    "node",
    "manager",
    "also",
    "responsible",
    "taking",
    "care",
    "job",
    "request",
    "things",
    "happen",
    "see",
    "resource",
    "manager",
    "higher",
    "level",
    "always",
    "say",
    "processing",
    "master",
    "everything",
    "reality",
    "resource",
    "manager",
    "internally",
    "different",
    "services",
    "components",
    "helping",
    "supposed",
    "let",
    "look",
    "mentioned",
    "resource",
    "manager",
    "services",
    "components",
    "basically",
    "helps",
    "things",
    "basically",
    "architecture",
    "multiple",
    "components",
    "working",
    "together",
    "achieve",
    "yarn",
    "allows",
    "resource",
    "manager",
    "mainly",
    "two",
    "components",
    "scheduler",
    "applications",
    "manager",
    "high",
    "level",
    "four",
    "main",
    "components",
    "talk",
    "resource",
    "manager",
    "processing",
    "master",
    "node",
    "managers",
    "processing",
    "slaves",
    "running",
    "every",
    "nodes",
    "concept",
    "container",
    "concept",
    "application",
    "master",
    "things",
    "work",
    "let",
    "look",
    "yarn",
    "components",
    "resource",
    "manager",
    "basically",
    "two",
    "main",
    "components",
    "say",
    "assist",
    "resource",
    "manager",
    "capable",
    "scheduler",
    "applications",
    "manager",
    "talk",
    "resources",
    "always",
    "requirement",
    "applications",
    "need",
    "run",
    "cluster",
    "resources",
    "application",
    "run",
    "submitted",
    "client",
    "needs",
    "resources",
    "resources",
    "coming",
    "multiple",
    "machines",
    "wherever",
    "relevant",
    "data",
    "stored",
    "node",
    "manager",
    "running",
    "always",
    "know",
    "node",
    "manager",
    "data",
    "nodes",
    "scheduler",
    "different",
    "kind",
    "schedulers",
    "basically",
    "capacity",
    "scheduler",
    "fair",
    "scheduler",
    "could",
    "fifo",
    "scheduler",
    "different",
    "schedulers",
    "take",
    "care",
    "resource",
    "allocation",
    "scheduler",
    "responsible",
    "allocating",
    "resources",
    "various",
    "running",
    "applications",
    "imagine",
    "particular",
    "environment",
    "different",
    "teams",
    "different",
    "departments",
    "working",
    "cluster",
    "would",
    "call",
    "cluster",
    "cluster",
    "cluster",
    "would",
    "different",
    "applications",
    "would",
    "want",
    "run",
    "simultaneously",
    "accessing",
    "resources",
    "cluster",
    "managed",
    "component",
    "concept",
    "pooling",
    "queuing",
    "different",
    "departments",
    "different",
    "users",
    "get",
    "dedicated",
    "resources",
    "share",
    "resources",
    "cluster",
    "scheduler",
    "responsible",
    "allocating",
    "resources",
    "various",
    "running",
    "applications",
    "perform",
    "monitoring",
    "tracking",
    "status",
    "applications",
    "part",
    "scheduler",
    "offer",
    "guarantee",
    "restarting",
    "failed",
    "tasks",
    "due",
    "hardware",
    "network",
    "failures",
    "scheduler",
    "mainly",
    "responsible",
    "allocating",
    "resources",
    "mentioned",
    "could",
    "different",
    "kind",
    "schedulers",
    "could",
    "fifo",
    "scheduler",
    "mainly",
    "older",
    "version",
    "hadoop",
    "stands",
    "first",
    "first",
    "could",
    "fair",
    "scheduler",
    "basically",
    "means",
    "multiple",
    "applications",
    "could",
    "running",
    "cluster",
    "would",
    "fair",
    "share",
    "resources",
    "could",
    "capacity",
    "scheduler",
    "would",
    "basically",
    "dedicated",
    "fixed",
    "amount",
    "resources",
    "across",
    "cluster",
    "whichever",
    "scheduler",
    "used",
    "scheduler",
    "mainly",
    "responsible",
    "allocating",
    "resources",
    "applications",
    "manager",
    "responsible",
    "accepting",
    "job",
    "submissions",
    "said",
    "higher",
    "level",
    "could",
    "always",
    "say",
    "resource",
    "manager",
    "state",
    "everything",
    "allocating",
    "resources",
    "negotiating",
    "resources",
    "also",
    "taking",
    "care",
    "listening",
    "clients",
    "taking",
    "care",
    "job",
    "submissions",
    "real",
    "components",
    "applications",
    "manager",
    "responsible",
    "accepting",
    "job",
    "submissions",
    "negotiates",
    "first",
    "container",
    "executing",
    "application",
    "specific",
    "application",
    "master",
    "provides",
    "service",
    "restarting",
    "application",
    "master",
    "work",
    "things",
    "happen",
    "coordination",
    "said",
    "node",
    "manager",
    "slave",
    "process",
    "would",
    "running",
    "every",
    "machine",
    "slave",
    "tracking",
    "resources",
    "tracking",
    "processes",
    "taking",
    "care",
    "running",
    "jobs",
    "basically",
    "tracking",
    "container",
    "resource",
    "utilization",
    "let",
    "understand",
    "container",
    "normally",
    "talk",
    "application",
    "request",
    "comes",
    "client",
    "let",
    "say",
    "client",
    "requesting",
    "coming",
    "application",
    "needs",
    "run",
    "cluster",
    "application",
    "could",
    "anything",
    "first",
    "contacts",
    "master",
    "resource",
    "manager",
    "master",
    "processing",
    "layer",
    "mentioned",
    "already",
    "know",
    "name",
    "node",
    "master",
    "cluster",
    "metadata",
    "ram",
    "aware",
    "data",
    "split",
    "blocks",
    "blocks",
    "stored",
    "multiple",
    "machines",
    "information",
    "obviously",
    "interaction",
    "master",
    "given",
    "information",
    "relevant",
    "nodes",
    "data",
    "exists",
    "processing",
    "need",
    "client",
    "basically",
    "application",
    "needs",
    "run",
    "cluster",
    "resource",
    "manager",
    "basically",
    "scheduler",
    "takes",
    "care",
    "allocating",
    "resources",
    "resource",
    "manager",
    "mainly",
    "two",
    "components",
    "helping",
    "work",
    "particular",
    "application",
    "might",
    "needing",
    "data",
    "multiple",
    "machines",
    "know",
    "would",
    "multiple",
    "machines",
    "would",
    "node",
    "manager",
    "running",
    "would",
    "data",
    "node",
    "running",
    "data",
    "nodes",
    "responsible",
    "storing",
    "data",
    "disk",
    "resource",
    "manager",
    "negotiate",
    "resources",
    "say",
    "negotiating",
    "resources",
    "could",
    "basically",
    "ask",
    "node",
    "managers",
    "amount",
    "resources",
    "example",
    "would",
    "saying",
    "1",
    "gb",
    "ram",
    "1",
    "cpu",
    "core",
    "data",
    "residing",
    "machine",
    "needs",
    "processed",
    "part",
    "application",
    "one",
    "gb",
    "one",
    "cpu",
    "core",
    "relevant",
    "data",
    "stored",
    "request",
    "resource",
    "manager",
    "makes",
    "holding",
    "resources",
    "total",
    "resources",
    "node",
    "manager",
    "resource",
    "manager",
    "negotiating",
    "asking",
    "resources",
    "processing",
    "slave",
    "request",
    "holding",
    "resources",
    "considered",
    "container",
    "resource",
    "manager",
    "know",
    "actually",
    "resource",
    "manager",
    "application",
    "manager",
    "negotiating",
    "resources",
    "negotiates",
    "resources",
    "called",
    "container",
    "request",
    "holding",
    "resource",
    "considered",
    "container",
    "basically",
    "container",
    "different",
    "sizes",
    "talk",
    "resource",
    "manager",
    "negotiates",
    "resources",
    "node",
    "manager",
    "node",
    "manager",
    "already",
    "giving",
    "update",
    "resources",
    "amount",
    "resources",
    "holds",
    "much",
    "busy",
    "basically",
    "approve",
    "deny",
    "request",
    "node",
    "manager",
    "would",
    "basically",
    "approve",
    "saying",
    "yes",
    "could",
    "hold",
    "resources",
    "could",
    "give",
    "container",
    "particular",
    "size",
    "container",
    "approved",
    "allocated",
    "say",
    "granted",
    "node",
    "manager",
    "resource",
    "manager",
    "knows",
    "resources",
    "process",
    "application",
    "available",
    "guaranteed",
    "node",
    "manager",
    "resource",
    "manager",
    "starts",
    "temporary",
    "demon",
    "called",
    "appmaster",
    "piece",
    "code",
    "would",
    "also",
    "running",
    "one",
    "containers",
    "would",
    "running",
    "one",
    "containers",
    "would",
    "take",
    "care",
    "execution",
    "tasks",
    "containers",
    "application",
    "master",
    "per",
    "application",
    "would",
    "10",
    "different",
    "applications",
    "coming",
    "client",
    "would",
    "10",
    "app",
    "masters",
    "one",
    "app",
    "master",
    "responsible",
    "per",
    "application",
    "app",
    "master",
    "basically",
    "piece",
    "code",
    "responsible",
    "execution",
    "application",
    "app",
    "master",
    "would",
    "run",
    "one",
    "containers",
    "would",
    "use",
    "containers",
    "node",
    "manager",
    "guaranteed",
    "give",
    "request",
    "application",
    "request",
    "comes",
    "using",
    "containers",
    "app",
    "master",
    "run",
    "processing",
    "tasks",
    "within",
    "designated",
    "resources",
    "mainly",
    "responsibility",
    "application",
    "master",
    "get",
    "execution",
    "done",
    "communicate",
    "master",
    "resource",
    "manager",
    "tracking",
    "resources",
    "negotiating",
    "resources",
    "resources",
    "negotiated",
    "basically",
    "gives",
    "control",
    "application",
    "master",
    "application",
    "master",
    "running",
    "within",
    "one",
    "containers",
    "one",
    "nodes",
    "using",
    "containers",
    "take",
    "care",
    "execution",
    "looks",
    "basically",
    "container",
    "said",
    "collection",
    "resources",
    "like",
    "cpu",
    "memory",
    "disk",
    "would",
    "used",
    "already",
    "data",
    "network",
    "node",
    "manager",
    "basically",
    "looking",
    "request",
    "application",
    "master",
    "basically",
    "granting",
    "request",
    "basically",
    "allocating",
    "containers",
    "could",
    "different",
    "sizing",
    "containers",
    "let",
    "take",
    "example",
    "mentioned",
    "total",
    "resources",
    "available",
    "particular",
    "node",
    "portion",
    "resources",
    "allocated",
    "node",
    "manager",
    "let",
    "imagine",
    "node",
    "node",
    "manager",
    "processing",
    "slave",
    "running",
    "total",
    "resources",
    "node",
    "portion",
    "ram",
    "cpu",
    "cores",
    "basically",
    "allocated",
    "node",
    "manager",
    "could",
    "say",
    "total",
    "100",
    "gb",
    "ram",
    "say",
    "around",
    "60",
    "cores",
    "particular",
    "node",
    "ram",
    "node",
    "cpu",
    "cores",
    "node",
    "portion",
    "right",
    "say",
    "might",
    "70",
    "60",
    "percent",
    "total",
    "resources",
    "could",
    "say",
    "around",
    "60",
    "gb",
    "ram",
    "could",
    "say",
    "around",
    "40",
    "v",
    "cores",
    "allocated",
    "node",
    "manager",
    "settings",
    "given",
    "yarn",
    "hyphen",
    "site",
    "file",
    "apart",
    "allocation",
    "60",
    "gb",
    "ram",
    "40v",
    "cores",
    "also",
    "properties",
    "say",
    "container",
    "sizes",
    "example",
    "could",
    "small",
    "container",
    "setting",
    "could",
    "say",
    "every",
    "container",
    "could",
    "2gb",
    "ram",
    "say",
    "one",
    "virtual",
    "cpu",
    "core",
    "smallest",
    "container",
    "based",
    "total",
    "resources",
    "could",
    "calculate",
    "many",
    "small",
    "containers",
    "could",
    "running",
    "say",
    "2gb",
    "ram",
    "could",
    "around",
    "30",
    "containers",
    "talking",
    "one",
    "virtual",
    "cpu",
    "core",
    "totally",
    "could",
    "around",
    "30",
    "small",
    "containers",
    "could",
    "running",
    "parallel",
    "particular",
    "node",
    "calculation",
    "would",
    "say",
    "10",
    "cpu",
    "cores",
    "utilized",
    "could",
    "bigger",
    "container",
    "size",
    "could",
    "say",
    "would",
    "go",
    "2",
    "cpu",
    "cores",
    "3gb",
    "ram",
    "3gb",
    "ram",
    "2",
    "cpu",
    "cores",
    "would",
    "give",
    "around",
    "20",
    "containers",
    "bigger",
    "size",
    "container",
    "sizing",
    "defined",
    "yarn",
    "hyphen",
    "site",
    "file",
    "know",
    "particular",
    "node",
    "kind",
    "allocation",
    "either",
    "could",
    "30",
    "small",
    "containers",
    "running",
    "could",
    "20",
    "big",
    "containers",
    "running",
    "would",
    "apply",
    "multiple",
    "nodes",
    "node",
    "manager",
    "based",
    "request",
    "application",
    "master",
    "allocate",
    "containers",
    "remember",
    "within",
    "one",
    "containers",
    "would",
    "application",
    "master",
    "running",
    "containers",
    "could",
    "used",
    "processing",
    "requirement",
    "application",
    "master",
    "per",
    "application",
    "one",
    "uses",
    "resources",
    "basically",
    "manages",
    "uses",
    "resources",
    "individual",
    "application",
    "remember",
    "10",
    "applications",
    "running",
    "yarn",
    "would",
    "10",
    "application",
    "masters",
    "one",
    "responsible",
    "application",
    "application",
    "master",
    "one",
    "also",
    "interacts",
    "scheduler",
    "basically",
    "know",
    "much",
    "amount",
    "resources",
    "could",
    "allocated",
    "one",
    "application",
    "application",
    "master",
    "one",
    "uses",
    "resources",
    "never",
    "negotiate",
    "resources",
    "node",
    "manager",
    "application",
    "master",
    "application",
    "master",
    "always",
    "go",
    "back",
    "resource",
    "manager",
    "needs",
    "resources",
    "always",
    "resource",
    "manager",
    "internally",
    "resource",
    "manager",
    "component",
    "application",
    "manager",
    "negotiates",
    "resources",
    "point",
    "time",
    "due",
    "node",
    "failures",
    "due",
    "requirements",
    "application",
    "master",
    "needs",
    "resources",
    "one",
    "multiple",
    "nodes",
    "always",
    "contacting",
    "resource",
    "manager",
    "internally",
    "applications",
    "manager",
    "containers",
    "looks",
    "client",
    "submits",
    "job",
    "request",
    "resource",
    "manager",
    "know",
    "resource",
    "manager",
    "internally",
    "scheduler",
    "applications",
    "manager",
    "node",
    "managers",
    "running",
    "multiple",
    "machines",
    "ones",
    "tracking",
    "resources",
    "giving",
    "information",
    "source",
    "manager",
    "resource",
    "manager",
    "would",
    "say",
    "component",
    "applications",
    "manager",
    "could",
    "request",
    "resources",
    "multiple",
    "node",
    "managers",
    "say",
    "request",
    "resources",
    "containers",
    "resource",
    "manager",
    "basically",
    "request",
    "resources",
    "one",
    "multiple",
    "nodes",
    "node",
    "manager",
    "one",
    "approves",
    "containers",
    "container",
    "approved",
    "resource",
    "manager",
    "triggers",
    "piece",
    "code",
    "application",
    "master",
    "obviously",
    "needs",
    "resources",
    "would",
    "run",
    "one",
    "containers",
    "use",
    "containers",
    "execution",
    "client",
    "submits",
    "application",
    "resource",
    "manager",
    "resource",
    "manager",
    "allocates",
    "container",
    "would",
    "say",
    "high",
    "level",
    "right",
    "resource",
    "manager",
    "negotiating",
    "resources",
    "internally",
    "negotiating",
    "resources",
    "applications",
    "manager",
    "granting",
    "request",
    "node",
    "manager",
    "say",
    "resource",
    "manager",
    "locates",
    "container",
    "application",
    "master",
    "basically",
    "contacts",
    "related",
    "node",
    "manager",
    "needs",
    "use",
    "containers",
    "node",
    "manager",
    "one",
    "launches",
    "container",
    "basically",
    "gives",
    "resources",
    "within",
    "application",
    "run",
    "application",
    "master",
    "accommodate",
    "one",
    "containers",
    "use",
    "containers",
    "processing",
    "within",
    "containers",
    "actual",
    "execution",
    "happens",
    "could",
    "map",
    "task",
    "could",
    "reduced",
    "task",
    "could",
    "spark",
    "executor",
    "taking",
    "care",
    "spark",
    "tasks",
    "many",
    "processing",
    "look",
    "demo",
    "yarn",
    "works",
    "would",
    "suggest",
    "looking",
    "one",
    "blogs",
    "cloudera",
    "look",
    "yarn",
    "untangling",
    "really",
    "good",
    "blog",
    "basically",
    "talks",
    "overall",
    "functionality",
    "explained",
    "mentioned",
    "basically",
    "master",
    "process",
    "worker",
    "process",
    "basically",
    "takes",
    "care",
    "processing",
    "resource",
    "manager",
    "master",
    "node",
    "manager",
    "slave",
    "also",
    "talks",
    "resources",
    "node",
    "manager",
    "talks",
    "yarn",
    "configuration",
    "file",
    "give",
    "properties",
    "basically",
    "shows",
    "node",
    "manager",
    "reports",
    "amount",
    "resources",
    "resource",
    "manager",
    "remember",
    "worker",
    "node",
    "shows",
    "18",
    "8",
    "cpu",
    "cores",
    "128",
    "gb",
    "ram",
    "node",
    "manager",
    "says",
    "64",
    "v",
    "cores",
    "ram",
    "128",
    "gb",
    "total",
    "capacity",
    "node",
    "portion",
    "node",
    "allocated",
    "node",
    "manager",
    "node",
    "manager",
    "reports",
    "resource",
    "manager",
    "requesting",
    "containers",
    "based",
    "application",
    "container",
    "basically",
    "logical",
    "name",
    "given",
    "combination",
    "vcore",
    "ram",
    "within",
    "container",
    "would",
    "basically",
    "process",
    "running",
    "application",
    "starts",
    "node",
    "manager",
    "guaranteed",
    "containers",
    "application",
    "resource",
    "manager",
    "basically",
    "already",
    "started",
    "application",
    "master",
    "within",
    "container",
    "application",
    "master",
    "uses",
    "containers",
    "tasks",
    "would",
    "run",
    "good",
    "blog",
    "refer",
    "also",
    "talks",
    "mapreduce",
    "already",
    "followed",
    "mapreduce",
    "tutorials",
    "past",
    "would",
    "know",
    "different",
    "kind",
    "tasks",
    "map",
    "reduce",
    "map",
    "reduce",
    "tasks",
    "could",
    "running",
    "within",
    "container",
    "one",
    "multiple",
    "said",
    "could",
    "map",
    "task",
    "could",
    "reduced",
    "task",
    "could",
    "spark",
    "based",
    "task",
    "would",
    "running",
    "within",
    "container",
    "task",
    "finishes",
    "basically",
    "resources",
    "freed",
    "container",
    "released",
    "resources",
    "given",
    "back",
    "yarn",
    "take",
    "care",
    "processing",
    "look",
    "blog",
    "also",
    "look",
    "part",
    "two",
    "talk",
    "mainly",
    "configuration",
    "settings",
    "always",
    "look",
    "talks",
    "much",
    "resources",
    "allocated",
    "node",
    "manager",
    "basically",
    "talks",
    "operating",
    "system",
    "overhead",
    "talks",
    "services",
    "talks",
    "clouded",
    "hotend",
    "works",
    "related",
    "services",
    "running",
    "processes",
    "might",
    "running",
    "based",
    "portion",
    "ram",
    "cpu",
    "cores",
    "would",
    "allocated",
    "node",
    "manager",
    "would",
    "done",
    "yarn",
    "hyphen",
    "site",
    "file",
    "basically",
    "shows",
    "total",
    "amount",
    "memory",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "within",
    "every",
    "machine",
    "node",
    "manager",
    "running",
    "every",
    "machine",
    "yarn",
    "hyphen",
    "site",
    "file",
    "would",
    "properties",
    "would",
    "say",
    "minimum",
    "container",
    "size",
    "maximum",
    "container",
    "size",
    "terms",
    "ram",
    "minimum",
    "cpu",
    "cores",
    "maximum",
    "cpu",
    "cores",
    "incremental",
    "size",
    "ram",
    "cpu",
    "cores",
    "increment",
    "properties",
    "define",
    "containers",
    "allocated",
    "application",
    "request",
    "look",
    "could",
    "good",
    "information",
    "talks",
    "different",
    "properties",
    "look",
    "talks",
    "scheduling",
    "look",
    "particular",
    "blog",
    "also",
    "talks",
    "scheduling",
    "talks",
    "scheduling",
    "yarn",
    "talks",
    "fair",
    "scheduler",
    "basically",
    "different",
    "cues",
    "allocations",
    "done",
    "also",
    "different",
    "ways",
    "queues",
    "managed",
    "different",
    "schedulers",
    "used",
    "always",
    "look",
    "series",
    "blog",
    "also",
    "checking",
    "yarn",
    "schedulers",
    "search",
    "hadoop",
    "definitive",
    "guide",
    "could",
    "give",
    "information",
    "looks",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "look",
    "book",
    "talks",
    "different",
    "resources",
    "mentioned",
    "could",
    "fee",
    "scheduler",
    "first",
    "first",
    "basically",
    "means",
    "long",
    "running",
    "application",
    "submitted",
    "cluster",
    "small",
    "running",
    "applications",
    "wait",
    "way",
    "would",
    "preferred",
    "option",
    "look",
    "fifo",
    "scheduler",
    "look",
    "capacity",
    "scheduler",
    "basically",
    "means",
    "could",
    "different",
    "queues",
    "created",
    "queues",
    "would",
    "resources",
    "allocated",
    "could",
    "production",
    "queue",
    "production",
    "jobs",
    "running",
    "particular",
    "queue",
    "fixed",
    "amount",
    "resources",
    "allocated",
    "could",
    "development",
    "queue",
    "development",
    "jobs",
    "running",
    "running",
    "parallel",
    "could",
    "also",
    "look",
    "fair",
    "scheduler",
    "basically",
    "means",
    "multiple",
    "applications",
    "could",
    "running",
    "cluster",
    "however",
    "would",
    "fair",
    "share",
    "say",
    "fair",
    "share",
    "brief",
    "means",
    "given",
    "50",
    "percent",
    "resources",
    "queue",
    "production",
    "50",
    "percent",
    "resources",
    "queue",
    "development",
    "running",
    "parallel",
    "would",
    "access",
    "50",
    "percent",
    "cluster",
    "resources",
    "however",
    "one",
    "queue",
    "unutilized",
    "second",
    "queue",
    "utilize",
    "cluster",
    "resources",
    "look",
    "fair",
    "scheduling",
    "part",
    "also",
    "shows",
    "allocations",
    "given",
    "learn",
    "schedulers",
    "cues",
    "used",
    "managing",
    "multiple",
    "applications",
    "spend",
    "time",
    "looking",
    "ways",
    "quick",
    "ways",
    "interacting",
    "yarn",
    "form",
    "demo",
    "understand",
    "learn",
    "yarn",
    "works",
    "look",
    "particular",
    "cluster",
    "designated",
    "cluster",
    "used",
    "could",
    "using",
    "similar",
    "kind",
    "commands",
    "apache",
    "based",
    "cluster",
    "cloudera",
    "quick",
    "start",
    "vm",
    "already",
    "cloud",
    "error",
    "hortonworks",
    "cluster",
    "running",
    "different",
    "ways",
    "interact",
    "yarn",
    "look",
    "information",
    "one",
    "basically",
    "looking",
    "admin",
    "console",
    "would",
    "look",
    "cloud",
    "data",
    "manager",
    "basically",
    "admin",
    "console",
    "cloudera",
    "distribution",
    "hadoop",
    "similarly",
    "could",
    "hortonworks",
    "cluster",
    "access",
    "admin",
    "console",
    "even",
    "read",
    "access",
    "cluster",
    "admin",
    "console",
    "search",
    "yarn",
    "service",
    "running",
    "click",
    "yarn",
    "service",
    "gives",
    "different",
    "tabs",
    "instances",
    "tells",
    "basically",
    "different",
    "roles",
    "yarn",
    "service",
    "running",
    "multiple",
    "node",
    "managers",
    "showing",
    "stop",
    "status",
    "nothing",
    "worry",
    "three",
    "six",
    "node",
    "managers",
    "resource",
    "manager",
    "one",
    "also",
    "high",
    "availability",
    "active",
    "standby",
    "also",
    "job",
    "history",
    "server",
    "would",
    "show",
    "applications",
    "completed",
    "look",
    "yarn",
    "configurations",
    "explaining",
    "always",
    "look",
    "properties",
    "related",
    "allocation",
    "search",
    "course",
    "show",
    "properties",
    "talk",
    "allocations",
    "see",
    "looking",
    "yarn",
    "app",
    "mapreduce",
    "application",
    "master",
    "resource",
    "cpu",
    "course",
    "cpu",
    "course",
    "allocated",
    "mapreduce",
    "map",
    "task",
    "reduce",
    "task",
    "looking",
    "yarn",
    "node",
    "manager",
    "resource",
    "cpu",
    "course",
    "basically",
    "says",
    "every",
    "node",
    "manager",
    "every",
    "node",
    "would",
    "allocated",
    "six",
    "cpu",
    "cores",
    "container",
    "sizing",
    "minimum",
    "allocation",
    "one",
    "cpu",
    "core",
    "maximum",
    "could",
    "two",
    "cpu",
    "cores",
    "similarly",
    "could",
    "also",
    "searching",
    "memory",
    "allocation",
    "could",
    "scroll",
    "see",
    "kind",
    "memory",
    "allocation",
    "done",
    "node",
    "manager",
    "look",
    "give",
    "information",
    "node",
    "manager",
    "basically",
    "says",
    "container",
    "minimum",
    "allocation",
    "2",
    "gb",
    "maximum",
    "3",
    "gb",
    "look",
    "node",
    "manager",
    "given",
    "25",
    "gb",
    "per",
    "node",
    "combination",
    "memory",
    "cpu",
    "cores",
    "total",
    "amount",
    "resources",
    "allocated",
    "every",
    "node",
    "manager",
    "always",
    "look",
    "applications",
    "tab",
    "would",
    "show",
    "us",
    "different",
    "applications",
    "submitted",
    "yarn",
    "example",
    "right",
    "see",
    "spark",
    "application",
    "running",
    "basically",
    "user",
    "using",
    "spark",
    "shell",
    "triggered",
    "application",
    "spark",
    "running",
    "yarn",
    "look",
    "different",
    "applications",
    "workload",
    "information",
    "always",
    "search",
    "based",
    "number",
    "days",
    "many",
    "applications",
    "run",
    "always",
    "go",
    "web",
    "ui",
    "searching",
    "resource",
    "manager",
    "web",
    "ui",
    "access",
    "give",
    "overall",
    "information",
    "cluster",
    "basically",
    "says",
    "100",
    "gb",
    "memory",
    "allocated",
    "could",
    "say",
    "25",
    "gb",
    "per",
    "node",
    "four",
    "node",
    "managers",
    "running",
    "24",
    "cores",
    "six",
    "cores",
    "per",
    "node",
    "look",
    "nodes",
    "could",
    "get",
    "information",
    "tells",
    "four",
    "node",
    "managers",
    "running",
    "node",
    "managers",
    "basically",
    "25",
    "gb",
    "memory",
    "allocated",
    "per",
    "node",
    "six",
    "cores",
    "portion",
    "utilized",
    "always",
    "look",
    "scheduler",
    "give",
    "us",
    "information",
    "kind",
    "scheduler",
    "allocated",
    "basically",
    "see",
    "root",
    "cue",
    "within",
    "root",
    "default",
    "queue",
    "basically",
    "user",
    "queue",
    "based",
    "different",
    "users",
    "always",
    "scroll",
    "give",
    "us",
    "information",
    "fair",
    "share",
    "see",
    "root",
    "dot",
    "default",
    "50",
    "percent",
    "resources",
    "queue",
    "also",
    "50",
    "percent",
    "resources",
    "also",
    "gives",
    "idea",
    "fair",
    "scheduler",
    "used",
    "always",
    "confirm",
    "using",
    "fair",
    "scheduler",
    "capacity",
    "scheduler",
    "takes",
    "care",
    "location",
    "search",
    "scheduler",
    "give",
    "understanding",
    "kind",
    "scheduler",
    "used",
    "allocations",
    "given",
    "particular",
    "scheduler",
    "fair",
    "scheduler",
    "shows",
    "root",
    "root",
    "q",
    "given",
    "hundred",
    "100",
    "capacity",
    "within",
    "default",
    "also",
    "takes",
    "100",
    "understand",
    "yarn",
    "looking",
    "yarn",
    "web",
    "ui",
    "looking",
    "configurations",
    "look",
    "applications",
    "always",
    "look",
    "different",
    "actions",
    "since",
    "admin",
    "access",
    "information",
    "download",
    "client",
    "configuration",
    "always",
    "look",
    "history",
    "server",
    "give",
    "us",
    "information",
    "applications",
    "successfully",
    "completed",
    "yarn",
    "ui",
    "also",
    "going",
    "hue",
    "web",
    "interface",
    "web",
    "interface",
    "also",
    "basically",
    "allows",
    "look",
    "jobs",
    "click",
    "hue",
    "web",
    "ui",
    "access",
    "show",
    "way",
    "get",
    "hue",
    "graphical",
    "user",
    "interface",
    "mainly",
    "comes",
    "cloud",
    "error",
    "also",
    "configure",
    "apache",
    "hortonworks",
    "different",
    "way",
    "giving",
    "web",
    "ui",
    "access",
    "click",
    "get",
    "hue",
    "also",
    "one",
    "way",
    "look",
    "yarn",
    "look",
    "jobs",
    "running",
    "issues",
    "web",
    "interfaces",
    "either",
    "look",
    "yarn",
    "web",
    "ui",
    "hue",
    "something",
    "called",
    "job",
    "browser",
    "also",
    "give",
    "information",
    "different",
    "applications",
    "might",
    "run",
    "remove",
    "one",
    "would",
    "basically",
    "give",
    "list",
    "different",
    "kind",
    "jobs",
    "workflows",
    "run",
    "either",
    "spark",
    "based",
    "application",
    "map",
    "reduce",
    "coming",
    "hive",
    "list",
    "applications",
    "says",
    "mapreduce",
    "spark",
    "something",
    "killed",
    "something",
    "successful",
    "basically",
    "probably",
    "hive",
    "query",
    "triggered",
    "mapreduce",
    "job",
    "click",
    "application",
    "tells",
    "many",
    "tasks",
    "run",
    "map",
    "task",
    "ran",
    "get",
    "metadata",
    "information",
    "obviously",
    "also",
    "look",
    "yarn",
    "ui",
    "look",
    "applications",
    "give",
    "detailed",
    "information",
    "map",
    "reduce",
    "many",
    "map",
    "reduce",
    "tasks",
    "run",
    "different",
    "counters",
    "spark",
    "application",
    "let",
    "follow",
    "spark",
    "history",
    "server",
    "job",
    "history",
    "server",
    "always",
    "use",
    "web",
    "ui",
    "look",
    "jobs",
    "finding",
    "lot",
    "useful",
    "information",
    "also",
    "looking",
    "many",
    "resources",
    "used",
    "happened",
    "job",
    "successful",
    "fail",
    "job",
    "status",
    "apart",
    "web",
    "ui",
    "always",
    "might",
    "access",
    "particular",
    "cluster",
    "production",
    "cluster",
    "might",
    "restrictions",
    "organization",
    "might",
    "access",
    "given",
    "users",
    "graphical",
    "user",
    "interface",
    "like",
    "might",
    "would",
    "access",
    "cloud",
    "era",
    "manager",
    "admin",
    "console",
    "probably",
    "organization",
    "managing",
    "multiple",
    "clusters",
    "using",
    "admin",
    "console",
    "one",
    "way",
    "would",
    "access",
    "web",
    "console",
    "basically",
    "edge",
    "node",
    "client",
    "machine",
    "connect",
    "cluster",
    "working",
    "let",
    "log",
    "give",
    "different",
    "commands",
    "command",
    "line",
    "access",
    "different",
    "details",
    "always",
    "check",
    "typing",
    "map",
    "red",
    "gives",
    "different",
    "options",
    "look",
    "map",
    "reduce",
    "related",
    "jobs",
    "look",
    "different",
    "queues",
    "queues",
    "configured",
    "look",
    "history",
    "server",
    "also",
    "admin",
    "stuff",
    "provided",
    "access",
    "example",
    "say",
    "map",
    "red",
    "q",
    "basically",
    "gives",
    "option",
    "says",
    "would",
    "want",
    "would",
    "want",
    "list",
    "queues",
    "want",
    "information",
    "particular",
    "queue",
    "let",
    "try",
    "list",
    "give",
    "different",
    "queues",
    "used",
    "know",
    "per",
    "user",
    "queue",
    "dynamically",
    "gets",
    "created",
    "root",
    "dot",
    "users",
    "gives",
    "status",
    "queue",
    "capacity",
    "kind",
    "maximum",
    "capacity",
    "capping",
    "done",
    "get",
    "see",
    "huge",
    "list",
    "queues",
    "dynamically",
    "get",
    "configured",
    "environment",
    "also",
    "look",
    "root",
    "dot",
    "default",
    "could",
    "also",
    "picked",
    "one",
    "particular",
    "queue",
    "could",
    "said",
    "show",
    "jobs",
    "could",
    "also",
    "give",
    "yarn",
    "command",
    "let",
    "clear",
    "screen",
    "say",
    "yarn",
    "shows",
    "different",
    "options",
    "apart",
    "web",
    "interface",
    "something",
    "like",
    "web",
    "ui",
    "apart",
    "yarn",
    "web",
    "ui",
    "could",
    "also",
    "looking",
    "information",
    "using",
    "yarn",
    "commands",
    "list",
    "commands",
    "check",
    "type",
    "yarn",
    "version",
    "would",
    "want",
    "see",
    "version",
    "basically",
    "gives",
    "information",
    "hadoop",
    "version",
    "used",
    "distribution",
    "version",
    "see",
    "working",
    "cloudera",
    "distribution",
    "internally",
    "using",
    "hadoop",
    "similarly",
    "yarn",
    "application",
    "list",
    "give",
    "could",
    "exhaustive",
    "list",
    "applications",
    "running",
    "applications",
    "completed",
    "see",
    "applications",
    "right",
    "probably",
    "applications",
    "running",
    "also",
    "shows",
    "could",
    "pulling",
    "different",
    "status",
    "submitted",
    "accepted",
    "running",
    "could",
    "also",
    "say",
    "would",
    "want",
    "see",
    "services",
    "finished",
    "running",
    "could",
    "say",
    "yarn",
    "application",
    "list",
    "app",
    "state",
    "says",
    "finished",
    "could",
    "using",
    "command",
    "could",
    "say",
    "yarn",
    "list",
    "would",
    "want",
    "see",
    "app",
    "states",
    "gives",
    "applications",
    "finished",
    "would",
    "want",
    "list",
    "applications",
    "finished",
    "might",
    "applications",
    "succeeded",
    "right",
    "huge",
    "list",
    "application",
    "coming",
    "history",
    "server",
    "basically",
    "showing",
    "huge",
    "list",
    "applications",
    "completed",
    "one",
    "way",
    "could",
    "also",
    "searching",
    "one",
    "particular",
    "application",
    "would",
    "want",
    "search",
    "particular",
    "application",
    "application",
    "id",
    "could",
    "always",
    "grip",
    "simple",
    "way",
    "could",
    "say",
    "basically",
    "let",
    "pick",
    "one",
    "would",
    "want",
    "search",
    "would",
    "want",
    "details",
    "could",
    "obviously",
    "calling",
    "previous",
    "command",
    "could",
    "grip",
    "want",
    "would",
    "want",
    "search",
    "application",
    "list",
    "applications",
    "shows",
    "application",
    "could",
    "pull",
    "information",
    "application",
    "could",
    "look",
    "log",
    "files",
    "particular",
    "application",
    "giving",
    "application",
    "id",
    "could",
    "say",
    "yarn",
    "logs",
    "option",
    "every",
    "time",
    "anytime",
    "doubt",
    "hit",
    "enter",
    "always",
    "give",
    "options",
    "need",
    "give",
    "particular",
    "command",
    "say",
    "yarn",
    "logs",
    "application",
    "id",
    "copied",
    "application",
    "id",
    "could",
    "give",
    "could",
    "give",
    "options",
    "like",
    "app",
    "owner",
    "would",
    "want",
    "get",
    "container",
    "details",
    "would",
    "want",
    "check",
    "particular",
    "node",
    "giving",
    "yarn",
    "logs",
    "pointing",
    "application",
    "id",
    "says",
    "log",
    "aggregation",
    "completed",
    "might",
    "might",
    "application",
    "triggered",
    "based",
    "particular",
    "interactive",
    "shell",
    "based",
    "particular",
    "query",
    "log",
    "existing",
    "particular",
    "application",
    "always",
    "look",
    "status",
    "application",
    "kill",
    "application",
    "saying",
    "yan",
    "yan",
    "application",
    "would",
    "want",
    "application",
    "hit",
    "enter",
    "shows",
    "different",
    "options",
    "tried",
    "app",
    "states",
    "could",
    "always",
    "look",
    "last",
    "one",
    "says",
    "status",
    "status",
    "could",
    "giving",
    "application",
    "id",
    "tells",
    "status",
    "application",
    "connects",
    "resource",
    "manager",
    "tells",
    "application",
    "id",
    "kind",
    "application",
    "ran",
    "queue",
    "job",
    "running",
    "start",
    "end",
    "time",
    "progress",
    "status",
    "finished",
    "succeeded",
    "basically",
    "gives",
    "also",
    "information",
    "application",
    "master",
    "running",
    "gives",
    "information",
    "find",
    "job",
    "details",
    "history",
    "server",
    "interested",
    "looking",
    "also",
    "gives",
    "aggregate",
    "resource",
    "allocation",
    "tells",
    "much",
    "gb",
    "memory",
    "many",
    "c",
    "core",
    "seconds",
    "used",
    "basically",
    "looking",
    "application",
    "details",
    "could",
    "kill",
    "application",
    "application",
    "already",
    "running",
    "could",
    "always",
    "yarn",
    "application",
    "minus",
    "skill",
    "could",
    "giving",
    "application",
    "could",
    "try",
    "killing",
    "however",
    "would",
    "say",
    "application",
    "already",
    "finished",
    "application",
    "running",
    "application",
    "already",
    "given",
    "application",
    "id",
    "resource",
    "manager",
    "could",
    "kill",
    "also",
    "say",
    "yarn",
    "node",
    "list",
    "give",
    "list",
    "node",
    "managers",
    "looking",
    "yarn",
    "web",
    "ui",
    "pulling",
    "information",
    "get",
    "kind",
    "information",
    "command",
    "line",
    "always",
    "remember",
    "always",
    "try",
    "well",
    "accustomed",
    "command",
    "line",
    "various",
    "things",
    "command",
    "line",
    "obviously",
    "web",
    "uis",
    "help",
    "graphical",
    "interface",
    "easily",
    "able",
    "access",
    "things",
    "could",
    "also",
    "starting",
    "resource",
    "manager",
    "would",
    "already",
    "running",
    "cluster",
    "could",
    "give",
    "yarn",
    "resource",
    "manager",
    "could",
    "get",
    "logs",
    "resource",
    "manager",
    "would",
    "want",
    "giving",
    "yarn",
    "demon",
    "try",
    "say",
    "yarn",
    "demon",
    "says",
    "find",
    "demon",
    "give",
    "something",
    "like",
    "get",
    "level",
    "give",
    "node",
    "ip",
    "address",
    "want",
    "check",
    "logs",
    "resource",
    "manager",
    "could",
    "giving",
    "get",
    "cloud",
    "error",
    "manager",
    "look",
    "nodes",
    "ip",
    "address",
    "could",
    "giving",
    "command",
    "something",
    "like",
    "basically",
    "gives",
    "level",
    "log",
    "got",
    "resource",
    "manager",
    "address",
    "web",
    "ui",
    "giving",
    "command",
    "look",
    "demand",
    "log",
    "basically",
    "says",
    "would",
    "want",
    "look",
    "resource",
    "manager",
    "related",
    "log",
    "log",
    "4j",
    "used",
    "logging",
    "kind",
    "level",
    "set",
    "info",
    "changed",
    "way",
    "logging",
    "information",
    "try",
    "commands",
    "also",
    "yarn",
    "example",
    "looking",
    "yarn",
    "rm",
    "admin",
    "always",
    "yarn",
    "rm",
    "admin",
    "basically",
    "gives",
    "lot",
    "informations",
    "like",
    "refreshing",
    "cues",
    "refreshing",
    "nodes",
    "basically",
    "looking",
    "admin",
    "acls",
    "getting",
    "groups",
    "could",
    "always",
    "get",
    "group",
    "names",
    "particular",
    "user",
    "could",
    "search",
    "particular",
    "user",
    "yarn",
    "hdfs",
    "could",
    "say",
    "would",
    "want",
    "get",
    "groups",
    "could",
    "searching",
    "say",
    "username",
    "hdfs",
    "tells",
    "sdfs",
    "belongs",
    "hadoop",
    "group",
    "similarly",
    "could",
    "search",
    "say",
    "map",
    "red",
    "could",
    "search",
    "yarn",
    "service",
    "related",
    "users",
    "automatically",
    "get",
    "created",
    "pull",
    "information",
    "related",
    "always",
    "refresh",
    "nodes",
    "kind",
    "command",
    "mainly",
    "done",
    "internally",
    "useful",
    "commissioning",
    "decommissioning",
    "case",
    "cloudera",
    "hortonworks",
    "kind",
    "cluster",
    "would",
    "manually",
    "giving",
    "command",
    "commissioning",
    "decommissioning",
    "admin",
    "console",
    "administrator",
    "could",
    "restart",
    "services",
    "affected",
    "take",
    "care",
    "working",
    "apache",
    "cluster",
    "commissioning",
    "decommissioning",
    "would",
    "using",
    "two",
    "commands",
    "refresh",
    "nodes",
    "basically",
    "refreshing",
    "notes",
    "used",
    "processing",
    "similarly",
    "could",
    "command",
    "refresh",
    "notes",
    "comes",
    "sdfs",
    "different",
    "options",
    "use",
    "yarn",
    "command",
    "line",
    "could",
    "also",
    "using",
    "curl",
    "commands",
    "get",
    "information",
    "cluster",
    "giving",
    "curl",
    "minus",
    "x",
    "basically",
    "pointing",
    "resource",
    "manager",
    "web",
    "ui",
    "address",
    "would",
    "like",
    "print",
    "cluster",
    "related",
    "metrics",
    "could",
    "simply",
    "basically",
    "gives",
    "high",
    "level",
    "information",
    "many",
    "applications",
    "submitted",
    "many",
    "pending",
    "reserved",
    "resources",
    "available",
    "amount",
    "memory",
    "cpu",
    "cores",
    "information",
    "similarly",
    "using",
    "curl",
    "commands",
    "get",
    "information",
    "like",
    "scheduler",
    "information",
    "would",
    "replace",
    "metrics",
    "scheduler",
    "could",
    "get",
    "information",
    "different",
    "queues",
    "huge",
    "list",
    "cancel",
    "would",
    "give",
    "list",
    "queues",
    "allocated",
    "resources",
    "allocated",
    "queue",
    "could",
    "also",
    "get",
    "cluster",
    "information",
    "application",
    "ids",
    "status",
    "running",
    "applications",
    "running",
    "yarn",
    "would",
    "replace",
    "last",
    "bit",
    "would",
    "say",
    "would",
    "want",
    "look",
    "applications",
    "gives",
    "huge",
    "list",
    "applications",
    "grip",
    "filtering",
    "specific",
    "application",
    "related",
    "information",
    "similarly",
    "looking",
    "nodes",
    "always",
    "looking",
    "node",
    "specific",
    "information",
    "gives",
    "many",
    "nodes",
    "could",
    "mainly",
    "used",
    "application",
    "wants",
    "web",
    "application",
    "wants",
    "use",
    "curl",
    "command",
    "would",
    "want",
    "get",
    "information",
    "cluster",
    "http",
    "interface",
    "comes",
    "application",
    "basically",
    "try",
    "running",
    "simple",
    "sample",
    "mapreduce",
    "job",
    "could",
    "triggered",
    "yarn",
    "would",
    "use",
    "resources",
    "look",
    "application",
    "looking",
    "specific",
    "directory",
    "one",
    "lot",
    "files",
    "directories",
    "could",
    "pick",
    "one",
    "could",
    "using",
    "simple",
    "example",
    "processing",
    "let",
    "take",
    "file",
    "file",
    "could",
    "run",
    "simple",
    "word",
    "count",
    "could",
    "running",
    "hive",
    "query",
    "triggers",
    "mapreduce",
    "job",
    "could",
    "even",
    "run",
    "spark",
    "application",
    "would",
    "show",
    "application",
    "running",
    "cluster",
    "example",
    "would",
    "say",
    "spark",
    "shell",
    "know",
    "interactive",
    "way",
    "working",
    "spark",
    "internally",
    "triggers",
    "spark",
    "submit",
    "runs",
    "application",
    "spark",
    "shell",
    "default",
    "contact",
    "yarn",
    "gets",
    "application",
    "id",
    "running",
    "yarn",
    "master",
    "yarn",
    "access",
    "interactive",
    "way",
    "working",
    "spark",
    "go",
    "look",
    "applications",
    "able",
    "see",
    "application",
    "started",
    "shows",
    "application",
    "3827",
    "started",
    "yarn",
    "also",
    "look",
    "yarn",
    "ui",
    "shows",
    "application",
    "started",
    "basically",
    "one",
    "running",
    "container",
    "one",
    "cpu",
    "core",
    "allocated",
    "2gb",
    "ram",
    "progress",
    "although",
    "anything",
    "always",
    "look",
    "application",
    "yarn",
    "ui",
    "mentioned",
    "applications",
    "tab",
    "within",
    "yarn",
    "services",
    "gives",
    "us",
    "information",
    "even",
    "click",
    "application",
    "follow",
    "see",
    "information",
    "given",
    "access",
    "simple",
    "application",
    "triggered",
    "using",
    "spark",
    "shell",
    "similarly",
    "basically",
    "running",
    "mapreduce",
    "run",
    "mapreduce",
    "say",
    "hadoop",
    "jar",
    "basically",
    "needs",
    "class",
    "look",
    "default",
    "path",
    "opt",
    "cloud",
    "error",
    "parcels",
    "cdh",
    "lib",
    "hadoop",
    "mapreduce",
    "hadoop",
    "mapreduce",
    "examples",
    "look",
    "particular",
    "jar",
    "file",
    "hit",
    "enter",
    "shows",
    "different",
    "classes",
    "part",
    "jar",
    "would",
    "like",
    "use",
    "word",
    "count",
    "could",
    "give",
    "could",
    "say",
    "word",
    "count",
    "remember",
    "could",
    "run",
    "job",
    "particular",
    "queue",
    "giving",
    "argument",
    "could",
    "say",
    "minus",
    "map",
    "red",
    "dot",
    "job",
    "dot",
    "q",
    "dot",
    "name",
    "point",
    "job",
    "particular",
    "queue",
    "even",
    "give",
    "different",
    "arguments",
    "saying",
    "would",
    "want",
    "mapreduce",
    "output",
    "compressed",
    "want",
    "stored",
    "particular",
    "directory",
    "word",
    "count",
    "basically",
    "pointing",
    "particular",
    "input",
    "path",
    "output",
    "getting",
    "stored",
    "directory",
    "need",
    "choose",
    "say",
    "output",
    "new",
    "submit",
    "job",
    "submitted",
    "job",
    "connects",
    "resource",
    "manager",
    "basically",
    "gets",
    "job",
    "id",
    "gets",
    "application",
    "id",
    "shows",
    "track",
    "application",
    "always",
    "go",
    "yarn",
    "ui",
    "looking",
    "application",
    "resources",
    "using",
    "application",
    "big",
    "one",
    "already",
    "completed",
    "triggered",
    "one",
    "map",
    "task",
    "launched",
    "one",
    "reduced",
    "task",
    "working",
    "around",
    "12",
    "466",
    "records",
    "output",
    "map",
    "many",
    "number",
    "output",
    "records",
    "taken",
    "combiner",
    "finally",
    "reducer",
    "basically",
    "gives",
    "output",
    "application",
    "completed",
    "could",
    "looking",
    "yarn",
    "ui",
    "job",
    "completed",
    "might",
    "see",
    "application",
    "shows",
    "word",
    "count",
    "ran",
    "also",
    "shows",
    "previous",
    "spark",
    "shell",
    "job",
    "shows",
    "application",
    "completed",
    "would",
    "want",
    "information",
    "click",
    "go",
    "history",
    "server",
    "given",
    "access",
    "directly",
    "go",
    "history",
    "server",
    "web",
    "ui",
    "application",
    "shows",
    "shows",
    "many",
    "map",
    "reduce",
    "tasks",
    "running",
    "click",
    "particular",
    "application",
    "basically",
    "gives",
    "information",
    "map",
    "reduce",
    "tasks",
    "look",
    "different",
    "counters",
    "application",
    "right",
    "always",
    "look",
    "map",
    "specific",
    "tasks",
    "always",
    "look",
    "one",
    "particular",
    "task",
    "node",
    "running",
    "looking",
    "complete",
    "application",
    "log",
    "always",
    "click",
    "logs",
    "click",
    "full",
    "log",
    "gives",
    "information",
    "always",
    "look",
    "application",
    "give",
    "information",
    "app",
    "master",
    "launched",
    "could",
    "search",
    "word",
    "container",
    "could",
    "see",
    "job",
    "needs",
    "one",
    "multiple",
    "containers",
    "could",
    "say",
    "container",
    "requested",
    "could",
    "see",
    "container",
    "allocated",
    "see",
    "container",
    "size",
    "basically",
    "task",
    "moves",
    "initializing",
    "running",
    "container",
    "finally",
    "even",
    "search",
    "release",
    "tell",
    "container",
    "released",
    "always",
    "look",
    "log",
    "information",
    "interact",
    "yarn",
    "interact",
    "command",
    "line",
    "look",
    "information",
    "using",
    "yarn",
    "web",
    "ui",
    "also",
    "looking",
    "hue",
    "information",
    "welcome",
    "scoop",
    "tutorial",
    "one",
    "many",
    "features",
    "hadoop",
    "ecosystem",
    "hadoop",
    "file",
    "system",
    "today",
    "going",
    "cover",
    "need",
    "scoop",
    "scoop",
    "scoop",
    "features",
    "scoop",
    "architecture",
    "scoop",
    "import",
    "scoop",
    "export",
    "scoop",
    "processing",
    "finally",
    "little",
    "demo",
    "scoop",
    "see",
    "looks",
    "like",
    "need",
    "scoop",
    "come",
    "big",
    "data",
    "hadoop",
    "file",
    "system",
    "processing",
    "huge",
    "volumes",
    "data",
    "requires",
    "loading",
    "data",
    "diverse",
    "sources",
    "hadoop",
    "cluster",
    "see",
    "data",
    "processing",
    "process",
    "loading",
    "data",
    "heterogeneous",
    "sources",
    "comes",
    "set",
    "challenges",
    "challenges",
    "maintaining",
    "data",
    "consistency",
    "ensuring",
    "efficient",
    "utilization",
    "resources",
    "especially",
    "talking",
    "big",
    "data",
    "certainly",
    "use",
    "resources",
    "importing",
    "terabytes",
    "petabytes",
    "data",
    "course",
    "time",
    "loading",
    "bulk",
    "data",
    "hadoop",
    "possible",
    "one",
    "big",
    "challenges",
    "came",
    "first",
    "hadoop",
    "file",
    "system",
    "going",
    "loading",
    "data",
    "using",
    "script",
    "slow",
    "words",
    "write",
    "script",
    "whatever",
    "language",
    "would",
    "slowly",
    "load",
    "piece",
    "parse",
    "solution",
    "scoop",
    "scooped",
    "helped",
    "overcoming",
    "challenges",
    "traditional",
    "approach",
    "could",
    "lead",
    "bulk",
    "data",
    "rdbms",
    "hadoop",
    "easily",
    "thank",
    "enterprise",
    "server",
    "want",
    "take",
    "mysql",
    "sql",
    "want",
    "bring",
    "data",
    "hadoop",
    "warehouse",
    "data",
    "filing",
    "system",
    "scoop",
    "comes",
    "exactly",
    "scoop",
    "scoop",
    "tool",
    "used",
    "transfer",
    "bulk",
    "data",
    "hadoop",
    "external",
    "data",
    "stores",
    "relational",
    "databases",
    "sql",
    "server",
    "microsoft",
    "sql",
    "server",
    "sql",
    "server",
    "scoop",
    "equals",
    "sql",
    "plus",
    "hadoop",
    "see",
    "rdbms",
    "data",
    "stored",
    "scoop",
    "middle",
    "ground",
    "brings",
    "import",
    "hadoop",
    "file",
    "system",
    "also",
    "one",
    "features",
    "goes",
    "grabs",
    "data",
    "hadoop",
    "exports",
    "back",
    "rdbms",
    "let",
    "take",
    "look",
    "scoop",
    "features",
    "sku",
    "features",
    "parallel",
    "import",
    "export",
    "import",
    "results",
    "sql",
    "query",
    "connectors",
    "major",
    "rdbms",
    "databases",
    "kerberos",
    "security",
    "integration",
    "provides",
    "full",
    "incremental",
    "load",
    "look",
    "parallel",
    "import",
    "export",
    "scoop",
    "uses",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "framework",
    "import",
    "export",
    "data",
    "provides",
    "fault",
    "tolerance",
    "top",
    "parallelism",
    "scoop",
    "allows",
    "us",
    "import",
    "result",
    "returned",
    "sql",
    "carry",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "see",
    "import",
    "results",
    "sql",
    "query",
    "come",
    "scoop",
    "provides",
    "connectors",
    "multiple",
    "relational",
    "database",
    "management",
    "system",
    "rdbms",
    "databases",
    "mysql",
    "microsoft",
    "sql",
    "server",
    "connectors",
    "major",
    "rdbms",
    "databases",
    "scoop",
    "supports",
    "kerberos",
    "computer",
    "network",
    "authentication",
    "protocol",
    "allows",
    "nodes",
    "communicating",
    "network",
    "prove",
    "identity",
    "one",
    "another",
    "secure",
    "manner",
    "scoop",
    "load",
    "whole",
    "table",
    "parts",
    "table",
    "single",
    "command",
    "hence",
    "supports",
    "full",
    "incremental",
    "load",
    "let",
    "dig",
    "little",
    "deeper",
    "scoop",
    "architecture",
    "client",
    "case",
    "hooded",
    "wizard",
    "behind",
    "laptop",
    "never",
    "know",
    "going",
    "accessing",
    "hadoop",
    "cluster",
    "client",
    "comes",
    "sends",
    "command",
    "goes",
    "scoop",
    "client",
    "submits",
    "import",
    "export",
    "command",
    "import",
    "export",
    "data",
    "data",
    "different",
    "databases",
    "fetched",
    "scoop",
    "enterprise",
    "data",
    "warehouse",
    "document",
    "based",
    "systems",
    "connect",
    "connector",
    "data",
    "warehouse",
    "connector",
    "document",
    "based",
    "systems",
    "reaches",
    "two",
    "entities",
    "connector",
    "rdbms",
    "connectors",
    "help",
    "working",
    "range",
    "popular",
    "databases",
    "multiple",
    "mappers",
    "perform",
    "map",
    "tasks",
    "load",
    "data",
    "onto",
    "hdfs",
    "hedo",
    "file",
    "system",
    "see",
    "map",
    "task",
    "remember",
    "hadoop",
    "hadoop",
    "based",
    "map",
    "reduce",
    "reducing",
    "data",
    "mapping",
    "accesses",
    "mappers",
    "opens",
    "multiple",
    "mappers",
    "parallel",
    "processing",
    "see",
    "hdfs",
    "hbase",
    "hive",
    "target",
    "particular",
    "one",
    "similarly",
    "multiple",
    "map",
    "tests",
    "export",
    "data",
    "hdfs",
    "onto",
    "rdbms",
    "using",
    "scoop",
    "export",
    "command",
    "like",
    "import",
    "export",
    "using",
    "multiple",
    "map",
    "routines",
    "scoop",
    "import",
    "dbms",
    "data",
    "store",
    "folders",
    "maybe",
    "company",
    "database",
    "maybe",
    "archive",
    "google",
    "searches",
    "going",
    "whatever",
    "usually",
    "think",
    "scoop",
    "think",
    "sql",
    "think",
    "mysql",
    "server",
    "microsoft",
    "sql",
    "server",
    "kind",
    "setup",
    "gathers",
    "metadata",
    "see",
    "scoop",
    "import",
    "introspect",
    "database",
    "together",
    "metadata",
    "primary",
    "key",
    "information",
    "submits",
    "uh",
    "see",
    "submits",
    "map",
    "job",
    "remember",
    "talked",
    "mapreduce",
    "needs",
    "map",
    "side",
    "reducing",
    "data",
    "mapping",
    "scoop",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "uses",
    "individual",
    "map",
    "tests",
    "push",
    "splits",
    "hdfs",
    "right",
    "hadoop",
    "file",
    "system",
    "see",
    "right",
    "kind",
    "small",
    "depiction",
    "hadoop",
    "cluster",
    "scoop",
    "export",
    "going",
    "go",
    "direction",
    "direction",
    "hadoop",
    "file",
    "system",
    "storage",
    "hadoop",
    "cluster",
    "scoop",
    "job",
    "one",
    "clusters",
    "gets",
    "map",
    "mapper",
    "comes",
    "one",
    "computers",
    "data",
    "first",
    "step",
    "got",
    "gather",
    "metadata",
    "step",
    "one",
    "gather",
    "metadata",
    "step",
    "two",
    "submits",
    "map",
    "job",
    "introspect",
    "database",
    "together",
    "metadata",
    "primary",
    "key",
    "information",
    "scoop",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "uses",
    "individual",
    "map",
    "tests",
    "push",
    "splits",
    "rdbms",
    "scoop",
    "export",
    "hadoop",
    "files",
    "back",
    "rdms",
    "tables",
    "think",
    "number",
    "different",
    "manners",
    "one",
    "would",
    "restoring",
    "backup",
    "hadoop",
    "file",
    "system",
    "enterprise",
    "machines",
    "certainly",
    "many",
    "others",
    "far",
    "exploring",
    "data",
    "data",
    "science",
    "dig",
    "little",
    "deeper",
    "scoop",
    "input",
    "connect",
    "jdbc",
    "url",
    "specify",
    "jdbc",
    "connect",
    "string",
    "connecting",
    "manager",
    "specify",
    "connection",
    "manager",
    "class",
    "use",
    "see",
    "driver",
    "class",
    "name",
    "manually",
    "specify",
    "jdbc",
    "driver",
    "class",
    "use",
    "hadoop",
    "mapreduce",
    "home",
    "directory",
    "override",
    "hadoop",
    "mapped",
    "home",
    "username",
    "set",
    "authentication",
    "username",
    "course",
    "help",
    "print",
    "usage",
    "instructions",
    "export",
    "see",
    "specify",
    "jdbc",
    "connect",
    "string",
    "specify",
    "connection",
    "manager",
    "class",
    "use",
    "manually",
    "specify",
    "jdbc",
    "driver",
    "class",
    "use",
    "let",
    "know",
    "override",
    "hadoop",
    "map",
    "reduce",
    "home",
    "true",
    "set",
    "authentication",
    "username",
    "finally",
    "print",
    "help",
    "setup",
    "see",
    "format",
    "scoop",
    "pretty",
    "straightforward",
    "import",
    "export",
    "let",
    "uh",
    "continue",
    "path",
    "look",
    "scoop",
    "processing",
    "computer",
    "goes",
    "talk",
    "scoop",
    "processing",
    "first",
    "scoop",
    "runs",
    "hadoop",
    "cluster",
    "imports",
    "data",
    "rdbms",
    "nosql",
    "database",
    "hadoop",
    "file",
    "system",
    "remember",
    "might",
    "importing",
    "data",
    "rdbms",
    "might",
    "actually",
    "coming",
    "osql",
    "sql",
    "many",
    "uses",
    "mappers",
    "slice",
    "incoming",
    "data",
    "multiple",
    "formats",
    "load",
    "data",
    "hdfs",
    "exports",
    "data",
    "back",
    "rdbms",
    "making",
    "sure",
    "schema",
    "data",
    "database",
    "maintained",
    "looked",
    "basic",
    "commands",
    "scoop",
    "scoop",
    "processing",
    "least",
    "basics",
    "far",
    "theory",
    "concerned",
    "let",
    "jump",
    "take",
    "look",
    "demo",
    "scoop",
    "demo",
    "going",
    "use",
    "cloudera",
    "quick",
    "start",
    "watching",
    "demos",
    "done",
    "see",
    "using",
    "pretty",
    "consistently",
    "certainly",
    "work",
    "horton",
    "sandbox",
    "also",
    "single",
    "node",
    "testing",
    "machine",
    "cloudera",
    "one",
    "um",
    "docker",
    "version",
    "instead",
    "virtualbox",
    "also",
    "set",
    "hadoop",
    "cluster",
    "plan",
    "little",
    "extra",
    "time",
    "admin",
    "actually",
    "pretty",
    "significant",
    "endeavor",
    "admin",
    "admitting",
    "linux",
    "machines",
    "long",
    "time",
    "know",
    "lot",
    "commands",
    "find",
    "admins",
    "takes",
    "two",
    "four",
    "hours",
    "first",
    "time",
    "go",
    "create",
    "virtual",
    "machine",
    "set",
    "hadoop",
    "case",
    "though",
    "learning",
    "getting",
    "set",
    "best",
    "start",
    "cloudera",
    "cloudera",
    "also",
    "includes",
    "installed",
    "version",
    "mysql",
    "way",
    "install",
    "sql",
    "version",
    "importing",
    "data",
    "cloudera",
    "quick",
    "start",
    "see",
    "opens",
    "nice",
    "centos",
    "linux",
    "interface",
    "desktop",
    "setup",
    "really",
    "nice",
    "learning",
    "looking",
    "command",
    "lines",
    "open",
    "default",
    "hue",
    "click",
    "hue",
    "kind",
    "fun",
    "little",
    "uh",
    "interface",
    "hue",
    "go",
    "query",
    "pick",
    "editor",
    "go",
    "right",
    "scoop",
    "going",
    "load",
    "scoop",
    "editor",
    "inner",
    "hue",
    "going",
    "switch",
    "command",
    "line",
    "want",
    "show",
    "actually",
    "hue",
    "interface",
    "reason",
    "like",
    "command",
    "line",
    "specifically",
    "computer",
    "runs",
    "much",
    "quicker",
    "command",
    "line",
    "run",
    "tends",
    "extra",
    "lag",
    "added",
    "layer",
    "going",
    "go",
    "ahead",
    "open",
    "command",
    "line",
    "second",
    "reason",
    "going",
    "need",
    "go",
    "ahead",
    "edit",
    "mysql",
    "something",
    "scoop",
    "otherwise",
    "anything",
    "going",
    "course",
    "zoom",
    "zoom",
    "increase",
    "size",
    "screen",
    "demo",
    "going",
    "use",
    "oracle",
    "virtualbox",
    "manager",
    "cloudera",
    "quick",
    "start",
    "familiar",
    "another",
    "tutorial",
    "put",
    "send",
    "note",
    "youtube",
    "video",
    "let",
    "team",
    "know",
    "send",
    "link",
    "come",
    "visit",
    "creates",
    "linux",
    "box",
    "windows",
    "computer",
    "going",
    "linux",
    "cloudera",
    "version",
    "scoop",
    "also",
    "using",
    "mysql",
    "mysql",
    "server",
    "inside",
    "cloudera",
    "virtualbox",
    "go",
    "hue",
    "editor",
    "going",
    "everything",
    "terminal",
    "window",
    "want",
    "aware",
    "hue",
    "editor",
    "go",
    "query",
    "editor",
    "see",
    "come",
    "scoop",
    "run",
    "scoop",
    "little",
    "exploration",
    "sql",
    "sql",
    "server",
    "way",
    "know",
    "data",
    "coming",
    "let",
    "go",
    "ahead",
    "open",
    "terminal",
    "window",
    "cloudera",
    "terminal",
    "window",
    "top",
    "click",
    "open",
    "let",
    "go",
    "ahead",
    "zoom",
    "go",
    "view",
    "zoom",
    "get",
    "sql",
    "server",
    "simply",
    "type",
    "mysql",
    "part",
    "depend",
    "setup",
    "cloudera",
    "quickstart",
    "comes",
    "username",
    "root",
    "password",
    "cloudera",
    "kind",
    "strange",
    "quirk",
    "put",
    "space",
    "minus",
    "u",
    "root",
    "minus",
    "p",
    "cloudera",
    "usually",
    "put",
    "minus",
    "capital",
    "p",
    "prompts",
    "password",
    "demo",
    "worry",
    "much",
    "knowing",
    "password",
    "go",
    "right",
    "sql",
    "server",
    "since",
    "standard",
    "password",
    "quick",
    "start",
    "see",
    "mysql",
    "going",
    "couple",
    "quick",
    "commands",
    "show",
    "databases",
    "follow",
    "semicolon",
    "standard",
    "shell",
    "commands",
    "knows",
    "end",
    "shell",
    "command",
    "see",
    "quick",
    "start",
    "cloudera",
    "quickstart",
    "mysql",
    "comes",
    "standard",
    "set",
    "databases",
    "like",
    "uzi",
    "uzi",
    "part",
    "hadoop",
    "others",
    "like",
    "customers",
    "employees",
    "stuff",
    "like",
    "demo",
    "purposes",
    "come",
    "standard",
    "setup",
    "people",
    "going",
    "first",
    "time",
    "database",
    "play",
    "really",
    "good",
    "us",
    "recreate",
    "databases",
    "see",
    "list",
    "retail",
    "underscore",
    "db",
    "simply",
    "uh",
    "use",
    "retail",
    "underscore",
    "db",
    "set",
    "default",
    "mysql",
    "want",
    "go",
    "ahead",
    "show",
    "tables",
    "show",
    "tables",
    "see",
    "database",
    "retail",
    "db",
    "database",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "orders",
    "products",
    "number",
    "tables",
    "going",
    "go",
    "ahead",
    "use",
    "standard",
    "sql",
    "command",
    "hive",
    "language",
    "note",
    "remember",
    "hql",
    "also",
    "going",
    "select",
    "star",
    "everything",
    "departments",
    "departments",
    "table",
    "going",
    "list",
    "everything",
    "department",
    "table",
    "see",
    "got",
    "six",
    "lines",
    "department",
    "id",
    "department",
    "name",
    "two",
    "fitness",
    "three",
    "footwear",
    "forth",
    "point",
    "go",
    "ahead",
    "exit",
    "kind",
    "nice",
    "data",
    "look",
    "flip",
    "back",
    "forth",
    "screens",
    "going",
    "open",
    "another",
    "terminal",
    "window",
    "go",
    "ahead",
    "zoom",
    "also",
    "important",
    "particular",
    "setup",
    "always",
    "kind",
    "fun",
    "fun",
    "know",
    "setup",
    "working",
    "host",
    "name",
    "go",
    "ahead",
    "type",
    "linux",
    "command",
    "host",
    "name",
    "minus",
    "f",
    "see",
    "quick",
    "start",
    "cloudera",
    "surprise",
    "next",
    "command",
    "going",
    "little",
    "bit",
    "longer",
    "going",
    "first",
    "scoop",
    "command",
    "going",
    "two",
    "going",
    "list",
    "databases",
    "list",
    "tables",
    "going",
    "take",
    "moment",
    "get",
    "bunch",
    "stuff",
    "going",
    "scoop",
    "list",
    "databases",
    "connect",
    "connect",
    "command",
    "need",
    "let",
    "know",
    "connecting",
    "going",
    "use",
    "jdbc",
    "standard",
    "one",
    "jdbc",
    "mysql",
    "see",
    "sql",
    "database",
    "started",
    "next",
    "part",
    "go",
    "look",
    "however",
    "created",
    "admin",
    "created",
    "mysql",
    "server",
    "certain",
    "setup",
    "go",
    "see",
    "usually",
    "list",
    "localhost",
    "see",
    "something",
    "like",
    "localhost",
    "sometimes",
    "lot",
    "different",
    "formats",
    "common",
    "either",
    "localhost",
    "actual",
    "connection",
    "case",
    "want",
    "go",
    "ahead",
    "quickstart",
    "quick",
    "start",
    "use",
    "name",
    "local",
    "host",
    "database",
    "hosted",
    "set",
    "quick",
    "start",
    "um",
    "hadoop",
    "cloudera",
    "port",
    "3306",
    "coming",
    "coming",
    "path",
    "put",
    "password",
    "typically",
    "type",
    "password",
    "look",
    "password",
    "cloudera",
    "quick",
    "start",
    "cloudera",
    "also",
    "let",
    "know",
    "username",
    "probably",
    "put",
    "minus",
    "capital",
    "actually",
    "prompt",
    "password",
    "leave",
    "prompt",
    "really",
    "matter",
    "care",
    "see",
    "password",
    "default",
    "one",
    "cloudera",
    "quickstart",
    "username",
    "simply",
    "root",
    "going",
    "put",
    "semicolon",
    "end",
    "full",
    "setup",
    "go",
    "ahead",
    "list",
    "databases",
    "see",
    "might",
    "get",
    "warnings",
    "run",
    "updates",
    "quick",
    "start",
    "suggest",
    "running",
    "updates",
    "either",
    "first",
    "time",
    "reformatting",
    "quickly",
    "pass",
    "see",
    "tables",
    "went",
    "go",
    "back",
    "previous",
    "window",
    "see",
    "tables",
    "match",
    "come",
    "databases",
    "see",
    "back",
    "cm",
    "customers",
    "employees",
    "databases",
    "match",
    "want",
    "go",
    "ahead",
    "list",
    "tables",
    "specific",
    "database",
    "let",
    "go",
    "ahead",
    "lazy",
    "typist",
    "put",
    "arrow",
    "see",
    "scoop",
    "list",
    "databases",
    "going",
    "go",
    "back",
    "change",
    "databases",
    "list",
    "tables",
    "want",
    "list",
    "tables",
    "connection",
    "connection",
    "except",
    "need",
    "know",
    "tables",
    "listing",
    "interesting",
    "fact",
    "create",
    "table",
    "without",
    "database",
    "left",
    "blank",
    "show",
    "open",
    "tables",
    "connected",
    "directly",
    "database",
    "database",
    "want",
    "right",
    "past",
    "last",
    "slash",
    "3306",
    "want",
    "put",
    "retail",
    "underscore",
    "db",
    "database",
    "going",
    "working",
    "go",
    "show",
    "tables",
    "listed",
    "database",
    "go",
    "got",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "products",
    "flip",
    "back",
    "real",
    "quick",
    "thing",
    "categories",
    "customers",
    "departments",
    "order",
    "items",
    "let",
    "go",
    "ahead",
    "run",
    "first",
    "import",
    "command",
    "lazy",
    "typer",
    "going",
    "scoop",
    "instead",
    "list",
    "tables",
    "want",
    "go",
    "ahead",
    "import",
    "import",
    "command",
    "import",
    "command",
    "need",
    "tell",
    "exactly",
    "going",
    "import",
    "everything",
    "else",
    "importing",
    "retail",
    "db",
    "keep",
    "end",
    "going",
    "tag",
    "dash",
    "dash",
    "table",
    "tells",
    "us",
    "tell",
    "table",
    "importing",
    "going",
    "import",
    "departments",
    "go",
    "pretty",
    "straightforward",
    "nice",
    "see",
    "commands",
    "got",
    "connection",
    "um",
    "change",
    "whatever",
    "database",
    "come",
    "password",
    "username",
    "going",
    "mysql",
    "server",
    "setup",
    "let",
    "know",
    "table",
    "entering",
    "run",
    "going",
    "actually",
    "go",
    "mapper",
    "process",
    "hadoop",
    "mapping",
    "process",
    "takes",
    "data",
    "maps",
    "different",
    "parts",
    "setup",
    "hadoop",
    "saves",
    "data",
    "hadoop",
    "file",
    "system",
    "take",
    "moment",
    "zip",
    "kind",
    "skipped",
    "since",
    "running",
    "know",
    "designed",
    "run",
    "across",
    "cluster",
    "single",
    "node",
    "running",
    "single",
    "node",
    "going",
    "run",
    "slow",
    "even",
    "dedicate",
    "couple",
    "cores",
    "think",
    "put",
    "dedicated",
    "four",
    "cores",
    "one",
    "uh",
    "see",
    "right",
    "get",
    "end",
    "mapped",
    "information",
    "go",
    "go",
    "flip",
    "back",
    "hue",
    "hue",
    "top",
    "databases",
    "second",
    "icon",
    "hadoop",
    "file",
    "system",
    "go",
    "look",
    "hadoop",
    "file",
    "system",
    "see",
    "show",
    "underneath",
    "documents",
    "departments",
    "cloudera",
    "departments",
    "see",
    "always",
    "delay",
    "working",
    "hue",
    "like",
    "quick",
    "start",
    "issue",
    "necessarily",
    "running",
    "server",
    "running",
    "server",
    "pretty",
    "much",
    "run",
    "kind",
    "server",
    "interface",
    "still",
    "prefer",
    "terminal",
    "window",
    "still",
    "runs",
    "lot",
    "quicker",
    "flip",
    "back",
    "command",
    "line",
    "hadoop",
    "type",
    "hadoop",
    "fs",
    "list",
    "minus",
    "ls",
    "run",
    "see",
    "underneath",
    "hadoop",
    "file",
    "system",
    "departments",
    "added",
    "also",
    "uh",
    "hadoop",
    "fs",
    "kind",
    "interesting",
    "gone",
    "hadoop",
    "file",
    "system",
    "everything",
    "recognize",
    "going",
    "list",
    "contents",
    "departments",
    "see",
    "underneath",
    "departments",
    "uh",
    "part",
    "part",
    "m0001002003",
    "interesting",
    "hadoop",
    "saves",
    "files",
    "file",
    "system",
    "hive",
    "directly",
    "import",
    "hive",
    "put",
    "hadoop",
    "file",
    "system",
    "depending",
    "would",
    "write",
    "schema",
    "hive",
    "look",
    "hadoop",
    "file",
    "system",
    "certainly",
    "visit",
    "hive",
    "tutorial",
    "information",
    "hive",
    "specific",
    "see",
    "different",
    "files",
    "forms",
    "part",
    "departments",
    "something",
    "like",
    "look",
    "contents",
    "one",
    "files",
    "fs",
    "minus",
    "ls",
    "number",
    "files",
    "simply",
    "full",
    "path",
    "user",
    "cloudera",
    "already",
    "know",
    "next",
    "one",
    "departments",
    "departments",
    "going",
    "put",
    "slash",
    "part",
    "star",
    "going",
    "say",
    "anything",
    "part",
    "part",
    "dash",
    "0",
    "0",
    "0",
    "go",
    "ahead",
    "cat",
    "use",
    "cut",
    "command",
    "list",
    "command",
    "bring",
    "use",
    "cat",
    "command",
    "actually",
    "display",
    "contents",
    "linux",
    "command",
    "hadoop",
    "linux",
    "command",
    "cat",
    "captinate",
    "confused",
    "catatonic",
    "catastrophic",
    "lot",
    "cat",
    "got",
    "tongue",
    "see",
    "fitness",
    "footwear",
    "apparel",
    "look",
    "really",
    "familiar",
    "mysql",
    "server",
    "went",
    "select",
    "fitness",
    "footwear",
    "apparel",
    "golf",
    "outdoors",
    "fan",
    "shop",
    "course",
    "really",
    "important",
    "let",
    "look",
    "back",
    "able",
    "tell",
    "put",
    "data",
    "go",
    "back",
    "import",
    "command",
    "scoop",
    "import",
    "connect",
    "db",
    "underneath",
    "connection",
    "sql",
    "server",
    "password",
    "username",
    "table",
    "going",
    "going",
    "mean",
    "table",
    "coming",
    "uh",
    "add",
    "target",
    "put",
    "uh",
    "target",
    "dash",
    "directory",
    "put",
    "full",
    "path",
    "hadoop",
    "thing",
    "good",
    "practice",
    "gon",
    "na",
    "add",
    "department",
    "uh",
    "department",
    "one",
    "add",
    "target",
    "directory",
    "user",
    "cloudera",
    "department",
    "one",
    "take",
    "moment",
    "go",
    "ahead",
    "skip",
    "process",
    "since",
    "going",
    "run",
    "slowly",
    "running",
    "like",
    "said",
    "couple",
    "cores",
    "also",
    "single",
    "node",
    "hadoop",
    "let",
    "arrow",
    "file",
    "system",
    "list",
    "want",
    "straight",
    "list",
    "hadoop",
    "file",
    "system",
    "minus",
    "ls",
    "list",
    "see",
    "department",
    "one",
    "course",
    "list",
    "department",
    "one",
    "see",
    "files",
    "inside",
    "department",
    "one",
    "mirrored",
    "saw",
    "files",
    "part",
    "zero",
    "zero",
    "want",
    "look",
    "thing",
    "cat",
    "except",
    "instead",
    "departments",
    "department",
    "one",
    "go",
    "something",
    "going",
    "come",
    "data",
    "one",
    "important",
    "things",
    "importing",
    "data",
    "always",
    "question",
    "ask",
    "filter",
    "data",
    "comes",
    "want",
    "filter",
    "data",
    "comes",
    "storing",
    "everything",
    "file",
    "system",
    "would",
    "think",
    "hadoop",
    "big",
    "data",
    "put",
    "know",
    "experience",
    "putting",
    "turn",
    "couple",
    "hundred",
    "terabytes",
    "petabyte",
    "rapidly",
    "suddenly",
    "really",
    "add",
    "data",
    "store",
    "storing",
    "duplicate",
    "data",
    "sometimes",
    "really",
    "need",
    "able",
    "filter",
    "data",
    "let",
    "go",
    "ahead",
    "use",
    "arrow",
    "go",
    "last",
    "import",
    "since",
    "still",
    "lot",
    "stuff",
    "commands",
    "import",
    "target",
    "going",
    "change",
    "department",
    "2",
    "going",
    "create",
    "new",
    "directory",
    "one",
    "departments",
    "another",
    "command",
    "really",
    "slide",
    "mapping",
    "show",
    "looks",
    "like",
    "minute",
    "going",
    "put",
    "m3",
    "nothing",
    "filtering",
    "show",
    "second",
    "though",
    "want",
    "put",
    "uh",
    "case",
    "want",
    "know",
    "department",
    "id",
    "want",
    "know",
    "came",
    "flip",
    "back",
    "department",
    "underscore",
    "ids",
    "coming",
    "name",
    "column",
    "come",
    "department",
    "id",
    "greater",
    "four",
    "simple",
    "um",
    "logic",
    "see",
    "use",
    "maybe",
    "creating",
    "buckets",
    "ages",
    "uh",
    "know",
    "age",
    "10",
    "15",
    "20",
    "might",
    "looking",
    "mean",
    "kinds",
    "reasons",
    "could",
    "use",
    "command",
    "filter",
    "information",
    "maybe",
    "word",
    "counting",
    "want",
    "know",
    "words",
    "used",
    "less",
    "100",
    "times",
    "want",
    "get",
    "rid",
    "stuff",
    "used",
    "uh",
    "go",
    "ahead",
    "put",
    "department",
    "id",
    "greater",
    "four",
    "go",
    "ahead",
    "hit",
    "enter",
    "create",
    "department",
    "two",
    "set",
    "uh",
    "go",
    "ahead",
    "skip",
    "runtime",
    "runs",
    "really",
    "slow",
    "single",
    "node",
    "real",
    "quick",
    "page",
    "commands",
    "let",
    "see",
    "go",
    "list",
    "see",
    "underneath",
    "list",
    "department",
    "2",
    "department",
    "2",
    "go",
    "ahead",
    "list",
    "department",
    "two",
    "see",
    "contents",
    "see",
    "three",
    "maps",
    "could",
    "data",
    "created",
    "three",
    "maps",
    "remember",
    "set",
    "use",
    "three",
    "mappers",
    "uh",
    "zero",
    "one",
    "two",
    "go",
    "ahead",
    "cat",
    "remember",
    "department",
    "two",
    "want",
    "look",
    "contents",
    "three",
    "different",
    "files",
    "greater",
    "four",
    "golf",
    "five",
    "outdoor",
    "six",
    "uh",
    "fan",
    "shop",
    "seven",
    "effectively",
    "filtered",
    "data",
    "storing",
    "data",
    "want",
    "file",
    "system",
    "going",
    "store",
    "data",
    "next",
    "stage",
    "export",
    "data",
    "remember",
    "lot",
    "times",
    "sql",
    "server",
    "continually",
    "dumping",
    "data",
    "long",
    "term",
    "storage",
    "access",
    "hadoop",
    "file",
    "system",
    "happens",
    "need",
    "pull",
    "data",
    "restore",
    "database",
    "maybe",
    "merged",
    "new",
    "company",
    "favorite",
    "topic",
    "merging",
    "companies",
    "merging",
    "databases",
    "listed",
    "nightmare",
    "many",
    "different",
    "names",
    "company",
    "see",
    "able",
    "export",
    "also",
    "equally",
    "important",
    "let",
    "go",
    "ahead",
    "going",
    "flip",
    "back",
    "sql",
    "server",
    "need",
    "go",
    "ahead",
    "create",
    "database",
    "going",
    "export",
    "going",
    "go",
    "much",
    "detail",
    "command",
    "simply",
    "creating",
    "table",
    "table",
    "going",
    "pretty",
    "much",
    "table",
    "already",
    "departments",
    "case",
    "going",
    "create",
    "table",
    "called",
    "dept",
    "setup",
    "going",
    "giving",
    "different",
    "name",
    "different",
    "schema",
    "done",
    "go",
    "ahead",
    "select",
    "star",
    "e",
    "p",
    "go",
    "empty",
    "expect",
    "new",
    "database",
    "new",
    "data",
    "table",
    "empty",
    "uh",
    "need",
    "go",
    "ahead",
    "export",
    "data",
    "filtered",
    "let",
    "flip",
    "back",
    "scoop",
    "setup",
    "linux",
    "terminal",
    "window",
    "let",
    "go",
    "back",
    "one",
    "commands",
    "scoop",
    "import",
    "case",
    "instead",
    "import",
    "going",
    "take",
    "scoop",
    "going",
    "export",
    "going",
    "change",
    "export",
    "connection",
    "going",
    "remain",
    "connect",
    "database",
    "also",
    "still",
    "retail",
    "db",
    "uh",
    "password",
    "none",
    "changes",
    "uh",
    "big",
    "change",
    "going",
    "table",
    "instead",
    "departments",
    "uh",
    "remember",
    "changed",
    "gave",
    "new",
    "name",
    "want",
    "change",
    "also",
    "department",
    "going",
    "worry",
    "mapper",
    "count",
    "part",
    "import",
    "go",
    "finally",
    "needs",
    "know",
    "export",
    "instead",
    "target",
    "directory",
    "export",
    "directory",
    "coming",
    "uh",
    "still",
    "user",
    "cloudera",
    "keep",
    "department",
    "see",
    "data",
    "coming",
    "back",
    "filtered",
    "let",
    "go",
    "ahead",
    "run",
    "take",
    "moment",
    "go",
    "steps",
    "low",
    "going",
    "go",
    "skip",
    "sit",
    "wrapped",
    "export",
    "flip",
    "back",
    "mysql",
    "use",
    "arrow",
    "time",
    "going",
    "select",
    "star",
    "department",
    "see",
    "exported",
    "golf",
    "outdoors",
    "fan",
    "shop",
    "imagine",
    "also",
    "might",
    "use",
    "command",
    "export",
    "also",
    "lot",
    "mixing",
    "command",
    "line",
    "scoop",
    "pretty",
    "straightforward",
    "changing",
    "different",
    "variables",
    "whether",
    "creating",
    "table",
    "listing",
    "table",
    "listing",
    "databases",
    "powerful",
    "tool",
    "bringing",
    "data",
    "hadoop",
    "file",
    "system",
    "exporting",
    "wrapped",
    "demo",
    "scoop",
    "gone",
    "lot",
    "basic",
    "commands",
    "let",
    "dive",
    "brief",
    "history",
    "hive",
    "history",
    "hive",
    "begins",
    "facebook",
    "facebook",
    "began",
    "using",
    "hadoop",
    "solution",
    "handle",
    "growing",
    "big",
    "data",
    "talking",
    "data",
    "fits",
    "one",
    "two",
    "even",
    "five",
    "computers",
    "talking",
    "due",
    "fits",
    "looked",
    "hadoop",
    "tutorials",
    "know",
    "talking",
    "big",
    "data",
    "data",
    "pools",
    "facebook",
    "certainly",
    "lot",
    "data",
    "tracks",
    "know",
    "hadoop",
    "uses",
    "mapreduce",
    "processing",
    "data",
    "mapreduce",
    "required",
    "users",
    "write",
    "long",
    "codes",
    "really",
    "extensive",
    "java",
    "codes",
    "complicated",
    "average",
    "person",
    "use",
    "users",
    "versed",
    "java",
    "coding",
    "languages",
    "proved",
    "disadvantage",
    "users",
    "comfortable",
    "writing",
    "queries",
    "sql",
    "sql",
    "around",
    "long",
    "time",
    "standard",
    "sql",
    "query",
    "language",
    "hive",
    "developed",
    "vision",
    "incorporate",
    "concepts",
    "tables",
    "columns",
    "like",
    "sql",
    "hive",
    "well",
    "problem",
    "processing",
    "analyzing",
    "data",
    "users",
    "found",
    "difficult",
    "code",
    "coding",
    "languages",
    "processing",
    "ever",
    "analyzing",
    "solution",
    "required",
    "language",
    "similar",
    "sql",
    "well",
    "known",
    "users",
    "thus",
    "hive",
    "hql",
    "language",
    "evolved",
    "hive",
    "hive",
    "data",
    "warehouse",
    "system",
    "used",
    "querying",
    "analyzing",
    "large",
    "data",
    "sets",
    "stored",
    "hdfs",
    "hadoop",
    "file",
    "system",
    "hive",
    "uses",
    "query",
    "language",
    "call",
    "hive",
    "ql",
    "hql",
    "similar",
    "sql",
    "take",
    "user",
    "user",
    "sends",
    "hive",
    "queries",
    "converted",
    "map",
    "reduce",
    "tasks",
    "accesses",
    "hadoop",
    "mapreduce",
    "system",
    "let",
    "take",
    "look",
    "architecture",
    "hive",
    "architecture",
    "hive",
    "hive",
    "client",
    "could",
    "programmer",
    "maybe",
    "manager",
    "knows",
    "enough",
    "sql",
    "basic",
    "query",
    "look",
    "data",
    "need",
    "hive",
    "client",
    "supports",
    "different",
    "types",
    "client",
    "applications",
    "different",
    "languages",
    "prefer",
    "performing",
    "queries",
    "thrift",
    "application",
    "hive",
    "thrift",
    "client",
    "thrift",
    "software",
    "framework",
    "hive",
    "server",
    "based",
    "thrift",
    "serve",
    "request",
    "programming",
    "language",
    "support",
    "thrift",
    "jdbc",
    "application",
    "hive",
    "jdbc",
    "driver",
    "jdbc",
    "java",
    "database",
    "connectivity",
    "jdbc",
    "application",
    "connected",
    "jdbc",
    "driver",
    "odbc",
    "application",
    "hive",
    "odbc",
    "driver",
    "odbc",
    "open",
    "database",
    "connectivity",
    "odbc",
    "application",
    "connected",
    "odbc",
    "driver",
    "growing",
    "development",
    "different",
    "scripting",
    "languages",
    "python",
    "c",
    "plus",
    "plus",
    "spar",
    "java",
    "find",
    "connection",
    "main",
    "scripting",
    "languages",
    "hive",
    "services",
    "look",
    "deeper",
    "architecture",
    "hive",
    "supports",
    "various",
    "services",
    "hive",
    "server",
    "basically",
    "thrift",
    "application",
    "hive",
    "thrift",
    "client",
    "jdbc",
    "hive",
    "jdbc",
    "driver",
    "odbc",
    "application",
    "hive",
    "odbc",
    "driver",
    "connect",
    "hive",
    "server",
    "hive",
    "web",
    "interface",
    "also",
    "cli",
    "hive",
    "web",
    "interface",
    "gui",
    "provided",
    "execute",
    "hive",
    "queries",
    "actually",
    "using",
    "later",
    "today",
    "see",
    "kind",
    "looks",
    "like",
    "get",
    "feel",
    "means",
    "commands",
    "executed",
    "directly",
    "cli",
    "cli",
    "direct",
    "terminal",
    "window",
    "also",
    "show",
    "see",
    "two",
    "different",
    "interfaces",
    "work",
    "push",
    "code",
    "hive",
    "driver",
    "hive",
    "driver",
    "responsible",
    "queries",
    "submitted",
    "everything",
    "goes",
    "driver",
    "let",
    "take",
    "closer",
    "look",
    "hive",
    "driver",
    "hive",
    "driver",
    "performs",
    "three",
    "steps",
    "internally",
    "one",
    "compiler",
    "hive",
    "driver",
    "passes",
    "query",
    "compiler",
    "checked",
    "analyzed",
    "optimizer",
    "kicks",
    "optimize",
    "logical",
    "plan",
    "form",
    "graph",
    "mapreduce",
    "hdfs",
    "tasks",
    "obtained",
    "finally",
    "executor",
    "final",
    "step",
    "tasks",
    "executed",
    "look",
    "architecture",
    "also",
    "note",
    "meta",
    "store",
    "metastore",
    "repository",
    "hive",
    "metadata",
    "stores",
    "metadata",
    "hive",
    "tables",
    "think",
    "schema",
    "located",
    "stored",
    "apache",
    "derby",
    "db",
    "processing",
    "resource",
    "management",
    "handled",
    "mapreduce",
    "v1",
    "see",
    "mapreduce",
    "v2",
    "yarn",
    "tez",
    "different",
    "ways",
    "managing",
    "resources",
    "depending",
    "version",
    "hadoop",
    "hive",
    "uses",
    "mapreduce",
    "framework",
    "process",
    "queries",
    "distributed",
    "storage",
    "hdfs",
    "looked",
    "hadoop",
    "tutorials",
    "know",
    "commodity",
    "machines",
    "linearly",
    "scalable",
    "means",
    "affordable",
    "lot",
    "time",
    "talking",
    "big",
    "data",
    "talking",
    "tenth",
    "price",
    "storing",
    "enterprise",
    "computers",
    "look",
    "data",
    "flow",
    "hive",
    "data",
    "flow",
    "hive",
    "hive",
    "hadoop",
    "system",
    "underneath",
    "user",
    "interface",
    "ui",
    "driver",
    "compiler",
    "execution",
    "engine",
    "meta",
    "store",
    "goes",
    "mapreduce",
    "hadoop",
    "file",
    "system",
    "execute",
    "query",
    "see",
    "coming",
    "goes",
    "driver",
    "step",
    "one",
    "step",
    "two",
    "get",
    "plan",
    "going",
    "refers",
    "query",
    "execution",
    "uh",
    "go",
    "metadata",
    "like",
    "well",
    "kind",
    "metadata",
    "actually",
    "looking",
    "data",
    "located",
    "schema",
    "comes",
    "back",
    "metadata",
    "compiler",
    "compiler",
    "takes",
    "information",
    "syn",
    "plan",
    "returns",
    "driver",
    "driver",
    "sends",
    "execute",
    "plan",
    "execution",
    "engine",
    "execution",
    "engine",
    "execution",
    "engine",
    "acts",
    "bridge",
    "hive",
    "hadoop",
    "process",
    "query",
    "going",
    "mapreduce",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "come",
    "back",
    "metadata",
    "operations",
    "goes",
    "back",
    "metastore",
    "update",
    "let",
    "know",
    "going",
    "also",
    "goes",
    "communication",
    "execution",
    "engine",
    "meta",
    "store",
    "execution",
    "engine",
    "communications",
    "metastore",
    "perform",
    "operations",
    "like",
    "create",
    "drop",
    "tables",
    "metastore",
    "stores",
    "information",
    "tables",
    "columns",
    "talking",
    "schema",
    "database",
    "send",
    "results",
    "communication",
    "back",
    "driver",
    "fetch",
    "results",
    "goes",
    "back",
    "client",
    "let",
    "take",
    "little",
    "bit",
    "look",
    "hive",
    "data",
    "modeling",
    "hive",
    "data",
    "modeling",
    "high",
    "data",
    "modeling",
    "tables",
    "partitions",
    "buckets",
    "tables",
    "hive",
    "created",
    "way",
    "done",
    "rdbms",
    "looking",
    "traditional",
    "sql",
    "server",
    "mysql",
    "server",
    "might",
    "enterprise",
    "equipment",
    "lot",
    "people",
    "pulling",
    "removing",
    "stuff",
    "tables",
    "going",
    "look",
    "similar",
    "makes",
    "easy",
    "take",
    "information",
    "let",
    "say",
    "need",
    "keep",
    "current",
    "information",
    "need",
    "store",
    "years",
    "transactions",
    "back",
    "hadoop",
    "hive",
    "match",
    "kind",
    "look",
    "tables",
    "databases",
    "look",
    "similar",
    "easily",
    "import",
    "back",
    "easily",
    "store",
    "hive",
    "system",
    "partitions",
    "tables",
    "organized",
    "partitions",
    "grouping",
    "type",
    "data",
    "based",
    "partition",
    "key",
    "become",
    "important",
    "speeding",
    "process",
    "queries",
    "looking",
    "dates",
    "far",
    "like",
    "employment",
    "dates",
    "employees",
    "tracking",
    "might",
    "add",
    "partition",
    "might",
    "one",
    "key",
    "things",
    "always",
    "looking",
    "far",
    "employees",
    "concerned",
    "finally",
    "buckets",
    "uh",
    "data",
    "present",
    "partitions",
    "divided",
    "buckets",
    "efficient",
    "querying",
    "efficiency",
    "level",
    "lot",
    "times",
    "taught",
    "working",
    "programmer",
    "admin",
    "hadoop",
    "file",
    "system",
    "maximize",
    "efficiency",
    "file",
    "system",
    "usually",
    "job",
    "talking",
    "hive",
    "data",
    "modeling",
    "want",
    "make",
    "sure",
    "work",
    "together",
    "maximizing",
    "resources",
    "hive",
    "data",
    "types",
    "talking",
    "hive",
    "data",
    "types",
    "primitive",
    "data",
    "types",
    "complex",
    "data",
    "types",
    "lot",
    "look",
    "familiar",
    "mirrors",
    "lot",
    "stuff",
    "sql",
    "primitive",
    "data",
    "types",
    "numerical",
    "data",
    "types",
    "string",
    "data",
    "type",
    "date",
    "time",
    "data",
    "type",
    "miscellaneous",
    "data",
    "type",
    "kind",
    "case",
    "numerical",
    "data",
    "floats",
    "integers",
    "short",
    "integers",
    "numerical",
    "data",
    "comes",
    "number",
    "string",
    "course",
    "characters",
    "numbers",
    "date",
    "time",
    "stamp",
    "kind",
    "general",
    "way",
    "pulling",
    "created",
    "data",
    "types",
    "miscellaneous",
    "data",
    "type",
    "complex",
    "data",
    "types",
    "store",
    "arrays",
    "store",
    "maps",
    "store",
    "structures",
    "even",
    "units",
    "dig",
    "hive",
    "data",
    "types",
    "primitive",
    "data",
    "types",
    "complex",
    "data",
    "types",
    "look",
    "primitive",
    "data",
    "types",
    "looking",
    "numeric",
    "data",
    "types",
    "data",
    "types",
    "like",
    "integer",
    "float",
    "decimal",
    "stored",
    "numbers",
    "hive",
    "data",
    "system",
    "string",
    "data",
    "type",
    "data",
    "types",
    "like",
    "characters",
    "strings",
    "store",
    "name",
    "person",
    "working",
    "uh",
    "know",
    "john",
    "doe",
    "city",
    "memphis",
    "state",
    "tennessee",
    "maybe",
    "boulder",
    "colorado",
    "usa",
    "maybe",
    "hyper",
    "bad",
    "india",
    "going",
    "string",
    "stored",
    "string",
    "character",
    "course",
    "date",
    "time",
    "data",
    "type",
    "data",
    "types",
    "like",
    "timestamp",
    "date",
    "interval",
    "common",
    "far",
    "tracking",
    "sales",
    "anything",
    "like",
    "think",
    "type",
    "stamp",
    "time",
    "maybe",
    "dealing",
    "race",
    "want",
    "know",
    "interval",
    "long",
    "person",
    "take",
    "complete",
    "whatever",
    "task",
    "date",
    "time",
    "data",
    "type",
    "talk",
    "miscellaneous",
    "data",
    "type",
    "like",
    "boolean",
    "binary",
    "get",
    "boolean",
    "binary",
    "actually",
    "almost",
    "create",
    "anything",
    "yes",
    "knows",
    "zero",
    "one",
    "let",
    "take",
    "look",
    "complex",
    "data",
    "types",
    "little",
    "closer",
    "uh",
    "arrays",
    "syntax",
    "data",
    "type",
    "array",
    "think",
    "array",
    "collection",
    "entities",
    "one",
    "two",
    "three",
    "four",
    "numbers",
    "maps",
    "collection",
    "key",
    "value",
    "pairs",
    "understanding",
    "maps",
    "central",
    "hadoop",
    "store",
    "maps",
    "key",
    "set",
    "one",
    "key",
    "per",
    "mapped",
    "value",
    "hadoop",
    "course",
    "collect",
    "uh",
    "keys",
    "add",
    "something",
    "contents",
    "key",
    "map",
    "primitive",
    "type",
    "data",
    "type",
    "collection",
    "key",
    "value",
    "pairs",
    "collection",
    "complex",
    "data",
    "comment",
    "structure",
    "column",
    "name",
    "data",
    "type",
    "comment",
    "call",
    "column",
    "comment",
    "get",
    "complicated",
    "structures",
    "collection",
    "data",
    "commented",
    "setup",
    "units",
    "collection",
    "heterogeneous",
    "data",
    "types",
    "syntax",
    "union",
    "type",
    "data",
    "type",
    "data",
    "type",
    "going",
    "little",
    "bit",
    "different",
    "arrays",
    "actually",
    "mix",
    "match",
    "different",
    "modes",
    "hive",
    "hive",
    "operates",
    "two",
    "modes",
    "depending",
    "number",
    "size",
    "data",
    "nodes",
    "local",
    "mode",
    "map",
    "reduce",
    "mode",
    "talk",
    "local",
    "mode",
    "used",
    "hadoop",
    "one",
    "data",
    "node",
    "data",
    "small",
    "processing",
    "fast",
    "smaller",
    "data",
    "sets",
    "present",
    "local",
    "machine",
    "might",
    "local",
    "file",
    "stuff",
    "uploading",
    "hive",
    "need",
    "processes",
    "go",
    "ahead",
    "run",
    "high",
    "processes",
    "queries",
    "usually",
    "see",
    "much",
    "way",
    "single",
    "node",
    "hadoop",
    "system",
    "going",
    "might",
    "well",
    "use",
    "like",
    "sql",
    "database",
    "even",
    "java",
    "sqlite",
    "something",
    "python",
    "sqlite",
    "really",
    "see",
    "lot",
    "single",
    "node",
    "hadoop",
    "databases",
    "see",
    "local",
    "mode",
    "hive",
    "working",
    "small",
    "amount",
    "data",
    "going",
    "integrated",
    "larger",
    "database",
    "map",
    "reduce",
    "mode",
    "used",
    "hadoop",
    "multiple",
    "data",
    "nodes",
    "data",
    "spread",
    "across",
    "various",
    "data",
    "nodes",
    "processing",
    "large",
    "data",
    "sets",
    "efficient",
    "using",
    "mode",
    "think",
    "instead",
    "one",
    "two",
    "three",
    "even",
    "five",
    "computers",
    "usually",
    "talking",
    "hadoop",
    "file",
    "system",
    "looking",
    "10",
    "computers",
    "15",
    "100",
    "data",
    "spread",
    "across",
    "different",
    "hadoop",
    "nodes",
    "difference",
    "hive",
    "rdbms",
    "remember",
    "rdbms",
    "stands",
    "relational",
    "database",
    "management",
    "system",
    "let",
    "take",
    "look",
    "difference",
    "hive",
    "rdbms",
    "hive",
    "hive",
    "enforces",
    "schema",
    "read",
    "important",
    "whatever",
    "coming",
    "hive",
    "looking",
    "making",
    "sure",
    "fits",
    "model",
    "rdbms",
    "enforces",
    "schema",
    "actually",
    "writes",
    "data",
    "database",
    "read",
    "data",
    "starts",
    "write",
    "going",
    "give",
    "error",
    "tell",
    "something",
    "incorrect",
    "scheme",
    "hive",
    "data",
    "size",
    "petabytes",
    "hard",
    "imagine",
    "um",
    "know",
    "looking",
    "personal",
    "computer",
    "desk",
    "maybe",
    "10",
    "terabytes",
    "computer",
    "talking",
    "petabytes",
    "hundreds",
    "computers",
    "grouped",
    "together",
    "rdbms",
    "data",
    "size",
    "terabytes",
    "rarely",
    "see",
    "rdbms",
    "system",
    "spread",
    "five",
    "computers",
    "lot",
    "reasons",
    "rdbms",
    "actually",
    "high",
    "end",
    "amount",
    "writes",
    "hard",
    "drive",
    "lot",
    "going",
    "writing",
    "pulling",
    "stuff",
    "really",
    "want",
    "get",
    "big",
    "rd",
    "bms",
    "going",
    "run",
    "lot",
    "problems",
    "hive",
    "take",
    "big",
    "want",
    "hive",
    "based",
    "notion",
    "write",
    "read",
    "many",
    "times",
    "important",
    "call",
    "worm",
    "write",
    "w",
    "wants",
    "read",
    "many",
    "times",
    "refer",
    "worm",
    "true",
    "lot",
    "hadoop",
    "setup",
    "altered",
    "little",
    "bit",
    "general",
    "looking",
    "archiving",
    "data",
    "want",
    "data",
    "analysis",
    "looking",
    "pulling",
    "stuff",
    "rd",
    "bms",
    "years",
    "years",
    "years",
    "business",
    "whatever",
    "company",
    "scientific",
    "research",
    "putting",
    "huge",
    "data",
    "pool",
    "queries",
    "get",
    "information",
    "rdbms",
    "based",
    "notion",
    "read",
    "write",
    "many",
    "times",
    "continually",
    "updating",
    "database",
    "continually",
    "bringing",
    "new",
    "stuff",
    "new",
    "sales",
    "account",
    "changes",
    "different",
    "licensing",
    "whatever",
    "software",
    "selling",
    "kind",
    "stuff",
    "data",
    "continually",
    "fluctuating",
    "hive",
    "resembles",
    "traditional",
    "database",
    "supporting",
    "sql",
    "database",
    "data",
    "warehouse",
    "important",
    "goes",
    "stuff",
    "talked",
    "looking",
    "database",
    "data",
    "warehouse",
    "store",
    "data",
    "still",
    "fast",
    "easy",
    "access",
    "queries",
    "think",
    "twitter",
    "facebook",
    "many",
    "posts",
    "archived",
    "back",
    "historically",
    "posts",
    "going",
    "change",
    "made",
    "post",
    "posted",
    "database",
    "store",
    "warehouse",
    "case",
    "want",
    "pull",
    "back",
    "rdbms",
    "type",
    "database",
    "management",
    "system",
    "based",
    "relational",
    "model",
    "data",
    "hive",
    "easily",
    "scalable",
    "low",
    "cost",
    "talking",
    "maybe",
    "thousand",
    "dollars",
    "per",
    "terabyte",
    "um",
    "rdbms",
    "scalable",
    "low",
    "cost",
    "first",
    "start",
    "lower",
    "end",
    "talking",
    "10",
    "000",
    "per",
    "terabyte",
    "data",
    "including",
    "backup",
    "models",
    "added",
    "necessities",
    "support",
    "scale",
    "scale",
    "computers",
    "hardware",
    "might",
    "start",
    "basic",
    "server",
    "upgrade",
    "sun",
    "computer",
    "run",
    "spend",
    "know",
    "tens",
    "thousands",
    "dollars",
    "hardware",
    "upgrade",
    "hive",
    "put",
    "another",
    "computer",
    "hadoop",
    "file",
    "system",
    "let",
    "look",
    "features",
    "hive",
    "looking",
    "features",
    "hive",
    "talking",
    "use",
    "sql",
    "like",
    "language",
    "called",
    "hive",
    "ql",
    "lot",
    "times",
    "see",
    "hql",
    "easier",
    "long",
    "codes",
    "nice",
    "working",
    "shareholders",
    "come",
    "say",
    "hey",
    "basic",
    "sql",
    "query",
    "pull",
    "information",
    "need",
    "way",
    "take",
    "programmers",
    "jump",
    "every",
    "time",
    "want",
    "look",
    "something",
    "database",
    "actually",
    "easily",
    "skilled",
    "programming",
    "script",
    "writing",
    "tables",
    "used",
    "similar",
    "rdbms",
    "hence",
    "easier",
    "understand",
    "one",
    "things",
    "like",
    "bringing",
    "tables",
    "mysql",
    "server",
    "sql",
    "server",
    "almost",
    "direct",
    "reflection",
    "two",
    "looking",
    "one",
    "data",
    "continually",
    "changing",
    "going",
    "archive",
    "database",
    "huge",
    "jump",
    "learn",
    "whole",
    "new",
    "language",
    "mirror",
    "schema",
    "hdfs",
    "hive",
    "making",
    "easy",
    "go",
    "two",
    "using",
    "hive",
    "ql",
    "multiple",
    "users",
    "simultaneously",
    "query",
    "data",
    "multiple",
    "clients",
    "send",
    "query",
    "also",
    "true",
    "rdbms",
    "kind",
    "queues",
    "running",
    "fast",
    "notice",
    "lag",
    "time",
    "well",
    "get",
    "also",
    "hql",
    "add",
    "computers",
    "query",
    "go",
    "quickly",
    "depending",
    "many",
    "computers",
    "much",
    "resources",
    "machine",
    "pull",
    "information",
    "hive",
    "supports",
    "variety",
    "data",
    "types",
    "hive",
    "designed",
    "hadoop",
    "system",
    "put",
    "almost",
    "anything",
    "hadoop",
    "file",
    "system",
    "let",
    "take",
    "look",
    "demo",
    "hive",
    "ql",
    "hql",
    "dive",
    "demo",
    "let",
    "take",
    "look",
    "website",
    "main",
    "website",
    "since",
    "apache",
    "apache",
    "open",
    "source",
    "software",
    "main",
    "software",
    "main",
    "site",
    "build",
    "go",
    "see",
    "slowly",
    "migrating",
    "hive",
    "beehive",
    "see",
    "beehive",
    "versus",
    "hive",
    "note",
    "beehive",
    "new",
    "release",
    "coming",
    "reflects",
    "lot",
    "functionality",
    "hive",
    "thing",
    "like",
    "pull",
    "kind",
    "documentation",
    "commands",
    "actually",
    "going",
    "go",
    "hortonworks",
    "hive",
    "cheat",
    "sheet",
    "hortonworks",
    "cloudera",
    "two",
    "common",
    "used",
    "builds",
    "hadoop",
    "four",
    "include",
    "hive",
    "different",
    "tools",
    "hortonworks",
    "pretty",
    "good",
    "pdf",
    "download",
    "cheat",
    "sheet",
    "believe",
    "cloudera",
    "go",
    "ahead",
    "look",
    "horton",
    "one",
    "one",
    "comes",
    "really",
    "good",
    "see",
    "look",
    "query",
    "language",
    "compares",
    "sql",
    "server",
    "hive",
    "ql",
    "hql",
    "see",
    "basic",
    "select",
    "select",
    "columns",
    "table",
    "conditions",
    "exist",
    "basic",
    "command",
    "different",
    "things",
    "like",
    "sql",
    "scroll",
    "see",
    "data",
    "types",
    "integer",
    "flow",
    "binary",
    "double",
    "string",
    "timestamp",
    "different",
    "data",
    "types",
    "use",
    "different",
    "semantics",
    "different",
    "keys",
    "features",
    "functions",
    "uh",
    "running",
    "hive",
    "query",
    "command",
    "line",
    "setup",
    "course",
    "hive",
    "shell",
    "uh",
    "set",
    "see",
    "right",
    "loop",
    "lot",
    "basic",
    "stuff",
    "basically",
    "looking",
    "sql",
    "across",
    "horton",
    "database",
    "going",
    "go",
    "ahead",
    "run",
    "hadoop",
    "cluster",
    "hive",
    "demo",
    "going",
    "go",
    "ahead",
    "use",
    "cloudera",
    "quick",
    "start",
    "virtual",
    "box",
    "oracle",
    "virtual",
    "box",
    "open",
    "source",
    "cloudera",
    "quick",
    "start",
    "hadoop",
    "setup",
    "single",
    "node",
    "obviously",
    "hadoop",
    "hive",
    "designed",
    "run",
    "across",
    "cluster",
    "computers",
    "talk",
    "single",
    "node",
    "education",
    "testing",
    "kind",
    "thing",
    "chance",
    "always",
    "go",
    "back",
    "look",
    "demo",
    "setting",
    "hadoop",
    "system",
    "single",
    "cluster",
    "set",
    "note",
    "youtube",
    "video",
    "team",
    "get",
    "contact",
    "send",
    "link",
    "already",
    "contact",
    "us",
    "always",
    "important",
    "note",
    "need",
    "computer",
    "running",
    "windows",
    "windows",
    "machine",
    "going",
    "need",
    "probably",
    "12",
    "gigabytes",
    "actually",
    "run",
    "used",
    "goodbye",
    "lot",
    "less",
    "things",
    "evolved",
    "take",
    "resources",
    "need",
    "professional",
    "version",
    "home",
    "version",
    "able",
    "get",
    "run",
    "boy",
    "take",
    "lot",
    "extra",
    "work",
    "get",
    "home",
    "version",
    "let",
    "use",
    "virtual",
    "setup",
    "simply",
    "click",
    "cloudera",
    "quick",
    "start",
    "going",
    "go",
    "start",
    "starting",
    "linux",
    "windows",
    "10",
    "computer",
    "virtual",
    "box",
    "going",
    "linux",
    "operating",
    "system",
    "skip",
    "ahead",
    "watch",
    "whole",
    "install",
    "something",
    "interesting",
    "know",
    "cloudera",
    "running",
    "linuxcentos",
    "whatever",
    "reason",
    "always",
    "click",
    "hit",
    "escape",
    "button",
    "spin",
    "see",
    "dos",
    "come",
    "cloudera",
    "spun",
    "virtual",
    "machine",
    "linux",
    "uh",
    "see",
    "uses",
    "thunderbird",
    "browser",
    "default",
    "automatically",
    "opens",
    "number",
    "different",
    "tabs",
    "us",
    "quick",
    "note",
    "mentioned",
    "like",
    "restrictions",
    "getting",
    "set",
    "computer",
    "home",
    "edition",
    "computer",
    "worried",
    "setting",
    "also",
    "go",
    "spin",
    "one",
    "month",
    "free",
    "service",
    "amazon",
    "web",
    "service",
    "play",
    "options",
    "stuck",
    "quick",
    "start",
    "menu",
    "spin",
    "many",
    "ways",
    "first",
    "thing",
    "want",
    "note",
    "come",
    "cloudera",
    "going",
    "access",
    "two",
    "ways",
    "first",
    "one",
    "going",
    "use",
    "hue",
    "going",
    "open",
    "hue",
    "take",
    "moment",
    "load",
    "setup",
    "hue",
    "nice",
    "go",
    "use",
    "hue",
    "editor",
    "hive",
    "hadoop",
    "setup",
    "usually",
    "admin",
    "side",
    "lot",
    "information",
    "lot",
    "visuals",
    "less",
    "know",
    "actually",
    "diving",
    "executing",
    "code",
    "also",
    "write",
    "code",
    "files",
    "scripts",
    "things",
    "otherwise",
    "upload",
    "hive",
    "today",
    "going",
    "look",
    "command",
    "lines",
    "upload",
    "hue",
    "go",
    "actually",
    "work",
    "terminal",
    "window",
    "hive",
    "shell",
    "hue",
    "browser",
    "window",
    "go",
    "query",
    "click",
    "pull",
    "menu",
    "go",
    "editor",
    "see",
    "hive",
    "go",
    "hive",
    "setup",
    "go",
    "click",
    "hive",
    "open",
    "query",
    "nice",
    "little",
    "b",
    "shows",
    "hive",
    "going",
    "go",
    "something",
    "simple",
    "like",
    "show",
    "databases",
    "follow",
    "semicolon",
    "standard",
    "hive",
    "always",
    "add",
    "punctuation",
    "end",
    "go",
    "ahead",
    "run",
    "query",
    "show",
    "underneath",
    "see",
    "since",
    "new",
    "quick",
    "start",
    "put",
    "see",
    "default",
    "databases",
    "database",
    "name",
    "actually",
    "created",
    "databases",
    "lot",
    "like",
    "assistant",
    "function",
    "tables",
    "databases",
    "kinds",
    "things",
    "research",
    "look",
    "hue",
    "far",
    "bigger",
    "picture",
    "downside",
    "always",
    "seems",
    "lag",
    "whenever",
    "always",
    "seem",
    "run",
    "slow",
    "cloudera",
    "open",
    "terminal",
    "window",
    "actually",
    "icon",
    "top",
    "also",
    "go",
    "applications",
    "applications",
    "system",
    "tools",
    "terminal",
    "either",
    "one",
    "work",
    "regular",
    "terminal",
    "window",
    "terminal",
    "window",
    "running",
    "underneath",
    "linux",
    "linux",
    "terminal",
    "window",
    "virtual",
    "machine",
    "resting",
    "regular",
    "windows",
    "10",
    "machine",
    "go",
    "ahead",
    "zoom",
    "see",
    "text",
    "better",
    "video",
    "simply",
    "clicked",
    "view",
    "zoom",
    "type",
    "hive",
    "open",
    "shell",
    "takes",
    "moment",
    "load",
    "starting",
    "hive",
    "also",
    "want",
    "note",
    "depending",
    "rights",
    "computer",
    "action",
    "might",
    "pseudohyme",
    "put",
    "password",
    "username",
    "computers",
    "usually",
    "set",
    "hive",
    "login",
    "depends",
    "accessing",
    "linux",
    "system",
    "hive",
    "shell",
    "go",
    "ahead",
    "simple",
    "uh",
    "hql",
    "command",
    "show",
    "databases",
    "see",
    "databases",
    "go",
    "ahead",
    "create",
    "database",
    "call",
    "office",
    "today",
    "moment",
    "show",
    "arrow",
    "arrow",
    "hotkey",
    "works",
    "linux",
    "hive",
    "go",
    "back",
    "paste",
    "commands",
    "typed",
    "see",
    "course",
    "default",
    "database",
    "office",
    "database",
    "created",
    "database",
    "pretty",
    "quick",
    "easy",
    "go",
    "ahead",
    "drop",
    "database",
    "drop",
    "database",
    "office",
    "work",
    "database",
    "empty",
    "database",
    "empty",
    "would",
    "cascade",
    "drops",
    "tables",
    "database",
    "database",
    "show",
    "database",
    "go",
    "ahead",
    "recreate",
    "database",
    "going",
    "use",
    "office",
    "database",
    "rest",
    "demo",
    "really",
    "handy",
    "command",
    "set",
    "sql",
    "hql",
    "use",
    "office",
    "sets",
    "office",
    "default",
    "database",
    "instead",
    "reference",
    "database",
    "every",
    "time",
    "work",
    "table",
    "automatically",
    "assumes",
    "database",
    "used",
    "whatever",
    "tables",
    "working",
    "difference",
    "put",
    "database",
    "name",
    "period",
    "table",
    "show",
    "minute",
    "looks",
    "like",
    "different",
    "going",
    "table",
    "database",
    "probably",
    "load",
    "data",
    "let",
    "go",
    "ahead",
    "switch",
    "gears",
    "open",
    "terminal",
    "window",
    "open",
    "another",
    "terminal",
    "window",
    "open",
    "right",
    "top",
    "one",
    "hive",
    "shell",
    "running",
    "terminal",
    "window",
    "first",
    "going",
    "go",
    "ahead",
    "list",
    "course",
    "linux",
    "command",
    "see",
    "files",
    "default",
    "load",
    "change",
    "directory",
    "documents",
    "list",
    "documents",
    "actually",
    "going",
    "looking",
    "linux",
    "command",
    "cat",
    "use",
    "actually",
    "combine",
    "documents",
    "kinds",
    "things",
    "cat",
    "want",
    "display",
    "contents",
    "file",
    "simply",
    "cat",
    "employee",
    "csv",
    "looking",
    "want",
    "know",
    "couple",
    "things",
    "one",
    "line",
    "top",
    "okay",
    "first",
    "thing",
    "notice",
    "header",
    "line",
    "next",
    "thing",
    "notice",
    "data",
    "comma",
    "separated",
    "particular",
    "case",
    "see",
    "space",
    "generally",
    "got",
    "real",
    "careful",
    "spaces",
    "kinds",
    "things",
    "got",
    "watch",
    "cause",
    "issues",
    "spaces",
    "wo",
    "strings",
    "space",
    "connected",
    "space",
    "next",
    "integer",
    "would",
    "get",
    "null",
    "value",
    "comes",
    "database",
    "without",
    "something",
    "extra",
    "hadoop",
    "important",
    "know",
    "writing",
    "data",
    "reading",
    "many",
    "times",
    "true",
    "almost",
    "hadoop",
    "things",
    "coming",
    "really",
    "want",
    "process",
    "data",
    "gets",
    "database",
    "studied",
    "uh",
    "data",
    "transformation",
    "etyl",
    "extract",
    "transfer",
    "form",
    "load",
    "data",
    "really",
    "want",
    "extract",
    "transform",
    "putting",
    "hive",
    "load",
    "hive",
    "transform",
    "data",
    "course",
    "also",
    "want",
    "note",
    "schema",
    "integer",
    "string",
    "string",
    "integer",
    "integer",
    "kept",
    "pretty",
    "simple",
    "far",
    "way",
    "data",
    "set",
    "last",
    "thing",
    "going",
    "want",
    "look",
    "source",
    "since",
    "local",
    "uploads",
    "want",
    "know",
    "path",
    "whole",
    "path",
    "case",
    "home",
    "slash",
    "cloudera",
    "slash",
    "documents",
    "text",
    "documents",
    "working",
    "right",
    "anything",
    "fancy",
    "simple",
    "get",
    "edit",
    "see",
    "comes",
    "text",
    "document",
    "easily",
    "remove",
    "added",
    "spaces",
    "go",
    "go",
    "save",
    "new",
    "setup",
    "edited",
    "g",
    "edit",
    "usually",
    "one",
    "default",
    "loads",
    "linux",
    "text",
    "editor",
    "back",
    "hive",
    "shell",
    "let",
    "go",
    "ahead",
    "create",
    "table",
    "employee",
    "want",
    "note",
    "put",
    "semicolon",
    "end",
    "semicolon",
    "tells",
    "execute",
    "line",
    "kind",
    "nice",
    "actually",
    "paste",
    "written",
    "another",
    "sheet",
    "see",
    "right",
    "create",
    "table",
    "employee",
    "goes",
    "next",
    "line",
    "commands",
    "typo",
    "errors",
    "went",
    "ahead",
    "pasted",
    "next",
    "three",
    "lines",
    "next",
    "one",
    "schema",
    "remember",
    "correctly",
    "side",
    "uh",
    "different",
    "values",
    "id",
    "name",
    "department",
    "year",
    "joining",
    "salary",
    "id",
    "integer",
    "name",
    "string",
    "department",
    "string",
    "air",
    "joining",
    "energy",
    "salary",
    "integer",
    "brackets",
    "put",
    "close",
    "brackets",
    "around",
    "could",
    "one",
    "line",
    "row",
    "format",
    "delimited",
    "fields",
    "terminated",
    "comma",
    "important",
    "default",
    "tabs",
    "wo",
    "find",
    "terminated",
    "fields",
    "get",
    "bunch",
    "null",
    "values",
    "loaded",
    "table",
    "finally",
    "table",
    "properties",
    "want",
    "skip",
    "header",
    "line",
    "count",
    "equals",
    "one",
    "lot",
    "work",
    "uploading",
    "single",
    "file",
    "kind",
    "goofy",
    "uploading",
    "single",
    "file",
    "put",
    "keep",
    "mind",
    "hive",
    "hadoop",
    "designed",
    "writing",
    "many",
    "files",
    "database",
    "write",
    "saved",
    "archive",
    "data",
    "warehouse",
    "able",
    "queries",
    "lot",
    "times",
    "looking",
    "one",
    "file",
    "coming",
    "loading",
    "hundreds",
    "files",
    "reports",
    "coming",
    "main",
    "database",
    "reports",
    "loaded",
    "log",
    "files",
    "mean",
    "different",
    "data",
    "dumped",
    "hadoop",
    "case",
    "hive",
    "top",
    "hadoop",
    "need",
    "let",
    "know",
    "hey",
    "handle",
    "files",
    "coming",
    "semicolon",
    "end",
    "lets",
    "us",
    "know",
    "go",
    "ahead",
    "run",
    "line",
    "go",
    "ahead",
    "run",
    "show",
    "tables",
    "see",
    "employee",
    "also",
    "describe",
    "describe",
    "employee",
    "see",
    "id",
    "integer",
    "name",
    "string",
    "department",
    "string",
    "year",
    "joining",
    "integer",
    "salary",
    "integer",
    "finally",
    "let",
    "select",
    "star",
    "employee",
    "basic",
    "sql",
    "nhql",
    "command",
    "selecting",
    "data",
    "going",
    "come",
    "put",
    "anything",
    "expect",
    "data",
    "flip",
    "back",
    "linux",
    "terminal",
    "window",
    "see",
    "cat",
    "see",
    "data",
    "expect",
    "come",
    "also",
    "pwd",
    "right",
    "see",
    "path",
    "need",
    "full",
    "path",
    "loading",
    "data",
    "know",
    "browse",
    "right",
    "name",
    "work",
    "really",
    "bad",
    "habit",
    "general",
    "loading",
    "data",
    "know",
    "else",
    "going",
    "computer",
    "want",
    "full",
    "path",
    "almost",
    "data",
    "loads",
    "let",
    "go",
    "ahead",
    "flip",
    "back",
    "hive",
    "shell",
    "working",
    "command",
    "load",
    "data",
    "says",
    "hey",
    "loading",
    "data",
    "hive",
    "command",
    "hql",
    "want",
    "local",
    "data",
    "got",
    "put",
    "local",
    "path",
    "needs",
    "know",
    "path",
    "make",
    "legible",
    "going",
    "go",
    "ahead",
    "hit",
    "enter",
    "paste",
    "full",
    "path",
    "stored",
    "side",
    "like",
    "good",
    "prepared",
    "demo",
    "see",
    "home",
    "cloudera",
    "documents",
    "whole",
    "path",
    "text",
    "document",
    "go",
    "ahead",
    "hit",
    "enter",
    "let",
    "know",
    "data",
    "going",
    "source",
    "need",
    "destination",
    "going",
    "go",
    "table",
    "call",
    "employee",
    "match",
    "table",
    "want",
    "execute",
    "put",
    "semicolon",
    "end",
    "goes",
    "ahead",
    "executes",
    "three",
    "lines",
    "go",
    "back",
    "remember",
    "select",
    "star",
    "employee",
    "using",
    "error",
    "page",
    "different",
    "commands",
    "already",
    "typed",
    "see",
    "right",
    "expect",
    "rows",
    "sam",
    "mike",
    "nick",
    "information",
    "showing",
    "four",
    "rows",
    "let",
    "go",
    "ahead",
    "uh",
    "select",
    "count",
    "let",
    "look",
    "couple",
    "different",
    "select",
    "options",
    "going",
    "count",
    "everything",
    "employee",
    "kind",
    "interesting",
    "first",
    "one",
    "pops",
    "basic",
    "select",
    "need",
    "go",
    "full",
    "map",
    "reduce",
    "phase",
    "start",
    "count",
    "go",
    "full",
    "map",
    "redo",
    "setup",
    "hive",
    "hadoop",
    "demo",
    "single",
    "node",
    "cloudera",
    "virtual",
    "box",
    "top",
    "windows",
    "10",
    "benefits",
    "running",
    "cluster",
    "gone",
    "instead",
    "going",
    "added",
    "layers",
    "takes",
    "longer",
    "run",
    "know",
    "like",
    "said",
    "single",
    "node",
    "said",
    "earlier",
    "good",
    "actual",
    "distribution",
    "running",
    "one",
    "computer",
    "added",
    "different",
    "layers",
    "run",
    "see",
    "comes",
    "four",
    "expect",
    "four",
    "rows",
    "expect",
    "four",
    "end",
    "remember",
    "cheat",
    "sheet",
    "brought",
    "hortons",
    "pretty",
    "good",
    "one",
    "different",
    "commands",
    "look",
    "one",
    "command",
    "uh",
    "call",
    "sub",
    "queries",
    "right",
    "really",
    "common",
    "lot",
    "sub",
    "queries",
    "select",
    "star",
    "different",
    "columns",
    "employee",
    "using",
    "office",
    "database",
    "would",
    "look",
    "like",
    "office",
    "dot",
    "employee",
    "either",
    "one",
    "work",
    "particular",
    "one",
    "office",
    "set",
    "default",
    "office",
    "employee",
    "command",
    "creates",
    "subset",
    "case",
    "want",
    "know",
    "salary",
    "greater",
    "25",
    "go",
    "course",
    "end",
    "semicolon",
    "run",
    "query",
    "see",
    "pops",
    "salaries",
    "people",
    "top",
    "earners",
    "rose",
    "mike",
    "hr",
    "kudos",
    "course",
    "fictional",
    "actually",
    "actually",
    "rose",
    "mic",
    "positions",
    "maybe",
    "finally",
    "want",
    "go",
    "ahead",
    "done",
    "table",
    "remember",
    "dealing",
    "data",
    "warehouse",
    "usually",
    "lot",
    "dropping",
    "tables",
    "databases",
    "going",
    "go",
    "ahead",
    "drop",
    "table",
    "drop",
    "one",
    "quick",
    "note",
    "change",
    "going",
    "going",
    "alter",
    "table",
    "office",
    "employee",
    "want",
    "go",
    "ahead",
    "rename",
    "commands",
    "rename",
    "pretty",
    "common",
    "going",
    "rename",
    "going",
    "stay",
    "office",
    "turns",
    "one",
    "shareholders",
    "really",
    "like",
    "word",
    "employee",
    "wants",
    "employees",
    "plural",
    "big",
    "deal",
    "let",
    "go",
    "ahead",
    "change",
    "name",
    "table",
    "easy",
    "changing",
    "metadata",
    "show",
    "tables",
    "see",
    "employees",
    "employee",
    "point",
    "maybe",
    "house",
    "cleaning",
    "practice",
    "going",
    "go",
    "ahead",
    "drop",
    "table",
    "drop",
    "table",
    "employees",
    "changed",
    "name",
    "employee",
    "give",
    "us",
    "error",
    "show",
    "tables",
    "see",
    "tables",
    "gone",
    "next",
    "thing",
    "want",
    "go",
    "take",
    "look",
    "going",
    "walk",
    "back",
    "loading",
    "data",
    "real",
    "quick",
    "going",
    "load",
    "two",
    "tables",
    "let",
    "float",
    "back",
    "terminal",
    "window",
    "see",
    "tables",
    "loading",
    "customer",
    "customer",
    "file",
    "order",
    "file",
    "want",
    "go",
    "ahead",
    "put",
    "customers",
    "orders",
    "two",
    "course",
    "always",
    "nice",
    "see",
    "working",
    "let",
    "cat",
    "could",
    "always",
    "g",
    "edit",
    "really",
    "need",
    "edit",
    "want",
    "take",
    "look",
    "data",
    "customer",
    "important",
    "header",
    "skip",
    "line",
    "comma",
    "separated",
    "nothing",
    "odd",
    "data",
    "schema",
    "integer",
    "string",
    "integer",
    "string",
    "integer",
    "want",
    "take",
    "note",
    "flip",
    "back",
    "forth",
    "let",
    "go",
    "ahead",
    "cat",
    "see",
    "oid",
    "guessing",
    "order",
    "id",
    "date",
    "something",
    "new",
    "done",
    "integers",
    "strings",
    "done",
    "date",
    "importing",
    "new",
    "never",
    "worked",
    "date",
    "date",
    "always",
    "one",
    "trickier",
    "fields",
    "port",
    "true",
    "scripting",
    "language",
    "worked",
    "idea",
    "date",
    "supposed",
    "formatted",
    "default",
    "particular",
    "format",
    "year",
    "four",
    "digits",
    "dash",
    "month",
    "two",
    "digits",
    "dash",
    "day",
    "standard",
    "import",
    "hive",
    "look",
    "see",
    "different",
    "formats",
    "going",
    "different",
    "format",
    "coming",
    "able",
    "data",
    "would",
    "data",
    "thing",
    "coming",
    "remember",
    "correctly",
    "edel",
    "uh",
    "e",
    "case",
    "able",
    "hear",
    "last",
    "time",
    "etl",
    "stands",
    "extract",
    "transform",
    "load",
    "want",
    "make",
    "sure",
    "transforming",
    "data",
    "gets",
    "going",
    "go",
    "ahead",
    "bring",
    "data",
    "really",
    "show",
    "basic",
    "join",
    "remember",
    "setup",
    "merge",
    "join",
    "kinds",
    "different",
    "things",
    "joining",
    "different",
    "data",
    "sets",
    "common",
    "really",
    "important",
    "know",
    "need",
    "go",
    "ahead",
    "bring",
    "two",
    "data",
    "sets",
    "see",
    "created",
    "table",
    "customer",
    "schema",
    "integer",
    "name",
    "age",
    "address",
    "salary",
    "eliminated",
    "commas",
    "table",
    "properties",
    "skip",
    "line",
    "well",
    "let",
    "go",
    "ahead",
    "load",
    "data",
    "first",
    "order",
    "let",
    "go",
    "ahead",
    "put",
    "got",
    "split",
    "three",
    "lines",
    "see",
    "easily",
    "got",
    "load",
    "data",
    "local",
    "path",
    "know",
    "loading",
    "data",
    "know",
    "local",
    "path",
    "complete",
    "path",
    "oops",
    "supposed",
    "order",
    "csv",
    "grab",
    "wrong",
    "one",
    "course",
    "going",
    "give",
    "errors",
    "ca",
    "recreate",
    "table",
    "go",
    "create",
    "table",
    "integer",
    "date",
    "customer",
    "basic",
    "setup",
    "coming",
    "schema",
    "row",
    "format",
    "commas",
    "table",
    "properties",
    "skip",
    "header",
    "line",
    "finally",
    "let",
    "load",
    "data",
    "order",
    "table",
    "load",
    "data",
    "local",
    "path",
    "home",
    "cloudera",
    "documents",
    "table",
    "order",
    "everything",
    "right",
    "able",
    "select",
    "star",
    "customer",
    "see",
    "seven",
    "customers",
    "select",
    "star",
    "order",
    "uh",
    "four",
    "orders",
    "uh",
    "like",
    "quick",
    "frame",
    "lot",
    "times",
    "customer",
    "databases",
    "business",
    "thousands",
    "customers",
    "years",
    "years",
    "know",
    "move",
    "close",
    "business",
    "change",
    "names",
    "kinds",
    "things",
    "happen",
    "uh",
    "want",
    "want",
    "go",
    "ahead",
    "find",
    "information",
    "connected",
    "orders",
    "connected",
    "let",
    "go",
    "ahead",
    "select",
    "going",
    "display",
    "information",
    "select",
    "kind",
    "interesting",
    "going",
    "c",
    "dot",
    "id",
    "going",
    "define",
    "c",
    "customer",
    "customer",
    "table",
    "minute",
    "going",
    "c",
    "dot",
    "name",
    "going",
    "define",
    "c",
    "c",
    "dot",
    "age",
    "means",
    "customer",
    "want",
    "know",
    "id",
    "name",
    "age",
    "know",
    "also",
    "like",
    "know",
    "order",
    "amount",
    "uh",
    "let",
    "dot",
    "amount",
    "need",
    "go",
    "ahead",
    "define",
    "uh",
    "going",
    "capitalize",
    "customer",
    "going",
    "take",
    "customer",
    "table",
    "going",
    "name",
    "c",
    "c",
    "comes",
    "customer",
    "table",
    "c",
    "want",
    "join",
    "order",
    "comes",
    "dot",
    "amount",
    "joining",
    "want",
    "got",
    "tell",
    "connect",
    "two",
    "tables",
    "c",
    "dot",
    "id",
    "equals",
    "dot",
    "customer",
    "underscore",
    "id",
    "know",
    "joined",
    "remember",
    "seven",
    "customers",
    "four",
    "orders",
    "processes",
    "get",
    "return",
    "four",
    "different",
    "names",
    "joined",
    "together",
    "joined",
    "based",
    "course",
    "orders",
    "done",
    "order",
    "number",
    "person",
    "made",
    "order",
    "age",
    "amount",
    "order",
    "came",
    "order",
    "table",
    "uh",
    "different",
    "information",
    "see",
    "join",
    "works",
    "common",
    "use",
    "tables",
    "hql",
    "sql",
    "let",
    "one",
    "thing",
    "database",
    "show",
    "couple",
    "hive",
    "commands",
    "let",
    "go",
    "ahead",
    "drop",
    "going",
    "drop",
    "database",
    "office",
    "looking",
    "remember",
    "earlier",
    "give",
    "error",
    "let",
    "see",
    "looks",
    "like",
    "says",
    "fill",
    "execute",
    "exception",
    "one",
    "tables",
    "exist",
    "remember",
    "ca",
    "drop",
    "database",
    "unless",
    "tell",
    "cascade",
    "lets",
    "know",
    "care",
    "many",
    "tables",
    "let",
    "get",
    "rid",
    "hadoop",
    "since",
    "art",
    "warehouse",
    "data",
    "warehouse",
    "usually",
    "lot",
    "dropping",
    "maybe",
    "beginning",
    "developing",
    "schemas",
    "realize",
    "messed",
    "might",
    "drop",
    "stuff",
    "road",
    "really",
    "adding",
    "commodity",
    "machines",
    "pick",
    "store",
    "stuff",
    "usually",
    "lot",
    "database",
    "dropping",
    "fun",
    "commands",
    "know",
    "select",
    "round",
    "round",
    "value",
    "round",
    "hive",
    "floor",
    "value",
    "going",
    "give",
    "us",
    "2",
    "turns",
    "integer",
    "versus",
    "float",
    "goes",
    "know",
    "basically",
    "truncates",
    "goes",
    "also",
    "ceiling",
    "going",
    "round",
    "looking",
    "next",
    "integer",
    "commands",
    "show",
    "single",
    "node",
    "admin",
    "help",
    "spediate",
    "process",
    "usually",
    "add",
    "partitions",
    "data",
    "buckets",
    "ca",
    "single",
    "node",
    "add",
    "partition",
    "partitions",
    "across",
    "separate",
    "nodes",
    "beyond",
    "see",
    "straightforward",
    "sql",
    "coming",
    "basic",
    "queries",
    "sql",
    "similar",
    "hql",
    "let",
    "get",
    "started",
    "pig",
    "pig",
    "pig",
    "mapreduce",
    "versus",
    "hive",
    "versus",
    "pig",
    "hopefully",
    "chance",
    "hive",
    "tutorial",
    "mapreduce",
    "tutorial",
    "send",
    "note",
    "simplylearn",
    "follow",
    "link",
    "look",
    "pig",
    "architecture",
    "working",
    "pig",
    "pig",
    "latin",
    "data",
    "model",
    "pig",
    "execution",
    "modes",
    "use",
    "case",
    "twitter",
    "features",
    "pig",
    "tag",
    "short",
    "demo",
    "see",
    "pig",
    "action",
    "pig",
    "know",
    "hadoop",
    "uses",
    "mapreduce",
    "analyze",
    "process",
    "big",
    "data",
    "processing",
    "big",
    "data",
    "consumed",
    "time",
    "hadoop",
    "system",
    "spend",
    "lot",
    "money",
    "huge",
    "set",
    "computers",
    "enterprise",
    "machines",
    "introduced",
    "hadoop",
    "map",
    "reduce",
    "afterwards",
    "processing",
    "big",
    "data",
    "faster",
    "using",
    "mapreduce",
    "problem",
    "map",
    "reduce",
    "prior",
    "2006",
    "mapreduce",
    "programs",
    "written",
    "java",
    "found",
    "difficult",
    "write",
    "lengthy",
    "java",
    "codes",
    "faced",
    "issues",
    "incorporating",
    "map",
    "sort",
    "reduced",
    "fundamentals",
    "mapreduce",
    "creating",
    "program",
    "see",
    "map",
    "face",
    "shuffle",
    "sort",
    "reduce",
    "phase",
    "eventually",
    "became",
    "difficult",
    "task",
    "maintain",
    "optimize",
    "code",
    "due",
    "processing",
    "time",
    "increased",
    "imagine",
    "manager",
    "trying",
    "go",
    "needing",
    "simple",
    "query",
    "find",
    "data",
    "go",
    "talk",
    "programmers",
    "anytime",
    "wants",
    "anything",
    "big",
    "problem",
    "everybody",
    "wants",
    "programmer",
    "every",
    "manager",
    "team",
    "yahoo",
    "faced",
    "problems",
    "process",
    "analyze",
    "large",
    "data",
    "sets",
    "using",
    "java",
    "codes",
    "complex",
    "lengthy",
    "necessity",
    "develop",
    "easier",
    "way",
    "analyze",
    "large",
    "datasets",
    "without",
    "using",
    "complex",
    "java",
    "modes",
    "codes",
    "scripts",
    "fun",
    "stuff",
    "apache",
    "pig",
    "developed",
    "yahoo",
    "developed",
    "vision",
    "analyze",
    "process",
    "large",
    "datasets",
    "without",
    "using",
    "complex",
    "java",
    "codes",
    "pig",
    "developed",
    "especially",
    "pig",
    "used",
    "simple",
    "steps",
    "analyze",
    "data",
    "sets",
    "time",
    "efficient",
    "exactly",
    "pik",
    "pig",
    "scripting",
    "platform",
    "runs",
    "hadoop",
    "clusters",
    "designed",
    "process",
    "analyze",
    "large",
    "data",
    "sets",
    "pig",
    "uses",
    "sql",
    "like",
    "queries",
    "definitely",
    "sql",
    "resemble",
    "sql",
    "queries",
    "use",
    "analyze",
    "data",
    "pig",
    "operates",
    "various",
    "types",
    "data",
    "like",
    "structured",
    "unstructured",
    "data",
    "let",
    "take",
    "closer",
    "look",
    "mapreduce",
    "versus",
    "hive",
    "versus",
    "pig",
    "start",
    "compiled",
    "language",
    "mapreduce",
    "hive",
    "sql",
    "like",
    "query",
    "pig",
    "scripting",
    "language",
    "similarities",
    "sql",
    "lot",
    "stuff",
    "remember",
    "sql",
    "like",
    "query",
    "hive",
    "based",
    "looks",
    "structured",
    "data",
    "get",
    "scripting",
    "languages",
    "like",
    "pig",
    "dealing",
    "even",
    "unstructured",
    "data",
    "hadoop",
    "map",
    "reduced",
    "need",
    "write",
    "long",
    "complex",
    "codes",
    "hive",
    "need",
    "write",
    "complex",
    "codes",
    "could",
    "put",
    "simple",
    "sql",
    "query",
    "hql",
    "hive",
    "ql",
    "pig",
    "need",
    "write",
    "complex",
    "codes",
    "piglet",
    "remember",
    "map",
    "reduce",
    "produce",
    "structured",
    "unstructured",
    "data",
    "mentioned",
    "hive",
    "process",
    "structured",
    "data",
    "think",
    "rows",
    "columns",
    "pig",
    "process",
    "structured",
    "unstructured",
    "data",
    "think",
    "structured",
    "data",
    "rows",
    "columns",
    "html",
    "xml",
    "documents",
    "like",
    "web",
    "pages",
    "unstructured",
    "could",
    "anything",
    "groups",
    "documents",
    "written",
    "format",
    "twitter",
    "tweets",
    "things",
    "come",
    "unstructured",
    "data",
    "hadoop",
    "map",
    "reduce",
    "lower",
    "level",
    "abstraction",
    "hive",
    "pig",
    "higher",
    "level",
    "abstraction",
    "much",
    "easy",
    "someone",
    "use",
    "without",
    "dive",
    "deep",
    "write",
    "lengthy",
    "map",
    "reduce",
    "code",
    "map",
    "reduce",
    "codes",
    "take",
    "70",
    "80",
    "lines",
    "code",
    "thing",
    "one",
    "two",
    "lines",
    "high",
    "ever",
    "pig",
    "advantage",
    "pig",
    "hive",
    "process",
    "structured",
    "data",
    "hive",
    "pig",
    "process",
    "structured",
    "unstructured",
    "data",
    "features",
    "know",
    "separates",
    "different",
    "query",
    "languages",
    "look",
    "map",
    "reduce",
    "mapreduce",
    "supports",
    "partitioning",
    "features",
    "hive",
    "pig",
    "concept",
    "partitioning",
    "pix",
    "support",
    "partitioning",
    "feature",
    "partitioning",
    "features",
    "allow",
    "partition",
    "data",
    "way",
    "queried",
    "quicker",
    "able",
    "pig",
    "mapreduce",
    "uses",
    "java",
    "python",
    "hive",
    "uses",
    "sql",
    "like",
    "query",
    "language",
    "known",
    "hive",
    "ql",
    "hql",
    "pig",
    "latin",
    "used",
    "procedural",
    "data",
    "language",
    "mapreduce",
    "used",
    "programmers",
    "pretty",
    "much",
    "straightforward",
    "java",
    "hive",
    "used",
    "data",
    "analysts",
    "pig",
    "used",
    "researchers",
    "programmers",
    "certainly",
    "lot",
    "mix",
    "three",
    "programmers",
    "known",
    "go",
    "use",
    "hive",
    "quick",
    "query",
    "anybody",
    "able",
    "use",
    "pig",
    "quick",
    "query",
    "research",
    "map",
    "reduce",
    "code",
    "performance",
    "really",
    "good",
    "hive",
    "code",
    "performance",
    "lesser",
    "map",
    "reduce",
    "pig",
    "pig",
    "code",
    "performance",
    "lesser",
    "mapreduce",
    "better",
    "hive",
    "going",
    "look",
    "speed",
    "time",
    "map",
    "reduce",
    "going",
    "fastest",
    "performance",
    "pig",
    "second",
    "high",
    "follows",
    "back",
    "let",
    "look",
    "components",
    "pig",
    "pig",
    "two",
    "main",
    "components",
    "pig",
    "latin",
    "pig",
    "latin",
    "procedural",
    "data",
    "flow",
    "language",
    "used",
    "pig",
    "analyze",
    "data",
    "easy",
    "program",
    "using",
    "piglet",
    "similar",
    "sql",
    "runtime",
    "engine",
    "runtime",
    "engine",
    "represents",
    "execution",
    "environment",
    "created",
    "run",
    "pig",
    "latin",
    "programs",
    "also",
    "compiler",
    "produces",
    "mapreduce",
    "programs",
    "uses",
    "hdfs",
    "hadoop",
    "file",
    "system",
    "storing",
    "retrieving",
    "data",
    "dig",
    "deeper",
    "pig",
    "architecture",
    "see",
    "pig",
    "latin",
    "scripts",
    "programmers",
    "write",
    "script",
    "piglet",
    "analyze",
    "data",
    "using",
    "pig",
    "grunt",
    "shell",
    "actually",
    "says",
    "grunt",
    "start",
    "show",
    "little",
    "bit",
    "goes",
    "pig",
    "server",
    "parser",
    "parser",
    "checks",
    "syntax",
    "pig",
    "script",
    "checking",
    "output",
    "dag",
    "directed",
    "acylic",
    "graph",
    "optimizer",
    "optimizes",
    "dag",
    "logical",
    "plan",
    "passed",
    "logical",
    "optimizer",
    "optimization",
    "takes",
    "place",
    "finally",
    "compiler",
    "converts",
    "dag",
    "mapreduce",
    "jobs",
    "executed",
    "map",
    "reduce",
    "execution",
    "engine",
    "results",
    "displayed",
    "using",
    "dump",
    "statement",
    "stored",
    "hdfs",
    "using",
    "store",
    "statement",
    "show",
    "kind",
    "end",
    "always",
    "want",
    "execute",
    "everything",
    "created",
    "dump",
    "kind",
    "execution",
    "statement",
    "see",
    "right",
    "talking",
    "earlier",
    "get",
    "execution",
    "engine",
    "coded",
    "mapreduce",
    "mapreduce",
    "processes",
    "onto",
    "hdfs",
    "working",
    "pig",
    "pig",
    "latin",
    "script",
    "written",
    "users",
    "low",
    "data",
    "write",
    "pig",
    "script",
    "pig",
    "operations",
    "look",
    "working",
    "pig",
    "pig",
    "latin",
    "script",
    "written",
    "users",
    "step",
    "one",
    "load",
    "data",
    "write",
    "pig",
    "script",
    "step",
    "two",
    "step",
    "pig",
    "operations",
    "performed",
    "parser",
    "optimizer",
    "compiler",
    "go",
    "pig",
    "operations",
    "get",
    "step",
    "three",
    "execution",
    "plan",
    "days",
    "results",
    "shown",
    "screen",
    "otherwise",
    "stored",
    "hdfs",
    "per",
    "code",
    "might",
    "small",
    "amount",
    "data",
    "reducing",
    "want",
    "put",
    "screen",
    "might",
    "converting",
    "huge",
    "amount",
    "data",
    "want",
    "put",
    "back",
    "hadoop",
    "file",
    "system",
    "use",
    "let",
    "take",
    "look",
    "pig",
    "latin",
    "data",
    "model",
    "data",
    "model",
    "pig",
    "latin",
    "helps",
    "pig",
    "handle",
    "various",
    "types",
    "data",
    "example",
    "adam",
    "rob",
    "atom",
    "represents",
    "single",
    "value",
    "primitive",
    "data",
    "type",
    "pig",
    "latin",
    "like",
    "integer",
    "float",
    "string",
    "stored",
    "string",
    "tuple",
    "go",
    "atom",
    "basic",
    "things",
    "look",
    "rob",
    "50",
    "atom",
    "basic",
    "object",
    "pig",
    "latin",
    "tuple",
    "tuple",
    "represents",
    "sequence",
    "fields",
    "data",
    "type",
    "row",
    "rdbms",
    "example",
    "set",
    "data",
    "single",
    "row",
    "see",
    "rob",
    "comma",
    "five",
    "imagine",
    "many",
    "examples",
    "used",
    "might",
    "id",
    "number",
    "name",
    "live",
    "age",
    "date",
    "starting",
    "job",
    "would",
    "one",
    "row",
    "stored",
    "tuple",
    "create",
    "bag",
    "bag",
    "collection",
    "tuples",
    "table",
    "rdbms",
    "represented",
    "brackets",
    "see",
    "table",
    "rob5",
    "mic",
    "10",
    "also",
    "map",
    "map",
    "set",
    "key",
    "value",
    "pairs",
    "key",
    "character",
    "array",
    "type",
    "value",
    "type",
    "represented",
    "brackets",
    "name",
    "age",
    "key",
    "value",
    "mic",
    "pig",
    "latin",
    "fully",
    "nestable",
    "data",
    "model",
    "means",
    "one",
    "data",
    "type",
    "nested",
    "within",
    "another",
    "diagram",
    "representation",
    "pig",
    "latin",
    "data",
    "model",
    "particular",
    "example",
    "basically",
    "id",
    "number",
    "name",
    "age",
    "place",
    "break",
    "apart",
    "look",
    "model",
    "pig",
    "latin",
    "perspective",
    "start",
    "field",
    "remember",
    "field",
    "contains",
    "basically",
    "atom",
    "one",
    "particular",
    "data",
    "type",
    "atom",
    "stored",
    "string",
    "converts",
    "either",
    "integer",
    "number",
    "character",
    "string",
    "next",
    "tuple",
    "case",
    "see",
    "represents",
    "row",
    "tuple",
    "would",
    "three",
    "comma",
    "joe",
    "comma",
    "29",
    "comma",
    "california",
    "finally",
    "bag",
    "contains",
    "three",
    "rows",
    "particular",
    "example",
    "let",
    "take",
    "quick",
    "look",
    "pig",
    "execution",
    "modes",
    "pig",
    "works",
    "two",
    "execution",
    "modes",
    "depending",
    "data",
    "reciting",
    "pig",
    "script",
    "going",
    "run",
    "local",
    "mode",
    "pig",
    "engine",
    "takes",
    "input",
    "linux",
    "file",
    "system",
    "output",
    "stored",
    "file",
    "system",
    "local",
    "mold",
    "local",
    "mode",
    "useful",
    "analyzing",
    "small",
    "data",
    "sets",
    "using",
    "pig",
    "map",
    "reduce",
    "mode",
    "pig",
    "engine",
    "directly",
    "interacts",
    "executes",
    "hdfs",
    "mapreduce",
    "map",
    "reduce",
    "mode",
    "queries",
    "written",
    "pig",
    "latin",
    "translated",
    "mapreduce",
    "jobs",
    "run",
    "hadoop",
    "cluster",
    "default",
    "pig",
    "runs",
    "mode",
    "three",
    "modes",
    "pig",
    "depending",
    "pig",
    "latin",
    "code",
    "written",
    "interactive",
    "mode",
    "batch",
    "mode",
    "embedded",
    "mode",
    "interactive",
    "mode",
    "means",
    "coding",
    "executing",
    "script",
    "line",
    "line",
    "example",
    "interactive",
    "mode",
    "batch",
    "mode",
    "scripts",
    "coded",
    "file",
    "file",
    "directly",
    "executed",
    "embedded",
    "mode",
    "pig",
    "lets",
    "users",
    "define",
    "functions",
    "udfss",
    "programming",
    "language",
    "java",
    "let",
    "take",
    "look",
    "see",
    "works",
    "use",
    "case",
    "case",
    "use",
    "case",
    "twitter",
    "users",
    "twitter",
    "generate",
    "500",
    "million",
    "tweets",
    "daily",
    "basis",
    "hadoop",
    "mapreduce",
    "used",
    "process",
    "analyze",
    "data",
    "analyzing",
    "number",
    "tweets",
    "created",
    "user",
    "tweet",
    "table",
    "done",
    "using",
    "mapreduce",
    "java",
    "programming",
    "language",
    "see",
    "problem",
    "difficult",
    "perform",
    "map",
    "reduce",
    "operations",
    "users",
    "well",
    "versed",
    "written",
    "complex",
    "java",
    "codes",
    "twitter",
    "used",
    "apache",
    "pig",
    "overcome",
    "problems",
    "let",
    "see",
    "let",
    "start",
    "problem",
    "statement",
    "analyze",
    "user",
    "table",
    "tweet",
    "table",
    "find",
    "many",
    "tweets",
    "created",
    "person",
    "see",
    "user",
    "table",
    "alice",
    "tim",
    "john",
    "id",
    "numbers",
    "one",
    "two",
    "three",
    "tweet",
    "table",
    "tweet",
    "table",
    "um",
    "id",
    "user",
    "tweeted",
    "google",
    "good",
    "whatever",
    "tennis",
    "dot",
    "spacecraft",
    "olympics",
    "politics",
    "whatever",
    "tweeting",
    "following",
    "operations",
    "performed",
    "analyzing",
    "given",
    "data",
    "first",
    "twitter",
    "data",
    "loaded",
    "pig",
    "storage",
    "using",
    "load",
    "command",
    "see",
    "data",
    "coming",
    "going",
    "pig",
    "storage",
    "data",
    "probably",
    "enterprise",
    "computer",
    "actually",
    "active",
    "twitter",
    "going",
    "goes",
    "hadoop",
    "file",
    "system",
    "remember",
    "hadoop",
    "file",
    "system",
    "data",
    "warehouse",
    "storing",
    "data",
    "first",
    "step",
    "want",
    "go",
    "ahead",
    "load",
    "pig",
    "storage",
    "data",
    "storage",
    "system",
    "remaining",
    "operations",
    "performed",
    "shown",
    "join",
    "group",
    "operation",
    "tweet",
    "user",
    "tables",
    "joined",
    "grouped",
    "using",
    "command",
    "see",
    "add",
    "whole",
    "column",
    "go",
    "user",
    "names",
    "tweet",
    "id",
    "link",
    "directly",
    "name",
    "alice",
    "user",
    "1",
    "10",
    "2",
    "john",
    "listed",
    "actual",
    "tweet",
    "next",
    "operation",
    "aggregation",
    "tweets",
    "counted",
    "according",
    "names",
    "command",
    "used",
    "count",
    "straightforward",
    "want",
    "count",
    "many",
    "tweets",
    "user",
    "finally",
    "result",
    "count",
    "operation",
    "joined",
    "user",
    "table",
    "find",
    "username",
    "see",
    "alice",
    "three",
    "tim",
    "two",
    "john",
    "pig",
    "reduces",
    "complexity",
    "operations",
    "would",
    "lengthy",
    "using",
    "mapreduce",
    "joining",
    "group",
    "operation",
    "tweet",
    "user",
    "tables",
    "joined",
    "grouped",
    "using",
    "command",
    "next",
    "operation",
    "aggregation",
    "tweets",
    "counted",
    "according",
    "names",
    "command",
    "used",
    "count",
    "result",
    "count",
    "operation",
    "joined",
    "user",
    "table",
    "find",
    "username",
    "see",
    "talking",
    "three",
    "lines",
    "script",
    "versus",
    "mapreduce",
    "code",
    "80",
    "lines",
    "finally",
    "could",
    "find",
    "number",
    "tweets",
    "created",
    "user",
    "simple",
    "way",
    "let",
    "go",
    "quickly",
    "features",
    "pig",
    "already",
    "went",
    "first",
    "ease",
    "programming",
    "pig",
    "latin",
    "similar",
    "sql",
    "lesser",
    "lines",
    "code",
    "need",
    "written",
    "short",
    "development",
    "time",
    "code",
    "simpler",
    "get",
    "queries",
    "rather",
    "quickly",
    "instead",
    "programmer",
    "spend",
    "hours",
    "handles",
    "kinds",
    "data",
    "like",
    "structured",
    "unstructured",
    "pig",
    "lets",
    "us",
    "create",
    "user",
    "defined",
    "functions",
    "pig",
    "offers",
    "large",
    "set",
    "operators",
    "join",
    "filter",
    "allows",
    "multiple",
    "queries",
    "process",
    "parallel",
    "optimization",
    "compilation",
    "easy",
    "done",
    "automatically",
    "internally",
    "enough",
    "theory",
    "let",
    "dive",
    "show",
    "quick",
    "demo",
    "commands",
    "pick",
    "today",
    "setup",
    "continue",
    "last",
    "three",
    "demos",
    "go",
    "use",
    "cloudera",
    "quick",
    "start",
    "virtual",
    "box",
    "tutorial",
    "setting",
    "send",
    "note",
    "simply",
    "learn",
    "team",
    "get",
    "linked",
    "cloudera",
    "quickstart",
    "spun",
    "remember",
    "virtualbox",
    "created",
    "virtual",
    "machine",
    "virtual",
    "machine",
    "centos",
    "linux",
    "spun",
    "full",
    "linux",
    "system",
    "see",
    "thunderbird",
    "browser",
    "opens",
    "hadoop",
    "basic",
    "system",
    "browser",
    "go",
    "underneath",
    "hue",
    "comes",
    "default",
    "click",
    "pull",
    "menu",
    "go",
    "editor",
    "see",
    "impala",
    "hive",
    "pig",
    "along",
    "bunch",
    "query",
    "languages",
    "use",
    "going",
    "pig",
    "pig",
    "go",
    "ahead",
    "use",
    "command",
    "line",
    "click",
    "little",
    "blue",
    "button",
    "start",
    "running",
    "actually",
    "working",
    "terminal",
    "window",
    "cloudera",
    "quick",
    "start",
    "open",
    "terminal",
    "window",
    "top",
    "setup",
    "logged",
    "easily",
    "use",
    "commands",
    "terminal",
    "window",
    "zoom",
    "way",
    "get",
    "nice",
    "view",
    "going",
    "go",
    "first",
    "command",
    "going",
    "hadoop",
    "command",
    "import",
    "data",
    "hadoop",
    "system",
    "case",
    "pig",
    "input",
    "let",
    "take",
    "look",
    "hadoop",
    "listen",
    "going",
    "hadoop",
    "command",
    "dfs",
    "actually",
    "four",
    "variations",
    "dfs",
    "hdfs",
    "whatever",
    "fine",
    "four",
    "point",
    "used",
    "different",
    "setups",
    "underneath",
    "different",
    "things",
    "thing",
    "want",
    "put",
    "file",
    "case",
    "home",
    "cloudera",
    "documents",
    "sample",
    "want",
    "take",
    "put",
    "pig",
    "input",
    "let",
    "take",
    "look",
    "file",
    "go",
    "document",
    "browsers",
    "open",
    "see",
    "got",
    "simple",
    "id",
    "name",
    "profession",
    "age",
    "one",
    "jack",
    "engineer",
    "25",
    "one",
    "earlier",
    "things",
    "let",
    "go",
    "ahead",
    "hit",
    "enter",
    "execute",
    "uploaded",
    "data",
    "gone",
    "pig",
    "input",
    "lot",
    "hadoop",
    "commands",
    "mimic",
    "linux",
    "commands",
    "see",
    "cat",
    "one",
    "commands",
    "hyphen",
    "execute",
    "hadoop",
    "dfs",
    "hyphen",
    "cat",
    "slash",
    "pig",
    "input",
    "called",
    "put",
    "sample",
    "csv",
    "execute",
    "see",
    "hadoop",
    "system",
    "going",
    "go",
    "pull",
    "sure",
    "enough",
    "pulls",
    "data",
    "file",
    "put",
    "simply",
    "enter",
    "pig",
    "latin",
    "pig",
    "editor",
    "mode",
    "typing",
    "pig",
    "see",
    "uh",
    "grunt",
    "told",
    "going",
    "tell",
    "pig",
    "latin",
    "grunt",
    "command",
    "line",
    "pig",
    "shell",
    "go",
    "ahead",
    "put",
    "load",
    "command",
    "way",
    "works",
    "going",
    "office",
    "equals",
    "load",
    "load",
    "case",
    "going",
    "pig",
    "input",
    "single",
    "brackets",
    "remember",
    "data",
    "hadoop",
    "file",
    "system",
    "dumped",
    "going",
    "using",
    "pig",
    "storage",
    "data",
    "separated",
    "comma",
    "comma",
    "separator",
    "case",
    "id",
    "character",
    "array",
    "name",
    "character",
    "array",
    "profession",
    "character",
    "age",
    "character",
    "ray",
    "going",
    "character",
    "arrays",
    "keep",
    "simple",
    "one",
    "hit",
    "put",
    "see",
    "full",
    "command",
    "line",
    "going",
    "semicolon",
    "end",
    "hit",
    "enter",
    "set",
    "office",
    "actually",
    "done",
    "anything",
    "yet",
    "anything",
    "dump",
    "office",
    "command",
    "execute",
    "whatever",
    "loaded",
    "whatever",
    "setup",
    "run",
    "see",
    "go",
    "different",
    "languages",
    "going",
    "map",
    "reduce",
    "remember",
    "locally",
    "hadoop",
    "setup",
    "finished",
    "dump",
    "see",
    "id",
    "name",
    "profession",
    "age",
    "information",
    "dumped",
    "pick",
    "oh",
    "let",
    "say",
    "oh",
    "let",
    "say",
    "request",
    "keep",
    "simple",
    "name",
    "age",
    "go",
    "office",
    "call",
    "variable",
    "underscore",
    "say",
    "office",
    "generate",
    "name",
    "comma",
    "h",
    "means",
    "going",
    "row",
    "thinking",
    "map",
    "reduce",
    "know",
    "map",
    "function",
    "mapping",
    "row",
    "generating",
    "name",
    "age",
    "course",
    "want",
    "go",
    "ahead",
    "close",
    "semicolon",
    "created",
    "query",
    "command",
    "line",
    "let",
    "go",
    "ahead",
    "dump",
    "office",
    "underscore",
    "semicolon",
    "go",
    "map",
    "reduce",
    "setup",
    "large",
    "cluster",
    "processing",
    "time",
    "would",
    "happen",
    "fact",
    "really",
    "slow",
    "multiple",
    "things",
    "computer",
    "particular",
    "virtual",
    "box",
    "using",
    "quarter",
    "processor",
    "dedicated",
    "see",
    "name",
    "age",
    "also",
    "included",
    "top",
    "row",
    "since",
    "delete",
    "tell",
    "fine",
    "example",
    "need",
    "aware",
    "things",
    "processing",
    "significantly",
    "large",
    "amount",
    "data",
    "data",
    "also",
    "office",
    "call",
    "dsc",
    "descending",
    "maybe",
    "boss",
    "comes",
    "says",
    "hey",
    "order",
    "office",
    "id",
    "descending",
    "course",
    "boss",
    "taught",
    "uh",
    "shareholder",
    "sounds",
    "little",
    "druggatory",
    "say",
    "boss",
    "talked",
    "shareholder",
    "said",
    "taught",
    "little",
    "bit",
    "pig",
    "latin",
    "know",
    "create",
    "office",
    "description",
    "order",
    "office",
    "id",
    "description",
    "course",
    "dump",
    "office",
    "underscore",
    "description",
    "actually",
    "execute",
    "goes",
    "map",
    "reduce",
    "take",
    "moment",
    "come",
    "running",
    "quarter",
    "processor",
    "see",
    "ids",
    "descending",
    "order",
    "returned",
    "let",
    "also",
    "look",
    "important",
    "anytime",
    "dealing",
    "big",
    "data",
    "let",
    "create",
    "office",
    "limit",
    "course",
    "instead",
    "office",
    "could",
    "office",
    "descending",
    "get",
    "top",
    "two",
    "ids",
    "going",
    "limit",
    "two",
    "course",
    "execute",
    "dump",
    "office",
    "underscore",
    "limit",
    "think",
    "dumping",
    "garbage",
    "pig",
    "pen",
    "pig",
    "eat",
    "go",
    "dump",
    "office",
    "limit",
    "two",
    "going",
    "limit",
    "office",
    "top",
    "two",
    "output",
    "get",
    "first",
    "row",
    "id",
    "name",
    "profession",
    "age",
    "second",
    "row",
    "jack",
    "engineer",
    "let",
    "filter",
    "call",
    "office",
    "underscore",
    "filter",
    "guessed",
    "equals",
    "filter",
    "office",
    "profession",
    "equals",
    "keep",
    "note",
    "uh",
    "similar",
    "python",
    "double",
    "equal",
    "signs",
    "equal",
    "true",
    "false",
    "statement",
    "logic",
    "statement",
    "remember",
    "use",
    "two",
    "equal",
    "signs",
    "pig",
    "going",
    "say",
    "equals",
    "doctor",
    "want",
    "find",
    "many",
    "doctors",
    "list",
    "go",
    "ahead",
    "dump",
    "dumping",
    "garbage",
    "pig",
    "pen",
    "letting",
    "pig",
    "take",
    "see",
    "find",
    "see",
    "doctor",
    "list",
    "find",
    "uh",
    "employee",
    "id",
    "number",
    "two",
    "bob",
    "doctor",
    "30",
    "years",
    "old",
    "next",
    "section",
    "going",
    "cover",
    "something",
    "see",
    "lot",
    "nowadays",
    "data",
    "analysis",
    "word",
    "counting",
    "tokenization",
    "one",
    "next",
    "big",
    "steps",
    "move",
    "forward",
    "data",
    "analysis",
    "go",
    "say",
    "stock",
    "market",
    "analysis",
    "highs",
    "lows",
    "numbers",
    "people",
    "saying",
    "companies",
    "twitter",
    "saying",
    "web",
    "pages",
    "facebook",
    "suddenly",
    "need",
    "start",
    "counting",
    "words",
    "finding",
    "many",
    "words",
    "totaled",
    "many",
    "first",
    "part",
    "document",
    "going",
    "cover",
    "basic",
    "word",
    "count",
    "example",
    "case",
    "created",
    "document",
    "called",
    "see",
    "simplylearn",
    "company",
    "supporting",
    "online",
    "learning",
    "simplylearn",
    "helps",
    "people",
    "attain",
    "certifications",
    "simplylearn",
    "online",
    "community",
    "love",
    "simply",
    "learn",
    "love",
    "programming",
    "love",
    "data",
    "analysis",
    "went",
    "saved",
    "documents",
    "folder",
    "could",
    "use",
    "let",
    "go",
    "ahead",
    "open",
    "new",
    "terminal",
    "window",
    "word",
    "count",
    "let",
    "go",
    "close",
    "old",
    "one",
    "going",
    "go",
    "instead",
    "pig",
    "going",
    "pig",
    "minus",
    "x",
    "local",
    "telling",
    "pig",
    "start",
    "pig",
    "shell",
    "going",
    "looking",
    "files",
    "local",
    "virtual",
    "box",
    "centos",
    "machine",
    "let",
    "go",
    "ahead",
    "hit",
    "enter",
    "maximize",
    "go",
    "load",
    "pig",
    "going",
    "look",
    "pig",
    "defaulted",
    "hi",
    "hadoop",
    "system",
    "hdfs",
    "defaulted",
    "local",
    "system",
    "going",
    "create",
    "lines",
    "going",
    "load",
    "straight",
    "file",
    "remember",
    "last",
    "time",
    "took",
    "hdfs",
    "loaded",
    "loaded",
    "pig",
    "since",
    "going",
    "local",
    "going",
    "run",
    "local",
    "script",
    "lines",
    "equals",
    "load",
    "home",
    "actual",
    "full",
    "path",
    "home",
    "cloud",
    "area",
    "documents",
    "called",
    "line",
    "character",
    "array",
    "line",
    "actually",
    "change",
    "read",
    "document",
    "certainly",
    "done",
    "lot",
    "document",
    "analysis",
    "go",
    "word",
    "counts",
    "different",
    "kind",
    "counts",
    "go",
    "ahead",
    "create",
    "line",
    "instead",
    "dump",
    "going",
    "go",
    "ahead",
    "start",
    "entering",
    "different",
    "setups",
    "steps",
    "want",
    "go",
    "let",
    "take",
    "look",
    "next",
    "one",
    "load",
    "straightforward",
    "loading",
    "particular",
    "file",
    "since",
    "locals",
    "loading",
    "directly",
    "instead",
    "going",
    "hadoop",
    "file",
    "system",
    "says",
    "line",
    "read",
    "character",
    "array",
    "going",
    "words",
    "equal",
    "lines",
    "generate",
    "flat",
    "tokenize",
    "line",
    "space",
    "word",
    "lot",
    "ways",
    "programmer",
    "splitting",
    "line",
    "spaces",
    "actual",
    "ways",
    "tokenize",
    "got",
    "ta",
    "look",
    "periods",
    "capitalization",
    "kinds",
    "things",
    "play",
    "basic",
    "word",
    "count",
    "going",
    "separate",
    "spaces",
    "flatten",
    "takes",
    "line",
    "creates",
    "flattens",
    "words",
    "uh",
    "going",
    "generate",
    "bunch",
    "words",
    "line",
    "words",
    "word",
    "little",
    "confusing",
    "really",
    "think",
    "going",
    "line",
    "separating",
    "generating",
    "list",
    "words",
    "one",
    "thing",
    "note",
    "default",
    "tokenize",
    "tokenized",
    "line",
    "without",
    "space",
    "automatically",
    "tokenize",
    "space",
    "either",
    "one",
    "going",
    "group",
    "going",
    "group",
    "words",
    "going",
    "group",
    "words",
    "word",
    "split",
    "token",
    "word",
    "list",
    "words",
    "going",
    "grouped",
    "equals",
    "group",
    "words",
    "word",
    "going",
    "group",
    "words",
    "together",
    "going",
    "group",
    "want",
    "go",
    "ahead",
    "count",
    "count",
    "go",
    "ahead",
    "create",
    "word",
    "count",
    "variable",
    "four",
    "group",
    "grouped",
    "line",
    "group",
    "words",
    "line",
    "similar",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "group",
    "line",
    "group",
    "words",
    "together",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "want",
    "know",
    "word",
    "count",
    "comes",
    "back",
    "word",
    "count",
    "finally",
    "want",
    "take",
    "want",
    "go",
    "ahead",
    "dump",
    "word",
    "count",
    "little",
    "bit",
    "see",
    "start",
    "looking",
    "grunt",
    "scripts",
    "see",
    "right",
    "lines",
    "right",
    "steps",
    "take",
    "get",
    "load",
    "file",
    "lines",
    "going",
    "generate",
    "tokenize",
    "words",
    "going",
    "take",
    "words",
    "going",
    "group",
    "words",
    "group",
    "going",
    "generate",
    "group",
    "going",
    "count",
    "words",
    "going",
    "summarize",
    "words",
    "let",
    "go",
    "ahead",
    "dump",
    "word",
    "count",
    "executes",
    "goes",
    "mapreduce",
    "actually",
    "local",
    "runner",
    "see",
    "start",
    "seeing",
    "still",
    "mapreduce",
    "special",
    "runner",
    "mapping",
    "part",
    "row",
    "counted",
    "grouped",
    "word",
    "count",
    "reducer",
    "reducer",
    "creates",
    "keys",
    "see",
    "used",
    "three",
    "times",
    "came",
    "came",
    "continue",
    "attain",
    "online",
    "people",
    "company",
    "analysis",
    "simply",
    "learn",
    "took",
    "top",
    "rating",
    "four",
    "certification",
    "things",
    "encountered",
    "many",
    "words",
    "used",
    "uh",
    "data",
    "analysis",
    "probably",
    "beginnings",
    "data",
    "analysis",
    "might",
    "look",
    "say",
    "oh",
    "mentioned",
    "love",
    "three",
    "times",
    "whatever",
    "going",
    "post",
    "love",
    "uh",
    "love",
    "might",
    "attach",
    "different",
    "objects",
    "see",
    "uh",
    "pig",
    "latin",
    "fairly",
    "easy",
    "use",
    "nothing",
    "really",
    "know",
    "might",
    "takes",
    "little",
    "bit",
    "learn",
    "script",
    "uh",
    "depending",
    "good",
    "memory",
    "get",
    "older",
    "memory",
    "leaks",
    "little",
    "bit",
    "memorize",
    "much",
    "pretty",
    "straightforward",
    "script",
    "put",
    "goes",
    "full",
    "map",
    "reduce",
    "localized",
    "run",
    "comes",
    "like",
    "said",
    "easy",
    "use",
    "people",
    "like",
    "pig",
    "latin",
    "intuitive",
    "one",
    "things",
    "like",
    "pig",
    "latin",
    "troubleshooting",
    "troubleshooting",
    "lot",
    "times",
    "working",
    "small",
    "amount",
    "data",
    "start",
    "one",
    "line",
    "time",
    "go",
    "lines",
    "equal",
    "load",
    "loaded",
    "text",
    "maybe",
    "dump",
    "lines",
    "going",
    "run",
    "going",
    "show",
    "lines",
    "working",
    "small",
    "amount",
    "data",
    "way",
    "test",
    "got",
    "error",
    "said",
    "oh",
    "working",
    "maybe",
    "oh",
    "gosh",
    "map",
    "reduce",
    "basic",
    "grunt",
    "shell",
    "instead",
    "local",
    "path",
    "grunt",
    "maybe",
    "generate",
    "error",
    "see",
    "shows",
    "lines",
    "going",
    "hive",
    "versus",
    "pig",
    "one",
    "side",
    "sharp",
    "stinger",
    "black",
    "yellow",
    "friend",
    "side",
    "thick",
    "hide",
    "pig",
    "let",
    "start",
    "introduction",
    "hbase",
    "back",
    "days",
    "data",
    "used",
    "less",
    "mostly",
    "structured",
    "see",
    "structured",
    "data",
    "usually",
    "like",
    "database",
    "uh",
    "every",
    "field",
    "exactly",
    "correct",
    "length",
    "name",
    "field",
    "exactly",
    "32",
    "characters",
    "remember",
    "old",
    "access",
    "database",
    "microsoft",
    "files",
    "small",
    "know",
    "hundreds",
    "people",
    "one",
    "database",
    "considered",
    "big",
    "data",
    "data",
    "could",
    "easily",
    "stored",
    "relational",
    "database",
    "rdbms",
    "talk",
    "relational",
    "database",
    "might",
    "think",
    "oracle",
    "might",
    "think",
    "sql",
    "microsoft",
    "sql",
    "mysql",
    "evolved",
    "even",
    "back",
    "lot",
    "today",
    "still",
    "fall",
    "short",
    "lot",
    "ways",
    "examples",
    "rdms",
    "relationship",
    "database",
    "internet",
    "evolved",
    "huge",
    "volumes",
    "structured",
    "data",
    "got",
    "generated",
    "see",
    "data",
    "email",
    "look",
    "spam",
    "filter",
    "know",
    "talking",
    "html",
    "pages",
    "xml",
    "lot",
    "time",
    "displayed",
    "html",
    "help",
    "desk",
    "pages",
    "json",
    "really",
    "even",
    "last",
    "year",
    "almost",
    "doubles",
    "year",
    "much",
    "generated",
    "storing",
    "processing",
    "data",
    "rdbms",
    "become",
    "major",
    "problem",
    "solution",
    "use",
    "apache",
    "hbase",
    "apache",
    "hbase",
    "solution",
    "let",
    "take",
    "look",
    "history",
    "hbase",
    "history",
    "look",
    "hbase",
    "history",
    "going",
    "start",
    "back",
    "2006",
    "november",
    "google",
    "released",
    "paper",
    "big",
    "table",
    "2017",
    "months",
    "later",
    "hbase",
    "prototype",
    "created",
    "hadoop",
    "contribution",
    "later",
    "year",
    "2007",
    "october",
    "first",
    "usable",
    "hbase",
    "along",
    "hadoop",
    "released",
    "january",
    "2008",
    "hbase",
    "became",
    "subproject",
    "hadoop",
    "later",
    "year",
    "october",
    "way",
    "september",
    "next",
    "year",
    "hbase",
    "released",
    "version",
    "version",
    "finally",
    "may",
    "2010",
    "hbase",
    "became",
    "apache",
    "top",
    "level",
    "project",
    "see",
    "course",
    "four",
    "years",
    "hbase",
    "started",
    "idea",
    "paper",
    "evolved",
    "way",
    "till",
    "2010",
    "solid",
    "project",
    "apache",
    "since",
    "2010",
    "continued",
    "evolve",
    "grow",
    "major",
    "source",
    "storing",
    "data",
    "data",
    "hbase",
    "hbase",
    "database",
    "management",
    "system",
    "derived",
    "google",
    "nosql",
    "database",
    "big",
    "table",
    "runs",
    "top",
    "hadoop",
    "file",
    "system",
    "hdfs",
    "open",
    "source",
    "project",
    "horizontally",
    "scalable",
    "important",
    "understand",
    "buy",
    "bunch",
    "huge",
    "expensive",
    "computers",
    "expanding",
    "continually",
    "adding",
    "commodity",
    "machines",
    "linear",
    "cost",
    "expansion",
    "opposed",
    "exponential",
    "sql",
    "database",
    "written",
    "java",
    "permits",
    "faster",
    "querying",
    "java",
    "backend",
    "hbase",
    "setup",
    "well",
    "suited",
    "sparse",
    "data",
    "sets",
    "contain",
    "missing",
    "n",
    "values",
    "boggle",
    "like",
    "would",
    "another",
    "database",
    "companies",
    "using",
    "hbase",
    "let",
    "take",
    "look",
    "see",
    "using",
    "nosql",
    "database",
    "servers",
    "storing",
    "data",
    "hortonworks",
    "surprise",
    "one",
    "like",
    "cloudera",
    "hortonworks",
    "behind",
    "hadoop",
    "one",
    "big",
    "developments",
    "backing",
    "course",
    "apache",
    "hbase",
    "open",
    "source",
    "behind",
    "capital",
    "one",
    "banks",
    "also",
    "see",
    "bank",
    "america",
    "collecting",
    "information",
    "people",
    "tracking",
    "information",
    "might",
    "sparse",
    "might",
    "one",
    "bank",
    "way",
    "back",
    "collected",
    "information",
    "far",
    "person",
    "family",
    "income",
    "whole",
    "family",
    "personal",
    "income",
    "maybe",
    "another",
    "one",
    "collect",
    "family",
    "income",
    "start",
    "seeing",
    "data",
    "difficult",
    "store",
    "missing",
    "bunch",
    "data",
    "hubspot",
    "using",
    "facebook",
    "certainly",
    "facebook",
    "twitter",
    "social",
    "medias",
    "using",
    "course",
    "jpmorgan",
    "chase",
    "company",
    "another",
    "bank",
    "uses",
    "hbase",
    "data",
    "warehouse",
    "nosql",
    "let",
    "take",
    "look",
    "hbase",
    "use",
    "case",
    "dig",
    "little",
    "bit",
    "see",
    "functions",
    "telecommunication",
    "company",
    "provides",
    "mobile",
    "voice",
    "multimedia",
    "services",
    "across",
    "china",
    "china",
    "mobile",
    "china",
    "mobile",
    "generate",
    "billions",
    "call",
    "detailed",
    "records",
    "cdr",
    "cdrs",
    "records",
    "calls",
    "long",
    "different",
    "aspects",
    "call",
    "maybe",
    "tower",
    "broadcasted",
    "recorded",
    "track",
    "traditional",
    "database",
    "systems",
    "unable",
    "scale",
    "vast",
    "volumes",
    "data",
    "provide",
    "solution",
    "good",
    "storing",
    "analysis",
    "billions",
    "call",
    "records",
    "major",
    "problem",
    "company",
    "solution",
    "apache",
    "hbase",
    "hbase",
    "stores",
    "billions",
    "rows",
    "detailed",
    "call",
    "records",
    "hbc",
    "performs",
    "fast",
    "processing",
    "records",
    "using",
    "sql",
    "queries",
    "mix",
    "sql",
    "sql",
    "queries",
    "usually",
    "say",
    "sql",
    "queries",
    "way",
    "query",
    "works",
    "applications",
    "hbase",
    "one",
    "would",
    "medical",
    "industry",
    "hbase",
    "used",
    "storing",
    "genome",
    "sequences",
    "storing",
    "disease",
    "history",
    "people",
    "area",
    "imagine",
    "sparse",
    "far",
    "genome",
    "sequence",
    "might",
    "pieces",
    "person",
    "unique",
    "unique",
    "different",
    "people",
    "thing",
    "disease",
    "really",
    "need",
    "column",
    "every",
    "possible",
    "disease",
    "person",
    "could",
    "get",
    "want",
    "know",
    "diseases",
    "people",
    "deal",
    "area",
    "hbase",
    "used",
    "storing",
    "logs",
    "customer",
    "search",
    "history",
    "performs",
    "analytics",
    "target",
    "advertisement",
    "better",
    "business",
    "insights",
    "sports",
    "hba",
    "stores",
    "match",
    "details",
    "history",
    "match",
    "uses",
    "data",
    "better",
    "prediction",
    "look",
    "hbase",
    "want",
    "know",
    "difference",
    "hbase",
    "versus",
    "rdbms",
    "relational",
    "database",
    "management",
    "system",
    "hbase",
    "versus",
    "rdbms",
    "hbase",
    "fixed",
    "schema",
    "defines",
    "column",
    "families",
    "show",
    "means",
    "later",
    "rdbms",
    "fixed",
    "schema",
    "describes",
    "structure",
    "tables",
    "think",
    "row",
    "columns",
    "column",
    "specific",
    "structure",
    "much",
    "data",
    "go",
    "hbase",
    "works",
    "well",
    "structured",
    "data",
    "rdbms",
    "works",
    "well",
    "structured",
    "data",
    "hbase",
    "denormalized",
    "data",
    "contain",
    "missing",
    "null",
    "values",
    "rdbms",
    "store",
    "normalized",
    "data",
    "still",
    "store",
    "null",
    "value",
    "rdbms",
    "still",
    "takes",
    "space",
    "storing",
    "regular",
    "value",
    "many",
    "cases",
    "also",
    "hbase",
    "built",
    "tables",
    "scaled",
    "horizontally",
    "instance",
    "tokenizer",
    "words",
    "word",
    "clusters",
    "might",
    "million",
    "different",
    "words",
    "pulling",
    "combinations",
    "words",
    "rdbms",
    "built",
    "thin",
    "tables",
    "hard",
    "scale",
    "want",
    "store",
    "million",
    "columns",
    "sql",
    "going",
    "crash",
    "going",
    "hard",
    "searches",
    "age",
    "base",
    "stores",
    "data",
    "part",
    "whatever",
    "row",
    "working",
    "let",
    "look",
    "features",
    "hbase",
    "scalable",
    "data",
    "scaled",
    "across",
    "various",
    "nodes",
    "stored",
    "hdfs",
    "always",
    "think",
    "linear",
    "terabyte",
    "data",
    "adding",
    "roughly",
    "thousand",
    "dollars",
    "commodity",
    "computing",
    "enterprise",
    "machine",
    "looking",
    "10",
    "000",
    "lower",
    "end",
    "terabyte",
    "data",
    "includes",
    "backup",
    "redundancy",
    "big",
    "difference",
    "like",
    "tenth",
    "cost",
    "store",
    "across",
    "hbase",
    "automatic",
    "failure",
    "support",
    "right",
    "ahead",
    "log",
    "across",
    "clusters",
    "provides",
    "automatic",
    "support",
    "failure",
    "consistent",
    "read",
    "write",
    "hbase",
    "provides",
    "consistent",
    "read",
    "write",
    "data",
    "java",
    "api",
    "client",
    "access",
    "provides",
    "easy",
    "use",
    "java",
    "api",
    "clients",
    "block",
    "cache",
    "bloom",
    "filters",
    "hbase",
    "supports",
    "block",
    "caching",
    "bloom",
    "filters",
    "high",
    "volume",
    "query",
    "optimization",
    "let",
    "dig",
    "little",
    "deeper",
    "hbase",
    "storage",
    "hbase",
    "column",
    "oriented",
    "storage",
    "told",
    "going",
    "look",
    "see",
    "stores",
    "data",
    "see",
    "row",
    "key",
    "really",
    "one",
    "important",
    "references",
    "row",
    "key",
    "row",
    "id",
    "column",
    "family",
    "see",
    "column",
    "family",
    "one",
    "column",
    "family",
    "two",
    "column",
    "family",
    "three",
    "column",
    "qualifiers",
    "column",
    "family",
    "one",
    "three",
    "columns",
    "might",
    "data",
    "go",
    "column",
    "family",
    "one",
    "query",
    "every",
    "column",
    "contains",
    "certain",
    "thing",
    "row",
    "might",
    "anything",
    "queried",
    "column",
    "family",
    "two",
    "maybe",
    "column",
    "1",
    "filled",
    "column",
    "3",
    "filled",
    "forth",
    "cell",
    "connected",
    "row",
    "data",
    "actually",
    "stored",
    "let",
    "take",
    "look",
    "looks",
    "like",
    "fill",
    "data",
    "row",
    "key",
    "row",
    "id",
    "employee",
    "id",
    "one",
    "two",
    "three",
    "pretty",
    "straightforward",
    "probably",
    "would",
    "even",
    "sql",
    "server",
    "column",
    "family",
    "starts",
    "really",
    "separating",
    "column",
    "family",
    "might",
    "personal",
    "data",
    "personal",
    "data",
    "would",
    "name",
    "city",
    "age",
    "might",
    "lot",
    "might",
    "number",
    "children",
    "might",
    "degree",
    "kinds",
    "different",
    "things",
    "go",
    "personal",
    "data",
    "might",
    "missing",
    "might",
    "name",
    "age",
    "employee",
    "might",
    "name",
    "city",
    "many",
    "children",
    "age",
    "see",
    "personal",
    "data",
    "collect",
    "large",
    "variety",
    "data",
    "store",
    "hbase",
    "easily",
    "maybe",
    "family",
    "professional",
    "data",
    "designation",
    "salary",
    "stuff",
    "employee",
    "company",
    "let",
    "dig",
    "little",
    "deeper",
    "hbase",
    "architecture",
    "see",
    "looks",
    "complicated",
    "chart",
    "complicated",
    "think",
    "apache",
    "space",
    "zookeeper",
    "used",
    "monitoring",
    "going",
    "h",
    "master",
    "hbase",
    "master",
    "assigns",
    "regions",
    "load",
    "balancing",
    "underneath",
    "region",
    "hbase",
    "master",
    "h",
    "master",
    "hbase",
    "master",
    "reader",
    "server",
    "serves",
    "data",
    "read",
    "write",
    "region",
    "server",
    "different",
    "computers",
    "hadoop",
    "cluster",
    "region",
    "h",
    "log",
    "store",
    "memory",
    "store",
    "different",
    "files",
    "h",
    "file",
    "stored",
    "separated",
    "across",
    "different",
    "computers",
    "part",
    "hdfs",
    "storage",
    "system",
    "look",
    "architectural",
    "components",
    "regions",
    "looking",
    "drilling",
    "little",
    "bit",
    "hbase",
    "tables",
    "divided",
    "horizontally",
    "row",
    "key",
    "range",
    "regions",
    "ids",
    "might",
    "ids",
    "1",
    "20",
    "21",
    "50",
    "whatever",
    "regions",
    "assigned",
    "nodes",
    "cluster",
    "called",
    "region",
    "servers",
    "region",
    "contains",
    "rows",
    "table",
    "region",
    "start",
    "key",
    "end",
    "key",
    "1",
    "10",
    "11",
    "20",
    "forth",
    "servers",
    "serve",
    "data",
    "read",
    "write",
    "see",
    "client",
    "get",
    "git",
    "sends",
    "finds",
    "start",
    "start",
    "keys",
    "n",
    "keys",
    "pulls",
    "data",
    "different",
    "region",
    "server",
    "region",
    "sign",
    "data",
    "definition",
    "language",
    "operation",
    "create",
    "delete",
    "handled",
    "h",
    "master",
    "telling",
    "data",
    "going",
    "assigning",
    "reassigning",
    "regions",
    "recovery",
    "load",
    "balancing",
    "monitoring",
    "servers",
    "also",
    "part",
    "know",
    "ids",
    "500",
    "ids",
    "across",
    "three",
    "servers",
    "going",
    "put",
    "400",
    "ids",
    "server",
    "1",
    "100",
    "server",
    "2",
    "leaves",
    "region",
    "3",
    "region",
    "4",
    "empty",
    "going",
    "split",
    "handled",
    "h",
    "master",
    "see",
    "monitors",
    "region",
    "servers",
    "assigns",
    "regions",
    "region",
    "servers",
    "assigns",
    "regions",
    "recent",
    "servers",
    "forth",
    "forth",
    "hbase",
    "distributed",
    "environment",
    "alone",
    "sufficient",
    "manage",
    "everything",
    "hence",
    "zookeeper",
    "introduced",
    "works",
    "active",
    "sends",
    "heartbeat",
    "signal",
    "zookeeper",
    "indicating",
    "active",
    "zookeeper",
    "also",
    "heartbeat",
    "region",
    "servers",
    "region",
    "servers",
    "send",
    "status",
    "zoo",
    "keeper",
    "indicating",
    "ready",
    "read",
    "write",
    "operation",
    "inactive",
    "server",
    "acts",
    "backup",
    "active",
    "hmaster",
    "fails",
    "come",
    "rescue",
    "active",
    "hmaster",
    "region",
    "servers",
    "connect",
    "session",
    "zookeeper",
    "see",
    "active",
    "hmaster",
    "selection",
    "region",
    "server",
    "session",
    "looking",
    "zookeeper",
    "keeping",
    "pulse",
    "active",
    "region",
    "server",
    "connects",
    "session",
    "zoo",
    "keeper",
    "see",
    "ephemeral",
    "nodes",
    "active",
    "sessions",
    "via",
    "heartbeats",
    "indicate",
    "region",
    "servers",
    "running",
    "let",
    "take",
    "look",
    "hbase",
    "read",
    "write",
    "going",
    "special",
    "hbase",
    "catalog",
    "table",
    "called",
    "meta",
    "table",
    "holds",
    "location",
    "regions",
    "cluster",
    "happens",
    "first",
    "time",
    "client",
    "reads",
    "writes",
    "data",
    "hbase",
    "client",
    "gets",
    "region",
    "server",
    "host",
    "meta",
    "table",
    "zookeeper",
    "see",
    "right",
    "client",
    "request",
    "region",
    "server",
    "goes",
    "hey",
    "zookeeper",
    "handle",
    "zookeeper",
    "takes",
    "look",
    "goes",
    "ah",
    "middle",
    "location",
    "stored",
    "zookeeper",
    "looks",
    "meta",
    "data",
    "metadata",
    "table",
    "location",
    "sent",
    "back",
    "client",
    "client",
    "query",
    "meta",
    "server",
    "get",
    "region",
    "server",
    "corresponding",
    "row",
    "key",
    "wants",
    "access",
    "client",
    "caches",
    "information",
    "along",
    "minute",
    "table",
    "location",
    "see",
    "client",
    "going",
    "back",
    "forth",
    "region",
    "server",
    "information",
    "might",
    "going",
    "across",
    "multiple",
    "region",
    "servers",
    "depending",
    "querying",
    "get",
    "region",
    "server",
    "row",
    "key",
    "meta",
    "table",
    "row",
    "key",
    "comes",
    "says",
    "hey",
    "going",
    "gets",
    "row",
    "key",
    "corresponding",
    "region",
    "server",
    "put",
    "row",
    "git",
    "row",
    "region",
    "server",
    "let",
    "take",
    "look",
    "hbase",
    "meta",
    "table",
    "special",
    "hbase",
    "catalog",
    "table",
    "maintains",
    "list",
    "region",
    "servers",
    "hbase",
    "storage",
    "system",
    "see",
    "meta",
    "table",
    "row",
    "key",
    "value",
    "table",
    "key",
    "region",
    "region",
    "server",
    "meta",
    "table",
    "used",
    "find",
    "region",
    "given",
    "table",
    "key",
    "see",
    "know",
    "meta",
    "table",
    "comes",
    "going",
    "fire",
    "going",
    "region",
    "server",
    "look",
    "little",
    "closer",
    "right",
    "mechanism",
    "hbase",
    "right",
    "ahead",
    "log",
    "wall",
    "abbreviate",
    "kind",
    "way",
    "remember",
    "wall",
    "right",
    "ahead",
    "log",
    "file",
    "used",
    "store",
    "new",
    "data",
    "yet",
    "put",
    "permanent",
    "storage",
    "used",
    "recovery",
    "case",
    "failure",
    "see",
    "client",
    "comes",
    "literally",
    "puts",
    "new",
    "data",
    "coming",
    "kind",
    "temporary",
    "storage",
    "wall",
    "gone",
    "wall",
    "memory",
    "store",
    "memstor",
    "right",
    "cache",
    "stores",
    "new",
    "data",
    "yet",
    "written",
    "disk",
    "one",
    "mems",
    "store",
    "per",
    "column",
    "family",
    "per",
    "region",
    "done",
    "three",
    "ack",
    "data",
    "placed",
    "mems",
    "store",
    "client",
    "receives",
    "acknowledgement",
    "mems",
    "store",
    "reaches",
    "threshold",
    "dumps",
    "commits",
    "data",
    "h",
    "file",
    "see",
    "right",
    "taken",
    "gun",
    "wall",
    "wall",
    "source",
    "different",
    "memory",
    "stores",
    "uh",
    "memory",
    "stores",
    "says",
    "hey",
    "reached",
    "ready",
    "dump",
    "h",
    "files",
    "moves",
    "h",
    "files",
    "h",
    "files",
    "store",
    "rows",
    "data",
    "stored",
    "key",
    "value",
    "disk",
    "done",
    "lot",
    "theory",
    "let",
    "dive",
    "take",
    "look",
    "see",
    "commands",
    "look",
    "like",
    "happens",
    "age",
    "base",
    "manipulating",
    "nosql",
    "setup",
    "music",
    "learning",
    "new",
    "setup",
    "always",
    "good",
    "start",
    "coming",
    "open",
    "source",
    "apache",
    "go",
    "see",
    "lot",
    "information",
    "actually",
    "download",
    "hbase",
    "separate",
    "hadoop",
    "although",
    "people",
    "install",
    "hadoop",
    "bundled",
    "go",
    "find",
    "reference",
    "guide",
    "go",
    "apache",
    "reference",
    "guide",
    "number",
    "things",
    "look",
    "going",
    "going",
    "apache",
    "hbase",
    "shell",
    "going",
    "working",
    "lot",
    "interfaces",
    "setup",
    "look",
    "lot",
    "different",
    "commands",
    "go",
    "apache",
    "hbase",
    "reference",
    "guide",
    "go",
    "read",
    "hbase",
    "shell",
    "commands",
    "command",
    "file",
    "see",
    "gives",
    "different",
    "options",
    "formats",
    "putting",
    "data",
    "listing",
    "data",
    "certainly",
    "also",
    "create",
    "files",
    "scripts",
    "going",
    "look",
    "basics",
    "going",
    "go",
    "basic",
    "hbase",
    "shell",
    "one",
    "last",
    "thing",
    "look",
    "course",
    "continue",
    "setup",
    "see",
    "detail",
    "far",
    "create",
    "get",
    "data",
    "hbase",
    "working",
    "virtual",
    "box",
    "oracle",
    "download",
    "oracle",
    "virtual",
    "box",
    "put",
    "note",
    "youtube",
    "previous",
    "session",
    "setting",
    "virtual",
    "setup",
    "run",
    "hadoop",
    "system",
    "using",
    "cloudera",
    "quick",
    "start",
    "installed",
    "hortons",
    "also",
    "use",
    "amazon",
    "web",
    "service",
    "number",
    "options",
    "trying",
    "case",
    "cloudera",
    "oracle",
    "virtualbox",
    "virtual",
    "box",
    "linux",
    "centos",
    "installed",
    "hadoop",
    "different",
    "hadoop",
    "flavors",
    "including",
    "hbase",
    "bring",
    "computer",
    "windows",
    "10",
    "operating",
    "system",
    "virtual",
    "box",
    "linux",
    "looking",
    "hbase",
    "data",
    "warehouse",
    "three",
    "different",
    "entities",
    "running",
    "computer",
    "confusing",
    "first",
    "time",
    "working",
    "kind",
    "setup",
    "notice",
    "cloudera",
    "setup",
    "actually",
    "hbase",
    "monitoring",
    "go",
    "underneath",
    "click",
    "hbase",
    "master",
    "tell",
    "going",
    "region",
    "servers",
    "tell",
    "going",
    "backup",
    "tables",
    "right",
    "user",
    "tables",
    "created",
    "single",
    "node",
    "single",
    "hbase",
    "tour",
    "gon",
    "na",
    "expect",
    "anything",
    "extensive",
    "since",
    "practice",
    "education",
    "perhaps",
    "testing",
    "package",
    "working",
    "really",
    "deploy",
    "cloudera",
    "course",
    "talk",
    "quick",
    "start",
    "single",
    "node",
    "setup",
    "really",
    "go",
    "different",
    "hbase",
    "see",
    "kinds",
    "different",
    "information",
    "zookeeper",
    "saw",
    "flash",
    "version",
    "working",
    "since",
    "zookeeper",
    "part",
    "hbase",
    "setup",
    "want",
    "go",
    "want",
    "open",
    "terminal",
    "window",
    "cloudera",
    "happens",
    "top",
    "click",
    "see",
    "cloudera",
    "terminal",
    "window",
    "open",
    "let",
    "expand",
    "nice",
    "full",
    "screen",
    "also",
    "going",
    "zoom",
    "way",
    "nice",
    "big",
    "picture",
    "see",
    "typing",
    "going",
    "open",
    "hbase",
    "shell",
    "simply",
    "type",
    "hbase",
    "shell",
    "get",
    "hit",
    "enter",
    "see",
    "takes",
    "moment",
    "load",
    "age",
    "based",
    "shell",
    "hbase",
    "commands",
    "gotten",
    "hbase",
    "shell",
    "see",
    "hbase",
    "prompt",
    "information",
    "ahead",
    "something",
    "simple",
    "like",
    "list",
    "going",
    "list",
    "whatever",
    "tables",
    "happens",
    "base",
    "table",
    "comes",
    "hbase",
    "go",
    "ahead",
    "create",
    "going",
    "type",
    "create",
    "nice",
    "going",
    "throw",
    "kind",
    "going",
    "say",
    "hey",
    "straight",
    "create",
    "come",
    "tell",
    "different",
    "formats",
    "use",
    "create",
    "create",
    "table",
    "one",
    "families",
    "add",
    "splits",
    "names",
    "versions",
    "kinds",
    "things",
    "let",
    "start",
    "basic",
    "one",
    "let",
    "go",
    "ahead",
    "create",
    "call",
    "new",
    "table",
    "let",
    "call",
    "new",
    "tbl",
    "table",
    "new",
    "table",
    "also",
    "want",
    "let",
    "knowledge",
    "let",
    "take",
    "look",
    "creating",
    "new",
    "table",
    "going",
    "family",
    "knowledge",
    "let",
    "hit",
    "enter",
    "going",
    "come",
    "going",
    "take",
    "second",
    "go",
    "ahead",
    "create",
    "new",
    "table",
    "go",
    "list",
    "see",
    "table",
    "new",
    "table",
    "see",
    "new",
    "table",
    "course",
    "default",
    "table",
    "set",
    "something",
    "like",
    "uh",
    "describe",
    "describe",
    "going",
    "new",
    "tbl",
    "describe",
    "going",
    "come",
    "going",
    "say",
    "hey",
    "name",
    "knowledge",
    "data",
    "block",
    "encoding",
    "none",
    "bloom",
    "filter",
    "row",
    "replication",
    "go",
    "version",
    "different",
    "information",
    "need",
    "new",
    "minimum",
    "version",
    "zero",
    "forever",
    "deleted",
    "cells",
    "false",
    "block",
    "size",
    "memory",
    "look",
    "stuff",
    "really",
    "track",
    "one",
    "things",
    "important",
    "note",
    "versions",
    "different",
    "versions",
    "data",
    "stored",
    "always",
    "important",
    "understand",
    "might",
    "talk",
    "little",
    "bit",
    "later",
    "describe",
    "also",
    "status",
    "status",
    "says",
    "one",
    "active",
    "master",
    "going",
    "hbase",
    "whole",
    "status",
    "summary",
    "thing",
    "status",
    "got",
    "thing",
    "coming",
    "created",
    "let",
    "go",
    "ahead",
    "put",
    "something",
    "going",
    "put",
    "new",
    "tbl",
    "want",
    "row",
    "one",
    "know",
    "even",
    "let",
    "type",
    "put",
    "see",
    "type",
    "put",
    "gives",
    "us",
    "like",
    "lot",
    "different",
    "options",
    "works",
    "different",
    "ways",
    "formatting",
    "data",
    "goes",
    "usually",
    "begin",
    "new",
    "table",
    "new",
    "tbl",
    "case",
    "call",
    "row",
    "one",
    "knowledge",
    "remember",
    "created",
    "knowledge",
    "already",
    "knowledge",
    "sports",
    "knowledge",
    "sports",
    "gon",
    "na",
    "set",
    "equal",
    "cricket",
    "gon",
    "na",
    "put",
    "underneath",
    "uh",
    "knowledge",
    "setup",
    "thing",
    "called",
    "sports",
    "see",
    "looks",
    "like",
    "second",
    "let",
    "go",
    "ahead",
    "put",
    "couple",
    "let",
    "see",
    "let",
    "another",
    "row",
    "one",
    "time",
    "set",
    "sports",
    "let",
    "science",
    "know",
    "person",
    "know",
    "row",
    "one",
    "knowledgeable",
    "cricket",
    "also",
    "chemistry",
    "chemist",
    "plays",
    "cricket",
    "row",
    "one",
    "uh",
    "let",
    "see",
    "let",
    "another",
    "row",
    "one",
    "keep",
    "going",
    "science",
    "case",
    "let",
    "physics",
    "chemistry",
    "also",
    "physicist",
    "quite",
    "joy",
    "physics",
    "go",
    "uh",
    "row",
    "one",
    "go",
    "let",
    "uh",
    "row",
    "two",
    "let",
    "see",
    "looks",
    "like",
    "start",
    "putting",
    "row",
    "two",
    "row",
    "two",
    "person",
    "knowledge",
    "economics",
    "master",
    "business",
    "maybe",
    "global",
    "economics",
    "maybe",
    "business",
    "fits",
    "country",
    "economics",
    "call",
    "macro",
    "economics",
    "guess",
    "whole",
    "country",
    "knowledge",
    "economics",
    "macroeconomics",
    "let",
    "one",
    "keep",
    "row",
    "two",
    "time",
    "economist",
    "also",
    "musician",
    "put",
    "music",
    "happen",
    "knowledge",
    "enjoy",
    "oh",
    "let",
    "pop",
    "music",
    "current",
    "pop",
    "music",
    "going",
    "loaded",
    "database",
    "see",
    "two",
    "rows",
    "row",
    "one",
    "row",
    "two",
    "list",
    "contents",
    "database",
    "simply",
    "scan",
    "scan",
    "let",
    "scan",
    "see",
    "looks",
    "always",
    "type",
    "tells",
    "different",
    "setups",
    "scan",
    "works",
    "case",
    "want",
    "scan",
    "new",
    "tbl",
    "scan",
    "new",
    "tbl",
    "row",
    "one",
    "row",
    "one",
    "row",
    "two",
    "row",
    "two",
    "see",
    "row",
    "one",
    "column",
    "called",
    "knowledge",
    "science",
    "time",
    "step",
    "value",
    "crickets",
    "value",
    "physics",
    "information",
    "created",
    "time",
    "stamp",
    "row",
    "one",
    "also",
    "knowledge",
    "sports",
    "value",
    "cricut",
    "sports",
    "science",
    "interesting",
    "remember",
    "also",
    "gave",
    "originally",
    "told",
    "come",
    "chemistry",
    "science",
    "chemistry",
    "science",
    "physics",
    "come",
    "see",
    "chemistry",
    "replaced",
    "chemistry",
    "physics",
    "new",
    "value",
    "physics",
    "let",
    "go",
    "ahead",
    "clear",
    "little",
    "bit",
    "going",
    "ask",
    "question",
    "enabled",
    "new",
    "table",
    "hit",
    "enter",
    "going",
    "see",
    "comes",
    "true",
    "go",
    "ahead",
    "disable",
    "let",
    "go",
    "ahead",
    "disable",
    "new",
    "table",
    "make",
    "sure",
    "quotes",
    "around",
    "disabled",
    "happens",
    "scan",
    "scan",
    "new",
    "table",
    "hit",
    "enter",
    "gon",
    "na",
    "see",
    "get",
    "error",
    "coming",
    "disabled",
    "ca",
    "anything",
    "enable",
    "table",
    "let",
    "alteration",
    "new",
    "table",
    "look",
    "little",
    "familiar",
    "similar",
    "create",
    "call",
    "test",
    "info",
    "hit",
    "enter",
    "take",
    "moment",
    "updating",
    "want",
    "go",
    "ahead",
    "enable",
    "let",
    "go",
    "ahead",
    "enable",
    "new",
    "table",
    "back",
    "running",
    "want",
    "describe",
    "describe",
    "new",
    "table",
    "come",
    "see",
    "name",
    "knowledge",
    "data",
    "encoding",
    "information",
    "knowledge",
    "also",
    "test",
    "info",
    "name",
    "test",
    "info",
    "information",
    "concerning",
    "test",
    "info",
    "simply",
    "enable",
    "new",
    "table",
    "enabled",
    "oops",
    "already",
    "guess",
    "enable",
    "twice",
    "let",
    "start",
    "looking",
    "well",
    "scan",
    "new",
    "table",
    "see",
    "brings",
    "information",
    "like",
    "want",
    "go",
    "ahead",
    "get",
    "row",
    "r1",
    "hbase",
    "r1",
    "see",
    "knowledge",
    "science",
    "timestamp",
    "value",
    "physics",
    "knowledge",
    "sports",
    "time",
    "stamp",
    "value",
    "cricket",
    "let",
    "see",
    "happens",
    "put",
    "new",
    "table",
    "want",
    "row",
    "one",
    "guess",
    "earlier",
    "something",
    "similar",
    "uh",
    "going",
    "knowledge",
    "economics",
    "going",
    "instead",
    "think",
    "macroeconomics",
    "market",
    "economics",
    "go",
    "back",
    "get",
    "command",
    "see",
    "looks",
    "like",
    "see",
    "knowledge",
    "economics",
    "timestamp",
    "value",
    "market",
    "economics",
    "physics",
    "cricket",
    "economics",
    "science",
    "sports",
    "three",
    "different",
    "columns",
    "one",
    "different",
    "information",
    "managed",
    "go",
    "commands",
    "look",
    "basics",
    "ability",
    "create",
    "basic",
    "hbase",
    "setup",
    "nosql",
    "setup",
    "based",
    "columns",
    "rows",
    "fun",
    "go",
    "back",
    "cloudera",
    "website",
    "hbase",
    "master",
    "status",
    "go",
    "ahead",
    "refresh",
    "go",
    "see",
    "user",
    "tables",
    "table",
    "set",
    "one",
    "click",
    "details",
    "goes",
    "uh",
    "admin",
    "looking",
    "go",
    "oh",
    "someone",
    "created",
    "new",
    "tbl",
    "underneath",
    "new",
    "table",
    "learn",
    "apache",
    "spark",
    "history",
    "spark",
    "spark",
    "hadoop",
    "framework",
    "wes",
    "spark",
    "components",
    "apache",
    "spark",
    "spark",
    "core",
    "spark",
    "sql",
    "spark",
    "streaming",
    "spark",
    "ml",
    "lab",
    "graphics",
    "learn",
    "spark",
    "architecture",
    "applications",
    "spark",
    "spark",
    "use",
    "cases",
    "let",
    "begin",
    "understanding",
    "history",
    "apache",
    "spark",
    "started",
    "2009",
    "project",
    "uc",
    "berkeley",
    "amp",
    "labs",
    "mate",
    "2010",
    "open",
    "source",
    "bsd",
    "license",
    "2013",
    "spark",
    "became",
    "apache",
    "top",
    "level",
    "project",
    "2014",
    "used",
    "data",
    "bricks",
    "sort",
    "data",
    "sets",
    "set",
    "new",
    "world",
    "record",
    "apache",
    "spark",
    "started",
    "today",
    "one",
    "demand",
    "processing",
    "framework",
    "would",
    "say",
    "memory",
    "computing",
    "framework",
    "used",
    "across",
    "big",
    "data",
    "industry",
    "apache",
    "spark",
    "let",
    "learn",
    "apache",
    "spark",
    "open",
    "source",
    "computing",
    "framework",
    "could",
    "say",
    "data",
    "processing",
    "engine",
    "used",
    "process",
    "data",
    "batch",
    "also",
    "real",
    "time",
    "across",
    "various",
    "cluster",
    "computers",
    "simple",
    "programming",
    "language",
    "behind",
    "scenes",
    "scala",
    "used",
    "although",
    "users",
    "would",
    "want",
    "work",
    "spark",
    "work",
    "python",
    "work",
    "scala",
    "work",
    "java",
    "even",
    "r",
    "matter",
    "supports",
    "programming",
    "languages",
    "one",
    "reasons",
    "called",
    "polyglot",
    "wherein",
    "good",
    "set",
    "libraries",
    "support",
    "programming",
    "languages",
    "developers",
    "data",
    "scientists",
    "incorporate",
    "spark",
    "applications",
    "build",
    "spark",
    "based",
    "applications",
    "process",
    "analyze",
    "query",
    "transform",
    "data",
    "large",
    "scale",
    "key",
    "features",
    "apache",
    "spark",
    "compare",
    "hadoop",
    "west",
    "spark",
    "know",
    "hadoop",
    "framework",
    "basically",
    "map",
    "reduce",
    "comes",
    "hadoop",
    "processing",
    "data",
    "however",
    "processing",
    "data",
    "using",
    "mapreduce",
    "hadoop",
    "quite",
    "slow",
    "batch",
    "oriented",
    "operation",
    "time",
    "consuming",
    "talk",
    "spark",
    "spark",
    "process",
    "data",
    "100",
    "times",
    "faster",
    "mapreduce",
    "computing",
    "framework",
    "well",
    "always",
    "conflicting",
    "ideas",
    "saying",
    "spark",
    "application",
    "really",
    "efficiently",
    "coded",
    "mapreduce",
    "application",
    "efficiently",
    "coded",
    "well",
    "different",
    "case",
    "however",
    "normally",
    "talk",
    "code",
    "efficiently",
    "written",
    "mapreduce",
    "spark",
    "based",
    "processing",
    "spark",
    "win",
    "battle",
    "almost",
    "100",
    "times",
    "faster",
    "mapreduce",
    "mentioned",
    "hadoop",
    "performs",
    "batch",
    "processing",
    "one",
    "paradigms",
    "map",
    "reduced",
    "programming",
    "model",
    "involves",
    "mapping",
    "reducing",
    "quite",
    "rigid",
    "performs",
    "batch",
    "processing",
    "intermittent",
    "data",
    "written",
    "sdfs",
    "written",
    "red",
    "back",
    "sdfs",
    "makes",
    "hadoop",
    "map",
    "reduce",
    "processing",
    "slower",
    "case",
    "spark",
    "perform",
    "batch",
    "processing",
    "however",
    "lot",
    "use",
    "cases",
    "based",
    "processing",
    "take",
    "example",
    "macy",
    "take",
    "example",
    "retail",
    "giant",
    "walmart",
    "many",
    "use",
    "cases",
    "would",
    "prefer",
    "real",
    "time",
    "processing",
    "would",
    "say",
    "near",
    "real",
    "time",
    "processing",
    "say",
    "real",
    "time",
    "near",
    "real",
    "time",
    "processing",
    "data",
    "comes",
    "talking",
    "streaming",
    "kind",
    "data",
    "hadoop",
    "hadoop",
    "mapreduce",
    "obviously",
    "started",
    "written",
    "java",
    "could",
    "also",
    "write",
    "scala",
    "python",
    "however",
    "talk",
    "mapreduce",
    "lines",
    "code",
    "since",
    "written",
    "java",
    "take",
    "times",
    "execute",
    "manage",
    "dependencies",
    "right",
    "declarations",
    "create",
    "mapper",
    "reducer",
    "driver",
    "classes",
    "however",
    "compare",
    "spark",
    "lines",
    "code",
    "implemented",
    "scala",
    "scala",
    "statically",
    "typed",
    "dynamically",
    "inferred",
    "language",
    "concise",
    "benefit",
    "features",
    "functional",
    "programming",
    "language",
    "case",
    "scala",
    "whatever",
    "code",
    "written",
    "converted",
    "byte",
    "codes",
    "runs",
    "jvm",
    "hadoop",
    "supports",
    "kerberos",
    "authentication",
    "different",
    "kind",
    "authentication",
    "mechanisms",
    "kerberos",
    "one",
    "ones",
    "really",
    "get",
    "difficult",
    "manage",
    "spark",
    "supports",
    "authentication",
    "via",
    "shared",
    "secret",
    "also",
    "run",
    "yarn",
    "leveraging",
    "capability",
    "kerberos",
    "spark",
    "features",
    "really",
    "makes",
    "unique",
    "demand",
    "processing",
    "framework",
    "talk",
    "spark",
    "features",
    "one",
    "key",
    "features",
    "fast",
    "processing",
    "spark",
    "contains",
    "resilient",
    "distributed",
    "data",
    "sets",
    "rdds",
    "building",
    "blocks",
    "spark",
    "learn",
    "rdds",
    "later",
    "spark",
    "contains",
    "rdds",
    "saves",
    "huge",
    "time",
    "taken",
    "reading",
    "writing",
    "operations",
    "100",
    "times",
    "say",
    "10",
    "100",
    "times",
    "faster",
    "hadoop",
    "say",
    "memory",
    "computing",
    "would",
    "like",
    "make",
    "note",
    "difference",
    "caching",
    "memory",
    "computing",
    "think",
    "caching",
    "mainly",
    "support",
    "read",
    "ahead",
    "mechanism",
    "data",
    "benefit",
    "queries",
    "however",
    "say",
    "memory",
    "computing",
    "talking",
    "lazy",
    "evaluation",
    "talking",
    "data",
    "loaded",
    "memory",
    "specific",
    "kind",
    "action",
    "invoked",
    "data",
    "stored",
    "ram",
    "say",
    "ram",
    "used",
    "processing",
    "also",
    "used",
    "storage",
    "decide",
    "whether",
    "would",
    "want",
    "ram",
    "used",
    "persistence",
    "computing",
    "access",
    "data",
    "quickly",
    "accelerate",
    "speed",
    "analytics",
    "spark",
    "quite",
    "flexible",
    "supports",
    "multiple",
    "languages",
    "already",
    "mentioned",
    "allows",
    "developers",
    "write",
    "applications",
    "java",
    "scala",
    "r",
    "python",
    "quite",
    "fault",
    "tolerance",
    "spark",
    "contains",
    "rdds",
    "could",
    "say",
    "execution",
    "logic",
    "could",
    "say",
    "temporary",
    "data",
    "sets",
    "initially",
    "data",
    "loaded",
    "data",
    "loaded",
    "rdds",
    "execution",
    "happening",
    "fault",
    "tolerant",
    "rdds",
    "distributed",
    "across",
    "multiple",
    "nodes",
    "failure",
    "one",
    "worker",
    "node",
    "cluster",
    "really",
    "affect",
    "rdds",
    "portion",
    "recomputed",
    "ensures",
    "loss",
    "data",
    "ensures",
    "data",
    "loss",
    "absolutely",
    "fault",
    "tolerant",
    "better",
    "analytics",
    "spark",
    "rich",
    "set",
    "sql",
    "queries",
    "machine",
    "learning",
    "algorithms",
    "complex",
    "analytics",
    "supported",
    "various",
    "par",
    "components",
    "learn",
    "coming",
    "slides",
    "functionalities",
    "analytics",
    "performed",
    "better",
    "terms",
    "spark",
    "key",
    "features",
    "spark",
    "however",
    "many",
    "features",
    "related",
    "different",
    "components",
    "spark",
    "learn",
    "components",
    "spark",
    "talking",
    "spark",
    "core",
    "core",
    "component",
    "basically",
    "rdds",
    "core",
    "engine",
    "takes",
    "care",
    "processing",
    "also",
    "spark",
    "sql",
    "people",
    "would",
    "interested",
    "working",
    "structured",
    "data",
    "data",
    "structuralized",
    "would",
    "want",
    "prefer",
    "using",
    "spark",
    "sql",
    "spark",
    "sql",
    "internally",
    "components",
    "features",
    "like",
    "data",
    "frames",
    "data",
    "sets",
    "used",
    "process",
    "structured",
    "data",
    "much",
    "much",
    "faster",
    "way",
    "spark",
    "streaming",
    "important",
    "component",
    "spark",
    "allows",
    "create",
    "spark",
    "streaming",
    "applications",
    "works",
    "data",
    "streamed",
    "data",
    "constantly",
    "getting",
    "generated",
    "would",
    "also",
    "could",
    "also",
    "transform",
    "data",
    "could",
    "analyze",
    "process",
    "data",
    "comes",
    "smaller",
    "chunks",
    "sparks",
    "mlib",
    "basically",
    "set",
    "libraries",
    "allows",
    "developers",
    "data",
    "scientists",
    "build",
    "machine",
    "learning",
    "algorithms",
    "predictive",
    "analytics",
    "prescriptive",
    "descriptive",
    "analytics",
    "could",
    "build",
    "recommendation",
    "systems",
    "bigger",
    "smarter",
    "machine",
    "learning",
    "algorithms",
    "using",
    "libraries",
    "graphics",
    "think",
    "organizations",
    "like",
    "linkedin",
    "say",
    "twitter",
    "data",
    "naturally",
    "network",
    "kind",
    "flow",
    "data",
    "could",
    "represented",
    "form",
    "graphs",
    "talk",
    "graphs",
    "talking",
    "pie",
    "charts",
    "bar",
    "charts",
    "talking",
    "network",
    "related",
    "data",
    "data",
    "networked",
    "together",
    "kind",
    "relationship",
    "think",
    "facebook",
    "think",
    "linkedin",
    "one",
    "person",
    "connected",
    "person",
    "one",
    "company",
    "connected",
    "companies",
    "data",
    "represented",
    "form",
    "network",
    "graphs",
    "spark",
    "component",
    "called",
    "graphics",
    "allows",
    "graph",
    "based",
    "processing",
    "components",
    "apache",
    "spark",
    "spark",
    "core",
    "spark",
    "sql",
    "spark",
    "streaming",
    "spark",
    "mlib",
    "graphics",
    "learn",
    "components",
    "spark",
    "let",
    "learn",
    "spark",
    "core",
    "base",
    "engine",
    "used",
    "large",
    "scale",
    "parallel",
    "distributed",
    "data",
    "processing",
    "work",
    "spark",
    "least",
    "minimum",
    "would",
    "work",
    "spark",
    "core",
    "rdds",
    "building",
    "blocks",
    "spark",
    "responsible",
    "memory",
    "management",
    "fault",
    "recovery",
    "scheduling",
    "distributing",
    "monitoring",
    "jobs",
    "cluster",
    "interacting",
    "storage",
    "systems",
    "would",
    "like",
    "make",
    "key",
    "point",
    "spark",
    "storage",
    "relies",
    "storage",
    "storage",
    "could",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "could",
    "database",
    "like",
    "nosql",
    "database",
    "hbase",
    "could",
    "database",
    "say",
    "rdbms",
    "could",
    "connect",
    "spark",
    "fetch",
    "data",
    "extract",
    "data",
    "process",
    "analyze",
    "let",
    "learn",
    "little",
    "bit",
    "rdds",
    "resilient",
    "distributed",
    "data",
    "sets",
    "spark",
    "core",
    "base",
    "engine",
    "core",
    "engine",
    "embedded",
    "building",
    "blocks",
    "spark",
    "nothing",
    "resilient",
    "distributed",
    "data",
    "set",
    "name",
    "says",
    "resilient",
    "existing",
    "shorter",
    "period",
    "time",
    "distributed",
    "distributed",
    "across",
    "nodes",
    "data",
    "set",
    "data",
    "loaded",
    "data",
    "existing",
    "processing",
    "immutable",
    "fault",
    "tolerant",
    "distributed",
    "collection",
    "objects",
    "rdd",
    "mainly",
    "two",
    "operations",
    "performed",
    "rdd",
    "take",
    "example",
    "say",
    "want",
    "process",
    "particular",
    "file",
    "could",
    "write",
    "simple",
    "code",
    "scala",
    "would",
    "basically",
    "mean",
    "something",
    "like",
    "say",
    "val",
    "declare",
    "variable",
    "would",
    "say",
    "val",
    "x",
    "could",
    "use",
    "call",
    "spark",
    "context",
    "basically",
    "important",
    "entry",
    "point",
    "application",
    "could",
    "use",
    "method",
    "spark",
    "context",
    "example",
    "text",
    "file",
    "could",
    "point",
    "particular",
    "file",
    "method",
    "spark",
    "context",
    "spark",
    "context",
    "entry",
    "point",
    "application",
    "could",
    "give",
    "path",
    "method",
    "step",
    "evaluation",
    "say",
    "val",
    "x",
    "creating",
    "immutable",
    "variable",
    "variable",
    "assigning",
    "file",
    "step",
    "actually",
    "creates",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "imagine",
    "simple",
    "execution",
    "logic",
    "empty",
    "data",
    "set",
    "created",
    "memory",
    "node",
    "would",
    "say",
    "multiple",
    "nodes",
    "data",
    "split",
    "stored",
    "imagining",
    "yarn",
    "spark",
    "working",
    "hadoop",
    "hadoop",
    "using",
    "say",
    "two",
    "nodes",
    "distributed",
    "file",
    "system",
    "sdfs",
    "basically",
    "means",
    "file",
    "written",
    "htfs",
    "also",
    "means",
    "file",
    "related",
    "blocks",
    "stored",
    "underlying",
    "disk",
    "machines",
    "say",
    "val",
    "x",
    "equals",
    "file",
    "using",
    "method",
    "spark",
    "context",
    "various",
    "methods",
    "like",
    "hold",
    "text",
    "files",
    "parallel",
    "eyes",
    "step",
    "create",
    "rdd",
    "imagine",
    "logical",
    "data",
    "set",
    "created",
    "memory",
    "across",
    "nodes",
    "nodes",
    "data",
    "however",
    "data",
    "loaded",
    "first",
    "rdd",
    "say",
    "first",
    "step",
    "call",
    "tag",
    "dag",
    "series",
    "steps",
    "get",
    "executed",
    "later",
    "stage",
    "later",
    "could",
    "processing",
    "could",
    "say",
    "val",
    "could",
    "something",
    "x",
    "could",
    "say",
    "x",
    "dot",
    "map",
    "would",
    "want",
    "apply",
    "function",
    "every",
    "record",
    "every",
    "element",
    "file",
    "could",
    "give",
    "logic",
    "x",
    "dot",
    "map",
    "second",
    "step",
    "creating",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "say",
    "second",
    "step",
    "dag",
    "okay",
    "external",
    "rdd",
    "one",
    "rdd",
    "created",
    "depends",
    "first",
    "rtd",
    "first",
    "rdd",
    "becomes",
    "base",
    "rdd",
    "parent",
    "rtd",
    "resultant",
    "rtd",
    "becomes",
    "child",
    "rdd",
    "go",
    "could",
    "say",
    "val",
    "zed",
    "would",
    "say",
    "okay",
    "would",
    "want",
    "filter",
    "filter",
    "could",
    "give",
    "logic",
    "might",
    "searching",
    "word",
    "searching",
    "pattern",
    "could",
    "say",
    "val",
    "z",
    "equals",
    "dot",
    "filter",
    "creates",
    "one",
    "rdd",
    "resilient",
    "distributed",
    "data",
    "set",
    "memory",
    "say",
    "nothing",
    "one",
    "step",
    "dag",
    "tag",
    "series",
    "steps",
    "executed",
    "execution",
    "happen",
    "data",
    "get",
    "data",
    "get",
    "loaded",
    "rdds",
    "using",
    "method",
    "using",
    "transformation",
    "like",
    "map",
    "using",
    "transformation",
    "like",
    "filter",
    "flat",
    "map",
    "anything",
    "else",
    "transformations",
    "operations",
    "map",
    "filter",
    "join",
    "union",
    "many",
    "others",
    "create",
    "rdds",
    "basically",
    "means",
    "creating",
    "execution",
    "logic",
    "data",
    "evaluated",
    "operation",
    "happening",
    "right",
    "invoke",
    "action",
    "might",
    "want",
    "print",
    "result",
    "might",
    "want",
    "take",
    "elements",
    "see",
    "might",
    "want",
    "count",
    "actions",
    "actually",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "say",
    "z",
    "dot",
    "count",
    "would",
    "want",
    "count",
    "number",
    "words",
    "filtering",
    "action",
    "invoked",
    "trigger",
    "execution",
    "dag",
    "right",
    "beginning",
    "happens",
    "spark",
    "z",
    "dot",
    "count",
    "start",
    "whole",
    "execution",
    "dag",
    "right",
    "beginning",
    "z",
    "dot",
    "count",
    "second",
    "time",
    "action",
    "invoked",
    "data",
    "loaded",
    "first",
    "rtd",
    "map",
    "filter",
    "finally",
    "result",
    "core",
    "concept",
    "rdds",
    "rtd",
    "works",
    "mainly",
    "spark",
    "two",
    "kind",
    "operations",
    "one",
    "transformations",
    "one",
    "actions",
    "transformations",
    "using",
    "method",
    "spark",
    "context",
    "always",
    "always",
    "create",
    "rtd",
    "could",
    "say",
    "step",
    "tag",
    "actions",
    "something",
    "invoke",
    "execution",
    "invoke",
    "execution",
    "first",
    "rdd",
    "till",
    "last",
    "rdd",
    "get",
    "result",
    "rdds",
    "work",
    "talk",
    "components",
    "spark",
    "let",
    "learn",
    "little",
    "bit",
    "spark",
    "sql",
    "spark",
    "sql",
    "component",
    "type",
    "processing",
    "framework",
    "used",
    "structured",
    "data",
    "processing",
    "usually",
    "people",
    "might",
    "structured",
    "data",
    "stored",
    "rdbms",
    "files",
    "data",
    "structured",
    "particular",
    "delimiters",
    "pattern",
    "one",
    "wants",
    "process",
    "structured",
    "data",
    "one",
    "wants",
    "use",
    "spark",
    "processing",
    "work",
    "structured",
    "data",
    "would",
    "prefer",
    "use",
    "spark",
    "sql",
    "work",
    "different",
    "data",
    "formats",
    "say",
    "csv",
    "json",
    "even",
    "work",
    "smarter",
    "formats",
    "like",
    "avro",
    "parquet",
    "even",
    "binary",
    "files",
    "sequence",
    "files",
    "could",
    "data",
    "coming",
    "rdbms",
    "extracted",
    "using",
    "jdbc",
    "connection",
    "bottom",
    "level",
    "talk",
    "spark",
    "sql",
    "data",
    "source",
    "api",
    "basically",
    "allows",
    "get",
    "data",
    "whichever",
    "format",
    "spark",
    "sql",
    "something",
    "called",
    "data",
    "frame",
    "api",
    "data",
    "frames",
    "data",
    "frames",
    "short",
    "visualize",
    "imagine",
    "rows",
    "columns",
    "data",
    "represented",
    "form",
    "rows",
    "columns",
    "column",
    "headings",
    "data",
    "frame",
    "api",
    "allows",
    "create",
    "data",
    "frames",
    "like",
    "previous",
    "example",
    "work",
    "file",
    "want",
    "process",
    "would",
    "convert",
    "rdd",
    "using",
    "method",
    "smart",
    "context",
    "transformations",
    "similar",
    "way",
    "use",
    "data",
    "frames",
    "want",
    "use",
    "spark",
    "sql",
    "would",
    "use",
    "sparks",
    "context",
    "sql",
    "context",
    "hive",
    "context",
    "spark",
    "allows",
    "work",
    "data",
    "frames",
    "like",
    "earlier",
    "example",
    "saying",
    "val",
    "x",
    "equals",
    "sc",
    "dot",
    "text",
    "file",
    "case",
    "data",
    "frames",
    "instead",
    "sc",
    "would",
    "using",
    "say",
    "spark",
    "dot",
    "something",
    "spark",
    "context",
    "available",
    "data",
    "frames",
    "api",
    "used",
    "older",
    "versions",
    "like",
    "spark",
    "using",
    "hive",
    "context",
    "sql",
    "context",
    "working",
    "spark",
    "would",
    "saying",
    "val",
    "x",
    "equals",
    "sql",
    "context",
    "dot",
    "would",
    "using",
    "spark",
    "dot",
    "data",
    "frame",
    "api",
    "basically",
    "allows",
    "create",
    "data",
    "frames",
    "structured",
    "data",
    "also",
    "lets",
    "spark",
    "know",
    "data",
    "already",
    "particular",
    "structure",
    "follows",
    "format",
    "based",
    "sparks",
    "dag",
    "scheduler",
    "right",
    "say",
    "talk",
    "sequence",
    "steps",
    "spark",
    "already",
    "aware",
    "different",
    "steps",
    "involved",
    "application",
    "data",
    "frame",
    "api",
    "basically",
    "allows",
    "create",
    "data",
    "frames",
    "data",
    "data",
    "frames",
    "say",
    "talking",
    "rows",
    "columns",
    "headings",
    "data",
    "frame",
    "dsl",
    "language",
    "use",
    "spark",
    "sql",
    "hive",
    "query",
    "language",
    "options",
    "used",
    "work",
    "data",
    "frames",
    "learn",
    "data",
    "frames",
    "follow",
    "next",
    "sessions",
    "talk",
    "spark",
    "streaming",
    "interesting",
    "organizations",
    "would",
    "want",
    "work",
    "streaming",
    "data",
    "imagine",
    "store",
    "like",
    "macy",
    "would",
    "want",
    "machine",
    "learning",
    "algorithms",
    "would",
    "machine",
    "learning",
    "algorithms",
    "suppose",
    "lot",
    "customers",
    "walking",
    "store",
    "searching",
    "particular",
    "product",
    "particular",
    "item",
    "could",
    "cameras",
    "placed",
    "store",
    "already",
    "done",
    "cameras",
    "placed",
    "store",
    "keep",
    "monitoring",
    "corner",
    "store",
    "customers",
    "camera",
    "captures",
    "information",
    "information",
    "streamed",
    "processed",
    "algorithms",
    "algorithms",
    "see",
    "product",
    "series",
    "product",
    "customers",
    "might",
    "interested",
    "algorithm",
    "real",
    "time",
    "process",
    "based",
    "number",
    "customers",
    "based",
    "available",
    "product",
    "store",
    "come",
    "attractive",
    "alternative",
    "price",
    "price",
    "displayed",
    "screen",
    "probably",
    "customers",
    "would",
    "buy",
    "product",
    "processing",
    "data",
    "comes",
    "algorithms",
    "work",
    "computation",
    "give",
    "result",
    "result",
    "customers",
    "buying",
    "particular",
    "product",
    "whole",
    "essence",
    "machine",
    "learning",
    "processing",
    "really",
    "hold",
    "good",
    "customers",
    "store",
    "could",
    "relate",
    "even",
    "online",
    "shopping",
    "portal",
    "might",
    "machine",
    "learning",
    "algorithms",
    "might",
    "real",
    "time",
    "processing",
    "based",
    "clicks",
    "customer",
    "based",
    "clicks",
    "based",
    "customer",
    "history",
    "based",
    "customer",
    "behavior",
    "algorithms",
    "come",
    "recommendation",
    "products",
    "better",
    "altered",
    "price",
    "sale",
    "happens",
    "case",
    "would",
    "seeing",
    "essence",
    "real",
    "time",
    "processing",
    "fixed",
    "particular",
    "duration",
    "time",
    "also",
    "means",
    "something",
    "process",
    "data",
    "comes",
    "spark",
    "streaming",
    "lightweight",
    "api",
    "allows",
    "developers",
    "perform",
    "batch",
    "processing",
    "also",
    "streaming",
    "processing",
    "data",
    "provides",
    "secure",
    "reliable",
    "fast",
    "processing",
    "live",
    "data",
    "streams",
    "happens",
    "spark",
    "streaming",
    "brief",
    "input",
    "data",
    "stream",
    "data",
    "stream",
    "could",
    "file",
    "constantly",
    "getting",
    "appended",
    "could",
    "kind",
    "metrics",
    "could",
    "kind",
    "events",
    "based",
    "clicks",
    "customers",
    "based",
    "products",
    "choosing",
    "store",
    "input",
    "data",
    "stream",
    "pushed",
    "spark",
    "streaming",
    "application",
    "spark",
    "streaming",
    "application",
    "broke",
    "break",
    "content",
    "smaller",
    "streams",
    "call",
    "disk",
    "criticized",
    "streams",
    "batches",
    "smaller",
    "data",
    "processing",
    "happen",
    "frames",
    "could",
    "say",
    "process",
    "file",
    "every",
    "five",
    "seconds",
    "latest",
    "data",
    "come",
    "also",
    "windows",
    "based",
    "uh",
    "options",
    "like",
    "say",
    "windows",
    "mean",
    "window",
    "past",
    "three",
    "events",
    "window",
    "past",
    "three",
    "events",
    "event",
    "five",
    "seconds",
    "batches",
    "smaller",
    "data",
    "processed",
    "spark",
    "engine",
    "process",
    "data",
    "stored",
    "used",
    "processing",
    "spark",
    "streaming",
    "talk",
    "mlib",
    "low",
    "level",
    "machine",
    "learning",
    "library",
    "simple",
    "use",
    "scalable",
    "compatible",
    "various",
    "programming",
    "languages",
    "hadoop",
    "also",
    "libraries",
    "like",
    "apache",
    "mahout",
    "used",
    "machine",
    "learning",
    "algorithms",
    "however",
    "terms",
    "spark",
    "talking",
    "machine",
    "learning",
    "algorithms",
    "built",
    "using",
    "ml",
    "labs",
    "libraries",
    "spark",
    "used",
    "processing",
    "mlib",
    "eases",
    "deployment",
    "development",
    "scalable",
    "machine",
    "learning",
    "algorithms",
    "mean",
    "think",
    "clustering",
    "techniques",
    "think",
    "classification",
    "would",
    "want",
    "classify",
    "data",
    "would",
    "want",
    "supervised",
    "unsupervised",
    "learning",
    "think",
    "collaborative",
    "filtering",
    "many",
    "data",
    "science",
    "related",
    "techniques",
    "techniques",
    "required",
    "build",
    "recommendation",
    "engines",
    "machine",
    "learning",
    "algorithms",
    "built",
    "using",
    "sparks",
    "mlip",
    "graphics",
    "spark",
    "graph",
    "computation",
    "engine",
    "mainly",
    "interested",
    "graph",
    "based",
    "processing",
    "think",
    "facebook",
    "think",
    "linkedin",
    "data",
    "stored",
    "data",
    "kind",
    "network",
    "connections",
    "could",
    "say",
    "well",
    "networked",
    "could",
    "say",
    "x",
    "connected",
    "connected",
    "z",
    "z",
    "connected",
    "x",
    "z",
    "terms",
    "graph",
    "terminologies",
    "call",
    "vertices",
    "vertex",
    "basically",
    "connected",
    "connection",
    "called",
    "edges",
    "could",
    "say",
    "friend",
    "b",
    "b",
    "vertices",
    "friend",
    "relation",
    "edge",
    "data",
    "represented",
    "form",
    "graphs",
    "would",
    "want",
    "processing",
    "way",
    "could",
    "social",
    "media",
    "could",
    "network",
    "devices",
    "could",
    "cloud",
    "platform",
    "could",
    "different",
    "applications",
    "connected",
    "particular",
    "environment",
    "data",
    "represented",
    "form",
    "graph",
    "graphics",
    "used",
    "etl",
    "extraction",
    "transformation",
    "load",
    "data",
    "analysis",
    "also",
    "interactive",
    "graph",
    "computation",
    "graphics",
    "quite",
    "powerful",
    "talk",
    "spark",
    "spark",
    "work",
    "different",
    "clustering",
    "technologies",
    "work",
    "apache",
    "mesos",
    "spark",
    "came",
    "initially",
    "prove",
    "credibility",
    "apache",
    "mesos",
    "spark",
    "work",
    "yarn",
    "usually",
    "see",
    "different",
    "working",
    "environments",
    "spark",
    "also",
    "work",
    "standalone",
    "means",
    "without",
    "hadoop",
    "spark",
    "setup",
    "master",
    "worker",
    "processes",
    "usually",
    "say",
    "technically",
    "spark",
    "uses",
    "master",
    "slave",
    "architecture",
    "consists",
    "driver",
    "program",
    "run",
    "master",
    "node",
    "also",
    "run",
    "client",
    "node",
    "depends",
    "configured",
    "application",
    "multiple",
    "executors",
    "run",
    "worker",
    "nodes",
    "master",
    "node",
    "driver",
    "program",
    "driver",
    "program",
    "internally",
    "spark",
    "context",
    "spark",
    "every",
    "spark",
    "application",
    "driver",
    "program",
    "driver",
    "program",
    "inbuilt",
    "internally",
    "used",
    "spark",
    "context",
    "basically",
    "entry",
    "point",
    "application",
    "spark",
    "functionality",
    "driver",
    "driver",
    "program",
    "interacts",
    "cluster",
    "manager",
    "say",
    "interacts",
    "cluster",
    "manager",
    "spark",
    "context",
    "entry",
    "point",
    "takes",
    "application",
    "request",
    "cluster",
    "manager",
    "said",
    "cluster",
    "manager",
    "could",
    "say",
    "apache",
    "mesos",
    "could",
    "yarn",
    "could",
    "spark",
    "standalone",
    "master",
    "cluster",
    "manager",
    "terms",
    "yarn",
    "resource",
    "manager",
    "spark",
    "application",
    "internally",
    "runs",
    "series",
    "set",
    "tasks",
    "processes",
    "driver",
    "program",
    "wherever",
    "run",
    "spark",
    "context",
    "spark",
    "context",
    "take",
    "care",
    "application",
    "execution",
    "spark",
    "context",
    "talk",
    "cluster",
    "manager",
    "cluster",
    "manager",
    "could",
    "terms",
    "say",
    "cluster",
    "manager",
    "yarn",
    "would",
    "resource",
    "manager",
    "high",
    "level",
    "say",
    "job",
    "split",
    "multiple",
    "tasks",
    "tasks",
    "distributed",
    "slave",
    "nodes",
    "worker",
    "nodes",
    "whenever",
    "kind",
    "transformation",
    "use",
    "method",
    "spark",
    "context",
    "rdd",
    "created",
    "rdd",
    "distributed",
    "across",
    "multiple",
    "nodes",
    "explained",
    "earlier",
    "worker",
    "nodes",
    "slaves",
    "run",
    "different",
    "tasks",
    "spark",
    "architecture",
    "looks",
    "like",
    "learn",
    "spark",
    "architecture",
    "interaction",
    "yarn",
    "usually",
    "happens",
    "spark",
    "context",
    "interacts",
    "cluster",
    "manager",
    "terms",
    "yarn",
    "could",
    "say",
    "resource",
    "manager",
    "already",
    "know",
    "yarn",
    "would",
    "say",
    "node",
    "managers",
    "running",
    "multiple",
    "machines",
    "machine",
    "ram",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "machine",
    "data",
    "nodes",
    "running",
    "obviously",
    "hadoop",
    "related",
    "data",
    "whenever",
    "application",
    "wants",
    "process",
    "data",
    "application",
    "via",
    "spark",
    "context",
    "contacts",
    "cluster",
    "managers",
    "resource",
    "manager",
    "resource",
    "manager",
    "resource",
    "manager",
    "makes",
    "request",
    "resource",
    "manager",
    "makes",
    "requests",
    "node",
    "manager",
    "machines",
    "wherever",
    "relevant",
    "data",
    "resides",
    "asking",
    "containers",
    "resource",
    "manager",
    "negotiating",
    "asking",
    "containers",
    "node",
    "manager",
    "saying",
    "hey",
    "container",
    "1gb",
    "ram",
    "one",
    "cpu",
    "core",
    "container",
    "1gb",
    "ram",
    "1",
    "cpu",
    "core",
    "node",
    "manager",
    "based",
    "kind",
    "processing",
    "approve",
    "deny",
    "node",
    "manager",
    "would",
    "say",
    "fine",
    "give",
    "container",
    "container",
    "allocated",
    "approved",
    "node",
    "manager",
    "resource",
    "manager",
    "basically",
    "start",
    "extra",
    "piece",
    "code",
    "called",
    "appmaster",
    "appmaster",
    "responsible",
    "execution",
    "applications",
    "whether",
    "spark",
    "applications",
    "mapreduce",
    "application",
    "master",
    "piece",
    "code",
    "run",
    "one",
    "containers",
    "use",
    "ram",
    "cpu",
    "core",
    "use",
    "containers",
    "allocated",
    "node",
    "manager",
    "run",
    "tasks",
    "within",
    "container",
    "take",
    "care",
    "execution",
    "container",
    "combination",
    "ram",
    "cpu",
    "core",
    "within",
    "container",
    "executable",
    "process",
    "would",
    "run",
    "executor",
    "process",
    "taking",
    "care",
    "application",
    "related",
    "tasks",
    "overall",
    "spark",
    "works",
    "integration",
    "yarn",
    "let",
    "learn",
    "spark",
    "cluster",
    "managers",
    "said",
    "spark",
    "work",
    "standalone",
    "mode",
    "without",
    "hadoop",
    "default",
    "application",
    "submitted",
    "spark",
    "standalone",
    "mode",
    "cluster",
    "run",
    "fifo",
    "order",
    "application",
    "try",
    "use",
    "available",
    "nodes",
    "could",
    "spark",
    "standalone",
    "cluster",
    "basically",
    "means",
    "could",
    "multiple",
    "nodes",
    "one",
    "nodes",
    "would",
    "master",
    "process",
    "running",
    "nodes",
    "would",
    "spark",
    "worker",
    "processes",
    "running",
    "would",
    "distributed",
    "file",
    "system",
    "spark",
    "standalone",
    "rely",
    "external",
    "storage",
    "get",
    "data",
    "probably",
    "file",
    "system",
    "nodes",
    "data",
    "stored",
    "processing",
    "happen",
    "across",
    "nodes",
    "worker",
    "processes",
    "running",
    "could",
    "spark",
    "working",
    "apache",
    "mesos",
    "said",
    "apache",
    "mesos",
    "open",
    "source",
    "project",
    "manage",
    "computer",
    "clusters",
    "also",
    "run",
    "hadoop",
    "applications",
    "apache",
    "mesos",
    "introduced",
    "earlier",
    "spark",
    "came",
    "existence",
    "prove",
    "credibility",
    "apache",
    "missiles",
    "spark",
    "working",
    "hadoop",
    "yarn",
    "something",
    "widely",
    "see",
    "different",
    "working",
    "environments",
    "yarn",
    "takes",
    "care",
    "processing",
    "take",
    "care",
    "different",
    "processing",
    "frameworks",
    "also",
    "supports",
    "spark",
    "could",
    "kubernetes",
    "something",
    "making",
    "lot",
    "news",
    "today",
    "world",
    "open",
    "source",
    "system",
    "automating",
    "deployment",
    "scaling",
    "management",
    "containerized",
    "applications",
    "could",
    "multiple",
    "docker",
    "based",
    "images",
    "connecting",
    "spark",
    "also",
    "works",
    "kubernetes",
    "let",
    "look",
    "applications",
    "spark",
    "jpmorgan",
    "chase",
    "company",
    "uses",
    "spark",
    "detect",
    "fraudulent",
    "transactions",
    "analyze",
    "business",
    "spends",
    "individual",
    "suggest",
    "offers",
    "identify",
    "patterns",
    "decide",
    "much",
    "invest",
    "invest",
    "one",
    "examples",
    "banking",
    "lot",
    "banking",
    "environments",
    "using",
    "spark",
    "due",
    "processing",
    "capabilities",
    "faster",
    "processing",
    "could",
    "working",
    "fraud",
    "detection",
    "credit",
    "analysis",
    "pattern",
    "identification",
    "many",
    "use",
    "cases",
    "alibaba",
    "group",
    "uses",
    "also",
    "spark",
    "analyze",
    "large",
    "data",
    "sets",
    "data",
    "transaction",
    "details",
    "might",
    "based",
    "online",
    "stores",
    "looking",
    "browsing",
    "history",
    "form",
    "spark",
    "jobs",
    "provides",
    "recommendations",
    "users",
    "alibaba",
    "group",
    "using",
    "spark",
    "domain",
    "iq",
    "leading",
    "healthcare",
    "company",
    "uses",
    "spark",
    "analyze",
    "patients",
    "data",
    "identify",
    "possible",
    "health",
    "issues",
    "diagnose",
    "based",
    "medical",
    "history",
    "lot",
    "work",
    "happening",
    "healthcare",
    "industry",
    "real",
    "time",
    "processing",
    "finding",
    "lot",
    "importance",
    "real",
    "time",
    "faster",
    "processing",
    "required",
    "health",
    "care",
    "industry",
    "iqvi",
    "also",
    "using",
    "spark",
    "netflix",
    "known",
    "riot",
    "games",
    "entertainment",
    "gaming",
    "companies",
    "like",
    "netflix",
    "ride",
    "games",
    "use",
    "apache",
    "spark",
    "showcase",
    "relevant",
    "advertisements",
    "users",
    "based",
    "videos",
    "watched",
    "shared",
    "liked",
    "domains",
    "find",
    "use",
    "cases",
    "spark",
    "banking",
    "health",
    "care",
    "entertainment",
    "many",
    "using",
    "spark",
    "day",
    "day",
    "activities",
    "real",
    "time",
    "memory",
    "faster",
    "processing",
    "let",
    "discuss",
    "sparks",
    "use",
    "case",
    "let",
    "talk",
    "conviva",
    "world",
    "leading",
    "video",
    "streaming",
    "companies",
    "video",
    "streaming",
    "challenge",
    "talk",
    "youtube",
    "data",
    "could",
    "always",
    "read",
    "youtube",
    "data",
    "worth",
    "watching",
    "10",
    "years",
    "huge",
    "amount",
    "data",
    "people",
    "uploading",
    "videos",
    "companies",
    "advertisements",
    "videos",
    "streamed",
    "watched",
    "users",
    "video",
    "streaming",
    "challenge",
    "especially",
    "increasing",
    "demand",
    "high",
    "quality",
    "streaming",
    "experiences",
    "conviva",
    "collects",
    "data",
    "video",
    "streaming",
    "quality",
    "give",
    "customers",
    "visibility",
    "end",
    "user",
    "experience",
    "delivering",
    "apache",
    "spark",
    "using",
    "apache",
    "spark",
    "conviva",
    "delivers",
    "better",
    "quality",
    "service",
    "customers",
    "removing",
    "screen",
    "buffering",
    "learning",
    "detail",
    "network",
    "conditions",
    "real",
    "time",
    "information",
    "stored",
    "video",
    "player",
    "manage",
    "live",
    "video",
    "traffic",
    "coming",
    "4",
    "billion",
    "video",
    "feeds",
    "every",
    "month",
    "ensure",
    "maximum",
    "retention",
    "using",
    "apache",
    "spark",
    "conviva",
    "created",
    "auto",
    "diagnostics",
    "alert",
    "automatically",
    "detects",
    "anomalies",
    "along",
    "video",
    "streaming",
    "pipeline",
    "diagnoses",
    "root",
    "cause",
    "issue",
    "really",
    "makes",
    "one",
    "leading",
    "video",
    "streaming",
    "companies",
    "based",
    "auto",
    "diagnostic",
    "alerts",
    "reduces",
    "waiting",
    "time",
    "video",
    "starts",
    "avoids",
    "buffering",
    "recovers",
    "video",
    "technical",
    "error",
    "whole",
    "goal",
    "maximize",
    "viewer",
    "engagement",
    "sparks",
    "use",
    "case",
    "conviva",
    "using",
    "spark",
    "different",
    "ways",
    "stay",
    "ahead",
    "video",
    "streaming",
    "related",
    "deliveries",
    "understood",
    "learned",
    "spark",
    "components",
    "spark",
    "architecture",
    "also",
    "see",
    "running",
    "spark",
    "application",
    "spark",
    "application",
    "run",
    "standalone",
    "mode",
    "could",
    "set",
    "ide",
    "eclipse",
    "scala",
    "could",
    "coded",
    "application",
    "written",
    "eclipse",
    "run",
    "local",
    "mode",
    "example",
    "application",
    "run",
    "local",
    "mode",
    "cluster",
    "application",
    "importing",
    "packages",
    "spark",
    "context",
    "spark",
    "conf",
    "created",
    "object",
    "first",
    "app",
    "main",
    "class",
    "project",
    "classes",
    "extending",
    "app",
    "rather",
    "main",
    "declare",
    "variable",
    "called",
    "x",
    "pointing",
    "file",
    "project",
    "directory",
    "looks",
    "file",
    "file",
    "basically",
    "content",
    "application",
    "create",
    "variable",
    "assign",
    "file",
    "define",
    "initialize",
    "spark",
    "context",
    "remember",
    "whenever",
    "work",
    "ide",
    "spark",
    "context",
    "spark",
    "available",
    "implicitly",
    "defined",
    "create",
    "configuration",
    "object",
    "set",
    "application",
    "name",
    "set",
    "master",
    "local",
    "want",
    "run",
    "local",
    "mode",
    "want",
    "run",
    "say",
    "windows",
    "machine",
    "would",
    "want",
    "run",
    "spark",
    "standalone",
    "cluster",
    "would",
    "running",
    "yarn",
    "would",
    "remove",
    "property",
    "set",
    "master",
    "defined",
    "configuration",
    "object",
    "basically",
    "using",
    "spark",
    "context",
    "use",
    "method",
    "result",
    "rdp",
    "happening",
    "line",
    "13",
    "val",
    "finally",
    "create",
    "variable",
    "would",
    "want",
    "flat",
    "map",
    "transformation",
    "would",
    "result",
    "internal",
    "rdd",
    "followed",
    "map",
    "transformation",
    "would",
    "result",
    "rdd",
    "finally",
    "reduce",
    "key",
    "aggregation",
    "steps",
    "done",
    "decide",
    "display",
    "result",
    "screen",
    "even",
    "use",
    "save",
    "text",
    "file",
    "point",
    "particular",
    "location",
    "application",
    "run",
    "eclipse",
    "refer",
    "sessions",
    "explained",
    "set",
    "ide",
    "windows",
    "different",
    "environment",
    "variables",
    "run",
    "applications",
    "particular",
    "cluster",
    "would",
    "want",
    "run",
    "application",
    "cluster",
    "give",
    "particular",
    "path",
    "case",
    "say",
    "let",
    "say",
    "file",
    "let",
    "save",
    "also",
    "say",
    "output",
    "getting",
    "stored",
    "default",
    "location",
    "intend",
    "run",
    "intend",
    "run",
    "application",
    "cluster",
    "case",
    "cluster",
    "would",
    "usually",
    "set",
    "linux",
    "machines",
    "would",
    "hadoop",
    "cluster",
    "run",
    "application",
    "would",
    "want",
    "run",
    "application",
    "cluster",
    "locally",
    "machine",
    "delete",
    "part",
    "keep",
    "application",
    "see",
    "application",
    "already",
    "compiles",
    "show",
    "error",
    "message",
    "project",
    "build",
    "path",
    "added",
    "spark",
    "related",
    "jars",
    "spark",
    "related",
    "jars",
    "get",
    "spark",
    "directory",
    "getting",
    "manually",
    "dependencies",
    "people",
    "would",
    "prefer",
    "use",
    "maven",
    "say",
    "sbt",
    "packaging",
    "application",
    "jar",
    "also",
    "done",
    "code",
    "compiles",
    "code",
    "fine",
    "pointing",
    "file",
    "also",
    "creating",
    "output",
    "word",
    "count",
    "since",
    "code",
    "already",
    "written",
    "know",
    "sbt",
    "installed",
    "machine",
    "would",
    "want",
    "package",
    "code",
    "jar",
    "run",
    "cluster",
    "look",
    "command",
    "prompt",
    "go",
    "workspace",
    "go",
    "scala",
    "project",
    "see",
    "build",
    "dot",
    "sbt",
    "file",
    "binaries",
    "source",
    "might",
    "spark",
    "related",
    "directories",
    "exist",
    "case",
    "using",
    "spark",
    "done",
    "previous",
    "runs",
    "within",
    "spark",
    "apps",
    "build",
    "package",
    "need",
    "build",
    "dot",
    "sbt",
    "file",
    "see",
    "contain",
    "contains",
    "name",
    "version",
    "package",
    "jar",
    "scala",
    "version",
    "spark",
    "version",
    "repository",
    "spark",
    "refer",
    "wants",
    "dependencies",
    "components",
    "parkour",
    "spark",
    "sql",
    "mlib",
    "file",
    "exists",
    "project",
    "folder",
    "intending",
    "use",
    "sbt",
    "package",
    "code",
    "jar",
    "run",
    "cluster",
    "case",
    "even",
    "skip",
    "adding",
    "spark",
    "related",
    "charge",
    "build",
    "path",
    "done",
    "make",
    "sure",
    "code",
    "compiles",
    "code",
    "written",
    "sbt",
    "installed",
    "made",
    "file",
    "path",
    "changes",
    "go",
    "command",
    "line",
    "within",
    "project",
    "folder",
    "say",
    "sbt",
    "package",
    "basically",
    "resolve",
    "dependencies",
    "based",
    "whatever",
    "given",
    "code",
    "create",
    "jar",
    "file",
    "place",
    "particular",
    "location",
    "use",
    "jar",
    "file",
    "run",
    "particular",
    "cluster",
    "sbd",
    "package",
    "busy",
    "creating",
    "jar",
    "meanwhile",
    "open",
    "lab",
    "content",
    "say",
    "example",
    "simply",
    "learn",
    "already",
    "lab",
    "set",
    "could",
    "spark",
    "standalone",
    "cluster",
    "could",
    "also",
    "run",
    "jar",
    "file",
    "could",
    "spark",
    "hadoop",
    "use",
    "submit",
    "application",
    "yarn",
    "cluster",
    "let",
    "go",
    "web",
    "console",
    "let",
    "copy",
    "link",
    "close",
    "say",
    "launch",
    "lab",
    "log",
    "paste",
    "password",
    "logged",
    "say",
    "spark",
    "2",
    "shell",
    "configured",
    "work",
    "spark",
    "version",
    "interactive",
    "way",
    "bringing",
    "spark",
    "shell",
    "running",
    "application",
    "interested",
    "running",
    "application",
    "jar",
    "file",
    "let",
    "go",
    "see",
    "code",
    "let",
    "see",
    "sbt",
    "done",
    "packaging",
    "yes",
    "done",
    "created",
    "jar",
    "file",
    "location",
    "need",
    "get",
    "jar",
    "file",
    "location",
    "onto",
    "cluster",
    "come",
    "ftp",
    "basically",
    "allows",
    "push",
    "whatever",
    "jars",
    "web",
    "console",
    "go",
    "say",
    "connect",
    "search",
    "already",
    "existing",
    "jar",
    "file",
    "might",
    "create",
    "conflict",
    "something",
    "delete",
    "done",
    "say",
    "upload",
    "file",
    "interested",
    "getting",
    "jar",
    "file",
    "click",
    "users",
    "win10",
    "workspace",
    "project",
    "get",
    "target",
    "scala",
    "take",
    "jar",
    "file",
    "say",
    "open",
    "upload",
    "jar",
    "file",
    "web",
    "console",
    "terminal",
    "wherein",
    "connect",
    "cluster",
    "let",
    "go",
    "let",
    "quit",
    "spark",
    "shell",
    "want",
    "run",
    "application",
    "cluster",
    "search",
    "jar",
    "file",
    "existing",
    "say",
    "spark",
    "submit",
    "point",
    "jar",
    "file",
    "simply",
    "say",
    "class",
    "know",
    "code",
    "package",
    "class",
    "name",
    "package",
    "object",
    "class",
    "name",
    "say",
    "main",
    "dot",
    "scala",
    "dot",
    "first",
    "app",
    "good",
    "check",
    "file",
    "exists",
    "code",
    "points",
    "comment",
    "check",
    "htfs",
    "ls",
    "default",
    "user",
    "directory",
    "search",
    "file",
    "search",
    "file",
    "called",
    "abc",
    "see",
    "anything",
    "let",
    "go",
    "ftp",
    "basically",
    "upload",
    "file",
    "like",
    "earlier",
    "time",
    "pick",
    "existing",
    "abc1",
    "file",
    "showed",
    "upload",
    "done",
    "put",
    "cluster",
    "saying",
    "sdfs",
    "dfs",
    "put",
    "let",
    "take",
    "abc1",
    "file",
    "let",
    "put",
    "directory",
    "input",
    "putting",
    "file",
    "spark",
    "submit",
    "submit",
    "application",
    "cluster",
    "see",
    "basically",
    "started",
    "application",
    "contacts",
    "resource",
    "manager",
    "gets",
    "application",
    "id",
    "processing",
    "file",
    "would",
    "want",
    "get",
    "done",
    "completed",
    "temporary",
    "directory",
    "deleted",
    "run",
    "spark",
    "application",
    "cluster",
    "done",
    "also",
    "go",
    "spark",
    "history",
    "server",
    "saying",
    "path",
    "spark",
    "history",
    "server",
    "shows",
    "application",
    "run",
    "today",
    "word",
    "count",
    "click",
    "application",
    "says",
    "ended",
    "save",
    "text",
    "file",
    "click",
    "shows",
    "dag",
    "visualization",
    "says",
    "started",
    "text",
    "file",
    "flat",
    "map",
    "map",
    "shuffling",
    "wanted",
    "reduce",
    "key",
    "said",
    "every",
    "rdd",
    "default",
    "two",
    "partitions",
    "want",
    "aggregate",
    "wider",
    "transformations",
    "like",
    "reduce",
    "key",
    "sort",
    "key",
    "group",
    "key",
    "count",
    "key",
    "case",
    "similar",
    "key",
    "related",
    "data",
    "shuffled",
    "brought",
    "one",
    "partition",
    "see",
    "shuffling",
    "happening",
    "also",
    "tells",
    "number",
    "tasks",
    "run",
    "per",
    "partition",
    "ran",
    "spark",
    "application",
    "packaging",
    "jar",
    "using",
    "sbt",
    "sbt",
    "created",
    "jar",
    "file",
    "basically",
    "brought",
    "jar",
    "file",
    "onto",
    "cluster",
    "submitted",
    "using",
    "spark",
    "submit",
    "spark",
    "application",
    "runs",
    "cluster",
    "said",
    "application",
    "driver",
    "program",
    "spark",
    "applications",
    "run",
    "independent",
    "processes",
    "run",
    "cluster",
    "across",
    "nodes",
    "saw",
    "run",
    "spark",
    "application",
    "cluster",
    "always",
    "look",
    "spark",
    "applications",
    "progress",
    "completed",
    "looking",
    "spark",
    "ui",
    "spark",
    "application",
    "mentioned",
    "driver",
    "program",
    "run",
    "application",
    "cluster",
    "always",
    "specify",
    "would",
    "want",
    "driver",
    "program",
    "run",
    "case",
    "ran",
    "spark",
    "application",
    "basically",
    "simple",
    "spark",
    "submit",
    "gave",
    "jar",
    "class",
    "name",
    "could",
    "also",
    "say",
    "master",
    "could",
    "specify",
    "would",
    "want",
    "application",
    "run",
    "local",
    "mode",
    "could",
    "say",
    "yarn",
    "default",
    "spark",
    "standalone",
    "cluster",
    "could",
    "giving",
    "something",
    "like",
    "host",
    "name",
    "port",
    "could",
    "options",
    "specifying",
    "minus",
    "minus",
    "master",
    "could",
    "also",
    "specify",
    "many",
    "executors",
    "need",
    "much",
    "memory",
    "per",
    "executor",
    "much",
    "course",
    "per",
    "executor",
    "need",
    "could",
    "also",
    "say",
    "deploy",
    "mode",
    "client",
    "basically",
    "means",
    "driver",
    "run",
    "machine",
    "however",
    "execution",
    "application",
    "happen",
    "cluster",
    "nodes",
    "also",
    "say",
    "deploy",
    "mode",
    "cluster",
    "basically",
    "means",
    "driver",
    "run",
    "one",
    "nodes",
    "cluster",
    "could",
    "submit",
    "application",
    "based",
    "whatever",
    "arguments",
    "given",
    "application",
    "submitted",
    "cluster",
    "run",
    "application",
    "running",
    "said",
    "application",
    "driver",
    "also",
    "either",
    "spark",
    "session",
    "spark",
    "context",
    "takes",
    "application",
    "request",
    "resource",
    "manager",
    "application",
    "completed",
    "always",
    "come",
    "back",
    "look",
    "spark",
    "history",
    "server",
    "spark",
    "ui",
    "choose",
    "application",
    "run",
    "go",
    "executors",
    "shows",
    "one",
    "driver",
    "running",
    "particular",
    "node",
    "client",
    "node",
    "executors",
    "nodes",
    "nodes",
    "cluster",
    "used",
    "one",
    "core",
    "ran",
    "two",
    "tasks",
    "partitions",
    "working",
    "shuffling",
    "involved",
    "word",
    "count",
    "uses",
    "reduce",
    "key",
    "run",
    "application",
    "yarn",
    "cluster",
    "manager",
    "resource",
    "manager",
    "negotiates",
    "resources",
    "slave",
    "processes",
    "worker",
    "node",
    "basically",
    "resources",
    "available",
    "kind",
    "execution",
    "said",
    "resource",
    "manager",
    "request",
    "containers",
    "worker",
    "nodes",
    "approve",
    "containers",
    "within",
    "containers",
    "executor",
    "run",
    "take",
    "care",
    "task",
    "process",
    "data",
    "task",
    "unit",
    "work",
    "data",
    "set",
    "worked",
    "upon",
    "rdds",
    "get",
    "created",
    "partitions",
    "every",
    "partition",
    "task",
    "taken",
    "care",
    "executor",
    "data",
    "loaded",
    "rdd",
    "action",
    "invoked",
    "task",
    "worked",
    "upon",
    "executor",
    "basically",
    "gets",
    "data",
    "partition",
    "rdd",
    "execution",
    "task",
    "particular",
    "partition",
    "results",
    "sent",
    "back",
    "driver",
    "also",
    "output",
    "saved",
    "disk",
    "application",
    "run",
    "whenever",
    "run",
    "particular",
    "application",
    "always",
    "go",
    "web",
    "console",
    "scroll",
    "look",
    "logs",
    "information",
    "application",
    "run",
    "see",
    "right",
    "beginning",
    "talk",
    "spark",
    "submit",
    "done",
    "says",
    "running",
    "spark",
    "version",
    "basically",
    "see",
    "driver",
    "started",
    "see",
    "memory",
    "calculations",
    "starts",
    "spark",
    "ui",
    "see",
    "requesting",
    "new",
    "application",
    "cluster",
    "four",
    "node",
    "managers",
    "verify",
    "application",
    "requested",
    "maximum",
    "memory",
    "capability",
    "cluster",
    "container",
    "sizing",
    "cluster",
    "level",
    "3",
    "gb",
    "per",
    "container",
    "application",
    "requesting",
    "container",
    "bigger",
    "starts",
    "application",
    "master",
    "container",
    "specific",
    "amount",
    "memory",
    "finally",
    "execution",
    "starts",
    "scroll",
    "look",
    "logs",
    "see",
    "driver",
    "runs",
    "also",
    "see",
    "show",
    "much",
    "memory",
    "utilized",
    "talk",
    "back",
    "scheduler",
    "taking",
    "care",
    "execution",
    "tag",
    "series",
    "rdds",
    "finally",
    "see",
    "result",
    "getting",
    "generated",
    "saved",
    "run",
    "spark",
    "application",
    "see",
    "spark",
    "applications",
    "history",
    "server",
    "spark",
    "ui",
    "overall",
    "apache",
    "spark",
    "works",
    "utilizing",
    "cluster",
    "manager",
    "help",
    "getting",
    "resources",
    "worker",
    "processes",
    "running",
    "executors",
    "within",
    "learn",
    "spark",
    "streaming",
    "spark",
    "streaming",
    "data",
    "sources",
    "features",
    "spark",
    "streaming",
    "working",
    "spark",
    "streaming",
    "disk",
    "criticized",
    "streams",
    "caching",
    "persistence",
    "call",
    "check",
    "pointing",
    "spark",
    "streaming",
    "demo",
    "spark",
    "streaming",
    "let",
    "learn",
    "spark",
    "streaming",
    "capable",
    "extension",
    "core",
    "spark",
    "api",
    "basically",
    "enables",
    "scalable",
    "high",
    "throughput",
    "fault",
    "tolerant",
    "stream",
    "processing",
    "live",
    "data",
    "streams",
    "data",
    "ingested",
    "different",
    "sources",
    "like",
    "kafka",
    "flume",
    "kinases",
    "tcp",
    "sockets",
    "processed",
    "using",
    "complex",
    "algorithms",
    "expressed",
    "different",
    "kind",
    "functions",
    "map",
    "reduce",
    "join",
    "window",
    "data",
    "processed",
    "data",
    "pushed",
    "file",
    "systems",
    "databases",
    "live",
    "dashboards",
    "look",
    "image",
    "clearly",
    "see",
    "would",
    "working",
    "input",
    "data",
    "stream",
    "goes",
    "spark",
    "streaming",
    "component",
    "spark",
    "gives",
    "us",
    "batches",
    "input",
    "data",
    "could",
    "say",
    "streaming",
    "data",
    "broken",
    "smaller",
    "patches",
    "would",
    "worked",
    "upon",
    "spark",
    "core",
    "engine",
    "finally",
    "batches",
    "processed",
    "data",
    "talk",
    "spark",
    "streaming",
    "data",
    "sources",
    "could",
    "streaming",
    "data",
    "sources",
    "coming",
    "kafka",
    "flume",
    "say",
    "twitter",
    "api",
    "also",
    "different",
    "formats",
    "parquet",
    "could",
    "also",
    "static",
    "data",
    "sources",
    "coming",
    "mongodb",
    "hbase",
    "mysql",
    "postgres",
    "talk",
    "spark",
    "streaming",
    "receives",
    "input",
    "data",
    "streams",
    "divides",
    "data",
    "patches",
    "processed",
    "spark",
    "engine",
    "generate",
    "final",
    "stream",
    "results",
    "batches",
    "spark",
    "streaming",
    "actually",
    "provides",
    "high",
    "level",
    "abstraction",
    "called",
    "disk",
    "criticize",
    "stream",
    "learn",
    "call",
    "stream",
    "represents",
    "continuous",
    "stream",
    "data",
    "look",
    "different",
    "data",
    "sources",
    "data",
    "come",
    "spark",
    "streaming",
    "would",
    "take",
    "input",
    "could",
    "even",
    "mlib",
    "component",
    "could",
    "used",
    "sparks",
    "component",
    "build",
    "machine",
    "learning",
    "algorithms",
    "wherein",
    "train",
    "models",
    "live",
    "data",
    "even",
    "use",
    "trained",
    "model",
    "could",
    "also",
    "going",
    "structured",
    "processing",
    "processing",
    "data",
    "structuralized",
    "could",
    "done",
    "using",
    "sparks",
    "components",
    "spark",
    "sql",
    "data",
    "frames",
    "data",
    "sets",
    "could",
    "process",
    "data",
    "data",
    "frames",
    "interactively",
    "query",
    "sql",
    "spark",
    "streaming",
    "working",
    "data",
    "constantly",
    "flowing",
    "finally",
    "data",
    "processed",
    "stored",
    "distributed",
    "file",
    "system",
    "sdfs",
    "nosql",
    "sql",
    "based",
    "database",
    "talk",
    "spark",
    "streaming",
    "good",
    "know",
    "features",
    "spark",
    "streaming",
    "features",
    "enables",
    "fast",
    "recovery",
    "failures",
    "working",
    "streaming",
    "data",
    "better",
    "load",
    "balancing",
    "resource",
    "usage",
    "also",
    "combine",
    "streaming",
    "data",
    "static",
    "data",
    "sets",
    "perform",
    "interactive",
    "queries",
    "spark",
    "streaming",
    "also",
    "supports",
    "native",
    "integration",
    "advanced",
    "processing",
    "libraries",
    "one",
    "benefits",
    "users",
    "using",
    "spark",
    "streaming",
    "let",
    "learn",
    "working",
    "spark",
    "streaming",
    "mentioned",
    "earlier",
    "one",
    "end",
    "data",
    "streams",
    "data",
    "streams",
    "caught",
    "perceived",
    "receivers",
    "enable",
    "application",
    "data",
    "streams",
    "streaming",
    "data",
    "constantly",
    "getting",
    "generated",
    "flowing",
    "broken",
    "smaller",
    "patches",
    "processed",
    "spark",
    "would",
    "final",
    "processed",
    "results",
    "look",
    "bigger",
    "picture",
    "talk",
    "live",
    "input",
    "data",
    "streams",
    "could",
    "divided",
    "smaller",
    "batches",
    "say",
    "batches",
    "batches",
    "input",
    "data",
    "rdds",
    "spark",
    "streaming",
    "performs",
    "computation",
    "expressed",
    "using",
    "streams",
    "generates",
    "rdd",
    "transformations",
    "would",
    "spark",
    "pad",
    "jobs",
    "execute",
    "rdd",
    "transformations",
    "would",
    "give",
    "final",
    "processed",
    "result",
    "various",
    "examples",
    "look",
    "examples",
    "later",
    "spark",
    "streaming",
    "works",
    "breaking",
    "input",
    "smaller",
    "streams",
    "process",
    "data",
    "finally",
    "giving",
    "batches",
    "result",
    "streaming",
    "data",
    "streams",
    "let",
    "understand",
    "streams",
    "disk",
    "criticized",
    "streams",
    "basic",
    "abstraction",
    "provided",
    "spark",
    "streaming",
    "represents",
    "continuous",
    "stream",
    "data",
    "either",
    "input",
    "data",
    "stream",
    "received",
    "source",
    "process",
    "data",
    "stream",
    "generated",
    "transforming",
    "input",
    "stream",
    "look",
    "stream",
    "would",
    "say",
    "would",
    "series",
    "rdds",
    "series",
    "transformations",
    "applied",
    "data",
    "flowing",
    "particular",
    "time",
    "frame",
    "say",
    "data",
    "time",
    "0",
    "1",
    "would",
    "result",
    "transformations",
    "would",
    "perform",
    "data",
    "come",
    "time",
    "zone",
    "time",
    "frame",
    "would",
    "data",
    "time",
    "1",
    "2",
    "spark",
    "streaming",
    "works",
    "look",
    "different",
    "transformations",
    "could",
    "various",
    "transformations",
    "could",
    "applied",
    "data",
    "something",
    "like",
    "streaming",
    "data",
    "comes",
    "would",
    "want",
    "say",
    "receiver",
    "monitors",
    "particular",
    "socket",
    "particular",
    "port",
    "looks",
    "data",
    "coming",
    "would",
    "also",
    "define",
    "time",
    "interval",
    "time",
    "interval",
    "data",
    "taken",
    "smaller",
    "batch",
    "stream",
    "could",
    "processing",
    "done",
    "within",
    "application",
    "would",
    "series",
    "steps",
    "nothing",
    "transformations",
    "would",
    "performed",
    "data",
    "within",
    "time",
    "frame",
    "giving",
    "result",
    "could",
    "stored",
    "could",
    "seen",
    "console",
    "could",
    "pushed",
    "processing",
    "keeps",
    "happening",
    "regular",
    "time",
    "intervals",
    "whatever",
    "specified",
    "till",
    "spark",
    "streaming",
    "application",
    "continues",
    "talk",
    "spark",
    "already",
    "know",
    "different",
    "kind",
    "transformations",
    "applied",
    "map",
    "transformation",
    "wherein",
    "map",
    "pass",
    "function",
    "basically",
    "says",
    "would",
    "want",
    "perform",
    "function",
    "every",
    "element",
    "say",
    "map",
    "function",
    "would",
    "return",
    "new",
    "stream",
    "passing",
    "element",
    "source",
    "stream",
    "function",
    "passed",
    "similarly",
    "flat",
    "map",
    "would",
    "passing",
    "function",
    "would",
    "want",
    "perform",
    "flat",
    "map",
    "transformation",
    "data",
    "stream",
    "stream",
    "comes",
    "input",
    "item",
    "mapped",
    "zero",
    "output",
    "elements",
    "could",
    "filter",
    "wherein",
    "return",
    "stream",
    "selecting",
    "records",
    "source",
    "stream",
    "function",
    "returns",
    "true",
    "filtering",
    "used",
    "want",
    "run",
    "transformation",
    "would",
    "want",
    "look",
    "input",
    "data",
    "particular",
    "time",
    "interval",
    "mentioned",
    "earlier",
    "would",
    "want",
    "filter",
    "data",
    "per",
    "whatever",
    "function",
    "applied",
    "could",
    "union",
    "could",
    "basically",
    "union",
    "multiple",
    "streams",
    "would",
    "return",
    "new",
    "stream",
    "contains",
    "union",
    "elements",
    "source",
    "tree",
    "stream",
    "string",
    "could",
    "transform",
    "function",
    "contains",
    "union",
    "elements",
    "could",
    "count",
    "could",
    "join",
    "transformations",
    "performed",
    "streams",
    "also",
    "concept",
    "windowing",
    "basically",
    "process",
    "data",
    "series",
    "time",
    "intervals",
    "mention",
    "windowed",
    "stream",
    "processing",
    "saying",
    "spark",
    "streaming",
    "would",
    "allow",
    "apply",
    "transformations",
    "sliding",
    "window",
    "data",
    "operation",
    "called",
    "windowed",
    "computation",
    "let",
    "see",
    "looks",
    "like",
    "original",
    "stream",
    "basically",
    "data",
    "coming",
    "would",
    "looked",
    "upon",
    "specific",
    "time",
    "intervals",
    "time",
    "one",
    "time",
    "two",
    "could",
    "windowed",
    "computation",
    "could",
    "basically",
    "mean",
    "could",
    "window",
    "series",
    "time",
    "intervals",
    "would",
    "want",
    "perform",
    "series",
    "rdd",
    "transformations",
    "window",
    "stream",
    "time",
    "one",
    "time",
    "two",
    "time",
    "three",
    "could",
    "one",
    "window",
    "wherein",
    "would",
    "want",
    "get",
    "output",
    "see",
    "window",
    "time",
    "three",
    "could",
    "also",
    "another",
    "sliding",
    "window",
    "would",
    "take",
    "time",
    "series",
    "time",
    "3",
    "time",
    "4",
    "time",
    "5",
    "could",
    "performing",
    "series",
    "transformations",
    "usually",
    "helpful",
    "would",
    "want",
    "process",
    "data",
    "particular",
    "time",
    "interval",
    "would",
    "want",
    "consolidated",
    "processing",
    "series",
    "intervals",
    "mean",
    "windowed",
    "computation",
    "understand",
    "caching",
    "persistence",
    "talk",
    "little",
    "bit",
    "windowing",
    "one",
    "would",
    "want",
    "understand",
    "window",
    "stream",
    "processing",
    "say",
    "spark",
    "streaming",
    "feature",
    "windowed",
    "computations",
    "need",
    "think",
    "applying",
    "transformations",
    "sliding",
    "window",
    "data",
    "see",
    "every",
    "time",
    "window",
    "slides",
    "source",
    "stream",
    "source",
    "rdds",
    "fall",
    "within",
    "window",
    "combined",
    "operated",
    "upon",
    "produce",
    "rdds",
    "windowed",
    "stream",
    "specific",
    "case",
    "say",
    "operation",
    "applied",
    "last",
    "three",
    "time",
    "units",
    "data",
    "slides",
    "two",
    "time",
    "units",
    "shows",
    "window",
    "operation",
    "needs",
    "specify",
    "two",
    "parameters",
    "one",
    "window",
    "length",
    "basically",
    "duration",
    "window",
    "example",
    "say",
    "three",
    "see",
    "figure",
    "sliding",
    "interval",
    "basically",
    "interval",
    "window",
    "operation",
    "performed",
    "two",
    "parameters",
    "must",
    "multiples",
    "batch",
    "interval",
    "source",
    "stream",
    "talk",
    "window",
    "various",
    "transformations",
    "applied",
    "window",
    "based",
    "operations",
    "done",
    "streams",
    "talk",
    "little",
    "bit",
    "streams",
    "said",
    "basic",
    "abstraction",
    "provided",
    "spark",
    "streaming",
    "represents",
    "remember",
    "continuous",
    "stream",
    "data",
    "either",
    "input",
    "data",
    "stream",
    "received",
    "source",
    "process",
    "data",
    "stream",
    "generated",
    "transforming",
    "input",
    "stream",
    "stream",
    "represented",
    "could",
    "say",
    "continuous",
    "series",
    "rdds",
    "sparks",
    "abstraction",
    "immutable",
    "distributed",
    "data",
    "set",
    "operation",
    "applied",
    "stream",
    "basically",
    "translating",
    "operations",
    "underlying",
    "rdds",
    "example",
    "say",
    "converting",
    "stream",
    "lines",
    "word",
    "flat",
    "map",
    "operation",
    "applied",
    "rdd",
    "lines",
    "stream",
    "generate",
    "rtds",
    "words",
    "stream",
    "talk",
    "discretized",
    "streams",
    "understand",
    "would",
    "series",
    "transformations",
    "would",
    "applied",
    "time",
    "interval",
    "whatever",
    "specified",
    "talk",
    "streaming",
    "spark",
    "streaming",
    "architecture",
    "mentioned",
    "receivers",
    "plays",
    "important",
    "role",
    "input",
    "streams",
    "data",
    "streams",
    "representing",
    "stream",
    "input",
    "data",
    "received",
    "streaming",
    "sources",
    "could",
    "different",
    "kind",
    "data",
    "could",
    "different",
    "kind",
    "transformations",
    "receiver",
    "basically",
    "object",
    "receives",
    "data",
    "source",
    "stores",
    "sparks",
    "memory",
    "processing",
    "main",
    "role",
    "receiver",
    "spark",
    "streaming",
    "provides",
    "two",
    "categories",
    "building",
    "streaming",
    "sources",
    "basic",
    "sources",
    "sources",
    "directly",
    "available",
    "streaming",
    "context",
    "class",
    "learn",
    "file",
    "systems",
    "socket",
    "connections",
    "could",
    "basic",
    "sources",
    "data",
    "coming",
    "could",
    "advanced",
    "sources",
    "like",
    "kafka",
    "flume",
    "kinases",
    "etc",
    "would",
    "available",
    "extra",
    "utility",
    "classes",
    "receiver",
    "going",
    "looking",
    "data",
    "constantly",
    "getting",
    "generated",
    "basically",
    "forwarding",
    "processing",
    "spark",
    "streaming",
    "know",
    "talk",
    "spark",
    "streaming",
    "one",
    "important",
    "aspect",
    "basically",
    "understanding",
    "caching",
    "persistence",
    "know",
    "spar",
    "core",
    "engine",
    "know",
    "rdds",
    "say",
    "logical",
    "steps",
    "rdds",
    "created",
    "perform",
    "transformations",
    "transformations",
    "computations",
    "rdds",
    "cached",
    "improve",
    "performance",
    "application",
    "computed",
    "rdds",
    "rdds",
    "result",
    "performing",
    "transformations",
    "cached",
    "reutilized",
    "application",
    "processing",
    "talk",
    "caching",
    "persistence",
    "streams",
    "also",
    "allows",
    "developers",
    "persist",
    "stream",
    "data",
    "memory",
    "similar",
    "concept",
    "rdds",
    "streams",
    "allow",
    "persist",
    "particular",
    "streams",
    "data",
    "memory",
    "using",
    "persist",
    "method",
    "stream",
    "automatically",
    "persist",
    "every",
    "rdd",
    "stream",
    "memory",
    "could",
    "every",
    "rdd",
    "could",
    "specifically",
    "chosen",
    "rdds",
    "really",
    "useful",
    "data",
    "stream",
    "would",
    "computed",
    "multiple",
    "times",
    "say",
    "application",
    "example",
    "say",
    "window",
    "based",
    "operations",
    "like",
    "reduce",
    "window",
    "reduce",
    "key",
    "window",
    "wherein",
    "group",
    "operations",
    "done",
    "could",
    "state",
    "based",
    "operations",
    "like",
    "update",
    "state",
    "key",
    "cases",
    "streams",
    "generated",
    "say",
    "window",
    "based",
    "operations",
    "automatically",
    "persistent",
    "memory",
    "without",
    "developer",
    "calling",
    "persist",
    "method",
    "input",
    "streams",
    "receive",
    "data",
    "networks",
    "kafka",
    "flume",
    "sockets",
    "etc",
    "default",
    "persistence",
    "level",
    "set",
    "replicate",
    "data",
    "two",
    "nodes",
    "fault",
    "tolerance",
    "one",
    "thing",
    "remember",
    "unlike",
    "rdds",
    "default",
    "persistence",
    "level",
    "streams",
    "keeps",
    "data",
    "data",
    "serialized",
    "memory",
    "discuss",
    "serializations",
    "deserialization",
    "one",
    "important",
    "aspect",
    "takes",
    "care",
    "fault",
    "recovery",
    "check",
    "pointing",
    "mechanism",
    "spark",
    "streaming",
    "say",
    "check",
    "pointing",
    "streaming",
    "application",
    "uh",
    "real",
    "scenario",
    "would",
    "want",
    "must",
    "operate",
    "24",
    "bar",
    "7",
    "streaming",
    "application",
    "constantly",
    "running",
    "mechanism",
    "make",
    "streaming",
    "application",
    "resilient",
    "failures",
    "unrelated",
    "application",
    "logic",
    "spark",
    "streaming",
    "needs",
    "check",
    "pointing",
    "needs",
    "checkpoint",
    "enough",
    "information",
    "fault",
    "tolerant",
    "underlying",
    "storage",
    "system",
    "recover",
    "failures",
    "check",
    "pointing",
    "process",
    "make",
    "streaming",
    "applications",
    "fault",
    "tolerant",
    "resilient",
    "failures",
    "usually",
    "used",
    "would",
    "want",
    "recover",
    "failure",
    "node",
    "running",
    "driver",
    "streaming",
    "application",
    "know",
    "driver",
    "existing",
    "every",
    "application",
    "basically",
    "one",
    "knows",
    "flow",
    "application",
    "driver",
    "also",
    "context",
    "case",
    "streaming",
    "application",
    "would",
    "say",
    "spark",
    "streaming",
    "context",
    "entry",
    "point",
    "application",
    "talk",
    "check",
    "pointing",
    "ensure",
    "streaming",
    "application",
    "fault",
    "tolerant",
    "two",
    "kinds",
    "checkpointing",
    "could",
    "metadata",
    "checkpointing",
    "could",
    "data",
    "checkpointing",
    "talk",
    "metadata",
    "include",
    "metadata",
    "includes",
    "configuration",
    "stream",
    "operations",
    "even",
    "incomplete",
    "batches",
    "talk",
    "metadata",
    "configurations",
    "configuration",
    "used",
    "create",
    "streaming",
    "application",
    "could",
    "stream",
    "operations",
    "set",
    "stream",
    "operations",
    "define",
    "streaming",
    "application",
    "series",
    "rdds",
    "incomplete",
    "batches",
    "batches",
    "whose",
    "jobs",
    "queued",
    "completed",
    "would",
    "check",
    "pointed",
    "metadata",
    "check",
    "pointing",
    "used",
    "recovering",
    "node",
    "failure",
    "running",
    "streaming",
    "application",
    "driver",
    "metadata",
    "configuration",
    "incomplete",
    "batches",
    "stream",
    "operations",
    "need",
    "saved",
    "underlying",
    "storage",
    "system",
    "talk",
    "data",
    "check",
    "pointing",
    "mainly",
    "saving",
    "generated",
    "rtds",
    "reliable",
    "storage",
    "whatever",
    "rdds",
    "computed",
    "saving",
    "information",
    "saving",
    "computations",
    "storage",
    "like",
    "sdfs",
    "used",
    "stateful",
    "transformations",
    "combining",
    "data",
    "across",
    "various",
    "batches",
    "talk",
    "transformations",
    "whatever",
    "rtds",
    "generated",
    "depend",
    "rdds",
    "previous",
    "batches",
    "talking",
    "stateful",
    "cause",
    "length",
    "dependency",
    "chain",
    "increase",
    "keep",
    "increasing",
    "time",
    "avoid",
    "kind",
    "increase",
    "recovery",
    "time",
    "intermediate",
    "rdds",
    "periodically",
    "checkpointed",
    "could",
    "done",
    "reliable",
    "storage",
    "basically",
    "cut",
    "growing",
    "dependency",
    "changes",
    "would",
    "want",
    "summarize",
    "would",
    "say",
    "metadata",
    "checkpointing",
    "primarily",
    "needed",
    "recovery",
    "driver",
    "failures",
    "whereas",
    "data",
    "rtd",
    "checkpointing",
    "necessary",
    "even",
    "basic",
    "functioning",
    "stateful",
    "transformations",
    "used",
    "remember",
    "talking",
    "stateful",
    "transformations",
    "talk",
    "check",
    "pointing",
    "question",
    "would",
    "enable",
    "checkpointing",
    "would",
    "enable",
    "checkpointing",
    "checkpointing",
    "must",
    "enabled",
    "applications",
    "different",
    "kind",
    "requirements",
    "example",
    "using",
    "stateful",
    "transformations",
    "one",
    "series",
    "rdds",
    "depend",
    "result",
    "previous",
    "batches",
    "something",
    "like",
    "update",
    "state",
    "key",
    "reduce",
    "key",
    "window",
    "kind",
    "operations",
    "used",
    "application",
    "checkpoint",
    "directory",
    "must",
    "provided",
    "allow",
    "periodic",
    "rdd",
    "checkpoint",
    "say",
    "requirements",
    "recovering",
    "failures",
    "driver",
    "taking",
    "care",
    "application",
    "metadata",
    "checkpoints",
    "used",
    "talk",
    "simple",
    "streaming",
    "applications",
    "without",
    "stateful",
    "transformations",
    "could",
    "run",
    "without",
    "enabling",
    "checkpointing",
    "one",
    "batch",
    "rdds",
    "really",
    "depend",
    "previous",
    "set",
    "rdds",
    "done",
    "previous",
    "time",
    "frame",
    "recovery",
    "driver",
    "failures",
    "also",
    "partial",
    "case",
    "talk",
    "stateless",
    "might",
    "received",
    "unprocessed",
    "data",
    "might",
    "lost",
    "pretty",
    "much",
    "acceptable",
    "talk",
    "spark",
    "streaming",
    "applications",
    "look",
    "checkpointing",
    "say",
    "start",
    "checkpointing",
    "whenever",
    "application",
    "creating",
    "streaming",
    "context",
    "would",
    "create",
    "would",
    "set",
    "checkpoint",
    "path",
    "would",
    "define",
    "stream",
    "nothing",
    "series",
    "rdd",
    "transformations",
    "streaming",
    "context",
    "starts",
    "basically",
    "application",
    "entry",
    "point",
    "cluster",
    "processing",
    "point",
    "time",
    "failure",
    "could",
    "always",
    "recover",
    "using",
    "checkpoint",
    "created",
    "case",
    "uh",
    "spark",
    "streaming",
    "one",
    "thing",
    "need",
    "remember",
    "checkpointing",
    "enabled",
    "setting",
    "directly",
    "fault",
    "tolerant",
    "reliable",
    "file",
    "system",
    "sdfs",
    "wherein",
    "checkpointing",
    "information",
    "saved",
    "add",
    "methods",
    "within",
    "application",
    "like",
    "streaming",
    "context",
    "checkpoint",
    "pointing",
    "checkpoint",
    "directory",
    "way",
    "stateful",
    "transformations",
    "metadata",
    "information",
    "stored",
    "underlying",
    "storage",
    "whichever",
    "chosen",
    "discussed",
    "spark",
    "streaming",
    "basics",
    "spark",
    "streaming",
    "let",
    "also",
    "understand",
    "shared",
    "variables",
    "call",
    "accumulator",
    "broadcast",
    "variables",
    "normally",
    "talk",
    "spark",
    "operations",
    "map",
    "reduce",
    "executed",
    "one",
    "node",
    "cluster",
    "happens",
    "talk",
    "operations",
    "work",
    "separate",
    "copies",
    "variables",
    "used",
    "function",
    "case",
    "variables",
    "copied",
    "machine",
    "updates",
    "variables",
    "remote",
    "machines",
    "propagated",
    "back",
    "driver",
    "program",
    "talk",
    "read",
    "write",
    "shared",
    "variables",
    "across",
    "tasks",
    "would",
    "inefficient",
    "spark",
    "actually",
    "provides",
    "two",
    "limited",
    "types",
    "shared",
    "variables",
    "common",
    "usage",
    "patterns",
    "broadcast",
    "variables",
    "accumulators",
    "talk",
    "accumulators",
    "variables",
    "added",
    "associative",
    "commutative",
    "operation",
    "spark",
    "natively",
    "supports",
    "accumulators",
    "numeric",
    "types",
    "programmers",
    "add",
    "support",
    "new",
    "types",
    "talk",
    "accumulators",
    "used",
    "implement",
    "counters",
    "mapreduce",
    "sums",
    "user",
    "create",
    "named",
    "unnamed",
    "accumulators",
    "see",
    "image",
    "named",
    "accumulator",
    "instance",
    "counter",
    "display",
    "web",
    "ui",
    "stage",
    "modifies",
    "accumulator",
    "spark",
    "displays",
    "value",
    "accumulator",
    "modified",
    "task",
    "tasks",
    "table",
    "tracking",
    "accumulators",
    "ui",
    "useful",
    "understanding",
    "progress",
    "running",
    "stages",
    "remember",
    "supported",
    "python",
    "might",
    "future",
    "support",
    "python",
    "also",
    "added",
    "talk",
    "accumulators",
    "say",
    "numeric",
    "accumulator",
    "created",
    "calling",
    "spark",
    "context",
    "method",
    "long",
    "accumulator",
    "could",
    "double",
    "accumulator",
    "accumulate",
    "values",
    "type",
    "long",
    "double",
    "respectively",
    "tasks",
    "running",
    "cluster",
    "add",
    "using",
    "add",
    "method",
    "mean",
    "case",
    "value",
    "read",
    "driver",
    "program",
    "read",
    "accumulator",
    "value",
    "using",
    "value",
    "method",
    "look",
    "examples",
    "understand",
    "later",
    "talk",
    "broadcast",
    "variables",
    "one",
    "type",
    "variables",
    "allows",
    "programmers",
    "keep",
    "read",
    "variable",
    "cached",
    "machine",
    "rather",
    "shipping",
    "copy",
    "tasks",
    "know",
    "sometimes",
    "might",
    "costlier",
    "operations",
    "like",
    "joins",
    "might",
    "working",
    "multiple",
    "rdds",
    "need",
    "joined",
    "rtds",
    "could",
    "also",
    "pair",
    "rdds",
    "could",
    "key",
    "value",
    "pairs",
    "whenever",
    "performing",
    "join",
    "would",
    "kind",
    "two",
    "level",
    "shuffling",
    "one",
    "within",
    "particular",
    "rdd",
    "joining",
    "two",
    "rdds",
    "first",
    "rdd",
    "might",
    "data",
    "form",
    "key",
    "value",
    "pairs",
    "rdd",
    "would",
    "shuffling",
    "similar",
    "keys",
    "brought",
    "one",
    "partition",
    "would",
    "happen",
    "second",
    "rdd",
    "also",
    "key",
    "value",
    "pairs",
    "join",
    "rtds",
    "shipped",
    "node",
    "basically",
    "data",
    "would",
    "loaded",
    "memory",
    "basically",
    "transformations",
    "happen",
    "costlier",
    "affair",
    "done",
    "could",
    "create",
    "broadcast",
    "variables",
    "example",
    "two",
    "rdds",
    "want",
    "perform",
    "join",
    "operation",
    "one",
    "rdd",
    "known",
    "smaller",
    "create",
    "broadcast",
    "variable",
    "smaller",
    "rdd",
    "variable",
    "shipped",
    "machine",
    "variable",
    "used",
    "join",
    "operations",
    "rdd",
    "existing",
    "memory",
    "nodes",
    "saves",
    "time",
    "improves",
    "performance",
    "spark",
    "basically",
    "attempts",
    "distribute",
    "broadcast",
    "variable",
    "using",
    "efficient",
    "broadcast",
    "algorithms",
    "reduce",
    "communication",
    "cost",
    "example",
    "multiple",
    "nodes",
    "cluster",
    "would",
    "want",
    "give",
    "every",
    "node",
    "copy",
    "large",
    "input",
    "data",
    "set",
    "efficient",
    "manner",
    "spark",
    "actions",
    "executed",
    "set",
    "stages",
    "know",
    "stages",
    "separated",
    "shuffle",
    "operations",
    "whenever",
    "talk",
    "narrow",
    "dependencies",
    "map",
    "flat",
    "map",
    "filter",
    "would",
    "shuffling",
    "involved",
    "go",
    "group",
    "key",
    "reduce",
    "key",
    "operations",
    "bring",
    "similar",
    "keys",
    "together",
    "would",
    "shuffling",
    "involved",
    "also",
    "applies",
    "join",
    "operations",
    "case",
    "broadcast",
    "variables",
    "could",
    "really",
    "plus",
    "one",
    "set",
    "data",
    "rdd",
    "already",
    "computed",
    "could",
    "broadcasted",
    "nodes",
    "data",
    "broadcasted",
    "cached",
    "serialized",
    "form",
    "deserialized",
    "running",
    "task",
    "means",
    "explicitly",
    "creating",
    "broadcast",
    "variables",
    "useful",
    "tasks",
    "across",
    "multiple",
    "stages",
    "need",
    "data",
    "caching",
    "data",
    "serialized",
    "form",
    "important",
    "create",
    "broadcast",
    "variables",
    "using",
    "spark",
    "context",
    "method",
    "called",
    "broadcast",
    "broadcast",
    "variable",
    "shipped",
    "nodes",
    "used",
    "operations",
    "talk",
    "smart",
    "streaming",
    "used",
    "various",
    "use",
    "cases",
    "mean",
    "could",
    "talk",
    "speech",
    "recognition",
    "could",
    "talk",
    "sentiment",
    "analysis",
    "could",
    "talk",
    "streaming",
    "applications",
    "would",
    "performing",
    "kind",
    "analytics",
    "data",
    "coming",
    "also",
    "used",
    "widely",
    "retail",
    "chain",
    "companies",
    "look",
    "example",
    "big",
    "retail",
    "chain",
    "companies",
    "would",
    "want",
    "build",
    "dashboards",
    "keep",
    "track",
    "inventory",
    "operations",
    "would",
    "need",
    "one",
    "streaming",
    "data",
    "data",
    "constantly",
    "getting",
    "generated",
    "source",
    "needs",
    "processed",
    "information",
    "populating",
    "dashboards",
    "give",
    "scenario",
    "information",
    "happening",
    "case",
    "inventory",
    "dashboard",
    "could",
    "use",
    "interactive",
    "dashboards",
    "uh",
    "wherein",
    "could",
    "draw",
    "insights",
    "business",
    "retail",
    "companies",
    "many",
    "products",
    "purchased",
    "products",
    "shipped",
    "many",
    "products",
    "delivered",
    "customers",
    "kind",
    "information",
    "would",
    "good",
    "capture",
    "real",
    "time",
    "streaming",
    "data",
    "basically",
    "processed",
    "one",
    "end",
    "data",
    "getting",
    "generated",
    "might",
    "based",
    "sales",
    "happening",
    "might",
    "based",
    "products",
    "shipped",
    "might",
    "based",
    "acknowledgement",
    "products",
    "received",
    "data",
    "getting",
    "generated",
    "various",
    "sources",
    "subjected",
    "smart",
    "streaming",
    "application",
    "look",
    "streaming",
    "data",
    "perform",
    "series",
    "transformations",
    "would",
    "want",
    "process",
    "data",
    "regular",
    "time",
    "intervals",
    "pushing",
    "dashboards",
    "storage",
    "layer",
    "wherein",
    "could",
    "used",
    "answer",
    "questions",
    "talk",
    "spark",
    "streaming",
    "ideal",
    "choice",
    "process",
    "kind",
    "data",
    "real",
    "time",
    "various",
    "use",
    "cases",
    "one",
    "end",
    "see",
    "input",
    "stream",
    "shows",
    "product",
    "status",
    "many",
    "products",
    "purchased",
    "shipped",
    "delivered",
    "would",
    "handled",
    "spark",
    "streaming",
    "also",
    "sparkcore",
    "engine",
    "would",
    "process",
    "data",
    "give",
    "output",
    "stream",
    "gives",
    "status",
    "total",
    "count",
    "products",
    "purchased",
    "products",
    "shipped",
    "products",
    "delivered",
    "quick",
    "brief",
    "introduction",
    "spark",
    "streaming",
    "works",
    "also",
    "see",
    "spark",
    "streaming",
    "works",
    "create",
    "application",
    "basically",
    "set",
    "eclipse",
    "scala",
    "based",
    "spark",
    "applications",
    "could",
    "could",
    "eclipse",
    "scala",
    "added",
    "somebody",
    "would",
    "want",
    "look",
    "scala",
    "plugin",
    "always",
    "go",
    "place",
    "scroll",
    "towards",
    "bottom",
    "click",
    "stable",
    "basically",
    "also",
    "shows",
    "video",
    "scala",
    "plugin",
    "added",
    "eclipse",
    "also",
    "tells",
    "get",
    "scala",
    "latest",
    "release",
    "added",
    "eclipse",
    "case",
    "already",
    "added",
    "eclipse",
    "bringing",
    "applications",
    "built",
    "using",
    "ide",
    "people",
    "would",
    "prefer",
    "use",
    "intellij",
    "also",
    "fine",
    "could",
    "also",
    "look",
    "videos",
    "setting",
    "intellij",
    "scala",
    "plugin",
    "two",
    "ways",
    "one",
    "build",
    "application",
    "run",
    "windows",
    "machine",
    "could",
    "kind",
    "utility",
    "like",
    "netcat",
    "used",
    "send",
    "messages",
    "streaming",
    "fashion",
    "could",
    "receiver",
    "looks",
    "particular",
    "socket",
    "particular",
    "port",
    "could",
    "build",
    "streaming",
    "application",
    "within",
    "ide",
    "run",
    "windows",
    "machine",
    "local",
    "mode",
    "would",
    "looking",
    "source",
    "data",
    "getting",
    "generated",
    "one",
    "option",
    "second",
    "option",
    "could",
    "build",
    "application",
    "package",
    "jar",
    "run",
    "cluster",
    "could",
    "spark",
    "standalone",
    "cluster",
    "spark",
    "yarn",
    "could",
    "packaging",
    "application",
    "using",
    "tools",
    "like",
    "sbt",
    "use",
    "spark",
    "submit",
    "submit",
    "application",
    "show",
    "ways",
    "work",
    "streaming",
    "applications",
    "first",
    "get",
    "eclipse",
    "make",
    "sure",
    "scala",
    "already",
    "added",
    "case",
    "right",
    "top",
    "corner",
    "see",
    "scala",
    "symbol",
    "says",
    "using",
    "scholar",
    "perspective",
    "projects",
    "create",
    "project",
    "saying",
    "new",
    "scala",
    "project",
    "give",
    "name",
    "example",
    "could",
    "say",
    "apps3",
    "say",
    "finish",
    "creates",
    "project",
    "within",
    "project",
    "create",
    "package",
    "example",
    "could",
    "say",
    "something",
    "like",
    "main",
    "dot",
    "scala",
    "say",
    "finish",
    "creates",
    "package",
    "also",
    "one",
    "thing",
    "remember",
    "would",
    "good",
    "change",
    "compiler",
    "instead",
    "normally",
    "different",
    "environments",
    "might",
    "spark",
    "different",
    "versions",
    "scala",
    "two",
    "point",
    "turner",
    "would",
    "good",
    "compiler",
    "change",
    "bundle",
    "select",
    "project",
    "right",
    "click",
    "go",
    "build",
    "path",
    "say",
    "configure",
    "build",
    "path",
    "click",
    "scala",
    "compiler",
    "use",
    "project",
    "settings",
    "choose",
    "bundle",
    "would",
    "make",
    "sure",
    "applications",
    "compile",
    "say",
    "okay",
    "might",
    "say",
    "compiler",
    "settings",
    "changed",
    "full",
    "rebuild",
    "required",
    "changes",
    "take",
    "effect",
    "hit",
    "ok",
    "first",
    "thing",
    "need",
    "second",
    "thing",
    "code",
    "compile",
    "would",
    "good",
    "jars",
    "added",
    "build",
    "path",
    "could",
    "said",
    "could",
    "writing",
    "application",
    "might",
    "compile",
    "within",
    "ide",
    "could",
    "package",
    "using",
    "sbt",
    "run",
    "jar",
    "machine",
    "within",
    "c",
    "drive",
    "already",
    "spark",
    "distribution",
    "downloaded",
    "basically",
    "downloaded",
    "spark",
    "untied",
    "spark",
    "unzipped",
    "untied",
    "kept",
    "c",
    "basically",
    "spark",
    "within",
    "jars",
    "spark",
    "related",
    "jars",
    "technically",
    "speaking",
    "even",
    "use",
    "windows",
    "command",
    "line",
    "start",
    "working",
    "spark",
    "interactive",
    "way",
    "packaging",
    "application",
    "running",
    "local",
    "mode",
    "could",
    "also",
    "basically",
    "spark",
    "see",
    "desktop",
    "hadoop",
    "folder",
    "within",
    "bin",
    "folder",
    "within",
    "win",
    "basically",
    "required",
    "want",
    "try",
    "spark",
    "based",
    "applications",
    "whether",
    "streaming",
    "data",
    "frames",
    "tested",
    "windows",
    "machine",
    "always",
    "search",
    "download",
    "place",
    "hadoop",
    "folder",
    "within",
    "bin",
    "folder",
    "machine",
    "get",
    "charts",
    "project",
    "say",
    "right",
    "click",
    "go",
    "build",
    "path",
    "say",
    "configure",
    "build",
    "path",
    "add",
    "external",
    "jars",
    "select",
    "jars",
    "would",
    "want",
    "add",
    "build",
    "path",
    "code",
    "compile",
    "test",
    "windows",
    "machine",
    "say",
    "open",
    "basically",
    "apply",
    "okay",
    "created",
    "project",
    "package",
    "compiler",
    "changed",
    "added",
    "external",
    "chars",
    "spark",
    "related",
    "jars",
    "good",
    "enough",
    "code",
    "need",
    "streaming",
    "application",
    "need",
    "build",
    "let",
    "show",
    "existing",
    "project",
    "looks",
    "like",
    "within",
    "source",
    "way",
    "main",
    "package",
    "streaming",
    "application",
    "streaming",
    "application",
    "test",
    "work",
    "capturing",
    "data",
    "generated",
    "particular",
    "stream",
    "particular",
    "ip",
    "particular",
    "port",
    "would",
    "want",
    "series",
    "transformations",
    "gives",
    "example",
    "word",
    "count",
    "would",
    "want",
    "print",
    "results",
    "could",
    "also",
    "saving",
    "output",
    "particular",
    "location",
    "application",
    "need",
    "basically",
    "import",
    "certain",
    "packages",
    "say",
    "spark",
    "streaming",
    "spark",
    "context",
    "also",
    "spark",
    "conf",
    "packages",
    "need",
    "import",
    "created",
    "object",
    "called",
    "fifth",
    "app",
    "saying",
    "extends",
    "app",
    "within",
    "project",
    "already",
    "application",
    "defined",
    "main",
    "application",
    "defined",
    "main",
    "one",
    "application",
    "already",
    "existing",
    "new",
    "objects",
    "extending",
    "app",
    "need",
    "define",
    "main",
    "method",
    "saying",
    "val",
    "conf",
    "new",
    "spark",
    "conf",
    "creating",
    "configuration",
    "object",
    "setting",
    "application",
    "name",
    "test",
    "windows",
    "set",
    "master",
    "local",
    "advisable",
    "give",
    "one",
    "thread",
    "would",
    "creating",
    "receiver",
    "within",
    "application",
    "would",
    "utilize",
    "one",
    "thread",
    "saying",
    "set",
    "master",
    "local",
    "saying",
    "two",
    "threads",
    "also",
    "creating",
    "spark",
    "context",
    "need",
    "initialize",
    "based",
    "configuration",
    "object",
    "created",
    "previous",
    "step",
    "need",
    "create",
    "spark",
    "streaming",
    "context",
    "streaming",
    "context",
    "depends",
    "spark",
    "context",
    "given",
    "time",
    "interval",
    "10",
    "seconds",
    "time",
    "interval",
    "setting",
    "would",
    "want",
    "work",
    "stream",
    "data",
    "comes",
    "every",
    "10",
    "seconds",
    "particular",
    "socket",
    "setting",
    "receiver",
    "say",
    "stream",
    "rdd",
    "basically",
    "spark",
    "streaming",
    "context",
    "use",
    "socket",
    "text",
    "stream",
    "various",
    "methods",
    "spark",
    "streaming",
    "context",
    "example",
    "go",
    "dot",
    "shows",
    "different",
    "options",
    "file",
    "stream",
    "q",
    "stream",
    "socket",
    "stream",
    "receiver",
    "stream",
    "using",
    "socket",
    "text",
    "stream",
    "would",
    "want",
    "point",
    "machine",
    "say",
    "also",
    "specify",
    "port",
    "2222",
    "okay",
    "created",
    "configuration",
    "object",
    "spark",
    "context",
    "spark",
    "streaming",
    "context",
    "time",
    "interval",
    "set",
    "receiver",
    "use",
    "socket",
    "text",
    "stream",
    "method",
    "particular",
    "ip",
    "particular",
    "port",
    "specifying",
    "would",
    "want",
    "data",
    "gets",
    "generated",
    "machine",
    "particular",
    "port",
    "saying",
    "val",
    "word",
    "count",
    "would",
    "want",
    "work",
    "stream",
    "rdd",
    "stream",
    "would",
    "want",
    "flat",
    "map",
    "split",
    "data",
    "based",
    "space",
    "would",
    "want",
    "map",
    "every",
    "word",
    "word",
    "comma",
    "1",
    "would",
    "want",
    "reduce",
    "key",
    "reduce",
    "key",
    "pass",
    "specifically",
    "function",
    "would",
    "want",
    "could",
    "count",
    "could",
    "print",
    "result",
    "could",
    "even",
    "save",
    "output",
    "using",
    "java",
    "method",
    "create",
    "random",
    "string",
    "attached",
    "output",
    "done",
    "mentioning",
    "spark",
    "streaming",
    "context",
    "start",
    "basically",
    "trigger",
    "spark",
    "streaming",
    "context",
    "start",
    "run",
    "till",
    "terminate",
    "application",
    "run",
    "windows",
    "machine",
    "always",
    "look",
    "run",
    "configuration",
    "done",
    "environment",
    "basically",
    "want",
    "use",
    "let",
    "say",
    "okay",
    "let",
    "look",
    "configuration",
    "need",
    "set",
    "streaming",
    "application",
    "let",
    "look",
    "run",
    "configuration",
    "basically",
    "shows",
    "application",
    "started",
    "running",
    "environment",
    "see",
    "added",
    "hadoop",
    "underscore",
    "home",
    "pointing",
    "hadoop",
    "directory",
    "utils",
    "x",
    "also",
    "given",
    "spark",
    "local",
    "ip",
    "run",
    "configuration",
    "see",
    "streaming",
    "application",
    "started",
    "streaming",
    "application",
    "yet",
    "started",
    "probably",
    "running",
    "different",
    "application",
    "come",
    "back",
    "check",
    "go",
    "run",
    "scala",
    "application",
    "let",
    "see",
    "tries",
    "connect",
    "receiver",
    "trying",
    "find",
    "anything",
    "particular",
    "machine",
    "particular",
    "port",
    "receiver",
    "able",
    "find",
    "able",
    "establish",
    "connection",
    "go",
    "command",
    "line",
    "go",
    "downloads",
    "already",
    "downloaded",
    "netcat",
    "utility",
    "windows",
    "basically",
    "search",
    "something",
    "machine",
    "within",
    "downloads",
    "netcat",
    "utility",
    "would",
    "come",
    "back",
    "would",
    "say",
    "lvp",
    "could",
    "say",
    "port",
    "specified",
    "streaming",
    "application",
    "start",
    "netcat",
    "utility",
    "says",
    "listening",
    "222",
    "says",
    "connection",
    "look",
    "background",
    "receiver",
    "able",
    "establish",
    "connection",
    "establish",
    "connection",
    "netcat",
    "utility",
    "whatever",
    "type",
    "taken",
    "processing",
    "every",
    "10",
    "seconds",
    "would",
    "see",
    "word",
    "count",
    "application",
    "running",
    "let",
    "test",
    "say",
    "test",
    "test",
    "done",
    "soon",
    "pass",
    "message",
    "see",
    "word",
    "count",
    "happening",
    "stream",
    "data",
    "coming",
    "say",
    "winters",
    "coming",
    "winters",
    "cold",
    "given",
    "messages",
    "see",
    "streaming",
    "application",
    "try",
    "work",
    "data",
    "coming",
    "process",
    "show",
    "us",
    "result",
    "say",
    "test",
    "winters",
    "let",
    "see",
    "continues",
    "processing",
    "shows",
    "us",
    "right",
    "streaming",
    "application",
    "running",
    "fine",
    "looking",
    "words",
    "passing",
    "say",
    "able",
    "process",
    "data",
    "every",
    "10",
    "seconds",
    "tech",
    "stock",
    "socket",
    "stream",
    "looking",
    "machine",
    "particular",
    "port",
    "series",
    "transformations",
    "series",
    "transformations",
    "seen",
    "within",
    "console",
    "application",
    "used",
    "word",
    "counts",
    "dot",
    "save",
    "text",
    "file",
    "could",
    "also",
    "output",
    "getting",
    "generated",
    "every",
    "10",
    "second",
    "interval",
    "would",
    "get",
    "saved",
    "could",
    "also",
    "decide",
    "output",
    "saved",
    "stored",
    "sdfs",
    "storage",
    "simple",
    "streaming",
    "application",
    "saw",
    "using",
    "socket",
    "tech",
    "stream",
    "looking",
    "machine",
    "particular",
    "port",
    "running",
    "netcat",
    "utility",
    "series",
    "rdd",
    "transformations",
    "flat",
    "map",
    "map",
    "reduce",
    "key",
    "finally",
    "invoking",
    "action",
    "print",
    "basically",
    "triggers",
    "rdds",
    "work",
    "streaming",
    "data",
    "one",
    "example",
    "could",
    "also",
    "particularly",
    "application",
    "already",
    "sbt",
    "windows",
    "installed",
    "going",
    "project",
    "space",
    "could",
    "say",
    "workspace",
    "could",
    "go",
    "apps",
    "project",
    "folder",
    "could",
    "say",
    "sbt",
    "package",
    "basically",
    "based",
    "build",
    "file",
    "already",
    "within",
    "project",
    "folder",
    "see",
    "build",
    "file",
    "says",
    "name",
    "version",
    "scala",
    "version",
    "spark",
    "version",
    "repository",
    "spark",
    "manage",
    "dependencies",
    "components",
    "like",
    "spark",
    "core",
    "sql",
    "ml",
    "streaming",
    "hive",
    "need",
    "build",
    "dot",
    "sbt",
    "file",
    "within",
    "project",
    "folder",
    "used",
    "package",
    "application",
    "jar",
    "done",
    "packaging",
    "shows",
    "created",
    "jar",
    "applications",
    "within",
    "particular",
    "folder",
    "basically",
    "import",
    "package",
    "cluster",
    "run",
    "cluster",
    "using",
    "spark",
    "submit",
    "run",
    "spark",
    "streaming",
    "application",
    "cluster",
    "two",
    "node",
    "cluster",
    "spark",
    "stand",
    "alone",
    "cluster",
    "could",
    "look",
    "previous",
    "videos",
    "wherein",
    "explain",
    "set",
    "spark",
    "standalone",
    "cluster",
    "go",
    "spark",
    "directory",
    "could",
    "say",
    "bin",
    "start",
    "dot",
    "sh",
    "start",
    "spark",
    "master",
    "worker",
    "note",
    "hadoop",
    "cluster",
    "also",
    "set",
    "right",
    "running",
    "spark",
    "standalone",
    "cluster",
    "shows",
    "master",
    "worker",
    "process",
    "machine",
    "worker",
    "process",
    "machine",
    "spark",
    "standalone",
    "cluster",
    "running",
    "always",
    "bring",
    "ui",
    "see",
    "looks",
    "like",
    "giving",
    "http",
    "slash",
    "um",
    "one",
    "eight",
    "zero",
    "eight",
    "zero",
    "spark",
    "standalone",
    "cluster",
    "two",
    "worker",
    "nodes",
    "right",
    "application",
    "running",
    "come",
    "back",
    "check",
    "jar",
    "file",
    "already",
    "placed",
    "jar",
    "point",
    "time",
    "jar",
    "packaged",
    "always",
    "go",
    "jar",
    "minus",
    "xvf",
    "choose",
    "jar",
    "see",
    "within",
    "jar",
    "already",
    "done",
    "machine",
    "look",
    "main",
    "folder",
    "scala",
    "within",
    "scala",
    "see",
    "different",
    "classes",
    "code",
    "packaged",
    "jar",
    "fifth",
    "app",
    "class",
    "would",
    "want",
    "run",
    "saying",
    "spark",
    "submit",
    "would",
    "want",
    "application",
    "run",
    "cluster",
    "say",
    "class",
    "main",
    "dot",
    "scala",
    "object",
    "name",
    "say",
    "fifth",
    "app",
    "class",
    "jar",
    "one",
    "also",
    "say",
    "master",
    "going",
    "spark",
    "running",
    "um",
    "one",
    "listens",
    "7077",
    "port",
    "would",
    "basically",
    "start",
    "streaming",
    "application",
    "would",
    "also",
    "need",
    "like",
    "windows",
    "would",
    "also",
    "need",
    "utility",
    "like",
    "netcat",
    "basically",
    "use",
    "netcat",
    "utility",
    "basically",
    "connect",
    "send",
    "messages",
    "streaming",
    "application",
    "let",
    "say",
    "let",
    "search",
    "already",
    "netcat",
    "might",
    "used",
    "let",
    "see",
    "window",
    "machine",
    "install",
    "netcat",
    "utility",
    "saying",
    "netcat",
    "say",
    "nc",
    "minus",
    "l",
    "say",
    "port",
    "two",
    "two",
    "two",
    "two",
    "see",
    "working",
    "test",
    "let",
    "see",
    "works",
    "let",
    "start",
    "application",
    "streaming",
    "application",
    "needs",
    "receiver",
    "establish",
    "connection",
    "particular",
    "machine",
    "let",
    "see",
    "done",
    "could",
    "also",
    "canceling",
    "give",
    "particular",
    "port",
    "say",
    "192",
    "168",
    "problem",
    "packaged",
    "application",
    "packaged",
    "application",
    "specifically",
    "giving",
    "ip",
    "also",
    "comment",
    "local",
    "package",
    "run",
    "cluster",
    "commented",
    "part",
    "obviously",
    "would",
    "giving",
    "127",
    "ip",
    "net",
    "cat",
    "running",
    "right",
    "let",
    "test",
    "say",
    "test",
    "new",
    "test",
    "application",
    "sending",
    "messages",
    "already",
    "word",
    "count",
    "expected",
    "come",
    "back",
    "would",
    "see",
    "application",
    "running",
    "cluster",
    "application",
    "using",
    "four",
    "cores",
    "memory",
    "per",
    "executor",
    "basically",
    "click",
    "application",
    "look",
    "application",
    "detail",
    "ui",
    "see",
    "done",
    "application",
    "streaming",
    "job",
    "running",
    "receiver",
    "says",
    "application",
    "says",
    "running",
    "basically",
    "click",
    "click",
    "says",
    "dag",
    "visualization",
    "say",
    "streaming",
    "job",
    "running",
    "look",
    "stages",
    "would",
    "want",
    "see",
    "multiple",
    "stages",
    "wider",
    "transformation",
    "look",
    "storage",
    "used",
    "persisting",
    "caching",
    "look",
    "executors",
    "used",
    "one",
    "important",
    "things",
    "talk",
    "streaming",
    "streaming",
    "tab",
    "gets",
    "activated",
    "show",
    "run",
    "batch",
    "applications",
    "streaming",
    "applications",
    "ui",
    "shows",
    "streaming",
    "tab",
    "basically",
    "says",
    "running",
    "batches",
    "10",
    "seconds",
    "2",
    "minutes",
    "9",
    "seconds",
    "already",
    "see",
    "input",
    "rate",
    "see",
    "receivers",
    "see",
    "delay",
    "scheduling",
    "processing",
    "time",
    "many",
    "batches",
    "completed",
    "basically",
    "shows",
    "processing",
    "time",
    "many",
    "tasks",
    "run",
    "application",
    "running",
    "fine",
    "unless",
    "go",
    "ahead",
    "cancel",
    "application",
    "continue",
    "run",
    "keep",
    "looking",
    "messages",
    "keep",
    "typing",
    "example",
    "would",
    "want",
    "copy",
    "start",
    "would",
    "keep",
    "passing",
    "say",
    "test",
    "one",
    "test",
    "one",
    "test",
    "streaming",
    "application",
    "right",
    "seeing",
    "messages",
    "processed",
    "getting",
    "word",
    "count",
    "say",
    "winter",
    "winter",
    "summer",
    "old",
    "winter",
    "always",
    "look",
    "streaming",
    "application",
    "word",
    "count",
    "simple",
    "way",
    "run",
    "streaming",
    "application",
    "either",
    "windows",
    "machine",
    "local",
    "mode",
    "could",
    "running",
    "spark",
    "standalone",
    "cluster",
    "could",
    "also",
    "deploy",
    "application",
    "run",
    "yarn",
    "based",
    "cluster",
    "seen",
    "streaming",
    "application",
    "word",
    "count",
    "interesting",
    "also",
    "see",
    "window",
    "based",
    "computation",
    "windowing",
    "operation",
    "supported",
    "spark",
    "streaming",
    "explained",
    "look",
    "different",
    "application",
    "streaming",
    "window",
    "option",
    "let",
    "look",
    "application",
    "importing",
    "packages",
    "streaming",
    "streaming",
    "context",
    "spark",
    "spark",
    "context",
    "spark",
    "configuration",
    "also",
    "using",
    "spark",
    "api",
    "java",
    "function",
    "using",
    "spark",
    "streaming",
    "api",
    "also",
    "using",
    "storage",
    "level",
    "also",
    "intend",
    "persistence",
    "caching",
    "done",
    "create",
    "application",
    "say",
    "streaming",
    "app",
    "extends",
    "app",
    "create",
    "configuration",
    "object",
    "say",
    "app",
    "name",
    "set",
    "master",
    "appropriate",
    "number",
    "threads",
    "create",
    "spark",
    "streaming",
    "context",
    "initialize",
    "spark",
    "streaming",
    "context",
    "depends",
    "sc",
    "given",
    "time",
    "interval",
    "10",
    "seconds",
    "set",
    "receiver",
    "say",
    "stream",
    "rdd",
    "use",
    "socket",
    "extreme",
    "like",
    "earlier",
    "look",
    "particular",
    "machine",
    "port",
    "wherein",
    "also",
    "using",
    "storage",
    "level",
    "would",
    "want",
    "data",
    "stream",
    "would",
    "get",
    "generated",
    "every",
    "10",
    "seconds",
    "cached",
    "memory",
    "could",
    "use",
    "different",
    "storage",
    "levels",
    "could",
    "use",
    "memory",
    "disk",
    "memory",
    "disk",
    "could",
    "memory",
    "disk",
    "replication",
    "factor",
    "different",
    "storage",
    "levels",
    "series",
    "transformations",
    "like",
    "earlier",
    "see",
    "previous",
    "example",
    "reduce",
    "key",
    "using",
    "transformation",
    "reduced",
    "key",
    "window",
    "one",
    "basically",
    "takes",
    "function",
    "said",
    "windowing",
    "takes",
    "associative",
    "commutative",
    "functions",
    "instead",
    "using",
    "shortcut",
    "wave",
    "specifying",
    "function",
    "say",
    "takes",
    "b",
    "basically",
    "uses",
    "function",
    "give",
    "interval",
    "30",
    "seconds",
    "window",
    "time",
    "frame",
    "within",
    "interval",
    "would",
    "want",
    "look",
    "two",
    "main",
    "aspects",
    "one",
    "using",
    "window",
    "based",
    "transformation",
    "also",
    "specifying",
    "time",
    "interval",
    "windowing",
    "basically",
    "means",
    "would",
    "still",
    "word",
    "count",
    "would",
    "basically",
    "word",
    "count",
    "every",
    "10",
    "seconds",
    "data",
    "coming",
    "would",
    "also",
    "want",
    "consolidated",
    "set",
    "computation",
    "done",
    "every",
    "30",
    "seconds",
    "looking",
    "last",
    "three",
    "time",
    "intervals",
    "also",
    "notice",
    "also",
    "check",
    "pointing",
    "wherein",
    "specified",
    "specific",
    "directory",
    "saying",
    "would",
    "want",
    "checkpoint",
    "would",
    "want",
    "basically",
    "save",
    "computations",
    "kind",
    "fault",
    "recovery",
    "streaming",
    "application",
    "window",
    "based",
    "computation",
    "using",
    "storage",
    "level",
    "persistence",
    "also",
    "check",
    "pointing",
    "takes",
    "care",
    "fault",
    "recovery",
    "run",
    "application",
    "already",
    "set",
    "environment",
    "variables",
    "basically",
    "start",
    "application",
    "run",
    "scala",
    "application",
    "would",
    "start",
    "streaming",
    "application",
    "window",
    "based",
    "computations",
    "open",
    "command",
    "line",
    "would",
    "want",
    "start",
    "netcat",
    "utility",
    "could",
    "go",
    "downloads",
    "let",
    "minimize",
    "window",
    "starting",
    "netcat",
    "utility",
    "could",
    "say",
    "minus",
    "lvp",
    "port",
    "soon",
    "background",
    "application",
    "receiver",
    "able",
    "establish",
    "connection",
    "netcat",
    "utility",
    "running",
    "particular",
    "port",
    "done",
    "see",
    "word",
    "count",
    "happens",
    "10",
    "second",
    "interval",
    "let",
    "say",
    "first",
    "test",
    "give",
    "word",
    "count",
    "data",
    "passed",
    "say",
    "second",
    "test",
    "third",
    "test",
    "see",
    "basically",
    "say",
    "first",
    "test",
    "since",
    "window",
    "based",
    "operation",
    "also",
    "see",
    "consolidated",
    "set",
    "transformations",
    "happen",
    "window",
    "three",
    "intervals",
    "see",
    "giving",
    "count",
    "showing",
    "totally",
    "4",
    "say",
    "looking",
    "window",
    "last",
    "2",
    "3",
    "time",
    "intervals",
    "back",
    "looking",
    "last",
    "one",
    "say",
    "test",
    "test",
    "done",
    "test",
    "done",
    "show",
    "word",
    "count",
    "whatever",
    "passed",
    "like",
    "test",
    "done",
    "twice",
    "test",
    "thrice",
    "since",
    "windowing",
    "based",
    "operation",
    "done",
    "look",
    "time",
    "interval",
    "messages",
    "passed",
    "within",
    "particular",
    "window",
    "also",
    "since",
    "application",
    "running",
    "windows",
    "bring",
    "spark",
    "ui",
    "go",
    "http",
    "slash",
    "slash",
    "looking",
    "machine",
    "port",
    "example",
    "could",
    "shows",
    "streaming",
    "application",
    "running",
    "wherein",
    "see",
    "window",
    "based",
    "application",
    "running",
    "would",
    "interesting",
    "look",
    "stages",
    "kind",
    "shuffling",
    "happening",
    "looking",
    "storage",
    "show",
    "persistence",
    "see",
    "shows",
    "persisting",
    "memory",
    "size",
    "occupies",
    "many",
    "partitions",
    "cached",
    "persistence",
    "storage",
    "level",
    "used",
    "memory",
    "change",
    "disk",
    "see",
    "rdds",
    "cached",
    "disk",
    "look",
    "streaming",
    "tab",
    "gives",
    "information",
    "patches",
    "simple",
    "example",
    "obviously",
    "change",
    "kind",
    "rtd",
    "computations",
    "would",
    "want",
    "whether",
    "want",
    "word",
    "count",
    "something",
    "else",
    "whether",
    "want",
    "save",
    "processed",
    "output",
    "able",
    "always",
    "refresh",
    "project",
    "see",
    "see",
    "entries",
    "folder",
    "getting",
    "created",
    "right",
    "decide",
    "whether",
    "would",
    "want",
    "output",
    "created",
    "also",
    "see",
    "within",
    "would",
    "containing",
    "since",
    "set",
    "check",
    "pointing",
    "project",
    "folder",
    "would",
    "also",
    "check",
    "pointing",
    "temporarily",
    "information",
    "kind",
    "failure",
    "classic",
    "example",
    "streaming",
    "application",
    "using",
    "window",
    "based",
    "computations",
    "persisting",
    "computations",
    "also",
    "check",
    "pointing",
    "simultaneously",
    "tutorial",
    "list",
    "questions",
    "explanation",
    "well",
    "prepared",
    "hadoop",
    "interviews",
    "let",
    "look",
    "general",
    "hadoop",
    "questions",
    "different",
    "vendor",
    "specific",
    "distributions",
    "hadoop",
    "might",
    "aware",
    "hadoop",
    "apache",
    "hadoop",
    "core",
    "distribution",
    "hadoop",
    "different",
    "vendors",
    "market",
    "packaged",
    "apache",
    "hadoop",
    "cluster",
    "management",
    "solution",
    "allows",
    "everyone",
    "easily",
    "deploy",
    "manage",
    "monitor",
    "upgrade",
    "clusters",
    "distributions",
    "cloudera",
    "dominant",
    "one",
    "market",
    "hortonworks",
    "might",
    "aware",
    "cloudera",
    "hortonworks",
    "merged",
    "become",
    "bigger",
    "entity",
    "map",
    "r",
    "microsoft",
    "azure",
    "ibm",
    "infosphere",
    "amazon",
    "web",
    "services",
    "popularly",
    "known",
    "distributions",
    "would",
    "want",
    "know",
    "hadoop",
    "distributions",
    "basically",
    "look",
    "google",
    "check",
    "hadoop",
    "different",
    "distributions",
    "wiki",
    "page",
    "type",
    "hadoop",
    "different",
    "distributions",
    "check",
    "wiki",
    "page",
    "take",
    "distributions",
    "commercial",
    "support",
    "page",
    "basically",
    "says",
    "sold",
    "products",
    "called",
    "release",
    "apache",
    "hadoop",
    "come",
    "open",
    "source",
    "community",
    "various",
    "distributions",
    "basically",
    "running",
    "one",
    "way",
    "apache",
    "hadoop",
    "packaged",
    "solution",
    "like",
    "installer",
    "easily",
    "set",
    "clusters",
    "set",
    "machines",
    "look",
    "page",
    "read",
    "different",
    "distributions",
    "hadoop",
    "coming",
    "back",
    "let",
    "look",
    "next",
    "question",
    "different",
    "hadoop",
    "configuration",
    "files",
    "whether",
    "talking",
    "apache",
    "hadoop",
    "cloudera",
    "hortonworks",
    "map",
    "r",
    "matter",
    "distribution",
    "config",
    "files",
    "important",
    "existing",
    "every",
    "distribution",
    "hadoop",
    "hadoop",
    "environment",
    "dot",
    "sh",
    "wherein",
    "environment",
    "variables",
    "java",
    "path",
    "would",
    "process",
    "id",
    "path",
    "logs",
    "get",
    "stored",
    "kind",
    "metrics",
    "collected",
    "core",
    "hyphen",
    "site",
    "file",
    "hdfs",
    "path",
    "many",
    "properties",
    "like",
    "enabling",
    "trash",
    "enabling",
    "high",
    "availability",
    "discussing",
    "mentioning",
    "zookeeper",
    "one",
    "important",
    "file",
    "hdfs",
    "hyphen",
    "site",
    "file",
    "file",
    "information",
    "related",
    "hadoop",
    "cluster",
    "replication",
    "factor",
    "name",
    "node",
    "store",
    "metadata",
    "disk",
    "data",
    "node",
    "running",
    "would",
    "data",
    "node",
    "store",
    "data",
    "secondary",
    "name",
    "node",
    "running",
    "would",
    "store",
    "copy",
    "name",
    "node",
    "metadata",
    "mapred",
    "hyphen",
    "site",
    "file",
    "file",
    "properties",
    "related",
    "mapreduce",
    "processing",
    "also",
    "masters",
    "slaves",
    "might",
    "deprecated",
    "distribution",
    "fact",
    "would",
    "yarn",
    "hyphen",
    "site",
    "file",
    "based",
    "yarn",
    "processing",
    "framework",
    "introduced",
    "hadoop",
    "version",
    "2",
    "would",
    "resource",
    "allocation",
    "resource",
    "manager",
    "node",
    "manager",
    "related",
    "properties",
    "would",
    "want",
    "look",
    "default",
    "properties",
    "one",
    "example",
    "let",
    "say",
    "hdfs",
    "hyphen",
    "site",
    "file",
    "could",
    "go",
    "google",
    "type",
    "one",
    "properties",
    "example",
    "would",
    "say",
    "know",
    "property",
    "belongs",
    "hdfs",
    "hyphen",
    "site",
    "file",
    "search",
    "take",
    "first",
    "link",
    "says",
    "sdfs",
    "default",
    "xml",
    "click",
    "show",
    "properties",
    "given",
    "sdfs",
    "hyphen",
    "site",
    "file",
    "also",
    "shows",
    "version",
    "looking",
    "always",
    "change",
    "version",
    "example",
    "would",
    "want",
    "look",
    "need",
    "change",
    "version",
    "show",
    "properties",
    "similarly",
    "give",
    "property",
    "belongs",
    "say",
    "core",
    "hyphen",
    "site",
    "file",
    "example",
    "would",
    "say",
    "fs",
    "dot",
    "default",
    "fs",
    "property",
    "core",
    "hyphen",
    "site",
    "file",
    "somewhere",
    "would",
    "see",
    "core",
    "minus",
    "show",
    "properties",
    "similarly",
    "could",
    "search",
    "properties",
    "related",
    "yarn",
    "hyphen",
    "site",
    "file",
    "map",
    "red",
    "hyphen",
    "site",
    "file",
    "could",
    "say",
    "yarn",
    "dot",
    "resource",
    "manager",
    "could",
    "look",
    "one",
    "properties",
    "directly",
    "take",
    "yarn",
    "default",
    "xml",
    "see",
    "properties",
    "given",
    "yarn",
    "similarly",
    "could",
    "say",
    "map",
    "reduce",
    "dot",
    "job",
    "dot",
    "reduces",
    "know",
    "property",
    "belongs",
    "mapreduce",
    "hyphen",
    "site",
    "file",
    "takes",
    "default",
    "xml",
    "important",
    "config",
    "files",
    "matter",
    "distribution",
    "hadoop",
    "working",
    "knowing",
    "config",
    "files",
    "whether",
    "work",
    "hadoop",
    "admin",
    "work",
    "hadoop",
    "developer",
    "knowing",
    "config",
    "properties",
    "would",
    "important",
    "would",
    "also",
    "showcase",
    "internal",
    "knowledge",
    "configs",
    "drive",
    "hadoop",
    "cluster",
    "let",
    "look",
    "next",
    "question",
    "three",
    "modes",
    "hadoop",
    "run",
    "hadoop",
    "running",
    "standalone",
    "mode",
    "default",
    "mode",
    "would",
    "basically",
    "use",
    "local",
    "file",
    "system",
    "single",
    "java",
    "process",
    "say",
    "standalone",
    "mode",
    "downloading",
    "hadoop",
    "related",
    "package",
    "one",
    "single",
    "machine",
    "would",
    "process",
    "running",
    "would",
    "test",
    "hadoop",
    "functionalities",
    "could",
    "pseudo",
    "distributed",
    "mode",
    "basically",
    "means",
    "single",
    "node",
    "hadoop",
    "deployment",
    "hadoop",
    "framework",
    "many",
    "many",
    "services",
    "lot",
    "services",
    "services",
    "would",
    "running",
    "irrespective",
    "distribution",
    "service",
    "would",
    "multiple",
    "processes",
    "mode",
    "mode",
    "cluster",
    "would",
    "important",
    "processes",
    "belonging",
    "one",
    "multiple",
    "services",
    "running",
    "single",
    "node",
    "would",
    "want",
    "work",
    "sudo",
    "distributed",
    "mode",
    "using",
    "cloudera",
    "always",
    "go",
    "google",
    "search",
    "cloudera",
    "quick",
    "start",
    "vm",
    "download",
    "saying",
    "cloudera",
    "quick",
    "start",
    "vm",
    "search",
    "allow",
    "download",
    "quick",
    "start",
    "vm",
    "follow",
    "instructions",
    "single",
    "node",
    "cloudera",
    "cluster",
    "running",
    "virtual",
    "machines",
    "information",
    "refer",
    "youtube",
    "tutorial",
    "explained",
    "set",
    "quick",
    "start",
    "vm",
    "coming",
    "back",
    "could",
    "finally",
    "production",
    "setup",
    "fully",
    "distributed",
    "mode",
    "basically",
    "means",
    "hadoop",
    "framework",
    "components",
    "would",
    "spread",
    "across",
    "multiple",
    "machines",
    "would",
    "multiple",
    "services",
    "sdfs",
    "yarn",
    "flume",
    "scope",
    "kafka",
    "hbase",
    "hive",
    "impala",
    "services",
    "would",
    "one",
    "multiple",
    "processes",
    "distributed",
    "across",
    "multiple",
    "nodes",
    "normally",
    "used",
    "production",
    "environment",
    "could",
    "say",
    "standalone",
    "would",
    "good",
    "testing",
    "pseudo",
    "distributed",
    "could",
    "good",
    "testing",
    "development",
    "fully",
    "distributed",
    "would",
    "mainly",
    "production",
    "setup",
    "differences",
    "regular",
    "file",
    "system",
    "hdfs",
    "say",
    "regular",
    "file",
    "system",
    "could",
    "talking",
    "linux",
    "file",
    "system",
    "could",
    "talking",
    "windows",
    "based",
    "operating",
    "system",
    "regular",
    "file",
    "system",
    "would",
    "data",
    "maintained",
    "single",
    "system",
    "single",
    "system",
    "files",
    "directories",
    "low",
    "fault",
    "tolerance",
    "right",
    "machine",
    "crashes",
    "data",
    "recovery",
    "would",
    "difficult",
    "unless",
    "backup",
    "data",
    "also",
    "affects",
    "processing",
    "machine",
    "crashes",
    "machine",
    "fails",
    "processing",
    "would",
    "blocked",
    "biggest",
    "challenge",
    "regular",
    "file",
    "system",
    "seek",
    "time",
    "time",
    "taken",
    "read",
    "data",
    "might",
    "one",
    "single",
    "machine",
    "huge",
    "amount",
    "disks",
    "huge",
    "amount",
    "ram",
    "time",
    "taken",
    "read",
    "data",
    "data",
    "stored",
    "one",
    "machine",
    "would",
    "high",
    "would",
    "least",
    "fault",
    "tolerance",
    "talk",
    "sdfs",
    "data",
    "distributed",
    "sdfs",
    "stands",
    "hadoop",
    "distributed",
    "file",
    "system",
    "data",
    "distributed",
    "maintained",
    "multiple",
    "systems",
    "never",
    "one",
    "single",
    "machine",
    "also",
    "supporting",
    "reliability",
    "whatever",
    "stored",
    "hdfs",
    "say",
    "file",
    "stored",
    "depending",
    "size",
    "split",
    "blocks",
    "blocks",
    "spread",
    "across",
    "multiple",
    "nodes",
    "every",
    "block",
    "stored",
    "node",
    "replicas",
    "stored",
    "nodes",
    "replication",
    "factor",
    "depends",
    "makes",
    "sdfs",
    "reliable",
    "cases",
    "slave",
    "nodes",
    "data",
    "nodes",
    "crashing",
    "rarely",
    "data",
    "loss",
    "auto",
    "replication",
    "feature",
    "time",
    "taken",
    "read",
    "data",
    "comparatively",
    "might",
    "situations",
    "data",
    "distributed",
    "across",
    "nodes",
    "even",
    "parallel",
    "read",
    "data",
    "read",
    "might",
    "take",
    "time",
    "needs",
    "coordination",
    "multiple",
    "machines",
    "however",
    "working",
    "huge",
    "data",
    "getting",
    "stored",
    "still",
    "beneficial",
    "comparison",
    "reading",
    "single",
    "machine",
    "always",
    "think",
    "reliability",
    "auto",
    "replication",
    "feature",
    "fault",
    "tolerance",
    "data",
    "getting",
    "stored",
    "across",
    "multiple",
    "machines",
    "capability",
    "scale",
    "talk",
    "sdfs",
    "talking",
    "horizontal",
    "scalability",
    "scaling",
    "talk",
    "regular",
    "file",
    "system",
    "talking",
    "vertical",
    "scalability",
    "scaling",
    "let",
    "look",
    "specific",
    "sdfs",
    "questions",
    "sdfs",
    "fault",
    "tolerant",
    "explained",
    "previous",
    "slides",
    "sdfs",
    "fault",
    "tolerant",
    "replicates",
    "data",
    "different",
    "data",
    "nodes",
    "master",
    "node",
    "multiple",
    "slave",
    "nodes",
    "data",
    "nodes",
    "actually",
    "data",
    "getting",
    "stored",
    "also",
    "default",
    "block",
    "size",
    "128",
    "mb",
    "minimum",
    "since",
    "hadoop",
    "version",
    "file",
    "128",
    "mb",
    "would",
    "using",
    "one",
    "logical",
    "block",
    "file",
    "size",
    "bigger",
    "128",
    "mb",
    "split",
    "blocks",
    "blocks",
    "stored",
    "across",
    "multiple",
    "machines",
    "since",
    "blocks",
    "stored",
    "across",
    "multiple",
    "machines",
    "makes",
    "fault",
    "tolerant",
    "even",
    "machines",
    "fail",
    "would",
    "still",
    "copy",
    "block",
    "existing",
    "machine",
    "two",
    "aspects",
    "one",
    "talked",
    "first",
    "rule",
    "replication",
    "basically",
    "means",
    "never",
    "two",
    "identical",
    "blocks",
    "sitting",
    "machine",
    "second",
    "rule",
    "replication",
    "terms",
    "rack",
    "awareness",
    "machines",
    "placed",
    "racks",
    "see",
    "right",
    "image",
    "never",
    "replicas",
    "placed",
    "rack",
    "even",
    "different",
    "machines",
    "fault",
    "tolerant",
    "maintain",
    "redundancy",
    "least",
    "one",
    "replica",
    "placed",
    "node",
    "rack",
    "sdfs",
    "fault",
    "tolerant",
    "let",
    "understand",
    "architecture",
    "sdfs",
    "mentioned",
    "earlier",
    "would",
    "hadoop",
    "cluster",
    "main",
    "service",
    "hdfs",
    "sdfs",
    "service",
    "would",
    "name",
    "node",
    "master",
    "process",
    "running",
    "one",
    "machines",
    "would",
    "data",
    "nodes",
    "slave",
    "machines",
    "getting",
    "stored",
    "across",
    "getting",
    "processes",
    "running",
    "across",
    "multiple",
    "machines",
    "one",
    "processes",
    "important",
    "role",
    "play",
    "talk",
    "sdfs",
    "whatever",
    "data",
    "written",
    "hdfs",
    "data",
    "split",
    "blocks",
    "depending",
    "size",
    "blocks",
    "randomly",
    "distributed",
    "across",
    "nodes",
    "auto",
    "replication",
    "feature",
    "blocks",
    "also",
    "auto",
    "replicated",
    "across",
    "multiple",
    "machines",
    "first",
    "condition",
    "two",
    "identical",
    "blocks",
    "sit",
    "machine",
    "soon",
    "cluster",
    "comes",
    "data",
    "nodes",
    "part",
    "cluster",
    "based",
    "config",
    "files",
    "would",
    "start",
    "sending",
    "heartbeat",
    "name",
    "node",
    "would",
    "every",
    "three",
    "seconds",
    "name",
    "node",
    "name",
    "node",
    "store",
    "information",
    "ram",
    "name",
    "node",
    "starts",
    "building",
    "metadata",
    "ram",
    "metadata",
    "information",
    "data",
    "nodes",
    "available",
    "beginning",
    "data",
    "writing",
    "activity",
    "starts",
    "blocks",
    "distributed",
    "across",
    "data",
    "nodes",
    "data",
    "nodes",
    "every",
    "10",
    "seconds",
    "also",
    "send",
    "block",
    "report",
    "name",
    "node",
    "name",
    "node",
    "adding",
    "information",
    "ram",
    "metadata",
    "ram",
    "earlier",
    "data",
    "node",
    "information",
    "name",
    "node",
    "also",
    "information",
    "files",
    "files",
    "split",
    "blocks",
    "blocks",
    "stored",
    "machines",
    "file",
    "permissions",
    "name",
    "node",
    "maintaining",
    "metadata",
    "ram",
    "name",
    "node",
    "also",
    "maintaining",
    "metadata",
    "disk",
    "see",
    "red",
    "box",
    "basically",
    "information",
    "whatever",
    "information",
    "written",
    "hdfs",
    "summarize",
    "name",
    "node",
    "metadata",
    "ram",
    "metadata",
    "disk",
    "data",
    "nodes",
    "machines",
    "blocks",
    "data",
    "actually",
    "getting",
    "stored",
    "auto",
    "replication",
    "feature",
    "always",
    "existing",
    "unless",
    "disabled",
    "read",
    "write",
    "activity",
    "parallel",
    "activity",
    "however",
    "replication",
    "sequential",
    "activity",
    "mentioned",
    "talk",
    "name",
    "node",
    "master",
    "process",
    "hosting",
    "metadata",
    "disk",
    "ram",
    "talk",
    "disk",
    "basically",
    "edit",
    "log",
    "transaction",
    "log",
    "fs",
    "image",
    "file",
    "system",
    "image",
    "right",
    "time",
    "cluster",
    "started",
    "metadata",
    "disk",
    "existing",
    "gets",
    "appended",
    "every",
    "time",
    "read",
    "write",
    "operations",
    "happen",
    "sdfs",
    "metadata",
    "ram",
    "dynamically",
    "built",
    "every",
    "time",
    "cluster",
    "comes",
    "basically",
    "means",
    "cluster",
    "coming",
    "name",
    "node",
    "initial",
    "seconds",
    "minutes",
    "would",
    "safe",
    "mode",
    "basically",
    "means",
    "busy",
    "registering",
    "information",
    "data",
    "nodes",
    "name",
    "node",
    "one",
    "critical",
    "processes",
    "name",
    "node",
    "processes",
    "running",
    "able",
    "access",
    "cluster",
    "name",
    "node",
    "metadata",
    "disk",
    "important",
    "name",
    "node",
    "come",
    "maintain",
    "cluster",
    "name",
    "node",
    "metadata",
    "ram",
    "basically",
    "satisfying",
    "client",
    "requests",
    "look",
    "data",
    "nodes",
    "mentioned",
    "data",
    "nodes",
    "hold",
    "actual",
    "data",
    "blocks",
    "sending",
    "block",
    "reports",
    "every",
    "10",
    "seconds",
    "metadata",
    "name",
    "nodes",
    "ram",
    "constantly",
    "getting",
    "updated",
    "metadata",
    "disk",
    "also",
    "constantly",
    "getting",
    "updated",
    "based",
    "kind",
    "write",
    "activity",
    "happening",
    "cluster",
    "data",
    "node",
    "storing",
    "block",
    "also",
    "help",
    "kind",
    "read",
    "activity",
    "whenever",
    "client",
    "requests",
    "whenever",
    "client",
    "application",
    "api",
    "would",
    "want",
    "read",
    "data",
    "would",
    "first",
    "talk",
    "name",
    "node",
    "name",
    "node",
    "would",
    "look",
    "metadata",
    "ram",
    "confirm",
    "client",
    "machines",
    "could",
    "reached",
    "get",
    "data",
    "client",
    "would",
    "try",
    "read",
    "data",
    "sdfs",
    "actually",
    "getting",
    "data",
    "data",
    "nodes",
    "read",
    "write",
    "requests",
    "satisfied",
    "two",
    "types",
    "metadata",
    "name",
    "node",
    "server",
    "holds",
    "mentioned",
    "earlier",
    "metadata",
    "disk",
    "important",
    "remember",
    "edit",
    "log",
    "nfs",
    "image",
    "metadata",
    "ram",
    "information",
    "data",
    "nodes",
    "files",
    "files",
    "split",
    "blocks",
    "blocks",
    "residing",
    "data",
    "nodes",
    "file",
    "permissions",
    "share",
    "good",
    "link",
    "always",
    "look",
    "detailed",
    "information",
    "metadata",
    "search",
    "sdfs",
    "metadata",
    "directories",
    "explained",
    "hortonworks",
    "however",
    "talks",
    "metadata",
    "disk",
    "name",
    "node",
    "manages",
    "details",
    "look",
    "link",
    "interested",
    "learning",
    "metadata",
    "disk",
    "coming",
    "back",
    "let",
    "look",
    "next",
    "question",
    "difference",
    "federation",
    "high",
    "availability",
    "features",
    "introduced",
    "hadoop",
    "version",
    "features",
    "horizontal",
    "scalability",
    "name",
    "node",
    "prior",
    "version",
    "2",
    "possibility",
    "could",
    "one",
    "single",
    "master",
    "basically",
    "means",
    "cluster",
    "could",
    "become",
    "unavailable",
    "name",
    "node",
    "would",
    "crash",
    "hadoop",
    "version",
    "2",
    "introduced",
    "two",
    "new",
    "features",
    "federation",
    "high",
    "availability",
    "however",
    "high",
    "availability",
    "popular",
    "one",
    "talk",
    "federation",
    "basically",
    "means",
    "number",
    "name",
    "nodes",
    "limitation",
    "number",
    "name",
    "nodes",
    "name",
    "nodes",
    "federated",
    "cluster",
    "basically",
    "means",
    "name",
    "nodes",
    "still",
    "belong",
    "cluster",
    "coordinating",
    "whenever",
    "write",
    "request",
    "comes",
    "one",
    "name",
    "node",
    "picks",
    "request",
    "guides",
    "request",
    "blocks",
    "written",
    "data",
    "nodes",
    "name",
    "node",
    "coordinate",
    "name",
    "node",
    "find",
    "block",
    "id",
    "assigned",
    "one",
    "assigned",
    "name",
    "node",
    "belong",
    "federated",
    "cluster",
    "linked",
    "via",
    "cluster",
    "id",
    "whenever",
    "application",
    "api",
    "trying",
    "talk",
    "cluster",
    "always",
    "going",
    "via",
    "cluster",
    "id",
    "one",
    "name",
    "node",
    "would",
    "pick",
    "read",
    "activity",
    "write",
    "activity",
    "processing",
    "activity",
    "name",
    "nodes",
    "sharing",
    "pool",
    "metadata",
    "name",
    "node",
    "dedicated",
    "pool",
    "remember",
    "term",
    "called",
    "namespace",
    "name",
    "service",
    "also",
    "provides",
    "high",
    "fault",
    "tolerance",
    "supposed",
    "one",
    "name",
    "node",
    "goes",
    "affect",
    "make",
    "cluster",
    "unavailable",
    "still",
    "cluster",
    "reachable",
    "name",
    "nodes",
    "running",
    "available",
    "comes",
    "heartbeats",
    "data",
    "nodes",
    "sending",
    "heart",
    "beats",
    "name",
    "nodes",
    "name",
    "nodes",
    "aware",
    "data",
    "nodes",
    "talk",
    "high",
    "availability",
    "would",
    "two",
    "name",
    "nodes",
    "would",
    "active",
    "would",
    "standby",
    "normally",
    "environment",
    "would",
    "see",
    "high",
    "availability",
    "setup",
    "zookeeper",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "talk",
    "active",
    "standby",
    "name",
    "notes",
    "election",
    "name",
    "node",
    "made",
    "active",
    "taking",
    "care",
    "automatic",
    "failover",
    "done",
    "zookeeper",
    "high",
    "availability",
    "set",
    "without",
    "zookeeper",
    "would",
    "mean",
    "admins",
    "intervention",
    "would",
    "required",
    "make",
    "name",
    "node",
    "active",
    "standby",
    "also",
    "take",
    "care",
    "failover",
    "point",
    "time",
    "high",
    "availability",
    "active",
    "name",
    "node",
    "would",
    "taking",
    "care",
    "storing",
    "edits",
    "whatever",
    "updates",
    "happening",
    "sdfs",
    "also",
    "writing",
    "edits",
    "shared",
    "location",
    "standby",
    "name",
    "node",
    "one",
    "constantly",
    "looking",
    "latest",
    "updates",
    "applying",
    "metadata",
    "actually",
    "copy",
    "whatever",
    "active",
    "name",
    "node",
    "way",
    "standby",
    "name",
    "node",
    "always",
    "sync",
    "active",
    "name",
    "node",
    "reason",
    "active",
    "name",
    "node",
    "fails",
    "standby",
    "name",
    "node",
    "take",
    "become",
    "active",
    "remember",
    "zookeeper",
    "plays",
    "important",
    "role",
    "centralized",
    "coordination",
    "service",
    "one",
    "thing",
    "remember",
    "high",
    "availability",
    "secondary",
    "name",
    "node",
    "allowed",
    "would",
    "active",
    "name",
    "node",
    "standby",
    "name",
    "node",
    "configured",
    "separate",
    "machine",
    "access",
    "shared",
    "location",
    "shared",
    "location",
    "could",
    "nfs",
    "could",
    "quorum",
    "general",
    "nodes",
    "information",
    "refer",
    "tutorial",
    "explained",
    "sdfs",
    "high",
    "availability",
    "federation",
    "let",
    "look",
    "logical",
    "question",
    "input",
    "file",
    "350",
    "mb",
    "obviously",
    "bigger",
    "128",
    "mb",
    "many",
    "input",
    "splits",
    "would",
    "created",
    "sdfs",
    "would",
    "size",
    "input",
    "split",
    "need",
    "remember",
    "default",
    "minimum",
    "block",
    "size",
    "128",
    "mb",
    "customizable",
    "environment",
    "number",
    "larger",
    "files",
    "written",
    "average",
    "obviously",
    "go",
    "bigger",
    "block",
    "size",
    "environment",
    "lot",
    "files",
    "written",
    "files",
    "smaller",
    "size",
    "could",
    "okay",
    "128",
    "mb",
    "remember",
    "hadoop",
    "every",
    "entity",
    "directory",
    "sdfs",
    "file",
    "sdfs",
    "file",
    "multiple",
    "blocks",
    "considered",
    "objects",
    "object",
    "hadoop",
    "name",
    "nodes",
    "ram",
    "150",
    "bytes",
    "utilized",
    "block",
    "size",
    "small",
    "would",
    "number",
    "blocks",
    "would",
    "directly",
    "affect",
    "name",
    "node",
    "ram",
    "keep",
    "block",
    "size",
    "high",
    "reduce",
    "number",
    "blocks",
    "remember",
    "might",
    "affect",
    "processing",
    "processing",
    "also",
    "depends",
    "splits",
    "number",
    "splits",
    "parallel",
    "processing",
    "setting",
    "block",
    "size",
    "done",
    "consideration",
    "parallelism",
    "requirement",
    "name",
    "nodes",
    "ram",
    "available",
    "coming",
    "question",
    "file",
    "350",
    "mb",
    "would",
    "split",
    "three",
    "blocks",
    "two",
    "blocks",
    "would",
    "128",
    "mb",
    "data",
    "third",
    "block",
    "although",
    "block",
    "size",
    "would",
    "still",
    "128",
    "would",
    "94",
    "mb",
    "data",
    "would",
    "split",
    "particular",
    "file",
    "let",
    "understand",
    "rack",
    "awareness",
    "rack",
    "awareness",
    "work",
    "even",
    "racks",
    "organizations",
    "always",
    "would",
    "want",
    "place",
    "nodes",
    "machines",
    "systematic",
    "way",
    "different",
    "approaches",
    "could",
    "rack",
    "would",
    "machines",
    "running",
    "master",
    "processes",
    "intention",
    "would",
    "particular",
    "rack",
    "could",
    "higher",
    "bandwidth",
    "cooling",
    "dedicated",
    "power",
    "supply",
    "top",
    "rack",
    "switch",
    "second",
    "approach",
    "could",
    "could",
    "one",
    "master",
    "process",
    "running",
    "one",
    "machine",
    "every",
    "rack",
    "could",
    "slave",
    "processes",
    "running",
    "talk",
    "rack",
    "awareness",
    "one",
    "thing",
    "understand",
    "machines",
    "placed",
    "within",
    "racks",
    "aware",
    "hadoop",
    "follows",
    "auto",
    "replication",
    "rule",
    "replication",
    "rack",
    "air",
    "cluster",
    "would",
    "would",
    "never",
    "replicas",
    "placed",
    "rack",
    "look",
    "block",
    "blue",
    "color",
    "never",
    "three",
    "blue",
    "boxes",
    "rack",
    "even",
    "different",
    "nodes",
    "makes",
    "us",
    "makes",
    "less",
    "fault",
    "tolerant",
    "would",
    "least",
    "one",
    "copy",
    "block",
    "would",
    "stored",
    "different",
    "track",
    "different",
    "note",
    "let",
    "look",
    "basically",
    "talking",
    "replicas",
    "placed",
    "way",
    "somebody",
    "could",
    "ask",
    "question",
    "block",
    "replicas",
    "spread",
    "across",
    "three",
    "lakhs",
    "yes",
    "order",
    "make",
    "redundant",
    "increasing",
    "bandwidth",
    "requirement",
    "better",
    "approach",
    "would",
    "two",
    "blocks",
    "rack",
    "different",
    "machines",
    "one",
    "copy",
    "different",
    "track",
    "let",
    "proceed",
    "restart",
    "name",
    "node",
    "demons",
    "hadoop",
    "working",
    "apache",
    "hadoop",
    "cluster",
    "could",
    "start",
    "stop",
    "using",
    "hadoop",
    "demon",
    "scripts",
    "hadoop",
    "demon",
    "scripts",
    "would",
    "used",
    "start",
    "stop",
    "hadoop",
    "talk",
    "apache",
    "hadoop",
    "let",
    "look",
    "one",
    "particular",
    "file",
    "would",
    "like",
    "show",
    "information",
    "talks",
    "different",
    "clusters",
    "let",
    "look",
    "let",
    "look",
    "start",
    "stop",
    "file",
    "let",
    "look",
    "one",
    "gives",
    "highlight",
    "talk",
    "apache",
    "hadoop",
    "setup",
    "would",
    "done",
    "would",
    "download",
    "adobe",
    "tar",
    "file",
    "would",
    "untie",
    "edit",
    "config",
    "files",
    "would",
    "formatting",
    "start",
    "cluster",
    "said",
    "using",
    "scripts",
    "case",
    "apache",
    "hadoop",
    "could",
    "using",
    "start",
    "script",
    "internally",
    "triggers",
    "start",
    "dfs",
    "start",
    "yarn",
    "scripts",
    "start",
    "dfs",
    "internally",
    "would",
    "run",
    "hadoop",
    "demon",
    "multiple",
    "times",
    "based",
    "configs",
    "start",
    "different",
    "processes",
    "start",
    "yarn",
    "would",
    "run",
    "yarn",
    "demon",
    "script",
    "start",
    "processing",
    "related",
    "processes",
    "happens",
    "apache",
    "hadoop",
    "case",
    "cloud",
    "era",
    "hortonworks",
    "basically",
    "vendor",
    "specific",
    "distribution",
    "would",
    "say",
    "multiple",
    "services",
    "would",
    "one",
    "multiple",
    "demons",
    "running",
    "across",
    "machines",
    "let",
    "take",
    "example",
    "would",
    "machine",
    "1",
    "machine",
    "2",
    "machine",
    "3",
    "processor",
    "spread",
    "across",
    "however",
    "case",
    "cloud",
    "era",
    "hortonworks",
    "cluster",
    "management",
    "solutions",
    "would",
    "never",
    "involved",
    "running",
    "script",
    "individually",
    "start",
    "stop",
    "processes",
    "fact",
    "case",
    "cloud",
    "error",
    "would",
    "clouded",
    "scm",
    "server",
    "running",
    "one",
    "machines",
    "clouded",
    "scm",
    "agents",
    "running",
    "every",
    "machine",
    "talk",
    "hortonworks",
    "would",
    "ambari",
    "server",
    "ambari",
    "agent",
    "running",
    "agents",
    "running",
    "every",
    "machine",
    "responsible",
    "monitor",
    "processes",
    "send",
    "also",
    "heartbeat",
    "master",
    "server",
    "server",
    "one",
    "service",
    "basically",
    "give",
    "instructions",
    "agents",
    "case",
    "vendor",
    "specific",
    "distribution",
    "start",
    "stop",
    "processes",
    "automatically",
    "taken",
    "care",
    "underlying",
    "services",
    "services",
    "internally",
    "still",
    "running",
    "commands",
    "however",
    "apache",
    "hadoop",
    "manually",
    "follow",
    "start",
    "stop",
    "coming",
    "back",
    "look",
    "command",
    "related",
    "questions",
    "command",
    "help",
    "find",
    "status",
    "blocks",
    "file",
    "system",
    "health",
    "always",
    "go",
    "file",
    "system",
    "check",
    "command",
    "show",
    "files",
    "particular",
    "sdfs",
    "path",
    "show",
    "blocks",
    "also",
    "give",
    "information",
    "status",
    "replicated",
    "blocks",
    "replicated",
    "blocks",
    "misreplicated",
    "blocks",
    "default",
    "replication",
    "fsck",
    "file",
    "system",
    "check",
    "utility",
    "repair",
    "problem",
    "blocks",
    "give",
    "information",
    "blocks",
    "related",
    "files",
    "machines",
    "stored",
    "replicated",
    "per",
    "replication",
    "factor",
    "problem",
    "particular",
    "replica",
    "would",
    "happen",
    "store",
    "many",
    "small",
    "files",
    "cluster",
    "relates",
    "block",
    "information",
    "gave",
    "time",
    "back",
    "remember",
    "hadoop",
    "coded",
    "java",
    "every",
    "directory",
    "every",
    "file",
    "file",
    "related",
    "block",
    "considered",
    "object",
    "every",
    "object",
    "within",
    "hadoop",
    "cluster",
    "name",
    "nodes",
    "ram",
    "gets",
    "utilized",
    "number",
    "blocks",
    "would",
    "usage",
    "name",
    "nodes",
    "ram",
    "storing",
    "many",
    "small",
    "files",
    "would",
    "affect",
    "disk",
    "would",
    "directly",
    "affect",
    "name",
    "nodes",
    "ram",
    "production",
    "clusters",
    "admin",
    "guys",
    "infrastructure",
    "specialist",
    "take",
    "care",
    "everyone",
    "writing",
    "data",
    "hdfs",
    "follows",
    "quota",
    "system",
    "could",
    "controlled",
    "amount",
    "data",
    "write",
    "plus",
    "count",
    "data",
    "individual",
    "rights",
    "hdfs",
    "copy",
    "data",
    "local",
    "system",
    "onto",
    "sdfs",
    "use",
    "put",
    "command",
    "copy",
    "local",
    "given",
    "local",
    "path",
    "source",
    "destination",
    "sdfs",
    "path",
    "remember",
    "always",
    "copy",
    "local",
    "using",
    "minus",
    "f",
    "option",
    "flag",
    "option",
    "also",
    "helps",
    "writing",
    "file",
    "new",
    "file",
    "hdfs",
    "minus",
    "f",
    "chance",
    "overwriting",
    "rewriting",
    "data",
    "existing",
    "sdfs",
    "copy",
    "local",
    "minus",
    "put",
    "thing",
    "also",
    "pass",
    "argument",
    "copying",
    "control",
    "replication",
    "aspects",
    "file",
    "use",
    "dfs",
    "admin",
    "refresh",
    "nodes",
    "rm",
    "admin",
    "refresh",
    "nodes",
    "command",
    "says",
    "basically",
    "refreshing",
    "node",
    "information",
    "refresh",
    "nodes",
    "mainly",
    "used",
    "say",
    "commissioning",
    "decommissioning",
    "nodes",
    "done",
    "node",
    "added",
    "cluster",
    "node",
    "removed",
    "cluster",
    "actually",
    "informing",
    "hadoop",
    "master",
    "particular",
    "node",
    "would",
    "used",
    "storage",
    "would",
    "used",
    "processing",
    "case",
    "would",
    "done",
    "process",
    "commissioning",
    "decommissioning",
    "would",
    "giving",
    "commands",
    "refresh",
    "nodes",
    "rm",
    "admin",
    "refresh",
    "nodes",
    "internally",
    "talk",
    "commissioning",
    "decommissioning",
    "include",
    "exclude",
    "files",
    "updated",
    "include",
    "exclude",
    "files",
    "entry",
    "machines",
    "added",
    "cluster",
    "machines",
    "removed",
    "cluster",
    "done",
    "cluster",
    "still",
    "running",
    "restart",
    "master",
    "process",
    "however",
    "use",
    "refresh",
    "commands",
    "take",
    "care",
    "commissioning",
    "decommissioning",
    "activities",
    "way",
    "change",
    "replication",
    "files",
    "sdfs",
    "already",
    "written",
    "answer",
    "course",
    "yes",
    "would",
    "want",
    "set",
    "replication",
    "factor",
    "cluster",
    "level",
    "admin",
    "access",
    "could",
    "edit",
    "sdfs",
    "hyphen",
    "site",
    "file",
    "could",
    "say",
    "hadoop",
    "hyphen",
    "site",
    "file",
    "would",
    "take",
    "care",
    "replication",
    "factor",
    "set",
    "cluster",
    "level",
    "however",
    "would",
    "want",
    "change",
    "replication",
    "data",
    "written",
    "could",
    "always",
    "use",
    "setrep",
    "command",
    "setrep",
    "command",
    "basically",
    "change",
    "replication",
    "data",
    "written",
    "could",
    "also",
    "write",
    "data",
    "different",
    "replication",
    "could",
    "use",
    "minus",
    "dfs",
    "dot",
    "replication",
    "give",
    "replication",
    "factor",
    "writing",
    "data",
    "cluster",
    "hadoop",
    "let",
    "data",
    "replicated",
    "per",
    "property",
    "set",
    "config",
    "file",
    "could",
    "write",
    "data",
    "different",
    "replication",
    "could",
    "change",
    "replication",
    "data",
    "written",
    "options",
    "available",
    "takes",
    "care",
    "replication",
    "consistency",
    "hadoop",
    "cluster",
    "mean",
    "replicated",
    "blocks",
    "mentioned",
    "fsck",
    "command",
    "give",
    "information",
    "replicated",
    "blocks",
    "cluster",
    "always",
    "always",
    "name",
    "node",
    "takes",
    "care",
    "replication",
    "consistency",
    "example",
    "set",
    "replication",
    "three",
    "since",
    "know",
    "first",
    "rule",
    "replication",
    "basically",
    "means",
    "two",
    "replicas",
    "residing",
    "node",
    "would",
    "mean",
    "replication",
    "three",
    "would",
    "need",
    "least",
    "three",
    "data",
    "nodes",
    "available",
    "say",
    "example",
    "cluster",
    "three",
    "nodes",
    "replication",
    "set",
    "three",
    "one",
    "point",
    "time",
    "one",
    "name",
    "node",
    "crashed",
    "happens",
    "blocks",
    "would",
    "replicated",
    "means",
    "replication",
    "factor",
    "set",
    "blocks",
    "replicated",
    "enough",
    "replicas",
    "per",
    "replication",
    "factor",
    "set",
    "problem",
    "master",
    "process",
    "name",
    "node",
    "wait",
    "time",
    "start",
    "replication",
    "data",
    "data",
    "road",
    "responding",
    "disk",
    "crashed",
    "name",
    "node",
    "get",
    "information",
    "replica",
    "name",
    "node",
    "wait",
    "time",
    "start",
    "missing",
    "blocks",
    "available",
    "nodes",
    "however",
    "name",
    "node",
    "blocks",
    "replicated",
    "situation",
    "talk",
    "replicated",
    "situation",
    "name",
    "node",
    "realizes",
    "extra",
    "copies",
    "block",
    "might",
    "case",
    "three",
    "nodes",
    "running",
    "replication",
    "three",
    "one",
    "node",
    "went",
    "due",
    "network",
    "failure",
    "issue",
    "within",
    "minutes",
    "name",
    "node",
    "data",
    "failed",
    "node",
    "back",
    "set",
    "blocks",
    "name",
    "node",
    "smart",
    "enough",
    "understand",
    "replication",
    "situation",
    "delete",
    "set",
    "blocks",
    "one",
    "nodes",
    "might",
    "node",
    "recently",
    "added",
    "might",
    "old",
    "node",
    "joined",
    "cluster",
    "node",
    "depends",
    "load",
    "particular",
    "node",
    "discussed",
    "hadoop",
    "discussed",
    "sdfs",
    "discuss",
    "mapreduce",
    "programming",
    "model",
    "say",
    "processing",
    "framework",
    "distributed",
    "cache",
    "mapreduce",
    "know",
    "talk",
    "mapreduce",
    "data",
    "processed",
    "might",
    "existing",
    "multiple",
    "nodes",
    "would",
    "mapreduce",
    "program",
    "running",
    "would",
    "basically",
    "read",
    "data",
    "underlying",
    "disks",
    "could",
    "costly",
    "operation",
    "every",
    "time",
    "data",
    "read",
    "disk",
    "distributed",
    "cache",
    "mechanism",
    "wherein",
    "data",
    "set",
    "data",
    "coming",
    "disk",
    "cached",
    "available",
    "worker",
    "nodes",
    "benefit",
    "map",
    "reduce",
    "running",
    "instead",
    "every",
    "time",
    "reading",
    "data",
    "disk",
    "would",
    "pick",
    "data",
    "distributed",
    "cache",
    "benefit",
    "mapreduce",
    "processing",
    "distributed",
    "cache",
    "set",
    "jobconf",
    "specify",
    "file",
    "picked",
    "distributed",
    "cache",
    "let",
    "understand",
    "roles",
    "record",
    "reader",
    "combiner",
    "partitioner",
    "kind",
    "roles",
    "play",
    "map",
    "reduce",
    "processing",
    "paradigm",
    "map",
    "reduce",
    "operation",
    "record",
    "reader",
    "communicates",
    "input",
    "split",
    "basically",
    "converts",
    "data",
    "key",
    "value",
    "pairs",
    "key",
    "value",
    "pairs",
    "ones",
    "worked",
    "upon",
    "mapper",
    "combiner",
    "optional",
    "face",
    "like",
    "mini",
    "radius",
    "combiner",
    "class",
    "relies",
    "reducer",
    "class",
    "basically",
    "combiner",
    "would",
    "receive",
    "data",
    "map",
    "tasks",
    "would",
    "completed",
    "works",
    "based",
    "whatever",
    "reducer",
    "class",
    "mentions",
    "passes",
    "output",
    "reducer",
    "phase",
    "partitioner",
    "basically",
    "phase",
    "decides",
    "many",
    "reduced",
    "tasks",
    "would",
    "used",
    "aggregate",
    "summarize",
    "data",
    "partitioner",
    "phase",
    "would",
    "decide",
    "based",
    "number",
    "keys",
    "based",
    "number",
    "map",
    "tasks",
    "partitioner",
    "would",
    "decide",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "would",
    "used",
    "take",
    "care",
    "processing",
    "either",
    "could",
    "partitioner",
    "decides",
    "many",
    "reduced",
    "tasks",
    "would",
    "run",
    "could",
    "based",
    "properties",
    "set",
    "within",
    "cluster",
    "take",
    "care",
    "number",
    "reduced",
    "tasks",
    "would",
    "used",
    "always",
    "remember",
    "partitioner",
    "decides",
    "outputs",
    "combiner",
    "sent",
    "reducer",
    "many",
    "reducers",
    "controls",
    "partitioning",
    "keys",
    "intermediate",
    "map",
    "outputs",
    "map",
    "phase",
    "whatever",
    "output",
    "generates",
    "intermediate",
    "output",
    "taken",
    "practitioner",
    "combiner",
    "partitioner",
    "sent",
    "one",
    "multiple",
    "reduced",
    "tasks",
    "one",
    "common",
    "questions",
    "might",
    "face",
    "mapreduce",
    "lower",
    "processing",
    "know",
    "mapreduce",
    "goes",
    "parallel",
    "processing",
    "know",
    "multiple",
    "map",
    "tasks",
    "running",
    "multiple",
    "nodes",
    "time",
    "also",
    "know",
    "multiple",
    "reduced",
    "tasks",
    "could",
    "running",
    "mapreduce",
    "become",
    "slower",
    "approach",
    "first",
    "mapreduce",
    "batch",
    "oriented",
    "operation",
    "mapreduce",
    "rigid",
    "strictly",
    "uses",
    "mapping",
    "reducing",
    "phases",
    "matter",
    "kind",
    "processing",
    "would",
    "want",
    "would",
    "still",
    "provide",
    "mapper",
    "function",
    "reducer",
    "function",
    "work",
    "data",
    "whenever",
    "map",
    "phase",
    "completes",
    "output",
    "map",
    "face",
    "intermittent",
    "output",
    "would",
    "written",
    "hdfs",
    "thereafter",
    "underlying",
    "disks",
    "data",
    "would",
    "shuffled",
    "sorted",
    "picked",
    "reducing",
    "phase",
    "every",
    "time",
    "data",
    "written",
    "hdfs",
    "retrieved",
    "sdfs",
    "makes",
    "mapreduce",
    "slower",
    "approach",
    "question",
    "mapreduce",
    "job",
    "possible",
    "change",
    "number",
    "mappers",
    "created",
    "default",
    "change",
    "number",
    "map",
    "tasks",
    "number",
    "map",
    "tasks",
    "depends",
    "input",
    "splits",
    "however",
    "different",
    "ways",
    "either",
    "set",
    "property",
    "number",
    "map",
    "tasks",
    "used",
    "customize",
    "code",
    "make",
    "use",
    "different",
    "format",
    "control",
    "number",
    "map",
    "tasks",
    "default",
    "number",
    "map",
    "tasks",
    "equal",
    "number",
    "splits",
    "file",
    "processing",
    "1gb",
    "file",
    "split",
    "8",
    "blocks",
    "128",
    "mb",
    "would",
    "8",
    "map",
    "tasks",
    "running",
    "cluster",
    "map",
    "tasks",
    "basically",
    "running",
    "mapper",
    "function",
    "hard",
    "coded",
    "properties",
    "map",
    "red",
    "hyphen",
    "site",
    "file",
    "specify",
    "number",
    "map",
    "tasks",
    "could",
    "control",
    "number",
    "map",
    "tasks",
    "let",
    "also",
    "talk",
    "data",
    "types",
    "prepare",
    "hadoop",
    "want",
    "get",
    "big",
    "data",
    "field",
    "start",
    "learning",
    "different",
    "data",
    "formats",
    "different",
    "data",
    "formats",
    "avro",
    "parquet",
    "sequence",
    "file",
    "binary",
    "format",
    "different",
    "formats",
    "used",
    "talk",
    "data",
    "types",
    "hadoop",
    "implementation",
    "writable",
    "writable",
    "comparable",
    "interfaces",
    "every",
    "data",
    "type",
    "java",
    "equivalent",
    "hadoop",
    "ind",
    "java",
    "would",
    "intriguable",
    "hadoop",
    "float",
    "would",
    "float",
    "writable",
    "long",
    "would",
    "long",
    "writable",
    "double",
    "writable",
    "boolean",
    "writable",
    "array",
    "writable",
    "map",
    "writable",
    "object",
    "credible",
    "different",
    "data",
    "types",
    "could",
    "used",
    "within",
    "mapreduce",
    "program",
    "implementation",
    "writable",
    "writable",
    "comparable",
    "interfaces",
    "speculative",
    "execution",
    "imagine",
    "cluster",
    "huge",
    "number",
    "nodes",
    "data",
    "spread",
    "across",
    "multiple",
    "slave",
    "machines",
    "multiple",
    "nodes",
    "point",
    "time",
    "due",
    "disk",
    "degrade",
    "network",
    "issues",
    "machine",
    "heating",
    "load",
    "particular",
    "node",
    "situation",
    "data",
    "node",
    "execute",
    "task",
    "slower",
    "manner",
    "case",
    "speculative",
    "execution",
    "turned",
    "would",
    "shadow",
    "task",
    "another",
    "similar",
    "task",
    "running",
    "node",
    "processing",
    "whichever",
    "task",
    "finishes",
    "first",
    "accepted",
    "task",
    "would",
    "killed",
    "speculative",
    "execution",
    "could",
    "good",
    "working",
    "intensive",
    "workload",
    "kind",
    "environment",
    "particular",
    "node",
    "slower",
    "could",
    "benefit",
    "unoccupied",
    "node",
    "less",
    "load",
    "take",
    "care",
    "processing",
    "going",
    "understand",
    "node",
    "might",
    "slower",
    "task",
    "would",
    "scheduler",
    "maintaining",
    "knowledge",
    "resources",
    "available",
    "speculative",
    "execution",
    "property",
    "turned",
    "task",
    "running",
    "slow",
    "copy",
    "task",
    "say",
    "shadow",
    "task",
    "would",
    "run",
    "node",
    "whichever",
    "task",
    "completes",
    "first",
    "considered",
    "happens",
    "speculative",
    "execution",
    "identity",
    "mapper",
    "different",
    "chain",
    "mapper",
    "getting",
    "deeper",
    "mapreduce",
    "concepts",
    "talk",
    "mapper",
    "identity",
    "mapper",
    "default",
    "mapper",
    "chosen",
    "mapper",
    "specified",
    "mapreduce",
    "driver",
    "class",
    "every",
    "mapreduce",
    "program",
    "would",
    "map",
    "class",
    "taking",
    "care",
    "mapping",
    "phase",
    "basically",
    "mapper",
    "function",
    "would",
    "run",
    "one",
    "multiple",
    "map",
    "tasks",
    "right",
    "programming",
    "program",
    "would",
    "also",
    "reduce",
    "class",
    "would",
    "running",
    "reducer",
    "function",
    "takes",
    "care",
    "reduced",
    "tasks",
    "running",
    "multiple",
    "nodes",
    "mapper",
    "specified",
    "within",
    "driver",
    "class",
    "driver",
    "class",
    "something",
    "information",
    "flow",
    "map",
    "class",
    "reduced",
    "class",
    "input",
    "format",
    "output",
    "format",
    "job",
    "configurations",
    "identity",
    "mapper",
    "default",
    "mapper",
    "chosen",
    "mapper",
    "class",
    "mentioned",
    "driver",
    "class",
    "basically",
    "implements",
    "identity",
    "function",
    "directly",
    "writes",
    "key",
    "pairs",
    "output",
    "defined",
    "old",
    "mapreduce",
    "api",
    "particular",
    "package",
    "talk",
    "chaining",
    "mappers",
    "chain",
    "mapper",
    "basically",
    "class",
    "run",
    "multiple",
    "mappers",
    "single",
    "map",
    "task",
    "basically",
    "could",
    "say",
    "multiple",
    "map",
    "tasks",
    "would",
    "run",
    "part",
    "processing",
    "output",
    "first",
    "mapper",
    "would",
    "become",
    "input",
    "second",
    "mapper",
    "defined",
    "mentioned",
    "class",
    "package",
    "major",
    "configuration",
    "parameters",
    "required",
    "mapreduce",
    "program",
    "obviously",
    "need",
    "input",
    "location",
    "need",
    "output",
    "location",
    "input",
    "location",
    "files",
    "picked",
    "would",
    "preferably",
    "sdfs",
    "directory",
    "output",
    "location",
    "path",
    "job",
    "output",
    "would",
    "written",
    "mapreduce",
    "program",
    "also",
    "need",
    "specify",
    "input",
    "output",
    "formats",
    "specify",
    "defaults",
    "considered",
    "need",
    "also",
    "classes",
    "map",
    "reduce",
    "functions",
    "intend",
    "run",
    "code",
    "cluster",
    "need",
    "package",
    "class",
    "jar",
    "file",
    "export",
    "cluster",
    "jar",
    "file",
    "would",
    "mapper",
    "reducer",
    "driver",
    "classes",
    "important",
    "configuration",
    "parameters",
    "need",
    "consider",
    "map",
    "reduce",
    "program",
    "difference",
    "mean",
    "map",
    "side",
    "join",
    "reduce",
    "side",
    "join",
    "map",
    "side",
    "join",
    "basically",
    "join",
    "performed",
    "mapping",
    "level",
    "mapping",
    "phase",
    "performed",
    "mapper",
    "input",
    "data",
    "worked",
    "upon",
    "divided",
    "number",
    "partitions",
    "input",
    "map",
    "form",
    "structured",
    "partition",
    "sorted",
    "order",
    "map",
    "site",
    "join",
    "understand",
    "simpler",
    "way",
    "compare",
    "rdbms",
    "concepts",
    "two",
    "tables",
    "joined",
    "always",
    "advisable",
    "give",
    "bigger",
    "table",
    "left",
    "side",
    "table",
    "first",
    "table",
    "join",
    "condition",
    "would",
    "smaller",
    "table",
    "left",
    "side",
    "bigger",
    "table",
    "right",
    "side",
    "basically",
    "means",
    "smaller",
    "table",
    "could",
    "loaded",
    "memory",
    "could",
    "used",
    "joining",
    "map",
    "site",
    "drawing",
    "similar",
    "kind",
    "mechanism",
    "input",
    "data",
    "divided",
    "number",
    "partitions",
    "talk",
    "reduced",
    "side",
    "join",
    "join",
    "performed",
    "reducer",
    "easier",
    "implement",
    "website",
    "join",
    "sorting",
    "shuffling",
    "send",
    "values",
    "send",
    "values",
    "identical",
    "keys",
    "reducer",
    "need",
    "data",
    "set",
    "structured",
    "form",
    "look",
    "map",
    "side",
    "join",
    "reduce",
    "side",
    "join",
    "joins",
    "understand",
    "mapreduce",
    "works",
    "however",
    "would",
    "suggest",
    "focus",
    "mapreduce",
    "still",
    "used",
    "processing",
    "amount",
    "mapreduce",
    "based",
    "processing",
    "decreased",
    "overall",
    "across",
    "industry",
    "role",
    "output",
    "committer",
    "class",
    "mapreduce",
    "job",
    "output",
    "committer",
    "name",
    "says",
    "describes",
    "commit",
    "task",
    "output",
    "mapreduce",
    "job",
    "could",
    "mentioned",
    "capacity",
    "hadoop",
    "mapreduce",
    "output",
    "committer",
    "could",
    "class",
    "extends",
    "output",
    "committer",
    "class",
    "mapreduce",
    "relies",
    "mapreduce",
    "relies",
    "output",
    "committer",
    "job",
    "set",
    "job",
    "initialization",
    "cleaning",
    "job",
    "job",
    "completion",
    "means",
    "resources",
    "used",
    "particular",
    "job",
    "setting",
    "task",
    "temporary",
    "output",
    "checking",
    "whether",
    "task",
    "needs",
    "commit",
    "committing",
    "task",
    "output",
    "discarding",
    "task",
    "important",
    "class",
    "used",
    "within",
    "mapreduce",
    "job",
    "process",
    "spilling",
    "mapreduce",
    "mean",
    "spilling",
    "basically",
    "process",
    "copying",
    "data",
    "memory",
    "buffer",
    "disk",
    "obviously",
    "buffer",
    "usage",
    "reaches",
    "certain",
    "threshold",
    "enough",
    "memory",
    "buffer",
    "memory",
    "content",
    "stored",
    "buffer",
    "memory",
    "flushed",
    "default",
    "background",
    "thread",
    "starts",
    "spilling",
    "content",
    "memory",
    "disk",
    "eighty",
    "percent",
    "buffer",
    "size",
    "filled",
    "buffer",
    "used",
    "mapreduce",
    "processing",
    "happening",
    "data",
    "data",
    "read",
    "disk",
    "loaded",
    "buffer",
    "processing",
    "happens",
    "thing",
    "also",
    "happens",
    "writing",
    "data",
    "cluster",
    "imagine",
    "100",
    "megabyte",
    "size",
    "buffer",
    "spilling",
    "start",
    "content",
    "buffer",
    "reaches",
    "80",
    "megabytes",
    "customizable",
    "set",
    "mappers",
    "reducers",
    "mapreduce",
    "job",
    "properties",
    "number",
    "mappers",
    "reducers",
    "mentioned",
    "earlier",
    "customized",
    "default",
    "number",
    "map",
    "tasks",
    "depends",
    "split",
    "number",
    "reduced",
    "tasks",
    "depends",
    "partitioning",
    "phase",
    "decides",
    "number",
    "reduced",
    "tasks",
    "would",
    "used",
    "depending",
    "keys",
    "however",
    "set",
    "properties",
    "either",
    "config",
    "files",
    "provide",
    "command",
    "line",
    "also",
    "make",
    "part",
    "code",
    "control",
    "number",
    "map",
    "tasks",
    "reduce",
    "tasks",
    "would",
    "run",
    "particular",
    "job",
    "let",
    "look",
    "one",
    "interesting",
    "question",
    "happens",
    "node",
    "running",
    "map",
    "task",
    "fails",
    "sending",
    "output",
    "reducer",
    "node",
    "running",
    "map",
    "task",
    "know",
    "could",
    "one",
    "multiple",
    "map",
    "tasks",
    "running",
    "one",
    "multiple",
    "nodes",
    "map",
    "tasks",
    "completed",
    "stages",
    "combiner",
    "reducer",
    "come",
    "existence",
    "case",
    "node",
    "crashes",
    "map",
    "task",
    "assigned",
    "whole",
    "task",
    "run",
    "note",
    "hadoop",
    "version",
    "2",
    "yarn",
    "framework",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "application",
    "master",
    "taking",
    "care",
    "execution",
    "application",
    "particular",
    "task",
    "particular",
    "node",
    "failed",
    "due",
    "unavailability",
    "node",
    "role",
    "application",
    "master",
    "task",
    "scheduled",
    "node",
    "write",
    "output",
    "mapreduce",
    "different",
    "formats",
    "course",
    "hadoop",
    "supports",
    "various",
    "input",
    "output",
    "formats",
    "write",
    "output",
    "mapreduce",
    "different",
    "formats",
    "could",
    "default",
    "format",
    "text",
    "output",
    "format",
    "wherein",
    "records",
    "written",
    "line",
    "text",
    "could",
    "sequence",
    "file",
    "basically",
    "write",
    "sequence",
    "files",
    "binary",
    "format",
    "files",
    "output",
    "files",
    "need",
    "fed",
    "another",
    "mapreduce",
    "jobs",
    "could",
    "go",
    "map",
    "file",
    "output",
    "format",
    "write",
    "output",
    "map",
    "files",
    "could",
    "go",
    "sequence",
    "file",
    "binary",
    "output",
    "format",
    "variant",
    "sequence",
    "file",
    "input",
    "format",
    "basically",
    "writes",
    "keys",
    "values",
    "sequence",
    "file",
    "talk",
    "binary",
    "format",
    "talking",
    "readable",
    "format",
    "db",
    "output",
    "format",
    "basically",
    "used",
    "would",
    "want",
    "write",
    "data",
    "say",
    "relational",
    "databases",
    "say",
    "sql",
    "databases",
    "hbase",
    "format",
    "also",
    "sends",
    "reduce",
    "output",
    "sql",
    "table",
    "let",
    "learn",
    "little",
    "bit",
    "yarn",
    "yarn",
    "stands",
    "yet",
    "another",
    "resource",
    "negotiator",
    "processing",
    "framework",
    "benefits",
    "yan",
    "bring",
    "hadoop",
    "version",
    "2",
    "solve",
    "issues",
    "mapreduce",
    "version",
    "mapreduce",
    "version",
    "1",
    "major",
    "issues",
    "comes",
    "scalability",
    "availability",
    "sorry",
    "hadoop",
    "version",
    "1",
    "one",
    "master",
    "process",
    "processing",
    "layer",
    "job",
    "tracker",
    "job",
    "tracker",
    "listening",
    "task",
    "trackers",
    "running",
    "multiple",
    "machines",
    "job",
    "tracker",
    "responsible",
    "resource",
    "tracking",
    "job",
    "scheduling",
    "yarn",
    "still",
    "processing",
    "master",
    "called",
    "resource",
    "manager",
    "instead",
    "job",
    "tracker",
    "hadoop",
    "version",
    "2",
    "could",
    "even",
    "resource",
    "manager",
    "running",
    "high",
    "availability",
    "mode",
    "node",
    "managers",
    "would",
    "running",
    "multiple",
    "machines",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "case",
    "hadoop",
    "version",
    "2",
    "resource",
    "manager",
    "master",
    "handling",
    "client",
    "connections",
    "taking",
    "care",
    "tracking",
    "resources",
    "jobs",
    "scheduling",
    "basically",
    "taking",
    "care",
    "execution",
    "across",
    "multiple",
    "nodes",
    "controlled",
    "application",
    "master",
    "till",
    "application",
    "completes",
    "yarn",
    "different",
    "kind",
    "resource",
    "allocations",
    "could",
    "done",
    "concept",
    "container",
    "container",
    "basically",
    "combination",
    "ram",
    "cpu",
    "cores",
    "yarn",
    "run",
    "different",
    "kind",
    "workloads",
    "mapreduce",
    "kind",
    "workload",
    "run",
    "adobe",
    "person",
    "2",
    "would",
    "graph",
    "processing",
    "massive",
    "parallel",
    "processing",
    "could",
    "real",
    "time",
    "processing",
    "huge",
    "processing",
    "applications",
    "could",
    "run",
    "cluster",
    "based",
    "yarn",
    "talk",
    "scalability",
    "case",
    "hadoop",
    "version",
    "2",
    "cluster",
    "size",
    "10",
    "000",
    "nodes",
    "run",
    "100",
    "000",
    "concurrent",
    "tasks",
    "every",
    "application",
    "launched",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "would",
    "10",
    "applications",
    "running",
    "would",
    "10",
    "app",
    "masters",
    "running",
    "taking",
    "care",
    "execution",
    "applications",
    "across",
    "multiple",
    "nodes",
    "compatibility",
    "hadoop",
    "version",
    "2",
    "fully",
    "compatible",
    "whatever",
    "developed",
    "per",
    "hadoop",
    "version",
    "1",
    "processing",
    "needs",
    "would",
    "taken",
    "care",
    "yarn",
    "dynamic",
    "allocation",
    "cluster",
    "resources",
    "taking",
    "care",
    "different",
    "workloads",
    "allocating",
    "resources",
    "across",
    "multiple",
    "machines",
    "using",
    "execution",
    "taken",
    "care",
    "yarn",
    "multi",
    "tenancy",
    "basically",
    "means",
    "could",
    "multiple",
    "users",
    "multiple",
    "teams",
    "could",
    "open",
    "source",
    "proprietary",
    "data",
    "access",
    "engines",
    "could",
    "basically",
    "hosted",
    "using",
    "cluster",
    "yarn",
    "allocate",
    "resources",
    "application",
    "help",
    "architecture",
    "basically",
    "client",
    "application",
    "api",
    "talks",
    "resource",
    "manager",
    "resource",
    "manager",
    "mentioned",
    "managing",
    "resource",
    "allocation",
    "cluster",
    "talk",
    "resource",
    "manager",
    "internal",
    "two",
    "components",
    "one",
    "scheduler",
    "one",
    "applications",
    "manager",
    "say",
    "resource",
    "manager",
    "master",
    "tracking",
    "resources",
    "source",
    "manager",
    "one",
    "negotiating",
    "resources",
    "slave",
    "actually",
    "resource",
    "manager",
    "internal",
    "components",
    "scheduler",
    "allocates",
    "resources",
    "various",
    "running",
    "applications",
    "scheduler",
    "bothered",
    "tracking",
    "resources",
    "basically",
    "tracking",
    "applications",
    "different",
    "kind",
    "schedulers",
    "fifo",
    "first",
    "first",
    "could",
    "fair",
    "scheduler",
    "could",
    "capacity",
    "scheduler",
    "schedulers",
    "basically",
    "control",
    "resources",
    "allocated",
    "multiple",
    "applications",
    "running",
    "parallel",
    "queue",
    "mechanism",
    "scheduler",
    "schedule",
    "resources",
    "based",
    "requirements",
    "application",
    "monitoring",
    "tracking",
    "status",
    "applications",
    "applications",
    "manager",
    "one",
    "accepting",
    "job",
    "submissions",
    "monitoring",
    "restarting",
    "application",
    "masters",
    "application",
    "manager",
    "basically",
    "launching",
    "application",
    "master",
    "responsible",
    "application",
    "looks",
    "whenever",
    "job",
    "submission",
    "happens",
    "already",
    "know",
    "resource",
    "manager",
    "aware",
    "resources",
    "available",
    "every",
    "node",
    "manager",
    "every",
    "node",
    "fixed",
    "amount",
    "ram",
    "cpu",
    "cores",
    "portion",
    "resources",
    "ram",
    "cpu",
    "cores",
    "allocated",
    "node",
    "manager",
    "resource",
    "manager",
    "already",
    "aware",
    "much",
    "resources",
    "available",
    "across",
    "nodes",
    "whenever",
    "client",
    "request",
    "comes",
    "resource",
    "manager",
    "make",
    "request",
    "node",
    "manager",
    "basically",
    "request",
    "node",
    "manager",
    "hold",
    "resources",
    "processing",
    "node",
    "manager",
    "would",
    "basically",
    "approve",
    "disapprove",
    "request",
    "holding",
    "sources",
    "resources",
    "combination",
    "ram",
    "cpu",
    "cores",
    "nothing",
    "containers",
    "allocate",
    "containers",
    "different",
    "sizes",
    "within",
    "yarn",
    "hyphen",
    "site",
    "file",
    "node",
    "manager",
    "based",
    "request",
    "resource",
    "manager",
    "guarantees",
    "container",
    "would",
    "available",
    "processing",
    "resource",
    "manager",
    "starts",
    "temporary",
    "demon",
    "called",
    "application",
    "master",
    "take",
    "care",
    "execution",
    "app",
    "master",
    "launched",
    "resource",
    "manager",
    "say",
    "internal",
    "away",
    "applications",
    "manager",
    "run",
    "one",
    "containers",
    "application",
    "master",
    "also",
    "piece",
    "code",
    "run",
    "one",
    "containers",
    "containers",
    "utilized",
    "execution",
    "yarn",
    "basically",
    "taking",
    "care",
    "allocation",
    "application",
    "master",
    "managing",
    "resource",
    "needs",
    "one",
    "interacting",
    "scheduler",
    "particular",
    "node",
    "crashes",
    "responsibility",
    "app",
    "master",
    "go",
    "back",
    "master",
    "resource",
    "manager",
    "negotiate",
    "resources",
    "app",
    "master",
    "never",
    "ever",
    "negotiate",
    "resources",
    "node",
    "manager",
    "directly",
    "always",
    "talk",
    "resource",
    "manager",
    "source",
    "manager",
    "one",
    "negotiates",
    "resources",
    "container",
    "said",
    "collection",
    "resources",
    "like",
    "ram",
    "cpu",
    "network",
    "bandwidth",
    "container",
    "allocated",
    "based",
    "availability",
    "resources",
    "particular",
    "node",
    "following",
    "occupied",
    "place",
    "job",
    "tracker",
    "mapreduce",
    "resource",
    "manager",
    "resource",
    "manager",
    "name",
    "master",
    "process",
    "adobe",
    "portion",
    "would",
    "write",
    "yarn",
    "commands",
    "check",
    "status",
    "application",
    "could",
    "say",
    "yarn",
    "application",
    "minus",
    "status",
    "application",
    "id",
    "could",
    "kill",
    "also",
    "command",
    "line",
    "remember",
    "yarn",
    "ui",
    "even",
    "look",
    "applications",
    "ui",
    "even",
    "kill",
    "applications",
    "ui",
    "however",
    "knowing",
    "command",
    "line",
    "commands",
    "would",
    "useful",
    "one",
    "resource",
    "manager",
    "young",
    "base",
    "cluster",
    "yes",
    "hadoop",
    "version",
    "2",
    "allows",
    "us",
    "high",
    "availability",
    "yarn",
    "cluster",
    "active",
    "standby",
    "coordination",
    "taking",
    "care",
    "zookeeper",
    "particular",
    "time",
    "one",
    "active",
    "resource",
    "manager",
    "active",
    "resource",
    "manager",
    "fails",
    "standby",
    "resource",
    "manager",
    "comes",
    "becomes",
    "active",
    "however",
    "zookeeper",
    "playing",
    "important",
    "role",
    "remember",
    "zookeeper",
    "one",
    "coordinating",
    "server",
    "state",
    "election",
    "active",
    "standby",
    "failover",
    "different",
    "schedulers",
    "available",
    "yarn",
    "fifo",
    "scheduler",
    "first",
    "first",
    "desirable",
    "option",
    "case",
    "longer",
    "running",
    "application",
    "might",
    "block",
    "small",
    "running",
    "applications",
    "capacity",
    "scheduler",
    "basically",
    "scheduler",
    "dedicated",
    "queues",
    "created",
    "fixed",
    "amount",
    "resources",
    "multiple",
    "applications",
    "accessing",
    "cluster",
    "time",
    "would",
    "using",
    "cues",
    "resources",
    "allocated",
    "talk",
    "fair",
    "scheduler",
    "need",
    "fixed",
    "amount",
    "resources",
    "percentage",
    "could",
    "decide",
    "kind",
    "fairness",
    "followed",
    "basically",
    "means",
    "allocated",
    "20",
    "gigabytes",
    "memory",
    "however",
    "cluster",
    "100",
    "gigabytes",
    "team",
    "assigned",
    "80",
    "gigabytes",
    "memory",
    "20",
    "percent",
    "access",
    "cluster",
    "another",
    "team",
    "80",
    "percent",
    "however",
    "team",
    "come",
    "use",
    "cluster",
    "fair",
    "scheduler",
    "go",
    "maximum",
    "100",
    "cluster",
    "find",
    "information",
    "schedulers",
    "could",
    "either",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "could",
    "could",
    "go",
    "google",
    "could",
    "type",
    "example",
    "yarn",
    "scheduler",
    "let",
    "search",
    "yarn",
    "scheduler",
    "look",
    "hadoop",
    "definitive",
    "guide",
    "hadoop",
    "definitive",
    "guide",
    "beautifully",
    "explains",
    "different",
    "schedulers",
    "multiple",
    "applications",
    "run",
    "could",
    "fifo",
    "kind",
    "scheduling",
    "could",
    "capacity",
    "scheduler",
    "could",
    "fair",
    "scheduling",
    "look",
    "link",
    "good",
    "link",
    "also",
    "search",
    "yarn",
    "untangling",
    "blog",
    "four",
    "series",
    "four",
    "blocks",
    "beautifully",
    "explained",
    "yarn",
    "works",
    "resource",
    "allocation",
    "happens",
    "container",
    "runs",
    "within",
    "container",
    "scroll",
    "reading",
    "also",
    "search",
    "part",
    "two",
    "talks",
    "allocation",
    "coming",
    "back",
    "basically",
    "schedulers",
    "happens",
    "resource",
    "manager",
    "fails",
    "executing",
    "application",
    "high",
    "availability",
    "cluster",
    "high",
    "availability",
    "cluster",
    "know",
    "would",
    "two",
    "resource",
    "managers",
    "one",
    "active",
    "one",
    "standby",
    "zookeeper",
    "keeping",
    "track",
    "server",
    "states",
    "rm",
    "fails",
    "case",
    "high",
    "availability",
    "standby",
    "elected",
    "active",
    "basically",
    "resource",
    "manager",
    "standby",
    "would",
    "become",
    "active",
    "one",
    "one",
    "would",
    "instruct",
    "application",
    "master",
    "abort",
    "beginning",
    "resource",
    "manager",
    "recovers",
    "running",
    "state",
    "something",
    "called",
    "rm",
    "state",
    "store",
    "applications",
    "running",
    "status",
    "stored",
    "resource",
    "manager",
    "recovers",
    "running",
    "state",
    "looking",
    "state",
    "store",
    "taking",
    "advantage",
    "container",
    "statuses",
    "continues",
    "take",
    "care",
    "processing",
    "cluster",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "10",
    "cores",
    "would",
    "total",
    "processing",
    "capacity",
    "cluster",
    "take",
    "minute",
    "think",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "ram",
    "per",
    "node",
    "10",
    "cores",
    "mention",
    "answer",
    "160",
    "gb",
    "ram",
    "100",
    "cores",
    "went",
    "wrong",
    "think",
    "cluster",
    "10",
    "data",
    "nodes",
    "16",
    "gb",
    "ram",
    "10",
    "cores",
    "remember",
    "every",
    "node",
    "hadoop",
    "cluster",
    "would",
    "one",
    "multiple",
    "processors",
    "running",
    "processes",
    "would",
    "need",
    "ram",
    "machine",
    "linux",
    "file",
    "system",
    "would",
    "processes",
    "would",
    "also",
    "ram",
    "usage",
    "basically",
    "means",
    "talk",
    "10",
    "data",
    "nodes",
    "deduct",
    "least",
    "20",
    "30",
    "percent",
    "towards",
    "overheads",
    "towards",
    "cloud",
    "database",
    "services",
    "towards",
    "processes",
    "running",
    "case",
    "could",
    "say",
    "could",
    "11",
    "12",
    "gb",
    "available",
    "every",
    "machine",
    "processing",
    "say",
    "6",
    "7",
    "cores",
    "multiply",
    "10",
    "processing",
    "capacity",
    "remember",
    "thing",
    "applies",
    "disk",
    "usage",
    "also",
    "somebody",
    "asks",
    "10",
    "data",
    "node",
    "cluster",
    "machine",
    "20",
    "terabytes",
    "disks",
    "total",
    "storage",
    "capacity",
    "available",
    "sdfs",
    "answer",
    "would",
    "200",
    "consider",
    "overheads",
    "basically",
    "gives",
    "processing",
    "capacity",
    "let",
    "look",
    "one",
    "question",
    "happens",
    "requested",
    "memory",
    "cpu",
    "cores",
    "beyond",
    "goes",
    "beyond",
    "size",
    "container",
    "said",
    "configurations",
    "say",
    "particular",
    "data",
    "node",
    "100",
    "gb",
    "ram",
    "could",
    "allocate",
    "say",
    "50",
    "gb",
    "processing",
    "like",
    "100",
    "cores",
    "could",
    "say",
    "50",
    "cores",
    "processing",
    "100",
    "gb",
    "ram",
    "100",
    "cores",
    "could",
    "ideally",
    "allocate",
    "100",
    "processing",
    "ideally",
    "possible",
    "100",
    "gb",
    "ram",
    "would",
    "go",
    "50",
    "gb",
    "100",
    "cores",
    "would",
    "go",
    "50",
    "cores",
    "within",
    "ram",
    "cpu",
    "course",
    "concept",
    "containers",
    "right",
    "container",
    "combination",
    "ram",
    "cpu",
    "cores",
    "could",
    "minimum",
    "size",
    "container",
    "maximum",
    "size",
    "container",
    "point",
    "time",
    "application",
    "starts",
    "demanding",
    "memory",
    "cpu",
    "cores",
    "fit",
    "container",
    "location",
    "application",
    "fail",
    "application",
    "fail",
    "requested",
    "memory",
    "combination",
    "memory",
    "cpu",
    "cores",
    "maximum",
    "container",
    "size",
    "look",
    "yarn",
    "tangling",
    "website",
    "mentioned",
    "look",
    "second",
    "blog",
    "series",
    "explains",
    "allocations",
    "discuss",
    "hive",
    "peg",
    "hbase",
    "components",
    "used",
    "industry",
    "various",
    "use",
    "cases",
    "let",
    "look",
    "questions",
    "let",
    "look",
    "prepare",
    "first",
    "learn",
    "hive",
    "data",
    "warehousing",
    "package",
    "question",
    "different",
    "components",
    "hive",
    "architecture",
    "talk",
    "hive",
    "already",
    "know",
    "hive",
    "data",
    "warehousing",
    "package",
    "basically",
    "allows",
    "work",
    "structured",
    "data",
    "data",
    "structuralized",
    "normally",
    "people",
    "well",
    "versed",
    "querying",
    "basically",
    "processing",
    "data",
    "using",
    "sql",
    "queries",
    "lot",
    "people",
    "come",
    "database",
    "backgrounds",
    "would",
    "find",
    "comfortable",
    "know",
    "structured",
    "query",
    "language",
    "hive",
    "one",
    "data",
    "warehousing",
    "package",
    "resides",
    "within",
    "hadoop",
    "ecosystem",
    "uses",
    "hadoop",
    "distributed",
    "file",
    "system",
    "store",
    "data",
    "uses",
    "rdbms",
    "usually",
    "store",
    "metadata",
    "although",
    "metadata",
    "stored",
    "locally",
    "also",
    "different",
    "components",
    "hive",
    "architecture",
    "user",
    "interface",
    "user",
    "interface",
    "calls",
    "execute",
    "interface",
    "driver",
    "creates",
    "session",
    "query",
    "sends",
    "query",
    "compiler",
    "generate",
    "execution",
    "plan",
    "usually",
    "whenever",
    "hive",
    "set",
    "would",
    "metadata",
    "stored",
    "rdbms",
    "establish",
    "connection",
    "rdbms",
    "hadoop",
    "need",
    "odbc",
    "jdbc",
    "connector",
    "jar",
    "file",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "driver",
    "class",
    "mandatory",
    "create",
    "connection",
    "hive",
    "hadoop",
    "user",
    "interface",
    "creates",
    "interface",
    "using",
    "driver",
    "metastore",
    "metastore",
    "stores",
    "metadata",
    "information",
    "object",
    "create",
    "database",
    "table",
    "indexes",
    "metadata",
    "stored",
    "metastore",
    "usually",
    "meta",
    "store",
    "stored",
    "rdbms",
    "multiple",
    "users",
    "connect",
    "hive",
    "metastore",
    "stores",
    "metadata",
    "information",
    "sends",
    "compiler",
    "execution",
    "query",
    "compiler",
    "generates",
    "execution",
    "plan",
    "dag",
    "dag",
    "stands",
    "direct",
    "cyclic",
    "graph",
    "dag",
    "stages",
    "stage",
    "either",
    "metadata",
    "operation",
    "map",
    "reduced",
    "job",
    "operation",
    "sdfs",
    "finally",
    "execution",
    "engine",
    "acts",
    "bridge",
    "hive",
    "hadoop",
    "process",
    "query",
    "execution",
    "engine",
    "communicates",
    "metastore",
    "perform",
    "operations",
    "like",
    "create",
    "drop",
    "tables",
    "four",
    "important",
    "components",
    "hive",
    "architecture",
    "difference",
    "external",
    "table",
    "manage",
    "table",
    "height",
    "various",
    "kinds",
    "table",
    "height",
    "external",
    "table",
    "manage",
    "table",
    "partition",
    "table",
    "major",
    "difference",
    "managed",
    "external",
    "table",
    "respect",
    "happens",
    "data",
    "table",
    "dropped",
    "usually",
    "whenever",
    "create",
    "table",
    "hive",
    "creates",
    "managed",
    "table",
    "could",
    "also",
    "call",
    "internal",
    "table",
    "manages",
    "data",
    "moves",
    "warehouse",
    "directory",
    "default",
    "whether",
    "create",
    "manage",
    "table",
    "external",
    "table",
    "usually",
    "data",
    "reside",
    "hive",
    "default",
    "warehouse",
    "directory",
    "could",
    "residing",
    "location",
    "chosen",
    "however",
    "talk",
    "manage",
    "table",
    "one",
    "drops",
    "managed",
    "table",
    "metadata",
    "information",
    "deleted",
    "table",
    "data",
    "also",
    "deleted",
    "dfs",
    "talk",
    "external",
    "table",
    "created",
    "external",
    "keyword",
    "explicitly",
    "external",
    "table",
    "dropped",
    "nothing",
    "happens",
    "data",
    "resides",
    "sdfs",
    "main",
    "difference",
    "managed",
    "external",
    "table",
    "might",
    "use",
    "case",
    "somebody",
    "asks",
    "might",
    "migration",
    "kind",
    "activity",
    "interested",
    "creating",
    "lot",
    "tables",
    "using",
    "queries",
    "case",
    "could",
    "dump",
    "data",
    "sdfs",
    "could",
    "create",
    "table",
    "pointing",
    "particular",
    "directory",
    "multiple",
    "directories",
    "could",
    "testing",
    "tables",
    "would",
    "decide",
    "might",
    "need",
    "tables",
    "case",
    "would",
    "advisable",
    "create",
    "external",
    "tables",
    "even",
    "table",
    "later",
    "dropped",
    "data",
    "sdfs",
    "intact",
    "unlike",
    "manage",
    "table",
    "dropping",
    "table",
    "delete",
    "data",
    "sdfs",
    "also",
    "let",
    "learn",
    "little",
    "bit",
    "partition",
    "partition",
    "height",
    "partitioning",
    "required",
    "hive",
    "somebody",
    "asks",
    "normally",
    "world",
    "rdbms",
    "partition",
    "process",
    "group",
    "similar",
    "type",
    "data",
    "together",
    "usually",
    "done",
    "basis",
    "column",
    "call",
    "partitioning",
    "key",
    "table",
    "usually",
    "one",
    "column",
    "context",
    "rdbms",
    "could",
    "used",
    "partition",
    "data",
    "avoid",
    "scanning",
    "complete",
    "table",
    "query",
    "restrict",
    "scan",
    "set",
    "data",
    "particular",
    "partition",
    "hive",
    "number",
    "partition",
    "keys",
    "partitioning",
    "provides",
    "granularity",
    "hive",
    "table",
    "reduces",
    "query",
    "latency",
    "scanning",
    "relevant",
    "partition",
    "data",
    "instead",
    "whole",
    "data",
    "set",
    "partition",
    "various",
    "levels",
    "compare",
    "rdbms",
    "hive",
    "case",
    "rdbms",
    "could",
    "one",
    "column",
    "could",
    "used",
    "partitioning",
    "could",
    "squaring",
    "specific",
    "partition",
    "case",
    "rdbms",
    "partition",
    "column",
    "usually",
    "part",
    "table",
    "definition",
    "example",
    "employee",
    "table",
    "might",
    "employee",
    "id",
    "employee",
    "name",
    "employee",
    "age",
    "employee",
    "salary",
    "four",
    "columns",
    "would",
    "decide",
    "partition",
    "table",
    "based",
    "salary",
    "column",
    "would",
    "partition",
    "feel",
    "employee",
    "table",
    "growing",
    "fast",
    "huge",
    "amount",
    "data",
    "later",
    "query",
    "table",
    "want",
    "scan",
    "complete",
    "table",
    "could",
    "split",
    "data",
    "multiple",
    "partition",
    "based",
    "salary",
    "column",
    "giving",
    "ranges",
    "hive",
    "little",
    "different",
    "hive",
    "partitioning",
    "concept",
    "static",
    "dynamic",
    "partitioning",
    "hive",
    "partition",
    "column",
    "part",
    "table",
    "definition",
    "might",
    "employee",
    "table",
    "employee",
    "id",
    "name",
    "age",
    "would",
    "table",
    "definition",
    "could",
    "partitioning",
    "done",
    "based",
    "salary",
    "column",
    "create",
    "specific",
    "folder",
    "sdfs",
    "case",
    "query",
    "data",
    "see",
    "partition",
    "column",
    "also",
    "showing",
    "partition",
    "transaction",
    "data",
    "bank",
    "example",
    "based",
    "month",
    "like",
    "chan",
    "feb",
    "etc",
    "operation",
    "regarding",
    "particular",
    "month",
    "allow",
    "us",
    "query",
    "particular",
    "folder",
    "partitioning",
    "useful",
    "hive",
    "store",
    "metadata",
    "information",
    "sdfs",
    "somebody",
    "asks",
    "know",
    "hives",
    "data",
    "stored",
    "sdfs",
    "hadoop",
    "distributed",
    "file",
    "system",
    "however",
    "metadata",
    "either",
    "stored",
    "locally",
    "mode",
    "hive",
    "would",
    "called",
    "embedded",
    "mode",
    "could",
    "hives",
    "metadata",
    "stored",
    "rdbms",
    "multiple",
    "clients",
    "initiate",
    "connection",
    "metadata",
    "important",
    "hive",
    "would",
    "stored",
    "sdfs",
    "already",
    "know",
    "sdfs",
    "read",
    "write",
    "operations",
    "time",
    "consuming",
    "distributed",
    "file",
    "system",
    "accommodate",
    "huge",
    "amount",
    "data",
    "hive",
    "stores",
    "metadata",
    "information",
    "metastore",
    "using",
    "rdbms",
    "instead",
    "sdfs",
    "allows",
    "achieve",
    "low",
    "latency",
    "faster",
    "data",
    "access",
    "somebody",
    "asks",
    "components",
    "used",
    "hive",
    "query",
    "processor",
    "usually",
    "main",
    "components",
    "parser",
    "execution",
    "engine",
    "logical",
    "plan",
    "generation",
    "optimizer",
    "type",
    "checking",
    "whenever",
    "query",
    "submitted",
    "go",
    "parser",
    "parser",
    "would",
    "check",
    "syntax",
    "would",
    "check",
    "objects",
    "queried",
    "things",
    "see",
    "query",
    "fine",
    "internally",
    "semantic",
    "analyzer",
    "also",
    "look",
    "query",
    "execution",
    "engine",
    "basically",
    "work",
    "execution",
    "part",
    "best",
    "generated",
    "execution",
    "plan",
    "could",
    "used",
    "get",
    "results",
    "query",
    "could",
    "also",
    "user",
    "defined",
    "functions",
    "user",
    "would",
    "want",
    "use",
    "normally",
    "created",
    "java",
    "java",
    "programming",
    "language",
    "basically",
    "user",
    "defined",
    "functions",
    "added",
    "class",
    "path",
    "would",
    "logical",
    "plan",
    "generation",
    "basically",
    "looks",
    "query",
    "generates",
    "logical",
    "plan",
    "best",
    "execution",
    "path",
    "would",
    "required",
    "get",
    "results",
    "internally",
    "physical",
    "plan",
    "generated",
    "looked",
    "optimizer",
    "get",
    "best",
    "path",
    "get",
    "data",
    "might",
    "also",
    "checking",
    "different",
    "operators",
    "using",
    "within",
    "query",
    "finally",
    "would",
    "also",
    "type",
    "checking",
    "important",
    "components",
    "hype",
    "somebody",
    "might",
    "ask",
    "querying",
    "data",
    "using",
    "hive",
    "different",
    "components",
    "involved",
    "could",
    "explain",
    "different",
    "components",
    "work",
    "query",
    "submitted",
    "components",
    "let",
    "look",
    "scenario",
    "based",
    "question",
    "suppose",
    "lot",
    "small",
    "csv",
    "files",
    "present",
    "sdfs",
    "directory",
    "want",
    "create",
    "single",
    "hive",
    "table",
    "files",
    "data",
    "files",
    "fields",
    "like",
    "registration",
    "number",
    "name",
    "email",
    "address",
    "needs",
    "done",
    "approach",
    "solve",
    "create",
    "single",
    "hive",
    "table",
    "lots",
    "small",
    "files",
    "without",
    "degrading",
    "performance",
    "system",
    "different",
    "approaches",
    "know",
    "lot",
    "small",
    "csv",
    "files",
    "present",
    "directory",
    "know",
    "create",
    "table",
    "hive",
    "use",
    "location",
    "parameter",
    "could",
    "say",
    "create",
    "table",
    "give",
    "table",
    "name",
    "give",
    "column",
    "data",
    "types",
    "could",
    "specify",
    "delimiters",
    "finally",
    "could",
    "say",
    "location",
    "point",
    "directory",
    "sdfs",
    "directory",
    "might",
    "directory",
    "lot",
    "csv",
    "files",
    "case",
    "avoid",
    "loading",
    "data",
    "table",
    "table",
    "point",
    "table",
    "pointing",
    "directory",
    "directly",
    "pick",
    "data",
    "one",
    "multiple",
    "files",
    "also",
    "know",
    "hive",
    "schema",
    "check",
    "read",
    "schema",
    "check",
    "write",
    "case",
    "one",
    "two",
    "files",
    "follow",
    "schema",
    "table",
    "would",
    "prevent",
    "data",
    "loading",
    "data",
    "would",
    "anyways",
    "loaded",
    "query",
    "data",
    "might",
    "show",
    "null",
    "values",
    "data",
    "loaded",
    "follow",
    "schema",
    "table",
    "one",
    "approach",
    "approach",
    "let",
    "look",
    "think",
    "sequence",
    "file",
    "format",
    "basically",
    "smart",
    "format",
    "binary",
    "format",
    "group",
    "small",
    "files",
    "together",
    "form",
    "sequence",
    "file",
    "could",
    "one",
    "smarter",
    "approach",
    "could",
    "create",
    "temporary",
    "table",
    "could",
    "say",
    "create",
    "table",
    "give",
    "table",
    "name",
    "give",
    "column",
    "names",
    "data",
    "types",
    "could",
    "specify",
    "delimiters",
    "shows",
    "row",
    "format",
    "fields",
    "terminated",
    "finally",
    "store",
    "text",
    "file",
    "load",
    "data",
    "table",
    "giving",
    "local",
    "file",
    "system",
    "path",
    "create",
    "table",
    "store",
    "data",
    "sequence",
    "file",
    "format",
    "point",
    "one",
    "storing",
    "data",
    "text",
    "text",
    "file",
    "point",
    "three",
    "would",
    "storing",
    "data",
    "sequence",
    "file",
    "format",
    "say",
    "create",
    "table",
    "give",
    "specifications",
    "say",
    "row",
    "format",
    "delimited",
    "fields",
    "terminated",
    "comma",
    "stored",
    "sequence",
    "file",
    "move",
    "data",
    "test",
    "table",
    "test",
    "sequence",
    "file",
    "table",
    "could",
    "say",
    "insert",
    "overwrite",
    "new",
    "table",
    "select",
    "star",
    "table",
    "remember",
    "hive",
    "insert",
    "update",
    "delete",
    "however",
    "table",
    "existing",
    "insert",
    "overwrite",
    "existing",
    "table",
    "new",
    "table",
    "could",
    "one",
    "approach",
    "could",
    "lot",
    "csv",
    "files",
    "smaller",
    "files",
    "club",
    "together",
    "one",
    "big",
    "sequence",
    "file",
    "store",
    "table",
    "somebody",
    "asks",
    "write",
    "query",
    "insert",
    "new",
    "column",
    "integer",
    "data",
    "type",
    "hive",
    "table",
    "requirement",
    "might",
    "would",
    "want",
    "insert",
    "table",
    "position",
    "existing",
    "column",
    "possible",
    "alter",
    "table",
    "giving",
    "table",
    "name",
    "specifying",
    "change",
    "column",
    "giving",
    "new",
    "column",
    "data",
    "type",
    "existing",
    "column",
    "simple",
    "way",
    "wherein",
    "insert",
    "new",
    "column",
    "hive",
    "table",
    "key",
    "differences",
    "hive",
    "pig",
    "might",
    "heard",
    "high",
    "visit",
    "data",
    "housing",
    "package",
    "pig",
    "scripting",
    "language",
    "used",
    "data",
    "analysis",
    "trend",
    "detection",
    "hypothesis",
    "testing",
    "data",
    "transformation",
    "many",
    "use",
    "cases",
    "compare",
    "hive",
    "pig",
    "hive",
    "uses",
    "declarative",
    "language",
    "called",
    "hive",
    "ql",
    "hive",
    "querying",
    "language",
    "similar",
    "sql",
    "reporting",
    "data",
    "analysis",
    "even",
    "data",
    "transformation",
    "data",
    "extraction",
    "pig",
    "uses",
    "high",
    "level",
    "procedural",
    "language",
    "called",
    "pig",
    "latin",
    "programming",
    "remember",
    "use",
    "mapreduce",
    "processing",
    "framework",
    "run",
    "query",
    "hive",
    "process",
    "data",
    "create",
    "submit",
    "pick",
    "script",
    "trigger",
    "mapreduce",
    "job",
    "unless",
    "set",
    "local",
    "mode",
    "hive",
    "operates",
    "server",
    "side",
    "cluster",
    "basically",
    "works",
    "structured",
    "data",
    "data",
    "structuralized",
    "pig",
    "usually",
    "works",
    "operates",
    "client",
    "side",
    "cluster",
    "allows",
    "structured",
    "unstructured",
    "even",
    "could",
    "say",
    "data",
    "hive",
    "support",
    "avro",
    "file",
    "format",
    "default",
    "however",
    "done",
    "using",
    "write",
    "serializer",
    "serializer",
    "hive",
    "table",
    "related",
    "data",
    "stored",
    "avro",
    "format",
    "sequence",
    "file",
    "format",
    "part",
    "k",
    "format",
    "even",
    "text",
    "file",
    "format",
    "however",
    "working",
    "smarter",
    "formats",
    "like",
    "avro",
    "sequence",
    "file",
    "par",
    "k",
    "might",
    "use",
    "specific",
    "serializers",
    "serializers",
    "avro",
    "package",
    "allows",
    "us",
    "use",
    "avro",
    "format",
    "pig",
    "supports",
    "avro",
    "format",
    "default",
    "hive",
    "developed",
    "facebook",
    "supports",
    "partitioning",
    "pig",
    "developed",
    "yahoo",
    "support",
    "partitioning",
    "high",
    "level",
    "differences",
    "lots",
    "lots",
    "differences",
    "remember",
    "hive",
    "data",
    "housing",
    "package",
    "pig",
    "scripting",
    "language",
    "strictly",
    "procedural",
    "flow",
    "following",
    "scripting",
    "language",
    "allows",
    "us",
    "process",
    "data",
    "let",
    "get",
    "let",
    "get",
    "deeper",
    "learn",
    "pig",
    "mentioned",
    "scripting",
    "language",
    "used",
    "data",
    "processing",
    "also",
    "uses",
    "mapreduce",
    "although",
    "even",
    "pig",
    "run",
    "local",
    "mode",
    "let",
    "learn",
    "pig",
    "next",
    "section",
    "let",
    "learn",
    "questions",
    "pig",
    "scripting",
    "language",
    "extensively",
    "used",
    "data",
    "processing",
    "data",
    "analysis",
    "question",
    "apache",
    "pig",
    "different",
    "mapreduce",
    "know",
    "mapreduce",
    "programming",
    "model",
    "quite",
    "rigid",
    "comes",
    "processing",
    "data",
    "mapping",
    "reducing",
    "write",
    "huge",
    "code",
    "usually",
    "mapreduce",
    "written",
    "java",
    "also",
    "written",
    "python",
    "written",
    "scala",
    "another",
    "programming",
    "languages",
    "compare",
    "pig",
    "mapreduce",
    "pig",
    "obviously",
    "concise",
    "less",
    "lines",
    "code",
    "compared",
    "mapreduce",
    "also",
    "know",
    "pig",
    "script",
    "internally",
    "trigger",
    "mapreduce",
    "job",
    "however",
    "user",
    "need",
    "know",
    "mapreduce",
    "programming",
    "model",
    "simply",
    "write",
    "simple",
    "scripts",
    "pig",
    "automatically",
    "converted",
    "mapreduce",
    "however",
    "mapreduce",
    "lines",
    "code",
    "peak",
    "high",
    "level",
    "language",
    "easily",
    "perform",
    "join",
    "operations",
    "data",
    "processing",
    "operations",
    "mapreduce",
    "low",
    "level",
    "language",
    "perform",
    "job",
    "join",
    "operations",
    "easily",
    "join",
    "using",
    "mapreduce",
    "however",
    "really",
    "easy",
    "comparison",
    "pick",
    "said",
    "execution",
    "every",
    "pig",
    "operator",
    "converted",
    "internally",
    "mapreduce",
    "job",
    "every",
    "pick",
    "script",
    "run",
    "would",
    "converted",
    "mapreduce",
    "job",
    "map",
    "reduce",
    "overall",
    "batch",
    "oriented",
    "processing",
    "takes",
    "time",
    "compile",
    "takes",
    "time",
    "execute",
    "either",
    "run",
    "mapreduce",
    "job",
    "triggered",
    "pingscript",
    "pig",
    "works",
    "versions",
    "hadoop",
    "talk",
    "mapreduce",
    "program",
    "written",
    "one",
    "hadoop",
    "version",
    "may",
    "work",
    "versions",
    "might",
    "work",
    "might",
    "depends",
    "dependencies",
    "compiler",
    "using",
    "programming",
    "language",
    "used",
    "version",
    "hadoop",
    "working",
    "main",
    "differences",
    "apache",
    "pig",
    "mapreduce",
    "different",
    "ways",
    "executing",
    "pig",
    "script",
    "could",
    "create",
    "script",
    "file",
    "store",
    "dot",
    "pic",
    "dot",
    "text",
    "could",
    "execute",
    "using",
    "pic",
    "command",
    "could",
    "bringing",
    "grunt",
    "shell",
    "pig",
    "shell",
    "usually",
    "starts",
    "mapreduce",
    "mode",
    "also",
    "bring",
    "local",
    "mode",
    "also",
    "run",
    "pic",
    "embedded",
    "embedded",
    "script",
    "programming",
    "language",
    "different",
    "ways",
    "executing",
    "big",
    "script",
    "major",
    "components",
    "pig",
    "execution",
    "environment",
    "common",
    "question",
    "interviewers",
    "would",
    "always",
    "want",
    "know",
    "different",
    "components",
    "hive",
    "different",
    "components",
    "pig",
    "even",
    "different",
    "components",
    "involved",
    "hadoop",
    "ecosystem",
    "want",
    "learn",
    "major",
    "components",
    "big",
    "execution",
    "environment",
    "pig",
    "scripts",
    "written",
    "pig",
    "latin",
    "using",
    "operators",
    "functions",
    "submitted",
    "execution",
    "environment",
    "happens",
    "would",
    "want",
    "process",
    "data",
    "using",
    "pic",
    "parser",
    "type",
    "checking",
    "checks",
    "syntax",
    "script",
    "output",
    "parser",
    "tag",
    "direct",
    "cyclic",
    "graph",
    "block",
    "wikipedia",
    "dag",
    "dag",
    "basically",
    "sequence",
    "steps",
    "run",
    "one",
    "direction",
    "optimizer",
    "optimizer",
    "performs",
    "optimization",
    "using",
    "merge",
    "transform",
    "split",
    "etc",
    "aims",
    "reduce",
    "amount",
    "data",
    "pipeline",
    "whole",
    "purpose",
    "optimizer",
    "internal",
    "compiler",
    "pic",
    "compiler",
    "converts",
    "optimized",
    "code",
    "mapreduce",
    "job",
    "user",
    "need",
    "know",
    "mapreduce",
    "programming",
    "model",
    "works",
    "written",
    "need",
    "know",
    "running",
    "big",
    "script",
    "would",
    "internally",
    "converted",
    "mapreduce",
    "job",
    "finally",
    "execution",
    "engine",
    "mapreduce",
    "jobs",
    "submitted",
    "execution",
    "engine",
    "generate",
    "desired",
    "results",
    "major",
    "components",
    "pig",
    "execution",
    "environment",
    "let",
    "learn",
    "different",
    "complex",
    "data",
    "types",
    "pig",
    "supports",
    "various",
    "data",
    "types",
    "main",
    "ones",
    "tuple",
    "bag",
    "map",
    "tuple",
    "tuple",
    "might",
    "heard",
    "tuple",
    "ordered",
    "set",
    "fields",
    "contain",
    "different",
    "data",
    "types",
    "field",
    "array",
    "would",
    "multiple",
    "elements",
    "would",
    "types",
    "list",
    "also",
    "different",
    "types",
    "tuple",
    "collection",
    "different",
    "fields",
    "field",
    "different",
    "type",
    "could",
    "example",
    "one",
    "comma",
    "three",
    "one",
    "comma",
    "three",
    "comma",
    "string",
    "float",
    "element",
    "form",
    "tuple",
    "bag",
    "set",
    "tuples",
    "represented",
    "curly",
    "braces",
    "could",
    "also",
    "imagine",
    "like",
    "dictionary",
    "various",
    "different",
    "collection",
    "elements",
    "map",
    "map",
    "set",
    "key",
    "value",
    "pairs",
    "used",
    "represent",
    "data",
    "work",
    "big",
    "data",
    "field",
    "need",
    "know",
    "different",
    "data",
    "types",
    "supported",
    "pig",
    "supported",
    "hive",
    "supported",
    "components",
    "hadoop",
    "tuple",
    "bag",
    "map",
    "array",
    "array",
    "buffer",
    "think",
    "list",
    "think",
    "dictionaries",
    "think",
    "map",
    "key",
    "value",
    "pair",
    "different",
    "complex",
    "data",
    "types",
    "primitive",
    "data",
    "type",
    "integer",
    "character",
    "string",
    "boolean",
    "float",
    "various",
    "diagnostic",
    "operators",
    "available",
    "apache",
    "pic",
    "operators",
    "options",
    "give",
    "pic",
    "script",
    "dumb",
    "dumb",
    "operator",
    "runs",
    "pig",
    "latin",
    "scripts",
    "displays",
    "result",
    "screen",
    "either",
    "could",
    "dumb",
    "see",
    "output",
    "screen",
    "even",
    "dump",
    "could",
    "store",
    "output",
    "particular",
    "file",
    "load",
    "data",
    "using",
    "load",
    "operator",
    "pick",
    "pico",
    "also",
    "different",
    "internal",
    "storage",
    "like",
    "json",
    "loader",
    "big",
    "storage",
    "used",
    "working",
    "specific",
    "kind",
    "data",
    "could",
    "dump",
    "either",
    "processing",
    "processing",
    "dump",
    "would",
    "produce",
    "result",
    "result",
    "could",
    "stored",
    "file",
    "seen",
    "screen",
    "also",
    "describe",
    "operator",
    "used",
    "view",
    "schema",
    "relation",
    "load",
    "data",
    "view",
    "schema",
    "relation",
    "using",
    "describe",
    "operator",
    "explain",
    "might",
    "already",
    "know",
    "displays",
    "physical",
    "logical",
    "mapreduce",
    "execution",
    "plans",
    "normally",
    "rdbms",
    "use",
    "would",
    "like",
    "see",
    "happens",
    "behind",
    "scenes",
    "particular",
    "script",
    "query",
    "runs",
    "could",
    "load",
    "data",
    "using",
    "load",
    "operator",
    "case",
    "would",
    "want",
    "display",
    "logical",
    "physical",
    "mapreduce",
    "execution",
    "plans",
    "could",
    "use",
    "explain",
    "operator",
    "also",
    "illustrate",
    "operator",
    "gives",
    "execution",
    "sequence",
    "statements",
    "sometimes",
    "would",
    "want",
    "analyze",
    "script",
    "see",
    "good",
    "bad",
    "would",
    "really",
    "serve",
    "purpose",
    "could",
    "use",
    "illustrate",
    "test",
    "loading",
    "data",
    "using",
    "load",
    "operator",
    "could",
    "use",
    "illustrate",
    "operator",
    "look",
    "execution",
    "sequence",
    "statements",
    "would",
    "want",
    "execute",
    "different",
    "diagnostic",
    "operators",
    "available",
    "apache",
    "pic",
    "somebody",
    "asks",
    "state",
    "usage",
    "group",
    "order",
    "distinct",
    "keywords",
    "picscript",
    "said",
    "pig",
    "scripting",
    "language",
    "could",
    "use",
    "various",
    "operators",
    "group",
    "basically",
    "collects",
    "various",
    "records",
    "key",
    "groups",
    "data",
    "one",
    "relations",
    "example",
    "could",
    "group",
    "data",
    "basically",
    "variable",
    "give",
    "name",
    "say",
    "group",
    "relation",
    "name",
    "h",
    "say",
    "file",
    "field",
    "various",
    "fields",
    "one",
    "field",
    "relation",
    "name",
    "could",
    "group",
    "different",
    "field",
    "order",
    "used",
    "display",
    "contents",
    "relation",
    "sorted",
    "order",
    "whether",
    "ascending",
    "descending",
    "could",
    "create",
    "variable",
    "called",
    "relation",
    "2",
    "could",
    "say",
    "order",
    "relation",
    "name",
    "1",
    "ascending",
    "descending",
    "order",
    "distinct",
    "basically",
    "removes",
    "duplicate",
    "records",
    "implemented",
    "entire",
    "records",
    "individual",
    "records",
    "would",
    "like",
    "want",
    "find",
    "distinct",
    "values",
    "relation",
    "name",
    "field",
    "could",
    "use",
    "distinct",
    "relational",
    "operators",
    "pig",
    "various",
    "relational",
    "operators",
    "help",
    "data",
    "scientists",
    "data",
    "analysts",
    "developers",
    "analyzing",
    "data",
    "joins",
    "two",
    "tables",
    "performs",
    "group",
    "operation",
    "join",
    "table",
    "result",
    "cross",
    "used",
    "compute",
    "cross",
    "product",
    "cartesian",
    "product",
    "two",
    "relations",
    "basically",
    "iteration",
    "iterate",
    "tuples",
    "relation",
    "generating",
    "data",
    "transformation",
    "example",
    "say",
    "variable",
    "equals",
    "load",
    "file",
    "could",
    "create",
    "variable",
    "called",
    "b",
    "could",
    "say",
    "would",
    "want",
    "something",
    "say",
    "group",
    "join",
    "join",
    "two",
    "tables",
    "relation",
    "limit",
    "limit",
    "number",
    "output",
    "tuples",
    "output",
    "results",
    "split",
    "split",
    "relation",
    "two",
    "relations",
    "union",
    "get",
    "combination",
    "merge",
    "contents",
    "two",
    "relations",
    "order",
    "get",
    "sorted",
    "result",
    "relational",
    "operators",
    "extensively",
    "used",
    "pig",
    "analysis",
    "use",
    "filters",
    "apache",
    "pic",
    "say",
    "example",
    "data",
    "three",
    "fields",
    "year",
    "product",
    "quantity",
    "phone",
    "sales",
    "data",
    "filter",
    "operator",
    "could",
    "used",
    "select",
    "required",
    "values",
    "relation",
    "based",
    "condition",
    "also",
    "allows",
    "remove",
    "unwanted",
    "records",
    "data",
    "file",
    "example",
    "filter",
    "products",
    "quantity",
    "greater",
    "thousand",
    "see",
    "one",
    "row",
    "wherein",
    "multiple",
    "rows",
    "quantity",
    "greater",
    "thousand",
    "1500",
    "1700",
    "1200",
    "could",
    "create",
    "variable",
    "called",
    "would",
    "load",
    "file",
    "using",
    "pix",
    "storage",
    "explained",
    "earlier",
    "pick",
    "storage",
    "internal",
    "parameter",
    "used",
    "specify",
    "delimiters",
    "delimiter",
    "comma",
    "could",
    "say",
    "using",
    "pick",
    "storage",
    "could",
    "specify",
    "data",
    "type",
    "field",
    "integer",
    "product",
    "character",
    "array",
    "quantity",
    "integer",
    "b",
    "could",
    "say",
    "filter",
    "whatever",
    "quantity",
    "greater",
    "thousand",
    "concise",
    "simple",
    "allows",
    "us",
    "extract",
    "process",
    "data",
    "simpler",
    "way",
    "suppose",
    "file",
    "called",
    "150",
    "records",
    "dfs",
    "file",
    "stored",
    "dfs",
    "150",
    "records",
    "consider",
    "every",
    "record",
    "one",
    "line",
    "somebody",
    "asks",
    "write",
    "pick",
    "command",
    "retrieve",
    "first",
    "10",
    "records",
    "file",
    "first",
    "load",
    "data",
    "could",
    "create",
    "variable",
    "called",
    "test",
    "underscore",
    "data",
    "would",
    "say",
    "load",
    "file",
    "using",
    "pick",
    "storage",
    "specifying",
    "delimiter",
    "comma",
    "could",
    "specify",
    "fields",
    "whatever",
    "fields",
    "file",
    "would",
    "want",
    "get",
    "10",
    "records",
    "could",
    "use",
    "limit",
    "operator",
    "could",
    "say",
    "limit",
    "test",
    "data",
    "give",
    "10",
    "records",
    "simple",
    "extract",
    "10",
    "records",
    "150",
    "records",
    "stored",
    "file",
    "sdfs",
    "learned",
    "pig",
    "learned",
    "questions",
    "hive",
    "could",
    "always",
    "look",
    "books",
    "like",
    "programming",
    "hive",
    "programming",
    "pig",
    "look",
    "examples",
    "try",
    "examples",
    "existing",
    "hadoop",
    "setup",
    "let",
    "learn",
    "hbase",
    "nosql",
    "database",
    "hbase",
    "four",
    "dimensional",
    "database",
    "comparison",
    "rdbms",
    "usually",
    "two",
    "dimensional",
    "rdbms",
    "rows",
    "columns",
    "hbase",
    "four",
    "coordinates",
    "row",
    "key",
    "always",
    "unique",
    "column",
    "family",
    "number",
    "column",
    "qualifiers",
    "number",
    "per",
    "column",
    "family",
    "version",
    "four",
    "coordinates",
    "make",
    "edge",
    "base",
    "four",
    "dimensional",
    "key",
    "value",
    "store",
    "column",
    "family",
    "store",
    "unique",
    "storing",
    "huge",
    "amount",
    "data",
    "extracting",
    "data",
    "hbase",
    "good",
    "link",
    "would",
    "suggest",
    "everyone",
    "look",
    "would",
    "want",
    "learn",
    "hbase",
    "could",
    "say",
    "edge",
    "base",
    "mapper",
    "basically",
    "brings",
    "documentation",
    "mapr",
    "specific",
    "mapper",
    "look",
    "link",
    "give",
    "detailed",
    "explanation",
    "hbase",
    "works",
    "architectural",
    "components",
    "data",
    "stored",
    "makes",
    "hbase",
    "powerful",
    "nosql",
    "database",
    "let",
    "learn",
    "important",
    "critical",
    "questions",
    "hbase",
    "might",
    "asked",
    "interviewer",
    "interview",
    "applying",
    "big",
    "data",
    "admin",
    "developer",
    "position",
    "role",
    "key",
    "components",
    "hbase",
    "said",
    "one",
    "favorite",
    "questions",
    "interviewers",
    "would",
    "want",
    "understand",
    "knowledge",
    "different",
    "components",
    "particular",
    "service",
    "hbase",
    "said",
    "nosql",
    "database",
    "comes",
    "part",
    "service",
    "cloudera",
    "hortonworks",
    "apache",
    "hadoop",
    "could",
    "also",
    "set",
    "hbase",
    "independent",
    "package",
    "key",
    "components",
    "hbase",
    "hbase",
    "region",
    "server",
    "edge",
    "base",
    "follows",
    "similar",
    "kind",
    "topology",
    "like",
    "hadoop",
    "hadoop",
    "master",
    "process",
    "named",
    "node",
    "slave",
    "processes",
    "data",
    "nodes",
    "secondary",
    "name",
    "node",
    "way",
    "hbase",
    "also",
    "master",
    "h",
    "master",
    "slave",
    "processes",
    "called",
    "region",
    "servers",
    "region",
    "servers",
    "usually",
    "data",
    "nodes",
    "however",
    "mandatory",
    "100",
    "data",
    "nodes",
    "would",
    "100",
    "region",
    "servers",
    "purely",
    "depends",
    "admin",
    "region",
    "server",
    "contain",
    "region",
    "server",
    "contains",
    "hbase",
    "tables",
    "divided",
    "horizontally",
    "regions",
    "could",
    "say",
    "group",
    "rows",
    "called",
    "regions",
    "edge",
    "base",
    "two",
    "aspects",
    "one",
    "group",
    "columns",
    "called",
    "column",
    "family",
    "one",
    "group",
    "rows",
    "called",
    "regions",
    "regions",
    "rows",
    "grouped",
    "based",
    "key",
    "values",
    "would",
    "say",
    "row",
    "keys",
    "always",
    "unique",
    "store",
    "data",
    "base",
    "would",
    "data",
    "form",
    "rows",
    "columns",
    "group",
    "rows",
    "called",
    "regions",
    "could",
    "say",
    "horizontal",
    "partitions",
    "table",
    "region",
    "server",
    "manages",
    "regions",
    "node",
    "data",
    "node",
    "running",
    "region",
    "server",
    "thousand",
    "regions",
    "runs",
    "every",
    "node",
    "decides",
    "size",
    "region",
    "region",
    "server",
    "said",
    "slave",
    "process",
    "responsible",
    "managing",
    "hbase",
    "data",
    "node",
    "region",
    "server",
    "worker",
    "node",
    "worker",
    "process",
    "data",
    "node",
    "take",
    "care",
    "read",
    "write",
    "update",
    "delete",
    "request",
    "clients",
    "talk",
    "components",
    "edge",
    "base",
    "said",
    "hp",
    "h",
    "master",
    "would",
    "always",
    "connection",
    "coming",
    "client",
    "application",
    "hmaster",
    "assigns",
    "regions",
    "monitors",
    "region",
    "servers",
    "assigns",
    "regions",
    "region",
    "servers",
    "load",
    "balancing",
    "without",
    "help",
    "zookeeper",
    "talk",
    "components",
    "hbase",
    "three",
    "main",
    "components",
    "zookeeper",
    "hmaster",
    "region",
    "server",
    "region",
    "server",
    "slave",
    "process",
    "edge",
    "master",
    "master",
    "process",
    "takes",
    "care",
    "table",
    "operations",
    "assigning",
    "regions",
    "region",
    "servers",
    "taking",
    "care",
    "read",
    "write",
    "requests",
    "come",
    "client",
    "edgemaster",
    "taken",
    "help",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "whenever",
    "client",
    "wants",
    "read",
    "write",
    "change",
    "schema",
    "metadata",
    "operations",
    "contact",
    "hmaster",
    "edge",
    "master",
    "internally",
    "contact",
    "zookeeper",
    "could",
    "hbase",
    "setup",
    "also",
    "high",
    "availability",
    "mode",
    "could",
    "active",
    "edge",
    "master",
    "backup",
    "edge",
    "master",
    "would",
    "zookeeper",
    "quorum",
    "way",
    "zookeeper",
    "works",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "always",
    "run",
    "quorum",
    "processes",
    "zookeeper",
    "would",
    "always",
    "run",
    "odd",
    "number",
    "processes",
    "3",
    "5",
    "7",
    "zookeeper",
    "works",
    "concept",
    "majority",
    "consensus",
    "zookeeper",
    "centralized",
    "coordination",
    "service",
    "keeping",
    "track",
    "servers",
    "alive",
    "available",
    "also",
    "keeps",
    "track",
    "status",
    "every",
    "server",
    "zookeeper",
    "monitoring",
    "zookeeper",
    "keeps",
    "session",
    "alive",
    "particular",
    "server",
    "h",
    "master",
    "would",
    "always",
    "check",
    "zookeeper",
    "region",
    "servers",
    "available",
    "alive",
    "regions",
    "assigned",
    "region",
    "server",
    "one",
    "end",
    "region",
    "server",
    "sending",
    "status",
    "zookeeper",
    "indicating",
    "ready",
    "kind",
    "read",
    "write",
    "operation",
    "end",
    "edge",
    "master",
    "querying",
    "zookeeper",
    "check",
    "status",
    "zookeeper",
    "internally",
    "manages",
    "meta",
    "table",
    "meta",
    "table",
    "information",
    "regions",
    "residing",
    "region",
    "server",
    "row",
    "keys",
    "regions",
    "contain",
    "case",
    "read",
    "activity",
    "hmaster",
    "query",
    "zookeeper",
    "find",
    "region",
    "server",
    "contains",
    "meta",
    "table",
    "edge",
    "master",
    "gets",
    "information",
    "meta",
    "table",
    "look",
    "meta",
    "table",
    "find",
    "row",
    "keys",
    "corresponding",
    "region",
    "servers",
    "contain",
    "regions",
    "row",
    "keys",
    "would",
    "want",
    "understand",
    "row",
    "key",
    "column",
    "families",
    "hbase",
    "let",
    "look",
    "would",
    "good",
    "could",
    "look",
    "excel",
    "sheet",
    "row",
    "key",
    "always",
    "unique",
    "acts",
    "primary",
    "key",
    "h",
    "base",
    "table",
    "allows",
    "logical",
    "grouping",
    "cells",
    "make",
    "sure",
    "cells",
    "row",
    "key",
    "server",
    "said",
    "four",
    "coordinates",
    "hbase",
    "row",
    "key",
    "always",
    "unique",
    "column",
    "families",
    "nothing",
    "group",
    "columns",
    "say",
    "column",
    "families",
    "one",
    "column",
    "family",
    "number",
    "columns",
    "talk",
    "h",
    "base",
    "h",
    "base",
    "four",
    "dimensional",
    "terms",
    "edge",
    "base",
    "also",
    "called",
    "column",
    "oriented",
    "database",
    "basically",
    "means",
    "every",
    "row",
    "one",
    "column",
    "could",
    "different",
    "data",
    "type",
    "row",
    "key",
    "uniquely",
    "identifies",
    "row",
    "column",
    "families",
    "could",
    "one",
    "many",
    "depending",
    "table",
    "defined",
    "column",
    "family",
    "number",
    "columns",
    "could",
    "say",
    "every",
    "row",
    "within",
    "column",
    "family",
    "could",
    "different",
    "number",
    "columns",
    "could",
    "say",
    "row",
    "one",
    "could",
    "two",
    "columns",
    "name",
    "city",
    "within",
    "column",
    "family",
    "row",
    "2",
    "could",
    "name",
    "city",
    "age",
    "designation",
    "salary",
    "third",
    "row",
    "could",
    "thousand",
    "columns",
    "could",
    "belong",
    "one",
    "column",
    "family",
    "horizontally",
    "scalable",
    "database",
    "column",
    "family",
    "consists",
    "group",
    "columns",
    "defined",
    "table",
    "creation",
    "column",
    "family",
    "number",
    "column",
    "qualifiers",
    "separated",
    "delimiter",
    "combination",
    "row",
    "key",
    "column",
    "family",
    "column",
    "qualifier",
    "name",
    "city",
    "age",
    "value",
    "within",
    "cell",
    "makes",
    "hbase",
    "unique",
    "four",
    "dimensional",
    "database",
    "information",
    "would",
    "want",
    "learn",
    "hbase",
    "please",
    "refer",
    "link",
    "hbase",
    "mapper",
    "gives",
    "complete",
    "hbase",
    "architecture",
    "three",
    "components",
    "name",
    "node",
    "three",
    "components",
    "name",
    "node",
    "region",
    "servers",
    "zookeeper",
    "works",
    "base",
    "edge",
    "master",
    "interacts",
    "zookeeper",
    "zookeeper",
    "coordination",
    "components",
    "working",
    "together",
    "hbs",
    "take",
    "care",
    "read",
    "write",
    "coming",
    "back",
    "continuing",
    "need",
    "disable",
    "table",
    "different",
    "table",
    "operations",
    "base",
    "one",
    "disabling",
    "table",
    "would",
    "want",
    "check",
    "status",
    "table",
    "could",
    "check",
    "disabled",
    "giving",
    "table",
    "name",
    "order",
    "enabled",
    "table",
    "name",
    "question",
    "need",
    "disable",
    "table",
    "would",
    "want",
    "modify",
    "table",
    "kind",
    "maintenance",
    "activity",
    "case",
    "disable",
    "table",
    "modify",
    "changes",
    "settings",
    "table",
    "disabled",
    "accessed",
    "scan",
    "command",
    "write",
    "code",
    "open",
    "connection",
    "base",
    "interact",
    "hbase",
    "one",
    "could",
    "either",
    "use",
    "graphical",
    "user",
    "interface",
    "hue",
    "could",
    "using",
    "command",
    "line",
    "hb",
    "shell",
    "could",
    "using",
    "hbase",
    "admin",
    "api",
    "working",
    "java",
    "say",
    "happy",
    "base",
    "working",
    "python",
    "may",
    "want",
    "open",
    "connection",
    "hbase",
    "work",
    "based",
    "programmatically",
    "case",
    "create",
    "configuration",
    "object",
    "configuration",
    "conf",
    "create",
    "configuration",
    "object",
    "use",
    "different",
    "classes",
    "like",
    "edge",
    "table",
    "interface",
    "work",
    "new",
    "table",
    "could",
    "use",
    "h",
    "column",
    "qualifier",
    "many",
    "classes",
    "available",
    "hbase",
    "admin",
    "api",
    "replication",
    "mean",
    "terms",
    "hbase",
    "edge",
    "base",
    "said",
    "works",
    "cluster",
    "way",
    "talk",
    "cluster",
    "could",
    "always",
    "set",
    "replication",
    "one",
    "hbase",
    "cluster",
    "hbase",
    "cluster",
    "replication",
    "feature",
    "edge",
    "base",
    "provides",
    "mechanism",
    "copy",
    "data",
    "clusters",
    "sync",
    "data",
    "different",
    "clusters",
    "feature",
    "used",
    "disaster",
    "recovery",
    "solution",
    "provides",
    "high",
    "availability",
    "hbase",
    "hbase",
    "cluster",
    "one",
    "one",
    "master",
    "multiple",
    "region",
    "servers",
    "running",
    "hadoop",
    "cluster",
    "could",
    "use",
    "hadoop",
    "cluster",
    "create",
    "hbase",
    "replica",
    "cluster",
    "could",
    "totally",
    "different",
    "hbase",
    "replica",
    "cluster",
    "intention",
    "things",
    "changing",
    "particular",
    "table",
    "cluster",
    "1",
    "would",
    "want",
    "replicated",
    "across",
    "different",
    "clusters",
    "could",
    "alter",
    "hbase",
    "table",
    "set",
    "replication",
    "scope",
    "one",
    "replication",
    "scope",
    "zero",
    "indicates",
    "table",
    "replicated",
    "set",
    "replication",
    "one",
    "basically",
    "set",
    "ah",
    "base",
    "cluster",
    "replicate",
    "edge",
    "base",
    "tables",
    "data",
    "cluster",
    "one",
    "cluster",
    "commands",
    "used",
    "enable",
    "replication",
    "replicate",
    "data",
    "table",
    "across",
    "clusters",
    "import",
    "export",
    "hbase",
    "course",
    "possible",
    "import",
    "export",
    "tables",
    "one",
    "hbase",
    "cluster",
    "hbase",
    "cluster",
    "even",
    "within",
    "cluster",
    "use",
    "hbase",
    "export",
    "utility",
    "comes",
    "particular",
    "package",
    "give",
    "table",
    "name",
    "target",
    "location",
    "export",
    "data",
    "hbase",
    "table",
    "directory",
    "sdfs",
    "could",
    "create",
    "different",
    "table",
    "would",
    "follow",
    "kind",
    "definition",
    "table",
    "exported",
    "could",
    "use",
    "import",
    "import",
    "data",
    "directory",
    "sdfs",
    "table",
    "would",
    "want",
    "learn",
    "hbase",
    "import",
    "export",
    "could",
    "look",
    "hbase",
    "import",
    "operations",
    "let",
    "search",
    "link",
    "link",
    "could",
    "learn",
    "hbase",
    "import",
    "export",
    "utilities",
    "could",
    "bulk",
    "import",
    "bulk",
    "export",
    "internally",
    "uses",
    "mapreduce",
    "could",
    "import",
    "export",
    "hbase",
    "tables",
    "moving",
    "mean",
    "compaction",
    "hbase",
    "know",
    "hbase",
    "nosql",
    "database",
    "used",
    "store",
    "huge",
    "amount",
    "data",
    "however",
    "whenever",
    "data",
    "written",
    "hbase",
    "first",
    "returned",
    "call",
    "right",
    "ahead",
    "log",
    "also",
    "mem",
    "store",
    "write",
    "cache",
    "data",
    "written",
    "wall",
    "mem",
    "store",
    "offloaded",
    "form",
    "internal",
    "hbase",
    "format",
    "file",
    "called",
    "h5",
    "usually",
    "edge",
    "files",
    "small",
    "nature",
    "also",
    "know",
    "sdfs",
    "good",
    "talk",
    "number",
    "larger",
    "files",
    "comparison",
    "large",
    "number",
    "smaller",
    "files",
    "due",
    "limitation",
    "name",
    "nodes",
    "memory",
    "compaction",
    "process",
    "merging",
    "hbase",
    "files",
    "smaller",
    "edge",
    "files",
    "single",
    "large",
    "file",
    "done",
    "reduce",
    "amount",
    "memory",
    "required",
    "store",
    "files",
    "number",
    "disk",
    "seeks",
    "needed",
    "could",
    "lot",
    "edge",
    "files",
    "get",
    "created",
    "data",
    "written",
    "hbase",
    "smaller",
    "files",
    "compacted",
    "major",
    "minor",
    "compaction",
    "creating",
    "one",
    "big",
    "edge",
    "file",
    "internally",
    "would",
    "written",
    "sdfs",
    "sdfs",
    "format",
    "blocks",
    "benefit",
    "compaction",
    "also",
    "feature",
    "called",
    "bloom",
    "filter",
    "bloom",
    "filter",
    "work",
    "bloom",
    "filter",
    "hbase",
    "bloom",
    "filter",
    "mechanism",
    "test",
    "whether",
    "h",
    "file",
    "contains",
    "specific",
    "row",
    "row",
    "column",
    "cell",
    "bloom",
    "filter",
    "named",
    "creator",
    "burton",
    "hovered",
    "bloom",
    "data",
    "structure",
    "predicts",
    "whether",
    "given",
    "element",
    "member",
    "set",
    "data",
    "provides",
    "index",
    "structure",
    "reduces",
    "disk",
    "reads",
    "determines",
    "probability",
    "finding",
    "row",
    "particular",
    "file",
    "one",
    "useful",
    "features",
    "hbase",
    "allows",
    "faster",
    "access",
    "avoids",
    "disk",
    "seeks",
    "hbase",
    "concept",
    "name",
    "space",
    "namespace",
    "similar",
    "elements",
    "grouped",
    "together",
    "namespace",
    "yes",
    "hb",
    "support",
    "name",
    "space",
    "namespace",
    "logical",
    "grouping",
    "tables",
    "analogous",
    "database",
    "rdbms",
    "create",
    "hbs",
    "namespace",
    "schema",
    "rdbms",
    "database",
    "could",
    "create",
    "namespace",
    "saying",
    "create",
    "namespace",
    "giving",
    "name",
    "could",
    "also",
    "list",
    "tables",
    "within",
    "namespace",
    "could",
    "create",
    "tables",
    "within",
    "specific",
    "namespace",
    "usually",
    "done",
    "production",
    "environment",
    "cluster",
    "might",
    "cluster",
    "might",
    "different",
    "users",
    "nosql",
    "database",
    "case",
    "admin",
    "would",
    "create",
    "specific",
    "namespace",
    "specific",
    "namespace",
    "would",
    "different",
    "directories",
    "htfs",
    "users",
    "particular",
    "business",
    "unit",
    "team",
    "work",
    "hbase",
    "objects",
    "within",
    "specific",
    "name",
    "space",
    "question",
    "important",
    "understand",
    "writes",
    "reads",
    "right",
    "ahead",
    "log",
    "wall",
    "help",
    "region",
    "server",
    "crashes",
    "said",
    "write",
    "happens",
    "happen",
    "mem",
    "store",
    "wall",
    "edit",
    "log",
    "write",
    "ahead",
    "log",
    "whenever",
    "write",
    "happens",
    "happen",
    "two",
    "places",
    "mem",
    "store",
    "right",
    "cache",
    "wall",
    "edit",
    "log",
    "data",
    "written",
    "places",
    "based",
    "limitation",
    "mem",
    "store",
    "data",
    "flushed",
    "create",
    "format",
    "file",
    "called",
    "files",
    "compacted",
    "created",
    "one",
    "bigger",
    "file",
    "stored",
    "sdfs",
    "sdfs",
    "data",
    "know",
    "stored",
    "form",
    "blocks",
    "underlying",
    "data",
    "nodes",
    "region",
    "server",
    "hosting",
    "mem",
    "store",
    "crashes",
    "region",
    "server",
    "running",
    "would",
    "data",
    "node",
    "data",
    "node",
    "crashes",
    "region",
    "server",
    "hosting",
    "mem",
    "store",
    "write",
    "cache",
    "crashes",
    "data",
    "memory",
    "data",
    "memory",
    "persisted",
    "lost",
    "hbase",
    "recover",
    "said",
    "data",
    "written",
    "wall",
    "mem",
    "store",
    "time",
    "hbase",
    "recovers",
    "writing",
    "wall",
    "write",
    "completes",
    "whenever",
    "write",
    "happens",
    "happens",
    "mem",
    "store",
    "wall",
    "time",
    "hbase",
    "cluster",
    "keeps",
    "wall",
    "record",
    "changes",
    "happen",
    "call",
    "also",
    "edit",
    "log",
    "hps",
    "goes",
    "node",
    "goes",
    "data",
    "flushed",
    "mem",
    "store",
    "edge",
    "file",
    "recovered",
    "replaying",
    "right",
    "ahead",
    "lock",
    "benefit",
    "edit",
    "log",
    "write",
    "ahead",
    "log",
    "would",
    "write",
    "hbs",
    "command",
    "list",
    "contents",
    "update",
    "column",
    "families",
    "table",
    "could",
    "scan",
    "would",
    "give",
    "complete",
    "data",
    "table",
    "specific",
    "would",
    "want",
    "look",
    "particular",
    "row",
    "could",
    "get",
    "table",
    "name",
    "give",
    "row",
    "key",
    "however",
    "could",
    "scan",
    "get",
    "complete",
    "data",
    "particular",
    "table",
    "could",
    "also",
    "describe",
    "see",
    "different",
    "column",
    "families",
    "would",
    "want",
    "alter",
    "table",
    "add",
    "new",
    "column",
    "family",
    "simple",
    "say",
    "alter",
    "give",
    "hvac",
    "table",
    "name",
    "give",
    "new",
    "column",
    "family",
    "name",
    "added",
    "table",
    "catalog",
    "tables",
    "base",
    "mentioned",
    "zookeeper",
    "knows",
    "location",
    "internal",
    "catalog",
    "table",
    "call",
    "meta",
    "table",
    "catalog",
    "tables",
    "edge",
    "base",
    "two",
    "tables",
    "one",
    "edge",
    "base",
    "meta",
    "table",
    "one",
    "hyphen",
    "root",
    "catalog",
    "table",
    "edge",
    "base",
    "meta",
    "exists",
    "hbase",
    "table",
    "filtered",
    "hbase",
    "shells",
    "list",
    "command",
    "give",
    "list",
    "command",
    "edge",
    "base",
    "would",
    "list",
    "tables",
    "space",
    "contains",
    "meta",
    "table",
    "internal",
    "table",
    "meta",
    "table",
    "keeps",
    "list",
    "regions",
    "system",
    "location",
    "hbase",
    "meta",
    "stored",
    "zookeeper",
    "somebody",
    "wants",
    "find",
    "look",
    "particular",
    "rows",
    "need",
    "know",
    "regions",
    "contain",
    "data",
    "regions",
    "located",
    "region",
    "server",
    "get",
    "information",
    "one",
    "look",
    "meta",
    "table",
    "however",
    "looking",
    "meta",
    "table",
    "directly",
    "would",
    "giving",
    "write",
    "read",
    "operation",
    "internally",
    "uh",
    "based",
    "master",
    "queries",
    "zookeeper",
    "zookeeper",
    "information",
    "meta",
    "table",
    "exists",
    "meta",
    "table",
    "existing",
    "region",
    "server",
    "contains",
    "information",
    "row",
    "keys",
    "region",
    "servers",
    "rows",
    "found",
    "root",
    "table",
    "keeps",
    "track",
    "location",
    "meta",
    "table",
    "hotspotting",
    "edge",
    "base",
    "avoid",
    "hot",
    "spotting",
    "common",
    "problem",
    "always",
    "admin",
    "guys",
    "guys",
    "managing",
    "infrastructure",
    "would",
    "think",
    "one",
    "main",
    "idea",
    "edge",
    "base",
    "would",
    "leveraging",
    "benefit",
    "sdfs",
    "read",
    "write",
    "requests",
    "uniformly",
    "distributed",
    "across",
    "regions",
    "region",
    "servers",
    "otherwise",
    "benefit",
    "distributed",
    "cluster",
    "would",
    "data",
    "stored",
    "across",
    "region",
    "servers",
    "form",
    "regions",
    "horizontal",
    "partitions",
    "table",
    "whenever",
    "read",
    "write",
    "requests",
    "happen",
    "uniformly",
    "distributed",
    "across",
    "regions",
    "region",
    "servers",
    "hot",
    "spotting",
    "occurs",
    "given",
    "region",
    "serviced",
    "region",
    "server",
    "receives",
    "read",
    "write",
    "request",
    "basically",
    "unbalanced",
    "way",
    "read",
    "write",
    "operations",
    "hotspot",
    "avoided",
    "designing",
    "row",
    "key",
    "way",
    "data",
    "written",
    "go",
    "multiple",
    "regions",
    "across",
    "cluster",
    "could",
    "techniques",
    "salting",
    "hashing",
    "reversing",
    "key",
    "many",
    "techniques",
    "employed",
    "users",
    "hbase",
    "need",
    "make",
    "sure",
    "regions",
    "distributed",
    "across",
    "region",
    "servers",
    "spread",
    "across",
    "region",
    "servers",
    "read",
    "write",
    "request",
    "satisfied",
    "different",
    "region",
    "servers",
    "parallel",
    "rather",
    "read",
    "write",
    "request",
    "hitting",
    "region",
    "server",
    "overloading",
    "region",
    "server",
    "may",
    "also",
    "lead",
    "crashing",
    "particular",
    "region",
    "server",
    "important",
    "questions",
    "hbase",
    "many",
    "please",
    "refer",
    "link",
    "specified",
    "discussion",
    "gives",
    "detailed",
    "explanation",
    "base",
    "works",
    "also",
    "look",
    "hp",
    "definitive",
    "guide",
    "hbase",
    "action",
    "really",
    "good",
    "books",
    "understand",
    "hbase",
    "internals",
    "works",
    "learned",
    "hive",
    "data",
    "warehousing",
    "package",
    "learnt",
    "pig",
    "scripting",
    "scripting",
    "language",
    "allows",
    "data",
    "analysis",
    "learned",
    "questions",
    "nosql",
    "database",
    "note",
    "225",
    "nosql",
    "databases",
    "existing",
    "market",
    "would",
    "want",
    "learn",
    "know",
    "nosql",
    "databases",
    "go",
    "google",
    "type",
    "sql",
    "databases",
    "org",
    "take",
    "link",
    "nosql",
    "databases",
    "shows",
    "225",
    "nosql",
    "databases",
    "existing",
    "market",
    "different",
    "use",
    "cases",
    "used",
    "different",
    "users",
    "different",
    "features",
    "look",
    "link",
    "talk",
    "data",
    "ingestion",
    "let",
    "look",
    "data",
    "ingestion",
    "one",
    "good",
    "link",
    "would",
    "suggest",
    "look",
    "lists",
    "around",
    "18",
    "different",
    "ingestion",
    "tools",
    "talk",
    "different",
    "data",
    "injection",
    "tools",
    "structured",
    "data",
    "streaming",
    "data",
    "data",
    "governance",
    "data",
    "ingestion",
    "transformation",
    "look",
    "link",
    "also",
    "gives",
    "comparison",
    "different",
    "data",
    "ingestion",
    "tools",
    "let",
    "learn",
    "questions",
    "scope",
    "one",
    "data",
    "injection",
    "tools",
    "mainly",
    "used",
    "structured",
    "data",
    "could",
    "say",
    "data",
    "coming",
    "rdbms",
    "data",
    "already",
    "structured",
    "would",
    "want",
    "ingest",
    "would",
    "want",
    "store",
    "sdfs",
    "could",
    "used",
    "hive",
    "could",
    "used",
    "kind",
    "processing",
    "using",
    "mapreduce",
    "hive",
    "pig",
    "spark",
    "processing",
    "frameworks",
    "would",
    "want",
    "load",
    "data",
    "say",
    "high",
    "voltage",
    "based",
    "tables",
    "scope",
    "mainly",
    "structured",
    "data",
    "extensively",
    "used",
    "organizations",
    "migrating",
    "rdbms",
    "big",
    "data",
    "platform",
    "would",
    "interested",
    "ingesting",
    "data",
    "import",
    "export",
    "data",
    "rdbms",
    "sdfs",
    "vice",
    "versa",
    "let",
    "learn",
    "important",
    "questions",
    "scope",
    "may",
    "asked",
    "interviewer",
    "apply",
    "big",
    "data",
    "related",
    "position",
    "scoop",
    "different",
    "flume",
    "common",
    "question",
    "asked",
    "scoop",
    "mainly",
    "structured",
    "data",
    "scope",
    "works",
    "rdbms",
    "also",
    "works",
    "nosql",
    "databases",
    "import",
    "export",
    "data",
    "import",
    "data",
    "sdfs",
    "import",
    "data",
    "data",
    "warehousing",
    "package",
    "hive",
    "directly",
    "also",
    "hbase",
    "could",
    "also",
    "export",
    "data",
    "hadoop",
    "ecosystem",
    "rdbms",
    "however",
    "comes",
    "flow",
    "flow",
    "data",
    "injection",
    "tool",
    "works",
    "streaming",
    "data",
    "unstructured",
    "data",
    "data",
    "constantly",
    "getting",
    "generated",
    "example",
    "log",
    "files",
    "metrics",
    "server",
    "chat",
    "messenger",
    "interested",
    "working",
    "capturing",
    "storing",
    "streaming",
    "data",
    "storage",
    "layer",
    "sdfs",
    "hbase",
    "could",
    "using",
    "flu",
    "could",
    "tools",
    "also",
    "like",
    "kafka",
    "storm",
    "chokwa",
    "samsa",
    "nifi",
    "scoop",
    "however",
    "mainly",
    "structured",
    "data",
    "loading",
    "data",
    "scope",
    "event",
    "driven",
    "based",
    "event",
    "basically",
    "works",
    "data",
    "already",
    "stored",
    "rdbms",
    "terms",
    "flow",
    "completely",
    "event",
    "driven",
    "messages",
    "events",
    "happen",
    "data",
    "getting",
    "generated",
    "data",
    "ingested",
    "using",
    "flow",
    "scope",
    "works",
    "structured",
    "data",
    "sources",
    "various",
    "scope",
    "connectors",
    "used",
    "fetch",
    "data",
    "external",
    "data",
    "structures",
    "rdbms",
    "every",
    "rdbms",
    "mysql",
    "oracle",
    "db2",
    "microsoft",
    "sql",
    "server",
    "different",
    "connectors",
    "available",
    "flume",
    "works",
    "fetching",
    "streaming",
    "data",
    "tweets",
    "log",
    "files",
    "server",
    "metrics",
    "different",
    "sources",
    "data",
    "getting",
    "generated",
    "interested",
    "ingesting",
    "data",
    "getting",
    "generated",
    "streaming",
    "fashion",
    "would",
    "interested",
    "processing",
    "data",
    "arrives",
    "scoop",
    "import",
    "data",
    "rdbms",
    "onto",
    "sdfs",
    "also",
    "export",
    "back",
    "rdbms",
    "flume",
    "used",
    "streaming",
    "data",
    "could",
    "one",
    "one",
    "one",
    "many",
    "many",
    "one",
    "kind",
    "relation",
    "terms",
    "floom",
    "components",
    "source",
    "sink",
    "channel",
    "main",
    "difference",
    "scoop",
    "flow",
    "different",
    "file",
    "formats",
    "import",
    "data",
    "using",
    "scope",
    "well",
    "lots",
    "lots",
    "formats",
    "import",
    "data",
    "scope",
    "talk",
    "scope",
    "delimited",
    "text",
    "file",
    "format",
    "default",
    "import",
    "format",
    "specified",
    "explicitly",
    "using",
    "text",
    "file",
    "argument",
    "want",
    "import",
    "data",
    "rdbms",
    "could",
    "get",
    "data",
    "sdfs",
    "using",
    "different",
    "compression",
    "schemes",
    "different",
    "formats",
    "using",
    "specific",
    "arguments",
    "could",
    "specify",
    "argument",
    "write",
    "string",
    "based",
    "representation",
    "record",
    "output",
    "files",
    "delimiters",
    "individual",
    "columns",
    "rows",
    "default",
    "format",
    "used",
    "import",
    "data",
    "using",
    "scope",
    "learn",
    "scope",
    "different",
    "arguments",
    "available",
    "click",
    "look",
    "documentation",
    "would",
    "suggest",
    "choosing",
    "one",
    "versions",
    "looking",
    "user",
    "guide",
    "search",
    "arguments",
    "look",
    "specific",
    "control",
    "arguments",
    "show",
    "import",
    "data",
    "using",
    "scope",
    "common",
    "arguments",
    "also",
    "import",
    "control",
    "arguments",
    "wherein",
    "different",
    "options",
    "like",
    "getting",
    "data",
    "avro",
    "sequence",
    "file",
    "text",
    "file",
    "parquet",
    "file",
    "different",
    "formats",
    "also",
    "get",
    "data",
    "default",
    "compression",
    "scheme",
    "gzip",
    "specify",
    "compression",
    "codec",
    "specify",
    "compression",
    "mechanism",
    "would",
    "want",
    "use",
    "importing",
    "data",
    "using",
    "scope",
    "comes",
    "default",
    "format",
    "flume",
    "could",
    "say",
    "sequence",
    "file",
    "binary",
    "format",
    "stores",
    "individual",
    "records",
    "record",
    "specific",
    "data",
    "types",
    "data",
    "types",
    "manifested",
    "java",
    "classes",
    "scope",
    "automatically",
    "generate",
    "data",
    "types",
    "scoop",
    "talk",
    "sequence",
    "file",
    "format",
    "terms",
    "scope",
    "could",
    "extracting",
    "storage",
    "data",
    "binary",
    "representation",
    "mentioned",
    "import",
    "data",
    "different",
    "formats",
    "avro",
    "parquet",
    "sequence",
    "file",
    "binary",
    "format",
    "machine",
    "readable",
    "format",
    "could",
    "also",
    "data",
    "different",
    "compression",
    "schemes",
    "let",
    "show",
    "quick",
    "examples",
    "look",
    "content",
    "could",
    "search",
    "scoop",
    "based",
    "file",
    "listed",
    "examples",
    "would",
    "want",
    "use",
    "different",
    "compression",
    "schemes",
    "examples",
    "look",
    "scoop",
    "import",
    "also",
    "giving",
    "argument",
    "scoop",
    "also",
    "triggers",
    "map",
    "reduced",
    "job",
    "would",
    "say",
    "map",
    "job",
    "run",
    "scoop",
    "import",
    "triggers",
    "map",
    "job",
    "reduce",
    "happens",
    "could",
    "specify",
    "parameter",
    "argument",
    "command",
    "line",
    "could",
    "run",
    "map",
    "job",
    "local",
    "mode",
    "save",
    "time",
    "would",
    "interact",
    "yarn",
    "run",
    "map",
    "job",
    "give",
    "connection",
    "connect",
    "whatever",
    "rdbms",
    "connecting",
    "mentioning",
    "database",
    "name",
    "give",
    "user",
    "name",
    "password",
    "give",
    "table",
    "name",
    "give",
    "target",
    "directory",
    "would",
    "create",
    "directory",
    "table",
    "name",
    "would",
    "work",
    "could",
    "say",
    "minus",
    "z",
    "get",
    "data",
    "compressed",
    "format",
    "gzip",
    "could",
    "specifying",
    "compression",
    "codec",
    "could",
    "specify",
    "compression",
    "codec",
    "would",
    "want",
    "use",
    "say",
    "snappy",
    "b",
    "lz4",
    "default",
    "could",
    "also",
    "run",
    "query",
    "giving",
    "scope",
    "import",
    "specifying",
    "query",
    "notice",
    "given",
    "table",
    "name",
    "would",
    "included",
    "query",
    "get",
    "data",
    "sequence",
    "file",
    "format",
    "binary",
    "format",
    "create",
    "huge",
    "file",
    "could",
    "also",
    "compression",
    "enabled",
    "could",
    "say",
    "output",
    "map",
    "job",
    "use",
    "compression",
    "record",
    "level",
    "data",
    "coming",
    "sequence",
    "file",
    "sequence",
    "file",
    "binary",
    "format",
    "supports",
    "compression",
    "record",
    "level",
    "block",
    "level",
    "could",
    "get",
    "data",
    "avro",
    "file",
    "data",
    "embedded",
    "schema",
    "within",
    "file",
    "parquet",
    "file",
    "also",
    "different",
    "ways",
    "set",
    "different",
    "compression",
    "schemes",
    "even",
    "get",
    "data",
    "different",
    "formats",
    "could",
    "simple",
    "scope",
    "import",
    "looking",
    "importance",
    "eval",
    "tool",
    "scope",
    "something",
    "called",
    "eval",
    "tool",
    "scoop",
    "eval",
    "tool",
    "allows",
    "users",
    "execute",
    "user",
    "defined",
    "queries",
    "respective",
    "database",
    "servers",
    "preview",
    "result",
    "console",
    "either",
    "could",
    "running",
    "straight",
    "away",
    "query",
    "import",
    "data",
    "sdfs",
    "could",
    "use",
    "scoop",
    "eval",
    "connect",
    "external",
    "rdbms",
    "specify",
    "username",
    "password",
    "could",
    "giving",
    "query",
    "see",
    "would",
    "result",
    "query",
    "intend",
    "import",
    "let",
    "learn",
    "scope",
    "imports",
    "exports",
    "data",
    "rdbms",
    "sdfs",
    "architecture",
    "rdbms",
    "know",
    "database",
    "structures",
    "tables",
    "logical",
    "internally",
    "always",
    "metadata",
    "stored",
    "scope",
    "import",
    "connects",
    "external",
    "rdbms",
    "connection",
    "uses",
    "internal",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "something",
    "needs",
    "set",
    "admin",
    "need",
    "make",
    "sure",
    "whichever",
    "rdbms",
    "intend",
    "connect",
    "need",
    "jdbc",
    "connector",
    "particular",
    "rdbms",
    "stored",
    "within",
    "scope",
    "lib",
    "folder",
    "scope",
    "import",
    "gets",
    "metadata",
    "scoop",
    "command",
    "converts",
    "map",
    "job",
    "might",
    "one",
    "multiple",
    "map",
    "tasks",
    "depends",
    "scope",
    "command",
    "could",
    "specifying",
    "would",
    "want",
    "import",
    "one",
    "task",
    "multiple",
    "tasks",
    "multiple",
    "map",
    "tasks",
    "run",
    "section",
    "data",
    "rdbms",
    "store",
    "sdfs",
    "high",
    "level",
    "could",
    "say",
    "scoop",
    "introspect",
    "database",
    "get",
    "gathered",
    "metadata",
    "divides",
    "input",
    "data",
    "set",
    "splits",
    "division",
    "data",
    "splits",
    "mainly",
    "happens",
    "primary",
    "key",
    "column",
    "table",
    "somebody",
    "might",
    "ask",
    "table",
    "rdbms",
    "primary",
    "key",
    "column",
    "scope",
    "import",
    "either",
    "import",
    "using",
    "one",
    "mapper",
    "task",
    "specifying",
    "hyphen",
    "hyphen",
    "equals",
    "one",
    "would",
    "say",
    "split",
    "parameter",
    "specify",
    "numeric",
    "column",
    "rdbms",
    "import",
    "data",
    "let",
    "show",
    "quick",
    "example",
    "could",
    "look",
    "scoop",
    "command",
    "file",
    "could",
    "looking",
    "example",
    "see",
    "one",
    "specifying",
    "minus",
    "minus",
    "equals",
    "1",
    "basically",
    "means",
    "would",
    "want",
    "import",
    "data",
    "using",
    "one",
    "map",
    "task",
    "case",
    "whether",
    "table",
    "primary",
    "key",
    "column",
    "primary",
    "key",
    "column",
    "matter",
    "say",
    "minus",
    "minus",
    "ms6",
    "specifying",
    "multiple",
    "map",
    "tasks",
    "imported",
    "look",
    "primary",
    "key",
    "column",
    "table",
    "importing",
    "table",
    "primary",
    "key",
    "column",
    "could",
    "specifying",
    "split",
    "specify",
    "column",
    "data",
    "could",
    "split",
    "multiple",
    "chunks",
    "multiple",
    "map",
    "tasks",
    "could",
    "take",
    "second",
    "scenario",
    "table",
    "primary",
    "key",
    "column",
    "numeric",
    "column",
    "could",
    "split",
    "case",
    "would",
    "want",
    "use",
    "multiple",
    "mappers",
    "could",
    "still",
    "say",
    "split",
    "textual",
    "column",
    "add",
    "property",
    "allows",
    "splitting",
    "data",
    "options",
    "given",
    "scoop",
    "link",
    "going",
    "scoop",
    "imports",
    "exports",
    "data",
    "rdbms",
    "sdfs",
    "architecture",
    "said",
    "submits",
    "map",
    "job",
    "cluster",
    "basically",
    "import",
    "export",
    "exporting",
    "data",
    "sdfs",
    "case",
    "would",
    "map",
    "job",
    "would",
    "look",
    "multiple",
    "splits",
    "data",
    "existing",
    "map",
    "job",
    "would",
    "process",
    "one",
    "one",
    "table",
    "map",
    "task",
    "export",
    "rdbms",
    "suppose",
    "database",
    "testdb",
    "mysql",
    "somebody",
    "asked",
    "write",
    "command",
    "connect",
    "database",
    "import",
    "tables",
    "scoop",
    "quick",
    "example",
    "showed",
    "command",
    "file",
    "could",
    "say",
    "scope",
    "import",
    "would",
    "want",
    "connect",
    "using",
    "jdbc",
    "work",
    "jdbc",
    "connector",
    "already",
    "exists",
    "within",
    "scope",
    "lib",
    "directory",
    "admin",
    "set",
    "connect",
    "rdbms",
    "point",
    "database",
    "database",
    "name",
    "test",
    "underscore",
    "db",
    "could",
    "give",
    "user",
    "name",
    "either",
    "could",
    "give",
    "password",
    "command",
    "line",
    "say",
    "capital",
    "p",
    "could",
    "prompted",
    "password",
    "could",
    "give",
    "table",
    "name",
    "would",
    "want",
    "import",
    "could",
    "also",
    "specifying",
    "minus",
    "minus",
    "specify",
    "many",
    "map",
    "tasks",
    "want",
    "use",
    "import",
    "showed",
    "previous",
    "screen",
    "export",
    "table",
    "back",
    "rdbms",
    "need",
    "data",
    "directory",
    "hdfs",
    "example",
    "department",
    "table",
    "retail",
    "database",
    "already",
    "imported",
    "scoop",
    "need",
    "export",
    "table",
    "back",
    "rdbms",
    "content",
    "table",
    "create",
    "new",
    "department",
    "table",
    "rdbms",
    "could",
    "create",
    "table",
    "specifying",
    "column",
    "names",
    "whether",
    "supports",
    "null",
    "primary",
    "key",
    "column",
    "always",
    "recommended",
    "scoop",
    "export",
    "connect",
    "rdbms",
    "specifying",
    "username",
    "password",
    "specify",
    "table",
    "want",
    "export",
    "data",
    "give",
    "export",
    "directory",
    "pointing",
    "directory",
    "sdfs",
    "contains",
    "data",
    "export",
    "data",
    "table",
    "seeing",
    "example",
    "could",
    "look",
    "file",
    "example",
    "import",
    "importing",
    "data",
    "directly",
    "hive",
    "scope",
    "import",
    "importing",
    "data",
    "directly",
    "hbase",
    "table",
    "query",
    "hbase",
    "table",
    "look",
    "data",
    "could",
    "also",
    "export",
    "running",
    "map",
    "job",
    "local",
    "mode",
    "connecting",
    "rdbms",
    "specifying",
    "username",
    "specifying",
    "table",
    "would",
    "want",
    "export",
    "directory",
    "sdfs",
    "kept",
    "relevant",
    "data",
    "simple",
    "example",
    "export",
    "looking",
    "role",
    "jdbc",
    "driver",
    "scope",
    "setup",
    "said",
    "would",
    "want",
    "use",
    "scoop",
    "connect",
    "external",
    "rdbms",
    "need",
    "jdbc",
    "odbc",
    "connector",
    "jar",
    "file",
    "one",
    "admin",
    "could",
    "download",
    "jdbc",
    "connector",
    "jar",
    "file",
    "place",
    "jar",
    "file",
    "within",
    "scoop",
    "lib",
    "directory",
    "wherever",
    "scoop",
    "installed",
    "jdbc",
    "connector",
    "jar",
    "file",
    "contains",
    "driver",
    "jdbc",
    "driver",
    "standard",
    "java",
    "api",
    "used",
    "accessing",
    "different",
    "databases",
    "rdbms",
    "connector",
    "jar",
    "file",
    "much",
    "required",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "driver",
    "class",
    "enables",
    "connection",
    "rdbms",
    "hadoop",
    "structure",
    "database",
    "vendor",
    "responsible",
    "writing",
    "implementation",
    "allow",
    "communication",
    "corresponding",
    "database",
    "need",
    "download",
    "drivers",
    "allow",
    "scoop",
    "connect",
    "external",
    "rdbms",
    "jdbc",
    "driver",
    "alone",
    "enough",
    "connect",
    "scope",
    "also",
    "need",
    "connectors",
    "interact",
    "different",
    "database",
    "connector",
    "plugable",
    "piece",
    "used",
    "fetch",
    "metadata",
    "allow",
    "scoop",
    "overcome",
    "differences",
    "sql",
    "dialects",
    "connection",
    "established",
    "normally",
    "admins",
    "would",
    "setting",
    "scope",
    "hadoop",
    "would",
    "download",
    "say",
    "mysql",
    "jdbc",
    "connector",
    "would",
    "go",
    "mysql",
    "connectors",
    "connecting",
    "mysql",
    "similarly",
    "rdbms",
    "could",
    "say",
    "going",
    "could",
    "looking",
    "previous",
    "version",
    "depending",
    "could",
    "going",
    "platform",
    "independent",
    "could",
    "downloading",
    "connected",
    "jar",
    "file",
    "enter",
    "jar",
    "file",
    "would",
    "see",
    "mysql",
    "connector",
    "jar",
    "look",
    "music",
    "package",
    "within",
    "connector",
    "jar",
    "file",
    "driver",
    "class",
    "allows",
    "connection",
    "scope",
    "rdbms",
    "things",
    "done",
    "admin",
    "scope",
    "connecting",
    "external",
    "rdbms",
    "update",
    "columns",
    "already",
    "exported",
    "export",
    "put",
    "data",
    "rdbms",
    "really",
    "update",
    "columns",
    "already",
    "exported",
    "yes",
    "using",
    "update",
    "key",
    "parameter",
    "scoop",
    "export",
    "command",
    "remains",
    "thing",
    "specify",
    "table",
    "name",
    "fields",
    "terminated",
    "specific",
    "delimiter",
    "say",
    "update",
    "key",
    "column",
    "name",
    "allows",
    "us",
    "update",
    "columns",
    "already",
    "exported",
    "rdbms",
    "code",
    "gen",
    "scope",
    "commands",
    "translate",
    "mapreduce",
    "job",
    "map",
    "job",
    "code",
    "gen",
    "basically",
    "tool",
    "scope",
    "generates",
    "data",
    "access",
    "objects",
    "dao",
    "java",
    "classes",
    "encapsulate",
    "interpret",
    "imported",
    "records",
    "scoop",
    "code",
    "gen",
    "connect",
    "rdbms",
    "using",
    "username",
    "give",
    "table",
    "generate",
    "java",
    "code",
    "employee",
    "table",
    "test",
    "database",
    "code",
    "gen",
    "useful",
    "us",
    "understand",
    "data",
    "particular",
    "table",
    "finally",
    "scoop",
    "used",
    "convert",
    "data",
    "different",
    "formats",
    "think",
    "already",
    "answered",
    "right",
    "tools",
    "used",
    "purpose",
    "scoop",
    "used",
    "convert",
    "data",
    "different",
    "formats",
    "depends",
    "different",
    "arguments",
    "use",
    "import",
    "avro",
    "file",
    "parquet",
    "file",
    "binary",
    "format",
    "record",
    "block",
    "level",
    "compression",
    "interested",
    "knowing",
    "different",
    "data",
    "formats",
    "think",
    "suggest",
    "link",
    "say",
    "hadoop",
    "formats",
    "think",
    "tech",
    "maggie",
    "avro",
    "parque",
    "let",
    "see",
    "find",
    "link",
    "take",
    "mac",
    "e",
    "yeah",
    "good",
    "link",
    "specifies",
    "talks",
    "different",
    "data",
    "formats",
    "know",
    "text",
    "file",
    "format",
    "different",
    "compression",
    "schemes",
    "data",
    "organization",
    "common",
    "formats",
    "text",
    "file",
    "structured",
    "binary",
    "sequence",
    "files",
    "compression",
    "without",
    "compression",
    "record",
    "level",
    "block",
    "level",
    "avro",
    "data",
    "file",
    "sequence",
    "parquet",
    "data",
    "file",
    "columnar",
    "format",
    "formats",
    "like",
    "orc",
    "rc",
    "please",
    "look",
    "thank",
    "watching",
    "full",
    "course",
    "video",
    "big",
    "data",
    "2022",
    "hope",
    "useful",
    "informative",
    "queries",
    "please",
    "feel",
    "free",
    "put",
    "comments",
    "section",
    "video",
    "happy",
    "help",
    "thanks",
    "stay",
    "safe",
    "keep",
    "learning",
    "music",
    "hi",
    "like",
    "video",
    "subscribe",
    "simply",
    "learn",
    "youtube",
    "channel",
    "click",
    "watch",
    "similar",
    "videos",
    "turn",
    "get",
    "certified",
    "click"
  ],
  "keywords": [
    "big",
    "data",
    "term",
    "sure",
    "users",
    "companies",
    "started",
    "machine",
    "learning",
    "things",
    "analytics",
    "helps",
    "different",
    "manage",
    "processes",
    "use",
    "sets",
    "real",
    "time",
    "well",
    "organization",
    "organizations",
    "get",
    "better",
    "understanding",
    "customers",
    "help",
    "market",
    "per",
    "business",
    "dollars",
    "us",
    "percent",
    "going",
    "top",
    "skill",
    "plan",
    "new",
    "relevant",
    "skills",
    "related",
    "roles",
    "high",
    "scope",
    "really",
    "good",
    "looking",
    "screen",
    "see",
    "oracle",
    "let",
    "look",
    "today",
    "video",
    "full",
    "course",
    "start",
    "become",
    "engineer",
    "next",
    "understand",
    "applications",
    "learn",
    "framework",
    "hadoop",
    "tools",
    "part",
    "apache",
    "spark",
    "architecture",
    "finally",
    "close",
    "questions",
    "tell",
    "come",
    "know",
    "generate",
    "whole",
    "lot",
    "volume",
    "called",
    "take",
    "people",
    "make",
    "amount",
    "find",
    "company",
    "words",
    "based",
    "test",
    "solution",
    "various",
    "systems",
    "working",
    "need",
    "able",
    "process",
    "processing",
    "build",
    "scalable",
    "one",
    "database",
    "transform",
    "load",
    "another",
    "store",
    "ways",
    "quality",
    "system",
    "programming",
    "languages",
    "together",
    "structured",
    "reduce",
    "sources",
    "work",
    "required",
    "first",
    "step",
    "knowledge",
    "important",
    "experience",
    "would",
    "want",
    "create",
    "possible",
    "without",
    "even",
    "easy",
    "way",
    "used",
    "python",
    "java",
    "c",
    "plus",
    "second",
    "sql",
    "perform",
    "write",
    "queries",
    "relational",
    "management",
    "mysql",
    "server",
    "warehouse",
    "operation",
    "operations",
    "constantly",
    "unstructured",
    "number",
    "information",
    "loading",
    "databases",
    "basically",
    "multiple",
    "analyzing",
    "stage",
    "since",
    "root",
    "access",
    "hardware",
    "like",
    "linux",
    "comes",
    "hdfs",
    "mapreduce",
    "edge",
    "base",
    "hive",
    "pig",
    "worked",
    "engine",
    "streaming",
    "facebook",
    "twitter",
    "also",
    "interactive",
    "analysis",
    "final",
    "requirement",
    "massive",
    "talk",
    "salary",
    "thousand",
    "year",
    "around",
    "four",
    "job",
    "architect",
    "cloudera",
    "result",
    "variety",
    "industry",
    "handle",
    "challenges",
    "long",
    "google",
    "cloud",
    "making",
    "2",
    "simply",
    "provides",
    "program",
    "developer",
    "scala",
    "much",
    "goes",
    "50",
    "go",
    "major",
    "ahead",
    "form",
    "videos",
    "gets",
    "generated",
    "every",
    "single",
    "user",
    "imagine",
    "traditional",
    "computing",
    "million",
    "search",
    "made",
    "log",
    "concept",
    "five",
    "value",
    "example",
    "across",
    "world",
    "records",
    "results",
    "types",
    "examples",
    "files",
    "benefit",
    "faster",
    "reduced",
    "known",
    "stores",
    "uses",
    "distributed",
    "file",
    "huge",
    "broken",
    "smaller",
    "chunks",
    "stored",
    "machines",
    "copies",
    "nodes",
    "task",
    "tasks",
    "b",
    "instead",
    "three",
    "complete",
    "parallel",
    "end",
    "becomes",
    "processed",
    "analyze",
    "3",
    "call",
    "customer",
    "taken",
    "could",
    "earlier",
    "analyzed",
    "back",
    "less",
    "total",
    "define",
    "using",
    "storage",
    "storing",
    "useful",
    "due",
    "size",
    "quick",
    "think",
    "move",
    "responsible",
    "batches",
    "clusters",
    "computers",
    "field",
    "products",
    "uh",
    "starting",
    "role",
    "primary",
    "efficient",
    "needs",
    "hence",
    "rdbms",
    "mentioned",
    "run",
    "windows",
    "large",
    "batch",
    "two",
    "makes",
    "name",
    "looks",
    "similar",
    "components",
    "scoop",
    "writing",
    "latin",
    "scripts",
    "jobs",
    "saw",
    "transformation",
    "code",
    "core",
    "scripting",
    "previous",
    "basic",
    "commands",
    "individual",
    "nosql",
    "hbase",
    "right",
    "online",
    "history",
    "might",
    "list",
    "available",
    "past",
    "works",
    "something",
    "nothing",
    "filter",
    "algorithms",
    "many",
    "times",
    "means",
    "defined",
    "mainly",
    "terms",
    "messages",
    "contains",
    "formats",
    "format",
    "method",
    "problem",
    "try",
    "order",
    "done",
    "simple",
    "series",
    "main",
    "product",
    "aware",
    "interesting",
    "always",
    "developed",
    "error",
    "save",
    "case",
    "decide",
    "location",
    "factor",
    "difference",
    "factors",
    "similarly",
    "particular",
    "everything",
    "service",
    "give",
    "fault",
    "happens",
    "remove",
    "minus",
    "still",
    "tool",
    "depending",
    "fields",
    "click",
    "type",
    "descriptive",
    "asks",
    "question",
    "diagnostic",
    "happen",
    "years",
    "graph",
    "running",
    "easily",
    "say",
    "creating",
    "space",
    "ask",
    "already",
    "query",
    "shows",
    "added",
    "point",
    "although",
    "site",
    "enough",
    "options",
    "activity",
    "last",
    "happening",
    "depends",
    "internal",
    "external",
    "change",
    "availability",
    "fair",
    "place",
    "automatically",
    "talking",
    "allows",
    "services",
    "open",
    "source",
    "environment",
    "replicated",
    "memory",
    "input",
    "output",
    "disk",
    "language",
    "later",
    "given",
    "tolerant",
    "application",
    "care",
    "cases",
    "interested",
    "network",
    "department",
    "coming",
    "details",
    "region",
    "property",
    "gives",
    "says",
    "app",
    "provided",
    "performed",
    "runs",
    "within",
    "capacity",
    "allocated",
    "resources",
    "tracking",
    "resource",
    "allocation",
    "kinds",
    "properties",
    "yet",
    "active",
    "bring",
    "taking",
    "never",
    "couple",
    "allow",
    "check",
    "getting",
    "portion",
    "send",
    "1",
    "connection",
    "however",
    "10",
    "okay",
    "probably",
    "changed",
    "100",
    "layer",
    "read",
    "said",
    "anything",
    "whether",
    "features",
    "connect",
    "show",
    "collection",
    "complex",
    "kind",
    "whatever",
    "stream",
    "variable",
    "servers",
    "certainly",
    "specific",
    "rows",
    "table",
    "schema",
    "mean",
    "values",
    "obviously",
    "actually",
    "cpu",
    "normally",
    "set",
    "directories",
    "master",
    "chunk",
    "slave",
    "contain",
    "metadata",
    "client",
    "api",
    "request",
    "reading",
    "internally",
    "ram",
    "divided",
    "block",
    "bigger",
    "split",
    "local",
    "cluster",
    "status",
    "replication",
    "mb",
    "default",
    "project",
    "object",
    "either",
    "download",
    "thing",
    "transformations",
    "compression",
    "hit",
    "commodity",
    "link",
    "person",
    "setup",
    "distributions",
    "talks",
    "support",
    "web",
    "hortonworks",
    "mapper",
    "package",
    "setting",
    "distribution",
    "documents",
    "columns",
    "processor",
    "giving",
    "component",
    "computer",
    "blocks",
    "megabytes",
    "splits",
    "128",
    "node",
    "crashes",
    "created",
    "count",
    "phase",
    "word",
    "reducer",
    "yarn",
    "manager",
    "containers",
    "managers",
    "container",
    "sends",
    "side",
    "little",
    "maybe",
    "far",
    "put",
    "bit",
    "family",
    "nice",
    "keep",
    "remember",
    "small",
    "pretty",
    "stuff",
    "folder",
    "got",
    "demo",
    "vm",
    "standalone",
    "box",
    "version",
    "select",
    "virtual",
    "import",
    "importing",
    "image",
    "dot",
    "cores",
    "sdfs",
    "settings",
    "script",
    "steps",
    "admin",
    "console",
    "command",
    "line",
    "interface",
    "connected",
    "terminal",
    "seconds",
    "takes",
    "dfs",
    "tells",
    "gb",
    "password",
    "copy",
    "id",
    "username",
    "browser",
    "port",
    "add",
    "key",
    "delete",
    "regular",
    "configuration",
    "hue",
    "issues",
    "existing",
    "drop",
    "pointing",
    "zookeeper",
    "upon",
    "logs",
    "ui",
    "switch",
    "usually",
    "starts",
    "racks",
    "ran",
    "map",
    "note",
    "approach",
    "enterprise",
    "track",
    "mapping",
    "edit",
    "fs",
    "secondary",
    "oh",
    "rack",
    "somebody",
    "common",
    "date",
    "creates",
    "standard",
    "update",
    "underneath",
    "pull",
    "text",
    "failure",
    "saying",
    "replicas",
    "home",
    "classes",
    "hey",
    "sending",
    "mechanism",
    "zoom",
    "office",
    "written",
    "loaded",
    "window",
    "ls",
    "editor",
    "cat",
    "path",
    "directory",
    "enter",
    "state",
    "floor",
    "model",
    "reducing",
    "class",
    "function",
    "wherein",
    "mode",
    "specify",
    "shuffling",
    "partitioner",
    "combiner",
    "binary",
    "supports",
    "record",
    "pairs",
    "lines",
    "character",
    "sequence",
    "functions",
    "keys",
    "directly",
    "tracker",
    "trackers",
    "submitted",
    "jar",
    "hyphen",
    "refresh",
    "demon",
    "scheduler",
    "submit",
    "meta",
    "pick",
    "specifying",
    "completed",
    "level",
    "scheduling",
    "execution",
    "column",
    "wants",
    "departments",
    "worker",
    "logical",
    "queue",
    "standby",
    "shell",
    "group",
    "x",
    "export",
    "equals",
    "connector",
    "tables",
    "jdbc",
    "string",
    "driver",
    "underscore",
    "hql",
    "age",
    "variables",
    "codes",
    "execute",
    "compiler",
    "flow",
    "partitions",
    "partition",
    "integer",
    "interval",
    "whenever",
    "employee",
    "comma",
    "row",
    "join",
    "partitioning",
    "dag",
    "dump",
    "tuple",
    "h",
    "regions",
    "wall",
    "scan",
    "rdds",
    "frames",
    "rdd",
    "context",
    "avro",
    "streams",
    "relation",
    "sbt",
    "receiver",
    "utility",
    "checkpointing",
    "netcat"
  ]
}