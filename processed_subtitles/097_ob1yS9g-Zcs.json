{
  "text": "[Music]\nneural networks typically used in deep\nlearning are computing systems inspired\nby the structure and function of a human\nbrain it is used to find solutions to\nthose problems for which the algorithmic\nmethod is expensive or does not exist\nneural networks learn by example so we\ndo not need to program it in depth hi\nguys welcome to the complete neural\nnetwork tutorial by simply learn but\nbefore we begin make sure to subscribe\nto our channel and hit the bell icon to\nnever miss an update first we will\nunderstand the basics of neural networks\nand see its practical implementation\nthen we learn how back propagation and\ngradient descent works finally we'll\nunderstand the concepts and practical\nimplementation of convolutional and\nrecurrent neural networks let's first\nlook at a small animated video to\nunderstand neural networks better last\nsummer my family and i visited russia\neven though none of us could read\nrussian we did not have any trouble in\nfiguring our way out all thanks to\ngoogle's real-time translation of\nrussian boards into english\nthis is just one of the several\napplications of neural networks\nneural networks form the base of deep\nlearning a subfield of machine learning\nwhere the algorithms are inspired by the\nstructure of the human brain\nneural networks take in data train\nthemselves to recognize the patterns in\nthis data and then predict the outputs\nfor a new set of similar data\nlet's understand how this is done\nlet's construct a neural network that\ndifferentiates between a square circle\nand triangle\nneural networks are made up of layers of\nneurons these neurons are the core\nprocessing units of the network\nfirst we have the input layer which\nreceives the input the output layer\npredicts our final output\nin between exists the hidden layers\nwhich perform most of the computations\nrequired by our network\nhere's an image of a circle\nthis image is composed of 28 by 28\npixels which make up for 784 pixels\neach pixel is fed as input to each\nneuron of the first layer\nneurons of one layer are connected to\nneurons of the next layer through\nchannels\neach of these channels is assigned a\nnumerical value known as weight\nthe inputs are multiplied to the\ncorresponding weights and their sum is\nsent as input to the neurons in the\nhidden layer\neach of these neurons is associated with\na numerical value called the bias which\nis then added to the input sum\nthis value is then passed through a\nthreshold function called the activation\nfunction\nthe result of the activation function\ndetermines if the particular neuron will\nget activated or not\nan activated neuron transmits data to\nthe neurons of the next layer over the\nchannels\nin this manner the data is propagated\nthrough the network\nthis is called forward propagation\nin the output layer the neuron with the\nhighest value fires and determines the\noutput the values are basically a\nprobability\nfor example here our neuron associated\nwith square has the highest probability\nhence that's the output predicted by the\nneural network\nof course just by a look at it we know\nour neural network has made a wrong\nprediction but how does the network\nfigure this out\nnote that our network is yet to be\ntrained\nduring this training process along with\nthe input our network also has the\noutput fed to it\nthe predicted output is compared against\nthe actual output to realize the error\nin prediction the magnitude of the error\nindicates how wrong we are and the sign\nsuggests if our predicted values are\nhigher or lower than expected\nthe arrows here give an indication of\nthe direction and magnitude of change to\nreduce the error\nthis information is then transferred\nbackward through our network this is\nknown as back propagation\nnow based on this information the\nweights are adjusted\nthis cycle of forward propagation and\nback propagation is iteratively\nperformed with multiple inputs\nthis process continues until our weights\nare assigned such that the network can\npredict the shapes correctly in most of\nthe cases\nthis brings our training process to an\nend\nyou might wonder how long this training\nprocess takes honestly neural networks\nmay take hours or even months to train\nbut time is a reasonable trade-off when\ncompared to its scope\nlet us look at some of the prime\napplications of neural networks facial\nrecognition cameras on smartphones these\ndays can estimate the age of the person\nbased on their facial features this is\nneural networks at play first\ndifferentiating the face from the\nbackground and then correlating the\nlines and spots on your face to a\npossible age forecasting neural networks\nare trained to understand the patterns\nand detect the possibility of rainfall\nor rise in stock prices with high\naccuracy music composition neural\nnetworks can even learn patterns in\nmusic and train itself enough to compose\na fresh tune so here's a question for\nyou which of the following statements\ndoes not hold true a activation\nfunctions are threshold functions b\nerror is calculated at each layer of the\nneural network c both forward and back\npropagation take place during the\ntraining process of a neural network d\nmost of the data processing is carried\nout in the hidden layers leave your\nanswers in the comments section below\nwith deep learning and neural networks\nwe are still taking baby steps the\ngrowth in this field has been foreseen\nby the big names companies such as\ngoogle amazon and nvidia have invested\nin developing products such as libraries\npredictive models and intuitive gpus\nthat support the implementation of\nneural networks the question dividing\nthe visionaries is on the reach of\nneural networks to what extent can we\nreplicate the human brain we'd have to\nwait a few more years to give a definite\nanswer but if you enjoyed this video it\nwould only take a few seconds to like\nand share it also if you haven't yet do\nsubscribe to our channel and hit the\nbell icon\nas we have a lot more exciting videos\ncoming up fun learning till then i hope\nyou're excited to learn more about\nneural networks now we have richard who\nwill take you through the remaining part\nof the course and make you understand\nneural networks in detail with that over\nto you richard\nwhat is a neural network welcome to\ntoday's lesson my name is richard\nkirschner i'm with the simply learn team\ntoday we want to discuss the very basics\nof a neural network and what that is\nso what's in it for you today what is\ndeep learning what is artificial neural\nnetwork how does neural network work\nadvantages of a neural network\napplications of a neural network and the\nfuture of neural networks let's start\nwith a brief history of the artificial\nintelligence hello i am the human brain\nthis one's seeking enlightenment and has\nsat and meditated i am the most complex\norgan in the human body and i help you\nto think understand and make decisions\nand the secret behind all my power is a\nneuron i'll get back to that in some\ntime ever since the 1950s scientists\nhave been trying to mimic the\nfunctioning of a neuron and use it to\nbuild smarter robots after a lot of\ntrial and error humans finally designed\na computer that can recognize human\nspeech it was only after 2000 that\nhumans were able to give birth to deep\nlearning that was able to see and\ndistinguish between different images and\nvideos\nso looking at that let's dive into what\nis deep learning now your first thought\nmight be is the opposite of shallow\nlearning no deep learning i like into a\nmagic box and let's go in there just\ntake a look as to why it's kind of a\nmagic box so what exactly is deep\nlearning these are the images of dogs\ndeep learning is a machine learning\ntechnique that teaches computers to do\nwhat comes naturally to humans learn by\nexample the robot gets trained with\nphotos as example now this is very\ndifferent than hardwiring a computer\nprogram so that it recognizes something\nit actually learns and that's where it's\na magic box because you don't really\ncontrol how it learns you control the\naspects that go in the computer comes\nback and says wait i know what you are\nlooks at the photograph of the dog and\nit's able to identify that it saw in the\nimages it says you are a doc woof woof\nso that's an example of deep learning\nyou'll notice we didn't go in we'll go\ninto the actually how it works behind\nthe scenes for a neural network but\nthere's a bit of filling of magic and\nthat's where the term deep learning\ncomes in and that's also the term where\ni like to call it a magic box you put\nthese things in here into the program\nand it starts running the deep learning\nand you have to understand those\nsettings but you don't have to follow\nthe exactly what's going on in the deep\nlearning model that brings us to the\nquestion how does deep learning do it\nremember the neuron scientists managed\nto build an artificial form of it that\npowers any deep learning based machine\nso let's talk about artificial neural\nnetworks what is an artificial neural\nnetwork to understand how an artificial\nneuron works we need to understand how\nthe real one works first we have a\ndendrite input to the neuron and you can\nsee these little hairs that come in and\nthey receive information then we have\nthe cell body information processing\nhappens here so it takes all these\ndifferent dendrites and information\ncoming in from the different dendrites\nand it looks at that information and\nthen you have your axon which is the\noutput to the neuron so there goes your\naxon and you see it goes all the way out\nand at the very end it flanges out each\none of those little flanges connects to\nthe dendrite or the hairs on the next\none now let's see what an artificial\nneural network looks like so an\nartificial neural network we have an\ninput layer so that could be an array of\ndata each one of those white dots and\nthe yellow bar would represent say a\npixel in the picture then you have the\nlines that connected to the hidden\nlayers which are your weights and they\nadd all those up on the hidden layers in\neach one of those dots kind of like a\ncell does something with all the inputs\nand then it puts an output into the next\nhidden layer and so on into the output\nlayer so information processing happens\nhere input to the neuron output to the\nneuron so you can see how they are\nsimilar we have an input which is our\nyellow bar coming in and then you like\nliken each of the hidden layers to being\na neuron and it passes it to the next\none and so on and then you have an\noutput to the next neuron or an output\nto the real world a neural network is a\nsystem of hardware and or software\npattern after the operation of neurons\nin the human brain neural networks also\ncalled artificial neural networks is a\nway of achieving deep learning how does\nartificial neural networks work let us\nfind out how does an artificial neural\nnetwork work hey siri what is the time\nnow it's 12 30 in the morning thanks\nlet's find out how she recognizes speech\nhere is a neural network in the\ndifferent layers on it so we have our\ninput layer our hidden layers and the\noutput layer this is the sentence that\nneeds to be recognized by the network\nwhat is the time so when it comes in\neach one comes in as a pattern of sound\nso what is the time first let's consider\nthe word what and you have w-h-a-t and\nyou can see each one of those in the\nsound bar probably looks a little\ndifferent than that just a\nrepresentation comes in as a different\npattern now we will split the sound wave\nfor the letter w into smaller segments\nso we split off w and then we take w\nwhen we analyze just w as the amplitude\nis varying in the sound wave for w we\ncollect the values at different\nintervals and form an array so we have\n0.5 1.5 1.7 1.9 that might be the\ndifferent amplitudes coming in and we\nfeed the array of amplitudes to the\ninput layer so each one of those goes\ninto its own box on the input layer\nrandom weights are assigned to each\ninterconnection between input and hidden\nlayer so remember all those little lines\ni said those are special weights now\nwe're going to start by doing it\nrandomly we always start with randoms if\nwe start with some kind of preset\nidentical pattern like if you set them\nall to three take forever to train it\nand you're less likely to get a good\nresult where random works really well in\nthis the weights are multiplied with the\ninputs and a bias is added to form the\ntransfer function so we make a sum of\nall the weights times the value so you\ntake 0.5 which is your x coming in and\nwe're going to multiply that by w1 w2 w3\nso on and then we get to the next level\nwe're going to add those together coming\nin what's coming in so we add the weight\ntimes x and there's always a bias added\nin if you ever build your own neural\nnetwork don't forget to add the bias in\notherwise it tends to not work quite as\nwell you need that extra layer in there\nto help it weights are assigned to the\ninterconnection between the hidden\nlayers the output of the transfer\nfunction is fed as an input to the\nactivation function so the output from\none hidden layer becomes the input to\nthe next hidden layer acoustic model\ncontains the statistical representation\nof each distinct sound that makes a word\nand so we start building these\nacoustical models and as these layers\nseparate them out they'll start learning\nwhat the different models are for the\ndifferent letters lexicon contains the\ndata for different pronunciations of\nevery word so we have the lexicon at the\nend where we end up with the a b c d and\nit identifies the different letters in\nthere now the term acoustic model and\nthe term lexicon are specific to this\ndomain the domain of understanding\nspeech certainly when you're doing\nphotographs and other things you'll have\ndifferent labels on here but the process\nis going to be the same and finally we\nget our output later following the same\nprocess for every word and letter the\nneural network recognizes the sentence\nyou said what is the time so identify as\na w-h-a-t and then identifies that\nthat's one word what is the time and so\non it's 12 30. that way siri can look up\nthe time and read it back to you so\nlet's look at the advantages of an\nartificial neural network so gentlemen\ncould you tell me the advantages of an\nartificial neural network it's amazing\nhow many times i've been in that\nsituation where i have to explain to the\npeople making the decisions in the\ncompany it's amazing how many times i've\nbeen in that space where i have to\nexplain to the owner of the company what\nthe artificial intelligence and the\nneural network actually do what are the\nadvantages of an artificial neural\nnetwork and what it can do for them and\nhow it works so an artificial neural\nnetwork outputs aren't limited entirely\nby inputs and results given to them\ninitially by an expert system this\nability comes in handy for robotics and\npattern recognition systems artificial\nneural networks have the potential for\nhigh fault tolerance artificial neural\nnetworks are capable of debugging or\ndiagnosing a network on their own very\ncommon use these days is to go through\nall the log files and sort them out\nthousands of log files if you're in\nworking as an admin non-linear systems\nhave the capability of finding shortcuts\nto reach computational expensive\nsolutions so we see this in banking\nwhere by hand they have an excel\nspreadsheet and then they start building\ncodes around that excel spreadsheet and\nover 20 years they might build a\nrepertoire of all these functions and\nthe neural network comes up with the\nsame answers done in days weeks or even\na month for a huge bank so let's take a\nlook a little bit more because i\nmentioned a couple applications of\nartificial intelligence there but let's\ndig deeper into the applications of\nartificial intelligence let's look at\nsome of the real life this is stuff\ngoing on right now in our world and\nwe're in such an exciting time with the\nneural networks and the machine learning\nand the artificial intelligence\ndevelopment so let's take a look at some\nof the current applications going on in\nreal life and you can use your\nimagination to dig for some new ones\nthat we don't have listed here because\nit's so\nlimitless the amount of applications\nthat are being worked on right now or\nbeing implemented handwriting\nrecognition neural network is used to\nconvert handwritten characters into\ndigital characters that the system can\nrecognize\nstock exchange prediction if you've ever\nworked with stock exchange which i have\nit is so fickled to track i mean it is\nreally hard to understand there are many\nfactors that affect the stock market\nneural network can examine a lot of\nfactors and predict the prices on a\ndaily basis helping the stock brokers so\nright now it's still intro phase where\nit helps them and they really have to\nlook closely at it when you realize that\nwe generate over three terabytes a day\njust from the stock exchange here in the\nunited states that's a lot of data to\ndig through and you have to sort it out\nbefore you even start focusing on even\none stock traveling salesman problem it\nrefers to finding the optimal path to\ntravel between all cities in an area\nneural network helps solve the problem\nproviding higher revenue at a minimal\ncost logistics is huge just the\nlogistics that we talk about salesmen\ntraveling from town to town logistics\nare used by amazon amazon loves to ship\ntheir packages and they have empty space\non their trucks so they'll pre-ship\npackages and fill that empty space on\nwho they think will buy it saves them a\nlot of time and people are a lot happier\nbecause they get that tomorrow instead\nof having to wait three weeks image\ncompression idea behind data compression\nneural network is to store encrypt and\nrecreate the actual image again so we\ncan optimize our compression and data\nimages are the biggest one but it's\nusing all kinds of data wonderful\napplication to save a hard drive and to\noptimize being able to read it back out\nagain those are just a few and like i\nsaid use your mind to dig deeper and\nlet's take it even further we're going\nto go step further here and let's look\nat the future of deep learning here we\nare that's not me thank goodness\nwonderful person there reading her\ncrystal ball i'll tell you what i see in\nthe future\nmore personalized choices for users and\ncustomers all over the world i certainly\nlike that when i go in there and\nwhatever online ordering system starts\nreferring stuff to me local company here\nwhere i live that uses this where you\ncan take a picture and it starts looking\nfor what you want based on your picture\nso if you see a couch you like starts\nlooking for furniture like that or\nclothing i think it's mainly clothing\nhyper intelligent virtual assistants\nwill make life easier if you played with\ngoogle assistant or siri or any of those\nyou can see how they're slowly evolving\nand they're just now getting over that\nhump where a virtual assistant can do\nall kinds of things even pre-write your\nemail response for you new forms of\nalgorithm for learning methods would be\ndiscovered there's always something\nrolling out they've had some really cool\nresearch in this area again this stuff\nis such a we're just in the infant stage\nof artificial intelligence and neural\nnetworks and actually applying them to\nthe real world wonderful time to jump in\nneural networks will be a lot faster in\nthe future neural network tools will be\nembedded in every design surface we\nalready see that that you can buy a\nlittle mini neural network that plugs\ninto a really cheap processing board or\ninto your laptop so the hardware is\nstarting to come out that goes right in\nthere where you can dump it on there and\nthat makes it also faster so because\nit's on the hardware instead of the\nsoftware side neural networks will be\nused in the field of medicine\nagriculture physics discoveries just\neverything you can imagine we see this\ntoday where it's going from a phd\nstudent in medicine trying to understand\nt cells and understand the statistic\nanalysis of that to cure people to help\nkeep them healthy to help find out how\nwe heal to something that anybody can go\naccess and process the data on they're\nworking on shared data systems this\nconcept of it being used in these\ndifferent fields and these different\ndomains is huge the world's wide open\nfor anybody jumping out there to start\nexploring them and start learning neural\nnetworks let's dive in and say how does\na neural network work so now we've come\nfar enough to understand how neural\nnetwork works let's go ahead and walk\nthrough this a nice graphical\nrepresentation they usually describe a\nneural network as having different\nlayers you'll see that we've identified\na green layer an orange layer and a red\nlayer the green layer is the input so\nyou have your data coming in it picks up\nthe input signals and passes them to the\nnext layer the next layer does all kinds\nof calculations and feature extraction\nit's called the hidden layer a lot of\ntimes there's more than one hidden layer\nwe're only showing one in this picture\nbut we'll show you how it looks like in\na more detail a little bit and then\nfinally we have an output layer this\nlayer delivers the final result so the\nonly two things we see is the input\nlayer and the output layer now let's\nmake use of this neural network and see\nhow it works wonder how traffic cameras\nidentify vehicles registration plate on\nthe road to detect speeding vehicles and\nthose breaking the law they got me going\nthrough a red light the other day well\nlast month that's like the horrible\nthing they sent you this picture of you\nand all your information because they\npulled it up off of your license plate\nand your picture they shouldn't have\ngone through the red light so here we\nare and we have an image of a car you\ncan see the license plates on there so\nlet's consider the image of this vehicle\nand find out what's on the number plate\nthe picture itself is 28 by 28 pixels\nand the image is fed as an input to\nidentify the registration plate each\nneuron has a number called activation\nthat represents the grayscale value of\nthe corresponding pixel range and we\nrange it from zero to one one for a\nwhite pixel and zero for a black pixel\nand you can see down here we have an\nexample where one of the pixels is\nregistered as like 0.82 meaning it's\nprobably pretty dark each neuron is lit\nup when its activation is close to one\nso as we get closer to black on white we\ncan really start seeing the details in\nthere and you can see again the pixel\nshows this one up there it's like part\nof the car and so it lights up so pixels\nin the form of arrays are fed to the\ninput layer and so we see here the pixel\nof a car image fed as an input and\nyou're going to see that the input layer\nwhich is green is one dimension while\nour image is two dimension now when we\nlook at our setup that we're programming\nin python it has a cool feature that\nautomatically does the work for us if\nyou're working with an older neural\nnetwork pattern package you then convert\neach one of those rows so it's all one\narray so you'd have like row one and\nthen just tack row two on to the end you\ncan almost feed the image directly into\nsome of these neural networks the key is\nthough is that if you're using a 28 by\n28 and you get a picture of this 30 by\n30 shrink the 30 by 30 down to fit the\n28 by 28 so you can't increase the\nnumber of input in this case green dots\nit's very important to remember when you\nwork on neural networks and let's name\nthe inputs x1 x2 x3 respectively so each\none of those represents one of the\npixels coming in and the input layer\npasses it to the hidden layer and you\ncan see here we now have two hidden\nlayers in this image in the orange and\neach one of those pixels connects to\neach one of those hidden layers and the\ninterconnections are assigned weights at\nrandom so they get these random weights\nthat come through that if x1 lights up\nthen it's going to be x1 times this\nweight going into the hidden layer and\nwe sum those weights the weights are\nmultiplied with the input signal and a\nbias is added to all of them so as you\ncan see here we have x1 comes in and it\nactually goes to all the different\nhidden layer nodes or in this case\nwhatever you want to call them network\nsetup the orange dots and so you take\nthe value of x1 you multiply it by the\nweight for the next hidden layer so x1\ngoes to hidden layer one x1 goes to\nhidden layer two x1 goes hidden layer\none node two hidden layer one node three\nand so on and the bias a lot of times\nthey just put the bias in as like\nanother green dot or another orange dot\nand they give the bias a value one and\nthen all the weights go in from the bias\ninto the next node so the bias can\nchange we always just remember that you\nneed to have that bias in there there's\nthings that can be done with it\ngenerally most the packages out there\ncontrol that for you so you don't have\nto worry about figuring out what the\nbias is but if you ever dive deep into\nneural networks you got to remember\nthere's a bias or the answer won't come\nout correctly the weight weighted sum of\nthe input is fed as an input to the\nactivation function to decide which\nnodes to fire and for feature extraction\nas the signal flows within the hidden\nlayers the weighted sum of inputs is\ncalculated and is fed to the activation\nfunction in each layer to decide which\nnodes to fire so here's our feature\nextraction of the number plate and you\ncan see these are still hidden nodes in\nthe middle and this becomes important\nwe're going to take a little detour here\nand look at the activation function so\nwe're going to dive just a little bit\ninto the math so you can start to\nunderstand where some of the games go on\nwhen you're playing with neural networks\nin your programming so let's look at the\ndifferent activation functions before we\nmove ahead here's our friendly red tag\nshopping robot and so one is a sigmoid\nfunction and the sigmoid function which\nis one over one plus e to the minus x\ntakes the x value and you can see where\nit generates almost a zero and almost a\none with a very small area in the middle\nwhere it crosses over and we can use\nthat value to feed into another function\nso if it's really uncertain it might\nhave a 0.1 or 0.2 or 0.3 but for the\nmost part it's going to be really close\nto 1 and really close to this case 0 0\nto 1. the threshold function so if you\ndon't want to worry about the\nuncertainty in the middle you just say\noh if x is greater than or equal to zero\nif not then x is zero so it's either\nzero or one really straightforward\nthere's no in between in the middle and\nthen you have the what they call the\nrelu relu function and you can see here\nwhere it puts out the value but then it\nsays well if it's over one it's going to\nbe one and if it's less than zero it's\nzero so it kind of just dead ends it on\nthose two ends but allows all the values\nin the middle and again this like the\nsigmoid function allows that information\nto go to the next level so it might be\nimportant to know if it's a 0.1 or a\nminus 0.1 the next hidden layer might\npick that up and say oh this piece of\ninformation is uncertain or this value\nhas a very low certainty to it and then\nthe hyperbolic tangent function and you\ncan see here it's a 1 minus e to the\nminus 2x over 1 plus e to the minus 2x\nand it's very much along the same theme\na little bit different in here in that\nit goes between one and one so you'll\nsee some of these they go zero to one\nbut this one goes minus one to one and\nif it's less than zero it's you know it\ndoesn't fire and if it's over zero it\nfires and it also still puts out a value\nso you still have a value you can get\noff of that just like you can with the\nsigmoid function and the relu function\nvery similar in use and i believe the\noriginal used to be everything was done\nin the sigmoid function that was the\nmost commonly used and now they just\nkind of use more the relu function the\nreason is one it processes faster\nbecause you already have the value and\nyou don't have to add another compute\nthe one over one plus e to the minus x\nfor each hidden node and the data coming\noff works pretty good as far as putting\nit into the next level if you want to\nknow just how close it is to zero how\nclose is it not to functioning you know\nis it minus point one minus point two\nusually they're float values you get\nlike minus point minus .00138 or\nsomething so yeah important information\nbut the relu is most commonly used these\ndays as far as the setup we're using but\nyou'll also see the sigmoid function\nvery commonly used also now that you\nknow what an activation function is\nlet's get back to the neural network so\nfinally the model would predict the\noutcome of applying a suitable\nactivation function to the output layer\nso we go in here we look at this we have\nthe optical character recognition ocr is\nused on the images to convert it into a\ntext in order to identify what's written\non the plate and as it comes out you'll\nsee the red node and the red node might\nactually represent just a letter a so\nthere's usually a lot of outputs when\nyou're doing text identification we're\nnot going to show that on here but you\nmight have it even in the order it might\nbe what order the license plates in so\nyou might have a b c d e f g\nyou know the alphabet plus the numbers\nand you might have the one two three\nfour five six seven eight nine ten\nplaces so it's a very large array that\ncomes out it's not a small amount of you\nknow we show three dots coming in eight\nhidden layer nodes you know two sets of\nfour we just saw one red coming out a\nlot of times this is uh you know 28\ntimes 28 if you did 30 times 30 that's\nyou know 900 nodes so 28 is a little bit\nless than that just on the input and so\nyou can imagine the hidden layer is just\nas big each hidden layer is just as big\nif not bigger and the output is going to\nbe there's so many digits yeah it's a\nlot there's it's a huge amount of input\nand output but we're only showing you\njust you know it would be hard to show\nin one picture and so it comes up and\nthis is what it finally gets out on the\noutput as it identifies a number on the\nplate and in this case we have 0 8\nd\n0 3 8 5 8. error in the output is back\npropagated through the network and\nweights are adjusted to minimize the\nerror rate this is calculated by a cost\nfunction when we're training our data\nthis is what's used and we'll look at\nthat in the code when we do the data\ntraining so we have stuff we know the\nanswer to and then we put the\ninformation through and it says yes that\nwas correct or no because remember we\nrandomly set all the weights to begin\nwith and if it's wrong we take that\nerror how far off are you you know are\nyou off but is it if it was like minus\none you're just a little bit off if it's\nlike minus 300 was your output remember\nwhen we're looking at those different\noptions you know hyperbolic or whatever\nand we're looking at the rel the rel\ndoesn't have a limit on top or bottom it\nactually just generates a number so if\nit's way off you have to adjust those\nweights a lot but if it's pretty close\nyou might adjust the relates just a\nlittle bit and you keep adjusting the\nweights until they fit all the different\ntraining models you put in so you might\nhave 500 training models and those\nweights will adjust using the back\npropagation it sends the error backward\nthe output is compared with the original\nresult and multiple iterations are done\nto get the maximum accuracy so not only\ndoes it look at each one but it goes\nthrough it and just keeps cycling\nthrough these the data making small\nchanges in the network until it gets the\nright answers with every iteration the\nweights at every interconnection are\nadjusted based on the error we're not\ngoing to dive into that math because it\nis a differential equation and it gets a\nlittle complicated but i will talk a\nlittle bit about some of the different\noptions they have when we look at the\ncode so we've explored a neural network\nlet's look at the different types of\nartificial neural networks and this is\nlike the biggest area growing is how\nthese all come together let's see the\ndifferent types of neural network and\nagain we're comparing this to human\nlearning so\nhere's a human brain i feel sorry for\nthat poor guy so we have a feed for\nforward neural network simplest form of\na they call it a n a neural network data\ntravels only in one direction input to\noutput this is what we just looked at so\nas the data comes in all the weights are\nadded it goes to the hidden layer all\nthe weights are added it goes to the\nnext hidden layer all the weights are\nadded and it goes to the output the only\ntime you use the reverse propagation is\nto train it so when you actually use it\nit's very fast when you're training it\nit takes a while because it has to\niterate through all your training data\nand you start getting into big data\nbecause you can train these with a huge\namount of data the more data you put in\nthe better train they get the\napplications vision and speech\nrecognition actually they're pretty much\neverything we talked about a lot almost\nall of them used this form of neural\nnetwork at some level radial basis\nfunction neural network this model\nclassifies the data point based on its\ndistance from a center point what that\nmeans is that you might not have\ntraining data so you want to group\nthings together and you create central\npoints and it looks for all the things\nyou know some of these things are just\nlike the other if you've ever watched a\nsesame street as a kid that dates me so\nit brings things together and this is a\ngreat way if you don't have the right\ntraining model you can start finding\nthings that are connected you might not\nhave noticed before applications power\nrestoration systems they try to figure\nout what's connected and then based on\nthat they can fix the problem if you\nhave a huge power system cajon and\nself-organizing neural network vectors\nof random dimensions are input to\ndiscrete map comprised of neurons so\nthey basically find a way to draw they\ncall them they say dimensions or vectors\nor planes because they actually chop the\ndata in one dimension two dimension\nthree dimension four five six they keep\nadding dimensions and finding ways to\nseparate the data and connect different\ndata pieces together applications used\nto recognize patterns and data like in\nmedical analysis the hidden layer saves\nits output to be used for future\nprediction recurrent neural networks so\nthe hidden layers remember its output\nfrom last time and that becomes part of\nits new input you might use that\nespecially in robotics or flying a drone\nyou want to know what your last change\nwas and how fast it was going to help\npredict what your next change you need\nto make is to get to where the drone\nwants to go applications text-to-speech\nconversation model so you know i talked\nabout drones but you know just\nidentifying on lexis or google assistant\nor any of these they're starting to add\nin i'd like to play a song on my pandora\nand i'd like it to be at volume 90\nso you now can add different things in\nthere and it connects them together the\ninput features are taken in batches like\na filter this allows a network to\nremember an image in parts convolution\nneural network today's world in photo\nidentification and taking apart photos\nand trying to you know you ever seen\nthat on google where you have five\npeople together this is the kind of\nthing separates all those people so then\nit can do a face recognition on each\nperson applications used in signal and\nimage processing in this case i use\nfacial images or google picture images\nas one of the options modular neural\nnetwork it has a collection of different\nneural networks working together to get\nthe output so wow we just went through\nall these different types of neural\nnetworks and the final one is to put\nmultiple neural networks together i\nmentioned that a little bit when we\nseparated people in a larger photo in\nindividuals in the photo and then do the\nfacial recognition on each person so one\nnetwork is used to separate them and the\nnext network is then used to figure out\nwho they are and do the facial\nrecognition applications still\nundergoing research this is a cutting\nedge you hear the term pipeline and\nthere's actual in python code and in\nalmost all the different neural network\nsetups out there they now have a\npipeline feature usually and it just\nmeans you take the data from one neural\nnetwork and maybe another neural network\nor you put it into the next neural\nnetwork and then you take three or four\nother neural networks and feed them into\nanother one so how we connect the neural\nnetworks is really just cutting edge and\nit's so experimental i mean it's almost\ncreative in its nature there's not\nreally a science to it because each\nspecific domain has different things\nit's looking at so if you're in the\nbanking domain it's going to be\ndifferent than the medical domain than\nthe automatic car domain and suddenly\nfiguring out how those all fit together\nis just a lot of fun and really cool so\nwe have our types of artificial neural\nnetwork we have our feed forward neural\nnetwork we have a radial basis function\nneural network we have our cohenon\nself-organizing neural network recurrent\nneural network convolution neural\nnetwork and modular neural network where\nit brings them all together and no the\ncolors on the brain do not match what\nyour brain actually does but they do\nbring it out that most of these were\ndeveloped by understanding how humans\nlearn and as we understand more and more\nof how humans learn we can build\nsomething in the computer industry to\nmimic that to reflect that and that's\nhow these were developed so exciting\npart use case problem statement so this\nis where we jump in this is my favorite\npart let's use the system to identify\nbetween a cat and a dog if you remember\ncorrectly i said we're going to do some\npython code and you can see over here my\nhair is kind of sticking up over the\ncomputer cup of coffee on one side and a\nlittle bit of old school a pencil and a\npen on the other side yeah most people\nnow take notes\ni love the stickies on the computer\nthat's great that's that is my computer\ni have sticky notes on my computer in\ndifferent colors so not too far from\ntoday's programmer so the problem is is\nwe want to classify photos of cats and\ndogs using a neural network and you can\nsee over here we have quite a variety of\ndogs in the pictures and cats and you\nknow just sorting out it is a cat it's\npretty amazing and why would anybody\nwant to even know the difference between\na cat and a dog okay you know why well i\nhave a cat door it'd be kind of fun that\ninstead of it identifying instead of\nhaving like a little collar with a\nmagnet on it which is what my cat has\nthe door would be able to see oh that's\nthe cat that's our cat coming in oh\nthat's the dog we have a dog too that's\na dog i want to let in maybe i don't\nwant to let this other animal in because\nit's a raccoon so you can see where you\ncould take this one step further and\nactually apply this you could actually\nstart a little startup company idea\nself-identifying door so this use case\nwill be implemented on python i am\nactually in python\n3.6 it's always nice to tell people the\nversion of python because it does affect\nsometimes which modules you load and\neverything and we're going to start by\nimporting the required packages i told\nyou we're going to do this in cross so\nwe're going to import from karas models\nsequential from the cross layers\nconversion 2d or conv 2d max pooling 2d\nflatten and dense and we'll talk about\nwhat each one of these do in just a\nsecond but before we do that let's talk\na little bit about the environment we're\ngoing to work in and you know in fact\nlet me go ahead and open a\nthe website cross's website so we can\nlearn a little bit more about cross so\nhere we are on the cross website and\nit's a k-e-r-a-s dot io that's the\nofficial website for karass and the\nfirst thing you'll notice is that cross\nruns on top of either tensorflow\ncntk and i think it's pronounced thanos\nor thiano what's important on here is\nthat tensorflow and the same is true for\nall these but tensorflow is probably one\nof the most widely used currently\npackages out there with the cross and of\ncourse you know tomorrow this is all\ngoing to change it's all going to\ndisappear and they'll have something new\nout there so make sure when you're\nlearning this code that you understand\nwhat's going on and also know the code i\nmean look when you look at the code it's\nnot as complicated once you understand\nwhat's going on the code itself is\npretty straightforward and the reason we\nlike karas and the reason that people\nare jumping on it right now is such a\nbig deal is if we come down here let me\njust scroll down a little bit we talk\nabout user friendliness modularity easy\nextensibility work with python python is\na big one because a lot of people in\ndata science now use python although you\ncan actually access cross other ways if\nwe continue down here is layers and this\nis where it gets really cool when we're\nworking with cross you just add layers\non remember those hidden layers we were\ntalking about and we talked about the\nrelu\nactivation you can see right here let me\njust up that a little bit in size there\nwe go that's big i can add in an relu\nlayer and then i can add in a soft max\nlayer the next instance we didn't talk\nabout soft max so you can do each layer\nseparate now if i'm working in some of\nthe other kits i use i take that and i\nhave one setup and then i feed the\noutput into the next one this one i can\njust add hidden layer after hidden layer\nwith the different information in it\nwhich makes it very powerful and very\nfast to spin up and try different setups\nand see how they work with the data\nyou're working on and we'll dig a little\nbit deeper in here and a lot of this is\nvery much the same so when we get to\nthat part i'll point that out to you\nalso now just a quick side note i'm\nusing anaconda with python in it and i\nwent ahead and created my own package\nand i called it the cross python36\nbecause i'm in python36 anaconda is cool\nthat way you can create different\nenvironments real easily if you're doing\na lot of different experimenting with\nthese different packages probably want\nto create your own environment in there\nand the first thing is you can see right\nhere there's a lot of dependencies a lot\nof these you should recognize by now if\nyou've done any of these videos if not\nkudos for you for jumping in today pip\ninstall numpy scipy the scikit learn\npillow and h5py are both needed for the\ntensorflow and then putting the cross on\nthere and then you'll see here and pip\nis just a standard installer that you\nuse with python you'll see here that we\ndid pip install tensorflow since we're\ngoing to do cross on top of tensorflow\nand then pip install and i went ahead\nand used the github so git plus get and\nyou'll see here github.com this is one\nof their releases one of the most\ncurrent release on there that goes on\ntop of tensorflow you can look up these\ninstructions pretty much anywhere this\nis for doing it on anaconda certainly\nyou'd want to install these if you're\ndoing it in ubuntu server setup you want\nto get i don't think you need the h5py\nand ubuntu but you do need the rest in\nthere because they are dependencies in\nthere and it's pretty straightforward\nand that's actually in some of the\ninstructions they have on their website\nso you don't have to initially go\nthrough this just remember their website\non there and then when i'm under my\nanaconda navigator which i like you'll\nsee where i have environments and on the\nbottom i created a new environment and i\ncalled it cross python 36 just to\nseparate everything you can say i have\npython 30.5 and python 36. i used to\nhave a bunch of other ones but it kind\nof cleaned house recently and of course\nonce i go in here i can launch my\njupiter notebook making sure i'm using\nthe right environment that i just set up\nthis of course opens up my in this case\ni'm using google chrome and in here i\ncan go and just create a new document in\nhere and this is all in your browser\nwindow when you use the anaconda do you\nhave to use anaconda and jupiter\nnotebook no you can use any kind of\npython editor whatever setup you're\ncomfortable with and whatever you're\ndoing in there so let's go ahead and go\nin here and paste the code in and we're\nimporting a number of different settings\nin here we have import sequential that's\nunder the models because that's the\nmodel we're going to use as far as our\nneural network and then we have layers\nand we have conversion 2d max pooling 2d\nflatten dense and you can actually just\nkind of guess at what these do we're\ntalking we're working in a 2d photograph\nand if you remember correctly i talked\nabout how the actual input layer is a\nsingle array it's not in two dimensions\nit's one dimension all these do is these\nare tools to help flatten the image so\nit takes a two-dimensional image and\nthen it creates its own proper setup you\ndon't have to worry about any of that\nyou don't have to do anything special\nwith the photograph you let the cross do\nit we're going to run this and you'll\nsee right here they have some stuff that\nis going to be depreciated and changed\nbecause that's what it does everything's\nbeing changed as we go you don't have to\nworry about that too much if you have\nwarnings if you run it a second time the\nwarning will disappear and this has just\nimported these packages for us to use\njupiter is nice about this you can do\neach thing step by step and i'll go\nahead and also zoom in there a little\ncontrol plus that's one of the nice\nthings about being in a browser\nenvironment so here we are back another\nsip of coffee\nif you're familiar with my other videos\nyou notice i'm always sipping coffee i\nalways have a in my case latte next to\nme an espresso so the next step is to go\nahead and initialize we're going to call\nit the cnn or classifier neural network\nand the reason we call it a classifier\nis because it's going to classify it\nbetween two things it's going to be cat\nor dog so when you're doing\nclassification you're picking specific\nobjects you're specific it's a true or\nfalse yes no it is something or it's not\nso first thing we're going to create our\nclassifier and it's going to equal\nsequential so their sequential setup is\nthe classifier that's the actual model\nwe're using that's the neural network so\nwe call it a classifier and the next\nstep is to add in our convolution and\nlet me just do a shrink that down in\nsize you can see the whole line and\nlet's talk a little bit about what's\ngoing on here i have my classifier and i\nadd something what am i adding well i'm\nadding my first layer this first layer\nwe're adding in is probably the one that\ntakes the most work to make sure you\nhave it set correct and the reason i say\nthat this is your actual input and we're\ngoing to jump here to the part that says\ninput shape equals 64 by 64 by 3. what\ndoes that mean well that means that our\npictures coming in and there's these\npictures remember we had like the\npicture of the car was 128 by 128 pixels\nwell this one is 64 by 64 pixels and\neach pixel has three values that's where\nthese numbers come from and it is so\nimportant that this matches i mentioned\na little bit that if you have like a\nlarger picture you have to reformat it\nto fit this shape if it comes in as\nsomething larger there's no input notes\nthere's no input neural network there\nthat will handle that extra space so you\nhave to reshape your data to fit in here\nnow the first layer is the most\nimportant because after that keras knows\nwhat your shape is coming in here and it\nknows what's coming out and so that\nreally sets the stage most important\nthing is that input shape matches your\ndata coming in and you'll get a lot of\nerrors if it doesn't you'll go through\nthere and picture number 55 doesn't\nmatch it correctly and guess what it\ndoes it usually gives you an error and\nthen the activation if you remember we\ntalked about the different activations\non here we're using the relu model like\ni said that is the most commonly used\nnow because one it's fast it doesn't\nhave the added calculations in it it\njust says here's the value coming out\nbased on the weights and the value going\nin and um from there you know it's uh if\nit's over one then it's good or over\nzero it's good if it's under zero then\nit's considered not active and then we\nhave this conversion 2d what the heck is\nconversion 2d i'm not going to go into\ntoo much detail in this because this has\na couple of things it's doing in here a\nlittle bit more in depth than we're\nready to cover in this tutorial but this\nis used to convert from the photo\nbecause we have 64 by 64 by three and\nwe're just converting it to two\ndimensional kind of setup so it's very\naware that this is a photograph and that\ndifferent pieces are next to each other\nand then we're going to add in a second\nconvolutional layer that's what the conv\nstands for 2d so it's these are hidden\nlayers so we have our input layer and\nour two hidden layers and they are two\ndimensional because we're doing with a\ntwo dimensional photograph and you'll\nsee down here that on the last one we\nadd a max pooling 2d and we put a pool\nsize equals 2 2. and so what this is is\nthat as you get to the end of these\nlayers one of the things you always want\nto think of is what they call mapping\nand then reducing wonderful terminology\nfrom the big data we're mapping this\ndata through all these layers and now we\nwant to reduce it to only two sets in\nthis case it's already in two sets\nbecause it's a 2d photograph but we had\nyou know two dimensions by we actually\nhave 64 by 64 by three so now we're just\ngetting it down to a two by two just the\ntwo dimension two dimensional instead of\nhaving the third dimension of colors and\nwe'll go ahead and run these we're not\nreally seeing anything in our run script\nbecause we're just setting up this is\nall set up and this is where you start\nplaying because maybe you'll add a\ndifferent layer in here to do something\nelse to see how it works and see what\nyour output is that's what makes cross\nso nice is i can with just a couple\nflips of code put in a whole new layer\nthat does a whole new processing and see\nwhether that improves my run or makes it\nworse and finally we're going to do the\nfinal setup which is to flatten\nclassifier add a flattened setup and\nthen we're going to also add a layer a\ndense layer and then we're going to add\nin another dense layer and then we're\ngoing to build it we're going to compile\nthis whole thing together so let's flip\nover and see what that looks like and\nwe've even numbered them for you so\nwe're going to do the flattening and\nflatten is exactly what it sounds like\nwe've been working in a two-dimensional\narray of picture which actually is in\nthree dimensions because the pixels the\npixels have a whole other dimension to\nit of three different values and we've\nkind of resized those down to two by two\nbut now we're just going to flatten it i\ndon't want to have multiple dimensions\nbeing worked on by tensor and by keras i\nwant just a single array so it's\nflattened out and then step four full\nconnection so we add in our final two\nlayers and you could actually do all\nkinds of things with this you could\nactually leave out this some of these\nlayers and play with them you do need to\nflatten it that's very important then we\nwant to use the dents again we're taking\nthis and we're taking whatever came into\nit so once we take all those different\nthe two dimensions or three dimensions\nas they are and we flatten it to one\ndimension we want to take that and we're\ngoing to pull it into units of 128. they\ngot that you say where did they get 128\nfrom you could actually play with that\nnumber and get all kinds of weird\nresults but in this case we took the 64\nplus 64 is 128. you could probably even\ndo this with 64 or 32. usually you want\nto keep it in the same multiple whatever\nthe data shape you're already using is\nin and we're using the activation the\nrelu just like we did before and then we\nfinally filter all that into a single\noutput and it has how many units one why\nbecause we want to know whether true or\nfalse it's either a dog or a cat you\ncould say one is dog zero is cat or\nmaybe you're a cat lover and it's one is\ncat and zero is dog if you love both\ndogs and cats you're gonna have to\nchoose and then we use the sigmoid\nactivation if you remember from before\nwe had the relu and there's also the\nsigmoid the sigmoid just makes it clear\nit's yes or no we don't want any kind of\nin-between number coming out and we'll\ngo ahead and run this and you'll see\nit's still all in setup and then finally\nwe want to go ahead and compile and\nlet's put the compiling our classifier\nneural network and we're going to use\nthe optimizer atom and i hinted at this\njust a little bit before where does adam\ncome in where does an optimizer come in\nwell the optimizer is the reverse\npropagation when we're training it it\ngoes all the way through and says error\nand then how does it readjust those\nweights there are a number of them atom\nis the most commonly used and it works\nbest on large data most people stick\nwith the atom because when they're\ntesting on smaller data see if their\nmodel is going to go through and get all\ntheir errors out before they run it on\nlarger data sets they're going to run it\non atom anyway so they just leave it on\natom most commonly used but there are\nsome other ones out there you should be\naware of that that you might try them if\nyou're stuck in a bind or you might blur\nthat in the future but usually atom is\njust fine on there and then you have two\nmore settings you have loss and metrics\nwe're not going to dig too much into\nloss or metrics these are things you\nreally have to explore cross because\nthere are so many choices this is how it\ncomputes the error there's so many\ndifferent ways to on your back\npropagation and your training so we're\nusing the atom model but you can compute\nthe error by standard deviation standard\ndeviation squared they use binary cross\nentropy i'd have to look that up to even\nknow what that is there's so many of\nthese a lot of times you just start with\nthe ones that look correct that are most\ncommonly used and then you have to go\nread the cross site and actually see\nwith these different losses and metrics\nand what different options they have so\nwe're not going to get too much into\nthem other than to reference you over to\nthe cross website to explore them deeper\nbut we are going to go ahead and run\nthem and now we've set up our classifier\nso we have an object classifier and if\nyou go back up here you'll see that\nwe've added in step one we added in our\nlayer for the input we added a layer\nthat comes in there and uses the relu\nfor activation and then it pulls the\ndata so this is even though these are\ntwo layers the actual neural network\nlayer is up here and then it uses this\nto pull the data into a two by two so\ninto a two-dimensional array from a\nthree-dimensional array with the colors\nthen we flatten it so there's our outer\nflattened and then we add another dense\nwhat they call dense layer this dense\nlayer goes in there and it downsizes it\nto 128 it reduces it so you can look at\nthis as we're mapping all this data down\nthe two dimensional setup and then we\nflatten it so we map it to a flattened\nmap and then we take it and reduce it\ndown to 128 and we use the relu again\nand then finally we reduce that down to\njust a single output and we use the\nsigmoid to do that to figure out whether\nit's yes no true false in this case cat\nor dog and then finally once we put all\nthese layers together we compile them\nthat's what we've done here and we've\ncompiled them as far as how it trains to\nuse these settings for the training back\npropagation so if you remember we talked\nabout training our setup and when we go\ninto this you'll see that we have two\ndata sets we have one called the\ntraining set and the testing set and\nthat's very standard in any data\nprocessing is you need to have that's\npretty common in any data processing is\nyou need to have a certain amount of\ndata to train it and then you got to\nknow whether it works or not is it any\ngood and that's why you have a separate\nset of data for testing it we already\nknow the answer but you don't want to\nuse that as part of the training set so\nin here we jump into part two fitting\nthe classifier neural network to the\nimages and then from cross and let me\njust zoom in there i always love that\nabout working with jupiter notebooks you\ncan really see we're going to come in\nhere we do the cross pre-processing and\nimage and we import image data generator\nso nice of cross it's such a high-end\nproduct right now going out and since\nimages are so common they already have\nall this stuff to help us process the\ndata which is great and so we come in\nhere we do train data gin and we're\ngoing to create our object for helping\nus train it for reshaping the data so\nthat it's going to work with our setup\nand we use an image data generator and\nwe're going to rescale it and you'll see\nhere we have one point which tells us\nit's a float value on the rescale over\n255. where does 255 come from well\nthat's the scale in the colors of the\npictures we're using their value from 0\nto 255. so we want to divide it by 255\nand it'll generate a number between 0\nand 1. they have shear range and zoom\nrange horizontal flip equals true and\nthis of course has to do with if the\nphotos are different shapes and sizes\nlike i said it's a wonderful package you\nreally need to dig in deep to see all\nthe different options you have for\nsetting up your images for right now\nthough we're going to stick with some\nbasic stuff here and let me go ahead and\nrun this code and again it doesn't\nreally do anything because we're still\nsetting up the pre-processing let's take\na look at this next set of code and this\none is just huge we're creating the\ntraining set so the training set is\ngoing to go in here and it's going to\nuse our train data gen we just created\ndot flow from directory it's going to\naccess in this case the path data set\ntraining set that's a folder so it's\ngoing to pull all the images out of that\nfolder now i'm actually running this in\nthe folder that the data sets in so if\nyou're doing the same setup and you load\nyour data in there and you're doing this\nmake sure wherever your jupyter notebook\nis saving things to that you create this\npath or you can do the complete path if\nyou need to you know c colon slash etc\nand the target size the batch size and\nclass mode is binary so the class is\nwe're switching everything to a binary\nvalue batch size what the heck is batch\nsize well that's how many pictures we're\ngoing to batch through the training each\ntime and the target size 64 by 64.\nlittle confusing but you can see right\nhere that this is just a general\ntraining and you can go in there and\nlook at all the different settings for\nyour training set and of course with\ndifferent data we're doing pictures\nthere's all kinds of different settings\ndepending on what you're working with\nlet's go ahead and run that and see what\nhappens and you'll see that it found 800\nimages belonging to one classes so we\nhave 800 images in the training set and\nif we're going to do this with the\ntraining set we also have to format the\npictures in the test set now we're not\nactually doing any predictions we're not\nactually programming the model yet\nall we're doing is preparing the data so\nwe're going to prepare a training set\nand the test set so any changes we make\nto the training set at this point also\nhave to be made to the test set so we've\ndone this thing we've done a train data\ngenerator we've done our training set\nand then we also have remember our test\nset of data so i'm going to do the same\nthing with that i'm going to create a\ntest data gen and we're going to do this\nimage data generator we're going to\nrescale 1 over 255 we don't need the\nother settings just the single setting\nfor the test datagen and we're going to\ncreate our test set we're going to do\nthe same thing we did with the test set\nexcept that we're pulling it from the\ntest set folder and we'll run that and\nyou'll see in our test set we found 2\n000 images that's about right we're\nusing 20 percent of the images as test\nand 80 to train it and then finally\nwe've set up all our data we've set up\nall our layers which is where all the\nwork is is cleaning up that data making\nsure it's going in there correctly and\nwe are actually going to fit it we're\ngoing to train our data set and let's\nsee what that looks like and here we go\nlet's put the information in here and\nlet's just take a quick look at what\nwe're looking at with our fit generator\nwe have our classifier dot fit generator\nthat's our back propagation so the\ninformation goes through forward with a\npicture and it says oh you're either\nright or you're wrong and then the air\ngoes backward and reprograms all those\nweights so we're training our neural\nnetwork and of course we're using the\ntraining set remember we created the\ntraining set up here and then we're\ngoing steps per epic so it's 8 000 steps\nepic means that that's how many times we\ngo through all the pictures so we're\ngoing to rerun each of the pictures and\nwe're going to go through the whole data\nset 25 times but we're going to look at\neach picture during each epic 8 000\ntimes so we're really programming the\nheck out of this and going back over it\nand then they have validation data\nequals test set so we have our training\nset and then we're gonna have our test\nset to validate it so we're gonna do\nthis all in one shot and we're gonna\nlook at that and they're gonna do 200\nsteps for each validation and we'll see\nwhat that looks like in just a minute\nlet's go ahead and run our training here\nand we're going to fit our data and as\nit goes it says epic one of 25 you start\nrealizing that this is going to take a\nwhile on my older computer it takes\nabout 45 minutes i have a dual processor\nwe're processing uh 10 000 photos that's\nnot a small amount of photographs to\nprocess so if you're on your laptop in\nwhich i am it's going to take a while so\nlet's go ahead and go get our cup of\ncoffee and a sip and come back and see\nwhat this looks like so i'm back you\ndidn't know i was gone that was actually\na lengthy pause there i made a couple\nchanges and let's discuss those changes\nreal quick and why i made them so the\nfirst thing i'm going to do is i'm going\nto go up here and insert a cell above\nand let's paste the original code back\nin there and you'll see that the\noriginal thing was steps per epic eight\nthousand twenty five epics and\nvalidation steps two thousand and i\nchanged these to four thousand epics or\nfour thousand steps per epic\n10 epics and just 10 validation steps\nand this will cause problems if you're\ndoing this as a commercial release but\nfor demo purposes this should work and\nif you remember our steps per epic\nthat's how many photos we're going to\nprocess in fact let me go ahead and get\nmy drawing pin out and let's just\nhighlight that right here well we have 8\n000 pictures we're going through so for\neach epic i'm going to change this to 4\n000 i'm going to cut that in half so\nit's going to randomly pick 4 000\npictures each time it goes through an\nepic and the epic is how many processes\nso this is 25 and i'm just going to cut\nthat to 10. so instead of doing 25 runs\nthrough 8 000 photos each which you can\ndo the math of 25 times 8 000. i'm only\ngoing to do 10 through 4 000 so i'm\ngoing to run this 40 000 times through\nthe processes and the next thing i know\nthat you'll you'll want to notice is\nthat i also change the validation step\nand this would cause some major problems\nin releasing because i dropped it all\nthe way down to 10. what the validation\nstep does is it says we have 2 000\nphotos in our trainings or in our\ntesting set and we're going to use that\nfor validation well i'm only going to\nuse a random 10 of those to validate so\nnot really the best settings but let me\nshow you why we did that\nlet's scroll down here just a little bit\nand let's look at the output here and\nsee what that what's going on there so\ni've got my drawing tool back on and\nyou'll see here it lists a run so each\ntime it goes through an epic it's going\nto do 4 000 steps and this is where the\n4 000 comes in so that's what we have we\nhave epic one of 10 4 000 steps it's\nrandomly picking half the pictures in\nthe file and going through them and then\nwe're going to look at this number right\nhere that is for the whole epic and\nthat's 2411\nseconds and if you remember correctly\nyou divide that by 60 you get minutes if\nyou divide that by 60 you get hours or\nyou can just divide the whole thing by\n60 times 60 which is 3600. if 3600 is an\nhour this is roughly 45 minutes right\nhere and that's 45 minutes to process\nhalf the pictures so if i was doing all\nthe pictures we're talking an hour and a\nhalf per epic times 36 or no 25 they had\n25 up above 25 so that's roughly a\ncouple days a couple days of processing\nwell for this demo we don't want to do\nthat i don't want to come back the next\nday plus my computer did a reboot in the\nmiddle of the night so we look at this\nand we say okay let's we're just testing\nthis out my computer that i'm running\nthis on is a dual core processor runs\npoint nine gigahertz per second for a\nlaptop you know it's good about four\nyears ago but for running something like\nthis is probably a little slow so we cut\nthe times down and the last one was\nvalidation we're only validating it on a\nrandom 10 photos and this comes into\neffect because you're going to see down\nhere where we have accuracy value loss\nvalue accuracy and loss those are very\nimportant numbers to look at so the 10\nmeans i'm only validating across 10\npictures that is where here we have\nvalue this is acc is for accuracy value\nloss we're not going to worry about that\ntoo much and accuracy now accuracy is\nwhile it's running it's putting these\ntwo numbers together that's what\naccuracy is and value accuracy is at the\nend of the epic what's our accuracy into\nthe epic what is it looking at in this\ntutorial we're not going to go so deep\nbut these numbers are really important\nwhen you start talking about\nthese two numbers reflect bias that is\nreally important i'll just put that up\nthere and bias is a little bit beyond\nthis tutorial but the short of it is is\nif this accuracy which is being our\nvalidation per step is going down and\nthe value accuracy continues to go up\nthat means there's a bias that means i'm\nmemorizing the photos i'm looking at i'm\nnot actually looking for what makes a\ndog a dog what makes a cat a cat i'm\njust memorizing them and so the more\nthis discrepancy grows the bigger the\nbias is and that is really the beauty of\nthe cross\nneural network is a lot of built-in\nfeatures like this that make that really\neasy to track so let's go ahead and take\na look at the next set of code so here\nwe are into part three we're going to\nmake a new prediction and so we're going\nto bring in a couple tools for that and\nthen we have to process the image coming\nin and find out whether it's an actual\ndog or cat we can actually use this to\nidentify it and of course the final step\nof part three is to print prediction\nwe'll go ahead and combine these and of\ncourse you can see me there adding more\nsticky notes to my computer screen\nhidden behind the screen and\nlast one was don't forget to feed the\ncat and the dog\nso let's go and take a look at that and\nsee what that looks like in code and put\nthat in our jupiter notebook all right\nand let's paste that in here and we'll\nstart by importing numpy as np numpy is\na very common package i pretty much\nimport it on any python project i'm\nworking on another one i use regularly\nis pandas they're just ways of\norganizing the data and then np is\nusually the standard in most machine\nlearning tools as a return for the data\narray although you know you can use the\nstandard data array from python and we\nhave cross pre-processing import image\nfish that all look familiar because\nwe're going to take a test image and\nwe're going to set that equal to in this\ncase cat or dog one as you can see over\nhere and you know let me get my drawing\ntool back on so let's take a look at\nthis we have our test image we're\nloading and in here we have test image\none and this one hasn't data hasn't seen\nthis one at all so this is all new oh\nlet me shrink the screen down let me\nstart that over so here we have my test\nimage and we went ahead and the cross\nprocessing has this nice image set up so\nwe're going to load the image and we're\ngoing to alter it to a 64 by 64 print so\nright off the bat we're going to cross\nas nice that way it automatically sets\nit up for us so we don't have to redo\nall our images and find a way to reset\nthose and then we use also to set the\nimage to an array so again we're all in\npre-processing the data just like we\npre-processed before with our test\ninformation and our training data and\nthen we use the numpy here's our numpy\nthat's uh from our right up here\nimportant numpy as in p expand the\ndimensions test image axes equals 0. so\nit puts it into a single array and then\nfinally\nall that work all that pre-processing\nand all we do is we run the result we\nclick on here we go result equals\nclassifier predict test image and then\nwe find out well what is the test image\nand let's just take a quick look and\njust see what that is and you can see\nwhen i ran it it comes up dog and if we\nlook at those images there it is cat or\ndog image number one that looks like a\nnice floppy eared lab\nfriendly with his tongue hanging out\nit's either that or a very floppy eared\ncat i'm not sure which but according to\nour software says it's a dog and uh we\nhave a second picture over here let's\njust see what happens when we run the\nsecond picture we can go up here and\nchange this uh from dog image one to two\nwe'll run that and it comes down here\nand says cat you can see me highlighting\nit down there as cat so our process\nworks we are able to label a dog a dog\nand a cat a cat just from the pictures\nthere we go cleared my drawing tool and\nthe last thing i want you to notice when\nwe come back up here to when i ran it\nyou'll see that has an accuracy of 1 and\nthe value accuracy of 1. well the value\naccuracy is the important one because\nthe value accuracy is what it actually\nruns on the test data remember i'm only\ntesting it on i'm only validating it on\na random 10 photos and those 10 folders\njust happened to come up one now when\nthey ran this on the server it actually\ncame up about 86 percent this is why\ncutting these numbers down so far for a\ncommercial release is bad so you want to\nmake sure you're a little careful of\nthat when you're testing your stuff that\nyou change these numbers back when you\nrun it on a more enterprise computer\nother than your old laptop that you're\njust practicing on or messing with and\nwe come down here and again you know we\nhad the validation of cat and so we have\nsuccessfully built a neural network that\ncould distinguish between photos of a\ncat and a dog imagine all the other\nthings you could distinguish imagine all\nthe different industries you could dive\ninto with that just being able to\nunderstand those two difference of\npictures what about mosquitoes could you\nfind the mosquitoes that bite versus\nmosquitoes that are friendly it turns\nout the mosquitoes that bite us are only\nfour percent of the mosquito population\nif even that maybe two percent there's\nall kinds of industries that use this\nand there's so many industries that are\njust now realizing how powerful these\ntools are just in the photos alone there\nis a myriad of industries sprouting up\nand i said it before i'll say it again\nwhat an exciting time to live in with\nthese tools and that we get to play with\nback propagation and gradient descent\nwe're talking neural networks so we talk\nabout the neural network this is a\nsimple neural network which must be\ntrained to recognize handwritten\nalphabets a b and c and you can see here\nwe have our input coming in in this case\nwe'll look at the letter a written out\non a 28 by 28 pixels so the handwritten\noutfits are presented as images of 28 by\n28 pixels and that image comes in in\nthis case we have 784 neurons that's 28\ntimes 28 and the initial prediction is\nmade using random weights assigned to\neach channel and so we have our forward\npropagation as you see here so each node\nis then their values are added up and\nadded up and so on going across and our\nnetwork predicts the input to be b with\nthe probability of 0.5 the predicted\nprobabilities are compared against the\nactual probabilities and the errors\ncalculated so the error is simply the\nactual minus predicted and you can see\nhere where we know it's not c so it's a\nminus 2 we know it's not b so it's a\nminus 0.5 but we do know that it is a\na so we go ahead and adjust that by 0.6\nthe magnitude indicates the amount of\nchange while the sign indicates an\nincrease or decrease in the weights the\ninformation is transmitted back through\nthe network so here comes our back\npropagation weights throughout the\nnetwork are adjusted in order to reduce\nthe loss in prediction so if we look at\nthis setup right here here comes our\nerror in this case 0.6 minus 0.5 minus\n0.2 that comes up and adjusts our 1.4.9\nall our multipliers in this manner we\nkeep training the network with multiple\ninputs until it is able to predict with\na high accuracy and you can see here we\nhave a different a quickly switches from\ncursive a to maybe more elongated a\nsimilarly our network is trained with\nthe images for b and c two so let's take\na look at this uh here's a\nstraightforward data sets let's build a\nneural network to predict the outputs\ngiven the inputs and so we have an input\nzero we expect an output of zero we have\nan input of one we expect six two equals\ntwelve three should come out as eighteen\nand four as twenty-four and we're just\ndoing multiples of six if you take the\ntime to look at it so in our example we\nhave our input and it goes into our\nneural network so this box represents\nour neural network one of the cool\nthings about neural networks is there\nalways this little black box that you\nkind of train to do what you want and\nyou really don't have to know exactly\nwhat the weights are although there are\nsome very high end setups to start\nlooking at those weights and how they\nwork and what they do and then you get\nyour output which is going to be in this\ncase our input is going to be x and our\noutput is going to be y and w is the\nweight so we have a value times the\nweight so if we're doing in this case\njust a single neuron going through we\nhave x times w the network starts\ntraining itself by choosing a random\nvalue for w we're going to guess that w\nequals three just roll the dice randomly\ngenerate the number three for w and then\nwe put w equals three in here we have\nour input zero or output zero w equals\nthree equals zero so we have no error on\nthe first line that actually comes out\ncorrect and then we have one times uh we\nput the 1 in we're looking for a 6 but\nwe get a 3 instead when we put the 2 in\nwe're looking for 12 we get a 6 in set\nso you can see here our predicted output\ndoesn't match the output we're looking\nfor and then we take this we have our w\nequals three we come up with the second\nmodel where the w equals six now we're\ngoing to look at how we figure out w\nequals six in just a minute that is part\nof the math behind this but you can see\nhere we put in w equals six and we build\nthe w equals 6 chart we end up with 0 6\n12 18 24 which is the output we're\nlooking for and in that manner we end up\nwith the correct answer but we'll go\nahead and put in a third model where w\nequals 9. so at this point this is one\nway of doing this is just to guess what\nw equals and you can see with w equals 9\nwe get the incorrect answers we get 9 18\n27 36 we as humans can know just by\ntaking a look at the data that our\nweight should be six but how does the\nmachine come to this conclusion how do\nwe program the computer to learn instead\nof waiting for us to tell it it has the\nright answer what the correct answer is\nand you can imagine this is a very\nsimple problem if we're doing guessing w\nequals three we guess w equals six we\nguess w equals nine and then we look at\nour results and we go oh it's gotta be w\nequals six that's the best result but as\nhumans we wanna take that element out\nand have the computer do that for us so\nwith that we're gonna have a loss\nfunction the loss function is a measure\nof error which defines the precision\nlost to comparing the predicted output\nto the actual output and it's simply\nloss equals actual output minus\npredicted output and then we square the\nwhole thing so let's apply the loss\nfunction to the input value 2 loss for\nour actual output predicted output\nsquared and our last function for the\ninput of 2 we end up with an actual\noutput of 12 and you can see here with w\nequals 3 12 minus 6 squared equals 36 so\nwe end up with a loss of 36 w equals 6\n12 12 times 12 squared equals 12 minus\n12 squared equals 0 or 12 minus 18\nsquared equals 36. and so you can see we\nhave a huge loss on w equals three and w\nequals nine we now plot a graph for the\nweight versus loss and it always helps\nto have a nice visual of what's going on\nhere so this graphical method of finding\nthe minimal of a function is called\nradiant descent and this is the logic\nbehind this you can see as we come in\nhere and we go ahead and graph the loss\nwe have 36 for 3 and 36 for 9. we happen\nto guess 6 which was the correct answer\nright in the middle and you can see\nright here forms a nice little parabola\nand you can see a nice mark right in the\nmiddle and as a human being we can look\nat that we go ah the answer is 6. a\nrandom point on this curve is chosen and\nthe slope at this point is calculated so\nnow we're getting away from the human\naspect of just looking at it and saying\nwhat the answer is and we look at what's\ngoing on with the math and so if we have\na positive slope it indicates an\nincrease in the weight and a negative\nslope indicates a decrease in weight\nthis time the slope is negative hence\nanother random point towards its left is\nchosen and you can see here we're\nactually kind of just playing a little\nhigh-low game going back and forth with\nthe gradient descent we continue\nchecking slopes at various points in\nthis manner so we have our input actual\noutput w3 w6 w9 we found our positive\nslope increase in increase in weight a\nnegative slope indicates a decrease in\nweight a zero slope indicates the\nappropriate weight so our aim is to\nreach a point where the slope is zero\nand when we talk about neural networks\nyou're usually processing a massive\namount of information and data so you're\nnot going to have all your data nice and\nneat where it's just a multiple of six\nit's gonna be messy and so we're gonna\nkeep approaching that number but you'll\nnever get everything to fit at 0. you're\ngoing to get stuff all over the place\nand so you're really looking for the\nminimum value you're not looking for an\nabsolute zero because you're not going\nto get it once we're talking about\ngradient descent that's what we're\ntalking about on there is finding the\nbottom of that curb even if it doesn't\ngo all the way to zero so how do we\napply that to our neural network well we\nuse back propagation back propagation is\na process of updating weights of the\nnetwork in order to reduce the error in\nprediction and so the magnitude of loss\nof any point on our graph combined with\nthe slope is fed back to the network and\nyou can see here here's our simple model\nwith just one node of x times w the\ninput comes in we have our x times w the\noutput and then we're going to propagate\nthat loss going the other way a random\npoint on the graph gives a loss value of\n36 with a positive slope and we continue\nchecking slopes at various points in\nthis manner so a random point in the\ngraph gives a loss value of 36 with a\npositive slope 36 is quite a large\nnumber this means our current weight\nneeds to change by a large number a\npositive slope indicates that the change\nof the weight must be positive similarly\nanother random point on the graph gives\na loss value of 10 with a negative slope\n10 is a small number hence the weight\nrequires to be tuned quite less a\nnegative slope indicates that the weight\nneeds to be reduced rather than\nincreased after multiple iterations of\nback propagation our weights are\nassigned the appropriate value you can\nsee here we have our input we just\nlooked at x times six in our output and\neventually we get it that the weight is\nsix for the single node problem that\nwe're working on right now at this point\nour network is trained and can be used\nto make predictions let's now get back\nto our first example and see where the\nback propagation and gradient descent\nfall into place and you can see here\nwe're not looking at a single node\nanymore now we have 28 by 28 grid or 784\ninputs coming into the first level which\nhas 784 nodes depending on how you build\nyour neural network the next layer might\nalso have 784 nodes or it might\ncontinually smallen depending on what\nyou need and what's needed for that to\nwork so as mentioned earlier predicted\noutput is compared against the actual\noutput and you can see our error over\nhere actual minus prediction and then we\ngo ahead and compute our loss so the\nloss of a is 0.7 squared equals 0.49\nloss of b is 0.5 squared or 0.25 and so\non so now we have our first iteration on\nthere so weights throughout the network\nare adjusted in order to reduce the loss\nin prediction now of course we do that\nby doing a second iteration coming\nthrough with our different losses on\nthere and then ways throughout the\nnetwork are just in order to reduce the\nloss of prediction again underneath the\nsecond and we do a third iteration and\nwe just keep doing these iterations\ngoing back until we get the right value\nnow you got to remember that when we're\ndoing a reverse propagation we're not\nlooking at just one letter a we're\nlooking at hundreds of letter a's and\nusually we propagate that loss going\nbackwards we only take a small piece of\nit so our adjustments are very small\nbecause one of them is not correct we\ndon't want to create a bias so we talk\nabout back propagation we're talking\nabout going through over and over and\nover this data until we get minimal loss\nfor our letter a so let's focus on the\nminimum loss for our variable a and you\ncan see here we look at that we end up\nwe'll assume for below to be our graph\nfor the loss of prediction what the\nvariable a as compared to the ways\ncontinuing it from the second layer and\nwe have our loss of a for 49 for 16 for\n0.04 and you can see it makes this nice\ncurve where we can guess where the\nbottom of this curve is and i like it on\nthis graph that they show that the curve\ndoesn't rest yeah on the x axis it\ndoesn't rest at where y equals zero\nbecause you usually don't get that you\ndon't get a perfect fit on anything or\nvery rarely do you ever get a perfect\nfit and so the random points chosen on\nthe graph is now back propagated through\nthe network in order to adjust the\nweights so we're able to go back through\nthe network and readjust those weights\nuntil we find that minimal value the\nnetwork is run once again with the new\nweights this process is repeated\nmultiple times till provides accurate\npredictions the weights are further\njustify adjusted to identify b and c\ntwo and this is interesting because you\nactually do them at the same time so as\nthe error goes back you kind of find the\noverall error for all the inputs coming\nin and then that's what gets propagated\ngoing back or the overall loss kind of\nhave to step away from that word error\nbecause it's not just about the error\nit's about the loss the weights are\nfurther adjusted to identify b and c2\nand so a lot of times you actually do\nthem all at the same time but you'll\nadjust those weights a b and c as i was\njust saying thus through gradient\ndescent and back propagation our network\nis completely trained and we've taught\nit to identify a b and c coming forward\none of the interesting things about\nneural networks is the training process\ntakes a lot longer than the predicting\nprocess so you can plan one of these\ntraining neural networks doing the back\npropagation to\nsignificantly longer because you're\ngoing over thousands of data points and\nthen when you actually run it forward\nit's very quick which makes these things\nvery useful and just really part of\ntoday's world in computing today we're\ngoing to be covering the convolutional\nneural network tutorial do you know how\ndeep learning recognizes the objects in\nan image and really this particular\nneural network is how image recognition\nworks it's very central one of the\nbiggest building blocks for image\nrecognition it does it using convolution\nneural network and we over here we have\nthe basic picture of a hummingbird\npixels of an image fed as input you have\nyour input layer coming in so it takes\nthat graphic and puts it into the input\nlayer you have all your hidden layers\nand then you have your output layer and\nyour output layer one of those is going\nto light up and say oh it's a bird we're\ngoing to go into depth we're going to\nactually go back and forth on this a\nnumber of times today so if you're not\ncatching all the image don't worry we're\ngoing to get into the details so we have\nour input layer accepts the pixels of\nthe image as input in the form of arrays\nand you can see up here where they've\nactually labeled each block of the bird\nin different arrays so we'll dive into\ndeep as to how that looks like and how\nthose matrixes are set up your hidden\nlayer carry out feature extraction by\nperforming certain calculations and\nmanipulation so this is the part that\nkind of reorganizes that picture\nmultiple ways until we get some data\nthat's easy to read for the neural\nnetwork this layer uses a matrix filter\nand performs convolution operation to\ndetect patterns in the image and if you\nremember that convolution means to coil\nor to twist so we're going to twist the\ndata around and alter it and use that\noperation to detect a new pattern there\nare multiple hidden layers like\nconvolution layer rel u is how that is\npronounced when that's the rectified\nlinear unit that has to do with the\nactivation function that's used\npooling layer also uses multiple filters\nto detect edges corners eyes feathers\nbeak etc and just like the term says\npooling is pulling information together\nand we'll look into that a lot closer\nhere so if you're if it's a little\nconfusing now we'll dig in deep and try\nto get you squared away with that and\nthen finally there is a fully connected\nlayer that identifies the object in the\nimage so we have these different layers\ncoming through in the hidden layers and\nthey come into the final area and that's\nwhere we have a one node or one neural\nnetwork entity that lights up that says\nit's a bird\nwhat's in it for you we're going to\ncover an introduction to the cnn what is\nconvolution neural network how cnn\nrecognizes images we're going to dig\ndeeper into that and really look at the\nindividual layers in the convolutional\nneural network and finally we do a use\ncase implementation using the cnn we'll\nbegin our introduction to the cnn by\nintroducing the pioneer of convolutional\nneural network jan lecun he was the\ndirector of facebook ai research group\nbuilt the first convolutional neural\nnetwork called lynette in 1988 so these\nhave been around for a while and have\nhad a chance to mature over the years it\nwas used for character recognition tasks\nlike reading zip code digits imagine\nprocessing mail and automating that\nprocess\ncnn is a feed forward neural network\nthat is generally used to analyze visual\nimages by producing data with a\ngrid-like topology a cnn is also known\nas a convent and very key to this is we\nare looking at images that was what this\nwas designed for and you'll see the\ndifferent layers as we dig in near some\nof the other some of them are actually\nnow used since we're using tensorflow\nand cross in our code later on you'll\nsee that some of those layers appear in\na lot of your other neural network\nframeworks but in this case this is very\ncentral to processing images and doing\nso in a variety that captures multiple\nimages and really drills down into their\ndifferent features in this example here\nyou see flowers are two varieties orchid\nand a rose i think the orchid is much\nmore dainty and beautiful and the rose\nsmells quite beautiful of a couple rose\nbushes in my yard they go into the input\nlayer that data is then sent to all the\ndifferent nodes in the next layer one of\nthe hidden layers based on its different\nweights and its setup it then comes out\nand gives those a new value those values\nthen are multiplied by their weights and\ngo to the next hidden layer and so on\nand then you have the output layer and\none of those notes comes out and says\nit's an orchid and the other one comes\nout and says it's a rose depending on\nhow it was well it was trained what\nseparates the cnn or the convolutional\nneural network from other neural\nnetworks is a convolutional operation\nforms the basis of any convolutional\nneural network in a cnn every image\nimage is represented in the form of\narrays of pixel values so here we have a\nreal image of the digit 8\nthat then gets put on to its pixel\nvalues represented in the form of an\narray in this case you have a two\ndimensional array and then you can see\nin the final in form we transform the\ndigit 8 into its representational form\nof pixels of zeros and ones where the\nones represent in this case the black\npart of the eight and the zeros\nrepresent the white background to\nunderstand the convolution neural\nnetwork or how that convolutional\noperation works we're going to take a\nside step and look at matrixes in this\ncase we're going to simplify it and\nwe're going to take two matrices a and b\nof one dimension now kind of separate\nthis from your thinking as we learned\nthat you want to focus just on the\nmatrix aspect of this and then we'll\nbring that back together and see what\nthat looks like when we put the pieces\nfor the convolutional operation here\nwe've set up two arrays we have in this\ncase are a single dimension matrix and\nwe have a equals five three seven five\nnine seven and we have b equals one two\nthree so in the convolution as it comes\nin there's gonna look at these two and\nwe're gonna start by doing multiplying\nthem a times b and so we multiply the\narrays element wise and we get five six\nsix\nwhere five is the five times one six is\nthree times two and then the other six\nis two times three and since the two\narrays aren't the same size they're not\nthe same setup we're going to just\ntruncate the first one and we're gonna\nlook at the second array multiplied just\nby the first three elements of the first\narray now that's going to be a little\nconfusing remember a computer gets to\nrepeat these processes hundreds of times\nso we're not going to just forget those\nother numbers later on we'll see we'll\nbring those back in and then we have the\nsum of the product in this case 5 plus 6\nplus 6 equals 17. so in our a times b\nour very first digit in that matrix of a\ntimes b is 17. and if you remember i\nsaid we're not going to forget the other\ndigits so we now have three two five we\nmove one set over and we take three two\nfive and we multiply that times b and\nyou'll see that three times one is three\ntwo times two is four and so on and so\non we sum it up so now we have the\nsecond digit of our a times b product in\nthe matrix and we continue on with that\nsame thing so on and so on so then we\nwould go from three seven five to seven\nfive nine to five nine seven this short\nmatrix that we have for a we've now\ncovered all the different entities in a\nthat match three different levels of b\nnow in a little bit we're going to cover\nwhere we use this math at this\nmultiplying of matrixes and how that\nworks but it's important to understand\nthat we're going through the matrix of\nmultiplying the different parts to it to\nmatch the smaller matrix with the larger\nmatrix i know a lot of people get lost\nat is you know what's going on here with\nthese matrixes uh oh scary math not\nreally that scary when you break it down\nwe're looking at a section of a and\nwe're comparing it to b so when you\nbreak that down your mind like that you\nrealize okay so i'm just taking these\ntwo matrixes and comparing them and i'm\nbringing the value down into one matrix\na times b we're deucing that information\nin a way that will help the computer see\ndifferent aspects let's go ahead and\nflip over again back to our images\nhere we are back to our images talking\nabout going to the most basic two\ndimensional image you can get to\nconsider the following two images the\nimage for the symbol backslash when you\npress the backslash the above image is\nprocessed you can see there for the\nimage for the forward slash is the\nopposite so we click the forward slash\nbutton that flips very basic we have\nfour pixels going in can't get any more\nbasic than that here we have a little\nbit more complicated picture we take a\nreal image of a smiley face\nthen we represent that in the form of\nblack and white pixels so if this was an\nimage in the computer it's black and\nwhite and like we saw before we convert\nthis into the zeros and one so where the\nother one would have just been a matrix\nof just four dots now we have a\nsignificantly larger image coming in so\ndon't worry we're going to bring this\nall together here in just a little bit\nlayers in convolutional neural network\nwe're looking at this we have our\nconvolution layer and that really is the\ncentral aspect of processing images in\nthe convolutional neural network that's\nwhy we have it and then that's going to\nbe feeding in and you have your relu\nlayer which is you know as we talked\nabout the rectified linear unit we'll\ntalk about that a little bit later the\nrelu isn't how it act is how that layer\nis activated is the math behind it what\nmakes the neurons fire you'll see that a\nlot of other neural networks when you're\nusing it just by itself it's for\nprocessing smaller amounts of data where\nyou use the atom activation feature for\nlarge data coming in now because we're\nprocessing small amounts of data in each\nimage the relu layer works great you\nhave your pooling layer that's where\nyou're pulling the data together pooling\nis a neural network term is very\ncommonly used i like to use the term\nreduce so if you're coming from the map\nand reduce side you'll see that we're\nmapping all this data through all these\nnetworks and then we're going to reduce\nit we're going to pull it together and\nthen finally we have the fully connected\nlayer that's where our output is going\nto come out so we have started to look\nat matrixes we've started to look at the\nconvolutional layer where it fits in and\neverything we've taken a look at images\nso we're going to focus more on the\nconvolution layer since this is a\nconvolutional neural network a\nconvolution layer has a number of\nfilters and perform convolution\noperation every image is considered as a\nmatrix of pixel values consider the\nfollowing five by five image whose pixel\nvalues are only zero and one now\nobviously when we're dealing with color\nthere's all kinds of things that come in\non color processing but we want to keep\nit simple and just keep it black and\nwhite and so we have our image pixels so\nwe're sliding the filter matrix over the\nimage and computing the dot product to\ndetect the patterns and right here\nyou're going to ask where does this\nfilter come from this is a bit confusing\nbecause the filter\nis going to be derived\nlater on we build the filters when we\nprogram or train our model so you don't\nneed to worry what the filter actually\nis but you do need to understand how a\nconvolution layer works is what is the\nfilter doing filter and you'll have mini\nfilters you don't have just one filter\nyou'll have lots of filters that are\ngoing to look for different aspects and\nso the filter might be looking for just\nedges it might be looking for different\nparts we'll cover that a little bit more\ndetail in a minute right now we're just\nfocusing on how the filter works as a\nmatrix remember earlier we talked about\nmultiplying matrixes together and here\nwe have our two dimensional matrix and\nyou can see we take the filter and we\nmultiply it in the upper left image and\nyou can see right here one times one one\ntimes zero one times one we multiply\nthose all together then sum them and we\nend up with the convolved feature of\nfour we're going to take that and\nsliding the filter matrix over the image\nand computing the dot product to detect\npatterns so we're just going to slide\nthis so we're going to predict the first\none and slide it over one notch predict\nthe second one and so on and so on all\nthe way through until we have a new\nmatrix and this matrix which is the same\nsize as the filter has reduced the image\nand whatever filter whatever that's\nfiltering out is going to be looking at\njust those features reduced down to a\nsmaller matrix so once the feature maps\nare extracted the next step is to move\nthem to the relu layer so the relu layer\nthe next step first is going to perform\nan element-wise operation so each of\nthose maps coming in if there's negative\npixels so it says all the negative\npixels to zero\nand you can see this nice graph where it\njust zeros out the negatives and then\nyou have a value that goes from zero up\nto whatever value is coming out of the\nmatrix this introduces non-linearity to\nthe network so up until now we have a we\nsay linearity we're talking about the\nfact that the feature has a value so\nit's a linear feature this feature\ncame up and has let's say the feature is\nthe edge of the beak you know it's like\nor the backslash that we saw you'll look\nat that and say okay this feature has a\nvalue from negative 10 to 10 in this\ncase um if it was one it'd say yeah this\nmight be a beak it might not might be an\nedge right there a minus five means no\nwe're not even going to look at it to\nzero and so we end up with an output and\nthe output takes all these features all\nthese filtered features remember we're\nnot just running one filter on this\nwe're running a number of filters on\nthis image and so we end up with a\nrectified feature map that is looking at\njust the features coming through and how\nthey weigh in from our filters so here\nwe have an input of a looks like a\ntoucan bird\nvery exotic looking real image is\nscanned in multiple convolution and the\nrelu layers for locating features and\nyou can see up here is turn it into a\nblack and white image and in this case\nwe're looking in the upper right hand\ncorner for a feature and that box scans\nover a lot of times it doesn't scan one\npixel at a time a lot of times it will\nskip by two or three or four pixels to\nspeed up the process that's one of the\nways you can compensate if you don't\nhave enough resources on your\ncomputation for large images and it's\nnot just one filter slowly goes across\nthe image you have multiple filters have\nbeen programmed in there so you're\nlooking at a lot of different filters\ngoing over the different aspects of the\nimage and just sliding across there and\nforming a new matrix one more aspect to\nnote about the relu layer is we're not\njust having one value coming in\nso not only do we have multiple features\ngoing through but we're generating\nmultiple relu layers for locating the\nfeatures that's very important to note\nyou know so we have a quite a bundle we\nhave multiple filters multiple rail u\nwhich brings us to the next step\nforward propagation now we're going to\nlook at the pooling layer the rectified\nfeature map now goes through a pooling\nlayer pooling is a down sampling\noperation that reduces the\ndimensionality of the feature map that's\nall we're trying to do we're trying to\ntake a huge amount of information and\nreduce it down to a single answer this\nis a specific kind of bird this is an\niris this is a rose so you have a\nrectified feature map and you see here\nwe have a rectified feature map coming\nin\nwe set the max pooling with a 2 by 2\nfilters and a stride of two and if you\nremember correctly i talked about not\ngoing one pixel at a time uh well that's\nwhere the stride comes in we end up with\na two by two pooled feature map but\ninstead of moving one over each time and\nlooking at every possible combination we\nskip a st we skip a few there we go by\ntwo we skip every other pixel and we\njust do every other one and this reduces\nour rectified feature map which is you\ncan see over here 16 by 16 to a four by\nfour so we're continually trying to\nfilter and reduce our data so that we\ncan get to something we can manage and\nover here you see that we have the max\n3 4 1 and 2 and in the max pooling we're\nlooking for the max value a little bit\ndifferent than what we were looking at\nbefore so coming from the rectified\nfeature we're now finding the max value\nand then we're pulling those features\ntogether so instead of think of this as\nimage of the map think of this as how\nvaluable is a feature in that area how\nmuch of a feature value do we have we\njust want to find the best or the\nmaximum feature for that area they might\nhave that one piece of the filter of the\nbeak said oh i see a one in this beak in\nthis image and then it skips over and\nsays i see a three in this image and\nsays oh this one is rated as a four we\ndon't want to sum it together because\nthen you know you might have like five\nones and it'll say ah five but you might\nhave uh four zeros and one ten and that\nten says well this is definitely a beak\nwhere the ones will say probably not a\nbeak a little strange analogy since\nwe're looking at a bird but you can see\nhow that pulled feature map comes down\nand we're just looking for the max value\nin each one of those matrixes pooling\nlayer uses different filters to identify\ndifferent parts of the image like edges\ncorners body feathers eyes beak etc i\nknow i focus mainly on the beak but\nobviously each feature could be each a\ndifferent part of the bird coming in so\nlet's take a look at what that looks\nlike structure of a convolution neural\nnetwork so far this is where we're at\nright now we have our input image coming\nin and then we use our filters and\nthere's multiple filters on there that\nare being developed to kind of twist and\nchange that data and so we multiply the\nmatrixes we take that little filter\nmaybe it's a two by two we multiply it\nby each piece of the image and if we\nstep two then it's every other piece of\nthe image that generates multiple\nconvolution layers so we have a number\nof convolution layers we have\nset up in there just looking at that\ndata we then take those convolution\nlayers we run them through the relu\nsetup and then once we've done through\nthe release setup and we have multiple\nvalues going on multiple layers that are\nrelu then we're going to take those\nmultiple layers and we're going to be\npooling them so now we have the pooling\nlayers or multiple poolings going on up\nuntil this point we're dealing with\nsometimes multiple dimensions you can\nhave three dimensions some strange data\nsetups that aren't doing images but\nlooking at other things they can have\nfour five six seven dimensions uh so\nright now we're looking at 2d image\ndimensions coming in into the pooling\nlayer so the next step is we want to\nreduce those dimensions or flatten them\nso flattening flattening is a process of\nconverting all of the resultant\ntwo-dimensional arrays from pooled\nfeature map into a single long\ncontinuous linear vector so over here\nyou see where we have a pooled feature\nmap maybe that's the bird wing and it\nhas values 6847 and we want to just\nflatten this out and turn it into 6847\nor a single linear vector and we find\nout that not only do we do each of the\npooled feature maps we do all of them\ninto one long linear vector so now we've\ngone through our convolutional neural\nnetwork part and we have the input layer\ninto the next setup all we've done is\ntaken all those different pooling layers\nand we flatten them out and combine them\ninto a single linear vector going in so\nafter we've done the flattening we have\njust a quick recap because we've covered\nso much so it's important to go back and\ntake a look at each of the steps we've\ngone through the structure of the\nnetwork so far we have our convolution\nwhere we twist it and we filter it and\nmultiply the matrixes we end up with our\nconvolutional layer which uses the relu\nto figure out the values going out into\nthe pooling as you have numerous\nconvolution layers that then create\nnumerous pooling layers pulling that\ndata together which is the max value\nwhich one we want to send forward we\nwant to send the best value and then\nwe're going to take all of that from\neach of the pooling layers and we're\ngoing to flatten it and we're going to\ncombine them into a single input going\ninto the final layer once you get to\nthat step you might be looking at that\ngoing boy that looks like the normal\nintuit to most neural network and you're\ncorrect it is so once we have the\nflattened matrix from the pooling layer\nthat becomes our input so the pooling\nlayer is fed as an input to the fully\nconnected layer to classify the image\nand so you can see as our flattened\nmatrix comes in in this case we have the\npixels from the flattened matrix fed as\nan input back to our toucan or whatever\nthat kind of bird that is i need one of\nthese to identify what kind of bird that\nis it comes into our ford propagation\nnetwork\nand that will then have the different\nweights coming down across and then\nfinally it selects that that's a bird\nand that is not a dog or a cat in this\ncase\neven though it's not labeled the final\nlayer there in red is our output layer\nour final output layer that says bird\ncat or dog\nso quick recap of everything we've\ncovered so far we have our input image\nwhich is twisted and multiplied the\nfilters are multiplied times the matrix\nand the two matrixes multiplied all the\nfilters to create our convolution layer\nour convolution layers there's multiple\nlayers in there because it's all\nbuilding multiple layers off the\ndifferent filters then goes through the\nrelu as this activation and that creates\nour pooling and so once we get into the\npooling layer we then and the pooling\nlook for who's the best what's the max\nvalue coming in from our convolution and\nwe take that layer and we flatten it and\nthen it goes into a fully connected\nlayer our fully connected neural network\nand then to the output and here we can\nsee the entire process how the cnn\nrecognizes a bird this is kind of nice\nbecause it's showing the little pixels\nand where they're going you can see the\nfilter is generating this convolution\nnetwork and that filter shows up in the\nbottom part of the convolution network\nand then based on that it uses the relu\nfor the pooling the pooling then find\nout which one's the best and so on all\nthe way to the fully connected layer at\nthe end or the classification in the\noutput layer so that'd be a\nclassification neural network at the end\nso we covered a lot of theory up till\nnow and you can imagine each one of\nthese steps has to be broken down in\ncode so putting that together can be a\nlittle complicated not that each step of\nthe process is overly complicated but\nbecause we have so many steps we have\none two three four five different steps\ngoing on here with sub steps in there\nwe're going to break that down and walk\nthrough that in code so in our use case\nimplementation using the cnn we'll be\nusing the cfar10 dataset from canadian\ninstitute for advanced research for\nclassifying images across 10 categories\nunfortunately they don't let me know\nwhether it's going to be a toucan or\nsome other kind of bird but we do get to\nfind out whether it can categorize\nbetween a ship a frog deer bird airplane\nautomobile cat dog horse truck so that's\na lot of fun and if you're looking\nanything in the news at all of our\nautomated cars and everything else you\ncan see where this kind of processing is\nso important in today's world and very\ncutting edge as far as what's coming out\nin the commercial deployment i mean this\nis really cool stuff we're starting to\nsee this just about everywhere in\nindustry so great time to be playing\nwith this and figuring it all out let's\ngo ahead and dive into the code and see\nwhat that looks like when we're actually\nwriting our script\nbefore we go on let's do uh one more\nquick look at what we have here let's\njust take a look at data batch one keys\nand remember in jupiter notebook i can\nget by with not doing the print\nstatement if i put a variable down there\nit'll just display the variable and you\ncan see under data batch one for the\nkeys since this is a dictionary we have\nthe batch one label data and file names\nso you can actually see how it's broken\nup in our data set so for the next step\nor step four as we're calling it uh we\nwant to display the image using matte\nplot library there's many ways to\ndisplay the images you can even well\nthere's other ways to drill into it but\nmatplot library is really good for this\nand we'll also look at our first reshape\nuh setup or shaping the data so you can\nhave a little glimpse into what that\nmeans uh so we're gonna start by\nimporting our map plot and of course\nsince i am doing jupiter notebook i need\nto do the matplot inline command so it\nshows up on my page so here we go we're\ngoing to import matplot library.pipelot\nas plt and if you remember matplot\nlibrary the pie plot is like a canvas\nthat we paint stuff onto and there's my\npercentage sign matplot library in line\nso it's going to show up in my notebook\nand then of course we're going to import\nnumpy as np for our numbers python array\nsetup and let's go ahead and set\nx equals to data batch one so this will\npull in all the data going into the x\nvalue and then because this is just a\nlong stream of binary data we need to go\na little bit of reshaping so in here we\nhave to go ahead and reshape the data we\nhave 10 000 images okay that looks\ncorrect and this is kind of an\ninteresting thing it took me a little\nbit to i had to go research this myself\nto figure out what's going on with this\ndata and what it is is it's a 32 by 32\npicture and let me do this let me go\nahead and do a drawing pad on here uh so\nwe have 32 bits by 32 bits and it's in\ncolor so there's three bits of color now\ni don't know why the data is\nparticularly like this it probably has\nto do with how they originally encoded\nit but most pictures put the three\nafterward so what we're doing here is\nwe're going to take uh the shape we're\ngoing to take the data which is just a\nlong stream of information and we're\ngoing to break it up into 10 000 pieces\nand those 10 000 pieces then are broken\ninto three pieces each and those three\npieces then are 32 by 32. you can look\nat this like an old-fashioned projector\nwhere they have the red screen or the\nred projector the blue projector and the\ngreen projector and they add them all\ntogether and each one of those is a 32\nby 32 bit so that's probably how this\nwas originally formatted was in that\nkind of ideal things have changed so\nwe're going to transpose it and we're\ngoing to take the three which was here\nand we're going to put it at the end so\nthe first part is reshaping the data\nfrom a single line of bit data or\nwhatever format it is into 10 000 by 3\nby 32 by 32 and then we're going to\ntranspose the color factor to the last\nplace so it's the image then the 32 by\n32 in the middle that's this part right\nhere and then finally we're going to\ntake this which is three bits of data\nand put it at the end so it's more like\nwe do process images now and then as\ntype this is really important that we're\ngonna use an integer eight you can come\nin here and you'll see a lot of these\nthey'll try to do this with a float or a\nfloat 64. what you got to remember\nthough is a float uses a lot of memory\nso once you switch this into uh\nsomething that's not integer 8 which\ngoes up to 128 you are just going to the\nthe amount of ram let's just put that in\nhere is going to go way up the amount of\nram that it loads\nso you want to go ahead and use this you\ncan try the other ones and see what\nhappens if you have a lot of ram on your\ncomputer but for this exercise this will\nwork just fine and let's go ahead and\ntake that and run this so now our x\nvariable is all loaded and it has all\nthe images in it from the batch one data\nbatch one and just to show we were\ntalking about with the as type on there\nif we go ahead and take x0 and just look\nfor its max value let me go ahead and\nrun that uh you'll see it doesn't oops i\nsaid 128 is 255. uh you'll see it\ndoesn't go over 255 because it's\nbasically an ascii character is what\nwe're keeping that down to we're keeping\nthose values down so they're only 255 0\nto 255 versus a float value which would\nbring this up\nexponentially in size and since we're\nusing the matplot library we can do oops\nthat's not what i wanted since we're\nusing the map plot library we can take\nour canvas and just do a plt dot im for\nimage show and let's just take a look at\nwhat x0 looks like and it comes in i'm\nnot sure what that is but you can see\nit's a very low grade image uh broken\ndown to the minimal pixels on there and\nif we did the same thing oh let's do uh\nlet's see what one looks like hopefully\nit's a little easier to see run on there\nnot enter let's hit the run on that uh\nand we can see this is probably a semi\ntruck that's a good guess on there and i\ncan just go back up here instead of\ntyping the same line in over and over\nand we'll look at three uh that looks\nlike a dump truck unloading uh and so on\nyou can do any of the 10 000 images we\ncan just jump to 55 looks like some kind\nof animal looking at us there probably a\ndog and just for fun let's do just one\nmore uh\nrun on there and we can see a nice car\nfor our image number four uh so you can\nsee we pace through all the different\nimages it's very easy to look at them\nand they've been reshaped to fit our\nview and what the\nmatte plot library uses for its format\nso the next step is we're going to start\ncreating some helper functions we'll\nstart by a one hot encoder to help us or\nprocessing the data remember that your\nlabels they can't just be words they\nhave to switch it and we use the one hot\nencoder to do that and then we'll also\ncreate a class\ncfar helper so it's going to have an\ninit and a setup for the images and then\nfinally we'll go ahead and run that code\nso you can see what that looks like and\nthen we get into the fun part where\nwe're actually going to start creating\nour model our actual neural network\nmodel so let's start by creating our one\nhot encoder we're going to create our\nown here and it's going to return an out\nand we'll have our vector coming in and\nour values equal 10. what this means is\nthat we have the 10 values the 10\npossible labels and remember we don't\nlook at the labels as a number because a\ncar isn't one more than a horse maybe\njust kind of bizarre to have horse\nequals zero car equals one plane equals\ntwo cat equals three so a cat plus a car\nequals what uh so instead we create\na numpy array of zeros and there's going\nto be 10 values so we have a 10\ndifferent values in there so you have\n0 or 1. 1 means it's a cat 0 means it's\nnot a cat\nin the next line it might be that one\nmeans it's a car zero means it's not a\ncar so instead of having one output with\na value of 0 to 10 you have 10 outputs\nwith the values of 0 to 1. that's what\nthe one hot encoder is doing here and\nwe're going to utilize this in code in\njust a minute so let's go ahead and take\na look at the next helpers we have a few\nof these helper functions we're going to\nbuild and when you're working with a\nvery complicated python project dividing\nit up into separate definitions and\nclasses is very important otherwise it\njust becomes really ungainly to work\nwith so let's go ahead and put in our\nnext helper which is a class and this is\na lot in this class so we'll break it\ndown here and let's just start oops we\nput a space right in there there we go\nthat was a little bit more readable at a\nsecond space so we're going to create\nour class the cipher helper and we'll\nstart by initializing it now there's a\nlot going on in here so let's start with\nthe init part uh self dot i equals zero\ni'll come in in a little bit we'll come\nback to that in the lower part we want\nto initialize our training batches so\nwhen we went through this there was like\na meta batch we don't need the meta\nbatch but we do need the data batch one\ntwo three four five and we do not want\nthe testing batch in here this is just\nthe self all train batches so we're\ngonna come make an array of all those\ndifferent images and then of course we\nleft the test batch out so we have our\nself.test batch\nwe're going to initialize the training\nimages and the training labels and also\nthe test images and the test labels so\nthese are just this is just to\ninitialize these variables in here then\nwe create another definition down here\nand this is going to set up the images\nlet's just take a look and see what's\ngoing on in there now we could have all\njust put this as part of the\ninit part\nsince this is all just helper stuff but\nbreaking it up again makes it easier to\nread it also makes it easier when we\nstart executing the different pieces to\nsee what's going on so that way we have\na nice print statement to say hey we're\nnow running this and this is what's\ngoing on in here we're going to set up\nthe self training images at this point\nand that's going to go to a numpy array\nv stack and in there we're going to load\nup\nin this case the data for d itself all\ntrain batches again that points right up\nto here so we're going to go through\neach one of these\nfiles or each one of these data sets\nbecause they're not a file anymore we've\nbrought them in data batch one points to\nthe actual data and so our self training\nimages is going to stack them all into\nour into a numpy array and then it's\nalways nice to get the training length\nand that's just a total number of self\ntraining images in there and then we're\ngoing to take the self training images\nlet me switch marker colors because i am\ngetting a little too too much on the\nmarkers up here oops there we go bring\ndown our marker change\nso we can see it a little better and at\nthis point this should look familiar\nwhere did we see this well when we\nwanted to uh look at this above and we\nwant to look at the images in the\nmatplot library we had to reshape it so\nwe're doing the same thing here we're\ntaking our self training images and\nbased on the training length total\nnumber of images because we stacked them\nall together so now it's just one large\nfile of images we're going to take and\nlook at it as our three video cameras\nthat are each displaying a 32 by 32\nwe're going to switch that around so\nthat now we have each of our images that\nstays the same place and then we have\nour 32 by 32 and then by our three our\nlast are three different values for the\ncolor and of course we want to go ahead\nand they run this where we say divide by\n255 that was from earlier it just brings\nall the data into zero to one that's\nwhat this is doing so we're turning this\ninto a zero to one array which is uh all\nthe pictures 32 by 32 by three and then\nwe're going to take the self training\nlabels and we're going to pump those\nthrough our one hot encoder we just made\nand we're going to stack them together\nand\nagain we're converting this into an\narray that goes from uh instead of\nhaving horse equals one dog equals two\nand then horse plus dog would equal\nthree which would be cat\nnow it's going to be you know an array\nof 10 where each one is 0 to 1. then we\nwant to go ahead and set up our test\nimages and labels and when we're doing\nthis you're going to see it's the same\nthing we just did with the rest of them\nwe just changed colors right here this\nis no different than what we were doing\nup here with our training set uh we're\ngoing to stack the different images uh\nwe're going to get the length of them so\nwe know how many images are in there you\ncertainly could add them by hand but\nit's nice to let the computer do it\nespecially if it ever changes on the\nother end and you're using other data\nand again we reshape them and transpose\nthem and we also do the one hot encoder\nsame thing we just did on our training\nimages so now our test images are in the\nsame format so now we have a definition\nwhich sets up all our images in there\nand then the next step is to go ahead\nand batch them or next batch and let's\ndo another breakout here for batches\nbecause this is really important to\nunderstand tends to throw me for a\nlittle loop when i'm working with\ntensorflow or cross or a lot of these we\nhave our data coming in if you remember\nwe had like 10 000 photos let me just\nput 10 000 down here we don't want to\nrun all 10 000 at once so we want to\nbreak this up into batch sizes and you\nalso remember that we had the number of\nphotos in this case a length of test or\nwhatever number is in there we also have\n32 by 32\nby 3. so when we're looking at the batch\nsize we want to change this from 10 000\nto\na batch of in this case i think we're\ngoing to do batches of a hundred so we\nwant to look at just 100 the first\nhundred of the photos and if you\nremember we set self-i equal to\nzero uh so what we're looking at here is\nwe're going to create x we're gonna get\nthe next batch from the very initialize\nwe've already initialized it for zero so\nwe're gonna look at x from zero to batch\nsize which we set to one hundred so just\nthe first 100 images and then we're\ngoing to reshape that into\nand this is important to let the data\nknow that we're looking at 100 by 32 by\n32 by 3. now we've already formatted it\nto the 32 by 32 by 3. this just sets\neverything up correctly so that x has\nthe data in there in the correct order\nand the correct shape and then the y\njust like the x\nis our labels so our training labels\nagain they go from 0 to batch size in\nthis case they do sell fi plus batch\nsize because the self is going to keep\nchanging and then finally we increment\nthe self i because we have zero so we so\nthe next time we call it we're going to\nget the next batch size and so basically\nwe have x and y x being the photograph\ndata coming in and y being the label and\nthat of course is labeled through one\nhot encoder so if you remember correctly\nif it was say horse is equal to zero it\nwould be um one for the zero position\nsince this is the horse and then\neverything else would be zero in here\nlet me just put lines through there\nthere we go there's our array\nhard to see that array so let's go ahead\nand take that and we're going to finish\nloading it since this is our class and\nnow we're armed with all this um uh our\nsetup over here let's go ahead and load\nthat up and so we're going to create a\nvariable ch with the cfar helper in it\nand then we're going to do ch.setup\nimages\nnow we could have just put all the setup\nimages under the init but by breaking\nthis up into two parts it makes it much\nmore readable and also if you're doing\nother work there's reasons to do that as\nfar as the setup let's go ahead and run\nthat and you can see where it says\nsetting up training images and labels\nsetting up test images and that's one of\nthe reasons we broke it up is so that if\nyou're testing this out you can actually\nhave print statements in there telling\nyou what's going on which is really nice\nuh they did a good job with this setup i\nlike the way that it was broken up in\nthe back and then one quick note you\nwant to remember that batch to set up\nthe next batch because we have to run uh\nbatch equals ch next batch of 100\nbecause we're going to use the 100 size\nbut we'll come back to that we're going\nto use that just remember that that's\npart of our code we're going to be using\nin a minute from the definition we just\nmade so now we're ready to create our\nmodel first thing we want to do is we\nwant to import our tensorflow as tf i'll\njust go ahead and run that so it's\nloaded up and you see we got a\nwarning here\nthat's because they're making some\nchanges it's always growing and they're\ngoing to be depreciating one of the\nvalues from float 64 to float type or is\ntreated as an np float 64. uh nothing to\nreally worry about this doesn't even\naffect what we're working on because\nwe've set all of our stuff to a 255\nvalue or zero to one and do keep in mind\nthat zero to one value that we converted\nto 255 is still a float value but it'll\neasily work with either the numpy float\n64 or the numpy d type float it doesn't\nmatter which one it goes through so the\ndepreciation would not affect our code\nas we have it and in our tensorflow uh\nwe'll go ahead and just increase the\nsize in there just a moment so you can\nget better view of the um what we're\ntyping in uh we're going to set a couple\nplaceholders here and so we have we're\ngoing to set x equals tf placeholder tf\nfloat 32 we just talked about the float\n64 versus the numpy float we're actually\njust going to keep this at float32 more\nthan a significant number of decimals\nfor what we're working with and since\nit's a placeholder we're going to set\nthe shape equal to and we've set it\nequal to none\nbecause at this point we're just holding\nthe place on there we'll be setting up\nas we run the batches that's what that\nfirst value is and then 32 by 32 by 3\nthat's what we reshaped our data to fit\nin and then we have our y true equals\nplaceholder tf float 32 and the shape\nequals none comma 10. 10 is the 10\ndifferent labels we have so it's an\narray of 10. and then let's create one\nmore placeholder we'll call this a hold\nprob or hold probability and we're going\nto use this we don't have to have a\nshape or anything for this this\nplaceholder is for what we call drop out\nif you remember from our theory before\nwe drop out so many nodes is looking at\nor the different values going through\nwhich helps decrease bias so we need to\ngo ahead and put a placeholder for that\nalso and we'll run this so it's all\nloaded up in there so we have our three\ndifferent placeholders and since we're\nin tensorflow when you use keras it does\nsome of this automatically but we're in\ntensorflow direct cross sits on\ntensorflow we're going to go ahead and\ncreate some more helper functions we're\ngoing to create something to help us\ninitialize the weights initialize our\nbias if you remember that each layer has\nto have a bias going in we're going to\ngo ahead and work on our conversional 2d\nour max pool so we have our pooling\nlayer our convolutional layer and then\nour normal full layer so we're going to\ngo ahead and put those all into\ndefinitions and let's see what that\nlooks like in code and you can also grab\nsome of these helper functions from the\nmnist the uh nist setup let me just put\nthat in there if you're under the\ntensorflow so a lot of these already in\nthere but we're going to go ahead and do\nour own and we're going to create our\ninit weights and one of the reasons\nwe're doing this is so that you can\nactually start thinking about what's\ngoing on in the back end so even though\nthere's ways to do this with an\nautomation sometimes these have to be\ntweaked and you have to put in your own\nsetup in here now we're not going to be\ndoing that we're just going to recreate\nthem for our code and let's take a look\nat this we have our weights and so it\ncomes in is going to be the shape and\nwhat comes out is going to be a random\nnumber so we're going to go ahead and\njust knit some random numbers based on\nthe shape with a standard deviation of\n0.1 kind of a fun way to do that and\nthen the tf variable\ninit random distribution so we're just\ncreating a random distribution on there\nthat's all that is for the weights now\nyou might change that you might have a\nhigher standard deviation in some cases\nyou actually load preset weights that's\npretty rare usually you're testing that\nagainst another model or something like\nthat and you want to see how those\nweights configure with each other now\nremember we have our bias so we need to\ngo ahead and initialize the bias with a\nconstant\nin this case we're using 0.1 a lot of\ntimes the bias is just put in as one and\nthen you have your weights add on to\nthat but we're going to set this as\npoint one uh so we want to return a\nconvolutional 2d in this case a neural\nnetwork this is uh would be a layer on\nhere what's going on with the con 2d is\nwe're taking our data coming in\nwe're going to filter it strides if you\nremember correctly strides came from\nhere's our image and then we only look\nat this picture here and then maybe we\nhave a stride of one so we look at this\npicture here and we continue to look at\nthe different filters going on there the\nother thing this does is that we have\nour data coming in as 32\nby 32\nby\n3 and we want to change this so that\nit's just this is three dimensions and\nit's going to reformat this as just two\ndimensions so it's going to take this\nnumber here and combine it with the 32\nby 32. so this is a very important layer\nhere because it's reducing our data down\nusing different means and it connects\ndown i'm just going to jump down one\nhere\nit goes with the convolutional layer so\nyou have your your kind of your\npreformatting and the setup and then you\nhave your actual convolution layer that\ngoes through on there and you can see\nhere we have init weights by the shape\nand knit bias shape of three because we\nhave the three different here's our\nthree again and then we return the tfnn\nrelu with the convention 2d so this\nconvolutional has this feeding into it\nright there it's using that as part of\nit and of course the input is the x y\nplus b the bias so that's quite a\nmouthful but these two are the are the\nkeys here to creating the convolutional\nlayers there the convolutional 2d coming\nin and then the convolutional layer\nwhich then steps through and creates all\nthose filters we saw then of course we\nhave our pooling uh so after each time\nwe run it through the convectional layer\nwe want to pull the data if you remember\ncorrectly on the on the pool side and\nlet me just get rid of all my marks it's\ngetting a little crazy there and in fact\nlet's go ahead and jump back to that\nslide let's just take a look at that\nslide over here uh so we have our image\ncoming in we create our convolutional\nlayer with all the filters remember the\nfilters go um you know the filter is\ncoming in here and it looks at these\nfour boxes and then if it's a step let's\nsay step two it then goes to these four\nboxes and then the next step and so on\nuh so we have our convolutional layer\nthat we generate or convolutional layers\nthey use the uh relu function there's\nother functions out there for this\nthough the relu is the\nmost the one that works the best at\nleast so far i'm sure that will change\nthen we have our pooling now if you\nremember correctly the pooling was max\nso if we had the filter coming in and\nthey did the multiplication on there and\nwe have a one and maybe a two here and\nanother one here and a three here three\nis the max and so out of all of these\nyou then create an array that would be\nthree and if the max is over here two or\nwhatever it is that's what goes into the\npooling of what's going on in our\npooling uh so again we're reducing that\ndata down reducing it down as small as\nwe can and then finally we're going to\nflatten it out into a single array and\nthat goes into our fully connected layer\nand you can see that here in the code\nright here we're going to create our\nnormal full layer so at some point we're\ngoing to take from our pooling layer\nthis will go into some kind of\nflattening process and then that will be\nfed into the full the different layers\ngoing in down here and so we have our\ninput size you'll see our input layer\nget shape which is just going to get the\nshape for whatever's coming in uh and\nthen input size initial weights is also\nbased on the input layer coming in and\nthe input size down here is based on the\ninput layer shape so we're just going to\nalready use the shape and already have\nour size coming in and of course uh you\nhave to make sure you knit the bias\nalways put your bias on there and we'll\ndo that based on the size so this will\nreturn\ntf.matimole\ninput layer w plus b this is just a\nnormal full layer that's what this means\nright down here that's what we're going\nto return so that was a lot of steps we\nwent through let's go ahead and run that\nso those are all loaded in there and\nlet's go ahead and uh create the layers\nlet's see what that looks like\nnow that we've done all the heavy\nlifting and everything uh we get to do\nall the easy part let's go ahead and\ncreate our layers we'll create a\nconvolution layer one and two two\ndifferent convolutional layers and then\nwe'll take that and we'll flatten that\nout and create a reshape pooling in\nthere for our reshape and then we'll\nhave our full uh layer at the end so\nlet's start by creating our first\nconvolutional layer then we come in here\nand let me just run that real quick and\ni want you to notice on here the 3\nand the 32 this is important because\ncoming into this convolutional layer we\nhave three different channels and 32\npixels each\nso that has to be in there the four and\nfour you can play with this is your\nfilter size so if you remember you have\na filter and you have your image and the\nfilter slowly steps over and filters out\nthis image depending on what your step\nis for this particular setup 4 4 is just\nfine that should work pretty good for\nwhat we're doing for the size of the\nimage and then of course at the end once\nyou have your convolutional layer set up\nyou also need to pull it and you'll see\nthat the pooling is automatically set up\nso that it would see the different shape\nbased on what's coming in so here we\nhave max toolbar 2x2 and we put in the\nconvolutional 1 that we just created the\nconvolutional layer we just created goes\nright back into it and that right up\nhere as you can see is the x that's\ncoming in from here so it knows to look\nat the first model and set the the data\naccordingly set that up so it matches\nand we went ahead and ran this already i\nthink i read let me go and run it again\nand if we're going to do one layer let's\ngo ahead and do a second layer down here\nand it's uh we'll call it convo 2.\nit's also a convolutional layer on this\nand you'll see that we're feeding\nconvolutional one in the pooling so it\ngoes from convolutional one into\nconvolutional one pooling from\nconvolutional one pooling into\nconvolutional two and then from\nconvolutional two into convolutional 2\npooling and we'll go ahead and take this\nand run this so these variables are all\nloaded into memory and for our flattened\nlayer\nlet's go ahead and we'll do since we\nhave 64 coming out of here and we have a\nfour by four going in let's do eight by\neight by 64. so let's do\n4096. this is going to be the flat layer\nso that's how many bits are coming\nthrough on the flat layer and we'll\nreshape this so we'll reshape our\nconvo 2 pooling and that will feed into\nhere the combo 2 pulling and then we're\ngoing to set it up as a single layer\nthat's 4096 in size that's what that\nmeans there we'll go ahead and run this\nso we've now created this variable the\nconvo too flat and then we have our\nfirst full layer this is the final\nneural network where the flat layer\ngoing in and we're going to again use\nthe relu for our setup on there on a\nneural network for evaluation and you'll\nnotice that we're going to create our\nfirst full layer our normal full layer\nthat's our definition so we created that\nthat's creating the normal full layer\nand our input for the data comes right\nhere from the this goes right into it\nthe convo too flat so this tells it how\nbig the data is and we're going to have\nit come out it's going to have 10 24\nthat's how big the layer is coming out\nwe'll go ahead and run this so now we\nhave our full layer one and with the\nfull layer one we want to also define\nthe full one dropout to go with that so\nour full layer one comes in\nkeep probability equals whole\nprobability remember we created that\nearlier and the full layer one is what's\ncoming into it and this is going\nbackwards and training the data we're\nnot training every weight we're only\ntraining a percentage of them each time\nwhich helps get rid of the bias so let\nme go ahead and run that and\nfinally we'll go ahead and create a y\npredict which is going to equal the\nnormal full one drop out and 10 because\nwe have 10 labels in there now in this\nneural network we could have added\nadditional layers that would be another\noption to play with you can also play\nwith instead of 10 24 you can use other\nnumbers for the way that sets up what's\ncoming out going into the next one we're\nonly going to do just the one layer and\nthe one layer drop out and you can see\nif we did another layer it'd be really\neasy just to feed in the full one drop\nout into full layer two and then full\nlayer two dropout would have full air 2\nfeed into it and then you'd switch that\nhere for the y prediction for right now\nthis is great this particular data set\nis tried and true and we know that this\nwill work on it and if we just type in y\npredict and we run that\nwe'll see that this is a tensor object\nuh shape question mark 10 d type 32 a\nquick way to double check what we're\nworking on so now we've got all of our\nwe've done a setup all the way to the y\npredict which we just did we want to go\nahead and apply the loss function and\nmake sure that's set up in there create\nthe optimizer and then\ntrainer optimizer and create a variable\nto initialize all the global tf\nvariables so before we dive in to the um\nloss function let me point out one quick\nthing or just kind of a rehab over a\ncouple things and that is when we're\nplaying with this these setups\nwe pointed out up here we can change the\n4 4 and use different numbers there the\nchange your outcome so depending on what\nnumbers you use here will have a huge\nimpact on how well your model fits and\nthat's the same here of the 1024 also\nthis is also another number that if you\ncontinue to raise that number you'll get\npossibly a better fit you might overfit\nand if you lower that number you'll use\nless resources and generally you want to\nuse this in\nthe exponential growth an exponential\nbeing 2 4\n8 16 and in this case the next one down\nwould be 5 12. you can use any number\nthere but those would be the ideal\nnumbers when you look at this data so\nthe next step in all this is we need to\nalso create a way of tracking how good\nour model is and we're going to call\nthis a loss function and so we're going\nto create a cross entropy loss function\nand so before we discuss exactly what\nthat is let's take a look and see what\nwe're feeding it\nwe're going to feed it our labels and we\nhave our true labels and our prediction\nlabels so coming in here is where the\ntwo different variables we're sending in\nor the two different probability\ndistributions is one that we know is\ntrue and what we think it's going to be\nnow this function right here when they\ntalk about cross entropy\nin information theory the cross entropy\nbetween two probability distributions\nover the same underlying set of events\nmeasures the average number of bits\nneeded to identify an event drawn from\nthe set that's a mouthful uh really\nwe're just looking at the amount of\nerror in here how many of these are\ncorrect and how many of these um are\nincorrect so how much of it matches and\nwe're going to look at that we're just\ngoing to look at the average that's what\nthe mean the reduced to the mean means\nhere so we're looking at the average\nerror on this\nand so the next step is we're going to\ntake the error we want to know our cross\nentropy or our loss function how much\nloss we have that's going to be part of\nhow we train the model so when you know\nwhat the loss is and we're training it\nwe feed that back into the back\npropagation setup and so we want to go\nahead and optimize that here's our\noptimizer we're going to create the\noptimizer using an atom optimizer\nremember there's a lot of different ways\nof optimizing the data atoms are most\npopular used uh so our optimizer is\ngoing to equal the tf train atom\noptimizer if you don't remember what the\nlearning rate is let me just pop this\nback into here here's our learning rate\nwhen you have your weights you have all\nyour weights and your different nodes\nthat are coming out here's our node\ncoming out and it has all its weights\nand then the error is being prop sent\nback through in reverse on our neural\nnetwork so we take this error we adjust\nthese weights based on the different\nformulas in this case the atom formula\nis what we're using we don't want to\njust adjust them completely we don't\nwant to change this weight so it exactly\nfits the data coming through because if\nwe made that kind of adjustment it's\ngoing to be biased to whatever the last\ndata we sent through is instead we're\ngoing to multiply that by .001 and make\na very small shift in this weight so our\ndelta w is only 0.001 of the actual\ndelta w of the full change we're going\nto compute from the atom and then we\nwant to go ahead and train it so our\ntraining or set up a training\nvariable or function and this is going\nto equal our optimizer minimize cross\nentropy and we make sure we go ahead and\nrun this\nso it's loaded in there and then we're\nalmost ready to train our model but\nbefore we do that we need to create one\nmore\nvariable in here and we're going to\ncreate a variable to initialize all the\nglobal tf variables and when we look at\nthis\nthe tf global variable initializer this\nis a tensorflow\nobject it goes through there and it\nlooks at all our different setup that we\nhave going under our tensorflow and then\ninitializes those variables uh so it's\nkind of like a magic wand because it's\nall hidden in the back end of tensorflow\nall you need to know about this is that\nyou have to have the initialization on\nthere which is an operation\nand you have to run that once you have\nyour setup going so we'll go ahead and\nrun this piece of code and then we're\ngoing to go ahead and train our data so\nlet me run this so it's loaded up there\nand so now we're going to go ahead and\nrun the model by creating a graph\nsession graph session is a tensorflow\nterm so you'll see that coming up it's\none of the things that throws me because\ni always think of graphics and spark and\ngraph as just general graphing uh but\nthey talk about a graph session so we're\ngoing to go ahead and run the model and\nlet's go ahead and walk through this uh\nwhat's going on here and let's paste\nthis data in here and here we go so\nwe're going to start off with it with a\ntf session as cess so that's our actual\ntf session we've created uh so we're\nright here with the tf uh\nsession our session we're creating we're\ngoing to run tf global variable\ninitializer so right off the bat we're\ninitializing our variables here and then\nwe have for i in range 500. so what's\ngoing on here remember 500 we're going\nto break the data up and we're going to\nbatch it in at 500 points each we've\ncreated our session run so we're going\nto do with tf session as session right\nhere we've created our variable session\nand then we're going to run and we're\ngoing to go ahead and initialize it so\nwe have our tf global variables\ninitializer that we created\nthat initializes our our session in here\nthe next thing we're going to do is\nwe're going to go for i in range of 500\nbatch equals ch.nextbatch so if you\nremember correctly this is loading up um\n100 pictures at a time and\nthis is going to loop through that 500\ntimes so we are literally doing uh what\nis that 500 times 100 is\n50 000. so that's 50 000 pictures we're\ngoing to process right there and the\nfirst process is we're going to do a\nsession run we're going to take our\ntrain we created our train variable or\noptimizer in there we're going to feed\nit the dictionary we had our feed\ndictionary that we created and we have x\nequals batch zero coming in y true batch\none\nhold the probability point five\nand then just so that we can keep track\nof what's going on we're gonna every 100\nsteps we're going to run a print so\ncurrently on step format\naccuracy is\nand we're going to look at matches\nequals tf.equal tf argument y prediction\n1 tf.arg max y true comma 1. so we're\ngoing to look at this as how many\nmatches it has and here our acc uh all\nwe're doing here is we're going to take\nthe matches how many matches they have\nit creates it generates a chart we're\ngoing to convert that to float that's\nwhat the tfcast does and then we just\nwant to know the average we just want to\nknow the average of the accuracy and\nthen we'll go ahead and print that out\nprint session run accuracy feed\ndictionary so it takes all this and it\nprints out our accuracy on there so\nlet's go ahead and take this oops\nscreen's there let's go ahead and take\nthis and let's run it and this is going\nto take a little bit to run uh so let's\nsee what happens on my old laptop and\nwe'll see here that we have our current\nuh we're currently on step zero it takes\na little bit to get through the accuracy\nand this will take just a moment to run\nwe can see that on our step 0 it has an\naccuracy of 0.1 or 0.1028\nand as it's running we'll go ahead you\ndon't need to watch it run all the way\nbut this accuracy is going to change a\nlittle bit up and down so we've actually\nlost some accuracy during our step two\nbut we'll see how that comes out let's\ncome back after we run it all the way\nthrough and see how the different steps\ncome out it's actually reading that\nbackwards\nthe way this works is the closer we get\nto one the more accuracy we have so you\ncan see here we've gone from a point one\nto a point three nine um and we'll go\nahead and pause this and come back and\nsee what happens when we're done with\nthe full run all right now that we've\nprepared the meal got it in the oven and\npulled out my finished dish here if\nyou've ever watched any of the old\ncooking shows let's discuss a little bit\nabout this accuracy going on here and\nhow do you interpret that we've done a\ncouple things first we've defined\naccuracy um the reason i got it\nbackwards before is you have\nloss or accuracy and with loss you'll\nget a graph that looks like this it goes\noops that's an s by the way there we go\nyou get a graph that curves down like\nthis and with accuracy you get a graph\nthat curves up this is how good it's\ndoing now in this case uh one is\nsupposed to be really good accuracy that\nmeans it gets close to one but it never\ncrosses one so if you have an accuracy\nof one that is phenomenal in fact that's\npretty much you know unheard of and the\nsame thing with loss if you have a loss\nof zero that's also unheard of and the\nzero's actually on this this axis right\nhere as we go in there so how do we\ninterpret that because you know if i was\nlooking at this and i go oh 0.51 that's\nuh 51 you're doing 50 50. no this is not\npercentage let me just put that in there\nit is not percentage uh this is log\nrhythmic what that means is that 0.2 is\ntwice as good as point one and uh when\nwe see point four that's twice as good\nas point two real way to convert this\ninto a percentage you really can't say\nthis is is a direct percentage\nconversion what you can do though is in\nyour head if we were to give this a\npercentage uh we might look at this as\nuh 50\nwe're just guessing equals 0.1 and if\nfifty percent roughly equals point one\nthat's we started up here at the top\nremember at the top here here's our\npoint one oh two eight the accuracy of\nfifty percent then seventy five percent\nis about point two and so on and so on\ndon't quote those numbers because it\ndoesn't work that way they say that if\nyou have 0.95\nthat's pretty much saying 100\nand if you have anywhere between you'd\nhave to go look this up let me go and\nremove all my drawings there\nso the magic number is 0.5 we really\nwant to be over a 0.5 in this whole\nthing and we have\nboth 0.504 remember this is accuracy if\nwe were looking at loss then we'd be\nlooking the other way but 0.05 you know\ninstead of how high it is we want how\nlow it is uh but with accuracy being\nover a 0.5 is pretty valid that means\nthis is pretty solid and if you get to a\n0.95 then it's a direct correlation\nthat's what we're looking for here in\nthese numbers you can see we finished\nwith this model at 0.5135\nso still good\nand if we look at when they ran this in\nthe other end remember there's a lot of\nrandomness that goes into it when we see\nthe weights they got\n.5251 so a little better than ours but\nthat's fine you'll find your own comes\nup a little bit better or worse\ndepending on just that randomness and so\nwe've gone through the whole model we've\ncreated we trained the model and we've\nalso gone through on every 100th run to\ntest the model to see how accurate it is\nwe will start with the course of\nfundamentals what is a neural network in\npopular neural networks it's important\nto know the framework we're in and what\nwe're going to be looking at\nspecifically then we'll touch on why a\nrecurrent neural network what is a\nrecurrent neural network and how does an\nrn in work\none of the big things about rnns is what\nthey call the vanishing and exploding\ngradient problem so we'll look at that\nand then we're going to be using a use\ncase uh study that's going to be in\ncross on tensorflow cross is a python\nmodule for doing neural networks in deep\nlearning and in there there's the what\nthey call long short term memory lstm\nand then we'll use the use case to\nimplement our lstm on the keras so when\nyou see that lstm that is basically the\nrnn network and we'll get into that the\nuse case is always my favorite part\nbefore we dive into any of this we're\ngoing to take a look at what is an rnn\nor an introduction to the rnn do you\nknow how google's auto complete feature\npredicts the rest of the words a user is\ntyping i love that autocomplete feature\nas i'm typing away saves me a lot of\ntime i can just kind of hit the enter\nkey and it auto fills everything and i\ndon't have to type as much well first\nthere's a collection of large volumes of\nmost frequently occurring consecutive\nwords\nthis is fed into a recurrent neural\nnetwork analysis the data by finding the\nsequence of words occurring frequently\nand builds a model to predict the next\nword in the sentence and then google\nwhat is the best food to eat in loss i'm\nguessing you're going to say los mexico\nno it's going to be las vegas\nso the google search will take a look at\nthat and say hey the most common\nautocomplete is going to be vegas in\nthere it usually gives you three or four\ndifferent choices so it's a very\npowerful tool it saves us a lot of time\nespecially when we're doing a google\nsearch or even in microsoft words has a\nsome people get very mad at it auto\nfills with the wrong stuff but you know\nyou're typing away and it helps you\nautofill i have that in a lot of my\ndifferent packages it's just a standard\nfeature that we're all used to now so\nbefore we dive into the rnn and getting\ninto the depths let's go ahead and talk\nabout what is a neural network neural\nnetworks used in deep learning consist\nof different layers connected to each\nother and work on the structure and\nfunctions of a human brain you're going\nto see that thread human in human brain\nand human thinking throughout deep\nlearning the only way we can evaluate an\nartificial intelligence or anything like\nthat is to compare it to human function\nvery important note on there and it\nlearns from a huge volumes of data and\nit uses complex algorithm to train a\nneural net so in here we have image\npixels of two different breeds of dog\none looks like a nice floppy eared lab\nand one a german shepherd you know both\nwonderful breeds of animals that image\nthen goes into an input layer uh that\ninput layer might be formatted at some\npoint because you have to let it know\nlike you know different pictures are\ngoing to be different sizes and\ndifferent color content then it'll feed\ninto hidden layers so each of those\npixels or each point of data goes in and\nthen splits into the hidden layer which\nthen goes into another hidden layer\nwhich then goes to an output layer rnn\nthere's some changes in there which\nwe're going to get into so it's not just\na straightforward propagation of data\nlike we've covered in many other\ntutorials and finally you have an output\nlayer and the output layer has two\noutputs it has one that lights up if\nit's a german shepherd and another that\nlights up as if it's a labrador so\nidentify as a dog's breed set networks\ndo not require memorizing the past\noutput so our forward propagation is\njust that it goes forward and doesn't\nhave to re-memorize stuff and you can\nsee there that's not actually me in the\npicture dressed up in my\nsuit i haven't worn a suit in years so\nas we're looking at this we're going to\nchange it up a little bit before we\ncover that let's talk about popular\nneural networks first there's the feed\nforward neural network used in general\nregression and classification problems\nand we have the convolution neural\nnetwork used for image recognition deep\nneural network used for acoustic\nmodeling deep belief network used for\ncancer detection and recurrent neural\nnetwork used for speech recognition now\ntaking a lot of these and mixed them\naround a little bit so just because it's\nused for one thing doesn't mean it can't\nbe used for other modeling but generally\nthis is where the field is and this is\nhow those models are generally being\nused right now so we talk about a feed\nforward neural network in a feed forward\nneural network information flows only in\nthe forward direction from the input\nnodes through the hidden layers if any\ninto the output nodes there are no\ncycles or loops in the network and so\nyou can see here we have our input layer\ni was talking about how it just goes\nstraight forward into the hidden layers\nso each one of those connects and then\nconnects to the next hidden layer\nconnects to the output layer and of\ncourse we have a nice simplified version\nwhere it has a predicted output and the\nrefer to the input is x a lot of times\nand the output as y decisions are based\non current input no memory about the\npast no future scope why recurrent\nneural network issues in feed forward\nneural network so one of the biggest\nissues is because it doesn't have a\nscope of memory or time a feed forward\nneural network doesn't know how to\nhandle sequential data it only considers\nonly the current input so if you have a\nseries of things and because three\npoints back affects what's happening now\nand what your output affects what's\nhappening that's very important so\nwhatever i put as an output is going to\naffect the next one a feed forward\ndoesn't look at any of that it just\nlooks at this is what's coming in and it\ncannot memorize previous inputs so it\ndoesn't have that list of inputs coming\nin solution to feed forward neural\nnetwork you'll see here where it says\nrecurrent neural network and we have our\nx on the bottom going to h going to y\nthat's your feed forward but right in\nthe middle it has a value c so there's a\nwhole another process so it's memorizing\nwhat's going on in the hidden layers and\nthe hidden layers as they produce data\nfeed into the next one so your hidden\nlayer might have an output that goes off\nto y\nbut that output goes back into the next\nprediction coming in what this does is\nthis allows it to handle sequential data\nit considers the current input and also\nthe previously received inputs and if\nwe're going to look at general drawings\nand solutions we should also look at\napplications of the rnn image captioning\nrnn is used to caption an image by\nanalyzing the activities present in it a\ndog catching a ball in midair that's\nvery tough i mean you know we have a lot\nof stuff that analyzes images of a dog\nand the image of a ball but it's able to\nadd one more feature in there that's\nactually catching the ball in midair\ntime series prediction any time series\nproblem like predicting the prices of\nstocks in a particular month can be\nsolved using rnn and we'll dive into\nthat in our use case and actually take a\nlook at some stock one of the things you\nshould know about analyzing stock today\nis that it is very difficult and if\nyou're analyzing the whole stock the\nstock market at the new york stock\nexchange in the u.s produces somewhere\nin the neighborhood if you count all the\nindividual trades and fluctuations by\nthe second\nit's like three terabytes a day of data\nso we're going to look at one stock just\nanalyzing one stock is really tricky in\nhere we'll give you a little jump on\nthat so that's exciting but don't expect\nto get rich off of it immediately\nanother application of the rnn is\nnatural language processing text mining\nand sentiment analysis can be carried\nout using rnn for natural language\nprocessing and you can see right here\nthe term natural language processing\nwhen you stream those three words\ntogether is very different than i if i\nsaid processing language natural leap so\nthe time series is very important when\nwe're analyzing sentiments it can change\nthe whole value of a sentence just by\nswitching the words around or if you're\njust counting the words you might get\none sentiment where if you actually look\nat the order they're in you get a\ncompletely different sentiment when it\nrains look for rainbows when it's dark\nlook for stars both of these are\npositive sentiments and they're based\nupon the order of which the sentence is\ngoing in machine translation given an\ninput in one language rnn can be used to\ntranslate the input into a different\nlanguages as output i myself very\nlinguistically challenged but if you\nstudy languages and you're good with\nlanguages you know right away that if\nyou're speaking english you would say\nbig cat and if you're speaking spanish\nyou would say cat big so that\ntranslation is really important to get\nthe right order to get there's all kinds\nof parts of speech that are important to\nknow by the order of the words here this\nperson is speaking in english and\ngetting translated and you can see here\na person is speaking in english in this\nlittle diagram i guess that's denoted by\nthe flags i have a flag i own it no um\nbut they're speaking in english and it's\ngetting translated into\nchinese italian french german and\nspanish languages some of the tools\ncoming out are just so cool so somebody\nlike myself who's very linguistically\nchallenged i can now travel into worlds\ni would never think of because i can\nhave something translate my english back\nand forth readily and i'm not stuck with\na communication gap so let's dive into\nwhat is a recurrent neural network\nrecurrent neural network works on the\nprinciple of saving the output of a\nlayer and feeding this back to the input\nin order to predict the output of the\nlayer sounds a little confusing when we\nstart breaking it down it'll make more\nsense and usually we have a propagation\nforward neural network with the input\nlayers the hidden layers the output\nlayer with the recurrent neural network\nwe turn that on its side so here it is\nand now our x comes up from the bottom\ninto the hidden layers into y and they\nusually draw very simplified x to h with\nc as a loop a to y where a b and c are\nthe perimeters a lot of times you'll see\nthis kind of drawing in here digging\ncloser and closer into the h and how it\nworks going from left to right you'll\nsee that the c goes in and then the x\ngoes in so the x is going upward bound\nand c is going to the right a is going\nout and c is also going out that's where\nit gets a little confusing so here we\nhave x n\nc n and then we have y out and c out and\nc is based on h t minus 1. so our value\nis based on the y and the h value are\nconnected to each other they're not\nnecessarily the same value because h can\nbe its own thing and usually we draw\nthis or we represent it as a function h\nof t equals a function of c where h of t\nminus 1 that's the last h output and x\nof t going in so it's the last output of\nh combined with the new input of x where\nh t is the new state fc is a function\nwith the parameters c that's a common\nway of denoting it h t minus 1 is the\nold state coming out and then x of t is\nan input vector at time of step t\nwell we need to cover types of recurrent\nneural networks and so the first one is\nthe most common one which is a\none-to-one single output\none-to-one neural network is usually\nknown as a vanilla neural network used\nfor regular machine learning problems\nwhy because vanilla is usually\nconsidered kind of a just a real basic\nflavor but because it's very basic a lot\nof times they'll call it the vanilla\nneural network which is not the common\nterm but it is kind of a slang term\npeople will know what you're talking\nabout usually if you say that then we\nrun one to mini so you have a single\ninput and you might have a multiple\noutputs in this case\nimage captioning as we looked at earlier\nwhere we have not just looking at it as\na dog but a dog catching a ball in the\nair and then you have mini to one\nnetwork takes in a sequence of inputs\nexamples sentiment analysis where a\ngiven sentence can be classified as\nexpressing positive or negative\nsentiments and we looked at that as we\nwere discussing if it rains look for a\nrainbow so positive sentiment where rain\nmight be a negative sentiment if you're\njust adding up the words in there and\nthen the course if you're going to do a\none-to-one many-to-one one-to-many\nthere's many to many networks takes in a\nsequence of inputs and generates a\nsequence of outputs example machine\ntranslation so we have a lengthy\nsentence coming in in english and then\ngoing out in all the different languages\nyou know just a wonderful tool very\ncomplicated set of computations you know\nif you're a translator you realize just\nhow difficult it is to translate into\ndifferent languages\none of the biggest things you need to\nunderstand when we're working with this\nneural network is what's called the\nvanishing gradient problem while\ntraining an rnn your slope can be either\ntoo small or very large and this makes\ntraining difficult when the slope is too\nsmall the problem is known as vanishing\ngradient and you'll see here they have a\nnice uh image loss of information\nthrough time so if you're pushing not\nenough information forward that\ninformation is lost and then when you go\nto train it you start losing the third\nword in the sentence or something like\nthat or it doesn't quite follow the full\nlogic of what you're working on\nexploding gradient problem oh this is\none that runs into everybody when you're\nworking with this particular neural\nnetwork when the slope tends to grow\nexponentially instead of decaying this\nproblem is called exploding gradient\nissues in gradient problem long training\ntime poor performance bad accuracy and\ni'll add one more in there your computer\nif you're on a lower end computer\ntesting out a model will lock up and\ngive you the memory error explaining\ngradient problem consider the following\ntwo examples to understand what should\nbe the next word in the sequence\nthe person who took my bike and blank a\nthief the students who got into\nengineering with blank from asia and you\ncan see in here we have our x value\ngoing in we have the previous value\ngoing forward and then you back\npropagate the error like you do with any\nneural network and as we're looking for\nthat missing word maybe we'll have the\nperson took my bike and blank was a\nthief and the student who got into\nengineering with a blank were from asia\nconsider the following example the\nperson who took the bike so we'll go\nback to the person who took the bike was\nblank a thief in order to understand\nwhat would be the next word in the\nsequence the rnn must memorize the\nprevious context whether the subject was\nsingular noun or a plural noun so was a\nthief is singular the student who got\ninto engineering well in order to\nunderstand what would be the next word\nin the sequence the rnn must memorize\nthe previous context whether the subject\nwas singular noun or a plural noun and\nso you can see here the students who got\ninto engineering with blank were from\nasia it might be sometimes difficult for\nthe air to back propagate to the\nbeginning of the sequence to predict\nwhat should be the output so when you\nrun into the gradient problem we need a\nsolution the solution to the gradient\nproblem first we're going to look at\nexploding gradient where we have three\ndifferent solutions depending on what's\ngoing on one is identity initialization\nso the first thing we want to do is see\nif we can find a way to minimize the\nidentities coming in instead of having\nit identify everything just the\nimportant information we're looking at\nnext is to truncate the back propagation\nso instead of having whatever\ninformation it's sending to the next\nseries we can truncate what it's sending\nwe can lower that particular set of\nlayers make those smaller and finally is\na gradient clipping so when we're\ntraining it we can clip what that\ngradient looks like and narrow the\ntraining model that we're using when you\nhave a vanishing gradient the option\nproblem\nwe can take a look at weight\ninitialization very similar to the\nidentity but we're going to add more\nweights in there so it can identify\ndifferent aspects of what's coming in\nbetter choosing the right activation\nfunction that's huge so we might be\nactivating based on one thing and we\nneed to limit that we haven't talked too\nmuch about activation functions so we'll\nlook at that just minimally there's a\nlot of choices out there and then\nfinally there's long short term memory\nnetworks the lstms and we can make\nadjustments to that so just like we can\nclip the gradient as it comes out we can\nalso\nexpand on that we can increase the\nmemory network the size of it so it\nhandles more information and one of the\nmost common problems in today's\nsetup is what they call long-term\ndependencies suppose we try to predict\nthe last word in the text the clouds are\nin the and you probably said sky here we\ndo not need any further context it's\npretty clear that the last word is going\nto be sky suppose we try to predict the\nlast word in the text i have been\nstaying in spain for the last 10 years i\ncan speak fluent maybe you said\nportuguese or french no you probably\nsaid spanish the word we predict will\ndepend on the previous few words in\ncontext here we need the context of\nspain to predict the last word in the\ntext it's possible that the gap between\nthe relevant information and the point\nwhere it is needed to become very large\nlstms help us solve this problem so the\nlstms are a special kind of recurrent\nneural network capable of learning\nlong-term dependencies remembering\ninformation for long periods of time is\ntheir default behavior all recurrent\nneural networks have the form of a chain\nof repeating modules of neural network\nconnections in standard rnns this\nrepeating module will have a very simple\nstructure such as a single tangent h\nlayer lstms's\nalso have a chain like structure but the\nrepeating module has a different\nstructure instead of having a single\nneural network layer there are four\ninteracting layers communicating in a\nvery special way\nlstms are a special kind of recurrent\nneural network capable of learning\nlong-term dependencies remembering\ninformation for long periods of time is\ntheir default behavior ls tmss also have\na chain-like structure but the repeating\nmodule has a different structure instead\nof having a single neural network layer\nthere are four interacting layers\ncommunicating in a very special way as\nyou can see the deeper we dig into this\nthe more complicated the graphs get in\nhere i want you to note that you have x\nof t minus 1 coming in you have x of t\ncoming in and you have x at t plus one\nand you have h of t minus one and h of t\ncoming in and h of t plus one going out\nand of course uh on the other side is\nthe output a\nin the middle we have our tangent h but\nit occurs in two different places so not\nonly when we're computing the x a t plus\none or we getting the tangent h from x\nto t but we're also getting that value\ncoming in from the x at t minus one so\nthe short of it is as you look at these\nlayers not only does it does the\npropagate through the first layer goes\ninto the second layer back into itself\nbut it's also going into the third layer\nso now we're kind of stacking those up\nand this can get very complicated as you\ngrow that in size it also grows in\nmemory too and in the amount of\nresources it takes but it's a very\npowerful tool to help us address the\nproblem of complicated long sequential\ninformation coming in like we were just\nlooking at in the sentence and when\nwe're looking at our long short term\nmemory network uh there's three steps of\nprocessing in the lstms that we look at\nthe first one is we want to forget\nirrelevant parts of the previous state\nyou know a lot of times like you know is\nas in a unless we're trying to look at\nwhether it's a plural noun or not they\ndon't really play a huge part in the\nlanguage so we want to get rid of them\nthen selectively update cell state\nvalues so we only want to update the\ncell state values that reflect what\nwe're working on and finally we want to\nput only output certain parts of the\ncell state so whatever's coming out we\nwant to limit what's going out too and\nlet's dig a little deeper into this\nlet's just see what this really looks\nlike\nso step one decides how much of the past\nit should remember first step in the\nlstm is to decide which information to\nbe omitted in from the cell in that\nparticular time step it is decided by\nthe sigmoid function it looks at the\nprevious state h to t minus 1 and the\ncurrent input x of t and computes the\nfunction so you can see over here we\nhave a function of t\nequals the sigmoid function of the\nweight of f the h at t minus 1 and then\nx of t plus of course you have a bias in\nthere with any of our neural networks so\nwe have a bias function so f of t equals\nforget gate decides which information to\ndelete that is not important from the\nprevious time step considering an stm is\nfed with the following inputs from the\nprevious and present time step alice is\ngood in physics john on the other hand\nis good in chemistry so previous output\njohn plays football well he told me\nyesterday over the phone that he had\nserved as a captain of his college\nfootball team that's our current input\nso as we look at this the first step is\nthe forget gate realizes there might be\na change in context after encountering\nthe first full stop compares with the\ncurrent input sentence of x a t so we're\nlooking at that full stop and then\ncompares it with the input of the new\nsentence the next sentence talks about\njohn so the information on alice is\ndeleted okay that's important to know so\nwe have this input coming in and if\nwe're going to continue on with john\nthen that's going to be the primary\ninformation we're looking at the\nposition of the subject is vacated and\nis assigned to john and so in this one\nwe've seen that we've weeded out a whole\nbunch of information and we're only\npassing information on john since that's\nnow the new topic so step two is then to\ndecide how much should this unit add to\nthe current state in the second layer\nthere are two parts one is the sigmoid\nfunction and the other is the tangent h\nin the sigmoid function it decides which\nvalues to let through zero or one\ntangent h function gives a weightage to\nthe values which are passed deciding\ntheir level of importance minus one to\none and you can see the two formulas\nthat come up the i of t equals the\nsigmoid of the weight of i h to t minus\none x of t plus the bias of i and the c\nof t equals the tangent of h of the\nweight of c of h of t minus one x of t\nplus the bias of c so our i of t equals\nthe input gate determines which\ninformation to let through based on its\nsignificance in the current time step if\nthis seems a little complicated don't\nworry because a lot of the programming\nis already done when we get to the case\nstudy understanding though that this is\npart of the program is important when\nyou're trying to figure out these what\nto set your settings at you should also\nnote when you're looking at this it\nshould have some semblance to your\nforward propagation neural networks\nwhere we have a value assigned to a\nweight plus a bias very important steps\nthan any of the neural network layers\nwhether we're propagating into them the\ninformation from one to the next or\nwe're just doing a straightforward\nneural network propagation let's take a\nquick look at this what it looks like\nfrom the human standpoint as i step out\nof my suit again consider the current\ninput at x of t john plays football well\nhe told me yesterday over the phone that\nhe had served as a captain of his\ncollege football team that's our input\ninput gate announces the important\ninformation john plays football and he\nwas a captain of his college team is\nimportant he told me over the phone\nyesterday is less important hence it is\nforgotten this process of adding some\nnew information can be done via the\ninput gate now this example is as a\nhuman form and we'll look at training\nthis stuff in just a minute\nbut as a human being if i wanted to get\nthis information from a conversation\nmaybe it's a google voice listening in\non you or something like that um how do\nwe weed out the information that he was\ntalking to me on the phone yesterday\nwell i don't want to memorize that he\ntalked to me on the phone yesterday or\nmaybe that is important but in this case\nit's not i want to know that he was the\ncaptain of the football team i want to\nknow that he served i want to know that\njohn plays football and he was a captain\nof the college football team those are\nthe two things that i want to take away\nas a human being again we measure a lot\nof this from the human viewpoint and\nthat's also how we try to train them so\nwe can understand these neural networks\nfinally we get to step three decides\nwhat part of the current cell state\nmakes it to the output the third step is\nto decide what will be our output first\nwe run a sigmoid layer which decides\nwhat parts of the cell state make it to\nthe output then we put the cell state\nthrough the tangent h to push the values\nto be between -1 and 1 and multiply it\nby the output of the sigmoid gate so\nwhen we talk about the output of t we\nset that equal to the sigmoid of the\nweight of 0 of the h of t minus 1 back\none step in time by the x of t plus of\ncourse the bias the h of t equals the\nouter t times the tangent of the tangent\nh of c of t so our o t equals the output\ngate allows the passed in information to\nimpact the output in the current time\nstep let's consider the example to\npredicting the next word in the sentence\njohn played tremendously well against\nthe opponent and won for his team for\nhis contributions brave blank was\nawarded player of the match there could\nbe a lot of choices for the empty space\ncurrent input brave is an adjective\nadjectives describe a noun john could be\nthe best output after brave thumbs up\nfor john awarded player of the match and\nif you were to pull just the nouns out\nof the sentence team doesn't look right\nbecause that's not really the subject\nwe're talking about contributions you\nknow brave contributions or brave teen\nbrave player brave match\nso you look at this and you can start to\ntrain this these this neural network so\nstarts looking at and goes oh no john is\nwhat we're talking about so brave is an\nadjective\njohn's going to be the best output and\nwe give john a big thumbs up and then of\ncourse we jump into my favorite part the\ncase study use case implementation of\nlstm let's predict the prices of stocks\nusing the lstm network based on the\nstock price data between 2012\n2016. we're going to try to predict the\nstock prices of 2017.\nand this will be a narrow set of data\nwe're not going to do the whole stock\nmarket it turns out that the new york\nstock exchange generates roughly three\nterabytes of data per day that's all the\ndifferent trades up and down of all the\ndifferent stocks going on in each\nindividual one\nsecond to second or nanosecond to\nnanosecond but we're going to limit that\nto just some very basic fundamental\ninformation so don't think you're going\nto get rich off this today but at least\nyou can give an eye you can give a step\nforward in how to start processing\nsomething like stock prices a very valid\nuse for machine learning in today's\nmarkets\nuse case implementation of lstm\nlet's dive in we're going to import our\nlibraries we're going to import the\ntraining set and get the scaling going\nnow if you watch any of our other\ntutorials a lot of these pieces just\nstart to look very familiar because it's\nvery similar setup let's take a look at\nthat and just reminder we're going to be\nusing anaconda the jupiter notebook so\nhere i have my anaconda navigator when\nwe go under environments i've actually\nset up a cross python 36 i'm in python36\nand\nnice thing about anaconda especially the\nnewer version remember a year ago\nmessing with anaconda in different\nversions of python in different\nenvironments anaconda now has a nice\ninterface\nand i have this installed both on a\nubuntu linux machine and on windows so\nit works fine on there you can go in\nhere and open a terminal window and then\nin here once you're in the terminal\nwindow this is where you're going to\nstart\ninstalling using pip to install your\ndifferent modules and everything now\nwe've already pre-installed them so we\ndon't need to do that in here but if you\ndon't have them installed in your\nparticular environment you'll need to do\nthat and of course you don't need to use\nthe anaconda or the jupiter you can use\nwhatever favorite python id you like i'm\njust a big fan of this because it keeps\nall my stuff separate you can see on\nthis machine i have specifically\ninstalled one for cross since we're\ngoing to be working with cross under\ntensorflow we go back to home i've gone\nup here to application and that's the\nenvironment i've loaded on here and then\nwe'll click on the launch jupiter\nnotebook now i've already in my jupiter\nnotebook um have set up a lot of stuff\nso that we're ready to go kind of like\nuh martha stewart's in the old cooking\nshow so we want to make sure we have all\nour tools for you so you're not waiting\nfor them to load and if we go up here to\nwhere it says new you can see where you\ncan\ncreate a new python 3. that's what we\ndid here underneath the setup so it\nalready has all the modules installed on\nit and i'm actually rename this if you\ngo under file you can rename it we've\ni'm calling it rnn stock and let's just\ntake a look and start diving into the\ncode let's get into the exciting part\nnow we've looked at the tool and of\ncourse you might be using a different\ntool which is fine let's start putting\nthat code in there and seeing what those\nimports and uploading everything looks\nlike now first half is kind of boring\nwhen we hit the run button because we're\ngoing to be importing numpy as np that's\nuh the number python which is your numpy\narray and the matplot library because\nwe're going to do some plotting at the\nend and our pandas for our data set our\npandas is pd and when i hit run it\nreally doesn't do anything except for\nload those modules just a quick note let\nme just do a quick draw here oops shift\nalt there we go you'll notice when we're\ndoing this setup if i was to divide this\nup oops i'm going to actually let's\noverlap these here we go\nthis first part that we're going to do\nis\nour data\nprep\na lot of prepping involved\num in fact depending on what your system\nand since we're using karas i put an\noverlap here uh but you'll find that\nalmost\nmaybe even half of the code we do is all\nabout the data prep and the reason i\noverlapped this with uh cross let me\njust put that down because that's what\nwe're working in uh is because cross has\nlike their own preset stuff so it's\nalready pre-built in which is really\nnice so there's a couple steps a lot of\ntimes that are in the kara setup we'll\ntake a look at that to see what comes up\nin our code as we go through and look at\nstock and the last part is to evaluate\nand if you're working with\nshareholders or\nclassroom whatever it is you're working\nwith uh the evaluate is the next biggest\npiece um so the actual code here crossed\nis a little bit more but when you're\nworking with some of the other packages\nyou might have like three lines that\nmight be it all your stuff is in your\npre-processing and your data since cross\nhas is cutting edge and you load the\nindividual layers you'll see that\nthere's a few more lines here and\ncrosses a little bit more robust and\nthen you spend a lot of times like i\nsaid with the evaluate you want to have\nsomething you present to everybody else\nand say hey this is what i did this is\nwhat it looks like so let's go through\nthose steps this is like a kind of just\ngeneral overview and let's just take a\nlook and see what the next set of code\nlooks like and in here we have a data\nset train and it's going to be read\nusing the pd or pandas dot read csv and\nit's a google stock price train dot csv\nand so under this we have training set\nequals data set train dot i location and\nwe've kind of sorted out part of that so\nwhat's going on here let's just take a\nlook at let's look at the actual file\nand see what's going on there now if we\nlook at this\nignore all the extra files on this i\nalready have a train and a test set\nwhere it's sorted out this is important\nto notice because a lot of times we do\nthat as part of the pre-processing of\nthe data we take\n20 percent of the data out so we can\ntest it and then we train the rest of it\nthat's what we use to create our neural\nnetwork that way we can find out how\ngood it is uh but let's go ahead and\njust take a look and see what that looks\nlike as far as the file itself and i\nwent ahead and just opened this up in a\nbasic word pad and text editor just so\nwe can take a look at it certainly you\ncan open up an excel or any other kind\nof spreadsheet\nand we note that this is a comma\nseparated variables we have a date\nopen high low close volume this is the\nstandard stuff that we import into our\nstock or the most basic set of\ninformation you can look at in stock\nit's all free to download in this case\nwe downloaded it from google that's why\nwe call it the google stock price\nand this specifically is google this is\nthe google stock values from as you can\nsee here we started off at 1 3 2012. so\nwhen we look at this first setup up here\nwe have a data set train equals pd\nunderscore csv and if you noticed on the\noriginal frame\nlet me just go back there\nthey had it set to home ubuntu downloads\ngoogle stock price train i went ahead\nand changed that because we're in the\nsame file where i'm running the code so\ni've saved this particular python code\nand i don't need to go through any\nspecial paths or have the full path on\nthere and then of course we want to take\nout\ncertain values in here and you're going\nto notice that we're using\nour data set\nand we're now in pandas\nso pandas basically it looks like a\nspreadsheet\nand in this case we're going to do eye\nlocation which is going to get specific\nlocations the first value is going to\nshow us that we're pulling all the rows\nand the data and the second one is we're\nonly going to look at columns one and\ntwo and if you remember here from our\ndata as we switch back on over columns\nwe always start with zero which is the\ndate and we're going to be looking at\nopen\nand high which would be one and two\nwe'll just label that right there so you\ncan see now\nwhen you go back and do this you\ncertainly can extrapolate and do this on\nall the columns\nbut for the example let's just limit a\nlittle bit here so that we can focus on\njust some\nkey aspects of stock\nand then we'll go up here and run the\ncode and again i said the first half is\nvery boring whenever we hit the run\nbutton it doesn't do anything because\nwe're still just loading the data and\nsetting it up\nnow that we've loaded our data we want\nto go ahead and scale it we want to do\nwhat they call feature scaling and in\nhere we're going to pull it up from the\nsk learn or the sk kit pre-processing\nimport min max scalar and when you look\nat this you got to remember that\nbiases in our data we want to get rid of\nthat so if you have something that's\nlike a really high value let's just draw\na quick graph\nand i have something here like the maybe\nthe stock has a value one stock has a\nvalue of a hundred and another stock has\na value of five\num you start to get a bias between\ndifferent stocks and so when we do this\nwe go ahead and say okay 100 is going to\nbe the max\nand 5 is going to be the min and then\neverything else goes and then we change\nthis so we just squish it down\ni like the word squish so it's between 1\nand 0. so 100 equals one or one equals a\nhundred and zero equals five and you can\njust multiply it's usually just a simple\nmultiplication we're using uh\nmultiplication\nso it's going to be uh minus five and\nthen 100 divided or 95 divided by 1 so\nor whatever value is is divided by 95.\nand once we've actually created our\nscale we've telling is going to be from\n0 to 1. we want to take our training set\nand we're going to create a training set\nscaled and we're going to use our scalar\nsc and we're going to fit we're going to\nfit and transform the training set uh so\nwe can now use the sc this this\nparticular object will use it later on\nour testing set because remember we have\nto also scale that when we go to test\nour model and see how it works and we'll\ngo ahead and click on the run again it's\nnot going to have any output yet because\nwe're just setting up all the variables\nokay so we paste the data in here and\nwe're going to create the data structure\nwith the 60 time steps and output\nfirst note we're running 60 time steps\nand that is where this value here also\ncomes in so the first thing we do is we\ncreate our x train and y train variables\nwe set them to an empty python array\nvery important to remember what kind of\narray we're in what we're working with\nand then we're going to come in here\nwe're going to go for i in range 60 to\n1258 there's our 60 60 time steps and\nthe reason we want to do this is as\nwe're adding the data in there's nothing\nbelow the 60. so if we're going to use\n60 time steps we have to start at point\n60 because it includes everything\nunderneath of it otherwise you'll get a\npointer error and then we're going to\ntake our x train and we're going to\nappend training set scaled this is a\nscaled value between 0 and 1\nand then as i is equal to 60 this value\nis going to be\n60 minus 60 is 0. so this actually is 0\nto i so it's going to be 0 is 60 1 to\n61. let me just circle this part right\nhere 1 to 61\n2 to 62 and so on and so on and if you\nremember i said 0 to 60 that's incorrect\nbecause it does not count remember it\nstarts at 0 so this is a count of 60. so\nit's actually 59. important to remember\nthat as we're looking at this and then\nthe second part of this that we're\nlooking at so if you remember correctly\nhere we go we go from 0 to 59 of i and\nthen we have a comma a 0 right here and\nso finally we're just going to look at\nthe open value now i know we did put it\nin there for 1 to 2.\nif you remember quickly it doesn't count\nthe second one so it's just the open\nvalue we're looking at just open\num and then finally we have y train dot\nappend training set i to zero and if you\nremember correctly i two or i comma zero\nif you remember correctly this is 0 to\n59 so there's 60 values in it uh so we\ndo i down here this is number 60. so\nwe're going to do this is we're creating\nan array and we have 0\nto 59\nand over here we have number 60 which is\ngoing into the y train it's being\nappended on there and then this just\ngoes all the way up so this is down here\nis a\n0 to 59 and we'll call it 60 since\nthat's the value over here and it goes\nall the way up to 12\n58. that's where this value here comes\nin that's the length of the data we're\nloading\nso we've loaded two arrays we've loaded\none array that has which is filled with\narrays from 0 to 59 and we loaded one\narray which is just the value and what\nwe're looking at you want to think about\nthis as a time sequence\nhere's my open open open open openopen\nwhat's the next one in the series so\nwe're looking at the google stock and\neach time it opens we want to know what\nthe next one 0 through 59 what's 60 1\nthrough 60 what's 61 2 through 62 what's\n62 and so on and so on going up and then\nonce we've loaded those in our for loop\nwe go ahead and take x train and y train\nequals\nnp.arrayxtrain.npraytrain we're just\nconverting this back into a numpy array\nthat way we can use all the cool tools\nthat we get with numpy array including\nreshaping so if we take a look and see\nwhat's going on here we're going to take\nour x train\nwe're going to reshape it\nwow what the heck does reshape mean\nthat means we have an array if you\nremember correctly\nso many numbers by 60.\nthat's how wide it is\nand so we're when you when you do x\ntrain dot shape\nthat gets one of the shapes and you get\nx train dot shape of one gets the other\nshape and we're just making sure the\ndata is formatted correctly and so you\nuse this to pull the fact that it's 60\nby um\nin this case where's that value 60 pi\n1199 1258 minus 60 11.99 and we're\nmaking sure that that is shaped\ncorrectly so the data is grouped into\n11\n99 by 60 different arrays and then the\none on the end just means at the end\nbecause this when you're dealing with\nshapes and numpy they look at this as\nlayers and so the end layer needs to be\none value that's like the leaf of a tree\nwhere this is the branch and then it\nbranches out some more\nand then you get the leaf np dot reshape\ncomes from and using the existing shapes\nto form it we'll go ahead and run this\npiece of code again there's no real\noutput and then we'll import our\ndifferent cross modules that we need so\nfrom cross models we're going to import\nthe sequential model dealing with\nsequential data we have our dense layers\nwe have actually three layers we're\ngoing to bring in our dents our lstm\nwhich is what we're focusing on and our\ndropout and we'll discuss these three\nlayers more in just a moment but you do\nneed the with the lstm you do need the\ndrop out and then the final layer will\nbe the dents but let's go ahead and run\nthis and they'll bring port our modules\nand you'll see we get an error on here\nand if you read it closer it's not\nactually an error it's a warning what\ndoes this warning mean these things come\nup all the time when you're working with\nsuch cutting edge modules they're\ncompletely being updated all the time\nwe're not going to worry too much about\nthe warning all it's saying is that the\nh5py\nmodule which is part of cross is going\nto be updated at some point and if\nyou're running new stuff on cross and\nyou start updating your cross system you\nbetter make sure that your h5 pi is\nupdated too otherwise you're going to\nhave an error later on and you can\nactually just run an update on the h5 pi\nnow if you wanted to not a big deal\nwe're not going to worry about that\ntoday and i said we were going to jump\nin and start looking at what those\nlayers mean i meant that and we're going\nto start off with initializing the rnn\nand then we'll start adding those layers\nin and you'll see that we have the lstm\nand then the dropout lstm then dropout\nlstm then dropout what the heck is that\ndoing so let's explore that we'll start\nby initializing the rnn regressor equals\nsequential because we're using the\nsequential model and we'll run that and\nload that up and then we're going to\nstart adding our lstm layer and some\ndropout regularization and right there\nshould be the q dropout regularization\nand if we go back here and remember our\nexploding gradient well that's what\nwe're talking about the dropout drops\nout unnecessary data so we're not just\nshifting huge amounts of data through\nthe network so and so we go in here\nlet's just go ahead and add this in i'll\ngo ahead and run this and we had three\nof them so let me go ahead and put all\nthree of them in and then we can go back\nover them there's the second one and\nlet's put one more in let's put that in\nand we'll go ahead and put two more in i\nmean i said one more in but it's\nactually two more in and then let's add\none more after that and as you can see\neach time i run these they don't\nactually have an output so let's take a\ncloser look and see what's going on here\nso we're going to add our first lstm\nlayer in here we're going to have units\n50. the units is the positive integer\nand it's the dimensionality of the\noutput space this is what's going out\ninto the next layer so we might have 60\ncoming in but we have 50 going out we\nhave a return sequence because it is a\nsequence data so we want to keep that\ntrue and then you have to tell it what\nshape it's in well we already know the\nshape by just going in here and looking\nat x train shape so input shape equals\nthe x train shape of one comma one it\nmakes it really easy you don't have to\nremember all the numbers and put in 60\nor whatever else is in there you just\nlet it tell the regressor what model to\nuse and so we follow our stm with a\ndropout layer now understanding the\ndropout layer is kind of exciting\nbecause one of the things that happens\nis we can over train our network that\nmeans that our neural network will\nmemorize such specific data that it has\ntrouble predicting anything that's not\nin that specific realm to fix for that\neach time we run through the training\nmode we're going to take point two or\ntwenty percent of our neurons they just\nturn them off so we're only gonna train\non the other ones and it's gonna be\nrandom that way each time we pass\nthrough this we don't over train these\nnodes come back in in the next training\ncycle we randomly pick a different 20.\nand finally i see a big difference as we\ngo from the first to the second and\nthird and fourth the first thing is we\ndon't have to input the shape because\nthe shapes already the output units is\n50 here this item the next step\nautomatically knows this layer is\nputting out 50 and because it's the next\nlayer it automatically sets that and\nsays oh 50 is coming out from our last\nlayer that's coming up you know goes\ninto the regressor and of course we have\nour dropout and that's what's coming\ninto this one and so on and so on and so\nthe next three layers we don't have to\nlet it know what the shape is it\nautomatically understands that and we're\ngoing to keep the units the same we're\nstill going to do 50 units it's still a\nsequence coming through 50 units and a\nsequence now the next piece of code is\nwhat brings it all together let's go\nahead and take a look at that and we\ncome in here we put the output layer the\ndense layer and if you remember up here\nwe had the three layers we had uh lstm\ndropout and dents dense just says we're\ngoing to bring this all down into one\noutput instead of putting out a sequence\nwe just know i want to know the answer\nat this point and let's go ahead and run\nthat and so in here you notice all we're\ndoing is setting things up one step at a\ntime so far we've brought in our way up\nhere we brought in our data we brought\nin our different modules we formatted\nthe data for training it we set it up\nyou know we have our y x train and our y\ntrain we have our source of data and the\nanswers where we know so far that we're\ngoing to put in there we've reshaped\nthat we've come in and built our cross\nwe've imported our different layers and\nwe have in here if you look we have what\nfive total layers now cross is a little\ndifferent than a lot of other systems\nthere's a lot of other systems put this\nall in one line and do it automatic but\nthey don't give you the options of how\nthose layers interface and they don't\ngive you the options of how the data\ncomes in cross is cutting edge for this\nreason so even those a lot of extra\nsteps in building the model this has a\nhuge impact on the output and what we\ncan do with this these new models from\ncross so we brought in our dense we have\nour full model put together a regressor\nso we need to go ahead and compile it\nand then we're going to go ahead and fit\nthe data we're going to compile the\npieces so they all come together and\nthen we're going to run our training\ndata on there and actually recreate our\nregressor so it's ready to be used so\nlet's go ahead and compile that and i'd\ngo ahead and run that and if you've been\nlooking at any of our other tutorials on\nneural networks you'll see we're going\nto use the optimizer atom atom is\noptimized for big data there's a couple\nother optimizers out there beyond the\nscope of this tutorial but certainly\natom will work pretty good for this and\nloss equals mean squared value so when\nwe're training it this is what we want\nto base the loss on how bad is our error\nwe're going to use the mean squared\nvalue for our error and the atom\noptimizer for its differential equations\nyou don't have to know the math behind\nthem but certainly it helps to know what\nthey're doing and where they fit into\nthe bigger models and then finally we're\ngoing to do our fit fitting the rnn to\nthe training set we have the\nregressor.fit x train y train epics and\nbatch size so we know where this is this\nis our data coming in for the x train\nour y train is the answer we're looking\nfor of our data our sequential input\nepics is how many times we're going to\ngo over the whole data set we created a\nwhole data set of x trains so this is\neach each of those rows which includes a\ntime sequence of 60. and batch size\nanother one of those things where cross\nreally shines is if you were pulling the\nsave from a large file instead of trying\nto load it all into ram it can now pick\nsmaller batches up and load those\nindirectly we're not worried about\npulling them off a file today this isn't\nbig enough to cause a computer too much\nof a problem to run not too straining on\nthe resources but as we run this you can\nimagine what happened if i was doing a\nlot more than just one column in one\nset of stock in this case google stock\nimagine if i was doing this across all\nthe stocks and i had instead of just the\nopen i had open close high low and you\ncan actually find yourself with about 13\ndifferent variables times 60 because\nit's a time sequence suddenly you find\nyourself with a gig of memory you're\nloading into your ram which will just\ncompletely you know if it's just if\nyou're not on multiple computers or\ncluster you can start running into\nresource problems but for this we don't\nhave to worry about that so let's go\nahead and run this and this will\nactually take a little bit on my\ncomputer it's an older laptop and give a\nsecond to kick in there there we go all\nright so we have epic so this is going\nto tell me it's running the first run\nthrough all the data and as it's going\nthrough it's batching them in 32 pieces\nso 32 lines each time and there's 1198 i\nthink i said 11.99 earlier but it's\n11.98 i was off by one and each one of\nthese is 13 seconds so you can imagine\nthis is roughly 20 to 30 minutes run\ntime on this computer like i said it's\nan older laptop running at 0.9 gigahertz\non a dual processor and that's fine what\nwe'll do is i'll go ahead and stop go\nget a drink of coffee and come back and\nlet's see what happens at the end and\nwhere this takes us and like any good\ncooking show\ni've gone and got in my latte i also had\nsome other stuff running in the\nbackground so you'll see these numbers\njumped up to like 19 seconds 15 seconds\nbut you can scroll through and you can\nsee we've run it through 100 steps or\n100 epics so the question is what does\nall this mean one of the first things\nyou'll notice is that our loss can is\nover here it kind of stopped at .014 but\nyou can see it kind of goes down until\nwe hit about 0.014 three times in a row\nso we guessed our epic pretty close\nsince our losses remain the same on\nthere so to find out what we're looking\nat we're going to go ahead and load up\nour test data the test data that we\ndidn't process yet and a real stock\nprice data set test eye location this is\nthe same thing we did it when we prepped\nthe data in the first place so let's go\nahead and go through this code and we\ncan see we've labeled it part three\nmaking the predictions and visualizing\nthe results so the first thing that we\nneed to do is go ahead and read the data\nin from our test csv you see i've\nchanged the path on it for my computer\nand then we'll call it the real stock\nprice and again we're doing just the one\ncolumn here and the values from i\nlocation so it's all the rows and just\nthe values from these that one location\nthat's the open stock open let's go\nahead and run that so that's loaded in\nthere and then let's go ahead and create\nwe have our inputs we're going to create\ninputs here and this should all look\nfamiliar because this is the same thing\nwe did before we're going to take our\ndata set total we're going to do a\nlittle panda concat from the data state\ntrain now remember the end of the data\nset train is part of the data going in\nand let's just visualize that just a\nlittle bit here's our train data let me\njust put tr for train and it went up to\nthis value here but each one of these\nvalues generated a bunch of columns it\nwas 60 across and this value here equals\nthis one and this value here equals this\none and this value here equals this one\nand so we need these top 60 to go into\nour new data so to find out we're\nlooking at we're going to go ahead and\nload up our test data the test data that\nwe didn't process yet and a real stock\nprice data set test\nthis is the same thing we did when we\nprepped the data in the first place so\nlet's go ahead and go through this code\nand we can see we've labeled it part\nthree making the predictions and\nvisualizing the results so the first\nthing we need to do is go ahead and read\nthe data in from our test csv you see\ni've changed the path on it for my\ncomputer and then we'll call it the real\nstock price and again we're doing just\nthe one column here and the values from\ni location so it's all the rows and just\nthe values from these that one location\nthat's the open stock open let's go\nahead and run that so that's loaded in\nthere and then let's go ahead and create\nwe have our inputs we're going to create\ninputs here and this would all look\nfamiliar because this is the same thing\nwe did before we're going to take our\ndata set total we're going to do a\nlittle panda concat from the datastate\ntrain now remember the end of the data\nset train is part of the data going in\nlet's just visualize that just a little\nbit here's our train data let me just\nput tr for train and it went up to this\nvalue here but each one of these values\ngenerated a bunch of columns it was 60\nacross and this value here equals this\none and this value here equals this one\nand this value here equals this one and\nso we need these top 60 to go into our\nnew data because that's part of the next\ndata or it's actually the top 59. so\nthat's what this first setup is over\nhere is we're going in we're doing the\nreal stock price and we're going to just\ntake the data set test and we're going\nto load that in and then the real stock\nprice is our data test test location so\nwe're just looking at that first column\nthe open price and then our data set\ntotal we're going to take pandas and\nwe're going to concat and we're going to\ntake our data set train for the open and\nour dataset test open and this is one\nway you can reference these columns\nwe've referenced them a couple of\ndifferent ways we've referenced them up\nhere with the one two but we know it's\nlabeled as a panda set as open so pandas\nis great that way lots of versatility\nthere and we'll go ahead and go back up\nhere and run this there we go and you'll\nnotice this is the same as what we did\nbefore we have our open data set where\npended our two different or concatenated\nour two data sets together we have our\ninputs equals data set total length data\nset total minus length of data set minus\ntest minus 60 values so we're going to\nrun this over all of them and you'll see\nwhy this works because normally when\nyou're running your test set versus your\ntraining set you run them completely\nseparate but when we graph this you'll\nsee that we're just going to be we'll be\nlooking at the part that we didn't train\nit with to see how well it graphs and we\nhave our inputs equals inputs dot\nreshapes or reshaping like we did before\nwe're transforming our inputs so if you\nremember from the transform between zero\nand one and uh finally we want to go\nahead and take our x test and we're\ngoing to create that x test and for i in\nrange 60 to 80. so here's our x test and\nwe're appending our inputs i to 60 which\nremember is 0 to 59 and i comma 0 on the\nother side so it's just the first column\nwhich is our open column and once again\nwe take our x test we convert it to a\nnumpy array we do the same reshape we\ndid before and then we get down to the\nfinal two lines and here we have\nsomething new right here on these last\ntwo lines let me just highlight those or\nor mark them predicted stock price\nequals regressor dot predicts x test so\nwe're predicting all the stock including\nboth the training and the testing model\nhere and then we want to take this\nprediction and we want to inverse the\ntransform so remember we put it between\n0 and 1. well that's not going to mean\nvery much to me to look at a float\nnumber between 0 and 1 i want the dollar\namount so i want to know what the cash\nvalue is and we'll go ahead and run this\nand you'll see it runs much quicker than\nthe training that's what's so wonderful\nabout these neural networks once you put\nthem together it takes just a second to\nrun the same neural network that took us\nwhat a half hour to train add and plot\nthe data we're going to plot what we\nthink it's going to be and we're going\nto plot it against the real data what\nthe google stock actually did so let's\ngo ahead and take a look at that in code\nand let's uh pull this code up so we\nhave our plt that's our oh if you\nremember from the very beginning let me\njust go back up to the top we have our\nmatte plot library dot pi plot as plt\nthat's where that comes in and we come\ndown here we're going to plot let me get\nmy drawing thing out again we're going\nto go ahead and plt is basically kind of\nlike an object it's one of the things\nthat always through me when i'm doing\ngraphs in python because i always think\nyou have to create an object and then it\nloads that class in there well in this\ncase plt is like a canvas you're putting\nstuff on so if you've done html5 you'll\nhave the canvas object this is the\ncanvas so we're going to plot the real\nstock price that's what it actually is\nand we're going to give that color red\nso it's going to be in bright red we're\ngoing to label it real google stock\nprice and then we're going to do our\npredicted stock we're going to do it in\nblue and it's going to be labeled\npredicted and we'll give it a title\nbecause it's always nice to give a title\nto your graph especially if you're going\nto present this to somebody you know to\nyour shareholders in the office and the\nx label is going to be time because it's\na time series and we didn't actually put\nthe actual date and times on here but\nthat's fine we just know they're\nincremented by time and then of course\nthe y label is the actual stock price\nplt.legend tells us to build the legend\non here so that the color red and\nreal google stock price show up on there\nand then the plot shows us that actual\ngraph so let's go ahead and run this and\nsee what that looks like and you can see\nhere we have a nice graph and let's talk\njust a little bit about this graph\nbefore we wrap it up here's our legend i\nwas telling you about that's why we have\nthe legend to show the prices we have\nour title and everything and you'll\nnotice on the bottom we have a time\nsequence we didn't put the actual time\nin here now we could have we could have\ngone ahead and\nplotted the x since we know what the\ndates are and plotted this to dates but\nwe also know that it's only the last\npiece of data that we're looking at so\nlast piece of data which in somewhere\nprobably around here on the graph i\nthink it's like about 20 percent of the\ndata probably less than that we have the\ngoogle price and the google price has\nthis little up jump and then down and\nyou'll see that the actual google\ninstead of a turn down here just didn't\ngo up as high and didn't load go down so\nour prediction has the same pattern but\nthe overall value is pretty far off as\nfar as um stock but then again we're\nonly looking at one column we're only\nlooking at the open price we're not\nlooking at how many volumes were traded\nlike i was pointing out earlier we talk\nabout stock just right off the bat\nthere's six columns there's open high\nlow close volume then there's weather i\nmean volume shares then there's the\nadjusted open adjusted high adjusted low\nadjusted close they have a special\nformula to predict exactly what it would\nreally be worth based on the value of\nthe stock and then from there there's\nall kinds of other stuff you can put in\nhere so we're only looking at one small\naspect the opening price of the stock\nand as you can see here we did a pretty\ngood job this curve follows the curve\npretty well it has like a little jumps\non it bins they don't quite match up so\nthis bin here does not quite match up\nwith that bin there but it's pretty darn\nclose we have the basic shape of it and\nthe prediction isn't too far off and you\ncan imagine that as we add more data in\nand look at different aspects in the\nspecific domain of stock we should be\nable to get a better representation each\ntime we drill in deeper of course this\ntook a half hour for my program my\ncomputer to train so you can imagine\nthat if i was running it across all\nthose different variables it might take\na little bit longer to train the data\nnot so good for doing a quick tutorial\nlike this with that we've reached the\nend of this complete neural networks\ntutorial i hope you enjoyed this video\ndo like and share it thank you for\nwatching and stay tuned for more from\nsimply learn\n[Music]\n",
  "words": [
    "music",
    "neural",
    "networks",
    "typically",
    "used",
    "deep",
    "learning",
    "computing",
    "systems",
    "inspired",
    "structure",
    "function",
    "human",
    "brain",
    "used",
    "find",
    "solutions",
    "problems",
    "algorithmic",
    "method",
    "expensive",
    "exist",
    "neural",
    "networks",
    "learn",
    "example",
    "need",
    "program",
    "depth",
    "hi",
    "guys",
    "welcome",
    "complete",
    "neural",
    "network",
    "tutorial",
    "simply",
    "learn",
    "begin",
    "make",
    "sure",
    "subscribe",
    "channel",
    "hit",
    "bell",
    "icon",
    "never",
    "miss",
    "update",
    "first",
    "understand",
    "basics",
    "neural",
    "networks",
    "see",
    "practical",
    "implementation",
    "learn",
    "back",
    "propagation",
    "gradient",
    "descent",
    "works",
    "finally",
    "understand",
    "concepts",
    "practical",
    "implementation",
    "convolutional",
    "recurrent",
    "neural",
    "networks",
    "let",
    "first",
    "look",
    "small",
    "animated",
    "video",
    "understand",
    "neural",
    "networks",
    "better",
    "last",
    "summer",
    "family",
    "visited",
    "russia",
    "even",
    "though",
    "none",
    "us",
    "could",
    "read",
    "russian",
    "trouble",
    "figuring",
    "way",
    "thanks",
    "google",
    "translation",
    "russian",
    "boards",
    "english",
    "one",
    "several",
    "applications",
    "neural",
    "networks",
    "neural",
    "networks",
    "form",
    "base",
    "deep",
    "learning",
    "subfield",
    "machine",
    "learning",
    "algorithms",
    "inspired",
    "structure",
    "human",
    "brain",
    "neural",
    "networks",
    "take",
    "data",
    "train",
    "recognize",
    "patterns",
    "data",
    "predict",
    "outputs",
    "new",
    "set",
    "similar",
    "data",
    "let",
    "understand",
    "done",
    "let",
    "construct",
    "neural",
    "network",
    "differentiates",
    "square",
    "circle",
    "triangle",
    "neural",
    "networks",
    "made",
    "layers",
    "neurons",
    "neurons",
    "core",
    "processing",
    "units",
    "network",
    "first",
    "input",
    "layer",
    "receives",
    "input",
    "output",
    "layer",
    "predicts",
    "final",
    "output",
    "exists",
    "hidden",
    "layers",
    "perform",
    "computations",
    "required",
    "network",
    "image",
    "circle",
    "image",
    "composed",
    "28",
    "28",
    "pixels",
    "make",
    "784",
    "pixels",
    "pixel",
    "fed",
    "input",
    "neuron",
    "first",
    "layer",
    "neurons",
    "one",
    "layer",
    "connected",
    "neurons",
    "next",
    "layer",
    "channels",
    "channels",
    "assigned",
    "numerical",
    "value",
    "known",
    "weight",
    "inputs",
    "multiplied",
    "corresponding",
    "weights",
    "sum",
    "sent",
    "input",
    "neurons",
    "hidden",
    "layer",
    "neurons",
    "associated",
    "numerical",
    "value",
    "called",
    "bias",
    "added",
    "input",
    "sum",
    "value",
    "passed",
    "threshold",
    "function",
    "called",
    "activation",
    "function",
    "result",
    "activation",
    "function",
    "determines",
    "particular",
    "neuron",
    "get",
    "activated",
    "activated",
    "neuron",
    "transmits",
    "data",
    "neurons",
    "next",
    "layer",
    "channels",
    "manner",
    "data",
    "propagated",
    "network",
    "called",
    "forward",
    "propagation",
    "output",
    "layer",
    "neuron",
    "highest",
    "value",
    "fires",
    "determines",
    "output",
    "values",
    "basically",
    "probability",
    "example",
    "neuron",
    "associated",
    "square",
    "highest",
    "probability",
    "hence",
    "output",
    "predicted",
    "neural",
    "network",
    "course",
    "look",
    "know",
    "neural",
    "network",
    "made",
    "wrong",
    "prediction",
    "network",
    "figure",
    "note",
    "network",
    "yet",
    "trained",
    "training",
    "process",
    "along",
    "input",
    "network",
    "also",
    "output",
    "fed",
    "predicted",
    "output",
    "compared",
    "actual",
    "output",
    "realize",
    "error",
    "prediction",
    "magnitude",
    "error",
    "indicates",
    "wrong",
    "sign",
    "suggests",
    "predicted",
    "values",
    "higher",
    "lower",
    "expected",
    "arrows",
    "give",
    "indication",
    "direction",
    "magnitude",
    "change",
    "reduce",
    "error",
    "information",
    "transferred",
    "backward",
    "network",
    "known",
    "back",
    "propagation",
    "based",
    "information",
    "weights",
    "adjusted",
    "cycle",
    "forward",
    "propagation",
    "back",
    "propagation",
    "iteratively",
    "performed",
    "multiple",
    "inputs",
    "process",
    "continues",
    "weights",
    "assigned",
    "network",
    "predict",
    "shapes",
    "correctly",
    "cases",
    "brings",
    "training",
    "process",
    "end",
    "might",
    "wonder",
    "long",
    "training",
    "process",
    "takes",
    "honestly",
    "neural",
    "networks",
    "may",
    "take",
    "hours",
    "even",
    "months",
    "train",
    "time",
    "reasonable",
    "compared",
    "scope",
    "let",
    "us",
    "look",
    "prime",
    "applications",
    "neural",
    "networks",
    "facial",
    "recognition",
    "cameras",
    "smartphones",
    "days",
    "estimate",
    "age",
    "person",
    "based",
    "facial",
    "features",
    "neural",
    "networks",
    "play",
    "first",
    "differentiating",
    "face",
    "background",
    "correlating",
    "lines",
    "spots",
    "face",
    "possible",
    "age",
    "forecasting",
    "neural",
    "networks",
    "trained",
    "understand",
    "patterns",
    "detect",
    "possibility",
    "rainfall",
    "rise",
    "stock",
    "prices",
    "high",
    "accuracy",
    "music",
    "composition",
    "neural",
    "networks",
    "even",
    "learn",
    "patterns",
    "music",
    "train",
    "enough",
    "compose",
    "fresh",
    "tune",
    "question",
    "following",
    "statements",
    "hold",
    "true",
    "activation",
    "functions",
    "threshold",
    "functions",
    "b",
    "error",
    "calculated",
    "layer",
    "neural",
    "network",
    "c",
    "forward",
    "back",
    "propagation",
    "take",
    "place",
    "training",
    "process",
    "neural",
    "network",
    "data",
    "processing",
    "carried",
    "hidden",
    "layers",
    "leave",
    "answers",
    "comments",
    "section",
    "deep",
    "learning",
    "neural",
    "networks",
    "still",
    "taking",
    "baby",
    "steps",
    "growth",
    "field",
    "foreseen",
    "big",
    "names",
    "companies",
    "google",
    "amazon",
    "nvidia",
    "invested",
    "developing",
    "products",
    "libraries",
    "predictive",
    "models",
    "intuitive",
    "gpus",
    "support",
    "implementation",
    "neural",
    "networks",
    "question",
    "dividing",
    "visionaries",
    "reach",
    "neural",
    "networks",
    "extent",
    "replicate",
    "human",
    "brain",
    "wait",
    "years",
    "give",
    "definite",
    "answer",
    "enjoyed",
    "video",
    "would",
    "take",
    "seconds",
    "like",
    "share",
    "also",
    "yet",
    "subscribe",
    "channel",
    "hit",
    "bell",
    "icon",
    "lot",
    "exciting",
    "videos",
    "coming",
    "fun",
    "learning",
    "till",
    "hope",
    "excited",
    "learn",
    "neural",
    "networks",
    "richard",
    "take",
    "remaining",
    "part",
    "course",
    "make",
    "understand",
    "neural",
    "networks",
    "detail",
    "richard",
    "neural",
    "network",
    "welcome",
    "today",
    "lesson",
    "name",
    "richard",
    "kirschner",
    "simply",
    "learn",
    "team",
    "today",
    "want",
    "discuss",
    "basics",
    "neural",
    "network",
    "today",
    "deep",
    "learning",
    "artificial",
    "neural",
    "network",
    "neural",
    "network",
    "work",
    "advantages",
    "neural",
    "network",
    "applications",
    "neural",
    "network",
    "future",
    "neural",
    "networks",
    "let",
    "start",
    "brief",
    "history",
    "artificial",
    "intelligence",
    "hello",
    "human",
    "brain",
    "one",
    "seeking",
    "enlightenment",
    "sat",
    "meditated",
    "complex",
    "organ",
    "human",
    "body",
    "help",
    "think",
    "understand",
    "make",
    "decisions",
    "secret",
    "behind",
    "power",
    "neuron",
    "get",
    "back",
    "time",
    "ever",
    "since",
    "1950s",
    "scientists",
    "trying",
    "mimic",
    "functioning",
    "neuron",
    "use",
    "build",
    "smarter",
    "robots",
    "lot",
    "trial",
    "error",
    "humans",
    "finally",
    "designed",
    "computer",
    "recognize",
    "human",
    "speech",
    "2000",
    "humans",
    "able",
    "give",
    "birth",
    "deep",
    "learning",
    "able",
    "see",
    "distinguish",
    "different",
    "images",
    "videos",
    "looking",
    "let",
    "dive",
    "deep",
    "learning",
    "first",
    "thought",
    "might",
    "opposite",
    "shallow",
    "learning",
    "deep",
    "learning",
    "like",
    "magic",
    "box",
    "let",
    "go",
    "take",
    "look",
    "kind",
    "magic",
    "box",
    "exactly",
    "deep",
    "learning",
    "images",
    "dogs",
    "deep",
    "learning",
    "machine",
    "learning",
    "technique",
    "teaches",
    "computers",
    "comes",
    "naturally",
    "humans",
    "learn",
    "example",
    "robot",
    "gets",
    "trained",
    "photos",
    "example",
    "different",
    "hardwiring",
    "computer",
    "program",
    "recognizes",
    "something",
    "actually",
    "learns",
    "magic",
    "box",
    "really",
    "control",
    "learns",
    "control",
    "aspects",
    "go",
    "computer",
    "comes",
    "back",
    "says",
    "wait",
    "know",
    "looks",
    "photograph",
    "dog",
    "able",
    "identify",
    "saw",
    "images",
    "says",
    "doc",
    "woof",
    "woof",
    "example",
    "deep",
    "learning",
    "notice",
    "go",
    "go",
    "actually",
    "works",
    "behind",
    "scenes",
    "neural",
    "network",
    "bit",
    "filling",
    "magic",
    "term",
    "deep",
    "learning",
    "comes",
    "also",
    "term",
    "like",
    "call",
    "magic",
    "box",
    "put",
    "things",
    "program",
    "starts",
    "running",
    "deep",
    "learning",
    "understand",
    "settings",
    "follow",
    "exactly",
    "going",
    "deep",
    "learning",
    "model",
    "brings",
    "us",
    "question",
    "deep",
    "learning",
    "remember",
    "neuron",
    "scientists",
    "managed",
    "build",
    "artificial",
    "form",
    "powers",
    "deep",
    "learning",
    "based",
    "machine",
    "let",
    "talk",
    "artificial",
    "neural",
    "networks",
    "artificial",
    "neural",
    "network",
    "understand",
    "artificial",
    "neuron",
    "works",
    "need",
    "understand",
    "real",
    "one",
    "works",
    "first",
    "dendrite",
    "input",
    "neuron",
    "see",
    "little",
    "hairs",
    "come",
    "receive",
    "information",
    "cell",
    "body",
    "information",
    "processing",
    "happens",
    "takes",
    "different",
    "dendrites",
    "information",
    "coming",
    "different",
    "dendrites",
    "looks",
    "information",
    "axon",
    "output",
    "neuron",
    "goes",
    "axon",
    "see",
    "goes",
    "way",
    "end",
    "flanges",
    "one",
    "little",
    "flanges",
    "connects",
    "dendrite",
    "hairs",
    "next",
    "one",
    "let",
    "see",
    "artificial",
    "neural",
    "network",
    "looks",
    "like",
    "artificial",
    "neural",
    "network",
    "input",
    "layer",
    "could",
    "array",
    "data",
    "one",
    "white",
    "dots",
    "yellow",
    "bar",
    "would",
    "represent",
    "say",
    "pixel",
    "picture",
    "lines",
    "connected",
    "hidden",
    "layers",
    "weights",
    "add",
    "hidden",
    "layers",
    "one",
    "dots",
    "kind",
    "like",
    "cell",
    "something",
    "inputs",
    "puts",
    "output",
    "next",
    "hidden",
    "layer",
    "output",
    "layer",
    "information",
    "processing",
    "happens",
    "input",
    "neuron",
    "output",
    "neuron",
    "see",
    "similar",
    "input",
    "yellow",
    "bar",
    "coming",
    "like",
    "liken",
    "hidden",
    "layers",
    "neuron",
    "passes",
    "next",
    "one",
    "output",
    "next",
    "neuron",
    "output",
    "real",
    "world",
    "neural",
    "network",
    "system",
    "hardware",
    "software",
    "pattern",
    "operation",
    "neurons",
    "human",
    "brain",
    "neural",
    "networks",
    "also",
    "called",
    "artificial",
    "neural",
    "networks",
    "way",
    "achieving",
    "deep",
    "learning",
    "artificial",
    "neural",
    "networks",
    "work",
    "let",
    "us",
    "find",
    "artificial",
    "neural",
    "network",
    "work",
    "hey",
    "siri",
    "time",
    "12",
    "30",
    "morning",
    "thanks",
    "let",
    "find",
    "recognizes",
    "speech",
    "neural",
    "network",
    "different",
    "layers",
    "input",
    "layer",
    "hidden",
    "layers",
    "output",
    "layer",
    "sentence",
    "needs",
    "recognized",
    "network",
    "time",
    "comes",
    "one",
    "comes",
    "pattern",
    "sound",
    "time",
    "first",
    "let",
    "consider",
    "word",
    "see",
    "one",
    "sound",
    "bar",
    "probably",
    "looks",
    "little",
    "different",
    "representation",
    "comes",
    "different",
    "pattern",
    "split",
    "sound",
    "wave",
    "letter",
    "w",
    "smaller",
    "segments",
    "split",
    "w",
    "take",
    "w",
    "analyze",
    "w",
    "amplitude",
    "varying",
    "sound",
    "wave",
    "w",
    "collect",
    "values",
    "different",
    "intervals",
    "form",
    "array",
    "might",
    "different",
    "amplitudes",
    "coming",
    "feed",
    "array",
    "amplitudes",
    "input",
    "layer",
    "one",
    "goes",
    "box",
    "input",
    "layer",
    "random",
    "weights",
    "assigned",
    "interconnection",
    "input",
    "hidden",
    "layer",
    "remember",
    "little",
    "lines",
    "said",
    "special",
    "weights",
    "going",
    "start",
    "randomly",
    "always",
    "start",
    "randoms",
    "start",
    "kind",
    "preset",
    "identical",
    "pattern",
    "like",
    "set",
    "three",
    "take",
    "forever",
    "train",
    "less",
    "likely",
    "get",
    "good",
    "result",
    "random",
    "works",
    "really",
    "well",
    "weights",
    "multiplied",
    "inputs",
    "bias",
    "added",
    "form",
    "transfer",
    "function",
    "make",
    "sum",
    "weights",
    "times",
    "value",
    "take",
    "x",
    "coming",
    "going",
    "multiply",
    "w1",
    "w2",
    "w3",
    "get",
    "next",
    "level",
    "going",
    "add",
    "together",
    "coming",
    "coming",
    "add",
    "weight",
    "times",
    "x",
    "always",
    "bias",
    "added",
    "ever",
    "build",
    "neural",
    "network",
    "forget",
    "add",
    "bias",
    "otherwise",
    "tends",
    "work",
    "quite",
    "well",
    "need",
    "extra",
    "layer",
    "help",
    "weights",
    "assigned",
    "interconnection",
    "hidden",
    "layers",
    "output",
    "transfer",
    "function",
    "fed",
    "input",
    "activation",
    "function",
    "output",
    "one",
    "hidden",
    "layer",
    "becomes",
    "input",
    "next",
    "hidden",
    "layer",
    "acoustic",
    "model",
    "contains",
    "statistical",
    "representation",
    "distinct",
    "sound",
    "makes",
    "word",
    "start",
    "building",
    "acoustical",
    "models",
    "layers",
    "separate",
    "start",
    "learning",
    "different",
    "models",
    "different",
    "letters",
    "lexicon",
    "contains",
    "data",
    "different",
    "pronunciations",
    "every",
    "word",
    "lexicon",
    "end",
    "end",
    "b",
    "c",
    "identifies",
    "different",
    "letters",
    "term",
    "acoustic",
    "model",
    "term",
    "lexicon",
    "specific",
    "domain",
    "domain",
    "understanding",
    "speech",
    "certainly",
    "photographs",
    "things",
    "different",
    "labels",
    "process",
    "going",
    "finally",
    "get",
    "output",
    "later",
    "following",
    "process",
    "every",
    "word",
    "letter",
    "neural",
    "network",
    "recognizes",
    "sentence",
    "said",
    "time",
    "identify",
    "identifies",
    "one",
    "word",
    "time",
    "12",
    "way",
    "siri",
    "look",
    "time",
    "read",
    "back",
    "let",
    "look",
    "advantages",
    "artificial",
    "neural",
    "network",
    "gentlemen",
    "could",
    "tell",
    "advantages",
    "artificial",
    "neural",
    "network",
    "amazing",
    "many",
    "times",
    "situation",
    "explain",
    "people",
    "making",
    "decisions",
    "company",
    "amazing",
    "many",
    "times",
    "space",
    "explain",
    "owner",
    "company",
    "artificial",
    "intelligence",
    "neural",
    "network",
    "actually",
    "advantages",
    "artificial",
    "neural",
    "network",
    "works",
    "artificial",
    "neural",
    "network",
    "outputs",
    "limited",
    "entirely",
    "inputs",
    "results",
    "given",
    "initially",
    "expert",
    "system",
    "ability",
    "comes",
    "handy",
    "robotics",
    "pattern",
    "recognition",
    "systems",
    "artificial",
    "neural",
    "networks",
    "potential",
    "high",
    "fault",
    "tolerance",
    "artificial",
    "neural",
    "networks",
    "capable",
    "debugging",
    "diagnosing",
    "network",
    "common",
    "use",
    "days",
    "go",
    "log",
    "files",
    "sort",
    "thousands",
    "log",
    "files",
    "working",
    "admin",
    "systems",
    "capability",
    "finding",
    "shortcuts",
    "reach",
    "computational",
    "expensive",
    "solutions",
    "see",
    "banking",
    "hand",
    "excel",
    "spreadsheet",
    "start",
    "building",
    "codes",
    "around",
    "excel",
    "spreadsheet",
    "20",
    "years",
    "might",
    "build",
    "repertoire",
    "functions",
    "neural",
    "network",
    "comes",
    "answers",
    "done",
    "days",
    "weeks",
    "even",
    "month",
    "huge",
    "bank",
    "let",
    "take",
    "look",
    "little",
    "bit",
    "mentioned",
    "couple",
    "applications",
    "artificial",
    "intelligence",
    "let",
    "dig",
    "deeper",
    "applications",
    "artificial",
    "intelligence",
    "let",
    "look",
    "real",
    "life",
    "stuff",
    "going",
    "right",
    "world",
    "exciting",
    "time",
    "neural",
    "networks",
    "machine",
    "learning",
    "artificial",
    "intelligence",
    "development",
    "let",
    "take",
    "look",
    "current",
    "applications",
    "going",
    "real",
    "life",
    "use",
    "imagination",
    "dig",
    "new",
    "ones",
    "listed",
    "limitless",
    "amount",
    "applications",
    "worked",
    "right",
    "implemented",
    "handwriting",
    "recognition",
    "neural",
    "network",
    "used",
    "convert",
    "handwritten",
    "characters",
    "digital",
    "characters",
    "system",
    "recognize",
    "stock",
    "exchange",
    "prediction",
    "ever",
    "worked",
    "stock",
    "exchange",
    "fickled",
    "track",
    "mean",
    "really",
    "hard",
    "understand",
    "many",
    "factors",
    "affect",
    "stock",
    "market",
    "neural",
    "network",
    "examine",
    "lot",
    "factors",
    "predict",
    "prices",
    "daily",
    "basis",
    "helping",
    "stock",
    "brokers",
    "right",
    "still",
    "intro",
    "phase",
    "helps",
    "really",
    "look",
    "closely",
    "realize",
    "generate",
    "three",
    "terabytes",
    "day",
    "stock",
    "exchange",
    "united",
    "states",
    "lot",
    "data",
    "dig",
    "sort",
    "even",
    "start",
    "focusing",
    "even",
    "one",
    "stock",
    "traveling",
    "salesman",
    "problem",
    "refers",
    "finding",
    "optimal",
    "path",
    "travel",
    "cities",
    "area",
    "neural",
    "network",
    "helps",
    "solve",
    "problem",
    "providing",
    "higher",
    "revenue",
    "minimal",
    "cost",
    "logistics",
    "huge",
    "logistics",
    "talk",
    "salesmen",
    "traveling",
    "town",
    "town",
    "logistics",
    "used",
    "amazon",
    "amazon",
    "loves",
    "ship",
    "packages",
    "empty",
    "space",
    "trucks",
    "packages",
    "fill",
    "empty",
    "space",
    "think",
    "buy",
    "saves",
    "lot",
    "time",
    "people",
    "lot",
    "happier",
    "get",
    "tomorrow",
    "instead",
    "wait",
    "three",
    "weeks",
    "image",
    "compression",
    "idea",
    "behind",
    "data",
    "compression",
    "neural",
    "network",
    "store",
    "encrypt",
    "recreate",
    "actual",
    "image",
    "optimize",
    "compression",
    "data",
    "images",
    "biggest",
    "one",
    "using",
    "kinds",
    "data",
    "wonderful",
    "application",
    "save",
    "hard",
    "drive",
    "optimize",
    "able",
    "read",
    "back",
    "like",
    "said",
    "use",
    "mind",
    "dig",
    "deeper",
    "let",
    "take",
    "even",
    "going",
    "go",
    "step",
    "let",
    "look",
    "future",
    "deep",
    "learning",
    "thank",
    "goodness",
    "wonderful",
    "person",
    "reading",
    "crystal",
    "ball",
    "tell",
    "see",
    "future",
    "personalized",
    "choices",
    "users",
    "customers",
    "world",
    "certainly",
    "like",
    "go",
    "whatever",
    "online",
    "ordering",
    "system",
    "starts",
    "referring",
    "stuff",
    "local",
    "company",
    "live",
    "uses",
    "take",
    "picture",
    "starts",
    "looking",
    "want",
    "based",
    "picture",
    "see",
    "couch",
    "like",
    "starts",
    "looking",
    "furniture",
    "like",
    "clothing",
    "think",
    "mainly",
    "clothing",
    "hyper",
    "intelligent",
    "virtual",
    "assistants",
    "make",
    "life",
    "easier",
    "played",
    "google",
    "assistant",
    "siri",
    "see",
    "slowly",
    "evolving",
    "getting",
    "hump",
    "virtual",
    "assistant",
    "kinds",
    "things",
    "even",
    "email",
    "response",
    "new",
    "forms",
    "algorithm",
    "learning",
    "methods",
    "would",
    "discovered",
    "always",
    "something",
    "rolling",
    "really",
    "cool",
    "research",
    "area",
    "stuff",
    "infant",
    "stage",
    "artificial",
    "intelligence",
    "neural",
    "networks",
    "actually",
    "applying",
    "real",
    "world",
    "wonderful",
    "time",
    "jump",
    "neural",
    "networks",
    "lot",
    "faster",
    "future",
    "neural",
    "network",
    "tools",
    "embedded",
    "every",
    "design",
    "surface",
    "already",
    "see",
    "buy",
    "little",
    "mini",
    "neural",
    "network",
    "plugs",
    "really",
    "cheap",
    "processing",
    "board",
    "laptop",
    "hardware",
    "starting",
    "come",
    "goes",
    "right",
    "dump",
    "makes",
    "also",
    "faster",
    "hardware",
    "instead",
    "software",
    "side",
    "neural",
    "networks",
    "used",
    "field",
    "medicine",
    "agriculture",
    "physics",
    "discoveries",
    "everything",
    "imagine",
    "see",
    "today",
    "going",
    "phd",
    "student",
    "medicine",
    "trying",
    "understand",
    "cells",
    "understand",
    "statistic",
    "analysis",
    "cure",
    "people",
    "help",
    "keep",
    "healthy",
    "help",
    "find",
    "heal",
    "something",
    "anybody",
    "go",
    "access",
    "process",
    "data",
    "working",
    "shared",
    "data",
    "systems",
    "concept",
    "used",
    "different",
    "fields",
    "different",
    "domains",
    "huge",
    "world",
    "wide",
    "open",
    "anybody",
    "jumping",
    "start",
    "exploring",
    "start",
    "learning",
    "neural",
    "networks",
    "let",
    "dive",
    "say",
    "neural",
    "network",
    "work",
    "come",
    "far",
    "enough",
    "understand",
    "neural",
    "network",
    "works",
    "let",
    "go",
    "ahead",
    "walk",
    "nice",
    "graphical",
    "representation",
    "usually",
    "describe",
    "neural",
    "network",
    "different",
    "layers",
    "see",
    "identified",
    "green",
    "layer",
    "orange",
    "layer",
    "red",
    "layer",
    "green",
    "layer",
    "input",
    "data",
    "coming",
    "picks",
    "input",
    "signals",
    "passes",
    "next",
    "layer",
    "next",
    "layer",
    "kinds",
    "calculations",
    "feature",
    "extraction",
    "called",
    "hidden",
    "layer",
    "lot",
    "times",
    "one",
    "hidden",
    "layer",
    "showing",
    "one",
    "picture",
    "show",
    "looks",
    "like",
    "detail",
    "little",
    "bit",
    "finally",
    "output",
    "layer",
    "layer",
    "delivers",
    "final",
    "result",
    "two",
    "things",
    "see",
    "input",
    "layer",
    "output",
    "layer",
    "let",
    "make",
    "use",
    "neural",
    "network",
    "see",
    "works",
    "wonder",
    "traffic",
    "cameras",
    "identify",
    "vehicles",
    "registration",
    "plate",
    "road",
    "detect",
    "speeding",
    "vehicles",
    "breaking",
    "law",
    "got",
    "going",
    "red",
    "light",
    "day",
    "well",
    "last",
    "month",
    "like",
    "horrible",
    "thing",
    "sent",
    "picture",
    "information",
    "pulled",
    "license",
    "plate",
    "picture",
    "gone",
    "red",
    "light",
    "image",
    "car",
    "see",
    "license",
    "plates",
    "let",
    "consider",
    "image",
    "vehicle",
    "find",
    "number",
    "plate",
    "picture",
    "28",
    "28",
    "pixels",
    "image",
    "fed",
    "input",
    "identify",
    "registration",
    "plate",
    "neuron",
    "number",
    "called",
    "activation",
    "represents",
    "grayscale",
    "value",
    "corresponding",
    "pixel",
    "range",
    "range",
    "zero",
    "one",
    "one",
    "white",
    "pixel",
    "zero",
    "black",
    "pixel",
    "see",
    "example",
    "one",
    "pixels",
    "registered",
    "like",
    "meaning",
    "probably",
    "pretty",
    "dark",
    "neuron",
    "lit",
    "activation",
    "close",
    "one",
    "get",
    "closer",
    "black",
    "white",
    "really",
    "start",
    "seeing",
    "details",
    "see",
    "pixel",
    "shows",
    "one",
    "like",
    "part",
    "car",
    "lights",
    "pixels",
    "form",
    "arrays",
    "fed",
    "input",
    "layer",
    "see",
    "pixel",
    "car",
    "image",
    "fed",
    "input",
    "going",
    "see",
    "input",
    "layer",
    "green",
    "one",
    "dimension",
    "image",
    "two",
    "dimension",
    "look",
    "setup",
    "programming",
    "python",
    "cool",
    "feature",
    "automatically",
    "work",
    "us",
    "working",
    "older",
    "neural",
    "network",
    "pattern",
    "package",
    "convert",
    "one",
    "rows",
    "one",
    "array",
    "like",
    "row",
    "one",
    "tack",
    "row",
    "two",
    "end",
    "almost",
    "feed",
    "image",
    "directly",
    "neural",
    "networks",
    "key",
    "though",
    "using",
    "28",
    "28",
    "get",
    "picture",
    "30",
    "30",
    "shrink",
    "30",
    "30",
    "fit",
    "28",
    "28",
    "ca",
    "increase",
    "number",
    "input",
    "case",
    "green",
    "dots",
    "important",
    "remember",
    "work",
    "neural",
    "networks",
    "let",
    "name",
    "inputs",
    "x1",
    "x2",
    "x3",
    "respectively",
    "one",
    "represents",
    "one",
    "pixels",
    "coming",
    "input",
    "layer",
    "passes",
    "hidden",
    "layer",
    "see",
    "two",
    "hidden",
    "layers",
    "image",
    "orange",
    "one",
    "pixels",
    "connects",
    "one",
    "hidden",
    "layers",
    "interconnections",
    "assigned",
    "weights",
    "random",
    "get",
    "random",
    "weights",
    "come",
    "x1",
    "lights",
    "going",
    "x1",
    "times",
    "weight",
    "going",
    "hidden",
    "layer",
    "sum",
    "weights",
    "weights",
    "multiplied",
    "input",
    "signal",
    "bias",
    "added",
    "see",
    "x1",
    "comes",
    "actually",
    "goes",
    "different",
    "hidden",
    "layer",
    "nodes",
    "case",
    "whatever",
    "want",
    "call",
    "network",
    "setup",
    "orange",
    "dots",
    "take",
    "value",
    "x1",
    "multiply",
    "weight",
    "next",
    "hidden",
    "layer",
    "x1",
    "goes",
    "hidden",
    "layer",
    "one",
    "x1",
    "goes",
    "hidden",
    "layer",
    "two",
    "x1",
    "goes",
    "hidden",
    "layer",
    "one",
    "node",
    "two",
    "hidden",
    "layer",
    "one",
    "node",
    "three",
    "bias",
    "lot",
    "times",
    "put",
    "bias",
    "like",
    "another",
    "green",
    "dot",
    "another",
    "orange",
    "dot",
    "give",
    "bias",
    "value",
    "one",
    "weights",
    "go",
    "bias",
    "next",
    "node",
    "bias",
    "change",
    "always",
    "remember",
    "need",
    "bias",
    "things",
    "done",
    "generally",
    "packages",
    "control",
    "worry",
    "figuring",
    "bias",
    "ever",
    "dive",
    "deep",
    "neural",
    "networks",
    "got",
    "remember",
    "bias",
    "answer",
    "wo",
    "come",
    "correctly",
    "weight",
    "weighted",
    "sum",
    "input",
    "fed",
    "input",
    "activation",
    "function",
    "decide",
    "nodes",
    "fire",
    "feature",
    "extraction",
    "signal",
    "flows",
    "within",
    "hidden",
    "layers",
    "weighted",
    "sum",
    "inputs",
    "calculated",
    "fed",
    "activation",
    "function",
    "layer",
    "decide",
    "nodes",
    "fire",
    "feature",
    "extraction",
    "number",
    "plate",
    "see",
    "still",
    "hidden",
    "nodes",
    "middle",
    "becomes",
    "important",
    "going",
    "take",
    "little",
    "detour",
    "look",
    "activation",
    "function",
    "going",
    "dive",
    "little",
    "bit",
    "math",
    "start",
    "understand",
    "games",
    "go",
    "playing",
    "neural",
    "networks",
    "programming",
    "let",
    "look",
    "different",
    "activation",
    "functions",
    "move",
    "ahead",
    "friendly",
    "red",
    "tag",
    "shopping",
    "robot",
    "one",
    "sigmoid",
    "function",
    "sigmoid",
    "function",
    "one",
    "one",
    "plus",
    "e",
    "minus",
    "x",
    "takes",
    "x",
    "value",
    "see",
    "generates",
    "almost",
    "zero",
    "almost",
    "one",
    "small",
    "area",
    "middle",
    "crosses",
    "use",
    "value",
    "feed",
    "another",
    "function",
    "really",
    "uncertain",
    "might",
    "part",
    "going",
    "really",
    "close",
    "1",
    "really",
    "close",
    "case",
    "0",
    "0",
    "threshold",
    "function",
    "want",
    "worry",
    "uncertainty",
    "middle",
    "say",
    "oh",
    "x",
    "greater",
    "equal",
    "zero",
    "x",
    "zero",
    "either",
    "zero",
    "one",
    "really",
    "straightforward",
    "middle",
    "call",
    "relu",
    "relu",
    "function",
    "see",
    "puts",
    "value",
    "says",
    "well",
    "one",
    "going",
    "one",
    "less",
    "zero",
    "zero",
    "kind",
    "dead",
    "ends",
    "two",
    "ends",
    "allows",
    "values",
    "middle",
    "like",
    "sigmoid",
    "function",
    "allows",
    "information",
    "go",
    "next",
    "level",
    "might",
    "important",
    "know",
    "minus",
    "next",
    "hidden",
    "layer",
    "might",
    "pick",
    "say",
    "oh",
    "piece",
    "information",
    "uncertain",
    "value",
    "low",
    "certainty",
    "hyperbolic",
    "tangent",
    "function",
    "see",
    "1",
    "minus",
    "e",
    "minus",
    "2x",
    "1",
    "plus",
    "e",
    "minus",
    "2x",
    "much",
    "along",
    "theme",
    "little",
    "bit",
    "different",
    "goes",
    "one",
    "one",
    "see",
    "go",
    "zero",
    "one",
    "one",
    "goes",
    "minus",
    "one",
    "one",
    "less",
    "zero",
    "know",
    "fire",
    "zero",
    "fires",
    "also",
    "still",
    "puts",
    "value",
    "still",
    "value",
    "get",
    "like",
    "sigmoid",
    "function",
    "relu",
    "function",
    "similar",
    "use",
    "believe",
    "original",
    "used",
    "everything",
    "done",
    "sigmoid",
    "function",
    "commonly",
    "used",
    "kind",
    "use",
    "relu",
    "function",
    "reason",
    "one",
    "processes",
    "faster",
    "already",
    "value",
    "add",
    "another",
    "compute",
    "one",
    "one",
    "plus",
    "e",
    "minus",
    "x",
    "hidden",
    "node",
    "data",
    "coming",
    "works",
    "pretty",
    "good",
    "far",
    "putting",
    "next",
    "level",
    "want",
    "know",
    "close",
    "zero",
    "close",
    "functioning",
    "know",
    "minus",
    "point",
    "one",
    "minus",
    "point",
    "two",
    "usually",
    "float",
    "values",
    "get",
    "like",
    "minus",
    "point",
    "minus",
    "something",
    "yeah",
    "important",
    "information",
    "relu",
    "commonly",
    "used",
    "days",
    "far",
    "setup",
    "using",
    "also",
    "see",
    "sigmoid",
    "function",
    "commonly",
    "used",
    "also",
    "know",
    "activation",
    "function",
    "let",
    "get",
    "back",
    "neural",
    "network",
    "finally",
    "model",
    "would",
    "predict",
    "outcome",
    "applying",
    "suitable",
    "activation",
    "function",
    "output",
    "layer",
    "go",
    "look",
    "optical",
    "character",
    "recognition",
    "ocr",
    "used",
    "images",
    "convert",
    "text",
    "order",
    "identify",
    "written",
    "plate",
    "comes",
    "see",
    "red",
    "node",
    "red",
    "node",
    "might",
    "actually",
    "represent",
    "letter",
    "usually",
    "lot",
    "outputs",
    "text",
    "identification",
    "going",
    "show",
    "might",
    "even",
    "order",
    "might",
    "order",
    "license",
    "plates",
    "might",
    "b",
    "c",
    "e",
    "f",
    "g",
    "know",
    "alphabet",
    "plus",
    "numbers",
    "might",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "nine",
    "ten",
    "places",
    "large",
    "array",
    "comes",
    "small",
    "amount",
    "know",
    "show",
    "three",
    "dots",
    "coming",
    "eight",
    "hidden",
    "layer",
    "nodes",
    "know",
    "two",
    "sets",
    "four",
    "saw",
    "one",
    "red",
    "coming",
    "lot",
    "times",
    "uh",
    "know",
    "28",
    "times",
    "28",
    "30",
    "times",
    "30",
    "know",
    "900",
    "nodes",
    "28",
    "little",
    "bit",
    "less",
    "input",
    "imagine",
    "hidden",
    "layer",
    "big",
    "hidden",
    "layer",
    "big",
    "bigger",
    "output",
    "going",
    "many",
    "digits",
    "yeah",
    "lot",
    "huge",
    "amount",
    "input",
    "output",
    "showing",
    "know",
    "would",
    "hard",
    "show",
    "one",
    "picture",
    "comes",
    "finally",
    "gets",
    "output",
    "identifies",
    "number",
    "plate",
    "case",
    "0",
    "8",
    "0",
    "3",
    "8",
    "5",
    "error",
    "output",
    "back",
    "propagated",
    "network",
    "weights",
    "adjusted",
    "minimize",
    "error",
    "rate",
    "calculated",
    "cost",
    "function",
    "training",
    "data",
    "used",
    "look",
    "code",
    "data",
    "training",
    "stuff",
    "know",
    "answer",
    "put",
    "information",
    "says",
    "yes",
    "correct",
    "remember",
    "randomly",
    "set",
    "weights",
    "begin",
    "wrong",
    "take",
    "error",
    "far",
    "know",
    "like",
    "minus",
    "one",
    "little",
    "bit",
    "like",
    "minus",
    "300",
    "output",
    "remember",
    "looking",
    "different",
    "options",
    "know",
    "hyperbolic",
    "whatever",
    "looking",
    "rel",
    "rel",
    "limit",
    "top",
    "bottom",
    "actually",
    "generates",
    "number",
    "way",
    "adjust",
    "weights",
    "lot",
    "pretty",
    "close",
    "might",
    "adjust",
    "relates",
    "little",
    "bit",
    "keep",
    "adjusting",
    "weights",
    "fit",
    "different",
    "training",
    "models",
    "put",
    "might",
    "500",
    "training",
    "models",
    "weights",
    "adjust",
    "using",
    "back",
    "propagation",
    "sends",
    "error",
    "backward",
    "output",
    "compared",
    "original",
    "result",
    "multiple",
    "iterations",
    "done",
    "get",
    "maximum",
    "accuracy",
    "look",
    "one",
    "goes",
    "keeps",
    "cycling",
    "data",
    "making",
    "small",
    "changes",
    "network",
    "gets",
    "right",
    "answers",
    "every",
    "iteration",
    "weights",
    "every",
    "interconnection",
    "adjusted",
    "based",
    "error",
    "going",
    "dive",
    "math",
    "differential",
    "equation",
    "gets",
    "little",
    "complicated",
    "talk",
    "little",
    "bit",
    "different",
    "options",
    "look",
    "code",
    "explored",
    "neural",
    "network",
    "let",
    "look",
    "different",
    "types",
    "artificial",
    "neural",
    "networks",
    "like",
    "biggest",
    "area",
    "growing",
    "come",
    "together",
    "let",
    "see",
    "different",
    "types",
    "neural",
    "network",
    "comparing",
    "human",
    "learning",
    "human",
    "brain",
    "feel",
    "sorry",
    "poor",
    "guy",
    "feed",
    "forward",
    "neural",
    "network",
    "simplest",
    "form",
    "call",
    "n",
    "neural",
    "network",
    "data",
    "travels",
    "one",
    "direction",
    "input",
    "output",
    "looked",
    "data",
    "comes",
    "weights",
    "added",
    "goes",
    "hidden",
    "layer",
    "weights",
    "added",
    "goes",
    "next",
    "hidden",
    "layer",
    "weights",
    "added",
    "goes",
    "output",
    "time",
    "use",
    "reverse",
    "propagation",
    "train",
    "actually",
    "use",
    "fast",
    "training",
    "takes",
    "iterate",
    "training",
    "data",
    "start",
    "getting",
    "big",
    "data",
    "train",
    "huge",
    "amount",
    "data",
    "data",
    "put",
    "better",
    "train",
    "get",
    "applications",
    "vision",
    "speech",
    "recognition",
    "actually",
    "pretty",
    "much",
    "everything",
    "talked",
    "lot",
    "almost",
    "used",
    "form",
    "neural",
    "network",
    "level",
    "radial",
    "basis",
    "function",
    "neural",
    "network",
    "model",
    "classifies",
    "data",
    "point",
    "based",
    "distance",
    "center",
    "point",
    "means",
    "might",
    "training",
    "data",
    "want",
    "group",
    "things",
    "together",
    "create",
    "central",
    "points",
    "looks",
    "things",
    "know",
    "things",
    "like",
    "ever",
    "watched",
    "sesame",
    "street",
    "kid",
    "dates",
    "brings",
    "things",
    "together",
    "great",
    "way",
    "right",
    "training",
    "model",
    "start",
    "finding",
    "things",
    "connected",
    "might",
    "noticed",
    "applications",
    "power",
    "restoration",
    "systems",
    "try",
    "figure",
    "connected",
    "based",
    "fix",
    "problem",
    "huge",
    "power",
    "system",
    "cajon",
    "neural",
    "network",
    "vectors",
    "random",
    "dimensions",
    "input",
    "discrete",
    "map",
    "comprised",
    "neurons",
    "basically",
    "find",
    "way",
    "draw",
    "call",
    "say",
    "dimensions",
    "vectors",
    "planes",
    "actually",
    "chop",
    "data",
    "one",
    "dimension",
    "two",
    "dimension",
    "three",
    "dimension",
    "four",
    "five",
    "six",
    "keep",
    "adding",
    "dimensions",
    "finding",
    "ways",
    "separate",
    "data",
    "connect",
    "different",
    "data",
    "pieces",
    "together",
    "applications",
    "used",
    "recognize",
    "patterns",
    "data",
    "like",
    "medical",
    "analysis",
    "hidden",
    "layer",
    "saves",
    "output",
    "used",
    "future",
    "prediction",
    "recurrent",
    "neural",
    "networks",
    "hidden",
    "layers",
    "remember",
    "output",
    "last",
    "time",
    "becomes",
    "part",
    "new",
    "input",
    "might",
    "use",
    "especially",
    "robotics",
    "flying",
    "drone",
    "want",
    "know",
    "last",
    "change",
    "fast",
    "going",
    "help",
    "predict",
    "next",
    "change",
    "need",
    "make",
    "get",
    "drone",
    "wants",
    "go",
    "applications",
    "conversation",
    "model",
    "know",
    "talked",
    "drones",
    "know",
    "identifying",
    "lexis",
    "google",
    "assistant",
    "starting",
    "add",
    "like",
    "play",
    "song",
    "pandora",
    "like",
    "volume",
    "90",
    "add",
    "different",
    "things",
    "connects",
    "together",
    "input",
    "features",
    "taken",
    "batches",
    "like",
    "filter",
    "allows",
    "network",
    "remember",
    "image",
    "parts",
    "convolution",
    "neural",
    "network",
    "today",
    "world",
    "photo",
    "identification",
    "taking",
    "apart",
    "photos",
    "trying",
    "know",
    "ever",
    "seen",
    "google",
    "five",
    "people",
    "together",
    "kind",
    "thing",
    "separates",
    "people",
    "face",
    "recognition",
    "person",
    "applications",
    "used",
    "signal",
    "image",
    "processing",
    "case",
    "use",
    "facial",
    "images",
    "google",
    "picture",
    "images",
    "one",
    "options",
    "modular",
    "neural",
    "network",
    "collection",
    "different",
    "neural",
    "networks",
    "working",
    "together",
    "get",
    "output",
    "wow",
    "went",
    "different",
    "types",
    "neural",
    "networks",
    "final",
    "one",
    "put",
    "multiple",
    "neural",
    "networks",
    "together",
    "mentioned",
    "little",
    "bit",
    "separated",
    "people",
    "larger",
    "photo",
    "individuals",
    "photo",
    "facial",
    "recognition",
    "person",
    "one",
    "network",
    "used",
    "separate",
    "next",
    "network",
    "used",
    "figure",
    "facial",
    "recognition",
    "applications",
    "still",
    "undergoing",
    "research",
    "cutting",
    "edge",
    "hear",
    "term",
    "pipeline",
    "actual",
    "python",
    "code",
    "almost",
    "different",
    "neural",
    "network",
    "setups",
    "pipeline",
    "feature",
    "usually",
    "means",
    "take",
    "data",
    "one",
    "neural",
    "network",
    "maybe",
    "another",
    "neural",
    "network",
    "put",
    "next",
    "neural",
    "network",
    "take",
    "three",
    "four",
    "neural",
    "networks",
    "feed",
    "another",
    "one",
    "connect",
    "neural",
    "networks",
    "really",
    "cutting",
    "edge",
    "experimental",
    "mean",
    "almost",
    "creative",
    "nature",
    "really",
    "science",
    "specific",
    "domain",
    "different",
    "things",
    "looking",
    "banking",
    "domain",
    "going",
    "different",
    "medical",
    "domain",
    "automatic",
    "car",
    "domain",
    "suddenly",
    "figuring",
    "fit",
    "together",
    "lot",
    "fun",
    "really",
    "cool",
    "types",
    "artificial",
    "neural",
    "network",
    "feed",
    "forward",
    "neural",
    "network",
    "radial",
    "basis",
    "function",
    "neural",
    "network",
    "cohenon",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "convolution",
    "neural",
    "network",
    "modular",
    "neural",
    "network",
    "brings",
    "together",
    "colors",
    "brain",
    "match",
    "brain",
    "actually",
    "bring",
    "developed",
    "understanding",
    "humans",
    "learn",
    "understand",
    "humans",
    "learn",
    "build",
    "something",
    "computer",
    "industry",
    "mimic",
    "reflect",
    "developed",
    "exciting",
    "part",
    "use",
    "case",
    "problem",
    "statement",
    "jump",
    "favorite",
    "part",
    "let",
    "use",
    "system",
    "identify",
    "cat",
    "dog",
    "remember",
    "correctly",
    "said",
    "going",
    "python",
    "code",
    "see",
    "hair",
    "kind",
    "sticking",
    "computer",
    "cup",
    "coffee",
    "one",
    "side",
    "little",
    "bit",
    "old",
    "school",
    "pencil",
    "pen",
    "side",
    "yeah",
    "people",
    "take",
    "notes",
    "love",
    "stickies",
    "computer",
    "great",
    "computer",
    "sticky",
    "notes",
    "computer",
    "different",
    "colors",
    "far",
    "today",
    "programmer",
    "problem",
    "want",
    "classify",
    "photos",
    "cats",
    "dogs",
    "using",
    "neural",
    "network",
    "see",
    "quite",
    "variety",
    "dogs",
    "pictures",
    "cats",
    "know",
    "sorting",
    "cat",
    "pretty",
    "amazing",
    "would",
    "anybody",
    "want",
    "even",
    "know",
    "difference",
    "cat",
    "dog",
    "okay",
    "know",
    "well",
    "cat",
    "door",
    "kind",
    "fun",
    "instead",
    "identifying",
    "instead",
    "like",
    "little",
    "collar",
    "magnet",
    "cat",
    "door",
    "would",
    "able",
    "see",
    "oh",
    "cat",
    "cat",
    "coming",
    "oh",
    "dog",
    "dog",
    "dog",
    "want",
    "let",
    "maybe",
    "want",
    "let",
    "animal",
    "raccoon",
    "see",
    "could",
    "take",
    "one",
    "step",
    "actually",
    "apply",
    "could",
    "actually",
    "start",
    "little",
    "startup",
    "company",
    "idea",
    "door",
    "use",
    "case",
    "implemented",
    "python",
    "actually",
    "python",
    "always",
    "nice",
    "tell",
    "people",
    "version",
    "python",
    "affect",
    "sometimes",
    "modules",
    "load",
    "everything",
    "going",
    "start",
    "importing",
    "required",
    "packages",
    "told",
    "going",
    "cross",
    "going",
    "import",
    "karas",
    "models",
    "sequential",
    "cross",
    "layers",
    "conversion",
    "2d",
    "conv",
    "2d",
    "max",
    "pooling",
    "2d",
    "flatten",
    "dense",
    "talk",
    "one",
    "second",
    "let",
    "talk",
    "little",
    "bit",
    "environment",
    "going",
    "work",
    "know",
    "fact",
    "let",
    "go",
    "ahead",
    "open",
    "website",
    "cross",
    "website",
    "learn",
    "little",
    "bit",
    "cross",
    "cross",
    "website",
    "dot",
    "io",
    "official",
    "website",
    "karass",
    "first",
    "thing",
    "notice",
    "cross",
    "runs",
    "top",
    "either",
    "tensorflow",
    "cntk",
    "think",
    "pronounced",
    "thanos",
    "thiano",
    "important",
    "tensorflow",
    "true",
    "tensorflow",
    "probably",
    "one",
    "widely",
    "used",
    "currently",
    "packages",
    "cross",
    "course",
    "know",
    "tomorrow",
    "going",
    "change",
    "going",
    "disappear",
    "something",
    "new",
    "make",
    "sure",
    "learning",
    "code",
    "understand",
    "going",
    "also",
    "know",
    "code",
    "mean",
    "look",
    "look",
    "code",
    "complicated",
    "understand",
    "going",
    "code",
    "pretty",
    "straightforward",
    "reason",
    "like",
    "karas",
    "reason",
    "people",
    "jumping",
    "right",
    "big",
    "deal",
    "come",
    "let",
    "scroll",
    "little",
    "bit",
    "talk",
    "user",
    "friendliness",
    "modularity",
    "easy",
    "extensibility",
    "work",
    "python",
    "python",
    "big",
    "one",
    "lot",
    "people",
    "data",
    "science",
    "use",
    "python",
    "although",
    "actually",
    "access",
    "cross",
    "ways",
    "continue",
    "layers",
    "gets",
    "really",
    "cool",
    "working",
    "cross",
    "add",
    "layers",
    "remember",
    "hidden",
    "layers",
    "talking",
    "talked",
    "relu",
    "activation",
    "see",
    "right",
    "let",
    "little",
    "bit",
    "size",
    "go",
    "big",
    "add",
    "relu",
    "layer",
    "add",
    "soft",
    "max",
    "layer",
    "next",
    "instance",
    "talk",
    "soft",
    "max",
    "layer",
    "separate",
    "working",
    "kits",
    "use",
    "take",
    "one",
    "setup",
    "feed",
    "output",
    "next",
    "one",
    "one",
    "add",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "different",
    "information",
    "makes",
    "powerful",
    "fast",
    "spin",
    "try",
    "different",
    "setups",
    "see",
    "work",
    "data",
    "working",
    "dig",
    "little",
    "bit",
    "deeper",
    "lot",
    "much",
    "get",
    "part",
    "point",
    "also",
    "quick",
    "side",
    "note",
    "using",
    "anaconda",
    "python",
    "went",
    "ahead",
    "created",
    "package",
    "called",
    "cross",
    "python36",
    "python36",
    "anaconda",
    "cool",
    "way",
    "create",
    "different",
    "environments",
    "real",
    "easily",
    "lot",
    "different",
    "experimenting",
    "different",
    "packages",
    "probably",
    "want",
    "create",
    "environment",
    "first",
    "thing",
    "see",
    "right",
    "lot",
    "dependencies",
    "lot",
    "recognize",
    "done",
    "videos",
    "kudos",
    "jumping",
    "today",
    "pip",
    "install",
    "numpy",
    "scipy",
    "scikit",
    "learn",
    "pillow",
    "h5py",
    "needed",
    "tensorflow",
    "putting",
    "cross",
    "see",
    "pip",
    "standard",
    "installer",
    "use",
    "python",
    "see",
    "pip",
    "install",
    "tensorflow",
    "since",
    "going",
    "cross",
    "top",
    "tensorflow",
    "pip",
    "install",
    "went",
    "ahead",
    "used",
    "github",
    "git",
    "plus",
    "get",
    "see",
    "one",
    "releases",
    "one",
    "current",
    "release",
    "goes",
    "top",
    "tensorflow",
    "look",
    "instructions",
    "pretty",
    "much",
    "anywhere",
    "anaconda",
    "certainly",
    "want",
    "install",
    "ubuntu",
    "server",
    "setup",
    "want",
    "get",
    "think",
    "need",
    "h5py",
    "ubuntu",
    "need",
    "rest",
    "dependencies",
    "pretty",
    "straightforward",
    "actually",
    "instructions",
    "website",
    "initially",
    "go",
    "remember",
    "website",
    "anaconda",
    "navigator",
    "like",
    "see",
    "environments",
    "bottom",
    "created",
    "new",
    "environment",
    "called",
    "cross",
    "python",
    "36",
    "separate",
    "everything",
    "say",
    "python",
    "python",
    "used",
    "bunch",
    "ones",
    "kind",
    "cleaned",
    "house",
    "recently",
    "course",
    "go",
    "launch",
    "jupiter",
    "notebook",
    "making",
    "sure",
    "using",
    "right",
    "environment",
    "set",
    "course",
    "opens",
    "case",
    "using",
    "google",
    "chrome",
    "go",
    "create",
    "new",
    "document",
    "browser",
    "window",
    "use",
    "anaconda",
    "use",
    "anaconda",
    "jupiter",
    "notebook",
    "use",
    "kind",
    "python",
    "editor",
    "whatever",
    "setup",
    "comfortable",
    "whatever",
    "let",
    "go",
    "ahead",
    "go",
    "paste",
    "code",
    "importing",
    "number",
    "different",
    "settings",
    "import",
    "sequential",
    "models",
    "model",
    "going",
    "use",
    "far",
    "neural",
    "network",
    "layers",
    "conversion",
    "2d",
    "max",
    "pooling",
    "2d",
    "flatten",
    "dense",
    "actually",
    "kind",
    "guess",
    "talking",
    "working",
    "2d",
    "photograph",
    "remember",
    "correctly",
    "talked",
    "actual",
    "input",
    "layer",
    "single",
    "array",
    "two",
    "dimensions",
    "one",
    "dimension",
    "tools",
    "help",
    "flatten",
    "image",
    "takes",
    "image",
    "creates",
    "proper",
    "setup",
    "worry",
    "anything",
    "special",
    "photograph",
    "let",
    "cross",
    "going",
    "run",
    "see",
    "right",
    "stuff",
    "going",
    "depreciated",
    "changed",
    "everything",
    "changed",
    "go",
    "worry",
    "much",
    "warnings",
    "run",
    "second",
    "time",
    "warning",
    "disappear",
    "imported",
    "packages",
    "us",
    "use",
    "jupiter",
    "nice",
    "thing",
    "step",
    "step",
    "go",
    "ahead",
    "also",
    "zoom",
    "little",
    "control",
    "plus",
    "one",
    "nice",
    "things",
    "browser",
    "environment",
    "back",
    "another",
    "sip",
    "coffee",
    "familiar",
    "videos",
    "notice",
    "always",
    "sipping",
    "coffee",
    "always",
    "case",
    "latte",
    "next",
    "espresso",
    "next",
    "step",
    "go",
    "ahead",
    "initialize",
    "going",
    "call",
    "cnn",
    "classifier",
    "neural",
    "network",
    "reason",
    "call",
    "classifier",
    "going",
    "classify",
    "two",
    "things",
    "going",
    "cat",
    "dog",
    "classification",
    "picking",
    "specific",
    "objects",
    "specific",
    "true",
    "false",
    "yes",
    "something",
    "first",
    "thing",
    "going",
    "create",
    "classifier",
    "going",
    "equal",
    "sequential",
    "sequential",
    "setup",
    "classifier",
    "actual",
    "model",
    "using",
    "neural",
    "network",
    "call",
    "classifier",
    "next",
    "step",
    "add",
    "convolution",
    "let",
    "shrink",
    "size",
    "see",
    "whole",
    "line",
    "let",
    "talk",
    "little",
    "bit",
    "going",
    "classifier",
    "add",
    "something",
    "adding",
    "well",
    "adding",
    "first",
    "layer",
    "first",
    "layer",
    "adding",
    "probably",
    "one",
    "takes",
    "work",
    "make",
    "sure",
    "set",
    "correct",
    "reason",
    "say",
    "actual",
    "input",
    "going",
    "jump",
    "part",
    "says",
    "input",
    "shape",
    "equals",
    "64",
    "64",
    "mean",
    "well",
    "means",
    "pictures",
    "coming",
    "pictures",
    "remember",
    "like",
    "picture",
    "car",
    "128",
    "128",
    "pixels",
    "well",
    "one",
    "64",
    "64",
    "pixels",
    "pixel",
    "three",
    "values",
    "numbers",
    "come",
    "important",
    "matches",
    "mentioned",
    "little",
    "bit",
    "like",
    "larger",
    "picture",
    "reformat",
    "fit",
    "shape",
    "comes",
    "something",
    "larger",
    "input",
    "notes",
    "input",
    "neural",
    "network",
    "handle",
    "extra",
    "space",
    "reshape",
    "data",
    "fit",
    "first",
    "layer",
    "important",
    "keras",
    "knows",
    "shape",
    "coming",
    "knows",
    "coming",
    "really",
    "sets",
    "stage",
    "important",
    "thing",
    "input",
    "shape",
    "matches",
    "data",
    "coming",
    "get",
    "lot",
    "errors",
    "go",
    "picture",
    "number",
    "55",
    "match",
    "correctly",
    "guess",
    "usually",
    "gives",
    "error",
    "activation",
    "remember",
    "talked",
    "different",
    "activations",
    "using",
    "relu",
    "model",
    "like",
    "said",
    "commonly",
    "used",
    "one",
    "fast",
    "added",
    "calculations",
    "says",
    "value",
    "coming",
    "based",
    "weights",
    "value",
    "going",
    "um",
    "know",
    "uh",
    "one",
    "good",
    "zero",
    "good",
    "zero",
    "considered",
    "active",
    "conversion",
    "2d",
    "heck",
    "conversion",
    "2d",
    "going",
    "go",
    "much",
    "detail",
    "couple",
    "things",
    "little",
    "bit",
    "depth",
    "ready",
    "cover",
    "tutorial",
    "used",
    "convert",
    "photo",
    "64",
    "64",
    "three",
    "converting",
    "two",
    "dimensional",
    "kind",
    "setup",
    "aware",
    "photograph",
    "different",
    "pieces",
    "next",
    "going",
    "add",
    "second",
    "convolutional",
    "layer",
    "conv",
    "stands",
    "2d",
    "hidden",
    "layers",
    "input",
    "layer",
    "two",
    "hidden",
    "layers",
    "two",
    "dimensional",
    "two",
    "dimensional",
    "photograph",
    "see",
    "last",
    "one",
    "add",
    "max",
    "pooling",
    "2d",
    "put",
    "pool",
    "size",
    "equals",
    "2",
    "get",
    "end",
    "layers",
    "one",
    "things",
    "always",
    "want",
    "think",
    "call",
    "mapping",
    "reducing",
    "wonderful",
    "terminology",
    "big",
    "data",
    "mapping",
    "data",
    "layers",
    "want",
    "reduce",
    "two",
    "sets",
    "case",
    "already",
    "two",
    "sets",
    "2d",
    "photograph",
    "know",
    "two",
    "dimensions",
    "actually",
    "64",
    "64",
    "three",
    "getting",
    "two",
    "two",
    "two",
    "dimension",
    "two",
    "dimensional",
    "instead",
    "third",
    "dimension",
    "colors",
    "go",
    "ahead",
    "run",
    "really",
    "seeing",
    "anything",
    "run",
    "script",
    "setting",
    "set",
    "start",
    "playing",
    "maybe",
    "add",
    "different",
    "layer",
    "something",
    "else",
    "see",
    "works",
    "see",
    "output",
    "makes",
    "cross",
    "nice",
    "couple",
    "flips",
    "code",
    "put",
    "whole",
    "new",
    "layer",
    "whole",
    "new",
    "processing",
    "see",
    "whether",
    "improves",
    "run",
    "makes",
    "worse",
    "finally",
    "going",
    "final",
    "setup",
    "flatten",
    "classifier",
    "add",
    "flattened",
    "setup",
    "going",
    "also",
    "add",
    "layer",
    "dense",
    "layer",
    "going",
    "add",
    "another",
    "dense",
    "layer",
    "going",
    "build",
    "going",
    "compile",
    "whole",
    "thing",
    "together",
    "let",
    "flip",
    "see",
    "looks",
    "like",
    "even",
    "numbered",
    "going",
    "flattening",
    "flatten",
    "exactly",
    "sounds",
    "like",
    "working",
    "array",
    "picture",
    "actually",
    "three",
    "dimensions",
    "pixels",
    "pixels",
    "whole",
    "dimension",
    "three",
    "different",
    "values",
    "kind",
    "resized",
    "two",
    "two",
    "going",
    "flatten",
    "want",
    "multiple",
    "dimensions",
    "worked",
    "tensor",
    "keras",
    "want",
    "single",
    "array",
    "flattened",
    "step",
    "four",
    "full",
    "connection",
    "add",
    "final",
    "two",
    "layers",
    "could",
    "actually",
    "kinds",
    "things",
    "could",
    "actually",
    "leave",
    "layers",
    "play",
    "need",
    "flatten",
    "important",
    "want",
    "use",
    "dents",
    "taking",
    "taking",
    "whatever",
    "came",
    "take",
    "different",
    "two",
    "dimensions",
    "three",
    "dimensions",
    "flatten",
    "one",
    "dimension",
    "want",
    "take",
    "going",
    "pull",
    "units",
    "got",
    "say",
    "get",
    "128",
    "could",
    "actually",
    "play",
    "number",
    "get",
    "kinds",
    "weird",
    "results",
    "case",
    "took",
    "64",
    "plus",
    "64",
    "could",
    "probably",
    "even",
    "64",
    "usually",
    "want",
    "keep",
    "multiple",
    "whatever",
    "data",
    "shape",
    "already",
    "using",
    "using",
    "activation",
    "relu",
    "like",
    "finally",
    "filter",
    "single",
    "output",
    "many",
    "units",
    "one",
    "want",
    "know",
    "whether",
    "true",
    "false",
    "either",
    "dog",
    "cat",
    "could",
    "say",
    "one",
    "dog",
    "zero",
    "cat",
    "maybe",
    "cat",
    "lover",
    "one",
    "cat",
    "zero",
    "dog",
    "love",
    "dogs",
    "cats",
    "gon",
    "na",
    "choose",
    "use",
    "sigmoid",
    "activation",
    "remember",
    "relu",
    "also",
    "sigmoid",
    "sigmoid",
    "makes",
    "clear",
    "yes",
    "want",
    "kind",
    "number",
    "coming",
    "go",
    "ahead",
    "run",
    "see",
    "still",
    "setup",
    "finally",
    "want",
    "go",
    "ahead",
    "compile",
    "let",
    "put",
    "compiling",
    "classifier",
    "neural",
    "network",
    "going",
    "use",
    "optimizer",
    "atom",
    "hinted",
    "little",
    "bit",
    "adam",
    "come",
    "optimizer",
    "come",
    "well",
    "optimizer",
    "reverse",
    "propagation",
    "training",
    "goes",
    "way",
    "says",
    "error",
    "readjust",
    "weights",
    "number",
    "atom",
    "commonly",
    "used",
    "works",
    "best",
    "large",
    "data",
    "people",
    "stick",
    "atom",
    "testing",
    "smaller",
    "data",
    "see",
    "model",
    "going",
    "go",
    "get",
    "errors",
    "run",
    "larger",
    "data",
    "sets",
    "going",
    "run",
    "atom",
    "anyway",
    "leave",
    "atom",
    "commonly",
    "used",
    "ones",
    "aware",
    "might",
    "try",
    "stuck",
    "bind",
    "might",
    "blur",
    "future",
    "usually",
    "atom",
    "fine",
    "two",
    "settings",
    "loss",
    "metrics",
    "going",
    "dig",
    "much",
    "loss",
    "metrics",
    "things",
    "really",
    "explore",
    "cross",
    "many",
    "choices",
    "computes",
    "error",
    "many",
    "different",
    "ways",
    "back",
    "propagation",
    "training",
    "using",
    "atom",
    "model",
    "compute",
    "error",
    "standard",
    "deviation",
    "standard",
    "deviation",
    "squared",
    "use",
    "binary",
    "cross",
    "entropy",
    "look",
    "even",
    "know",
    "many",
    "lot",
    "times",
    "start",
    "ones",
    "look",
    "correct",
    "commonly",
    "used",
    "go",
    "read",
    "cross",
    "site",
    "actually",
    "see",
    "different",
    "losses",
    "metrics",
    "different",
    "options",
    "going",
    "get",
    "much",
    "reference",
    "cross",
    "website",
    "explore",
    "deeper",
    "going",
    "go",
    "ahead",
    "run",
    "set",
    "classifier",
    "object",
    "classifier",
    "go",
    "back",
    "see",
    "added",
    "step",
    "one",
    "added",
    "layer",
    "input",
    "added",
    "layer",
    "comes",
    "uses",
    "relu",
    "activation",
    "pulls",
    "data",
    "even",
    "though",
    "two",
    "layers",
    "actual",
    "neural",
    "network",
    "layer",
    "uses",
    "pull",
    "data",
    "two",
    "two",
    "array",
    "array",
    "colors",
    "flatten",
    "outer",
    "flattened",
    "add",
    "another",
    "dense",
    "call",
    "dense",
    "layer",
    "dense",
    "layer",
    "goes",
    "downsizes",
    "128",
    "reduces",
    "look",
    "mapping",
    "data",
    "two",
    "dimensional",
    "setup",
    "flatten",
    "map",
    "flattened",
    "map",
    "take",
    "reduce",
    "128",
    "use",
    "relu",
    "finally",
    "reduce",
    "single",
    "output",
    "use",
    "sigmoid",
    "figure",
    "whether",
    "yes",
    "true",
    "false",
    "case",
    "cat",
    "dog",
    "finally",
    "put",
    "layers",
    "together",
    "compile",
    "done",
    "compiled",
    "far",
    "trains",
    "use",
    "settings",
    "training",
    "back",
    "propagation",
    "remember",
    "talked",
    "training",
    "setup",
    "go",
    "see",
    "two",
    "data",
    "sets",
    "one",
    "called",
    "training",
    "set",
    "testing",
    "set",
    "standard",
    "data",
    "processing",
    "need",
    "pretty",
    "common",
    "data",
    "processing",
    "need",
    "certain",
    "amount",
    "data",
    "train",
    "got",
    "know",
    "whether",
    "works",
    "good",
    "separate",
    "set",
    "data",
    "testing",
    "already",
    "know",
    "answer",
    "want",
    "use",
    "part",
    "training",
    "set",
    "jump",
    "part",
    "two",
    "fitting",
    "classifier",
    "neural",
    "network",
    "images",
    "cross",
    "let",
    "zoom",
    "always",
    "love",
    "working",
    "jupiter",
    "notebooks",
    "really",
    "see",
    "going",
    "come",
    "cross",
    "image",
    "import",
    "image",
    "data",
    "generator",
    "nice",
    "cross",
    "product",
    "right",
    "going",
    "since",
    "images",
    "common",
    "already",
    "stuff",
    "help",
    "us",
    "process",
    "data",
    "great",
    "come",
    "train",
    "data",
    "gin",
    "going",
    "create",
    "object",
    "helping",
    "us",
    "train",
    "reshaping",
    "data",
    "going",
    "work",
    "setup",
    "use",
    "image",
    "data",
    "generator",
    "going",
    "rescale",
    "see",
    "one",
    "point",
    "tells",
    "us",
    "float",
    "value",
    "rescale",
    "255",
    "come",
    "well",
    "scale",
    "colors",
    "pictures",
    "using",
    "value",
    "0",
    "want",
    "divide",
    "255",
    "generate",
    "number",
    "0",
    "shear",
    "range",
    "zoom",
    "range",
    "horizontal",
    "flip",
    "equals",
    "true",
    "course",
    "photos",
    "different",
    "shapes",
    "sizes",
    "like",
    "said",
    "wonderful",
    "package",
    "really",
    "need",
    "dig",
    "deep",
    "see",
    "different",
    "options",
    "setting",
    "images",
    "right",
    "though",
    "going",
    "stick",
    "basic",
    "stuff",
    "let",
    "go",
    "ahead",
    "run",
    "code",
    "really",
    "anything",
    "still",
    "setting",
    "let",
    "take",
    "look",
    "next",
    "set",
    "code",
    "one",
    "huge",
    "creating",
    "training",
    "set",
    "training",
    "set",
    "going",
    "go",
    "going",
    "use",
    "train",
    "data",
    "gen",
    "created",
    "dot",
    "flow",
    "directory",
    "going",
    "access",
    "case",
    "path",
    "data",
    "set",
    "training",
    "set",
    "folder",
    "going",
    "pull",
    "images",
    "folder",
    "actually",
    "running",
    "folder",
    "data",
    "sets",
    "setup",
    "load",
    "data",
    "make",
    "sure",
    "wherever",
    "jupyter",
    "notebook",
    "saving",
    "things",
    "create",
    "path",
    "complete",
    "path",
    "need",
    "know",
    "c",
    "colon",
    "slash",
    "etc",
    "target",
    "size",
    "batch",
    "size",
    "class",
    "mode",
    "binary",
    "class",
    "switching",
    "everything",
    "binary",
    "value",
    "batch",
    "size",
    "heck",
    "batch",
    "size",
    "well",
    "many",
    "pictures",
    "going",
    "batch",
    "training",
    "time",
    "target",
    "size",
    "64",
    "little",
    "confusing",
    "see",
    "right",
    "general",
    "training",
    "go",
    "look",
    "different",
    "settings",
    "training",
    "set",
    "course",
    "different",
    "data",
    "pictures",
    "kinds",
    "different",
    "settings",
    "depending",
    "working",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "happens",
    "see",
    "found",
    "800",
    "images",
    "belonging",
    "one",
    "classes",
    "800",
    "images",
    "training",
    "set",
    "going",
    "training",
    "set",
    "also",
    "format",
    "pictures",
    "test",
    "set",
    "actually",
    "predictions",
    "actually",
    "programming",
    "model",
    "yet",
    "preparing",
    "data",
    "going",
    "prepare",
    "training",
    "set",
    "test",
    "set",
    "changes",
    "make",
    "training",
    "set",
    "point",
    "also",
    "made",
    "test",
    "set",
    "done",
    "thing",
    "done",
    "train",
    "data",
    "generator",
    "done",
    "training",
    "set",
    "also",
    "remember",
    "test",
    "set",
    "data",
    "going",
    "thing",
    "going",
    "create",
    "test",
    "data",
    "gen",
    "going",
    "image",
    "data",
    "generator",
    "going",
    "rescale",
    "1",
    "255",
    "need",
    "settings",
    "single",
    "setting",
    "test",
    "datagen",
    "going",
    "create",
    "test",
    "set",
    "going",
    "thing",
    "test",
    "set",
    "except",
    "pulling",
    "test",
    "set",
    "folder",
    "run",
    "see",
    "test",
    "set",
    "found",
    "2",
    "000",
    "images",
    "right",
    "using",
    "20",
    "percent",
    "images",
    "test",
    "80",
    "train",
    "finally",
    "set",
    "data",
    "set",
    "layers",
    "work",
    "cleaning",
    "data",
    "making",
    "sure",
    "going",
    "correctly",
    "actually",
    "going",
    "fit",
    "going",
    "train",
    "data",
    "set",
    "let",
    "see",
    "looks",
    "like",
    "go",
    "let",
    "put",
    "information",
    "let",
    "take",
    "quick",
    "look",
    "looking",
    "fit",
    "generator",
    "classifier",
    "dot",
    "fit",
    "generator",
    "back",
    "propagation",
    "information",
    "goes",
    "forward",
    "picture",
    "says",
    "oh",
    "either",
    "right",
    "wrong",
    "air",
    "goes",
    "backward",
    "reprograms",
    "weights",
    "training",
    "neural",
    "network",
    "course",
    "using",
    "training",
    "set",
    "remember",
    "created",
    "training",
    "set",
    "going",
    "steps",
    "per",
    "epic",
    "8",
    "000",
    "steps",
    "epic",
    "means",
    "many",
    "times",
    "go",
    "pictures",
    "going",
    "rerun",
    "pictures",
    "going",
    "go",
    "whole",
    "data",
    "set",
    "25",
    "times",
    "going",
    "look",
    "picture",
    "epic",
    "8",
    "000",
    "times",
    "really",
    "programming",
    "heck",
    "going",
    "back",
    "validation",
    "data",
    "equals",
    "test",
    "set",
    "training",
    "set",
    "gon",
    "na",
    "test",
    "set",
    "validate",
    "gon",
    "na",
    "one",
    "shot",
    "gon",
    "na",
    "look",
    "gon",
    "na",
    "200",
    "steps",
    "validation",
    "see",
    "looks",
    "like",
    "minute",
    "let",
    "go",
    "ahead",
    "run",
    "training",
    "going",
    "fit",
    "data",
    "goes",
    "says",
    "epic",
    "one",
    "25",
    "start",
    "realizing",
    "going",
    "take",
    "older",
    "computer",
    "takes",
    "45",
    "minutes",
    "dual",
    "processor",
    "processing",
    "uh",
    "10",
    "000",
    "photos",
    "small",
    "amount",
    "photographs",
    "process",
    "laptop",
    "going",
    "take",
    "let",
    "go",
    "ahead",
    "go",
    "get",
    "cup",
    "coffee",
    "sip",
    "come",
    "back",
    "see",
    "looks",
    "like",
    "back",
    "know",
    "gone",
    "actually",
    "lengthy",
    "pause",
    "made",
    "couple",
    "changes",
    "let",
    "discuss",
    "changes",
    "real",
    "quick",
    "made",
    "first",
    "thing",
    "going",
    "going",
    "go",
    "insert",
    "cell",
    "let",
    "paste",
    "original",
    "code",
    "back",
    "see",
    "original",
    "thing",
    "steps",
    "per",
    "epic",
    "eight",
    "thousand",
    "twenty",
    "five",
    "epics",
    "validation",
    "steps",
    "two",
    "thousand",
    "changed",
    "four",
    "thousand",
    "epics",
    "four",
    "thousand",
    "steps",
    "per",
    "epic",
    "10",
    "epics",
    "10",
    "validation",
    "steps",
    "cause",
    "problems",
    "commercial",
    "release",
    "demo",
    "purposes",
    "work",
    "remember",
    "steps",
    "per",
    "epic",
    "many",
    "photos",
    "going",
    "process",
    "fact",
    "let",
    "go",
    "ahead",
    "get",
    "drawing",
    "pin",
    "let",
    "highlight",
    "right",
    "well",
    "8",
    "000",
    "pictures",
    "going",
    "epic",
    "going",
    "change",
    "4",
    "000",
    "going",
    "cut",
    "half",
    "going",
    "randomly",
    "pick",
    "4",
    "000",
    "pictures",
    "time",
    "goes",
    "epic",
    "epic",
    "many",
    "processes",
    "25",
    "going",
    "cut",
    "instead",
    "25",
    "runs",
    "8",
    "000",
    "photos",
    "math",
    "25",
    "times",
    "8",
    "going",
    "10",
    "4",
    "000",
    "going",
    "run",
    "40",
    "000",
    "times",
    "processes",
    "next",
    "thing",
    "know",
    "want",
    "notice",
    "also",
    "change",
    "validation",
    "step",
    "would",
    "cause",
    "major",
    "problems",
    "releasing",
    "dropped",
    "way",
    "validation",
    "step",
    "says",
    "2",
    "000",
    "photos",
    "trainings",
    "testing",
    "set",
    "going",
    "use",
    "validation",
    "well",
    "going",
    "use",
    "random",
    "10",
    "validate",
    "really",
    "best",
    "settings",
    "let",
    "show",
    "let",
    "scroll",
    "little",
    "bit",
    "let",
    "look",
    "output",
    "see",
    "going",
    "got",
    "drawing",
    "tool",
    "back",
    "see",
    "lists",
    "run",
    "time",
    "goes",
    "epic",
    "going",
    "4",
    "000",
    "steps",
    "4",
    "000",
    "comes",
    "epic",
    "one",
    "10",
    "4",
    "000",
    "steps",
    "randomly",
    "picking",
    "half",
    "pictures",
    "file",
    "going",
    "going",
    "look",
    "number",
    "right",
    "whole",
    "epic",
    "2411",
    "seconds",
    "remember",
    "correctly",
    "divide",
    "60",
    "get",
    "minutes",
    "divide",
    "60",
    "get",
    "hours",
    "divide",
    "whole",
    "thing",
    "60",
    "times",
    "60",
    "3600",
    "hour",
    "roughly",
    "45",
    "minutes",
    "right",
    "45",
    "minutes",
    "process",
    "half",
    "pictures",
    "pictures",
    "talking",
    "hour",
    "half",
    "per",
    "epic",
    "times",
    "36",
    "25",
    "25",
    "25",
    "roughly",
    "couple",
    "days",
    "couple",
    "days",
    "processing",
    "well",
    "demo",
    "want",
    "want",
    "come",
    "back",
    "next",
    "day",
    "plus",
    "computer",
    "reboot",
    "middle",
    "night",
    "look",
    "say",
    "okay",
    "let",
    "testing",
    "computer",
    "running",
    "dual",
    "core",
    "processor",
    "runs",
    "point",
    "nine",
    "gigahertz",
    "per",
    "second",
    "laptop",
    "know",
    "good",
    "four",
    "years",
    "ago",
    "running",
    "something",
    "like",
    "probably",
    "little",
    "slow",
    "cut",
    "times",
    "last",
    "one",
    "validation",
    "validating",
    "random",
    "10",
    "photos",
    "comes",
    "effect",
    "going",
    "see",
    "accuracy",
    "value",
    "loss",
    "value",
    "accuracy",
    "loss",
    "important",
    "numbers",
    "look",
    "10",
    "means",
    "validating",
    "across",
    "10",
    "pictures",
    "value",
    "acc",
    "accuracy",
    "value",
    "loss",
    "going",
    "worry",
    "much",
    "accuracy",
    "accuracy",
    "running",
    "putting",
    "two",
    "numbers",
    "together",
    "accuracy",
    "value",
    "accuracy",
    "end",
    "epic",
    "accuracy",
    "epic",
    "looking",
    "tutorial",
    "going",
    "go",
    "deep",
    "numbers",
    "really",
    "important",
    "start",
    "talking",
    "two",
    "numbers",
    "reflect",
    "bias",
    "really",
    "important",
    "put",
    "bias",
    "little",
    "bit",
    "beyond",
    "tutorial",
    "short",
    "accuracy",
    "validation",
    "per",
    "step",
    "going",
    "value",
    "accuracy",
    "continues",
    "go",
    "means",
    "bias",
    "means",
    "memorizing",
    "photos",
    "looking",
    "actually",
    "looking",
    "makes",
    "dog",
    "dog",
    "makes",
    "cat",
    "cat",
    "memorizing",
    "discrepancy",
    "grows",
    "bigger",
    "bias",
    "really",
    "beauty",
    "cross",
    "neural",
    "network",
    "lot",
    "features",
    "like",
    "make",
    "really",
    "easy",
    "track",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "next",
    "set",
    "code",
    "part",
    "three",
    "going",
    "make",
    "new",
    "prediction",
    "going",
    "bring",
    "couple",
    "tools",
    "process",
    "image",
    "coming",
    "find",
    "whether",
    "actual",
    "dog",
    "cat",
    "actually",
    "use",
    "identify",
    "course",
    "final",
    "step",
    "part",
    "three",
    "print",
    "prediction",
    "go",
    "ahead",
    "combine",
    "course",
    "see",
    "adding",
    "sticky",
    "notes",
    "computer",
    "screen",
    "hidden",
    "behind",
    "screen",
    "last",
    "one",
    "forget",
    "feed",
    "cat",
    "dog",
    "let",
    "go",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "code",
    "put",
    "jupiter",
    "notebook",
    "right",
    "let",
    "paste",
    "start",
    "importing",
    "numpy",
    "np",
    "numpy",
    "common",
    "package",
    "pretty",
    "much",
    "import",
    "python",
    "project",
    "working",
    "another",
    "one",
    "use",
    "regularly",
    "pandas",
    "ways",
    "organizing",
    "data",
    "np",
    "usually",
    "standard",
    "machine",
    "learning",
    "tools",
    "return",
    "data",
    "array",
    "although",
    "know",
    "use",
    "standard",
    "data",
    "array",
    "python",
    "cross",
    "import",
    "image",
    "fish",
    "look",
    "familiar",
    "going",
    "take",
    "test",
    "image",
    "going",
    "set",
    "equal",
    "case",
    "cat",
    "dog",
    "one",
    "see",
    "know",
    "let",
    "get",
    "drawing",
    "tool",
    "back",
    "let",
    "take",
    "look",
    "test",
    "image",
    "loading",
    "test",
    "image",
    "one",
    "one",
    "data",
    "seen",
    "one",
    "new",
    "oh",
    "let",
    "shrink",
    "screen",
    "let",
    "start",
    "test",
    "image",
    "went",
    "ahead",
    "cross",
    "processing",
    "nice",
    "image",
    "set",
    "going",
    "load",
    "image",
    "going",
    "alter",
    "64",
    "64",
    "print",
    "right",
    "bat",
    "going",
    "cross",
    "nice",
    "way",
    "automatically",
    "sets",
    "us",
    "redo",
    "images",
    "find",
    "way",
    "reset",
    "use",
    "also",
    "set",
    "image",
    "array",
    "data",
    "like",
    "test",
    "information",
    "training",
    "data",
    "use",
    "numpy",
    "numpy",
    "uh",
    "right",
    "important",
    "numpy",
    "p",
    "expand",
    "dimensions",
    "test",
    "image",
    "axes",
    "equals",
    "puts",
    "single",
    "array",
    "finally",
    "work",
    "run",
    "result",
    "click",
    "go",
    "result",
    "equals",
    "classifier",
    "predict",
    "test",
    "image",
    "find",
    "well",
    "test",
    "image",
    "let",
    "take",
    "quick",
    "look",
    "see",
    "see",
    "ran",
    "comes",
    "dog",
    "look",
    "images",
    "cat",
    "dog",
    "image",
    "number",
    "one",
    "looks",
    "like",
    "nice",
    "floppy",
    "eared",
    "lab",
    "friendly",
    "tongue",
    "hanging",
    "either",
    "floppy",
    "eared",
    "cat",
    "sure",
    "according",
    "software",
    "says",
    "dog",
    "uh",
    "second",
    "picture",
    "let",
    "see",
    "happens",
    "run",
    "second",
    "picture",
    "go",
    "change",
    "uh",
    "dog",
    "image",
    "one",
    "two",
    "run",
    "comes",
    "says",
    "cat",
    "see",
    "highlighting",
    "cat",
    "process",
    "works",
    "able",
    "label",
    "dog",
    "dog",
    "cat",
    "cat",
    "pictures",
    "go",
    "cleared",
    "drawing",
    "tool",
    "last",
    "thing",
    "want",
    "notice",
    "come",
    "back",
    "ran",
    "see",
    "accuracy",
    "1",
    "value",
    "accuracy",
    "well",
    "value",
    "accuracy",
    "important",
    "one",
    "value",
    "accuracy",
    "actually",
    "runs",
    "test",
    "data",
    "remember",
    "testing",
    "validating",
    "random",
    "10",
    "photos",
    "10",
    "folders",
    "happened",
    "come",
    "one",
    "ran",
    "server",
    "actually",
    "came",
    "86",
    "percent",
    "cutting",
    "numbers",
    "far",
    "commercial",
    "release",
    "bad",
    "want",
    "make",
    "sure",
    "little",
    "careful",
    "testing",
    "stuff",
    "change",
    "numbers",
    "back",
    "run",
    "enterprise",
    "computer",
    "old",
    "laptop",
    "practicing",
    "messing",
    "come",
    "know",
    "validation",
    "cat",
    "successfully",
    "built",
    "neural",
    "network",
    "could",
    "distinguish",
    "photos",
    "cat",
    "dog",
    "imagine",
    "things",
    "could",
    "distinguish",
    "imagine",
    "different",
    "industries",
    "could",
    "dive",
    "able",
    "understand",
    "two",
    "difference",
    "pictures",
    "mosquitoes",
    "could",
    "find",
    "mosquitoes",
    "bite",
    "versus",
    "mosquitoes",
    "friendly",
    "turns",
    "mosquitoes",
    "bite",
    "us",
    "four",
    "percent",
    "mosquito",
    "population",
    "even",
    "maybe",
    "two",
    "percent",
    "kinds",
    "industries",
    "use",
    "many",
    "industries",
    "realizing",
    "powerful",
    "tools",
    "photos",
    "alone",
    "myriad",
    "industries",
    "sprouting",
    "said",
    "say",
    "exciting",
    "time",
    "live",
    "tools",
    "get",
    "play",
    "back",
    "propagation",
    "gradient",
    "descent",
    "talking",
    "neural",
    "networks",
    "talk",
    "neural",
    "network",
    "simple",
    "neural",
    "network",
    "must",
    "trained",
    "recognize",
    "handwritten",
    "alphabets",
    "b",
    "c",
    "see",
    "input",
    "coming",
    "case",
    "look",
    "letter",
    "written",
    "28",
    "28",
    "pixels",
    "handwritten",
    "outfits",
    "presented",
    "images",
    "28",
    "28",
    "pixels",
    "image",
    "comes",
    "case",
    "784",
    "neurons",
    "28",
    "times",
    "28",
    "initial",
    "prediction",
    "made",
    "using",
    "random",
    "weights",
    "assigned",
    "channel",
    "forward",
    "propagation",
    "see",
    "node",
    "values",
    "added",
    "added",
    "going",
    "across",
    "network",
    "predicts",
    "input",
    "b",
    "probability",
    "predicted",
    "probabilities",
    "compared",
    "actual",
    "probabilities",
    "errors",
    "calculated",
    "error",
    "simply",
    "actual",
    "minus",
    "predicted",
    "see",
    "know",
    "c",
    "minus",
    "2",
    "know",
    "b",
    "minus",
    "know",
    "go",
    "ahead",
    "adjust",
    "magnitude",
    "indicates",
    "amount",
    "change",
    "sign",
    "indicates",
    "increase",
    "decrease",
    "weights",
    "information",
    "transmitted",
    "back",
    "network",
    "comes",
    "back",
    "propagation",
    "weights",
    "throughout",
    "network",
    "adjusted",
    "order",
    "reduce",
    "loss",
    "prediction",
    "look",
    "setup",
    "right",
    "comes",
    "error",
    "case",
    "minus",
    "minus",
    "comes",
    "adjusts",
    "multipliers",
    "manner",
    "keep",
    "training",
    "network",
    "multiple",
    "inputs",
    "able",
    "predict",
    "high",
    "accuracy",
    "see",
    "different",
    "quickly",
    "switches",
    "cursive",
    "maybe",
    "elongated",
    "similarly",
    "network",
    "trained",
    "images",
    "b",
    "c",
    "two",
    "let",
    "take",
    "look",
    "uh",
    "straightforward",
    "data",
    "sets",
    "let",
    "build",
    "neural",
    "network",
    "predict",
    "outputs",
    "given",
    "inputs",
    "input",
    "zero",
    "expect",
    "output",
    "zero",
    "input",
    "one",
    "expect",
    "six",
    "two",
    "equals",
    "twelve",
    "three",
    "come",
    "eighteen",
    "four",
    "multiples",
    "six",
    "take",
    "time",
    "look",
    "example",
    "input",
    "goes",
    "neural",
    "network",
    "box",
    "represents",
    "neural",
    "network",
    "one",
    "cool",
    "things",
    "neural",
    "networks",
    "always",
    "little",
    "black",
    "box",
    "kind",
    "train",
    "want",
    "really",
    "know",
    "exactly",
    "weights",
    "although",
    "high",
    "end",
    "setups",
    "start",
    "looking",
    "weights",
    "work",
    "get",
    "output",
    "going",
    "case",
    "input",
    "going",
    "x",
    "output",
    "going",
    "w",
    "weight",
    "value",
    "times",
    "weight",
    "case",
    "single",
    "neuron",
    "going",
    "x",
    "times",
    "w",
    "network",
    "starts",
    "training",
    "choosing",
    "random",
    "value",
    "w",
    "going",
    "guess",
    "w",
    "equals",
    "three",
    "roll",
    "dice",
    "randomly",
    "generate",
    "number",
    "three",
    "w",
    "put",
    "w",
    "equals",
    "three",
    "input",
    "zero",
    "output",
    "zero",
    "w",
    "equals",
    "three",
    "equals",
    "zero",
    "error",
    "first",
    "line",
    "actually",
    "comes",
    "correct",
    "one",
    "times",
    "uh",
    "put",
    "1",
    "looking",
    "6",
    "get",
    "3",
    "instead",
    "put",
    "2",
    "looking",
    "12",
    "get",
    "6",
    "set",
    "see",
    "predicted",
    "output",
    "match",
    "output",
    "looking",
    "take",
    "w",
    "equals",
    "three",
    "come",
    "second",
    "model",
    "w",
    "equals",
    "six",
    "going",
    "look",
    "figure",
    "w",
    "equals",
    "six",
    "minute",
    "part",
    "math",
    "behind",
    "see",
    "put",
    "w",
    "equals",
    "six",
    "build",
    "w",
    "equals",
    "6",
    "chart",
    "end",
    "0",
    "6",
    "12",
    "18",
    "24",
    "output",
    "looking",
    "manner",
    "end",
    "correct",
    "answer",
    "go",
    "ahead",
    "put",
    "third",
    "model",
    "w",
    "equals",
    "point",
    "one",
    "way",
    "guess",
    "w",
    "equals",
    "see",
    "w",
    "equals",
    "9",
    "get",
    "incorrect",
    "answers",
    "get",
    "9",
    "18",
    "27",
    "36",
    "humans",
    "know",
    "taking",
    "look",
    "data",
    "weight",
    "six",
    "machine",
    "come",
    "conclusion",
    "program",
    "computer",
    "learn",
    "instead",
    "waiting",
    "us",
    "tell",
    "right",
    "answer",
    "correct",
    "answer",
    "imagine",
    "simple",
    "problem",
    "guessing",
    "w",
    "equals",
    "three",
    "guess",
    "w",
    "equals",
    "six",
    "guess",
    "w",
    "equals",
    "nine",
    "look",
    "results",
    "go",
    "oh",
    "got",
    "ta",
    "w",
    "equals",
    "six",
    "best",
    "result",
    "humans",
    "wan",
    "na",
    "take",
    "element",
    "computer",
    "us",
    "gon",
    "na",
    "loss",
    "function",
    "loss",
    "function",
    "measure",
    "error",
    "defines",
    "precision",
    "lost",
    "comparing",
    "predicted",
    "output",
    "actual",
    "output",
    "simply",
    "loss",
    "equals",
    "actual",
    "output",
    "minus",
    "predicted",
    "output",
    "square",
    "whole",
    "thing",
    "let",
    "apply",
    "loss",
    "function",
    "input",
    "value",
    "2",
    "loss",
    "actual",
    "output",
    "predicted",
    "output",
    "squared",
    "last",
    "function",
    "input",
    "2",
    "end",
    "actual",
    "output",
    "12",
    "see",
    "w",
    "equals",
    "3",
    "12",
    "minus",
    "6",
    "squared",
    "equals",
    "36",
    "end",
    "loss",
    "36",
    "w",
    "equals",
    "6",
    "12",
    "12",
    "times",
    "12",
    "squared",
    "equals",
    "12",
    "minus",
    "12",
    "squared",
    "equals",
    "0",
    "12",
    "minus",
    "18",
    "squared",
    "equals",
    "see",
    "huge",
    "loss",
    "w",
    "equals",
    "three",
    "w",
    "equals",
    "nine",
    "plot",
    "graph",
    "weight",
    "versus",
    "loss",
    "always",
    "helps",
    "nice",
    "visual",
    "going",
    "graphical",
    "method",
    "finding",
    "minimal",
    "function",
    "called",
    "radiant",
    "descent",
    "logic",
    "behind",
    "see",
    "come",
    "go",
    "ahead",
    "graph",
    "loss",
    "36",
    "3",
    "36",
    "happen",
    "guess",
    "6",
    "correct",
    "answer",
    "right",
    "middle",
    "see",
    "right",
    "forms",
    "nice",
    "little",
    "parabola",
    "see",
    "nice",
    "mark",
    "right",
    "middle",
    "human",
    "look",
    "go",
    "ah",
    "answer",
    "random",
    "point",
    "curve",
    "chosen",
    "slope",
    "point",
    "calculated",
    "getting",
    "away",
    "human",
    "aspect",
    "looking",
    "saying",
    "answer",
    "look",
    "going",
    "math",
    "positive",
    "slope",
    "indicates",
    "increase",
    "weight",
    "negative",
    "slope",
    "indicates",
    "decrease",
    "weight",
    "time",
    "slope",
    "negative",
    "hence",
    "another",
    "random",
    "point",
    "towards",
    "left",
    "chosen",
    "see",
    "actually",
    "kind",
    "playing",
    "little",
    "game",
    "going",
    "back",
    "forth",
    "gradient",
    "descent",
    "continue",
    "checking",
    "slopes",
    "various",
    "points",
    "manner",
    "input",
    "actual",
    "output",
    "w3",
    "w6",
    "w9",
    "found",
    "positive",
    "slope",
    "increase",
    "increase",
    "weight",
    "negative",
    "slope",
    "indicates",
    "decrease",
    "weight",
    "zero",
    "slope",
    "indicates",
    "appropriate",
    "weight",
    "aim",
    "reach",
    "point",
    "slope",
    "zero",
    "talk",
    "neural",
    "networks",
    "usually",
    "processing",
    "massive",
    "amount",
    "information",
    "data",
    "going",
    "data",
    "nice",
    "neat",
    "multiple",
    "six",
    "gon",
    "na",
    "messy",
    "gon",
    "na",
    "keep",
    "approaching",
    "number",
    "never",
    "get",
    "everything",
    "fit",
    "going",
    "get",
    "stuff",
    "place",
    "really",
    "looking",
    "minimum",
    "value",
    "looking",
    "absolute",
    "zero",
    "going",
    "get",
    "talking",
    "gradient",
    "descent",
    "talking",
    "finding",
    "bottom",
    "curb",
    "even",
    "go",
    "way",
    "zero",
    "apply",
    "neural",
    "network",
    "well",
    "use",
    "back",
    "propagation",
    "back",
    "propagation",
    "process",
    "updating",
    "weights",
    "network",
    "order",
    "reduce",
    "error",
    "prediction",
    "magnitude",
    "loss",
    "point",
    "graph",
    "combined",
    "slope",
    "fed",
    "back",
    "network",
    "see",
    "simple",
    "model",
    "one",
    "node",
    "x",
    "times",
    "w",
    "input",
    "comes",
    "x",
    "times",
    "w",
    "output",
    "going",
    "propagate",
    "loss",
    "going",
    "way",
    "random",
    "point",
    "graph",
    "gives",
    "loss",
    "value",
    "36",
    "positive",
    "slope",
    "continue",
    "checking",
    "slopes",
    "various",
    "points",
    "manner",
    "random",
    "point",
    "graph",
    "gives",
    "loss",
    "value",
    "36",
    "positive",
    "slope",
    "36",
    "quite",
    "large",
    "number",
    "means",
    "current",
    "weight",
    "needs",
    "change",
    "large",
    "number",
    "positive",
    "slope",
    "indicates",
    "change",
    "weight",
    "must",
    "positive",
    "similarly",
    "another",
    "random",
    "point",
    "graph",
    "gives",
    "loss",
    "value",
    "10",
    "negative",
    "slope",
    "10",
    "small",
    "number",
    "hence",
    "weight",
    "requires",
    "tuned",
    "quite",
    "less",
    "negative",
    "slope",
    "indicates",
    "weight",
    "needs",
    "reduced",
    "rather",
    "increased",
    "multiple",
    "iterations",
    "back",
    "propagation",
    "weights",
    "assigned",
    "appropriate",
    "value",
    "see",
    "input",
    "looked",
    "x",
    "times",
    "six",
    "output",
    "eventually",
    "get",
    "weight",
    "six",
    "single",
    "node",
    "problem",
    "working",
    "right",
    "point",
    "network",
    "trained",
    "used",
    "make",
    "predictions",
    "let",
    "get",
    "back",
    "first",
    "example",
    "see",
    "back",
    "propagation",
    "gradient",
    "descent",
    "fall",
    "place",
    "see",
    "looking",
    "single",
    "node",
    "anymore",
    "28",
    "28",
    "grid",
    "784",
    "inputs",
    "coming",
    "first",
    "level",
    "784",
    "nodes",
    "depending",
    "build",
    "neural",
    "network",
    "next",
    "layer",
    "might",
    "also",
    "784",
    "nodes",
    "might",
    "continually",
    "smallen",
    "depending",
    "need",
    "needed",
    "work",
    "mentioned",
    "earlier",
    "predicted",
    "output",
    "compared",
    "actual",
    "output",
    "see",
    "error",
    "actual",
    "minus",
    "prediction",
    "go",
    "ahead",
    "compute",
    "loss",
    "loss",
    "squared",
    "equals",
    "loss",
    "b",
    "squared",
    "first",
    "iteration",
    "weights",
    "throughout",
    "network",
    "adjusted",
    "order",
    "reduce",
    "loss",
    "prediction",
    "course",
    "second",
    "iteration",
    "coming",
    "different",
    "losses",
    "ways",
    "throughout",
    "network",
    "order",
    "reduce",
    "loss",
    "prediction",
    "underneath",
    "second",
    "third",
    "iteration",
    "keep",
    "iterations",
    "going",
    "back",
    "get",
    "right",
    "value",
    "got",
    "remember",
    "reverse",
    "propagation",
    "looking",
    "one",
    "letter",
    "looking",
    "hundreds",
    "letter",
    "usually",
    "propagate",
    "loss",
    "going",
    "backwards",
    "take",
    "small",
    "piece",
    "adjustments",
    "small",
    "one",
    "correct",
    "want",
    "create",
    "bias",
    "talk",
    "back",
    "propagation",
    "talking",
    "going",
    "data",
    "get",
    "minimal",
    "loss",
    "letter",
    "let",
    "focus",
    "minimum",
    "loss",
    "variable",
    "see",
    "look",
    "end",
    "assume",
    "graph",
    "loss",
    "prediction",
    "variable",
    "compared",
    "ways",
    "continuing",
    "second",
    "layer",
    "loss",
    "49",
    "16",
    "see",
    "makes",
    "nice",
    "curve",
    "guess",
    "bottom",
    "curve",
    "like",
    "graph",
    "show",
    "curve",
    "rest",
    "yeah",
    "x",
    "axis",
    "rest",
    "equals",
    "zero",
    "usually",
    "get",
    "get",
    "perfect",
    "fit",
    "anything",
    "rarely",
    "ever",
    "get",
    "perfect",
    "fit",
    "random",
    "points",
    "chosen",
    "graph",
    "back",
    "propagated",
    "network",
    "order",
    "adjust",
    "weights",
    "able",
    "go",
    "back",
    "network",
    "readjust",
    "weights",
    "find",
    "minimal",
    "value",
    "network",
    "run",
    "new",
    "weights",
    "process",
    "repeated",
    "multiple",
    "times",
    "till",
    "provides",
    "accurate",
    "predictions",
    "weights",
    "justify",
    "adjusted",
    "identify",
    "b",
    "c",
    "two",
    "interesting",
    "actually",
    "time",
    "error",
    "goes",
    "back",
    "kind",
    "find",
    "overall",
    "error",
    "inputs",
    "coming",
    "gets",
    "propagated",
    "going",
    "back",
    "overall",
    "loss",
    "kind",
    "step",
    "away",
    "word",
    "error",
    "error",
    "loss",
    "weights",
    "adjusted",
    "identify",
    "b",
    "c2",
    "lot",
    "times",
    "actually",
    "time",
    "adjust",
    "weights",
    "b",
    "c",
    "saying",
    "thus",
    "gradient",
    "descent",
    "back",
    "propagation",
    "network",
    "completely",
    "trained",
    "taught",
    "identify",
    "b",
    "c",
    "coming",
    "forward",
    "one",
    "interesting",
    "things",
    "neural",
    "networks",
    "training",
    "process",
    "takes",
    "lot",
    "longer",
    "predicting",
    "process",
    "plan",
    "one",
    "training",
    "neural",
    "networks",
    "back",
    "propagation",
    "significantly",
    "longer",
    "going",
    "thousands",
    "data",
    "points",
    "actually",
    "run",
    "forward",
    "quick",
    "makes",
    "things",
    "useful",
    "really",
    "part",
    "today",
    "world",
    "computing",
    "today",
    "going",
    "covering",
    "convolutional",
    "neural",
    "network",
    "tutorial",
    "know",
    "deep",
    "learning",
    "recognizes",
    "objects",
    "image",
    "really",
    "particular",
    "neural",
    "network",
    "image",
    "recognition",
    "works",
    "central",
    "one",
    "biggest",
    "building",
    "blocks",
    "image",
    "recognition",
    "using",
    "convolution",
    "neural",
    "network",
    "basic",
    "picture",
    "hummingbird",
    "pixels",
    "image",
    "fed",
    "input",
    "input",
    "layer",
    "coming",
    "takes",
    "graphic",
    "puts",
    "input",
    "layer",
    "hidden",
    "layers",
    "output",
    "layer",
    "output",
    "layer",
    "one",
    "going",
    "light",
    "say",
    "oh",
    "bird",
    "going",
    "go",
    "depth",
    "going",
    "actually",
    "go",
    "back",
    "forth",
    "number",
    "times",
    "today",
    "catching",
    "image",
    "worry",
    "going",
    "get",
    "details",
    "input",
    "layer",
    "accepts",
    "pixels",
    "image",
    "input",
    "form",
    "arrays",
    "see",
    "actually",
    "labeled",
    "block",
    "bird",
    "different",
    "arrays",
    "dive",
    "deep",
    "looks",
    "like",
    "matrixes",
    "set",
    "hidden",
    "layer",
    "carry",
    "feature",
    "extraction",
    "performing",
    "certain",
    "calculations",
    "manipulation",
    "part",
    "kind",
    "reorganizes",
    "picture",
    "multiple",
    "ways",
    "get",
    "data",
    "easy",
    "read",
    "neural",
    "network",
    "layer",
    "uses",
    "matrix",
    "filter",
    "performs",
    "convolution",
    "operation",
    "detect",
    "patterns",
    "image",
    "remember",
    "convolution",
    "means",
    "coil",
    "twist",
    "going",
    "twist",
    "data",
    "around",
    "alter",
    "use",
    "operation",
    "detect",
    "new",
    "pattern",
    "multiple",
    "hidden",
    "layers",
    "like",
    "convolution",
    "layer",
    "rel",
    "u",
    "pronounced",
    "rectified",
    "linear",
    "unit",
    "activation",
    "function",
    "used",
    "pooling",
    "layer",
    "also",
    "uses",
    "multiple",
    "filters",
    "detect",
    "edges",
    "corners",
    "eyes",
    "feathers",
    "beak",
    "etc",
    "like",
    "term",
    "says",
    "pooling",
    "pulling",
    "information",
    "together",
    "look",
    "lot",
    "closer",
    "little",
    "confusing",
    "dig",
    "deep",
    "try",
    "get",
    "squared",
    "away",
    "finally",
    "fully",
    "connected",
    "layer",
    "identifies",
    "object",
    "image",
    "different",
    "layers",
    "coming",
    "hidden",
    "layers",
    "come",
    "final",
    "area",
    "one",
    "node",
    "one",
    "neural",
    "network",
    "entity",
    "lights",
    "says",
    "bird",
    "going",
    "cover",
    "introduction",
    "cnn",
    "convolution",
    "neural",
    "network",
    "cnn",
    "recognizes",
    "images",
    "going",
    "dig",
    "deeper",
    "really",
    "look",
    "individual",
    "layers",
    "convolutional",
    "neural",
    "network",
    "finally",
    "use",
    "case",
    "implementation",
    "using",
    "cnn",
    "begin",
    "introduction",
    "cnn",
    "introducing",
    "pioneer",
    "convolutional",
    "neural",
    "network",
    "jan",
    "lecun",
    "director",
    "facebook",
    "ai",
    "research",
    "group",
    "built",
    "first",
    "convolutional",
    "neural",
    "network",
    "called",
    "lynette",
    "1988",
    "around",
    "chance",
    "mature",
    "years",
    "used",
    "character",
    "recognition",
    "tasks",
    "like",
    "reading",
    "zip",
    "code",
    "digits",
    "imagine",
    "processing",
    "mail",
    "automating",
    "process",
    "cnn",
    "feed",
    "forward",
    "neural",
    "network",
    "generally",
    "used",
    "analyze",
    "visual",
    "images",
    "producing",
    "data",
    "topology",
    "cnn",
    "also",
    "known",
    "convent",
    "key",
    "looking",
    "images",
    "designed",
    "see",
    "different",
    "layers",
    "dig",
    "near",
    "actually",
    "used",
    "since",
    "using",
    "tensorflow",
    "cross",
    "code",
    "later",
    "see",
    "layers",
    "appear",
    "lot",
    "neural",
    "network",
    "frameworks",
    "case",
    "central",
    "processing",
    "images",
    "variety",
    "captures",
    "multiple",
    "images",
    "really",
    "drills",
    "different",
    "features",
    "example",
    "see",
    "flowers",
    "two",
    "varieties",
    "orchid",
    "rose",
    "think",
    "orchid",
    "much",
    "dainty",
    "beautiful",
    "rose",
    "smells",
    "quite",
    "beautiful",
    "couple",
    "rose",
    "bushes",
    "yard",
    "go",
    "input",
    "layer",
    "data",
    "sent",
    "different",
    "nodes",
    "next",
    "layer",
    "one",
    "hidden",
    "layers",
    "based",
    "different",
    "weights",
    "setup",
    "comes",
    "gives",
    "new",
    "value",
    "values",
    "multiplied",
    "weights",
    "go",
    "next",
    "hidden",
    "layer",
    "output",
    "layer",
    "one",
    "notes",
    "comes",
    "says",
    "orchid",
    "one",
    "comes",
    "says",
    "rose",
    "depending",
    "well",
    "trained",
    "separates",
    "cnn",
    "convolutional",
    "neural",
    "network",
    "neural",
    "networks",
    "convolutional",
    "operation",
    "forms",
    "basis",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "every",
    "image",
    "image",
    "represented",
    "form",
    "arrays",
    "pixel",
    "values",
    "real",
    "image",
    "digit",
    "8",
    "gets",
    "put",
    "pixel",
    "values",
    "represented",
    "form",
    "array",
    "case",
    "two",
    "dimensional",
    "array",
    "see",
    "final",
    "form",
    "transform",
    "digit",
    "8",
    "representational",
    "form",
    "pixels",
    "zeros",
    "ones",
    "ones",
    "represent",
    "case",
    "black",
    "part",
    "eight",
    "zeros",
    "represent",
    "white",
    "background",
    "understand",
    "convolution",
    "neural",
    "network",
    "convolutional",
    "operation",
    "works",
    "going",
    "take",
    "side",
    "step",
    "look",
    "matrixes",
    "case",
    "going",
    "simplify",
    "going",
    "take",
    "two",
    "matrices",
    "b",
    "one",
    "dimension",
    "kind",
    "separate",
    "thinking",
    "learned",
    "want",
    "focus",
    "matrix",
    "aspect",
    "bring",
    "back",
    "together",
    "see",
    "looks",
    "like",
    "put",
    "pieces",
    "convolutional",
    "operation",
    "set",
    "two",
    "arrays",
    "case",
    "single",
    "dimension",
    "matrix",
    "equals",
    "five",
    "three",
    "seven",
    "five",
    "nine",
    "seven",
    "b",
    "equals",
    "one",
    "two",
    "three",
    "convolution",
    "comes",
    "gon",
    "na",
    "look",
    "two",
    "gon",
    "na",
    "start",
    "multiplying",
    "times",
    "b",
    "multiply",
    "arrays",
    "element",
    "wise",
    "get",
    "five",
    "six",
    "six",
    "five",
    "five",
    "times",
    "one",
    "six",
    "three",
    "times",
    "two",
    "six",
    "two",
    "times",
    "three",
    "since",
    "two",
    "arrays",
    "size",
    "setup",
    "going",
    "truncate",
    "first",
    "one",
    "gon",
    "na",
    "look",
    "second",
    "array",
    "multiplied",
    "first",
    "three",
    "elements",
    "first",
    "array",
    "going",
    "little",
    "confusing",
    "remember",
    "computer",
    "gets",
    "repeat",
    "processes",
    "hundreds",
    "times",
    "going",
    "forget",
    "numbers",
    "later",
    "see",
    "bring",
    "back",
    "sum",
    "product",
    "case",
    "5",
    "plus",
    "6",
    "plus",
    "6",
    "equals",
    "times",
    "b",
    "first",
    "digit",
    "matrix",
    "times",
    "b",
    "remember",
    "said",
    "going",
    "forget",
    "digits",
    "three",
    "two",
    "five",
    "move",
    "one",
    "set",
    "take",
    "three",
    "two",
    "five",
    "multiply",
    "times",
    "b",
    "see",
    "three",
    "times",
    "one",
    "three",
    "two",
    "times",
    "two",
    "four",
    "sum",
    "second",
    "digit",
    "times",
    "b",
    "product",
    "matrix",
    "continue",
    "thing",
    "would",
    "go",
    "three",
    "seven",
    "five",
    "seven",
    "five",
    "nine",
    "five",
    "nine",
    "seven",
    "short",
    "matrix",
    "covered",
    "different",
    "entities",
    "match",
    "three",
    "different",
    "levels",
    "b",
    "little",
    "bit",
    "going",
    "cover",
    "use",
    "math",
    "multiplying",
    "matrixes",
    "works",
    "important",
    "understand",
    "going",
    "matrix",
    "multiplying",
    "different",
    "parts",
    "match",
    "smaller",
    "matrix",
    "larger",
    "matrix",
    "know",
    "lot",
    "people",
    "get",
    "lost",
    "know",
    "going",
    "matrixes",
    "uh",
    "oh",
    "scary",
    "math",
    "really",
    "scary",
    "break",
    "looking",
    "section",
    "comparing",
    "b",
    "break",
    "mind",
    "like",
    "realize",
    "okay",
    "taking",
    "two",
    "matrixes",
    "comparing",
    "bringing",
    "value",
    "one",
    "matrix",
    "times",
    "b",
    "deucing",
    "information",
    "way",
    "help",
    "computer",
    "see",
    "different",
    "aspects",
    "let",
    "go",
    "ahead",
    "flip",
    "back",
    "images",
    "back",
    "images",
    "talking",
    "going",
    "basic",
    "two",
    "dimensional",
    "image",
    "get",
    "consider",
    "following",
    "two",
    "images",
    "image",
    "symbol",
    "backslash",
    "press",
    "backslash",
    "image",
    "processed",
    "see",
    "image",
    "forward",
    "slash",
    "opposite",
    "click",
    "forward",
    "slash",
    "button",
    "flips",
    "basic",
    "four",
    "pixels",
    "going",
    "ca",
    "get",
    "basic",
    "little",
    "bit",
    "complicated",
    "picture",
    "take",
    "real",
    "image",
    "smiley",
    "face",
    "represent",
    "form",
    "black",
    "white",
    "pixels",
    "image",
    "computer",
    "black",
    "white",
    "like",
    "saw",
    "convert",
    "zeros",
    "one",
    "one",
    "would",
    "matrix",
    "four",
    "dots",
    "significantly",
    "larger",
    "image",
    "coming",
    "worry",
    "going",
    "bring",
    "together",
    "little",
    "bit",
    "layers",
    "convolutional",
    "neural",
    "network",
    "looking",
    "convolution",
    "layer",
    "really",
    "central",
    "aspect",
    "processing",
    "images",
    "convolutional",
    "neural",
    "network",
    "going",
    "feeding",
    "relu",
    "layer",
    "know",
    "talked",
    "rectified",
    "linear",
    "unit",
    "talk",
    "little",
    "bit",
    "later",
    "relu",
    "act",
    "layer",
    "activated",
    "math",
    "behind",
    "makes",
    "neurons",
    "fire",
    "see",
    "lot",
    "neural",
    "networks",
    "using",
    "processing",
    "smaller",
    "amounts",
    "data",
    "use",
    "atom",
    "activation",
    "feature",
    "large",
    "data",
    "coming",
    "processing",
    "small",
    "amounts",
    "data",
    "image",
    "relu",
    "layer",
    "works",
    "great",
    "pooling",
    "layer",
    "pulling",
    "data",
    "together",
    "pooling",
    "neural",
    "network",
    "term",
    "commonly",
    "used",
    "like",
    "use",
    "term",
    "reduce",
    "coming",
    "map",
    "reduce",
    "side",
    "see",
    "mapping",
    "data",
    "networks",
    "going",
    "reduce",
    "going",
    "pull",
    "together",
    "finally",
    "fully",
    "connected",
    "layer",
    "output",
    "going",
    "come",
    "started",
    "look",
    "matrixes",
    "started",
    "look",
    "convolutional",
    "layer",
    "fits",
    "everything",
    "taken",
    "look",
    "images",
    "going",
    "focus",
    "convolution",
    "layer",
    "since",
    "convolutional",
    "neural",
    "network",
    "convolution",
    "layer",
    "number",
    "filters",
    "perform",
    "convolution",
    "operation",
    "every",
    "image",
    "considered",
    "matrix",
    "pixel",
    "values",
    "consider",
    "following",
    "five",
    "five",
    "image",
    "whose",
    "pixel",
    "values",
    "zero",
    "one",
    "obviously",
    "dealing",
    "color",
    "kinds",
    "things",
    "come",
    "color",
    "processing",
    "want",
    "keep",
    "simple",
    "keep",
    "black",
    "white",
    "image",
    "pixels",
    "sliding",
    "filter",
    "matrix",
    "image",
    "computing",
    "dot",
    "product",
    "detect",
    "patterns",
    "right",
    "going",
    "ask",
    "filter",
    "come",
    "bit",
    "confusing",
    "filter",
    "going",
    "derived",
    "later",
    "build",
    "filters",
    "program",
    "train",
    "model",
    "need",
    "worry",
    "filter",
    "actually",
    "need",
    "understand",
    "convolution",
    "layer",
    "works",
    "filter",
    "filter",
    "mini",
    "filters",
    "one",
    "filter",
    "lots",
    "filters",
    "going",
    "look",
    "different",
    "aspects",
    "filter",
    "might",
    "looking",
    "edges",
    "might",
    "looking",
    "different",
    "parts",
    "cover",
    "little",
    "bit",
    "detail",
    "minute",
    "right",
    "focusing",
    "filter",
    "works",
    "matrix",
    "remember",
    "earlier",
    "talked",
    "multiplying",
    "matrixes",
    "together",
    "two",
    "dimensional",
    "matrix",
    "see",
    "take",
    "filter",
    "multiply",
    "upper",
    "left",
    "image",
    "see",
    "right",
    "one",
    "times",
    "one",
    "one",
    "times",
    "zero",
    "one",
    "times",
    "one",
    "multiply",
    "together",
    "sum",
    "end",
    "convolved",
    "feature",
    "four",
    "going",
    "take",
    "sliding",
    "filter",
    "matrix",
    "image",
    "computing",
    "dot",
    "product",
    "detect",
    "patterns",
    "going",
    "slide",
    "going",
    "predict",
    "first",
    "one",
    "slide",
    "one",
    "notch",
    "predict",
    "second",
    "one",
    "way",
    "new",
    "matrix",
    "matrix",
    "size",
    "filter",
    "reduced",
    "image",
    "whatever",
    "filter",
    "whatever",
    "filtering",
    "going",
    "looking",
    "features",
    "reduced",
    "smaller",
    "matrix",
    "feature",
    "maps",
    "extracted",
    "next",
    "step",
    "move",
    "relu",
    "layer",
    "relu",
    "layer",
    "next",
    "step",
    "first",
    "going",
    "perform",
    "operation",
    "maps",
    "coming",
    "negative",
    "pixels",
    "says",
    "negative",
    "pixels",
    "zero",
    "see",
    "nice",
    "graph",
    "zeros",
    "negatives",
    "value",
    "goes",
    "zero",
    "whatever",
    "value",
    "coming",
    "matrix",
    "introduces",
    "network",
    "say",
    "linearity",
    "talking",
    "fact",
    "feature",
    "value",
    "linear",
    "feature",
    "feature",
    "came",
    "let",
    "say",
    "feature",
    "edge",
    "beak",
    "know",
    "like",
    "backslash",
    "saw",
    "look",
    "say",
    "okay",
    "feature",
    "value",
    "negative",
    "10",
    "10",
    "case",
    "um",
    "one",
    "say",
    "yeah",
    "might",
    "beak",
    "might",
    "might",
    "edge",
    "right",
    "minus",
    "five",
    "means",
    "even",
    "going",
    "look",
    "zero",
    "end",
    "output",
    "output",
    "takes",
    "features",
    "filtered",
    "features",
    "remember",
    "running",
    "one",
    "filter",
    "running",
    "number",
    "filters",
    "image",
    "end",
    "rectified",
    "feature",
    "map",
    "looking",
    "features",
    "coming",
    "weigh",
    "filters",
    "input",
    "looks",
    "like",
    "toucan",
    "bird",
    "exotic",
    "looking",
    "real",
    "image",
    "scanned",
    "multiple",
    "convolution",
    "relu",
    "layers",
    "locating",
    "features",
    "see",
    "turn",
    "black",
    "white",
    "image",
    "case",
    "looking",
    "upper",
    "right",
    "hand",
    "corner",
    "feature",
    "box",
    "scans",
    "lot",
    "times",
    "scan",
    "one",
    "pixel",
    "time",
    "lot",
    "times",
    "skip",
    "two",
    "three",
    "four",
    "pixels",
    "speed",
    "process",
    "one",
    "ways",
    "compensate",
    "enough",
    "resources",
    "computation",
    "large",
    "images",
    "one",
    "filter",
    "slowly",
    "goes",
    "across",
    "image",
    "multiple",
    "filters",
    "programmed",
    "looking",
    "lot",
    "different",
    "filters",
    "going",
    "different",
    "aspects",
    "image",
    "sliding",
    "across",
    "forming",
    "new",
    "matrix",
    "one",
    "aspect",
    "note",
    "relu",
    "layer",
    "one",
    "value",
    "coming",
    "multiple",
    "features",
    "going",
    "generating",
    "multiple",
    "relu",
    "layers",
    "locating",
    "features",
    "important",
    "note",
    "know",
    "quite",
    "bundle",
    "multiple",
    "filters",
    "multiple",
    "rail",
    "u",
    "brings",
    "us",
    "next",
    "step",
    "forward",
    "propagation",
    "going",
    "look",
    "pooling",
    "layer",
    "rectified",
    "feature",
    "map",
    "goes",
    "pooling",
    "layer",
    "pooling",
    "sampling",
    "operation",
    "reduces",
    "dimensionality",
    "feature",
    "map",
    "trying",
    "trying",
    "take",
    "huge",
    "amount",
    "information",
    "reduce",
    "single",
    "answer",
    "specific",
    "kind",
    "bird",
    "iris",
    "rose",
    "rectified",
    "feature",
    "map",
    "see",
    "rectified",
    "feature",
    "map",
    "coming",
    "set",
    "max",
    "pooling",
    "2",
    "2",
    "filters",
    "stride",
    "two",
    "remember",
    "correctly",
    "talked",
    "going",
    "one",
    "pixel",
    "time",
    "uh",
    "well",
    "stride",
    "comes",
    "end",
    "two",
    "two",
    "pooled",
    "feature",
    "map",
    "instead",
    "moving",
    "one",
    "time",
    "looking",
    "every",
    "possible",
    "combination",
    "skip",
    "st",
    "skip",
    "go",
    "two",
    "skip",
    "every",
    "pixel",
    "every",
    "one",
    "reduces",
    "rectified",
    "feature",
    "map",
    "see",
    "16",
    "16",
    "four",
    "four",
    "continually",
    "trying",
    "filter",
    "reduce",
    "data",
    "get",
    "something",
    "manage",
    "see",
    "max",
    "3",
    "4",
    "1",
    "2",
    "max",
    "pooling",
    "looking",
    "max",
    "value",
    "little",
    "bit",
    "different",
    "looking",
    "coming",
    "rectified",
    "feature",
    "finding",
    "max",
    "value",
    "pulling",
    "features",
    "together",
    "instead",
    "think",
    "image",
    "map",
    "think",
    "valuable",
    "feature",
    "area",
    "much",
    "feature",
    "value",
    "want",
    "find",
    "best",
    "maximum",
    "feature",
    "area",
    "might",
    "one",
    "piece",
    "filter",
    "beak",
    "said",
    "oh",
    "see",
    "one",
    "beak",
    "image",
    "skips",
    "says",
    "see",
    "three",
    "image",
    "says",
    "oh",
    "one",
    "rated",
    "four",
    "want",
    "sum",
    "together",
    "know",
    "might",
    "like",
    "five",
    "ones",
    "say",
    "ah",
    "five",
    "might",
    "uh",
    "four",
    "zeros",
    "one",
    "ten",
    "ten",
    "says",
    "well",
    "definitely",
    "beak",
    "ones",
    "say",
    "probably",
    "beak",
    "little",
    "strange",
    "analogy",
    "since",
    "looking",
    "bird",
    "see",
    "pulled",
    "feature",
    "map",
    "comes",
    "looking",
    "max",
    "value",
    "one",
    "matrixes",
    "pooling",
    "layer",
    "uses",
    "different",
    "filters",
    "identify",
    "different",
    "parts",
    "image",
    "like",
    "edges",
    "corners",
    "body",
    "feathers",
    "eyes",
    "beak",
    "etc",
    "know",
    "focus",
    "mainly",
    "beak",
    "obviously",
    "feature",
    "could",
    "different",
    "part",
    "bird",
    "coming",
    "let",
    "take",
    "look",
    "looks",
    "like",
    "structure",
    "convolution",
    "neural",
    "network",
    "far",
    "right",
    "input",
    "image",
    "coming",
    "use",
    "filters",
    "multiple",
    "filters",
    "developed",
    "kind",
    "twist",
    "change",
    "data",
    "multiply",
    "matrixes",
    "take",
    "little",
    "filter",
    "maybe",
    "two",
    "two",
    "multiply",
    "piece",
    "image",
    "step",
    "two",
    "every",
    "piece",
    "image",
    "generates",
    "multiple",
    "convolution",
    "layers",
    "number",
    "convolution",
    "layers",
    "set",
    "looking",
    "data",
    "take",
    "convolution",
    "layers",
    "run",
    "relu",
    "setup",
    "done",
    "release",
    "setup",
    "multiple",
    "values",
    "going",
    "multiple",
    "layers",
    "relu",
    "going",
    "take",
    "multiple",
    "layers",
    "going",
    "pooling",
    "pooling",
    "layers",
    "multiple",
    "poolings",
    "going",
    "point",
    "dealing",
    "sometimes",
    "multiple",
    "dimensions",
    "three",
    "dimensions",
    "strange",
    "data",
    "setups",
    "images",
    "looking",
    "things",
    "four",
    "five",
    "six",
    "seven",
    "dimensions",
    "uh",
    "right",
    "looking",
    "2d",
    "image",
    "dimensions",
    "coming",
    "pooling",
    "layer",
    "next",
    "step",
    "want",
    "reduce",
    "dimensions",
    "flatten",
    "flattening",
    "flattening",
    "process",
    "converting",
    "resultant",
    "arrays",
    "pooled",
    "feature",
    "map",
    "single",
    "long",
    "continuous",
    "linear",
    "vector",
    "see",
    "pooled",
    "feature",
    "map",
    "maybe",
    "bird",
    "wing",
    "values",
    "6847",
    "want",
    "flatten",
    "turn",
    "6847",
    "single",
    "linear",
    "vector",
    "find",
    "pooled",
    "feature",
    "maps",
    "one",
    "long",
    "linear",
    "vector",
    "gone",
    "convolutional",
    "neural",
    "network",
    "part",
    "input",
    "layer",
    "next",
    "setup",
    "done",
    "taken",
    "different",
    "pooling",
    "layers",
    "flatten",
    "combine",
    "single",
    "linear",
    "vector",
    "going",
    "done",
    "flattening",
    "quick",
    "recap",
    "covered",
    "much",
    "important",
    "go",
    "back",
    "take",
    "look",
    "steps",
    "gone",
    "structure",
    "network",
    "far",
    "convolution",
    "twist",
    "filter",
    "multiply",
    "matrixes",
    "end",
    "convolutional",
    "layer",
    "uses",
    "relu",
    "figure",
    "values",
    "going",
    "pooling",
    "numerous",
    "convolution",
    "layers",
    "create",
    "numerous",
    "pooling",
    "layers",
    "pulling",
    "data",
    "together",
    "max",
    "value",
    "one",
    "want",
    "send",
    "forward",
    "want",
    "send",
    "best",
    "value",
    "going",
    "take",
    "pooling",
    "layers",
    "going",
    "flatten",
    "going",
    "combine",
    "single",
    "input",
    "going",
    "final",
    "layer",
    "get",
    "step",
    "might",
    "looking",
    "going",
    "boy",
    "looks",
    "like",
    "normal",
    "intuit",
    "neural",
    "network",
    "correct",
    "flattened",
    "matrix",
    "pooling",
    "layer",
    "becomes",
    "input",
    "pooling",
    "layer",
    "fed",
    "input",
    "fully",
    "connected",
    "layer",
    "classify",
    "image",
    "see",
    "flattened",
    "matrix",
    "comes",
    "case",
    "pixels",
    "flattened",
    "matrix",
    "fed",
    "input",
    "back",
    "toucan",
    "whatever",
    "kind",
    "bird",
    "need",
    "one",
    "identify",
    "kind",
    "bird",
    "comes",
    "ford",
    "propagation",
    "network",
    "different",
    "weights",
    "coming",
    "across",
    "finally",
    "selects",
    "bird",
    "dog",
    "cat",
    "case",
    "even",
    "though",
    "labeled",
    "final",
    "layer",
    "red",
    "output",
    "layer",
    "final",
    "output",
    "layer",
    "says",
    "bird",
    "cat",
    "dog",
    "quick",
    "recap",
    "everything",
    "covered",
    "far",
    "input",
    "image",
    "twisted",
    "multiplied",
    "filters",
    "multiplied",
    "times",
    "matrix",
    "two",
    "matrixes",
    "multiplied",
    "filters",
    "create",
    "convolution",
    "layer",
    "convolution",
    "layers",
    "multiple",
    "layers",
    "building",
    "multiple",
    "layers",
    "different",
    "filters",
    "goes",
    "relu",
    "activation",
    "creates",
    "pooling",
    "get",
    "pooling",
    "layer",
    "pooling",
    "look",
    "best",
    "max",
    "value",
    "coming",
    "convolution",
    "take",
    "layer",
    "flatten",
    "goes",
    "fully",
    "connected",
    "layer",
    "fully",
    "connected",
    "neural",
    "network",
    "output",
    "see",
    "entire",
    "process",
    "cnn",
    "recognizes",
    "bird",
    "kind",
    "nice",
    "showing",
    "little",
    "pixels",
    "going",
    "see",
    "filter",
    "generating",
    "convolution",
    "network",
    "filter",
    "shows",
    "bottom",
    "part",
    "convolution",
    "network",
    "based",
    "uses",
    "relu",
    "pooling",
    "pooling",
    "find",
    "one",
    "best",
    "way",
    "fully",
    "connected",
    "layer",
    "end",
    "classification",
    "output",
    "layer",
    "classification",
    "neural",
    "network",
    "end",
    "covered",
    "lot",
    "theory",
    "till",
    "imagine",
    "one",
    "steps",
    "broken",
    "code",
    "putting",
    "together",
    "little",
    "complicated",
    "step",
    "process",
    "overly",
    "complicated",
    "many",
    "steps",
    "one",
    "two",
    "three",
    "four",
    "five",
    "different",
    "steps",
    "going",
    "sub",
    "steps",
    "going",
    "break",
    "walk",
    "code",
    "use",
    "case",
    "implementation",
    "using",
    "cnn",
    "using",
    "cfar10",
    "dataset",
    "canadian",
    "institute",
    "advanced",
    "research",
    "classifying",
    "images",
    "across",
    "10",
    "categories",
    "unfortunately",
    "let",
    "know",
    "whether",
    "going",
    "toucan",
    "kind",
    "bird",
    "get",
    "find",
    "whether",
    "categorize",
    "ship",
    "frog",
    "deer",
    "bird",
    "airplane",
    "automobile",
    "cat",
    "dog",
    "horse",
    "truck",
    "lot",
    "fun",
    "looking",
    "anything",
    "news",
    "automated",
    "cars",
    "everything",
    "else",
    "see",
    "kind",
    "processing",
    "important",
    "today",
    "world",
    "cutting",
    "edge",
    "far",
    "coming",
    "commercial",
    "deployment",
    "mean",
    "really",
    "cool",
    "stuff",
    "starting",
    "see",
    "everywhere",
    "industry",
    "great",
    "time",
    "playing",
    "figuring",
    "let",
    "go",
    "ahead",
    "dive",
    "code",
    "see",
    "looks",
    "like",
    "actually",
    "writing",
    "script",
    "go",
    "let",
    "uh",
    "one",
    "quick",
    "look",
    "let",
    "take",
    "look",
    "data",
    "batch",
    "one",
    "keys",
    "remember",
    "jupiter",
    "notebook",
    "get",
    "print",
    "statement",
    "put",
    "variable",
    "display",
    "variable",
    "see",
    "data",
    "batch",
    "one",
    "keys",
    "since",
    "dictionary",
    "batch",
    "one",
    "label",
    "data",
    "file",
    "names",
    "actually",
    "see",
    "broken",
    "data",
    "set",
    "next",
    "step",
    "step",
    "four",
    "calling",
    "uh",
    "want",
    "display",
    "image",
    "using",
    "matte",
    "plot",
    "library",
    "many",
    "ways",
    "display",
    "images",
    "even",
    "well",
    "ways",
    "drill",
    "matplot",
    "library",
    "really",
    "good",
    "also",
    "look",
    "first",
    "reshape",
    "uh",
    "setup",
    "shaping",
    "data",
    "little",
    "glimpse",
    "means",
    "uh",
    "gon",
    "na",
    "start",
    "importing",
    "map",
    "plot",
    "course",
    "since",
    "jupiter",
    "notebook",
    "need",
    "matplot",
    "inline",
    "command",
    "shows",
    "page",
    "go",
    "going",
    "import",
    "matplot",
    "plt",
    "remember",
    "matplot",
    "library",
    "pie",
    "plot",
    "like",
    "canvas",
    "paint",
    "stuff",
    "onto",
    "percentage",
    "sign",
    "matplot",
    "library",
    "line",
    "going",
    "show",
    "notebook",
    "course",
    "going",
    "import",
    "numpy",
    "np",
    "numbers",
    "python",
    "array",
    "setup",
    "let",
    "go",
    "ahead",
    "set",
    "x",
    "equals",
    "data",
    "batch",
    "one",
    "pull",
    "data",
    "going",
    "x",
    "value",
    "long",
    "stream",
    "binary",
    "data",
    "need",
    "go",
    "little",
    "bit",
    "reshaping",
    "go",
    "ahead",
    "reshape",
    "data",
    "10",
    "000",
    "images",
    "okay",
    "looks",
    "correct",
    "kind",
    "interesting",
    "thing",
    "took",
    "little",
    "bit",
    "go",
    "research",
    "figure",
    "going",
    "data",
    "32",
    "32",
    "picture",
    "let",
    "let",
    "go",
    "ahead",
    "drawing",
    "pad",
    "uh",
    "32",
    "bits",
    "32",
    "bits",
    "color",
    "three",
    "bits",
    "color",
    "know",
    "data",
    "particularly",
    "like",
    "probably",
    "originally",
    "encoded",
    "pictures",
    "put",
    "three",
    "afterward",
    "going",
    "take",
    "uh",
    "shape",
    "going",
    "take",
    "data",
    "long",
    "stream",
    "information",
    "going",
    "break",
    "10",
    "000",
    "pieces",
    "10",
    "000",
    "pieces",
    "broken",
    "three",
    "pieces",
    "three",
    "pieces",
    "32",
    "look",
    "like",
    "projector",
    "red",
    "screen",
    "red",
    "projector",
    "blue",
    "projector",
    "green",
    "projector",
    "add",
    "together",
    "one",
    "32",
    "32",
    "bit",
    "probably",
    "originally",
    "formatted",
    "kind",
    "ideal",
    "things",
    "changed",
    "going",
    "transpose",
    "going",
    "take",
    "three",
    "going",
    "put",
    "end",
    "first",
    "part",
    "reshaping",
    "data",
    "single",
    "line",
    "bit",
    "data",
    "whatever",
    "format",
    "10",
    "000",
    "3",
    "32",
    "32",
    "going",
    "transpose",
    "color",
    "factor",
    "last",
    "place",
    "image",
    "32",
    "32",
    "middle",
    "part",
    "right",
    "finally",
    "going",
    "take",
    "three",
    "bits",
    "data",
    "put",
    "end",
    "like",
    "process",
    "images",
    "type",
    "really",
    "important",
    "gon",
    "na",
    "use",
    "integer",
    "eight",
    "come",
    "see",
    "lot",
    "try",
    "float",
    "float",
    "got",
    "remember",
    "though",
    "float",
    "uses",
    "lot",
    "memory",
    "switch",
    "uh",
    "something",
    "integer",
    "8",
    "goes",
    "128",
    "going",
    "amount",
    "ram",
    "let",
    "put",
    "going",
    "go",
    "way",
    "amount",
    "ram",
    "loads",
    "want",
    "go",
    "ahead",
    "use",
    "try",
    "ones",
    "see",
    "happens",
    "lot",
    "ram",
    "computer",
    "exercise",
    "work",
    "fine",
    "let",
    "go",
    "ahead",
    "take",
    "run",
    "x",
    "variable",
    "loaded",
    "images",
    "batch",
    "one",
    "data",
    "batch",
    "one",
    "show",
    "talking",
    "type",
    "go",
    "ahead",
    "take",
    "x0",
    "look",
    "max",
    "value",
    "let",
    "go",
    "ahead",
    "run",
    "uh",
    "see",
    "oops",
    "said",
    "128",
    "uh",
    "see",
    "go",
    "255",
    "basically",
    "ascii",
    "character",
    "keeping",
    "keeping",
    "values",
    "255",
    "0",
    "255",
    "versus",
    "float",
    "value",
    "would",
    "bring",
    "exponentially",
    "size",
    "since",
    "using",
    "matplot",
    "library",
    "oops",
    "wanted",
    "since",
    "using",
    "map",
    "plot",
    "library",
    "take",
    "canvas",
    "plt",
    "dot",
    "im",
    "image",
    "show",
    "let",
    "take",
    "look",
    "x0",
    "looks",
    "like",
    "comes",
    "sure",
    "see",
    "low",
    "grade",
    "image",
    "uh",
    "broken",
    "minimal",
    "pixels",
    "thing",
    "oh",
    "let",
    "uh",
    "let",
    "see",
    "one",
    "looks",
    "like",
    "hopefully",
    "little",
    "easier",
    "see",
    "run",
    "enter",
    "let",
    "hit",
    "run",
    "uh",
    "see",
    "probably",
    "semi",
    "truck",
    "good",
    "guess",
    "go",
    "back",
    "instead",
    "typing",
    "line",
    "look",
    "three",
    "uh",
    "looks",
    "like",
    "dump",
    "truck",
    "unloading",
    "uh",
    "10",
    "000",
    "images",
    "jump",
    "55",
    "looks",
    "like",
    "kind",
    "animal",
    "looking",
    "us",
    "probably",
    "dog",
    "fun",
    "let",
    "one",
    "uh",
    "run",
    "see",
    "nice",
    "car",
    "image",
    "number",
    "four",
    "uh",
    "see",
    "pace",
    "different",
    "images",
    "easy",
    "look",
    "reshaped",
    "fit",
    "view",
    "matte",
    "plot",
    "library",
    "uses",
    "format",
    "next",
    "step",
    "going",
    "start",
    "creating",
    "helper",
    "functions",
    "start",
    "one",
    "hot",
    "encoder",
    "help",
    "us",
    "processing",
    "data",
    "remember",
    "labels",
    "ca",
    "words",
    "switch",
    "use",
    "one",
    "hot",
    "encoder",
    "also",
    "create",
    "class",
    "cfar",
    "helper",
    "going",
    "init",
    "setup",
    "images",
    "finally",
    "go",
    "ahead",
    "run",
    "code",
    "see",
    "looks",
    "like",
    "get",
    "fun",
    "part",
    "actually",
    "going",
    "start",
    "creating",
    "model",
    "actual",
    "neural",
    "network",
    "model",
    "let",
    "start",
    "creating",
    "one",
    "hot",
    "encoder",
    "going",
    "create",
    "going",
    "return",
    "vector",
    "coming",
    "values",
    "equal",
    "means",
    "10",
    "values",
    "10",
    "possible",
    "labels",
    "remember",
    "look",
    "labels",
    "number",
    "car",
    "one",
    "horse",
    "maybe",
    "kind",
    "bizarre",
    "horse",
    "equals",
    "zero",
    "car",
    "equals",
    "one",
    "plane",
    "equals",
    "two",
    "cat",
    "equals",
    "three",
    "cat",
    "plus",
    "car",
    "equals",
    "uh",
    "instead",
    "create",
    "numpy",
    "array",
    "zeros",
    "going",
    "10",
    "values",
    "10",
    "different",
    "values",
    "0",
    "1",
    "1",
    "means",
    "cat",
    "0",
    "means",
    "cat",
    "next",
    "line",
    "might",
    "one",
    "means",
    "car",
    "zero",
    "means",
    "car",
    "instead",
    "one",
    "output",
    "value",
    "0",
    "10",
    "10",
    "outputs",
    "values",
    "0",
    "one",
    "hot",
    "encoder",
    "going",
    "utilize",
    "code",
    "minute",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "next",
    "helpers",
    "helper",
    "functions",
    "going",
    "build",
    "working",
    "complicated",
    "python",
    "project",
    "dividing",
    "separate",
    "definitions",
    "classes",
    "important",
    "otherwise",
    "becomes",
    "really",
    "ungainly",
    "work",
    "let",
    "go",
    "ahead",
    "put",
    "next",
    "helper",
    "class",
    "lot",
    "class",
    "break",
    "let",
    "start",
    "oops",
    "put",
    "space",
    "right",
    "go",
    "little",
    "bit",
    "readable",
    "second",
    "space",
    "going",
    "create",
    "class",
    "cipher",
    "helper",
    "start",
    "initializing",
    "lot",
    "going",
    "let",
    "start",
    "init",
    "part",
    "uh",
    "self",
    "dot",
    "equals",
    "zero",
    "come",
    "little",
    "bit",
    "come",
    "back",
    "lower",
    "part",
    "want",
    "initialize",
    "training",
    "batches",
    "went",
    "like",
    "meta",
    "batch",
    "need",
    "meta",
    "batch",
    "need",
    "data",
    "batch",
    "one",
    "two",
    "three",
    "four",
    "five",
    "want",
    "testing",
    "batch",
    "self",
    "train",
    "batches",
    "gon",
    "na",
    "come",
    "make",
    "array",
    "different",
    "images",
    "course",
    "left",
    "test",
    "batch",
    "batch",
    "going",
    "initialize",
    "training",
    "images",
    "training",
    "labels",
    "also",
    "test",
    "images",
    "test",
    "labels",
    "initialize",
    "variables",
    "create",
    "another",
    "definition",
    "going",
    "set",
    "images",
    "let",
    "take",
    "look",
    "see",
    "going",
    "could",
    "put",
    "part",
    "init",
    "part",
    "since",
    "helper",
    "stuff",
    "breaking",
    "makes",
    "easier",
    "read",
    "also",
    "makes",
    "easier",
    "start",
    "executing",
    "different",
    "pieces",
    "see",
    "going",
    "way",
    "nice",
    "print",
    "statement",
    "say",
    "hey",
    "running",
    "going",
    "going",
    "set",
    "self",
    "training",
    "images",
    "point",
    "going",
    "go",
    "numpy",
    "array",
    "v",
    "stack",
    "going",
    "load",
    "case",
    "data",
    "train",
    "batches",
    "points",
    "right",
    "going",
    "go",
    "one",
    "files",
    "one",
    "data",
    "sets",
    "file",
    "anymore",
    "brought",
    "data",
    "batch",
    "one",
    "points",
    "actual",
    "data",
    "self",
    "training",
    "images",
    "going",
    "stack",
    "numpy",
    "array",
    "always",
    "nice",
    "get",
    "training",
    "length",
    "total",
    "number",
    "self",
    "training",
    "images",
    "going",
    "take",
    "self",
    "training",
    "images",
    "let",
    "switch",
    "marker",
    "colors",
    "getting",
    "little",
    "much",
    "markers",
    "oops",
    "go",
    "bring",
    "marker",
    "change",
    "see",
    "little",
    "better",
    "point",
    "look",
    "familiar",
    "see",
    "well",
    "wanted",
    "uh",
    "look",
    "want",
    "look",
    "images",
    "matplot",
    "library",
    "reshape",
    "thing",
    "taking",
    "self",
    "training",
    "images",
    "based",
    "training",
    "length",
    "total",
    "number",
    "images",
    "stacked",
    "together",
    "one",
    "large",
    "file",
    "images",
    "going",
    "take",
    "look",
    "three",
    "video",
    "cameras",
    "displaying",
    "32",
    "32",
    "going",
    "switch",
    "around",
    "images",
    "stays",
    "place",
    "32",
    "32",
    "three",
    "last",
    "three",
    "different",
    "values",
    "color",
    "course",
    "want",
    "go",
    "ahead",
    "run",
    "say",
    "divide",
    "255",
    "earlier",
    "brings",
    "data",
    "zero",
    "one",
    "turning",
    "zero",
    "one",
    "array",
    "uh",
    "pictures",
    "32",
    "32",
    "three",
    "going",
    "take",
    "self",
    "training",
    "labels",
    "going",
    "pump",
    "one",
    "hot",
    "encoder",
    "made",
    "going",
    "stack",
    "together",
    "converting",
    "array",
    "goes",
    "uh",
    "instead",
    "horse",
    "equals",
    "one",
    "dog",
    "equals",
    "two",
    "horse",
    "plus",
    "dog",
    "would",
    "equal",
    "three",
    "would",
    "cat",
    "going",
    "know",
    "array",
    "10",
    "one",
    "0",
    "want",
    "go",
    "ahead",
    "set",
    "test",
    "images",
    "labels",
    "going",
    "see",
    "thing",
    "rest",
    "changed",
    "colors",
    "right",
    "different",
    "training",
    "set",
    "uh",
    "going",
    "stack",
    "different",
    "images",
    "uh",
    "going",
    "get",
    "length",
    "know",
    "many",
    "images",
    "certainly",
    "could",
    "add",
    "hand",
    "nice",
    "let",
    "computer",
    "especially",
    "ever",
    "changes",
    "end",
    "using",
    "data",
    "reshape",
    "transpose",
    "also",
    "one",
    "hot",
    "encoder",
    "thing",
    "training",
    "images",
    "test",
    "images",
    "format",
    "definition",
    "sets",
    "images",
    "next",
    "step",
    "go",
    "ahead",
    "batch",
    "next",
    "batch",
    "let",
    "another",
    "breakout",
    "batches",
    "really",
    "important",
    "understand",
    "tends",
    "throw",
    "little",
    "loop",
    "working",
    "tensorflow",
    "cross",
    "lot",
    "data",
    "coming",
    "remember",
    "like",
    "10",
    "000",
    "photos",
    "let",
    "put",
    "10",
    "000",
    "want",
    "run",
    "10",
    "000",
    "want",
    "break",
    "batch",
    "sizes",
    "also",
    "remember",
    "number",
    "photos",
    "case",
    "length",
    "test",
    "whatever",
    "number",
    "also",
    "32",
    "32",
    "looking",
    "batch",
    "size",
    "want",
    "change",
    "10",
    "000",
    "batch",
    "case",
    "think",
    "going",
    "batches",
    "hundred",
    "want",
    "look",
    "100",
    "first",
    "hundred",
    "photos",
    "remember",
    "set",
    "equal",
    "zero",
    "uh",
    "looking",
    "going",
    "create",
    "x",
    "gon",
    "na",
    "get",
    "next",
    "batch",
    "initialize",
    "already",
    "initialized",
    "zero",
    "gon",
    "na",
    "look",
    "x",
    "zero",
    "batch",
    "size",
    "set",
    "one",
    "hundred",
    "first",
    "100",
    "images",
    "going",
    "reshape",
    "important",
    "let",
    "data",
    "know",
    "looking",
    "100",
    "32",
    "32",
    "already",
    "formatted",
    "32",
    "32",
    "sets",
    "everything",
    "correctly",
    "x",
    "data",
    "correct",
    "order",
    "correct",
    "shape",
    "like",
    "x",
    "labels",
    "training",
    "labels",
    "go",
    "0",
    "batch",
    "size",
    "case",
    "sell",
    "fi",
    "plus",
    "batch",
    "size",
    "self",
    "going",
    "keep",
    "changing",
    "finally",
    "increment",
    "self",
    "zero",
    "next",
    "time",
    "call",
    "going",
    "get",
    "next",
    "batch",
    "size",
    "basically",
    "x",
    "x",
    "photograph",
    "data",
    "coming",
    "label",
    "course",
    "labeled",
    "one",
    "hot",
    "encoder",
    "remember",
    "correctly",
    "say",
    "horse",
    "equal",
    "zero",
    "would",
    "um",
    "one",
    "zero",
    "position",
    "since",
    "horse",
    "everything",
    "else",
    "would",
    "zero",
    "let",
    "put",
    "lines",
    "go",
    "array",
    "hard",
    "see",
    "array",
    "let",
    "go",
    "ahead",
    "take",
    "going",
    "finish",
    "loading",
    "since",
    "class",
    "armed",
    "um",
    "uh",
    "setup",
    "let",
    "go",
    "ahead",
    "load",
    "going",
    "create",
    "variable",
    "ch",
    "cfar",
    "helper",
    "going",
    "images",
    "could",
    "put",
    "setup",
    "images",
    "init",
    "breaking",
    "two",
    "parts",
    "makes",
    "much",
    "readable",
    "also",
    "work",
    "reasons",
    "far",
    "setup",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "says",
    "setting",
    "training",
    "images",
    "labels",
    "setting",
    "test",
    "images",
    "one",
    "reasons",
    "broke",
    "testing",
    "actually",
    "print",
    "statements",
    "telling",
    "going",
    "really",
    "nice",
    "uh",
    "good",
    "job",
    "setup",
    "like",
    "way",
    "broken",
    "back",
    "one",
    "quick",
    "note",
    "want",
    "remember",
    "batch",
    "set",
    "next",
    "batch",
    "run",
    "uh",
    "batch",
    "equals",
    "ch",
    "next",
    "batch",
    "100",
    "going",
    "use",
    "100",
    "size",
    "come",
    "back",
    "going",
    "use",
    "remember",
    "part",
    "code",
    "going",
    "using",
    "minute",
    "definition",
    "made",
    "ready",
    "create",
    "model",
    "first",
    "thing",
    "want",
    "want",
    "import",
    "tensorflow",
    "tf",
    "go",
    "ahead",
    "run",
    "loaded",
    "see",
    "got",
    "warning",
    "making",
    "changes",
    "always",
    "growing",
    "going",
    "depreciating",
    "one",
    "values",
    "float",
    "64",
    "float",
    "type",
    "treated",
    "np",
    "float",
    "uh",
    "nothing",
    "really",
    "worry",
    "even",
    "affect",
    "working",
    "set",
    "stuff",
    "255",
    "value",
    "zero",
    "one",
    "keep",
    "mind",
    "zero",
    "one",
    "value",
    "converted",
    "255",
    "still",
    "float",
    "value",
    "easily",
    "work",
    "either",
    "numpy",
    "float",
    "64",
    "numpy",
    "type",
    "float",
    "matter",
    "one",
    "goes",
    "depreciation",
    "would",
    "affect",
    "code",
    "tensorflow",
    "uh",
    "go",
    "ahead",
    "increase",
    "size",
    "moment",
    "get",
    "better",
    "view",
    "um",
    "typing",
    "uh",
    "going",
    "set",
    "couple",
    "placeholders",
    "going",
    "set",
    "x",
    "equals",
    "tf",
    "placeholder",
    "tf",
    "float",
    "32",
    "talked",
    "float",
    "64",
    "versus",
    "numpy",
    "float",
    "actually",
    "going",
    "keep",
    "float32",
    "significant",
    "number",
    "decimals",
    "working",
    "since",
    "placeholder",
    "going",
    "set",
    "shape",
    "equal",
    "set",
    "equal",
    "none",
    "point",
    "holding",
    "place",
    "setting",
    "run",
    "batches",
    "first",
    "value",
    "32",
    "32",
    "3",
    "reshaped",
    "data",
    "fit",
    "true",
    "equals",
    "placeholder",
    "tf",
    "float",
    "32",
    "shape",
    "equals",
    "none",
    "comma",
    "10",
    "10",
    "10",
    "different",
    "labels",
    "array",
    "let",
    "create",
    "one",
    "placeholder",
    "call",
    "hold",
    "prob",
    "hold",
    "probability",
    "going",
    "use",
    "shape",
    "anything",
    "placeholder",
    "call",
    "drop",
    "remember",
    "theory",
    "drop",
    "many",
    "nodes",
    "looking",
    "different",
    "values",
    "going",
    "helps",
    "decrease",
    "bias",
    "need",
    "go",
    "ahead",
    "put",
    "placeholder",
    "also",
    "run",
    "loaded",
    "three",
    "different",
    "placeholders",
    "since",
    "tensorflow",
    "use",
    "keras",
    "automatically",
    "tensorflow",
    "direct",
    "cross",
    "sits",
    "tensorflow",
    "going",
    "go",
    "ahead",
    "create",
    "helper",
    "functions",
    "going",
    "create",
    "something",
    "help",
    "us",
    "initialize",
    "weights",
    "initialize",
    "bias",
    "remember",
    "layer",
    "bias",
    "going",
    "going",
    "go",
    "ahead",
    "work",
    "conversional",
    "2d",
    "max",
    "pool",
    "pooling",
    "layer",
    "convolutional",
    "layer",
    "normal",
    "full",
    "layer",
    "going",
    "go",
    "ahead",
    "put",
    "definitions",
    "let",
    "see",
    "looks",
    "like",
    "code",
    "also",
    "grab",
    "helper",
    "functions",
    "mnist",
    "uh",
    "nist",
    "setup",
    "let",
    "put",
    "tensorflow",
    "lot",
    "already",
    "going",
    "go",
    "ahead",
    "going",
    "create",
    "init",
    "weights",
    "one",
    "reasons",
    "actually",
    "start",
    "thinking",
    "going",
    "back",
    "end",
    "even",
    "though",
    "ways",
    "automation",
    "sometimes",
    "tweaked",
    "put",
    "setup",
    "going",
    "going",
    "recreate",
    "code",
    "let",
    "take",
    "look",
    "weights",
    "comes",
    "going",
    "shape",
    "comes",
    "going",
    "random",
    "number",
    "going",
    "go",
    "ahead",
    "knit",
    "random",
    "numbers",
    "based",
    "shape",
    "standard",
    "deviation",
    "kind",
    "fun",
    "way",
    "tf",
    "variable",
    "init",
    "random",
    "distribution",
    "creating",
    "random",
    "distribution",
    "weights",
    "might",
    "change",
    "might",
    "higher",
    "standard",
    "deviation",
    "cases",
    "actually",
    "load",
    "preset",
    "weights",
    "pretty",
    "rare",
    "usually",
    "testing",
    "another",
    "model",
    "something",
    "like",
    "want",
    "see",
    "weights",
    "configure",
    "remember",
    "bias",
    "need",
    "go",
    "ahead",
    "initialize",
    "bias",
    "constant",
    "case",
    "using",
    "lot",
    "times",
    "bias",
    "put",
    "one",
    "weights",
    "add",
    "going",
    "set",
    "point",
    "one",
    "uh",
    "want",
    "return",
    "convolutional",
    "2d",
    "case",
    "neural",
    "network",
    "uh",
    "would",
    "layer",
    "going",
    "con",
    "2d",
    "taking",
    "data",
    "coming",
    "going",
    "filter",
    "strides",
    "remember",
    "correctly",
    "strides",
    "came",
    "image",
    "look",
    "picture",
    "maybe",
    "stride",
    "one",
    "look",
    "picture",
    "continue",
    "look",
    "different",
    "filters",
    "going",
    "thing",
    "data",
    "coming",
    "32",
    "32",
    "3",
    "want",
    "change",
    "three",
    "dimensions",
    "going",
    "reformat",
    "two",
    "dimensions",
    "going",
    "take",
    "number",
    "combine",
    "32",
    "important",
    "layer",
    "reducing",
    "data",
    "using",
    "different",
    "means",
    "connects",
    "going",
    "jump",
    "one",
    "goes",
    "convolutional",
    "layer",
    "kind",
    "preformatting",
    "setup",
    "actual",
    "convolution",
    "layer",
    "goes",
    "see",
    "init",
    "weights",
    "shape",
    "knit",
    "bias",
    "shape",
    "three",
    "three",
    "different",
    "three",
    "return",
    "tfnn",
    "relu",
    "convention",
    "2d",
    "convolutional",
    "feeding",
    "right",
    "using",
    "part",
    "course",
    "input",
    "x",
    "plus",
    "b",
    "bias",
    "quite",
    "mouthful",
    "two",
    "keys",
    "creating",
    "convolutional",
    "layers",
    "convolutional",
    "2d",
    "coming",
    "convolutional",
    "layer",
    "steps",
    "creates",
    "filters",
    "saw",
    "course",
    "pooling",
    "uh",
    "time",
    "run",
    "convectional",
    "layer",
    "want",
    "pull",
    "data",
    "remember",
    "correctly",
    "pool",
    "side",
    "let",
    "get",
    "rid",
    "marks",
    "getting",
    "little",
    "crazy",
    "fact",
    "let",
    "go",
    "ahead",
    "jump",
    "back",
    "slide",
    "let",
    "take",
    "look",
    "slide",
    "uh",
    "image",
    "coming",
    "create",
    "convolutional",
    "layer",
    "filters",
    "remember",
    "filters",
    "go",
    "um",
    "know",
    "filter",
    "coming",
    "looks",
    "four",
    "boxes",
    "step",
    "let",
    "say",
    "step",
    "two",
    "goes",
    "four",
    "boxes",
    "next",
    "step",
    "uh",
    "convolutional",
    "layer",
    "generate",
    "convolutional",
    "layers",
    "use",
    "uh",
    "relu",
    "function",
    "functions",
    "though",
    "relu",
    "one",
    "works",
    "best",
    "least",
    "far",
    "sure",
    "change",
    "pooling",
    "remember",
    "correctly",
    "pooling",
    "max",
    "filter",
    "coming",
    "multiplication",
    "one",
    "maybe",
    "two",
    "another",
    "one",
    "three",
    "three",
    "max",
    "create",
    "array",
    "would",
    "three",
    "max",
    "two",
    "whatever",
    "goes",
    "pooling",
    "going",
    "pooling",
    "uh",
    "reducing",
    "data",
    "reducing",
    "small",
    "finally",
    "going",
    "flatten",
    "single",
    "array",
    "goes",
    "fully",
    "connected",
    "layer",
    "see",
    "code",
    "right",
    "going",
    "create",
    "normal",
    "full",
    "layer",
    "point",
    "going",
    "take",
    "pooling",
    "layer",
    "go",
    "kind",
    "flattening",
    "process",
    "fed",
    "full",
    "different",
    "layers",
    "going",
    "input",
    "size",
    "see",
    "input",
    "layer",
    "get",
    "shape",
    "going",
    "get",
    "shape",
    "whatever",
    "coming",
    "uh",
    "input",
    "size",
    "initial",
    "weights",
    "also",
    "based",
    "input",
    "layer",
    "coming",
    "input",
    "size",
    "based",
    "input",
    "layer",
    "shape",
    "going",
    "already",
    "use",
    "shape",
    "already",
    "size",
    "coming",
    "course",
    "uh",
    "make",
    "sure",
    "knit",
    "bias",
    "always",
    "put",
    "bias",
    "based",
    "size",
    "return",
    "input",
    "layer",
    "w",
    "plus",
    "b",
    "normal",
    "full",
    "layer",
    "means",
    "right",
    "going",
    "return",
    "lot",
    "steps",
    "went",
    "let",
    "go",
    "ahead",
    "run",
    "loaded",
    "let",
    "go",
    "ahead",
    "uh",
    "create",
    "layers",
    "let",
    "see",
    "looks",
    "like",
    "done",
    "heavy",
    "lifting",
    "everything",
    "uh",
    "get",
    "easy",
    "part",
    "let",
    "go",
    "ahead",
    "create",
    "layers",
    "create",
    "convolution",
    "layer",
    "one",
    "two",
    "two",
    "different",
    "convolutional",
    "layers",
    "take",
    "flatten",
    "create",
    "reshape",
    "pooling",
    "reshape",
    "full",
    "uh",
    "layer",
    "end",
    "let",
    "start",
    "creating",
    "first",
    "convolutional",
    "layer",
    "come",
    "let",
    "run",
    "real",
    "quick",
    "want",
    "notice",
    "3",
    "32",
    "important",
    "coming",
    "convolutional",
    "layer",
    "three",
    "different",
    "channels",
    "32",
    "pixels",
    "four",
    "four",
    "play",
    "filter",
    "size",
    "remember",
    "filter",
    "image",
    "filter",
    "slowly",
    "steps",
    "filters",
    "image",
    "depending",
    "step",
    "particular",
    "setup",
    "4",
    "4",
    "fine",
    "work",
    "pretty",
    "good",
    "size",
    "image",
    "course",
    "end",
    "convolutional",
    "layer",
    "set",
    "also",
    "need",
    "pull",
    "see",
    "pooling",
    "automatically",
    "set",
    "would",
    "see",
    "different",
    "shape",
    "based",
    "coming",
    "max",
    "toolbar",
    "2x2",
    "put",
    "convolutional",
    "1",
    "created",
    "convolutional",
    "layer",
    "created",
    "goes",
    "right",
    "back",
    "right",
    "see",
    "x",
    "coming",
    "knows",
    "look",
    "first",
    "model",
    "set",
    "data",
    "accordingly",
    "set",
    "matches",
    "went",
    "ahead",
    "ran",
    "already",
    "think",
    "read",
    "let",
    "go",
    "run",
    "going",
    "one",
    "layer",
    "let",
    "go",
    "ahead",
    "second",
    "layer",
    "uh",
    "call",
    "convo",
    "also",
    "convolutional",
    "layer",
    "see",
    "feeding",
    "convolutional",
    "one",
    "pooling",
    "goes",
    "convolutional",
    "one",
    "convolutional",
    "one",
    "pooling",
    "convolutional",
    "one",
    "pooling",
    "convolutional",
    "two",
    "convolutional",
    "two",
    "convolutional",
    "2",
    "pooling",
    "go",
    "ahead",
    "take",
    "run",
    "variables",
    "loaded",
    "memory",
    "flattened",
    "layer",
    "let",
    "go",
    "ahead",
    "since",
    "64",
    "coming",
    "four",
    "four",
    "going",
    "let",
    "eight",
    "eight",
    "let",
    "going",
    "flat",
    "layer",
    "many",
    "bits",
    "coming",
    "flat",
    "layer",
    "reshape",
    "reshape",
    "convo",
    "2",
    "pooling",
    "feed",
    "combo",
    "2",
    "pulling",
    "going",
    "set",
    "single",
    "layer",
    "4096",
    "size",
    "means",
    "go",
    "ahead",
    "run",
    "created",
    "variable",
    "convo",
    "flat",
    "first",
    "full",
    "layer",
    "final",
    "neural",
    "network",
    "flat",
    "layer",
    "going",
    "going",
    "use",
    "relu",
    "setup",
    "neural",
    "network",
    "evaluation",
    "notice",
    "going",
    "create",
    "first",
    "full",
    "layer",
    "normal",
    "full",
    "layer",
    "definition",
    "created",
    "creating",
    "normal",
    "full",
    "layer",
    "input",
    "data",
    "comes",
    "right",
    "goes",
    "right",
    "convo",
    "flat",
    "tells",
    "big",
    "data",
    "going",
    "come",
    "going",
    "10",
    "24",
    "big",
    "layer",
    "coming",
    "go",
    "ahead",
    "run",
    "full",
    "layer",
    "one",
    "full",
    "layer",
    "one",
    "want",
    "also",
    "define",
    "full",
    "one",
    "dropout",
    "go",
    "full",
    "layer",
    "one",
    "comes",
    "keep",
    "probability",
    "equals",
    "whole",
    "probability",
    "remember",
    "created",
    "earlier",
    "full",
    "layer",
    "one",
    "coming",
    "going",
    "backwards",
    "training",
    "data",
    "training",
    "every",
    "weight",
    "training",
    "percentage",
    "time",
    "helps",
    "get",
    "rid",
    "bias",
    "let",
    "go",
    "ahead",
    "run",
    "finally",
    "go",
    "ahead",
    "create",
    "predict",
    "going",
    "equal",
    "normal",
    "full",
    "one",
    "drop",
    "10",
    "10",
    "labels",
    "neural",
    "network",
    "could",
    "added",
    "additional",
    "layers",
    "would",
    "another",
    "option",
    "play",
    "also",
    "play",
    "instead",
    "10",
    "24",
    "use",
    "numbers",
    "way",
    "sets",
    "coming",
    "going",
    "next",
    "one",
    "going",
    "one",
    "layer",
    "one",
    "layer",
    "drop",
    "see",
    "another",
    "layer",
    "really",
    "easy",
    "feed",
    "full",
    "one",
    "drop",
    "full",
    "layer",
    "two",
    "full",
    "layer",
    "two",
    "dropout",
    "would",
    "full",
    "air",
    "2",
    "feed",
    "switch",
    "prediction",
    "right",
    "great",
    "particular",
    "data",
    "set",
    "tried",
    "true",
    "know",
    "work",
    "type",
    "predict",
    "run",
    "see",
    "tensor",
    "object",
    "uh",
    "shape",
    "question",
    "mark",
    "10",
    "type",
    "32",
    "quick",
    "way",
    "double",
    "check",
    "working",
    "got",
    "done",
    "setup",
    "way",
    "predict",
    "want",
    "go",
    "ahead",
    "apply",
    "loss",
    "function",
    "make",
    "sure",
    "set",
    "create",
    "optimizer",
    "trainer",
    "optimizer",
    "create",
    "variable",
    "initialize",
    "global",
    "tf",
    "variables",
    "dive",
    "um",
    "loss",
    "function",
    "let",
    "point",
    "one",
    "quick",
    "thing",
    "kind",
    "rehab",
    "couple",
    "things",
    "playing",
    "setups",
    "pointed",
    "change",
    "4",
    "4",
    "use",
    "different",
    "numbers",
    "change",
    "outcome",
    "depending",
    "numbers",
    "use",
    "huge",
    "impact",
    "well",
    "model",
    "fits",
    "1024",
    "also",
    "also",
    "another",
    "number",
    "continue",
    "raise",
    "number",
    "get",
    "possibly",
    "better",
    "fit",
    "might",
    "overfit",
    "lower",
    "number",
    "use",
    "less",
    "resources",
    "generally",
    "want",
    "use",
    "exponential",
    "growth",
    "exponential",
    "2",
    "4",
    "8",
    "16",
    "case",
    "next",
    "one",
    "would",
    "5",
    "use",
    "number",
    "would",
    "ideal",
    "numbers",
    "look",
    "data",
    "next",
    "step",
    "need",
    "also",
    "create",
    "way",
    "tracking",
    "good",
    "model",
    "going",
    "call",
    "loss",
    "function",
    "going",
    "create",
    "cross",
    "entropy",
    "loss",
    "function",
    "discuss",
    "exactly",
    "let",
    "take",
    "look",
    "see",
    "feeding",
    "going",
    "feed",
    "labels",
    "true",
    "labels",
    "prediction",
    "labels",
    "coming",
    "two",
    "different",
    "variables",
    "sending",
    "two",
    "different",
    "probability",
    "distributions",
    "one",
    "know",
    "true",
    "think",
    "going",
    "function",
    "right",
    "talk",
    "cross",
    "entropy",
    "information",
    "theory",
    "cross",
    "entropy",
    "two",
    "probability",
    "distributions",
    "underlying",
    "set",
    "events",
    "measures",
    "average",
    "number",
    "bits",
    "needed",
    "identify",
    "event",
    "drawn",
    "set",
    "mouthful",
    "uh",
    "really",
    "looking",
    "amount",
    "error",
    "many",
    "correct",
    "many",
    "um",
    "incorrect",
    "much",
    "matches",
    "going",
    "look",
    "going",
    "look",
    "average",
    "mean",
    "reduced",
    "mean",
    "means",
    "looking",
    "average",
    "error",
    "next",
    "step",
    "going",
    "take",
    "error",
    "want",
    "know",
    "cross",
    "entropy",
    "loss",
    "function",
    "much",
    "loss",
    "going",
    "part",
    "train",
    "model",
    "know",
    "loss",
    "training",
    "feed",
    "back",
    "back",
    "propagation",
    "setup",
    "want",
    "go",
    "ahead",
    "optimize",
    "optimizer",
    "going",
    "create",
    "optimizer",
    "using",
    "atom",
    "optimizer",
    "remember",
    "lot",
    "different",
    "ways",
    "optimizing",
    "data",
    "atoms",
    "popular",
    "used",
    "uh",
    "optimizer",
    "going",
    "equal",
    "tf",
    "train",
    "atom",
    "optimizer",
    "remember",
    "learning",
    "rate",
    "let",
    "pop",
    "back",
    "learning",
    "rate",
    "weights",
    "weights",
    "different",
    "nodes",
    "coming",
    "node",
    "coming",
    "weights",
    "error",
    "prop",
    "sent",
    "back",
    "reverse",
    "neural",
    "network",
    "take",
    "error",
    "adjust",
    "weights",
    "based",
    "different",
    "formulas",
    "case",
    "atom",
    "formula",
    "using",
    "want",
    "adjust",
    "completely",
    "want",
    "change",
    "weight",
    "exactly",
    "fits",
    "data",
    "coming",
    "made",
    "kind",
    "adjustment",
    "going",
    "biased",
    "whatever",
    "last",
    "data",
    "sent",
    "instead",
    "going",
    "multiply",
    "make",
    "small",
    "shift",
    "weight",
    "delta",
    "w",
    "actual",
    "delta",
    "w",
    "full",
    "change",
    "going",
    "compute",
    "atom",
    "want",
    "go",
    "ahead",
    "train",
    "training",
    "set",
    "training",
    "variable",
    "function",
    "going",
    "equal",
    "optimizer",
    "minimize",
    "cross",
    "entropy",
    "make",
    "sure",
    "go",
    "ahead",
    "run",
    "loaded",
    "almost",
    "ready",
    "train",
    "model",
    "need",
    "create",
    "one",
    "variable",
    "going",
    "create",
    "variable",
    "initialize",
    "global",
    "tf",
    "variables",
    "look",
    "tf",
    "global",
    "variable",
    "initializer",
    "tensorflow",
    "object",
    "goes",
    "looks",
    "different",
    "setup",
    "going",
    "tensorflow",
    "initializes",
    "variables",
    "uh",
    "kind",
    "like",
    "magic",
    "wand",
    "hidden",
    "back",
    "end",
    "tensorflow",
    "need",
    "know",
    "initialization",
    "operation",
    "run",
    "setup",
    "going",
    "go",
    "ahead",
    "run",
    "piece",
    "code",
    "going",
    "go",
    "ahead",
    "train",
    "data",
    "let",
    "run",
    "loaded",
    "going",
    "go",
    "ahead",
    "run",
    "model",
    "creating",
    "graph",
    "session",
    "graph",
    "session",
    "tensorflow",
    "term",
    "see",
    "coming",
    "one",
    "things",
    "throws",
    "always",
    "think",
    "graphics",
    "spark",
    "graph",
    "general",
    "graphing",
    "uh",
    "talk",
    "graph",
    "session",
    "going",
    "go",
    "ahead",
    "run",
    "model",
    "let",
    "go",
    "ahead",
    "walk",
    "uh",
    "going",
    "let",
    "paste",
    "data",
    "go",
    "going",
    "start",
    "tf",
    "session",
    "cess",
    "actual",
    "tf",
    "session",
    "created",
    "uh",
    "right",
    "tf",
    "uh",
    "session",
    "session",
    "creating",
    "going",
    "run",
    "tf",
    "global",
    "variable",
    "initializer",
    "right",
    "bat",
    "initializing",
    "variables",
    "range",
    "going",
    "remember",
    "500",
    "going",
    "break",
    "data",
    "going",
    "batch",
    "500",
    "points",
    "created",
    "session",
    "run",
    "going",
    "tf",
    "session",
    "session",
    "right",
    "created",
    "variable",
    "session",
    "going",
    "run",
    "going",
    "go",
    "ahead",
    "initialize",
    "tf",
    "global",
    "variables",
    "initializer",
    "created",
    "initializes",
    "session",
    "next",
    "thing",
    "going",
    "going",
    "go",
    "range",
    "500",
    "batch",
    "equals",
    "remember",
    "correctly",
    "loading",
    "um",
    "100",
    "pictures",
    "time",
    "going",
    "loop",
    "500",
    "times",
    "literally",
    "uh",
    "500",
    "times",
    "100",
    "50",
    "50",
    "000",
    "pictures",
    "going",
    "process",
    "right",
    "first",
    "process",
    "going",
    "session",
    "run",
    "going",
    "take",
    "train",
    "created",
    "train",
    "variable",
    "optimizer",
    "going",
    "feed",
    "dictionary",
    "feed",
    "dictionary",
    "created",
    "x",
    "equals",
    "batch",
    "zero",
    "coming",
    "true",
    "batch",
    "one",
    "hold",
    "probability",
    "point",
    "five",
    "keep",
    "track",
    "going",
    "gon",
    "na",
    "every",
    "100",
    "steps",
    "going",
    "run",
    "print",
    "currently",
    "step",
    "format",
    "accuracy",
    "going",
    "look",
    "matches",
    "equals",
    "tf",
    "argument",
    "prediction",
    "1",
    "max",
    "true",
    "comma",
    "going",
    "look",
    "many",
    "matches",
    "acc",
    "uh",
    "going",
    "take",
    "matches",
    "many",
    "matches",
    "creates",
    "generates",
    "chart",
    "going",
    "convert",
    "float",
    "tfcast",
    "want",
    "know",
    "average",
    "want",
    "know",
    "average",
    "accuracy",
    "go",
    "ahead",
    "print",
    "print",
    "session",
    "run",
    "accuracy",
    "feed",
    "dictionary",
    "takes",
    "prints",
    "accuracy",
    "let",
    "go",
    "ahead",
    "take",
    "oops",
    "screen",
    "let",
    "go",
    "ahead",
    "take",
    "let",
    "run",
    "going",
    "take",
    "little",
    "bit",
    "run",
    "uh",
    "let",
    "see",
    "happens",
    "old",
    "laptop",
    "see",
    "current",
    "uh",
    "currently",
    "step",
    "zero",
    "takes",
    "little",
    "bit",
    "get",
    "accuracy",
    "take",
    "moment",
    "run",
    "see",
    "step",
    "0",
    "accuracy",
    "running",
    "go",
    "ahead",
    "need",
    "watch",
    "run",
    "way",
    "accuracy",
    "going",
    "change",
    "little",
    "bit",
    "actually",
    "lost",
    "accuracy",
    "step",
    "two",
    "see",
    "comes",
    "let",
    "come",
    "back",
    "run",
    "way",
    "see",
    "different",
    "steps",
    "come",
    "actually",
    "reading",
    "backwards",
    "way",
    "works",
    "closer",
    "get",
    "one",
    "accuracy",
    "see",
    "gone",
    "point",
    "one",
    "point",
    "three",
    "nine",
    "um",
    "go",
    "ahead",
    "pause",
    "come",
    "back",
    "see",
    "happens",
    "done",
    "full",
    "run",
    "right",
    "prepared",
    "meal",
    "got",
    "oven",
    "pulled",
    "finished",
    "dish",
    "ever",
    "watched",
    "old",
    "cooking",
    "shows",
    "let",
    "discuss",
    "little",
    "bit",
    "accuracy",
    "going",
    "interpret",
    "done",
    "couple",
    "things",
    "first",
    "defined",
    "accuracy",
    "um",
    "reason",
    "got",
    "backwards",
    "loss",
    "accuracy",
    "loss",
    "get",
    "graph",
    "looks",
    "like",
    "goes",
    "oops",
    "way",
    "go",
    "get",
    "graph",
    "curves",
    "like",
    "accuracy",
    "get",
    "graph",
    "curves",
    "good",
    "case",
    "uh",
    "one",
    "supposed",
    "really",
    "good",
    "accuracy",
    "means",
    "gets",
    "close",
    "one",
    "never",
    "crosses",
    "one",
    "accuracy",
    "one",
    "phenomenal",
    "fact",
    "pretty",
    "much",
    "know",
    "unheard",
    "thing",
    "loss",
    "loss",
    "zero",
    "also",
    "unheard",
    "zero",
    "actually",
    "axis",
    "right",
    "go",
    "interpret",
    "know",
    "looking",
    "go",
    "oh",
    "uh",
    "51",
    "50",
    "percentage",
    "let",
    "put",
    "percentage",
    "uh",
    "log",
    "rhythmic",
    "means",
    "twice",
    "good",
    "point",
    "one",
    "uh",
    "see",
    "point",
    "four",
    "twice",
    "good",
    "point",
    "two",
    "real",
    "way",
    "convert",
    "percentage",
    "really",
    "ca",
    "say",
    "direct",
    "percentage",
    "conversion",
    "though",
    "head",
    "give",
    "percentage",
    "uh",
    "might",
    "look",
    "uh",
    "50",
    "guessing",
    "equals",
    "fifty",
    "percent",
    "roughly",
    "equals",
    "point",
    "one",
    "started",
    "top",
    "remember",
    "top",
    "point",
    "one",
    "oh",
    "two",
    "eight",
    "accuracy",
    "fifty",
    "percent",
    "seventy",
    "five",
    "percent",
    "point",
    "two",
    "quote",
    "numbers",
    "work",
    "way",
    "say",
    "pretty",
    "much",
    "saying",
    "100",
    "anywhere",
    "go",
    "look",
    "let",
    "go",
    "remove",
    "drawings",
    "magic",
    "number",
    "really",
    "want",
    "whole",
    "thing",
    "remember",
    "accuracy",
    "looking",
    "loss",
    "looking",
    "way",
    "know",
    "instead",
    "high",
    "want",
    "low",
    "uh",
    "accuracy",
    "pretty",
    "valid",
    "means",
    "pretty",
    "solid",
    "get",
    "direct",
    "correlation",
    "looking",
    "numbers",
    "see",
    "finished",
    "model",
    "still",
    "good",
    "look",
    "ran",
    "end",
    "remember",
    "lot",
    "randomness",
    "goes",
    "see",
    "weights",
    "got",
    "little",
    "better",
    "fine",
    "find",
    "comes",
    "little",
    "bit",
    "better",
    "worse",
    "depending",
    "randomness",
    "gone",
    "whole",
    "model",
    "created",
    "trained",
    "model",
    "also",
    "gone",
    "every",
    "100th",
    "run",
    "test",
    "model",
    "see",
    "accurate",
    "start",
    "course",
    "fundamentals",
    "neural",
    "network",
    "popular",
    "neural",
    "networks",
    "important",
    "know",
    "framework",
    "going",
    "looking",
    "specifically",
    "touch",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "rn",
    "work",
    "one",
    "big",
    "things",
    "rnns",
    "call",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "look",
    "going",
    "using",
    "use",
    "case",
    "uh",
    "study",
    "going",
    "cross",
    "tensorflow",
    "cross",
    "python",
    "module",
    "neural",
    "networks",
    "deep",
    "learning",
    "call",
    "long",
    "short",
    "term",
    "memory",
    "lstm",
    "use",
    "use",
    "case",
    "implement",
    "lstm",
    "keras",
    "see",
    "lstm",
    "basically",
    "rnn",
    "network",
    "get",
    "use",
    "case",
    "always",
    "favorite",
    "part",
    "dive",
    "going",
    "take",
    "look",
    "rnn",
    "introduction",
    "rnn",
    "know",
    "google",
    "auto",
    "complete",
    "feature",
    "predicts",
    "rest",
    "words",
    "user",
    "typing",
    "love",
    "autocomplete",
    "feature",
    "typing",
    "away",
    "saves",
    "lot",
    "time",
    "kind",
    "hit",
    "enter",
    "key",
    "auto",
    "fills",
    "everything",
    "type",
    "much",
    "well",
    "first",
    "collection",
    "large",
    "volumes",
    "frequently",
    "occurring",
    "consecutive",
    "words",
    "fed",
    "recurrent",
    "neural",
    "network",
    "analysis",
    "data",
    "finding",
    "sequence",
    "words",
    "occurring",
    "frequently",
    "builds",
    "model",
    "predict",
    "next",
    "word",
    "sentence",
    "google",
    "best",
    "food",
    "eat",
    "loss",
    "guessing",
    "going",
    "say",
    "los",
    "mexico",
    "going",
    "las",
    "vegas",
    "google",
    "search",
    "take",
    "look",
    "say",
    "hey",
    "common",
    "autocomplete",
    "going",
    "vegas",
    "usually",
    "gives",
    "three",
    "four",
    "different",
    "choices",
    "powerful",
    "tool",
    "saves",
    "us",
    "lot",
    "time",
    "especially",
    "google",
    "search",
    "even",
    "microsoft",
    "words",
    "people",
    "get",
    "mad",
    "auto",
    "fills",
    "wrong",
    "stuff",
    "know",
    "typing",
    "away",
    "helps",
    "autofill",
    "lot",
    "different",
    "packages",
    "standard",
    "feature",
    "used",
    "dive",
    "rnn",
    "getting",
    "depths",
    "let",
    "go",
    "ahead",
    "talk",
    "neural",
    "network",
    "neural",
    "networks",
    "used",
    "deep",
    "learning",
    "consist",
    "different",
    "layers",
    "connected",
    "work",
    "structure",
    "functions",
    "human",
    "brain",
    "going",
    "see",
    "thread",
    "human",
    "human",
    "brain",
    "human",
    "thinking",
    "throughout",
    "deep",
    "learning",
    "way",
    "evaluate",
    "artificial",
    "intelligence",
    "anything",
    "like",
    "compare",
    "human",
    "function",
    "important",
    "note",
    "learns",
    "huge",
    "volumes",
    "data",
    "uses",
    "complex",
    "algorithm",
    "train",
    "neural",
    "net",
    "image",
    "pixels",
    "two",
    "different",
    "breeds",
    "dog",
    "one",
    "looks",
    "like",
    "nice",
    "floppy",
    "eared",
    "lab",
    "one",
    "german",
    "shepherd",
    "know",
    "wonderful",
    "breeds",
    "animals",
    "image",
    "goes",
    "input",
    "layer",
    "uh",
    "input",
    "layer",
    "might",
    "formatted",
    "point",
    "let",
    "know",
    "like",
    "know",
    "different",
    "pictures",
    "going",
    "different",
    "sizes",
    "different",
    "color",
    "content",
    "feed",
    "hidden",
    "layers",
    "pixels",
    "point",
    "data",
    "goes",
    "splits",
    "hidden",
    "layer",
    "goes",
    "another",
    "hidden",
    "layer",
    "goes",
    "output",
    "layer",
    "rnn",
    "changes",
    "going",
    "get",
    "straightforward",
    "propagation",
    "data",
    "like",
    "covered",
    "many",
    "tutorials",
    "finally",
    "output",
    "layer",
    "output",
    "layer",
    "two",
    "outputs",
    "one",
    "lights",
    "german",
    "shepherd",
    "another",
    "lights",
    "labrador",
    "identify",
    "dog",
    "breed",
    "set",
    "networks",
    "require",
    "memorizing",
    "past",
    "output",
    "forward",
    "propagation",
    "goes",
    "forward",
    "stuff",
    "see",
    "actually",
    "picture",
    "dressed",
    "suit",
    "worn",
    "suit",
    "years",
    "looking",
    "going",
    "change",
    "little",
    "bit",
    "cover",
    "let",
    "talk",
    "popular",
    "neural",
    "networks",
    "first",
    "feed",
    "forward",
    "neural",
    "network",
    "used",
    "general",
    "regression",
    "classification",
    "problems",
    "convolution",
    "neural",
    "network",
    "used",
    "image",
    "recognition",
    "deep",
    "neural",
    "network",
    "used",
    "acoustic",
    "modeling",
    "deep",
    "belief",
    "network",
    "used",
    "cancer",
    "detection",
    "recurrent",
    "neural",
    "network",
    "used",
    "speech",
    "recognition",
    "taking",
    "lot",
    "mixed",
    "around",
    "little",
    "bit",
    "used",
    "one",
    "thing",
    "mean",
    "ca",
    "used",
    "modeling",
    "generally",
    "field",
    "models",
    "generally",
    "used",
    "right",
    "talk",
    "feed",
    "forward",
    "neural",
    "network",
    "feed",
    "forward",
    "neural",
    "network",
    "information",
    "flows",
    "forward",
    "direction",
    "input",
    "nodes",
    "hidden",
    "layers",
    "output",
    "nodes",
    "cycles",
    "loops",
    "network",
    "see",
    "input",
    "layer",
    "talking",
    "goes",
    "straight",
    "forward",
    "hidden",
    "layers",
    "one",
    "connects",
    "connects",
    "next",
    "hidden",
    "layer",
    "connects",
    "output",
    "layer",
    "course",
    "nice",
    "simplified",
    "version",
    "predicted",
    "output",
    "refer",
    "input",
    "x",
    "lot",
    "times",
    "output",
    "decisions",
    "based",
    "current",
    "input",
    "memory",
    "past",
    "future",
    "scope",
    "recurrent",
    "neural",
    "network",
    "issues",
    "feed",
    "forward",
    "neural",
    "network",
    "one",
    "biggest",
    "issues",
    "scope",
    "memory",
    "time",
    "feed",
    "forward",
    "neural",
    "network",
    "know",
    "handle",
    "sequential",
    "data",
    "considers",
    "current",
    "input",
    "series",
    "things",
    "three",
    "points",
    "back",
    "affects",
    "happening",
    "output",
    "affects",
    "happening",
    "important",
    "whatever",
    "put",
    "output",
    "going",
    "affect",
    "next",
    "one",
    "feed",
    "forward",
    "look",
    "looks",
    "coming",
    "memorize",
    "previous",
    "inputs",
    "list",
    "inputs",
    "coming",
    "solution",
    "feed",
    "forward",
    "neural",
    "network",
    "see",
    "says",
    "recurrent",
    "neural",
    "network",
    "x",
    "bottom",
    "going",
    "h",
    "going",
    "feed",
    "forward",
    "right",
    "middle",
    "value",
    "c",
    "whole",
    "another",
    "process",
    "memorizing",
    "going",
    "hidden",
    "layers",
    "hidden",
    "layers",
    "produce",
    "data",
    "feed",
    "next",
    "one",
    "hidden",
    "layer",
    "might",
    "output",
    "goes",
    "output",
    "goes",
    "back",
    "next",
    "prediction",
    "coming",
    "allows",
    "handle",
    "sequential",
    "data",
    "considers",
    "current",
    "input",
    "also",
    "previously",
    "received",
    "inputs",
    "going",
    "look",
    "general",
    "drawings",
    "solutions",
    "also",
    "look",
    "applications",
    "rnn",
    "image",
    "captioning",
    "rnn",
    "used",
    "caption",
    "image",
    "analyzing",
    "activities",
    "present",
    "dog",
    "catching",
    "ball",
    "midair",
    "tough",
    "mean",
    "know",
    "lot",
    "stuff",
    "analyzes",
    "images",
    "dog",
    "image",
    "ball",
    "able",
    "add",
    "one",
    "feature",
    "actually",
    "catching",
    "ball",
    "midair",
    "time",
    "series",
    "prediction",
    "time",
    "series",
    "problem",
    "like",
    "predicting",
    "prices",
    "stocks",
    "particular",
    "month",
    "solved",
    "using",
    "rnn",
    "dive",
    "use",
    "case",
    "actually",
    "take",
    "look",
    "stock",
    "one",
    "things",
    "know",
    "analyzing",
    "stock",
    "today",
    "difficult",
    "analyzing",
    "whole",
    "stock",
    "stock",
    "market",
    "new",
    "york",
    "stock",
    "exchange",
    "produces",
    "somewhere",
    "neighborhood",
    "count",
    "individual",
    "trades",
    "fluctuations",
    "second",
    "like",
    "three",
    "terabytes",
    "day",
    "data",
    "going",
    "look",
    "one",
    "stock",
    "analyzing",
    "one",
    "stock",
    "really",
    "tricky",
    "give",
    "little",
    "jump",
    "exciting",
    "expect",
    "get",
    "rich",
    "immediately",
    "another",
    "application",
    "rnn",
    "natural",
    "language",
    "processing",
    "text",
    "mining",
    "sentiment",
    "analysis",
    "carried",
    "using",
    "rnn",
    "natural",
    "language",
    "processing",
    "see",
    "right",
    "term",
    "natural",
    "language",
    "processing",
    "stream",
    "three",
    "words",
    "together",
    "different",
    "said",
    "processing",
    "language",
    "natural",
    "leap",
    "time",
    "series",
    "important",
    "analyzing",
    "sentiments",
    "change",
    "whole",
    "value",
    "sentence",
    "switching",
    "words",
    "around",
    "counting",
    "words",
    "might",
    "get",
    "one",
    "sentiment",
    "actually",
    "look",
    "order",
    "get",
    "completely",
    "different",
    "sentiment",
    "rains",
    "look",
    "rainbows",
    "dark",
    "look",
    "stars",
    "positive",
    "sentiments",
    "based",
    "upon",
    "order",
    "sentence",
    "going",
    "machine",
    "translation",
    "given",
    "input",
    "one",
    "language",
    "rnn",
    "used",
    "translate",
    "input",
    "different",
    "languages",
    "output",
    "linguistically",
    "challenged",
    "study",
    "languages",
    "good",
    "languages",
    "know",
    "right",
    "away",
    "speaking",
    "english",
    "would",
    "say",
    "big",
    "cat",
    "speaking",
    "spanish",
    "would",
    "say",
    "cat",
    "big",
    "translation",
    "really",
    "important",
    "get",
    "right",
    "order",
    "get",
    "kinds",
    "parts",
    "speech",
    "important",
    "know",
    "order",
    "words",
    "person",
    "speaking",
    "english",
    "getting",
    "translated",
    "see",
    "person",
    "speaking",
    "english",
    "little",
    "diagram",
    "guess",
    "denoted",
    "flags",
    "flag",
    "um",
    "speaking",
    "english",
    "getting",
    "translated",
    "chinese",
    "italian",
    "french",
    "german",
    "spanish",
    "languages",
    "tools",
    "coming",
    "cool",
    "somebody",
    "like",
    "linguistically",
    "challenged",
    "travel",
    "worlds",
    "would",
    "never",
    "think",
    "something",
    "translate",
    "english",
    "back",
    "forth",
    "readily",
    "stuck",
    "communication",
    "gap",
    "let",
    "dive",
    "recurrent",
    "neural",
    "network",
    "recurrent",
    "neural",
    "network",
    "works",
    "principle",
    "saving",
    "output",
    "layer",
    "feeding",
    "back",
    "input",
    "order",
    "predict",
    "output",
    "layer",
    "sounds",
    "little",
    "confusing",
    "start",
    "breaking",
    "make",
    "sense",
    "usually",
    "propagation",
    "forward",
    "neural",
    "network",
    "input",
    "layers",
    "hidden",
    "layers",
    "output",
    "layer",
    "recurrent",
    "neural",
    "network",
    "turn",
    "side",
    "x",
    "comes",
    "bottom",
    "hidden",
    "layers",
    "usually",
    "draw",
    "simplified",
    "x",
    "h",
    "c",
    "loop",
    "b",
    "c",
    "perimeters",
    "lot",
    "times",
    "see",
    "kind",
    "drawing",
    "digging",
    "closer",
    "closer",
    "h",
    "works",
    "going",
    "left",
    "right",
    "see",
    "c",
    "goes",
    "x",
    "goes",
    "x",
    "going",
    "upward",
    "bound",
    "c",
    "going",
    "right",
    "going",
    "c",
    "also",
    "going",
    "gets",
    "little",
    "confusing",
    "x",
    "n",
    "c",
    "n",
    "c",
    "c",
    "based",
    "h",
    "minus",
    "value",
    "based",
    "h",
    "value",
    "connected",
    "necessarily",
    "value",
    "h",
    "thing",
    "usually",
    "draw",
    "represent",
    "function",
    "h",
    "equals",
    "function",
    "c",
    "h",
    "minus",
    "1",
    "last",
    "h",
    "output",
    "x",
    "going",
    "last",
    "output",
    "h",
    "combined",
    "new",
    "input",
    "x",
    "h",
    "new",
    "state",
    "fc",
    "function",
    "parameters",
    "c",
    "common",
    "way",
    "denoting",
    "h",
    "minus",
    "1",
    "old",
    "state",
    "coming",
    "x",
    "input",
    "vector",
    "time",
    "step",
    "well",
    "need",
    "cover",
    "types",
    "recurrent",
    "neural",
    "networks",
    "first",
    "one",
    "common",
    "one",
    "single",
    "output",
    "neural",
    "network",
    "usually",
    "known",
    "vanilla",
    "neural",
    "network",
    "used",
    "regular",
    "machine",
    "learning",
    "problems",
    "vanilla",
    "usually",
    "considered",
    "kind",
    "real",
    "basic",
    "flavor",
    "basic",
    "lot",
    "times",
    "call",
    "vanilla",
    "neural",
    "network",
    "common",
    "term",
    "kind",
    "slang",
    "term",
    "people",
    "know",
    "talking",
    "usually",
    "say",
    "run",
    "one",
    "mini",
    "single",
    "input",
    "might",
    "multiple",
    "outputs",
    "case",
    "image",
    "captioning",
    "looked",
    "earlier",
    "looking",
    "dog",
    "dog",
    "catching",
    "ball",
    "air",
    "mini",
    "one",
    "network",
    "takes",
    "sequence",
    "inputs",
    "examples",
    "sentiment",
    "analysis",
    "given",
    "sentence",
    "classified",
    "expressing",
    "positive",
    "negative",
    "sentiments",
    "looked",
    "discussing",
    "rains",
    "look",
    "rainbow",
    "positive",
    "sentiment",
    "rain",
    "might",
    "negative",
    "sentiment",
    "adding",
    "words",
    "course",
    "going",
    "many",
    "many",
    "networks",
    "takes",
    "sequence",
    "inputs",
    "generates",
    "sequence",
    "outputs",
    "example",
    "machine",
    "translation",
    "lengthy",
    "sentence",
    "coming",
    "english",
    "going",
    "different",
    "languages",
    "know",
    "wonderful",
    "tool",
    "complicated",
    "set",
    "computations",
    "know",
    "translator",
    "realize",
    "difficult",
    "translate",
    "different",
    "languages",
    "one",
    "biggest",
    "things",
    "need",
    "understand",
    "working",
    "neural",
    "network",
    "called",
    "vanishing",
    "gradient",
    "problem",
    "training",
    "rnn",
    "slope",
    "either",
    "small",
    "large",
    "makes",
    "training",
    "difficult",
    "slope",
    "small",
    "problem",
    "known",
    "vanishing",
    "gradient",
    "see",
    "nice",
    "uh",
    "image",
    "loss",
    "information",
    "time",
    "pushing",
    "enough",
    "information",
    "forward",
    "information",
    "lost",
    "go",
    "train",
    "start",
    "losing",
    "third",
    "word",
    "sentence",
    "something",
    "like",
    "quite",
    "follow",
    "full",
    "logic",
    "working",
    "exploding",
    "gradient",
    "problem",
    "oh",
    "one",
    "runs",
    "everybody",
    "working",
    "particular",
    "neural",
    "network",
    "slope",
    "tends",
    "grow",
    "exponentially",
    "instead",
    "decaying",
    "problem",
    "called",
    "exploding",
    "gradient",
    "issues",
    "gradient",
    "problem",
    "long",
    "training",
    "time",
    "poor",
    "performance",
    "bad",
    "accuracy",
    "add",
    "one",
    "computer",
    "lower",
    "end",
    "computer",
    "testing",
    "model",
    "lock",
    "give",
    "memory",
    "error",
    "explaining",
    "gradient",
    "problem",
    "consider",
    "following",
    "two",
    "examples",
    "understand",
    "next",
    "word",
    "sequence",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "students",
    "got",
    "engineering",
    "blank",
    "asia",
    "see",
    "x",
    "value",
    "going",
    "previous",
    "value",
    "going",
    "forward",
    "back",
    "propagate",
    "error",
    "like",
    "neural",
    "network",
    "looking",
    "missing",
    "word",
    "maybe",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "student",
    "got",
    "engineering",
    "blank",
    "asia",
    "consider",
    "following",
    "example",
    "person",
    "took",
    "bike",
    "go",
    "back",
    "person",
    "took",
    "bike",
    "blank",
    "thief",
    "order",
    "understand",
    "would",
    "next",
    "word",
    "sequence",
    "rnn",
    "must",
    "memorize",
    "previous",
    "context",
    "whether",
    "subject",
    "singular",
    "noun",
    "plural",
    "noun",
    "thief",
    "singular",
    "student",
    "got",
    "engineering",
    "well",
    "order",
    "understand",
    "would",
    "next",
    "word",
    "sequence",
    "rnn",
    "must",
    "memorize",
    "previous",
    "context",
    "whether",
    "subject",
    "singular",
    "noun",
    "plural",
    "noun",
    "see",
    "students",
    "got",
    "engineering",
    "blank",
    "asia",
    "might",
    "sometimes",
    "difficult",
    "air",
    "back",
    "propagate",
    "beginning",
    "sequence",
    "predict",
    "output",
    "run",
    "gradient",
    "problem",
    "need",
    "solution",
    "solution",
    "gradient",
    "problem",
    "first",
    "going",
    "look",
    "exploding",
    "gradient",
    "three",
    "different",
    "solutions",
    "depending",
    "going",
    "one",
    "identity",
    "initialization",
    "first",
    "thing",
    "want",
    "see",
    "find",
    "way",
    "minimize",
    "identities",
    "coming",
    "instead",
    "identify",
    "everything",
    "important",
    "information",
    "looking",
    "next",
    "truncate",
    "back",
    "propagation",
    "instead",
    "whatever",
    "information",
    "sending",
    "next",
    "series",
    "truncate",
    "sending",
    "lower",
    "particular",
    "set",
    "layers",
    "make",
    "smaller",
    "finally",
    "gradient",
    "clipping",
    "training",
    "clip",
    "gradient",
    "looks",
    "like",
    "narrow",
    "training",
    "model",
    "using",
    "vanishing",
    "gradient",
    "option",
    "problem",
    "take",
    "look",
    "weight",
    "initialization",
    "similar",
    "identity",
    "going",
    "add",
    "weights",
    "identify",
    "different",
    "aspects",
    "coming",
    "better",
    "choosing",
    "right",
    "activation",
    "function",
    "huge",
    "might",
    "activating",
    "based",
    "one",
    "thing",
    "need",
    "limit",
    "talked",
    "much",
    "activation",
    "functions",
    "look",
    "minimally",
    "lot",
    "choices",
    "finally",
    "long",
    "short",
    "term",
    "memory",
    "networks",
    "lstms",
    "make",
    "adjustments",
    "like",
    "clip",
    "gradient",
    "comes",
    "also",
    "expand",
    "increase",
    "memory",
    "network",
    "size",
    "handles",
    "information",
    "one",
    "common",
    "problems",
    "today",
    "setup",
    "call",
    "dependencies",
    "suppose",
    "try",
    "predict",
    "last",
    "word",
    "text",
    "clouds",
    "probably",
    "said",
    "sky",
    "need",
    "context",
    "pretty",
    "clear",
    "last",
    "word",
    "going",
    "sky",
    "suppose",
    "try",
    "predict",
    "last",
    "word",
    "text",
    "staying",
    "spain",
    "last",
    "10",
    "years",
    "speak",
    "fluent",
    "maybe",
    "said",
    "portuguese",
    "french",
    "probably",
    "said",
    "spanish",
    "word",
    "predict",
    "depend",
    "previous",
    "words",
    "context",
    "need",
    "context",
    "spain",
    "predict",
    "last",
    "word",
    "text",
    "possible",
    "gap",
    "relevant",
    "information",
    "point",
    "needed",
    "become",
    "large",
    "lstms",
    "help",
    "us",
    "solve",
    "problem",
    "lstms",
    "special",
    "kind",
    "recurrent",
    "neural",
    "network",
    "capable",
    "learning",
    "dependencies",
    "remembering",
    "information",
    "long",
    "periods",
    "time",
    "default",
    "behavior",
    "recurrent",
    "neural",
    "networks",
    "form",
    "chain",
    "repeating",
    "modules",
    "neural",
    "network",
    "connections",
    "standard",
    "rnns",
    "repeating",
    "module",
    "simple",
    "structure",
    "single",
    "tangent",
    "h",
    "layer",
    "lstms",
    "also",
    "chain",
    "like",
    "structure",
    "repeating",
    "module",
    "different",
    "structure",
    "instead",
    "single",
    "neural",
    "network",
    "layer",
    "four",
    "interacting",
    "layers",
    "communicating",
    "special",
    "way",
    "lstms",
    "special",
    "kind",
    "recurrent",
    "neural",
    "network",
    "capable",
    "learning",
    "dependencies",
    "remembering",
    "information",
    "long",
    "periods",
    "time",
    "default",
    "behavior",
    "ls",
    "tmss",
    "also",
    "structure",
    "repeating",
    "module",
    "different",
    "structure",
    "instead",
    "single",
    "neural",
    "network",
    "layer",
    "four",
    "interacting",
    "layers",
    "communicating",
    "special",
    "way",
    "see",
    "deeper",
    "dig",
    "complicated",
    "graphs",
    "get",
    "want",
    "note",
    "x",
    "minus",
    "1",
    "coming",
    "x",
    "coming",
    "x",
    "plus",
    "one",
    "h",
    "minus",
    "one",
    "h",
    "coming",
    "h",
    "plus",
    "one",
    "going",
    "course",
    "uh",
    "side",
    "output",
    "middle",
    "tangent",
    "h",
    "occurs",
    "two",
    "different",
    "places",
    "computing",
    "x",
    "plus",
    "one",
    "getting",
    "tangent",
    "h",
    "x",
    "also",
    "getting",
    "value",
    "coming",
    "x",
    "minus",
    "one",
    "short",
    "look",
    "layers",
    "propagate",
    "first",
    "layer",
    "goes",
    "second",
    "layer",
    "back",
    "also",
    "going",
    "third",
    "layer",
    "kind",
    "stacking",
    "get",
    "complicated",
    "grow",
    "size",
    "also",
    "grows",
    "memory",
    "amount",
    "resources",
    "takes",
    "powerful",
    "tool",
    "help",
    "us",
    "address",
    "problem",
    "complicated",
    "long",
    "sequential",
    "information",
    "coming",
    "like",
    "looking",
    "sentence",
    "looking",
    "long",
    "short",
    "term",
    "memory",
    "network",
    "uh",
    "three",
    "steps",
    "processing",
    "lstms",
    "look",
    "first",
    "one",
    "want",
    "forget",
    "irrelevant",
    "parts",
    "previous",
    "state",
    "know",
    "lot",
    "times",
    "like",
    "know",
    "unless",
    "trying",
    "look",
    "whether",
    "plural",
    "noun",
    "really",
    "play",
    "huge",
    "part",
    "language",
    "want",
    "get",
    "rid",
    "selectively",
    "update",
    "cell",
    "state",
    "values",
    "want",
    "update",
    "cell",
    "state",
    "values",
    "reflect",
    "working",
    "finally",
    "want",
    "put",
    "output",
    "certain",
    "parts",
    "cell",
    "state",
    "whatever",
    "coming",
    "want",
    "limit",
    "going",
    "let",
    "dig",
    "little",
    "deeper",
    "let",
    "see",
    "really",
    "looks",
    "like",
    "step",
    "one",
    "decides",
    "much",
    "past",
    "remember",
    "first",
    "step",
    "lstm",
    "decide",
    "information",
    "omitted",
    "cell",
    "particular",
    "time",
    "step",
    "decided",
    "sigmoid",
    "function",
    "looks",
    "previous",
    "state",
    "h",
    "minus",
    "1",
    "current",
    "input",
    "x",
    "computes",
    "function",
    "see",
    "function",
    "equals",
    "sigmoid",
    "function",
    "weight",
    "f",
    "h",
    "minus",
    "1",
    "x",
    "plus",
    "course",
    "bias",
    "neural",
    "networks",
    "bias",
    "function",
    "f",
    "equals",
    "forget",
    "gate",
    "decides",
    "information",
    "delete",
    "important",
    "previous",
    "time",
    "step",
    "considering",
    "stm",
    "fed",
    "following",
    "inputs",
    "previous",
    "present",
    "time",
    "step",
    "alice",
    "good",
    "physics",
    "john",
    "hand",
    "good",
    "chemistry",
    "previous",
    "output",
    "john",
    "plays",
    "football",
    "well",
    "told",
    "yesterday",
    "phone",
    "served",
    "captain",
    "college",
    "football",
    "team",
    "current",
    "input",
    "look",
    "first",
    "step",
    "forget",
    "gate",
    "realizes",
    "might",
    "change",
    "context",
    "encountering",
    "first",
    "full",
    "stop",
    "compares",
    "current",
    "input",
    "sentence",
    "x",
    "looking",
    "full",
    "stop",
    "compares",
    "input",
    "new",
    "sentence",
    "next",
    "sentence",
    "talks",
    "john",
    "information",
    "alice",
    "deleted",
    "okay",
    "important",
    "know",
    "input",
    "coming",
    "going",
    "continue",
    "john",
    "going",
    "primary",
    "information",
    "looking",
    "position",
    "subject",
    "vacated",
    "assigned",
    "john",
    "one",
    "seen",
    "weeded",
    "whole",
    "bunch",
    "information",
    "passing",
    "information",
    "john",
    "since",
    "new",
    "topic",
    "step",
    "two",
    "decide",
    "much",
    "unit",
    "add",
    "current",
    "state",
    "second",
    "layer",
    "two",
    "parts",
    "one",
    "sigmoid",
    "function",
    "tangent",
    "h",
    "sigmoid",
    "function",
    "decides",
    "values",
    "let",
    "zero",
    "one",
    "tangent",
    "h",
    "function",
    "gives",
    "weightage",
    "values",
    "passed",
    "deciding",
    "level",
    "importance",
    "minus",
    "one",
    "one",
    "see",
    "two",
    "formulas",
    "come",
    "equals",
    "sigmoid",
    "weight",
    "h",
    "minus",
    "one",
    "x",
    "plus",
    "bias",
    "c",
    "equals",
    "tangent",
    "h",
    "weight",
    "c",
    "h",
    "minus",
    "one",
    "x",
    "plus",
    "bias",
    "c",
    "equals",
    "input",
    "gate",
    "determines",
    "information",
    "let",
    "based",
    "significance",
    "current",
    "time",
    "step",
    "seems",
    "little",
    "complicated",
    "worry",
    "lot",
    "programming",
    "already",
    "done",
    "get",
    "case",
    "study",
    "understanding",
    "though",
    "part",
    "program",
    "important",
    "trying",
    "figure",
    "set",
    "settings",
    "also",
    "note",
    "looking",
    "semblance",
    "forward",
    "propagation",
    "neural",
    "networks",
    "value",
    "assigned",
    "weight",
    "plus",
    "bias",
    "important",
    "steps",
    "neural",
    "network",
    "layers",
    "whether",
    "propagating",
    "information",
    "one",
    "next",
    "straightforward",
    "neural",
    "network",
    "propagation",
    "let",
    "take",
    "quick",
    "look",
    "looks",
    "like",
    "human",
    "standpoint",
    "step",
    "suit",
    "consider",
    "current",
    "input",
    "x",
    "john",
    "plays",
    "football",
    "well",
    "told",
    "yesterday",
    "phone",
    "served",
    "captain",
    "college",
    "football",
    "team",
    "input",
    "input",
    "gate",
    "announces",
    "important",
    "information",
    "john",
    "plays",
    "football",
    "captain",
    "college",
    "team",
    "important",
    "told",
    "phone",
    "yesterday",
    "less",
    "important",
    "hence",
    "forgotten",
    "process",
    "adding",
    "new",
    "information",
    "done",
    "via",
    "input",
    "gate",
    "example",
    "human",
    "form",
    "look",
    "training",
    "stuff",
    "minute",
    "human",
    "wanted",
    "get",
    "information",
    "conversation",
    "maybe",
    "google",
    "voice",
    "listening",
    "something",
    "like",
    "um",
    "weed",
    "information",
    "talking",
    "phone",
    "yesterday",
    "well",
    "want",
    "memorize",
    "talked",
    "phone",
    "yesterday",
    "maybe",
    "important",
    "case",
    "want",
    "know",
    "captain",
    "football",
    "team",
    "want",
    "know",
    "served",
    "want",
    "know",
    "john",
    "plays",
    "football",
    "captain",
    "college",
    "football",
    "team",
    "two",
    "things",
    "want",
    "take",
    "away",
    "human",
    "measure",
    "lot",
    "human",
    "viewpoint",
    "also",
    "try",
    "train",
    "understand",
    "neural",
    "networks",
    "finally",
    "get",
    "step",
    "three",
    "decides",
    "part",
    "current",
    "cell",
    "state",
    "makes",
    "output",
    "third",
    "step",
    "decide",
    "output",
    "first",
    "run",
    "sigmoid",
    "layer",
    "decides",
    "parts",
    "cell",
    "state",
    "make",
    "output",
    "put",
    "cell",
    "state",
    "tangent",
    "h",
    "push",
    "values",
    "1",
    "multiply",
    "output",
    "sigmoid",
    "gate",
    "talk",
    "output",
    "set",
    "equal",
    "sigmoid",
    "weight",
    "0",
    "h",
    "minus",
    "1",
    "back",
    "one",
    "step",
    "time",
    "x",
    "plus",
    "course",
    "bias",
    "h",
    "equals",
    "outer",
    "times",
    "tangent",
    "tangent",
    "h",
    "c",
    "equals",
    "output",
    "gate",
    "allows",
    "passed",
    "information",
    "impact",
    "output",
    "current",
    "time",
    "step",
    "let",
    "consider",
    "example",
    "predicting",
    "next",
    "word",
    "sentence",
    "john",
    "played",
    "tremendously",
    "well",
    "opponent",
    "team",
    "contributions",
    "brave",
    "blank",
    "awarded",
    "player",
    "match",
    "could",
    "lot",
    "choices",
    "empty",
    "space",
    "current",
    "input",
    "brave",
    "adjective",
    "adjectives",
    "describe",
    "noun",
    "john",
    "could",
    "best",
    "output",
    "brave",
    "thumbs",
    "john",
    "awarded",
    "player",
    "match",
    "pull",
    "nouns",
    "sentence",
    "team",
    "look",
    "right",
    "really",
    "subject",
    "talking",
    "contributions",
    "know",
    "brave",
    "contributions",
    "brave",
    "teen",
    "brave",
    "player",
    "brave",
    "match",
    "look",
    "start",
    "train",
    "neural",
    "network",
    "starts",
    "looking",
    "goes",
    "oh",
    "john",
    "talking",
    "brave",
    "adjective",
    "john",
    "going",
    "best",
    "output",
    "give",
    "john",
    "big",
    "thumbs",
    "course",
    "jump",
    "favorite",
    "part",
    "case",
    "study",
    "use",
    "case",
    "implementation",
    "lstm",
    "let",
    "predict",
    "prices",
    "stocks",
    "using",
    "lstm",
    "network",
    "based",
    "stock",
    "price",
    "data",
    "2012",
    "going",
    "try",
    "predict",
    "stock",
    "prices",
    "narrow",
    "set",
    "data",
    "going",
    "whole",
    "stock",
    "market",
    "turns",
    "new",
    "york",
    "stock",
    "exchange",
    "generates",
    "roughly",
    "three",
    "terabytes",
    "data",
    "per",
    "day",
    "different",
    "trades",
    "different",
    "stocks",
    "going",
    "individual",
    "one",
    "second",
    "second",
    "nanosecond",
    "nanosecond",
    "going",
    "limit",
    "basic",
    "fundamental",
    "information",
    "think",
    "going",
    "get",
    "rich",
    "today",
    "least",
    "give",
    "eye",
    "give",
    "step",
    "forward",
    "start",
    "processing",
    "something",
    "like",
    "stock",
    "prices",
    "valid",
    "use",
    "machine",
    "learning",
    "today",
    "markets",
    "use",
    "case",
    "implementation",
    "lstm",
    "let",
    "dive",
    "going",
    "import",
    "libraries",
    "going",
    "import",
    "training",
    "set",
    "get",
    "scaling",
    "going",
    "watch",
    "tutorials",
    "lot",
    "pieces",
    "start",
    "look",
    "familiar",
    "similar",
    "setup",
    "let",
    "take",
    "look",
    "reminder",
    "going",
    "using",
    "anaconda",
    "jupiter",
    "notebook",
    "anaconda",
    "navigator",
    "go",
    "environments",
    "actually",
    "set",
    "cross",
    "python",
    "36",
    "python36",
    "nice",
    "thing",
    "anaconda",
    "especially",
    "newer",
    "version",
    "remember",
    "year",
    "ago",
    "messing",
    "anaconda",
    "different",
    "versions",
    "python",
    "different",
    "environments",
    "anaconda",
    "nice",
    "interface",
    "installed",
    "ubuntu",
    "linux",
    "machine",
    "windows",
    "works",
    "fine",
    "go",
    "open",
    "terminal",
    "window",
    "terminal",
    "window",
    "going",
    "start",
    "installing",
    "using",
    "pip",
    "install",
    "different",
    "modules",
    "everything",
    "already",
    "need",
    "installed",
    "particular",
    "environment",
    "need",
    "course",
    "need",
    "use",
    "anaconda",
    "jupiter",
    "use",
    "whatever",
    "favorite",
    "python",
    "id",
    "like",
    "big",
    "fan",
    "keeps",
    "stuff",
    "separate",
    "see",
    "machine",
    "specifically",
    "installed",
    "one",
    "cross",
    "since",
    "going",
    "working",
    "cross",
    "tensorflow",
    "go",
    "back",
    "home",
    "gone",
    "application",
    "environment",
    "loaded",
    "click",
    "launch",
    "jupiter",
    "notebook",
    "already",
    "jupiter",
    "notebook",
    "um",
    "set",
    "lot",
    "stuff",
    "ready",
    "go",
    "kind",
    "like",
    "uh",
    "martha",
    "stewart",
    "old",
    "cooking",
    "show",
    "want",
    "make",
    "sure",
    "tools",
    "waiting",
    "load",
    "go",
    "says",
    "new",
    "see",
    "create",
    "new",
    "python",
    "underneath",
    "setup",
    "already",
    "modules",
    "installed",
    "actually",
    "rename",
    "go",
    "file",
    "rename",
    "calling",
    "rnn",
    "stock",
    "let",
    "take",
    "look",
    "start",
    "diving",
    "code",
    "let",
    "get",
    "exciting",
    "part",
    "looked",
    "tool",
    "course",
    "might",
    "using",
    "different",
    "tool",
    "fine",
    "let",
    "start",
    "putting",
    "code",
    "seeing",
    "imports",
    "uploading",
    "everything",
    "looks",
    "like",
    "first",
    "half",
    "kind",
    "boring",
    "hit",
    "run",
    "button",
    "going",
    "importing",
    "numpy",
    "np",
    "uh",
    "number",
    "python",
    "numpy",
    "array",
    "matplot",
    "library",
    "going",
    "plotting",
    "end",
    "pandas",
    "data",
    "set",
    "pandas",
    "pd",
    "hit",
    "run",
    "really",
    "anything",
    "except",
    "load",
    "modules",
    "quick",
    "note",
    "let",
    "quick",
    "draw",
    "oops",
    "shift",
    "alt",
    "go",
    "notice",
    "setup",
    "divide",
    "oops",
    "going",
    "actually",
    "let",
    "overlap",
    "go",
    "first",
    "part",
    "going",
    "data",
    "prep",
    "lot",
    "prepping",
    "involved",
    "um",
    "fact",
    "depending",
    "system",
    "since",
    "using",
    "karas",
    "put",
    "overlap",
    "uh",
    "find",
    "almost",
    "maybe",
    "even",
    "half",
    "code",
    "data",
    "prep",
    "reason",
    "overlapped",
    "uh",
    "cross",
    "let",
    "put",
    "working",
    "uh",
    "cross",
    "like",
    "preset",
    "stuff",
    "already",
    "really",
    "nice",
    "couple",
    "steps",
    "lot",
    "times",
    "kara",
    "setup",
    "take",
    "look",
    "see",
    "comes",
    "code",
    "go",
    "look",
    "stock",
    "last",
    "part",
    "evaluate",
    "working",
    "shareholders",
    "classroom",
    "whatever",
    "working",
    "uh",
    "evaluate",
    "next",
    "biggest",
    "piece",
    "um",
    "actual",
    "code",
    "crossed",
    "little",
    "bit",
    "working",
    "packages",
    "might",
    "like",
    "three",
    "lines",
    "might",
    "stuff",
    "data",
    "since",
    "cross",
    "cutting",
    "edge",
    "load",
    "individual",
    "layers",
    "see",
    "lines",
    "crosses",
    "little",
    "bit",
    "robust",
    "spend",
    "lot",
    "times",
    "like",
    "said",
    "evaluate",
    "want",
    "something",
    "present",
    "everybody",
    "else",
    "say",
    "hey",
    "looks",
    "like",
    "let",
    "go",
    "steps",
    "like",
    "kind",
    "general",
    "overview",
    "let",
    "take",
    "look",
    "see",
    "next",
    "set",
    "code",
    "looks",
    "like",
    "data",
    "set",
    "train",
    "going",
    "read",
    "using",
    "pd",
    "pandas",
    "dot",
    "read",
    "csv",
    "google",
    "stock",
    "price",
    "train",
    "dot",
    "csv",
    "training",
    "set",
    "equals",
    "data",
    "set",
    "train",
    "dot",
    "location",
    "kind",
    "sorted",
    "part",
    "going",
    "let",
    "take",
    "look",
    "let",
    "look",
    "actual",
    "file",
    "see",
    "going",
    "look",
    "ignore",
    "extra",
    "files",
    "already",
    "train",
    "test",
    "set",
    "sorted",
    "important",
    "notice",
    "lot",
    "times",
    "part",
    "data",
    "take",
    "20",
    "percent",
    "data",
    "test",
    "train",
    "rest",
    "use",
    "create",
    "neural",
    "network",
    "way",
    "find",
    "good",
    "uh",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "see",
    "looks",
    "like",
    "far",
    "file",
    "went",
    "ahead",
    "opened",
    "basic",
    "word",
    "pad",
    "text",
    "editor",
    "take",
    "look",
    "certainly",
    "open",
    "excel",
    "kind",
    "spreadsheet",
    "note",
    "comma",
    "separated",
    "variables",
    "date",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "standard",
    "stuff",
    "import",
    "stock",
    "basic",
    "set",
    "information",
    "look",
    "stock",
    "free",
    "download",
    "case",
    "downloaded",
    "google",
    "call",
    "google",
    "stock",
    "price",
    "specifically",
    "google",
    "google",
    "stock",
    "values",
    "see",
    "started",
    "1",
    "3",
    "look",
    "first",
    "setup",
    "data",
    "set",
    "train",
    "equals",
    "pd",
    "underscore",
    "csv",
    "noticed",
    "original",
    "frame",
    "let",
    "go",
    "back",
    "set",
    "home",
    "ubuntu",
    "downloads",
    "google",
    "stock",
    "price",
    "train",
    "went",
    "ahead",
    "changed",
    "file",
    "running",
    "code",
    "saved",
    "particular",
    "python",
    "code",
    "need",
    "go",
    "special",
    "paths",
    "full",
    "path",
    "course",
    "want",
    "take",
    "certain",
    "values",
    "going",
    "notice",
    "using",
    "data",
    "set",
    "pandas",
    "pandas",
    "basically",
    "looks",
    "like",
    "spreadsheet",
    "case",
    "going",
    "eye",
    "location",
    "going",
    "get",
    "specific",
    "locations",
    "first",
    "value",
    "going",
    "show",
    "us",
    "pulling",
    "rows",
    "data",
    "second",
    "one",
    "going",
    "look",
    "columns",
    "one",
    "two",
    "remember",
    "data",
    "switch",
    "back",
    "columns",
    "always",
    "start",
    "zero",
    "date",
    "going",
    "looking",
    "open",
    "high",
    "would",
    "one",
    "two",
    "label",
    "right",
    "see",
    "go",
    "back",
    "certainly",
    "extrapolate",
    "columns",
    "example",
    "let",
    "limit",
    "little",
    "bit",
    "focus",
    "key",
    "aspects",
    "stock",
    "go",
    "run",
    "code",
    "said",
    "first",
    "half",
    "boring",
    "whenever",
    "hit",
    "run",
    "button",
    "anything",
    "still",
    "loading",
    "data",
    "setting",
    "loaded",
    "data",
    "want",
    "go",
    "ahead",
    "scale",
    "want",
    "call",
    "feature",
    "scaling",
    "going",
    "pull",
    "sk",
    "learn",
    "sk",
    "kit",
    "import",
    "min",
    "max",
    "scalar",
    "look",
    "got",
    "remember",
    "biases",
    "data",
    "want",
    "get",
    "rid",
    "something",
    "like",
    "really",
    "high",
    "value",
    "let",
    "draw",
    "quick",
    "graph",
    "something",
    "like",
    "maybe",
    "stock",
    "value",
    "one",
    "stock",
    "value",
    "hundred",
    "another",
    "stock",
    "value",
    "five",
    "um",
    "start",
    "get",
    "bias",
    "different",
    "stocks",
    "go",
    "ahead",
    "say",
    "okay",
    "100",
    "going",
    "max",
    "5",
    "going",
    "min",
    "everything",
    "else",
    "goes",
    "change",
    "squish",
    "like",
    "word",
    "squish",
    "1",
    "100",
    "equals",
    "one",
    "one",
    "equals",
    "hundred",
    "zero",
    "equals",
    "five",
    "multiply",
    "usually",
    "simple",
    "multiplication",
    "using",
    "uh",
    "multiplication",
    "going",
    "uh",
    "minus",
    "five",
    "100",
    "divided",
    "95",
    "divided",
    "1",
    "whatever",
    "value",
    "divided",
    "actually",
    "created",
    "scale",
    "telling",
    "going",
    "0",
    "want",
    "take",
    "training",
    "set",
    "going",
    "create",
    "training",
    "set",
    "scaled",
    "going",
    "use",
    "scalar",
    "sc",
    "going",
    "fit",
    "going",
    "fit",
    "transform",
    "training",
    "set",
    "uh",
    "use",
    "sc",
    "particular",
    "object",
    "use",
    "later",
    "testing",
    "set",
    "remember",
    "also",
    "scale",
    "go",
    "test",
    "model",
    "see",
    "works",
    "go",
    "ahead",
    "click",
    "run",
    "going",
    "output",
    "yet",
    "setting",
    "variables",
    "okay",
    "paste",
    "data",
    "going",
    "create",
    "data",
    "structure",
    "60",
    "time",
    "steps",
    "output",
    "first",
    "note",
    "running",
    "60",
    "time",
    "steps",
    "value",
    "also",
    "comes",
    "first",
    "thing",
    "create",
    "x",
    "train",
    "train",
    "variables",
    "set",
    "empty",
    "python",
    "array",
    "important",
    "remember",
    "kind",
    "array",
    "working",
    "going",
    "come",
    "going",
    "go",
    "range",
    "60",
    "1258",
    "60",
    "60",
    "time",
    "steps",
    "reason",
    "want",
    "adding",
    "data",
    "nothing",
    "going",
    "use",
    "60",
    "time",
    "steps",
    "start",
    "point",
    "60",
    "includes",
    "everything",
    "underneath",
    "otherwise",
    "get",
    "pointer",
    "error",
    "going",
    "take",
    "x",
    "train",
    "going",
    "append",
    "training",
    "set",
    "scaled",
    "scaled",
    "value",
    "0",
    "1",
    "equal",
    "60",
    "value",
    "going",
    "60",
    "minus",
    "60",
    "actually",
    "0",
    "going",
    "0",
    "60",
    "1",
    "let",
    "circle",
    "part",
    "right",
    "1",
    "61",
    "2",
    "62",
    "remember",
    "said",
    "0",
    "60",
    "incorrect",
    "count",
    "remember",
    "starts",
    "0",
    "count",
    "actually",
    "important",
    "remember",
    "looking",
    "second",
    "part",
    "looking",
    "remember",
    "correctly",
    "go",
    "go",
    "0",
    "59",
    "comma",
    "0",
    "right",
    "finally",
    "going",
    "look",
    "open",
    "value",
    "know",
    "put",
    "1",
    "remember",
    "quickly",
    "count",
    "second",
    "one",
    "open",
    "value",
    "looking",
    "open",
    "um",
    "finally",
    "train",
    "dot",
    "append",
    "training",
    "set",
    "zero",
    "remember",
    "correctly",
    "two",
    "comma",
    "zero",
    "remember",
    "correctly",
    "0",
    "59",
    "60",
    "values",
    "uh",
    "number",
    "going",
    "creating",
    "array",
    "0",
    "59",
    "number",
    "60",
    "going",
    "train",
    "appended",
    "goes",
    "way",
    "0",
    "59",
    "call",
    "60",
    "since",
    "value",
    "goes",
    "way",
    "12",
    "value",
    "comes",
    "length",
    "data",
    "loading",
    "loaded",
    "two",
    "arrays",
    "loaded",
    "one",
    "array",
    "filled",
    "arrays",
    "0",
    "59",
    "loaded",
    "one",
    "array",
    "value",
    "looking",
    "want",
    "think",
    "time",
    "sequence",
    "open",
    "open",
    "open",
    "open",
    "openopen",
    "next",
    "one",
    "series",
    "looking",
    "google",
    "stock",
    "time",
    "opens",
    "want",
    "know",
    "next",
    "one",
    "0",
    "59",
    "60",
    "1",
    "60",
    "61",
    "2",
    "62",
    "62",
    "going",
    "loaded",
    "loop",
    "go",
    "ahead",
    "take",
    "x",
    "train",
    "train",
    "equals",
    "converting",
    "back",
    "numpy",
    "array",
    "way",
    "use",
    "cool",
    "tools",
    "get",
    "numpy",
    "array",
    "including",
    "reshaping",
    "take",
    "look",
    "see",
    "going",
    "going",
    "take",
    "x",
    "train",
    "going",
    "reshape",
    "wow",
    "heck",
    "reshape",
    "mean",
    "means",
    "array",
    "remember",
    "correctly",
    "many",
    "numbers",
    "wide",
    "x",
    "train",
    "dot",
    "shape",
    "gets",
    "one",
    "shapes",
    "get",
    "x",
    "train",
    "dot",
    "shape",
    "one",
    "gets",
    "shape",
    "making",
    "sure",
    "data",
    "formatted",
    "correctly",
    "use",
    "pull",
    "fact",
    "60",
    "um",
    "case",
    "value",
    "60",
    "pi",
    "1199",
    "1258",
    "minus",
    "60",
    "making",
    "sure",
    "shaped",
    "correctly",
    "data",
    "grouped",
    "11",
    "99",
    "60",
    "different",
    "arrays",
    "one",
    "end",
    "means",
    "end",
    "dealing",
    "shapes",
    "numpy",
    "look",
    "layers",
    "end",
    "layer",
    "needs",
    "one",
    "value",
    "like",
    "leaf",
    "tree",
    "branch",
    "branches",
    "get",
    "leaf",
    "np",
    "dot",
    "reshape",
    "comes",
    "using",
    "existing",
    "shapes",
    "form",
    "go",
    "ahead",
    "run",
    "piece",
    "code",
    "real",
    "output",
    "import",
    "different",
    "cross",
    "modules",
    "need",
    "cross",
    "models",
    "going",
    "import",
    "sequential",
    "model",
    "dealing",
    "sequential",
    "data",
    "dense",
    "layers",
    "actually",
    "three",
    "layers",
    "going",
    "bring",
    "dents",
    "lstm",
    "focusing",
    "dropout",
    "discuss",
    "three",
    "layers",
    "moment",
    "need",
    "lstm",
    "need",
    "drop",
    "final",
    "layer",
    "dents",
    "let",
    "go",
    "ahead",
    "run",
    "bring",
    "port",
    "modules",
    "see",
    "get",
    "error",
    "read",
    "closer",
    "actually",
    "error",
    "warning",
    "warning",
    "mean",
    "things",
    "come",
    "time",
    "working",
    "cutting",
    "edge",
    "modules",
    "completely",
    "updated",
    "time",
    "going",
    "worry",
    "much",
    "warning",
    "saying",
    "h5py",
    "module",
    "part",
    "cross",
    "going",
    "updated",
    "point",
    "running",
    "new",
    "stuff",
    "cross",
    "start",
    "updating",
    "cross",
    "system",
    "better",
    "make",
    "sure",
    "h5",
    "pi",
    "updated",
    "otherwise",
    "going",
    "error",
    "later",
    "actually",
    "run",
    "update",
    "h5",
    "pi",
    "wanted",
    "big",
    "deal",
    "going",
    "worry",
    "today",
    "said",
    "going",
    "jump",
    "start",
    "looking",
    "layers",
    "mean",
    "meant",
    "going",
    "start",
    "initializing",
    "rnn",
    "start",
    "adding",
    "layers",
    "see",
    "lstm",
    "dropout",
    "lstm",
    "dropout",
    "lstm",
    "dropout",
    "heck",
    "let",
    "explore",
    "start",
    "initializing",
    "rnn",
    "regressor",
    "equals",
    "sequential",
    "using",
    "sequential",
    "model",
    "run",
    "load",
    "going",
    "start",
    "adding",
    "lstm",
    "layer",
    "dropout",
    "regularization",
    "right",
    "q",
    "dropout",
    "regularization",
    "go",
    "back",
    "remember",
    "exploding",
    "gradient",
    "well",
    "talking",
    "dropout",
    "drops",
    "unnecessary",
    "data",
    "shifting",
    "huge",
    "amounts",
    "data",
    "network",
    "go",
    "let",
    "go",
    "ahead",
    "add",
    "go",
    "ahead",
    "run",
    "three",
    "let",
    "go",
    "ahead",
    "put",
    "three",
    "go",
    "back",
    "second",
    "one",
    "let",
    "put",
    "one",
    "let",
    "put",
    "go",
    "ahead",
    "put",
    "two",
    "mean",
    "said",
    "one",
    "actually",
    "two",
    "let",
    "add",
    "one",
    "see",
    "time",
    "run",
    "actually",
    "output",
    "let",
    "take",
    "closer",
    "look",
    "see",
    "going",
    "going",
    "add",
    "first",
    "lstm",
    "layer",
    "going",
    "units",
    "units",
    "positive",
    "integer",
    "dimensionality",
    "output",
    "space",
    "going",
    "next",
    "layer",
    "might",
    "60",
    "coming",
    "50",
    "going",
    "return",
    "sequence",
    "sequence",
    "data",
    "want",
    "keep",
    "true",
    "tell",
    "shape",
    "well",
    "already",
    "know",
    "shape",
    "going",
    "looking",
    "x",
    "train",
    "shape",
    "input",
    "shape",
    "equals",
    "x",
    "train",
    "shape",
    "one",
    "comma",
    "one",
    "makes",
    "really",
    "easy",
    "remember",
    "numbers",
    "put",
    "60",
    "whatever",
    "else",
    "let",
    "tell",
    "regressor",
    "model",
    "use",
    "follow",
    "stm",
    "dropout",
    "layer",
    "understanding",
    "dropout",
    "layer",
    "kind",
    "exciting",
    "one",
    "things",
    "happens",
    "train",
    "network",
    "means",
    "neural",
    "network",
    "memorize",
    "specific",
    "data",
    "trouble",
    "predicting",
    "anything",
    "specific",
    "realm",
    "fix",
    "time",
    "run",
    "training",
    "mode",
    "going",
    "take",
    "point",
    "two",
    "twenty",
    "percent",
    "neurons",
    "turn",
    "gon",
    "na",
    "train",
    "ones",
    "gon",
    "na",
    "random",
    "way",
    "time",
    "pass",
    "train",
    "nodes",
    "come",
    "back",
    "next",
    "training",
    "cycle",
    "randomly",
    "pick",
    "different",
    "finally",
    "see",
    "big",
    "difference",
    "go",
    "first",
    "second",
    "third",
    "fourth",
    "first",
    "thing",
    "input",
    "shape",
    "shapes",
    "already",
    "output",
    "units",
    "50",
    "item",
    "next",
    "step",
    "automatically",
    "knows",
    "layer",
    "putting",
    "50",
    "next",
    "layer",
    "automatically",
    "sets",
    "says",
    "oh",
    "50",
    "coming",
    "last",
    "layer",
    "coming",
    "know",
    "goes",
    "regressor",
    "course",
    "dropout",
    "coming",
    "one",
    "next",
    "three",
    "layers",
    "let",
    "know",
    "shape",
    "automatically",
    "understands",
    "going",
    "keep",
    "units",
    "still",
    "going",
    "50",
    "units",
    "still",
    "sequence",
    "coming",
    "50",
    "units",
    "sequence",
    "next",
    "piece",
    "code",
    "brings",
    "together",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "come",
    "put",
    "output",
    "layer",
    "dense",
    "layer",
    "remember",
    "three",
    "layers",
    "uh",
    "lstm",
    "dropout",
    "dents",
    "dense",
    "says",
    "going",
    "bring",
    "one",
    "output",
    "instead",
    "putting",
    "sequence",
    "know",
    "want",
    "know",
    "answer",
    "point",
    "let",
    "go",
    "ahead",
    "run",
    "notice",
    "setting",
    "things",
    "one",
    "step",
    "time",
    "far",
    "brought",
    "way",
    "brought",
    "data",
    "brought",
    "different",
    "modules",
    "formatted",
    "data",
    "training",
    "set",
    "know",
    "x",
    "train",
    "train",
    "source",
    "data",
    "answers",
    "know",
    "far",
    "going",
    "put",
    "reshaped",
    "come",
    "built",
    "cross",
    "imported",
    "different",
    "layers",
    "look",
    "five",
    "total",
    "layers",
    "cross",
    "little",
    "different",
    "lot",
    "systems",
    "lot",
    "systems",
    "put",
    "one",
    "line",
    "automatic",
    "give",
    "options",
    "layers",
    "interface",
    "give",
    "options",
    "data",
    "comes",
    "cross",
    "cutting",
    "edge",
    "reason",
    "even",
    "lot",
    "extra",
    "steps",
    "building",
    "model",
    "huge",
    "impact",
    "output",
    "new",
    "models",
    "cross",
    "brought",
    "dense",
    "full",
    "model",
    "put",
    "together",
    "regressor",
    "need",
    "go",
    "ahead",
    "compile",
    "going",
    "go",
    "ahead",
    "fit",
    "data",
    "going",
    "compile",
    "pieces",
    "come",
    "together",
    "going",
    "run",
    "training",
    "data",
    "actually",
    "recreate",
    "regressor",
    "ready",
    "used",
    "let",
    "go",
    "ahead",
    "compile",
    "go",
    "ahead",
    "run",
    "looking",
    "tutorials",
    "neural",
    "networks",
    "see",
    "going",
    "use",
    "optimizer",
    "atom",
    "atom",
    "optimized",
    "big",
    "data",
    "couple",
    "optimizers",
    "beyond",
    "scope",
    "tutorial",
    "certainly",
    "atom",
    "work",
    "pretty",
    "good",
    "loss",
    "equals",
    "mean",
    "squared",
    "value",
    "training",
    "want",
    "base",
    "loss",
    "bad",
    "error",
    "going",
    "use",
    "mean",
    "squared",
    "value",
    "error",
    "atom",
    "optimizer",
    "differential",
    "equations",
    "know",
    "math",
    "behind",
    "certainly",
    "helps",
    "know",
    "fit",
    "bigger",
    "models",
    "finally",
    "going",
    "fit",
    "fitting",
    "rnn",
    "training",
    "set",
    "x",
    "train",
    "train",
    "epics",
    "batch",
    "size",
    "know",
    "data",
    "coming",
    "x",
    "train",
    "train",
    "answer",
    "looking",
    "data",
    "sequential",
    "input",
    "epics",
    "many",
    "times",
    "going",
    "go",
    "whole",
    "data",
    "set",
    "created",
    "whole",
    "data",
    "set",
    "x",
    "trains",
    "rows",
    "includes",
    "time",
    "sequence",
    "batch",
    "size",
    "another",
    "one",
    "things",
    "cross",
    "really",
    "shines",
    "pulling",
    "save",
    "large",
    "file",
    "instead",
    "trying",
    "load",
    "ram",
    "pick",
    "smaller",
    "batches",
    "load",
    "indirectly",
    "worried",
    "pulling",
    "file",
    "today",
    "big",
    "enough",
    "cause",
    "computer",
    "much",
    "problem",
    "run",
    "straining",
    "resources",
    "run",
    "imagine",
    "happened",
    "lot",
    "one",
    "column",
    "one",
    "set",
    "stock",
    "case",
    "google",
    "stock",
    "imagine",
    "across",
    "stocks",
    "instead",
    "open",
    "open",
    "close",
    "high",
    "low",
    "actually",
    "find",
    "13",
    "different",
    "variables",
    "times",
    "60",
    "time",
    "sequence",
    "suddenly",
    "find",
    "gig",
    "memory",
    "loading",
    "ram",
    "completely",
    "know",
    "multiple",
    "computers",
    "cluster",
    "start",
    "running",
    "resource",
    "problems",
    "worry",
    "let",
    "go",
    "ahead",
    "run",
    "actually",
    "take",
    "little",
    "bit",
    "computer",
    "older",
    "laptop",
    "give",
    "second",
    "kick",
    "go",
    "right",
    "epic",
    "going",
    "tell",
    "running",
    "first",
    "run",
    "data",
    "going",
    "batching",
    "32",
    "pieces",
    "32",
    "lines",
    "time",
    "1198",
    "think",
    "said",
    "earlier",
    "one",
    "one",
    "13",
    "seconds",
    "imagine",
    "roughly",
    "20",
    "30",
    "minutes",
    "run",
    "time",
    "computer",
    "like",
    "said",
    "older",
    "laptop",
    "running",
    "gigahertz",
    "dual",
    "processor",
    "fine",
    "go",
    "ahead",
    "stop",
    "go",
    "get",
    "drink",
    "coffee",
    "come",
    "back",
    "let",
    "see",
    "happens",
    "end",
    "takes",
    "us",
    "like",
    "good",
    "cooking",
    "show",
    "gone",
    "got",
    "latte",
    "also",
    "stuff",
    "running",
    "background",
    "see",
    "numbers",
    "jumped",
    "like",
    "19",
    "seconds",
    "15",
    "seconds",
    "scroll",
    "see",
    "run",
    "100",
    "steps",
    "100",
    "epics",
    "question",
    "mean",
    "one",
    "first",
    "things",
    "notice",
    "loss",
    "kind",
    "stopped",
    "see",
    "kind",
    "goes",
    "hit",
    "three",
    "times",
    "row",
    "guessed",
    "epic",
    "pretty",
    "close",
    "since",
    "losses",
    "remain",
    "find",
    "looking",
    "going",
    "go",
    "ahead",
    "load",
    "test",
    "data",
    "test",
    "data",
    "process",
    "yet",
    "real",
    "stock",
    "price",
    "data",
    "set",
    "test",
    "eye",
    "location",
    "thing",
    "prepped",
    "data",
    "first",
    "place",
    "let",
    "go",
    "ahead",
    "go",
    "code",
    "see",
    "labeled",
    "part",
    "three",
    "making",
    "predictions",
    "visualizing",
    "results",
    "first",
    "thing",
    "need",
    "go",
    "ahead",
    "read",
    "data",
    "test",
    "csv",
    "see",
    "changed",
    "path",
    "computer",
    "call",
    "real",
    "stock",
    "price",
    "one",
    "column",
    "values",
    "location",
    "rows",
    "values",
    "one",
    "location",
    "open",
    "stock",
    "open",
    "let",
    "go",
    "ahead",
    "run",
    "loaded",
    "let",
    "go",
    "ahead",
    "create",
    "inputs",
    "going",
    "create",
    "inputs",
    "look",
    "familiar",
    "thing",
    "going",
    "take",
    "data",
    "set",
    "total",
    "going",
    "little",
    "panda",
    "concat",
    "data",
    "state",
    "train",
    "remember",
    "end",
    "data",
    "set",
    "train",
    "part",
    "data",
    "going",
    "let",
    "visualize",
    "little",
    "bit",
    "train",
    "data",
    "let",
    "put",
    "tr",
    "train",
    "went",
    "value",
    "one",
    "values",
    "generated",
    "bunch",
    "columns",
    "60",
    "across",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "need",
    "top",
    "60",
    "go",
    "new",
    "data",
    "find",
    "looking",
    "going",
    "go",
    "ahead",
    "load",
    "test",
    "data",
    "test",
    "data",
    "process",
    "yet",
    "real",
    "stock",
    "price",
    "data",
    "set",
    "test",
    "thing",
    "prepped",
    "data",
    "first",
    "place",
    "let",
    "go",
    "ahead",
    "go",
    "code",
    "see",
    "labeled",
    "part",
    "three",
    "making",
    "predictions",
    "visualizing",
    "results",
    "first",
    "thing",
    "need",
    "go",
    "ahead",
    "read",
    "data",
    "test",
    "csv",
    "see",
    "changed",
    "path",
    "computer",
    "call",
    "real",
    "stock",
    "price",
    "one",
    "column",
    "values",
    "location",
    "rows",
    "values",
    "one",
    "location",
    "open",
    "stock",
    "open",
    "let",
    "go",
    "ahead",
    "run",
    "loaded",
    "let",
    "go",
    "ahead",
    "create",
    "inputs",
    "going",
    "create",
    "inputs",
    "would",
    "look",
    "familiar",
    "thing",
    "going",
    "take",
    "data",
    "set",
    "total",
    "going",
    "little",
    "panda",
    "concat",
    "datastate",
    "train",
    "remember",
    "end",
    "data",
    "set",
    "train",
    "part",
    "data",
    "going",
    "let",
    "visualize",
    "little",
    "bit",
    "train",
    "data",
    "let",
    "put",
    "tr",
    "train",
    "went",
    "value",
    "one",
    "values",
    "generated",
    "bunch",
    "columns",
    "60",
    "across",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "value",
    "equals",
    "one",
    "need",
    "top",
    "60",
    "go",
    "new",
    "data",
    "part",
    "next",
    "data",
    "actually",
    "top",
    "first",
    "setup",
    "going",
    "real",
    "stock",
    "price",
    "going",
    "take",
    "data",
    "set",
    "test",
    "going",
    "load",
    "real",
    "stock",
    "price",
    "data",
    "test",
    "test",
    "location",
    "looking",
    "first",
    "column",
    "open",
    "price",
    "data",
    "set",
    "total",
    "going",
    "take",
    "pandas",
    "going",
    "concat",
    "going",
    "take",
    "data",
    "set",
    "train",
    "open",
    "dataset",
    "test",
    "open",
    "one",
    "way",
    "reference",
    "columns",
    "referenced",
    "couple",
    "different",
    "ways",
    "referenced",
    "one",
    "two",
    "know",
    "labeled",
    "panda",
    "set",
    "open",
    "pandas",
    "great",
    "way",
    "lots",
    "versatility",
    "go",
    "ahead",
    "go",
    "back",
    "run",
    "go",
    "notice",
    "open",
    "data",
    "set",
    "pended",
    "two",
    "different",
    "concatenated",
    "two",
    "data",
    "sets",
    "together",
    "inputs",
    "equals",
    "data",
    "set",
    "total",
    "length",
    "data",
    "set",
    "total",
    "minus",
    "length",
    "data",
    "set",
    "minus",
    "test",
    "minus",
    "60",
    "values",
    "going",
    "run",
    "see",
    "works",
    "normally",
    "running",
    "test",
    "set",
    "versus",
    "training",
    "set",
    "run",
    "completely",
    "separate",
    "graph",
    "see",
    "going",
    "looking",
    "part",
    "train",
    "see",
    "well",
    "graphs",
    "inputs",
    "equals",
    "inputs",
    "dot",
    "reshapes",
    "reshaping",
    "like",
    "transforming",
    "inputs",
    "remember",
    "transform",
    "zero",
    "one",
    "uh",
    "finally",
    "want",
    "go",
    "ahead",
    "take",
    "x",
    "test",
    "going",
    "create",
    "x",
    "test",
    "range",
    "60",
    "x",
    "test",
    "appending",
    "inputs",
    "60",
    "remember",
    "0",
    "59",
    "comma",
    "0",
    "side",
    "first",
    "column",
    "open",
    "column",
    "take",
    "x",
    "test",
    "convert",
    "numpy",
    "array",
    "reshape",
    "get",
    "final",
    "two",
    "lines",
    "something",
    "new",
    "right",
    "last",
    "two",
    "lines",
    "let",
    "highlight",
    "mark",
    "predicted",
    "stock",
    "price",
    "equals",
    "regressor",
    "dot",
    "predicts",
    "x",
    "test",
    "predicting",
    "stock",
    "including",
    "training",
    "testing",
    "model",
    "want",
    "take",
    "prediction",
    "want",
    "inverse",
    "transform",
    "remember",
    "put",
    "0",
    "well",
    "going",
    "mean",
    "much",
    "look",
    "float",
    "number",
    "0",
    "1",
    "want",
    "dollar",
    "amount",
    "want",
    "know",
    "cash",
    "value",
    "go",
    "ahead",
    "run",
    "see",
    "runs",
    "much",
    "quicker",
    "training",
    "wonderful",
    "neural",
    "networks",
    "put",
    "together",
    "takes",
    "second",
    "run",
    "neural",
    "network",
    "took",
    "us",
    "half",
    "hour",
    "train",
    "add",
    "plot",
    "data",
    "going",
    "plot",
    "think",
    "going",
    "going",
    "plot",
    "real",
    "data",
    "google",
    "stock",
    "actually",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "code",
    "let",
    "uh",
    "pull",
    "code",
    "plt",
    "oh",
    "remember",
    "beginning",
    "let",
    "go",
    "back",
    "top",
    "matte",
    "plot",
    "library",
    "dot",
    "pi",
    "plot",
    "plt",
    "comes",
    "come",
    "going",
    "plot",
    "let",
    "get",
    "drawing",
    "thing",
    "going",
    "go",
    "ahead",
    "plt",
    "basically",
    "kind",
    "like",
    "object",
    "one",
    "things",
    "always",
    "graphs",
    "python",
    "always",
    "think",
    "create",
    "object",
    "loads",
    "class",
    "well",
    "case",
    "plt",
    "like",
    "canvas",
    "putting",
    "stuff",
    "done",
    "html5",
    "canvas",
    "object",
    "canvas",
    "going",
    "plot",
    "real",
    "stock",
    "price",
    "actually",
    "going",
    "give",
    "color",
    "red",
    "going",
    "bright",
    "red",
    "going",
    "label",
    "real",
    "google",
    "stock",
    "price",
    "going",
    "predicted",
    "stock",
    "going",
    "blue",
    "going",
    "labeled",
    "predicted",
    "give",
    "title",
    "always",
    "nice",
    "give",
    "title",
    "graph",
    "especially",
    "going",
    "present",
    "somebody",
    "know",
    "shareholders",
    "office",
    "x",
    "label",
    "going",
    "time",
    "time",
    "series",
    "actually",
    "put",
    "actual",
    "date",
    "times",
    "fine",
    "know",
    "incremented",
    "time",
    "course",
    "label",
    "actual",
    "stock",
    "price",
    "tells",
    "us",
    "build",
    "legend",
    "color",
    "red",
    "real",
    "google",
    "stock",
    "price",
    "show",
    "plot",
    "shows",
    "us",
    "actual",
    "graph",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "looks",
    "like",
    "see",
    "nice",
    "graph",
    "let",
    "talk",
    "little",
    "bit",
    "graph",
    "wrap",
    "legend",
    "telling",
    "legend",
    "show",
    "prices",
    "title",
    "everything",
    "notice",
    "bottom",
    "time",
    "sequence",
    "put",
    "actual",
    "time",
    "could",
    "could",
    "gone",
    "ahead",
    "plotted",
    "x",
    "since",
    "know",
    "dates",
    "plotted",
    "dates",
    "also",
    "know",
    "last",
    "piece",
    "data",
    "looking",
    "last",
    "piece",
    "data",
    "somewhere",
    "probably",
    "around",
    "graph",
    "think",
    "like",
    "20",
    "percent",
    "data",
    "probably",
    "less",
    "google",
    "price",
    "google",
    "price",
    "little",
    "jump",
    "see",
    "actual",
    "google",
    "instead",
    "turn",
    "go",
    "high",
    "load",
    "go",
    "prediction",
    "pattern",
    "overall",
    "value",
    "pretty",
    "far",
    "far",
    "um",
    "stock",
    "looking",
    "one",
    "column",
    "looking",
    "open",
    "price",
    "looking",
    "many",
    "volumes",
    "traded",
    "like",
    "pointing",
    "earlier",
    "talk",
    "stock",
    "right",
    "bat",
    "six",
    "columns",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "weather",
    "mean",
    "volume",
    "shares",
    "adjusted",
    "open",
    "adjusted",
    "high",
    "adjusted",
    "low",
    "adjusted",
    "close",
    "special",
    "formula",
    "predict",
    "exactly",
    "would",
    "really",
    "worth",
    "based",
    "value",
    "stock",
    "kinds",
    "stuff",
    "put",
    "looking",
    "one",
    "small",
    "aspect",
    "opening",
    "price",
    "stock",
    "see",
    "pretty",
    "good",
    "job",
    "curve",
    "follows",
    "curve",
    "pretty",
    "well",
    "like",
    "little",
    "jumps",
    "bins",
    "quite",
    "match",
    "bin",
    "quite",
    "match",
    "bin",
    "pretty",
    "darn",
    "close",
    "basic",
    "shape",
    "prediction",
    "far",
    "imagine",
    "add",
    "data",
    "look",
    "different",
    "aspects",
    "specific",
    "domain",
    "stock",
    "able",
    "get",
    "better",
    "representation",
    "time",
    "drill",
    "deeper",
    "course",
    "took",
    "half",
    "hour",
    "program",
    "computer",
    "train",
    "imagine",
    "running",
    "across",
    "different",
    "variables",
    "might",
    "take",
    "little",
    "bit",
    "longer",
    "train",
    "data",
    "good",
    "quick",
    "tutorial",
    "like",
    "reached",
    "end",
    "complete",
    "neural",
    "networks",
    "tutorial",
    "hope",
    "enjoyed",
    "video",
    "like",
    "share",
    "thank",
    "watching",
    "stay",
    "tuned",
    "simply",
    "learn",
    "music"
  ],
  "keywords": [
    "neural",
    "networks",
    "used",
    "deep",
    "learning",
    "structure",
    "function",
    "human",
    "brain",
    "find",
    "learn",
    "example",
    "need",
    "network",
    "tutorial",
    "make",
    "sure",
    "hit",
    "first",
    "understand",
    "see",
    "back",
    "propagation",
    "gradient",
    "works",
    "finally",
    "convolutional",
    "recurrent",
    "let",
    "look",
    "small",
    "better",
    "last",
    "even",
    "though",
    "us",
    "could",
    "read",
    "way",
    "google",
    "one",
    "applications",
    "form",
    "machine",
    "take",
    "data",
    "train",
    "predict",
    "outputs",
    "new",
    "set",
    "done",
    "made",
    "layers",
    "neurons",
    "processing",
    "units",
    "input",
    "layer",
    "output",
    "final",
    "hidden",
    "image",
    "28",
    "pixels",
    "pixel",
    "fed",
    "neuron",
    "connected",
    "next",
    "assigned",
    "value",
    "weight",
    "inputs",
    "multiplied",
    "weights",
    "sum",
    "called",
    "bias",
    "added",
    "activation",
    "particular",
    "get",
    "forward",
    "values",
    "probability",
    "predicted",
    "course",
    "know",
    "prediction",
    "figure",
    "note",
    "trained",
    "training",
    "process",
    "also",
    "actual",
    "error",
    "indicates",
    "give",
    "change",
    "reduce",
    "information",
    "based",
    "adjusted",
    "multiple",
    "correctly",
    "end",
    "might",
    "long",
    "takes",
    "time",
    "recognition",
    "person",
    "features",
    "play",
    "lines",
    "stock",
    "high",
    "accuracy",
    "true",
    "functions",
    "b",
    "c",
    "place",
    "still",
    "taking",
    "steps",
    "big",
    "models",
    "answer",
    "would",
    "like",
    "lot",
    "coming",
    "part",
    "today",
    "team",
    "want",
    "artificial",
    "work",
    "start",
    "help",
    "think",
    "behind",
    "ever",
    "since",
    "trying",
    "use",
    "build",
    "computer",
    "able",
    "different",
    "images",
    "looking",
    "dive",
    "box",
    "go",
    "kind",
    "comes",
    "gets",
    "photos",
    "something",
    "actually",
    "really",
    "says",
    "looks",
    "dog",
    "identify",
    "notice",
    "bit",
    "term",
    "call",
    "put",
    "things",
    "running",
    "settings",
    "going",
    "model",
    "remember",
    "talk",
    "real",
    "little",
    "come",
    "cell",
    "happens",
    "goes",
    "array",
    "white",
    "say",
    "picture",
    "add",
    "world",
    "system",
    "pattern",
    "operation",
    "12",
    "30",
    "sentence",
    "consider",
    "word",
    "probably",
    "w",
    "feed",
    "random",
    "said",
    "special",
    "always",
    "three",
    "less",
    "good",
    "well",
    "times",
    "x",
    "multiply",
    "together",
    "quite",
    "makes",
    "separate",
    "every",
    "specific",
    "certainly",
    "labels",
    "many",
    "people",
    "making",
    "space",
    "common",
    "working",
    "finding",
    "huge",
    "couple",
    "dig",
    "deeper",
    "stuff",
    "right",
    "current",
    "ones",
    "amount",
    "convert",
    "mean",
    "problem",
    "packages",
    "instead",
    "using",
    "kinds",
    "wonderful",
    "step",
    "whatever",
    "uses",
    "getting",
    "cool",
    "jump",
    "tools",
    "already",
    "side",
    "everything",
    "imagine",
    "keep",
    "open",
    "far",
    "ahead",
    "nice",
    "usually",
    "red",
    "feature",
    "show",
    "two",
    "got",
    "thing",
    "gone",
    "car",
    "number",
    "range",
    "zero",
    "black",
    "pretty",
    "close",
    "arrays",
    "dimension",
    "setup",
    "python",
    "almost",
    "fit",
    "case",
    "important",
    "x1",
    "nodes",
    "node",
    "another",
    "dot",
    "worry",
    "middle",
    "math",
    "sigmoid",
    "plus",
    "minus",
    "1",
    "0",
    "oh",
    "equal",
    "relu",
    "piece",
    "tangent",
    "much",
    "commonly",
    "reason",
    "putting",
    "point",
    "float",
    "order",
    "numbers",
    "four",
    "five",
    "six",
    "eight",
    "nine",
    "large",
    "sets",
    "uh",
    "8",
    "3",
    "code",
    "correct",
    "top",
    "bottom",
    "adjust",
    "complicated",
    "talked",
    "means",
    "create",
    "points",
    "try",
    "dimensions",
    "map",
    "adding",
    "ways",
    "pieces",
    "batches",
    "filter",
    "parts",
    "convolution",
    "went",
    "edge",
    "maybe",
    "match",
    "bring",
    "cat",
    "pictures",
    "okay",
    "modules",
    "load",
    "cross",
    "import",
    "sequential",
    "2d",
    "max",
    "pooling",
    "flatten",
    "dense",
    "second",
    "tensorflow",
    "talking",
    "size",
    "quick",
    "anaconda",
    "created",
    "numpy",
    "standard",
    "36",
    "jupiter",
    "notebook",
    "guess",
    "single",
    "anything",
    "run",
    "changed",
    "initialize",
    "cnn",
    "classifier",
    "whole",
    "shape",
    "equals",
    "64",
    "matches",
    "reshape",
    "um",
    "dimensional",
    "2",
    "setting",
    "whether",
    "flattened",
    "full",
    "pull",
    "took",
    "gon",
    "na",
    "optimizer",
    "atom",
    "best",
    "testing",
    "fine",
    "loss",
    "squared",
    "object",
    "255",
    "basic",
    "creating",
    "batch",
    "class",
    "depending",
    "test",
    "pulling",
    "000",
    "percent",
    "per",
    "epic",
    "25",
    "validation",
    "10",
    "4",
    "half",
    "tool",
    "file",
    "60",
    "across",
    "print",
    "pandas",
    "6",
    "plot",
    "graph",
    "slope",
    "positive",
    "negative",
    "variable",
    "bird",
    "matrixes",
    "matrix",
    "rectified",
    "filters",
    "beak",
    "color",
    "library",
    "matplot",
    "32",
    "memory",
    "loaded",
    "oops",
    "helper",
    "words",
    "self",
    "variables",
    "total",
    "100",
    "tf",
    "dropout",
    "session",
    "50",
    "lstm",
    "rnn",
    "sequence",
    "previous",
    "h",
    "state",
    "john",
    "football",
    "brave",
    "price",
    "location"
  ]
}