{
  "text": "[MUSIC PLAYING]\nJOSH GORDON: Hey, everyone.\nWelcome back.\nFeatures are the way you\nrepresent your knowledge\nabout the world\nfor the classifier,\nand today I'll walk\nyou through techniques\nyou can use to represent\nyour features and utilities\nTensorFlow provides to help.\nYou use a dataset from the\nUS census as an example,\nand the goal is to predict\nif someone's income is\ngreater than $50,000 based\non attributes like their age\nand occupation.\nThe dataset is\nstored as a CSV file,\nand previously we've seen how to\nuse the column values directly\nas features.\nBut today we'll use\nfeature engineering\nto transform them into a\nmore useful representation.\nAs we go, I'll visualize\nwhat these transformations do\nusing a tool called Facets,\nand you can find a link to it\nin the description.\nYou'll also find complete code\nto train a TensorFlow estimator\non this dataset.\nOK, let's get started.\nLet's begin with a numeric\nattribute like age,\nand think about how we can\nuse it to predict income.\nNow if you think about how\nage correlates with income,\nour first intuition is\nthat as age increases,\nusually so does income.\nAnd the simplest way\nto represent this\nwould just be to take\nthe raw numeric value\nand use that as a feature.\nHere we're building\na list of features\nwe use to train the\nmodel, and each of these\nis stored as a feature column.\nThis contains data about\nthe column from the CSV file\nand how to represent it.\nHere we'll write a feature that\njust uses the raw value of age,\nand this string corresponds\nto a column in the CSV file.\nNow what can go wrong\nwith this approach?\nWell, if we think more\nclosely about age,\nwe realize it's not in a linear\nrelationship with income.\nThe curve might look\nsomething like this.\nIt's flat for children, then\nincreases during working age,\nand decreases during retirement.\nA linear classifier,\nfor example,\nis unable to capture\nthis relationship.\nThat's because it learns a\nsingle weight for each feature.\nTo make it easier for the\nclassifier, one thing we can do\nis bucket the feature.\nAnd bucketing transforms\na numeric feature\ninto several\ncategorical ones based\non the range it falls into,\nand each of these new features\nindicate whether a person's\nage falls into that range.\nAnd now a linear model can\ncapture the relationship\nby learning different\nweights for each bucket.\nLet's see how this\nlooks in Facets.\nConveniently,\nthere's a live demo\nthat runs in the browser with\nour census data preloaded,\nand each individual from\nthe CSV is visualized\nas a dot colored by income.\nIf you click on a dot, you can\nsee stats about the person.\nNow let's bucket\nby age, and you can\nadjust the number of buckets to\nmake it more or less granular.\nHow you choose the number\nof buckets is up to you,\nand ideally, you'd want to use\nyour knowledge of the problem\nto do this well.\nIn TensorFlow, we can\ncreate a bucketized feature\nby wrapping a numeric\ncolumn from the CSV.\nAnd here we're\nspecifying the number\nand the ranges of the\nbuckets we'd like created.\nOnce this is done, we can\nadd the bucketized feature\nto the list used\nto train our model.\nNow let's see how to represent\na categorical feature,\nand I'll use the education\ncolumn as an example.\nBecause there are\nonly a few values,\nthe best way to represent this\nis just use the raw value.\nAnd here we'll create\na feature column\nthat says education can be a\nsingle value from this list.\nOf course, you could also read\nthe values from a file on disk\nrather than writing\nthem out in code.\nNow using the raw value\nis the right thing\nto do when there are only a\nsmall number of possibilities.\nWe'll cover the case\nwhere there are thousands\nof possibilities in a moment.\nFirst, let's take a look\nat feature crossing.\nFeature crossing is a way\nto create new features that\nare combinations\nof existing ones,\nand these can be especially\nhelpful to linear classifiers,\nwhich can't model\ninteractions between features.\nHere's what this\nlooks like in Facets.\nI'll take our age\nbuckets from before\nand cross them with education.\nUnder the hood, you can think\nof a true-false feature being\ncreated for each\nbucket that tells\nthe classifier whether\nan individual falls\ninto that range.\nNow these buckets\ncan be informative,\nand here we see some groups are\nlikely to have a high income,\nand others low.\nIn code, using a feature cross\nworks the same way as before.\nWe'll cross our age\nbuckets with education\nand add it to the list\nof features to use.\nA feature cross can generate\nmany possibilities quickly,\nwhich is why they\nare often represented\nunder the hood with a hash.\nA hashed feature column is one\nway to efficiently represent\na categorical feature\nwith a large vocabulary.\nMore importantly,\nyou can use these\nas a way to make\nyour data easier\nto work with because\nthey free you from having\nto provide a vocabulary list.\nIn this example, we'll\nrepresent the occupation column\nfrom our CSV file\nby using a hash\nwith 1,000 possible values.\nNotice we don't have to\nprovide a vocabulary list,\nand to avoid collisions,\nI've set the hash size\nso it's larger than the number\nof items in the vocabulary.\nHere's how this\nworks under the hood.\nNormally, a categorical\nfeature is represented\nas a one hot encoding.\nThat means there's one bit\nfor each possible value\nin the vocabulary.\nAnd we can create a lookup\nbecause we know the vocabulary\nlist in advance.\nNow if we don't\nknow the vocab, we\ncan use a hash function to\ncompute the bit automatically.\nThe downside is there\ncould be collisions,\nmeaning different items are\nmapped to the same value.\nHashes can also be used\nto limit memory usage\nat the cost of adding some\nnoise to your training data.\nIf you have a large\nvocabulary, it\ncan be memory intensive\nto use that as input\nto a neural network.\nA hashed column can\nbe used to limit\nthe maximum number\nof possibilities,\nbut I prefer them\nsimply as a tool\nto save you programming time.\nFinally, I'd like to\nmention embeddings,\nand these can be less intuitive\nthan the other techniques,\nbut they're a powerful way\nto work with categorical data\nin a deep learning setting.\nYou can think of an embedding\nas a vector that represents\nthe meaning of a word.\nAnd we can visualize a\ndataset of word embeddings\nusing the TensorFlow\nEmbedding Projector,\nand there's an online demo you\ncan find in the description.\nHere we're looking at a dataset\nof 10,000 words, each of which\nis represented by a vector\nwith many dimensions,\nprojected down to 3D\nso we can see them.\nYou can search for words\nin the box to the right.\nAnd if you experiment\na bit, you'll\nfind similar words are\noften close together.\nFor example, all of the words\nin this cluster are cities.\nWhat's neat about\nembeddings is that they're\nlearned automatically in the\nprocess of training a DNN.\nAnd to make that happen,\nall you need to do\nis write an embedding column.\nHere we'll create an\nembedding for education\nwith 10 dimensions.\nNow embeddings are helpful if\nyou have a categorical column\nwith a large\nvocabulary and you want\nto compress the representation\nso the classifier learns\ngeneral concepts\nrather than memorizing\nthe meaning of specific words.\nFor example, imagine\nif the census data\nhad a column called job title.\nThere are thousands\nof different jobs,\nand an embedding could be used\nto help your classifier learn\nthat words like programmer\nand software engineer\noften mean the same thing.\nOK, hope this was\na helpful intro,\nand thinking about how to\nrepresent your features\nis one of the most\nimportant contributions\nyou can make to a machine\nlearning experiment.\nFeature columns are\ngreat because they\nlet you experiment with\ndifferent representations\nin code and make advanced\nfeatures like embeddings\naccessible.\nAs a next step,\nI'd recommend you\ntry the code in the description\nand see if you can modify it\nfor a problem you care about.\nThanks for watching everyone,\nand I'll see you next time.\n[MUSIC PLAYING]\n",
  "words": [
    "music",
    "playing",
    "josh",
    "gordon",
    "hey",
    "everyone",
    "welcome",
    "back",
    "features",
    "way",
    "represent",
    "knowledge",
    "world",
    "classifier",
    "today",
    "walk",
    "techniques",
    "use",
    "represent",
    "features",
    "utilities",
    "tensorflow",
    "provides",
    "help",
    "use",
    "dataset",
    "us",
    "census",
    "example",
    "goal",
    "predict",
    "someone",
    "income",
    "greater",
    "based",
    "attributes",
    "like",
    "age",
    "occupation",
    "dataset",
    "stored",
    "csv",
    "file",
    "previously",
    "seen",
    "use",
    "column",
    "values",
    "directly",
    "features",
    "today",
    "use",
    "feature",
    "engineering",
    "transform",
    "useful",
    "representation",
    "go",
    "visualize",
    "transformations",
    "using",
    "tool",
    "called",
    "facets",
    "find",
    "link",
    "description",
    "also",
    "find",
    "complete",
    "code",
    "train",
    "tensorflow",
    "estimator",
    "dataset",
    "ok",
    "let",
    "get",
    "started",
    "let",
    "begin",
    "numeric",
    "attribute",
    "like",
    "age",
    "think",
    "use",
    "predict",
    "income",
    "think",
    "age",
    "correlates",
    "income",
    "first",
    "intuition",
    "age",
    "increases",
    "usually",
    "income",
    "simplest",
    "way",
    "represent",
    "would",
    "take",
    "raw",
    "numeric",
    "value",
    "use",
    "feature",
    "building",
    "list",
    "features",
    "use",
    "train",
    "model",
    "stored",
    "feature",
    "column",
    "contains",
    "data",
    "column",
    "csv",
    "file",
    "represent",
    "write",
    "feature",
    "uses",
    "raw",
    "value",
    "age",
    "string",
    "corresponds",
    "column",
    "csv",
    "file",
    "go",
    "wrong",
    "approach",
    "well",
    "think",
    "closely",
    "age",
    "realize",
    "linear",
    "relationship",
    "income",
    "curve",
    "might",
    "look",
    "something",
    "like",
    "flat",
    "children",
    "increases",
    "working",
    "age",
    "decreases",
    "retirement",
    "linear",
    "classifier",
    "example",
    "unable",
    "capture",
    "relationship",
    "learns",
    "single",
    "weight",
    "feature",
    "make",
    "easier",
    "classifier",
    "one",
    "thing",
    "bucket",
    "feature",
    "bucketing",
    "transforms",
    "numeric",
    "feature",
    "several",
    "categorical",
    "ones",
    "based",
    "range",
    "falls",
    "new",
    "features",
    "indicate",
    "whether",
    "person",
    "age",
    "falls",
    "range",
    "linear",
    "model",
    "capture",
    "relationship",
    "learning",
    "different",
    "weights",
    "bucket",
    "let",
    "see",
    "looks",
    "facets",
    "conveniently",
    "live",
    "demo",
    "runs",
    "browser",
    "census",
    "data",
    "preloaded",
    "individual",
    "csv",
    "visualized",
    "dot",
    "colored",
    "income",
    "click",
    "dot",
    "see",
    "stats",
    "person",
    "let",
    "bucket",
    "age",
    "adjust",
    "number",
    "buckets",
    "make",
    "less",
    "granular",
    "choose",
    "number",
    "buckets",
    "ideally",
    "want",
    "use",
    "knowledge",
    "problem",
    "well",
    "tensorflow",
    "create",
    "bucketized",
    "feature",
    "wrapping",
    "numeric",
    "column",
    "csv",
    "specifying",
    "number",
    "ranges",
    "buckets",
    "like",
    "created",
    "done",
    "add",
    "bucketized",
    "feature",
    "list",
    "used",
    "train",
    "model",
    "let",
    "see",
    "represent",
    "categorical",
    "feature",
    "use",
    "education",
    "column",
    "example",
    "values",
    "best",
    "way",
    "represent",
    "use",
    "raw",
    "value",
    "create",
    "feature",
    "column",
    "says",
    "education",
    "single",
    "value",
    "list",
    "course",
    "could",
    "also",
    "read",
    "values",
    "file",
    "disk",
    "rather",
    "writing",
    "code",
    "using",
    "raw",
    "value",
    "right",
    "thing",
    "small",
    "number",
    "possibilities",
    "cover",
    "case",
    "thousands",
    "possibilities",
    "moment",
    "first",
    "let",
    "take",
    "look",
    "feature",
    "crossing",
    "feature",
    "crossing",
    "way",
    "create",
    "new",
    "features",
    "combinations",
    "existing",
    "ones",
    "especially",
    "helpful",
    "linear",
    "classifiers",
    "ca",
    "model",
    "interactions",
    "features",
    "looks",
    "like",
    "facets",
    "take",
    "age",
    "buckets",
    "cross",
    "education",
    "hood",
    "think",
    "feature",
    "created",
    "bucket",
    "tells",
    "classifier",
    "whether",
    "individual",
    "falls",
    "range",
    "buckets",
    "informative",
    "see",
    "groups",
    "likely",
    "high",
    "income",
    "others",
    "low",
    "code",
    "using",
    "feature",
    "cross",
    "works",
    "way",
    "cross",
    "age",
    "buckets",
    "education",
    "add",
    "list",
    "features",
    "use",
    "feature",
    "cross",
    "generate",
    "many",
    "possibilities",
    "quickly",
    "often",
    "represented",
    "hood",
    "hash",
    "hashed",
    "feature",
    "column",
    "one",
    "way",
    "efficiently",
    "represent",
    "categorical",
    "feature",
    "large",
    "vocabulary",
    "importantly",
    "use",
    "way",
    "make",
    "data",
    "easier",
    "work",
    "free",
    "provide",
    "vocabulary",
    "list",
    "example",
    "represent",
    "occupation",
    "column",
    "csv",
    "file",
    "using",
    "hash",
    "possible",
    "values",
    "notice",
    "provide",
    "vocabulary",
    "list",
    "avoid",
    "collisions",
    "set",
    "hash",
    "size",
    "larger",
    "number",
    "items",
    "vocabulary",
    "works",
    "hood",
    "normally",
    "categorical",
    "feature",
    "represented",
    "one",
    "hot",
    "encoding",
    "means",
    "one",
    "bit",
    "possible",
    "value",
    "vocabulary",
    "create",
    "lookup",
    "know",
    "vocabulary",
    "list",
    "advance",
    "know",
    "vocab",
    "use",
    "hash",
    "function",
    "compute",
    "bit",
    "automatically",
    "downside",
    "could",
    "collisions",
    "meaning",
    "different",
    "items",
    "mapped",
    "value",
    "hashes",
    "also",
    "used",
    "limit",
    "memory",
    "usage",
    "cost",
    "adding",
    "noise",
    "training",
    "data",
    "large",
    "vocabulary",
    "memory",
    "intensive",
    "use",
    "input",
    "neural",
    "network",
    "hashed",
    "column",
    "used",
    "limit",
    "maximum",
    "number",
    "possibilities",
    "prefer",
    "simply",
    "tool",
    "save",
    "programming",
    "time",
    "finally",
    "like",
    "mention",
    "embeddings",
    "less",
    "intuitive",
    "techniques",
    "powerful",
    "way",
    "work",
    "categorical",
    "data",
    "deep",
    "learning",
    "setting",
    "think",
    "embedding",
    "vector",
    "represents",
    "meaning",
    "word",
    "visualize",
    "dataset",
    "word",
    "embeddings",
    "using",
    "tensorflow",
    "embedding",
    "projector",
    "online",
    "demo",
    "find",
    "description",
    "looking",
    "dataset",
    "words",
    "represented",
    "vector",
    "many",
    "dimensions",
    "projected",
    "3d",
    "see",
    "search",
    "words",
    "box",
    "right",
    "experiment",
    "bit",
    "find",
    "similar",
    "words",
    "often",
    "close",
    "together",
    "example",
    "words",
    "cluster",
    "cities",
    "neat",
    "embeddings",
    "learned",
    "automatically",
    "process",
    "training",
    "dnn",
    "make",
    "happen",
    "need",
    "write",
    "embedding",
    "column",
    "create",
    "embedding",
    "education",
    "10",
    "dimensions",
    "embeddings",
    "helpful",
    "categorical",
    "column",
    "large",
    "vocabulary",
    "want",
    "compress",
    "representation",
    "classifier",
    "learns",
    "general",
    "concepts",
    "rather",
    "memorizing",
    "meaning",
    "specific",
    "words",
    "example",
    "imagine",
    "census",
    "data",
    "column",
    "called",
    "job",
    "title",
    "thousands",
    "different",
    "jobs",
    "embedding",
    "could",
    "used",
    "help",
    "classifier",
    "learn",
    "words",
    "like",
    "programmer",
    "software",
    "engineer",
    "often",
    "mean",
    "thing",
    "ok",
    "hope",
    "helpful",
    "intro",
    "thinking",
    "represent",
    "features",
    "one",
    "important",
    "contributions",
    "make",
    "machine",
    "learning",
    "experiment",
    "feature",
    "columns",
    "great",
    "let",
    "experiment",
    "different",
    "representations",
    "code",
    "make",
    "advanced",
    "features",
    "like",
    "embeddings",
    "accessible",
    "next",
    "step",
    "recommend",
    "try",
    "code",
    "description",
    "see",
    "modify",
    "problem",
    "care",
    "thanks",
    "watching",
    "everyone",
    "see",
    "next",
    "time",
    "music",
    "playing"
  ],
  "keywords": [
    "features",
    "way",
    "represent",
    "classifier",
    "use",
    "tensorflow",
    "dataset",
    "census",
    "example",
    "income",
    "like",
    "age",
    "csv",
    "file",
    "column",
    "values",
    "feature",
    "using",
    "facets",
    "find",
    "description",
    "also",
    "code",
    "train",
    "let",
    "numeric",
    "think",
    "take",
    "raw",
    "value",
    "list",
    "model",
    "data",
    "linear",
    "relationship",
    "make",
    "one",
    "thing",
    "bucket",
    "categorical",
    "range",
    "falls",
    "learning",
    "different",
    "see",
    "number",
    "buckets",
    "create",
    "used",
    "education",
    "could",
    "possibilities",
    "helpful",
    "cross",
    "hood",
    "often",
    "represented",
    "hash",
    "large",
    "vocabulary",
    "bit",
    "meaning",
    "embeddings",
    "embedding",
    "words",
    "experiment"
  ]
}