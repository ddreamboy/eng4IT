{
  "text": "today we are going to discuss how\nrecurrent neural networks work I'm\nassuming you know how a regular neural\nnetwork works in a regular neural\nnetwork the input passes through the\ninput layer to a hidden layer and then\nto the output layer of course there can\nbe many hidden layers A recurrent neural\nnetwork or RNN differs from a regular\nneural network the RN specifically\nhandles sequence data whereas a regular\nneural network accepts all features of a\ndata point together at a time the\nsequence or in what order the data\npoints are entered to the network does\nnot matter much in a regular neural\nnetwork so the RNN can take a sequence\none element after another and learn from\nthat sequence here is the abstract form\nof an RNN as the input of an RNN first\none input element of the sequence comes\nthe element is passed through the hidden\nlayer the outcome of the Hidden layer is\npassed through activation to process\noutput not only that a simple but\nelegant thing is done with the outcome\nof the Hidden layer whatever comes out\nof the Hidden layer is fed back with the\nnext input that is the next input to the\nRNN receives the hidden state of the\nprevious input let us go with an example\nlet us say that our sequence is I eat\nrice the first word is the word I the\nword I goes in as the input the hidden\nlayer output is passed through whatever\nactivation function you have but before\nthat the hidden state is fed back to the\ncell with the word eat with the word I\nthe hidden state was either random or\nall zeros it does not matter much\nbecause I was the first word but now\nwith the next word eat the hidden state\nof the word I is\nprocessed after they go through the\nhidden layer the output of the Hidden\nlayer contains processed information\nabout the word eat not only that it also\ncontains processed information about the\nword I as the word I's hidden state was\nfed with the word eat that means so far\nthe latest hidden state is keeping\ninformation about I eat the next word is\nrice rice goes in with the latest hidden\nstate which contains information about I\neat the new hidden State coming out of\nthe Hidden layer will contain\ninformation about I eat rice now the\nexpected output eat for the input word I\nand the output rice for the input word\neat will help determine the error of the\nRNN which can be used for back\npropagation let us discuss the math\nwhich will clarify it further let us\ncall each word a Time step the word I is\nat time step one word eat is at time\nstep two word rice is at time step three\nlet us say that the word at time step T\nis represented by a vector XT\nthe word Vector XT becomes the input and\nis Multiplied with the weight Matrix U\njust like the regular neural network\nwhat will be the size of the Matrix U it\nis the weight Matrix of the rnn's input\nelement so the size of U will be the\nlength of the word Vector XT times the\nnumber of neurons in the hidden State\nthe previous time step had an output\nfrom the hidden layer which we call y t\nminus1 which is another Vector that also\nbecomes an input the length of YT minus1\nis equal to the number of neurons in the\nhidden layer obviously because y t minus\none is the outcome of the Hidden layer\nthe vector y t minus1 is Multiplied with\nanother weight Matrix W the size of w is\nthe length of y t minus1 times the\nnumber of neurons in the hidden layer\nnote again y t minus1 has a length equal\nto the number of neurons in the h hidden\nlayer therefore the weight Matrix W is a\nsquare Matrix W * previous hidden state\nis added with u * input word vector and\nit is added with the bias term\nB the length of the bias term B in an\nRNN is equal to the number of neurons in\nthe hidden layer during training the\nnetwork learns the optimal values for\nthe weights w u and B in rnns how you\nprocess the final output for error\ncalculation depends on the specific\napplication and the nature of the task\nfor instance in a language modeling\nscenario where the goal is to predict\nthe next word in a sequence the\nNetwork's output at each time step is\noften a probability distribution over\nthe vocabulary predicting the next word\nbased on the preceding context if the\ntask is to predict one specific word\ngiven a sequence of a certain length\nthen the error calculation or loss\nfunction would typically focus on the\naccuracy of the prediction at the\nspecific time step relevant to the task\nfor example with a sequence length of\ntwo words if the application is to\npredict the third word the model is\ntrained to minimize the error in\npredicting this third word the errors at\nintermediate steps such as predicting\nthe second word given the first might\nnot be the primary focus unless they\ncontribute to the overall learning\nobjective the overall output is the\nexpected next word that comes after the\nsequence so this part of the equation is\nsent through a nonlinear activation\nfunction here denoted as F which could\nbe tan H activation or relo function\nwhile processing a single sequence in a\nrecurrent neural network the weights Wu\nand the bias term B remain constant for\nthe sequence\nthe parameters u w and B are typically\nupdated via back propagation after\nprocessing a sequence of data what if\nyou have several thousand words in a\nsequence should we keep feeding the\nwords through the RN one after another\nwithout updating w u and B no rnns have\nserious issues rnns are particularly\nsusceptible to the problems of Vanishing\nand exploding gradients especially with\nlong sequences in such cases the\ngradients can either become too small or\ntoo large as they are propagated back\nthrough each time step of the sequence\nmaking it difficult to train the network\neffectively instead of repeatedly\nfeeding thousands of words without\nupdating the weights the common practice\nis to create chunks of the sequence and\ntrain for each chunk that is after each\nchunk update the weights w u and B by\nrunning the back propagation for the\nnext chunk use these updated Wu and B as\ninitialization keep doing this until\nthere is no more chunk left this\nchunking approach makes the RNN more\nmanageable however this approach might\ncause the loss of long-term dependencies\nthat span beyond the length of the\ntruncated sequences using more advanced\narchitectures like lstms or gruse is\ngenerally recommended for tasks where\nlong range dependencies are crucial we\nhave yet to discuss one small item what\nis\nXT we said earlier in the video that XT\nis a vector representation of a word yes\nthat is correct but where will we get\nthat Vector it is common to use an\nembedding layer before the RN unit to\ncreate the XT vectors which are the\ninput vectors for each time step to\nincorporate embedding each word in the\ntext is mapped to a unique integer an\nindex in the embedding Matrix the\nembedding layer uses these indices to\nlook up the corresponding embedding\nVector for each word these embedding\nvectors xtes are then passed as inputs\nto the RNN at each time step the\nembeddings in the embedding layer will\nbe updated during the back propagation\nprocess of an RNN provided that the\nembedding layer is set to be trainable\nwhen the RNN is trained on a task such\nas language modeling or text\nclassification the gradients calculated\nduring back propagation are used not\nonly to update the weights and biases of\nthe RNN but also to adjust the\nembeddings in the embedding layer note\nthat this RNN can not only be used for\ntext but also it can be used for time\nseries data both univariate and\nmultivariate for multivariate time\nseries the input Vector at each time\nstep can be conceptually similar to the\nembedding Vector of a word in natural\nlanguage processing\napplications that is the theory of r in\nshort see you in the next video for the\npython implementation of RNN as a text\ngenerator\n",
  "words": [
    "today",
    "going",
    "discuss",
    "recurrent",
    "neural",
    "networks",
    "work",
    "assuming",
    "know",
    "regular",
    "neural",
    "network",
    "works",
    "regular",
    "neural",
    "network",
    "input",
    "passes",
    "input",
    "layer",
    "hidden",
    "layer",
    "output",
    "layer",
    "course",
    "many",
    "hidden",
    "layers",
    "recurrent",
    "neural",
    "network",
    "rnn",
    "differs",
    "regular",
    "neural",
    "network",
    "rn",
    "specifically",
    "handles",
    "sequence",
    "data",
    "whereas",
    "regular",
    "neural",
    "network",
    "accepts",
    "features",
    "data",
    "point",
    "together",
    "time",
    "sequence",
    "order",
    "data",
    "points",
    "entered",
    "network",
    "matter",
    "much",
    "regular",
    "neural",
    "network",
    "rnn",
    "take",
    "sequence",
    "one",
    "element",
    "another",
    "learn",
    "sequence",
    "abstract",
    "form",
    "rnn",
    "input",
    "rnn",
    "first",
    "one",
    "input",
    "element",
    "sequence",
    "comes",
    "element",
    "passed",
    "hidden",
    "layer",
    "outcome",
    "hidden",
    "layer",
    "passed",
    "activation",
    "process",
    "output",
    "simple",
    "elegant",
    "thing",
    "done",
    "outcome",
    "hidden",
    "layer",
    "whatever",
    "comes",
    "hidden",
    "layer",
    "fed",
    "back",
    "next",
    "input",
    "next",
    "input",
    "rnn",
    "receives",
    "hidden",
    "state",
    "previous",
    "input",
    "let",
    "us",
    "go",
    "example",
    "let",
    "us",
    "say",
    "sequence",
    "eat",
    "rice",
    "first",
    "word",
    "word",
    "word",
    "goes",
    "input",
    "hidden",
    "layer",
    "output",
    "passed",
    "whatever",
    "activation",
    "function",
    "hidden",
    "state",
    "fed",
    "back",
    "cell",
    "word",
    "eat",
    "word",
    "hidden",
    "state",
    "either",
    "random",
    "zeros",
    "matter",
    "much",
    "first",
    "word",
    "next",
    "word",
    "eat",
    "hidden",
    "state",
    "word",
    "processed",
    "go",
    "hidden",
    "layer",
    "output",
    "hidden",
    "layer",
    "contains",
    "processed",
    "information",
    "word",
    "eat",
    "also",
    "contains",
    "processed",
    "information",
    "word",
    "word",
    "hidden",
    "state",
    "fed",
    "word",
    "eat",
    "means",
    "far",
    "latest",
    "hidden",
    "state",
    "keeping",
    "information",
    "eat",
    "next",
    "word",
    "rice",
    "rice",
    "goes",
    "latest",
    "hidden",
    "state",
    "contains",
    "information",
    "eat",
    "new",
    "hidden",
    "state",
    "coming",
    "hidden",
    "layer",
    "contain",
    "information",
    "eat",
    "rice",
    "expected",
    "output",
    "eat",
    "input",
    "word",
    "output",
    "rice",
    "input",
    "word",
    "eat",
    "help",
    "determine",
    "error",
    "rnn",
    "used",
    "back",
    "propagation",
    "let",
    "us",
    "discuss",
    "math",
    "clarify",
    "let",
    "us",
    "call",
    "word",
    "time",
    "step",
    "word",
    "time",
    "step",
    "one",
    "word",
    "eat",
    "time",
    "step",
    "two",
    "word",
    "rice",
    "time",
    "step",
    "three",
    "let",
    "us",
    "say",
    "word",
    "time",
    "step",
    "represented",
    "vector",
    "xt",
    "word",
    "vector",
    "xt",
    "becomes",
    "input",
    "multiplied",
    "weight",
    "matrix",
    "u",
    "like",
    "regular",
    "neural",
    "network",
    "size",
    "matrix",
    "u",
    "weight",
    "matrix",
    "rnn",
    "input",
    "element",
    "size",
    "u",
    "length",
    "word",
    "vector",
    "xt",
    "times",
    "number",
    "neurons",
    "hidden",
    "state",
    "previous",
    "time",
    "step",
    "output",
    "hidden",
    "layer",
    "call",
    "minus1",
    "another",
    "vector",
    "also",
    "becomes",
    "input",
    "length",
    "yt",
    "minus1",
    "equal",
    "number",
    "neurons",
    "hidden",
    "layer",
    "obviously",
    "minus",
    "one",
    "outcome",
    "hidden",
    "layer",
    "vector",
    "minus1",
    "multiplied",
    "another",
    "weight",
    "matrix",
    "w",
    "size",
    "w",
    "length",
    "minus1",
    "times",
    "number",
    "neurons",
    "hidden",
    "layer",
    "note",
    "minus1",
    "length",
    "equal",
    "number",
    "neurons",
    "h",
    "hidden",
    "layer",
    "therefore",
    "weight",
    "matrix",
    "w",
    "square",
    "matrix",
    "w",
    "previous",
    "hidden",
    "state",
    "added",
    "u",
    "input",
    "word",
    "vector",
    "added",
    "bias",
    "term",
    "b",
    "length",
    "bias",
    "term",
    "b",
    "rnn",
    "equal",
    "number",
    "neurons",
    "hidden",
    "layer",
    "training",
    "network",
    "learns",
    "optimal",
    "values",
    "weights",
    "w",
    "u",
    "b",
    "rnns",
    "process",
    "final",
    "output",
    "error",
    "calculation",
    "depends",
    "specific",
    "application",
    "nature",
    "task",
    "instance",
    "language",
    "modeling",
    "scenario",
    "goal",
    "predict",
    "next",
    "word",
    "sequence",
    "network",
    "output",
    "time",
    "step",
    "often",
    "probability",
    "distribution",
    "vocabulary",
    "predicting",
    "next",
    "word",
    "based",
    "preceding",
    "context",
    "task",
    "predict",
    "one",
    "specific",
    "word",
    "given",
    "sequence",
    "certain",
    "length",
    "error",
    "calculation",
    "loss",
    "function",
    "would",
    "typically",
    "focus",
    "accuracy",
    "prediction",
    "specific",
    "time",
    "step",
    "relevant",
    "task",
    "example",
    "sequence",
    "length",
    "two",
    "words",
    "application",
    "predict",
    "third",
    "word",
    "model",
    "trained",
    "minimize",
    "error",
    "predicting",
    "third",
    "word",
    "errors",
    "intermediate",
    "steps",
    "predicting",
    "second",
    "word",
    "given",
    "first",
    "might",
    "primary",
    "focus",
    "unless",
    "contribute",
    "overall",
    "learning",
    "objective",
    "overall",
    "output",
    "expected",
    "next",
    "word",
    "comes",
    "sequence",
    "part",
    "equation",
    "sent",
    "nonlinear",
    "activation",
    "function",
    "denoted",
    "f",
    "could",
    "tan",
    "h",
    "activation",
    "relo",
    "function",
    "processing",
    "single",
    "sequence",
    "recurrent",
    "neural",
    "network",
    "weights",
    "wu",
    "bias",
    "term",
    "b",
    "remain",
    "constant",
    "sequence",
    "parameters",
    "u",
    "w",
    "b",
    "typically",
    "updated",
    "via",
    "back",
    "propagation",
    "processing",
    "sequence",
    "data",
    "several",
    "thousand",
    "words",
    "sequence",
    "keep",
    "feeding",
    "words",
    "rn",
    "one",
    "another",
    "without",
    "updating",
    "w",
    "u",
    "b",
    "rnns",
    "serious",
    "issues",
    "rnns",
    "particularly",
    "susceptible",
    "problems",
    "vanishing",
    "exploding",
    "gradients",
    "especially",
    "long",
    "sequences",
    "cases",
    "gradients",
    "either",
    "become",
    "small",
    "large",
    "propagated",
    "back",
    "time",
    "step",
    "sequence",
    "making",
    "difficult",
    "train",
    "network",
    "effectively",
    "instead",
    "repeatedly",
    "feeding",
    "thousands",
    "words",
    "without",
    "updating",
    "weights",
    "common",
    "practice",
    "create",
    "chunks",
    "sequence",
    "train",
    "chunk",
    "chunk",
    "update",
    "weights",
    "w",
    "u",
    "b",
    "running",
    "back",
    "propagation",
    "next",
    "chunk",
    "use",
    "updated",
    "wu",
    "b",
    "initialization",
    "keep",
    "chunk",
    "left",
    "chunking",
    "approach",
    "makes",
    "rnn",
    "manageable",
    "however",
    "approach",
    "might",
    "cause",
    "loss",
    "dependencies",
    "span",
    "beyond",
    "length",
    "truncated",
    "sequences",
    "using",
    "advanced",
    "architectures",
    "like",
    "lstms",
    "gruse",
    "generally",
    "recommended",
    "tasks",
    "long",
    "range",
    "dependencies",
    "crucial",
    "yet",
    "discuss",
    "one",
    "small",
    "item",
    "xt",
    "said",
    "earlier",
    "video",
    "xt",
    "vector",
    "representation",
    "word",
    "yes",
    "correct",
    "get",
    "vector",
    "common",
    "use",
    "embedding",
    "layer",
    "rn",
    "unit",
    "create",
    "xt",
    "vectors",
    "input",
    "vectors",
    "time",
    "step",
    "incorporate",
    "embedding",
    "word",
    "text",
    "mapped",
    "unique",
    "integer",
    "index",
    "embedding",
    "matrix",
    "embedding",
    "layer",
    "uses",
    "indices",
    "look",
    "corresponding",
    "embedding",
    "vector",
    "word",
    "embedding",
    "vectors",
    "xtes",
    "passed",
    "inputs",
    "rnn",
    "time",
    "step",
    "embeddings",
    "embedding",
    "layer",
    "updated",
    "back",
    "propagation",
    "process",
    "rnn",
    "provided",
    "embedding",
    "layer",
    "set",
    "trainable",
    "rnn",
    "trained",
    "task",
    "language",
    "modeling",
    "text",
    "classification",
    "gradients",
    "calculated",
    "back",
    "propagation",
    "used",
    "update",
    "weights",
    "biases",
    "rnn",
    "also",
    "adjust",
    "embeddings",
    "embedding",
    "layer",
    "note",
    "rnn",
    "used",
    "text",
    "also",
    "used",
    "time",
    "series",
    "data",
    "univariate",
    "multivariate",
    "multivariate",
    "time",
    "series",
    "input",
    "vector",
    "time",
    "step",
    "conceptually",
    "similar",
    "embedding",
    "vector",
    "word",
    "natural",
    "language",
    "processing",
    "applications",
    "theory",
    "r",
    "short",
    "see",
    "next",
    "video",
    "python",
    "implementation",
    "rnn",
    "text",
    "generator"
  ],
  "keywords": [
    "discuss",
    "recurrent",
    "neural",
    "regular",
    "network",
    "input",
    "layer",
    "hidden",
    "output",
    "rnn",
    "rn",
    "sequence",
    "data",
    "time",
    "one",
    "element",
    "another",
    "first",
    "comes",
    "passed",
    "outcome",
    "activation",
    "process",
    "fed",
    "back",
    "next",
    "state",
    "previous",
    "let",
    "us",
    "eat",
    "rice",
    "word",
    "function",
    "processed",
    "contains",
    "information",
    "also",
    "error",
    "used",
    "propagation",
    "step",
    "vector",
    "xt",
    "weight",
    "matrix",
    "u",
    "size",
    "length",
    "number",
    "neurons",
    "minus1",
    "equal",
    "w",
    "bias",
    "term",
    "b",
    "weights",
    "rnns",
    "specific",
    "task",
    "language",
    "predict",
    "predicting",
    "words",
    "processing",
    "updated",
    "gradients",
    "chunk",
    "embedding",
    "vectors",
    "text"
  ]
}