{
  "text": "you've probably read in the news that\ndeep learning is the secret recipe\nbehind many exciting developments and\nhas made many of our world's dreams and\nperhaps also nightmares come true\nwho would have thought that deep minds\nalphago could be at least a doll in a\nboat game which boasts in more possible\nmoves than there are atoms in the entire\nuniverse\na lot of people including me never saw\nit coming it seemed impossible but it's\nhere now deep learning is everywhere\nit's beating physicians are diagnosing\ncancer it's responsible for translating\nweb pages in a matter of mere seconds to\nthe autonomous vehicles by weimo and\ntesla\nhi my name is jason and welcome to this\ncourse in deep learning where you'll\nlearn everything you need to get started\nwith deep learning in python how to\nbuild remarkable algorithms capable of\nsolving complex problems that weren't\npossible just a few decades ago we'll\ntalk about what deep learning is and the\ndifference between artificial\nintelligence and machine learning i'll\nintroduce new networks what they are and\njust how essential they are to deep\nlearning you're going to learn about how\ndeep learning models train and learn and\nthe various types of learning associated\nsupervised unsupervised and\nreinforcement learning we're going to\ntalk about loss functions optimizers the\ngrading descent algorithm the different\ntypes of neural network architectures\nand the various steps involved in deep\nlearning\nthis entire course is centered on the\nnotion of deep learning but what is it\ndeep learning is a subset of machine\nlearning which in turn is a subset of\nartificial intelligence which involves\nmore traditional methods to learn\nrepresentations directly from data\nmachine learning involves teaching\ncomputers to recognize patterns in data\nin the same way as our brains do so as\nhumans it's easy for us to distinguish\nbetween a cat and a dog but it's much\nmore difficult to teach a machine to do\nthis and we'll talk more about this\nlater on in this course before i do that\ni want to give you a sense of the\namazing successes of deep learning in\nthe past\nin 1997 gary kasparov the most\nsuccessful champion in the history of\nchess lost to ibm's deep blue one of the\nfirst computer or artificial systems\nit was the first defeat of a reigning\nworld chess champion by a computer\nin 2011 ibm's watson competed in game\nshow jeopardy against his champions brad\nrotter and ken jennings and won the\nfirst prize a million dollars\nin 2015 alphago a deep learning computer\nprogram created by google's deepmind\ndivision defeated lisa dole an 18 time\nworld champion at go a game of google\nmore times complex than chess\nbut deep learning can do more than just\nbetas at boat games it finds\napplications anywhere from self-driving\nvehicles to fake news detection to even\npredicting earthquakes\nthese were astonishing moments not only\nbecause machines beat humans at their\nown games but because of the endless\npossibilities that they opened up\nwhat followed such events have been a\nseries of striking breakthroughs in\nartificial intelligence machine learning\nand yes deep learning\nto put it simply deep learning is a\nmachine learning technique that learns\nfeatures and tasks directly from data by\nrunning inputs through a biologically\ninspired neural network architecture\nthese neural networks contain a number\nof hidden layers through which data is\nprocessed allowing for the machine to go\ndeep in its learning making connections\nand weighing input for the best results\nwe'll go over neural networks in the\nnext video so why deep learning\nthe problem with traditional machine\nlearning algorithms is that no matter\nhow complex they get they'll always be\nmachine like they need a lot of domain\nexpertise human intervention and are\nonly capable of what they're designed\nfor\nfor example if i show you the image of a\nface you will automatically recognize\nit's a face\nbut how would a computer know what this\nis well if we follow traditional machine\nlearning we'd have to manually and\npainstakingly define to a computer what\nit faces for example it has eyes ears\nand mouth but now how do you define an\neye or a mouth to a computer well if you\nlook at an eye the corners are at some\nangle they're definitely not 90 degrees\nthey're definitely not zero degrees\nthere's some angle in between so we\ncould work with that and train our\nclassifier to recognize these kinds of\nlines in certain orientations\nthis is complicated\nfor ei practitioners and the rest of the\nworld that's where deep learning holds a\nbit of promise\nthe key idea in deep learning is that\nyou can learn these features just from\nraw data so i can feed a bunch of images\nor faces to my deep learning algorithm\nand it's going to develop some kind of\nhierarchical representation of detecting\nlines and edges and then using these\nlines and edges to detect eyes and a\nmouth and composing it together to\nultimately detect the face\nas it turns out the underlying\nalgorithms for training these models\nhave existed for quite a long time\nso why has deep learning gaining\npopularity many decades later\nwell for one data has become much more\npervasive we're living in the age of big\ndata and these algorithms require\nmassive amounts of data to effectively\nbe implemented\nsecond we have hardware and architecture\nthat are capable of handling the vast\namount of data and computational power\nthat these algorithms require hardware\nthat simply wasn't available a few\ndecades ago\nthird building and deploying these\nalgorithms models as i call is extremely\nstreamlined with the increasing\npopularity of open source software like\ntensorflow and pytorch\ndeep learning models refer to the\ntraining of things called neural\nnetworks\nneural networks form the basis of deep\nlearning a sub-field of machine learning\nwhere algorithms are inspired by the\nstructure of the human brain\njust like neurons make up the brain the\nfundamental building blocks of a neural\nnetwork is also a neuron\nneural networks take in data they train\nthemselves to recognize patterns in this\ndata and predict outputs for a new set\nof similar data\nin a new network information propagates\nthrough three central components that\nform the basis of every neural network\narchitecture the input layer the output\nlayer and several hidden layers between\nthe two\nin the next video we'll go over the\nlearning process of a neural network\nthe learning process of a neural network\ncan be broken into two main processes\nforward propagation and back propagation\nfull propagation is the propagation of\ninformation from the input layer to the\noutput layer we can define our input\nlayer as several neurons x1 through xn\nthese neurons connect to the neurons of\nthe next layer through channels and they\nare assigned numerical values called\nweights the inputs are multiplied to the\nweights and their sum is sent as input\nto the neurons in the hidden layer where\neach neuron in turn is associated to a\nnumerical value called the bias which is\nthen added to the input sum\nthis weighted sum is then passed through\na non-linear function called the\nactivation function which essentially\ndecides if that particular neuron can\ncontribute to the next layer\nin the output layer it's basically a\nform of probability the neuron with the\nhighest value determines what the output\nfinally is\nso let's go over a few terms\nthe weight of a neuron tells us how\nimportant the neuron is the higher the\nvalue the more important it is in the\nrelationship the bias is like the new on\nhaving an opinion to the relationship it\nserves to shift the activation function\nto the right or to the left if you have\nhad some experience with high school\nmath you should know that adding a\nscalar value to a function shifts a\ngraph either to the left or to the right\nand this is exactly what the bias does\nit shifts the activation function to the\nright or to the left that propagation is\nalmost like for propagation except in\nthe reverse direction information here\nis passed from the output layer to the\nhidden layers not the input layer\nbut what information gets passed on from\nthe output layer isn't the output layer\nsupposed to be the final layer where we\nget the final output\nwell\nyes but no bad propagation is the reason\nwhy new networks is so powerful it is\nthe reason why new networks can learn by\nthemselves\nin the last step before propagation a\nnew network spits out a prediction\nthis prediction could have two\npossibilities either right or wrong\nin bad propagation the new network\nevaluates its own performance and checks\nif it is right or wrong if it is wrong\nthe network uses something called a loss\nfunction to quantify the deviation from\nthe expected output and it is this\ninformation that's sent back to the\nhidden layers for the weights and biases\nto be adjusted so that the network's\naccuracy level increases let's visualize\nthe training process with a real example\nlet's suppose we have a data set this\ndataset gives us the weight of a vehicle\nand the number of goods carried by that\nvehicle and also tells us if those\nvehicles are cars or trucks\nwe want to go through this data trade a\nnew networks to predict cars or trucks\nbased on their weights and goods to\nstart off let's initialize the neural\nnetwork by giving it random weights and\nbiases these can be anything we really\ndon't care what these values are as long\nas they're there\nin the first entry of a data set we have\nvehicle weight equal to a value which in\nthis case is 15 and good as two\naccording to this it's a car\nwe now start moving these input\ndimensions through the newer network so\nbasically what we want to do is take\nboth the inputs multiply them by their\nweight and add a bias\nand this is where the magic happens we\nrun this weighted sum through an\nactivation function\nokay now let's say that the output of\nthis activation function is 0.001\nthis again is multiplied by the weight\nand added to the bias and finally in the\noutput layer we have a guess\nnow according to this neural network the\ntype of vehicle with weight 15 and goods\n2 has a greater probability of being a\ntruck of course this is not true and a\nnew network knows this so we use back\npropagation we're going to quantify the\ndifference between the expected result\nand the predicted output using a loss\nfunction in bank propagation we're going\nto go backwards and adjust our initial\nrates and biases remember that during\nthe initialization of the neural network\nwe chose completely random weight and\nbiases\nwell during back propagation these\nvalues will be adjusted to better fit\nthe prediction model\nokay so that was one iteration through\nthe first piece of the data set in the\nsecond entry we have vehicle weight 34\nand goods 67.\nwe're going to use the same process as\nbefore multiply the input with the\nweight and add a box pass this result\ninto an activation function and repeat\ntill the output layer check the error\ndifference and employ back propagation\nto adjust the weights in the biases your\nnew network will continue doing this\nrepeated processor for propagation\ncalculating the error and then back\npropagation for as many entries there\nare in this data set\nthe more data you give the newer network\nthe better it will be at predicting the\nright output but there's a tradeoff\nbecause too much data and you'll end up\nwith a problem like overfitting which\ni'll discuss later in this course but\nthat's essentially how a new network\nworks you feed input the network\ninitializes with random weights and\nbiases that are adjusted each time\nduring back propagation\nuntil the network's gone through all\nyour data and is now able to make\npredictions\nthis learning algorithm can be\nsummarized as follows first we\ninitialize the network with random\nvalues for the network's parameters or\nthe weights in the biases we take a set\nof input data and pass them through the\nnetwork we compare these predictions\nobtained with the values of the expected\nlabels and calculate the loss using a\nloss function\nwe perform back propagation in order to\npropagate this loss to each and every\nweight and bias\nwe use this propagated information to\nupdate the weights and biases of neural\nnetwork with the gradient descent\nalgorithm in such a way that the total\nloss is reduced and a better model is\nobtained\nthe last step is continue iterating the\nprevious steps until we consider that we\nhave a good enough model\nin this section we're going to talk\nabout the most common terminologies used\nin deep learning today\nlet's start off with the activation\nfunction\nthe activation function serves to\nintroduce something called non-linearity\ninto the network and also decides\nwhether a particular neuron can\ncontribute to the next layer\nbut how do you decide if the neuron can\nfire or activate\nwell we had a couple of ideas which led\nto the creation of different activation\nfunctions\nthe first idea we had is how about we\nactivate the neuron if it is above a\ncertain value or threshold if it is less\nthan the threshold don't activate it\nactivation function a is equal to\nactivated if y is greater than some\nthreshold else it's not\nthis is essentially a step function its\noutput is 1 or activated when value is\ngreater than 0. its output is activated\nwhen value is greater than some\nthreshold and outputs not activated\notherwise\ngreat so this makes an activation\nfunction for a neuron no confusions life\nis perfect except there are some\ndrawbacks with this to understand about\nit think about the following\nthink about a case where you want to\nclassify multiple such neurons into\nclasses say class 1 class 2 class 3 etc\nwhat will happen if more than one neuron\nis activated all these neurons will\noutput a one\nwell how do you decide\nnow how do you decide which class it\nbelongs to it's complicated right you\nwould want the network to activate only\none neuron and the other should be zero\nonly then you would be able to say it\nwas classified properly\nin real practice however it is harder to\ntrain and converge it this way it would\nbe better if the activation was not\nbinary and instead some probable value\nlike 75 activated or 16 activated\nthere's a 75 chance that it belongs to\nclass 2 etc\nthen if more than one neuron activates\nyou could find which neuron fires based\non which has the highest probability\nokay maybe you'll ask yourself i want\nsomething to give me a more analog value\nrather than just saying activated or not\nactivated something other than in binary\nand maybe you would have thought about a\nlinear function a straight line function\nwhere the activation is proportional to\nthe input by a value called the slope of\nthe line\nthis way it gives us a range of\nactivations so it isn't binary\nactivation we can definitely connect a\nfew neurons together and if more than\none fires we could take the maximum\nvalue and decide based on that so that\nis okay too and what is the problem with\nthis\nwell if you are familiar with gradient\ndescent which i'll come to in just a bit\nyou'll notice that the derivative of a\nlinear function is a constant\nmakes sense because the slope isn't\nchanging at any point\nfor a function f of x is equal to mx\nplus c the derivative is m this means\nthat the gradient has no relationship\nwhatsoever with x this also means that\nduring back propagation the adjustments\nmade to the weights and the biases\naren't dependent on x at all and this is\nnot a good thing additionally think\nabout if you have connected layers no\nmatter how many layers you have if all\nof them are linear in nature the\nactivation function of the final layer\nis nothing but just a linear function of\nthe input of the first layer\npause for a bit and think about it\nthis means that the entire neural\nnetwork of dozens of layers can be\nreplaced by a single layer remember a\ncombination of linear functions in the\nlinear manner is still another linear\nfunction\nand this is terrible because we've just\nlost the ability to stack layers this\nway no matter how much we stack the\nwhole network is still equivalent to a\nsingle layer with single activation next\nwe have a sigmoid function and if you've\never watched a video on activation\nfunctions this is the kind of function\nused in the examples a sigmoid function\nis defined as a if x is equal to 1 over\n1 plus e to the negative x\nwell this looks smooth and kind of like\na step function what are its benefits\nthink about it for a moment\nwell first things first it has\nnon-linear nature combinations of this\nfunction are also non-linear great so\nnow we can stack layers\nwhat about non-binary activations yes\nthat too this function outputs an analog\nactivation unlike the step function and\nalso has a smooth gradient an advantage\nof this activation function is that\nunlike the linear function the output of\nthis function is going to be in the\nrange zero to one inclusive compared to\nthe negative infinity to infinity of the\nlatter\nso we have activations bound in a range\nand this won't blow up the activations\nand this is great and sigmoid functions\nare one of the most widely used\nactivation functions today\nbut life isn't always rosy and sigmoids\ntwo tend to have the share of\ndisadvantages\nif you look closely between x is equal\nto negative two and x is equal to two\nthe y values are very steep any small\nchanges in values of x in that region\nwill cause values of y to change\ndrastically\nalso towards either end of the function\nthe y values tend to respond very less\nto changes in x\nthe gradient at those regions is going\nto be really really small\nalmost zero and it gives rise to the\nvanishing gradient problem which just\nsays that if the input to the activation\nfunction is either large or small the\nsigmoids are going to squish that down\nto a value between zero and one and the\ngradient of this function becomes really\nsmall and you'll see why when we talk\nabout gradient descent this is a huge\nproblem another activation function that\nis used is the tan h function\nthis looks very similar to sigmoid in\nfact\nmathematically this is what's known as a\nshifted sigmoid function\nokay so like the sigmoid it has\ncharacteristics that we discussed above\nit is nonlinear nature so we can stack\nlayers it is bound to arrange from\nnegative one to one so there's no\nworrying about the activations blowing\nup\nthe derivative of the tanning function\nhowever is steeper than that of the\nsigmoid so deciding between the sigmoid\nand the tanh will really depend on your\nrequirement of the gradient strength\nlike sigmoid tanh is also a very popular\nand widely used activation function\nand yes like the sigmoid tanh does have\na vanishing gradient problem the\nrectified linear unit or the value\nfunction is defined as a of x is equal\nto the max from 0 to x at first look\nthis would look like a linear function\nright the graph is linear in the\npositive axis\nlet me tell you rather is in fact\nnon-linear nature and combinations of\nrelu are also non-linear great so this\nmeans that we can stack layers however\nunlike the previous two functions that\nwe discussed is not bounded the range of\nthe relu is from zero to infinity this\nmeans there is a chance of blowing up\nthe activation\nanother point i would like to discuss\nhere is sparsity of inactivation imagine\na big neural network with lots of\nneurons using a sigmoid or a tanning\nwill cause almost all the neurons to\nfire in an analog way\nthis means that almost all activations\nwill be processed to describe the\nnetwork's output in other words the\nactivation would be dense\nand this is costly ideally we want only\na few neurons in the network to activate\nand thereby making the activations pass\nand efficient\nhere's where the relu comes in imagine a\nnetwork with randomly initialized\nweights and almost 50 percent of the\nnetwork yields zero activation because\nof the characteristic relu it outputs\nzero for negative values of x\nthis means that only 50 percent of the\nneurons fire sparse activation making\nthe network lighter but when life gives\nyou an apple it comes with a little worm\ninside\nbecause of that horizontal line in relu\nfor negative values of x the gradient is\nzero in that region which means that\nduring back propagation the weights will\nnot get adjusted during descent this\nmeans that those neurons which go into\nthat state will stop responding to\nvariations in the error simply because\nthe gradient is zero nothing changes\nthis is called the dying value problem\nthis problem can cause several neurons\nto just die and not respond thus making\na substantial part of the network\npassive rather than what we want out of\nthere are workarounds for this one way\nespecially is to simply make the\nhorizontal line into a non-horizontal\ncomponent by adding a slope usually the\nslope is around 0.001\nand this this new version of the relu is\ncalled leaky value the main idea is that\nthe gradient should never be zero one\nmajor advantage of the relu is the fact\nthat it's less computationally expensive\nthan functions like tannage and sigmoid\nbecause it involves simpler mathematical\noperations\nthis is a really good point to consider\nwhen you were designing your own deep\nneural networks great so now the\nquestion is which activation function to\nuse\nbecause of the advantages that relu\noffers does this mean that you should\nuse reload for everything you do\nor could you consider sigmoid and tan h\nwell both\nwhen you know the function that you're\ntrying to approximate has certain\ncharacteristics you should choose an\nactivation function with which will\napproximate the function faster leading\nto faster training processes\nfor example a sigmoid function works\nwell for binary classification problems\nbecause approximating our classifier\nfunctions as combinations of the sigmoid\nis easier than maybe the relu this will\nlead to faster training processes and\nlarger convergence\nyou can use your own custom functions\ntoo if you don't know the nature of the\nfunction you're trying to learn i would\nsuggest you start with relu and then\nwork backwards from there\nbefore we move on to the next section i\nwant to talk about why we use non-linear\nactivation functions as opposed to\nlinear ones\nif you recall in my definition of\nactivation functions i mentioned that\nactivation functions serve to introduce\nsomething called non-linearity in the\nnetwork for all intensive purposes\nintroducing non-linearity simply means\nthat your activation function must be\nnon-linear that is not a straight line\nmathematically linear functions are\npolynomials of degree 1 that when\ngraphed in the x y plane are straight\nlines inclined to the x-axis at a\ncertain value we call this the slope of\nthe line\nnon-linear functions are polynomials of\ndegree greater than one\nand when graphed they don't form\nstraight lines rather than more curved\nif we use linear activation functions to\nmodel our data then no matter how many\nhidden layers our network has it will\nalways become equivalent to having a\nsingle layer network and in deep\nlearning we want to be able to model\nevery type of data without being\nrestricted as would be the case should\nwe use linear functions\nwe discussed previously in the learning\nprocess of neural networks that we\nstarted with random weight and biases\nthe neural network makes a prediction\nthis prediction is compared against the\nexpected output and the weights and\nbiases are adjusted accordingly\nwell loss functions are the reason that\nwe're able to calculate that difference\nreally simply a loss function is a way\nto quantify the deviation of the\npredicted output by the neural network\nto the expected output it's as simple as\nthat nothing mo nothing less\nthere are plenty of loss functions out\nthere for example under regression we\nhave squared error loss absolute ever\nloss in huber loss in binary\nclassification we have binary cross\nentropy and hinge loss in multi-class\nclassification problems we have the\nmulti-class cross entropy and the\ncallback liability divergence loss and\nso on\nthe choice of the best function really\ndepends on what kind of project you're\nworking on different projects require\ndifferent loss functions\nnow i don't want to talk any further\nloss functions right now we'll do this\nunder the optimization section because\nthat's really where most functions are\nutilized\nin the previous section we dealt with\nloss functions which are mathematical\nways of measuring how wrong predictions\nmade by neural network are\nduring the training process we tweak and\nchange the parameters or the weights of\nthe model\nto try and minimize that loss function\nand make our predictions as correct and\noptimized as possible\nbut how exactly do you do that how do\nyou change the parameters of your model\nby how much and when\nwe have the ingredients how do we make\nthe cake\nthis is where optimizers come in they\ntied together the loss function and\nmodel parameters or the weight and\nbiases by updating the network in\nresponse to the output of the loss\nfunction\nin simpler terms optimizers shape and\nmold your model into more accurate\nmodels by adjusting the weights and the\nbiases\nthe loss function is its guide it tells\nthe optimizer whether it's moving in the\nright or the wrong direction\nto understand this better\nimagine did you have just killed mount\neverest and now you decide to descend\nthe mountain blindfolded it's impossible\nto know which direction to go in you\ncould either go up which is away from\nyour goal or go down which is towards\nyour goal but to begin you would start\ntaking steps using your feet you'll be\nable to gauge whether you're going up or\ndown\nin this analogy you resemble the neural\nnetwork going down your goal is trying\nto minimize the error and your feet are\nresemblance of the loss functions they\nmeasure whether you're going in the\nright way or the wrong way\nsimilarly it's impossible to know what\nyour model's weights should be right\nfrom the start but with some trial and\nerror based on the loss function you\ncould end up getting there eventually\nwe now come to grading descent often\ncalled the grand daddy of optimizers\ngrading descent is an iterative\nalgorithm that starts up at a random\npoint in the loss function and travels\ndown its slope in steps until it reaches\nthe lowest point or the minimum of the\nfunction\nit is the most popular optimizer we use\nnowadays it's fast robust and flexible\nand here's how it works\nfirst we calculated what a small change\nin each individual weight would do to\nthe loss function\nwe adjust each individual weight based\non its gradient that is take a small\nstep in the determined direction\nthe last step is to repeat the first and\nthe second step until the loss function\ngets as low as possible i want to talk\nabout this notion of a gradient the\ngradient of a function is the vector of\nthe partial derivatives with respect to\nall independent variables the gradient\nalways points in the direction of the\nsteepest increase in the function\nsuppose we have a graph like so with\nloss on the y-axis and the value of the\nweight on the x-axis\nwe have a little data point here that\ncorresponds to the randomly initialized\nweight to minimize this loss that is to\nget this data point to the minimum of\nthe function\nwe need to take the negative gradient\nsince we want to find the steepest\ndecrease in function\nthis process happens iteratively through\nthe losses as minimized as possible\nand that's grading descent in a nutshell\nwhen dealing with high dimensional data\nsets that is a lot of variables it's\npossible you'll find yourself in an area\nwhere it seems like you've reached the\nlowest possible value for your loss\nfunction but in reality it's just a\nlocal minimum\nto avoid getting stuck in a local minima\nwe make sure we use the proper learning\nrate\nchanging our weights too fast by adding\nor subtracting too much that is taking\nsteps that are too large or too small\ncan hinder your ability to minimize the\nloss function\nwe don't want to make a jump so large\nthat we skip over the optimal value for\na given weight to make sure this doesn't\nhappen we use a variable called the\nlearning rate\nthis thing is usually just a small\nnumber like 0.001\nthat we multiply the gradients by to\nscale them this ensures that any changes\nwe make to our weights are pretty small\nin math talk taking steps that are too\nlarge can mean that the algorithm will\nnever converge to an optimum at the same\ntime we don't want to take steps that\nare too small because then we might\nnever end up with the right values for\nour weights in math talk steps that are\ntoo small might lead to our optimizer\nconverging on a local minimum for the\nloss function but never the absolute\nminimum\nfor a simple summary just remember that\nthe learning rate ensures that we change\nour weight at the right pace not making\nany changes that are too big or too\nsmall\ninstead of calculating the gradients for\nall your training examples on every part\nof the gradient descent it's sometimes\nmore efficient to only use a subset of\nthe training examples each time\nstochastic gradient descent is an\nimplementation that either uses batches\nof examples at a time or random examples\non each pass\nstochastic gradient descent uses the\nconcept of momentum momentum accumulates\ngradients of the past steps to dictate\nwhat might happen in the next steps also\nbecause we don't include the entire\ntraining set\nsjd is less computationally expensive\nit's difficult to overstate how popular\ngradient descent really is back\npropagation is basically gradient\ndescent implemented on a network there\nare other types of optimizers based on\ngradient descent that are used today ad\ngrad adapts the learning rate\nspecifically to individual features\nthat means that some of the weights in\nyour data set will have different\nlearning rates than others\nthis works really well for sparse data\nsets where a lot of input examples are\nmissing\nat a grad has a major issue though the\nadaptive learning rate tends to get\nreally really small over time rms prop\nis a special version of adegrad\ndeveloped by professor jeffrey hinton\ninstead of letting all the gradients\naccumulate for momentum it accumulates\ngradients in a fixed window\nrms prop is similar to add a prop which\nis another optimizer that seeks to solve\nsome of the issues that atograd leaves\nopen atom stands for adaptive moment\nestimation and is another way of using\npast gradients to calculate the carbon\ngradient atom also utilizes the concept\nof momentum which is basically our way\nof telling the neural network whether we\nwant past changes to affect the new\nchange by adding fractions of the\nprevious gradients to the current one\nthis optimizer has become pretty\nwidespread and is practically accepted\nfor use in training new networks\nit's easy to get lost in the complexity\nof some of these new optimizers just\nremember that they all have the same\ngoal minimizing the loss function and\ntrial and error will get you there\nyou may have heard me referring to the\nwords parameters quite a bit and often\nthis word is confused with the term\nhyperparameters\nin this video i'm going to outline the\nbasic difference between the two a model\nparameter is a variable that is internal\nto the new network and whose values can\nbe estimated from the data itself\nthey are required by the model when\nmaking predictions these values define\nthe skill of the model on your problem\nthey can be estimated directly from the\ndata\nand are often not manually set by the\npractitioner\nand oftentimes when you save your model\nyou are essentially saving your model's\nparameters parameters are key to machine\nlearning algorithms and examples of\nthese include the weights and the biases\na hyper parameter is a configuration\nthat is external to the model and whose\nvalue cannot be estimated from data\nthere's no way that we can find the best\nvalue for a model hyper parameter on a\ngiven problem we may use rules of thumb\ncopy values used in other problems or\nsearch for the best value by trial and\nerror\nwhen a machine learning algorithm is\ntuned for a specific problem such as\nwhen you're using grid search or random\nsearch then you are in fact tuning the\nhyper parameters of the model in order\nto discover the parameters that result\nin more skillful predictions\nmodel hyper parameters are often\nreferred to as parameters which can make\nthings confusing so a good rule of thumb\nto overcome this confusion is as follows\nif you have to specify a parameter\nmanually then it is probably a hyper\nparameter\nparameters are inherent to the model\nitself some examples of hyper parameters\ninclude the learning rate for training\non your network the c in sigma\nhyper parameters for sport vector\nmachines and the k and k newest\nneighbors\nwe need terminologies like epochs batch\nsize and iterations only when the data\nis too big which happens all the time in\nmachine learning and when we can't pass\nall this data to the computer at once so\nto overcome this problem we need to\ndivide the data set into smaller chunks\ngive it to our computer one by one and\nupdate the weights of the new network at\nthe end of every step to fit it into the\ndata given\none epoch is when an entire data set is\npassed forward and backward through the\nnetwork once\nin a majority of deep learning models we\nuse more than one epoch i know it does\nmake sense in the beginning why do we\nneed to pass the entire data set many\ntimes through the same neural network\npassing the entire data set through the\nnetwork only once is trying to read the\nentire lyrics of a song once he won't be\nable to remember the entire song\nimmediately you have to reread the\nlyrics a couple more times before you\ncan say you know the song by memory\nthe same is true with the neural network\nwe pass the data set multiple times\nthrough the neural network so it's able\nto generalize better\ngradient descent is an iterative process\nand updating the parameters and back\npropagation in a single pass or one\nepoch is not enough as the number of\nepochs increases the more the parameters\nare adjusted leading to a better\nperforming model\nbut too many epochs could spell disaster\nand lead to something called overfitting\nwhere a model has essentially memorized\nthe patterns in the training data and\nperforms terribly on data it's never\nseen before\nso what is the right number of epochs\nunfortunately there is no right answer\nthe answer is different for different\ndata sets sometimes your data set can\ninclude millions of examples passing\nthis entire data set at once becomes\nextremely difficult so what we do\ninstead is divide the data set into a\nnumber of batches rather than passing\nthe entire dataset once the total number\nof training examples present in a single\nbatch is called a batch size iterations\nis the number of batches needed to\ncomplete one epoch\nnote the number of batches is equal to\nthe number of iterations for one epoch\nlet's say that we have a data set of 34\n000 training examples if we divide the\ndata set into batches of 500 then it\nwill take 68 iterations to complete one\nepoch\nwell i hope that gives you some kind of\nsense about the very basic terminologies\nused in deep learning before we move on\ni do want to mention this and you will\nsee this a lot in deep learning you'll\noften have a bunch of different choices\nto make how many hidden layers should i\nchoose or which activation function must\ni use and where and to be honest there\nare no clear-cut guidelines as to what\nyour choice should always be\nthat's a fun part about deep learning\nit's extremely difficult to know in the\nbeginning what's the right combination\nto use for your project what works for\nme might not work for you and a\nsuggestion from my end would be that you\ndabble along with material shown try\nvarious combinations and see what works\nfor you best\nultimately that's a learning process pun\nintended throughout this course i'll\ngive you quite a bit of intuition as to\nwhat's popular so that when it comes to\nbuilding a deep learning project you\nwon't find yourself lost\nin this section we're going to talk\nabout the different types of learning\nwhich are machine learning concepts but\ni extended to deep learning as well\nin this course we'll go over supervised\nlearning unsupervised learning and\nreinforcement learning\nsupervised learning is the most common\nsub branch machine learning today\ntypically if you're new to machine\nlearning your journey will begin with\nsupervised learning algorithms\nlet's explore what these are\nsupervised machine learning algorithms\nare designed to learn by example the\nname supervised learning originates from\nthe idea that training this type of\nalgorithm is almost like there's a human\nsupervising the whole process\nin supervised learning we train our\nmodels on well-labeled data\neach example is a pair consisting of an\ninput object which is typically a vector\nand a desired output value also called a\nsupervisory signal\nduring training a supervised learning\nalgorithm will search for patterns in\nthe data that correlate with the desired\noutputs\nafter training it will take in new\nunseen inputs and will determine which\nlabel the new inputs will be classified\nas based on prior training data the\nobjective of a supervised learning model\nis to predict the correct label for\nnewly presented input data at its most\nbasic form a supervised learning\nalgorithm can simply be written as y is\nequal f x\nwhere y is the predicted output that is\ndetermined by a mapping function that\nassigns a class to an input value x the\nfunction used to connect input features\nto a predicted output is created by the\nmachine learning model during training\nsupervised learning can be split into\ntwo subcategories classification and\nregression\nduring training a classification\nalgorithm will be given data points with\nan assigned category the job of a\nclassification algorithm is then to take\nthis input value and assigned to a class\nof category that it fits into based on\nthe training data provided\nthe most common example of\nclassification is determining if an\nemail is spam or not with two classes to\nchoose from spam or not spam this\nproblem is called a binary\nclassification problem\nthe algorithm will be given training\ndata with emails that are both spam and\nnot spam and the model will find the\nfeatures within the data that correlate\nto either class and create a mapping\nfunction\nthen when provided with an unseen email\nthe model will use this function to\ndetermine whether or not the email is\npam\nan example of a classification problem\nwould be the mnist handwritten digits\ndataset where the inputs are images of\nhandwritten digits pixel data and the\noutput is the class label for what digit\nthe image represents that is numbers\nzero to nine there are numerous\nalgorithms to solve classification\nproblems each which depends on the data\nand the situation here are a few popular\nclassification algorithms\nlinear classifiers support vector\nmachines decision trees k-nearest\nneighbors and drellum forest regression\nis a predictive statistical process\nwhere the model attempts to find the\nimportant relationship between dependent\nand independent variables\nthe goal of a regression algorithm is to\npredict a continuous number such as\nsales income and tax scores\nthe equation for a basic linear\nregression can be written as follows\nwhere x and 5 represents the features of\nthe data and w of i and b are parameters\nwhich are developed during training for\nsimple linear regression models with\nonly one feature in the data the formula\nlooks like this where w is the slope x\nis the single feature and b is a\ny-intercept\nfamiliar\nfor simple regression problems such as\nthis the model's predictions are\nrepresented by the line of best fit\nfor models using two features a plane is\nused and for models with more than two\nfeatures a hyperplane is used\nimagine we want to determine a student's\ntest grade based on how many hours they\nstudy the week of the test\nlet's say the plot data with line of\nbest fit looks like this\nthere is a clear positive correlation\nbetween our studied the independent\nvariable and the student's final test\ngoals the dependent variable a line of\nbest fit can be drawn through the data\npoints to show the model's predictions\nwhen given new input\nsay we wanted to know how well a student\nwould do with five hours of study\nwe can use the line of best fit to\npredict the test call based on other\nstudents performances another example of\nregression problem would be the boston\nhouse prices data set where the input of\nvariables that describe a neighborhood\nand the output is a house price in\ndollars\nthere are many different types of\nregression algorithms three most common\nare linear regression lasso regression\nand multivariate regression supervised\nlearning finds applications in\nclassification and regression problems\nlike\nbioinformatics such as fingerprint iris\nand face recognition in smartphones\nobject recognition spam detection and\nspeech recognition\nunsupervised learning is a branch of\nmachine learning that is used to\nmanifest underlying patterns and data\nand is often used in exploratory data\nanalysis\nunlike supervised learning unsupervised\nlearning does not use label data but\ninstead focuses on the data's features\nlabel training data has a corresponding\noutput for each input\nthe goal of an unsupervised learning\nalgorithm is to analyze data and find\nimportant features in that data\nunsupervised learning will often find\nsubgroups or hidden patterns within the\ndataset that a human observer might not\npick up on and this is extremely useful\nas we'll soon find out\nunsupervised learning can be of two\ntypes clustering and association\nclustering is the simplest and among the\nmost common applications of unsupervised\nlearning it is the process of grouping\nthe given data into different clusters\nor groups\nclasses will contain data points that\nare as similar as possible to each other\nand as dissimilar as possible to data\npoints in other clusters\nclustering helps find underlying\npatterns within the data that may not be\nnoticeable through a human observer\nit can be broken down into partitional\nclustering and hierarchical clustering\npartitional clustering refers to a set\nof clustering algorithms where each data\npoint in a data set can belong to only\none cluster\nhierarchical clustering finds clusters\nby a system of hierarchies\nevery data point can belong to multiple\nclusters some classes will contain\nsmaller clusters within it\nthis hierarchy system can be organized\nas a tree diagram\nsome of the more commonly used\nclustering algorithms are the k-means\nexpectation\nand the hierarchical cluster analysis of\nthe aca\nassociation on the other hand attempts\nto find relationships between different\nentities the classic example of\nassociation rules is market basket\nanalysis this means using a database of\ntransactions in the supermarket to find\nitems that are frequently bought\ntogether for example a person who buys\npotatoes and burgers usually buys beer\nfor example the person who buys tomatoes\nand pizza cheese\nmight want to bring pizza bread and so\non\nunsupervised learning finds applications\nalmost everywhere for example airbnb\nwhich helps host stays and experiences\nand connect people all over the world\nthis application uses unsupervised\nlearning algorithms where a potential\nclient\nqueries their requirements and airbnb\nlearns these patterns and recommends\nstays and experiences which fall under\nthe same group of cluster example a\nperson looking for houses in san\nfrancisco might not be interested in\nfinding houses in boston amazon also\nuses unsupervised learning to learn the\ncustomers purchases and recommend\nproducts which are frequently bought\ntogether which is an example of\nassociation rule mining\ncredit card fraud detection is another\nunsupervised learning algorithm that\nlearns the various patterns of a user\nand and their usage of a credit card if\nthe card is used in parts that do not\nmatch the behavior an alarm is generated\nwhich could possibly be marked as fraud\nand in some cases your bank might call\nyou to confirm whether it was you using\nthe card or not\nreinforcement learning is a type of\nmachine learning technique that enables\nan agent to learn in an interactive\nenvironment by trial and error using\nfeedback from its own actions and\nexperiences\nlike supervised learning it uses mapping\nbetween the input and the output but\nunlike supervised learning where\nfeedback provided to the agent is a\ncorrect set of actions for performing a\ntask\nreinforcement learning uses rewards and\npunishments as signals for positive and\nnegative behavior\nwhen you compare with unsupervised\nlearning reinforcement learning is\ndifferent in terms of its goals\nwhile the goal in unsupervised learning\nis to find similarities and differences\nbetween data points\nin reinforcement learning the goal is to\nfind a suitable action model that would\nmaximize the total cumulative reward of\nthe agent\nreinforcement learning refers to\ngoal-oriented algorithms which learn how\nto attain a complex objective or goal or\nhow to maximize along a particular\ndimension over many steps\nfor example they can maximize the points\nof one in a game over many moves\nreinforcement learning algorithms can\nstart from a blank slate and under the\nright conditions achieve superhuman\nperformance\nlike a pet incentivized by scolding and\ntreats these algorithms are penalized\nwhen they make the wrong decisions and\nrewarded when they make the right ones\nthis is reinforcement\nreinforcement learning is usually\nmodeled as a markov decision process\nalthough other frameworks like queue\nlearning are used\nsome key terms that describe the\nelements of a reinforcement learning\nproblem are the environment which is the\nphysical world in which the agent\noperates\nthe state represents the current\nsituation of the agent\nreward is a feedback received from the\nenvironment\npolicy sometimes is the method to map\nthe agent state to the agent's actions\nand finally value is the future reward\nthat an agent will receive by taking an\naction in a particular state\na reinforcement learning problem can be\nbest explained through games let's take\nthe game of pac-man where the goal of\nthe agent or pac-man is to eat the food\nin the grid while avoiding the ghosts on\nits way\nthe grid world is the interactive\nenvironment for the agent\npac-man receives a reward for eating\nfood and punishment if it gets killed by\nthe ghost that is it loses the game\nthe states are the location of pac-man\nin the grid world and the total\ncumulative reward is pac-man winning the\ngame\nreinforcement learning finds\napplications in robotics business\nstrategy planning traffic light control\nweb system configuration and aircraft\nand robot motion control\na central problem in deep learning is\nhow to make an algorithm that will\nperform well not just in training data\nbut also on new inputs\none of the most common challenges you'll\nface when training models is a problem\nof overfitting a situation where your\nmodel performs exceptionally well on\ntraining data but not in testing data\nsee i have a data set graphed on the xy\nplane like so\nnow i want to construct a model that\nwould best fit the data set\nwhat i could do is draw a line of some\nrandom slope in intercept now evidently\nthis isn't the best model and in fact\nthis is called underfitting because it\ndoesn't fit the model well in fact it\nunderestimates the data set\ninstead what we could do is draw a line\nthat looks something like this now this\nreally fits our model the best\nbut this is overfitting\nremember that while training we show our\nnetworks and training data and once\nthat's done\nwe'd expect it to be almost close to\nperfect\nthe problem with this graph is that\nalthough it is probably the best line of\nfit for this graph it is the best line\nof fit only if you're considering your\ntrading data\nwhat your network is done in this graph\nis memorize the patterns between the\ntrading data\nand won't give accurate predictions at\nall on data it's never seen before\nand this makes sense because instead of\nmemorizing patterns generally to perform\nwell on both training as well as new\ntesting data\nour network in fact has memorized the\npatterns only on the training data so it\nobviously won't perform well on new data\nit's never seen before\nthis is the problem of overfitting it\nfitted too much\nand by the way this would be the more\naccurate kind of fitting it's not\nperfect but it'll do well on both\ntraining as well as new testing data\nwith sizeable accuracy\nthere are a couple of ways to tackle\noverfitting\nthe most interesting type of\nregularization is dropout it produces\nvery good results and is consequently\nthe most frequently used regularization\ntechnique in the field of deep learning\nto understand dropout let's say that we\nhave a new network with two hidden\nlayers\nwhat dropout does is that at every\niteration it randomly selects some nodes\nand removes them along with their\nincoming and outgoing connections as\nshown\nso each iteration has a different set of\nnodes and this results in a different\nset of outputs\nso why do these models perform better\nthese models usually perform better than\na single model as they capture more\nrandomness and memorizes less of the\ntraining data and hence will be folks to\ngeneralize better and build a more\nrobust predictive model\nsometimes the best way to make a deep\nlearning model generalized better is to\ntrain it on mode data\nin practice the amount of data we have\nis limited and one way to get around\nthis problem is to create fake data and\nadd it to the training set\nfor some deep learning tasks it is\nreasonably straightforward to create new\nfake data\nthis approach is easiest for\nclassification a classifier needs to\ntake complicated high dimensional input\nx and summarize it with the category\nidentity y\nthis means that the main task facing a\nclassifier is to be invariant to a wide\nvariety of transformations\nwe can generate new xy pairs easily just\nby applying transformations on the xy\ninputs in our training set\ndataset augmentation has been a\nparticularly effective technique for a\nspecific classification problem object\nrecognition\nimages are high dimensional and include\nan enormous range of factors of\nvariation many of which can easily be\nsimulated\noperations like translating the training\nimages a few pixels in each direction\ncan often greatly improve generalization\nmany other operations such as rotating\nthe image or scaling the image have also\nproved quite effective\nyou must be careful not to apply\ntransformation that would change the\ncorrect class for example in optical\ncharacter recognition tasks that require\nrecognizing the difference between a b\nand a d and the difference between a six\nand a nine\nhorizontal flips and 180 degree\nrotations are not appropriate ways of\naugmenting data sets with these tasks\nwhen training large models with\nsufficient representational capacity to\noverfit the task\nwe often observe that the training error\ndecreases steadily over time but the\nerror validation set begins to rise\nagain\nthis means we can obtain a model with\nbetter validation set error and thus\nhopefully better test that error by\nstopping training at the point where the\nerror in the validation set starts to\nincrease\nthis strategy is known as early stopping\nit is probably the most commonly used\nform of regularization in deploying\ntoday its popularity is due to both its\neffectiveness and its simplicity\nin this section i'm going to introduce\nthe three most common types of neural\nnetwork architectures today\nfully connected v4 with new networks\nrecurring neural networks and\nconvolutional neural networks\nthe first type of new network\narchitecture we're going to discuss is a\nfully connected feed forward neural\nnetwork\nby fully connected i mean that each\nneuron in the preceding layer is\nconnected to every neuron in the\nsubsequent layer without any connection\nbackwards there are no cycles or loops\nin the connections in the network\nas i mentioned previously each neuron in\na neural network contains an activation\nfunction that changes the output of a\nneuron when given its input\nthere are several types of activation\nfunctions that can change this input to\noutput relationship to make a neuron\nbehave in a variety of ways\nsome of the most well-known activation\nfunctions are the linear function which\nis a straight line that essentially\nmultiplies the input by a constant value\nthe sigmoid function that ranges from\nzero to one the hyperbolic tangent of\nthe tanning function ranges from\nnegative one to positive one and the\nrectified linear unit or the relu\nfunction which is a piecewise function\nthat outputs a zero if the input is less\nthan a certain value or a linear\nmultiple if the input is greater than a\ncertain value\neach type of activation function has its\npros and cons so we use them in various\nlayers in the deep neural network based\non the problem each is designed to solve\nin addition the last three activation\nfunctions we refer to as non-linear\nfunctions because the output is not a\nlinear multiple of the input\nnon-linearity is what allows deep neural\nnetworks to model complex functions\nusing everything we've learned so far we\ncan create a wide variety of fully\nconnected feed for neural networks\nwe can create networks with various\ninputs various outputs various hidden\nlayers neurons per hidden layer and a\nvariety of activation functions\nthese numerous combinations allow us to\ncreate a variety of powerful deep neural\nnetworks that can solve the wide array\nof problems the more neurons we add to\neach hidden layer the wider the network\nbecomes\nin addition the more hidden layers we\nadd the deeper the network becomes\nhowever each neuron we add increases the\ncomplexity and thus the computational\nresource necessary to train a new\nnetwork increases\nthis increasing complexity isn't linear\nin the number of neurons we add so it\nleads to an explosion in complexity and\ntraining time for large neural networks\nthat's the trade-off you need to\nconsider when you are building deep\nneural networks\nall the new networks we've discussed so\nfar are known as feed forward neural\nnetworks they take in a fixed sized\ninput and give you a fixed size output\nthat's all it does\nand that's what we expect neural\nnetworks to do take in an input and give\na sizeable output\nbut as it turns out these plane of\nvanilla neural networks aren't able to\nmodel every single problem that we have\ntoday\nto better understand this use this\nanalogy suppose i show you the picture\nof a ball a round spherical ball that\nwas moving in space in some direction\ni've just taken a photo of the ball or a\nsnapshot of the ball at some time t now\ni want you to predict the next position\nof the ball in say two or three seconds\nyou're probably not going to give me an\naccurate answer\nnow let's look at another example\nsuppose i walk up to you and say the\nword\ndog you will never understand my\nstatement because well it doesn't make\nsense there are trilling combinations\nsolely using the word dog and among\nthese trillion combinations i'm\nexpecting you to now guess what i'm\ntrying to tell\nyou what these two examples have in\ncommon is that it doesn't make sense it\ndoesn't in the first case i'm expecting\nyou to predict the next position in time\nand in the second i'm expecting you to\nunderstand what i mean by dog these two\nexamples cannot be understood and\ninterpreted unless some information\nabout the pass was supplied\nnow in the first example if i give you\nthe previous position states of the ball\nand now ask you to predict the future\ntrajectory of the ball you're going to\nbe able to do this accurately\nand in the second case if i give you a\nfull sentence saying i have a dog\nthis makes sense because now you\nunderstand that out of the trillion\npossible combinations involving a dog my\noriginal intent was for you to\nunderstand that i have a dog\nwhy did i give you this example how does\nthis apply to neural networks\nin the introduction i said vanilla\nneural networks can't model every single\nsituation or problem that we have and\nthe biggest problem it turns out is that\nplain vanilla feed forward neural\nnetworks cannot model sequential data\nsequential data is data in a sequence\nfor example a sentence is a sequence of\nwhat a ball moving in space is a\nsequence of all its position states\nin the sentence that i've shown you you\nunderstood each word based off your\nunderstanding of the previous words\nthis is called sequential memory you\nwere able to understand the data point\nin the sequence by your memory of the\nprevious data point in that sequence\ntraditional neural networks can't do\nthis and it seems like a major\nshortcoming\none of the disadvantages of modelling\nsequences with traditional neural\nnetworks is the fact that they don't\nshare parameters across time\nlet us take for example these two\nsentences on tuesday it was raining and\nit was raining on tuesday\nthese sentences mean the same thing\nalthough the details are in different\nparts of the sequence\nactually when we feed these sentences\ninto a field for new network for a\nprediction task\nthe model will assign different weights\nto on tuesday and it was raining at each\nmoment in time\nthings we learn about the sequence won't\ntransfer if they appear at different\npoints in the sequence\nsharing parameters gives the network the\nability to look for a given feature\neverywhere in the sequence rather than\njust in a certain area\nthat's the model sequences we need a\nspecific learning framework able to deal\nwith variable lens sequences maintain\nsequence order and to keep track of\nlong-term dependencies rather than\ncutting input data too short\nand finally to share parameters across\nthe sequence so as to not reload things\nand that's where recurrent neural\nnetworks come in\nrnns are a type of new network\narchitecture that use something called a\nfeedback loop in the hidden layer\nunlike feed forward new networks the\nrecurrent neural network or rnn can\noperate effectively on sequences of data\nwith variable input length\nthis is how an rnn is usually\nrepresented\nthis little loop here is called the\nfeedback loop\nsometimes you may find the rnns depicted\nover time like this\nthe first path represents the network in\nthe first time step\nthe hidden node h1 uses the input x1 to\nproduce output y1\nthis is exactly what we've seen with\nbasic feed forward new networks however\nat the second time step the hidden node\nat the current time step h2 uses both\nthe new input x2 as well as the state\nfrom the previous time step h1 as input\nto make new predictions\nthis means that a current neural network\nuses knowledge of its previous states as\ninput for its current prediction\nand we can repeat this process for an\narbitrary number of steps allowing for\nthe network to propagate information by\nits hidden state throughout time\nthis is almost like giving a neural\nnetwork a short-term memory\nthey have this abstract concept of\nsequential memory and because of this\nwe're able to model certain areas of\ndata sequential data that standalone\nneural networks aren't able to model\nrecurrent neural networks remember their\npast and their decisions are influenced\nby what it has learned from the past\nbasic feed-forward networks remember\nthings too but they remember things they\nlearned during training for example an\nimage classifier learns what a three\nlooks like during training and then uses\nthat knowledge to classify things in\nproduction so how do we train an rnn\nwell it is almost the same as training a\nbasic fully connected feed forward\nnetwork except that the back propagation\nalgorithm is applied for every sequence\ndata point rather than the entire\nsequence\nthis algorithm is sometimes called the\nback propagation through time algorithm\nor the btt algorithm\nto really understand how this works\nimagine we're creating a recurring new\nnetwork to predict the next letter a\nperson is likely to type based on the\nprevious letters they've already typed\nthe letter that a user just typed is\nquite important to predicting the new\nletter\nhowever all the previous letters are\nalso very important to this prediction\nas well\nat the first time step say the user\ntyped the letter f so a network might\npredict that the next letter is in e\nbased on all of the previous training\nexamples that included the word fe\nat the next time step the user types the\nletter r so our network uses both the\nnew letter r plus the state of the first\nhidden neuron in order to compute the\nnext prediction l\nthe network predicts this because of the\nhigh frequency of occurrences in the\nword fel in our training data set\nadding the letter a might predict the\nletter t adding an n would predict the\nletter k which would match the word i\nuse in tender to type which is frank\nthere however is an issue with rnn's\nknown as short-term memory short-term\nmemory is caused by the infamous\nvanishing and exploding gradient\nproblems\nas the rnn processes more words it has\ntrouble retaining information from\nprevious steps kind of like our memory\nif you're given a long sequence of\nnumbers like pi and you tried reading\nthem out you're probably going to forget\nthe initial few digits right\nshort-term memory and the vanishing\ngradient is due to the nature of back\npropagation the algorithm used to train\nand optimize neural networks\nafter the forward propagation or the\npass\nthe network compares this prediction to\nthe ground truth using a loss function\nwhich outputs an error value\nan estimate of how poorly the network is\nperforming\nthe network uses that error value to\nperform back propagation which\ncalculates the gradients for each node\nin the network the gradient is the value\nused to adjust the network's internal\nweights allowing for the network to\nlearn\nthe bigger the gradient the bigger the\nadjustments are and vice versa\nhere's where the problem lies when\nperforming back propagation each node in\na layer calculates its gradient with\nrespect to the effects of the gradient\nin the layer before it\nso if the adjustment to the layers\nbefore it is small\nthen the adjustments to the current\nlayer will be even smaller\nand this causes gradients to\nexponentially shrink as it back\npropagates down the earlier layers fail\nto do any learning as the internal\nweights are barely being adjusted due to\nextremely small gradients and that is\nthe vanishing gradient problem\nlet's see how this applies to recover\nnew networks you can think of each time\nstep in a recurrent neural network as a\nlayer to train recon neural network you\nuse an application of back propagation\ncalled back propagation through time\nthe gradient values will exponentially\nshrink as the back propagates through\neach time step\nagain the gradient is used to make\nadjustments in the new network weights\nthus allowing it to learn\nsmall gradients means small adjustments\nand this causes the early layers not\nalone\nbecause of the vanishing gradient the\nrnn doesn't learn the long range\ndependencies across time steps\nthis means that in a sequence it was\nraining on tuesday there is a\npossibility that the words it and was\nare not considered when trying to\npredict the user's intention\nthe network then has to make the best\nguess with on tuesday and that's pretty\nambiguous and would be difficult even\nfor a human so not being able to learn\non earlier time steps causes the network\nto have a short-term memory\nwe can combat the short-term memory of\nan rnn by using two variants of\nrecurrent neural networks\ngated rnns and long short-term memory\nrnns also known as lcms both these\nvariants function just like rns but\nthey're capable of learning long-term\ndependencies using mechanisms called\ngates\nthese gates are different tensor\noperations that learn information that\ncan learn what information to add or\nremove to the hidden state or the\nfeedback loop\nthe main difference between a gated rnn\nand an lscm is that the gated rnn has\ntwo gates to control its memory an\nupdate gate and reset gate while an lsem\nhas three gates an input gate an output\ngate and a forget gate rnns work well\nfor applications that involve sequences\nof data that change over time\nthese applications include natural\nlanguage processing sentiment\nclassification dna sequence\nclassification speech recognition and\nlanguage translation\na convolutional neural network or cnn\nfor short is a type of deep neural\nnetwork architecture designed for\nspecific tasks like image classification\ncnns were inspired by the organization\nof neurons in the visual cortex of the\nanimal brain\nas a result they provide some very\ninteresting features that are useful for\nprocessing certain types of data like\nimages audio and video\nlike a fully connected neural network a\ncnn is composed of an input layer an\noutput layer and several hidden layers\nbetween the two\ncnns derive their names from the type of\nhidden layers it consists of\nthe hidden layers of a cnn typically\nconsist of convolutional layers pooling\nlayers fully connected layers and\nnormalization layers this means that\ninstead of traditional activation\nfunctions we use in feed-forward neural\nnetworks convolution and pooling\nfunctions are used instead more often\nthan not the input of a cnn is typically\na two-dimensional array of neurons which\ncorrespond to the pixels of an image for\nexample if you're doing image\nclassification\nthe output layer is typically\none-dimensional convolution is a\ntechnique that allows us to extract\nvisual features from a 2d array in small\nchunks each neuron in a convolution\nlayer is responsible for a small cluster\nof neurons in the preceding layer\nthe bounding box that determines the\ncluster of neurons is called a filter\nalso called a kernel\nconceptually you can think of it as a\nfilter moving across an image and\nperforming a mathematical operation on\nindividual regions of the image it then\nsends its result to the corresponding\nneuron in the convolution layer\nmathematically a convolution of two\nfunctions f and g is defined as follows\nwhich is in fact the dot product of the\ninput function and the kernel function\npooling also known as sub sampling or\ndown sampling is the next step in a\nconvolutional neural network its\nobjective is to further reduce the\nnumber of neurons necessary in\nsubsequent layers of the network while\nstill retaining the most important\ninformation\nthere are two different types of pooling\nthat can be performed max pulling and\nmin pooling\nas the name suggests max pooling is\nbased on picking up the maximum value\nfrom the selected region and min pooling\nis based on picking up the minimum value\nfrom that region\nwhen we put all these techniques\ntogether we get an architecture for a\ndeep neural network quite different from\na fully connected neural network for\nimage classification where cnns are used\nheavily we first take an input image\nwhich is a two-dimensional matrix of\npixels typically with three color\nchannels red green and blue\nnext we use a convolution layer with\nmultiple filters to create a\ntwo-dimensional feature matrix as the\noutput for each filter\nwe then pool the results to produce a\ndownsample feature matrix for each\nfilter in the convolution layer\nnext we typically repeat the convolution\nand pooling steps multiple times using\nprevious features as input\nthen we add a few fully connected hidden\nlayers to help classify the image\nand finally we produce a classification\nprediction in the output layer\nconvolutional neural networks are used\nheavily in the field of computer vision\nand work well for a variety of tasks\nincluding image recognition image\nprocessing image segmentation video\nanalysis and natural language processing\nin this section i'm going to discuss the\n5 steps that are common in every deep\nlearning project that you build\nthese can be extended to include various\nother aspects but at its very core they\nare very fundamentally five steps\ndata is at the core of what deep\nlearning is all about\nyour model will only be as powerful as\nthe data you bring\nwhich brings me to the first step\ngathering your data\nthe choice of data and how much data you\nwould require entirely depends on the\nproblem you're trying to solve\npicking the right data is key and i\ncan't stress how important this part is\nbad data implies a bad model\na good rule of thumb is to make\nassumptions about the data you require\nand be careful to record these\nassumptions so that you can test them\nlater if needed data comes in a variety\nof sizes for example iris flaw data set\ncontains about 150 images in the total\nset\ngmail smart reply has around 238 million\nexamples in its running sets and google\ntranslate reportedly has trillions of\ndata points\nwhen you're choosing a data set there's\nno one-size-fits-all but the general\nrule of thumb is that the amount of data\nyou need for a well-performing model\nshould be 10 times the number of\nparameters in that model\nhowever this may differ from time to\ntime depending on the type of model\nyou're building for example in\nregression analysis you should use\naround 10 examples per predictor\nvariable\nfor image classification the minimum you\nshould have is around a thousand images\nper class that you're trying to classify\nwhile quantity of data matters quality\nmatters too\nthere's no use having a lot of data if\nit's bad data\nthere are certain aspects of quality\nthat tend to correspond to\nwell-performing models\none aspect is reliability\nreliability refers to the degree in\nwhich you can trust your data a model\ntrain on a reliable data set is more\nlikely to yield useful predictions than\nmodel trained on unreliable data\nhow common are label errors if your data\nis labeled by humans sometimes there may\nbe mistakes are your features noisy is\nit completely accurate\nsome noise is all right you'll never be\nable to purge your data of all the noise\nthere are many other factors that\ndetermine quality for the purpose of\nthis video though i'm not going to talk\nabout the remaining although if you're\ninterested i'll leave them in the show\nnotes below lucky for us there are\nplenty of resources on the web that\noffer good data sets for free here are a\nfew sites where you can begin your\ndataset search the uci machine learning\nrepository maintains around 500\nextremely well maintained data sets that\nyou can use in your deep learning\nprojects\nkaggle's another one you'll love how\ndetailed that data sets are they give\nyou info on the features data types\nnumber of records and so on you can use\na kernel too and you won't have to\ndownload the data set\ngoogle's dataset search is still in beta\nbut is one of the most amazing sites\nthat you can find today\nreddit2 is a great place to request for\ndata sets you want but again there is a\nchance of not being properly organized\ncreate your own data set that will work\ntoo you can use web scrapers like\nbeautiful soup to get your required data\nfor the data set\nafter you have selected your data set\nyou now need to think of how you're\ngoing to use this data\nthere are some common pre-processing\nsteps that you should follow\nfirst splitting the data set into\nsubsets\nin general we usually split a data set\ninto three parts training testing and\nvalidating sets\nwe train our modules with the training\nset evaluated on the validation set and\nfinally once it's ready to use test it\none last time on the testing data set\nnow it is reasonable to ask the\nfollowing question\nwhy not have two sets training and\ntesting in that way the process will be\nmuch simpler just train the model on the\ntraining data and test it on the testing\ndata\nthe answer to that is developing a model\ninvolves tuning its configuration in\nother words choosing certain values for\nthe hyper parameters or the weight and\nbiases this tuning is done with the\nfeedback received from the validation\nset and is in essence a form of learning\nit turns out we just can't split the\ndata set randomly do that and you'll get\nrandom results there has to be some kind\nof logic to split the data set\nessentially what you want is for all\nthree sets the training testing and\nvalidation sets to be very similar to\neach other and to eliminate skewing as\nmuch as possible\nthis mainly depends on two things\nfirst the total number of samples in\nyour data and second or the actual model\nyou're trying to train\nmodels with very few hyper parameters\nwill be very easy to validate in tune so\nyou can probably reduce the size of your\nvalidation set\nbut if your model has many hyper\nparameters\nyou would want to have a large\nvalidation set as well as consider cross\nvalidation\nalso if you happen to have a model with\nno hyper parameters whatsoever or ones\nthat cannot be easily tuned you probably\ndon't need a validation set\nall in all like many other things in\nmachine learning and deep learning the\ntrain test validation split ratio is\nalso quite specific to your use case and\nit gets easier to make judgment as you\ntrain and build more and more models so\nhere's a quick note on cross validation\nusually you'd want to split your data\nset into two the train and the test\nafter this you keep aside the test set\nand randomly choose some percentage of\nthe training set to be the actual train\nset and the remaining to be the\nvalidation set\nthe model is then iteratively trained\nand validated on these different sets\nthere are multiple ways to do this and\nthis is commonly known as cross\nvalidation\nbasically you use your training set to\ngenerate multiple splits of the train\nand validation set cross validation\navoids overfitting and is getting more\nand more popular with k fold cross\nvalidation being the most popular method\nadditionally if you're working on time\nseries data a frequent technique is to\nsplit the data by time for example if\nyou have a dataset with 40 days of data\nyou can train your data from days 1 to\n39 and evaluate your model on the data\nfrom day 40.\nfor systems like this the training data\nis older than the serving data so this\ntechnique ensures your validation set\nmirrors the lag between training and\nserving\nhowever keep in mind that time-based\nsplits work best with very very large\ndata sets such as those with tens of\nmillions of examples the second method\nthat we have in pre-processing is\nformatting the data set that you've\npicked might not be in the right format\nthat you like for example the data might\nbe in the form of a database but you'd\nlike it as a csv file vice versa\nof course there are a couple of ways to\ndo this and you can google them if you'd\nlike dealing with missing data is one of\nthe most challenging steps in the\ngathering of data for your deep learning\nprojects unless you're extremely lucky\nto land with the perfect data set which\nis quite rare\ndealing with missing data will probably\ntake a significant chunk of your time\nit is quite common in real world\nproblems to miss some values of our data\nsamples\nthis may be due to errors on the data\ncollection blank spaces on surveys\nmeasurements not applicable and so on\nmissing values are typically represented\nwith the nan or the null indicators\nthe problem with this is that most\nalgorithms can't handle these kind of\nmissing values so we need to take care\nof them before feeding data to our\nmodels\nthere are a couple of ways to deal with\nthem\none is eliminating the samples of the\nfeatures with missing values the\ndownside of code is that you risk to\ndelete relevant information\nthe second step is to impute the missing\nvalues\na common way is to set the missing\nvalues as the mean value for the rest of\nthe samples but of course there are\nother ways to deal with specific data\nsets be smart as handling missing data\nin the wrong way can spell disasters\nsometimes you may have too much data\nthat what you require more data can\nresult in larger computational and\nmemory requirements\nin cases like this it's best practice to\nuse a small sample of the data set it\nwill be faster and ultimately an\nincrease in time for you to explore and\nprototype solutions\nin most real world data sets you're\ngoing to come across imbalanced data\nthat is classification data that has\nskewed class proportions leading to the\nrise of a minority class and a majority\nclass if we train a model on data like\nthis a model will only spend time\nlearning about the majority class and a\nlot less time on the minority class\nand hence a model will ultimately be\nbiased to the majority class and so in\ncases like this we usually use a process\ncalled down sampling and up weighting\nwhich is essentially reducing majority\ncost by some factor and adding example\nweights of that factor to the down\nsample class\nfor example if we down sample the\nmajority cost by a factor of 10 then the\nexample weighted we add to that class\nshould be 10. it may seem odd to add\nexample weights after down sampling\nwhat is its purpose well there are a\ncouple of reasons\nat least a faster convergence during\ntraining we see the minority class more\noften which helps the model converge\nfaster\nby consolidating the majority class into\nfewer examples with larger weights we\nspend less disk space storing them\noperating ensures their model is still\ncalibrated we add operating after down\nsampling so as to keep the data set in\nsimilar proportion\nthese processes essentially help a model\nsee more of the minority costs rather\nthan just solely the majority class\nthis helps our model perform better in\nreal world situations feature scaling is\na crucial step in the pre-processing\nphase as the majority of deep learning\nalgorithms perform much better when\ndealing with features that are on the\nsame scale\nthe most common techniques are\nnormalization which refers to the\nrescaling of features to a range between\n0 and 1\nwhich in fact is a special case of min\nmax scaling\nto normalize that data we need to apply\nmin max scaling to each feature column\nstandardization consists of centering\nthe field at mean 0 with standard\ndeviation 1 so that the feature columns\nhave the same parameters as a standard\nnormal distribution that is 0 mean and\nunit variance\nthis makes it much easier for the\nlearning algorithms to learn the weights\nof the parameters in addition it keeps\nuseful information about outliers and\nmakes the algorithms less sensitive to\nthem\nonce our data has been prepared we now\nfeed this into our network to trade\nwe've discussed the learning process of\na neural network in the previous module\nso if you are unsure i'd advise you to\nwatch that module first\nbut essentially once a data has been fed\nforward propagation occurs and the\nlosses compared against the loss\nfunction and the parameters are adjusted\nbased on this loss incurred again\nnothing too different from what we\ndiscussed previously\nyour model has successfully trained\ncongratulations\nnow we need to test how good our model\nis using the validation set that we had\nset aside earlier\nthe evaluation process allows us to test\na model against data it has never seen\nbefore and this is meant to be\nrepresentative of how good the model\nmight perform in the real world\nafter the evaluation process there's a\nhigh chance that your model could be\noptimized further\nremember we started with random weights\nand biases and these were fine-tuned\nduring back propagation\nwell in quite a few cases bad\npropagation won't get it right the first\ntime and that's okay there are a few\nways to optimize your model further\ntuning hyper parameters is a good way of\noptimizing your model's performance one\nway to do this is by showing the model\nthe entire data set multiple times\nthat is by increasing the number of\nepochs\nthis has sometimes shown to improve\naccuracy in other ways by adjusting the\nlearning rate we talked about what the\nlearning rate was in the previous module\nso if you don't know what the learning\nrate is i do advise you to check out the\nprevious module\nbut essentially the learning rate\ndefines how far we shift the line during\neach step based on information from the\nprevious training step in back\npropagation\nthese values all play a role in how\naccurate a model can become and how long\nthe training takes\nfor complex models initial conditions\ncan play a significant role in\ndetermining the outcome of training\nthere are many considerations at this\nphase of training and it's important you\ndefine what makes a model good enough\notherwise you might find yourself\ntweaking parameters for a long long time\nthe adjustment of these hyper parameters\nremains a bit of an art and is more of\nan experimental process that heavily\ndepends on the specifics of your data\nset model and training process\nyou will develop this as you go more and\nmore into deep learning so don't worry\ntoo much about this now\none of the more common problems you will\nencounter is when your model performs\nwell on training data but performs\nterribly on data it's never seen before\nthis is the problem of overfitting\nthis happens when the model learns a\npattern specific to the training data\nset that aren't relevant to other unseen\ndata\nthere are two ways to avoid this\noverfitting\ngetting more data and regularization\ngetting more data is usually the best\nsolution a model trainer mode data will\nnaturally generalize better\nreducing the model size by reducing the\nnumber of learnable parameters in the\nmodel and with it its learning capacity\nis another way\nhowever by lowering the capacity of the\nnetwork you force it to learn patterns\nthat matter or then minimize the loss\non the other hand reducing the network's\ncapacity too much will lead to\nunderfitting the model will not be able\nto learn the relevant patterns in the\ntrained data\nunfortunately there are no magical\nformulas to determine this balance it\nmust be tested and evaluated by setting\ndifferent number of parameters and\nobserving its performance the second\nmethod to addressing overfitting is by\napplying weight regularization to the\nmodel\na common way to achieve this is to\nconstraint the complexity of the network\nby forcing its weights to take only\nsmall values\nregularizing the distribution of weight\nvalues\nthis is done by adding to the loss\nfunction of the network a cost\nassociated with having larger weights\nand this cost comes in two ways l1\nregularization at the cost with regards\nto the absolute value of the weight\ncoefficient or the l1 norm of the\nweights\nl2 regularization adds a cost with\nregards to the squared value of the\nweight's coefficient that is the l2 norm\nof the weight\nanother way of reducing overfitting is\nby augmenting data\nfor a model to perform well or\nsatisfactory we need to have a lot of\ndata\nwe've just have just already but\ntypically if you're working with images\nthere's always a chance that your model\nwon't perform as well as you'd like it\nno matter how much data you have\nin cases like this when you have limited\ndata sets\ndata augmentation is a good way of\nincreasing your data set without really\nincreasing it we artificially augment\nour data or in this case images so that\nwe get more data from already existing\ndata so what kind of augmentations are\nwe talking about well anything from\nflipping the image of the y-axis\nflipping over the x-axis applying blur\nto even zooming on in the image\nwhat this does is that it shows your\nmodel more than what meets the eye it\nexposes your model to more of the\nexisting data so that in testing it will\nautomatically perform better because it\nhas seen images represented in almost\nevery single form\nfinally the last method we're going to\ntalk about is dropout dropout is a\ntechnique used in deep learning that\nrandomly drops out units or neurons in\nthe network\nsimply put dropout refers to the\nignoring of neurons during the training\nphase of a randomly chosen set of\nneurons\nby ignoring i mean that these units are\nnot considered during a particular\nforward or backward pass\nso why do we need dropout at all\nwhy do we need to shut down parts of a\nneural network a fully connected layer\noccupies most of the parameters and\nhence neurons develop a co-dependency\namongst each other during training\nwhich curbs the individual power of each\nneuron and which ultimately leads to\noverfitting of the training data\nso drop out a good way of reducing\noverfitting\ni hope that this introductory course has\nhelped you develop a good intuition of\ndeep learning as a whole\nof course we've only just scraped the\nsurface there's a whole new world out\nthere\nif you like this course please consider\nliking and subscribing it really helps\nme make courses like this\ni have a couple of videos on computer\nvision with opencv that i will be\nreleasing in a couple of weeks so stay\ntuned for that\nin the meantime good luck\n",
  "words": [
    "probably",
    "read",
    "news",
    "deep",
    "learning",
    "secret",
    "recipe",
    "behind",
    "many",
    "exciting",
    "developments",
    "made",
    "many",
    "world",
    "dreams",
    "perhaps",
    "also",
    "nightmares",
    "come",
    "true",
    "would",
    "thought",
    "deep",
    "minds",
    "alphago",
    "could",
    "least",
    "doll",
    "boat",
    "game",
    "boasts",
    "possible",
    "moves",
    "atoms",
    "entire",
    "universe",
    "lot",
    "people",
    "including",
    "never",
    "saw",
    "coming",
    "seemed",
    "impossible",
    "deep",
    "learning",
    "everywhere",
    "beating",
    "physicians",
    "diagnosing",
    "cancer",
    "responsible",
    "translating",
    "web",
    "pages",
    "matter",
    "mere",
    "seconds",
    "autonomous",
    "vehicles",
    "weimo",
    "tesla",
    "hi",
    "name",
    "jason",
    "welcome",
    "course",
    "deep",
    "learning",
    "learn",
    "everything",
    "need",
    "get",
    "started",
    "deep",
    "learning",
    "python",
    "build",
    "remarkable",
    "algorithms",
    "capable",
    "solving",
    "complex",
    "problems",
    "possible",
    "decades",
    "ago",
    "talk",
    "deep",
    "learning",
    "difference",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "introduce",
    "new",
    "networks",
    "essential",
    "deep",
    "learning",
    "going",
    "learn",
    "deep",
    "learning",
    "models",
    "train",
    "learn",
    "various",
    "types",
    "learning",
    "associated",
    "supervised",
    "unsupervised",
    "reinforcement",
    "learning",
    "going",
    "talk",
    "loss",
    "functions",
    "optimizers",
    "grading",
    "descent",
    "algorithm",
    "different",
    "types",
    "neural",
    "network",
    "architectures",
    "various",
    "steps",
    "involved",
    "deep",
    "learning",
    "entire",
    "course",
    "centered",
    "notion",
    "deep",
    "learning",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "turn",
    "subset",
    "artificial",
    "intelligence",
    "involves",
    "traditional",
    "methods",
    "learn",
    "representations",
    "directly",
    "data",
    "machine",
    "learning",
    "involves",
    "teaching",
    "computers",
    "recognize",
    "patterns",
    "data",
    "way",
    "brains",
    "humans",
    "easy",
    "us",
    "distinguish",
    "cat",
    "dog",
    "much",
    "difficult",
    "teach",
    "machine",
    "talk",
    "later",
    "course",
    "want",
    "give",
    "sense",
    "amazing",
    "successes",
    "deep",
    "learning",
    "past",
    "1997",
    "gary",
    "kasparov",
    "successful",
    "champion",
    "history",
    "chess",
    "lost",
    "ibm",
    "deep",
    "blue",
    "one",
    "first",
    "computer",
    "artificial",
    "systems",
    "first",
    "defeat",
    "reigning",
    "world",
    "chess",
    "champion",
    "computer",
    "2011",
    "ibm",
    "watson",
    "competed",
    "game",
    "show",
    "jeopardy",
    "champions",
    "brad",
    "rotter",
    "ken",
    "jennings",
    "first",
    "prize",
    "million",
    "dollars",
    "2015",
    "alphago",
    "deep",
    "learning",
    "computer",
    "program",
    "created",
    "google",
    "deepmind",
    "division",
    "defeated",
    "lisa",
    "dole",
    "18",
    "time",
    "world",
    "champion",
    "go",
    "game",
    "google",
    "times",
    "complex",
    "chess",
    "deep",
    "learning",
    "betas",
    "boat",
    "games",
    "finds",
    "applications",
    "anywhere",
    "vehicles",
    "fake",
    "news",
    "detection",
    "even",
    "predicting",
    "earthquakes",
    "astonishing",
    "moments",
    "machines",
    "beat",
    "humans",
    "games",
    "endless",
    "possibilities",
    "opened",
    "followed",
    "events",
    "series",
    "striking",
    "breakthroughs",
    "artificial",
    "intelligence",
    "machine",
    "learning",
    "yes",
    "deep",
    "learning",
    "put",
    "simply",
    "deep",
    "learning",
    "machine",
    "learning",
    "technique",
    "learns",
    "features",
    "tasks",
    "directly",
    "data",
    "running",
    "inputs",
    "biologically",
    "inspired",
    "neural",
    "network",
    "architecture",
    "neural",
    "networks",
    "contain",
    "number",
    "hidden",
    "layers",
    "data",
    "processed",
    "allowing",
    "machine",
    "go",
    "deep",
    "learning",
    "making",
    "connections",
    "weighing",
    "input",
    "best",
    "results",
    "go",
    "neural",
    "networks",
    "next",
    "video",
    "deep",
    "learning",
    "problem",
    "traditional",
    "machine",
    "learning",
    "algorithms",
    "matter",
    "complex",
    "get",
    "always",
    "machine",
    "like",
    "need",
    "lot",
    "domain",
    "expertise",
    "human",
    "intervention",
    "capable",
    "designed",
    "example",
    "show",
    "image",
    "face",
    "automatically",
    "recognize",
    "face",
    "would",
    "computer",
    "know",
    "well",
    "follow",
    "traditional",
    "machine",
    "learning",
    "manually",
    "painstakingly",
    "define",
    "computer",
    "faces",
    "example",
    "eyes",
    "ears",
    "mouth",
    "define",
    "eye",
    "mouth",
    "computer",
    "well",
    "look",
    "eye",
    "corners",
    "angle",
    "definitely",
    "90",
    "degrees",
    "definitely",
    "zero",
    "degrees",
    "angle",
    "could",
    "work",
    "train",
    "classifier",
    "recognize",
    "kinds",
    "lines",
    "certain",
    "orientations",
    "complicated",
    "ei",
    "practitioners",
    "rest",
    "world",
    "deep",
    "learning",
    "holds",
    "bit",
    "promise",
    "key",
    "idea",
    "deep",
    "learning",
    "learn",
    "features",
    "raw",
    "data",
    "feed",
    "bunch",
    "images",
    "faces",
    "deep",
    "learning",
    "algorithm",
    "going",
    "develop",
    "kind",
    "hierarchical",
    "representation",
    "detecting",
    "lines",
    "edges",
    "using",
    "lines",
    "edges",
    "detect",
    "eyes",
    "mouth",
    "composing",
    "together",
    "ultimately",
    "detect",
    "face",
    "turns",
    "underlying",
    "algorithms",
    "training",
    "models",
    "existed",
    "quite",
    "long",
    "time",
    "deep",
    "learning",
    "gaining",
    "popularity",
    "many",
    "decades",
    "later",
    "well",
    "one",
    "data",
    "become",
    "much",
    "pervasive",
    "living",
    "age",
    "big",
    "data",
    "algorithms",
    "require",
    "massive",
    "amounts",
    "data",
    "effectively",
    "implemented",
    "second",
    "hardware",
    "architecture",
    "capable",
    "handling",
    "vast",
    "amount",
    "data",
    "computational",
    "power",
    "algorithms",
    "require",
    "hardware",
    "simply",
    "available",
    "decades",
    "ago",
    "third",
    "building",
    "deploying",
    "algorithms",
    "models",
    "call",
    "extremely",
    "streamlined",
    "increasing",
    "popularity",
    "open",
    "source",
    "software",
    "like",
    "tensorflow",
    "pytorch",
    "deep",
    "learning",
    "models",
    "refer",
    "training",
    "things",
    "called",
    "neural",
    "networks",
    "neural",
    "networks",
    "form",
    "basis",
    "deep",
    "learning",
    "machine",
    "learning",
    "algorithms",
    "inspired",
    "structure",
    "human",
    "brain",
    "like",
    "neurons",
    "make",
    "brain",
    "fundamental",
    "building",
    "blocks",
    "neural",
    "network",
    "also",
    "neuron",
    "neural",
    "networks",
    "take",
    "data",
    "train",
    "recognize",
    "patterns",
    "data",
    "predict",
    "outputs",
    "new",
    "set",
    "similar",
    "data",
    "new",
    "network",
    "information",
    "propagates",
    "three",
    "central",
    "components",
    "form",
    "basis",
    "every",
    "neural",
    "network",
    "architecture",
    "input",
    "layer",
    "output",
    "layer",
    "several",
    "hidden",
    "layers",
    "two",
    "next",
    "video",
    "go",
    "learning",
    "process",
    "neural",
    "network",
    "learning",
    "process",
    "neural",
    "network",
    "broken",
    "two",
    "main",
    "processes",
    "forward",
    "propagation",
    "back",
    "propagation",
    "full",
    "propagation",
    "propagation",
    "information",
    "input",
    "layer",
    "output",
    "layer",
    "define",
    "input",
    "layer",
    "several",
    "neurons",
    "x1",
    "xn",
    "neurons",
    "connect",
    "neurons",
    "next",
    "layer",
    "channels",
    "assigned",
    "numerical",
    "values",
    "called",
    "weights",
    "inputs",
    "multiplied",
    "weights",
    "sum",
    "sent",
    "input",
    "neurons",
    "hidden",
    "layer",
    "neuron",
    "turn",
    "associated",
    "numerical",
    "value",
    "called",
    "bias",
    "added",
    "input",
    "sum",
    "weighted",
    "sum",
    "passed",
    "function",
    "called",
    "activation",
    "function",
    "essentially",
    "decides",
    "particular",
    "neuron",
    "contribute",
    "next",
    "layer",
    "output",
    "layer",
    "basically",
    "form",
    "probability",
    "neuron",
    "highest",
    "value",
    "determines",
    "output",
    "finally",
    "let",
    "go",
    "terms",
    "weight",
    "neuron",
    "tells",
    "us",
    "important",
    "neuron",
    "higher",
    "value",
    "important",
    "relationship",
    "bias",
    "like",
    "new",
    "opinion",
    "relationship",
    "serves",
    "shift",
    "activation",
    "function",
    "right",
    "left",
    "experience",
    "high",
    "school",
    "math",
    "know",
    "adding",
    "scalar",
    "value",
    "function",
    "shifts",
    "graph",
    "either",
    "left",
    "right",
    "exactly",
    "bias",
    "shifts",
    "activation",
    "function",
    "right",
    "left",
    "propagation",
    "almost",
    "like",
    "propagation",
    "except",
    "reverse",
    "direction",
    "information",
    "passed",
    "output",
    "layer",
    "hidden",
    "layers",
    "input",
    "layer",
    "information",
    "gets",
    "passed",
    "output",
    "layer",
    "output",
    "layer",
    "supposed",
    "final",
    "layer",
    "get",
    "final",
    "output",
    "well",
    "yes",
    "bad",
    "propagation",
    "reason",
    "new",
    "networks",
    "powerful",
    "reason",
    "new",
    "networks",
    "learn",
    "last",
    "step",
    "propagation",
    "new",
    "network",
    "spits",
    "prediction",
    "prediction",
    "could",
    "two",
    "possibilities",
    "either",
    "right",
    "wrong",
    "bad",
    "propagation",
    "new",
    "network",
    "evaluates",
    "performance",
    "checks",
    "right",
    "wrong",
    "wrong",
    "network",
    "uses",
    "something",
    "called",
    "loss",
    "function",
    "quantify",
    "deviation",
    "expected",
    "output",
    "information",
    "sent",
    "back",
    "hidden",
    "layers",
    "weights",
    "biases",
    "adjusted",
    "network",
    "accuracy",
    "level",
    "increases",
    "let",
    "visualize",
    "training",
    "process",
    "real",
    "example",
    "let",
    "suppose",
    "data",
    "set",
    "dataset",
    "gives",
    "us",
    "weight",
    "vehicle",
    "number",
    "goods",
    "carried",
    "vehicle",
    "also",
    "tells",
    "us",
    "vehicles",
    "cars",
    "trucks",
    "want",
    "go",
    "data",
    "trade",
    "new",
    "networks",
    "predict",
    "cars",
    "trucks",
    "based",
    "weights",
    "goods",
    "start",
    "let",
    "initialize",
    "neural",
    "network",
    "giving",
    "random",
    "weights",
    "biases",
    "anything",
    "really",
    "care",
    "values",
    "long",
    "first",
    "entry",
    "data",
    "set",
    "vehicle",
    "weight",
    "equal",
    "value",
    "case",
    "15",
    "good",
    "two",
    "according",
    "car",
    "start",
    "moving",
    "input",
    "dimensions",
    "newer",
    "network",
    "basically",
    "want",
    "take",
    "inputs",
    "multiply",
    "weight",
    "add",
    "bias",
    "magic",
    "happens",
    "run",
    "weighted",
    "sum",
    "activation",
    "function",
    "okay",
    "let",
    "say",
    "output",
    "activation",
    "function",
    "multiplied",
    "weight",
    "added",
    "bias",
    "finally",
    "output",
    "layer",
    "guess",
    "according",
    "neural",
    "network",
    "type",
    "vehicle",
    "weight",
    "15",
    "goods",
    "2",
    "greater",
    "probability",
    "truck",
    "course",
    "true",
    "new",
    "network",
    "knows",
    "use",
    "back",
    "propagation",
    "going",
    "quantify",
    "difference",
    "expected",
    "result",
    "predicted",
    "output",
    "using",
    "loss",
    "function",
    "bank",
    "propagation",
    "going",
    "go",
    "backwards",
    "adjust",
    "initial",
    "rates",
    "biases",
    "remember",
    "initialization",
    "neural",
    "network",
    "chose",
    "completely",
    "random",
    "weight",
    "biases",
    "well",
    "back",
    "propagation",
    "values",
    "adjusted",
    "better",
    "fit",
    "prediction",
    "model",
    "okay",
    "one",
    "iteration",
    "first",
    "piece",
    "data",
    "set",
    "second",
    "entry",
    "vehicle",
    "weight",
    "34",
    "goods",
    "going",
    "use",
    "process",
    "multiply",
    "input",
    "weight",
    "add",
    "box",
    "pass",
    "result",
    "activation",
    "function",
    "repeat",
    "till",
    "output",
    "layer",
    "check",
    "error",
    "difference",
    "employ",
    "back",
    "propagation",
    "adjust",
    "weights",
    "biases",
    "new",
    "network",
    "continue",
    "repeated",
    "processor",
    "propagation",
    "calculating",
    "error",
    "back",
    "propagation",
    "many",
    "entries",
    "data",
    "set",
    "data",
    "give",
    "newer",
    "network",
    "better",
    "predicting",
    "right",
    "output",
    "tradeoff",
    "much",
    "data",
    "end",
    "problem",
    "like",
    "overfitting",
    "discuss",
    "later",
    "course",
    "essentially",
    "new",
    "network",
    "works",
    "feed",
    "input",
    "network",
    "initializes",
    "random",
    "weights",
    "biases",
    "adjusted",
    "time",
    "back",
    "propagation",
    "network",
    "gone",
    "data",
    "able",
    "make",
    "predictions",
    "learning",
    "algorithm",
    "summarized",
    "follows",
    "first",
    "initialize",
    "network",
    "random",
    "values",
    "network",
    "parameters",
    "weights",
    "biases",
    "take",
    "set",
    "input",
    "data",
    "pass",
    "network",
    "compare",
    "predictions",
    "obtained",
    "values",
    "expected",
    "labels",
    "calculate",
    "loss",
    "using",
    "loss",
    "function",
    "perform",
    "back",
    "propagation",
    "order",
    "propagate",
    "loss",
    "every",
    "weight",
    "bias",
    "use",
    "propagated",
    "information",
    "update",
    "weights",
    "biases",
    "neural",
    "network",
    "gradient",
    "descent",
    "algorithm",
    "way",
    "total",
    "loss",
    "reduced",
    "better",
    "model",
    "obtained",
    "last",
    "step",
    "continue",
    "iterating",
    "previous",
    "steps",
    "consider",
    "good",
    "enough",
    "model",
    "section",
    "going",
    "talk",
    "common",
    "terminologies",
    "used",
    "deep",
    "learning",
    "today",
    "let",
    "start",
    "activation",
    "function",
    "activation",
    "function",
    "serves",
    "introduce",
    "something",
    "called",
    "network",
    "also",
    "decides",
    "whether",
    "particular",
    "neuron",
    "contribute",
    "next",
    "layer",
    "decide",
    "neuron",
    "fire",
    "activate",
    "well",
    "couple",
    "ideas",
    "led",
    "creation",
    "different",
    "activation",
    "functions",
    "first",
    "idea",
    "activate",
    "neuron",
    "certain",
    "value",
    "threshold",
    "less",
    "threshold",
    "activate",
    "activation",
    "function",
    "equal",
    "activated",
    "greater",
    "threshold",
    "else",
    "essentially",
    "step",
    "function",
    "output",
    "1",
    "activated",
    "value",
    "greater",
    "output",
    "activated",
    "value",
    "greater",
    "threshold",
    "outputs",
    "activated",
    "otherwise",
    "great",
    "makes",
    "activation",
    "function",
    "neuron",
    "confusions",
    "life",
    "perfect",
    "except",
    "drawbacks",
    "understand",
    "think",
    "following",
    "think",
    "case",
    "want",
    "classify",
    "multiple",
    "neurons",
    "classes",
    "say",
    "class",
    "1",
    "class",
    "2",
    "class",
    "3",
    "etc",
    "happen",
    "one",
    "neuron",
    "activated",
    "neurons",
    "output",
    "one",
    "well",
    "decide",
    "decide",
    "class",
    "belongs",
    "complicated",
    "right",
    "would",
    "want",
    "network",
    "activate",
    "one",
    "neuron",
    "zero",
    "would",
    "able",
    "say",
    "classified",
    "properly",
    "real",
    "practice",
    "however",
    "harder",
    "train",
    "converge",
    "way",
    "would",
    "better",
    "activation",
    "binary",
    "instead",
    "probable",
    "value",
    "like",
    "75",
    "activated",
    "16",
    "activated",
    "75",
    "chance",
    "belongs",
    "class",
    "2",
    "etc",
    "one",
    "neuron",
    "activates",
    "could",
    "find",
    "neuron",
    "fires",
    "based",
    "highest",
    "probability",
    "okay",
    "maybe",
    "ask",
    "want",
    "something",
    "give",
    "analog",
    "value",
    "rather",
    "saying",
    "activated",
    "activated",
    "something",
    "binary",
    "maybe",
    "would",
    "thought",
    "linear",
    "function",
    "straight",
    "line",
    "function",
    "activation",
    "proportional",
    "input",
    "value",
    "called",
    "slope",
    "line",
    "way",
    "gives",
    "us",
    "range",
    "activations",
    "binary",
    "activation",
    "definitely",
    "connect",
    "neurons",
    "together",
    "one",
    "fires",
    "could",
    "take",
    "maximum",
    "value",
    "decide",
    "based",
    "okay",
    "problem",
    "well",
    "familiar",
    "gradient",
    "descent",
    "come",
    "bit",
    "notice",
    "derivative",
    "linear",
    "function",
    "constant",
    "makes",
    "sense",
    "slope",
    "changing",
    "point",
    "function",
    "f",
    "x",
    "equal",
    "mx",
    "plus",
    "c",
    "derivative",
    "means",
    "gradient",
    "relationship",
    "whatsoever",
    "x",
    "also",
    "means",
    "back",
    "propagation",
    "adjustments",
    "made",
    "weights",
    "biases",
    "dependent",
    "x",
    "good",
    "thing",
    "additionally",
    "think",
    "connected",
    "layers",
    "matter",
    "many",
    "layers",
    "linear",
    "nature",
    "activation",
    "function",
    "final",
    "layer",
    "nothing",
    "linear",
    "function",
    "input",
    "first",
    "layer",
    "pause",
    "bit",
    "think",
    "means",
    "entire",
    "neural",
    "network",
    "dozens",
    "layers",
    "replaced",
    "single",
    "layer",
    "remember",
    "combination",
    "linear",
    "functions",
    "linear",
    "manner",
    "still",
    "another",
    "linear",
    "function",
    "terrible",
    "lost",
    "ability",
    "stack",
    "layers",
    "way",
    "matter",
    "much",
    "stack",
    "whole",
    "network",
    "still",
    "equivalent",
    "single",
    "layer",
    "single",
    "activation",
    "next",
    "sigmoid",
    "function",
    "ever",
    "watched",
    "video",
    "activation",
    "functions",
    "kind",
    "function",
    "used",
    "examples",
    "sigmoid",
    "function",
    "defined",
    "x",
    "equal",
    "1",
    "1",
    "plus",
    "e",
    "negative",
    "x",
    "well",
    "looks",
    "smooth",
    "kind",
    "like",
    "step",
    "function",
    "benefits",
    "think",
    "moment",
    "well",
    "first",
    "things",
    "first",
    "nature",
    "combinations",
    "function",
    "also",
    "great",
    "stack",
    "layers",
    "activations",
    "yes",
    "function",
    "outputs",
    "analog",
    "activation",
    "unlike",
    "step",
    "function",
    "also",
    "smooth",
    "gradient",
    "advantage",
    "activation",
    "function",
    "unlike",
    "linear",
    "function",
    "output",
    "function",
    "going",
    "range",
    "zero",
    "one",
    "inclusive",
    "compared",
    "negative",
    "infinity",
    "infinity",
    "latter",
    "activations",
    "bound",
    "range",
    "wo",
    "blow",
    "activations",
    "great",
    "sigmoid",
    "functions",
    "one",
    "widely",
    "used",
    "activation",
    "functions",
    "today",
    "life",
    "always",
    "rosy",
    "sigmoids",
    "two",
    "tend",
    "share",
    "disadvantages",
    "look",
    "closely",
    "x",
    "equal",
    "negative",
    "two",
    "x",
    "equal",
    "two",
    "values",
    "steep",
    "small",
    "changes",
    "values",
    "x",
    "region",
    "cause",
    "values",
    "change",
    "drastically",
    "also",
    "towards",
    "either",
    "end",
    "function",
    "values",
    "tend",
    "respond",
    "less",
    "changes",
    "x",
    "gradient",
    "regions",
    "going",
    "really",
    "really",
    "small",
    "almost",
    "zero",
    "gives",
    "rise",
    "vanishing",
    "gradient",
    "problem",
    "says",
    "input",
    "activation",
    "function",
    "either",
    "large",
    "small",
    "sigmoids",
    "going",
    "squish",
    "value",
    "zero",
    "one",
    "gradient",
    "function",
    "becomes",
    "really",
    "small",
    "see",
    "talk",
    "gradient",
    "descent",
    "huge",
    "problem",
    "another",
    "activation",
    "function",
    "used",
    "tan",
    "h",
    "function",
    "looks",
    "similar",
    "sigmoid",
    "fact",
    "mathematically",
    "known",
    "shifted",
    "sigmoid",
    "function",
    "okay",
    "like",
    "sigmoid",
    "characteristics",
    "discussed",
    "nonlinear",
    "nature",
    "stack",
    "layers",
    "bound",
    "arrange",
    "negative",
    "one",
    "one",
    "worrying",
    "activations",
    "blowing",
    "derivative",
    "tanning",
    "function",
    "however",
    "steeper",
    "sigmoid",
    "deciding",
    "sigmoid",
    "tanh",
    "really",
    "depend",
    "requirement",
    "gradient",
    "strength",
    "like",
    "sigmoid",
    "tanh",
    "also",
    "popular",
    "widely",
    "used",
    "activation",
    "function",
    "yes",
    "like",
    "sigmoid",
    "tanh",
    "vanishing",
    "gradient",
    "problem",
    "rectified",
    "linear",
    "unit",
    "value",
    "function",
    "defined",
    "x",
    "equal",
    "max",
    "0",
    "x",
    "first",
    "look",
    "would",
    "look",
    "like",
    "linear",
    "function",
    "right",
    "graph",
    "linear",
    "positive",
    "axis",
    "let",
    "tell",
    "rather",
    "fact",
    "nature",
    "combinations",
    "relu",
    "also",
    "great",
    "means",
    "stack",
    "layers",
    "however",
    "unlike",
    "previous",
    "two",
    "functions",
    "discussed",
    "bounded",
    "range",
    "relu",
    "zero",
    "infinity",
    "means",
    "chance",
    "blowing",
    "activation",
    "another",
    "point",
    "would",
    "like",
    "discuss",
    "sparsity",
    "inactivation",
    "imagine",
    "big",
    "neural",
    "network",
    "lots",
    "neurons",
    "using",
    "sigmoid",
    "tanning",
    "cause",
    "almost",
    "neurons",
    "fire",
    "analog",
    "way",
    "means",
    "almost",
    "activations",
    "processed",
    "describe",
    "network",
    "output",
    "words",
    "activation",
    "would",
    "dense",
    "costly",
    "ideally",
    "want",
    "neurons",
    "network",
    "activate",
    "thereby",
    "making",
    "activations",
    "pass",
    "efficient",
    "relu",
    "comes",
    "imagine",
    "network",
    "randomly",
    "initialized",
    "weights",
    "almost",
    "50",
    "percent",
    "network",
    "yields",
    "zero",
    "activation",
    "characteristic",
    "relu",
    "outputs",
    "zero",
    "negative",
    "values",
    "x",
    "means",
    "50",
    "percent",
    "neurons",
    "fire",
    "sparse",
    "activation",
    "making",
    "network",
    "lighter",
    "life",
    "gives",
    "apple",
    "comes",
    "little",
    "worm",
    "inside",
    "horizontal",
    "line",
    "relu",
    "negative",
    "values",
    "x",
    "gradient",
    "zero",
    "region",
    "means",
    "back",
    "propagation",
    "weights",
    "get",
    "adjusted",
    "descent",
    "means",
    "neurons",
    "go",
    "state",
    "stop",
    "responding",
    "variations",
    "error",
    "simply",
    "gradient",
    "zero",
    "nothing",
    "changes",
    "called",
    "dying",
    "value",
    "problem",
    "problem",
    "cause",
    "several",
    "neurons",
    "die",
    "respond",
    "thus",
    "making",
    "substantial",
    "part",
    "network",
    "passive",
    "rather",
    "want",
    "workarounds",
    "one",
    "way",
    "especially",
    "simply",
    "make",
    "horizontal",
    "line",
    "component",
    "adding",
    "slope",
    "usually",
    "slope",
    "around",
    "new",
    "version",
    "relu",
    "called",
    "leaky",
    "value",
    "main",
    "idea",
    "gradient",
    "never",
    "zero",
    "one",
    "major",
    "advantage",
    "relu",
    "fact",
    "less",
    "computationally",
    "expensive",
    "functions",
    "like",
    "tannage",
    "sigmoid",
    "involves",
    "simpler",
    "mathematical",
    "operations",
    "really",
    "good",
    "point",
    "consider",
    "designing",
    "deep",
    "neural",
    "networks",
    "great",
    "question",
    "activation",
    "function",
    "use",
    "advantages",
    "relu",
    "offers",
    "mean",
    "use",
    "reload",
    "everything",
    "could",
    "consider",
    "sigmoid",
    "tan",
    "h",
    "well",
    "know",
    "function",
    "trying",
    "approximate",
    "certain",
    "characteristics",
    "choose",
    "activation",
    "function",
    "approximate",
    "function",
    "faster",
    "leading",
    "faster",
    "training",
    "processes",
    "example",
    "sigmoid",
    "function",
    "works",
    "well",
    "binary",
    "classification",
    "problems",
    "approximating",
    "classifier",
    "functions",
    "combinations",
    "sigmoid",
    "easier",
    "maybe",
    "relu",
    "lead",
    "faster",
    "training",
    "processes",
    "larger",
    "convergence",
    "use",
    "custom",
    "functions",
    "know",
    "nature",
    "function",
    "trying",
    "learn",
    "would",
    "suggest",
    "start",
    "relu",
    "work",
    "backwards",
    "move",
    "next",
    "section",
    "want",
    "talk",
    "use",
    "activation",
    "functions",
    "opposed",
    "linear",
    "ones",
    "recall",
    "definition",
    "activation",
    "functions",
    "mentioned",
    "activation",
    "functions",
    "serve",
    "introduce",
    "something",
    "called",
    "network",
    "intensive",
    "purposes",
    "introducing",
    "simply",
    "means",
    "activation",
    "function",
    "must",
    "straight",
    "line",
    "mathematically",
    "linear",
    "functions",
    "polynomials",
    "degree",
    "1",
    "graphed",
    "x",
    "plane",
    "straight",
    "lines",
    "inclined",
    "certain",
    "value",
    "call",
    "slope",
    "line",
    "functions",
    "polynomials",
    "degree",
    "greater",
    "one",
    "graphed",
    "form",
    "straight",
    "lines",
    "rather",
    "curved",
    "use",
    "linear",
    "activation",
    "functions",
    "model",
    "data",
    "matter",
    "many",
    "hidden",
    "layers",
    "network",
    "always",
    "become",
    "equivalent",
    "single",
    "layer",
    "network",
    "deep",
    "learning",
    "want",
    "able",
    "model",
    "every",
    "type",
    "data",
    "without",
    "restricted",
    "would",
    "case",
    "use",
    "linear",
    "functions",
    "discussed",
    "previously",
    "learning",
    "process",
    "neural",
    "networks",
    "started",
    "random",
    "weight",
    "biases",
    "neural",
    "network",
    "makes",
    "prediction",
    "prediction",
    "compared",
    "expected",
    "output",
    "weights",
    "biases",
    "adjusted",
    "accordingly",
    "well",
    "loss",
    "functions",
    "reason",
    "able",
    "calculate",
    "difference",
    "really",
    "simply",
    "loss",
    "function",
    "way",
    "quantify",
    "deviation",
    "predicted",
    "output",
    "neural",
    "network",
    "expected",
    "output",
    "simple",
    "nothing",
    "mo",
    "nothing",
    "less",
    "plenty",
    "loss",
    "functions",
    "example",
    "regression",
    "squared",
    "error",
    "loss",
    "absolute",
    "ever",
    "loss",
    "huber",
    "loss",
    "binary",
    "classification",
    "binary",
    "cross",
    "entropy",
    "hinge",
    "loss",
    "classification",
    "problems",
    "cross",
    "entropy",
    "callback",
    "liability",
    "divergence",
    "loss",
    "choice",
    "best",
    "function",
    "really",
    "depends",
    "kind",
    "project",
    "working",
    "different",
    "projects",
    "require",
    "different",
    "loss",
    "functions",
    "want",
    "talk",
    "loss",
    "functions",
    "right",
    "optimization",
    "section",
    "really",
    "functions",
    "utilized",
    "previous",
    "section",
    "dealt",
    "loss",
    "functions",
    "mathematical",
    "ways",
    "measuring",
    "wrong",
    "predictions",
    "made",
    "neural",
    "network",
    "training",
    "process",
    "tweak",
    "change",
    "parameters",
    "weights",
    "model",
    "try",
    "minimize",
    "loss",
    "function",
    "make",
    "predictions",
    "correct",
    "optimized",
    "possible",
    "exactly",
    "change",
    "parameters",
    "model",
    "much",
    "ingredients",
    "make",
    "cake",
    "optimizers",
    "come",
    "tied",
    "together",
    "loss",
    "function",
    "model",
    "parameters",
    "weight",
    "biases",
    "updating",
    "network",
    "response",
    "output",
    "loss",
    "function",
    "simpler",
    "terms",
    "optimizers",
    "shape",
    "mold",
    "model",
    "accurate",
    "models",
    "adjusting",
    "weights",
    "biases",
    "loss",
    "function",
    "guide",
    "tells",
    "optimizer",
    "whether",
    "moving",
    "right",
    "wrong",
    "direction",
    "understand",
    "better",
    "imagine",
    "killed",
    "mount",
    "everest",
    "decide",
    "descend",
    "mountain",
    "blindfolded",
    "impossible",
    "know",
    "direction",
    "go",
    "could",
    "either",
    "go",
    "away",
    "goal",
    "go",
    "towards",
    "goal",
    "begin",
    "would",
    "start",
    "taking",
    "steps",
    "using",
    "feet",
    "able",
    "gauge",
    "whether",
    "going",
    "analogy",
    "resemble",
    "neural",
    "network",
    "going",
    "goal",
    "trying",
    "minimize",
    "error",
    "feet",
    "resemblance",
    "loss",
    "functions",
    "measure",
    "whether",
    "going",
    "right",
    "way",
    "wrong",
    "way",
    "similarly",
    "impossible",
    "know",
    "model",
    "weights",
    "right",
    "start",
    "trial",
    "error",
    "based",
    "loss",
    "function",
    "could",
    "end",
    "getting",
    "eventually",
    "come",
    "grading",
    "descent",
    "often",
    "called",
    "grand",
    "daddy",
    "optimizers",
    "grading",
    "descent",
    "iterative",
    "algorithm",
    "starts",
    "random",
    "point",
    "loss",
    "function",
    "travels",
    "slope",
    "steps",
    "reaches",
    "lowest",
    "point",
    "minimum",
    "function",
    "popular",
    "optimizer",
    "use",
    "nowadays",
    "fast",
    "robust",
    "flexible",
    "works",
    "first",
    "calculated",
    "small",
    "change",
    "individual",
    "weight",
    "would",
    "loss",
    "function",
    "adjust",
    "individual",
    "weight",
    "based",
    "gradient",
    "take",
    "small",
    "step",
    "determined",
    "direction",
    "last",
    "step",
    "repeat",
    "first",
    "second",
    "step",
    "loss",
    "function",
    "gets",
    "low",
    "possible",
    "want",
    "talk",
    "notion",
    "gradient",
    "gradient",
    "function",
    "vector",
    "partial",
    "derivatives",
    "respect",
    "independent",
    "variables",
    "gradient",
    "always",
    "points",
    "direction",
    "steepest",
    "increase",
    "function",
    "suppose",
    "graph",
    "like",
    "loss",
    "value",
    "weight",
    "little",
    "data",
    "point",
    "corresponds",
    "randomly",
    "initialized",
    "weight",
    "minimize",
    "loss",
    "get",
    "data",
    "point",
    "minimum",
    "function",
    "need",
    "take",
    "negative",
    "gradient",
    "since",
    "want",
    "find",
    "steepest",
    "decrease",
    "function",
    "process",
    "happens",
    "iteratively",
    "losses",
    "minimized",
    "possible",
    "grading",
    "descent",
    "nutshell",
    "dealing",
    "high",
    "dimensional",
    "data",
    "sets",
    "lot",
    "variables",
    "possible",
    "find",
    "area",
    "seems",
    "like",
    "reached",
    "lowest",
    "possible",
    "value",
    "loss",
    "function",
    "reality",
    "local",
    "minimum",
    "avoid",
    "getting",
    "stuck",
    "local",
    "minima",
    "make",
    "sure",
    "use",
    "proper",
    "learning",
    "rate",
    "changing",
    "weights",
    "fast",
    "adding",
    "subtracting",
    "much",
    "taking",
    "steps",
    "large",
    "small",
    "hinder",
    "ability",
    "minimize",
    "loss",
    "function",
    "want",
    "make",
    "jump",
    "large",
    "skip",
    "optimal",
    "value",
    "given",
    "weight",
    "make",
    "sure",
    "happen",
    "use",
    "variable",
    "called",
    "learning",
    "rate",
    "thing",
    "usually",
    "small",
    "number",
    "like",
    "multiply",
    "gradients",
    "scale",
    "ensures",
    "changes",
    "make",
    "weights",
    "pretty",
    "small",
    "math",
    "talk",
    "taking",
    "steps",
    "large",
    "mean",
    "algorithm",
    "never",
    "converge",
    "optimum",
    "time",
    "want",
    "take",
    "steps",
    "small",
    "might",
    "never",
    "end",
    "right",
    "values",
    "weights",
    "math",
    "talk",
    "steps",
    "small",
    "might",
    "lead",
    "optimizer",
    "converging",
    "local",
    "minimum",
    "loss",
    "function",
    "never",
    "absolute",
    "minimum",
    "simple",
    "summary",
    "remember",
    "learning",
    "rate",
    "ensures",
    "change",
    "weight",
    "right",
    "pace",
    "making",
    "changes",
    "big",
    "small",
    "instead",
    "calculating",
    "gradients",
    "training",
    "examples",
    "every",
    "part",
    "gradient",
    "descent",
    "sometimes",
    "efficient",
    "use",
    "subset",
    "training",
    "examples",
    "time",
    "stochastic",
    "gradient",
    "descent",
    "implementation",
    "either",
    "uses",
    "batches",
    "examples",
    "time",
    "random",
    "examples",
    "pass",
    "stochastic",
    "gradient",
    "descent",
    "uses",
    "concept",
    "momentum",
    "momentum",
    "accumulates",
    "gradients",
    "past",
    "steps",
    "dictate",
    "might",
    "happen",
    "next",
    "steps",
    "also",
    "include",
    "entire",
    "training",
    "set",
    "sjd",
    "less",
    "computationally",
    "expensive",
    "difficult",
    "overstate",
    "popular",
    "gradient",
    "descent",
    "really",
    "back",
    "propagation",
    "basically",
    "gradient",
    "descent",
    "implemented",
    "network",
    "types",
    "optimizers",
    "based",
    "gradient",
    "descent",
    "used",
    "today",
    "ad",
    "grad",
    "adapts",
    "learning",
    "rate",
    "specifically",
    "individual",
    "features",
    "means",
    "weights",
    "data",
    "set",
    "different",
    "learning",
    "rates",
    "others",
    "works",
    "really",
    "well",
    "sparse",
    "data",
    "sets",
    "lot",
    "input",
    "examples",
    "missing",
    "grad",
    "major",
    "issue",
    "though",
    "adaptive",
    "learning",
    "rate",
    "tends",
    "get",
    "really",
    "really",
    "small",
    "time",
    "rms",
    "prop",
    "special",
    "version",
    "adegrad",
    "developed",
    "professor",
    "jeffrey",
    "hinton",
    "instead",
    "letting",
    "gradients",
    "accumulate",
    "momentum",
    "accumulates",
    "gradients",
    "fixed",
    "window",
    "rms",
    "prop",
    "similar",
    "add",
    "prop",
    "another",
    "optimizer",
    "seeks",
    "solve",
    "issues",
    "atograd",
    "leaves",
    "open",
    "atom",
    "stands",
    "adaptive",
    "moment",
    "estimation",
    "another",
    "way",
    "using",
    "past",
    "gradients",
    "calculate",
    "carbon",
    "gradient",
    "atom",
    "also",
    "utilizes",
    "concept",
    "momentum",
    "basically",
    "way",
    "telling",
    "neural",
    "network",
    "whether",
    "want",
    "past",
    "changes",
    "affect",
    "new",
    "change",
    "adding",
    "fractions",
    "previous",
    "gradients",
    "current",
    "one",
    "optimizer",
    "become",
    "pretty",
    "widespread",
    "practically",
    "accepted",
    "use",
    "training",
    "new",
    "networks",
    "easy",
    "get",
    "lost",
    "complexity",
    "new",
    "optimizers",
    "remember",
    "goal",
    "minimizing",
    "loss",
    "function",
    "trial",
    "error",
    "get",
    "may",
    "heard",
    "referring",
    "words",
    "parameters",
    "quite",
    "bit",
    "often",
    "word",
    "confused",
    "term",
    "hyperparameters",
    "video",
    "going",
    "outline",
    "basic",
    "difference",
    "two",
    "model",
    "parameter",
    "variable",
    "internal",
    "new",
    "network",
    "whose",
    "values",
    "estimated",
    "data",
    "required",
    "model",
    "making",
    "predictions",
    "values",
    "define",
    "skill",
    "model",
    "problem",
    "estimated",
    "directly",
    "data",
    "often",
    "manually",
    "set",
    "practitioner",
    "oftentimes",
    "save",
    "model",
    "essentially",
    "saving",
    "model",
    "parameters",
    "parameters",
    "key",
    "machine",
    "learning",
    "algorithms",
    "examples",
    "include",
    "weights",
    "biases",
    "hyper",
    "parameter",
    "configuration",
    "external",
    "model",
    "whose",
    "value",
    "estimated",
    "data",
    "way",
    "find",
    "best",
    "value",
    "model",
    "hyper",
    "parameter",
    "given",
    "problem",
    "may",
    "use",
    "rules",
    "thumb",
    "copy",
    "values",
    "used",
    "problems",
    "search",
    "best",
    "value",
    "trial",
    "error",
    "machine",
    "learning",
    "algorithm",
    "tuned",
    "specific",
    "problem",
    "using",
    "grid",
    "search",
    "random",
    "search",
    "fact",
    "tuning",
    "hyper",
    "parameters",
    "model",
    "order",
    "discover",
    "parameters",
    "result",
    "skillful",
    "predictions",
    "model",
    "hyper",
    "parameters",
    "often",
    "referred",
    "parameters",
    "make",
    "things",
    "confusing",
    "good",
    "rule",
    "thumb",
    "overcome",
    "confusion",
    "follows",
    "specify",
    "parameter",
    "manually",
    "probably",
    "hyper",
    "parameter",
    "parameters",
    "inherent",
    "model",
    "examples",
    "hyper",
    "parameters",
    "include",
    "learning",
    "rate",
    "training",
    "network",
    "c",
    "sigma",
    "hyper",
    "parameters",
    "sport",
    "vector",
    "machines",
    "k",
    "k",
    "newest",
    "neighbors",
    "need",
    "terminologies",
    "like",
    "epochs",
    "batch",
    "size",
    "iterations",
    "data",
    "big",
    "happens",
    "time",
    "machine",
    "learning",
    "ca",
    "pass",
    "data",
    "computer",
    "overcome",
    "problem",
    "need",
    "divide",
    "data",
    "set",
    "smaller",
    "chunks",
    "give",
    "computer",
    "one",
    "one",
    "update",
    "weights",
    "new",
    "network",
    "end",
    "every",
    "step",
    "fit",
    "data",
    "given",
    "one",
    "epoch",
    "entire",
    "data",
    "set",
    "passed",
    "forward",
    "backward",
    "network",
    "majority",
    "deep",
    "learning",
    "models",
    "use",
    "one",
    "epoch",
    "know",
    "make",
    "sense",
    "beginning",
    "need",
    "pass",
    "entire",
    "data",
    "set",
    "many",
    "times",
    "neural",
    "network",
    "passing",
    "entire",
    "data",
    "set",
    "network",
    "trying",
    "read",
    "entire",
    "lyrics",
    "song",
    "wo",
    "able",
    "remember",
    "entire",
    "song",
    "immediately",
    "reread",
    "lyrics",
    "couple",
    "times",
    "say",
    "know",
    "song",
    "memory",
    "true",
    "neural",
    "network",
    "pass",
    "data",
    "set",
    "multiple",
    "times",
    "neural",
    "network",
    "able",
    "generalize",
    "better",
    "gradient",
    "descent",
    "iterative",
    "process",
    "updating",
    "parameters",
    "back",
    "propagation",
    "single",
    "pass",
    "one",
    "epoch",
    "enough",
    "number",
    "epochs",
    "increases",
    "parameters",
    "adjusted",
    "leading",
    "better",
    "performing",
    "model",
    "many",
    "epochs",
    "could",
    "spell",
    "disaster",
    "lead",
    "something",
    "called",
    "overfitting",
    "model",
    "essentially",
    "memorized",
    "patterns",
    "training",
    "data",
    "performs",
    "terribly",
    "data",
    "never",
    "seen",
    "right",
    "number",
    "epochs",
    "unfortunately",
    "right",
    "answer",
    "answer",
    "different",
    "different",
    "data",
    "sets",
    "sometimes",
    "data",
    "set",
    "include",
    "millions",
    "examples",
    "passing",
    "entire",
    "data",
    "set",
    "becomes",
    "extremely",
    "difficult",
    "instead",
    "divide",
    "data",
    "set",
    "number",
    "batches",
    "rather",
    "passing",
    "entire",
    "dataset",
    "total",
    "number",
    "training",
    "examples",
    "present",
    "single",
    "batch",
    "called",
    "batch",
    "size",
    "iterations",
    "number",
    "batches",
    "needed",
    "complete",
    "one",
    "epoch",
    "note",
    "number",
    "batches",
    "equal",
    "number",
    "iterations",
    "one",
    "epoch",
    "let",
    "say",
    "data",
    "set",
    "34",
    "000",
    "training",
    "examples",
    "divide",
    "data",
    "set",
    "batches",
    "500",
    "take",
    "68",
    "iterations",
    "complete",
    "one",
    "epoch",
    "well",
    "hope",
    "gives",
    "kind",
    "sense",
    "basic",
    "terminologies",
    "used",
    "deep",
    "learning",
    "move",
    "want",
    "mention",
    "see",
    "lot",
    "deep",
    "learning",
    "often",
    "bunch",
    "different",
    "choices",
    "make",
    "many",
    "hidden",
    "layers",
    "choose",
    "activation",
    "function",
    "must",
    "use",
    "honest",
    "guidelines",
    "choice",
    "always",
    "fun",
    "part",
    "deep",
    "learning",
    "extremely",
    "difficult",
    "know",
    "beginning",
    "right",
    "combination",
    "use",
    "project",
    "works",
    "might",
    "work",
    "suggestion",
    "end",
    "would",
    "dabble",
    "along",
    "material",
    "shown",
    "try",
    "various",
    "combinations",
    "see",
    "works",
    "best",
    "ultimately",
    "learning",
    "process",
    "pun",
    "intended",
    "throughout",
    "course",
    "give",
    "quite",
    "bit",
    "intuition",
    "popular",
    "comes",
    "building",
    "deep",
    "learning",
    "project",
    "wo",
    "find",
    "lost",
    "section",
    "going",
    "talk",
    "different",
    "types",
    "learning",
    "machine",
    "learning",
    "concepts",
    "extended",
    "deep",
    "learning",
    "well",
    "course",
    "go",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "supervised",
    "learning",
    "common",
    "sub",
    "branch",
    "machine",
    "learning",
    "today",
    "typically",
    "new",
    "machine",
    "learning",
    "journey",
    "begin",
    "supervised",
    "learning",
    "algorithms",
    "let",
    "explore",
    "supervised",
    "machine",
    "learning",
    "algorithms",
    "designed",
    "learn",
    "example",
    "name",
    "supervised",
    "learning",
    "originates",
    "idea",
    "training",
    "type",
    "algorithm",
    "almost",
    "like",
    "human",
    "supervising",
    "whole",
    "process",
    "supervised",
    "learning",
    "train",
    "models",
    "data",
    "example",
    "pair",
    "consisting",
    "input",
    "object",
    "typically",
    "vector",
    "desired",
    "output",
    "value",
    "also",
    "called",
    "supervisory",
    "signal",
    "training",
    "supervised",
    "learning",
    "algorithm",
    "search",
    "patterns",
    "data",
    "correlate",
    "desired",
    "outputs",
    "training",
    "take",
    "new",
    "unseen",
    "inputs",
    "determine",
    "label",
    "new",
    "inputs",
    "classified",
    "based",
    "prior",
    "training",
    "data",
    "objective",
    "supervised",
    "learning",
    "model",
    "predict",
    "correct",
    "label",
    "newly",
    "presented",
    "input",
    "data",
    "basic",
    "form",
    "supervised",
    "learning",
    "algorithm",
    "simply",
    "written",
    "equal",
    "f",
    "x",
    "predicted",
    "output",
    "determined",
    "mapping",
    "function",
    "assigns",
    "class",
    "input",
    "value",
    "x",
    "function",
    "used",
    "connect",
    "input",
    "features",
    "predicted",
    "output",
    "created",
    "machine",
    "learning",
    "model",
    "training",
    "supervised",
    "learning",
    "split",
    "two",
    "subcategories",
    "classification",
    "regression",
    "training",
    "classification",
    "algorithm",
    "given",
    "data",
    "points",
    "assigned",
    "category",
    "job",
    "classification",
    "algorithm",
    "take",
    "input",
    "value",
    "assigned",
    "class",
    "category",
    "fits",
    "based",
    "training",
    "data",
    "provided",
    "common",
    "example",
    "classification",
    "determining",
    "email",
    "spam",
    "two",
    "classes",
    "choose",
    "spam",
    "spam",
    "problem",
    "called",
    "binary",
    "classification",
    "problem",
    "algorithm",
    "given",
    "training",
    "data",
    "emails",
    "spam",
    "spam",
    "model",
    "find",
    "features",
    "within",
    "data",
    "correlate",
    "either",
    "class",
    "create",
    "mapping",
    "function",
    "provided",
    "unseen",
    "email",
    "model",
    "use",
    "function",
    "determine",
    "whether",
    "email",
    "pam",
    "example",
    "classification",
    "problem",
    "would",
    "mnist",
    "handwritten",
    "digits",
    "dataset",
    "inputs",
    "images",
    "handwritten",
    "digits",
    "pixel",
    "data",
    "output",
    "class",
    "label",
    "digit",
    "image",
    "represents",
    "numbers",
    "zero",
    "nine",
    "numerous",
    "algorithms",
    "solve",
    "classification",
    "problems",
    "depends",
    "data",
    "situation",
    "popular",
    "classification",
    "algorithms",
    "linear",
    "classifiers",
    "support",
    "vector",
    "machines",
    "decision",
    "trees",
    "neighbors",
    "drellum",
    "forest",
    "regression",
    "predictive",
    "statistical",
    "process",
    "model",
    "attempts",
    "find",
    "important",
    "relationship",
    "dependent",
    "independent",
    "variables",
    "goal",
    "regression",
    "algorithm",
    "predict",
    "continuous",
    "number",
    "sales",
    "income",
    "tax",
    "scores",
    "equation",
    "basic",
    "linear",
    "regression",
    "written",
    "follows",
    "x",
    "5",
    "represents",
    "features",
    "data",
    "w",
    "b",
    "parameters",
    "developed",
    "training",
    "simple",
    "linear",
    "regression",
    "models",
    "one",
    "feature",
    "data",
    "formula",
    "looks",
    "like",
    "w",
    "slope",
    "x",
    "single",
    "feature",
    "b",
    "familiar",
    "simple",
    "regression",
    "problems",
    "model",
    "predictions",
    "represented",
    "line",
    "best",
    "fit",
    "models",
    "using",
    "two",
    "features",
    "plane",
    "used",
    "models",
    "two",
    "features",
    "hyperplane",
    "used",
    "imagine",
    "want",
    "determine",
    "student",
    "test",
    "grade",
    "based",
    "many",
    "hours",
    "study",
    "week",
    "test",
    "let",
    "say",
    "plot",
    "data",
    "line",
    "best",
    "fit",
    "looks",
    "like",
    "clear",
    "positive",
    "correlation",
    "studied",
    "independent",
    "variable",
    "student",
    "final",
    "test",
    "goals",
    "dependent",
    "variable",
    "line",
    "best",
    "fit",
    "drawn",
    "data",
    "points",
    "show",
    "model",
    "predictions",
    "given",
    "new",
    "input",
    "say",
    "wanted",
    "know",
    "well",
    "student",
    "would",
    "five",
    "hours",
    "study",
    "use",
    "line",
    "best",
    "fit",
    "predict",
    "test",
    "call",
    "based",
    "students",
    "performances",
    "another",
    "example",
    "regression",
    "problem",
    "would",
    "boston",
    "house",
    "prices",
    "data",
    "set",
    "input",
    "variables",
    "describe",
    "neighborhood",
    "output",
    "house",
    "price",
    "dollars",
    "many",
    "different",
    "types",
    "regression",
    "algorithms",
    "three",
    "common",
    "linear",
    "regression",
    "lasso",
    "regression",
    "multivariate",
    "regression",
    "supervised",
    "learning",
    "finds",
    "applications",
    "classification",
    "regression",
    "problems",
    "like",
    "bioinformatics",
    "fingerprint",
    "iris",
    "face",
    "recognition",
    "smartphones",
    "object",
    "recognition",
    "spam",
    "detection",
    "speech",
    "recognition",
    "unsupervised",
    "learning",
    "branch",
    "machine",
    "learning",
    "used",
    "manifest",
    "underlying",
    "patterns",
    "data",
    "often",
    "used",
    "exploratory",
    "data",
    "analysis",
    "unlike",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "use",
    "label",
    "data",
    "instead",
    "focuses",
    "data",
    "features",
    "label",
    "training",
    "data",
    "corresponding",
    "output",
    "input",
    "goal",
    "unsupervised",
    "learning",
    "algorithm",
    "analyze",
    "data",
    "find",
    "important",
    "features",
    "data",
    "unsupervised",
    "learning",
    "often",
    "find",
    "subgroups",
    "hidden",
    "patterns",
    "within",
    "dataset",
    "human",
    "observer",
    "might",
    "pick",
    "extremely",
    "useful",
    "soon",
    "find",
    "unsupervised",
    "learning",
    "two",
    "types",
    "clustering",
    "association",
    "clustering",
    "simplest",
    "among",
    "common",
    "applications",
    "unsupervised",
    "learning",
    "process",
    "grouping",
    "given",
    "data",
    "different",
    "clusters",
    "groups",
    "classes",
    "contain",
    "data",
    "points",
    "similar",
    "possible",
    "dissimilar",
    "possible",
    "data",
    "points",
    "clusters",
    "clustering",
    "helps",
    "find",
    "underlying",
    "patterns",
    "within",
    "data",
    "may",
    "noticeable",
    "human",
    "observer",
    "broken",
    "partitional",
    "clustering",
    "hierarchical",
    "clustering",
    "partitional",
    "clustering",
    "refers",
    "set",
    "clustering",
    "algorithms",
    "data",
    "point",
    "data",
    "set",
    "belong",
    "one",
    "cluster",
    "hierarchical",
    "clustering",
    "finds",
    "clusters",
    "system",
    "hierarchies",
    "every",
    "data",
    "point",
    "belong",
    "multiple",
    "clusters",
    "classes",
    "contain",
    "smaller",
    "clusters",
    "within",
    "hierarchy",
    "system",
    "organized",
    "tree",
    "diagram",
    "commonly",
    "used",
    "clustering",
    "algorithms",
    "expectation",
    "hierarchical",
    "cluster",
    "analysis",
    "aca",
    "association",
    "hand",
    "attempts",
    "find",
    "relationships",
    "different",
    "entities",
    "classic",
    "example",
    "association",
    "rules",
    "market",
    "basket",
    "analysis",
    "means",
    "using",
    "database",
    "transactions",
    "supermarket",
    "find",
    "items",
    "frequently",
    "bought",
    "together",
    "example",
    "person",
    "buys",
    "potatoes",
    "burgers",
    "usually",
    "buys",
    "beer",
    "example",
    "person",
    "buys",
    "tomatoes",
    "pizza",
    "cheese",
    "might",
    "want",
    "bring",
    "pizza",
    "bread",
    "unsupervised",
    "learning",
    "finds",
    "applications",
    "almost",
    "everywhere",
    "example",
    "airbnb",
    "helps",
    "host",
    "stays",
    "experiences",
    "connect",
    "people",
    "world",
    "application",
    "uses",
    "unsupervised",
    "learning",
    "algorithms",
    "potential",
    "client",
    "queries",
    "requirements",
    "airbnb",
    "learns",
    "patterns",
    "recommends",
    "stays",
    "experiences",
    "fall",
    "group",
    "cluster",
    "example",
    "person",
    "looking",
    "houses",
    "san",
    "francisco",
    "might",
    "interested",
    "finding",
    "houses",
    "boston",
    "amazon",
    "also",
    "uses",
    "unsupervised",
    "learning",
    "learn",
    "customers",
    "purchases",
    "recommend",
    "products",
    "frequently",
    "bought",
    "together",
    "example",
    "association",
    "rule",
    "mining",
    "credit",
    "card",
    "fraud",
    "detection",
    "another",
    "unsupervised",
    "learning",
    "algorithm",
    "learns",
    "various",
    "patterns",
    "user",
    "usage",
    "credit",
    "card",
    "card",
    "used",
    "parts",
    "match",
    "behavior",
    "alarm",
    "generated",
    "could",
    "possibly",
    "marked",
    "fraud",
    "cases",
    "bank",
    "might",
    "call",
    "confirm",
    "whether",
    "using",
    "card",
    "reinforcement",
    "learning",
    "type",
    "machine",
    "learning",
    "technique",
    "enables",
    "agent",
    "learn",
    "interactive",
    "environment",
    "trial",
    "error",
    "using",
    "feedback",
    "actions",
    "experiences",
    "like",
    "supervised",
    "learning",
    "uses",
    "mapping",
    "input",
    "output",
    "unlike",
    "supervised",
    "learning",
    "feedback",
    "provided",
    "agent",
    "correct",
    "set",
    "actions",
    "performing",
    "task",
    "reinforcement",
    "learning",
    "uses",
    "rewards",
    "punishments",
    "signals",
    "positive",
    "negative",
    "behavior",
    "compare",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "different",
    "terms",
    "goals",
    "goal",
    "unsupervised",
    "learning",
    "find",
    "similarities",
    "differences",
    "data",
    "points",
    "reinforcement",
    "learning",
    "goal",
    "find",
    "suitable",
    "action",
    "model",
    "would",
    "maximize",
    "total",
    "cumulative",
    "reward",
    "agent",
    "reinforcement",
    "learning",
    "refers",
    "algorithms",
    "learn",
    "attain",
    "complex",
    "objective",
    "goal",
    "maximize",
    "along",
    "particular",
    "dimension",
    "many",
    "steps",
    "example",
    "maximize",
    "points",
    "one",
    "game",
    "many",
    "moves",
    "reinforcement",
    "learning",
    "algorithms",
    "start",
    "blank",
    "slate",
    "right",
    "conditions",
    "achieve",
    "superhuman",
    "performance",
    "like",
    "pet",
    "incentivized",
    "scolding",
    "treats",
    "algorithms",
    "penalized",
    "make",
    "wrong",
    "decisions",
    "rewarded",
    "make",
    "right",
    "ones",
    "reinforcement",
    "reinforcement",
    "learning",
    "usually",
    "modeled",
    "markov",
    "decision",
    "process",
    "although",
    "frameworks",
    "like",
    "queue",
    "learning",
    "used",
    "key",
    "terms",
    "describe",
    "elements",
    "reinforcement",
    "learning",
    "problem",
    "environment",
    "physical",
    "world",
    "agent",
    "operates",
    "state",
    "represents",
    "current",
    "situation",
    "agent",
    "reward",
    "feedback",
    "received",
    "environment",
    "policy",
    "sometimes",
    "method",
    "map",
    "agent",
    "state",
    "agent",
    "actions",
    "finally",
    "value",
    "future",
    "reward",
    "agent",
    "receive",
    "taking",
    "action",
    "particular",
    "state",
    "reinforcement",
    "learning",
    "problem",
    "best",
    "explained",
    "games",
    "let",
    "take",
    "game",
    "goal",
    "agent",
    "eat",
    "food",
    "grid",
    "avoiding",
    "ghosts",
    "way",
    "grid",
    "world",
    "interactive",
    "environment",
    "agent",
    "receives",
    "reward",
    "eating",
    "food",
    "punishment",
    "gets",
    "killed",
    "ghost",
    "loses",
    "game",
    "states",
    "location",
    "grid",
    "world",
    "total",
    "cumulative",
    "reward",
    "winning",
    "game",
    "reinforcement",
    "learning",
    "finds",
    "applications",
    "robotics",
    "business",
    "strategy",
    "planning",
    "traffic",
    "light",
    "control",
    "web",
    "system",
    "configuration",
    "aircraft",
    "robot",
    "motion",
    "control",
    "central",
    "problem",
    "deep",
    "learning",
    "make",
    "algorithm",
    "perform",
    "well",
    "training",
    "data",
    "also",
    "new",
    "inputs",
    "one",
    "common",
    "challenges",
    "face",
    "training",
    "models",
    "problem",
    "overfitting",
    "situation",
    "model",
    "performs",
    "exceptionally",
    "well",
    "training",
    "data",
    "testing",
    "data",
    "see",
    "data",
    "set",
    "graphed",
    "xy",
    "plane",
    "like",
    "want",
    "construct",
    "model",
    "would",
    "best",
    "fit",
    "data",
    "set",
    "could",
    "draw",
    "line",
    "random",
    "slope",
    "intercept",
    "evidently",
    "best",
    "model",
    "fact",
    "called",
    "underfitting",
    "fit",
    "model",
    "well",
    "fact",
    "underestimates",
    "data",
    "set",
    "instead",
    "could",
    "draw",
    "line",
    "looks",
    "something",
    "like",
    "really",
    "fits",
    "model",
    "best",
    "overfitting",
    "remember",
    "training",
    "show",
    "networks",
    "training",
    "data",
    "done",
    "expect",
    "almost",
    "close",
    "perfect",
    "problem",
    "graph",
    "although",
    "probably",
    "best",
    "line",
    "fit",
    "graph",
    "best",
    "line",
    "fit",
    "considering",
    "trading",
    "data",
    "network",
    "done",
    "graph",
    "memorize",
    "patterns",
    "trading",
    "data",
    "wo",
    "give",
    "accurate",
    "predictions",
    "data",
    "never",
    "seen",
    "makes",
    "sense",
    "instead",
    "memorizing",
    "patterns",
    "generally",
    "perform",
    "well",
    "training",
    "well",
    "new",
    "testing",
    "data",
    "network",
    "fact",
    "memorized",
    "patterns",
    "training",
    "data",
    "obviously",
    "wo",
    "perform",
    "well",
    "new",
    "data",
    "never",
    "seen",
    "problem",
    "overfitting",
    "fitted",
    "much",
    "way",
    "would",
    "accurate",
    "kind",
    "fitting",
    "perfect",
    "well",
    "training",
    "well",
    "new",
    "testing",
    "data",
    "sizeable",
    "accuracy",
    "couple",
    "ways",
    "tackle",
    "overfitting",
    "interesting",
    "type",
    "regularization",
    "dropout",
    "produces",
    "good",
    "results",
    "consequently",
    "frequently",
    "used",
    "regularization",
    "technique",
    "field",
    "deep",
    "learning",
    "understand",
    "dropout",
    "let",
    "say",
    "new",
    "network",
    "two",
    "hidden",
    "layers",
    "dropout",
    "every",
    "iteration",
    "randomly",
    "selects",
    "nodes",
    "removes",
    "along",
    "incoming",
    "outgoing",
    "connections",
    "shown",
    "iteration",
    "different",
    "set",
    "nodes",
    "results",
    "different",
    "set",
    "outputs",
    "models",
    "perform",
    "better",
    "models",
    "usually",
    "perform",
    "better",
    "single",
    "model",
    "capture",
    "randomness",
    "memorizes",
    "less",
    "training",
    "data",
    "hence",
    "folks",
    "generalize",
    "better",
    "build",
    "robust",
    "predictive",
    "model",
    "sometimes",
    "best",
    "way",
    "make",
    "deep",
    "learning",
    "model",
    "generalized",
    "better",
    "train",
    "mode",
    "data",
    "practice",
    "amount",
    "data",
    "limited",
    "one",
    "way",
    "get",
    "around",
    "problem",
    "create",
    "fake",
    "data",
    "add",
    "training",
    "set",
    "deep",
    "learning",
    "tasks",
    "reasonably",
    "straightforward",
    "create",
    "new",
    "fake",
    "data",
    "approach",
    "easiest",
    "classification",
    "classifier",
    "needs",
    "take",
    "complicated",
    "high",
    "dimensional",
    "input",
    "x",
    "summarize",
    "category",
    "identity",
    "means",
    "main",
    "task",
    "facing",
    "classifier",
    "invariant",
    "wide",
    "variety",
    "transformations",
    "generate",
    "new",
    "xy",
    "pairs",
    "easily",
    "applying",
    "transformations",
    "xy",
    "inputs",
    "training",
    "set",
    "dataset",
    "augmentation",
    "particularly",
    "effective",
    "technique",
    "specific",
    "classification",
    "problem",
    "object",
    "recognition",
    "images",
    "high",
    "dimensional",
    "include",
    "enormous",
    "range",
    "factors",
    "variation",
    "many",
    "easily",
    "simulated",
    "operations",
    "like",
    "translating",
    "training",
    "images",
    "pixels",
    "direction",
    "often",
    "greatly",
    "improve",
    "generalization",
    "many",
    "operations",
    "rotating",
    "image",
    "scaling",
    "image",
    "also",
    "proved",
    "quite",
    "effective",
    "must",
    "careful",
    "apply",
    "transformation",
    "would",
    "change",
    "correct",
    "class",
    "example",
    "optical",
    "character",
    "recognition",
    "tasks",
    "require",
    "recognizing",
    "difference",
    "b",
    "difference",
    "six",
    "nine",
    "horizontal",
    "flips",
    "180",
    "degree",
    "rotations",
    "appropriate",
    "ways",
    "augmenting",
    "data",
    "sets",
    "tasks",
    "training",
    "large",
    "models",
    "sufficient",
    "representational",
    "capacity",
    "overfit",
    "task",
    "often",
    "observe",
    "training",
    "error",
    "decreases",
    "steadily",
    "time",
    "error",
    "validation",
    "set",
    "begins",
    "rise",
    "means",
    "obtain",
    "model",
    "better",
    "validation",
    "set",
    "error",
    "thus",
    "hopefully",
    "better",
    "test",
    "error",
    "stopping",
    "training",
    "point",
    "error",
    "validation",
    "set",
    "starts",
    "increase",
    "strategy",
    "known",
    "early",
    "stopping",
    "probably",
    "commonly",
    "used",
    "form",
    "regularization",
    "deploying",
    "today",
    "popularity",
    "due",
    "effectiveness",
    "simplicity",
    "section",
    "going",
    "introduce",
    "three",
    "common",
    "types",
    "neural",
    "network",
    "architectures",
    "today",
    "fully",
    "connected",
    "v4",
    "new",
    "networks",
    "recurring",
    "neural",
    "networks",
    "convolutional",
    "neural",
    "networks",
    "first",
    "type",
    "new",
    "network",
    "architecture",
    "going",
    "discuss",
    "fully",
    "connected",
    "feed",
    "forward",
    "neural",
    "network",
    "fully",
    "connected",
    "mean",
    "neuron",
    "preceding",
    "layer",
    "connected",
    "every",
    "neuron",
    "subsequent",
    "layer",
    "without",
    "connection",
    "backwards",
    "cycles",
    "loops",
    "connections",
    "network",
    "mentioned",
    "previously",
    "neuron",
    "neural",
    "network",
    "contains",
    "activation",
    "function",
    "changes",
    "output",
    "neuron",
    "given",
    "input",
    "several",
    "types",
    "activation",
    "functions",
    "change",
    "input",
    "output",
    "relationship",
    "make",
    "neuron",
    "behave",
    "variety",
    "ways",
    "activation",
    "functions",
    "linear",
    "function",
    "straight",
    "line",
    "essentially",
    "multiplies",
    "input",
    "constant",
    "value",
    "sigmoid",
    "function",
    "ranges",
    "zero",
    "one",
    "hyperbolic",
    "tangent",
    "tanning",
    "function",
    "ranges",
    "negative",
    "one",
    "positive",
    "one",
    "rectified",
    "linear",
    "unit",
    "relu",
    "function",
    "piecewise",
    "function",
    "outputs",
    "zero",
    "input",
    "less",
    "certain",
    "value",
    "linear",
    "multiple",
    "input",
    "greater",
    "certain",
    "value",
    "type",
    "activation",
    "function",
    "pros",
    "cons",
    "use",
    "various",
    "layers",
    "deep",
    "neural",
    "network",
    "based",
    "problem",
    "designed",
    "solve",
    "addition",
    "last",
    "three",
    "activation",
    "functions",
    "refer",
    "functions",
    "output",
    "linear",
    "multiple",
    "input",
    "allows",
    "deep",
    "neural",
    "networks",
    "model",
    "complex",
    "functions",
    "using",
    "everything",
    "learned",
    "far",
    "create",
    "wide",
    "variety",
    "fully",
    "connected",
    "feed",
    "neural",
    "networks",
    "create",
    "networks",
    "various",
    "inputs",
    "various",
    "outputs",
    "various",
    "hidden",
    "layers",
    "neurons",
    "per",
    "hidden",
    "layer",
    "variety",
    "activation",
    "functions",
    "numerous",
    "combinations",
    "allow",
    "us",
    "create",
    "variety",
    "powerful",
    "deep",
    "neural",
    "networks",
    "solve",
    "wide",
    "array",
    "problems",
    "neurons",
    "add",
    "hidden",
    "layer",
    "wider",
    "network",
    "becomes",
    "addition",
    "hidden",
    "layers",
    "add",
    "deeper",
    "network",
    "becomes",
    "however",
    "neuron",
    "add",
    "increases",
    "complexity",
    "thus",
    "computational",
    "resource",
    "necessary",
    "train",
    "new",
    "network",
    "increases",
    "increasing",
    "complexity",
    "linear",
    "number",
    "neurons",
    "add",
    "leads",
    "explosion",
    "complexity",
    "training",
    "time",
    "large",
    "neural",
    "networks",
    "need",
    "consider",
    "building",
    "deep",
    "neural",
    "networks",
    "new",
    "networks",
    "discussed",
    "far",
    "known",
    "feed",
    "forward",
    "neural",
    "networks",
    "take",
    "fixed",
    "sized",
    "input",
    "give",
    "fixed",
    "size",
    "output",
    "expect",
    "neural",
    "networks",
    "take",
    "input",
    "give",
    "sizeable",
    "output",
    "turns",
    "plane",
    "vanilla",
    "neural",
    "networks",
    "able",
    "model",
    "every",
    "single",
    "problem",
    "today",
    "better",
    "understand",
    "use",
    "analogy",
    "suppose",
    "show",
    "picture",
    "ball",
    "round",
    "spherical",
    "ball",
    "moving",
    "space",
    "direction",
    "taken",
    "photo",
    "ball",
    "snapshot",
    "ball",
    "time",
    "want",
    "predict",
    "next",
    "position",
    "ball",
    "say",
    "two",
    "three",
    "seconds",
    "probably",
    "going",
    "give",
    "accurate",
    "answer",
    "let",
    "look",
    "another",
    "example",
    "suppose",
    "walk",
    "say",
    "word",
    "dog",
    "never",
    "understand",
    "statement",
    "well",
    "make",
    "sense",
    "trilling",
    "combinations",
    "solely",
    "using",
    "word",
    "dog",
    "among",
    "trillion",
    "combinations",
    "expecting",
    "guess",
    "trying",
    "tell",
    "two",
    "examples",
    "common",
    "make",
    "sense",
    "first",
    "case",
    "expecting",
    "predict",
    "next",
    "position",
    "time",
    "second",
    "expecting",
    "understand",
    "mean",
    "dog",
    "two",
    "examples",
    "understood",
    "interpreted",
    "unless",
    "information",
    "pass",
    "supplied",
    "first",
    "example",
    "give",
    "previous",
    "position",
    "states",
    "ball",
    "ask",
    "predict",
    "future",
    "trajectory",
    "ball",
    "going",
    "able",
    "accurately",
    "second",
    "case",
    "give",
    "full",
    "sentence",
    "saying",
    "dog",
    "makes",
    "sense",
    "understand",
    "trillion",
    "possible",
    "combinations",
    "involving",
    "dog",
    "original",
    "intent",
    "understand",
    "dog",
    "give",
    "example",
    "apply",
    "neural",
    "networks",
    "introduction",
    "said",
    "vanilla",
    "neural",
    "networks",
    "ca",
    "model",
    "every",
    "single",
    "situation",
    "problem",
    "biggest",
    "problem",
    "turns",
    "plain",
    "vanilla",
    "feed",
    "forward",
    "neural",
    "networks",
    "model",
    "sequential",
    "data",
    "sequential",
    "data",
    "data",
    "sequence",
    "example",
    "sentence",
    "sequence",
    "ball",
    "moving",
    "space",
    "sequence",
    "position",
    "states",
    "sentence",
    "shown",
    "understood",
    "word",
    "based",
    "understanding",
    "previous",
    "words",
    "called",
    "sequential",
    "memory",
    "able",
    "understand",
    "data",
    "point",
    "sequence",
    "memory",
    "previous",
    "data",
    "point",
    "sequence",
    "traditional",
    "neural",
    "networks",
    "ca",
    "seems",
    "like",
    "major",
    "shortcoming",
    "one",
    "disadvantages",
    "modelling",
    "sequences",
    "traditional",
    "neural",
    "networks",
    "fact",
    "share",
    "parameters",
    "across",
    "time",
    "let",
    "us",
    "take",
    "example",
    "two",
    "sentences",
    "tuesday",
    "raining",
    "raining",
    "tuesday",
    "sentences",
    "mean",
    "thing",
    "although",
    "details",
    "different",
    "parts",
    "sequence",
    "actually",
    "feed",
    "sentences",
    "field",
    "new",
    "network",
    "prediction",
    "task",
    "model",
    "assign",
    "different",
    "weights",
    "tuesday",
    "raining",
    "moment",
    "time",
    "things",
    "learn",
    "sequence",
    "wo",
    "transfer",
    "appear",
    "different",
    "points",
    "sequence",
    "sharing",
    "parameters",
    "gives",
    "network",
    "ability",
    "look",
    "given",
    "feature",
    "everywhere",
    "sequence",
    "rather",
    "certain",
    "area",
    "model",
    "sequences",
    "need",
    "specific",
    "learning",
    "framework",
    "able",
    "deal",
    "variable",
    "lens",
    "sequences",
    "maintain",
    "sequence",
    "order",
    "keep",
    "track",
    "dependencies",
    "rather",
    "cutting",
    "input",
    "data",
    "short",
    "finally",
    "share",
    "parameters",
    "across",
    "sequence",
    "reload",
    "things",
    "recurrent",
    "neural",
    "networks",
    "come",
    "rnns",
    "type",
    "new",
    "network",
    "architecture",
    "use",
    "something",
    "called",
    "feedback",
    "loop",
    "hidden",
    "layer",
    "unlike",
    "feed",
    "forward",
    "new",
    "networks",
    "recurrent",
    "neural",
    "network",
    "rnn",
    "operate",
    "effectively",
    "sequences",
    "data",
    "variable",
    "input",
    "length",
    "rnn",
    "usually",
    "represented",
    "little",
    "loop",
    "called",
    "feedback",
    "loop",
    "sometimes",
    "may",
    "find",
    "rnns",
    "depicted",
    "time",
    "like",
    "first",
    "path",
    "represents",
    "network",
    "first",
    "time",
    "step",
    "hidden",
    "node",
    "h1",
    "uses",
    "input",
    "x1",
    "produce",
    "output",
    "y1",
    "exactly",
    "seen",
    "basic",
    "feed",
    "forward",
    "new",
    "networks",
    "however",
    "second",
    "time",
    "step",
    "hidden",
    "node",
    "current",
    "time",
    "step",
    "h2",
    "uses",
    "new",
    "input",
    "x2",
    "well",
    "state",
    "previous",
    "time",
    "step",
    "h1",
    "input",
    "make",
    "new",
    "predictions",
    "means",
    "current",
    "neural",
    "network",
    "uses",
    "knowledge",
    "previous",
    "states",
    "input",
    "current",
    "prediction",
    "repeat",
    "process",
    "arbitrary",
    "number",
    "steps",
    "allowing",
    "network",
    "propagate",
    "information",
    "hidden",
    "state",
    "throughout",
    "time",
    "almost",
    "like",
    "giving",
    "neural",
    "network",
    "memory",
    "abstract",
    "concept",
    "sequential",
    "memory",
    "able",
    "model",
    "certain",
    "areas",
    "data",
    "sequential",
    "data",
    "standalone",
    "neural",
    "networks",
    "able",
    "model",
    "recurrent",
    "neural",
    "networks",
    "remember",
    "past",
    "decisions",
    "influenced",
    "learned",
    "past",
    "basic",
    "networks",
    "remember",
    "things",
    "remember",
    "things",
    "learned",
    "training",
    "example",
    "image",
    "classifier",
    "learns",
    "three",
    "looks",
    "like",
    "training",
    "uses",
    "knowledge",
    "classify",
    "things",
    "production",
    "train",
    "rnn",
    "well",
    "almost",
    "training",
    "basic",
    "fully",
    "connected",
    "feed",
    "forward",
    "network",
    "except",
    "back",
    "propagation",
    "algorithm",
    "applied",
    "every",
    "sequence",
    "data",
    "point",
    "rather",
    "entire",
    "sequence",
    "algorithm",
    "sometimes",
    "called",
    "back",
    "propagation",
    "time",
    "algorithm",
    "btt",
    "algorithm",
    "really",
    "understand",
    "works",
    "imagine",
    "creating",
    "recurring",
    "new",
    "network",
    "predict",
    "next",
    "letter",
    "person",
    "likely",
    "type",
    "based",
    "previous",
    "letters",
    "already",
    "typed",
    "letter",
    "user",
    "typed",
    "quite",
    "important",
    "predicting",
    "new",
    "letter",
    "however",
    "previous",
    "letters",
    "also",
    "important",
    "prediction",
    "well",
    "first",
    "time",
    "step",
    "say",
    "user",
    "typed",
    "letter",
    "f",
    "network",
    "might",
    "predict",
    "next",
    "letter",
    "e",
    "based",
    "previous",
    "training",
    "examples",
    "included",
    "word",
    "fe",
    "next",
    "time",
    "step",
    "user",
    "types",
    "letter",
    "r",
    "network",
    "uses",
    "new",
    "letter",
    "r",
    "plus",
    "state",
    "first",
    "hidden",
    "neuron",
    "order",
    "compute",
    "next",
    "prediction",
    "l",
    "network",
    "predicts",
    "high",
    "frequency",
    "occurrences",
    "word",
    "fel",
    "training",
    "data",
    "set",
    "adding",
    "letter",
    "might",
    "predict",
    "letter",
    "adding",
    "n",
    "would",
    "predict",
    "letter",
    "k",
    "would",
    "match",
    "word",
    "use",
    "tender",
    "type",
    "frank",
    "however",
    "issue",
    "rnn",
    "known",
    "memory",
    "memory",
    "caused",
    "infamous",
    "vanishing",
    "exploding",
    "gradient",
    "problems",
    "rnn",
    "processes",
    "words",
    "trouble",
    "retaining",
    "information",
    "previous",
    "steps",
    "kind",
    "like",
    "memory",
    "given",
    "long",
    "sequence",
    "numbers",
    "like",
    "pi",
    "tried",
    "reading",
    "probably",
    "going",
    "forget",
    "initial",
    "digits",
    "right",
    "memory",
    "vanishing",
    "gradient",
    "due",
    "nature",
    "back",
    "propagation",
    "algorithm",
    "used",
    "train",
    "optimize",
    "neural",
    "networks",
    "forward",
    "propagation",
    "pass",
    "network",
    "compares",
    "prediction",
    "ground",
    "truth",
    "using",
    "loss",
    "function",
    "outputs",
    "error",
    "value",
    "estimate",
    "poorly",
    "network",
    "performing",
    "network",
    "uses",
    "error",
    "value",
    "perform",
    "back",
    "propagation",
    "calculates",
    "gradients",
    "node",
    "network",
    "gradient",
    "value",
    "used",
    "adjust",
    "network",
    "internal",
    "weights",
    "allowing",
    "network",
    "learn",
    "bigger",
    "gradient",
    "bigger",
    "adjustments",
    "vice",
    "versa",
    "problem",
    "lies",
    "performing",
    "back",
    "propagation",
    "node",
    "layer",
    "calculates",
    "gradient",
    "respect",
    "effects",
    "gradient",
    "layer",
    "adjustment",
    "layers",
    "small",
    "adjustments",
    "current",
    "layer",
    "even",
    "smaller",
    "causes",
    "gradients",
    "exponentially",
    "shrink",
    "back",
    "propagates",
    "earlier",
    "layers",
    "fail",
    "learning",
    "internal",
    "weights",
    "barely",
    "adjusted",
    "due",
    "extremely",
    "small",
    "gradients",
    "vanishing",
    "gradient",
    "problem",
    "let",
    "see",
    "applies",
    "recover",
    "new",
    "networks",
    "think",
    "time",
    "step",
    "recurrent",
    "neural",
    "network",
    "layer",
    "train",
    "recon",
    "neural",
    "network",
    "use",
    "application",
    "back",
    "propagation",
    "called",
    "back",
    "propagation",
    "time",
    "gradient",
    "values",
    "exponentially",
    "shrink",
    "back",
    "propagates",
    "time",
    "step",
    "gradient",
    "used",
    "make",
    "adjustments",
    "new",
    "network",
    "weights",
    "thus",
    "allowing",
    "learn",
    "small",
    "gradients",
    "means",
    "small",
    "adjustments",
    "causes",
    "early",
    "layers",
    "alone",
    "vanishing",
    "gradient",
    "rnn",
    "learn",
    "long",
    "range",
    "dependencies",
    "across",
    "time",
    "steps",
    "means",
    "sequence",
    "raining",
    "tuesday",
    "possibility",
    "words",
    "considered",
    "trying",
    "predict",
    "user",
    "intention",
    "network",
    "make",
    "best",
    "guess",
    "tuesday",
    "pretty",
    "ambiguous",
    "would",
    "difficult",
    "even",
    "human",
    "able",
    "learn",
    "earlier",
    "time",
    "steps",
    "causes",
    "network",
    "memory",
    "combat",
    "memory",
    "rnn",
    "using",
    "two",
    "variants",
    "recurrent",
    "neural",
    "networks",
    "gated",
    "rnns",
    "long",
    "memory",
    "rnns",
    "also",
    "known",
    "lcms",
    "variants",
    "function",
    "like",
    "rns",
    "capable",
    "learning",
    "dependencies",
    "using",
    "mechanisms",
    "called",
    "gates",
    "gates",
    "different",
    "tensor",
    "operations",
    "learn",
    "information",
    "learn",
    "information",
    "add",
    "remove",
    "hidden",
    "state",
    "feedback",
    "loop",
    "main",
    "difference",
    "gated",
    "rnn",
    "lscm",
    "gated",
    "rnn",
    "two",
    "gates",
    "control",
    "memory",
    "update",
    "gate",
    "reset",
    "gate",
    "lsem",
    "three",
    "gates",
    "input",
    "gate",
    "output",
    "gate",
    "forget",
    "gate",
    "rnns",
    "work",
    "well",
    "applications",
    "involve",
    "sequences",
    "data",
    "change",
    "time",
    "applications",
    "include",
    "natural",
    "language",
    "processing",
    "sentiment",
    "classification",
    "dna",
    "sequence",
    "classification",
    "speech",
    "recognition",
    "language",
    "translation",
    "convolutional",
    "neural",
    "network",
    "cnn",
    "short",
    "type",
    "deep",
    "neural",
    "network",
    "architecture",
    "designed",
    "specific",
    "tasks",
    "like",
    "image",
    "classification",
    "cnns",
    "inspired",
    "organization",
    "neurons",
    "visual",
    "cortex",
    "animal",
    "brain",
    "result",
    "provide",
    "interesting",
    "features",
    "useful",
    "processing",
    "certain",
    "types",
    "data",
    "like",
    "images",
    "audio",
    "video",
    "like",
    "fully",
    "connected",
    "neural",
    "network",
    "cnn",
    "composed",
    "input",
    "layer",
    "output",
    "layer",
    "several",
    "hidden",
    "layers",
    "two",
    "cnns",
    "derive",
    "names",
    "type",
    "hidden",
    "layers",
    "consists",
    "hidden",
    "layers",
    "cnn",
    "typically",
    "consist",
    "convolutional",
    "layers",
    "pooling",
    "layers",
    "fully",
    "connected",
    "layers",
    "normalization",
    "layers",
    "means",
    "instead",
    "traditional",
    "activation",
    "functions",
    "use",
    "neural",
    "networks",
    "convolution",
    "pooling",
    "functions",
    "used",
    "instead",
    "often",
    "input",
    "cnn",
    "typically",
    "array",
    "neurons",
    "correspond",
    "pixels",
    "image",
    "example",
    "image",
    "classification",
    "output",
    "layer",
    "typically",
    "convolution",
    "technique",
    "allows",
    "us",
    "extract",
    "visual",
    "features",
    "2d",
    "array",
    "small",
    "chunks",
    "neuron",
    "convolution",
    "layer",
    "responsible",
    "small",
    "cluster",
    "neurons",
    "preceding",
    "layer",
    "bounding",
    "box",
    "determines",
    "cluster",
    "neurons",
    "called",
    "filter",
    "also",
    "called",
    "kernel",
    "conceptually",
    "think",
    "filter",
    "moving",
    "across",
    "image",
    "performing",
    "mathematical",
    "operation",
    "individual",
    "regions",
    "image",
    "sends",
    "result",
    "corresponding",
    "neuron",
    "convolution",
    "layer",
    "mathematically",
    "convolution",
    "two",
    "functions",
    "f",
    "g",
    "defined",
    "follows",
    "fact",
    "dot",
    "product",
    "input",
    "function",
    "kernel",
    "function",
    "pooling",
    "also",
    "known",
    "sub",
    "sampling",
    "sampling",
    "next",
    "step",
    "convolutional",
    "neural",
    "network",
    "objective",
    "reduce",
    "number",
    "neurons",
    "necessary",
    "subsequent",
    "layers",
    "network",
    "still",
    "retaining",
    "important",
    "information",
    "two",
    "different",
    "types",
    "pooling",
    "performed",
    "max",
    "pulling",
    "min",
    "pooling",
    "name",
    "suggests",
    "max",
    "pooling",
    "based",
    "picking",
    "maximum",
    "value",
    "selected",
    "region",
    "min",
    "pooling",
    "based",
    "picking",
    "minimum",
    "value",
    "region",
    "put",
    "techniques",
    "together",
    "get",
    "architecture",
    "deep",
    "neural",
    "network",
    "quite",
    "different",
    "fully",
    "connected",
    "neural",
    "network",
    "image",
    "classification",
    "cnns",
    "used",
    "heavily",
    "first",
    "take",
    "input",
    "image",
    "matrix",
    "pixels",
    "typically",
    "three",
    "color",
    "channels",
    "red",
    "green",
    "blue",
    "next",
    "use",
    "convolution",
    "layer",
    "multiple",
    "filters",
    "create",
    "feature",
    "matrix",
    "output",
    "filter",
    "pool",
    "results",
    "produce",
    "downsample",
    "feature",
    "matrix",
    "filter",
    "convolution",
    "layer",
    "next",
    "typically",
    "repeat",
    "convolution",
    "pooling",
    "steps",
    "multiple",
    "times",
    "using",
    "previous",
    "features",
    "input",
    "add",
    "fully",
    "connected",
    "hidden",
    "layers",
    "help",
    "classify",
    "image",
    "finally",
    "produce",
    "classification",
    "prediction",
    "output",
    "layer",
    "convolutional",
    "neural",
    "networks",
    "used",
    "heavily",
    "field",
    "computer",
    "vision",
    "work",
    "well",
    "variety",
    "tasks",
    "including",
    "image",
    "recognition",
    "image",
    "processing",
    "image",
    "segmentation",
    "video",
    "analysis",
    "natural",
    "language",
    "processing",
    "section",
    "going",
    "discuss",
    "5",
    "steps",
    "common",
    "every",
    "deep",
    "learning",
    "project",
    "build",
    "extended",
    "include",
    "various",
    "aspects",
    "core",
    "fundamentally",
    "five",
    "steps",
    "data",
    "core",
    "deep",
    "learning",
    "model",
    "powerful",
    "data",
    "bring",
    "brings",
    "first",
    "step",
    "gathering",
    "data",
    "choice",
    "data",
    "much",
    "data",
    "would",
    "require",
    "entirely",
    "depends",
    "problem",
    "trying",
    "solve",
    "picking",
    "right",
    "data",
    "key",
    "ca",
    "stress",
    "important",
    "part",
    "bad",
    "data",
    "implies",
    "bad",
    "model",
    "good",
    "rule",
    "thumb",
    "make",
    "assumptions",
    "data",
    "require",
    "careful",
    "record",
    "assumptions",
    "test",
    "later",
    "needed",
    "data",
    "comes",
    "variety",
    "sizes",
    "example",
    "iris",
    "flaw",
    "data",
    "set",
    "contains",
    "150",
    "images",
    "total",
    "set",
    "gmail",
    "smart",
    "reply",
    "around",
    "238",
    "million",
    "examples",
    "running",
    "sets",
    "google",
    "translate",
    "reportedly",
    "trillions",
    "data",
    "points",
    "choosing",
    "data",
    "set",
    "general",
    "rule",
    "thumb",
    "amount",
    "data",
    "need",
    "model",
    "10",
    "times",
    "number",
    "parameters",
    "model",
    "however",
    "may",
    "differ",
    "time",
    "time",
    "depending",
    "type",
    "model",
    "building",
    "example",
    "regression",
    "analysis",
    "use",
    "around",
    "10",
    "examples",
    "per",
    "predictor",
    "variable",
    "image",
    "classification",
    "minimum",
    "around",
    "thousand",
    "images",
    "per",
    "class",
    "trying",
    "classify",
    "quantity",
    "data",
    "matters",
    "quality",
    "matters",
    "use",
    "lot",
    "data",
    "bad",
    "data",
    "certain",
    "aspects",
    "quality",
    "tend",
    "correspond",
    "models",
    "one",
    "aspect",
    "reliability",
    "reliability",
    "refers",
    "degree",
    "trust",
    "data",
    "model",
    "train",
    "reliable",
    "data",
    "set",
    "likely",
    "yield",
    "useful",
    "predictions",
    "model",
    "trained",
    "unreliable",
    "data",
    "common",
    "label",
    "errors",
    "data",
    "labeled",
    "humans",
    "sometimes",
    "may",
    "mistakes",
    "features",
    "noisy",
    "completely",
    "accurate",
    "noise",
    "right",
    "never",
    "able",
    "purge",
    "data",
    "noise",
    "many",
    "factors",
    "determine",
    "quality",
    "purpose",
    "video",
    "though",
    "going",
    "talk",
    "remaining",
    "although",
    "interested",
    "leave",
    "show",
    "notes",
    "lucky",
    "us",
    "plenty",
    "resources",
    "web",
    "offer",
    "good",
    "data",
    "sets",
    "free",
    "sites",
    "begin",
    "dataset",
    "search",
    "uci",
    "machine",
    "learning",
    "repository",
    "maintains",
    "around",
    "500",
    "extremely",
    "well",
    "maintained",
    "data",
    "sets",
    "use",
    "deep",
    "learning",
    "projects",
    "kaggle",
    "another",
    "one",
    "love",
    "detailed",
    "data",
    "sets",
    "give",
    "info",
    "features",
    "data",
    "types",
    "number",
    "records",
    "use",
    "kernel",
    "wo",
    "download",
    "data",
    "set",
    "google",
    "dataset",
    "search",
    "still",
    "beta",
    "one",
    "amazing",
    "sites",
    "find",
    "today",
    "reddit2",
    "great",
    "place",
    "request",
    "data",
    "sets",
    "want",
    "chance",
    "properly",
    "organized",
    "create",
    "data",
    "set",
    "work",
    "use",
    "web",
    "scrapers",
    "like",
    "beautiful",
    "soup",
    "get",
    "required",
    "data",
    "data",
    "set",
    "selected",
    "data",
    "set",
    "need",
    "think",
    "going",
    "use",
    "data",
    "common",
    "steps",
    "follow",
    "first",
    "splitting",
    "data",
    "set",
    "subsets",
    "general",
    "usually",
    "split",
    "data",
    "set",
    "three",
    "parts",
    "training",
    "testing",
    "validating",
    "sets",
    "train",
    "modules",
    "training",
    "set",
    "evaluated",
    "validation",
    "set",
    "finally",
    "ready",
    "use",
    "test",
    "one",
    "last",
    "time",
    "testing",
    "data",
    "set",
    "reasonable",
    "ask",
    "following",
    "question",
    "two",
    "sets",
    "training",
    "testing",
    "way",
    "process",
    "much",
    "simpler",
    "train",
    "model",
    "training",
    "data",
    "test",
    "testing",
    "data",
    "answer",
    "developing",
    "model",
    "involves",
    "tuning",
    "configuration",
    "words",
    "choosing",
    "certain",
    "values",
    "hyper",
    "parameters",
    "weight",
    "biases",
    "tuning",
    "done",
    "feedback",
    "received",
    "validation",
    "set",
    "essence",
    "form",
    "learning",
    "turns",
    "ca",
    "split",
    "data",
    "set",
    "randomly",
    "get",
    "random",
    "results",
    "kind",
    "logic",
    "split",
    "data",
    "set",
    "essentially",
    "want",
    "three",
    "sets",
    "training",
    "testing",
    "validation",
    "sets",
    "similar",
    "eliminate",
    "skewing",
    "much",
    "possible",
    "mainly",
    "depends",
    "two",
    "things",
    "first",
    "total",
    "number",
    "samples",
    "data",
    "second",
    "actual",
    "model",
    "trying",
    "train",
    "models",
    "hyper",
    "parameters",
    "easy",
    "validate",
    "tune",
    "probably",
    "reduce",
    "size",
    "validation",
    "set",
    "model",
    "many",
    "hyper",
    "parameters",
    "would",
    "want",
    "large",
    "validation",
    "set",
    "well",
    "consider",
    "cross",
    "validation",
    "also",
    "happen",
    "model",
    "hyper",
    "parameters",
    "whatsoever",
    "ones",
    "easily",
    "tuned",
    "probably",
    "need",
    "validation",
    "set",
    "like",
    "many",
    "things",
    "machine",
    "learning",
    "deep",
    "learning",
    "train",
    "test",
    "validation",
    "split",
    "ratio",
    "also",
    "quite",
    "specific",
    "use",
    "case",
    "gets",
    "easier",
    "make",
    "judgment",
    "train",
    "build",
    "models",
    "quick",
    "note",
    "cross",
    "validation",
    "usually",
    "want",
    "split",
    "data",
    "set",
    "two",
    "train",
    "test",
    "keep",
    "aside",
    "test",
    "set",
    "randomly",
    "choose",
    "percentage",
    "training",
    "set",
    "actual",
    "train",
    "set",
    "remaining",
    "validation",
    "set",
    "model",
    "iteratively",
    "trained",
    "validated",
    "different",
    "sets",
    "multiple",
    "ways",
    "commonly",
    "known",
    "cross",
    "validation",
    "basically",
    "use",
    "training",
    "set",
    "generate",
    "multiple",
    "splits",
    "train",
    "validation",
    "set",
    "cross",
    "validation",
    "avoids",
    "overfitting",
    "getting",
    "popular",
    "k",
    "fold",
    "cross",
    "validation",
    "popular",
    "method",
    "additionally",
    "working",
    "time",
    "series",
    "data",
    "frequent",
    "technique",
    "split",
    "data",
    "time",
    "example",
    "dataset",
    "40",
    "days",
    "data",
    "train",
    "data",
    "days",
    "1",
    "39",
    "evaluate",
    "model",
    "data",
    "day",
    "systems",
    "like",
    "training",
    "data",
    "older",
    "serving",
    "data",
    "technique",
    "ensures",
    "validation",
    "set",
    "mirrors",
    "lag",
    "training",
    "serving",
    "however",
    "keep",
    "mind",
    "splits",
    "work",
    "best",
    "large",
    "data",
    "sets",
    "tens",
    "millions",
    "examples",
    "second",
    "method",
    "formatting",
    "data",
    "set",
    "picked",
    "might",
    "right",
    "format",
    "like",
    "example",
    "data",
    "might",
    "form",
    "database",
    "like",
    "csv",
    "file",
    "vice",
    "versa",
    "course",
    "couple",
    "ways",
    "google",
    "like",
    "dealing",
    "missing",
    "data",
    "one",
    "challenging",
    "steps",
    "gathering",
    "data",
    "deep",
    "learning",
    "projects",
    "unless",
    "extremely",
    "lucky",
    "land",
    "perfect",
    "data",
    "set",
    "quite",
    "rare",
    "dealing",
    "missing",
    "data",
    "probably",
    "take",
    "significant",
    "chunk",
    "time",
    "quite",
    "common",
    "real",
    "world",
    "problems",
    "miss",
    "values",
    "data",
    "samples",
    "may",
    "due",
    "errors",
    "data",
    "collection",
    "blank",
    "spaces",
    "surveys",
    "measurements",
    "applicable",
    "missing",
    "values",
    "typically",
    "represented",
    "nan",
    "null",
    "indicators",
    "problem",
    "algorithms",
    "ca",
    "handle",
    "kind",
    "missing",
    "values",
    "need",
    "take",
    "care",
    "feeding",
    "data",
    "models",
    "couple",
    "ways",
    "deal",
    "one",
    "eliminating",
    "samples",
    "features",
    "missing",
    "values",
    "downside",
    "code",
    "risk",
    "delete",
    "relevant",
    "information",
    "second",
    "step",
    "impute",
    "missing",
    "values",
    "common",
    "way",
    "set",
    "missing",
    "values",
    "mean",
    "value",
    "rest",
    "samples",
    "course",
    "ways",
    "deal",
    "specific",
    "data",
    "sets",
    "smart",
    "handling",
    "missing",
    "data",
    "wrong",
    "way",
    "spell",
    "disasters",
    "sometimes",
    "may",
    "much",
    "data",
    "require",
    "data",
    "result",
    "larger",
    "computational",
    "memory",
    "requirements",
    "cases",
    "like",
    "best",
    "practice",
    "use",
    "small",
    "sample",
    "data",
    "set",
    "faster",
    "ultimately",
    "increase",
    "time",
    "explore",
    "prototype",
    "solutions",
    "real",
    "world",
    "data",
    "sets",
    "going",
    "come",
    "across",
    "imbalanced",
    "data",
    "classification",
    "data",
    "skewed",
    "class",
    "proportions",
    "leading",
    "rise",
    "minority",
    "class",
    "majority",
    "class",
    "train",
    "model",
    "data",
    "like",
    "model",
    "spend",
    "time",
    "learning",
    "majority",
    "class",
    "lot",
    "less",
    "time",
    "minority",
    "class",
    "hence",
    "model",
    "ultimately",
    "biased",
    "majority",
    "class",
    "cases",
    "like",
    "usually",
    "use",
    "process",
    "called",
    "sampling",
    "weighting",
    "essentially",
    "reducing",
    "majority",
    "cost",
    "factor",
    "adding",
    "example",
    "weights",
    "factor",
    "sample",
    "class",
    "example",
    "sample",
    "majority",
    "cost",
    "factor",
    "10",
    "example",
    "weighted",
    "add",
    "class",
    "may",
    "seem",
    "odd",
    "add",
    "example",
    "weights",
    "sampling",
    "purpose",
    "well",
    "couple",
    "reasons",
    "least",
    "faster",
    "convergence",
    "training",
    "see",
    "minority",
    "class",
    "often",
    "helps",
    "model",
    "converge",
    "faster",
    "consolidating",
    "majority",
    "class",
    "fewer",
    "examples",
    "larger",
    "weights",
    "spend",
    "less",
    "disk",
    "space",
    "storing",
    "operating",
    "ensures",
    "model",
    "still",
    "calibrated",
    "add",
    "operating",
    "sampling",
    "keep",
    "data",
    "set",
    "similar",
    "proportion",
    "processes",
    "essentially",
    "help",
    "model",
    "see",
    "minority",
    "costs",
    "rather",
    "solely",
    "majority",
    "class",
    "helps",
    "model",
    "perform",
    "better",
    "real",
    "world",
    "situations",
    "feature",
    "scaling",
    "crucial",
    "step",
    "phase",
    "majority",
    "deep",
    "learning",
    "algorithms",
    "perform",
    "much",
    "better",
    "dealing",
    "features",
    "scale",
    "common",
    "techniques",
    "normalization",
    "refers",
    "rescaling",
    "features",
    "range",
    "0",
    "1",
    "fact",
    "special",
    "case",
    "min",
    "max",
    "scaling",
    "normalize",
    "data",
    "need",
    "apply",
    "min",
    "max",
    "scaling",
    "feature",
    "column",
    "standardization",
    "consists",
    "centering",
    "field",
    "mean",
    "0",
    "standard",
    "deviation",
    "1",
    "feature",
    "columns",
    "parameters",
    "standard",
    "normal",
    "distribution",
    "0",
    "mean",
    "unit",
    "variance",
    "makes",
    "much",
    "easier",
    "learning",
    "algorithms",
    "learn",
    "weights",
    "parameters",
    "addition",
    "keeps",
    "useful",
    "information",
    "outliers",
    "makes",
    "algorithms",
    "less",
    "sensitive",
    "data",
    "prepared",
    "feed",
    "network",
    "trade",
    "discussed",
    "learning",
    "process",
    "neural",
    "network",
    "previous",
    "module",
    "unsure",
    "advise",
    "watch",
    "module",
    "first",
    "essentially",
    "data",
    "fed",
    "forward",
    "propagation",
    "occurs",
    "losses",
    "compared",
    "loss",
    "function",
    "parameters",
    "adjusted",
    "based",
    "loss",
    "incurred",
    "nothing",
    "different",
    "discussed",
    "previously",
    "model",
    "successfully",
    "trained",
    "congratulations",
    "need",
    "test",
    "good",
    "model",
    "using",
    "validation",
    "set",
    "set",
    "aside",
    "earlier",
    "evaluation",
    "process",
    "allows",
    "us",
    "test",
    "model",
    "data",
    "never",
    "seen",
    "meant",
    "representative",
    "good",
    "model",
    "might",
    "perform",
    "real",
    "world",
    "evaluation",
    "process",
    "high",
    "chance",
    "model",
    "could",
    "optimized",
    "remember",
    "started",
    "random",
    "weights",
    "biases",
    "back",
    "propagation",
    "well",
    "quite",
    "cases",
    "bad",
    "propagation",
    "wo",
    "get",
    "right",
    "first",
    "time",
    "okay",
    "ways",
    "optimize",
    "model",
    "tuning",
    "hyper",
    "parameters",
    "good",
    "way",
    "optimizing",
    "model",
    "performance",
    "one",
    "way",
    "showing",
    "model",
    "entire",
    "data",
    "set",
    "multiple",
    "times",
    "increasing",
    "number",
    "epochs",
    "sometimes",
    "shown",
    "improve",
    "accuracy",
    "ways",
    "adjusting",
    "learning",
    "rate",
    "talked",
    "learning",
    "rate",
    "previous",
    "module",
    "know",
    "learning",
    "rate",
    "advise",
    "check",
    "previous",
    "module",
    "essentially",
    "learning",
    "rate",
    "defines",
    "far",
    "shift",
    "line",
    "step",
    "based",
    "information",
    "previous",
    "training",
    "step",
    "back",
    "propagation",
    "values",
    "play",
    "role",
    "accurate",
    "model",
    "become",
    "long",
    "training",
    "takes",
    "complex",
    "models",
    "initial",
    "conditions",
    "play",
    "significant",
    "role",
    "determining",
    "outcome",
    "training",
    "many",
    "considerations",
    "phase",
    "training",
    "important",
    "define",
    "makes",
    "model",
    "good",
    "enough",
    "otherwise",
    "might",
    "find",
    "tweaking",
    "parameters",
    "long",
    "long",
    "time",
    "adjustment",
    "hyper",
    "parameters",
    "remains",
    "bit",
    "art",
    "experimental",
    "process",
    "heavily",
    "depends",
    "specifics",
    "data",
    "set",
    "model",
    "training",
    "process",
    "develop",
    "go",
    "deep",
    "learning",
    "worry",
    "much",
    "one",
    "common",
    "problems",
    "encounter",
    "model",
    "performs",
    "well",
    "training",
    "data",
    "performs",
    "terribly",
    "data",
    "never",
    "seen",
    "problem",
    "overfitting",
    "happens",
    "model",
    "learns",
    "pattern",
    "specific",
    "training",
    "data",
    "set",
    "relevant",
    "unseen",
    "data",
    "two",
    "ways",
    "avoid",
    "overfitting",
    "getting",
    "data",
    "regularization",
    "getting",
    "data",
    "usually",
    "best",
    "solution",
    "model",
    "trainer",
    "mode",
    "data",
    "naturally",
    "generalize",
    "better",
    "reducing",
    "model",
    "size",
    "reducing",
    "number",
    "learnable",
    "parameters",
    "model",
    "learning",
    "capacity",
    "another",
    "way",
    "however",
    "lowering",
    "capacity",
    "network",
    "force",
    "learn",
    "patterns",
    "matter",
    "minimize",
    "loss",
    "hand",
    "reducing",
    "network",
    "capacity",
    "much",
    "lead",
    "underfitting",
    "model",
    "able",
    "learn",
    "relevant",
    "patterns",
    "trained",
    "data",
    "unfortunately",
    "magical",
    "formulas",
    "determine",
    "balance",
    "must",
    "tested",
    "evaluated",
    "setting",
    "different",
    "number",
    "parameters",
    "observing",
    "performance",
    "second",
    "method",
    "addressing",
    "overfitting",
    "applying",
    "weight",
    "regularization",
    "model",
    "common",
    "way",
    "achieve",
    "constraint",
    "complexity",
    "network",
    "forcing",
    "weights",
    "take",
    "small",
    "values",
    "regularizing",
    "distribution",
    "weight",
    "values",
    "done",
    "adding",
    "loss",
    "function",
    "network",
    "cost",
    "associated",
    "larger",
    "weights",
    "cost",
    "comes",
    "two",
    "ways",
    "l1",
    "regularization",
    "cost",
    "regards",
    "absolute",
    "value",
    "weight",
    "coefficient",
    "l1",
    "norm",
    "weights",
    "l2",
    "regularization",
    "adds",
    "cost",
    "regards",
    "squared",
    "value",
    "weight",
    "coefficient",
    "l2",
    "norm",
    "weight",
    "another",
    "way",
    "reducing",
    "overfitting",
    "augmenting",
    "data",
    "model",
    "perform",
    "well",
    "satisfactory",
    "need",
    "lot",
    "data",
    "already",
    "typically",
    "working",
    "images",
    "always",
    "chance",
    "model",
    "wo",
    "perform",
    "well",
    "like",
    "matter",
    "much",
    "data",
    "cases",
    "like",
    "limited",
    "data",
    "sets",
    "data",
    "augmentation",
    "good",
    "way",
    "increasing",
    "data",
    "set",
    "without",
    "really",
    "increasing",
    "artificially",
    "augment",
    "data",
    "case",
    "images",
    "get",
    "data",
    "already",
    "existing",
    "data",
    "kind",
    "augmentations",
    "talking",
    "well",
    "anything",
    "flipping",
    "image",
    "flipping",
    "applying",
    "blur",
    "even",
    "zooming",
    "image",
    "shows",
    "model",
    "meets",
    "eye",
    "exposes",
    "model",
    "existing",
    "data",
    "testing",
    "automatically",
    "perform",
    "better",
    "seen",
    "images",
    "represented",
    "almost",
    "every",
    "single",
    "form",
    "finally",
    "last",
    "method",
    "going",
    "talk",
    "dropout",
    "dropout",
    "technique",
    "used",
    "deep",
    "learning",
    "randomly",
    "drops",
    "units",
    "neurons",
    "network",
    "simply",
    "put",
    "dropout",
    "refers",
    "ignoring",
    "neurons",
    "training",
    "phase",
    "randomly",
    "chosen",
    "set",
    "neurons",
    "ignoring",
    "mean",
    "units",
    "considered",
    "particular",
    "forward",
    "backward",
    "pass",
    "need",
    "dropout",
    "need",
    "shut",
    "parts",
    "neural",
    "network",
    "fully",
    "connected",
    "layer",
    "occupies",
    "parameters",
    "hence",
    "neurons",
    "develop",
    "amongst",
    "training",
    "curbs",
    "individual",
    "power",
    "neuron",
    "ultimately",
    "leads",
    "overfitting",
    "training",
    "data",
    "drop",
    "good",
    "way",
    "reducing",
    "overfitting",
    "hope",
    "introductory",
    "course",
    "helped",
    "develop",
    "good",
    "intuition",
    "deep",
    "learning",
    "whole",
    "course",
    "scraped",
    "surface",
    "whole",
    "new",
    "world",
    "like",
    "course",
    "please",
    "consider",
    "liking",
    "subscribing",
    "really",
    "helps",
    "make",
    "courses",
    "like",
    "couple",
    "videos",
    "computer",
    "vision",
    "opencv",
    "releasing",
    "couple",
    "weeks",
    "stay",
    "tuned",
    "meantime",
    "good",
    "luck"
  ],
  "keywords": [
    "probably",
    "deep",
    "learning",
    "many",
    "world",
    "also",
    "come",
    "would",
    "could",
    "game",
    "possible",
    "entire",
    "lot",
    "never",
    "web",
    "matter",
    "course",
    "learn",
    "need",
    "get",
    "build",
    "algorithms",
    "capable",
    "complex",
    "problems",
    "talk",
    "difference",
    "artificial",
    "machine",
    "introduce",
    "new",
    "networks",
    "going",
    "models",
    "train",
    "various",
    "types",
    "supervised",
    "unsupervised",
    "reinforcement",
    "loss",
    "functions",
    "optimizers",
    "grading",
    "descent",
    "algorithm",
    "different",
    "neural",
    "network",
    "steps",
    "involves",
    "traditional",
    "data",
    "recognize",
    "patterns",
    "way",
    "us",
    "dog",
    "much",
    "difficult",
    "later",
    "want",
    "give",
    "sense",
    "past",
    "lost",
    "one",
    "first",
    "computer",
    "show",
    "google",
    "time",
    "go",
    "times",
    "finds",
    "applications",
    "even",
    "yes",
    "simply",
    "technique",
    "learns",
    "features",
    "tasks",
    "inputs",
    "architecture",
    "number",
    "hidden",
    "layers",
    "allowing",
    "making",
    "input",
    "best",
    "results",
    "next",
    "video",
    "problem",
    "always",
    "like",
    "human",
    "designed",
    "example",
    "image",
    "face",
    "know",
    "well",
    "define",
    "look",
    "zero",
    "work",
    "classifier",
    "lines",
    "certain",
    "bit",
    "key",
    "idea",
    "feed",
    "images",
    "develop",
    "kind",
    "hierarchical",
    "using",
    "together",
    "ultimately",
    "turns",
    "training",
    "quite",
    "long",
    "become",
    "big",
    "require",
    "second",
    "building",
    "call",
    "extremely",
    "increasing",
    "things",
    "called",
    "form",
    "neurons",
    "make",
    "neuron",
    "take",
    "predict",
    "outputs",
    "set",
    "similar",
    "information",
    "three",
    "every",
    "layer",
    "output",
    "several",
    "two",
    "process",
    "main",
    "processes",
    "forward",
    "propagation",
    "back",
    "connect",
    "values",
    "weights",
    "sum",
    "value",
    "bias",
    "passed",
    "function",
    "activation",
    "essentially",
    "particular",
    "basically",
    "finally",
    "let",
    "terms",
    "weight",
    "important",
    "relationship",
    "right",
    "high",
    "adding",
    "graph",
    "either",
    "almost",
    "direction",
    "gets",
    "final",
    "bad",
    "last",
    "step",
    "prediction",
    "wrong",
    "performance",
    "uses",
    "something",
    "expected",
    "biases",
    "adjusted",
    "increases",
    "real",
    "suppose",
    "dataset",
    "gives",
    "vehicle",
    "goods",
    "based",
    "start",
    "random",
    "really",
    "equal",
    "case",
    "good",
    "moving",
    "add",
    "happens",
    "okay",
    "say",
    "type",
    "greater",
    "use",
    "result",
    "predicted",
    "adjust",
    "remember",
    "better",
    "fit",
    "model",
    "pass",
    "repeat",
    "error",
    "end",
    "overfitting",
    "discuss",
    "works",
    "able",
    "predictions",
    "follows",
    "parameters",
    "perform",
    "order",
    "gradient",
    "total",
    "previous",
    "consider",
    "section",
    "common",
    "used",
    "today",
    "whether",
    "decide",
    "activate",
    "couple",
    "threshold",
    "less",
    "activated",
    "1",
    "great",
    "makes",
    "perfect",
    "understand",
    "think",
    "classify",
    "multiple",
    "classes",
    "class",
    "happen",
    "however",
    "binary",
    "instead",
    "chance",
    "find",
    "rather",
    "linear",
    "straight",
    "line",
    "slope",
    "range",
    "activations",
    "point",
    "f",
    "x",
    "means",
    "adjustments",
    "connected",
    "nature",
    "nothing",
    "single",
    "still",
    "another",
    "stack",
    "whole",
    "sigmoid",
    "examples",
    "negative",
    "looks",
    "combinations",
    "unlike",
    "wo",
    "small",
    "changes",
    "region",
    "change",
    "vanishing",
    "large",
    "becomes",
    "see",
    "fact",
    "known",
    "discussed",
    "popular",
    "max",
    "0",
    "positive",
    "relu",
    "imagine",
    "words",
    "comes",
    "randomly",
    "state",
    "thus",
    "part",
    "usually",
    "around",
    "operations",
    "mean",
    "trying",
    "choose",
    "faster",
    "classification",
    "lead",
    "larger",
    "must",
    "degree",
    "plane",
    "simple",
    "regression",
    "cross",
    "depends",
    "project",
    "ways",
    "minimize",
    "correct",
    "accurate",
    "optimizer",
    "goal",
    "taking",
    "trial",
    "getting",
    "often",
    "minimum",
    "individual",
    "vector",
    "variables",
    "points",
    "dealing",
    "sets",
    "rate",
    "given",
    "variable",
    "gradients",
    "ensures",
    "might",
    "sometimes",
    "batches",
    "momentum",
    "include",
    "missing",
    "solve",
    "current",
    "complexity",
    "may",
    "word",
    "basic",
    "parameter",
    "hyper",
    "thumb",
    "search",
    "specific",
    "grid",
    "tuning",
    "rule",
    "k",
    "epochs",
    "size",
    "iterations",
    "ca",
    "epoch",
    "majority",
    "memory",
    "performing",
    "performs",
    "seen",
    "answer",
    "shown",
    "typically",
    "determine",
    "label",
    "split",
    "spam",
    "within",
    "create",
    "represents",
    "situation",
    "feature",
    "represented",
    "test",
    "recognition",
    "analysis",
    "useful",
    "clustering",
    "association",
    "clusters",
    "helps",
    "refers",
    "cluster",
    "person",
    "card",
    "user",
    "parts",
    "cases",
    "agent",
    "environment",
    "feedback",
    "task",
    "reward",
    "although",
    "method",
    "states",
    "testing",
    "done",
    "regularization",
    "dropout",
    "field",
    "variety",
    "scaling",
    "capacity",
    "validation",
    "due",
    "fully",
    "convolutional",
    "ball",
    "position",
    "sequential",
    "sequence",
    "sequences",
    "across",
    "tuesday",
    "raining",
    "keep",
    "recurrent",
    "rnns",
    "loop",
    "rnn",
    "node",
    "letter",
    "gates",
    "gate",
    "processing",
    "cnn",
    "pooling",
    "convolution",
    "filter",
    "sampling",
    "min",
    "trained",
    "samples",
    "minority",
    "reducing",
    "cost",
    "module"
  ]
}