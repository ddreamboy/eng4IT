{
  "text": "if we're looking at a sequence of\nletters what's likely to come next this\nis a prediction problem we're given a\nletter the goal is to predict the next\nletter in the sequence but this\nprediction is impossible without the\ncontext of the sequence of letters for\nexample here o is followed by three\ndifferent letters but in context the\nnext letter becomes easier and easier to\npredict as a sequence progresses long\nshort term memory networks or LS TMS are\ndesigned for applications where the\ninput is an ordered sequence where\ninformation from earlier in the sequence\nmay be important LS TMS are a type of\nrecurrent network which are networks\nthat reuse the output from a previous\nstep as an input for the next step like\nall neural networks the node performs a\ncalculation using the inputs and returns\nan output value in a recurrent Network\nthis output is then used along with the\nnext element as the inputs for the next\nstep\nand so on in an LS TM the nodes are\nrecurrent but they also have an internal\nstate the node uses an internal state as\na working memory space which means\ninformation can be stored and retrieved\nover many time steps the input value\nprevious output and the internal state\nare all use in the nodes calculations\nthe results of the calculations are used\nnot only to provide an output value but\nalso to update the state like any neural\nnetwork LS TM nodes have parameters that\ndetermine how the inputs are used in the\ncalculations but LS TMS also have\nparameters known as gates that control\nthe flow of information within the node\nin particular how much the safe state\ninformation is used as an input to the\ncalculations these gate parameters are\nweights and biases which means the\nbehavior depends on the inputs so for\nexample an input of Q doesn't need much\npassed information the next letter is\nalmost certainly a u but an input of e\nwell we might need to recall much more\npassed information\nsimilarly there are gates to control how\nmuch of the current information is saved\nto the state and how much the output is\ndetermined by the current calculation\nversus the saved information so LS TM\nnodes are certainly more complicated\nthan regular recurrent nodes but this\nmakes them better at learning the\ncomplex interdependencies in sequences\nof data and ultimately they're still\njust a node with a bunch of parameters\nand these parameters are learned during\ntraining just like with any other neural\nnetwork\n",
  "words": [
    "looking",
    "sequence",
    "letters",
    "likely",
    "come",
    "next",
    "prediction",
    "problem",
    "given",
    "letter",
    "goal",
    "predict",
    "next",
    "letter",
    "sequence",
    "prediction",
    "impossible",
    "without",
    "context",
    "sequence",
    "letters",
    "example",
    "followed",
    "three",
    "different",
    "letters",
    "context",
    "next",
    "letter",
    "becomes",
    "easier",
    "easier",
    "predict",
    "sequence",
    "progresses",
    "long",
    "short",
    "term",
    "memory",
    "networks",
    "ls",
    "tms",
    "designed",
    "applications",
    "input",
    "ordered",
    "sequence",
    "information",
    "earlier",
    "sequence",
    "may",
    "important",
    "ls",
    "tms",
    "type",
    "recurrent",
    "network",
    "networks",
    "reuse",
    "output",
    "previous",
    "step",
    "input",
    "next",
    "step",
    "like",
    "neural",
    "networks",
    "node",
    "performs",
    "calculation",
    "using",
    "inputs",
    "returns",
    "output",
    "value",
    "recurrent",
    "network",
    "output",
    "used",
    "along",
    "next",
    "element",
    "inputs",
    "next",
    "step",
    "ls",
    "tm",
    "nodes",
    "recurrent",
    "also",
    "internal",
    "state",
    "node",
    "uses",
    "internal",
    "state",
    "working",
    "memory",
    "space",
    "means",
    "information",
    "stored",
    "retrieved",
    "many",
    "time",
    "steps",
    "input",
    "value",
    "previous",
    "output",
    "internal",
    "state",
    "use",
    "nodes",
    "calculations",
    "results",
    "calculations",
    "used",
    "provide",
    "output",
    "value",
    "also",
    "update",
    "state",
    "like",
    "neural",
    "network",
    "ls",
    "tm",
    "nodes",
    "parameters",
    "determine",
    "inputs",
    "used",
    "calculations",
    "ls",
    "tms",
    "also",
    "parameters",
    "known",
    "gates",
    "control",
    "flow",
    "information",
    "within",
    "node",
    "particular",
    "much",
    "safe",
    "state",
    "information",
    "used",
    "input",
    "calculations",
    "gate",
    "parameters",
    "weights",
    "biases",
    "means",
    "behavior",
    "depends",
    "inputs",
    "example",
    "input",
    "q",
    "need",
    "much",
    "passed",
    "information",
    "next",
    "letter",
    "almost",
    "certainly",
    "u",
    "input",
    "e",
    "well",
    "might",
    "need",
    "recall",
    "much",
    "passed",
    "information",
    "similarly",
    "gates",
    "control",
    "much",
    "current",
    "information",
    "saved",
    "state",
    "much",
    "output",
    "determined",
    "current",
    "calculation",
    "versus",
    "saved",
    "information",
    "ls",
    "tm",
    "nodes",
    "certainly",
    "complicated",
    "regular",
    "recurrent",
    "nodes",
    "makes",
    "better",
    "learning",
    "complex",
    "interdependencies",
    "sequences",
    "data",
    "ultimately",
    "still",
    "node",
    "bunch",
    "parameters",
    "parameters",
    "learned",
    "training",
    "like",
    "neural",
    "network"
  ],
  "keywords": [
    "sequence",
    "letters",
    "next",
    "prediction",
    "letter",
    "predict",
    "context",
    "example",
    "easier",
    "memory",
    "networks",
    "ls",
    "tms",
    "input",
    "information",
    "recurrent",
    "network",
    "output",
    "previous",
    "step",
    "like",
    "neural",
    "node",
    "calculation",
    "inputs",
    "value",
    "used",
    "tm",
    "nodes",
    "also",
    "internal",
    "state",
    "means",
    "calculations",
    "parameters",
    "gates",
    "control",
    "much",
    "need",
    "passed",
    "certainly",
    "current",
    "saved"
  ]
}