{
  "text": "IULIA TURC: Hi, everyone.\nMy name is Iulia, and I\nwork at Google Research\non natural language processing.\nIn this presentation, we'll\nwalk through the recent history\nof natural language processing,\nincluding the current\nstate-of-the-art architecture,\nthe transformers.\nAnd we'll also discuss\nhow transformers\nbecame particularly\nsuccessful when\nused in the context of\ntransfer learning, which\nis a technique that leverages\nmassive amounts of text.\nThis tutorial is structured\nin three sections.\nFirst, we're going to\ntalk about encoding text\ninto a numerical representation\nthat can be manipulated\nby machine learning models.\nSecond, we'll discuss one\nof the most important tasks\nin natural language processing--\nnamely, language modeling.\nAmong other architectures,\nwe'll see how transformers\nsolve this particular task.\nAnd third, we'll dive\ninto transfer learning,\nwhich is the mechanism that\nenabled transformers to be\nthe state-of-the-art in\nnatural language processing.\nAnd finally, we'll look at\nBERT, one of the most popular\ntransformer models.\nSo let's start by answering\nthe following question.\nHow do we encode text\nin a numerical format?\nWell, currently, natural\nlanguage processing\nis heavily machine\nlearning-driven.\nMost NLP systems follow\nthis high-level contract,\nwhere a piece of\nnatural language text\nis input into a machine\nlearning model, which\nproduces a prediction that\nqualifies or describes\nthe input in some way.\nThis could be a discrete class,\na real value, another piece\nof text, and so on.\nLet's look at sentiment\nclassification,\nwhich is one of the\nmost common NLP tasks.\nThe input could be\nsomething like a TV show\nreview from Rotten Tomatoes,\nlike this positive review\nfor \"BoJack Horseman.\"\nThe expected output is a\nprobability distribution\nover two classes,\npositive and negative,\nindicating how the author of the\nreview perceived the TV show.\nIn this section, we'll\ndiscuss the first part\nof this contract-- namely,\nhow to encode a TV show\nreview into a numerical\nrepresentation that\ncan be handled by the\nmachine learning model.\nWe can start with a naive\nview and just assume\nthat words are discrete\nand independent tokens.\nOne way forward\nwould be to gather\nall of the unique tokens\nin a training corpus\nand build a dictionary of tokens\nby sorting them alphabetically.\nHere, our first\nword is aardvark,\nand king and queen are\nsomewhere in the dictionary.\nRemember, we want to\nconvert text into numbers.\nSo the first thing\nwe could do is just\nto assign an index\nto each token,\nreflecting its order\nin alphabetical order.\nThe problem is these\nvery high numbers\nare not very amenable to machine\nlearning, and gradient descent,\nin particular.\nSo another thing we could do\nis use one-hot embeddings.\nThese are vectors that have\nthe same dimensionality\nas the dictionary.\nSo if the dictionary\nhas size 100,000,\nthen a one-hot vector would\nhave the same size, 100,000,\nwhere only one of the\nentries would be 1\nand all of the others will be 0.\nSo for instance,\naardvark has a 1\non the very first position\nand a 0 for all of the others.\nWhile one-hot embeddings\ndo use values that\nare amenable to\nmachine learning,\nthey have two main\ndisadvantages.\nFirst, when stored\nas the inspectors,\nhave very high\ndimensionality, since they\nhave one dimension for every\nentry in the vocabulary.\nAnd second, they fail to capture\nany world knowledge whatsoever\nabout the tokens.\nIn this particular\ncase, king and queen\nhave more in common\nwith each other\nthan they have with aardvark.\nYet, all of these vectors have\n90-degree angles between them.\nTo see this more clearly,\nlet's limit the vocabulary\nto only three tokens.\nIn this case, the size of\nthe vocabulary is three.\nThe dimensionality of the\none-hot vectors is three.\nSo the words are vectors in\na three-dimensional space.\nWith this one-hot\nembedding, words\nare unit vectors aligned\nagainst the axes.\nAnd there's an\nopportunity cost here.\nWhy not allow the\nwords to occupy\nthe entire space, as opposed\nto being perfectly aligned\nto the axes?\nA more practical view\nwould be that words\nare continuous vectors in\nan n-dimensional space.\nSo we'll allow queen,\nking, and aardvark\nto flow anywhere in this\nthree-dimensional space.\nSo their representation\ncontains real values\nlike 0.3, 1.9, and minus 0.4.\nBefore we move on, just a\nquick note on terminology.\nThis way of embedding tokens\ncomes under different names.\nIt's known as continuous,\nor distributed, embeddings,\nvectors, representations,\nand so on.\nSo I will use these\nterms interchangeably\nthroughout the presentation.\nNow what sort of\nproperties would\nbe like these word tokens to\nreflect as we aim for them\nto encompass world knowledge?\nWe can enumerate certain\naspects that we'd\nlike them to capture, such\nas gender or part of speech.\nConceptually, various\ndimensions of the vector\ncould be dedicated to\neach of these aspects.\nSo for instance, the first three\ndimensions could encode gender\nand the next three could\nencode the part of speech.\nOf course, we will not enforce\nthis particular attribution\nof meaning to the\ndimensions, but we\ncan hope that the\nmodel discovers\nsomething of the sort.\nThe mechanism that we use to\nexpress desired relationships\nis relative vector distance.\nFor instance, on the\ngender dimension,\nking and queen should be as\nfar apart as men and women.\nHowever, on the part-of-speech\ndimension, all of these words\nshould be clustered\ntogether at a distance\nvirtually 0, since\nthey are all nouns.\nIn contrast, play, which is\na verb, and playful, which\nis an adjective, should\nbe at the same distance\nas joy and joyful.\nSo we've now enumerated\nthe desirable properties\nof our embeddings and\nwhat sort of relationships\nwe want to encode.\nBut how exactly can we\nlearn useful embeddings?\nWell, luckily, the\ninternet is free\nand contains a massive\namount of information,\nand a good chunk of it is text.\nEven better, there's Wikipedia,\nwhich currently contains\n28 billion words in\n309 languages, which\nis a reasonably controlled and\nreliable source of information.\nLooking at the Wikipedia\npages for king and queen,\nwe can see a lot of commonality.\nThey both reference\neach other and they also\ninclude common\nwords like monarch.\nSurely, there's quite\na bit of word knowledge\nthat we can extract from here.\nNow you might know that\nmachine learning operates\non pairs of inputs and outputs.\nA machine learning model\nis given some input, x,\nand it has to produce\na prediction that\ndescribes the input\nin one way or another.\nBut Wikipedia comes\nin freeform text.\nSo how can we gamify\nthis text in order\nto have the model learn\nsomething from it?\nA common practice is to\nturn unsupervised text data\ninto a supervised task.\nFor instance, we could ask\nthe model to fill in the gaps\nand predict the next\nword after a sentence\nlike, king is the\ntitle given to a male.\nThe model could then produce\na probability distribution\nover the words in\nthe vocabulary,\nindicating which ones are more\nlikely to follow the words so\nfar.\nIn this case, monarch\nwould be a good candidate\nto fill in the gap.\nWe'll now move on to\ndiscuss language modeling,\nor this gamification of\nunstructured text, that\nallows us to learn\nfrom text, even\nin the absence of\nexplicit labels.\nTraditionally, language\nmodel is defined\nas the task of predicting\nthe next word in a sentence.\nGiven words 0 through n\nminus 1, the challenge\nis to predict the nth word.\nThere are some variations on\nthis traditional formulation.\nFor instance, the\ncontext could be\nlimited to the previous\nthree words only.\nOr instead of\nmaking a prediction\nbased on the previous\nwords, we could\nmake a prediction based on\nall of the surrounding words,\nnot just the ones to the left.\nSo generally, language\nmodeling refers\nto predicting a piece of text\ngiven some textual context.\nWe'll discuss three\nmain turning points\nin the history of\nlanguage modeling--\nWord2vec, introduced in 2013;\nrecurring neural networks,\nintroduced in 2014;\nand the transformers,\npublished in 2017.\nTransformers are currently the\nstate-of-the-art architecture\nfor language modeling.\nLet's start with\nWord2vec, the model\nthat introduced distributed\nrepresentations for language\nmodeling.\nWe'll now build a very\nsimple language model\nthat is inspired by Word2vec.\nLet's start with a simple\ninput, like on the river,\nhoping that the model will\nguess the missing word,\nwhich is bank.\nJust for simplicity,\nwe'll limit our dictionary\nto only five tokens--\nbank, fish, on,\nriver, and \"the.\"\nFor now, we're going to start\nwith randomly initialized\nembeddings.\nThat's the best we can do\nwith no world knowledge\nat this point.\nThe first step is to\nlook up every word\nin the token-embedding matrix.\nSo now we've pulled out these\ntwo-dimensional vectors.\nThe next step is to embed or\naggregate all of these tokens\ninto a wholesome view\nover the entire sentence.\nIn order to predict what\nfollows after three words,\nwe need to understand\nthem together.\nThe most simple\nthing that we can do\nis to average all of\nthese embedding together.\nRemember, this is a dummy\nmodel, so for now, we're\ngoing to ignore the fact that\nit's not even capable of taking\nword order into account.\nOK, so we've averaged\nthese embeddings\ninto a single sentence\nrepresentation.\nOur end goal is to produce\na score for every word\nin the vocabulary\nindicating how likely it\nis for it to follow the sentence\nor the piece of sentence,\non the river.\nHow can we turn a\ntwo-dimensional vector\ninto a five-dimensional vector?\nWell, the simplest option is a\nlinear transformation using a 2\nby 5 matrix.\nThis is the role of\nthe softmax parameters,\nwhich will be trained together\nwith the rest of the model.\nNow remember that the\nweights of the model\nare arbitrary at this point and\nso are the scores it assigns\nto the five vocabulary tokens.\nThe next step would be to\nnormalize them in such a way\nthat all of them sum to 1,\nso they form a probability\ndistribution.\nA common way to do that is\nto use logistic regression.\nIn this expression, the\nred dots would correspond\nto the x in the formula.\nSo now we have a\nprobability distribution,\nproduced by the\nmodel guessing what\nshould come after on the river.\nIt seems like the model\nis giving high probability\nto bank, and for some\nreason, also to \"on.\"\nHow do we teach the\nmodel or tell it\nwhether it made a\ngood prediction?\nWell, we can peek\nat the word that\nshould have been\npredicted, which is bank,\nand turn it into a\nprobability distribution.\nSo the true distribution, the\none represented on the right,\nwill assign probably 1\nto bank and probability 0\nto all of the other words.\nHaving two\ndistributions, we can now\ncompute some sort of\ndistance between these two\ndistributions, and that can\nbe done by cross entropy.\nOnce we have the distance\nbetween the two distributions,\nwe can now\nbackpropagate this error\nand tell the model\nhow it can adjust\nthe weights in a\nway that would have\nled to the correct prediction.\nThis, in a nutshell, is how\na language model is trained,\nand these principles will apply\nfor all of the other language\nmodels that we're\ngoing to discuss.\nSo while Word2vec is\nan admirable effort\nand it really revolutionized\nthe field when it came out,\nthere are some disadvantages\nto this paradigm.\nOne of them is that when\nassociating a fixed embedding\nto a word, we cannot handle\ncases in which words have\nmultiple meanings.\nSo for instance, the word\nbank in \"open a bank account\"\nand \"on the river bank\" would\nmap to the same embedding.\nIdeally, we'd want\nthese embeddings\nto be more contextualized\nto reflect the surrounding\nwords around them, so\n\"open a bank account\"\nwould have a different embedding\nfrom \"on the river bank.\"\nWe'll now move on to Recurring\nNeural Networks, or RNNs,\nwhich provide a way\nof contextualizing\nthe embeddings so that\nwords can be disambiguated\nbased on their context.\nLet's trace the behavior of\nRNNs for an input sentence,\nlike on the river bank they\nsat, where the task of the model\nis to predict the\nmissing word sat.\nWhen building an\nRNN, we start off\nwith noncontextual\nembeddings, like Word2vec,\nthat are looked up in a\nregular embedding table.\nIn a sequential manner, we feed\nevery noncontextual embedding\ninto an RNN cell.\nThe RNN cell has an internal\nstate that it continuously\nupdates as it progresses\nthrough the sentence, denoted\nby H on the slide.\nYou can think of it as an\nencoding of the sentence\nso far.\nThe cell produces a\ncontextualized embedding, Y,\nbased on its internal\nrepresentation,\nH, which captures the\nleft-hand side context\nand the noncontextual\nembedding of the current token,\nX. For the word\nbank, this mechanism\nis particularly helpful.\nBy the time the\nRNN reaches bank,\nit will have already\nseen the word river\nand encoded it in\nits internal state,\nso it's in a better\nposition to disambiguate it\nfrom its other meaning of\na financial institution.\nOnce the final contextual\nembedding is computed,\nwe can use the final\ninternal state of the RNN\nto make a prediction\nfor the next word.\nJust as before, we can\nadd a linear classifier\nthat maps the\nhidden state vector\nH5 into a vector with\nthe same dimensionality\nas the vocabulary.\nAfter we get a score\nper vocabulary token,\nwe can train the model using\ncross entropy as before.\nWhile RNN encoders are great\nbecause they contextualize\nthe word embeddings, there\nare certain disadvantages.\nThe first one is that they\nare quite slow because\nof their sequential manner.\nIf the embedding\nof a word N depends\non the embeddings of all\nof the previous words,\nit means it has to wait\nfor them to be computed.\nSo that's why the speed is O\nof N in the number of tokens.\nAnother problem is the\nproblem of vanishing gradient.\nThis is another way of\nsaying that, by the time\nwe arrive at the\nend of the sentence,\nwe might have forgotten what\nthe start of the sentence\nlooked like.\nBecause of this\nsequential processing,\nthere's no way of revisiting\nvery early tokens.\nAnd the effect of\nthis structure might\nbe that we learn very\nlittle from early tokens.\nA third disadvantage\nis that RNNs--\nmost of them-- are\nunidirectional,\nwhich means they\nprocess text from left\nto right, unlike humans.\nThis is sometimes a problem\nwhen the relevant context\nis to the right.\nSo we were lucky with a\nsentence like on the river bank,\nbecause river happened\nto occur before bank.\nBut if we were to parse\non the bank of the river,\nby the time we process bank,\nthere's no information--\nthere's no river, so we\ncannot disambiguate it from\nthe financial institution.\nThe third language\nmodeling technique,\nwhich is currently\nthe state-of-the-art,\nare the transformers.\nAnd they address this\nproblem of unidirectionality\nby teaching the model to\ntake into account context\nboth from the left-hand side\nand from the right-hand side\nof the sentence.\nNow just for\nsimplicity, in order\nto be consistent with\nthe other models,\nI'm going to go through the\ntransformer left to right.\nSo let's see how the\ntransformer works.\nAs before, we're going\nto start with a sentence,\nlike on the river bank\nthey, hoping that the model\nwill fill in the blank.\nNote that this time,\nfor the missing token,\nI'm passing a special\ntoken called the MASK.\nAnd this is part\nof the vocabulary,\nso it's just like\na regular token.\nThe first step is to look up\nthese words in an embedding\ntable and get some\nnoncontextual embeddings, Xi.\nThese could be, for\ninstance, Word2vec.\nAnd then we're going to feed\nevery token into a box that\nis called self-attention.\nSelf-attention is one\nof the main building\nblocks for transformers.\nSelf-attention works\nby taking into account\nor paying attention to\nall of the other tokens.\nSo when embedding\nposition 0, we do\ntake into account all of the\nother positions 1 through 5.\nAnd the output-- for\nnow, we can see it\nas a weighted\naverage of the tokens\nthat we paid attention to.\nSelf-attention comes\nwith some terminology\nthat will become easier\nto understand a bit later.\nThe token that's\ncurrently being embedded\nis called the query token.\nAnd the tokens that\nwe pay attention to\nare called the key tokens.\nThe key tokens don't need\nto span the entire sentence.\nThey could just span the next\nthree words, for instance.\nBut in general,\nmost transformers\ndo use the entire\nsentence for context.\nWhen we embed the word\n\"the,\" the word \"the\"\nbecomes the query token\nand all of the others\nare the key tokens.\nSame for river and bank.\nLet's take a step\nback and explain where\nthe terminology comes from.\nTo get some intuition about\nthis naming convention,\nwe can think of a attention\nas a soft dictionary lookup.\nSay we have a dictionary, dict,\nmapping tokens to some value.\nWhen looking up the word\nbank in this dictionary,\nwell, bank becomes a query.\nWe'd like to retrieve not\njust the value of bank--\nthat would be v3.\nBut we'd like to get a\nweighing of all of the values,\nreflecting how similar\ntheir keys are to bank.\nOf course, the most\nsimilar word to bank\nis probably the\nword bank itself.\nThat's why its\nweight will be 0.6.\nThe next most relevant\nword is river,\nbecause it disambiguates\nthe meaning of bank.\nSo it gets a pretty high\nscore, which is 0.3.\nAnd all of the other words--\non, the, and they--\nget some non-zero\nscore, but pretty low.\nAnd finally, the mask\nitself has weight 0,\nbecause it carries\nno information.\nSo this is just an\naside to justify\nwhy the current token is\ncalled the query token\nand all of the others are\ncalled the key tokens.\nNow we're ready to open\nup the self-attention box\nand get some more clarity on\nwhat exactly happens inside.\nSelf-attention has\nthree model parameters.\nThese are two-dimensional\nmatrices--\nthe key parameters,\nthe query parameters,\nand the value parameters.\nThe terminology should\nalready be familiar.\nUnsurprisingly, what\nthe key parameters do\nis apply a transformation\nto the key tokens.\nSo by multiplying a vector,\nlike x0, by a matrix, K,\nwe obtain another vector, Key0.\nRemember, these can be viewed\nas the keys into a dictionary.\nSo we're going to put\nthese aside for now.\nLet's move on to the\nquery parameters.\nLet's see what they do.\nWell, also unsurprisingly,\nthe query parameters\ntransform the query token.\nSo we multiply the\nnoncontextual embedding of bank\nby this matrix in order to\nobtain some other embedding\nfor the query.\nLet's put this one\naside, as well.\nI hinted a bit\nearlier at the fact\nthat self-attention works as\na weighted sum, conceptually.\nBasically, we want to assign\na weight to every single token\nin the sentence.\nAnd this weight should\nreflect the usefulness\nof a particular token in\nembedding the query token.\nWe can do that by measuring\nthe similarity between keys\nand queries.\nThere are multiple similarity\nmeasures between two vectors.\nBut most commonly,\nself-attention\nuses the dot product.\nSo by applying the dot product\nbetween a key and the query,\nwe get a scaler, which\nis the attention score,\nand reflects how relevant\na key is to the query.\nLet's put the attention\nscores to the side for now.\nThe only remaining parameter\nthat we haven't talked about\nis the values matrix.\nWe're going to multiply the\ninput tokens by this value\nmatrix and obtain\nanother set of vectors\nthat's called the values.\nReferencing back to\nthe previous slide\nwith the analogy\nof a dictionary,\nyou can think of these values\nas being the values stored\nin the dictionary.\nSo each key has a value.\nFinally, what the\nself-attention box returns\nis a sum of these\nweighted values.\nRemember, the attention\nscores show the similarity\nbetween the key and the\nquery and they get multiplied\nby the values themselves.\nSo the output is, indeed,\na weighted sum of values.\nOne implementation\ndetail is that there\nmight be multiple sets of keys,\nquery, and value parameters.\nThese are called\nattention heads.\nIn this illustration, there\nare three attention heads.\nAnd having multiple\nattention heads\nis equivalent to having multiple\nkernels in computer vision.\nWhen processing an image,\nit's common to apply\nmultiple effects, like blurring,\nsharpening, or outlining,\nin order to understand\nvarious aspects of the input.\nLanguage is a lot more\nabstract, so it's not\ntrivial to describe what each\ntransformation of each head\nis expected to do.\nBut you could see how focusing\non various linguistic aspects\none at a time might\nbe beneficial.\nFor instance, one head might pay\nattention to parts of speech,\nwhile another head could\nfocus on verb tenses.\nIn other words, everything that\nwe saw on the previous slide,\nwe just perform three times.\nSo we apply those\ntransformations three times.\nWe're going to end up\nwith three outputs.\nWhat do we do with\nthese three outputs?\nHow exactly do we combine\ninto just one output?\nWell, we can put a\nfeed forward network\non top of the three inputs\nto just combine them back\ninto one single output.\nAnd this is multiheaded\nself-attention.\nSo let's zoom out of the box.\nI should have mentioned earlier\nthat, even though there's\nmultiple self-attention boxes\non this slide, all of them\nrepresent or are\ncopies of the same box.\nSo in this illustration,\nwe would have\nonly one set of parameters.\nYou might have noticed\nalready that self-attention\nis somewhat expensive.\nSince every token attends\nto every other token,\nthat is O of N\nsquare complexity.\nThe good news, though, is\nthat because this is not\na recurrent neural network,\nprocessing input N does not\nneed to wait on processing\ninput N minus 1, N minus 2,\nand so on.\nAll the dependencies refer\nto the previous layer.\nThere's no arrows going in\nbetween the boxes, which\nmeans if we only depend\non the previous layer,\nthe only sequence that\nwe need to respect\nis to compute things\nlayer by layer.\nBut within one\nlayer, we can compute\nall of the token embeddings.\nSo with the right\nhardware, even though we\nsee O of N square\nconnections here,\nwe could, in theory, compute\nall of them in parallel.\nSo what's a transformer?\nWe've only talked about\nself-attention so far.\nWell, self-attention is\nthe main building block\nfor a transformer,\nand a transformer\nis nothing more than a stack\nof self-attention layers.\nEach layer gets its own\nseparate set of parameters.\nThe transformer is also the\nfirst architecture to advocate\nfor extremely deep networks.\nIt's not uncommon for models\nto have 24 layers or more.\nSo far, we've talked\nabout how transformers\nturn noncontextual embeddings\ninto contextual embeddings.\nThe remaining question is, how\nexactly do we build a language\nmodel out of transformers?\nWell, one possibility is to\npick one embedding output\nby the transformer-- for\ninstance, the first one.\nRemember that the\ntransformer outputs\none contextual embedding\nfor every single input\nin the sentence.\nBut by the time we've\nreached the top of the stack,\nall of these embeddings are\nso contextualized that they\nunderstand the entire sentence.\nSo we can just arbitrarily\npick the very first embedding\nand build on top of it.\nPreviously, for\nWord2vec and for RNN,\nwe just used a\nlinear transformation\nto map an embedding into\nscores for every word\nin the dictionary.\nThat's exactly what we're\ngoing to do here, and produce\none score for every word.\nNow the transformer encoder is a\ncomplicated piece of machinery,\nand we've talked about it from\na very high level perspective.\nThere are some\nimplementation details\nthat are crucial\nto making it work,\nbut they were not\nincluded in the slides.\nThese include residual\nand skip connections,\nlayer normalization\nand dropout, and so on.\nI invite you to read the\noriginal papers if you're\ninterested in that much detail.\nWhile transformers are the\nstate-of-the-art for language\nmodeling, they come with\ntheir disadvantages.\nOf course, there's this\ncomputationally intensive\nwork--\nnumber of layers multiplied\nby N squared, where N\nis the number of input tokens.\nThis could be\nalleviated by hardware\nthat can perform\ncertain operations\nin parallel, like TPUs.\nBut on normal hardware,\nthis is very expensive.\nAnother downside is that\ntransformers require\na fixed number of tokens.\nIn other words, the\nlength of a sentence\nneeds to be set when\ndesigning the model.\nIt's very common for all of the\ninputs to be set to 512 tokens.\nSo if the input sentence\nis longer than that,\nit has to be truncated.\nAnd if it's shorter than\nthat, it has to be padded.\nThis concludes our discussion\nabout language models.\nWe visited three\ndifferent approaches\nto language modeling.\nWord2vec brings the\ninnovation of continuous word\nrepresentations, which are\nsuperior to one-hot encodings,\nbecause they can\nencode world knowledge\nand also have lower\ndimensionality.\nRNNs made these embeddings\ncontextual-- that is,\naware of their\nleft-hand side context--\nso that a\nbrick-and-mortar bank is\nassigned a different\nembedding from a riverbank.\nAnd finally, transformers\nensure that embeddings\nare bidirectional and\nthey're aware of both\nthe left-hand side and\nright-hand side of the context.\nSo for instance, in\nthe phrase, the bank\nof the river, the\nmeaning of bank\nis determined by river,\neven though river\nis to its right-hand side.\nIn the final section,\nwe'll discuss\nthe concept of transfer\nlearning and one\nof its most successful\napplications, the BERT model.\nThere's a wide\nrange of NLP tasks,\nfrom sentiment\nclassification, named\nentity recognition, question\nanswering, machine translation,\nand so on.\nThe common thread\nacross all tasks\nis that they require\nsome general knowledge\nabout language.\nFor instance, knowing\nwhen something\nis a verb or a proper\nname is useful,\nregardless of the\nexact application.\nTransfer learning was born based\non the observation that it's\na lot more effective to acquire\nthis knowledge once and reuse\nit in all of these applications.\nThis is similar to\nhow humans behave.\nWe learn to speak in\nchildhood, and then we\ncarry this knowledge\nthroughout our lives,\nadapting how we speak\nto various situations.\nIn machine learning,\nin particular, we also\nface the challenge of\na limited label data.\nLabels are, most of the\ntime, produced by humans,\nand human labor is\ntime-consuming and expensive.\nIf your goal is to build\na sentiment classification\nsystem for movie\nreviews, having someone\nsift through every\nreview and decide\nwhether it's positive or\nnegative is very laborious.\nNot to mention that, in certain\ncases, a lot of expertise\nis required.\nFor instance, if you want to\nbuild a system that translates\nnatural language\ninto executable code,\nyou need a programmer to\nproduce training examples.\nThe philosophy of\ntransfer learning\nis to leverage very\ncheap, unstructured\ndata that is readily\navailable online to pretrain\na model so that\nwhen it's presented\nwith the scarce labeled data,\nit hits the ground running.\nThe most popular paradigm\nfor transferred learning\nis pretaining and\nfollowed by fine-tuning.\nIt consists of two training\nstages applied sequentially.\nFirst, we train a\ngeneral purpose model\nusing unstructured\ndata from the internet,\nusually with a language\nmodel objective.\nAnd then we continue\ntraining it on the label data\nand specialize it for our\nparticular target task.\nThis technique is currently\nthe state-of-the-art across\nthe majority of natural\nlanguage applications.\nLet's revisit our initial task\nof automatically detecting\nwhether this TV show\nreview from Rotten Tomatoes\nis positive or negative.\nWhen we initially\nasked the question\nof how to encode\nthe text in a way\nthat a machine learning\nmodel can process it,\nwe agreed that using continuous\nvector representations\nfor each token was\nthe way forward.\nThe next challenge was\nto learn these embeddings\nin a way that captures\nmeaningful semantic\nrelationships\nbetween words, and we\nagreed that Wikipedia was a good\nsource of linguistic and world\nknowledge.\nWe then discussed\nthree language models\nthat gamify unstructured\ntext by predicting\nthe next word in a sentence.\nSo the remaining\npiece of mystery\nis, once we've trained\na language model,\nhow exactly do we\nleverage it in order\nto solve our original task?\nHow do we do this\ntransfer of knowledge?\nA potential answer is\nthe sequential paradigm\nof pretraining followed\nby fine-tuning.\nNext, we're going to talk\nabout how exactly this arrow is\nimplemented.\nWe're going to start off\nwith a transformer stack.\nThis transformer\nlearns a language model\nwhen inputted inputs\nlike, the king\nis the title given\nto a male blank.\nSo its goal is to\nfigure out what exactly\nis hiding behind the mask.\nThe stack contains\nan embedding table\nwith noncontextual\ncontinuous representations,\nfollowed by a transformer\nencoder that contextualizes\nthese embeddings.\nAnd at the end, the model\nwill predict a token.\nHopefully, monarch.\nWhat do we do in the\npretraining and fine-tuning\nparadigm is take a\ncopy of the model\nthat we just learned using\na language model objective.\nSo we've just copied\nover the embedding table\nand the transformer encoder.\nHow exactly do we\nuse this model, which\nonly knows how to do\nlanguage modeling,\nto do sentiment classification?\nWell, we add a classifier\non top, which could just\nbe a linear transformation.\nAnd these are newly\nadded parameters.\nThis classifier\nis able to ingest\nsome contextual embeddings,\nproduced by the transformer\nencoder, and output\nthe desired label.\nWhen training on the\nlabel data, now we\nfine-tune the entire stack.\nSo remember that the embedding\ntable and the transformer\nencoder were already in\npretty good positions.\nWe just copied them from a model\nthat can do language modeling.\nThe only randomly\ninitialized bit of the model\nis the classifier.\nWhen we do training on the\nsentiment classification task,\nwe update all three\ncomponents of the model--\nthe classifier, the\ntransformer encoder,\nand the embedding table.\nThe hope is that because\nthe embedding table\nand the transformer\nencoder, which\ncontain most of the\nparameters, are already\nin a pretty good state,\nthis fine-tuning process\nis relatively lightweight,\ncompared to full pretraining.\nFinally, we'll talk\nabout BERT, which\nstands for Bidirectional\nEncoded Representations\nfor Transformers,\nwhich is just one\ninstantiation of the\ntransformer architecture.\nBERT was trained at\nGoogle on Wikipedia data\nwith a language\nmodeling objective.\nBERT is readily available for\ndownload on multiple platforms,\nincluding Github,\nTF-Hub, and Hugging Face.\nThere's multiple models.\nThe ones in English\ncome in different sizes.\nThe smallest model has only two\nlayers and embedding size 128,\nand the biggest one has 24\nlayers and embedding size\n1,024.\nSo there's quite a\nhuge range of models.\nThere's also a model\nin Chinese and a model\nthat is multilingual and\nsupports 104 languages.\nThe multilingual model\nis pretty impressive,\nbecause you can feed an input\nin any of the 104 languages,\nnot even specify what\nlanguage you are inputting\nor what language the input\nhas, and it will probably\ndo the right thing.\nHow do we use BERT\nin a downstream task?\nWell, there is multiple classes\nof tasks, and each of them\ncomes with its own\nspecial use case.\nThe simplest class of\ntasks is a single sentence\nclassification--\nfor instance, sentiment\nclassification.\nGiven a movie review, we\nhave to output a label,\neither positive or negative.\nHere's how we can\nuse the BERT stack.\nSo remember, the first step\nwas just you copy BERT,\ndownload it, as it was made\nreadily available by Google,\nand then add a\nclassifier on top.\nThere are some\nimplementation details\nthat come with BERT that\nI want to dive into.\nIf we look at the\nbottom of the diagram,\nyou'll see that, in addition\nto the input tokens,\nthat single sentence that\nwe're inputting to the model,\nthere's a special\ntoken called CLS.\nIt stands for Classification.\nThis is a special\nartificial token\nthat we add to our\nvocabulary to mark\nthe beginning of a sentence.\nThe rest of the stack works just\nas a normal transformer encoder\nwould do.\nAt the bottom, we have the\nnoncontextual embeddings,\nand at the top, we have the\ncontextualized embeddings.\nWhat exactly do we feed\ninto the classifier?\nWell, in all honesty, we could\nfeed any of the top embeddings,\nbecause all of them are\nfully contextualized\nand they're aware of\nthe entire sentence.\nBut they might be\nlocalized to the meaning\nof a particular token.\nSo to get around\nthat artifact, we\ncan simply take the embedding\nof the CLS token, which\nremember, it does know about the\nmeaning of the entire sentence,\nbecause at the top\nof the network,\neverything is fully\ncontextualized.\nBut it doesn't\nparticularly focus\non one word of the sentence.\nSo if we feed the\nembedding of the CLS token\ninto a classifier, we can then\nget a prediction for our end\nsentiment classification task.\nAnother class of natural\nlanguage processing tasks\nis classification\nwhere the inputs\nare a pair of sentences, as\nopposed to a single sentence.\nParaphrased classification\nis one example.\nYou're given two sentences,\nlike she accepted immediately\nand the woman did not\nhesitate to accept.\nAnd the task is to decide\nwhether these two sentences are\nparaphrases of each other.\nAs before, we just make a\ncopy of the BERT model that's\nreadily available online and\nwe add a classifier on top.\nThe detail that I\nwant to emphasize\nis the fact that there's\na special separator\ntoken that is added in between\nthe two input sentences.\nJust like CLS, this\nis a special token\nthat's added to the dictionary\nwith the purpose of marking\nthe end of one sentence and the\nbeginning of the other sentence\nso that the model\nknows what tokens fall\ninto the first sentence\nand what tokens\nfall into the other sentence.\nAnd finally, another class of\nnatural language processing\ntasks is span annotation.\nThis refers to the\nfact that we need\nto make a classification\nor a decision\nat every point in the sentence.\nPart of speech tagging\nis one example.\nFor a sentence like she\naccepted immediately,\nwe want to tag every single\nword with its part of speech.\nSo she is a pronoun.\nAccepted is a verb.\nImmediately is an adverb.\nSame story-- we copy\nthe model, as it's\nreadily available to download.\nAnd of course, we're going\nto need a classifier.\nBut how do we use this\nclassifier in a way\nthat it makes\nmultiple predictions?\nWell, we're just going\nto pass every embedding\nin the sentence, one by\none, through the classifier.\nLet's say that T1\ncorresponds to she.\nWe pass the contextualized\nembedding of she\nthrough the classifier\nand produce a label.\nSimilarly, let's say T2\ncorresponds to accepted.\nWe pass the contextual\nembedding of accepted\nthrough the classifier\nand hopefully get\na verb label, and so on\nfor the entire sentence.\nSo now we've visited three main\ncategories of downstream tasks\nthat can be solved by\nsimply fine-tuning BERT.\nHere's a piece of\ncode that shows you\nhow to do that in practice.\nBERT is available on\nmultiple platforms,\nTF-Hub being one of them.\nTF-Hub is particularly\neasy to use,\nbecause everything\nyou have to do\nis provide a URL with the model\nthat you want to download.\nSo we're going to\nuse the hub library,\nand we're going to ask\nTF-Hub to download the model\nfrom this specific location.\nIn my case, I'm choosing a very\nsmall model, because I don't\nwant to wait a long time\nfor it to get downloaded\nand it has sequence length 128.\nThe API of this\nlibrary requires me\nto define what the inputs\nto the model look like,\nor what are the\nplaceholders where I'm going\nto feed in the input text?\nAnd these can be\ndefined as Keras inputs.\nThe next step is to\nextract the CLS embedding\nfrom the BERT outputs.\nRemember that if I want to\ndo sentiment classification,\nI need to take the contextual\nembedding of the CLS token\nand pass it through\na classifier.\nOnce I take the CLS embedding,\nI can define my own classifier.\nIn this case, the classifier\nis a dense KerasLayer\nwith two units, because the\nsentiment classification\ntask has two possible classes.\nAnd finally, I pass the\ncontextual CLS embedding\nthrough the classifier.\nAnd I define a model that\ntakes as inputs whatever\nI feed into BERT, and it\ntakes as outputs whatever\nthe classifier outputs.\nAt this point, I'm ready to\njust call model.fit() and train\nmy model.\nBecause BERT was\nquite a success story,\nit inspired quite a\nbit of follow-up work.\nThere are models that\nrevisit the training process\nand make it more\nrobust, like RoBERTa.\nThere's a lot of\nresearch that has\ngone into preserving\nthe accuracy of BERT,\nbut shrinking it or making\nit faster for inference.\nAnd this is the case for\nALBERT, MobileBERT, TInyBERT,\nand a lot of other models.\nThere's also language-specific\nmodels trained specifically\nfor French, Chinese, and so on.\nMost of these are,\nagain, readily\navailable to be downloaded.\nAnd you can build your own model\nby simply fine-tuning them.\nThis concludes our journey\nthrough the recent history\nof natural language processing.\nAs of today, transformers remain\nthe preferred architecture\nfor most NLP tasks.\nAnd pretraining\nand fine-tuning--\nthe paradigm itself\nis ubiquitous.\nThe biggest roadblock\nthat we're facing today\nis that transformers require\na fixed input length.\nAnd because of their\ncomputational complexity,\nthey cannot be effectively\nscaled beyond a few hundreds\nof tokens.\nThe solution might come either\nfrom specialized hardware\nor from innovations\non the modeling side.\nEither way, natural\nlanguage processing\nremains a very interesting\nfield for research\nand for industry applications.\nThanks for your attention,\nand make sure to check out\nthe other videos in the series.\n[MUSIC PLAYING]\n",
  "words": [
    "iulia",
    "turc",
    "hi",
    "everyone",
    "name",
    "iulia",
    "work",
    "google",
    "research",
    "natural",
    "language",
    "processing",
    "presentation",
    "walk",
    "recent",
    "history",
    "natural",
    "language",
    "processing",
    "including",
    "current",
    "architecture",
    "transformers",
    "also",
    "discuss",
    "transformers",
    "became",
    "particularly",
    "successful",
    "used",
    "context",
    "transfer",
    "learning",
    "technique",
    "leverages",
    "massive",
    "amounts",
    "text",
    "tutorial",
    "structured",
    "three",
    "sections",
    "first",
    "going",
    "talk",
    "encoding",
    "text",
    "numerical",
    "representation",
    "manipulated",
    "machine",
    "learning",
    "models",
    "second",
    "discuss",
    "one",
    "important",
    "tasks",
    "natural",
    "language",
    "processing",
    "namely",
    "language",
    "modeling",
    "among",
    "architectures",
    "see",
    "transformers",
    "solve",
    "particular",
    "task",
    "third",
    "dive",
    "transfer",
    "learning",
    "mechanism",
    "enabled",
    "transformers",
    "natural",
    "language",
    "processing",
    "finally",
    "look",
    "bert",
    "one",
    "popular",
    "transformer",
    "models",
    "let",
    "start",
    "answering",
    "following",
    "question",
    "encode",
    "text",
    "numerical",
    "format",
    "well",
    "currently",
    "natural",
    "language",
    "processing",
    "heavily",
    "machine",
    "nlp",
    "systems",
    "follow",
    "contract",
    "piece",
    "natural",
    "language",
    "text",
    "input",
    "machine",
    "learning",
    "model",
    "produces",
    "prediction",
    "qualifies",
    "describes",
    "input",
    "way",
    "could",
    "discrete",
    "class",
    "real",
    "value",
    "another",
    "piece",
    "text",
    "let",
    "look",
    "sentiment",
    "classification",
    "one",
    "common",
    "nlp",
    "tasks",
    "input",
    "could",
    "something",
    "like",
    "tv",
    "show",
    "review",
    "rotten",
    "tomatoes",
    "like",
    "positive",
    "review",
    "bojack",
    "horseman",
    "expected",
    "output",
    "probability",
    "distribution",
    "two",
    "classes",
    "positive",
    "negative",
    "indicating",
    "author",
    "review",
    "perceived",
    "tv",
    "show",
    "section",
    "discuss",
    "first",
    "part",
    "contract",
    "namely",
    "encode",
    "tv",
    "show",
    "review",
    "numerical",
    "representation",
    "handled",
    "machine",
    "learning",
    "model",
    "start",
    "naive",
    "view",
    "assume",
    "words",
    "discrete",
    "independent",
    "tokens",
    "one",
    "way",
    "forward",
    "would",
    "gather",
    "unique",
    "tokens",
    "training",
    "corpus",
    "build",
    "dictionary",
    "tokens",
    "sorting",
    "alphabetically",
    "first",
    "word",
    "aardvark",
    "king",
    "queen",
    "somewhere",
    "dictionary",
    "remember",
    "want",
    "convert",
    "text",
    "numbers",
    "first",
    "thing",
    "could",
    "assign",
    "index",
    "token",
    "reflecting",
    "order",
    "alphabetical",
    "order",
    "problem",
    "high",
    "numbers",
    "amenable",
    "machine",
    "learning",
    "gradient",
    "descent",
    "particular",
    "another",
    "thing",
    "could",
    "use",
    "embeddings",
    "vectors",
    "dimensionality",
    "dictionary",
    "dictionary",
    "size",
    "vector",
    "would",
    "size",
    "one",
    "entries",
    "would",
    "1",
    "others",
    "instance",
    "aardvark",
    "1",
    "first",
    "position",
    "0",
    "others",
    "embeddings",
    "use",
    "values",
    "amenable",
    "machine",
    "learning",
    "two",
    "main",
    "disadvantages",
    "first",
    "stored",
    "inspectors",
    "high",
    "dimensionality",
    "since",
    "one",
    "dimension",
    "every",
    "entry",
    "vocabulary",
    "second",
    "fail",
    "capture",
    "world",
    "knowledge",
    "whatsoever",
    "tokens",
    "particular",
    "case",
    "king",
    "queen",
    "common",
    "aardvark",
    "yet",
    "vectors",
    "angles",
    "see",
    "clearly",
    "let",
    "limit",
    "vocabulary",
    "three",
    "tokens",
    "case",
    "size",
    "vocabulary",
    "three",
    "dimensionality",
    "vectors",
    "three",
    "words",
    "vectors",
    "space",
    "embedding",
    "words",
    "unit",
    "vectors",
    "aligned",
    "axes",
    "opportunity",
    "cost",
    "allow",
    "words",
    "occupy",
    "entire",
    "space",
    "opposed",
    "perfectly",
    "aligned",
    "axes",
    "practical",
    "view",
    "would",
    "words",
    "continuous",
    "vectors",
    "space",
    "allow",
    "queen",
    "king",
    "aardvark",
    "flow",
    "anywhere",
    "space",
    "representation",
    "contains",
    "real",
    "values",
    "like",
    "minus",
    "move",
    "quick",
    "note",
    "terminology",
    "way",
    "embedding",
    "tokens",
    "comes",
    "different",
    "names",
    "known",
    "continuous",
    "distributed",
    "embeddings",
    "vectors",
    "representations",
    "use",
    "terms",
    "interchangeably",
    "throughout",
    "presentation",
    "sort",
    "properties",
    "would",
    "like",
    "word",
    "tokens",
    "reflect",
    "aim",
    "encompass",
    "world",
    "knowledge",
    "enumerate",
    "certain",
    "aspects",
    "like",
    "capture",
    "gender",
    "part",
    "speech",
    "conceptually",
    "various",
    "dimensions",
    "vector",
    "could",
    "dedicated",
    "aspects",
    "instance",
    "first",
    "three",
    "dimensions",
    "could",
    "encode",
    "gender",
    "next",
    "three",
    "could",
    "encode",
    "part",
    "speech",
    "course",
    "enforce",
    "particular",
    "attribution",
    "meaning",
    "dimensions",
    "hope",
    "model",
    "discovers",
    "something",
    "sort",
    "mechanism",
    "use",
    "express",
    "desired",
    "relationships",
    "relative",
    "vector",
    "distance",
    "instance",
    "gender",
    "dimension",
    "king",
    "queen",
    "far",
    "apart",
    "men",
    "women",
    "however",
    "dimension",
    "words",
    "clustered",
    "together",
    "distance",
    "virtually",
    "0",
    "since",
    "nouns",
    "contrast",
    "play",
    "verb",
    "playful",
    "adjective",
    "distance",
    "joy",
    "joyful",
    "enumerated",
    "desirable",
    "properties",
    "embeddings",
    "sort",
    "relationships",
    "want",
    "encode",
    "exactly",
    "learn",
    "useful",
    "embeddings",
    "well",
    "luckily",
    "internet",
    "free",
    "contains",
    "massive",
    "amount",
    "information",
    "good",
    "chunk",
    "text",
    "even",
    "better",
    "wikipedia",
    "currently",
    "contains",
    "28",
    "billion",
    "words",
    "309",
    "languages",
    "reasonably",
    "controlled",
    "reliable",
    "source",
    "information",
    "looking",
    "wikipedia",
    "pages",
    "king",
    "queen",
    "see",
    "lot",
    "commonality",
    "reference",
    "also",
    "include",
    "common",
    "words",
    "like",
    "monarch",
    "surely",
    "quite",
    "bit",
    "word",
    "knowledge",
    "extract",
    "might",
    "know",
    "machine",
    "learning",
    "operates",
    "pairs",
    "inputs",
    "outputs",
    "machine",
    "learning",
    "model",
    "given",
    "input",
    "x",
    "produce",
    "prediction",
    "describes",
    "input",
    "one",
    "way",
    "another",
    "wikipedia",
    "comes",
    "freeform",
    "text",
    "gamify",
    "text",
    "order",
    "model",
    "learn",
    "something",
    "common",
    "practice",
    "turn",
    "unsupervised",
    "text",
    "data",
    "supervised",
    "task",
    "instance",
    "could",
    "ask",
    "model",
    "fill",
    "gaps",
    "predict",
    "next",
    "word",
    "sentence",
    "like",
    "king",
    "title",
    "given",
    "male",
    "model",
    "could",
    "produce",
    "probability",
    "distribution",
    "words",
    "vocabulary",
    "indicating",
    "ones",
    "likely",
    "follow",
    "words",
    "far",
    "case",
    "monarch",
    "would",
    "good",
    "candidate",
    "fill",
    "gap",
    "move",
    "discuss",
    "language",
    "modeling",
    "gamification",
    "unstructured",
    "text",
    "allows",
    "us",
    "learn",
    "text",
    "even",
    "absence",
    "explicit",
    "labels",
    "traditionally",
    "language",
    "model",
    "defined",
    "task",
    "predicting",
    "next",
    "word",
    "sentence",
    "given",
    "words",
    "0",
    "n",
    "minus",
    "1",
    "challenge",
    "predict",
    "nth",
    "word",
    "variations",
    "traditional",
    "formulation",
    "instance",
    "context",
    "could",
    "limited",
    "previous",
    "three",
    "words",
    "instead",
    "making",
    "prediction",
    "based",
    "previous",
    "words",
    "could",
    "make",
    "prediction",
    "based",
    "surrounding",
    "words",
    "ones",
    "left",
    "generally",
    "language",
    "modeling",
    "refers",
    "predicting",
    "piece",
    "text",
    "given",
    "textual",
    "context",
    "discuss",
    "three",
    "main",
    "turning",
    "points",
    "history",
    "language",
    "modeling",
    "word2vec",
    "introduced",
    "2013",
    "recurring",
    "neural",
    "networks",
    "introduced",
    "2014",
    "transformers",
    "published",
    "transformers",
    "currently",
    "architecture",
    "language",
    "modeling",
    "let",
    "start",
    "word2vec",
    "model",
    "introduced",
    "distributed",
    "representations",
    "language",
    "modeling",
    "build",
    "simple",
    "language",
    "model",
    "inspired",
    "word2vec",
    "let",
    "start",
    "simple",
    "input",
    "like",
    "river",
    "hoping",
    "model",
    "guess",
    "missing",
    "word",
    "bank",
    "simplicity",
    "limit",
    "dictionary",
    "five",
    "tokens",
    "bank",
    "fish",
    "river",
    "going",
    "start",
    "randomly",
    "initialized",
    "embeddings",
    "best",
    "world",
    "knowledge",
    "point",
    "first",
    "step",
    "look",
    "every",
    "word",
    "matrix",
    "pulled",
    "vectors",
    "next",
    "step",
    "embed",
    "aggregate",
    "tokens",
    "wholesome",
    "view",
    "entire",
    "sentence",
    "order",
    "predict",
    "follows",
    "three",
    "words",
    "need",
    "understand",
    "together",
    "simple",
    "thing",
    "average",
    "embedding",
    "together",
    "remember",
    "dummy",
    "model",
    "going",
    "ignore",
    "fact",
    "even",
    "capable",
    "taking",
    "word",
    "order",
    "account",
    "ok",
    "averaged",
    "embeddings",
    "single",
    "sentence",
    "representation",
    "end",
    "goal",
    "produce",
    "score",
    "every",
    "word",
    "vocabulary",
    "indicating",
    "likely",
    "follow",
    "sentence",
    "piece",
    "sentence",
    "river",
    "turn",
    "vector",
    "vector",
    "well",
    "simplest",
    "option",
    "linear",
    "transformation",
    "using",
    "2",
    "5",
    "matrix",
    "role",
    "softmax",
    "parameters",
    "trained",
    "together",
    "rest",
    "model",
    "remember",
    "weights",
    "model",
    "arbitrary",
    "point",
    "scores",
    "assigns",
    "five",
    "vocabulary",
    "tokens",
    "next",
    "step",
    "would",
    "normalize",
    "way",
    "sum",
    "1",
    "form",
    "probability",
    "distribution",
    "common",
    "way",
    "use",
    "logistic",
    "regression",
    "expression",
    "red",
    "dots",
    "would",
    "correspond",
    "x",
    "formula",
    "probability",
    "distribution",
    "produced",
    "model",
    "guessing",
    "come",
    "river",
    "seems",
    "like",
    "model",
    "giving",
    "high",
    "probability",
    "bank",
    "reason",
    "also",
    "teach",
    "model",
    "tell",
    "whether",
    "made",
    "good",
    "prediction",
    "well",
    "peek",
    "word",
    "predicted",
    "bank",
    "turn",
    "probability",
    "distribution",
    "true",
    "distribution",
    "one",
    "represented",
    "right",
    "assign",
    "probably",
    "1",
    "bank",
    "probability",
    "0",
    "words",
    "two",
    "distributions",
    "compute",
    "sort",
    "distance",
    "two",
    "distributions",
    "done",
    "cross",
    "entropy",
    "distance",
    "two",
    "distributions",
    "backpropagate",
    "error",
    "tell",
    "model",
    "adjust",
    "weights",
    "way",
    "would",
    "led",
    "correct",
    "prediction",
    "nutshell",
    "language",
    "model",
    "trained",
    "principles",
    "apply",
    "language",
    "models",
    "going",
    "discuss",
    "word2vec",
    "admirable",
    "effort",
    "really",
    "revolutionized",
    "field",
    "came",
    "disadvantages",
    "paradigm",
    "one",
    "associating",
    "fixed",
    "embedding",
    "word",
    "handle",
    "cases",
    "words",
    "multiple",
    "meanings",
    "instance",
    "word",
    "bank",
    "open",
    "bank",
    "account",
    "river",
    "bank",
    "would",
    "map",
    "embedding",
    "ideally",
    "want",
    "embeddings",
    "contextualized",
    "reflect",
    "surrounding",
    "words",
    "around",
    "open",
    "bank",
    "account",
    "would",
    "different",
    "embedding",
    "river",
    "bank",
    "move",
    "recurring",
    "neural",
    "networks",
    "rnns",
    "provide",
    "way",
    "contextualizing",
    "embeddings",
    "words",
    "disambiguated",
    "based",
    "context",
    "let",
    "trace",
    "behavior",
    "rnns",
    "input",
    "sentence",
    "like",
    "river",
    "bank",
    "sat",
    "task",
    "model",
    "predict",
    "missing",
    "word",
    "sat",
    "building",
    "rnn",
    "start",
    "noncontextual",
    "embeddings",
    "like",
    "word2vec",
    "looked",
    "regular",
    "embedding",
    "table",
    "sequential",
    "manner",
    "feed",
    "every",
    "noncontextual",
    "embedding",
    "rnn",
    "cell",
    "rnn",
    "cell",
    "internal",
    "state",
    "continuously",
    "updates",
    "progresses",
    "sentence",
    "denoted",
    "h",
    "slide",
    "think",
    "encoding",
    "sentence",
    "far",
    "cell",
    "produces",
    "contextualized",
    "embedding",
    "based",
    "internal",
    "representation",
    "h",
    "captures",
    "side",
    "context",
    "noncontextual",
    "embedding",
    "current",
    "token",
    "word",
    "bank",
    "mechanism",
    "particularly",
    "helpful",
    "time",
    "rnn",
    "reaches",
    "bank",
    "already",
    "seen",
    "word",
    "river",
    "encoded",
    "internal",
    "state",
    "better",
    "position",
    "disambiguate",
    "meaning",
    "financial",
    "institution",
    "final",
    "contextual",
    "embedding",
    "computed",
    "use",
    "final",
    "internal",
    "state",
    "rnn",
    "make",
    "prediction",
    "next",
    "word",
    "add",
    "linear",
    "classifier",
    "maps",
    "hidden",
    "state",
    "vector",
    "h5",
    "vector",
    "dimensionality",
    "vocabulary",
    "get",
    "score",
    "per",
    "vocabulary",
    "token",
    "train",
    "model",
    "using",
    "cross",
    "entropy",
    "rnn",
    "encoders",
    "great",
    "contextualize",
    "word",
    "embeddings",
    "certain",
    "disadvantages",
    "first",
    "one",
    "quite",
    "slow",
    "sequential",
    "manner",
    "embedding",
    "word",
    "n",
    "depends",
    "embeddings",
    "previous",
    "words",
    "means",
    "wait",
    "computed",
    "speed",
    "n",
    "number",
    "tokens",
    "another",
    "problem",
    "problem",
    "vanishing",
    "gradient",
    "another",
    "way",
    "saying",
    "time",
    "arrive",
    "end",
    "sentence",
    "might",
    "forgotten",
    "start",
    "sentence",
    "looked",
    "like",
    "sequential",
    "processing",
    "way",
    "revisiting",
    "early",
    "tokens",
    "effect",
    "structure",
    "might",
    "learn",
    "little",
    "early",
    "tokens",
    "third",
    "disadvantage",
    "rnns",
    "unidirectional",
    "means",
    "process",
    "text",
    "left",
    "right",
    "unlike",
    "humans",
    "sometimes",
    "problem",
    "relevant",
    "context",
    "right",
    "lucky",
    "sentence",
    "like",
    "river",
    "bank",
    "river",
    "happened",
    "occur",
    "bank",
    "parse",
    "bank",
    "river",
    "time",
    "process",
    "bank",
    "information",
    "river",
    "disambiguate",
    "financial",
    "institution",
    "third",
    "language",
    "modeling",
    "technique",
    "currently",
    "transformers",
    "address",
    "problem",
    "unidirectionality",
    "teaching",
    "model",
    "take",
    "account",
    "context",
    "side",
    "side",
    "sentence",
    "simplicity",
    "order",
    "consistent",
    "models",
    "going",
    "go",
    "transformer",
    "left",
    "right",
    "let",
    "see",
    "transformer",
    "works",
    "going",
    "start",
    "sentence",
    "like",
    "river",
    "bank",
    "hoping",
    "model",
    "fill",
    "blank",
    "note",
    "time",
    "missing",
    "token",
    "passing",
    "special",
    "token",
    "called",
    "mask",
    "part",
    "vocabulary",
    "like",
    "regular",
    "token",
    "first",
    "step",
    "look",
    "words",
    "embedding",
    "table",
    "get",
    "noncontextual",
    "embeddings",
    "xi",
    "could",
    "instance",
    "word2vec",
    "going",
    "feed",
    "every",
    "token",
    "box",
    "called",
    "one",
    "main",
    "building",
    "blocks",
    "transformers",
    "works",
    "taking",
    "account",
    "paying",
    "attention",
    "tokens",
    "embedding",
    "position",
    "0",
    "take",
    "account",
    "positions",
    "1",
    "output",
    "see",
    "weighted",
    "average",
    "tokens",
    "paid",
    "attention",
    "comes",
    "terminology",
    "become",
    "easier",
    "understand",
    "bit",
    "later",
    "token",
    "currently",
    "embedded",
    "called",
    "query",
    "token",
    "tokens",
    "pay",
    "attention",
    "called",
    "key",
    "tokens",
    "key",
    "tokens",
    "need",
    "span",
    "entire",
    "sentence",
    "could",
    "span",
    "next",
    "three",
    "words",
    "instance",
    "general",
    "transformers",
    "use",
    "entire",
    "sentence",
    "context",
    "embed",
    "word",
    "word",
    "becomes",
    "query",
    "token",
    "others",
    "key",
    "tokens",
    "river",
    "bank",
    "let",
    "take",
    "step",
    "back",
    "explain",
    "terminology",
    "comes",
    "get",
    "intuition",
    "naming",
    "convention",
    "think",
    "attention",
    "soft",
    "dictionary",
    "lookup",
    "say",
    "dictionary",
    "dict",
    "mapping",
    "tokens",
    "value",
    "looking",
    "word",
    "bank",
    "dictionary",
    "well",
    "bank",
    "becomes",
    "query",
    "like",
    "retrieve",
    "value",
    "bank",
    "would",
    "v3",
    "like",
    "get",
    "weighing",
    "values",
    "reflecting",
    "similar",
    "keys",
    "bank",
    "course",
    "similar",
    "word",
    "bank",
    "probably",
    "word",
    "bank",
    "weight",
    "next",
    "relevant",
    "word",
    "river",
    "disambiguates",
    "meaning",
    "bank",
    "gets",
    "pretty",
    "high",
    "score",
    "words",
    "get",
    "score",
    "pretty",
    "low",
    "finally",
    "mask",
    "weight",
    "0",
    "carries",
    "information",
    "aside",
    "justify",
    "current",
    "token",
    "called",
    "query",
    "token",
    "others",
    "called",
    "key",
    "tokens",
    "ready",
    "open",
    "box",
    "get",
    "clarity",
    "exactly",
    "happens",
    "inside",
    "three",
    "model",
    "parameters",
    "matrices",
    "key",
    "parameters",
    "query",
    "parameters",
    "value",
    "parameters",
    "terminology",
    "already",
    "familiar",
    "unsurprisingly",
    "key",
    "parameters",
    "apply",
    "transformation",
    "key",
    "tokens",
    "multiplying",
    "vector",
    "like",
    "x0",
    "matrix",
    "k",
    "obtain",
    "another",
    "vector",
    "key0",
    "remember",
    "viewed",
    "keys",
    "dictionary",
    "going",
    "put",
    "aside",
    "let",
    "move",
    "query",
    "parameters",
    "let",
    "see",
    "well",
    "also",
    "unsurprisingly",
    "query",
    "parameters",
    "transform",
    "query",
    "token",
    "multiply",
    "noncontextual",
    "embedding",
    "bank",
    "matrix",
    "order",
    "obtain",
    "embedding",
    "query",
    "let",
    "put",
    "one",
    "aside",
    "well",
    "hinted",
    "bit",
    "earlier",
    "fact",
    "works",
    "weighted",
    "sum",
    "conceptually",
    "basically",
    "want",
    "assign",
    "weight",
    "every",
    "single",
    "token",
    "sentence",
    "weight",
    "reflect",
    "usefulness",
    "particular",
    "token",
    "embedding",
    "query",
    "token",
    "measuring",
    "similarity",
    "keys",
    "queries",
    "multiple",
    "similarity",
    "measures",
    "two",
    "vectors",
    "commonly",
    "uses",
    "dot",
    "product",
    "applying",
    "dot",
    "product",
    "key",
    "query",
    "get",
    "scaler",
    "attention",
    "score",
    "reflects",
    "relevant",
    "key",
    "query",
    "let",
    "put",
    "attention",
    "scores",
    "side",
    "remaining",
    "parameter",
    "talked",
    "values",
    "matrix",
    "going",
    "multiply",
    "input",
    "tokens",
    "value",
    "matrix",
    "obtain",
    "another",
    "set",
    "vectors",
    "called",
    "values",
    "referencing",
    "back",
    "previous",
    "slide",
    "analogy",
    "dictionary",
    "think",
    "values",
    "values",
    "stored",
    "dictionary",
    "key",
    "value",
    "finally",
    "box",
    "returns",
    "sum",
    "weighted",
    "values",
    "remember",
    "attention",
    "scores",
    "show",
    "similarity",
    "key",
    "query",
    "get",
    "multiplied",
    "values",
    "output",
    "indeed",
    "weighted",
    "sum",
    "values",
    "one",
    "implementation",
    "detail",
    "might",
    "multiple",
    "sets",
    "keys",
    "query",
    "value",
    "parameters",
    "called",
    "attention",
    "heads",
    "illustration",
    "three",
    "attention",
    "heads",
    "multiple",
    "attention",
    "heads",
    "equivalent",
    "multiple",
    "kernels",
    "computer",
    "vision",
    "processing",
    "image",
    "common",
    "apply",
    "multiple",
    "effects",
    "like",
    "blurring",
    "sharpening",
    "outlining",
    "order",
    "understand",
    "various",
    "aspects",
    "input",
    "language",
    "lot",
    "abstract",
    "trivial",
    "describe",
    "transformation",
    "head",
    "expected",
    "could",
    "see",
    "focusing",
    "various",
    "linguistic",
    "aspects",
    "one",
    "time",
    "might",
    "beneficial",
    "instance",
    "one",
    "head",
    "might",
    "pay",
    "attention",
    "parts",
    "speech",
    "another",
    "head",
    "could",
    "focus",
    "verb",
    "tenses",
    "words",
    "everything",
    "saw",
    "previous",
    "slide",
    "perform",
    "three",
    "times",
    "apply",
    "transformations",
    "three",
    "times",
    "going",
    "end",
    "three",
    "outputs",
    "three",
    "outputs",
    "exactly",
    "combine",
    "one",
    "output",
    "well",
    "put",
    "feed",
    "forward",
    "network",
    "top",
    "three",
    "inputs",
    "combine",
    "back",
    "one",
    "single",
    "output",
    "multiheaded",
    "let",
    "zoom",
    "box",
    "mentioned",
    "earlier",
    "even",
    "though",
    "multiple",
    "boxes",
    "slide",
    "represent",
    "copies",
    "box",
    "illustration",
    "would",
    "one",
    "set",
    "parameters",
    "might",
    "noticed",
    "already",
    "somewhat",
    "expensive",
    "since",
    "every",
    "token",
    "attends",
    "every",
    "token",
    "n",
    "square",
    "complexity",
    "good",
    "news",
    "though",
    "recurrent",
    "neural",
    "network",
    "processing",
    "input",
    "n",
    "need",
    "wait",
    "processing",
    "input",
    "n",
    "minus",
    "1",
    "n",
    "minus",
    "2",
    "dependencies",
    "refer",
    "previous",
    "layer",
    "arrows",
    "going",
    "boxes",
    "means",
    "depend",
    "previous",
    "layer",
    "sequence",
    "need",
    "respect",
    "compute",
    "things",
    "layer",
    "layer",
    "within",
    "one",
    "layer",
    "compute",
    "token",
    "embeddings",
    "right",
    "hardware",
    "even",
    "though",
    "see",
    "n",
    "square",
    "connections",
    "could",
    "theory",
    "compute",
    "parallel",
    "transformer",
    "talked",
    "far",
    "well",
    "main",
    "building",
    "block",
    "transformer",
    "transformer",
    "nothing",
    "stack",
    "layers",
    "layer",
    "gets",
    "separate",
    "set",
    "parameters",
    "transformer",
    "also",
    "first",
    "architecture",
    "advocate",
    "extremely",
    "deep",
    "networks",
    "uncommon",
    "models",
    "24",
    "layers",
    "far",
    "talked",
    "transformers",
    "turn",
    "noncontextual",
    "embeddings",
    "contextual",
    "embeddings",
    "remaining",
    "question",
    "exactly",
    "build",
    "language",
    "model",
    "transformers",
    "well",
    "one",
    "possibility",
    "pick",
    "one",
    "embedding",
    "output",
    "transformer",
    "instance",
    "first",
    "one",
    "remember",
    "transformer",
    "outputs",
    "one",
    "contextual",
    "embedding",
    "every",
    "single",
    "input",
    "sentence",
    "time",
    "reached",
    "top",
    "stack",
    "embeddings",
    "contextualized",
    "understand",
    "entire",
    "sentence",
    "arbitrarily",
    "pick",
    "first",
    "embedding",
    "build",
    "top",
    "previously",
    "word2vec",
    "rnn",
    "used",
    "linear",
    "transformation",
    "map",
    "embedding",
    "scores",
    "every",
    "word",
    "dictionary",
    "exactly",
    "going",
    "produce",
    "one",
    "score",
    "every",
    "word",
    "transformer",
    "encoder",
    "complicated",
    "piece",
    "machinery",
    "talked",
    "high",
    "level",
    "perspective",
    "implementation",
    "details",
    "crucial",
    "making",
    "work",
    "included",
    "slides",
    "include",
    "residual",
    "skip",
    "connections",
    "layer",
    "normalization",
    "dropout",
    "invite",
    "read",
    "original",
    "papers",
    "interested",
    "much",
    "detail",
    "transformers",
    "language",
    "modeling",
    "come",
    "disadvantages",
    "course",
    "computationally",
    "intensive",
    "work",
    "number",
    "layers",
    "multiplied",
    "n",
    "squared",
    "n",
    "number",
    "input",
    "tokens",
    "could",
    "alleviated",
    "hardware",
    "perform",
    "certain",
    "operations",
    "parallel",
    "like",
    "tpus",
    "normal",
    "hardware",
    "expensive",
    "another",
    "downside",
    "transformers",
    "require",
    "fixed",
    "number",
    "tokens",
    "words",
    "length",
    "sentence",
    "needs",
    "set",
    "designing",
    "model",
    "common",
    "inputs",
    "set",
    "512",
    "tokens",
    "input",
    "sentence",
    "longer",
    "truncated",
    "shorter",
    "padded",
    "concludes",
    "discussion",
    "language",
    "models",
    "visited",
    "three",
    "different",
    "approaches",
    "language",
    "modeling",
    "word2vec",
    "brings",
    "innovation",
    "continuous",
    "word",
    "representations",
    "superior",
    "encodings",
    "encode",
    "world",
    "knowledge",
    "also",
    "lower",
    "dimensionality",
    "rnns",
    "made",
    "embeddings",
    "contextual",
    "aware",
    "side",
    "context",
    "bank",
    "assigned",
    "different",
    "embedding",
    "riverbank",
    "finally",
    "transformers",
    "ensure",
    "embeddings",
    "bidirectional",
    "aware",
    "side",
    "side",
    "context",
    "instance",
    "phrase",
    "bank",
    "river",
    "meaning",
    "bank",
    "determined",
    "river",
    "even",
    "though",
    "river",
    "side",
    "final",
    "section",
    "discuss",
    "concept",
    "transfer",
    "learning",
    "one",
    "successful",
    "applications",
    "bert",
    "model",
    "wide",
    "range",
    "nlp",
    "tasks",
    "sentiment",
    "classification",
    "named",
    "entity",
    "recognition",
    "question",
    "answering",
    "machine",
    "translation",
    "common",
    "thread",
    "across",
    "tasks",
    "require",
    "general",
    "knowledge",
    "language",
    "instance",
    "knowing",
    "something",
    "verb",
    "proper",
    "name",
    "useful",
    "regardless",
    "exact",
    "application",
    "transfer",
    "learning",
    "born",
    "based",
    "observation",
    "lot",
    "effective",
    "acquire",
    "knowledge",
    "reuse",
    "applications",
    "similar",
    "humans",
    "behave",
    "learn",
    "speak",
    "childhood",
    "carry",
    "knowledge",
    "throughout",
    "lives",
    "adapting",
    "speak",
    "various",
    "situations",
    "machine",
    "learning",
    "particular",
    "also",
    "face",
    "challenge",
    "limited",
    "label",
    "data",
    "labels",
    "time",
    "produced",
    "humans",
    "human",
    "labor",
    "expensive",
    "goal",
    "build",
    "sentiment",
    "classification",
    "system",
    "movie",
    "reviews",
    "someone",
    "sift",
    "every",
    "review",
    "decide",
    "whether",
    "positive",
    "negative",
    "laborious",
    "mention",
    "certain",
    "cases",
    "lot",
    "expertise",
    "required",
    "instance",
    "want",
    "build",
    "system",
    "translates",
    "natural",
    "language",
    "executable",
    "code",
    "need",
    "programmer",
    "produce",
    "training",
    "examples",
    "philosophy",
    "transfer",
    "learning",
    "leverage",
    "cheap",
    "unstructured",
    "data",
    "readily",
    "available",
    "online",
    "pretrain",
    "model",
    "presented",
    "scarce",
    "labeled",
    "data",
    "hits",
    "ground",
    "running",
    "popular",
    "paradigm",
    "transferred",
    "learning",
    "pretaining",
    "followed",
    "consists",
    "two",
    "training",
    "stages",
    "applied",
    "sequentially",
    "first",
    "train",
    "general",
    "purpose",
    "model",
    "using",
    "unstructured",
    "data",
    "internet",
    "usually",
    "language",
    "model",
    "objective",
    "continue",
    "training",
    "label",
    "data",
    "specialize",
    "particular",
    "target",
    "task",
    "technique",
    "currently",
    "across",
    "majority",
    "natural",
    "language",
    "applications",
    "let",
    "revisit",
    "initial",
    "task",
    "automatically",
    "detecting",
    "whether",
    "tv",
    "show",
    "review",
    "rotten",
    "tomatoes",
    "positive",
    "negative",
    "initially",
    "asked",
    "question",
    "encode",
    "text",
    "way",
    "machine",
    "learning",
    "model",
    "process",
    "agreed",
    "using",
    "continuous",
    "vector",
    "representations",
    "token",
    "way",
    "forward",
    "next",
    "challenge",
    "learn",
    "embeddings",
    "way",
    "captures",
    "meaningful",
    "semantic",
    "relationships",
    "words",
    "agreed",
    "wikipedia",
    "good",
    "source",
    "linguistic",
    "world",
    "knowledge",
    "discussed",
    "three",
    "language",
    "models",
    "gamify",
    "unstructured",
    "text",
    "predicting",
    "next",
    "word",
    "sentence",
    "remaining",
    "piece",
    "mystery",
    "trained",
    "language",
    "model",
    "exactly",
    "leverage",
    "order",
    "solve",
    "original",
    "task",
    "transfer",
    "knowledge",
    "potential",
    "answer",
    "sequential",
    "paradigm",
    "pretraining",
    "followed",
    "next",
    "going",
    "talk",
    "exactly",
    "arrow",
    "implemented",
    "going",
    "start",
    "transformer",
    "stack",
    "transformer",
    "learns",
    "language",
    "model",
    "inputted",
    "inputs",
    "like",
    "king",
    "title",
    "given",
    "male",
    "blank",
    "goal",
    "figure",
    "exactly",
    "hiding",
    "behind",
    "mask",
    "stack",
    "contains",
    "embedding",
    "table",
    "noncontextual",
    "continuous",
    "representations",
    "followed",
    "transformer",
    "encoder",
    "contextualizes",
    "embeddings",
    "end",
    "model",
    "predict",
    "token",
    "hopefully",
    "monarch",
    "pretraining",
    "paradigm",
    "take",
    "copy",
    "model",
    "learned",
    "using",
    "language",
    "model",
    "objective",
    "copied",
    "embedding",
    "table",
    "transformer",
    "encoder",
    "exactly",
    "use",
    "model",
    "knows",
    "language",
    "modeling",
    "sentiment",
    "classification",
    "well",
    "add",
    "classifier",
    "top",
    "could",
    "linear",
    "transformation",
    "newly",
    "added",
    "parameters",
    "classifier",
    "able",
    "ingest",
    "contextual",
    "embeddings",
    "produced",
    "transformer",
    "encoder",
    "output",
    "desired",
    "label",
    "training",
    "label",
    "data",
    "entire",
    "stack",
    "remember",
    "embedding",
    "table",
    "transformer",
    "encoder",
    "already",
    "pretty",
    "good",
    "positions",
    "copied",
    "model",
    "language",
    "modeling",
    "randomly",
    "initialized",
    "bit",
    "model",
    "classifier",
    "training",
    "sentiment",
    "classification",
    "task",
    "update",
    "three",
    "components",
    "model",
    "classifier",
    "transformer",
    "encoder",
    "embedding",
    "table",
    "hope",
    "embedding",
    "table",
    "transformer",
    "encoder",
    "contain",
    "parameters",
    "already",
    "pretty",
    "good",
    "state",
    "process",
    "relatively",
    "lightweight",
    "compared",
    "full",
    "pretraining",
    "finally",
    "talk",
    "bert",
    "stands",
    "bidirectional",
    "encoded",
    "representations",
    "transformers",
    "one",
    "instantiation",
    "transformer",
    "architecture",
    "bert",
    "trained",
    "google",
    "wikipedia",
    "data",
    "language",
    "modeling",
    "objective",
    "bert",
    "readily",
    "available",
    "download",
    "multiple",
    "platforms",
    "including",
    "github",
    "hugging",
    "face",
    "multiple",
    "models",
    "ones",
    "english",
    "come",
    "different",
    "sizes",
    "smallest",
    "model",
    "two",
    "layers",
    "embedding",
    "size",
    "128",
    "biggest",
    "one",
    "24",
    "layers",
    "embedding",
    "size",
    "quite",
    "huge",
    "range",
    "models",
    "also",
    "model",
    "chinese",
    "model",
    "multilingual",
    "supports",
    "104",
    "languages",
    "multilingual",
    "model",
    "pretty",
    "impressive",
    "feed",
    "input",
    "104",
    "languages",
    "even",
    "specify",
    "language",
    "inputting",
    "language",
    "input",
    "probably",
    "right",
    "thing",
    "use",
    "bert",
    "downstream",
    "task",
    "well",
    "multiple",
    "classes",
    "tasks",
    "comes",
    "special",
    "use",
    "case",
    "simplest",
    "class",
    "tasks",
    "single",
    "sentence",
    "classification",
    "instance",
    "sentiment",
    "classification",
    "given",
    "movie",
    "review",
    "output",
    "label",
    "either",
    "positive",
    "negative",
    "use",
    "bert",
    "stack",
    "remember",
    "first",
    "step",
    "copy",
    "bert",
    "download",
    "made",
    "readily",
    "available",
    "google",
    "add",
    "classifier",
    "top",
    "implementation",
    "details",
    "come",
    "bert",
    "want",
    "dive",
    "look",
    "bottom",
    "diagram",
    "see",
    "addition",
    "input",
    "tokens",
    "single",
    "sentence",
    "inputting",
    "model",
    "special",
    "token",
    "called",
    "cls",
    "stands",
    "classification",
    "special",
    "artificial",
    "token",
    "add",
    "vocabulary",
    "mark",
    "beginning",
    "sentence",
    "rest",
    "stack",
    "works",
    "normal",
    "transformer",
    "encoder",
    "would",
    "bottom",
    "noncontextual",
    "embeddings",
    "top",
    "contextualized",
    "embeddings",
    "exactly",
    "feed",
    "classifier",
    "well",
    "honesty",
    "could",
    "feed",
    "top",
    "embeddings",
    "fully",
    "contextualized",
    "aware",
    "entire",
    "sentence",
    "might",
    "localized",
    "meaning",
    "particular",
    "token",
    "get",
    "around",
    "artifact",
    "simply",
    "take",
    "embedding",
    "cls",
    "token",
    "remember",
    "know",
    "meaning",
    "entire",
    "sentence",
    "top",
    "network",
    "everything",
    "fully",
    "contextualized",
    "particularly",
    "focus",
    "one",
    "word",
    "sentence",
    "feed",
    "embedding",
    "cls",
    "token",
    "classifier",
    "get",
    "prediction",
    "end",
    "sentiment",
    "classification",
    "task",
    "another",
    "class",
    "natural",
    "language",
    "processing",
    "tasks",
    "classification",
    "inputs",
    "pair",
    "sentences",
    "opposed",
    "single",
    "sentence",
    "paraphrased",
    "classification",
    "one",
    "example",
    "given",
    "two",
    "sentences",
    "like",
    "accepted",
    "immediately",
    "woman",
    "hesitate",
    "accept",
    "task",
    "decide",
    "whether",
    "two",
    "sentences",
    "paraphrases",
    "make",
    "copy",
    "bert",
    "model",
    "readily",
    "available",
    "online",
    "add",
    "classifier",
    "top",
    "detail",
    "want",
    "emphasize",
    "fact",
    "special",
    "separator",
    "token",
    "added",
    "two",
    "input",
    "sentences",
    "like",
    "cls",
    "special",
    "token",
    "added",
    "dictionary",
    "purpose",
    "marking",
    "end",
    "one",
    "sentence",
    "beginning",
    "sentence",
    "model",
    "knows",
    "tokens",
    "fall",
    "first",
    "sentence",
    "tokens",
    "fall",
    "sentence",
    "finally",
    "another",
    "class",
    "natural",
    "language",
    "processing",
    "tasks",
    "span",
    "annotation",
    "refers",
    "fact",
    "need",
    "make",
    "classification",
    "decision",
    "every",
    "point",
    "sentence",
    "part",
    "speech",
    "tagging",
    "one",
    "example",
    "sentence",
    "like",
    "accepted",
    "immediately",
    "want",
    "tag",
    "every",
    "single",
    "word",
    "part",
    "speech",
    "pronoun",
    "accepted",
    "verb",
    "immediately",
    "adverb",
    "story",
    "copy",
    "model",
    "readily",
    "available",
    "download",
    "course",
    "going",
    "need",
    "classifier",
    "use",
    "classifier",
    "way",
    "makes",
    "multiple",
    "predictions",
    "well",
    "going",
    "pass",
    "every",
    "embedding",
    "sentence",
    "one",
    "one",
    "classifier",
    "let",
    "say",
    "t1",
    "corresponds",
    "pass",
    "contextualized",
    "embedding",
    "classifier",
    "produce",
    "label",
    "similarly",
    "let",
    "say",
    "t2",
    "corresponds",
    "accepted",
    "pass",
    "contextual",
    "embedding",
    "accepted",
    "classifier",
    "hopefully",
    "get",
    "verb",
    "label",
    "entire",
    "sentence",
    "visited",
    "three",
    "main",
    "categories",
    "downstream",
    "tasks",
    "solved",
    "simply",
    "bert",
    "piece",
    "code",
    "shows",
    "practice",
    "bert",
    "available",
    "multiple",
    "platforms",
    "one",
    "particularly",
    "easy",
    "use",
    "everything",
    "provide",
    "url",
    "model",
    "want",
    "download",
    "going",
    "use",
    "hub",
    "library",
    "going",
    "ask",
    "download",
    "model",
    "specific",
    "location",
    "case",
    "choosing",
    "small",
    "model",
    "want",
    "wait",
    "long",
    "time",
    "get",
    "downloaded",
    "sequence",
    "length",
    "api",
    "library",
    "requires",
    "define",
    "inputs",
    "model",
    "look",
    "like",
    "placeholders",
    "going",
    "feed",
    "input",
    "text",
    "defined",
    "keras",
    "inputs",
    "next",
    "step",
    "extract",
    "cls",
    "embedding",
    "bert",
    "outputs",
    "remember",
    "want",
    "sentiment",
    "classification",
    "need",
    "take",
    "contextual",
    "embedding",
    "cls",
    "token",
    "pass",
    "classifier",
    "take",
    "cls",
    "embedding",
    "define",
    "classifier",
    "case",
    "classifier",
    "dense",
    "keraslayer",
    "two",
    "units",
    "sentiment",
    "classification",
    "task",
    "two",
    "possible",
    "classes",
    "finally",
    "pass",
    "contextual",
    "cls",
    "embedding",
    "classifier",
    "define",
    "model",
    "takes",
    "inputs",
    "whatever",
    "feed",
    "bert",
    "takes",
    "outputs",
    "whatever",
    "classifier",
    "outputs",
    "point",
    "ready",
    "call",
    "train",
    "model",
    "bert",
    "quite",
    "success",
    "story",
    "inspired",
    "quite",
    "bit",
    "work",
    "models",
    "revisit",
    "training",
    "process",
    "make",
    "robust",
    "like",
    "roberta",
    "lot",
    "research",
    "gone",
    "preserving",
    "accuracy",
    "bert",
    "shrinking",
    "making",
    "faster",
    "inference",
    "case",
    "albert",
    "mobilebert",
    "tinybert",
    "lot",
    "models",
    "also",
    "models",
    "trained",
    "specifically",
    "french",
    "chinese",
    "readily",
    "available",
    "downloaded",
    "build",
    "model",
    "simply",
    "concludes",
    "journey",
    "recent",
    "history",
    "natural",
    "language",
    "processing",
    "today",
    "transformers",
    "remain",
    "preferred",
    "architecture",
    "nlp",
    "tasks",
    "pretraining",
    "paradigm",
    "ubiquitous",
    "biggest",
    "roadblock",
    "facing",
    "today",
    "transformers",
    "require",
    "fixed",
    "input",
    "length",
    "computational",
    "complexity",
    "effectively",
    "scaled",
    "beyond",
    "hundreds",
    "tokens",
    "solution",
    "might",
    "come",
    "either",
    "specialized",
    "hardware",
    "innovations",
    "modeling",
    "side",
    "either",
    "way",
    "natural",
    "language",
    "processing",
    "remains",
    "interesting",
    "field",
    "research",
    "industry",
    "applications",
    "thanks",
    "attention",
    "make",
    "sure",
    "check",
    "videos",
    "series",
    "music",
    "playing"
  ],
  "keywords": [
    "work",
    "natural",
    "language",
    "processing",
    "architecture",
    "transformers",
    "also",
    "discuss",
    "particularly",
    "context",
    "transfer",
    "learning",
    "text",
    "three",
    "first",
    "going",
    "representation",
    "machine",
    "models",
    "one",
    "tasks",
    "modeling",
    "see",
    "particular",
    "task",
    "finally",
    "look",
    "bert",
    "transformer",
    "let",
    "start",
    "question",
    "encode",
    "well",
    "currently",
    "nlp",
    "piece",
    "input",
    "model",
    "prediction",
    "way",
    "could",
    "class",
    "value",
    "another",
    "sentiment",
    "classification",
    "common",
    "something",
    "like",
    "tv",
    "show",
    "review",
    "positive",
    "output",
    "probability",
    "distribution",
    "two",
    "negative",
    "part",
    "words",
    "tokens",
    "would",
    "training",
    "build",
    "dictionary",
    "word",
    "aardvark",
    "king",
    "queen",
    "remember",
    "want",
    "thing",
    "token",
    "order",
    "problem",
    "high",
    "use",
    "embeddings",
    "vectors",
    "dimensionality",
    "size",
    "vector",
    "1",
    "others",
    "instance",
    "0",
    "values",
    "main",
    "disadvantages",
    "every",
    "vocabulary",
    "world",
    "knowledge",
    "case",
    "space",
    "embedding",
    "entire",
    "continuous",
    "contains",
    "minus",
    "move",
    "terminology",
    "comes",
    "different",
    "representations",
    "sort",
    "certain",
    "aspects",
    "speech",
    "various",
    "next",
    "course",
    "meaning",
    "distance",
    "far",
    "together",
    "verb",
    "exactly",
    "learn",
    "information",
    "good",
    "even",
    "wikipedia",
    "lot",
    "quite",
    "bit",
    "might",
    "inputs",
    "outputs",
    "given",
    "produce",
    "turn",
    "data",
    "predict",
    "sentence",
    "unstructured",
    "n",
    "previous",
    "based",
    "make",
    "word2vec",
    "river",
    "bank",
    "point",
    "step",
    "matrix",
    "need",
    "understand",
    "fact",
    "account",
    "single",
    "end",
    "score",
    "linear",
    "transformation",
    "using",
    "parameters",
    "trained",
    "scores",
    "sum",
    "come",
    "whether",
    "right",
    "compute",
    "apply",
    "paradigm",
    "multiple",
    "contextualized",
    "rnns",
    "rnn",
    "noncontextual",
    "table",
    "sequential",
    "feed",
    "internal",
    "state",
    "slide",
    "side",
    "time",
    "already",
    "contextual",
    "add",
    "classifier",
    "get",
    "number",
    "process",
    "take",
    "works",
    "special",
    "called",
    "box",
    "attention",
    "weighted",
    "query",
    "key",
    "keys",
    "weight",
    "pretty",
    "put",
    "talked",
    "set",
    "top",
    "though",
    "layer",
    "hardware",
    "stack",
    "layers",
    "encoder",
    "applications",
    "label",
    "readily",
    "available",
    "pretraining",
    "copy",
    "download",
    "cls",
    "sentences",
    "accepted",
    "pass"
  ]
}