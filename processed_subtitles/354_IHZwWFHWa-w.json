{
  "text": "Last video I laid out the structure of a neural network.\nI'll give a quick recap here so that it's fresh in our minds, \nand then I have two main goals for this video.\nThe first is to introduce the idea of gradient descent, \nwhich underlies not only how neural networks learn, \nbut how a lot of other machine learning works as well.\nThen after that we'll dig in a little more into how this particular network performs, \nand what those hidden layers of neurons end up looking for.\nAs a reminder, our goal here is the classic example of handwritten digit recognition, \nthe hello world of neural networks.\nThese digits are rendered on a 28x28 pixel grid, \neach pixel with some grayscale value between 0 and 1.\nThose are what determine the activations of 784 neurons in the input layer of the network.\nAnd then the activation for each neuron in the following layers is based on a weighted \nsum of all the activations in the previous layer, plus some special number called a bias.\nThen you compose that sum with some other function, \nlike the sigmoid squishification, or a relu, the way I walked through last video.\nIn total, given the somewhat arbitrary choice of two hidden layers here with 16 \nneurons each, the network has about 13,000 weights and biases that we can adjust, \nand it's these values that determine what exactly the network actually does.\nThen what we mean when we say that this network classifies a given digit is that \nthe brightest of those 10 neurons in the final layer corresponds to that digit.\nAnd remember, the motivation we had in mind here for the layered \nstructure was that maybe the second layer could pick up on the edges, \nand the third layer might pick up on patterns like loops and lines, \nand the last one could just piece together those patterns to recognize digits.\nSo here, we learn how the network learns.\nWhat we want is an algorithm where you can show this network a whole bunch of \ntraining data, which comes in the form of a bunch of different images of handwritten \ndigits, along with labels for what they're supposed to be, \nand it'll adjust those 13,000 weights and biases so as to improve its performance \non the training data.\nHopefully, this layered structure will mean that what it \nlearns generalizes to images beyond that training data.\nThe way we test that is that after you train the network, \nyou show it more labeled data that it's never seen before, \nand you see how accurately it classifies those new images.\nFortunately for us, and what makes this such a common example to start with, \nis that the good people behind the MNIST database have put together a collection of tens \nof thousands of handwritten digit images, each one labeled with the numbers they're \nsupposed to be.\nAnd as provocative as it is to describe a machine as learning, \nonce you see how it works, it feels a lot less like some crazy sci-fi premise, \nand a lot more like a calculus exercise.\nI mean, basically it comes down to finding the minimum of a certain function.\nRemember, conceptually, we're thinking of each neuron as being connected to \nall the neurons in the previous layer, and the weights in the weighted sum \ndefining its activation are kind of like the strengths of those connections, \nand the bias is some indication of whether that neuron tends to be active or inactive.\nAnd to start things off, we're just going to initialize \nall of those weights and biases totally randomly.\nNeedless to say, this network is going to perform pretty horribly \non a given training example, since it's just doing something random.\nFor example, you feed in this image of a 3, and the output layer just looks like a mess.\nSo what you do is define a cost function, a way of telling the computer, \nno, bad computer, that output should have activations which are 0 for most neurons, \nbut 1 for this neuron, what you gave me is utter trash.\nTo say that a little more mathematically, you add up the squares of the \ndifferences between each of those trash output activations and the value you \nwant them to have, and this is what we'll call the cost of a single training example.\nNotice this sum is small when the network confidently classifies the image correctly, \nbut it's large when the network seems like it doesn't know what it's doing.\nSo then what you do is consider the average cost over all of \nthe tens of thousands of training examples at your disposal.\nThis average cost is our measure for how lousy the network is, \nand how bad the computer should feel.\nAnd that's a complicated thing.\nRemember how the network itself was basically a function, \none that takes in 784 numbers as inputs, the pixel values, \nand spits out 10 numbers as its output, and in a sense it's parameterized \nby all these weights and biases?\nWell the cost function is a layer of complexity on top of that.\nIt takes as its input those 13,000 or so weights and biases, \nand spits out a single number describing how bad those weights and biases are, \nand the way it's defined depends on the network's behavior over all the tens of \nthousands of pieces of training data.\nThat's a lot to think about.\nBut just telling the computer what a crappy job it's doing isn't very helpful.\nYou want to tell it how to change those weights and biases so that it gets better.\nTo make it easier, rather than struggling to imagine a function with 13,000 inputs, \njust imagine a simple function that has one number as an input and one number as an \noutput.\nHow do you find an input that minimizes the value of this function?\nCalculus students will know that you can sometimes figure out that minimum explicitly, \nbut that's not always feasible for really complicated functions, \ncertainly not in the 13,000 input version of this situation for our crazy complicated \nneural network cost function.\nA more flexible tactic is to start at any input, \nand figure out which direction you should step to make that output lower.\nSpecifically, if you can figure out the slope of the function where you are, \nthen shift to the left if that slope is positive, \nand shift the input to the right if that slope is negative.\nIf you do this repeatedly, at each point checking the new slope and taking the \nappropriate step, you're going to approach some local minimum of the function.\nThe image you might have in mind here is a ball rolling down a hill.\nNotice, even for this really simplified single input function, \nthere are many possible valleys that you might land in, \ndepending on which random input you start at, and there's no guarantee \nthat the local minimum you land in is going to be the smallest possible \nvalue of the cost function.\nThat will carry over to our neural network case as well.\nAnd I also want you to notice how if you make your step sizes proportional to the slope, \nthen when the slope is flattening out towards the minimum, \nyour steps get smaller and smaller, and that kind of helps you from overshooting.\nBumping up the complexity a bit, imagine instead \na function with two inputs and one output.\nYou might think of the input space as the xy-plane, \nand the cost function as being graphed as a surface above it.\nInstead of asking about the slope of the function, \nyou have to ask which direction you should step in this input space so as to decrease \nthe output of the function most quickly.\nIn other words, what's the downhill direction?\nAgain, it's helpful to think of a ball rolling down that hill.\nThose of you familiar with multivariable calculus will know that the \ngradient of a function gives you the direction of steepest ascent, \nwhich direction should you step to increase the function most quickly.\nNaturally enough, taking the negative of that gradient gives you \nthe direction to step that decreases the function most quickly.\nEven more than that, the length of this gradient vector \nis an indication for just how steep that steepest slope is.\nIf you're unfamiliar with multivariable calculus and want to learn more, \ncheck out some of the work I did for Khan Academy on the topic.\nHonestly though, all that matters for you and me right now is that \nin principle there exists a way to compute this vector, \nthis vector that tells you what the downhill direction is and how steep it is.\nYou'll be okay if that's all you know and you're not rock solid on the details.\nIf you can get that, the algorithm for minimizing the function is to compute this \ngradient direction, then take a small step downhill, and repeat that over and over.\nIt's the same basic idea for a function that has 13,000 inputs instead of 2 inputs.\nImagine organizing all 13,000 weights and biases \nof our network into a giant column vector.\nThe negative gradient of the cost function is just a vector, \nit's some direction inside this insanely huge input space that tells you which \nnudges to all of those numbers is going to cause the most rapid decrease to \nthe cost function.\nAnd of course, with our specially designed cost function, \nchanging the weights and biases to decrease it means making the \noutput of the network on each piece of training data look less like \na random array of 10 values, and more like an actual decision we want it to make.\nIt's important to remember, this cost function involves an average over all of the \ntraining data, so if you minimize it, it means it's a better performance on all of those \nsamples.\nThe algorithm for computing this gradient efficiently, \nwhich is effectively the heart of how a neural network learns, \nis called backpropagation, and it's what I'm going to be talking about next video.\nThere, I really want to take the time to walk through what exactly happens to \neach weight and bias for a given piece of training data, \ntrying to give an intuitive feel for what's happening beyond the pile of relevant \ncalculus and formulas.\nRight here, right now, the main thing I want you to know, \nindependent of implementation details, is that what we mean when we \ntalk about a network learning is that it's just minimizing a cost function.\nAnd notice, one consequence of that is that it's important for this cost function to have \na nice smooth output, so that we can find a local minimum by taking little steps downhill.\nThis is why, by the way, artificial neurons have continuously ranging activations, \nrather than simply being active or inactive in a binary way, \nthe way biological neurons are.\nThis process of repeatedly nudging an input of a function by \nsome multiple of the negative gradient is called gradient descent.\nIt's a way to converge towards some local minimum of a cost function, \nbasically a valley in this graph.\nI'm still showing the picture of a function with two inputs, of course, \nbecause nudges in a 13,000 dimensional input space are a little hard to \nwrap your mind around, but there is a nice non-spatial way to think about this.\nEach component of the negative gradient tells us two things.\nThe sign, of course, tells us whether the corresponding \ncomponent of the input vector should be nudged up or down.\nBut importantly, the relative magnitudes of all these \ncomponents kind of tells you which changes matter more.\nYou see, in our network, an adjustment to one of the weights might have a much \ngreater impact on the cost function than the adjustment to some other weight.\nSome of these connections just matter more for our training data.\nSo a way you can think about this gradient vector of our mind-warpingly massive \ncost function is that it encodes the relative importance of each weight and bias, \nthat is, which of these changes is going to carry the most bang for your buck.\nThis really is just another way of thinking about direction.\nTo take a simpler example, if you have some function with two variables as an input, \nand you compute that its gradient at some particular point comes out as 3,1, \nthen on the one hand you can interpret that as saying that when you're \nstanding at that input, moving along this direction increases the function most quickly, \nthat when you graph the function above the plane of input points, \nthat vector is what's giving you the straight uphill direction.\nBut another way to read that is to say that changes to this first variable have 3 \ntimes the importance as changes to the second variable, \nthat at least in the neighborhood of the relevant input, \nnudging the x-value carries a lot more bang for your buck.\nLet's zoom out and sum up where we are so far.\nThe network itself is this function with 784 inputs and 10 outputs, \ndefined in terms of all these weighted sums.\nThe cost function is a layer of complexity on top of that.\nIt takes the 13,000 weights and biases as inputs and spits out \na single measure of lousiness based on the training examples.\nAnd the gradient of the cost function is one more layer of complexity still.\nIt tells us what nudges to all these weights and biases cause the \nfastest change to the value of the cost function, \nwhich you might interpret as saying which changes to which weights matter the most.\nSo, when you initialize the network with random weights and biases, \nand adjust them many times based on this gradient descent process, \nhow well does it actually perform on images it's never seen before?\nThe one I've described here, with the two hidden layers of 16 neurons each, \nchosen mostly for aesthetic reasons, is not bad, \nclassifying about 96% of the new images it sees correctly.\nAnd honestly, if you look at some of the examples it messes up on, \nyou feel compelled to cut it a little slack.\nNow if you play around with the hidden layer structure and make a couple tweaks, \nyou can get this up to 98%.\nAnd that's pretty good!\nIt's not the best, you can certainly get better performance by getting more sophisticated \nthan this plain vanilla network, but given how daunting the initial task is, \nI think there's something incredible about any network doing this well on images it's \nnever seen before, given that we never specifically told it what patterns to look for.\nOriginally, the way I motivated this structure was by describing a hope we might have, \nthat the second layer might pick up on little edges, \nthat the third layer would piece together those edges to recognize loops \nand longer lines, and that those might be pieced together to recognize digits.\nSo is this what our network is actually doing?\nWell, for this one at least, not at all.\nRemember how last video we looked at how the weights of the connections from \nall the neurons in the first layer to a given neuron in the second layer can be \nvisualized as a given pixel pattern that the second layer neuron is picking up on?\nWell, when we actually do that for the weights associated with these transitions, \nfrom the first layer to the next, instead of picking up on isolated little edges here \nand there, they look, well, almost random, just with some very loose patterns in the \nmiddle there.\nIt would seem that in the unfathomably large 13,000 dimensional space \nof possible weights and biases, our network found itself a happy \nlittle local minimum that, despite successfully classifying most images, \ndoesn't exactly pick up on the patterns we might have hoped for.\nAnd to really drive this point home, watch what happens when you input a random image.\nIf the system was smart, you might expect it to feel uncertain, \nmaybe not really activating any of those 10 output neurons or activating them all evenly, \nbut instead it confidently gives you some nonsense answer, \nas if it feels as sure that this random noise is a 5 as it does that an actual \nimage of a 5 is a 5.\nPhrased differently, even if this network can recognize digits pretty well, \nit has no idea how to draw them.\nA lot of this is because it's such a tightly constrained training setup.\nI mean, put yourself in the network's shoes here.\nFrom its point of view, the entire universe consists of nothing but clearly \ndefined unmoving digits centered in a tiny grid, \nand its cost function never gave it any incentive to be anything but utterly \nconfident in its decisions.\nSo with this as the image of what those second layer neurons are really doing, \nyou might wonder why I would introduce this network with the \nmotivation of picking up on edges and patterns.\nI mean, that's just not at all what it ends up doing.\nWell, this is not meant to be our end goal, but instead a starting point.\nFrankly, this is old technology, the kind researched in the 80s and 90s, \nand you do need to understand it before you can understand more detailed modern variants, \nand it clearly is capable of solving some interesting problems, \nbut the more you dig into what those hidden layers are really doing, \nthe less intelligent it seems.\nShifting the focus for a moment from how networks learn to how you learn, \nthat'll only happen if you engage actively with the material here somehow.\nOne pretty simple thing I want you to do is just pause right now and think deeply \nfor a moment about what changes you might make to this system and how it perceives \nimages if you wanted it to better pick up on things like edges and patterns.\nBut better than that, to actually engage with the material, \nI highly recommend the book by Michael Nielsen on deep learning and neural networks.\nIn it, you can find the code and the data to download and play with for this exact \nexample, and the book will walk you through step by step what that code is doing.\nWhat's awesome is that this book is free and publicly available, \nso if you do get something out of it, consider joining me in making a donation towards \nNielsen's efforts.\nI've also linked a couple other resources I like a lot in the description, \nincluding the phenomenal and beautiful blog post by Chris Ola and the articles in Distill.\nTo close things off here for the last few minutes, \nI want to jump back into a snippet of the interview I had with Leisha Lee.\nYou might remember her from the last video, she did her PhD work in deep learning.\nIn this little snippet she talks about two recent papers that really dig into \nhow some of the more modern image recognition networks are actually learning.\nJust to set up where we were in the conversation, \nthe first paper took one of these particularly deep neural networks that's really good \nat image recognition, and instead of training it on a properly labeled dataset, \nshuffled all the labels around before training.\nObviously the testing accuracy here was going to be no better than random, \nsince everything's just randomly labeled. But it was still able to achieve \nthe same training accuracy as you would on a properly labeled dataset.\nBasically, the millions of weights for this particular network were \nenough for it to just memorize the random data, \nwhich raises the question for whether minimizing this cost function \nactually corresponds to any sort of structure in the image, or is it just memorization?\n...to memorize the entire dataset of what the correct classification is. \nAnd so a couple of, you know, half a year later at ICML this year, \nthere was not exactly rebuttal paper, but paper that addressed some aspects of like, \nhey, actually these networks are doing something a little bit smarter than that. \nIf you look at that accuracy c\nWhereas if you're actually training on a structured dataset, \none that has the right labels, you fiddle around a little bit in the beginning, \nbut then you kind of dropped very fast to get to that accuracy level, \nand so in some sense it was easier to find that local maxima.\nAnd so what was also interesting about that is it brings into light another paper from \nactually a couple of years ago, which has a lot more simplifications about the network \nlayers, but one of the results was saying how if you look at the optimization landscape, \nthe local minima that these networks tend to learn are actually of equal quality, \nso in some sense if your dataset is structured, \nyou should be able to find that much more easily.\nMy thanks, as always, to those of you supporting on Patreon.\nI've said before just what a game changer Patreon is, \nbut these videos really would not be possible without you.\n301\n00:20:07,125 --> 00:20:06,800\nI also want to give a special thanks to the VC firm Amplify Partners \n302\n00:20:07,460 --> 00:20:07,125\nand their support of these initial videos in the series. Thank you.\n",
  "words": [
    "last",
    "video",
    "laid",
    "structure",
    "neural",
    "network",
    "give",
    "quick",
    "recap",
    "fresh",
    "minds",
    "two",
    "main",
    "goals",
    "video",
    "first",
    "introduce",
    "idea",
    "gradient",
    "descent",
    "underlies",
    "neural",
    "networks",
    "learn",
    "lot",
    "machine",
    "learning",
    "works",
    "well",
    "dig",
    "little",
    "particular",
    "network",
    "performs",
    "hidden",
    "layers",
    "neurons",
    "end",
    "looking",
    "reminder",
    "goal",
    "classic",
    "example",
    "handwritten",
    "digit",
    "recognition",
    "hello",
    "world",
    "neural",
    "networks",
    "digits",
    "rendered",
    "28x28",
    "pixel",
    "grid",
    "pixel",
    "grayscale",
    "value",
    "0",
    "determine",
    "activations",
    "784",
    "neurons",
    "input",
    "layer",
    "network",
    "activation",
    "neuron",
    "following",
    "layers",
    "based",
    "weighted",
    "sum",
    "activations",
    "previous",
    "layer",
    "plus",
    "special",
    "number",
    "called",
    "bias",
    "compose",
    "sum",
    "function",
    "like",
    "sigmoid",
    "squishification",
    "relu",
    "way",
    "walked",
    "last",
    "video",
    "total",
    "given",
    "somewhat",
    "arbitrary",
    "choice",
    "two",
    "hidden",
    "layers",
    "16",
    "neurons",
    "network",
    "weights",
    "biases",
    "adjust",
    "values",
    "determine",
    "exactly",
    "network",
    "actually",
    "mean",
    "say",
    "network",
    "classifies",
    "given",
    "digit",
    "brightest",
    "10",
    "neurons",
    "final",
    "layer",
    "corresponds",
    "digit",
    "remember",
    "motivation",
    "mind",
    "layered",
    "structure",
    "maybe",
    "second",
    "layer",
    "could",
    "pick",
    "edges",
    "third",
    "layer",
    "might",
    "pick",
    "patterns",
    "like",
    "loops",
    "lines",
    "last",
    "one",
    "could",
    "piece",
    "together",
    "patterns",
    "recognize",
    "digits",
    "learn",
    "network",
    "learns",
    "want",
    "algorithm",
    "show",
    "network",
    "whole",
    "bunch",
    "training",
    "data",
    "comes",
    "form",
    "bunch",
    "different",
    "images",
    "handwritten",
    "digits",
    "along",
    "labels",
    "supposed",
    "adjust",
    "weights",
    "biases",
    "improve",
    "performance",
    "training",
    "data",
    "hopefully",
    "layered",
    "structure",
    "mean",
    "learns",
    "generalizes",
    "images",
    "beyond",
    "training",
    "data",
    "way",
    "test",
    "train",
    "network",
    "show",
    "labeled",
    "data",
    "never",
    "seen",
    "see",
    "accurately",
    "classifies",
    "new",
    "images",
    "fortunately",
    "us",
    "makes",
    "common",
    "example",
    "start",
    "good",
    "people",
    "behind",
    "mnist",
    "database",
    "put",
    "together",
    "collection",
    "tens",
    "thousands",
    "handwritten",
    "digit",
    "images",
    "one",
    "labeled",
    "numbers",
    "supposed",
    "provocative",
    "describe",
    "machine",
    "learning",
    "see",
    "works",
    "feels",
    "lot",
    "less",
    "like",
    "crazy",
    "premise",
    "lot",
    "like",
    "calculus",
    "exercise",
    "mean",
    "basically",
    "comes",
    "finding",
    "minimum",
    "certain",
    "function",
    "remember",
    "conceptually",
    "thinking",
    "neuron",
    "connected",
    "neurons",
    "previous",
    "layer",
    "weights",
    "weighted",
    "sum",
    "defining",
    "activation",
    "kind",
    "like",
    "strengths",
    "connections",
    "bias",
    "indication",
    "whether",
    "neuron",
    "tends",
    "active",
    "inactive",
    "start",
    "things",
    "going",
    "initialize",
    "weights",
    "biases",
    "totally",
    "randomly",
    "needless",
    "say",
    "network",
    "going",
    "perform",
    "pretty",
    "horribly",
    "given",
    "training",
    "example",
    "since",
    "something",
    "random",
    "example",
    "feed",
    "image",
    "3",
    "output",
    "layer",
    "looks",
    "like",
    "mess",
    "define",
    "cost",
    "function",
    "way",
    "telling",
    "computer",
    "bad",
    "computer",
    "output",
    "activations",
    "0",
    "neurons",
    "1",
    "neuron",
    "gave",
    "utter",
    "trash",
    "say",
    "little",
    "mathematically",
    "add",
    "squares",
    "differences",
    "trash",
    "output",
    "activations",
    "value",
    "want",
    "call",
    "cost",
    "single",
    "training",
    "example",
    "notice",
    "sum",
    "small",
    "network",
    "confidently",
    "classifies",
    "image",
    "correctly",
    "large",
    "network",
    "seems",
    "like",
    "know",
    "consider",
    "average",
    "cost",
    "tens",
    "thousands",
    "training",
    "examples",
    "disposal",
    "average",
    "cost",
    "measure",
    "lousy",
    "network",
    "bad",
    "computer",
    "feel",
    "complicated",
    "thing",
    "remember",
    "network",
    "basically",
    "function",
    "one",
    "takes",
    "784",
    "numbers",
    "inputs",
    "pixel",
    "values",
    "spits",
    "10",
    "numbers",
    "output",
    "sense",
    "parameterized",
    "weights",
    "biases",
    "well",
    "cost",
    "function",
    "layer",
    "complexity",
    "top",
    "takes",
    "input",
    "weights",
    "biases",
    "spits",
    "single",
    "number",
    "describing",
    "bad",
    "weights",
    "biases",
    "way",
    "defined",
    "depends",
    "network",
    "behavior",
    "tens",
    "thousands",
    "pieces",
    "training",
    "data",
    "lot",
    "think",
    "telling",
    "computer",
    "crappy",
    "job",
    "helpful",
    "want",
    "tell",
    "change",
    "weights",
    "biases",
    "gets",
    "better",
    "make",
    "easier",
    "rather",
    "struggling",
    "imagine",
    "function",
    "inputs",
    "imagine",
    "simple",
    "function",
    "one",
    "number",
    "input",
    "one",
    "number",
    "output",
    "find",
    "input",
    "minimizes",
    "value",
    "function",
    "calculus",
    "students",
    "know",
    "sometimes",
    "figure",
    "minimum",
    "explicitly",
    "always",
    "feasible",
    "really",
    "complicated",
    "functions",
    "certainly",
    "input",
    "version",
    "situation",
    "crazy",
    "complicated",
    "neural",
    "network",
    "cost",
    "function",
    "flexible",
    "tactic",
    "start",
    "input",
    "figure",
    "direction",
    "step",
    "make",
    "output",
    "lower",
    "specifically",
    "figure",
    "slope",
    "function",
    "shift",
    "left",
    "slope",
    "positive",
    "shift",
    "input",
    "right",
    "slope",
    "negative",
    "repeatedly",
    "point",
    "checking",
    "new",
    "slope",
    "taking",
    "appropriate",
    "step",
    "going",
    "approach",
    "local",
    "minimum",
    "function",
    "image",
    "might",
    "mind",
    "ball",
    "rolling",
    "hill",
    "notice",
    "even",
    "really",
    "simplified",
    "single",
    "input",
    "function",
    "many",
    "possible",
    "valleys",
    "might",
    "land",
    "depending",
    "random",
    "input",
    "start",
    "guarantee",
    "local",
    "minimum",
    "land",
    "going",
    "smallest",
    "possible",
    "value",
    "cost",
    "function",
    "carry",
    "neural",
    "network",
    "case",
    "well",
    "also",
    "want",
    "notice",
    "make",
    "step",
    "sizes",
    "proportional",
    "slope",
    "slope",
    "flattening",
    "towards",
    "minimum",
    "steps",
    "get",
    "smaller",
    "smaller",
    "kind",
    "helps",
    "overshooting",
    "bumping",
    "complexity",
    "bit",
    "imagine",
    "instead",
    "function",
    "two",
    "inputs",
    "one",
    "output",
    "might",
    "think",
    "input",
    "space",
    "cost",
    "function",
    "graphed",
    "surface",
    "instead",
    "asking",
    "slope",
    "function",
    "ask",
    "direction",
    "step",
    "input",
    "space",
    "decrease",
    "output",
    "function",
    "quickly",
    "words",
    "downhill",
    "direction",
    "helpful",
    "think",
    "ball",
    "rolling",
    "hill",
    "familiar",
    "multivariable",
    "calculus",
    "know",
    "gradient",
    "function",
    "gives",
    "direction",
    "steepest",
    "ascent",
    "direction",
    "step",
    "increase",
    "function",
    "quickly",
    "naturally",
    "enough",
    "taking",
    "negative",
    "gradient",
    "gives",
    "direction",
    "step",
    "decreases",
    "function",
    "quickly",
    "even",
    "length",
    "gradient",
    "vector",
    "indication",
    "steep",
    "steepest",
    "slope",
    "unfamiliar",
    "multivariable",
    "calculus",
    "want",
    "learn",
    "check",
    "work",
    "khan",
    "academy",
    "topic",
    "honestly",
    "though",
    "matters",
    "right",
    "principle",
    "exists",
    "way",
    "compute",
    "vector",
    "vector",
    "tells",
    "downhill",
    "direction",
    "steep",
    "okay",
    "know",
    "rock",
    "solid",
    "details",
    "get",
    "algorithm",
    "minimizing",
    "function",
    "compute",
    "gradient",
    "direction",
    "take",
    "small",
    "step",
    "downhill",
    "repeat",
    "basic",
    "idea",
    "function",
    "inputs",
    "instead",
    "2",
    "inputs",
    "imagine",
    "organizing",
    "weights",
    "biases",
    "network",
    "giant",
    "column",
    "vector",
    "negative",
    "gradient",
    "cost",
    "function",
    "vector",
    "direction",
    "inside",
    "insanely",
    "huge",
    "input",
    "space",
    "tells",
    "nudges",
    "numbers",
    "going",
    "cause",
    "rapid",
    "decrease",
    "cost",
    "function",
    "course",
    "specially",
    "designed",
    "cost",
    "function",
    "changing",
    "weights",
    "biases",
    "decrease",
    "means",
    "making",
    "output",
    "network",
    "piece",
    "training",
    "data",
    "look",
    "less",
    "like",
    "random",
    "array",
    "10",
    "values",
    "like",
    "actual",
    "decision",
    "want",
    "make",
    "important",
    "remember",
    "cost",
    "function",
    "involves",
    "average",
    "training",
    "data",
    "minimize",
    "means",
    "better",
    "performance",
    "samples",
    "algorithm",
    "computing",
    "gradient",
    "efficiently",
    "effectively",
    "heart",
    "neural",
    "network",
    "learns",
    "called",
    "backpropagation",
    "going",
    "talking",
    "next",
    "video",
    "really",
    "want",
    "take",
    "time",
    "walk",
    "exactly",
    "happens",
    "weight",
    "bias",
    "given",
    "piece",
    "training",
    "data",
    "trying",
    "give",
    "intuitive",
    "feel",
    "happening",
    "beyond",
    "pile",
    "relevant",
    "calculus",
    "formulas",
    "right",
    "right",
    "main",
    "thing",
    "want",
    "know",
    "independent",
    "implementation",
    "details",
    "mean",
    "talk",
    "network",
    "learning",
    "minimizing",
    "cost",
    "function",
    "notice",
    "one",
    "consequence",
    "important",
    "cost",
    "function",
    "nice",
    "smooth",
    "output",
    "find",
    "local",
    "minimum",
    "taking",
    "little",
    "steps",
    "downhill",
    "way",
    "artificial",
    "neurons",
    "continuously",
    "ranging",
    "activations",
    "rather",
    "simply",
    "active",
    "inactive",
    "binary",
    "way",
    "way",
    "biological",
    "neurons",
    "process",
    "repeatedly",
    "nudging",
    "input",
    "function",
    "multiple",
    "negative",
    "gradient",
    "called",
    "gradient",
    "descent",
    "way",
    "converge",
    "towards",
    "local",
    "minimum",
    "cost",
    "function",
    "basically",
    "valley",
    "graph",
    "still",
    "showing",
    "picture",
    "function",
    "two",
    "inputs",
    "course",
    "nudges",
    "dimensional",
    "input",
    "space",
    "little",
    "hard",
    "wrap",
    "mind",
    "around",
    "nice",
    "way",
    "think",
    "component",
    "negative",
    "gradient",
    "tells",
    "us",
    "two",
    "things",
    "sign",
    "course",
    "tells",
    "us",
    "whether",
    "corresponding",
    "component",
    "input",
    "vector",
    "nudged",
    "importantly",
    "relative",
    "magnitudes",
    "components",
    "kind",
    "tells",
    "changes",
    "matter",
    "see",
    "network",
    "adjustment",
    "one",
    "weights",
    "might",
    "much",
    "greater",
    "impact",
    "cost",
    "function",
    "adjustment",
    "weight",
    "connections",
    "matter",
    "training",
    "data",
    "way",
    "think",
    "gradient",
    "vector",
    "massive",
    "cost",
    "function",
    "encodes",
    "relative",
    "importance",
    "weight",
    "bias",
    "changes",
    "going",
    "carry",
    "bang",
    "buck",
    "really",
    "another",
    "way",
    "thinking",
    "direction",
    "take",
    "simpler",
    "example",
    "function",
    "two",
    "variables",
    "input",
    "compute",
    "gradient",
    "particular",
    "point",
    "comes",
    "one",
    "hand",
    "interpret",
    "saying",
    "standing",
    "input",
    "moving",
    "along",
    "direction",
    "increases",
    "function",
    "quickly",
    "graph",
    "function",
    "plane",
    "input",
    "points",
    "vector",
    "giving",
    "straight",
    "uphill",
    "direction",
    "another",
    "way",
    "read",
    "say",
    "changes",
    "first",
    "variable",
    "3",
    "times",
    "importance",
    "changes",
    "second",
    "variable",
    "least",
    "neighborhood",
    "relevant",
    "input",
    "nudging",
    "carries",
    "lot",
    "bang",
    "buck",
    "let",
    "zoom",
    "sum",
    "far",
    "network",
    "function",
    "784",
    "inputs",
    "10",
    "outputs",
    "defined",
    "terms",
    "weighted",
    "sums",
    "cost",
    "function",
    "layer",
    "complexity",
    "top",
    "takes",
    "weights",
    "biases",
    "inputs",
    "spits",
    "single",
    "measure",
    "lousiness",
    "based",
    "training",
    "examples",
    "gradient",
    "cost",
    "function",
    "one",
    "layer",
    "complexity",
    "still",
    "tells",
    "us",
    "nudges",
    "weights",
    "biases",
    "cause",
    "fastest",
    "change",
    "value",
    "cost",
    "function",
    "might",
    "interpret",
    "saying",
    "changes",
    "weights",
    "matter",
    "initialize",
    "network",
    "random",
    "weights",
    "biases",
    "adjust",
    "many",
    "times",
    "based",
    "gradient",
    "descent",
    "process",
    "well",
    "actually",
    "perform",
    "images",
    "never",
    "seen",
    "one",
    "described",
    "two",
    "hidden",
    "layers",
    "16",
    "neurons",
    "chosen",
    "mostly",
    "aesthetic",
    "reasons",
    "bad",
    "classifying",
    "96",
    "new",
    "images",
    "sees",
    "correctly",
    "honestly",
    "look",
    "examples",
    "messes",
    "feel",
    "compelled",
    "cut",
    "little",
    "slack",
    "play",
    "around",
    "hidden",
    "layer",
    "structure",
    "make",
    "couple",
    "tweaks",
    "get",
    "98",
    "pretty",
    "good",
    "best",
    "certainly",
    "get",
    "better",
    "performance",
    "getting",
    "sophisticated",
    "plain",
    "vanilla",
    "network",
    "given",
    "daunting",
    "initial",
    "task",
    "think",
    "something",
    "incredible",
    "network",
    "well",
    "images",
    "never",
    "seen",
    "given",
    "never",
    "specifically",
    "told",
    "patterns",
    "look",
    "originally",
    "way",
    "motivated",
    "structure",
    "describing",
    "hope",
    "might",
    "second",
    "layer",
    "might",
    "pick",
    "little",
    "edges",
    "third",
    "layer",
    "would",
    "piece",
    "together",
    "edges",
    "recognize",
    "loops",
    "longer",
    "lines",
    "might",
    "pieced",
    "together",
    "recognize",
    "digits",
    "network",
    "actually",
    "well",
    "one",
    "least",
    "remember",
    "last",
    "video",
    "looked",
    "weights",
    "connections",
    "neurons",
    "first",
    "layer",
    "given",
    "neuron",
    "second",
    "layer",
    "visualized",
    "given",
    "pixel",
    "pattern",
    "second",
    "layer",
    "neuron",
    "picking",
    "well",
    "actually",
    "weights",
    "associated",
    "transitions",
    "first",
    "layer",
    "next",
    "instead",
    "picking",
    "isolated",
    "little",
    "edges",
    "look",
    "well",
    "almost",
    "random",
    "loose",
    "patterns",
    "middle",
    "would",
    "seem",
    "unfathomably",
    "large",
    "dimensional",
    "space",
    "possible",
    "weights",
    "biases",
    "network",
    "found",
    "happy",
    "little",
    "local",
    "minimum",
    "despite",
    "successfully",
    "classifying",
    "images",
    "exactly",
    "pick",
    "patterns",
    "might",
    "hoped",
    "really",
    "drive",
    "point",
    "home",
    "watch",
    "happens",
    "input",
    "random",
    "image",
    "system",
    "smart",
    "might",
    "expect",
    "feel",
    "uncertain",
    "maybe",
    "really",
    "activating",
    "10",
    "output",
    "neurons",
    "activating",
    "evenly",
    "instead",
    "confidently",
    "gives",
    "nonsense",
    "answer",
    "feels",
    "sure",
    "random",
    "noise",
    "5",
    "actual",
    "image",
    "5",
    "phrased",
    "differently",
    "even",
    "network",
    "recognize",
    "digits",
    "pretty",
    "well",
    "idea",
    "draw",
    "lot",
    "tightly",
    "constrained",
    "training",
    "setup",
    "mean",
    "put",
    "network",
    "shoes",
    "point",
    "view",
    "entire",
    "universe",
    "consists",
    "nothing",
    "clearly",
    "defined",
    "unmoving",
    "digits",
    "centered",
    "tiny",
    "grid",
    "cost",
    "function",
    "never",
    "gave",
    "incentive",
    "anything",
    "utterly",
    "confident",
    "decisions",
    "image",
    "second",
    "layer",
    "neurons",
    "really",
    "might",
    "wonder",
    "would",
    "introduce",
    "network",
    "motivation",
    "picking",
    "edges",
    "patterns",
    "mean",
    "ends",
    "well",
    "meant",
    "end",
    "goal",
    "instead",
    "starting",
    "point",
    "frankly",
    "old",
    "technology",
    "kind",
    "researched",
    "80s",
    "90s",
    "need",
    "understand",
    "understand",
    "detailed",
    "modern",
    "variants",
    "clearly",
    "capable",
    "solving",
    "interesting",
    "problems",
    "dig",
    "hidden",
    "layers",
    "really",
    "less",
    "intelligent",
    "seems",
    "shifting",
    "focus",
    "moment",
    "networks",
    "learn",
    "learn",
    "happen",
    "engage",
    "actively",
    "material",
    "somehow",
    "one",
    "pretty",
    "simple",
    "thing",
    "want",
    "pause",
    "right",
    "think",
    "deeply",
    "moment",
    "changes",
    "might",
    "make",
    "system",
    "perceives",
    "images",
    "wanted",
    "better",
    "pick",
    "things",
    "like",
    "edges",
    "patterns",
    "better",
    "actually",
    "engage",
    "material",
    "highly",
    "recommend",
    "book",
    "michael",
    "nielsen",
    "deep",
    "learning",
    "neural",
    "networks",
    "find",
    "code",
    "data",
    "download",
    "play",
    "exact",
    "example",
    "book",
    "walk",
    "step",
    "step",
    "code",
    "awesome",
    "book",
    "free",
    "publicly",
    "available",
    "get",
    "something",
    "consider",
    "joining",
    "making",
    "donation",
    "towards",
    "nielsen",
    "efforts",
    "also",
    "linked",
    "couple",
    "resources",
    "like",
    "lot",
    "description",
    "including",
    "phenomenal",
    "beautiful",
    "blog",
    "post",
    "chris",
    "ola",
    "articles",
    "distill",
    "close",
    "things",
    "last",
    "minutes",
    "want",
    "jump",
    "back",
    "snippet",
    "interview",
    "leisha",
    "lee",
    "might",
    "remember",
    "last",
    "video",
    "phd",
    "work",
    "deep",
    "learning",
    "little",
    "snippet",
    "talks",
    "two",
    "recent",
    "papers",
    "really",
    "dig",
    "modern",
    "image",
    "recognition",
    "networks",
    "actually",
    "learning",
    "set",
    "conversation",
    "first",
    "paper",
    "took",
    "one",
    "particularly",
    "deep",
    "neural",
    "networks",
    "really",
    "good",
    "image",
    "recognition",
    "instead",
    "training",
    "properly",
    "labeled",
    "dataset",
    "shuffled",
    "labels",
    "around",
    "training",
    "obviously",
    "testing",
    "accuracy",
    "going",
    "better",
    "random",
    "since",
    "everything",
    "randomly",
    "labeled",
    "still",
    "able",
    "achieve",
    "training",
    "accuracy",
    "would",
    "properly",
    "labeled",
    "dataset",
    "basically",
    "millions",
    "weights",
    "particular",
    "network",
    "enough",
    "memorize",
    "random",
    "data",
    "raises",
    "question",
    "whether",
    "minimizing",
    "cost",
    "function",
    "actually",
    "corresponds",
    "sort",
    "structure",
    "image",
    "memorization",
    "memorize",
    "entire",
    "dataset",
    "correct",
    "classification",
    "couple",
    "know",
    "half",
    "year",
    "later",
    "icml",
    "year",
    "exactly",
    "rebuttal",
    "paper",
    "paper",
    "addressed",
    "aspects",
    "like",
    "hey",
    "actually",
    "networks",
    "something",
    "little",
    "bit",
    "smarter",
    "look",
    "accuracy",
    "c",
    "whereas",
    "actually",
    "training",
    "structured",
    "dataset",
    "one",
    "right",
    "labels",
    "fiddle",
    "around",
    "little",
    "bit",
    "beginning",
    "kind",
    "dropped",
    "fast",
    "get",
    "accuracy",
    "level",
    "sense",
    "easier",
    "find",
    "local",
    "maxima",
    "also",
    "interesting",
    "brings",
    "light",
    "another",
    "paper",
    "actually",
    "couple",
    "years",
    "ago",
    "lot",
    "simplifications",
    "network",
    "layers",
    "one",
    "results",
    "saying",
    "look",
    "optimization",
    "landscape",
    "local",
    "minima",
    "networks",
    "tend",
    "learn",
    "actually",
    "equal",
    "quality",
    "sense",
    "dataset",
    "structured",
    "able",
    "find",
    "much",
    "easily",
    "thanks",
    "always",
    "supporting",
    "patreon",
    "said",
    "game",
    "changer",
    "patreon",
    "videos",
    "really",
    "would",
    "possible",
    "without",
    "301",
    "also",
    "want",
    "give",
    "special",
    "thanks",
    "vc",
    "firm",
    "amplify",
    "partners",
    "302",
    "support",
    "initial",
    "videos",
    "series",
    "thank"
  ],
  "keywords": [
    "last",
    "video",
    "structure",
    "neural",
    "network",
    "give",
    "two",
    "first",
    "idea",
    "gradient",
    "descent",
    "networks",
    "learn",
    "lot",
    "learning",
    "well",
    "dig",
    "little",
    "particular",
    "hidden",
    "layers",
    "neurons",
    "example",
    "handwritten",
    "digit",
    "recognition",
    "digits",
    "pixel",
    "value",
    "activations",
    "784",
    "input",
    "layer",
    "neuron",
    "based",
    "weighted",
    "sum",
    "number",
    "called",
    "bias",
    "function",
    "like",
    "way",
    "given",
    "weights",
    "biases",
    "adjust",
    "values",
    "exactly",
    "actually",
    "mean",
    "say",
    "classifies",
    "10",
    "remember",
    "mind",
    "second",
    "pick",
    "edges",
    "might",
    "patterns",
    "one",
    "piece",
    "together",
    "recognize",
    "learns",
    "want",
    "algorithm",
    "training",
    "data",
    "comes",
    "images",
    "labels",
    "performance",
    "labeled",
    "never",
    "seen",
    "see",
    "new",
    "us",
    "start",
    "good",
    "tens",
    "thousands",
    "numbers",
    "less",
    "calculus",
    "basically",
    "minimum",
    "kind",
    "connections",
    "whether",
    "things",
    "going",
    "pretty",
    "something",
    "random",
    "image",
    "output",
    "cost",
    "computer",
    "bad",
    "single",
    "notice",
    "know",
    "average",
    "examples",
    "feel",
    "complicated",
    "thing",
    "takes",
    "inputs",
    "spits",
    "sense",
    "complexity",
    "defined",
    "think",
    "better",
    "make",
    "imagine",
    "find",
    "figure",
    "really",
    "direction",
    "step",
    "slope",
    "right",
    "negative",
    "point",
    "taking",
    "local",
    "even",
    "possible",
    "also",
    "towards",
    "get",
    "bit",
    "instead",
    "space",
    "decrease",
    "quickly",
    "downhill",
    "gives",
    "vector",
    "compute",
    "tells",
    "minimizing",
    "take",
    "nudges",
    "course",
    "look",
    "weight",
    "still",
    "around",
    "changes",
    "matter",
    "another",
    "saying",
    "couple",
    "would",
    "picking",
    "book",
    "deep",
    "paper",
    "dataset",
    "accuracy"
  ]
}