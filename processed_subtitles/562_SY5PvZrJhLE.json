{
  "text": "hello there today we're looking at\nlanguage models are few shot learners by\nTom B brown Benjamin man Nick Ryder and\nMelanie sabaha and a whole of authors\nall slew of authors from OPI this paper\nalso called GPT three just came out\nrecently so GPT three is a model that is\na language model and it comes out of a\nsuccession of language models of open e\nI this paper is basically an\ninvestigation into what you can do with\ngiant language models now this language\nmodel is an order of magnitude larger\nthan anyone has ever built a language\nmodel and it can do some absolutely\ncrazy things so we'll basically go over\nthe architecture over what the model\ndoes and over the experimental results\nit turns out that if you train a\nlanguage model on enough data it is able\nto solve NLP tasks that it has never\nseen just out of the box and we're gonna\nlook into this very cool kind of\nformulation of the problem as you can\nsee here the paper is 40 pages long\nwithout the appendix it needs its own\ntable of contents which is crazy so\nwe're going to skip a fair bit of things\nso first of all what is a language model\nfor those of you don't know I've done a\nbunch of videos and you can see those in\nmy natural language processing playlist\nabout language models and specifically\nabout transformer language models so a\nlanguage model let's just take an\nexample this sentence right here just\nthe sentence as such like third humans\ndo not require to do not require large\nsupervised data sets to learn most\nlanguage tasks right this is an English\nsentence and a language model would be a\nmodel that if you cross out a portion\nfrom the end here like this right here\nit would be able to tell you what comes\nnext so in a language model you would\ninput this part right here and it will\ntell you the next word is data sets so\nthat's basically all the language model\ndoes and once you've trained one you can\nbe\nbasically generate word after word after\nword from it or you can ask it a\nquestion like which word is most likely\nto come next or more likely so a\nlanguage model is nothing but a model\nthat can kind of generate language in a\nprobabilistic way and the cool thing\nabout language models is that you can\ntrain it on any sort of text data and\nthat's what they do here so they train a\nlanguage model on giant amounts of data\nspecifically right here they go into the\ndata sets they use they use this let's\nskip down they use this common crawl\ndata set which they filter down for\nquality and this is basically a crawl of\nthe entire Internet if you will together\nwith these books data sets and the web\ntext data set and the Wikipedia data set\nso if they throw all of this text that\nthey scrape from the internet together\nand then train a language model on that\nnow the the the the language model right\nhere is called GPT three and they train\nvarious sizes of it and we'll get into\nhow it's built in a second but just\ncompare this to a language model like\nBurt Burt required this much flops to\nTrain and these this is a log scale so\nthis is right here this is several\norders of magnitude larger and bigger\nmodel and is trained for way longer on\nthis text so naturally it is going to be\na lot better at language modeling you\ncan see right here the size of these\nmodels that they trained on remember the\nprevious largest language model the\nTuring nlg of Microsoft had something\nlike 17 billion parameters so it would\nbe comparable to this right here whereas\nGPT 3 has 175 billion parameters which\nthis is absolutely crazy is an order of\nmagnitude higher than anything that ever\nexisted and if you look at the last GPT\nthe GPT\nto model that if you remember I've made\na video about it is too dangerous to be\nreleased well now it has been released\nbut was too dangerous to be released it\nclocked in at about 1.5 billion\nparameters so compared to this GPT three\nExcel model right here they trained\nthese multiple models to basically\nestimate the effect of the model size\nand you can see here the largest model\nhas ninety-six attention layers it each\nlayer has 96 attention heads and each\nhead is 128 dimensional and it trains on\nbatches of size 3.2 million this is the\nbatch size absolutely crazy so they\ntrain this on a giant distributed\ncluster that apparently is provided by\nMicrosoft and yes crazy crazy things so\nhow does this model look this model is a\ntransformer model and right here we\ndon't even have like a description of a\ntransformer model let's just assume you\nknow what that is I have made several\nvideos on transformer models and\nespecially things like attention is all\nyou need or Burt or something like this\nbut for those who don't know if I have a\ntransformer model and I want to build a\nlanguage model from it let's take this\nsentence right here\nI would input a what's called a context\nwhich is the thing I already have right\nI would input that into a transformer\nmodel and a transformer model is just\nseveral layers of attention mechanism\nnow an attention mechanism is basically\na way where information is routed in\nbetween the different tokens right here\nand as it goes up the layer basically\nthe the information is routed around and\nthe model can make various inferences\nand at the end the model is supposed to\ncome up with the next word that you're\ngoing to put here specifically in this\npaper they use sub words like word piece\ntokens like it is common in NLP right\nnow but essentially this is an auto\nregressive language model so it's not\nlike Bert it's not by direction\nit is autoregressive it goes from left\nto right always produces the next word\nit is like GPT - they even say this they\nsay we use the same model and\narchitecture as GPT - they just have\nmore layers and wider layers and more\ndata to train it on so how do they train\nit\nokaythat's we already said they train it\nin simply in simply a language modeling\nway just next word prediction that's it\nokay it's so it's not even something\nfancy like Bert the interesting part is\nwhen you do the now the single tasks so\nwhat you usually did with something like\nBert so with something like Bert you\nwould do first pre train so there you\nwould this is the language modeling\nright here this pre training phase where\nyou teach Bert about the English\nlanguage by just feeding it a lot of\ndata and then second you had a step\ncalled fine tuning fine I can't even\nwrite tuning so on the second one you'd\nhave something like the task you're\nactually interested in and let's say the\ntask you're actually interested in is\nsentiment classification so in sentiment\nclassification you have like a sentence\nlike blah blah blah and you want to know\nis that a positive sentiment like is a\nhappy sentence or is it a sad sentence\nand you would have a database of labeled\ninstances of that so in this database\nyou'd have a bunch of sentences and for\neach one you would know is it good is it\nis it positive or is it negative and\nthen you'd have like a smaller test set\nright here and you would you would train\nyou would basically take this pre\ntrained model train it on this dataset\nin a supervised machine learning way and\nthen test it on this test set right here\nthis is called fine tuning that's what\nthey display here so in fine tuning the\nmodel is trained via repeated gradient\nupdates using a large corpus of example\ntasks\nright so the example task right here\ncould be translating to French so in\nyour training database of the\ntranslation task would be this would be\nC order is called Lu treadmill and in\nand and then you'd actually change your\nmodel you'd do a gradient update I mean\nif if you're in the NLP world this seems\nvery natural but they are going to argue\nin a second that this isn't the only way\nthat you can teach a model a task right\nso this this seems very natural right\nyou don't change your model you take\nyour pre trained model and you're going\nto fine-tune it on this task and if you\nhave a different task right if you have\nnow question answering tasks you're\ngoing to have a different data set right\nhere with a train and test data set and\nyou're going to take the pre trained\nmodel and then fine-tune it on that data\nset and evaluate it on that test set so\nthis gives you basically with as many\nmodels as you have tasks Andy for each\none you need a big big training data set\nin order to perform well sometimes we\nhave this sometimes we don't what they\nare interested in is basically to take\nthe pre trained model and directly go\nand evaluate it on the test data set in\na sort of a zero shot fashion now it is\nnot zero shot as they will argue so what\nare they doing in a true zero shot\nfashion you would just take your your\nlanguage model that you pre trained and\nyou just input the following text you\ninput what they call a task description\nand a prompt so this is the input and\nyou were simply asked the model as a\nlanguage model to predict the next word\nit's just what comes here now what\nyou're counting on is basically that in\nthe training data the model has seen a\nstructure like this enough to understand\nwhat's going on\nso that in the training data somewhere\nin the internet there was the structure\nof translate something to something and\nthen there would be a word here of\nsomething and you know it kind of has to\nrealize that this goes here like\nthe next word so basically what you're\nasking it is if you were to find this\ntext on a website or on Wikipedia or in\nany of the books data set if you were to\nfind this piece of text what would be\nthe next word in that piece of text and\nyou kind of hope that this this is\nenough if you've trained a good language\nmodel that this is enough to to to\nactually produce the French translation\nhere now before I realize I've said the\nlanguage modeling is to teach the model\nthe English language actually not true\nin this common crawl corpus you also\nhave many foreign languages so you\nbasically teach you the general model of\nthe internet now they translate they\ncontrast this to what they call one-shot\nlearning so in one-shot learning you not\nonly do you have the task description\nright here and this is this is a string\nright you don't specifically tell the\nmodel that this is now a translation\ntask you simply input this as a string\nso not only do you have the task\ndescription and the prompt right here\nbut you also have one example and the\nexample and this is where they this is\nwhere they bring in the where they say\nit's not exactly zero shot where's my\nlittle drawing here so the example is\ngoing to come from the training data set\nof the task that you're interested in\nbut the important part is you never\ntrain on it\nyou never explicitly train on that\nexample you simply put it in the context\nso you simply put this string so\ntranslate English to French newline C\norder lute is Lu to the mere newline\ncheese is what you simply input that\nstring into the model as a language\nmodel and you ask it what's the next\nword right here okay so I hope I hope\nthis is clear this is what they call\nkind of one-shot generalization and by\none-shot they basically mean you simply\nprovide this thing in the\ntexts of the model as a language model\nnow the the advantage here is\nimmediately clear that you only have to\ntrain one model then and then basically\nat inference time you can just input the\ntask description and the sort of\ntraining data for the task into its its\nevaluation context and the task itself\nand it will if if it is if it really\ndoes what they claim it does it would be\nable to sort of understand the prompt\nhere understand what it means to\ntranslate from English to French it\nwould look at this example and say oh\nthat's what you want me to do okay and\nthen it would be able to generalize to\nthis input right here to say ah okay\nfrom the task description and the\nexample I saw I get I get what you want\nme to do I will the next word here is\ncheese what's cheese in French I don't\nremember homage homage now the way the\nlanguage model is going to interpret\nthat is slightly different as we said\nbefore the way the language model is\ngoing to interpret is if you were to\nfind the following text on a website\nsomewhere the text is called translate\nEnglish to French new line C order goes\nto Luton a new line cheese goes to what\nwould be the next word on that website\nso that's what the model sees right you\nhave to differentiate between what the\nhuman wants and what the model sees the\nmodel is just a language model that is\ngoing to take the next that it's just\ngoing to determine if I were to see this\ntext somewhere what will be the most\nlikely next word so you have to phrase\nyour tasks in a way that makes sense in\nthat thing and they also have this few\nshort thing where you not only provide\none context but you provide a bunch of\ncontext to basically tell the model more\nof what it what it should do now this\ndoesn't only work in a free mode where\nyou basically say what's the next word\nhere what you can also do if you have\nsuch a language hold with the exact same\nmodel you can give it basically a\na couple of possibilities so you can\ngive it it's you can say like it's\neither shop or its format or its hotel I\nthink that has like this so you can you\ncan basically restrict it to only\nproduce one of these three things\nso in translation it might not be you\nknow the way to go but in if you have\nlike yes/no answers questions you can\nrestrict it to that so in a lot of these\nNLP tasks you have some options given\nfor a given question and you can also\nrestrict it so don't you know you always\nhave to go with the task at hand but\nthis is in essence what the model does\nand this is I think this is the new well\nnot the new per se but this is one of\nthe core ideas of this paper if you take\nanything from it there's no new\narchitecture right here there's no new\nwisdom in training they train in a\nstandard way in a standard language\nmodeling fashion a standard transformer\narchitecture this just happens to be\nginormous okay this right here this\nthing where they say most of these\nthings would fine tune and then\nbasically end up with one model per task\nand you need a big data set per task but\nwe simply can do this since we have such\na large language model it is basically\nalready basically already knows how to\ndo this tasks as long as we formulate\nthem in a language model way we can have\nthe model perform these tasks and they\nwill show that this works surprisingly\nwell throughout this paper now we get\ninto the experimental results right here\nand the experimental results first of\nall on language modeling as you can see\nhere they basically say as you go up\nwith the parameters you see the Moriya\nones are the parameters you go into your\nvalidation loss goes down and down and\ndown and down and I believe this is sort\nof a log scale as well so this is the\nlog probability so the the perplexity\nand that the this basically follows a\ntrend\nthis is a log scale this this is a log\nscale it follows a trend where as you\nscale up the model and as you scale up\nthe compute that the model gets and we\nknow for these big language models we\nbasically know you have to scale up\nmodel size compute time and dataset size\nin the same fashion for them to make\nthese gains but if you do that it\nfollows like a a power law where as you\nscale up these things the model\nbasically gets better and better and\nbetter and the question of course is you\nknow how far how far can we go with this\nbut for now it seems to hold quite well\nthat you can just make improvements by\nscaling up your model on language\nmodeling at least so where do we where\ndo we basically go from here so before\nwe dive into the actual results of the\nindividual task so now they're going to\nformulate these individual tasks so they\nhave like pure language modeling tasks\nright here like Alice was friends with\nBob Alice went to visit her friend and\nthen it's like what's the next word okay\nit's Bob and George bought some baseball\nequipment a ball a glove and a what's\nthe next word and I guess this should be\nhat that's re bat right here but we're\ngoing to go into the into the tasks and\none of them is for example question\nanswering\nso in question answering you simply get\neither you get just a pure question or a\ncontext and a question and they do the\nfact that they test where a situation\nwhere you just get the question so you\njust get I don't know who is the Queen\nof England or something like this and\nthe model is simply to produce either\nthe results direct or to choose from a\nbunch of answers which one is the most\nlikely as a language model and as you\ncan see as you scale up the language\nmodel the zero shot one shot and few\nshot predictions so in few shot you give\n64 different examples from the training\nset in the context so you always have so\nyour context is going to look something\nlike this and they have examples at the\nbottom and I haven't looked at the QA\ntask but the the example is going to be\nsomething like this you have a task\ndescription like answer the following\nquestions answer the question and then\nyou have your example so in zero shot\nthat's zero and one shot it's one that's\nwhat I like and then you say how tall\nwho sorry who I don't know who climbed\nEverest the first the rest the first and\nthen you say Hillary I think it was\nHillary no I don't remember and then you\nsay I don't know how how tall is the\nEmpire State Building and then you have\nlike some number here and at the end you\nsay what was was it was a question from\nbefore I don't know who is the queen of\nEngland yeah who is the queen of England\nand then you ask the model to predict\nthe next word right here okay and you do\nthis in a closed book setting which\nmeans you have no access to Wikipedia or\nwhatever like usually these systems they\ncan go and query Wikipedia but this\nsystem doesn't so you just you just want\nto know what has the model learned about\nthe world by simply absorbing giant\namounts of text so if somewhere in the\ntraining data the fact that the Queen of\nEngland is Elizabeth the second is\npresent it should complete this right\nhere and it performs surprisingly well\nas you can see here so it manages to\noutperform a fine-tuned state-of-the-art\nmodel that is that is fine-tuned on\nquestion answering right this has it has\nbeen built for question answering and\nthis model outperforms it by simply\nhaving a lot of of language so this here\nis the results on on these open domain\nQA tasks and you you see right here\nit ad this this few shot it outperforms\nthis open domain that open domain means\nthat the model can go and look at some\nWikipedia page and yeah so so this is\npretty cool but there are other things\nlike the natural questions where it\nunder performs compared to this open\ndomain thing and they say this is mainly\ndue to the natural questions being like\nit's very much about factual Wikipedia\nknowledge and so on maybe like the\nquestion we just made maybe is more of a\nnatural question type of thing and since\nand the model is apparently not as good\nat that but it's still impressive that\nthe model is able to do this out of the\nbox okay so before I said something like\nbefore we go into the experiments I want\nthe following so I have like some sort\nof hypothesis it's not it's an it's not\nan uncommon hypothesis that basically\nthese things these giant language models\nright they they're just these\ntransformers layer after layer after\nlayer with their connections in here\nwhat I think is happening is they are\nsimply storing the training data right\nthey're simply storing the training data\nin these connections right here\nso usually you think of storing the\ntraining data in some form of maybe we\nhave like some module right here some\ndatabase module in the neural network\nand it learns to query the module but\nultimately if you train a neural network\nwhat you have is data and you train a\nfunction with parameters on that data\nand ultimately what you're doing is\nyou're distilling the data into these\nparameters and you you kind of hope to\nlearn some regularities from it but\nultimately the information about your\ntraining data influences or determines\nyour final parameters of your function\nnow I can imagine that if you have\nsuch a giant neural network with so many\nweights like 17\nsorry 170 billion weights that you can\npretty efficiently actually store the\ntraining data in that model and when you\nask this model now to do something what\nit basically does is what these people\nsort of argue is that it has learned\nthese language tasks has learned to\nreason over language and so on what I\nthink is happening much more is it will\nsimply go to the training data since it\nhas stored the entire training data in\nits weights and it will sort of pull out\nthe five to ten 250 training examples\nthat are most relevant to what you put\nin and it was sort of intercalate right\nit could go to the training data and it\nwill pull out a bunch of training\nsamples that are relevant to the context\nyou put in right now and then it will\nsort of integrate those into the next\nword that's going to come out right here\nand I think if you look at this paper in\nterms of this so you always write you\ninput a context and the context is split\ninto a task description and then it is\nsplit into K different examples and then\nit is it is it has a prompt sorry the\nseries is the prompt so the task\ndescription is please translate from\nEnglish to French and the K different\nthings are K different translations and\nthen the prompt is you know what what\nyou should do so it's like half of AK a\nhalf of one of these boxes right here so\nthese boxes are have blah blah blah\nturns to blah blah blah and then the\nprompt is simply without the deal at the\nright side I think what it does is it\nwill simply take all of this and it will\ngo to its own training data which it has\nstored in its weights and it will filter\nthe training data and basically take out\nthe the things that sort of pattern\nmatch sort of greg x match in a fuzzy\nway to this context and then it will\nkind of interpolate these training\nexamples in order to come up with the\nanswer I don't think there\nhis reasoning happening here and I'm\nwe're going to if you go through the\npaper with this view then you can a lot\nof things actually make sense and I\nactually I think that we need we need\nwhat we need when think people think of\nlike explainable machine learning they\noften think that if I'm going to input\nsomething like I'm going to input an\nimage into a classifier da da da da and\nit comes out a certain class car I like\nthe explained ability should be a which\npart of this image was it the wheels was\nit the the hood which part of the image\nwhich part of the input image is\nresponsible for making that\ndetermination what I think in especially\nin these language models what we should\ndo is if the model predicts something\nright here the next word I think we\nshould somehow have a method of\ndetermining which of the training\nexamples that the model used to\ninterpolate given this context because\nI'm pretty sure these training is you\nwill find so if you'll find that for\nexample this weight and this weight and\nthis weight was very responsible for\nmaking this prediction happen I'm pretty\nsure you can somehow during training\nbuild an index of which of the which\nfive training examples had most\ninfluence on that particular weight or\non this combination of weights and then\nyou can sort of go backwards and say you\nmade this decision right here model\nplease tell me which of the training\ndata samples were responsible for making\nthat decision actually pretty sure that\nalready exists like I'm never the first\none to think of these things though if I\nam site may like the channel now but\njust an interesting way to think about\nthis model and an interesting way to\nthink about kind of what does what would\nexplain ability even mean in a model\nlike this and my argument is since it\ninterpolates the training data the\ninterpretability should come from the\nfact of which training samples does it\ninterpolate okay let's go to Tran\nhalation so in translation as we said\nthey simply input the like the task and\nthen the few examples and then and then\nat the output okay and you can see right\nhere what you can see is that again as\nthe model goes up in parameters the\nperformance generally increases and also\nyou can see that the performance is\npretty good every time that this model\ngoes to English so it goes if it if the\ntarget language is English which sort of\nmakes sense because like a large part of\nthe corpus they trained on is English so\nbeing an English language model it\nshould be pretty good if it is asked to\nproduce English and it's not as good if\nit is asked to go into a different\ndirection now what you also see is that\nit is not really a difference whether\nyou translate from from which language\nyou translate but if you go to English\nbut it very much matters to which\nlanguage you go if it is from English so\nthis sort of makes sense in that it is\njust trained on a lot of English data\nand right here sometimes they are on par\nwith the with the state-of-the-art\nsupervised methods and also other times\nthey outperform these methods right here\nand these methods are unsupervised but\nare specifically so they don't have a\nsupervised training data set that goes\nlet's say from English to French but\nthey are built with this in mind that\nthey need to translate later so they are\nsort of task specific but don't have a\nsupervised training set and this model\nright here it just learns whatever it\nlearns and it it just it just does it\njust does this this language model\nlearning and at the end just because it\nhas seen some websites where language of\nboth things appear it can now translate\nreasonably well okay now\nyeah so the results here are a bit noisy\nbut it is still interesting to see that\nit sometimes even gets close to the\nsupervised thing though they say that\nthey are not familiar with the\nliterature and are not sure that these\nmodel that these numbers are you know\ngood okay okay the next thing is these\num Winograd schemes where you do have\nwhere is the text here is a classic NLP\ntask that involves determining which\nword a pronoun refers to when the\npronoun is grammatically ambiguous but\nsemantically unambiguous to a human so\nthese are sort of human produced\nsentences where it's kind of program\ncould refer to multiple things\nI don't have a example present but where\ndo we have the right here you can see\nthat this model will out produce a\nfine-tuned Bert large but will not out\nproduce a fine-tuned roberta large so it\nis going to it is going to come it is\ncompeting at least with the fine-tuned\nmodels that were made specifically for\nthat task right again this is pretty\npretty interesting and you also see that\nthe larger models here it starts to make\na difference whether or not you give it\none zero or one or more examples okay so\nwe'll get into we'll get into the more\ninteresting things right here in this\nthing right here where is it yes this is\nthe kind of a physical physical question\nphysical QA where it is a bit of common\nsense reasoning so you're asked to I\ndon't\nyeah these are like science questions\nmultiple choice questions collected from\na third to ninth grade exams and the\nphysical QA is physical QA asks\ncommon-sense question about how the\nphysical word work world works and is\nintended as a probe of grounded\nunderstanding of the world\nso it has questions as I understand it\nit has questions like if a drop a ball\nwill it fall on the ground or where will\nit fall or something like this and they\nsay that they can outperform a\nfine-tuned state-of-the-art model on\nthis if they go just high enough and you\ncan also see that there isn't much of a\ndifference between zero one and few\nshort the methods of this model right\nhere\neven those zero shot is even higher than\none shot so this is probably just noise\nbut then you find out that they have an\nasterisks here and this means that this\nthis is potentially a contaminated data\nset so they have potential contamination\nissues so what they found was there was\na significant overlap between the data\nset this data set and their training\ndata set and they even they only\nrealized this too late because there was\na bug in their deduplication code and\nthen they couldn't change it anymore\nlike I because this model is so large\nthat they couldn't restart the training\nbecause they've already spent like so\nmuch money and energy on it and this is\ncrazy I think these language models are\ngetting so large that we should building\nthem\nwe should more think of it like we built\nthe the International Space Station or\nsomething like this where it's a project\nwhere humanity sort of collaborates or\nthere's a big effort and you build it\nonce and whatever you have you have\nright so these these good numbers here\nare simply or not simply or\nbecause or could be influenced by this\ncontamination and I think that's what's\nhappening right here even though they\nwill make the case that this\ncontamination isn't really an issue I\ncan probably show you that it may be it\nmay be actually is an issue because on\nthe other data sets at the the\nfine-tuned state-of-the-art model\noutperform the GPT three quite a bit so\nand also the the fact that the you know\nif you provide a demonstration or many\ndemonstrations it doesn't actually\nchange that much it kind of tells me\nthat the model sort of already knows\nwhat the answer is and doesn't really\nneed demonstrations because it doesn't\nhelp if you have the training data\nstored or the the test data you don't\nreally have to get demonstrations right\nso they have a few other a few other\nthings right here we're on this cocoa\ntasks they perform pretty poorly\ncompared to others or poorly let's say\nthey perform well but not particularly\nmore well than a state of the art and\nthey perform especially poorly on the\nreading comprehension sorry that's the\nthat's the cocoa so in reading\ncomprehension what you have to do is\nabstractive multiple choice and span\nbased answer formats in both dialogue\nand single question settings so\nbasically if you read a piece of text\nlike this and then answer a question\nabout the piece of text now this is\nsomething where I think you cannot\nreally interpolate the training data\nsuper well and therefore so you can't\nreally just pattern match and interpret\nbecause you have to do actual reasoning\nand I think that's why the model\nperforms poorly here they do measure\nthis on on super glue which is a NLP\nbenchmark and\nalso here you can see it doesn't\noutperform a fine-tuned state-of-the-art\nmodel on these tasks but it does\noutperform a fine-tuned berthed model\nslightly the word model is fine-tuned on\nthese things whereas gt3 isn't but\nnotice the tasks in which it does well\nand in which it doesn't do well compared\nto the state-of-the-art model so for\nexample in the book you it doesn't do\nparticularly well right the state of\nyour is 91 it only has 76 that's quite a\nlarge difference and actually have the\nglue benchmark open here and you can see\nthis is the bull queue so an example\nhere would be is France the same time\nzone as the UK and then there is like a\npassage and you need to reason about\nfrom this passage about whether or not\nthis answer is true or false\nokay this this is very much not language\nmodeling this is reasoning and that's\nwhy the model is doing poorly here\nwhereas in another thing you see these\nfor example is Coppa right here the\nmodel is doing almost as good as a\nfine-tuned state of the art and I have\nto stress this model has never actually\nlearned this task in a supervised duay\nit's simply a language model and I have\nthis COPO task right here and these are\nthe examples so one example is the\npremise the man broke his toe what was\nthe cause of this and you have two\ndifferent things that it could be either\nhe got a hole in his sock or he dropped\na hammer on his foot and the way you\nphrase it in this model is he would give\nthe premise as the context and then you\nsimply ask the model since it's a\nlanguage model which of these two things\nis more probable to come and of course\nit is going to select the thing that can\nhave happened more often in the training\ndata and you know broke his toe the\ncause of breaking his toe that is the\nhammer this is entirely conceivable that\na language\nwould know this and with enough training\ndata could sort of pull from the\ntraining data examples where hammer on\nfoot and broke toe appear a bunch of\ntimes and hole in sock would be rather\nunrelated so as long as these questions\nare not to adversarial constructed\nspecifically that a language model can't\nsolve them there the model is going to\nperform pretty well right here right so\nit's very interesting to see that if you\nview this as interpolating the training\ndata it's only makes sense where it's\ngood and where it isn't good so this was\nthe super glue and and nli it is\nperforming particularly poorly on nli\nwhich is the ability to understand the\nrelationship between two sentences right\nso where the model classifies whether\nthe second sentence logically follows\nfrom the first contradicts the first or\nis possibly true neutral okay so this is\nthe reasoning part of this model is not\ngiven\nit is simply recalling the training data\nand doing language modeling now they say\noh we can test this we can test this\nwith synthetic and qualitative tasks so\nthey invent some own task sinks you know\nnow it's pretty easy since you don't\nhave to fine-tune the model you don't\nhave to turn to generate an actual\ntraining set for it tasks so you can\nfocus on generating a test set and and\nyou know that's what they do so they do\nsomething like arithmetic so they say\nokay can we come up with a bunch of\narithmetic tasks for example two digit\ndigit addition so what the model would\nsee would that this is an example and\nwhat the model would see is simply this\nas a context right here for the prompt\nand if you give it examples so if this\nis like one-shot learning you would\ninput add the following numbers\nthe following numbers as a string right\nthen a new line and then you would give\nit one example like what is 11 plus 12\nand with the answer together with the\nanswer answer is I don't know 23 and\nthen you the prompt goes here so what is\n48 plus 76 and then you ask what is the\nnext word right here what is the next\nstring tok and the comes here now the\nthe inference here is that if the model\nmanages to do this it can't simply\nbecause these are all strings the model\nbasically has no clue how to do math\nthese are numbers to the model these are\njust tokens or strings and the inference\nis if the model can do this it must have\nlearned you know some kind of reasoning\nability it must have learned to like\nperform some logic inside so they go\ninto two-digit addition three digit\naddition four digit addition five digit\naddition and even multiplication and\nsubtraction and the results are right\nhere so as you can see the lower\nparameter models they perform pretty\npoorly but as you go up the parameters\nthe big model is performing really well\nin the two-digit range is performing\nalso really well so accuracy of look\nthat accuracy 8090 percent in three\ndigit addition and subtraction but then\nif as soon as you get into the four\ndigit or the two digit multiplication\nand so on the performance drops now they\nsay that's because multiplication is\nharder and if you know it is logically\nvery computationally you know but the\ntwo digit addition and so on model has\nlearned something about the world I\ndisagree because so here's the because\nwhat you will do is you will simply and\nthis you simply recall the training data\nso look at the two digit addition with\nzero shot you already get seventies\n% but with one shot you get 99% and with\nfew shot you get a hundred percent so if\nyou interpret this model is simply\nfiltering the training data to pattern\nmatch then it makes a lot of sense that\nthe one shot would like the examples\nhere will give you a much improvement\nbecause if you have a bunch of examples\nwhere please add right at and then oh I\nerased our example again so you have\nlike 48 plus 72 equals blah blah blah\nyou have these of this if you give more\nand more example all of a sudden this\nlooks like a table and they say we made\nsure that the strings here these\nparticular strings were not in our\ntraining data right so these strings\nnever appeared but I just have an issue\nwith this deduplication stuff because\nwhat can appear actually is not the what\ncan appear is a table and in table often\nyou have columns and then another column\nwill be the some of these columns on the\nleft and if you are asked to pattern\nmatch you'll naturally find websites\nright if you have a few of these\nexamples you'll find websites where the\ncolumns exactly refer to these things\nand then you'll find the sum here and if\nyou filter for websites that appear to\nmatch your scheme in the examples you\nwill find all the website with a table\non them where the the column 1 column is\nan addition of the others and I can\nactually do that so I went and I typed\nin just a bunch of these things so 98\nplus 45 is 143 18 plus 55 is 70 I\nbelieve at least and I can find now\nGoogle makes it hard because they\nlocalize and everything but you can\nstill find what you're going to find our\ntables and tables and tables and tables\nand now I actually went to dr. go to\nbasically say\nyou know they they don't you know really\npersonalize it to me and what's the\nfirst thing I find when I type in just\nthese numbers is math skip counting\nmissing sequence number and a website\nwhere basically the answers are already\ngiven look at that so all the model has\nto do is recall this particular training\nexample from the samples it already has\nright and it will it will basically be\nable in quotes to perform addition like\nthis is financial data and another one\nwhere you have to subtract stuff right\nso I'm pretty sure all the model is\ndoing here is interpolating the training\ndata and that's also why it performs\nworse if if you up the digits because\nlonger digit numbers are simply less\nfrequent in the in in the training data\nmultiplication is first of all less\nfrequent and second of all it also\nresults in larger numbers which are less\nfrequent right so it explains a lot so I\nyeah I have my issues with people saying\nyeah this this shows some reasoning I\ndon't think it does the same thing here\nwith word scramble so in word scramble\nthey have different things you see okay\nthey they they look whether or not only\n17 matches 0.8% of the math things are\nin their training data is like no you\nhaven't searched well enough and the\nrest of their deduplication by the way\nis also pretty weak I would say because\nthey just look for like 13 gram overlaps\nbetween the training data and the inde\nand their their test data so they have\nthese words scrambling tasks where they\nbasically scramble words and they asked\nthe model to unscramble it for example\nthis word is inevitably scrambled so\nthey always you know they give like\nanagrams and they give random insertion\ninto the world like this word right here\nor they reverse the word\nand they say so this I think this is the\nthing at the very beginning but if you\ncan see right here also as the model\ngoes up then this this improves and they\nalso say well this means maybe some kind\nof reasoning but I think this is just\nit's learning the language and it's\nlearning that you know the the words in\nin sorry that the letters make up a word\nand the letters correspond to word\npieces Laura are associated with word\npieces and it always learns to English a\ngood tasks to check this would actually\nbe to scramble words so if you\nunscramble words you always end up with\nan English word\nso all it has to do is basically check\nwhich word has the highest overlap in\nword pieces but you could do something\nlike please scramble this word and then\nalways count it correctly when any of\nthe scrambling of the words so instead\nof going from this to this which you can\nsimply solve by knowing the English\nlanguage but you would have basically no\nclue what the task is that you don't\nhave to understand that as a model you\ncould ask it to go from this to this\ngiven a few examples right then it would\nreally need to understand what the task\nis that it's supposed to actually\nscramble a word and would would need to\nlearn that from its context given\nexamples but they as far as I see they\ndon't do that and again I think it's\nrecalling the the training data the this\nis Sat analogies so the SAT or this test\nthat in the US high schoolers take to\nget into college and the this if they\nsay a typical example this is dying on\nme now it scrolled okay a typical\nexample is the following this I find I\nfind pretty hilarious audacious is to\nboldness as sanctimonious is to\nhypocrisy anonymous is to identity\nremorseful still missed\ndeleterious is to result or\nimpressionable is to temptation this is\na as as a okay I'm not a native speaker\nbut this is a hard question right and\nyou have to you know see that these\nthese high-schoolers they're stressed\nlike this is very much a time-based test\nso you need to make a decision quickly\nwell the model of course is basically\nable to sift through its entire training\ndata in the time it takes to GPUs to\nperform inference but it's still funny\nthat gt3 achieves fifty sixty five\npercent in the few shots setting and\nfifty-nine percent in one shot setting\nfifty three percent is zero short\nsetting whereas the average score among\ncollege applicants was fifty seven\npercent so it outperforms the average\ncollege applicant it's pretty funny but\nyou would expect the language model to\nhave a pretty good grasp of these kind\nof synonyms and relations between words\nbecause these are just absolutely\nstatistical associations between words\nso yeah this I found this to be pretty\npretty funny and the last thing and this\nis what everyone's freaking out over is\nthis news article generation where\nbasically they give it the beginning of\na few of a news article and then they\nlet humans decide whether or not the\nnews article is written by a machine or\nby a human and they say here by contrast\nmean human accuracy at detecting\narticles that were produced by the one\nhundred seventy five billion parameter\nmodel it was barely above chance at\nfifty two percent human abilities to\ndetect model generated text appear to\ndecrease as model size increases there\nappears to be a trend towards chance\naccuracy with model size and human\ndetection of g PT three is close to\nchance okay\nso what they do is they give indeed they\nhave some examples right here\nthey give the model the following input\nthe title the subtitle of an article and\nthen this word article the model is\nsupposed to complete\nthe rest of the article right here and\nyou can also you know give do this in a\nfew shots setting such that the model\nbasically knows that it's if you give it\na few a few examples the model knows it\nis supposed to produce a news article\nright okay so there are two two ways\nthat you can think of this first way the\nmodel has learned the language so well\nand it writes code it has learned to\nwrite coherent language and so on is\nlearn to reason keep context and blah\nblah blah okay\nsecond way the model sees this thing\nright here it sees the few you know K\nfew shot examples that it has before in\nthe context it will take them filter the\ntraining data to in this case it just\nsees news articles so do just news\narticles it will take this thing filter\nthe training data even more to just the\nnews articles that pertain largely to\ntopics or words that appear in here and\nthen lastly will interpolate the few\ntraining examples to produce this thing\nnow they argue that this isn't really\npossible because they have actually\nchecked that this news article is not in\nthe training data but I have simply gone\nand taken a you I've really taken a\nrandom substring here I've taken this\nsubstring voted to strengthen a ban on\nthe ordination of just this substring\nand I've put it into Google and Bob Reba\nI find a book with voted to strengthen\nprohibitions to ban LGBTQ people from\nbeing ordained and ministers so it's you\nknow I find this it's not the same\narticle but it's talking about the same\nincident the article talks about and it\nis using the same language probably read\nthe article and the author is like I\ncan't really you know copy paste that\nwould be you know not really cool so\nI'll just kind of you know write it in\nmy own words but largely the same thing\nThe Associated Press here also a\ndifferent article you know see\ntitle than this one right here but about\nthe same thing and also with the same\nlanguage right here voted Tuesday to\nstrengthen the faiths divisive bans on\nsame-sex marriage and ordination of LGBT\nclergy and generally so the argument\nthis article wasn't in the training data\nis just not really something I buy in\nthis in this case so I think it the\narticle as such wasn't there but many\narticles about this topics were and I\nthink this will just interpolate these\nnow they say this was the hardest\narticle for the humans to decide and\nthis here was the easiest so it's it\nsays I don't know star talks promise\ndraws Megyn Kelly's sarcasm and says a\nyear ago joke in Phoenix made headlines\nwhen he appeared on the red carpet at\nGolden Globes wearing a tuxedo with a\npaper bag over his head that read I'm a\nshapeshifter above you you would guess\nthat joke in Phoenix would do something\nlike this but they say they're human\nraiders were US based right and you see\nright here it says men Kelly was not\nimpressed and she let him have it on The\nTonight Show\nanother Tonight Show is not when megyn\nkelly is and us-based people would I\nguess know something like this and would\nimmediately feel like this is wrong so I\nthink this thing is interpolated from is\ninterpolated from a bunch of different\nnews articles about this and the\ninterpolation just let it like made it\nteach that this person is on this show\nwhich that they aren't and the humans\nnoticed right well it doesn't change the\nfact that it probably just went to the\ntraining data filtered a bunch of\narticles about these words and then\ninterpolated like mash them together it\nis a good language model right it can\ngrammar it's very good at grammar so we\ncan interpolate different passages of\ntext and I feel that the the really\nreally useful application of this will\nbe sort of as a search engine as a fuzzy\nsearch engine so now\ncan like input for example my my machine\nlearning research ideas and what will\noutput will be sort of an abstract of a\npaper that is kind of a merge together\nof other papers on the same thing and\nthat that you know you can think of many\napplications I don't think we have built\nsomething really intelligent here and\nwhat this is this is though is pretty\ncool\nthey they give examples like this here\nwhere they make up a world and then ask\nthe model to use the word in a sentence\nso to skree is something sorry to\nscreech something is to swing a sword at\nit an example of a sentence that uses\nthe word scree is and of course the\nmodel what's the models going to do is\nit's going to take this it's going to\nfilter the training data for all of the\ninstances we're sort of this\nconstruction appears like an example of\nusing the word which is mostly so\ndictionaries then it's going to not know\nthat word but it can interpolate from\ninterpolate it from all this data right\nhere and the cool thing is it actually\nconjugates the where we screed at each\nother for several minutes and then we\nwent outside and ate ice cream so you\ncan see how this comes to be but I think\nit would really be fun to have a model\nthat tells us which training data\nsamples were used here it can also\ncorrect English grammar which is pretty\nobvious though again it can never\ncorrect so the the input always here is\npoor English good English poor English\ngood image poor good poor English and\nthen good English and that's what the\nmodel is asked to to output and I'm\nactually not sure pretty sure this here\nshouldn't be bold I'm fairly sure this\nshouldn't be bold this is given to the\nmodel the model is only asked to produce\nthis otherwise I'd be I'd be actually\nimpressed but yes\nnothing task-specific is provided aside\nfrom the examples from few example as\nconditioning and\npoor English input good English output\nframing so the good English output thing\nhere should not be in boldface authors\nif you're listening this should not be\nbold thank you okay but again it is\nalways as you can see it's too good\nEnglish it's always the target is good\nEnglish whereas if the model really\nunderstood the task it should also be\nable to do the inverse it should be able\nto to produce something poor from\nsomething good because then you\neliminate the fact that it's just a good\nEnglish language model right because it\ncan basically produce something like\nthis without having a clue what the task\nis it will simply you condition on this\ninput and it will simply output this\nsentence because it's very likely\nbecause it's already here almost here\nand it will output it in better English\nbecause it's a good language model right\nit's it's a good English language model\nso yeah that so they measure this\noverfitting the degree to which they're\ntraining to which their test data is in\nthis common crawl thing and they say\nthey have a conservative bound on how\nmany percent of the data in the data set\nare clean and as you can see here they\nmeasure then how much the performance\ndiffers to - up or down if you only\nevaluate on the clean portion of this\ndata set but again their deduplication\nis so weak they do like Engram\ndeduplication whereas I think you should\nreally like in the news articles you\nshould really do much more fuzzy\ndeduplication much more of a meaning\ndeduplication if you then want to argue\nthat the model has learned to reason\nlike if you simply want to argue that\nthe model is a good language model fine\nright but yeah and also look at this\nlike I would expect of a dataset a test\ndataset if you know if you have like a\nnatural questions dataset it is\nconstructed from Wikipedia pages and you\nhave the Wikipedia page in there you can\neither either the entire thing is clean\nor none of it is clean and also these\nWinograd dataset if this dataset somehow\nleaked into the common crawl corpus\neither the entire thing is clean or none\nof it is clean I just have kind of\nproblems with the fact that there are so\nmany in-between things right here and\nyeah so I'm not I'm not convinced here\nthat this deduplication I still think\nit's a cool thing but I don't I think\nit's mostly a training data filter and\ninterpolator rather than actual\nreasoning and they go through some of\nthe limitations here and the broader in\nthis broader impact statements like five\npages long and yeah okay you can do you\ncan you know bad people take the model\nto do bad things okay and that's pretty\nmuch it so what I appreciate here is at\nthe bottom they have basically all the\nresults but also a lot of tasks\ndescriptions like how they framed each\ntasks or outputs and they gave more\noutputs on their website rightly so you\ncan see here how each of the tasks was\nframed where you always have this is\nwhat this here is what the model sees\nand then this is what it's asked to\nproduce right so you have this for for\nall many of these things and so on squad\nyou have this context and the question\nokay so the the context is actually in\nthere I've didn't know that but you have\nthe context and the question and the\nmodel is asked to complete something\nright here so you can look at how the\nmodel sees tasks and maybe you can\nevaluate for yourself how you think how\ndifficult you think these tasks or\nalright I hope this was informative it\nis a long paper therefore it is a long\nvideo if you're still here and haven't\nsubscribed yet maybe if you like this if\nyou want more de leave it a like tell me\nin the comments what you think of it\nwhether you think it's actually\nIGI or not and I'll see you next time\nbye-bye\n",
  "words": [
    "hello",
    "today",
    "looking",
    "language",
    "models",
    "shot",
    "learners",
    "tom",
    "b",
    "brown",
    "benjamin",
    "man",
    "nick",
    "ryder",
    "melanie",
    "sabaha",
    "whole",
    "authors",
    "slew",
    "authors",
    "opi",
    "paper",
    "also",
    "called",
    "gpt",
    "three",
    "came",
    "recently",
    "gpt",
    "three",
    "model",
    "language",
    "model",
    "comes",
    "succession",
    "language",
    "models",
    "open",
    "e",
    "paper",
    "basically",
    "investigation",
    "giant",
    "language",
    "models",
    "language",
    "model",
    "order",
    "magnitude",
    "larger",
    "anyone",
    "ever",
    "built",
    "language",
    "model",
    "absolutely",
    "crazy",
    "things",
    "basically",
    "go",
    "architecture",
    "model",
    "experimental",
    "results",
    "turns",
    "train",
    "language",
    "model",
    "enough",
    "data",
    "able",
    "solve",
    "nlp",
    "tasks",
    "never",
    "seen",
    "box",
    "gon",
    "na",
    "look",
    "cool",
    "kind",
    "formulation",
    "problem",
    "see",
    "paper",
    "40",
    "pages",
    "long",
    "without",
    "appendix",
    "needs",
    "table",
    "contents",
    "crazy",
    "going",
    "skip",
    "fair",
    "bit",
    "things",
    "first",
    "language",
    "model",
    "know",
    "done",
    "bunch",
    "videos",
    "see",
    "natural",
    "language",
    "processing",
    "playlist",
    "language",
    "models",
    "specifically",
    "transformer",
    "language",
    "models",
    "language",
    "model",
    "let",
    "take",
    "example",
    "sentence",
    "right",
    "sentence",
    "like",
    "third",
    "humans",
    "require",
    "require",
    "large",
    "supervised",
    "data",
    "sets",
    "learn",
    "language",
    "tasks",
    "right",
    "english",
    "sentence",
    "language",
    "model",
    "would",
    "model",
    "cross",
    "portion",
    "end",
    "like",
    "right",
    "would",
    "able",
    "tell",
    "comes",
    "next",
    "language",
    "model",
    "would",
    "input",
    "part",
    "right",
    "tell",
    "next",
    "word",
    "data",
    "sets",
    "basically",
    "language",
    "model",
    "trained",
    "one",
    "basically",
    "generate",
    "word",
    "word",
    "word",
    "ask",
    "question",
    "like",
    "word",
    "likely",
    "come",
    "next",
    "likely",
    "language",
    "model",
    "nothing",
    "model",
    "kind",
    "generate",
    "language",
    "probabilistic",
    "way",
    "cool",
    "thing",
    "language",
    "models",
    "train",
    "sort",
    "text",
    "data",
    "train",
    "language",
    "model",
    "giant",
    "amounts",
    "data",
    "specifically",
    "right",
    "go",
    "data",
    "sets",
    "use",
    "use",
    "let",
    "skip",
    "use",
    "common",
    "crawl",
    "data",
    "set",
    "filter",
    "quality",
    "basically",
    "crawl",
    "entire",
    "internet",
    "together",
    "books",
    "data",
    "sets",
    "web",
    "text",
    "data",
    "set",
    "wikipedia",
    "data",
    "set",
    "throw",
    "text",
    "scrape",
    "internet",
    "together",
    "train",
    "language",
    "model",
    "language",
    "model",
    "right",
    "called",
    "gpt",
    "three",
    "train",
    "various",
    "sizes",
    "get",
    "built",
    "second",
    "compare",
    "language",
    "model",
    "like",
    "burt",
    "burt",
    "required",
    "much",
    "flops",
    "train",
    "log",
    "scale",
    "right",
    "several",
    "orders",
    "magnitude",
    "larger",
    "bigger",
    "model",
    "trained",
    "way",
    "longer",
    "text",
    "naturally",
    "going",
    "lot",
    "better",
    "language",
    "modeling",
    "see",
    "right",
    "size",
    "models",
    "trained",
    "remember",
    "previous",
    "largest",
    "language",
    "model",
    "turing",
    "nlg",
    "microsoft",
    "something",
    "like",
    "17",
    "billion",
    "parameters",
    "would",
    "comparable",
    "right",
    "whereas",
    "gpt",
    "3",
    "175",
    "billion",
    "parameters",
    "absolutely",
    "crazy",
    "order",
    "magnitude",
    "higher",
    "anything",
    "ever",
    "existed",
    "look",
    "last",
    "gpt",
    "gpt",
    "model",
    "remember",
    "made",
    "video",
    "dangerous",
    "released",
    "well",
    "released",
    "dangerous",
    "released",
    "clocked",
    "billion",
    "parameters",
    "compared",
    "gpt",
    "three",
    "excel",
    "model",
    "right",
    "trained",
    "multiple",
    "models",
    "basically",
    "estimate",
    "effect",
    "model",
    "size",
    "see",
    "largest",
    "model",
    "attention",
    "layers",
    "layer",
    "96",
    "attention",
    "heads",
    "head",
    "128",
    "dimensional",
    "trains",
    "batches",
    "size",
    "million",
    "batch",
    "size",
    "absolutely",
    "crazy",
    "train",
    "giant",
    "distributed",
    "cluster",
    "apparently",
    "provided",
    "microsoft",
    "yes",
    "crazy",
    "crazy",
    "things",
    "model",
    "look",
    "model",
    "transformer",
    "model",
    "right",
    "even",
    "like",
    "description",
    "transformer",
    "model",
    "let",
    "assume",
    "know",
    "made",
    "several",
    "videos",
    "transformer",
    "models",
    "especially",
    "things",
    "like",
    "attention",
    "need",
    "burt",
    "something",
    "like",
    "know",
    "transformer",
    "model",
    "want",
    "build",
    "language",
    "model",
    "let",
    "take",
    "sentence",
    "right",
    "would",
    "input",
    "called",
    "context",
    "thing",
    "already",
    "right",
    "would",
    "input",
    "transformer",
    "model",
    "transformer",
    "model",
    "several",
    "layers",
    "attention",
    "mechanism",
    "attention",
    "mechanism",
    "basically",
    "way",
    "information",
    "routed",
    "different",
    "tokens",
    "right",
    "goes",
    "layer",
    "basically",
    "information",
    "routed",
    "around",
    "model",
    "make",
    "various",
    "inferences",
    "end",
    "model",
    "supposed",
    "come",
    "next",
    "word",
    "going",
    "put",
    "specifically",
    "paper",
    "use",
    "sub",
    "words",
    "like",
    "word",
    "piece",
    "tokens",
    "like",
    "common",
    "nlp",
    "right",
    "essentially",
    "auto",
    "regressive",
    "language",
    "model",
    "like",
    "bert",
    "direction",
    "autoregressive",
    "goes",
    "left",
    "right",
    "always",
    "produces",
    "next",
    "word",
    "like",
    "gpt",
    "even",
    "say",
    "say",
    "use",
    "model",
    "architecture",
    "gpt",
    "layers",
    "wider",
    "layers",
    "data",
    "train",
    "train",
    "okaythat",
    "already",
    "said",
    "train",
    "simply",
    "simply",
    "language",
    "modeling",
    "way",
    "next",
    "word",
    "prediction",
    "okay",
    "even",
    "something",
    "fancy",
    "like",
    "bert",
    "interesting",
    "part",
    "single",
    "tasks",
    "usually",
    "something",
    "like",
    "bert",
    "something",
    "like",
    "bert",
    "would",
    "first",
    "pre",
    "train",
    "would",
    "language",
    "modeling",
    "right",
    "pre",
    "training",
    "phase",
    "teach",
    "bert",
    "english",
    "language",
    "feeding",
    "lot",
    "data",
    "second",
    "step",
    "called",
    "fine",
    "tuning",
    "fine",
    "ca",
    "even",
    "write",
    "tuning",
    "second",
    "one",
    "something",
    "like",
    "task",
    "actually",
    "interested",
    "let",
    "say",
    "task",
    "actually",
    "interested",
    "sentiment",
    "classification",
    "sentiment",
    "classification",
    "like",
    "sentence",
    "like",
    "blah",
    "blah",
    "blah",
    "want",
    "know",
    "positive",
    "sentiment",
    "like",
    "happy",
    "sentence",
    "sad",
    "sentence",
    "would",
    "database",
    "labeled",
    "instances",
    "database",
    "bunch",
    "sentences",
    "one",
    "would",
    "know",
    "good",
    "positive",
    "negative",
    "like",
    "smaller",
    "test",
    "set",
    "right",
    "would",
    "would",
    "train",
    "would",
    "basically",
    "take",
    "pre",
    "trained",
    "model",
    "train",
    "dataset",
    "supervised",
    "machine",
    "learning",
    "way",
    "test",
    "test",
    "set",
    "right",
    "called",
    "fine",
    "tuning",
    "display",
    "fine",
    "tuning",
    "model",
    "trained",
    "via",
    "repeated",
    "gradient",
    "updates",
    "using",
    "large",
    "corpus",
    "example",
    "tasks",
    "right",
    "example",
    "task",
    "right",
    "could",
    "translating",
    "french",
    "training",
    "database",
    "translation",
    "task",
    "would",
    "would",
    "c",
    "order",
    "called",
    "lu",
    "treadmill",
    "actually",
    "change",
    "model",
    "gradient",
    "update",
    "mean",
    "nlp",
    "world",
    "seems",
    "natural",
    "going",
    "argue",
    "second",
    "way",
    "teach",
    "model",
    "task",
    "right",
    "seems",
    "natural",
    "right",
    "change",
    "model",
    "take",
    "pre",
    "trained",
    "model",
    "going",
    "task",
    "different",
    "task",
    "right",
    "question",
    "answering",
    "tasks",
    "going",
    "different",
    "data",
    "set",
    "right",
    "train",
    "test",
    "data",
    "set",
    "going",
    "take",
    "pre",
    "trained",
    "model",
    "data",
    "set",
    "evaluate",
    "test",
    "set",
    "gives",
    "basically",
    "many",
    "models",
    "tasks",
    "andy",
    "one",
    "need",
    "big",
    "big",
    "training",
    "data",
    "set",
    "order",
    "perform",
    "well",
    "sometimes",
    "sometimes",
    "interested",
    "basically",
    "take",
    "pre",
    "trained",
    "model",
    "directly",
    "go",
    "evaluate",
    "test",
    "data",
    "set",
    "sort",
    "zero",
    "shot",
    "fashion",
    "zero",
    "shot",
    "argue",
    "true",
    "zero",
    "shot",
    "fashion",
    "would",
    "take",
    "language",
    "model",
    "pre",
    "trained",
    "input",
    "following",
    "text",
    "input",
    "call",
    "task",
    "description",
    "prompt",
    "input",
    "simply",
    "asked",
    "model",
    "language",
    "model",
    "predict",
    "next",
    "word",
    "comes",
    "counting",
    "basically",
    "training",
    "data",
    "model",
    "seen",
    "structure",
    "like",
    "enough",
    "understand",
    "going",
    "training",
    "data",
    "somewhere",
    "internet",
    "structure",
    "translate",
    "something",
    "something",
    "would",
    "word",
    "something",
    "know",
    "kind",
    "realize",
    "goes",
    "like",
    "next",
    "word",
    "basically",
    "asking",
    "find",
    "text",
    "website",
    "wikipedia",
    "books",
    "data",
    "set",
    "find",
    "piece",
    "text",
    "would",
    "next",
    "word",
    "piece",
    "text",
    "kind",
    "hope",
    "enough",
    "trained",
    "good",
    "language",
    "model",
    "enough",
    "actually",
    "produce",
    "french",
    "translation",
    "realize",
    "said",
    "language",
    "modeling",
    "teach",
    "model",
    "english",
    "language",
    "actually",
    "true",
    "common",
    "crawl",
    "corpus",
    "also",
    "many",
    "foreign",
    "languages",
    "basically",
    "teach",
    "general",
    "model",
    "internet",
    "translate",
    "contrast",
    "call",
    "learning",
    "learning",
    "task",
    "description",
    "right",
    "string",
    "right",
    "specifically",
    "tell",
    "model",
    "translation",
    "task",
    "simply",
    "input",
    "string",
    "task",
    "description",
    "prompt",
    "right",
    "also",
    "one",
    "example",
    "example",
    "bring",
    "say",
    "exactly",
    "zero",
    "shot",
    "little",
    "drawing",
    "example",
    "going",
    "come",
    "training",
    "data",
    "set",
    "task",
    "interested",
    "important",
    "part",
    "never",
    "train",
    "never",
    "explicitly",
    "train",
    "example",
    "simply",
    "put",
    "context",
    "simply",
    "put",
    "string",
    "translate",
    "english",
    "french",
    "newline",
    "c",
    "order",
    "lute",
    "lu",
    "mere",
    "newline",
    "cheese",
    "simply",
    "input",
    "string",
    "model",
    "language",
    "model",
    "ask",
    "next",
    "word",
    "right",
    "okay",
    "hope",
    "hope",
    "clear",
    "call",
    "kind",
    "generalization",
    "basically",
    "mean",
    "simply",
    "provide",
    "thing",
    "texts",
    "model",
    "language",
    "model",
    "advantage",
    "immediately",
    "clear",
    "train",
    "one",
    "model",
    "basically",
    "inference",
    "time",
    "input",
    "task",
    "description",
    "sort",
    "training",
    "data",
    "task",
    "evaluation",
    "context",
    "task",
    "really",
    "claim",
    "would",
    "able",
    "sort",
    "understand",
    "prompt",
    "understand",
    "means",
    "translate",
    "english",
    "french",
    "would",
    "look",
    "example",
    "say",
    "oh",
    "want",
    "okay",
    "would",
    "able",
    "generalize",
    "input",
    "right",
    "say",
    "ah",
    "okay",
    "task",
    "description",
    "example",
    "saw",
    "get",
    "get",
    "want",
    "next",
    "word",
    "cheese",
    "cheese",
    "french",
    "remember",
    "homage",
    "homage",
    "way",
    "language",
    "model",
    "going",
    "interpret",
    "slightly",
    "different",
    "said",
    "way",
    "language",
    "model",
    "going",
    "interpret",
    "find",
    "following",
    "text",
    "website",
    "somewhere",
    "text",
    "called",
    "translate",
    "english",
    "french",
    "new",
    "line",
    "c",
    "order",
    "goes",
    "luton",
    "new",
    "line",
    "cheese",
    "goes",
    "would",
    "next",
    "word",
    "website",
    "model",
    "sees",
    "right",
    "differentiate",
    "human",
    "wants",
    "model",
    "sees",
    "model",
    "language",
    "model",
    "going",
    "take",
    "next",
    "going",
    "determine",
    "see",
    "text",
    "somewhere",
    "likely",
    "next",
    "word",
    "phrase",
    "tasks",
    "way",
    "makes",
    "sense",
    "thing",
    "also",
    "short",
    "thing",
    "provide",
    "one",
    "context",
    "provide",
    "bunch",
    "context",
    "basically",
    "tell",
    "model",
    "work",
    "free",
    "mode",
    "basically",
    "say",
    "next",
    "word",
    "also",
    "language",
    "hold",
    "exact",
    "model",
    "give",
    "basically",
    "couple",
    "possibilities",
    "give",
    "say",
    "like",
    "either",
    "shop",
    "format",
    "hotel",
    "think",
    "like",
    "basically",
    "restrict",
    "produce",
    "one",
    "three",
    "things",
    "translation",
    "might",
    "know",
    "way",
    "go",
    "like",
    "answers",
    "questions",
    "restrict",
    "lot",
    "nlp",
    "tasks",
    "options",
    "given",
    "given",
    "question",
    "also",
    "restrict",
    "know",
    "always",
    "go",
    "task",
    "hand",
    "essence",
    "model",
    "think",
    "new",
    "well",
    "new",
    "per",
    "se",
    "one",
    "core",
    "ideas",
    "paper",
    "take",
    "anything",
    "new",
    "architecture",
    "right",
    "new",
    "wisdom",
    "training",
    "train",
    "standard",
    "way",
    "standard",
    "language",
    "modeling",
    "fashion",
    "standard",
    "transformer",
    "architecture",
    "happens",
    "ginormous",
    "okay",
    "right",
    "thing",
    "say",
    "things",
    "would",
    "fine",
    "tune",
    "basically",
    "end",
    "one",
    "model",
    "per",
    "task",
    "need",
    "big",
    "data",
    "set",
    "per",
    "task",
    "simply",
    "since",
    "large",
    "language",
    "model",
    "basically",
    "already",
    "basically",
    "already",
    "knows",
    "tasks",
    "long",
    "formulate",
    "language",
    "model",
    "way",
    "model",
    "perform",
    "tasks",
    "show",
    "works",
    "surprisingly",
    "well",
    "throughout",
    "paper",
    "get",
    "experimental",
    "results",
    "right",
    "experimental",
    "results",
    "first",
    "language",
    "modeling",
    "see",
    "basically",
    "say",
    "go",
    "parameters",
    "see",
    "moriya",
    "ones",
    "parameters",
    "go",
    "validation",
    "loss",
    "goes",
    "believe",
    "sort",
    "log",
    "scale",
    "well",
    "log",
    "probability",
    "perplexity",
    "basically",
    "follows",
    "trend",
    "log",
    "scale",
    "log",
    "scale",
    "follows",
    "trend",
    "scale",
    "model",
    "scale",
    "compute",
    "model",
    "gets",
    "know",
    "big",
    "language",
    "models",
    "basically",
    "know",
    "scale",
    "model",
    "size",
    "compute",
    "time",
    "dataset",
    "size",
    "fashion",
    "make",
    "gains",
    "follows",
    "like",
    "power",
    "law",
    "scale",
    "things",
    "model",
    "basically",
    "gets",
    "better",
    "better",
    "better",
    "question",
    "course",
    "know",
    "far",
    "far",
    "go",
    "seems",
    "hold",
    "quite",
    "well",
    "make",
    "improvements",
    "scaling",
    "model",
    "language",
    "modeling",
    "least",
    "basically",
    "go",
    "dive",
    "actual",
    "results",
    "individual",
    "task",
    "going",
    "formulate",
    "individual",
    "tasks",
    "like",
    "pure",
    "language",
    "modeling",
    "tasks",
    "right",
    "like",
    "alice",
    "friends",
    "bob",
    "alice",
    "went",
    "visit",
    "friend",
    "like",
    "next",
    "word",
    "okay",
    "bob",
    "george",
    "bought",
    "baseball",
    "equipment",
    "ball",
    "glove",
    "next",
    "word",
    "guess",
    "hat",
    "bat",
    "right",
    "going",
    "go",
    "tasks",
    "one",
    "example",
    "question",
    "answering",
    "question",
    "answering",
    "simply",
    "get",
    "either",
    "get",
    "pure",
    "question",
    "context",
    "question",
    "fact",
    "test",
    "situation",
    "get",
    "question",
    "get",
    "know",
    "queen",
    "england",
    "something",
    "like",
    "model",
    "simply",
    "produce",
    "either",
    "results",
    "direct",
    "choose",
    "bunch",
    "answers",
    "one",
    "likely",
    "language",
    "model",
    "see",
    "scale",
    "language",
    "model",
    "zero",
    "shot",
    "one",
    "shot",
    "shot",
    "predictions",
    "shot",
    "give",
    "64",
    "different",
    "examples",
    "training",
    "set",
    "context",
    "always",
    "context",
    "going",
    "look",
    "something",
    "like",
    "examples",
    "bottom",
    "looked",
    "qa",
    "task",
    "example",
    "going",
    "something",
    "like",
    "task",
    "description",
    "like",
    "answer",
    "following",
    "questions",
    "answer",
    "question",
    "example",
    "zero",
    "shot",
    "zero",
    "one",
    "shot",
    "one",
    "like",
    "say",
    "tall",
    "sorry",
    "know",
    "climbed",
    "everest",
    "first",
    "rest",
    "first",
    "say",
    "hillary",
    "think",
    "hillary",
    "remember",
    "say",
    "know",
    "tall",
    "empire",
    "state",
    "building",
    "like",
    "number",
    "end",
    "say",
    "question",
    "know",
    "queen",
    "england",
    "yeah",
    "queen",
    "england",
    "ask",
    "model",
    "predict",
    "next",
    "word",
    "right",
    "okay",
    "closed",
    "book",
    "setting",
    "means",
    "access",
    "wikipedia",
    "whatever",
    "like",
    "usually",
    "systems",
    "go",
    "query",
    "wikipedia",
    "system",
    "want",
    "know",
    "model",
    "learned",
    "world",
    "simply",
    "absorbing",
    "giant",
    "amounts",
    "text",
    "somewhere",
    "training",
    "data",
    "fact",
    "queen",
    "england",
    "elizabeth",
    "second",
    "present",
    "complete",
    "right",
    "performs",
    "surprisingly",
    "well",
    "see",
    "manages",
    "outperform",
    "model",
    "question",
    "answering",
    "right",
    "built",
    "question",
    "answering",
    "model",
    "outperforms",
    "simply",
    "lot",
    "language",
    "results",
    "open",
    "domain",
    "qa",
    "tasks",
    "see",
    "right",
    "ad",
    "shot",
    "outperforms",
    "open",
    "domain",
    "open",
    "domain",
    "means",
    "model",
    "go",
    "look",
    "wikipedia",
    "page",
    "yeah",
    "pretty",
    "cool",
    "things",
    "like",
    "natural",
    "questions",
    "performs",
    "compared",
    "open",
    "domain",
    "thing",
    "say",
    "mainly",
    "due",
    "natural",
    "questions",
    "like",
    "much",
    "factual",
    "wikipedia",
    "knowledge",
    "maybe",
    "like",
    "question",
    "made",
    "maybe",
    "natural",
    "question",
    "type",
    "thing",
    "since",
    "model",
    "apparently",
    "good",
    "still",
    "impressive",
    "model",
    "able",
    "box",
    "okay",
    "said",
    "something",
    "like",
    "go",
    "experiments",
    "want",
    "following",
    "like",
    "sort",
    "hypothesis",
    "uncommon",
    "hypothesis",
    "basically",
    "things",
    "giant",
    "language",
    "models",
    "right",
    "transformers",
    "layer",
    "layer",
    "layer",
    "connections",
    "think",
    "happening",
    "simply",
    "storing",
    "training",
    "data",
    "right",
    "simply",
    "storing",
    "training",
    "data",
    "connections",
    "right",
    "usually",
    "think",
    "storing",
    "training",
    "data",
    "form",
    "maybe",
    "like",
    "module",
    "right",
    "database",
    "module",
    "neural",
    "network",
    "learns",
    "query",
    "module",
    "ultimately",
    "train",
    "neural",
    "network",
    "data",
    "train",
    "function",
    "parameters",
    "data",
    "ultimately",
    "distilling",
    "data",
    "parameters",
    "kind",
    "hope",
    "learn",
    "regularities",
    "ultimately",
    "information",
    "training",
    "data",
    "influences",
    "determines",
    "final",
    "parameters",
    "function",
    "imagine",
    "giant",
    "neural",
    "network",
    "many",
    "weights",
    "like",
    "17",
    "sorry",
    "170",
    "billion",
    "weights",
    "pretty",
    "efficiently",
    "actually",
    "store",
    "training",
    "data",
    "model",
    "ask",
    "model",
    "something",
    "basically",
    "people",
    "sort",
    "argue",
    "learned",
    "language",
    "tasks",
    "learned",
    "reason",
    "language",
    "think",
    "happening",
    "much",
    "simply",
    "go",
    "training",
    "data",
    "since",
    "stored",
    "entire",
    "training",
    "data",
    "weights",
    "sort",
    "pull",
    "five",
    "ten",
    "250",
    "training",
    "examples",
    "relevant",
    "put",
    "sort",
    "intercalate",
    "right",
    "could",
    "go",
    "training",
    "data",
    "pull",
    "bunch",
    "training",
    "samples",
    "relevant",
    "context",
    "put",
    "right",
    "sort",
    "integrate",
    "next",
    "word",
    "going",
    "come",
    "right",
    "think",
    "look",
    "paper",
    "terms",
    "always",
    "write",
    "input",
    "context",
    "context",
    "split",
    "task",
    "description",
    "split",
    "k",
    "different",
    "examples",
    "prompt",
    "sorry",
    "series",
    "prompt",
    "task",
    "description",
    "please",
    "translate",
    "english",
    "french",
    "k",
    "different",
    "things",
    "k",
    "different",
    "translations",
    "prompt",
    "know",
    "like",
    "half",
    "ak",
    "half",
    "one",
    "boxes",
    "right",
    "boxes",
    "blah",
    "blah",
    "blah",
    "turns",
    "blah",
    "blah",
    "blah",
    "prompt",
    "simply",
    "without",
    "deal",
    "right",
    "side",
    "think",
    "simply",
    "take",
    "go",
    "training",
    "data",
    "stored",
    "weights",
    "filter",
    "training",
    "data",
    "basically",
    "take",
    "things",
    "sort",
    "pattern",
    "match",
    "sort",
    "greg",
    "x",
    "match",
    "fuzzy",
    "way",
    "context",
    "kind",
    "interpolate",
    "training",
    "examples",
    "order",
    "come",
    "answer",
    "think",
    "reasoning",
    "happening",
    "going",
    "go",
    "paper",
    "view",
    "lot",
    "things",
    "actually",
    "make",
    "sense",
    "actually",
    "think",
    "need",
    "need",
    "need",
    "think",
    "people",
    "think",
    "like",
    "explainable",
    "machine",
    "learning",
    "often",
    "think",
    "going",
    "input",
    "something",
    "like",
    "going",
    "input",
    "image",
    "classifier",
    "da",
    "da",
    "da",
    "da",
    "comes",
    "certain",
    "class",
    "car",
    "like",
    "explained",
    "ability",
    "part",
    "image",
    "wheels",
    "hood",
    "part",
    "image",
    "part",
    "input",
    "image",
    "responsible",
    "making",
    "determination",
    "think",
    "especially",
    "language",
    "models",
    "model",
    "predicts",
    "something",
    "right",
    "next",
    "word",
    "think",
    "somehow",
    "method",
    "determining",
    "training",
    "examples",
    "model",
    "used",
    "interpolate",
    "given",
    "context",
    "pretty",
    "sure",
    "training",
    "find",
    "find",
    "example",
    "weight",
    "weight",
    "weight",
    "responsible",
    "making",
    "prediction",
    "happen",
    "pretty",
    "sure",
    "somehow",
    "training",
    "build",
    "index",
    "five",
    "training",
    "examples",
    "influence",
    "particular",
    "weight",
    "combination",
    "weights",
    "sort",
    "go",
    "backwards",
    "say",
    "made",
    "decision",
    "right",
    "model",
    "please",
    "tell",
    "training",
    "data",
    "samples",
    "responsible",
    "making",
    "decision",
    "actually",
    "pretty",
    "sure",
    "already",
    "exists",
    "like",
    "never",
    "first",
    "one",
    "think",
    "things",
    "though",
    "site",
    "may",
    "like",
    "channel",
    "interesting",
    "way",
    "think",
    "model",
    "interesting",
    "way",
    "think",
    "kind",
    "would",
    "explain",
    "ability",
    "even",
    "mean",
    "model",
    "like",
    "argument",
    "since",
    "interpolates",
    "training",
    "data",
    "interpretability",
    "come",
    "fact",
    "training",
    "samples",
    "interpolate",
    "okay",
    "let",
    "go",
    "tran",
    "halation",
    "translation",
    "said",
    "simply",
    "input",
    "like",
    "task",
    "examples",
    "output",
    "okay",
    "see",
    "right",
    "see",
    "model",
    "goes",
    "parameters",
    "performance",
    "generally",
    "increases",
    "also",
    "see",
    "performance",
    "pretty",
    "good",
    "every",
    "time",
    "model",
    "goes",
    "english",
    "goes",
    "target",
    "language",
    "english",
    "sort",
    "makes",
    "sense",
    "like",
    "large",
    "part",
    "corpus",
    "trained",
    "english",
    "english",
    "language",
    "model",
    "pretty",
    "good",
    "asked",
    "produce",
    "english",
    "good",
    "asked",
    "go",
    "different",
    "direction",
    "also",
    "see",
    "really",
    "difference",
    "whether",
    "translate",
    "language",
    "translate",
    "go",
    "english",
    "much",
    "matters",
    "language",
    "go",
    "english",
    "sort",
    "makes",
    "sense",
    "trained",
    "lot",
    "english",
    "data",
    "right",
    "sometimes",
    "par",
    "supervised",
    "methods",
    "also",
    "times",
    "outperform",
    "methods",
    "right",
    "methods",
    "unsupervised",
    "specifically",
    "supervised",
    "training",
    "data",
    "set",
    "goes",
    "let",
    "say",
    "english",
    "french",
    "built",
    "mind",
    "need",
    "translate",
    "later",
    "sort",
    "task",
    "specific",
    "supervised",
    "training",
    "set",
    "model",
    "right",
    "learns",
    "whatever",
    "learns",
    "language",
    "model",
    "learning",
    "end",
    "seen",
    "websites",
    "language",
    "things",
    "appear",
    "translate",
    "reasonably",
    "well",
    "okay",
    "yeah",
    "results",
    "bit",
    "noisy",
    "still",
    "interesting",
    "see",
    "sometimes",
    "even",
    "gets",
    "close",
    "supervised",
    "thing",
    "though",
    "say",
    "familiar",
    "literature",
    "sure",
    "model",
    "numbers",
    "know",
    "good",
    "okay",
    "okay",
    "next",
    "thing",
    "um",
    "winograd",
    "schemes",
    "text",
    "classic",
    "nlp",
    "task",
    "involves",
    "determining",
    "word",
    "pronoun",
    "refers",
    "pronoun",
    "grammatically",
    "ambiguous",
    "semantically",
    "unambiguous",
    "human",
    "sort",
    "human",
    "produced",
    "sentences",
    "kind",
    "program",
    "could",
    "refer",
    "multiple",
    "things",
    "example",
    "present",
    "right",
    "see",
    "model",
    "produce",
    "bert",
    "large",
    "produce",
    "roberta",
    "large",
    "going",
    "going",
    "come",
    "competing",
    "least",
    "models",
    "made",
    "specifically",
    "task",
    "right",
    "pretty",
    "pretty",
    "interesting",
    "also",
    "see",
    "larger",
    "models",
    "starts",
    "make",
    "difference",
    "whether",
    "give",
    "one",
    "zero",
    "one",
    "examples",
    "okay",
    "get",
    "get",
    "interesting",
    "things",
    "right",
    "thing",
    "right",
    "yes",
    "kind",
    "physical",
    "physical",
    "question",
    "physical",
    "qa",
    "bit",
    "common",
    "sense",
    "reasoning",
    "asked",
    "yeah",
    "like",
    "science",
    "questions",
    "multiple",
    "choice",
    "questions",
    "collected",
    "third",
    "ninth",
    "grade",
    "exams",
    "physical",
    "qa",
    "physical",
    "qa",
    "asks",
    "question",
    "physical",
    "word",
    "work",
    "world",
    "works",
    "intended",
    "probe",
    "grounded",
    "understanding",
    "world",
    "questions",
    "understand",
    "questions",
    "like",
    "drop",
    "ball",
    "fall",
    "ground",
    "fall",
    "something",
    "like",
    "say",
    "outperform",
    "model",
    "go",
    "high",
    "enough",
    "also",
    "see",
    "much",
    "difference",
    "zero",
    "one",
    "short",
    "methods",
    "model",
    "right",
    "even",
    "zero",
    "shot",
    "even",
    "higher",
    "one",
    "shot",
    "probably",
    "noise",
    "find",
    "asterisks",
    "means",
    "potentially",
    "contaminated",
    "data",
    "set",
    "potential",
    "contamination",
    "issues",
    "found",
    "significant",
    "overlap",
    "data",
    "set",
    "data",
    "set",
    "training",
    "data",
    "set",
    "even",
    "realized",
    "late",
    "bug",
    "deduplication",
    "code",
    "could",
    "change",
    "anymore",
    "like",
    "model",
    "large",
    "could",
    "restart",
    "training",
    "already",
    "spent",
    "like",
    "much",
    "money",
    "energy",
    "crazy",
    "think",
    "language",
    "models",
    "getting",
    "large",
    "building",
    "think",
    "like",
    "built",
    "international",
    "space",
    "station",
    "something",
    "like",
    "project",
    "humanity",
    "sort",
    "collaborates",
    "big",
    "effort",
    "build",
    "whatever",
    "right",
    "good",
    "numbers",
    "simply",
    "simply",
    "could",
    "influenced",
    "contamination",
    "think",
    "happening",
    "right",
    "even",
    "though",
    "make",
    "case",
    "contamination",
    "really",
    "issue",
    "probably",
    "show",
    "may",
    "may",
    "actually",
    "issue",
    "data",
    "sets",
    "model",
    "outperform",
    "gpt",
    "three",
    "quite",
    "bit",
    "also",
    "fact",
    "know",
    "provide",
    "demonstration",
    "many",
    "demonstrations",
    "actually",
    "change",
    "much",
    "kind",
    "tells",
    "model",
    "sort",
    "already",
    "knows",
    "answer",
    "really",
    "need",
    "demonstrations",
    "help",
    "training",
    "data",
    "stored",
    "test",
    "data",
    "really",
    "get",
    "demonstrations",
    "right",
    "things",
    "right",
    "cocoa",
    "tasks",
    "perform",
    "pretty",
    "poorly",
    "compared",
    "others",
    "poorly",
    "let",
    "say",
    "perform",
    "well",
    "particularly",
    "well",
    "state",
    "art",
    "perform",
    "especially",
    "poorly",
    "reading",
    "comprehension",
    "sorry",
    "cocoa",
    "reading",
    "comprehension",
    "abstractive",
    "multiple",
    "choice",
    "span",
    "based",
    "answer",
    "formats",
    "dialogue",
    "single",
    "question",
    "settings",
    "basically",
    "read",
    "piece",
    "text",
    "like",
    "answer",
    "question",
    "piece",
    "text",
    "something",
    "think",
    "really",
    "interpolate",
    "training",
    "data",
    "super",
    "well",
    "therefore",
    "ca",
    "really",
    "pattern",
    "match",
    "interpret",
    "actual",
    "reasoning",
    "think",
    "model",
    "performs",
    "poorly",
    "measure",
    "super",
    "glue",
    "nlp",
    "benchmark",
    "also",
    "see",
    "outperform",
    "model",
    "tasks",
    "outperform",
    "berthed",
    "model",
    "slightly",
    "word",
    "model",
    "things",
    "whereas",
    "gt3",
    "notice",
    "tasks",
    "well",
    "well",
    "compared",
    "model",
    "example",
    "book",
    "particularly",
    "well",
    "right",
    "state",
    "91",
    "76",
    "quite",
    "large",
    "difference",
    "actually",
    "glue",
    "benchmark",
    "open",
    "see",
    "bull",
    "queue",
    "example",
    "would",
    "france",
    "time",
    "zone",
    "uk",
    "like",
    "passage",
    "need",
    "reason",
    "passage",
    "whether",
    "answer",
    "true",
    "false",
    "okay",
    "much",
    "language",
    "modeling",
    "reasoning",
    "model",
    "poorly",
    "whereas",
    "another",
    "thing",
    "see",
    "example",
    "coppa",
    "right",
    "model",
    "almost",
    "good",
    "state",
    "art",
    "stress",
    "model",
    "never",
    "actually",
    "learned",
    "task",
    "supervised",
    "duay",
    "simply",
    "language",
    "model",
    "copo",
    "task",
    "right",
    "examples",
    "one",
    "example",
    "premise",
    "man",
    "broke",
    "toe",
    "cause",
    "two",
    "different",
    "things",
    "could",
    "either",
    "got",
    "hole",
    "sock",
    "dropped",
    "hammer",
    "foot",
    "way",
    "phrase",
    "model",
    "would",
    "give",
    "premise",
    "context",
    "simply",
    "ask",
    "model",
    "since",
    "language",
    "model",
    "two",
    "things",
    "probable",
    "come",
    "course",
    "going",
    "select",
    "thing",
    "happened",
    "often",
    "training",
    "data",
    "know",
    "broke",
    "toe",
    "cause",
    "breaking",
    "toe",
    "hammer",
    "entirely",
    "conceivable",
    "language",
    "would",
    "know",
    "enough",
    "training",
    "data",
    "could",
    "sort",
    "pull",
    "training",
    "data",
    "examples",
    "hammer",
    "foot",
    "broke",
    "toe",
    "appear",
    "bunch",
    "times",
    "hole",
    "sock",
    "would",
    "rather",
    "unrelated",
    "long",
    "questions",
    "adversarial",
    "constructed",
    "specifically",
    "language",
    "model",
    "ca",
    "solve",
    "model",
    "going",
    "perform",
    "pretty",
    "well",
    "right",
    "right",
    "interesting",
    "see",
    "view",
    "interpolating",
    "training",
    "data",
    "makes",
    "sense",
    "good",
    "good",
    "super",
    "glue",
    "nli",
    "performing",
    "particularly",
    "poorly",
    "nli",
    "ability",
    "understand",
    "relationship",
    "two",
    "sentences",
    "right",
    "model",
    "classifies",
    "whether",
    "second",
    "sentence",
    "logically",
    "follows",
    "first",
    "contradicts",
    "first",
    "possibly",
    "true",
    "neutral",
    "okay",
    "reasoning",
    "part",
    "model",
    "given",
    "simply",
    "recalling",
    "training",
    "data",
    "language",
    "modeling",
    "say",
    "oh",
    "test",
    "test",
    "synthetic",
    "qualitative",
    "tasks",
    "invent",
    "task",
    "sinks",
    "know",
    "pretty",
    "easy",
    "since",
    "model",
    "turn",
    "generate",
    "actual",
    "training",
    "set",
    "tasks",
    "focus",
    "generating",
    "test",
    "set",
    "know",
    "something",
    "like",
    "arithmetic",
    "say",
    "okay",
    "come",
    "bunch",
    "arithmetic",
    "tasks",
    "example",
    "two",
    "digit",
    "digit",
    "addition",
    "model",
    "would",
    "see",
    "would",
    "example",
    "model",
    "would",
    "see",
    "simply",
    "context",
    "right",
    "prompt",
    "give",
    "examples",
    "like",
    "learning",
    "would",
    "input",
    "add",
    "following",
    "numbers",
    "following",
    "numbers",
    "string",
    "right",
    "new",
    "line",
    "would",
    "give",
    "one",
    "example",
    "like",
    "11",
    "plus",
    "12",
    "answer",
    "together",
    "answer",
    "answer",
    "know",
    "23",
    "prompt",
    "goes",
    "48",
    "plus",
    "76",
    "ask",
    "next",
    "word",
    "right",
    "next",
    "string",
    "tok",
    "comes",
    "inference",
    "model",
    "manages",
    "ca",
    "simply",
    "strings",
    "model",
    "basically",
    "clue",
    "math",
    "numbers",
    "model",
    "tokens",
    "strings",
    "inference",
    "model",
    "must",
    "learned",
    "know",
    "kind",
    "reasoning",
    "ability",
    "must",
    "learned",
    "like",
    "perform",
    "logic",
    "inside",
    "go",
    "addition",
    "three",
    "digit",
    "addition",
    "four",
    "digit",
    "addition",
    "five",
    "digit",
    "addition",
    "even",
    "multiplication",
    "subtraction",
    "results",
    "right",
    "see",
    "lower",
    "parameter",
    "models",
    "perform",
    "pretty",
    "poorly",
    "go",
    "parameters",
    "big",
    "model",
    "performing",
    "really",
    "well",
    "range",
    "performing",
    "also",
    "really",
    "well",
    "accuracy",
    "look",
    "accuracy",
    "8090",
    "percent",
    "three",
    "digit",
    "addition",
    "subtraction",
    "soon",
    "get",
    "four",
    "digit",
    "two",
    "digit",
    "multiplication",
    "performance",
    "drops",
    "say",
    "multiplication",
    "harder",
    "know",
    "logically",
    "computationally",
    "know",
    "two",
    "digit",
    "addition",
    "model",
    "learned",
    "something",
    "world",
    "disagree",
    "simply",
    "simply",
    "recall",
    "training",
    "data",
    "look",
    "two",
    "digit",
    "addition",
    "zero",
    "shot",
    "already",
    "get",
    "seventies",
    "one",
    "shot",
    "get",
    "99",
    "shot",
    "get",
    "hundred",
    "percent",
    "interpret",
    "model",
    "simply",
    "filtering",
    "training",
    "data",
    "pattern",
    "match",
    "makes",
    "lot",
    "sense",
    "one",
    "shot",
    "would",
    "like",
    "examples",
    "give",
    "much",
    "improvement",
    "bunch",
    "examples",
    "please",
    "add",
    "right",
    "oh",
    "erased",
    "example",
    "like",
    "48",
    "plus",
    "72",
    "equals",
    "blah",
    "blah",
    "blah",
    "give",
    "example",
    "sudden",
    "looks",
    "like",
    "table",
    "say",
    "made",
    "sure",
    "strings",
    "particular",
    "strings",
    "training",
    "data",
    "right",
    "strings",
    "never",
    "appeared",
    "issue",
    "deduplication",
    "stuff",
    "appear",
    "actually",
    "appear",
    "table",
    "table",
    "often",
    "columns",
    "another",
    "column",
    "columns",
    "left",
    "asked",
    "pattern",
    "match",
    "naturally",
    "find",
    "websites",
    "right",
    "examples",
    "find",
    "websites",
    "columns",
    "exactly",
    "refer",
    "things",
    "find",
    "sum",
    "filter",
    "websites",
    "appear",
    "match",
    "scheme",
    "examples",
    "find",
    "website",
    "table",
    "column",
    "1",
    "column",
    "addition",
    "others",
    "actually",
    "went",
    "typed",
    "bunch",
    "things",
    "98",
    "plus",
    "45",
    "143",
    "18",
    "plus",
    "55",
    "70",
    "believe",
    "least",
    "find",
    "google",
    "makes",
    "hard",
    "localize",
    "everything",
    "still",
    "find",
    "going",
    "find",
    "tables",
    "tables",
    "tables",
    "tables",
    "actually",
    "went",
    "go",
    "basically",
    "say",
    "know",
    "know",
    "really",
    "personalize",
    "first",
    "thing",
    "find",
    "type",
    "numbers",
    "math",
    "skip",
    "counting",
    "missing",
    "sequence",
    "number",
    "website",
    "basically",
    "answers",
    "already",
    "given",
    "look",
    "model",
    "recall",
    "particular",
    "training",
    "example",
    "samples",
    "already",
    "right",
    "basically",
    "able",
    "quotes",
    "perform",
    "addition",
    "like",
    "financial",
    "data",
    "another",
    "one",
    "subtract",
    "stuff",
    "right",
    "pretty",
    "sure",
    "model",
    "interpolating",
    "training",
    "data",
    "also",
    "performs",
    "worse",
    "digits",
    "longer",
    "digit",
    "numbers",
    "simply",
    "less",
    "frequent",
    "training",
    "data",
    "multiplication",
    "first",
    "less",
    "frequent",
    "second",
    "also",
    "results",
    "larger",
    "numbers",
    "less",
    "frequent",
    "right",
    "explains",
    "lot",
    "yeah",
    "issues",
    "people",
    "saying",
    "yeah",
    "shows",
    "reasoning",
    "think",
    "thing",
    "word",
    "scramble",
    "word",
    "scramble",
    "different",
    "things",
    "see",
    "okay",
    "look",
    "whether",
    "17",
    "matches",
    "math",
    "things",
    "training",
    "data",
    "like",
    "searched",
    "well",
    "enough",
    "rest",
    "deduplication",
    "way",
    "also",
    "pretty",
    "weak",
    "would",
    "say",
    "look",
    "like",
    "13",
    "gram",
    "overlaps",
    "training",
    "data",
    "inde",
    "test",
    "data",
    "words",
    "scrambling",
    "tasks",
    "basically",
    "scramble",
    "words",
    "asked",
    "model",
    "unscramble",
    "example",
    "word",
    "inevitably",
    "scrambled",
    "always",
    "know",
    "give",
    "like",
    "anagrams",
    "give",
    "random",
    "insertion",
    "world",
    "like",
    "word",
    "right",
    "reverse",
    "word",
    "say",
    "think",
    "thing",
    "beginning",
    "see",
    "right",
    "also",
    "model",
    "goes",
    "improves",
    "also",
    "say",
    "well",
    "means",
    "maybe",
    "kind",
    "reasoning",
    "think",
    "learning",
    "language",
    "learning",
    "know",
    "words",
    "sorry",
    "letters",
    "make",
    "word",
    "letters",
    "correspond",
    "word",
    "pieces",
    "laura",
    "associated",
    "word",
    "pieces",
    "always",
    "learns",
    "english",
    "good",
    "tasks",
    "check",
    "would",
    "actually",
    "scramble",
    "words",
    "unscramble",
    "words",
    "always",
    "end",
    "english",
    "word",
    "basically",
    "check",
    "word",
    "highest",
    "overlap",
    "word",
    "pieces",
    "could",
    "something",
    "like",
    "please",
    "scramble",
    "word",
    "always",
    "count",
    "correctly",
    "scrambling",
    "words",
    "instead",
    "going",
    "simply",
    "solve",
    "knowing",
    "english",
    "language",
    "would",
    "basically",
    "clue",
    "task",
    "understand",
    "model",
    "could",
    "ask",
    "go",
    "given",
    "examples",
    "right",
    "would",
    "really",
    "need",
    "understand",
    "task",
    "supposed",
    "actually",
    "scramble",
    "word",
    "would",
    "would",
    "need",
    "learn",
    "context",
    "given",
    "examples",
    "far",
    "see",
    "think",
    "recalling",
    "training",
    "data",
    "sat",
    "analogies",
    "sat",
    "test",
    "us",
    "high",
    "schoolers",
    "take",
    "get",
    "college",
    "say",
    "typical",
    "example",
    "dying",
    "scrolled",
    "okay",
    "typical",
    "example",
    "following",
    "find",
    "find",
    "pretty",
    "hilarious",
    "audacious",
    "boldness",
    "sanctimonious",
    "hypocrisy",
    "anonymous",
    "identity",
    "remorseful",
    "still",
    "missed",
    "deleterious",
    "result",
    "impressionable",
    "temptation",
    "okay",
    "native",
    "speaker",
    "hard",
    "question",
    "right",
    "know",
    "see",
    "stressed",
    "like",
    "much",
    "test",
    "need",
    "make",
    "decision",
    "quickly",
    "well",
    "model",
    "course",
    "basically",
    "able",
    "sift",
    "entire",
    "training",
    "data",
    "time",
    "takes",
    "gpus",
    "perform",
    "inference",
    "still",
    "funny",
    "gt3",
    "achieves",
    "fifty",
    "sixty",
    "five",
    "percent",
    "shots",
    "setting",
    "percent",
    "one",
    "shot",
    "setting",
    "fifty",
    "three",
    "percent",
    "zero",
    "short",
    "setting",
    "whereas",
    "average",
    "score",
    "among",
    "college",
    "applicants",
    "fifty",
    "seven",
    "percent",
    "outperforms",
    "average",
    "college",
    "applicant",
    "pretty",
    "funny",
    "would",
    "expect",
    "language",
    "model",
    "pretty",
    "good",
    "grasp",
    "kind",
    "synonyms",
    "relations",
    "words",
    "absolutely",
    "statistical",
    "associations",
    "words",
    "yeah",
    "found",
    "pretty",
    "pretty",
    "funny",
    "last",
    "thing",
    "everyone",
    "freaking",
    "news",
    "article",
    "generation",
    "basically",
    "give",
    "beginning",
    "news",
    "article",
    "let",
    "humans",
    "decide",
    "whether",
    "news",
    "article",
    "written",
    "machine",
    "human",
    "say",
    "contrast",
    "mean",
    "human",
    "accuracy",
    "detecting",
    "articles",
    "produced",
    "one",
    "hundred",
    "seventy",
    "five",
    "billion",
    "parameter",
    "model",
    "barely",
    "chance",
    "fifty",
    "two",
    "percent",
    "human",
    "abilities",
    "detect",
    "model",
    "generated",
    "text",
    "appear",
    "decrease",
    "model",
    "size",
    "increases",
    "appears",
    "trend",
    "towards",
    "chance",
    "accuracy",
    "model",
    "size",
    "human",
    "detection",
    "g",
    "pt",
    "three",
    "close",
    "chance",
    "okay",
    "give",
    "indeed",
    "examples",
    "right",
    "give",
    "model",
    "following",
    "input",
    "title",
    "subtitle",
    "article",
    "word",
    "article",
    "model",
    "supposed",
    "complete",
    "rest",
    "article",
    "right",
    "also",
    "know",
    "give",
    "shots",
    "setting",
    "model",
    "basically",
    "knows",
    "give",
    "examples",
    "model",
    "knows",
    "supposed",
    "produce",
    "news",
    "article",
    "right",
    "okay",
    "two",
    "two",
    "ways",
    "think",
    "first",
    "way",
    "model",
    "learned",
    "language",
    "well",
    "writes",
    "code",
    "learned",
    "write",
    "coherent",
    "language",
    "learn",
    "reason",
    "keep",
    "context",
    "blah",
    "blah",
    "blah",
    "okay",
    "second",
    "way",
    "model",
    "sees",
    "thing",
    "right",
    "sees",
    "know",
    "k",
    "shot",
    "examples",
    "context",
    "take",
    "filter",
    "training",
    "data",
    "case",
    "sees",
    "news",
    "articles",
    "news",
    "articles",
    "take",
    "thing",
    "filter",
    "training",
    "data",
    "even",
    "news",
    "articles",
    "pertain",
    "largely",
    "topics",
    "words",
    "appear",
    "lastly",
    "interpolate",
    "training",
    "examples",
    "produce",
    "thing",
    "argue",
    "really",
    "possible",
    "actually",
    "checked",
    "news",
    "article",
    "training",
    "data",
    "simply",
    "gone",
    "taken",
    "really",
    "taken",
    "random",
    "substring",
    "taken",
    "substring",
    "voted",
    "strengthen",
    "ban",
    "ordination",
    "substring",
    "put",
    "google",
    "bob",
    "reba",
    "find",
    "book",
    "voted",
    "strengthen",
    "prohibitions",
    "ban",
    "lgbtq",
    "people",
    "ordained",
    "ministers",
    "know",
    "find",
    "article",
    "talking",
    "incident",
    "article",
    "talks",
    "using",
    "language",
    "probably",
    "read",
    "article",
    "author",
    "like",
    "ca",
    "really",
    "know",
    "copy",
    "paste",
    "would",
    "know",
    "really",
    "cool",
    "kind",
    "know",
    "write",
    "words",
    "largely",
    "thing",
    "associated",
    "press",
    "also",
    "different",
    "article",
    "know",
    "see",
    "title",
    "one",
    "right",
    "thing",
    "also",
    "language",
    "right",
    "voted",
    "tuesday",
    "strengthen",
    "faiths",
    "divisive",
    "bans",
    "marriage",
    "ordination",
    "lgbt",
    "clergy",
    "generally",
    "argument",
    "article",
    "training",
    "data",
    "really",
    "something",
    "buy",
    "case",
    "think",
    "article",
    "many",
    "articles",
    "topics",
    "think",
    "interpolate",
    "say",
    "hardest",
    "article",
    "humans",
    "decide",
    "easiest",
    "says",
    "know",
    "star",
    "talks",
    "promise",
    "draws",
    "megyn",
    "kelly",
    "sarcasm",
    "says",
    "year",
    "ago",
    "joke",
    "phoenix",
    "made",
    "headlines",
    "appeared",
    "red",
    "carpet",
    "golden",
    "globes",
    "wearing",
    "tuxedo",
    "paper",
    "bag",
    "head",
    "read",
    "shapeshifter",
    "would",
    "guess",
    "joke",
    "phoenix",
    "would",
    "something",
    "like",
    "say",
    "human",
    "raiders",
    "us",
    "based",
    "right",
    "see",
    "right",
    "says",
    "men",
    "kelly",
    "impressed",
    "let",
    "tonight",
    "show",
    "another",
    "tonight",
    "show",
    "megyn",
    "kelly",
    "people",
    "would",
    "guess",
    "know",
    "something",
    "like",
    "would",
    "immediately",
    "feel",
    "like",
    "wrong",
    "think",
    "thing",
    "interpolated",
    "interpolated",
    "bunch",
    "different",
    "news",
    "articles",
    "interpolation",
    "let",
    "like",
    "made",
    "teach",
    "person",
    "show",
    "humans",
    "noticed",
    "right",
    "well",
    "change",
    "fact",
    "probably",
    "went",
    "training",
    "data",
    "filtered",
    "bunch",
    "articles",
    "words",
    "interpolated",
    "like",
    "mash",
    "together",
    "good",
    "language",
    "model",
    "right",
    "grammar",
    "good",
    "grammar",
    "interpolate",
    "different",
    "passages",
    "text",
    "feel",
    "really",
    "really",
    "useful",
    "application",
    "sort",
    "search",
    "engine",
    "fuzzy",
    "search",
    "engine",
    "like",
    "input",
    "example",
    "machine",
    "learning",
    "research",
    "ideas",
    "output",
    "sort",
    "abstract",
    "paper",
    "kind",
    "merge",
    "together",
    "papers",
    "thing",
    "know",
    "think",
    "many",
    "applications",
    "think",
    "built",
    "something",
    "really",
    "intelligent",
    "though",
    "pretty",
    "cool",
    "give",
    "examples",
    "like",
    "make",
    "world",
    "ask",
    "model",
    "use",
    "word",
    "sentence",
    "skree",
    "something",
    "sorry",
    "screech",
    "something",
    "swing",
    "sword",
    "example",
    "sentence",
    "uses",
    "word",
    "scree",
    "course",
    "model",
    "models",
    "going",
    "going",
    "take",
    "going",
    "filter",
    "training",
    "data",
    "instances",
    "sort",
    "construction",
    "appears",
    "like",
    "example",
    "using",
    "word",
    "mostly",
    "dictionaries",
    "going",
    "know",
    "word",
    "interpolate",
    "interpolate",
    "data",
    "right",
    "cool",
    "thing",
    "actually",
    "conjugates",
    "screed",
    "several",
    "minutes",
    "went",
    "outside",
    "ate",
    "ice",
    "cream",
    "see",
    "comes",
    "think",
    "would",
    "really",
    "fun",
    "model",
    "tells",
    "us",
    "training",
    "data",
    "samples",
    "used",
    "also",
    "correct",
    "english",
    "grammar",
    "pretty",
    "obvious",
    "though",
    "never",
    "correct",
    "input",
    "always",
    "poor",
    "english",
    "good",
    "english",
    "poor",
    "english",
    "good",
    "image",
    "poor",
    "good",
    "poor",
    "english",
    "good",
    "english",
    "model",
    "asked",
    "output",
    "actually",
    "sure",
    "pretty",
    "sure",
    "bold",
    "fairly",
    "sure",
    "bold",
    "given",
    "model",
    "model",
    "asked",
    "produce",
    "otherwise",
    "actually",
    "impressed",
    "yes",
    "nothing",
    "provided",
    "aside",
    "examples",
    "example",
    "conditioning",
    "poor",
    "english",
    "input",
    "good",
    "english",
    "output",
    "framing",
    "good",
    "english",
    "output",
    "thing",
    "boldface",
    "authors",
    "listening",
    "bold",
    "thank",
    "okay",
    "always",
    "see",
    "good",
    "english",
    "always",
    "target",
    "good",
    "english",
    "whereas",
    "model",
    "really",
    "understood",
    "task",
    "also",
    "able",
    "inverse",
    "able",
    "produce",
    "something",
    "poor",
    "something",
    "good",
    "eliminate",
    "fact",
    "good",
    "english",
    "language",
    "model",
    "right",
    "basically",
    "produce",
    "something",
    "like",
    "without",
    "clue",
    "task",
    "simply",
    "condition",
    "input",
    "simply",
    "output",
    "sentence",
    "likely",
    "already",
    "almost",
    "output",
    "better",
    "english",
    "good",
    "language",
    "model",
    "right",
    "good",
    "english",
    "language",
    "model",
    "yeah",
    "measure",
    "overfitting",
    "degree",
    "training",
    "test",
    "data",
    "common",
    "crawl",
    "thing",
    "say",
    "conservative",
    "bound",
    "many",
    "percent",
    "data",
    "data",
    "set",
    "clean",
    "see",
    "measure",
    "much",
    "performance",
    "differs",
    "evaluate",
    "clean",
    "portion",
    "data",
    "set",
    "deduplication",
    "weak",
    "like",
    "engram",
    "deduplication",
    "whereas",
    "think",
    "really",
    "like",
    "news",
    "articles",
    "really",
    "much",
    "fuzzy",
    "deduplication",
    "much",
    "meaning",
    "deduplication",
    "want",
    "argue",
    "model",
    "learned",
    "reason",
    "like",
    "simply",
    "want",
    "argue",
    "model",
    "good",
    "language",
    "model",
    "fine",
    "right",
    "yeah",
    "also",
    "look",
    "like",
    "would",
    "expect",
    "dataset",
    "test",
    "dataset",
    "know",
    "like",
    "natural",
    "questions",
    "dataset",
    "constructed",
    "wikipedia",
    "pages",
    "wikipedia",
    "page",
    "either",
    "either",
    "entire",
    "thing",
    "clean",
    "none",
    "clean",
    "also",
    "winograd",
    "dataset",
    "dataset",
    "somehow",
    "leaked",
    "common",
    "crawl",
    "corpus",
    "either",
    "entire",
    "thing",
    "clean",
    "none",
    "clean",
    "kind",
    "problems",
    "fact",
    "many",
    "things",
    "right",
    "yeah",
    "convinced",
    "deduplication",
    "still",
    "think",
    "cool",
    "thing",
    "think",
    "mostly",
    "training",
    "data",
    "filter",
    "interpolator",
    "rather",
    "actual",
    "reasoning",
    "go",
    "limitations",
    "broader",
    "broader",
    "impact",
    "statements",
    "like",
    "five",
    "pages",
    "long",
    "yeah",
    "okay",
    "know",
    "bad",
    "people",
    "take",
    "model",
    "bad",
    "things",
    "okay",
    "pretty",
    "much",
    "appreciate",
    "bottom",
    "basically",
    "results",
    "also",
    "lot",
    "tasks",
    "descriptions",
    "like",
    "framed",
    "tasks",
    "outputs",
    "gave",
    "outputs",
    "website",
    "rightly",
    "see",
    "tasks",
    "framed",
    "always",
    "model",
    "sees",
    "asked",
    "produce",
    "right",
    "many",
    "things",
    "squad",
    "context",
    "question",
    "okay",
    "context",
    "actually",
    "know",
    "context",
    "question",
    "model",
    "asked",
    "complete",
    "something",
    "right",
    "look",
    "model",
    "sees",
    "tasks",
    "maybe",
    "evaluate",
    "think",
    "difficult",
    "think",
    "tasks",
    "alright",
    "hope",
    "informative",
    "long",
    "paper",
    "therefore",
    "long",
    "video",
    "still",
    "subscribed",
    "yet",
    "maybe",
    "like",
    "want",
    "de",
    "leave",
    "like",
    "tell",
    "comments",
    "think",
    "whether",
    "think",
    "actually",
    "igi",
    "see",
    "next",
    "time"
  ],
  "keywords": [
    "language",
    "models",
    "shot",
    "paper",
    "also",
    "called",
    "gpt",
    "three",
    "model",
    "comes",
    "open",
    "basically",
    "giant",
    "order",
    "larger",
    "built",
    "absolutely",
    "crazy",
    "things",
    "go",
    "architecture",
    "results",
    "train",
    "enough",
    "data",
    "able",
    "nlp",
    "tasks",
    "never",
    "look",
    "cool",
    "kind",
    "see",
    "long",
    "table",
    "going",
    "bit",
    "first",
    "know",
    "bunch",
    "natural",
    "specifically",
    "transformer",
    "let",
    "take",
    "example",
    "sentence",
    "right",
    "like",
    "humans",
    "large",
    "supervised",
    "sets",
    "learn",
    "english",
    "would",
    "end",
    "tell",
    "next",
    "input",
    "part",
    "word",
    "trained",
    "one",
    "ask",
    "question",
    "likely",
    "come",
    "way",
    "thing",
    "sort",
    "text",
    "use",
    "common",
    "crawl",
    "set",
    "filter",
    "entire",
    "internet",
    "together",
    "wikipedia",
    "get",
    "second",
    "much",
    "log",
    "scale",
    "several",
    "lot",
    "better",
    "modeling",
    "size",
    "remember",
    "something",
    "billion",
    "parameters",
    "whereas",
    "made",
    "well",
    "compared",
    "multiple",
    "attention",
    "layers",
    "layer",
    "even",
    "description",
    "need",
    "want",
    "context",
    "already",
    "different",
    "goes",
    "make",
    "supposed",
    "put",
    "words",
    "piece",
    "bert",
    "always",
    "say",
    "said",
    "simply",
    "okay",
    "interesting",
    "pre",
    "training",
    "teach",
    "fine",
    "tuning",
    "ca",
    "write",
    "task",
    "actually",
    "interested",
    "blah",
    "database",
    "good",
    "test",
    "dataset",
    "machine",
    "learning",
    "corpus",
    "could",
    "french",
    "translation",
    "change",
    "mean",
    "world",
    "argue",
    "answering",
    "evaluate",
    "many",
    "big",
    "perform",
    "sometimes",
    "zero",
    "fashion",
    "true",
    "following",
    "prompt",
    "asked",
    "understand",
    "somewhere",
    "translate",
    "find",
    "website",
    "hope",
    "produce",
    "string",
    "cheese",
    "provide",
    "inference",
    "time",
    "really",
    "means",
    "interpret",
    "new",
    "sees",
    "human",
    "makes",
    "sense",
    "give",
    "either",
    "think",
    "questions",
    "given",
    "since",
    "knows",
    "show",
    "follows",
    "course",
    "actual",
    "went",
    "fact",
    "queen",
    "england",
    "examples",
    "qa",
    "answer",
    "sorry",
    "state",
    "yeah",
    "setting",
    "learned",
    "performs",
    "outperform",
    "domain",
    "pretty",
    "maybe",
    "still",
    "happening",
    "learns",
    "weights",
    "people",
    "reason",
    "five",
    "samples",
    "k",
    "please",
    "pattern",
    "match",
    "interpolate",
    "reasoning",
    "image",
    "da",
    "ability",
    "sure",
    "weight",
    "though",
    "output",
    "performance",
    "difference",
    "whether",
    "methods",
    "websites",
    "appear",
    "numbers",
    "physical",
    "probably",
    "deduplication",
    "poorly",
    "another",
    "toe",
    "two",
    "digit",
    "addition",
    "plus",
    "strings",
    "multiplication",
    "accuracy",
    "percent",
    "tables",
    "scramble",
    "fifty",
    "news",
    "article",
    "articles",
    "poor",
    "clean"
  ]
}