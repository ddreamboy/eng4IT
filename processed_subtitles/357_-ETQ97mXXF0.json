{
  "text": "Undoubtedly, Data Science is\nthe most revolutionary\ntechnology of the era.\nIt's all about deriving\nuseful insights from data\nin order to solve\nreal-world complex problems.\nHi all I welcome you\nto this session\non Data Science full course\nthat contains everything\nthat you need to know\nin order to master data science.\nNow before we get started,\nlet's take a look\nat the agenda.\nThe first module is\nan reduction to data science\nthat covers all\nthe basic fundamentals of\ndata science followed by this.\nWe have statistics\nand probability module\nwhere you'll understand\nthe statistics and math\nbehind data science\nand machine learning algorithms.\nThe next module is the basics\nof machine learning\nwhere will understand what\nexactly machine learning is\nthe different types\nof machine learning\nthe different machine\nlearning algorithms\nand so on the next module\nis the supervised learning\nalgorithms module\nwhere we'll start\nby understanding the most\nbasic With them or\nwhich is linear regression.\nThe next module is\nthe logistic regression module\nwhere we will see\nhow logistic regression\ncan be used to solve\nclassification problems.\nAfter this we'll discuss\nabout decision trees\nand we'll see\nhow decision trees\ncan be used to solve\ncomplex data-driven problems.\nThe next module is random Forest\nhere will understand\nhow random Forest can be used\nto solve classification problems\nand regression problems\nwith the help\nof use cases and examples.\nThe next module\nwill be be discussing is\nthe k-nearest neighbor module.\nWe will understand how gain\nand can be used to solve\ncomplex classification problems\nfollowed by this.\nWe look at the\nnaive bias module,\nwhich is one of the most\nimportant algorithms\nin the Gmail spam detection.\nThe next algorithm\nis support Vector machine\nwhere we will understand\nhow svm's can be used\nto draw a hyperplane between\ndifferent classes of data.\nFinally.\nWe move on to the unsupervised\nlearning module where we\nwill understand how genes\ncan be used for clustering.\nAnd how you can perform\nMarket Basket analysis by using\nAssociation rule mining.\nThe next module\nis reinforcement learning\nwhere we will understand\nthe different concepts\nof reinforcement learning\nalong with a couple\nof demonstrations\nfollowed by this bill.\nLook at the Deep learning module\nwhere we will understand what\nexactly deep learning is what\nour neural networks\nwith different types\nof neural networks.\nAnd so on.\nThe last module is\nthe data science\ninterview questions module\nwhere we will understand\nthe important concepts of data.\nAlong with a few tips in order\nto Ace the interview now\nbefore we get started\nmake sure you subscribe\nto Adorama YouTube channel\nin order to stay updated\nabout the most trending\nTechnologies data science is one\nof the most in-demand\nTechnologies right now.\nNow this is probably\nbecause we're generating data\nat an Unstoppable pace.\nAnd obviously we need to process\nand make sense out\nof this much data.\nThis is exactly\nwhere data science comes\nin in today's session.\nWe'll be talking\nabout data science in depth.\nSo let's move ahead and take\na look at today's agenda.\nWe're going to begin\nwith discussing the various\nsources of data and\nhow the evolution of technology\nand introduction of IOD\nand social media have led\nto the need of data sign next.\nWe'll discuss how Walmart\nis using insightful patterns\nfrom their database to increase\nthe potential of their business.\nAfter that.\nWe will see what\nexactly data science is,\nthen we'll move on and discuss\nwho are data scientist is\nwhere we will also discuss\nthe various skill sets.\nNeeded to become\na data scientist next\nwe can move on to see\nthe various data\nscience job roles\nsuch as data analyst data\narchitect data engineer\nand so on after this we\nwill cover the data life cycle\nwhere we will discuss\nhow data is extracted processed\nand finally use as a solution.\nOnce we're done with that.\nWe'll cover the basics\nof machine learning\nwhere we'll see what\nexactly machine learning is\nand the different types\nof machine learning next.\nWe will move onto\nthe K means algorithm\nand we'll discuss a use case\nof the k-means clustering\nafter which we Discuss\nthe various steps involved\nin the k-means algorithm\nand then we will finally move on\nto the Hands-On part\nwhere we use the k-means\nalgorithm to Cluster movies\nbased on their popularity\non social media platforms,\nlike Facebook at the end\nof today's session\nwill also discuss about what\na data science certification is\nand why you should take it up.\nSo guys, there's a lot to cover\nin today's session.\nLet's jump into the first topic.\nDo you guys remember the times\nwhen we have telephones and we\nhad to go to PC your boots\nin order to make a phone call.\nCall now those things\nare very simple\nbecause we didn't generate\na lot of data.\nWe didn't even store\nthe contacts and our phones\nor our telephones.\nWe used to memorize phone\nnumbers back then or you know,\nthese have a diary\nof all our contact\nbut these days\nwe have smartphones\nwith store a lot of data.\nSo there's everything\nabout us in our mobile phones.\nWe have images we have contacts.\nWe have various apps.\nWe have games.\nEverything is stored\non a mobile phones these days\nsimilarly the PCS that we use\nin the earlier times.\nIt used to process\nvery little data.\nAll right, there was A lot\nof data processing needed\nbecause technology was\nan evolved that much.\nSo if you guys remember\nwe use floppy disk\nback then and floppy.\nThis was used to store\nsmall amounts of data,\nbut later on hard disks\nwere created and those\nused to store GBS of data.\nBut now if you look\naround there's data\neverywhere around us.\nAll right, we have a data\nstored in the cloud.\nWe have data in each and every\nAppliance at our houses.\nSimilarly.\nIf you look at smart cars\nthese days they're connected\nto the internet they connected\nto a mobile phones\nand this also generates\na lot of data.\nWhat we don't realize is\nthat evolution of technology\nhas generated a lot of data.\nAll right.\nNow initially there\nwas very little data\nand most of it was even\nstructured only a small part\nof the data was unstructured\nor semi-structured.\nAnd in those days you could use\nSimple bi Tools in order\nto process all of this data\nand make sense out of it.\nBut now we have way\ntoo much data and order\nto process this much data.\nWe need more complex algorithms.\nWe need a better process.\nAll right, and this is\nwhere data science\ncomes in now guys,\nI'm not going to get\ninto the depth of data science.\nYet I'm sure all\nof you have heard of iot\nor Internet of things.\nNow.\nDid you guys know\nthat we produce\n2.5 quintillion bytes\nof data each day.\nAnd this is only accelerating\nwith the growth of iot.\nNow iot or Internet\nof Things is just a fancy term\nthat we use for network\nof tools or devices\nthat communicate and transfer\ndata through the internet.\nSo various devices\nare connected to each other\nthrough the internet\nand they communicate\nwith each other right\nnow the communication happens\nby exchange of data or by.\nGeneration of data now these\ndevices include the vehicles.\nWe drive the include our TVs\nof coffee machines\nrefrigerators washing machines\nand almost everything else\nthat we use in a daily basis.\nNow, these interconnected\ndevices produce an unimaginable\namount of data guys iot data\nis measured in zettabytes\nand one zettabyte is equal\nto trillion gigabytes.\nSo according to a recent\nsurvey by Cisco.\nIt's estimated that by\nthe end of 2019,\nwhich is almost here.\nThe iot will generate more\nthan five hundred zettabytes\nof data per year.\nAnd this number will only\nincrease through time.\nIt's hard to imagine data\nin that much volume,\nimagine processing analyzing\nand managing this much of data.\nIt's only going\nto cause as a migraine\nso guys having to deal\nwith this much data\nis not something that\ntraditional bi tools can do.\nOkay.\nWe no longer can rely\non traditional data\nprocessing methods.\nThat's exactly why\nwe need data science.\nIt's our only hope right\nnow now let's not get\ninto the details here.\nYet moving on.\nLet's see how social\nmedia is adding on\nto the generation of data.\nNow the fact\nthat we are all in love\nwith social media.\nIt's actually generating\na lot of data for us.\nOkay.\nIt's certainly one of the fuels\nfor data creation Now\nall these numbers\nthat you see on the screen\nare generated every minute\nof the day.\nOkay, and this number\nis just going to increase so\nfor Instagram it says\nthat approximately\n1.7 million pictures uploaded\nin a minute and similarly\non Twitter approximately.\nA hundred and forty eight\nthousand tweets are published\nevery minute of the day.\nSo guys imagine in one are\nhow much that would be\nand then imagine in 24 hours.\nSo guys, this is\nthe amount of data\nthat is generated\nthrough social media.\nIt's unimaginable.\nImagine processing this much\ndata analyzing it and then\ntrying to figure out, you know,\nthe important insights\nfrom this much data analyzing\nthis much data is going to be\nvery hard with traditional tools\nor traditional methods.\nThat's why data science\nwas introduced data science\nis a simple process\nthat will just extract the\nuseful information from data.\nAll right, it's just\ngoing to process\nand analyze the entire data\nand then it's just\ngoing to extract\nwhat is needed now guys apart\nfrom social media and iot,\nthere are other factors as well\nwhich contribute to\ndata generation these days\nall our transactions\nare done online, right?\nWe pay bills online.\nWe shop online.\nWe even buy homes online\nthese days you can even sell\nyour pets on oil excuses.\nNot only that\nwhen we stream music\nand Watch videos on YouTube all\nof this is generating a lot\nof data not to forget.\nWe've also brought Health Care\ninto the internet wall.\nNow there are various\nwatches like bit fit\nwhich basically trans\nour heart rate\nand it generates data about\na health conditions education is\nalso an online thing right now.\nThat's exactly what you\nare doing right now.\nSo with the emergence\nof the internet,\nwe now perform all\nour activities online.\nOkay, obviously, this\nis helping us,\nbut we are unaware of\nhow much data we are generating\nwhat can be done with All\nof this data and what\nif we could use the data\nthat we generated\nto our benefit?\nWell, that's exactly\nwhat data science\ndoes data science is all\nabout extracting the useful\ninsights from data and using\nit to grow your business.\nNow before we get into\nthe details of data science,\nlet's see how Walmart uses data\nscience to grow that business.\nSo guys Walmart is\nthe world's biggest retailer\nwith over 20,000 stores\nin just 28 countries.\nOkay.\nNow, it's currently building\nthe world's biggest.\nGood Cloud,\nwhich will be able to process\ntwo point five petabytes\nof data every hour now.\nThe reason behind\nWalmart success is\nhow the user customer data\nto get useful insights about\ncustomers shopping patterns.\nNow the data analyst and\nthe data scientist at Walmart.\nThey know every detail\nabout their customers.\nThey know that\nif a customer buys Pop-Tarts,\nthey might also buy cookies,\nhow do they know all of this?\nLike how do they generate\ninformation like this now\nthe user data that they get\nfrom their customers.\nHours and the analyze it\nto see what a particular\ncustomer is looking for.\nNow.\nLet's look at a few cases\nwhere Walmart actually\nanalyze the data\nand they figured out\nthe customer needs.\nSo let's consider the Halloween\nand the cookie sales example now\nduring Halloween sales Analyst\nat Walmart took\na look at the data.\nOkay, and he found out\nthat a specific\ncookie was popular\nacross all Walmart stores.\nSo every Walmart store was\nselling these cookies very well,\nbut he found out\nthat they would to stores\nwhich are not selling.\nA DOT.\nOkay.\nSo the situation was immediately\ninvestigated and it was found\nthat there was\na simple stocking oversight.\nOkay, because of which\nthe cookies were not put\non the shelves for sale.\nSo because this issue\nwas immediately identified\nthey prevented any further loss\nof sales now\nanother such example,\nis that true Association\nrule mining Walmart found out\nthat strawberry Pop-Tart sales\nincreased by seven times\nbefore a hurricane.\nSo a data analyst at Walmart\nidentified the association\nbetween ha Hurricane\nand strawberry pop tarts\nthrough data mining now guys.\nDon't ask me the relationship\nbetween Pop-Tarts\nand Harry Caine,\nbut for some reason whenever\nthere was a hurricane\napproaching people really wanted\nto eat strawberry Pop-Tart.\nSo what Walmart did\nwas they place all\nthe strawberry Pop-Tarts?\nI will check out\nbefore a hurricane would occur.\nSo this way the increase sales\nof the Pop-Tarts Now,\nwhere's this is a natural thing.\nI'm not making it up.\nYou can look it up\non the internet.\nNot only that Walmart\nis analyzing the data generated\nby Social media to find out\nall the training product so\nthrough social media.\nYou can find out the likes\nand dislikes of a person right?\nSo what Walmart did is\nthey are quite smart\nthe user data generated\nby social media to find out\nwhat products are trending\nor what products\nare liked by customers.\nOkay an example\nof this is 1 mod analyze\nsocial media data to find out\nthat Facebook users were crazy\nabout cake pops.\nOkay, so Walmart\nimmediately took a decision\nand they introduced cake pops\ninto the Walmart stores.\nSo guys the only reason\nWalmart is so successful is\nbecause the huge amount of data\nthat they get they don't see\nit as a burden instead.\nThey process this data analyze\nit and then you try to draw\nuseful insights from it.\nOkay, so they invest a lot\nof money a lot of effort\nand a lot of time\nand data analysis.\nOkay, they spend a lot\nof time analyzing data in order\nto find any hidden patterns.\nSo as soon as they find out\nhidden pattern or association\nbetween any two products,\nthese are giving out offers\nor Started having discount\nor something along that line.\nSo basically Walmart uses data\nin a very effective manner\nthe analyzer very, well.\nThey process the data very well\nand they find out\nthe useful insights\nthat they need in order to get\nmore customers or in order\nto improve their business.\nSo guys, this was all\nabout how Walmart uses\ndata science now,\nlet's move ahead and look\nat what is data set now\nguys data science is all about\nuncovering findings from data.\nIt's all about surfacing\nthe hidden insights\nthat can help.\nPonies to make\nsmart business decisions.\nSo all these hidden insights\nor these hidden patterns can\nbe used to make better decisions\nin a business now an example\nof this is also Netflix.\nSo Netflix, basically analyzes\nthe movie viewing patterns\nof users to understand\nwhat drives user interest\nand to see what users want\nto watch and then\nonce they find out\nthey give people\nwhat they want.\nSo guys actually data\nhas a lot of power.\nYou should just know\nhow to process this data\nand how to extract\nthe useful information.\nFrom data.\nOkay.\nThat's what data\nscience is all about.\nSo guys a big question\nover here is\nhow do data scientists get\nuseful insights from data.\nSo it's all starts\nwith data exploration.\nWhenever a data scientist comes\nacross any challenging question\nor any sort\nof challenging situation,\nthey become detectives so\nthe investigative leads\nand they try to understand\nthe different patterns\nor the different\ncharacteristics of the data.\nOkay.\nThey try to get\nall the information\nthat they can from the data\nand then Then they use it\nfor the betterment\nof the organization\nor the business.\nNow, let's look at\nwho is a data scientist.\nSo guys the data scientists\nhas to be able to view data\nthrough a quantitative lengths.\nSo guys knowing math is one\nof the very important skills\nof data scientists.\nOkay.\nSo mathematics is important\nbecause in order to find\na solution you're going to build\na lot of predictive models\nand these predictive models are\ngoing to be based on hard math.\nSo you have to be able\nto understand all\nthe Underlying mechanics\nwith these models most\nof the predictive models most\nof the algorithms\nrequire mathematics.\nNow, there's a\nmajor misconception\nthat data science is\nall about statistics.\nNow, I'm not saying\nthat statistics is an important.\nIt is very important,\nbut it's not the only type\nof math that is utilized\nin data science.\nThere are actually\nmany machine learning algorithms\nwhich are based\non linear algebra.\nSo guys overall you need\nto have a good understanding\nof math and apart\nfrom that data scientist.\nEli's technology,\nso data scientists have to be\nreally good with technology.\nOkay.\nSo their main work is\nthey utilize all the technology\nso that they can analyze\nthese enormous data sets and\nwork with complex algorithms.\nSo all of this requires tools,\nwhich are much more\nsophisticated than Excel\nso there's data scientist need\nto be very efficient\nwith coding languages\nand few of the core language\nhas associated with data science\ninclude SQL python R & sass.\nIt is also important\nfor a data scientist.\nBe a tactical\nbusiness consultant.\nSo guys business problems can be\non a sword by data scientist\nsince our data scientists\nwork so closely with data\nthey know everything\nabout the business.\nIf you have a business\nand you give the entire data set\nof your business\nstored data scientist,\nhe know each and every aspect\nof your business.\nOkay?\nThat's how data scientists work.\nThey get the entire data set.\nThey study the data set\nthe analyze it and then we see\nwhere things are going wrong\nor what needs to be\ndone more or what?\nNeeds to be excluded.\nSo guys having this business\nAcumen is just as important\nas having skills\nin algorithms or being good\nwith math and technology.\nSo guys business is\nalso as important as\nthese other fields now,\nyou know who\nour data scientist is.\nLet's look at the skill sets\nthat a data scientist names.\nOkay, it always starts\nwith Statistics statistics\nwill give you the numbers\nfrom the data.\nSo a good understanding\nof Statistics is very important\nfor becoming a data scientist.\nYou have to be familiar\nwith satisfaction.\nContest distributions maximum\nlikelihood estimators and all\nof that apart\nfrom that you should also\nhave a good understanding\nof probability Theory\nand descriptive statistics.\nThese Concepts will help you\nmake Better Business decisions.\nSo no matter what type\nof company or role\nyou're interviewing for.\nYou're going to be\nexpected to know\nhow to use the tools\nof the trade.\nOkay.\nThis means that you have\nto know a statistical\nprogramming language like our\nor Python and also you'll need\nto know or database.\nWiring language like SQL now\nthe main reason why\npeople prefer our\nand python is because of\nthe number of packages\nthat these languages have\nand these predefined\npackages have most\nof the algorithms in them.\nSo you don't have\nto actually sit down\nand code the algorithms instead.\nYou can just load one\nof these packages\nfrom their libraries and run it.\nSo programming languages\nis a must at the minimum.\nYou should know our\nor python and a database\nquery language now,\nlet's move on to data\nextraction and processing.\nSo guys That you have\nmultiple data sources like\nmySQL database Mongo database.\nOkay.\nSo what you have to do\nis you have to extract\nfrom such sources\nand then in order to analyze\nand query this database you have\nto store it in a proper format\nor a proper structure.\nOkay, finally, then you can load\nthe data in the data warehouse\nand you can analyze\nthe data over here.\nOkay.\nSo this entire process is called\nextraction and processing.\nSo guys extraction\nand processing is all\nabout getting data.\nFrom these different\ndata sources and then\nputting it in a format\nso that you can analyze it\nnow next is data wrangling\nand exploration now\nguys data wrangling is one\nof the most difficult tasks\nin data science.\nThis is the most\ntime-consuming task\nbecause data wrangling is all\nabout cleaning the data.\nThere are a lot of instances\nwhere the data sets\nhave missing values\nor they have null values\nor they have inconsistent\nformats or inconsistent values\nand you need to understand\nwhat to do with such values.\nThis is Data wrangling\nor data cleaning comes\ninto the picture then\nafter you're done with that.\nYou are going\nto analyze the data.\nSo where's after data wrangling\nand cleaning is done.\nYou're going to start exploring.\nThis is where you try to make\nsense out of the data.\nOkay, so you can do this\nby looking at the different\npatterns in the data\nthe different Trends outliers\nand various unexpected results\nin all of that.\nNext.\nWe have machine learning.\nSo guys if you're\na large company\nor with huge amounts of data or\nif you're working at a company.\nSee where the product\nis data driven,\nlike if you're working\nin Netflix or Google Maps,\nthen you have to be\nfamiliar with machine\nlearning methods, right?\nYou cannot process\nlarge amount of data\nwith traditional methods.\nSo that's why you need\na machine learning algorithms.\nSo there are few algorithms.\nLike knok nearest neighbor\ndoes random Forest\nthis K means algorithm\nthis support Vector machines,\nall of these algorithms.\nYou have to be aware of all\nof these algorithms\nand let me tell you\nthat most of these algorithms\ncan be implemented.\nUsing our or python libraries.\nOkay, you need to\nhave an understanding\nof machine learning.\nIf you have large amount\nof data in front of you\nwhich is going to be the case\nfor most of the people right now\nbecause data is being generated\nat an Unstoppable Pace earlier\nin the session we discussed\nhow much of data is generated.\nSo for now knowing\nmachine learning algorithms\nand machine learning Concepts\nis a very required skill\nif you want to become\na data scientist,\nso if you're sitting\nfor an interview as\na data scientist,\nyou will be asked\nmachine learning.\nSeems you will be asked\nhow good you are\nwith these algorithms\nand how well you\ncan Implement them.\nNext we have big\ndata processing Frameworks.\nSo guys, we know\nthat we've been generating\na lot of data and most\nof this data can be structured\nor unstructured as well.\nSo on such data,\nyou cannot use traditional\ndata processing system.\nSo that's why you need\nto know Frameworks\nlike Hadoop and Spark.\nOkay.\nThese Frameworks can be used\nto handle big data lastly.\nWe have data visualization.\nSo guys data visualization is Is\none of the most important part\nof data analysis,\nit is always very important\nto present the data\nin an understandable\nand Visually appealing format.\nSo data visualization\nis one of the skills\nthat data scientists\nhave to master.\nOkay, if you want to communicate\nthe data with the end users\nin a better way then\ndata visualization is a must\nso guys are a lot of tools\nwhich can be used for data\nvisualization tools like Diablo\nand power bi are few the most\npopular visualization tools.\nSo with this we sum up\nthe entire skill set\nthat is needed to become\na data scientist apart from this\nyou should also have data-driven\nproblem solving approach.\nYou should also be\nvery creative with data.\nSo now that we know the skills\nthat are needed to become\na data scientist.\nLet's look at the different\njob roles just data science is\na very vast field.\nThere are many job roles\nunder data science.\nSo let's take a look\nat each role.\nLet's start off\nwith a data scientist.\nSo there's data scientists\nhave to understand.\nThe challenge is over business\nand they have to offer the best\nsolution using data analysis\nand data processing.\nSo for instance\nif they are expected\nto perform predictive analysis,\nthey should also be able\nto identify Trends and patterns\nthat can have the companies\nin making better decisions\nto become a data scientist.\nYou have to be an expert in\nour Matlab SQL Python and other\ncomplementary Technologies.\nIt can also help\nif you have a higher\ndegree in mathematics\nor computer engineering\nnext we have data.\nAn analyst so a data\nanalyst is responsible\nfor a variety of tasks,\nincluding visualization\nprocessing of massive amount\nof data and among them.\nThey have to also perform\nqueries on databases.\nSo they should be aware\nof the different query languages\nand guys one of the most\nimportant skills of\na data analyst is optimization.\nThis is because they have\nto create and modify algorithms\nthat can be used to pull\ninformation from some\nof the biggest databases\nwithout corrupting the data\nso to become Be done.\nYou must know Technologies\nsuch as SQL our SAS and python.\nSo certification in any\nof these Technologies\ncan boost your job application.\nYou should also have\na good problem solving quality.\nNext.\nWe have a data architect.\nSo a data architect\ncreates the blueprints\nfor a data management\nso that the databases\ncan be easily integrated\ncentralized and protected\nwith a best security measures.\nOkay.\nThey also ensure\nthat the data Engineers\nhave the best tools\nand systems to work with So\nto become a data architect,\nyou have to have expertise\nand data warehousing\ndata modeling extraction\ntransformation and loan.\nOkay.\nYou should also be\nwell versed in Hive Pig\nand Spark now apart from this\nthere are data Engineers.\nSo guys,\nthe main responsibilities of\na data engineer is to build\nand test scalable\nBig Data ecosystems.\nOkay, they are also needed\nto update the existing systems\nwith newer or upgraded versions\nand they are also responsible\nfor improving the efficiency.\nFor database now.\nIf you are interested\nin a career as a data engineer,\nthen technologies\nthat require hands-on\nexperience include Hive nosql\nare Ruby Java C++ and Matlab,\nit would also help\nif you can work\nwith popular data apis\nand ETL tools next.\nWe have a statistician.\nSo as the name suggests you have\nto have a sound understanding\nof statistical theories\nand data organization.\nNot only do they extract\nand offer valuable insights.\nThey also create new.\nMethodologies for engineers\nto apply now.\nIf you want to become\na statistician then you have\nto have a passion for logic.\nThey are also good variety\nof database systems\nsuch as SQL Data Mining\nand other various machine\nlearning Technologies by that.\nI mean, you should be good\nwith math and you should also\nhave a good knowledge\nabout the weight is\ndatabase system such as SQL\nand also the various\nmachine learning Concepts\nand algorithms is\nthe most next we have\nthe database administrator.\nSo guys the job profile of\na database administrator\nis Much self-explanatory,\nthey are basically responsible\nfor the proper functioning\nof all the databases\nand they are also responsible\nfor granting permission\nor the working in services to\nthe employees of the company.\nThey also have to take care\nof the database backups\nand recoveries.\nSo some of the skills\nthat are needed to become\na database administrator include\ndatabase backup and Recovery\ndata security data modeling\nand design next.\nWe have the business analyst now\nthe role of a business analyst\nis a little It different\nfrom all of the other\ndata signs job now.\nDon't get me wrong.\nThey have a very\ngood understanding of the data\noriented Technologies.\nThey know how to handle a lot\nof data and process it\nbut they are also very focused\non how this data can be linked\nto actionable business inside.\nSo they mainly focus\non business growth.\nOkay.\nNow a business analyst\nacts like a link\nbetween the data engineers\nand the management Executives.\nSo in order to become\na business analyst you have\nto have an understanding\nof business finances\nbusiness intelligence.\nAnd also I did acknowledge,\nhe's like data modeling data\nvisualization tools and Etc\nat last we have a data\nand analytics manager\na data and analytics\nmanager is responsible\nfor the data science operations.\nNow the main responsibilities\nof a data and analytics\nmanager is to oversee\nthe data science operation.\nOkay, he's responsible\nfor assigning the duties\nto the team according\nto their skills\nand expertise now their strength\nshould include Technologies\nlike SAS our SQL.\nAnd of course,\nthey should have good management\nskills apart from that.\nThey must have excellent social\nskills leadership qualities\nand and out-of-the-box\nthinking attitude.\nAnd like I said earlier\nyou need to have a good\nunderstanding of Technologies.\nLike pythons as\nour Java and Etc.\nSo Guys, these were\nthe different job roles\nin data science.\nI hope you all found\nthis informative.\nNow, let's move ahead\nand look at the data lifecycle.\nSo guys are basically six steps\nin the data life cycle.\nIt starts with\na business requirement.\nNext is the data acquisition\nafter that you\nwould process the data\nwhich is called data processing.\nThen there is\ndata exploration modeling\nand finally deployment.\nSo guys before you even start\non a data science project.\nIt is important\nthat you understand the problem\nyou're trying to solve.\nSo in this stage,\nyou're just going to focus\non identifying the central\nobjectives of the project\nand you will do this\nby identifying the variables\nthat need to be\npredicted next up.\nWe have data acquisition.\nOkay.\nSo now that you have\nyour objectives I find it's time\nfor you to start\nGathering the data.\nSo data mining is the process\nof gathering your data\nfrom different sources\nat this stage some\nof the questions you\ncan ask yourself is\nwhat data do I need\nfor my project?\nWhere does it live?\nHow can I obtain it?\nAnd what is the most\nefficient way to store\nand access all of it?\nNext up there is data processing\nnow usually all the data\nthat you collected\nis a huge mess.\nOkay.\nIt's not formatted.\nIt's not structured.\nIt's not cleaned.\nSo if Find any data set\nthat is cleaned\nand it's packaged well for you,\nthen you've actually\nwon the lottery\nbecause finding the right data\ntakes a lot of time\nand it takes a lot of effort\nand one of the major\ntime-consuming task\nin the data science process\nis data cleaning.\nOkay, this requires\na lot of time.\nIt requires a lot of effort\nbecause you have to go\nthrough the entire data set\nto find out any missing values\nor if there are\nany inconsistent values\nor corrupted data,\nand you also find\nthe unnecessary data.\nOver here and you\nremove that data.\nSo this was all\nabout data processing next\nwe have data exploration.\nSo now that you have sparkling\nclean set of data,\nyou are finally ready to get\nstarted with your analysis.\nOkay, the data exploration stage\nis basically the brainstorming\nof data analysis.\nSo in order to understand\nthe patterns in your data,\nyou can use histogram.\nYou can just pull up\na random subset of data\nand plot a histogram.\nYou can even create\ninteractive visualizations.\nThis is the point\nwhere you Dive deep\ninto the data\nand you try to explore\nthe different models\nthat can be applied\nto your data next up.\nWe have data modeling.\nSo after processing the data,\nwhat you're going to do\nis you're going to carry\nout model training.\nOkay.\nNow model training is basically\nabout finding a model\nthat answers the\nquestions more accurately.\nSo the process of model training\ninvolves a lot of steps.\nSo firstly you'll start\nby splitting the input data\ninto the training data set\nand the testing data set.\nOkay, you're going to take\nthe entire data set\nand you're going to separate it\ninto Two two parts one is\nthe training and one\nis the testing data\nafter that your build a model\nby using the training data set\nand once you're done with that,\nyou'll evaluate the training\nand the test data set now\nto evaluate the training\nand testing data.\nSo you'll be using series\nof machine learning\nalgorithms after that.\nYou'll find out the model\nwhich is the most suitable\nfor your business requirement.\nSo this was\nmainly data modeling.\nOkay.\nThis is where you build a model\nout of your training data set\nand then you evaluate this model\nby using the testing data set.\nYou have deployment.\nSo guys a goal of this stage\nis to deploy the model\ninto a production or maybe\na production like environment.\nSo this is basically done\nfor final user acceptance\nand the users have to validate\nthe performance of the models\nand if there are any issues\nwith the model or any issues\nwith the algorithm,\nthen they have to be\nfixed in this stage.\nSo guys with this\nwe come to the end\nof the data lifecycle.\nI hope this was clear statistics\nand probability are essential\nbecause these disciples\nform the basic Foundation\nof all machine\nlearning algorithms deep\nlearning artificial intelligence\nand data science.\nIn fact, mathematics\nand probability is\nbehind everything around us\nfrom shapes patterns\nand colors to the count\nof petals in a flower\nmathematics is embedded\nin each and every aspect\nof our lives with this in mind.\nI welcome you all\nto today's session.\nSo I'm going to go ahead\nand Scoffs the agenda for today\nwith you all now going to begin\nthe session by understanding\nwhat is data after that.\nWe'll move on and look at the\ndifferent categories of data,\nlike quantitative\nand qualitative data,\nthen we'll discuss what\nexactly statistics is\nthe basic terminologies in\nstatistics and a couple\nof sampling techniques.\nOnce we're done with that.\nWe'll discuss the different\ntypes of Statistics\nwhich involve descriptive\nand inferential statistics.\nThen in the next session\nwill mainly be focusing\non descriptive statistics\nhere will understand\nthe different measures\nof center measures\nof spread Information Gain\nand entropy will also\nunderstand all of these measures\nwith the help of a use case\nand finally we'll discuss\nwhat exactly a\nconfusion Matrix is\nonce we've covered the entire\ndescriptive statistics module\nwill discuss the probability\nmodule here will understand what\nexactly probability is\nthe different terminologies\nin probability will also\nstudy the Different\nprobability distributions,\nthen we'll discuss the types\nof probability which include\nmarginal probability joint\nand conditional probability.\nThen we move on\nand discuss a use case\nwhere and we'll see\nexamples that show us\nhow the different types\nof probability work\nand to better\nunderstand Bayes theorem.\nWe look at a small example.\nAlso, I forgot to mention\nthat at the end of the\ndescriptive statistics module\nwill be running a small demo\nin the our language.\nSo for those of you\nwho don't know much\nabout our I'll be explaining\nevery line in depth,\nbut if you want to have\na more in-depth understanding\nabout our I'll leave\na couple of blocks.\nAnd a couple of videos\nin the description box\nyou all can definitely\ncheck out that content.\nNow after we've completed the\nprobability module will discuss\nthe inferential statistics\nmodule will start this module\nby understanding\nwhat is point estimation.\nWe will discuss\nwhat is confidence interval\nand how you can estimate\nthe confidence interval.\nWe will also discuss margin\nof error and will understand all\nof these concepts by looking\nat a small use case.\nWe'd finally end the inferential\nReal statistic module by looking\nat what hypothesis\ntesting is hypothesis.\nTesting is a very important part\nof inferential statistics.\nSo we'll end the session\nby looking at a use case\nthat discusses how\nhypothesis testing works\nand to sum everything up.\nWe'll look at a demo\nthat explains how\ninferential statistics Works.\nAlright, so guys,\nthere's a lot to cover today.\nSo let's move ahead\nand take a look\nat our first topic\nwhich is what is data.\nNow, this is\na quite simple question\nif I ask any of You\nwhat is data?\nYou'll see that it's\na set of numbers\nor some sort of documents\nthat have stored in my computer\nnow data is actually everything.\nAll right, look around you there\nis data everywhere each click\non your phone generates\nmore data than you know,\nnow this generated data\nprovides insights for analysis\nand helps us make\nBetter Business decisions.\nThis is why data is\nso important to give you\na formal definition data refers\nto facts and statistics.\nCollected together\nfor reference or analysis.\nAll right.\nThis is the definition\nof data in terms\nof statistics and probability.\nSo as we know data\ncan be collected it\ncan be measured and analyzed\nit can be visualized by\nusing statistical models\nand graphs now data is divided\ninto two major subcategories.\nAlright, so first we\nhave qualitative data\nand quantitative data.\nThese are the two\ndifferent types of data\nunder qualitative data.\nWe have nominal and ordinal data\nand under quantitative data.\nWe have discrete\nand continuous data.\nNow, let's focus\non qualitative data.\nNow this type of data deals with\ncharacteristics and descriptors\nthat can't be easily measured\nbut can be observed subjectively\nnow qualitative data\nis further divided\ninto nominal and ordinal data.\nSo nominal data is\nany sort of data\nthat doesn't have\nany order or ranking?\nOkay.\nAn example of nominal\ndata is gender.\nNow.\nThere is no ranking in gender.\nThere's only male female\nor other right?\nThere is no one two,\nthree four or any sort\nof ordering in gender race is\nanother example of nominal data.\nNow ordinal data is basically an\nordered series of information.\nOkay, let's say\nthat you went to a restaurant.\nOkay.\nYour information is stored\nin the form of customer ID.\nAll right.\nSo basically you are represented\nwith a customer ID.\nNow you would have rated\ntheir service as\neither good or average.\nAll right, that's\nhow no ordinal data is\nand similarly they'll have\na record of other customers\nwho visit the restaurant\nalong with their ratings.\nAll right.\nSo any data which has\nsome sort of sequence\nor some sort of order\nto it is known as ordinal data.\nAll right, so guys,\nthis is pretty simple\nto understand now,\nlet's move on and look\nat quantitative data.\nSo quantitative data\nbasically these He's\nwith numbers and things.\nOkay, you can understand\nthat by the word quantitative\nitself quantitative is\nbasically quantity.\nRight Saudis will numbers\na deals with anything that you\ncan measure objectively.\nAll right, so\nthere are two types\nof quantitative data there is\ndiscrete and continuous data\nnow discrete data is also\nknown as categorical data\nand it can hold a finite number\nof possible values.\nNow, the number of students\nin a class is a finite Number.\nAll right, you can't\nhave infinite number\nof students in a class.\nLet's say in your fifth grade.\nThey have a hundred students\nin your class.\nAll right, there weren't\ninfinite number but there\nwas a definite finite number\nof students in your class.\nOkay, that's discrete data.\nNext.\nWe have continuous data.\nNow this type of data\ncan hold infinite number\nof possible values.\nOkay.\nSo when you say weight\nof a person is an example\nof continuous data\nwhat I mean to see is my weight\ncan be 50 kgs or it NB 50.1 kgs\nor it can be 50.00 one kgs\nor 50.000 one or is\n50.0 2 3 and so\non right there\nare infinite number\nof possible values, right?\nSo this is what I mean\nby a continuous data.\nAll right.\nThis is the difference between\ndiscrete and continuous data.\nAnd also I'd like to mention\na few other things over here.\nNow, there are a couple\nof types of variables as well.\nWe have a discrete variable\nand we have a continuous\nvariable discrete variable\nis also known as\na categorical variable\nor and it can hold values\nof different categories.\nLet's say that you have\na variable called message\nand there are two types\nof values that this variable\ncan hold let's say\nthat your message\ncan either be a Spam message\nor a non spam message.\nOkay, that's when you call\na variable as discrete\nor categorical variable.\nAll right, because it\ncan hold values\nthat represent different\ncategories of data\nnow continuous variables\nare basically variables\nthat can store infinite\nnumber of values.\nSo the weight of a person\ncan be denoted as\na continuous variable.\nAll right, let's say there is\na variable called weight\nand it can store infinite number\nof possible values.\nThat's why we will call\nit a continuous variable.\nSo guys basically\nvariable is anything\nthat can store a value right?\nSo if you associate any sort\nof data with a Able,\nthen it will become\neither discrete variable\nor continuous variable.\nThere is also dependent and\nindependent type of variables.\nNow, we won't discuss all\nof that in death because\nthat's pretty understandable.\nI'm sure all of you know,\nwhat is independent variable\nand dependent variable right?\nDependent variable is\nany variable whose value\ndepends on any other\nindependent variable?\nSo guys that much\nknowledge I expect\nor if you do have all right.\nSo now let's move on and look\nat our next topic which Which is\nwhat is statistics now coming\nto the formal definition\nof statistics statistics is\nan area of Applied Mathematics,\nwhich is concerned\nwith data collection\nanalysis interpretation\nand presentation now usually\nwhen I speak about statistics\npeople think statistics is\nall about analysis\nbut statistics has other parts\nto it it has data collection is\nalso a part of Statistics data\ninterpretation presentation.\nAll of this comes\ninto statistics already are\ngoing to use statistical methods\nto visualize data to collect\ndata to interpret data.\nAlright, so the area\nof mathematics deals\nwith understanding\nhow data can be used\nto solve complex problems.\nOkay.\nNow I'll give you\na couple of examples\nthat can be solved\nby using statistics.\nOkay, let's say\nthat your company\nhas created a new drug\nthat may cure cancer.\nHow would you conduct\na test to confirm\nthe As Effectiveness now,\neven though this sounds\nlike a biology problem.\nThis can be solved\nwith Statistics already\nwill have to create a test\nwhich can confirm\nthe effectiveness of the drum\nor a this is a common problem\nthat can be solved\nusing statistics.\nLet me give you\nanother example you\nand a friend are at a baseball\ngame and out of the blue.\nHe offers you a bet\nthat neither team will hit\na home run in that game.\nShould you take the BET?\nAll right here you just\ndiscuss the probability\nof I know you'll win or lose.\nAll right, this\nis another problem\nthat comes under statistics.\nLet's look at another example.\nThe latest sales data\nhas just come in\nand your boss wants\nyou to prepare a report\nfor management on places\nwhere the company\ncould improve its business.\nWhat should you look for?\nAnd what should you\nnot look for now?\nThis problem involves a lot\nof data analysis will have to\nlook at the different variables\nthat are causing\nyour business to go down\nor the you have to look\nat a few variables.\nThat are increasing\nthe performance of your models\nand thus growing your business.\nAlright, so this involves\na lot of data analysis\nand the basic idea\nbehind data analysis is\nto use statistical techniques\nin order to figure\nout the relationship\nbetween different variables\nor different components\nin your business.\nOkay.\nSo now let's move on\nand look at our next topic\nwhich is basic\nterminologies in statistics.\nNow before you dive\ndeep into statistics,\nit is important that you\nunderstand basic terminologies\nused in statistics.\nThe two most important\nterminologies in statistics\nare population and Sample.\nSo throughout the statistics\ncourse or throughout any problem\nthat you're trying\nto stall with Statistics.\nYou will come\nacross these two words,\nwhich is population and Sample\nNow population is a collection\nor a set of individuals\nor objects or events.\nEvents whose properties\nare to be analyzed.\nOkay.\nSo basically you can refer\nto population as a subject\nthat you're trying to analyze\nnow a sample is just\nlike the word suggests.\nIt's a subset of the population.\nSo you have to make sure\nthat you choose the sample\nin such a way\nthat it represents\nthe entire population.\nAll right.\nIt shouldn't Focus add one part\nof the population instead.\nIt should represent\nthe entire population.\nThat's how your sample\nshould be chosen.\nSo Well chosen sample\nwill contain most\nof the information about a\nparticular population parameter.\nNow, you must be wondering\nhow can one choose a sample\nthat best represents\nthe entire population now\nsampling is a statistical method\nthat deals with the selection\nof individual observations\nwithin a population.\nSo sampling is performed\nin order to infer statistical\nknowledge about a population.\nAll right, if you\nwant to understand\nthe different statistics\nof a population\nlike the mean\nthe median Median the mode\nor the standard deviation\nor the variance of a population.\nThen you're going\nto perform sampling.\nAll right, because it's\nnot reasonable for you to study\na large population\nand find out the mean median\nand everything else.\nSo why is sampling\nperformed you might ask?\nWhat is the point of sampling?\nWe can just study\nthe entire population now guys,\nthink of a scenario\nwhere in your asked\nto perform a survey\nabout the eating habits\nof teenagers in the US.\nSo at present there are\nover 42 million teens in the US\nand this number is growing\nas we are speaking\nright now, correct.\nIs it possible to survey each\nof these 42 million individuals\nabout their health?\nIs it possible?\nWell, it might be possible\nbut this will take\nforever to do now.\nObviously, it's not it's\nnot reasonable to go around\nknocking each door\nand asking for what does\nyour teenage son eat\nand all of that right?\nThis is not very reasonable.\nThat's By sampling is used.\nIt's a method wherein a sample\nof the population is studied\nin order to draw inferences\nabout the entire population.\nSo it's basically\na shortcut to studying\nthe entire population instead\nof taking the entire population\nand finding out\nall the solutions.\nYou just going to take\na part of the population\nthat represents the\nentire population\nand you're going to perform\nall your statistical analysis\nyour inferential statistics\non that small sample.\nAll right,\nand that sample basically here\nPresents the entire population.\nAll right, so I'm short\nof made this clear\nto y'all what is sample\nand what is population now?\nThere are two main types\nof sampling techniques\nthat are discussed today.\nWe have probability sampling\nand non-probability\nsampling now in this video\nwill only be focusing on\nprobability sampling techniques\nbecause non-probability sampling\nis not within the scope\nof this video.\nAll right will only discuss\nthe probability part\nbecause we're focusing\non statistics and\nprobability, correct.\nNow again under\nprobability sampling.\nWe have three different types.\nWe have random\nsampling systematic\nand stratified sampling.\nAll right, and just\nto mention the different types\nof non-probability sampling,\n's we have no bald Kota judgment\nand convenience sampling.\nAll right now guys\nin this session.\nI'll only be\nfocusing on probability.\nSo let's move on\nand look at the different types\nof probability sampling.\nSo what is probability sampling\nit is a sampling technique\nin which samples\nfrom a large population\nare chosen by using\nthe theory of probability.\nAll right, so there\nare three types\nof probability sampling.\nAll right first we have\nthe random sampling now\nin this method each member\nof the population\nhas an equal chance\nof being selected in the sample.\nAll right, so each\nand every individual or each\nand every object\nin the population\nhas an equal John's\nof being a part of the sample.\nThat's what random\nsampling is all about.\nOkay, you are randomly going\nto select any individual\nor any object.\nSo this Bay each individual has\nan equal chance\nof being selected.\nCorrect?\nNext.\nWe have systematic sampling now\nin systematic sampling\nevery nth record is chosen\nfrom the population to be\na part of the sample.\nAll right.\nNow refer this image\nthat I've shown over\nhere out of these six.\nGroups every second group\nis chosen as a sample.\nOkay.\nSo every second record\nis chosen here and this is\nour systematic sampling works.\nOkay, you're randomly\nselecting the nth record\nand you're going to add\nthat to your sample.\nNext.\nWe have stratified\nsampling now in this type\nof technique a stratum\nis used to form samples\nfrom a large population.\nSo what is a stratum\na stratum is basically a subset\nof the population that shares\nat One common characteristics.\nSo let's say\nthat your population has a mix\nof both male and female\nso you can create to straightens\nout of this one will have\nonly the male subset\nand the other will have\nthe female subset.\nAll right, this is\nwhat stratum is.\nIt is basically a subset\nof the population\nthat shares at least\none common characteristics.\nAll right in our example,\nit is gender.\nSo after you've created\na stratum you're going\nto use random sampling\non these stratums\nand you're going to choose.\nChoose a final sample.\nSo random sampling meaning\nthat all of the individuals\nin each of the stratum\nwill have an equal chance\nof being selected in the sample.\nCorrect.\nSo Guys, these were\nthe three different types\nof sampling techniques.\nNow, let's move on and look\nat our next topic\nwhich is the different\ntypes of Statistics.\nSo after this,\nwe'll be looking at the more\nadvanced concepts of Statistics,\nright so far we discuss\nthe basics of Statistics,\nwhich is basically\nwhat is statistics the Friend\nsampling techniques and the\nterminologies and statistics.\nAll right.\nNow we look at the different\ntypes of Statistics.\nSo there are two major\ntypes of Statistics\ndescriptive statistics\nand inferential statistics\nin today's session.\nWe will be discussing\nboth of these types\nof Statistics in depth.\nAll right, we'll also\nbe looking at a demo\nwhich I'll be running\nin the our language\nin order to make\nyou understand what exactly\ndescriptive and inferential\nstatistics is soaked.\nAs which is going\nto look at the basic,\nso don't worry.\nIf you don't\nhave much knowledge,\nI'm explaining everything\nfrom the basic level.\nAll right, so guys descriptive\nstatistics is a method\nwhich is used to describe\nand understand the features\nof specific data set by giving\na short summary of the data.\nOkay, so it is mainly\nfocused upon the\ncharacteristics of data.\nIt also provides a graphical\nsummary of the data now\nin order to make you understand\nwhat descriptive statistics is.\nLet's suppose that\nyou want to gift all\nyour classmates or t-shirt.\nSo to study the average\nshirt size of a student\nin a classroom.\nSo if you were to use\ndescriptive statistics to study\nthe average shirt size\nof students in your classroom,\nthen what you would do is you\nwould record the shirt size\nof all students in the class\nand then you would find out\nthe maximum minimum and average\nshirt size of the cloud.\nOkay.\nSo coming to inferential\nstatistics inferential.\nSix makes inferences\nand predictions about\na population based\non the sample of data taken\nfrom the population.\nOkay.\nSo in simple words,\nit generalizes a large data set\nand it applies probability\nto draw a conclusion.\nOkay.\nSo it allows you\nto infer data parameters\nbased on a statistical model\nby using sample data.\nSo if we consider\nthe same example of finding\nthe average shirt size\nof students in a class\nin infinite real statistics.\nWe'll take a sample set\nof the class\nwhich is basically a few people\nfrom the entire class.\nAll right, you already\nhave had grouped the class\ninto large medium and small.\nAll right in this method\nyou basically build\na statistical model\nand expand it for the entire\npopulation in the class.\nSo guys, there was a brief\nunderstanding of descriptive\nand inferential statistics.\nSo that's the difference\nbetween descriptive\nand inferential now\nin the next section,\nwe will go in depth\nabout descriptive statistics.\nRight.\nSo let's discuss more\nabout descriptive statistics.\nSo like I mentioned\nearlier descriptive\nstatistics is a method\nthat is used to describe\nand understand the features\nof a specific data set by giving\nshort summaries about the sample\nand measures of the data.\nThere are two important measures\nin descriptive statistics.\nWe have measure\nof central tendency,\nwhich is also known as measure\nof center and we have\nmeasures of variability.\nThis is also known\nas Measures of spread\nso measures of center include\nmean median and mode now\nwhat is measures\nof center measures of the center\nare statistical measures\nthat represent the summary\nof a data set?\nOkay, the three main measures\nof center are mean median\nand mode coming\nto measures of variability\nor measures of spread.\nWe have range\ninterquartile range variance\nand standard deviation.\nAll right.\nSo now let's discuss each\nof these measures.\nHas in a little\nmore depth starting\nwith the measures of center.\nNow, I'm sure all of you know,\nwhat the mean is mean is\nbasically the measure\nof the average of all\nthe values in a sample.\nOkay, so it's basically\nthe average of all\nthe values in a sample.\nHow do you measure the mean I\nhope all of you know\nhow the main is measured\nif there are 10 numbers\nand you want to find the mean\nof these 10 numbers.\nAll you have to do is you have\nto add up all the 10 numbers\nand you have to divide\nit by 10 then.\nRepresents the number\nof samples in your data set.\nAll right, since we\nhave 10 numbers,\nwe're going to\ndivide this by 10.\nAll right, this will\ngive us the average\nor the mean so to better\nunderstand the measures\nof central tendency.\nLet's look at an example.\nNow the data set over here is\nbasically the cars data set\nand it contains a few variables.\nAll right, it has\nsomething known as cars.\nIt has mileage per gallon\ncylinder type displacement\nhorsepower and relax.\nSilver ratio.\nAll right, all of these measures\nare related to cars.\nOkay.\nSo what you're going\nto do is you're going\nto use descriptive analysis\nand you're going to analyze\neach of the variables\nin the sample data set\nfor the mean standard deviation\nmedian more and so on.\nSo let's say that you want\nto find out the mean\nor the average horsepower\nof the cars among\nthe population of cards.\nLike I mentioned earlier\nwhat you'll do is you'll check\nthe average of all the values.\nSo in this case we will take\nThe sum of the horsepower\nof each car and we'll divide\nthat by the total\nnumber of cards.\nOkay, that's exactly\nwhat I've done here\nin the calculation part.\nSo this hundred\nand ten basically\nrepresents the horsepower\nfor the first car.\nAll right.\nSimilarly.\nI've just added up all\nthe values of horsepower\nfor each of the cars\nand I've divided it by 8 now\n8 is basically the number\nof cars in our data set.\nAll right, so hundred and three\npoint six two five is\nwhat army mean is or the average\nof horsepower is all right.\nNow, let's understand\nwhat median is with an example.\nOkay.\nSo to Define median median\nis basically a measure\nof the central value\nof the sample set\nis called the median.\nAll right, you can see\nthat it is the middle value.\nSo if we want to find\nout the center value\nof the mileage per gallon\namong the population\nof cars first,\nwhat we'll do is we'll arrange\nthe MGP values in ascending\nor descending Order\nand choose a middle value\nright in this case\nsince we have\neight values, right?\nWe have eight values\nwhich is an even entry.\nSo whenever you have even\nnumber of data points\nor samples in your data set,\nthen you're going\nto take the average\nof the two middle values.\nIf we had nine values over here.\nWe can easily figure\nout the middle value\nand you know choose\nthat as a median.\nBut since they're even number\nof values we are going\nto take the average\nof the two middle values.\nAll right.\nRight.\nSo 22.8 and 23 are\nmy two middle values\nand I'm taking the mean\nof those 2 and hence I\nget twenty two point nine,\nwhich is my median.\nAll right, lastly,\nlet's look at\nhow mode is calculated.\nSo what is mode the value\nthat is most recurrent\nin the sample set is known as\nmode or basically the value\nthat occurs most often.\nOkay, that is known as mode.\nSo let's say\nthat we want to find out\nthe most common type of cylinder\namong the population of cards.\nWhat we have to do is\nwe will check the value\nwhich is repeated\nthe most number of times here.\nWe can see that the cylinders\ncome in two types.\nWe have cylinder of Type\n4 and cylinder of type 6, right?\nSo take a look at the data set.\nYou can see that the most\nrecurring value is 6 right.\nWe have one two,\nthree four and five.\nWe have five six\nand we have one two, three.\nYeah, we have three\nfour types of lenders\nand five six types of lenders.\nSo basically we have\nthree four type cylinders\nand we have five\nsix type cylinders.\nAll right.\nSo our mode is going\nto be 6 since 6 is more\nrecurrent than 4 so guys\nthose were the measures\nof the center or the measures\nof central tendency.\nNow, let's move on and look\nat the measures of the spread.\nAll right.\nNow, what is the measure\nof spread a measure of spread?\nSometimes also called\nas measure of dispersion is Used\nto describe the variability\nin a sample or population.\nOkay, you can think\nof it as some sort\nof deviation in the sample.\nAll right, so you measure\nthis with the help\nof the different\nmeasure of spreads.\nWe have range\ninterquartile range variance\nand standard deviation.\nNow range is pretty\nself-explanatory, right?\nIt is the given measure of\nhow spread apart the values\nin a data set are\nthe range can be calculated\nas shown in this formula.\nYou basically going\nto subtract the maximum value\nin your data set\nfrom the minimum value\nin your data set.\nThat's how you calculate\nthe range of the data.\nAlright, next we\nhave interquartile range.\nSo before we discuss\ninterquartile range,\nlet's understand.\nWhat a quartile is red.\nSo quartiles basically tell us\nabout the spread of a data set\nby breaking the data set\ninto different quarters.\nOkay, just like how the median\nbreaks the data into two parts\nthe court is We'll break it\ninto different quarters.\nSo to better understand\nhow quartile and\ninterquartile are calculated.\nLet's look at a small example.\nNow this data set basically\nrepresents the marks\nof hundred students\nordered from the lowest\nto the highest scores red.\nSo the quartiles lie\nin the following ranges\nthe first quartile,\nwhich is also known as q1 it\nlies between the 25th\nand 26th observation.\nAll right.\nSo if you look at this\nI've highlighted Add the 25th\nand the 26th observation.\nSo how you can calculate\nQ 1 or first quartile is\nby taking the average\nof these two values.\nAlright, since both\nthe values are 45\nwhen you add them up\nand divide them by two\nyou'll still get 45 now\nthe second quartile\nor Q 2 is between the 50th\nand the 51st observation.\nSo you're going to take\nthe average of 58 and 59\nand you will get\na value of 58.5.\nNow, this is my second quarter\nthe third quartile.\nAh Q3 is between the 75th\nand the 76th observation here.\nAgain, we'll take the average\nof the two values\nwhich is the 75th value\nand the 76 value right\nand you'll get a value of 71.\nAll right, so guys\nthis is exactly\nhow you calculate\nthe different quarters.\nNow, let's look at\nwhat is interquartile range.\nSo IQR or the interquartile\nrange is a measure\nof variability based\non dividing a data set\ninto quartiles now\nthe The interquartile range\nis calculated by\nsubtracting the q1 from Q3.\nSo basically Q3\nminus q1 is your IQ are so\nyour IQR is your Q3 minus q1?\nAll right.\nNow this is how each\nof the quartiles are each core\ntile represents a quarter,\nwhich is 25% All right.\nSo guys, I hope all\nof you are clear\nwith interquartile range\nand what our quartiles now,\nlet's look at\nvariance covariance is\nbasically a measure\nthat shows How much\na random variable the first\nfrom its expected value?\nOkay.\nIt's basically the variance\nin any variable now variance\ncan be calculated by using\nthis formula right here x\nbasically represents\nany data point in your data set\nn is the total number\nof data points in your data set\nand X bar is basically\nthe main of data points.\nAll right.\nThis is how you calculate\nvariance variance is\nbasically a Computing\nthe squares of deviations.\nOkay.\nThat's why it says\ns Square there.\nNow let's look at\nwhat is deviation deviation is\njust the difference\nbetween each element\nfrom the mean.\nOkay, so it can be calculated\nby using this simple formula\nwhere X I basically\nrepresents a data point\nand mu is the mean\nof the population\nor add this is exactly\nhow you calculate the deviation\nNow population variance\nand Sample variance\nare very specific to\nwhether you're calculating the\nvariance in your population data\nset or in your sample data set.\nThat's the A difference\nbetween population\nand Sample variance.\nSo the formula for population\nvariance is pretty explanatory.\nSo X is basically\neach data point mu is the mean\nof the population\nn is the number of samples\nin your data set.\nAll right.\nNow, let's look at sample.\nVariance Now sample variance\nis the average of squared\ndifferences from the mean.\nAll right here x\ni is any data point\nor any sample in your data\nset X bar is the mean\nof your sample.\nAll right.\nIt's not the main\nof your population.\nAtion, it's the mean\nof your sample.\nAnd if you notice\nn here is a smaller\nn is the number\nof data points in your sample.\nAnd this is basically\nthe difference between sample\nand population variance.\nI hope that is clear coming\nto standard deviation is\nthe measure of dispersion\nof a set of data from its mean.\nAll right, so it's basically\nthe deviation from your mean.\nThat's what standard deviation\nis now to better understand\nhow the measures\nof spread are calculated.\nLet's look at a small use case.\nSo let's see Daenerys\nhas 20 dragons.\nThey have the numbers\nnine to five four and so on\nas shown on the screen,\nwhat you have to do is\nyou have to work out\nthe standard deviation or at\nin order to calculate\nthe standard deviation.\nYou need to know the mean right?\nSo first you're going to find\nout the mean of your sample set.\nSo how do you calculate\nthe mean you add all the numbers\nin your data set\nand divided by the total number\nof samples in your data set\nso you get a value of 7.\nHere then you calculate the rhs\nof your standard\ndeviation formula.\nAll right.\nSo from each data point you're\ngoing to subtract the mean\nand you're going to square that.\nAll right.\nSo when you do that,\nyou will get\nthe following result.\nYou'll basically get\nthis 425 for 925\nand so on so finally you\nwill just find the mean\nof the squared differences.\nAll right.\nSo your standard deviation\nwill come up to two point\nnine eight three\nonce you take the square root.\nSo guys, it's pretty simple.\nIt's a simple\nAt the magic technique,\nall you have to do is you have\nto substitute the values\nin the formula.\nAll right.\nI hope this was clear\nto all of you.\nNow let's move on\nand discuss the next topic\nwhich is Information Gain\nand entropy now.\nThis is one of my favorite\ntopics in statistics.\nIt's very interesting and\nthis topic is mainly involved\nin machine learning algorithms,\nlike decision trees\nand random forest.\nAll right, it's very important\nfor you to know\nhow Information Gain and entropy\nreally work and why they are\nso essential in building\nmachine learning models.\nWe focus on the statistic parts\nof Information Gain\nand entropy and after that\nwe'll discuss a use case.\nAnd see how Information Gain\nand entropy is used\nin decision trees.\nSo for those of you\nwho don't know what\na decision tree is it is\nbasically a machine\nlearning algorithm.\nYou don't have to know\nanything about this.\nI'll explain\neverything in depth.\nSo don't worry.\nNow.\nLet's look at\nwhat exactly entropy\nand Information Gain Is Now\nguys entropy is\nbasically the measure\nof any sort of uncertainty\nthat is present in the data.\nAll right, so it can be measured\nby using this formula.\nSo here s is the set\nof all instances in the data set\nor all the data items\nin the data set\nn is the different type\nof classes in your data set\nPi is the event probability.\nNow this might seem\na little confusing\nto y'all but when we go\nthrough the use case,\nyou'll understand all\nof these terms even better.\nAll right cam.\nThe information gained\nas the word suggests\nInformation Gain indicates\nhow much information\na particular feature\nor a particular variable gives\nus about the final outcome.\nOkay, it can be measured\nby using this formula.\nSo again here heads\nof s is the entropy\nof the whole data set\ns SJ is the number\nof instances with the J value\nof an attribute a s\nis the total number\nof instances in the data set V\nis the set of distinct values\nof an attribute a h\nof s j is the entropy\nof subsets of instances\nand hedge of a comma\ns is the entropy\nof an attribute a even\nthough this seems confusing.\nI'll clear out the confusion.\nAll right, let's discuss\na small problem statement\nwhere we will understand\nhow Information Gain\nand entropy is used to study\nthe significance of a model.\nSo like I said Information Gain\nand entropy are very\nimportant statistical measures\nthat let us understand\nthe significance of\na predictive model.\nOkay to get a more\nclear understanding.\nLet's look at a use case.\nAll right now suppose we\nare given a problem statement.\nAll right, the statement is\nthat you have to predict\nwhether a match can be played\nor Not by studying\nthe weather conditions.\nSo the predictor variables here\nare outlook humidity wind day\nis also a predictor variable.\nThe target variable\nis basically played\nor a the target variable\nis the variable\nthat you're trying to protect.\nOkay.\nNow the value of the target\nvariable will decide\nwhether or not a game\ncan be played.\nAll right, so that's\nwhy The play has two values.\nIt has no and yes, no,\nmeaning that the weather\nconditions are not good.\nAnd therefore you\ncannot play the game.\nYes, meaning that the weather\nconditions are good and suitable\nfor you to play the game.\nAlright, so that was\nour problem statement.\nI hope the problem statement\nis clear to all of you now\nto solve such a problem.\nWe make use of something\nknown as decision trees.\nSo guys think\nof an inverted tree\nand each branch of the tree\ndenotes some decision.\nAll right, each branch is\nIs known as the branch known\nand at each branch node,\nyou're going to take\na decision in such a manner\nthat you will get an outcome\nat the end of the branch.\nAll right.\nNow this figure\nhere basically shows\nthat out of 14 observations\n9 observations result in a yes,\nmeaning that out of 14 days.\nThe match can be played\nonly on nine days.\nAlright, so here\nif you see on day 1 Day\n2 Day 8 day 9 and 11.\nThe Outlook has been Alright,\nso basically we try\nto plaster a data set\ndepending on the Outlook.\nSo when the Outlook is sunny,\nthis is our data set\nwhen the Outlook is overcast.\nThis is what we have\nand when the Outlook\nis the rain this is\nwhat we have.\nAll right, so\nwhen it is sunny we have\ntwo yeses and three nodes.\nOkay, when the\nOutlook is overcast.\nWe have all four\nas yes has meaning\nthat on the four days\nwhen the Outlook was overcast.\nWe can play the game.\nAll right.\nNow when it comes to rain,\nwe have three yeses\nand two nodes.\nAll right.\nSo if you notice here,\nthe decision is being made by\nchoosing the Outlook variable\nas the root node.\nOkay.\nSo the root node is\nbasically the topmost node\nin a decision tree.\nNow, what we've done here is\nwe've created a decision tree\nthat starts with\nthe Outlook node.\nAll right, then you're splitting\nthe decision tree further\ndepending on other parameters\nlike Sunny overcast and rain.\nAll right now like we know\nthat Outlook has three values.\nSunny overcast and brain\nso let me explain this\nin a more in-depth manner.\nOkay.\nSo what you're doing\nhere is you're making\nthe decision Tree by choosing\nthe Outlook variable\nat the root node.\nThe root note is\nbasically the topmost node\nin a decision tree.\nNow the Outlook node has three\nbranches coming out from it,\nwhich is sunny\novercast and rain.\nSo basically Outlook\ncan have three values\neither it can be sunny.\nIt can be overcast\nor it can be rainy.\nOkay now these three values\nUse are assigned\nto the immediate Branch\nnodes and for each\nof these values the possibility\nof play is equal\nto yes is calculated.\nSo the sunny\nand the rain branches\nwill give you an impure output.\nMeaning that there is a mix\nof yes and no right.\nThere are two yeses\nhere three nodes here.\nThere are three yeses here\nand two nodes over here,\nbut when it comes\nto the overcast variable,\nit results in a hundred\npercent pure subset.\nAll right, this shows that\nthe overcast baby.\nWill result in a definite\nand certain output.\nThis is exactly what entropy\nis used to measure.\nAll right, it calculates\nthe impurity or the uncertainty.\nAlright, so the lesser\nthe uncertainty or the entropy\nof a variable more\nsignificant is that variable?\nSo when it comes to overcast\nthere's literally no impurity\nin the data set.\nIt is a hundred percent\npure subset, right?\nSo be want variables like these\nin order to build a model.\nAll right now,\nwe don't always Ways get lucky\nand we don't always find\nvariables that will result\nin pure subsets.\nThat's why we have\nthe measure entropy.\nSo the lesser the entropy of\na particular variable the most\nsignificant that variable\nwill be so in a decision tree.\nThe root node is assigned\nthe best attribute\nso that the decision tree\ncan predict the most\nprecise outcome meaning\nthat on the root note.\nYou should have the most\nsignificant variable.\nAll right, that's why\nwe've chosen Outlook\nor and now some of you might ask\nme why haven't you chosen\novercast Okay is overcast\nis not a variable.\nIt is a value\nof the Outlook variable.\nAll right.\nThat's why we've chosen\nour true cure\nbecause it has a hundred\npercent pure subset\nwhich is overcast.\nAll right.\nNow the question in your head is\nhow do I decide which variable\nor attribute best Blitz\nthe data now right now,\nI know I looked at the data\nand I told you that,\nyou know here we have\na hundred percent pure subset,\nbut what if it's\na more complex problem\nand you're not able\nto understand which variable\nwill best split the data,\nso guys when it comes\nto decision tree\nInformation and gain\nand entropy will help\nyou understand which variable\nwill best split the data set.\nAll right, or which variable you\nhave to assign to the root node\nbecause whichever variable\nis assigned to the root node.\nIt will best let the data set\nand it has to be the most\nsignificant variable.\nAll right.\nSo how we can do this\nis we need to use\nInformation Gain and entropy.\nSo from the total\nof the 14 instances\nthat we saw nine\nof them said yes and five\nof the instances said know\nthat you cannot play\non that particular day.\nAll right.\nSo how do you\ncalculate the entropy?\nSo this is the formula\nyou just substitute\nthe values in the formula.\nSo when you substitute\nthe values in the formula,\nyou will get a value of 0.9940.\nAll right.\nThis is the entropy\nor this is the uncertainty\nof the data present in a sample.\nNow in order to ensure\nthat we choose the best variable\nfor the root node.\nLet us look at all\nthe possible combinations\nthat you can use\non the root node.\nOkay, so these are All\nthe possible combinations\nyou can either have\nOutlook you can have\nwindy humidity or temperature.\nOkay, these are four variables\nand you can have any one\nof these variables\nas your root note.\nBut how do you select\nwhich variable best\nfits the root node?\nThat's what we are going\nto see by using\nInformation Gain and entropy.\nSo guys now the task at hand\nis to find the information gain\nfor each of these attributes.\nAll right.\nSo for Outlook for windy for\nhumidity and for temperature,\nwe're going to find\nout the information.\nNation gained all right.\nNow a point to remember is\nthat the variable\nthat results in the highest\nInformation Gain must be chosen\nbecause it will give us the most\nprecise and output information.\nAll right.\nSo the information gain for\nattribute windy will calculate\nthat first here.\nWe have six instances of true\nand eight instances of false.\nOkay.\nSo when you substitute all\nthe values in the formula,\nyou will get a value\nof zero point zero four eight.\nSo we get a value\nof You 2.0 for it.\nNow.\nThis is a very low value\nfor Information Gain.\nAll right, so the information\nthat you're going to get from\nWindy attribute is pretty low.\nSo let's calculate\nthe information gain\nof attribute Outlook.\nAll right, so from the total\nof 14 instances,\nwe have five instances\nwith say Sunny for instances,\nwhich are overcast\nand five instances,\nwhich are rainy.\nAll right for Sonny.\nWe have three yeses\nand to nose for overcast we have\nOr the for as yes for any\nwe have three years\nand two nodes.\nOkay.\nSo when you calculate\nthe information gain\nof the Outlook variable\nwill get a value\nof zero point 2 4 7 now compare\nthis to the information gain\nof the windy attribute.\nThis value is\nactually pretty good.\nRight we have zero point 2 4 7\nwhich is a pretty good value\nfor Information Gain.\nNow, let's look\nat the information gain\nof attribute humidity\nnow over here.\nWe have seven instances\nwith say hi\nand seven instances with same.\nRight and under\nthe high Branch node.\nWe have three instances\nwith say yes,\nand the rest for instances\nwould say no similarly\nunder the normal Branch.\nWe have one two, three,\nfour, five six seven\ninstances would say yes\nand one instance with says no.\nAll right.\nSo when you calculate\nthe information gain\nfor the humidity variable,\nyou're going to get\na value of 0.15 one.\nNow.\nThis is also\na pretty decent value,\nbut when you compare it\nto the Information Gain,\nOf the attribute Outlook it\nis less right now.\nLet's look at the information\ngain of attribute temperature.\nAll right, so the temperature\ncan hold repeat.\nSo basically the temperature\nattribute can hold\nhot mild and cool.\nOkay under hot.\nWe have two instances\nwith says yes and two instances\nfor no under mild.\nWe have four instances of yes\nand two instances of no\nand under col we have\nthree instances of yes\nand one instance of no.\nAll right.\nWhen you calculate\nthe information gain\nfor this attribute,\nyou will get a value\nof zero point zero to nine,\nwhich is again very less.\nSo what you can summarize\nfrom here is if we look\nat the information gain for each\nof these variable will see\nthat for Outlook.\nWe have the maximum gain.\nAll right, we have\nzero point two four seven,\nwhich is the highest\nInformation Gain value\nand you must always\nchoose a variable\nwith the highest Information\nGain to split the data\nat the root node.\nSo that's why we assign\nThe Outlook variable\nat the root node.\nAll right, so guys.\nI hope this use case was clear.\nIf any of you have doubts.\nPlease keep commenting\nthose doubts now,\nlet's move on and look at what\nexactly a confusion Matrix is\nthe confusion Matrix\nis the last topic\nfor descriptive statistics\nread after this.\nI'll be running a short demo\nwhere I'll be showing you\nhow you can calculate\nmean median mode\nand standard deviation variance\nand all of those values\nby using our okay.\nSo let's talk about\nconfusion Matrix now guys.\nWhat is the confusion Matrix\nnow don't get confused.\nThis is not any complex\ntopic now confusion.\nMatrix is a matrix\nthat is often used to describe\nthe performance of a model.\nRight?\nAnd this is specifically used\nfor classification models\nor a classifier\nand what it does is it\nwill calculate the accuracy\nor it will calculate the\nperformance of your classifier\nby comparing your actual results\nand Your predicted results.\nAll right.\nSo this is what it looks\nlike to prosit\nof true- and all of that.\nNow this is a little confusing.\nI'll get back to what\nexactly true positive\nto negative and all\nof this stands for for now.\nLet's look at an\nexample and let's try\nand understand what exactly\nconfusion Matrix is.\nSo guys.\nI made sure\nthat I put examples\nafter each and every topic\nbecause it's important you\nunderstand the Practical\npart of Statistics.\nAll right statistics has\nliterally nothing to do\nwith Theory you need\nto understand how Calculations\nare done in statistics.\nOkay.\nSo here what I've done is\nlet's look at a small use case.\nOkay, let's consider\nthat your given data\nabout a hundred and sixty-five\npatient's out of which hundred\nand five patients have a disease\nand the remaining 50 patients\ndon't have a disease.\nOkay.\nSo what you're going to do is\nyou will build a classifier\nthat predicts by using\nthese hundred and sixty five\nobservations your feed all\nof these 165 observations\nto your classifier\nand It will predict\nthe output every time\na new patients detail is fed\nto the classifier right now\nout of these 165 cases.\nLet's say that\nthe classifier predicted.\nYes hundred and ten times\nand no 55 times.\nAlright, so yes\nbasically stands for yes.\nThe person has a disease\nand no stands for know.\nThe person has\nnot have a disease.\nAll right, that's\npretty self-explanatory.\nBut yeah, so it predicted\nthat a hundred and ten times.\nPatient has a disease\nand 55 times that\nnor the patient\ndoesn't have a disease.\nHowever in reality only\nhundred and five patients\nin the samples have\nthe disease and 60 patients\nwho do not have\nthe disease, right?\nSo how do you calculate\nthe accuracy of your model?\nYou basically build\nthe confusion Matrix?\nAll right.\nThis is how the Matrix looks\nlike and basically denotes\nthe total number of observations\nthat you have\nwhich is 165 in our case\nactual denotes the actual use\nin the data set\nand predicted denotes\nthe predicted values\nby the classifier.\nSo the actual value is no here\nand the predicted\nvalue is no here.\nSo your classifier\nwas correctly able\nto classify 50 cases as no.\nAll right, since both\nof these are no so 50\nit was correctly able\nto classify but 10\nof these cases it\nincorrectly classified meaning\nthat your actual value here\nis no but you classifier\npredicted it as yes or a\nthat's why this And over here\nsimilarly it wrongly predicted\nthat five patients\ndo not have diseases\nwhereas they actually\ndid have diseases\nand it correctly\npredicted hundred patients,\nwhich have the disease.\nAll right.\nI know this is\na little bit confusing.\nBut if you look\nat these values no,\nno 50 meaning\nthat it correctly\npredicted 50 values No\nYes means that it\nwrongly predicted.\nYes for the values are it\nwas supposed to predict.\nNo.\nAll right.\nNow what exactly is?\nIs this true positive\nto negative and all of that?\nI'll tell you what\nexactly it is.\nSo true positive are the cases\nin which we predicted a yes\nand they do not actually\nhave the disease.\nAll right, so it is\nbasically this value\nalready predicted a yes here,\neven though they\ndid not have the disease.\nSo we have 10 true positives\nright similarly true-\nis we predicted know\nand they don't have\nthe disease meaning\nthat this is correct.\nFalse positive is be predicted.\nYes, but they do not actually\nhave the disease.\nAll right.\nThis is also known as type\n1 error falls- is we predicted.\nNo, but they actually\ndo not have the disease.\nSo guys basically false negative\nand true negatives are basically\ncorrect classifications.\nAll right.\nSo this was confusion Matrix\nand I hope this concept\nis clear again guys.\nIf you have doubts,\nplease comment your doubt\nin the comment section.\nSo guys that was\ndescriptive statistics now,\nBefore we go to probability.\nI promised all\nthat will run a small demo\nin our all right,\nwe'll try and understand\nhow mean median mode\nworks in our okay,\nso let's do that first.\nSo guys again\nwhat we just discussed so far\nwas descriptive statistics.\nAll right, next we're going\nto discuss probability\nand then we'll move\non to inferential statistics.\nOkay in financial statistics is\nbasically the second\ntype of Statistics.\nOkay now to make things\nmore clear of you,\nlet me just zoom in.\nSo guys it's always best\nto perform practical\nimplementations in order\nto understand the concepts\nin a better way.\nOkay, so here will be executing\na small demo that will show you\nhow to calculate the mean median\nmode variance standard deviation\nand how to study the variables\nby plotting a histogram.\nOkay.\nDon't worry.\nIf you don't know\nwhat a histogram is.\nIt's basically a frequency plot.\nThere's no big signs behind it.\nAlright, this is\na very simple demo\nbut it also forms\na foundation that everything.\nMachine learning algorithm\nis built upon.\nOkay, you can say\nthat most of the machine\nlearning algorithms actually\nall the machine learning\nalgorithms and deep\nlearning algorithms have\nthis basic concept behind them.\nOkay, you need to know\nhow mean median mode\nand all of that is calculated.\nSo guys am using\nthe our language to perform this\nand I'm running this\non our studio.\nFor those of you\nwho don't know our language.\nI will leave a couple of links\nin the description box.\nYou can go through those videos.\nSo what we're doing is we\nare randomly generated.\nEating numbers and Miss\nstoring it in a variable\ncalled data, right?\nSo if you want to see\nthe generated numbers\njust to run the line data,\nright this variable basically\nstores all our numbers.\nAll right.\nNow, what we're going\nto do is we're going\nto calculate the mean now.\nAll you have to do in our\nis specify the word mean\nalong with the data\nthat you're calculating\nthe mean of and I\nwas assigned this whole thing\ninto a variable called mean\nJust hold the mean\nvalue of this data.\nSo now let's look at the mean\nfor that abuser function\ncalled print and mean.\nAll right.\nSo our mean is around 5.99.\nOkay.\nNext is calculating the median.\nIt's very simple guys.\nAll you have to do is use\nthe function median\nor write and pass the data as\na parameter to this function.\nThat's all you have to do.\nSo our provides functions\nfor each and everything.\nAll right statistics is\nvery easy when it comes to R\nbecause R is basically\na statistical language.\nOkay.\nSo all you have to do is\njust name the function\nand that function is Ready\nin built in your art.\nOkay, so your median\nis around 6.4.\nSimilarly.\nWe will calculate the mode.\nAll right.\nLet's run this function.\nI basically created\na small function\nfor calculating the mode.\nSo guys, this is\nour mode meaning\nthat this is the most\nrecurrent value right now.\nWe're going to calculate\nthe variance and the standard\ndeviation for that.\nAgain.\nWe have a function in are called\nas we're all right.\nAll you have to do is pass\nthe data to that function.\nOkay, similarly will calculate\nthe standard deviation,\nwhich is basically\nthe square root of your variance\nright now will Rent\nthe standard deviation, right?\nThis is our\nstandard deviation value.\nNow.\nFinally, we will just plot\na small histogram histogram\nis nothing but it's\na frequency plot already in\nshow you how frequently\na data point is occurring.\nSo this is the histogram\nthat we've just created\nit's quite simple in our\nbecause our has a lot\nof packages and a lot\nof inbuilt functions\nthat support statistics.\nAll right.\nIt is a statistical language\nthat is mainly used by\ndata scientists or by data\nand analysts and\nmachine learning Engineers\nbecause they don't have\nto student code these functions.\nAll they have to do is\nthey have to mention the name\nof the function and pass\nthe corresponding parameters.\nSo guys that was the entire\ndescriptive statistics module\nand now we will discuss\nabout probability.\nOkay.\nSo before we understand\nwhat exactly probability is,\nlet me clear out a very\ncommon misconception people\noften tend to ask\nme this question.\nWhat is the relationship between\nstatistics and probability?\nSo probability and statistics\nare related fields.\nAll right.\nSo probability is\na mathematical method used\nfor statistical analysis.\nTherefore we can say\nthat a probability and\nstatistics are interconnected\nbranches of mathematics\nthat deal with analyzing the\nrelative frequency of events.\nSo they're very\ninterconnected feels\nand probability makes\nuse of statistics\nand statistics makes use\nof probability or a they're\nvery interconnected Fields.\nSo that is the relationship\nbetween said It is\nsix and probability.\nNow.\nLet's understand what\nexactly is probability.\nSo probability is the measure\nof How likely an event\nwill occur to be more precise.\nIt is the ratio\nof desired outcome\nto the total outcomes.\nNow, the probability\nof all outcomes always sum up\nto 1 the probability will always\nsum up to 1 probability\ncannot go beyond one.\nOkay.\nSo either your probability\ncan be 0 or it can be 1\nor it can In the form\nof decimals like 0.5\nto or 0.55 or it can be\nin the form of 0.5 0.7 0.9.\nBut it's valuable always stay\nbetween the range 0 and 1 okay,\nanother famous example\nof probability is rolling\na dice example.\nSo when you roll a dice you get\nsix possible outcomes, right?\nYou get one two,\nthree four and five six phases\nof a dies now each possibility\nonly has one outcome.\nSo what is the probability\nthat on rolling a dice?\nYou will get 3 the probability\nis 1 by 6 right\nbecause there's only one phase\nwhich has the number 3 on it\nout of six phases.\nThere's only one phase\nwhich has the number three.\nSo the probability of getting 3\nwhen you roll a dice\nis 1 by 6 similarly.\nIf you want to find\nthe probability of getting\na number 5 again,\nthe probability is\ngoing to be 1 by 6.\nAll right.\nSo all of this will sum up to 1.\nAll right, so guys,\nthis is exactly what Ability is\nit's a very simple concept.\nWe all learnt it in 8 standard\nonwards right now.\nLet's understand the\ndifferent terminologies\nthat are related to probability.\nNow that three terminologies\nthat you often come across\nwhen we talk about probability.\nWe have something known\nas the random experiment.\nOkay.\nIt's basically an experiment\nor a process for which\nthe outcomes cannot be\npredicted with certainty.\nAll right.\nThat's why you use probability.\nYou're going to use probability\nin order to predict the outcome\nwith Some sort\nof certainty sample space is the\nentire possible set of outcomes\nof a random experiment\nand event is one or more\noutcomes of an experiment.\nSo if you consider the example\nof rolling a dice now,\nlet's say that you want\nto find out the probability\nof getting a to\nwhen you roll the dice.\nOkay.\nSo finding this probability\nis the random experiment\nthe sample space is basically\nyour entire possibility.\nOkay.\nSo one two, three,\nfour five six Is are\nthere and out of that you\nneed to find the probability\nof getting a 2 right?\nSo all the possible outcomes\nwill basically represent\nyour sample space gives a 1 to 6\nare all your possible outcomes.\nThis represents your sample\nspace now event is one or more\noutcome of an experiment.\nSo in this case\nmy event is to get a tattoo\nwhen I roll a dice, right?\nSo my event is the probability\nof getting a to\nwhen I roll a dice,\nso guys, this is basically\nwhat random experiment samples.\nAll space and event\nreally means alright now,\nlet's discuss the different\ntypes of events.\nThere are two types of events\nthat you should know\nabout there is disjoint\nand non disjoint events.\nDisjoint events are events\nthat do not have\nany common outcome.\nFor example,\nif you draw a single card\nfrom a deck of cards,\nit cannot be a king\nand a queen correct\nit can either be king\nor it can be Queen now\na non disjoint events are events\nthat have common out.\nFor example a student\ncan get hundred marks\nin statistics and hundred\nmarks in probability.\nAll right, and also the outcome\nof a ball delivered\ncan be a no ball\nand it can be a 6 right.\nSo this is what non\ndisjoint events are or n?\nThese are very simple\nto understand right now.\nLet's move on and look\nat the different types\nof probability distribution.\nAll right, I'll be discussing\nthe three main probability\ndistribution functions.\nI'll be talking\nabout probability density.\nAaron normal distribution\nand Central limit theorem.\nOkay probability density\nfunction also known\nas PDF is concerned\nwith the relative likelihood\nfor a continuous random variable\nto take on a given value.\nAlright, so the PDF gives\nthe probability of a variable\nthat lies between\nthe range A and B.\nSo basically what you're trying\nto do is you're going to try\nand find the probability\nof a continuous random variable\nover a specified range.\nOkay.\nNow this graph denotes the PDF\nof a continuous variable.\nNow this graph is also known\nas the bell curve right?\nIt's famously called\nthe bell curve\nbecause of its shape and\nthe three important properties\nthat you need to know about\na probability density function.\nNow the graph of a PDF\nwill be continuous\nover a range this is\nbecause you're finding\nthe probability that\na continuous variable lies\nbetween the ranges A and B,\nright the second property.\nIs that the area bounded by By\nthe curve of a density function\nand the x-axis is equal\nto 1 basically the area\nbelow the curve is equal\nto 1 all right,\nbecause it denotes probability\nagain the probability\ncannot arrange more\nthan one it has to be\nbetween 0 and 1\nproperty number three is\nthat the probability\nthat our random variable\nassumes a value between A\nand B is equal to the area\nunder the PDF bounded\nby A and B. Okay.\nNow what this means,\nis that the probability\nYou is denoted by\nthe area of the graph.\nAll right, so whatever value\nthat you get here,\nwhich basically one\nis the probability\nthat a random variable will lie\nbetween the range A and B.\nAll right.\nSo I hope all\nof you have understood the\nprobability density function.\nIt's basically the probability\nof finding the value\nof a continuous random variable\nbetween the range A and B.\nAll right.\nNow, let's look\nat our next distribution,\nwhich is normal\ndistribution now.\nNormal distribution,\nwhich is also known as\nthe gaussian distribution\nis a probability distribution\nthat denotes the\nsymmetric property\nof the mean right meaning\nthat the idea\nbehind this function.\nIs that the data\nnear the mean occurs more\nfrequently than the data\naway from the mean.\nSo what it means to say is\nthat the data around the mean\nrepresents the entire data set.\nOkay.\nSo if you just take\na sample of data\naround the mean it can represent\nthe entire data set now similar\nto Probability density function\nthe normal distribution appears\nas a bell curve right now\nwhen it comes\nto normal distribution.\nThere are two important factors.\nAll right, we have the mean\nof the population\nand the standard deviation.\nOkay, so the mean and the graph\ndetermines the location\nof the center of the graph,\nright and the standard deviation\ndetermines the height\nof the graph.\nOkay.\nSo if the standard deviation\nis large the curve is going\nto look something like this.\nAll right, it'll\nbe short and wide.\nI'd and if the standard\ndeviation is small the curve\nis tall and narrow.\nAll right.\nSo this was it\nabout normal distribution.\nNow, let's look\nat the central limit theorem.\nNow the central\nlimit theorem states\nthat the sampling distribution\nof the mean of any independent\nrandom variable will be normal\nor nearly normal\nif the sample size\nis large enough now,\nthat's a little confusing.\nOkay.\nLet me break it down for\nyou now in simple terms\nif we had a large population\nand be Why did it\nin too many samples,\nthen the mean of all the samples\nfrom the population will be\nalmost equal to the mean\nof the entire population right?\nMeaning that each of the sample\nis normally distributed.\nRight?\nSo if you compare the mean\nof each of the sample,\nit will almost be equal\nto the mean of the population.\nRight?\nSo this graph basically shows\na more clear understanding\nof the central limit theorem red\nyou can see each sample here\nand the mean of each sample.\nOil is almost along\nthe same line, right?\nOkay.\nSo this is exactly\nwhat the central limit theorem\nStates now the accuracy\nor the resemblance to\nthe normal distribution depends\non two main factors, right?\nSo the first is the number\nof sample points\nthat you consider.\nAll right,\nand the second is the shape\nof the underlying population.\nNow the shape obviously depends\non the standard deviation\nand the mean\nof a sample, correct.\nSo guys the central\nlimit theorem basically states\nthat eats Bill will be normally\ndistributed in such a way\nthat the mean of each sample\nwill coincide with the mean\nof the actual population.\nAll right in short terms.\nThat's what central\nlimit theorem States.\nAll right, and this holds true\nonly for a large data set mostly\nfor a small data set\nand there are more deviations\nwhen compared to a large\ndata set is because of\nthe scaling Factor, right?\nThe small is deviation\nin a small data set will change\nthe value vary drastically,\nbut in a large data\nset a small deviation\nwill not matter at all.\nNow, let's move.\nVaughn and look\nat our next topic\nwhich is the different\ntypes of probability.\nThis is a important topic\nbecause most of your problems\ncan be solved by understanding\nwhich type of probability should\nI use to solve this problem?\nRight?\nSo we have three important\ntypes of probability.\nWe have marginal joint\nand conditional probability.\nSo let's discuss each\nof these now the probability of\nan event occurring unconditioned\non any other event\nis known as marginal.\nOr unconditional probability.\nSo let's say that you want\nto find the probability\nthat a card drawn is a heart.\nAll right.\nSo if you want to\nfind the probability\nthat a card drawn is a heart\nThe Profit will be 13 by 52\nsince there are\n52 cards in a deck\nand there are 13 hearts\nin a deck of cards.\nRight and there are\n52 cards in a total deck.\nSo your marginal probability\nwill be 13 by 52.\nThat's about\nmarginal probability.\nNow, let's understand\nwhat is joint probability.\nAnd now joint probability\nis a measure of two events\nhappening at the same time.\nOkay, let's say\nthat the two events are A and B.\nSo the probability of event A\nand B occurring is\nthe intersection of A and B.\nSo for example,\nif you want to\nfind the probability\nthat a card is a four and a red\nthat would be joint probability.\nAll right, because\nyou're finding a card\nthat is 4 and the card\nhas to be red in color.\nSo for the answer to this\nwould be to Biceps you do\nbecause we have 1/2\nin heart and we have 1/2\nand diamonds, correct.\nSo both of these are red\nand color therefore.\nOur probability is to by 52\nand if you further down\nit is 1 by 26, right?\nSo this is what\njoint probability is all\nabout moving on.\nLet's look at what exactly\nconditional probability is.\nSo if the probability\nof an event or an outcome\nis based on the occurrence\nof a previous event\nor an outcome.\nThen you call it as\na conditional probability.\nOkay.\nSo the conditional probability\nof an event B is the probability\nthat the event will occur given\nthat an event a\nhas already occurred.\nRight?\nSo if a and b are\ndependent events,\nthen the expression\nfor conditional probability\nis given by this.\nNow this first term\non the left hand side,\nwhich is p b of a is\nbasically the probability\nof event B occurring\ngiven that event a\nhas already occurred.\nSo like I said,\nif a and b are dependent events\nthan this is the expression\nbut if a and b are\nindependent events,\nand the expression\nfor conditional probability is\nlike this, right?\nSo guys P of A and B of B\nis obviously the probability\nof a and probability\nof B right now,\nlet's move on now in order\nto understand conditional\nprobability joint probability\nand marginal probability.\nLet's look at a small use case.\nOkay now basically\nwe're going to Take a data set\nwhich examines the salary\npackage and training\nundergone my candidates.\nOkay.\nNow in this there are\n60 candidates a without training\nand forty five candidates,\nwhich have enrolled\nfor Adder Acres training right.\nNow the task here is you have\nto assess the training\nwith a salary package.\nOkay.\nLet's look at this\nin a little more depth.\nSo in total,\nwe have hundred and five\ncandidates out of which 60\nof them have not enrolled\nFrederick has training\nand 45 of them have enrolled\nfor a deer Acres.\nInning.\nAll right.\nThis is the small survey\nthat was conducted\nand this is the rating\nof the package or the salary\nthat they got right?\nSo if you read through the data,\nyou can understand\nthere were five candidates\nwithout Eddie record training\nwho got a very\npoor salary package.\nOkay.\nSimilarly, there are\n30 candidates with\nEd Eureka training\nwho got a good package, right?\nSo guys, basically you're\ncomparing the salary package\nof a person depending on\nwhether or not they've enrolled\nfor a A core training right?\nThis is our data set.\nNow.\nLet's look at our problem\nstatement find the probability\nthat a candidate\nhas undergone editor Acres\ntraining quite simple,\nwhich type of\nprobability is this.\nThis is marginal probability.\nRight?\nSo the probability\nthat a candidate has undergone\nEdge rakers training is\nobviously 45 divided\nby a hundred and five\nsince 45 is the number\nof candidates with\nEddie record raining\nand hundred and five is\nthe total number of candidates,\nso you Value\nof approximately 0.4 to or\nI that's the probability\nof a candidate\nthat has undergone a Judaica\nstraining next question\nfind the probability\nthat a candidate has attended\nedger a constraining\nand also has good package.\nNow.\nThis is obviously a joint\nprobability problem, right?\nSo how do you\ncalculate this now?\nSince our table is quite\nformatted we can directly find\nthat people who have\ngotten a good package\nalong with Eddie record\nraining or 30, right?\nSo out of hundred and\nfive people 30 people\nhave education training\nand a good package, right?\nThey specifically asking\nfor people with Ado Rekha\ntraining remember that right?\nThe question is\nfind the probability\nthat a candidate has attended\neditor Acres training\nand also has a good package.\nAlright, so we need\nto consider two factors\nthat is a candidate who's\naddenda deaderick has training\nand who has a good package.\nSo clearly that number\nis 30 30 divided by\ntotal number of candidates,\nwhich is 1 0 Five, right.\nSo here you get\nthe answer clearly.\nNext we have\nfind the probability\nthat a candidate has\na good package given that he\nhas not undergone training.\nOkay.\nNow this is clearly\nconditional probability\nbecause here you're defining\na condition you're saying\nthat you want to find\nthe probability of a candidate\nwho has a good package given\nthat he's not undergone.\nAny training, right?\nThe condition is that he's\nnot undergone any training.\nAll right.\nSo the number of people\nwho have not undergone\ntraining are 60 and out\nof that five of them have got\na good package, right?\nSo that's why this is Phi by 60\nand not 5 by hundred and five\nbecause here they\nhave clearly mentioned has\na good package given that he\nhas not undergone training.\nYou have to only consider people\nwho have not\nundergone training, right?\nSo only five people\nwho have not undergone\ntraining have gotten\na good package, right?\nSo 5 divided by 60 you get\na probability of around 208\nwhich is pretty low, right?\nOkay.\nSo this was all\nabout the different\ntypes of probability.\nNow, let's move on and look at\nour last Topic in probability,\nwhich is base theorem.\nNow guys Bayes theorem is\na very important concept\nwhen it comes\nto statistics and probability.\nIt is majorly used\nin knife bias algorithm.\nThose of you who aren't aware.\nNow I've bias is\na supervised learning\nclassification algorithm\nand it is mainly Used\nin Gmail spam filtering,\nright a lot of you\nmight have noticed\nthat if you open up Gmail,\nyou'll see that you have\na folder called spam right\nor that is carried out\nthrough machine learning\nand the algorithm used\nthere is knife bias, right?\nSo now let's discuss what\nexactly the Bayes theorem is\nand what it denotes\nthe bias theorem is used\nto show the relation between\none conditional probability\nand it's inverse.\nAll right, basically Nothing,\nbut the probability\nof an event occurring based\non prior knowledge of conditions\nthat might be related\nto the same event.\nOkay.\nSo mathematically\nthe bell's theorem\nis represented like this,\nright like shown\nin this equation.\nThe left-hand term is referred\nto as the likelihood ratio,\nwhich measures the probability\nof occurrence of event B,\ngiven an event a okay\non the left hand side is\nwhat is known as\nthe posterior right\nis referred to as posterior.\nAre which means\nthat the probability\nof occurrence of a given\nan event B, right?\nThe second term is referred\nto as the likelihood ratio\nor a this measures the\nprobability of occurrence of B,\ngiven an event a now P\nof a is also known as the prior\nwhich refers to the actual\nprobability distribution of A\nand P of B is again,\nthe probability of B, right.\nThis is the bias theorem\nin order to better\nunderstand the base theorem.\nLet's look at a small example.\nLet's say that we Three balls we\nhave about a bowel be\nand bouncy okay barley\ncontains two blue balls\nand for red balls bowel\nbe contains eight blue balls\nand for red balls baozi\ncontains one blue ball\nand three red balls.\nNow if we draw one ball\nfrom each Bowl,\nwhat is the probability to draw\na blue ball from a bowel a\nif we know that we drew\nexactly a total\nof two blue balls right if you\ndidn't Understand the question.\nPlease.\nRead it.\nI shall pause\nfor a second or two.\nRight.\nSo I hope all of you\nhave understood the question.\nOkay.\nNow what I'm going to do\nis I'm going to draw\na blueprint for you\nand tell you how exactly\nto solve the problem.\nBut I want you all to give\nme the solution\nto this problem, right?\nI'll draw a blueprint.\nI'll tell you\nwhat exactly the steps are\nbut I want you to come\nup with a solution\non your own right the formula\nis also given to you.\nEverything is given to you.\nAll you have to do is come up\nwith the final answer.\nRight?\nLet's look at how you\ncan solve this problem.\nSo first of all,\nwhat we will do is\nLet's consider a all right,\nlet a be the event\nof picking a blue ball\nfrom bag in and let\nX be the event of picking\nexactly two blue balls,\nright because these\nare the two events\nthat we need to calculate\nthe probability of now\nthere are two probabilities\nthat you need to consider here.\nOne is the event of picking\na blue ball from bag a\nand the other is the event of\npicking exactly two blue balls.\nOkay.\nSo these two are represented\nby a and X respectively Lee\nso what we want is\nthe probability of occurrence\nof event a given X,\nwhich means that given\nthat we're picking\nexactly two blue balls,\nwhat is the probability\nthat we are picking\na blue ball from bag?\nSo by the definition\nof conditional probability,\nthis is exactly what\nour equation will look like.\nCorrect.\nThis is basically a occurrence\nof event a given an event X\nand this is\nthe probability of a and x\nand this is the probability\nof X alone, correct?\nAnd what we need to do\nis we need to find\nthese two probabilities\nwhich is probability of a\nand X occurring together\nand probability of X. Okay.\nThis is the entire solution.\nSo how do you find P probability\nof X this you can do\nin three ways.\nSo first is white ball\nfrom a either white from be\nor read from see now first is\nto find the probability of x x\nbasically represents the event\nof picking exactly\ntwo blue balls.\nRight.\nSo these are the three ways\nin which it is possible.\nSo you'll pick one blue ball\nfrom bowel a and one from bowel\nbe in the second case.\nYou can pick one\nfrom a and another blue ball\nfrom see in the third case.\nYou can pick a blue\nball from Bagby\nand a blue ball from bagsy.\nRight?\nThese are the three ways\nin which it is possible.\nSo you need to find\nthe probability of each\nof this step two is\nthat you need to find\nthe probability of a\nand X occurring together.\nThis is the sum\nof terms 1 and 2.\nOkay, this is\nbecause in both\nof these events,\nwe are picking a ball\nfrom bag, correct.\nSo there is find out\nthis probability and let\nme know your answer\nin the comment section.\nAll right.\nWe'll see if you get\nthe answer right?\nI gave you the entire\nsolution to this.\nAll you have to do is\nsubstitute the value right?\nIf you want a second or two,\nI'm going to pause on the screen\nso that you can go through this\nin a more clear away.\nRight?\nRemember that you need\nto calculate two.\nTease the first probability\nthat you need to calculate is\nthe event of picking a blue ball\nfrom bag a given\nthat you're picking\nexactly two blue balls.\nOkay, II probability\nyou need to calculate\nis the event of picking\nexactly two blue bonds.\nAll right.\nThese are the two probabilities.\nYou need to calculate so\nremember that and this\nis the solution.\nAll right, so guys make sure\nyou mention your answers\nin the comment section for now.\nLet's move on and Look\nat our next topic,\nwhich is the\ninferential statistics.\nSo guys, we just completed the\nprobability module right now.\nWe will discuss\ninferential statistics,\nwhich is the second\ntype of Statistics.\nWe discussed descriptive\nstatistics earlier.\nAlright, so like I\nmentioned earlier inferential\nstatistics also known as\nstatistical inference is\na branch of Statistics\nthat deals with forming\ninferences and predictions\nabout a population based\non a sample of data.\nAre taken from the population.\nAll right, and the question\nyou should ask is\nhow does one form inferences\nor predictions on a sample?\nThe answer is you\nuse Point estimation?\nOkay.\nNow you must be wondering\nwhat is point estimation\none estimation is concerned\nwith the use of the sample data\nto measure a single value\nwhich serves as\nan approximate value\nor the best estimate of\nan unknown population parameter.\nThat's a little confusing.\nLet me break it down\nto you for Camping\nin order to calculate the mean\nof a huge population.\nWhat we do is we first draw out\nthe sample of the population\nand then we find the sample mean\nright the sample mean\nis then used to estimate\nthe population mean this is\nbasically Point estimate,\nyou're estimating the value\nof one of the parameters\nof the population, right?\nBasically the main\nyou're trying to estimate\nthe value of the mean.\nThis is what point estimation is\nthe two main terms\nin point estimation.\nThere's something known as\nas the estimator\nand the something known\nas the estimate estimator\nis a function of the sample\nthat is used to find\nout the estimate.\nAlright in this example.\nIt's basically the sample\nmean right so a function\nthat calculates the sample\nmean is known as the estimator\nand the realized value\nof the estimator is\nthe estimate right?\nSo I hope Point\nestimation is clear.\nNow, how do you\nfind the estimates?\nThere are four common ways\nin which you can do this.\nThe first one is method\nof Moment you'll\nwhat you do is\nyou form an equation\nin the sample data set\nand then you analyze\nthe similar equation\nin the population data\nset as well like\nthe population mean\npopulation variance and so on.\nSo in simple terms,\nwhat you're doing is you're\ntaking down some known facts\nabout the population\nand you're extending\nthose ideas to the sample.\nAlright, once you do that,\nyou can analyze the sample\nand estimate more\nessential or more complex\nvalues right next.\nWe have maximum likelihood.\nBut this method basically uses\na model to estimate a value.\nAll right.\nNow a maximum likelihood\nis majorly based on probability.\nSo there's a lot of probability\ninvolved in this method next.\nWe have the base estimator\nthis works by minimizing\nthe errors or the average risk.\nOkay, the base estimator\nhas a lot to do\nwith the Bayes theorem.\nAll right, let's\nnot get into the depth\nof these estimation methods.\nFinally.\nWe have the best unbiased\nestimators in this method.\nThere are seven unbiased\nestimators that can be used\nto approximate a parameter.\nOkay.\nSo Guys these were\na couple of methods\nthat are used\nto find the estimate\nbut the most well-known method\nto find the estimate is known as\nthe interval estimation.\nOkay.\nThis is one of the most\nimportant estimation\nmethods or at this is\nwhere confidence interval also\ncomes into the picture right\napart from interval estimation.\nWe also have something\nknown as margin of error.\nSo I'll be discussing\nall of this.\nIn the upcoming slides.\nSo first let's understand.\nWhat is interval estimate?\nOkay, an interval\nor range of values,\nwhich are used to estimate a\npopulation parameter is known as\nan interval estimation, right?\nThat's very understandable.\nBasically what they're trying to\nsee is you're going to estimate\nthe value of a parameter.\nLet's say you're trying to find\nthe mean of a population.\nWhat you're going to do is\nyou're going to build a range\nand your value will lie in\nthat range or in that interval.\nAll right.\nSo this way your output\nis going to be more accurate\nbecause you've not predicted\na point estimation instead.\nYou have estimated an interval\nwithin which your value\nmight occur, right?\nOkay.\nNow this image clearly shows\nhow Point estimate and interval\nestimate or different.\nSo where's interval estimate\nis obviously more accurate\nbecause you're not just focusing\non a particular value\nor a particular point\nin order to predict\nthe probability instead.\nYou're saying that\nthe value might be\nwithin this range between\nthe lower confidence limit\nand the upper confidence limit.\nAll right, this is denotes\nthe range or the interval.\nOkay, if you're still confused\nabout interval estimation,\nlet me give you a small example\nif I stated that I will take\n30 minutes to reach the theater.\nThis is known\nas Point estimation.\nOkay, but if I stated\nthat I will take\nbetween 45 minutes\nto an hour to reach the theater.\nThis is an example\nof Will estimation all right.\nI hope it's clear.\nNow now interval estimation\ngives rise to two important\nstatistical terminologies one\nis known as confidence interval\nand the other is known\nas margin of error.\nAll right.\nSo there's it's important\nthat you pay attention\nto both of these terminologies\nconfidence interval is one\nof the most significant measures\nthat are used to check\nhow essential machine\nlearning model is.\nAll right.\nSo what is confidence interval\nconfidence interval is\nthe measure of your confidence\nthat the interval\nestimated contains\nthe population parameter\nor the population mean\nor any of those parameters\nright now statisticians\nuse confidence interval\nto describe the amount\nof uncertainty associated\nwith the sample estimate of\na population parameter now guys,\nthis is a lot of definition.\nLet me just make you\nunderstand confidence interval\nwith a small example.\nOkay.\nLet's say that you perform\na survey and you survey\na group of cat owners.\nThe see how many cans of cat\nfood they purchase in one year.\nOkay, you test\nyour statistics at the 99\npercent confidence level\nand you get\na confidence interval\nof hundred comma 200 this means\nthat you think\nthat the cat owners\nby between hundred to two\nhundred cans in a year and also\nsince the confidence\nlevel is 99% shows\nthat you're very confident\nthat the results are, correct.\nOkay.\nI hope all of you\nare clear with that.\nAlright, so your confidence\ninterval here will be\na hundred and two hundred\nand your confidence level\nwill be 99% Right?\nThat's the difference\nbetween confidence interval\nand confidence level So\nwithin your confidence interval\nyour value is going to lie and\nyour confidence level will show\nhow confident you are\nabout your estimation, right?\nI hope that was clear.\nLet's look at margin of error.\nNo margin of error\nfor a given level of confidence\nis a greatest possible distance\nbetween the Point estimate\nand the value of the parameter\nthat it is estimating\nyou can say\nthat it is a deviation from\nthe actual point estimate right.\nNow.\nThe margin of error\ncan be calculated\nusing this formula now zc\nher denotes the critical value\nor the confidence interval\nand this is X standard\ndeviation divided by root\nof the sample size.\nAll right, n is basically\nthe sample size now,\nlet's understand how\nyou can estimate\nthe confidence intervals.\nSo guys the level of confidence\nwhich is denoted by\nC is the probability\nthat the interval estimate\ncontains a population parameter.\nLet's say that you're trying\nto estimate the mean.\nAll right.\nSo the level of confidence\nis the probability\nthat the interval estimate\ncontains a population parameter.\nSo this interval\nbetween minus Z and z\nor the area beneath this curve\nis nothing but the probability\nthat the interval estimate\ncontains a population parameter.\nYou don't all right.\nIt should basically\ncontain the value\nthat you are predicting right.\nNow.\nThese are known\nas critical values.\nThis is basically\nyour lower limit\nand your higher\nlimit confidence level.\nAlso, there's something\nknown as the Z score now.\nThis court can be calculated\nby using the standard\nnormal table, right?\nIf you look it up anywhere\non Google you'll find\nthe z-score table\nor the standard normal\ntable get to understand\nhow this is done.\nLet's look at a small example.\nOkay, let's say that the level\nof Vince is 90% This means\nthat you are 90% confident\nthat the interval contains\nthe population mean.\nOkay, so the remaining 10%\nwhich is out of hundred percent.\nThe remaining 10%\nis equally distributed\non these Dale regions.\nOkay, so you have 0.05 here\nand 0.05 over here, right?\nSo on either side\nof see you will distribute\nthe other leftover percentage\nnow these these scores\nare calculated from the table\nas I mentioned before.\nAll right one.\nN64 5 is get collated\nfrom the standard normal table.\nOkay.\nSo guys how you estimate\nthe level of confidence.\nSo to sum it up.\nLet me tell you the steps\nthat are involved\nin constructing a\nconfidence interval first.\nYou'll start by identifying\na sample statistic.\nOkay.\nThis is the statistic\nthat you will use to estimate\na population parameter.\nThis can be anything\nlike the mean\nof the sample next you\nwill select a confidence level\nnow the confidence level\ndescribes the uncertainty\nof a Sampling method right\nafter that you'll find\nsomething known as the margin\nof error, right?\nWe discuss margin\nof error earlier.\nSo you find this based\non the equation\nthat I explained\nin the previous slide,\nthen you'll finally specify\nthe confidence interval.\nAll right.\nNow, let's look\nat a problem statement\nto better understand\nthis concept a random sample\nof 32 textbook prices is taken\nfrom a local College Bookstore.\nThe mean of the sample is so so\nand so and the sample\nstandard deviation is\nThis use a 95% confident level\nand find the margin\nof error for the mean price\nof all text books\nin the bookstore.\nOkay.\nNow, this is a very\nstraightforward question.\nIf you want you can read\nthe question again.\nAll you have to do is you have\nto just substitute the values\ninto the equation.\nAll right, so guys,\nwe know the formula for margin\nof error you take the Z score\nfrom the table.\nAfter that we have deviation\nMadrid's 23.4 for right\nand that's standard deviation\nand n stands for the number\nof samples here.\nThe number of samples is\n32 basically 32 textbooks.\nSo approximately your margin\nof error is going to be\naround 8.1 to this is\na pretty simple question.\nAll right.\nI hope all of you\nunderstood this now\nthat you know,\nthe idea behind\nconfidence interval.\nLet's move ahead to one\nof the most important topics\nin statistical inference,\nwhich is hypothesis\ntesting, right?\nSo Sigelei statisticians\nuse hypothesis testing\nto formally check\nwhether the hypothesis\nis accepted or rejected.\nOkay, hypothesis.\nTesting is an inferential\nstatistical technique\nused to determine\nwhether there is enough evidence\nin a data sample to infer\nthat a certain condition holds\ntrue for an entire population.\nSo to understand\nthe characteristics\nof a general population,\nwe take a random sample,\nand we analyze the properties\nof the sample right we test.\nWhether or not the identified\nconclusion represent\nthe population accurately\nand finally we interpret\ntheir results now\nwhether or not to accept\nthe hypothesis depends\nupon the percentage value\nthat we get from the hypothesis.\nOkay, so to\nbetter understand this,\nlet's look at a small\nexample before that.\nThere are few steps\nthat are followed in hypothesis,\ntesting you begin\nby stating the null\nand the alternative hypothesis.\nAll right.\nI'll tell you what\nexactly these terms are\nand then you formulate.\nAnalysis plan right after that\nyou analyze the sample data\nand finally you can\ninterpret the results\nright now to understand\nthe entire hypothesis testing.\nWe look at a good example.\nOkay now consider\nfor boys Nick jean-bob\nand Harry these boys\nwere caught bunking a class\nand they were asked\nto stay back at school\nand clean the classroom\nas a punishment, right?\nSo what John did is he decided\nthat four of them would take\nturns to clean their classrooms.\nHe came up with a plan\nof writing each of their names\non chits and putting them\nin a bout now every day.\nThey had to pick up\na name from the bowel\nand that person had to play\nin the clock, right?\nThat sounds pretty fair enough\nnow it is been three days\nand everybody's name has come up\nexcept John's assuming\nthat this event\nis completely random\nand free of bias.\nWhat is a probability\nof John not treating\nright or is the probability\nthat he's not actually\ncheating this can Solved\nby using hypothesis testing.\nOkay.\nSo we'll Begin by calculating\nthe probability of John\nnot being picked for a day.\nAlright, so we're\ngoing to assume\nthat the event is free of bias.\nSo we need to find\nout the probability\nof John not cheating right\nfirst we'll find the probability\nthat John is not picked\nfor a day, right?\nWe get 3 out of 4,\nwhich is basically 75%\n75% is fairly high.\nSo if John is not picked\nfor three days in a row\nthe Probability will drop down\nto approximately 42% Okay.\nSo three days in a row meaning\nthat is the probability\ndrops down to 42 percent.\nNow, let's consider a situation\nwhere John is not picked\nfor 12 days in a row\nthe probability drops down\nto Tea Point two percent.\nOkay, that's the probability\nof John cheating becomes\nfairly high, right?\nSo in order\nfor statisticians to come\nto a conclusion,\nthey Define what is known\nas the threshold value.\nRight considering\nthe above situation\nif the threshold value\nis set to 5 percent.\nIt would indicate\nthat if the probability lies\nbelow 5% then John is cheating\nhis way out of detention.\nBut if the probability is\nabout threshold value then John\nit just lucky and his name\nisn't getting picked.\nSo the probability\nand hypothesis testing give rise\nto two important components\nof hypothesis testing,\nwhich is null hypothesis\nand alternative hypothesis.\nNull.\nHypothesis is based.\nBasically approving\nthe Assumption alternate\nhypothesis is\nwhen your result disapproves\nthe Assumption right therefore\nin our example,\nif the probability\nof an event occurring\nis less than 5% which it is\nthen the event is biased hence.\nIt proves the\nalternate hypothesis.\nUndoubtedly machine learning is\nthe most in-demand technology\nin today's market.\nIt's applications.\nFrom Seth driving cause\nto predicting deadly diseases\nsuch as ALS the high demand\nfor machine learning skills\nis the motivation\nbehind today's session.\nSo let me discuss\nthe agenda with you first.\nNow, we're going\nto begin the session\nby understanding the need\nfor machine learning and why\nit is important after that.\nWe look at what exactly\nmachine learning is\nand then we'll discuss a couple\nof machine learning definitions.\nOnce we're done with that.\nWe'll look at the\nmachine learning process\nand how you can solve\na problem by using Using\nthe machine learning process\nnext we will discuss the types\nof machine learning\nwhich includes\nsupervised unsupervised\nand reinforcement learning.\nOnce we're done with that.\nWe'll discuss the different\ntypes of problems\nthat can be solved by\nusing machine learning.\nFinally.\nWe will end this session\nby looking at a demo\nwhere we'll see how you\ncan perform weather forecasting\nby using machine learning.\nAll right, so guys,\nlet's get started\nwith our first topic.\nSo what is the importance\nor what is the need\nfor machine learning now?\nSince the technical Revolution,\nwe've been generating\nan immeasurable amount\nof data as for research\nwith generating around\n2.5 quintillion bytes\nof data every single day\nand it is estimated\nthat by 2020 1.7 MB of data\nwill be created every second\nfor every person on earth.\nNow that is a lot\nof data right now.\nThis data comes\nfrom sources such as\nthe cloud iot devices social\nmedia and all of that.\nSince all of us\nare very interested\nin the internet right now\nwith generating a lot of data.\nAll right, you have no idea\nhow much data we generate\nthrough social media\nall the chatting\nthat we do and all the images\nthat we post\non Instagram the videos\nthat we watch all of this\ngenerates a lot of data.\nNow how does machine\nlearning fit into all of this\nsince we're producing\nthis much data,\nwe need to find a method\nthat can analyze process\nand interpret this much data.\nAll right, and we\nneed to find a method.\nThat can make sense out of data.\nAnd that method\nis machine learning.\nNow the lot\nof talk tire companies\nand data driven company\nsuch as Netflix and Amazon\nwhich build machine learning\nmodels by using tons of data\nin order to identify\nany profitable opportunities.\nAnd if they want to avoid\nany unwanted risk it make use\nof machine learning.\nAlright, so through machine\nlearning You can predict risk\nYou can predict profits you\ncan identify opportunities,\nwhich will help you\ngrow your business.\nBusiness so now I'll show you\na couple of examples of where\nin machine learning is used.\nAll right, so I'm sure all of\nyou have been watch on Netflix.\nNow the most important thing\nabout Netflix is\nits recommendation engine.\nAll right.\nMost of Netflix's Revenue comes\nfrom its recommendation engine.\nSo the recommendation engine\nbasically studies the movie\nviewing patterns of its users\nand then recommends\nrelevant movies to them.\nAll right, it recommends movies\ndepending on users interests.\nDepending on the type\nof movies the user\nwatches and all of that.\nAlright, so that is\nhow Netflix uses\nmachine learning.\nNext.\nWe have Facebook's\nAuto tagging feature.\nNow the logic behind Facebook's\nAuto tagging feature\nis machine learning\nand neural networks.\nI'm not sure how many\nof you know this but Facebook\nmakes use of deepmind\nface verification system,\nwhich is based\non machine learning\nnatural language processing\nand neural networks.\nSo deep mine basically\nstudies the facial features\nin an image and it tag\nyour friends and family.\nAnother such example is\nAmazon's Alexa now Alexa\nis basically an advanced\nlevel virtual assistant\nthat is based\non natural language processing\nand machine learning.\nNow, it can do more\nthan just play music for you.\nAll right, it can book\nyour Uber it can connect\nwith other I/O devices\nthat your house it\ncan track your health.\nIt can order food\nonline and all of that.\nSo data, and machine learning\nare basically the main factors\nbehind Alex has power\nanother such example is\nthe Google spam filter.\nSo guys Gmail basically\nmakes use of machine learning\nto filter out spam messages.\nIf any of you just\nopen your Gmail inbox,\nyou'll see that there\nare separate sections.\nThere's one for primary\nthis social the spam\nand the Joe general made now\nbasically Gmail makes use\nof machine learning algorithms\nand natural language processing\nto an Is emails in real time\nand then classify\nthem as either spam\nor non-spam now,\nthis is another famous\napplication of machine learning.\nSo to sum this up,\nlet's look at a few reasons.\nWhy machine learning\nis so important.\nSo the first reason\nis obviously increase\nin data generation.\nSo because of excessive\nproduction of data,\nwe need a method\nthat can be used to structure\nand lies and draw\nuseful insights from data.\nThis is where machine learning\ncomes as in it uses data\nto solve problems\nand find solutions\nto the most complex tasks\nfaced by organizations.\nAnother important reason is that\nit improves decision-making.\nSo by making use of various\nalgorithms machine learning\ncan be used to make\nBetter Business decisions.\nFor example machine learning\nis used to forecast sales.\nIt is used to predict any\ndownfalls in the stock market.\nIt is used to identify\nrisks anomalies and so\non now the next reason\nIs it uncovers patterns\nand Trends in data finding\nhidden patterns and extracting\nkey insights from data is\nthe most essential part\nof machine learning.\nSo by building predictive models\nand using statistical\ntechniques machine learning\nallows you to dig\nbeneath the surface\nand explore the data\nat a minut scale\nnow understanding data\nand extracting patterns manually\nwill take a lot of days.\nNow, if you do this through\nmachine learning algorithms,\nyou can perform\nsuch computations.\nNations in less than a second.\nAnother reason is\nthat it's solved\ncomplex problems.\nSo from detecting genes\nthat are linked\nto deadly ALS disease\nis to building self-driving cars\nand building phase detection\nsystems machine learning\ncan be used to solve\nthe most complex problems.\nSo guys now that you know,\nwhy machine learning\nis so important.\nLet's look at what exactly\nmachine learning is.\nThe term machine learning\nwas first coined by\nArthur Samuel in the year\n1959 now looking back\nthat your was probably\nthe most significant in terms\nof technological advancements.\nThere is if you browse\nthrough the net about\nwhat is machine learning\nyou'll get at least\na hundred different definitions.\nNow the first and very formal\ndefinition was given by Tom\nand Mitchell now,\nthe definition says\nthat a computer program is set\nto learn from experience e\nwith respect to some class.\nOf caste and\nperformance measure P\nif its performance at tasks in D\nas measured by P improves\nwith experience e all right.\nNow I know this is\na little confusing.\nSo let's break it down\ninto simple words.\nNow in simple terms\nmachine learning is a subset\nof artificial intelligence\nwhich provides machines the\nability to learn automatically\nand improve from experience\nwithout being explicitly\nprogrammed to do\nso in the sense.\nIt is the practice of getting\nmachines to solve problems\nby gaining the ability\nto think but wait now\nhow can a machine think\nor make decisions?\nWell, if you feel a machine\na good amount of data,\nit will learn\nhow to interpret process\nand analyze this data by using\nmachine learning algorithm.\nOkay.\nNow guys, look\nat this figure on top.\nNow this figure basically shows\nhow a machine learning algorithm\nor how the machine learning\nprocess really works.\nSo the machine learning Begins\nby feeding the machine lots\nand lots of data okay\nby using this data.\nThe machine is trained to detect\nhidden insights and Trends.\nNow these insights\nare then used to build\na machine learning model\nby using an algorithm\nin order to solve a problem.\nOkay.\nSo basically you're\ngoing to feed a lot\nof data to the machine.\nThe machine is going to get\ntrained by using this data.\nIt's going to use this data\nand it's going to\ndraw useful insights\nand patterns from it,\nand then it's going\nto build a model by Using\nmachine learning algorithms.\nNow this model will help\nyou predict the outcome\nor help you solve\nany complex problem\nor any business problem.\nSo that's a simple explanation\nof how machine learning works.\nNow, let's move on and look\nat some of the most commonly\nused machine learning terms.\nSo first of all,\nwe have algorithm.\nNow, this is\nquite self-explanatory.\nBasically algorithm\nis a set of rules\nor statistical techniques,\nwhich are used to learn\npatterns from data now\nan algorithm is The logic\nbehind a machine learning model.\nAll right, an example\nof a machine learning\nalgorithm is linear regression.\nI'm not sure how many of you\nhave heard of linear regression.\nIt's the most simple and basic\nmachine learning algorithm.\nAll right.\nNext we have model now\nmodel is the main component\nof machine learning.\nAll right.\nSo model will basically map\nthe input to your output\nby using the machine learning\nalgorithm and by using the data\nthat you're feeding the machine.\nSo basically the model is\na representation of the entire\nmachine learning process.\nSo the model is\nbasically fed input\nwhich has a lot of data\nand then it will output\na particular result\nor a particular outcome by using\nmachine learning algorithms.\nNext we have something\nknown as predictor variable.\nNow predictor variable\nis a feature of the data\nthat can be used\nto predict the output.\nSo for example, let's say\nthat you're trying to predict\nthe weight of a person depending\non the person's height\nand their age.\nAll right.\nSo over here the predictor\nvariables are your height\nand your age\nbecause you're using\nheight and age of a person\nto predict the person's weight.\nAlright, so the height\nand the A's are\nthe predictor variables now,\nWait on the other hand\nis the response\nor the target variable.\nSo response variable is\na feature or the output variable\nthat needs to be predicted by\nusing the predictor variables.\nAll right,\nafter that we have something\nknown as training data.\nSo guys the data\nthat is fed to a machine\nlearning model is always split\ninto two parts first.\nWe have the training data\nand then we have\nthe testing data now training\ndata is basically used to build\nthe machine learning model.\nSo usually training data\nis much larger.\nThan the testing data\nbecause obviously\nif you're trying to train\nthe machine then you're going\nto feed it a lot more data.\nTesting data is just used\nto validate and evaluate\nthe efficiency of the model.\nAlright, so that was training\ndata and testing data.\nSo Guys, these were a few terms\nthat I thought you should know\nbefore we move any further.\nOkay.\nNow, let's move on and discuss\nthe machine learning process.\nNow, this is going\nto get very interesting\nbecause I'm going\nto give you an example\nand make you understand\nhow the machine learning.\nprocess works So first of all,\nlet's define\nthe different stages\nor the different steps involved\nin the machine learning process.\nSo machine learning\nprocess always begins\nwith defining the objective\nor defining the problem\nthat you're trying to solve\nnext is is data Gathering\nor data collection.\nNow the data that you\nneed to solve this problem\nis collected at this stage.\nThis is followed\nby data preparation\nor data processing after that.\nYou have data\nexploration and Analysis.\nIsis and the next\nstage is building\na machine learning model.\nThis is followed\nby model evaluation.\nAnd finally you have\nprediction or your output.\nNow, let's try to understand\nthis entire process\nwith an example.\nSo our problem statement here\nis to predict the possibility\nof rain by studying\nthe weather conditions.\nSo let's say\nthat you're given\na problem statement\nand you're asked to use\na machine learning process\nto solve this problem statement.\nSo let's get started.\nAlright, so the first step\nis to Find the objective\nof the problem statement.\nOur objective here is\nto predict the possibility\nof rain by studying\nthe weather conditions.\nNow in the first stage\nof a machine learning process.\nYou must understand\nwhat exactly needs\nto be predicted.\nNow in our case the objective\nis to predict the possibility\nof rain by studying\nweather conditions, right?\nSo at this stage,\nit is also essential to take\nmental notes on what kind\nof data can be used\nto solve this problem\nor the type of approach\nthat you can follow to get.\nGet to the solution.\nAll right, a few questions\nthat are worth asking\nduring this stage is\nwhat are we trying to predict?\nWhat are the Target features\nor what are\nthe predictor variables?\nWhat kind of input\ndata do we need?\nAnd what kind\nof problem are we facing?\nIs it a binary classification\nproblem or is it\na clustering problem\nnow, don't worry.\nIf you don't know\nwhat classification\nand clustering is\nI'll be explaining this\nin the upcoming slides.\nSo guys this was the first step\nof a machine learning process,\nwhich is Define\nthe Double the problem.\nAll right.\nNow, let's move on and look\nat step number two.\nSo step number two is\nbasically data collection\nor data Gathering\nnow at this stage.\nYou must be asking questions\nsuch as what kind of data\nis needed to solve the problem\nis the data available and\nif it is available,\nhow can I get the data?\nOkay.\nSo once you know the type\nof data that is required,\nyou must understand\nhow you can derive\nthis data data collection\ncan be done manually\nor by web scraping,\nbut if you're a beginner Nor\nand you're just looking to learn\nmachine learning you don't have\nto worry about getting the data.\nOK there are thousands\nof data resources on the web.\nYou can just go ahead\nand download the datasets\nfrom websites such as kaggle.\nOkay, now coming\nback to the problem\nat hand the data needed\nfor weather forecasting includes\nmeasures such as humidity level\ntemperature pressure locality\nwhether or not you live\nin a hill station\nand so on so guys\nsuch data must be collected\nand stored for analysis.\nNow the next stage\nin machine learning\nis preparing your data\nthe data you collected is almost\nnever in the right format.\nSo basically you'll encounter\na lot of inconsistencies\nin the data set.\nOkay, this includes\nmissing values redundant\nvariables duplicate values\nand so on removing\nsuch values is very important\nbecause they might lead\nto wrongful computations\nand predictions.\nSo that's why at this stage you\nmust can the entire data set\nfor any inconsistencies.\nYou have to fix them\nat this stage.\nNow.\nThe next step is\nexploratory data analysis.\nNow data analysis is\nall about diving deep\ninto data and finding all\nthe hidden data Mysteries.\nOkay.\nThis is where you\nbecome a detective.\nSo edu or exploratory data\nanalysis is like a brainstorming\nof machine learning\ndata exploration involves\nunderstanding the patterns\nand the trends in your data.\nSo at this stage all\nthe useful insights are drawn\nand all the correlations.\nTurns between the\nvariables are understood.\nSo you might ask what sort\nof correlations are\nyou talking about?\nFor example in the case\nof predicting rain fall.\nWe know that there is\na strong possibility of rain\nif the temperature\nhas fallen low.\nOkay.\nSo such correlations\nhave to be understood\nand mapped at this stage.\nNow.\nThis stage is followed\nby stage number 5,\nwhich is building\na machine learning model.\nSo all the insights\nand the patterns\nthat you derive\nduring data exploration are used\nto build the machine learning.\nSo this stage always Begins\nby splitting the data set\ninto two parts training data\nand the testing data.\nSo earlier in the session.\nI already told you what training\nand testing data is\nnow the training data\nwill be used to build\nand analyze the model\nand the logic of the model\nwill be based on the machine\nlearning algorithm\nthat is being implemented.\nOkay.\nNow in the case\nof predicting rainfall\nsince the output will be\nin the form of true\nor false we can use\na classification algorithm\nlike logistically.\nRegression now choosing\nthe right algorithm depends\non the type of problem.\nYou're trying to solve\nthe data set you have\nand the level of complexity\nof the problem.\nSo in the upcoming sections will\nbe discussing different types\nof problems that can be solved\nby using machine learning.\nSo don't worry.\nIf you don't know\nwhat classification algorithm is\nand what logistic regression in.\nOkay.\nSo all you need to know\nis at this stage,\nyou'll be building\na machine learning model\nby using machine\nlearning algorithm\nand by using the training\ndata set the next\nBut in on machine learning\nprocess is model evaluation\nand optimization.\nSo after building a model\nby using the training data set\nit is finally time to put\nthe model to a test.\nOkay.\nSo the testing data set\nis used to check the efficiency\nof the model and how accurately\nit can predict the outcome.\nSo once you calculate\nthe accuracy any improvements\nin the model have\nto be implemented in this stage.\nOkay, so methods like parameter\ntuning and cross-validation\ncan be used to improve\nthe The performance\nof the model this is followed\nby the last stage,\nwhich is predictions.\nSo once the model is evaluated\nand improved it is finally\nused to make predictions.\nThe final output can be\na categorical variable\nor it can be a continuous\nquantity in our case\nfor predicting the occurrence\nof rainfall the output\nwill be a categorical variable\nin the sense.\nOur output will be\nin the form of true or false.\nYes or no.\nYes, basically represents\nthat is going to rain\nand no will represent that.\nIt wondering okay\nas simple as that,\nso guys that was the entire\nmachine learning process.\nA linear regression is one\nof the easiest algorithm\nin machine learning.\nIt is a statistical model\nthat attempts to\nshow the relationship\nbetween two variables.\nSo the linear equation,\nbut before we drill down\nto linear regression\nalgorithm in depth,\nI'll give you a quick overview\nof today's agenda.\nSo we'll start a session\nwith a quick overview\nof what is regression\nas linear regression\nis one of a type\nof regression algorithm.\nOnce we learn about regression,\nits use case the various\ntypes of it next.\nWe'll learn about the algorithm\nfrom scratch where I live\nTo its mathematical\nimplementation first,\nthen we'll drill down\nto the coding part\nand Implement linear\nregression using python\nin today's session will deal\nwith linear regression\nalgorithm using least Square\nmethod checketts goodness of fit\nor how close the data is\nto the fitted regression line\nusing the R square method\nand then finally\nwhat we'll do well\noptimized it using\nthe gradient descent method\nin the last part\non the coding session.\nI'll teach you to implement\nlinear regression using Python\nand the coding session.\nWould be divided into two parts\nthe first part would consist\nof linear regression\nusing python from scratch\nwhere you will use\nthe mathematical algorithm\nthat you have learned\nin this session.\nAnd in the next part\nof the coding session\nwill be using scikit-learn\nfor direct implementation\nof linear regression.\nAll right.\nI hope the agenda is clear\nto you guys are like\nso let's begin our session\nwith what is regression.\nWell regression analysis\nis a form of predictive\nmodeling technique\nwhich investigates\nthe relationship between\na dependent and independent.\nAble a regression analysis\ninvolves graphing a line\nover a set of data points\nthat most closely fits\nthe overall shape of the data\nor regression shows the changes\nin a dependent variable\non the y-axis\nto the changes\nin the explanatory variable\non the x-axis fine.\nNow you would ask\nwhat are the uses of regression?\nWell, they are major three uses\nof regression analysis\nthe first being determining\nthe strength of predicator,\n's the regression\nmight be used to identify\nthe strength of the effect\nthat the independent.\nVariables have on\nthe dependent variable.\nFor example, you\ncan ask question.\nLike what is the strength\nof relationship between sales\nand marketing spending or what\nis the relationship between age\nand income second is forecasting\nan effect in this the regression\ncan be used to forecast effects\nor impact of changes.\nThat is the regression analysis\nhelp us to understand\nhow much the dependent variable\nchanges with the change\nin one or more\nindependent variable fine.\nFor example, you can ask\nquestion like how Additional\nseal income will I get\nfor each thousand dollars spent\non marketing third\nis Trend forecasting\nin this the regression analysis\nto predict Trends\nand future values.\nThe regression analysis\ncan be used to get\nPoint estimates in this\nyou can ask questions.\nLike what will be\nthe price of Bitcoin\nand next six months, right?\nSo next topic is linear versus\nlogistic regression by now.\nI hope that you know,\nwhat a regression is.\nSo let's move on\nand understand its type.\nSo there are various kinds\nof regression like linear.\nSession logistic regression\npolynomial regression\nand others.\nAll right, but for this session\nwill be focusing on linear\nand logistic regression.\nSo let's move on and let me tell\nyou what is linear regression.\nAnd what is logistic regression\nthen what we'll do\nwe'll compare both of them.\nAll right.\nSo starting with\nlinear regression\nin simple linear regression.\nWe are interested in things\nlike y equal MX plus C.\nSo what we are trying to find\nis the correlation between X\nand Y variable this means\nthat every value of X has\na corresponding value of y in it\nif it is continuous.\nI like however\nin logistic regression we\nare not fitting our data\nto a straight line\nlike linear regression instead\nwhat we are doing.\nWe are mapping Y versus X\nto a sigmoid function\nin logistic regression.\nWhat we find out is is y 1 or 0\nfor this particular value of x\nso thus we are essentially\ndeciding true or false value\nfor a given value of x fine.\nSo as a core concept\nof linear regression You can say\nthat the data is modeled\nusing a straight line\nwhere in the case\nof logistic regression\nthe data is model using\na sigmoid function.\nThe linear regression is used\nwith continuous variables\non the other hand\nthe logistic regression.\nIt is used with categorical\nvariable the output\nor the prediction\nof a linear regression\nis the value of the variable\non the other hand\nthe output of production\nof a logistic regression\nis the probability\nof occurrence of the event.\nNow, how will you\ncheck the accuracy\nand goodness of fit in case\nof linear regression?\nWe are various methods.\nTake measured by loss r squared\nadjusted r squared Etc\nwhile in the case\nof logistic regression you\nhave accuracy precision\nrecall F1 score,\nwhich is nothing but\nthe harmonic mean of precision\nand recall next is Roc curve\nfor determining the probability\nthreshold for classification\nor the confusion Matrix Etc.\nThere are many all right.\nSo summarizing the difference\nbetween linear and\nlogistic regression.\nYou can say that the type\nof function you are mapping\nto is the main point\nof difference between linear\nand regression a linear\nregression Maps a continuous X2\na continuous fi\non the other hand a logistic\nregression Maps a continuous x\nto the bindery why\nso we can use logistic\nregression to make category\nor true false decisions\nfrom the data find\nso let's move on ahead.\nNext is linear\nregression selection criteria,\nor you can say when will\nyou use linear regression?\nSo the first is classification\nand regression capabilities\nregression models predict\na continuous variable\nsuch as the Don't a day\nor predict the temperature\nof a city their Reliance\non a polynomial\nlike a straight line\nto fit a data set\nposes a real challenge\nwhen it comes towards building\na classification capability.\nLet's imagine that you fit\na line with the training points\nthat you have now imagine you\nadd some more data points to it.\nBut in order to fit it,\nwhat do you have to do?\nYou have to change\nyour existing model\nthat is maybe you have\nto change the threshold itself.\nSo this will happen\nwith each new data point you add\nto the model, hence.\nThe linear regression is\nnot good for classification.\nAll's fine.\nNext is data quality\neach missing value removes\none data point that could\noptimize the regression\nin simple linear regression.\nThe outliers can significantly\ndisrupt the outcome\njust for now.\nYou can know that if you\nremove the outliers your model\nwill become very good.\nAll right.\nSo this is about data quality.\nNext is computational complexity\na linear regression is often\nnot computationally expensive as\ncompared to the decision tree\nor the clustering\nalgorithm the order\nof complexity for n\ntraining example and X features.\nUsually Falls in either Big O\nof x square or big\nof xn next is comprehensible\nand transparent the\nlinear regression are\neasily comprehensible\nand transparent in nature.\nThey can be represented by\na simple mathematical notation\nto anyone and can be\nunderstood very easily.\nSo these are some\nof the criteria based\non which you will select\nthe linear regression algorithm.\nAll right.\nNext is where is linear\nregression used first\nis evaluating Trends\nand sales estimate.\nWell linear regression\ncan be used in Business\nto evaluate Trends\nand make estimates\nor focused for example,\nif a company sales have\nincreased steadily every month\nfor past few years then\nconducting a linear analysis\non the sales data\nwith monthly sales on the y axis\nand time on the x axis.\nThis will give you a line\nthat predicts the upward Trends\nin the sale after creating\nthe trendline the company\ncould use the slope\nof the lines too focused sale\nin future months.\nNext is analyzing.\nThe impact of price changes\nwill linear regression\ncan be To analyze the effect\nof pricing on consumer behavior.\nFor instance.\nIf a company changes\nthe price on a certain\nproduct several times,\nthen it can record the quantity\nitself for each price level\nand then perform\na linear regression\nwith sold quantity as\na dependent variable and price\nas the independent variable.\nThis would result in a line\nthat depicts the extent\nto which the customer reduce\ntheir consumption of the product\nas the prices increasing.\nSo this result would help us\nin future pricing decisions.\nNext is assessment\nof risk and fine.\nFinancial services\nand insurance domain.\nWell linear regression\ncan be used to analyze the risk,\nfor example health insurance\ncompany might conduct\na linear regression algorithm\nhow it can do it can do it\nby plotting the number of claims\nper customer against its age\nand they might discover\nthat the old customers\nthen to make more\nhealth insurance claim.\nWell the result\nof such analysis might guide\nimportant business decisions.\nAll right, so by now you\nhave just a rough idea of\nwhat linear regression\nalgorithm as like,\nWhat it does where it is used\nwhen you should use\nit early now,\nlet's move on and understand\nthe algorithm and depth.\nSo suppose you have independent\nvariable on the x-axis\nand dependent variable\non the y-axis.\nAll right suppose.\nThis is the data point\non the x axis.\nThe independent variable\nis increasing on the x axis.\nAnd so does the dependent\nvariable on the y-axis?\nSo what kind of linear\nregression line you would get\nyou would get a positive\nlinear regression line.\nAll right as the slope\nwould be positive.\nNext is suppose.\nYou have an independent\nvariable on the x-axis\nwhich is increasing\nand on the other hand the\ndependent variable on the y-axis\nthat is decreasing.\nSo what kind of line\nwill you get in that case?\nYou will get\na negative regression line.\nIn this case as the slope\nof the line is negative.\nAnd this particular line\nthat is line of y equal MX\nplus C is a line\nof linear regression\nwhich shows the relationship\nbetween independent variable\nand dependent variable\nand this line is only known\nas line of linear regression.\nOkay?\nSo let's add some data\npoints to our graph.\nSo these are some observation\nor data points on our graphs.\nLet's plot some more.\nOkay.\nNow all our data points\nare plotted now our task is\nto create a regression line\nor the best fit line.\nAll right now\nonce our regression\nline is drawn now,\nit's the task\nof production now suppose.\nThis is our estimated value\nor the predicted value\nand this is our actual value.\nOkay.\nSo what we have to do our main\ngoal is to reduce this error.\nThat is to reduce the distance\nbetween the estimated\nor the predicted value\nand the actual value.\nThe best fit line would be the\none which had the least error\nor the least difference\nin estimated value\nand the actual value.\nAll right, and other words we\nhave to minimize the error.\nThis was a brief\nunderstanding of linear\nregression algorithm soon.\nWe'll jump towards\nmathematical implementation.\nAll right, but for then\nlet me tell you this\nsuppose you draw a graph\nwith speed on the x-axis\nand distance covered.\nOn the y axis with the time\ndemeaning constant,\nif you plot a graph\nbetween the speed travel\nby the vehicle\nand the distance traveled\nin a fixed unit of time,\nthen you will get\na positive relationship.\nAll right.\nSo suppose the equation\nof line as y equal MX plus C.\nThen in this case Y is\nthe distance traveled\nin a fixed duration of time x\nis the speed of vehicle m\nis the positive slope\nof the line and see is\nthe y-intercept of the line.\nAll right suppose\nthe distance remaining constant.\nYou have to plot a graph\nbetween the Rid of the vehicle\nand the time taken to travel\na fixed distance then\nin that case you will get a line\nwith a negative relationship.\nAll right, the slope of the line\nis negative here the equation\nof line changes to y\nequal minus of MX plus C\nwhere Y is the time\ntaken to travel\na fixed distance X is the speed\nof vehicle m is\nthe negative slope\nof the line and see is\nthe y-intercept of the line.\nAll right.\nNow, let's get back\nto our independent\nand dependent variable.\nSo in that term why is\nour dependent variable and That\nis our independent variable.\nNow, let's move on and see\nthe mathematical implementation\nof the things.\nAlright, so we have x\nequal 1 2 3 4 5 let's plot\nthem on the x-axis.\nSo 0 1 2 3 4 5 6 alike\nand we have y as 3 4 2 4 5.\nAll right.\nSo let's plot 1\n2 3 4 5 on the y-axis now,\nlet's plot our coordinates 1\nby 1 so x equal 1 and y equal 3,\nso We have here x\nequal 1 and y equal 3.\nSo this is the point\n1 comma 3 so similarly\nwe have 1 3 2 4 3 2 4 4 & 5 5.\nAll right.\nSo moving on ahead.\nLet's calculate the mean of X\nand Y and plot it on the graph.\nAll right, so mean of X is 1\nplus 2 plus 3 plus 4\nplus 5 divided by 5.\nThat is 3.\nAll right, similarly mean\nof Y is 3 plus 4 plus 2\nplus 4 plus 5 that is 18.\nSo it in divided by 5.\nThat is nothing\nbut 3.6 aligned so next\nwhat we'll do we'll plot\nour mean that is 3 comma 3 .6\non the graph.\nOkay.\nSo there's a point 3 comma 3 .6\nsee our goal is to find\nor predict the best fit line\nusing the least Square\nMethod All right.\nSo in order to find\nthat we first need to find\nthe equation of line,\nso let's find the equation\nof our regression line.\nAll right.\nSo let's suppose this is\nour regression line\ny equal MX plus C.\nNow.\nWe have an equation of line.\nSo all we need to do is\nfind the value of M and see\nwhere m equals summation of x\nminus X bar X Y minus y bar\nupon the summation of x\nminus X bar whole Square\ndon't get confused.\nLet me resolve it for you.\nAll right.\nSo moving on ahead\nas a part of formula.\nWhat we are going to do\nwill calculate x minus X bar.\nSo we have X as 1 minus X bar\nas 3 so 1 minus 3\nthat is minus 2 next.\nWe have x equal\nto minus its mean 3\nthat is minus 1 similarly.\nWe have 3 minus 3 is 0 4 -\n3 1 5 - 3 2 alight\nso x minus X bar.\nIt's nothing but the distance\nof all the point\nthrough the line y equal 3\nand what does this y\nminus y bar implies it implies\nthat distance of all the point\nfrom the line x equal 3 .6 fine.\nSo let's calculate the value\nof y minus y bar.\nSo starting with y equal 3 -\nvalue of y. A bar\nthat is 3.6.\nSo it is three minus 3.6\nhow much -\nof 0.6 next is 4 minus 3.6\nthat is 0.4 next to minus 3.6\nthat is minus of 1 point\n6 next is 4 minus 3.6\nthat is 0.4 again,\n5 minus 3.6 that is 1.4.\nAlright, so now we are done\nwith Y minus y bar fine now next\nwe will calculate x\nminus X bar whole Square\nLet's calculate x\nminus X bar whole Square.\nSo it is minus 2 whole square.\nThat is 4 minus 1 whole square.\nThat is 1 0 squared is\n0 1 Square 1 2 square for fine.\nSo now in our table we have x\nminus X bar y minus y bar\nand x minus X bar whole Square.\nNow what we need.\nWe need the product of x\nminus X bar X Y minus y bar.\nAlright, so let's see\nthe product of x\nminus X bar X Y minus\ny bar that is minus\nof 2 x minus of 0.6.\nThat is one.\nPoint 2 minus of 1 x\n0 point 4 that is\nminus of 0 point 4 0 x\nminus of 1.6.\nThat is 0 1 multiplied\nby zero point four\nthat is 0.4.\nAnd next 2 multiplied\nby 1 point for that is 2.8.\nAll right.\nNow almost all the parts\nof our formula is done.\nSo now what we need\nto do is get the summation\nof last two columns.\nAll right, so the summation of x\nminus X bar whole square is 10\nand the summation\nof x minus X bar.\nX Y minus y bar is 4\nso the value of M will be equal\nto 4 by 10 fine.\nSo let's put this value\nof m equals zero point 4\nand our line y equal MX plus C.\nSo let's file all the points\ninto the equation\nand find the value of C.\nSo we have y as 3.6 remember\nthe mean by m as 0.4\nwhich we calculated just\nnow X as the mean value of x\nthat is 3 and we have the\nin as 3 point 6 equals\n0 point 4 x 3 plus C. Alright\nthat is 3.6 equal\n1 Point 2 plus C.\nSo what is the value of C\nthat is 3.6 minus 1 Point 2.\nThat is 2 point 4.\nAll right.\nSo what we had we had m\nequals zero point four see\nas 2.4 and then finally\nwhen we calculate the equation\nof the regression line\nwhat we get is y equal\nzero point four times of X\nplus two point four.\nSo there is the regression line.\nLike so there's\nhow you're plotting your points.\nThis is your actual point.\nAll right.\nNow for given m equals\nzero point four and SQL 2.4.\nLet's predict the value of y\nfor x equal 1 2 3 4 & 5.\nSo when x equal\n1 the predicted value\nof y will be zero point four x\none plus two point\nfour that is 2.8.\nSimilarly when x equal\nto predicted value\nof y will be zero point 4 x\n2 plus 2 point 4\nthat equals to 3 point.\nTwo similarly x\nequal 3 y will be 3 point 6 x\nequal 4 y will be 4 point 0\nx equal 5 y will be\nfour point four.\nSo let's plot them on the graph\nand the line passing through\nall these predicting point\nand cutting y-axis at 2.4\nas the line of regression.\nNow your task is to calculate\nthe distance between the actual\nand the predicted value\nand your job is\nto reduce the distance.\nAll right, or in other words,\nyou have to reduce the error\nbetween the actual\nand the predicted.\nThe line with the least\nerror will be the line\nof linear regression\nor regression line and it\nwill also be the best fit line.\nAlright, so this is\nhow things work in computer.\nSo what it do it performs\na number of iteration\nfor different values of M\nfor different values of M.\nIt will calculate\nthe equation of line\nwhere y equals MX plus C.\nRight?\nSo as the value\nof M changes the line\nis changing so iteration\nwill start from one.\nAll right, and it will perform\na number of iteration so\nafter Every iteration\nwhat it will do it will\ncalculate the predicted value\naccording to the line\nand compare the distance\nof actual value\nto the predicted value\nand the value of M\nfor which the distance\nbetween the actual\nand the predicted value is\nminimum will be selected\nas the best fit line.\nAll right.\nNow that we have calculated\nthe best fit line now,\nit's time to check the goodness\nof fit or to check\nhow good a model is performing.\nSo in order to do that,\nwe have a method\ncalled R square method.\nSo what is this R square?\nWell r-squared value is\na statistical measure of\nhow close the data are\nto the fitted regression\nline in general.\nIt is considered\nthat a high r-squared\nvalue model is a good model,\nbut you can also have\na lower squared value\nfor a good model as well or\na higher Squad value for a model\nthat does not fit at all.\nAll right.\nIt is also known as\ncoefficient of determination\nor the coefficient\nof multiple determination.\nLet's move on and see\nhow a square is calculated.\nSo these are our actual values\nplotted on the graph.\nWe had calculated\nthe predicted values\nof Y as 2.8 3.2 3.6 4.0 4.4.\nRemember when we calculated\nthe predicted values\nof Y for the equation Y\npredicted equals 0 1 4 x\nof X plus two point\nfour for every x\nequal 1 2 3 4 & 5 from there.\nWe got the power.\nGood values of Phi.\nAll right.\nSo let's plot it on the graph.\nSo these are point\nand the line passing\nthrough these points are nothing\nbut the regression line.\nAll right.\nNow, what you need to do is\nyou have to check and compare\nthe distance of actual -\nmean versus the distance\nof predicted - mean.\nAlright.\nSo basically what you are doing\nyou are calculating the distance\nof actual value\nto the mean to distance\nof predicted value to the mean.\nAll right, so there is nothing\nbut a square in mathematically\nyou can represent our school.\nWhereas summation of Y\npredicted values minus y\nbar whole Square divided\nby summation of Y minus\ny bar whole Square\nwhere Y is the actual value\ny p is the predicted value\nand Y Bar is the mean value of y\nthat is nothing but 3.6.\nRemember, this is our formula.\nSo next what we'll do\nwe'll calculate y minus y bar.\nSo we have y is 3y bar as\n3 point 6 so we'll calculate\nit as 3 minus 3.6\nthat is nothing but\nminus of 0.6 similarly\nfor y equals 4\nand Y Bar equal 3.6.\nWe have y minus y bar as\nzero point 4 then 2 minus 3.6.\nIt has 1 point\n6 4 minus 3.6 again\nzero point four and five\nminus 3.6 it is 1.4.\nSo we got the value\nof y minus y bar.\nNow what we have to do we\nhave to take it Square.\nSo we have minus of 0.6 Square\nas 0.36 0.4 Square as 0.16 -\nof 1.6 Square as 2.56 0.4 Square\nas 0.16 and 1.4 squared\nis 1.96 now is a part\nof formula what we need.\nWe need our YP\nminus y BAR value.\nSo these are VIP values\nand we have to subtract it\nfrom the No, right.\nSo 2 .8 minus 3.6\nthat is minus 0.8.\nSimilarly.\nWe will get 3.2 minus 3.6\nthat is 0.4 and 3.6 minus 3.6\nthat is 0 for 1 0 minus\n3.6 that is 0.4.\nThen 4 .4 minus 3.6 that is 0.8.\nSo we calculated the value\nof YP minus y bar now,\nit's our turn to calculate\nthe value of y b minus\ny bar whole Square next.\nWe have -\nof 0.8 Square as 0.64 - of Point\nfour square as 0.160 Square\n0 0 point 4 Square as again 0.16\nand 0.8 Square as 0.64.\nAll right.\nNow as a part of formula\nwhat it suggests it suggests\nme to take the summation of Y P\nminus y bar whole square\nand summation of Y minus\ny bar whole Square.\nAll right.\nLet's see.\nSo on submitting y\nminus y bar whole Square\nwhat you get is five point two\nand summation of Y P minus\ny bar whole Square you\nget one point six.\nSo the value of R square\ncan be calculated as\n1 point 6 upon 5.2 fine.\nSo the result which will get is\napproximately equal to 0.3.\nWell, this is not a good fit.\nAll right, so it suggests\nthat the data points are far\naway from the regression line.\nAlright, so this is\nhow your graph will look\nlike when R square is 0.3\nwhen you increase the value\nof R square to 0.7.\nSo you'll see\nthat the actual value would like\ncloser to the regression line\nwhen it reaches to 0.9 it comes.\nMore clothes and when the value\nof approximately equals\nto 1 then the actual values lies\non the regression line itself,\nfor example, in this case.\nIf you get a very low value\nof R square suppose 0.02.\nSo in that case what you'll see\nthat the actual values are\nvery far away from\nthe regression line,\nor you can say\nthat there are too\nmany outliers in your data.\nYou cannot focus\nanything from the data.\nAll right.\nSo this was all about\nthe calculation of R square now,\nyou might get a question\nlike are low values\nof Square always bad.\nWell in some field it\nis entirely expected that I ask\nwhere value will be low.\nFor example any field\nthat attempts to predict human\nbehavior such as psychology\ntypically has r-squared values\nlower than around 50%\nthrough which you can conclude\nthat humans are simply harder\nto predict the under\nphysical process furthermore.\nIf you are squared value is low,\nbut you have statistically\nsignificant predictors,\nthen you can still\ndraw important conclusion\nabout how changes in the\npredicator values associated.\nOh sated with the changes\nin the response value regardless\nof the r-squared\nthe significant coefficient\nstill represent the mean change\nin the response for one unit\nof change in the predicator\nwhile holding other predators\nin the model constant,\nobviously this type\nof information can be\nextremely valuable.\nAll right.\nAll right.\nSo this was all about\nthe theoretical concept now,\nlet's move on to the coding\npart and understand\nthe code in depth.\nSo for implementing\nlinear regression using python,\nI will be using Anaconda\nwith jupyter notebook\ninstalled on it.\nSo I like there's\na jupyter notebook\nand we are using python 3.01 it\nalright, so we are going\nto use a data set consisting\nof head size and human brain\nof different people.\nAll right.\nSo let's import our data set\npercent matplotlib and line.\nWe are importing numpy\nas NP pandas as speedy and\nmatplotlib and from matplotlib.\nWe are importing pipe\nout of that as PLT.\nAlright next we will import\nour data had brain dot CSV\nand store it\nin the data variable.\nLet's execute the Run button\nand see the armor.\nBut so this asterisk\nsymbol it symbolizes\nthat it still executing.\nSo there's a output\nor dataset consists\nof two thirty seven rows\nand four columns.\nWe have columns as\ngender age range head size\nin centimeter Cube\nand brain weights\nand Graham fine.\nSo there's our sample data set\nthat is how it looks it consists\nof all these data set.\nSo now that we\nhave imported our data,\nso as you can see they are\n237 values in the training set\nso we can find a linear.\nRelationship between the head\nsize and the Brain weights.\nSo now what we'll do\nwe'll collect X & Y\nthe X would consist\nof the head size values\nand the Y would consist\nof brain with values.\nSo collecting X and Y.\nLet's execute the Run.\nDone next what we'll do we\nneed to find the values of b 1\nor B not or you can say m and C.\nSo we'll need the mean of X\nand Y values first of all\nwhat we'll do we'll calculate\nthe mean of X and Y so mean x\nequal NP dot Min X.\nSo mean is a predefined function\nof Numb by similarly mean\nunderscore y equal\nNP dot mean of Y,\nso what it will return\nif you'll return\nthe mean values of Y\nnext we'll check\nthe total number of values.\nSo m equals.\nWell length of X. Alright,\nthen we'll use the formula\nto calculate the values of b 1\nand B naught or fnc.\nAll right, let's execute\nthe Run button and see\nwhat is the result.\nSo as you can see here\non the screen we have got\nb 1 as 0 point 2 6 3 +\nB not as three twenty\nfive point five seven.\nAlright, so now\nthat we have a coefficient.\nSo comparing it with\nthe equation y equal MX plus C.\nYou can say\nthat brain weight equals\nzero point 2 6 3 X Head size\nplus three twenty five point\nfive seven so you can say\nthat the value of M here\nis 0.26 3 and the value\nof C. Here is three twenty\nfive point five seven.\nAll right, so there's\nour linear model now,\nlet's plot it\nand see graphically.\nLet's execute it.\nSo this is how our plot looks\nlike this model is not so bad.\nBut we need to find out\nhow good our model is.\nSo in order to find\nit the many methods\nlike root means Square method\nthe coefficient of determination\nor the a square method.\nSo in this tutorial,\nI have told you\nabout our score method.\nSo let's focus on that and see\nhow good our model is.\nSo let's calculate\nthe R square value.\nAll right here SS underscore T\nis the total sum of square SS.\nOur is the total sum of square\nof residuals and R square\nas the formula is\n1 minus total sum\nof squares upon total sum\nof square of residuals.\nAll right next\nwhen you execute it,\nyou will get the value\nof R square as 0.63\nwhich is pretty very good.\nNow that you have implemented\nsimple linear regression model\nusing least Square method,\nlet's move on and see\nhow will you implement the model\nusing machine learning library\ncalled scikit-learn.\nAll right.\nSo this scikit-learn\nis a simple machine.\nYoung Library in Python welding\nmachine learning model are\nvery easy using scikit-learn.\nSo suppose there's\na python code.\nSo using the scikit-learn\nlibraries your code shortens\nto this length\nlike so let's execute\nthe Run button and see you\nwill get the same our\nto score as Well,\nthis was all\nfor today's discussion.\nMost of the entities\nin this world are\nrelated in one way\nor another at times finding\nrelationship between entities\ncan help you take valuable\nbusiness decisions today.\nI'm going to talk\nabout logistic regression,\nwhich is one\nsuch approach towards\npredicting relationships.\nNow, let us see\nwhat all we are going to cover\nin today's training.\nSo we'll start off the session\nby getting a quick introduction\nto what is regression.\nThen we'll see the different\ntypes of regression\nand we'll be discussing the what\nand by of logistic regression.\nSo in this part,\nwe'll discuss what\nexactly it is.\nIt is used why it is used\nand all those things moving\nahead will compare\nlinear regression\nversus logistic regression\nalong with the various\nreal-life use cases\nand finally towards the end.\nI will be practically\nimplementing logistic\nregression algorithm.\nSo let's quickly start off\nwith the very first topic\nwhat is regression.\nThe regression analysis is\na predictive modeling technique.\nSo it always\ninvolves predictions.\nSo in this session,\nwe'll just talk\nabout predictive analysis\nand not prescriptive analysis.\nNow why because\nif descriptive analysis\nyou Need to have a good base\nand a stronghold\non the predictive part first.\nNow, it estimates relationship\nbetween the dependent variable\nand an independent variable.\nSo for those of you\nwho are not aware\nof these terminologies,\nlet me give you\na quick summary of it.\nSo dependent variable is\nnothing but a variable\nwhich you want to predict now,\nlet's say I want to know\nwhat will be the sales\non 26th of this month.\nSo sales becomes\na dependent variable\nor you can see\nthe target variable.\nNow this dependent variable\nor Target variable are going\nto depend on a lot of actors.\nThe number of products\nyou sold till date\nor what is the season out there?\nIs there the availability\nof product or how is\nthe product quality\nand all these things?\nSo these are\nthe NeverEnding factors\nwhich are nothing\nbut the different features\nthat leads to sail\nso these variables are called\nas an independent variable\nor you can say the predictor now\nif you look\nat the graph over here,\nwe have some values of X\nand we have values of Y now\nas you can see over here\nif X increases the value\nof by also increases so\nlet me explain you this\nwith an example.\nLet's say we have\nuntil the value of x\nwhich is six point seven five\nand somebody asked you.\nWhat was the value of y\nwhen the value\nof x is 7 so the way\nthat you can do it\nor how regression comes\ninto the picture is\nby fitting a straight line\nby all these points\nand getting the value\nof M and C.\nSo this is straight line guys\nand the formula for the straight\nline is y is equal to MX plus C.\nSo using this we can try to\npredict the value of y so here\nif you notice the X variable\ncan increase as much as it can\nbut the Y variable\nwill increase according to x\nso Why is basically dependent\non your X variable?\nSo for any arbitrary value\nof x You can predict the value\nof y and this is always\ndone through regression.\nSo that is\nhow regression is useful.\nNow regression is basically\nclassified into three types\nyour linear regression,\nthen your logistic regression\nand polynomial regression.\nSo today we will be discussing\nlogistic regression.\nSo let's move forward\nand understand the what and by\nof logistic regression.\nNow this algorithm\nis most widely used\nwhen the dependent variable\nor you can see the output is\nin the binary.\nA format.\nSo here you need\nto predict the outcome\nof a categorical\ndependent variable.\nSo the outcome should be\nalways discreet or categorical\nin nature Now by discrete.\nI mean the value\nshould be binary\nor you can say you just have\ntwo values it can either be 0\nor 1 it can either be yes\nor a no either be true\nor false or high or low.\nSo only these can be\nthe outcomes so the value\nwhich you need to create\nit should be discrete\nor you can say\ncategorical in nature.\nWhereas in linear regression.\nWe have the value of by\nor you can see Val you need\nto predict within a range\nthat is how there's a difference\nbetween linear regression\nand logistic regression.\nWe must be having question.\nWhy not linear regression now\nguys in linear regression\nthe value of by or the value,\nwhich you need to\npredict is in a range,\nbut in our case as\nin the logistic regression,\nwe just have two values\nit can be either 0\nor it can be one.\nIt should not entertain\nthe values which is\nbelow zero or above one.\nBut in linear regression,\nwe have the value of y\nin the range so here\nin order to implement\nlogic regression we\nneed To clip this part\nso we don't need the value\nthat is below zero\nor we don't need the value\nwhich is above 1\nso since the value of y will be\nbetween only 0 and 1\nthat is the main rule\nof logistic regression.\nThe linear line has\nto be clipped at 0 and 1 now.\nOnce we clip this graph it\nwould look somewhat like this.\nSo here you're getting the curve\nwhich is nothing but\nthree different straight lines.\nSo here we need to make\na new way to solve this problem.\nSo this has to be\nformulated into equation.\nAnd hence we come up\nwith logistic regression.\nSo here the outcome is either 0\nOr one which is the main rule\nof logistic regression.\nSo with this our resulting curve\ncannot be formulated.\nSo hence our main aim\nto bring the values to 0\nand 1 is fulfilled.\nSo that is how we came up with\nlarge stick regression now here\nonce it gets formulated\ninto an equation.\nIt looks somewhat like this.\nSo guys, this is\nnothing but an S curve\nor you can say the sigmoid curve\na sigmoid function curve.\nSo this sigmoid function\nbasically converts any value\nfrom minus infinity to Infinity\nto your discrete values,\nwhich a Logitech regression\nwants or it Can say the values\nwhich are in binary\nformat either 0 or 1.\nSo if you see here\nthe values as either 0\nor 1 and this is nothing\nbut just a transition of it,\nbut guys there's\na catch over here.\nSo let's say I have\na data point that is 0.8.\nNow, how can you decide\nwhether your value is 0\nor 1 now here you\nhave the concept\nof threshold which basically\ndivides your line.\nSo here threshold value\nbasically indicates the\nprobability of either winning\nor losing so here by winning.\nI mean the value is equal.\nOne and by losing I mean\nthe values equal to 0\nbut how does it do that?\nLet's have a data point\nwhich is over here.\nLet's say my cursor is at 0.8.\nSo here I check\nwhether this value is less\nthan the threshold value or not.\nLet's say if it is more\nthan the threshold value.\nIt should give me the result\nas 1 if it is less than that,\nthen should give me\nthe result is zero.\nSo here my threshold\nvalue is 0.5.\nI need to Define that\nif my value let's is 0.8.\nIt is more than 0.5.\nThen the value shall\nbe rounded of two one.\nOne and let's say\nif it is less than 0.5.\nLet's I have a value 0.2 then\nshould reduce it to zero.\nSo here you can use the concept\nof threshold value\nto find output.\nSo here it should be discreet.\nIt should be either 0\nor it should be one.\nSo I hope you caught this curve\nof logistic regression.\nSo guys, this is\nthe sigmoid S curve.\nSo to make this curve\nwe need to make an equation.\nSo let me address\nthat part as well.\nSo let's see how an equation\nis formed to imitate\nthis functionality so over here,\nwe have an equation\nof a straight.\nLine, which is y is\nequal to MX plus C.\nSo in this case,\nI just have only one independent\nvariable but let's say\nif we have many independent\nvariable then the equation\nbecomes m 1 x 1\nplus m 2 x 2 plus m 3 x\n3 and so on till M NX n now,\nlet us put in B and X.\nSo here the equation\nbecomes Y is equal to b 1 x\n1 plus beta 2 x\n2 plus b 3 x 3 and so on\ntill be nxn plus\nC. So guys equation\nof the straight line has a range\nfrom minus infinity to Infinity.\nYeah, but in our case\nor you can say largest equation\nthe value which we need\nto predict or you can say\nthe Y value it can have\nthe range only from 0 to 1.\nSo in that case we need\nto transform this equation.\nSo to do that what we\nhad done we have just divide\nthis equation by 1 minus y\nso now Y is equal\nto 0 so 0 over 1 minus\n0 which is equal to 1\nso 0 over 1 is again 0\nand if we take Y is equals to 1\nthen 1 over 1 minus 1 which is 0\nso 1 over 0 is infinity.\nSo here are my range is now.\nBetween 0 to Infinity,\nbut again, we want the range\nfrom minus infinity to Infinity.\nSo for that\nwhat we'll do we'll have\nthe log of this equation.\nSo let's go ahead\nand have the logarithmic\nof this equation.\nSo here we have this transform\nit further to get the range\nbetween minus infinity\nto Infinity so over\nhere we have log of Y\nover 1 minus 1\nand this is your final\nlogistic regression equation.\nSo guys, don't worry.\nYou don't have to write\nthis formula or memorize\nthis formula in Python.\nYou just need to\ncall this function\nwhich is logistic regression\nand Everything will be\nautomatically for you.\nSo I don't want to scare\nyou with the maths\nin the formulas behind it.\nBut it is always good to know\nhow this formula was generated.\nSo I hope you guys are clear\nwith how logistic regression\ncomes into the picture next.\nLet us see what are\nthe major differences\nbetween linear regression was\na logistic regression the first\nof all in linear regression,\nwe have the value\nof y as a continuous variable\nor the variable\nbetween need to predict\nare continuous in nature.\nWhereas in logistic regression.\nWe have the categorical variable\nso here the value\nwhich you need to Should\nbe discrete in nature.\nIt should be either 0\nor 1 or should have\njust two values to it.\nFor example,\nwhether it is raining\nor it is not raining\nis it humid outside\nor it is not humid outside.\nNow, how's it going to snow\nand it's not going to snow.\nSo these are the few example,\nwe need to predict\nwhere the values are discrete\nor you can just predict\nwhere this is happening or not.\nNext linear equation solves\nyour regression problems.\nSo here you have a concept\nof independent variable\nand a dependent variable.\nSo here you can calculate\nthe value of y\nwhich you need to Plate it.\nUsing the value of x.\nSo here your y variable\nor you can see the value\nthat you need to\npredict are in a range.\nBut whereas in\nlogistic regression,\nyou have discrete values.\nSo logistic regression basically\nsolves a classification problem\nso it can basically classify it\nand it can just give you result\nwhether this event\nis happening or not.\nSo I hope it is pretty\nmuch Clear till now\nnext in linear regression.\nThe graph that you have seen\nis a straight line\ngraph so over here,\nyou can calculate the value of y\nwith respect to the value of x\nwhere as in logistic regression.\nGlad that we got was a Escobar.\nYou can see the sigmoid curve.\nSo using the sigmoid function\nYou can predict your y values.\nSo I hope you guys are clear\nwith the differences\nbetween the linear regression\nand logistic regression\nmoving the a little see\nthe various use cases\nwhere in logistic regression\nis implemented in real life.\nSo the very first is\nweather prediction now\nlargest aggression helps\nyou to predict your weather.\nFor example, it\nis used to predict\nwhether it is raining\nor not whether it is sunny.\nIs it cloudy or not?\nSo all these things\nthings can be predicted\nusing logistic regression.\nWhere as you need\nto keep in mind\nthat both linear regression\nand logistic regression can be\nused in predicting the weather.\nSo in that case linear\nregression helps you to predict\nwhat will be\nthe temperature tomorrow\nwhereas logistic regression\nwill only tell you\nwhich is going to rain or not\nor whether it's cloudy or not,\nwhich is going to snow or not.\nSo these values are discrete.\nWhereas if you apply\nlinear regression,\nyou will predicting things like\nwhat is the temperature tomorrow\nor what is the temperature\nday after tomorrow\nand all those thing?\nSo these are the slight?\nIs between linear regression\nand logistic regression\nthe moving ahead.\nWe have classification problem.\nSo python performs\nmulti-class classification,\nso here it can help you tell\nwhether it's a bird.\nIt's not a board.\nThen you classify\ndifferent kind of mammals.\nLet's say whether it's a dog\nor it's not a dog similarly,\nyou can check it for reptile\nwhether it's a reptile\nor not a reptile.\nSo in logistic regression,\nit can perform\nmulti-class classification.\nSo this point\nI've already discussed\nthat it is using\nclassification problems next.\nIt also helps you\nto determine the illnesses.\nWhere so let me take an example.\nLet's say a patient goes for\na routine check up in hospital.\nSo what doctor will do it,\nit will perform various tests\non the patient and we'll check\nwhether the patient is\nactually a law or not.\nSo what will be the features\nso doctor can check\nthe sugar level\nthe blood pressure then what\nis the age of the patient?\nIs it very small or is\nit the old person then?\nWhat is the previous medical\nhistory of the patient\nand all of these features\nwill be recorded by the doctor\nand finally, dr.\nChecks the patient\ndata and Data -\nthe outcome of Illness\nand the severity of illness.\nSo using all the data\nof a doctor can identify\nwhether a patient is ill or not.\nSo these are\nthe various use cases\nin which you can use\nlogistic regression now,\nI guess enough of theory part.\nSo let's move ahead and see some\nof the Practical implementation\nof logistic regression\nso over here,\nI be implementing two projects\nwhen I have the data set\nof a Titanic so over here\nwill predict what factors made\npeople more likely\nto survive the sinking\nof the Titanic ship anime.\nSecond project will see\nthe data analysis.\nOn the SUV cars so over here.\nWe have the data of the SUV cars\nwho can purchase it\nand what factors made people\nmore interested in buying SUV.\nSo these will be\nthe major questions as\nto why you should Implement\nlogistic regression and\nwhat output will you get by it?\nSo let's start by\nthe very first project\nthat is Titanic data analysis.\nSo some of you might know\nthat there was a ship\ncalled as Titanic\nwith basically hit an iceberg\nand sank to the bottom\nof the ocean and it was\na big disaster at that time\nbecause it was the first\nvoyage of the ship.\nIt was supposed to be really\nreally strongly built and one\nof the best ships of that time.\nSo it was a big disaster\nof that time.\nAnd of course there is a movie\nabout this as well.\nSo many of you\nmight have washed it.\nSo what we have we have data\nof the passengers those\nwho survived and those\nwho did not survive\nin this particular tragedy.\nSo what you have to do you\nhave to look at this data\nand analyze which factors\nwould have been contributed\nthe most to the chances\nof a person survival\non the ship or not.\nSo using the logistic\nregression, we can predict\nwhether the person survived\nor the person died.\nNow apart from this\nwe also have a look\nwith the various features\nalong with that.\nSo first it is explore\nthe data set so over here,\nwe have the index value\nthen the First Column\nis passenger ID,\nthen my next column\nis survived so over here,\nwe have two values\na 0 and a 1 so 0 stands\nfor did not survive\nand one stands for survive.\nSo this column is categorical\nwhere the values\nare discrete next.\nWe have passenger class\nso over here,\nwe have three values 1 2 and 3.\nSo this basically tells you\nthat whether a I think\na stabbing in the first class\nsecond class or third class.\nThen we have the name\nof the passenger.\nWe have the six or you can see\nthe gender of the passenger\nwhere the passenger\nis a male or female.\nThen we have the age\nwe have the Sip SP.\nSo this basically means\nthe number of siblings\nor the spouses aboard\nthe Titanic so over here,\nwe have values such as 1\n0 and so on then we have\nParts apart is basically\nthe number of parents\nor children aboard\nthe Titanic so over here,\nwe also have some values\nthen we I have\nthe ticket number.\nWe have the fear.\nWe have the cabin number\nand we have the embarked column.\nSo in my inbox column,\nwe have three values\nwe have SC and Q.\nSo s basically stands\nfor Southampton C\nstands for Cherbourg\nand Q stands for Queenstown.\nSo these are the features\nthat will be applying\nour model on so here\nwe'll perform various steps\nand then we'll be implementing\nlogistic regression.\nSo now these are\nthe various steps\nwhich are required\nto implement any algorithm.\nSo now in our case\nwe are implementing\nlogistic regression, so,\nVery first step is\nto collect your data\nor to import the libraries\nthat are used for\ncollecting your data\nand then taking it forward then\nmy second step is to analyze\nyour data so over here,\nI can go to the various fields\nand then I can analyze the data.\nI can check did the females\nor children survive\nbetter than the males\nor did the rich\npassenger survived more\nthan the poor passenger\nor did the money matter as in\nwho paid more to get\ninto the shape\nwith the evacuated first?\nAnd what about the workers\ndoes the worker survived\nor what is the survival rate?\nIf you were the worker\nin the ship and not just\na traveling passenger,\nso all of these are very\nvery interesting questions\nand you would be going\nthrough all of them one by one.\nSo in this stage,\nyou need to analyze our data\nand explore your data as much as\nyou can then the third step is\nto Wrangle your data now\ndata wrangling basically means\ncleaning your data so over here,\nyou can simply remove\nthe unnecessary items or\nif you have a null values\nin the data set.\nYou can just clear that data and\nthen you can take it forward.\nSo in this step you can build\nyour model using the train data.\nAnd then you can test it\nusing a test so over here you\nwill be performing a split\nwhich basically split\nyour data set into training\nand testing data set and find\nyou will check the accuracy.\nSo as to ensure\nhow much accurate\nyour values are.\nSo I hope you guys got\nthese five steps\nthat you're going to implement\nin autistic regression.\nSo now let's go into all\nthese steps in detail.\nSo number one.\nWe have to collect your data\nor you can say\nimport the libraries.\nSo it may show you\nthe implementation part as well.\nSo I just open\nmy jupyter notebook\nand I just Implement\nall of these steps.\nIt's side-by-side.\nSo guys this is\nmy jupyter notebook first.\nLet me just rename\njupyter notebook to let's say\nTitanic data analysis.\nNow our first step was\nto import all the libraries\nand collect the data.\nSo let me just import\nall the libraries first.\nSo first of all,\nI'll import pandas.\nSo pandas is used\nfor data analysis.\nSo I'll say input pandas as PD\nthen I will be importing numpy.\nSo I'll say import numpy as NP\nso numpy is a library in Python\nwhich basically stands\nfor numerical Python\nand it is widely used to perform\nany scientific computation.\nNext.\nWe will be importing Seaborn.\nSo c 1 is a library for\nstatistical brought think so.\nSay import Seaborn as SNS.\nI'll also import matplotlib.\nSo matplotlib library\nis again for plotting.\nSo I'll say import\nmatplotlib dot Pi plot\nas PLT now to run this library\nin jupyter Notebook all I have\nto write in his percentage\nmatplotlib in line.\nNext I will be importing\none module as well.\nSo as to calculate the basic\nmathematical functions,\nso I'll say import mats.\nSo these are the libraries\nthat I will be needing\nin this Titanic data analysis.\nSo now let me just\nimport my data set.\nSo I will take a variable.\nLet's say Titanic data\nand using the pandas.\nI will just read my CSV\nor you can see the data set.\nI like the name of my data set\nthat is Titanic dot CSV.\nNow.\nI have already showed you\nthe data set so over here.\nLet me just print\nthe top 10 rows.\nSo for that I will just say\nI take the variable\nTitanic data dot head\nand I'll say the top ten rules.\nSo now I'll just run this\nso to run these fellows\nhave to press shift + enter\nor else you can just directly\nclick on this cell so over here.\nI have the index.\nWe have the passenger ID,\nwhich is nothing.\nBut again the index\nwhich is starting from 1 then\nwe have the survived column\nwhich has a category.\nCall values or you can say\nthe discrete values,\nwhich is in the form of 0 or 1.\nThen we have\nthe passenger class.\nWe have the name\nof the passenger 6 8\nand so on so this\nis the data set\nthat I will be going forward\nwith next let us bring\nthe number of passengers\nwhich are there in\nthis original data set for that.\nI'll just simply type in print.\nI'll say a number of passengers.\nAnd using the length function,\nI can calculate\nthe total length.\nSo I'll say length\nand inside this I\nwill be passing this variable\nbecause Titanic data,\nso I'll just copy it from here.\nI'll just paste it dot index\nand next set me\njust bring this one.\nSo here the number of passengers\nwhich are there in the original\ndata set we have is 891\nso around this number\nwere traveling in\nthe Titanic ship so over here,\nmy first step is done\nwhere you have just collected\ndata imported all the libraries\nand find out the total\nnumber of passengers,\nwhich are Titanic so\nnow let me just go back\nto presentation and let's see.\nWhat is my next step.\nSo we're done with\nthe collecting data.\nNext step is to analyze\nyour data so over here,\nwe will be creating different\nplots to check the relationship\nbetween variables as\nin how one variable\nis affecting the other\nso you can simply explore\nyour data set by making use\nof various columns\nand then you can plot\na graph between them.\nSo you can either plot\na correlation graph.\nYou can plot\na distribution curve.\nIt's up to you guys.\nSo let me just go back\nto my jupyter notebook and let\nme analyze some of the data.\nOver here.\nMy second part is\nto analyze data.\nSo I just put this in headed\nto now to put this in here\nto I just have to go\nand code click on mark down\nand I just run this so first\nlet us plot account plot\nwhere you can pay\nbetween the passengers\nwho survived and\nwho did not survive.\nSo for that I will be using\nthe Seabourn Library so over\nhere I have imported\nSeaborn as SNS\nso I don't have\nto write the whole name.\nI'll simply say\nSNS dot count plot.\nI say axis with the survive\nand the data\nthat I'll be using\nis the Titanic data\nor you can say the name\nof variable in which you\nhave store your data set.\nSo now let me just run this\nso who were here as you can see\nI have survived column on my x\naxis and on the y axis.\nI have the count.\nSo 0 basically stands\nfor did not survive\nand one stands\nfor the passengers\nwho did survive so over here,\nyou can see that around 550\nof the passengers\nwho did not survive and they\nwere around 350 passengers\nwho only survive so here\nyou can basically compute.\nThere are very less survivors\nthan on survivors.\nSo this was the very\nfirst floor now\nthat is not another plot\nto compare the sex as to whether\nout of all the passengers\nwho survived and\nwho did not survive.\nHow many were men and\nhow many were female\nso to do that?\nI'll simply say\nSNS dot count plot.\nI add the Hue as six\nso I want to know\nhow many females and\nhow many male survive\nthen I'll be\nspecifying the data.\nSo I'm using Titanic data\nset and let me just run\nthis you have done a mistake\nover here so over here you\ncan see I have survived\ncolumn on the x-axis\nand I have the count\non the why now.\nSo here your view color stands\nfor your male passengers\nand orange stands\nfor your female.\nSo as you can see\nhere the passengers\nwho did not survive\nthat has a value\n0 so we can see that.\nMajority of males did not\nsurvive and if we see the people\nwho survived here,\nwe can see the majority\nof female survive.\nSo this basically concludes\nthe gender of the survival rate.\nSo it appears on average\nwomen were more than three\ntimes more likely\nto survive than men next.\nLet us plot another plot\nwhere we have the Hue as\nthe passenger class so over\nhere we can see which class at\nthe passenger was traveling in\nwhether it was traveling\nin class one two,\nor three so for that I just\ntried the same command.\nI'll say SNS dot count plot.\nI keep my x-axis as\nsubtly I'll change my you\nto passenger class.\nSo my variable\nnamed as PE class.\nAnd the data said\nthat I'll be using\nis Titanic data.\nSo this is my result\nso over here you can see I have\nblue for first-class orange\nfor second class and green\nfor the third class.\nSo here the passengers\nwho did not survive a majorly\nof the third class\nor you can say the lowest class\nor the cheapest class to get\ninto the dynamic and the people\nwho did survive majorly belong\nto the higher classes.\nSo here 1 & 2 has more eyes\nthan the passenger\nwho were traveling\nin the third class.\nSo here we have concluded\nthat the passengers\nwho did not survive\na majorly of third class.\nUs all you can see\nthe lowest class\nand the passengers\nwho were traveling\nin first and second class\nwould tend to survive more next.\nI just got a graph for\nthe age distribution over here.\nI can simply use my data.\nSo we'll be using\npandas library for this.\nI will declare an array\nand I'll pass in the column.\nThat is age.\nSo I plot and I want a histogram\nso I'll say plot da test.\nSo you can notice over here\nthat we have more\nof young passengers,\nor you can see the children\nbetween the ages 0 to 10\nand then we have\nthe average people\nand if you go ahead Lester\nwould be the population.\nSo this is the analysis\non the age column.\nSo we saw that we have\nmore young passengers and more\nmediocre eight passengers,\nwhich are traveling\nin the Titanic.\nSo next let me plot\na graph of fare as well.\nSo I'll say Titanic data.\nI say fair.\nAnd again, I got a histogram\nso I'll say haste.\nSo here you can see\nthe fair size is\nbetween zero to hundred now.\nLet me add the bin size.\nSo as to make it\nmore clear over here,\nI'll say Ben is equals to let's\nsay 20 and I'll increase\nthe figure size as well.\nSo I'll say fixed size.\nLet's say I'll give\nthe dimensions as 10 by 5.\nSo it is bins.\nSo this is more clear now next.\nIt is analyzed\nthe other columns as well.\nSo I'll just type\nin Titanic data\nand I want the information as\nto what all columns are left.\nSo here we have passenger ID,\nwhich I guess it's\nof no use then you have see\nhow many passengers survived\nand how many did not we\nalso see the analysis\non the gender basis.\nWe saw when the female\ntend to survive more\nor the maintain to survive more\nthen we saw the passenger class\nwhere the passenger is traveling\nin the first class second class\nor third class.\nThen we have the name.\nSo in name,\nwe cannot do any analysis.\nWe saw the sex we\nsaw the age as well.\nThen we have sea bass P.\nSo this stands for the number\nof siblings or the spouses\nwhich Are aboard the Titanic so\nlet us do this as well.\nSo I'll say SNS dot count plot.\nI mentioned X SC SP.\nAnd I will be using\nthe Titanic data\nso you can see the plot\nover here so over here you\ncan conclude that.\nIt has the maximum value\non zero so you can conclude\nthat neither children\nnor a spouse was\non board the Titanic now\nsecond most highest value is 1\nand then we have various values\nfor 2 3 4 and so on next\nif I go above the store\nthis column as well.\nSimilarly can do four parts.\nSo next we have part\nso you can see the number\nof parents or children\nwhich were aboard the Titanic\nso similarly can do.\nAs well then we have\nthe ticket number.\nSo I don't think so.\nAny analysis is\nrequired for Ticket.\nThen we have fears of a we\nhave already discussed as\nin the people would tend\nto travel in the first class.\nYou will be the highest view\nthen we have the cable number\nand we have embarked.\nSo these are the columns\nthat will be doing\ndata wrangling on\nso we have analyzed the data\nand we have seen\nquite a few graphs\nin which we can conclude which\nvariable is better than another\nor what is the relationship\nthe whole third step\nis my data wrangling\nso data wrangling basically\nmeans Cleaning your data.\nSo if you have a large data set,\nyou might be having\nsome null values\nor you can say Nan values.\nSo it's very important\nthat you remove all\nthe unnecessary items\nthat are present\nin your data set.\nSo removing this directly\naffects your accuracy.\nSo I'll just go ahead\nand clean my data\nby removing all the n n values\nand unnecessary columns,\nwhich has a null value\nin the data set\nthe next time you're\nperforming data wrangling.\nSupposed to fall I check\nwhether my data set\nis null or not.\nSo I'll say Titanic data,\nwhich is the name of my data set\nand I'll say is null.\nSo this will basically tell\nme what all values are null\nand will return me\na Boolean result.\nSo this basically\nchecks the missing data\nand your result will be\nin Boolean format\nas in the result will be true\nor false so Falls mean\nif it is not null\nand prove means\nif it is null,\nso let me just run this.\nOver here you can see\nthe values as false or true.\nSo Falls is where the value is\nnot null and Drew is\nwhere the value is none.\nSo over here you can see\nin the cabin column.\nWe have the very first value\nwhich is null so we have to do\nsomething on this so you can see\nthat we have a large data set.\nSo the counting does not stop\nand we can actually\nsee the some of it.\nWe can actually print\nthe number of passengers\nwho have the Nan value\nin each column.\nSo I'll say Titanic\nunderscore data is null\nand I want the sum of it all.\nSame thought some so\nthis is basically print\nthe number of passengers\nwho have the n n values\nin each column\nso we can see\nthat we have missing values\nin each column that is 177.\nThen we have the maximum value\nin the cave in column\nand we have very Less\nin the Embark column.\nThat is 2 so here\nif you don't want\nto see this numbers,\nyou can also plot a heat map\nand then you can visually\nanalyze it let me just do\nthat as well.\nSo I'll say SNSD heat map.\nAnd save I take labels.\nFalse Choice run this\nas we have already seen\nthat there were three columns\nin which missing data\nvalue was present.\nSo this might be age\nso over here almost 20%\nof each column has\na missing value.\nThen we have\nthe cabling columns.\nSo this is quite a large value\nand then we have two values\nfor embark column as well.\nAdd a see map for color coding.\nSo I'll say see map.\nSo if I do this\nso the graph becomes\nmore attractive so over here\nyellow stands for Drew or you\ncan say the values are null.\nSo here we have computed\nthat we have the missing value\nof H. We have a lot\nof missing values\nin the cabin column\nand we have very less value,\nwhich is not even visible\nin the Embark column as well.\nSo to remove\nthese missing values,\nyou can either replace\nthe values and you can put in\nsome dummy values to it or you\ncan simply drop the column.\nSo here let us suppose\npick the age column.\nSo first, let me\njust plot a box plot\nand they will analyze\nwith having a column as H.\nSo I'll say SNS dot box plot.\nI'll say x is equals\nto passenger class.\nSo it's p class.\nI'll say Y is equal\nto H and the data set\nthat I'll be using\nis Titanic side.\nSo I'll say three times goes\nto Titanic data.\nYou can see the edge\nin first class and second class\ntends to be more older rather\nthan we have it\nin the third class.\nWell that depends\non The Experience\nhow much you earn\nor might be there any number\nof reasons so here we concluded\nthat passengers who were\ntraveling in class one and class\ntwo a tend to be older than\nwhat we have in the class 3\nso we have found that we have\nsome missing values in EM.\nNow one way is to either just\ndrop the column\nor you can just simply fill\nin some values to them.\nSo this method is called\nas imputation now\nto perform data wrangling\nor cleaning it is for spring\nthe head of the data set.\nSo I'll say\ntightening knot head.\nSo it's Titanic.\nData, let's say I\njust want the five rows.\nSo here we have survived\nwhich is again categorical.\nSo in this particular column,\nI can apply\nlogic to progression.\nSo this can be my y value\nor the value\nthat you need to predict.\nThen we have\nthe passenger class.\nWe have the name.\nThen we have ticket number.\nWe're taping so over here.\nWe have seen that in keeping.\nWe have a lot of null values\nor you can say that any invalid\nwhich is quite visible as well.\nSo first of all,\nwe'll just drop this column\nfor dropping it.\nI'll just say\nTitanic underscore data.\nAnd I'll simply type\nin drop and the column\nwhich I need to draw so I\nhave to drop the cable column.\nI mention the access equals\nto 1 and I'll say\nin place also to true.\nSo now again, I just print\nthe head and let us see\nwhether this column\nhas been removed\nfrom the data set or not.\nSo I'll say Titanic dot head.\nSo as you can see here,\nwe don't have\ngiven column anymore.\nNow, you can also\ndrop the na values.\nSo I'll say\nTitanic data dot drop\nall the any values\nor you can say Nan\nwhich is not a number\nand I will say in place is equal\nto True its Titanic.\nSo over here,\nlet me again plot\nthe heat map and let's say\nfor the values we should before\nshowing a lot of null values.\nHas it been removed or not.\nSo I'll say SNS dot heat map.\nI'll pass in the data set.\nI'll check it is null.\nI'll say why tick labels\nis equal to false.\nAnd I don't want color coding.\nSo again I say false.\nSo this will basically\nhelp me to check\nwhether my values\nhas been removed\nfrom the data set or not.\nSo as you can see here,\nI don't have any null values.\nSo it's entirely black now.\nYou can actually know\nthe some as well.\nSo I'll just go above So\nI'll just copy this part\nand I just use the sum function\nto calculate the sum.\nSo here the tells me\nthat data set is clean as\nin the data set does not contain\nany null value or any Nan value.\nSo now we have R Angela data.\nYou can see cleaner data.\nSo here we have done just\none step in data wrangling\nthat is just removing\none column out of it.\nNow you can do a lot\nof things you can actually\nfill in the values\nwith some other values\nor you can just\ncalculate the mean\nand then you can just fit\nin the null values.\nBut now if I see my data set,\nso I'll say\nTitanic data dot head.\nBut now if I see you over here I\nhave a lot of string values.\nSo this has to be converted\nto a categorical variables\nin order to implement\nlogistic regression.\nSo what we will do\nwe will convert this\nto categorical variable\ninto some dummy variables and\nthis can be done using pandas\nbecause logistic regression\njust take two values.\nSo whenever you apply machine\nlearning you need to make sure\nthat there are\nno string values present\nbecause it won't be taking\nthese as your input variables.\nSo using string you don't have\nto predict anything but\nin my case I have the survived\ncolumns 2210 how many?\nPeople tend to survive\nand how many did not so CEO\nstands for did not survive\nand one stands for survive.\nSo now let me just\nconvert these variables\ninto dummy variables.\nSo I'll just use pandas\nand a say PD not get dummies.\nYou can simply press\ntab to autocomplete\nand say Titanic data\nand I'll pass the six\nso you can just simply click\non shift + tab to get\nmore information on this.\nSo here we have\nthe type data frame\nand we have the passenger ID\nsurvived and passenger class.\nSo if Run this you'll see\nthat 0 basically stands\nfor not a female and one stand\nfor it is a female similarly\nfor male 0 Stanford's not made\nand one Stanford may now we\ndon't require both these columns\nbecause one column\nitself is enough to tell us\nwhether it's male\nor you can say female or not.\nSo let's say if I want\nto keep only male I'll say\nif the value of mail is 1\nso it is definitely a maid\nand is not a female.\nSo that is how you don't need\nboth of these values.\nSo for that I just\nremove the First Column,\nlet's say a female so\nI'll say drop first.\nAndrew it has given\nme just one column\nwhich is male and has\na value 0 and 1.\nLet me just set this\nas a variable hsx so\nover here I can say sex dot head\nand just want to see\nthe first five rows.\nSorry, it's dot.\nSo this is how my data\nlooks like now here.\nWe have done it for sex.\nThen we have\nthe numerical values in age.\nWe have the numerical\nvalues in spouses.\nThen we have the ticket number.\nWe have the pair and we\nhave embarked as well.\nSo in Embark the values are in.\nC and Q so here also we can\napply this get dummy function.\nSo let's say I\nwill take a variable.\nLet's say embark.\nI'll use the pandas Library.\nI'll enter the column name\nthat is embarked.\nLet me just print\nthe head of it.\nSo I'll say Embark\ndot head so over here.\nWe have c q and s now here also\nwe can drop the First Column\nbecause these two\nvalues are enough\nwith the passenger\nis either traveling for Q.\nThat is Q in stone S4 sound time\nand if both the values\nare 0 then definitely\nthe passenger is from Cherbourg.\nThat is the third value\nso you can again\ndrop the first value.\nSo I'll say drop and true.\nLet me just run this.\nSo this is how my output looks\nlike now similarly you can do it\nfor The class as well.\nSo here also we have\nthree classes one two,\nand three so I'll just\ncopy the whole statement.\nSo let's say I want\nthe variable name.\nLet's say PCL.\nI'll pass in the column name\nthat is PE class and I'll just\ndrop the First Column.\nSo here also the values\nwill be 1 2 or 3\nand I'll just remove\nthe First Column.\nSo here we just left\nwith two and three so\nif both the values\nare 0 then definitely\nthe passengers travelling\nin the first class now,\nwe have made the values\nas categorical now,\nmy next step would be\nto concatenate all\nthese new rules into a data set.\nWe can see Titanic data using\nthe pandas will just concatenate\nall these columns.\nSo I'll Superior.\nOne cat and then say\nif we have to concatenate sex,\nwe have to concatenate\nembarked and PCL\nand then I will mention\nthe access to one.\nI'll just run this can you\nto print the head so\nover here you can see\nthat these columns\nhave been added over here.\nSo we have the mail column\nwith basically tells\nwhere the person is male\nor it's a female then\nwe have the Embark\nwhich is basically q\nand s so if it's traveling\nfrom Queenstown value\nwould be one else it\nwould be 0 and If both\nof these values are zeroed,\nit is definitely\ntraveling from Cherbourg.\nThen we have the passenger\nclass as 2 and 3.\nSo the value of both\nthese is 0 then passengers\ntravelling in class one.\nSo I hope you got this\ntill now now these are\nthe irrelevant columns\nthat we have it\nover here so we can just\ndrop these columns will drop\nin PE class the embarked column\nand the sex column.\nSo I'll just type\nin Titanic data dot drop\nand mention the columns\nthat I want to drop.\nSo I say I even read\nthe passenger ID\nbecause it's nothing\nbut just the index value\nwhich is starting from one.\nSo I'll drop this as well then\nI don't want name as well.\nSo I'll delete name as well.\nThen what else we can drop we\ncan drop the ticket as well.\nAnd then I'll just\nmention the axis.\nI'll say in place\nis equal to True.\nOkay.\nSo now my column name\nstarts uppercase.\nSo these has been dropped now,\nlet me just bring\nmy data set again.\nSo this is my final\nleader said guys,\nwe have the survived column\nwhich has the value 0\nand 1 then we have\nthe passenger class or we\nforgot to drop this as well.\nSo no worries.\nI'll drop this again.\nSo now let me just run this.\nSo over here we\nhave the survive.\nWe have the age.\nWe have the same SP.\nWe have the part.\nWe have Fair mail and these\nwe have just converted.\nSo here we have just\nperformed data angle.\nYou can see clean the data\nand then we have just\nconverted the values of gender\nto male then embarked to q\nand s and the passenger\nClass 2 2 & 3.\nSo this was all\nabout my data wrangling\nor just cleaning the data then\nmy next up is training\nand testing your data.\nSo here we will split\nthe data set into train subset\nand test steps.\nAnd then what we'll do\nwe'll build a model\non the train data\nand then predict the output\non your test data set.\nSo let me just go\nback to Jupiter\nand it is implement\nthis as well over here.\nI need to train my data set.\nSo I just put this\nindeed heading 3.\nSo over here,\nyou need to Define\nyour dependent variable\nand independent variable.\nSo here my Y is the output\nfor you can say the value\nthat you need to\npredict so over here,\nI will write Titanic data.\nI'll take the column\nwhich is survive.\nSo basically I have\nto predict this column\nwhether the passenger\nsurvived or not.\nAnd as you can see we have\nthe discrete outcome,\nwhich is in the form of 0\nand 1 and rest all the things we\ncan take it as a features or you\ncan say independent variable.\nSo I'll say Titanic data.\nNot drop so we just\nsimply drop the survive\nand all the other columns\nwill be my independent variable.\nSo everything else as\na features which leads\nto the survival rate.\nSo once we have defined\nthe independent variable\nand the dependent variable\nnext step is to split\nyour data into training\nand testing subset.\nSo for that we will\nbe using SK loan.\nI just type in from sklearn\ndot cross validation.\nimport train test plate Now here\nif you just click\non shift and tab,\nyou can go to the documentation\nand you can just see\nthe examples over here.\nI second class to open it\nand then I just go\nto examples and see\nhow you can split your data.\nSo over here you have\nextra next test wide range\nwhy test and then using\nthis train test platelet\nand just passing\nyour independent variable\nand dependent variable\nand just Define a size\nand a random straight to it.\nSo, let me just copy this\nand I'll just paste over here.\nOver here we will train test\nthen we have the dependent\nvariable train and test\nand using the split function\nwill pass in the independent\nand dependent variable\nand then we'll set a split size.\nSo let's say I'll put it up 0.3.\nSo this basically means\nthat your data set\nis divided in 0.3\nthat is in 70/30 ratio,\nand then I can add\nany random straight to it.\nSo let's say I'm applying\none this is not necessary.\nIf you want the same result\nas that of mine,\nyou can add the random shape.\nSo this will basically\ntake exactly the same sample\nevery Next I have to train\nand predict by creating a model.\nSo here logistic\nregression will graph\nfrom the linear regression.\nSo next I'll just type in\nfrom SK loan dot linear model\nimport logistic regression.\nNext I'll just create\nthe instance of this\nlogistic regression model.\nSo I'll say log model is equals\nto largest aggression now.\nI just need to fit my model.\nSo I'll say log model dot fit\nand I'll just pass\nin my ex train.\nand white rain Alright,\nso here it gives\nme all the details\nof logistic regression.\nSo here it gives me the class\nway dual fit intercept\nand all those things then\nwhat I need to do,\nI need to make prediction.\nSo I'll take a variable\nand checked addictions\nand I'll pass\non the model to it.\nSo I'll say log model dot\npredict and I'll pass\nin the value that is X test.\nSo here we have just\ncreated a model fit\nthat model and then we\nhad made predictions.\nSo now to evaluate how my model\nhas been performing.\nSo you can simply\ncalculate the accuracy\nor you can also calculate\na classification report.\nSo don't worry guys.\nI'll be showing both\nof these methods.\nSo I'll say\nfrom sklearn dot matrix\ninput classification report.\nIt's all here are used\nas fiction report.\nAnd inside this I'll be\npassing in white test\nand the predictions.\nSo guys this is\nmy classification report.\nSo over here,\nI have the Precision.\nI have the recall.\nWe have the advanced code\nand then we have support.\nSo here we have the value\nof decision as 75 72 and 73\nwhich is not that bad now\nin order to calculate\nthe accuracy as well.\nYou can also use the concept\nof confusion Matrix.\nSo if you want to print\nthe confusion Matrix,\nI will simply say\nfrom sklearn dot matrix import\nconfusion Matrix first of all,\nand then we just print this So\nhow my function has\nbeen imported successfully\nso I'll say confusion Matrix.\nAnd again passing\nthe same variables\nwhich is why\ntest and predictions.\nSo I hope you guys already know\nthe concept of confusion Matrix.\nSo I just tell you\nin a brief what\nconfusion Matrix is all about?\nSo confusion Matrix is nothing\nbut a 2 by 2 Matrix\nwhich has a four outcomes.\nThis basically tells us that\nhow accurate your values are.\nSo here we have\nthe column as predicted.\nNo predicted.\nWhy?\nAnd we have actual no\nand then actually yes.\nSo this is the concept\nof confusion Matrix.\nSo here let me just fade\nin these values\nwhich we have just calculated.\nSo here we have 105.\n105 2125 and 63 So\nas you can see here,\nwe have got four outcomes now\n105 is the value\nwhere a model has predicted.\nNo, and in reality.\nIt was also a no so\nwhere we have predicted know\nan actual know similarly.\nWe have 63 as a predicted.\nYes.\nSo here the model predicted.\nYes, and actually\nalso it was a yes.\nSo in order to\ncalculate the accuracy,\nyou just need to add the sum\nof these two values and divide\nthe whole by the some.\nSo here these two values\ntells me where the order\nhas actually predicted\nthe correct output.\nThis value is also\ncalled as true-\nThis is called\nas false positive.\nThis is called as true positive\nand this is called\na false negative.\nNow in order to\ncalculate the accuracy.\nYou don't have\nto do it manually.\nSo in Python,\nyou can just import\naccuracy score function\nand you can get\nthe results from that.\nSo I'll just do that as well.\nSo I'll say from sklearn\ndot-matrix import accuracy score\nand I'll simply\nprint the accuracy\nand we'll pass in\nthe same variables.\nThat is why it is\nand predictions so over.\nHere, it tells me the address.\nHe has 78 which is quite good so\nover here if you want to do it\nmanually, we have 2\nplus these two numbers,\nwhich is 105 263.\nSo this comes out to almost 168\nand then you have to divide\nby the sum of all\nthe phone numbers.\nSo 105 plus 63 plus 21 plus 25,\nso this gives me\na result of to 1/4.\nSo now if you divide\nthese two number,\nyou'll get the same accuracy\nthat is 78 percent or you\ncan say point seven eight.\nSo that is how you\ncan calculate the See,\nso now let me just go back\nto my presentation.\nI let's see what all we\nhave covered till now.\nSo here we have first\nplate our data into train\nand test subset then\nwe have build a model\non the train data\nand then predicted the output\non the test data set\nand then my fifth step\nis to check the accuracy.\nSo here we have calculator\naccuracy to almost 78 percent\nwhich is quite good.\nYou cannot say\nthat accuracy is bad.\nSo here it tells me\nhow accurate your results are\nso him accuracy score defines\nthat and hence got\na good accuracy.\nSo now moving ahead.\nLet us see the second project\nthat is SUV data analysis.\nSo in this a car company has\nreleased new SUV in the market\nand using the previous data\nabout the sales of their SUV.\nThey want to predict\nthe category of people\nwho might be interested\nin buying this.\nSo using the\nlogistic regression,\nyou need to find what factors\nmade people more interested\nin buying this SUV.\nSo for this let us hear data set\nwhere I have user ID.\nI have gender as male\nand female then we have\nthe age we have the estimated.\nMelody and then we have\nthe purchased column.\nSo this is my discreet column\nor you can see\nthe categorical column.\nSo here we just have the value\nthat is 0 and 1 and this column\nwe need to predict\nwhether a person can actually\npurchase a SUV or Not.\nSo based on these factors,\nwe will be deciding\nwhether a person can actually\npurchase a SUV or not.\nSo we know the salary\nof a person we know the age\nand using these we can predict\nwhether person can actually\npurchase SUV or not.\nSo, let me just go\nto my jupyter notebook\nand it is Implement\na logistic regression.\nSo guys, I I will not be going\nthrough all the details\nof data cleaning and analyzing\nthe part start part.\nI'll just leave it on you.\nSo just go ahead\nand practice as much as you can.\nAlright, so the second project\nis SUV predictions.\nSo first of all,\nI have to import\nall the libraries\nso I say import numpy as\nNP and similarly.\nI'll do the rest of it.\nAlright, so now let\nme just print the head\nof this data set.\nSo this we have already seen\nthat we have columns as user ID.\nWe have gender.\nWe have the H we have the salary\nand then we have to calculate\nwhether person can actually\npurchase a SUV or not.\nSo now let us just simply go on\nto the algorithm part.\nSo we'll directly start off\nwith the logistic regression\non how you can train a model.\nSo for doing all those things,\nwe first need to Define\nyour independent variable\nand dependent variable.\nSo in this case,\nI want my ex at is\nan independent variable is\na data set.\nI lock so here I will\nbe specifying all the School\nand basically stands for that\nand in the columns,\nI want only two and\nthree dot values.\nSo here we should fetch\nme all the rows\nand only the second\nand third column which is age\nand estimated salary.\nSo these are the factors\nwhich will be used to predict\nthe dependent variable\nthat is purchase.\nSo here my dependent\nvariable is purchase\nand independent variable\nis of age and salary\nso I'll say Lena said dot\nI love I'll have all the rows\nand add just one fourth column.\nThat is my purchased column.\nYou don't values.\nAll right, so I just forgot\nwhen one square\nbracket over here.\nAlright, so over here.\nI have defined my independent\nvariable and dependent variable.\nSo here my independent variable\nis age and salary\nand dependent variable\nis the column purchase.\nNow, you must be wondering\nwhat is this?\nI lock function.\nSo I look function is basically\nan index of a panda's data frame\nand it is used\nfor integer based indexing\nor you can also say\nselection by index now,\nlet me just bring\nthese independent variables\nand dependent variable.\nIf I bring the independent\nvariable I have age as\nwell as a salary next.\nLet me print the dependent\nvariable as well.\nSo over here you can see I\njust have the values in 0\nand 1 so 0 stands\nfor did not purchase next.\nLet me just divide my data set\ninto training and test subset.\nSo I'll simply write in\nfrom sklearn dot cross plate\nnot cross-validation.\nImport drain test next I'll just\npress shift + Tab\nand over here.\nI'll go to the examples\nand just copy the same line.\nSo I'll just copy this.\nAs move the points now,\nI want to text size\nto be let's see 25,\nso I have divided the train\nin tested in 75/25 ratio.\nNow, let's say I'll take\nthe random set of 0 So\nRandom State basically\nensures the same result\nor you can say the same samples\ntaken whenever you run the code.\nSo let me just run this now.\nYou can also scale\nyour input values\nfor better performing\nand this can be done\nusing standard scalar.\nSo let me do that as well.\nSo I'll say from sklearn\nDot pre-processing.\nImport standard scale now.\nWhy do we scale it now?\nIf you see a data set we\nare dealing with large numbers.\nWell, although we are using\na very small data set.\nSo whenever you're working\nin a prod environment,\nyou'll be working\nwith large data set we\nwill be using thousands and\nhundred thousands of you pulls\nso they're scaling\ndown will definitely\naffect the performance\nby a large extent.\nSo here let me just show you\nhow we can scale down\nthese input values and then\nthe pre-processing contains all\nyour methods & functionality,\nwhich is Required\nto transform your data.\nSo now let us scale down\nfor test as well as\na training data set.\nSo else First Make\nan instance of it.\nSo I'll say standard scalar.\nThen I have Extreme sasc Dot\nfit fit underscore transform.\nI'll pass in my Xtreme video.\nAnd similarly I can do\nit for test wherein\nI'll pass the X test.\nAll right.\nNow my next step is\nto import logistic regression.\nSo I'll simply apply\nlogistic regression\nby first importing it.\nSo I'll say from sklearn sklearn\nthe linear model import\nlogistic regression over here.\nI'll be using classifier.\nSo I said classifier dot\nis equals to logistically\naggression so over here,\nI just make an instance of it.\nSo I'll say logistic\nregression and over here.\nI just pass in the random state,\nwhich is 0 No,\nI simply fit the model.\nAnd I simply passing\nnext rain and white rain.\nSo here it tells\nme all the details\nof logistic regression.\nThen I have to\npredict the value.\nSo I'll say why I prayed\nit's equal to classifier.\nThen predict function\nand then I just pass in X test.\nSo now we have\ncreated the model.\nWe have scaled down\nour input values.\nThen we have applied\nlogistic regression.\nWe have predicted the values\nand now we want\nto know the accuracy.\nSo now the accuracy first we\nneed to import accuracy scores.\nSo I'll say from sklearn dot\nmatrix input accuracy school\nand using this function we\ncan calculate the accuracy\nor you can manually do\nthat by creating\na confusion Matrix.\nSo I'll just pass.\nmy lightest and my y\npredicted All right,\nso over here I get the accuracy\nas 89% So we want to know\nthe accuracy in percentage.\nSo I just have to multiply it\nby a hundred and if I run this\nso it gives me 89%\nSo I hope you guys are clear\nwith whatever I\nhave taught you today.\nSo here I have taken\nmy independent variable as age\nand salary and then\nwe have calculated\nthat how many people\ncan purchase SUV\nand then we have calculated\nour model by checking\nthe accuracy so over here\nwe get the accuracies\n89 which is great.\nAlright guys that is\nit for today.\nSo I'll Scoffs what all we have\ncovered in today's training.\nFirst of all,\nwe had a quick introduction\nto what is regression\nand where the regression\nis actually use then\nwe have understood\nthe types of regression\nand then got into the details\nof what and why\nof logistic regression\nof compared linear was\nin logistic regression.\nWe have also seen\nthe various use cases\nwhere you can Implement\nlogistic regression in real life\nand then we have picked\nup two projects\nthat is Titanic data analysis\nand SUV prediction so\nover here we have seen\nhow we can collect your data\nanalyze your data then perform.\nModeling on that data train\nthe data test the data\nand then finally\nhave calculated the accuracy.\nSo in your SUV prediction,\nyou can actually\nanalyze clean your data\nand you can do a lot of things\nso you can just go ahead\npick up any data set\nand explore it as much as you\ncan open your eyes and see\naround you will find\ndozens of applications\nof machine learning\nwhich you are using\nand interacting with\nin your daily life peed\nbe using the phase detection.\nAnd Facebook are getting\nthe recommendation\nfor similar products\nfrom Amazon machine learning\nis applied almost everywhere.\nSo hello and welcome all\nto this YouTube session\nwill learn about\nhow to build a decision tree.\nThis session is\ndesigned in a way\nthat you get most out of it.\nAlright.\nSo this decision tree is a type\nof classification algorithm\nwhich comes under these\nsupervised learning technique.\nSo before learning\nabout decision tree,\nI'll give you a short\nintroduction to classification\nwhere we'll learn about.\nWhat is classification\nwhat I'd say,\nVarious types where it is used\nor what I'd see use cases now,\nonce you get your fundamental\nclear will jump\nto the decision tree\npart under this.\nFirst of all, I will teach\nyou to mathematically\ncreate a decision tree\nfrom scratch then once you\nget your Concepts clear,\nwe'll see how you can write\na decision tree classifier\nfrom scratch in Python\nusing the card algorithm.\nAll right.\nI hope the agenda is scared you\nguys what is classification?\nI hope every one of you\nmust have used Gmail.\nSo how do you think the male\nis getting classified as Spam\nor not spam mail.\nWell, there's nothing\nbut classification So\nWhat It Is Well\nclassification is the process\nof dividing the data set\ninto different categories\nor groups by adding label.\nIn other way, you can say\nthat it is a technique\nof categorizing the observation\ninto different category.\nSo basically what you\nare doing is you are taking\nthe data analyzing it\nand on the basis\nof some condition\nyou finely divided\ninto various categories.\nNow, why do we classify it?\nWell, we classify it to perform\npredictive analysis on it.\nLike when you get the mail\nthe machine predicts it\nto be a Spam or not spam mail\nand on the basis\nof that prediction it\nadd the irrelevant or spam mail\nto the respective folder\nin general this classification.\nAlgorithm handle questions.\nLike is this data belongs\nto a category or B category?\nLike is this a male or is this\na female something like that now\nthe question arises\nwhere will you use it?\nWell, you can use this\nof protection order\nto check whether\nthe transaction is genuine\nor not suppose I am using.\nA credit card here\nin India now due to some reason\nI had to fly to Dubai now.\nIf I'm using the credit\ncard over there,\nI will get a notification alert\nregarding my transaction.\nThey would ask me to confirm\nabout the transaction.\nSo this is also kind\nof predictive analysis\nas the machine predicts\nthat something fishy is\nin the transaction\nas very for our ago.\nI made the transaction using\nthe same credit card and India\nand 24 hour later.\nThe same credit card is being\nused for the payment in Dubai.\nSo the Machine predicts\nthat something fishy is going on\nin the transaction.\nSo in order to confirm it it\nsends you a notification alert.\nAll right.\nWell, this is one of\nthe use case of classification\nyou can even use it\nto classify different items\nlike fruits on the base\nof its taste color size\noverweight a machine.\nWell trained using\nthe classification algorithm\ncan easily predict the class\nor the type of fruit whenever\nnew data is given to it.\nNot just the fruit.\nIt can be any item.\nIt can be a car.\nIt can be a house.\nIt can be\na I'm bored or anything.\nHave you noticed\nthat while you visit some sites\nor you try to login\ninto some you get\na picture capture for that right\nwhere you have to identify\nwhether the given image is of\na car or its of a pole or not?\nYou have to select it\nfor example that 10 images\nand you're selecting\nthree Mages out of it.\nSo in a way you are training the\nmachine right you are telling\nthat these three are the picture\nof a car and rest are not\nso who knows you are training at\nfor something big right?\nSo moving on ahead.\nLet's discuss the types.\nS of classification online.\nWell, there are\nseveral different ways\nto perform the same tasks\nlike in order to predict\nwhether a given person is a male\nor a female the machine\nhad to be trained first.\nAll right,\nbut there are multiple ways\nto train the machine and you\ncan choose any one of them just\nfor Predictive Analytics.\nThere are many different\ntechniques but the most\ncommon of them all is\nthe decision tree,\nwhich we'll cover in depth\nin today's session.\nSo as a part of classification\nalgorithm we have\ndecision tree random Forest name\nbuys k-nearest neighbor.\nLogistic regression\nlinear regression support\nVector machines and so\non there are many.\nAlright, so let me give\nyou an idea about few\nof them starting\nwith decision tree.\nWell decision tree is\na graphical representation\nof all the possible solution\nto a decision the decisions\nwhich are made they\ncan be explained very easily.\nFor example here is a task,\nwhich says that should I go\nto a restaurant\nor should I buy a hamburger\nyou are confused on that.\nSo for that what you will do,\nyou will create a dish entry\nfor it starting\nwith the root node\nwill be first of all,\nyou will check\nwhether you are hungry or not.\nAll right,\nif you're not hungry then\njust go back to sleep.\nRight?\nIf you are hungry\nand you have $25 then you\nwill decide to go to restaurant.\nAnd if you're hungry\nand you don't have $25,\nthen you will just\ngo and buy a hamburger.\nThat's it.\nAll right.\nSo there's about decision tree\nnow moving on ahead.\nLet's see.\nWhat is a random Forest.\nWell random Forest build\nmultiple decision trees\nand merges them together\nto get a more accurate\nand stable production.\nAll right, most of the time\nrandom Forest is trained\nwith a bagging method.\nThe bagging method\nis based on the idea\nthat the combination\nof learning model increases\nthe overall result.\nIf you are combining the\nlearning from different models\nand then clubbing it together\nwhat it will do it will Increase\nthe overall result fine.\nJust one more thing.\nIf the size of your\ndata set is huge.\nThen in that case one single\ndecision tree would lead\nto our Offutt model same way\nlike a single person\nmight have its own perspective\non the complete population as\na population is very huge.\nRight?\nHowever, if we implement\nthe voting system and ask\ndifferent individual\nto interpret the data,\nthen we would be able\nto cover the pattern\nin a much meticulous way even\nfrom the diagram.\nYou can see that in section A\nwe have Howard large\ntraining data set what we do.\nWe first divide\nour training data set\ninto n sub-samples on it\nand we create a decision tree\nfor each cell sample.\nNow in the B part\nwhat we do we take the vote\nout of every decision made by\nevery decision tree.\nAnd finally we Club\nthe vote to get\nthe random Forest dition fine.\nLet's move on ahead.\nNext.\nWe have neighbor Buys.\nSo named by is is\na classification technique,\nwhich is based on Bayes theorem.\nIt assumes that It's\nof any particular feature\nin a class is completely\nunrelated to the presence\nof any other feature\nnamed buys is simple\nand easy-to-implement algorithm\nand due to a Simplicity\nthis algorithm might out\nperform more complex model\nwhen the size of the data set\nis not large enough.\nAll right, a classical use case\nof name bias is\na document classification.\nAnd that what you\ndo you determine\nwhether a given text corresponds\nto one or more categories\nin the text case,\nthe features used might be\nthe presence or absence.\nAbsence of any keyword.\nSo this was about Nev\nfrom the diagram.\nYou can see\nthat using neighbor buys.\nWe have to decide\nwhether we have\na disease or not.\nFirst what we do we\ncheck the probability\nof having a disease\nand not having the disease\nright probability\nof having a disease is 0.1\nwhile on the other hand\nprobability of not having\na disease is 0.9.\nOkay first, let's see\nwhen we have disease\nand we go to the doctor.\nAll right, so when we\nvisited the doctor\nand the test is positive\nAdjective so probability\nof having a positive test\nwhen you're having a disease\nis 0.8 0 and probability\nof a negative test\nwhen you already have\na disease that is 0.20.\nThis is also a false negative\nstatement as the test\nis detecting negative,\nbut you still have\nthe disease, right?\nSo it's a false\nnegative statement.\nNow, let's move ahead\nwhen you don't have\nthe disease at all.\nSo probability of not having\na disease is 0.9.\nAnd when you visit the doctor\nand the doctor is like, yes,\nyou have the disease.\nBut you already know\nthat you don't have the disease.\nSo it's a false\npositive statement.\nSo probability of having\na disease when you actually\nknow there is no disease\nis 0.1 and probability\nof not having a disease\nwhen you actually know\nthere is no disease.\nSo and the probability\nof it is around 0.90 fine.\nIt is same as probability\nof not having a disease\nin the test is showing\nthe same results\na true positive statement.\nSo it is 0.9.\nAll right.\nSo let's move on ahead and\ndiscuss about kn n algorithm.\nSo this KNN algorithm\nor the k-nearest neighbor,\nit stores all\nthe available cases\nand classifies new cases based\non the similarity measure the K\nin the KNN algorithm as\nthe nearest neighbor,\nwe wish to take vote\nfrom for example,\nif k equal 1 then the object\nis simply assigned to the class\nof that single nearest neighbor\nfrom the diagram.\nYou can see the difference\nin the image\nwhen k equal 1 k equal 3\nand k equal 5, right?\nWell the And systems\nare now able to use\nthe k-nearest neighbor\nfor visual pattern\nrecognization to scan\nand detect hidden packages\nin the bottom bin\nof a shopping cart\nat the checkout\nif an object is detected\nwhich matches exactly\nto the object listed\nin the database.\nThen the price of the spotted\nproduct could even\nautomatically be added\nto the customers Bill\nwhile this automated\nbilling practice is not used\nextensively at this time,\nbut the technology\nhas been developed\nand is available for use\nif you want you can\njust use It and yeah,\none more thing k-nearest\nneighbor is also used\nin retail to detect patterns\nin the credit card\nuses many new\ntransaction scrutinizing\nsoftware application\nuse Cayenne algorithms\nto analyze register data\nand spot unusual pattern\nthat indicates a\nspecies activity.\nFor example,\nif register data indicates\nthat a lot\nof customers information\nis being entered manually rather\nthan to automated scanning\nand swapping then in that case.\nThis could indicate\nthat the employees\nwere using the register.\nAre in fact stealing customers\npersonal information or\nif I register data indicates\nthat a particular good\nis being returned\nor exchanged multiple times.\nThis could indicate\nthat employees are misusing\nthe return policy\nor trying to make money\nfrom doing the fake returns.\nRight?\nSo this was about KNN algorithm\nsince our main focus\nfor this session will be\non decision tree.\nSo starting with\nwhat is decision tree,\nbut first, let me tell\nyou why did we choose\nthe Gentry to start with?\nWell, these decision tree\nare really very easy.\nEasy to read and understand it\nbelongs to one of the few models\nthat are interpretable\nwhere you can understand exactly\nwhy the classifier has made\nthat particular decision right?\nLet me tell you a fact\nthat for a given data set.\nYou cannot say\nthat this algorithm performs\nbetter than that.\nIt's like you cannot say that\nthe Asian trees\nbetter than a buys\nor name biases performing better\nthan decision tree.\nIt depends on\nthe data set, right?\nYou have to apply hit\nand trial method with all\nthe algorithms one by one\nand then compare the The model\nwhich gives the best\nresult as a model\nwhich you can use\nat for better accuracy\nfor your data set.\nAll right.\nSo let's start with\nwhat is decision tree.\nWell a decision tree is\na graphical representation\nof all the possible solution\nto our decision based\non certain conditions.\nNow, you might be wondering why\nthis thing is called\nas decision tree.\nWell, it is called so\nbecause it starts with the root\nand then branches off\nto a number of solution just\nlike a tree right\neven the trees.\nStarts from a roux and it\nstarts growing its branches\nonce it gets bigger\nand bigger similarly\nin a decision tree.\nIt has a roux\nwhich keeps on growing with\nincreasing number of decision\nand the conditions now,\nlet me tell you\na real life scenario.\nI won't say that all of you,\nbut most of you\nmust have used it.\nRemember whenever you dial\nthe toll-free number\nof your credit card company.\nIt redirects you\nto his intelligent\ncomputerised assistant\nwhere it asks\nyou questions like,\npress one for English\nor press 2 for Henry,\npress 3 for this press\n4 for that.\nGreat now once you\nselect one now again,\nit redirects you\nto a certain set\nof questions like press\n1 for this press 1 for that\nand similarly, right?\nSo this keeps on repeating\nuntil you finally get\nto the right person, right?\nYou might think\nthat you are caught\nin a voicemail hell\nbut what the company\nwas actually doing it\nwas just using a decision tree\nto get you to the right person.\nI lied.\nI'd like you to focus\non this particular image\nfor a moment on\nthis particular slide.\nYou can see I image\nwhere the task is.\nShould I accept a new job offer?\nOr not.\nAll right, so you have\nto decide that for that\nwhat you did you created\na decision tree starting\nwith the base condition\nor the root node.\nWas that the basic salary\nor the minimum salary\nshould be $50,000\nif it is not $50,000.\nThen you are not at all\naccepting the offer.\nAll right.\nSo if your salary is\ngreater than $50,000,\nthen you will further check\nwhether the commute is\nmore than one hour or not.\nIf it is more than one are you\nwill just decline the offer\nif it is less than one hour,\nthen you are getting closer\nto accepting the job offer.\nPhoto what you will\ndo you will check\nwhether the company\nis offering free coffee or not.\nRight if the company\nis not offering the free coffee,\nthen you will just declined off\nand if it is offering\nthe free coffee and\nyeah, you will happily\naccept the offer right there\nare just an example\nof a decision tree.\nNow, let's move ahead\nand understand a decision tree.\nWell, here is a sample data set\nthat I will be using\nit to explain you\nabout the decision tree.\nAlright in this data set\neach row is an example\nand the first two columns\nprovide features.\nAttributes that describes\nthe data and the last column\ngives the label or the class\nwe want to predict and\nif you like you can just\nmodify this data\nby adding additional features\nand more example\nand our program will work\nin exactly the same way fine.\nNow this data set\nis pretty straightforward\nexcept for one thing.\nI hope you have noticed that\nit is not perfectly separable.\nLet me tell you something\nmore about that as\nin the second\nand fifth examples,\nthey have the same features,\nbut different labels,\nboth are Yellow as a Colour\nand diameter as three,\nbut the labels are mango\nand lemon right?\nLet's move on and see\nhow our decision tree\nhandles this case.\nAll right, in order\nto build a tree will use\na decision tree algorithm\ncalled card this card algorithm\nstands for classification\nand regression tree\nalgorithm online.\nLet's see a preview\nof how it works.\nAll right to begin with We'll\nadd a root note for the tree\nand all the nodes receive a list\nof rows as input and the root\nwill receive the entire.\nTraining data set now each node\nwill ask true and false question\nabout one other feature.\nAnd in response\nto that question will split\nor partition the data set\ninto two different subsets\nthese subsets then become\ninput to child node.\nWe are to the tree\nand the goal of the question\nis to finally unmix the labels\nas we proceed down or in\nother words to produce\nthe purest possible distribution\nof the labels at each node.\nFor example, the input\nof this node contains only\none single type of label.\nSo we See\nthat it's perfectly unmixed.\nThere is no uncertainty\nabout the type of label\nas it consists\nof only grapes right\non the other hand the labels\nin this node are still mixed up.\nSo we would ask another question\nto further drill it down.\nRight but before that\nwe need to understand\nwhich question to ask and\nwhen and to do\nthat we need to conduct\nby how much question\nhelps to unmix the label\nand we can quantify\nthe amount of uncertainty\nat a single node using a metric.\nCalled gini impurity\nand we can quantify\nhow much a question reduces\nthat uncertainty using a concept\ncalled Information Gain will use\nthese to select the best\nquestion to ask at each point.\nAnd then what we'll do\nwe'll iterate the steps\nwill recursively build the tree\non each of the new node\nwill continue dividing the data\nuntil they are\nno further question to ask\nand finally we\nreach to our Leaf.\nAlright, alright.\nSo this was about decision tree.\nSo in order to create\na decision tree,\nfirst of all what you have\nto do you have to identify\nA different set of questions\nthat you can ask to a tree\nlike is this color green\nand what will be these question?\nThese questions will be decided\nby your data set like as\nthis colored green is\nthe diameter greater than equal\nto 3 is the color yellow\nright questions resembles\nto your data set remember that?\nAll right.\nSo if my color is green,\nthen what it will do it\nwill divide into two parts.\nFirst.\nThe Green Mango will be\nin the true while on the false.\nWe have lemon and the Mac.\nAll right if the color\nis green or the diameter.\nMeter is greater than equal to 3\nor the color is yellow\nAsian tree terminologies.\nSo starting with root node\nroot node is a base node\nof a tree the entire tree\nstarts from a root node.\nIn other words.\nIt is the first node\nof a tree it represents\nthe entire population or sample\nand this entire population\nis further segregated\nor divided into two or more\nhomogeneous set fine.\nNext is the leaf node.\nWell Leaf node is the one\nwhen you reach\nat the The tree right\nthat is you cannot\nfurther segregated down\nto any other level\nthat is the leaf node.\nNext is splitting splitting\nis dividing your root node\nor node into different sub part\non the basis of some condition.\nAll right, then comes\nthe branch or the sub tree.\nWell, this Branch\nor subtree gets formed\nwhen you split the tree suppose\nwhen you split a root node,\nit gets divided\ninto two branches\nor two subtrees.\nRight?\nNext is the concept of pruning.\nWell you can Say that pruning is\njust opposite of splitting\nwhat we are doing here.\nWe are just removing\nthe sub node of a decision tree\nwill see more about pruning\nlater in this session.\nAll right, let's move on ahead.\nNext is parent or child node.\nWell, first of all root node\nis always the parent node\nand all other nodes associated\nwith that is known\nas chalky node.\nWell, you can understand it\nin a way that all the top node\nbelongs to a parent node\nand all the bottom node,\nwhich are derived\nfrom a top node is a child node.\nNode producing a further note is\na child node and the node\nwhich is producing\nit as a parent node\nsimple concept, right?\nIt's use the cart algorithm\nand design a tree manually.\nSo first of all\nwhat you will do you\ndecide which question\nto ask and when so\nhow will you do that?\nSo let's first of all visualize\nthe decision tree.\nSo there's the decision tree\nwhich will be creating manually\nor like first of all,\nlet's have a look\nat the data set.\nYou have Outlook\ntemperature humidity\nand windy as your different\nattribute on the basis of\nthat you have to predict\nthat whether you\ncan play or not.\nSo which one among them should\nyou pick first answer determine\nthe best attribute that\nclassifies the training data?\nAll right.\nSo how will you choose\nthe best attribute\nor how does a tree decide\nwhere to split or how the tree\nwill decide its root node?\nWell before we move on\nand split a tree there\nare some terminologies\nthat you should know.\nAll right, first\nbeing the gini index.\nSo what is this gini index?\nThe gini index is the measure\nof impurity or Purity used\nin building a day.\nGentry and cart algorithm.\nAll right.\nNext is Information Gain\nthis Information Gain is\nthe decrease in entropy\nafter data set is split\non the basis of an attribute\nconstructing a decision tree is\nall about finding an attribute\nthat Returns the highest\nInformation Gain.\nAll right, so you\nwill be selecting the node\nthat would give you\nthe highest Information Gain.\nAlright next is\nreduction in variance.\nThis reduction in variance\nis an algorithm,\nwhich is used for\ncontinuous Target variable\nor regression problems the split\nWith lower variance\nis selected as a criteria\nto let the population see\nin general term.\nWhat do you mean by variance?\nVariance is how much\nyour data is wearing?\nRight?\nSo if your data is\nless impure or is more pure\nthan in that case\nthe variation would be less\nas all the data\nalmost similar, right?\nSo there's also a way\nof setting a tree the split\nwith lower variance\nis selected as the criteria\nto split the population.\nAlright.\nNext is the chi Square C Square.\nIt is an algorithm\nwhich is used to find out\nthese statistical significance\nbetween the Is between sub nodes\nand the parent nodes fine.\nLet's move ahead.\nNow.\nThe main question is\nhow will you decide\nthe best attribute\nfor now just understand\nthat you need to calculate\nsomething known as\nInformation Gain the attribute\nwith the highest Information\nGain is considered the best.\nYeah.\nI know your next question\nmight be like,\nwhat is this information again?\nBut before we move on and see\nwhat exactly Information Gain\nIs let me first introduce you\nto a term called entropy\nbecause this term\nwill be used in calculating\nthe Information Gain.\nMmmmmm.\nWell entropy is just a metric\nwhich measures the impurity\nof something or in other words,\nyou can say that as\nthe first step to do\nbefore you solve the problem\nof a decision tree\nas I mentioned is\nsomething about impurity.\nSo let's move on and understand\nwhat is impurity suppose.\nYou are a basket full of apples\nand another Bowl\nwhich is full of same label,\nwhich says Apple now\nif you are asked to pick\none item from each basket\nand ball then the probability\nof getting the apple\nand it's correct label is 1\nso in this case,\nYou can see\nthat impurities zero.\nAll right.\nNow what if there are\nfour different fruits\nin the basket and four different\nlabels in the bowl,\nthen the probability\nof matching the fruit\nto a label is obviously not one.\nIt's something less than that.\nWell, it could be possible\nthat I picked banana\nfrom the basket\nand when I randomly picked\nthe label from the ball,\nit says a cherry\nany random permutation\nand combination can be possible.\nSo in this case I'd say\nthat impurities is nonzero.\nI hope the concept\nof impurities care.\nAre so coming back to entropy\nas I said entropy is\nthe measure of impurity\nfrom the graph on your left.\nYou can see that\nas the probability\nis zero or one\nthat has either they\nare highly impure\nor they are highly pure\nthan in that case the value\nof entropy is zero.\nAnd when the probability is 0.5,\nthen the value\nof entropy is maximum.\nWell, what is impurity\nimpurities the degree\nof Randomness how random data is\nso if the data is\ncompletely pure in that case\nthe randomness equals 0 or\nif the Dies completely\nEmpire even in that case\nthe value of impurity\nwill be zero question.\nLike why is it\nthat the value\nof entropy is maximum\nat 0.5 might arise\nin a mine, right?\nSo let me discuss about that.\nLet me derive at mathematically\nas you can see here\non the slide,\nthe mathematical formula\nof entropy is -\nof probability of yes,\nlet's move on and see\nwhat this graph has to say\nmathematically suppose s is\nour total sample space\nand it's divided into two parts.\nYes, and no.\nNo, like in our data\nset the result for playing\nwas divided into two parts.\nYes or no,\nwhich we have to predict\neither we have to play or not.\nRight?\nSo for that particular case,\nyou can Define the formula\nof entropy as entropy\nof total sample\nspace equals negative\nof probability of e\nis multiplied by log\nof probability of years\nwith a base 2 minus probability\nof no X log of probability\nof no with base to where s\nis your total sample space\nand P of v s is the probability\nof E. And be of known as\nthe probability of no, well,\nif the number of yes\nequal number of know\nthat is probability\nof s equals 0.5 right since you\nhave equal number of yes,\nand no so in that case value\nof entropy will be one just\nput the value over there.\nAll right.\nLet me just move\nto the next slide.\nI'll show you this.\nAlright next is\nif it contains all Yes,\nor all know that is probability\nof a sample space is either 1\nor 0 then in that case entropy\nwill be equal to 0\nLet's see the\nmathematically one by one.\nSo let's start\nwith the first condition\nwhere the probability was 0.5.\nSo this is our formula\nfor entropy, right?\nSo there's our first case\nright which we discuss the art\nwhen the probability\nof vs equal probability of node\nthat is in our data set.\nWe have equal number\nof yes, and no.\nAll right.\nSo probability of yes\nequal probability of no\nand that equals\n0.5 or in other words,\nyou can say that yes\nplus no equal to Total sample.\nHe's all right,\nsince the probability is 0.5.\nSo when you put the values\nin the formula you get\nsomething like this\nand when you calculate it,\nyou will get the entropy of\nthe total sample space as one.\nAll right.\nLet's see for the next case.\nWhat is the next case\neither you have totally us\nor you have totally know so\nif you have total,\nyes, let's see the formula\nwhen we have totally as so\nyou have all yes and 0\nno fine.\nSo probability of\ne s equal 1 and yes.\nYes as the total\nsample space obviously.\nSo in the formula\nwhen you put that thing up here,\nyou get entropy\nof sample space equal negative X\nof 1 multiplied by log of 1\nas the value of log 1 equals 0.\nSo the total thing will result\nto 0 similarly is the case\nwith no even in that case,\nyou will get the entropy\nof total sample space as 0\nso this was all about entropy.\nAll right.\nNext is what is\nInformation Gain?\nWell Information Gain\nwhat it does is it measures\nthe reduction in entropy?\nIt decides which attributes\nshould be selected\nas the decision node.\nIf s is our total collection\nthan Information Gain\nequals entropy,\nwhich we calculated\njust now that -\nweighted average X entropy\nof each feature.\nDon't worry.\nWe'll just see\nhow it to calculate\nit with an example.\nLet's manually build\na decision tree\nfor our data set.\nSo there's our data set\nwhich consists of\n14 different instances\nout of which we have nine.\nYes and five know I like\nso we have the formula\nfor entropy just put\nover that since 9 years.\nSo total probability\nof e s equals 9\nby 14 and total probability\nof no equals Phi by 14\nand when you put up the value\nand calculate the result,\nyou will get the value\nof entropy as 0.94.\nAll right.\nSo this was your first step\nthat is compute the entropy for\nthe entire data set only now,\nyou have to select\nthat out of Outlook\ntemperature humidity and windy,\nwhich of the node should you\nselect as the root node\nbig question right?\nI will Decide\nthat this particular node should\nbe chosen at the base note.\nAnd on the basis of\nthat only I will be creating\nthe entire tree.\nI will select that.\nLet's see.\nSo you have to do it one\nby one you have\nto calculate the entropy\nand Information Gain for all\nof the different nodes.\nSo starting with Outlook.\nSo Outlook has\nthree different parameters\nSunny overcast and rainy.\nSo first of all select\nhow many number of years\nand no are there in the case\nof Sunny like when it is sunny\nhow many number of years\nand how many number of knows?\nAre there so in total we\nhave to yes and three Nos\nand case of sunny\nin case of overcast.\nWe have all yes.\nSo if it is overcast then\nwe will surely go to play.\nIt's like that.\nAlright and next it\nis rainy then total number\nof vs equal 3 and total number\nof no equals 2 fine next\nwhat we do we\ncalculate the entropy\nfor each feature for here.\nWe are calculating the entropy\nwhen Outlook equals Sunny.\nFirst of all,\nwe are assuming\nthat Outlook is our root node\nand for that we are calculating\nthe Can gain for it.\nAll right.\nSo in order to calculate\nthe Information Gain remember\nthe formula it was entropy\nof the total sample space -\nweighted average X entropy\nof each feature.\nAll right.\nSo what we are doing here,\nwe are calculating\nthe entropy of Outlook\nwhen it was sunny.\nSo total number of yes,\nwhen it was Sonny was\nto and total number of know\nthat was three fine.\nSo let's put up in the formula\nsince the probability\nof yes is 2 by 5\nand the probability\nof no is 3 by 5.\nSo you will get\nsomething like this.\nAll right.\nSo you are getting the entropy\nof sunny as zero point\nnine seven one fine.\nNext we will calculate\nthe entropy for overcast\nwhen it was overcast.\nRemember it was all yes, right.\nSo the probability\nof e is equal 1 and\nwhen you put over\nthat you will get the value\nof entropy as 0 fine\nand when it was rainy rainy\nhas 3s and to nose.\nSo probability of e s\nin case of Sonny's 3 by 5\nand probability of know\nin case of Sonny's 2 by 5\nand when you add the You\nof probability of vs\nand probability of note\nthe formula you get the entropy\nof sunny as zero point\nnine seven one point.\nNow, you have to calculate\nhow much information you\nare getting from Outlook\nthat equals weighted average.\nAll right.\nSo what was this weighted\naverage total number of years\nand total number of no fine.\nSo information from Outlook\nequals 5 by 14 from\nwhere does this 5 came over?\nWe are calculating\nthe total number of sample space\nwithin that particular Outlook\nwhen it was sunny, right?\nSo in case of Sunny there\nwas two years and three NOS.\nAll right.\nSo weighted average for Sonny\nwould be equal to 5 by 14.\nAll right,\nsince the formula was five\nby 14 x entropy of each feature.\nAll right, so\nas calculated the entropy\nfor Sonny is zero point\nnine seven one, right?\nSo what we'll do we'll multiply\nfive by 14 with 0.97 one, right?\nWell, this was\nthe calculation for information\nwhen Outlook equal sunny,\nbut Outlook even equals\novercast and rainy.\nIn that case,\nwhat we'll do again similarly\nwill calculate for everything\nfor overcast and sunny\nfor overcast weighted averages\nfor by 14 x its entropy.\nThat is 0 and for Sonny\nit is same 5i 14-3.\nYes and two nodes X its entropy\nthat is zero point\nnine seven one.\nAnd finally we'll take the sum\nof all of them which equals\nto 0.693 right next.\nWe will calculate\nthe information gained this\nwhat we did earlier was\nMalaysian taken from Outlook.\nNow.\nWe are calculating.\nWhat is the information?\nWe are gaining\nfrom Outlook right.\nNow this Information Gain\nthat equals to Total entropy\nminus the information\nthat is taken from Outlook.\nAll right.\nSo total entropy we had 0.94 -\ninformation we took\nfrom Outlook as 0.693.\nSo the value of information\ngained from Outlook results\nto zero point two four seven.\nAll right.\nSo next what we have to do.\nLet's assume that\nWendy is our root node.\nSo Wendy consists of\ntwo parameters false and true.\nLet's see how many years\nand how many nodes are there\nin case of true and false.\nSo when Wendy has\nFalls as its parameter,\nthen in that case,\nit has six years\nand two nodes and when it\nas true as its parameter,\nit has 3 S and 3 nodes.\nAll right.\nSo let's move ahead\nand similarly calculate\nthe information taken from Wendy\nand finally calculate the\ninformation gained from Wendy.\nAlright, so first of all,\nwhat we'll do we'll calculate\nthe entropy of each feature.\nER starting with\nwindy equal true.\nSo in case of true we\nhad equal number of yes\nand equal number of know.\nWe'll remember the graph\nwhen we had the probability as\n0.5 as total number of years\nequal total number of know\nand for that case\nthe entropy equals 1\nso we can directly\nwrite entropy of room\nwhen it's windy is one\nas we had already proved it\nwhen probability equals 0.5\nthe entropy is the maximum\nthat equals to 1.\nAll right.\nNext is entropy of false\nwhen it is Vending.\nI like so similarly just\nput the probability of yes\nand no in the formula\nand then calculate the result\nsince you have six years\nand to nose.\nSo in total,\nyou'll get the probability\nof yes 6 by 8 and probability\nof no as 2 by 8.\nAll right, so when you\nwill calculate it,\nyou will get the entropy\nof false as zero point\neight one one.\nAlright now, let's calculate\nthe information from windy.\nSo total information\ncollected from Windy\nequals information taken\nwhen Wendy equal true\nplus Action taken\nwhen Wendy equal false.\nSo we'll calculate the weighted\naverage for each one of them\nand then we'll sum it up\nto finally get the total\ninformation taken from windy.\nSo in this case,\nit equals to 8 by 14\nmultiplied by 0.8 1 1\nplus 6 by 14 x 1.\nWhat is this?\n8 it is total number of yes, and\nno in case when when D\nequals false, right?\nSo when it was false,\nso total number of BS\nthat equals to 6 and total more\nof know that equal to 2\nthat some UPS to 8.\nAlright, so that is\nwhy the waiter.\nResul results to Aid by\n14 similarly information taken\nwhen windy equals\ntrue equals to 3 plus 3\nthat is 3 S and 3 no equal 6\ndivided by total number\nof sample space that is 14 x 1\nthat is entropy of true.\nAll right.\nSo it is 8 by 14 multiplied\nby 0.8 1 1 plus 6 by 14 x one\nwhich results to 0.89 to this\nis information taken from Windy.\nAll right.\nNow how much information\nyou are gaining from Wendy?\nSo for that what you will do,\nso total information\ngained from Windy\nthat equals to Total entropy -\ninformation taken from Windy.\nAll right, that is 0.94 -\n0.89 to that equals\nto zero point zero four eight.\nSo 0.048 is the information\ngained from Windy.\nSimilarly.\nWe calculated for the rest too.\nSo for Outlook\nas you can see,\nthe information was 0.693,\nand it's Information Gain was\nzero point two four seven in\ncase of temperature\nthe information was around.\nZero point nine one one\nand the Information Gain\nthat was equal to 0.02\n9 in case of humidity.\nThe information gained was 0.15\nto and in the case of windy.\nThe information\ngained was 0.048.\nSo what we'll do\nwe'll select the attribute\nwith the maximum fine.\nNow, we are selected\nOutlook as our root node,\nand it is further subdivided\ninto three different parts\nSunny overcast and rain,\nso in case of overcast\nwe have seen\nthat it consists of all ears\nso we can consider\nit as a Leaf node,\nbut in case of sunny\nand rainy it's doubtful\nas it consists of both.\nYes and both know\nso you need to recalculate\nthe things right again\nfor this node.\nYou have to\nrecalculate the things.\nAll right, you have to again\nselect the attribute\nwhich is having\nthe maximum Information Gain.\nAll right, so there is\nhow your complete tree\nwill look like.\nAll right.\nSo, let's see when you can play\nso you can play\nwhen Outlook is overcast.\nAll right in that case.\nYou can always play\nif the Outlook is sunny.\nYou will further drill.\nTime to check\nthe humidity condition.\nAll right, if the\nhumidity is normal,\nthen you will play\nif the humidity is high\nthen you won't play right\nwhen the Outlook predicts\nthat it's raining then\nfurther you will check\nwhether it's windy or not.\nIf it is a week went\nthen you will go\nand offer play but\nif it has strong wind,\nthen you won't play right?\nSo this is\nhow your entire decision tree\nwould look like at the end.\nNow comes the concept\nof pruning say is\nthat what should I do to play?\nWell you have to do pruning\npruning will decide\nhow you will play.\nSay what is this pruning?\nWell, this pruning is nothing\nbut cutting down the nodes\nand order to get\nthe optimal solution.\nAll right.\nSo what pruning does it\nreduces the complexity?\nAll right,\nas are you can see on the screen\nthat it showing only the result\nfor yes that is it showing\nall the result which says\nthat you can play\nbefore we drill down\nto a practical session\na common question\nmight come in your mind.\nYou might think that our tree\nbased model better\nthan linear model right?\nYou can think like if I\ncan Was a logistic regression\nfor classification problem\nand linear regression\nfor regression problem.\nThen why there is\na need to use the tree.\nWell, many of us have\nthis question in their mind\nand well there's\na valid question too.\nWell actually as I said earlier,\nyou can use any algorithm.\nIt depends on\nthe type of problem.\nYou're solving let's look\nat some key factor,\nwhich will help you to decide\nwhich algorithm to use and\nwhen so the first point being\nif the relationship between\ndependent and independent\nvariable as well approximated\nby By a linear model,\nthen linear regression\nwill outperform tree\nbase model second case\nif there is a high non-linearity\nand complex relationship\nbetween dependent\nand independent variables\nat remodel will outperform\na classical regression model\nin third case.\nIf you need to build a model\nwhich is easy to explain\nto people a decision tree model\nwill always do better\nthan a linear model\nas the decision tree models\nare simpler to interpret\nthen linear regression.\nAll right.\nNow let's move on ahead and see\nhow you can write it as\nGentry classifier from scratch\nand python using\nthe cart algorithm.\nAll right for this.\nI will be using\njupyter notebook with python\n3.0 installed on it.\nAlright, so let's\nopen the Anaconda\nand the jupyter notebook.\nWhere is that?\nSo this is\nour Anaconda Navigator\nand I will directly jump over\nto jupyter notebook and hit\nthe launch button.\nI guess everyone\nknows that jupyter.\nNotebook is a web-based\ninteractive Computing notebook\nenvironment where you\ncan run your python codes.\nSo my Jupiter notebook it opens\non my Local Host w89\n1 so I will be using\nthis jupyter notebook\nin order to write\nmy decision tree classifier\nusing python for this\ndecision tree classifier.\nI have already written\nthe set of codes.\nLet me explain you\njust one by one.\nSo we'll start with initializing\nour training data set.\nSo there's our sample data set\nfor which each row\nis an example.\nThe last column is a label\nand the first two columns\nare the features.\nIf you want you can add some\nmore features an example\nfor your practice\ninteresting fact is\nthat This data set\nis design and way\nthat the second and fifth\nexample have almost\nthe same features,\nbut they have different labels.\nAll right, so\nlet's move on and see\nhow the tree handles this case\nas you can see here.\nBoth of them II and the fifth\ncolumn have the same features.\nWhat did different\nis just their label?\nRight?\nSo let's move ahead.\nSo this is our training data\nset next what we are doing we\nare adding some column labels.\nSo they are used only\nto print the trees fine.\nSo what we'll do we'll add\nheader to the columns\nlike the First Column is\nof Close second is of diameter\nand third is a label column.\nAll right, next\nwhat we'll do we'll Define\na function as unique values\nin which will pass the rows\nand the columns.\nSo this function\nwhat it will do it will find\nthe unique values for a column\nin the data set.\nSo there's an example for that.\nSo what we are doing here,\nwe are passing\ntraining data Hazard row\nand column number as 0 so\nwhat we are doing we are finding\nunique values in terms of color.\nAnd in this\nsince the row is training data\nand the column is 1\nso what you are doing here,\nso we are finding the you Values\nin terms of diameter fine.\nSo this is just an example next\nwhat we'll do we'll Define\na function as class count\nand we'll pass the rows into it.\nSo what it does,\nit counts the number\nof each type of example\nwithin data set.\nSo in this function\nwhat you are basically doing\nwe are counting the number\nof each type for example\nin the data set\nor what we are doing we\nare counting the unique values\nfor the label in the data\nset as a sample.\nYou can see here we can pass\nthat entire training data set\nto this particular function\nas class underscore count\nwhat it will do it will find\nall the different types of Label\nwithin the training data set\nas you can see here\nthe unique label consists\nof mango grape and lemon.\nSo next what we'll do.\nWe'll Define a function\nis numeric and we'll pass\na value into it.\nSo what it will do\nit will just test\nif the value is numeric or not\nand it will return if the value\nis an integer or a float.\nFor example, you\ncan see is numeric.\nWe are passing 7\nso it is an integer\nso it will return in value\nand if we are passing red,\nit's not a numeric value, right?\nSo moving on ahead\nwhere you define a class\nnamed as question,\nso This question\ndoes this question is used\nto partition the data set.\nThis class voted does it\njust records a column number?\nFor example 0 for color a light\nand a column value for example,\ngreen next what we are doing\nwe are defining a match method\nwhich is used to compare\nthe feature value in the example\nto the feature values\nstored in the question.\nLet's see how first of all\nwhat you are doing.\nWe are defining an init\nfunction and inside\nthat we are passing\nthe self column\nand the value as parameter.\nSo next what we do\nwe Define a function\nas match what it Does it\ncompares the feature value\nin an example to the feature\nvalue in this question\nwhen next we'll Define\na function as re PR,\nwhich is just a helper method\nto print the question\nin a readable format next\nwhat we are doing we are\ndefining a function partition.\nWell, this function\nis used to partition\nthe data set each row\nin the data set it checks\nif it match the question or not\nif it does so it adds it\nto the true rose or\nif not then it adds\nto the false Rose.\nAll right, for example,\nas you can see, it's\npartition the training data.\nBased on whether the roses\nare red or not here.\nWe are calling\nthe function question\nand we are passing a value\nof zero and read to it.\nSo what did we do it\nwill assign all the red rose\nto True underscore Rose\nand everything else\nwill be assigned\nto false underscore rose fine.\nNext.\nWhat we'll do we'll Define\na gini impurity function\nand inside that will pass\nthe list of rows.\nSo what it will do it will just\ncalculate the gini impurity\nfor the list of rows.\nNext what we are doing here.\nWe defining a function\nas Information Gain.\nSo what this Information Gain\nfunction does it calculates\nthe information game\nusing the uncertainty\nof the starting node -\nthe weighted impurity\nof the child node.\nThe next function\nis find the best plate.\nWell, this function is used\nto find the best question to ask\nby iterating over\nevery feature of value\nand then calculating\nthe Information Gain.\nBut the detail explanation\non the code,\nyou can find the code\nin the description given below.\nAll right next we'll define\na class as leave\nfor classifying the data.\nIt holds a dictionary of glass\nlike mango for how many times\nit appears in the row\nfrom the training data\nthat reaches the sleeve.\nAlright, next is\nthe decision node.\nSo this decision node,\nit will ask a question.\nThis holds a reference\nto the question\nand the two child nodes\non the base of it.\nYou are deciding which node\nto add further to which branch.\nAlright so next.\nWhat we are doing we\nare defining a function\nof build tree and inside\nthat we are passing\nour number of rows.\nSo this is the function\nthat is used to build the tree.\nSo initially what we did we\nDefine all the various function\nthat we'll be using\nin order to build a tree.\nSo let's start\nby partitioning the data set\nfor each unique attribute,\nthen we'll calculate\nthe information gain\nand then return the question\nthat produces the highest gain\nand on the basis of that\nwill split the tree.\nSo what we are doing here,\nwe are partitioning\nthe data set calculating\nthe Information Gain.\nAnd then what this is returning\nit is returning the question\nthat is producing\nthe highest gain.\nAll right.\nNow if gain equals\n0 return Leaf Rose,\nso what it will do.\nSo if you are getting\nno for the gain\nthat is gain equals\n0 then in that case\nsince no further question\ncould be asked\nso what it will do it\nwill return a leaf fine now true\nor underscore Rose\nor false underscore Rose\nequal partition with rose\nand the question.\nSo if we are reaching\ntill this position,\nthen you have already found.\nA feature of value\nwhich will be used\nto partition the data set then\nwhat you will do you\nwill recursively build\nthe true branch\nand similarly recursively\nbuild the false Branch.\nSo return Division\nand Discord node and side\nthat will be passing question\nto branch and false front.\nSo what it will do it\nwill return a question node.\nAlice question owed this\nrecalls the best feature\nor the value to ask\nat this point fine.\nNow that we have built\nour tree next\nwhat we'll do we'll Define\na print underscore tree function\nwhich will be used\nto print the tree fine.\nSo finally what we are doing\nin this particular function\nthat we are printing our tree\nnext is the classify function\nwhich will use it to decide\nwhether to follow the true\nBranch or the false branch\nand then compared\nto the feature values stored\nin the node to the example.\nWe are considering and last\nwhat we'll do we'll finally\nprint the production at Leaf.\nSo let's execute\nit and see okay,\nso there's our testing data.\nAll right.\nSo we printed all Leaf\nas well now that we\nhave trained our algorithm\nwith our training data set\nnow it's time to test it.\nSo there's our testing data set.\nSo let's finally execute\nit and see what is the result.\nSo this is the result you\nwill get so first question,\nwhich is asked by the algorithm\nis is diameter greater\nthan equal to 3 if it is true,\nthen it will further ask\nif the color is yellow again,\nif it is true,\nthen it will predict mango\nas one and lemon with one.\nAnd in case it is false,\nthen it will just\npredict the mango.\nNow.\nThis was the true part.\nNow next coming\nto diameter is not greater\nthan or equal to 3 then\nin that case it's false\nand what it will do it will just\npredict the grape fine.\nOkay.\nSo this was all\nabout the coding part now,\nlet's conclude this session.\nBut before concluding let me\njust show you one more thing.\nNow, there's a scikit-learn\nalgorithm cheat sheet,\nwhich explains you\nwhich algorithm you should use\nand when all right,\nlet's build in\na decision tree format.\nLet's see how it is built.\nSo first condition it will check\nwhether you have\n50 samples or not.\nIf your samples\nare greater than 50,\nthen we'll move ahead\nif it is less than 50,\nthen you need\nto collect more data\nif you sample\nis greater than 50,\nthen you have to decide\nwhether you want to predict\na category or not.\nIf you want to\npredict a category,\nthen further you will see\nthat whether you\nhave labeled data or not.\nIf you have label data,\nthen that would\nbe a classification\nalgorithm problem.\nIf you don't have\nthe label data,\nthen it would be\na clustering problem.\nNow if you don't want\nto Category then what?\nDo you want to predict\npredict a quantity?\nWell, if you want\nto predict a quantity,\nthen in that case,\nit would be\na regression problem.\nIf you don't want\nto predict a quantity\nand you want to\nkeep looking further,\nthen in that case,\nyou should go for dimensionality\nreduction problems and still\nif you don't want to look\nand the predicting structure\nis not working.\nThen you have\ntough luck for that.\nI hope this doesn't recession\nclarifies all your doubt\nover decision tree algorithm.\nLet's begin this tutorial\nby looking at the topics\nthat we'll be covering today.\nSo first of all,\nwe'll start Away by getting\na brief introduction\nof random forest\nand then we'll go\nas to see why we actually need\nrandom Forest right?\nWhy not anything else\nbut actually random Forest.\nSo once we understand\nit's need at first place,\nthen we'll go on to learn more\nabout what is random forest\nand we'll also look at various.\nExamples of random Forest\nso that we get a very\nclear understanding of it.\nSo for the will\nalso delve inside\nin to understand the working\nof random Forest as to\nhow exactly random Forest Works\nwill also watch out\nthe random Forest\nalgorithm step by step,\nright so that you are able\nto write any piece\nof code any domain specific\nalgorithm on your own now,\nI personally believe\nthat any learning is\nreally incomplete.\nIf it's not put into application\nso for its completion will also\nImplement random forest in r\nwith a very simple use case\nthat is diabetes prevention.\nSo let's get started\nwith the introduction then.\nNo, random Forest is actually\none of the classifiers\nwhich is used for solving\nclassification problems.\nNow since some of you\nmight not be really aware\nof what classification is.\nSo let's quickly understand\nclassification first,\nand then we'll try to related\nto the random Forest.\nSo basically classification is\na machine learning technique\nin which you already\nhave predefined categories\nunder which you can\nclassify your data.\nSo it's nothing but to\nsupervised learning model\nwhere you already have a data\nbased on which you can train\nyour machine, right?\nSo your machine actually\nlearns from this data.\nSo whatever all\nthat predefined data\nthat you already have it\nactually works as a fuel\nfor your machine, right?\nSo let's say for\nan example ever wondered\nhow your Gmail gets to know\nabout the spam emails\nand filters it out\nfrom the rest of the genuine\nemails any guesses.\nAll right.\nI'll give you a hint try\nto think something on the line\nthat what would it actually\nlook for what can be\nthe possible parameters based\non which you can decide or read.\nThis is a genuine email\nor this is a spam email.\nSo there are certain parameters\nthat your classifier\nwill actually look\nfor like The subject line\nor the text or the HTML tags\nand also the IP address\nof the source from where\nis this mail getting\nfrom so it will analyze\nall these variables\nand then it will classify\nthem into this Pam\nor the genuine folder.\nSo let's say for an example\nif your subject line\nStates like mad\nor cute or pretty\nand some other absurd keywords.\nYour classifier is smart enough\nand it's trained\nin such a manner\nthat it will Get to know.\nAll right, this is\na spam email and it\nwill automatically filter it out\nfrom your genuine emails.\nSo that is how you classify\nit works basically,\nso that's pretty much\nabout the classification now,\nlet's move forward and see\nwhat always can be there\nthrough which you can actually\nperform classification.\nSo we have three classifiers\nnamely decision tree\nrandom forest and a base,\nright so speaking briefly\nabout Season 3 at first\nso decision tree actually\nsplits your entire data set\nin this structure of a tree\nand it makes decision\nat every node and hence\ncalled decision tree.\nSo no big bang theory, right?\nSo you have certain data set.\nThere are certain\nnodes at each node.\nIt will for the split\ninto the child nodes\nand at each node.\nIt will make a decision.\nSo final decision will be\nin the form of positive\nand negative, right?\nSo let's say for an example you\nwant to purchase a car, right?\nSo what all will\nbe the parameters?\nLet's say I have a go\nand I want to purchase a car\nand I will keep certain\nparameters in my mind.\nThat would be what\nexactly is my income.\nWhat is my budget?\nWhat is the particular brand\nthat I want to go for?\nWhat is the mileage of the car?\nWhat is the cylinder capacity\nof the car and so on\nand so forth, right?\nSo I'll make\nmy decision based on.\nAll these parameters,\nright and that is how you make\ndecisions and further.\nIf you really want to know more\nabout decision tree as to\nhow it exactly works.\nYou can also check out our\ndecision tree tutorial as well.\nSo let's begin now\nto the random Forest now.\nSo Random Forest\nisn't in simple classifier.\nActually now, let's understand\nwhat this war in symbol means.\nSo in simple methods actually.\nUse multiple machine learning\nalgorithms to obtain\nbetter predictive performance.\nSo particularly talking\nabout random Forest\nSo Random forests uses\nmultiple decision trees\nfor prediction, right?\nSo you are in assembling a lot\nof decision trees to come up\nto your final outcome.\nAs you can also look\nhere in the image\nthat your entire data set\nis actually for the split\ninto three subsets,\nright and each subset for Leads\nto a particular decision tree.\nSo here you have\nthree decision trees\nand each decision tree will lead\nto certain outcome.\nNow what random Forest will do\nis it will compile the results\nfrom all the decision trees\nand then it will lead\nto a final outcome.\nRight?\nSo it's compiled a section of\nall the multiple decision trees.\nThat's all about\nthe random Forest now,\nlet's see what's lies\nthere in a pace, right?\nSo naive Bayes is\nvery famous classifier,\nwhich is made on a very famous\nrule called Bayes theorem.\nYou might have studied\nabout Nee Bayes theorem\nin your 10 standard as well.\nSo let's just see\nwhat Bayes theorem describes.\nSo based on actually\ndescribes the probability\nof an event based on certain\nprior knowledge of conditions\nthat might be related\nto the event, right?\nSo for example,\nif cancer is related\nto age, right,\nso then person's age\ncan be used to more\naccurately assess probability\nof having a cancer\nthan without having\nthe knowledge of age.\nSo if you know the age then it\nwill become handy in addicting\nthe occurrence of cancer\nfor a particular person.\nRight?\nSo the outcome of first event\nhere is actually affecting\nyour final outcome, isn't it?\nYeah.\nSo this is how naive\nBayes classifier actually works.\nSo that was all\nto give an overview\nof Nave Bayes classifier.\nAnd this were pretty much\nabout the types\nof classifiers now,\nwe'll try to find out the answer\nto this particular question as\nto why we need\nrandom Forest fine.\nSo like human beings learn\nfrom the past experiences.\nSo unlike human beings\na computer does not have\nexperiences then how does\nmachine takes decisions?\nWhere does it learn from?\nUm, well a computer\nsystem actually learns\nfrom the data which represents\nsome past experiences\nof an application domain.\nSo now let's see\nhow random Forest helps\nin building up in learning model\nwith a very simple use case\nof credit risk detection.\nNow needless to say\nthat credit card companies\nhave a very nested\ninterest in identifying\nFinancial transactions\nthat are illegitimate\nand criminal in nature.\nAnd also I would like\nto mention this point\nthat according to the Federal\nReserve payment study Americans\nused credit cards to pay\nfor twenty six point\ntwo million purchases in 2012,\nand the estimated loss\ndue to unauthorized transactions\nthat here was us six point\n1 billion dollars now\nin the banking industry\nmeasuring risk is very critical\nbecause the stakes are too high.\nSo the overall goal is\nactually to figure out Out\nwho all can be fraudulent\nbefore too much Financial\ndamage has been done.\nSo for this a credit card\ncompany receives thousands\nof applications for new cards\nand each application\ncontains information\nabout an applicant, right?\nSo so here as you can see\nthat from all those applications\nwhat we can actually\nfigure out is\nthat predictor variables.\nLike what is the marital\nstatus of the person?\nWhat is the gender\nof the person?\nThe age of the person\nand the status\nwhich is actually\nwhether it is a default pair\nor a non-default pair.\nSo default payments are\nbasically when payments\nare not made in time\nand according to the agreement\nsigned by the cardholder.\nSo now that account is actually\nset to be in the default.\nSo you can easily\nfigure out the history\nof the particular card holder\nfrom this then we can also look\nat the time of payment\nwhether he has been\na regular pair or not.\nRegular one, what is\nthe source of income\nfor that particular person?\nAnd so and so forth.\nSo to minimize loss\nthe back actually needs\ncertain decision rule to predict\nwhether to approve\na particular loan of\nthat particular person or not.\nNow here is where the random\nForest actually comes\ninto the picture right now.\nLet's see how random\nForest can actually help us\nin this particular scenario.\nNow, we have taken\nrandomly two parameters.\nOut of all\nthe predictive variables\nthat we saw previously now,\nwe have taken two\npredictor variables here.\nThe first one is the income\nand the second one\nis the H right\nand similarly parallel\nit to decision trees\nhave been implemented\nupon those predicted variables\nand let's first assume the case\nof the income variable, right?\nSo here we have divided\nour income into three categories\nthe first one being the person\nearning over 35,000.\nAnd dollars second\nfrom 15 to 35 thousand dollars\nthe third one running\nin the range of 0 to\n15 thousand dollars.\nNow if a person\nis earning over $35,000,\nwhich is a pretty good\nincome pretty decent.\nSo now we'll check out\nfor the credit history.\nNow the here the probability is\nthat if a person is earning\na good amount then\nthere is very low risk\nthat he won't be able to pay\nback already earning good.\nSo the It is\nthat his application\nof loan will get approved.\nRight?\nSo there is actually low risk\nor moderate risk,\nbut there's no real\nissue of high risk\nas such we can approve\nthe applicants request here.\nNow, let's move on and watch out\nfor the second category\nwhere the person\nis actually earning\nfrom 15 to 35 thousand dollars\nright now here the person may\nor may not pay back.\nSo in such scenarios\nwill look for the credit.\nHistory as to what has been\nhis previous history.\nNow if his previous\nhistory has been bad\nlike he has been a default.\nER in the previous transactions\nwill definitely not consider\napproving his request\nand he will be at the high risk\nin which is not good\nfor the bank.\nIf the previous history\nof that particular applicant\nis really good then we\nwill just to clarify our doubt\nwill consider another pair.\nDress.\nWell, that will be on depth.\nI have his already in\nreally high depth then\nthe risks again increases\nand there are chances\nthat he might not pay\nrepay in the future.\nSo here will not accept\nthe request of the person\nhaving high dipped\nif the person is\nin the low depth\nand he has been a good pair\nin his past history.\nThen there are chances\nthat he might be back\nand we can consider\napproving the request\nof this particular applicant.\nAnd let's look\nat the third category,\nwhich is a person earning\nfrom 0 to 15 thousand dollars.\nNow, this is something\nwhich actually raises I broke\nand this person\nwill actually lie\nin the category of high risk.\nAll right.\nSo the probability is\nthat his application of loan\nwould probably get rejected now,\nwe'll get one final outcome from\nthis income parameter, right?\nNow let us look\nat our second variable\nthat is age which will lead\ninto the second decision tree.\nNow.\nLet us say\nif the person is Young, right?\nSo now we will look forward to\nif it is a student now\nif it is a student then\nthe chances are high\nthat he won't be\nable to repay back\nbecause he has\nno learning Source, right?\nSo here the risks are too high\nand probability is\nthat his application\nof loan will get rejected fine.\nNow if the person is Young\nAnd he's not a student\nthen we'll probably go on\nand look for another variable.\nThat is pan balance.\nNow.\nLet's look if the bank balance\nis less than 5 lakhs.\nSo again the risk arises\nand the probabilities\nthat his application\nof loan will get rejected.\nNow if the person\nis Young is not a student\nand his bank balance\nof greater than 5 lakhs\nis got a pretty good\nand stable and balanced\nthen the probabilities\nthat his zone of application\nwill get approved.\nOf not let us\ntake another scenario\nif he's a senior, right?\nSo if he is a senior\nwill probably go and check out\nfor this credit history.\nHow well has he been\nin his previous transactions?\nWhat kind of a person he is like\nwhether he's a defaulter or is\nAnanda falter now\nif he is a very\nfair kind of person\nin his previous transactions\nthen again the risk arises\nand the probability\nof his application\ngetting rejected actually\nincreases right now.\nIf he has been\nan excellent person as\nper his transactions\nin the previous history.\nSo now again here\nthere is least risk\nand the probabilities\nthat his application\nof loan will get approved.\nSo now here these two variables\nincome and age have led\nto two different decision trees.\nRight and these two different\ndecision trees actually led\nto two different results.\nNow what random forest does is\nit will actually compile\nthese two different results\nfrom these two different.\nDecision trees and then finally,\nit will lead\nto a final outcome.\nThat is how random\nForest actually works.\nRight?\nSo that is actually the motive\nof the random Forest.\nNow let us move forward and see\nwhat is random Forest right?\nYou can get an idea\nof the mechanism from the name\nitself random forests.\nSo a collection\nof trees is a fortress\nthat's why I called\nfor is probably and here\nalso the trees are actually\nbecause being trained on subsets\nwhich are being\nselected at random.\nAnd therefore they\nare called random forests.\nSo a random forests\nis a collection\nor an in symbol of decision.\nEat straight head\na decision trees actually\nbuilt using the whole data\nset considering all features,\nbut actually in random Forest\nonly a fraction of the number\nof rows is selected\nand that too at random and\na particular number of features,\nwhich are actually selected\nat random are trained\nupon and that is\nhow the decision trees\nare built upon.\nRight?\nSo similarly number\nof decision trees will be grown\nand each decision tree\nwill result in two.\nWith a certain final outcome\nand random Forest\nwill do nothing,\nbut actually just\ncompiled the results\nof all those decision trees\nto bring up the final result.\nAs you can see\nin this particular figure\nthat a particular instance\nactually has resulted\ninto three different\ndecision trees right sonar tree\none results into a final\noutcome called Class A\nand tree to results\ninto class B. Similarly tree\nthree results into class P\nSo Random Forest\nwill compile the results\nof all these decision trees.\nAnd it will go by the goal\nof the majority voting now\nsince head to decision trees\nhave actually voted\ninto the favor of the Class B\nthat is decision tree two,\nand three therefore\nthe final outcome will be\nin the favor of the Class B.\nAnd that is how random\nForest actually works upon.\nNow one really\nbeautiful thing about\nthis particular algorithm is\nthat it is one\nof the versatile algorithms\nwhich is capable\nof Performing both regression\nas well as classification.\nNow, let's try to understand\nrandom Forest further\nwith a very beautiful example\nor a this is my favorite one.\nSo let's say you want to decide\nif you want to watch edge\nof tomorrow or not, right?\nSo in this particular scenario,\nyou will have two different\nactions to work Bond either.\nYou can just straight away go\nto your best friend asked\nhim about or read.\nWhether should I go\nfor Edge of Tomorrow?\nAnd what will I like this movie\nor you can ask a bunch?\nYour friends and take\ntheir opinion consideration\nand then based\non the final results.\nYou can go out and watch Edge\nof Tomorrow, right?\nSo now let's just take\nthe first scenario.\nSo where you go\nto your best friend asked about\nwhether you should go\nout to watch edge\nof tomorrow or not.\nSo your friend will probably\nask you certain questions\nlike the first one\nbeing here Jonah.\nSo so let's say\nyour friend asks you\nif you really like\nThe Adventurous kind\nof movies or not.\nSo you say yes,\ndefinitely I would love to watch\nit Venture kind of movie.\nSo the probabilities\nthat you will like edge\nof tomorrow as well.\nSince it's of Tomorrow is\nalso a movie of Adventure\nand sci-fi kind of Jonah, right?\nSo let's say you do not like\nthe adventure John a movie.\nSo then again\nthe probability reduces\nthat you might really\nnot like edge of Morrow right.\nSo from here you can come\nto a certain conclusion right?\nLet's say your best friend puts\nyou into another situation\nwhere he'll ask you\nor a do you like Emily plant?\nAnd you see definitely\nI like Emily Blunt\nand then he puts\nanother question to you.\nDo you like Emily Blunt\nto be in the main lead\nand you say yes, then again,\nthe probability arises\nthat you will definitely\nlike edge of tomorrow as\nwell because Edge of Tomorrow\nis Has the Emily plant\nin the main lead cast so\nand if you say oh I do not like\nEmily Blunt then again,\nthe probability reduces\nthat you would like Edge\nof Tomorrow to write.\nSo this is one way\nwhere you have one decision tree\nand your final outcome.\nYour final decision will be\nbased on your one decision tree,\nor you can see your final\noutcome will be based\non just one friend.\nNo, definitely not\nreally convinced.\nYou want to consider the options\nof your other friends also\nso that you can make\nvery precise and crisp\ndecision right you go out\nand you approach some other\nbunch of friends of yours.\nSo now let's say you go\nto three of your friends\nand you ask them\nthe same question\nwhether I would like to watch\nAge of Tomorrow or not.\nSo you go out and approach\nthree or four friends friend\none friend twin friend three.\nNow, you will consider\neach of their Sport\nand then you will your decision\nnow will be dependent\non the compiled results of all\nof your three friends, right?\nNow here, let's say you go\nto your first friend\nand you ask him whether you\nwould like to watch it\nif tomorrow not and your first\nfriend puts you to one question.\nDid you like Top Gun?\nAnd you say yes,\ndefinitely I did like\nthe movie Top Gun\nand the probabilities\nthat you would like\nedge of tomorrow as\nwell because topgun is actually\na military action drama,\nwhich is also Tom Cruise.\nSo now again the probability\nRises that yes,\nyou will like edge\nof tomorrow as well and\nIf you say no I didn't like\nTop Gun then again.\nThe chances are\nthat you wouldn't like Edge\nof Tomorrow, right?\nAnd then another question\nthat he puts you across is\nthat do you really like\nto watch action movies?\nAnd you say yes,\nI would love to watch them.\nThen again.\nThe chances are\nthat you would like\nto watch Edge of Tomorrow.\nSo from your friend\nwhen you can come\nto one conclusion,\nI hear since the ratio\nof liking the movie\nto don't like is actually 2\nis to 1 so the final result.\nActually, you would like\nEdge of Tomorrow.\nNow you go to your second friend\nand you ask the same question.\nSo now you are second friend\nasks you did you like far\nand away when we went\nout and did the last time\nwhen we washed it\nand you say no I really\ndidn't like far and away\nthen you would say then\nyou are definitely going\nto like Edge of Tomorrow.\nWhy does so because far\nand away is actually\nsince most of whom\nmight not be knowing it so far\nin a ways Johner of romance\nand it revolves around a girl\nand a guy Guy falling in love\nwith each other and so on.\nSo the probability is\nthat you wouldn't like\nedge of tomorrow.\nSo he ask you another question.\nDid you like Bolivian\nand to really like\nto watch Tom Cruise?\nAnd you say Yes, again.\nThe probability is\nthat you would like\nto watch Edge of Tomorrow.\nWhy because Oblivion\nagain is a science fiction\ncasting Tom Cruise full\nof strange experiences.\nAnd where Tom Cruise is\nthe savior of the masses.\nKind well,\nthat is the same kind of plot\nin edge of tomorrow as well.\nSo here it is pure yes\nthat you would like\nto watch edge of tomorrow.\nSo you get\nanother second decision\nfrom your second friend.\nNow you go to your third\nfriend and ask him so\nprobably our third friend is\nnot really interesting\nin having any sort\nof conversation with you say\njust simply asks you did you\nlike Godzilla and you say\nno I didn't like Godzilla's we\nsay definitely you wouldn't like\nEdge of Tomorrow why\nso because Godzilla is\nalso actually Fiction movie\nfrom the adventure Jonah.\nSo now you have got\nthree results from\nthree different decision trees\nfrom three different friends.\nNow you compile the results\nof all those friends\nand then you make\na final call that yes,\nwould you like to watch edge\nof tomorrow or not?\nSo this is some very real time\nand very interesting example\nwhere you can actually\nImplement random Forest\ninto ground reality.\nNow let us look\nat various domains\nwhere random Forest\nis actually used.\nSo because of its diversity\nrandom Forest is actually used\nin various diverse to means\nlike so beat banking beat\nmedicine beat land use\nbeat marketing name it\nand random Forest is there so\nin banking particularly\nrandom Forest is being\nactually used to make it out\nwhether the applicant\nwill be a default a pair\nor it will be Older one\nso that it can accordingly\napprove or reject\nthe applications of loan,\nright?\nSo that is how random Forest\nis being used in banking\ntalking about medicine.\nRandom.\nForest is widely used\nin medicine field\nto predict beforehand.\nWhat is the probability\nif a person will actually have\na particular disease or not?\nRight?\nSo it's actually used to look\nat the various disease Trends.\nLet's say you want to figure\nout what is the probability\nthat a person will have\ndiabetes or not?\nIt and so what would you do?\nIt'd probably look\nat the medical history\nof the patient\nand then you will see.\nAll right.\nThis has been\nthe glucose concentration.\nWhat was the BMI?\nWhat was the insulin levels\nin the patient in the past\nprevious three months.\nWhat is the age\nof this particular person\nand do it'll make a different\ndecision trees based on each one\nof these predictor variables\nand then you'll finally\ncompiled the results\nof all those variables and then\nyou will make a final decision.\nAs to whether the person\nwill have diabetes\nin the near future or not.\nThat is how random\nForest will be used\nin medicine sector now move.\nRandom Forest is also actually\nused to find out the land use.\nFor example, I want to set\nup a particular industry\nin certain area.\nSo what would I probably\nlook for a look for?\nWhat is the\nvegetation over there?\nWhat is the Urban\npopulation over there?\nRight and how much\nis the distance\nfrom the nearest modes\nof Transport like\nfrom the bus station\nor the railway station\nand accordingly.\nI will split my parameters\nand I will make decision\non each one of these parameters\nand finally I'll compile\nmy decision of all\nthese parameters in that\nwill be my final outcome.\nSo that is how I\nam finally going to predict\nwhether I should put my industry\nat this particular\nlocation or not.\nRight?\nSo these three examples\nhave actually been of\nmajorly Classification problem\nbecause we are\ntrying to classify\nwhether or not with actually\ntrying to answer this question\nwhether or not right now,\nlet's move forward and look\nhow marketing is revolving\naround random Forest.\nSo particularly in marketing\nwe try to identify\nthe customer churn.\nSo this is particularly\nthe regression kind\nof problem right now\nhow let's see so customer churn\nis nothing but actually\nthe number of people\nwhich are actually\non the number.\nOf customers who are losing out.\nSo we're going\nout of your market.\nNow you want to identify\nwhat will be your customer churn\nin near future.\nSo you'll most of them\ne-commerce Industries are\nactually using this\nlike Amazon Flipkart Etc.\nSo they particularly look\nat your each Behavior as to\nwhat has been your past history.\nWhat has been\nyour purchasing history.\nWhat do you like\nbased on your activity\naround certain things\naround certain ads\naround certain discounts?\nAnd I'm certain kind\nof materials right\nif you would like a particular\ntop your activity will be more\naround that particular top.\nSo that is how they track each\nand every particular move\nof yours and then\nthey try to predict\nwhether you will be\nmoving out or not.\nSo that is how they identify\nthe customer churn.\nSo these all are various domains\nwhere random Forest is used.\nAnd this is not the only\nlist so there are\nnumerous other examples,\nwhich actually Lee\nare using random forests\nthat makes it\nso special actually.\nNow, let's move\nforward and see how random\nForest actually works.\nRight.\nSo let us start with the random\nForest algorithm first.\nLet's just see it step\nby step as to how random\nForest algorithm works.\nSo the first step is\nto actually select\ncertain M features from T.\nWhere m is less than T.\nSo here T is the total number\nof the predictor variables\nthat you have\nin your data set and out of\nthose total predictor variables.\nYou will select\nsome random Lisa.\nUm few features out of those now\nwhy we are actually selecting\na few features only.\nThe reason is\nthat if you will select all\nthe predictive variables\nor the total predictor variables\nthen each of your decision tree\nwill be same\nso we model is not actually\nlearning something new.\nIt is learning\nthe same previous thing\nbecause all those decision trees\nwill be similar right\nif you actually split\nyour predicted variables\nand you select randomly\na few predicted variables.\nNeed let's say there are\n14 total number of variables\nand out of those\nwho randomly pick\njust three right?\nSo every time you will get\na new decision tree,\nso there will be\na variety right?\nSo the classification model\nwill be actually\nmuch more intelligent\nthan the previous one.\nNow.\nIt has got very yet experiences.\nSo definitely it will make\ndifferent decisions each time.\nAnd then when you will compile\nall those different decisions,\nit will be a new more.\nAre accurate and\nefficient result, right?\nSo the first important step\nis to select certain number\nof features out of all\nthe features now,\nlet's move on to\nthe second step.\nLet's say for any node D. Now.\nThe first step is to calculate\nthe best plate at that point.\nSo, you know that decision tree\nhow decision trees\nactually implemented so\nyou pick up a the most\nsignificant variable right?\nAnd then you will split\nthat particular node.\nFor the child nodes,\nthat is how the split\ntakes place, right?\nSo you will do it\nfor M number of variables\nthat you've selected.\nLet's say you\nhave selected three\nso you will implement\nthe split at all.\nThose three nodes\nin one particular decision tree,\nright the third step\nis split up the node\ninto two daughter nodes.\nSo now you can split\nyour root note\ninto as many notes\nas you want to but here\nwe'll split our node\ninto 2.2 notes as to this or\nthat so it will be an answer.\nIn terms of this or that right\nat fourth step will be\nto repeat all these three steps\nthat we've done previously\nand we'll repeat\nall this splitting\nuntil we have reached all\nthe N number of nodes, right?\nSo we need to repeat\nuntil we have reached\ntill the leaf nodes\nof a decision tree that is\nhow we will do it right now\nafter these four steps.\nWe will have\nour one decision tree.\nBut random Forest is actually\nabout Decision trees.\nSo here our fifth step\nwill come into the picture\nwhich will actually repeat\nall these previous steps\nfor D number of times now\nhit these the the number\nof decision trees.\nLet's say I want to implement\nfive decision trees.\nSo my first step\nwill be to implement\nall the previous steps 5 times.\nSo the head the eye tration is\n4/5 number of times right now.\nOnce I have created\nthese five decision trees still\nmy task is not completed.\nPleat yet.\nNow.\nMy final task will be\nto compile the results\nof all these five\ndifferent decision trees\nand I will make a call\nin the majority\nvoting right here.\nAs you can see in this picture.\nI had in different instances.\nThen I created\nindifferent decision trees.\nAnd finally,\nI will compile the result of all\nthese n different decision trees\nand I will take my call\non the majority voting right.\nSo whatever my majority vote\nsays It will be my final result.\nSo this is basically an overview\nof the random Forest algorithm\nhow it actually works.\nLet's just have a look\nat this example to get\nmuch better understanding\nof what we have learnt.\nSo let's say I have\nthis data set\nwhich consists of four\ndifferent instances, right?\nSo basically it consists\nof the weather information\nof previous 14 days right\nfrom D1 tildy 14,\nand this basically\nOutlook humidity and Win,\nthis basically gives me\nthe weather condition\nof those 14 days.\nAnd finally I have play\nwhich is my target variable\nweather match did take place\non that particular day\nor not right.\nNow.\nMy main goal is to find out\nwhether the match\nwill actually take place\nif I have following\nthese weather conditions\nwith me on any particular day.\nLet's say the Outlook\nis rainy that day\nand humidity is high\nand the wind is very weak.\nSo now I need to predict\nwhether I will be able\nto play The match\nthat they are not all right.\nSo this is\na problem statement fine.\nNow, let's see\nhow random Forest is used\nin this to sort it out now here\nthe first step is to actually\nsplit my entire data set\ninto subsets here.\nI have split my entire\n14 variables into further\nsmaller subsets right\nnow these subsets may\nor may not overlap\nlike there is certain\noverlapping between d 1\ntill D3 and D3 till D6.\nFine, so there is\nan overlapping of D3.\nSo it might happen\nthat there might be overlapping\nso you need not really worry\nabout the overlapping\nbut you have to make sure\nthat all those subsets are\nactually different right?\nSo here I have taken\nthree different subsets\nmy first sub set consists of D1\ntill D3 Mexican subset\nconsists of D3\ntill D6 and methods subset\nconsists of D7 tildy.\nNow now I will first be focusing\non my first subset now here,\nlet's say that\nparticular day the out\nIt was overcast fine.\nIf yes, it was overcast\nthen the probabilities\nthat the match will take place.\nSo overcast is basically\nwhen your weather is too cloudy.\nSo if that is the condition then\ndefinitely the match will take\nplace and let's say\nit wasn't overcast.\nThen you will consider these\nsecond most probable option\nthat will be the wind\nand we will make a decision\nbased on this now\nwhether wind was weak or strong\nif wind was weak,\nthen you will definitely go out.\nAnd play the match\nelse you would not.\nSo now the final outcome out of\nthis decision tree will be Play\nBecause here the ratio\nbetween the play\nand no play is to is to 1\nso we get to a certain decision\nfrom a first decision tree.\nNow, let us look\nat the second subset now\nsince second subset has\ndifferent number of variables.\nSo that is why this decision\ntrees absolutely different from\nwhat we saw in our four subsets.\nSo let's say if it was overcast\nthen you will play the match.\nIf it isn't the overcast\nand you would go and look out\nfor humidity now further,\nit will get split into two\nwhether it was high or normal.\nNow, we'll take the first case\nif the humidity was high\nand wind was week.\nThen you will play\nthe match else\nif humidity was high\nbut wind was too strong,\nthen you would not go out\nand play the match right now.\nLet us look at the second dot\nto node of humidity\nif the humidity was Oil\nand the wind was weak then\nyou will definitely go out\nand play the match\nas you want go out\nand play the match.\nSo here if you look\nat the final result,\nthen the ratio of placed no play\nis 3 is to 2 then again.\nThe final outcome\nis actually play, right?\nSo from second subset,\nwe get the final\ndecision of play now,\nlet us look at our third subset\nwhich consists of D7\ntill D9 here\nif again the overcast is yes,\nthen you will A match\nit's you will go\nand check out for humidity.\nAnd if the humidity\nis really high then you\nwon't play the match\nand you will play the match\nagain the probability\nof playing the matches.\nYes, because the ratio\nof no play is Twist one, right?\nSo three different subsets\nthree different decision trees\nthree different outcomes\nand one final outcome\nafter compiling all the results\nfrom these three different\ndecision trees are so I hope\nthis gives a better perspective\na bit understanding\nof random Forest like\nhow it really works.\nAll right.\nSo now let's just have a look\nat various features\nof random Forest Ray.\nSo the first\nand the foremost feature is\nthat it is one\nof the most accurate\nlearning algorithms, right?\nSo why it is so\nbecause single decision trees\nare actually prone\nto having high variance\nor Hive bias and on\nthe contrary actually.\nRandom Forest it averages\nthe entire variance\nacross the decision trees.\nSo let's say\nif the variances say\nX4 decision tree,\nbut for random Forest,\nlet's say we have\nimplemented n number\nof decision trees parallely.\nSo my entire variance\ngets averaged to upon\nand my final variance\nactually becomes X upon n so\nthat is how the entire variance\nactually goes down\nas compared to other algorithms.\nThumbs right now second most\nimportant feature is\nthat it works?\nWell for both classification\nand regression problems\nand by far I have come\nacross this is one\nand the only algorithm\nwhich works equally well\nfor both of them.\nBeh classification kind\nof problem or a regression kind\nof problem, right?\nThen it's really runs efficient\non large databases.\nSo basically it's\nreally scalable.\nEven if you work for\nthe lesser amount of database\nor if you work for really\nhuge volume of data, right?\nSo that's a very\ngood part about it.\nThen the fourth most\nimportant point is\nthat it requires almost\nno input preparation.\nNow, why am I saying this is\nbecause it has got\ncertain implicit methods,\nwhich actually take care.\nAnd remove all the outliers\nand all the missing data and you\nreally don't have to take care\nabout all that thing\nwhile you are in the stages\nof input preparations.\nSo Random Forest is\nall here to take care\nof everything else and next.\nIs it performs implicit\nfeature selection, right?\nSo while we are implementing\nmultiple decision trees,\nso it has got implicit method\nwhich will automatically\npick up some random features.\nResult of all your parameters\nand then it will go\non and implementing\ndifferent decision trees.\nSo for example,\nif you just give\none simple command\nthat all right,\nI want to implement\n500 decision trees no matter\nhow so Random Forest\nwill automatically take care\nand it will Implement all\nthose 500 decision trees\nand those all 500 decision trees\nwill be different\nfrom each other and this is\nbecause it has\ngot implicit methods\nwhich will automatically\ncollect different parameters.\nHas itself out of\nall the variables\nthat you have right,\nthen it can be easily grown\nin parallel why it is so\nbecause we are actually\nimplementing multiple\ndecision trees and all\nthose decision trees are running\nor all those decisions\ntrees are actually\ngetting implemented parallely.\nSo if you say I want thousand\ntrees to be implemented.\nSo all those thousand trees are\ngetting implemented parallely.\nSo that is how the\ncomputation time reduces.\nRight, and the last point is\nthat it has got methods\nfor balancing error\nin unbalanced it\nas it's now what exactly\nunbalanced data sets\nare let me just give\nyou an example of that.\nSo let's say you're working\non a data set fine\nand you create a random\nforest model and get\n90% accuracy immediately.\nFantastic you think right.\nSo now you start diving deep\nyou go a little Little deeper\nand you discovered\nthat ninety percent\nof that data actually belongs\nto just one class tan\nyour entire data set\nyour entire decision\nis actually biased\nto just one particular class.\nSo Random Forest actually\ntakes care of this thing\nand it is really not biased\ntowards any particular decision\ntree or any particular variable\nor any class.\nSo it has got methods\nwhich looks after it\nand they Does all the balance\nof errors in your data sets?\nSo that's pretty much\nabout the features\nof random forests.\nK-nearest neighbor is\na simple algorithm\nwhich uses entire data set\nin its training phase\nwhen our prediction\nis required for unseen data.\nWhat it does is it searches\nthrough the entire training data\nset for kaymu similar instances\nand the data with the most\nsimilar instance is finally\nreturned as the prediction.\nSo hello.\nOh and welcome all\nto this YouTube session\nand in today's session will\nbe dealing with KNN algorithm.\nSo without doing any further,\nlet's move on and discuss agenda\nfor today's session.\nSo we'll start our session\nwith what is KN\nwhere I'll brief you\nabout the topic\nand we'll move ahead to see\nwhat its popular use cases\nor how the industry is using KN\nfor their benefit.\nOnce we are done with it.\nWe will drill down\nto the working of algorithm\nand while learning the algorithm\nyou will also understand\nthe significance of K,\nor what does this case stands\nfor in the nearest\nneighbor algorithm?\nThen we'll see\nhow the prediction is made\nusing Canon algorithm\nmanually or mathematically.\nAll right.\nNow once we are done\nwith the theoretical concept\nwill start the Practical\nor the demo session\nwhere we'll learn\nhow to implement KNN\nalgorithm using python.\nSo let's start our session.\nSo starting with what\nis KNN algorithm will k-nearest\nneighbor is a simple algorithm\nthat stores all\nthe available cases\nand classify the new data\nor case based\non a similarity measure.\nIt suggests that\nif you are similar\nto your neighbors,\nthen you have one\nof them right for example,\nif apple looks more similar\nto banana orange\nor Melon rather than\na monkey rat or a cat\nthat most likely Apple belong\nto the group of fruits.\nAll right.\nWell in general Cayenne is used\nin Search application\nwhere you are looking\nfor similar items\nthat is when your task is\nsome form of fine items\nsimilar to this one.\nThen you call this search\nas a Cayenne in search.\nBut what is this KN KN?\nWell this K denotes the number\nof nearest neighbor\nwhich are voting class\nof the new data\nor the testing data.\nFor example,\nif k equal 1 then the Sting data\nare given the same label\nas a close this example\nin the training set similarly.\nIf k equal 3 the labels are the\nthree closes classes are checked\nand the most common label is\nassigned to then testing data.\nSo this is\nwhat a KN KN algorithm means\nso moving on ahead.\nLet's see some\nof the example of scenarios\nwhere KN is used\nin the industry.\nSo, let's see\nthe industrial application\nof KNN algorithm starting\nwith recommender system.\nWell the biggest use case\nof cayenne and search\nis a recommender system.\nThus recommended system is\nlike an automated.\nGood form of a shop counter guy\nwhen you asked him for a product\nnot only shows you the product\nbut also suggest you or displays\nyour relevant set of products,\nwhich are related to the item.\nYou're already interested\nin buying this KNN algorithm\napplies to recommending\nproducts like an Amazon\nor for recommending media,\nlike in case of Netflix or even\nfor recommending advertisement\nto display to a user\nif I'm not wrong almost all\nof you must have used Amazon\nfor shopping, right?\nSo just to tell you\nmore than 35% of\namazon.com revenue is generated\nby its recommendation engine.\nSo what's the strategy Amazon\nuses recommendation as\na targeted marketing tool\nin both the email\ncampaigns around most\nof its website Pages Amazon will\nrecommend many products\nfrom different categories based\non what you have browser\nand it will pull those products\nin front of you\nwhich you are likely to buy\nlike the frequently\nbought together option\nthat comes at the bottom\nof the product page to tempt you\ninto buying the combo.\nWell, this recommendation\nhas just one main goal\nthat is increase average\norder value or to upsell\nand cross-sell customers by\nproviding product suggestions.\nEastern items in the shopping\ncart or based on the product.\nThey're currently\nlooking at on site.\nSo next industrial\napplication of KNN\nalgorithm is concept search\nor searching semantically\nsimilar documents\nand classifying documents\ncontaining similar topics.\nSo as you know,\nthe data on the Internet\nis increasing exponentially\nevery single second.\nThere are billions and billions\nof documents on the internet\neach document on the internet\ncontains multiple Concepts,\nthat could be\na potential concept.\nNow, this is a situation\nwhere the main problem is\nto Extract concept\nfrom a set of documents\nas each page could have\nthousands of combination\nthat could be potential Concepts\nan average document could have\nmillions of concept combined\nthat the vast amount\nof data on the web.\nWell, we are talking\nabout an enormous amount\nof data set and Sample.\nSo what we need is we need\nto find a concept\nfrom the enormous amount\nof data set and samples, right?\nSo for this purpose,\nwe will be using KNN\nalgorithm more advanced example\ncould include handwriting\ndetection like an OCR\nor image recognization\nor even video.\nOrganization.\nAll right.\nSo now that you know\nvarious use cases\nof KNN algorithm.\nLet's proceed and see\nhow does it work.\nSo how does\na KNN algorithm work?\nLet's start by plotting\nthese blue and orange\npoint on our graph.\nSo these Blue Points\nthe belong to class A\nand the orange ones\nthey belong to class B.\nNow you get a star as a new pony\nand your task is to predict\nwhether this new point\nit belongs to class A\nor it belongs to the class B.\nSo to start the production\nthe very first thing\nthat you have to do is\nselect the Value of K. Just\nas I told you KN KN\nalgorithm refers to the number\nof nearest neighbors\nthat you want to select.\nFor example, in\nthis case k equal to 3.\nSo what does it mean it means\nthat I am selecting three points\nwhich are the least distance\nto the new point\nor you can say I am selecting\nthree different points\nwhich are closest to the star.\nWell at this point\nof time you can ask\nhow will you calculate\nthe least distance?\nSo once you\ncalculate the distance,\nyou will get one blue\nand two orange points which\nare closest to this star now.\nSince in this case\nas we have a majority\nof orange points,\nso you can say that for k equal\n3D star belongs to class B,\nor you can say\nthat the star is more similar\nto the orange points\nmoving on ahead.\nWell, what if k equal\nto 6 well for this case,\nyou have to look\nfor six different points\nwhich are closest to this star.\nSo in this case\nafter calculating the distance,\nwe find that we have\nfour blue points\nand two Orange Point\nwhich are closest\nto the star now\nas you can see that the blue\npoints are in majority,\nso you Can say\nthat for k equals\n6 this star belongs to class A\nor the star is more similar\nto Blue Points.\nSo by now,\nI guess you know\nhow a KNN algorithm work\nand what is the significance\nof gain KNN algorithm.\nSo how will you\nchoose the value of K?\nSo keeping in mind this case\nthe most important parameter\nin KNN algorithm.\nSo, let's see when you build\na k nearest neighbor classifier.\nHow will you choose\na value of K?\nWell, you might have\na specific value of K in mind\nor you could divide up\nyour data and use something\nlike cross-validation technique\nto test several values\nof K in order.\nTo determine which works best\nfor your data, for example,\nif n equal 2,000 cases then\nin that case the optimal value\nof K lies somewhere\nin between 1 to 19.\nBut yes,\nunless you try it you\ncannot be sure of it.\nSo, you know how the algorithm\nis working on a higher level.\nLet's move on and see\nhow things are predicted\nusing KNN algorithm.\nRemember I told you\nthe KNN algorithm uses\nthe least distance measure\nin order to find\nits nearest neighbors.\nSo, let's see how\nthese distance is calculated.\nWell, there are\nseveral distance measure\nwhich can be used.\nSo to start with Will mainly\nfocus on euclidean distance\nand Manhattan distance\nin this session.\nSo what is\nthis euclidean distance?\nWell, this euclidean distance\nis defined as the square root\nof the sum of difference\nbetween a new point x\nand an existing Point why\nso for example here we\nhave Point P1 and P2 Point\nT. 1 is 1 1 and point p 2 is 5\nfor so what is the euclidean\ndistance between both of them?\nSo you can see\nthat euclidean distance is\na direct distance\nbetween two points.\nSo what is the distance\nbetween the point P1 and P2\nso we can calculate it\nas 5 minus 1 whole square\nplus 4 minus 1 whole square\nand we can route it\nover which results to 5.\nSo next is\nthe Manhattan distance.\nWell, this Manhattan distance is\nused to calculate the distance\nbetween real Vector\nusing this some\nof their absolute difference\nin this case\nthe Manhattan distance\nbetween the point P1 and P2\nis Mode of 5 minus 1\nplus mod value of 4 minus 1\nwhich results to 3 plus 4\nthat is 7 so this slide\nshows the difference\nbetween euclidean and Manhattan\ndistance from point A to point\nB.\nSo euclidean distance is\nnothing but the direct\nor the least possible distance\nbetween A and B.\nWhereas the Manhattan distance\nis a distance between A\nand B measured along the axis\nat right angle.\nLet's take an example and see\nhow things are predicted\nusing KNN algorithm\nor how the cannon\nalgorithm is working.\nSuppose we have a data set\nwhich consists of height weight\nand T-shirt size\nof some customers.\nNow when a new customer\ncome we only have his height\nand weight as the information\nnow our task is to predict.\nWhat is the T-shirt size\nof that particular customer\nso for this will be using\nthe KNN algorithm.\nSo the very first thing\nwhat we need to do,\nwe need to calculate\nthe euclidean distance.\nSo now that you have a new data\nof height 160 one centimeter\nand weight are 61 kg.\nSo the very first thing\nthat we'll do is we'll calculate\nthe euclidean distance.\nStance which is nothing\nbut the square root\nof 160 1 minus 158 whole square\nplus 61 minus 58 whole square\nand square root of that is 4.24.\nLet's drag and drop it.\nSo these are the various\neuclidean distance\nof other points.\nNow, let's suppose k equal\nto 5 then the algorithm\nwhat it does is it searches\nfor the five customer\nclosest to the new customer\nthat is most similar\nto the new data in terms\nof its attribute for k equal 5.\nLet's find the top five\nminimum euclidian distance.\nSo these are the distance\nwhich we are going to use\nTwo three four and five.\nSo let's rank them\nin the order first.\nThis is second.\nThis is third then\nthis one is for again.\nThis one is 5 so\nthere is our order.\nSo for k equal 5 we\nhave for t-shirts\nwhich commanders size\nM and one t-shirt\nwhich comes under size l\nso obviously best guess\nfor the best protection\nfor the T-shirt size\nof height 160 one centimeter\nand wait 60 1 kg is M.\nOr you can say that a new\ncustomer Fittin to size\nM. Well this was all\nabout Body theoretical session,\nbut before we drill down\nto the coding part,\nlet me just tell you why people\ncall KN as a lazy learner.\nWell Cannon for classification\nis a very simple algorithm,\nbut that's not why they are\ncalled lazy KN is a lazy learner\nbecause it doesn't have\na discriminative function\nfrom the training data.\nBut what it does it\nmemorizes the training data,\nthere is no learning phase\nof the model and all\nof the work happens at the time.\nYour prediction is requested.\nSo as such there's the reason\nwhy KN is often referred\nto us lazy learning algorithm.\nSo this was all about\nOr detail reticle session now,\nlet's move on the coding part.\nSo for the Practical\nimplementation of\nthe Hands-On part,\nI'll be using the IRS data set.\nSo this data set consists\nof 150 observation.\nWe have four features\nand one class label\nthe four features include\nthe sepal length sepal\nwidth petal length\nand the petrol head\nwhereas the class label\ndecides which flower belongs\nto which category.\nSo this was the description\nof the data set,\nwhich we are using now,\nlet's move on and see\nwhat are the step\nby step solution\nto perform a KNN algorithm.\nSo first we'll start\nby handling the The data\nwhat we have to do we\nhave to open the data set\nfrom the CSV format\nand split the data set\ninto train and test part next\nwe'll take the similarity\nwhere we have to\ncalculate the distance\nbetween two data instances.\nOnce we calculate\nthe distance next.\nWe'll look for the neighbor\nand select K Neighbors\nwhich are having the least\ndistance from a new point.\nNow once we get our neighbor,\nthen we'll generate a response\nfrom a set of data instances.\nSo this will decide\nwhether the new Point belongs\nto class A or Class B.\nFinally, we'll create\nthe accuracy function\nand in the end.\nWe'll tie it all together\nin the main function.\nSo let's start with our code\nfor implementing KNN\nalgorithm using python.\nI'll be using jupyter notebook\npython 3.0 installed on it.\nNow, let's move on and see\nhow can an algorithm\ncan be implemented using python.\nSo there's my jupyter notebook,\nwhich is a web-based interactive\nComputing notebook environment\nwith python 3.0 installed on it\nso that the launched\nits launching so there's\nour jupyter notebook\nand we'll be riding\nour python codes on it.\nSo the first thing\nthat we need to do\nis load our file,\nour data is in CSV format\nwithout a header line\nor any code we can open\nthe file the open function\nand read the data line\nusing the reader function\nin the CSV module.\nSo let's write a code\nto load our data file.\nLet's execute the Run button.\nSo once you execute\nthe Run button,\nyou can see the entire training\ndata set as the output next.\nWe need to split the data\ninto a training data set\nthat KN can use to make\nprediction and a test data set\nthat we can use to evaluate\nthe accuracy of The module\nso we first need to convert\nthe flower measure\nthat were loaded as\nstring into numbers\nthat we can work.\nNext.\nWe need to split the data set\nrandomly to train and test ratio\nof 67 is 233 for test is\nto train as a standard ratio,\nwhich is used for this purpose.\nSo let's define a function\nas load data set\nthat loads a CSV\nwith the provided file named and\nsplit it randomly into training\nand test data set using\nthe provided split ratio.\nSo this is our function\nload data set\nwhich is using filenames\nthat ratio training data set\nand testing data set.\nAs its input.\nAll right.\nSo let's execute the Run button\nand check for any errors.\nSo it's executed\nwith zero errors.\nLet's test this function.\nSo there's our training\nset testing set load data set.\nSo this is our function\nload data set on inside\nthat we are passing.\nOur file is data\nwith a split ratio of 0.66\nand training data set\nand test data set.\nLet's see what\nour training data set\nand test data set its dividing\ninto so it's giving a count\nof training data set\nand testing data set.\nThe total number\nof training data set\nas split into is\n97 and total number\nof Test data set we have is 53.\nSo total number of training data\nset we have here is 97 and total\nnumber of test data\nset we have here is 53.\nAll right.\nOkay.\nSo our function load\ndata set is performing.\nWell, so let's move\non to step two\nwhich is similarity.\nSo in order to make prediction,\nwe need to calculate\nthe similarity between\nany two given data instances.\nThis is needed\nso that we can locate the kamo\nsimilar data instances\nin the training data set are\nin turn make a prediction given\nthat all for flour measurement\nare numeric and have same unit.\nWe can directly use\nthe euclidean distance measure.\nThis is nothing\nbut the square root of the sum\nof squared differences\nbetween two eras\nof the number given\nthat all the for flower\nmeasurements are numeric\nand have same unit.\nWe can directly use\nthe euclidean distance measure\nwhich is nothing\nbut the square root of the sum\nof squared difference\nbetween two arrays\nor the number additionally\nwe want to control\nwhich field to include\nin the distance calculation.\nSo specifically we only want\nto include first for attribute.\nSo our approach will be\nto limit the euclidean distance\nto a fixed length.\nAll right.\nSo let's define\nour euclidean function.\nSo these are euclidean\ndistance function\nwhich takes instance\none instance to and length\nas parameters instance one and\ninstance two are the two points\nof which you want to calculate\nthe euclidean distance,\nwhereas this length and denote\nthat how many attributes\nyou want to include.\nOkay.\nSo there's our\neuclidean function.\nLet's execute it.\nIt's executing fine\nwithout any errors.\nLet's test the function suppose\nthe data one or the\nfirst instance consists\nof the data point us to to\nto and it belongs to class A. A\nand data to consist\nof four for four\nand it belongs to class P.\nSo when we calculate\nthe euclidean distance\nof data one to data to and\nwhat we have to do we\nhave to consider only\nfirst three features of them.\nAll right.\nSo let's print the distance\nas you can see here.\nThe distance comes out to be\nthree point four six four.\nAll right.\nSo this is nothing\nbut the square root\nof 4 minus 2 whole Square.\nSo this distance is nothing\nbut the euclidean distance\nand it is calculated as square\nroot of 4 minus 2 whole square\nplus 4 minus 2 whole square\nthat is nothing but 3 times\nor 4 minus 2 whole That is 12 +\nsquare root of 12 is nothing\nbut 3.46 for all right.\nSo now that we have calculated\nthe distance now,\nwe need to look\nfor K nearest neighbors.\nNow that we have a similarity\nmeasure we can use it to collect\nthe kamo similar instances\nfor a given unseen instance.\nWell, this is\na straightforward process\nof calculating the distance\nfor all the instances\nand selecting a subset with\nthe smallest distance value.\nAnd now what we have\nto do we have to select\nthe smallest distance values.\nSo for that will be\ndefining a function\nas get neighbors.\nSo for that\nwhat we will be doing\nwill be defining a function\nas get neighbors\nwhat it will do it will return\nthe K most similar Neighbors\nFrom the training set\nfor a given test instance.\nAll right.\nSo this is how our get\nnabal function look\nlike it takes training data set\nand test instance\nand K as its input here.\nThe K is nothing but the number\nof nearest neighbor\nyou want to check for.\nAll right.\nSo basically what\nyou'll be getting\nfrom this get Mabel's\nfunction is K different points\nhaving least euclidean distance\nfrom the test instance.\nAll right, let's execute it.\nSo the function executed\nwithout any errors.\nSo let's test our function.\nSuppose the training data set\nincludes the data like 2 to 2\nand it belongs to class A\nand other data\nincludes four four four\nand it belongs to class P\nand our testing instances\nfive five five or now.\nWe have to predict\nwhether this test instance\nbelongs to class A\nor it belongs to class be.\nAll right for k equal 1\nwe have to predict\nits nearest neighbor and predict\nwhether this test instance\nit will belong to class A\nor will it belong to class be?\nAll right.\nSo let's execute\nthe Run button aligned.\nSo an executing\nthe Run button you can see\nthat we have output is 4 4 4\nand B. Be a new instance 5 5 5\nis closest to point 4 4 4\nwhich belongs to class be?\nAll right.\nNow once you have located\nthe most similar neighbor\nfor a test instance next task\nis to predict a response based\non those neighbors.\nSo how we can do that.\nWell, we can do this\nby allowing each neighbor\nto vote for the class attribute\nand take the majority vote\nas a prediction.\nLet's see how we can do that.\nSo we have a function\nas getresponse with takes\nneighbors as the input.\nWell, this neighbor was\nnothing but the output\nof this get me / function.\nThe output of get\nneighbor function will be fed\nto get response.\nAll right, let's execute\nthe Run button.\nIt's executed.\nLet's move ahead and test\nour function get response.\nSo we have a neighbor\nas one one one.\nIt belongs to class A to to\nto it belongs to class a33.\nIt belongs to class B.\nSo this response\nwhat it will do it will store\nthe value of get response\nby passing this neighbor value.\nAll right.\nSo what we want to check\nis we want to predict\nwhether that test instance\nfive five five.\nIt belongs to class A\nor Class B. Be\nwhen the neighbors are\n1 1 1 a 2 2 A + 3 3 p.\nSo let's check our response now\nthat we have created all\nthe different function\nwhich are required\nfor a KNN algorithm.\nSo important main concern is\nhow do you evaluate\nthe accuracy of the prediction\nand easy way to evaluate\nthe accuracy of the model\nis to calculate a ratio\nof the total correct prediction\nto all the prediction made.\nSo for this I will\nbe defining function\nas get accuracy and inside\nthat I'll be passing\nmy test data set\nand the predictions\nget accuracy function.\nCheck it.\nExecuted without any error.\nLet's check it\nfor a sample data set.\nSo we have our test\ndata set as 1 1 1\nwhich belongs to class A 2/2\nwhich again belongs to class\n3 3 3 which belongs to class B\nand my predictions is\nfor first test data.\nIt predicted latter belongs\nto class A which is true\nfor next it predicted\nthat belongs to class E,\nwhich is again to and for\nthe next again it predictive\nthat it belongs to class A\nwhich is false in this case\ncause the test data\nbelongs to class be.\nAll right.\nSo in total we have to correct\nprediction out of three.\nAll right.\nRight.\nSo the ratio\nwill be 2 by 3,\nwhich is nothing but 66.6.\nSo our accuracy rate is 66.6.\nSo now that you have created\nall the function\nthat are required\nfor KNN algorithm.\nLet's compile them\ninto one single main function.\nAlright, so this is\nour main function\nand we are using Iris data set\nwith a split of 0.67 and\nthe value of K is 3 Let's see.\nWhat is the accuracy score\nof this check\nhow accurate are modulus so\nin training data set,\nwe have 113 values\nand then the test data\nset we have Seven values.\nThese are the predicted\nand the actual values\nof the output.\nOkay.\nSo in total,\nwe got an accuracy of ninety\nseven point two nine percent,\nwhich is really very good.\nAlright, so I hope the concept\nof this KNN algorithm\nis here devised in a world\nfull of machine learning\nand artificial intelligence\nsurrounding almost everything\naround us classification\nand prediction is one\nof the most important aspects\nof machine learning.\nSo before moving forward,\nlet's have a Look at the agenda.\nI'll start of this video\nby explaining you guys.\nWhat exactly is Nave\nbiased then we'll understand\nwhat is space theorem\nwhich serves as a logic\nbehind the name pass\nalgorithm moving forward.\nI'll explain the steps involved\nin the neighb as algorithm one\nby one and finally,\nI'll finish off this video\nwith a demo on the Nave bass\nusing the sklearn package noun\na bass is a simple\nbut surprisingly\npowerful algorithm\nfrom predictive analysis.\nIt is a classification\ntechnique based on base.\nhim with an assumption\nof Independence among predictors\nit comprises of two parts,\nwhich is knave\nand bias in simple terms\nneighbors classifier assumes\nthat the presence\nof a particular feature\nin a class is unrelated\nto the presence\nof any other feature,\neven if this features\ndepend on each other\nor upon the existence\nof the other features,\nall of these properties\nindependently contribute\nto the probability\nwhether a fruit is an apple\nor an orange or a banana,\nso That is why it\nis known as naive now naive\nbased model is easy to build\nand particularly useful\nfor very large data sets\nin probability Theory\nand statistics based theorem,\nwhich is already\nknown as the base law\nor the base rule describes\nthe probability of an event\nbased on prior knowledge\nof the conditions\nthat might be related\nto the event now pasted\nhere m is a way to figure\nout conditional probability.\nThe conditional probability\nis the probability\nof an event happening given\nthat it has some relationship.\nOne or more other\nevents, for example,\nyour probability of getting\na parking space is connected\nto the time of the day.\nYou park where you park\nand what conventions are you\ngoing on at that time\nBayes theorem is slightly\nmore nuanced in a nutshell.\nIt gives you an actual\nprobability of an event given\ninformation about the tests.\nNow, if you look\nat the definition\nof Bayes theorem,\nwe can see\nthat given a hypothesis H\nand the evidence\ne-base term states that\nthe relationship between the E\nof the hypothesis\nbefore getting the evidence,\nwhich is the P of H\nand the probability\nof the hypothesis\nafter getting the evidence\nthat is p of H given e\nis defined as probability\nof e given H into probability\nof H divided by probability of e\nit's rather confusing, right?\nSo let's take an example\nto understand this theorem.\nSo suppose I have\na deck of cards and\nif a single card is drawn\nfrom the deck of playing cards,\nthe probability that the card\nis a king is for by 52\nsince there are four Kings\nin a standard deck of 52 cards.\nNow if King is an event,\nthis card is a king.\nThe probability of King\nis given as 4 by 52\nthat is equal to 1 by 13.\nNow if the evidence is provided\nfor instance someone\nlooks Such as the card\nthat the single card is\na face card the probability\nof King given\nthat it's a face\ncan be calculated\nusing the base theorem\nby this formula.\nNow since every King\nis also a face card\nthe probability of face given\nthat it's a king is equal to 1\nand since there are\nthree phase cards in each suit.\nThat is the chat king and queen.\nThe probability of the face card\nis equal to 12 by 52.\nThat is 3 by 30.\nNo using base certain we\ncan find out the probability\nof King given that it's a face.\nSo our final answer\ncomes to 1 by 3,\nwhich is also true.\nSo if you have a deck of cards,\nwhich has having only faces now,\nthere are three types\nof phases which are\nthe chat king and queen.\nSo the probability\nthat it's the king is 1 by 3.\nNow.\nThis is the simple example\nof how based on works now\nif we look at the proof as in\nhow this paste Serum evolved.\nSo here we have\nprobability of a given B\nand probability of B\ngiven a now for\na joint probability distribution\nover the sets A and B,\nthe probability of\na intersection B,\nthe conditional probability\nof a given B is defined\nas the probability\nof a intersection B divided\nby probability of B,\nand similarly probability of B,\ngiven a is defined as\nprobability of B intersection\na divided by probability\nof a now we Equate probability\nof a intersection p\nand probability of\nB intersection a as\nboth are the same thing\nnow from this method\nas you can see,\nwe get our final\nbase theorem proof,\nwhich is the probability of a\ngiven b equals probability of B,\ngiven a into probability\nof P divided by\nthe probability of a now\nwhile this is the equation\nthat applies to\nany probability distribution\nover the events A and B.\nIt has a particular nice\ninterpretation in case\nwhere a is represented\nas the hypothesis h H\nand B is represented\nas some observed evidence e\nin that case the formula is p\nof H given e is equal to P\nof e given H into probability\nof H divided by probability\nof e now this relates\nthe probability of hypothesis\nbefore cutting the evidence,\nwhich is p of H\nto the probability\nof the hypothesis\nafter getting the evidence\nwhich is p of H given e\nfor this reason P of H is known\nas the prior probability\nwhile P of It's given e is known\nas the posterior probability\nand the factor\nthat relates the two is known as\nthe likelihood ratio Now using\nthis term space theorem\ncan be rephrased\nas the procedure\nprobability equals.\nThe prior probability\ntimes the likelihood ratio.\nSo now that we know the maths\nwhich is involved\nbehind the Bayes theorem.\nLet's see how we can implement\nthis in real life scenario.\nSo suppose we have a data set.\nSet in which we have\nthe Outlook the humidity\nand we need to find out\nwhether we should play\nor not on that day.\nSo the Outlook can be\nsunny overcast rain\nand the humidity are high normal\nand the wind are categorized\ninto two phases\nwhich are the weak\nand the strong winds.\nThe first of all will create\na frequency table using\neach attribute of the data set.\nSo the frequency table\nfor the Outlook looks\nlike this we have Sunny overcast\nand rainy the frequency table\nof humidity looks like this.\nAnd a frequency table of when\nlooks like this we have strong\nand weak for wind and high\nand normal ranges for humidity.\nSo for each frequency table,\nwe will generate\na likelihood table now now\nthe likelihood table\ncontains the probability\nof a particular day\nsuppose we take the sunny\nand we take the play as yes\nand no so the probability\nof Sunny given that we\nplay yes is 3 by 10,\nwhich is 0.3 the\nprobability of X,\nwhich is the probability\nof Sunny He is equal to 5 by 14.\nNow.\nThese are all the terms\nwhich are just generated\nfrom the data\nwhich we have here.\nAnd finally the probability\nof yes is 10 out of 14.\nSo if we have a look\nat the likelihood of yes given\nthat it's a sunny we\ncan see using Bayes theorem.\nIt's the probability\nof Sunny given yes\ninto probability of yes divided\nby the probability of Sunny.\nSo we have all\nthe values here calculated.\nSo if you put\nthat in our base serum equation,\nwe get the likelihood of Is\na 0.59 similarly the likelihood\nof no can also be calculated\nhere is 0.40 now similarly.\nWe are going to create\nthe likelihood table\nfor both the humidity\nand the win there's a\nfor humidity the likelihood\nfor yes given the humidity\nis high is equal to 0.4\nto and the probability\nof playing know\ngiven the Venice High is 0.58\nthe similarly for table wind.\nThe probability of e is given\nthat the wind is week is 0.75\nand the probability of no given\nthat the win is week is 0.25\nnow suppose we have of day\nwhich has high rain\nwhich has high humidity\nand the wind is weak.\nSo should we play or not?\nThat's all for that.\nWe use the base theorem\nhere again the likelihood\nof yes on that day is equal\nto the probability\nof Outlook rain given\nthat it's a yes\ninto probability.\nOf humidity given that say yes,\nand the probability of\nwhen that is we given\nthat it's we are playing yes\ninto the probability of yes,\nwhich equals to zero\npoint zero one nine\nand similarly the likelihood\nof know on that day is equal\nto zero point zero one six.\nNow if we look\nat the probability\nof yes for that day\nof playing we just\nneed to divide it\nwith the likelihood\nsome of both the yes\nand no so the probability\nof playing tomorrow,\nwhich is yes is .5.\nWhereas the probability\nof not playing is equal to 0.45.\nNow.\nThis is based upon the data\nwhich we already have with us.\nSo now that you have an idea\nof what exactly is named by as\nhow it works and we have seen\nhow it can be implemented\non a particular data set.\nLet's see where it\nis used in the industry.\nThe started with our first\nindustrial use case,\nwhich is news categorized.\nIt's move on to them\nor we can use\nthe term text classification\nto broaden the spectrum\nof this algorithm news\nin the web are rapidly growing\nin the era of Information Age\nwhere each new site has\nits own different layout\nand categorization\nfor grouping news.\nNow these heterogeneity\nof layout and categorization\ncannot always satisfy\nindividual users need\nto remove these heterogeneity\nand classifying\nthe news articles.\nOwing to the user preference\nis a formidable task companies\nuse web crawler\nto extract useful text\nfrom HTML Pages\nthe news articles\nand each of these news articles\nis then tokenized now\nthese tokens are nothing\nbut the categories\nof the news now\nin order to achieve\nbetter classification result.\nWe remove the less\nsignificant Words,\nwhich are the stop was\nfrom the documents\nor the Articles\nand then we apply\nthe Nave base classifier\nfor classifying the news\ncontents based on the news.\nNow this is by far one\nof the best examples\nof Neighbors classifier,\nwhich is Spam filtering.\nNow.\nIt's the Nave\nBayes classifier are\na popular statistical technique\nfor email filtering.\nThey typically use bag-of-words\nfeatures to identify\nat the spam email\nand approach commonly used\nin text classification as well.\nNow it works by correlating\nthe use of tokens,\nbut the spam and non-spam emails\nand then the Bayes theorem,\nwhich I explained\nearlier is used to\ncalculate the probability\nthat an email is\nor not a Spam so named\nby a Spam filtering is\na baseline technique\nfor dealing with Spam\nthat container itself\nto the emails need\nof an individual user\nand give low false positive\nspam detection rates\nthat are generally\nacceptable to users.\nIt is one of the oldest ways\nof doing spam filtering\nwith its roots\nin the 1990s particular words\nhave particular probabilities\nof occurring in spam.\nAnd in legitimate email as well\nfor instance most emails\nusers will frequently\nencounter the world lottery\nor the lucky draw a spam email,\nbut we'll sell them\nsee it in other emails.\nThe filter doesn't know\nthese probabilities in advance\nand must be friends.\nSo it can build them\nup to train the filter.\nThe user must manually indicate\nwhether a new email is Spam\nor not for all the words\nin each straining email.\nThe filter will\nadjust the probability\nthat each word will appear\nin a Spam or legitimate.\nAll in the database now\nafter training the word\nprobabilities also known\nas the likelihood functions are\nused to compute the probability\nthat an email with a particular\nset of words as in belongs\nto either category each word\nin the email contributes\nthe email spam probability.\nThis contribution is called\nthe posterior probability\nand is computed again\nusing the base 0\nthen the email spam probability\nis computed over all\nthe verse in the email\nand if the total exceeds\na certain threshold say\nOr 95% the filter will Mark\nthe email as spam.\nNow object detection is\nthe process of finding instances\nof real-world objects\nsuch as faces bicycles\nand buildings in images\nor video now object detection\nalgorithm typically\nuse extracted features\nand learning algorithm\nto recognize instance of\nan object category here again,\na bias plays an important\nrole of categorization\nand classification of object\nnow medical area.\nThis is increasingly voluminous\namount of electronic data,\nwhich are becoming more\nand more complicated.\nThe produced medical data\nhas certain characteristics\nthat make the analysis\nvery challenging and attractive\nas well among all\nthe different approaches.\nThe knave bias is used.\nIt is the most effective\nand efficient classification\nalgorithm and has\nbeen successfully applied\nto many medical problems\nempirical comparison\nof knave bias versus\nfive popular classifiers\non Medical data sets shows\nthat may bias is well suited\nfor medical application and has\nhigh performance in most\nof the examine medical problems.\nNow in the past various\nstatistical methods have been\nused for modeling in the area\nof disease diagnosis.\nThese methods require\nprior assumptions and are\nless capable of dealing\nwith massive and complicated\nnonlinear and dependent data one\nof the main advantages\nof neighbor as approach\nwhich is appealing\nto Physicians is\nthat all the available\ninformation is used?\nTo explain the decision\nthis explanation seems\nto be natural for medical\ndiagnosis and prognosis.\nThat is it is very\nclose to the way\nhow physician diagnosed patients\nnow weather is one\nof the most influential factor\nin our daily life to an extent\nthat it may affect\nthe economy of a country\nthat depends on occupation\nlike agriculture.\nTherefore as a countermeasure\nto reduce the damage\ncaused by uncertainty\nin whether Behavior,\nthere should be an efficient way\nto print the weather now\nwhether projecting has\nChallenging problem\nin the meteorological department\nsince ears even\nafter the technology skill\nand scientific advancement\nthe accuracy\nand production of weather\nhas never been sufficient even\nin current day this domain\nremains as a research topic\nin which scientists\nand mathematicians are working\nto produce a model\nor an algorithm\nthat will accurately\npredict the weather now\na bias in approach\nbased model is created by\nwhere procedure probabilities\nare used to calculate\nthe likelihood of\neach class label for input.\nData instance and the one\nwith the maximum likelihood\nis considered as the resulting\noutput now earlier.\nWe saw a small implementation\nof this algorithm as well\nwhere we predicted\nwhether we should play\nor not based on the data,\nwhich we have collected earlier.\nNow, this is a python Library\nwhich is known as scikit-learn\nit helps to build in a bias\nand model in Python.\nNow, there are three types\nof named by ass model\nunder scikit-learn Library.\nThe first one is the caution.\nIt is used in classification\nand it Assumes\nthat the feature follow\na normal distribution.\nThe next we have is multinomial.\nIt is used for discrete counts.\nFor example, let's say we have\na text classification problem\nand here we\nconsider bernouli trials,\nwhich is one step further\nand instead of word\noccurring in the document.\nWe have count\nhow often word occurs\nin the document you\ncan think of it\nas a number of times\noutcomes number is observed\nin the given number of Trials.\nAnd finally we have\nthe bernouli type.\nOf Naples, the binomial\nmodel is useful\nif your feature vectors are\nbinary bag of words model\nwhere the once\nand the zeros are words occur\nin the document and the verse\nwhich do not occur\nin the document respectively\nbased on their data set.\nYou can choose any of\nthe given discussed model here,\nwhich is the gaussian\nthe multinomial or the bernouli.\nSo let's understand\nhow this algorithm works.\nAnd what are\nthe different steps?\nOne can take to create\na bison model and use knave bias\nto predict the output so\nhere to understand better.\nWe are going to predict\nthe onset of diabetes Now\nthis problem comprises\nof 768 observations\nof medical details\nfor Pima Indian patients.\nThe record describes\ninstantaneous measurement taken\nfrom the patient such as\nthe age the number\nof times pregnant\nand the blood work group\nnow all the patients are\nwomen aged 21 and Old\nand all the attributes\nare numeric and the unit's vary\nfrom attribute to attribute.\nEach record has\na class value that indicate\nwhether the patient suffered\non onset of diabetes\nwithin five years\nor the measurements.\nNow, these are\nclassified as zero.\nNow, I've broken\nthe whole process down\ninto the following steps.\nThe first step is handling\nthe data in which we load\nthe data from the CSV file\nand split it into training\nand test data sets.\nThe second step\nis summarizing the data.\nIn which we summarize\nthe properties in the training\ndata sets so that we\ncan calculate the probabilities\nand make predictions.\nNow the third step comes is\nmaking a particular prediction.\nWe use the summaries\nof the data set to generate\na single prediction.\nAnd after that we\ngenerate predictions\ngiven a test data set and\na summarize training data sets.\nAnd finally we evaluate\nthe accuracy of the predictions\nmade for a test data set\nas the percentage correct\nout of all the predictions made\nand finally We tied\ntogether and form.\nOur own model\nof nape is classifier.\nNow.\nThe first thing we need to do\nis load our data the data is\nin the CSV format\nwithout a header line\nor any codes.\nWe can open the file\nwith the open function\nand read the data lines\nusing the read functions\nin the CSV module.\nNow, we also need\nto convert the attributes\nthat were loaded as\nstrings into numbers\nso that we can work with them.\nSo let me show you\nhow this can be implemented now\nfor that you need to Tall python\non a system and use\nthe jupyter notebook\nor the python shell.\nHey, I'm using\nthe Anaconda Navigator\nwhich has all\nthe things required to do\nthe programming in Python.\nWe have the Jupiter lab.\nWe have the notebook.\nWe have the QT console.\nEven we have a studio as well.\nSo what you need to do is just\ninstall the Anaconda Navigator\nit comes with the pre\ninstalled python also,\nso the moment you click launch\non The jupyter Notebook.\nIt will take you\nto the Jupiter homepage\nin a local system\nand here you can do\nprogramming in Python.\nSo let me just rename it as\nby my India diabetes.\nSo first, we need\nto load the data set.\nSo I'm creating here a function\nload CSV now before that.\nWe need to import\ncertain CSV the math\nand the random method.\nSo as you can see,\nI've created a load CSV function\nwhich will take the pie\nmy Indian diabetes\ndata dot CSV file using\nthe CSV dot reader method\nand then we are converting\nevery element of that data set\ninto float originally all\nthe Ants are in string,\nbut we need to convert\nthem into floor\nfor our calculation purposes.\nNow next we need to split\nthe data into training data sets\nthat nay bias can use\nto make the prediction\nand this data set\nthat we can use to evaluate\nthe accuracy of the model.\nWe need to split the data\nset randomly into training\nand testing data set\nin the ratio of usually\nwhich is 70 to 30,\nbut for this example,\nI am going to use 67\nand 33 now 70 and 30 is a Ratio\nfor testing algorithms\nso you can play around\nwith this number.\nSo this is our split\ndata set function.\nNow the Navy base model is\ncomprised of summary of the data\nin the training data set.\nNow this summary is then used\nwhile making predictions.\nNow the summary\nof the training data\ncollected involves the mean\nthe standard deviation\nof each attribute\nby class value now, for example,\nif there are two class values\nand seven numerical attributes,\nthen we need a mean\nand the standard deviation for\neach of these seven attributes\nand the class value\nwhich makes The 14\nattribute summaries\nso we can break the preparation\nof this summary down\ninto the following sub tasks\nwhich are the separating data\nby class calculating mean\ncalculating standard deviation\nsummarizing the data sets\nand summarizing\nattributes by class.\nSo the first task is to separate\nthe training data set\ninstances by class value\nso that we can calculate\nstatistics for each class.\nWe can do that by creating a map\nof each class value\nto a list of instances\nthat belong to the class.\nClass and sort the entire\ndataset of instances\ninto the appropriate list.\nNow the separate\nby class function just the same.\nSo as you can see\nthe function assumes\nthat the last attribute\nis the class value\nthe function returns a map\nof class value to the list\nof data instances next.\nWe need to calculate\nthe mean of each attribute\nfor a class value.\nNow, the mean is the central\nmiddle or the central tendency\nof the data and we\nuse it as a middle\nof our gaussian distribution\nwhen Calculating\nthe probabilities.\nSo this is our function\nfor mean now.\nWe also need to calculate\nthe standard deviation\nof each attribute\nfor a class value.\nThe standard deviation\nis calculated as a square root\nof the variance\nand the variance\nis calculated as the average\nof the squared differences\nfor each attribute value\nfrom the mean now\none thing to note\nthat here is\nthat we are using\nn minus one method\nwhich subtracts one\nfrom the number\nof attributes values\nwhen calculating the variance.\nThe now that we have the tools\nto summarize the data\nfor a given list of instances,\nwe can calculate the mean\nand standard deviation\nfor each attribute.\nNow that's if function groups\nthe values for each attribute\nacross our data instances\ninto their own lists\nso that we can compute the mean\nand standard deviation values\nfor each attribute.\nThe next comes the summarizing\nattributes by class.\nWe can pull it all together\nby first separating\nour training data sets\ninto instances growth by class\nthen calculating the summaries\nfor each a To be with now.\nWe are ready to make predictions\nusing the summaries prepared\nfrom our training data\nmaking predictions involves\ncalculating the probability\nthat a given data instance\nbelong to each class then\nselecting the class\nwith the largest probability\nas a prediction.\nNow we can divide this whole\nmethod into four tasks\nwhich are the calculating\ngaussian probability density\nfunction calculating class\nprobability making a prediction\nand then estimating the accuracy\nnow to calculate the gaussian\nprobability density function.\nWe use the gaussian function\nto estimate the probability\nof a given attribute value\ngiven the node mean\nand the standard deviation\nof the attribute estimated\nfrom the training data.\nAs you can see\nthe parameters are x\nmean and the standard deviation\nnow in the calculate\nprobability function,\nwe calculate the exponent first\nthen calculate the main division\nthis lets us fit the equation\nnicely into two lines.\nNow, the next task\nis calculating the\nclass properties now\nthat we had can calculate\nthe probability of an attribute\nbelonging to a class.\nWe can combine the probabilities\nof all the attributes values\nfor a data instance\nand come up with a probability\nof the entire.\nOur data instance\nbelonging to the class.\nSo now that we have calculated\nthe class properties.\nIt's time to finally make\nour first prediction now,\nwe can calculate the probability\nof the data instance belong\nto each class value\nand we can look\nfor the largest probability\nand return the associated class\nand for that we are going\nto use this function to predict\nwhich uses the summaries\nand the input Vector which is\nbasically all the probabilities\nwhich are being input\nfor a particular label\nnow finally we can\nAn estimate the accuracy\nof the model\nby making predictions\nfor each data instances\nin our test data for that.\nWe use the cat\npredictions method.\nNow this method is used\nto calculate the predictions\nbased upon the test data sets\nand the summary\nof the training data set.\nNow, the predictions\ncan be compared\nto the class values\nin our test data set\nand classification accuracy\ncan be calculated as\nan accuracy ratio\nbetween the zeros\nand the hundred percent.\nNow the get accuracy method will\ncalculate this accuracy ratio.\nNow finally to sum it all up.\nWe Define our main function\nwe call all these methods\nwhich we have defined\nearlier one by one to get\nthe Courtesy of the model\nwhich we have created.\nSo as you can see,\nthis is our main function\nin which we have the file name.\nWe have defined the split ratio.\nWe have the data set.\nWe have the training\nand test data set.\nWe are using the split\ndata set method next.\nWe are using the summarized\nby class function using\nthe get prediction and\nthe get accuracy method as well.\nSo guys as you can see\nthe output of this one gives us\nthat we are splitting the seven\nsixty eight rows into 514\nwhich is the training and 254\nwhich is the test data set rows\nand the accuracy of this model\nis 68% Now we can play\nwith the amount of training\nand test data sets\nwhich are to be used\nso we can change\nthe split ratio to seventies.\n238 is 220 to get\ndifferent sort of accuracy.\nSo suppose I change\nthe split ratio from 0.67 20.8.\nSo as you can see,\nwe get the accuracy\nof 62 percent.\nSo splitting it into 0.67\ngave us a better result\nwhich was 68 percent.\nSo this is how you can Implement\nNavy bias caution classifier.\nThese are the step\nby step methods\nwhich you need to do in case of\nusing the Nave Bayes classifier,\nbut don't worry.\nWe do not need to write\nall this many lines\nof code to make a model this\nwith The Sacketts.\nAnd I really comes into picture\nthe scikit-learn library has\na predefined method\nor as say a predefined function\nof neighbor bias,\nwhich converts all\nof these lines,\nof course into merely just\ntwo or three lines of codes.\nSo, let me just open\nanother jupyter notebook.\nSo let me name it\nas sklearn a pass.\nNow here we are going to use\nthe most famous data set\nwhich is the iris dataset.\nNow, the iris flower data\nset is a multivariate\ndata set introduced by\nthe British statistician\nand biologists Roland Fisher\nand based on this fish is linear\ndiscriminant model this data set\nbecame a typical test case\nfor many statistical\nclassification techniques\nin machine learning.\nSo here we are going to use\nthe caution NB model,\nwhich is already available\nin the sklearn.\nAs I mentioned earlier,\nthere were three\ntypes of Neighbors\nwhich are the question\nmultinomial and the bernouli.\nSo here we are going to use\nthe caution and be model\nwhich is already present\nin the sklearn library,\nwhich is the\ncycle learn Library.\nSo first of all,\nwhat we need to do is\nimport the sklearn data sets\nand the metrics\nand we also need to import\nthe caution and be Now\nonce all these libraries\nare lowered we need\nto load the data set\nwhich is the iris dataset.\nThe next what we need\nto do is fit a Nave\nby a small to this data set.\nSo as you can see we have so\neasily defined the model\nwhich is the gaussian\nNB which contains\nall the programming\nwhich I just showed you\nearlier all the methods\nwhich are taking the input\ncalculating the mean\nthe standard deviation\nseparating it bike last\nand finally making predictions.\nCalculating the\nprediction accuracy.\nAll of this comes\nunder the caution and be method\nwhich is inside already present\nin the sklearn library.\nWe just need to fit it\naccording to the data set\nwhich we have so next\nif we print the model we see\nwhich is the gaussian NB model.\nThe next what we need to do\nis make the predictions.\nSo the expected output\nis data set dot Target\nand the projected\nis using the pretend model\nand the model we are using\nis the cause in NB here.\nHere now to summarize the model\nwhich created we calculate\nthe confusion Matrix\nand the classification report.\nSo guys, as you can see\nthe classification to provide\nwe have the Precision\nof Point Ninety Six,\nwe have the recall of 0.96.\nWe have the F1 score\nand the support and finally if\nwe print our confusion Matrix,\nas you can see it gives\nus this output.\nSo as you can see\nusing the gaussian\nand we method just\nputting it in the model\nand using any of the data.\nFitting the model\nwhich you created\ninto a particular data set\nand getting the desired\noutput is so easy\nwith the scikit-learn library.\nSo guys, this is it.\nI hope you understood a lot\nabout the nape Bayes classifier\nhow it is used\nwhere it is used and what are\nthe different steps involved\nin the classification technique\nand how the scikit-learn\nmakes all of those techniques\nvery easy to implement\nin any data set which we have.\nAs we M or support\nVector machine is one\nof the most effective\nmachine learning classifier\nand it has been used\nin various Fields\nsuch as face recognition\ncancer classification\nand so on today's session\nis dedicated to how svm works\nthe various features of svm\nand how it is used\nin the real world.\nSo without any further due\nlet's take a look\nat the agenda for today.\nWe're going to begin the session\nwith an introduction\nto machine learning\nand the different types\nof machine learning.\nNext we'll discuss\nwhat exactly support\nVector machines are\nand then we'll move on and see\nhow svm works\nand how it can be used\nto classify linearly\nseparable data will also\nbriefly discuss about\nhow nonlinear svm's work\nand then we'll move on\nand look at the use case of svm\nin colon cancer classification\nand finally we'll end\nthe session by running a demo\nwhere we'll use svm to predict\nwhether a patient is suffering\nfrom a heart disease or not.\nOkay, so that was the agenda.\nLet's get stood\nwith our first topic.\nSo what is machine learning\nmachine learning is a science\nof getting computers to act\nby feeding them data\nand letting them learn\na few tricks on their own.\nOkay, we're not going\nto explicitly program\nthe machine instead.\nWe're going to feed it\ndata and let it learn\nthe key to machine learning is\nthe data machines learn just\nlike us humans.\nWe humans need\nto collect information\nand data to learn similarly\nmachines must also be fed data\nin order to learn\nand make decisions.\nLet's say that you want\na machine to predict\nthe value of a stock.\nAll right in such situations.\nYou just feed the machine\nwith relevant data\nafter which you develop a model\nwhich is used to predict\nthe value of the stock.\nNOW one thing to keep\nin mind is the more data\nyou feed the machine the\nbetter it will learn\nand make more accurate\npredictions obviously machine\nlearning is not so simple\nin order for a machine\nto analyze and get\nuseful insights from data.\nIt must process\nand study the data\nby running different.\nAlgorithms on it.\nAll right.\nAnd today we'll be discussing\nabout one of the most widely\nused algorithm called\nthe support Vector machine.\nOkay.\nNow that you have a brief idea\nabout what machine learning is,\nlet's look at the different ways\nin which machines Lon first.\nWe have supervised\nlearning in this type\nof learning the machine\nlearns under guidance.\nAll right, that's why\nit's called supervised learning\nnow at school.\nOur teachers guided us\nand taught us similarly\nin supervised learning machines\nlearn by feeding\nthem labeled data.\nExplicitly telling them.\nHey, this is the input\nand this is\nhow the output must look.\nOkay.\nSo guys the teacher in this case\nis the training data.\nNext we have\nunsupervised learning here.\nThe data is not labeled\nand there is no guide\nof any sort.\nOkay, the machine must figure\nout the data set given\nand must find hidden patterns\nin order to make predictions\nabout the output an example\nof unsupervised learning is\nan adult's like you and me.\nWe don't need a guide to help us\nwith our daily activities.\nThey figured things out on\nour own without any supervision.\nAll right, that's exactly how\nI'm supervised learning work.\nFinally.\nWe have reinforcement learning.\nLet's say you were dropped off\nat an isolated island.\nWhat would you do now\ninitially you would panic\nand you'll be unsure\nof what to do\nwhere to get food from How\nTo Live and all of that\nbut after a while you will have\nto adapt you must learn\nhow to live in the island adapt\nto the changing climate learn\nwhat to eat and what not to eat.\nYou're basically following\nthe hit and trial.\nBecause you're new\nto the surrounding\nand the only way to learn\nis experience and then learn\nfrom your experience.\nThis is exactly what\nreinforcement learning is.\nIt is a learning method\nwherein an agent interacts\nwith its environment\nby producing actions\nand discovers errors or words.\nAlright, and once it gets\ntrained it gets ready to predict\nthe new data presented to it.\nNow in our case the agent\nwas you basically stuck\non the island\nand the environment\nwas the island.\nAll right?\nOkay now now let's\nmove on and see\nwhat svm algorithm is all about.\nSo guys svm\nor support Vector machine is\na supervised learning algorithm,\nwhich is mainly used to classify\ndata into different classes now\nunlike most algorithms svm\nmakes use of a hyperplane\nwhich acts like\na decision boundary\nbetween the various classes\nin general svm can\nbe used to generate\nmultiple separating hyperplanes\nso that the data\nis divided into segments.\nOkay and each These segments\nwill contain only\none kind of data.\nIt's mainly used\nfor classification purpose\nwearing you want to classify\nor data into two different\nsegments depending\non the features of the data.\nNow before moving any further,\nlet's discuss a few\nfeatures of svm.\nLike I mentioned earlier svm is\na supervised learning algorithm.\nThis means that svm trains\non a set of labeled data svm\nstudies the label training data\nand then classifies\nany new input data depending on\nwhat it learned in the training.\nIn Phase a main advantage\nof support Vector machine is\nthat it can be used\nfor both classification\nand regression problems.\nAll right.\nNow even though svm is mainly\nknown for classification the svr\nwhich is the support\nVector regressor is used\nfor regression problems.\nAll right, so svm can be used\nboth for classification.\nAnd for regression.\nNow, this is one of the reasons\nwhy a lot of people prefer svm\nbecause it's a very good\nclassifier and along with that.\nIt is also used for regression.\nAnother feature is the svm\nkernel functions svm can be used\nfor classifying nonlinear data\nby using the kernel trick\nthe kernel trick basically means\nto transform your data\ninto another dimension\nso that you can easily\ndraw a hyperplane\nbetween the different\nclasses of the data.\nAlright, nonlinear data\nis basically data\nwhich cannot be separated\nwith a straight line.\nAlright, so svm can even be used\non nonlinear data sets.\nYou just have to use\na kernel functions to do this.\nAll right, so Guys,\nI hope you all are clear\nwith the basic concepts of svm.\nNow.\nLet's move on and look\nat how svm works so guys\nan order to understand\nhow svm Works let's consider\na small scenario now\nfor a second pretend\nthat you own a firm.\nOkay, and let's say\nthat you have a problem\nand you want to set up a fence\nto protect your rabbits\nfrom the pack of wolves.\nOkay, but where do you\nbuild your fence\none way to get around?\nThe problem is to build\na classifier based\non the position of the rabbits\nand words in your Faster.\nSo what I'm telling you is\nyou can classify the group\nof rabbits as one group\nand draw a decision\nboundary between the rabbits\nand the world.\nAll right.\nSo if I do that and if I try\nto draw a decision boundary\nbetween the rabbits\nand the Wolves,\nit looks something like this.\nOkay.\nNow you can clearly build\na fence along this line\nin simple terms.\nThis is exactly\nhow SPM work it draws\na decision boundary,\nwhich is a hyperplane\nbetween any two classes in order\nto separate them or class.\nAsif I them now,\nI know you're thinking\nhow do you know\nwhere to draw a hyperplane\nthe basic principle behind\nsvm is to draw a hyperplane\nthat best separates\nthe two classes\nin our case the two glasses\nof the rabbits and the Wolves.\nSo you start off by drawing\na random hyperplane\nand then you check the distance\nbetween the hyperplane\nand the closest data points\nfrom each glove these closes\non your is data points\nto the hyperplane are known\nas support vectors and that's\nwhere the name\ncomes from support.\nActive machine.\nSo basically the\nhyperplane is drawn\nbased on these support vectors.\nSo guys an Optimum\nhyperplane will have\na maximum distance from each\nof these support vectors.\nAll right.\nSo basically the hyper plane\nwhich has the maximum distance\nfrom the support vectors is\nthe most optimal hyperplane\nand this distance\nbetween the hyperplane\nand the support vectors\nis known as the margin.\nAll right.\nSo to sum it up svm\nis used to classify data\nby using a hyper plane such\nthat the distance distance\nbetween the hyperplane\nand the support\nvectors is maximum.\nSo basically your margin\nhas to be maximum.\nAll right, that way,\nyou know that you're actually\nseparating your classes or add\nbecause the distance between\nthe two classes is maximum.\nOkay.\nNow, let's try\nto solve a problem.\nOkay.\nSo let's say that I input\na new data point.\nOkay.\nThis is a new data point\nand now I want to draw\na hyper plane such\nthat it best separates\nthe two classes.\nOkay, so I start off by drawing\na hyperplane like this\nand then I check the distance\nbetween Hyper plane\nand the support vectors.\nOkay, so I'm trying to check\nif the margin is maximum\nfor this hyperplane,\nbut what if I draw a hyper plane\nwhich is like this?\nAll right.\nNow I'm going to check\nthe support vectors over here.\nThen I'm going\nto check the distance\nfrom the support vectors\nand with this hyperplane,\nit's clear that the\nmargin is more right\nwhen you compare the margin\nof the previous one\nto this hyperplane.\nIt is more.\nSo the reason why I'm choosing\nthis hyperplane is\nbecause the distance\nbetween the support vectors\nand the hi Hyperplane\nis maximum in this scenario.\nOkay, so guys this is\nhow you choose a hyperplane.\nYou basically have to make sure\nthat the hyper plane\nhas a maximum.\nMargin.\nAll right, it has two best\nseparate the two classes.\nAll right.\nOkay so far it was quite easy.\nOur data was linearly separable\nwhich means that you\ncould draw a straight line\nto separate the two classes.\nAll right, but what will you do?\nIf the data set is like this\nyou possibly can't draw\na hyper plane like this.\nAll right.\nIt doesn't separate the two.\nAt all, so what do you do\nin such situations now earlier\nin the session I mentioned\nhow a kernel can be used\nto transform data\ninto another dimension\nthat has a clear dividing margin\nbetween the classes of data.\nAlright, so kernel functions\noffer the user this option\nof transforming nonlinear spaces\ninto linear ones.\nNonlinear data set is the one\nthat you can't separate\nusing a straight line.\nAll right, in order to deal\nwith such data sets you're going\nto Ants form them\ninto linear data sets\nand then use svm on them.\nOkay.\nSo simple trick would be\nto transform the two variables\nX and Y into a new\nfeature space involving\na new variable called Z.\nAll right, so guys so far\nwe were plotting our data\non two dimensional space.\nCorrect?\nWe will only using the X\nand the y axis so we had only\nthose two variables X and Y now\nin order to deal with this kind\nof data a simple trick would be\nto transform the two variables X\nand I into a new feature space\ninvolving a new variable\ncalled Z. Ok,\nso we're basically\nvisualizing the data\non a three-dimensional space.\nNow when you transform\nthe 2D space into a 3D space,\nyou can clearly see\na dividing margin\nbetween the two classes\nof data right now.\nYou can go ahead\nand separate the two classes\nby drawing the best\nhyperplane between them.\nOkay, that's exactly\nwhat we discussed\nin the previous slides.\nSo guys, why don't you try\nthis yourself dry\ndrawing a hyperplane,\nwhich is the most Optimum.\nFor these two classes.\nAll right, so guys,\nI hope you have\na good understanding\nabout nonlinear svm's now.\nLet's look at a real world use\ncase of support Vector machines.\nSo guys s VM\nas a classifier has been used\nin cancer classification\nsince the early 2000s.\nSo there was an experiment held\nby a group of professionals\nwho applied svm in a colon\ncancer tissue classification.\nSo the data set consisted\nof about 2,000\ntransmembrane protein samples\nand Only about 50 to 200\ngenes samples were input\nInto the svm classifier\nNow this sample\nwhich was input\ninto the svm classifier had\nboth colon cancer tissue samples\nand normal colon tissue\nsamples right now.\nThe main objective of this study\nwas to classify Gene samples\nbased on whether they\nare cancerous or not.\nOkay, so svm was trained\nusing the 50 to 200 samples\nin order to discriminate\nbetween non-tumor\nfrom tumor specimens.\nSo the performance\nof The svm classifier\nwas very accurate\nfor even a small data set.\nAll right, we had only\n50 to 200 samples.\nAnd even for the small data\nset svm was pretty accurate\nwith its results.\nNot only that its\nperformance was compared\nto other classification\nalgorithm like naive Bayes\nand in each case svm\noutperform naive Bayes.\nSo after this experiment\nit was clear\nthat svm classify\nthe data more effectively\nand it worked exceptionally good\nwith small data sets.\nLet's go ahead\nand understand what exactly\nis unsupervised learning.\nSo sometimes the given data\nis unstructured and unlabeled\nso it becomes difficult\nto classify the data\ninto different categories.\nSo unsupervised learning\nhelps to solve this problem.\nThis learning is used\nto Cluster the input data\nin classes on the basis\nof their statistical properties.\nSo example, we can\ncluster Different Bikes\nbased upon the speed\nlimit their acceleration\nor the average.\nAverage that they are giving so\nand suppose learning is a type\nof machine learning algorithm\nused to draw inferences\nfrom data sets consisting\nof input data\nwithout labels responses.\nSo if you have a look\nat the workflow\nor the process flow\nof unsupervised learning,\nso the training data is\ncollection of information\nwithout any label.\nWe have the machine\nlearning algorithm\nand then we have\nthe clustering malls.\nSo what it does is\nthat distributes the data\ninto different clusters\nand again if you provide\nany Lebanon new data,\nit will make a prediction\nand find out to which cluster\nthat particular data\nor the data set belongs\nto or the particular data point\nbelongs to so one\nof the most important\nalgorithms in unsupervised\nlearning is clustering.\nSo let's understand exactly\nwhat is clustering.\nSo a clustering\nbasically is the process\nof dividing the data sets\ninto groups consisting\nof similar data points.\nIt means grouping\nof objects based\non the information found in\nthe data describing the objects\nor their relationships,\nso So clustering malls focus on\nand defying groups\nof similar records\nand labeling records\naccording to the group\nto which they belong now.\nThis is done without the benefit\nof prior knowledge\nabout the groups\nand their creator districts.\nSo and in fact,\nwe may not even know exactly\nhow many groups are\nthere to look for.\nNow.\nThese models are often\nreferred to as\nunsupervised learning models,\nsince there's no external\nstandard by which\nto judge the malls\nclassification performance.\nThere are no right or wrong\nanswers to these model and\nif we talk about why\nclustering is used\nso the goal of clustering\nis to determine\nthe intrinsic growth in a set\nof unlabeled data sometime.\nThe partitioning is the goal\nor the purpose of clustering\nalgorithm is to make sense\nof and exact value\nfrom the last set of structured\nand unstructured data.\nSo that is why clustering\nis used in the industry.\nAnd if you have a look\nat the various use cases\nof clustering in Industry\nso first of all,\nit's being used in marketing.\nSo discovering distinct groups\nin customer databases\nsuch as customers\nwho make a lot of long\ndistance calls customers\nwho use internet more\nthan cause they're also\nusing insurance companies\nfor like identifying groups\nof Corporation insurance policy\nholders with high average\nclaim rate Farmers crash cops,\nwhich is profitable.\nThey are using C Smith studies\nand Define probability areas\nof oil or gas exploration based.\nDon't cease make data\nand they're also used\nin the recommendation of movies.\nIf you'd say they are also used\nin Flickr photos.\nThey also used by Amazon\nfor recommending the product\nwhich category it lies in.\nSo basically if we talk\nabout clustering there are\nthree types of clustering.\nSo first of all,\nwe have the exclusive clustering\nwhich is the hard clustering\nso here and item belongs\nexclusively to one cluster\nnot several clusters\nand the datapoint belong\nexclusively to one cluster.\nER so an example of this is\nthe k-means clustering so\nclaiming clustering does\nthis exclusive kind\nof clustering so secondly,\nwe have overlapping clustering\nso it is also known as\nsoft clusters in this\nand item can belong\nto multiple clusters as\nits degree of association\nwith each cluster\nis shown and for example,\nwe have fuzzy\nor the c means clustering\nwhich has been used\nfor overlapping clustering\nand finally we have\nthe hierarchical clustering\nso When two clusters have\na parent-child relationship\nor a tree-like structure,\nthen it is known\nas hierarchical cluster.\nSo as you can see here\nfrom the example,\nwe have a parent-child kind\nof relationship in\nthe cluster given here.\nSo let's understand\nwhat exactly is\nK means clustering.\nSo today means clustering is\nan Enquirer them whose main goal\nis to group similar elements\nof data points into a cluster\nand it is a process\nby which objects are classified\ninto a predefined\nnumber of groups\nso that they They are\nas much just similar as\npossible from one group\nto another group\nbut as much as similar or\npossible within each group now\nif you have a look\nat the algorithm working here,\nyou're right.\nSo first of all,\nit starts with and defying\nthe number of clusters,\nwhich is K\nthat I can we find the centroid\nwe find that distance objects\nto the distance object\nto the centroid distance\nof object to the centroid.\nThen we find the grouping based\non the minimum distance.\nPast the centroid Converse\nif true then we make\na cluster false.\nWe then I can't find\nthe centroid repeat\nall of the steps\nagain and again,\nso let me show you\nhow exactly clustering was\nwith an example here.\nSo first we need\nto decide the number\nof clusters to be made now\nanother important task here is\nhow to decide the important\nnumber of clusters\nor how to decide the number\nof classes will get\ninto that later.\nSo first, let's assume\nthat the number\nof clusters we have decided.\nIt is three.\nSo after that then we\nprovide the centroids\nfor all the Clusters\nwhich is guessing\nand the algorithm calculates\nthe euclidean distance\nof the point from each centroid\nand assize the data point\nto the closest cluster\nnow euclidean distance.\nAll of you know\nis the square root\nof the distance the square root\nof the square of the distance.\nSo next when the centroids\nare calculated again,\nwe have our new clusters\nfor each data point then again\nthe distance from the points.\nTo the new classes\nare calculated and then\nagain the points are assigned\nto the closest cluster.\nAnd then again,\nwe have the new centroid\nscattered and now\nthese steps are repeated\nuntil we have\na repetition the centroids\nor the new centralized are very\nclose to the very previous ones.\nSo until unless our output\ngets repeated or the outputs\nare very very close enough.\nWe do not stop this process.\nWe keep on calculating\nthe euclidean distance\nof all the points\nto the centroid.\nIt's then we calculate\nthe new centroids\nand that is how K means\nclustering Works basically,\nso an important part\nhere is to understand\nhow to decide the value of K\nor the number of clusters\nbecause it does\nnot make any sense.\nIf you do not know\nhow many classes\nare you going to make?\nSo to decide\nthe number of clusters?\nWe have the elbow method.\nSo let's assume first\nof all compute\nthe sum squared error,\nwhich is sse4 some value\nof a for example.\nTake two four six\nand eight now the SSE\nwhich is the sum squared error\nis defined as a sum\nof the squared distance\nbetween each number member\nof the cluster\nand its centroid\nmathematically and\nif you mathematically it\nis given by the equation\nwhich is provided here.\nAnd if you brought\nthe key against the SSE,\nyou will see\nthat the error decreases\nas K gets large not this is\nbecause the number\nof cluster increases\nthey should be smaller.\nSo the Distortion is\nalso smaller know.\nThe idea of the elbow method\nis to choose the K at which\nthe SSE decreases abruptly.\nSo for example here\nif we have a look\nat the figure given here.\nWe see that the best number\nof cluster is at the elbow\nas you can see here the graph\nhere changes abruptly\nafter the number four.\nSo for this particular example,\nwe're going to use\nfor as a number of cluster.\nSo first of all\nwhile working with\nk-means clustering there\nare two key points\nto know first of all,\nBe careful about\nwhere you start so choosing\nthe first center at random\nduring the second center.\nThat is far away from the first\ncenter similarly choosing\nthe NIH Center as far away\nas possible from the closest\nof the of the other centers\nand the second idea\nis to do as many runs\nof k-means each with different\nrandom starting points\nso that you get an idea\nof where exactly\nand how many clusters\nyou need to make\nand where exactly\nthe centroid lies\nand how the data\nis getting converted.\nDivorced now k-means is\nnot exactly a very good method.\nSo let's understand the pros\nand cons of k-means clustering.\nWe know that k-means is simple\nand understandable.\nEveryone learns to the first go\nthe items automatically assigned\nto the Clusters.\nNow if we have\na look at the cons,\nso first of all one needs to\ndefine the number of clusters,\nthere's a very\nheavy task asks us\nif we have three four or\nif we have 10 categories,\nand if you do not know\nwhat the number\nof clusters are going to be.\nIt's very difficult for anyone.\nYou know to guess the number\nof clusters not all the items\nare forced into clusters\nwhether they are actually belong\nto any other cluster\nor any other category.\nThey are forced to rely\nin that other category\nin which they are closest\nto this against happens\nbecause of the number\nof clusters with not defining\nthe correct number of clusters\nor not being able to guess\nthe correct number of clusters.\nSo and for most of all,\nit's unable to handle\nthe noisy data and the outliners\nbecause anyways machine\nlearning engineers and date.\nOur scientists have\nto clean the data.\nBut then again it comes\ndown to the analysis\nwhat they're doing\nand the method\nthat they are using so typically\npeople do not clean the data\nfor k-means clustering or even\nif the clean there's\nsometimes a now see noisy\nand outliners data\nwhich affect the whole model\nso that was all\nfor k-means clustering.\nSo what we're going to do\nis now use k-means clustering\nfor the movie datasets,\nso, Have to find out\nthe number of clusters\nand divide it accordingly.\nSo the use case is\nthat first of all,\nwe have a data set\nof five thousand movies.\nAnd what we want\nto do is grip them\nif the movies into clusters\nbased on the Facebook likes,\nso guys, let's have a look\nat the demo here.\nSo first of all,\nwhat we're going to do is\nimport deep copy numpy pandas\nSeaborn the various libraries,\nwhich we're going to use now\nand from my proclivities\nin the use ply plot.\nAnd we're going to use\nthis ggplot and next\nwhat we're going to do is\nimport the data set and look\nat the shape of the data set.\nSo if you have a look at the\nshape of the data set we can see\nthat it has 5043 rose\nwith 28 columns.\nAnd if you have a look\nat the head of the data\nset we can see it\njust 5043 data points,\nso George we going to do\nis place the data points\nin the plot we take\nthe director Facebook likes\nand we have a look\nat the data columns\nface number in post cars\ntotal Facebook likes\ndirector Facebook likes.\nSo what we have done here\nnow is taking the director\nFacebook likes and the actor\nthree Facebook likes, right.\nSo we have five thousand\nforty three rows\nand two columns Now using\nthe k-means from sklearn\nwhat we're going\nto do is import it.\nFirst we're going to import\nk-means from scale\nand Dot cluster.\nRemember guys eschaton is\na very important library\nin Python for machine learning.\nSo and the number of cluster\nwhat we're going to do is\nprovide as five now this again,\nthe number of cluster\ndepends upon the SSE,\nwhich is the sum\nof squared errors all the we're\ngoing to use the elbow method.\nSo I'm not going to go\ninto the details of that again.\nSo we're going to fit the data\ninto the k-means to fit and\nif you find the cluster,\nUs than for the\nk-means and printed.\nSo what we find is is\nan array of five clusters\nand Fa print the label\nof the k-means cluster.\nNow next what we're going\nto do is plot the data\nwhich we have with the Clusters\nwith the new data clusters,\nwhich we have found\nand for this we're going\nto use the CC Bond\nand as you can see here,\nwe have plotted that car.\nWe have plotted the data\ninto the grid and you can see\nhere we have five clusters.\nSo probably what I would say is\nthat the cluster\n3 and the cluster\nzero are very very close.\nSo it might depend see\nthat's exactly what I\nwas going to say.\nIs that initially\nthe main Challenge\nand k-means clustering is\nto define the number of centers\nwhich are the K.\nSo as you can see here\nthat the third Center\nand the zeroth cluster\nthe third cluster\nand the zeroth cluster up\nvery very close to each other.\nSo guys It probably\ncould have been\nin one another cluster\nand the another disadvantage was\nthat we do not exactly know\nhow the points are\nto be arranged.\nSo it's very difficult to force\nthe data into any other cluster\nwhich makes our analysis\na little different works fine.\nBut sometimes it\nmight be difficult to code\nin the k-means clustering now,\nlet's understand what exactly is\nc means clustering.\nSo the fuzzy see means\nis an extension of the k-means\nclustering the popular simple.\nClustering technique so\nfuzzy clustering also referred\nas soft clustering is a form\nof clustering in which\neach data point can belong\nto more than one cluster.\nSo k-means tries to find\nthe heart clusters\nwhere each point belongs\nto one cluster.\nWhereas the fuzzy c means\ndiscovers the soft clusters\nin a soft cluster\nany point can belong\nto more than one cluster\nat a time with\na certain Affinity value\ntowards each 4zc means assigns\nthe degree of membership,\nwhich Just from 0 to 1\nto an object to a given cluster.\nSo there is a stipulation\nthat the sum of Z membership\nof an object to all the cluster.\nIt belongs to must be equal\nto 1 so the degree of membership\nof this particular point to pull\nof these clusters as 0.6 0.4.\nAnd if you add up we get 1\nso that is one of the logic\nbehind the fuzzy c means\nso and and this Affinity\nis proportional to the distance\nfrom the point to the center\nof a cluster now\nthen again We have the pros\nand cons of fuzzy see means.\nSo first of all,\nit allows a data point to be\nin multiple cluster.\nThat's a pro.\nIt's a more neutral\nrepresentation of the behavior\nof jeans jeans usually are\ninvolved in multiple functions.\nSo it is a very\ngood type of clustering\nwhen we're talking\nabout genes First of and again,\nif we talk about the cons again,\nwe have to Define c\nwhich is the number\nof clusters same as K next.\nWe need to determine the\nmembership cutoff value also,\nso that takes a lot of I'm\nand it's time-consuming\nand the Clusters\nare sensitive to initial\nassignment of centroid.\nSo a slight change\nor deviation from the center's\nit's going to result\nin a very different\nkind of, you know,\na funny kind of output with that\nfrom the fuzzy see means and one\nof the major disadvantage\nof c means clustering is\nthat it's this\na non-deterministic algorithm.\nSo it does not give you\na particular output as\nin such that's\nthat now let's have a look\nat At the throat type\nof clustering which is\nthe hierarchical clustering.\nSo hierarchical clustering\nis an alternative approach\nwhich builds a hierarchy\nfrom the bottom up\nor the top to bottom\nand does not require\nto specify the number\nof clusters beforehand.\nNow, the algorithm works\nas in first of all,\nwe put each data point\nin its own cluster and\nif I the closest to Cluster\nand combine them into one more\ncluster repeat the above step\ntill the data points are\nin a single cluster.\nNow, there are two types of\nhierarchical clustering one is\nI've number 80 plus string\nand the other one\nis division clustering.\nSo a cumulative clustering bills\nthe dendogram from bottom level\nwhile the division clustering\nit starts all the data points\nin one cluster\nthe fruit cluster now again\nhierarchical clustering also\nhas some sort of pros and cons.\nSo in the pros\ndon't know Assumption\nof a particular number\nof cluster is required\nand it may correspond\nto meaningful tax anomalies.\nWhereas if we talk\nabout the cons\nonce a decision is made\nto combine two clusters.\nIt cannot be undone and one\nof the major disadvantage of\nthese hierarchical clustering is\nthat it becomes very slow.\nIf we talked about very very\nlarge data sets and nowadays.\nI think every industry\nare using last year\nas it's and collecting\nlarge amounts of data.\nSo hierarchical clustering is\nnot the act or the best method\nsomeone might need\nto go for so there's\nthat Hello everyone\nand welcome to this interesting\nsession on a prairie algorithm.\nNow many of us have visited\nretails shops such as\nWalmart or Target\nfor our household needs.\nWell, let's say\nthat we are planning to buy\na new iPhone from Target.\nWhat we would typically do is\nsearch for the model by visiting\nthe mobile section of the stove\nand then select the product\nand head towards\nthe billing counter.\nBut in today's world the goal\nof the organization is\nto increase the revenue.\nCan this be done\nby just pitching one?\nI worked at a time\nto the customer.\nNow.\nThe answer to Is is clearly\nno hence organization began\nmining data relating\nto frequently bought items.\nSo a Market Basket analysis\nis one of the key techniques\nused by large retailers\nto uncover associations\nbetween items now examples\ncould be the customers\nwho purchase Bread have\na 60 percent likelihood\nto also purchase Jam customers\nwho purchase laptops are\nmore likely to purchase\nlaptop bags as well.\nThey try to find out\nassociations between different\nitems and products\nthat can be sold together\nwhich gives assisting\nin the right product placement.\nTypically, it figures out\nwhat products are\nbeing bought together\nand organizations can place\nproducts in a similar manner,\nfor example, people\nwho buy bread also\ntend to buy butter,\nright and the marketing team\nat retail stores\nshould Target customers\nwho buy bread and butter\nand provide an offer to them\nso that they buy a\nBut item suppose X\nso if a customer buys bread\nand butter and sees\na discount offer on X,\nhe will be encouraged\nto spend more and buy the eggs\nand this is what Market\nBasket analysis is all about.\nThis is what we are going\nto talk about in this session,\nwhich is Association rule Mining\nand the a prayer real Corinth\nnow Association rule\ncan be thought of as\nan if-then relationship\njust to elaborate on that.\nWe have come up\nwith a rule suppose\nif an item a is Been bought\nby the customer.\nThen the chances\nof Item B being picked\nby the customer to under\nthe same transaction ID is found\nout you need to understand here\nthat it's not a\ncash reality rather.\nIt's a co-occurrence pattern\nthat comes to the force.\nNow, there are two elements\nto this rule first if\nand second is the then now\nif is also known as antecedent.\nThis is an item\nor a group of items\nthat are typically\nfound in the item set\nand the later one.\nIs called the consequent\nthis comes along as an item\nwith an antecedent group\nor the group\nof antecedents a purchase.\nNow if we look\nat the image here a arrow B,\nit means that\nif a person buys an item a\nthen he will also buy an item b\nor he will most\nprobably by an item B.\nNow the simple example\nthat I gave you about\nthe bread-and-butter and the x\nis just a small example,\nbut what if you have thousands\nand thousands of items\nif you go to any proof\nadditional data scientist\nwith that data,\nyou can just imagine\nhow much of profit you can make\nif the data scientist provides\nyou with the right examples\nand the right placement\nof the items,\nwhich you can do and you\ncan get a lot of insights.\nThat is why Association\nrule mining is a very\ngood algorithm which helps\nthe business make profit.\nSo, let's see\nhow this algorithm works.\nSo Association rule mining is\nall about building the rules\nand we have just seen one rule\nthat If you buy a then\nthere's a slight possibility\nor there is a chance\nthat you might buy\nbe also this type\nof a relationship in which\nwe can find the relationship\nbetween these two items\nis known as single cardinality,\nbut what if the customer\nwho bought a and b also wants\nto buy C or if a customer\nwho bought a b and c\nalso wants to buy D. Then\nin these cases the cardinality\nusually increases\nand we can have a lot\nof combination around.\nThese data and\nif you have around 10,000\nor more than 10,000 data\nor items just imagine\nhow many rules you're going\nto create for each product.\nThat is why Association rule\nmining has such measures so\nthat we do not end up creating\ntens of thousands of rules.\nNow that is where the a priori\nalgorithm comes in.\nBut before we get\ninto the a priori algorithm,\nlet's understand.\nWhat's the maths behind it.\nNow there are three\ntypes of matrices.\nWhich help to\nmeasure the association?\nWe have support\nconfidence and lift.\nSo support is\nthe frequency of item a\nor the combination of item ARB.\nIt's basically the frequency\nof the items,\nwhich we have bought\nand what are the combination\nof the frequency of the item.\nWe have bought.\nSo with this what we can do\nis filter out the items,\nwhich have been\nbought less frequently.\nThis is one of the measures\nwhich is support now\nwhat confidence tells\nus so conference.\nGives us how often the items\nNB occur together given\nthe number of times a occur.\nNow this also helps us solve\na lot of other problems\nbecause if somebody is buying a\nand b together and not buying\nsee we can just rule out see\nat that point of time.\nSo this solves\nanother problem is\nthat we obviously do not need\nto analyze the process\nwhich people just by barely.\nSo what we can do is\naccording to the sages we\ncan Define our minimum support\nand confidence and when you\nhave set Values we can put\nthis values in the algorithm\nand we can filter\nout the data and we\ncan create different rules\nand suppose even\nafter filtering you have\nlike five thousand rules.\nAnd for every item we\ncreate these 5,000 rules.\nSo that's\npractically impossible.\nSo for that we need\nthe third calculation,\nwhich is the lift\nso lift is basically\nthe strength of any Rule now,\nlet's have a look\nat the denominator\nof the formula given here\nand if you see Here,\nwe have the independent\nsupport values of A and B.\nSo this gives us\nthe independent occurrence\nprobability of A and B.\nAnd obviously there's\na lot of difference\nbetween the random occurrence\nand Association and\nif the denominator\nof the lift is more\nwhat it means is\nthat the occurrence\nof Randomness is more\nrather than the occurs\nbecause of any association.\nSo left is the final verdict\nwhere we know\nwhether we have to spend time.\nOn this particular rule\nwhat we have got here or not.\nNow, let's have a look\nat a simple example\nof Association rule mining.\nSo suppose.\nWe have a set of items a b c d\nand e and a set\nof transactions T1 T2 T3 T4\nand T5 and as you can see here,\nwe have the transactions T1\nin which we have ABC T to a CD\nt3b CDT for a d e and T5 BCE.\nNow what we generally\ndo is create.\nAt some rules or Association\nrules such as a gives T\nor C gives a a gift C B\nand C gives a what this\nbasically means is\nthat if a person buys a then\nhe's most likely to buy D.\nAnd if a person by C,\nthen he's most likely\nto buy a and\nif you have a look\nat the last one,\nif a person buys B and C is\nmost likely to buy the item\na as well now if we calculate\nthe support confidence\nand lift using these rules\nas you can see here\nin the table,\nwe have the rule.\nAnd the support confidence\nhandle lift values.\nLet's discuss about a prairie.\nSo a priori algorithm\nuses the frequent itemsets\nto generate the association Rule\nand it is based on the concept\nthat subset of a frequent\nitemsets must also be\na frequent item set itself.\nNow this raises the question\nwhat exactly is\na frequent item set.\nSo a frequent item\nset is an item set\nwhose support value is greater\nthan the threshold value\njust now we discussed\nthat the marketing team\naccording to the says have\na minimum threshold value\nfor the confidence as\nwell as the support.\nSo frequent itemsets\nis that animset\nwho support value is greater\nthan the threshold value\nalready specified example,\nif A and B is a freaker item set\nThan A and B should also be\nfrequent itemsets individually.\nNow, let's consider\nthe following transaction\nto make the things\nsuch as easier suppose.\nWe have transactions 1\n2 3 4 5 and these\nItems out there.\nSo T 1 has 1 3 & 4 T\n2 has 2 3 and 5 T3 has\n1 2 3 5 T 4 to 5 and T 5 1 3 & 5\nnow the first step\nis to build a list\nof items sets of size 1 by\nusing this transactional data.\nAnd one thing to note here is\nthat the minimum support count\nwhich is given here is\nto Let's suppose it's too\nso the first step is\nto create item sets\nof size 1 and calculate\ntheir support values.\nSo as you can see here.\nWe have the table see one\nin which we have\nthe item sets 1 2 3 4 5\nand the support values\nif you remember\nthe formula of support,\nit was frequency divided by\nthe total number of occurrence.\nSo as you can see\nhere for the items\nthat one the support is 3\nas you can see here\nthat item set one up here s\nand t 1 T 3 and T 5.\nSo as you can see,\nit's frequency is 1 2 & 3 now\nas you can see the item set\nfor has a support of one\nas it occurs only once\nin Transaction one\nbut the minimum\nsupport value is 2\nthat's why it's going\nto be eliminated.\nSo we have the final table\nwhich is the table F1,\nwhich we have the item\nsets 1 2 3 and 5\nand we have the support values\n3 3 4 & 4 now the next step is\nto create Adam sets\nof size 2 and calculate\ntheir support values now\nall the combination\nof the item sets in the F1,\nwhich is the final table\nin which it is carded the for\nare going to be used\nfor this iteration.\nSo So we get the table c 2.\nSo as you can see here,\nwe have 1 2 1 3 1\n5 2 3 2 5 & 3 5 now\nif you calculate\nthe support here again,\nwe can see\nthat the item set 1 comma\n2 has a support of one\nwhich is again less\nthan the specified threshold.\nSo we're going to discard\nthat so if we have a look\nat the table f 2\nwe have 1 comma 3 1 5\n2 3 2 5 & 3 5 again,\nwe're going to move forward\nand create the atoms.\nThat of size 3 and calculate\nthis support values.\nNow all the combinations\nare going to be used\nfrom the item set F to\nfor this particular iterations.\nNow before calculating\nsupport values,\nlet's perform proning\non the data set.\nNow what is pruning now\nafter the combinations\nare being made we device\nc 3 item sets to check\nif there is another subset\nwhose support is less\nthan the minimum support value.\nThat is what frequent\nitems that means.\nSo if you have a look\nhere the item sets.\nWe have is 1 2 3 1 2 1\n3 2 3 4 the first one\nbecause as you can see here\nif we have a look\nat the subsets of one two,\nthree, we have\n1 comma 2 as well,\nso we are going to discard\nthis whole item set same goes\nfor the second one.\nWe have one to five.\nWe have 1/2 in that\nwhich was discarded\nin the previous set\nor the previous step.\nThat's why we're going\nto discard that also\nwhich leaves us\nwith only two factors,\nwhich is 1 3 5 8.\nI'm set and the two three five\nand the support for this is 2\nand 2 as well.\nNow if we create the table C\nfor using four elements,\nwe going to have\nonly one item set,\nwhich is 1 2 3 and 5 and\nif you have a look at the table\nhere the transaction table one,\ntwo, three and five\nappears only one.\nSo the support is one\nand since C for the support\nof the whole table C\n4 is less than 2 so\nwe're going to stop here\nand return to the\nprevious item set\nthat It is 3 3\nso the frequent itemsets have\n1 3 5 and 2 3 5 now let's assume\nour minimum confidence value is\n60 percent for that.\nWe're going to generate\nall the non-empty subsets\nfor each frequent itemsets.\nNow for I equals 1 comma\n3 comma 5 which is the item set.\nWe get the subset one three one\nfive three five one three\nand five similarly\nfor 2 3 5 we get\nto three to five\nthree five two three.\nand five now this rule states\nthat for every subset s\nof I the output of the rule\ngives something like s gives i2s\nthat implies s recommends I of s\nand this is only possible\nif the support of I divided\nby the support of s is greater\nthan equal to the minimum\nconfidence value now applying\nthese rules to the item set\nof F3 we get rule 1 which is 1 3\ngives 1 comma 3 comma 5 and 1/3\n3 it means 1 and 3 gives 5\nso the confidence is equal\nto the support of 1 comma\n3 comma fire driver support\nof 1 comma 3 that equals 2 by 3\nwhich is 66% and\nwhich is greater\nthan the 60 percent.\nSo the rule 1 is selected now\nif we come to rule 2\nwhich is 1 comma 5 it gives\n1 comma 3 comma 5 and 1 5\nit means if we have\n1 & 5 it implies.\nWe also going\nto have three know.\nCalculate the confidence\nof this one.\nWe're going to have support\n1 3 5 whereby support 1/5\nwhich gives us a hundred percent\nwhich means rule\n2 is selected as well.\nBut again if you have a look\nat rule 506 over here similarly,\nif it's select 3 gives\n1 3 5 & 3 it means\nif you have three,\nwe also get one and five.\nSo the confidence for this comes\nat 50% Which is less than\nthe given 60 percent Target.\nSo we're going to reject\nthis Rule and same.\nGoes for the rule number six.\nNow one thing to keep\nin mind here is\nthat all those are rule\n1 and Rule 5 look\na lot similar they are\nnot so it really depends\nwhat's on the left\nhand side of the arrow.\nAnd what's on the right-hand\nsides of the arrow.\nIt's the if-then possibility.\nI'm sure you guys can understand\nwhat exactly these rows are\nand how to proceed\nwith this rules.\nSo, let's see\nhow we can implement\nthe same in Python, right?\nSo for that what I'm going\nto do is create a new python.\nand I'm going to use\nthe chapter notebook.\nYou're free to use\nany sort of ID.\nI'm going to name\nit as a priority.\nSo the first thing\nwhat we're going to do\nis we will be using\nthe online transactional data\nof retail store for\ngenerating Association rules.\nSo firstly what we need to do\nis get the pandas and ml x\n10 libraries imported\nand read the file.\nSo as you can see here,\nwe are using the online\nretail dot xlsx format file\nand from ml extant.\nWe're going to import a prairie\nand Association rules at\nall comes under MX 10.\nSo as you can see here,\nwe have the invoice\nthe stock quote\nthe description the quantity\nthe invoice data unit\nprice customer ID\nand the country now\nnext in this step.\nWhat we're going to do\nis do data cleanup\nwhich includes removing\nthe spaces from some\nof the descriptions.\nAnd drop the rules\nthat do not have invoice\nnumbers and remove\nthe great grab transactions\nbecause that is of no use to us.\nSo as you can see here\nat the output in which\nwe have like five hundred\nand thirty two thousand rows\nwith eight columns.\nSo after the cleanup,\nwe need to consolidate the items\ninto one transaction per row\nwith each product for the sake\nof keeping the data set small.\nWe are only looking\nat the sales for France.\nSo as you can see here,\nwe have excluded all the other\nsays we're just looking\nat the sales for France.\nNow.\nThere are a lot\nof zeros in the data.\nBut we also need to make sure\nany positive values\nare converted to 1\nand anything less\nthan zero is set to 0\nso as you can see here,\nwe are still 392 Rose.\nWe're going to\nencode it and see.\nCheck again.\nNow that you have structured\nthe data properly in this step.\nWhat we're going to do is\ngenerate frequent itemsets\nthat have support at\nleast seven percent,\nbut this number is chosen\nso that you can get close enough\nand generated rules\nwith the corresponding\nsupport confidence and lift.\nSo go ahead you can see here.\nThe minimum support\nis 0.71 of what\nif we add another constraint\non the rules such as\nthe lift is greater than 6\nand the conference\nis greater than 0.8.\nSo as you can see here,\nwe have the left-hand side\nand the right-hand side\nof the association rule,\nwhich is the antecedent\nand the consequence.\nWe have the support.\nWe have the confidence\nto lift the leverage\nand the conviction.\nSo guys, that's it\nfor this session.\nThat is how you create\nAssociation rules using the API.\nReal gold tone which helps a lot\nin the marketing business.\nIt runs on the principle\nof Market Basket analysis,\nwhich is exactly what big\ncompanies like Walmart.\nYou have Reliance\nand Target to even Ikea does it\nand I hope you got\nto know what exactly is\nAssociation rule mining\nwhat is lift confidence\nand support and how to\ncreate Association rules.\nSo guys reinforcement learning.\nDying is a part\nof machine learning\nwhere an agent is put\nin an environment\nand he learns to behave\nin this environment\nby performing certain actions.\nOkay, so it basically performs\nactions and it either gets\na rewards on the actions\nor it gets a punishment\nand observing the reward\nwhich it gets from those actions\nreinforcement learning is all\nabout taking an appropriate\naction in order\nto maximize the reward\nin a particular situation.\nSo guys in supervised learning\nthe training data comprises\nof the input\nand the expected output\nAnd so the model is trained\nwith the expected output itself,\nbut when it comes\nto reinforcement learning,\nthere is no\nexpected output here.\nThe reinforcement agent\ndecides what actions\nto take in order to perform\na given task in the absence\nof a training data set.\nIt is bound to learn\nfrom its experience itself.\nAlright.\nSo reinforcement learning\nis all about an agent\nwho's put in\nan unknown environment\nand he's going to use\na hit and trial method\nin order to figure out\nthe environment and then come up\nwith an outcome.\nOkay.\nNow, let's look at it.\nReinforcement learning\nwithin an analogy.\nSo consider a scenario\nwhere in a baby is learning\nhow to walk the scenario\ncan go about in two ways.\nNow in the first case\nthe baby starts walking\nand makes it to the candy here.\nThe candy is basically\nthe reward it's going to get so\nsince the candy is\nthe end goal the baby is happy.\nIt's positive.\nOkay, so the baby is happy\nand it gets rewarded a set\nof candies now another way\nin which this could go is\nthat the baby starts walking\nbut Falls due to some hurdle\nin between The baby gets hot\nand it doesn't get any candy\nand obviously the baby is sad.\nSo this is a negative reward.\nOkay, or you can say\nthis is a setback.\nSo just like how we humans learn\nfrom our mistakes by trial\nand error reinforcement\nlearning is also similar.\nOkay, so we have an agent\nwhich is basically\nthe baby and a reward\nwhich is the candy over here.\nOkay, and with many hurdles\nin between the agent is supposed\nto find the best possible path\nto read through the reward.\nSo guys.\nI hope you all are clear with\nthe reinforcement learning now,\nlet's look at At the\nreinforcement learning process.\nSo generally a reinforcement\nlearning system has\ntwo main components, right?\nThe first is an agent\nand the second one\nis an environment.\nNow in the previous case,\nwe saw that the agent was\nthe baby and the environment\nwas the living room\nwhere in the baby was crawling.\nOkay.\nThe environment is the setting\nthat the agent is acting\non and the agent over here\nrepresents the reinforcement\nlearning algorithm.\nSo guys the reinforcement\nlearning process starts\nwhen the environment\nsends a state to the\nAnd then the agent\nwill take some actions based\non the observations\nin turn the environment\nwill send the next state\nand the respective reward\nback to the agent.\nThe agent will\nupdate its knowledge\nwith the reward returned by\nthe environment and it uses\nthat to evaluate\nits previous action.\nSo guys this\nLoop keeps continuing\nuntil the environment sends\na terminal state which means\nthat the agent has\naccomplished all his tasks\nand he finally gets the reward.\nOkay.\nThis is exactly\nwhat was depicted\nin this scenario.\nSo the agent keeps\nclimbing up ladders\nuntil he reaches his reward\nto understand this better.\nLet's suppose that our agent is\nlearning to play Counter Strike.\nOkay.\nSo let's break it down\nnow initially the RL agent\nwhich is basically\nthe player player 1.\nLet's say it's a player one\nwho is trying to learn\nhow to play the game.\nOkay.\nHe collects some state\nfrom the environment.\nOkay.\nThis could be the first date\nof Counter-Strike now based\non the state the agent\nwill take some action.\nOkay, and this action\ncan be anything\nthat causes a result.\nSo if the Almost left\nor right it's also\nconsidered as an action.\nOkay, so initially the action\nis going to be random\nbecause obviously the first time\nyou pick up Counter-Strike,\nyou're not going\nto be a master at it.\nSo you're going to try\nwith different actions\nand you just want to pick up a\nrandom action in the beginning.\nNow the environment is going\nto give a new state.\nSo after clearing\nthat the environment\nis now going to give a new state\nto the agent or to the player.\nSo maybe he's across th one now.\nHe's in stage 2.\nSo now the player\nwill get a reward\nour one from the environment.\nBecause it cleared stage 1.\nSo this reward can be anything.\nIt can be additional points\nor coins or anything like that.\nOkay.\nSo basically this Loop\nkeeps going on\nuntil the player is dead\nor reaches the destination.\nOkay, and it continuously\noutputs a sequence\nof States actions and rewards.\nSo guys, this was\na small example to show you\nhow reinforcement\nlearning process works.\nSo you start\nwith an initial State\nand once a player clothes\nthat state he gets a reward\nafter that the environment\nwill give another stage\nto the player.\nAnd after it clears that state\nit's going to get another award\nand it's going to keep happening\nuntil the player\nreaches his destination.\nAll right, so guys,\nI hope this is clear now,\nlet's move on and look\nat the reinforcement\nlearning definitions.\nSo there are a few Concepts\nthat you should be aware\nof while studying\nreinforcement learning.\nLet's look at those\ndefinitions over here.\nSo first we have the agent\nnow an agent is basically\nthe reinforcement learning\nalgorithm that learns\nfrom trial and error.\nOkay, so an agent takes actions\nlike For example a soldier\nin Counter-Strike navigating\nthrough the game.\nThat's also an action.\nOkay, if he moves left right\nor if he shoots at somebody\nthat's also an action.\nOkay.\nSo the agent is responsible\nfor taking actions\nin the environment.\nNow the environment is\nthe whole Counter-Strike game.\nOkay.\nIt's basically the world\nthrough which the agent\nmoves the environment takes\nthe agents current state\nand action as input\nand it Returns the agency reward\nand its next state as output.\nAlright next we have action\nnow all the possible.\nSteps that an agent\ncan take are called actions.\nSo like I said,\nit can be moving right left\nor shooting or any of that.\nAlright, then we have\nstate now state is\nbasically the current condition\nreturned by the environment.\nSo whichever State you are in\nif you are in state 1 or\nif you're in state\nto that represents\nyour current condition.\nAll right.\nNext we have reward a reward\nis basically an instant return\nfrom the environment\nto appraise Your Last Action.\nOkay, so it can be\nanything like coins\nor it can be audition.\nTwo points.\nSo basically a reward\nis given to an agent\nafter it clears\nthe specific stages.\nNext we have policy policies\nbasically the strategy\nthat the agent uses to find\nout his next action based\non his current state policy is\njust the strategy with which you\napproach the game.\nThen we have value.\nNow while you is\nthe expected long-term return\nwith discount so value\nin action value can be a little\nbit confusing for you right now,\nbut as we move further,\nyou'll understand\nwhat I'm talking.\nKima okay.\nSo value is basically\nthe long-term return\nthat you get with discount.\nOkay discount.\nI'll explain in\nthe furthest lines.\nThen we have action value\nnow action value\nis also known as Q value.\nOkay.\nIt's very similar to Value\nexcept that it takes\nan extra parameter,\nwhich is the current action.\nSo basically here you'll find\nout the Q value depending\non the particular action\nthat you took.\nAll right.\nSo guys don't get confused\nwith value and action value.\nWe look at examples\nin the further slides and you\nwill understand this better.\nOkay.\nSo guys make sure that you're\nfamiliar with these terms\nbecause you'll be seeing\na lot of these terms\nin the further slides.\nAll right.\nNow before we move any further,\nI'd like to discuss\na few more Concepts.\nOkay.\nSo first we will discuss\nthe reward maximization.\nSo if you haven't already\nrealized it the basic aim\nof the RL agent is\nto maximize the reward now,\nhow does that happen?\nLet's try to understand\nthis in depth.\nSo the agent must be\ntrained in such a way\nthat he takes the best action so\nthat the reward is\nBecause the end goal\nof reinforcement learning\nis to maximize your reward\nbased on a set of actions.\nSo let me explain this\nwith a small game now\nin the figure you can see\nthere is a fox there's some meat\nand there's a tiger\nso our agent is basically\nthe fox and his end goal\nis to eat the maximum amount\nof meat before being eaten\nby the tiger now\nsince the fox is a clever\nfellow he eats the meat\nthat is closer to him\nrather than the meat\nwhich is closer to the tiger.\nNow this is because the\ncloser he is to the tiger the\nhigher our his chances\nof getting killed.\nSo because of this the rewards\nwhich are near the tiger,\neven if they are\nbigger meat chunks,\nthey will be discounted.\nSo this is exactly\nwhat discounting means\nso our agent is not going\nto eat the meat chunks\nwhich are closer to the tiger\nbecause of the risk.\nAll right now,\neven though the meat chunks\nmight be larger.\nHe does not want to take\nthe chances of getting killed.\nOkay.\nThis is called discounting.\nOkay.\nThis is where you discount\nbecause it improvise\nand you just eat the meat\nwhich are closer to you\ninstead of taking risks\nand eating the meat\nwhich are The to your opponent.\nAll right.\nNow the discounting\nof reward Works based\non a value called gamma\nwill be discussing gamma\nin our further slides\nbut in short the value\nof gamma is between 0 and 1.\nOkay.\nSo the smaller the gamma the\nlarger is the discount value.\nOkay.\nSo if the gamma value is lesser,\nit means that the agent\nis not going to explore\nand he's not going\nto try and eat the meat chunks\nwhich are closer to the tiger.\nOkay, but if the gamma value\nis closer to 1 it means\nthat our agent is actually\nWe're going to explore\nand it's going to dry\nand eat the meat chunks\nwhich are closer to the tiger.\nAll right, now,\nI'll be explaining this\nin depth in the further slides.\nSo don't worry\nif you haven't got\na clear concept yet,\nbut just understand\nthat reward maximization is\na very important step\nwhen it comes\nto reinforcement learning\nbecause the agent has\nto collect maximum rewards\nby the end of the game.\nAll right.\nNow, let's look\nat another concept\nwhich is called exploration\nand exploitation.\nSo exploration like\nthe name suggests is\nabout exploring and capturing.\nMore information about\nan environment on the other\nhand exploitation is\nabout using the already\nknown exploited information\nto heighten the rewards.\nSo guys consider the fox\nand tiger example\nthat we discussed now here the\nfox eats only the meat chunks\nwhich are close to him,\nbut he does not eat\nthe meat chunks\nwhich are closer to the tiger.\nOkay, even though they\nmight give him more Awards.\nHe does not eat them\nif the fox only focuses\non the closest rewards,\nhe will never reach\nthe big chunks of meat.\nOkay, this is what\nexploitation is the\nabout you just going to use\nthe currently known information\nand you're going\nto try and get rewards based\non that information.\nBut if the fox decides\nto explore a bit,\nit can find the bigger award\nwhich is the big chunks of meat.\nThis is exactly\nwhat exploration is.\nSo the agent is not going\nto stick to one corner instead.\nHe's going to explore\nthe entire environment and try\nand collect bigger rewards.\nAll right, so guys,\nI hope you all are clear with\nexploration and exploitation.\nNow, let's look\nat the markers decision process.\nSo guys this is basically\na mathematical approach\nfor mapping a solution in\nreinforcement learning in a way.\nThe purpose of reinforcement\nlearning is to solve\na Markov decision process.\nOkay.\nSo there are a few parameters\nthat are used to get\nto the solution.\nSo the parameters include\nthe set of actions the set\nof states the rewards the policy\nthat you're taking to approach\nthe problem and the value\nthat you get.\nOkay, so to sum it up\nthe agent must take\nan action a to transition\nfrom a start state.\nThe end State s while doing\nso the agent will receive\na reward are for each action\nthat he takes.\nSo guys a series\nof actions taken by\nthe agent Define the policy\nor it defines the approach\nand the rewards\nthat are collected\nDefine the value.\nSo the main goal here is\nto maximize the rewards\nby choosing the optimum policy.\nAll right.\nNow, let's try to understand\nthis with the help\nof the shortest path problem.\nI'm sure a lot of you might\nhave gone through this problem\nwhen you are in college.\nSo guys look\nat the graph over here.\nSo our aim here is\nto find the shortest path\nbetween a and d\nwith minimum possible cost.\nSo the value that you see\non each of these edges\nbasically denotes the cost.\nSo if I want to go from a to c\nit's going to cost me 15 points.\nOkay.\nSo let's look at\nhow this is done.\nNow before we move\nand look at the problem\nin this problem the set of\nstates are denoted by the nodes,\nwhich is ABCD\nand the action is to Traverse\nfrom one node to the other.\nSo if I'm going from a Be\nthat's an action\nsimilarly a to see\nthat's an action.\nOkay, the reward is\nbasically the cost\nwhich is represented\nby each Edge over here.\nAll right.\nNow the policy is\nbasically the path\nthat I choose to\nreach the destination.\nSo let's say I choose\na seed be okay\nthat's one policy in order\nto get to D and choosing a CD\nwhich is a policy.\nOkay.\nIt's basically how\nI'm approaching the problem.\nSo guys here you\ncan start off at node a\nand you can take baby steps\nto your destination now\ninitially you're Clueless.\nSo you can just take\nthe next possible node,\nwhich is visible to you.\nSo guys if you're smart enough,\nyou're going to choose a\nto see instead of ABCD or ABD.\nAll right.\nSo now if you are\nat nodes see you want\nto Traverse to note D. You\nmust again choose a wise path\nor red you just have\nto calculate which path\nhas the highest cost\nor which path will give\nyou the maximum rewards.\nSo guys, this is\na simple problem.\nWe just drank to calculate\nthe shortest path between a\nand d by traversing\nthrough these nodes.\nSo if I travels from a CD it\ngives me the maximum reward.\nOkay, it gives me 65\nwhich is more than any other\npolicy would give me okay.\nSo if I go from ABD,\nit would be 40 when you\ncompare this to a CD.\nIt gives me more reward.\nSo obviously I'm going\nto go with a CB.\nOkay, so guys was\na simple problem\nin order to understand how\nMarkov decision process works.\nAll right, so guys,\nI want to ask you a question.\nWhat do you think?\nI did hear did\nI perform exploration\nor did I perform exploitation?\nNow the policy for the above\nexample is of exploitation\nbecause we didn't explore\nthe other nodes.\nOkay.\nWe just selected three notes\nand we Traverse through them.\nSo that's why this\nis called exploitation.\nWe must always explore\nthe different notes\nso that we can find\na more optimal policy.\nBut in this case, obviously\na CD has the highest reward\nand we're going with a CD,\nbut generally it's\nnot so simple.\nThere are a lot of nodes there\nhundreds of notes to Traverse\nand they're like 50 60 policies.\nOkay, 50 60 different policies.\nSo you make sure you explore.\nAll the policies and then decide\non an Optimum policy\nwhich will give you\na maximum reward.\nSo guys before we perform\nthe Hands-On part.\nLet's try to understand\nthe math behind our demo.\nOkay.\nSo in our demo will be using\nthe Q learning algorithm\nwhich is a type of reinforcement\nlearning algorithm.\nOkay, it's simple,\nit just means that if you\ntake the best possible actions\nto reach your goal\nor to get the most rewards.\nAll right, let's try to\nunderstand this with an example.\nSo guys, this is exactly\nwhat be running in In our demo,\nso make sure you\nunderstand this properly.\nOkay.\nSo our goal here is\nwe're going to place an agent\nin any one of the rooms.\nOkay.\nSo basically these squares\nyou see here our rooms.\nOK 0 is a room\nfor is a room three is\na room one is a room\nand 2:05 is also a room.\nIt's basically a way\noutside the building.\nAll right.\nSo what we're going to do is\nwe're going to place an agent\nin any one of these rooms\nand the goal is to reach\noutside the building.\nOkay outside.\nThe building is\nroom number five.\nOkay, so these are These spaces\nare basically doors,\nwhich means that you can go\nfrom zero to four.\nYou can go from 4\nto 3 3 to 1 1 to 5\nand similarly 3 to 2,\nbut you can't go\nfrom 5 to 2 directly.\nAll right, so there\nare certain set of rooms\nthat don't get\nconnected directly.\nOkay.\nSo like of mentioned here each\nroom is numbered from 0 to 4,\nand the outside of the building\nis numbered as five and one\nthing to note here\nis Room 1 and room\nfor directly lead\nto room number five.\nAll right.\nSo room number one and four\nwill directly lead out\nto room number five.\nSo basically our goal over here\nis to get to room number five.\nOkay to set this room\nas a goal will associate\na reward value to each door.\nOkay.\nDon't worry.\nI'll explain what I'm saying.\nSo if you re present these rooms\nin a graph this is\nhow the graph is going to look.\nOkay.\nSo for example from true,\nyou can go to three\nand then three two,\none one two five\nwhich will lead us to our goal\nthese arrows represent the link\nbetween the dose.\nNo, this is quite\nunderstandable now.\nOur next step is\nto associate a reward value\nto each of these doors.\nOkay, so the rooms\nthat are directly connected\nto our end room,\nwhich is room number five will\nget a reward of hundred.\nOkay.\nSo basically our room number\none will have a reward five now.\nThis is obviously\nbecause it's directly\nconnected to 5 similarly\nfor will also be associated\nwith a reward of hundred\nbecause it's directly\nconnected to 5.\nOkay.\nSo if you go out\nfrom for it will lead\nto five now the other know.\nRoads are not directly\nconnected to 5.\nSo you can't directly\ngo from 0 to 5.\nOkay.\nSo for this will be assigning\na reward of zero.\nSo basically other doors\nnot directly connected\nto the Target room\nhave a zero reward.\nOkay now because the doors\nare to weigh the two arrows\nare assigned to each room.\nOkay, you can see two arrows\nassigned to each room.\nSo basically zero leads to four\nand four leads back to 0 now.\nWe have assigned 0 0 over here\nbecause 0 does not directly\nlead to five but one\ndirectly leads to Five\nand that's why you can see\na hundred over here similarly\nfor directly leads\nto our goal State and\nthat's why we were signed\na hundred over here\nand obviously five two five\nis hundred as well.\nSo here all the direct\nconnections to room number\nfive are rewarded hundred\nand all the indirect connections\nare awarded zero.\nSo guys in q-learning the end\ngoal is to reach the state\nwith the highest reward\nso that the agent\narrives at the goal.\nOkay.\nSo let me just explain\nthis graph to you\nin detail now these These rooms\nover here labeled one, two,\nthree to five they represent\nthe state an agent is in so\nif I stay to one It means\nthat the agent is\nin room number one similarly\nthe agents movement\nfrom one room to the other\nrepresents the action.\nOkay.\nSo if I say one two, three,\nit represents an action.\nAll right.\nSo basically the state\nis represented as node\nand the action is represented\nby these arrows.\nOkay.\nSo this is what this graph is\nabout these nodes represent\nthe rooms and these Arrows\nrepresent the actions.\nOkay.\nLet's look at a small example.\nLet's set the initial\nstate to 0.\nSo my agent is placed\nin room number two,\nand he has to travel all the way\nto room number five.\nSo if I set the initial stage\nto to he can travel to State 3.\nOkay from three he\ncan either go to one\nor you can go back to to\nor you can go to for\nif he chooses to go to\nfor it will directly take\nhim to room number 5, okay,\nwhich is our end goal and even\nif he goes from room number\n3 2 1 it will take\nhim to room number.\nHigh five, so this is\nhow our algorithm works is going\nto drivers different rooms.\nIn order to reach\nthe Gold Room,\nwhich is room number 5.\nNow, let's try\nand depict these rewards\nin the form of a matrix.\nOkay, because we'll be\nusing this our Matrix\nor the reward Matrix to\ncalculate the Q value\nor the Q Matrix.\nOkay.\nWe'll see what the Q value is\nin the next step.\nBut for now,\nlet's see how this reward\nMatrix is calculated.\nNow the -\nones that you see\nin the table,\nthey represent the null values.\nNow these -1 basically means\nthat Wherever there is\nno link between nodes.\nIt's represented as minus 1 so 0\n2 0 is minus 1 0 to 1\nthere is no link.\nOkay, there's no direct\nlink from 0 to 1.\nSo it's represented as\nminus 1 similarly 0 to 2 or 2.\nThere is no link.\nYou can see there's\nno line over here.\nSo this is also minus 1,\nbut when it comes to 0 to 4,\nthere is a connection\nand we have numbered 0\nbecause the reward for a state\nwhich is not directly connected\nto the goal is zero,\nbut if you look\nat this 1 comma 5\nwhich is is basically traversing\nfrom Node 1 to node 5, you\ncan see the reward is hundred.\nOkay, that's basically\nbecause one and five\nare directly connected\nand five is our end goal.\nSo any node\nwhich will directly connected\nto our goal state will get\na reward of hundred.\nOkay.\nThat's why I've put hundred\nover here similarly.\nIf you look at the\nfourth row over here.\nI've assigned hundred over here.\nThis is because from 4 to 5\nthat is a direct connection.\nThere's a direct connection\nwhich gives them\na hundred reward.\nOkay, you can see from 4 to 5.\nThere is a direct link.\nOkay, so from room number\nfor to room number\nfive you can go directly.\nThat's why there's\na hundred reward over here.\nSo guys, this is\nhow the reward Matrix is made.\nAlright, I hope this\nis clear to you all.\nOkay.\nNow that we have\nthe reward Matrix.\nWe need to create another Matrix\ncalled The Q Matrix.\nOK here, you'll store\nor the Q values\nthat will calculate now\nthis Q Matrix basically\nrepresents the memory of\nwhat the agent has learned\nthrough experience.\nOkay.\nSo once he traverses\nfrom one room to the final room,\nwhatever he's learned.\nIt is stored in this Q Matrix.\nOkay, in order\nfor him to remember\nthat the next time he travels\nthis we use this Matrix.\nOkay.\nIt's basically like a memory.\nSo guys the rows of the Q Matrix\nwill represent the current state\nof the agent The Columns will\nrepresent the possible actions\nand to calculate the Q value\nuse this formula.\nAll right, I'll show you\nwhat the Q Matrix looks like,\nbut first, let's\nunderstand this formula.\nNow this Q value\nwill calculating because we\nwant to fill in the Q Matrix.\nOkay.\nSo this is basically a Matrix\nover here initially,\nit's all 0\nbut as the agent Traverse is\nfrom different nodes\nto the destination node.\nThis Matrix will get filled up.\nOkay.\nSo basically it will be\nlike a memory to the agent.\nHe'll know that okay,\nwhen he traversed using\na particular path,\nhe found out\nthat his value was maximum or\nas a reward was maximum of year.\nSo next time he'll\nchoose that path.\nThis is exactly what\nthe Q Matrix is.\nOkay.\nLet's go back now guys,\ndon't worry about\nthis formula for now\nbecause we'll be implementing\nthis formula in an example.\nIn the next slide.\nOkay, so don't worry\nabout this formula for now,\nbut here just remember\nthat this Q basically represents\nthe Q Matrix the r represents\nthe reward Matrix\nand the gamma is the gamma value\nwhich I'll talk about shortly\nand here you just finding out\nthe maximum from the Q Matrix.\nSo basically the gamma parameter\nhas a range from 0 to 1\nso you can have a value of\n0.1 0.3 0.5 0.8 and all of that.\nSo if the gamma is closer\nto zero it means\nThat the agent will consider\nonly the immediate\nrewards which means\nthat the agent will\nnot explore the surrounding.\nBasically, it won't\nexplore different rooms.\nIt will just choose\na particular room\nand then we'll try\nsticking to it.\nBut if the value of gamma\nis high meaning that\nif it's closer to one the agent\nwill consider future Awards\nwith greater weight.\nThis means that the agent\nwill explore all\nthe possible approaches\nor all the possible policies\nin order to get to the end goal.\nSo guys, this is what I\nwas talking about when I\nmention ation and exploration.\nAll right.\nSo if the gamma value is closer\nto 1 it basically means\nthat you're actually exploring\nthe entire environment\nand then choosing\nan Optimum policy.\nBut if your gamma value\nis closer to zero,\nit means that the agent\nwill only stick\nto a certain set of policies\nand it will calculate\nthe maximum reward based\non those policies.\nNow next.\nWe have the Q learning algorithm\nthat we're going to use\nto solve this problem.\nSo guys now this is going\nto look very confusing to y'all.\nSo let me just explain\nIn this with an example.\nOkay.\nWe'll see what we're actually\ngoing to run in our demo.\nWe will do the math behind it.\nAnd then I'll tell you what\nthis Q learning algorithm is.\nOkay, you'll understand it\nas I'm showing you the example.\nSo guys in the Q learning\nalgorithm the agent learns\nfrom his experience.\nOkay, so each episode,\nwhich is basically\nwhen the agents are traversing\nfrom an initial room\nto the end goal is equivalent\nto one training session\nand in every training session\nthe agent will explore\nthe environment it\nwill Receive some reward\nuntil it reaches the goal state\nwhich is five.\nSo there's a purpose\nof training is to enhance\nthe brain of our agent.\nOkay only if he knows\nthe environment very well,\nwill he know\nwhich action to take\nand this is why we calculate\nthe Q Matrix okay in Q Matrix,\nwhich is going to calculate\nthe value of traversing\nfrom every state to the end\nstate from every initial room\nto the end room.\nOkay, so when we\ncalculate all the values\nor how much reward\nwe're getting from each policy\nthat we We know\nthe optimum policy\nthat will give us\nthe maximum reward.\nOkay, that's why\nwe have the Q Matrix.\nThis is very important\nbecause the more\nyou train the agent\nand the more Optimum your output\nwill be so basically here\nthe agent will not perform\nexploitation instead.\nHe'll explore around\nand go back and forth\nthrough the different rooms\nand find the fastest\nroute to the goal.\nAll right.\nNow, let's look at an example.\nOkay.\nLet's see how\nthe algorithm works.\nOkay.\nLet's go back\nto the previous slide\nand Here it says\nthat the first step is\nto set the gamma parameter.\nOkay.\nSo let's do that.\nNow the first step\nis to set the value\nof the learning parameter,\nwhich is gamma and we\nhave randomly set it\nto zero point eight.\nOkay.\nThe next step is to initialize\nthe Matrix Q 2 0 Okay.\nSo we've set Matrix Q\n2 0 over here and then we\nwill select the initial stage\nOkay, the third step is select\na random initial State and here\nwe've selected the initial State\nas room number one.\nOkay.\nSo after you initialize\nthe matter Q as a zero Matrix\nfrom room number one,\nyou can either go to room number\nthree or number five.\nSo if you look\nat the reward Matrix can see\nthat from room number one,\nyou can only go to room number\nthree or room number five.\nThe other values\nare minus 1 here,\nwhich means that there is\nno link from 1 to 0 1\n2 1 1 2 2 and 1 to 4.\nSo the only possible actions\nfrom room number one is to go\nto room number 3 and to go\nto room number five.\nAll right.\nOkay.\nSo let's select\nroom number five, okay.\nSo from room number one,\nyou can go to 3 and 5 and we\nhave randomly selected five.\nYou can also select\nthree but for example,\nlet's select five over here.\nNow from Rome five,\nyou're going to calculate\nthe maximum Q value\nfor the next state based\non all possible actions.\nSo from number five,\nthe next state can be\nroom number one four or five.\nSo you're going to calculate\nthe Q value for traversing\n5 to 1 5 2 4 5 2 5\nand you're going to find out\nwhich has the maximum Q value\nand that's how you're going.\nCompute the Q value.\nSo let's Implement our formula.\nOkay, this is\nthe q-learning formula.\nSo right now we're traversing\nfrom room number\none to room number 5.\nOkay.\nThis is our state.\nSo here I've written\nQ 1 comma 5.\nOkay one represents\nour current state\nwhich is room number one.\nOkay.\nOur initial state was room\nnumber one and we are traversing\nto room number five.\nOkay.\nIt's shown in this figure room\nnumber 5 now for this we need\nto calculate the Q value\nnext in our formula.\nIt says the reward\nMatrix State and action.\nSo the reward Matrix for 1 comma\n5 let's look at 1 comma\n5 1 comma 5 corresponds\nto a hundred.\nOkay, so I reward over\nhere will be hundred so\nr 1 comma 5 is basically\nhundred then you're going\nto add the gamma value.\nNow the gamma value\nwill be initialized it\nto zero point eight.\nSo that's what we\nhave written over here.\nAnd we're going to multiply it\nwith the maximum value\nthat we're going to get\nfor the next date based\non all possible actions.\nOkay.\nSo from 5,\nthe next state is 1 4 and 5.\nSo if Travis from five to one\nthat's what I've written\nover here 5 to 4.\nYou're going to calculate the Q\nvalue of Fire 2 4 & 5 to 5.\nOkay.\nThat's what I\nmentioned over here.\nSo Q 5 comma 1 5 comma 4\nand 5 comma 5 are\nthe next possible actions\nthat you can take from State V.\nSo r 1 comma 5 is hundred.\nOkay, because from\nthe reward Matrix,\nyou can see that 1 comma\n5 is hundred 0.8 is the value\nof gamma after that.\nWe will calculate Q\nof 5 comma 1 5 comma\n4 and 5 comma 5 Like\nI mentioned earlier\nthat we're going to initialize\nMatrix Q as zero Matrix\nSo based setting the value of 0\nbecause initially obviously\nthe agent doesn't have\nany memory of what is happening.\nOkay, so he just\nstarting from scratch.\nThat's why all\nthese values are 0 so Q\nof 5 comma 1 will obviously\nbe 0 5 comma 4 would be 0\nand 5 comma 5 will also be zero\nand to find out the maximum\nbetween these it's obviously 0.\nSo when you\ncompute this equation,\nyou will get hundred so\nthe Q value of 1 comma 5 is\nSo if I agent goes from room\nnumber one to room number five,\nhe's going to have\na maximum reward\nor Q value of hundred.\nAll right.\nNow in the next\nslide you can see\nthat I've updated the value\nof Q of 1 comma 5.\nOkay, it said 200.\nAll right now similarly,\nlet's look at another example so\nthat you understand this better.\nSo guys, this is exactly\nwhat we're going\nto do in our demo.\nIt's only going to be coded.\nOkay.\nI'm just explaining\nour code right now.\nI'm just telling you\nthe math behind it.\nAlright now, let's look\nat another example.\nExample OK this time.\nWe'll start with a randomly\nchosen initial State.\nLet's say that\nwe've chosen State 3.\nOkay.\nSo from room 3,\nyou can either go\nto room number one two,\nor four randomly\nwill select room number\none and from room number one,\nyou're going to calculate\nthe maximum Q value\nfor the next state based\non all possible actions.\nSo the possible actions\nfrom one is to go to 3\nand to go to 5 now\nif you calculate the Q value\nusing this formula,\nso let me explain this\nto you once again now,\n3 comma 1 basically represents\nthat we're in room number\nthree and we are going\nto room number one.\nOkay.\nSo this represents our action?\nOkay.\nSo we're going from 3 to 1\nwhich is our action\nand three is our current state\nnext we will look at the reward\nof going from 3 to 1.\nOkay, if you go to the reward\nMatrix 3 comma 1 is 0 okay.\nNow this is\nbecause there's no direct link\nbetween three and five.\nOkay, so that's why\nthe reward here is zero.\nSo the value here will be 0\nafter that we have\nthe gamma value,\nwhich is zero point.\nEight and then we're going\nto calculate the Q Max\nof 1 comma 3 and 1 comma\n5 out of these whichever\nhas the maximum value\nwe're going to use that.\nOkay, so Q of 1 comma 3 is 0.\nAll right 0 you can see\nhere 1 comma 3 is 0\nand 1 comma 5 if you\nremember we just calculated\n1 comma 5 in the previous slide.\nOkay 1 comma 5 is hundred.\nSo here I'm going\nto put a hundred.\nSo the maximum here is hundred.\nSo 0.8 in 200 will give us c t\nso that's the Q value.\nGoing to get if you Traverse\nfrom three two one.\nOkay.\nI hope that was clear.\nSo now we have Travers\nfrom room number\nthree to room number\none with the reward of 80.\nOkay, but we still\nhaven't reached the end goal\nwhich is room number five.\nSo for our next episode\nthe state will be room.\nNumber one.\nSo guys, like I said,\nwe'll repeat this in a loop\nbecause room number\none is not our end goal.\nOkay, our end goal\nis room number 5.\nSo now we need to figure out\nhow to get from room number\none to room number 5.\nSo from room number one,\nyou can either either go\nto three or five.\nThat's what I've\ndrawn over here.\nSo if we select five we know\nthat it's our end goal.\nOkay.\nSo from room number 5,\nthen you have to calculate\nthe maximum Q value\nfor the next possible actions.\nSo the next possible actions\nfrom five is to go\nto room number one room number\nfour or room number five.\nSo you're going to calculate\nthe Q value of 5 to 1 5 2 4 & 5\n2 5 and find out\nwhich is the maximum Q value\nhere and you're going\nto use that value.\nAll right.\nSo let's look\nat the formula now now again,\nwe're in room number\none and Want to go\nto room number 5.\nOkay, so that's exactly\nwhat I've written here Q 1 comma\n5 next is the reward Matrix.\nSo reward of 1 comma\n5 which is hundred.\nAll right, then we have added\nthe gamma value which is 0.8.\nAnd then we're going\nto find the maximum Q value\nfrom 5 to 1 5 2 4 & 5 to 5.\nSo this is what\nwe're performing over here.\nSo 5 comma 1 5 comma 4\nand 5 comma 5 are all 0 this is\nbecause we initially set all\nthe values of the Q Matrix as 0\nso you get Hundred over here\nand the Matrix Remains the Same\nbecause we already\nhad calculated Q 1 comma 5\nso the value of 1 comma\n5 is already fed to the agent.\nSo when he comes back here,\nhe knows our okay.\nHe's already done\nthis before now.\nHe's going to try\nand Implement another method.\nOkay is going to try\nand take another route\nor another policy.\nSo he's going to try to go\nfrom different rooms\nand finally land up\nin room number 5,\nso guys, this is exactly\nhow our code runs.\nWe're going to Traverse\nthrough each and every node\nbecause we want an Optimum ball.\nSee, okay.\nAn Optimum policy\nis attained only\nwhen you Traverse\nthrough all possible actions.\nOkay.\nSo if you go through\nall possible actions\nthat you can perform only\nthen will you understand\nwhich is the best action\nwhich will lead us\nto the reward.\nI hope this is clear now,\nlet's move on\nand look at our code.\nSo guys, this is our code\nand this is executed in Python\nand I'm assuming\nthat all of you have\na good background in Python.\nOkay, if you don't understand\npython very well.\nI'm going to leave a link\nin the description.\nYou can check out\nthat video on Python\nand then maybe come\nback to this later.\nOkay, but I'll be explaining\nthe code to you anyway,\nbut I'm not going to spend a lot\nof time explaining each\nand every line of code\nbecause I'm assuming\nthat you know python.\nOkay.\nSo let's look at the first line\nof code over here.\nSo what we're going to do is\nwe're going to import numpy.\nOkay numpy is basically\na python library\nfor adding support for\nlarge multi-dimensional arrays\nand matrices and it's\nbasically for computing\nmathematical functions.\nOkay so first Want to import\nthat after that we're going\nto create the our Matrix.\nOkay.\nSo this is the our Matrix next\nwe're going to create a q Matrix\nand it's a 6 into 6 Matrix\nbecause obviously we have\nsix states starting from 0 to 5.\nOkay, and we are going\nto initialize the value to zero.\nSo basically the Q Matrix\nis going to be initialized\nto zero over here.\nAll right,\nafter that we're setting\nthe gamma parameter to 0.8.\nSo guys you can play\nwith this parameter\nand you know move it\nto 0.9 or movement logo to 0.8.\nOkay, you can see see\nwhat happens then then\nwe'll set an initial stage.\nOkay initial stage\nis set as 1 after that.\nWe're defining a function\ncalled available actions.\nOkay.\nSo basically what\nwe're doing here is\nsince our initial state is one.\nWe're going to check\nour row number one.\nOkay, this is\nour own number one.\nOkay.\nThis is wrong number zero.\nThis is zero number\none and so on.\nSo we're going to check the row\nnumber one and we're going\nto find the values\nwhich are greater\nthan or equal to 0\nbecause these values\nbasically The nodes that\nwe can travel to now\nif you select minus 1\nyou can Traverse 2-1.\nOkay, I explained\nthis earlier the -\none represents all the nodes\nthat we can travel to but we\ncan travel to these nodes.\nOkay.\nSo basically over here\na checking all the values\nwhich are equal to 0\nor greater than 0 these\nwill be our available actions.\nSo if our initial state is one\nwe can travel to other states\nwhose value is equal to 0\nor greater than 0\nand this is stored\nin this variable called.\nAll available act right now.\nThis will basically get\nthe available actions\nin the current state.\nOkay.\nSo we're just storing\nthe possible actions\nin this available\nact variable over here.\nSo basically over here\nsince our initial state is\none we're going to find out\nthe next possible States\nwe can go to okay\nthat is stored\nin the available act variable.\nNow next is this function\nchooses at random which action\nto be performed\nwithin the range.\nSo if you remember over here,\nso guys initially we\nare in stage number.\nOkay are available actions is\nto go to stage number\n3 or stage number five.\nSorry room number\n3 or room number 5.\nOkay.\nNow randomly, we need\nto choose one room.\nSo for that using\nthis line of code, okay.\nSo here we are randomly going\nto choose one of the actions\nfrom the available act\nthis available act.\nLike I said earlier stores\nall our possible actions.\nOkay from the initial State.\nOkay.\nSo once it chooses an action\nis going to store it\nin next action,\nso guys this action will Present\nthe next available action to\ntake now next is our Q Matrix.\nRemember this formula\nthat we used.\nSo guys this formula\nthat we use is\nwhat we are going to calculate\nin the next few lines of code.\nSo in this block of code,\nwhich is executing\nand Computing the value of Q.\nOkay, this is our formula\nfor computing the value\nof Q current state Karma action.\nOur current state Karma action\ngamma into the maximum value.\nSo here basically\nwe're going to calculate\nthe maximum index meaning\nthat To be going to check\nwhich of the possible\nactions will give us\nthe maximum Q value read\nif you remember\nin our explanation over here\nthis value over here Max Q\nor five comma 1 5 comma 4\nand 5 comma 5 we had\nto choose a maximum Q value\nthat we get from these three.\nSo basically that's exactly\nwhat we're doing\nin this line of code,\nthe calculating the index\nwhich gives us the maximum value\nafter we finish Computing\nthe value of Q will just\nhave to update our Matrix.\nAfter that, we'll be\nupdating the Q value\nand will be choosing\na new initial State.\nOkay.\nSo this is the update function\nthat is defined over here.\nOkay.\nSo I've just called\nthe function over here.\nSo guys this whole set of code\nwill just calculate the Q value.\nOkay.\nThis is exactly what we did\nin our examples after that.\nWe have the training phase.\nSo guys remember the more\nyou train an algorithm the\nbetter it's going to learn.\nOkay so over here\nI have provided\naround 10,000 titrations.\nOkay.\nSo my range is\n10 thousand iterations meaning\nthat my age It will take\n10,000 possible scenarios\nand in go to 10,000 titrations\nto find out the best policy.\nSo you're exactly\nwhat I'm doing is I'm choosing\nthe current state\nrandomly after that.\nI'm choosing the available\naction from the current state.\nSo either I can go to stage\n3 or straight five then\nI'm calculating the next action\nand then I'm finally\nupdating the value\nin the Q Matrix and next.\nWe just normalize the Q Matrix.\nSo sometimes in our Q Matrix\nthe value might exceed.\nOkay, let's say it.\nHeated to 500 600 so\nthat time you want\nto normalize The Matrix.\nOkay, we want to bring\nit down a little bit.\nOkay, because larger numbers\nwe won't be able to understand\nand computation would be\nvery hard on larger numbers.\nThat's why we\nperform normalization.\nYou're taking your calculated\nvalue and you're dividing it\nwith the maximum Q value in 200.\nAll right, so you\nare normalizing it over here.\nSo guys, this is\nthe testing phase.\nOkay here you will just randomly\nset a current state and you\nwant given any other data\nbecause you've already\ntrained our model.\nOkay, you're To give\na Garden State then\nyou're going to tell your agent\nthat listen you're in room.\nNumber one.\nNow.\nYou need to go\nto room number five.\nOkay, so he has to figure out\nhow to go to room number 5\nbecause we have trained him now.\nAll right.\nSo here we have set\nthe current state to one\nand we need to make sure\nthat it's not equal to 5\nbecause 5 is the end goal.\nSo guys this is the same Loop\nthat we executed earlier.\nSo we're going to do\nthe same I trations again now\nif I run this entire code,\nlet's look at the result.\nSo our current state\nhere we've chosen as one.\nOkay and And if we go\nback to our Matrix,\nyou can see that there is\na direct link from 1 to 5,\nwhich means that the route\nthat the agent should\ntake is one to five.\nOkay directly.\nYou should go from 1 to 5\nbecause it will get\nthe maximum reward like that.\nOkay.\nLet's see if that's happening.\nSo if I run this it should give\nme a direct path from 1 to 5.\nOkay, that's exactly\nwhat happened.\nSo this is the selected path\nso directly from one to five\nit went and it calculated\nthe entire Q Matrix.\nWorks for me.\nSo guys this is exactly\nhow it works.\nNow.\nLet's try to set\nthe initial stage\nas that's a to so\nif I set the initial stage as\nto and if I try to run the code,\nlet's see the path\nthat it gives so\nthe selected path is\n2 3 4 5 now chose this path\nbecause it's giving\nus the maximum reward\nfrom this path.\nOkay.\nThis is the Q Matrix\nthat are calculated\nand this is the selected path.\nAll right, so guys with this we\ncome to the end of this demo.\nSo basically what we did\nwas we just placed an agent\nin a room random room\nand we ask it to Traverse\nthrough and reach\nto the end room,\nwhich is room number five.\nSo basically we trained\nour agent and we made sure\nthat it went through all\nthe possible paths.\nto calculate the best\npath the for a robot\nand environment is a place\nwhere it has been put to use.\nNow.\nRemember this reward is\nitself the agent for example\nan automobile Factory\nwhere a robot is used\nto move materials\nfrom one place to another now\nthe task we discussed just now\nhave a property in common.\nNow, these tasks involve\nand environment and expect\nthe agent to learn\nfrom the environment.\nNow, this is where traditional\nmachine learning phase\nand hence the need\nfor reinforcement learning now,\nit is good to have Establish\noverview of the problem\nthat is to be solved\nusing the Q learning\nor the reinforcement learning.\nSo it helps to define\nthe main components\nof a reinforcement\nlearning solution.\nThat is the agent environment\naction rewards and States.\nSo let's suppose we are to build\na few autonomous robots for\nan automobile building Factory.\nNow, these robots will help\nthe factory personal\nby conveying them\nthe necessary parts\nthat they would need\nin order to pull the car.\nNow these different\nparts are located\nat Nine different positions\nwithin the factory warehouse\nthe car part include the chassis\nWheels dashboard the engine\nand so on and\nthe factory workers\nhave prioritized the location\nthat contains the body\nor the chassis to be\nthe topmost but they\nhave provided the priorities\nfor other locations as well,\nwhich will look into the moment.\nNow these locations\nwithin the factory look\nsomewhat like this.\nSo as you can see here,\nwe have L1 L2 L3\nall of these stations.\nNow one thing you\nmight notice here\nthat there are little obstacle\nprison in between the locations.\nSo L6 is the top\npriority location\nthat contains the chassis\nfor preparing the car bodies.\nNow the task is\nto enable the robots\nso that they can find\nthe shortest route\nfrom any given location to\nanother location on their own.\nNow the agents in this case are\nthe robots the environment is\nthe automobile factory\nwarehouse the let's talk\nabout the state's the states.\nAre the location in which\na particular robot is present\nin the particular\ninstance of time\nwhich will denote it states\nthe machines understand numbers\nrather than let us so let's map\nthe location codes to number.\nSo as you can see here,\nwe have map location l\n1 to this t 0 L 2 and 1\nand so on we have L8 as\nstate 7 + L line at state.\nSo next what we're going to talk\nabout are the actions.\nSo in our example,\nthe action will be the direct\nlocation that a robot can.\nCall from a particular location,\nright consider a robot\nthat is a tel to location\nand the Direct locations\nto which it can move\nour L5 L1 and L3.\nNow the figure here may come\nin handy to visualize this now\nas you might have already\nguessed the set of actions\nhere is nothing but the set\nof all possible states\nof the robot for each location\nthe set of actions\nthat a robot can take\nwill be different.\nFor example, the set\nof actions will change\nif the robot is.\nAn L1 rather than L2.\nSo if the robot is in L1,\nit can only go to L\n4 and L 2 directly now\nthat we are done with the states\nand the actions.\nLet's talk about the rewards.\nSo the states are\nbasically zero one two,\nthree four and the\nactions are also 0 1\n2 3 4 up till 8:00.\nNow, the rewards now\nwill be given to a robot.\nIf a location\nwhich is the state\nis directly reachable\nfrom a particular location.\nSo let's take an example\nsuppose l Lane is\ndirectly reachable from L8.\nRight?\nSo if a robot goes from LA\nto align and vice versa,\nit will be rewarded by one\nand if a location is\nnot directly reachable\nfrom a particular equation.\nWe do not give any reward\na reward of 0 now the reward\nis just a number\nand nothing else it enables\nthe robots to make sense\nof the movements helping them\nin deciding what locations\nare directly reachable\nand what are not now\nwith this Q. We\ncan construct a reward table\nwhich contains all the required.\nUse mapping between\nall possible States.\nSo as you can see here\nin the table the positions\nwhich are marked green\nhave a positive reward.\nAnd as you can see here,\nwe have all the possible rewards\nthat a robot can get by moving\nin between the different states.\nNow comes an\ninteresting decision.\nNow remember that the factory\nadministrator prioritized L6\nto be the topmost.\nSo how do we incorporate this\nfact in the above table now,\nthis is done by associating\nthe topmost priority location\nwith a very high reward.\nThe usual ones so let's put 999\nin the cell L 6 comma\nand six now the table\nof rewards with a higher reward\nfor the topmost location\nlooks something like this.\nWe have not formally defined\nall the vital components\nfor the solution.\nWe are aiming for\nthe problem discussed now,\nwe will shift gears\na bit and study some\nof the fundamental concepts\nthat Prevail in the world\nof reinforcement learning\nand q-learning the first\nof all we'll start\nwith the Bellman equation now\nconsider the following Square.\nRooms, which is analogous\nto the actual environment\nfrom our original problem.\nBut without the barriers now\nsuppose a robot needs to go\nto the room marked\nin the green\nfrom its current position a\nusing the specified Direction.\nNow, how can we enable the robot\nto do this programmatically\none idea would be introduced\nsome kind of a footprint\nwhich the robot will be able\nto follow now here\na constant value is specified\nin each of the rooms,\nwhich will come\nalong the robots way\nif it follows the directions\nby Fight about now in this way\nif it starts at location\na it will be able to scan\nthrough this constant value\nand will move accordingly\nbut this will only work\nif the direction is prefix\nand the robot always starts\nat the location a now\nconsider the robot starts\nat this location rather\nthan its previous one.\nNow the robot\nnow sees Footprints\nin two different directions.\nIt is therefore unable\nto decide which way to go\nin order to get the destination\nwhich is the Green Room.\nIt happens.\nPrimarily because the robot\ndoes not have a way to remember\nthe directions to proceed.\nSo our job now is to enable\nthe robot with a memory.\nNow, this is where the Bellman\nequation comes into play.\nSo as you can see here,\nthe main reason\nof the Bellman equation\nis to enable the reward\nwith the memory.\nThat's the thing\nwe're going to use.\nSo the equation goes\nsomething like this V\nof s gives maximum a r\nof s comma a plus gamma of vs -\nwhere s is a particular state\nWhich is a room is\nthe Action Moving\nbetween the rooms as -\nis the state to which\nthe robot goes from s\nand gamma is the discount Factor\nnow we'll get\ninto it in a moment\nand obviously R of s comma\na is a reward function\nwhich takes a state as an action\na and outputs the reward now V\nof s is the value of being\nin a particular state\nwhich is the footprint\nnow we consider all\nthe possible actions\nand take the one\nthat yields the maximum value.\nNow there is one constraint.\nHowever regarding\nthe value footprint\nthat is the room marked\nin the yellow just\nbelow the Green Room.\nIt will always have\nthe value of 1 to denote\nthat is one of the nearest room\nadjacent to the green room.\nNow.\nThis is also to ensure\nthat a robot gets a reward\nwhen it goes from a yellow room\nto The Green Room.\nLet's see how to make\nsense of the equation\nwhich we have here.\nSo let's assume\na discount factor of 0.9\nas remember gamma is\nthe discount value\nor the discount Factor.\nSo let's Take a 0.9.\nNow for the room,\nwhich is Mark just below the one\nor the yellow room,\nwhich is the Aztec Mark\nfor this room.\nWhat will be the V of s\nthat is the value of being\nin a particular state?\nSo for this V of s\nwould be something\nlike maximum of a will take 0\nwhich is the initial\nof our s comma.\nHey plus 0.9\nwhich is gamma into 1\nthat gives us zero point\nnine now here the robot\nwill not get any reward\nfor Owing to a state marked\nin yellow hence the IR s\ncomma a is 0 here\nbut the robot knows the value\nof being in the yellow room.\nHence V of s Dash is\none following this\nfor the other states.\nWe should get 0.9 then again,\nif we put 0.9 in this equation,\nwe get 0.81 then zero point\nseven to nine and then we again\nreached the starting point.\nSo this is\nhow the table looks with\nsome value Footprints computer.\nFrom the Bellman equation now\na couple of things\nto notice here is\nthat the max function\nhas the robot to always\nchoose the state\nthat gives it the maximum value\nof being in that state now\nthe discount Factor\ngamma notifies the robot\nabout how far it is\nfrom the destination.\nThis is typically specified by\nthe developer of the algorithm.\nThat would be installed\nin the robot.\nNow, the other states can also\nbe given their respective values\nin a similar way.\nSo as you can see here the boxes\nInto the green one have one and\nif we move away from one we\nget 0.9 0.8 1 0 1 7 to 9.\nAnd finally we reach\n0.66 now the robot now\ncan precede its way\nthrough the Green Room utilizing\nthese value Footprints event\nif it's dropped\nat any arbitrary room\nin the given location now,\nif a robot Lance up in\nthe highlighted Sky Blue Area,\nit will still find\ntwo options to choose\nfrom but eventually\neither of the parties.\nIt's will be good enough\nfor the robot to take\nbecause Auto V\nthe value Footprints\nare not only that out.\nNow one thing to note is\nthat the Bellman equation is one\nof the key equations\nin the world of reinforcement\nlearning and Q learning.\nSo if we think realistically our\nsurroundings do not always work\nin the way we expect\nthere is always a bit\nof stochastic City\ninvolved in it.\nSo this applies\nto robot as well.\nSometimes it might so happen\nthat the robots\nMachinery got corrupted.\nSometimes the robot makes come\nacross some hindrance on its way\nwhich may not be known\nto it beforehand.\nRight and sometimes even\nif the robot knows\nthat it needs to take\nthe right turn it will not so\nhow do we introduce\nthis to cast a city\nin our case now here comes\nthe Markov decision process\nnow consider the robot is\ncurrently in the Red Room\nand it needs to go\nto the green room.\nNow.\nLet's now consider\nthe robot has a slight chance\nof dysfunctioning\nand might take the left\nor the right or the bottom.\nOn instead updating\nthe upper turn in order to get\nto The Green Room\nfrom where it is now,\nwhich is the Red Room.\nNow the question is,\nhow do we enable the robot\nto handle this when it is out\nin the given environment right.\nNow, this is a situation\nwhere the decision making\nregarding which turn is\nto be taken is partly random\nand partly another control\nof the robot now partly random\nbecause we are not sure\nwhen exactly the robot mind\ndysfunctional and partly\nunder the control of the robot\nbecause it is still\nMaking a decision\nof taking a turn right\non its own and with the help\nof the program embedded into it.\nSo a Markov decision process\nis a discrete time\nstochastic Control process.\nIt provides a mathematical\nframework for modeling\ndecision-making in situations\nwhere the outcomes\nare partly random\nand partly under control\nof the decision maker.\nNow we need to give this concept\na mathematical shape most\nlikely an equation\nwhich then can be taken\nfurther now you might be Price\nthat we can do this\nwith the help\nof the Bellman equation\nwith a few minor tweaks.\nSo if we have a look\nat the original Bellman equation\nV of X is equal to maximum\nof our s comma a plus\ngamma V of s stash\nwhat needs to be changed\nin the above equation\nso that we can introduce\nsome amount of Randomness\nhere as long as we are not sure\nwhen the robot might not take\nthe expected turn.\nWe are then also not sure\nin which room it might end up\nin which is nothing\nbut the room it.\nMoves from its current\nroom at this point\naccording to the equation.\nWe are not sure of the S stash\nwhich is the next state\nor the room,\nbut we do know all the probable\nturns the reward might take now\nin order to incorporate each\nof this probabilities\ninto the above equation.\nWe need to associate\na probability with each\nof the turns to\nquantify the robot\nif it has got any experts it is\nchance of taking this turn now\nif we do,\nso We get PS is equal to maximum\nof our s comma a plus gamma\ninto summation of s -\nPS comma a comma s stash into V\nof his stash now the PS a--\nand a stash is the probability\nof moving from room s\nto establish with the action a\nand the submission\nhere is the expectation\nof the situation that\nthe robot in curse,\nwhich is the randomness now,\nlet's take a look\nat this example here.\nSo when We associate\nthe probabilities to each\nof these Stones.\nWe essentially mean\nthat there is an 80% chance\nthat the robot will\ntake the upper turn.\nNow, if you put all\nthe required values\nin our equation,\nwe get V of s is equal\nto maximum of our of s comma a +\ncomma of 0.8 into V\nof room up plus 0.1\ninto V of room down 0.03\ninto a room of V of from left\nplus 0.03 into Vo Right now note\nthat the value Footprints\nwill not change due to the fact\nthat we are incorporating\nstochastic Ali here.\nBut this time we\nwill not calculate\nthose values Footprints instead.\nWe will let the robot\nto figure it out.\nNow up until this point.\nWe have not considered\nabout rewarding the robot\nfor its action of going\ninto a particular room.\nWe are only watering the robot\nwhen it gets\nto the destination now,\nideally there should be a reward\nfor each action the robot\ntakes to help it better\nas Assess the quality\nof the actions,\nbut there was need\nnot to be always be the same\nbut it is much better\nthan having some amount\nof reward for the actions\nthan having no rewards at all.\nRight and this idea is known as\nthe living penalty in reality.\nThe reward system\ncan be very complex\nand particularly modeling\nsparse rewards is an active area\nof research in the domain\nof reinforcement learning.\nSo by now we have\ngot the equation\nwhich we have a so what?\nTo do is now transition\nto Q learning.\nSo this equation gives\nus the value of going\nto a particular State\ntaking the stochastic city\nof the environment into account.\nNow, we have also learned\nvery briefly about the idea\nof living penalty\nwhich deals with associating\neach move of the robot\nwith a reward so\nQ learning processes\nand idea of assessing\nthe quality of an action\nthat is taken to move\nto a state rather than\ndetermining the possible\nvalue of the state\nwhich is being moved\nto So earlier we had 0.8\ninto V of s 1 0.03 into V\nof S 2 0 point 1 into V\nof S 3 and so on now\nif you incorporate the idea\nof assessing the quality\nof the action for moving\nto a certain state\nso the environment\nwith the agent\nand the quality of the action\nwill look something like this.\nSo instead of 0.8 V\nof s 1 will have q of s\n1 comma a one will have q\nof S 2 comma 2 You\nof S3 not the robot now has\nfour different states to choose\nfrom and along with that.\nThere are four different actions\nalso for the current\nstate it is in so\nhow do we calculate Q of s comma\na that is the cumulative quality\nof the possible actions\nthe robot might take so\nlet's break it down.\nNow from the equation V of s\nequals maximum a RS comma a +\ncomma summation s -\nPSAs stash -\ninto V of s -\nif we discard the maximum\nfunction we have is\nof a plus gamma into summation p\nand v now essentially\nin the equation\nthat produces V\nof s we are considering\nall possible actions\nand all possible States\nfrom the current state\nthat the robot is in\nand then we are taking\nthe maximum value caused\nby taking a certain action\nand the equation produces\na value footprint,\nwhich is for just\none possible action.\nIn fact if we can think\nof it as the quality\nof the action so\nQ of s comma a is equal\nto RS comma a plus gamma\nof summation p and v now\nthat we have got an equation\nto quantify the quality\nof a particular action.\nWe are going to make\na little adjustment\nin the equation we can now say\nthat we of s is the maximum\nof all the possible values\nof Q of s comma a right.\nSo let's utilize this fact\nand replace V of s\nStash as a function\nof Q so q s comma\na becomes R of s comma a\n+ comma of summation PSAs -\nand maximum of the que es -\na - so the equation\nof V is now turned\ninto an equation of Q,\nwhich is the quality.\nBut why would we do that now?\nThis is done to\nease our calculations\nbecause now we have\nonly one function Q,\nwhich is also the core\nof the Programming language.\nWe have only one function Q\nto calculate an R of s comma\na is a Quantified metric\nwhich produces reward\nof moving to a certain State.\nNow, the qualities\nof the actions are\ncalled The Q values\nand from now on we will refer\nto the value Footprints\nas the Q values\nan important piece\nof the puzzle is\nthe temporal difference.\nNow temporal difference\nis the component\nthat will help the robot\ncalculate the Q values\nwhich respect to the change.\nChanges in the\nenvironment over time.\nSo consider our robot is\ncurrently in the mark State\nand it wants to move\nto the Upper State.\nOne thing to note that here is\nthat the robot already knows\nthe Q value of making the action\nthat is moving through\nthe Upper State and we know\nthat the environment\nis stochastic in nature\nand the reward\nthat the robot will get\nafter moving to the Upper State\nmight be different\nfrom an earlier observation.\nSo how do we capture\nthis change the real difference?\nWe calculate the new Q as My a\nwith the same formula\nand subtract the previous you\nknown qsa from it.\nSo this will in turn give us\nthe new QA now the equation\nthat we just derived gifts\nthe temporal difference\nin the Q values\nwhich further helps\nto capture the random changes\nin the environment\nwhich may impose now\nthe new q s comma a\nis updated as the following\nso Q T of s comma is equal\nto QT minus 1 s comma\na plus Alpha TD.\nET of a comma s now here\nAlpha is the learning\nrate which controls\nhow quickly the robot adapts\nto the random changes imposed\nby the environment the qts comma\nis the current state q value\nand a QT minus 1 s comma is\nthe previously recorded Q value.\nSo if we replace the TDS comma a\nwith its full form equation,\nwe should get Q T of s\ncomma is equal to QT -\n1 of s comma y plus Alpha\ninto our of S comma\na plus gamma maximum\nof q s Dash a dash minus QT\nminus 1 s comma a now\nthat we have all the little\npieces of q line together.\nLet's move forward\nto its implementation part.\nNow, this is the final equation\nof q-learning, right?\nSo, let's see\nhow we can implement this\nand obtain the best path\nfor any robot to take now\nto implement the algorithm.\nWe need to\nunderstand the warehouse.\nIan and how that can be mapped\nto different states.\nSo let's start by reconnecting\nthe sample environment.\nSo as you can see here,\nwe have L1 L2 L3 to align\nand as you can see here,\nwe have certain borders also.\nSo first of all,\nlet's map each of the above\nlocations in the warehouse\ntwo numbers or the states\nso that it will ease\nour calculations, right?\nSo what I'm going to do is\ncreate a new Python 3 file\nin the jupyter notebook\nand I'll name it as\nlearning Numb, but\nokay, so let's\ndefine the states.\nBut before that what we\nneed to do is import numpy\nbecause we're going to use numpy\nfor this purpose and let's\ninitialize the parameters.\nThat is the gamma\nand Alpha parameters.\nSo gamma is 0.75,\nwhich is the discount Factor\nwhereas Alpha is 0.9,\nwhich is the learning rate.\nNow next what we're going to do\nis Define the states and map\nit to numbers.\nSo as I mentioned earlier\nl 1 is Zero and online.\nWe have defined the states\nin the numerical form.\nNow.\nThe next step is to define\nthe actions which is\nas mentioned above\nrepresents the transition\nto the next state.\nSo as you can see here,\nwe have an array\nof actions from 0 to 8.\nNow, what we're going to do\nis Define the reward table.\nSo as you can see here\nis the same Matrix\nthat we created just now\nthat I showed you just now now\nif you understood it correctly,\nthere isn't any real\nBarrel limitation\nas depicted in the image,\nfor example, the transitional\nfor tell one is allowed\nbut the reward will be 0\nto discourage that path\nor in tough situation.\nWhat we do is add\na minus 1 there\nso that it gets\na negative reward.\nSo in the above code snippet\nas you can see here,\nwe took each of the It's and put\nonce in the respective state\nthat are directly reachable\nfrom the certain State.\nNow.\nIf you refer to that reward\ntable, once again,\nwhich we created the above\nor reconstruction will\nbe easy to understand\nbut one thing to note here is\nthat we did not consider the top\npriority location L6 yet.\nWe would also need\nan inverse mapping\nfrom the state's back\nto its original location\nand it will be cleaner\nwhen we reach to the other\ndepths of the algorithms.\nSo for that what we're going\nto do is Have the inverse\nmap location state to location.\nWe will take the distinct\nState and location\nand convert it back.\nNow.\nWhat will do is will not Define\na function get optimal\nwhich is the get optimal route,\nwhich will have a start location\nand an N location.\nDon't worry the code is back.\nBut I'll explain you\neach and every bit of the code.\nIt's not the get optimal root\nfunction will take two arguments\nthe starting location\nin the warehouse\nand the end location\nin the warehouse recipe lovely\nand it will return\nthe optimal route\nfor reaching the end location\nfrom the starting location\nin the form of an ordered list\ncontaining the letters.\nSo we'll start by defining\nthe function by initializing\nthe Q values to be all zeros.\nSo as you can see here we have\nEven the Q value has to be 0\nbut before that\nwhat we need to do is copy\nthe reward Matrix to a new one.\nSo this the rewards\nnew and next again,\nwhat we need to do is get\nthe ending State corresponding\nto the ending location.\nAnd with this information\nautomatically will set\nthe priority of the given ending\nstay to the highest one\nthat we are not defining it now,\nbut will automatically\nset the priority\nof the given ending\nState as nine nine nine.\nSo what we're going to do is\ninitialize the Q values to be 0\nand in the Learning process\nwhat you can see here.\nWe are taking I in range\n1000 and we're going to pick\nup a state randomly.\nSo we're going to use\nthe MP dot random randint\nand for traversing\nthrough the neighbor location\nin the same maze\nwe're going to iterate\nthrough the new reward\nMatrix and get the actions\nwhich are greater\nthan 0 and after that\nwhat we're going to do is pick\nan action randomly from the list\nof the playable actions\nin years to the next state\nwill going to compute\nthe temporal difference,\nwhich is TD,\nwhich is the rewards plus gamma\ninto the queue of next state\nand will take n p dot ARG Max\nof Q of next 8 minus Q\nof the current state.\nWe going to then update\nthe Q values using\nthe Bellman equation\nas you can see here.\nWe have the Bellman equation\nand we're going\nto update the Q values\nand after that we're going\nto initialize the optimal route\nwith a starting location\nnow here we do not know\nwhat the next location yet.\nSo initialize it with a value\nof the starting location,\nwhich Again is\nthe random location.\nSo we do not know\nabout the exact number\nof iteration needed to reach\nto the final location.\nHence while loop will be\na good choice for the iteration.\nSo when you're going to fetch\nthe starting State fetch\nthe highest Q value penetrating\nto the starting State\nwe go to the index\nor the next state,\nbut we need\nthe corresponding letter.\nSo we're going to use that state\nto location function.\nWe just mentioned there\nand after that we're going\nto update the starting location\nfor the The next iteration\nand finally we'll\nreturn the root.\nSo let's take the starting\nlocation of n line\nand and location\nof L while and see what part\ndo we actually get?\nSo as you can see here we\nget Airline l8l 5 L2 and L1.\nAnd if you have a look\nat the image here,\nwe have if we start\nfrom L9 to L1.\nWe got L8 L5 L\n2 l 1 l 8l v L2 L1\nthat would He does the maximum\nvalue of the maximum reward\nfor the robot.\nSo now we have come to the end\nof this Q learning session\nand I hope you got to know\nwhat exactly is Q learning\nwith the analogy\nall the way starting\nfrom the number of rooms\nand I hope the example\nwhich I took the analogy\nwhich I took was good enough\nfor you to understand q-learning\nunderstand the Bellman equation\nhow to make quick changes\nto the Bellman equation\nand how to create\nthe reward table the cue.\nWill and how to update\nthe Q values using\nthe Bellman equation,\nwhat does alpha do\nwhat does karma do?\n",
  "words": [
    "undoubtedly",
    "data",
    "science",
    "revolutionary",
    "technology",
    "era",
    "deriving",
    "useful",
    "insights",
    "data",
    "order",
    "solve",
    "complex",
    "problems",
    "hi",
    "welcome",
    "session",
    "data",
    "science",
    "full",
    "course",
    "contains",
    "everything",
    "need",
    "know",
    "order",
    "master",
    "data",
    "science",
    "get",
    "started",
    "let",
    "take",
    "look",
    "agenda",
    "first",
    "module",
    "reduction",
    "data",
    "science",
    "covers",
    "basic",
    "fundamentals",
    "data",
    "science",
    "followed",
    "statistics",
    "probability",
    "module",
    "understand",
    "statistics",
    "math",
    "behind",
    "data",
    "science",
    "machine",
    "learning",
    "algorithms",
    "next",
    "module",
    "basics",
    "machine",
    "learning",
    "understand",
    "exactly",
    "machine",
    "learning",
    "different",
    "types",
    "machine",
    "learning",
    "different",
    "machine",
    "learning",
    "algorithms",
    "next",
    "module",
    "supervised",
    "learning",
    "algorithms",
    "module",
    "start",
    "understanding",
    "basic",
    "linear",
    "regression",
    "next",
    "module",
    "logistic",
    "regression",
    "module",
    "see",
    "logistic",
    "regression",
    "used",
    "solve",
    "classification",
    "problems",
    "discuss",
    "decision",
    "trees",
    "see",
    "decision",
    "trees",
    "used",
    "solve",
    "complex",
    "problems",
    "next",
    "module",
    "random",
    "forest",
    "understand",
    "random",
    "forest",
    "used",
    "solve",
    "classification",
    "problems",
    "regression",
    "problems",
    "help",
    "use",
    "cases",
    "examples",
    "next",
    "module",
    "discussing",
    "neighbor",
    "module",
    "understand",
    "gain",
    "used",
    "solve",
    "complex",
    "classification",
    "problems",
    "followed",
    "look",
    "naive",
    "bias",
    "module",
    "one",
    "important",
    "algorithms",
    "gmail",
    "spam",
    "detection",
    "next",
    "algorithm",
    "support",
    "vector",
    "machine",
    "understand",
    "svm",
    "used",
    "draw",
    "hyperplane",
    "different",
    "classes",
    "data",
    "finally",
    "move",
    "unsupervised",
    "learning",
    "module",
    "understand",
    "genes",
    "used",
    "clustering",
    "perform",
    "market",
    "basket",
    "analysis",
    "using",
    "association",
    "rule",
    "mining",
    "next",
    "module",
    "reinforcement",
    "learning",
    "understand",
    "different",
    "concepts",
    "reinforcement",
    "learning",
    "along",
    "couple",
    "demonstrations",
    "followed",
    "bill",
    "look",
    "deep",
    "learning",
    "module",
    "understand",
    "exactly",
    "deep",
    "learning",
    "neural",
    "networks",
    "different",
    "types",
    "neural",
    "networks",
    "last",
    "module",
    "data",
    "science",
    "interview",
    "questions",
    "module",
    "understand",
    "important",
    "concepts",
    "data",
    "along",
    "tips",
    "order",
    "ace",
    "interview",
    "get",
    "started",
    "make",
    "sure",
    "subscribe",
    "adorama",
    "youtube",
    "channel",
    "order",
    "stay",
    "updated",
    "trending",
    "technologies",
    "data",
    "science",
    "one",
    "technologies",
    "right",
    "probably",
    "generating",
    "data",
    "unstoppable",
    "pace",
    "obviously",
    "need",
    "process",
    "make",
    "sense",
    "much",
    "data",
    "exactly",
    "data",
    "science",
    "comes",
    "today",
    "session",
    "talking",
    "data",
    "science",
    "depth",
    "let",
    "move",
    "ahead",
    "take",
    "look",
    "today",
    "agenda",
    "going",
    "begin",
    "discussing",
    "various",
    "sources",
    "data",
    "evolution",
    "technology",
    "introduction",
    "iod",
    "social",
    "media",
    "led",
    "need",
    "data",
    "sign",
    "next",
    "discuss",
    "walmart",
    "using",
    "insightful",
    "patterns",
    "database",
    "increase",
    "potential",
    "business",
    "see",
    "exactly",
    "data",
    "science",
    "move",
    "discuss",
    "data",
    "scientist",
    "also",
    "discuss",
    "various",
    "skill",
    "sets",
    "needed",
    "become",
    "data",
    "scientist",
    "next",
    "move",
    "see",
    "various",
    "data",
    "science",
    "job",
    "roles",
    "data",
    "analyst",
    "data",
    "architect",
    "data",
    "engineer",
    "cover",
    "data",
    "life",
    "cycle",
    "discuss",
    "data",
    "extracted",
    "processed",
    "finally",
    "use",
    "solution",
    "done",
    "cover",
    "basics",
    "machine",
    "learning",
    "see",
    "exactly",
    "machine",
    "learning",
    "different",
    "types",
    "machine",
    "learning",
    "next",
    "move",
    "onto",
    "k",
    "means",
    "algorithm",
    "discuss",
    "use",
    "case",
    "clustering",
    "discuss",
    "various",
    "steps",
    "involved",
    "algorithm",
    "finally",
    "move",
    "part",
    "use",
    "algorithm",
    "cluster",
    "movies",
    "based",
    "popularity",
    "social",
    "media",
    "platforms",
    "like",
    "facebook",
    "end",
    "today",
    "session",
    "also",
    "discuss",
    "data",
    "science",
    "certification",
    "take",
    "guys",
    "lot",
    "cover",
    "today",
    "session",
    "let",
    "jump",
    "first",
    "topic",
    "guys",
    "remember",
    "times",
    "telephones",
    "go",
    "pc",
    "boots",
    "order",
    "make",
    "phone",
    "call",
    "call",
    "things",
    "simple",
    "generate",
    "lot",
    "data",
    "even",
    "store",
    "contacts",
    "phones",
    "telephones",
    "used",
    "memorize",
    "phone",
    "numbers",
    "back",
    "know",
    "diary",
    "contact",
    "days",
    "smartphones",
    "store",
    "lot",
    "data",
    "everything",
    "us",
    "mobile",
    "phones",
    "images",
    "contacts",
    "various",
    "apps",
    "games",
    "everything",
    "stored",
    "mobile",
    "phones",
    "days",
    "similarly",
    "pcs",
    "use",
    "earlier",
    "times",
    "used",
    "process",
    "little",
    "data",
    "right",
    "lot",
    "data",
    "processing",
    "needed",
    "technology",
    "evolved",
    "much",
    "guys",
    "remember",
    "use",
    "floppy",
    "disk",
    "back",
    "floppy",
    "used",
    "store",
    "small",
    "amounts",
    "data",
    "later",
    "hard",
    "disks",
    "created",
    "used",
    "store",
    "gbs",
    "data",
    "look",
    "around",
    "data",
    "everywhere",
    "around",
    "us",
    "right",
    "data",
    "stored",
    "cloud",
    "data",
    "every",
    "appliance",
    "houses",
    "similarly",
    "look",
    "smart",
    "cars",
    "days",
    "connected",
    "internet",
    "connected",
    "mobile",
    "phones",
    "also",
    "generates",
    "lot",
    "data",
    "realize",
    "evolution",
    "technology",
    "generated",
    "lot",
    "data",
    "right",
    "initially",
    "little",
    "data",
    "even",
    "structured",
    "small",
    "part",
    "data",
    "unstructured",
    "days",
    "could",
    "use",
    "simple",
    "bi",
    "tools",
    "order",
    "process",
    "data",
    "make",
    "sense",
    "way",
    "much",
    "data",
    "order",
    "process",
    "much",
    "data",
    "need",
    "complex",
    "algorithms",
    "need",
    "better",
    "process",
    "right",
    "data",
    "science",
    "comes",
    "guys",
    "going",
    "get",
    "depth",
    "data",
    "science",
    "yet",
    "sure",
    "heard",
    "iot",
    "internet",
    "things",
    "guys",
    "know",
    "produce",
    "quintillion",
    "bytes",
    "data",
    "day",
    "accelerating",
    "growth",
    "iot",
    "iot",
    "internet",
    "things",
    "fancy",
    "term",
    "use",
    "network",
    "tools",
    "devices",
    "communicate",
    "transfer",
    "data",
    "internet",
    "various",
    "devices",
    "connected",
    "internet",
    "communicate",
    "right",
    "communication",
    "happens",
    "exchange",
    "data",
    "generation",
    "data",
    "devices",
    "include",
    "vehicles",
    "drive",
    "include",
    "tvs",
    "coffee",
    "machines",
    "refrigerators",
    "washing",
    "machines",
    "almost",
    "everything",
    "else",
    "use",
    "daily",
    "basis",
    "interconnected",
    "devices",
    "produce",
    "unimaginable",
    "amount",
    "data",
    "guys",
    "iot",
    "data",
    "measured",
    "zettabytes",
    "one",
    "zettabyte",
    "equal",
    "trillion",
    "gigabytes",
    "according",
    "recent",
    "survey",
    "cisco",
    "estimated",
    "end",
    "2019",
    "almost",
    "iot",
    "generate",
    "five",
    "hundred",
    "zettabytes",
    "data",
    "per",
    "year",
    "number",
    "increase",
    "time",
    "hard",
    "imagine",
    "data",
    "much",
    "volume",
    "imagine",
    "processing",
    "analyzing",
    "managing",
    "much",
    "data",
    "going",
    "cause",
    "migraine",
    "guys",
    "deal",
    "much",
    "data",
    "something",
    "traditional",
    "bi",
    "tools",
    "okay",
    "longer",
    "rely",
    "traditional",
    "data",
    "processing",
    "methods",
    "exactly",
    "need",
    "data",
    "science",
    "hope",
    "right",
    "let",
    "get",
    "details",
    "yet",
    "moving",
    "let",
    "see",
    "social",
    "media",
    "adding",
    "generation",
    "data",
    "fact",
    "love",
    "social",
    "media",
    "actually",
    "generating",
    "lot",
    "data",
    "us",
    "okay",
    "certainly",
    "one",
    "fuels",
    "data",
    "creation",
    "numbers",
    "see",
    "screen",
    "generated",
    "every",
    "minute",
    "day",
    "okay",
    "number",
    "going",
    "increase",
    "instagram",
    "says",
    "approximately",
    "million",
    "pictures",
    "uploaded",
    "minute",
    "similarly",
    "twitter",
    "approximately",
    "hundred",
    "forty",
    "eight",
    "thousand",
    "tweets",
    "published",
    "every",
    "minute",
    "day",
    "guys",
    "imagine",
    "one",
    "much",
    "would",
    "imagine",
    "24",
    "hours",
    "guys",
    "amount",
    "data",
    "generated",
    "social",
    "media",
    "unimaginable",
    "imagine",
    "processing",
    "much",
    "data",
    "analyzing",
    "trying",
    "figure",
    "know",
    "important",
    "insights",
    "much",
    "data",
    "analyzing",
    "much",
    "data",
    "going",
    "hard",
    "traditional",
    "tools",
    "traditional",
    "methods",
    "data",
    "science",
    "introduced",
    "data",
    "science",
    "simple",
    "process",
    "extract",
    "useful",
    "information",
    "data",
    "right",
    "going",
    "process",
    "analyze",
    "entire",
    "data",
    "going",
    "extract",
    "needed",
    "guys",
    "apart",
    "social",
    "media",
    "iot",
    "factors",
    "well",
    "contribute",
    "data",
    "generation",
    "days",
    "transactions",
    "done",
    "online",
    "right",
    "pay",
    "bills",
    "online",
    "shop",
    "online",
    "even",
    "buy",
    "homes",
    "online",
    "days",
    "even",
    "sell",
    "pets",
    "oil",
    "excuses",
    "stream",
    "music",
    "watch",
    "videos",
    "youtube",
    "generating",
    "lot",
    "data",
    "forget",
    "also",
    "brought",
    "health",
    "care",
    "internet",
    "wall",
    "various",
    "watches",
    "like",
    "bit",
    "fit",
    "basically",
    "trans",
    "heart",
    "rate",
    "generates",
    "data",
    "health",
    "conditions",
    "education",
    "also",
    "online",
    "thing",
    "right",
    "exactly",
    "right",
    "emergence",
    "internet",
    "perform",
    "activities",
    "online",
    "okay",
    "obviously",
    "helping",
    "us",
    "unaware",
    "much",
    "data",
    "generating",
    "done",
    "data",
    "could",
    "use",
    "data",
    "generated",
    "benefit",
    "well",
    "exactly",
    "data",
    "science",
    "data",
    "science",
    "extracting",
    "useful",
    "insights",
    "data",
    "using",
    "grow",
    "business",
    "get",
    "details",
    "data",
    "science",
    "let",
    "see",
    "walmart",
    "uses",
    "data",
    "science",
    "grow",
    "business",
    "guys",
    "walmart",
    "world",
    "biggest",
    "retailer",
    "stores",
    "28",
    "countries",
    "okay",
    "currently",
    "building",
    "world",
    "biggest",
    "good",
    "cloud",
    "able",
    "process",
    "two",
    "point",
    "five",
    "petabytes",
    "data",
    "every",
    "hour",
    "reason",
    "behind",
    "walmart",
    "success",
    "user",
    "customer",
    "data",
    "get",
    "useful",
    "insights",
    "customers",
    "shopping",
    "patterns",
    "data",
    "analyst",
    "data",
    "scientist",
    "walmart",
    "know",
    "every",
    "detail",
    "customers",
    "know",
    "customer",
    "buys",
    "might",
    "also",
    "buy",
    "cookies",
    "know",
    "like",
    "generate",
    "information",
    "like",
    "user",
    "data",
    "get",
    "customers",
    "hours",
    "analyze",
    "see",
    "particular",
    "customer",
    "looking",
    "let",
    "look",
    "cases",
    "walmart",
    "actually",
    "analyze",
    "data",
    "figured",
    "customer",
    "needs",
    "let",
    "consider",
    "halloween",
    "cookie",
    "sales",
    "example",
    "halloween",
    "sales",
    "analyst",
    "walmart",
    "took",
    "look",
    "data",
    "okay",
    "found",
    "specific",
    "cookie",
    "popular",
    "across",
    "walmart",
    "stores",
    "every",
    "walmart",
    "store",
    "selling",
    "cookies",
    "well",
    "found",
    "would",
    "stores",
    "selling",
    "dot",
    "okay",
    "situation",
    "immediately",
    "investigated",
    "found",
    "simple",
    "stocking",
    "oversight",
    "okay",
    "cookies",
    "put",
    "shelves",
    "sale",
    "issue",
    "immediately",
    "identified",
    "prevented",
    "loss",
    "sales",
    "another",
    "example",
    "true",
    "association",
    "rule",
    "mining",
    "walmart",
    "found",
    "strawberry",
    "sales",
    "increased",
    "seven",
    "times",
    "hurricane",
    "data",
    "analyst",
    "walmart",
    "identified",
    "association",
    "ha",
    "hurricane",
    "strawberry",
    "pop",
    "tarts",
    "data",
    "mining",
    "guys",
    "ask",
    "relationship",
    "harry",
    "caine",
    "reason",
    "whenever",
    "hurricane",
    "approaching",
    "people",
    "really",
    "wanted",
    "eat",
    "strawberry",
    "walmart",
    "place",
    "strawberry",
    "check",
    "hurricane",
    "would",
    "occur",
    "way",
    "increase",
    "sales",
    "natural",
    "thing",
    "making",
    "look",
    "internet",
    "walmart",
    "analyzing",
    "data",
    "generated",
    "social",
    "media",
    "find",
    "training",
    "product",
    "social",
    "media",
    "find",
    "likes",
    "dislikes",
    "person",
    "right",
    "walmart",
    "quite",
    "smart",
    "user",
    "data",
    "generated",
    "social",
    "media",
    "find",
    "products",
    "trending",
    "products",
    "liked",
    "customers",
    "okay",
    "example",
    "1",
    "mod",
    "analyze",
    "social",
    "media",
    "data",
    "find",
    "facebook",
    "users",
    "crazy",
    "cake",
    "pops",
    "okay",
    "walmart",
    "immediately",
    "took",
    "decision",
    "introduced",
    "cake",
    "pops",
    "walmart",
    "stores",
    "guys",
    "reason",
    "walmart",
    "successful",
    "huge",
    "amount",
    "data",
    "get",
    "see",
    "burden",
    "instead",
    "process",
    "data",
    "analyze",
    "try",
    "draw",
    "useful",
    "insights",
    "okay",
    "invest",
    "lot",
    "money",
    "lot",
    "effort",
    "lot",
    "time",
    "data",
    "analysis",
    "okay",
    "spend",
    "lot",
    "time",
    "analyzing",
    "data",
    "order",
    "find",
    "hidden",
    "patterns",
    "soon",
    "find",
    "hidden",
    "pattern",
    "association",
    "two",
    "products",
    "giving",
    "offers",
    "started",
    "discount",
    "something",
    "along",
    "line",
    "basically",
    "walmart",
    "uses",
    "data",
    "effective",
    "manner",
    "analyzer",
    "well",
    "process",
    "data",
    "well",
    "find",
    "useful",
    "insights",
    "need",
    "order",
    "get",
    "customers",
    "order",
    "improve",
    "business",
    "guys",
    "walmart",
    "uses",
    "data",
    "science",
    "let",
    "move",
    "ahead",
    "look",
    "data",
    "set",
    "guys",
    "data",
    "science",
    "uncovering",
    "findings",
    "data",
    "surfacing",
    "hidden",
    "insights",
    "help",
    "ponies",
    "make",
    "smart",
    "business",
    "decisions",
    "hidden",
    "insights",
    "hidden",
    "patterns",
    "used",
    "make",
    "better",
    "decisions",
    "business",
    "example",
    "also",
    "netflix",
    "netflix",
    "basically",
    "analyzes",
    "movie",
    "viewing",
    "patterns",
    "users",
    "understand",
    "drives",
    "user",
    "interest",
    "see",
    "users",
    "want",
    "watch",
    "find",
    "give",
    "people",
    "want",
    "guys",
    "actually",
    "data",
    "lot",
    "power",
    "know",
    "process",
    "data",
    "extract",
    "useful",
    "information",
    "data",
    "okay",
    "data",
    "science",
    "guys",
    "big",
    "question",
    "data",
    "scientists",
    "get",
    "useful",
    "insights",
    "data",
    "starts",
    "data",
    "exploration",
    "whenever",
    "data",
    "scientist",
    "comes",
    "across",
    "challenging",
    "question",
    "sort",
    "challenging",
    "situation",
    "become",
    "detectives",
    "investigative",
    "leads",
    "try",
    "understand",
    "different",
    "patterns",
    "different",
    "characteristics",
    "data",
    "okay",
    "try",
    "get",
    "information",
    "data",
    "use",
    "betterment",
    "organization",
    "business",
    "let",
    "look",
    "data",
    "scientist",
    "guys",
    "data",
    "scientists",
    "able",
    "view",
    "data",
    "quantitative",
    "lengths",
    "guys",
    "knowing",
    "math",
    "one",
    "important",
    "skills",
    "data",
    "scientists",
    "okay",
    "mathematics",
    "important",
    "order",
    "find",
    "solution",
    "going",
    "build",
    "lot",
    "predictive",
    "models",
    "predictive",
    "models",
    "going",
    "based",
    "hard",
    "math",
    "able",
    "understand",
    "underlying",
    "mechanics",
    "models",
    "predictive",
    "models",
    "algorithms",
    "require",
    "mathematics",
    "major",
    "misconception",
    "data",
    "science",
    "statistics",
    "saying",
    "statistics",
    "important",
    "important",
    "type",
    "math",
    "utilized",
    "data",
    "science",
    "actually",
    "many",
    "machine",
    "learning",
    "algorithms",
    "based",
    "linear",
    "algebra",
    "guys",
    "overall",
    "need",
    "good",
    "understanding",
    "math",
    "apart",
    "data",
    "scientist",
    "eli",
    "technology",
    "data",
    "scientists",
    "really",
    "good",
    "technology",
    "okay",
    "main",
    "work",
    "utilize",
    "technology",
    "analyze",
    "enormous",
    "data",
    "sets",
    "work",
    "complex",
    "algorithms",
    "requires",
    "tools",
    "much",
    "sophisticated",
    "excel",
    "data",
    "scientist",
    "need",
    "efficient",
    "coding",
    "languages",
    "core",
    "language",
    "associated",
    "data",
    "science",
    "include",
    "sql",
    "python",
    "r",
    "sass",
    "also",
    "important",
    "data",
    "scientist",
    "tactical",
    "business",
    "consultant",
    "guys",
    "business",
    "problems",
    "sword",
    "data",
    "scientist",
    "since",
    "data",
    "scientists",
    "work",
    "closely",
    "data",
    "know",
    "everything",
    "business",
    "business",
    "give",
    "entire",
    "data",
    "set",
    "business",
    "stored",
    "data",
    "scientist",
    "know",
    "every",
    "aspect",
    "business",
    "okay",
    "data",
    "scientists",
    "work",
    "get",
    "entire",
    "data",
    "set",
    "study",
    "data",
    "set",
    "analyze",
    "see",
    "things",
    "going",
    "wrong",
    "needs",
    "done",
    "needs",
    "excluded",
    "guys",
    "business",
    "acumen",
    "important",
    "skills",
    "algorithms",
    "good",
    "math",
    "technology",
    "guys",
    "business",
    "also",
    "important",
    "fields",
    "know",
    "data",
    "scientist",
    "let",
    "look",
    "skill",
    "sets",
    "data",
    "scientist",
    "names",
    "okay",
    "always",
    "starts",
    "statistics",
    "statistics",
    "give",
    "numbers",
    "data",
    "good",
    "understanding",
    "statistics",
    "important",
    "becoming",
    "data",
    "scientist",
    "familiar",
    "satisfaction",
    "contest",
    "distributions",
    "maximum",
    "likelihood",
    "estimators",
    "apart",
    "also",
    "good",
    "understanding",
    "probability",
    "theory",
    "descriptive",
    "statistics",
    "concepts",
    "help",
    "make",
    "better",
    "business",
    "decisions",
    "matter",
    "type",
    "company",
    "role",
    "interviewing",
    "going",
    "expected",
    "know",
    "use",
    "tools",
    "trade",
    "okay",
    "means",
    "know",
    "statistical",
    "programming",
    "language",
    "like",
    "python",
    "also",
    "need",
    "know",
    "database",
    "wiring",
    "language",
    "like",
    "sql",
    "main",
    "reason",
    "people",
    "prefer",
    "python",
    "number",
    "packages",
    "languages",
    "predefined",
    "packages",
    "algorithms",
    "actually",
    "sit",
    "code",
    "algorithms",
    "instead",
    "load",
    "one",
    "packages",
    "libraries",
    "run",
    "programming",
    "languages",
    "must",
    "minimum",
    "know",
    "python",
    "database",
    "query",
    "language",
    "let",
    "move",
    "data",
    "extraction",
    "processing",
    "guys",
    "multiple",
    "data",
    "sources",
    "like",
    "mysql",
    "database",
    "mongo",
    "database",
    "okay",
    "extract",
    "sources",
    "order",
    "analyze",
    "query",
    "database",
    "store",
    "proper",
    "format",
    "proper",
    "structure",
    "okay",
    "finally",
    "load",
    "data",
    "data",
    "warehouse",
    "analyze",
    "data",
    "okay",
    "entire",
    "process",
    "called",
    "extraction",
    "processing",
    "guys",
    "extraction",
    "processing",
    "getting",
    "data",
    "different",
    "data",
    "sources",
    "putting",
    "format",
    "analyze",
    "next",
    "data",
    "wrangling",
    "exploration",
    "guys",
    "data",
    "wrangling",
    "one",
    "difficult",
    "tasks",
    "data",
    "science",
    "task",
    "data",
    "wrangling",
    "cleaning",
    "data",
    "lot",
    "instances",
    "data",
    "sets",
    "missing",
    "values",
    "null",
    "values",
    "inconsistent",
    "formats",
    "inconsistent",
    "values",
    "need",
    "understand",
    "values",
    "data",
    "wrangling",
    "data",
    "cleaning",
    "comes",
    "picture",
    "done",
    "going",
    "analyze",
    "data",
    "data",
    "wrangling",
    "cleaning",
    "done",
    "going",
    "start",
    "exploring",
    "try",
    "make",
    "sense",
    "data",
    "okay",
    "looking",
    "different",
    "patterns",
    "data",
    "different",
    "trends",
    "outliers",
    "various",
    "unexpected",
    "results",
    "next",
    "machine",
    "learning",
    "guys",
    "large",
    "company",
    "huge",
    "amounts",
    "data",
    "working",
    "company",
    "see",
    "product",
    "data",
    "driven",
    "like",
    "working",
    "netflix",
    "google",
    "maps",
    "familiar",
    "machine",
    "learning",
    "methods",
    "right",
    "process",
    "large",
    "amount",
    "data",
    "traditional",
    "methods",
    "need",
    "machine",
    "learning",
    "algorithms",
    "algorithms",
    "like",
    "knok",
    "nearest",
    "neighbor",
    "random",
    "forest",
    "k",
    "means",
    "algorithm",
    "support",
    "vector",
    "machines",
    "algorithms",
    "aware",
    "algorithms",
    "let",
    "tell",
    "algorithms",
    "implemented",
    "using",
    "python",
    "libraries",
    "okay",
    "need",
    "understanding",
    "machine",
    "learning",
    "large",
    "amount",
    "data",
    "front",
    "going",
    "case",
    "people",
    "right",
    "data",
    "generated",
    "unstoppable",
    "pace",
    "earlier",
    "session",
    "discussed",
    "much",
    "data",
    "generated",
    "knowing",
    "machine",
    "learning",
    "algorithms",
    "machine",
    "learning",
    "concepts",
    "required",
    "skill",
    "want",
    "become",
    "data",
    "scientist",
    "sitting",
    "interview",
    "data",
    "scientist",
    "asked",
    "machine",
    "learning",
    "seems",
    "asked",
    "good",
    "algorithms",
    "well",
    "implement",
    "next",
    "big",
    "data",
    "processing",
    "frameworks",
    "guys",
    "know",
    "generating",
    "lot",
    "data",
    "data",
    "structured",
    "unstructured",
    "well",
    "data",
    "use",
    "traditional",
    "data",
    "processing",
    "system",
    "need",
    "know",
    "frameworks",
    "like",
    "hadoop",
    "spark",
    "okay",
    "frameworks",
    "used",
    "handle",
    "big",
    "data",
    "lastly",
    "data",
    "visualization",
    "guys",
    "data",
    "visualization",
    "one",
    "important",
    "part",
    "data",
    "analysis",
    "always",
    "important",
    "present",
    "data",
    "understandable",
    "visually",
    "appealing",
    "format",
    "data",
    "visualization",
    "one",
    "skills",
    "data",
    "scientists",
    "master",
    "okay",
    "want",
    "communicate",
    "data",
    "end",
    "users",
    "better",
    "way",
    "data",
    "visualization",
    "must",
    "guys",
    "lot",
    "tools",
    "used",
    "data",
    "visualization",
    "tools",
    "like",
    "diablo",
    "power",
    "bi",
    "popular",
    "visualization",
    "tools",
    "sum",
    "entire",
    "skill",
    "set",
    "needed",
    "become",
    "data",
    "scientist",
    "apart",
    "also",
    "problem",
    "solving",
    "approach",
    "also",
    "creative",
    "data",
    "know",
    "skills",
    "needed",
    "become",
    "data",
    "scientist",
    "let",
    "look",
    "different",
    "job",
    "roles",
    "data",
    "science",
    "vast",
    "field",
    "many",
    "job",
    "roles",
    "data",
    "science",
    "let",
    "take",
    "look",
    "role",
    "let",
    "start",
    "data",
    "scientist",
    "data",
    "scientists",
    "understand",
    "challenge",
    "business",
    "offer",
    "best",
    "solution",
    "using",
    "data",
    "analysis",
    "data",
    "processing",
    "instance",
    "expected",
    "perform",
    "predictive",
    "analysis",
    "also",
    "able",
    "identify",
    "trends",
    "patterns",
    "companies",
    "making",
    "better",
    "decisions",
    "become",
    "data",
    "scientist",
    "expert",
    "matlab",
    "sql",
    "python",
    "complementary",
    "technologies",
    "also",
    "help",
    "higher",
    "degree",
    "mathematics",
    "computer",
    "engineering",
    "next",
    "data",
    "analyst",
    "data",
    "analyst",
    "responsible",
    "variety",
    "tasks",
    "including",
    "visualization",
    "processing",
    "massive",
    "amount",
    "data",
    "among",
    "also",
    "perform",
    "queries",
    "databases",
    "aware",
    "different",
    "query",
    "languages",
    "guys",
    "one",
    "important",
    "skills",
    "data",
    "analyst",
    "optimization",
    "create",
    "modify",
    "algorithms",
    "used",
    "pull",
    "information",
    "biggest",
    "databases",
    "without",
    "corrupting",
    "data",
    "become",
    "done",
    "must",
    "know",
    "technologies",
    "sql",
    "sas",
    "python",
    "certification",
    "technologies",
    "boost",
    "job",
    "application",
    "also",
    "good",
    "problem",
    "solving",
    "quality",
    "next",
    "data",
    "architect",
    "data",
    "architect",
    "creates",
    "blueprints",
    "data",
    "management",
    "databases",
    "easily",
    "integrated",
    "centralized",
    "protected",
    "best",
    "security",
    "measures",
    "okay",
    "also",
    "ensure",
    "data",
    "engineers",
    "best",
    "tools",
    "systems",
    "work",
    "become",
    "data",
    "architect",
    "expertise",
    "data",
    "warehousing",
    "data",
    "modeling",
    "extraction",
    "transformation",
    "loan",
    "okay",
    "also",
    "well",
    "versed",
    "hive",
    "pig",
    "spark",
    "apart",
    "data",
    "engineers",
    "guys",
    "main",
    "responsibilities",
    "data",
    "engineer",
    "build",
    "test",
    "scalable",
    "big",
    "data",
    "ecosystems",
    "okay",
    "also",
    "needed",
    "update",
    "existing",
    "systems",
    "newer",
    "upgraded",
    "versions",
    "also",
    "responsible",
    "improving",
    "efficiency",
    "database",
    "interested",
    "career",
    "data",
    "engineer",
    "technologies",
    "require",
    "experience",
    "include",
    "hive",
    "nosql",
    "ruby",
    "java",
    "matlab",
    "would",
    "also",
    "help",
    "work",
    "popular",
    "data",
    "apis",
    "etl",
    "tools",
    "next",
    "statistician",
    "name",
    "suggests",
    "sound",
    "understanding",
    "statistical",
    "theories",
    "data",
    "organization",
    "extract",
    "offer",
    "valuable",
    "insights",
    "also",
    "create",
    "new",
    "methodologies",
    "engineers",
    "apply",
    "want",
    "become",
    "statistician",
    "passion",
    "logic",
    "also",
    "good",
    "variety",
    "database",
    "systems",
    "sql",
    "data",
    "mining",
    "various",
    "machine",
    "learning",
    "technologies",
    "mean",
    "good",
    "math",
    "also",
    "good",
    "knowledge",
    "weight",
    "database",
    "system",
    "sql",
    "also",
    "various",
    "machine",
    "learning",
    "concepts",
    "algorithms",
    "next",
    "database",
    "administrator",
    "guys",
    "job",
    "profile",
    "database",
    "administrator",
    "much",
    "basically",
    "responsible",
    "proper",
    "functioning",
    "databases",
    "also",
    "responsible",
    "granting",
    "permission",
    "working",
    "services",
    "employees",
    "company",
    "also",
    "take",
    "care",
    "database",
    "backups",
    "recoveries",
    "skills",
    "needed",
    "become",
    "database",
    "administrator",
    "include",
    "database",
    "backup",
    "recovery",
    "data",
    "security",
    "data",
    "modeling",
    "design",
    "next",
    "business",
    "analyst",
    "role",
    "business",
    "analyst",
    "little",
    "different",
    "data",
    "signs",
    "job",
    "get",
    "wrong",
    "good",
    "understanding",
    "data",
    "oriented",
    "technologies",
    "know",
    "handle",
    "lot",
    "data",
    "process",
    "also",
    "focused",
    "data",
    "linked",
    "actionable",
    "business",
    "inside",
    "mainly",
    "focus",
    "business",
    "growth",
    "okay",
    "business",
    "analyst",
    "acts",
    "like",
    "link",
    "data",
    "engineers",
    "management",
    "executives",
    "order",
    "become",
    "business",
    "analyst",
    "understanding",
    "business",
    "finances",
    "business",
    "intelligence",
    "also",
    "acknowledge",
    "like",
    "data",
    "modeling",
    "data",
    "visualization",
    "tools",
    "etc",
    "last",
    "data",
    "analytics",
    "manager",
    "data",
    "analytics",
    "manager",
    "responsible",
    "data",
    "science",
    "operations",
    "main",
    "responsibilities",
    "data",
    "analytics",
    "manager",
    "oversee",
    "data",
    "science",
    "operation",
    "okay",
    "responsible",
    "assigning",
    "duties",
    "team",
    "according",
    "skills",
    "expertise",
    "strength",
    "include",
    "technologies",
    "like",
    "sas",
    "sql",
    "course",
    "good",
    "management",
    "skills",
    "apart",
    "must",
    "excellent",
    "social",
    "skills",
    "leadership",
    "qualities",
    "thinking",
    "attitude",
    "like",
    "said",
    "earlier",
    "need",
    "good",
    "understanding",
    "technologies",
    "like",
    "pythons",
    "java",
    "etc",
    "guys",
    "different",
    "job",
    "roles",
    "data",
    "science",
    "hope",
    "found",
    "informative",
    "let",
    "move",
    "ahead",
    "look",
    "data",
    "lifecycle",
    "guys",
    "basically",
    "six",
    "steps",
    "data",
    "life",
    "cycle",
    "starts",
    "business",
    "requirement",
    "next",
    "data",
    "acquisition",
    "would",
    "process",
    "data",
    "called",
    "data",
    "processing",
    "data",
    "exploration",
    "modeling",
    "finally",
    "deployment",
    "guys",
    "even",
    "start",
    "data",
    "science",
    "project",
    "important",
    "understand",
    "problem",
    "trying",
    "solve",
    "stage",
    "going",
    "focus",
    "identifying",
    "central",
    "objectives",
    "project",
    "identifying",
    "variables",
    "need",
    "predicted",
    "next",
    "data",
    "acquisition",
    "okay",
    "objectives",
    "find",
    "time",
    "start",
    "gathering",
    "data",
    "data",
    "mining",
    "process",
    "gathering",
    "data",
    "different",
    "sources",
    "stage",
    "questions",
    "ask",
    "data",
    "need",
    "project",
    "live",
    "obtain",
    "efficient",
    "way",
    "store",
    "access",
    "next",
    "data",
    "processing",
    "usually",
    "data",
    "collected",
    "huge",
    "mess",
    "okay",
    "formatted",
    "structured",
    "cleaned",
    "find",
    "data",
    "set",
    "cleaned",
    "packaged",
    "well",
    "actually",
    "lottery",
    "finding",
    "right",
    "data",
    "takes",
    "lot",
    "time",
    "takes",
    "lot",
    "effort",
    "one",
    "major",
    "task",
    "data",
    "science",
    "process",
    "data",
    "cleaning",
    "okay",
    "requires",
    "lot",
    "time",
    "requires",
    "lot",
    "effort",
    "go",
    "entire",
    "data",
    "set",
    "find",
    "missing",
    "values",
    "inconsistent",
    "values",
    "corrupted",
    "data",
    "also",
    "find",
    "unnecessary",
    "data",
    "remove",
    "data",
    "data",
    "processing",
    "next",
    "data",
    "exploration",
    "sparkling",
    "clean",
    "set",
    "data",
    "finally",
    "ready",
    "get",
    "started",
    "analysis",
    "okay",
    "data",
    "exploration",
    "stage",
    "basically",
    "brainstorming",
    "data",
    "analysis",
    "order",
    "understand",
    "patterns",
    "data",
    "use",
    "histogram",
    "pull",
    "random",
    "subset",
    "data",
    "plot",
    "histogram",
    "even",
    "create",
    "interactive",
    "visualizations",
    "point",
    "dive",
    "deep",
    "data",
    "try",
    "explore",
    "different",
    "models",
    "applied",
    "data",
    "next",
    "data",
    "modeling",
    "processing",
    "data",
    "going",
    "going",
    "carry",
    "model",
    "training",
    "okay",
    "model",
    "training",
    "basically",
    "finding",
    "model",
    "answers",
    "questions",
    "accurately",
    "process",
    "model",
    "training",
    "involves",
    "lot",
    "steps",
    "firstly",
    "start",
    "splitting",
    "input",
    "data",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "okay",
    "going",
    "take",
    "entire",
    "data",
    "set",
    "going",
    "separate",
    "two",
    "two",
    "parts",
    "one",
    "training",
    "one",
    "testing",
    "data",
    "build",
    "model",
    "using",
    "training",
    "data",
    "set",
    "done",
    "evaluate",
    "training",
    "test",
    "data",
    "set",
    "evaluate",
    "training",
    "testing",
    "data",
    "using",
    "series",
    "machine",
    "learning",
    "algorithms",
    "find",
    "model",
    "suitable",
    "business",
    "requirement",
    "mainly",
    "data",
    "modeling",
    "okay",
    "build",
    "model",
    "training",
    "data",
    "set",
    "evaluate",
    "model",
    "using",
    "testing",
    "data",
    "set",
    "deployment",
    "guys",
    "goal",
    "stage",
    "deploy",
    "model",
    "production",
    "maybe",
    "production",
    "like",
    "environment",
    "basically",
    "done",
    "final",
    "user",
    "acceptance",
    "users",
    "validate",
    "performance",
    "models",
    "issues",
    "model",
    "issues",
    "algorithm",
    "fixed",
    "stage",
    "guys",
    "come",
    "end",
    "data",
    "lifecycle",
    "hope",
    "clear",
    "statistics",
    "probability",
    "essential",
    "disciples",
    "form",
    "basic",
    "foundation",
    "machine",
    "learning",
    "algorithms",
    "deep",
    "learning",
    "artificial",
    "intelligence",
    "data",
    "science",
    "fact",
    "mathematics",
    "probability",
    "behind",
    "everything",
    "around",
    "us",
    "shapes",
    "patterns",
    "colors",
    "count",
    "petals",
    "flower",
    "mathematics",
    "embedded",
    "every",
    "aspect",
    "lives",
    "mind",
    "welcome",
    "today",
    "session",
    "going",
    "go",
    "ahead",
    "scoffs",
    "agenda",
    "today",
    "going",
    "begin",
    "session",
    "understanding",
    "data",
    "move",
    "look",
    "different",
    "categories",
    "data",
    "like",
    "quantitative",
    "qualitative",
    "data",
    "discuss",
    "exactly",
    "statistics",
    "basic",
    "terminologies",
    "statistics",
    "couple",
    "sampling",
    "techniques",
    "done",
    "discuss",
    "different",
    "types",
    "statistics",
    "involve",
    "descriptive",
    "inferential",
    "statistics",
    "next",
    "session",
    "mainly",
    "focusing",
    "descriptive",
    "statistics",
    "understand",
    "different",
    "measures",
    "center",
    "measures",
    "spread",
    "information",
    "gain",
    "entropy",
    "also",
    "understand",
    "measures",
    "help",
    "use",
    "case",
    "finally",
    "discuss",
    "exactly",
    "confusion",
    "matrix",
    "covered",
    "entire",
    "descriptive",
    "statistics",
    "module",
    "discuss",
    "probability",
    "module",
    "understand",
    "exactly",
    "probability",
    "different",
    "terminologies",
    "probability",
    "also",
    "study",
    "different",
    "probability",
    "distributions",
    "discuss",
    "types",
    "probability",
    "include",
    "marginal",
    "probability",
    "joint",
    "conditional",
    "probability",
    "move",
    "discuss",
    "use",
    "case",
    "see",
    "examples",
    "show",
    "us",
    "different",
    "types",
    "probability",
    "work",
    "better",
    "understand",
    "bayes",
    "theorem",
    "look",
    "small",
    "example",
    "also",
    "forgot",
    "mention",
    "end",
    "descriptive",
    "statistics",
    "module",
    "running",
    "small",
    "demo",
    "language",
    "know",
    "much",
    "explaining",
    "every",
    "line",
    "depth",
    "want",
    "understanding",
    "leave",
    "couple",
    "blocks",
    "couple",
    "videos",
    "description",
    "box",
    "definitely",
    "check",
    "content",
    "completed",
    "probability",
    "module",
    "discuss",
    "inferential",
    "statistics",
    "module",
    "start",
    "module",
    "understanding",
    "point",
    "estimation",
    "discuss",
    "confidence",
    "interval",
    "estimate",
    "confidence",
    "interval",
    "also",
    "discuss",
    "margin",
    "error",
    "understand",
    "concepts",
    "looking",
    "small",
    "use",
    "case",
    "finally",
    "end",
    "inferential",
    "real",
    "statistic",
    "module",
    "looking",
    "hypothesis",
    "testing",
    "hypothesis",
    "testing",
    "important",
    "part",
    "inferential",
    "statistics",
    "end",
    "session",
    "looking",
    "use",
    "case",
    "discusses",
    "hypothesis",
    "testing",
    "works",
    "sum",
    "everything",
    "look",
    "demo",
    "explains",
    "inferential",
    "statistics",
    "works",
    "alright",
    "guys",
    "lot",
    "cover",
    "today",
    "let",
    "move",
    "ahead",
    "take",
    "look",
    "first",
    "topic",
    "data",
    "quite",
    "simple",
    "question",
    "ask",
    "data",
    "see",
    "set",
    "numbers",
    "sort",
    "documents",
    "stored",
    "computer",
    "data",
    "actually",
    "everything",
    "right",
    "look",
    "around",
    "data",
    "everywhere",
    "click",
    "phone",
    "generates",
    "data",
    "know",
    "generated",
    "data",
    "provides",
    "insights",
    "analysis",
    "helps",
    "us",
    "make",
    "better",
    "business",
    "decisions",
    "data",
    "important",
    "give",
    "formal",
    "definition",
    "data",
    "refers",
    "facts",
    "statistics",
    "collected",
    "together",
    "reference",
    "analysis",
    "right",
    "definition",
    "data",
    "terms",
    "statistics",
    "probability",
    "know",
    "data",
    "collected",
    "measured",
    "analyzed",
    "visualized",
    "using",
    "statistical",
    "models",
    "graphs",
    "data",
    "divided",
    "two",
    "major",
    "subcategories",
    "alright",
    "first",
    "qualitative",
    "data",
    "quantitative",
    "data",
    "two",
    "different",
    "types",
    "data",
    "qualitative",
    "data",
    "nominal",
    "ordinal",
    "data",
    "quantitative",
    "data",
    "discrete",
    "continuous",
    "data",
    "let",
    "focus",
    "qualitative",
    "data",
    "type",
    "data",
    "deals",
    "characteristics",
    "descriptors",
    "ca",
    "easily",
    "measured",
    "observed",
    "subjectively",
    "qualitative",
    "data",
    "divided",
    "nominal",
    "ordinal",
    "data",
    "nominal",
    "data",
    "sort",
    "data",
    "order",
    "ranking",
    "okay",
    "example",
    "nominal",
    "data",
    "gender",
    "ranking",
    "gender",
    "male",
    "female",
    "right",
    "one",
    "two",
    "three",
    "four",
    "sort",
    "ordering",
    "gender",
    "race",
    "another",
    "example",
    "nominal",
    "data",
    "ordinal",
    "data",
    "basically",
    "ordered",
    "series",
    "information",
    "okay",
    "let",
    "say",
    "went",
    "restaurant",
    "okay",
    "information",
    "stored",
    "form",
    "customer",
    "id",
    "right",
    "basically",
    "represented",
    "customer",
    "id",
    "would",
    "rated",
    "service",
    "either",
    "good",
    "average",
    "right",
    "ordinal",
    "data",
    "similarly",
    "record",
    "customers",
    "visit",
    "restaurant",
    "along",
    "ratings",
    "right",
    "data",
    "sort",
    "sequence",
    "sort",
    "order",
    "known",
    "ordinal",
    "data",
    "right",
    "guys",
    "pretty",
    "simple",
    "understand",
    "let",
    "move",
    "look",
    "quantitative",
    "data",
    "quantitative",
    "data",
    "basically",
    "numbers",
    "things",
    "okay",
    "understand",
    "word",
    "quantitative",
    "quantitative",
    "basically",
    "quantity",
    "right",
    "saudis",
    "numbers",
    "deals",
    "anything",
    "measure",
    "objectively",
    "right",
    "two",
    "types",
    "quantitative",
    "data",
    "discrete",
    "continuous",
    "data",
    "discrete",
    "data",
    "also",
    "known",
    "categorical",
    "data",
    "hold",
    "finite",
    "number",
    "possible",
    "values",
    "number",
    "students",
    "class",
    "finite",
    "number",
    "right",
    "ca",
    "infinite",
    "number",
    "students",
    "class",
    "let",
    "say",
    "fifth",
    "grade",
    "hundred",
    "students",
    "class",
    "right",
    "infinite",
    "number",
    "definite",
    "finite",
    "number",
    "students",
    "class",
    "okay",
    "discrete",
    "data",
    "next",
    "continuous",
    "data",
    "type",
    "data",
    "hold",
    "infinite",
    "number",
    "possible",
    "values",
    "okay",
    "say",
    "weight",
    "person",
    "example",
    "continuous",
    "data",
    "mean",
    "see",
    "weight",
    "50",
    "kgs",
    "nb",
    "kgs",
    "one",
    "kgs",
    "one",
    "2",
    "3",
    "right",
    "infinite",
    "number",
    "possible",
    "values",
    "right",
    "mean",
    "continuous",
    "data",
    "right",
    "difference",
    "discrete",
    "continuous",
    "data",
    "also",
    "like",
    "mention",
    "things",
    "couple",
    "types",
    "variables",
    "well",
    "discrete",
    "variable",
    "continuous",
    "variable",
    "discrete",
    "variable",
    "also",
    "known",
    "categorical",
    "variable",
    "hold",
    "values",
    "different",
    "categories",
    "let",
    "say",
    "variable",
    "called",
    "message",
    "two",
    "types",
    "values",
    "variable",
    "hold",
    "let",
    "say",
    "message",
    "either",
    "spam",
    "message",
    "non",
    "spam",
    "message",
    "okay",
    "call",
    "variable",
    "discrete",
    "categorical",
    "variable",
    "right",
    "hold",
    "values",
    "represent",
    "different",
    "categories",
    "data",
    "continuous",
    "variables",
    "basically",
    "variables",
    "store",
    "infinite",
    "number",
    "values",
    "weight",
    "person",
    "denoted",
    "continuous",
    "variable",
    "right",
    "let",
    "say",
    "variable",
    "called",
    "weight",
    "store",
    "infinite",
    "number",
    "possible",
    "values",
    "call",
    "continuous",
    "variable",
    "guys",
    "basically",
    "variable",
    "anything",
    "store",
    "value",
    "right",
    "associate",
    "sort",
    "data",
    "able",
    "become",
    "either",
    "discrete",
    "variable",
    "continuous",
    "variable",
    "also",
    "dependent",
    "independent",
    "type",
    "variables",
    "wo",
    "discuss",
    "death",
    "pretty",
    "understandable",
    "sure",
    "know",
    "independent",
    "variable",
    "dependent",
    "variable",
    "right",
    "dependent",
    "variable",
    "variable",
    "whose",
    "value",
    "depends",
    "independent",
    "variable",
    "guys",
    "much",
    "knowledge",
    "expect",
    "right",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "statistics",
    "coming",
    "formal",
    "definition",
    "statistics",
    "statistics",
    "area",
    "applied",
    "mathematics",
    "concerned",
    "data",
    "collection",
    "analysis",
    "interpretation",
    "presentation",
    "usually",
    "speak",
    "statistics",
    "people",
    "think",
    "statistics",
    "analysis",
    "statistics",
    "parts",
    "data",
    "collection",
    "also",
    "part",
    "statistics",
    "data",
    "interpretation",
    "presentation",
    "comes",
    "statistics",
    "already",
    "going",
    "use",
    "statistical",
    "methods",
    "visualize",
    "data",
    "collect",
    "data",
    "interpret",
    "data",
    "alright",
    "area",
    "mathematics",
    "deals",
    "understanding",
    "data",
    "used",
    "solve",
    "complex",
    "problems",
    "okay",
    "give",
    "couple",
    "examples",
    "solved",
    "using",
    "statistics",
    "okay",
    "let",
    "say",
    "company",
    "created",
    "new",
    "drug",
    "may",
    "cure",
    "cancer",
    "would",
    "conduct",
    "test",
    "confirm",
    "effectiveness",
    "even",
    "though",
    "sounds",
    "like",
    "biology",
    "problem",
    "solved",
    "statistics",
    "already",
    "create",
    "test",
    "confirm",
    "effectiveness",
    "drum",
    "common",
    "problem",
    "solved",
    "using",
    "statistics",
    "let",
    "give",
    "another",
    "example",
    "friend",
    "baseball",
    "game",
    "blue",
    "offers",
    "bet",
    "neither",
    "team",
    "hit",
    "home",
    "run",
    "game",
    "take",
    "bet",
    "right",
    "discuss",
    "probability",
    "know",
    "win",
    "lose",
    "right",
    "another",
    "problem",
    "comes",
    "statistics",
    "let",
    "look",
    "another",
    "example",
    "latest",
    "sales",
    "data",
    "come",
    "boss",
    "wants",
    "prepare",
    "report",
    "management",
    "places",
    "company",
    "could",
    "improve",
    "business",
    "look",
    "look",
    "problem",
    "involves",
    "lot",
    "data",
    "analysis",
    "look",
    "different",
    "variables",
    "causing",
    "business",
    "go",
    "look",
    "variables",
    "increasing",
    "performance",
    "models",
    "thus",
    "growing",
    "business",
    "alright",
    "involves",
    "lot",
    "data",
    "analysis",
    "basic",
    "idea",
    "behind",
    "data",
    "analysis",
    "use",
    "statistical",
    "techniques",
    "order",
    "figure",
    "relationship",
    "different",
    "variables",
    "different",
    "components",
    "business",
    "okay",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "basic",
    "terminologies",
    "statistics",
    "dive",
    "deep",
    "statistics",
    "important",
    "understand",
    "basic",
    "terminologies",
    "used",
    "statistics",
    "two",
    "important",
    "terminologies",
    "statistics",
    "population",
    "sample",
    "throughout",
    "statistics",
    "course",
    "throughout",
    "problem",
    "trying",
    "stall",
    "statistics",
    "come",
    "across",
    "two",
    "words",
    "population",
    "sample",
    "population",
    "collection",
    "set",
    "individuals",
    "objects",
    "events",
    "events",
    "whose",
    "properties",
    "analyzed",
    "okay",
    "basically",
    "refer",
    "population",
    "subject",
    "trying",
    "analyze",
    "sample",
    "like",
    "word",
    "suggests",
    "subset",
    "population",
    "make",
    "sure",
    "choose",
    "sample",
    "way",
    "represents",
    "entire",
    "population",
    "right",
    "focus",
    "add",
    "one",
    "part",
    "population",
    "instead",
    "represent",
    "entire",
    "population",
    "sample",
    "chosen",
    "well",
    "chosen",
    "sample",
    "contain",
    "information",
    "particular",
    "population",
    "parameter",
    "must",
    "wondering",
    "one",
    "choose",
    "sample",
    "best",
    "represents",
    "entire",
    "population",
    "sampling",
    "statistical",
    "method",
    "deals",
    "selection",
    "individual",
    "observations",
    "within",
    "population",
    "sampling",
    "performed",
    "order",
    "infer",
    "statistical",
    "knowledge",
    "population",
    "right",
    "want",
    "understand",
    "different",
    "statistics",
    "population",
    "like",
    "mean",
    "median",
    "median",
    "mode",
    "standard",
    "deviation",
    "variance",
    "population",
    "going",
    "perform",
    "sampling",
    "right",
    "reasonable",
    "study",
    "large",
    "population",
    "find",
    "mean",
    "median",
    "everything",
    "else",
    "sampling",
    "performed",
    "might",
    "ask",
    "point",
    "sampling",
    "study",
    "entire",
    "population",
    "guys",
    "think",
    "scenario",
    "asked",
    "perform",
    "survey",
    "eating",
    "habits",
    "teenagers",
    "us",
    "present",
    "42",
    "million",
    "teens",
    "us",
    "number",
    "growing",
    "speaking",
    "right",
    "correct",
    "possible",
    "survey",
    "42",
    "million",
    "individuals",
    "health",
    "possible",
    "well",
    "might",
    "possible",
    "take",
    "forever",
    "obviously",
    "reasonable",
    "go",
    "around",
    "knocking",
    "door",
    "asking",
    "teenage",
    "son",
    "eat",
    "right",
    "reasonable",
    "sampling",
    "used",
    "method",
    "wherein",
    "sample",
    "population",
    "studied",
    "order",
    "draw",
    "inferences",
    "entire",
    "population",
    "basically",
    "shortcut",
    "studying",
    "entire",
    "population",
    "instead",
    "taking",
    "entire",
    "population",
    "finding",
    "solutions",
    "going",
    "take",
    "part",
    "population",
    "represents",
    "entire",
    "population",
    "going",
    "perform",
    "statistical",
    "analysis",
    "inferential",
    "statistics",
    "small",
    "sample",
    "right",
    "sample",
    "basically",
    "presents",
    "entire",
    "population",
    "right",
    "short",
    "made",
    "clear",
    "sample",
    "population",
    "two",
    "main",
    "types",
    "sampling",
    "techniques",
    "discussed",
    "today",
    "probability",
    "sampling",
    "sampling",
    "video",
    "focusing",
    "probability",
    "sampling",
    "techniques",
    "sampling",
    "within",
    "scope",
    "video",
    "right",
    "discuss",
    "probability",
    "part",
    "focusing",
    "statistics",
    "probability",
    "correct",
    "probability",
    "sampling",
    "three",
    "different",
    "types",
    "random",
    "sampling",
    "systematic",
    "stratified",
    "sampling",
    "right",
    "mention",
    "different",
    "types",
    "sampling",
    "bald",
    "kota",
    "judgment",
    "convenience",
    "sampling",
    "right",
    "guys",
    "session",
    "focusing",
    "probability",
    "let",
    "move",
    "look",
    "different",
    "types",
    "probability",
    "sampling",
    "probability",
    "sampling",
    "sampling",
    "technique",
    "samples",
    "large",
    "population",
    "chosen",
    "using",
    "theory",
    "probability",
    "right",
    "three",
    "types",
    "probability",
    "sampling",
    "right",
    "first",
    "random",
    "sampling",
    "method",
    "member",
    "population",
    "equal",
    "chance",
    "selected",
    "sample",
    "right",
    "every",
    "individual",
    "every",
    "object",
    "population",
    "equal",
    "john",
    "part",
    "sample",
    "random",
    "sampling",
    "okay",
    "randomly",
    "going",
    "select",
    "individual",
    "object",
    "bay",
    "individual",
    "equal",
    "chance",
    "selected",
    "correct",
    "next",
    "systematic",
    "sampling",
    "systematic",
    "sampling",
    "every",
    "nth",
    "record",
    "chosen",
    "population",
    "part",
    "sample",
    "right",
    "refer",
    "image",
    "shown",
    "six",
    "groups",
    "every",
    "second",
    "group",
    "chosen",
    "sample",
    "okay",
    "every",
    "second",
    "record",
    "chosen",
    "systematic",
    "sampling",
    "works",
    "okay",
    "randomly",
    "selecting",
    "nth",
    "record",
    "going",
    "add",
    "sample",
    "next",
    "stratified",
    "sampling",
    "type",
    "technique",
    "stratum",
    "used",
    "form",
    "samples",
    "large",
    "population",
    "stratum",
    "stratum",
    "basically",
    "subset",
    "population",
    "shares",
    "one",
    "common",
    "characteristics",
    "let",
    "say",
    "population",
    "mix",
    "male",
    "female",
    "create",
    "straightens",
    "one",
    "male",
    "subset",
    "female",
    "subset",
    "right",
    "stratum",
    "basically",
    "subset",
    "population",
    "shares",
    "least",
    "one",
    "common",
    "characteristics",
    "right",
    "example",
    "gender",
    "created",
    "stratum",
    "going",
    "use",
    "random",
    "sampling",
    "stratums",
    "going",
    "choose",
    "choose",
    "final",
    "sample",
    "random",
    "sampling",
    "meaning",
    "individuals",
    "stratum",
    "equal",
    "chance",
    "selected",
    "sample",
    "correct",
    "guys",
    "three",
    "different",
    "types",
    "sampling",
    "techniques",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "different",
    "types",
    "statistics",
    "looking",
    "advanced",
    "concepts",
    "statistics",
    "right",
    "far",
    "discuss",
    "basics",
    "statistics",
    "basically",
    "statistics",
    "friend",
    "sampling",
    "techniques",
    "terminologies",
    "statistics",
    "right",
    "look",
    "different",
    "types",
    "statistics",
    "two",
    "major",
    "types",
    "statistics",
    "descriptive",
    "statistics",
    "inferential",
    "statistics",
    "today",
    "session",
    "discussing",
    "types",
    "statistics",
    "depth",
    "right",
    "also",
    "looking",
    "demo",
    "running",
    "language",
    "order",
    "make",
    "understand",
    "exactly",
    "descriptive",
    "inferential",
    "statistics",
    "soaked",
    "going",
    "look",
    "basic",
    "worry",
    "much",
    "knowledge",
    "explaining",
    "everything",
    "basic",
    "level",
    "right",
    "guys",
    "descriptive",
    "statistics",
    "method",
    "used",
    "describe",
    "understand",
    "features",
    "specific",
    "data",
    "set",
    "giving",
    "short",
    "summary",
    "data",
    "okay",
    "mainly",
    "focused",
    "upon",
    "characteristics",
    "data",
    "also",
    "provides",
    "graphical",
    "summary",
    "data",
    "order",
    "make",
    "understand",
    "descriptive",
    "statistics",
    "let",
    "suppose",
    "want",
    "gift",
    "classmates",
    "study",
    "average",
    "shirt",
    "size",
    "student",
    "classroom",
    "use",
    "descriptive",
    "statistics",
    "study",
    "average",
    "shirt",
    "size",
    "students",
    "classroom",
    "would",
    "would",
    "record",
    "shirt",
    "size",
    "students",
    "class",
    "would",
    "find",
    "maximum",
    "minimum",
    "average",
    "shirt",
    "size",
    "cloud",
    "okay",
    "coming",
    "inferential",
    "statistics",
    "inferential",
    "six",
    "makes",
    "inferences",
    "predictions",
    "population",
    "based",
    "sample",
    "data",
    "taken",
    "population",
    "okay",
    "simple",
    "words",
    "generalizes",
    "large",
    "data",
    "set",
    "applies",
    "probability",
    "draw",
    "conclusion",
    "okay",
    "allows",
    "infer",
    "data",
    "parameters",
    "based",
    "statistical",
    "model",
    "using",
    "sample",
    "data",
    "consider",
    "example",
    "finding",
    "average",
    "shirt",
    "size",
    "students",
    "class",
    "infinite",
    "real",
    "statistics",
    "take",
    "sample",
    "set",
    "class",
    "basically",
    "people",
    "entire",
    "class",
    "right",
    "already",
    "grouped",
    "class",
    "large",
    "medium",
    "small",
    "right",
    "method",
    "basically",
    "build",
    "statistical",
    "model",
    "expand",
    "entire",
    "population",
    "class",
    "guys",
    "brief",
    "understanding",
    "descriptive",
    "inferential",
    "statistics",
    "difference",
    "descriptive",
    "inferential",
    "next",
    "section",
    "go",
    "depth",
    "descriptive",
    "statistics",
    "right",
    "let",
    "discuss",
    "descriptive",
    "statistics",
    "like",
    "mentioned",
    "earlier",
    "descriptive",
    "statistics",
    "method",
    "used",
    "describe",
    "understand",
    "features",
    "specific",
    "data",
    "set",
    "giving",
    "short",
    "summaries",
    "sample",
    "measures",
    "data",
    "two",
    "important",
    "measures",
    "descriptive",
    "statistics",
    "measure",
    "central",
    "tendency",
    "also",
    "known",
    "measure",
    "center",
    "measures",
    "variability",
    "also",
    "known",
    "measures",
    "spread",
    "measures",
    "center",
    "include",
    "mean",
    "median",
    "mode",
    "measures",
    "center",
    "measures",
    "center",
    "statistical",
    "measures",
    "represent",
    "summary",
    "data",
    "set",
    "okay",
    "three",
    "main",
    "measures",
    "center",
    "mean",
    "median",
    "mode",
    "coming",
    "measures",
    "variability",
    "measures",
    "spread",
    "range",
    "interquartile",
    "range",
    "variance",
    "standard",
    "deviation",
    "right",
    "let",
    "discuss",
    "measures",
    "little",
    "depth",
    "starting",
    "measures",
    "center",
    "sure",
    "know",
    "mean",
    "mean",
    "basically",
    "measure",
    "average",
    "values",
    "sample",
    "okay",
    "basically",
    "average",
    "values",
    "sample",
    "measure",
    "mean",
    "hope",
    "know",
    "main",
    "measured",
    "10",
    "numbers",
    "want",
    "find",
    "mean",
    "10",
    "numbers",
    "add",
    "10",
    "numbers",
    "divide",
    "10",
    "represents",
    "number",
    "samples",
    "data",
    "set",
    "right",
    "since",
    "10",
    "numbers",
    "going",
    "divide",
    "right",
    "give",
    "us",
    "average",
    "mean",
    "better",
    "understand",
    "measures",
    "central",
    "tendency",
    "let",
    "look",
    "example",
    "data",
    "set",
    "basically",
    "cars",
    "data",
    "set",
    "contains",
    "variables",
    "right",
    "something",
    "known",
    "cars",
    "mileage",
    "per",
    "gallon",
    "cylinder",
    "type",
    "displacement",
    "horsepower",
    "relax",
    "silver",
    "ratio",
    "right",
    "measures",
    "related",
    "cars",
    "okay",
    "going",
    "going",
    "use",
    "descriptive",
    "analysis",
    "going",
    "analyze",
    "variables",
    "sample",
    "data",
    "set",
    "mean",
    "standard",
    "deviation",
    "median",
    "let",
    "say",
    "want",
    "find",
    "mean",
    "average",
    "horsepower",
    "cars",
    "among",
    "population",
    "cards",
    "like",
    "mentioned",
    "earlier",
    "check",
    "average",
    "values",
    "case",
    "take",
    "sum",
    "horsepower",
    "car",
    "divide",
    "total",
    "number",
    "cards",
    "okay",
    "exactly",
    "done",
    "calculation",
    "part",
    "hundred",
    "ten",
    "basically",
    "represents",
    "horsepower",
    "first",
    "car",
    "right",
    "similarly",
    "added",
    "values",
    "horsepower",
    "cars",
    "divided",
    "8",
    "8",
    "basically",
    "number",
    "cars",
    "data",
    "set",
    "right",
    "hundred",
    "three",
    "point",
    "six",
    "two",
    "five",
    "army",
    "mean",
    "average",
    "horsepower",
    "right",
    "let",
    "understand",
    "median",
    "example",
    "okay",
    "define",
    "median",
    "median",
    "basically",
    "measure",
    "central",
    "value",
    "sample",
    "set",
    "called",
    "median",
    "right",
    "see",
    "middle",
    "value",
    "want",
    "find",
    "center",
    "value",
    "mileage",
    "per",
    "gallon",
    "among",
    "population",
    "cars",
    "first",
    "arrange",
    "mgp",
    "values",
    "ascending",
    "descending",
    "order",
    "choose",
    "middle",
    "value",
    "right",
    "case",
    "since",
    "eight",
    "values",
    "right",
    "eight",
    "values",
    "even",
    "entry",
    "whenever",
    "even",
    "number",
    "data",
    "points",
    "samples",
    "data",
    "set",
    "going",
    "take",
    "average",
    "two",
    "middle",
    "values",
    "nine",
    "values",
    "easily",
    "figure",
    "middle",
    "value",
    "know",
    "choose",
    "median",
    "since",
    "even",
    "number",
    "values",
    "going",
    "take",
    "average",
    "two",
    "middle",
    "values",
    "right",
    "right",
    "23",
    "two",
    "middle",
    "values",
    "taking",
    "mean",
    "2",
    "hence",
    "get",
    "twenty",
    "two",
    "point",
    "nine",
    "median",
    "right",
    "lastly",
    "let",
    "look",
    "mode",
    "calculated",
    "mode",
    "value",
    "recurrent",
    "sample",
    "set",
    "known",
    "mode",
    "basically",
    "value",
    "occurs",
    "often",
    "okay",
    "known",
    "mode",
    "let",
    "say",
    "want",
    "find",
    "common",
    "type",
    "cylinder",
    "among",
    "population",
    "cards",
    "check",
    "value",
    "repeated",
    "number",
    "times",
    "see",
    "cylinders",
    "come",
    "two",
    "types",
    "cylinder",
    "type",
    "4",
    "cylinder",
    "type",
    "6",
    "right",
    "take",
    "look",
    "data",
    "set",
    "see",
    "recurring",
    "value",
    "6",
    "right",
    "one",
    "two",
    "three",
    "four",
    "five",
    "five",
    "six",
    "one",
    "two",
    "three",
    "yeah",
    "three",
    "four",
    "types",
    "lenders",
    "five",
    "six",
    "types",
    "lenders",
    "basically",
    "three",
    "four",
    "type",
    "cylinders",
    "five",
    "six",
    "type",
    "cylinders",
    "right",
    "mode",
    "going",
    "6",
    "since",
    "6",
    "recurrent",
    "4",
    "guys",
    "measures",
    "center",
    "measures",
    "central",
    "tendency",
    "let",
    "move",
    "look",
    "measures",
    "spread",
    "right",
    "measure",
    "spread",
    "measure",
    "spread",
    "sometimes",
    "also",
    "called",
    "measure",
    "dispersion",
    "used",
    "describe",
    "variability",
    "sample",
    "population",
    "okay",
    "think",
    "sort",
    "deviation",
    "sample",
    "right",
    "measure",
    "help",
    "different",
    "measure",
    "spreads",
    "range",
    "interquartile",
    "range",
    "variance",
    "standard",
    "deviation",
    "range",
    "pretty",
    "right",
    "given",
    "measure",
    "spread",
    "apart",
    "values",
    "data",
    "set",
    "range",
    "calculated",
    "shown",
    "formula",
    "basically",
    "going",
    "subtract",
    "maximum",
    "value",
    "data",
    "set",
    "minimum",
    "value",
    "data",
    "set",
    "calculate",
    "range",
    "data",
    "alright",
    "next",
    "interquartile",
    "range",
    "discuss",
    "interquartile",
    "range",
    "let",
    "understand",
    "quartile",
    "red",
    "quartiles",
    "basically",
    "tell",
    "us",
    "spread",
    "data",
    "set",
    "breaking",
    "data",
    "set",
    "different",
    "quarters",
    "okay",
    "like",
    "median",
    "breaks",
    "data",
    "two",
    "parts",
    "court",
    "break",
    "different",
    "quarters",
    "better",
    "understand",
    "quartile",
    "interquartile",
    "calculated",
    "let",
    "look",
    "small",
    "example",
    "data",
    "set",
    "basically",
    "represents",
    "marks",
    "hundred",
    "students",
    "ordered",
    "lowest",
    "highest",
    "scores",
    "red",
    "quartiles",
    "lie",
    "following",
    "ranges",
    "first",
    "quartile",
    "also",
    "known",
    "q1",
    "lies",
    "25th",
    "26th",
    "observation",
    "right",
    "look",
    "highlighted",
    "add",
    "25th",
    "26th",
    "observation",
    "calculate",
    "q",
    "1",
    "first",
    "quartile",
    "taking",
    "average",
    "two",
    "values",
    "alright",
    "since",
    "values",
    "45",
    "add",
    "divide",
    "two",
    "still",
    "get",
    "45",
    "second",
    "quartile",
    "q",
    "2",
    "50th",
    "51st",
    "observation",
    "going",
    "take",
    "average",
    "58",
    "59",
    "get",
    "value",
    "second",
    "quarter",
    "third",
    "quartile",
    "ah",
    "q3",
    "75th",
    "76th",
    "observation",
    "take",
    "average",
    "two",
    "values",
    "75th",
    "value",
    "76",
    "value",
    "right",
    "get",
    "value",
    "right",
    "guys",
    "exactly",
    "calculate",
    "different",
    "quarters",
    "let",
    "look",
    "interquartile",
    "range",
    "iqr",
    "interquartile",
    "range",
    "measure",
    "variability",
    "based",
    "dividing",
    "data",
    "set",
    "quartiles",
    "interquartile",
    "range",
    "calculated",
    "subtracting",
    "q1",
    "q3",
    "basically",
    "q3",
    "minus",
    "q1",
    "iq",
    "iqr",
    "q3",
    "minus",
    "q1",
    "right",
    "quartiles",
    "core",
    "tile",
    "represents",
    "quarter",
    "25",
    "right",
    "guys",
    "hope",
    "clear",
    "interquartile",
    "range",
    "quartiles",
    "let",
    "look",
    "variance",
    "covariance",
    "basically",
    "measure",
    "shows",
    "much",
    "random",
    "variable",
    "first",
    "expected",
    "value",
    "okay",
    "basically",
    "variance",
    "variable",
    "variance",
    "calculated",
    "using",
    "formula",
    "right",
    "x",
    "basically",
    "represents",
    "data",
    "point",
    "data",
    "set",
    "n",
    "total",
    "number",
    "data",
    "points",
    "data",
    "set",
    "x",
    "bar",
    "basically",
    "main",
    "data",
    "points",
    "right",
    "calculate",
    "variance",
    "variance",
    "basically",
    "computing",
    "squares",
    "deviations",
    "okay",
    "says",
    "square",
    "let",
    "look",
    "deviation",
    "deviation",
    "difference",
    "element",
    "mean",
    "okay",
    "calculated",
    "using",
    "simple",
    "formula",
    "x",
    "basically",
    "represents",
    "data",
    "point",
    "mu",
    "mean",
    "population",
    "add",
    "exactly",
    "calculate",
    "deviation",
    "population",
    "variance",
    "sample",
    "variance",
    "specific",
    "whether",
    "calculating",
    "variance",
    "population",
    "data",
    "set",
    "sample",
    "data",
    "set",
    "difference",
    "population",
    "sample",
    "variance",
    "formula",
    "population",
    "variance",
    "pretty",
    "explanatory",
    "x",
    "basically",
    "data",
    "point",
    "mu",
    "mean",
    "population",
    "n",
    "number",
    "samples",
    "data",
    "set",
    "right",
    "let",
    "look",
    "sample",
    "variance",
    "sample",
    "variance",
    "average",
    "squared",
    "differences",
    "mean",
    "right",
    "x",
    "data",
    "point",
    "sample",
    "data",
    "set",
    "x",
    "bar",
    "mean",
    "sample",
    "right",
    "main",
    "population",
    "ation",
    "mean",
    "sample",
    "notice",
    "n",
    "smaller",
    "n",
    "number",
    "data",
    "points",
    "sample",
    "basically",
    "difference",
    "sample",
    "population",
    "variance",
    "hope",
    "clear",
    "coming",
    "standard",
    "deviation",
    "measure",
    "dispersion",
    "set",
    "data",
    "mean",
    "right",
    "basically",
    "deviation",
    "mean",
    "standard",
    "deviation",
    "better",
    "understand",
    "measures",
    "spread",
    "calculated",
    "let",
    "look",
    "small",
    "use",
    "case",
    "let",
    "see",
    "daenerys",
    "20",
    "dragons",
    "numbers",
    "nine",
    "five",
    "four",
    "shown",
    "screen",
    "work",
    "standard",
    "deviation",
    "order",
    "calculate",
    "standard",
    "deviation",
    "need",
    "know",
    "mean",
    "right",
    "first",
    "going",
    "find",
    "mean",
    "sample",
    "set",
    "calculate",
    "mean",
    "add",
    "numbers",
    "data",
    "set",
    "divided",
    "total",
    "number",
    "samples",
    "data",
    "set",
    "get",
    "value",
    "calculate",
    "rhs",
    "standard",
    "deviation",
    "formula",
    "right",
    "data",
    "point",
    "going",
    "subtract",
    "mean",
    "going",
    "square",
    "right",
    "get",
    "following",
    "result",
    "basically",
    "get",
    "425",
    "925",
    "finally",
    "find",
    "mean",
    "squared",
    "differences",
    "right",
    "standard",
    "deviation",
    "come",
    "two",
    "point",
    "nine",
    "eight",
    "three",
    "take",
    "square",
    "root",
    "guys",
    "pretty",
    "simple",
    "simple",
    "magic",
    "technique",
    "substitute",
    "values",
    "formula",
    "right",
    "hope",
    "clear",
    "let",
    "move",
    "discuss",
    "next",
    "topic",
    "information",
    "gain",
    "entropy",
    "one",
    "favorite",
    "topics",
    "statistics",
    "interesting",
    "topic",
    "mainly",
    "involved",
    "machine",
    "learning",
    "algorithms",
    "like",
    "decision",
    "trees",
    "random",
    "forest",
    "right",
    "important",
    "know",
    "information",
    "gain",
    "entropy",
    "really",
    "work",
    "essential",
    "building",
    "machine",
    "learning",
    "models",
    "focus",
    "statistic",
    "parts",
    "information",
    "gain",
    "entropy",
    "discuss",
    "use",
    "case",
    "see",
    "information",
    "gain",
    "entropy",
    "used",
    "decision",
    "trees",
    "know",
    "decision",
    "tree",
    "basically",
    "machine",
    "learning",
    "algorithm",
    "know",
    "anything",
    "explain",
    "everything",
    "depth",
    "worry",
    "let",
    "look",
    "exactly",
    "entropy",
    "information",
    "gain",
    "guys",
    "entropy",
    "basically",
    "measure",
    "sort",
    "uncertainty",
    "present",
    "data",
    "right",
    "measured",
    "using",
    "formula",
    "set",
    "instances",
    "data",
    "set",
    "data",
    "items",
    "data",
    "set",
    "n",
    "different",
    "type",
    "classes",
    "data",
    "set",
    "pi",
    "event",
    "probability",
    "might",
    "seem",
    "little",
    "confusing",
    "go",
    "use",
    "case",
    "understand",
    "terms",
    "even",
    "better",
    "right",
    "cam",
    "information",
    "gained",
    "word",
    "suggests",
    "information",
    "gain",
    "indicates",
    "much",
    "information",
    "particular",
    "feature",
    "particular",
    "variable",
    "gives",
    "us",
    "final",
    "outcome",
    "okay",
    "measured",
    "using",
    "formula",
    "heads",
    "entropy",
    "whole",
    "data",
    "set",
    "sj",
    "number",
    "instances",
    "j",
    "value",
    "attribute",
    "total",
    "number",
    "instances",
    "data",
    "set",
    "v",
    "set",
    "distinct",
    "values",
    "attribute",
    "h",
    "j",
    "entropy",
    "subsets",
    "instances",
    "hedge",
    "comma",
    "entropy",
    "attribute",
    "even",
    "though",
    "seems",
    "confusing",
    "clear",
    "confusion",
    "right",
    "let",
    "discuss",
    "small",
    "problem",
    "statement",
    "understand",
    "information",
    "gain",
    "entropy",
    "used",
    "study",
    "significance",
    "model",
    "like",
    "said",
    "information",
    "gain",
    "entropy",
    "important",
    "statistical",
    "measures",
    "let",
    "us",
    "understand",
    "significance",
    "predictive",
    "model",
    "okay",
    "get",
    "clear",
    "understanding",
    "let",
    "look",
    "use",
    "case",
    "right",
    "suppose",
    "given",
    "problem",
    "statement",
    "right",
    "statement",
    "predict",
    "whether",
    "match",
    "played",
    "studying",
    "weather",
    "conditions",
    "predictor",
    "variables",
    "outlook",
    "humidity",
    "wind",
    "day",
    "also",
    "predictor",
    "variable",
    "target",
    "variable",
    "basically",
    "played",
    "target",
    "variable",
    "variable",
    "trying",
    "protect",
    "okay",
    "value",
    "target",
    "variable",
    "decide",
    "whether",
    "game",
    "played",
    "right",
    "play",
    "two",
    "values",
    "yes",
    "meaning",
    "weather",
    "conditions",
    "good",
    "therefore",
    "play",
    "game",
    "yes",
    "meaning",
    "weather",
    "conditions",
    "good",
    "suitable",
    "play",
    "game",
    "alright",
    "problem",
    "statement",
    "hope",
    "problem",
    "statement",
    "clear",
    "solve",
    "problem",
    "make",
    "use",
    "something",
    "known",
    "decision",
    "trees",
    "guys",
    "think",
    "inverted",
    "tree",
    "branch",
    "tree",
    "denotes",
    "decision",
    "right",
    "branch",
    "known",
    "branch",
    "known",
    "branch",
    "node",
    "going",
    "take",
    "decision",
    "manner",
    "get",
    "outcome",
    "end",
    "branch",
    "right",
    "figure",
    "basically",
    "shows",
    "14",
    "observations",
    "9",
    "observations",
    "result",
    "yes",
    "meaning",
    "14",
    "days",
    "match",
    "played",
    "nine",
    "days",
    "alright",
    "see",
    "day",
    "1",
    "day",
    "2",
    "day",
    "8",
    "day",
    "9",
    "outlook",
    "alright",
    "basically",
    "try",
    "plaster",
    "data",
    "set",
    "depending",
    "outlook",
    "outlook",
    "sunny",
    "data",
    "set",
    "outlook",
    "overcast",
    "outlook",
    "rain",
    "right",
    "sunny",
    "two",
    "yeses",
    "three",
    "nodes",
    "okay",
    "outlook",
    "overcast",
    "four",
    "yes",
    "meaning",
    "four",
    "days",
    "outlook",
    "overcast",
    "play",
    "game",
    "right",
    "comes",
    "rain",
    "three",
    "yeses",
    "two",
    "nodes",
    "right",
    "notice",
    "decision",
    "made",
    "choosing",
    "outlook",
    "variable",
    "root",
    "node",
    "okay",
    "root",
    "node",
    "basically",
    "topmost",
    "node",
    "decision",
    "tree",
    "done",
    "created",
    "decision",
    "tree",
    "starts",
    "outlook",
    "node",
    "right",
    "splitting",
    "decision",
    "tree",
    "depending",
    "parameters",
    "like",
    "sunny",
    "overcast",
    "rain",
    "right",
    "like",
    "know",
    "outlook",
    "three",
    "values",
    "sunny",
    "overcast",
    "brain",
    "let",
    "explain",
    "manner",
    "okay",
    "making",
    "decision",
    "tree",
    "choosing",
    "outlook",
    "variable",
    "root",
    "node",
    "root",
    "note",
    "basically",
    "topmost",
    "node",
    "decision",
    "tree",
    "outlook",
    "node",
    "three",
    "branches",
    "coming",
    "sunny",
    "overcast",
    "rain",
    "basically",
    "outlook",
    "three",
    "values",
    "either",
    "sunny",
    "overcast",
    "rainy",
    "okay",
    "three",
    "values",
    "use",
    "assigned",
    "immediate",
    "branch",
    "nodes",
    "values",
    "possibility",
    "play",
    "equal",
    "yes",
    "calculated",
    "sunny",
    "rain",
    "branches",
    "give",
    "impure",
    "output",
    "meaning",
    "mix",
    "yes",
    "right",
    "two",
    "yeses",
    "three",
    "nodes",
    "three",
    "yeses",
    "two",
    "nodes",
    "comes",
    "overcast",
    "variable",
    "results",
    "hundred",
    "percent",
    "pure",
    "subset",
    "right",
    "shows",
    "overcast",
    "baby",
    "result",
    "definite",
    "certain",
    "output",
    "exactly",
    "entropy",
    "used",
    "measure",
    "right",
    "calculates",
    "impurity",
    "uncertainty",
    "alright",
    "lesser",
    "uncertainty",
    "entropy",
    "variable",
    "significant",
    "variable",
    "comes",
    "overcast",
    "literally",
    "impurity",
    "data",
    "set",
    "hundred",
    "percent",
    "pure",
    "subset",
    "right",
    "want",
    "variables",
    "like",
    "order",
    "build",
    "model",
    "right",
    "always",
    "ways",
    "get",
    "lucky",
    "always",
    "find",
    "variables",
    "result",
    "pure",
    "subsets",
    "measure",
    "entropy",
    "lesser",
    "entropy",
    "particular",
    "variable",
    "significant",
    "variable",
    "decision",
    "tree",
    "root",
    "node",
    "assigned",
    "best",
    "attribute",
    "decision",
    "tree",
    "predict",
    "precise",
    "outcome",
    "meaning",
    "root",
    "note",
    "significant",
    "variable",
    "right",
    "chosen",
    "outlook",
    "might",
    "ask",
    "chosen",
    "overcast",
    "okay",
    "overcast",
    "variable",
    "value",
    "outlook",
    "variable",
    "right",
    "chosen",
    "true",
    "cure",
    "hundred",
    "percent",
    "pure",
    "subset",
    "overcast",
    "right",
    "question",
    "head",
    "decide",
    "variable",
    "attribute",
    "best",
    "blitz",
    "data",
    "right",
    "know",
    "looked",
    "data",
    "told",
    "know",
    "hundred",
    "percent",
    "pure",
    "subset",
    "complex",
    "problem",
    "able",
    "understand",
    "variable",
    "best",
    "split",
    "data",
    "guys",
    "comes",
    "decision",
    "tree",
    "information",
    "gain",
    "entropy",
    "help",
    "understand",
    "variable",
    "best",
    "split",
    "data",
    "set",
    "right",
    "variable",
    "assign",
    "root",
    "node",
    "whichever",
    "variable",
    "assigned",
    "root",
    "node",
    "best",
    "let",
    "data",
    "set",
    "significant",
    "variable",
    "right",
    "need",
    "use",
    "information",
    "gain",
    "entropy",
    "total",
    "14",
    "instances",
    "saw",
    "nine",
    "said",
    "yes",
    "five",
    "instances",
    "said",
    "know",
    "play",
    "particular",
    "day",
    "right",
    "calculate",
    "entropy",
    "formula",
    "substitute",
    "values",
    "formula",
    "substitute",
    "values",
    "formula",
    "get",
    "value",
    "right",
    "entropy",
    "uncertainty",
    "data",
    "present",
    "sample",
    "order",
    "ensure",
    "choose",
    "best",
    "variable",
    "root",
    "node",
    "let",
    "us",
    "look",
    "possible",
    "combinations",
    "use",
    "root",
    "node",
    "okay",
    "possible",
    "combinations",
    "either",
    "outlook",
    "windy",
    "humidity",
    "temperature",
    "okay",
    "four",
    "variables",
    "one",
    "variables",
    "root",
    "note",
    "select",
    "variable",
    "best",
    "fits",
    "root",
    "node",
    "going",
    "see",
    "using",
    "information",
    "gain",
    "entropy",
    "guys",
    "task",
    "hand",
    "find",
    "information",
    "gain",
    "attributes",
    "right",
    "outlook",
    "windy",
    "humidity",
    "temperature",
    "going",
    "find",
    "information",
    "nation",
    "gained",
    "right",
    "point",
    "remember",
    "variable",
    "results",
    "highest",
    "information",
    "gain",
    "must",
    "chosen",
    "give",
    "us",
    "precise",
    "output",
    "information",
    "right",
    "information",
    "gain",
    "attribute",
    "windy",
    "calculate",
    "first",
    "six",
    "instances",
    "true",
    "eight",
    "instances",
    "false",
    "okay",
    "substitute",
    "values",
    "formula",
    "get",
    "value",
    "zero",
    "point",
    "zero",
    "four",
    "eight",
    "get",
    "value",
    "low",
    "value",
    "information",
    "gain",
    "right",
    "information",
    "going",
    "get",
    "windy",
    "attribute",
    "pretty",
    "low",
    "let",
    "calculate",
    "information",
    "gain",
    "attribute",
    "outlook",
    "right",
    "total",
    "14",
    "instances",
    "five",
    "instances",
    "say",
    "sunny",
    "instances",
    "overcast",
    "five",
    "instances",
    "rainy",
    "right",
    "sonny",
    "three",
    "yeses",
    "nose",
    "overcast",
    "yes",
    "three",
    "years",
    "two",
    "nodes",
    "okay",
    "calculate",
    "information",
    "gain",
    "outlook",
    "variable",
    "get",
    "value",
    "zero",
    "point",
    "2",
    "4",
    "7",
    "compare",
    "information",
    "gain",
    "windy",
    "attribute",
    "value",
    "actually",
    "pretty",
    "good",
    "right",
    "zero",
    "point",
    "2",
    "4",
    "7",
    "pretty",
    "good",
    "value",
    "information",
    "gain",
    "let",
    "look",
    "information",
    "gain",
    "attribute",
    "humidity",
    "seven",
    "instances",
    "say",
    "hi",
    "seven",
    "instances",
    "right",
    "high",
    "branch",
    "node",
    "three",
    "instances",
    "say",
    "yes",
    "rest",
    "instances",
    "would",
    "say",
    "similarly",
    "normal",
    "branch",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "instances",
    "would",
    "say",
    "yes",
    "one",
    "instance",
    "says",
    "right",
    "calculate",
    "information",
    "gain",
    "humidity",
    "variable",
    "going",
    "get",
    "value",
    "one",
    "also",
    "pretty",
    "decent",
    "value",
    "compare",
    "information",
    "gain",
    "attribute",
    "outlook",
    "less",
    "right",
    "let",
    "look",
    "information",
    "gain",
    "attribute",
    "temperature",
    "right",
    "temperature",
    "hold",
    "repeat",
    "basically",
    "temperature",
    "attribute",
    "hold",
    "hot",
    "mild",
    "cool",
    "okay",
    "hot",
    "two",
    "instances",
    "says",
    "yes",
    "two",
    "instances",
    "mild",
    "four",
    "instances",
    "yes",
    "two",
    "instances",
    "col",
    "three",
    "instances",
    "yes",
    "one",
    "instance",
    "right",
    "calculate",
    "information",
    "gain",
    "attribute",
    "get",
    "value",
    "zero",
    "point",
    "zero",
    "nine",
    "less",
    "summarize",
    "look",
    "information",
    "gain",
    "variable",
    "see",
    "outlook",
    "maximum",
    "gain",
    "right",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "highest",
    "information",
    "gain",
    "value",
    "must",
    "always",
    "choose",
    "variable",
    "highest",
    "information",
    "gain",
    "split",
    "data",
    "root",
    "node",
    "assign",
    "outlook",
    "variable",
    "root",
    "node",
    "right",
    "guys",
    "hope",
    "use",
    "case",
    "clear",
    "doubts",
    "please",
    "keep",
    "commenting",
    "doubts",
    "let",
    "move",
    "look",
    "exactly",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "last",
    "topic",
    "descriptive",
    "statistics",
    "read",
    "running",
    "short",
    "demo",
    "showing",
    "calculate",
    "mean",
    "median",
    "mode",
    "standard",
    "deviation",
    "variance",
    "values",
    "using",
    "okay",
    "let",
    "talk",
    "confusion",
    "matrix",
    "guys",
    "confusion",
    "matrix",
    "get",
    "confused",
    "complex",
    "topic",
    "confusion",
    "matrix",
    "matrix",
    "often",
    "used",
    "describe",
    "performance",
    "model",
    "right",
    "specifically",
    "used",
    "classification",
    "models",
    "classifier",
    "calculate",
    "accuracy",
    "calculate",
    "performance",
    "classifier",
    "comparing",
    "actual",
    "results",
    "predicted",
    "results",
    "right",
    "looks",
    "like",
    "prosit",
    "little",
    "confusing",
    "get",
    "back",
    "exactly",
    "true",
    "positive",
    "negative",
    "stands",
    "let",
    "look",
    "example",
    "let",
    "try",
    "understand",
    "exactly",
    "confusion",
    "matrix",
    "guys",
    "made",
    "sure",
    "put",
    "examples",
    "every",
    "topic",
    "important",
    "understand",
    "practical",
    "part",
    "statistics",
    "right",
    "statistics",
    "literally",
    "nothing",
    "theory",
    "need",
    "understand",
    "calculations",
    "done",
    "statistics",
    "okay",
    "done",
    "let",
    "look",
    "small",
    "use",
    "case",
    "okay",
    "let",
    "consider",
    "given",
    "data",
    "hundred",
    "patient",
    "hundred",
    "five",
    "patients",
    "disease",
    "remaining",
    "50",
    "patients",
    "disease",
    "okay",
    "going",
    "build",
    "classifier",
    "predicts",
    "using",
    "hundred",
    "sixty",
    "five",
    "observations",
    "feed",
    "165",
    "observations",
    "classifier",
    "predict",
    "output",
    "every",
    "time",
    "new",
    "patients",
    "detail",
    "fed",
    "classifier",
    "right",
    "165",
    "cases",
    "let",
    "say",
    "classifier",
    "predicted",
    "yes",
    "hundred",
    "ten",
    "times",
    "55",
    "times",
    "alright",
    "yes",
    "basically",
    "stands",
    "yes",
    "person",
    "disease",
    "stands",
    "know",
    "person",
    "disease",
    "right",
    "pretty",
    "yeah",
    "predicted",
    "hundred",
    "ten",
    "times",
    "patient",
    "disease",
    "55",
    "times",
    "patient",
    "disease",
    "however",
    "reality",
    "hundred",
    "five",
    "patients",
    "samples",
    "disease",
    "60",
    "patients",
    "disease",
    "right",
    "calculate",
    "accuracy",
    "model",
    "basically",
    "build",
    "confusion",
    "matrix",
    "right",
    "matrix",
    "looks",
    "like",
    "basically",
    "denotes",
    "total",
    "number",
    "observations",
    "165",
    "case",
    "actual",
    "denotes",
    "actual",
    "use",
    "data",
    "set",
    "predicted",
    "denotes",
    "predicted",
    "values",
    "classifier",
    "actual",
    "value",
    "predicted",
    "value",
    "classifier",
    "correctly",
    "able",
    "classify",
    "50",
    "cases",
    "right",
    "since",
    "50",
    "correctly",
    "able",
    "classify",
    "10",
    "cases",
    "incorrectly",
    "classified",
    "meaning",
    "actual",
    "value",
    "classifier",
    "predicted",
    "yes",
    "similarly",
    "wrongly",
    "predicted",
    "five",
    "patients",
    "diseases",
    "whereas",
    "actually",
    "diseases",
    "correctly",
    "predicted",
    "hundred",
    "patients",
    "disease",
    "right",
    "know",
    "little",
    "bit",
    "confusing",
    "look",
    "values",
    "50",
    "meaning",
    "correctly",
    "predicted",
    "50",
    "values",
    "yes",
    "means",
    "wrongly",
    "predicted",
    "yes",
    "values",
    "supposed",
    "predict",
    "right",
    "exactly",
    "true",
    "positive",
    "negative",
    "tell",
    "exactly",
    "true",
    "positive",
    "cases",
    "predicted",
    "yes",
    "actually",
    "disease",
    "right",
    "basically",
    "value",
    "already",
    "predicted",
    "yes",
    "even",
    "though",
    "disease",
    "10",
    "true",
    "positives",
    "right",
    "similarly",
    "predicted",
    "know",
    "disease",
    "meaning",
    "correct",
    "false",
    "positive",
    "predicted",
    "yes",
    "actually",
    "disease",
    "right",
    "also",
    "known",
    "type",
    "1",
    "error",
    "predicted",
    "actually",
    "disease",
    "guys",
    "basically",
    "false",
    "negative",
    "true",
    "negatives",
    "basically",
    "correct",
    "classifications",
    "right",
    "confusion",
    "matrix",
    "hope",
    "concept",
    "clear",
    "guys",
    "doubts",
    "please",
    "comment",
    "doubt",
    "comment",
    "section",
    "guys",
    "descriptive",
    "statistics",
    "go",
    "probability",
    "promised",
    "run",
    "small",
    "demo",
    "right",
    "try",
    "understand",
    "mean",
    "median",
    "mode",
    "works",
    "okay",
    "let",
    "first",
    "guys",
    "discussed",
    "far",
    "descriptive",
    "statistics",
    "right",
    "next",
    "going",
    "discuss",
    "probability",
    "move",
    "inferential",
    "statistics",
    "okay",
    "financial",
    "statistics",
    "basically",
    "second",
    "type",
    "statistics",
    "okay",
    "make",
    "things",
    "clear",
    "let",
    "zoom",
    "guys",
    "always",
    "best",
    "perform",
    "practical",
    "implementations",
    "order",
    "understand",
    "concepts",
    "better",
    "way",
    "okay",
    "executing",
    "small",
    "demo",
    "show",
    "calculate",
    "mean",
    "median",
    "mode",
    "variance",
    "standard",
    "deviation",
    "study",
    "variables",
    "plotting",
    "histogram",
    "okay",
    "worry",
    "know",
    "histogram",
    "basically",
    "frequency",
    "plot",
    "big",
    "signs",
    "behind",
    "alright",
    "simple",
    "demo",
    "also",
    "forms",
    "foundation",
    "everything",
    "machine",
    "learning",
    "algorithm",
    "built",
    "upon",
    "okay",
    "say",
    "machine",
    "learning",
    "algorithms",
    "actually",
    "machine",
    "learning",
    "algorithms",
    "deep",
    "learning",
    "algorithms",
    "basic",
    "concept",
    "behind",
    "okay",
    "need",
    "know",
    "mean",
    "median",
    "mode",
    "calculated",
    "guys",
    "using",
    "language",
    "perform",
    "running",
    "studio",
    "know",
    "language",
    "leave",
    "couple",
    "links",
    "description",
    "box",
    "go",
    "videos",
    "randomly",
    "generated",
    "eating",
    "numbers",
    "miss",
    "storing",
    "variable",
    "called",
    "data",
    "right",
    "want",
    "see",
    "generated",
    "numbers",
    "run",
    "line",
    "data",
    "right",
    "variable",
    "basically",
    "stores",
    "numbers",
    "right",
    "going",
    "going",
    "calculate",
    "mean",
    "specify",
    "word",
    "mean",
    "along",
    "data",
    "calculating",
    "mean",
    "assigned",
    "whole",
    "thing",
    "variable",
    "called",
    "mean",
    "hold",
    "mean",
    "value",
    "data",
    "let",
    "look",
    "mean",
    "abuser",
    "function",
    "called",
    "print",
    "mean",
    "right",
    "mean",
    "around",
    "okay",
    "next",
    "calculating",
    "median",
    "simple",
    "guys",
    "use",
    "function",
    "median",
    "write",
    "pass",
    "data",
    "parameter",
    "function",
    "provides",
    "functions",
    "everything",
    "right",
    "statistics",
    "easy",
    "comes",
    "r",
    "r",
    "basically",
    "statistical",
    "language",
    "okay",
    "name",
    "function",
    "function",
    "ready",
    "built",
    "art",
    "okay",
    "median",
    "around",
    "similarly",
    "calculate",
    "mode",
    "right",
    "let",
    "run",
    "function",
    "basically",
    "created",
    "small",
    "function",
    "calculating",
    "mode",
    "guys",
    "mode",
    "meaning",
    "recurrent",
    "value",
    "right",
    "going",
    "calculate",
    "variance",
    "standard",
    "deviation",
    "function",
    "called",
    "right",
    "pass",
    "data",
    "function",
    "okay",
    "similarly",
    "calculate",
    "standard",
    "deviation",
    "basically",
    "square",
    "root",
    "variance",
    "right",
    "rent",
    "standard",
    "deviation",
    "right",
    "standard",
    "deviation",
    "value",
    "finally",
    "plot",
    "small",
    "histogram",
    "histogram",
    "nothing",
    "frequency",
    "plot",
    "already",
    "show",
    "frequently",
    "data",
    "point",
    "occurring",
    "histogram",
    "created",
    "quite",
    "simple",
    "lot",
    "packages",
    "lot",
    "inbuilt",
    "functions",
    "support",
    "statistics",
    "right",
    "statistical",
    "language",
    "mainly",
    "used",
    "data",
    "scientists",
    "data",
    "analysts",
    "machine",
    "learning",
    "engineers",
    "student",
    "code",
    "functions",
    "mention",
    "name",
    "function",
    "pass",
    "corresponding",
    "parameters",
    "guys",
    "entire",
    "descriptive",
    "statistics",
    "module",
    "discuss",
    "probability",
    "okay",
    "understand",
    "exactly",
    "probability",
    "let",
    "clear",
    "common",
    "misconception",
    "people",
    "often",
    "tend",
    "ask",
    "question",
    "relationship",
    "statistics",
    "probability",
    "probability",
    "statistics",
    "related",
    "fields",
    "right",
    "probability",
    "mathematical",
    "method",
    "used",
    "statistical",
    "analysis",
    "therefore",
    "say",
    "probability",
    "statistics",
    "interconnected",
    "branches",
    "mathematics",
    "deal",
    "analyzing",
    "relative",
    "frequency",
    "events",
    "interconnected",
    "feels",
    "probability",
    "makes",
    "use",
    "statistics",
    "statistics",
    "makes",
    "use",
    "probability",
    "interconnected",
    "fields",
    "relationship",
    "said",
    "six",
    "probability",
    "let",
    "understand",
    "exactly",
    "probability",
    "probability",
    "measure",
    "likely",
    "event",
    "occur",
    "precise",
    "ratio",
    "desired",
    "outcome",
    "total",
    "outcomes",
    "probability",
    "outcomes",
    "always",
    "sum",
    "1",
    "probability",
    "always",
    "sum",
    "1",
    "probability",
    "go",
    "beyond",
    "one",
    "okay",
    "either",
    "probability",
    "0",
    "1",
    "form",
    "decimals",
    "like",
    "form",
    "valuable",
    "always",
    "stay",
    "range",
    "0",
    "1",
    "okay",
    "another",
    "famous",
    "example",
    "probability",
    "rolling",
    "dice",
    "example",
    "roll",
    "dice",
    "get",
    "six",
    "possible",
    "outcomes",
    "right",
    "get",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "phases",
    "dies",
    "possibility",
    "one",
    "outcome",
    "probability",
    "rolling",
    "dice",
    "get",
    "3",
    "probability",
    "1",
    "6",
    "right",
    "one",
    "phase",
    "number",
    "3",
    "six",
    "phases",
    "one",
    "phase",
    "number",
    "three",
    "probability",
    "getting",
    "3",
    "roll",
    "dice",
    "1",
    "6",
    "similarly",
    "want",
    "find",
    "probability",
    "getting",
    "number",
    "5",
    "probability",
    "going",
    "1",
    "right",
    "sum",
    "right",
    "guys",
    "exactly",
    "ability",
    "simple",
    "concept",
    "learnt",
    "8",
    "standard",
    "onwards",
    "right",
    "let",
    "understand",
    "different",
    "terminologies",
    "related",
    "probability",
    "three",
    "terminologies",
    "often",
    "come",
    "across",
    "talk",
    "probability",
    "something",
    "known",
    "random",
    "experiment",
    "okay",
    "basically",
    "experiment",
    "process",
    "outcomes",
    "predicted",
    "certainty",
    "right",
    "use",
    "probability",
    "going",
    "use",
    "probability",
    "order",
    "predict",
    "outcome",
    "sort",
    "certainty",
    "sample",
    "space",
    "entire",
    "possible",
    "set",
    "outcomes",
    "random",
    "experiment",
    "event",
    "one",
    "outcomes",
    "experiment",
    "consider",
    "example",
    "rolling",
    "dice",
    "let",
    "say",
    "want",
    "find",
    "probability",
    "getting",
    "roll",
    "dice",
    "okay",
    "finding",
    "probability",
    "random",
    "experiment",
    "sample",
    "space",
    "basically",
    "entire",
    "possibility",
    "okay",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "need",
    "find",
    "probability",
    "getting",
    "2",
    "right",
    "possible",
    "outcomes",
    "basically",
    "represent",
    "sample",
    "space",
    "gives",
    "1",
    "6",
    "possible",
    "outcomes",
    "represents",
    "sample",
    "space",
    "event",
    "one",
    "outcome",
    "experiment",
    "case",
    "event",
    "get",
    "tattoo",
    "roll",
    "dice",
    "right",
    "event",
    "probability",
    "getting",
    "roll",
    "dice",
    "guys",
    "basically",
    "random",
    "experiment",
    "samples",
    "space",
    "event",
    "really",
    "means",
    "alright",
    "let",
    "discuss",
    "different",
    "types",
    "events",
    "two",
    "types",
    "events",
    "know",
    "disjoint",
    "non",
    "disjoint",
    "events",
    "disjoint",
    "events",
    "events",
    "common",
    "outcome",
    "example",
    "draw",
    "single",
    "card",
    "deck",
    "cards",
    "king",
    "queen",
    "correct",
    "either",
    "king",
    "queen",
    "non",
    "disjoint",
    "events",
    "events",
    "common",
    "example",
    "student",
    "get",
    "hundred",
    "marks",
    "statistics",
    "hundred",
    "marks",
    "probability",
    "right",
    "also",
    "outcome",
    "ball",
    "delivered",
    "ball",
    "6",
    "right",
    "non",
    "disjoint",
    "events",
    "n",
    "simple",
    "understand",
    "right",
    "let",
    "move",
    "look",
    "different",
    "types",
    "probability",
    "distribution",
    "right",
    "discussing",
    "three",
    "main",
    "probability",
    "distribution",
    "functions",
    "talking",
    "probability",
    "density",
    "aaron",
    "normal",
    "distribution",
    "central",
    "limit",
    "theorem",
    "okay",
    "probability",
    "density",
    "function",
    "also",
    "known",
    "pdf",
    "concerned",
    "relative",
    "likelihood",
    "continuous",
    "random",
    "variable",
    "take",
    "given",
    "value",
    "alright",
    "pdf",
    "gives",
    "probability",
    "variable",
    "lies",
    "range",
    "basically",
    "trying",
    "going",
    "try",
    "find",
    "probability",
    "continuous",
    "random",
    "variable",
    "specified",
    "range",
    "okay",
    "graph",
    "denotes",
    "pdf",
    "continuous",
    "variable",
    "graph",
    "also",
    "known",
    "bell",
    "curve",
    "right",
    "famously",
    "called",
    "bell",
    "curve",
    "shape",
    "three",
    "important",
    "properties",
    "need",
    "know",
    "probability",
    "density",
    "function",
    "graph",
    "pdf",
    "continuous",
    "range",
    "finding",
    "probability",
    "continuous",
    "variable",
    "lies",
    "ranges",
    "b",
    "right",
    "second",
    "property",
    "area",
    "bounded",
    "curve",
    "density",
    "function",
    "equal",
    "1",
    "basically",
    "area",
    "curve",
    "equal",
    "1",
    "right",
    "denotes",
    "probability",
    "probability",
    "arrange",
    "one",
    "0",
    "1",
    "property",
    "number",
    "three",
    "probability",
    "random",
    "variable",
    "assumes",
    "value",
    "b",
    "equal",
    "area",
    "pdf",
    "bounded",
    "okay",
    "means",
    "probability",
    "denoted",
    "area",
    "graph",
    "right",
    "whatever",
    "value",
    "get",
    "basically",
    "one",
    "probability",
    "random",
    "variable",
    "lie",
    "range",
    "right",
    "hope",
    "understood",
    "probability",
    "density",
    "function",
    "basically",
    "probability",
    "finding",
    "value",
    "continuous",
    "random",
    "variable",
    "range",
    "right",
    "let",
    "look",
    "next",
    "distribution",
    "normal",
    "distribution",
    "normal",
    "distribution",
    "also",
    "known",
    "gaussian",
    "distribution",
    "probability",
    "distribution",
    "denotes",
    "symmetric",
    "property",
    "mean",
    "right",
    "meaning",
    "idea",
    "behind",
    "function",
    "data",
    "near",
    "mean",
    "occurs",
    "frequently",
    "data",
    "away",
    "mean",
    "means",
    "say",
    "data",
    "around",
    "mean",
    "represents",
    "entire",
    "data",
    "set",
    "okay",
    "take",
    "sample",
    "data",
    "around",
    "mean",
    "represent",
    "entire",
    "data",
    "set",
    "similar",
    "probability",
    "density",
    "function",
    "normal",
    "distribution",
    "appears",
    "bell",
    "curve",
    "right",
    "comes",
    "normal",
    "distribution",
    "two",
    "important",
    "factors",
    "right",
    "mean",
    "population",
    "standard",
    "deviation",
    "okay",
    "mean",
    "graph",
    "determines",
    "location",
    "center",
    "graph",
    "right",
    "standard",
    "deviation",
    "determines",
    "height",
    "graph",
    "okay",
    "standard",
    "deviation",
    "large",
    "curve",
    "going",
    "look",
    "something",
    "like",
    "right",
    "short",
    "wide",
    "standard",
    "deviation",
    "small",
    "curve",
    "tall",
    "narrow",
    "right",
    "normal",
    "distribution",
    "let",
    "look",
    "central",
    "limit",
    "theorem",
    "central",
    "limit",
    "theorem",
    "states",
    "sampling",
    "distribution",
    "mean",
    "independent",
    "random",
    "variable",
    "normal",
    "nearly",
    "normal",
    "sample",
    "size",
    "large",
    "enough",
    "little",
    "confusing",
    "okay",
    "let",
    "break",
    "simple",
    "terms",
    "large",
    "population",
    "many",
    "samples",
    "mean",
    "samples",
    "population",
    "almost",
    "equal",
    "mean",
    "entire",
    "population",
    "right",
    "meaning",
    "sample",
    "normally",
    "distributed",
    "right",
    "compare",
    "mean",
    "sample",
    "almost",
    "equal",
    "mean",
    "population",
    "right",
    "graph",
    "basically",
    "shows",
    "clear",
    "understanding",
    "central",
    "limit",
    "theorem",
    "red",
    "see",
    "sample",
    "mean",
    "sample",
    "oil",
    "almost",
    "along",
    "line",
    "right",
    "okay",
    "exactly",
    "central",
    "limit",
    "theorem",
    "states",
    "accuracy",
    "resemblance",
    "normal",
    "distribution",
    "depends",
    "two",
    "main",
    "factors",
    "right",
    "first",
    "number",
    "sample",
    "points",
    "consider",
    "right",
    "second",
    "shape",
    "underlying",
    "population",
    "shape",
    "obviously",
    "depends",
    "standard",
    "deviation",
    "mean",
    "sample",
    "correct",
    "guys",
    "central",
    "limit",
    "theorem",
    "basically",
    "states",
    "eats",
    "bill",
    "normally",
    "distributed",
    "way",
    "mean",
    "sample",
    "coincide",
    "mean",
    "actual",
    "population",
    "right",
    "short",
    "terms",
    "central",
    "limit",
    "theorem",
    "states",
    "right",
    "holds",
    "true",
    "large",
    "data",
    "set",
    "mostly",
    "small",
    "data",
    "set",
    "deviations",
    "compared",
    "large",
    "data",
    "set",
    "scaling",
    "factor",
    "right",
    "small",
    "deviation",
    "small",
    "data",
    "set",
    "change",
    "value",
    "vary",
    "drastically",
    "large",
    "data",
    "set",
    "small",
    "deviation",
    "matter",
    "let",
    "move",
    "vaughn",
    "look",
    "next",
    "topic",
    "different",
    "types",
    "probability",
    "important",
    "topic",
    "problems",
    "solved",
    "understanding",
    "type",
    "probability",
    "use",
    "solve",
    "problem",
    "right",
    "three",
    "important",
    "types",
    "probability",
    "marginal",
    "joint",
    "conditional",
    "probability",
    "let",
    "discuss",
    "probability",
    "event",
    "occurring",
    "unconditioned",
    "event",
    "known",
    "marginal",
    "unconditional",
    "probability",
    "let",
    "say",
    "want",
    "find",
    "probability",
    "card",
    "drawn",
    "heart",
    "right",
    "want",
    "find",
    "probability",
    "card",
    "drawn",
    "heart",
    "profit",
    "13",
    "52",
    "since",
    "52",
    "cards",
    "deck",
    "13",
    "hearts",
    "deck",
    "cards",
    "right",
    "52",
    "cards",
    "total",
    "deck",
    "marginal",
    "probability",
    "13",
    "marginal",
    "probability",
    "let",
    "understand",
    "joint",
    "probability",
    "joint",
    "probability",
    "measure",
    "two",
    "events",
    "happening",
    "time",
    "okay",
    "let",
    "say",
    "two",
    "events",
    "probability",
    "event",
    "b",
    "occurring",
    "intersection",
    "example",
    "want",
    "find",
    "probability",
    "card",
    "four",
    "red",
    "would",
    "joint",
    "probability",
    "right",
    "finding",
    "card",
    "4",
    "card",
    "red",
    "color",
    "answer",
    "would",
    "biceps",
    "heart",
    "diamonds",
    "correct",
    "red",
    "color",
    "therefore",
    "probability",
    "52",
    "1",
    "26",
    "right",
    "joint",
    "probability",
    "moving",
    "let",
    "look",
    "exactly",
    "conditional",
    "probability",
    "probability",
    "event",
    "outcome",
    "based",
    "occurrence",
    "previous",
    "event",
    "outcome",
    "call",
    "conditional",
    "probability",
    "okay",
    "conditional",
    "probability",
    "event",
    "b",
    "probability",
    "event",
    "occur",
    "given",
    "event",
    "already",
    "occurred",
    "right",
    "b",
    "dependent",
    "events",
    "expression",
    "conditional",
    "probability",
    "given",
    "first",
    "term",
    "left",
    "hand",
    "side",
    "p",
    "b",
    "basically",
    "probability",
    "event",
    "b",
    "occurring",
    "given",
    "event",
    "already",
    "occurred",
    "like",
    "said",
    "b",
    "dependent",
    "events",
    "expression",
    "b",
    "independent",
    "events",
    "expression",
    "conditional",
    "probability",
    "like",
    "right",
    "guys",
    "p",
    "b",
    "b",
    "obviously",
    "probability",
    "probability",
    "b",
    "right",
    "let",
    "move",
    "order",
    "understand",
    "conditional",
    "probability",
    "joint",
    "probability",
    "marginal",
    "probability",
    "let",
    "look",
    "small",
    "use",
    "case",
    "okay",
    "basically",
    "going",
    "take",
    "data",
    "set",
    "examines",
    "salary",
    "package",
    "training",
    "undergone",
    "candidates",
    "okay",
    "60",
    "candidates",
    "without",
    "training",
    "forty",
    "five",
    "candidates",
    "enrolled",
    "adder",
    "acres",
    "training",
    "right",
    "task",
    "assess",
    "training",
    "salary",
    "package",
    "okay",
    "let",
    "look",
    "little",
    "depth",
    "total",
    "hundred",
    "five",
    "candidates",
    "60",
    "enrolled",
    "frederick",
    "training",
    "45",
    "enrolled",
    "deer",
    "acres",
    "inning",
    "right",
    "small",
    "survey",
    "conducted",
    "rating",
    "package",
    "salary",
    "got",
    "right",
    "read",
    "data",
    "understand",
    "five",
    "candidates",
    "without",
    "eddie",
    "record",
    "training",
    "got",
    "poor",
    "salary",
    "package",
    "okay",
    "similarly",
    "30",
    "candidates",
    "ed",
    "eureka",
    "training",
    "got",
    "good",
    "package",
    "right",
    "guys",
    "basically",
    "comparing",
    "salary",
    "package",
    "person",
    "depending",
    "whether",
    "enrolled",
    "core",
    "training",
    "right",
    "data",
    "set",
    "let",
    "look",
    "problem",
    "statement",
    "find",
    "probability",
    "candidate",
    "undergone",
    "editor",
    "acres",
    "training",
    "quite",
    "simple",
    "type",
    "probability",
    "marginal",
    "probability",
    "right",
    "probability",
    "candidate",
    "undergone",
    "edge",
    "rakers",
    "training",
    "obviously",
    "45",
    "divided",
    "hundred",
    "five",
    "since",
    "45",
    "number",
    "candidates",
    "eddie",
    "record",
    "raining",
    "hundred",
    "five",
    "total",
    "number",
    "candidates",
    "value",
    "approximately",
    "probability",
    "candidate",
    "undergone",
    "judaica",
    "straining",
    "next",
    "question",
    "find",
    "probability",
    "candidate",
    "attended",
    "edger",
    "constraining",
    "also",
    "good",
    "package",
    "obviously",
    "joint",
    "probability",
    "problem",
    "right",
    "calculate",
    "since",
    "table",
    "quite",
    "formatted",
    "directly",
    "find",
    "people",
    "gotten",
    "good",
    "package",
    "along",
    "eddie",
    "record",
    "raining",
    "30",
    "right",
    "hundred",
    "five",
    "people",
    "30",
    "people",
    "education",
    "training",
    "good",
    "package",
    "right",
    "specifically",
    "asking",
    "people",
    "ado",
    "rekha",
    "training",
    "remember",
    "right",
    "question",
    "find",
    "probability",
    "candidate",
    "attended",
    "editor",
    "acres",
    "training",
    "also",
    "good",
    "package",
    "alright",
    "need",
    "consider",
    "two",
    "factors",
    "candidate",
    "addenda",
    "deaderick",
    "training",
    "good",
    "package",
    "clearly",
    "number",
    "30",
    "30",
    "divided",
    "total",
    "number",
    "candidates",
    "1",
    "0",
    "five",
    "right",
    "get",
    "answer",
    "clearly",
    "next",
    "find",
    "probability",
    "candidate",
    "good",
    "package",
    "given",
    "undergone",
    "training",
    "okay",
    "clearly",
    "conditional",
    "probability",
    "defining",
    "condition",
    "saying",
    "want",
    "find",
    "probability",
    "candidate",
    "good",
    "package",
    "given",
    "undergone",
    "training",
    "right",
    "condition",
    "undergone",
    "training",
    "right",
    "number",
    "people",
    "undergone",
    "training",
    "60",
    "five",
    "got",
    "good",
    "package",
    "right",
    "phi",
    "60",
    "5",
    "hundred",
    "five",
    "clearly",
    "mentioned",
    "good",
    "package",
    "given",
    "undergone",
    "training",
    "consider",
    "people",
    "undergone",
    "training",
    "right",
    "five",
    "people",
    "undergone",
    "training",
    "gotten",
    "good",
    "package",
    "right",
    "5",
    "divided",
    "60",
    "get",
    "probability",
    "around",
    "208",
    "pretty",
    "low",
    "right",
    "okay",
    "different",
    "types",
    "probability",
    "let",
    "move",
    "look",
    "last",
    "topic",
    "probability",
    "base",
    "theorem",
    "guys",
    "bayes",
    "theorem",
    "important",
    "concept",
    "comes",
    "statistics",
    "probability",
    "majorly",
    "used",
    "knife",
    "bias",
    "algorithm",
    "aware",
    "bias",
    "supervised",
    "learning",
    "classification",
    "algorithm",
    "mainly",
    "used",
    "gmail",
    "spam",
    "filtering",
    "right",
    "lot",
    "might",
    "noticed",
    "open",
    "gmail",
    "see",
    "folder",
    "called",
    "spam",
    "right",
    "carried",
    "machine",
    "learning",
    "algorithm",
    "used",
    "knife",
    "bias",
    "right",
    "let",
    "discuss",
    "exactly",
    "bayes",
    "theorem",
    "denotes",
    "bias",
    "theorem",
    "used",
    "show",
    "relation",
    "one",
    "conditional",
    "probability",
    "inverse",
    "right",
    "basically",
    "nothing",
    "probability",
    "event",
    "occurring",
    "based",
    "prior",
    "knowledge",
    "conditions",
    "might",
    "related",
    "event",
    "okay",
    "mathematically",
    "bell",
    "theorem",
    "represented",
    "like",
    "right",
    "like",
    "shown",
    "equation",
    "term",
    "referred",
    "likelihood",
    "ratio",
    "measures",
    "probability",
    "occurrence",
    "event",
    "b",
    "given",
    "event",
    "okay",
    "left",
    "hand",
    "side",
    "known",
    "posterior",
    "right",
    "referred",
    "posterior",
    "means",
    "probability",
    "occurrence",
    "given",
    "event",
    "b",
    "right",
    "second",
    "term",
    "referred",
    "likelihood",
    "ratio",
    "measures",
    "probability",
    "occurrence",
    "b",
    "given",
    "event",
    "p",
    "also",
    "known",
    "prior",
    "refers",
    "actual",
    "probability",
    "distribution",
    "p",
    "b",
    "probability",
    "b",
    "right",
    "bias",
    "theorem",
    "order",
    "better",
    "understand",
    "base",
    "theorem",
    "let",
    "look",
    "small",
    "example",
    "let",
    "say",
    "three",
    "balls",
    "bowel",
    "bouncy",
    "okay",
    "barley",
    "contains",
    "two",
    "blue",
    "balls",
    "red",
    "balls",
    "bowel",
    "contains",
    "eight",
    "blue",
    "balls",
    "red",
    "balls",
    "baozi",
    "contains",
    "one",
    "blue",
    "ball",
    "three",
    "red",
    "balls",
    "draw",
    "one",
    "ball",
    "bowl",
    "probability",
    "draw",
    "blue",
    "ball",
    "bowel",
    "know",
    "drew",
    "exactly",
    "total",
    "two",
    "blue",
    "balls",
    "right",
    "understand",
    "question",
    "please",
    "read",
    "shall",
    "pause",
    "second",
    "two",
    "right",
    "hope",
    "understood",
    "question",
    "okay",
    "going",
    "going",
    "draw",
    "blueprint",
    "tell",
    "exactly",
    "solve",
    "problem",
    "want",
    "give",
    "solution",
    "problem",
    "right",
    "draw",
    "blueprint",
    "tell",
    "exactly",
    "steps",
    "want",
    "come",
    "solution",
    "right",
    "formula",
    "also",
    "given",
    "everything",
    "given",
    "come",
    "final",
    "answer",
    "right",
    "let",
    "look",
    "solve",
    "problem",
    "first",
    "let",
    "consider",
    "right",
    "let",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "let",
    "x",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "right",
    "two",
    "events",
    "need",
    "calculate",
    "probability",
    "two",
    "probabilities",
    "need",
    "consider",
    "one",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "okay",
    "two",
    "represented",
    "x",
    "respectively",
    "lee",
    "want",
    "probability",
    "occurrence",
    "event",
    "given",
    "x",
    "means",
    "given",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "probability",
    "picking",
    "blue",
    "ball",
    "bag",
    "definition",
    "conditional",
    "probability",
    "exactly",
    "equation",
    "look",
    "like",
    "correct",
    "basically",
    "occurrence",
    "event",
    "given",
    "event",
    "x",
    "probability",
    "x",
    "probability",
    "x",
    "alone",
    "correct",
    "need",
    "need",
    "find",
    "two",
    "probabilities",
    "probability",
    "x",
    "occurring",
    "together",
    "probability",
    "okay",
    "entire",
    "solution",
    "find",
    "p",
    "probability",
    "x",
    "three",
    "ways",
    "first",
    "white",
    "ball",
    "either",
    "white",
    "read",
    "see",
    "first",
    "find",
    "probability",
    "x",
    "x",
    "basically",
    "represents",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "right",
    "three",
    "ways",
    "possible",
    "pick",
    "one",
    "blue",
    "ball",
    "bowel",
    "one",
    "bowel",
    "second",
    "case",
    "pick",
    "one",
    "another",
    "blue",
    "ball",
    "see",
    "third",
    "case",
    "pick",
    "blue",
    "ball",
    "bagby",
    "blue",
    "ball",
    "bagsy",
    "right",
    "three",
    "ways",
    "possible",
    "need",
    "find",
    "probability",
    "step",
    "two",
    "need",
    "find",
    "probability",
    "x",
    "occurring",
    "together",
    "sum",
    "terms",
    "1",
    "okay",
    "events",
    "picking",
    "ball",
    "bag",
    "correct",
    "find",
    "probability",
    "let",
    "know",
    "answer",
    "comment",
    "section",
    "right",
    "see",
    "get",
    "answer",
    "right",
    "gave",
    "entire",
    "solution",
    "substitute",
    "value",
    "right",
    "want",
    "second",
    "two",
    "going",
    "pause",
    "screen",
    "go",
    "clear",
    "away",
    "right",
    "remember",
    "need",
    "calculate",
    "two",
    "tease",
    "first",
    "probability",
    "need",
    "calculate",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "given",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "okay",
    "ii",
    "probability",
    "need",
    "calculate",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "bonds",
    "right",
    "two",
    "probabilities",
    "need",
    "calculate",
    "remember",
    "solution",
    "right",
    "guys",
    "make",
    "sure",
    "mention",
    "answers",
    "comment",
    "section",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "inferential",
    "statistics",
    "guys",
    "completed",
    "probability",
    "module",
    "right",
    "discuss",
    "inferential",
    "statistics",
    "second",
    "type",
    "statistics",
    "discussed",
    "descriptive",
    "statistics",
    "earlier",
    "alright",
    "like",
    "mentioned",
    "earlier",
    "inferential",
    "statistics",
    "also",
    "known",
    "statistical",
    "inference",
    "branch",
    "statistics",
    "deals",
    "forming",
    "inferences",
    "predictions",
    "population",
    "based",
    "sample",
    "data",
    "taken",
    "population",
    "right",
    "question",
    "ask",
    "one",
    "form",
    "inferences",
    "predictions",
    "sample",
    "answer",
    "use",
    "point",
    "estimation",
    "okay",
    "must",
    "wondering",
    "point",
    "estimation",
    "one",
    "estimation",
    "concerned",
    "use",
    "sample",
    "data",
    "measure",
    "single",
    "value",
    "serves",
    "approximate",
    "value",
    "best",
    "estimate",
    "unknown",
    "population",
    "parameter",
    "little",
    "confusing",
    "let",
    "break",
    "camping",
    "order",
    "calculate",
    "mean",
    "huge",
    "population",
    "first",
    "draw",
    "sample",
    "population",
    "find",
    "sample",
    "mean",
    "right",
    "sample",
    "mean",
    "used",
    "estimate",
    "population",
    "mean",
    "basically",
    "point",
    "estimate",
    "estimating",
    "value",
    "one",
    "parameters",
    "population",
    "right",
    "basically",
    "main",
    "trying",
    "estimate",
    "value",
    "mean",
    "point",
    "estimation",
    "two",
    "main",
    "terms",
    "point",
    "estimation",
    "something",
    "known",
    "estimator",
    "something",
    "known",
    "estimate",
    "estimator",
    "function",
    "sample",
    "used",
    "find",
    "estimate",
    "alright",
    "example",
    "basically",
    "sample",
    "mean",
    "right",
    "function",
    "calculates",
    "sample",
    "mean",
    "known",
    "estimator",
    "realized",
    "value",
    "estimator",
    "estimate",
    "right",
    "hope",
    "point",
    "estimation",
    "clear",
    "find",
    "estimates",
    "four",
    "common",
    "ways",
    "first",
    "one",
    "method",
    "moment",
    "form",
    "equation",
    "sample",
    "data",
    "set",
    "analyze",
    "similar",
    "equation",
    "population",
    "data",
    "set",
    "well",
    "like",
    "population",
    "mean",
    "population",
    "variance",
    "simple",
    "terms",
    "taking",
    "known",
    "facts",
    "population",
    "extending",
    "ideas",
    "sample",
    "alright",
    "analyze",
    "sample",
    "estimate",
    "essential",
    "complex",
    "values",
    "right",
    "next",
    "maximum",
    "likelihood",
    "method",
    "basically",
    "uses",
    "model",
    "estimate",
    "value",
    "right",
    "maximum",
    "likelihood",
    "majorly",
    "based",
    "probability",
    "lot",
    "probability",
    "involved",
    "method",
    "next",
    "base",
    "estimator",
    "works",
    "minimizing",
    "errors",
    "average",
    "risk",
    "okay",
    "base",
    "estimator",
    "lot",
    "bayes",
    "theorem",
    "right",
    "let",
    "get",
    "depth",
    "estimation",
    "methods",
    "finally",
    "best",
    "unbiased",
    "estimators",
    "method",
    "seven",
    "unbiased",
    "estimators",
    "used",
    "approximate",
    "parameter",
    "okay",
    "guys",
    "couple",
    "methods",
    "used",
    "find",
    "estimate",
    "method",
    "find",
    "estimate",
    "known",
    "interval",
    "estimation",
    "okay",
    "one",
    "important",
    "estimation",
    "methods",
    "confidence",
    "interval",
    "also",
    "comes",
    "picture",
    "right",
    "apart",
    "interval",
    "estimation",
    "also",
    "something",
    "known",
    "margin",
    "error",
    "discussing",
    "upcoming",
    "slides",
    "first",
    "let",
    "understand",
    "interval",
    "estimate",
    "okay",
    "interval",
    "range",
    "values",
    "used",
    "estimate",
    "population",
    "parameter",
    "known",
    "interval",
    "estimation",
    "right",
    "understandable",
    "basically",
    "trying",
    "see",
    "going",
    "estimate",
    "value",
    "parameter",
    "let",
    "say",
    "trying",
    "find",
    "mean",
    "population",
    "going",
    "going",
    "build",
    "range",
    "value",
    "lie",
    "range",
    "interval",
    "right",
    "way",
    "output",
    "going",
    "accurate",
    "predicted",
    "point",
    "estimation",
    "instead",
    "estimated",
    "interval",
    "within",
    "value",
    "might",
    "occur",
    "right",
    "okay",
    "image",
    "clearly",
    "shows",
    "point",
    "estimate",
    "interval",
    "estimate",
    "different",
    "interval",
    "estimate",
    "obviously",
    "accurate",
    "focusing",
    "particular",
    "value",
    "particular",
    "point",
    "order",
    "predict",
    "probability",
    "instead",
    "saying",
    "value",
    "might",
    "within",
    "range",
    "lower",
    "confidence",
    "limit",
    "upper",
    "confidence",
    "limit",
    "right",
    "denotes",
    "range",
    "interval",
    "okay",
    "still",
    "confused",
    "interval",
    "estimation",
    "let",
    "give",
    "small",
    "example",
    "stated",
    "take",
    "30",
    "minutes",
    "reach",
    "theater",
    "known",
    "point",
    "estimation",
    "okay",
    "stated",
    "take",
    "45",
    "minutes",
    "hour",
    "reach",
    "theater",
    "example",
    "estimation",
    "right",
    "hope",
    "clear",
    "interval",
    "estimation",
    "gives",
    "rise",
    "two",
    "important",
    "statistical",
    "terminologies",
    "one",
    "known",
    "confidence",
    "interval",
    "known",
    "margin",
    "error",
    "right",
    "important",
    "pay",
    "attention",
    "terminologies",
    "confidence",
    "interval",
    "one",
    "significant",
    "measures",
    "used",
    "check",
    "essential",
    "machine",
    "learning",
    "model",
    "right",
    "confidence",
    "interval",
    "confidence",
    "interval",
    "measure",
    "confidence",
    "interval",
    "estimated",
    "contains",
    "population",
    "parameter",
    "population",
    "mean",
    "parameters",
    "right",
    "statisticians",
    "use",
    "confidence",
    "interval",
    "describe",
    "amount",
    "uncertainty",
    "associated",
    "sample",
    "estimate",
    "population",
    "parameter",
    "guys",
    "lot",
    "definition",
    "let",
    "make",
    "understand",
    "confidence",
    "interval",
    "small",
    "example",
    "okay",
    "let",
    "say",
    "perform",
    "survey",
    "survey",
    "group",
    "cat",
    "owners",
    "see",
    "many",
    "cans",
    "cat",
    "food",
    "purchase",
    "one",
    "year",
    "okay",
    "test",
    "statistics",
    "99",
    "percent",
    "confidence",
    "level",
    "get",
    "confidence",
    "interval",
    "hundred",
    "comma",
    "200",
    "means",
    "think",
    "cat",
    "owners",
    "hundred",
    "two",
    "hundred",
    "cans",
    "year",
    "also",
    "since",
    "confidence",
    "level",
    "99",
    "shows",
    "confident",
    "results",
    "correct",
    "okay",
    "hope",
    "clear",
    "alright",
    "confidence",
    "interval",
    "hundred",
    "two",
    "hundred",
    "confidence",
    "level",
    "99",
    "right",
    "difference",
    "confidence",
    "interval",
    "confidence",
    "level",
    "within",
    "confidence",
    "interval",
    "value",
    "going",
    "lie",
    "confidence",
    "level",
    "show",
    "confident",
    "estimation",
    "right",
    "hope",
    "clear",
    "let",
    "look",
    "margin",
    "error",
    "margin",
    "error",
    "given",
    "level",
    "confidence",
    "greatest",
    "possible",
    "distance",
    "point",
    "estimate",
    "value",
    "parameter",
    "estimating",
    "say",
    "deviation",
    "actual",
    "point",
    "estimate",
    "right",
    "margin",
    "error",
    "calculated",
    "using",
    "formula",
    "zc",
    "denotes",
    "critical",
    "value",
    "confidence",
    "interval",
    "x",
    "standard",
    "deviation",
    "divided",
    "root",
    "sample",
    "size",
    "right",
    "n",
    "basically",
    "sample",
    "size",
    "let",
    "understand",
    "estimate",
    "confidence",
    "intervals",
    "guys",
    "level",
    "confidence",
    "denoted",
    "c",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "let",
    "say",
    "trying",
    "estimate",
    "mean",
    "right",
    "level",
    "confidence",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "interval",
    "minus",
    "z",
    "z",
    "area",
    "beneath",
    "curve",
    "nothing",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "right",
    "basically",
    "contain",
    "value",
    "predicting",
    "right",
    "known",
    "critical",
    "values",
    "basically",
    "lower",
    "limit",
    "higher",
    "limit",
    "confidence",
    "level",
    "also",
    "something",
    "known",
    "z",
    "score",
    "court",
    "calculated",
    "using",
    "standard",
    "normal",
    "table",
    "right",
    "look",
    "anywhere",
    "google",
    "find",
    "table",
    "standard",
    "normal",
    "table",
    "get",
    "understand",
    "done",
    "let",
    "look",
    "small",
    "example",
    "okay",
    "let",
    "say",
    "level",
    "vince",
    "90",
    "means",
    "90",
    "confident",
    "interval",
    "contains",
    "population",
    "mean",
    "okay",
    "remaining",
    "10",
    "hundred",
    "percent",
    "remaining",
    "10",
    "equally",
    "distributed",
    "dale",
    "regions",
    "okay",
    "right",
    "either",
    "side",
    "see",
    "distribute",
    "leftover",
    "percentage",
    "scores",
    "calculated",
    "table",
    "mentioned",
    "right",
    "one",
    "n64",
    "5",
    "get",
    "collated",
    "standard",
    "normal",
    "table",
    "okay",
    "guys",
    "estimate",
    "level",
    "confidence",
    "sum",
    "let",
    "tell",
    "steps",
    "involved",
    "constructing",
    "confidence",
    "interval",
    "first",
    "start",
    "identifying",
    "sample",
    "statistic",
    "okay",
    "statistic",
    "use",
    "estimate",
    "population",
    "parameter",
    "anything",
    "like",
    "mean",
    "sample",
    "next",
    "select",
    "confidence",
    "level",
    "confidence",
    "level",
    "describes",
    "uncertainty",
    "sampling",
    "method",
    "right",
    "find",
    "something",
    "known",
    "margin",
    "error",
    "right",
    "discuss",
    "margin",
    "error",
    "earlier",
    "find",
    "based",
    "equation",
    "explained",
    "previous",
    "slide",
    "finally",
    "specify",
    "confidence",
    "interval",
    "right",
    "let",
    "look",
    "problem",
    "statement",
    "better",
    "understand",
    "concept",
    "random",
    "sample",
    "32",
    "textbook",
    "prices",
    "taken",
    "local",
    "college",
    "bookstore",
    "mean",
    "sample",
    "sample",
    "standard",
    "deviation",
    "use",
    "95",
    "confident",
    "level",
    "find",
    "margin",
    "error",
    "mean",
    "price",
    "text",
    "books",
    "bookstore",
    "okay",
    "straightforward",
    "question",
    "want",
    "read",
    "question",
    "substitute",
    "values",
    "equation",
    "right",
    "guys",
    "know",
    "formula",
    "margin",
    "error",
    "take",
    "z",
    "score",
    "table",
    "deviation",
    "madrid",
    "right",
    "standard",
    "deviation",
    "n",
    "stands",
    "number",
    "samples",
    "number",
    "samples",
    "32",
    "basically",
    "32",
    "textbooks",
    "approximately",
    "margin",
    "error",
    "going",
    "around",
    "pretty",
    "simple",
    "question",
    "right",
    "hope",
    "understood",
    "know",
    "idea",
    "behind",
    "confidence",
    "interval",
    "let",
    "move",
    "ahead",
    "one",
    "important",
    "topics",
    "statistical",
    "inference",
    "hypothesis",
    "testing",
    "right",
    "sigelei",
    "statisticians",
    "use",
    "hypothesis",
    "testing",
    "formally",
    "check",
    "whether",
    "hypothesis",
    "accepted",
    "rejected",
    "okay",
    "hypothesis",
    "testing",
    "inferential",
    "statistical",
    "technique",
    "used",
    "determine",
    "whether",
    "enough",
    "evidence",
    "data",
    "sample",
    "infer",
    "certain",
    "condition",
    "holds",
    "true",
    "entire",
    "population",
    "understand",
    "characteristics",
    "general",
    "population",
    "take",
    "random",
    "sample",
    "analyze",
    "properties",
    "sample",
    "right",
    "test",
    "whether",
    "identified",
    "conclusion",
    "represent",
    "population",
    "accurately",
    "finally",
    "interpret",
    "results",
    "whether",
    "accept",
    "hypothesis",
    "depends",
    "upon",
    "percentage",
    "value",
    "get",
    "hypothesis",
    "okay",
    "better",
    "understand",
    "let",
    "look",
    "small",
    "example",
    "steps",
    "followed",
    "hypothesis",
    "testing",
    "begin",
    "stating",
    "null",
    "alternative",
    "hypothesis",
    "right",
    "tell",
    "exactly",
    "terms",
    "formulate",
    "analysis",
    "plan",
    "right",
    "analyze",
    "sample",
    "data",
    "finally",
    "interpret",
    "results",
    "right",
    "understand",
    "entire",
    "hypothesis",
    "testing",
    "look",
    "good",
    "example",
    "okay",
    "consider",
    "boys",
    "nick",
    "harry",
    "boys",
    "caught",
    "bunking",
    "class",
    "asked",
    "stay",
    "back",
    "school",
    "clean",
    "classroom",
    "punishment",
    "right",
    "john",
    "decided",
    "four",
    "would",
    "take",
    "turns",
    "clean",
    "classrooms",
    "came",
    "plan",
    "writing",
    "names",
    "chits",
    "putting",
    "bout",
    "every",
    "day",
    "pick",
    "name",
    "bowel",
    "person",
    "play",
    "clock",
    "right",
    "sounds",
    "pretty",
    "fair",
    "enough",
    "three",
    "days",
    "everybody",
    "name",
    "come",
    "except",
    "john",
    "assuming",
    "event",
    "completely",
    "random",
    "free",
    "bias",
    "probability",
    "john",
    "treating",
    "right",
    "probability",
    "actually",
    "cheating",
    "solved",
    "using",
    "hypothesis",
    "testing",
    "okay",
    "begin",
    "calculating",
    "probability",
    "john",
    "picked",
    "day",
    "alright",
    "going",
    "assume",
    "event",
    "free",
    "bias",
    "need",
    "find",
    "probability",
    "john",
    "cheating",
    "right",
    "first",
    "find",
    "probability",
    "john",
    "picked",
    "day",
    "right",
    "get",
    "3",
    "4",
    "basically",
    "75",
    "75",
    "fairly",
    "high",
    "john",
    "picked",
    "three",
    "days",
    "row",
    "probability",
    "drop",
    "approximately",
    "42",
    "okay",
    "three",
    "days",
    "row",
    "meaning",
    "probability",
    "drops",
    "42",
    "percent",
    "let",
    "consider",
    "situation",
    "john",
    "picked",
    "12",
    "days",
    "row",
    "probability",
    "drops",
    "tea",
    "point",
    "two",
    "percent",
    "okay",
    "probability",
    "john",
    "cheating",
    "becomes",
    "fairly",
    "high",
    "right",
    "order",
    "statisticians",
    "come",
    "conclusion",
    "define",
    "known",
    "threshold",
    "value",
    "right",
    "considering",
    "situation",
    "threshold",
    "value",
    "set",
    "5",
    "percent",
    "would",
    "indicate",
    "probability",
    "lies",
    "5",
    "john",
    "cheating",
    "way",
    "detention",
    "probability",
    "threshold",
    "value",
    "john",
    "lucky",
    "name",
    "getting",
    "picked",
    "probability",
    "hypothesis",
    "testing",
    "give",
    "rise",
    "two",
    "important",
    "components",
    "hypothesis",
    "testing",
    "null",
    "hypothesis",
    "alternative",
    "hypothesis",
    "null",
    "hypothesis",
    "based",
    "basically",
    "approving",
    "assumption",
    "alternate",
    "hypothesis",
    "result",
    "disapproves",
    "assumption",
    "right",
    "therefore",
    "example",
    "probability",
    "event",
    "occurring",
    "less",
    "5",
    "event",
    "biased",
    "hence",
    "proves",
    "alternate",
    "hypothesis",
    "undoubtedly",
    "machine",
    "learning",
    "technology",
    "today",
    "market",
    "applications",
    "seth",
    "driving",
    "cause",
    "predicting",
    "deadly",
    "diseases",
    "als",
    "high",
    "demand",
    "machine",
    "learning",
    "skills",
    "motivation",
    "behind",
    "today",
    "session",
    "let",
    "discuss",
    "agenda",
    "first",
    "going",
    "begin",
    "session",
    "understanding",
    "need",
    "machine",
    "learning",
    "important",
    "look",
    "exactly",
    "machine",
    "learning",
    "discuss",
    "couple",
    "machine",
    "learning",
    "definitions",
    "done",
    "look",
    "machine",
    "learning",
    "process",
    "solve",
    "problem",
    "using",
    "using",
    "machine",
    "learning",
    "process",
    "next",
    "discuss",
    "types",
    "machine",
    "learning",
    "includes",
    "supervised",
    "unsupervised",
    "reinforcement",
    "learning",
    "done",
    "discuss",
    "different",
    "types",
    "problems",
    "solved",
    "using",
    "machine",
    "learning",
    "finally",
    "end",
    "session",
    "looking",
    "demo",
    "see",
    "perform",
    "weather",
    "forecasting",
    "using",
    "machine",
    "learning",
    "right",
    "guys",
    "let",
    "get",
    "started",
    "first",
    "topic",
    "importance",
    "need",
    "machine",
    "learning",
    "since",
    "technical",
    "revolution",
    "generating",
    "immeasurable",
    "amount",
    "data",
    "research",
    "generating",
    "around",
    "quintillion",
    "bytes",
    "data",
    "every",
    "single",
    "day",
    "estimated",
    "2020",
    "mb",
    "data",
    "created",
    "every",
    "second",
    "every",
    "person",
    "earth",
    "lot",
    "data",
    "right",
    "data",
    "comes",
    "sources",
    "cloud",
    "iot",
    "devices",
    "social",
    "media",
    "since",
    "us",
    "interested",
    "internet",
    "right",
    "generating",
    "lot",
    "data",
    "right",
    "idea",
    "much",
    "data",
    "generate",
    "social",
    "media",
    "chatting",
    "images",
    "post",
    "instagram",
    "videos",
    "watch",
    "generates",
    "lot",
    "data",
    "machine",
    "learning",
    "fit",
    "since",
    "producing",
    "much",
    "data",
    "need",
    "find",
    "method",
    "analyze",
    "process",
    "interpret",
    "much",
    "data",
    "right",
    "need",
    "find",
    "method",
    "make",
    "sense",
    "data",
    "method",
    "machine",
    "learning",
    "lot",
    "talk",
    "tire",
    "companies",
    "data",
    "driven",
    "company",
    "netflix",
    "amazon",
    "build",
    "machine",
    "learning",
    "models",
    "using",
    "tons",
    "data",
    "order",
    "identify",
    "profitable",
    "opportunities",
    "want",
    "avoid",
    "unwanted",
    "risk",
    "make",
    "use",
    "machine",
    "learning",
    "alright",
    "machine",
    "learning",
    "predict",
    "risk",
    "predict",
    "profits",
    "identify",
    "opportunities",
    "help",
    "grow",
    "business",
    "business",
    "show",
    "couple",
    "examples",
    "machine",
    "learning",
    "used",
    "right",
    "sure",
    "watch",
    "netflix",
    "important",
    "thing",
    "netflix",
    "recommendation",
    "engine",
    "right",
    "netflix",
    "revenue",
    "comes",
    "recommendation",
    "engine",
    "recommendation",
    "engine",
    "basically",
    "studies",
    "movie",
    "viewing",
    "patterns",
    "users",
    "recommends",
    "relevant",
    "movies",
    "right",
    "recommends",
    "movies",
    "depending",
    "users",
    "interests",
    "depending",
    "type",
    "movies",
    "user",
    "watches",
    "alright",
    "netflix",
    "uses",
    "machine",
    "learning",
    "next",
    "facebook",
    "auto",
    "tagging",
    "feature",
    "logic",
    "behind",
    "facebook",
    "auto",
    "tagging",
    "feature",
    "machine",
    "learning",
    "neural",
    "networks",
    "sure",
    "many",
    "know",
    "facebook",
    "makes",
    "use",
    "deepmind",
    "face",
    "verification",
    "system",
    "based",
    "machine",
    "learning",
    "natural",
    "language",
    "processing",
    "neural",
    "networks",
    "deep",
    "mine",
    "basically",
    "studies",
    "facial",
    "features",
    "image",
    "tag",
    "friends",
    "family",
    "another",
    "example",
    "amazon",
    "alexa",
    "alexa",
    "basically",
    "advanced",
    "level",
    "virtual",
    "assistant",
    "based",
    "natural",
    "language",
    "processing",
    "machine",
    "learning",
    "play",
    "music",
    "right",
    "book",
    "uber",
    "connect",
    "devices",
    "house",
    "track",
    "health",
    "order",
    "food",
    "online",
    "data",
    "machine",
    "learning",
    "basically",
    "main",
    "factors",
    "behind",
    "alex",
    "power",
    "another",
    "example",
    "google",
    "spam",
    "filter",
    "guys",
    "gmail",
    "basically",
    "makes",
    "use",
    "machine",
    "learning",
    "filter",
    "spam",
    "messages",
    "open",
    "gmail",
    "inbox",
    "see",
    "separate",
    "sections",
    "one",
    "primary",
    "social",
    "spam",
    "joe",
    "general",
    "made",
    "basically",
    "gmail",
    "makes",
    "use",
    "machine",
    "learning",
    "algorithms",
    "natural",
    "language",
    "processing",
    "emails",
    "real",
    "time",
    "classify",
    "either",
    "spam",
    "another",
    "famous",
    "application",
    "machine",
    "learning",
    "sum",
    "let",
    "look",
    "reasons",
    "machine",
    "learning",
    "important",
    "first",
    "reason",
    "obviously",
    "increase",
    "data",
    "generation",
    "excessive",
    "production",
    "data",
    "need",
    "method",
    "used",
    "structure",
    "lies",
    "draw",
    "useful",
    "insights",
    "data",
    "machine",
    "learning",
    "comes",
    "uses",
    "data",
    "solve",
    "problems",
    "find",
    "solutions",
    "complex",
    "tasks",
    "faced",
    "organizations",
    "another",
    "important",
    "reason",
    "improves",
    "making",
    "use",
    "various",
    "algorithms",
    "machine",
    "learning",
    "used",
    "make",
    "better",
    "business",
    "decisions",
    "example",
    "machine",
    "learning",
    "used",
    "forecast",
    "sales",
    "used",
    "predict",
    "downfalls",
    "stock",
    "market",
    "used",
    "identify",
    "risks",
    "anomalies",
    "next",
    "reason",
    "uncovers",
    "patterns",
    "trends",
    "data",
    "finding",
    "hidden",
    "patterns",
    "extracting",
    "key",
    "insights",
    "data",
    "essential",
    "part",
    "machine",
    "learning",
    "building",
    "predictive",
    "models",
    "using",
    "statistical",
    "techniques",
    "machine",
    "learning",
    "allows",
    "dig",
    "beneath",
    "surface",
    "explore",
    "data",
    "minut",
    "scale",
    "understanding",
    "data",
    "extracting",
    "patterns",
    "manually",
    "take",
    "lot",
    "days",
    "machine",
    "learning",
    "algorithms",
    "perform",
    "computations",
    "nations",
    "less",
    "second",
    "another",
    "reason",
    "solved",
    "complex",
    "problems",
    "detecting",
    "genes",
    "linked",
    "deadly",
    "als",
    "disease",
    "building",
    "cars",
    "building",
    "phase",
    "detection",
    "systems",
    "machine",
    "learning",
    "used",
    "solve",
    "complex",
    "problems",
    "guys",
    "know",
    "machine",
    "learning",
    "important",
    "let",
    "look",
    "exactly",
    "machine",
    "learning",
    "term",
    "machine",
    "learning",
    "first",
    "coined",
    "arthur",
    "samuel",
    "year",
    "1959",
    "looking",
    "back",
    "probably",
    "significant",
    "terms",
    "technological",
    "advancements",
    "browse",
    "net",
    "machine",
    "learning",
    "get",
    "least",
    "hundred",
    "different",
    "definitions",
    "first",
    "formal",
    "definition",
    "given",
    "tom",
    "mitchell",
    "definition",
    "says",
    "computer",
    "program",
    "set",
    "learn",
    "experience",
    "e",
    "respect",
    "class",
    "caste",
    "performance",
    "measure",
    "p",
    "performance",
    "tasks",
    "measured",
    "p",
    "improves",
    "experience",
    "e",
    "right",
    "know",
    "little",
    "confusing",
    "let",
    "break",
    "simple",
    "words",
    "simple",
    "terms",
    "machine",
    "learning",
    "subset",
    "artificial",
    "intelligence",
    "provides",
    "machines",
    "ability",
    "learn",
    "automatically",
    "improve",
    "experience",
    "without",
    "explicitly",
    "programmed",
    "sense",
    "practice",
    "getting",
    "machines",
    "solve",
    "problems",
    "gaining",
    "ability",
    "think",
    "wait",
    "machine",
    "think",
    "make",
    "decisions",
    "well",
    "feel",
    "machine",
    "good",
    "amount",
    "data",
    "learn",
    "interpret",
    "process",
    "analyze",
    "data",
    "using",
    "machine",
    "learning",
    "algorithm",
    "okay",
    "guys",
    "look",
    "figure",
    "top",
    "figure",
    "basically",
    "shows",
    "machine",
    "learning",
    "algorithm",
    "machine",
    "learning",
    "process",
    "really",
    "works",
    "machine",
    "learning",
    "begins",
    "feeding",
    "machine",
    "lots",
    "lots",
    "data",
    "okay",
    "using",
    "data",
    "machine",
    "trained",
    "detect",
    "hidden",
    "insights",
    "trends",
    "insights",
    "used",
    "build",
    "machine",
    "learning",
    "model",
    "using",
    "algorithm",
    "order",
    "solve",
    "problem",
    "okay",
    "basically",
    "going",
    "feed",
    "lot",
    "data",
    "machine",
    "machine",
    "going",
    "get",
    "trained",
    "using",
    "data",
    "going",
    "use",
    "data",
    "going",
    "draw",
    "useful",
    "insights",
    "patterns",
    "going",
    "build",
    "model",
    "using",
    "machine",
    "learning",
    "algorithms",
    "model",
    "help",
    "predict",
    "outcome",
    "help",
    "solve",
    "complex",
    "problem",
    "business",
    "problem",
    "simple",
    "explanation",
    "machine",
    "learning",
    "works",
    "let",
    "move",
    "look",
    "commonly",
    "used",
    "machine",
    "learning",
    "terms",
    "first",
    "algorithm",
    "quite",
    "basically",
    "algorithm",
    "set",
    "rules",
    "statistical",
    "techniques",
    "used",
    "learn",
    "patterns",
    "data",
    "algorithm",
    "logic",
    "behind",
    "machine",
    "learning",
    "model",
    "right",
    "example",
    "machine",
    "learning",
    "algorithm",
    "linear",
    "regression",
    "sure",
    "many",
    "heard",
    "linear",
    "regression",
    "simple",
    "basic",
    "machine",
    "learning",
    "algorithm",
    "right",
    "next",
    "model",
    "model",
    "main",
    "component",
    "machine",
    "learning",
    "right",
    "model",
    "basically",
    "map",
    "input",
    "output",
    "using",
    "machine",
    "learning",
    "algorithm",
    "using",
    "data",
    "feeding",
    "machine",
    "basically",
    "model",
    "representation",
    "entire",
    "machine",
    "learning",
    "process",
    "model",
    "basically",
    "fed",
    "input",
    "lot",
    "data",
    "output",
    "particular",
    "result",
    "particular",
    "outcome",
    "using",
    "machine",
    "learning",
    "algorithms",
    "next",
    "something",
    "known",
    "predictor",
    "variable",
    "predictor",
    "variable",
    "feature",
    "data",
    "used",
    "predict",
    "output",
    "example",
    "let",
    "say",
    "trying",
    "predict",
    "weight",
    "person",
    "depending",
    "person",
    "height",
    "age",
    "right",
    "predictor",
    "variables",
    "height",
    "age",
    "using",
    "height",
    "age",
    "person",
    "predict",
    "person",
    "weight",
    "alright",
    "height",
    "predictor",
    "variables",
    "wait",
    "hand",
    "response",
    "target",
    "variable",
    "response",
    "variable",
    "feature",
    "output",
    "variable",
    "needs",
    "predicted",
    "using",
    "predictor",
    "variables",
    "right",
    "something",
    "known",
    "training",
    "data",
    "guys",
    "data",
    "fed",
    "machine",
    "learning",
    "model",
    "always",
    "split",
    "two",
    "parts",
    "first",
    "training",
    "data",
    "testing",
    "data",
    "training",
    "data",
    "basically",
    "used",
    "build",
    "machine",
    "learning",
    "model",
    "usually",
    "training",
    "data",
    "much",
    "larger",
    "testing",
    "data",
    "obviously",
    "trying",
    "train",
    "machine",
    "going",
    "feed",
    "lot",
    "data",
    "testing",
    "data",
    "used",
    "validate",
    "evaluate",
    "efficiency",
    "model",
    "alright",
    "training",
    "data",
    "testing",
    "data",
    "guys",
    "terms",
    "thought",
    "know",
    "move",
    "okay",
    "let",
    "move",
    "discuss",
    "machine",
    "learning",
    "process",
    "going",
    "get",
    "interesting",
    "going",
    "give",
    "example",
    "make",
    "understand",
    "machine",
    "learning",
    "process",
    "works",
    "first",
    "let",
    "define",
    "different",
    "stages",
    "different",
    "steps",
    "involved",
    "machine",
    "learning",
    "process",
    "machine",
    "learning",
    "process",
    "always",
    "begins",
    "defining",
    "objective",
    "defining",
    "problem",
    "trying",
    "solve",
    "next",
    "data",
    "gathering",
    "data",
    "collection",
    "data",
    "need",
    "solve",
    "problem",
    "collected",
    "stage",
    "followed",
    "data",
    "preparation",
    "data",
    "processing",
    "data",
    "exploration",
    "analysis",
    "isis",
    "next",
    "stage",
    "building",
    "machine",
    "learning",
    "model",
    "followed",
    "model",
    "evaluation",
    "finally",
    "prediction",
    "output",
    "let",
    "try",
    "understand",
    "entire",
    "process",
    "example",
    "problem",
    "statement",
    "predict",
    "possibility",
    "rain",
    "studying",
    "weather",
    "conditions",
    "let",
    "say",
    "given",
    "problem",
    "statement",
    "asked",
    "use",
    "machine",
    "learning",
    "process",
    "solve",
    "problem",
    "statement",
    "let",
    "get",
    "started",
    "alright",
    "first",
    "step",
    "find",
    "objective",
    "problem",
    "statement",
    "objective",
    "predict",
    "possibility",
    "rain",
    "studying",
    "weather",
    "conditions",
    "first",
    "stage",
    "machine",
    "learning",
    "process",
    "must",
    "understand",
    "exactly",
    "needs",
    "predicted",
    "case",
    "objective",
    "predict",
    "possibility",
    "rain",
    "studying",
    "weather",
    "conditions",
    "right",
    "stage",
    "also",
    "essential",
    "take",
    "mental",
    "notes",
    "kind",
    "data",
    "used",
    "solve",
    "problem",
    "type",
    "approach",
    "follow",
    "get",
    "get",
    "solution",
    "right",
    "questions",
    "worth",
    "asking",
    "stage",
    "trying",
    "predict",
    "target",
    "features",
    "predictor",
    "variables",
    "kind",
    "input",
    "data",
    "need",
    "kind",
    "problem",
    "facing",
    "binary",
    "classification",
    "problem",
    "clustering",
    "problem",
    "worry",
    "know",
    "classification",
    "clustering",
    "explaining",
    "upcoming",
    "slides",
    "guys",
    "first",
    "step",
    "machine",
    "learning",
    "process",
    "define",
    "double",
    "problem",
    "right",
    "let",
    "move",
    "look",
    "step",
    "number",
    "two",
    "step",
    "number",
    "two",
    "basically",
    "data",
    "collection",
    "data",
    "gathering",
    "stage",
    "must",
    "asking",
    "questions",
    "kind",
    "data",
    "needed",
    "solve",
    "problem",
    "data",
    "available",
    "available",
    "get",
    "data",
    "okay",
    "know",
    "type",
    "data",
    "required",
    "must",
    "understand",
    "derive",
    "data",
    "data",
    "collection",
    "done",
    "manually",
    "web",
    "scraping",
    "beginner",
    "looking",
    "learn",
    "machine",
    "learning",
    "worry",
    "getting",
    "data",
    "ok",
    "thousands",
    "data",
    "resources",
    "web",
    "go",
    "ahead",
    "download",
    "datasets",
    "websites",
    "kaggle",
    "okay",
    "coming",
    "back",
    "problem",
    "hand",
    "data",
    "needed",
    "weather",
    "forecasting",
    "includes",
    "measures",
    "humidity",
    "level",
    "temperature",
    "pressure",
    "locality",
    "whether",
    "live",
    "hill",
    "station",
    "guys",
    "data",
    "must",
    "collected",
    "stored",
    "analysis",
    "next",
    "stage",
    "machine",
    "learning",
    "preparing",
    "data",
    "data",
    "collected",
    "almost",
    "never",
    "right",
    "format",
    "basically",
    "encounter",
    "lot",
    "inconsistencies",
    "data",
    "set",
    "okay",
    "includes",
    "missing",
    "values",
    "redundant",
    "variables",
    "duplicate",
    "values",
    "removing",
    "values",
    "important",
    "might",
    "lead",
    "wrongful",
    "computations",
    "predictions",
    "stage",
    "must",
    "entire",
    "data",
    "set",
    "inconsistencies",
    "fix",
    "stage",
    "next",
    "step",
    "exploratory",
    "data",
    "analysis",
    "data",
    "analysis",
    "diving",
    "deep",
    "data",
    "finding",
    "hidden",
    "data",
    "mysteries",
    "okay",
    "become",
    "detective",
    "edu",
    "exploratory",
    "data",
    "analysis",
    "like",
    "brainstorming",
    "machine",
    "learning",
    "data",
    "exploration",
    "involves",
    "understanding",
    "patterns",
    "trends",
    "data",
    "stage",
    "useful",
    "insights",
    "drawn",
    "correlations",
    "turns",
    "variables",
    "understood",
    "might",
    "ask",
    "sort",
    "correlations",
    "talking",
    "example",
    "case",
    "predicting",
    "rain",
    "fall",
    "know",
    "strong",
    "possibility",
    "rain",
    "temperature",
    "fallen",
    "low",
    "okay",
    "correlations",
    "understood",
    "mapped",
    "stage",
    "stage",
    "followed",
    "stage",
    "number",
    "5",
    "building",
    "machine",
    "learning",
    "model",
    "insights",
    "patterns",
    "derive",
    "data",
    "exploration",
    "used",
    "build",
    "machine",
    "learning",
    "stage",
    "always",
    "begins",
    "splitting",
    "data",
    "set",
    "two",
    "parts",
    "training",
    "data",
    "testing",
    "data",
    "earlier",
    "session",
    "already",
    "told",
    "training",
    "testing",
    "data",
    "training",
    "data",
    "used",
    "build",
    "analyze",
    "model",
    "logic",
    "model",
    "based",
    "machine",
    "learning",
    "algorithm",
    "implemented",
    "okay",
    "case",
    "predicting",
    "rainfall",
    "since",
    "output",
    "form",
    "true",
    "false",
    "use",
    "classification",
    "algorithm",
    "like",
    "logistically",
    "regression",
    "choosing",
    "right",
    "algorithm",
    "depends",
    "type",
    "problem",
    "trying",
    "solve",
    "data",
    "set",
    "level",
    "complexity",
    "problem",
    "upcoming",
    "sections",
    "discussing",
    "different",
    "types",
    "problems",
    "solved",
    "using",
    "machine",
    "learning",
    "worry",
    "know",
    "classification",
    "algorithm",
    "logistic",
    "regression",
    "okay",
    "need",
    "know",
    "stage",
    "building",
    "machine",
    "learning",
    "model",
    "using",
    "machine",
    "learning",
    "algorithm",
    "using",
    "training",
    "data",
    "set",
    "next",
    "machine",
    "learning",
    "process",
    "model",
    "evaluation",
    "optimization",
    "building",
    "model",
    "using",
    "training",
    "data",
    "set",
    "finally",
    "time",
    "put",
    "model",
    "test",
    "okay",
    "testing",
    "data",
    "set",
    "used",
    "check",
    "efficiency",
    "model",
    "accurately",
    "predict",
    "outcome",
    "calculate",
    "accuracy",
    "improvements",
    "model",
    "implemented",
    "stage",
    "okay",
    "methods",
    "like",
    "parameter",
    "tuning",
    "used",
    "improve",
    "performance",
    "model",
    "followed",
    "last",
    "stage",
    "predictions",
    "model",
    "evaluated",
    "improved",
    "finally",
    "used",
    "make",
    "predictions",
    "final",
    "output",
    "categorical",
    "variable",
    "continuous",
    "quantity",
    "case",
    "predicting",
    "occurrence",
    "rainfall",
    "output",
    "categorical",
    "variable",
    "sense",
    "output",
    "form",
    "true",
    "false",
    "yes",
    "yes",
    "basically",
    "represents",
    "going",
    "rain",
    "represent",
    "wondering",
    "okay",
    "simple",
    "guys",
    "entire",
    "machine",
    "learning",
    "process",
    "linear",
    "regression",
    "one",
    "easiest",
    "algorithm",
    "machine",
    "learning",
    "statistical",
    "model",
    "attempts",
    "show",
    "relationship",
    "two",
    "variables",
    "linear",
    "equation",
    "drill",
    "linear",
    "regression",
    "algorithm",
    "depth",
    "give",
    "quick",
    "overview",
    "today",
    "agenda",
    "start",
    "session",
    "quick",
    "overview",
    "regression",
    "linear",
    "regression",
    "one",
    "type",
    "regression",
    "algorithm",
    "learn",
    "regression",
    "use",
    "case",
    "various",
    "types",
    "next",
    "learn",
    "algorithm",
    "scratch",
    "live",
    "mathematical",
    "implementation",
    "first",
    "drill",
    "coding",
    "part",
    "implement",
    "linear",
    "regression",
    "using",
    "python",
    "today",
    "session",
    "deal",
    "linear",
    "regression",
    "algorithm",
    "using",
    "least",
    "square",
    "method",
    "checketts",
    "goodness",
    "fit",
    "close",
    "data",
    "fitted",
    "regression",
    "line",
    "using",
    "r",
    "square",
    "method",
    "finally",
    "well",
    "optimized",
    "using",
    "gradient",
    "descent",
    "method",
    "last",
    "part",
    "coding",
    "session",
    "teach",
    "implement",
    "linear",
    "regression",
    "using",
    "python",
    "coding",
    "session",
    "would",
    "divided",
    "two",
    "parts",
    "first",
    "part",
    "would",
    "consist",
    "linear",
    "regression",
    "using",
    "python",
    "scratch",
    "use",
    "mathematical",
    "algorithm",
    "learned",
    "session",
    "next",
    "part",
    "coding",
    "session",
    "using",
    "direct",
    "implementation",
    "linear",
    "regression",
    "right",
    "hope",
    "agenda",
    "clear",
    "guys",
    "like",
    "let",
    "begin",
    "session",
    "regression",
    "well",
    "regression",
    "analysis",
    "form",
    "predictive",
    "modeling",
    "technique",
    "investigates",
    "relationship",
    "dependent",
    "independent",
    "able",
    "regression",
    "analysis",
    "involves",
    "graphing",
    "line",
    "set",
    "data",
    "points",
    "closely",
    "fits",
    "overall",
    "shape",
    "data",
    "regression",
    "shows",
    "changes",
    "dependent",
    "variable",
    "changes",
    "explanatory",
    "variable",
    "fine",
    "would",
    "ask",
    "uses",
    "regression",
    "well",
    "major",
    "three",
    "uses",
    "regression",
    "analysis",
    "first",
    "determining",
    "strength",
    "predicator",
    "regression",
    "might",
    "used",
    "identify",
    "strength",
    "effect",
    "independent",
    "variables",
    "dependent",
    "variable",
    "example",
    "ask",
    "question",
    "like",
    "strength",
    "relationship",
    "sales",
    "marketing",
    "spending",
    "relationship",
    "age",
    "income",
    "second",
    "forecasting",
    "effect",
    "regression",
    "used",
    "forecast",
    "effects",
    "impact",
    "changes",
    "regression",
    "analysis",
    "help",
    "us",
    "understand",
    "much",
    "dependent",
    "variable",
    "changes",
    "change",
    "one",
    "independent",
    "variable",
    "fine",
    "example",
    "ask",
    "question",
    "like",
    "additional",
    "seal",
    "income",
    "get",
    "thousand",
    "dollars",
    "spent",
    "marketing",
    "third",
    "trend",
    "forecasting",
    "regression",
    "analysis",
    "predict",
    "trends",
    "future",
    "values",
    "regression",
    "analysis",
    "used",
    "get",
    "point",
    "estimates",
    "ask",
    "questions",
    "like",
    "price",
    "bitcoin",
    "next",
    "six",
    "months",
    "right",
    "next",
    "topic",
    "linear",
    "versus",
    "logistic",
    "regression",
    "hope",
    "know",
    "regression",
    "let",
    "move",
    "understand",
    "type",
    "various",
    "kinds",
    "regression",
    "like",
    "linear",
    "session",
    "logistic",
    "regression",
    "polynomial",
    "regression",
    "others",
    "right",
    "session",
    "focusing",
    "linear",
    "logistic",
    "regression",
    "let",
    "move",
    "let",
    "tell",
    "linear",
    "regression",
    "logistic",
    "regression",
    "compare",
    "right",
    "starting",
    "linear",
    "regression",
    "simple",
    "linear",
    "regression",
    "interested",
    "things",
    "like",
    "equal",
    "mx",
    "plus",
    "trying",
    "find",
    "correlation",
    "x",
    "variable",
    "means",
    "every",
    "value",
    "x",
    "corresponding",
    "value",
    "continuous",
    "like",
    "however",
    "logistic",
    "regression",
    "fitting",
    "data",
    "straight",
    "line",
    "like",
    "linear",
    "regression",
    "instead",
    "mapping",
    "versus",
    "x",
    "sigmoid",
    "function",
    "logistic",
    "regression",
    "find",
    "1",
    "0",
    "particular",
    "value",
    "x",
    "thus",
    "essentially",
    "deciding",
    "true",
    "false",
    "value",
    "given",
    "value",
    "x",
    "fine",
    "core",
    "concept",
    "linear",
    "regression",
    "say",
    "data",
    "modeled",
    "using",
    "straight",
    "line",
    "case",
    "logistic",
    "regression",
    "data",
    "model",
    "using",
    "sigmoid",
    "function",
    "linear",
    "regression",
    "used",
    "continuous",
    "variables",
    "hand",
    "logistic",
    "regression",
    "used",
    "categorical",
    "variable",
    "output",
    "prediction",
    "linear",
    "regression",
    "value",
    "variable",
    "hand",
    "output",
    "production",
    "logistic",
    "regression",
    "probability",
    "occurrence",
    "event",
    "check",
    "accuracy",
    "goodness",
    "fit",
    "case",
    "linear",
    "regression",
    "various",
    "methods",
    "take",
    "measured",
    "loss",
    "r",
    "squared",
    "adjusted",
    "r",
    "squared",
    "etc",
    "case",
    "logistic",
    "regression",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "nothing",
    "harmonic",
    "mean",
    "precision",
    "recall",
    "next",
    "roc",
    "curve",
    "determining",
    "probability",
    "threshold",
    "classification",
    "confusion",
    "matrix",
    "etc",
    "many",
    "right",
    "summarizing",
    "difference",
    "linear",
    "logistic",
    "regression",
    "say",
    "type",
    "function",
    "mapping",
    "main",
    "point",
    "difference",
    "linear",
    "regression",
    "linear",
    "regression",
    "maps",
    "continuous",
    "x2",
    "continuous",
    "fi",
    "hand",
    "logistic",
    "regression",
    "maps",
    "continuous",
    "x",
    "bindery",
    "use",
    "logistic",
    "regression",
    "make",
    "category",
    "true",
    "false",
    "decisions",
    "data",
    "find",
    "let",
    "move",
    "ahead",
    "next",
    "linear",
    "regression",
    "selection",
    "criteria",
    "say",
    "use",
    "linear",
    "regression",
    "first",
    "classification",
    "regression",
    "capabilities",
    "regression",
    "models",
    "predict",
    "continuous",
    "variable",
    "day",
    "predict",
    "temperature",
    "city",
    "reliance",
    "polynomial",
    "like",
    "straight",
    "line",
    "fit",
    "data",
    "set",
    "poses",
    "real",
    "challenge",
    "comes",
    "towards",
    "building",
    "classification",
    "capability",
    "let",
    "imagine",
    "fit",
    "line",
    "training",
    "points",
    "imagine",
    "add",
    "data",
    "points",
    "order",
    "fit",
    "change",
    "existing",
    "model",
    "maybe",
    "change",
    "threshold",
    "happen",
    "new",
    "data",
    "point",
    "add",
    "model",
    "hence",
    "linear",
    "regression",
    "good",
    "classification",
    "fine",
    "next",
    "data",
    "quality",
    "missing",
    "value",
    "removes",
    "one",
    "data",
    "point",
    "could",
    "optimize",
    "regression",
    "simple",
    "linear",
    "regression",
    "outliers",
    "significantly",
    "disrupt",
    "outcome",
    "know",
    "remove",
    "outliers",
    "model",
    "become",
    "good",
    "right",
    "data",
    "quality",
    "next",
    "computational",
    "complexity",
    "linear",
    "regression",
    "often",
    "computationally",
    "expensive",
    "compared",
    "decision",
    "tree",
    "clustering",
    "algorithm",
    "order",
    "complexity",
    "n",
    "training",
    "example",
    "x",
    "features",
    "usually",
    "falls",
    "either",
    "big",
    "x",
    "square",
    "big",
    "xn",
    "next",
    "comprehensible",
    "transparent",
    "linear",
    "regression",
    "easily",
    "comprehensible",
    "transparent",
    "nature",
    "represented",
    "simple",
    "mathematical",
    "notation",
    "anyone",
    "understood",
    "easily",
    "criteria",
    "based",
    "select",
    "linear",
    "regression",
    "algorithm",
    "right",
    "next",
    "linear",
    "regression",
    "used",
    "first",
    "evaluating",
    "trends",
    "sales",
    "estimate",
    "well",
    "linear",
    "regression",
    "used",
    "business",
    "evaluate",
    "trends",
    "make",
    "estimates",
    "focused",
    "example",
    "company",
    "sales",
    "increased",
    "steadily",
    "every",
    "month",
    "past",
    "years",
    "conducting",
    "linear",
    "analysis",
    "sales",
    "data",
    "monthly",
    "sales",
    "axis",
    "time",
    "x",
    "axis",
    "give",
    "line",
    "predicts",
    "upward",
    "trends",
    "sale",
    "creating",
    "trendline",
    "company",
    "could",
    "use",
    "slope",
    "lines",
    "focused",
    "sale",
    "future",
    "months",
    "next",
    "analyzing",
    "impact",
    "price",
    "changes",
    "linear",
    "regression",
    "analyze",
    "effect",
    "pricing",
    "consumer",
    "behavior",
    "instance",
    "company",
    "changes",
    "price",
    "certain",
    "product",
    "several",
    "times",
    "record",
    "quantity",
    "price",
    "level",
    "perform",
    "linear",
    "regression",
    "sold",
    "quantity",
    "dependent",
    "variable",
    "price",
    "independent",
    "variable",
    "would",
    "result",
    "line",
    "depicts",
    "extent",
    "customer",
    "reduce",
    "consumption",
    "product",
    "prices",
    "increasing",
    "result",
    "would",
    "help",
    "us",
    "future",
    "pricing",
    "decisions",
    "next",
    "assessment",
    "risk",
    "fine",
    "financial",
    "services",
    "insurance",
    "domain",
    "well",
    "linear",
    "regression",
    "used",
    "analyze",
    "risk",
    "example",
    "health",
    "insurance",
    "company",
    "might",
    "conduct",
    "linear",
    "regression",
    "algorithm",
    "plotting",
    "number",
    "claims",
    "per",
    "customer",
    "age",
    "might",
    "discover",
    "old",
    "customers",
    "make",
    "health",
    "insurance",
    "claim",
    "well",
    "result",
    "analysis",
    "might",
    "guide",
    "important",
    "business",
    "decisions",
    "right",
    "rough",
    "idea",
    "linear",
    "regression",
    "algorithm",
    "like",
    "used",
    "use",
    "early",
    "let",
    "move",
    "understand",
    "algorithm",
    "depth",
    "suppose",
    "independent",
    "variable",
    "dependent",
    "variable",
    "right",
    "suppose",
    "data",
    "point",
    "x",
    "axis",
    "independent",
    "variable",
    "increasing",
    "x",
    "axis",
    "dependent",
    "variable",
    "kind",
    "linear",
    "regression",
    "line",
    "would",
    "get",
    "would",
    "get",
    "positive",
    "linear",
    "regression",
    "line",
    "right",
    "slope",
    "would",
    "positive",
    "next",
    "suppose",
    "independent",
    "variable",
    "increasing",
    "hand",
    "dependent",
    "variable",
    "decreasing",
    "kind",
    "line",
    "get",
    "case",
    "get",
    "negative",
    "regression",
    "line",
    "case",
    "slope",
    "line",
    "negative",
    "particular",
    "line",
    "line",
    "equal",
    "mx",
    "plus",
    "c",
    "line",
    "linear",
    "regression",
    "shows",
    "relationship",
    "independent",
    "variable",
    "dependent",
    "variable",
    "line",
    "known",
    "line",
    "linear",
    "regression",
    "okay",
    "let",
    "add",
    "data",
    "points",
    "graph",
    "observation",
    "data",
    "points",
    "graphs",
    "let",
    "plot",
    "okay",
    "data",
    "points",
    "plotted",
    "task",
    "create",
    "regression",
    "line",
    "best",
    "fit",
    "line",
    "right",
    "regression",
    "line",
    "drawn",
    "task",
    "production",
    "suppose",
    "estimated",
    "value",
    "predicted",
    "value",
    "actual",
    "value",
    "okay",
    "main",
    "goal",
    "reduce",
    "error",
    "reduce",
    "distance",
    "estimated",
    "predicted",
    "value",
    "actual",
    "value",
    "best",
    "fit",
    "line",
    "would",
    "one",
    "least",
    "error",
    "least",
    "difference",
    "estimated",
    "value",
    "actual",
    "value",
    "right",
    "words",
    "minimize",
    "error",
    "brief",
    "understanding",
    "linear",
    "regression",
    "algorithm",
    "soon",
    "jump",
    "towards",
    "mathematical",
    "implementation",
    "right",
    "let",
    "tell",
    "suppose",
    "draw",
    "graph",
    "speed",
    "distance",
    "covered",
    "axis",
    "time",
    "demeaning",
    "constant",
    "plot",
    "graph",
    "speed",
    "travel",
    "vehicle",
    "distance",
    "traveled",
    "fixed",
    "unit",
    "time",
    "get",
    "positive",
    "relationship",
    "right",
    "suppose",
    "equation",
    "line",
    "equal",
    "mx",
    "plus",
    "case",
    "distance",
    "traveled",
    "fixed",
    "duration",
    "time",
    "x",
    "speed",
    "vehicle",
    "positive",
    "slope",
    "line",
    "see",
    "line",
    "right",
    "suppose",
    "distance",
    "remaining",
    "constant",
    "plot",
    "graph",
    "rid",
    "vehicle",
    "time",
    "taken",
    "travel",
    "fixed",
    "distance",
    "case",
    "get",
    "line",
    "negative",
    "relationship",
    "right",
    "slope",
    "line",
    "negative",
    "equation",
    "line",
    "changes",
    "equal",
    "minus",
    "mx",
    "plus",
    "c",
    "time",
    "taken",
    "travel",
    "fixed",
    "distance",
    "x",
    "speed",
    "vehicle",
    "negative",
    "slope",
    "line",
    "see",
    "line",
    "right",
    "let",
    "get",
    "back",
    "independent",
    "dependent",
    "variable",
    "term",
    "dependent",
    "variable",
    "independent",
    "variable",
    "let",
    "move",
    "see",
    "mathematical",
    "implementation",
    "things",
    "alright",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "5",
    "let",
    "plot",
    "0",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "alike",
    "3",
    "4",
    "2",
    "4",
    "right",
    "let",
    "plot",
    "1",
    "2",
    "3",
    "4",
    "5",
    "let",
    "plot",
    "coordinates",
    "1",
    "1",
    "x",
    "equal",
    "1",
    "equal",
    "3",
    "x",
    "equal",
    "1",
    "equal",
    "point",
    "1",
    "comma",
    "3",
    "similarly",
    "1",
    "3",
    "2",
    "4",
    "3",
    "2",
    "4",
    "4",
    "5",
    "right",
    "moving",
    "ahead",
    "let",
    "calculate",
    "mean",
    "x",
    "plot",
    "graph",
    "right",
    "mean",
    "x",
    "1",
    "plus",
    "2",
    "plus",
    "3",
    "plus",
    "4",
    "plus",
    "5",
    "divided",
    "right",
    "similarly",
    "mean",
    "3",
    "plus",
    "4",
    "plus",
    "2",
    "plus",
    "4",
    "plus",
    "5",
    "divided",
    "nothing",
    "aligned",
    "next",
    "plot",
    "mean",
    "3",
    "comma",
    "3",
    "graph",
    "okay",
    "point",
    "3",
    "comma",
    "3",
    "see",
    "goal",
    "find",
    "predict",
    "best",
    "fit",
    "line",
    "using",
    "least",
    "square",
    "method",
    "right",
    "order",
    "find",
    "first",
    "need",
    "find",
    "equation",
    "line",
    "let",
    "find",
    "equation",
    "regression",
    "line",
    "right",
    "let",
    "suppose",
    "regression",
    "line",
    "equal",
    "mx",
    "plus",
    "equation",
    "line",
    "need",
    "find",
    "value",
    "see",
    "equals",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "upon",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "get",
    "confused",
    "let",
    "resolve",
    "right",
    "moving",
    "ahead",
    "part",
    "formula",
    "going",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "1",
    "minus",
    "x",
    "bar",
    "3",
    "1",
    "minus",
    "3",
    "minus",
    "2",
    "next",
    "x",
    "equal",
    "minus",
    "mean",
    "3",
    "minus",
    "1",
    "similarly",
    "3",
    "minus",
    "3",
    "0",
    "4",
    "3",
    "1",
    "5",
    "3",
    "2",
    "alight",
    "x",
    "minus",
    "x",
    "bar",
    "nothing",
    "distance",
    "point",
    "line",
    "equal",
    "3",
    "minus",
    "bar",
    "implies",
    "implies",
    "distance",
    "point",
    "line",
    "x",
    "equal",
    "3",
    "fine",
    "let",
    "calculate",
    "value",
    "minus",
    "bar",
    "starting",
    "equal",
    "3",
    "value",
    "bar",
    "three",
    "minus",
    "much",
    "next",
    "4",
    "minus",
    "next",
    "minus",
    "minus",
    "1",
    "point",
    "6",
    "next",
    "4",
    "minus",
    "5",
    "minus",
    "alright",
    "done",
    "minus",
    "bar",
    "fine",
    "next",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "let",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "minus",
    "2",
    "whole",
    "square",
    "4",
    "minus",
    "1",
    "whole",
    "square",
    "1",
    "0",
    "squared",
    "0",
    "1",
    "square",
    "1",
    "2",
    "square",
    "fine",
    "table",
    "x",
    "minus",
    "x",
    "bar",
    "minus",
    "bar",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "need",
    "need",
    "product",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "alright",
    "let",
    "see",
    "product",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "minus",
    "2",
    "x",
    "minus",
    "one",
    "point",
    "2",
    "minus",
    "1",
    "x",
    "0",
    "point",
    "4",
    "minus",
    "0",
    "point",
    "4",
    "0",
    "x",
    "minus",
    "0",
    "1",
    "multiplied",
    "zero",
    "point",
    "four",
    "next",
    "2",
    "multiplied",
    "1",
    "point",
    "right",
    "almost",
    "parts",
    "formula",
    "done",
    "need",
    "get",
    "summation",
    "last",
    "two",
    "columns",
    "right",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "10",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "4",
    "value",
    "equal",
    "4",
    "10",
    "fine",
    "let",
    "put",
    "value",
    "equals",
    "zero",
    "point",
    "4",
    "line",
    "equal",
    "mx",
    "plus",
    "let",
    "file",
    "points",
    "equation",
    "find",
    "value",
    "remember",
    "mean",
    "calculated",
    "x",
    "mean",
    "value",
    "x",
    "3",
    "3",
    "point",
    "6",
    "equals",
    "0",
    "point",
    "4",
    "x",
    "3",
    "plus",
    "alright",
    "equal",
    "1",
    "point",
    "2",
    "plus",
    "value",
    "c",
    "minus",
    "1",
    "point",
    "2",
    "point",
    "right",
    "equals",
    "zero",
    "point",
    "four",
    "see",
    "finally",
    "calculate",
    "equation",
    "regression",
    "line",
    "get",
    "equal",
    "zero",
    "point",
    "four",
    "times",
    "x",
    "plus",
    "two",
    "point",
    "four",
    "regression",
    "line",
    "like",
    "plotting",
    "points",
    "actual",
    "point",
    "right",
    "given",
    "equals",
    "zero",
    "point",
    "four",
    "sql",
    "let",
    "predict",
    "value",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "x",
    "equal",
    "1",
    "predicted",
    "value",
    "zero",
    "point",
    "four",
    "x",
    "one",
    "plus",
    "two",
    "point",
    "four",
    "similarly",
    "x",
    "equal",
    "predicted",
    "value",
    "zero",
    "point",
    "4",
    "x",
    "2",
    "plus",
    "2",
    "point",
    "4",
    "equals",
    "3",
    "point",
    "two",
    "similarly",
    "x",
    "equal",
    "3",
    "3",
    "point",
    "6",
    "x",
    "equal",
    "4",
    "4",
    "point",
    "0",
    "x",
    "equal",
    "5",
    "four",
    "point",
    "four",
    "let",
    "plot",
    "graph",
    "line",
    "passing",
    "predicting",
    "point",
    "cutting",
    "line",
    "regression",
    "task",
    "calculate",
    "distance",
    "actual",
    "predicted",
    "value",
    "job",
    "reduce",
    "distance",
    "right",
    "words",
    "reduce",
    "error",
    "actual",
    "predicted",
    "line",
    "least",
    "error",
    "line",
    "linear",
    "regression",
    "regression",
    "line",
    "also",
    "best",
    "fit",
    "line",
    "alright",
    "things",
    "work",
    "computer",
    "performs",
    "number",
    "iteration",
    "different",
    "values",
    "different",
    "values",
    "calculate",
    "equation",
    "line",
    "equals",
    "mx",
    "plus",
    "right",
    "value",
    "changes",
    "line",
    "changing",
    "iteration",
    "start",
    "one",
    "right",
    "perform",
    "number",
    "iteration",
    "every",
    "iteration",
    "calculate",
    "predicted",
    "value",
    "according",
    "line",
    "compare",
    "distance",
    "actual",
    "value",
    "predicted",
    "value",
    "value",
    "distance",
    "actual",
    "predicted",
    "value",
    "minimum",
    "selected",
    "best",
    "fit",
    "line",
    "right",
    "calculated",
    "best",
    "fit",
    "line",
    "time",
    "check",
    "goodness",
    "fit",
    "check",
    "good",
    "model",
    "performing",
    "order",
    "method",
    "called",
    "r",
    "square",
    "method",
    "r",
    "square",
    "well",
    "value",
    "statistical",
    "measure",
    "close",
    "data",
    "fitted",
    "regression",
    "line",
    "general",
    "considered",
    "high",
    "value",
    "model",
    "good",
    "model",
    "also",
    "lower",
    "squared",
    "value",
    "good",
    "model",
    "well",
    "higher",
    "squad",
    "value",
    "model",
    "fit",
    "right",
    "also",
    "known",
    "coefficient",
    "determination",
    "coefficient",
    "multiple",
    "determination",
    "let",
    "move",
    "see",
    "square",
    "calculated",
    "actual",
    "values",
    "plotted",
    "graph",
    "calculated",
    "predicted",
    "values",
    "remember",
    "calculated",
    "predicted",
    "values",
    "equation",
    "predicted",
    "equals",
    "0",
    "1",
    "4",
    "x",
    "x",
    "plus",
    "two",
    "point",
    "four",
    "every",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "5",
    "got",
    "power",
    "good",
    "values",
    "phi",
    "right",
    "let",
    "plot",
    "graph",
    "point",
    "line",
    "passing",
    "points",
    "nothing",
    "regression",
    "line",
    "right",
    "need",
    "check",
    "compare",
    "distance",
    "actual",
    "mean",
    "versus",
    "distance",
    "predicted",
    "mean",
    "alright",
    "basically",
    "calculating",
    "distance",
    "actual",
    "value",
    "mean",
    "distance",
    "predicted",
    "value",
    "mean",
    "right",
    "nothing",
    "square",
    "mathematically",
    "represent",
    "school",
    "whereas",
    "summation",
    "predicted",
    "values",
    "minus",
    "bar",
    "whole",
    "square",
    "divided",
    "summation",
    "minus",
    "bar",
    "whole",
    "square",
    "actual",
    "value",
    "p",
    "predicted",
    "value",
    "bar",
    "mean",
    "value",
    "nothing",
    "remember",
    "formula",
    "next",
    "calculate",
    "minus",
    "bar",
    "3y",
    "bar",
    "3",
    "point",
    "6",
    "calculate",
    "3",
    "minus",
    "nothing",
    "minus",
    "similarly",
    "equals",
    "4",
    "bar",
    "equal",
    "minus",
    "bar",
    "zero",
    "point",
    "4",
    "2",
    "minus",
    "1",
    "point",
    "6",
    "4",
    "minus",
    "zero",
    "point",
    "four",
    "five",
    "minus",
    "got",
    "value",
    "minus",
    "bar",
    "take",
    "square",
    "minus",
    "square",
    "square",
    "square",
    "square",
    "squared",
    "part",
    "formula",
    "need",
    "need",
    "yp",
    "minus",
    "bar",
    "value",
    "vip",
    "values",
    "subtract",
    "right",
    "2",
    "minus",
    "minus",
    "similarly",
    "get",
    "minus",
    "minus",
    "0",
    "1",
    "0",
    "minus",
    "4",
    "minus",
    "calculated",
    "value",
    "yp",
    "minus",
    "bar",
    "turn",
    "calculate",
    "value",
    "b",
    "minus",
    "bar",
    "whole",
    "square",
    "next",
    "square",
    "point",
    "four",
    "square",
    "square",
    "0",
    "0",
    "point",
    "4",
    "square",
    "square",
    "right",
    "part",
    "formula",
    "suggests",
    "suggests",
    "take",
    "summation",
    "p",
    "minus",
    "bar",
    "whole",
    "square",
    "summation",
    "minus",
    "bar",
    "whole",
    "square",
    "right",
    "let",
    "see",
    "submitting",
    "minus",
    "bar",
    "whole",
    "square",
    "get",
    "five",
    "point",
    "two",
    "summation",
    "p",
    "minus",
    "bar",
    "whole",
    "square",
    "get",
    "one",
    "point",
    "six",
    "value",
    "r",
    "square",
    "calculated",
    "1",
    "point",
    "6",
    "upon",
    "fine",
    "result",
    "get",
    "approximately",
    "equal",
    "well",
    "good",
    "fit",
    "right",
    "suggests",
    "data",
    "points",
    "far",
    "away",
    "regression",
    "line",
    "alright",
    "graph",
    "look",
    "like",
    "r",
    "square",
    "increase",
    "value",
    "r",
    "square",
    "see",
    "actual",
    "value",
    "would",
    "like",
    "closer",
    "regression",
    "line",
    "reaches",
    "comes",
    "clothes",
    "value",
    "approximately",
    "equals",
    "1",
    "actual",
    "values",
    "lies",
    "regression",
    "line",
    "example",
    "case",
    "get",
    "low",
    "value",
    "r",
    "square",
    "suppose",
    "case",
    "see",
    "actual",
    "values",
    "far",
    "away",
    "regression",
    "line",
    "say",
    "many",
    "outliers",
    "data",
    "focus",
    "anything",
    "data",
    "right",
    "calculation",
    "r",
    "square",
    "might",
    "get",
    "question",
    "like",
    "low",
    "values",
    "square",
    "always",
    "bad",
    "well",
    "field",
    "entirely",
    "expected",
    "ask",
    "value",
    "low",
    "example",
    "field",
    "attempts",
    "predict",
    "human",
    "behavior",
    "psychology",
    "typically",
    "values",
    "lower",
    "around",
    "50",
    "conclude",
    "humans",
    "simply",
    "harder",
    "predict",
    "physical",
    "process",
    "furthermore",
    "squared",
    "value",
    "low",
    "statistically",
    "significant",
    "predictors",
    "still",
    "draw",
    "important",
    "conclusion",
    "changes",
    "predicator",
    "values",
    "associated",
    "oh",
    "sated",
    "changes",
    "response",
    "value",
    "regardless",
    "significant",
    "coefficient",
    "still",
    "represent",
    "mean",
    "change",
    "response",
    "one",
    "unit",
    "change",
    "predicator",
    "holding",
    "predators",
    "model",
    "constant",
    "obviously",
    "type",
    "information",
    "extremely",
    "valuable",
    "right",
    "right",
    "theoretical",
    "concept",
    "let",
    "move",
    "coding",
    "part",
    "understand",
    "code",
    "depth",
    "implementing",
    "linear",
    "regression",
    "using",
    "python",
    "using",
    "anaconda",
    "jupyter",
    "notebook",
    "installed",
    "like",
    "jupyter",
    "notebook",
    "using",
    "python",
    "alright",
    "going",
    "use",
    "data",
    "set",
    "consisting",
    "head",
    "size",
    "human",
    "brain",
    "different",
    "people",
    "right",
    "let",
    "import",
    "data",
    "set",
    "percent",
    "matplotlib",
    "line",
    "importing",
    "numpy",
    "np",
    "pandas",
    "speedy",
    "matplotlib",
    "matplotlib",
    "importing",
    "pipe",
    "plt",
    "alright",
    "next",
    "import",
    "data",
    "brain",
    "dot",
    "csv",
    "store",
    "data",
    "variable",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "armor",
    "asterisk",
    "symbol",
    "symbolizes",
    "still",
    "executing",
    "output",
    "dataset",
    "consists",
    "two",
    "thirty",
    "seven",
    "rows",
    "four",
    "columns",
    "columns",
    "gender",
    "age",
    "range",
    "head",
    "size",
    "centimeter",
    "cube",
    "brain",
    "weights",
    "graham",
    "fine",
    "sample",
    "data",
    "set",
    "looks",
    "consists",
    "data",
    "set",
    "imported",
    "data",
    "see",
    "237",
    "values",
    "training",
    "set",
    "find",
    "linear",
    "relationship",
    "head",
    "size",
    "brain",
    "weights",
    "collect",
    "x",
    "x",
    "would",
    "consist",
    "head",
    "size",
    "values",
    "would",
    "consist",
    "brain",
    "values",
    "collecting",
    "x",
    "let",
    "execute",
    "run",
    "done",
    "next",
    "need",
    "find",
    "values",
    "b",
    "1",
    "b",
    "say",
    "need",
    "mean",
    "x",
    "values",
    "first",
    "calculate",
    "mean",
    "x",
    "mean",
    "x",
    "equal",
    "np",
    "dot",
    "min",
    "mean",
    "predefined",
    "function",
    "numb",
    "similarly",
    "mean",
    "underscore",
    "equal",
    "np",
    "dot",
    "mean",
    "return",
    "return",
    "mean",
    "values",
    "next",
    "check",
    "total",
    "number",
    "values",
    "equals",
    "well",
    "length",
    "alright",
    "use",
    "formula",
    "calculate",
    "values",
    "b",
    "1",
    "b",
    "naught",
    "fnc",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "result",
    "see",
    "screen",
    "got",
    "b",
    "1",
    "0",
    "point",
    "2",
    "6",
    "3",
    "b",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "alright",
    "coefficient",
    "comparing",
    "equation",
    "equal",
    "mx",
    "plus",
    "say",
    "brain",
    "weight",
    "equals",
    "zero",
    "point",
    "2",
    "6",
    "3",
    "x",
    "head",
    "size",
    "plus",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "say",
    "value",
    "3",
    "value",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "right",
    "linear",
    "model",
    "let",
    "plot",
    "see",
    "graphically",
    "let",
    "execute",
    "plot",
    "looks",
    "like",
    "model",
    "bad",
    "need",
    "find",
    "good",
    "model",
    "order",
    "find",
    "many",
    "methods",
    "like",
    "root",
    "means",
    "square",
    "method",
    "coefficient",
    "determination",
    "square",
    "method",
    "tutorial",
    "told",
    "score",
    "method",
    "let",
    "focus",
    "see",
    "good",
    "model",
    "let",
    "calculate",
    "r",
    "square",
    "value",
    "right",
    "ss",
    "underscore",
    "total",
    "sum",
    "square",
    "ss",
    "total",
    "sum",
    "square",
    "residuals",
    "r",
    "square",
    "formula",
    "1",
    "minus",
    "total",
    "sum",
    "squares",
    "upon",
    "total",
    "sum",
    "square",
    "residuals",
    "right",
    "next",
    "execute",
    "get",
    "value",
    "r",
    "square",
    "pretty",
    "good",
    "implemented",
    "simple",
    "linear",
    "regression",
    "model",
    "using",
    "least",
    "square",
    "method",
    "let",
    "move",
    "see",
    "implement",
    "model",
    "using",
    "machine",
    "learning",
    "library",
    "called",
    "right",
    "simple",
    "machine",
    "young",
    "library",
    "python",
    "welding",
    "machine",
    "learning",
    "model",
    "easy",
    "using",
    "suppose",
    "python",
    "code",
    "using",
    "libraries",
    "code",
    "shortens",
    "length",
    "like",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "get",
    "score",
    "well",
    "today",
    "discussion",
    "entities",
    "world",
    "related",
    "one",
    "way",
    "another",
    "times",
    "finding",
    "relationship",
    "entities",
    "help",
    "take",
    "valuable",
    "business",
    "decisions",
    "today",
    "going",
    "talk",
    "logistic",
    "regression",
    "one",
    "approach",
    "towards",
    "predicting",
    "relationships",
    "let",
    "us",
    "see",
    "going",
    "cover",
    "today",
    "training",
    "start",
    "session",
    "getting",
    "quick",
    "introduction",
    "regression",
    "see",
    "different",
    "types",
    "regression",
    "discussing",
    "logistic",
    "regression",
    "part",
    "discuss",
    "exactly",
    "used",
    "used",
    "things",
    "moving",
    "ahead",
    "compare",
    "linear",
    "regression",
    "versus",
    "logistic",
    "regression",
    "along",
    "various",
    "use",
    "cases",
    "finally",
    "towards",
    "end",
    "practically",
    "implementing",
    "logistic",
    "regression",
    "algorithm",
    "let",
    "quickly",
    "start",
    "first",
    "topic",
    "regression",
    "regression",
    "analysis",
    "predictive",
    "modeling",
    "technique",
    "always",
    "involves",
    "predictions",
    "session",
    "talk",
    "predictive",
    "analysis",
    "prescriptive",
    "analysis",
    "descriptive",
    "analysis",
    "need",
    "good",
    "base",
    "stronghold",
    "predictive",
    "part",
    "first",
    "estimates",
    "relationship",
    "dependent",
    "variable",
    "independent",
    "variable",
    "aware",
    "terminologies",
    "let",
    "give",
    "quick",
    "summary",
    "dependent",
    "variable",
    "nothing",
    "variable",
    "want",
    "predict",
    "let",
    "say",
    "want",
    "know",
    "sales",
    "26th",
    "month",
    "sales",
    "becomes",
    "dependent",
    "variable",
    "see",
    "target",
    "variable",
    "dependent",
    "variable",
    "target",
    "variable",
    "going",
    "depend",
    "lot",
    "actors",
    "number",
    "products",
    "sold",
    "till",
    "date",
    "season",
    "availability",
    "product",
    "product",
    "quality",
    "things",
    "neverending",
    "factors",
    "nothing",
    "different",
    "features",
    "leads",
    "sail",
    "variables",
    "called",
    "independent",
    "variable",
    "say",
    "predictor",
    "look",
    "graph",
    "values",
    "x",
    "values",
    "see",
    "x",
    "increases",
    "value",
    "also",
    "increases",
    "let",
    "explain",
    "example",
    "let",
    "say",
    "value",
    "x",
    "six",
    "point",
    "seven",
    "five",
    "somebody",
    "asked",
    "value",
    "value",
    "x",
    "7",
    "way",
    "regression",
    "comes",
    "picture",
    "fitting",
    "straight",
    "line",
    "points",
    "getting",
    "value",
    "straight",
    "line",
    "guys",
    "formula",
    "straight",
    "line",
    "equal",
    "mx",
    "plus",
    "using",
    "try",
    "predict",
    "value",
    "notice",
    "x",
    "variable",
    "increase",
    "much",
    "variable",
    "increase",
    "according",
    "x",
    "basically",
    "dependent",
    "x",
    "variable",
    "arbitrary",
    "value",
    "x",
    "predict",
    "value",
    "always",
    "done",
    "regression",
    "regression",
    "useful",
    "regression",
    "basically",
    "classified",
    "three",
    "types",
    "linear",
    "regression",
    "logistic",
    "regression",
    "polynomial",
    "regression",
    "today",
    "discussing",
    "logistic",
    "regression",
    "let",
    "move",
    "forward",
    "understand",
    "logistic",
    "regression",
    "algorithm",
    "widely",
    "used",
    "dependent",
    "variable",
    "see",
    "output",
    "binary",
    "format",
    "need",
    "predict",
    "outcome",
    "categorical",
    "dependent",
    "variable",
    "outcome",
    "always",
    "discreet",
    "categorical",
    "nature",
    "discrete",
    "mean",
    "value",
    "binary",
    "say",
    "two",
    "values",
    "either",
    "0",
    "1",
    "either",
    "yes",
    "either",
    "true",
    "false",
    "high",
    "low",
    "outcomes",
    "value",
    "need",
    "create",
    "discrete",
    "say",
    "categorical",
    "nature",
    "whereas",
    "linear",
    "regression",
    "value",
    "see",
    "val",
    "need",
    "predict",
    "within",
    "range",
    "difference",
    "linear",
    "regression",
    "logistic",
    "regression",
    "must",
    "question",
    "linear",
    "regression",
    "guys",
    "linear",
    "regression",
    "value",
    "value",
    "need",
    "predict",
    "range",
    "case",
    "logistic",
    "regression",
    "two",
    "values",
    "either",
    "0",
    "one",
    "entertain",
    "values",
    "zero",
    "one",
    "linear",
    "regression",
    "value",
    "range",
    "order",
    "implement",
    "logic",
    "regression",
    "need",
    "clip",
    "part",
    "need",
    "value",
    "zero",
    "need",
    "value",
    "1",
    "since",
    "value",
    "0",
    "1",
    "main",
    "rule",
    "logistic",
    "regression",
    "linear",
    "line",
    "clipped",
    "0",
    "1",
    "clip",
    "graph",
    "would",
    "look",
    "somewhat",
    "like",
    "getting",
    "curve",
    "nothing",
    "three",
    "different",
    "straight",
    "lines",
    "need",
    "make",
    "new",
    "way",
    "solve",
    "problem",
    "formulated",
    "equation",
    "hence",
    "come",
    "logistic",
    "regression",
    "outcome",
    "either",
    "0",
    "one",
    "main",
    "rule",
    "logistic",
    "regression",
    "resulting",
    "curve",
    "formulated",
    "hence",
    "main",
    "aim",
    "bring",
    "values",
    "0",
    "1",
    "fulfilled",
    "came",
    "large",
    "stick",
    "regression",
    "gets",
    "formulated",
    "equation",
    "looks",
    "somewhat",
    "like",
    "guys",
    "nothing",
    "curve",
    "say",
    "sigmoid",
    "curve",
    "sigmoid",
    "function",
    "curve",
    "sigmoid",
    "function",
    "basically",
    "converts",
    "value",
    "minus",
    "infinity",
    "infinity",
    "discrete",
    "values",
    "logitech",
    "regression",
    "wants",
    "say",
    "values",
    "binary",
    "format",
    "either",
    "0",
    "see",
    "values",
    "either",
    "0",
    "1",
    "nothing",
    "transition",
    "guys",
    "catch",
    "let",
    "say",
    "data",
    "point",
    "decide",
    "whether",
    "value",
    "0",
    "1",
    "concept",
    "threshold",
    "basically",
    "divides",
    "line",
    "threshold",
    "value",
    "basically",
    "indicates",
    "probability",
    "either",
    "winning",
    "losing",
    "winning",
    "mean",
    "value",
    "equal",
    "one",
    "losing",
    "mean",
    "values",
    "equal",
    "0",
    "let",
    "data",
    "point",
    "let",
    "say",
    "cursor",
    "check",
    "whether",
    "value",
    "less",
    "threshold",
    "value",
    "let",
    "say",
    "threshold",
    "value",
    "give",
    "result",
    "1",
    "less",
    "give",
    "result",
    "zero",
    "threshold",
    "value",
    "need",
    "define",
    "value",
    "let",
    "value",
    "shall",
    "rounded",
    "two",
    "one",
    "one",
    "let",
    "say",
    "less",
    "let",
    "value",
    "reduce",
    "zero",
    "use",
    "concept",
    "threshold",
    "value",
    "find",
    "output",
    "discreet",
    "either",
    "0",
    "one",
    "hope",
    "caught",
    "curve",
    "logistic",
    "regression",
    "guys",
    "sigmoid",
    "curve",
    "make",
    "curve",
    "need",
    "make",
    "equation",
    "let",
    "address",
    "part",
    "well",
    "let",
    "see",
    "equation",
    "formed",
    "imitate",
    "functionality",
    "equation",
    "straight",
    "line",
    "equal",
    "mx",
    "plus",
    "case",
    "one",
    "independent",
    "variable",
    "let",
    "say",
    "many",
    "independent",
    "variable",
    "equation",
    "becomes",
    "1",
    "x",
    "1",
    "plus",
    "2",
    "x",
    "2",
    "plus",
    "3",
    "x",
    "3",
    "till",
    "nx",
    "n",
    "let",
    "us",
    "put",
    "b",
    "equation",
    "becomes",
    "equal",
    "b",
    "1",
    "x",
    "1",
    "plus",
    "beta",
    "2",
    "x",
    "2",
    "plus",
    "b",
    "3",
    "x",
    "3",
    "till",
    "nxn",
    "plus",
    "guys",
    "equation",
    "straight",
    "line",
    "range",
    "minus",
    "infinity",
    "infinity",
    "yeah",
    "case",
    "say",
    "largest",
    "equation",
    "value",
    "need",
    "predict",
    "say",
    "value",
    "range",
    "0",
    "case",
    "need",
    "transform",
    "equation",
    "done",
    "divide",
    "equation",
    "1",
    "minus",
    "equal",
    "0",
    "0",
    "1",
    "minus",
    "0",
    "equal",
    "1",
    "0",
    "1",
    "0",
    "take",
    "equals",
    "1",
    "1",
    "1",
    "minus",
    "1",
    "0",
    "1",
    "0",
    "infinity",
    "range",
    "0",
    "infinity",
    "want",
    "range",
    "minus",
    "infinity",
    "infinity",
    "log",
    "equation",
    "let",
    "go",
    "ahead",
    "logarithmic",
    "equation",
    "transform",
    "get",
    "range",
    "minus",
    "infinity",
    "infinity",
    "log",
    "1",
    "minus",
    "1",
    "final",
    "logistic",
    "regression",
    "equation",
    "guys",
    "worry",
    "write",
    "formula",
    "memorize",
    "formula",
    "python",
    "need",
    "call",
    "function",
    "logistic",
    "regression",
    "everything",
    "automatically",
    "want",
    "scare",
    "maths",
    "formulas",
    "behind",
    "always",
    "good",
    "know",
    "formula",
    "generated",
    "hope",
    "guys",
    "clear",
    "logistic",
    "regression",
    "comes",
    "picture",
    "next",
    "let",
    "us",
    "see",
    "major",
    "differences",
    "linear",
    "regression",
    "logistic",
    "regression",
    "first",
    "linear",
    "regression",
    "value",
    "continuous",
    "variable",
    "variable",
    "need",
    "predict",
    "continuous",
    "nature",
    "whereas",
    "logistic",
    "regression",
    "categorical",
    "variable",
    "value",
    "need",
    "discrete",
    "nature",
    "either",
    "0",
    "1",
    "two",
    "values",
    "example",
    "whether",
    "raining",
    "raining",
    "humid",
    "outside",
    "humid",
    "outside",
    "going",
    "snow",
    "going",
    "snow",
    "example",
    "need",
    "predict",
    "values",
    "discrete",
    "predict",
    "happening",
    "next",
    "linear",
    "equation",
    "solves",
    "regression",
    "problems",
    "concept",
    "independent",
    "variable",
    "dependent",
    "variable",
    "calculate",
    "value",
    "need",
    "plate",
    "using",
    "value",
    "variable",
    "see",
    "value",
    "need",
    "predict",
    "range",
    "whereas",
    "logistic",
    "regression",
    "discrete",
    "values",
    "logistic",
    "regression",
    "basically",
    "solves",
    "classification",
    "problem",
    "basically",
    "classify",
    "give",
    "result",
    "whether",
    "event",
    "happening",
    "hope",
    "pretty",
    "much",
    "clear",
    "till",
    "next",
    "linear",
    "regression",
    "graph",
    "seen",
    "straight",
    "line",
    "graph",
    "calculate",
    "value",
    "respect",
    "value",
    "x",
    "logistic",
    "regression",
    "glad",
    "got",
    "escobar",
    "see",
    "sigmoid",
    "curve",
    "using",
    "sigmoid",
    "function",
    "predict",
    "values",
    "hope",
    "guys",
    "clear",
    "differences",
    "linear",
    "regression",
    "logistic",
    "regression",
    "moving",
    "little",
    "see",
    "various",
    "use",
    "cases",
    "logistic",
    "regression",
    "implemented",
    "real",
    "life",
    "first",
    "weather",
    "prediction",
    "largest",
    "aggression",
    "helps",
    "predict",
    "weather",
    "example",
    "used",
    "predict",
    "whether",
    "raining",
    "whether",
    "sunny",
    "cloudy",
    "things",
    "things",
    "predicted",
    "using",
    "logistic",
    "regression",
    "need",
    "keep",
    "mind",
    "linear",
    "regression",
    "logistic",
    "regression",
    "used",
    "predicting",
    "weather",
    "case",
    "linear",
    "regression",
    "helps",
    "predict",
    "temperature",
    "tomorrow",
    "whereas",
    "logistic",
    "regression",
    "tell",
    "going",
    "rain",
    "whether",
    "cloudy",
    "going",
    "snow",
    "values",
    "discrete",
    "whereas",
    "apply",
    "linear",
    "regression",
    "predicting",
    "things",
    "like",
    "temperature",
    "tomorrow",
    "temperature",
    "day",
    "tomorrow",
    "thing",
    "slight",
    "linear",
    "regression",
    "logistic",
    "regression",
    "moving",
    "ahead",
    "classification",
    "problem",
    "python",
    "performs",
    "classification",
    "help",
    "tell",
    "whether",
    "bird",
    "board",
    "classify",
    "different",
    "kind",
    "mammals",
    "let",
    "say",
    "whether",
    "dog",
    "dog",
    "similarly",
    "check",
    "reptile",
    "whether",
    "reptile",
    "reptile",
    "logistic",
    "regression",
    "perform",
    "classification",
    "point",
    "already",
    "discussed",
    "using",
    "classification",
    "problems",
    "next",
    "also",
    "helps",
    "determine",
    "illnesses",
    "let",
    "take",
    "example",
    "let",
    "say",
    "patient",
    "goes",
    "routine",
    "check",
    "hospital",
    "doctor",
    "perform",
    "various",
    "tests",
    "patient",
    "check",
    "whether",
    "patient",
    "actually",
    "law",
    "features",
    "doctor",
    "check",
    "sugar",
    "level",
    "blood",
    "pressure",
    "age",
    "patient",
    "small",
    "old",
    "person",
    "previous",
    "medical",
    "history",
    "patient",
    "features",
    "recorded",
    "doctor",
    "finally",
    "checks",
    "patient",
    "data",
    "data",
    "outcome",
    "illness",
    "severity",
    "illness",
    "using",
    "data",
    "doctor",
    "identify",
    "whether",
    "patient",
    "ill",
    "various",
    "use",
    "cases",
    "use",
    "logistic",
    "regression",
    "guess",
    "enough",
    "theory",
    "part",
    "let",
    "move",
    "ahead",
    "see",
    "practical",
    "implementation",
    "logistic",
    "regression",
    "implementing",
    "two",
    "projects",
    "data",
    "set",
    "titanic",
    "predict",
    "factors",
    "made",
    "people",
    "likely",
    "survive",
    "sinking",
    "titanic",
    "ship",
    "anime",
    "second",
    "project",
    "see",
    "data",
    "analysis",
    "suv",
    "cars",
    "data",
    "suv",
    "cars",
    "purchase",
    "factors",
    "made",
    "people",
    "interested",
    "buying",
    "suv",
    "major",
    "questions",
    "implement",
    "logistic",
    "regression",
    "output",
    "get",
    "let",
    "start",
    "first",
    "project",
    "titanic",
    "data",
    "analysis",
    "might",
    "know",
    "ship",
    "called",
    "titanic",
    "basically",
    "hit",
    "iceberg",
    "sank",
    "bottom",
    "ocean",
    "big",
    "disaster",
    "time",
    "first",
    "voyage",
    "ship",
    "supposed",
    "really",
    "really",
    "strongly",
    "built",
    "one",
    "best",
    "ships",
    "time",
    "big",
    "disaster",
    "time",
    "course",
    "movie",
    "well",
    "many",
    "might",
    "washed",
    "data",
    "passengers",
    "survived",
    "survive",
    "particular",
    "tragedy",
    "look",
    "data",
    "analyze",
    "factors",
    "would",
    "contributed",
    "chances",
    "person",
    "survival",
    "ship",
    "using",
    "logistic",
    "regression",
    "predict",
    "whether",
    "person",
    "survived",
    "person",
    "died",
    "apart",
    "also",
    "look",
    "various",
    "features",
    "along",
    "first",
    "explore",
    "data",
    "set",
    "index",
    "value",
    "first",
    "column",
    "passenger",
    "id",
    "next",
    "column",
    "survived",
    "two",
    "values",
    "0",
    "1",
    "0",
    "stands",
    "survive",
    "one",
    "stands",
    "survive",
    "column",
    "categorical",
    "values",
    "discrete",
    "next",
    "passenger",
    "class",
    "three",
    "values",
    "1",
    "2",
    "basically",
    "tells",
    "whether",
    "think",
    "stabbing",
    "first",
    "class",
    "second",
    "class",
    "third",
    "class",
    "name",
    "passenger",
    "six",
    "see",
    "gender",
    "passenger",
    "passenger",
    "male",
    "female",
    "age",
    "sip",
    "sp",
    "basically",
    "means",
    "number",
    "siblings",
    "spouses",
    "aboard",
    "titanic",
    "values",
    "1",
    "0",
    "parts",
    "apart",
    "basically",
    "number",
    "parents",
    "children",
    "aboard",
    "titanic",
    "also",
    "values",
    "ticket",
    "number",
    "fear",
    "cabin",
    "number",
    "embarked",
    "column",
    "inbox",
    "column",
    "three",
    "values",
    "sc",
    "basically",
    "stands",
    "southampton",
    "c",
    "stands",
    "cherbourg",
    "q",
    "stands",
    "queenstown",
    "features",
    "applying",
    "model",
    "perform",
    "various",
    "steps",
    "implementing",
    "logistic",
    "regression",
    "various",
    "steps",
    "required",
    "implement",
    "algorithm",
    "case",
    "implementing",
    "logistic",
    "regression",
    "first",
    "step",
    "collect",
    "data",
    "import",
    "libraries",
    "used",
    "collecting",
    "data",
    "taking",
    "forward",
    "second",
    "step",
    "analyze",
    "data",
    "go",
    "various",
    "fields",
    "analyze",
    "data",
    "check",
    "females",
    "children",
    "survive",
    "better",
    "males",
    "rich",
    "passenger",
    "survived",
    "poor",
    "passenger",
    "money",
    "matter",
    "paid",
    "get",
    "shape",
    "evacuated",
    "first",
    "workers",
    "worker",
    "survived",
    "survival",
    "rate",
    "worker",
    "ship",
    "traveling",
    "passenger",
    "interesting",
    "questions",
    "would",
    "going",
    "one",
    "one",
    "stage",
    "need",
    "analyze",
    "data",
    "explore",
    "data",
    "much",
    "third",
    "step",
    "wrangle",
    "data",
    "data",
    "wrangling",
    "basically",
    "means",
    "cleaning",
    "data",
    "simply",
    "remove",
    "unnecessary",
    "items",
    "null",
    "values",
    "data",
    "set",
    "clear",
    "data",
    "take",
    "forward",
    "step",
    "build",
    "model",
    "using",
    "train",
    "data",
    "test",
    "using",
    "test",
    "performing",
    "split",
    "basically",
    "split",
    "data",
    "set",
    "training",
    "testing",
    "data",
    "set",
    "find",
    "check",
    "accuracy",
    "ensure",
    "much",
    "accurate",
    "values",
    "hope",
    "guys",
    "got",
    "five",
    "steps",
    "going",
    "implement",
    "autistic",
    "regression",
    "let",
    "go",
    "steps",
    "detail",
    "number",
    "one",
    "collect",
    "data",
    "say",
    "import",
    "libraries",
    "may",
    "show",
    "implementation",
    "part",
    "well",
    "open",
    "jupyter",
    "notebook",
    "implement",
    "steps",
    "guys",
    "jupyter",
    "notebook",
    "first",
    "let",
    "rename",
    "jupyter",
    "notebook",
    "let",
    "say",
    "titanic",
    "data",
    "analysis",
    "first",
    "step",
    "import",
    "libraries",
    "collect",
    "data",
    "let",
    "import",
    "libraries",
    "first",
    "first",
    "import",
    "pandas",
    "pandas",
    "used",
    "data",
    "analysis",
    "say",
    "input",
    "pandas",
    "pd",
    "importing",
    "numpy",
    "say",
    "import",
    "numpy",
    "np",
    "numpy",
    "library",
    "python",
    "basically",
    "stands",
    "numerical",
    "python",
    "widely",
    "used",
    "perform",
    "scientific",
    "computation",
    "next",
    "importing",
    "seaborn",
    "c",
    "1",
    "library",
    "statistical",
    "brought",
    "think",
    "say",
    "import",
    "seaborn",
    "sns",
    "also",
    "import",
    "matplotlib",
    "matplotlib",
    "library",
    "plotting",
    "say",
    "import",
    "matplotlib",
    "dot",
    "pi",
    "plot",
    "plt",
    "run",
    "library",
    "jupyter",
    "notebook",
    "write",
    "percentage",
    "matplotlib",
    "line",
    "next",
    "importing",
    "one",
    "module",
    "well",
    "calculate",
    "basic",
    "mathematical",
    "functions",
    "say",
    "import",
    "mats",
    "libraries",
    "needing",
    "titanic",
    "data",
    "analysis",
    "let",
    "import",
    "data",
    "set",
    "take",
    "variable",
    "let",
    "say",
    "titanic",
    "data",
    "using",
    "pandas",
    "read",
    "csv",
    "see",
    "data",
    "set",
    "like",
    "name",
    "data",
    "set",
    "titanic",
    "dot",
    "csv",
    "already",
    "showed",
    "data",
    "set",
    "let",
    "print",
    "top",
    "10",
    "rows",
    "say",
    "take",
    "variable",
    "titanic",
    "data",
    "dot",
    "head",
    "say",
    "top",
    "ten",
    "rules",
    "run",
    "run",
    "fellows",
    "press",
    "shift",
    "enter",
    "else",
    "directly",
    "click",
    "cell",
    "index",
    "passenger",
    "id",
    "nothing",
    "index",
    "starting",
    "1",
    "survived",
    "column",
    "category",
    "call",
    "values",
    "say",
    "discrete",
    "values",
    "form",
    "0",
    "passenger",
    "class",
    "name",
    "passenger",
    "6",
    "8",
    "data",
    "set",
    "going",
    "forward",
    "next",
    "let",
    "us",
    "bring",
    "number",
    "passengers",
    "original",
    "data",
    "set",
    "simply",
    "type",
    "print",
    "say",
    "number",
    "passengers",
    "using",
    "length",
    "function",
    "calculate",
    "total",
    "length",
    "say",
    "length",
    "inside",
    "passing",
    "variable",
    "titanic",
    "data",
    "copy",
    "paste",
    "dot",
    "index",
    "next",
    "set",
    "bring",
    "one",
    "number",
    "passengers",
    "original",
    "data",
    "set",
    "891",
    "around",
    "number",
    "traveling",
    "titanic",
    "ship",
    "first",
    "step",
    "done",
    "collected",
    "data",
    "imported",
    "libraries",
    "find",
    "total",
    "number",
    "passengers",
    "titanic",
    "let",
    "go",
    "back",
    "presentation",
    "let",
    "see",
    "next",
    "step",
    "done",
    "collecting",
    "data",
    "next",
    "step",
    "analyze",
    "data",
    "creating",
    "different",
    "plots",
    "check",
    "relationship",
    "variables",
    "one",
    "variable",
    "affecting",
    "simply",
    "explore",
    "data",
    "set",
    "making",
    "use",
    "various",
    "columns",
    "plot",
    "graph",
    "either",
    "plot",
    "correlation",
    "graph",
    "plot",
    "distribution",
    "curve",
    "guys",
    "let",
    "go",
    "back",
    "jupyter",
    "notebook",
    "let",
    "analyze",
    "data",
    "second",
    "part",
    "analyze",
    "data",
    "put",
    "headed",
    "put",
    "go",
    "code",
    "click",
    "mark",
    "run",
    "first",
    "let",
    "us",
    "plot",
    "account",
    "plot",
    "pay",
    "passengers",
    "survived",
    "survive",
    "using",
    "seabourn",
    "library",
    "imported",
    "seaborn",
    "sns",
    "write",
    "whole",
    "name",
    "simply",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "say",
    "axis",
    "survive",
    "data",
    "using",
    "titanic",
    "data",
    "say",
    "name",
    "variable",
    "store",
    "data",
    "set",
    "let",
    "run",
    "see",
    "survived",
    "column",
    "x",
    "axis",
    "axis",
    "count",
    "0",
    "basically",
    "stands",
    "survive",
    "one",
    "stands",
    "passengers",
    "survive",
    "see",
    "around",
    "550",
    "passengers",
    "survive",
    "around",
    "350",
    "passengers",
    "survive",
    "basically",
    "compute",
    "less",
    "survivors",
    "survivors",
    "first",
    "floor",
    "another",
    "plot",
    "compare",
    "sex",
    "whether",
    "passengers",
    "survived",
    "survive",
    "many",
    "men",
    "many",
    "female",
    "simply",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "add",
    "hue",
    "six",
    "want",
    "know",
    "many",
    "females",
    "many",
    "male",
    "survive",
    "specifying",
    "data",
    "using",
    "titanic",
    "data",
    "set",
    "let",
    "run",
    "done",
    "mistake",
    "see",
    "survived",
    "column",
    "count",
    "view",
    "color",
    "stands",
    "male",
    "passengers",
    "orange",
    "stands",
    "female",
    "see",
    "passengers",
    "survive",
    "value",
    "0",
    "see",
    "majority",
    "males",
    "survive",
    "see",
    "people",
    "survived",
    "see",
    "majority",
    "female",
    "survive",
    "basically",
    "concludes",
    "gender",
    "survival",
    "rate",
    "appears",
    "average",
    "women",
    "three",
    "times",
    "likely",
    "survive",
    "men",
    "next",
    "let",
    "us",
    "plot",
    "another",
    "plot",
    "hue",
    "passenger",
    "class",
    "see",
    "class",
    "passenger",
    "traveling",
    "whether",
    "traveling",
    "class",
    "one",
    "two",
    "three",
    "tried",
    "command",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "keep",
    "subtly",
    "change",
    "passenger",
    "class",
    "variable",
    "named",
    "pe",
    "class",
    "data",
    "said",
    "using",
    "titanic",
    "data",
    "result",
    "see",
    "blue",
    "orange",
    "second",
    "class",
    "green",
    "third",
    "class",
    "passengers",
    "survive",
    "majorly",
    "third",
    "class",
    "say",
    "lowest",
    "class",
    "cheapest",
    "class",
    "get",
    "dynamic",
    "people",
    "survive",
    "majorly",
    "belong",
    "higher",
    "classes",
    "1",
    "2",
    "eyes",
    "passenger",
    "traveling",
    "third",
    "class",
    "concluded",
    "passengers",
    "survive",
    "majorly",
    "third",
    "class",
    "us",
    "see",
    "lowest",
    "class",
    "passengers",
    "traveling",
    "first",
    "second",
    "class",
    "would",
    "tend",
    "survive",
    "next",
    "got",
    "graph",
    "age",
    "distribution",
    "simply",
    "use",
    "data",
    "using",
    "pandas",
    "library",
    "declare",
    "array",
    "pass",
    "column",
    "age",
    "plot",
    "want",
    "histogram",
    "say",
    "plot",
    "da",
    "test",
    "notice",
    "young",
    "passengers",
    "see",
    "children",
    "ages",
    "0",
    "10",
    "average",
    "people",
    "go",
    "ahead",
    "lester",
    "would",
    "population",
    "analysis",
    "age",
    "column",
    "saw",
    "young",
    "passengers",
    "mediocre",
    "eight",
    "passengers",
    "traveling",
    "titanic",
    "next",
    "let",
    "plot",
    "graph",
    "fare",
    "well",
    "say",
    "titanic",
    "data",
    "say",
    "fair",
    "got",
    "histogram",
    "say",
    "haste",
    "see",
    "fair",
    "size",
    "zero",
    "hundred",
    "let",
    "add",
    "bin",
    "size",
    "make",
    "clear",
    "say",
    "ben",
    "equals",
    "let",
    "say",
    "20",
    "increase",
    "figure",
    "size",
    "well",
    "say",
    "fixed",
    "size",
    "let",
    "say",
    "give",
    "dimensions",
    "10",
    "bins",
    "clear",
    "next",
    "analyzed",
    "columns",
    "well",
    "type",
    "titanic",
    "data",
    "want",
    "information",
    "columns",
    "left",
    "passenger",
    "id",
    "guess",
    "use",
    "see",
    "many",
    "passengers",
    "survived",
    "many",
    "also",
    "see",
    "analysis",
    "gender",
    "basis",
    "saw",
    "female",
    "tend",
    "survive",
    "maintain",
    "survive",
    "saw",
    "passenger",
    "class",
    "passenger",
    "traveling",
    "first",
    "class",
    "second",
    "class",
    "third",
    "class",
    "name",
    "name",
    "analysis",
    "saw",
    "sex",
    "saw",
    "age",
    "well",
    "sea",
    "bass",
    "stands",
    "number",
    "siblings",
    "spouses",
    "aboard",
    "titanic",
    "let",
    "us",
    "well",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "mentioned",
    "x",
    "sc",
    "sp",
    "using",
    "titanic",
    "data",
    "see",
    "plot",
    "conclude",
    "maximum",
    "value",
    "zero",
    "conclude",
    "neither",
    "children",
    "spouse",
    "board",
    "titanic",
    "second",
    "highest",
    "value",
    "1",
    "various",
    "values",
    "2",
    "3",
    "4",
    "next",
    "go",
    "store",
    "column",
    "well",
    "similarly",
    "four",
    "parts",
    "next",
    "part",
    "see",
    "number",
    "parents",
    "children",
    "aboard",
    "titanic",
    "similarly",
    "well",
    "ticket",
    "number",
    "think",
    "analysis",
    "required",
    "ticket",
    "fears",
    "already",
    "discussed",
    "people",
    "would",
    "tend",
    "travel",
    "first",
    "class",
    "highest",
    "view",
    "cable",
    "number",
    "embarked",
    "columns",
    "data",
    "wrangling",
    "analyzed",
    "data",
    "seen",
    "quite",
    "graphs",
    "conclude",
    "variable",
    "better",
    "another",
    "relationship",
    "whole",
    "third",
    "step",
    "data",
    "wrangling",
    "data",
    "wrangling",
    "basically",
    "means",
    "cleaning",
    "data",
    "large",
    "data",
    "set",
    "might",
    "null",
    "values",
    "say",
    "nan",
    "values",
    "important",
    "remove",
    "unnecessary",
    "items",
    "present",
    "data",
    "set",
    "removing",
    "directly",
    "affects",
    "accuracy",
    "go",
    "ahead",
    "clean",
    "data",
    "removing",
    "n",
    "n",
    "values",
    "unnecessary",
    "columns",
    "null",
    "value",
    "data",
    "set",
    "next",
    "time",
    "performing",
    "data",
    "wrangling",
    "supposed",
    "fall",
    "check",
    "whether",
    "data",
    "set",
    "null",
    "say",
    "titanic",
    "data",
    "name",
    "data",
    "set",
    "say",
    "null",
    "basically",
    "tell",
    "values",
    "null",
    "return",
    "boolean",
    "result",
    "basically",
    "checks",
    "missing",
    "data",
    "result",
    "boolean",
    "format",
    "result",
    "true",
    "false",
    "falls",
    "mean",
    "null",
    "prove",
    "means",
    "null",
    "let",
    "run",
    "see",
    "values",
    "false",
    "true",
    "falls",
    "value",
    "null",
    "drew",
    "value",
    "none",
    "see",
    "cabin",
    "column",
    "first",
    "value",
    "null",
    "something",
    "see",
    "large",
    "data",
    "set",
    "counting",
    "stop",
    "actually",
    "see",
    "actually",
    "print",
    "number",
    "passengers",
    "nan",
    "value",
    "column",
    "say",
    "titanic",
    "underscore",
    "data",
    "null",
    "want",
    "sum",
    "thought",
    "basically",
    "print",
    "number",
    "passengers",
    "n",
    "n",
    "values",
    "column",
    "see",
    "missing",
    "values",
    "column",
    "maximum",
    "value",
    "cave",
    "column",
    "less",
    "embark",
    "column",
    "2",
    "want",
    "see",
    "numbers",
    "also",
    "plot",
    "heat",
    "map",
    "visually",
    "analyze",
    "let",
    "well",
    "say",
    "snsd",
    "heat",
    "map",
    "save",
    "take",
    "labels",
    "false",
    "choice",
    "run",
    "already",
    "seen",
    "three",
    "columns",
    "missing",
    "data",
    "value",
    "present",
    "might",
    "age",
    "almost",
    "20",
    "column",
    "missing",
    "value",
    "cabling",
    "columns",
    "quite",
    "large",
    "value",
    "two",
    "values",
    "embark",
    "column",
    "well",
    "add",
    "see",
    "map",
    "color",
    "coding",
    "say",
    "see",
    "map",
    "graph",
    "becomes",
    "attractive",
    "yellow",
    "stands",
    "drew",
    "say",
    "values",
    "null",
    "computed",
    "missing",
    "value",
    "lot",
    "missing",
    "values",
    "cabin",
    "column",
    "less",
    "value",
    "even",
    "visible",
    "embark",
    "column",
    "well",
    "remove",
    "missing",
    "values",
    "either",
    "replace",
    "values",
    "put",
    "dummy",
    "values",
    "simply",
    "drop",
    "column",
    "let",
    "us",
    "suppose",
    "pick",
    "age",
    "column",
    "first",
    "let",
    "plot",
    "box",
    "plot",
    "analyze",
    "column",
    "say",
    "sns",
    "dot",
    "box",
    "plot",
    "say",
    "x",
    "equals",
    "passenger",
    "class",
    "p",
    "class",
    "say",
    "equal",
    "h",
    "data",
    "set",
    "using",
    "titanic",
    "side",
    "say",
    "three",
    "times",
    "goes",
    "titanic",
    "data",
    "see",
    "edge",
    "first",
    "class",
    "second",
    "class",
    "tends",
    "older",
    "rather",
    "third",
    "class",
    "well",
    "depends",
    "experience",
    "much",
    "earn",
    "might",
    "number",
    "reasons",
    "concluded",
    "passengers",
    "traveling",
    "class",
    "one",
    "class",
    "two",
    "tend",
    "older",
    "class",
    "3",
    "found",
    "missing",
    "values",
    "em",
    "one",
    "way",
    "either",
    "drop",
    "column",
    "simply",
    "fill",
    "values",
    "method",
    "called",
    "imputation",
    "perform",
    "data",
    "wrangling",
    "cleaning",
    "spring",
    "head",
    "data",
    "set",
    "say",
    "tightening",
    "knot",
    "head",
    "titanic",
    "data",
    "let",
    "say",
    "want",
    "five",
    "rows",
    "survived",
    "categorical",
    "particular",
    "column",
    "apply",
    "logic",
    "progression",
    "value",
    "value",
    "need",
    "predict",
    "passenger",
    "class",
    "name",
    "ticket",
    "number",
    "taping",
    "seen",
    "keeping",
    "lot",
    "null",
    "values",
    "say",
    "invalid",
    "quite",
    "visible",
    "well",
    "first",
    "drop",
    "column",
    "dropping",
    "say",
    "titanic",
    "underscore",
    "data",
    "simply",
    "type",
    "drop",
    "column",
    "need",
    "draw",
    "drop",
    "cable",
    "column",
    "mention",
    "access",
    "equals",
    "1",
    "say",
    "place",
    "also",
    "true",
    "print",
    "head",
    "let",
    "us",
    "see",
    "whether",
    "column",
    "removed",
    "data",
    "set",
    "say",
    "titanic",
    "dot",
    "head",
    "see",
    "given",
    "column",
    "anymore",
    "also",
    "drop",
    "na",
    "values",
    "say",
    "titanic",
    "data",
    "dot",
    "drop",
    "values",
    "say",
    "nan",
    "number",
    "say",
    "place",
    "equal",
    "true",
    "titanic",
    "let",
    "plot",
    "heat",
    "map",
    "let",
    "say",
    "values",
    "showing",
    "lot",
    "null",
    "values",
    "removed",
    "say",
    "sns",
    "dot",
    "heat",
    "map",
    "pass",
    "data",
    "set",
    "check",
    "null",
    "say",
    "tick",
    "labels",
    "equal",
    "false",
    "want",
    "color",
    "coding",
    "say",
    "false",
    "basically",
    "help",
    "check",
    "whether",
    "values",
    "removed",
    "data",
    "set",
    "see",
    "null",
    "values",
    "entirely",
    "black",
    "actually",
    "know",
    "well",
    "go",
    "copy",
    "part",
    "use",
    "sum",
    "function",
    "calculate",
    "sum",
    "tells",
    "data",
    "set",
    "clean",
    "data",
    "set",
    "contain",
    "null",
    "value",
    "nan",
    "value",
    "r",
    "angela",
    "data",
    "see",
    "cleaner",
    "data",
    "done",
    "one",
    "step",
    "data",
    "wrangling",
    "removing",
    "one",
    "column",
    "lot",
    "things",
    "actually",
    "fill",
    "values",
    "values",
    "calculate",
    "mean",
    "fit",
    "null",
    "values",
    "see",
    "data",
    "set",
    "say",
    "titanic",
    "data",
    "dot",
    "head",
    "see",
    "lot",
    "string",
    "values",
    "converted",
    "categorical",
    "variables",
    "order",
    "implement",
    "logistic",
    "regression",
    "convert",
    "categorical",
    "variable",
    "dummy",
    "variables",
    "done",
    "using",
    "pandas",
    "logistic",
    "regression",
    "take",
    "two",
    "values",
    "whenever",
    "apply",
    "machine",
    "learning",
    "need",
    "make",
    "sure",
    "string",
    "values",
    "present",
    "wo",
    "taking",
    "input",
    "variables",
    "using",
    "string",
    "predict",
    "anything",
    "case",
    "survived",
    "columns",
    "2210",
    "many",
    "people",
    "tend",
    "survive",
    "many",
    "ceo",
    "stands",
    "survive",
    "one",
    "stands",
    "survive",
    "let",
    "convert",
    "variables",
    "dummy",
    "variables",
    "use",
    "pandas",
    "say",
    "pd",
    "get",
    "dummies",
    "simply",
    "press",
    "tab",
    "autocomplete",
    "say",
    "titanic",
    "data",
    "pass",
    "six",
    "simply",
    "click",
    "shift",
    "tab",
    "get",
    "information",
    "type",
    "data",
    "frame",
    "passenger",
    "id",
    "survived",
    "passenger",
    "class",
    "run",
    "see",
    "0",
    "basically",
    "stands",
    "female",
    "one",
    "stand",
    "female",
    "similarly",
    "male",
    "0",
    "stanford",
    "made",
    "one",
    "stanford",
    "may",
    "require",
    "columns",
    "one",
    "column",
    "enough",
    "tell",
    "us",
    "whether",
    "male",
    "say",
    "female",
    "let",
    "say",
    "want",
    "keep",
    "male",
    "say",
    "value",
    "mail",
    "1",
    "definitely",
    "maid",
    "female",
    "need",
    "values",
    "remove",
    "first",
    "column",
    "let",
    "say",
    "female",
    "say",
    "drop",
    "first",
    "andrew",
    "given",
    "one",
    "column",
    "male",
    "value",
    "0",
    "let",
    "set",
    "variable",
    "hsx",
    "say",
    "sex",
    "dot",
    "head",
    "want",
    "see",
    "first",
    "five",
    "rows",
    "sorry",
    "dot",
    "data",
    "looks",
    "like",
    "done",
    "sex",
    "numerical",
    "values",
    "age",
    "numerical",
    "values",
    "spouses",
    "ticket",
    "number",
    "pair",
    "embarked",
    "well",
    "embark",
    "values",
    "c",
    "q",
    "also",
    "apply",
    "get",
    "dummy",
    "function",
    "let",
    "say",
    "take",
    "variable",
    "let",
    "say",
    "embark",
    "use",
    "pandas",
    "library",
    "enter",
    "column",
    "name",
    "embarked",
    "let",
    "print",
    "head",
    "say",
    "embark",
    "dot",
    "head",
    "c",
    "q",
    "also",
    "drop",
    "first",
    "column",
    "two",
    "values",
    "enough",
    "passenger",
    "either",
    "traveling",
    "q",
    "stone",
    "s4",
    "sound",
    "time",
    "values",
    "0",
    "definitely",
    "passenger",
    "cherbourg",
    "third",
    "value",
    "drop",
    "first",
    "value",
    "say",
    "drop",
    "true",
    "let",
    "run",
    "output",
    "looks",
    "like",
    "similarly",
    "class",
    "well",
    "also",
    "three",
    "classes",
    "one",
    "two",
    "three",
    "copy",
    "whole",
    "statement",
    "let",
    "say",
    "want",
    "variable",
    "name",
    "let",
    "say",
    "pcl",
    "pass",
    "column",
    "name",
    "pe",
    "class",
    "drop",
    "first",
    "column",
    "also",
    "values",
    "1",
    "2",
    "3",
    "remove",
    "first",
    "column",
    "left",
    "two",
    "three",
    "values",
    "0",
    "definitely",
    "passengers",
    "travelling",
    "first",
    "class",
    "made",
    "values",
    "categorical",
    "next",
    "step",
    "would",
    "concatenate",
    "new",
    "rules",
    "data",
    "set",
    "see",
    "titanic",
    "data",
    "using",
    "pandas",
    "concatenate",
    "columns",
    "superior",
    "one",
    "cat",
    "say",
    "concatenate",
    "sex",
    "concatenate",
    "embarked",
    "pcl",
    "mention",
    "access",
    "one",
    "run",
    "print",
    "head",
    "see",
    "columns",
    "added",
    "mail",
    "column",
    "basically",
    "tells",
    "person",
    "male",
    "female",
    "embark",
    "basically",
    "q",
    "traveling",
    "queenstown",
    "value",
    "would",
    "one",
    "else",
    "would",
    "0",
    "values",
    "zeroed",
    "definitely",
    "traveling",
    "cherbourg",
    "passenger",
    "class",
    "2",
    "value",
    "0",
    "passengers",
    "travelling",
    "class",
    "one",
    "hope",
    "got",
    "till",
    "irrelevant",
    "columns",
    "drop",
    "columns",
    "drop",
    "pe",
    "class",
    "embarked",
    "column",
    "sex",
    "column",
    "type",
    "titanic",
    "data",
    "dot",
    "drop",
    "mention",
    "columns",
    "want",
    "drop",
    "say",
    "even",
    "read",
    "passenger",
    "id",
    "nothing",
    "index",
    "value",
    "starting",
    "one",
    "drop",
    "well",
    "want",
    "name",
    "well",
    "delete",
    "name",
    "well",
    "else",
    "drop",
    "drop",
    "ticket",
    "well",
    "mention",
    "axis",
    "say",
    "place",
    "equal",
    "true",
    "okay",
    "column",
    "name",
    "starts",
    "uppercase",
    "dropped",
    "let",
    "bring",
    "data",
    "set",
    "final",
    "leader",
    "said",
    "guys",
    "survived",
    "column",
    "value",
    "0",
    "1",
    "passenger",
    "class",
    "forgot",
    "drop",
    "well",
    "worries",
    "drop",
    "let",
    "run",
    "survive",
    "age",
    "sp",
    "part",
    "fair",
    "mail",
    "converted",
    "performed",
    "data",
    "angle",
    "see",
    "clean",
    "data",
    "converted",
    "values",
    "gender",
    "male",
    "embarked",
    "q",
    "passenger",
    "class",
    "2",
    "2",
    "data",
    "wrangling",
    "cleaning",
    "data",
    "next",
    "training",
    "testing",
    "data",
    "split",
    "data",
    "set",
    "train",
    "subset",
    "test",
    "steps",
    "build",
    "model",
    "train",
    "data",
    "predict",
    "output",
    "test",
    "data",
    "set",
    "let",
    "go",
    "back",
    "jupiter",
    "implement",
    "well",
    "need",
    "train",
    "data",
    "set",
    "put",
    "indeed",
    "heading",
    "need",
    "define",
    "dependent",
    "variable",
    "independent",
    "variable",
    "output",
    "say",
    "value",
    "need",
    "predict",
    "write",
    "titanic",
    "data",
    "take",
    "column",
    "survive",
    "basically",
    "predict",
    "column",
    "whether",
    "passenger",
    "survived",
    "see",
    "discrete",
    "outcome",
    "form",
    "0",
    "1",
    "rest",
    "things",
    "take",
    "features",
    "say",
    "independent",
    "variable",
    "say",
    "titanic",
    "data",
    "drop",
    "simply",
    "drop",
    "survive",
    "columns",
    "independent",
    "variable",
    "everything",
    "else",
    "features",
    "leads",
    "survival",
    "rate",
    "defined",
    "independent",
    "variable",
    "dependent",
    "variable",
    "next",
    "step",
    "split",
    "data",
    "training",
    "testing",
    "subset",
    "using",
    "sk",
    "loan",
    "type",
    "sklearn",
    "dot",
    "cross",
    "validation",
    "import",
    "train",
    "test",
    "plate",
    "click",
    "shift",
    "tab",
    "go",
    "documentation",
    "see",
    "examples",
    "second",
    "class",
    "open",
    "go",
    "examples",
    "see",
    "split",
    "data",
    "extra",
    "next",
    "test",
    "wide",
    "range",
    "test",
    "using",
    "train",
    "test",
    "platelet",
    "passing",
    "independent",
    "variable",
    "dependent",
    "variable",
    "define",
    "size",
    "random",
    "straight",
    "let",
    "copy",
    "paste",
    "train",
    "test",
    "dependent",
    "variable",
    "train",
    "test",
    "using",
    "split",
    "function",
    "pass",
    "independent",
    "dependent",
    "variable",
    "set",
    "split",
    "size",
    "let",
    "say",
    "put",
    "basically",
    "means",
    "data",
    "set",
    "divided",
    "ratio",
    "add",
    "random",
    "straight",
    "let",
    "say",
    "applying",
    "one",
    "necessary",
    "want",
    "result",
    "mine",
    "add",
    "random",
    "shape",
    "basically",
    "take",
    "exactly",
    "sample",
    "every",
    "next",
    "train",
    "predict",
    "creating",
    "model",
    "logistic",
    "regression",
    "graph",
    "linear",
    "regression",
    "next",
    "type",
    "sk",
    "loan",
    "dot",
    "linear",
    "model",
    "import",
    "logistic",
    "regression",
    "next",
    "create",
    "instance",
    "logistic",
    "regression",
    "model",
    "say",
    "log",
    "model",
    "equals",
    "largest",
    "aggression",
    "need",
    "fit",
    "model",
    "say",
    "log",
    "model",
    "dot",
    "fit",
    "pass",
    "ex",
    "train",
    "white",
    "rain",
    "alright",
    "gives",
    "details",
    "logistic",
    "regression",
    "gives",
    "class",
    "way",
    "dual",
    "fit",
    "intercept",
    "things",
    "need",
    "need",
    "make",
    "prediction",
    "take",
    "variable",
    "checked",
    "addictions",
    "pass",
    "model",
    "say",
    "log",
    "model",
    "dot",
    "predict",
    "pass",
    "value",
    "x",
    "test",
    "created",
    "model",
    "fit",
    "model",
    "made",
    "predictions",
    "evaluate",
    "model",
    "performing",
    "simply",
    "calculate",
    "accuracy",
    "also",
    "calculate",
    "classification",
    "report",
    "worry",
    "guys",
    "showing",
    "methods",
    "say",
    "sklearn",
    "dot",
    "matrix",
    "input",
    "classification",
    "report",
    "used",
    "fiction",
    "report",
    "inside",
    "passing",
    "white",
    "test",
    "predictions",
    "guys",
    "classification",
    "report",
    "precision",
    "recall",
    "advanced",
    "code",
    "support",
    "value",
    "decision",
    "75",
    "72",
    "73",
    "bad",
    "order",
    "calculate",
    "accuracy",
    "well",
    "also",
    "use",
    "concept",
    "confusion",
    "matrix",
    "want",
    "print",
    "confusion",
    "matrix",
    "simply",
    "say",
    "sklearn",
    "dot",
    "matrix",
    "import",
    "confusion",
    "matrix",
    "first",
    "print",
    "function",
    "imported",
    "successfully",
    "say",
    "confusion",
    "matrix",
    "passing",
    "variables",
    "test",
    "predictions",
    "hope",
    "guys",
    "already",
    "know",
    "concept",
    "confusion",
    "matrix",
    "tell",
    "brief",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "nothing",
    "2",
    "2",
    "matrix",
    "four",
    "outcomes",
    "basically",
    "tells",
    "us",
    "accurate",
    "values",
    "column",
    "predicted",
    "predicted",
    "actual",
    "actually",
    "yes",
    "concept",
    "confusion",
    "matrix",
    "let",
    "fade",
    "values",
    "calculated",
    "105",
    "105",
    "2125",
    "63",
    "see",
    "got",
    "four",
    "outcomes",
    "105",
    "value",
    "model",
    "predicted",
    "reality",
    "also",
    "predicted",
    "know",
    "actual",
    "know",
    "similarly",
    "63",
    "predicted",
    "yes",
    "model",
    "predicted",
    "yes",
    "actually",
    "also",
    "yes",
    "order",
    "calculate",
    "accuracy",
    "need",
    "add",
    "sum",
    "two",
    "values",
    "divide",
    "whole",
    "two",
    "values",
    "tells",
    "order",
    "actually",
    "predicted",
    "correct",
    "output",
    "value",
    "also",
    "called",
    "called",
    "false",
    "positive",
    "called",
    "true",
    "positive",
    "called",
    "false",
    "negative",
    "order",
    "calculate",
    "accuracy",
    "manually",
    "python",
    "import",
    "accuracy",
    "score",
    "function",
    "get",
    "results",
    "well",
    "say",
    "sklearn",
    "import",
    "accuracy",
    "score",
    "simply",
    "print",
    "accuracy",
    "pass",
    "variables",
    "predictions",
    "tells",
    "address",
    "78",
    "quite",
    "good",
    "want",
    "manually",
    "2",
    "plus",
    "two",
    "numbers",
    "105",
    "comes",
    "almost",
    "168",
    "divide",
    "sum",
    "phone",
    "numbers",
    "105",
    "plus",
    "63",
    "plus",
    "21",
    "plus",
    "25",
    "gives",
    "result",
    "divide",
    "two",
    "number",
    "get",
    "accuracy",
    "78",
    "percent",
    "say",
    "point",
    "seven",
    "eight",
    "calculate",
    "see",
    "let",
    "go",
    "back",
    "presentation",
    "let",
    "see",
    "covered",
    "till",
    "first",
    "plate",
    "data",
    "train",
    "test",
    "subset",
    "build",
    "model",
    "train",
    "data",
    "predicted",
    "output",
    "test",
    "data",
    "set",
    "fifth",
    "step",
    "check",
    "accuracy",
    "calculator",
    "accuracy",
    "almost",
    "78",
    "percent",
    "quite",
    "good",
    "say",
    "accuracy",
    "bad",
    "tells",
    "accurate",
    "results",
    "accuracy",
    "score",
    "defines",
    "hence",
    "got",
    "good",
    "accuracy",
    "moving",
    "ahead",
    "let",
    "us",
    "see",
    "second",
    "project",
    "suv",
    "data",
    "analysis",
    "car",
    "company",
    "released",
    "new",
    "suv",
    "market",
    "using",
    "previous",
    "data",
    "sales",
    "suv",
    "want",
    "predict",
    "category",
    "people",
    "might",
    "interested",
    "buying",
    "using",
    "logistic",
    "regression",
    "need",
    "find",
    "factors",
    "made",
    "people",
    "interested",
    "buying",
    "suv",
    "let",
    "us",
    "hear",
    "data",
    "set",
    "user",
    "id",
    "gender",
    "male",
    "female",
    "age",
    "estimated",
    "melody",
    "purchased",
    "column",
    "discreet",
    "column",
    "see",
    "categorical",
    "column",
    "value",
    "0",
    "1",
    "column",
    "need",
    "predict",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "based",
    "factors",
    "deciding",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "know",
    "salary",
    "person",
    "know",
    "age",
    "using",
    "predict",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "let",
    "go",
    "jupyter",
    "notebook",
    "implement",
    "logistic",
    "regression",
    "guys",
    "going",
    "details",
    "data",
    "cleaning",
    "analyzing",
    "part",
    "start",
    "part",
    "leave",
    "go",
    "ahead",
    "practice",
    "much",
    "alright",
    "second",
    "project",
    "suv",
    "predictions",
    "first",
    "import",
    "libraries",
    "say",
    "import",
    "numpy",
    "np",
    "similarly",
    "rest",
    "alright",
    "let",
    "print",
    "head",
    "data",
    "set",
    "already",
    "seen",
    "columns",
    "user",
    "id",
    "gender",
    "h",
    "salary",
    "calculate",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "let",
    "us",
    "simply",
    "go",
    "algorithm",
    "part",
    "directly",
    "start",
    "logistic",
    "regression",
    "train",
    "model",
    "things",
    "first",
    "need",
    "define",
    "independent",
    "variable",
    "dependent",
    "variable",
    "case",
    "want",
    "ex",
    "independent",
    "variable",
    "data",
    "set",
    "lock",
    "specifying",
    "school",
    "basically",
    "stands",
    "columns",
    "want",
    "two",
    "three",
    "dot",
    "values",
    "fetch",
    "rows",
    "second",
    "third",
    "column",
    "age",
    "estimated",
    "salary",
    "factors",
    "used",
    "predict",
    "dependent",
    "variable",
    "purchase",
    "dependent",
    "variable",
    "purchase",
    "independent",
    "variable",
    "age",
    "salary",
    "say",
    "lena",
    "said",
    "dot",
    "love",
    "rows",
    "add",
    "one",
    "fourth",
    "column",
    "purchased",
    "column",
    "values",
    "right",
    "forgot",
    "one",
    "square",
    "bracket",
    "alright",
    "defined",
    "independent",
    "variable",
    "dependent",
    "variable",
    "independent",
    "variable",
    "age",
    "salary",
    "dependent",
    "variable",
    "column",
    "purchase",
    "must",
    "wondering",
    "lock",
    "function",
    "look",
    "function",
    "basically",
    "index",
    "panda",
    "data",
    "frame",
    "used",
    "integer",
    "based",
    "indexing",
    "also",
    "say",
    "selection",
    "index",
    "let",
    "bring",
    "independent",
    "variables",
    "dependent",
    "variable",
    "bring",
    "independent",
    "variable",
    "age",
    "well",
    "salary",
    "next",
    "let",
    "print",
    "dependent",
    "variable",
    "well",
    "see",
    "values",
    "0",
    "1",
    "0",
    "stands",
    "purchase",
    "next",
    "let",
    "divide",
    "data",
    "set",
    "training",
    "test",
    "subset",
    "simply",
    "write",
    "sklearn",
    "dot",
    "cross",
    "plate",
    "import",
    "drain",
    "test",
    "next",
    "press",
    "shift",
    "tab",
    "go",
    "examples",
    "copy",
    "line",
    "copy",
    "move",
    "points",
    "want",
    "text",
    "size",
    "let",
    "see",
    "25",
    "divided",
    "train",
    "tested",
    "ratio",
    "let",
    "say",
    "take",
    "random",
    "set",
    "0",
    "random",
    "state",
    "basically",
    "ensures",
    "result",
    "say",
    "samples",
    "taken",
    "whenever",
    "run",
    "code",
    "let",
    "run",
    "also",
    "scale",
    "input",
    "values",
    "better",
    "performing",
    "done",
    "using",
    "standard",
    "scalar",
    "let",
    "well",
    "say",
    "sklearn",
    "dot",
    "import",
    "standard",
    "scale",
    "scale",
    "see",
    "data",
    "set",
    "dealing",
    "large",
    "numbers",
    "well",
    "although",
    "using",
    "small",
    "data",
    "set",
    "whenever",
    "working",
    "prod",
    "environment",
    "working",
    "large",
    "data",
    "set",
    "using",
    "thousands",
    "hundred",
    "thousands",
    "pulls",
    "scaling",
    "definitely",
    "affect",
    "performance",
    "large",
    "extent",
    "let",
    "show",
    "scale",
    "input",
    "values",
    "contains",
    "methods",
    "functionality",
    "required",
    "transform",
    "data",
    "let",
    "us",
    "scale",
    "test",
    "well",
    "training",
    "data",
    "set",
    "else",
    "first",
    "make",
    "instance",
    "say",
    "standard",
    "scalar",
    "extreme",
    "sasc",
    "dot",
    "fit",
    "fit",
    "underscore",
    "transform",
    "pass",
    "xtreme",
    "video",
    "similarly",
    "test",
    "wherein",
    "pass",
    "x",
    "test",
    "right",
    "next",
    "step",
    "import",
    "logistic",
    "regression",
    "simply",
    "apply",
    "logistic",
    "regression",
    "first",
    "importing",
    "say",
    "sklearn",
    "sklearn",
    "linear",
    "model",
    "import",
    "logistic",
    "regression",
    "using",
    "classifier",
    "said",
    "classifier",
    "dot",
    "equals",
    "logistically",
    "aggression",
    "make",
    "instance",
    "say",
    "logistic",
    "regression",
    "pass",
    "random",
    "state",
    "0",
    "simply",
    "fit",
    "model",
    "simply",
    "passing",
    "next",
    "rain",
    "white",
    "rain",
    "tells",
    "details",
    "logistic",
    "regression",
    "predict",
    "value",
    "say",
    "prayed",
    "equal",
    "classifier",
    "predict",
    "function",
    "pass",
    "x",
    "test",
    "created",
    "model",
    "scaled",
    "input",
    "values",
    "applied",
    "logistic",
    "regression",
    "predicted",
    "values",
    "want",
    "know",
    "accuracy",
    "accuracy",
    "first",
    "need",
    "import",
    "accuracy",
    "scores",
    "say",
    "sklearn",
    "dot",
    "matrix",
    "input",
    "accuracy",
    "school",
    "using",
    "function",
    "calculate",
    "accuracy",
    "manually",
    "creating",
    "confusion",
    "matrix",
    "pass",
    "lightest",
    "predicted",
    "right",
    "get",
    "accuracy",
    "89",
    "want",
    "know",
    "accuracy",
    "percentage",
    "multiply",
    "hundred",
    "run",
    "gives",
    "89",
    "hope",
    "guys",
    "clear",
    "whatever",
    "taught",
    "today",
    "taken",
    "independent",
    "variable",
    "age",
    "salary",
    "calculated",
    "many",
    "people",
    "purchase",
    "suv",
    "calculated",
    "model",
    "checking",
    "accuracy",
    "get",
    "accuracies",
    "89",
    "great",
    "alright",
    "guys",
    "today",
    "scoffs",
    "covered",
    "today",
    "training",
    "first",
    "quick",
    "introduction",
    "regression",
    "regression",
    "actually",
    "use",
    "understood",
    "types",
    "regression",
    "got",
    "details",
    "logistic",
    "regression",
    "compared",
    "linear",
    "logistic",
    "regression",
    "also",
    "seen",
    "various",
    "use",
    "cases",
    "implement",
    "logistic",
    "regression",
    "real",
    "life",
    "picked",
    "two",
    "projects",
    "titanic",
    "data",
    "analysis",
    "suv",
    "prediction",
    "seen",
    "collect",
    "data",
    "analyze",
    "data",
    "perform",
    "modeling",
    "data",
    "train",
    "data",
    "test",
    "data",
    "finally",
    "calculated",
    "accuracy",
    "suv",
    "prediction",
    "actually",
    "analyze",
    "clean",
    "data",
    "lot",
    "things",
    "go",
    "ahead",
    "pick",
    "data",
    "set",
    "explore",
    "much",
    "open",
    "eyes",
    "see",
    "around",
    "find",
    "dozens",
    "applications",
    "machine",
    "learning",
    "using",
    "interacting",
    "daily",
    "life",
    "peed",
    "using",
    "phase",
    "detection",
    "facebook",
    "getting",
    "recommendation",
    "similar",
    "products",
    "amazon",
    "machine",
    "learning",
    "applied",
    "almost",
    "everywhere",
    "hello",
    "welcome",
    "youtube",
    "session",
    "learn",
    "build",
    "decision",
    "tree",
    "session",
    "designed",
    "way",
    "get",
    "alright",
    "decision",
    "tree",
    "type",
    "classification",
    "algorithm",
    "comes",
    "supervised",
    "learning",
    "technique",
    "learning",
    "decision",
    "tree",
    "give",
    "short",
    "introduction",
    "classification",
    "learn",
    "classification",
    "say",
    "various",
    "types",
    "used",
    "see",
    "use",
    "cases",
    "get",
    "fundamental",
    "clear",
    "jump",
    "decision",
    "tree",
    "part",
    "first",
    "teach",
    "mathematically",
    "create",
    "decision",
    "tree",
    "scratch",
    "get",
    "concepts",
    "clear",
    "see",
    "write",
    "decision",
    "tree",
    "classifier",
    "scratch",
    "python",
    "using",
    "card",
    "algorithm",
    "right",
    "hope",
    "agenda",
    "scared",
    "guys",
    "classification",
    "hope",
    "every",
    "one",
    "must",
    "used",
    "gmail",
    "think",
    "male",
    "getting",
    "classified",
    "spam",
    "spam",
    "mail",
    "well",
    "nothing",
    "classification",
    "well",
    "classification",
    "process",
    "dividing",
    "data",
    "set",
    "different",
    "categories",
    "groups",
    "adding",
    "label",
    "way",
    "say",
    "technique",
    "categorizing",
    "observation",
    "different",
    "category",
    "basically",
    "taking",
    "data",
    "analyzing",
    "basis",
    "condition",
    "finely",
    "divided",
    "various",
    "categories",
    "classify",
    "well",
    "classify",
    "perform",
    "predictive",
    "analysis",
    "like",
    "get",
    "mail",
    "machine",
    "predicts",
    "spam",
    "spam",
    "mail",
    "basis",
    "prediction",
    "add",
    "irrelevant",
    "spam",
    "mail",
    "respective",
    "folder",
    "general",
    "classification",
    "algorithm",
    "handle",
    "questions",
    "like",
    "data",
    "belongs",
    "category",
    "b",
    "category",
    "like",
    "male",
    "female",
    "something",
    "like",
    "question",
    "arises",
    "use",
    "well",
    "use",
    "protection",
    "order",
    "check",
    "whether",
    "transaction",
    "genuine",
    "suppose",
    "using",
    "credit",
    "card",
    "india",
    "due",
    "reason",
    "fly",
    "dubai",
    "using",
    "credit",
    "card",
    "get",
    "notification",
    "alert",
    "regarding",
    "transaction",
    "would",
    "ask",
    "confirm",
    "transaction",
    "also",
    "kind",
    "predictive",
    "analysis",
    "machine",
    "predicts",
    "something",
    "fishy",
    "transaction",
    "ago",
    "made",
    "transaction",
    "using",
    "credit",
    "card",
    "india",
    "24",
    "hour",
    "later",
    "credit",
    "card",
    "used",
    "payment",
    "dubai",
    "machine",
    "predicts",
    "something",
    "fishy",
    "going",
    "transaction",
    "order",
    "confirm",
    "sends",
    "notification",
    "alert",
    "right",
    "well",
    "one",
    "use",
    "case",
    "classification",
    "even",
    "use",
    "classify",
    "different",
    "items",
    "like",
    "fruits",
    "base",
    "taste",
    "color",
    "size",
    "overweight",
    "machine",
    "well",
    "trained",
    "using",
    "classification",
    "algorithm",
    "easily",
    "predict",
    "class",
    "type",
    "fruit",
    "whenever",
    "new",
    "data",
    "given",
    "fruit",
    "item",
    "car",
    "house",
    "bored",
    "anything",
    "noticed",
    "visit",
    "sites",
    "try",
    "login",
    "get",
    "picture",
    "capture",
    "right",
    "identify",
    "whether",
    "given",
    "image",
    "car",
    "pole",
    "select",
    "example",
    "10",
    "images",
    "selecting",
    "three",
    "mages",
    "way",
    "training",
    "machine",
    "right",
    "telling",
    "three",
    "picture",
    "car",
    "rest",
    "knows",
    "training",
    "something",
    "big",
    "right",
    "moving",
    "ahead",
    "let",
    "discuss",
    "types",
    "classification",
    "online",
    "well",
    "several",
    "different",
    "ways",
    "perform",
    "tasks",
    "like",
    "order",
    "predict",
    "whether",
    "given",
    "person",
    "male",
    "female",
    "machine",
    "trained",
    "first",
    "right",
    "multiple",
    "ways",
    "train",
    "machine",
    "choose",
    "one",
    "predictive",
    "analytics",
    "many",
    "different",
    "techniques",
    "common",
    "decision",
    "tree",
    "cover",
    "depth",
    "today",
    "session",
    "part",
    "classification",
    "algorithm",
    "decision",
    "tree",
    "random",
    "forest",
    "name",
    "buys",
    "neighbor",
    "logistic",
    "regression",
    "linear",
    "regression",
    "support",
    "vector",
    "machines",
    "many",
    "alright",
    "let",
    "give",
    "idea",
    "starting",
    "decision",
    "tree",
    "well",
    "decision",
    "tree",
    "graphical",
    "representation",
    "possible",
    "solution",
    "decision",
    "decisions",
    "made",
    "explained",
    "easily",
    "example",
    "task",
    "says",
    "go",
    "restaurant",
    "buy",
    "hamburger",
    "confused",
    "create",
    "dish",
    "entry",
    "starting",
    "root",
    "node",
    "first",
    "check",
    "whether",
    "hungry",
    "right",
    "hungry",
    "go",
    "back",
    "sleep",
    "right",
    "hungry",
    "25",
    "decide",
    "go",
    "restaurant",
    "hungry",
    "25",
    "go",
    "buy",
    "hamburger",
    "right",
    "decision",
    "tree",
    "moving",
    "ahead",
    "let",
    "see",
    "random",
    "forest",
    "well",
    "random",
    "forest",
    "build",
    "multiple",
    "decision",
    "trees",
    "merges",
    "together",
    "get",
    "accurate",
    "stable",
    "production",
    "right",
    "time",
    "random",
    "forest",
    "trained",
    "bagging",
    "method",
    "bagging",
    "method",
    "based",
    "idea",
    "combination",
    "learning",
    "model",
    "increases",
    "overall",
    "result",
    "combining",
    "learning",
    "different",
    "models",
    "clubbing",
    "together",
    "increase",
    "overall",
    "result",
    "fine",
    "one",
    "thing",
    "size",
    "data",
    "set",
    "huge",
    "case",
    "one",
    "single",
    "decision",
    "tree",
    "would",
    "lead",
    "offutt",
    "model",
    "way",
    "like",
    "single",
    "person",
    "might",
    "perspective",
    "complete",
    "population",
    "population",
    "huge",
    "right",
    "however",
    "implement",
    "voting",
    "system",
    "ask",
    "different",
    "individual",
    "interpret",
    "data",
    "would",
    "able",
    "cover",
    "pattern",
    "much",
    "meticulous",
    "way",
    "even",
    "diagram",
    "see",
    "section",
    "howard",
    "large",
    "training",
    "data",
    "set",
    "first",
    "divide",
    "training",
    "data",
    "set",
    "n",
    "create",
    "decision",
    "tree",
    "cell",
    "sample",
    "b",
    "part",
    "take",
    "vote",
    "every",
    "decision",
    "made",
    "every",
    "decision",
    "tree",
    "finally",
    "club",
    "vote",
    "get",
    "random",
    "forest",
    "dition",
    "fine",
    "let",
    "move",
    "ahead",
    "next",
    "neighbor",
    "buys",
    "named",
    "classification",
    "technique",
    "based",
    "bayes",
    "theorem",
    "assumes",
    "particular",
    "feature",
    "class",
    "completely",
    "unrelated",
    "presence",
    "feature",
    "named",
    "buys",
    "simple",
    "algorithm",
    "due",
    "simplicity",
    "algorithm",
    "might",
    "perform",
    "complex",
    "model",
    "size",
    "data",
    "set",
    "large",
    "enough",
    "right",
    "classical",
    "use",
    "case",
    "name",
    "bias",
    "document",
    "classification",
    "determine",
    "whether",
    "given",
    "text",
    "corresponds",
    "one",
    "categories",
    "text",
    "case",
    "features",
    "used",
    "might",
    "presence",
    "absence",
    "absence",
    "keyword",
    "nev",
    "diagram",
    "see",
    "using",
    "neighbor",
    "buys",
    "decide",
    "whether",
    "disease",
    "first",
    "check",
    "probability",
    "disease",
    "disease",
    "right",
    "probability",
    "disease",
    "hand",
    "probability",
    "disease",
    "okay",
    "first",
    "let",
    "see",
    "disease",
    "go",
    "doctor",
    "right",
    "visited",
    "doctor",
    "test",
    "positive",
    "adjective",
    "probability",
    "positive",
    "test",
    "disease",
    "0",
    "probability",
    "negative",
    "test",
    "already",
    "disease",
    "also",
    "false",
    "negative",
    "statement",
    "test",
    "detecting",
    "negative",
    "still",
    "disease",
    "right",
    "false",
    "negative",
    "statement",
    "let",
    "move",
    "ahead",
    "disease",
    "probability",
    "disease",
    "visit",
    "doctor",
    "doctor",
    "like",
    "yes",
    "disease",
    "already",
    "know",
    "disease",
    "false",
    "positive",
    "statement",
    "probability",
    "disease",
    "actually",
    "know",
    "disease",
    "probability",
    "disease",
    "actually",
    "know",
    "disease",
    "probability",
    "around",
    "fine",
    "probability",
    "disease",
    "test",
    "showing",
    "results",
    "true",
    "positive",
    "statement",
    "right",
    "let",
    "move",
    "ahead",
    "discuss",
    "kn",
    "n",
    "algorithm",
    "knn",
    "algorithm",
    "neighbor",
    "stores",
    "available",
    "cases",
    "classifies",
    "new",
    "cases",
    "based",
    "similarity",
    "measure",
    "k",
    "knn",
    "algorithm",
    "nearest",
    "neighbor",
    "wish",
    "take",
    "vote",
    "example",
    "k",
    "equal",
    "1",
    "object",
    "simply",
    "assigned",
    "class",
    "single",
    "nearest",
    "neighbor",
    "diagram",
    "see",
    "difference",
    "image",
    "k",
    "equal",
    "1",
    "k",
    "equal",
    "3",
    "k",
    "equal",
    "5",
    "right",
    "well",
    "systems",
    "able",
    "use",
    "neighbor",
    "visual",
    "pattern",
    "recognization",
    "scan",
    "detect",
    "hidden",
    "packages",
    "bottom",
    "bin",
    "shopping",
    "cart",
    "checkout",
    "object",
    "detected",
    "matches",
    "exactly",
    "object",
    "listed",
    "database",
    "price",
    "spotted",
    "product",
    "could",
    "even",
    "automatically",
    "added",
    "customers",
    "bill",
    "automated",
    "billing",
    "practice",
    "used",
    "extensively",
    "time",
    "technology",
    "developed",
    "available",
    "use",
    "want",
    "use",
    "yeah",
    "one",
    "thing",
    "neighbor",
    "also",
    "used",
    "retail",
    "detect",
    "patterns",
    "credit",
    "card",
    "uses",
    "many",
    "new",
    "transaction",
    "scrutinizing",
    "software",
    "application",
    "use",
    "cayenne",
    "algorithms",
    "analyze",
    "register",
    "data",
    "spot",
    "unusual",
    "pattern",
    "indicates",
    "species",
    "activity",
    "example",
    "register",
    "data",
    "indicates",
    "lot",
    "customers",
    "information",
    "entered",
    "manually",
    "rather",
    "automated",
    "scanning",
    "swapping",
    "case",
    "could",
    "indicate",
    "employees",
    "using",
    "register",
    "fact",
    "stealing",
    "customers",
    "personal",
    "information",
    "register",
    "data",
    "indicates",
    "particular",
    "good",
    "returned",
    "exchanged",
    "multiple",
    "times",
    "could",
    "indicate",
    "employees",
    "misusing",
    "return",
    "policy",
    "trying",
    "make",
    "money",
    "fake",
    "returns",
    "right",
    "knn",
    "algorithm",
    "since",
    "main",
    "focus",
    "session",
    "decision",
    "tree",
    "starting",
    "decision",
    "tree",
    "first",
    "let",
    "tell",
    "choose",
    "gentry",
    "start",
    "well",
    "decision",
    "tree",
    "really",
    "easy",
    "easy",
    "read",
    "understand",
    "belongs",
    "one",
    "models",
    "interpretable",
    "understand",
    "exactly",
    "classifier",
    "made",
    "particular",
    "decision",
    "right",
    "let",
    "tell",
    "fact",
    "given",
    "data",
    "set",
    "say",
    "algorithm",
    "performs",
    "better",
    "like",
    "say",
    "asian",
    "trees",
    "better",
    "buys",
    "name",
    "biases",
    "performing",
    "better",
    "decision",
    "tree",
    "depends",
    "data",
    "set",
    "right",
    "apply",
    "hit",
    "trial",
    "method",
    "algorithms",
    "one",
    "one",
    "compare",
    "model",
    "gives",
    "best",
    "result",
    "model",
    "use",
    "better",
    "accuracy",
    "data",
    "set",
    "right",
    "let",
    "start",
    "decision",
    "tree",
    "well",
    "decision",
    "tree",
    "graphical",
    "representation",
    "possible",
    "solution",
    "decision",
    "based",
    "certain",
    "conditions",
    "might",
    "wondering",
    "thing",
    "called",
    "decision",
    "tree",
    "well",
    "called",
    "starts",
    "root",
    "branches",
    "number",
    "solution",
    "like",
    "tree",
    "right",
    "even",
    "trees",
    "starts",
    "roux",
    "starts",
    "growing",
    "branches",
    "gets",
    "bigger",
    "bigger",
    "similarly",
    "decision",
    "tree",
    "roux",
    "keeps",
    "growing",
    "increasing",
    "number",
    "decision",
    "conditions",
    "let",
    "tell",
    "real",
    "life",
    "scenario",
    "wo",
    "say",
    "must",
    "used",
    "remember",
    "whenever",
    "dial",
    "number",
    "credit",
    "card",
    "company",
    "redirects",
    "intelligent",
    "computerised",
    "assistant",
    "asks",
    "questions",
    "like",
    "press",
    "one",
    "english",
    "press",
    "2",
    "henry",
    "press",
    "3",
    "press",
    "4",
    "great",
    "select",
    "one",
    "redirects",
    "certain",
    "set",
    "questions",
    "like",
    "press",
    "1",
    "press",
    "1",
    "similarly",
    "right",
    "keeps",
    "repeating",
    "finally",
    "get",
    "right",
    "person",
    "right",
    "might",
    "think",
    "caught",
    "voicemail",
    "hell",
    "company",
    "actually",
    "using",
    "decision",
    "tree",
    "get",
    "right",
    "person",
    "lied",
    "like",
    "focus",
    "particular",
    "image",
    "moment",
    "particular",
    "slide",
    "see",
    "image",
    "task",
    "accept",
    "new",
    "job",
    "offer",
    "right",
    "decide",
    "created",
    "decision",
    "tree",
    "starting",
    "base",
    "condition",
    "root",
    "node",
    "basic",
    "salary",
    "minimum",
    "salary",
    "accepting",
    "offer",
    "right",
    "salary",
    "greater",
    "check",
    "whether",
    "commute",
    "one",
    "hour",
    "one",
    "decline",
    "offer",
    "less",
    "one",
    "hour",
    "getting",
    "closer",
    "accepting",
    "job",
    "offer",
    "photo",
    "check",
    "whether",
    "company",
    "offering",
    "free",
    "coffee",
    "right",
    "company",
    "offering",
    "free",
    "coffee",
    "declined",
    "offering",
    "free",
    "coffee",
    "yeah",
    "happily",
    "accept",
    "offer",
    "right",
    "example",
    "decision",
    "tree",
    "let",
    "move",
    "ahead",
    "understand",
    "decision",
    "tree",
    "well",
    "sample",
    "data",
    "set",
    "using",
    "explain",
    "decision",
    "tree",
    "alright",
    "data",
    "set",
    "row",
    "example",
    "first",
    "two",
    "columns",
    "provide",
    "features",
    "attributes",
    "describes",
    "data",
    "last",
    "column",
    "gives",
    "label",
    "class",
    "want",
    "predict",
    "like",
    "modify",
    "data",
    "adding",
    "additional",
    "features",
    "example",
    "program",
    "work",
    "exactly",
    "way",
    "fine",
    "data",
    "set",
    "pretty",
    "straightforward",
    "except",
    "one",
    "thing",
    "hope",
    "noticed",
    "perfectly",
    "separable",
    "let",
    "tell",
    "something",
    "second",
    "fifth",
    "examples",
    "features",
    "different",
    "labels",
    "yellow",
    "colour",
    "diameter",
    "three",
    "labels",
    "mango",
    "lemon",
    "right",
    "let",
    "move",
    "see",
    "decision",
    "tree",
    "handles",
    "case",
    "right",
    "order",
    "build",
    "tree",
    "use",
    "decision",
    "tree",
    "algorithm",
    "called",
    "card",
    "card",
    "algorithm",
    "stands",
    "classification",
    "regression",
    "tree",
    "algorithm",
    "online",
    "let",
    "see",
    "preview",
    "works",
    "right",
    "begin",
    "add",
    "root",
    "note",
    "tree",
    "nodes",
    "receive",
    "list",
    "rows",
    "input",
    "root",
    "receive",
    "entire",
    "training",
    "data",
    "set",
    "node",
    "ask",
    "true",
    "false",
    "question",
    "one",
    "feature",
    "response",
    "question",
    "split",
    "partition",
    "data",
    "set",
    "two",
    "different",
    "subsets",
    "subsets",
    "become",
    "input",
    "child",
    "node",
    "tree",
    "goal",
    "question",
    "finally",
    "unmix",
    "labels",
    "proceed",
    "words",
    "produce",
    "purest",
    "possible",
    "distribution",
    "labels",
    "node",
    "example",
    "input",
    "node",
    "contains",
    "one",
    "single",
    "type",
    "label",
    "see",
    "perfectly",
    "unmixed",
    "uncertainty",
    "type",
    "label",
    "consists",
    "grapes",
    "right",
    "hand",
    "labels",
    "node",
    "still",
    "mixed",
    "would",
    "ask",
    "another",
    "question",
    "drill",
    "right",
    "need",
    "understand",
    "question",
    "ask",
    "need",
    "conduct",
    "much",
    "question",
    "helps",
    "unmix",
    "label",
    "quantify",
    "amount",
    "uncertainty",
    "single",
    "node",
    "using",
    "metric",
    "called",
    "gini",
    "impurity",
    "quantify",
    "much",
    "question",
    "reduces",
    "uncertainty",
    "using",
    "concept",
    "called",
    "information",
    "gain",
    "use",
    "select",
    "best",
    "question",
    "ask",
    "point",
    "iterate",
    "steps",
    "recursively",
    "build",
    "tree",
    "new",
    "node",
    "continue",
    "dividing",
    "data",
    "question",
    "ask",
    "finally",
    "reach",
    "leaf",
    "alright",
    "alright",
    "decision",
    "tree",
    "order",
    "create",
    "decision",
    "tree",
    "first",
    "identify",
    "different",
    "set",
    "questions",
    "ask",
    "tree",
    "like",
    "color",
    "green",
    "question",
    "questions",
    "decided",
    "data",
    "set",
    "like",
    "colored",
    "green",
    "diameter",
    "greater",
    "equal",
    "3",
    "color",
    "yellow",
    "right",
    "questions",
    "resembles",
    "data",
    "set",
    "remember",
    "right",
    "color",
    "green",
    "divide",
    "two",
    "parts",
    "first",
    "green",
    "mango",
    "true",
    "false",
    "lemon",
    "mac",
    "right",
    "color",
    "green",
    "diameter",
    "meter",
    "greater",
    "equal",
    "3",
    "color",
    "yellow",
    "asian",
    "tree",
    "terminologies",
    "starting",
    "root",
    "node",
    "root",
    "node",
    "base",
    "node",
    "tree",
    "entire",
    "tree",
    "starts",
    "root",
    "node",
    "words",
    "first",
    "node",
    "tree",
    "represents",
    "entire",
    "population",
    "sample",
    "entire",
    "population",
    "segregated",
    "divided",
    "two",
    "homogeneous",
    "set",
    "fine",
    "next",
    "leaf",
    "node",
    "well",
    "leaf",
    "node",
    "one",
    "reach",
    "tree",
    "right",
    "segregated",
    "level",
    "leaf",
    "node",
    "next",
    "splitting",
    "splitting",
    "dividing",
    "root",
    "node",
    "node",
    "different",
    "sub",
    "part",
    "basis",
    "condition",
    "right",
    "comes",
    "branch",
    "sub",
    "tree",
    "well",
    "branch",
    "subtree",
    "gets",
    "formed",
    "split",
    "tree",
    "suppose",
    "split",
    "root",
    "node",
    "gets",
    "divided",
    "two",
    "branches",
    "two",
    "subtrees",
    "right",
    "next",
    "concept",
    "pruning",
    "well",
    "say",
    "pruning",
    "opposite",
    "splitting",
    "removing",
    "sub",
    "node",
    "decision",
    "tree",
    "see",
    "pruning",
    "later",
    "session",
    "right",
    "let",
    "move",
    "ahead",
    "next",
    "parent",
    "child",
    "node",
    "well",
    "first",
    "root",
    "node",
    "always",
    "parent",
    "node",
    "nodes",
    "associated",
    "known",
    "chalky",
    "node",
    "well",
    "understand",
    "way",
    "top",
    "node",
    "belongs",
    "parent",
    "node",
    "bottom",
    "node",
    "derived",
    "top",
    "node",
    "child",
    "node",
    "node",
    "producing",
    "note",
    "child",
    "node",
    "node",
    "producing",
    "parent",
    "node",
    "simple",
    "concept",
    "right",
    "use",
    "cart",
    "algorithm",
    "design",
    "tree",
    "manually",
    "first",
    "decide",
    "question",
    "ask",
    "let",
    "first",
    "visualize",
    "decision",
    "tree",
    "decision",
    "tree",
    "creating",
    "manually",
    "like",
    "first",
    "let",
    "look",
    "data",
    "set",
    "outlook",
    "temperature",
    "humidity",
    "windy",
    "different",
    "attribute",
    "basis",
    "predict",
    "whether",
    "play",
    "one",
    "among",
    "pick",
    "first",
    "answer",
    "determine",
    "best",
    "attribute",
    "classifies",
    "training",
    "data",
    "right",
    "choose",
    "best",
    "attribute",
    "tree",
    "decide",
    "split",
    "tree",
    "decide",
    "root",
    "node",
    "well",
    "move",
    "split",
    "tree",
    "terminologies",
    "know",
    "right",
    "first",
    "gini",
    "index",
    "gini",
    "index",
    "gini",
    "index",
    "measure",
    "impurity",
    "purity",
    "used",
    "building",
    "day",
    "gentry",
    "cart",
    "algorithm",
    "right",
    "next",
    "information",
    "gain",
    "information",
    "gain",
    "decrease",
    "entropy",
    "data",
    "set",
    "split",
    "basis",
    "attribute",
    "constructing",
    "decision",
    "tree",
    "finding",
    "attribute",
    "returns",
    "highest",
    "information",
    "gain",
    "right",
    "selecting",
    "node",
    "would",
    "give",
    "highest",
    "information",
    "gain",
    "alright",
    "next",
    "reduction",
    "variance",
    "reduction",
    "variance",
    "algorithm",
    "used",
    "continuous",
    "target",
    "variable",
    "regression",
    "problems",
    "split",
    "lower",
    "variance",
    "selected",
    "criteria",
    "let",
    "population",
    "see",
    "general",
    "term",
    "mean",
    "variance",
    "variance",
    "much",
    "data",
    "wearing",
    "right",
    "data",
    "less",
    "impure",
    "pure",
    "case",
    "variation",
    "would",
    "less",
    "data",
    "almost",
    "similar",
    "right",
    "also",
    "way",
    "setting",
    "tree",
    "split",
    "lower",
    "variance",
    "selected",
    "criteria",
    "split",
    "population",
    "alright",
    "next",
    "chi",
    "square",
    "c",
    "square",
    "algorithm",
    "used",
    "find",
    "statistical",
    "significance",
    "sub",
    "nodes",
    "parent",
    "nodes",
    "fine",
    "let",
    "move",
    "ahead",
    "main",
    "question",
    "decide",
    "best",
    "attribute",
    "understand",
    "need",
    "calculate",
    "something",
    "known",
    "information",
    "gain",
    "attribute",
    "highest",
    "information",
    "gain",
    "considered",
    "best",
    "yeah",
    "know",
    "next",
    "question",
    "might",
    "like",
    "information",
    "move",
    "see",
    "exactly",
    "information",
    "gain",
    "let",
    "first",
    "introduce",
    "term",
    "called",
    "entropy",
    "term",
    "used",
    "calculating",
    "information",
    "gain",
    "mmmmmm",
    "well",
    "entropy",
    "metric",
    "measures",
    "impurity",
    "something",
    "words",
    "say",
    "first",
    "step",
    "solve",
    "problem",
    "decision",
    "tree",
    "mentioned",
    "something",
    "impurity",
    "let",
    "move",
    "understand",
    "impurity",
    "suppose",
    "basket",
    "full",
    "apples",
    "another",
    "bowl",
    "full",
    "label",
    "says",
    "apple",
    "asked",
    "pick",
    "one",
    "item",
    "basket",
    "ball",
    "probability",
    "getting",
    "apple",
    "correct",
    "label",
    "1",
    "case",
    "see",
    "impurities",
    "zero",
    "right",
    "four",
    "different",
    "fruits",
    "basket",
    "four",
    "different",
    "labels",
    "bowl",
    "probability",
    "matching",
    "fruit",
    "label",
    "obviously",
    "one",
    "something",
    "less",
    "well",
    "could",
    "possible",
    "picked",
    "banana",
    "basket",
    "randomly",
    "picked",
    "label",
    "ball",
    "says",
    "cherry",
    "random",
    "permutation",
    "combination",
    "possible",
    "case",
    "say",
    "impurities",
    "nonzero",
    "hope",
    "concept",
    "impurities",
    "care",
    "coming",
    "back",
    "entropy",
    "said",
    "entropy",
    "measure",
    "impurity",
    "graph",
    "left",
    "see",
    "probability",
    "zero",
    "one",
    "either",
    "highly",
    "impure",
    "highly",
    "pure",
    "case",
    "value",
    "entropy",
    "zero",
    "probability",
    "value",
    "entropy",
    "maximum",
    "well",
    "impurity",
    "impurities",
    "degree",
    "randomness",
    "random",
    "data",
    "data",
    "completely",
    "pure",
    "case",
    "randomness",
    "equals",
    "0",
    "dies",
    "completely",
    "empire",
    "even",
    "case",
    "value",
    "impurity",
    "zero",
    "question",
    "like",
    "value",
    "entropy",
    "maximum",
    "might",
    "arise",
    "mine",
    "right",
    "let",
    "discuss",
    "let",
    "derive",
    "mathematically",
    "see",
    "slide",
    "mathematical",
    "formula",
    "entropy",
    "probability",
    "yes",
    "let",
    "move",
    "see",
    "graph",
    "say",
    "mathematically",
    "suppose",
    "total",
    "sample",
    "space",
    "divided",
    "two",
    "parts",
    "yes",
    "like",
    "data",
    "set",
    "result",
    "playing",
    "divided",
    "two",
    "parts",
    "yes",
    "predict",
    "either",
    "play",
    "right",
    "particular",
    "case",
    "define",
    "formula",
    "entropy",
    "entropy",
    "total",
    "sample",
    "space",
    "equals",
    "negative",
    "probability",
    "e",
    "multiplied",
    "log",
    "probability",
    "years",
    "base",
    "2",
    "minus",
    "probability",
    "x",
    "log",
    "probability",
    "base",
    "total",
    "sample",
    "space",
    "p",
    "v",
    "probability",
    "known",
    "probability",
    "well",
    "number",
    "yes",
    "equal",
    "number",
    "know",
    "probability",
    "equals",
    "right",
    "since",
    "equal",
    "number",
    "yes",
    "case",
    "value",
    "entropy",
    "one",
    "put",
    "value",
    "right",
    "let",
    "move",
    "next",
    "slide",
    "show",
    "alright",
    "next",
    "contains",
    "yes",
    "know",
    "probability",
    "sample",
    "space",
    "either",
    "1",
    "0",
    "case",
    "entropy",
    "equal",
    "0",
    "let",
    "see",
    "mathematically",
    "one",
    "one",
    "let",
    "start",
    "first",
    "condition",
    "probability",
    "formula",
    "entropy",
    "right",
    "first",
    "case",
    "right",
    "discuss",
    "art",
    "probability",
    "vs",
    "equal",
    "probability",
    "node",
    "data",
    "set",
    "equal",
    "number",
    "yes",
    "right",
    "probability",
    "yes",
    "equal",
    "probability",
    "equals",
    "words",
    "say",
    "yes",
    "plus",
    "equal",
    "total",
    "sample",
    "right",
    "since",
    "probability",
    "put",
    "values",
    "formula",
    "get",
    "something",
    "like",
    "calculate",
    "get",
    "entropy",
    "total",
    "sample",
    "space",
    "one",
    "right",
    "let",
    "see",
    "next",
    "case",
    "next",
    "case",
    "either",
    "totally",
    "us",
    "totally",
    "know",
    "total",
    "yes",
    "let",
    "see",
    "formula",
    "totally",
    "yes",
    "0",
    "fine",
    "probability",
    "e",
    "equal",
    "1",
    "yes",
    "yes",
    "total",
    "sample",
    "space",
    "obviously",
    "formula",
    "put",
    "thing",
    "get",
    "entropy",
    "sample",
    "space",
    "equal",
    "negative",
    "x",
    "1",
    "multiplied",
    "log",
    "1",
    "value",
    "log",
    "1",
    "equals",
    "total",
    "thing",
    "result",
    "0",
    "similarly",
    "case",
    "even",
    "case",
    "get",
    "entropy",
    "total",
    "sample",
    "space",
    "0",
    "entropy",
    "right",
    "next",
    "information",
    "gain",
    "well",
    "information",
    "gain",
    "measures",
    "reduction",
    "entropy",
    "decides",
    "attributes",
    "selected",
    "decision",
    "node",
    "total",
    "collection",
    "information",
    "gain",
    "equals",
    "entropy",
    "calculated",
    "weighted",
    "average",
    "x",
    "entropy",
    "feature",
    "worry",
    "see",
    "calculate",
    "example",
    "let",
    "manually",
    "build",
    "decision",
    "tree",
    "data",
    "set",
    "data",
    "set",
    "consists",
    "14",
    "different",
    "instances",
    "nine",
    "yes",
    "five",
    "know",
    "like",
    "formula",
    "entropy",
    "put",
    "since",
    "9",
    "years",
    "total",
    "probability",
    "e",
    "equals",
    "9",
    "14",
    "total",
    "probability",
    "equals",
    "phi",
    "14",
    "put",
    "value",
    "calculate",
    "result",
    "get",
    "value",
    "entropy",
    "right",
    "first",
    "step",
    "compute",
    "entropy",
    "entire",
    "data",
    "set",
    "select",
    "outlook",
    "temperature",
    "humidity",
    "windy",
    "node",
    "select",
    "root",
    "node",
    "big",
    "question",
    "right",
    "decide",
    "particular",
    "node",
    "chosen",
    "base",
    "note",
    "basis",
    "creating",
    "entire",
    "tree",
    "select",
    "let",
    "see",
    "one",
    "one",
    "calculate",
    "entropy",
    "information",
    "gain",
    "different",
    "nodes",
    "starting",
    "outlook",
    "outlook",
    "three",
    "different",
    "parameters",
    "sunny",
    "overcast",
    "rainy",
    "first",
    "select",
    "many",
    "number",
    "years",
    "case",
    "sunny",
    "like",
    "sunny",
    "many",
    "number",
    "years",
    "many",
    "number",
    "knows",
    "total",
    "yes",
    "three",
    "nos",
    "case",
    "sunny",
    "case",
    "overcast",
    "yes",
    "overcast",
    "surely",
    "go",
    "play",
    "like",
    "alright",
    "next",
    "rainy",
    "total",
    "number",
    "vs",
    "equal",
    "3",
    "total",
    "number",
    "equals",
    "2",
    "fine",
    "next",
    "calculate",
    "entropy",
    "feature",
    "calculating",
    "entropy",
    "outlook",
    "equals",
    "sunny",
    "first",
    "assuming",
    "outlook",
    "root",
    "node",
    "calculating",
    "gain",
    "right",
    "order",
    "calculate",
    "information",
    "gain",
    "remember",
    "formula",
    "entropy",
    "total",
    "sample",
    "space",
    "weighted",
    "average",
    "x",
    "entropy",
    "feature",
    "right",
    "calculating",
    "entropy",
    "outlook",
    "sunny",
    "total",
    "number",
    "yes",
    "sonny",
    "total",
    "number",
    "know",
    "three",
    "fine",
    "let",
    "put",
    "formula",
    "since",
    "probability",
    "yes",
    "2",
    "5",
    "probability",
    "3",
    "get",
    "something",
    "like",
    "right",
    "getting",
    "entropy",
    "sunny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "fine",
    "next",
    "calculate",
    "entropy",
    "overcast",
    "overcast",
    "remember",
    "yes",
    "right",
    "probability",
    "e",
    "equal",
    "1",
    "put",
    "get",
    "value",
    "entropy",
    "0",
    "fine",
    "rainy",
    "rainy",
    "3s",
    "nose",
    "probability",
    "e",
    "case",
    "sonny",
    "3",
    "5",
    "probability",
    "know",
    "case",
    "sonny",
    "2",
    "5",
    "add",
    "probability",
    "vs",
    "probability",
    "note",
    "formula",
    "get",
    "entropy",
    "sunny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "point",
    "calculate",
    "much",
    "information",
    "getting",
    "outlook",
    "equals",
    "weighted",
    "average",
    "right",
    "weighted",
    "average",
    "total",
    "number",
    "years",
    "total",
    "number",
    "fine",
    "information",
    "outlook",
    "equals",
    "5",
    "14",
    "5",
    "came",
    "calculating",
    "total",
    "number",
    "sample",
    "space",
    "within",
    "particular",
    "outlook",
    "sunny",
    "right",
    "case",
    "sunny",
    "two",
    "years",
    "three",
    "nos",
    "right",
    "weighted",
    "average",
    "sonny",
    "would",
    "equal",
    "5",
    "right",
    "since",
    "formula",
    "five",
    "14",
    "x",
    "entropy",
    "feature",
    "right",
    "calculated",
    "entropy",
    "sonny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "right",
    "multiply",
    "five",
    "14",
    "one",
    "right",
    "well",
    "calculation",
    "information",
    "outlook",
    "equal",
    "sunny",
    "outlook",
    "even",
    "equals",
    "overcast",
    "rainy",
    "case",
    "similarly",
    "calculate",
    "everything",
    "overcast",
    "sunny",
    "overcast",
    "weighted",
    "averages",
    "14",
    "x",
    "entropy",
    "0",
    "sonny",
    "5i",
    "yes",
    "two",
    "nodes",
    "x",
    "entropy",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "finally",
    "take",
    "sum",
    "equals",
    "right",
    "next",
    "calculate",
    "information",
    "gained",
    "earlier",
    "malaysian",
    "taken",
    "outlook",
    "calculating",
    "information",
    "gaining",
    "outlook",
    "right",
    "information",
    "gain",
    "equals",
    "total",
    "entropy",
    "minus",
    "information",
    "taken",
    "outlook",
    "right",
    "total",
    "entropy",
    "information",
    "took",
    "outlook",
    "value",
    "information",
    "gained",
    "outlook",
    "results",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "right",
    "next",
    "let",
    "assume",
    "wendy",
    "root",
    "node",
    "wendy",
    "consists",
    "two",
    "parameters",
    "false",
    "true",
    "let",
    "see",
    "many",
    "years",
    "many",
    "nodes",
    "case",
    "true",
    "false",
    "wendy",
    "falls",
    "parameter",
    "case",
    "six",
    "years",
    "two",
    "nodes",
    "true",
    "parameter",
    "3",
    "3",
    "nodes",
    "right",
    "let",
    "move",
    "ahead",
    "similarly",
    "calculate",
    "information",
    "taken",
    "wendy",
    "finally",
    "calculate",
    "information",
    "gained",
    "wendy",
    "alright",
    "first",
    "calculate",
    "entropy",
    "feature",
    "er",
    "starting",
    "windy",
    "equal",
    "true",
    "case",
    "true",
    "equal",
    "number",
    "yes",
    "equal",
    "number",
    "know",
    "remember",
    "graph",
    "probability",
    "total",
    "number",
    "years",
    "equal",
    "total",
    "number",
    "know",
    "case",
    "entropy",
    "equals",
    "1",
    "directly",
    "write",
    "entropy",
    "room",
    "windy",
    "one",
    "already",
    "proved",
    "probability",
    "equals",
    "entropy",
    "maximum",
    "equals",
    "right",
    "next",
    "entropy",
    "false",
    "vending",
    "like",
    "similarly",
    "put",
    "probability",
    "yes",
    "formula",
    "calculate",
    "result",
    "since",
    "six",
    "years",
    "nose",
    "total",
    "get",
    "probability",
    "yes",
    "6",
    "8",
    "probability",
    "2",
    "right",
    "calculate",
    "get",
    "entropy",
    "false",
    "zero",
    "point",
    "eight",
    "one",
    "one",
    "alright",
    "let",
    "calculate",
    "information",
    "windy",
    "total",
    "information",
    "collected",
    "windy",
    "equals",
    "information",
    "taken",
    "wendy",
    "equal",
    "true",
    "plus",
    "action",
    "taken",
    "wendy",
    "equal",
    "false",
    "calculate",
    "weighted",
    "average",
    "one",
    "sum",
    "finally",
    "get",
    "total",
    "information",
    "taken",
    "windy",
    "case",
    "equals",
    "8",
    "14",
    "multiplied",
    "1",
    "1",
    "plus",
    "6",
    "14",
    "x",
    "8",
    "total",
    "number",
    "yes",
    "case",
    "equals",
    "false",
    "right",
    "false",
    "total",
    "number",
    "bs",
    "equals",
    "6",
    "total",
    "know",
    "equal",
    "2",
    "ups",
    "alright",
    "waiter",
    "resul",
    "results",
    "aid",
    "14",
    "similarly",
    "information",
    "taken",
    "windy",
    "equals",
    "true",
    "equals",
    "3",
    "plus",
    "3",
    "3",
    "3",
    "equal",
    "6",
    "divided",
    "total",
    "number",
    "sample",
    "space",
    "14",
    "x",
    "1",
    "entropy",
    "true",
    "right",
    "8",
    "14",
    "multiplied",
    "1",
    "1",
    "plus",
    "6",
    "14",
    "x",
    "one",
    "results",
    "information",
    "taken",
    "windy",
    "right",
    "much",
    "information",
    "gaining",
    "wendy",
    "total",
    "information",
    "gained",
    "windy",
    "equals",
    "total",
    "entropy",
    "information",
    "taken",
    "windy",
    "right",
    "equals",
    "zero",
    "point",
    "zero",
    "four",
    "eight",
    "information",
    "gained",
    "windy",
    "similarly",
    "calculated",
    "rest",
    "outlook",
    "see",
    "information",
    "information",
    "gain",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "case",
    "temperature",
    "information",
    "around",
    "zero",
    "point",
    "nine",
    "one",
    "one",
    "information",
    "gain",
    "equal",
    "9",
    "case",
    "humidity",
    "information",
    "gained",
    "case",
    "windy",
    "information",
    "gained",
    "select",
    "attribute",
    "maximum",
    "fine",
    "selected",
    "outlook",
    "root",
    "node",
    "subdivided",
    "three",
    "different",
    "parts",
    "sunny",
    "overcast",
    "rain",
    "case",
    "overcast",
    "seen",
    "consists",
    "ears",
    "consider",
    "leaf",
    "node",
    "case",
    "sunny",
    "rainy",
    "doubtful",
    "consists",
    "yes",
    "know",
    "need",
    "recalculate",
    "things",
    "right",
    "node",
    "recalculate",
    "things",
    "right",
    "select",
    "attribute",
    "maximum",
    "information",
    "gain",
    "right",
    "complete",
    "tree",
    "look",
    "like",
    "right",
    "let",
    "see",
    "play",
    "play",
    "outlook",
    "overcast",
    "right",
    "case",
    "always",
    "play",
    "outlook",
    "sunny",
    "drill",
    "time",
    "check",
    "humidity",
    "condition",
    "right",
    "humidity",
    "normal",
    "play",
    "humidity",
    "high",
    "wo",
    "play",
    "right",
    "outlook",
    "predicts",
    "raining",
    "check",
    "whether",
    "windy",
    "week",
    "went",
    "go",
    "offer",
    "play",
    "strong",
    "wind",
    "wo",
    "play",
    "right",
    "entire",
    "decision",
    "tree",
    "would",
    "look",
    "like",
    "end",
    "comes",
    "concept",
    "pruning",
    "say",
    "play",
    "well",
    "pruning",
    "pruning",
    "decide",
    "play",
    "say",
    "pruning",
    "well",
    "pruning",
    "nothing",
    "cutting",
    "nodes",
    "order",
    "get",
    "optimal",
    "solution",
    "right",
    "pruning",
    "reduces",
    "complexity",
    "right",
    "see",
    "screen",
    "showing",
    "result",
    "yes",
    "showing",
    "result",
    "says",
    "play",
    "drill",
    "practical",
    "session",
    "common",
    "question",
    "might",
    "come",
    "mind",
    "might",
    "think",
    "tree",
    "based",
    "model",
    "better",
    "linear",
    "model",
    "right",
    "think",
    "like",
    "logistic",
    "regression",
    "classification",
    "problem",
    "linear",
    "regression",
    "regression",
    "problem",
    "need",
    "use",
    "tree",
    "well",
    "many",
    "us",
    "question",
    "mind",
    "well",
    "valid",
    "question",
    "well",
    "actually",
    "said",
    "earlier",
    "use",
    "algorithm",
    "depends",
    "type",
    "problem",
    "solving",
    "let",
    "look",
    "key",
    "factor",
    "help",
    "decide",
    "algorithm",
    "use",
    "first",
    "point",
    "relationship",
    "dependent",
    "independent",
    "variable",
    "well",
    "approximated",
    "linear",
    "model",
    "linear",
    "regression",
    "outperform",
    "tree",
    "base",
    "model",
    "second",
    "case",
    "high",
    "complex",
    "relationship",
    "dependent",
    "independent",
    "variables",
    "remodel",
    "outperform",
    "classical",
    "regression",
    "model",
    "third",
    "case",
    "need",
    "build",
    "model",
    "easy",
    "explain",
    "people",
    "decision",
    "tree",
    "model",
    "always",
    "better",
    "linear",
    "model",
    "decision",
    "tree",
    "models",
    "simpler",
    "interpret",
    "linear",
    "regression",
    "right",
    "let",
    "move",
    "ahead",
    "see",
    "write",
    "gentry",
    "classifier",
    "scratch",
    "python",
    "using",
    "cart",
    "algorithm",
    "right",
    "using",
    "jupyter",
    "notebook",
    "python",
    "installed",
    "alright",
    "let",
    "open",
    "anaconda",
    "jupyter",
    "notebook",
    "anaconda",
    "navigator",
    "directly",
    "jump",
    "jupyter",
    "notebook",
    "hit",
    "launch",
    "button",
    "guess",
    "everyone",
    "knows",
    "jupyter",
    "notebook",
    "interactive",
    "computing",
    "notebook",
    "environment",
    "run",
    "python",
    "codes",
    "jupiter",
    "notebook",
    "opens",
    "local",
    "host",
    "w89",
    "1",
    "using",
    "jupyter",
    "notebook",
    "order",
    "write",
    "decision",
    "tree",
    "classifier",
    "using",
    "python",
    "decision",
    "tree",
    "classifier",
    "already",
    "written",
    "set",
    "codes",
    "let",
    "explain",
    "one",
    "one",
    "start",
    "initializing",
    "training",
    "data",
    "set",
    "sample",
    "data",
    "set",
    "row",
    "example",
    "last",
    "column",
    "label",
    "first",
    "two",
    "columns",
    "features",
    "want",
    "add",
    "features",
    "example",
    "practice",
    "interesting",
    "fact",
    "data",
    "set",
    "design",
    "way",
    "second",
    "fifth",
    "example",
    "almost",
    "features",
    "different",
    "labels",
    "right",
    "let",
    "move",
    "see",
    "tree",
    "handles",
    "case",
    "see",
    "ii",
    "fifth",
    "column",
    "features",
    "different",
    "label",
    "right",
    "let",
    "move",
    "ahead",
    "training",
    "data",
    "set",
    "next",
    "adding",
    "column",
    "labels",
    "used",
    "print",
    "trees",
    "fine",
    "add",
    "header",
    "columns",
    "like",
    "first",
    "column",
    "close",
    "second",
    "diameter",
    "third",
    "label",
    "column",
    "right",
    "next",
    "define",
    "function",
    "unique",
    "values",
    "pass",
    "rows",
    "columns",
    "function",
    "find",
    "unique",
    "values",
    "column",
    "data",
    "set",
    "example",
    "passing",
    "training",
    "data",
    "hazard",
    "row",
    "column",
    "number",
    "0",
    "finding",
    "unique",
    "values",
    "terms",
    "color",
    "since",
    "row",
    "training",
    "data",
    "column",
    "1",
    "finding",
    "values",
    "terms",
    "diameter",
    "fine",
    "example",
    "next",
    "define",
    "function",
    "class",
    "count",
    "pass",
    "rows",
    "counts",
    "number",
    "type",
    "example",
    "within",
    "data",
    "set",
    "function",
    "basically",
    "counting",
    "number",
    "type",
    "example",
    "data",
    "set",
    "counting",
    "unique",
    "values",
    "label",
    "data",
    "set",
    "sample",
    "see",
    "pass",
    "entire",
    "training",
    "data",
    "set",
    "particular",
    "function",
    "class",
    "underscore",
    "count",
    "find",
    "different",
    "types",
    "label",
    "within",
    "training",
    "data",
    "set",
    "see",
    "unique",
    "label",
    "consists",
    "mango",
    "grape",
    "lemon",
    "next",
    "define",
    "function",
    "numeric",
    "pass",
    "value",
    "test",
    "value",
    "numeric",
    "return",
    "value",
    "integer",
    "float",
    "example",
    "see",
    "numeric",
    "passing",
    "7",
    "integer",
    "return",
    "value",
    "passing",
    "red",
    "numeric",
    "value",
    "right",
    "moving",
    "ahead",
    "define",
    "class",
    "named",
    "question",
    "question",
    "question",
    "used",
    "partition",
    "data",
    "set",
    "class",
    "voted",
    "records",
    "column",
    "number",
    "example",
    "0",
    "color",
    "light",
    "column",
    "value",
    "example",
    "green",
    "next",
    "defining",
    "match",
    "method",
    "used",
    "compare",
    "feature",
    "value",
    "example",
    "feature",
    "values",
    "stored",
    "question",
    "let",
    "see",
    "first",
    "defining",
    "init",
    "function",
    "inside",
    "passing",
    "self",
    "column",
    "value",
    "parameter",
    "next",
    "define",
    "function",
    "match",
    "compares",
    "feature",
    "value",
    "example",
    "feature",
    "value",
    "question",
    "next",
    "define",
    "function",
    "pr",
    "helper",
    "method",
    "print",
    "question",
    "readable",
    "format",
    "next",
    "defining",
    "function",
    "partition",
    "well",
    "function",
    "used",
    "partition",
    "data",
    "set",
    "row",
    "data",
    "set",
    "checks",
    "match",
    "question",
    "adds",
    "true",
    "rose",
    "adds",
    "false",
    "rose",
    "right",
    "example",
    "see",
    "partition",
    "training",
    "data",
    "based",
    "whether",
    "roses",
    "red",
    "calling",
    "function",
    "question",
    "passing",
    "value",
    "zero",
    "read",
    "assign",
    "red",
    "rose",
    "true",
    "underscore",
    "rose",
    "everything",
    "else",
    "assigned",
    "false",
    "underscore",
    "rose",
    "fine",
    "next",
    "define",
    "gini",
    "impurity",
    "function",
    "inside",
    "pass",
    "list",
    "rows",
    "calculate",
    "gini",
    "impurity",
    "list",
    "rows",
    "next",
    "defining",
    "function",
    "information",
    "gain",
    "information",
    "gain",
    "function",
    "calculates",
    "information",
    "game",
    "using",
    "uncertainty",
    "starting",
    "node",
    "weighted",
    "impurity",
    "child",
    "node",
    "next",
    "function",
    "find",
    "best",
    "plate",
    "well",
    "function",
    "used",
    "find",
    "best",
    "question",
    "ask",
    "iterating",
    "every",
    "feature",
    "value",
    "calculating",
    "information",
    "gain",
    "detail",
    "explanation",
    "code",
    "find",
    "code",
    "description",
    "given",
    "right",
    "next",
    "define",
    "class",
    "leave",
    "classifying",
    "data",
    "holds",
    "dictionary",
    "glass",
    "like",
    "mango",
    "many",
    "times",
    "appears",
    "row",
    "training",
    "data",
    "reaches",
    "sleeve",
    "alright",
    "next",
    "decision",
    "node",
    "decision",
    "node",
    "ask",
    "question",
    "holds",
    "reference",
    "question",
    "two",
    "child",
    "nodes",
    "base",
    "deciding",
    "node",
    "add",
    "branch",
    "alright",
    "next",
    "defining",
    "function",
    "build",
    "tree",
    "inside",
    "passing",
    "number",
    "rows",
    "function",
    "used",
    "build",
    "tree",
    "initially",
    "define",
    "various",
    "function",
    "using",
    "order",
    "build",
    "tree",
    "let",
    "start",
    "partitioning",
    "data",
    "set",
    "unique",
    "attribute",
    "calculate",
    "information",
    "gain",
    "return",
    "question",
    "produces",
    "highest",
    "gain",
    "basis",
    "split",
    "tree",
    "partitioning",
    "data",
    "set",
    "calculating",
    "information",
    "gain",
    "returning",
    "returning",
    "question",
    "producing",
    "highest",
    "gain",
    "right",
    "gain",
    "equals",
    "0",
    "return",
    "leaf",
    "rose",
    "getting",
    "gain",
    "gain",
    "equals",
    "0",
    "case",
    "since",
    "question",
    "could",
    "asked",
    "return",
    "leaf",
    "fine",
    "true",
    "underscore",
    "rose",
    "false",
    "underscore",
    "rose",
    "equal",
    "partition",
    "rose",
    "question",
    "reaching",
    "till",
    "position",
    "already",
    "found",
    "feature",
    "value",
    "used",
    "partition",
    "data",
    "set",
    "recursively",
    "build",
    "true",
    "branch",
    "similarly",
    "recursively",
    "build",
    "false",
    "branch",
    "return",
    "division",
    "discord",
    "node",
    "side",
    "passing",
    "question",
    "branch",
    "false",
    "front",
    "return",
    "question",
    "node",
    "alice",
    "question",
    "owed",
    "recalls",
    "best",
    "feature",
    "value",
    "ask",
    "point",
    "fine",
    "built",
    "tree",
    "next",
    "define",
    "print",
    "underscore",
    "tree",
    "function",
    "used",
    "print",
    "tree",
    "fine",
    "finally",
    "particular",
    "function",
    "printing",
    "tree",
    "next",
    "classify",
    "function",
    "use",
    "decide",
    "whether",
    "follow",
    "true",
    "branch",
    "false",
    "branch",
    "compared",
    "feature",
    "values",
    "stored",
    "node",
    "example",
    "considering",
    "last",
    "finally",
    "print",
    "production",
    "leaf",
    "let",
    "execute",
    "see",
    "okay",
    "testing",
    "data",
    "right",
    "printed",
    "leaf",
    "well",
    "trained",
    "algorithm",
    "training",
    "data",
    "set",
    "time",
    "test",
    "testing",
    "data",
    "set",
    "let",
    "finally",
    "execute",
    "see",
    "result",
    "result",
    "get",
    "first",
    "question",
    "asked",
    "algorithm",
    "diameter",
    "greater",
    "equal",
    "3",
    "true",
    "ask",
    "color",
    "yellow",
    "true",
    "predict",
    "mango",
    "one",
    "lemon",
    "one",
    "case",
    "false",
    "predict",
    "mango",
    "true",
    "part",
    "next",
    "coming",
    "diameter",
    "greater",
    "equal",
    "3",
    "case",
    "false",
    "predict",
    "grape",
    "fine",
    "okay",
    "coding",
    "part",
    "let",
    "conclude",
    "session",
    "concluding",
    "let",
    "show",
    "one",
    "thing",
    "algorithm",
    "cheat",
    "sheet",
    "explains",
    "algorithm",
    "use",
    "right",
    "let",
    "build",
    "decision",
    "tree",
    "format",
    "let",
    "see",
    "built",
    "first",
    "condition",
    "check",
    "whether",
    "50",
    "samples",
    "samples",
    "greater",
    "50",
    "move",
    "ahead",
    "less",
    "50",
    "need",
    "collect",
    "data",
    "sample",
    "greater",
    "50",
    "decide",
    "whether",
    "want",
    "predict",
    "category",
    "want",
    "predict",
    "category",
    "see",
    "whether",
    "labeled",
    "data",
    "label",
    "data",
    "would",
    "classification",
    "algorithm",
    "problem",
    "label",
    "data",
    "would",
    "clustering",
    "problem",
    "want",
    "category",
    "want",
    "predict",
    "predict",
    "quantity",
    "well",
    "want",
    "predict",
    "quantity",
    "case",
    "would",
    "regression",
    "problem",
    "want",
    "predict",
    "quantity",
    "want",
    "keep",
    "looking",
    "case",
    "go",
    "dimensionality",
    "reduction",
    "problems",
    "still",
    "want",
    "look",
    "predicting",
    "structure",
    "working",
    "tough",
    "luck",
    "hope",
    "recession",
    "clarifies",
    "doubt",
    "decision",
    "tree",
    "algorithm",
    "let",
    "begin",
    "tutorial",
    "looking",
    "topics",
    "covering",
    "today",
    "first",
    "start",
    "away",
    "getting",
    "brief",
    "introduction",
    "random",
    "forest",
    "go",
    "see",
    "actually",
    "need",
    "random",
    "forest",
    "right",
    "anything",
    "else",
    "actually",
    "random",
    "forest",
    "understand",
    "need",
    "first",
    "place",
    "go",
    "learn",
    "random",
    "forest",
    "also",
    "look",
    "various",
    "examples",
    "random",
    "forest",
    "get",
    "clear",
    "understanding",
    "also",
    "delve",
    "inside",
    "understand",
    "working",
    "random",
    "forest",
    "exactly",
    "random",
    "forest",
    "works",
    "also",
    "watch",
    "random",
    "forest",
    "algorithm",
    "step",
    "step",
    "right",
    "able",
    "write",
    "piece",
    "code",
    "domain",
    "specific",
    "algorithm",
    "personally",
    "believe",
    "learning",
    "really",
    "incomplete",
    "put",
    "application",
    "completion",
    "also",
    "implement",
    "random",
    "forest",
    "r",
    "simple",
    "use",
    "case",
    "diabetes",
    "prevention",
    "let",
    "get",
    "started",
    "introduction",
    "random",
    "forest",
    "actually",
    "one",
    "classifiers",
    "used",
    "solving",
    "classification",
    "problems",
    "since",
    "might",
    "really",
    "aware",
    "classification",
    "let",
    "quickly",
    "understand",
    "classification",
    "first",
    "try",
    "related",
    "random",
    "forest",
    "basically",
    "classification",
    "machine",
    "learning",
    "technique",
    "already",
    "predefined",
    "categories",
    "classify",
    "data",
    "nothing",
    "supervised",
    "learning",
    "model",
    "already",
    "data",
    "based",
    "train",
    "machine",
    "right",
    "machine",
    "actually",
    "learns",
    "data",
    "whatever",
    "predefined",
    "data",
    "already",
    "actually",
    "works",
    "fuel",
    "machine",
    "right",
    "let",
    "say",
    "example",
    "ever",
    "wondered",
    "gmail",
    "gets",
    "know",
    "spam",
    "emails",
    "filters",
    "rest",
    "genuine",
    "emails",
    "guesses",
    "right",
    "give",
    "hint",
    "try",
    "think",
    "something",
    "line",
    "would",
    "actually",
    "look",
    "possible",
    "parameters",
    "based",
    "decide",
    "read",
    "genuine",
    "email",
    "spam",
    "email",
    "certain",
    "parameters",
    "classifier",
    "actually",
    "look",
    "like",
    "subject",
    "line",
    "text",
    "html",
    "tags",
    "also",
    "ip",
    "address",
    "source",
    "mail",
    "getting",
    "analyze",
    "variables",
    "classify",
    "pam",
    "genuine",
    "folder",
    "let",
    "say",
    "example",
    "subject",
    "line",
    "states",
    "like",
    "mad",
    "cute",
    "pretty",
    "absurd",
    "keywords",
    "classifier",
    "smart",
    "enough",
    "trained",
    "manner",
    "get",
    "know",
    "right",
    "spam",
    "email",
    "automatically",
    "filter",
    "genuine",
    "emails",
    "classify",
    "works",
    "basically",
    "pretty",
    "much",
    "classification",
    "let",
    "move",
    "forward",
    "see",
    "always",
    "actually",
    "perform",
    "classification",
    "three",
    "classifiers",
    "namely",
    "decision",
    "tree",
    "random",
    "forest",
    "base",
    "right",
    "speaking",
    "briefly",
    "season",
    "3",
    "first",
    "decision",
    "tree",
    "actually",
    "splits",
    "entire",
    "data",
    "set",
    "structure",
    "tree",
    "makes",
    "decision",
    "every",
    "node",
    "hence",
    "called",
    "decision",
    "tree",
    "big",
    "bang",
    "theory",
    "right",
    "certain",
    "data",
    "set",
    "certain",
    "nodes",
    "node",
    "split",
    "child",
    "nodes",
    "node",
    "make",
    "decision",
    "final",
    "decision",
    "form",
    "positive",
    "negative",
    "right",
    "let",
    "say",
    "example",
    "want",
    "purchase",
    "car",
    "right",
    "parameters",
    "let",
    "say",
    "go",
    "want",
    "purchase",
    "car",
    "keep",
    "certain",
    "parameters",
    "mind",
    "would",
    "exactly",
    "income",
    "budget",
    "particular",
    "brand",
    "want",
    "go",
    "mileage",
    "car",
    "cylinder",
    "capacity",
    "car",
    "forth",
    "right",
    "make",
    "decision",
    "based",
    "parameters",
    "right",
    "make",
    "decisions",
    "really",
    "want",
    "know",
    "decision",
    "tree",
    "exactly",
    "works",
    "also",
    "check",
    "decision",
    "tree",
    "tutorial",
    "well",
    "let",
    "begin",
    "random",
    "forest",
    "random",
    "forest",
    "simple",
    "classifier",
    "actually",
    "let",
    "understand",
    "war",
    "symbol",
    "means",
    "simple",
    "methods",
    "actually",
    "use",
    "multiple",
    "machine",
    "learning",
    "algorithms",
    "obtain",
    "better",
    "predictive",
    "performance",
    "particularly",
    "talking",
    "random",
    "forest",
    "random",
    "forests",
    "uses",
    "multiple",
    "decision",
    "trees",
    "prediction",
    "right",
    "assembling",
    "lot",
    "decision",
    "trees",
    "come",
    "final",
    "outcome",
    "also",
    "look",
    "image",
    "entire",
    "data",
    "set",
    "actually",
    "split",
    "three",
    "subsets",
    "right",
    "subset",
    "leads",
    "particular",
    "decision",
    "tree",
    "three",
    "decision",
    "trees",
    "decision",
    "tree",
    "lead",
    "certain",
    "outcome",
    "random",
    "forest",
    "compile",
    "results",
    "decision",
    "trees",
    "lead",
    "final",
    "outcome",
    "right",
    "compiled",
    "section",
    "multiple",
    "decision",
    "trees",
    "random",
    "forest",
    "let",
    "see",
    "lies",
    "pace",
    "right",
    "naive",
    "bayes",
    "famous",
    "classifier",
    "made",
    "famous",
    "rule",
    "called",
    "bayes",
    "theorem",
    "might",
    "studied",
    "nee",
    "bayes",
    "theorem",
    "10",
    "standard",
    "well",
    "let",
    "see",
    "bayes",
    "theorem",
    "describes",
    "based",
    "actually",
    "describes",
    "probability",
    "event",
    "based",
    "certain",
    "prior",
    "knowledge",
    "conditions",
    "might",
    "related",
    "event",
    "right",
    "example",
    "cancer",
    "related",
    "age",
    "right",
    "person",
    "age",
    "used",
    "accurately",
    "assess",
    "probability",
    "cancer",
    "without",
    "knowledge",
    "age",
    "know",
    "age",
    "become",
    "handy",
    "addicting",
    "occurrence",
    "cancer",
    "particular",
    "person",
    "right",
    "outcome",
    "first",
    "event",
    "actually",
    "affecting",
    "final",
    "outcome",
    "yeah",
    "naive",
    "bayes",
    "classifier",
    "actually",
    "works",
    "give",
    "overview",
    "nave",
    "bayes",
    "classifier",
    "pretty",
    "much",
    "types",
    "classifiers",
    "try",
    "find",
    "answer",
    "particular",
    "question",
    "need",
    "random",
    "forest",
    "fine",
    "like",
    "human",
    "beings",
    "learn",
    "past",
    "experiences",
    "unlike",
    "human",
    "beings",
    "computer",
    "experiences",
    "machine",
    "takes",
    "decisions",
    "learn",
    "um",
    "well",
    "computer",
    "system",
    "actually",
    "learns",
    "data",
    "represents",
    "past",
    "experiences",
    "application",
    "domain",
    "let",
    "see",
    "random",
    "forest",
    "helps",
    "building",
    "learning",
    "model",
    "simple",
    "use",
    "case",
    "credit",
    "risk",
    "detection",
    "needless",
    "say",
    "credit",
    "card",
    "companies",
    "nested",
    "interest",
    "identifying",
    "financial",
    "transactions",
    "illegitimate",
    "criminal",
    "nature",
    "also",
    "would",
    "like",
    "mention",
    "point",
    "according",
    "federal",
    "reserve",
    "payment",
    "study",
    "americans",
    "used",
    "credit",
    "cards",
    "pay",
    "twenty",
    "six",
    "point",
    "two",
    "million",
    "purchases",
    "2012",
    "estimated",
    "loss",
    "due",
    "unauthorized",
    "transactions",
    "us",
    "six",
    "point",
    "1",
    "billion",
    "dollars",
    "banking",
    "industry",
    "measuring",
    "risk",
    "critical",
    "stakes",
    "high",
    "overall",
    "goal",
    "actually",
    "figure",
    "fraudulent",
    "much",
    "financial",
    "damage",
    "done",
    "credit",
    "card",
    "company",
    "receives",
    "thousands",
    "applications",
    "new",
    "cards",
    "application",
    "contains",
    "information",
    "applicant",
    "right",
    "see",
    "applications",
    "actually",
    "figure",
    "predictor",
    "variables",
    "like",
    "marital",
    "status",
    "person",
    "gender",
    "person",
    "age",
    "person",
    "status",
    "actually",
    "whether",
    "default",
    "pair",
    "pair",
    "default",
    "payments",
    "basically",
    "payments",
    "made",
    "time",
    "according",
    "agreement",
    "signed",
    "cardholder",
    "account",
    "actually",
    "set",
    "default",
    "easily",
    "figure",
    "history",
    "particular",
    "card",
    "holder",
    "also",
    "look",
    "time",
    "payment",
    "whether",
    "regular",
    "pair",
    "regular",
    "one",
    "source",
    "income",
    "particular",
    "person",
    "forth",
    "minimize",
    "loss",
    "back",
    "actually",
    "needs",
    "certain",
    "decision",
    "rule",
    "predict",
    "whether",
    "approve",
    "particular",
    "loan",
    "particular",
    "person",
    "random",
    "forest",
    "actually",
    "comes",
    "picture",
    "right",
    "let",
    "see",
    "random",
    "forest",
    "actually",
    "help",
    "us",
    "particular",
    "scenario",
    "taken",
    "randomly",
    "two",
    "parameters",
    "predictive",
    "variables",
    "saw",
    "previously",
    "taken",
    "two",
    "predictor",
    "variables",
    "first",
    "one",
    "income",
    "second",
    "one",
    "h",
    "right",
    "similarly",
    "parallel",
    "decision",
    "trees",
    "implemented",
    "upon",
    "predicted",
    "variables",
    "let",
    "first",
    "assume",
    "case",
    "income",
    "variable",
    "right",
    "divided",
    "income",
    "three",
    "categories",
    "first",
    "one",
    "person",
    "earning",
    "dollars",
    "second",
    "15",
    "35",
    "thousand",
    "dollars",
    "third",
    "one",
    "running",
    "range",
    "0",
    "15",
    "thousand",
    "dollars",
    "person",
    "earning",
    "pretty",
    "good",
    "income",
    "pretty",
    "decent",
    "check",
    "credit",
    "history",
    "probability",
    "person",
    "earning",
    "good",
    "amount",
    "low",
    "risk",
    "wo",
    "able",
    "pay",
    "back",
    "already",
    "earning",
    "good",
    "application",
    "loan",
    "get",
    "approved",
    "right",
    "actually",
    "low",
    "risk",
    "moderate",
    "risk",
    "real",
    "issue",
    "high",
    "risk",
    "approve",
    "applicants",
    "request",
    "let",
    "move",
    "watch",
    "second",
    "category",
    "person",
    "actually",
    "earning",
    "15",
    "35",
    "thousand",
    "dollars",
    "right",
    "person",
    "may",
    "may",
    "pay",
    "back",
    "scenarios",
    "look",
    "credit",
    "history",
    "previous",
    "history",
    "previous",
    "history",
    "bad",
    "like",
    "default",
    "er",
    "previous",
    "transactions",
    "definitely",
    "consider",
    "approving",
    "request",
    "high",
    "risk",
    "good",
    "bank",
    "previous",
    "history",
    "particular",
    "applicant",
    "really",
    "good",
    "clarify",
    "doubt",
    "consider",
    "another",
    "pair",
    "dress",
    "well",
    "depth",
    "already",
    "really",
    "high",
    "depth",
    "risks",
    "increases",
    "chances",
    "might",
    "pay",
    "repay",
    "future",
    "accept",
    "request",
    "person",
    "high",
    "dipped",
    "person",
    "low",
    "depth",
    "good",
    "pair",
    "past",
    "history",
    "chances",
    "might",
    "back",
    "consider",
    "approving",
    "request",
    "particular",
    "applicant",
    "let",
    "look",
    "third",
    "category",
    "person",
    "earning",
    "0",
    "15",
    "thousand",
    "dollars",
    "something",
    "actually",
    "raises",
    "broke",
    "person",
    "actually",
    "lie",
    "category",
    "high",
    "risk",
    "right",
    "probability",
    "application",
    "loan",
    "would",
    "probably",
    "get",
    "rejected",
    "get",
    "one",
    "final",
    "outcome",
    "income",
    "parameter",
    "right",
    "let",
    "us",
    "look",
    "second",
    "variable",
    "age",
    "lead",
    "second",
    "decision",
    "tree",
    "let",
    "us",
    "say",
    "person",
    "young",
    "right",
    "look",
    "forward",
    "student",
    "student",
    "chances",
    "high",
    "wo",
    "able",
    "repay",
    "back",
    "learning",
    "source",
    "right",
    "risks",
    "high",
    "probability",
    "application",
    "loan",
    "get",
    "rejected",
    "fine",
    "person",
    "young",
    "student",
    "probably",
    "go",
    "look",
    "another",
    "variable",
    "pan",
    "balance",
    "let",
    "look",
    "bank",
    "balance",
    "less",
    "5",
    "lakhs",
    "risk",
    "arises",
    "probabilities",
    "application",
    "loan",
    "get",
    "rejected",
    "person",
    "young",
    "student",
    "bank",
    "balance",
    "greater",
    "5",
    "lakhs",
    "got",
    "pretty",
    "good",
    "stable",
    "balanced",
    "probabilities",
    "zone",
    "application",
    "get",
    "approved",
    "let",
    "us",
    "take",
    "another",
    "scenario",
    "senior",
    "right",
    "senior",
    "probably",
    "go",
    "check",
    "credit",
    "history",
    "well",
    "previous",
    "transactions",
    "kind",
    "person",
    "like",
    "whether",
    "defaulter",
    "ananda",
    "falter",
    "fair",
    "kind",
    "person",
    "previous",
    "transactions",
    "risk",
    "arises",
    "probability",
    "application",
    "getting",
    "rejected",
    "actually",
    "increases",
    "right",
    "excellent",
    "person",
    "per",
    "transactions",
    "previous",
    "history",
    "least",
    "risk",
    "probabilities",
    "application",
    "loan",
    "get",
    "approved",
    "two",
    "variables",
    "income",
    "age",
    "led",
    "two",
    "different",
    "decision",
    "trees",
    "right",
    "two",
    "different",
    "decision",
    "trees",
    "actually",
    "led",
    "two",
    "different",
    "results",
    "random",
    "forest",
    "actually",
    "compile",
    "two",
    "different",
    "results",
    "two",
    "different",
    "decision",
    "trees",
    "finally",
    "lead",
    "final",
    "outcome",
    "random",
    "forest",
    "actually",
    "works",
    "right",
    "actually",
    "motive",
    "random",
    "forest",
    "let",
    "us",
    "move",
    "forward",
    "see",
    "random",
    "forest",
    "right",
    "get",
    "idea",
    "mechanism",
    "name",
    "random",
    "forests",
    "collection",
    "trees",
    "fortress",
    "called",
    "probably",
    "also",
    "trees",
    "actually",
    "trained",
    "subsets",
    "selected",
    "random",
    "therefore",
    "called",
    "random",
    "forests",
    "random",
    "forests",
    "collection",
    "symbol",
    "decision",
    "eat",
    "straight",
    "head",
    "decision",
    "trees",
    "actually",
    "built",
    "using",
    "whole",
    "data",
    "set",
    "considering",
    "features",
    "actually",
    "random",
    "forest",
    "fraction",
    "number",
    "rows",
    "selected",
    "random",
    "particular",
    "number",
    "features",
    "actually",
    "selected",
    "random",
    "trained",
    "upon",
    "decision",
    "trees",
    "built",
    "upon",
    "right",
    "similarly",
    "number",
    "decision",
    "trees",
    "grown",
    "decision",
    "tree",
    "result",
    "two",
    "certain",
    "final",
    "outcome",
    "random",
    "forest",
    "nothing",
    "actually",
    "compiled",
    "results",
    "decision",
    "trees",
    "bring",
    "final",
    "result",
    "see",
    "particular",
    "figure",
    "particular",
    "instance",
    "actually",
    "resulted",
    "three",
    "different",
    "decision",
    "trees",
    "right",
    "sonar",
    "tree",
    "one",
    "results",
    "final",
    "outcome",
    "called",
    "class",
    "tree",
    "results",
    "class",
    "similarly",
    "tree",
    "three",
    "results",
    "class",
    "p",
    "random",
    "forest",
    "compile",
    "results",
    "decision",
    "trees",
    "go",
    "goal",
    "majority",
    "voting",
    "since",
    "head",
    "decision",
    "trees",
    "actually",
    "voted",
    "favor",
    "class",
    "b",
    "decision",
    "tree",
    "two",
    "three",
    "therefore",
    "final",
    "outcome",
    "favor",
    "class",
    "random",
    "forest",
    "actually",
    "works",
    "upon",
    "one",
    "really",
    "beautiful",
    "thing",
    "particular",
    "algorithm",
    "one",
    "versatile",
    "algorithms",
    "capable",
    "performing",
    "regression",
    "well",
    "classification",
    "let",
    "try",
    "understand",
    "random",
    "forest",
    "beautiful",
    "example",
    "favorite",
    "one",
    "let",
    "say",
    "want",
    "decide",
    "want",
    "watch",
    "edge",
    "tomorrow",
    "right",
    "particular",
    "scenario",
    "two",
    "different",
    "actions",
    "work",
    "bond",
    "either",
    "straight",
    "away",
    "go",
    "best",
    "friend",
    "asked",
    "read",
    "whether",
    "go",
    "edge",
    "tomorrow",
    "like",
    "movie",
    "ask",
    "bunch",
    "friends",
    "take",
    "opinion",
    "consideration",
    "based",
    "final",
    "results",
    "go",
    "watch",
    "edge",
    "tomorrow",
    "right",
    "let",
    "take",
    "first",
    "scenario",
    "go",
    "best",
    "friend",
    "asked",
    "whether",
    "go",
    "watch",
    "edge",
    "tomorrow",
    "friend",
    "probably",
    "ask",
    "certain",
    "questions",
    "like",
    "first",
    "one",
    "jonah",
    "let",
    "say",
    "friend",
    "asks",
    "really",
    "like",
    "adventurous",
    "kind",
    "movies",
    "say",
    "yes",
    "definitely",
    "would",
    "love",
    "watch",
    "venture",
    "kind",
    "movie",
    "probabilities",
    "like",
    "edge",
    "tomorrow",
    "well",
    "since",
    "tomorrow",
    "also",
    "movie",
    "adventure",
    "kind",
    "jonah",
    "right",
    "let",
    "say",
    "like",
    "adventure",
    "john",
    "movie",
    "probability",
    "reduces",
    "might",
    "really",
    "like",
    "edge",
    "morrow",
    "right",
    "come",
    "certain",
    "conclusion",
    "right",
    "let",
    "say",
    "best",
    "friend",
    "puts",
    "another",
    "situation",
    "ask",
    "like",
    "emily",
    "plant",
    "see",
    "definitely",
    "like",
    "emily",
    "blunt",
    "puts",
    "another",
    "question",
    "like",
    "emily",
    "blunt",
    "main",
    "lead",
    "say",
    "yes",
    "probability",
    "arises",
    "definitely",
    "like",
    "edge",
    "tomorrow",
    "well",
    "edge",
    "tomorrow",
    "emily",
    "plant",
    "main",
    "lead",
    "cast",
    "say",
    "oh",
    "like",
    "emily",
    "blunt",
    "probability",
    "reduces",
    "would",
    "like",
    "edge",
    "tomorrow",
    "write",
    "one",
    "way",
    "one",
    "decision",
    "tree",
    "final",
    "outcome",
    "final",
    "decision",
    "based",
    "one",
    "decision",
    "tree",
    "see",
    "final",
    "outcome",
    "based",
    "one",
    "friend",
    "definitely",
    "really",
    "convinced",
    "want",
    "consider",
    "options",
    "friends",
    "also",
    "make",
    "precise",
    "crisp",
    "decision",
    "right",
    "go",
    "approach",
    "bunch",
    "friends",
    "let",
    "say",
    "go",
    "three",
    "friends",
    "ask",
    "question",
    "whether",
    "would",
    "like",
    "watch",
    "age",
    "tomorrow",
    "go",
    "approach",
    "three",
    "four",
    "friends",
    "friend",
    "one",
    "friend",
    "twin",
    "friend",
    "three",
    "consider",
    "sport",
    "decision",
    "dependent",
    "compiled",
    "results",
    "three",
    "friends",
    "right",
    "let",
    "say",
    "go",
    "first",
    "friend",
    "ask",
    "whether",
    "would",
    "like",
    "watch",
    "tomorrow",
    "first",
    "friend",
    "puts",
    "one",
    "question",
    "like",
    "top",
    "gun",
    "say",
    "yes",
    "definitely",
    "like",
    "movie",
    "top",
    "gun",
    "probabilities",
    "would",
    "like",
    "edge",
    "tomorrow",
    "well",
    "topgun",
    "actually",
    "military",
    "action",
    "drama",
    "also",
    "tom",
    "cruise",
    "probability",
    "rises",
    "yes",
    "like",
    "edge",
    "tomorrow",
    "well",
    "say",
    "like",
    "top",
    "gun",
    "chances",
    "would",
    "like",
    "edge",
    "tomorrow",
    "right",
    "another",
    "question",
    "puts",
    "across",
    "really",
    "like",
    "watch",
    "action",
    "movies",
    "say",
    "yes",
    "would",
    "love",
    "watch",
    "chances",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "friend",
    "come",
    "one",
    "conclusion",
    "hear",
    "since",
    "ratio",
    "liking",
    "movie",
    "like",
    "actually",
    "2",
    "1",
    "final",
    "result",
    "actually",
    "would",
    "like",
    "edge",
    "tomorrow",
    "go",
    "second",
    "friend",
    "ask",
    "question",
    "second",
    "friend",
    "asks",
    "like",
    "far",
    "away",
    "went",
    "last",
    "time",
    "washed",
    "say",
    "really",
    "like",
    "far",
    "away",
    "would",
    "say",
    "definitely",
    "going",
    "like",
    "edge",
    "tomorrow",
    "far",
    "away",
    "actually",
    "since",
    "might",
    "knowing",
    "far",
    "ways",
    "johner",
    "romance",
    "revolves",
    "around",
    "girl",
    "guy",
    "guy",
    "falling",
    "love",
    "probability",
    "would",
    "like",
    "edge",
    "tomorrow",
    "ask",
    "another",
    "question",
    "like",
    "bolivian",
    "really",
    "like",
    "watch",
    "tom",
    "cruise",
    "say",
    "yes",
    "probability",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "oblivion",
    "science",
    "fiction",
    "casting",
    "tom",
    "cruise",
    "full",
    "strange",
    "experiences",
    "tom",
    "cruise",
    "savior",
    "masses",
    "kind",
    "well",
    "kind",
    "plot",
    "edge",
    "tomorrow",
    "well",
    "pure",
    "yes",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "get",
    "another",
    "second",
    "decision",
    "second",
    "friend",
    "go",
    "third",
    "friend",
    "ask",
    "probably",
    "third",
    "friend",
    "really",
    "interesting",
    "sort",
    "conversation",
    "say",
    "simply",
    "asks",
    "like",
    "godzilla",
    "say",
    "like",
    "godzilla",
    "say",
    "definitely",
    "would",
    "like",
    "edge",
    "tomorrow",
    "godzilla",
    "also",
    "actually",
    "fiction",
    "movie",
    "adventure",
    "jonah",
    "got",
    "three",
    "results",
    "three",
    "different",
    "decision",
    "trees",
    "three",
    "different",
    "friends",
    "compile",
    "results",
    "friends",
    "make",
    "final",
    "call",
    "yes",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "real",
    "time",
    "interesting",
    "example",
    "actually",
    "implement",
    "random",
    "forest",
    "ground",
    "reality",
    "let",
    "us",
    "look",
    "various",
    "domains",
    "random",
    "forest",
    "actually",
    "used",
    "diversity",
    "random",
    "forest",
    "actually",
    "used",
    "various",
    "diverse",
    "means",
    "like",
    "beat",
    "banking",
    "beat",
    "medicine",
    "beat",
    "land",
    "use",
    "beat",
    "marketing",
    "name",
    "random",
    "forest",
    "banking",
    "particularly",
    "random",
    "forest",
    "actually",
    "used",
    "make",
    "whether",
    "applicant",
    "default",
    "pair",
    "older",
    "one",
    "accordingly",
    "approve",
    "reject",
    "applications",
    "loan",
    "right",
    "random",
    "forest",
    "used",
    "banking",
    "talking",
    "medicine",
    "random",
    "forest",
    "widely",
    "used",
    "medicine",
    "field",
    "predict",
    "beforehand",
    "probability",
    "person",
    "actually",
    "particular",
    "disease",
    "right",
    "actually",
    "used",
    "look",
    "various",
    "disease",
    "trends",
    "let",
    "say",
    "want",
    "figure",
    "probability",
    "person",
    "diabetes",
    "would",
    "probably",
    "look",
    "medical",
    "history",
    "patient",
    "see",
    "right",
    "glucose",
    "concentration",
    "bmi",
    "insulin",
    "levels",
    "patient",
    "past",
    "previous",
    "three",
    "months",
    "age",
    "particular",
    "person",
    "make",
    "different",
    "decision",
    "trees",
    "based",
    "one",
    "predictor",
    "variables",
    "finally",
    "compiled",
    "results",
    "variables",
    "make",
    "final",
    "decision",
    "whether",
    "person",
    "diabetes",
    "near",
    "future",
    "random",
    "forest",
    "used",
    "medicine",
    "sector",
    "move",
    "random",
    "forest",
    "also",
    "actually",
    "used",
    "find",
    "land",
    "use",
    "example",
    "want",
    "set",
    "particular",
    "industry",
    "certain",
    "area",
    "would",
    "probably",
    "look",
    "look",
    "vegetation",
    "urban",
    "population",
    "right",
    "much",
    "distance",
    "nearest",
    "modes",
    "transport",
    "like",
    "bus",
    "station",
    "railway",
    "station",
    "accordingly",
    "split",
    "parameters",
    "make",
    "decision",
    "one",
    "parameters",
    "finally",
    "compile",
    "decision",
    "parameters",
    "final",
    "outcome",
    "finally",
    "going",
    "predict",
    "whether",
    "put",
    "industry",
    "particular",
    "location",
    "right",
    "three",
    "examples",
    "actually",
    "majorly",
    "classification",
    "problem",
    "trying",
    "classify",
    "whether",
    "actually",
    "trying",
    "answer",
    "question",
    "whether",
    "right",
    "let",
    "move",
    "forward",
    "look",
    "marketing",
    "revolving",
    "around",
    "random",
    "forest",
    "particularly",
    "marketing",
    "try",
    "identify",
    "customer",
    "churn",
    "particularly",
    "regression",
    "kind",
    "problem",
    "right",
    "let",
    "see",
    "customer",
    "churn",
    "nothing",
    "actually",
    "number",
    "people",
    "actually",
    "number",
    "customers",
    "losing",
    "going",
    "market",
    "want",
    "identify",
    "customer",
    "churn",
    "near",
    "future",
    "industries",
    "actually",
    "using",
    "like",
    "amazon",
    "flipkart",
    "etc",
    "particularly",
    "look",
    "behavior",
    "past",
    "history",
    "purchasing",
    "history",
    "like",
    "based",
    "activity",
    "around",
    "certain",
    "things",
    "around",
    "certain",
    "ads",
    "around",
    "certain",
    "discounts",
    "certain",
    "kind",
    "materials",
    "right",
    "would",
    "like",
    "particular",
    "top",
    "activity",
    "around",
    "particular",
    "top",
    "track",
    "every",
    "particular",
    "move",
    "try",
    "predict",
    "whether",
    "moving",
    "identify",
    "customer",
    "churn",
    "various",
    "domains",
    "random",
    "forest",
    "used",
    "list",
    "numerous",
    "examples",
    "actually",
    "lee",
    "using",
    "random",
    "forests",
    "makes",
    "special",
    "actually",
    "let",
    "move",
    "forward",
    "see",
    "random",
    "forest",
    "actually",
    "works",
    "right",
    "let",
    "us",
    "start",
    "random",
    "forest",
    "algorithm",
    "first",
    "let",
    "see",
    "step",
    "step",
    "random",
    "forest",
    "algorithm",
    "works",
    "first",
    "step",
    "actually",
    "select",
    "certain",
    "features",
    "less",
    "total",
    "number",
    "predictor",
    "variables",
    "data",
    "set",
    "total",
    "predictor",
    "variables",
    "select",
    "random",
    "lisa",
    "um",
    "features",
    "actually",
    "selecting",
    "features",
    "reason",
    "select",
    "predictive",
    "variables",
    "total",
    "predictor",
    "variables",
    "decision",
    "tree",
    "model",
    "actually",
    "learning",
    "something",
    "new",
    "learning",
    "previous",
    "thing",
    "decision",
    "trees",
    "similar",
    "right",
    "actually",
    "split",
    "predicted",
    "variables",
    "select",
    "randomly",
    "predicted",
    "variables",
    "need",
    "let",
    "say",
    "14",
    "total",
    "number",
    "variables",
    "randomly",
    "pick",
    "three",
    "right",
    "every",
    "time",
    "get",
    "new",
    "decision",
    "tree",
    "variety",
    "right",
    "classification",
    "model",
    "actually",
    "much",
    "intelligent",
    "previous",
    "one",
    "got",
    "yet",
    "experiences",
    "definitely",
    "make",
    "different",
    "decisions",
    "time",
    "compile",
    "different",
    "decisions",
    "new",
    "accurate",
    "efficient",
    "result",
    "right",
    "first",
    "important",
    "step",
    "select",
    "certain",
    "number",
    "features",
    "features",
    "let",
    "move",
    "second",
    "step",
    "let",
    "say",
    "node",
    "first",
    "step",
    "calculate",
    "best",
    "plate",
    "point",
    "know",
    "decision",
    "tree",
    "decision",
    "trees",
    "actually",
    "implemented",
    "pick",
    "significant",
    "variable",
    "right",
    "split",
    "particular",
    "node",
    "child",
    "nodes",
    "split",
    "takes",
    "place",
    "right",
    "number",
    "variables",
    "selected",
    "let",
    "say",
    "selected",
    "three",
    "implement",
    "split",
    "three",
    "nodes",
    "one",
    "particular",
    "decision",
    "tree",
    "right",
    "third",
    "step",
    "split",
    "node",
    "two",
    "daughter",
    "nodes",
    "split",
    "root",
    "note",
    "many",
    "notes",
    "want",
    "split",
    "node",
    "notes",
    "answer",
    "terms",
    "right",
    "fourth",
    "step",
    "repeat",
    "three",
    "steps",
    "done",
    "previously",
    "repeat",
    "splitting",
    "reached",
    "n",
    "number",
    "nodes",
    "right",
    "need",
    "repeat",
    "reached",
    "till",
    "leaf",
    "nodes",
    "decision",
    "tree",
    "right",
    "four",
    "steps",
    "one",
    "decision",
    "tree",
    "random",
    "forest",
    "actually",
    "decision",
    "trees",
    "fifth",
    "step",
    "come",
    "picture",
    "actually",
    "repeat",
    "previous",
    "steps",
    "number",
    "times",
    "hit",
    "number",
    "decision",
    "trees",
    "let",
    "say",
    "want",
    "implement",
    "five",
    "decision",
    "trees",
    "first",
    "step",
    "implement",
    "previous",
    "steps",
    "5",
    "times",
    "head",
    "eye",
    "tration",
    "number",
    "times",
    "right",
    "created",
    "five",
    "decision",
    "trees",
    "still",
    "task",
    "completed",
    "pleat",
    "yet",
    "final",
    "task",
    "compile",
    "results",
    "five",
    "different",
    "decision",
    "trees",
    "make",
    "call",
    "majority",
    "voting",
    "right",
    "see",
    "picture",
    "different",
    "instances",
    "created",
    "indifferent",
    "decision",
    "trees",
    "finally",
    "compile",
    "result",
    "n",
    "different",
    "decision",
    "trees",
    "take",
    "call",
    "majority",
    "voting",
    "right",
    "whatever",
    "majority",
    "vote",
    "says",
    "final",
    "result",
    "basically",
    "overview",
    "random",
    "forest",
    "algorithm",
    "actually",
    "works",
    "let",
    "look",
    "example",
    "get",
    "much",
    "better",
    "understanding",
    "learnt",
    "let",
    "say",
    "data",
    "set",
    "consists",
    "four",
    "different",
    "instances",
    "right",
    "basically",
    "consists",
    "weather",
    "information",
    "previous",
    "14",
    "days",
    "right",
    "d1",
    "tildy",
    "14",
    "basically",
    "outlook",
    "humidity",
    "win",
    "basically",
    "gives",
    "weather",
    "condition",
    "14",
    "days",
    "finally",
    "play",
    "target",
    "variable",
    "weather",
    "match",
    "take",
    "place",
    "particular",
    "day",
    "right",
    "main",
    "goal",
    "find",
    "whether",
    "match",
    "actually",
    "take",
    "place",
    "following",
    "weather",
    "conditions",
    "particular",
    "day",
    "let",
    "say",
    "outlook",
    "rainy",
    "day",
    "humidity",
    "high",
    "wind",
    "weak",
    "need",
    "predict",
    "whether",
    "able",
    "play",
    "match",
    "right",
    "problem",
    "statement",
    "fine",
    "let",
    "see",
    "random",
    "forest",
    "used",
    "sort",
    "first",
    "step",
    "actually",
    "split",
    "entire",
    "data",
    "set",
    "subsets",
    "split",
    "entire",
    "14",
    "variables",
    "smaller",
    "subsets",
    "right",
    "subsets",
    "may",
    "may",
    "overlap",
    "like",
    "certain",
    "overlapping",
    "1",
    "till",
    "d3",
    "d3",
    "till",
    "d6",
    "fine",
    "overlapping",
    "d3",
    "might",
    "happen",
    "might",
    "overlapping",
    "need",
    "really",
    "worry",
    "overlapping",
    "make",
    "sure",
    "subsets",
    "actually",
    "different",
    "right",
    "taken",
    "three",
    "different",
    "subsets",
    "first",
    "sub",
    "set",
    "consists",
    "d1",
    "till",
    "d3",
    "mexican",
    "subset",
    "consists",
    "d3",
    "till",
    "d6",
    "methods",
    "subset",
    "consists",
    "d7",
    "tildy",
    "first",
    "focusing",
    "first",
    "subset",
    "let",
    "say",
    "particular",
    "day",
    "overcast",
    "fine",
    "yes",
    "overcast",
    "probabilities",
    "match",
    "take",
    "place",
    "overcast",
    "basically",
    "weather",
    "cloudy",
    "condition",
    "definitely",
    "match",
    "take",
    "place",
    "let",
    "say",
    "overcast",
    "consider",
    "second",
    "probable",
    "option",
    "wind",
    "make",
    "decision",
    "based",
    "whether",
    "wind",
    "weak",
    "strong",
    "wind",
    "weak",
    "definitely",
    "go",
    "play",
    "match",
    "else",
    "would",
    "final",
    "outcome",
    "decision",
    "tree",
    "play",
    "ratio",
    "play",
    "play",
    "1",
    "get",
    "certain",
    "decision",
    "first",
    "decision",
    "tree",
    "let",
    "us",
    "look",
    "second",
    "subset",
    "since",
    "second",
    "subset",
    "different",
    "number",
    "variables",
    "decision",
    "trees",
    "absolutely",
    "different",
    "saw",
    "four",
    "subsets",
    "let",
    "say",
    "overcast",
    "play",
    "match",
    "overcast",
    "would",
    "go",
    "look",
    "humidity",
    "get",
    "split",
    "two",
    "whether",
    "high",
    "normal",
    "take",
    "first",
    "case",
    "humidity",
    "high",
    "wind",
    "week",
    "play",
    "match",
    "else",
    "humidity",
    "high",
    "wind",
    "strong",
    "would",
    "go",
    "play",
    "match",
    "right",
    "let",
    "us",
    "look",
    "second",
    "dot",
    "node",
    "humidity",
    "humidity",
    "oil",
    "wind",
    "weak",
    "definitely",
    "go",
    "play",
    "match",
    "want",
    "go",
    "play",
    "match",
    "look",
    "final",
    "result",
    "ratio",
    "placed",
    "play",
    "3",
    "2",
    "final",
    "outcome",
    "actually",
    "play",
    "right",
    "second",
    "subset",
    "get",
    "final",
    "decision",
    "play",
    "let",
    "us",
    "look",
    "third",
    "subset",
    "consists",
    "d7",
    "till",
    "d9",
    "overcast",
    "yes",
    "match",
    "go",
    "check",
    "humidity",
    "humidity",
    "really",
    "high",
    "wo",
    "play",
    "match",
    "play",
    "match",
    "probability",
    "playing",
    "matches",
    "yes",
    "ratio",
    "play",
    "twist",
    "one",
    "right",
    "three",
    "different",
    "subsets",
    "three",
    "different",
    "decision",
    "trees",
    "three",
    "different",
    "outcomes",
    "one",
    "final",
    "outcome",
    "compiling",
    "results",
    "three",
    "different",
    "decision",
    "trees",
    "hope",
    "gives",
    "better",
    "perspective",
    "bit",
    "understanding",
    "random",
    "forest",
    "like",
    "really",
    "works",
    "right",
    "let",
    "look",
    "various",
    "features",
    "random",
    "forest",
    "ray",
    "first",
    "foremost",
    "feature",
    "one",
    "accurate",
    "learning",
    "algorithms",
    "right",
    "single",
    "decision",
    "trees",
    "actually",
    "prone",
    "high",
    "variance",
    "hive",
    "bias",
    "contrary",
    "actually",
    "random",
    "forest",
    "averages",
    "entire",
    "variance",
    "across",
    "decision",
    "trees",
    "let",
    "say",
    "variances",
    "say",
    "x4",
    "decision",
    "tree",
    "random",
    "forest",
    "let",
    "say",
    "implemented",
    "n",
    "number",
    "decision",
    "trees",
    "parallely",
    "entire",
    "variance",
    "gets",
    "averaged",
    "upon",
    "final",
    "variance",
    "actually",
    "becomes",
    "x",
    "upon",
    "n",
    "entire",
    "variance",
    "actually",
    "goes",
    "compared",
    "algorithms",
    "thumbs",
    "right",
    "second",
    "important",
    "feature",
    "works",
    "well",
    "classification",
    "regression",
    "problems",
    "far",
    "come",
    "across",
    "one",
    "algorithm",
    "works",
    "equally",
    "well",
    "beh",
    "classification",
    "kind",
    "problem",
    "regression",
    "kind",
    "problem",
    "right",
    "really",
    "runs",
    "efficient",
    "large",
    "databases",
    "basically",
    "really",
    "scalable",
    "even",
    "work",
    "lesser",
    "amount",
    "database",
    "work",
    "really",
    "huge",
    "volume",
    "data",
    "right",
    "good",
    "part",
    "fourth",
    "important",
    "point",
    "requires",
    "almost",
    "input",
    "preparation",
    "saying",
    "got",
    "certain",
    "implicit",
    "methods",
    "actually",
    "take",
    "care",
    "remove",
    "outliers",
    "missing",
    "data",
    "really",
    "take",
    "care",
    "thing",
    "stages",
    "input",
    "preparations",
    "random",
    "forest",
    "take",
    "care",
    "everything",
    "else",
    "next",
    "performs",
    "implicit",
    "feature",
    "selection",
    "right",
    "implementing",
    "multiple",
    "decision",
    "trees",
    "got",
    "implicit",
    "method",
    "automatically",
    "pick",
    "random",
    "features",
    "result",
    "parameters",
    "go",
    "implementing",
    "different",
    "decision",
    "trees",
    "example",
    "give",
    "one",
    "simple",
    "command",
    "right",
    "want",
    "implement",
    "500",
    "decision",
    "trees",
    "matter",
    "random",
    "forest",
    "automatically",
    "take",
    "care",
    "implement",
    "500",
    "decision",
    "trees",
    "500",
    "decision",
    "trees",
    "different",
    "got",
    "implicit",
    "methods",
    "automatically",
    "collect",
    "different",
    "parameters",
    "variables",
    "right",
    "easily",
    "grown",
    "parallel",
    "actually",
    "implementing",
    "multiple",
    "decision",
    "trees",
    "decision",
    "trees",
    "running",
    "decisions",
    "trees",
    "actually",
    "getting",
    "implemented",
    "parallely",
    "say",
    "want",
    "thousand",
    "trees",
    "implemented",
    "thousand",
    "trees",
    "getting",
    "implemented",
    "parallely",
    "computation",
    "time",
    "reduces",
    "right",
    "last",
    "point",
    "got",
    "methods",
    "balancing",
    "error",
    "unbalanced",
    "exactly",
    "unbalanced",
    "data",
    "sets",
    "let",
    "give",
    "example",
    "let",
    "say",
    "working",
    "data",
    "set",
    "fine",
    "create",
    "random",
    "forest",
    "model",
    "get",
    "90",
    "accuracy",
    "immediately",
    "fantastic",
    "think",
    "right",
    "start",
    "diving",
    "deep",
    "go",
    "little",
    "little",
    "deeper",
    "discovered",
    "ninety",
    "percent",
    "data",
    "actually",
    "belongs",
    "one",
    "class",
    "tan",
    "entire",
    "data",
    "set",
    "entire",
    "decision",
    "actually",
    "biased",
    "one",
    "particular",
    "class",
    "random",
    "forest",
    "actually",
    "takes",
    "care",
    "thing",
    "really",
    "biased",
    "towards",
    "particular",
    "decision",
    "tree",
    "particular",
    "variable",
    "class",
    "got",
    "methods",
    "looks",
    "balance",
    "errors",
    "data",
    "sets",
    "pretty",
    "much",
    "features",
    "random",
    "forests",
    "neighbor",
    "simple",
    "algorithm",
    "uses",
    "entire",
    "data",
    "set",
    "training",
    "phase",
    "prediction",
    "required",
    "unseen",
    "data",
    "searches",
    "entire",
    "training",
    "data",
    "set",
    "kaymu",
    "similar",
    "instances",
    "data",
    "similar",
    "instance",
    "finally",
    "returned",
    "prediction",
    "hello",
    "oh",
    "welcome",
    "youtube",
    "session",
    "today",
    "session",
    "dealing",
    "knn",
    "algorithm",
    "without",
    "let",
    "move",
    "discuss",
    "agenda",
    "today",
    "session",
    "start",
    "session",
    "kn",
    "brief",
    "topic",
    "move",
    "ahead",
    "see",
    "popular",
    "use",
    "cases",
    "industry",
    "using",
    "kn",
    "benefit",
    "done",
    "drill",
    "working",
    "algorithm",
    "learning",
    "algorithm",
    "also",
    "understand",
    "significance",
    "k",
    "case",
    "stands",
    "nearest",
    "neighbor",
    "algorithm",
    "see",
    "prediction",
    "made",
    "using",
    "canon",
    "algorithm",
    "manually",
    "mathematically",
    "right",
    "done",
    "theoretical",
    "concept",
    "start",
    "practical",
    "demo",
    "session",
    "learn",
    "implement",
    "knn",
    "algorithm",
    "using",
    "python",
    "let",
    "start",
    "session",
    "starting",
    "knn",
    "algorithm",
    "neighbor",
    "simple",
    "algorithm",
    "stores",
    "available",
    "cases",
    "classify",
    "new",
    "data",
    "case",
    "based",
    "similarity",
    "measure",
    "suggests",
    "similar",
    "neighbors",
    "one",
    "right",
    "example",
    "apple",
    "looks",
    "similar",
    "banana",
    "orange",
    "melon",
    "rather",
    "monkey",
    "rat",
    "cat",
    "likely",
    "apple",
    "belong",
    "group",
    "fruits",
    "right",
    "well",
    "general",
    "cayenne",
    "used",
    "search",
    "application",
    "looking",
    "similar",
    "items",
    "task",
    "form",
    "fine",
    "items",
    "similar",
    "one",
    "call",
    "search",
    "cayenne",
    "search",
    "kn",
    "kn",
    "well",
    "k",
    "denotes",
    "number",
    "nearest",
    "neighbor",
    "voting",
    "class",
    "new",
    "data",
    "testing",
    "data",
    "example",
    "k",
    "equal",
    "1",
    "sting",
    "data",
    "given",
    "label",
    "close",
    "example",
    "training",
    "set",
    "similarly",
    "k",
    "equal",
    "3",
    "labels",
    "three",
    "closes",
    "classes",
    "checked",
    "common",
    "label",
    "assigned",
    "testing",
    "data",
    "kn",
    "kn",
    "algorithm",
    "means",
    "moving",
    "ahead",
    "let",
    "see",
    "example",
    "scenarios",
    "kn",
    "used",
    "industry",
    "let",
    "see",
    "industrial",
    "application",
    "knn",
    "algorithm",
    "starting",
    "recommender",
    "system",
    "well",
    "biggest",
    "use",
    "case",
    "cayenne",
    "search",
    "recommender",
    "system",
    "thus",
    "recommended",
    "system",
    "like",
    "automated",
    "good",
    "form",
    "shop",
    "counter",
    "guy",
    "asked",
    "product",
    "shows",
    "product",
    "also",
    "suggest",
    "displays",
    "relevant",
    "set",
    "products",
    "related",
    "item",
    "already",
    "interested",
    "buying",
    "knn",
    "algorithm",
    "applies",
    "recommending",
    "products",
    "like",
    "amazon",
    "recommending",
    "media",
    "like",
    "case",
    "netflix",
    "even",
    "recommending",
    "advertisement",
    "display",
    "user",
    "wrong",
    "almost",
    "must",
    "used",
    "amazon",
    "shopping",
    "right",
    "tell",
    "35",
    "revenue",
    "generated",
    "recommendation",
    "engine",
    "strategy",
    "amazon",
    "uses",
    "recommendation",
    "targeted",
    "marketing",
    "tool",
    "email",
    "campaigns",
    "around",
    "website",
    "pages",
    "amazon",
    "recommend",
    "many",
    "products",
    "different",
    "categories",
    "based",
    "browser",
    "pull",
    "products",
    "front",
    "likely",
    "buy",
    "like",
    "frequently",
    "bought",
    "together",
    "option",
    "comes",
    "bottom",
    "product",
    "page",
    "tempt",
    "buying",
    "combo",
    "well",
    "recommendation",
    "one",
    "main",
    "goal",
    "increase",
    "average",
    "order",
    "value",
    "upsell",
    "customers",
    "providing",
    "product",
    "suggestions",
    "eastern",
    "items",
    "shopping",
    "cart",
    "based",
    "product",
    "currently",
    "looking",
    "site",
    "next",
    "industrial",
    "application",
    "knn",
    "algorithm",
    "concept",
    "search",
    "searching",
    "semantically",
    "similar",
    "documents",
    "classifying",
    "documents",
    "containing",
    "similar",
    "topics",
    "know",
    "data",
    "internet",
    "increasing",
    "exponentially",
    "every",
    "single",
    "second",
    "billions",
    "billions",
    "documents",
    "internet",
    "document",
    "internet",
    "contains",
    "multiple",
    "concepts",
    "could",
    "potential",
    "concept",
    "situation",
    "main",
    "problem",
    "extract",
    "concept",
    "set",
    "documents",
    "page",
    "could",
    "thousands",
    "combination",
    "could",
    "potential",
    "concepts",
    "average",
    "document",
    "could",
    "millions",
    "concept",
    "combined",
    "vast",
    "amount",
    "data",
    "web",
    "well",
    "talking",
    "enormous",
    "amount",
    "data",
    "set",
    "sample",
    "need",
    "need",
    "find",
    "concept",
    "enormous",
    "amount",
    "data",
    "set",
    "samples",
    "right",
    "purpose",
    "using",
    "knn",
    "algorithm",
    "advanced",
    "example",
    "could",
    "include",
    "handwriting",
    "detection",
    "like",
    "ocr",
    "image",
    "recognization",
    "even",
    "video",
    "organization",
    "right",
    "know",
    "various",
    "use",
    "cases",
    "knn",
    "algorithm",
    "let",
    "proceed",
    "see",
    "work",
    "knn",
    "algorithm",
    "work",
    "let",
    "start",
    "plotting",
    "blue",
    "orange",
    "point",
    "graph",
    "blue",
    "points",
    "belong",
    "class",
    "orange",
    "ones",
    "belong",
    "class",
    "get",
    "star",
    "new",
    "pony",
    "task",
    "predict",
    "whether",
    "new",
    "point",
    "belongs",
    "class",
    "belongs",
    "class",
    "start",
    "production",
    "first",
    "thing",
    "select",
    "value",
    "told",
    "kn",
    "kn",
    "algorithm",
    "refers",
    "number",
    "nearest",
    "neighbors",
    "want",
    "select",
    "example",
    "case",
    "k",
    "equal",
    "mean",
    "means",
    "selecting",
    "three",
    "points",
    "least",
    "distance",
    "new",
    "point",
    "say",
    "selecting",
    "three",
    "different",
    "points",
    "closest",
    "star",
    "well",
    "point",
    "time",
    "ask",
    "calculate",
    "least",
    "distance",
    "calculate",
    "distance",
    "get",
    "one",
    "blue",
    "two",
    "orange",
    "points",
    "closest",
    "star",
    "since",
    "case",
    "majority",
    "orange",
    "points",
    "say",
    "k",
    "equal",
    "3d",
    "star",
    "belongs",
    "class",
    "b",
    "say",
    "star",
    "similar",
    "orange",
    "points",
    "moving",
    "ahead",
    "well",
    "k",
    "equal",
    "6",
    "well",
    "case",
    "look",
    "six",
    "different",
    "points",
    "closest",
    "star",
    "case",
    "calculating",
    "distance",
    "find",
    "four",
    "blue",
    "points",
    "two",
    "orange",
    "point",
    "closest",
    "star",
    "see",
    "blue",
    "points",
    "majority",
    "say",
    "k",
    "equals",
    "6",
    "star",
    "belongs",
    "class",
    "star",
    "similar",
    "blue",
    "points",
    "guess",
    "know",
    "knn",
    "algorithm",
    "work",
    "significance",
    "gain",
    "knn",
    "algorithm",
    "choose",
    "value",
    "k",
    "keeping",
    "mind",
    "case",
    "important",
    "parameter",
    "knn",
    "algorithm",
    "let",
    "see",
    "build",
    "k",
    "nearest",
    "neighbor",
    "classifier",
    "choose",
    "value",
    "k",
    "well",
    "might",
    "specific",
    "value",
    "k",
    "mind",
    "could",
    "divide",
    "data",
    "use",
    "something",
    "like",
    "technique",
    "test",
    "several",
    "values",
    "k",
    "order",
    "determine",
    "works",
    "best",
    "data",
    "example",
    "n",
    "equal",
    "cases",
    "case",
    "optimal",
    "value",
    "k",
    "lies",
    "somewhere",
    "1",
    "yes",
    "unless",
    "try",
    "sure",
    "know",
    "algorithm",
    "working",
    "higher",
    "level",
    "let",
    "move",
    "see",
    "things",
    "predicted",
    "using",
    "knn",
    "algorithm",
    "remember",
    "told",
    "knn",
    "algorithm",
    "uses",
    "least",
    "distance",
    "measure",
    "order",
    "find",
    "nearest",
    "neighbors",
    "let",
    "see",
    "distance",
    "calculated",
    "well",
    "several",
    "distance",
    "measure",
    "used",
    "start",
    "mainly",
    "focus",
    "euclidean",
    "distance",
    "manhattan",
    "distance",
    "session",
    "euclidean",
    "distance",
    "well",
    "euclidean",
    "distance",
    "defined",
    "square",
    "root",
    "sum",
    "difference",
    "new",
    "point",
    "x",
    "existing",
    "point",
    "example",
    "point",
    "p1",
    "p2",
    "point",
    "1",
    "1",
    "1",
    "point",
    "p",
    "2",
    "5",
    "euclidean",
    "distance",
    "see",
    "euclidean",
    "distance",
    "direct",
    "distance",
    "two",
    "points",
    "distance",
    "point",
    "p1",
    "p2",
    "calculate",
    "5",
    "minus",
    "1",
    "whole",
    "square",
    "plus",
    "4",
    "minus",
    "1",
    "whole",
    "square",
    "route",
    "results",
    "next",
    "manhattan",
    "distance",
    "well",
    "manhattan",
    "distance",
    "used",
    "calculate",
    "distance",
    "real",
    "vector",
    "using",
    "absolute",
    "difference",
    "case",
    "manhattan",
    "distance",
    "point",
    "p1",
    "p2",
    "mode",
    "5",
    "minus",
    "1",
    "plus",
    "mod",
    "value",
    "4",
    "minus",
    "1",
    "results",
    "3",
    "plus",
    "4",
    "7",
    "slide",
    "shows",
    "difference",
    "euclidean",
    "manhattan",
    "distance",
    "point",
    "point",
    "euclidean",
    "distance",
    "nothing",
    "direct",
    "least",
    "possible",
    "distance",
    "whereas",
    "manhattan",
    "distance",
    "distance",
    "b",
    "measured",
    "along",
    "axis",
    "right",
    "angle",
    "let",
    "take",
    "example",
    "see",
    "things",
    "predicted",
    "using",
    "knn",
    "algorithm",
    "cannon",
    "algorithm",
    "working",
    "suppose",
    "data",
    "set",
    "consists",
    "height",
    "weight",
    "size",
    "customers",
    "new",
    "customer",
    "come",
    "height",
    "weight",
    "information",
    "task",
    "predict",
    "size",
    "particular",
    "customer",
    "using",
    "knn",
    "algorithm",
    "first",
    "thing",
    "need",
    "need",
    "calculate",
    "euclidean",
    "distance",
    "new",
    "data",
    "height",
    "160",
    "one",
    "centimeter",
    "weight",
    "61",
    "kg",
    "first",
    "thing",
    "calculate",
    "euclidean",
    "distance",
    "stance",
    "nothing",
    "square",
    "root",
    "160",
    "1",
    "minus",
    "158",
    "whole",
    "square",
    "plus",
    "61",
    "minus",
    "58",
    "whole",
    "square",
    "square",
    "root",
    "let",
    "drag",
    "drop",
    "various",
    "euclidean",
    "distance",
    "points",
    "let",
    "suppose",
    "k",
    "equal",
    "5",
    "algorithm",
    "searches",
    "five",
    "customer",
    "closest",
    "new",
    "customer",
    "similar",
    "new",
    "data",
    "terms",
    "attribute",
    "k",
    "equal",
    "let",
    "find",
    "top",
    "five",
    "minimum",
    "euclidian",
    "distance",
    "distance",
    "going",
    "use",
    "two",
    "three",
    "four",
    "five",
    "let",
    "rank",
    "order",
    "first",
    "second",
    "third",
    "one",
    "one",
    "5",
    "order",
    "k",
    "equal",
    "5",
    "commanders",
    "size",
    "one",
    "comes",
    "size",
    "l",
    "obviously",
    "best",
    "guess",
    "best",
    "protection",
    "size",
    "height",
    "160",
    "one",
    "centimeter",
    "wait",
    "60",
    "1",
    "kg",
    "say",
    "new",
    "customer",
    "fittin",
    "size",
    "well",
    "body",
    "theoretical",
    "session",
    "drill",
    "coding",
    "part",
    "let",
    "tell",
    "people",
    "call",
    "kn",
    "lazy",
    "learner",
    "well",
    "cannon",
    "classification",
    "simple",
    "algorithm",
    "called",
    "lazy",
    "kn",
    "lazy",
    "learner",
    "discriminative",
    "function",
    "training",
    "data",
    "memorizes",
    "training",
    "data",
    "learning",
    "phase",
    "model",
    "work",
    "happens",
    "time",
    "prediction",
    "requested",
    "reason",
    "kn",
    "often",
    "referred",
    "us",
    "lazy",
    "learning",
    "algorithm",
    "detail",
    "reticle",
    "session",
    "let",
    "move",
    "coding",
    "part",
    "practical",
    "implementation",
    "part",
    "using",
    "irs",
    "data",
    "set",
    "data",
    "set",
    "consists",
    "150",
    "observation",
    "four",
    "features",
    "one",
    "class",
    "label",
    "four",
    "features",
    "include",
    "sepal",
    "length",
    "sepal",
    "width",
    "petal",
    "length",
    "petrol",
    "head",
    "whereas",
    "class",
    "label",
    "decides",
    "flower",
    "belongs",
    "category",
    "description",
    "data",
    "set",
    "using",
    "let",
    "move",
    "see",
    "step",
    "step",
    "solution",
    "perform",
    "knn",
    "algorithm",
    "first",
    "start",
    "handling",
    "data",
    "open",
    "data",
    "set",
    "csv",
    "format",
    "split",
    "data",
    "set",
    "train",
    "test",
    "part",
    "next",
    "take",
    "similarity",
    "calculate",
    "distance",
    "two",
    "data",
    "instances",
    "calculate",
    "distance",
    "next",
    "look",
    "neighbor",
    "select",
    "k",
    "neighbors",
    "least",
    "distance",
    "new",
    "point",
    "get",
    "neighbor",
    "generate",
    "response",
    "set",
    "data",
    "instances",
    "decide",
    "whether",
    "new",
    "point",
    "belongs",
    "class",
    "class",
    "finally",
    "create",
    "accuracy",
    "function",
    "end",
    "tie",
    "together",
    "main",
    "function",
    "let",
    "start",
    "code",
    "implementing",
    "knn",
    "algorithm",
    "using",
    "python",
    "using",
    "jupyter",
    "notebook",
    "python",
    "installed",
    "let",
    "move",
    "see",
    "algorithm",
    "implemented",
    "using",
    "python",
    "jupyter",
    "notebook",
    "interactive",
    "computing",
    "notebook",
    "environment",
    "python",
    "installed",
    "launched",
    "launching",
    "jupyter",
    "notebook",
    "riding",
    "python",
    "codes",
    "first",
    "thing",
    "need",
    "load",
    "file",
    "data",
    "csv",
    "format",
    "without",
    "header",
    "line",
    "code",
    "open",
    "file",
    "open",
    "function",
    "read",
    "data",
    "line",
    "using",
    "reader",
    "function",
    "csv",
    "module",
    "let",
    "write",
    "code",
    "load",
    "data",
    "file",
    "let",
    "execute",
    "run",
    "button",
    "execute",
    "run",
    "button",
    "see",
    "entire",
    "training",
    "data",
    "set",
    "output",
    "next",
    "need",
    "split",
    "data",
    "training",
    "data",
    "set",
    "kn",
    "use",
    "make",
    "prediction",
    "test",
    "data",
    "set",
    "use",
    "evaluate",
    "accuracy",
    "module",
    "first",
    "need",
    "convert",
    "flower",
    "measure",
    "loaded",
    "string",
    "numbers",
    "work",
    "next",
    "need",
    "split",
    "data",
    "set",
    "randomly",
    "train",
    "test",
    "ratio",
    "67",
    "233",
    "test",
    "train",
    "standard",
    "ratio",
    "used",
    "purpose",
    "let",
    "define",
    "function",
    "load",
    "data",
    "set",
    "loads",
    "csv",
    "provided",
    "file",
    "named",
    "split",
    "randomly",
    "training",
    "test",
    "data",
    "set",
    "using",
    "provided",
    "split",
    "ratio",
    "function",
    "load",
    "data",
    "set",
    "using",
    "filenames",
    "ratio",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "input",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "check",
    "errors",
    "executed",
    "zero",
    "errors",
    "let",
    "test",
    "function",
    "training",
    "set",
    "testing",
    "set",
    "load",
    "data",
    "set",
    "function",
    "load",
    "data",
    "set",
    "inside",
    "passing",
    "file",
    "data",
    "split",
    "ratio",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "let",
    "see",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "dividing",
    "giving",
    "count",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "total",
    "number",
    "training",
    "data",
    "set",
    "split",
    "97",
    "total",
    "number",
    "test",
    "data",
    "set",
    "total",
    "number",
    "training",
    "data",
    "set",
    "97",
    "total",
    "number",
    "test",
    "data",
    "set",
    "right",
    "okay",
    "function",
    "load",
    "data",
    "set",
    "performing",
    "well",
    "let",
    "move",
    "step",
    "two",
    "similarity",
    "order",
    "make",
    "prediction",
    "need",
    "calculate",
    "similarity",
    "two",
    "given",
    "data",
    "instances",
    "needed",
    "locate",
    "kamo",
    "similar",
    "data",
    "instances",
    "training",
    "data",
    "set",
    "turn",
    "make",
    "prediction",
    "given",
    "flour",
    "measurement",
    "numeric",
    "unit",
    "directly",
    "use",
    "euclidean",
    "distance",
    "measure",
    "nothing",
    "square",
    "root",
    "sum",
    "squared",
    "differences",
    "two",
    "eras",
    "number",
    "given",
    "flower",
    "measurements",
    "numeric",
    "unit",
    "directly",
    "use",
    "euclidean",
    "distance",
    "measure",
    "nothing",
    "square",
    "root",
    "sum",
    "squared",
    "difference",
    "two",
    "arrays",
    "number",
    "additionally",
    "want",
    "control",
    "field",
    "include",
    "distance",
    "calculation",
    "specifically",
    "want",
    "include",
    "first",
    "attribute",
    "approach",
    "limit",
    "euclidean",
    "distance",
    "fixed",
    "length",
    "right",
    "let",
    "define",
    "euclidean",
    "function",
    "euclidean",
    "distance",
    "function",
    "takes",
    "instance",
    "one",
    "instance",
    "length",
    "parameters",
    "instance",
    "one",
    "instance",
    "two",
    "two",
    "points",
    "want",
    "calculate",
    "euclidean",
    "distance",
    "whereas",
    "length",
    "denote",
    "many",
    "attributes",
    "want",
    "include",
    "okay",
    "euclidean",
    "function",
    "let",
    "execute",
    "executing",
    "fine",
    "without",
    "errors",
    "let",
    "test",
    "function",
    "suppose",
    "data",
    "one",
    "first",
    "instance",
    "consists",
    "data",
    "point",
    "us",
    "belongs",
    "class",
    "data",
    "consist",
    "four",
    "four",
    "belongs",
    "class",
    "calculate",
    "euclidean",
    "distance",
    "data",
    "one",
    "data",
    "consider",
    "first",
    "three",
    "features",
    "right",
    "let",
    "print",
    "distance",
    "see",
    "distance",
    "comes",
    "three",
    "point",
    "four",
    "six",
    "four",
    "right",
    "nothing",
    "square",
    "root",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "distance",
    "nothing",
    "euclidean",
    "distance",
    "calculated",
    "square",
    "root",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "plus",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "nothing",
    "3",
    "times",
    "4",
    "minus",
    "2",
    "whole",
    "12",
    "square",
    "root",
    "12",
    "nothing",
    "right",
    "calculated",
    "distance",
    "need",
    "look",
    "k",
    "nearest",
    "neighbors",
    "similarity",
    "measure",
    "use",
    "collect",
    "kamo",
    "similar",
    "instances",
    "given",
    "unseen",
    "instance",
    "well",
    "straightforward",
    "process",
    "calculating",
    "distance",
    "instances",
    "selecting",
    "subset",
    "smallest",
    "distance",
    "value",
    "select",
    "smallest",
    "distance",
    "values",
    "defining",
    "function",
    "get",
    "neighbors",
    "defining",
    "function",
    "get",
    "neighbors",
    "return",
    "k",
    "similar",
    "neighbors",
    "training",
    "set",
    "given",
    "test",
    "instance",
    "right",
    "get",
    "nabal",
    "function",
    "look",
    "like",
    "takes",
    "training",
    "data",
    "set",
    "test",
    "instance",
    "k",
    "input",
    "k",
    "nothing",
    "number",
    "nearest",
    "neighbor",
    "want",
    "check",
    "right",
    "basically",
    "getting",
    "get",
    "mabel",
    "function",
    "k",
    "different",
    "points",
    "least",
    "euclidean",
    "distance",
    "test",
    "instance",
    "right",
    "let",
    "execute",
    "function",
    "executed",
    "without",
    "errors",
    "let",
    "test",
    "function",
    "suppose",
    "training",
    "data",
    "set",
    "includes",
    "data",
    "like",
    "2",
    "2",
    "belongs",
    "class",
    "data",
    "includes",
    "four",
    "four",
    "four",
    "belongs",
    "class",
    "p",
    "testing",
    "instances",
    "five",
    "five",
    "five",
    "predict",
    "whether",
    "test",
    "instance",
    "belongs",
    "class",
    "belongs",
    "class",
    "right",
    "k",
    "equal",
    "1",
    "predict",
    "nearest",
    "neighbor",
    "predict",
    "whether",
    "test",
    "instance",
    "belong",
    "class",
    "belong",
    "class",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "aligned",
    "executing",
    "run",
    "button",
    "see",
    "output",
    "4",
    "4",
    "4",
    "new",
    "instance",
    "5",
    "5",
    "5",
    "closest",
    "point",
    "4",
    "4",
    "4",
    "belongs",
    "class",
    "right",
    "located",
    "similar",
    "neighbor",
    "test",
    "instance",
    "next",
    "task",
    "predict",
    "response",
    "based",
    "neighbors",
    "well",
    "allowing",
    "neighbor",
    "vote",
    "class",
    "attribute",
    "take",
    "majority",
    "vote",
    "prediction",
    "let",
    "see",
    "function",
    "getresponse",
    "takes",
    "neighbors",
    "input",
    "well",
    "neighbor",
    "nothing",
    "output",
    "get",
    "function",
    "output",
    "get",
    "neighbor",
    "function",
    "fed",
    "get",
    "response",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "executed",
    "let",
    "move",
    "ahead",
    "test",
    "function",
    "get",
    "response",
    "neighbor",
    "one",
    "one",
    "one",
    "belongs",
    "class",
    "belongs",
    "class",
    "a33",
    "belongs",
    "class",
    "response",
    "store",
    "value",
    "get",
    "response",
    "passing",
    "neighbor",
    "value",
    "right",
    "want",
    "check",
    "want",
    "predict",
    "whether",
    "test",
    "instance",
    "five",
    "five",
    "five",
    "belongs",
    "class",
    "class",
    "neighbors",
    "1",
    "1",
    "1",
    "2",
    "2",
    "3",
    "3",
    "let",
    "check",
    "response",
    "created",
    "different",
    "function",
    "required",
    "knn",
    "algorithm",
    "important",
    "main",
    "concern",
    "evaluate",
    "accuracy",
    "prediction",
    "easy",
    "way",
    "evaluate",
    "accuracy",
    "model",
    "calculate",
    "ratio",
    "total",
    "correct",
    "prediction",
    "prediction",
    "made",
    "defining",
    "function",
    "get",
    "accuracy",
    "inside",
    "passing",
    "test",
    "data",
    "set",
    "predictions",
    "get",
    "accuracy",
    "function",
    "check",
    "executed",
    "without",
    "error",
    "let",
    "check",
    "sample",
    "data",
    "set",
    "test",
    "data",
    "set",
    "1",
    "1",
    "1",
    "belongs",
    "class",
    "belongs",
    "class",
    "3",
    "3",
    "3",
    "belongs",
    "class",
    "b",
    "predictions",
    "first",
    "test",
    "data",
    "predicted",
    "latter",
    "belongs",
    "class",
    "true",
    "next",
    "predicted",
    "belongs",
    "class",
    "e",
    "next",
    "predictive",
    "belongs",
    "class",
    "false",
    "case",
    "cause",
    "test",
    "data",
    "belongs",
    "class",
    "right",
    "total",
    "correct",
    "prediction",
    "three",
    "right",
    "right",
    "ratio",
    "2",
    "3",
    "nothing",
    "accuracy",
    "rate",
    "created",
    "function",
    "required",
    "knn",
    "algorithm",
    "let",
    "compile",
    "one",
    "single",
    "main",
    "function",
    "alright",
    "main",
    "function",
    "using",
    "iris",
    "data",
    "set",
    "split",
    "value",
    "k",
    "3",
    "let",
    "see",
    "accuracy",
    "score",
    "check",
    "accurate",
    "modulus",
    "training",
    "data",
    "set",
    "113",
    "values",
    "test",
    "data",
    "set",
    "seven",
    "values",
    "predicted",
    "actual",
    "values",
    "output",
    "okay",
    "total",
    "got",
    "accuracy",
    "ninety",
    "seven",
    "point",
    "two",
    "nine",
    "percent",
    "really",
    "good",
    "alright",
    "hope",
    "concept",
    "knn",
    "algorithm",
    "devised",
    "world",
    "full",
    "machine",
    "learning",
    "artificial",
    "intelligence",
    "surrounding",
    "almost",
    "everything",
    "around",
    "us",
    "classification",
    "prediction",
    "one",
    "important",
    "aspects",
    "machine",
    "learning",
    "moving",
    "forward",
    "let",
    "look",
    "agenda",
    "start",
    "video",
    "explaining",
    "guys",
    "exactly",
    "nave",
    "biased",
    "understand",
    "space",
    "theorem",
    "serves",
    "logic",
    "behind",
    "name",
    "pass",
    "algorithm",
    "moving",
    "forward",
    "explain",
    "steps",
    "involved",
    "neighb",
    "algorithm",
    "one",
    "one",
    "finally",
    "finish",
    "video",
    "demo",
    "nave",
    "bass",
    "using",
    "sklearn",
    "package",
    "noun",
    "bass",
    "simple",
    "surprisingly",
    "powerful",
    "algorithm",
    "predictive",
    "analysis",
    "classification",
    "technique",
    "based",
    "base",
    "assumption",
    "independence",
    "among",
    "predictors",
    "comprises",
    "two",
    "parts",
    "knave",
    "bias",
    "simple",
    "terms",
    "neighbors",
    "classifier",
    "assumes",
    "presence",
    "particular",
    "feature",
    "class",
    "unrelated",
    "presence",
    "feature",
    "even",
    "features",
    "depend",
    "upon",
    "existence",
    "features",
    "properties",
    "independently",
    "contribute",
    "probability",
    "whether",
    "fruit",
    "apple",
    "orange",
    "banana",
    "known",
    "naive",
    "naive",
    "based",
    "model",
    "easy",
    "build",
    "particularly",
    "useful",
    "large",
    "data",
    "sets",
    "probability",
    "theory",
    "statistics",
    "based",
    "theorem",
    "already",
    "known",
    "base",
    "law",
    "base",
    "rule",
    "describes",
    "probability",
    "event",
    "based",
    "prior",
    "knowledge",
    "conditions",
    "might",
    "related",
    "event",
    "pasted",
    "way",
    "figure",
    "conditional",
    "probability",
    "conditional",
    "probability",
    "probability",
    "event",
    "happening",
    "given",
    "relationship",
    "one",
    "events",
    "example",
    "probability",
    "getting",
    "parking",
    "space",
    "connected",
    "time",
    "day",
    "park",
    "park",
    "conventions",
    "going",
    "time",
    "bayes",
    "theorem",
    "slightly",
    "nuanced",
    "nutshell",
    "gives",
    "actual",
    "probability",
    "event",
    "given",
    "information",
    "tests",
    "look",
    "definition",
    "bayes",
    "theorem",
    "see",
    "given",
    "hypothesis",
    "h",
    "evidence",
    "term",
    "states",
    "relationship",
    "e",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "probability",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "given",
    "e",
    "defined",
    "probability",
    "e",
    "given",
    "h",
    "probability",
    "h",
    "divided",
    "probability",
    "e",
    "rather",
    "confusing",
    "right",
    "let",
    "take",
    "example",
    "understand",
    "theorem",
    "suppose",
    "deck",
    "cards",
    "single",
    "card",
    "drawn",
    "deck",
    "playing",
    "cards",
    "probability",
    "card",
    "king",
    "52",
    "since",
    "four",
    "kings",
    "standard",
    "deck",
    "52",
    "cards",
    "king",
    "event",
    "card",
    "king",
    "probability",
    "king",
    "given",
    "4",
    "52",
    "equal",
    "1",
    "evidence",
    "provided",
    "instance",
    "someone",
    "looks",
    "card",
    "single",
    "card",
    "face",
    "card",
    "probability",
    "king",
    "given",
    "face",
    "calculated",
    "using",
    "base",
    "theorem",
    "formula",
    "since",
    "every",
    "king",
    "also",
    "face",
    "card",
    "probability",
    "face",
    "given",
    "king",
    "equal",
    "1",
    "since",
    "three",
    "phase",
    "cards",
    "suit",
    "chat",
    "king",
    "queen",
    "probability",
    "face",
    "card",
    "equal",
    "12",
    "3",
    "using",
    "base",
    "certain",
    "find",
    "probability",
    "king",
    "given",
    "face",
    "final",
    "answer",
    "comes",
    "1",
    "3",
    "also",
    "true",
    "deck",
    "cards",
    "faces",
    "three",
    "types",
    "phases",
    "chat",
    "king",
    "queen",
    "probability",
    "king",
    "1",
    "simple",
    "example",
    "based",
    "works",
    "look",
    "proof",
    "paste",
    "serum",
    "evolved",
    "probability",
    "given",
    "b",
    "probability",
    "b",
    "given",
    "joint",
    "probability",
    "distribution",
    "sets",
    "b",
    "probability",
    "intersection",
    "b",
    "conditional",
    "probability",
    "given",
    "b",
    "defined",
    "probability",
    "intersection",
    "b",
    "divided",
    "probability",
    "b",
    "similarly",
    "probability",
    "b",
    "given",
    "defined",
    "probability",
    "b",
    "intersection",
    "divided",
    "probability",
    "equate",
    "probability",
    "intersection",
    "p",
    "probability",
    "b",
    "intersection",
    "thing",
    "method",
    "see",
    "get",
    "final",
    "base",
    "theorem",
    "proof",
    "probability",
    "given",
    "b",
    "equals",
    "probability",
    "b",
    "given",
    "probability",
    "p",
    "divided",
    "probability",
    "equation",
    "applies",
    "probability",
    "distribution",
    "events",
    "particular",
    "nice",
    "interpretation",
    "case",
    "represented",
    "hypothesis",
    "h",
    "h",
    "b",
    "represented",
    "observed",
    "evidence",
    "e",
    "case",
    "formula",
    "p",
    "h",
    "given",
    "e",
    "equal",
    "p",
    "e",
    "given",
    "h",
    "probability",
    "h",
    "divided",
    "probability",
    "e",
    "relates",
    "probability",
    "hypothesis",
    "cutting",
    "evidence",
    "p",
    "h",
    "probability",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "given",
    "e",
    "reason",
    "p",
    "h",
    "known",
    "prior",
    "probability",
    "p",
    "given",
    "e",
    "known",
    "posterior",
    "probability",
    "factor",
    "relates",
    "two",
    "known",
    "likelihood",
    "ratio",
    "using",
    "term",
    "space",
    "theorem",
    "rephrased",
    "procedure",
    "probability",
    "equals",
    "prior",
    "probability",
    "times",
    "likelihood",
    "ratio",
    "know",
    "maths",
    "involved",
    "behind",
    "bayes",
    "theorem",
    "let",
    "see",
    "implement",
    "real",
    "life",
    "scenario",
    "suppose",
    "data",
    "set",
    "set",
    "outlook",
    "humidity",
    "need",
    "find",
    "whether",
    "play",
    "day",
    "outlook",
    "sunny",
    "overcast",
    "rain",
    "humidity",
    "high",
    "normal",
    "wind",
    "categorized",
    "two",
    "phases",
    "weak",
    "strong",
    "winds",
    "first",
    "create",
    "frequency",
    "table",
    "using",
    "attribute",
    "data",
    "set",
    "frequency",
    "table",
    "outlook",
    "looks",
    "like",
    "sunny",
    "overcast",
    "rainy",
    "frequency",
    "table",
    "humidity",
    "looks",
    "like",
    "frequency",
    "table",
    "looks",
    "like",
    "strong",
    "weak",
    "wind",
    "high",
    "normal",
    "ranges",
    "humidity",
    "frequency",
    "table",
    "generate",
    "likelihood",
    "table",
    "likelihood",
    "table",
    "contains",
    "probability",
    "particular",
    "day",
    "suppose",
    "take",
    "sunny",
    "take",
    "play",
    "yes",
    "probability",
    "sunny",
    "given",
    "play",
    "yes",
    "3",
    "10",
    "probability",
    "x",
    "probability",
    "sunny",
    "equal",
    "5",
    "terms",
    "generated",
    "data",
    "finally",
    "probability",
    "yes",
    "10",
    "look",
    "likelihood",
    "yes",
    "given",
    "sunny",
    "see",
    "using",
    "bayes",
    "theorem",
    "probability",
    "sunny",
    "given",
    "yes",
    "probability",
    "yes",
    "divided",
    "probability",
    "sunny",
    "values",
    "calculated",
    "put",
    "base",
    "serum",
    "equation",
    "get",
    "likelihood",
    "similarly",
    "likelihood",
    "also",
    "calculated",
    "similarly",
    "going",
    "create",
    "likelihood",
    "table",
    "humidity",
    "win",
    "humidity",
    "likelihood",
    "yes",
    "given",
    "humidity",
    "high",
    "equal",
    "probability",
    "playing",
    "know",
    "given",
    "venice",
    "high",
    "similarly",
    "table",
    "wind",
    "probability",
    "e",
    "given",
    "wind",
    "week",
    "probability",
    "given",
    "win",
    "week",
    "suppose",
    "day",
    "high",
    "rain",
    "high",
    "humidity",
    "wind",
    "weak",
    "play",
    "use",
    "base",
    "theorem",
    "likelihood",
    "yes",
    "day",
    "equal",
    "probability",
    "outlook",
    "rain",
    "given",
    "yes",
    "probability",
    "humidity",
    "given",
    "say",
    "yes",
    "probability",
    "given",
    "playing",
    "yes",
    "probability",
    "yes",
    "equals",
    "zero",
    "point",
    "zero",
    "one",
    "nine",
    "similarly",
    "likelihood",
    "know",
    "day",
    "equal",
    "zero",
    "point",
    "zero",
    "one",
    "six",
    "look",
    "probability",
    "yes",
    "day",
    "playing",
    "need",
    "divide",
    "likelihood",
    "yes",
    "probability",
    "playing",
    "tomorrow",
    "yes",
    "whereas",
    "probability",
    "playing",
    "equal",
    "based",
    "upon",
    "data",
    "already",
    "us",
    "idea",
    "exactly",
    "named",
    "works",
    "seen",
    "implemented",
    "particular",
    "data",
    "set",
    "let",
    "see",
    "used",
    "industry",
    "started",
    "first",
    "industrial",
    "use",
    "case",
    "news",
    "categorized",
    "move",
    "use",
    "term",
    "text",
    "classification",
    "broaden",
    "spectrum",
    "algorithm",
    "news",
    "web",
    "rapidly",
    "growing",
    "era",
    "information",
    "age",
    "new",
    "site",
    "different",
    "layout",
    "categorization",
    "grouping",
    "news",
    "heterogeneity",
    "layout",
    "categorization",
    "always",
    "satisfy",
    "individual",
    "users",
    "need",
    "remove",
    "heterogeneity",
    "classifying",
    "news",
    "articles",
    "owing",
    "user",
    "preference",
    "formidable",
    "task",
    "companies",
    "use",
    "web",
    "crawler",
    "extract",
    "useful",
    "text",
    "html",
    "pages",
    "news",
    "articles",
    "news",
    "articles",
    "tokenized",
    "tokens",
    "nothing",
    "categories",
    "news",
    "order",
    "achieve",
    "better",
    "classification",
    "result",
    "remove",
    "less",
    "significant",
    "words",
    "stop",
    "documents",
    "articles",
    "apply",
    "nave",
    "base",
    "classifier",
    "classifying",
    "news",
    "contents",
    "based",
    "news",
    "far",
    "one",
    "best",
    "examples",
    "neighbors",
    "classifier",
    "spam",
    "filtering",
    "nave",
    "bayes",
    "classifier",
    "popular",
    "statistical",
    "technique",
    "email",
    "filtering",
    "typically",
    "use",
    "features",
    "identify",
    "spam",
    "email",
    "approach",
    "commonly",
    "used",
    "text",
    "classification",
    "well",
    "works",
    "correlating",
    "use",
    "tokens",
    "spam",
    "emails",
    "bayes",
    "theorem",
    "explained",
    "earlier",
    "used",
    "calculate",
    "probability",
    "email",
    "spam",
    "named",
    "spam",
    "filtering",
    "baseline",
    "technique",
    "dealing",
    "spam",
    "container",
    "emails",
    "need",
    "individual",
    "user",
    "give",
    "low",
    "false",
    "positive",
    "spam",
    "detection",
    "rates",
    "generally",
    "acceptable",
    "users",
    "one",
    "oldest",
    "ways",
    "spam",
    "filtering",
    "roots",
    "1990s",
    "particular",
    "words",
    "particular",
    "probabilities",
    "occurring",
    "spam",
    "legitimate",
    "email",
    "well",
    "instance",
    "emails",
    "users",
    "frequently",
    "encounter",
    "world",
    "lottery",
    "lucky",
    "draw",
    "spam",
    "email",
    "sell",
    "see",
    "emails",
    "filter",
    "know",
    "probabilities",
    "advance",
    "must",
    "friends",
    "build",
    "train",
    "filter",
    "user",
    "must",
    "manually",
    "indicate",
    "whether",
    "new",
    "email",
    "spam",
    "words",
    "straining",
    "email",
    "filter",
    "adjust",
    "probability",
    "word",
    "appear",
    "spam",
    "legitimate",
    "database",
    "training",
    "word",
    "probabilities",
    "also",
    "known",
    "likelihood",
    "functions",
    "used",
    "compute",
    "probability",
    "email",
    "particular",
    "set",
    "words",
    "belongs",
    "either",
    "category",
    "word",
    "email",
    "contributes",
    "email",
    "spam",
    "probability",
    "contribution",
    "called",
    "posterior",
    "probability",
    "computed",
    "using",
    "base",
    "0",
    "email",
    "spam",
    "probability",
    "computed",
    "verse",
    "email",
    "total",
    "exceeds",
    "certain",
    "threshold",
    "say",
    "95",
    "filter",
    "mark",
    "email",
    "spam",
    "object",
    "detection",
    "process",
    "finding",
    "instances",
    "objects",
    "faces",
    "bicycles",
    "buildings",
    "images",
    "video",
    "object",
    "detection",
    "algorithm",
    "typically",
    "use",
    "extracted",
    "features",
    "learning",
    "algorithm",
    "recognize",
    "instance",
    "object",
    "category",
    "bias",
    "plays",
    "important",
    "role",
    "categorization",
    "classification",
    "object",
    "medical",
    "area",
    "increasingly",
    "voluminous",
    "amount",
    "electronic",
    "data",
    "becoming",
    "complicated",
    "produced",
    "medical",
    "data",
    "certain",
    "characteristics",
    "make",
    "analysis",
    "challenging",
    "attractive",
    "well",
    "among",
    "different",
    "approaches",
    "knave",
    "bias",
    "used",
    "effective",
    "efficient",
    "classification",
    "algorithm",
    "successfully",
    "applied",
    "many",
    "medical",
    "problems",
    "empirical",
    "comparison",
    "knave",
    "bias",
    "versus",
    "five",
    "popular",
    "classifiers",
    "medical",
    "data",
    "sets",
    "shows",
    "may",
    "bias",
    "well",
    "suited",
    "medical",
    "application",
    "high",
    "performance",
    "examine",
    "medical",
    "problems",
    "past",
    "various",
    "statistical",
    "methods",
    "used",
    "modeling",
    "area",
    "disease",
    "diagnosis",
    "methods",
    "require",
    "prior",
    "assumptions",
    "less",
    "capable",
    "dealing",
    "massive",
    "complicated",
    "nonlinear",
    "dependent",
    "data",
    "one",
    "main",
    "advantages",
    "neighbor",
    "approach",
    "appealing",
    "physicians",
    "available",
    "information",
    "used",
    "explain",
    "decision",
    "explanation",
    "seems",
    "natural",
    "medical",
    "diagnosis",
    "prognosis",
    "close",
    "way",
    "physician",
    "diagnosed",
    "patients",
    "weather",
    "one",
    "influential",
    "factor",
    "daily",
    "life",
    "extent",
    "may",
    "affect",
    "economy",
    "country",
    "depends",
    "occupation",
    "like",
    "agriculture",
    "therefore",
    "countermeasure",
    "reduce",
    "damage",
    "caused",
    "uncertainty",
    "whether",
    "behavior",
    "efficient",
    "way",
    "print",
    "weather",
    "whether",
    "projecting",
    "challenging",
    "problem",
    "meteorological",
    "department",
    "since",
    "ears",
    "even",
    "technology",
    "skill",
    "scientific",
    "advancement",
    "accuracy",
    "production",
    "weather",
    "never",
    "sufficient",
    "even",
    "current",
    "day",
    "domain",
    "remains",
    "research",
    "topic",
    "scientists",
    "mathematicians",
    "working",
    "produce",
    "model",
    "algorithm",
    "accurately",
    "predict",
    "weather",
    "bias",
    "approach",
    "based",
    "model",
    "created",
    "procedure",
    "probabilities",
    "used",
    "calculate",
    "likelihood",
    "class",
    "label",
    "input",
    "data",
    "instance",
    "one",
    "maximum",
    "likelihood",
    "considered",
    "resulting",
    "output",
    "earlier",
    "saw",
    "small",
    "implementation",
    "algorithm",
    "well",
    "predicted",
    "whether",
    "play",
    "based",
    "data",
    "collected",
    "earlier",
    "python",
    "library",
    "known",
    "helps",
    "build",
    "bias",
    "model",
    "python",
    "three",
    "types",
    "named",
    "ass",
    "model",
    "library",
    "first",
    "one",
    "caution",
    "used",
    "classification",
    "assumes",
    "feature",
    "follow",
    "normal",
    "distribution",
    "next",
    "multinomial",
    "used",
    "discrete",
    "counts",
    "example",
    "let",
    "say",
    "text",
    "classification",
    "problem",
    "consider",
    "bernouli",
    "trials",
    "one",
    "step",
    "instead",
    "word",
    "occurring",
    "document",
    "count",
    "often",
    "word",
    "occurs",
    "document",
    "think",
    "number",
    "times",
    "outcomes",
    "number",
    "observed",
    "given",
    "number",
    "trials",
    "finally",
    "bernouli",
    "type",
    "naples",
    "binomial",
    "model",
    "useful",
    "feature",
    "vectors",
    "binary",
    "bag",
    "words",
    "model",
    "zeros",
    "words",
    "occur",
    "document",
    "verse",
    "occur",
    "document",
    "respectively",
    "based",
    "data",
    "set",
    "choose",
    "given",
    "discussed",
    "model",
    "gaussian",
    "multinomial",
    "bernouli",
    "let",
    "understand",
    "algorithm",
    "works",
    "different",
    "steps",
    "one",
    "take",
    "create",
    "bison",
    "model",
    "use",
    "knave",
    "bias",
    "predict",
    "output",
    "understand",
    "better",
    "going",
    "predict",
    "onset",
    "diabetes",
    "problem",
    "comprises",
    "768",
    "observations",
    "medical",
    "details",
    "pima",
    "indian",
    "patients",
    "record",
    "describes",
    "instantaneous",
    "measurement",
    "taken",
    "patient",
    "age",
    "number",
    "times",
    "pregnant",
    "blood",
    "work",
    "group",
    "patients",
    "women",
    "aged",
    "21",
    "old",
    "attributes",
    "numeric",
    "unit",
    "vary",
    "attribute",
    "attribute",
    "record",
    "class",
    "value",
    "indicate",
    "whether",
    "patient",
    "suffered",
    "onset",
    "diabetes",
    "within",
    "five",
    "years",
    "measurements",
    "classified",
    "zero",
    "broken",
    "whole",
    "process",
    "following",
    "steps",
    "first",
    "step",
    "handling",
    "data",
    "load",
    "data",
    "csv",
    "file",
    "split",
    "training",
    "test",
    "data",
    "sets",
    "second",
    "step",
    "summarizing",
    "data",
    "summarize",
    "properties",
    "training",
    "data",
    "sets",
    "calculate",
    "probabilities",
    "make",
    "predictions",
    "third",
    "step",
    "comes",
    "making",
    "particular",
    "prediction",
    "use",
    "summaries",
    "data",
    "set",
    "generate",
    "single",
    "prediction",
    "generate",
    "predictions",
    "given",
    "test",
    "data",
    "set",
    "summarize",
    "training",
    "data",
    "sets",
    "finally",
    "evaluate",
    "accuracy",
    "predictions",
    "made",
    "test",
    "data",
    "set",
    "percentage",
    "correct",
    "predictions",
    "made",
    "finally",
    "tied",
    "together",
    "form",
    "model",
    "nape",
    "classifier",
    "first",
    "thing",
    "need",
    "load",
    "data",
    "data",
    "csv",
    "format",
    "without",
    "header",
    "line",
    "codes",
    "open",
    "file",
    "open",
    "function",
    "read",
    "data",
    "lines",
    "using",
    "read",
    "functions",
    "csv",
    "module",
    "also",
    "need",
    "convert",
    "attributes",
    "loaded",
    "strings",
    "numbers",
    "work",
    "let",
    "show",
    "implemented",
    "need",
    "tall",
    "python",
    "system",
    "use",
    "jupyter",
    "notebook",
    "python",
    "shell",
    "hey",
    "using",
    "anaconda",
    "navigator",
    "things",
    "required",
    "programming",
    "python",
    "jupiter",
    "lab",
    "notebook",
    "qt",
    "console",
    "even",
    "studio",
    "well",
    "need",
    "install",
    "anaconda",
    "navigator",
    "comes",
    "pre",
    "installed",
    "python",
    "also",
    "moment",
    "click",
    "launch",
    "jupyter",
    "notebook",
    "take",
    "jupiter",
    "homepage",
    "local",
    "system",
    "programming",
    "python",
    "let",
    "rename",
    "india",
    "diabetes",
    "first",
    "need",
    "load",
    "data",
    "set",
    "creating",
    "function",
    "load",
    "csv",
    "need",
    "import",
    "certain",
    "csv",
    "math",
    "random",
    "method",
    "see",
    "created",
    "load",
    "csv",
    "function",
    "take",
    "pie",
    "indian",
    "diabetes",
    "data",
    "dot",
    "csv",
    "file",
    "using",
    "csv",
    "dot",
    "reader",
    "method",
    "converting",
    "every",
    "element",
    "data",
    "set",
    "float",
    "originally",
    "ants",
    "string",
    "need",
    "convert",
    "floor",
    "calculation",
    "purposes",
    "next",
    "need",
    "split",
    "data",
    "training",
    "data",
    "sets",
    "nay",
    "bias",
    "use",
    "make",
    "prediction",
    "data",
    "set",
    "use",
    "evaluate",
    "accuracy",
    "model",
    "need",
    "split",
    "data",
    "set",
    "randomly",
    "training",
    "testing",
    "data",
    "set",
    "ratio",
    "usually",
    "70",
    "30",
    "example",
    "going",
    "use",
    "67",
    "33",
    "70",
    "30",
    "ratio",
    "testing",
    "algorithms",
    "play",
    "around",
    "number",
    "split",
    "data",
    "set",
    "function",
    "navy",
    "base",
    "model",
    "comprised",
    "summary",
    "data",
    "training",
    "data",
    "set",
    "summary",
    "used",
    "making",
    "predictions",
    "summary",
    "training",
    "data",
    "collected",
    "involves",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "class",
    "value",
    "example",
    "two",
    "class",
    "values",
    "seven",
    "numerical",
    "attributes",
    "need",
    "mean",
    "standard",
    "deviation",
    "seven",
    "attributes",
    "class",
    "value",
    "makes",
    "14",
    "attribute",
    "summaries",
    "break",
    "preparation",
    "summary",
    "following",
    "sub",
    "tasks",
    "separating",
    "data",
    "class",
    "calculating",
    "mean",
    "calculating",
    "standard",
    "deviation",
    "summarizing",
    "data",
    "sets",
    "summarizing",
    "attributes",
    "class",
    "first",
    "task",
    "separate",
    "training",
    "data",
    "set",
    "instances",
    "class",
    "value",
    "calculate",
    "statistics",
    "class",
    "creating",
    "map",
    "class",
    "value",
    "list",
    "instances",
    "belong",
    "class",
    "class",
    "sort",
    "entire",
    "dataset",
    "instances",
    "appropriate",
    "list",
    "separate",
    "class",
    "function",
    "see",
    "function",
    "assumes",
    "last",
    "attribute",
    "class",
    "value",
    "function",
    "returns",
    "map",
    "class",
    "value",
    "list",
    "data",
    "instances",
    "next",
    "need",
    "calculate",
    "mean",
    "attribute",
    "class",
    "value",
    "mean",
    "central",
    "middle",
    "central",
    "tendency",
    "data",
    "use",
    "middle",
    "gaussian",
    "distribution",
    "calculating",
    "probabilities",
    "function",
    "mean",
    "also",
    "need",
    "calculate",
    "standard",
    "deviation",
    "attribute",
    "class",
    "value",
    "standard",
    "deviation",
    "calculated",
    "square",
    "root",
    "variance",
    "variance",
    "calculated",
    "average",
    "squared",
    "differences",
    "attribute",
    "value",
    "mean",
    "one",
    "thing",
    "note",
    "using",
    "n",
    "minus",
    "one",
    "method",
    "subtracts",
    "one",
    "number",
    "attributes",
    "values",
    "calculating",
    "variance",
    "tools",
    "summarize",
    "data",
    "given",
    "list",
    "instances",
    "calculate",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "function",
    "groups",
    "values",
    "attribute",
    "across",
    "data",
    "instances",
    "lists",
    "compute",
    "mean",
    "standard",
    "deviation",
    "values",
    "attribute",
    "next",
    "comes",
    "summarizing",
    "attributes",
    "class",
    "pull",
    "together",
    "first",
    "separating",
    "training",
    "data",
    "sets",
    "instances",
    "growth",
    "class",
    "calculating",
    "summaries",
    "ready",
    "make",
    "predictions",
    "using",
    "summaries",
    "prepared",
    "training",
    "data",
    "making",
    "predictions",
    "involves",
    "calculating",
    "probability",
    "given",
    "data",
    "instance",
    "belong",
    "class",
    "selecting",
    "class",
    "largest",
    "probability",
    "prediction",
    "divide",
    "whole",
    "method",
    "four",
    "tasks",
    "calculating",
    "gaussian",
    "probability",
    "density",
    "function",
    "calculating",
    "class",
    "probability",
    "making",
    "prediction",
    "estimating",
    "accuracy",
    "calculate",
    "gaussian",
    "probability",
    "density",
    "function",
    "use",
    "gaussian",
    "function",
    "estimate",
    "probability",
    "given",
    "attribute",
    "value",
    "given",
    "node",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "estimated",
    "training",
    "data",
    "see",
    "parameters",
    "x",
    "mean",
    "standard",
    "deviation",
    "calculate",
    "probability",
    "function",
    "calculate",
    "exponent",
    "first",
    "calculate",
    "main",
    "division",
    "lets",
    "us",
    "fit",
    "equation",
    "nicely",
    "two",
    "lines",
    "next",
    "task",
    "calculating",
    "class",
    "properties",
    "calculate",
    "probability",
    "attribute",
    "belonging",
    "class",
    "combine",
    "probabilities",
    "attributes",
    "values",
    "data",
    "instance",
    "come",
    "probability",
    "entire",
    "data",
    "instance",
    "belonging",
    "class",
    "calculated",
    "class",
    "properties",
    "time",
    "finally",
    "make",
    "first",
    "prediction",
    "calculate",
    "probability",
    "data",
    "instance",
    "belong",
    "class",
    "value",
    "look",
    "largest",
    "probability",
    "return",
    "associated",
    "class",
    "going",
    "use",
    "function",
    "predict",
    "uses",
    "summaries",
    "input",
    "vector",
    "basically",
    "probabilities",
    "input",
    "particular",
    "label",
    "finally",
    "estimate",
    "accuracy",
    "model",
    "making",
    "predictions",
    "data",
    "instances",
    "test",
    "data",
    "use",
    "cat",
    "predictions",
    "method",
    "method",
    "used",
    "calculate",
    "predictions",
    "based",
    "upon",
    "test",
    "data",
    "sets",
    "summary",
    "training",
    "data",
    "set",
    "predictions",
    "compared",
    "class",
    "values",
    "test",
    "data",
    "set",
    "classification",
    "accuracy",
    "calculated",
    "accuracy",
    "ratio",
    "zeros",
    "hundred",
    "percent",
    "get",
    "accuracy",
    "method",
    "calculate",
    "accuracy",
    "ratio",
    "finally",
    "sum",
    "define",
    "main",
    "function",
    "call",
    "methods",
    "defined",
    "earlier",
    "one",
    "one",
    "get",
    "courtesy",
    "model",
    "created",
    "see",
    "main",
    "function",
    "file",
    "name",
    "defined",
    "split",
    "ratio",
    "data",
    "set",
    "training",
    "test",
    "data",
    "set",
    "using",
    "split",
    "data",
    "set",
    "method",
    "next",
    "using",
    "summarized",
    "class",
    "function",
    "using",
    "get",
    "prediction",
    "get",
    "accuracy",
    "method",
    "well",
    "guys",
    "see",
    "output",
    "one",
    "gives",
    "us",
    "splitting",
    "seven",
    "sixty",
    "eight",
    "rows",
    "514",
    "training",
    "254",
    "test",
    "data",
    "set",
    "rows",
    "accuracy",
    "model",
    "68",
    "play",
    "amount",
    "training",
    "test",
    "data",
    "sets",
    "used",
    "change",
    "split",
    "ratio",
    "seventies",
    "238",
    "220",
    "get",
    "different",
    "sort",
    "accuracy",
    "suppose",
    "change",
    "split",
    "ratio",
    "see",
    "get",
    "accuracy",
    "62",
    "percent",
    "splitting",
    "gave",
    "us",
    "better",
    "result",
    "68",
    "percent",
    "implement",
    "navy",
    "bias",
    "caution",
    "classifier",
    "step",
    "step",
    "methods",
    "need",
    "case",
    "using",
    "nave",
    "bayes",
    "classifier",
    "worry",
    "need",
    "write",
    "many",
    "lines",
    "code",
    "make",
    "model",
    "sacketts",
    "really",
    "comes",
    "picture",
    "library",
    "predefined",
    "method",
    "say",
    "predefined",
    "function",
    "neighbor",
    "bias",
    "converts",
    "lines",
    "course",
    "merely",
    "two",
    "three",
    "lines",
    "codes",
    "let",
    "open",
    "another",
    "jupyter",
    "notebook",
    "let",
    "name",
    "sklearn",
    "pass",
    "going",
    "use",
    "famous",
    "data",
    "set",
    "iris",
    "dataset",
    "iris",
    "flower",
    "data",
    "set",
    "multivariate",
    "data",
    "set",
    "introduced",
    "british",
    "statistician",
    "biologists",
    "roland",
    "fisher",
    "based",
    "fish",
    "linear",
    "discriminant",
    "model",
    "data",
    "set",
    "became",
    "typical",
    "test",
    "case",
    "many",
    "statistical",
    "classification",
    "techniques",
    "machine",
    "learning",
    "going",
    "use",
    "caution",
    "nb",
    "model",
    "already",
    "available",
    "sklearn",
    "mentioned",
    "earlier",
    "three",
    "types",
    "neighbors",
    "question",
    "multinomial",
    "bernouli",
    "going",
    "use",
    "caution",
    "model",
    "already",
    "present",
    "sklearn",
    "library",
    "cycle",
    "learn",
    "library",
    "first",
    "need",
    "import",
    "sklearn",
    "data",
    "sets",
    "metrics",
    "also",
    "need",
    "import",
    "caution",
    "libraries",
    "lowered",
    "need",
    "load",
    "data",
    "set",
    "iris",
    "dataset",
    "next",
    "need",
    "fit",
    "nave",
    "small",
    "data",
    "set",
    "see",
    "easily",
    "defined",
    "model",
    "gaussian",
    "nb",
    "contains",
    "programming",
    "showed",
    "earlier",
    "methods",
    "taking",
    "input",
    "calculating",
    "mean",
    "standard",
    "deviation",
    "separating",
    "bike",
    "last",
    "finally",
    "making",
    "predictions",
    "calculating",
    "prediction",
    "accuracy",
    "comes",
    "caution",
    "method",
    "inside",
    "already",
    "present",
    "sklearn",
    "library",
    "need",
    "fit",
    "according",
    "data",
    "set",
    "next",
    "print",
    "model",
    "see",
    "gaussian",
    "nb",
    "model",
    "next",
    "need",
    "make",
    "predictions",
    "expected",
    "output",
    "data",
    "set",
    "dot",
    "target",
    "projected",
    "using",
    "pretend",
    "model",
    "model",
    "using",
    "cause",
    "nb",
    "summarize",
    "model",
    "created",
    "calculate",
    "confusion",
    "matrix",
    "classification",
    "report",
    "guys",
    "see",
    "classification",
    "provide",
    "precision",
    "point",
    "ninety",
    "six",
    "recall",
    "f1",
    "score",
    "support",
    "finally",
    "print",
    "confusion",
    "matrix",
    "see",
    "gives",
    "us",
    "output",
    "see",
    "using",
    "gaussian",
    "method",
    "putting",
    "model",
    "using",
    "data",
    "fitting",
    "model",
    "created",
    "particular",
    "data",
    "set",
    "getting",
    "desired",
    "output",
    "easy",
    "library",
    "guys",
    "hope",
    "understood",
    "lot",
    "nape",
    "bayes",
    "classifier",
    "used",
    "used",
    "different",
    "steps",
    "involved",
    "classification",
    "technique",
    "makes",
    "techniques",
    "easy",
    "implement",
    "data",
    "set",
    "support",
    "vector",
    "machine",
    "one",
    "effective",
    "machine",
    "learning",
    "classifier",
    "used",
    "various",
    "fields",
    "face",
    "recognition",
    "cancer",
    "classification",
    "today",
    "session",
    "dedicated",
    "svm",
    "works",
    "various",
    "features",
    "svm",
    "used",
    "real",
    "world",
    "without",
    "due",
    "let",
    "take",
    "look",
    "agenda",
    "today",
    "going",
    "begin",
    "session",
    "introduction",
    "machine",
    "learning",
    "different",
    "types",
    "machine",
    "learning",
    "next",
    "discuss",
    "exactly",
    "support",
    "vector",
    "machines",
    "move",
    "see",
    "svm",
    "works",
    "used",
    "classify",
    "linearly",
    "separable",
    "data",
    "also",
    "briefly",
    "discuss",
    "nonlinear",
    "svm",
    "work",
    "move",
    "look",
    "use",
    "case",
    "svm",
    "colon",
    "cancer",
    "classification",
    "finally",
    "end",
    "session",
    "running",
    "demo",
    "use",
    "svm",
    "predict",
    "whether",
    "patient",
    "suffering",
    "heart",
    "disease",
    "okay",
    "agenda",
    "let",
    "get",
    "stood",
    "first",
    "topic",
    "machine",
    "learning",
    "machine",
    "learning",
    "science",
    "getting",
    "computers",
    "act",
    "feeding",
    "data",
    "letting",
    "learn",
    "tricks",
    "okay",
    "going",
    "explicitly",
    "program",
    "machine",
    "instead",
    "going",
    "feed",
    "data",
    "let",
    "learn",
    "key",
    "machine",
    "learning",
    "data",
    "machines",
    "learn",
    "like",
    "us",
    "humans",
    "humans",
    "need",
    "collect",
    "information",
    "data",
    "learn",
    "similarly",
    "machines",
    "must",
    "also",
    "fed",
    "data",
    "order",
    "learn",
    "make",
    "decisions",
    "let",
    "say",
    "want",
    "machine",
    "predict",
    "value",
    "stock",
    "right",
    "situations",
    "feed",
    "machine",
    "relevant",
    "data",
    "develop",
    "model",
    "used",
    "predict",
    "value",
    "stock",
    "one",
    "thing",
    "keep",
    "mind",
    "data",
    "feed",
    "machine",
    "better",
    "learn",
    "make",
    "accurate",
    "predictions",
    "obviously",
    "machine",
    "learning",
    "simple",
    "order",
    "machine",
    "analyze",
    "get",
    "useful",
    "insights",
    "data",
    "must",
    "process",
    "study",
    "data",
    "running",
    "different",
    "algorithms",
    "right",
    "today",
    "discussing",
    "one",
    "widely",
    "used",
    "algorithm",
    "called",
    "support",
    "vector",
    "machine",
    "okay",
    "brief",
    "idea",
    "machine",
    "learning",
    "let",
    "look",
    "different",
    "ways",
    "machines",
    "lon",
    "first",
    "supervised",
    "learning",
    "type",
    "learning",
    "machine",
    "learns",
    "guidance",
    "right",
    "called",
    "supervised",
    "learning",
    "school",
    "teachers",
    "guided",
    "us",
    "taught",
    "us",
    "similarly",
    "supervised",
    "learning",
    "machines",
    "learn",
    "feeding",
    "labeled",
    "data",
    "explicitly",
    "telling",
    "hey",
    "input",
    "output",
    "must",
    "look",
    "okay",
    "guys",
    "teacher",
    "case",
    "training",
    "data",
    "next",
    "unsupervised",
    "learning",
    "data",
    "labeled",
    "guide",
    "sort",
    "okay",
    "machine",
    "must",
    "figure",
    "data",
    "set",
    "given",
    "must",
    "find",
    "hidden",
    "patterns",
    "order",
    "make",
    "predictions",
    "output",
    "example",
    "unsupervised",
    "learning",
    "adult",
    "like",
    "need",
    "guide",
    "help",
    "us",
    "daily",
    "activities",
    "figured",
    "things",
    "without",
    "supervision",
    "right",
    "exactly",
    "supervised",
    "learning",
    "work",
    "finally",
    "reinforcement",
    "learning",
    "let",
    "say",
    "dropped",
    "isolated",
    "island",
    "would",
    "initially",
    "would",
    "panic",
    "unsure",
    "get",
    "food",
    "live",
    "adapt",
    "must",
    "learn",
    "live",
    "island",
    "adapt",
    "changing",
    "climate",
    "learn",
    "eat",
    "eat",
    "basically",
    "following",
    "hit",
    "trial",
    "new",
    "surrounding",
    "way",
    "learn",
    "experience",
    "learn",
    "experience",
    "exactly",
    "reinforcement",
    "learning",
    "learning",
    "method",
    "wherein",
    "agent",
    "interacts",
    "environment",
    "producing",
    "actions",
    "discovers",
    "errors",
    "words",
    "alright",
    "gets",
    "trained",
    "gets",
    "ready",
    "predict",
    "new",
    "data",
    "presented",
    "case",
    "agent",
    "basically",
    "stuck",
    "island",
    "environment",
    "island",
    "right",
    "okay",
    "let",
    "move",
    "see",
    "svm",
    "algorithm",
    "guys",
    "svm",
    "support",
    "vector",
    "machine",
    "supervised",
    "learning",
    "algorithm",
    "mainly",
    "used",
    "classify",
    "data",
    "different",
    "classes",
    "unlike",
    "algorithms",
    "svm",
    "makes",
    "use",
    "hyperplane",
    "acts",
    "like",
    "decision",
    "boundary",
    "various",
    "classes",
    "general",
    "svm",
    "used",
    "generate",
    "multiple",
    "separating",
    "hyperplanes",
    "data",
    "divided",
    "segments",
    "okay",
    "segments",
    "contain",
    "one",
    "kind",
    "data",
    "mainly",
    "used",
    "classification",
    "purpose",
    "wearing",
    "want",
    "classify",
    "data",
    "two",
    "different",
    "segments",
    "depending",
    "features",
    "data",
    "moving",
    "let",
    "discuss",
    "features",
    "svm",
    "like",
    "mentioned",
    "earlier",
    "svm",
    "supervised",
    "learning",
    "algorithm",
    "means",
    "svm",
    "trains",
    "set",
    "labeled",
    "data",
    "svm",
    "studies",
    "label",
    "training",
    "data",
    "classifies",
    "new",
    "input",
    "data",
    "depending",
    "learned",
    "training",
    "phase",
    "main",
    "advantage",
    "support",
    "vector",
    "machine",
    "used",
    "classification",
    "regression",
    "problems",
    "right",
    "even",
    "though",
    "svm",
    "mainly",
    "known",
    "classification",
    "svr",
    "support",
    "vector",
    "regressor",
    "used",
    "regression",
    "problems",
    "right",
    "svm",
    "used",
    "classification",
    "regression",
    "one",
    "reasons",
    "lot",
    "people",
    "prefer",
    "svm",
    "good",
    "classifier",
    "along",
    "also",
    "used",
    "regression",
    "another",
    "feature",
    "svm",
    "kernel",
    "functions",
    "svm",
    "used",
    "classifying",
    "nonlinear",
    "data",
    "using",
    "kernel",
    "trick",
    "kernel",
    "trick",
    "basically",
    "means",
    "transform",
    "data",
    "another",
    "dimension",
    "easily",
    "draw",
    "hyperplane",
    "different",
    "classes",
    "data",
    "alright",
    "nonlinear",
    "data",
    "basically",
    "data",
    "separated",
    "straight",
    "line",
    "alright",
    "svm",
    "even",
    "used",
    "nonlinear",
    "data",
    "sets",
    "use",
    "kernel",
    "functions",
    "right",
    "guys",
    "hope",
    "clear",
    "basic",
    "concepts",
    "svm",
    "let",
    "move",
    "look",
    "svm",
    "works",
    "guys",
    "order",
    "understand",
    "svm",
    "works",
    "let",
    "consider",
    "small",
    "scenario",
    "second",
    "pretend",
    "firm",
    "okay",
    "let",
    "say",
    "problem",
    "want",
    "set",
    "fence",
    "protect",
    "rabbits",
    "pack",
    "wolves",
    "okay",
    "build",
    "fence",
    "one",
    "way",
    "get",
    "around",
    "problem",
    "build",
    "classifier",
    "based",
    "position",
    "rabbits",
    "words",
    "faster",
    "telling",
    "classify",
    "group",
    "rabbits",
    "one",
    "group",
    "draw",
    "decision",
    "boundary",
    "rabbits",
    "world",
    "right",
    "try",
    "draw",
    "decision",
    "boundary",
    "rabbits",
    "wolves",
    "looks",
    "something",
    "like",
    "okay",
    "clearly",
    "build",
    "fence",
    "along",
    "line",
    "simple",
    "terms",
    "exactly",
    "spm",
    "work",
    "draws",
    "decision",
    "boundary",
    "hyperplane",
    "two",
    "classes",
    "order",
    "separate",
    "class",
    "asif",
    "know",
    "thinking",
    "know",
    "draw",
    "hyperplane",
    "basic",
    "principle",
    "behind",
    "svm",
    "draw",
    "hyperplane",
    "best",
    "separates",
    "two",
    "classes",
    "case",
    "two",
    "glasses",
    "rabbits",
    "wolves",
    "start",
    "drawing",
    "random",
    "hyperplane",
    "check",
    "distance",
    "hyperplane",
    "closest",
    "data",
    "points",
    "glove",
    "closes",
    "data",
    "points",
    "hyperplane",
    "known",
    "support",
    "vectors",
    "name",
    "comes",
    "support",
    "active",
    "machine",
    "basically",
    "hyperplane",
    "drawn",
    "based",
    "support",
    "vectors",
    "guys",
    "optimum",
    "hyperplane",
    "maximum",
    "distance",
    "support",
    "vectors",
    "right",
    "basically",
    "hyper",
    "plane",
    "maximum",
    "distance",
    "support",
    "vectors",
    "optimal",
    "hyperplane",
    "distance",
    "hyperplane",
    "support",
    "vectors",
    "known",
    "margin",
    "right",
    "sum",
    "svm",
    "used",
    "classify",
    "data",
    "using",
    "hyper",
    "plane",
    "distance",
    "distance",
    "hyperplane",
    "support",
    "vectors",
    "maximum",
    "basically",
    "margin",
    "maximum",
    "right",
    "way",
    "know",
    "actually",
    "separating",
    "classes",
    "add",
    "distance",
    "two",
    "classes",
    "maximum",
    "okay",
    "let",
    "try",
    "solve",
    "problem",
    "okay",
    "let",
    "say",
    "input",
    "new",
    "data",
    "point",
    "okay",
    "new",
    "data",
    "point",
    "want",
    "draw",
    "hyper",
    "plane",
    "best",
    "separates",
    "two",
    "classes",
    "okay",
    "start",
    "drawing",
    "hyperplane",
    "like",
    "check",
    "distance",
    "hyper",
    "plane",
    "support",
    "vectors",
    "okay",
    "trying",
    "check",
    "margin",
    "maximum",
    "hyperplane",
    "draw",
    "hyper",
    "plane",
    "like",
    "right",
    "going",
    "check",
    "support",
    "vectors",
    "going",
    "check",
    "distance",
    "support",
    "vectors",
    "hyperplane",
    "clear",
    "margin",
    "right",
    "compare",
    "margin",
    "previous",
    "one",
    "hyperplane",
    "reason",
    "choosing",
    "hyperplane",
    "distance",
    "support",
    "vectors",
    "hi",
    "hyperplane",
    "maximum",
    "scenario",
    "okay",
    "guys",
    "choose",
    "hyperplane",
    "basically",
    "make",
    "sure",
    "hyper",
    "plane",
    "maximum",
    "margin",
    "right",
    "two",
    "best",
    "separate",
    "two",
    "classes",
    "right",
    "okay",
    "far",
    "quite",
    "easy",
    "data",
    "linearly",
    "separable",
    "means",
    "could",
    "draw",
    "straight",
    "line",
    "separate",
    "two",
    "classes",
    "right",
    "data",
    "set",
    "like",
    "possibly",
    "ca",
    "draw",
    "hyper",
    "plane",
    "like",
    "right",
    "separate",
    "two",
    "situations",
    "earlier",
    "session",
    "mentioned",
    "kernel",
    "used",
    "transform",
    "data",
    "another",
    "dimension",
    "clear",
    "dividing",
    "margin",
    "classes",
    "data",
    "alright",
    "kernel",
    "functions",
    "offer",
    "user",
    "option",
    "transforming",
    "nonlinear",
    "spaces",
    "linear",
    "ones",
    "nonlinear",
    "data",
    "set",
    "one",
    "ca",
    "separate",
    "using",
    "straight",
    "line",
    "right",
    "order",
    "deal",
    "data",
    "sets",
    "going",
    "ants",
    "form",
    "linear",
    "data",
    "sets",
    "use",
    "svm",
    "okay",
    "simple",
    "trick",
    "would",
    "transform",
    "two",
    "variables",
    "x",
    "new",
    "feature",
    "space",
    "involving",
    "new",
    "variable",
    "called",
    "right",
    "guys",
    "far",
    "plotting",
    "data",
    "two",
    "dimensional",
    "space",
    "correct",
    "using",
    "x",
    "axis",
    "two",
    "variables",
    "x",
    "order",
    "deal",
    "kind",
    "data",
    "simple",
    "trick",
    "would",
    "transform",
    "two",
    "variables",
    "x",
    "new",
    "feature",
    "space",
    "involving",
    "new",
    "variable",
    "called",
    "ok",
    "basically",
    "visualizing",
    "data",
    "space",
    "transform",
    "2d",
    "space",
    "3d",
    "space",
    "clearly",
    "see",
    "dividing",
    "margin",
    "two",
    "classes",
    "data",
    "right",
    "go",
    "ahead",
    "separate",
    "two",
    "classes",
    "drawing",
    "best",
    "hyperplane",
    "okay",
    "exactly",
    "discussed",
    "previous",
    "slides",
    "guys",
    "try",
    "dry",
    "drawing",
    "hyperplane",
    "optimum",
    "two",
    "classes",
    "right",
    "guys",
    "hope",
    "good",
    "understanding",
    "nonlinear",
    "svm",
    "let",
    "look",
    "real",
    "world",
    "use",
    "case",
    "support",
    "vector",
    "machines",
    "guys",
    "vm",
    "classifier",
    "used",
    "cancer",
    "classification",
    "since",
    "early",
    "2000s",
    "experiment",
    "held",
    "group",
    "professionals",
    "applied",
    "svm",
    "colon",
    "cancer",
    "tissue",
    "classification",
    "data",
    "set",
    "consisted",
    "transmembrane",
    "protein",
    "samples",
    "50",
    "200",
    "genes",
    "samples",
    "input",
    "svm",
    "classifier",
    "sample",
    "input",
    "svm",
    "classifier",
    "colon",
    "cancer",
    "tissue",
    "samples",
    "normal",
    "colon",
    "tissue",
    "samples",
    "right",
    "main",
    "objective",
    "study",
    "classify",
    "gene",
    "samples",
    "based",
    "whether",
    "cancerous",
    "okay",
    "svm",
    "trained",
    "using",
    "50",
    "200",
    "samples",
    "order",
    "discriminate",
    "tumor",
    "specimens",
    "performance",
    "svm",
    "classifier",
    "accurate",
    "even",
    "small",
    "data",
    "set",
    "right",
    "50",
    "200",
    "samples",
    "even",
    "small",
    "data",
    "set",
    "svm",
    "pretty",
    "accurate",
    "results",
    "performance",
    "compared",
    "classification",
    "algorithm",
    "like",
    "naive",
    "bayes",
    "case",
    "svm",
    "outperform",
    "naive",
    "bayes",
    "experiment",
    "clear",
    "svm",
    "classify",
    "data",
    "effectively",
    "worked",
    "exceptionally",
    "good",
    "small",
    "data",
    "sets",
    "let",
    "go",
    "ahead",
    "understand",
    "exactly",
    "unsupervised",
    "learning",
    "sometimes",
    "given",
    "data",
    "unstructured",
    "unlabeled",
    "becomes",
    "difficult",
    "classify",
    "data",
    "different",
    "categories",
    "unsupervised",
    "learning",
    "helps",
    "solve",
    "problem",
    "learning",
    "used",
    "cluster",
    "input",
    "data",
    "classes",
    "basis",
    "statistical",
    "properties",
    "example",
    "cluster",
    "different",
    "bikes",
    "based",
    "upon",
    "speed",
    "limit",
    "acceleration",
    "average",
    "average",
    "giving",
    "suppose",
    "learning",
    "type",
    "machine",
    "learning",
    "algorithm",
    "used",
    "draw",
    "inferences",
    "data",
    "sets",
    "consisting",
    "input",
    "data",
    "without",
    "labels",
    "responses",
    "look",
    "workflow",
    "process",
    "flow",
    "unsupervised",
    "learning",
    "training",
    "data",
    "collection",
    "information",
    "without",
    "label",
    "machine",
    "learning",
    "algorithm",
    "clustering",
    "malls",
    "distributes",
    "data",
    "different",
    "clusters",
    "provide",
    "lebanon",
    "new",
    "data",
    "make",
    "prediction",
    "find",
    "cluster",
    "particular",
    "data",
    "data",
    "set",
    "belongs",
    "particular",
    "data",
    "point",
    "belongs",
    "one",
    "important",
    "algorithms",
    "unsupervised",
    "learning",
    "clustering",
    "let",
    "understand",
    "exactly",
    "clustering",
    "clustering",
    "basically",
    "process",
    "dividing",
    "data",
    "sets",
    "groups",
    "consisting",
    "similar",
    "data",
    "points",
    "means",
    "grouping",
    "objects",
    "based",
    "information",
    "found",
    "data",
    "describing",
    "objects",
    "relationships",
    "clustering",
    "malls",
    "focus",
    "defying",
    "groups",
    "similar",
    "records",
    "labeling",
    "records",
    "according",
    "group",
    "belong",
    "done",
    "without",
    "benefit",
    "prior",
    "knowledge",
    "groups",
    "creator",
    "districts",
    "fact",
    "may",
    "even",
    "know",
    "exactly",
    "many",
    "groups",
    "look",
    "models",
    "often",
    "referred",
    "unsupervised",
    "learning",
    "models",
    "since",
    "external",
    "standard",
    "judge",
    "malls",
    "classification",
    "performance",
    "right",
    "wrong",
    "answers",
    "model",
    "talk",
    "clustering",
    "used",
    "goal",
    "clustering",
    "determine",
    "intrinsic",
    "growth",
    "set",
    "unlabeled",
    "data",
    "sometime",
    "partitioning",
    "goal",
    "purpose",
    "clustering",
    "algorithm",
    "make",
    "sense",
    "exact",
    "value",
    "last",
    "set",
    "structured",
    "unstructured",
    "data",
    "clustering",
    "used",
    "industry",
    "look",
    "various",
    "use",
    "cases",
    "clustering",
    "industry",
    "first",
    "used",
    "marketing",
    "discovering",
    "distinct",
    "groups",
    "customer",
    "databases",
    "customers",
    "make",
    "lot",
    "long",
    "distance",
    "calls",
    "customers",
    "use",
    "internet",
    "cause",
    "also",
    "using",
    "insurance",
    "companies",
    "like",
    "identifying",
    "groups",
    "corporation",
    "insurance",
    "policy",
    "holders",
    "high",
    "average",
    "claim",
    "rate",
    "farmers",
    "crash",
    "cops",
    "profitable",
    "using",
    "c",
    "smith",
    "studies",
    "define",
    "probability",
    "areas",
    "oil",
    "gas",
    "exploration",
    "based",
    "cease",
    "make",
    "data",
    "also",
    "used",
    "recommendation",
    "movies",
    "say",
    "also",
    "used",
    "flickr",
    "photos",
    "also",
    "used",
    "amazon",
    "recommending",
    "product",
    "category",
    "lies",
    "basically",
    "talk",
    "clustering",
    "three",
    "types",
    "clustering",
    "first",
    "exclusive",
    "clustering",
    "hard",
    "clustering",
    "item",
    "belongs",
    "exclusively",
    "one",
    "cluster",
    "several",
    "clusters",
    "datapoint",
    "belong",
    "exclusively",
    "one",
    "cluster",
    "er",
    "example",
    "clustering",
    "claiming",
    "clustering",
    "exclusive",
    "kind",
    "clustering",
    "secondly",
    "overlapping",
    "clustering",
    "also",
    "known",
    "soft",
    "clusters",
    "item",
    "belong",
    "multiple",
    "clusters",
    "degree",
    "association",
    "cluster",
    "shown",
    "example",
    "fuzzy",
    "c",
    "means",
    "clustering",
    "used",
    "overlapping",
    "clustering",
    "finally",
    "hierarchical",
    "clustering",
    "two",
    "clusters",
    "relationship",
    "structure",
    "known",
    "hierarchical",
    "cluster",
    "see",
    "example",
    "kind",
    "relationship",
    "cluster",
    "given",
    "let",
    "understand",
    "exactly",
    "k",
    "means",
    "clustering",
    "today",
    "means",
    "clustering",
    "enquirer",
    "whose",
    "main",
    "goal",
    "group",
    "similar",
    "elements",
    "data",
    "points",
    "cluster",
    "process",
    "objects",
    "classified",
    "predefined",
    "number",
    "groups",
    "much",
    "similar",
    "possible",
    "one",
    "group",
    "another",
    "group",
    "much",
    "similar",
    "possible",
    "within",
    "group",
    "look",
    "algorithm",
    "working",
    "right",
    "first",
    "starts",
    "defying",
    "number",
    "clusters",
    "k",
    "find",
    "centroid",
    "find",
    "distance",
    "objects",
    "distance",
    "object",
    "centroid",
    "distance",
    "object",
    "centroid",
    "find",
    "grouping",
    "based",
    "minimum",
    "distance",
    "past",
    "centroid",
    "converse",
    "true",
    "make",
    "cluster",
    "false",
    "ca",
    "find",
    "centroid",
    "repeat",
    "steps",
    "let",
    "show",
    "exactly",
    "clustering",
    "example",
    "first",
    "need",
    "decide",
    "number",
    "clusters",
    "made",
    "another",
    "important",
    "task",
    "decide",
    "important",
    "number",
    "clusters",
    "decide",
    "number",
    "classes",
    "get",
    "later",
    "first",
    "let",
    "assume",
    "number",
    "clusters",
    "decided",
    "three",
    "provide",
    "centroids",
    "clusters",
    "guessing",
    "algorithm",
    "calculates",
    "euclidean",
    "distance",
    "point",
    "centroid",
    "assize",
    "data",
    "point",
    "closest",
    "cluster",
    "euclidean",
    "distance",
    "know",
    "square",
    "root",
    "distance",
    "square",
    "root",
    "square",
    "distance",
    "next",
    "centroids",
    "calculated",
    "new",
    "clusters",
    "data",
    "point",
    "distance",
    "points",
    "new",
    "classes",
    "calculated",
    "points",
    "assigned",
    "closest",
    "cluster",
    "new",
    "centroid",
    "scattered",
    "steps",
    "repeated",
    "repetition",
    "centroids",
    "new",
    "centralized",
    "close",
    "previous",
    "ones",
    "unless",
    "output",
    "gets",
    "repeated",
    "outputs",
    "close",
    "enough",
    "stop",
    "process",
    "keep",
    "calculating",
    "euclidean",
    "distance",
    "points",
    "centroid",
    "calculate",
    "new",
    "centroids",
    "k",
    "means",
    "clustering",
    "works",
    "basically",
    "important",
    "part",
    "understand",
    "decide",
    "value",
    "k",
    "number",
    "clusters",
    "make",
    "sense",
    "know",
    "many",
    "classes",
    "going",
    "make",
    "decide",
    "number",
    "clusters",
    "elbow",
    "method",
    "let",
    "assume",
    "first",
    "compute",
    "sum",
    "squared",
    "error",
    "sse4",
    "value",
    "example",
    "take",
    "two",
    "four",
    "six",
    "eight",
    "sse",
    "sum",
    "squared",
    "error",
    "defined",
    "sum",
    "squared",
    "distance",
    "number",
    "member",
    "cluster",
    "centroid",
    "mathematically",
    "mathematically",
    "given",
    "equation",
    "provided",
    "brought",
    "key",
    "sse",
    "see",
    "error",
    "decreases",
    "k",
    "gets",
    "large",
    "number",
    "cluster",
    "increases",
    "smaller",
    "distortion",
    "also",
    "smaller",
    "know",
    "idea",
    "elbow",
    "method",
    "choose",
    "k",
    "sse",
    "decreases",
    "abruptly",
    "example",
    "look",
    "figure",
    "given",
    "see",
    "best",
    "number",
    "cluster",
    "elbow",
    "see",
    "graph",
    "changes",
    "abruptly",
    "number",
    "four",
    "particular",
    "example",
    "going",
    "use",
    "number",
    "cluster",
    "first",
    "working",
    "clustering",
    "two",
    "key",
    "points",
    "know",
    "first",
    "careful",
    "start",
    "choosing",
    "first",
    "center",
    "random",
    "second",
    "center",
    "far",
    "away",
    "first",
    "center",
    "similarly",
    "choosing",
    "nih",
    "center",
    "far",
    "away",
    "possible",
    "closest",
    "centers",
    "second",
    "idea",
    "many",
    "runs",
    "different",
    "random",
    "starting",
    "points",
    "get",
    "idea",
    "exactly",
    "many",
    "clusters",
    "need",
    "make",
    "exactly",
    "centroid",
    "lies",
    "data",
    "getting",
    "converted",
    "divorced",
    "exactly",
    "good",
    "method",
    "let",
    "understand",
    "pros",
    "cons",
    "clustering",
    "know",
    "simple",
    "understandable",
    "everyone",
    "learns",
    "first",
    "go",
    "items",
    "automatically",
    "assigned",
    "clusters",
    "look",
    "cons",
    "first",
    "one",
    "needs",
    "define",
    "number",
    "clusters",
    "heavy",
    "task",
    "asks",
    "us",
    "three",
    "four",
    "10",
    "categories",
    "know",
    "number",
    "clusters",
    "going",
    "difficult",
    "anyone",
    "know",
    "guess",
    "number",
    "clusters",
    "items",
    "forced",
    "clusters",
    "whether",
    "actually",
    "belong",
    "cluster",
    "category",
    "forced",
    "rely",
    "category",
    "closest",
    "happens",
    "number",
    "clusters",
    "defining",
    "correct",
    "number",
    "clusters",
    "able",
    "guess",
    "correct",
    "number",
    "clusters",
    "unable",
    "handle",
    "noisy",
    "data",
    "outliners",
    "anyways",
    "machine",
    "learning",
    "engineers",
    "date",
    "scientists",
    "clean",
    "data",
    "comes",
    "analysis",
    "method",
    "using",
    "typically",
    "people",
    "clean",
    "data",
    "clustering",
    "even",
    "clean",
    "sometimes",
    "see",
    "noisy",
    "outliners",
    "data",
    "affect",
    "whole",
    "model",
    "clustering",
    "going",
    "use",
    "clustering",
    "movie",
    "datasets",
    "find",
    "number",
    "clusters",
    "divide",
    "accordingly",
    "use",
    "case",
    "first",
    "data",
    "set",
    "five",
    "thousand",
    "movies",
    "want",
    "grip",
    "movies",
    "clusters",
    "based",
    "facebook",
    "likes",
    "guys",
    "let",
    "look",
    "demo",
    "first",
    "going",
    "import",
    "deep",
    "copy",
    "numpy",
    "pandas",
    "seaborn",
    "various",
    "libraries",
    "going",
    "use",
    "proclivities",
    "use",
    "ply",
    "plot",
    "going",
    "use",
    "ggplot",
    "next",
    "going",
    "import",
    "data",
    "set",
    "look",
    "shape",
    "data",
    "set",
    "look",
    "shape",
    "data",
    "set",
    "see",
    "5043",
    "rose",
    "28",
    "columns",
    "look",
    "head",
    "data",
    "set",
    "see",
    "5043",
    "data",
    "points",
    "george",
    "going",
    "place",
    "data",
    "points",
    "plot",
    "take",
    "director",
    "facebook",
    "likes",
    "look",
    "data",
    "columns",
    "face",
    "number",
    "post",
    "cars",
    "total",
    "facebook",
    "likes",
    "director",
    "facebook",
    "likes",
    "done",
    "taking",
    "director",
    "facebook",
    "likes",
    "actor",
    "three",
    "facebook",
    "likes",
    "right",
    "five",
    "thousand",
    "forty",
    "three",
    "rows",
    "two",
    "columns",
    "using",
    "sklearn",
    "going",
    "import",
    "first",
    "going",
    "import",
    "scale",
    "dot",
    "cluster",
    "remember",
    "guys",
    "eschaton",
    "important",
    "library",
    "python",
    "machine",
    "learning",
    "number",
    "cluster",
    "going",
    "provide",
    "five",
    "number",
    "cluster",
    "depends",
    "upon",
    "sse",
    "sum",
    "squared",
    "errors",
    "going",
    "use",
    "elbow",
    "method",
    "going",
    "go",
    "details",
    "going",
    "fit",
    "data",
    "fit",
    "find",
    "cluster",
    "us",
    "printed",
    "find",
    "array",
    "five",
    "clusters",
    "fa",
    "print",
    "label",
    "cluster",
    "next",
    "going",
    "plot",
    "data",
    "clusters",
    "new",
    "data",
    "clusters",
    "found",
    "going",
    "use",
    "cc",
    "bond",
    "see",
    "plotted",
    "car",
    "plotted",
    "data",
    "grid",
    "see",
    "five",
    "clusters",
    "probably",
    "would",
    "say",
    "cluster",
    "3",
    "cluster",
    "zero",
    "close",
    "might",
    "depend",
    "see",
    "exactly",
    "going",
    "say",
    "initially",
    "main",
    "challenge",
    "clustering",
    "define",
    "number",
    "centers",
    "see",
    "third",
    "center",
    "zeroth",
    "cluster",
    "third",
    "cluster",
    "zeroth",
    "cluster",
    "close",
    "guys",
    "probably",
    "could",
    "one",
    "another",
    "cluster",
    "another",
    "disadvantage",
    "exactly",
    "know",
    "points",
    "arranged",
    "difficult",
    "force",
    "data",
    "cluster",
    "makes",
    "analysis",
    "little",
    "different",
    "works",
    "fine",
    "sometimes",
    "might",
    "difficult",
    "code",
    "clustering",
    "let",
    "understand",
    "exactly",
    "c",
    "means",
    "clustering",
    "fuzzy",
    "see",
    "means",
    "extension",
    "clustering",
    "popular",
    "simple",
    "clustering",
    "technique",
    "fuzzy",
    "clustering",
    "also",
    "referred",
    "soft",
    "clustering",
    "form",
    "clustering",
    "data",
    "point",
    "belong",
    "one",
    "cluster",
    "tries",
    "find",
    "heart",
    "clusters",
    "point",
    "belongs",
    "one",
    "cluster",
    "whereas",
    "fuzzy",
    "c",
    "means",
    "discovers",
    "soft",
    "clusters",
    "soft",
    "cluster",
    "point",
    "belong",
    "one",
    "cluster",
    "time",
    "certain",
    "affinity",
    "value",
    "towards",
    "4zc",
    "means",
    "assigns",
    "degree",
    "membership",
    "0",
    "1",
    "object",
    "given",
    "cluster",
    "stipulation",
    "sum",
    "z",
    "membership",
    "object",
    "cluster",
    "belongs",
    "must",
    "equal",
    "1",
    "degree",
    "membership",
    "particular",
    "point",
    "pull",
    "clusters",
    "add",
    "get",
    "1",
    "one",
    "logic",
    "behind",
    "fuzzy",
    "c",
    "means",
    "affinity",
    "proportional",
    "distance",
    "point",
    "center",
    "cluster",
    "pros",
    "cons",
    "fuzzy",
    "see",
    "means",
    "first",
    "allows",
    "data",
    "point",
    "multiple",
    "cluster",
    "pro",
    "neutral",
    "representation",
    "behavior",
    "jeans",
    "jeans",
    "usually",
    "involved",
    "multiple",
    "functions",
    "good",
    "type",
    "clustering",
    "talking",
    "genes",
    "first",
    "talk",
    "cons",
    "define",
    "c",
    "number",
    "clusters",
    "k",
    "next",
    "need",
    "determine",
    "membership",
    "cutoff",
    "value",
    "also",
    "takes",
    "lot",
    "clusters",
    "sensitive",
    "initial",
    "assignment",
    "centroid",
    "slight",
    "change",
    "deviation",
    "center",
    "going",
    "result",
    "different",
    "kind",
    "know",
    "funny",
    "kind",
    "output",
    "fuzzy",
    "see",
    "means",
    "one",
    "major",
    "disadvantage",
    "c",
    "means",
    "clustering",
    "algorithm",
    "give",
    "particular",
    "output",
    "let",
    "look",
    "throat",
    "type",
    "clustering",
    "hierarchical",
    "clustering",
    "hierarchical",
    "clustering",
    "alternative",
    "approach",
    "builds",
    "hierarchy",
    "bottom",
    "top",
    "bottom",
    "require",
    "specify",
    "number",
    "clusters",
    "beforehand",
    "algorithm",
    "works",
    "first",
    "put",
    "data",
    "point",
    "cluster",
    "closest",
    "cluster",
    "combine",
    "one",
    "cluster",
    "repeat",
    "step",
    "till",
    "data",
    "points",
    "single",
    "cluster",
    "two",
    "types",
    "hierarchical",
    "clustering",
    "one",
    "number",
    "80",
    "plus",
    "string",
    "one",
    "division",
    "clustering",
    "cumulative",
    "clustering",
    "bills",
    "dendogram",
    "bottom",
    "level",
    "division",
    "clustering",
    "starts",
    "data",
    "points",
    "one",
    "cluster",
    "fruit",
    "cluster",
    "hierarchical",
    "clustering",
    "also",
    "sort",
    "pros",
    "cons",
    "pros",
    "know",
    "assumption",
    "particular",
    "number",
    "cluster",
    "required",
    "may",
    "correspond",
    "meaningful",
    "tax",
    "anomalies",
    "whereas",
    "talk",
    "cons",
    "decision",
    "made",
    "combine",
    "two",
    "clusters",
    "undone",
    "one",
    "major",
    "disadvantage",
    "hierarchical",
    "clustering",
    "becomes",
    "slow",
    "talked",
    "large",
    "data",
    "sets",
    "nowadays",
    "think",
    "every",
    "industry",
    "using",
    "last",
    "year",
    "collecting",
    "large",
    "amounts",
    "data",
    "hierarchical",
    "clustering",
    "act",
    "best",
    "method",
    "someone",
    "might",
    "need",
    "go",
    "hello",
    "everyone",
    "welcome",
    "interesting",
    "session",
    "prairie",
    "algorithm",
    "many",
    "us",
    "visited",
    "retails",
    "shops",
    "walmart",
    "target",
    "household",
    "needs",
    "well",
    "let",
    "say",
    "planning",
    "buy",
    "new",
    "iphone",
    "target",
    "would",
    "typically",
    "search",
    "model",
    "visiting",
    "mobile",
    "section",
    "stove",
    "select",
    "product",
    "head",
    "towards",
    "billing",
    "counter",
    "today",
    "world",
    "goal",
    "organization",
    "increase",
    "revenue",
    "done",
    "pitching",
    "one",
    "worked",
    "time",
    "customer",
    "answer",
    "clearly",
    "hence",
    "organization",
    "began",
    "mining",
    "data",
    "relating",
    "frequently",
    "bought",
    "items",
    "market",
    "basket",
    "analysis",
    "one",
    "key",
    "techniques",
    "used",
    "large",
    "retailers",
    "uncover",
    "associations",
    "items",
    "examples",
    "could",
    "customers",
    "purchase",
    "bread",
    "60",
    "percent",
    "likelihood",
    "also",
    "purchase",
    "jam",
    "customers",
    "purchase",
    "laptops",
    "likely",
    "purchase",
    "laptop",
    "bags",
    "well",
    "try",
    "find",
    "associations",
    "different",
    "items",
    "products",
    "sold",
    "together",
    "gives",
    "assisting",
    "right",
    "product",
    "placement",
    "typically",
    "figures",
    "products",
    "bought",
    "together",
    "organizations",
    "place",
    "products",
    "similar",
    "manner",
    "example",
    "people",
    "buy",
    "bread",
    "also",
    "tend",
    "buy",
    "butter",
    "right",
    "marketing",
    "team",
    "retail",
    "stores",
    "target",
    "customers",
    "buy",
    "bread",
    "butter",
    "provide",
    "offer",
    "buy",
    "item",
    "suppose",
    "x",
    "customer",
    "buys",
    "bread",
    "butter",
    "sees",
    "discount",
    "offer",
    "x",
    "encouraged",
    "spend",
    "buy",
    "eggs",
    "market",
    "basket",
    "analysis",
    "going",
    "talk",
    "session",
    "association",
    "rule",
    "mining",
    "prayer",
    "real",
    "corinth",
    "association",
    "rule",
    "thought",
    "relationship",
    "elaborate",
    "come",
    "rule",
    "suppose",
    "item",
    "bought",
    "customer",
    "chances",
    "item",
    "b",
    "picked",
    "customer",
    "transaction",
    "id",
    "found",
    "need",
    "understand",
    "cash",
    "reality",
    "rather",
    "pattern",
    "comes",
    "force",
    "two",
    "elements",
    "rule",
    "first",
    "second",
    "also",
    "known",
    "antecedent",
    "item",
    "group",
    "items",
    "typically",
    "found",
    "item",
    "set",
    "later",
    "one",
    "called",
    "consequent",
    "comes",
    "along",
    "item",
    "antecedent",
    "group",
    "group",
    "antecedents",
    "purchase",
    "look",
    "image",
    "arrow",
    "b",
    "means",
    "person",
    "buys",
    "item",
    "also",
    "buy",
    "item",
    "b",
    "probably",
    "item",
    "simple",
    "example",
    "gave",
    "x",
    "small",
    "example",
    "thousands",
    "thousands",
    "items",
    "go",
    "proof",
    "additional",
    "data",
    "scientist",
    "data",
    "imagine",
    "much",
    "profit",
    "make",
    "data",
    "scientist",
    "provides",
    "right",
    "examples",
    "right",
    "placement",
    "items",
    "get",
    "lot",
    "insights",
    "association",
    "rule",
    "mining",
    "good",
    "algorithm",
    "helps",
    "business",
    "make",
    "profit",
    "let",
    "see",
    "algorithm",
    "works",
    "association",
    "rule",
    "mining",
    "building",
    "rules",
    "seen",
    "one",
    "rule",
    "buy",
    "slight",
    "possibility",
    "chance",
    "might",
    "buy",
    "also",
    "type",
    "relationship",
    "find",
    "relationship",
    "two",
    "items",
    "known",
    "single",
    "cardinality",
    "customer",
    "bought",
    "b",
    "also",
    "wants",
    "buy",
    "c",
    "customer",
    "bought",
    "b",
    "c",
    "also",
    "wants",
    "buy",
    "cases",
    "cardinality",
    "usually",
    "increases",
    "lot",
    "combination",
    "around",
    "data",
    "around",
    "data",
    "items",
    "imagine",
    "many",
    "rules",
    "going",
    "create",
    "product",
    "association",
    "rule",
    "mining",
    "measures",
    "end",
    "creating",
    "tens",
    "thousands",
    "rules",
    "priori",
    "algorithm",
    "comes",
    "get",
    "priori",
    "algorithm",
    "let",
    "understand",
    "maths",
    "behind",
    "three",
    "types",
    "matrices",
    "help",
    "measure",
    "association",
    "support",
    "confidence",
    "lift",
    "support",
    "frequency",
    "item",
    "combination",
    "item",
    "arb",
    "basically",
    "frequency",
    "items",
    "bought",
    "combination",
    "frequency",
    "item",
    "bought",
    "filter",
    "items",
    "bought",
    "less",
    "frequently",
    "one",
    "measures",
    "support",
    "confidence",
    "tells",
    "us",
    "conference",
    "gives",
    "us",
    "often",
    "items",
    "nb",
    "occur",
    "together",
    "given",
    "number",
    "times",
    "occur",
    "also",
    "helps",
    "us",
    "solve",
    "lot",
    "problems",
    "somebody",
    "buying",
    "b",
    "together",
    "buying",
    "see",
    "rule",
    "see",
    "point",
    "time",
    "solves",
    "another",
    "problem",
    "obviously",
    "need",
    "analyze",
    "process",
    "people",
    "barely",
    "according",
    "sages",
    "define",
    "minimum",
    "support",
    "confidence",
    "set",
    "values",
    "put",
    "values",
    "algorithm",
    "filter",
    "data",
    "create",
    "different",
    "rules",
    "suppose",
    "even",
    "filtering",
    "like",
    "five",
    "thousand",
    "rules",
    "every",
    "item",
    "create",
    "rules",
    "practically",
    "impossible",
    "need",
    "third",
    "calculation",
    "lift",
    "lift",
    "basically",
    "strength",
    "rule",
    "let",
    "look",
    "denominator",
    "formula",
    "given",
    "see",
    "independent",
    "support",
    "values",
    "gives",
    "us",
    "independent",
    "occurrence",
    "probability",
    "obviously",
    "lot",
    "difference",
    "random",
    "occurrence",
    "association",
    "denominator",
    "lift",
    "means",
    "occurrence",
    "randomness",
    "rather",
    "occurs",
    "association",
    "left",
    "final",
    "verdict",
    "know",
    "whether",
    "spend",
    "time",
    "particular",
    "rule",
    "got",
    "let",
    "look",
    "simple",
    "example",
    "association",
    "rule",
    "mining",
    "suppose",
    "set",
    "items",
    "b",
    "c",
    "e",
    "set",
    "transactions",
    "t1",
    "t2",
    "t3",
    "t4",
    "t5",
    "see",
    "transactions",
    "t1",
    "abc",
    "cd",
    "t3b",
    "cdt",
    "e",
    "t5",
    "bce",
    "generally",
    "create",
    "rules",
    "association",
    "rules",
    "gives",
    "c",
    "gives",
    "gift",
    "c",
    "b",
    "c",
    "gives",
    "basically",
    "means",
    "person",
    "buys",
    "likely",
    "buy",
    "person",
    "c",
    "likely",
    "buy",
    "look",
    "last",
    "one",
    "person",
    "buys",
    "b",
    "c",
    "likely",
    "buy",
    "item",
    "well",
    "calculate",
    "support",
    "confidence",
    "lift",
    "using",
    "rules",
    "see",
    "table",
    "rule",
    "support",
    "confidence",
    "handle",
    "lift",
    "values",
    "let",
    "discuss",
    "prairie",
    "priori",
    "algorithm",
    "uses",
    "frequent",
    "itemsets",
    "generate",
    "association",
    "rule",
    "based",
    "concept",
    "subset",
    "frequent",
    "itemsets",
    "must",
    "also",
    "frequent",
    "item",
    "set",
    "raises",
    "question",
    "exactly",
    "frequent",
    "item",
    "set",
    "frequent",
    "item",
    "set",
    "item",
    "set",
    "whose",
    "support",
    "value",
    "greater",
    "threshold",
    "value",
    "discussed",
    "marketing",
    "team",
    "according",
    "says",
    "minimum",
    "threshold",
    "value",
    "confidence",
    "well",
    "support",
    "frequent",
    "itemsets",
    "animset",
    "support",
    "value",
    "greater",
    "threshold",
    "value",
    "already",
    "specified",
    "example",
    "b",
    "freaker",
    "item",
    "set",
    "b",
    "also",
    "frequent",
    "itemsets",
    "individually",
    "let",
    "consider",
    "following",
    "transaction",
    "make",
    "things",
    "easier",
    "suppose",
    "transactions",
    "1",
    "2",
    "3",
    "4",
    "5",
    "items",
    "1",
    "1",
    "3",
    "4",
    "2",
    "2",
    "3",
    "5",
    "t3",
    "1",
    "2",
    "3",
    "5",
    "4",
    "5",
    "5",
    "1",
    "3",
    "5",
    "first",
    "step",
    "build",
    "list",
    "items",
    "sets",
    "size",
    "1",
    "using",
    "transactional",
    "data",
    "one",
    "thing",
    "note",
    "minimum",
    "support",
    "count",
    "given",
    "let",
    "suppose",
    "first",
    "step",
    "create",
    "item",
    "sets",
    "size",
    "1",
    "calculate",
    "support",
    "values",
    "see",
    "table",
    "see",
    "one",
    "item",
    "sets",
    "1",
    "2",
    "3",
    "4",
    "5",
    "support",
    "values",
    "remember",
    "formula",
    "support",
    "frequency",
    "divided",
    "total",
    "number",
    "occurrence",
    "see",
    "items",
    "one",
    "support",
    "3",
    "see",
    "item",
    "set",
    "one",
    "1",
    "3",
    "see",
    "frequency",
    "1",
    "2",
    "3",
    "see",
    "item",
    "set",
    "support",
    "one",
    "occurs",
    "transaction",
    "one",
    "minimum",
    "support",
    "value",
    "2",
    "going",
    "eliminated",
    "final",
    "table",
    "table",
    "f1",
    "item",
    "sets",
    "1",
    "2",
    "3",
    "5",
    "support",
    "values",
    "3",
    "3",
    "4",
    "4",
    "next",
    "step",
    "create",
    "adam",
    "sets",
    "size",
    "2",
    "calculate",
    "support",
    "values",
    "combination",
    "item",
    "sets",
    "f1",
    "final",
    "table",
    "carded",
    "going",
    "used",
    "iteration",
    "get",
    "table",
    "c",
    "see",
    "1",
    "2",
    "1",
    "3",
    "1",
    "5",
    "2",
    "3",
    "2",
    "5",
    "3",
    "5",
    "calculate",
    "support",
    "see",
    "item",
    "set",
    "1",
    "comma",
    "2",
    "support",
    "one",
    "less",
    "specified",
    "threshold",
    "going",
    "discard",
    "look",
    "table",
    "f",
    "2",
    "1",
    "comma",
    "3",
    "1",
    "5",
    "2",
    "3",
    "2",
    "5",
    "3",
    "5",
    "going",
    "move",
    "forward",
    "create",
    "atoms",
    "size",
    "3",
    "calculate",
    "support",
    "values",
    "combinations",
    "going",
    "used",
    "item",
    "set",
    "f",
    "particular",
    "iterations",
    "calculating",
    "support",
    "values",
    "let",
    "perform",
    "proning",
    "data",
    "set",
    "pruning",
    "combinations",
    "made",
    "device",
    "c",
    "3",
    "item",
    "sets",
    "check",
    "another",
    "subset",
    "whose",
    "support",
    "less",
    "minimum",
    "support",
    "value",
    "frequent",
    "items",
    "means",
    "look",
    "item",
    "sets",
    "1",
    "2",
    "3",
    "1",
    "2",
    "1",
    "3",
    "2",
    "3",
    "4",
    "first",
    "one",
    "see",
    "look",
    "subsets",
    "one",
    "two",
    "three",
    "1",
    "comma",
    "2",
    "well",
    "going",
    "discard",
    "whole",
    "item",
    "set",
    "goes",
    "second",
    "one",
    "one",
    "five",
    "discarded",
    "previous",
    "set",
    "previous",
    "step",
    "going",
    "discard",
    "also",
    "leaves",
    "us",
    "two",
    "factors",
    "1",
    "3",
    "5",
    "set",
    "two",
    "three",
    "five",
    "support",
    "2",
    "2",
    "well",
    "create",
    "table",
    "c",
    "using",
    "four",
    "elements",
    "going",
    "one",
    "item",
    "set",
    "1",
    "2",
    "3",
    "5",
    "look",
    "table",
    "transaction",
    "table",
    "one",
    "two",
    "three",
    "five",
    "appears",
    "one",
    "support",
    "one",
    "since",
    "c",
    "support",
    "whole",
    "table",
    "c",
    "4",
    "less",
    "2",
    "going",
    "stop",
    "return",
    "previous",
    "item",
    "set",
    "3",
    "3",
    "frequent",
    "itemsets",
    "1",
    "3",
    "5",
    "2",
    "3",
    "5",
    "let",
    "assume",
    "minimum",
    "confidence",
    "value",
    "60",
    "percent",
    "going",
    "generate",
    "subsets",
    "frequent",
    "itemsets",
    "equals",
    "1",
    "comma",
    "3",
    "comma",
    "5",
    "item",
    "set",
    "get",
    "subset",
    "one",
    "three",
    "one",
    "five",
    "three",
    "five",
    "one",
    "three",
    "five",
    "similarly",
    "2",
    "3",
    "5",
    "get",
    "three",
    "five",
    "three",
    "five",
    "two",
    "three",
    "five",
    "rule",
    "states",
    "every",
    "subset",
    "output",
    "rule",
    "gives",
    "something",
    "like",
    "gives",
    "i2s",
    "implies",
    "recommends",
    "possible",
    "support",
    "divided",
    "support",
    "greater",
    "equal",
    "minimum",
    "confidence",
    "value",
    "applying",
    "rules",
    "item",
    "set",
    "f3",
    "get",
    "rule",
    "1",
    "1",
    "3",
    "gives",
    "1",
    "comma",
    "3",
    "comma",
    "5",
    "3",
    "means",
    "1",
    "3",
    "gives",
    "5",
    "confidence",
    "equal",
    "support",
    "1",
    "comma",
    "3",
    "comma",
    "fire",
    "driver",
    "support",
    "1",
    "comma",
    "3",
    "equals",
    "2",
    "3",
    "66",
    "greater",
    "60",
    "percent",
    "rule",
    "1",
    "selected",
    "come",
    "rule",
    "2",
    "1",
    "comma",
    "5",
    "gives",
    "1",
    "comma",
    "3",
    "comma",
    "5",
    "1",
    "5",
    "means",
    "1",
    "5",
    "implies",
    "also",
    "going",
    "three",
    "know",
    "calculate",
    "confidence",
    "one",
    "going",
    "support",
    "1",
    "3",
    "5",
    "whereby",
    "support",
    "gives",
    "us",
    "hundred",
    "percent",
    "means",
    "rule",
    "2",
    "selected",
    "well",
    "look",
    "rule",
    "506",
    "similarly",
    "select",
    "3",
    "gives",
    "1",
    "3",
    "5",
    "3",
    "means",
    "three",
    "also",
    "get",
    "one",
    "five",
    "confidence",
    "comes",
    "50",
    "less",
    "given",
    "60",
    "percent",
    "target",
    "going",
    "reject",
    "rule",
    "goes",
    "rule",
    "number",
    "six",
    "one",
    "thing",
    "keep",
    "mind",
    "rule",
    "1",
    "rule",
    "5",
    "look",
    "lot",
    "similar",
    "really",
    "depends",
    "left",
    "hand",
    "side",
    "arrow",
    "sides",
    "arrow",
    "possibility",
    "sure",
    "guys",
    "understand",
    "exactly",
    "rows",
    "proceed",
    "rules",
    "let",
    "see",
    "implement",
    "python",
    "right",
    "going",
    "create",
    "new",
    "python",
    "going",
    "use",
    "chapter",
    "notebook",
    "free",
    "use",
    "sort",
    "id",
    "going",
    "name",
    "priority",
    "first",
    "thing",
    "going",
    "using",
    "online",
    "transactional",
    "data",
    "retail",
    "store",
    "generating",
    "association",
    "rules",
    "firstly",
    "need",
    "get",
    "pandas",
    "ml",
    "x",
    "10",
    "libraries",
    "imported",
    "read",
    "file",
    "see",
    "using",
    "online",
    "retail",
    "dot",
    "xlsx",
    "format",
    "file",
    "ml",
    "extant",
    "going",
    "import",
    "prairie",
    "association",
    "rules",
    "comes",
    "mx",
    "see",
    "invoice",
    "stock",
    "quote",
    "description",
    "quantity",
    "invoice",
    "data",
    "unit",
    "price",
    "customer",
    "id",
    "country",
    "next",
    "step",
    "going",
    "data",
    "cleanup",
    "includes",
    "removing",
    "spaces",
    "descriptions",
    "drop",
    "rules",
    "invoice",
    "numbers",
    "remove",
    "great",
    "grab",
    "transactions",
    "use",
    "us",
    "see",
    "output",
    "like",
    "five",
    "hundred",
    "thirty",
    "two",
    "thousand",
    "rows",
    "eight",
    "columns",
    "cleanup",
    "need",
    "consolidate",
    "items",
    "one",
    "transaction",
    "per",
    "row",
    "product",
    "sake",
    "keeping",
    "data",
    "set",
    "small",
    "looking",
    "sales",
    "france",
    "see",
    "excluded",
    "says",
    "looking",
    "sales",
    "france",
    "lot",
    "zeros",
    "data",
    "also",
    "need",
    "make",
    "sure",
    "positive",
    "values",
    "converted",
    "1",
    "anything",
    "less",
    "zero",
    "set",
    "0",
    "see",
    "still",
    "392",
    "rose",
    "going",
    "encode",
    "see",
    "check",
    "structured",
    "data",
    "properly",
    "step",
    "going",
    "generate",
    "frequent",
    "itemsets",
    "support",
    "least",
    "seven",
    "percent",
    "number",
    "chosen",
    "get",
    "close",
    "enough",
    "generated",
    "rules",
    "corresponding",
    "support",
    "confidence",
    "lift",
    "go",
    "ahead",
    "see",
    "minimum",
    "support",
    "add",
    "another",
    "constraint",
    "rules",
    "lift",
    "greater",
    "6",
    "conference",
    "greater",
    "see",
    "side",
    "side",
    "association",
    "rule",
    "antecedent",
    "consequence",
    "support",
    "confidence",
    "lift",
    "leverage",
    "conviction",
    "guys",
    "session",
    "create",
    "association",
    "rules",
    "using",
    "api",
    "real",
    "gold",
    "tone",
    "helps",
    "lot",
    "marketing",
    "business",
    "runs",
    "principle",
    "market",
    "basket",
    "analysis",
    "exactly",
    "big",
    "companies",
    "like",
    "walmart",
    "reliance",
    "target",
    "even",
    "ikea",
    "hope",
    "got",
    "know",
    "exactly",
    "association",
    "rule",
    "mining",
    "lift",
    "confidence",
    "support",
    "create",
    "association",
    "rules",
    "guys",
    "reinforcement",
    "learning",
    "dying",
    "part",
    "machine",
    "learning",
    "agent",
    "put",
    "environment",
    "learns",
    "behave",
    "environment",
    "performing",
    "certain",
    "actions",
    "okay",
    "basically",
    "performs",
    "actions",
    "either",
    "gets",
    "rewards",
    "actions",
    "gets",
    "punishment",
    "observing",
    "reward",
    "gets",
    "actions",
    "reinforcement",
    "learning",
    "taking",
    "appropriate",
    "action",
    "order",
    "maximize",
    "reward",
    "particular",
    "situation",
    "guys",
    "supervised",
    "learning",
    "training",
    "data",
    "comprises",
    "input",
    "expected",
    "output",
    "model",
    "trained",
    "expected",
    "output",
    "comes",
    "reinforcement",
    "learning",
    "expected",
    "output",
    "reinforcement",
    "agent",
    "decides",
    "actions",
    "take",
    "order",
    "perform",
    "given",
    "task",
    "absence",
    "training",
    "data",
    "set",
    "bound",
    "learn",
    "experience",
    "alright",
    "reinforcement",
    "learning",
    "agent",
    "put",
    "unknown",
    "environment",
    "going",
    "use",
    "hit",
    "trial",
    "method",
    "order",
    "figure",
    "environment",
    "come",
    "outcome",
    "okay",
    "let",
    "look",
    "reinforcement",
    "learning",
    "within",
    "analogy",
    "consider",
    "scenario",
    "baby",
    "learning",
    "walk",
    "scenario",
    "go",
    "two",
    "ways",
    "first",
    "case",
    "baby",
    "starts",
    "walking",
    "makes",
    "candy",
    "candy",
    "basically",
    "reward",
    "going",
    "get",
    "since",
    "candy",
    "end",
    "goal",
    "baby",
    "happy",
    "positive",
    "okay",
    "baby",
    "happy",
    "gets",
    "rewarded",
    "set",
    "candies",
    "another",
    "way",
    "could",
    "go",
    "baby",
    "starts",
    "walking",
    "falls",
    "due",
    "hurdle",
    "baby",
    "gets",
    "hot",
    "get",
    "candy",
    "obviously",
    "baby",
    "sad",
    "negative",
    "reward",
    "okay",
    "say",
    "setback",
    "like",
    "humans",
    "learn",
    "mistakes",
    "trial",
    "error",
    "reinforcement",
    "learning",
    "also",
    "similar",
    "okay",
    "agent",
    "basically",
    "baby",
    "reward",
    "candy",
    "okay",
    "many",
    "hurdles",
    "agent",
    "supposed",
    "find",
    "best",
    "possible",
    "path",
    "read",
    "reward",
    "guys",
    "hope",
    "clear",
    "reinforcement",
    "learning",
    "let",
    "look",
    "reinforcement",
    "learning",
    "process",
    "generally",
    "reinforcement",
    "learning",
    "system",
    "two",
    "main",
    "components",
    "right",
    "first",
    "agent",
    "second",
    "one",
    "environment",
    "previous",
    "case",
    "saw",
    "agent",
    "baby",
    "environment",
    "living",
    "room",
    "baby",
    "crawling",
    "okay",
    "environment",
    "setting",
    "agent",
    "acting",
    "agent",
    "represents",
    "reinforcement",
    "learning",
    "algorithm",
    "guys",
    "reinforcement",
    "learning",
    "process",
    "starts",
    "environment",
    "sends",
    "state",
    "agent",
    "take",
    "actions",
    "based",
    "observations",
    "turn",
    "environment",
    "send",
    "next",
    "state",
    "respective",
    "reward",
    "back",
    "agent",
    "agent",
    "update",
    "knowledge",
    "reward",
    "returned",
    "environment",
    "uses",
    "evaluate",
    "previous",
    "action",
    "guys",
    "loop",
    "keeps",
    "continuing",
    "environment",
    "sends",
    "terminal",
    "state",
    "means",
    "agent",
    "accomplished",
    "tasks",
    "finally",
    "gets",
    "reward",
    "okay",
    "exactly",
    "depicted",
    "scenario",
    "agent",
    "keeps",
    "climbing",
    "ladders",
    "reaches",
    "reward",
    "understand",
    "better",
    "let",
    "suppose",
    "agent",
    "learning",
    "play",
    "counter",
    "strike",
    "okay",
    "let",
    "break",
    "initially",
    "rl",
    "agent",
    "basically",
    "player",
    "player",
    "let",
    "say",
    "player",
    "one",
    "trying",
    "learn",
    "play",
    "game",
    "okay",
    "collects",
    "state",
    "environment",
    "okay",
    "could",
    "first",
    "date",
    "based",
    "state",
    "agent",
    "take",
    "action",
    "okay",
    "action",
    "anything",
    "causes",
    "result",
    "almost",
    "left",
    "right",
    "also",
    "considered",
    "action",
    "okay",
    "initially",
    "action",
    "going",
    "random",
    "obviously",
    "first",
    "time",
    "pick",
    "going",
    "master",
    "going",
    "try",
    "different",
    "actions",
    "want",
    "pick",
    "random",
    "action",
    "beginning",
    "environment",
    "going",
    "give",
    "new",
    "state",
    "clearing",
    "environment",
    "going",
    "give",
    "new",
    "state",
    "agent",
    "player",
    "maybe",
    "across",
    "th",
    "one",
    "stage",
    "player",
    "get",
    "reward",
    "one",
    "environment",
    "cleared",
    "stage",
    "reward",
    "anything",
    "additional",
    "points",
    "coins",
    "anything",
    "like",
    "okay",
    "basically",
    "loop",
    "keeps",
    "going",
    "player",
    "dead",
    "reaches",
    "destination",
    "okay",
    "continuously",
    "outputs",
    "sequence",
    "states",
    "actions",
    "rewards",
    "guys",
    "small",
    "example",
    "show",
    "reinforcement",
    "learning",
    "process",
    "works",
    "start",
    "initial",
    "state",
    "player",
    "clothes",
    "state",
    "gets",
    "reward",
    "environment",
    "give",
    "another",
    "stage",
    "player",
    "clears",
    "state",
    "going",
    "get",
    "another",
    "award",
    "going",
    "keep",
    "happening",
    "player",
    "reaches",
    "destination",
    "right",
    "guys",
    "hope",
    "clear",
    "let",
    "move",
    "look",
    "reinforcement",
    "learning",
    "definitions",
    "concepts",
    "aware",
    "studying",
    "reinforcement",
    "learning",
    "let",
    "look",
    "definitions",
    "first",
    "agent",
    "agent",
    "basically",
    "reinforcement",
    "learning",
    "algorithm",
    "learns",
    "trial",
    "error",
    "okay",
    "agent",
    "takes",
    "actions",
    "like",
    "example",
    "soldier",
    "navigating",
    "game",
    "also",
    "action",
    "okay",
    "moves",
    "left",
    "right",
    "shoots",
    "somebody",
    "also",
    "action",
    "okay",
    "agent",
    "responsible",
    "taking",
    "actions",
    "environment",
    "environment",
    "whole",
    "game",
    "okay",
    "basically",
    "world",
    "agent",
    "moves",
    "environment",
    "takes",
    "agents",
    "current",
    "state",
    "action",
    "input",
    "returns",
    "agency",
    "reward",
    "next",
    "state",
    "output",
    "alright",
    "next",
    "action",
    "possible",
    "steps",
    "agent",
    "take",
    "called",
    "actions",
    "like",
    "said",
    "moving",
    "right",
    "left",
    "shooting",
    "alright",
    "state",
    "state",
    "basically",
    "current",
    "condition",
    "returned",
    "environment",
    "whichever",
    "state",
    "state",
    "1",
    "state",
    "represents",
    "current",
    "condition",
    "right",
    "next",
    "reward",
    "reward",
    "basically",
    "instant",
    "return",
    "environment",
    "appraise",
    "last",
    "action",
    "okay",
    "anything",
    "like",
    "coins",
    "audition",
    "two",
    "points",
    "basically",
    "reward",
    "given",
    "agent",
    "clears",
    "specific",
    "stages",
    "next",
    "policy",
    "policies",
    "basically",
    "strategy",
    "agent",
    "uses",
    "find",
    "next",
    "action",
    "based",
    "current",
    "state",
    "policy",
    "strategy",
    "approach",
    "game",
    "value",
    "expected",
    "return",
    "discount",
    "value",
    "action",
    "value",
    "little",
    "bit",
    "confusing",
    "right",
    "move",
    "understand",
    "talking",
    "kima",
    "okay",
    "value",
    "basically",
    "return",
    "get",
    "discount",
    "okay",
    "discount",
    "explain",
    "furthest",
    "lines",
    "action",
    "value",
    "action",
    "value",
    "also",
    "known",
    "q",
    "value",
    "okay",
    "similar",
    "value",
    "except",
    "takes",
    "extra",
    "parameter",
    "current",
    "action",
    "basically",
    "find",
    "q",
    "value",
    "depending",
    "particular",
    "action",
    "took",
    "right",
    "guys",
    "get",
    "confused",
    "value",
    "action",
    "value",
    "look",
    "examples",
    "slides",
    "understand",
    "better",
    "okay",
    "guys",
    "make",
    "sure",
    "familiar",
    "terms",
    "seeing",
    "lot",
    "terms",
    "slides",
    "right",
    "move",
    "like",
    "discuss",
    "concepts",
    "okay",
    "first",
    "discuss",
    "reward",
    "maximization",
    "already",
    "realized",
    "basic",
    "aim",
    "rl",
    "agent",
    "maximize",
    "reward",
    "happen",
    "let",
    "try",
    "understand",
    "depth",
    "agent",
    "must",
    "trained",
    "way",
    "takes",
    "best",
    "action",
    "reward",
    "end",
    "goal",
    "reinforcement",
    "learning",
    "maximize",
    "reward",
    "based",
    "set",
    "actions",
    "let",
    "explain",
    "small",
    "game",
    "figure",
    "see",
    "fox",
    "meat",
    "tiger",
    "agent",
    "basically",
    "fox",
    "end",
    "goal",
    "eat",
    "maximum",
    "amount",
    "meat",
    "eaten",
    "tiger",
    "since",
    "fox",
    "clever",
    "fellow",
    "eats",
    "meat",
    "closer",
    "rather",
    "meat",
    "closer",
    "tiger",
    "closer",
    "tiger",
    "higher",
    "chances",
    "getting",
    "killed",
    "rewards",
    "near",
    "tiger",
    "even",
    "bigger",
    "meat",
    "chunks",
    "discounted",
    "exactly",
    "discounting",
    "means",
    "agent",
    "going",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "risk",
    "right",
    "even",
    "though",
    "meat",
    "chunks",
    "might",
    "larger",
    "want",
    "take",
    "chances",
    "getting",
    "killed",
    "okay",
    "called",
    "discounting",
    "okay",
    "discount",
    "improvise",
    "eat",
    "meat",
    "closer",
    "instead",
    "taking",
    "risks",
    "eating",
    "meat",
    "opponent",
    "right",
    "discounting",
    "reward",
    "works",
    "based",
    "value",
    "called",
    "gamma",
    "discussing",
    "gamma",
    "slides",
    "short",
    "value",
    "gamma",
    "0",
    "okay",
    "smaller",
    "gamma",
    "larger",
    "discount",
    "value",
    "okay",
    "gamma",
    "value",
    "lesser",
    "means",
    "agent",
    "going",
    "explore",
    "going",
    "try",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "okay",
    "gamma",
    "value",
    "closer",
    "1",
    "means",
    "agent",
    "actually",
    "going",
    "explore",
    "going",
    "dry",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "right",
    "explaining",
    "depth",
    "slides",
    "worry",
    "got",
    "clear",
    "concept",
    "yet",
    "understand",
    "reward",
    "maximization",
    "important",
    "step",
    "comes",
    "reinforcement",
    "learning",
    "agent",
    "collect",
    "maximum",
    "rewards",
    "end",
    "game",
    "right",
    "let",
    "look",
    "another",
    "concept",
    "called",
    "exploration",
    "exploitation",
    "exploration",
    "like",
    "name",
    "suggests",
    "exploring",
    "capturing",
    "information",
    "environment",
    "hand",
    "exploitation",
    "using",
    "already",
    "known",
    "exploited",
    "information",
    "heighten",
    "rewards",
    "guys",
    "consider",
    "fox",
    "tiger",
    "example",
    "discussed",
    "fox",
    "eats",
    "meat",
    "chunks",
    "close",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "okay",
    "even",
    "though",
    "might",
    "give",
    "awards",
    "eat",
    "fox",
    "focuses",
    "closest",
    "rewards",
    "never",
    "reach",
    "big",
    "chunks",
    "meat",
    "okay",
    "exploitation",
    "going",
    "use",
    "currently",
    "known",
    "information",
    "going",
    "try",
    "get",
    "rewards",
    "based",
    "information",
    "fox",
    "decides",
    "explore",
    "bit",
    "find",
    "bigger",
    "award",
    "big",
    "chunks",
    "meat",
    "exactly",
    "exploration",
    "agent",
    "going",
    "stick",
    "one",
    "corner",
    "instead",
    "going",
    "explore",
    "entire",
    "environment",
    "try",
    "collect",
    "bigger",
    "rewards",
    "right",
    "guys",
    "hope",
    "clear",
    "exploration",
    "exploitation",
    "let",
    "look",
    "markers",
    "decision",
    "process",
    "guys",
    "basically",
    "mathematical",
    "approach",
    "mapping",
    "solution",
    "reinforcement",
    "learning",
    "way",
    "purpose",
    "reinforcement",
    "learning",
    "solve",
    "markov",
    "decision",
    "process",
    "okay",
    "parameters",
    "used",
    "get",
    "solution",
    "parameters",
    "include",
    "set",
    "actions",
    "set",
    "states",
    "rewards",
    "policy",
    "taking",
    "approach",
    "problem",
    "value",
    "get",
    "okay",
    "sum",
    "agent",
    "must",
    "take",
    "action",
    "transition",
    "start",
    "state",
    "end",
    "state",
    "agent",
    "receive",
    "reward",
    "action",
    "takes",
    "guys",
    "series",
    "actions",
    "taken",
    "agent",
    "define",
    "policy",
    "defines",
    "approach",
    "rewards",
    "collected",
    "define",
    "value",
    "main",
    "goal",
    "maximize",
    "rewards",
    "choosing",
    "optimum",
    "policy",
    "right",
    "let",
    "try",
    "understand",
    "help",
    "shortest",
    "path",
    "problem",
    "sure",
    "lot",
    "might",
    "gone",
    "problem",
    "college",
    "guys",
    "look",
    "graph",
    "aim",
    "find",
    "shortest",
    "path",
    "minimum",
    "possible",
    "cost",
    "value",
    "see",
    "edges",
    "basically",
    "denotes",
    "cost",
    "want",
    "go",
    "c",
    "going",
    "cost",
    "15",
    "points",
    "okay",
    "let",
    "look",
    "done",
    "move",
    "look",
    "problem",
    "problem",
    "set",
    "states",
    "denoted",
    "nodes",
    "abcd",
    "action",
    "traverse",
    "one",
    "node",
    "going",
    "action",
    "similarly",
    "see",
    "action",
    "okay",
    "reward",
    "basically",
    "cost",
    "represented",
    "edge",
    "right",
    "policy",
    "basically",
    "path",
    "choose",
    "reach",
    "destination",
    "let",
    "say",
    "choose",
    "seed",
    "okay",
    "one",
    "policy",
    "order",
    "get",
    "choosing",
    "cd",
    "policy",
    "okay",
    "basically",
    "approaching",
    "problem",
    "guys",
    "start",
    "node",
    "take",
    "baby",
    "steps",
    "destination",
    "initially",
    "clueless",
    "take",
    "next",
    "possible",
    "node",
    "visible",
    "guys",
    "smart",
    "enough",
    "going",
    "choose",
    "see",
    "instead",
    "abcd",
    "abd",
    "right",
    "nodes",
    "see",
    "want",
    "traverse",
    "note",
    "must",
    "choose",
    "wise",
    "path",
    "red",
    "calculate",
    "path",
    "highest",
    "cost",
    "path",
    "give",
    "maximum",
    "rewards",
    "guys",
    "simple",
    "problem",
    "drank",
    "calculate",
    "shortest",
    "path",
    "traversing",
    "nodes",
    "travels",
    "cd",
    "gives",
    "maximum",
    "reward",
    "okay",
    "gives",
    "65",
    "policy",
    "would",
    "give",
    "okay",
    "go",
    "abd",
    "would",
    "40",
    "compare",
    "cd",
    "gives",
    "reward",
    "obviously",
    "going",
    "go",
    "cb",
    "okay",
    "guys",
    "simple",
    "problem",
    "order",
    "understand",
    "markov",
    "decision",
    "process",
    "works",
    "right",
    "guys",
    "want",
    "ask",
    "question",
    "think",
    "hear",
    "perform",
    "exploration",
    "perform",
    "exploitation",
    "policy",
    "example",
    "exploitation",
    "explore",
    "nodes",
    "okay",
    "selected",
    "three",
    "notes",
    "traverse",
    "called",
    "exploitation",
    "must",
    "always",
    "explore",
    "different",
    "notes",
    "find",
    "optimal",
    "policy",
    "case",
    "obviously",
    "cd",
    "highest",
    "reward",
    "going",
    "cd",
    "generally",
    "simple",
    "lot",
    "nodes",
    "hundreds",
    "notes",
    "traverse",
    "like",
    "50",
    "60",
    "policies",
    "okay",
    "50",
    "60",
    "different",
    "policies",
    "make",
    "sure",
    "explore",
    "policies",
    "decide",
    "optimum",
    "policy",
    "give",
    "maximum",
    "reward",
    "guys",
    "perform",
    "part",
    "let",
    "try",
    "understand",
    "math",
    "behind",
    "demo",
    "okay",
    "demo",
    "using",
    "q",
    "learning",
    "algorithm",
    "type",
    "reinforcement",
    "learning",
    "algorithm",
    "okay",
    "simple",
    "means",
    "take",
    "best",
    "possible",
    "actions",
    "reach",
    "goal",
    "get",
    "rewards",
    "right",
    "let",
    "try",
    "understand",
    "example",
    "guys",
    "exactly",
    "running",
    "demo",
    "make",
    "sure",
    "understand",
    "properly",
    "okay",
    "goal",
    "going",
    "place",
    "agent",
    "one",
    "rooms",
    "okay",
    "basically",
    "squares",
    "see",
    "rooms",
    "ok",
    "0",
    "room",
    "room",
    "three",
    "room",
    "one",
    "room",
    "also",
    "room",
    "basically",
    "way",
    "outside",
    "building",
    "right",
    "going",
    "going",
    "place",
    "agent",
    "one",
    "rooms",
    "goal",
    "reach",
    "outside",
    "building",
    "okay",
    "outside",
    "building",
    "room",
    "number",
    "five",
    "okay",
    "spaces",
    "basically",
    "doors",
    "means",
    "go",
    "zero",
    "four",
    "go",
    "4",
    "3",
    "3",
    "1",
    "1",
    "5",
    "similarly",
    "3",
    "2",
    "ca",
    "go",
    "5",
    "2",
    "directly",
    "right",
    "certain",
    "set",
    "rooms",
    "get",
    "connected",
    "directly",
    "okay",
    "like",
    "mentioned",
    "room",
    "numbered",
    "0",
    "4",
    "outside",
    "building",
    "numbered",
    "five",
    "one",
    "thing",
    "note",
    "room",
    "1",
    "room",
    "directly",
    "lead",
    "room",
    "number",
    "five",
    "right",
    "room",
    "number",
    "one",
    "four",
    "directly",
    "lead",
    "room",
    "number",
    "five",
    "basically",
    "goal",
    "get",
    "room",
    "number",
    "five",
    "okay",
    "set",
    "room",
    "goal",
    "associate",
    "reward",
    "value",
    "door",
    "okay",
    "worry",
    "explain",
    "saying",
    "present",
    "rooms",
    "graph",
    "graph",
    "going",
    "look",
    "okay",
    "example",
    "true",
    "go",
    "three",
    "three",
    "two",
    "one",
    "one",
    "two",
    "five",
    "lead",
    "us",
    "goal",
    "arrows",
    "represent",
    "link",
    "dose",
    "quite",
    "understandable",
    "next",
    "step",
    "associate",
    "reward",
    "value",
    "doors",
    "okay",
    "rooms",
    "directly",
    "connected",
    "end",
    "room",
    "room",
    "number",
    "five",
    "get",
    "reward",
    "hundred",
    "okay",
    "basically",
    "room",
    "number",
    "one",
    "reward",
    "five",
    "obviously",
    "directly",
    "connected",
    "5",
    "similarly",
    "also",
    "associated",
    "reward",
    "hundred",
    "directly",
    "connected",
    "okay",
    "go",
    "lead",
    "five",
    "know",
    "roads",
    "directly",
    "connected",
    "ca",
    "directly",
    "go",
    "0",
    "okay",
    "assigning",
    "reward",
    "zero",
    "basically",
    "doors",
    "directly",
    "connected",
    "target",
    "room",
    "zero",
    "reward",
    "okay",
    "doors",
    "weigh",
    "two",
    "arrows",
    "assigned",
    "room",
    "okay",
    "see",
    "two",
    "arrows",
    "assigned",
    "room",
    "basically",
    "zero",
    "leads",
    "four",
    "four",
    "leads",
    "back",
    "0",
    "assigned",
    "0",
    "0",
    "0",
    "directly",
    "lead",
    "five",
    "one",
    "directly",
    "leads",
    "five",
    "see",
    "hundred",
    "similarly",
    "directly",
    "leads",
    "goal",
    "state",
    "signed",
    "hundred",
    "obviously",
    "five",
    "two",
    "five",
    "hundred",
    "well",
    "direct",
    "connections",
    "room",
    "number",
    "five",
    "rewarded",
    "hundred",
    "indirect",
    "connections",
    "awarded",
    "zero",
    "guys",
    "end",
    "goal",
    "reach",
    "state",
    "highest",
    "reward",
    "agent",
    "arrives",
    "goal",
    "okay",
    "let",
    "explain",
    "graph",
    "detail",
    "rooms",
    "labeled",
    "one",
    "two",
    "three",
    "five",
    "represent",
    "state",
    "agent",
    "stay",
    "one",
    "means",
    "agent",
    "room",
    "number",
    "one",
    "similarly",
    "agents",
    "movement",
    "one",
    "room",
    "represents",
    "action",
    "okay",
    "say",
    "one",
    "two",
    "three",
    "represents",
    "action",
    "right",
    "basically",
    "state",
    "represented",
    "node",
    "action",
    "represented",
    "arrows",
    "okay",
    "graph",
    "nodes",
    "represent",
    "rooms",
    "arrows",
    "represent",
    "actions",
    "okay",
    "let",
    "look",
    "small",
    "example",
    "let",
    "set",
    "initial",
    "state",
    "agent",
    "placed",
    "room",
    "number",
    "two",
    "travel",
    "way",
    "room",
    "number",
    "five",
    "set",
    "initial",
    "stage",
    "travel",
    "state",
    "okay",
    "three",
    "either",
    "go",
    "one",
    "go",
    "back",
    "go",
    "chooses",
    "go",
    "directly",
    "take",
    "room",
    "number",
    "5",
    "okay",
    "end",
    "goal",
    "even",
    "goes",
    "room",
    "number",
    "3",
    "2",
    "1",
    "take",
    "room",
    "number",
    "high",
    "five",
    "algorithm",
    "works",
    "going",
    "drivers",
    "different",
    "rooms",
    "order",
    "reach",
    "gold",
    "room",
    "room",
    "number",
    "let",
    "try",
    "depict",
    "rewards",
    "form",
    "matrix",
    "okay",
    "using",
    "matrix",
    "reward",
    "matrix",
    "calculate",
    "q",
    "value",
    "q",
    "matrix",
    "okay",
    "see",
    "q",
    "value",
    "next",
    "step",
    "let",
    "see",
    "reward",
    "matrix",
    "calculated",
    "ones",
    "see",
    "table",
    "represent",
    "null",
    "values",
    "basically",
    "means",
    "wherever",
    "link",
    "nodes",
    "represented",
    "minus",
    "1",
    "0",
    "2",
    "0",
    "minus",
    "1",
    "0",
    "1",
    "link",
    "okay",
    "direct",
    "link",
    "0",
    "represented",
    "minus",
    "1",
    "similarly",
    "0",
    "2",
    "link",
    "see",
    "line",
    "also",
    "minus",
    "1",
    "comes",
    "0",
    "4",
    "connection",
    "numbered",
    "0",
    "reward",
    "state",
    "directly",
    "connected",
    "goal",
    "zero",
    "look",
    "1",
    "comma",
    "5",
    "basically",
    "traversing",
    "node",
    "1",
    "node",
    "5",
    "see",
    "reward",
    "hundred",
    "okay",
    "basically",
    "one",
    "five",
    "directly",
    "connected",
    "five",
    "end",
    "goal",
    "node",
    "directly",
    "connected",
    "goal",
    "state",
    "get",
    "reward",
    "hundred",
    "okay",
    "put",
    "hundred",
    "similarly",
    "look",
    "fourth",
    "row",
    "assigned",
    "hundred",
    "4",
    "5",
    "direct",
    "connection",
    "direct",
    "connection",
    "gives",
    "hundred",
    "reward",
    "okay",
    "see",
    "4",
    "direct",
    "link",
    "okay",
    "room",
    "number",
    "room",
    "number",
    "five",
    "go",
    "directly",
    "hundred",
    "reward",
    "guys",
    "reward",
    "matrix",
    "made",
    "alright",
    "hope",
    "clear",
    "okay",
    "reward",
    "matrix",
    "need",
    "create",
    "another",
    "matrix",
    "called",
    "q",
    "matrix",
    "ok",
    "store",
    "q",
    "values",
    "calculate",
    "q",
    "matrix",
    "basically",
    "represents",
    "memory",
    "agent",
    "learned",
    "experience",
    "okay",
    "traverses",
    "one",
    "room",
    "final",
    "room",
    "whatever",
    "learned",
    "stored",
    "q",
    "matrix",
    "okay",
    "order",
    "remember",
    "next",
    "time",
    "travels",
    "use",
    "matrix",
    "okay",
    "basically",
    "like",
    "memory",
    "guys",
    "rows",
    "q",
    "matrix",
    "represent",
    "current",
    "state",
    "agent",
    "columns",
    "represent",
    "possible",
    "actions",
    "calculate",
    "q",
    "value",
    "use",
    "formula",
    "right",
    "show",
    "q",
    "matrix",
    "looks",
    "like",
    "first",
    "let",
    "understand",
    "formula",
    "q",
    "value",
    "calculating",
    "want",
    "fill",
    "q",
    "matrix",
    "okay",
    "basically",
    "matrix",
    "initially",
    "0",
    "agent",
    "traverse",
    "different",
    "nodes",
    "destination",
    "node",
    "matrix",
    "get",
    "filled",
    "okay",
    "basically",
    "like",
    "memory",
    "agent",
    "know",
    "okay",
    "traversed",
    "using",
    "particular",
    "path",
    "found",
    "value",
    "maximum",
    "reward",
    "maximum",
    "year",
    "next",
    "time",
    "choose",
    "path",
    "exactly",
    "q",
    "matrix",
    "okay",
    "let",
    "go",
    "back",
    "guys",
    "worry",
    "formula",
    "implementing",
    "formula",
    "example",
    "next",
    "slide",
    "okay",
    "worry",
    "formula",
    "remember",
    "q",
    "basically",
    "represents",
    "q",
    "matrix",
    "r",
    "represents",
    "reward",
    "matrix",
    "gamma",
    "gamma",
    "value",
    "talk",
    "shortly",
    "finding",
    "maximum",
    "q",
    "matrix",
    "basically",
    "gamma",
    "parameter",
    "range",
    "0",
    "1",
    "value",
    "gamma",
    "closer",
    "zero",
    "means",
    "agent",
    "consider",
    "immediate",
    "rewards",
    "means",
    "agent",
    "explore",
    "surrounding",
    "basically",
    "wo",
    "explore",
    "different",
    "rooms",
    "choose",
    "particular",
    "room",
    "try",
    "sticking",
    "value",
    "gamma",
    "high",
    "meaning",
    "closer",
    "one",
    "agent",
    "consider",
    "future",
    "awards",
    "greater",
    "weight",
    "means",
    "agent",
    "explore",
    "possible",
    "approaches",
    "possible",
    "policies",
    "order",
    "get",
    "end",
    "goal",
    "guys",
    "talking",
    "mention",
    "ation",
    "exploration",
    "right",
    "gamma",
    "value",
    "closer",
    "1",
    "basically",
    "means",
    "actually",
    "exploring",
    "entire",
    "environment",
    "choosing",
    "optimum",
    "policy",
    "gamma",
    "value",
    "closer",
    "zero",
    "means",
    "agent",
    "stick",
    "certain",
    "set",
    "policies",
    "calculate",
    "maximum",
    "reward",
    "based",
    "policies",
    "next",
    "q",
    "learning",
    "algorithm",
    "going",
    "use",
    "solve",
    "problem",
    "guys",
    "going",
    "look",
    "confusing",
    "let",
    "explain",
    "example",
    "okay",
    "see",
    "actually",
    "going",
    "run",
    "demo",
    "math",
    "behind",
    "tell",
    "q",
    "learning",
    "algorithm",
    "okay",
    "understand",
    "showing",
    "example",
    "guys",
    "q",
    "learning",
    "algorithm",
    "agent",
    "learns",
    "experience",
    "okay",
    "episode",
    "basically",
    "agents",
    "traversing",
    "initial",
    "room",
    "end",
    "goal",
    "equivalent",
    "one",
    "training",
    "session",
    "every",
    "training",
    "session",
    "agent",
    "explore",
    "environment",
    "receive",
    "reward",
    "reaches",
    "goal",
    "state",
    "five",
    "purpose",
    "training",
    "enhance",
    "brain",
    "agent",
    "okay",
    "knows",
    "environment",
    "well",
    "know",
    "action",
    "take",
    "calculate",
    "q",
    "matrix",
    "okay",
    "q",
    "matrix",
    "going",
    "calculate",
    "value",
    "traversing",
    "every",
    "state",
    "end",
    "state",
    "every",
    "initial",
    "room",
    "end",
    "room",
    "okay",
    "calculate",
    "values",
    "much",
    "reward",
    "getting",
    "policy",
    "know",
    "optimum",
    "policy",
    "give",
    "us",
    "maximum",
    "reward",
    "okay",
    "q",
    "matrix",
    "important",
    "train",
    "agent",
    "optimum",
    "output",
    "basically",
    "agent",
    "perform",
    "exploitation",
    "instead",
    "explore",
    "around",
    "go",
    "back",
    "forth",
    "different",
    "rooms",
    "find",
    "fastest",
    "route",
    "goal",
    "right",
    "let",
    "look",
    "example",
    "okay",
    "let",
    "see",
    "algorithm",
    "works",
    "okay",
    "let",
    "go",
    "back",
    "previous",
    "slide",
    "says",
    "first",
    "step",
    "set",
    "gamma",
    "parameter",
    "okay",
    "let",
    "first",
    "step",
    "set",
    "value",
    "learning",
    "parameter",
    "gamma",
    "randomly",
    "set",
    "zero",
    "point",
    "eight",
    "okay",
    "next",
    "step",
    "initialize",
    "matrix",
    "q",
    "2",
    "0",
    "okay",
    "set",
    "matrix",
    "q",
    "2",
    "0",
    "select",
    "initial",
    "stage",
    "okay",
    "third",
    "step",
    "select",
    "random",
    "initial",
    "state",
    "selected",
    "initial",
    "state",
    "room",
    "number",
    "one",
    "okay",
    "initialize",
    "matter",
    "q",
    "zero",
    "matrix",
    "room",
    "number",
    "one",
    "either",
    "go",
    "room",
    "number",
    "three",
    "number",
    "five",
    "look",
    "reward",
    "matrix",
    "see",
    "room",
    "number",
    "one",
    "go",
    "room",
    "number",
    "three",
    "room",
    "number",
    "five",
    "values",
    "minus",
    "1",
    "means",
    "link",
    "1",
    "0",
    "1",
    "2",
    "1",
    "1",
    "2",
    "2",
    "1",
    "possible",
    "actions",
    "room",
    "number",
    "one",
    "go",
    "room",
    "number",
    "3",
    "go",
    "room",
    "number",
    "five",
    "right",
    "okay",
    "let",
    "select",
    "room",
    "number",
    "five",
    "okay",
    "room",
    "number",
    "one",
    "go",
    "3",
    "5",
    "randomly",
    "selected",
    "five",
    "also",
    "select",
    "three",
    "example",
    "let",
    "select",
    "five",
    "rome",
    "five",
    "going",
    "calculate",
    "maximum",
    "q",
    "value",
    "next",
    "state",
    "based",
    "possible",
    "actions",
    "number",
    "five",
    "next",
    "state",
    "room",
    "number",
    "one",
    "four",
    "five",
    "going",
    "calculate",
    "q",
    "value",
    "traversing",
    "5",
    "1",
    "5",
    "2",
    "4",
    "5",
    "2",
    "5",
    "going",
    "find",
    "maximum",
    "q",
    "value",
    "going",
    "compute",
    "q",
    "value",
    "let",
    "implement",
    "formula",
    "okay",
    "formula",
    "right",
    "traversing",
    "room",
    "number",
    "one",
    "room",
    "number",
    "okay",
    "state",
    "written",
    "q",
    "1",
    "comma",
    "okay",
    "one",
    "represents",
    "current",
    "state",
    "room",
    "number",
    "one",
    "okay",
    "initial",
    "state",
    "room",
    "number",
    "one",
    "traversing",
    "room",
    "number",
    "five",
    "okay",
    "shown",
    "figure",
    "room",
    "number",
    "5",
    "need",
    "calculate",
    "q",
    "value",
    "next",
    "formula",
    "says",
    "reward",
    "matrix",
    "state",
    "action",
    "reward",
    "matrix",
    "1",
    "comma",
    "5",
    "let",
    "look",
    "1",
    "comma",
    "5",
    "1",
    "comma",
    "5",
    "corresponds",
    "hundred",
    "okay",
    "reward",
    "hundred",
    "r",
    "1",
    "comma",
    "5",
    "basically",
    "hundred",
    "going",
    "add",
    "gamma",
    "value",
    "gamma",
    "value",
    "initialized",
    "zero",
    "point",
    "eight",
    "written",
    "going",
    "multiply",
    "maximum",
    "value",
    "going",
    "get",
    "next",
    "date",
    "based",
    "possible",
    "actions",
    "okay",
    "5",
    "next",
    "state",
    "1",
    "4",
    "travis",
    "five",
    "one",
    "written",
    "5",
    "going",
    "calculate",
    "q",
    "value",
    "fire",
    "2",
    "4",
    "5",
    "okay",
    "mentioned",
    "q",
    "5",
    "comma",
    "1",
    "5",
    "comma",
    "4",
    "5",
    "comma",
    "5",
    "next",
    "possible",
    "actions",
    "take",
    "state",
    "r",
    "1",
    "comma",
    "5",
    "hundred",
    "okay",
    "reward",
    "matrix",
    "see",
    "1",
    "comma",
    "5",
    "hundred",
    "value",
    "gamma",
    "calculate",
    "q",
    "5",
    "comma",
    "1",
    "5",
    "comma",
    "4",
    "5",
    "comma",
    "5",
    "like",
    "mentioned",
    "earlier",
    "going",
    "initialize",
    "matrix",
    "q",
    "zero",
    "matrix",
    "based",
    "setting",
    "value",
    "0",
    "initially",
    "obviously",
    "agent",
    "memory",
    "happening",
    "okay",
    "starting",
    "scratch",
    "values",
    "0",
    "q",
    "5",
    "comma",
    "1",
    "obviously",
    "0",
    "5",
    "comma",
    "4",
    "would",
    "0",
    "5",
    "comma",
    "5",
    "also",
    "zero",
    "find",
    "maximum",
    "obviously",
    "compute",
    "equation",
    "get",
    "hundred",
    "q",
    "value",
    "1",
    "comma",
    "5",
    "agent",
    "goes",
    "room",
    "number",
    "one",
    "room",
    "number",
    "five",
    "going",
    "maximum",
    "reward",
    "q",
    "value",
    "hundred",
    "right",
    "next",
    "slide",
    "see",
    "updated",
    "value",
    "q",
    "1",
    "comma",
    "okay",
    "said",
    "right",
    "similarly",
    "let",
    "look",
    "another",
    "example",
    "understand",
    "better",
    "guys",
    "exactly",
    "going",
    "demo",
    "going",
    "coded",
    "okay",
    "explaining",
    "code",
    "right",
    "telling",
    "math",
    "behind",
    "alright",
    "let",
    "look",
    "another",
    "example",
    "example",
    "ok",
    "time",
    "start",
    "randomly",
    "chosen",
    "initial",
    "state",
    "let",
    "say",
    "chosen",
    "state",
    "okay",
    "room",
    "3",
    "either",
    "go",
    "room",
    "number",
    "one",
    "two",
    "four",
    "randomly",
    "select",
    "room",
    "number",
    "one",
    "room",
    "number",
    "one",
    "going",
    "calculate",
    "maximum",
    "q",
    "value",
    "next",
    "state",
    "based",
    "possible",
    "actions",
    "possible",
    "actions",
    "one",
    "go",
    "3",
    "go",
    "5",
    "calculate",
    "q",
    "value",
    "using",
    "formula",
    "let",
    "explain",
    "3",
    "comma",
    "1",
    "basically",
    "represents",
    "room",
    "number",
    "three",
    "going",
    "room",
    "number",
    "one",
    "okay",
    "represents",
    "action",
    "okay",
    "going",
    "3",
    "1",
    "action",
    "three",
    "current",
    "state",
    "next",
    "look",
    "reward",
    "going",
    "3",
    "okay",
    "go",
    "reward",
    "matrix",
    "3",
    "comma",
    "1",
    "0",
    "okay",
    "direct",
    "link",
    "three",
    "five",
    "okay",
    "reward",
    "zero",
    "value",
    "0",
    "gamma",
    "value",
    "zero",
    "point",
    "eight",
    "going",
    "calculate",
    "q",
    "max",
    "1",
    "comma",
    "3",
    "1",
    "comma",
    "5",
    "whichever",
    "maximum",
    "value",
    "going",
    "use",
    "okay",
    "q",
    "1",
    "comma",
    "3",
    "right",
    "0",
    "see",
    "1",
    "comma",
    "3",
    "0",
    "1",
    "comma",
    "5",
    "remember",
    "calculated",
    "1",
    "comma",
    "5",
    "previous",
    "slide",
    "okay",
    "1",
    "comma",
    "5",
    "hundred",
    "going",
    "put",
    "hundred",
    "maximum",
    "hundred",
    "200",
    "give",
    "us",
    "c",
    "q",
    "value",
    "going",
    "get",
    "traverse",
    "three",
    "two",
    "one",
    "okay",
    "hope",
    "clear",
    "travers",
    "room",
    "number",
    "three",
    "room",
    "number",
    "one",
    "reward",
    "okay",
    "still",
    "reached",
    "end",
    "goal",
    "room",
    "number",
    "five",
    "next",
    "episode",
    "state",
    "room",
    "number",
    "one",
    "guys",
    "like",
    "said",
    "repeat",
    "loop",
    "room",
    "number",
    "one",
    "end",
    "goal",
    "okay",
    "end",
    "goal",
    "room",
    "number",
    "need",
    "figure",
    "get",
    "room",
    "number",
    "one",
    "room",
    "number",
    "room",
    "number",
    "one",
    "either",
    "either",
    "go",
    "three",
    "five",
    "drawn",
    "select",
    "five",
    "know",
    "end",
    "goal",
    "okay",
    "room",
    "number",
    "5",
    "calculate",
    "maximum",
    "q",
    "value",
    "next",
    "possible",
    "actions",
    "next",
    "possible",
    "actions",
    "five",
    "go",
    "room",
    "number",
    "one",
    "room",
    "number",
    "four",
    "room",
    "number",
    "five",
    "going",
    "calculate",
    "q",
    "value",
    "5",
    "1",
    "5",
    "2",
    "4",
    "5",
    "2",
    "5",
    "find",
    "maximum",
    "q",
    "value",
    "going",
    "use",
    "value",
    "right",
    "let",
    "look",
    "formula",
    "room",
    "number",
    "one",
    "want",
    "go",
    "room",
    "number",
    "okay",
    "exactly",
    "written",
    "q",
    "1",
    "comma",
    "5",
    "next",
    "reward",
    "matrix",
    "reward",
    "1",
    "comma",
    "5",
    "hundred",
    "right",
    "added",
    "gamma",
    "value",
    "going",
    "find",
    "maximum",
    "q",
    "value",
    "5",
    "1",
    "5",
    "2",
    "4",
    "5",
    "performing",
    "5",
    "comma",
    "1",
    "5",
    "comma",
    "4",
    "5",
    "comma",
    "5",
    "0",
    "initially",
    "set",
    "values",
    "q",
    "matrix",
    "0",
    "get",
    "hundred",
    "matrix",
    "remains",
    "already",
    "calculated",
    "q",
    "1",
    "comma",
    "5",
    "value",
    "1",
    "comma",
    "5",
    "already",
    "fed",
    "agent",
    "comes",
    "back",
    "knows",
    "okay",
    "already",
    "done",
    "going",
    "try",
    "implement",
    "another",
    "method",
    "okay",
    "going",
    "try",
    "take",
    "another",
    "route",
    "another",
    "policy",
    "going",
    "try",
    "go",
    "different",
    "rooms",
    "finally",
    "land",
    "room",
    "number",
    "5",
    "guys",
    "exactly",
    "code",
    "runs",
    "going",
    "traverse",
    "every",
    "node",
    "want",
    "optimum",
    "ball",
    "see",
    "okay",
    "optimum",
    "policy",
    "attained",
    "traverse",
    "possible",
    "actions",
    "okay",
    "go",
    "possible",
    "actions",
    "perform",
    "understand",
    "best",
    "action",
    "lead",
    "us",
    "reward",
    "hope",
    "clear",
    "let",
    "move",
    "look",
    "code",
    "guys",
    "code",
    "executed",
    "python",
    "assuming",
    "good",
    "background",
    "python",
    "okay",
    "understand",
    "python",
    "well",
    "going",
    "leave",
    "link",
    "description",
    "check",
    "video",
    "python",
    "maybe",
    "come",
    "back",
    "later",
    "okay",
    "explaining",
    "code",
    "anyway",
    "going",
    "spend",
    "lot",
    "time",
    "explaining",
    "every",
    "line",
    "code",
    "assuming",
    "know",
    "python",
    "okay",
    "let",
    "look",
    "first",
    "line",
    "code",
    "going",
    "going",
    "import",
    "numpy",
    "okay",
    "numpy",
    "basically",
    "python",
    "library",
    "adding",
    "support",
    "large",
    "arrays",
    "matrices",
    "basically",
    "computing",
    "mathematical",
    "functions",
    "okay",
    "first",
    "want",
    "import",
    "going",
    "create",
    "matrix",
    "okay",
    "matrix",
    "next",
    "going",
    "create",
    "q",
    "matrix",
    "6",
    "6",
    "matrix",
    "obviously",
    "six",
    "states",
    "starting",
    "0",
    "okay",
    "going",
    "initialize",
    "value",
    "zero",
    "basically",
    "q",
    "matrix",
    "going",
    "initialized",
    "zero",
    "right",
    "setting",
    "gamma",
    "parameter",
    "guys",
    "play",
    "parameter",
    "know",
    "move",
    "movement",
    "logo",
    "okay",
    "see",
    "see",
    "happens",
    "set",
    "initial",
    "stage",
    "okay",
    "initial",
    "stage",
    "set",
    "1",
    "defining",
    "function",
    "called",
    "available",
    "actions",
    "okay",
    "basically",
    "since",
    "initial",
    "state",
    "one",
    "going",
    "check",
    "row",
    "number",
    "one",
    "okay",
    "number",
    "one",
    "okay",
    "wrong",
    "number",
    "zero",
    "zero",
    "number",
    "one",
    "going",
    "check",
    "row",
    "number",
    "one",
    "going",
    "find",
    "values",
    "greater",
    "equal",
    "0",
    "values",
    "basically",
    "nodes",
    "travel",
    "select",
    "minus",
    "1",
    "traverse",
    "okay",
    "explained",
    "earlier",
    "one",
    "represents",
    "nodes",
    "travel",
    "travel",
    "nodes",
    "okay",
    "basically",
    "checking",
    "values",
    "equal",
    "0",
    "greater",
    "0",
    "available",
    "actions",
    "initial",
    "state",
    "one",
    "travel",
    "states",
    "whose",
    "value",
    "equal",
    "0",
    "greater",
    "0",
    "stored",
    "variable",
    "called",
    "available",
    "act",
    "right",
    "basically",
    "get",
    "available",
    "actions",
    "current",
    "state",
    "okay",
    "storing",
    "possible",
    "actions",
    "available",
    "act",
    "variable",
    "basically",
    "since",
    "initial",
    "state",
    "one",
    "going",
    "find",
    "next",
    "possible",
    "states",
    "go",
    "okay",
    "stored",
    "available",
    "act",
    "variable",
    "next",
    "function",
    "chooses",
    "random",
    "action",
    "performed",
    "within",
    "range",
    "remember",
    "guys",
    "initially",
    "stage",
    "number",
    "okay",
    "available",
    "actions",
    "go",
    "stage",
    "number",
    "3",
    "stage",
    "number",
    "five",
    "sorry",
    "room",
    "number",
    "3",
    "room",
    "number",
    "okay",
    "randomly",
    "need",
    "choose",
    "one",
    "room",
    "using",
    "line",
    "code",
    "okay",
    "randomly",
    "going",
    "choose",
    "one",
    "actions",
    "available",
    "act",
    "available",
    "act",
    "like",
    "said",
    "earlier",
    "stores",
    "possible",
    "actions",
    "okay",
    "initial",
    "state",
    "okay",
    "chooses",
    "action",
    "going",
    "store",
    "next",
    "action",
    "guys",
    "action",
    "present",
    "next",
    "available",
    "action",
    "take",
    "next",
    "q",
    "matrix",
    "remember",
    "formula",
    "used",
    "guys",
    "formula",
    "use",
    "going",
    "calculate",
    "next",
    "lines",
    "code",
    "block",
    "code",
    "executing",
    "computing",
    "value",
    "okay",
    "formula",
    "computing",
    "value",
    "q",
    "current",
    "state",
    "karma",
    "action",
    "current",
    "state",
    "karma",
    "action",
    "gamma",
    "maximum",
    "value",
    "basically",
    "going",
    "calculate",
    "maximum",
    "index",
    "meaning",
    "going",
    "check",
    "possible",
    "actions",
    "give",
    "us",
    "maximum",
    "q",
    "value",
    "read",
    "remember",
    "explanation",
    "value",
    "max",
    "q",
    "five",
    "comma",
    "1",
    "5",
    "comma",
    "4",
    "5",
    "comma",
    "5",
    "choose",
    "maximum",
    "q",
    "value",
    "get",
    "three",
    "basically",
    "exactly",
    "line",
    "code",
    "calculating",
    "index",
    "gives",
    "us",
    "maximum",
    "value",
    "finish",
    "computing",
    "value",
    "q",
    "update",
    "matrix",
    "updating",
    "q",
    "value",
    "choosing",
    "new",
    "initial",
    "state",
    "okay",
    "update",
    "function",
    "defined",
    "okay",
    "called",
    "function",
    "guys",
    "whole",
    "set",
    "code",
    "calculate",
    "q",
    "value",
    "okay",
    "exactly",
    "examples",
    "training",
    "phase",
    "guys",
    "remember",
    "train",
    "algorithm",
    "better",
    "going",
    "learn",
    "okay",
    "provided",
    "around",
    "titrations",
    "okay",
    "range",
    "10",
    "thousand",
    "iterations",
    "meaning",
    "age",
    "take",
    "possible",
    "scenarios",
    "go",
    "titrations",
    "find",
    "best",
    "policy",
    "exactly",
    "choosing",
    "current",
    "state",
    "randomly",
    "choosing",
    "available",
    "action",
    "current",
    "state",
    "either",
    "go",
    "stage",
    "3",
    "straight",
    "five",
    "calculating",
    "next",
    "action",
    "finally",
    "updating",
    "value",
    "q",
    "matrix",
    "next",
    "normalize",
    "q",
    "matrix",
    "sometimes",
    "q",
    "matrix",
    "value",
    "might",
    "exceed",
    "okay",
    "let",
    "say",
    "heated",
    "500",
    "600",
    "time",
    "want",
    "normalize",
    "matrix",
    "okay",
    "want",
    "bring",
    "little",
    "bit",
    "okay",
    "larger",
    "numbers",
    "wo",
    "able",
    "understand",
    "computation",
    "would",
    "hard",
    "larger",
    "numbers",
    "perform",
    "normalization",
    "taking",
    "calculated",
    "value",
    "dividing",
    "maximum",
    "q",
    "value",
    "right",
    "normalizing",
    "guys",
    "testing",
    "phase",
    "okay",
    "randomly",
    "set",
    "current",
    "state",
    "want",
    "given",
    "data",
    "already",
    "trained",
    "model",
    "okay",
    "give",
    "garden",
    "state",
    "going",
    "tell",
    "agent",
    "listen",
    "room",
    "number",
    "one",
    "need",
    "go",
    "room",
    "number",
    "five",
    "okay",
    "figure",
    "go",
    "room",
    "number",
    "5",
    "trained",
    "right",
    "set",
    "current",
    "state",
    "one",
    "need",
    "make",
    "sure",
    "equal",
    "5",
    "5",
    "end",
    "goal",
    "guys",
    "loop",
    "executed",
    "earlier",
    "going",
    "trations",
    "run",
    "entire",
    "code",
    "let",
    "look",
    "result",
    "current",
    "state",
    "chosen",
    "one",
    "okay",
    "go",
    "back",
    "matrix",
    "see",
    "direct",
    "link",
    "1",
    "5",
    "means",
    "route",
    "agent",
    "take",
    "one",
    "five",
    "okay",
    "directly",
    "go",
    "1",
    "5",
    "get",
    "maximum",
    "reward",
    "like",
    "okay",
    "let",
    "see",
    "happening",
    "run",
    "give",
    "direct",
    "path",
    "1",
    "okay",
    "exactly",
    "happened",
    "selected",
    "path",
    "directly",
    "one",
    "five",
    "went",
    "calculated",
    "entire",
    "q",
    "matrix",
    "works",
    "guys",
    "exactly",
    "works",
    "let",
    "try",
    "set",
    "initial",
    "stage",
    "set",
    "initial",
    "stage",
    "try",
    "run",
    "code",
    "let",
    "see",
    "path",
    "gives",
    "selected",
    "path",
    "2",
    "3",
    "4",
    "5",
    "chose",
    "path",
    "giving",
    "us",
    "maximum",
    "reward",
    "path",
    "okay",
    "q",
    "matrix",
    "calculated",
    "selected",
    "path",
    "right",
    "guys",
    "come",
    "end",
    "demo",
    "basically",
    "placed",
    "agent",
    "room",
    "random",
    "room",
    "ask",
    "traverse",
    "reach",
    "end",
    "room",
    "room",
    "number",
    "five",
    "basically",
    "trained",
    "agent",
    "made",
    "sure",
    "went",
    "possible",
    "paths",
    "calculate",
    "best",
    "path",
    "robot",
    "environment",
    "place",
    "put",
    "use",
    "remember",
    "reward",
    "agent",
    "example",
    "automobile",
    "factory",
    "robot",
    "used",
    "move",
    "materials",
    "one",
    "place",
    "another",
    "task",
    "discussed",
    "property",
    "common",
    "tasks",
    "involve",
    "environment",
    "expect",
    "agent",
    "learn",
    "environment",
    "traditional",
    "machine",
    "learning",
    "phase",
    "hence",
    "need",
    "reinforcement",
    "learning",
    "good",
    "establish",
    "overview",
    "problem",
    "solved",
    "using",
    "q",
    "learning",
    "reinforcement",
    "learning",
    "helps",
    "define",
    "main",
    "components",
    "reinforcement",
    "learning",
    "solution",
    "agent",
    "environment",
    "action",
    "rewards",
    "states",
    "let",
    "suppose",
    "build",
    "autonomous",
    "robots",
    "automobile",
    "building",
    "factory",
    "robots",
    "help",
    "factory",
    "personal",
    "conveying",
    "necessary",
    "parts",
    "would",
    "need",
    "order",
    "pull",
    "car",
    "different",
    "parts",
    "located",
    "nine",
    "different",
    "positions",
    "within",
    "factory",
    "warehouse",
    "car",
    "part",
    "include",
    "chassis",
    "wheels",
    "dashboard",
    "engine",
    "factory",
    "workers",
    "prioritized",
    "location",
    "contains",
    "body",
    "chassis",
    "topmost",
    "provided",
    "priorities",
    "locations",
    "well",
    "look",
    "moment",
    "locations",
    "within",
    "factory",
    "look",
    "somewhat",
    "like",
    "see",
    "l1",
    "l2",
    "l3",
    "stations",
    "one",
    "thing",
    "might",
    "notice",
    "little",
    "obstacle",
    "prison",
    "locations",
    "l6",
    "top",
    "priority",
    "location",
    "contains",
    "chassis",
    "preparing",
    "car",
    "bodies",
    "task",
    "enable",
    "robots",
    "find",
    "shortest",
    "route",
    "given",
    "location",
    "another",
    "location",
    "agents",
    "case",
    "robots",
    "environment",
    "automobile",
    "factory",
    "warehouse",
    "let",
    "talk",
    "state",
    "states",
    "location",
    "particular",
    "robot",
    "present",
    "particular",
    "instance",
    "time",
    "denote",
    "states",
    "machines",
    "understand",
    "numbers",
    "rather",
    "let",
    "us",
    "let",
    "map",
    "location",
    "codes",
    "number",
    "see",
    "map",
    "location",
    "l",
    "1",
    "0",
    "l",
    "2",
    "1",
    "l8",
    "state",
    "7",
    "l",
    "line",
    "state",
    "next",
    "going",
    "talk",
    "actions",
    "example",
    "action",
    "direct",
    "location",
    "robot",
    "call",
    "particular",
    "location",
    "right",
    "consider",
    "robot",
    "tel",
    "location",
    "direct",
    "locations",
    "move",
    "l5",
    "l1",
    "l3",
    "figure",
    "may",
    "come",
    "handy",
    "visualize",
    "might",
    "already",
    "guessed",
    "set",
    "actions",
    "nothing",
    "set",
    "possible",
    "states",
    "robot",
    "location",
    "set",
    "actions",
    "robot",
    "take",
    "different",
    "example",
    "set",
    "actions",
    "change",
    "robot",
    "l1",
    "rather",
    "l2",
    "robot",
    "l1",
    "go",
    "l",
    "4",
    "l",
    "2",
    "directly",
    "done",
    "states",
    "actions",
    "let",
    "talk",
    "rewards",
    "states",
    "basically",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "actions",
    "also",
    "0",
    "1",
    "2",
    "3",
    "4",
    "till",
    "rewards",
    "given",
    "robot",
    "location",
    "state",
    "directly",
    "reachable",
    "particular",
    "location",
    "let",
    "take",
    "example",
    "suppose",
    "l",
    "lane",
    "directly",
    "reachable",
    "l8",
    "right",
    "robot",
    "goes",
    "la",
    "align",
    "vice",
    "versa",
    "rewarded",
    "one",
    "location",
    "directly",
    "reachable",
    "particular",
    "equation",
    "give",
    "reward",
    "reward",
    "0",
    "reward",
    "number",
    "nothing",
    "else",
    "enables",
    "robots",
    "make",
    "sense",
    "movements",
    "helping",
    "deciding",
    "locations",
    "directly",
    "reachable",
    "construct",
    "reward",
    "table",
    "contains",
    "required",
    "use",
    "mapping",
    "possible",
    "states",
    "see",
    "table",
    "positions",
    "marked",
    "green",
    "positive",
    "reward",
    "see",
    "possible",
    "rewards",
    "robot",
    "get",
    "moving",
    "different",
    "states",
    "comes",
    "interesting",
    "decision",
    "remember",
    "factory",
    "administrator",
    "prioritized",
    "l6",
    "topmost",
    "incorporate",
    "fact",
    "table",
    "done",
    "associating",
    "topmost",
    "priority",
    "location",
    "high",
    "reward",
    "usual",
    "ones",
    "let",
    "put",
    "999",
    "cell",
    "l",
    "6",
    "comma",
    "six",
    "table",
    "rewards",
    "higher",
    "reward",
    "topmost",
    "location",
    "looks",
    "something",
    "like",
    "formally",
    "defined",
    "vital",
    "components",
    "solution",
    "aiming",
    "problem",
    "discussed",
    "shift",
    "gears",
    "bit",
    "study",
    "fundamental",
    "concepts",
    "prevail",
    "world",
    "reinforcement",
    "learning",
    "first",
    "start",
    "bellman",
    "equation",
    "consider",
    "following",
    "square",
    "rooms",
    "analogous",
    "actual",
    "environment",
    "original",
    "problem",
    "without",
    "barriers",
    "suppose",
    "robot",
    "needs",
    "go",
    "room",
    "marked",
    "green",
    "current",
    "position",
    "using",
    "specified",
    "direction",
    "enable",
    "robot",
    "programmatically",
    "one",
    "idea",
    "would",
    "introduced",
    "kind",
    "footprint",
    "robot",
    "able",
    "follow",
    "constant",
    "value",
    "specified",
    "rooms",
    "come",
    "along",
    "robots",
    "way",
    "follows",
    "directions",
    "fight",
    "way",
    "starts",
    "location",
    "able",
    "scan",
    "constant",
    "value",
    "move",
    "accordingly",
    "work",
    "direction",
    "prefix",
    "robot",
    "always",
    "starts",
    "location",
    "consider",
    "robot",
    "starts",
    "location",
    "rather",
    "previous",
    "one",
    "robot",
    "sees",
    "footprints",
    "two",
    "different",
    "directions",
    "therefore",
    "unable",
    "decide",
    "way",
    "go",
    "order",
    "get",
    "destination",
    "green",
    "room",
    "happens",
    "primarily",
    "robot",
    "way",
    "remember",
    "directions",
    "proceed",
    "job",
    "enable",
    "robot",
    "memory",
    "bellman",
    "equation",
    "comes",
    "play",
    "see",
    "main",
    "reason",
    "bellman",
    "equation",
    "enable",
    "reward",
    "memory",
    "thing",
    "going",
    "use",
    "equation",
    "goes",
    "something",
    "like",
    "v",
    "gives",
    "maximum",
    "r",
    "comma",
    "plus",
    "gamma",
    "vs",
    "particular",
    "state",
    "room",
    "action",
    "moving",
    "rooms",
    "state",
    "robot",
    "goes",
    "gamma",
    "discount",
    "factor",
    "get",
    "moment",
    "obviously",
    "r",
    "comma",
    "reward",
    "function",
    "takes",
    "state",
    "action",
    "outputs",
    "reward",
    "v",
    "value",
    "particular",
    "state",
    "footprint",
    "consider",
    "possible",
    "actions",
    "take",
    "one",
    "yields",
    "maximum",
    "value",
    "one",
    "constraint",
    "however",
    "regarding",
    "value",
    "footprint",
    "room",
    "marked",
    "yellow",
    "green",
    "room",
    "always",
    "value",
    "1",
    "denote",
    "one",
    "nearest",
    "room",
    "adjacent",
    "green",
    "room",
    "also",
    "ensure",
    "robot",
    "gets",
    "reward",
    "goes",
    "yellow",
    "room",
    "green",
    "room",
    "let",
    "see",
    "make",
    "sense",
    "equation",
    "let",
    "assume",
    "discount",
    "factor",
    "remember",
    "gamma",
    "discount",
    "value",
    "discount",
    "factor",
    "let",
    "take",
    "room",
    "mark",
    "one",
    "yellow",
    "room",
    "aztec",
    "mark",
    "room",
    "v",
    "value",
    "particular",
    "state",
    "v",
    "would",
    "something",
    "like",
    "maximum",
    "take",
    "0",
    "initial",
    "comma",
    "hey",
    "plus",
    "gamma",
    "1",
    "gives",
    "us",
    "zero",
    "point",
    "nine",
    "robot",
    "get",
    "reward",
    "owing",
    "state",
    "marked",
    "yellow",
    "hence",
    "ir",
    "comma",
    "0",
    "robot",
    "knows",
    "value",
    "yellow",
    "room",
    "hence",
    "v",
    "dash",
    "one",
    "following",
    "states",
    "get",
    "put",
    "equation",
    "get",
    "zero",
    "point",
    "seven",
    "nine",
    "reached",
    "starting",
    "point",
    "table",
    "looks",
    "value",
    "footprints",
    "computer",
    "bellman",
    "equation",
    "couple",
    "things",
    "notice",
    "max",
    "function",
    "robot",
    "always",
    "choose",
    "state",
    "gives",
    "maximum",
    "value",
    "state",
    "discount",
    "factor",
    "gamma",
    "notifies",
    "robot",
    "far",
    "destination",
    "typically",
    "specified",
    "developer",
    "algorithm",
    "would",
    "installed",
    "robot",
    "states",
    "also",
    "given",
    "respective",
    "values",
    "similar",
    "way",
    "see",
    "boxes",
    "green",
    "one",
    "one",
    "move",
    "away",
    "one",
    "get",
    "1",
    "0",
    "1",
    "7",
    "finally",
    "reach",
    "robot",
    "precede",
    "way",
    "green",
    "room",
    "utilizing",
    "value",
    "footprints",
    "event",
    "dropped",
    "arbitrary",
    "room",
    "given",
    "location",
    "robot",
    "lance",
    "highlighted",
    "sky",
    "blue",
    "area",
    "still",
    "find",
    "two",
    "options",
    "choose",
    "eventually",
    "either",
    "parties",
    "good",
    "enough",
    "robot",
    "take",
    "auto",
    "v",
    "value",
    "footprints",
    "one",
    "thing",
    "note",
    "bellman",
    "equation",
    "one",
    "key",
    "equations",
    "world",
    "reinforcement",
    "learning",
    "q",
    "learning",
    "think",
    "realistically",
    "surroundings",
    "always",
    "work",
    "way",
    "expect",
    "always",
    "bit",
    "stochastic",
    "city",
    "involved",
    "applies",
    "robot",
    "well",
    "sometimes",
    "might",
    "happen",
    "robots",
    "machinery",
    "got",
    "corrupted",
    "sometimes",
    "robot",
    "makes",
    "come",
    "across",
    "hindrance",
    "way",
    "may",
    "known",
    "beforehand",
    "right",
    "sometimes",
    "even",
    "robot",
    "knows",
    "needs",
    "take",
    "right",
    "turn",
    "introduce",
    "cast",
    "city",
    "case",
    "comes",
    "markov",
    "decision",
    "process",
    "consider",
    "robot",
    "currently",
    "red",
    "room",
    "needs",
    "go",
    "green",
    "room",
    "let",
    "consider",
    "robot",
    "slight",
    "chance",
    "dysfunctioning",
    "might",
    "take",
    "left",
    "right",
    "bottom",
    "instead",
    "updating",
    "upper",
    "turn",
    "order",
    "get",
    "green",
    "room",
    "red",
    "room",
    "question",
    "enable",
    "robot",
    "handle",
    "given",
    "environment",
    "right",
    "situation",
    "decision",
    "making",
    "regarding",
    "turn",
    "taken",
    "partly",
    "random",
    "partly",
    "another",
    "control",
    "robot",
    "partly",
    "random",
    "sure",
    "exactly",
    "robot",
    "mind",
    "dysfunctional",
    "partly",
    "control",
    "robot",
    "still",
    "making",
    "decision",
    "taking",
    "turn",
    "right",
    "help",
    "program",
    "embedded",
    "markov",
    "decision",
    "process",
    "discrete",
    "time",
    "stochastic",
    "control",
    "process",
    "provides",
    "mathematical",
    "framework",
    "modeling",
    "situations",
    "outcomes",
    "partly",
    "random",
    "partly",
    "control",
    "decision",
    "maker",
    "need",
    "give",
    "concept",
    "mathematical",
    "shape",
    "likely",
    "equation",
    "taken",
    "might",
    "price",
    "help",
    "bellman",
    "equation",
    "minor",
    "tweaks",
    "look",
    "original",
    "bellman",
    "equation",
    "v",
    "x",
    "equal",
    "maximum",
    "comma",
    "plus",
    "gamma",
    "v",
    "stash",
    "needs",
    "changed",
    "equation",
    "introduce",
    "amount",
    "randomness",
    "long",
    "sure",
    "robot",
    "might",
    "take",
    "expected",
    "turn",
    "also",
    "sure",
    "room",
    "might",
    "end",
    "nothing",
    "room",
    "moves",
    "current",
    "room",
    "point",
    "according",
    "equation",
    "sure",
    "stash",
    "next",
    "state",
    "room",
    "know",
    "probable",
    "turns",
    "reward",
    "might",
    "take",
    "order",
    "incorporate",
    "probabilities",
    "equation",
    "need",
    "associate",
    "probability",
    "turns",
    "quantify",
    "robot",
    "got",
    "experts",
    "chance",
    "taking",
    "turn",
    "get",
    "ps",
    "equal",
    "maximum",
    "comma",
    "plus",
    "gamma",
    "summation",
    "ps",
    "comma",
    "comma",
    "stash",
    "v",
    "stash",
    "ps",
    "stash",
    "probability",
    "moving",
    "room",
    "establish",
    "action",
    "submission",
    "expectation",
    "situation",
    "robot",
    "curse",
    "randomness",
    "let",
    "take",
    "look",
    "example",
    "associate",
    "probabilities",
    "stones",
    "essentially",
    "mean",
    "80",
    "chance",
    "robot",
    "take",
    "upper",
    "turn",
    "put",
    "required",
    "values",
    "equation",
    "get",
    "v",
    "equal",
    "maximum",
    "comma",
    "comma",
    "v",
    "room",
    "plus",
    "v",
    "room",
    "room",
    "v",
    "left",
    "plus",
    "vo",
    "right",
    "note",
    "value",
    "footprints",
    "change",
    "due",
    "fact",
    "incorporating",
    "stochastic",
    "ali",
    "time",
    "calculate",
    "values",
    "footprints",
    "instead",
    "let",
    "robot",
    "figure",
    "point",
    "considered",
    "rewarding",
    "robot",
    "action",
    "going",
    "particular",
    "room",
    "watering",
    "robot",
    "gets",
    "destination",
    "ideally",
    "reward",
    "action",
    "robot",
    "takes",
    "help",
    "better",
    "assess",
    "quality",
    "actions",
    "need",
    "always",
    "much",
    "better",
    "amount",
    "reward",
    "actions",
    "rewards",
    "right",
    "idea",
    "known",
    "living",
    "penalty",
    "reality",
    "reward",
    "system",
    "complex",
    "particularly",
    "modeling",
    "sparse",
    "rewards",
    "active",
    "area",
    "research",
    "domain",
    "reinforcement",
    "learning",
    "got",
    "equation",
    "transition",
    "q",
    "learning",
    "equation",
    "gives",
    "us",
    "value",
    "going",
    "particular",
    "state",
    "taking",
    "stochastic",
    "city",
    "environment",
    "account",
    "also",
    "learned",
    "briefly",
    "idea",
    "living",
    "penalty",
    "deals",
    "associating",
    "move",
    "robot",
    "reward",
    "q",
    "learning",
    "processes",
    "idea",
    "assessing",
    "quality",
    "action",
    "taken",
    "move",
    "state",
    "rather",
    "determining",
    "possible",
    "value",
    "state",
    "moved",
    "earlier",
    "v",
    "1",
    "v",
    "2",
    "0",
    "point",
    "1",
    "v",
    "3",
    "incorporate",
    "idea",
    "assessing",
    "quality",
    "action",
    "moving",
    "certain",
    "state",
    "environment",
    "agent",
    "quality",
    "action",
    "look",
    "something",
    "like",
    "instead",
    "v",
    "1",
    "q",
    "1",
    "comma",
    "one",
    "q",
    "2",
    "comma",
    "2",
    "s3",
    "robot",
    "four",
    "different",
    "states",
    "choose",
    "along",
    "four",
    "different",
    "actions",
    "also",
    "current",
    "state",
    "calculate",
    "q",
    "comma",
    "cumulative",
    "quality",
    "possible",
    "actions",
    "robot",
    "might",
    "take",
    "let",
    "break",
    "equation",
    "v",
    "equals",
    "maximum",
    "rs",
    "comma",
    "comma",
    "summation",
    "psas",
    "stash",
    "v",
    "discard",
    "maximum",
    "function",
    "plus",
    "gamma",
    "summation",
    "p",
    "v",
    "essentially",
    "equation",
    "produces",
    "v",
    "considering",
    "possible",
    "actions",
    "possible",
    "states",
    "current",
    "state",
    "robot",
    "taking",
    "maximum",
    "value",
    "caused",
    "taking",
    "certain",
    "action",
    "equation",
    "produces",
    "value",
    "footprint",
    "one",
    "possible",
    "action",
    "fact",
    "think",
    "quality",
    "action",
    "q",
    "comma",
    "equal",
    "rs",
    "comma",
    "plus",
    "gamma",
    "summation",
    "p",
    "v",
    "got",
    "equation",
    "quantify",
    "quality",
    "particular",
    "action",
    "going",
    "make",
    "little",
    "adjustment",
    "equation",
    "say",
    "maximum",
    "possible",
    "values",
    "q",
    "comma",
    "right",
    "let",
    "utilize",
    "fact",
    "replace",
    "v",
    "stash",
    "function",
    "q",
    "q",
    "comma",
    "becomes",
    "r",
    "comma",
    "comma",
    "summation",
    "psas",
    "maximum",
    "que",
    "es",
    "equation",
    "v",
    "turned",
    "equation",
    "q",
    "quality",
    "would",
    "done",
    "ease",
    "calculations",
    "one",
    "function",
    "q",
    "also",
    "core",
    "programming",
    "language",
    "one",
    "function",
    "q",
    "calculate",
    "r",
    "comma",
    "quantified",
    "metric",
    "produces",
    "reward",
    "moving",
    "certain",
    "state",
    "qualities",
    "actions",
    "called",
    "q",
    "values",
    "refer",
    "value",
    "footprints",
    "q",
    "values",
    "important",
    "piece",
    "puzzle",
    "temporal",
    "difference",
    "temporal",
    "difference",
    "component",
    "help",
    "robot",
    "calculate",
    "q",
    "values",
    "respect",
    "change",
    "changes",
    "environment",
    "time",
    "consider",
    "robot",
    "currently",
    "mark",
    "state",
    "wants",
    "move",
    "upper",
    "state",
    "one",
    "thing",
    "note",
    "robot",
    "already",
    "knows",
    "q",
    "value",
    "making",
    "action",
    "moving",
    "upper",
    "state",
    "know",
    "environment",
    "stochastic",
    "nature",
    "reward",
    "robot",
    "get",
    "moving",
    "upper",
    "state",
    "might",
    "different",
    "earlier",
    "observation",
    "capture",
    "change",
    "real",
    "difference",
    "calculate",
    "new",
    "q",
    "formula",
    "subtract",
    "previous",
    "known",
    "qsa",
    "turn",
    "give",
    "us",
    "new",
    "qa",
    "equation",
    "derived",
    "gifts",
    "temporal",
    "difference",
    "q",
    "values",
    "helps",
    "capture",
    "random",
    "changes",
    "environment",
    "may",
    "impose",
    "new",
    "q",
    "comma",
    "updated",
    "following",
    "q",
    "comma",
    "equal",
    "qt",
    "minus",
    "1",
    "comma",
    "plus",
    "alpha",
    "td",
    "et",
    "comma",
    "alpha",
    "learning",
    "rate",
    "controls",
    "quickly",
    "robot",
    "adapts",
    "random",
    "changes",
    "imposed",
    "environment",
    "qts",
    "comma",
    "current",
    "state",
    "q",
    "value",
    "qt",
    "minus",
    "1",
    "comma",
    "previously",
    "recorded",
    "q",
    "value",
    "replace",
    "tds",
    "comma",
    "full",
    "form",
    "equation",
    "get",
    "q",
    "comma",
    "equal",
    "qt",
    "1",
    "comma",
    "plus",
    "alpha",
    "comma",
    "plus",
    "gamma",
    "maximum",
    "q",
    "dash",
    "dash",
    "minus",
    "qt",
    "minus",
    "1",
    "comma",
    "little",
    "pieces",
    "q",
    "line",
    "together",
    "let",
    "move",
    "forward",
    "implementation",
    "part",
    "final",
    "equation",
    "right",
    "let",
    "see",
    "implement",
    "obtain",
    "best",
    "path",
    "robot",
    "take",
    "implement",
    "algorithm",
    "need",
    "understand",
    "warehouse",
    "ian",
    "mapped",
    "different",
    "states",
    "let",
    "start",
    "reconnecting",
    "sample",
    "environment",
    "see",
    "l1",
    "l2",
    "l3",
    "align",
    "see",
    "certain",
    "borders",
    "also",
    "first",
    "let",
    "map",
    "locations",
    "warehouse",
    "two",
    "numbers",
    "states",
    "ease",
    "calculations",
    "right",
    "going",
    "create",
    "new",
    "python",
    "3",
    "file",
    "jupyter",
    "notebook",
    "name",
    "learning",
    "numb",
    "okay",
    "let",
    "define",
    "states",
    "need",
    "import",
    "numpy",
    "going",
    "use",
    "numpy",
    "purpose",
    "let",
    "initialize",
    "parameters",
    "gamma",
    "alpha",
    "parameters",
    "gamma",
    "discount",
    "factor",
    "whereas",
    "alpha",
    "learning",
    "rate",
    "next",
    "going",
    "define",
    "states",
    "map",
    "numbers",
    "mentioned",
    "earlier",
    "l",
    "1",
    "zero",
    "online",
    "defined",
    "states",
    "numerical",
    "form",
    "next",
    "step",
    "define",
    "actions",
    "mentioned",
    "represents",
    "transition",
    "next",
    "state",
    "see",
    "array",
    "actions",
    "0",
    "going",
    "define",
    "reward",
    "table",
    "see",
    "matrix",
    "created",
    "showed",
    "understood",
    "correctly",
    "real",
    "barrel",
    "limitation",
    "depicted",
    "image",
    "example",
    "transitional",
    "tell",
    "one",
    "allowed",
    "reward",
    "0",
    "discourage",
    "path",
    "tough",
    "situation",
    "add",
    "minus",
    "1",
    "gets",
    "negative",
    "reward",
    "code",
    "snippet",
    "see",
    "took",
    "put",
    "respective",
    "state",
    "directly",
    "reachable",
    "certain",
    "state",
    "refer",
    "reward",
    "table",
    "created",
    "reconstruction",
    "easy",
    "understand",
    "one",
    "thing",
    "note",
    "consider",
    "top",
    "priority",
    "location",
    "l6",
    "yet",
    "would",
    "also",
    "need",
    "inverse",
    "mapping",
    "state",
    "back",
    "original",
    "location",
    "cleaner",
    "reach",
    "depths",
    "algorithms",
    "going",
    "inverse",
    "map",
    "location",
    "state",
    "location",
    "take",
    "distinct",
    "state",
    "location",
    "convert",
    "back",
    "define",
    "function",
    "get",
    "optimal",
    "get",
    "optimal",
    "route",
    "start",
    "location",
    "n",
    "location",
    "worry",
    "code",
    "back",
    "explain",
    "every",
    "bit",
    "code",
    "get",
    "optimal",
    "root",
    "function",
    "take",
    "two",
    "arguments",
    "starting",
    "location",
    "warehouse",
    "end",
    "location",
    "warehouse",
    "recipe",
    "lovely",
    "return",
    "optimal",
    "route",
    "reaching",
    "end",
    "location",
    "starting",
    "location",
    "form",
    "ordered",
    "list",
    "containing",
    "letters",
    "start",
    "defining",
    "function",
    "initializing",
    "q",
    "values",
    "zeros",
    "see",
    "even",
    "q",
    "value",
    "0",
    "need",
    "copy",
    "reward",
    "matrix",
    "new",
    "one",
    "rewards",
    "new",
    "next",
    "need",
    "get",
    "ending",
    "state",
    "corresponding",
    "ending",
    "location",
    "information",
    "automatically",
    "set",
    "priority",
    "given",
    "ending",
    "stay",
    "highest",
    "one",
    "defining",
    "automatically",
    "set",
    "priority",
    "given",
    "ending",
    "state",
    "nine",
    "nine",
    "nine",
    "going",
    "initialize",
    "q",
    "values",
    "0",
    "learning",
    "process",
    "see",
    "taking",
    "range",
    "1000",
    "going",
    "pick",
    "state",
    "randomly",
    "going",
    "use",
    "mp",
    "dot",
    "random",
    "randint",
    "traversing",
    "neighbor",
    "location",
    "maze",
    "going",
    "iterate",
    "new",
    "reward",
    "matrix",
    "get",
    "actions",
    "greater",
    "0",
    "going",
    "pick",
    "action",
    "randomly",
    "list",
    "playable",
    "actions",
    "years",
    "next",
    "state",
    "going",
    "compute",
    "temporal",
    "difference",
    "td",
    "rewards",
    "plus",
    "gamma",
    "queue",
    "next",
    "state",
    "take",
    "n",
    "p",
    "dot",
    "arg",
    "max",
    "q",
    "next",
    "8",
    "minus",
    "q",
    "current",
    "state",
    "going",
    "update",
    "q",
    "values",
    "using",
    "bellman",
    "equation",
    "see",
    "bellman",
    "equation",
    "going",
    "update",
    "q",
    "values",
    "going",
    "initialize",
    "optimal",
    "route",
    "starting",
    "location",
    "know",
    "next",
    "location",
    "yet",
    "initialize",
    "value",
    "starting",
    "location",
    "random",
    "location",
    "know",
    "exact",
    "number",
    "iteration",
    "needed",
    "reach",
    "final",
    "location",
    "hence",
    "loop",
    "good",
    "choice",
    "iteration",
    "going",
    "fetch",
    "starting",
    "state",
    "fetch",
    "highest",
    "q",
    "value",
    "penetrating",
    "starting",
    "state",
    "go",
    "index",
    "next",
    "state",
    "need",
    "corresponding",
    "letter",
    "going",
    "use",
    "state",
    "location",
    "function",
    "mentioned",
    "going",
    "update",
    "starting",
    "location",
    "next",
    "iteration",
    "finally",
    "return",
    "root",
    "let",
    "take",
    "starting",
    "location",
    "n",
    "line",
    "location",
    "l",
    "see",
    "part",
    "actually",
    "get",
    "see",
    "get",
    "airline",
    "l8l",
    "5",
    "l2",
    "l1",
    "look",
    "image",
    "start",
    "l9",
    "l1",
    "got",
    "l8",
    "l5",
    "l",
    "2",
    "l",
    "1",
    "l",
    "8l",
    "v",
    "l2",
    "l1",
    "would",
    "maximum",
    "value",
    "maximum",
    "reward",
    "robot",
    "come",
    "end",
    "q",
    "learning",
    "session",
    "hope",
    "got",
    "know",
    "exactly",
    "q",
    "learning",
    "analogy",
    "way",
    "starting",
    "number",
    "rooms",
    "hope",
    "example",
    "took",
    "analogy",
    "took",
    "good",
    "enough",
    "understand",
    "understand",
    "bellman",
    "equation",
    "make",
    "quick",
    "changes",
    "bellman",
    "equation",
    "create",
    "reward",
    "table",
    "cue",
    "update",
    "q",
    "values",
    "using",
    "bellman",
    "equation",
    "alpha",
    "karma"
  ],
  "keywords": [
    "data",
    "science",
    "technology",
    "useful",
    "insights",
    "order",
    "solve",
    "complex",
    "problems",
    "session",
    "contains",
    "everything",
    "need",
    "know",
    "get",
    "let",
    "take",
    "look",
    "agenda",
    "first",
    "module",
    "basic",
    "statistics",
    "probability",
    "understand",
    "math",
    "behind",
    "machine",
    "learning",
    "algorithms",
    "next",
    "exactly",
    "different",
    "types",
    "supervised",
    "start",
    "understanding",
    "linear",
    "regression",
    "logistic",
    "see",
    "used",
    "classification",
    "discuss",
    "decision",
    "trees",
    "random",
    "forest",
    "help",
    "use",
    "cases",
    "examples",
    "discussing",
    "neighbor",
    "gain",
    "bias",
    "one",
    "important",
    "spam",
    "algorithm",
    "support",
    "vector",
    "svm",
    "draw",
    "hyperplane",
    "classes",
    "finally",
    "move",
    "clustering",
    "perform",
    "analysis",
    "using",
    "association",
    "rule",
    "mining",
    "reinforcement",
    "concepts",
    "along",
    "couple",
    "deep",
    "last",
    "questions",
    "make",
    "sure",
    "technologies",
    "right",
    "probably",
    "obviously",
    "process",
    "sense",
    "much",
    "comes",
    "today",
    "depth",
    "ahead",
    "going",
    "begin",
    "various",
    "social",
    "media",
    "walmart",
    "patterns",
    "database",
    "increase",
    "business",
    "scientist",
    "also",
    "sets",
    "needed",
    "become",
    "job",
    "analyst",
    "solution",
    "done",
    "k",
    "means",
    "case",
    "steps",
    "involved",
    "part",
    "cluster",
    "based",
    "like",
    "facebook",
    "end",
    "guys",
    "lot",
    "topic",
    "remember",
    "times",
    "go",
    "call",
    "things",
    "simple",
    "generate",
    "even",
    "store",
    "numbers",
    "back",
    "days",
    "us",
    "stored",
    "similarly",
    "earlier",
    "little",
    "processing",
    "small",
    "created",
    "around",
    "every",
    "cars",
    "connected",
    "internet",
    "generated",
    "initially",
    "could",
    "tools",
    "way",
    "better",
    "day",
    "term",
    "include",
    "machines",
    "almost",
    "else",
    "basis",
    "amount",
    "equal",
    "according",
    "estimated",
    "five",
    "hundred",
    "number",
    "time",
    "something",
    "okay",
    "methods",
    "hope",
    "moving",
    "fact",
    "actually",
    "says",
    "eight",
    "thousand",
    "would",
    "trying",
    "figure",
    "information",
    "analyze",
    "entire",
    "apart",
    "factors",
    "well",
    "transactions",
    "online",
    "buy",
    "watch",
    "fit",
    "basically",
    "conditions",
    "thing",
    "uses",
    "world",
    "building",
    "good",
    "able",
    "two",
    "point",
    "reason",
    "user",
    "customer",
    "customers",
    "buys",
    "might",
    "particular",
    "looking",
    "needs",
    "consider",
    "sales",
    "example",
    "found",
    "across",
    "dot",
    "situation",
    "put",
    "another",
    "true",
    "seven",
    "ask",
    "relationship",
    "people",
    "really",
    "eat",
    "place",
    "check",
    "making",
    "find",
    "training",
    "product",
    "person",
    "quite",
    "products",
    "1",
    "users",
    "instead",
    "try",
    "hidden",
    "discount",
    "line",
    "set",
    "decisions",
    "movie",
    "want",
    "give",
    "big",
    "question",
    "scientists",
    "starts",
    "exploration",
    "sort",
    "skills",
    "build",
    "predictive",
    "models",
    "type",
    "many",
    "main",
    "work",
    "coding",
    "language",
    "python",
    "r",
    "since",
    "study",
    "always",
    "maximum",
    "likelihood",
    "descriptive",
    "company",
    "expected",
    "statistical",
    "code",
    "load",
    "libraries",
    "run",
    "must",
    "minimum",
    "multiple",
    "format",
    "called",
    "getting",
    "wrangling",
    "task",
    "instances",
    "missing",
    "values",
    "null",
    "picture",
    "trends",
    "results",
    "large",
    "working",
    "nearest",
    "tell",
    "implemented",
    "discussed",
    "required",
    "asked",
    "implement",
    "system",
    "present",
    "sum",
    "problem",
    "approach",
    "offer",
    "best",
    "instance",
    "identify",
    "create",
    "without",
    "application",
    "quality",
    "easily",
    "measures",
    "modeling",
    "loan",
    "test",
    "experience",
    "name",
    "new",
    "mean",
    "knowledge",
    "weight",
    "inside",
    "mainly",
    "focus",
    "link",
    "said",
    "six",
    "stage",
    "central",
    "variables",
    "predicted",
    "collected",
    "finding",
    "takes",
    "remove",
    "clean",
    "subset",
    "plot",
    "explore",
    "model",
    "input",
    "testing",
    "separate",
    "parts",
    "evaluate",
    "goal",
    "environment",
    "final",
    "performance",
    "come",
    "clear",
    "form",
    "count",
    "mind",
    "categories",
    "terminologies",
    "sampling",
    "techniques",
    "inferential",
    "center",
    "entropy",
    "confusion",
    "matrix",
    "conditional",
    "show",
    "bayes",
    "theorem",
    "mention",
    "demo",
    "definitely",
    "estimation",
    "confidence",
    "interval",
    "estimate",
    "margin",
    "error",
    "real",
    "hypothesis",
    "works",
    "alright",
    "helps",
    "together",
    "terms",
    "divided",
    "discrete",
    "continuous",
    "gender",
    "male",
    "female",
    "three",
    "four",
    "say",
    "id",
    "represented",
    "either",
    "average",
    "record",
    "known",
    "pretty",
    "anything",
    "measure",
    "categorical",
    "possible",
    "class",
    "50",
    "2",
    "3",
    "difference",
    "variable",
    "represent",
    "value",
    "dependent",
    "independent",
    "wo",
    "depends",
    "area",
    "collection",
    "think",
    "already",
    "collect",
    "may",
    "common",
    "friend",
    "game",
    "blue",
    "idea",
    "population",
    "sample",
    "words",
    "events",
    "choose",
    "represents",
    "add",
    "chosen",
    "parameter",
    "method",
    "within",
    "median",
    "mode",
    "standard",
    "deviation",
    "variance",
    "scenario",
    "correct",
    "taking",
    "made",
    "technique",
    "samples",
    "selected",
    "object",
    "john",
    "randomly",
    "select",
    "image",
    "groups",
    "second",
    "group",
    "least",
    "meaning",
    "far",
    "worry",
    "level",
    "features",
    "upon",
    "suppose",
    "size",
    "makes",
    "predictions",
    "taken",
    "parameters",
    "mentioned",
    "range",
    "starting",
    "10",
    "divide",
    "ratio",
    "related",
    "cards",
    "car",
    "total",
    "8",
    "define",
    "points",
    "nine",
    "hence",
    "calculated",
    "4",
    "6",
    "given",
    "formula",
    "calculate",
    "red",
    "highest",
    "following",
    "lies",
    "q",
    "still",
    "third",
    "minus",
    "shows",
    "x",
    "n",
    "bar",
    "square",
    "whether",
    "calculating",
    "squared",
    "result",
    "root",
    "tree",
    "explain",
    "uncertainty",
    "items",
    "event",
    "confusing",
    "feature",
    "gives",
    "outcome",
    "whole",
    "attribute",
    "v",
    "h",
    "subsets",
    "comma",
    "statement",
    "predict",
    "match",
    "weather",
    "predictor",
    "outlook",
    "humidity",
    "wind",
    "target",
    "decide",
    "play",
    "yes",
    "branch",
    "denotes",
    "node",
    "14",
    "sunny",
    "overcast",
    "rain",
    "nodes",
    "choosing",
    "note",
    "rainy",
    "assigned",
    "output",
    "percent",
    "baby",
    "certain",
    "impurity",
    "significant",
    "ways",
    "head",
    "split",
    "saw",
    "windy",
    "temperature",
    "hand",
    "attributes",
    "false",
    "zero",
    "low",
    "years",
    "compare",
    "high",
    "normal",
    "less",
    "keep",
    "read",
    "talk",
    "classifier",
    "accuracy",
    "actual",
    "looks",
    "positive",
    "negative",
    "stands",
    "nothing",
    "patient",
    "patients",
    "disease",
    "60",
    "classify",
    "whereas",
    "concept",
    "frequency",
    "function",
    "print",
    "write",
    "pass",
    "functions",
    "easy",
    "occurring",
    "mathematical",
    "likely",
    "outcomes",
    "0",
    "phase",
    "5",
    "space",
    "single",
    "card",
    "king",
    "ball",
    "distribution",
    "limit",
    "graph",
    "curve",
    "b",
    "away",
    "similar",
    "location",
    "states",
    "enough",
    "change",
    "color",
    "answer",
    "occurrence",
    "previous",
    "left",
    "p",
    "salary",
    "package",
    "undergone",
    "got",
    "edge",
    "table",
    "directly",
    "defining",
    "condition",
    "base",
    "open",
    "equation",
    "balls",
    "picking",
    "probabilities",
    "pick",
    "step",
    "risk",
    "accurate",
    "reach",
    "purchase",
    "distance",
    "c",
    "predicting",
    "score",
    "row",
    "drop",
    "threshold",
    "friends",
    "manually",
    "learn",
    "e",
    "automatically",
    "top",
    "trained",
    "rules",
    "map",
    "age",
    "response",
    "train",
    "prediction",
    "kind",
    "available",
    "lead",
    "close",
    "direct",
    "changes",
    "fine",
    "marketing",
    "income",
    "mx",
    "plus",
    "straight",
    "category",
    "axis",
    "travel",
    "equals",
    "summation",
    "columns",
    "file",
    "passing",
    "performing",
    "turn",
    "closer",
    "simply",
    "implementing",
    "jupyter",
    "notebook",
    "import",
    "numpy",
    "pandas",
    "csv",
    "execute",
    "button",
    "consists",
    "rows",
    "underscore",
    "return",
    "length",
    "library",
    "till",
    "forward",
    "gets",
    "infinity",
    "seen",
    "tomorrow",
    "goes",
    "medical",
    "history",
    "titanic",
    "survive",
    "suv",
    "passengers",
    "survived",
    "index",
    "column",
    "passenger",
    "traveling",
    "orange",
    "green",
    "belong",
    "labels",
    "yellow",
    "rather",
    "defined",
    "sklearn",
    "state",
    "label",
    "belongs",
    "transaction",
    "credit",
    "item",
    "kn",
    "knn",
    "policy",
    "greater",
    "list",
    "leaf",
    "pruning",
    "room",
    "action",
    "rose",
    "email",
    "actions",
    "neighbors",
    "closest",
    "euclidean",
    "l",
    "current",
    "vectors",
    "agent",
    "clusters",
    "centroid",
    "initial",
    "lift",
    "frequent",
    "rewards",
    "reward",
    "path",
    "meat",
    "tiger",
    "gamma",
    "traverse",
    "rooms",
    "robot",
    "bellman"
  ]
}