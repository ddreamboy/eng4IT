{
  "text": "ensemble methods take multiple models\nwhich we'll call weak learners\nand combine them together to form a more\npowerful model overall\nensemble models are often at the top of\nthe leaderboard for data science\ncompetitions\nsuch as those found on kaggle there are\nseveral different ensembling techniques\nand today we'll talk about bagging\nboosting and stacking\nbagging or bootstrap aggregating\ncombines homogeneous weak learners\nwhen i say homogeneous what i mean here\nis that the weak learners are all the\nsame type of model\nfor example perhaps they're all decision\ntrees\nwith bagging we train each of the weak\nlearners independently\nwhich means we can parallelize the model\ntraining across multiple cores or\ncomputers\nvery easily to obtain predictions on\nunseen data\nwe then aggregate the predictions from\neach of the weak learners and perform\nsome type of averaging\nso why is it called bootstrap\naggregating well\nafter we've split our data into a\ntraining set and a test set\nwe create b different bootstrap samples\nand train a weak learner on each of\nthese b\nbootstrap samples and train a weak\nlearner on each one\nto make predictions using the bagged\nmodel we pass an observation through\neach of the b\nweak learners and then we aggregate the\npredictions\nif it's classification this usually\ntakes the form of a vote\nif we have a continuous response\nvariable this is typically performed as\naveraging so what's the difference\nbetween bagging decision trees\nand a random forest a random forest is a\nbagging algorithm\nbut there's one main difference between\nbagging decision trees in general\nand the random forest algorithm with a\nrandom forest we not only use a\ndifferent subset of observations to\ntrain each of the individual decision\ntrees\nbut we also take a different subset of\nthe variables\nto train each of the individual decision\ntrees\nboosting is an ensembling method that\ntrains models sequentially or\niteratively\ntherefore the weak learners are not\nindependent\nthe training of the model at the current\nstep depends on previous models\none example of a boosting algorithm is\nadaboost or\nadaptive boost for binary classification\nthe adaboost algorithm works in the\nfollowing way each model in the sequence\nis trained where higher importance is\ngiven to the observations that were most\ndifficult to predict in the previous\nstep\nspecifically the weights of the\npreviously misclassified observations\nare increased\nat the end in order to form the final\nstrong learner from the weak learners\na weighted sum of the weak learners is\ntaken the weights for each of the weak\nlearners is dependent on the performance\nof the weak learner\nthe weights for each of the weak\nlearners is dependent on the performance\nof the weak learner\nin other words the better the weak\nlearner performed the higher its weight\nwill be\nanother example of a boosting algorithm\nis gradient boosting\nwith gradient boosting the final model\nis also a weighted sum of the weak\nlearners\nhowever gradient descent is used to\ndetermine how to improve at\neach step in the sequence gradient\nboosting is a generalization of boosting\nwhere optimization can be based on any\narbitrary differentiable\nloss function gradient boosting is often\nused for decision trees\nand gradient boosted trees often\noutperform random forests\nstacking is an ensembling method that\ncombines heterogeneous weak learners\nfor example you might combine neural\nnetworks with decision trees with glms\nand so on\nit's also important to note that it's\ncommon to have bagged and boosted models\nas weak learners in a stacked ensemble\nfor this reason\nstacked models can be very very\ndifficult to interpret meaningfully\nwith that being said they are usually\ntop performing models\nso if ensemble methods are not\ninterpretable why may we want to use\nthem\nwell often our priority is not\ninterpretability\nthere are situations where we want to\nhave the model with the highest possible\naccuracy\nin these cases ensemble methods are\nhighly desirable\n",
  "words": [
    "ensemble",
    "methods",
    "take",
    "multiple",
    "models",
    "call",
    "weak",
    "learners",
    "combine",
    "together",
    "form",
    "powerful",
    "model",
    "overall",
    "ensemble",
    "models",
    "often",
    "top",
    "leaderboard",
    "data",
    "science",
    "competitions",
    "found",
    "kaggle",
    "several",
    "different",
    "ensembling",
    "techniques",
    "today",
    "talk",
    "bagging",
    "boosting",
    "stacking",
    "bagging",
    "bootstrap",
    "aggregating",
    "combines",
    "homogeneous",
    "weak",
    "learners",
    "say",
    "homogeneous",
    "mean",
    "weak",
    "learners",
    "type",
    "model",
    "example",
    "perhaps",
    "decision",
    "trees",
    "bagging",
    "train",
    "weak",
    "learners",
    "independently",
    "means",
    "parallelize",
    "model",
    "training",
    "across",
    "multiple",
    "cores",
    "computers",
    "easily",
    "obtain",
    "predictions",
    "unseen",
    "data",
    "aggregate",
    "predictions",
    "weak",
    "learners",
    "perform",
    "type",
    "averaging",
    "called",
    "bootstrap",
    "aggregating",
    "well",
    "split",
    "data",
    "training",
    "set",
    "test",
    "set",
    "create",
    "b",
    "different",
    "bootstrap",
    "samples",
    "train",
    "weak",
    "learner",
    "b",
    "bootstrap",
    "samples",
    "train",
    "weak",
    "learner",
    "one",
    "make",
    "predictions",
    "using",
    "bagged",
    "model",
    "pass",
    "observation",
    "b",
    "weak",
    "learners",
    "aggregate",
    "predictions",
    "classification",
    "usually",
    "takes",
    "form",
    "vote",
    "continuous",
    "response",
    "variable",
    "typically",
    "performed",
    "averaging",
    "difference",
    "bagging",
    "decision",
    "trees",
    "random",
    "forest",
    "random",
    "forest",
    "bagging",
    "algorithm",
    "one",
    "main",
    "difference",
    "bagging",
    "decision",
    "trees",
    "general",
    "random",
    "forest",
    "algorithm",
    "random",
    "forest",
    "use",
    "different",
    "subset",
    "observations",
    "train",
    "individual",
    "decision",
    "trees",
    "also",
    "take",
    "different",
    "subset",
    "variables",
    "train",
    "individual",
    "decision",
    "trees",
    "boosting",
    "ensembling",
    "method",
    "trains",
    "models",
    "sequentially",
    "iteratively",
    "therefore",
    "weak",
    "learners",
    "independent",
    "training",
    "model",
    "current",
    "step",
    "depends",
    "previous",
    "models",
    "one",
    "example",
    "boosting",
    "algorithm",
    "adaboost",
    "adaptive",
    "boost",
    "binary",
    "classification",
    "adaboost",
    "algorithm",
    "works",
    "following",
    "way",
    "model",
    "sequence",
    "trained",
    "higher",
    "importance",
    "given",
    "observations",
    "difficult",
    "predict",
    "previous",
    "step",
    "specifically",
    "weights",
    "previously",
    "misclassified",
    "observations",
    "increased",
    "end",
    "order",
    "form",
    "final",
    "strong",
    "learner",
    "weak",
    "learners",
    "weighted",
    "sum",
    "weak",
    "learners",
    "taken",
    "weights",
    "weak",
    "learners",
    "dependent",
    "performance",
    "weak",
    "learner",
    "weights",
    "weak",
    "learners",
    "dependent",
    "performance",
    "weak",
    "learner",
    "words",
    "better",
    "weak",
    "learner",
    "performed",
    "higher",
    "weight",
    "another",
    "example",
    "boosting",
    "algorithm",
    "gradient",
    "boosting",
    "gradient",
    "boosting",
    "final",
    "model",
    "also",
    "weighted",
    "sum",
    "weak",
    "learners",
    "however",
    "gradient",
    "descent",
    "used",
    "determine",
    "improve",
    "step",
    "sequence",
    "gradient",
    "boosting",
    "generalization",
    "boosting",
    "optimization",
    "based",
    "arbitrary",
    "differentiable",
    "loss",
    "function",
    "gradient",
    "boosting",
    "often",
    "used",
    "decision",
    "trees",
    "gradient",
    "boosted",
    "trees",
    "often",
    "outperform",
    "random",
    "forests",
    "stacking",
    "ensembling",
    "method",
    "combines",
    "heterogeneous",
    "weak",
    "learners",
    "example",
    "might",
    "combine",
    "neural",
    "networks",
    "decision",
    "trees",
    "glms",
    "also",
    "important",
    "note",
    "common",
    "bagged",
    "boosted",
    "models",
    "weak",
    "learners",
    "stacked",
    "ensemble",
    "reason",
    "stacked",
    "models",
    "difficult",
    "interpret",
    "meaningfully",
    "said",
    "usually",
    "top",
    "performing",
    "models",
    "ensemble",
    "methods",
    "interpretable",
    "may",
    "want",
    "use",
    "well",
    "often",
    "priority",
    "interpretability",
    "situations",
    "want",
    "model",
    "highest",
    "possible",
    "accuracy",
    "cases",
    "ensemble",
    "methods",
    "highly",
    "desirable"
  ],
  "keywords": [
    "ensemble",
    "methods",
    "take",
    "multiple",
    "models",
    "weak",
    "learners",
    "combine",
    "form",
    "model",
    "often",
    "top",
    "data",
    "different",
    "ensembling",
    "bagging",
    "boosting",
    "stacking",
    "bootstrap",
    "aggregating",
    "combines",
    "homogeneous",
    "type",
    "example",
    "decision",
    "trees",
    "train",
    "training",
    "predictions",
    "aggregate",
    "averaging",
    "well",
    "set",
    "b",
    "samples",
    "learner",
    "one",
    "bagged",
    "classification",
    "usually",
    "performed",
    "difference",
    "random",
    "forest",
    "algorithm",
    "use",
    "subset",
    "observations",
    "individual",
    "also",
    "method",
    "step",
    "previous",
    "adaboost",
    "sequence",
    "higher",
    "difficult",
    "weights",
    "final",
    "weighted",
    "sum",
    "dependent",
    "performance",
    "gradient",
    "used",
    "boosted",
    "stacked",
    "want"
  ]
}