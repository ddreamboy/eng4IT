{
  "text": "it's super exciting\nlast week openai released gpt3 the\nlargest\nlanguage model ever released we had a\nfantastic conversation about this today\nnow the most exciting thing about the\nmodel isn't its sheer size\nbut that it's only been trained to do\nlanguage modeling\nbut it can do any number of nlp tasks\nout of the box without any fine tuning\nand that\nworking in this kind of precision has\nnever been seen before\nand it raises many questions about the\nfuture of building\nnlp models and how we will interact with\nthem\nin the future gpt3 is the biggest\nlanguage model ever produced 175\nbillion parameters you need a super\ncomputer\nto train it and openai has done\nexactly that gpt-3 is a 175 billion\nparameter\nneural network 10 times larger than the\nbiggest artificial neural network that\nhad ever been trained before now\nso how do they train such an enormous\nmodel this is powered by the zero two\noptimizations\nin microsoft's open source deep speed\nlibrary\nthere's been a lot of research in\nefficiently scaling training to hundreds\nor thousands of gpus\nthe foundational ideas of this research\nare data parallelism\nwhere we copy the model on each gpu and\nthen send each gpu a different batch of\ndata\nand model parallelism where we split the\nmodel up onto different gpus\nmodel parallelism is a necessary\ncomponent of this because we can only\nfit a 1.4 billion parameter model\non a single nvidia v100 gpu that has 32\ngigabytes of memory\nso the next step in this story was the\ndevelopment of pipeline parallelism\nmost famously implemented in google's\ng-pipe which was used to hit\nstate-of-the-art image net accuracy\nwith 557 million parameters\ng-pipe partitions the mini batches such\nthat different subsets of a model\nsplit on multiple gpus has something to\ndo while they're waiting for the\nsequential forward and backward passes\nthrough the network to finish in a\nsimilar vein xero recognizes the\ninefficiency\nwith how data parallelism partitions\ntraining memory across accelerators\nif we're splitting our model across\nmultiple accelerators we don't need each\none to hold the entire set of optimizer\nstates\ngradients and parameters so to solve\nthis inefficiency\nxero uses a dynamic scheduling of\noptimizer state\ngradients and parameters to communicate\nbetween multiple gpus\nthe first iteration of xero had produced\nturing nlg\na 17 billion parameter language model\nthat continued to improve the perplexity\nmetric\nfor auto regressive language modeling\nfrom the previous iteration which was\nnvidia's megatron lm\nat 8.3 billion parameters this was the\nresult of implementing the optimizer\nstate memory reductions\nin zero one well zero two further\nimplements the gradient and parameter\noptimizations as well\nthe bar chart in their blog post\nhighlights the enormous gains from zero\none\nto zero two which evidences the training\ndifferences between turing nlg and gpt3\ngpt3 is an autoregressive language model\nas opposed to a denoising autoencoder\nlike birth\nand in today's conversation we talk\nabout some of the comparative\ndifferences\nbetween those two architectures of\nlanguage model there's been such an\nincredible amount of progress in\nlanguage processing\nespecially using neural networks kind of\nstarted in about 2013 when mickelov\nintroduced word to vec\nand that was a distributed\nrepresentation for words which was\nlearned in a self-supervised way based\non the context\nand then we had lstm type language\nmodels and they were learning a\nsequential\nrepresentation of tokens through time\nand there were bidirectional variants of\nthis\na really cool thing that happened in\nabout 2018 was jeremy howard\nand sebastian ruder released ulm fit and\nthat introduced transfer learning to\nnatural language processing\nword to vect technically is transfer\nlearning but it was transfer learning in\nthe sense of\nthe language model was learning\ndependencies between a whole bunch of\ntokens\nnot individual tokens and then of course\nit could be fine-tuned on a downstream\ntask\nattention is all you need introduce\ntransformers and transformers are these\nincredible\nkind of routing machines they can learn\ndependencies between\nany tokens in the input and then it goes\nthrough this successive\nclever routing system which is learned\nas part of the training process\nin the original recurrent neural\nnetworks the tokens and the input\nsequence couldn't directly attend to\neach other\nand that meant that those models\nsuffered from catastrophic forgetting\nand also they were quite difficult to\ntrain because they had\nvanishing and exploding gradients so\ntransformers came along and it was a\ndifferent paradigm\nyou could learn a complex hierarchy of\nrelationships\ndirectly between the tokens it was no\nlonger this sequential\nrepresentation where you only knew what\nhappened at one\ntime step backwards it was a real\nparadigm shift\nand straight away these language\nprocessing models produce\nstate-of-the-art results\nacross the board generally speaking\nthere are two architectural patterns\nthat we see with these transformers\narchitectures\nthe first is the auto-regressive pattern\nwhere the model is just predicting the\nnext word and the next word and the next\nword and the\nthe answer from the previous prediction\ngets fed into the model the next time\naround\nthe other type of model is what's called\na denoising autoencoder like\nburton roberta and excel net and what\nthese do is\nyou feed in an input sentence and then\nyou typically you add some noise to it\nand then you say what you expect\nso these things seem to be appropriate\nfor things like question answering when\nyou need to\npoint to spans in your input sequence\nfor example if you're doing\nin-span question answering the\nauto-regressive models are quite\nattractive because\nyou can just keep generating data\nforever\nso they seem appropriate for natural\nlanguage generation one of the things i\nwanted to explore today was the\ncommercial utility of this\nis this something that can help us in\nindustry or is this something which is\nmainly an academic endeavor and only in\nreach for\nthe very top tech companies like\nmicrosoft and google\nnow yannick did an incredible video\nabout gpt3 last week on his channel\nand he's starting to develop a bit of a\nname for himself online\nlightspeed yannick you might say within\nhours of the paper being released he had\nthe video\nout on his youtube channel and it was\neven linked by\nandre capathi the director of ai at\ntesla so\nit's incredible i've cut about five\nminutes of the most descriptive elements\nout of yannick's video on gbt3 and i'm\ngoing to play them now\ni think it will serve as a good\nrefresher for the topic this paper is\nbasically an investigation into\nwhat you can do with giant language\nmodels now this language model is an\norder of magnitude larger than anyone\nhas ever\nbuilt a language model and it can do\nsome absolutely crazy things\nthis paper is basically an investigation\ninto what you can do\nwith giant language models now this\nlanguage model is an\norder of magnitude larger than anyone\nhas ever\nbuilt a language model and it can do\nsome absolutely crazy things\nso we'll basically go over the\narchitecture over what the model does\nand over the experimental results it\nturns out that if you train a language\nmodel on enough data\nit is able to solve nlp\ntasks that it has never seen just out of\nthe box\nso a language model let's just take an\nexample this\nsentence right here just the sentence as\nsuch like\nthird humans do not require large\nsupervised data sets to learn\nmost language tasks but this is an\nenglish sentence\nand a language model would be a model\nthat if you cross out a portion\nfrom the end here like this right here\nit would be able to tell you what comes\nnext\nso in a language model you would input\nthis part right here\nand it will tell you the next word is\ndata sets\nso that's basically all the language\nmodel does and\nonce you've trained one you can\nbasically generate word\nafter word after word from it or you can\nask it\na question like which word is most\nlikely to\ncome next or more likely so a language\nmodel is nothing but a model\nthat can kind of generate language in a\nprobabilistic way\nand the cool thing about language models\nis that you can train it\non any sort of text data and that's what\nthey do here\nso they train a language model on giant\namounts\nof data just compare this to a language\nmodel\nlike bert bert required\nthis much flops to train and these this\nis a log scale\nso this is right here this is several\norders of magnitude\na larger and bigger model and is trained\nfor\nway longer on this text so naturally it\nis going to be a lot better at\nlanguage modeling you can see right here\nthe size of these models that they\ntrained on remember the previous largest\nlanguage model the touring nlg of\nmicrosoft had something like 17 billion\nparameters so would be comparable to\nthis right here whereas gpt-3 has\n175 billion parameters\nwhich this is absolutely crazy it's an\norder of magnitude higher\nthan anything that's ever existed and if\nyou look at the\nlast gpt the gpt-2 model that if you\nremember i've made a video about it\nis too dangerous to be released well now\nit has been released but was too\ndangerous to be released\nit clocked in at about 1.5 billion\nparameters\nbut essentially this is an\nauto-regressive language model so it's\nnot like bert\nit's not bi-directional it is\nauto-regressive it goes from left to\nright\nalways produces the next word it is like\ngpt-2 they even say this they say\nwe use the same model and architecture\nas\ngpt2 they just have more layers\nand wider layers and more data to train\nit on\nso with something like bird you would do\nfirst pre-train so there you would this\nis the language modeling right here this\npre-training phase\nwhere you teach bert about the english\nlanguage by just feeding it a lot of\ndata\nand then second you had a step called\nfine tuning\nso on the second one you'd have\nsomething like\nthe task you're actually interested in\nand let's say the task you're actually\ninterested in\nis sentiment classification so in\nsentiment classification you have like a\nsentence like blah blah blah\nand you want to know is that a positive\nsentiment like is a happy sentence or is\nit a sad sentence\nand you would have a database of labeled\ninstances of that so\nin this database you'd have a bunch of\nsentences and for each one\nyou would know is it good is it is it\npositive or is it negative\nand then you'd have like a smaller test\nset right here\nyou would basically take this\npre-trained model train it\non this data set in a supervised machine\nlearning way\nand then test it on this test set right\nhere this is called fine tuning\nwhat they are interested in is basically\nto take the pre-trained model\nand directly go and evaluate it on the\ntest data set in a sort of a zero-shot\nfashion\nyour language model that you pre-trained\nand you just\ninput the following text you input what\nthey call a task description\nand a prompt so this is the input and\nyou\nsimply ask the model as a language model\nto predict the next word it's just what\ncomes here\nnow what you're counting on is basically\nthat in the training data the model has\nseen a structure like this enough to\nunderstand what's going on\nso that in the training data somewhere\nin the internet there was this structure\nof\ntranslate something to something and\nthen there would be a word here of\nsomething and you know it kind of has to\nrealize that this goes here\nlike that the next word so basically\nwhat you're asking it is\nif you were to find this\ntext on a website or on wikipedia or in\nany of the books data set if you were to\nfind\nthis piece of text what would be\nthe next word in that piece of text you\nsimply input this\nas a string so not only do you have the\ntask description\nand the prompt right here but you also\nhave one\nexample and the example so the example\nis going to come from the training data\nset\nof the task that you're interested in\nbut the important part is you never\ntrain on it you never explicitly\ntrain on that example you simply put it\nin the context so you simply put this\nstring so translate english to french\nnew line\nsee other loot is\nnew line cheese is what you simply input\nthat string\ninto the model as a language model and\nyou ask it\nwhat's the next word right here\nokay so i hope i hope this is clear this\nis\nwhat they call kind of one shot\ngeneralization\nand by one shot they basically mean you\nsimply provide\nthis thing in the context of the model\nas a language model now the the\nadvantage here is immediately clear\nthat you only have to train one model\nthen\nand then basically at inference time you\ncan just\ninput the task description\nand the sort of training data for the\ntask\ninto its its evaluation context\nand the task itself i think what it does\nis it will simply\ntake all of this\nand it will go to its own training data\nwhich it has\nstored in its weights and it will filter\nthe training data\nand basically take out the the things\nthat sort of\npattern match sort of regex match in a\nfuzzy way\nto this context and then it will kind of\ninterpolate these training examples in\norder to come up with the answer i don't\nthink there is reasoning happening here\ni was looking at some of the comments\nactually on the video and some of them\nmake for interesting reading\nyannick says he gets better comments on\nhis youtube videos than he does\nfrom conferences when he submits his\npapers here are some of the reviews\nso as you can see this is a janik's\nvideo that he published\nand it's been viewed nearly 28 000 times\n1.2 000 up votes and nine people\ndown voted it you should be ashamed of\nyourselves\n you alex bravo chips in with t6\none trillion text to text transfer\ntransformer the next model coming out of\ngoogle\nmallow marsh comments that just looking\nat the wall of authors at the beginning\nis making me sweat some person with an\nincomprehensible\nyoutube name and a human adversarial\nexample as a profile picture\nadds i don't think the intuition of the\nmodel essentially just storing or the\ntraining data\nin a quasi lookup table is correct if\nanything\nthe model acts as a very elaborate\ncompression algorithm\nalso modeling the semantic structure of\nthe language\nneeded to pass the natural language\nmodel input certainly is achieved in a\nway\nthat doesn't resemble a plane lookup\ntable okay so\nhe seemed to lose his grammatical um\nfidelity towards the end of that message\nbut you get the idea and yannick replies\nyes there's an argument to be made for\nthat i'm not only saying they're plain\nlook up tables\nbut more like fuzzy look up and\ninterpolation tables\nmy main point is that all of these tasks\nwhere the model performs well\ncan be explained by lookup and\ninterpolation and there's\nnone where the model succeeds where\nyou'd have to say it was due to\nreasoning abilities\nand this seems to be the really clever\nthing about gpt3 what it does\nis it internalizes all of the\ninformation you give it\nand it deconstructs it and packs it into\nsome internal representation\nand then when you ask it to go and do\nsomething for you it will\ngo and find all of the information it\nwas trained on and it will reconstruct\nit by interpolating between all of the\ninternal structures that it learned\nsuper clever stuff happy teacher says\nwow\nit's incredible how fast you are i agree\nwith you happy teacher\nit really is gary blauer says\ncontinuing to expand the class of\nproblems which can be solved by pattern\nrecognition\nwon't get us all the way to artificial\ngeneral intelligence\nbut it's very interesting and impressive\nnonetheless this is a remarkable example\nhe then goes on to quote chalet without\ncrediting charlay i might say\nany problem can be treated as a pattern\nrecognition problem if your training\ndata covers a sufficiently dense\nsampling of the problem space\nwhat's interesting is what happens when\nyour training data\nis sparse indeed\npoodle chen says i like to think of\nlanguage models as really smart parrots\nthey repeat what you've said\nthank you poodle chen we like that dodon\nko\nalso unwittingly quotes francois chalet\n175 billion parameters sometimes i feel\nthat that's like trying to reach the\nmoon by just building higher and higher\nskyscrapers until we get there\nhe said reach the moon twice i think\nthis might have been generated with gpt3\nrussian person says i think that a lot\nof people missed the news\nthat microsoft fired all of their\njournalists right off gpt-3\nindeed they did don't need them anymore\nandrew owens says\nnot agi enjoy watching your take on\nthese papers\ni usually watch the video at 1.75 times\nnormal speed\nit makes me feel more comfortable with\nyour sloppy handwriting\ni can apologize on behalf of yannick his\nhandwriting is\nsean hardy says phenomenal analysis you\nreally make this field\napproachable to pre-university students\nlike myself\nchristian garcia says i was thinking\nabout the addition being memorized\nargument which i totally agree with\nand it reminded me that we humans also\ntend to replace a lot of logic with\nmemory\nfor example multiplication tables\nanecdotally i think i've memorized\nvarious combinations of numbers that add\nup to ten\nfive plus five six plus four seven plus\nthree deep learning still needs a good\nway to do logic reasoning\nbut what if i have a vast amount of\nknowledge that is a good portion of\nhuman-like intelligence\nwell this is one of the things that we\ntalked about in the show today\nthat there are so many things that\ninitially we need to reason about using\nour system too\nbut then it seems to get distilled and\nbaked into a system one program\nborislav dizodzo says sorry to crash the\nmensa\nparty but isn't the prediction of the\nnext word inherently a shallow target\ni don't know how to exactly formulate a\ndeeper thought target\nbut perhaps a whole sentence prediction\nwith blur\nor some other and better similarity\nmeasure would do the trick\nand the trick would only work if\ntransformers were inherently good at\nmodeling\nreasoning some other thoughts that come\nto mind is the prediction of\nany sentence that would take place in\nthe future\nand the further in the future if the\ntext it takes place the bigger the\nreward\nsince i don't have the super computers\nand the youtube comments are not cited\nmaybe you can just tell me why i'm wrong\nor silently implement something if you\nthink i'm right\nand yannick says well the problem with\nblur is that it's not differentiable\nso you'd essentially have to do\nreinforcement learning and people have\ndone that\nas for predicting longer sequences you\nwould have to back propagate through\nmultiple invocations of this already\ngiant model\nso that's out of the question to predict\na single word\nthat's further in the future than the\nnext word and you can do that\nbut it will likely increase your\nvariance more than it will benefit your\nmodel\ngood ideas though and good old herp\nderpingsson\nher your comments are always legendary\nthat you say a single part of dl\nresearch is just\nflexing how many gpu hours you can\nafford\nvery true unfortunately i can't afford\nvery many\ni hope you have fun of us analyzing the\nhype and the stories around gpt3\nand we do get into a fair bit of various\ntopics here\nso i had fun doing this remember to like\ncomment and subscribe and we'll see you\nback\nnext week hello folks and welcome back\nto the machine learning street talk\nyoutube channel\nuh with me tim scarf and my two\ncompadres\nyannick kilcher and connor shorten\nit it's been a really exciting week\nbecause open ai have released\nyet another paper microsoft have\nintroduced some new approaches for\ntraining these huge\nlanguage models as you guys have done\nvideos about this on your youtube\nchannels but\nlast week yannick you actually made a\nvideo on gpt3\nand how many views has it got 20 000 20\n23 000 as of now it's crazy\nthat is incredible is that your most\npopular video to date\nno no attention is all you need is the\nmost popular\nby far but it's been up since uh 2017 or\nso\nawesome so what did you guys think about\ngbt3\ni was surprised just by by\nthe sheer publicity of it because so\ngpt2\ni think a lot of the publicity was sort\nof manufactured by this fact that\nit was oh it's too dangerous to release\nand\nuh you remember all of this right it's\nit's like oh\nwe're concerned about the ethical you\nknow the bad applications if we release\nthis is so dangerous\nit seems that none of that is necessary\nwith gpt3 it just\nkind of took off by itself and\ni think the most surprising thing was\nthat\npeople probably thought that this trend\nthat you could just scale up\nwould have to end at some point and i\nthink gpt3\njust not only does it show that you can\npush it another\norder of magnitude but it kind of\nfortifies the trend\nthat we could probably do this another\ntwo or three orders of magnitude i i\nthink that's a lot of\nnot only you know it's not only this\norder of magnitude\nit it is such an indication for\nmore and more and more scaling and\nthat's still going to benefit\nus a lot is it showing signs of slowing\ndown\nyeah it's not that's the crazy part\nright it's it's it's not the truth it's\nalmost it's it's like pretty much a\ncomplete trajectory down like they have\nthis plot\nof the perplexity and it just goes down\nand you might\nthink that their you know their largest\nmodel kind of breaks the trend a bit but\nthen if you look at the training curve\nit's not\nit doesn't even look converged yet and\nthey haven't even even gotten through a\nfull epoch\non their big corpus yeah\nso thomas wolfe we did a video about his\npresentation on youtube and we still\nhaven't put it live so\neventually we'll put that live and he\ncited uh\nyeovil you know one of the israeli\nresearchers and he was talking about\nthis\nphase change in language models so they\nasked it questions like uh\nyears later than this year this year is\nlater than this year and they created\nthis kind of trick\nshowing how good the language model was\nat answering\nquestions and and of course it's just\nmemorization i think these\nthese folks were arguing that there's a\nphase change\npast a certain point where it's actually\nlearning to reason\nbut of course it doesn't even know that\na year is a number it's just it's just a\ntoken\nand if you train on enough data it will\nknow\nthat 1976 is after 1945.\nso what's it doing really memorization\nand generalization\ngenerally these language models they\njust memorize in their parameters but\nit's still\nuseful right even if it is memorizing it\nthat's kind of the most criticism i've\ngotten on my video is that i argue\nthat there is a you know that it's\nbasically just memorizing the training\ndata and i\nmight maybe i should have expressed that\nmore precisely what i mean\nbasically is that it is memorizing the\ntraining data in like a fuzzy way\nso it's mem it's memorizing little\nsnippets it's memorizing grammar\nconstructs and so on\nuh that's what i that's what i mean by\nmemorizing kind of\nin this distributed way but very much\ntied to the training data and there\nthereby it can like just use these\nmemorized\nyou know i take the grammar construct\nfrom this sentence and i know that these\ntwo\npeople often occur together and i know\nthis fact and so on\nso it's i think the memory this this\nsort of memorization\nwill lead to generalization because what\nyou essentially learn is how to\ninterpolate\nbetween the what you've seen in training\nyou're saying\ngeneralization but the thing that you\nsaid in your video\nwas that it's not reasoning it's\nmemorizing\nnow what you've just described is quite\ninteresting because you're saying okay\nit's not memorizing\nit's deconstructing and it's cleverly\ninterpolating between different\ninstances of reconstructed sentences\nand that will make it generalize at what\npoint does that become reasoning or will\nit never become reasoning\nwell it's philosophy right but but they\nhave some interesting experiments in the\npaper where they\nfor example show that the model can\nunscramble\na word so you you know you take the\nscram you give the\nsome examples right this scrambled word\nunscrambled scrambled word on scramble\nand they say look\nor at least that's the argument i don't\nknow their precise formulation but\nsomeone might say look the model has\nlearned what it means to\nunscramble a word and i'm\nall i'm saying is that if you have like\na perfect language model\nand or you condition on these things it\nwill just output\nwhatever word has the highest\nprobability given those word pieces\nright so it's absolutely not surprising\nto me\nand it doesn't mean that it has learned\nto reason a much better experiment\nwould be to learn to scramble a word\nright so so it's the same experiment if\nthe model has learned to reason\nacross the english language really\nunderstand what it means\nto scramble a word and can actually\ninfer this from the same context\nlearning\nit should be as good scrambling as\nunscrambling\nwords so are you arguing it should be\nsymmetric so if it can do one thing it\nshould be able to do this yes it should\nbe\nquite symmetric in that but so any\nasymmetry in scrambling versus\nunscrambling would come from the fact\nthat it is a very good english language\nmodel right because it can unscramble\nthere's a that's two effects that's for\none it understands what unscrambling\nmeans\nand the second effect is it just knows a\nlot of english words\nbut if you are going to scramble words\nnow\nyour knowledge of english doesn't help\nyou because the scrambled word\nis probably not an english word so you\nnow need to rely on the\nunderstanding of what scrambling means\nand therefore if the model has learned\nto reason i would expect\nit to perform scrambling as much as\nunscrambling but\nunfortunately they don't do this\nexperiment and i would be very\nvery surprised if the model could do the\nscrambling i think there's a reason why\nthe paper shows unscrambling yeah either\nway do you think it's ever seen\nsomething like that in the training data\nset\nit uses inevitably and scrambled how\ncould it have ever seen something like\nthat\nin the training data but all it needs to\nknow\nright is is that is that the word pieces\noften occur together it sees the\nscrambled version of inevitably\nand like the highest likelihood word\nafter that is the same word\nbut in absence of the same you can't\nproduce the same word because it's not a\nword\nso you'll produce the word in the\ncorrect order\ni i think that's much more what's going\non than it has like seen some website\nwhere you scramble and unscramble things\nthe mechanics of this is quite\ninteresting because we we spoke before\nwe had this philosophical discussion\nabout\nour frame of reference being a function\nof the convex hull of all the different\nwords that we have you know these words\nare\nshared placeholders to things that we\nthat we have a common understanding of\nso straight away the the frame of\nreference of these language models is\ntruncated by virtue of the fact that it\nonly has these word peace and beddings\nnow the other thing is i think it's a\ntruncation\nin the formulation of the language model\nit's just predicting the next word\nand it seems a little bit bizarre to me\nthat you're\ndoing all of these examples you know one\nshot and two shot learning so you're\nsaying here's an example of something\nnow continue\nso i'm thinking doesn't that limit the\namount of use cases\ni think it's because of the complexity\nof the decoding in the output space like\nif you only have to predict the next\ntoken\ndoesn't that give you a much less\ncomputationally complex output space\nthan having to reconstruct the entire\ninput with the\npredictions of the mass well i think the\ntransformers architecture has a\nquadratic layer-wise time complexity\nnow the computationally the auto\naggressive models are quite nice because\nthey can just keep predicting forever\nbecause you just put the\nthe previous prediction into the input\nof the next one and\nyou could just you could generate an\narticle of any size\nwhereas with a bert type model there are\nhuge limitations obviously they have 512\ninputs and\nbut it does allow you to do things that\nyou can't do with an auto aggressive\nmodel like for example you can do\nquestion answering or be it on a very\nsmall input\ni think because in invert all the\nweights\nneed to be able to contribute to all the\noutputs right because you have one\noutput\nfor each element in the sequence and in\ngpt3\nthe entire apparatus of weights can just\nbe focused on predicting that one\nnext word yeah on the subject of having\nscaled this up we have this auto\nregressive compared to the mass language\nmodel right\njust talking about the difference\nbetween say gbt and then burt and then\nwhat each of them are still good for\ni still think the specialized\narchitecture i think bert will always be\nbetter at\nsentence classification right so there's\ngoing to be uses for the different\nspecialized architecture\nbert has this bi-directional context and\ni think there is\nan assertion there that on things like\nquestion and answering you need\na bi-directional context one thing that\ni didn't get from watching\nyour video on gpt3 yannick was what was\nthe input size\nhow many tokens was it i think it was\n2048\nyeah exactly yeah oh that's huge yeah\nyeah so what do we think with the\nreformer\nand transformer xl do you think if we\nhave a longer context window\nsay twenty thousand forty eight how i'm\nnot a power two but just a random number\nbut like um does that make this a lot\nstronger\nwell the transformers xl was\nsignificantly better\nbecause of that reduction in context\nfragmentation\nit probably depends on the task right\nbut with respect to\nbi-directionality i'm not i'm not\nyeah okay i mean in that case bert is\nmore powerful because it's like an\nall-one\nencoder whereas as a gpt-2\nis this has this autoregressive property\nbut it is all also\nsort of bi-directional it is\nunidirectional in the way it produces\noutput right but it can still attend to\neverything\nand probably something like reformer or\nlong-former or whatever that they trade\noff\nthis ability to attend to anything in a\nprecise way\nby trade this off for then getting\nlonger input\nit probably very much depends on the\ntask and it might\njust work because if you think about it\nin natural language does really every\nsingle\nyou know position need to attend to\nevery other\nposition it's probably not probably\nyou'll have like some hierarchical\nstructure in your sentences\nand whatnot i think we simply\nhaven't found out yet how to make that\nconcrete\nto say oh you know you\nbecause basically language is a tree\nsort of\nbecause you can parse it in various ways\nbut then also sometimes it's really\nimportant what that one word is\nso there needs to be an organization\nwhere you mainly attend to this tree\nstructure but can also attend\nto the individual words but then you\ndon't want to attend to all the words\nbecause\nthat would be computationally hard\nand we're back to we can't learn sparse\nthings\nthe regressive models there is some kind\nof a filter right to to stop the model\nfrom cheating\nand to stop tokens from seeing things\nthat are ahead of them\nnow i don't think that exists in the\nbert type architecture\nyou just give it some text and maybe\nlike a noise from a masked version of\nthe same text\nnow architecturally something like\nquestion answering is reasonably\nstraightforward to do in in the\nbi-directional architecture\nbecause you can say you could do in-span\nquestion answering by\nputting in a span of text and and a\nquestion and saying\nthis is this is where the answer should\nbe and you could train it on that\ndownstream so you wouldn't be able to do\nthat on an auto aggressive model\nno no but you could actually do the same\nwith bird that you do with gpd3 and that\nyou simply input the thing into bird\njust take the very last\nlanguage model prediction and just shift\nall the things by one append that and\nso you could do the producing the same\nwith bird but you're absolutely right\nyou couldn't do this the gpt3 cannot\npoint to its own input like bird can\nthat's that's the power of bird yeah\nwell ask\ngpt3 in language what you want would\nwant to know\nright i think just one more interesting\nthing on the auto aggressive and the\nmass language modeling is like spam bur\nand how we're masking like\nspans instead of just tokens and i think\nthat's kind of interesting too how much\nknowledge can you pack into the\nparameters of a language model they're\nlooking at just\nasking t5 questions based on giving it\nno extra context\nand they show in appendix c the gains\nwhen you use this\nspan masking and you see a pretty huge\ngain with\nusing this kind of training where you\nmask out a span of tokens\nyeah it's is it is this even further\nthan the full word\nmasking or is this the same\nbecause so the reasoning i can\nunderstand the reasoning behind the full\nword mask it's where you say\ninstead of just masking word pieces we\nmask\nif we mask a word piece we mask all the\nword pieces that belong to the same word\nusually if you just mask one word piece\nthe other word pieces will give it away\nit should be so determined so you don't\nreally need to know to learn\nall of you know sentence structure and\ngrammar and so on you can just go\non the other word pieces and if you mask\nthe whole word\nyou can't do that anymore but is the\nspan bird is that the same i don't know\nyeah i don't know if they explicitly are\nsearching for that but that is\ninteresting\nyeah i thought it was where they just\nmasked off a span\nof words i can't remember how they\narrived at the length of the span but it\nmight be in a length of two or three or\nsomething\nokay yeah i think they call it like\ndegeneration how these language\ngenerators they'll repeat the word\nyou see that all the time do you think\nthat would be alleviated by having\ninstead of\npredict the next one you'd mask out five\nyou know and predict five at a time\nthese are all variations because bert is\na denoising auto encoder\nso it's super interesting when we\ncovered the t5 paper or thomas wolf's\nthing\nthey were talking about all these\ndifferent variations on bert and\nsometimes\nthey were masking off words sometimes\nthey were masking off a span of words\nsometimes they were\nreplacing words or just adding noise\nwhat they all seem to be doing is taking\nsomething that's on the manifold\nand pushing it off the manifold\nyeah so i think we're trying to get to\nthe bottom of like gpt versus bert right\nyeah well yeah but seems really\ninteresting because i'm\ni'm in industry at the moment and the\nnumber one thing that everyone wants to\ndo\nis knowledge mining so many companies\nhave\na wealth of information in unstructured\ndocuments\non a data lake or even inside office 365\nand what we need to be able to do is is\nuse these models to\nextract useful information out that i'm\ninterested in\nnow gpt-3 seems almost entirely useless\nfor that\ni can ask gpt3 who's the queen of\nengland and it seems to know a lot of\ncommon sense\nbut if i want to extract out some\nsemantic information from my documents\nabout something i care about\nit's useless well are you sure because\nif if you use something like bert it's\nstill it still requires like\ntraining data to fine-tune the thing\nright whereas in gpt-3 you could just\npotentially throw\nall of your customer documents all of\nyour\ncontracts all of the things in there and\nyou could just go ahead and\nsay think about what you need provide\nlike two three examples of the structure\nwhat you need\ndon't think these examples that they\nprovide need to be correct\nit would be so interesting another\nexperiment to do for them is to just\nsimply provide\nstructurally the same grammatically or\nyou know the same things but incorrect\nthings\nand just see what the model comes up i'm\npretty sure the model would still come\nup with\nwhat you're looking for because yeah\ni've made this argument in my video that\nwhat these models basically do is they\ngo to the\nsemantic fuzzily go to the training data\nand\nfilter the training data for whatever\nyou condition\non this in context learning it's\nbasically a fil you filter the entire\ntraining data for all the documents that\nmatch\nthat particular structure and then you\nyou just run your language model\nconditioned on that set of you just\ninterpolate those\nthings wouldn't it be awesome\nif as a company you just throw\neverything in there and then you could\ndo that\ni'm interested to know how brittle it is\nwith bert for example i get the\nimpression\nthat i could fine-tune it even this is\nthe thing with language models you\nyou train on a large corpus of text on\nthe internet it learns some common sense\nand then you fine-tune it on your own\ndomain\nmy intuition is that if i took a gpt\ntype model\nit wouldn't be as good maybe i'm wrong\nso let's say\ni take the gpt3 model and somehow i've\ngot loads and loads of compute that i\ncan access and i can\ni can start to continue to train it on\nmy own corpus of data at work\nand then i ask it questions because you\nwere saying that it filters\nit finds all of the things that that are\nsimilar to that and it cleverly\ninterpolates between them\nbut surely that depends on several\nfactors it depends on\nhow much stuff it's been trained on it\ndepends on how\nhomogeneous the data is it depends on\nwhether there was a critical mass of\nthings on on that particular topic that\nit learned\ndo you have any intuition on what those\nfactors are sure absolutely i mean\nthe the sheer number of data i think\nultimately the compressed size of the\ntraining corpus is over half\na terabyte is that correct if i remember\nfrom the\npaper and that's half a terabyte of text\nright we're used to\nbig data sets but this is text right it\nwas\ntex you can compress text to like\nminuscule things so this is a giant\namount\ni think it's like a trillion tokens or\nsomething\nand yeah chances that you will have this\nin your company\nare slim well isn't that a reason not to\nuse it then because if i start training\ni'm in i'm in banking\nand i want to learn about financial\ndefaults or\npeople's financial prospectus documents\nor something and i put it in there\nwould it be a drop in the ocean would it\nit wouldn't even move the needle\nprobably probably i mean it's very hard\nto say but\nprobably yes i was just hypothesizing if\nyou had if you could\nright then maybe it would be super\ninteresting\nto just have your company documents all\nin this thing\nthat just sort of interpolates so what\ndo you think about fine tuning it's very\nintuitive that we have this new data set\nwhen we fine-tune the parameters but\nso it doesn't make sense to think that\nyou could just put all your company\ndocuments in the context and you\nintuitively you'd want to fine-tune it\nbut maybe\nfine-tuning itself doesn't make as much\nsense as we think\nbecause for one it's going to take a ton\nof resources to do so\nhasn't gpt-3 already learned the best\nrepresentation of words\nyeah that's the question it has probably\nlearned the best representation of words\non the general internet or on wikipedia\nor things like this\nwe're going to see an era where right\nnow when you fine-tune bert what you do\nis you\ntake the pre-trained checkpoint that\ncomes from wikipedia\nand you fine-tune specifically on your\ntask right you fine-tune with the label\nbeing\nwhatever you want maybe we'll see an era\nwhere you take gpt\nthree or something and then fine-tune as\nlanguage model continue the language\nmodel but in you on your data\nand then you do this in context learning\nto actually answer your task\ni mean that that that is entirely\nentirely possible\none of the other drawbacks is if i ask\ngpt a question it will give me a kind of\nabstractive answer probably what i want\nin a business context is an extractive\nanswer\nyeah if i mean you can you can trick it\nyou you can\nthat's some of what they do in the paper\nis where because\nif you for example do sentiment\nclassification you just\nyou don't want the word you don't want\nany word you just want the word either\npositive or negative\nright so you can actually restrict the\nbeam search of the output to just those\nwords so you can just ask it\nwhich one of those is more probable so\nyou can\nyou can whenever you can phrase your\nquestion as\nlet's say a multiple choice between\nbunch of answers you can just ask it\nwhich one of these answers is more\nprobable\nand then it will sort of yeah yeah\nanother paper that i really like is\npattern exploiting training\nwhere they it's the same idea you you\ntake some expression that you can append\nto your task that will\ngive a better use of the language model\nfor answering the question\nso patterns like with the there's like a\nyelp review and then you would\nadd it was mask to get a label from the\nlanguage model so there's a yeah\nyes this idea of coming up with these\nprompts\nbut then the problem is how do we have\nto manually find these prompts now they\nin the pattern exploding training they\ntry this automatic verbalizer search\nwhere they're trying to find the\npatterns automatically but\nit still doesn't look like they have any\nway of taking the human out of the loop\nwith what kind of\nadditional context you give to it to\ngive it a better answer\nabstractively that goes into two\ndirections of these math examples\nwhich are quite prominent in the paper i\nfeel and\nwhat a lot of people talk about and i\ndon't know what are\nyou views on on what's happening right\nthere you already said at the beginning\ntim like it's it's these are strings\nright you\nwould somehow have to learn the decimal\nsystem\nit's so weird and but yet what what are\nyour thoughts on this\non the math of gpt being able to do math\ni don't think it can i completely agree\nwith your assessment\nbut but it's not as black and white as\nthat\nit's not just well maybe it is just\nmemorization but there is a nuance to it\nit is learning some kind of internal\nstructure\nwe've said before that in days gone past\nwe would manually create these knowledge\ngraphs\nand inside our own minds maybe there's\ntwo systems that we've spoken about\nsystem one and system two\nwhen we verbalize our knowledge there is\na certain structure to it that we're\ncapturing\nto what extent is the transformers model\nreplicating that i'm not sure\nyeah i mean i've that that's i've also\ngotten a lot of pushback on that\nand i'm i'm like already half convinced\nthat there's more happening but still\nnot\nfully convinced but what what i found a\ngood\na good comment was that humans for\nexample also\ndo a lot of this math as memorization\nlike you you\nremember multiplication tables up to you\nknow\n10 10 by 10. it's this is you don't\nperform the math you just know it from\nstrings basically\nand all of these of these like low-level\nadditions you just\nyou know by heart and it's like the\nargument is that\nthat's just what gpt is is doing\nbasically learns by heart these\nlow-level\nmath and that's why it no longer works\nif you then go multiply three\ndigit numbers yeah like if if you look\nat the chart it\nit really dies that it works for like\ntwo digit addition and subtraction\nbut this is basically randomly get you\nknow you only have 10 possible outputs\nso this is randomly guessing it\nyeah you covered this in your thing\njanet\nso surely it's just memorizing because\nif it was reasoning\nthere wouldn't be such a precipitous\nchange exactly\nwell that was my point basically if you\nthink about addition and subtraction\nhow many websites are there and we're\nwe're looking at a crawl of the web\nright how many websites are there where\nthere's just like a giant table\nand one of the columns is surely going\nto be the sum of some other columns\nright so so by pure language modeling\nyou\nwill learn to add and subtract and and i\nthink\nthat's why addition and subtraction are\nso so high on there\nwhereas you have you'd be much more\nhard-pressed to find a table where\nthree-digit multiplication is intrinsic\nto the structure\ni'm not talking about websites where\nsomeone specifically constructs a math\ntable\nbut it's like oh here are the number of\npeople that live in\nuh west west side of town and here are\nthe number of people that live in the\neast side of town and then there's a\ncolumn\ntotal number of people right and it's\nlike so that's how you learn\ni love your conception though you can\nsay that humans are dumb\nright because\ni think that we do memorize a lot of\nknowledge that if you look at the way\nthat humans learn\nwe are really efficient about our use of\nthinking it's\nit's very taxing for us to think so we\nquite often\nlook at the decisions of others to guide\nour own behavior\nand there are lots of psychological\nbiases like social proof that do that so\na lot of the time we're not thinking and\nalso\nwith this precipitous change in\nabilities we we think of human\nintelligence as being really general\nbut it isn't there's lots of things that\nwe just can't do like\nsolving traveling salesman problem we\ncan solve it up to a reasonable number\nof cities and then we suddenly are\nreally crap at it or if we\ntry and reverse the problem and we solve\nthe reverse which is finding the longest\npossible path\nwe're really bad at it we're good in 3d\nwe're really bad in 4d\nso it's it's not so different\nyeah maybe yeah but i mean the\nthe quite the the big question i think\nthat people have is\nis is there an explicit like\nwhat we said at the beginning is there\nan explicit notion of reasoning going on\ninside gpt3 like or or is that\ncompletely absent well it does it does\ncome back to this system one versus\nsystem two because i think\ngpt3 is system one it's exactly the same\nas you were describing we do many things\nwithout needing to think about it\nconsciously because our intelligence has\nbaked it\ninto a skill program and that program\nalmost runs deterministically just like\nin gpt3 it's just a deterministic\nprogram\none which of course we couldn't\nverbalize it's been baked by this\nback propagation program and it's\ninformed by various inductive priors and\nand so on and experience that's been\ngiven but\nit's now a fully baked system one\nprogram\nit's not reasoning yeah i mean i\ni i that's actually a good\ncharacterization is that\nif you if you characterize gpd3 as like\nreasoning or intelligent basically\nyou're saying that\nsystem two doesn't exist or something\nlike that\nbecause i completely agree it's not it's\nnot\nprobably not doing that well what do you\nthink about this\nlike connectionist idea of kind of like\nthat we can't with that we store all\nlike maybe it is like because\nto me it seems like the attention on the\nintermediate weights could be a way of\nreasoning kind of\nso i think the fact that this can do the\ntrend the zero shot transfer\nkind of points in this direction that\nmaybe it is just system one\na little bit yeah that's a good point\nlike is is the is this attention\ncomputation is\nthat a form of reasoning like is\nthat's i don't i don't because it's\ntransforming the representations\nlike in a meaningful way it's true\nbut is it possible to do reasoning in\nthe context of a neural network\ni think the transformation yeah well i\nmean to say\nanother way human beings create you know\nimperative programming code\nand the constructs in programming code\nare things like looping semantics and\nsymbols and it's it's a logical\none-to-one correspondence with the\nverbalization\nof how you do something it's an\nalgorithm\nwhat you do in a deep neural network is\ncompletely different\nwell we we know that these things are\nlike can compute arbitrary\nfunctions so from like a purely powerful\npower analysis they should be able to do\nanything but\ni think here's the notion of where\npeople think\nthat something like a reasoning should\nhappen\nlike it happens in your head like you\nhave to sit down and go like\nokay logical step one logical step two\nlogical step three right\nso we can't we almost can't imagine a\nsystem\nthat it does reasoning that doesn't do\nthat we think like some there's got to\nbe like a module in there to retrieve a\nmemory\nand then a module to update that and\nthen a forward simulation or something\nmaybe you maybe the this is just a\nhuman-centric view and the\nreasoning like the function that is\nactually being computed the exact\nfunction not an approximation\nthe exact function that's being computed\ncan also be represented by simply\nforward pass through attention layers\nit's entirely possible i don't know but\nthen there's a difference between\nrepresenting it because i think system\ntwo is\nthe intelligence that creates the system\none skill program\nso when you have a new situation\nwhich is similar to something that you\nhaven't seen before and you need to\ncreate a new skill program\nthat's when you need the reasoning yeah\nit's debatable how much you can do that\nor how much\nyou simply go more abstract like if you\nsee\nthere's a situation you've never seen\nbefore i'm sure you've seen it before in\nsome abstract way\nlike okay you've never been to london\nbut you have been to a new city\nbefore right you've never you've never\nbeen\nin a in in i don't know in in the\nphilippines but i have been in a country\nbefore that where my\nthe culture is not mine and so i kind of\nknow i have to\nadapt in that way you know the question\nis can you\nreally generalize\nto situations that you haven't seen\nbefore ever\nor is it just that you can generalize\nin to situations and transfer their\nabstract properties\nwell in that situation you would have a\ntaxonomy of skill programs which you\nhave learned elsewhere\nand they would all have a degree of\ngeneralizability\nand you could fall back on those skill\nprograms\nand your system too would learn to\nrecombine them\nas you as you acted in that environment\nyeah i have another kind of funny\nexample so we were drive we were headed\nto trader joe's and as is common now you\nsee this long line outside of trader\njoe's because they're you know filtering\nit to keep it\nempty in there so we come up and there's\nno line and so\nquickly we're like what's happened why\nis there no line outside of trader joe's\nbut it's not like we've never seen no\nline outside of trader joe's before\nit's not like it's lined up with aliens\nor the building is on fire\nso like we can be like whoa this is out\nof distribution but i've seen this\nbefore so it's not like it's blown my\nmind\nbut then it's like so with our neural\nnetworks when they see some crazy added\ndistribution thing and\nit blows their mind right like gpt3\ncould at least maybe\nstart trying to say something about it\nlike our classifiers they just give you\nthis\nawful prediction but the abstractive\ngeneration models they can at least\nstart trying to tell you what i think\ni'm seeing right like\nmoving towards that if you want let's\nlet's play with that give let's let's\ncome up with an with an example\nof something that would be completely\nout of distribution what would gpt3 do\nwe had this i think when we talked about\nbenjo's thing in that\nis human consciousness is it generative\nor discriminative because\nin in all of these models that we have\nat least\nit's sort of they assume that their\ninput comes from the same\ndistribution as the training\ndistribution so\nthey will have a very hard time to\nassess that the input that they're\ngetting\nis very improbable right now gpt3 could\nat least\nsay something like that what are getting\nat this context\nthat's improbable so yeah it makes it\nmakes a case that gpt3 could actually be\nbe like whoa this context isn't\nsomething i expect right but then\nagain if you give it a novel task it is\nalso\nsomething that it doesn't necessarily\nexpect so i\ni don't know but it's a good point that\ngpd3 might actually\nbe able to recognize out of distribution\nwell my intuition is if you give it\nan example which is well off the\nmanifold\nand presumably there there are a\ncombinatorially many\npermutation there are many things way\noff the manifold and there are things on\nthe manifold\nif you gave it something off the\nmanifold it would just\nnot know what to do what would it do\njust just generate garbage\nmost likely it's the same as humans do\nyou just put them in a situation where\nthey're completely lost\nwhat do they do either they like freeze\nor they run or they cry i mean it's\nbecause there's a difference because\nhumans\nfirst of all we i would imagine we have\na taxonomy of skill programs but our\nskill\nprograms generalize so much more whereas\nthe\ndeep learning manifold that we've\nlearned you know we had that energy\nbased surface that we were talking about\nit's it's very very small if you give it\nsomething just slightly off the manifold\nyou're lost what do you think about\nlike so this is one paper you can see my\nscreen\nwhere they're trying to like they mix in\nthe data with like\nexplain the task would be like explain\nnli premise and then so they want you to\nnot only give the answer but then\nexplain this do you think this could be\na promising direction towards like added\ndistribution like\nexplanation and then you can get a sense\nof it\nif you said something like explain and\nthen you give something that's\ncompletely\nunexplainable or yeah but\nmaybe it's like a way of getting a sense\nof\nhow they're thinking maybe i mean that\nwould still that would then require\nthat the human interpret what the model\nsays\nright it's because the model would just\noutput the words i\ncannot explain this or something like\nthat\ni'm not sure if that's the same because\nfor the model that would still be\nin distribution because the the\nfact that it produces that still means\nit can actually answer the question\nbut i guess it's a different question\nfrom asking what happens if the\ninput itself is like completely\nout of distribution for the model and\nthen i would argue it's just going to\nfall back to\nbecause the pro the conditioning\ninformation is so weak\nbecause there's no signal it's just\ngoing to fall back on its prior and\nthat's the english language\nso it's probably just going to produce a\nprobable sentence\nlike i don't know i think\nthese when when google introduced this i\nheard a public talk about this when they\nintroduced this\nautorespond not autoresponder quick\nresponse for gmail\nright these three things you can click\none by one and they\nthey tried i think they tried generative\nfirst and so on but then\nit turned out that it would always start\nwith sorry\nfor the late reply\n[Laughter]\nit was like whatever whatever input it's\nlike sorry for the late response\nyes so that's that's maybe when the the\nprior is too overwhelming and and\nin a case where the input just doesn't\nmake sense it's just gonna fall back to\nthe prior\nit's really interesting that you bring\nthat up because that would have been my\nexpectation surely these models would\njust do something like that all the time\nit's surprising to me that when you look\nat the gpt3 paper\nit just gives incredible results\nyes apparently with no hacking or\ntweaking or massaging\nis that surprising to you very\nwell in in some somewhat it's it's so\nsurprising\nlike it's not surprising if you just do\nlanguage modeling because all it has\nis that context right and if it wants to\ndo better than\nrandom it better going to start paying\nattention to that context\nbecause that's the only way it gets a\nbetter like a lower loss\nbut that then what you're saying what\nthat it then actually generalizes to\ngive other contexts and it does\nsomething useful not just falls back\nit's\nit would be interesting to\nexperimentally see\nhow far you can push it how far you can\npush garbage into that context until it\nactually\nbreaks until it just always gets back to\nthe most probable sentence\nis it possible to conflate an auto\naggressive model and\nan encoded you know like a denoising\nauto encoder type model\nor to bro to broaden the question are we\nbarking up the right tree\ndo you think this type of architecture\nis what we should be looking at well\ni guess it depends on what you want to\ndo right this is this is a this is an\nenormously good language model because\nit can throw all its power into\npredicting the next word and not like\nbert you know has to like predict all\nthe words at the same time\nand whether the two sentences are follow\none another\nuh this can just this can be like full\nblast\nnext word is the only thing that matters\ni suppose my intuition is that a\nbi-directional model would learn\nmore about the structure of the language\nbut it's easier to train an\nauto-aggressive model\nis that fair maybe at like scale it\ndoesn't matter anymore like\ni mean like you know once you scale up\nbi-directional\nyou know unidirectional it's all okay\nnow that we've trained on\ntons of data with bur\nthere is this horrible quadratic layer\nwise time complexity\nso they had to truncate the input size\nto 512.\nif that restriction wasn't there i would\ni would imagine\nif you could train a much longer input\nsize in a bi-directional model surely it\nwould give much better results than an\nauto aggressive model\nwell the the the the other aggressive\none has the same\ncomplexity like it's it's also a\ntransformer it has the quadratic\ncomplexity just as much\nand but it has a much longer impact so\nif you think of the like\noriginal transformer of the original\nattention is all you need paper\nyou have two inputs right you have the\nencoding\npart and the decoding part and those\nthose have internal attention\nso in in that case the encode if we\ntranslate to this to our thing the\nencoding part would be\nthis context whatever you give us this\nin context\nand the decoding part would be whatever\nyou have output\nso far now you can construct this in\nvery different ways\nbut at best you're going to to basically\nhave your the size of the thing so in\nwhere in bert\neverything could attend to everything in\nevery single layer\nnow you have attention internally but\nthat only like\nthat gets you like to d square half or\nor d hat the\nhalf squared it's not like you're\nyou're significantly reducing the\ncomplexity of the transformer here\nso it has it has the same limitations\nand the reason it can take\n2 000 tokens as input is just because\nmicrosoft has built like\nthis big of a computer that's the reason\nare there any trade-offs to having a\nlonger input size\nis longer at some point worse\nit's a good question because with a\nmodels like lstms\nthe answer was definitely yes there is a\npoint at which you\nhave so much information that you know\nyou can only encode\nin this much hidden state so at some\npoint you overload\ni still think yes and transformers at\nlike if you throw\ninformation and information in there\nespecially correlated information\nthat is going to at some point be\ndetrimental but\nbecause something like a cnn for example\nhas a receptive field\nwhich means you can feed in as much\ninformation as you want and it will only\nuse this receptive field\nis it a similar concept with\ntransformers that even if you fed in a\nhuge amount of information\nbecause of the attention mechanism it\nwould only pay attention to\nthe substructures and the symmetries and\nand it would ignore\neverything else so it kind of wouldn't\nmatter if you fed it more information\nyes possibly i mean as a like the\nproblem i see is really when you\ninput correlated information and you\nmight you might know this from\nsimply doing linear regressions if you\nhave correlated features or logistic\nregressions\nit's horrible because you could\ntechnically if you have two very\ncorrelated features you could\ntechnically predict the output from one\nor the other so and then through\ndue to noise in you know your stochastic\nprocedure\nin one step is going to push one up and\nthe other one down\nand then the other step it's going to do\nthe exact opposite so they're like\nit's like ah should i pay attention to\nthis no to that no to this no to that\nbut this works well\nbut that works well as well and you'll\njust end up with like a very fuzzy very\nconfused model\num so i think there's definitely a point\nwhere too much\ninformation conditioning becomes\ndetrimental but\nwell i think they even show that in the\npaper where more examples doesn't help\nlike with the demonstrations and\nand also i think that's the reason why\nthey also\nlimit the training corpus because if you\nyou have to filter out\nthe garbage samples right and and that's\nsort of the same thing where you want\nhigh quality\ndata where you can actually predict\nsomething\nbecause another observation is just like\nin images\ncnn's had this inducted prior that\npresupposed that\nlocal connectivity between pixels was\nimportant\nand global connectivity was not\nimportant and\nwith documents for example if you look\non average how long is a document it's\nprobably not that big\nso at some point if you had a huge\nreceptive field you would be\nwasting all of those potential\nattention points between tokens because\nno document is that long\nmaybe there are some documents out there\nthat are huge but maybe if you had an\ninfinite receptive field and documents\ncould attend between each other when\nthey're talking about similar things\ni guess i'm just trying to reason i'm\ntrying i'm trying to play with this a\nlittle bit because at some point if you\ngo past the average document\nlength you would be wasting that\nrepresentational capacity\nbut maybe if you just infinitely\nincreased it it would be a good thing\nagain\nbecause you would encounter other things\nthat you could attend to in a useful way\nwell well here's the point if\ni guess if you had much longer receptive\nfield you could input more than\none document but the question is\nwhich documents do you input together\nbecause you can't just\ntake two random ones because what you'll\nteach the model is that\nthe second one follows the first one\nright that's\nit it thinks like oh this is a sequence\nthat happens in nature\nand if you just input two like random\nthings after one another it's\nit's also more confused probably and\nlike keep in mind these transformers\nthey don't have an explicit\nactually they don't have an explicit\nconstraint\non the input length you could actually\ntechnically input\nany in the same transformer so invert\nright now you can\nyou could input uh sequence of length 10\nor a sequence of length 10 000. now the\nfirst problem with the 10\n000 sequence length is that the position\nencodings\nhave only been trained up to 512.\nbut that's a that's like a minor point\nthe major point is that your gpu is\ngoing to explode\nbut in the same model that the\ntransformer has no notion of the\nsequence length it is it is a set comp\nset computation algorithm that\njust sees elements of the set\nyeah yeah that's exactly right so that\nthese positional encodings are kind of\nlike\nthese sincoidal functions and that's the\nonly way it knows that one thing was\nbefore another thing\nso you were right you can only put in\none document at a time because\nyou can't really impute an ordinal\nor a relationship between documents i\nmean maybe you could you could invent it\nyou could say well it comes from the\nwikipedia bucket and\nit was this one was created on a\nwednesday and this one on thursday but\nbut the interesting thinking point\nthough is\nwhat if you could get the\nlayers of a transformer model to attend\nto\nthe layers of another document did you\nsee what i mean so\nthen here's another similar document and\ni want this to be able to attend to that\ndocument\nand i could dynamically pull the other\ndocument out because i've encoded it\nand i could do some locally sensitive\nhashing or something would that be a\ngood way to go\nwell that's it sounds like awesome if\nyou could if you could do if you could\nlike\nforward prop and then reach because i've\ni've\nbasically come up with and this has been\ndone so people in\nmy very helpful like my comments are the\nmost helpful\nmy youtube comments are like way more\nhelpful than any reviews\nof my papers or any any of that so\nso people have pointed out this has been\ndone where i said\nsurely you know if the model is is is\nlearning to interpolate the training\ndates in a fuzzy way\nthen you should be able for each output\nyou generate\nto pinpoint back to the like here are\nthe\nfive training examples that led to me\nmaking this decision right now maybe we\ncan spin this a bit further\nand say okay i could do that i could\nlike forward prop and then i could\nretrieve the training documents that are\nmost relevant\nput those in the context and\nyou know be able to explicitly attend to\nthem\nnot only through the weights but it it's\nit's sort of like\nright now we're trying to distill all\nthis knowledge and\nwhat i'm arguing is mem fuzzy\nmemorization of the training data\ninto the weights of the transformer but\nif you were to build an architecture\nlike by programming what you would have\nis like a database\nwhere you could retrieve things\nyou know and then attend to them so\nwe're we're basically trying to build\nthe logic\nand the the knowledge itself into these\nweight connections where i think what's\nmissing here\nare explicit memory modules where\nyou store and retrieve information and\nhave the\ntransformer itself do the computation on\nthat information\nrather than both so that be like\nmatching the query with the nearest\nneighbor in the training data or would\nit be like\ni have this this path through my\ntransformer\nmade the most influence on this\nprediction so i'm gonna see like\nwhat had caused the biggest like\nmagnitude gradient change during the\ntraining\nlike which mini batch i don't i don't i\nhave no clue like i\ni'm just think like this but what you're\nsaying tim is\nis like super interesting if if that\nwere i feel that's\nsort of out of reach right now out of\nthe computational\nright capabilities but it's like super\nfun to think about this could be\nyeah the next step in these transformers\ni'd love to explore this because you\nsaid\ni want to know here's the output i got\nwhich examples that i trained on have\nled to this output\nnow in vision models you did that thing\non chris ola's\nfeature visualization and you can say\nwell here's an output neuron and i'm\ngoing to\nsolve an optimization problem to\ngenerate the input which maximally\nactivates that neuron and that's quite a\ngood way of going backwards in vision\narchitectures\nand i'm not sure how your setting would\nwork here on the transformers model\nwould you find the intermediate\nrepresentations for all the different\ninputs and then you would trace back\nfrom the output and you would then\ndo a vector search and you would find\nthe ones that\nthat were most similar because you were\ntalking about something quite\ninteresting\nyou said basically it's a memorization\nmachine and what if we actually took\nthis to the next level and we created\nsome symbolic reasoning and we\nexplicitly\nmade it into a memorization machine so\nwe took in\na representation we we stored it in\nmemory\nwe turned it into a computer program\nessentially a kind of hybrid\nwould that be more interesting and would\nit give us more transparency in the way\nthat you want\nyeah i mean that's so that's one of the\none of the\nthe next thoughts you can spin from this\nexactly a direction like this basically\nwhat my\nmy argument was that if i train whatever\na logistic regression or a\none of the smaller vision models what\nthey learn is\nmap features to outputs in the in the\nway they have to they have to abstract\nbut as we go larger and larger and\nlarger amount of weights\nand and just just from you know from the\ntrend of\ndoing larger models from the output of\nwhat these models can produce\ni strongly feel that they are more and\nmore\nactually memorizing the training data in\nin the sort of fuzzy way i described\nand where as opposed to extracting\nfeatures and and predicting based on\nthose so\ni maybe the most basic imagination\nwould be something like during training\nlike build a reverse index from weights\nto training examples where you said oh i\njust passed this training example\nand it influenced this weight here a lot\nright\nnow index just from this weight index\nback and maybe you could update that\nduring\ntraining you see other samples index\nupdate that weight even more\nyou can like kick the first one out\nagain\nbut then you basically have this reverse\nindex and then if you forward prop\nyour test sample or your context your\nquestion\nyou would simply observe maybe the\nthe self gradient the gradient on its\nown output or you would observe like the\nforward prop signal\nand see ah these connections here\nthey're really important\nthey're activated right now let's go\nback in the reverse index\nand see which training examples led to\nand i feel this could be\nfirst of all explainability wise\nthat that could be massive and again\nthis has been done like people tell me\nthis has been done i haven't read it but\nuh if you're interested check out work\nthat has been done it could be\nexplainability-wise super interesting\nbecause you'll see ah okay\nhere's what the model you know basically\nyou can\nit's it's explained by example it's like\noh this output yeah you know here are\nthe things i i\nbase it on and second it could actually\nbe the next sort of\nsearch engine right if you like the\nfuzzy search engine through your\ndocuments you throw your documents in\nthere\nand then you know you just type\nsomething\nand it will just pop up oh here are\nthings that i found on your computer\nthat are relevant you've got a continuum\non one side of the continuum is it's\njust memorizing stuff\nand in your gbt3 video you gave an\nexample\nof an utterance and and it was very very\nclose to an article that you could find\non google\nwhat if it's more sophisticated what if\nit\nis abstractly generating tech and\nbeing far more nuanced and intelligent\nthan we realize\nhow would you measure that that's a hard\nquestion\ni i i i don't know\npro probably you would measure it by\nyeah how abstractly it is interpolating\nthe training data because ultimately\nif if you see that it is really working\non the level of the idea and the\nabstract concepts\nit's hard to devise a test for this\nbut i think in the outputs of gpt3 right\nnow you can\nclearly see in these news articles for\nexample that it has\nprobably just taken a bunch of these and\npushed them together with grammatical\nstructures that it has learned\nand yeah it's very hard to test\nfor that i i wouldn't know yeah because\nall of our metrics for this\nare rubbish yes and\nit makes me think what if we went back\nto first principles and and had a much\nsimpler language model and we generated\nour own\ncorpus using some grammar and we could\ncome up with some kind of a level of\nabstractive text generation and we could\nunderstand\nwhich factors influenced it i just\nsuppose\nat the moment if we're just using this\nhuge corpus and this huge language model\nit's impossible for us to because it\nwould just generate anything\nand it and it would be really good and\nit's impossible for us to reason about\nor measure\nthe abstractness or intelligence of that\ngeneration\nsure but then i mean as i i\nagree but then if you if you do this\nkind of thing on your abstract task\nwhere you know how it's created then the\nquestion is\nyou know so what how like what does that\nhave to do with the real world\nyeah it's the eternal question between\npeople that work on toy problems\nthat they understand and people that\nwork on the real world\nlike we don't know what the real world\nis yeah i think this is a great example\nof this where they have\nthey construct this toy data set where\nit's charlie is big charlie is blue and\nthen you learn how to combine the rules\nand then\nso then they start to scale this simple\ntoy thing up into more complex language\nand then they try to take it to the\nquestion generation\nbut i do agree with tim i think we need\nbetter generalization probes i mean like\nbetter tests even if it's like a toy\ndata set that can you know layer\nlevel up its complexity to you know\nmatch something like squad or\na more complex data set yeah\nyeah i just have no intuition how we\nwould even start because one\none thing we could do let's assume that\nall of our training data was\nperfectly spaced on the language\nmanifold and\nwe could start by inputting an article\nof course what it's going to do is\ncomplete that article\nand then we did a mixture of two\narticles that were next to each other on\nthe manifold and presumably it would\ninterpolate halfway between those two\narticles and do you see what i mean\nwe need to come up with some framework\nof reasoning about its behavior and it\nfeels to me that we're a million miles\naway from doing that yeah\nabsolutely we have like nearest neighbor\nprobes\nkind of that you know i think that's the\nmost popular visualization is we'd love\nto go see what's the most similar\ndocument in our\nin our representation or i think with\nimages is a little easier to think about\nyou know which dog image is the most\nsimilar to this but then you have this\nentanglement between\nthe k nearest neighbors it's on top of\nthis representation i think\nyeah and interesting have you could you\nthis radioactive data\ni think i've seen this before but it\nthat seems like very much\nsomething just from the title something\nthat would go into the direction of\nyeah i thought this paper was really\ninteresting it's it's like so you take\nyour like\nuh batch your sgd batch and you're gonna\nlike\nput some signature in the data such that\nit comes out\ninto the model like you put these little\nencodings\nlike you know just like kind of how we\nput these little encryptions on our\nimages like a watermark\nyou know that doesn't show so maybe like\ni was thinking about\nlistening to your idea that maybe we\ncould like sign our mini batches of text\ndata\nin the token tokenizations\nmaybe something like that yeah that's\nsmart i mean\nthis this here is probably also for if\nyou want to detect if\nyour personal data was used to train a\nparticular model or something like this\ni mean that's\nyeah but it is yeah it could be a way to\nfind your way back to the original\nthe training examples that gave rise to\nsomething\nyeah that's quite smart yeah it's a bit\nlike adding a watermark\nyes yeah my intuition is though even in\na cnn architecture but in the\ntransformers architecture as well that\nthey're very clever at taking shortcuts\nand your watermark would just represent\na different path through the network it\nwouldn't necessarily\nhelp you locate the thing that you're\ninterested in\nit would just be a different thing in\nthe network and if you use the watermark\nin a lot of different places it would be\nquite a\nwell well-trodden path through the\nnetwork and it wouldn't\nif it was on a picture of a mountain or\nan article about a mountain\nthere wouldn't be a pool between that\narticle and the watermark you created if\nthat makes sense\ni imagine there's a lot of overlap\nbetween like if you can easily find an\nadversarial example you can also easily\nencode the watermark so\nyou would have to trade off like it's\ngonna be you can't you can have like\nadversarial robustness and then also be\nable to put these watermarks through the\nnetwork right\nyeah i mean the watermark is gonna if if\nif your goal is really that\nif someone trains on this data i'm going\nto spot this in the output\nand i guess the the watermark is almost\nequal to an adversarial example\nbecause what you want to do is kind of\nalign the features such that\nyou know the output is very\nvery weird i haven't read that\nparticular paper but\ni'm pretty sure the mechanisms are going\nto be exactly the same as adversarial\nexamples so i was going to ask you\nyannick about the other comments you got\nbecause i noticed on your chalet\nvideo i was surprised to see some people\nuh laying into chile and chile is my\nspiritual\ndeity so i was very pissed off about\nthat it turns out that there are some\nother people that have different\nopinions about measuring intelligence uh\nwho would have thought it but\nwhat comments did you get about the gpt3\npaper\nwhat on on the charlay thing no\nnothing to do with shirley just what\ncomments on the gpt3 paper\nwell i was just saying i i i was\nexasperated\ni read your comments yeah because\nchalet i would just like to say\nofficially that chalet\nis the is the man he is the one of the\npillars of the deep learning community\nand if anyone wants to have a go at\nchalet you have to come through me first\nthat's very very normal i know i mean\ni've got to do my bit\nfor for france and for sure but we'll\nwe'll get back to chile next week i\ndon't\nwe should keep our powder dry on chili\nwe are we are going to\ndo chalet justice next week but in the\nmeantime\nyeah what comments did you get on gpt3 i\nguess most\nmost people with like substantial\ncomments were\nwere talking about these there's a\nreasoning ability\neither agreeing or disagreeing of what\nit\nyou know is it doing reasoning or not\nand specifically\nlike the math part you know is you know\nis that\nis it feasible that it learns this as\na language modeling task or not yeah\nit's just a lot of a lot of opinions and\nvery i mean i've i've learned a lot just\nby\nreading these comments so that was\npretty that was pretty cool\ni thought i think i got i got no zero\ncomments on this\nnews thing right because what i so\nif people haven't seen this what i did\nwas basically gpt\nthree was asked to complete this news\narticle about\ni think the mormon church or something\nnow ordaining lgbtq ministers and then\nthere was a split in the church and so\non\nso it was asked to complete this news\narticle from like just the title\nand like the first paragraph or so\nand i could basically show that if\ni could find because they say well we've\nde-duplicated the training data\nsuch that this news article wasn't\nin the training data that we got the\ntitle and subtitle from but i was\nbasically able to show that\nlike a lot of other i think i've shown\none but\ni found like a bunch of news article\nabout the same thing or books\nthat that quoted not quoted but not\nverbatim but\nbasically books that had as a source\nthat news article that were using like\nextremely similar language\nto describe like same sentence structure\na couple of words\nswitched where i feel the the\ndeduplication efforts\nof so if if they show look you can\nproduce this news article\ni'm not super convinced because i think\nit's kind of seen almost that news\narticle before\nso you know there's there's a couple of\nthings there though one thing that\ni think is is good is that was quite a\nspecific event\ni would imagine that there were not many\nnews articles talking about\nthat event so i'm impressed that on a\nhuge language model\nbecause you know we were talking about\nthe specificity\nand diversity and scale needed to\nto move the needle on a large language\nmodel because i'm in the industry and i\nwanted to learn about financial defaults\nor something i'm in banking\nand we asserted that my corpus wouldn't\npush the needle\nthis seems to suggest to me that you can\npush the needle\nwith a small number of training examples\npresumably it's learning a taxonomy so\nthere are lots of concepts in there it\nknows about what lgbt is it knows about\nchurches it knows it\nyou know so it's all of these things are\nactivating in the network and\nthey're being kind of combined together\nand\nit's learned that and it can retrieve it\ngiven one example that's\nnot i'm not sure that it learns what\nthese concepts are\ni mean the the other extreme would be to\nsimply say here's the title\nthis is the training example that or\nthese are the three training examples\nthat match that title now just\ninterpolate them\nwhatever they are and this is actually i\ncan make a credible claim that that's\nhappening because one of these articles\nwho i found was from google books and\ngoogle books is one of the corpuses that\nthey have\nused in a in a prom like in a dominant\nfashion they\nuse like different weights to different\ncorp\ncorpora corporacies corporate\nokay and and google books was pretty\nweighted up so\nit's arguably it has seen that\nparticular example multiple\ntimes and given that it has so much\nweight\nright it's possible that it has used a\nbunch of these weights specifically for\nthat or similar\nsimilar articles and doesn't really know\nwhat lgbtq\ndoesn't really know what mormon means in\nin\nin that sense right so yeah that's kind\nof the two extremes and\ni have not gotten many comments about\nthis which\ni interpreted as no one's no one wants\nto debate me about it\nthere's also the observation that you\nknow we were saying that\nmost humans are um like drones they\ndon't really think for themselves\nfor example if i was to go out and write\nan article on cnns right now\nwhat would i do i would go and read a\nbook on cnns and i would go and read\nsome articles on cnns and i and i would\ndo exactly what gbt3 is doing\nso you can argue that there is an agency\nand\nautonomy has limits yes but it's not the\nsame\nas as a news reporter that you know\nsees things happening and then\nsynthesizes\nyou know this news story right it's not\nbecause there are als\nall there are already so many texts\nabout cnns\nthere's nothing really left to do about\ninterpolating them\nbut a new a journalist that you know you\ngive a title and you say please or even\nif you're if you're\nallowed to invent an essay she says\nhere's a title of an essay\nright you are going to come up with\nsomething much\nless interpolated than gpt3\ni would argue you're going to come up\nwith something that's\nmore interpolated maybe in the concept\nspace you're like i'm going to tell a\nstory it has a beginning there's a\nconflict and then conflict gets resolved\nbut i'm not yeah i love the way you\narticulated that\nbecause if i'm writing about cnns for\nexample\nyou know we were talking about the\nconvex whole and if it's\nif it's a topic which has been maturing\nfor a significant period of time\nthat convex hole will solidify\nas you say if a new event happens\ntomorrow and i started blogging about it\nthat thing would take shape and it would\nevolve and sometimes things change over\ntime\nour politics and our views about things\nchange over time\nyeah i mean i mean that's i mean that's\nan entirely different problem with\nthings like gpt3 right\nhere as you crawl the entire internet\nand so first of all\nto the point before 500 gigabytes of\ntext data like really a lot of stuff is\ngoing to be repeated over and over\nlike no deduplication can save you from\nthe fact that you know there's like\nthere's a lot of stuff that's kind of\nover and over and over in there okay but\nsecond of all of course it's a snapshot\nit's like\nsome of these things are old some of\nthese things are\nfrom sources that are sketchy uh\nsome of you know it's it's so that\nthere's an entirely different\naspect and i think that's one of the\naspects that you know in gpt2\nand general in the you know the fairness\ncommunity that's that's very prevalent\nif you\nif you input data from 1950\nyou're going gonna get output from 1950\nand and that's\nso that's a question i had for you what\nwhat are your thoughts on the\non the dangers of of these mod because\nin gpt2 especially but i think\nin gpthree the broader impact statement\nis five pages long or so\non like these things like let's say fake\nnews generation\nwith that and so on what are your\nthoughts on this\noh and i'm just gonna say that i do\nthink that you know a lot of it has to\nbe a publicity play don't you think like\nmost people don't really know how to\nread these papers i it's just\nnot most people but a lot of people and\ni think they want to keep this\nlike mystery to what they can do at\nleast like the eyes of the tech\ninvestors\narguably it's going to be a short time\nuntil this is available\nyou know publicly i mean think of things\nlike deep fakes right\nit it's it's been like maybe a year\nfrom the point where it was you know\nkind of working in academia to the point\nof where i can just\nyou know have an app on my phone be like\ndeep\ncoming back to what you said a second\nago when we train classification models\nwe we have more rigor about\nyou know stratifying the the training\nset for example\nyou know we would balance the client the\nthe cats and the dogs\nbecause we don't want dogs to be over\nrepresentative and dominant in the model\nand that level of due diligence is\nimpossible to do in the natural language\nprocessing model\nfirst of all some articles will just be\ndoubled or tripled or quadrupled\ndoes that mean that they have more\nimpact but the other thing is if you\nlook at all of the different\ndirections that the model learns for\nexample\ngender vectors or racial bias or it\nmight be learning\nlots and lots of things about 1950s\nengland but not things about\nnow and once we develop these really\nsophisticated machine learning models\nthey are they're so heterogeneous and\ndiverse we couldn't possibly\nbalance their training in any meaningful\nway\nyeah yeah i mean that makes sense\nbut do we want to if you think about it\nthe convex whole of our physical reality\nit has a shape\nand that shape is evolving\nwell the question is just if i have a\nsnapshot\nof that that's that's always going to be\nout of date let's let's just care about\ncare about the date\nright now historic versus current right\nit's\nlike any snapshot of the of the internet\nis necessarily going to be out of date\nbecause\ni've kept even if i downloaded the whole\ninternet yesterday\nit is it's out of date by at least one\nday\nthat's given that every website was\nwritten yesterday\nso even everything's evolving but\nyou know that that's an interesting\ntrade of those so there's the time thing\nnow if we only trained it on the data\nfrom yesterday\nit would be super up to date and it\nwould know loads of gender pronouns and\nso on\nbut it wouldn't have the scale\nso there's a trade-off between i want to\nhave\nmore training data but inevitably i'm\ngoing to have to give it\nolder information but there's also the\nspecificity and personalization aspect\nso it would also be good if i\npartitioned the data so it was only in\nlondon in the united kingdom and then it\nwould know about\nall of the colloquialisms and all the\nthings that we're talking about\nso there are always trade-offs to be\nmade yes maybe that's another\nactually thing for this reverse index\nthing where you could say\ni condition i only want the weights\nwhere\nthese particular training subsets\nmattered for training or something like\nthis it would be\ni mean this is kind of what they do with\nthis in context learning where they say\nyou can sort of condition it\nbut what you're saying is basically i\nwould i want the model\nthat acts as if i only had trained it\non data from london\ni mean that's going to be very hard but\nlike a cool idea\nwell yeah that's personalization but\nthen there's bias as well\nyeah so so what what if i want to remove\nmeaningfully gender bias\nwell you first of all i have to come up\nwith like a definition\nwhich people are still arguing\nover what that even means um but i guess\npresuming you have a definition i i also\nthink that\nthere i mean this is still an actively\ndeveloping field this whole\nfairness bias removal and so on there\nare lots of\nopinions and techniques i i i'm not\nqualified\ntoo much to give something here but\ngoing back to my\ninitial question like\nspecifically let's talk about the\ndangers of fake news\nlike this this is brought up it was\nbrought up in gpt2\nand it's brought up in this broader\nimpact statement here i believe i\nactually haven't\nread it fully but i think that was one\nof the points\nand it seems like this is a you know a\nthing that people say\noh this can be used to generate fake\nnews\nand my question is is that a problem\nwell let's say you have like a not even\ngpt3 but like a gam that can produce\nlike this video\nyeah that could be a huge problem right\nbecause you could make these videos of\nlike these\ncrazy things that are happening and then\nhit the social media thing and then\noutrage well exactly right that's that's\none of the things\ni see i feel with the fake news\nit is people are people already know\nthat whatever is written must not be\ntrue\ni think people with video people most\npeople think if they see a video\nit might you know be deceptively cut or\nsomething but certainly the\nthe pixel information is what actually\nhappened but\nmost people are probably aware that word\ninformation\nmust not represent like it's possible\nthat it\ndoesn't represent the truth like some\nsomeone could be just\nwriting you know crap and\nfake news right someone could be writing\na lie\nand it seems to me that\nthe argument you'd have to make is that\nthe ability to now automatically do this\ninstead of\nmanually is somehow dangerous\nso do you do you think i don't know i'm\ni have yet to hear\nan argument that this automatically\ngenerating\nof fake news is a is a dangerous thing\nit's not a dangerous thing because we\nhave authoritative\nsources so it doesn't matter if you can\ngenerate realistic looking text\npeople know that you're not an\nauthoritative source i think the real\nproblem is bias\njust to use another example what if we\nfelt as a society that we should have\ndiversity\nin colors of car you know there should\nbe\ngreen cars and yellow cars and orange\ncars\nand at the moment people have been\nbuying more orange cars and we think\nit's incredibly unfair\nand the kind of society we want to move\nto is where there are more blue cars\nso what we need to do is we need to\nstratify our language models to make\nsure\nthat blue cars are represented um\ndiversely well they're like they're like\n10 different layers of this problem so\nfirst of all i mean this is an unpopular\nopinion but i've been arguing this\nwe should we should like this term of d\nbiasing or unbiasing and so on like what\nyou're describing is\nbiasing like let's be like it has a bad\nrep a bad connotation but statistically\nyou want to bias the model because what\nyou want to do is\nsay okay here is a world that i would\nlike to have right i would here is my\nworlds that i aspire to right and\ni want my money to conform to that world\nright i\ni and that's irrespective in the case\nwhat you're saying of with the cars\nand that's absolutely has nothing to do\nwith what the real world looks like\nlike you simply say i want this\ndistribution of cars that's what i\naspire to that's my ideal\nand therefore i will bias my model\ntowards that now that's a different\nthing from saying\nthe model isn't representing the world\nas it is right for example i've seen a\npaper recently that\nmakes a credible case that\nregularization\nis one of the drivers or can be one of\nthe drivers\nof bias in models where let's say\nit's actually very simple right you you\nhave the ferraris and the lamborghinis\nthen the drive the ferrari drivers and\nthe lamborghini drivers\nand you want make a model that predicts\nthe accident probability\nnow it just so happens that the ferrari\ndrivers are a bit more reckless\nand and they do slightly higher\naccidents right and now i train my\nlogistic regression and it tells me okay\n60 40. cool but now i train my logistic\nregression with an l1 penalty\nand i say i want my model to be you know\nexplainable\ni want my explainable model and so i\nwant it to be sparse i want\nthe least amount of variables to be\ncontributing to\nwhat's the model going to say the model\nis going to say ferrari drivers\nbad lamborghini drivers good right\nso i think there are there are two very\ndifferent\nconcepts here one is one and i think we\nshould\nfrankly separate the subfields here\nbecause one says\nthese models by some by how we train\nthem\nby what loss functions we use by\nwhat data we input to a certain degree\nare not representing\nthe the data as it occurs in the world\nlike they are not representing the the\ncumulative distribution of data and the\nother\nthe entirely different field is saying\nhere is a state that i want to achieve\nhow can i make the model achieve that so\nin the one case you can\nlegitimately speak about d biasing or\nunbiasing in the other case it's\nactually\nbiasing but to play devil's advocate\npeople asian would say there are\nstructures of interlocking oppressions\nin society\nand people have been moving away from\nbuying blue cars because their\nperception is that blue cone\ndon't work very well and the existing\nworld model tells them\nnot to buy blue cars so even though we\nwant to move our\nbuying blue cars that there are all of\nthese blockers in the way\nokay there i can again think of like two\ndifferent\nsub genres here where you could\nyou could so one is this this\ncompounding effect where you say\nbecause the world is in a certain way we\ntrain our models the models might be you\nknow a bit regularized so they're a bit\nbiased\nand the models actually inform the\npeople that then will only go\nmore into this direction which will\ntrain the models if we update them more\ninto that direction and so on so there's\nthere's that which is a problem of\ncourse if you use model\nand then train on data that comes out of\nthe process you're going to\njust amplify and amplify in in certain\ncases like\nyou can argue that it doesn't always\nhappen but\ni think that's not exactly what you were\nsaying what you were saying is\nthere is like in society there for\nwhatever reason there is a process\nthat's steering people away from blue\ncars\nright okay let's say that the yellow car\nlobby the evil yellow car lobby\num is steering people away from blue\ncars\nand that's not right and it shouldn't be\nbecause blue cars are exactly as good as\nyellow cars or like a more concrete\nexample that the grain lobby\nright grain lobby breakfast is the most\nimportant meal of the day that's a\nmarketing slogan\nlike people think it's a health slogan\nbut it's it's a marketing slogan to sell\ncereal\nand you can say well you know actually\nlunch lunch is as important you\nshouldn't just eat breakfast and skip\nlunch you can just eat lunch and skip\nbreakfast and and the whole society\nsomehow\nhas this wrong notion but and here is\nthe\nthe point that's still in that second\ncamp that described initially\nbecause the data of the world the state\nof the world\nis such that people eat breakfast\nthat people you know eat more breakfast\nbecause they think breakfast is the most\nimportant meal of the day and\nwhat you're describing is still a world\nthat you would like to\nget to where people don't differentiate\nbetween breakfast and lunch\ni like that another way to verbalize\nthis is politically you have\nconservativism\nversus progressivism and conservatives\nwant us to have a strong memory\nabout how things have always been and\nprogressivists want us to forget\neverything we've\nwe've learned over generations and just\nto\ndynamically evolve very quickly well so\nas a thought experiment because\nthere's there's a lot there's a lot to\ndisagree here\nbut just just to continue this thought\nthese models\nthe reason you could argue they are\nunethical is because they\nincrease our memory so what they do is\nif we train them on concepts from the\n1950s\nthey will stop us from as i said in\nlondon now all of the cool kids are\nusing all of these colloquialisms and so\non\nand if they use that on their cvs they\nwon't get the job because the model is\nstill trained on the 1950s\nso there's an argument to just get rid\nof the model or to somehow\nimpute stuff into the model the war\nagainst ai will be\nfought with sentences like okay boomer\nmodel\nwell i've well to compare it to\nconservatism i would\nbecause i guess a conservative would say\nthey memorize\nthings because they want to learn from\nthe things that they've memorized and\nnot necessarily that\nthey would that they would always stick\nto them and maybe a progressive would\nsay\ni still remember but i want to change\nthe things that are\nhave not worked and but you i mean you\nwere right in that\na model from the olden days would\nactually\njust i mean what it does is\nit sees every data point equally and\nit is going to aggregate over everything\nthat it you give to it\nand it's going to think that the average\nof what it sees is the world today\nwhich is not true right the average of\nwhat it sees is actually\n1975. and there's this discrepancy\nis that the model the model has no\nnotion that this data point\nis out of date if you gave that to the\nmodel\nthen i like your analogy in my\nopinion become much more relevant\nbecause we're getting to the age-old\nthing that we do in machine learning\nwe start to incorporate\ninductive priors or temperature\nparameters\none thing we might do for example is\ndecide that recency is important\nwe do this with online ridge regression\nfor example we have a sliding window\nwe we assume that recent information is\nmore important than old information\nbecause\nmaybe we're going into a new regime now\nand the old information won't help us\npredict stuff about the new information\nbut we are also throwing away a lot of\nknowledge so\nas machine learning practitioners we\nneed to tweak all of these\nknobs and levers and we need to\nestablish what the best representation\nof this data is\nand it seems like a completely arbitrary\nprocess absolutely like i think\nin these in these types of models and i\ndon't think that has been considered in\nlanguage models yet but\nthis must exist because there has to be\nlike a notion of\ni can extract timeless principles from\ndata no matter how old it is right\nthat's why people go back to\nmarcus aurelius and and plato and\naristotle because\nthey had they just wrote down timeless\nthings that\nare as applicable today as they were\nin in their days not everything that\nthey've written down is applicable\ntoday so these models i feel\nif they're trained on ever growing\namounts of data that comes from\ndifferent\ntime periods and whatnot they should\nhave a notion\nof of what sort of information they must\nextract and\nthat seems very hard like that means\nyou'd have to understand wait a minute i\ni can probably write this down on my\nsmartphone nowadays and\nmarcus aurelius even though marcus\naurelia says i should write it down on\nmy clay tablet\nbut i should write it down nonetheless\nlike that's the timeless part\nwithout grounding that's going to be\nsuper hard\nyeah emanuel kant for example was racist\nhe had some incredible ideas on\nphilosophy he introduced\nconsequentialism for example\nbut he was a racist and now we look back\nand we say well um\nwe well it doesn't discredit his other\nideas\nbut that's that's something that he\nthought which we now no longer think is\nis is right and maybe that might happen\nto\nsome of these older philosophers as well\nmaybe we'll decide that\nthis particular viewpoint is no longer\nsomething which is relevant\nyeah but how do you make how do you make\nthat distinction between\nthis is i mean even that's an even\nmore complicated thing because that is\nsomehow\non the level of abstraction that is\nactually an abstract\nidea racism right that we changed our\nmind on right we've we've somehow\nwe've somehow as a society gotten\npast that and and recognized more of the\nindividual value of every human being\nand so on and\nyou can argue that has also gone with\nscience because a lot of these things\nwere based on like junk science\nbut also it's like society\nevolves and and the consciousness of\nsociety changes and\ni mean how are you going to teach that\nto a model like it's\nit's one thing that it says oh you know\nclay tablets aren't a thing anymore\nbecause\nthere are no more clay tablets but\nnow it must also recognize wait society\nchanged its mind about this particular\nabstract issue\nit's it's going to be a long time before\nwe have\nreally solid methods of dealing with\nthat thing\nand i don't i don't know what to do\nabout this\ni mean as you say there is this immense\ntrade-off\nif you want more data you're probably\ngoing to have to go back in history\nand you're just going to increase that\nhistory that recency or on recency bias\nbut in a way there is a recency bias\nbecause we are producing exponentially\nmore data\nall the time yes even now the\nwhen the philosophers were around in the\nmedieval times not much information\nhas gone on record from that time so if\nanything we need to over sample from\nthat period\nwell but how how much of the data we\nproduce is just\ncopies of the philosopher's thing but\nwhat you could hope is that their\ncopies adjusted to the modern times\nright that would be the perfect actually\nthe perfect training data would be\npeople\nyou know taking taking marcus aurelius's\nwork\nbut translating it to the modern time\nsays what marcus aurelius says\nis that you should write down uh your\ndreams\nand you could do you you might do so on\nyour smartphone\ni've no i don't think marcus aurelius\nwas into dream\njournaling yeah that's if like that\nwould be the perfect training data\nbecause\nyou'd have this exponential production\nof data but you keep\nthis timeless knowledge around but that\nrequires humans\nright that requires i\ni'm not sure how the machine itself\ncould do that\nit shouldn't matter that we've got so\nmuch oversampling and duplication of\ndata now because we're just learning a\nlanguage manifold\nbut i want to be able to do with\nlanguage like what we\ncan do with gans and faces you know now\nyou can\ni want it to have more hair and i want\nit to have a beard and glasses and so on\nand perhaps we can do the same thing\nwith language we can have slide bars to\nsay well i want it to be a bit more\nmedieval\ni want it to be a bit more united\nkingdom\nand we can start to get control over\nthis manifold in dimensions that we care\nabout\nyeah i think the best way to do that is\nto fine-tune the model though\nright like you just start fine-tuning i\nwant you to write positive articles\nright and then score it with reinforced\nlearning or auxiliary classifiers\nyeah i mean this in gans this only works\nbecause\nthese are on this face data set right\nand the faces\nthey can only have like 10 to 20\nattributes if you're if you're\nif you're serious right that that are\nreally significantly different they're\nlike all super\npretty aligned and and you can have you\nknow your hair\ncolor and your whether you have glasses\nor not but\nultimately if i gave you a language\nmodel and say\nhere are 100 relevant latent directions\nbut i'm not going to tell you what they\nare\nit's just those are the directions of\nthe 100\nprincipal components basically of\nmaximal variants that are also\ndisentangled\nyou'd have no clue what you would be\ndoing you're like\nit's like me using like a video editing\nsoftware like um\n[Laughter]\nyes that was a bit of a bombshell then i\nmean it's incredible\ni think there are the the big parts of\nthis development are yet\nin front of us and also like big\nchallenges like\nuh like this big corpora we have no clue\nwe don't even have a clue what's in them\nand how do we deal with the fact that\nyou know information is old and\nthings like this it's crazy i don't know\nokay well\ni think that is a suitable place to\nround off the episode\nyeah thanks such folks next week we're\ngoing to be back with francoise chalet\nthe fact the fabulous frenchman don't\nforget uh we are on all\nmajor audio podcast platforms now as\naudio like as sound\nbecause we sound so beautiful\nwe do made it to all the platforms so if\nyou enjoy that more\nyeah check us out there fantastic see\nyou next week folks\nyeah cool peace out peace\n",
  "words": [
    "super",
    "exciting",
    "last",
    "week",
    "openai",
    "released",
    "gpt3",
    "largest",
    "language",
    "model",
    "ever",
    "released",
    "fantastic",
    "conversation",
    "today",
    "exciting",
    "thing",
    "model",
    "sheer",
    "size",
    "trained",
    "language",
    "modeling",
    "number",
    "nlp",
    "tasks",
    "box",
    "without",
    "fine",
    "tuning",
    "working",
    "kind",
    "precision",
    "never",
    "seen",
    "raises",
    "many",
    "questions",
    "future",
    "building",
    "nlp",
    "models",
    "interact",
    "future",
    "gpt3",
    "biggest",
    "language",
    "model",
    "ever",
    "produced",
    "175",
    "billion",
    "parameters",
    "need",
    "super",
    "computer",
    "train",
    "openai",
    "done",
    "exactly",
    "175",
    "billion",
    "parameter",
    "neural",
    "network",
    "10",
    "times",
    "larger",
    "biggest",
    "artificial",
    "neural",
    "network",
    "ever",
    "trained",
    "train",
    "enormous",
    "model",
    "powered",
    "zero",
    "two",
    "optimizations",
    "microsoft",
    "open",
    "source",
    "deep",
    "speed",
    "library",
    "lot",
    "research",
    "efficiently",
    "scaling",
    "training",
    "hundreds",
    "thousands",
    "gpus",
    "foundational",
    "ideas",
    "research",
    "data",
    "parallelism",
    "copy",
    "model",
    "gpu",
    "send",
    "gpu",
    "different",
    "batch",
    "data",
    "model",
    "parallelism",
    "split",
    "model",
    "onto",
    "different",
    "gpus",
    "model",
    "parallelism",
    "necessary",
    "component",
    "fit",
    "billion",
    "parameter",
    "model",
    "single",
    "nvidia",
    "v100",
    "gpu",
    "32",
    "gigabytes",
    "memory",
    "next",
    "step",
    "story",
    "development",
    "pipeline",
    "parallelism",
    "famously",
    "implemented",
    "google",
    "used",
    "hit",
    "image",
    "net",
    "accuracy",
    "557",
    "million",
    "parameters",
    "partitions",
    "mini",
    "batches",
    "different",
    "subsets",
    "model",
    "split",
    "multiple",
    "gpus",
    "something",
    "waiting",
    "sequential",
    "forward",
    "backward",
    "passes",
    "network",
    "finish",
    "similar",
    "vein",
    "xero",
    "recognizes",
    "inefficiency",
    "data",
    "parallelism",
    "partitions",
    "training",
    "memory",
    "across",
    "accelerators",
    "splitting",
    "model",
    "across",
    "multiple",
    "accelerators",
    "need",
    "one",
    "hold",
    "entire",
    "set",
    "optimizer",
    "states",
    "gradients",
    "parameters",
    "solve",
    "inefficiency",
    "xero",
    "uses",
    "dynamic",
    "scheduling",
    "optimizer",
    "state",
    "gradients",
    "parameters",
    "communicate",
    "multiple",
    "gpus",
    "first",
    "iteration",
    "xero",
    "produced",
    "turing",
    "nlg",
    "17",
    "billion",
    "parameter",
    "language",
    "model",
    "continued",
    "improve",
    "perplexity",
    "metric",
    "auto",
    "regressive",
    "language",
    "modeling",
    "previous",
    "iteration",
    "nvidia",
    "megatron",
    "lm",
    "billion",
    "parameters",
    "result",
    "implementing",
    "optimizer",
    "state",
    "memory",
    "reductions",
    "zero",
    "one",
    "well",
    "zero",
    "two",
    "implements",
    "gradient",
    "parameter",
    "optimizations",
    "well",
    "bar",
    "chart",
    "blog",
    "post",
    "highlights",
    "enormous",
    "gains",
    "zero",
    "one",
    "zero",
    "two",
    "evidences",
    "training",
    "differences",
    "turing",
    "nlg",
    "gpt3",
    "gpt3",
    "autoregressive",
    "language",
    "model",
    "opposed",
    "denoising",
    "autoencoder",
    "like",
    "birth",
    "today",
    "conversation",
    "talk",
    "comparative",
    "differences",
    "two",
    "architectures",
    "language",
    "model",
    "incredible",
    "amount",
    "progress",
    "language",
    "processing",
    "especially",
    "using",
    "neural",
    "networks",
    "kind",
    "started",
    "2013",
    "mickelov",
    "introduced",
    "word",
    "vec",
    "distributed",
    "representation",
    "words",
    "learned",
    "way",
    "based",
    "context",
    "lstm",
    "type",
    "language",
    "models",
    "learning",
    "sequential",
    "representation",
    "tokens",
    "time",
    "bidirectional",
    "variants",
    "really",
    "cool",
    "thing",
    "happened",
    "2018",
    "jeremy",
    "howard",
    "sebastian",
    "ruder",
    "released",
    "ulm",
    "fit",
    "introduced",
    "transfer",
    "learning",
    "natural",
    "language",
    "processing",
    "word",
    "vect",
    "technically",
    "transfer",
    "learning",
    "transfer",
    "learning",
    "sense",
    "language",
    "model",
    "learning",
    "dependencies",
    "whole",
    "bunch",
    "tokens",
    "individual",
    "tokens",
    "course",
    "could",
    "downstream",
    "task",
    "attention",
    "need",
    "introduce",
    "transformers",
    "transformers",
    "incredible",
    "kind",
    "routing",
    "machines",
    "learn",
    "dependencies",
    "tokens",
    "input",
    "goes",
    "successive",
    "clever",
    "routing",
    "system",
    "learned",
    "part",
    "training",
    "process",
    "original",
    "recurrent",
    "neural",
    "networks",
    "tokens",
    "input",
    "sequence",
    "could",
    "directly",
    "attend",
    "meant",
    "models",
    "suffered",
    "catastrophic",
    "forgetting",
    "also",
    "quite",
    "difficult",
    "train",
    "vanishing",
    "exploding",
    "gradients",
    "transformers",
    "came",
    "along",
    "different",
    "paradigm",
    "could",
    "learn",
    "complex",
    "hierarchy",
    "relationships",
    "directly",
    "tokens",
    "longer",
    "sequential",
    "representation",
    "knew",
    "happened",
    "one",
    "time",
    "step",
    "backwards",
    "real",
    "paradigm",
    "shift",
    "straight",
    "away",
    "language",
    "processing",
    "models",
    "produce",
    "results",
    "across",
    "board",
    "generally",
    "speaking",
    "two",
    "architectural",
    "patterns",
    "see",
    "transformers",
    "architectures",
    "first",
    "pattern",
    "model",
    "predicting",
    "next",
    "word",
    "next",
    "word",
    "next",
    "word",
    "answer",
    "previous",
    "prediction",
    "gets",
    "fed",
    "model",
    "next",
    "time",
    "around",
    "type",
    "model",
    "called",
    "denoising",
    "autoencoder",
    "like",
    "burton",
    "roberta",
    "excel",
    "net",
    "feed",
    "input",
    "sentence",
    "typically",
    "add",
    "noise",
    "say",
    "expect",
    "things",
    "seem",
    "appropriate",
    "things",
    "like",
    "question",
    "answering",
    "need",
    "point",
    "spans",
    "input",
    "sequence",
    "example",
    "question",
    "answering",
    "models",
    "quite",
    "attractive",
    "keep",
    "generating",
    "data",
    "forever",
    "seem",
    "appropriate",
    "natural",
    "language",
    "generation",
    "one",
    "things",
    "wanted",
    "explore",
    "today",
    "commercial",
    "utility",
    "something",
    "help",
    "us",
    "industry",
    "something",
    "mainly",
    "academic",
    "endeavor",
    "reach",
    "top",
    "tech",
    "companies",
    "like",
    "microsoft",
    "google",
    "yannick",
    "incredible",
    "video",
    "gpt3",
    "last",
    "week",
    "channel",
    "starting",
    "develop",
    "bit",
    "name",
    "online",
    "lightspeed",
    "yannick",
    "might",
    "say",
    "within",
    "hours",
    "paper",
    "released",
    "video",
    "youtube",
    "channel",
    "even",
    "linked",
    "andre",
    "capathi",
    "director",
    "ai",
    "tesla",
    "incredible",
    "cut",
    "five",
    "minutes",
    "descriptive",
    "elements",
    "yannick",
    "video",
    "gbt3",
    "going",
    "play",
    "think",
    "serve",
    "good",
    "refresher",
    "topic",
    "paper",
    "basically",
    "investigation",
    "giant",
    "language",
    "models",
    "language",
    "model",
    "order",
    "magnitude",
    "larger",
    "anyone",
    "ever",
    "built",
    "language",
    "model",
    "absolutely",
    "crazy",
    "things",
    "paper",
    "basically",
    "investigation",
    "giant",
    "language",
    "models",
    "language",
    "model",
    "order",
    "magnitude",
    "larger",
    "anyone",
    "ever",
    "built",
    "language",
    "model",
    "absolutely",
    "crazy",
    "things",
    "basically",
    "go",
    "architecture",
    "model",
    "experimental",
    "results",
    "turns",
    "train",
    "language",
    "model",
    "enough",
    "data",
    "able",
    "solve",
    "nlp",
    "tasks",
    "never",
    "seen",
    "box",
    "language",
    "model",
    "let",
    "take",
    "example",
    "sentence",
    "right",
    "sentence",
    "like",
    "third",
    "humans",
    "require",
    "large",
    "supervised",
    "data",
    "sets",
    "learn",
    "language",
    "tasks",
    "english",
    "sentence",
    "language",
    "model",
    "would",
    "model",
    "cross",
    "portion",
    "end",
    "like",
    "right",
    "would",
    "able",
    "tell",
    "comes",
    "next",
    "language",
    "model",
    "would",
    "input",
    "part",
    "right",
    "tell",
    "next",
    "word",
    "data",
    "sets",
    "basically",
    "language",
    "model",
    "trained",
    "one",
    "basically",
    "generate",
    "word",
    "word",
    "word",
    "ask",
    "question",
    "like",
    "word",
    "likely",
    "come",
    "next",
    "likely",
    "language",
    "model",
    "nothing",
    "model",
    "kind",
    "generate",
    "language",
    "probabilistic",
    "way",
    "cool",
    "thing",
    "language",
    "models",
    "train",
    "sort",
    "text",
    "data",
    "train",
    "language",
    "model",
    "giant",
    "amounts",
    "data",
    "compare",
    "language",
    "model",
    "like",
    "bert",
    "bert",
    "required",
    "much",
    "flops",
    "train",
    "log",
    "scale",
    "right",
    "several",
    "orders",
    "magnitude",
    "larger",
    "bigger",
    "model",
    "trained",
    "way",
    "longer",
    "text",
    "naturally",
    "going",
    "lot",
    "better",
    "language",
    "modeling",
    "see",
    "right",
    "size",
    "models",
    "trained",
    "remember",
    "previous",
    "largest",
    "language",
    "model",
    "touring",
    "nlg",
    "microsoft",
    "something",
    "like",
    "17",
    "billion",
    "parameters",
    "would",
    "comparable",
    "right",
    "whereas",
    "175",
    "billion",
    "parameters",
    "absolutely",
    "crazy",
    "order",
    "magnitude",
    "higher",
    "anything",
    "ever",
    "existed",
    "look",
    "last",
    "gpt",
    "model",
    "remember",
    "made",
    "video",
    "dangerous",
    "released",
    "well",
    "released",
    "dangerous",
    "released",
    "clocked",
    "billion",
    "parameters",
    "essentially",
    "language",
    "model",
    "like",
    "bert",
    "goes",
    "left",
    "right",
    "always",
    "produces",
    "next",
    "word",
    "like",
    "even",
    "say",
    "say",
    "use",
    "model",
    "architecture",
    "gpt2",
    "layers",
    "wider",
    "layers",
    "data",
    "train",
    "something",
    "like",
    "bird",
    "would",
    "first",
    "would",
    "language",
    "modeling",
    "right",
    "phase",
    "teach",
    "bert",
    "english",
    "language",
    "feeding",
    "lot",
    "data",
    "second",
    "step",
    "called",
    "fine",
    "tuning",
    "second",
    "one",
    "something",
    "like",
    "task",
    "actually",
    "interested",
    "let",
    "say",
    "task",
    "actually",
    "interested",
    "sentiment",
    "classification",
    "sentiment",
    "classification",
    "like",
    "sentence",
    "like",
    "blah",
    "blah",
    "blah",
    "want",
    "know",
    "positive",
    "sentiment",
    "like",
    "happy",
    "sentence",
    "sad",
    "sentence",
    "would",
    "database",
    "labeled",
    "instances",
    "database",
    "bunch",
    "sentences",
    "one",
    "would",
    "know",
    "good",
    "positive",
    "negative",
    "like",
    "smaller",
    "test",
    "set",
    "right",
    "would",
    "basically",
    "take",
    "model",
    "train",
    "data",
    "set",
    "supervised",
    "machine",
    "learning",
    "way",
    "test",
    "test",
    "set",
    "right",
    "called",
    "fine",
    "tuning",
    "interested",
    "basically",
    "take",
    "model",
    "directly",
    "go",
    "evaluate",
    "test",
    "data",
    "set",
    "sort",
    "fashion",
    "language",
    "model",
    "input",
    "following",
    "text",
    "input",
    "call",
    "task",
    "description",
    "prompt",
    "input",
    "simply",
    "ask",
    "model",
    "language",
    "model",
    "predict",
    "next",
    "word",
    "comes",
    "counting",
    "basically",
    "training",
    "data",
    "model",
    "seen",
    "structure",
    "like",
    "enough",
    "understand",
    "going",
    "training",
    "data",
    "somewhere",
    "internet",
    "structure",
    "translate",
    "something",
    "something",
    "would",
    "word",
    "something",
    "know",
    "kind",
    "realize",
    "goes",
    "like",
    "next",
    "word",
    "basically",
    "asking",
    "find",
    "text",
    "website",
    "wikipedia",
    "books",
    "data",
    "set",
    "find",
    "piece",
    "text",
    "would",
    "next",
    "word",
    "piece",
    "text",
    "simply",
    "input",
    "string",
    "task",
    "description",
    "prompt",
    "right",
    "also",
    "one",
    "example",
    "example",
    "example",
    "going",
    "come",
    "training",
    "data",
    "set",
    "task",
    "interested",
    "important",
    "part",
    "never",
    "train",
    "never",
    "explicitly",
    "train",
    "example",
    "simply",
    "put",
    "context",
    "simply",
    "put",
    "string",
    "translate",
    "english",
    "french",
    "new",
    "line",
    "see",
    "loot",
    "new",
    "line",
    "cheese",
    "simply",
    "input",
    "string",
    "model",
    "language",
    "model",
    "ask",
    "next",
    "word",
    "right",
    "okay",
    "hope",
    "hope",
    "clear",
    "call",
    "kind",
    "one",
    "shot",
    "generalization",
    "one",
    "shot",
    "basically",
    "mean",
    "simply",
    "provide",
    "thing",
    "context",
    "model",
    "language",
    "model",
    "advantage",
    "immediately",
    "clear",
    "train",
    "one",
    "model",
    "basically",
    "inference",
    "time",
    "input",
    "task",
    "description",
    "sort",
    "training",
    "data",
    "task",
    "evaluation",
    "context",
    "task",
    "think",
    "simply",
    "take",
    "go",
    "training",
    "data",
    "stored",
    "weights",
    "filter",
    "training",
    "data",
    "basically",
    "take",
    "things",
    "sort",
    "pattern",
    "match",
    "sort",
    "regex",
    "match",
    "fuzzy",
    "way",
    "context",
    "kind",
    "interpolate",
    "training",
    "examples",
    "order",
    "come",
    "answer",
    "think",
    "reasoning",
    "happening",
    "looking",
    "comments",
    "actually",
    "video",
    "make",
    "interesting",
    "reading",
    "yannick",
    "says",
    "gets",
    "better",
    "comments",
    "youtube",
    "videos",
    "conferences",
    "submits",
    "papers",
    "reviews",
    "see",
    "janik",
    "video",
    "published",
    "viewed",
    "nearly",
    "28",
    "000",
    "times",
    "000",
    "votes",
    "nine",
    "people",
    "voted",
    "ashamed",
    "alex",
    "bravo",
    "chips",
    "t6",
    "one",
    "trillion",
    "text",
    "text",
    "transfer",
    "transformer",
    "next",
    "model",
    "coming",
    "google",
    "mallow",
    "marsh",
    "comments",
    "looking",
    "wall",
    "authors",
    "beginning",
    "making",
    "sweat",
    "person",
    "incomprehensible",
    "youtube",
    "name",
    "human",
    "adversarial",
    "example",
    "profile",
    "picture",
    "adds",
    "think",
    "intuition",
    "model",
    "essentially",
    "storing",
    "training",
    "data",
    "quasi",
    "lookup",
    "table",
    "correct",
    "anything",
    "model",
    "acts",
    "elaborate",
    "compression",
    "algorithm",
    "also",
    "modeling",
    "semantic",
    "structure",
    "language",
    "needed",
    "pass",
    "natural",
    "language",
    "model",
    "input",
    "certainly",
    "achieved",
    "way",
    "resemble",
    "plane",
    "lookup",
    "table",
    "okay",
    "seemed",
    "lose",
    "grammatical",
    "um",
    "fidelity",
    "towards",
    "end",
    "message",
    "get",
    "idea",
    "yannick",
    "replies",
    "yes",
    "argument",
    "made",
    "saying",
    "plain",
    "look",
    "tables",
    "like",
    "fuzzy",
    "look",
    "interpolation",
    "tables",
    "main",
    "point",
    "tasks",
    "model",
    "performs",
    "well",
    "explained",
    "lookup",
    "interpolation",
    "none",
    "model",
    "succeeds",
    "say",
    "due",
    "reasoning",
    "abilities",
    "seems",
    "really",
    "clever",
    "thing",
    "gpt3",
    "internalizes",
    "information",
    "give",
    "deconstructs",
    "packs",
    "internal",
    "representation",
    "ask",
    "go",
    "something",
    "go",
    "find",
    "information",
    "trained",
    "reconstruct",
    "interpolating",
    "internal",
    "structures",
    "learned",
    "super",
    "clever",
    "stuff",
    "happy",
    "teacher",
    "says",
    "wow",
    "incredible",
    "fast",
    "agree",
    "happy",
    "teacher",
    "really",
    "gary",
    "blauer",
    "says",
    "continuing",
    "expand",
    "class",
    "problems",
    "solved",
    "pattern",
    "recognition",
    "wo",
    "get",
    "us",
    "way",
    "artificial",
    "general",
    "intelligence",
    "interesting",
    "impressive",
    "nonetheless",
    "remarkable",
    "example",
    "goes",
    "quote",
    "chalet",
    "without",
    "crediting",
    "charlay",
    "might",
    "say",
    "problem",
    "treated",
    "pattern",
    "recognition",
    "problem",
    "training",
    "data",
    "covers",
    "sufficiently",
    "dense",
    "sampling",
    "problem",
    "space",
    "interesting",
    "happens",
    "training",
    "data",
    "sparse",
    "indeed",
    "poodle",
    "chen",
    "says",
    "like",
    "think",
    "language",
    "models",
    "really",
    "smart",
    "parrots",
    "repeat",
    "said",
    "thank",
    "poodle",
    "chen",
    "like",
    "dodon",
    "ko",
    "also",
    "unwittingly",
    "quotes",
    "francois",
    "chalet",
    "175",
    "billion",
    "parameters",
    "sometimes",
    "feel",
    "like",
    "trying",
    "reach",
    "moon",
    "building",
    "higher",
    "higher",
    "skyscrapers",
    "get",
    "said",
    "reach",
    "moon",
    "twice",
    "think",
    "might",
    "generated",
    "gpt3",
    "russian",
    "person",
    "says",
    "think",
    "lot",
    "people",
    "missed",
    "news",
    "microsoft",
    "fired",
    "journalists",
    "right",
    "indeed",
    "need",
    "anymore",
    "andrew",
    "owens",
    "says",
    "agi",
    "enjoy",
    "watching",
    "take",
    "papers",
    "usually",
    "watch",
    "video",
    "times",
    "normal",
    "speed",
    "makes",
    "feel",
    "comfortable",
    "sloppy",
    "handwriting",
    "apologize",
    "behalf",
    "yannick",
    "handwriting",
    "sean",
    "hardy",
    "says",
    "phenomenal",
    "analysis",
    "really",
    "make",
    "field",
    "approachable",
    "students",
    "like",
    "christian",
    "garcia",
    "says",
    "thinking",
    "addition",
    "memorized",
    "argument",
    "totally",
    "agree",
    "reminded",
    "humans",
    "also",
    "tend",
    "replace",
    "lot",
    "logic",
    "memory",
    "example",
    "multiplication",
    "tables",
    "anecdotally",
    "think",
    "memorized",
    "various",
    "combinations",
    "numbers",
    "add",
    "ten",
    "five",
    "plus",
    "five",
    "six",
    "plus",
    "four",
    "seven",
    "plus",
    "three",
    "deep",
    "learning",
    "still",
    "needs",
    "good",
    "way",
    "logic",
    "reasoning",
    "vast",
    "amount",
    "knowledge",
    "good",
    "portion",
    "intelligence",
    "well",
    "one",
    "things",
    "talked",
    "show",
    "today",
    "many",
    "things",
    "initially",
    "need",
    "reason",
    "using",
    "system",
    "seems",
    "get",
    "distilled",
    "baked",
    "system",
    "one",
    "program",
    "borislav",
    "dizodzo",
    "says",
    "sorry",
    "crash",
    "mensa",
    "party",
    "prediction",
    "next",
    "word",
    "inherently",
    "shallow",
    "target",
    "know",
    "exactly",
    "formulate",
    "deeper",
    "thought",
    "target",
    "perhaps",
    "whole",
    "sentence",
    "prediction",
    "blur",
    "better",
    "similarity",
    "measure",
    "would",
    "trick",
    "trick",
    "would",
    "work",
    "transformers",
    "inherently",
    "good",
    "modeling",
    "reasoning",
    "thoughts",
    "come",
    "mind",
    "prediction",
    "sentence",
    "would",
    "take",
    "place",
    "future",
    "future",
    "text",
    "takes",
    "place",
    "bigger",
    "reward",
    "since",
    "super",
    "computers",
    "youtube",
    "comments",
    "cited",
    "maybe",
    "tell",
    "wrong",
    "silently",
    "implement",
    "something",
    "think",
    "right",
    "yannick",
    "says",
    "well",
    "problem",
    "blur",
    "differentiable",
    "essentially",
    "reinforcement",
    "learning",
    "people",
    "done",
    "predicting",
    "longer",
    "sequences",
    "would",
    "back",
    "propagate",
    "multiple",
    "invocations",
    "already",
    "giant",
    "model",
    "question",
    "predict",
    "single",
    "word",
    "future",
    "next",
    "word",
    "likely",
    "increase",
    "variance",
    "benefit",
    "model",
    "good",
    "ideas",
    "though",
    "good",
    "old",
    "herp",
    "derpingsson",
    "comments",
    "always",
    "legendary",
    "say",
    "single",
    "part",
    "dl",
    "research",
    "flexing",
    "many",
    "gpu",
    "hours",
    "afford",
    "true",
    "unfortunately",
    "ca",
    "afford",
    "many",
    "hope",
    "fun",
    "us",
    "analyzing",
    "hype",
    "stories",
    "around",
    "gpt3",
    "get",
    "fair",
    "bit",
    "various",
    "topics",
    "fun",
    "remember",
    "like",
    "comment",
    "subscribe",
    "see",
    "back",
    "next",
    "week",
    "hello",
    "folks",
    "welcome",
    "back",
    "machine",
    "learning",
    "street",
    "talk",
    "youtube",
    "channel",
    "uh",
    "tim",
    "scarf",
    "two",
    "compadres",
    "yannick",
    "kilcher",
    "connor",
    "shorten",
    "really",
    "exciting",
    "week",
    "open",
    "ai",
    "released",
    "yet",
    "another",
    "paper",
    "microsoft",
    "introduced",
    "new",
    "approaches",
    "training",
    "huge",
    "language",
    "models",
    "guys",
    "done",
    "videos",
    "youtube",
    "channels",
    "last",
    "week",
    "yannick",
    "actually",
    "made",
    "video",
    "gpt3",
    "many",
    "views",
    "got",
    "20",
    "000",
    "20",
    "23",
    "000",
    "crazy",
    "incredible",
    "popular",
    "video",
    "date",
    "attention",
    "need",
    "popular",
    "far",
    "since",
    "uh",
    "2017",
    "awesome",
    "guys",
    "think",
    "gbt3",
    "surprised",
    "sheer",
    "publicity",
    "gpt2",
    "think",
    "lot",
    "publicity",
    "sort",
    "manufactured",
    "fact",
    "oh",
    "dangerous",
    "release",
    "uh",
    "remember",
    "right",
    "like",
    "oh",
    "concerned",
    "ethical",
    "know",
    "bad",
    "applications",
    "release",
    "dangerous",
    "seems",
    "none",
    "necessary",
    "gpt3",
    "kind",
    "took",
    "think",
    "surprising",
    "thing",
    "people",
    "probably",
    "thought",
    "trend",
    "could",
    "scale",
    "would",
    "end",
    "point",
    "think",
    "gpt3",
    "show",
    "push",
    "another",
    "order",
    "magnitude",
    "kind",
    "fortifies",
    "trend",
    "could",
    "probably",
    "another",
    "two",
    "three",
    "orders",
    "magnitude",
    "think",
    "lot",
    "know",
    "order",
    "magnitude",
    "indication",
    "scaling",
    "still",
    "going",
    "benefit",
    "us",
    "lot",
    "showing",
    "signs",
    "slowing",
    "yeah",
    "crazy",
    "part",
    "right",
    "truth",
    "almost",
    "like",
    "pretty",
    "much",
    "complete",
    "trajectory",
    "like",
    "plot",
    "perplexity",
    "goes",
    "might",
    "think",
    "know",
    "largest",
    "model",
    "kind",
    "breaks",
    "trend",
    "bit",
    "look",
    "training",
    "curve",
    "even",
    "look",
    "converged",
    "yet",
    "even",
    "even",
    "gotten",
    "full",
    "epoch",
    "big",
    "corpus",
    "yeah",
    "thomas",
    "wolfe",
    "video",
    "presentation",
    "youtube",
    "still",
    "put",
    "live",
    "eventually",
    "put",
    "live",
    "cited",
    "uh",
    "yeovil",
    "know",
    "one",
    "israeli",
    "researchers",
    "talking",
    "phase",
    "change",
    "language",
    "models",
    "asked",
    "questions",
    "like",
    "uh",
    "years",
    "later",
    "year",
    "year",
    "later",
    "year",
    "created",
    "kind",
    "trick",
    "showing",
    "good",
    "language",
    "model",
    "answering",
    "questions",
    "course",
    "memorization",
    "think",
    "folks",
    "arguing",
    "phase",
    "change",
    "past",
    "certain",
    "point",
    "actually",
    "learning",
    "reason",
    "course",
    "even",
    "know",
    "year",
    "number",
    "token",
    "train",
    "enough",
    "data",
    "know",
    "1976",
    "really",
    "memorization",
    "generalization",
    "generally",
    "language",
    "models",
    "memorize",
    "parameters",
    "still",
    "useful",
    "right",
    "even",
    "memorizing",
    "kind",
    "criticism",
    "gotten",
    "video",
    "argue",
    "know",
    "basically",
    "memorizing",
    "training",
    "data",
    "might",
    "maybe",
    "expressed",
    "precisely",
    "mean",
    "basically",
    "memorizing",
    "training",
    "data",
    "like",
    "fuzzy",
    "way",
    "mem",
    "memorizing",
    "little",
    "snippets",
    "memorizing",
    "grammar",
    "constructs",
    "uh",
    "mean",
    "memorizing",
    "kind",
    "distributed",
    "way",
    "much",
    "tied",
    "training",
    "data",
    "thereby",
    "like",
    "use",
    "memorized",
    "know",
    "take",
    "grammar",
    "construct",
    "sentence",
    "know",
    "two",
    "people",
    "often",
    "occur",
    "together",
    "know",
    "fact",
    "think",
    "memory",
    "sort",
    "memorization",
    "lead",
    "generalization",
    "essentially",
    "learn",
    "interpolate",
    "seen",
    "training",
    "saying",
    "generalization",
    "thing",
    "said",
    "video",
    "reasoning",
    "memorizing",
    "described",
    "quite",
    "interesting",
    "saying",
    "okay",
    "memorizing",
    "deconstructing",
    "cleverly",
    "interpolating",
    "different",
    "instances",
    "reconstructed",
    "sentences",
    "make",
    "generalize",
    "point",
    "become",
    "reasoning",
    "never",
    "become",
    "reasoning",
    "well",
    "philosophy",
    "right",
    "interesting",
    "experiments",
    "paper",
    "example",
    "show",
    "model",
    "unscramble",
    "word",
    "know",
    "take",
    "scram",
    "give",
    "examples",
    "right",
    "scrambled",
    "word",
    "unscrambled",
    "scrambled",
    "word",
    "scramble",
    "say",
    "look",
    "least",
    "argument",
    "know",
    "precise",
    "formulation",
    "someone",
    "might",
    "say",
    "look",
    "model",
    "learned",
    "means",
    "unscramble",
    "word",
    "saying",
    "like",
    "perfect",
    "language",
    "model",
    "condition",
    "things",
    "output",
    "whatever",
    "word",
    "highest",
    "probability",
    "given",
    "word",
    "pieces",
    "right",
    "absolutely",
    "surprising",
    "mean",
    "learned",
    "reason",
    "much",
    "better",
    "experiment",
    "would",
    "learn",
    "scramble",
    "word",
    "right",
    "experiment",
    "model",
    "learned",
    "reason",
    "across",
    "english",
    "language",
    "really",
    "understand",
    "means",
    "scramble",
    "word",
    "actually",
    "infer",
    "context",
    "learning",
    "good",
    "scrambling",
    "unscrambling",
    "words",
    "arguing",
    "symmetric",
    "one",
    "thing",
    "able",
    "yes",
    "quite",
    "symmetric",
    "asymmetry",
    "scrambling",
    "versus",
    "unscrambling",
    "would",
    "come",
    "fact",
    "good",
    "english",
    "language",
    "model",
    "right",
    "unscramble",
    "two",
    "effects",
    "one",
    "understands",
    "unscrambling",
    "means",
    "second",
    "effect",
    "knows",
    "lot",
    "english",
    "words",
    "going",
    "scramble",
    "words",
    "knowledge",
    "english",
    "help",
    "scrambled",
    "word",
    "probably",
    "english",
    "word",
    "need",
    "rely",
    "understanding",
    "scrambling",
    "means",
    "therefore",
    "model",
    "learned",
    "reason",
    "would",
    "expect",
    "perform",
    "scrambling",
    "much",
    "unscrambling",
    "unfortunately",
    "experiment",
    "would",
    "surprised",
    "model",
    "could",
    "scrambling",
    "think",
    "reason",
    "paper",
    "shows",
    "unscrambling",
    "yeah",
    "either",
    "way",
    "think",
    "ever",
    "seen",
    "something",
    "like",
    "training",
    "data",
    "set",
    "uses",
    "inevitably",
    "scrambled",
    "could",
    "ever",
    "seen",
    "something",
    "like",
    "training",
    "data",
    "needs",
    "know",
    "right",
    "word",
    "pieces",
    "often",
    "occur",
    "together",
    "sees",
    "scrambled",
    "version",
    "inevitably",
    "like",
    "highest",
    "likelihood",
    "word",
    "word",
    "absence",
    "ca",
    "produce",
    "word",
    "word",
    "produce",
    "word",
    "correct",
    "order",
    "think",
    "much",
    "going",
    "like",
    "seen",
    "website",
    "scramble",
    "unscramble",
    "things",
    "mechanics",
    "quite",
    "interesting",
    "spoke",
    "philosophical",
    "discussion",
    "frame",
    "reference",
    "function",
    "convex",
    "hull",
    "different",
    "words",
    "know",
    "words",
    "shared",
    "placeholders",
    "things",
    "common",
    "understanding",
    "straight",
    "away",
    "frame",
    "reference",
    "language",
    "models",
    "truncated",
    "virtue",
    "fact",
    "word",
    "peace",
    "beddings",
    "thing",
    "think",
    "truncation",
    "formulation",
    "language",
    "model",
    "predicting",
    "next",
    "word",
    "seems",
    "little",
    "bit",
    "bizarre",
    "examples",
    "know",
    "one",
    "shot",
    "two",
    "shot",
    "learning",
    "saying",
    "example",
    "something",
    "continue",
    "thinking",
    "limit",
    "amount",
    "use",
    "cases",
    "think",
    "complexity",
    "decoding",
    "output",
    "space",
    "like",
    "predict",
    "next",
    "token",
    "give",
    "much",
    "less",
    "computationally",
    "complex",
    "output",
    "space",
    "reconstruct",
    "entire",
    "input",
    "predictions",
    "mass",
    "well",
    "think",
    "transformers",
    "architecture",
    "quadratic",
    "time",
    "complexity",
    "computationally",
    "auto",
    "aggressive",
    "models",
    "quite",
    "nice",
    "keep",
    "predicting",
    "forever",
    "put",
    "previous",
    "prediction",
    "input",
    "next",
    "one",
    "could",
    "could",
    "generate",
    "article",
    "size",
    "whereas",
    "bert",
    "type",
    "model",
    "huge",
    "limitations",
    "obviously",
    "512",
    "inputs",
    "allow",
    "things",
    "ca",
    "auto",
    "aggressive",
    "model",
    "like",
    "example",
    "question",
    "answering",
    "small",
    "input",
    "think",
    "invert",
    "weights",
    "need",
    "able",
    "contribute",
    "outputs",
    "right",
    "one",
    "output",
    "element",
    "sequence",
    "gpt3",
    "entire",
    "apparatus",
    "weights",
    "focused",
    "predicting",
    "one",
    "next",
    "word",
    "yeah",
    "subject",
    "scaled",
    "auto",
    "regressive",
    "compared",
    "mass",
    "language",
    "model",
    "right",
    "talking",
    "difference",
    "say",
    "gbt",
    "burt",
    "still",
    "good",
    "still",
    "think",
    "specialized",
    "architecture",
    "think",
    "bert",
    "always",
    "better",
    "sentence",
    "classification",
    "right",
    "going",
    "uses",
    "different",
    "specialized",
    "architecture",
    "bert",
    "context",
    "think",
    "assertion",
    "things",
    "like",
    "question",
    "answering",
    "need",
    "context",
    "one",
    "thing",
    "get",
    "watching",
    "video",
    "gpt3",
    "yannick",
    "input",
    "size",
    "many",
    "tokens",
    "think",
    "2048",
    "yeah",
    "exactly",
    "yeah",
    "oh",
    "huge",
    "yeah",
    "yeah",
    "think",
    "reformer",
    "transformer",
    "xl",
    "think",
    "longer",
    "context",
    "window",
    "say",
    "twenty",
    "thousand",
    "forty",
    "eight",
    "power",
    "two",
    "random",
    "number",
    "like",
    "um",
    "make",
    "lot",
    "stronger",
    "well",
    "transformers",
    "xl",
    "significantly",
    "better",
    "reduction",
    "context",
    "fragmentation",
    "probably",
    "depends",
    "task",
    "right",
    "respect",
    "yeah",
    "okay",
    "mean",
    "case",
    "bert",
    "powerful",
    "like",
    "encoder",
    "whereas",
    "autoregressive",
    "property",
    "also",
    "sort",
    "unidirectional",
    "way",
    "produces",
    "output",
    "right",
    "still",
    "attend",
    "everything",
    "probably",
    "something",
    "like",
    "reformer",
    "whatever",
    "trade",
    "ability",
    "attend",
    "anything",
    "precise",
    "way",
    "trade",
    "getting",
    "longer",
    "input",
    "probably",
    "much",
    "depends",
    "task",
    "might",
    "work",
    "think",
    "natural",
    "language",
    "really",
    "every",
    "single",
    "know",
    "position",
    "need",
    "attend",
    "every",
    "position",
    "probably",
    "probably",
    "like",
    "hierarchical",
    "structure",
    "sentences",
    "whatnot",
    "think",
    "simply",
    "found",
    "yet",
    "make",
    "concrete",
    "say",
    "oh",
    "know",
    "basically",
    "language",
    "tree",
    "sort",
    "parse",
    "various",
    "ways",
    "also",
    "sometimes",
    "really",
    "important",
    "one",
    "word",
    "needs",
    "organization",
    "mainly",
    "attend",
    "tree",
    "structure",
    "also",
    "attend",
    "individual",
    "words",
    "want",
    "attend",
    "words",
    "would",
    "computationally",
    "hard",
    "back",
    "ca",
    "learn",
    "sparse",
    "things",
    "regressive",
    "models",
    "kind",
    "filter",
    "right",
    "stop",
    "model",
    "cheating",
    "stop",
    "tokens",
    "seeing",
    "things",
    "ahead",
    "think",
    "exists",
    "bert",
    "type",
    "architecture",
    "give",
    "text",
    "maybe",
    "like",
    "noise",
    "masked",
    "version",
    "text",
    "architecturally",
    "something",
    "like",
    "question",
    "answering",
    "reasonably",
    "straightforward",
    "architecture",
    "say",
    "could",
    "question",
    "answering",
    "putting",
    "span",
    "text",
    "question",
    "saying",
    "answer",
    "could",
    "train",
    "downstream",
    "would",
    "able",
    "auto",
    "aggressive",
    "model",
    "could",
    "actually",
    "bird",
    "gpd3",
    "simply",
    "input",
    "thing",
    "bird",
    "take",
    "last",
    "language",
    "model",
    "prediction",
    "shift",
    "things",
    "one",
    "append",
    "could",
    "producing",
    "bird",
    "absolutely",
    "right",
    "could",
    "gpt3",
    "point",
    "input",
    "like",
    "bird",
    "power",
    "bird",
    "yeah",
    "well",
    "ask",
    "gpt3",
    "language",
    "want",
    "would",
    "want",
    "know",
    "right",
    "think",
    "one",
    "interesting",
    "thing",
    "auto",
    "aggressive",
    "mass",
    "language",
    "modeling",
    "like",
    "spam",
    "bur",
    "masking",
    "like",
    "spans",
    "instead",
    "tokens",
    "think",
    "kind",
    "interesting",
    "much",
    "knowledge",
    "pack",
    "parameters",
    "language",
    "model",
    "looking",
    "asking",
    "t5",
    "questions",
    "based",
    "giving",
    "extra",
    "context",
    "show",
    "appendix",
    "c",
    "gains",
    "use",
    "span",
    "masking",
    "see",
    "pretty",
    "huge",
    "gain",
    "using",
    "kind",
    "training",
    "mask",
    "span",
    "tokens",
    "yeah",
    "even",
    "full",
    "word",
    "masking",
    "reasoning",
    "understand",
    "reasoning",
    "behind",
    "full",
    "word",
    "mask",
    "say",
    "instead",
    "masking",
    "word",
    "pieces",
    "mask",
    "mask",
    "word",
    "piece",
    "mask",
    "word",
    "pieces",
    "belong",
    "word",
    "usually",
    "mask",
    "one",
    "word",
    "piece",
    "word",
    "pieces",
    "give",
    "away",
    "determined",
    "really",
    "need",
    "know",
    "learn",
    "know",
    "sentence",
    "structure",
    "grammar",
    "go",
    "word",
    "pieces",
    "mask",
    "whole",
    "word",
    "ca",
    "anymore",
    "span",
    "bird",
    "know",
    "yeah",
    "know",
    "explicitly",
    "searching",
    "interesting",
    "yeah",
    "thought",
    "masked",
    "span",
    "words",
    "ca",
    "remember",
    "arrived",
    "length",
    "span",
    "might",
    "length",
    "two",
    "three",
    "something",
    "okay",
    "yeah",
    "think",
    "call",
    "like",
    "degeneration",
    "language",
    "generators",
    "repeat",
    "word",
    "see",
    "time",
    "think",
    "would",
    "alleviated",
    "instead",
    "predict",
    "next",
    "one",
    "mask",
    "five",
    "know",
    "predict",
    "five",
    "time",
    "variations",
    "bert",
    "denoising",
    "auto",
    "encoder",
    "super",
    "interesting",
    "covered",
    "t5",
    "paper",
    "thomas",
    "wolf",
    "thing",
    "talking",
    "different",
    "variations",
    "bert",
    "sometimes",
    "masking",
    "words",
    "sometimes",
    "masking",
    "span",
    "words",
    "sometimes",
    "replacing",
    "words",
    "adding",
    "noise",
    "seem",
    "taking",
    "something",
    "manifold",
    "pushing",
    "manifold",
    "yeah",
    "think",
    "trying",
    "get",
    "bottom",
    "like",
    "gpt",
    "versus",
    "bert",
    "right",
    "yeah",
    "well",
    "yeah",
    "seems",
    "really",
    "interesting",
    "industry",
    "moment",
    "number",
    "one",
    "thing",
    "everyone",
    "wants",
    "knowledge",
    "mining",
    "many",
    "companies",
    "wealth",
    "information",
    "unstructured",
    "documents",
    "data",
    "lake",
    "even",
    "inside",
    "office",
    "365",
    "need",
    "able",
    "use",
    "models",
    "extract",
    "useful",
    "information",
    "interested",
    "seems",
    "almost",
    "entirely",
    "useless",
    "ask",
    "gpt3",
    "queen",
    "england",
    "seems",
    "know",
    "lot",
    "common",
    "sense",
    "want",
    "extract",
    "semantic",
    "information",
    "documents",
    "something",
    "care",
    "useless",
    "well",
    "sure",
    "use",
    "something",
    "like",
    "bert",
    "still",
    "still",
    "requires",
    "like",
    "training",
    "data",
    "thing",
    "right",
    "whereas",
    "could",
    "potentially",
    "throw",
    "customer",
    "documents",
    "contracts",
    "things",
    "could",
    "go",
    "ahead",
    "say",
    "think",
    "need",
    "provide",
    "like",
    "two",
    "three",
    "examples",
    "structure",
    "need",
    "think",
    "examples",
    "provide",
    "need",
    "correct",
    "would",
    "interesting",
    "another",
    "experiment",
    "simply",
    "provide",
    "structurally",
    "grammatically",
    "know",
    "things",
    "incorrect",
    "things",
    "see",
    "model",
    "comes",
    "pretty",
    "sure",
    "model",
    "would",
    "still",
    "come",
    "looking",
    "yeah",
    "made",
    "argument",
    "video",
    "models",
    "basically",
    "go",
    "semantic",
    "fuzzily",
    "go",
    "training",
    "data",
    "filter",
    "training",
    "data",
    "whatever",
    "condition",
    "context",
    "learning",
    "basically",
    "fil",
    "filter",
    "entire",
    "training",
    "data",
    "documents",
    "match",
    "particular",
    "structure",
    "run",
    "language",
    "model",
    "conditioned",
    "set",
    "interpolate",
    "things",
    "would",
    "awesome",
    "company",
    "throw",
    "everything",
    "could",
    "interested",
    "know",
    "brittle",
    "bert",
    "example",
    "get",
    "impression",
    "could",
    "even",
    "thing",
    "language",
    "models",
    "train",
    "large",
    "corpus",
    "text",
    "internet",
    "learns",
    "common",
    "sense",
    "domain",
    "intuition",
    "took",
    "gpt",
    "type",
    "model",
    "would",
    "good",
    "maybe",
    "wrong",
    "let",
    "say",
    "take",
    "gpt3",
    "model",
    "somehow",
    "got",
    "loads",
    "loads",
    "compute",
    "access",
    "start",
    "continue",
    "train",
    "corpus",
    "data",
    "work",
    "ask",
    "questions",
    "saying",
    "filters",
    "finds",
    "things",
    "similar",
    "cleverly",
    "interpolates",
    "surely",
    "depends",
    "several",
    "factors",
    "depends",
    "much",
    "stuff",
    "trained",
    "depends",
    "homogeneous",
    "data",
    "depends",
    "whether",
    "critical",
    "mass",
    "things",
    "particular",
    "topic",
    "learned",
    "intuition",
    "factors",
    "sure",
    "absolutely",
    "mean",
    "sheer",
    "number",
    "data",
    "think",
    "ultimately",
    "compressed",
    "size",
    "training",
    "corpus",
    "half",
    "terabyte",
    "correct",
    "remember",
    "paper",
    "half",
    "terabyte",
    "text",
    "right",
    "used",
    "big",
    "data",
    "sets",
    "text",
    "right",
    "tex",
    "compress",
    "text",
    "like",
    "minuscule",
    "things",
    "giant",
    "amount",
    "think",
    "like",
    "trillion",
    "tokens",
    "something",
    "yeah",
    "chances",
    "company",
    "slim",
    "well",
    "reason",
    "use",
    "start",
    "training",
    "banking",
    "want",
    "learn",
    "financial",
    "defaults",
    "people",
    "financial",
    "prospectus",
    "documents",
    "something",
    "put",
    "would",
    "drop",
    "ocean",
    "would",
    "would",
    "even",
    "move",
    "needle",
    "probably",
    "probably",
    "mean",
    "hard",
    "say",
    "probably",
    "yes",
    "hypothesizing",
    "could",
    "right",
    "maybe",
    "would",
    "super",
    "interesting",
    "company",
    "documents",
    "thing",
    "sort",
    "interpolates",
    "think",
    "fine",
    "tuning",
    "intuitive",
    "new",
    "data",
    "set",
    "parameters",
    "make",
    "sense",
    "think",
    "could",
    "put",
    "company",
    "documents",
    "context",
    "intuitively",
    "want",
    "maybe",
    "make",
    "much",
    "sense",
    "think",
    "one",
    "going",
    "take",
    "ton",
    "resources",
    "already",
    "learned",
    "best",
    "representation",
    "words",
    "yeah",
    "question",
    "probably",
    "learned",
    "best",
    "representation",
    "words",
    "general",
    "internet",
    "wikipedia",
    "things",
    "like",
    "going",
    "see",
    "era",
    "right",
    "bert",
    "take",
    "checkpoint",
    "comes",
    "wikipedia",
    "specifically",
    "task",
    "right",
    "label",
    "whatever",
    "want",
    "maybe",
    "see",
    "era",
    "take",
    "gpt",
    "three",
    "something",
    "language",
    "model",
    "continue",
    "language",
    "model",
    "data",
    "context",
    "learning",
    "actually",
    "answer",
    "task",
    "mean",
    "entirely",
    "entirely",
    "possible",
    "one",
    "drawbacks",
    "ask",
    "gpt",
    "question",
    "give",
    "kind",
    "abstractive",
    "answer",
    "probably",
    "want",
    "business",
    "context",
    "extractive",
    "answer",
    "yeah",
    "mean",
    "trick",
    "paper",
    "example",
    "sentiment",
    "classification",
    "want",
    "word",
    "want",
    "word",
    "want",
    "word",
    "either",
    "positive",
    "negative",
    "right",
    "actually",
    "restrict",
    "beam",
    "search",
    "output",
    "words",
    "ask",
    "one",
    "probable",
    "whenever",
    "phrase",
    "question",
    "let",
    "say",
    "multiple",
    "choice",
    "bunch",
    "answers",
    "ask",
    "one",
    "answers",
    "probable",
    "sort",
    "yeah",
    "yeah",
    "another",
    "paper",
    "really",
    "like",
    "pattern",
    "exploiting",
    "training",
    "idea",
    "take",
    "expression",
    "append",
    "task",
    "give",
    "better",
    "use",
    "language",
    "model",
    "answering",
    "question",
    "patterns",
    "like",
    "like",
    "yelp",
    "review",
    "would",
    "add",
    "mask",
    "get",
    "label",
    "language",
    "model",
    "yeah",
    "yes",
    "idea",
    "coming",
    "prompts",
    "problem",
    "manually",
    "find",
    "prompts",
    "pattern",
    "exploding",
    "training",
    "try",
    "automatic",
    "verbalizer",
    "search",
    "trying",
    "find",
    "patterns",
    "automatically",
    "still",
    "look",
    "like",
    "way",
    "taking",
    "human",
    "loop",
    "kind",
    "additional",
    "context",
    "give",
    "give",
    "better",
    "answer",
    "abstractively",
    "goes",
    "two",
    "directions",
    "math",
    "examples",
    "quite",
    "prominent",
    "paper",
    "feel",
    "lot",
    "people",
    "talk",
    "know",
    "views",
    "happening",
    "right",
    "already",
    "said",
    "beginning",
    "tim",
    "like",
    "strings",
    "right",
    "would",
    "somehow",
    "learn",
    "decimal",
    "system",
    "weird",
    "yet",
    "thoughts",
    "math",
    "gpt",
    "able",
    "math",
    "think",
    "completely",
    "agree",
    "assessment",
    "black",
    "white",
    "well",
    "maybe",
    "memorization",
    "nuance",
    "learning",
    "kind",
    "internal",
    "structure",
    "said",
    "days",
    "gone",
    "past",
    "would",
    "manually",
    "create",
    "knowledge",
    "graphs",
    "inside",
    "minds",
    "maybe",
    "two",
    "systems",
    "spoken",
    "system",
    "one",
    "system",
    "two",
    "verbalize",
    "knowledge",
    "certain",
    "structure",
    "capturing",
    "extent",
    "transformers",
    "model",
    "replicating",
    "sure",
    "yeah",
    "mean",
    "also",
    "gotten",
    "lot",
    "pushback",
    "like",
    "already",
    "half",
    "convinced",
    "happening",
    "still",
    "fully",
    "convinced",
    "found",
    "good",
    "good",
    "comment",
    "humans",
    "example",
    "also",
    "lot",
    "math",
    "memorization",
    "like",
    "remember",
    "multiplication",
    "tables",
    "know",
    "10",
    "10",
    "perform",
    "math",
    "know",
    "strings",
    "basically",
    "like",
    "additions",
    "know",
    "heart",
    "like",
    "argument",
    "gpt",
    "basically",
    "learns",
    "heart",
    "math",
    "longer",
    "works",
    "go",
    "multiply",
    "three",
    "digit",
    "numbers",
    "yeah",
    "like",
    "look",
    "chart",
    "really",
    "dies",
    "works",
    "like",
    "two",
    "digit",
    "addition",
    "subtraction",
    "basically",
    "randomly",
    "get",
    "know",
    "10",
    "possible",
    "outputs",
    "randomly",
    "guessing",
    "yeah",
    "covered",
    "thing",
    "janet",
    "surely",
    "memorizing",
    "reasoning",
    "would",
    "precipitous",
    "change",
    "exactly",
    "well",
    "point",
    "basically",
    "think",
    "addition",
    "subtraction",
    "many",
    "websites",
    "looking",
    "crawl",
    "web",
    "right",
    "many",
    "websites",
    "like",
    "giant",
    "table",
    "one",
    "columns",
    "surely",
    "going",
    "sum",
    "columns",
    "right",
    "pure",
    "language",
    "modeling",
    "learn",
    "add",
    "subtract",
    "think",
    "addition",
    "subtraction",
    "high",
    "whereas",
    "much",
    "find",
    "table",
    "multiplication",
    "intrinsic",
    "structure",
    "talking",
    "websites",
    "someone",
    "specifically",
    "constructs",
    "math",
    "table",
    "like",
    "oh",
    "number",
    "people",
    "live",
    "uh",
    "west",
    "west",
    "side",
    "town",
    "number",
    "people",
    "live",
    "east",
    "side",
    "town",
    "column",
    "total",
    "number",
    "people",
    "right",
    "like",
    "learn",
    "love",
    "conception",
    "though",
    "say",
    "humans",
    "dumb",
    "right",
    "think",
    "memorize",
    "lot",
    "knowledge",
    "look",
    "way",
    "humans",
    "learn",
    "really",
    "efficient",
    "use",
    "thinking",
    "taxing",
    "us",
    "think",
    "quite",
    "often",
    "look",
    "decisions",
    "others",
    "guide",
    "behavior",
    "lots",
    "psychological",
    "biases",
    "like",
    "social",
    "proof",
    "lot",
    "time",
    "thinking",
    "also",
    "precipitous",
    "change",
    "abilities",
    "think",
    "human",
    "intelligence",
    "really",
    "general",
    "lots",
    "things",
    "ca",
    "like",
    "solving",
    "traveling",
    "salesman",
    "problem",
    "solve",
    "reasonable",
    "number",
    "cities",
    "suddenly",
    "really",
    "crap",
    "try",
    "reverse",
    "problem",
    "solve",
    "reverse",
    "finding",
    "longest",
    "possible",
    "path",
    "really",
    "bad",
    "good",
    "3d",
    "really",
    "bad",
    "4d",
    "different",
    "yeah",
    "maybe",
    "yeah",
    "mean",
    "quite",
    "big",
    "question",
    "think",
    "people",
    "explicit",
    "like",
    "said",
    "beginning",
    "explicit",
    "notion",
    "reasoning",
    "going",
    "inside",
    "gpt3",
    "like",
    "completely",
    "absent",
    "well",
    "come",
    "back",
    "system",
    "one",
    "versus",
    "system",
    "two",
    "think",
    "gpt3",
    "system",
    "one",
    "exactly",
    "describing",
    "many",
    "things",
    "without",
    "needing",
    "think",
    "consciously",
    "intelligence",
    "baked",
    "skill",
    "program",
    "program",
    "almost",
    "runs",
    "deterministically",
    "like",
    "gpt3",
    "deterministic",
    "program",
    "one",
    "course",
    "could",
    "verbalize",
    "baked",
    "back",
    "propagation",
    "program",
    "informed",
    "various",
    "inductive",
    "priors",
    "experience",
    "given",
    "fully",
    "baked",
    "system",
    "one",
    "program",
    "reasoning",
    "yeah",
    "mean",
    "actually",
    "good",
    "characterization",
    "characterize",
    "gpd3",
    "like",
    "reasoning",
    "intelligent",
    "basically",
    "saying",
    "system",
    "two",
    "exist",
    "something",
    "like",
    "completely",
    "agree",
    "probably",
    "well",
    "think",
    "like",
    "connectionist",
    "idea",
    "kind",
    "like",
    "ca",
    "store",
    "like",
    "maybe",
    "like",
    "seems",
    "like",
    "attention",
    "intermediate",
    "weights",
    "could",
    "way",
    "reasoning",
    "kind",
    "think",
    "fact",
    "trend",
    "zero",
    "shot",
    "transfer",
    "kind",
    "points",
    "direction",
    "maybe",
    "system",
    "one",
    "little",
    "bit",
    "yeah",
    "good",
    "point",
    "like",
    "attention",
    "computation",
    "form",
    "reasoning",
    "like",
    "transforming",
    "representations",
    "like",
    "meaningful",
    "way",
    "true",
    "possible",
    "reasoning",
    "context",
    "neural",
    "network",
    "think",
    "transformation",
    "yeah",
    "well",
    "mean",
    "say",
    "another",
    "way",
    "human",
    "beings",
    "create",
    "know",
    "imperative",
    "programming",
    "code",
    "constructs",
    "programming",
    "code",
    "things",
    "like",
    "looping",
    "semantics",
    "symbols",
    "logical",
    "correspondence",
    "verbalization",
    "something",
    "algorithm",
    "deep",
    "neural",
    "network",
    "completely",
    "different",
    "well",
    "know",
    "things",
    "like",
    "compute",
    "arbitrary",
    "functions",
    "like",
    "purely",
    "powerful",
    "power",
    "analysis",
    "able",
    "anything",
    "think",
    "notion",
    "people",
    "think",
    "something",
    "like",
    "reasoning",
    "happen",
    "like",
    "happens",
    "head",
    "like",
    "sit",
    "go",
    "like",
    "okay",
    "logical",
    "step",
    "one",
    "logical",
    "step",
    "two",
    "logical",
    "step",
    "three",
    "right",
    "ca",
    "almost",
    "ca",
    "imagine",
    "system",
    "reasoning",
    "think",
    "like",
    "got",
    "like",
    "module",
    "retrieve",
    "memory",
    "module",
    "update",
    "forward",
    "simulation",
    "something",
    "maybe",
    "maybe",
    "view",
    "reasoning",
    "like",
    "function",
    "actually",
    "computed",
    "exact",
    "function",
    "approximation",
    "exact",
    "function",
    "computed",
    "also",
    "represented",
    "simply",
    "forward",
    "pass",
    "attention",
    "layers",
    "entirely",
    "possible",
    "know",
    "difference",
    "representing",
    "think",
    "system",
    "two",
    "intelligence",
    "creates",
    "system",
    "one",
    "skill",
    "program",
    "new",
    "situation",
    "similar",
    "something",
    "seen",
    "need",
    "create",
    "new",
    "skill",
    "program",
    "need",
    "reasoning",
    "yeah",
    "debatable",
    "much",
    "much",
    "simply",
    "go",
    "abstract",
    "like",
    "see",
    "situation",
    "never",
    "seen",
    "sure",
    "seen",
    "abstract",
    "way",
    "like",
    "okay",
    "never",
    "london",
    "new",
    "city",
    "right",
    "never",
    "never",
    "know",
    "philippines",
    "country",
    "culture",
    "mine",
    "kind",
    "know",
    "adapt",
    "way",
    "know",
    "question",
    "really",
    "generalize",
    "situations",
    "seen",
    "ever",
    "generalize",
    "situations",
    "transfer",
    "abstract",
    "properties",
    "well",
    "situation",
    "would",
    "taxonomy",
    "skill",
    "programs",
    "learned",
    "elsewhere",
    "would",
    "degree",
    "generalizability",
    "could",
    "fall",
    "back",
    "skill",
    "programs",
    "system",
    "would",
    "learn",
    "recombine",
    "acted",
    "environment",
    "yeah",
    "another",
    "kind",
    "funny",
    "example",
    "drive",
    "headed",
    "trader",
    "joe",
    "common",
    "see",
    "long",
    "line",
    "outside",
    "trader",
    "joe",
    "know",
    "filtering",
    "keep",
    "empty",
    "come",
    "line",
    "quickly",
    "like",
    "happened",
    "line",
    "outside",
    "trader",
    "joe",
    "like",
    "never",
    "seen",
    "line",
    "outside",
    "trader",
    "joe",
    "like",
    "lined",
    "aliens",
    "building",
    "fire",
    "like",
    "like",
    "whoa",
    "distribution",
    "seen",
    "like",
    "blown",
    "mind",
    "like",
    "neural",
    "networks",
    "see",
    "crazy",
    "added",
    "distribution",
    "thing",
    "blows",
    "mind",
    "right",
    "like",
    "gpt3",
    "could",
    "least",
    "maybe",
    "start",
    "trying",
    "say",
    "something",
    "like",
    "classifiers",
    "give",
    "awful",
    "prediction",
    "abstractive",
    "generation",
    "models",
    "least",
    "start",
    "trying",
    "tell",
    "think",
    "seeing",
    "right",
    "like",
    "moving",
    "towards",
    "want",
    "let",
    "let",
    "play",
    "give",
    "let",
    "let",
    "come",
    "example",
    "something",
    "would",
    "completely",
    "distribution",
    "would",
    "gpt3",
    "think",
    "talked",
    "benjo",
    "thing",
    "human",
    "consciousness",
    "generative",
    "discriminative",
    "models",
    "least",
    "sort",
    "assume",
    "input",
    "comes",
    "distribution",
    "training",
    "distribution",
    "hard",
    "time",
    "assess",
    "input",
    "getting",
    "improbable",
    "right",
    "gpt3",
    "could",
    "least",
    "say",
    "something",
    "like",
    "getting",
    "context",
    "improbable",
    "yeah",
    "makes",
    "makes",
    "case",
    "gpt3",
    "could",
    "actually",
    "like",
    "whoa",
    "context",
    "something",
    "expect",
    "right",
    "give",
    "novel",
    "task",
    "also",
    "something",
    "necessarily",
    "expect",
    "know",
    "good",
    "point",
    "gpd3",
    "might",
    "actually",
    "able",
    "recognize",
    "distribution",
    "well",
    "intuition",
    "give",
    "example",
    "well",
    "manifold",
    "presumably",
    "combinatorially",
    "many",
    "permutation",
    "many",
    "things",
    "way",
    "manifold",
    "things",
    "manifold",
    "gave",
    "something",
    "manifold",
    "would",
    "know",
    "would",
    "generate",
    "garbage",
    "likely",
    "humans",
    "put",
    "situation",
    "completely",
    "lost",
    "either",
    "like",
    "freeze",
    "run",
    "cry",
    "mean",
    "difference",
    "humans",
    "first",
    "would",
    "imagine",
    "taxonomy",
    "skill",
    "programs",
    "skill",
    "programs",
    "generalize",
    "much",
    "whereas",
    "deep",
    "learning",
    "manifold",
    "learned",
    "know",
    "energy",
    "based",
    "surface",
    "talking",
    "small",
    "give",
    "something",
    "slightly",
    "manifold",
    "lost",
    "think",
    "like",
    "one",
    "paper",
    "see",
    "screen",
    "trying",
    "like",
    "mix",
    "data",
    "like",
    "explain",
    "task",
    "would",
    "like",
    "explain",
    "nli",
    "premise",
    "want",
    "give",
    "answer",
    "explain",
    "think",
    "could",
    "promising",
    "direction",
    "towards",
    "like",
    "added",
    "distribution",
    "like",
    "explanation",
    "get",
    "sense",
    "said",
    "something",
    "like",
    "explain",
    "give",
    "something",
    "completely",
    "unexplainable",
    "yeah",
    "maybe",
    "like",
    "way",
    "getting",
    "sense",
    "thinking",
    "maybe",
    "mean",
    "would",
    "still",
    "would",
    "require",
    "human",
    "interpret",
    "model",
    "says",
    "right",
    "model",
    "would",
    "output",
    "words",
    "explain",
    "something",
    "like",
    "sure",
    "model",
    "would",
    "still",
    "distribution",
    "fact",
    "produces",
    "still",
    "means",
    "actually",
    "answer",
    "question",
    "guess",
    "different",
    "question",
    "asking",
    "happens",
    "input",
    "like",
    "completely",
    "distribution",
    "model",
    "would",
    "argue",
    "going",
    "fall",
    "back",
    "pro",
    "conditioning",
    "information",
    "weak",
    "signal",
    "going",
    "fall",
    "back",
    "prior",
    "english",
    "language",
    "probably",
    "going",
    "produce",
    "probable",
    "sentence",
    "like",
    "know",
    "think",
    "google",
    "introduced",
    "heard",
    "public",
    "talk",
    "introduced",
    "autorespond",
    "autoresponder",
    "quick",
    "response",
    "gmail",
    "right",
    "three",
    "things",
    "click",
    "one",
    "one",
    "tried",
    "think",
    "tried",
    "generative",
    "first",
    "turned",
    "would",
    "always",
    "start",
    "sorry",
    "late",
    "reply",
    "laughter",
    "like",
    "whatever",
    "whatever",
    "input",
    "like",
    "sorry",
    "late",
    "response",
    "yes",
    "maybe",
    "prior",
    "overwhelming",
    "case",
    "input",
    "make",
    "sense",
    "gon",
    "na",
    "fall",
    "back",
    "prior",
    "really",
    "interesting",
    "bring",
    "would",
    "expectation",
    "surely",
    "models",
    "would",
    "something",
    "like",
    "time",
    "surprising",
    "look",
    "gpt3",
    "paper",
    "gives",
    "incredible",
    "results",
    "yes",
    "apparently",
    "hacking",
    "tweaking",
    "massaging",
    "surprising",
    "well",
    "somewhat",
    "surprising",
    "like",
    "surprising",
    "language",
    "modeling",
    "context",
    "right",
    "wants",
    "better",
    "random",
    "better",
    "going",
    "start",
    "paying",
    "attention",
    "context",
    "way",
    "gets",
    "better",
    "like",
    "lower",
    "loss",
    "saying",
    "actually",
    "generalizes",
    "give",
    "contexts",
    "something",
    "useful",
    "falls",
    "back",
    "would",
    "interesting",
    "experimentally",
    "see",
    "far",
    "push",
    "far",
    "push",
    "garbage",
    "context",
    "actually",
    "breaks",
    "always",
    "gets",
    "back",
    "probable",
    "sentence",
    "possible",
    "conflate",
    "auto",
    "aggressive",
    "model",
    "encoded",
    "know",
    "like",
    "denoising",
    "auto",
    "encoder",
    "type",
    "model",
    "bro",
    "broaden",
    "question",
    "barking",
    "right",
    "tree",
    "think",
    "type",
    "architecture",
    "looking",
    "well",
    "guess",
    "depends",
    "want",
    "right",
    "enormously",
    "good",
    "language",
    "model",
    "throw",
    "power",
    "predicting",
    "next",
    "word",
    "like",
    "bert",
    "know",
    "like",
    "predict",
    "words",
    "time",
    "whether",
    "two",
    "sentences",
    "follow",
    "one",
    "another",
    "uh",
    "like",
    "full",
    "blast",
    "next",
    "word",
    "thing",
    "matters",
    "suppose",
    "intuition",
    "model",
    "would",
    "learn",
    "structure",
    "language",
    "easier",
    "train",
    "model",
    "fair",
    "maybe",
    "like",
    "scale",
    "matter",
    "anymore",
    "like",
    "mean",
    "like",
    "know",
    "scale",
    "know",
    "unidirectional",
    "okay",
    "trained",
    "tons",
    "data",
    "bur",
    "horrible",
    "quadratic",
    "layer",
    "wise",
    "time",
    "complexity",
    "truncate",
    "input",
    "size",
    "restriction",
    "would",
    "would",
    "imagine",
    "could",
    "train",
    "much",
    "longer",
    "input",
    "size",
    "model",
    "surely",
    "would",
    "give",
    "much",
    "better",
    "results",
    "auto",
    "aggressive",
    "model",
    "well",
    "aggressive",
    "one",
    "complexity",
    "like",
    "also",
    "transformer",
    "quadratic",
    "complexity",
    "much",
    "much",
    "longer",
    "impact",
    "think",
    "like",
    "original",
    "transformer",
    "original",
    "attention",
    "need",
    "paper",
    "two",
    "inputs",
    "right",
    "encoding",
    "part",
    "decoding",
    "part",
    "internal",
    "attention",
    "case",
    "encode",
    "translate",
    "thing",
    "encoding",
    "part",
    "would",
    "context",
    "whatever",
    "give",
    "us",
    "context",
    "decoding",
    "part",
    "would",
    "whatever",
    "output",
    "far",
    "construct",
    "different",
    "ways",
    "best",
    "going",
    "basically",
    "size",
    "thing",
    "bert",
    "everything",
    "could",
    "attend",
    "everything",
    "every",
    "single",
    "layer",
    "attention",
    "internally",
    "like",
    "gets",
    "like",
    "square",
    "half",
    "hat",
    "half",
    "squared",
    "like",
    "significantly",
    "reducing",
    "complexity",
    "transformer",
    "limitations",
    "reason",
    "take",
    "2",
    "000",
    "tokens",
    "input",
    "microsoft",
    "built",
    "like",
    "big",
    "computer",
    "reason",
    "longer",
    "input",
    "size",
    "longer",
    "point",
    "worse",
    "good",
    "question",
    "models",
    "like",
    "lstms",
    "answer",
    "definitely",
    "yes",
    "point",
    "much",
    "information",
    "know",
    "encode",
    "much",
    "hidden",
    "state",
    "point",
    "overload",
    "still",
    "think",
    "yes",
    "transformers",
    "like",
    "throw",
    "information",
    "information",
    "especially",
    "correlated",
    "information",
    "going",
    "point",
    "detrimental",
    "something",
    "like",
    "cnn",
    "example",
    "receptive",
    "field",
    "means",
    "feed",
    "much",
    "information",
    "want",
    "use",
    "receptive",
    "field",
    "similar",
    "concept",
    "transformers",
    "even",
    "fed",
    "huge",
    "amount",
    "information",
    "attention",
    "mechanism",
    "would",
    "pay",
    "attention",
    "substructures",
    "symmetries",
    "would",
    "ignore",
    "everything",
    "else",
    "kind",
    "would",
    "matter",
    "fed",
    "information",
    "yes",
    "possibly",
    "mean",
    "like",
    "problem",
    "see",
    "really",
    "input",
    "correlated",
    "information",
    "might",
    "might",
    "know",
    "simply",
    "linear",
    "regressions",
    "correlated",
    "features",
    "logistic",
    "regressions",
    "horrible",
    "could",
    "technically",
    "two",
    "correlated",
    "features",
    "could",
    "technically",
    "predict",
    "output",
    "one",
    "due",
    "noise",
    "know",
    "stochastic",
    "procedure",
    "one",
    "step",
    "going",
    "push",
    "one",
    "one",
    "step",
    "going",
    "exact",
    "opposite",
    "like",
    "like",
    "ah",
    "pay",
    "attention",
    "works",
    "well",
    "works",
    "well",
    "well",
    "end",
    "like",
    "fuzzy",
    "confused",
    "model",
    "um",
    "think",
    "definitely",
    "point",
    "much",
    "information",
    "conditioning",
    "becomes",
    "detrimental",
    "well",
    "think",
    "even",
    "show",
    "paper",
    "examples",
    "help",
    "like",
    "demonstrations",
    "also",
    "think",
    "reason",
    "also",
    "limit",
    "training",
    "corpus",
    "filter",
    "garbage",
    "samples",
    "right",
    "sort",
    "thing",
    "want",
    "high",
    "quality",
    "data",
    "actually",
    "predict",
    "something",
    "another",
    "observation",
    "like",
    "images",
    "cnn",
    "inducted",
    "prior",
    "presupposed",
    "local",
    "connectivity",
    "pixels",
    "important",
    "global",
    "connectivity",
    "important",
    "documents",
    "example",
    "look",
    "average",
    "long",
    "document",
    "probably",
    "big",
    "point",
    "huge",
    "receptive",
    "field",
    "would",
    "wasting",
    "potential",
    "attention",
    "points",
    "tokens",
    "document",
    "long",
    "maybe",
    "documents",
    "huge",
    "maybe",
    "infinite",
    "receptive",
    "field",
    "documents",
    "could",
    "attend",
    "talking",
    "similar",
    "things",
    "guess",
    "trying",
    "reason",
    "trying",
    "trying",
    "play",
    "little",
    "bit",
    "point",
    "go",
    "past",
    "average",
    "document",
    "length",
    "would",
    "wasting",
    "representational",
    "capacity",
    "maybe",
    "infinitely",
    "increased",
    "would",
    "good",
    "thing",
    "would",
    "encounter",
    "things",
    "could",
    "attend",
    "useful",
    "way",
    "well",
    "well",
    "point",
    "guess",
    "much",
    "longer",
    "receptive",
    "field",
    "could",
    "input",
    "one",
    "document",
    "question",
    "documents",
    "input",
    "together",
    "ca",
    "take",
    "two",
    "random",
    "ones",
    "teach",
    "model",
    "second",
    "one",
    "follows",
    "first",
    "one",
    "right",
    "thinks",
    "like",
    "oh",
    "sequence",
    "happens",
    "nature",
    "input",
    "two",
    "like",
    "random",
    "things",
    "one",
    "another",
    "also",
    "confused",
    "probably",
    "like",
    "keep",
    "mind",
    "transformers",
    "explicit",
    "actually",
    "explicit",
    "constraint",
    "input",
    "length",
    "could",
    "actually",
    "technically",
    "input",
    "transformer",
    "invert",
    "right",
    "could",
    "input",
    "uh",
    "sequence",
    "length",
    "10",
    "sequence",
    "length",
    "10",
    "first",
    "problem",
    "10",
    "000",
    "sequence",
    "length",
    "position",
    "encodings",
    "trained",
    "like",
    "minor",
    "point",
    "major",
    "point",
    "gpu",
    "going",
    "explode",
    "model",
    "transformer",
    "notion",
    "sequence",
    "length",
    "set",
    "comp",
    "set",
    "computation",
    "algorithm",
    "sees",
    "elements",
    "set",
    "yeah",
    "yeah",
    "exactly",
    "right",
    "positional",
    "encodings",
    "kind",
    "like",
    "sincoidal",
    "functions",
    "way",
    "knows",
    "one",
    "thing",
    "another",
    "thing",
    "right",
    "put",
    "one",
    "document",
    "time",
    "ca",
    "really",
    "impute",
    "ordinal",
    "relationship",
    "documents",
    "mean",
    "maybe",
    "could",
    "could",
    "invent",
    "could",
    "say",
    "well",
    "comes",
    "wikipedia",
    "bucket",
    "one",
    "created",
    "wednesday",
    "one",
    "thursday",
    "interesting",
    "thinking",
    "point",
    "though",
    "could",
    "get",
    "layers",
    "transformer",
    "model",
    "attend",
    "layers",
    "another",
    "document",
    "see",
    "mean",
    "another",
    "similar",
    "document",
    "want",
    "able",
    "attend",
    "document",
    "could",
    "dynamically",
    "pull",
    "document",
    "encoded",
    "could",
    "locally",
    "sensitive",
    "hashing",
    "something",
    "would",
    "good",
    "way",
    "go",
    "well",
    "sounds",
    "like",
    "awesome",
    "could",
    "could",
    "could",
    "like",
    "forward",
    "prop",
    "reach",
    "basically",
    "come",
    "done",
    "people",
    "helpful",
    "like",
    "comments",
    "helpful",
    "youtube",
    "comments",
    "like",
    "way",
    "helpful",
    "reviews",
    "papers",
    "people",
    "pointed",
    "done",
    "said",
    "surely",
    "know",
    "model",
    "learning",
    "interpolate",
    "training",
    "dates",
    "fuzzy",
    "way",
    "able",
    "output",
    "generate",
    "pinpoint",
    "back",
    "like",
    "five",
    "training",
    "examples",
    "led",
    "making",
    "decision",
    "right",
    "maybe",
    "spin",
    "bit",
    "say",
    "okay",
    "could",
    "could",
    "like",
    "forward",
    "prop",
    "could",
    "retrieve",
    "training",
    "documents",
    "relevant",
    "put",
    "context",
    "know",
    "able",
    "explicitly",
    "attend",
    "weights",
    "sort",
    "like",
    "right",
    "trying",
    "distill",
    "knowledge",
    "arguing",
    "mem",
    "fuzzy",
    "memorization",
    "training",
    "data",
    "weights",
    "transformer",
    "build",
    "architecture",
    "like",
    "programming",
    "would",
    "like",
    "database",
    "could",
    "retrieve",
    "things",
    "know",
    "attend",
    "basically",
    "trying",
    "build",
    "logic",
    "knowledge",
    "weight",
    "connections",
    "think",
    "missing",
    "explicit",
    "memory",
    "modules",
    "store",
    "retrieve",
    "information",
    "transformer",
    "computation",
    "information",
    "rather",
    "like",
    "matching",
    "query",
    "nearest",
    "neighbor",
    "training",
    "data",
    "would",
    "like",
    "path",
    "transformer",
    "made",
    "influence",
    "prediction",
    "gon",
    "na",
    "see",
    "like",
    "caused",
    "biggest",
    "like",
    "magnitude",
    "gradient",
    "change",
    "training",
    "like",
    "mini",
    "batch",
    "clue",
    "like",
    "think",
    "like",
    "saying",
    "tim",
    "like",
    "super",
    "interesting",
    "feel",
    "sort",
    "reach",
    "right",
    "computational",
    "right",
    "capabilities",
    "like",
    "super",
    "fun",
    "think",
    "could",
    "yeah",
    "next",
    "step",
    "transformers",
    "love",
    "explore",
    "said",
    "want",
    "know",
    "output",
    "got",
    "examples",
    "trained",
    "led",
    "output",
    "vision",
    "models",
    "thing",
    "chris",
    "ola",
    "feature",
    "visualization",
    "say",
    "well",
    "output",
    "neuron",
    "going",
    "solve",
    "optimization",
    "problem",
    "generate",
    "input",
    "maximally",
    "activates",
    "neuron",
    "quite",
    "good",
    "way",
    "going",
    "backwards",
    "vision",
    "architectures",
    "sure",
    "setting",
    "would",
    "work",
    "transformers",
    "model",
    "would",
    "find",
    "intermediate",
    "representations",
    "different",
    "inputs",
    "would",
    "trace",
    "back",
    "output",
    "would",
    "vector",
    "search",
    "would",
    "find",
    "ones",
    "similar",
    "talking",
    "something",
    "quite",
    "interesting",
    "said",
    "basically",
    "memorization",
    "machine",
    "actually",
    "took",
    "next",
    "level",
    "created",
    "symbolic",
    "reasoning",
    "explicitly",
    "made",
    "memorization",
    "machine",
    "took",
    "representation",
    "stored",
    "memory",
    "turned",
    "computer",
    "program",
    "essentially",
    "kind",
    "hybrid",
    "would",
    "interesting",
    "would",
    "give",
    "us",
    "transparency",
    "way",
    "want",
    "yeah",
    "mean",
    "one",
    "one",
    "next",
    "thoughts",
    "spin",
    "exactly",
    "direction",
    "like",
    "basically",
    "argument",
    "train",
    "whatever",
    "logistic",
    "regression",
    "one",
    "smaller",
    "vision",
    "models",
    "learn",
    "map",
    "features",
    "outputs",
    "way",
    "abstract",
    "go",
    "larger",
    "larger",
    "larger",
    "amount",
    "weights",
    "know",
    "trend",
    "larger",
    "models",
    "output",
    "models",
    "produce",
    "strongly",
    "feel",
    "actually",
    "memorizing",
    "training",
    "data",
    "sort",
    "fuzzy",
    "way",
    "described",
    "opposed",
    "extracting",
    "features",
    "predicting",
    "based",
    "maybe",
    "basic",
    "imagination",
    "would",
    "something",
    "like",
    "training",
    "like",
    "build",
    "reverse",
    "index",
    "weights",
    "training",
    "examples",
    "said",
    "oh",
    "passed",
    "training",
    "example",
    "influenced",
    "weight",
    "lot",
    "right",
    "index",
    "weight",
    "index",
    "back",
    "maybe",
    "could",
    "update",
    "training",
    "see",
    "samples",
    "index",
    "update",
    "weight",
    "even",
    "like",
    "kick",
    "first",
    "one",
    "basically",
    "reverse",
    "index",
    "forward",
    "prop",
    "test",
    "sample",
    "context",
    "question",
    "would",
    "simply",
    "observe",
    "maybe",
    "self",
    "gradient",
    "gradient",
    "output",
    "would",
    "observe",
    "like",
    "forward",
    "prop",
    "signal",
    "see",
    "ah",
    "connections",
    "really",
    "important",
    "activated",
    "right",
    "let",
    "go",
    "back",
    "reverse",
    "index",
    "see",
    "training",
    "examples",
    "led",
    "feel",
    "could",
    "first",
    "explainability",
    "wise",
    "could",
    "massive",
    "done",
    "like",
    "people",
    "tell",
    "done",
    "read",
    "uh",
    "interested",
    "check",
    "work",
    "done",
    "could",
    "super",
    "interesting",
    "see",
    "ah",
    "okay",
    "model",
    "know",
    "basically",
    "explained",
    "example",
    "like",
    "oh",
    "output",
    "yeah",
    "know",
    "things",
    "base",
    "second",
    "could",
    "actually",
    "next",
    "sort",
    "search",
    "engine",
    "right",
    "like",
    "fuzzy",
    "search",
    "engine",
    "documents",
    "throw",
    "documents",
    "know",
    "type",
    "something",
    "pop",
    "oh",
    "things",
    "found",
    "computer",
    "relevant",
    "got",
    "continuum",
    "one",
    "side",
    "continuum",
    "memorizing",
    "stuff",
    "gbt3",
    "video",
    "gave",
    "example",
    "utterance",
    "close",
    "article",
    "could",
    "find",
    "google",
    "sophisticated",
    "abstractly",
    "generating",
    "tech",
    "far",
    "nuanced",
    "intelligent",
    "realize",
    "would",
    "measure",
    "hard",
    "question",
    "know",
    "pro",
    "probably",
    "would",
    "measure",
    "yeah",
    "abstractly",
    "interpolating",
    "training",
    "data",
    "ultimately",
    "see",
    "really",
    "working",
    "level",
    "idea",
    "abstract",
    "concepts",
    "hard",
    "devise",
    "test",
    "think",
    "outputs",
    "gpt3",
    "right",
    "clearly",
    "see",
    "news",
    "articles",
    "example",
    "probably",
    "taken",
    "bunch",
    "pushed",
    "together",
    "grammatical",
    "structures",
    "learned",
    "yeah",
    "hard",
    "test",
    "would",
    "know",
    "yeah",
    "metrics",
    "rubbish",
    "yes",
    "makes",
    "think",
    "went",
    "back",
    "first",
    "principles",
    "much",
    "simpler",
    "language",
    "model",
    "generated",
    "corpus",
    "using",
    "grammar",
    "could",
    "come",
    "kind",
    "level",
    "abstractive",
    "text",
    "generation",
    "could",
    "understand",
    "factors",
    "influenced",
    "suppose",
    "moment",
    "using",
    "huge",
    "corpus",
    "huge",
    "language",
    "model",
    "impossible",
    "us",
    "would",
    "generate",
    "anything",
    "would",
    "really",
    "good",
    "impossible",
    "us",
    "reason",
    "measure",
    "abstractness",
    "intelligence",
    "generation",
    "sure",
    "mean",
    "agree",
    "kind",
    "thing",
    "abstract",
    "task",
    "know",
    "created",
    "question",
    "know",
    "like",
    "real",
    "world",
    "yeah",
    "eternal",
    "question",
    "people",
    "work",
    "toy",
    "problems",
    "understand",
    "people",
    "work",
    "real",
    "world",
    "like",
    "know",
    "real",
    "world",
    "yeah",
    "think",
    "great",
    "example",
    "construct",
    "toy",
    "data",
    "set",
    "charlie",
    "big",
    "charlie",
    "blue",
    "learn",
    "combine",
    "rules",
    "start",
    "scale",
    "simple",
    "toy",
    "thing",
    "complex",
    "language",
    "try",
    "take",
    "question",
    "generation",
    "agree",
    "tim",
    "think",
    "need",
    "better",
    "generalization",
    "probes",
    "mean",
    "like",
    "better",
    "tests",
    "even",
    "like",
    "toy",
    "data",
    "set",
    "know",
    "layer",
    "level",
    "complexity",
    "know",
    "match",
    "something",
    "like",
    "squad",
    "complex",
    "data",
    "set",
    "yeah",
    "yeah",
    "intuition",
    "would",
    "even",
    "start",
    "one",
    "one",
    "thing",
    "could",
    "let",
    "assume",
    "training",
    "data",
    "perfectly",
    "spaced",
    "language",
    "manifold",
    "could",
    "start",
    "inputting",
    "article",
    "course",
    "going",
    "complete",
    "article",
    "mixture",
    "two",
    "articles",
    "next",
    "manifold",
    "presumably",
    "would",
    "interpolate",
    "halfway",
    "two",
    "articles",
    "see",
    "mean",
    "need",
    "come",
    "framework",
    "reasoning",
    "behavior",
    "feels",
    "million",
    "miles",
    "away",
    "yeah",
    "absolutely",
    "like",
    "nearest",
    "neighbor",
    "probes",
    "kind",
    "know",
    "think",
    "popular",
    "visualization",
    "love",
    "go",
    "see",
    "similar",
    "document",
    "representation",
    "think",
    "images",
    "little",
    "easier",
    "think",
    "know",
    "dog",
    "image",
    "similar",
    "entanglement",
    "k",
    "nearest",
    "neighbors",
    "top",
    "representation",
    "think",
    "yeah",
    "interesting",
    "could",
    "radioactive",
    "data",
    "think",
    "seen",
    "seems",
    "like",
    "much",
    "something",
    "title",
    "something",
    "would",
    "go",
    "direction",
    "yeah",
    "thought",
    "paper",
    "really",
    "interesting",
    "like",
    "take",
    "like",
    "uh",
    "batch",
    "sgd",
    "batch",
    "gon",
    "na",
    "like",
    "put",
    "signature",
    "data",
    "comes",
    "model",
    "like",
    "put",
    "little",
    "encodings",
    "like",
    "know",
    "like",
    "kind",
    "put",
    "little",
    "encryptions",
    "images",
    "like",
    "watermark",
    "know",
    "show",
    "maybe",
    "like",
    "thinking",
    "listening",
    "idea",
    "maybe",
    "could",
    "like",
    "sign",
    "mini",
    "batches",
    "text",
    "data",
    "token",
    "tokenizations",
    "maybe",
    "something",
    "like",
    "yeah",
    "smart",
    "mean",
    "probably",
    "also",
    "want",
    "detect",
    "personal",
    "data",
    "used",
    "train",
    "particular",
    "model",
    "something",
    "like",
    "mean",
    "yeah",
    "yeah",
    "could",
    "way",
    "find",
    "way",
    "back",
    "original",
    "training",
    "examples",
    "gave",
    "rise",
    "something",
    "yeah",
    "quite",
    "smart",
    "yeah",
    "bit",
    "like",
    "adding",
    "watermark",
    "yes",
    "yeah",
    "intuition",
    "though",
    "even",
    "cnn",
    "architecture",
    "transformers",
    "architecture",
    "well",
    "clever",
    "taking",
    "shortcuts",
    "watermark",
    "would",
    "represent",
    "different",
    "path",
    "network",
    "would",
    "necessarily",
    "help",
    "locate",
    "thing",
    "interested",
    "would",
    "different",
    "thing",
    "network",
    "use",
    "watermark",
    "lot",
    "different",
    "places",
    "would",
    "quite",
    "well",
    "path",
    "network",
    "would",
    "picture",
    "mountain",
    "article",
    "mountain",
    "would",
    "pool",
    "article",
    "watermark",
    "created",
    "makes",
    "sense",
    "imagine",
    "lot",
    "overlap",
    "like",
    "easily",
    "find",
    "adversarial",
    "example",
    "also",
    "easily",
    "encode",
    "watermark",
    "would",
    "trade",
    "like",
    "gon",
    "na",
    "ca",
    "like",
    "adversarial",
    "robustness",
    "also",
    "able",
    "put",
    "watermarks",
    "network",
    "right",
    "yeah",
    "mean",
    "watermark",
    "gon",
    "na",
    "goal",
    "really",
    "someone",
    "trains",
    "data",
    "going",
    "spot",
    "output",
    "guess",
    "watermark",
    "almost",
    "equal",
    "adversarial",
    "example",
    "want",
    "kind",
    "align",
    "features",
    "know",
    "output",
    "weird",
    "read",
    "particular",
    "paper",
    "pretty",
    "sure",
    "mechanisms",
    "going",
    "exactly",
    "adversarial",
    "examples",
    "going",
    "ask",
    "yannick",
    "comments",
    "got",
    "noticed",
    "chalet",
    "video",
    "surprised",
    "see",
    "people",
    "uh",
    "laying",
    "chile",
    "chile",
    "spiritual",
    "deity",
    "pissed",
    "turns",
    "people",
    "different",
    "opinions",
    "measuring",
    "intelligence",
    "uh",
    "would",
    "thought",
    "comments",
    "get",
    "gpt3",
    "paper",
    "charlay",
    "thing",
    "nothing",
    "shirley",
    "comments",
    "gpt3",
    "paper",
    "well",
    "saying",
    "exasperated",
    "read",
    "comments",
    "yeah",
    "chalet",
    "would",
    "like",
    "say",
    "officially",
    "chalet",
    "man",
    "one",
    "pillars",
    "deep",
    "learning",
    "community",
    "anyone",
    "wants",
    "go",
    "chalet",
    "come",
    "first",
    "normal",
    "know",
    "mean",
    "got",
    "bit",
    "france",
    "sure",
    "get",
    "back",
    "chile",
    "next",
    "week",
    "keep",
    "powder",
    "dry",
    "chili",
    "going",
    "chalet",
    "justice",
    "next",
    "week",
    "meantime",
    "yeah",
    "comments",
    "get",
    "gpt3",
    "guess",
    "people",
    "like",
    "substantial",
    "comments",
    "talking",
    "reasoning",
    "ability",
    "either",
    "agreeing",
    "disagreeing",
    "know",
    "reasoning",
    "specifically",
    "like",
    "math",
    "part",
    "know",
    "know",
    "feasible",
    "learns",
    "language",
    "modeling",
    "task",
    "yeah",
    "lot",
    "lot",
    "opinions",
    "mean",
    "learned",
    "lot",
    "reading",
    "comments",
    "pretty",
    "pretty",
    "cool",
    "thought",
    "think",
    "got",
    "got",
    "zero",
    "comments",
    "news",
    "thing",
    "right",
    "people",
    "seen",
    "basically",
    "gpt",
    "three",
    "asked",
    "complete",
    "news",
    "article",
    "think",
    "mormon",
    "church",
    "something",
    "ordaining",
    "lgbtq",
    "ministers",
    "split",
    "church",
    "asked",
    "complete",
    "news",
    "article",
    "like",
    "title",
    "like",
    "first",
    "paragraph",
    "could",
    "basically",
    "show",
    "could",
    "find",
    "say",
    "well",
    "training",
    "data",
    "news",
    "article",
    "training",
    "data",
    "got",
    "title",
    "subtitle",
    "basically",
    "able",
    "show",
    "like",
    "lot",
    "think",
    "shown",
    "one",
    "found",
    "like",
    "bunch",
    "news",
    "article",
    "thing",
    "books",
    "quoted",
    "quoted",
    "verbatim",
    "basically",
    "books",
    "source",
    "news",
    "article",
    "using",
    "like",
    "extremely",
    "similar",
    "language",
    "describe",
    "like",
    "sentence",
    "structure",
    "couple",
    "words",
    "switched",
    "feel",
    "deduplication",
    "efforts",
    "show",
    "look",
    "produce",
    "news",
    "article",
    "super",
    "convinced",
    "think",
    "kind",
    "seen",
    "almost",
    "news",
    "article",
    "know",
    "couple",
    "things",
    "though",
    "one",
    "thing",
    "think",
    "good",
    "quite",
    "specific",
    "event",
    "would",
    "imagine",
    "many",
    "news",
    "articles",
    "talking",
    "event",
    "impressed",
    "huge",
    "language",
    "model",
    "know",
    "talking",
    "specificity",
    "diversity",
    "scale",
    "needed",
    "move",
    "needle",
    "large",
    "language",
    "model",
    "industry",
    "wanted",
    "learn",
    "financial",
    "defaults",
    "something",
    "banking",
    "asserted",
    "corpus",
    "would",
    "push",
    "needle",
    "seems",
    "suggest",
    "push",
    "needle",
    "small",
    "number",
    "training",
    "examples",
    "presumably",
    "learning",
    "taxonomy",
    "lots",
    "concepts",
    "knows",
    "lgbt",
    "knows",
    "churches",
    "knows",
    "know",
    "things",
    "activating",
    "network",
    "kind",
    "combined",
    "together",
    "learned",
    "retrieve",
    "given",
    "one",
    "example",
    "sure",
    "learns",
    "concepts",
    "mean",
    "extreme",
    "would",
    "simply",
    "say",
    "title",
    "training",
    "example",
    "three",
    "training",
    "examples",
    "match",
    "title",
    "interpolate",
    "whatever",
    "actually",
    "make",
    "credible",
    "claim",
    "happening",
    "one",
    "articles",
    "found",
    "google",
    "books",
    "google",
    "books",
    "one",
    "corpuses",
    "used",
    "prom",
    "like",
    "dominant",
    "fashion",
    "use",
    "like",
    "different",
    "weights",
    "different",
    "corp",
    "corpora",
    "corporacies",
    "corporate",
    "okay",
    "google",
    "books",
    "pretty",
    "weighted",
    "arguably",
    "seen",
    "particular",
    "example",
    "multiple",
    "times",
    "given",
    "much",
    "weight",
    "right",
    "possible",
    "used",
    "bunch",
    "weights",
    "specifically",
    "similar",
    "similar",
    "articles",
    "really",
    "know",
    "lgbtq",
    "really",
    "know",
    "mormon",
    "means",
    "sense",
    "right",
    "yeah",
    "kind",
    "two",
    "extremes",
    "gotten",
    "many",
    "comments",
    "interpreted",
    "one",
    "one",
    "wants",
    "debate",
    "also",
    "observation",
    "know",
    "saying",
    "humans",
    "um",
    "like",
    "drones",
    "really",
    "think",
    "example",
    "go",
    "write",
    "article",
    "cnns",
    "right",
    "would",
    "would",
    "go",
    "read",
    "book",
    "cnns",
    "would",
    "go",
    "read",
    "articles",
    "cnns",
    "would",
    "exactly",
    "gbt3",
    "argue",
    "agency",
    "autonomy",
    "limits",
    "yes",
    "news",
    "reporter",
    "know",
    "sees",
    "things",
    "happening",
    "synthesizes",
    "know",
    "news",
    "story",
    "right",
    "als",
    "already",
    "many",
    "texts",
    "cnns",
    "nothing",
    "really",
    "left",
    "interpolating",
    "new",
    "journalist",
    "know",
    "give",
    "title",
    "say",
    "please",
    "even",
    "allowed",
    "invent",
    "essay",
    "says",
    "title",
    "essay",
    "right",
    "going",
    "come",
    "something",
    "much",
    "less",
    "interpolated",
    "gpt3",
    "would",
    "argue",
    "going",
    "come",
    "something",
    "interpolated",
    "maybe",
    "concept",
    "space",
    "like",
    "going",
    "tell",
    "story",
    "beginning",
    "conflict",
    "conflict",
    "gets",
    "resolved",
    "yeah",
    "love",
    "way",
    "articulated",
    "writing",
    "cnns",
    "example",
    "know",
    "talking",
    "convex",
    "whole",
    "topic",
    "maturing",
    "significant",
    "period",
    "time",
    "convex",
    "hole",
    "solidify",
    "say",
    "new",
    "event",
    "happens",
    "tomorrow",
    "started",
    "blogging",
    "thing",
    "would",
    "take",
    "shape",
    "would",
    "evolve",
    "sometimes",
    "things",
    "change",
    "time",
    "politics",
    "views",
    "things",
    "change",
    "time",
    "yeah",
    "mean",
    "mean",
    "mean",
    "entirely",
    "different",
    "problem",
    "things",
    "like",
    "gpt3",
    "right",
    "crawl",
    "entire",
    "internet",
    "first",
    "point",
    "500",
    "gigabytes",
    "text",
    "data",
    "like",
    "really",
    "lot",
    "stuff",
    "going",
    "repeated",
    "like",
    "deduplication",
    "save",
    "fact",
    "know",
    "like",
    "lot",
    "stuff",
    "kind",
    "okay",
    "second",
    "course",
    "snapshot",
    "like",
    "things",
    "old",
    "things",
    "sources",
    "sketchy",
    "uh",
    "know",
    "entirely",
    "different",
    "aspect",
    "think",
    "one",
    "aspects",
    "know",
    "gpt2",
    "general",
    "know",
    "fairness",
    "community",
    "prevalent",
    "input",
    "data",
    "1950",
    "going",
    "gon",
    "na",
    "get",
    "output",
    "1950",
    "question",
    "thoughts",
    "dangers",
    "mod",
    "gpt2",
    "especially",
    "think",
    "gpthree",
    "broader",
    "impact",
    "statement",
    "five",
    "pages",
    "long",
    "like",
    "things",
    "like",
    "let",
    "say",
    "fake",
    "news",
    "generation",
    "thoughts",
    "oh",
    "gon",
    "na",
    "say",
    "think",
    "know",
    "lot",
    "publicity",
    "play",
    "think",
    "like",
    "people",
    "really",
    "know",
    "read",
    "papers",
    "people",
    "lot",
    "people",
    "think",
    "want",
    "keep",
    "like",
    "mystery",
    "least",
    "like",
    "eyes",
    "tech",
    "investors",
    "arguably",
    "going",
    "short",
    "time",
    "available",
    "know",
    "publicly",
    "mean",
    "think",
    "things",
    "like",
    "deep",
    "fakes",
    "right",
    "like",
    "maybe",
    "year",
    "point",
    "know",
    "kind",
    "working",
    "academia",
    "point",
    "know",
    "app",
    "phone",
    "like",
    "deep",
    "coming",
    "back",
    "said",
    "second",
    "ago",
    "train",
    "classification",
    "models",
    "rigor",
    "know",
    "stratifying",
    "training",
    "set",
    "example",
    "know",
    "would",
    "balance",
    "client",
    "cats",
    "dogs",
    "want",
    "dogs",
    "representative",
    "dominant",
    "model",
    "level",
    "due",
    "diligence",
    "impossible",
    "natural",
    "language",
    "processing",
    "model",
    "first",
    "articles",
    "doubled",
    "tripled",
    "quadrupled",
    "mean",
    "impact",
    "thing",
    "look",
    "different",
    "directions",
    "model",
    "learns",
    "example",
    "gender",
    "vectors",
    "racial",
    "bias",
    "might",
    "learning",
    "lots",
    "lots",
    "things",
    "1950s",
    "england",
    "things",
    "develop",
    "really",
    "sophisticated",
    "machine",
    "learning",
    "models",
    "heterogeneous",
    "diverse",
    "could",
    "possibly",
    "balance",
    "training",
    "meaningful",
    "way",
    "yeah",
    "yeah",
    "mean",
    "makes",
    "sense",
    "want",
    "think",
    "convex",
    "whole",
    "physical",
    "reality",
    "shape",
    "shape",
    "evolving",
    "well",
    "question",
    "snapshot",
    "always",
    "going",
    "date",
    "let",
    "let",
    "care",
    "care",
    "date",
    "right",
    "historic",
    "versus",
    "current",
    "right",
    "like",
    "snapshot",
    "internet",
    "necessarily",
    "going",
    "date",
    "kept",
    "even",
    "downloaded",
    "whole",
    "internet",
    "yesterday",
    "date",
    "least",
    "one",
    "day",
    "given",
    "every",
    "website",
    "written",
    "yesterday",
    "even",
    "everything",
    "evolving",
    "know",
    "interesting",
    "trade",
    "time",
    "thing",
    "trained",
    "data",
    "yesterday",
    "would",
    "super",
    "date",
    "would",
    "know",
    "loads",
    "gender",
    "pronouns",
    "would",
    "scale",
    "want",
    "training",
    "data",
    "inevitably",
    "going",
    "give",
    "older",
    "information",
    "also",
    "specificity",
    "personalization",
    "aspect",
    "would",
    "also",
    "good",
    "partitioned",
    "data",
    "london",
    "united",
    "kingdom",
    "would",
    "know",
    "colloquialisms",
    "things",
    "talking",
    "always",
    "made",
    "yes",
    "maybe",
    "another",
    "actually",
    "thing",
    "reverse",
    "index",
    "thing",
    "could",
    "say",
    "condition",
    "want",
    "weights",
    "particular",
    "training",
    "subsets",
    "mattered",
    "training",
    "something",
    "like",
    "would",
    "mean",
    "kind",
    "context",
    "learning",
    "say",
    "sort",
    "condition",
    "saying",
    "basically",
    "would",
    "want",
    "model",
    "acts",
    "trained",
    "data",
    "london",
    "mean",
    "going",
    "hard",
    "like",
    "cool",
    "idea",
    "well",
    "yeah",
    "personalization",
    "bias",
    "well",
    "yeah",
    "want",
    "remove",
    "meaningfully",
    "gender",
    "bias",
    "well",
    "first",
    "come",
    "like",
    "definition",
    "people",
    "still",
    "arguing",
    "even",
    "means",
    "um",
    "guess",
    "presuming",
    "definition",
    "also",
    "think",
    "mean",
    "still",
    "actively",
    "developing",
    "field",
    "whole",
    "fairness",
    "bias",
    "removal",
    "lots",
    "opinions",
    "techniques",
    "qualified",
    "much",
    "give",
    "something",
    "going",
    "back",
    "initial",
    "question",
    "like",
    "specifically",
    "let",
    "talk",
    "dangers",
    "fake",
    "news",
    "like",
    "brought",
    "brought",
    "gpt2",
    "brought",
    "broader",
    "impact",
    "statement",
    "believe",
    "actually",
    "read",
    "fully",
    "think",
    "one",
    "points",
    "seems",
    "like",
    "know",
    "thing",
    "people",
    "say",
    "oh",
    "used",
    "generate",
    "fake",
    "news",
    "question",
    "problem",
    "well",
    "let",
    "say",
    "like",
    "even",
    "gpt3",
    "like",
    "gam",
    "produce",
    "like",
    "video",
    "yeah",
    "could",
    "huge",
    "problem",
    "right",
    "could",
    "make",
    "videos",
    "like",
    "crazy",
    "things",
    "happening",
    "hit",
    "social",
    "media",
    "thing",
    "outrage",
    "well",
    "exactly",
    "right",
    "one",
    "things",
    "see",
    "feel",
    "fake",
    "news",
    "people",
    "people",
    "already",
    "know",
    "whatever",
    "written",
    "must",
    "true",
    "think",
    "people",
    "video",
    "people",
    "people",
    "think",
    "see",
    "video",
    "might",
    "know",
    "deceptively",
    "cut",
    "something",
    "certainly",
    "pixel",
    "information",
    "actually",
    "happened",
    "people",
    "probably",
    "aware",
    "word",
    "information",
    "must",
    "represent",
    "like",
    "possible",
    "represent",
    "truth",
    "like",
    "someone",
    "could",
    "writing",
    "know",
    "crap",
    "fake",
    "news",
    "right",
    "someone",
    "could",
    "writing",
    "lie",
    "seems",
    "argument",
    "make",
    "ability",
    "automatically",
    "instead",
    "manually",
    "somehow",
    "dangerous",
    "think",
    "know",
    "yet",
    "hear",
    "argument",
    "automatically",
    "generating",
    "fake",
    "news",
    "dangerous",
    "thing",
    "dangerous",
    "thing",
    "authoritative",
    "sources",
    "matter",
    "generate",
    "realistic",
    "looking",
    "text",
    "people",
    "know",
    "authoritative",
    "source",
    "think",
    "real",
    "problem",
    "bias",
    "use",
    "another",
    "example",
    "felt",
    "society",
    "diversity",
    "colors",
    "car",
    "know",
    "green",
    "cars",
    "yellow",
    "cars",
    "orange",
    "cars",
    "moment",
    "people",
    "buying",
    "orange",
    "cars",
    "think",
    "incredibly",
    "unfair",
    "kind",
    "society",
    "want",
    "move",
    "blue",
    "cars",
    "need",
    "need",
    "stratify",
    "language",
    "models",
    "make",
    "sure",
    "blue",
    "cars",
    "represented",
    "um",
    "diversely",
    "well",
    "like",
    "like",
    "10",
    "different",
    "layers",
    "problem",
    "first",
    "mean",
    "unpopular",
    "opinion",
    "arguing",
    "like",
    "term",
    "biasing",
    "unbiasing",
    "like",
    "describing",
    "biasing",
    "like",
    "let",
    "like",
    "bad",
    "rep",
    "bad",
    "connotation",
    "statistically",
    "want",
    "bias",
    "model",
    "want",
    "say",
    "okay",
    "world",
    "would",
    "like",
    "right",
    "would",
    "worlds",
    "aspire",
    "right",
    "want",
    "money",
    "conform",
    "world",
    "right",
    "irrespective",
    "case",
    "saying",
    "cars",
    "absolutely",
    "nothing",
    "real",
    "world",
    "looks",
    "like",
    "like",
    "simply",
    "say",
    "want",
    "distribution",
    "cars",
    "aspire",
    "ideal",
    "therefore",
    "bias",
    "model",
    "towards",
    "different",
    "thing",
    "saying",
    "model",
    "representing",
    "world",
    "right",
    "example",
    "seen",
    "paper",
    "recently",
    "makes",
    "credible",
    "case",
    "regularization",
    "one",
    "drivers",
    "one",
    "drivers",
    "bias",
    "models",
    "let",
    "say",
    "actually",
    "simple",
    "right",
    "ferraris",
    "lamborghinis",
    "drive",
    "ferrari",
    "drivers",
    "lamborghini",
    "drivers",
    "want",
    "make",
    "model",
    "predicts",
    "accident",
    "probability",
    "happens",
    "ferrari",
    "drivers",
    "bit",
    "reckless",
    "slightly",
    "higher",
    "accidents",
    "right",
    "train",
    "logistic",
    "regression",
    "tells",
    "okay",
    "60",
    "cool",
    "train",
    "logistic",
    "regression",
    "l1",
    "penalty",
    "say",
    "want",
    "model",
    "know",
    "explainable",
    "want",
    "explainable",
    "model",
    "want",
    "sparse",
    "want",
    "least",
    "amount",
    "variables",
    "contributing",
    "model",
    "going",
    "say",
    "model",
    "going",
    "say",
    "ferrari",
    "drivers",
    "bad",
    "lamborghini",
    "drivers",
    "good",
    "right",
    "think",
    "two",
    "different",
    "concepts",
    "one",
    "one",
    "think",
    "frankly",
    "separate",
    "subfields",
    "one",
    "says",
    "models",
    "train",
    "loss",
    "functions",
    "use",
    "data",
    "input",
    "certain",
    "degree",
    "representing",
    "data",
    "occurs",
    "world",
    "like",
    "representing",
    "cumulative",
    "distribution",
    "data",
    "entirely",
    "different",
    "field",
    "saying",
    "state",
    "want",
    "achieve",
    "make",
    "model",
    "achieve",
    "one",
    "case",
    "legitimately",
    "speak",
    "biasing",
    "unbiasing",
    "case",
    "actually",
    "biasing",
    "play",
    "devil",
    "advocate",
    "people",
    "asian",
    "would",
    "say",
    "structures",
    "interlocking",
    "oppressions",
    "society",
    "people",
    "moving",
    "away",
    "buying",
    "blue",
    "cars",
    "perception",
    "blue",
    "cone",
    "work",
    "well",
    "existing",
    "world",
    "model",
    "tells",
    "buy",
    "blue",
    "cars",
    "even",
    "though",
    "want",
    "move",
    "buying",
    "blue",
    "cars",
    "blockers",
    "way",
    "okay",
    "think",
    "like",
    "two",
    "different",
    "sub",
    "genres",
    "could",
    "could",
    "one",
    "compounding",
    "effect",
    "say",
    "world",
    "certain",
    "way",
    "train",
    "models",
    "models",
    "might",
    "know",
    "bit",
    "regularized",
    "bit",
    "biased",
    "models",
    "actually",
    "inform",
    "people",
    "go",
    "direction",
    "train",
    "models",
    "update",
    "direction",
    "problem",
    "course",
    "use",
    "model",
    "train",
    "data",
    "comes",
    "process",
    "going",
    "amplify",
    "amplify",
    "certain",
    "cases",
    "like",
    "argue",
    "always",
    "happen",
    "think",
    "exactly",
    "saying",
    "saying",
    "like",
    "society",
    "whatever",
    "reason",
    "process",
    "steering",
    "people",
    "away",
    "blue",
    "cars",
    "right",
    "okay",
    "let",
    "say",
    "yellow",
    "car",
    "lobby",
    "evil",
    "yellow",
    "car",
    "lobby",
    "um",
    "steering",
    "people",
    "away",
    "blue",
    "cars",
    "right",
    "blue",
    "cars",
    "exactly",
    "good",
    "yellow",
    "cars",
    "like",
    "concrete",
    "example",
    "grain",
    "lobby",
    "right",
    "grain",
    "lobby",
    "breakfast",
    "important",
    "meal",
    "day",
    "marketing",
    "slogan",
    "like",
    "people",
    "think",
    "health",
    "slogan",
    "marketing",
    "slogan",
    "sell",
    "cereal",
    "say",
    "well",
    "know",
    "actually",
    "lunch",
    "lunch",
    "important",
    "eat",
    "breakfast",
    "skip",
    "lunch",
    "eat",
    "lunch",
    "skip",
    "breakfast",
    "whole",
    "society",
    "somehow",
    "wrong",
    "notion",
    "point",
    "still",
    "second",
    "camp",
    "described",
    "initially",
    "data",
    "world",
    "state",
    "world",
    "people",
    "eat",
    "breakfast",
    "people",
    "know",
    "eat",
    "breakfast",
    "think",
    "breakfast",
    "important",
    "meal",
    "day",
    "describing",
    "still",
    "world",
    "would",
    "like",
    "get",
    "people",
    "differentiate",
    "breakfast",
    "lunch",
    "like",
    "another",
    "way",
    "verbalize",
    "politically",
    "conservativism",
    "versus",
    "progressivism",
    "conservatives",
    "want",
    "us",
    "strong",
    "memory",
    "things",
    "always",
    "progressivists",
    "want",
    "us",
    "forget",
    "everything",
    "learned",
    "generations",
    "dynamically",
    "evolve",
    "quickly",
    "well",
    "thought",
    "experiment",
    "lot",
    "lot",
    "disagree",
    "continue",
    "thought",
    "models",
    "reason",
    "could",
    "argue",
    "unethical",
    "increase",
    "memory",
    "train",
    "concepts",
    "1950s",
    "stop",
    "us",
    "said",
    "london",
    "cool",
    "kids",
    "using",
    "colloquialisms",
    "use",
    "cvs",
    "wo",
    "get",
    "job",
    "model",
    "still",
    "trained",
    "1950s",
    "argument",
    "get",
    "rid",
    "model",
    "somehow",
    "impute",
    "stuff",
    "model",
    "war",
    "ai",
    "fought",
    "sentences",
    "like",
    "okay",
    "boomer",
    "model",
    "well",
    "well",
    "compare",
    "conservatism",
    "would",
    "guess",
    "conservative",
    "would",
    "say",
    "memorize",
    "things",
    "want",
    "learn",
    "things",
    "memorized",
    "necessarily",
    "would",
    "would",
    "always",
    "stick",
    "maybe",
    "progressive",
    "would",
    "say",
    "still",
    "remember",
    "want",
    "change",
    "things",
    "worked",
    "mean",
    "right",
    "model",
    "olden",
    "days",
    "would",
    "actually",
    "mean",
    "sees",
    "every",
    "data",
    "point",
    "equally",
    "going",
    "aggregate",
    "everything",
    "give",
    "going",
    "think",
    "average",
    "sees",
    "world",
    "today",
    "true",
    "right",
    "average",
    "sees",
    "actually",
    "discrepancy",
    "model",
    "model",
    "notion",
    "data",
    "point",
    "date",
    "gave",
    "model",
    "like",
    "analogy",
    "opinion",
    "become",
    "much",
    "relevant",
    "getting",
    "thing",
    "machine",
    "learning",
    "start",
    "incorporate",
    "inductive",
    "priors",
    "temperature",
    "parameters",
    "one",
    "thing",
    "might",
    "example",
    "decide",
    "recency",
    "important",
    "online",
    "ridge",
    "regression",
    "example",
    "sliding",
    "window",
    "assume",
    "recent",
    "information",
    "important",
    "old",
    "information",
    "maybe",
    "going",
    "new",
    "regime",
    "old",
    "information",
    "wo",
    "help",
    "us",
    "predict",
    "stuff",
    "new",
    "information",
    "also",
    "throwing",
    "away",
    "lot",
    "knowledge",
    "machine",
    "learning",
    "practitioners",
    "need",
    "tweak",
    "knobs",
    "levers",
    "need",
    "establish",
    "best",
    "representation",
    "data",
    "seems",
    "like",
    "completely",
    "arbitrary",
    "process",
    "absolutely",
    "like",
    "think",
    "types",
    "models",
    "think",
    "considered",
    "language",
    "models",
    "yet",
    "must",
    "exist",
    "like",
    "notion",
    "extract",
    "timeless",
    "principles",
    "data",
    "matter",
    "old",
    "right",
    "people",
    "go",
    "back",
    "marcus",
    "aurelius",
    "plato",
    "aristotle",
    "wrote",
    "timeless",
    "things",
    "applicable",
    "today",
    "days",
    "everything",
    "written",
    "applicable",
    "today",
    "models",
    "feel",
    "trained",
    "ever",
    "growing",
    "amounts",
    "data",
    "comes",
    "different",
    "time",
    "periods",
    "whatnot",
    "notion",
    "sort",
    "information",
    "must",
    "extract",
    "seems",
    "hard",
    "like",
    "means",
    "understand",
    "wait",
    "minute",
    "probably",
    "write",
    "smartphone",
    "nowadays",
    "marcus",
    "aurelius",
    "even",
    "though",
    "marcus",
    "aurelia",
    "says",
    "write",
    "clay",
    "tablet",
    "write",
    "nonetheless",
    "like",
    "timeless",
    "part",
    "without",
    "grounding",
    "going",
    "super",
    "hard",
    "yeah",
    "emanuel",
    "kant",
    "example",
    "racist",
    "incredible",
    "ideas",
    "philosophy",
    "introduced",
    "consequentialism",
    "example",
    "racist",
    "look",
    "back",
    "say",
    "well",
    "um",
    "well",
    "discredit",
    "ideas",
    "something",
    "thought",
    "longer",
    "think",
    "right",
    "maybe",
    "might",
    "happen",
    "older",
    "philosophers",
    "well",
    "maybe",
    "decide",
    "particular",
    "viewpoint",
    "longer",
    "something",
    "relevant",
    "yeah",
    "make",
    "make",
    "distinction",
    "mean",
    "even",
    "even",
    "complicated",
    "thing",
    "somehow",
    "level",
    "abstraction",
    "actually",
    "abstract",
    "idea",
    "racism",
    "right",
    "changed",
    "mind",
    "right",
    "somehow",
    "somehow",
    "society",
    "gotten",
    "past",
    "recognized",
    "individual",
    "value",
    "every",
    "human",
    "argue",
    "also",
    "gone",
    "science",
    "lot",
    "things",
    "based",
    "like",
    "junk",
    "science",
    "also",
    "like",
    "society",
    "evolves",
    "consciousness",
    "society",
    "changes",
    "mean",
    "going",
    "teach",
    "model",
    "like",
    "one",
    "thing",
    "says",
    "oh",
    "know",
    "clay",
    "tablets",
    "thing",
    "anymore",
    "clay",
    "tablets",
    "must",
    "also",
    "recognize",
    "wait",
    "society",
    "changed",
    "mind",
    "particular",
    "abstract",
    "issue",
    "going",
    "long",
    "time",
    "really",
    "solid",
    "methods",
    "dealing",
    "thing",
    "know",
    "mean",
    "say",
    "immense",
    "want",
    "data",
    "probably",
    "going",
    "go",
    "back",
    "history",
    "going",
    "increase",
    "history",
    "recency",
    "recency",
    "bias",
    "way",
    "recency",
    "bias",
    "producing",
    "exponentially",
    "data",
    "time",
    "yes",
    "even",
    "philosophers",
    "around",
    "medieval",
    "times",
    "much",
    "information",
    "gone",
    "record",
    "time",
    "anything",
    "need",
    "sample",
    "period",
    "well",
    "much",
    "data",
    "produce",
    "copies",
    "philosopher",
    "thing",
    "could",
    "hope",
    "copies",
    "adjusted",
    "modern",
    "times",
    "right",
    "would",
    "perfect",
    "actually",
    "perfect",
    "training",
    "data",
    "would",
    "people",
    "know",
    "taking",
    "taking",
    "marcus",
    "aurelius",
    "work",
    "translating",
    "modern",
    "time",
    "says",
    "marcus",
    "aurelius",
    "says",
    "write",
    "uh",
    "dreams",
    "could",
    "might",
    "smartphone",
    "think",
    "marcus",
    "aurelius",
    "dream",
    "journaling",
    "yeah",
    "like",
    "would",
    "perfect",
    "training",
    "data",
    "exponential",
    "production",
    "data",
    "keep",
    "timeless",
    "knowledge",
    "around",
    "requires",
    "humans",
    "right",
    "requires",
    "sure",
    "machine",
    "could",
    "matter",
    "got",
    "much",
    "oversampling",
    "duplication",
    "data",
    "learning",
    "language",
    "manifold",
    "want",
    "able",
    "language",
    "like",
    "gans",
    "faces",
    "know",
    "want",
    "hair",
    "want",
    "beard",
    "glasses",
    "perhaps",
    "thing",
    "language",
    "slide",
    "bars",
    "say",
    "well",
    "want",
    "bit",
    "medieval",
    "want",
    "bit",
    "united",
    "kingdom",
    "start",
    "get",
    "control",
    "manifold",
    "dimensions",
    "care",
    "yeah",
    "think",
    "best",
    "way",
    "model",
    "though",
    "right",
    "like",
    "start",
    "want",
    "write",
    "positive",
    "articles",
    "right",
    "score",
    "reinforced",
    "learning",
    "auxiliary",
    "classifiers",
    "yeah",
    "mean",
    "gans",
    "works",
    "face",
    "data",
    "set",
    "right",
    "faces",
    "like",
    "10",
    "20",
    "attributes",
    "serious",
    "right",
    "really",
    "significantly",
    "different",
    "like",
    "super",
    "pretty",
    "aligned",
    "know",
    "hair",
    "color",
    "whether",
    "glasses",
    "ultimately",
    "gave",
    "language",
    "model",
    "say",
    "100",
    "relevant",
    "latent",
    "directions",
    "going",
    "tell",
    "directions",
    "100",
    "principal",
    "components",
    "basically",
    "maximal",
    "variants",
    "also",
    "disentangled",
    "clue",
    "would",
    "like",
    "like",
    "using",
    "like",
    "video",
    "editing",
    "software",
    "like",
    "um",
    "laughter",
    "yes",
    "bit",
    "bombshell",
    "mean",
    "incredible",
    "think",
    "big",
    "parts",
    "development",
    "yet",
    "front",
    "us",
    "also",
    "like",
    "big",
    "challenges",
    "like",
    "uh",
    "like",
    "big",
    "corpora",
    "clue",
    "even",
    "clue",
    "deal",
    "fact",
    "know",
    "information",
    "old",
    "things",
    "like",
    "crazy",
    "know",
    "okay",
    "well",
    "think",
    "suitable",
    "place",
    "round",
    "episode",
    "yeah",
    "thanks",
    "folks",
    "next",
    "week",
    "going",
    "back",
    "francoise",
    "chalet",
    "fact",
    "fabulous",
    "frenchman",
    "forget",
    "uh",
    "major",
    "audio",
    "podcast",
    "platforms",
    "audio",
    "like",
    "sound",
    "sound",
    "beautiful",
    "made",
    "platforms",
    "enjoy",
    "yeah",
    "check",
    "us",
    "fantastic",
    "see",
    "next",
    "week",
    "folks",
    "yeah",
    "cool",
    "peace",
    "peace"
  ],
  "keywords": [
    "super",
    "last",
    "week",
    "released",
    "gpt3",
    "language",
    "model",
    "ever",
    "today",
    "thing",
    "size",
    "trained",
    "modeling",
    "number",
    "kind",
    "never",
    "seen",
    "many",
    "questions",
    "future",
    "models",
    "billion",
    "parameters",
    "need",
    "train",
    "done",
    "exactly",
    "neural",
    "network",
    "10",
    "times",
    "larger",
    "zero",
    "two",
    "microsoft",
    "deep",
    "lot",
    "training",
    "data",
    "parallelism",
    "gpu",
    "different",
    "single",
    "memory",
    "next",
    "step",
    "google",
    "used",
    "multiple",
    "something",
    "forward",
    "similar",
    "one",
    "entire",
    "set",
    "solve",
    "state",
    "first",
    "auto",
    "well",
    "like",
    "talk",
    "incredible",
    "amount",
    "using",
    "introduced",
    "word",
    "representation",
    "words",
    "learned",
    "way",
    "based",
    "context",
    "type",
    "learning",
    "tokens",
    "time",
    "really",
    "cool",
    "transfer",
    "natural",
    "sense",
    "whole",
    "bunch",
    "course",
    "could",
    "task",
    "attention",
    "transformers",
    "learn",
    "input",
    "goes",
    "system",
    "part",
    "sequence",
    "attend",
    "also",
    "quite",
    "longer",
    "real",
    "away",
    "produce",
    "see",
    "pattern",
    "predicting",
    "answer",
    "prediction",
    "gets",
    "sentence",
    "say",
    "things",
    "question",
    "answering",
    "point",
    "example",
    "keep",
    "generation",
    "help",
    "us",
    "reach",
    "yannick",
    "video",
    "bit",
    "might",
    "paper",
    "youtube",
    "even",
    "five",
    "going",
    "play",
    "think",
    "good",
    "basically",
    "giant",
    "order",
    "magnitude",
    "absolutely",
    "crazy",
    "go",
    "architecture",
    "able",
    "let",
    "take",
    "right",
    "humans",
    "english",
    "would",
    "tell",
    "comes",
    "generate",
    "ask",
    "come",
    "sort",
    "text",
    "bert",
    "much",
    "scale",
    "better",
    "remember",
    "whereas",
    "anything",
    "look",
    "gpt",
    "made",
    "dangerous",
    "essentially",
    "always",
    "use",
    "gpt2",
    "layers",
    "bird",
    "second",
    "actually",
    "interested",
    "classification",
    "want",
    "know",
    "sentences",
    "test",
    "machine",
    "simply",
    "predict",
    "structure",
    "understand",
    "internet",
    "find",
    "books",
    "important",
    "put",
    "new",
    "line",
    "okay",
    "shot",
    "generalization",
    "mean",
    "weights",
    "filter",
    "match",
    "fuzzy",
    "interpolate",
    "examples",
    "reasoning",
    "happening",
    "looking",
    "comments",
    "make",
    "interesting",
    "says",
    "000",
    "people",
    "transformer",
    "human",
    "adversarial",
    "intuition",
    "table",
    "um",
    "get",
    "idea",
    "yes",
    "argument",
    "saying",
    "seems",
    "information",
    "give",
    "stuff",
    "agree",
    "intelligence",
    "chalet",
    "problem",
    "happens",
    "said",
    "sometimes",
    "feel",
    "trying",
    "news",
    "makes",
    "field",
    "thinking",
    "three",
    "still",
    "knowledge",
    "show",
    "reason",
    "program",
    "thought",
    "work",
    "thoughts",
    "mind",
    "maybe",
    "back",
    "already",
    "though",
    "old",
    "ca",
    "uh",
    "yet",
    "another",
    "huge",
    "got",
    "date",
    "far",
    "fact",
    "oh",
    "bad",
    "surprising",
    "probably",
    "trend",
    "push",
    "yeah",
    "almost",
    "pretty",
    "gotten",
    "big",
    "corpus",
    "talking",
    "change",
    "year",
    "created",
    "memorization",
    "arguing",
    "certain",
    "memorizing",
    "argue",
    "little",
    "together",
    "scrambled",
    "scramble",
    "least",
    "someone",
    "means",
    "output",
    "whatever",
    "given",
    "pieces",
    "experiment",
    "scrambling",
    "unscrambling",
    "versus",
    "knows",
    "sees",
    "complexity",
    "aggressive",
    "article",
    "depends",
    "case",
    "everything",
    "getting",
    "every",
    "found",
    "hard",
    "span",
    "masking",
    "mask",
    "length",
    "taking",
    "manifold",
    "documents",
    "entirely",
    "sure",
    "throw",
    "particular",
    "learns",
    "somehow",
    "start",
    "surely",
    "half",
    "best",
    "specifically",
    "possible",
    "search",
    "math",
    "completely",
    "works",
    "lots",
    "reverse",
    "explicit",
    "notion",
    "skill",
    "direction",
    "imagine",
    "retrieve",
    "abstract",
    "long",
    "distribution",
    "gave",
    "explain",
    "guess",
    "gon",
    "na",
    "matter",
    "receptive",
    "features",
    "document",
    "relevant",
    "weight",
    "level",
    "index",
    "read",
    "concepts",
    "articles",
    "world",
    "blue",
    "title",
    "watermark",
    "write",
    "cnns",
    "fake",
    "bias",
    "must",
    "society",
    "cars",
    "drivers",
    "breakfast",
    "lunch",
    "marcus",
    "aurelius"
  ]
}