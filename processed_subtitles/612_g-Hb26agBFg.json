{
  "text": "hello i'm luis serrano and in this video\nwe're gonna learn principle component\nanalysis or PCA principle component\nanalysis or PCA\nis one of the most important\ndimensionality reduction techniques out\nthere\nwhat is dimensionality reduction well\nlet's say you have a huge table of data\nso big that it's hard to process and you\nwould like to make it into a smaller one\nwhile still keeping it as much of the\ninformation as possible so\ndimensionality reduction techniques what\nthey do is they reduce the number of\ncolumns so today we're going to learn\nPCA in a few steps first we're gonna\nlearn mode projections are then another\non variance covariance the covariance\nmatrix then we will learn what I give\nvalues and eigenvectors are and finally\nwe're gonna go to PCA so let's begin so\nlet's start with a simple problem let's\nsay we have a bunch of friends standing\nlike this and we want to take a picture\nof them so the question is where do we\ntake the picture from what angle so it\ncould be this one it could be this one\nor this one\nwell it seems like this would be one of\nthe best angles to take the picture\nright and that's really what\ndimensionality reduction is it's taking\na picture of your data and trying to\nkeep as much information as possible in\nthis picture so when your data looks\nlike this then a picture of the data\ncould be for example projecting it over\nthis line then you get these points over\nhere another one could be projecting it\nover this line now the question is\nintuitively which of the two projections\nseems better let's compare them put them\nside-by-side well it seems to me another\none on the right is better because the\npoints are more spaced out and you can\nactually tell them apart if you only\nhave this picture you could actually\nmake out the day a little better than\nwith the one on the left so what we're\ngonna learn in this video is how to\nobtain the projection in the right how\nto get that sort of ideal line to\nproject our data and keep it as far as\nas possible so first a little\nparentheses why does this matter\nwhy did we do this so let me give you an\nexample of some housing data let's say\nwe have a housing data set with a bunch\nof columns with the fall information\nsighs number of rooms number of\nbathrooms the schools are around the\narea and the crime rate so if I gave you\nthis data set and I told you there's too\nmany columns there's five and I want\nless what would you think feel free to\npause the video and think how can we\nsort of reduce this and keep the\ninformation well just from my balling it\nI can see a difference here right the\nfirst three features are related to size\nso maybe we can lump them together into\nsomething called a size feature and the\nsecond two are related to the\nneighborhood so let's lump them together\ninto something called a location feature\nso that's what dimensionality reduction\nis right we went from five columns to 2\nand we're capturing the information\nbecause it seems to me that a bigger\nhouse may also have a higher number of\nrooms and a higher number of bathrooms\nso we may not need the 3 piece of\ninformation we may need maybe maybe a\nsum of the 3 or maybe a weighted sum and\nthe same thing for the second for the\nlocation feature so how to do this\nideally that's what PCA does so let's\nlook at an example not not from going\nfrom five columns to two but let's go\nfrom two columns to one so we have\nnumber of rooms in size and we're gonna\ntry to lump those into one feature so we\nhave a graph here where on the\nhorizontal axis we have number of rooms\nand on the vertical axis we have size\nand notice that our suspicion was\ncorrect the bigger the house the more\nrooms it has there is some variation but\nfor the most part and so where would be\nprojected data how do we take a picture\nof this houses so that they look as\nspaced apart as possible well let seems\nlike this line would work right and a\nlittle aside we're not doing linear\nregression we're not trying to fit the\nclosest line to the data we're trying to\nfit the one that makes the best\nprojection so it's slightly different\nand definitely with the different\npurpose but if we do this then we\nproject each cows to the line by drawing\nthe perpendicular and sending it there\nso we do this and then our dataset\nchanges a little bit it's not exactly\nthe same one it's not faithful to the\ndata but it's much much simpler because\nnow it's not two-dimensional it's\none-dimensional because now we can see\nit like this and now we only have one\nwhich is the size feature so now all of\na sudden our data set became more simple\nit went from being a two dimensional\ndata set to a table of two columns to a\none dimensional data sets to a table\nwith only one column now every house\nonly has one number attached to it not\ntwo so from two dimensions to one\ndimension namely size and number of\nrooms to the size feature now we did it\nfrom two dimensions to one dimension\nbecause it is graphically nice to see\nbut you can imagine that from five\ndimensions to two dimensions it would be\ngoing from something like these features\nto the size and the location feature so\nit's hard to picture five dimensions in\nour head but imagine what we just\ndifferent to the one imagine it doing\nfrom five to two so if you seen\nstatistics before the next couple of\nminutes may look like a Ryu but I still\nfind it interesting because I like to\nsee things in a more geometric way so\nwhat is the mean of a set of numbers\nwell let's say that I have three weights\nthat are exactly the same and I want to\nbalance them so how do we balance them\nhow do we find the perfect point of\nbalance for these three weights and\nlet's just assume that that bar has no\nweight so what I'm gonna do is I'm gonna\nimagine there's a wall here at the left\nand I'm gonna measure the distance from\nevery point to the wall so the one is\nthat distance one then the other one is\nto one and the other one is six and I'm\ngonna take the average of these points\nso the mean is one plus two plus six\ndivided by three and that's the point of\nequilibrium if I were to put a balance\nhere at three then the points would\nbalance but now there are a lot more\nthings I can say about a set for example\nwhat if I have these three points and\nthese three points how do I tell those\napart well not using the mean because\nthey have the same mean notice that they\nbalance on the same point but if you can\nsee the points on the top are a lot\ncloser to each other and the ones at the\nbottom are a lot more spread out so how\ndo I measure that what I'm gonna do is\nI'm gonna take the distance from each\npoint to the center so the point in the\nleft is a distance one from the center\nand the point of the right distance one\nfrom the center and the point in the\nmiddle is this distance zero for descent\nfrom the center because it's actually\nthe center is the point where the points\nbalance\nand the same thing in the bottom except\nthese are 5 5 and 0 so the variance is\nyou're gonna add these numbers but\nactually it's gonna square them and the\nreason we square them is because\nactually when you take a distance that\ngoes towards the left you're subtracting\nand it coordinates and then so you're\ngonna get a negative number so you\nsquare everything so you get everything\npositive and so for the top you get 1\nsquared plus 0 squared plus 1 squared\ndivided by 3 which is 2/3 and for the\nbottom one we get 5 squared plus 0\nsquared plus 5 squared divided by 3\nwhich is 50 over 3 so it's much bigger\nso this is basically a measure of how\nspread out is a set so now we have what\nis the variance of the original set well\nfrom the center the point of the left\nside distance to the next point is a\ndistance 1 and the point in the rises\ndistance 3 so the variance is 2 squared\nplus 1 squared plus 3 squared divided by\n3 the average of these squares and so\nthat is 14 divided by 3 so you can see\nthat it makes sense 14-mile 3 because\nthat kind of spread out now the question\nis what is the variance for a\n2-dimensional data sets on the set is in\nthe plane what do we mean by spread out\nwell two things we can do is ask how\nspread out it is in the horizontal\ndirection and how spread out is in the\nvertical direction so basically we\nforget about the height and we send\neverything to the horizontal or x axis\nand then calculate the variance of those\npoints which is the experience and then\ndo the same thing by forgetting the x\ncoordinate sending everything to the\nvertical axis and now with those points\nwe calculate the y variance and then we\nhave two numbers the X variance and the\nY variance and they should tell us well\nhow spread out as a set however there\nare some technicalities let's look at\nthis set and let's look at this set what\nare the x and y variances of those two\nwell let's calculate the X variance and\nlet's say these distances are 2 so it's\ngonna be 2 squared plus 0 squared plus 2\nsquared divided by 3 which is 8 divided\nby 3 and what's the Y variance well it's\nthe\ndistances are one then it's one squared\nplus 0 squared plus 1 squared divided by\n3 which is 2/3 so those two sets have\nthe same X variance and the same way\nvariance but they're fundamentally very\ndifferent so how do we tell those two\napart can we have a third metric that\ntells us that these two are different\nand the answer is yes and it's gonna be\ncovariance so the question the\ncovariance answers is how do we tell\nthese 3 points apart from these three\npoints so let's think can you think of a\nnumber or a metric that tells those two\nsets apart so if we're to pause the\nvideo and think about it now tell you\nthe one that I think it's the product of\ncoordinates so what if you look at the\ntwo numbers and multiply them to 201 the\n-2 and the 1 etc well let's see for the\npoint in the middle 0 times 0 is\nobviously 0 for the top right in the\nbottom left corner it's actually plus 2\nbecause 2 times 1 is 2 but minus 2 times\nminus 1 is also 2 and for the air two\npoints is minus 2 because 2 times minus\n1 is the same as minus 2 times 1 which\nis minus 2 so this product of\ncoordinates is positive for these two\npoints and negative for these two points\nand it's therefore the one in the middle\nbut that that's ok\nit still it still helps us tell the\npoints apart so basically covariances\nthat is the sum of products of\ncoordinates see before it was the sum of\nsquares of one of the coordinates now is\nthe sum of products of coordinates so\nlet's calculate the covariance of this\nset we see that the products are -2 0\nminus 2 and for this set where the\nproducts are 2 0 & 2 so the covariance\nfor the center-left is negative 2 plus 0\nplus negative 2 which is minus 4 over 3\ndivided by 3 the average of those and\nthe one on the right is 2 plus 0 plus 2\ndivided by 3 which is 4 over 3 so the\npoint in the left have a negative\ncovariance and the point ins in the\nright have a positive covariance if you\nheard the team correlation that's also\nvery related to this the points on the\nleft are negatively correlated and the\nones on the right are positively\ncorrelated because in the ones on the\nleft as the X Direction grows the Y\ndirection decreases whereas the ones in\nthe\nas the acceleration grows the\ny-direction grows as well so let's look\nat another set and try to calculate the\ncovariance this is our set what do you\nthink the covariance of this one is\ngonna be well let me tell you let's\ncalculate all the products they are -2 0\n& 2\naccording to this numbers over here and\nthen the covariance is just the sum of\nall those things but you can see that\nthey all cancel out so the sum of all\nthose things divided by 9 the average is\nactually 0 and it made sense because\nthis set is not positively correlated or\nnegatively correlated it doesn't look\nlike a diagonal in either one of the\ndirections it looks like a sort of thing\nin the middle so by looking at the type\nof set we can sort of tell the\ncovariance if we have a set like this or\nlike this or like this what do you think\nthe covariance is are so the first one\nlooks like an inverted diagonal so it's\nnot negative covariance the first one\nlooks like it's just sort of well\ncentered so it's have probably\ncovariance of zero or a very small\ncovariance and the third one is bus has\na positive covariance because it looks\nlike sort of forwards diagonal so this\ncovariance is gonna help us a lot with\nPCA so we're now ready to start thinking\nabout PCA let's say we have a data set\nlike this one how do we find the perfect\nprojection well first let's put it in\nsome coordinate axes and the first we're\ngonna do is Center it so Center it means\nwe're gonna take the average of the\ncoordinates both in the X and in the Y\ndirection find that center of mass think\nof the point where these set would\nbalance and move that to zero so we're\ngoing to move it so that this data set\nis centered at zero and now we are gonna\ncreate a 2 by 2 matrix and the 2 by 2\nmatrix is going to be very simple it's\njust going to be formed by the two\nvariances and the covariance of this\ndata set so on the top left corner we're\ngonna put the X variance so the variance\non the horizontal axis and this\ncoordinate we're gonna put the Y\nvariance so the variance in the vertical\ndirection and then in the other two\nspots we're gonna put the covariance\nwe're gonna put it there twice and in\nthe literature this matrix is normally\ncalled\nSigma but we're just gonna call it the\ncovariance matrix and so let me just\nmake up some numbers this data set looks\nlike it could have this covariance\nmatrix nine four four three because nine\nis the experience because the dancer\nlooks like it's pretty spread in the X\nDirection is not so much spread in the y\ndirection so the variance wise may be\nsomething like three and it looks like\nit's up has positive covariance because\nit looks like like a forwards diagonal\nso let's just say that the covariance is\nfour so now we're gonna do a little\naside on linear transformations because\nthat matrix we're gonna think about it\nnow as a linear transformation don't get\nscared with the term linear\ntransformation is really just a function\nor a map from say the plane to the plane\nthis could be also be in higher\ndimensions but here it's just gonna be\nin two so it sends any point in the\nplane on the left to a point in the\nplane and the right in the following way\nusing the numbers of the matrix so a\npoint of coordinates X comma Y gets sent\nto the point 9 X + 4 y + 4 X + 3 y so\ntwo coordinates go to two coordinates\nand the numbers are taken from matrix\nthis nine and this for getting coded\nhere and this 9x was for Y and this 403\ngetting coded in this 4 X + 3 y if you\nfeel more comfortable thinking of points\nas a two rows instead of two columns the\nway I did it feel free to think of that\nso that you can think of matrix\nmultiplication but if you want to just\nsimplify think about it this way so\nlet's see what this transformation does\nlet's see what it does to several points\nwhere does the point 0 0 go well what is\n9 times 0 plus 4 times 0 well that's\nyour 0 and 4 times 0 plus 3 times 0\nthat's 0 so this book goes to a point 0\n0 so let's draw the point in the left\nthe point 0 0 and that goes to the point\n0 0 so very simple let's do another\nslightly more complicated point where do\nyou think they go point 1 0 goes at any\nmoment feels very pause the video and\nthink about it yourself\n1 0 goes to 9 4 because if you plug in\nthe numbers 1 and 0 for x and y you get\n9 + 4 4 9 x + 4 y because you get 9\ntimes 1 + 4 times 0 and then 4 times 1 +\n3 times 0 so that goes to the point nine\nfour so we're gonna send the point 1 0\nto the in the left to the point nine\nfour at the right and these are not\ndrawn at scale\nbut I'm trying to capture the entire\nentire movement here next point would be\n0 1 which is over here and you can do\nthe math that goes to 4 3 so it goes\nover here the next point is minus 1 0\nwhich is here and it goes to minus 9\nminus 4 which is over here and finally\nthe point 0 minus 1 which is over here\ngoes to the point minus 4 3 which is\nhere and hopefully you can see a pattern\nhere right as a matter of fact if you\ntake this circle the unit sphere where\ndo you think it goes well it goes to\nthis ellipse over here now it almost\nseems like what we did was stretching\nthe plane in a sort of diagonal\ndirection actually we did two\nstretchings but one is very small but\nthat's really what a linear\ntransformation does most of them so\nlet's think about this there is some\nvector here the red vector that actually\ngoes to this one so what happens is we\nsort of stretch the plane in that\ndirection the direction is we can\ncalculate in a minute but it's 2 1 so\nit's 2 units to the right and one unit\nup that's that's just a direction that's\nnot a vector and the magnitude is 11\nwe'll see why it's 11 but basically the\nplane got stretched by a factor of 11 in\nthat direction and then there's another\nvector which has the direction - 1 2 and\nthat one gets sent to itself so it's\nactually stretched by a direction of 1\nsometimes you will have different\nnumbers but basically think of it as 2\nstretchings\nand they happen to be perpendicular in\nthis particular case so let's let's see\nthe directions are 2 1 and -1 - these\nare the eigenvectors which give us\ndirection and the eigenvalues which are\nthese factors of stretching are 11 and 1\nso those are as I said called\neigenvalues and they give us the magnet\nso again we can think of a linear\ntransformation given the eigenvectors\nand the eigenvalues as in the direction\nto one so two units to the right and one\nup we stretch by eleven in the direction\nminus 1/2 which is one unit to the left\nas minus one and two units up we stretch\nby a factor one so we don't do much and\nso the transformation basically does\nthis so notice that these two vectors\nare very special because an average\nvector so anything that goes like in\nthis direction for example I'm just\ndrawing a random vector goes to some\nother thing but the red vector goes to\nsomething that points in the same\ndirection and the green vector goes to\nsomething that points in the same\ndirection so those two are the\neigenvectors the two vectors that\nactually go to something that points in\nthe same direction then basically what\nyou're trying to say is that the linear\ntransformation which is this matrix for\nmost vectors it does some crazy thing\nbut for two particular vectors applying\nit to the vector V is the same as\nmultiplying that vector B via number\nright\nso it's scaling and it's sending in the\nsame direction so these are the\neigenvectors the red and the green\nvector and this value is the eigenvalue\nso there are many ways to calculate and\ngiven values and eigenvectors my\nfavorite was just type the matrix into\nWolfram Alpha and it just tells me\neverything there's another right way\nthat I won't get into much detail but\nbasically you take your matrix you take\nthe characteristic polynomial which is\nsubtract X minus the things in the\ndiagonal and then do - for everything\nthat's off the diagonal take the\ndeterminant of this which is this\nformula over here and express it as a\nquadratic and factor this quadratic into\ntwo things and then you magically get X\nminus 11x minus 1 I'm going a little\nfast here because it's something that if\nyou really are interested you can you\ncan read further then the eigenvalues\nare gonna be the two roots of this\npolynomial so xi + 1 that's how we find\nthe eigenvalues and how do we find the\neigenvectors well now that we have the\neigenvalues and\nwe have transformation times a vector UV\nand I'm reusing the letter V but that\ndoesn't matter we just have two\ncoordinates is equal to 11 times that\nvector and the matrix times another\nvector is 1 times that vector so if we\nsolve these students equations for u and\nV we get the eigenvector to 1 and the\neigenvector minus 1/2 if you find this\ninteresting and you want to see some\namazing animations with amazing\nexplanations go to this channel three\nblue one Brown he has a whole series on\nlinear algebra and his treatment of\neigenvectors and eigenvalues is actually\nfantastic so that may have been a little\nbit too much math but think about this\nas the moral just says that if you have\na linear transformation it is stretching\nthe plane in two directions and those\ndirections are given by the eigenvectors\nand the amount you're stretching is\ngiven by the eigenvalues okay so now\nwe're ready to do PCA first we start\nwith a data set and we Center it now we\ncalculate the covariance matrix which as\nwe saw the X variance is 9 the Y\nvariance is 3 and the covariance is 4 so\nthis matrix has eigenvectors which give\nus direction I give values to choose\nmagnitude the first one is Direction to\n1 and 9211 which is this one over here\nthe second wise direction minus 1/2 and\nmagnitude 1 which is this one over here\nand notice that the two eigen vectors\nand eigen values tell us a lot about our\ndata set right because it sort of tells\nus in what direction is spread and by\nhow much\nalso notice that the red and the green\nvectors are perpendicular and that's not\nout of luck that's something that\nhappens when we have a symmetric matrix\nsymmetric means that in this case this 4\nand this 4 are the same because in\ngeneral symmetric means if you reflect\nthe matrix over the main diagonal you\nget the same matrix so the entries on\ntop of that ion are the same and the\nones below the diagonal that was a\ncorresponding one and when the matrix is\nsymmetric then the eigenvectors are\northogonal or perpendicular even in\nhigher dimensions another thing that is\npretty lucky is that the transformation\nis actually\nto stretchings this also doesn't happen\nall the time some transformations are\nrotations or different things and that's\nbecause sometimes the eigenvalues are\nimaginary numbers or complex numbers in\nthis case they are real which is also\nsomething that happens when the met\nmatrix is symmetric so because\ncovariance matrices are always symmetric\nbecause that's how we built them we put\nthe entry on top and on the bottom that\nthat four we repeated it so in this\nparticular case of covariance matrices\nthey always have very nice eigenvalues\nand eigen vectors that are perpendicular\nso what we're gonna do is we have\nsummarized our data set as the red\nvector and the green vector and what\nwe're gonna do is we're gonna say okay\nwhich one is more important which one do\nyou think is more important well the one\nwith the largest eigenvalue right one of\nthem matters 11 times the other one so\nlet's say the 11th one wins and the\nother one gets the boot so we delete\neverything that's green and only care\nabout what's red and what does that mean\nwell that means we're gonna consider\nthis entire line and we're gonna project\neverything into this line and that is\nthe projection that is the picture of\nthe data set and let's see what we did\nwhat we did is we turn our\ntwo-dimensional data set into this 1\ndimensional data set and because we\npicked projecting over the eigenvector\nwith the highest eigen value that means\nwe project over the axis that carries\nthe most amount of information ok so now\nwe're finally ready for the big picture\nof pca are you ready we start with a\nlarge table of data with lots of rows\nand lots of columns in this case we're\ngonna say five columns but you're gonna\nsee how this works in general so this\ngets plotted in a five dimensional plot\nI can't draw in five dimensions but it's\nmy best sort of imaginary representation\nof how things would work on five\ndimensions with five axis so now from\nthis data we get a five by five\ncovariance matrix in the main diagonal\nentries we see all the variances\nrespect to each of the dimensions and in\nthe off-diagonal entries we get the\ncovariance for each pair of dimensions\ntaking a at a time and notice that this\nis symmetric because we put the same\nentry on the top right then as a bottom\nleft so now that we have a matrix we\ncalculate a bunch of eigen stuff we\ncalculate five eigenvectors with there's\ncorresponding eigenvalues then we order\nthem from small to big and now it's up\nto us to pick how many of the big ones\nwe want that depends on the on the eigen\nvalue or on or on how many dimensions we\nwant to get to but let's say we pick the\ntop two and erase the bottom three so\nnow what do we do we go back to our four\ndimensional plot and plot those two\neigenvectors with the length being the\ncorresponding eigenvalue so we get this\nto the red and the green now this\ncreates a smaller dimensional space in\nthis case it creates a plane plane we\ncan see so now what we do is we project\neverything with respect to this plane so\nwe sort of take the shadow or like take\na picture that prints in this plane and\nwe get a two-dimensional plot and this\ntwo-dimensional plot is a pre faithful\nrepresentation of our five dimensional\nplot because we did it the smartest\npossible way by picking the directions\nin which the data is spread the most and\ntherefore from this two-dimensional plot\nwe get a smaller table with two columns\nthat actually captures the data as well\nas possible so that's it that's the\ndimensionality reduction and that's a\nbig picture of PCA and that's it for\nthis video thank you very much for your\nattention\nas usual if you liked this please\nsubscribe for more videos coming up\nplease hit like and share with your\nfriends and write a comment I'd love to\nread your comments and it's always very\nuseful for me to see suggestions of what\ntopics you would like to see etcetera\nand if you would like to tweet at me\nthis is my Twitter handle Louis likes\nmath so thank you very much and I'll see\nyou in the next video\n",
  "words": [
    "hello",
    "luis",
    "serrano",
    "video",
    "gon",
    "na",
    "learn",
    "principle",
    "component",
    "analysis",
    "pca",
    "principle",
    "component",
    "analysis",
    "pca",
    "one",
    "important",
    "dimensionality",
    "reduction",
    "techniques",
    "dimensionality",
    "reduction",
    "well",
    "let",
    "say",
    "huge",
    "table",
    "data",
    "big",
    "hard",
    "process",
    "would",
    "like",
    "make",
    "smaller",
    "one",
    "still",
    "keeping",
    "much",
    "information",
    "possible",
    "dimensionality",
    "reduction",
    "techniques",
    "reduce",
    "number",
    "columns",
    "today",
    "going",
    "learn",
    "pca",
    "steps",
    "first",
    "gon",
    "na",
    "learn",
    "mode",
    "projections",
    "another",
    "variance",
    "covariance",
    "covariance",
    "matrix",
    "learn",
    "give",
    "values",
    "eigenvectors",
    "finally",
    "gon",
    "na",
    "go",
    "pca",
    "let",
    "begin",
    "let",
    "start",
    "simple",
    "problem",
    "let",
    "say",
    "bunch",
    "friends",
    "standing",
    "like",
    "want",
    "take",
    "picture",
    "question",
    "take",
    "picture",
    "angle",
    "could",
    "one",
    "could",
    "one",
    "one",
    "well",
    "seems",
    "like",
    "would",
    "one",
    "best",
    "angles",
    "take",
    "picture",
    "right",
    "really",
    "dimensionality",
    "reduction",
    "taking",
    "picture",
    "data",
    "trying",
    "keep",
    "much",
    "information",
    "possible",
    "picture",
    "data",
    "looks",
    "like",
    "picture",
    "data",
    "could",
    "example",
    "projecting",
    "line",
    "get",
    "points",
    "another",
    "one",
    "could",
    "projecting",
    "line",
    "question",
    "intuitively",
    "two",
    "projections",
    "seems",
    "better",
    "let",
    "compare",
    "put",
    "well",
    "seems",
    "another",
    "one",
    "right",
    "better",
    "points",
    "spaced",
    "actually",
    "tell",
    "apart",
    "picture",
    "could",
    "actually",
    "make",
    "day",
    "little",
    "better",
    "one",
    "left",
    "gon",
    "na",
    "learn",
    "video",
    "obtain",
    "projection",
    "right",
    "get",
    "sort",
    "ideal",
    "line",
    "project",
    "data",
    "keep",
    "far",
    "possible",
    "first",
    "little",
    "parentheses",
    "matter",
    "let",
    "give",
    "example",
    "housing",
    "data",
    "let",
    "say",
    "housing",
    "data",
    "set",
    "bunch",
    "columns",
    "fall",
    "information",
    "sighs",
    "number",
    "rooms",
    "number",
    "bathrooms",
    "schools",
    "around",
    "area",
    "crime",
    "rate",
    "gave",
    "data",
    "set",
    "told",
    "many",
    "columns",
    "five",
    "want",
    "less",
    "would",
    "think",
    "feel",
    "free",
    "pause",
    "video",
    "think",
    "sort",
    "reduce",
    "keep",
    "information",
    "well",
    "balling",
    "see",
    "difference",
    "right",
    "first",
    "three",
    "features",
    "related",
    "size",
    "maybe",
    "lump",
    "together",
    "something",
    "called",
    "size",
    "feature",
    "second",
    "two",
    "related",
    "neighborhood",
    "let",
    "lump",
    "together",
    "something",
    "called",
    "location",
    "feature",
    "dimensionality",
    "reduction",
    "right",
    "went",
    "five",
    "columns",
    "2",
    "capturing",
    "information",
    "seems",
    "bigger",
    "house",
    "may",
    "also",
    "higher",
    "number",
    "rooms",
    "higher",
    "number",
    "bathrooms",
    "may",
    "need",
    "3",
    "piece",
    "information",
    "may",
    "need",
    "maybe",
    "maybe",
    "sum",
    "3",
    "maybe",
    "weighted",
    "sum",
    "thing",
    "second",
    "location",
    "feature",
    "ideally",
    "pca",
    "let",
    "look",
    "example",
    "going",
    "five",
    "columns",
    "two",
    "let",
    "go",
    "two",
    "columns",
    "one",
    "number",
    "rooms",
    "size",
    "gon",
    "na",
    "try",
    "lump",
    "one",
    "feature",
    "graph",
    "horizontal",
    "axis",
    "number",
    "rooms",
    "vertical",
    "axis",
    "size",
    "notice",
    "suspicion",
    "correct",
    "bigger",
    "house",
    "rooms",
    "variation",
    "part",
    "would",
    "projected",
    "data",
    "take",
    "picture",
    "houses",
    "look",
    "spaced",
    "apart",
    "possible",
    "well",
    "let",
    "seems",
    "like",
    "line",
    "would",
    "work",
    "right",
    "little",
    "aside",
    "linear",
    "regression",
    "trying",
    "fit",
    "closest",
    "line",
    "data",
    "trying",
    "fit",
    "one",
    "makes",
    "best",
    "projection",
    "slightly",
    "different",
    "definitely",
    "different",
    "purpose",
    "project",
    "cows",
    "line",
    "drawing",
    "perpendicular",
    "sending",
    "dataset",
    "changes",
    "little",
    "bit",
    "exactly",
    "one",
    "faithful",
    "data",
    "much",
    "much",
    "simpler",
    "see",
    "like",
    "one",
    "size",
    "feature",
    "sudden",
    "data",
    "set",
    "became",
    "simple",
    "went",
    "two",
    "dimensional",
    "data",
    "set",
    "table",
    "two",
    "columns",
    "one",
    "dimensional",
    "data",
    "sets",
    "table",
    "one",
    "column",
    "every",
    "house",
    "one",
    "number",
    "attached",
    "two",
    "two",
    "dimensions",
    "one",
    "dimension",
    "namely",
    "size",
    "number",
    "rooms",
    "size",
    "feature",
    "two",
    "dimensions",
    "one",
    "dimension",
    "graphically",
    "nice",
    "see",
    "imagine",
    "five",
    "dimensions",
    "two",
    "dimensions",
    "would",
    "going",
    "something",
    "like",
    "features",
    "size",
    "location",
    "feature",
    "hard",
    "picture",
    "five",
    "dimensions",
    "head",
    "imagine",
    "different",
    "one",
    "imagine",
    "five",
    "two",
    "seen",
    "statistics",
    "next",
    "couple",
    "minutes",
    "may",
    "look",
    "like",
    "ryu",
    "still",
    "find",
    "interesting",
    "like",
    "see",
    "things",
    "geometric",
    "way",
    "mean",
    "set",
    "numbers",
    "well",
    "let",
    "say",
    "three",
    "weights",
    "exactly",
    "want",
    "balance",
    "balance",
    "find",
    "perfect",
    "point",
    "balance",
    "three",
    "weights",
    "let",
    "assume",
    "bar",
    "weight",
    "gon",
    "na",
    "gon",
    "na",
    "imagine",
    "wall",
    "left",
    "gon",
    "na",
    "measure",
    "distance",
    "every",
    "point",
    "wall",
    "one",
    "distance",
    "one",
    "one",
    "one",
    "one",
    "six",
    "gon",
    "na",
    "take",
    "average",
    "points",
    "mean",
    "one",
    "plus",
    "two",
    "plus",
    "six",
    "divided",
    "three",
    "point",
    "equilibrium",
    "put",
    "balance",
    "three",
    "points",
    "would",
    "balance",
    "lot",
    "things",
    "say",
    "set",
    "example",
    "three",
    "points",
    "three",
    "points",
    "tell",
    "apart",
    "well",
    "using",
    "mean",
    "mean",
    "notice",
    "balance",
    "point",
    "see",
    "points",
    "top",
    "lot",
    "closer",
    "ones",
    "bottom",
    "lot",
    "spread",
    "measure",
    "gon",
    "na",
    "gon",
    "na",
    "take",
    "distance",
    "point",
    "center",
    "point",
    "left",
    "distance",
    "one",
    "center",
    "point",
    "right",
    "distance",
    "one",
    "center",
    "point",
    "middle",
    "distance",
    "zero",
    "descent",
    "center",
    "actually",
    "center",
    "point",
    "points",
    "balance",
    "thing",
    "bottom",
    "except",
    "5",
    "5",
    "0",
    "variance",
    "gon",
    "na",
    "add",
    "numbers",
    "actually",
    "gon",
    "na",
    "square",
    "reason",
    "square",
    "actually",
    "take",
    "distance",
    "goes",
    "towards",
    "left",
    "subtracting",
    "coordinates",
    "gon",
    "na",
    "get",
    "negative",
    "number",
    "square",
    "everything",
    "get",
    "everything",
    "positive",
    "top",
    "get",
    "1",
    "squared",
    "plus",
    "0",
    "squared",
    "plus",
    "1",
    "squared",
    "divided",
    "3",
    "bottom",
    "one",
    "get",
    "5",
    "squared",
    "plus",
    "0",
    "squared",
    "plus",
    "5",
    "squared",
    "divided",
    "3",
    "50",
    "3",
    "much",
    "bigger",
    "basically",
    "measure",
    "spread",
    "set",
    "variance",
    "original",
    "set",
    "well",
    "center",
    "point",
    "left",
    "side",
    "distance",
    "next",
    "point",
    "distance",
    "1",
    "point",
    "rises",
    "distance",
    "3",
    "variance",
    "2",
    "squared",
    "plus",
    "1",
    "squared",
    "plus",
    "3",
    "squared",
    "divided",
    "3",
    "average",
    "squares",
    "14",
    "divided",
    "3",
    "see",
    "makes",
    "sense",
    "3",
    "kind",
    "spread",
    "question",
    "variance",
    "data",
    "sets",
    "set",
    "plane",
    "mean",
    "spread",
    "well",
    "two",
    "things",
    "ask",
    "spread",
    "horizontal",
    "direction",
    "spread",
    "vertical",
    "direction",
    "basically",
    "forget",
    "height",
    "send",
    "everything",
    "horizontal",
    "x",
    "axis",
    "calculate",
    "variance",
    "points",
    "experience",
    "thing",
    "forgetting",
    "x",
    "coordinate",
    "sending",
    "everything",
    "vertical",
    "axis",
    "points",
    "calculate",
    "variance",
    "two",
    "numbers",
    "x",
    "variance",
    "variance",
    "tell",
    "us",
    "well",
    "spread",
    "set",
    "however",
    "technicalities",
    "let",
    "look",
    "set",
    "let",
    "look",
    "set",
    "x",
    "variances",
    "two",
    "well",
    "let",
    "calculate",
    "x",
    "variance",
    "let",
    "say",
    "distances",
    "2",
    "gon",
    "na",
    "2",
    "squared",
    "plus",
    "0",
    "squared",
    "plus",
    "2",
    "squared",
    "divided",
    "3",
    "8",
    "divided",
    "3",
    "variance",
    "well",
    "distances",
    "one",
    "one",
    "squared",
    "plus",
    "0",
    "squared",
    "plus",
    "1",
    "squared",
    "divided",
    "3",
    "two",
    "sets",
    "x",
    "variance",
    "way",
    "variance",
    "fundamentally",
    "different",
    "tell",
    "two",
    "apart",
    "third",
    "metric",
    "tells",
    "us",
    "two",
    "different",
    "answer",
    "yes",
    "gon",
    "na",
    "covariance",
    "question",
    "covariance",
    "answers",
    "tell",
    "3",
    "points",
    "apart",
    "three",
    "points",
    "let",
    "think",
    "think",
    "number",
    "metric",
    "tells",
    "two",
    "sets",
    "apart",
    "pause",
    "video",
    "think",
    "tell",
    "one",
    "think",
    "product",
    "coordinates",
    "look",
    "two",
    "numbers",
    "multiply",
    "201",
    "1",
    "etc",
    "well",
    "let",
    "see",
    "point",
    "middle",
    "0",
    "times",
    "0",
    "obviously",
    "0",
    "top",
    "right",
    "bottom",
    "left",
    "corner",
    "actually",
    "plus",
    "2",
    "2",
    "times",
    "1",
    "2",
    "minus",
    "2",
    "times",
    "minus",
    "1",
    "also",
    "2",
    "air",
    "two",
    "points",
    "minus",
    "2",
    "2",
    "times",
    "minus",
    "1",
    "minus",
    "2",
    "times",
    "1",
    "minus",
    "2",
    "product",
    "coordinates",
    "positive",
    "two",
    "points",
    "negative",
    "two",
    "points",
    "therefore",
    "one",
    "middle",
    "ok",
    "still",
    "still",
    "helps",
    "us",
    "tell",
    "points",
    "apart",
    "basically",
    "covariances",
    "sum",
    "products",
    "coordinates",
    "see",
    "sum",
    "squares",
    "one",
    "coordinates",
    "sum",
    "products",
    "coordinates",
    "let",
    "calculate",
    "covariance",
    "set",
    "see",
    "products",
    "0",
    "minus",
    "2",
    "set",
    "products",
    "2",
    "0",
    "2",
    "covariance",
    "negative",
    "2",
    "plus",
    "0",
    "plus",
    "negative",
    "2",
    "minus",
    "4",
    "3",
    "divided",
    "3",
    "average",
    "one",
    "right",
    "2",
    "plus",
    "0",
    "plus",
    "2",
    "divided",
    "3",
    "4",
    "3",
    "point",
    "left",
    "negative",
    "covariance",
    "point",
    "ins",
    "right",
    "positive",
    "covariance",
    "heard",
    "team",
    "correlation",
    "also",
    "related",
    "points",
    "left",
    "negatively",
    "correlated",
    "ones",
    "right",
    "positively",
    "correlated",
    "ones",
    "left",
    "x",
    "direction",
    "grows",
    "direction",
    "decreases",
    "whereas",
    "ones",
    "acceleration",
    "grows",
    "grows",
    "well",
    "let",
    "look",
    "another",
    "set",
    "try",
    "calculate",
    "covariance",
    "set",
    "think",
    "covariance",
    "one",
    "gon",
    "na",
    "well",
    "let",
    "tell",
    "let",
    "calculate",
    "products",
    "0",
    "2",
    "according",
    "numbers",
    "covariance",
    "sum",
    "things",
    "see",
    "cancel",
    "sum",
    "things",
    "divided",
    "9",
    "average",
    "actually",
    "0",
    "made",
    "sense",
    "set",
    "positively",
    "correlated",
    "negatively",
    "correlated",
    "look",
    "like",
    "diagonal",
    "either",
    "one",
    "directions",
    "looks",
    "like",
    "sort",
    "thing",
    "middle",
    "looking",
    "type",
    "set",
    "sort",
    "tell",
    "covariance",
    "set",
    "like",
    "like",
    "like",
    "think",
    "covariance",
    "first",
    "one",
    "looks",
    "like",
    "inverted",
    "diagonal",
    "negative",
    "covariance",
    "first",
    "one",
    "looks",
    "like",
    "sort",
    "well",
    "centered",
    "probably",
    "covariance",
    "zero",
    "small",
    "covariance",
    "third",
    "one",
    "bus",
    "positive",
    "covariance",
    "looks",
    "like",
    "sort",
    "forwards",
    "diagonal",
    "covariance",
    "gon",
    "na",
    "help",
    "us",
    "lot",
    "pca",
    "ready",
    "start",
    "thinking",
    "pca",
    "let",
    "say",
    "data",
    "set",
    "like",
    "one",
    "find",
    "perfect",
    "projection",
    "well",
    "first",
    "let",
    "put",
    "coordinate",
    "axes",
    "first",
    "gon",
    "na",
    "center",
    "center",
    "means",
    "gon",
    "na",
    "take",
    "average",
    "coordinates",
    "x",
    "direction",
    "find",
    "center",
    "mass",
    "think",
    "point",
    "set",
    "would",
    "balance",
    "move",
    "zero",
    "going",
    "move",
    "data",
    "set",
    "centered",
    "zero",
    "gon",
    "na",
    "create",
    "2",
    "2",
    "matrix",
    "2",
    "2",
    "matrix",
    "going",
    "simple",
    "going",
    "formed",
    "two",
    "variances",
    "covariance",
    "data",
    "set",
    "top",
    "left",
    "corner",
    "gon",
    "na",
    "put",
    "x",
    "variance",
    "variance",
    "horizontal",
    "axis",
    "coordinate",
    "gon",
    "na",
    "put",
    "variance",
    "variance",
    "vertical",
    "direction",
    "two",
    "spots",
    "gon",
    "na",
    "put",
    "covariance",
    "gon",
    "na",
    "put",
    "twice",
    "literature",
    "matrix",
    "normally",
    "called",
    "sigma",
    "gon",
    "na",
    "call",
    "covariance",
    "matrix",
    "let",
    "make",
    "numbers",
    "data",
    "set",
    "looks",
    "like",
    "could",
    "covariance",
    "matrix",
    "nine",
    "four",
    "four",
    "three",
    "nine",
    "experience",
    "dancer",
    "looks",
    "like",
    "pretty",
    "spread",
    "x",
    "direction",
    "much",
    "spread",
    "direction",
    "variance",
    "wise",
    "may",
    "something",
    "like",
    "three",
    "looks",
    "like",
    "positive",
    "covariance",
    "looks",
    "like",
    "like",
    "forwards",
    "diagonal",
    "let",
    "say",
    "covariance",
    "four",
    "gon",
    "na",
    "little",
    "aside",
    "linear",
    "transformations",
    "matrix",
    "gon",
    "na",
    "think",
    "linear",
    "transformation",
    "get",
    "scared",
    "term",
    "linear",
    "transformation",
    "really",
    "function",
    "map",
    "say",
    "plane",
    "plane",
    "could",
    "also",
    "higher",
    "dimensions",
    "gon",
    "na",
    "two",
    "sends",
    "point",
    "plane",
    "left",
    "point",
    "plane",
    "right",
    "following",
    "way",
    "using",
    "numbers",
    "matrix",
    "point",
    "coordinates",
    "x",
    "comma",
    "gets",
    "sent",
    "point",
    "9",
    "x",
    "4",
    "4",
    "x",
    "3",
    "two",
    "coordinates",
    "go",
    "two",
    "coordinates",
    "numbers",
    "taken",
    "matrix",
    "nine",
    "getting",
    "coded",
    "9x",
    "403",
    "getting",
    "coded",
    "4",
    "x",
    "3",
    "feel",
    "comfortable",
    "thinking",
    "points",
    "two",
    "rows",
    "instead",
    "two",
    "columns",
    "way",
    "feel",
    "free",
    "think",
    "think",
    "matrix",
    "multiplication",
    "want",
    "simplify",
    "think",
    "way",
    "let",
    "see",
    "transformation",
    "let",
    "see",
    "several",
    "points",
    "point",
    "0",
    "0",
    "go",
    "well",
    "9",
    "times",
    "0",
    "plus",
    "4",
    "times",
    "0",
    "well",
    "0",
    "4",
    "times",
    "0",
    "plus",
    "3",
    "times",
    "0",
    "0",
    "book",
    "goes",
    "point",
    "0",
    "0",
    "let",
    "draw",
    "point",
    "left",
    "point",
    "0",
    "0",
    "goes",
    "point",
    "0",
    "0",
    "simple",
    "let",
    "another",
    "slightly",
    "complicated",
    "point",
    "think",
    "go",
    "point",
    "1",
    "0",
    "goes",
    "moment",
    "feels",
    "pause",
    "video",
    "think",
    "1",
    "0",
    "goes",
    "9",
    "4",
    "plug",
    "numbers",
    "1",
    "0",
    "x",
    "get",
    "9",
    "4",
    "4",
    "9",
    "x",
    "4",
    "get",
    "9",
    "times",
    "1",
    "4",
    "times",
    "0",
    "4",
    "times",
    "1",
    "3",
    "times",
    "0",
    "goes",
    "point",
    "nine",
    "four",
    "gon",
    "na",
    "send",
    "point",
    "1",
    "0",
    "left",
    "point",
    "nine",
    "four",
    "right",
    "drawn",
    "scale",
    "trying",
    "capture",
    "entire",
    "entire",
    "movement",
    "next",
    "point",
    "would",
    "0",
    "1",
    "math",
    "goes",
    "4",
    "3",
    "goes",
    "next",
    "point",
    "minus",
    "1",
    "0",
    "goes",
    "minus",
    "9",
    "minus",
    "4",
    "finally",
    "point",
    "0",
    "minus",
    "1",
    "goes",
    "point",
    "minus",
    "4",
    "3",
    "hopefully",
    "see",
    "pattern",
    "right",
    "matter",
    "fact",
    "take",
    "circle",
    "unit",
    "sphere",
    "think",
    "goes",
    "well",
    "goes",
    "ellipse",
    "almost",
    "seems",
    "like",
    "stretching",
    "plane",
    "sort",
    "diagonal",
    "direction",
    "actually",
    "two",
    "stretchings",
    "one",
    "small",
    "really",
    "linear",
    "transformation",
    "let",
    "think",
    "vector",
    "red",
    "vector",
    "actually",
    "goes",
    "one",
    "happens",
    "sort",
    "stretch",
    "plane",
    "direction",
    "direction",
    "calculate",
    "minute",
    "2",
    "1",
    "2",
    "units",
    "right",
    "one",
    "unit",
    "direction",
    "vector",
    "magnitude",
    "11",
    "see",
    "11",
    "basically",
    "plane",
    "got",
    "stretched",
    "factor",
    "11",
    "direction",
    "another",
    "vector",
    "direction",
    "1",
    "2",
    "one",
    "gets",
    "sent",
    "actually",
    "stretched",
    "direction",
    "1",
    "sometimes",
    "different",
    "numbers",
    "basically",
    "think",
    "2",
    "stretchings",
    "happen",
    "perpendicular",
    "particular",
    "case",
    "let",
    "let",
    "see",
    "directions",
    "2",
    "1",
    "eigenvectors",
    "give",
    "us",
    "direction",
    "eigenvalues",
    "factors",
    "stretching",
    "11",
    "1",
    "said",
    "called",
    "eigenvalues",
    "give",
    "us",
    "magnet",
    "think",
    "linear",
    "transformation",
    "given",
    "eigenvectors",
    "eigenvalues",
    "direction",
    "one",
    "two",
    "units",
    "right",
    "one",
    "stretch",
    "eleven",
    "direction",
    "minus",
    "one",
    "unit",
    "left",
    "minus",
    "one",
    "two",
    "units",
    "stretch",
    "factor",
    "one",
    "much",
    "transformation",
    "basically",
    "notice",
    "two",
    "vectors",
    "special",
    "average",
    "vector",
    "anything",
    "goes",
    "like",
    "direction",
    "example",
    "drawing",
    "random",
    "vector",
    "goes",
    "thing",
    "red",
    "vector",
    "goes",
    "something",
    "points",
    "direction",
    "green",
    "vector",
    "goes",
    "something",
    "points",
    "direction",
    "two",
    "eigenvectors",
    "two",
    "vectors",
    "actually",
    "go",
    "something",
    "points",
    "direction",
    "basically",
    "trying",
    "say",
    "linear",
    "transformation",
    "matrix",
    "vectors",
    "crazy",
    "thing",
    "two",
    "particular",
    "vectors",
    "applying",
    "vector",
    "v",
    "multiplying",
    "vector",
    "b",
    "via",
    "number",
    "right",
    "scaling",
    "sending",
    "direction",
    "eigenvectors",
    "red",
    "green",
    "vector",
    "value",
    "eigenvalue",
    "many",
    "ways",
    "calculate",
    "given",
    "values",
    "eigenvectors",
    "favorite",
    "type",
    "matrix",
    "wolfram",
    "alpha",
    "tells",
    "everything",
    "another",
    "right",
    "way",
    "wo",
    "get",
    "much",
    "detail",
    "basically",
    "take",
    "matrix",
    "take",
    "characteristic",
    "polynomial",
    "subtract",
    "x",
    "minus",
    "things",
    "diagonal",
    "everything",
    "diagonal",
    "take",
    "determinant",
    "formula",
    "express",
    "quadratic",
    "factor",
    "quadratic",
    "two",
    "things",
    "magically",
    "get",
    "x",
    "minus",
    "11x",
    "minus",
    "1",
    "going",
    "little",
    "fast",
    "something",
    "really",
    "interested",
    "read",
    "eigenvalues",
    "gon",
    "na",
    "two",
    "roots",
    "polynomial",
    "xi",
    "1",
    "find",
    "eigenvalues",
    "find",
    "eigenvectors",
    "well",
    "eigenvalues",
    "transformation",
    "times",
    "vector",
    "uv",
    "reusing",
    "letter",
    "v",
    "matter",
    "two",
    "coordinates",
    "equal",
    "11",
    "times",
    "vector",
    "matrix",
    "times",
    "another",
    "vector",
    "1",
    "times",
    "vector",
    "solve",
    "students",
    "equations",
    "u",
    "v",
    "get",
    "eigenvector",
    "1",
    "eigenvector",
    "minus",
    "find",
    "interesting",
    "want",
    "see",
    "amazing",
    "animations",
    "amazing",
    "explanations",
    "go",
    "channel",
    "three",
    "blue",
    "one",
    "brown",
    "whole",
    "series",
    "linear",
    "algebra",
    "treatment",
    "eigenvectors",
    "eigenvalues",
    "actually",
    "fantastic",
    "may",
    "little",
    "bit",
    "much",
    "math",
    "think",
    "moral",
    "says",
    "linear",
    "transformation",
    "stretching",
    "plane",
    "two",
    "directions",
    "directions",
    "given",
    "eigenvectors",
    "amount",
    "stretching",
    "given",
    "eigenvalues",
    "okay",
    "ready",
    "pca",
    "first",
    "start",
    "data",
    "set",
    "center",
    "calculate",
    "covariance",
    "matrix",
    "saw",
    "x",
    "variance",
    "9",
    "variance",
    "3",
    "covariance",
    "4",
    "matrix",
    "eigenvectors",
    "give",
    "us",
    "direction",
    "give",
    "values",
    "choose",
    "magnitude",
    "first",
    "one",
    "direction",
    "1",
    "9211",
    "one",
    "second",
    "wise",
    "direction",
    "minus",
    "magnitude",
    "1",
    "one",
    "notice",
    "two",
    "eigen",
    "vectors",
    "eigen",
    "values",
    "tell",
    "us",
    "lot",
    "data",
    "set",
    "right",
    "sort",
    "tells",
    "us",
    "direction",
    "spread",
    "much",
    "also",
    "notice",
    "red",
    "green",
    "vectors",
    "perpendicular",
    "luck",
    "something",
    "happens",
    "symmetric",
    "matrix",
    "symmetric",
    "means",
    "case",
    "4",
    "4",
    "general",
    "symmetric",
    "means",
    "reflect",
    "matrix",
    "main",
    "diagonal",
    "get",
    "matrix",
    "entries",
    "top",
    "ion",
    "ones",
    "diagonal",
    "corresponding",
    "one",
    "matrix",
    "symmetric",
    "eigenvectors",
    "orthogonal",
    "perpendicular",
    "even",
    "higher",
    "dimensions",
    "another",
    "thing",
    "pretty",
    "lucky",
    "transformation",
    "actually",
    "stretchings",
    "also",
    "happen",
    "time",
    "transformations",
    "rotations",
    "different",
    "things",
    "sometimes",
    "eigenvalues",
    "imaginary",
    "numbers",
    "complex",
    "numbers",
    "case",
    "real",
    "also",
    "something",
    "happens",
    "met",
    "matrix",
    "symmetric",
    "covariance",
    "matrices",
    "always",
    "symmetric",
    "built",
    "put",
    "entry",
    "top",
    "bottom",
    "four",
    "repeated",
    "particular",
    "case",
    "covariance",
    "matrices",
    "always",
    "nice",
    "eigenvalues",
    "eigen",
    "vectors",
    "perpendicular",
    "gon",
    "na",
    "summarized",
    "data",
    "set",
    "red",
    "vector",
    "green",
    "vector",
    "gon",
    "na",
    "gon",
    "na",
    "say",
    "okay",
    "one",
    "important",
    "one",
    "think",
    "important",
    "well",
    "one",
    "largest",
    "eigenvalue",
    "right",
    "one",
    "matters",
    "11",
    "times",
    "one",
    "let",
    "say",
    "11th",
    "one",
    "wins",
    "one",
    "gets",
    "boot",
    "delete",
    "everything",
    "green",
    "care",
    "red",
    "mean",
    "well",
    "means",
    "gon",
    "na",
    "consider",
    "entire",
    "line",
    "gon",
    "na",
    "project",
    "everything",
    "line",
    "projection",
    "picture",
    "data",
    "set",
    "let",
    "see",
    "turn",
    "data",
    "set",
    "1",
    "dimensional",
    "data",
    "set",
    "picked",
    "projecting",
    "eigenvector",
    "highest",
    "eigen",
    "value",
    "means",
    "project",
    "axis",
    "carries",
    "amount",
    "information",
    "ok",
    "finally",
    "ready",
    "big",
    "picture",
    "pca",
    "ready",
    "start",
    "large",
    "table",
    "data",
    "lots",
    "rows",
    "lots",
    "columns",
    "case",
    "gon",
    "na",
    "say",
    "five",
    "columns",
    "gon",
    "na",
    "see",
    "works",
    "general",
    "gets",
    "plotted",
    "five",
    "dimensional",
    "plot",
    "ca",
    "draw",
    "five",
    "dimensions",
    "best",
    "sort",
    "imaginary",
    "representation",
    "things",
    "would",
    "work",
    "five",
    "dimensions",
    "five",
    "axis",
    "data",
    "get",
    "five",
    "five",
    "covariance",
    "matrix",
    "main",
    "diagonal",
    "entries",
    "see",
    "variances",
    "respect",
    "dimensions",
    "entries",
    "get",
    "covariance",
    "pair",
    "dimensions",
    "taking",
    "time",
    "notice",
    "symmetric",
    "put",
    "entry",
    "top",
    "right",
    "bottom",
    "left",
    "matrix",
    "calculate",
    "bunch",
    "eigen",
    "stuff",
    "calculate",
    "five",
    "eigenvectors",
    "corresponding",
    "eigenvalues",
    "order",
    "small",
    "big",
    "us",
    "pick",
    "many",
    "big",
    "ones",
    "want",
    "depends",
    "eigen",
    "value",
    "many",
    "dimensions",
    "want",
    "get",
    "let",
    "say",
    "pick",
    "top",
    "two",
    "erase",
    "bottom",
    "three",
    "go",
    "back",
    "four",
    "dimensional",
    "plot",
    "plot",
    "two",
    "eigenvectors",
    "length",
    "corresponding",
    "eigenvalue",
    "get",
    "red",
    "green",
    "creates",
    "smaller",
    "dimensional",
    "space",
    "case",
    "creates",
    "plane",
    "plane",
    "see",
    "project",
    "everything",
    "respect",
    "plane",
    "sort",
    "take",
    "shadow",
    "like",
    "take",
    "picture",
    "prints",
    "plane",
    "get",
    "plot",
    "plot",
    "pre",
    "faithful",
    "representation",
    "five",
    "dimensional",
    "plot",
    "smartest",
    "possible",
    "way",
    "picking",
    "directions",
    "data",
    "spread",
    "therefore",
    "plot",
    "get",
    "smaller",
    "table",
    "two",
    "columns",
    "actually",
    "captures",
    "data",
    "well",
    "possible",
    "dimensionality",
    "reduction",
    "big",
    "picture",
    "pca",
    "video",
    "thank",
    "much",
    "attention",
    "usual",
    "liked",
    "please",
    "subscribe",
    "videos",
    "coming",
    "please",
    "hit",
    "like",
    "share",
    "friends",
    "write",
    "comment",
    "love",
    "read",
    "comments",
    "always",
    "useful",
    "see",
    "suggestions",
    "topics",
    "would",
    "like",
    "see",
    "etcetera",
    "would",
    "like",
    "tweet",
    "twitter",
    "handle",
    "louis",
    "likes",
    "math",
    "thank",
    "much",
    "see",
    "next",
    "video"
  ],
  "keywords": [
    "video",
    "gon",
    "na",
    "learn",
    "pca",
    "one",
    "dimensionality",
    "reduction",
    "well",
    "let",
    "say",
    "table",
    "data",
    "big",
    "would",
    "like",
    "much",
    "information",
    "possible",
    "number",
    "columns",
    "going",
    "first",
    "another",
    "variance",
    "covariance",
    "matrix",
    "give",
    "eigenvectors",
    "go",
    "want",
    "take",
    "picture",
    "could",
    "seems",
    "right",
    "trying",
    "looks",
    "example",
    "line",
    "get",
    "points",
    "two",
    "put",
    "actually",
    "tell",
    "apart",
    "little",
    "left",
    "sort",
    "project",
    "set",
    "rooms",
    "five",
    "think",
    "see",
    "three",
    "size",
    "something",
    "feature",
    "2",
    "may",
    "also",
    "3",
    "sum",
    "thing",
    "look",
    "axis",
    "notice",
    "linear",
    "different",
    "perpendicular",
    "dimensional",
    "dimensions",
    "next",
    "find",
    "things",
    "way",
    "mean",
    "numbers",
    "balance",
    "point",
    "distance",
    "average",
    "plus",
    "divided",
    "lot",
    "top",
    "ones",
    "bottom",
    "spread",
    "center",
    "0",
    "goes",
    "coordinates",
    "negative",
    "everything",
    "positive",
    "1",
    "squared",
    "basically",
    "plane",
    "direction",
    "x",
    "calculate",
    "us",
    "times",
    "minus",
    "products",
    "4",
    "9",
    "diagonal",
    "directions",
    "means",
    "nine",
    "four",
    "transformation",
    "vector",
    "red",
    "11",
    "case",
    "eigenvalues",
    "vectors",
    "green",
    "eigen",
    "symmetric",
    "plot"
  ]
}