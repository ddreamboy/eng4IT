{
  "text": "I'm sure you all agree\nthat machine learning is one\nof the hottest Trend in today's\nmarket right Gartner predicts\nthat by 2022 there\nwould be at least 40%\nof new application development\nproject going on in the market\nthat would be requiring\nmachine learning co-developers\non their team.\nIt's expected that these project\nwill generate a revenue\nof around three point\nnine trillion dollar,\nisn't it cute so\nlooking at the huge?\nUpcoming demand of machine\nlearning around the world.\nWe guys at Eureka have come up\nand designed a well-structured\nmachine learning full course\nfor you guys.\nBut before we actually\ndrill down over there,\nlet me just introduce myself.\nHello all I am Atul from Edureka.\nAnd today I'll be guiding you\nthrough this entire\nmachine learning course.\nWell, this course\nhas been designed in a way\nthat you get the most out of it.\nSo we'll slowly\nand gradually start\nwith a beginner level and then\nmove towards the advanced topic.\nSo without delaying any further,\nlet's start with the agenda\nof today's Action\non machine learning\ncourse has been segregated\ninto six different module\nwill start our first module\nwith introduction to\nmachine learning here.\nWe'll discuss things.\nLike what exactly\nis machine learning\nhow it differs from artificial\nintelligence and the planning\nwhat is various types\nor dead space application\nand finally we'll end\nup first module\nwith a basic demo and python.\nOkay a second module\nfocuses on starts\nand probability here\nwill cover things\nlike descriptive statistics and\ninferential statistics to Bob.\nRarity Theory and so\non our third module\nis unsupervised learning.\nWell supervised learning is one\nof a type of machine learning\nwhich focuses mainly\non regression and\nclassification type of problem.\nIt deals with label data sets\nand the algorithm\nwhich are a part of it\nare linear regression\nlogistic regression Napier's\nrandom Forest decision tree\nand so on.\nOur fourth module is\non unsupervised learning.\nWell this module focuses\nmainly on dealing\nwith unlabeled data sets\nand the algorithm\nwhich are a part.\nOffered or k-means algorithm\nand a priori algorithm as\na part of fifth module.\nWe have reinforcement\nlearning here.\nWe are going to discuss\nabout reinforcement learning\nand depth on also\nabout Q learning algorithm\nfinally in the end.\nIt's all about to make\nyou industry ready.\nOkay.\nSo here we are going to discuss\nabout three different projects\nwhich are based\non supervised learning\nand unsupervised learning\nand reinforcement learning\nfinally in the end.\nI tell you about some\nof the skills\nthat you need to become\na machine learnings and Jean.\nNia okay, and also I\nam discussing about some\nof the important questions\nthat are asked in a\nmachine-learning interview fine\nwith this we come\nto the end of this agenda\nbefore you move ahead\ndon't forget to subscribe\nto a dareka and press\nthe Bell icon to never miss\nany update from us.\nHello everyone.\nThis is a toll from Eureka\nand welcome to today's session\non what is machine learning.\nAs you know,\nwe are living\nin a world of humans\nand machines humans\nhave been evolving\nand learning from the past\nexperience since millions\nof years on the other hand\nthe era of machines\nand robots have just\nbegun in today's world.\nThese machines are\nthe rewards are\nlike they need to be program\nbefore they actually\nfollow your instructions.\nBut what if the machine\nstarted to learn\non their own and this is\nwhere machine learning comes\ninto picture machine\nlearning is the core\nof many futuristic technology\nadvancement in our world.\nAnd today you can\nsee various examples\nor implementation of\nmachine learning around us\nsuch as Tesla's self-driving\ncar Apple Siri, Sophia.\nI do bot and many\nmore are there.\nSo what exactly\nis machine learning?\nWell Machine learning\nis a subfield\nof artificial intelligence\nthat focuses on\nthe design of system\nthat can learn from\nand make decisions\nand predictions based\non the experience\nwhich is data in the case\nof machines machine learning\nenables computer to act\nand make data-driven\ndecisions rather than\nBeing explicitly programmed\nto carry out a certain\ntask these programs\nare designed to learn\nand improve over time\nwhen exposed to new data.\nLet's move on and discuss one\nof the biggest confusion\nof the people in the world.\nThey think that all\nthe three of them\nthe AI the machine learning and\nthe Deep learning all are same,\nyou know, what they are wrong.\nLet me clarify things\nfor you artificial intelligence\nis a broader concept\nof machines being able to carry\nout tasks in a smarter way.\nIt covers anything which enables\nthe computer to be.\nHave like humans think of a\nfamous Turing test to determine\nwhether a computer is\ncapable of thinking\nlike a human being or not.\nIf you are talking\nto Siri on your phone\nand you get an answer you're\nalready very close to it.\nSo this was about the artificial\nintelligence now coming\nto the machine learning part.\nSo as I already said\nmachine learning is a subset\nor a current application\nof AI it is based on the idea\nthat we should be able\nto give machine the access\nto data and let them learn\nfrom done cells.\nIt's a subset\nof artificial intelligence.\nIs that deals\nwith the extraction\nof pattern from data set?\nThis means that the machine\ncan not only find the rules\nfor optimal Behavior,\nbut also can adapt\nto the changes in the world many\nof the algorithms\ninvolved have been known\nfor decades centuries\neven thanks to the advances\nin the computer science\nand parallel Computing.\nThey can now scale up\nto massive data volumes.\nSo this was about the machine\nlearning part now coming over\nto deep learning deep learning\nis a subset of machine learning\nwhere similar machine learning.\nTamar used to train\ndeep neural network.\nSo as to achieve better\naccuracy in those cases\nwhere former was not performing\nup to the mark, right?\nI hope now you understood\nthat machine learning Ai\nand deep learning\nall three are different.\nOkay moving on ahead.\nLet's see in general\nhow a machine learning work.\nOne of the approaches is\nwhere the machine learning\nalgorithm is strained\nusing a labeled\nor unlabeled training data\nset to produce a model\nnew input data is introduced to\nthe machine learning algorithm\nand it make prediction\nbased on the model.\nThe prediction is\nevaluated for accuracy.\nAnd if the accuracy\nis acceptable the machine\nlearning algorithm is deployed.\nNow if the accuracy\nis not acceptable\nthe machine learning\nalgorithm is strained again,\nand again with an argument\na training data set.\nThis was just\nin high-level example\nas they are many more factor\nand other steps involved in it.\nNow, let's move on\nand subcategorize the Machine\nlearning into three different\ntypes the supervised learning\nand unsupervised learning\nand reinforcement\nlearning and let's see what each\nof them are how they work.\nWork and how each\nof them is used in the field\nof banking Healthcare retail\nand other domains.\nDon't worry.\nI'll make sure\nthat I use enough examples\nand implementation of all three\nof them to give you\na proper understanding of it.\nSo starting with\nsupervised learning.\nWhat is it?\nSo let's see\na mathematical definition\nof supervised learning\nsupervised learning is\nwhere you have input variables X\nand an output variable Y\nand you use an algorithm\nto learn the mapping function\nfrom the input to the output.\nThat is y Affects\nthe goal is to approximate\nthe mapping function.\nSo well that whenever\nyou have a new input data\nX you could predict\nthe output variable.\nThat is why\nfor that data, right?\nI think this\nwas confusing for you.\nLet me simplify the definition\nof supervised learning\nso we can rephrase\nthe understanding\nof the mathematical definition\nas a machine learning method\nwhere each instances of\na training data set is composed\nof different input attribute\nand an expected output\nthe input attributes\nof a training data set can be\nof any End of data it can be\na pixel of the image.\nIt can be a value\nof a data base row\nor it can even be an audio\nfrequency histogram right\nfor each input instance\nand expected output values\nAssociated value can be discreet\nrepresenting a category\nor can be a real or continuous\nvalue in either case.\nThe algorithm learns\nthe input pattern\nthat generate the\nexpected output now\nonce the algorithm is strain,\nit can be used to predict\nthe correct output\nof a never seen input.\nYou can see I image\non your screen right\nin this image.\nAnd see that we are feeding\nraw inputs as image of Apple\nto the algorithm as a part\nof the algorithm.\nWe have a supervisor\nwho keeps on correcting\nthe machine or who keeps\non training the machine.\nIt keeps on telling him\nthat yes, it is a Apple.\nNo, it is not an apple\nthings like that.\nSo this process keeps\non repeating until we\nget a final train model.\nOnce the model is ready.\nIt can easily predict\nthe correct output\nof a never seen input\nin this slide.\nYou can see\nthat we are giving an image\nof a green apple to the machine\nand the Machine can easily\nidentify it as yes,\nit is an apple and it is giving\nthe correct result right?\nLet me make things\nmore clearer to you.\nLet's discuss another\nexample of it.\nSo in this Slide,\nthe image shows an example\nof a supervised learning process\nused to produce a model\nwhich is capable of recognizing\nthe ducks in the image.\nThe training data set\nis composed of labeled picture\nof ducks and non Ducks.\nThe result of supervised\nlearning process is\na predictor model\nwhich is capable\nof associating a label duck.\nOr not duck to the new image\npresented to the model.\nNow one strain,\nthe resulting predictive\nmodel can be deployed\nto the production environment.\nYou can see a mobile app.\nFor example once deployed\nit is ready to recognize\nthe new pictures right now.\nYou might be wondering\nwhy this category\nof machine learning is named\nas supervised learning.\nWell, it is called\na supervised learning\nbecause the process\nof an algorithm learning\nfrom the training data\nset can be thought\nof as a teacher supervising\nthe learning process\nif we know the correct answers.\nI will go Rhythm\niteratively makes\nwhile predicting on\nthe training data\nand is corrected by\nthe teacher the learning stops\nwhen the algorithm achieves an\nacceptable level of performance.\nNow, let's move on and see some\nof the popular supervised\nlearning algorithm.\nSo we have linear\nregression random forest\nand support Vector machines.\nThese are just\nfor your information.\nWe will discuss\nabout these algorithms\nin our next video.\nNow, let's see some\nof the popular use cases\nof supervised learning\nso we have Donna codon\nor any other speech\nAutomation in your mobile\nphone trains using your voice\nand one strain it start working\nbased on the training.\nThis is an application\nof supervised learning suppose.\nYou are telling\nOK Google call Sam\nor you say Hey Siri call\nSam you get an answer to it\nand action is performed\nand automatically\na call goes to Sam.\nSo these are just an example\nof supervised learning next\ncomes the weather up\nbased on some\nof the prior knowledge\nlike when it is sunny\nthe temperature is high.\nFire when it is cloudy humidity\nis higher any kind of that they\npredict the parameters\nfor a given time.\nSo this is also an example\nof supervised learning\nas we are feeding the data\nto the machine and telling\nthat whenever it is sunny.\nThe temperature should be higher\nwhenever it is cloudy.\nThe humidity should be higher.\nSo it's an example\nof supervised learning.\nAnother example is\nbiometric attendance\nwhere you train the machine\nand after couple of inputs\nof your biometric identity\nbeat your thumb your iris\nor yellow or anything\nonce trained Machine gun\nvalidate your future input\nand can identify you next comes\nin the field of banking sector\nin banking sector\nsupervised learning is used\nto predict the credit worthiness\nof a credit card holder\nby building a machine\nlearning model to look\nfor faulty attributes\nby providing it\nwith a data on deliquent\nand non-delinquent customers.\nNext comes the healthcare sector\nin the healthcare sector.\nIt is used to predict\nthe patient's readmission rates\nby building a regression model\nby providing data\non the patients treatment\nAdministration and readmissions\nto show variables\nthat best correlate\nwith readmission.\nNext comes the retail sector\nand Retail sector.\nIt is used to\nanalyze the product\nthat a customer by together.\nIt does this by building\na supervised model\nto identify frequent itemsets\nand Association rule\nfrom the transactional data now,\nlets learn about\nthe next category\nof machine learning the\nunsupervised part mathematically\nunsupervised learning is\nwhere you only\nhave Put data X and no\ncorresponding output variable.\nThe goal for unsupervised\nlearning is to model\nthe underlying structure\nor distribution in the data\nin order to learn\nmore about the data.\nSo let me rephrase you\nthis in simple terms\nin unsupervised learning\napproach the data instances\nof a training data\nset do not have\nan expected output Associated\nto them instead unsupervised\nlearning algorithm\ndetects pattern based\non innate characteristics\nof the input data an example\nof machine learning tasks.\nAsk that applies unsupervised\nlearning is clustering\nin this task similar data\ninstances are grouped together\nin order to identify clusters\nof data in this slide.\nYou can see that initially\nwe have different varieties\nof fruits as input.\nNow these set of fruits as\ninput X are given to the model.\nNow, what is the model\nis trained using\nunsupervised learning algorithm.\nThe model will create clusters\non the basis of its training.\nIt will grip the similar fruits\nand make their cluster.\nLet me make things\nmore clearer to you.\nLet's take another\nexample of it.\nSo in this Slide the image\nbelow shows an example\nof unsupervised learning process\nthis algorithm processes\nan unlabeled training data set\nand based on\nthe characteristics.\nIt grips the picture\ninto three different clusters\nof data despite the ability\nof grouping similar\ndata into clusters.\nThe algorithm is not capable\nto add labels to the crow.\nThe algorithm only knows which\ndata instances are similar,\nbut it cannot identify\nthe meaning of this group.\nSo, Now you might be wondering\nwhy this category\nof machine learning is named\nas unsupervised learning.\nSo these are called as\nunsupervised learning because\nunlike supervised learning ever.\nThere are no correct answer\nand there is no teacher\nalgorithms are left\non their own to discover\nand present the interesting\nstructure in the data.\nLet's move on and see some\nof the popular unsupervised\nlearning algorithm.\nSo we have here\nk-means apriori algorithm\nand hierarchical clustering now,\nlet's move on and see\nsome of the examples\nof Is learning suppose a friend\ninvites you to his party\nand where you meet\ntotally strangers.\nNow, you will classify them\nusing unsupervised learning\nas you don't have\nany prior knowledge about them\nand this classification\ncan be done on the basis\nof gender age group\ndressing education qualification\nor whatever way you\nmight like now why\nthis learning is different\nfrom supervised learning\nsince you didn't use\nany pasta prior knowledge\nabout the people you kept on\nclassifying them on the go\nas they kept on coming you\nkept on classifying them.\nYeah, this category\nof people belong to this group\nthis category of people belong\nto that group and so on.\nOkay, let's see\none more example.\nLet's suppose you have never\nseen a football match before\nand by chance you watch\na video on the internet.\nNow, you can easily classify\nthe players on the basis\nof different Criterion,\nlike player wearing\nthe same kind of Jersey are\nin one class player\nwearing different kind\nof Jersey aren't different class\nor you can classify\nthem on the basis\nof their playing style\nlike the guys are attacker.\nSo he's in one class.\nHe's a Defender\nhe's Another class\nor you can classify them.\nWhatever Way You\nobserve the things\nso this was also an example\nof unsupervised learning.\nLet's move on and see\nhow unsupervised learning\nis used in the sectors\nof banking Healthcare undertale.\nSo starting at banking sector.\nSo in banking sector it\nis used to segment customers\nby behavioral characteristic\nby surveying prospects\nand customers to develop\nmultiple segments\nusing clustering and\nHealthcare sector.\nIt is used to categorize the MRI\ndata by normal or abnormal.\nAges it uses deep learning\ntechniques to build a model\nthat learns from different\nfeatures of images to recognize\na different pattern.\nNext is the retail sector\nand Retail sector.\nIt is used to recommend\nthe products to customer\nbased on their past purchases.\nIt does this by building\na collaborative filtering model\nbased on the past\npurchases by them.\nI assume you guys\nnow have a proper idea of\nwhat unsupervised learning means\nif you have any slightest doubt\ndon't hesitate and add your\ndoubt to the I'm in section.\nSo let's discuss the third\nand the last type\nof machine learning\nthat is reinforcement learning.\nSo what is\nreinforcement learning?\nWell reinforcement learning\nis a type of machine\nlearning algorithm\nwhich allows software agents\nand machine to automatically\ndetermine the ideal Behavior\nwithin a specific context\nto maximize its performance.\nThe reinforcement learning\nis about interaction\nbetween two elements\nthe environment and\nthe learning agent\nthe learning agent leverages\nto mechanism namely exploration.\nAnd exploitation when\nlearning agent acts on trial\nand error basis,\nit is termed as exploration\nand when it acts based\non the knowledge gained\nfrom the environment,\nit is referred\nto as exploitation.\nNow this environment rewards\nthe agent for correct actions,\nwhich is reinforcement signal\nleveraging the rewards\nobtain the agent\nimproves its environment\nknowledge to select\nthe next action in this image.\nYou can see\nthat the machine is confused\nwhether it is an apple\nor it's not an apple\nthen the Sheena's chain\nusing reinforcement learning.\nIf it makes correct decision.\nIt get rewards point for it\nand in case of wrong it gets\na penalty for that.\nOnce the training is done.\nNow.\nThe machine can easily identify\nwhich one of them is an apple.\nLet's see an example here.\nWe can see that we have an agent\nwho has to judge\nfrom the environment to find out\nwhich of the two is\na duck the first task\nhe did is to observe\nthe environment next.\nWe select some action\nusing some policy.\nIt seems that the machine\nhas made a wrong decision.\nBye.\nChoosing a bunny as a duck.\nSo the machine will\nget penalty for it.\nFor example -\n50.4 a wrong answer right now.\nThe machine will\nupdate its policy\nand this will continue\ntill the machine gets\nan optimal policy\nfrom the next time machine will\nknow that bunny is not a duck.\nLet's see some of the use cases\nof reinforcement learning\nbut before that lets see\nhow Pavlo trained his dog\nusing reinforcement learning\nor how he applied\nthe reinforcement method\nto train his dog.\nBabu integrated learning\nin four stages initially\nPavlo gave me to his dog\nand in response to the meet\nthe dog started salivating next\nwhat he did he created a sound\nwith the bell for this the dog\ndid not respond anything\nin the third part it\ntried to condition the dog\nby using the bell\nand then giving him\nthe food seeing the food\nthe dog started salivating\neventually a situation came\nwhen the dog started salivating\njust after hearing the Bell even\nif the food was not given to him\nas the The dog was reinforced\nthat whenever the master\nwill ring the bell he\nwill get the food now.\nLet's move on and see\nhow reinforcement learning\nis applied in the field\nof banking Healthcare\nand Retail sector.\nSo starting with\nthe banking sector\nin banking sector reinforcement\nlearning is used to create\na next best offer model\nfor a call center\nby building a predictive model\nthat learns over time\nas user accept or reject offer\nmade by the sales staff fine now\nin healthcare sector it\nis used to allocate the scars.\nResources to handle\ndifferent type of er\ncases by building\na Markov decision process\nthat learns treatment strategies\nfor each type of er case next\nand the last comes\nin retail sector.\nSo let's see\nhow reinforcement learning\nis applied to retail sector\nand Retail sector.\nIt can be used to\nreduce excess stock\nwith Dynamic pricing by building\na dynamic pricing model\nthat are just the price based\non customer response\nto the offers.\nI hope by now you have attained\nsome understanding of\nwhat is machine learning\nand you are ready to move.\nMove ahead.\nWelcome to today's topic\nof discussion on AI\nversus machine learning\nversus deep learning.\nThese are the term\nwhich have confused a lot\nof people and if you\ntwo are one among them,\nlet me resolve it for you.\nWell artificial intelligence\nis a broader umbrella\nunder which machine learning\nand deep learning come you\ncan also see in the diagram\nthat even deep learning\nis a subset of machine\nlearning so you can say\nthat all three of them The AI\nand machine learning\nand deep learning are just\nthe subset of each other.\nSo let's move on and understand\nhow exactly the differ\nfrom each other.\nSo let's start\nwith artificial intelligence.\nThe term artificial intelligence\nwas first coined\nin the year 1956.\nThe concept is pretty old,\nbut it has gained\nits popularity recently.\nBut why well,\nthe reason is earlier we had\nvery small amount of data\nthe data we had was not enough\nto predict the Turret result\nbut now there's\na tremendous increase\nin the amount of data statistics\nsuggest that by 2020\nthe accumulated volume\nof data will increase\nfrom 4.4 zettabyte stew\nroughly around 44 zettabytes\nor 44 trillion jeebies\nof data along with such\nenormous amount of data.\nNow, we have more\nadvanced algorithm\nand high-end computing\npower and storage\nthat can deal with such large\namount of data as a result.\nIt is expected\nthat 70% of The price\nwill Implement a i\nover the next 12 months\nwhich is up from 40 percent\nin 2016 and 51 percent in 2017.\nJust for your understanding.\nWhat does AI well,\nit's nothing but a technique\nthat enables the machine\nto act like humans\nby replicating the behavior\nand nature with AI\nit is possible\nfor machine to learn\nfrom the experience.\nThe machines are just\ntheir responses based\non new input there\nby performing human-like tasks\nartificial intelligence can be\nand to accomplish\nspecific tasks by processing\nlarge amount of data\nand recognizing pattern in them.\nYou can consider\nthat building an artificial\nintelligence is like Building\na Church the first church\ntook generations to finish.\nSo most of the workers\nwere working in it never saw\nthe final outcome those working\non it took pride\nin their craft building bricks\nand chiseling stone\nthat was going to be placed\ninto the great structure.\nSo as AI researchers,\nwe should think of ourselves\nas humble brick makers was job.\nIt's just study\nhow to build components\nexample Parts is planners\nor learning algorithm\nor Etc anything\nthat someday someone\nand somewhere will integrate\ninto the intelligent systems\nsome of the examples\nof artificial intelligence\nfrom our day-to-day life\nare Apple series chess-playing\ncomputer Tesla self-driving car\nand many more these examples\nare based on deep learning\nand natural language processing.\nWell, this was about what is AI\nand how it gains its hype.\nSo moving on ahead.\nLet's Gus about machine\nlearning and see what it is\nand why it was the\nwhen introduced well\nMachine learning came\ninto existence in the late 80s\nand the early 90s,\nbut what were the issues\nwith the people\nwhich made the machine learning\ncome into existence let\nus discuss them one by one\nin the field of Statistics.\nThe problem was\nhow to efficiently train\nlarge complex model in the field\nof computer science\nand artificial intelligence.\nThe problem was\nhow to train more robust version\nof AI system while in\nthe case of Neuroscience.\nProblem faced by\nthe researchers was\nhow to design operation\nmodel of the brain.\nSo these were some of the issues\nwhich had the largest influence\nand led to the existence\nof the machine learning.\nNow this machine learning\nshifted its focus\nfrom the symbolic approaches.\nIt had inherited\nfrom the AI and move\ntowards the methods and model.\nIt had borrowed from statistics\nand probability Theory.\nSo let's proceed and see\nwhat exactly is\nmachine learning.\nWell Machine learning\nis a subset of AI\nwhich enables the\ncomputer to act\nand make data-driven decisions\nto carry out a certain task.\nThese programs are algorithms\nare designed in a way\nthat they can learn\nand improve over time\nwhen exposed to new data.\nLet's see an example\nof machine learning.\nLet's say you want\nto create a system\nwhich tells the expected weight\nof a person based on its side.\nThe first thing you do\nis you collect the data.\nLet's see there is\nhow your data looks\nlike now each point\non the graph represent\none data point to start\nwith we can draw a simple line\nto predict the weight based\non the height for Sample\na simple line W equal x\nminus hundred with W\nis waiting kgs and edges hide\nand centimeter this line can\nhelp us to make the prediction.\nOur main goal is\nto reduce the difference\nbetween the estimated value\nand the actual value.\nSo in order to achieve it,\nwe try to draw a straight line\nthat fits through all\nthese different points\nand minimize the error.\nSo our main goal is\nto minimize the error\nand make them as small as\npossible decreasing the error\nor the difference between\nthe actual value and estimated.\nValue increases the performance\nof the model further\non the more data points.\nWe collect the better.\nOur model will become we\ncan also improve our model\nby adding more variables\nand creating different\nproduction lines for them.\nOnce the line is created.\nSo from the next time\nif we feed a new data,\nfor example height\nof a person to the model,\nit would easily predict the data\nfor you and it will tell you\nwhat has predicted\nweight could be.\nI hope you got\na clear understanding\nof machine learning.\nSo moving on ahead.\nLet's learn about deep learning\nnow what is deep learning?\nYou can consider deep learning\nmodel as a rocket engine\nand its fuel is\nits huge amount of data\nthat we feed to\nthese algorithms the concept\nof deep learning is not new,\nbut recently it's\nhype as increase\nand deep learning\nis getting more attention.\nThis field is a particular kind\nof machine learning\nthat is inspired by\nthe functionality of\nour brain cells called neurons\nwhich led to the concept\nof artificial neural network.\nIt simply takes\nthe data connection between all\nthe artificial neurons\nand adjust them according\nto the data pattern.\nMore neurons are added\nat the size of the data is large\nit automatically features\nlearning at multiple\nlevels of abstraction.\nThereby allowing a system\nto learn complex function\nmapping without depending\non any specific algorithm.\nYou know, what no one\nactually knows what happens\ninside a neural network\nand why it works so well,\nso currently you can call\nit as a black box.\nLet us discuss some\nof the example of deep learning\nand understand it\nin a better way.\nLet me start with\nin simple example\nand explain you how things And\nat a conceptual level,\nlet us try and understand\nhow you would recognize\na square from other shapes.\nThe first thing\nyou do is you check\nwhether there are four lines\nassociated with a figure\nor not simple concept, right?\nIf yes, we further check\nif they are connected\nand closed again a few years.\nWe finally check\nwhether it is perpendicular\nand all its sides\nare equal, correct.\nIf everything fulfills.\nYes, it is a square.\nWell, it is nothing but\na nested hierarchy of Concepts.\nWhat we did here we\ntook a complex task\nof identifying a square\nand this case and broken\ninto simpler tasks.\nNow this deep learning\nalso does the same thing\nbut at a larger scale,\nlet's take an example\nof machine which recognizes\nthe animal the task\nof the machine is to recognize\nwhether the given image is\nof a cat or a dog.\nWhat if we were asked to resolve\nthe same issue using the concept\nof machine learning\nwhat we would do first.\nWe would Define\nthe features such as\ncheck whether the animal has\nwhiskers or not a check.\nThe animal has pointed ears\nor not or whether its tail\nis straight or curved in short.\nWe will Define\nthe facial features and let\nthe system identify which\nfeatures are more important\nin classifying a\nparticular animal now\nwhen it comes to deep learning\nit takes this to one step ahead\ndeep learning automatically\nfinds are the feature\nwhich are most important\nfor classification compare\ninto machine learning\nwhere we had to manually give\nout that features by now.\nI guess you have understood\nthat AI is the bigger picture\nand machine learning and\ndeep learning are it's apart.\nSo let's move on\nand focus our discussion\non machine learning\nand deep learning the easiest\nway to understand the difference\nbetween the machine learning\nand deep learning is to know\nthat deep learning is machine\nlearning more specifically.\nIt is the next evolution\nof machine learning.\nLet's take few\nimportant parameter\nand compare machine learning\nwith deep learning.\nSo starting with\ndata dependencies,\nthe most important difference\nbetween deep learning\nand machine learning is\nits performance as the volume\nof the data gets\nFrom the below graph.\nYou can see\nthat when the size of the data\nis small deep learning algorithm\ndoesn't perform that well,\nbut why well,\nthis is because deep\nlearning algorithm needs\na large amount of data\nto understand it perfectly\non the other hand the machine\nlearning algorithm can easily\nwork with smaller data set fine.\nNext comes the hardware\ndependencies deep learning\nalgorithms are heavily dependent\non high-end machines\nwhile the machine learning\nalgorithm can work\non low and machines as Well,\nthis is because the requirement\nof deep learning\nalgorithm include gpus\nwhich is an integral part\nof its working the Deep learning\nalgorithm requires gpus\nas they do a large\namount of matrix\nmultiplication operations,\nand these operations\ncan only be efficiently\noptimized using a GPU\nas it is built for this purpose.\nOnly our third parameter\nwill be feature engineering well\nfeature engineering is a process\nof putting the domain knowledge\nto reduce the complexity\nof the data.\nMake patterns more visible\nto learning algorithms.\nThis process is difficult\nand expensive in terms of time\nand expertise in case\nof machine learning\nmost other features are needed\nto be identified by an expert\nand then hand coded\nas per the domain\nand the data type.\nFor example, the features\ncan be a pixel value shapes\ntexture position orientation\nor anything fine the performance\nof most of the machine\nlearning algorithm depends\non how accurately the features\nare identified and stood\nwhere as in case\nof deep learning algorithms it\ntry to learn high level features\nfrom the data.\nThis is a very distinctive part\nof deep learning\nwhich makes it way ahead\nof traditional machine learning\ndeep learning reduces the task\nof developing new feature\nextractor for every problem\nlike in the case\nof CNN algorithm it first try\nto learn the low-level features\nof the image such as\nedges and lines\nand then it proceeds\nto the parts of faces of people\nand then finally to\nthe high-level representation\nof the face.\nI hope that things\nGetting clearer to you.\nSo let's move on ahead and see\nthe next parameter.\nSo our next parameter is\nproblem solving approach\nwhen we are solving a problem\nusing traditional machine\nlearning algorithm.\nIt is generally recommended\nthat we first break\ndown the problem\ninto different sub parts\nsolve them individually\nand then finally combine them\nto get the desired result.\nThis is how the machine learning\nalgorithm handles the problem\non the other hand\nthe Deep learning algorithm\nsolves the problem\nfrom end to end.\nLet's take an example.\nTo understand this\nsuppose you have a task\nof multiple object detection.\nAnd your task is to identify.\nWhat is the object and where it\nis present in the image.\nSo, let's see and compare.\nHow will you tackle\nthis issue using the concept\nof machine learning\nand deep learning starting\nwith machine learning\nin a typical machine\nlearning approach.\nYou would first divide\nthe problem into two step\nfirst object detection\nand then object recognization.\nFirst of all,\nyou would use a bounding\nbox detection algorithm\nlike grab could fight.\nSample to scan through the image\nand find out all\nthe possible objects.\nNow, once the objects\nare recognized you would use\nobject recognization algorithm,\nlike svm with hog\nto recognize relevant objects.\nNow, finally,\nwhen you combine the result you\nwould be able to identify.\nWhat is the object\nand where it is present\nin the image on the other hand\nin deep learning approach.\nYou would do the process\nfrom end to end for example\nin a yellow net\nwhich is a type of deep learning\nalgorithm you would pass.\nAn image and it would give out\nthe location along with the name\nof the object.\nNow, let's move on to\nour fifth comparison parameter\nits execution time.\nUsually a deep learning\nalgorithm takes a long time\nto train this is\nbecause there's so\nmany parameter in\na deep learning algorithm\nthat makes the training longer\nthan usual the training\nmight even last for two weeks\nor more than that.\nIf you are training\ncompletely from the scratch,\nwhereas in the case\nof machine learning,\nit relatively takes much\nless time to train ranging\nfrom a few weeks.\nToo few Arts.\nNow.\nThe execution time\nis completely reversed\nwhen it comes to the testing\nof data during testing\nthe Deep learning algorithm\ntakes much less time to run.\nWhereas if you compare it\nwith a KNN algorithm,\nwhich is a type of machine\nlearning algorithm the test\ntime increases as the size\nof the data increase last\nbut not the least we\nhave interpretability as\na factor for comparison\nof machine learning\nand deep learning.\nThis fact is the main reason\nwhy deep learning is still\nthought ten times\nbefore anyone knew.\nUses it in the industry.\nLet's take an example suppose.\nWe use deep learning to give\nautomated scoring two essays\nthe performance it gives\nand scoring is quite\nexcellent and is near\nto the human performance,\nbut there's an issue with it.\nIt does not reveal white\nhas given that score\nindeed mathematically.\nIt is possible to find out\nthat which node of a deep\nneural network were activated,\nbut we don't know\nwhat the neurons\nare supposed to model\nand what these layers of neurons\nare doing collectively.\nSo if To interpret the result\non the other hand machine\nlearning algorithm,\nlike decision tree gives us\na crisp rule for void chose\nand watered chose.\nSo it is particularly easy\nto interpret the reasoning\nbehind therefore the algorithms\nlike decision tree\nand linear or logistic\nregression are primarily used in\nindustry for interpretability.\nLet me summarize things\nfor you machine learning\nuses algorithm to parse\nthe data learn from the data\nand make informed decision based\non what it has learned fine.\nin this deep learning structures\nalgorithms in layers to create\nartificial neural network\nthat can learn\nand make Intelligent Decisions\non their own finally\ndeep learning is a subfield\nof machine learning\nwhile both fall\nunder the broad category\nof artificial intelligence\ndeep learning is usually\nwhat's behind the most\nhuman-like artificial\nintelligence now\nin early days scientists\nused to have a lab notebook\nto Test progress results\nand conclusions now\nJupiter is a modern-day\nto that allows data scientists\nto record the complete\nanalysis process much\nin the same way other scientists\nuse a lab notebook.\nNow, the Jupiter product was\noriginally developed as a part\nof IPython project the iPad\nand project was used to provide\ninteractive online access\nto python over time.\nIt became useful to interact\nwith other data analysis tools\nsuch as are in the same manner\nwith the split from python\nthe tool crew in in his current\nmanifestation of Jupiter.\nNow IPython is\nstill an active tool\nthat's available for use.\nThe name Jupiter itself is\nderived from the combination\nof Julia Python.\nAnd our while Jupiter runs code\nin many programming languages\npython is a requirement\nfor installing the jupyter\nnotebook itself now\nto download jupyter notebook.\nThere are a few ways\nin their official website.\nIt is strongly recommended\ninstalling Python and Jupiter\nusing Anaconda distribution,\nwhich includes python\nDon't know what book\nand other commonly used packages\nfor scientific Computing\nas well as data science.\nAlthough one can also\ndo so using the pipe\ninstallation method personally.\nWhat I would suggest\nis downloading an app\non a navigator, which is\na desktop graphical user\ninterface included in Anaconda.\nNow, this allows you\nto launch application\nand easily manage\nconda packages environments\nand channels without the need\nto use command line commands.\nSo all you need to do is go\nto another Corner dot orgy\nand inside you go.\nTo Anaconda Navigators.\nSo as you can see here,\nwe have the conda installation\ncode which you're going\nto use to install it\nin your particular PC.\nSo either you can\nuse these installers.\nSo once you download\nthe Anaconda Navigator,\nit looks something like this.\nSo as you can see here,\nwe have Jupiter lab jupyter\nnotebook you have QT console,\nwhich is IPython console.\nWe have spider which is\nsomewhat similar to a studio\nin terms of python again,\nwe have a studio\nso we have orange three\nWe have glue is\nand we have VSC code.\nOur Focus today would be\non this jupyter notebook itself.\nNow when you\nlaunch the Navigator,\nyou can see there are\nmany options available\nfor launching python as well.\nAs our instances Now\nby definition are jupyter.\nNotebook is fundamentally\na Json file with\na number of annotations.\nNow, it has three main parts\nwhich are the metadata\nThe Notebook format and the list\nof cells now you\nshould get yourself acquainted\nwith the environment\nthat Jupiter user interface\nhas a number of components.\nSo it's important to know\nwhat our components\nyou should be using\non a daily basis and you\nshould get acquainted with it.\nSo as you can see here\nour Focus today will be\non the jupyter notebook.\nSo let me just launched\nthe Japan and notebook.\nNow what it does is creates\na online python instance\nfor you to use it over the web.\nSo let's launch now\nas you can see we have\nJupiter on the top left\nas expected and this acts\nas a button to go\nto your home page\nwhenever you click\non this you get back\nto your particular home paste.\nIs the dashboard now there are\nthree tabs displayed\nwith other files\nrunning and clusters.\nNow, what will do is\nwill understand all\nof these three and understand\nwhat are the importance\nof these three tabs\nother file tab shows the list\nof the current files\nin the directory.\nSo as you can see we have\nso many files here.\nNow the running tab\npresents another screen of\nthe currently running processes\nand the notebooks now the\ndrop-down list for the terminals\nand notebooks are\npopulated with there.\nRunning numbers.\nSo as you can see inside,\nwe do not have\nany running terminals\nor there no running\nnotebooks as of now\nand the cluster tab\npresents another screen\nto display the list\nof clusters available see in the\ntop right corner of the screen.\nThere are three buttons\nwhich are upload new\nand the refresh button.\nLet me go back\nso you can see here.\nWe have the upload new\nand the refresh button.\nNow the upload button\nis used to add files\nto The Notebook space and you\nmay also just drag and drop\nas you would\nwhen handling files.\nSimilarly, you can drag\nand drop notebooks\ninto specific folders as well.\nNow the menu with the new\nin the top residents\nof further many\nof text file folders terminal\nand Python 3.\nNow, the test file option\nis used to add a text file\nto the current directory Jupiter\nwill open a new browser window\nfor you for the running\nnew text editor.\nNow, the text entered\nis automatically saved\nand will be displayed\nin your notebooks files display.\nNow the folder option\nwhat it does is\ncreates a new folder.\nWith the name Untitled folder\nand remember all the files\nand folder names are editable.\nNow the terminal option\nallows you to start\nand IPython session.\nThe node would options\navailable will be activated\nwhen additional note books are\navailable in your environment.\nThe Python 3 option is used\nto begin pythons recession\ninteractively in your note.\nThe interface looks\nlike the following screen shot.\nNow what you have is\nfull file editing capabilities\nfor your script\nincluding saving as new file.\nYou also have a complete ID\nfor your python script now we\ncome to the refresh button.\nThe refresh button is used\nto update the display.\nIt's not really necessary\nas a display is reactive\nto any changes in\nthe underlying file structure.\nI had a talk with\nthe files tab item.\nThere is a check box drop\ndown menu and a home button\nas you can see here.\nWe have the checkbox\nthe drop-down menu\nand the home button.\nNow the check box is used\nto toggle all the checkboxes\nin the item list.\nSo as you can see you can select\nall of these when either move\nor either delete all\nof the file selected,\nIt or what you\ncan do is select all\nand deselect some of the files\nas your wish now the drop\ndown menu presents a list\nof choices available,\nwhich are the folders\nall notebooks running\nand files to the folder section\nwill select all the folders\nin the display\nand present account\nof the folders in the small box.\nSo as you can see here,\nwe have 18 number of folders\nnow all the notebooks section\nwill change the count\nof the number of nodes\nand provide you\nwith three option\nso you can see here.\nIt has selected all\nthe given notebooks\nwhich are a In a number\nand you get the option to either\nduplicate the current notebook.\nYou need to move it view\nit edit it or delete.\nNow, the writing section\nwill select any running scripts\nas you can see here.\nWe have zero running scripts\nand update the count\nto the number selected.\nNow the file section\nwill select all the files\nin the notebook display and\nupdate the counts accordingly.\nSo if you select the files here,\nwe are seven files\nas you can see here.\nWe have seven files\nsome datasets CSV files\nand text files now\nthe home button.\nBrings you back to the home\nscreen of the notebook.\nSo on you to do is click\non the jupyter.\nNotebook lower.\nIt will bring you back to\nthe Jupiter notebook dashboard.\nNow, as you can see\non the left hand side\nof every item is a checkbox\nand I can and the items name.\nThe checkbox is used to build\na set of files to operate\nupon and the icon is indicated\nof of the type of the item.\nAnd in this case,\nall of the items are\nfolder here coming down.\nWe have the ring notebooks.\nAnd finally we have certain\nfiles which are the text files\nand the As we files\nnow a typical workflow\nof any jupyter.\nNotebook is to first\nof all create a notebook\nfor the project\nor your data analysis.\nAdd your analysis step coding\nand output and Surround\nyour analysis with organization\nand presentation mark\ndown to communicate\nand entire story\nnow interactive notebooks\nthat include widgets\nand display modules\nwill then be used by others\nby modifying parameters\nand the data to note the effects\nof the changes now\nif we talk about security\njupyter notebooks are created\nin order to Be shared\nwith other users in many cases\nover the Internet.\nHowever, jupyter notebook\ncan execute arbitrary code\nand generate arbitrary code.\nThis can be a problem.\nIf malicious aspects\nhave been placed\nin the note Now the default\nsecurity mechanism for Japan\nor notebooks include raw HTML,\nwhich is always sanitized\nand check for malicious coding.\nAnother aspect is you cannot run\nexternal Java scripts.\nNow the cell contents,\nespecially the HTML and\nthe JavaScript are not trusted\nit requires user value.\nNation to continue\nand the output from any cell\nis not trusted all other HTML\nor JavaScript is never trusted\nand clearing the output\nwill cause the notebook\nto become trusted\nwhen save now notebooks\ncan also use a security digest\nto ensure the correct user\nis modifying the contents.\nSo for that what you\nneed to do is a digest\nwhat it does is takes\ninto the account\nthe entire contents\nof the notebook and a secret\nwhich is only known by\nThe Notebook Creator\nand this combination ensures\nthat malicious coding is\nis not going to be added\nto the notebook\nso you can add security\nto address to notebook\nusing the following command\nwhich I have given here.\nSo it's Jupiter the profile\nwhat you have selected\nand inside you\nwhat you need to do is security\nand notebook secret.\nSo what you can do is\nreplace the notebooks\nsecret with your putter secret\nand that will act as a key\nfor the particular notebook.\nSo what you need to do\nis share that particular key\nwith all your colleagues\nor whoever you want to share\nthat particular notebook\nwith and in that case,\nit keeps the notebooks.\nGeode and away from\nother malicious coders\nand all other aspect\nof Jupiter is configuration.\nSo you can configure some\nof the display parameters used\nand presenting notebooks.\nNow, these aren't configurable\ndue to the use of product known\nas code mirror to present\nand modify the notebook.\nSo cold mirror water basically\nis it is a JavaScript\nbased editor for the u.s.\nWithin the web pages\nand notebooks.\nSo what you do is\nwhat you do code mirror,\nso as you can see here\ncode mirror is a versatile\ntext editor implemented.\nIn JavaScript for the browser.\nSo what it does is\nallow you to configure\nthe options for Jupiter.\nSo now let's execute\nsome python code\nand understand the notebook\nin A Better Way Jupiter\ndoes not interact\nwith your scripts as\nmuch as it executes your script\nand request the result.\nSo I think this is\nhow jupyter notebooks\nhave been extended to other\nlanguages besides python\nas it just takes\na script runs it against\na particular language engine\nand across the output\nfrom the engine all\nthe while not Really\nknowing what kind\nof a script is being executed\nnow the new windows shows\nand empty cell\nfor you to enter\nthe python code know\nwhat you need to do is under new\nyou select the Python 3 and\nwhat I will do is open\na new notebook.\nNow this notebook is Untitled.\nSo let's give the new work area\nand name python code.\nSo as you can see we have\nrenamed this particular cell\nnow order save option should be\non the next to the title\nas you can see last.\nCheckpoint a few days\nago, unsaved changes.\nThe autosave option is\nalways on what we do is\nwith an accurate name.\nWe can find the selection\nand this particular\nnotebook very easily\nfrom The Notebook home page.\nSo if you select\nyour browser's Home tab\nand refresh you will find\nthis new window name\ndisplayed here again.\nSo if you just go\nto a notebook home\nand as you can see,\nI mentioned it by then quotes\nand under running.\nAlso, you have the pilot\nand quotes here.\nSo let's get back\nto the Particular page\nor the notebook\none thing to note here\nthat it has\nand does an item icon\nversus a folder icon\nthough automatically\nassigned extension\nas you can see here is ipy\nand be the IPython note and says\nthe item is in a browser\nin a Jupiter environment.\nIt is marked as running answer\nis a file by that name\nin this directory as well.\nSo if you go\nto your directory,\nlet me go and check it.\nSo as you can see\nif you go into the users are you\ncan see we have the\nin class projects\nthat Python codes\nlike the series automatically\nhave that particular\nIPython notebook created\nin our working environment\nand the local disk space also.\nSo if you open the IP y +\nB file in a text editor,\nyou will see basic context\nof a Jupiter code as you can see\nif I'm opening it.\nThe cells are empty.\nNothing is there so let's type\nin some code here.\nFor example, I'm going to put\nin name equals edgy Rekha.\nNext what I'm going to do\nis provide subscribers\nthat equals seven hundred gay\nand to run this particular cell.\nWhat you need to do\nis click on the run Icon\nand it will see\nhere we have one.\nSo this is the first set to be\nexecuted in the second cell.\nWe enter python code\nthat references the variables\nfrom the first cell.\nSo as you can see here,\nwe have friend named\nhas strings subscribers.\nSo let me just\nrun this particular.\nSo as you can see here note.\nNow that we have an output here\nthat Erica has 700k\nYouTube subscriber now\nsince more than 700 K now\nto know more about Jupiter\nand other Technologies,\nwhat you can do is subscribe\nto our Channel and get\nupdates on the latest\ntrending Technologies.\nSo note that Jupiter color codes\nyour python just as\ndecent editor vote\nand we have empty braces\nto the left of each code block\nsuch as you can see here.\nIf we execute the cell\nthe results are displayed\nin line now, it's interesting\nthat Jupiter keeps.\nThe output last generated\nin the saved version of the file\nand it's a save checkpoints.\nNow, if we were to rerun\nyour cells using the rerun\nor the run all the output\nwould be generated\nand c8y autosave now,\nthe cell number is incremented\nand as you can see\nif I rerun this you see\nthe cell number change\nfrom one to three\nand if I rerun this the Selma\nwill change from 2 to 4.\nSo what Jupiter does is keeps\na track of the latest version\nof each cell so similarly\nif you are to close\nthe browser tab It's the display\nin the Home tab.\nYou will find\na new item we created\nwhich is the python code\nyour notebook saved autosaved\nas you can see here\nin the bracket has autosaved.\nSo if we close this\nin the home button,\nyou can see here.\nWe have python codes.\nSo as you can see if we click\nthat it opens the same notebook.\nIt has the previously\ndisplayed items will be always\nthere showing the output sweat\nthat we generated\nin the last run now\nthat we have seen\nhow python Works\nin Jupiter including\nthe underlying encoding then\nhow this python.\nThis allows data set\nor data set Works in Jupiter.\nSo let me create\nanother new python notebook.\nSo what I'm going to do\nis name this as pandas.\nSo from here,\nwhat we will do is read\nin last dataset\nand compute some standard\nstatistics of data.\nNow what we are interested\nin in seeing\nhow to use the pandas in Jupiter\nhow well the script performs\nand what information\nis stored in the metadata,\nespecially if it's\na large dataset\nso our Python script accesses\nthe iris dataset here\nthat's built into one\nof the Python packages.\nNow.\nAll we are looking in to do is\nto read in slightly large number\nof items and calculate\nsome basic operations\non the data set.\nSo first of all,\nwhat we need to do is\nfrom sklearn import\nthe data set so sklearn\nis scikit-learn and it is\nanother library of python.\nIt contains a lot of data sets\nfor machine learning\nand all the algorithms\nwhich are present\nfor machine learning\nand the data sets\nwhich are there so,\nSo import was successful.\nSo what we're going to do\nnow is pull in the IRS data.\nWhat we're going to do is Iris\nunderscore data set equals\nand the load on the screen now\nthat should do and I'm sorry,\nit's data set start lower.\nSo so as you can see here,\nthe number here\nis considered three now\nbecause in the second drawer\nand we encountered\nan error it was data set.\nHe's not data set.\nSo so what we're going\nto do is grab the first\ntwo corner of the data.\nSo let's pretend x equals.\nIf you press the tab,\nit automatically detects\nwhat you're going to write\nas Todd datasets dot data.\nAnd what we're going to do is\ntake the first two rows comma\nnot to run it\nfrom your keyboard.\nAll you need to do is\npress shift + enter.\nSo next what we're going\nto do is calculate\nsome basic statistics.\nSo what we're going\nto do is X underscore.\nCount equals x I'm going to use\nthe length function and said\nthat we're going to use x\ndot flat similarly.\nWe going to see X-Men\nand X Max and the Min\nour display our results.\nWhat we're going to do is you\njust play the results now, so\nas you can see the counter 300\nthe minimum value is 3.8 m/s.\nAnd what is 0.4\nand the mean is five point\neight four three three three.\nSo let me connect you\nto the real life\nand tell you what\nall are the things\nwhich you can easily do using\nthe concepts of machine learning\nso you can easily get\nanswer to the questions\nlike which types of house\nlies in this segment\nor what is the market value\nof this house or is this\na male as spam or not spam?\nIs there any fraud?\nWell, these are some\nof the question you could ask\nto the machine\nbut for getting an answer\nto these you need some algorithm\nthe machine need to train\non the basis of some algorithm.\nOkay, but how will you\ndecide which algorithm\nto choose and when?\nOkay.\nSo the best option for us is\nto explore them one by one.\nSo the first is\nclassification algorithm\nwhere the categories\npredicted using the data\nif you have some question,\nlike is this person a male\nor a female or is\nthis male a Spam or not?\nSpam then these category\nof question would fall\nunder the classification\nalgorithm classification is\na supervised learning approach\nin which the computer program\nlearns from the input\ngiven to it\nand then uses\nthis learning to classify\nnew observation some examples\nof classification problems\nare speech organization\nhandwriting recognized.\nShouldn't biometric\nidentification document\nclassification Etc.\nSo next is the anomaly\ndetection algorithm\nwhere you identify\nthe unusual data point.\nSo what is an anomaly detection.\nWell, it's a technique\nthat is used to\nidentify unusual pattern\nthat does not conform\nto expected Behavior\nor you can say the outliers.\nIt has many application\nin business like\nintrusion detection,\nlike identifying strange\npatterns in the network traffic\nthat could signal a hack\nor system Health monitoring\nthat is sporting a deadly tumor\nin the MRI scan\nor you can even use it\nfor fraud detection\ncredit card transaction\nor to deal with fault detection\nin operating environment.\nSo next comes\nthe clustering algorithm,\nyou can use this clustering\nalgorithm to group the data\nbased on some similar condition.\nNow you can get answer\nto which type of houses lies\nin this segment or what type\nof customer buys this product.\nThe clustering is a task\nof dividing the population\nor data points into\nnumber of groups such\nthat the data point\nand the same groups are more.\nHello to other data points\nin the same group than those\nin the other groups\nin simple words.\nThe aim is to segregate\ngroups with similar trait\nand assigning them into cluster.\nNow this clustering is a task\nof dividing the population\nor data points into\na number of groups such\nthat the data points\nin the X group is more similar\nto the other data points\nin the same group rather than\nthose in the other group.\nIn other words.\nThe aim is to segregate\nthe groups with similar traits\nand assigning them\ninto different clusters.\nLet's understand this\nwith an example Suppose you are\nthe head of a rental store\nand you wish to understand\nthe preference of your customer\nto scale up your business.\nSo is it possible\nfor you to look at the detail\nof each customer and design\na unique business strategy\nfor each of them?\nDefinitely not right?\nBut what you can do is to\nCluster all your customer saying\nto 10 different groups based\non their purchasing habit\nand you can use\na separate strategy\nfor customers in each\nof these ten different groups.\nAnd this is\nwhat we call clustering.\nNext we have regression\nalgorithm where the data\nitself is predicted question.\nYou may ask to this type\nof model is like what is\nthe market value of this house\nor is it going to rain\ntomorrow or not?\nSo regression is one of the most\nimportant and broadly\nused machine learning\nand statistics tool.\nIt allows you to make prediction\nfrom data by learning\nthe relationship between\nthe features of your data\nand some observe continuous\nvalued response regulation\nis used in a massive\nnumber of application.\nYou know, what stock Isis\nprediction can be done\nusing regression now,\nyou know about different\nmachine learning algorithm.\nHow will you decide\nwhich algorithm to choose\nand when so let's cover\nthis part using a demo.\nSo in this demo part\nwhat we will do will create six\ndifferent machine learning model\nand pick the best model\nand build the confidence such\nthat it has the most\nreliable accuracy.\nSo far our demo part\nwill be using the IRS data set.\nThis data set is\nquite very famous\nand is considered one of\nthe best small project to start\nwith you can consider\nthis as a hello world data set\nfor machine learning.\nSo this data set consists\nof 150 observation\nof Iris flower.\nTherefore Columns of measurement\nof flowers in centimeters\nthe fifth column\nbeing the species\nof the flower observe all\nthe observed flowers belong\nto one of the three species\nof Iris setosa Iris virginica\nand Iris versicolor.\nWell, this is\na good good project\nbecause it is so\nwell to understand\nthe attributes are numeric.\nSo you have to figure out\nhow to load and handle the data.\nIt is a classification problem.\nThereby allowing you to practice\nwith perhaps an easier type of\nsupervised learning algorithm.\nIt has only four\nattributes and 150 rose.\nMeaning it is very small\nand can easily fit\ninto the memory and even all\nof the numeric attributes\nare in same unit\nand the same scale means you do\nnot require any special scaling\nor transformation\nto get started.\nSo let's start coding and\nas I told earlier for the\nBut I'll be using Anaconda\nwith python 3.0 install on it.\nSo when you install Anaconda\nhow your Navigator\nwould look like.\nSo there's my home page of\nmy anaconda navigator on this.\nI'll be using\nthe jupyter notebook,\nwhich is a web-based interactive\nComputing notebook environment,\nwhich will help me to write and\nexecute my python codes on it.\nSo let's hit the launch\nbutton and execute\nour jupyter notebook.\nSo as you can see\nthat my jupyter notebook\nis starting on localhost\ndouble eight nine zero.\nOkay, so there's\nmy jupyter notebook\nwhat I'll do here.\nI'll select new.\nbook Python 3 Does\nmy environment where I can write\nand execute all\nmy python codes on it?\nSo let's start\nby checking the version\nof the libraries in order\nto make this video short\nand more interactive\nand more informative.\nI've already written\nthe set of code.\nSo let me just copy\nand paste it down.\nI'll explain you\nthen one by one.\nSo let's start\nby checking the version\nof the Python libraries.\nOkay, so there is\nthe code let's just copy\nit copied and let's paste it.\nOkay first let\nme summarize things for you\nwhat we are doing here.\nWe are just checking the version\nof the different\nlibraries starting\nwith python will first\ncheck what version\nof python we are working\non then we'll check\nwhat are the version\nof sci-fi we are using\nthe numpy matplotlib then\nPanda then scikit-learn.\nOkay.\nSo let's execute\nthe Run button and see\nwhat are the various\nversions of libraries\nwhich we are using it the run.\nSo we are working on Python 3\npoint 6 point 4 PSI by 1.0 now.\nBy 1.1 for matplotlib 2.12\npandas 0.22 and scikit-learn\nor version 0.19.\nOkay.\nSo these are the version\nwhich I'm using ideally your\nversion should be more recent\nor it should match\nbut don't worry\nif you lack\na few versions behind\nas the API is do not change\nso quickly everything\nin this tutorial will very\nlikely still work for you.\nOkay, but in case you\nare getting an error stop\nand try to fix that error\nin case you are unable to find\nthe solution for the error,\nfeel free to reach out at Eureka\neven after the This class.\nLet me tell you this\nif you are not able to run\nthe script properly,\nyou will not be able\nto complete this tutorial.\nOkay, so whenever you\nget a doubt reach out\nto a deal-breaker\nand just resolve it now,\neverything is working\nsmoothly then now is the time\nto load the data set.\nSo as I said,\nI'll be using the iris flower\ndata set for this tutorial\nbut before loading the data set,\nlet's import all the modules\nfunction and the object\nwhich we are going to use\nin this tutorial same\nI've already written\nthe set of code.\nSo let's just copy\nand paste them.\nLet's load all the libraries.\nSo these are\nthe various libraries\nwhich will be using\nin our tutorial.\nSo everything should work\nfine without an error.\nIf you get an error just\nstop you need to work\non your cyber environment\nbefore you continue any further.\nSo I guess everything\nshould work fine.\nLet's hit the Run\nbutton and see.\nOkay, it worked.\nSo let's now move ahead\nand load the data.\nWe can load the data direct\nfrom the UCI machine\nlearning repository.\nFirst of all,\nlet me tell you we are using\nPanda to load the data.\nOkay.\nSo let's say my URL.\nIs this so This is\nMy URL for the use\nyour machine learning repository\nfrom where I will be\ndownloading the data set.\nOkay.\nNow what I'll do,\nI'll specify the name\nof each column\nwhen loading the data.\nThis will help me later\nto explore the data.\nOkay, so I'll just copy\nand paste it down.\nOkay, so I'm defining\na variable names\nwhich consists of\nvarious parameters\nincluding sepal length sepal\nwidth petal length battle\nwith and class.\nSo these are just the name\nof column from the data set.\nOkay.\nNow let's define the data set.\nSo data set equals Panda\ndot read underscore CSV inside\nthat we are defining\nURL and the names\nthat is equal to name.\nAs I already said we'll be using\nPanda to load the data.\nAlright, so we are using\nPanda dot read CSV,\nso we are reading.\nThe CSV file and inside that\nfrom where that CSV is coming\nfrom the URL which you are.\nSo there's my URL.\nOkay name sequel names.\nIt's just specifying the names\nof the various columns\nin that particular CSV file.\nOkay.\nSo let's move forward\nand execute it.\nSo even our data set is loaded.\nIn case you have some network\nissues just go ahead\nand download the iris data file\ninto your working directory\nand loaded using the same method\nbut your make sure\nthat you change the url\nto the local name\nor else you might get an error.\nOkay.\nYeah, our data set is loaded.\nSo let's move ahead\nand check out data set.\nLet's see how many columns\nor rows we have in our data set.\nOkay.\nSo let's print the number\nof rows and columns\nin our data set.\nSo our data set is\ndata set dot shape\nwhat this will do.\nIt will just give you\nthe numbers of total number\nof rows and 2.\nLittle more of column\nor you can say the total number\nof instances are attributes\nin your data set fine.\nSo print data set dot shape\naudio getting 150 and 500.\nSo 150 is the total number\nof rows in your data set\nand five is the total number\nof columns fine.\nSo moving on ahead.\nWhat if I want to see\nthe sample data set?\nOkay.\nSo let me just print\nthe first certain instances\nof the data set.\nOkay, so print data set.\nHead.\nWhat I want is the first\n30 instances fine.\nThis will give me the first\n30 result of my data set.\nOkay.\nSo when I hit the Run button\nwhat I am getting is\nthe first 30 result,\nokay 0 to 29.\nSo this is\nhow my sample data set looks\nlike sepal length sepal\nwidth petal and petal width\nand the class, okay.\nSo this is how our data\nset looks like now,\nlet's move on\nand look at the summary\nof each attribute.\nWhat if I want to find out\nthe count mean the minimum\nand the maximum values and\nsome other percentiles as well.\nSo what should I do then\nfor that print data\nset dot described.\nWhat did we give let's see.\nSo you can see\nthat all the numbers are\nthe same scales of similar range\nbetween 0 to 8 centimeters,\nright the mean value\nthe standard deviation\nthe minimum value\nthe 25 percentile\n50 percentile 75 percentile\nthe maximum value all\nthese values lies\nin the range between\n0 to 8 centimeter.\nOkay.\nSo what we just did is\nwe just took a summary\nof each attribute.\nNow, let's look\nat the number of instances\nthat belong to each class.\nSo for that what we'll do\nprint data set.\nFirst of all,\nso let's print data set\nand I want to group it\nGroup by using class\nand I want the size\nof it size of each class fine,\nand let's hit the Run.\nOkay.\nSo what I want to do,\nI want to print\nprint out data set.\nHowever want to get it.\nI want it by class.\nSo Group by class.\nOkay.\nNow I want the size\nof each class find\nthe size of each class.\nSo Group by class dot size\nand skewed the run\nso you can see\nthat I have 50 instances\nof Iris setosa 50 instances\nof Iris versicolor\nand 50 instances\nof Iris virginica.\nOkay, all our of data type\ninteger of base64 fine.\nSo now we have a basic\nidea of Data, now,\nlet's move ahead and create\nsome visualization for it.\nSo for this we are going\nto create two different types\nof plot first would be\nthe univariate plot\nand the next would be\nthe multivariate plot.\nSo we'll be creating univariate\nplots to better understand\nabout each attribute\nand the next will be creating\nthe multivariate plot to better\nunderstand the relationship\nbetween different attributes.\nOkay.\nSo we start with\nsome univariate plot\nthat is plot\nof each individual variable.\nSo given that the input\nvariables are numeric\nwe can create box\nand whiskers plot for it.\nOkay.\nSo let's move ahead and create\na box and whiskers plot\nso data set Dot Plot.\nWhat kind I want it's a box.\nOkay, I'm do I need a subplot?\nYeah, I need subplots for that.\nSo subplots equal to what type\nof layout do I won't so\nmy layout structure is\n2 cross 2 next do I want\nto share my coordinates X\nand Y coordinates.\nNo, I don't want to share it.\nSo share x equal false\nand even share why\nthat 2 equals false?\nOkay.\nSo we have our data set\nDot Plot kind equal box.\nMy subplots is to lay out\nto Us too and then\nwhat I want to do it,\nI want to see so Plot show\nwhatever I created short.\nOkay, execute it.\nNot just gives us\na much clearer idea\nabout the distribution\nof the input attribute.\nNow what if I had given\nthe layout to 2 cross 2 instead\nof that I would have given\nit for cross for so\nwhat it will result\njust see fine.\nEverything would be printed\nin just one single row.\nHold on guys area is a doubt.\nHe's asking that why\nwe're using the sheriff's\nand share y values.\nWhat are these why we have\nassigned false values to it?\nOkay Ariel.\nSo in order to\nresolve this query,\nI need to show you\nwhat will happen\nif I give True Values to them.\nOkay, so be with me\nso share its go.\nPull through and share why\nthat equals true.\nSo let's see\nwhat result will get.\nYou're getting it the X\nand y-coordinates are just\nshared among all the\nfor visualization.\nRight?\nSo are you can see\nthat the sepal length\nand sepal width has\ny values ranging from zero point\nzero two seven point five\nwhich are being shared\namong both the visualization so\nis with the petal length.\nIt has shared value\nbetween zero point\nzero two seven point five.\nOkay, so that is why\nI don't want to share\nthe value of X and Y,\nso it's just giving us\na cluttered visualization.\nSo Aria why I'm doing this.\nI'm just doing it\ncause I don't want my X\nand Y coordinates To be shared\namong any visualization.\nOkay.\nThat is why my share X and share\nby value are false.\nOkay, let's execute it.\nSo this is a pretty\nmuch Clear visualization\nwhich gives a clear idea\nabout the distribution\nof the input attribute.\nNow if you want you\ncan also create a histogram\nof each input variable\nto get a clear idea\nof the distribution.\nSo let's create\na histogram for it.\nSo data set dot his okay.\nI would need to see it.\nSo plot dot show.\nLet's see.\nSo there's my histogram\nand it seems\nthat we have two input variables\nthat have a go.\nAnd distribution so\nthis is useful to note\nas we can use the algorithms\nthat can exploit\nthis assumption.\nOkay.\nSo next comes\nthe multivariate lat now\nthat we have created the\nunivariate plot to understand\nabout each attribute.\nLet's move on and look\nat the multivariate plot and see\nthe interaction between\nthe different variables.\nSo first, let's look\nat the scatter plot\nof all the attribute\nthis can be helpful\nto spot structured relationship\nbetween input variables.\nOkay.\nSo let's create\na scatter Matrix.\nSo for creating a scatter plot,\nwe need scatter Matrix,\nand we need to pass\nour data set into It okay.\nAnd then what I want\nI want to see it.\nSo plot dot show.\nSo this is\nhow my scatter Matrix looks\nlike it's like\nthat the diagonal grouping\nof some pear, right?\nSo this suggests\na high correlation\nand a predictable relationship.\nAll right.\nThis was our multivariate plot.\nNow, let's move on\nand evaluate some algorithm\nthat's time to create\nsome model of the data\nand estimate the accuracy\non the basis of unseen data.\nOkay.\nSo now we know all\nabout our data set, right?\nWe know how many instances\nand attributes are\nthere in our data set.\nWe know the summary\nof each attribute.\nSo I guess we have seen much\nabout our data set.\nNow.\nLet's move on\nand create some algorithm\nand estimate their accuracy\nbased on the Unseen data.\nOkay.\nNow what we'll do we'll create\nsome model of the data\nand estimate the accuracy based\non the some unseen data.\nOkay.\nSo for that first of all,\nlet's create a\nvalidation data set.\nWhat is the validation data\nset validation data set is\nyour training data set\nthat will be using it\nto trainer model fine.\nAll right.\nSo how will create\na validation data\nset for creating\na validation data set?\nWhat we are going to do is we\nare going to split our data set\ninto two point.\nOkay.\nSo the very first thing\nwe'll do is to create\na validation data set.\nSo why do we even need\na validation data set?\nSo we need a validation\ndata set know\nthat the model we\ncreated is any good later.\nWhat we'll do we'll use\nthe statistical method\nto estimate the accuracy\nof the model that we create\non the Unseen data.\nWe also want a more concrete\nestimate of the accuracy\nof the best model\non unseen data by evaluating it\non the actual unseen data.\nOkay confused.\nLet me simplify this for you.\nWhat we'll do we'll\nsplit the loaded data\ninto two parts the first\n80 percent of the data.\nUser to train our model\nand the rest 20% will hold back\nas the validation data set\nthat will use it to verify\nour trained model.\nOkay fine.\nSo let's define an array.\nThis is my ra water it\nwill consist of will consist\nof all the values\nfrom the data set.\nSo data set dot values.\nOkay next.\nI'll Define a variable X\nwhich will consist\nof all the column\nfrom the array from 0\nto 4 starting from 0 to 4\nand the next variable Y\nwhich would consist of\nof the array starting from this.\nSo first of all,\nwe will Define a variable X\nthat will consist of the values\nin the array starting\nfrom the beginning 0 Del for\nokay.\nSo these are the column\nwhich will include\nin the X variable\nand for a y variable I'll Define\nit as a class or the output.\nSo what I need,\nI just need the fourth column\nthat is my class column.\nSo I'll start it\nfrom the beginning\nand I just want\nthe fourth column.\nOkay now I'll Define\nthe my validation size.\nValidation underscore sighs,\nI'll Define it as 0.20\nand our use a seed I\nDefine CD equals 6.\nSo this method seed sets\nthe integers starting value used\nin generating random number.\nOkay, I'll Define the value\nof C R equals x.\nI'll tell you what is\nthe importance of that later on?\nOkay.\nSo let me Define first\nfew variables such as X\nunderscore train test\nwhy underscore train\nand why underscore test Okay,\nso What do you want to do\nis Select some model.\nOkay, so module\nunderscore selection.\nBut before doing\nthat what we have to do\nis split our training data set\ninto two halves.\nOkay, so dot train underscore\ntest underscore split\nwhat you want to split\nis a value of X and Y.\nOkay and my test size is equals\nto validation size,\nwhich is a 0.20 correct\nand my random state.\nIs equal to seed\nso what the city is doing?\nIt's helping me to keep the same\nRandomness in the training\nand testing data set fine.\nSo let's execute it and see\nwhat is our result.\nIt's executed next.\nWe'll create a test\nharness for this.\nWe'll use 10-fold\ncross-validation to\nestimate the accuracy.\nSo what it will do it\nwill split a data set\ninto 10 parts crane\non the nine part\nand test on the one part\nand this will repeat\nfor all combination of train\nand test pilots.\nOkay.\nSo for that,\nlet's define again my CD\nthat was six already\nDefine and scoring\nequals accuracy fine.\nSo we are using the metric of\naccuracy to evaluate the model.\nSo what is this?\nThis is a ratio of number\nof correctly predicted instances\ndivided by the total number\nof instances in the data set x\nhundred giving a\npercentage example.\nIt's 98% accurate or 99%\naccurate things like that.\nOkay, so we'll be\nIn the scoring variable\nwhen we run the build\nand evaluate each model\nin the next step.\nThe next part is building\nmodel till now.\nWe don't know which algorithm\nwould be good for this problem\nor what configuration to use.\nSo let's begin with\nsix different algorithm.\nI'll be using\nlogistic regression linear\ndiscriminant analysis,\nk-nearest neighbor\nclassification and\nregression trees neighbor buys.\nAnd what Vector machine well\nthese algorithms chime using is\na good mixture of simple linear\nor non-linear algorithms\nin simple linear switch.\nIncluded the logistic regression\nand the linear discriminant\nanalysis or the nonlinear part\nwhich included the KNN\nalgorithm the card algorithm\nthat the neighbor buys\nand the support Vector machines.\nOkay.\nSo we reset\nthe random number seed\nbefore each run\nto ensure that evaluation\nof each algorithm\nis performed using exactly\nthe same data spreads.\nIt ensures the result\nare directly comparable.\nOkay, so, let me\njust copy and paste it.\nOkay.\nSo what we're doing here,\nwe are building\nfive different types of model.\nWe are building\nlogistic regression\nlinear discriminant analysis,\nk-nearest neighbor decision\ntree ghajini buys\nand the support Vector machine.\nOkay next what we'll do we'll\nevaluate model in each turn.\nOkay.\nSo what is this?\nSo we have six different model\nand accuracy estimation\nfor each one of them\nnow we need to compare\nthe model to each other\nand select the most\naccurate of them all.\nSo running the script we\nsaw the following result\nso we can see some\nof the results on the screen.\nWhat is It is just\nthe accuracy score using\ndifferent set of algorithms.\nOkay, when we are using\nlogistic regression,\nwhat is the accuracy rate\nwhen we are using\nlinear discriminant algorithm?\nWhat is the accuracy\nand so-and-so?\nOkay.\nSo from the output with seems\nthat LD algorithm was\nthe most accurate model\nthat we tested now,\nwe want to get an idea\nof the accuracy of the model\non our validation set\nor the testing data set.\nSo this will give us\nan independent final check\non the accuracy\nof the best model.\nIt is always valuable\nto keep our testing data set\nfor just in case you\nmade a our overfitting\nto the testing data set\nor you made a data leak\nboth will result\nin an overly optimistic result.\nOkay, you can run the ldo model\ndirectly on the validation set\nand summarize the result as\na final score a confusion Matrix\nand a classification statistics\nand probability are essential\nbecause these disciples\nform the basic Foundation\nof all machine learning\nalgorithms deep learning.\nSocial intelligence\nand data science,\nin fact mathematics\nand probability is\nbehind everything around us\nfrom shapes patterns\nand colors to the count\nof petals in a flower\nmathematics is embedded\nin each and every\naspect of our lives.\nSo I'm going to go ahead\nand discuss the agenda\nfor today with you\nall we're going to begin\nthe session by understanding\nwhat is data after that.\nWe'll move on and look\nat the different categories\nof data like quantitative\nand Qualitative data,\nthen we'll discuss what\nexactly statistics is\nthe basic terminologies in\nstatistics and a couple\nof sampling techniques.\nOnce we're done with that.\nWe'll discuss a different\ntypes of Statistics\nwhich involve descriptive\nand inferential statistics.\nThen in the next session,\nwe will mainly be focusing\non descriptive statistics\nhere will understand\nthe different measures\nof center measures\nof spread Information Gain\nand entropy will also\nunderstand all of these measures\nwith the help of a user.\nAnd finally, we'll discuss what\nexactly a confusion Matrix is.\nOnce we've covered\nthe entire descriptive\nstatistics module will discuss\nthe probability module\nhere will understand\nwhat exactly probability\nis the different\nterminologies in probability.\nWe will also study the different\nprobability distributions,\nthen we'll discuss the types\nof probability which include\nmarginal probability joint\nand conditional probability.\nThen we move on\nand discuss a use case\nwherein we will see examples\nthat show us\nhow the different types\nof probability work\nand to better\nunderstand Bayes theorem.\nWe look at a small example.\nAlso, I forgot to mention\nthat at the end of the\ndescriptive statistics module\nwill be running a small demo\nin the our language.\nSo for those of you\nwho don't know much\nabout our I'll be explaining\nevery line in depth,\nbut if you want to have\na more in-depth understanding\nabout our I'll leave\na couple of blocks.\nAnd a couple of videos\nin the description box\nyou all can definitely\ncheck out that content.\nNow after we've completed the\nprobability module will discuss\nthe inferential statistics\nmodule will start this module\nby understanding\nwhat is point\nestimation will discuss\nwhat is confidence interval\nand how you can estimate\nthe confidence interval will\nalso discuss margin of error\nand will understand all\nof these concepts by looking\nat a small use case.\nWe finally end the inferential\nReal statistic module by looking\nat what hypothesis\ntesting is hypothesis.\nTesting is a very important part\nof inferential statistics.\nSo we'll end the session\nby looking at a use case\nthat discusses how\nhypothesis testing works\nand to sum everything up.\nWe'll look at a demo\nthat explains how\ninferential statistics works.\nRight?\nSo guys, there's\na lot to cover today.\nSo let's move ahead and take\na look at our first topic\nwhich is what is data.\nNow, this is\na quite simple question\nif I ask any of You\nwhat is data?\nYou'll see that it's\na set of numbers\nor some sort of documents\nthat have stored in my computer\nnow data is actually everything.\nAll right, look around you there\nis data everywhere each click\non your phone generates\nmore data than you know,\nnow this generated data\nprovides insights for analysis\nand helps us make\nBetter Business decisions.\nThis is why data is\nso important to give you\na formal definition data refers\nto facts and statistics.\nCollected together\nfor reference or analysis.\nAll right.\nThis is the definition\nof data in terms\nof statistics and probability.\nSo as we know data\ncan be collected it\ncan be measured and analyzed\nit can be visualized by\nusing statistical models\nand graphs now data is divided\ninto two major subcategories.\nAlright, so first we\nhave qualitative data\nand quantitative data.\nThese are the two\ndifferent types of data\nunder qualitative data.\nI'll be have nominal\nand ordinal data\nand under quantitative data.\nWe have discrete\nand continuous data.\nNow, let's focus\non qualitative data.\nNow this type of data deals with\ncharacteristics and descriptors\nthat can't be easily measured\nbut can be observed subjectively\nnow qualitative data\nis further divided\ninto nominal and ordinal data.\nSo nominal data is\nany sort of data\nthat doesn't have\nany order or ranking?\nOkay.\nAn example of nominal\ndata is gender.\nNow.\nThere is no ranking in gender.\nThere's only male female\nor other right?\nThere is no one two,\nthree four or any sort\nof ordering in gender race is\nanother example of nominal data.\nNow ordinal data is basically an\nordered series of information.\nOkay, let's say\nthat you went to a restaurant.\nOkay.\nYour information is stored\nin the form of customer ID.\nAll right.\nSo basically you are represented\nwith a customer ID.\nNow you would have rated\ntheir service as\neither good or average.\nAll right, that's\nhow no ordinal data is\nand similarly they'll have\na record of other customers\nwho visit the restaurant\nalong with their ratings.\nAll right.\nSo any data which has\nsome sort of sequence\nor some sort of order\nto it is known as ordinal data.\nAll right, so guys,\nthis is pretty simple\nto understand now,\nlet's move on and look\nat quantitative data.\nSo quantitative data\nbasically these He's\nwith numbers and things.\nOkay, you can understand\nthat by the word quantitative\nitself quantitative is\nbasically quantity.\nRight Saudis with numbers\na deals with anything\nthat you can measure\nobjectively, right?\nSo there are two types\nof quantitative data there is\ndiscrete and continuous data\nnow discrete data is also\nknown as categorical data\nand it can hold a finite number\nof possible values.\nNow, the number of students\nin a class is a finite Number.\nAll right, you can't\nhave infinite number\nof students in a class.\nLet's say in your fifth grade.\nThere were a hundred students\nin your class.\nAll right, there weren't\ninfinite number but there was\na definite finite number\nof students in your class.\nOkay, that's discrete data.\nNext.\nWe have continuous data.\nNow this type of data\ncan hold infinite number\nof possible values.\nOkay.\nSo when you say weight\nof a person is an example\nof continuous data\nwhat I mean to see is\nmy weight can be 50 kgs\nor it Can be 50.1 kgs\nor it can be 50.00 one kgs\nor 50.000 one or is 50.0 2 3\nand so on right?\nThere are infinite number\nof possible values, right?\nSo this is what I mean\nby continuous data.\nAll right.\nThis is the difference between\ndiscrete and continuous data.\nAnd also I would like to mention\na few other things over here.\nNow, there are a couple\nof types of variables as well.\nWe have a discrete variable\nand we have a continuous\nvariable discrete variable\nis also known as\na categorical variable\nor and it can hold values\nof different categories.\nLet's say that you have\na variable called message\nand there are two types\nof values that this variable\ncan hold let's say\nthat your message\ncan either be a Spam message\nor a non spam message.\nOkay, that's when you call\na variable as discrete\nor categorical variable.\nAll right, because it\ncan hold values\nthat represent different\ncategories of data\nnow continuous variables\nare basically variables\nthat can store in\nfinite number of values.\nSo the weight of a person\ncan be denoted as\na continuous variable.\nAll right, let's say there is\na variable called weight\nand it can store infinite number\nof possible values.\nThat's why we'll call it\na continuous variable.\nSo guys basically\nvariable is anything\nthat can store a value right?\nSo if you associate any sort\nof data with a A table,\nthen it will become\neither discrete variable\nor continuous variable.\nThere is also dependent and\nindependent type of variables.\nNow, we won't discuss all\nwith that in depth because\nthat's pretty understandable.\nI'm sure all of you know,\nwhat is independent variable\nand dependent variable right?\nDependent variable is\nany variable whose value\ndepends on any other\nindependent variable?\nSo guys that much\nknowledge I expect or\nif you do have all right.\nSo now let's move on and look\nat our next topic which Which is\nwhat is statistics now coming\nto the formal definition\nof statistics statistics is\nan area of Applied Mathematics,\nwhich is concerned\nwith data collection\nanalysis interpretation\nand presentation now usually\nwhen I speak about statistics\npeople think statistics is\nall about analysis\nbut statistics has other path\ntoward it has data collection is\nalso part of Statistics data\ninterpretation presentation.\nAll of this comes\ninto statistics already are\ngoing to use statistical methods\nto visualize data to collect\ndata to interpret data.\nAlright, so the area\nof mathematics deals\nwith understanding\nhow data can be used\nto solve complex problems.\nOkay.\nNow I'll give you\na couple of examples\nthat can be solved\nby using statistics.\nOkay, let's say\nthat your company\nhas created a new drug\nthat may cure cancer.\nHow would you conduct\na test to confirm\nthe As Effectiveness now,\neven though this sounds\nlike a biology problem.\nThis can be\nsolved with Statistics.\nAll right, you will have\nto create a test\nwhich can confirm\nthe effectiveness of the drum\nor a this is a common problem\nthat can be solved\nusing statistics.\nLet me give you\nanother example you\nand a friend are at a baseball\ngame and out of the blue.\nHe offers you a bet\nthat neither team will hit\na home run in that game.\nShould you take the BET?\nAll right here you just\ndiscuss the probability\nof I know you'll win or lose.\nAll right, this\nis another problem\nthat comes under statistics.\nLet's look at another example.\nThe latest sales data\nhas just come in\nand your boss wants\nyou to prepare a report\nfor management on places\nwhere the company\ncould improve its business.\nWhat should you look for?\nAnd what should you\nnot look for now?\nThis problem involves a lot\nof data analysis will have to\nlook at the different variables\nthat are causing\nyour business to go down\nor the you have to look\nat a few variables.\nThat are increasing\nthe performance of your models\nand does growing your business.\nAlright, so this involves\na lot of data analysis\nand the basic idea\nbehind data analysis is\nto use statistical techniques\nin order to figure\nout the relationship\nbetween different variables\nor different components\nin your business.\nOkay.\nSo now let's move on and look\nat our next topic which is basic\nterminologies and statistics.\nNow before you dive deep\ninto statistics, it is important\nthat you understand\nthe basic terminologies\nused in statistics.\nThe two most important\nterminologies in statistics\nare population and Sample.\nSo throughout the statistics\ncourse or throughout any problem\nthat you're trying\nto stall with Statistics.\nYou will come\nacross these two words,\nwhich is population and Sample\nNow population is a collection\nor a set of individuals\nor objects or events.\nEvents whose properties\nare to be analyzed.\nOkay.\nSo basically you can refer\nto population as a subject\nthat you're trying to analyze\nnow a sample is just\nlike the word suggests.\nIt's a subset of the population.\nSo you have to make sure\nthat you choose the sample\nin such a way\nthat it represents\nthe entire population.\nAll right.\nIt shouldn't Focus add one part\nof the population instead.\nIt should represent\nthe entire population.\nThat's how your sample\nshould be chosen.\nSo Well chosen sample\nwill contain most\nof the information about a\nparticular population parameter.\nNow, you must be wondering\nhow can one choose a sample\nthat best represents\nthe entire population now\nsampling is a statistical method\nthat deals with the selection\nof individual observations\nwithin a population.\nSo sampling is performed\nin order to infer statistical\nknowledge about a population.\nAll right, if you\nwant to understand\nthe different statistics\nof a population\nlike the mean\nthe median Median the mode\nor the standard deviation\nor the variance of a population.\nThen you're going\nto perform sampling.\nAll right,\nbecause it's not reasonable for\nyou to study a large population\nand find out the mean median\nand everything else.\nSo why is sampling\nperformed you might ask?\nWhat is the point of sampling?\nWe can just study\nthe entire population now guys,\nthink of a scenario\nwhere in you're asked\nto perform a survey\nabout the eating habits\nof teenagers in the US.\nSo at present there are\nover 42 million teens in the US\nand this number is growing\nas we are speaking\nright now, correct.\nIs it possible to survey each\nof these 42 million individuals\nabout their health?\nIs it possible?\nWell, it might be possible\nbut this will take\nforever to do now.\nObviously, it's not it's\nnot reasonable to go around\nknocking each door\nand asking for what does\nyour teenage son eat\nand all of that right?\nThis is not very reasonable.\nThat's Why sampling is used?\nIt's a method wherein a sample\nof the population is studied\nin order to draw inferences\nabout the entire population.\nSo it's basically\na shortcut to starting\nthe entire population instead\nof taking the entire population\nand finding out\nall the solutions.\nYou just going to take\na part of the population\nthat represents the\nentire population\nand you're going to perform\nall your statistical analysis\nyour inferential statistics\non that small sample.\nAll right,\nand that sample basically here\nPresents the entire population.\nAll right, so I'm sure\nhave made this clear\nto you all what is sample\nand what is population now?\nThere are two main types\nof sampling techniques\nthat are discussed today.\nWe have probability sampling\nand non-probability\nsampling now in this video\nwill only be focusing on\nprobability sampling techniques\nbecause non-probability sampling\nis not within the scope\nof this video.\nAll right will only discuss\nthe probability part\nbecause we're focusing\non Statistics and\nprobability correct.\nNow again under\nprobability sampling.\nWe have three different types.\nWe have random\nsampling systematic\nand stratified sampling.\nAll right, and just\nto mention the different types\nof non-probability\nsampling zwi have\nno ball Kota judgment\nand convenience sampling.\nAll right now guys\nin this session.\nI'll only be\nfocusing on probability.\nSo let's move on\nand look at the different types\nof probability sampling.\nSo what is Probability sampling.\nIt is a sampling technique\nin which samples\nfrom a large population\nare chosen by using\nthe theory of probability.\nAll right, so there\nare three types\nof probability sampling.\nAll right first we have\nthe random sampling now\nin this method each member\nof the population\nhas an equal chance\nof being selected in the sample.\nAll right.\nSo each and every individual\nor each and every object\nin the population\nhas an equal chance\nof being a A part of the sample.\nThat's what random\nsampling is all about.\nOkay, you are randomly going\nto select any individual\nor any object.\nSo this Bay each individual has\nan equal chance\nof being selected.\nCorrect?\nNext.\nWe have systematic sampling now\nin systematic sampling\nevery nth record is chosen\nfrom the population to be\na part of the sample.\nAll right.\nNow refer this image\nthat I've shown over here\nout of these six groups\nevery Skinned group\nis chosen as a sample.\nOkay.\nSo every second record\nis chosen here and this is\nour systematic sampling works.\nOkay, you're randomly\nselecting the nth record\nand you're going to add\nthat to your sample.\nNext.\nWe have stratified sampling.\nNow in this type\nof technique a stratum\nis used to form samples\nfrom a large population.\nSo what is a stratum\na stratum is basically a subset\nof the population\nthat shares at\nleast one comment.\nCharacteristics so let's say\nthat your population has a mix\nof both male and female\nso you can create to straightens\nout of this one will have\nonly the male subset\nand the other will have\nthe female subset\nor a this is what stratum is\nit is basically a subset\nof the population\nthat shares at least\none common characteristics.\nAll right in our example,\nit is gender.\nSo after you've created\na stratum you're going\nto use random sampling\non the stratums and you're going\nto choose a final Samba.\nBut so random sampling meaning\nthat all of the individuals\nin each of the stratum\nwill have an equal chance\nof being selected\nin the sample, correct.\nSo Guys, these were\nthe three different types\nof sampling techniques.\nNow, let's move on and look\nat our next topic\nwhich is the different\ntypes of Statistics.\nSo after this,\nwe'll be looking at the more\nadvanced concepts of Statistics,\nright so far we discuss\nthe basics of Statistics,\nwhich is basically\nwhat is statistics\nthe different sampling.\nTechniques and the\nterminologies and statistics.\nAll right.\nNow we look at the different\ntypes of Statistics.\nSo there are two major\ntypes of Statistics\ndescriptive statistics\nand inferential statistics\nin today's session.\nWe will be discussing\nboth of these types\nof Statistics in depth.\nAll right, we'll also\nbe looking at a demo\nwhich I'll be running\nin the our language\nin order to make\nyou understand what exactly\ndescriptive and inferential\nstatistics is so guys,\nwhich is going to look\nat the 600 don't worry,\nif you don't\nhave much knowledge,\nI'm explaining everything\nfrom the basic level.\nAll right, so guys descriptive\nstatistics is a method\nwhich is used to describe\nand understand the features\nof specific data set by giving\na short summary of the data.\nOkay, so it is mainly\nfocused upon the\ncharacteristics of data.\nIt also provides a graphical\nsummary of the data now\nin order to make you understand\nwhat descriptive statistics is,\nlet's suppose.\nPose that you want to gift all\nyour classmates or t-shirt.\nSo to study the average\nshirt size of a student\nin a classroom.\nSo if you were to use\ndescriptive statistics to study\nthe average shirt size\nof students in your classroom,\nthen what you would do is you\nwould record the shirt size\nof all students in the class\nand then you would find out\nthe maximum minimum and average\nshirt size of the club.\nOkay.\nSo coming to inferential\nstatistics inferential\nstatistics makes Is\nand predictions about\na population based\non the sample of data taken\nfrom the population?\nOkay.\nSo in simple words,\nit generalizes a large data set\nand it applies probability\nto draw a conclusion.\nOkay.\nSo it allows you\nto infer data parameters\nbased on a statistical model\nby using sample data.\nSo if we consider\nthe same example of finding\nthe average shirt size\nof students in a class\nin infinite shal statistics,\nyou will take a sample.\nAll set of the class\nwhich is basically a few people\nfrom the entire class.\nAll right, you already\nhave had grouped the class\ninto large medium and small.\nAll right in this method\nyou basically build\na statistical model\nand expand it for the entire\npopulation in the class.\nSo guys, there was a brief\nunderstanding of descriptive\nand inferential statistics.\nSo that's the difference\nbetween descriptive\nand inferential now\nin the next section,\nwe will go in depth\nabout descriptive statistics.\nAll right, so,\nThat's a discuss more\nabout descriptive statistics.\nSo like I mentioned\nearlier descriptive\nstatistics is a method\nthat is used to describe\nand understand the features\nof a specific data set by giving\nshort summaries about the sample\nand measures of the data.\nThere are two important measures\nin descriptive statistics.\nWe have measure\nof central tendency,\nwhich is also known as measure\nof center and we have\nmeasures of variability.\nThis is also known as\nmeasures of spread.\nEd so measures of center\ninclude mean median and mode now\nwhat is measures\nof center measures of the center\nare statistical measures\nthat represent the summary\nof a data set?\nOkay, the three main measures\nof center are mean median\nand mode coming\nto measures of variability\nor measures of spread.\nWe have range\ninterquartile range variance\nand standard deviation.\nAll right.\nSo now let's discuss each\nof these measures\nin a little more.\nUp starting with\nthe measures of center.\nNow.\nI'm sure all of you know what\nthe mean is mean is basically\nthe measure of the average\nof all the values in a sample.\nOkay, so it's basically\nthe average of all\nthe values in a sample.\nHow do you measure the mean I\nhope all of you know\nhow the main is measured\nif there are 10 numbers\nand you want to find the mean\nof these 10 numbers.\nAll you have to do is you have\nto add up all the 10 numbers\nand you have to divide\nit by 10 then here\nrepresents the Number\nof samples in your data set.\nAll right, since we\nhave 10 numbers,\nwe're going to\ndivide this by 10.\nAll right, this will\ngive us the average\nor the mean so to better\nunderstand the measures\nof central tendency.\nLet's look at an example.\nNow the data set over here is\nbasically the cars data set\nand it contains a few variables.\nAll right, it has\nsomething known as cars.\nIt has mileage\nper gallon cylinder\ntype displacement horsepower\nand roll axle ratio.\nAll right, all of these measures\nare related to cars.\nOkay.\nSo what you're going\nto do is you're going\nto use descriptive analysis\nand you're going to analyze\neach of the variables\nin the sample data set\nfor the mean standard\ndeviation median mode and so on.\nSo let's say that you want\nto find out the mean\nor the average horsepower\nof the cars among\nthe population of cards.\nLike I mentioned earlier\nwhat you'll do is you will check\nthe average of all the values.\nSo in this case,\nwe will take the sum\nof the horizontal.\nHorsepower of each car\nand we'll divide that\nby the total number of cards.\nOkay, that's exactly\nwhat I've done here\nin the calculation part.\nSo this hundred\nand ten basically\nrepresents the horsepower\nfor the first car.\nAlright, similarly.\nI've just added up all\nthe values of horsepower\nfor each of the cars\nand I've divided it by 8 now\n8 is basically the number\nof cars in our data set.\nAll right, so hundred and three\npoint six two five is what\nour mean is or Average\nof horsepower is all right.\nNow, let's understand\nwhat median is with an example?\nOkay.\nSo to Define median median\nis basically a measure\nof the central value\nof the sample set\nis called the median.\nAll right, you can see\nthat it is a middle value.\nSo if we want to find\nout the center value\nof the mileage per gallon\namong the population\nof cars first,\nwhat we'll do is we'll arrange\nthe MGP values in ascending\nor descending order\nand Choose a middle value\nright in this case\nsince we have\neight values, right?\nWe have eight values\nwhich is an even entry.\nSo whenever you have even\nnumber of data points\nor samples in your data set,\nthen you're going\nto take the average\nof the two middle values.\nIf we had nine values over here.\nWe can easily figure\nout the middle value\nand you know choose\nthat as a median.\nBut since they're even number\nof values we're going\nto take the average\nof the two middle values.\nAll right, so,\nEight and twenty three\nare my two middle values\nand I'm taking\nthe mean of those 2\nand hence I get\ntwenty two point nine,\nwhich is my median.\nAll right.\nLastly let's look at\nhow mode is calculated.\nSo what is mode the value\nthat is most recurrent\nin the sample set is known as\nmode or basically the value\nthat occurs most often.\nOkay, that is known as mode.\nSo let's say\nthat we want to find out\nthe most common type of cylinder\namong the population\nof cards all we have to Do\nis we will check the value\nwhich is repeated\nthe most number of times here.\nWe can see that the cylinders\ncome in two types.\nWe have cylinder of Type\n4 and cylinder of type 6, right?\nSo take a look at the data set.\nYou can see that the most\nrecurring value is 6 right.\nWe have one two,\nthree four and five.\nWe have five six\nand we have one two, three.\nYeah, we have three four types\nof lenders and 5/6.\nCylinders.\nSo basically we have\nthree four type cylinders and we\nhave five six type cylinders.\nAll right.\nSo our mode is going\nto be 6 since 6 is more\nrecurrent than 4 so guys\nthose were the measures\nof the center or the measures\nof central tendency.\nNow, let's move on and look\nat the measures of the spread.\nAll right.\nNow, what is the measure\nof spread a measure of spread?\nSometimes also called\nas measure of dispersion is used\nto describe the The variability\nin a sample or population.\nOkay, you can think\nof it as some sort\nof deviation in the sample.\nAll right.\nSo you measure this\nwith the help of the different\nmeasure of spreads.\nWe have range\ninterquartile range variance\nand standard deviation.\nNow range is pretty\nself-explanatory, right?\nIt is the given measure of\nhow spread apart the values\nin a data set are\nthe range can be calculated\nas shown in this formula.\nSo you're basically going\nto The maximum value\nin your data set\nfrom the minimum value\nin your data set.\nThat's how you calculate\nthe range of the data.\nAlright, next we\nhave interquartile range.\nSo before we discuss\ninterquartile range,\nlet's understand.\nWhat a quartile is red.\nSo quartiles basically tell us\nabout the spread of a data set\nby breaking the data set\ninto different quarters.\nOkay, just like\nhow the median breaks\nthe data into two parts.\nThe quartile will break it.\nIn two different quarters,\nso to better understand\nhow quartile and\ninterquartile are calculated.\nLet's look at a small example.\nNow this data set basically\nrepresents the marks\nof hundred students\nordered from the lowest\nto the highest scores red.\nSo the quartiles lie\nin the following ranges\nthe first quartile,\nwhich is also known as q1 it\nlies between the 25th\nand 26th observation.\nAll right.\nSo if you look at this\nI've highlighted the 25th\nand the Six observation.\nSo how you can calculate\nQ 1 or first quartile is\nby taking the average\nof these two values.\nAlright, since both\nthe values are 45\nwhen you add them up\nand divide them by two\nyou'll still get 45 now\nthe second quartile\nor Q 2 is between the 50th\nand the fifty first observation.\nSo you're going to take\nthe average of 58\nand 59 and you will get\na value of 58.5 now,\nthis is my second quarter\nthe third quartile Q3.\nIs between the 75th\nand the 76th observation here\nagain will take the average\nof the two values\nwhich is the 75th value\nand the 76 value right\nand you'll get a value of 71.\nAll right, so guys\nthis is exactly\nhow you calculate\nthe different quarters.\nNow, let's look at\nwhat is interquartile range.\nSo IQR or the interquartile\nrange is a measure\nof variability based on dividing\na data set into quartiles.\nNow, the interquartile\nrange is Calculated\nby subtracting the q1 from Q3.\nSo basically Q3\nminus q1 is your IQ are so\nyour IQR is your Q3 minus q1?\nAll right.\nNow this is how each\nof the quartiles are each core\ntile represents a quarter,\nwhich is 25% All right.\nSo guys, I hope all\nof you are clear\nwith the interquartile range\nand what our quartiles now,\nlet's look at\nvariance covariance is\nbasically a measure\nthat shows how much a\nI'm variable the first\nfrom its expected value.\nOkay.\nIt's basically the variance\nin any variable now variance\ncan be calculated by using\nthis formula right here x\nbasically represents\nany data point in your data set\nn is the total number\nof data points in your data set\nand X bar is basically\nthe mean of data points.\nAll right.\nThis is how you calculate\nvariance variance is\nbasically a Computing\nthe squares of deviations.\nOkay.\nThat's why it says\ns Square there now.\nLook at what is deviation\ndeviation is just the difference\nbetween each element\nfrom the mean.\nOkay, so it can be calculated\nby using this simple formula\nwhere X I basically\nrepresents a data point\nand mu is the mean\nof the population\nor add this is exactly\nhow you calculate the deviation\nNow population variance\nand Sample variance\nare very specific to\nwhether you're calculating\nthe variance\nin your population data\nset or in your sample data\nset now the only\ndifference between Elation\nand Sample variance.\nSo the formula\nfor population variance\nis pretty explanatory.\nSo X is basically\neach data point mu is the mean\nof the population\nn is the number of samples\nin your data set.\nAll right.\nNow, let's look at sample.\nVariance Now sample variance\nis the average of squared\ndifferences from the mean.\nAll right here x\ni is any data point\nor any sample in your data\nset X bar is the mean\nof your sample.\nAll right.\nIt's not the main\nof your population.\nIt's the If your sample and\nif you notice any here is\na smaller n is the number\nof data points in your sample.\nAnd this is basically\nthe difference between sample\nand population variance.\nI hope that is clear coming\nto standard deviation is\nthe measure of dispersion\nof a set of data from its mean.\nAll right, so it's basically\nthe deviation from your mean.\nThat's what standard deviation\nis now to better understand\nhow the measures\nof spread are calculated.\nLet's look at a small use case.\nSo let's say the Daenerys\nhas 20 dragons.\nThey have the numbers\nnine to five four and so on\nas shown on the screen,\nwhat you have to do is\nyou have to work out\nthe standard deviation or at\nin order to calculate\nthe standard deviation.\nYou need to know the mean right?\nSo first you're going to find\nout the mean of your sample set.\nSo how do you calculate\nthe mean you add all the numbers\nin your data set\nand divided by the total number\nof samples in your data set\nso you get a value of 7 here\nthen you I'll clear the rhs of\nyour standard deviation formula.\nAll right, so from\neach data point you're going\nto subtract the mean\nand you're going to square that.\nAll right.\nSo when you do that,\nyou will get\nthe following result.\nYou'll basically get\nthis 425 for 925\nand so on so finally you\nwill just find the mean\nof the squared differences.\nAll right.\nSo your standard deviation\nwill come up to two point\nnine eight three\nonce you take the square root.\nSo guys, this is pretty simple.\nIt's a simple\nmathematic technique.\nAll you have to do is you have\nto substitute the values\nin the formula.\nAll right.\nI hope this was clear\nto all of you.\nNow let's move on\nand discuss the next topic\nwhich is Information Gain\nand entropy now.\nThis is one of my favorite\ntopics in statistics.\nIt's very interesting and\nthis topic is mainly involved\nin machine learning algorithms,\nlike decision trees\nand random forest.\nAll right, it's very important\nfor you to know\nhow Information Gain and entropy\nreally work and why they are\nso essential in building\nmachine learning models.\nWe focus on the statistic parts\nof Information Gain and entropy\nand after that we'll discuss\nAs a use case and see\nhow Information Gain\nand entropy is used\nin decision trees.\nSo for those of you\nwho don't know what\na decision tree is it is\nbasically a machine\nlearning algorithm.\nYou don't have to know\nanything about this.\nI'll explain\neverything in depth.\nSo don't worry.\nNow.\nLet's look at what exactly\nentropy and Information Gain Is\nAs entropy is\nbasically the measure\nof any sort of uncertainty\nthat is present in the data.\nAll right, so it can be measured\nby using this formula.\nSo here s is the set\nof all instances in the data set\nor although data items\nin the data set\nn is the different type\nof classes in your data set\nPi is the event probability.\nNow this might seem\na little confusing\nto you all but when we\ngo to the use case,\nyou'll understand all\nof these terms even better.\nAll right cam.\nTo Information Gain\nas the word suggests\nInformation Gain indicates\nhow much information\na particular feature\nor a particular variable gives\nus about the final outcome.\nOkay, it can be measured\nby using this formula.\nSo again here hedge\nof s is the entropy\nof the whole data set\ns SJ is the number\nof instances with the J value\nof an attribute a s is\nthe total number of instances\nin the data set V is the set\nof distinct values\nof an attribute a hedge\nof SJ is the entropy\nof subsets of instances\nand hedge of a comma s is\nthe entropy of an attribute\na even though\nthis seems confusing.\nI'll clear out the confusion.\nAll right, let's discuss\na small problem statement\nwhere we will understand\nhow Information Gain\nand entropy is used to study\nthe significance of a model.\nSo like I said Information Gain\nand entropy are very\nimportant statistical measures\nthat let us understand\nthe significance of\na predictive model.\nOkay to get a more\nclear understanding.\nLet's look at a use case.\nAll right now suppose we\nare given a problem statement.\nAll right, the statement is\nthat you have to predict\nwhether a match can be played\nor Not by studying\nthe weather conditions.\nSo the predictor variables here\nare outlook humidity wind day\nis also a predictor variable.\nThe target variable\nis basically played already.\nThe target variable\nis the variable\nthat you're trying to protect.\nOkay.\nNow the value of the target\nvariable will decide\nwhether or not a game\ncan be played.\nAll right, so that's\nwhy The play has two values.\nIt has no and yes, no,\nmeaning that the weather\nconditions are not good.\nAnd therefore you\ncannot play the game.\nYes, meaning that the weather\nconditions are good and suitable\nfor you to play the game.\nAlright, so that was\na problem statement.\nI hope the problem statement\nis clear to all of you now\nto solve such a problem.\nWe make use of something\nknown as decision trees.\nSo guys think\nof an inverted tree\nand each branch of the tree\ndenotes some decision.\nAll right, each branch is\nIs known as the branch node\nand at each branch node,\nyou're going to take\na decision in such a manner\nthat you will get an outcome\nat the end of the branch.\nAll right.\nNow this figure\nhere basically shows\nthat out of 14 observations\n9 observations result in a yes,\nmeaning that out of 14 days.\nThe match can be played\nonly on nine days.\nAlright, so here\nif you see on day 1 Day\n2 Day 8 day 9 and 11.\nThe Outlook has been Alright,\nso basically we try\nto plaster a data set\ndepending on the Outlook.\nSo when the Outlook is sunny,\nthis is our data set\nwhen the Outlook is overcast.\nThis is what we have\nand when the Outlook is\nthe rain this is what we have.\nAll right, so\nwhen it is sunny we have\ntwo yeses and three nodes.\nOkay, when the\nOutlook is overcast.\nWe have all four\nas yes has meaning\nthat on the four days\nwhen the Outlook was overcast.\nWe can play the game.\nAll right.\nNow when it comes to drain,\nwe have three yeses\nand two nodes.\nAll right.\nSo if you notice here,\nthe decision is being made by\nchoosing the Outlook variable\nas the root node.\nOkay.\nSo the root node is\nbasically the topmost node\nin a decision tree.\nNow, what we've done here is\nwe've created a decision tree\nthat starts with\nthe Outlook node.\nAll right, then you're splitting\nthe decision tree further\ndepending on other parameters\nlike Sunny overcast and rain.\nAll right now like we know\nthat Outlook has three values.\nSunny overcast and brain\nso let me explain this\nin a more in-depth manner.\nOkay.\nSo what you're doing\nhere is you're making\nthe decision Tree by choosing\nthe Outlook variable\nat the root node.\nThe root note is\nbasically the topmost node\nin a decision tree.\nNow the Outlook node has three\nbranches coming out from it,\nwhich is sunny\novercast and rain.\nSo basically Outlook\ncan have three values\neither it can be sunny.\nIt can be overcast\nor it can be rainy.\nOkay now these three values\nUse are assigned\nto the immediate Branch\nnodes and for each\nof these values the possibility\nof play is equal\nto yes is calculated.\nSo the sunny\nand the rain branches\nwill give you an impure output.\nMeaning that there is a mix\nof yes and no right.\nThere are two yeses\nhere three nodes here.\nThere are three yeses here\nand two nodes over here,\nbut when it comes\nto the overcast variable,\nit results in a hundred\npercent pure subset.\nAll right, this shows that\nthe overcast baby.\nWill result in a definite\nand certain output.\nThis is exactly what entropy\nis used to measure.\nAll right, it calculates\nthe impurity or the uncertainty.\nAlright, so the lesser\nthe uncertainty or the entropy\nof a variable more\nsignificant is that variable?\nSo when it comes to overcast\nthere's literally no impurity\nin the data set.\nIt is a hundred percent\npure subset, right?\nSo be want variables like these\nin order to build a model.\nAll right now,\nwe don't always Ways get lucky\nand we don't always find\nvariables that will result\nin pure subsets.\nThat's why we have\nthe measure entropy.\nSo the lesser the entropy of\na particular variable the most\nsignificant that variable\nwill be so in a decision tree.\nThe root node is assigned\nthe best attribute\nso that the decision tree\ncan predict the most\nprecise outcome meaning\nthat on the root note.\nYou should have the most\nsignificant variable.\nAll right, that's why\nwe've chosen Outlook\nor and now some of you might ask\nme why haven't you chosen\novercast Okay is overcast\nis not a variable.\nIt is a value\nof the Outlook variable.\nAll right.\nThat's why we've chosen\noutlook here because it has\na hundred percent pure subset\nwhich is overcast.\nAll right.\nNow the question in your head is\nhow do I decide which variable\nor attribute best Blitz\nthe data now right now,\nI know I looked at the data\nand I told you that,\nyou know here we have\na hundred percent pure subset,\nbut what if it's\na more complex problem\nand you're not able\nto understand which variable\nwill best split the data,\nso guys when it comes\nto decision tree\nInformation and gain\nand entropy will help\nyou understand which variable\nwill best split the data set.\nAll right, or which variable you\nhave to assign to the root node\nbecause whichever variable\nis assigned to the dude node.\nIt will best let the data set\nand it has to be the most\nsignificant variable.\nAll right.\nSo how we can do this\nis we need to use\nInformation Gain and entropy.\nSo from the total\nof the 14 instances\nthat we saw nine\nof them said yes\nand 5 of the instances said know\nthat you cannot play\non that particular day.\nAll right.\nSo how do you\ncalculate the entropy?\nSo this is the formula\nyou just substitute\nthe values in the formula.\nSo when you substitute\nthe values in the formula,\nyou will get a value of 0.9940.\nAll right.\nThis is the entropy\nor this is the uncertainty\nof the data present in a sample.\nNow in order to ensure\nthat we choose the best variable\nfor the root node.\nLet us look at all\nthe possible combinations\nthat you can use\non the root node.\nOkay, so these are All\nthe possible combinations\nyou can either have\nOutlook you can have\nwindy humidity or temperature.\nOkay, these are four variables\nand you can have any one\nof these variables\nas your root node.\nBut how do you select\nwhich variable best\nfits the root node?\nThat's what we are going\nto see by using\nInformation Gain and entropy.\nSo guys now the task at hand\nis to find the information gain\nfor each of these attributes.\nAll right.\nSo for Outlook for windy for\nhumidity and for temperature,\nwe're going to find\nout the information.\nNation gained right now\na point to remember is\nthat the variable\nthat results in the highest\nInformation Gain must be chosen\nbecause it will give us the most\nprecise and output information.\nAll right.\nSo the information gain for\nattribute windy will calculate\nthat first here.\nWe have six instances of true\nand eight instances of false.\nOkay.\nSo when you substitute all\nthe values in the formula,\nyou will get a value\nof zero point zero four eight.\nSo we get a value\nof You 2.0 for it.\nNow.\nThis is a very low value\nfor Information Gain.\nAll right, so the information\nthat you're going to get from\nWindy attribute is pretty low.\nSo let's calculate\nthe information gain\nof attribute Outlook.\nAll right, so from the total\nof 14 instances,\nwe have five instances\nwith say Sunny for instances,\nwhich are overcast\nand five instances,\nwhich are rainy.\nAll right for Sonny.\nWe have three yeses and to nose\nfor overcast we have All the\nfor as yes for any we have\nthree years and two nodes.\nOkay.\nSo when you calculate\nthe information gain\nof the Outlook variable\nwill get a value\nof zero point 2 4 7 now compare\nthis to the information gain\nof the windy attribute.\nThis value is\nactually pretty good.\nRight we have zero point 2 4 7\nwhich is a pretty good value\nfor Information Gain.\nNow, let's look\nat the information gain\nof attribute humidity\nnow over here.\nWe have seven instances\nwith say hi and seven instances\nwith say normal.\nRight and under\nthe high Branch node.\nWe have three instances\nwith say yes,\nand the rest for instances\nwould say no similarly\nunder the normal Branch.\nWe have one two, three,\nfour, five six seven\ninstances would say yes\nand one instance with says no.\nAll right.\nSo when you calculate\nthe information gain\nfor the humidity variable,\nyou're going to get\na value of 0.15 one.\nNow.\nThis is also\na pretty decent value,\nbut when you compare it\nto the Information Gain,\nOf the attribute Outlook it\nis less right now.\nLet's look at the information\ngain of attribute temperature.\nAll right, so the temperature\ncan hold repeat.\nSo basically the temperature\nattribute can hold\nhot mild and cool.\nOkay under hot.\nWe have two instances\nwith says yes and two instances\nfor no under mild.\nWe have four instances of yes\nand two instances of no\nand under col we have\nthree instances of yes\nand one instance of no.\nAll right.\nWhen you calculate\nthe information gain\nfor this attribute,\nyou will get a value\nof zero point zero to nine,\nwhich is again very less.\nSo what you can summarize\nfrom here is if we look\nat the information gain for each\nof these variable will see\nthat for Outlook.\nWe have the maximum gain.\nAll right, we have\nzero point two four seven,\nwhich is the highest\nInformation Gain value\nand you must always choose\na variable with the highest\nInformation Gain to split\nthe data at the root node.\nSo that's why we assign\nThe Outlook variable\nat the root node.\nAll right, so guys.\nI hope this use case with clear\nif any of you have doubts.\nPlease keep commenting\nthose doubts now,\nlet's move on and look at what\nexactly a confusion Matrix is\nthe confusion Matrix\nis the last topic\nfor descriptive statistics\nread after this.\nI'll be running a short demo\nwhere I'll be showing you\nhow you can calculate\nmean median mode\nand standard deviation variance\nand all of those values\nby using our okay.\nSo let's talk about\nconfusion Matrix now guys.\nWhat is the confusion Matrix\nnow don't get confused.\nThis is not any complex\ntopic now confusion.\nMatrix is a matrix\nthat is often used to describe\nthe performance of a model.\nAll right, and this\nis specifically used\nfor classification models\nor a classifier\nand what it does is it\nwill calculate the accuracy\nor it will calculate the\nperformance of your classifier\nby comparing your actual results\nand Your predicted results.\nAll right.\nSo this is what it looks\nlike to positive to\nnegative and all of that.\nNow this is a little confusing.\nI'll get back to what\nexactly true positive\nto negative and all\nof this stands for for now.\nLet's look at an example and\nlet's try and understand what\nexactly confusion Matrix is.\nSo guys have made sure\nthat I put examples\nafter each and every topic\nbecause it's important you\nunderstand the Practical\npart of Statistics.\nAll right statistics has\nliterally nothing to do\nwith Theory you need\nto understand how Calculations\nare done in statistics.\nOkay.\nSo here what I've done is now\nlet's look at a small use case.\nOkay, let's consider\nthat your given data\nabout a hundred and sixty five\npatients out of which hundred\nand five patients have a disease\nand the remaining 50 patients\ndon't have a disease.\nOkay.\nSo what you're going to do is\nyou will build a classifier\nthat predicts by using\nthese hundred and\nsixty five observations.\nYou'll feed all\nof these 165 observations\nto your classifier\nand it will predict\nthe output every time\na new patients detail is fed\nto the classifier right now\nout of these 165 cases.\nLet's say that\nthe classifier predicted.\nYes hundred and ten times\nand no 55 times.\nAlright, so yes\nbasically stands for yes.\nThe person has a disease\nand no stands for know.\nThe person does\nnot have a disease.\nAll right, that's\npretty self-explanatory.\nBut yeah, so it predicted\nthat a hundred and ten times.\nPatient has a disease\nand 55 times\nthat know the patient\ndoesn't have a disease.\nHowever in reality only\nhundred and five patients\nin the sample have\nthe disease and 60 patients\nwho do not have\nthe disease, right?\nSo how do you calculate\nthe accuracy of your model?\nYou basically build\nthe confusion Matrix?\nAll right.\nThis is how the Matrix looks\nlike and basically denotes\nthe total number of observations\nthat you have\nwhich is 165 in our case\nactual denotes the actual use\nin the data set\nand predicted denotes\nthe predicted values\nby the classifier.\nSo the actual value is no here\nand the predicted\nvalue is no here.\nSo your classifier\nwas correctly able\nto classify 50 cases as no.\nAll right, since both\nof these are no so 50\nit was correctly able\nto classify but 10\nof these cases it\nincorrectly classified meaning\nthat your actual value here\nis no but you classifier\npredicted it as yes\nor I that's why this\nAnd over here similarly\nit wrongly predicted\nthat five patients\ndo not have diseases\nwhereas they actually\ndid have diseases\nand it correctly\npredicted hundred patients,\nwhich have the disease.\nAll right.\nI know this is\na little bit confusing.\nBut if you look\nat these values no,\nno 50 meaning\nthat it correctly\npredicted 50 values No\nYes means that it\nwrongly predicted.\nYes for the values are it\nwas supposed to predict.\nNo.\nAll right.\nNow what exactly is?\nIs this true positive\nto negative and all of that?\nI'll tell you what\nexactly it is.\nSo true positive are the cases\nin which we predicted a yes\nand they do not actually\nhave the disease.\nAll right, so it is\nbasically this value\nalready predicted a yes here,\neven though they\ndid not have the disease.\nSo we have 10 true positives\nright similarly true-\nis we predicted know\nand they don't have\nthe disease meaning\nthat this is correct.\nFalse positive is be predicted.\nYes, but they do not\nactually have the disease\nor at this is also known as type\n1 error falls- is we predicted.\nNo, but they actually\ndo not have the disease.\nSo guys basically falls-\nand true negatives are basically\ncorrect classifications.\nAll right.\nSo this was confusion Matrix\nand I hope this concept\nis clear again guys.\nIf you have doubts,\nplease comment your doubt\nin the comment section.\nSo guys, that was\nthe entire descriptive.\nX module and now we\nwill discuss about probability.\nOkay.\nSo before we understand\nwhat exactly probability is,\nlet me clear out a very\ncommon misconception people\noften tend to ask\nme this question.\nWhat is the relationship between\nstatistics and probability?\nSo probability and statistics\nare related fields.\nAll right.\nSo probability is\na mathematical method used\nfor statistical analysis.\nTherefore we can say\nthat a probability and\nstatistics are interconnected.\nLaunches of mathematics\nthat deal with analyzing the\nrelative frequency of events.\nSo they're very\ninterconnected feels\nand probability makes\nuse of statistics\nand statistics makes use\nof probability or a they're\nvery interconnected Fields.\nSo that is the relationship\nbetween statistics\nand probability.\nNow, let's understand\nwhat exactly is probability.\nSo probability is the measure\nof How likely an event\nwill occur to be more precise.\nIt is the ratio.\nOf desired outcome\nto the total outcomes.\nNow, the probability\nof all outcomes always sum up\nto 1 the probability will always\nsum up to 1 probability\ncannot go beyond one.\nOkay.\nSo either your probability\ncan be 0 or it can be 1\nor it can be in the form\nof decimals like 0.5\nto or 0.55 or it can be\nin the form of 0.5 0.7 0.9.\nBut it's valuable always stay\nbetween the range 0 and 1.\nOkay at the famous example\nof probability is rolling\na dice example.\nSo when you roll a dice you get\nsix possible outcomes, right?\nYou get one two,\nthree four and five six\nphases of a dice now\neach possibility only\nhas one outcome.\nSo what is the probability\nthat on rolling a dice?\nYou will get 3 the probability\nis 1 by 6, right\nbecause there's only one phase\nwhich has the number 3 on it\nout of six phases.\nThere's only one phase\nwhich has the number three.\nSo the probability of getting 3\nwhen you roll a dice\nis 1 by 6 similarly,\nif you want to find\nthe probability of getting\na number 5 again,\nthe probability is\ngoing to be 1 by 6.\nAll right, so all\nof this will sum up to 1.\nAll right, so guys this is\nexactly what probability is.\nIt's a very simple concept\nwe all learnt it\nin 8 standard onwards right now.\nLet's understand the\ndifferent terminologies\nthat are related to probability.\nNow the three terminologies\nthat you often come across\nwhen We talk about probability.\nWe have something known\nas the random experiment.\nOkay, it's basically\nan experiment or a process\nfor which the outcomes cannot be\npredicted with certainty.\nAll right.\nThat's why you use probability.\nYou're going to use probability\nin order to predict the outcome\nwith some sort\nof certainty sample space\nis the entire possible set\nof outcomes of a random\nexperiment an event is\none or more outcomes\nof an experiment.\nSo if you consider the example\nLove rolling a dice.\nNow.\nLet's say that you want\nto find out the probability\nof getting a to\nwhen you roll the dice.\nOkay.\nSo finding this probability\nis the random experiment\nthe sample space is basically\nyour entire possibility.\nOkay.\nSo one two, three, four,\nfive six phases are there\nand out of that you need\nto find the probability\nof getting a 2, right.\nSo all the possible outcomes\nwill basically represent\nyour sample space.\nOkay.\nSo 1 to 6 are all your possible\noutcomes this represents.\nSample space event is\none or more outcome\nof an experiment.\nSo in this case\nmy event is to get a to\nwhen I roll a dice, right?\nSo my event is the probability\nof getting a to\nwhen I roll a dice.\nSo guys, this is basically what\nrandom experiment sample space\nand event really means alright.\nNow, let's discuss\nthe different types of events.\nThere are two types of events\nthat you should know about there\nis disjoint and non disjoint\nevents disjoint events.\nThese are events\nthat do not have\nany common outcome.\nFor example,\nif you draw a single card\nfrom a deck of cards,\nit cannot be a king\nand a queen correct.\nIt can either be king\nor it can be Queen.\nNow a non disjoint\nevents are events\nthat have common outcomes.\nFor example, a student\ncan get hundred marks\nin statistics and hundred\nmarks in probability.\nAll right, and also the outcome\nof a ball delibird\ncan be a no ball\nand it can be a 6 right.\nSo this is Non\ndisjoint events are or n.\nThese are very simple\nto understand right now.\nLet's move on and look\nat the different types\nof probability distribution.\nAll right, I'll be discussing\nthe three main probability\ndistribution functions.\nI'll be talking\nabout probability density\nfunction normal distribution\nand Central limit theorem.\nOkay probability density\nfunction also known\nas PDF is concerned\nwith the relative likelihood for\na continuous random variable.\nTo take on a given value.\nAll right.\nSo the PDF gives the probability\nof a variable that lies\nbetween the range A and B.\nSo basically what you're trying\nto do is you're going to try\nand find the probability\nof a continuous random variable\nover a specified range.\nOkay.\nNow this graph denotes the PDF\nof a continuous variable.\nNow, this graph is also known\nas the bell curve right?\nIt's famously called\nthe bell curve because of\nits shape and there are\nthree important properties\nthat you To know about\na probability density function.\nNow the graph of a PDF\nwill be continuous over a range.\nThis is because you're\nfinding the probability\nthat a continuous variable lies\nbetween the ranges A and B,\nright the second property is\nthat the area bounded by\nthe curve of a density function\nand the x-axis is equal\nto 1 basically the area\nbelow the curve is equal\nto 1 all right,\nbecause it denotes\nprobability again\nthe probability cannot arrange.\nMore than one it has to be\nbetween 0 and 1 property number\nthree is that the probability\nthat our random variable\nassumes a value between A\nand B is equal to the area\nunder the PDF bounded\nby A and B. Okay.\nNow what this means is\nthat the probability value\nis denoted by the area\nof the graph.\nAll right, so whatever value\nthat you get here,\nwhich basically one\nis the probability\nthat a random variable will lie\nbetween the range A and B.\nAll right, so I hope\nIf you have understood the\nprobability density function,\nit's basically the probability\nof finding the value\nof a continuous random variable\nbetween the range A and B.\nAll right.\nNow, let's look\nat our next distribution,\nwhich is normal distribution\nnow normal distribution,\nwhich is also known as\nthe gaussian distribution is\na probability distribution\nthat denotes the\nsymmetric property\nof the mean right meaning\nthat the idea\nbehind this function is\nthat The data near the mean\noccurs more frequently than\nthe data away from the mean.\nSo what it means to say is\nthat the data around the mean\nrepresents the entire data set.\nOkay.\nSo if you just take\na sample of data\naround the mean it can represent\nthe entire data set now similar\nto the probability density\nfunction the normal distribution\nappears as a bell curve.\nAll right.\nNow when it comes\nto normal distribution,\nthere are two important factors.\nAll right, we have the mean\nof the population.\nAnd the standard deviation.\nOkay, so the mean and the graph\ndetermines the location\nof the center of the graph,\nright and the standard deviation\ndetermines the height\nof the graph.\nOkay.\nSo if the standard deviation\nis large the curve is going\nto look something like this.\nAll right, it'll be\nshort and wide and\nif the standard deviation\nis small the curve\nis tall and narrow.\nAll right.\nSo this was it\nabout normal distribution.\nNow, let's look\nat the central limit theorem.\nNow the central\nlimit theorem states\nthat the sampling distribution\nof the mean of any independent\nrandom variable will be normal\nor nearly normal\nif the sample size\nis large enough now,\nthat's a little confusing.\nOkay.\nLet me break it down for\nyou now in simple terms\nif we had a large population\nand we divided it\ninto many samples.\nThen the mean of all the samples\nfrom the population\nwill be almost equal\nto the mean of the entire\npopulation right meaning\nthat each of the sample\nis normally distributed.\nRight.\nSo if you compare the mean\nof each of the sample,\nit will almost be equal\nto the mean of the population.\nRight?\nSo this graph basically shows\na more clear understanding\nof the central limit theorem red\nyou can see each sample here\nand the mean\nof each sample is almost\nalong the same line, right?\nOkay.\nSo this is exactly\nwhat the central limit theorem\nStates now the accuracy\nor the resemblance\nto the normal distribution\ndepends on two main factors.\nRight.\nSo the first is the number\nof sample points\nthat you consider.\nAll right,\nand the second is a shape\nof the underlying population.\nNow the shape obviously depends\non the standard deviation\nand the mean\nof a sample, correct.\nSo guys the central\nlimit theorem basically states\nthat each sample\nwill be normally distributed\nin such a way\nthat the mean of each sample\nwill coincide with the mean\nof the actual population.\nAll right in short terms.\nThat's what central\nlimit theorem States.\nAlright, and this\nholds true only for a large.\nIs it mostly\nfor a small data set\nand there are more deviations\nwhen compared to a large\ndata set is because of\nthe scaling Factor, right?\nThe small is deviation\nin a small data set will change\nthe value very drastically,\nbut in a large data\nset a small deviation\nwill not matter at all.\nNow, let's move on and look\nat our next topic\nwhich is the different\ntypes of probability.\nNow, this is a important topic\nbecause most of your problems\ncan be solved by understanding\nwhich type of probability\nshould I use to solve?\nThis problem right?\nSo we have three important\ntypes of probability.\nWe have marginal joint\nand conditional probability.\nSo let's discuss each\nof these now the probability of\nan event occurring unconditioned\non any other event is known\nas marginal probability\nor unconditional probability.\nSo let's say that you want\nto find the probability\nthat a card drawn is a heart.\nAll right.\nSo if you want to\nfind the probability\nthat a card drawn is\na heart the prophet.\nB13 by 52 since there\nare 52 cards in a deck\nand there are 13 hearts\nin a deck of cards.\nRight and there are\n52 cards in a turtleneck.\nSo your marginal probability\nwill be 13 by 52.\nThat's about\nmarginal probability.\nNow, let's understand.\nWhat is joint probability.\nNow joint probability\nis a measure of two events\nhappening at the same time.\nOkay.\nLet's say that the two\nevents are A and B.\nSo the probability of event A\nand B occurring is\nthe dissection of A and B.\nSo for example,\nif you want to\nfind the probability\nthat a card is a four and a red\nthat would be joint probability.\nAll right, because\nyou're finding a card\nthat is 4 and the card\nhas to be red in color.\nSo for the answer,\nthis will be 2 by 52\nbecause we have 1/2\nin heart and we have 1/2\nand diamonds correct.\nSo both of these are red\nand color therefore.\nOur probability is to by 52\nand if you further down\nit Is 1 by 26, right?\nSo this is what\njoint probability is all\nabout moving on.\nLet's look at what exactly\nconditional probability is.\nSo if the probability\nof an event or an outcome\nis based on the occurrence\nof a previous event\nor an outcome,\nthen you call it as\na conditional probability.\nOkay.\nSo the conditional probability\nof an event B is the probability\nthat the event will occur given\nthat an event a has\nalready occurred, right?\nSo if a and b are\ndependent events,\nthen the expression\nfor conditional probability\nis given by this.\nNow this first term\non the left hand side,\nwhich is p b of a is\nbasically the probability\nof event B occurring\ngiven that event a\nhas already occurred.\nAll right.\nSo like I said,\nif a and b are dependent events,\nthen this is\nthe expression but if a\nand b are independent events,\nand the expression\nfor conditional probability is\nlike this, right?\nSo guys P of A and B of B is\nobviously the probability of A\nand probability of B right now.\nLet's move on now in order\nto understand conditional\nprobability joint probability\nand marginal probability.\nLet's look at a small use case.\nOkay now basically\nwe're going to take a data set\nwhich examines the salary\npackage and training\nundergone my candidates.\nOkay.\nNow in this there are\n60 candidates without training\nand forty five candidates,\nwhich have enrolled for\nAdder a curse training.\nRight.\nNow the task here is you have\nto assess the training\nwith a salary package.\nOkay, let's look at this\nin a little more depth.\nSo in total,\nwe have hundred and five\ncandidates out of which 60\nof them have not enrolled\nFrederick has training\nand 45 of them have enrolled\nfor a deer Acres training\nor this is a small survey\nthat was conducted\nand this is the rating\nof the package or the salary\nthat they got right?\nSo if you read through the data,\nyou can understand\nthere were five candidates.\nIt's without education\nor training who got a very\npoor salary package.\nOkay.\nSimilarly, there are\n30 candidates with\nEd Eureka training\nwho got a good package, right?\nSo guys basically you're\ncomparing the salary package\nof a person depending on\nwhether or not they've enrolled\nfor a director training, right?\nThis is our data set.\nNow, let's look at our problem\nstatement find the probability\nthat a candidate\nhas undergone a Drake\nhas training quite simple,\nwhich type of probability\nis this Is this is\nmarginal probability?\nRight?\nSo the probability\nthat a candidate has undergone\nedger Acres training is\nobviously 45 divided\nby a hundred and five\nsince 45 is the number\nof candidates with\nEddie record raining\nand hundred and five is\nthe total number of candidates.\nSo you get a value\nof approximately 0.4\nto all right,\nthat's the probability\nof a candidate\nthat has undergone educate\na girl straining next question\nfind the probability\nthat a candidate has attended\nedger Acres training.\nAlso has good package.\nNow.\nThis is obviously a joint\nprobability problem, right?\nSo how do you\ncalculate this now?\nSince our table is quite\nformatted we can directly find\nthat people who have\ngotten a good package\nalong with Eddie record\nraining or 30, right?\nSo out of hundred and\nfive people 30 people\nhave education training\nand a good package, right?\nThey specifically\nasking for people\nwith Eddie record raining.\nRemember that night.\nThe question is find the\nprobability that a gang Today,\nit has attended\neditor Acres training\nand also has a good package.\nAll right, so we need\nto consider two factors\nthat is a candidate\nwho's addenda deaderick\nhas training and\nwho has a good package.\nSo clearly that number\nis 30 30 divided by\ntotal number of candidates,\nwhich is 1:05, right?\nSo here you get\nthe answer clearly next.\nWe have find the probability\nthat a candidate has\na good package given\nthat he has not\nundergone training.\nOkay.\nNow this is Early\nconditional probability\nbecause here you're defining\na condition you're saying\nthat you want to find\nthe probability of a candidate\nwho has a good package given\nthat he's not undergone.\nAny training, right?\nThe condition is that he's\nnot undergone any training.\nAll right.\nSo the number of people\nwho have not undergone\ntraining are 60 and out\nof that five of them\nhave got a good package\nthat so that's why this is Phi\nby 60 and not five\nby a hundred and five\nbecause here they have clearly\nmentioned has a good pack.\nGiven that he has\nnot undergone training.\nSo you have to only\nconsider people who have\nnot undergone training, right?\nSo any five people\nwho have not undergone\ntraining have gotten\na good package, right?\nSo 5 divided by 60 you get\na probability of around 0.08\nwhich is pretty low, right?\nOkay.\nSo this was all\nabout the different types\nof probability now,\nlet's move on and look at\nour last Topic in probability,\nwhich is base theorem.\nNow guys base.\nYour room is a very\nimportant concept when it comes\nto statistics and probability.\nIt is majorly used\nin knife bias algorithm.\nThose of you who aren't aware.\nNow I've bias is a supervised\nlearning classification\nalgorithm and it is mainly used\nin Gmail spam filtering right?\nA lot of you might have noticed\nthat if you open up Gmail,\nyou'll see that you have\na folder called spam right\nor that is carried out\nthrough machine learning\nand And the algorithm use\nthere is knife bias, right?\nSo now let's discuss what\nexactly the Bayes theorem is\nand what it denotes\nthe bias theorem is used\nto show the relation between\none conditional probability\nand it's inverse.\nAll right.\nBasically it's nothing\nbut the probability\nof an event occurring based\non prior knowledge of conditions\nthat might be related\nto the same event.\nOkay.\nSo mathematically the bell's\ntheorem is represented\nlike this right now.\nShown in this equation.\nThe left-hand term is referred\nto as the likelihood ratio\nwhich measures the probability\nof occurrence of event\nbe given an event a okay\non the left hand side is\nwhat is known as\nthe posterior right\nis referred to as posterior,\nwhich means that the probability\nof occurrence of a given\nan event be right.\nThe second term is referred\nto as the likelihood Ratio or at\nthis measures the probability\nof occurrence of B\ngiven an event.\nA now P of a is also\nknown as the prior\nwhich refers to the actual\nprobability distribution of A\nand P of B is again,\nthe probability of B, right.\nThis is the bias theorem\nand in order to better\nunderstand the base theorem.\nLet's look at a small example.\nLet's say that we have\nthree bowels we have bow is\na bow will be and bouncy.\nOkay barley contains\ntwo blue balls\nand for red balls bowel B\ncontains eight blue\nballs and for red balls.\nWow Zeke.\nGames one blue ball\nand three red balls now\nif we draw one ball\nfrom each Bowl,\nwhat is the probability\nto draw a blue ball\nfrom a bowel a if we know\nthat we drew exactly a total\nof two blue balls, right?\nIf you didn't\nunderstand the question,\nplease read it I shall pause\nfor a second or two.\nRight.\nSo I hope all of you\nhave understood the question.\nOkay.\nNow what I'm going to do\nis I'm going to draw\na blueprint for you\nand tell you how exactly\nto solve the problem.\nBut I want you all to give\nme the solution\nto this problem, right?\nI'll draw a blueprint.\nI'll tell you\nwhat exactly the steps are\nbut I want you to come\nup with a solution\non your own right the formula\nis also given to you.\nEverything is given to you.\nAll you have to do is come up\nwith the final answer.\nRight?\nLet's look at how you\ncan solve this problem.\nSo first of all,\nwhat we will do is\nLet's consider a all right,\nlet a be the event\nof picking a blue ball\nfrom bag in and let\nX be the event of picking\nexactly two blue balls,\nright because these\nare the two events\nthat we need to calculate\nthe probability of now\nthere are two probabilities\nthat you need to consider here.\nOne is the event of picking\na blue ball from bag a\nand the other is the event of\npicking exactly two blue balls.\nOkay.\nSo these two are represented\nby a and X respectively\nand so what we want is\nthe probability of occurrence\nof event a given X,\nwhich means that given\nthat we're picking\nexactly two blue balls.\nWhat is the probability\nthat we are picking\na blue ball from bag?\nSo by the definition\nof conditional probability,\nthis is exactly what\nour equation will look like.\nCorrect.\nThis is basically a occurrence\nof event a given element X\nand this is\nthe probability of a and x\nand this is the probability\nof X alone, correct.\nWhat we need to do is we need\nto find these two probabilities\nwhich is probability of a\nand X occurring together\nand probability of X. Okay.\nThis is the entire solution.\nSo how do you find P probability\nof X this you can do\nin three ways.\nSo first is white ball\nfrom a either white from be\nor read from see now first is\nto find the probability of x x\nbasically represents the event\nof picking exactly\ntwo blue balls.\nRight.\nSo these are the three ways\nin which it is possible.\nSo you'll pick one blue ball\nfrom bowel a and one from bowel\nbe in the second case.\nYou can pick one\nfrom a and another blue ball\nfrom see in the third case.\nYou can pick a blue\nball from Bagby\nand a blue ball from bagsy.\nRight?\nThese are the three ways\nin which it is possible.\nSo you need to find\nthe probability of each\nof this step do is\nthat you need to find\nthe probability of a\nand X occurring together.\nThis is the sum\nof terms one and two.\nOkay, this is\nbecause in both\nof these events,\nyou're picking a ball\nfrom bag, correct?\nSo there is find out\nthis probability and let\nme know your answer\nin the comment section.\nAll right.\nWe'll see if you get\nthe answer right?\nI gave you the entire\nsolution to this.\nAll you have to do is\nsubstitute the value right?\nIf you want a second or two,\nI'm going to pause on the screen\nso that you can go through this\nin a more clearer way right?\nRemember that you need\nto calculate two.\nHe's the first probability\nthat you need to calculate is\nthe event of picking a blue ball\nfrom bag a given\nthat you're picking\nexactly two blue balls.\nOkay, II probability you need\nto calculate is the event\nof picking exactly to bluebirds.\nAll right.\nThese are the two probabilities.\nYou need to calculate so\nremember that and this\nis the solution.\nAll right, so guys,\nmake sure you\nmention your answers\nin the comment section for now.\nLet's move on and Get\nour next topic,\nwhich is the\ninferential statistics.\nSo guys, we just completed the\nprobability module right now.\nWe will discuss\ninferential statistics,\nwhich is the second\ntype of Statistics.\nWe discussed descriptive\nstatistics earlier.\nAll right.\nSo like I mentioned earlier\ninferential statistics also\nknown as statistical inference\nis a branch of Statistics\nthat deals with forming\ninferences and predictions\nabout a population based\non a sample of data.\nTaken from the population.\nAll right, and the question\nyou should ask is\nhow does one form inferences\nor predictions on a sample?\nThe answer is you\nuse Point estimation?\nOkay.\nNow you must be wondering\nwhat is point estimation\none estimation is concerned\nwith the use of the sample data\nto measure a single value\nwhich serves as\nan approximate value\nor the best estimate of\nan unknown population parameter.\nThat's a little confusing.\nLet me break it down\nto you for Camping\nin order to calculate the mean\nof a huge population.\nWhat we do is we first draw out\nthe sample of the population\nand then we find the sample mean\nright the sample mean\nis then used to estimate\nthe population mean this is\nbasically Point estimate,\nyou're estimating the value\nof one of the parameters\nof the population, right?\nBasically the main\nyou're trying to estimate\nthe value of the mean.\nThis is what point estimation is\nthe two main terms\nin point estimation.\nThere's something known as\nas the estimator\nand the something known\nas the estimate estimator\nis a function of the sample\nthat is used to find\nout the estimate.\nAlright in this example.\nIt's basically the sample\nmean right so a function\nthat calculates the sample\nmean is known as the estimator\nand the realized value\nof the estimator is\nthe estimate right?\nSo I hope Point\nestimation is clear.\nNow, how do you\nfind the estimates?\nThere are four common ways\nin which you can do this.\nThe first one is\nmethod of Moment yo,\nwhat you do is\nyou form an equation\nin the sample data set\nand then you analyze\nthe similar equation\nin the population data set\nas well like the population mean\npopulation variance and so on.\nSo in simple terms,\nwhat you're doing is you're\ntaking down some known facts\nabout the population\nand you're extending\nthose ideas to the sample.\nAlright, once you do that,\nyou can analyze the sample\nand estimate more\nessential or more complex\nvalues right next.\nWe have maximum likelihood.\nThis method basically uses\na model to estimate a value.\nAll right.\nNow a maximum likelihood\nis majorly based on probability.\nSo there's a lot of probability\ninvolved in this method next.\nWe have the base estimator\nthis works by minimizing\nthe errors or the average risk.\nOkay, the base estimator\nhas a lot to do\nwith the Bayes theorem.\nAll right, let's\nnot get into the depth\nof these estimation methods.\nFinally.\nWe have the best unbiased\nestimators in this method.\nThere are seven unbiased\nestimators that can be used\nto approximate a parameter.\nOkay.\nSo Guys these were\na couple of methods\nthat are used\nto find the estimate\nbut the most well-known method\nto find the estimate is known as\nthe interval estimation.\nOkay.\nThis is one\nof the most important\nestimation methods right?\nThis is where confidence\ninterval also comes\ninto the picture right apart\nfrom interval estimation.\nWe also have something\nknown as margin of error.\nSo I'll be discussing\nall of this.\nIn the upcoming slides.\nSo first let's understand.\nWhat is interval estimate?\nOkay, an interval\nor range of values,\nwhich are used to estimate a\npopulation parameter is known as\nan interval estimation, right?\nThat's very understandable.\nBasically what they're trying to\nsee is you're going to estimate\nthe value of a parameter.\nLet's say you're trying to find\nthe mean of a population.\nWhat you're going to do is\nyou're going to build a range\nand your value will lie in\nthat range or in that interval.\nAlright, so this way your output\nis going to be more accurate\nbecause you've not predicted\na point estimation instead.\nYou have estimated an interval\nwithin which your value\nmight occur, right?\nOkay.\nNow this image clearly shows\nhow Point estimate and interval\nestimate or different\nso guys interval estimate\nis obviously more accurate\nbecause you are not just\nfocusing on a particular value\nor a particular point\nin order to predict\nthe probability instead.\nYou're saying that\nthe value might be\nwithin this range between\nthe lower confidence limit\nand the upper confidence limit.\nAll right, this is denotes\nthe range or the interval.\nOkay, if you're still confused\nabout interval estimation,\nlet me give you a small example\nif I stated that I will take\n30 minutes to reach the theater.\nThis is known\nas Point estimation.\nOkay, but if I stated\nthat I will take\nbetween 45 minutes\nto an hour to reach the theater.\nThis is an example\nof into Estimation.\nAll right.\nI hope it's clear.\nNow now interval estimation\ngives rise to two important\nstatistical terminologies one\nis known as confidence interval\nand the other is known\nas margin of error.\nAll right.\nSo there's it's important\nthat you pay attention\nto both of these terminologies\nconfidence interval is one\nof the most significant measures\nthat are used to check\nhow essential machine\nlearning model is.\nAll right.\nSo what is confidence interval\nconfidence interval is\nthe measure of your confidence\nthat the interval\nestimated contains\nthe population parameter\nor the population mean\nor any of those parameters\nright now statisticians\nuse confidence interval\nto describe the amount\nof uncertainty associated\nwith the sample estimate of\na population parameter now guys,\nthis is a lot of definition.\nLet me just make you\nunderstand confidence interval\nwith a small example.\nOkay.\nLet's say that you perform\na survey and you survey\na group of cat owners.\nThe see how many cans\nof cat food they purchase\nin one year.\nOkay, you test\nyour statistics at the 99\npercent confidence level\nand you get\na confidence interval\nof hundred comma 200 this means\nthat you think\nthat the cat owners\nby between hundred to two\nhundred cans in a year and also\nsince the confidence\nlevel is 99% shows\nthat you're very confident\nthat the results are, correct.\nOkay.\nI hope all of you\nare clear with that.\nAlright, so your confidence\ninterval here will be\na hundred and two hundred\nand your confidence level\nwill be 99% Right?\nThat's the difference\nbetween confidence interval\nand confidence level So\nwithin your confidence interval\nyour value is going to lie and\nyour confidence level will show\nhow confident you are\nabout your estimation, right?\nI hope that was clear.\nLet's look at margin of error.\nNo margin of error\nfor a given level of confidence\nis a greatest possible distance\nbetween the Point estimate\nand the value of the parameter\nthat it is estimating\nyou can say\nthat it is a deviation from\nthe actual point estimate right.\nNow.\nThe margin of error\ncan be calculated\nusing this formula now zc\nher denotes the critical value\nor the confidence interval\nand this is X standard\ndeviation divided by root\nof the sample size.\nAll right, n is basically\nthe sample size now,\nlet's understand how\nyou can estimate\nthe confidence intervals.\nSo guys the level of confidence\nwhich is denoted by\nC is the probability\nthat the interval estimate\ncontains a population parameter.\nLet's say that you're trying\nto estimate the mean.\nAll right.\nSo the level of confidence\nis the probability\nthat the interval\nestimate contains\nthe population parameter.\nSo this interval\nbetween minus Z and z\nor the area beneath this curve\nis nothing but the probability\nthat the interval estimate\ncontains a population parameter.\nYou don't all right.\nIt should basically\ncontain the value\nthat you are predicting right.\nNow.\nThese are known\nas critical values.\nThis is basically\nyour lower limit\nand your higher\nlimit confidence level.\nAlso, there's something\nknown as the Z score now.\nThis court can be calculated by\nusing the standard normal table.\nAll right, if you look\nit up anywhere on Google\nyou'll find the z-score table\nor the standard normal\ntable to understand\nhow this is done.\nLet's look at a small example.\nOkay, let's say\nthat the level of confidence.\nVince is 90% This means\nthat you are 90% confident\nthat the interval contains\nthe population mean.\nOkay, so the remaining 10%\nwhich is out of hundred percent.\nThe remaining 10%\nis equally distributed\non these tail regions.\nOkay, so you have 0.05 here\nand 0.05 over here, right?\nSo on either side\nof see you will distribute\nthe other leftover percentage\nnow these Z scores\nare calculated from the table\nas I mentioned before.\nAll right one.\nI'm 6 4 5 is get collated\nfrom the standard normal table.\nOkay, so guys how you estimate\nthe level of confidence?\nSo to sum it up.\nLet me tell you the steps that\nare involved in constructing\na confidence interval first.\nYou would start by identifying\na sample statistic.\nOkay.\nThis is the statistic\nthat you will use to estimate\na population parameter.\nThis can be anything\nlike the mean\nof the sample next you\nwill select a confidence level\nnow the confidence level\ndescribes the uncertainty\nof a Sampling method right\nafter that you'll find\nsomething known as the margin\nof error right?\nWe discussed margin\nof error earlier.\nSo you find this based\non the equation\nthat I explained\nin the previous slide,\nthen you'll finally specify\nthe confidence interval.\nAll right.\nNow, let's look\nat a problem statement\nto better understand\nthis concept a random sample\nof 32 textbook prices is taken\nfrom a local College Bookstore.\nThe mean of the sample is so so\nand so and the sample\nstandard deviation is\nThis use a 95% confident level\nand find the margin\nof error for the mean price\nof all text books\nin the bookstore.\nOkay.\nNow, this is a very\nstraightforward question.\nIf you want you can read\nthe question again.\nAll you have to do is you have\nto just substitute the values\ninto the equation.\nAll right, so guys,\nwe know the formula for margin\nof error you take the Z score\nfrom the table.\nAfter that we have deviation\nMadrid's 23.4 for right\nand that's standard deviation\nand n stands for the number\nof samples here.\nThe number of samples is\n32 basically 32 textbooks.\nSo approximately your margin\nof error is going to be\naround 8.1 to this is\na pretty simple question.\nAll right.\nI hope all of you\nunderstood this now\nthat you know,\nthe idea behind\nconfidence interval.\nLet's move ahead to one\nof the most important topics\nin statistical inference,\nwhich is hypothesis\ntesting, right?\nSo Ugly statisticians\nuse hypothesis testing\nto formally check\nwhether the hypothesis\nis accepted or rejected.\nOkay, hypothesis.\nTesting is an inferential\nstatistical technique\nused to determine\nwhether there is enough evidence\nin a data sample to infer\nthat a certain condition holds\ntrue for an entire population.\nSo to understand\nthe characteristics\nof a general population,\nwe take a random sample,\nand we analyze the properties\nof the sample right we test.\nWhether or not the identified\nconclusion represents\nthe population accurately\nand finally we interpret\nthe results now\nwhether or not to accept\nthe hypothesis depends\nupon the percentage value\nthat we get from the hypothesis.\nOkay, so to\nbetter understand this,\nlet's look at a small\nexample before that.\nThere are a few steps\nthat are followed\nin hypothesis testing you begin\nby stating the null\nand the alternative hypothesis.\nAll right.\nI'll tell you what\nexactly these terms are\nand then you formulate.\nAnalysis plan right after that\nyou analyze the sample data\nand finally you can\ninterpret the results\nright now to understand\nthe entire hypothesis testing.\nWe look at a good example.\nOkay now consider\nfor boys Nick jean-bob\nand Harry these boys\nwere caught bunking a class\nand they were asked\nto stay back at school\nand clean the classroom\nas a punishment, right?\nSo what John did is he decided\nthat four of them would take\nturns to clean their classrooms.\nHe came up with a plan\nof writing each\nof their names on chits\nand putting them\nin a bowl now every day they had\nto pick up a name from the bowel\nand that person had to play\nin the clock, right?\nThat sounds pretty fair\nenough now it is been three days\nand everybody's name has come up\nexcept John's assuming\nthat this event\nis completely random\nand free of bias.\nWhat is a probability\nof John not cheating\nright or is the probability\nthat he's not actually\ncheating this can Solved\nby using hypothesis testing.\nOkay.\nSo we'll Begin by calculating\nthe probability of John\nnot being picked for a day.\nAlright, so we're\ngoing to assume\nthat the event is free of bias.\nSo we need to find\nout the probability\nof John not cheating right first\nwe will find the probability\nthat John is not picked\nfor a day, right?\nWe get 3 out of 4,\nwhich is basically 75%\n75% is fairly high.\nSo if John is not picked\nfor three days in a row\nthe Probability will drop down\nto approximately 42% Okay.\nSo three days in a row meaning\nthat is the probability\ndrops down to 42 percent.\nNow, let's consider a situation\nwhere John is not picked\nfor 12 days in a row\nthe probability drops down\nto three point two percent.\nOkay.\nThat's the probability\nof John cheating becomes\nfairly high, right?\nSo in order\nfor statisticians to come\nto a conclusion,\nthey Define what is known\nas a threshold value.\nRight considering\nthe above situation\nif the threshold value\nis set to 5 percent.\nIt would indicate\nthat if the probability lies\nbelow 5% then John is cheating\nhis way out of detention.\nBut if the probability is\nabout threshold value then John\nit just lucky and his name\nisn't getting picked.\nSo the probability\nand hypothesis testing give rise\nto two important components\nof hypothesis testing,\nwhich is null hypothesis\nand alternative hypothesis.\nNull.\nHypothesis is based.\nBasically approving\nthe Assumption alternate\nhypothesis is\nwhen your result disapproves\nthe Assumption right therefore\nin our example,\nif the probability\nof an event occurring\nis less than 5% which it is\nthen the event is biased hence.\nIt proves the\nalternate hypothesis.\nSo guys with this we come\nto the end of this session.\nLet's go ahead\nand understand what exactly is.\nWas learning so\nsupervised learning is\nwhere you have\nthe input variable X\nand the output variable Y\nand use an algorithm\nto learn the map Egg function\nfrom the input to the output\nas I mentioned earlier\nwith the example\nof face detection.\nSo it is called\nsupervised learning\nbecause the process\nof an algorithm learning\nfrom the training data\nset can be thought\nof as a teacher supervising\nthe learning process.\nSo if we have a look\nat the supervised learning steps\nor What would rather\nsay the workflow?\nSo the model is used\nas you can see here.\nWe have the historic data.\nThen we again we have\nthe random sampling.\nWe split the data\ninto train your asset\nand the testing data set using\nthe training data set.\nWe with the help\nof machine learning\nwhich is supervised\nmachine learning.\nWe create statistical model then\nafter we have a mod\nwhich is being generated\nwith the help\nof the training data set.\nWhat we do is use\nthe testing data set\nfor production and testing.\nWhat we do is get the output\nand finally we have\nthe model validation outcome.\nThat was the\ntraining and testing.\nSo if we have a look\nat the prediction part\nof any particular supervised\nlearning algorithm,\nso the model is used\nfor operating outcome\nof a new data set.\nSo whenever performance\nof the model degraded\nthe model is retrained\nor if there are\nany performance issues,\nthe model is retained with\nthe help of the new data now\nwhen we talk about supervisor\nin there not just one\nbut quite a few algorithms here.\nSo we have linear\nregression logistic regression.\nThis is entry.\nWe have random Forest.\nWe have made by classifiers.\nSo linear regression is used\nto estimate real values.\nFor example, the cost of houses.\nThe number of calls\nthe total sales based\non the continuous variables.\nSo that is what\nreading regression is.\nNow when we talk\nabout logistic regression,\nwhich is used to estimate\ndiscrete values, for example,\nwhich are binary values\nlike 0 and 1 yes,\nor no true.\nFalse based on the given set\nof independent variables.\nSo for example,\nwhen you are talking\nabout something like the chances\nof winning or if you talk\nabout winning which can be\neither true or false\nif will it rain today\nwith it can be the yes or no,\nso it cannot be\nlike when the output\nof a particular algorithm\nor the particular\nquestion is either.\nYes.\nNo or Banner e then\nonly we use a large\nstick regression the next\nwe have decision trees.\nSo now these are used for\nclassification problems it work.\nX for both\ncategorical and continuous\ndependent variables and\nif we talk about random Forest\nSo Random Forest is an M symbol\nof a decision tree,\nit gives better prediction\naccuracy than decision tree.\nSo that is another type of\nsupervised learning algorithm.\nAnd finally we have\nthe need based classifier.\nSo it was\na classification technique based\non the Bayes theorem\nwith an assumption of\nIndependence between predictors.\nA linear regression is one\nof the easiest algorithm\nin machine learning.\nIt is a statistical model\nthat attempts to\nshow the relationship\nbetween two variables\nwith a linear equation.\nBut before we drill down\nto linear regression\nalgorithm in depth,\nI'll give you a quick overview\nof today's agenda.\nSo we'll start a session\nwith a quick overview\nof what is regression\nas linear regression\nis one of a type\nof regression algorithm.\nOnce we learn about regression,\nits use case the various\ntypes of it next.\nWe'll learn about\nthe algorithm from scratch.\nEach where I'll teach\nyou it's mathematical\nimplementation first,\nthen we'll drill down\nto the coding part\nand Implement linear\nregression using python\nin today's session will deal\nwith linear regression algorithm\nusing least Square method check\nits goodness of fit\nor how close the data is\nto the fitted regression line\nusing the R square method.\nAnd then finally\nwhat will do will optimize it\nusing the gradient decent method\nin the last part\non the coding session.\nI'll teach you to implement\nlinear regression using Python\nand Coding session\nwould be divided into two parts\nthe first part would consist\nof linear regression\nusing python from scratch\nwhere you will use\nthe mathematical algorithm\nthat you have learned\nin this session.\nAnd in the next part\nof the coding session\nwill be using scikit-learn\nfor direct implementation\nof linear regression.\nSo let's begin our session\nwith what is regression.\nWell regression analysis is\na form of predictive\nmodeling technique\nwhich investigates the\nrelationship between a dependent\nand independent variable\na regression analysis.\nVols graphing a line\nover a set of data points\nthat most closely fits\nthe overall shape of the data\nor regression shows the changes\nin a dependent variable\non the y-axis\nto the changes\nin the explanation variable\non the x-axis fine.\nNow you would ask\nwhat are the uses of regression?\nWell, there are major three uses\nof regression analysis the first\nbeing determining the strength\nof predicates errs,\nthe regression might be used\nto identify the strength\nof the effect\nthat the independent variables\nhave on the dependent variable\nor But you can ask question.\nLike what is the strength\nof relationship between sales\nand marketing spending or what\nis the relationship between age\nand income second is forecasting\nan effect in this the regression\ncan be used to forecast effects\nor impact of changes.\nThat is the regression analysis\nhelp us to understand\nhow much the dependent variable\nchanges with the change\nand one or more\nindependent variable fine.\nFor example, you can ask\nquestion like how much\nadditional say Lancome\nwill I get for each?\nThousand dollars\nspent on marketing.\nSo it is Trend forecasting\nin this the regression\nanalysis predict Trends\nand future values.\nThe regression analysis can\nbe used to get Point estimates\nin this you can ask questions.\nLike what will be\nthe price of Bitcoin\nand next six months, right?\nSo next topic is linear versus\nlogistic regression by now.\nI hope that you know,\nwhat a regression is.\nSo let's move on\nand understand its type.\nSo there are various kinds\nof regression like linear\nregression logistic regression\npolynomial regression.\nOthers only but for this session\nwill be focusing on linear\nand logistic regression.\nSo let's move on and let me tell\nyou what is linear regression.\nAnd what is logistic regression\nthen what we'll do\nwe'll compare both of them.\nAll right.\nSo starting with\nlinear regression\nin simple linear regression,\nwe are interested in things\nlike y equal MX plus C.\nSo what we are trying to find\nis the correlation between X\nand Y variable this means\nthat every value of x\nhas a corresponding\nvalue of y and it\nif it is continuous.\nAll right, however\nin logistic regression,\nwe are not fitting our data\nto a straight line\nlike linear regression instead\nwhat we are doing.\nWe are mapping Y versus X\nto a sigmoid function\nin logistic regression.\nWhat we find out is is y 1 or 0\nfor this particular value of x\nso thus we are essentially\ndeciding true or false value\nfor a given value of x fine.\nSo as a core concept\nof linear regression,\nyou can say that the data\nis modeled using a straight.\nBut in the case\nof logistic regression\nthe data is module using\na sigmoid function.\nThe linear regression is used\nwith continuous variables\non the other hand\nthe logistic regression.\nIt is used with categorical\nvariable the output\nor the prediction\nof a linear regression\nis a value of the variable\non the other hand\nthe output of production\nof a logistic regression\nis the probability\nof occurrence of the event.\nNow, how will you\ncheck the accuracy\nand goodness of fit in case\nof linear regression?\nWe are various methods\nlike measured by loss R square.\nAre adjusted r squared Etc\nwhile in the case\nof logistic regression you\nhave accuracy precision\nrecall F1 score,\nwhich is nothing but\nthe harmonic mean of precision\nand recall next is Roc curve\nfor determining the probability\nthreshold for classification\nor the confusion Matrix Etc.\nThere are many all right.\nSo summarizing the difference\nbetween linear and\nlogistic regression.\nYou can say\nthat the type of function you\nare mapping to is the main point\nof difference between linear\nand logistic regression\na linear regression model.\nThe Continuous X2 a continuous\nfile on the other hand\na logistic regression\nMaps a continuous x\nto the bindery why\nso we can use logistic\nregression to make category\nor true false decisions\nfrom the data find\nso let's move\non ahead next is linear\nregression selection criteria,\nor you can say when will\nyou use linear regression?\nSo the first is classification\nand regression capabilities\nregression models predict\na continuous variable such as\nthe sales made on a day\nor predict the temperature\nof a city T their Reliance\non a polynomial\nlike a straight line\nto fit a data set\nposes a real challenge\nwhen it comes towards building\na classification capability.\nLet's imagine that you fit\na line with a train points\nthat you have now imagine you\nadd some more data points to it.\nBut in order to fit it,\nwhat do you have to do?\nYou have to change\nyour existing model\nthat is maybe you have\nto change the threshold itself.\nSo this will happen\nwith each new data point you are\nto the model hence.\nThe linear regression is not\ngood for classification models.\nFine.\nNext is data quality.\nEach missing value\nremoves one data point\nthat could optimize\nthe regression and\nsimple linear regression.\nThe outliers can significantly\ndisrupt the outcome\njust for now.\nYou can know that if you\nremove the outliers your model\nwill become very good.\nAll right.\nSo this is about data quality.\nNext is computational complexity\na linear regression is often\nnot computationally expensive as\ncompared to the decision tree\nor the clustering algorithm\nthe order of complexity\nfor n training example\nand X features usually Falls\nin either Big O of x\nOr bigger of xn\nnext is comprehensible\nand transparent the\nlinear regression are\neasily comprehensible\nand transparent in nature.\nThey can be represented by\na simple mathematical notation\nto anyone and can be\nunderstood very easily.\nSo these are some\nof the criteria based\non which you will select\nthe linear regression algorithm.\nAll right.\nNext is where is linear\nregression used first\nis evaluating trans\nand sales estimate.\nWell linear regression\ncan be used in business\nto evaluate Trends\nand make estimates.\nForecast for example,\nif a company sales have\nincreased steadily every month\nfor past few years then\nconducting a linear analysis\non the sales data\nwith monthly sales on the y axis\nand time on the x axis.\nThis will give you a line\nthat predicts the upward Trends\nin the sale after creating\nthe trendline the company\ncould use the slope\nof the lines to focused\nsale in future months.\nNext is analyzing.\nThe impact of price changes\nwill linear regression\ncan be used to analyze\nthe effect of pricing\non Omer behavior for instance,\nif a company changes\nthe price on a certain\nproduct several times,\nthen it can record the quantity\nitself for each price level\nand then perform\na linear regression\nwith sold quantity as\na dependent variable and price\nas the independent variable.\nThis would result in a line\nthat depicts the extent\nto which the customer reduce\ntheir consumption of the product\nas the prices increasing.\nSo this result would help us\nin future pricing decisions.\nNext is assessment of risk\nin financial services\nand insurance domain.\nLinear regression can be used\nto analyze the risk,\nfor example health insurance\ncompany might conduct\na linear regression algorithm\nhow it can do it can do it\nby plotting the number of claims\nper customer against its age\nand they might discover\nthat the old customers\ntend to make more\nhealth insurance claim.\nWell the result\nof such analysis might guide\nimportant business decisions.\nAll right, so by now you\nhave just a rough idea of\nwhat linear regression\nalgorithm as like\nwhat it does where it is used\nwhen You should use it early.\nNow.\nLet's move on\nand understand the algorithm\nand depth so suppose\nyou have independent variable\non the x-axis and dependent\nvariable on the y-axis.\nAll right suppose.\nThis is the data point\non the x axis.\nThe independent variable\nis increasing on the x-axis.\nAnd so does the dependent\nvariable on the y-axis?\nSo what kind of linear\nregression line you would get\nyou would get a positive\nlinear regression line.\nAll right as the slope would\nbe positive next is suppose.\nYou have an independent\nvariable on the X axis\nwhich is increasing\nand on the other hand the\ndependent variable on the y-axis\nthat is decreasing.\nSo what kind of line\nwill you get in that case?\nYou will get\na negative regression line.\nIn this case as the slope\nof the line is negative\nand this particular line\nthat is line of y equal MX\nplus C is a line\nof linear regression\nwhich shows the relationship\nbetween independent variable\nand dependent variable\nand this line is only known\nas line of linear regression.\nOkay.\nSo let's add\nsome data points, too.\nOur graph so these\nare some observation\nor data points on our graph.\nSo let's plot some more.\nOkay.\nNow all our data points\nare plotted now our task is\nto create a regression line\nor the best fit line.\nAll right now\nonce our regression\nline is drawn now,\nit's the task\nof production now suppose.\nThis is our estimated value\nor the predicted value\nand this is our actual value.\nOkay.\nSo what we have to do our main\ngoal is to reduce this error\nthat is to reduce the distance\nbetween the Estimated\nor the predicted value\nand the actual value the best\nfit line would be the one\nwhich had the least error\nor the least difference\nin estimated value\nand the actual value.\nAll right, and other words we\nhave to minimize the error.\nThis was a brief understanding\nof linear regression\nalgorithm soon.\nWe'll jump towards\nmathematical implementation.\nBut for then let me tell you\nthis suppose you draw a graph\nwith speed on the x-axis\nand distance covered\non the y axis with\nthe time domain in constant.\nIf you plot a graph\nbetween the speed travel\nby the vehicle\nand the distance traveled\nin a fixed unit of time,\nthen you will get\na positive relationship.\nAll right.\nSo suppose the equation\nof a line is y equal MX plus C.\nThen in this case Y is\nthe distance traveled\nin a fixed duration of time x\nis the speed of vehicle m\nis the positive slope\nof the line and see is\nthe y-intercept of the line.\nAll right suppose\nthe distance remaining constant.\nYou have to plot a graph\nbetween the speed of the vehicle\nand the time taken\nto travel a fixed distance.\nThen in that case\nyou will get a line\nwith a negative relationship.\nAll right, the slope of the line\nis negative here the equation\nof line changes to y\nequal minus of MX plus C\nwhere Y is the time\ntaken to travel\na fixed distance X is the speed\nof vehicle m is\nthe negative slope\nof the line and see is\nthe y-intercept of the line.\nAll right.\nNow, let's get back\nto our independent\nand dependent variable.\nSo in that term,\nwhy is our dependent variable\nand X that is\nour independent variable now,\nlet's move on.\nAnd see them at the magical\nimplementation of the things.\nAlright, so we have x\nequal 1 2 3 4 5 let's plot\nthem on the x-axis.\nSo 0 1 2 3 4 5 6 align\nand we have y as 3 4 2 4 5.\nAll right.\nSo let's plot 1\n2 3 4 5 on the y-axis now,\nlet's plot our coordinates 1\nby 1 so x equal 1 and y equal 3,\nso we have here x\nequal 1 and y equal\n3 So this is the point\n1 comma 3 so similarly\nwe have 1 3 2 4 3 2 4 4 & 5 5.\nAlright, so moving on ahead.\nLet's calculate the mean of X\nand Y and plot it on the graph.\nAll right, so mean of X is 1\nplus 2 plus 3 plus 4\nplus 5 divided by 5.\nThat is 3.\nAll right, similarly mean\nof Y is 3 plus 4 plus 2\nplus 4 plus 5 that is 18.\nSo we 10 divided by 5.\nThat is nothing but 3.6.\nAlright, so next\nwhat we'll do we'll plot.\nI mean that is 3 comma\n3 .6 on the graph.\nOkay.\nSo there's a point 3 comma 3 .6\nsee our goal is to find\nor predict the best fit line\nusing the least Square\nMethod All right.\nSo in order to find\nthat we first need to find\nthe equation of line,\nso let's find the equation\nof our regression line.\nAlright, so let's suppose\nthis is our regression line\ny equal MX plus C.\nNow.\nWe have an equation of line.\nSo all we need to do\nis find the value of M\nand C. I wear m equals\nsummation of x\nminus X bar X Y minus y bar\nupon the summation of x\nminus X bar whole Square\ndon't get confused.\nLet me resolve it for you.\nAll right.\nSo moving on ahead\nas a part of formula.\nWhat we are going to do\nwill calculate x minus X bar.\nSo we have X as 1 minus X bar\nas 3 so 1 minus 3\nthat is minus 2 next.\nWe have x equal\nto minus its mean 3\nthat is minus 1\nsimilarly we 3 -\n3 0 4 minus 3 1 5 - 3 2.\nAll right, so x minus X bar.\nIt's nothing but the distance\nof all the point\nthrough the line y equal 3\nand what does this y\nminus y bar implies\nit implies the distance\nof all the point from the line\nx equal 3 .6 fine.\nSo let's calculate the value\nof y minus y bar.\nSo starting with y equal 3 -\nvalue of y bar\nthat is 3.6.\nSo it is three minus three. .6.\nHow much -\nof 0.6 next is 4 minus 3.6\nthat is 0.4 next to minus 3.6\nthat is - of 1.6.\nNext is 4 minus 3.6\nthat is 0.4 again,\n5 minus 3.6 that is 1.4.\nAlright, so now we are done\nwith Y minus y bar fine now next\nwe will calculate x\nminus X bar whole Square.\nSo let's calculate x\nminus X bar whole Square\nso it is -\n2 whole square that is\n4 minus 1 whole square.\nThat is 1 0 squared is\n0 1 Square 1 2 square for fine.\nSo now in our table we have x\nminus X bar y minus y bar\nand x minus X bar whole Square.\nNow what we need.\nWe need the product of x\nminus X bar X Y minus y bar.\nAlright, so let's see\nthe product of x\nminus X bar X Y minus\ny bar that is minus\nof 2 x minus of 0.6.\nThat is 1.2 minus\nof 1 x 0 point 4.\nThat is minus.\n- of zero point 4 0 x\nminus of 1.6.\nThat is 0 1 multiplied\nby zero point four\nthat is 0.4.\nAnd next 2 multiplied\nby 1 point for that is 2.8.\nAll right.\nNow almost all the parts\nof our formula is done.\nSo now what we need\nto do is get the summation\nof last two columns.\nAll right, so the summation of x\nminus X bar whole square is 10\nand the summation of x\nminus X bar X Y minus y bar is\nfor So the value of M\nwill be equal to 4 by 10 fine.\nSo let's put this value\nof m equals zero point 4\nand our line y equal MX plus C.\nSo let's file all the points\ninto the equation\nand find the value of C.\nSo we have y as 3.6 remember\nthe mean by m as 0.4\nwhich we calculated just\nnow X as the mean value of x\nthat is 3 and we\nhave the equation as\n3 point 6 equals 0 .4\nApplied by 3 plus C. Alright\nthat is 3.6 equal\n1 Point 2 plus C.\nSo what is the value of C\nthat is 3.6 minus 1.2.\nThat is 2.4.\nAll right.\nSo what we had we had m equals\nzero point four C as 2.4.\nAnd then finally\nwhen we calculate the equation\nof the regression line,\nwhat we get is y equal\nzero point four times of X\nplus two point four.\nSo this is the regression line.\nAll right, so there is\nhow you are plotting\nyour points this Actual point.\nAll right now for given m equals\nzero point four and SQL 2.4.\nLet's predict the value of y\nfor x equal 1 2 3 4 & 5.\nSo when x equal\n1 the predicted value\nof y will be zero point four x\none plus two point\nfour that is 2.8.\nSimilarly when x equal\nto predicted value\nof y will be zero point 4 x\n2 + 2 point 4 that equals\nto 3 point 2 similarly x\nequal 3 y will be 3. .6.\nX equals 4 y will be 4 point 0 x\nequal 5 y will be\nfour point four.\nSo let's plot them on the graph\nand the line passing through\nall these predicting point\nand cutting y-axis at 2.4\nas the line of regression.\nNow your task is to calculate\nthe distance between the actual\nand the predicted value\nand your job is\nto reduce the distance.\nAll right, or in other words,\nyou have to reduce the error\nbetween the actual\nand the predicted value the line\nwith the least error will be\nthe line of linear regression.\nChicken or regression line\nand it will also be\nthe best fit line.\nAll right.\nSo this is how things\nwork in computer.\nSo what it do it performs\nn number of iteration\nfor different values of M\nfor different values of M.\nIt will calculate\nthe equation of line\nwhere y equals MX plus C.\nRight?\nSo as the value\nof M changes the line\nis changing so iteration\nwill start from one.\nAll right, and it will perform\na number of iteration.\nSo after every iteration\nwhat it will do it\nwill calculate the predicted.\nValue according to the line\nand compare the distance\nof actual value\nto the predicted value\nand the value of M\nfor which the distance\nbetween the actual\nand the predicted value is\nminimum will be selected\nas the best fit line.\nAll right.\nNow that we have calculated\nthe best fit line now,\nit's time to check the goodness\nof fit or to check\nhow good a model is performing.\nSo in order to do that,\nwe have a method\ncalled R square method.\nSo what is this R square?\nWell r-squared value is\na statistical measure\nof how close the data are\nto the fitted regression\nline in general.\nIt is considered\nthat a high r-squared\nvalue model is a good model,\nbut you can also have\na lower squared value\nfor a good model as well\nor a higher squared\nvalue for a model\nthat does not fit at all.\nI like it is also known as\ncoefficient of determination\nor the coefficient\nof multiple determination.\nLet's move on and see\nhow a square is calculated.\nSo these are our actual values\nplotted on the graph.\nWe had calculated\nthe predicted values\nof Y as 2.8 3.2 3.6 4.0 4.4.\nRemember when we calculated\nthe predicted values\nof Y for the equation Y\npredicted equals 0 1 4 x\nof X plus two point\nfour for every x\nequal 1 2 3 4 & 5 from there.\nWe got the Ed values\nof Phi all right.\nSo let's plot it on the graph.\nSo these are point\nand the line passing\nthrough these points are nothing\nbut the regression line.\nAll right.\nNow what you need to do is\nyou have to check and compare\nthe distance of actual -\nmean versus the distance\nof predicted - mean alike.\nSo basically what you are doing\nyou are calculating the distance\nof actual value to the mean\nto distance of predicted value\nto the mean I like\nso there is nothing\nbut a square in mathematically\nyou can represent our school.\nWhereas summation of Y\npredicted values minus y\nbar whole Square divided\nby summation of Y minus\ny bar whole Square\nwhere Y is the actual value\ny p is the predicted value\nand Y Bar is the mean value of y\nthat is nothing but 3.6.\nRemember, this is our formula.\nSo next what we'll do\nwe'll calculate y minus.\nY1.\nSo we have y is\n3y bar as 3 point 6,\nso we'll calculate\nit as 3 minus 3.6\nthat is nothing but minus of 0.6\nsimilarly for y equal 4\nand Y Bar equal 3.6.\nWe have y minus y bar as\nzero point 4 then 2 minus 3.6.\nIt is 1 point 6 4 minus\n3.6 again zero point four\nand five minus 3.6\nit is 1.4.\nSo we got the value\nof y minus y bar.\nNow what we have to do we\nhave to take it Square.\nSo we have minus 0.6 Square\nas 0.36 0.4 Square as 0.16 -\nof 1.6 Square as 2.56 0.4 Square\nas 0.16 and 1.4 squared\nis 1.96 now is a part\nof formula what we need.\nWe need our YP\nminus y BAR value.\nSo these are VIP values and we\nhave to subtract it from the No,\nwhy so 2 .8 minus 3.6\nthat is minus 0.8.\nSimilarly.\nWe will get 3.2 minus 3.6\nthat is 0.4 and 3.6 minus 3.6.\nThat is 0 for 1 0 minus\n3.6 that is 0.4.\nThen 4 .4 minus 3.6 that is 0.8.\nSo we calculated the value\nof YP minus y bar now,\nit's our turn to calculate\nthe value of y b minus\ny bar whole Square next.\nWe have -\nof 0.8 Square as 0.64 - of Point\nfour square as 0.160 Square\n0 0 point 4 Square as again 0.16\nand 0.8 Square as 0.64.\nAll right.\nNow as a part of formula\nwhat it suggests it suggests\nme to take the summation of Y P\nminus y bar whole square\nand summation of Y minus\ny bar whole Square.\nAll right.\nLet's see.\nSo in submitting y\nminus y bar whole Square\nwhat you get is five point two\nand summation of Y P minus\ny bar whole Square you\nget one point six.\nSo the value of R square\ncan be calculated as\n1 point 6 upon 5.2 fine.\nSo the result which will get\nis approximately equal to 0.3.\nWell, this is not a good fit.\nAll right, so it suggests\nthat the data points are far\naway from the regression line.\nAlright, so this is\nhow your graph will look\nlike when R square is 0.3\nwhen you increase the value\nof R square to 0.7.\nSo you'll see\nthat the actual value would like\ncloser to the regression line\nwhen it reaches to 0.9 it comes.\nMore clothes and when the value\nof approximately equals\nto 1 then the actual values lies\non the regression line itself,\nfor example, in this case.\nIf you get a very low value\nof R square suppose 0.02.\nSo in that case what will see\nthat the actual values\nare very far away\nfrom the regression line\nor you can say\nthat there are too\nmany outliers in your data.\nYou cannot focus\nand thing from the data.\nAll right.\nSo this was all about the\ncalculation of our Square now,\nyou might get a question\nlike are low values\nof Square always bad.\nWell in some field it\nis entirely expected that I ask\nwhere value will be low.\nFor example any field\nthat attempts to predict human\nbehavior such as psychology\ntypically has r-squared values\nlower than around 50%\nthrough which you can conclude\nthat humans are simply harder\nto predict the under\nphysical process furthermore.\nIf you ask what value is low,\nbut you have statistically\nsignificant predictors,\nthen you can still\ndraw important conclusion\nabout how changes in the\npredicator values associated.\nCreated with the changes\nin the response value regardless\nof the r-squared\nthe significant coefficient\nstill represent the mean change\nin the response for one unit\nof change in the predicator\nwhile holding other predicated\nin the model constant.\nObviously this type\nof information can be\nextremely valuable.\nAll right.\nAll right.\nSo this was all about\nthe theoretical concept now,\nlet's move on to the coding part\nand understand the\ncode in depth.\nSo for implementing\nlinear regression using python,\nI will be using Anaconda\nwith jupyter notebook\ninstalled on it.\nSo I like there's\na jupyter notebook\nand we are using\npython 3.0 on it.\nAlright, so we are going\nto use a data set consisting\nof head size and human brain\nof different people.\nAll right.\nSo let's import our data set\npercent matplotlib and line.\nWe are importing numpy\nas NP pandas as speedy and\nmatplotlib and from matplotlib.\nWe are importing pipe lot\nof that as PLT.\nAlright next we will import\nour data had brain dot CSV\nand store it\nin the database table.\nLet's execute the Run button\nand see the armor.\nBut so this task\nsymbol it symbolizes\nthat it still executing.\nSo there's a output\nour data set consists\nof two thirty seven rows\nand 4 columns.\nWe have columns as\ngender age range head size\nin centimeter Cube\nand brain weights\nand Graham fine.\nSo there's our sample data set.\nThis is how it looks it consists\nof all these data set.\nSo now that we\nhave imported our data,\nso as you can see they are\n237 values in the training set\nso we can find a linear.\nRelationship between the head\nsize and the Brain weights.\nSo now what we'll do\nwe'll collect X & Y\nthe X would consist\nof the head size values\nand the Y would consist\nof brain with values.\nSo collecting X and Y.\nLet's execute the Run.\nDone next what we'll do we\nneed to find the values of b 1\nor B not or you can say m and C.\nSo we'll need the mean of X\nand Y values first of all\nwhat we'll do we'll calculate\nthe mean of X and Y so mean x\nequal NP dot Min X.\nSo mean is a predefined function\nof Numb by similarly mean\nunderscore y equal\nNP dot mean of Y,\nso what it will return\nif you'll return\nthe mean values of Y\nnext we'll check\nthe total number of values.\nSo m equals.\nWell length of X. Alright,\nthen we'll use the formula\nto calculate the values\nof b 1 and B naught or MNC.\nAll right, let's execute\nthe Run button and see\nwhat is the result.\nSo as you can see\nhere on the screen,\nwe have got d 1 as 0 point 2 6 3\nand be not as three twenty\nfive point five seven.\nAlright, so now\nthat we have a coefficient.\nSo comparing it with\nthe equation y equal MX plus C.\nYou can say\nthat brain weight equals\nzero point 2 6 3 X Head size\nplus three twenty five point\nfive seven so you can say\nthat the value of M\nhere is zero point\n2 6 3 and the value of C.\nHere is three twenty\nfive point five seven.\nAll right, so there's\nour linear model now,\nlet's plot it\nand see graphically.\nLet's execute it.\nSo this is how our plot looks\nlike this model is not so bad.\nBut we need to find out\nhow good our model has.\nSo in order to find\nit the many methods\nlike root mean Square method\nthe coefficient of determination\nor the a square method.\nSo in this tutorial,\nI have told you\nabout our score method.\nSo let's focus on that and see\nhow good our model is.\nSo let's calculate\nthe R square value.\nAll right here SS underscore T\nis the total sum of square SS.\nI is the total sum of square\nof residuals and R square\nas the formula is\n1 minus total sum\nof squares upon total sum\nof square of the residuals.\nAll right next\nwhen you execute it,\nyou will get the value\nof R square as 0.63\nwhich is pretty very good.\nNow that you have implemented\nsimple linear regression model\nusing least Square method,\nlet's move on and see\nhow will you implement the model\nusing machine learning library\ncalled scikit-learn.\nAll right.\nSo this scikit-learn\nis a simple machine.\nOwning library in Python welding\nmachine learning model are\nvery easy using scikit-learn.\nSo suppose there's\na python code.\nSo using the scikit-learn\nlibraries your code shortens\nto this length\nlike so let's execute\nthe Run button and see you\nwill get the same our to score.\nSo today we'll be discussing\nlogistic regression.\nSo let's move forward\nand understand the what and by\nof logistic regression.\nNow this algorithm\nis most widely used\nwhen the dependent variable\nor you can see the output is\nin the binary format.\nAnd so here you need\nto predict the outcome\nof a categorical\ndependent variable.\nSo the outcome should be\nalways discreet or categorical\nin nature Now by discrete.\nI mean the value\nshould be binary\nor you can say you just have\ntwo values it can either be 0\nor 1 you can either be yes\nor a no either be true\nor false or high or low.\nSo only these can be\nthe outcomes so the value\nwhich you need to protect\nshould be discrete\nor you can say\ncategorical in nature.\nWhereas in linear regression.\nWe have the value of by\nor you can say the value.\nTwo predictors in a Range\nthat is how there's a difference\nbetween linear regression\nand logistic regression.\nWe must be having question.\nWhy not linear regression now\nguys in linear regression\nthe value of buyer\nor the value which you need\nto predict is in a range,\nbut in our case as\nin the logistic regression,\nwe just have two values\nit can be either 0\nor it can be one.\nIt should not entertain\nthe values which is\nbelow zero or above one.\nBut in linear regression,\nwe have the value of y\nin the range so here in order\nto implement logic regression.\nWe need to clip this This part\nso we don't need the value\nthat is below zero\nor we don't need the value\nwhich is above 1\nso since the value of y will be\nbetween only 0 and 1\nthat is the main rule\nof logistic regression.\nThe linear line has to be\nclipped at zero and one now.\nOnce we clip this graph it\nwould look somewhat like this.\nSo here you are\ngetting the curve\nwhich is nothing but\nthree different straight lines.\nSo here we need to make\na new way to solve this problem.\nSo this has to be\nformulated into equation\nand hence we come up\nwith logistic regression.\nSo here the outcome\nis either 0 or 1.\nWhich is the main rule\nof logistic regression.\nSo with this our resulting curve\ncannot be formulated.\nSo hence our main aim\nto bring the values to 0\nand 1 is fulfilled.\nSo that is how we came up with\nlarge stick regression now here\nonce it gets formulated\ninto an equation.\nIt looks somewhat like this.\nSo guys, this is\nnothing but an S curve\nor you can say the sigmoid curve\na sigmoid function curve.\nSo this sigmoid function\nbasically converts any value\nfrom minus infinity to Infinity\npure discrete values,\nwhich a Logitech regression\nwants or you can say the Values\nwhich are in binary\nformat either 0 or 1.\nSo if you see here\nthe values and either 0\nor 1 and this is nothing\nbut just a transition of it,\nbut guys there's\na catch over here.\nSo let's say I have\na data point that is 0.8.\nNow, how can you decide\nwhether your value is 0\nor 1 now here you\nhave the concept\nof threshold which basically\ndivides your line.\nSo here threshold value\nbasically indicates the\nprobability of either winning\nor losing so here by winning.\nI mean the value is equals to 1.\nAm I losing I mean\nthe values equal to 0\nbut how does it do that?\nLet's have a data point\nwhich is over here.\nLet's say my cursor is at 0.8.\nSo here I check\nwhether this value is less\nthan the threshold value or not.\nLet's say if it is more\nthan the threshold value.\nIt should give me the result\nas 1 if it is less than that,\nthen should give me\nthe result is zero.\nSo here my threshold\nvalue is 0.5.\nI need to Define that\nif my value let's is 0.8.\nIt is a more than 0.5.\nThen the value should\nbe rounded of to 1.\nLet's see if it is\nless than 0.5.\nLet's I have a value 0.2 then\nshould reduce it to zero.\nSo here you can use the concept\nof threshold value\nto find output.\nSo here it should be discreet.\nIt should be either 0\nor it should be one.\nSo I hope you caught this curve\nof logistic regression.\nSo guys, this is\nthe sigmoid S curve.\nSo to make this curve\nwe need to make an equation.\nSo let me address\nthat part as well.\nSo let's see how an equation\nis formed to imitate\nthis functionality so over here,\nwe have an equation\nof a straight line.\nIt is y is equal to MX plus C.\nSo in this case,\nI just have only one independent\nvariable but let's say\nif we have many\nindependent variable then\nthe equation becomes m 1 x\n1 plus m 2 x 2 plus m 3 x\n3 and so on till M NX n now,\nlet us put in B and X.\nSo here the equation\nbecomes Y is equal to b 1 x\n1 plus beta 2 x\n2 plus b 3 x 3 and so on\ntill be nxn plus C.\nSo guys the equation\nof the straight line has a range\nfrom minus infinity to Infinity.\nBut in our case or you can say\nlargest equation the value\nwhich we need to predict\nor you can say the Y value\nit can have the range\nonly from 0 to 1.\nSo in that case we need\nto transform this equation.\nSo to do that what we\nhad done we have just divide\nthe equation by 1 minus y\nso now Y is equal to 0 so 0\nover 1 minus 0\nwhich is equal to 1\nso 0 over 1 is again 0\nand if you take Y is equals\nto 1 then 1 over 1 minus 1\nwhich is 0\nso 1 over 0 is infinity.\nSo here my range is now\nbetween You know to Infinity,\nbut again, we want the range\nfrom minus infinity to Infinity.\nSo for that\nwhat we'll do we'll have\nthe log of this equation.\nSo let's go ahead\nand have the logarithmic\nof this equation.\nSo here we have this transform\nit further to get the range\nbetween minus infinity\nto Infinity so over\nhere we have log of Y\nover 1 minus 1\nand this is your final\nlogistic regression equation.\nSo guys, don't worry.\nYou don't have to write\nthis formula or memorize\nthis formula in Python.\nYou just need to\ncall this function\nwhich is logistic regression\nand everything will be be\nautomatically for you.\nSo I don't want to scare\nyou with the maths\nin the formulas behind it,\nwhich is always good to know\nhow this formula was generated.\nSo I hope you guys are clear\nwith how logistic regression\ncomes into the picture next.\nLet us see what are\nthe major differences\nbetween linear regression was\na logistic regression the first\nof all in linear regression,\nwe have the value\nof y as a continuous variable\nor the variable\nbetween need to predict\nare continuous in nature.\nWhereas in logistic regression.\nWe have the categorical variable\nso here the value\nwhich you need to predict\nshould be Creating nature.\nIt should be either 0\nor 1 or should have\njust two values to it.\nFor example,\nwhether it is raining\nor it is not raining\nis it humid outside\nor it is not humid outside.\nNow, does it going to snow\nand it's not going to snow?\nSo these are the few example,\nwe need to predict\nwhere the values are discrete\nor you can just predict\nwhether this is\nhappening or not.\nNext linear equation solves\nyour regression problems.\nSo here you have a concept\nof independent variable\nand the dependent variable.\nSo here you can calculate\nthe value of y\nwhich you need to predict\nusing the A of X so\nhere your y variable\nor you can see the value\nthat you need to\npredict are in a range.\nBut whereas in\nlogistic regression you\nhave discrete values.\nSo logistic regression basically\nsolves a classification problem\nso it can basically classify it\nand it can just give you result\nwhether this event\nis happening or not.\nSo I hope it is\npretty much Clear till now\nnext in linear equation.\nThe graph that you have seen is\na straight line graph\nso over here,\nyou can calculate the value\nof y with respect to the value\nof x where as in logistic\nregression because of that.\nThe got was a Escobar you\ncan see the sigmoid curve.\nSo using the sigmoid function\nYou can predict\nyour y-values moving the I let\nus see the various use cases\nwhere in logistic regression\nis implemented in real life.\nSo the very first is\nweather prediction now\nlargest aggression helps\nyou to predict your weather.\nFor example, it\nis used to predict\nwhether it is raining\nor not whether it is sunny.\nIs it cloudy or not?\nSo all these things\ncan be predicted\nusing logistic regression.\nWhere as you need\nto keep in mind\nthat both linear regression.\nAnd logistic regression can be\nused in predicting the weather.\nSo in that case linear equation\nhelps you to predict\nwhat will be\nthe temperature tomorrow\nwhereas logistic regression\nwill only tell you\nwhich is going to rain or not\nor whether it's cloudy or not,\nwhich is going to snow or not.\nSo these values are discrete.\nWhereas if you apply\nlinear regression you\nthe predicting things like what\nis the temperature tomorrow\nor what is the temperature\nday after tomorrow\nand all those thing?\nSo these are\nthe slight differences\nbetween linear regression\nand logistic regression\nthe moving ahead.\nWe have classification problem.\nSighs on performs\nmulti-class classification.\nSo here it can help you tell\nwhether it's a bird.\nIt's not a bird.\nThen you classify\ndifferent kind of mammals.\nLet's say whether it's a dog\nor it's not a dog similarly.\nYou can check it for reptile\nwhether it's a reptile\nor not a reptile.\nSo in logistic regression,\nit can perform\nmulti-class classification.\nSo this point\nI've already discussed\nthat it is used\nin classification problems next.\nIt also helps you to determine\nthe illness as well.\nSo let me take an example.\nLet's say a patient goes for\na routine check up in hospital.\nSo what doctor will do it,\nit will perform various tests\non the patient and will check\nwhether the patient is\nactually l or not.\nSo what will be the features\nso doctor can check\nthe sugar level\nthe blood pressure then what\nis the age of the patient?\nIs it very small or is\nit old person then?\nWhat is the previous medical\nhistory of the patient\nand all of these features\nwill be recorded by the doctor\nand finally doctor checks\nthe patient data and Data -\nthe outcome of an illness\nand the severity of illness.\nSo using all the data\na doctor can identify\nwith A patient is ill or not.\nSo these are\nthe various use cases\nin which you can use\nlogistic regression now,\nI guess enough of theory part.\nSo let's move ahead and see some\nof the Practical implementation\nof logistic regression\nso over here,\nI be implementing two projects\nwhen I have the data set\nof Titanic so over here\nwill predict what factors made\npeople more likely to survive\nthe sinking of the Titanic ship\nand my second project\nwill see the data analysis\non the SUV cars so over here we\nhave the data of the SUV cars\nwho can purchase it.\nAnd what factors made people\nmore interested in buying SUV?\nSo these will be\nthe major questions as\nto why you should Implement\nlogistic regression and\nwhat output will you get by it?\nSo let's start by\nthe very first project\nthat is Titanic data analysis.\nSo some of you might know\nthat there was a ship\ncalled as Titanic\nwith basically hit an iceberg\nand it sunk to the bottom\nof the ocean and it was\na big disaster at that time\nbecause it was the first\nvoyage of the ship\nand it was supposed to be really\nreally strongly built and one\nof the best ships of that time.\nSo it was a big Disaster\nof that time and of course there\nis a movie about this as well.\nSo many of you\nmight have washed it.\nSo what we have we have data\nof the passengers those\nwho survived and those\nwho did not survive\nin this particular tragedy.\nSo what you have to do you\nhave to look at this data\nand analyze which factors\nwould have been contributed\nthe most to the chances\nof a person survival\non the ship or not.\nSo using the logistic\nregression, we can predict\nwhether the person survived\nor the person died\nnow apart from this.\nWe also have a look\nwith the various features\nalong with that.\nSo first, let us explore\nThe data set so over here.\nWe have the index value\nthen the First Column\nis passenger ID.\nThen my next column is survived.\nSo over here,\nwe have two values\na 0 and a 1 so 0 stands\nfor did not survive\nand one stands for survive.\nSo this column is categorical\nwhere the values\nare discrete next.\nWe have passenger class\nso over here,\nwe have three values 1 2 and 3.\nSo this basically tells you that\nwhether a passengers travelling\nin the first class second class\nor third class,\nthen we have the name of the We\nhave the six or you can see\nthe gender of the passenger\nwhere the passenger\nis a male or female.\nThen we have the age\nwe had sip SP.\nSo this basically means\nthe number of siblings\nor the spouses aboard\nthe Titanic so over here,\nwe have values such as 1\n0 and so on then we have\nParts apart is basically\nthe number of parents\nor children aboard\nthe Titanic so over here,\nwe also have some values\nthen we have the ticket number.\nWe have the fair.\nWe have the table number\nand we have the embarked column.\nSo in my inbox column,\nwe have three values\nwe have SC and Q.\nSo as basically stands\nfor Southampton C\nstands for Cherbourg\nand Q stands for Cubans down.\nSo these are the features\nthat will be applying\nour model on so here\nwe'll perform various steps\nand then we'll be implementing\nlogistic regression.\nSo now these are\nthe various steps\nwhich are required\nto implement any algorithm.\nSo now in our case\nwe are implementing\nlogistic regression soft.\nVery first step is\nto collect your data\nor to import the libraries\nthat are used for\ncollecting your data.\nAnd then taking it forward then\nmy second step is to analyze\nyour data so over here I can go\nto the various fields\nand then I can analyze the data.\nI can check that the females\nor children survive\nbetter than the males\nor did the rich\npassenger survived more\nthan the poor passenger\nor did the money matter as in\nwho paid mode to get\ninto the ship\nwith the evacuated first?\nAnd what about the workers\ndoes the worker survived\nor what is the survival rate\nif you were the worker\nin the ship and not just\na traveling passenger?\nSo all of these are\nvery very and questions\nand you would be going\nthrough all of them one by one.\nSo in this stage,\nyou need to analyze our data\nand explore your data as much\nas you can then my third step is\nto Wrangle your data now\ndata wrangling basically means\ncleaning your data so over here,\nyou can simply remove\nthe unnecessary items or\nif you have a null values\nin the data set.\nYou can just clear that data and\nthen you can take it forward.\nSo in this step you\ncan build your model\nusing the train data set\nand then you can test it\nusing a test so over here you\nwill be performing a split\nwhich basically Get\nyour data set into training\nand testing data set and find\nyou will check the accuracy.\nSo as to ensure\nhow much accurate\nyour values are.\nSo I hope you guys got\nthese five steps\nthat you're going to implement\nin logistic regression.\nSo now let's go into all\nthese steps in detail.\nSo number one.\nWe have to collect your data\nor you can say\nimport the libraries.\nSo it may show you\nthe implementation part as well.\nSo I just open\nmy jupyter notebook\nand I just Implement all\nof these steps side by side.\nSo guys this is\nmy jupyter notebook.\nSo first, let me just rename\njupyter notebook to let's say\nTitanic data analysis.\nNow a full step was\nto import all the libraries\nand collect the data.\nSo let me just import\nall the library's first.\nSo first of all,\nI'll import pandas.\nSo pandas is used\nfor data analysis.\nSo I'll say import pandas as PD\nthen I will be importing numpy.\nSo I'll say import numpy as NP\nso number is a library in Python\nwhich basically stands\nfor numerical Python\nand it is widely used to perform\nany scientific computation.\nNext.\nWe will be importing Seaborn.\nSo c 1 is a library\nfor statistical plotting so\nSay import Seaborn as SNS.\nI'll also import matplotlib.\nSo matplotlib library\nis again for plotting.\nSo I'll say import\nmatplotlib dot Pi plot\nas PLT now to run this library\nin jupyter Notebook all I have\nto write in his percentage\nmatplotlib in line.\nNext I will be importing\none module as well.\nSo as to calculate the basic\nmathematical functions,\nso I'll say import maths.\nSo these are the libraries\nthat I will be needing\nin this Titanic data analysis.\nSo now let me just\nimport my data set.\nSo I'll take a variable.\nLet's say Titanic data\nand using the pandas.\nI will just read my CSV\nor you can see the data set.\nI like the name of my data set\nthat is Titanic dot CSV.\nNow.\nI have already showed you\nthe data set so over here.\nLet me just bring\nthe top 10 rows.\nSo for that I will just say\nI take the variable\nTitanic data dot head\nand I'll say the top ten rules.\nSo now I'll just run this\nso to run this style is have\nto press shift + enter\nor else you can just directly\nclick on this cell so over here.\nI have the index.\nWe have the passenger ID,\nwhich is nothing.\nBut again the index\nwhich is starting from 1 then\nwe have the survived column\nwhich has a category.\nCall values or you can say\nthe discrete values,\nwhich is in the form of 0 or 1.\nThen we have\nthe passenger class.\nWe have the name of\nthe passenger sex age and so on.\nSo this is the data set\nthat I will be going forward\nwith next let us print\nthe number of passengers\nwhich are there in this original\ndata frame for that.\nI'll just simply type in print.\nI'll say a number of passengers.\nAnd using the length function,\nI can calculate\nthe total length.\nSo I'll say length\nand inside this I'll be\npassing this variable\nbecause Titanic data,\nso I'll just copy it from here.\nI'll just paste it dot index\nand next set me\njust bring this one.\nSo here the number of passengers\nwhich are there\nin the original data set\nwe have is 891 so around\nthis number would traveling in\nthe Titanic ship so over here,\nmy first step is done\nwhere you have just collected\ndata imported all the libraries\nand find out the total\nnumber of passengers,\nwhich are Titanic so\nnow let me just go back\nto presentation and let's see.\nWhat is my next step.\nSo we're done with\nthe collecting data.\nNext step is to analyze\nyour data so over here\nwill be creating different plots\nto check the relationship\nbetween variables as\nin how one variable\nis affecting the other\nso you can simply explore\nyour data set by making use\nof various columns\nand then you can plot\na graph between them.\nSo you can either plot\na correlation graph.\nYou can plot\na distribution curve.\nIt's up to you guys.\nSo let me just go back\nto my jupyter notebook and let\nme analyze some of the data.\nOver here.\nMy second part is\nto analyze data.\nSo I just put this in headed\nto now to put this in here\nto I just have to go\non code click on mark down\nand I just run this so first\nlet us plot account plot\nwhere you can pay\nbetween the passengers\nwho survived and\nwho did not survive.\nSo for that I will be using\nthe Seabourn Library so over\nhere I have imported\nSeaborn as SNS\nso I don't have\nto write the whole name.\nI'll simply say\nSNS dot count plot.\nI say axis with the survive\nand the data\nthat I'll be using\nis the Titanic data\nor you can say the name\nof variable in which you\nhave store your data set.\nSo now let me just run this\nso who were here as you can see\nI have survived column on my x\naxis and on the y axis.\nI have the count.\nSo zero basically stands\nfor did not survive\nand one stands\nfor the passengers\nwho did survive so over here,\nyou can see that around 550\nof the passengers\nwho did not survive and they\nwere around 350 passengers\nwho only survive so here\nyou can basically conclude.\nThere are very less survivors\nthan on survivors.\nSo this was the very first plot\nnow there is not another plot\nto compare the sex as to whether\nout of all the passengers\nwho survived and\nwho did not survive.\nHow many were men and\nhow many were female\nso to do that?\nI'll simply say\nSNS dot count plot.\nI add the Hue as six\nso I want to know\nhow many females and\nhow many male survive\nthen I'll be\nspecifying the data.\nSo I'm using Titanic data\nset and let me just run\nthis you have done a mistake\nover here so over here you\ncan see I have survived\ncolumn on the x-axis\nand I have the count\non the why now.\nSo have you color stands\nfor your male passengers\nand orange stands\nfor your female?\nSo as you can see\nhere the passengers\nwho did not survive\nthat has a value\n0 so we can see that.\nMajority of males did not\nsurvive and if we see the people\nwho survived here,\nwe can see the majority\nof female survive.\nSo this basically concludes\nthe gender of the survival rate.\nSo it appears on average\nwomen were more than three\ntimes more likely\nto survive than men next.\nLet us plot another plot\nwhere we have the Hue as\nthe passenger class so over\nhere we can see which class at\nthe passenger was traveling in\nwhether it was traveling\nin class 1 2 or 3.\nSo for that I just\narrived the same command.\nI will say as soon as.com plot.\nI gave my x-axis as a family.\nI'll change my Hue\nto passenger class.\nSo my variable\nnamed as PE class.\nAnd the data said\nthat I'll be using\nthis Titanic data.\nSo this is my result\nso over here you can see I have\nblue for first-class orange\nfor second class and green\nfor the third class.\nSo here the passengers\nwho did not survive a majorly\nof the third class\nor you can say the lowest class\nor the cheapest class to get\ninto the dynamic and the people\nwho did survive majorly belong\nto the higher classes.\nSo here 1 & 2 has more eyes\nthan the passenger\nwho were traveling\nin the third class.\nSo here we have computed\nthat the passengers\nwho did not survive\na majorly of third.\nOr you can see the lowest class\nand the passengers\nwho were traveling\nin first and second class\nwould tend to survive mode next.\nI just got a graph for\nthe age distribution over here.\nI can simply use my data.\nSo we'll be using\npandas library for this.\nI will declare an array\nand I'll pass in the column.\nThat is H.\nSo I plot and I\nwant a histogram.\nSo I'll see plot da test.\nSo you can notice over here\nthat we have more\nof young passengers,\nor you can see the children\nbetween the ages 0 to 10\nand then we have\nthe average people\nand if you go ahead Lester\nwould be the population.\nSo this is the analysis\non the age column.\nSo we saw that we have\nmore young passengers and\nmore video courage passengers\nwhich are traveling\nin the Titanic.\nSo next let me plot\na graph of fare as well.\nSo I'll say Titanic data.\nI say fair and again,\nI've got a histogram\nso I'll say hissed.\nSo here you can see\nthe fair size is\nbetween zero to hundred now.\nLet me add the bin size.\nSo as to make it\nmore clear over here,\nI'll say Ben is equals to let's\nsay 20 and I'll increase\nthe figure size as well.\nSo I'll say fixed size.\nLet's say I'll give\nthe dimensions as 10 by 5.\nSo it is bins.\nSo this is more clear now next.\nIt is analyzed\nthe other columns as well.\nSo I'll just type\nin Titanic data\nand I want the information as\nto what all columns are left.\nSo here we have passenger ID,\nwhich I guess it's\nof no use then we have see\nhow many passengers survived\nand how many did not we\nalso do the analysis\non the gender basis.\nWe saw with a female\ntend to survive more\nor the maintain to survive more\nthen we saw the passenger class\nwhere the passenger is traveling\nin the first class second class\nor third class.\nThen we have the name.\nSo in name,\nwe cannot do any analysis.\nWe saw the sex we saw the ages.\nWell, then we have sea bass P.\nSo this stands for the number\nof siblings or the spouse is\nwhich Are aboard the Titanic so\nlet us do this as well.\nSo I'll say SNS dot count plot.\nI mentioned X SC SP.\nAnd I will be using\nthe Titanic data\nso you can see the plot\nover here so over here\nyou can conclude that.\nIt has the maximum value\non zero so we can conclude\nthat neither children\nnor a spouse was\non board the Titanic now\nsecond most highest value is 1\nand then we have various values\nfor 2 3 4 and so on next\nif I go above the store\nthis column as well.\nSimilarly can do four parts.\nSo next we have part\nso you can see the number\nof parents or children\nwhich are both the Titanic\nso similarly can do.\nIsrael then we have\nthe ticket number.\nSo I don't think so.\nAny analysis is\nrequired for Ticket.\nThen we have fears of a we\nhave already discussed as\nin the people would tend\nto travel in the first class.\nYou will pay the highest view\nthen we have the cable number\nand we have embarked.\nSo these are the columns\nthat will be doing\ndata wrangling on\nso we have analyzed the data\nand we have seen\nquite a few graphs\nin which we can conclude which\nvariable is better than another\nor what is the relationship\nthe whole third step\nis my data wrangling\nso data wrangling basically\nmeans Cleaning your data.\nSo if you have a large data set,\nyou might be having\nsome null values\nor you can say n values.\nSo it's very important\nthat you remove all\nthe unnecessary items\nthat are present\nin your data set.\nSo removing this directly\naffects your accuracy.\nSo I just go ahead\nand clean my data\nby removing all the Nan values\nand unnecessary columns,\nwhich has a null value\nin the data set\nthe next time you're\nperforming data wrangling.\nSupposed to fall I'll check\nwhether my dataset\nis null or not.\nSo I'll say Titanic data,\nwhich is the name of my data set\nand I'll say is null.\nSo this will basically tell\nme what all values are null\nand will return me\na Boolean result.\nSo this basically\nchecks the missing data\nand your result will be\nin Boolean format\nas in the result will be true\nor false so Falls mean\nif it is not null\nand true means\nif it is null,\nso let me just run this.\nOver here you can see\nthe values as false or true.\nSo Falls is where the value is\nnot null and true is\nwhere the value is none.\nSo over here you can see\nin the cabin column.\nWe have the very first value\nwhich is null so we have to do\nsomething on this so you can see\nthat we have a large data set.\nSo the counting does not stop\nand we can actually\nsee the some of it.\nWe can actually print\nthe number of passengers\nwho have the Nan value\nin each column.\nSo I say Titanic\nunderscore data is null\nand I want the sum of it.\nThey've got some so this is\nbasically print the number\nof passengers who have the n\nn values in each column\nso we can see\nthat we have missing values\nin each column that is 177.\nThen we have the maximum value\nin the cave in column\nand we have very Less\nin the Embark column.\nThat is 2 so here\nif you don't want\nto see this numbers,\nyou can also plot a heat map\nand then you can visually\nanalyze it so let me just do\nthat as well.\nSo I'll say SNSD heat map.\nand say why tick labels\nFalse child has run this\nas we have already seen\nthat there were three columns\nin which missing data\nvalue was present.\nSo this might be age so over\nhere almost 20% of each column\nhas a missing value then\nwe have the caping columns.\nSo this is quite a large value\nand then we have two values\nfor embark column as well.\nAdd a see map for color coding.\nSo I'll say see map.\nSo if I do this\nso the graph becomes\nmore attractive so over here\nyellow stands for Drew or you\ncan say the values are null.\nSo here we have computed\nthat we have the missing value\nof H. We have a lot\nof missing values\nin the cabin column\nand we have very less value,\nwhich is not even visible\nin the Embark column as well.\nSo to remove\nthese missing values,\nyou can either replace\nthe values and you can put in\nsome dummy values to it or you\ncan simply drop the column.\nSo here let us suppose\npick the age column.\nSo first, let me\njust plot a box plot\nand they will analyze\nwith having a column as age\nso I'll say SNS dot box plot.\nI'll say x is equals\nto passenger class.\nSo it's PE class.\nI'll say Y is equal\nto H and the data set\nthat I'll be using\nis Titanic side.\nSo I'll say the data\nis goes to Titanic data.\nYou can see the edge\nin first class and second class\ntends to be more older rather\nthan we have it\nin the third place.\nWell that depends\nOn the experience\nhow much you earn on might be\nthere any number of reasons?\nSo here we concluded\nthat passengers\nwho were traveling in class\none and class two a tend\nto be older than what we have\nin the class 3 so we have found\nthat we have some\nmissing values in EM.\nNow one way is to either just\ndrop the column\nor you can just simply fill\nin some values to them.\nSo this method is called\nas imputation now\nto perform data wrangling\nor cleaning it is for spring\nthe head of the data set.\nSo I'll say Titanic not head\nso it's Titanic.\nFor data, let's say I\njust want the five rows.\nSo here we have survived\nwhich is again categorical.\nSo in this particular column,\nI can apply\nlogic to progression.\nSo this can be my y value\nor the value\nthat you need to predict.\nThen we have\nthe passenger class.\nWe have the name then we\nhave ticket number Fair\ngiven so over here.\nWe have seen that in keeping.\nWe have a lot of null values\nor you can say that any invalid\nwhich is quite visible as well.\nSo first of all,\nwe'll just drop this column\nfor dropping it.\nI'll just say\nTitanic underscore data.\nAnd I'll simply type\nin drop and the column\nwhich I need to drop so I\nhave to drop the cable column.\nI mention the access equals\nto 1 and I'll say\nin place also to true.\nSo now again,\nI just print the head\nand a to see whether this column\nhas been removed\nfrom the data set or not.\nSo I'll say Titanic dot head.\nSo as you can see here,\nwe don't have\ngiven column anymore.\nNow, you can also\ndrop the na values.\nSo I'll say Titanic data\ndot drop all the any values\nor you can say Nan\nwhich is not a number and I will\nsay in place is equal to True.\nLet's Titanic.\nSo over here,\nlet me again plot the heat map\nand let's say what the values\nwhich will be for showing\na lot of null values.\nHas it been removed or not.\nSo I'll say SNSD heat map.\nI'll pass in the data set.\nI'll check it is null I say why\ndick labels is equal to false.\nAnd I don't want color coding.\nSo again I say false.\nSo this will basically\nhelp me to check\nwhether my values\nhas been removed\nfrom the data set or not.\nSo as you can see here,\nI don't have any null values.\nSo it's entirely black now.\nYou can actually know\nthe some as well.\nSo I'll just go above So\nI'll just copy this part\nand I just use the sum function\nto calculate the sum.\nSo here the tells me\nthat data set is green as\nin the data set does not contain\nany null value or any n value.\nSo now we have R Angela data.\nYou can see cleaner data.\nSo here we have done just\none step in data wrangling\nthat is just removing\none column out of it.\nNow you can do a lot\nof things you can actually\nfill in the values\nwith some other values\nor you can just\ncalculate the mean\nand then you can just fit\nin the null values.\nBut now if I see my data set,\nso I'll say\nTitanic data dot head.\nBut now if I see you over here I\nhave a lot of string values.\nSo this has to be converted\nto a categorical variables\nin order to implement\nlogistic regression.\nSo what we will do\nwe will convert this\nto categorical variable\ninto some dummy variables and\nthis can be done using pandas\nbecause logistic regression\njust take two values.\nSo whenever you apply machine\nlearning you need to make sure\nthat there are\nno string values present\nbecause it won't be taking\nthese as your input variables.\nSo using string you don't have\nto predict anything but\nin my case I have the survived\ncolumns 2210 how many?\nPeople tend to survive\nand how men did not so 0 stands\nfor did not survive\nand one stands for survive.\nSo now let me just\nconvert these variables\ninto dummy variables.\nSo I'll just use pandas\nand I say PD not get dummies.\nYou can simply press\ntab to autocomplete\nand say Titanic data\nand I'll pass the sex\nso you can just simply click\non shift + tab to get\nmore information on this.\nSo here we have\nthe type data frame\nand we have the passenger ID\nsurvived and passenger class.\nSo if Run this you'll see\nthat 0 basically stands\nfor not a female and once and\nfor it is a female similarly for\nmale zero Stanford's not made\nand one Stanford main now,\nwe don't require\nboth these columns\nbecause one column\nitself is enough to tell us\nwhether it's male\nor you can say female or not.\nSo let's say if I want\nto keep only mail I will say\nif the value of mail is 1\nso it is definitely a maid\nand is not a female.\nSo that is how you don't need\nboth of these values.\nSo for that I just\nremove the First Column,\nlet's say a female so\nI'll say drop first.\nAndrew it has given\nme just one column\nwhich is male and has\na value 0 and 1.\nLet me just set this as\na variable hsx so over here\nI can say sex dot head.\nI'll just want to see\nthe first pie Bros.\nSorry, it's Dot.\nSo this is how my data\nlooks like now here.\nWe have done it for sex.\nThen we have\nthe numerical values in age.\nWe have the numerical\nvalues in spouses.\nThen we have the ticket number.\nWe have the pair and we\nhave embarked as well.\nSo in Embark,\nthe values are in SC and Q.\nSo here also we can apply\nthis get dummy function.\nSo let's say I\nwill take a variable.\nLet's say Embark.\nI'll use the pandas Library.\nI need the column name\nthat is embarked.\nLet me just print\nthe head of it.\nSo I'll say Embark\ndot head so over here.\nWe have c q and s now here also\nwe can drop the First Column\nbecause these two\nvalues are enough\nwith the passenger\nis either traveling for Q\nthat is toonstone S4 sound time\nand if both the values\nare 0 then definitely\nthe passenger is from Cherbourg.\nThat is the third value\nso you can again drop the first\nvalue so I'll say drop.\nLet me just run this\nso this is how my output looks\nlike now similarly.\nYou can do it for\npassenger class as well.\nSo here also we have\nthree classes one two,\nand three so I'll just\ncopy the whole statement.\nSo let's say I want\nthe variable name.\nLet's say PCL.\nI'll pass in the column name\nthat is PE class and I'll just\ndrop the First Column.\nSo here also the values\nwill be 1 2 or 3\nand I'll just remove\nthe First Column.\nSo here we just left\nwith two and three so\nif both the values are 0 then\ndefinitely the passengers\ntraveling the first class now,\nwe have made the values\nas categorical now,\nmy next step would be\nto concatenate all\nthese new rows into a data set.\nWe can see Titanic data using\nthe pandas will just concatenate\nall these columns.\nSo I'll say p Dot.\nOne cat and then say\nwe have to concatenate sex.\nWe have to concatenate\nEmbark and PCL\nand then I will mention\nthe access to one.\nI'll just run this can you\nto print the head so\nover here you can see\nthat these columns\nhave been added over here.\nSo we have the mail column\nwith basically tells\nwhere the person is male\nor it's a female then\nwe have the Embark\nwhich is basically q\nand s so if it's traveling\nfrom Queenstown value\nwould be one else it\nwould be 0 and If both\nof these values are zeroed,\nit is definitely\ntraveling from Cherbourg.\nThen we have the passenger\nclass as 2 and 3.\nSo the value of both\nthese is 0 then passengers\ntravelling in class one.\nSo I hope you got this\ntill now now these are\nthe irrelevant columns\nthat we have done over here\nso we can just drop\nthese columns will drop\nin PE class the embarked column\nand the sex column.\nSo I'll just type\nin Titanic data dot drop\nand mention the columns\nthat I want to drop.\nSo I say And even lead\nthe passenger ID\nbecause it's nothing\nbut just the index value\nwhich is starting from one.\nSo I'll drop this as well then\nI don't want name as well.\nSo I'll delete name as well.\nThen what else we can drop we\ncan drop the ticket as well.\nAnd then I'll just\nmention the axis L say\nin place is equal to True.\nOkay, so the my column\nname starts uppercase.\nSo these has been dropped now,\nlet me just bring\nmy data set again.\nSo this is\nmy final leadership guys.\nWe have the survived column\nwhich has the value zero and one\nthen we have the passenger class\nor we forgot to drop\nthis as well.\nSo no worries.\nI'll drop this again.\nSo now let me just run this.\nSo over here we\nhave the survive.\nWe have the H we\nhave the same SP.\nWe have the parts.\nWe have Fair mail and these\nwe have just converted.\nSo here we have just\nperformed data angling\nfor you can see clean the data\nand then we have just\nconverted the values of gender\nto male then embarked to qns\nand the passenger Class 2 2 & 3.\nSo this was all\nabout my data wrangling\nor just cleaning the data then\nmy next up is training\nand testing your data.\nSo here we will split\nthe data set into train subset\nand test steps.\nAnd then what we'll do\nwe'll build a model\non the train data\nand then predict the output\non your test data set.\nSo let me just go\nback to Jupiter\nand it is implement\nthis as well over here.\nI need to train my data set.\nSo I'll just put this\nindeed heading 3.\nSo over you need to Define\nyour dependent variable\nand independent variable.\nSo here my Y is the output\nfor you can say the value\nthat I need to predict\nso over here,\nI will write Titanic data.\nI'll take the column\nwhich is survive.\nSo basically I have\nto predict this column\nwhether the passenger\nsurvived or not.\nAnd as you can see we have\nthe discrete outcome,\nwhich is in the form of 0\nand 1 and rest all the things we\ncan take it as a features or you\ncan say independent variable.\nSo I'll say Titanic data.\nNot a drop,\nso we just simply\ndrop the survive\nand all the other columns\nwill be my independent variable.\nSo everything else are\nthe features which leads\nto the survival rate.\nSo once we have defined\nthe independent variable\nand the dependent variable\nnext step is to split\nyour data into training\nand testing subset.\nSo for that we will\nbe using SK loan.\nI just type in from sklearn\ndot cross validation.\nimport train display Now here\nif you just click\non shift and tab,\nyou can go to the documentation\nand you can just see\nthe examples over here.\nAnd she can blast open it\nand then I just go\nto examples and see\nhow you can split your data.\nSo over here you have\nextra next test why drain\nwhy test and then using\nthe string test platelet\nand just passing\nyour independent variable\nand dependent variable\nand just Define a size\nand a random straight to it.\nSo, let me just copy this\nand I'll just paste over here.\nOver here, we'll train test.\nThen we have the dependent\nvariable train and test\nand using the split function\nwill pass in the independent\nand dependent variable\nand then we'll set a split size.\nSo let's say I'll put it up 0.3.\nSo this basically means\nthat your data set\nis divided in 0.3\nthat is in 70/30 ratio.\nAnd then I can add\nany random straight to it.\nSo let's say I'm applying\none this is not necessary.\nIf you want the same result\nas that of mine,\nyou can add the random stream.\nSo this would basically\ntake exactly the same sample\nevery Next I have to train\nand predict by creating a model.\nSo here logistic\nregression will graph\nfrom the linear regression.\nSo next I'll just type in\nfrom SK loan dot linear model\nimport logistic regression.\nNext I'll just create\nthe instance of this\nlogistic regression model.\nSo I'll say log model is equals\nto largest aggression now.\nI just need to fit my model.\nSo I'll say log model dot fit\nand I'll just pass\nin my ex train.\nAnd why it rain?\nIt gives me all the details\nof logistic regression.\nSo here it gives me the class\nmade dual fit intercept\nand all those things then\nwhat I need to do,\nI need to make prediction.\nSo I will take a variable\ninsect addictions and I'll pass\non the model to it.\nSo I'll say\nlog model dot protect\nand I'll pass in the value\nthat is X test.\nSo here we have just\ncreated a model fit\nthat model and then we\nhad made predictions.\nSo now to evaluate how my model\nhas been performing.\nSo you can simply\ncalculate the accuracy\nor you can also calculate\na classification report.\nSo don't worry guys.\nI'll be showing both\nof these methods.\nSo I'll say\nfrom sklearn dot matrix\ninput classification report.\nAre you start fishing report?\nAnd inside this I'll be passing\nin why test and the predictions?\nSo guys this is\nmy classification report.\nSo over here,\nI have the Precision.\nI have the recall.\nWe have the advanced code\nand then we have support.\nSo here we have the value\nof decision as 75 72 and 73,\nwhich is not that bad now\nin order to calculate\nthe accuracy as well.\nYou can also use the concept\nof confusion Matrix.\nSo if you want to print\nthe confusion Matrix,\nI will simply say\nfrom sklearn dot matrix import\nconfusion Matrix first of all,\nand then we'll just\nprint this So\nhow am I function\nhas been imported successfully\nso is a confusion Matrix.\nAnd I'll again passing\nthe same variables\nwhich is why\ntest and predictions.\nSo I hope you guys already know\nthe concept of confusion Matrix.\nSo can you guys give me\na quick confirmation as\nto whether you guys\nremember this confusion\nMatrix concept or not?\nSo if not,\nI can just quickly\nsummarize this as well.\nOkay charged with you say\nso yes.\nOkay.\nSo what is not clear with this?\nSo I'll just tell\nyou in a brief what\nconfusion Matrix is all about?\nSo confusion Matrix is nothing\nbut a 2 by 2 Matrix\nwhich has a four outcomes\nthis basic tells us\nthat how accurate\nyour values are.\nSo here we have\nthe column as predicted.\nNo predicted Y and we\nhave actual know an actual.\nYes.\nSo this is the concept\nof confusion Matrix.\nSo here let me just fade\nin these values\nwhich we have just calculated.\nSo here we have 105.\n105 2125 and 63 So\nas you can see here,\nwe have got four outcomes now\n105 is the value\nwhere a model has predicted.\nNo, and in reality.\nIt was also a no so\nwhere we have predicted know\nan actual know similarly.\nWe have 63 as a predicted.\nYes.\nSo here the model predicted.\nYes, and actually\nalso it was yes.\nSo in order to\ncalculate the accuracy,\nyou just need to add the sum\nof these two values and divide\nthe whole by the some.\nSo here these two values\ntells me where the order has.\nWe predicted the correct output.\nSo this value is also\ncalled as true-\nThis is called\nas false positive.\nThis is called as true positive\nand this is called\nas false negative.\nNow in order to\ncalculate the accuracy.\nYou don't have\nto do it manually.\nSo in Python,\nyou can just import\naccuracy score function\nand you can get\nthe results from that.\nSo I'll just do that as well.\nSo I'll say from sklearn\ndot-matrix import accuracy score\nand I'll simply\nprint the accuracy.\nI'm passing the same variables.\nThat is why I test\nand predictions so over here.\nIt tells me the accuracy as 78\nwhich is quite good so over here\nif you want to do it manually we\nhave 2 plus these two numbers,\nwhich is 105 263.\nSo this comes out to almost 168\nand then you have to divide\nby the sum of all\nthe phone numbers.\nSo 105 plus 63 plus 21 plus 25,\nso this gives me\na result of to 1/4.\nSo now if you divide\nthese two number you'll get\nthe same accuracy that is 98%\nor you can say .78.\nSo that is how you\ncan calculate the accuracy.\nSo now let me just go back\nto my presentation and let's see\nwhat all we have\ncovered till now.\nSo here we have First Data data\ninto train and test subset then\nwe have build a model\non the train data\nand then predicted the output\non the test data set\nand then my fifth step\nis to check the accuracy.\nSo here we have\ncalculator accuracy to almost\nseventy eight percent,\nwhich is quite good.\nYou cannot say\nthat accuracy is bad.\nSo here tells me\nhow accurate your results.\nSo him accuracy skoda finds\nthat enhanced got\na good accuracy.\nSo now moving ahead.\nLet us see the second project\nthat is SUV data analysis.\nSo in this a car company has\nreleased new SUV in the market\nand using the previous data\nabout the sales of their SUV.\nThey want to predict\nthe category of people\nwho might be interested\nin buying this.\nSo using the\nlogistic regression,\nyou need to find what factors\nmade people more interested\nin buying this SUV.\nSo for this let us hear data set\nwhere I have user ID I have\nOf gender as male\nand female then we have the age.\nWe have the estimated salary\nand then we have\nthe purchased column.\nSo this is my discreet column\nor you can see\nthe categorical column.\nSo here we just have the value\nthat is 0 and 1 and this column\nwe need to predict\nwhether a person can actually\npurchase a SUV or Not.\nSo based on these factors,\nwe will be deciding\nwhether a person can\nactually purchase SUV or not.\nSo we know the salary\nof a person we know the age\nand using these we can predict\nwhether person can\nactually purchase SUV\non Let me just go to my jupyter.\nNotebook and has implemented\na logistic regression.\nSo guys, I will not be going\nthrough all the details\nof data cleaning and analyzing\nthe part start part.\nI'll just leave it on you.\nSo just go ahead\nand practice as much as you can.\nAlright, so the second project\nis SUV predictions.\nAlright, so first of all,\nI have to import\nall the libraries\nso I say import numpy\nSNP and similarly.\nI'll do the rest of it.\nAlright, so now let\nme just bring the head\nof this data set.\nSo this give already seen\nthat we have columns as user ID.\nWe have gender.\nWe have the age.\nWe have the salary\nand then we have to calculate\nwhether person can actually\npurchase a SUV or not.\nSo now let us just simply go on\nto the algorithm part.\nSo we'll directly start off\nwith the logistic regression\nhow you can train a model so\nfor doing all those things\nwe first need to Define\nan independent variable\nand a dependent variable.\nSo in this case,\nI want my ex at is\nan independent variable is\na data set.\nI lock so here I will specify\nsighing all the rows.\nSo cool and basically stands\nfor that and in the columns,\nI want only two and\nthree dot values.\nSo here we should fetch\nme all the rows\nand only the second\nand third column which is age\nand estimated salary.\nSo these are the factors\nwhich will be used to predict\nthe dependent variable\nthat is purchase.\nSo here my dependent\nvariable is purchase\nany dependent variable is\nof age and salary.\nSo I'll say later said dot\nI log I'll have all the rows\nand add just one for column.\nThat is my position.\nIs column values.\nAll right, so I just forgot\nwhen one square\nbracket over here.\nAlright so over here.\nI have defined my independent\nvariable and dependent variable.\nSo here my independent variable\nis age and salary\nand dependent variable\nis the column purchase.\nNow, you must be wondering\nwhat is this?\nI lock function.\nSo I look function is basically\nan index of a panda's data frame\nand it is used\nfor integer based indexing\nor you can also say\nselection by index now,\nlet me just bring\nthese independent variables\nand dependent variable.\nSo if I bring the independent\nvariable I have aged as\nwell as a salary next.\nLet me print the dependent\nvariable as well.\nSo over here you can see I\njust have the values in 0\nand 1 so 0 stands\nfor did not purchase next.\nLet me just divide my data set\ninto training and test subset.\nSo I'll simply write in\nfrom SK loaned cross plate\ndot cross validation.\nimport rain test next I\njust press shift and tab\nand over here.\nI will go to the examples\nand just copy the same line.\nSo I'll just copy this.\nI'll move the points now.\nI want to text size\nto be let's see 25,\nso I have divided the trained\nand tested in 75/25 ratio.\nNow, let's say I'll take\nthe random set of 0 So\nRandom State basically\nensures the same result\nor you can say the same samples\ntaken whenever you run the code.\nSo let me just run this now.\nYou can also scale\nyour input values\nfor better performing\nand this can be done\nusing standard scale.\nOh, so let me do that as well.\nSo I'll say\nfrom sklearn pre-processing.\nImport standard scalar now.\nWhy do we scale it now?\nIf you see a data set we\nare dealing with large numbers.\nWell, although we are using\na very small data set.\nSo whenever you're working\nin a prod environment,\nyou'll be working\nwith large data set\nwe will be using thousands\nand hundred thousands of do\npeople's so they're scaling\ndown will definitely\naffect the performance\nby a large extent.\nSo here let me just show you\nhow you can scale down\nthese input values and then\nthe pre-processing contains all\nyour methods & functionality,\nwhich is required\nto transform your data.\nSo now let us scale down\nfor tests as well as\ntheir training data set.\nSo else First Make\nan instance of it.\nSo I'll say standard scalar\nthen I'll have Xtreme sasc dot\nfit fit underscore transform.\nI'll pass in my Xtreme variable.\nAnd similarly I can do\nit for test wherein\nI'll pass the X test.\nAll right.\nNow my next step is\nto import logistic regression.\nSo I'll simply apply\nlogistically creation\nby first importing it\nso I'll say from sklearn sklearn\nthe linear model import\nlogistic regression over here.\nI'll be using classifier.\nSo is a classifier DOT is equals\nto largest aggression\nso over here,\nI just make an instance of it.\nSo I'll say logistic\nregression and over here.\nI just pass in the random state,\nwhich is 0 No,\nI simply fit the model.\nAnd I simply pass in\nX train and white rain.\nSo here it tells\nme all the details\nof logistic regression.\nThen I have to\npredict the value.\nSo I'll say why I prayed\nit's equals to classifier.\nThen predict function\nand then I just pass in X test.\nSo now we have\ncreated the model.\nWe have scale down\nour input values.\nThen we have applied\nlogistic regression.\nWe have predicted the values\nand now we want\nto know the accuracy.\nSo now the accuracy first we\nneed to import accuracy scores.\nSo I'll say from\nsklearn dot-matrix import\nactually see school\nand using this function we\ncan calculate the accuracy\nor you can manually do\nthat by creating\na confusion Matrix.\nSo I'll just pass.\nmy lightest and my y\npredicted All right.\nSo over here I get\nthe accuracy is 89%\nSo we want to know\nthe accuracy in percentage.\nSo I just have to multiply it\nby a hundred and if I run this\nso it gives me 89%\nSo I hope you guys are clear\nwith whatever I\nhave taught you today.\nSo here I have taken\nmy independent variables as age\nand salary and then\nwe have calculated\nthat how many people\ncan purchase the SUV\nand then we have calculated\nour model by checking\nthe accuracy so over here\nwe get the accuracy is 89\nwhich is great.\nAlright guys that is\nit for today.\nSo I'll Discuss\nwhat we have covered\nin today's training.\nFirst of all,\nwe had a quick introduction\nto what is regression\nand where their aggression\nis actually use then\nwe have understood\nthe types of regression\nand then got into the details\nof what and why\nof logistic regression\nof compared linear was\nin logistic regression.\nIf you've also seen\nthe various use cases\nwhere you can Implement\nlogistic regression in real life\nand then we have picked\nup two projects\nthat is Titanic data analysis\nand SUV prediction so\nover here we have seen\nhow you can collect your data\nanalyze your data then perform.\nModeling on that date\nthat train the data test\nthe data and then finally\nhave calculated the accuracy.\nSo in your SUV prediction,\nyou can actually\nanalyze clean your data\nand you can do a lot of things\nso you can just go ahead\npick up any data set\nand explore it as\nmuch as you can.\nWhat is classification.\nI hope every one of you\nmust have used Gmail.\nSo how do you think the male\nis getting classified as a Spam\nor not spam mail?\nWell, there's But\nclassification So\nWhat It Is Well\nclassification is the process\nof dividing the data set\ninto different categories\nor groups by adding label.\nIn other way,\nyou can say\nthat it is a technique\nof categorizing the observation\ninto different category.\nSo basically what you\nare doing is you are taking\nthe data analyzing it\nand on the basis\nof some condition\nyou finely divided\ninto various categories.\nNow, why do we classify it?\nWell, we classify it\nto perform predictive analysis\non it like when you get\nthe mail the machine\npredicts it Be a Spam\nor not spam mail\nand on the basis\nof that prediction it\nadd the irrelevant or spam mail\nto the respective folder\nin general this classification.\nAlgorithm handled questions.\nLike is this data belongs\nto a category or B category?\nLike is this a male or is this\na female something like that?\nI getting it?\nOkay fine.\nNow the question arises\nwhere will you use it?\nWell, you can use this\nof protection order to check\nwhether the transaction\nis genuine or not suppose.\nI am using a credit.\nHere in India now due\nto some reason I had\nto fly to Dubai now.\nIf I'm using the credit\ncard over there,\nI will get a notification alert\nregarding my transaction.\nThey would ask me to confirm\nabout the transaction.\nSo this is also kind\nof predictive analysis\nas the machine predicts\nthat something fishy is\nin the transaction\nas very for our ago.\nI made the transaction using\nthe same credit card and India\nand 24 hour later.\nThe same credit card is being\nused for the payment in Dubai.\nSo the machine texts that\nsomething fishy is going on\nin the transaction.\nSo in order to confirm it it\nsends you a notification alert.\nAll right.\nWell, this is one of\nthe use case of classification\nyou can even use it\nto classify different items\nlike fruits on the base\nof its taste color size\nor weight a machine\nwell trained using\nthe classification algorithm\ncan easily predict the class\nor the type of fruit whenever\nnew data is given to it.\nNot just the fruit.\nIt can be any item.\nIt can be a car.\nIt can be a house.\nIt can be a signboard.\nOr anything.\nHave you noticed\nthat while you visit some sites\nor you try to login\ninto some you get\na picture capture for that right\nwhere you have to identify\nwhether the given image is of\na car or its of a pole or not?\nYou have to select it\nfor example that 10 images\nand you're selecting\nthree Mages out of it.\nSo in a way you are\ntraining the machine,\nright you're telling\nthat these three are\nthe picture of a car\nand rest are not so\nwho knows you are training\nat for something big\nright?\nSo moving on ahead.\nLet's discuss the types\nof education online.\nWell, there are\nseveral different ways\nto perform the same tasks\nlike in order to predict\nwhether a given person is a male\nor a female the machine\nhad to be trained first.\nAll right,\nbut there are multiple ways\nto train the machine and you\ncan choose any one of them just\nfor Predictive Analytics.\nThere are many\ndifferent techniques,\nbut the most common of them\nall is the decision tree,\nwhich we'll cover in depth\nin today's session.\nSo it's a part\nof classification algorithm.\nWe have decision tree\nrandom Forest name buys.\nK-nearest neighbor Lodge\nis Regression linear regression\nsupport Vector machines\nand so on there are many.\nAlright, so let me give\nyou an idea about few\nof them starting\nwith decision tree.\nWell decision tree is\na graphical representation\nof all the possible solution\nto a decision the decisions\nwhich are made they\ncan be explained very easily.\nFor example here is a task,\nwhich says that should I go\nto a restaurant\nor should I buy a hamburger\nyou are confused on that.\nSo for the artboard you\nwill do you will create\na dish entry for it starting\nwith the root node\nwill be first of all,\nyou will check\nwhether you are hungry or not.\nAll right,\nif you're not hungry then\njust go back to sleep.\nRight?\nIf you are hungry\nand you have $25 then you\nwill decide to go to restaurant\nand if you're hungry\nand you don't have $25,\nthen you will just\ngo and buy a hamburger.\nThat's it.\nAll right.\nSo there's about decision tree\nnow moving on ahead.\nLet's see.\nWhat is a random Forest.\nWell random Forest build\nmultiple decision trees\nand merges them together\nto get a more accurate\nand stable production.\nAll right, most of the time\nrandom Forest is trained\nwith a bagging method.\nThe bragging method\nis based on the idea\nthat the combination\nof learning module increases\nthe overall result.\nIf you are combining the\nlearning from different models\nand then clubbing it together\nwhat it will do it will Increase\nthe overall result fine.\nJust one more thing.\nIf the size of your\ndata set is huge.\nThen in that case one single\ndecision tree would lead\nto our Offutt model same way\nlike a single person\nmight have its own perspective\non the complete population as\na population is very huge.\nRight?\nHowever, if we implement\nthe voting system and ask\ndifferent individual\nto interpret the data,\nthen we would be able\nto cover the pattern\nin a much meticulous way\neven from the diagram.\nYou can see that in section A\nwe have Howard large\ntraining data set what we do.\nWe first divide\nour training data set\ninto n sub-samples on it\nand we create a decision tree\nfor each cell sample.\nNow in the B part\nwhat we do we take the vote\nout of every decision made\nby every decision tree.\nAnd finally we Club\nthe vote to get\nthe random Forest dition fine.\nLet's move on ahead.\nNext.\nWe have neighbor Buys.\nSo name bias is\na classification technique,\nwhich is based on Bayes theorem.\nIt assumes that it's\nof any particular feature in\na class is completely unrelated\nto the presence\nof any other feature\nnamed buys is simple\nand easy to implement algorithm\nand due to a Simplicity\nthis algorithm might out perform\nmore complex model\nwhen the size of the data set\nis not large enough.\nAll right, a classical use case\nof Navy bias is\na document classification.\nAnd that what you\ndo you determine\nwhether a given text corresponds\nto one or more categories\nin the Texas case,\nthe features used might be\nthe presence or absence.\nAbsence of any keyword.\nSo this was about Nev\nfrom the diagram.\nYou can see\nthat using neighbor buys.\nWe have to decide\nwhether we have\na disease or not.\nFirst what we do we\ncheck the probability\nof having a disease\nand not having the disease\nright probability\nof having a disease is 0.1\nwhile on the other hand\nprobability of not having\na disease is 0.9.\nOkay first, let's see\nwhen we have disease\nand we go to the doctor.\nAll right, so when we\nvisited the doctor\nand the test is positive\nAdjective so probability\nof having a positive test\nwhen you're having a disease\nis 0.8 0 and probability\nof a negative test\nwhen you already have\na disease that is 0.20.\nThis is also a false negative\nstatement as the test\nis detecting negative,\nbut you still have\nthe disease, right?\nSo it's a false\nnegative statement.\nNow, let's move ahead\nwhen you don't have\nthe disease at all.\nSo probability of not having\na disease is 0.9.\nAnd when you visit the doctor\nand the doctor is like, yes,\nyou have the disease.\nBut you already know\nthat you don't have the disease.\nSo it's a false\npositive statement.\nSo probability of having\na disease when you actually\nknow there is no disease\nis 0.1 and probability\nof not having a disease\nwhen you actually know\nthere is no disease.\nSo and the probability\nof it is around 0.90 fine.\nIt is same as probability\nof not having a disease even\nthe test is showing\nthe same results\na true positive statement.\nSo it is 0.9.\nAll right.\nSo let's move on ahead and\ndiscuss about kn n algorithm.\nSo this KNN algorithm\nor the k-nearest neighbor,\nit stores all\nthe available cases\nand classifies new cases based\non the similarity measure the K\nin the KNN algorithm as\nthe nearest neighbor,\nwe wish to take vote\nfrom for example,\nif k equal 1 then the object\nis simply assigned to the class\nof that single nearest neighbor\nfrom the diagram.\nYou can see the difference\nin the image\nwhen k equal 1 k equal 3\nand k equal 5, right?\nWell the And systems\nare now able to use\nthe k-nearest neighbor\nfor visual pattern\nrecognization to scan\nand detect hidden packages\nin the bottom bin\nof a shopping cart\nat the checkout\nif an object is detected\nwhich matches exactly\nto the object listed\nin the database.\nThen the price of the spotted\nproduct could even\nautomatically be added\nto the customers Bill\nwhile this automated\nbilling practice is not used\nextensively at this time,\nbut the technology\nhas been developed\nand is available for use\nif you want you can\njust use It and yeah,\none more thing k-nearest\nneighbor is also used\nin retail to detect patterns\nin the credit card users many\nnew transaction scrutinizing\nsoftware application use\nCayenne algorithms to\nanalyze register data\nand spot unusual pattern\nthat indicates\nsuspicious activity.\nFor example,\nif register data indicates\nthat a lot\nof customers information\nis being entered manually rather\nthan through automated scanning\nand swapping then in that case.\nThis could indicate\nthat the employees\nwere using the register.\nIn fact stealing customers\npersonal information or\nif I register data indicates\nthat a particular good\nis being returned\nor exchanged multiple times.\nThis could indicate\nthat employees are misusing\nthe return policy\nor trying to make money from\ndoing the fake returns, right?\nSo this was about KNN algorithm.\nSo starting with\nwhat is decision tree,\nbut first, let me tell\nyou why did we choose\nthe Gentry to start with?\nWell, these decision tree\nare really very easy to read\nand understand it belongs\nto one of The few models\nthat interpretable\nwhere you can understand exactly\nwhy the classifier has made\nthat particular decision right?\nLet me tell you a fact\nthat for a given data set.\nYou cannot say\nthat this algorithm performs\nbetter than that.\nIt's like you cannot say\nthat decision trees\nbetter than a buys\nor name biases performing better\nthan decision tree.\nIt depends on the data set,\nright you have to apply\nhit and trial method\nwith all the algorithms one\nby one and then compare\nthe result the model\nwhich gives the best\nresult as the Order\nwhich you can use\nat for better accuracy\nfor your data set.\nAll right, so let's start\nwith what is decision tree.\nWell a decision tree is\na graphical representation\nof all the possible solution\nto our decision based\non certain conditions.\nNow, you might be wondering\nwhy this thing is called\nas decision tree.\nWell, it is called so\nbecause it starts with the root\nand then branches off\nto a number of solution just\nlike a tree right even\nthe tree starts from a roux\nand it starts\ngrowing its branches.\nAs once it gets bigger\nand bigger similarly\nin a decision tree.\nIt has a roux\nwhich keeps on growing with\nincreasing number of decision\nand the conditions now,\nlet me tell you\na real life scenario.\nI won't say that all of you,\nbut most of you\nmust have used it.\nRemember whenever you dial\nthe toll-free number\nof your credit card company,\nit redirects you\nto his intelligent\ncomputerised assistant\nwhere it asks\nyou questions like,\npress one for English\nor press 2 for Henry,\npress 3 for this press\n4 for that right now\nonce you select one now again,\nIt redirects you\nto a certain set\nof questions like press\n1 for this press 1 for that\nand similarly, right?\nSo this keeps on repeating\nuntil you finally get\nto the right person, right?\nYou might think\nthat you are caught\nin a voicemail hell\nbut what the company\nwas actually doing it\nwas just using a decision tree\nto get you to the right person.\nI lied.\nI'd like you to focus\non this particular image\nfor a moment on\nthis particular slide.\nYou can see I image\nwhere the task is.\nShould I accept\na new job offer or not?\nAlright, so you have\nto decide that for That\nwhat you did you created\na decision tree starting\nwith the base condition\nor the root node.\nWas that the basic salary\nor the minimum salary\nshould be $50,000\nif it is not $50,000.\nThen you are not at all\naccepting the offer.\nAll right.\nSo if your salary is\ngreater than $50,000,\nthen you will further check\nwhether the commute is\nmore than one hour or not.\nIf it is more than one are you\nwill just decline the offer\nif it is less than one hour,\nthen you are getting closer\nto accepting the job offer then\nfurther what you will do.\nYou will check\nwhether the company is offering.\nFree coffee or not,\nright if the company\nis not offering the free coffee,\nthen you will just\ndecline the offer\nand have fit as offering\nthe free coffee.\nAnd yeah, you will happily\naccept the offer right?\nThis is just an example\nof a decision tree.\nNow, let's move ahead\nand understand a decision tree.\nWell, here is a sample data set\nthat I will be using\nit to explain you\nabout the decision tree.\nAll right in this data set\neach row is an example.\nAnd the first two columns\nprovide features or attributes\nthat describes the data\nand the last column\ngives the label\nor the class we want to predict\nand if you like you\ncan just modify this data\nby adding additional features\nand more example\nand our program will work\nin exactly the same way fine.\nNow this data set\nis pretty straightforward\nexcept for one thing.\nI hope you have noticed that\nit is not perfectly separable.\nLet me tell you something\nmore about that as\nin the second and fifth examples\nthey have the same features.\nBut different labels\nboth have yellow as a Colour\nand diameter as three,\nbut the labels are mango\nand lemon right?\nLet's move on and see\nhow our decision tree\nhandles this case.\nAll right, in order to build\na tree will use a decision tree\nalgorithm called card\nthis card algorithm\nstands for classification\nand regression tree\nalgorithm online.\nLet's see a preview\nof how it works.\nAll right to begin\nwith We'll add a root node\nfor the tree and all\nthe nodes receive a list\nof rows as a input\nand the route will receive\nthe entire training data set now\neach node will ask\ntrue and false question\nabout one other feature.\nAnd in response\nto that question will split\nor partition the data set\ninto two different subsets\nthese subsets then become\ninput to child node.\nWe are to the tree\nand the goal of the question\nis to finally unmix the labels\nas we proceed down or in\nother words to produce\nthe purest possible distribution\nof the labels at each node.\nFor example, the input\nof this node contains only.\nOne single type\nof label so we could say\nthat it's perfectly unmixed.\nThere is no uncertainty\nabout the type of label\nas it consists\nof only grapes right\non the other hand the labels\nin this node are still mixed up.\nSo we would ask another question\nto further drill it down,\nright but before that we need to\nunderstand which question to ask\nand when and to do\nthat we need to conduct\nby how much question\nhelps to unmix the label\nand we can quantify\nthe amount of Uncertainty\nat a single node using\na metric called gini impurity\nand we can quantify\nhow much a question reduces\nthat uncertainty using a concept\ncalled information game will use\nthese to select the best\nquestion to ask at each point.\nAnd then what we'll do\nwe'll iterate the steps\nwill recursively build the tree\non each of the new node\nwill continue dividing the data\nuntil there are\nno further question to ask\nand finally we\nreach to our Leaf.\nAlright, alright,\nso this was about decision tree.\nSo in order to create\na diversion First of all\nwhat you have to do\nyou have to identify\ndifferent set of questions\nthat you can ask to a tree\nlike is this color green\nand what will be these question\nthis question will be decided by\nyour data set like as\nthis colored green\nas the diameter greater\nthan equal to 3 is the color\nyellow right questions resembles\nto your data set remember that?\nAll right.\nSo if my color is green,\nthen what it will do it\nwill divide into two part first.\nThe Green Mango will be\nin the true while on the false.\nWe have lemon\nand the map all right.\nAnd if the color is green\nor the diameter is greater\nthan equal to 3\nor the color is yellow.\nNow let's move on\nand understand about\ndecision tree terminologies.\nAlright, so starting\nwith root node root node\nis a base node of a tree\nthe entire tree starts\nfrom a root node.\nIn other words.\nIt is the first node\nof a tree it represents\nthe entire population or sample\nand this entire population\nis further segregated\nor divided into two\nor more homogeneous set.\nFine.\nNext is the leaf node.\nWell, Leaf node is the one\nwhen you reach\nat the end of the tree,\nright that is you\ncannot further segregated down\nto any other level.\nThat is the leaf node.\nNext is splitting splitting\nis dividing your root node\nor node into different sub part\non the basis of some condition.\nAll right, then comes\nthe branch or the sub tree.\nWell, this Branch\nor subtree gets formed\nwhen you split the tree suppose\nwhen you split a root node,\nit gets divided\ninto two branches\nor two subtrees right next.\nThe concept of pruning.\nWell, you can say\nthat pruning is just opposite\nof splitting what we\nare doing here.\nWe are just removing\nthe sub node of a decision tree\nwill see more about pruning\nlater in this session.\nAll right, let's move on ahead.\nNext is parent or child node.\nWell, first of all root node\nis always the parent node\nand all other nodes\nassociated with that\nis known as child node.\nWell, you can understand it\nin a way that all the top node\nbelongs to a parent node\nand all the bottom node\nwhich are derived from\na Top node zhi node the node\nproducing a further note is\na child node and the node\nwhich is producing.\nIt is a parent node\nsimple concept, right?\nLet's use the cartel Gotham\nand design a tree manually.\nSo first of all,\nwhat you do you decide\nwhich question to ask\nand when so\nhow will you do that?\nSo let's first of all visualize\nthe decision tree.\nSo there's the decision tree\nwhich will be creating manually\nor like first of all,\nlet's have a look\nat the Data set you have\nOutlook temperature\nhumidity and windy\nas you have different attributes\non the basis of\nthat you have to predict that\nwhether you can play or not.\nSo which one among them should\nyou pick first answer determine\nthe best attribute that\nclassifies the training data?\nAll right.\nSo how will you choose\nthe best attribute\nor how does a tree decide\nwhere to split or how the tree\nwill decide its root node?\nWell before we move on\nand split a tree there\nare some terminologies\nthat you should know.\nAll right first\nbeing the gini index.\nX so what is this gini Index?\nThis gini index is the measure\nof impurity or Purity used\nin building a decision\nTree in cartel Gotham.\nAll right.\nNext is Information Gain\nthis Information Gain is\nthe decrease in entropy\nafter data set is split\non the basis of an attribute\nconstructing a decision tree is\nall about finding an attribute\nthat Returns the highest\nInformation Gain.\nAll right, so you\nwill be selecting the node\nthat would give you\nthe highest Information Gain.\nAlright next is\nreduction in variance.\nReduction in variance is\nan algorithm which is used\nfor continuous Target variable\nor regression problems.\nThe split with lower variance\nis selected as a criteria to let\nthe population see\nin general term.\nWhat do you mean by variance?\nVariance is how much\nyour data is wearing?\nRight?\nSo if your data is\nless impure or is more pure\nthan in that case\nthe variation would be less\nas all the data\nalmost similar, right?\nSo there's also a way\nof setting a tree the split\nwith lower variance\nis selected as the criteria\nto split the population.\nAll right.\nNext is the chi Square t Square.\nIt is an algorithm\nwhich is used to find out\nthese statistical significance\nbetween the differences\nbetween sub nodes\nand the parent nodes fine.\nLet's move ahead now\nthe main question is\nhow will you decide\nthe best attribute\nfor now just understand\nthat you need to calculate\nsomething known as\ninformation game the attribute\nwith the highest Information\nGain is considered the best.\nYeah.\nI know your next question\nmight be like what?\nThis information,\nbut before we move on and see\nwhat exactly Information Gain\nIs let me first introduce you\nto a term called entropy\nbecause this term\nwill be used in calculating\nthe Information Gain.\nWell entropy is just a metric\nwhich measures the impurity\nof something or in other words.\nYou can say that as\nthe first step to do\nbefore you solve the problem\nof a decision tree\nas I mentioned is\nsomething about impurity.\nSo let's move on and understand\nwhat is impurity suppose.\nYou are a basket full of apples\nand another Bowl Which\nis full of same label,\nwhich says Apple now\nif you are asked\nto pick one item\nfrom each basket and ball,\nthen the probability\nof getting the apple\nand it's correct label is 1 so\nin this case, you can say\nthat impurities zero.\nAll right.\nNow what if there are\nfour different fruits\nin the basket and four different\nlabels in the ball,\nthen the probability\nof matching the fruit\nto a label is obviously not one.\nIt's something less than that.\nWell, it could be possible\nthat I picked banana\nfrom the basket\nand when I randomly\npicked Level from the ball.\nIt says a cherry\nany random permutation\nand combination can be possible.\nSo in this case, I'd say\nthat impurities is nonzero.\nI hope the concept\nof impurities here.\nSo coming back to entropy\nas I said entropy is\nthe measure of impurity\nfrom the graph on your left.\nYou can see that\nas the probability\nis zero or one\nthat is either they\nare highly impure\nor they are highly pure\nthan in that case the value\nof entropy is zero.\nAnd when the probability is\n0.5 then the value of entropy.\nIs maximum.\nWell, what is impurity\nimpurities the degree\nof Randomness how random data is\nso if the data is\ncompletely pure in that case\nthe randomness equals zero or\nif the data is completely empty\nor even in that case\nthe value of impurity\nwill be zero question.\nLike why is it\nthat the value\nof entropy is maximum\nat 0.5 might arise\nin a mine, right?\nSo let me discuss about that.\nLet me derive it mathematically\nas you can see here on the slide\nthe mathematical formula\nof entropy is -\nof probability of yes,\nlet's move on and see\nwhat this graph has to say\nmathematically suppose s is\nour total sample space\nand it's divided into two parts.\nYes, and no like\nin our data set the result\nfor playing was divided\ninto two parts.\nYes or no,\nwhich we have to predict\neither we have to play or not.\nRight?\nSo for that particular case,\nyou can Define the formula\nof entropy as entropy\nof total sample\nspace equals negative\nof probability of e\nis multiplied by\nlog of probability.\nWe of yes,\nwhether base 2 minus probability\nof no X log of probability of no\nwith base to where s is\nyour total sample space\nand P of v s is\nthe probability of e s--\nand p-- of know is\nthe probability of no.\nWell, if the number\nof BS equal number of know\nthat is probability\nof s equals 0.5 right\nsince you have equal number\nof BS and know so\nin that case the value\nof entropy will be one just\nput the value over there.\nAll right.\nLet me just move to Next slide\nI'll show you this.\nAlright next is\nif it contains all Yes,\nor all know that is probability\nof a sample space is either 1\nor 0 then in that case entropy\nwill be equal to 0\nLet's see the\nmathematically one by one.\nSo let's start\nwith the first condition\nwhere the probability was 0.5.\nSo this is our formula\nfor entropy, right?\nSo there's our first case right\nwhich will discuss the art\nwhen the probability\nof vs equal probability of node\nthat is in our data set we have\nRule number of yes, and no.\nAll right.\nSo probability of yes\nequal probability of no\nand that equals\n0.5 or in other words,\nyou can say that yes\nplus no equal\nto Total sample space.\nAll right, since\nthe probability is 0.5.\nSo when you put the values\nin the formula you get\nsomething like this\nand when you calculate it,\nyou will get the entropy of\nthe total sample space as one.\nAll right.\nLet's see for the next case.\nWhat is the next case\neither you have totally us\nor you have to No,\nso if you have total, yes,\nlet's see the formula\nwhen we have total.\nYes.\nSo you have all yes\nand 0 no fine.\nSo probability of e s equal one.\nAnd yes as the total\nsample space obviously.\nSo in the formula\nwhen you put that thing up here,\nyou get entropy\nof sample space equal negative X\nof 1 multiplied by log of 1\nas the value of log 1 equals 0.\nSo the total thing will result\nto 0 similarly is the case\nwith no even in that case\nyou will get the entropy\nof total sample.\nCase as 0 so this was\nall about entropy.\nAll right.\nNext is what is\nInformation Gain?\nWell Information Gain\nwhat it does is it measures\nthe reduction in entropy.\nIt decides which attribute\nshould be selected\nas the decision node.\nIf s is our total collection\nthan Information Gain\nequals entropy,\nwhich we calculated\njust now that -\nweighted average multiplied\nby entropy of each feature.\nDon't worry.\nWe'll just see\nhow it to calculate\nit with an example.\nAll right.\nSo let's manually build\na decision tree\nfor our data set.\nSo there's our data set\nwhich consists of\n14 different instances\nout of which we have nine.\nYes and five know I like\nso we have the formula\nfor entropy just put\nover that since 9 years.\nSo total probability\nof e s equals 9\nby 14 and total probability\nof no equals Phi by 14\nand when you put up the value\nand calculate the result\nyou will get the value.\nOh of entropy as 0.94.\nAll right.\nSo this was your first step\nthat is compute the entropy\nfor the entire data set.\nAll right.\nNow you have to select\nthat out of Outlook\ntemperature humidity and windy,\nwhich of the node should you\nselect as the root node\nbig question, right?\nHow will you decide that?\nThis particular node should\nbe chosen at the base note\nand on the basis of\nthat only I will be creating\nthe entire tree.\nI will select that.\nLet's see so you have to do\nit one by one you have\nto calculate the entropy\nand Information Gain for all\nof the Front note so\nstarting with Outlook.\nSo Outlook has\nthree different parameters\nSunny overcast and rainy.\nSo first of all select\nhow many number of years\nand no are there in the case\nof Sunny like when it is sunny\nhow many number of years\nand how many number\nof nodes are there?\nSo in total we have to yes\nand three Nos and case\nof sunny in case of overcast.\nWe have all yes.\nSo if it is overcast then\nwill surely go to play.\nIt's like that.\nAlright and next it is rainy\nthen total number of vs equal.\nThree and total number\nof no equals 2 fine next\nwhat we do we\ncalculate the entropy\nfor each feature for here.\nWe are calculating the entropy\nwhen Outlook equals Sunny.\nFirst of all,\nwe are assuming\nthat Outlook is our root node\nand for that we are calculating\nthe information gain for it.\nAlright.\nSo in order to calculate\nthe Information Gain remember\nthe formula it was entropy\nof the total sample space -\nweighted average X entropy\nof each feature.\nAll right.\nSo what we are doing here,\nwe are calculating\nthe entropy of out.\nLook when it was sunny.\nSo total number of yes,\nwhen it was sunny was\nto and total number of know\nthat was three fine.\nSo let's put up in the formula\nsince the probability\nof yes is 2 by 5\nand the probability\nof no is 3 by 5.\nSo you will get\nsomething like this.\nAlright, so you are\ngetting the entropy\nof sunny as zero point\nnine seven one fine.\nNext we will calculate\nthe entropy for overcast\nwhen it was overcast.\nRemember it was all yes, right.\nSo the probability\nof yes is equal 1\nand when you put over\nthat you will get the value\nof entropy as 0 fine\nand when it was rainy rainy\nhas 3s and to nose.\nSo probability of e s\nin case of Sonny's 3 by 5\nand probability of know\nin case of Sonny's 2 by 5.\nAnd when you add the value\nof probability of vs\nand probability of no\nto the formula,\nyou get the entropy of sunny as\nzero point nine seven one point.\nNow, you have to calculate\nhow much information you\nare getting from Outlook\nthat equals weighted average.\nAll right.\nSo what was this?\nTo diverge total number of years\nand total number of no fine.\nSo information from Outlook\nequals 5 by 14 from\nwhere does this 5 came over?\nWe are calculating\nthe total number of sample space\nwithin that particular Outlook\nwhen it was sunny, right?\nSo in case of Sunny there\nwas two years and three NOS.\nAll right.\nSo weighted average for Sonny\nwould be equal to 5 by 14.\nAll right,\nsince the formula was five\nby 14 x entropy of each feature.\nAll right, so\nas calculated the entropy He\nfor Sonny is zero point nine.\nSeven one, right?\nSo what we'll do we'll multiply\n5 by 14 with 0.97 one.\nRight?\nWell, this was\nthe calculation for information\nwhen Outlook equal sunny,\nbut Outlook even equals overcast\nand rainy for in that case.\nWhat we'll do again similarly\nwill calculate for everything\nfor overcast and sunny\nfor overcast weighted averages\nfor by 14 multiplied\nby its entropy.\nThat is 0 and for Sonny\nit is same Phi by 14.\nYes, and to Knows X its entropy\nthat is zero point\nnine seven one.\nAnd finally we'll take the sum\nof all of them which equals\nto 0.693 right next.\nWe will calculate\nthe information gained this\nwhat we did earlier was\ninformation taken from Outlook.\nNow, we are calculating.\nWhat is the information?\nWe are gaining\nfrom Outlook right.\nNow this Information Gain\nthat equals to Total entropy\nminus the information\nthat is taken from Outlook.\nAll right, so So\ntotal entropy we had 0.94 -\ninformation we took\nfrom Outlook as 0.693.\nSo the value of information\ngained from Outlook results\nto zero point two four seven.\nAll right.\nSo next what we have to do.\nLet's assume that\nWendy is our root node.\nSo Wendy consists of\ntwo parameters false and true.\nLet's see how many years\nand how many nodes are there\nin case of true and false.\nSo when Wendy has\nFalls as its parameter,\nthen in that case it has\nsix years and to knows.\nAnd when it as true\nas its parameter,\nit has 3 S and 3 nodes.\nAll right.\nSo let's move ahead\nand similarly calculate\nthe information taken from Wendy\nand finally calculate the\ninformation gained from Wendy.\nAlright, so first of all,\nwhat we'll do we'll\ncalculate the entropy\nof each feature starting\nwith windy equal true.\nSo in case of true we\nhad equal number of yes\nand equal number\nof no will remember the graph\nwhen we had the probability as\n0.5 as total number of years\nequal total number of know.\nFor that case\nthe entropy equals 1\nso we can directly\nwrite entropy of room\nwhen it's windy is one\nas we had already proved it\nwhen probability equals 0.5\nthe entropy is the maximum\nthat equals to 1.\nAll right.\nNext is entropy of false\nwhen it is windy.\nAll right, so similarly just\nput the probability of yes\nand no in the formula\nand then calculate the result\nsince you have six years\nand two nodes.\nSo in total,\nyou'll get the probability\nof e S6 by 8 and probability\nof know Two by eight.\nAll right, so when you\nwill calculate it,\nyou will get the entropy\nof false as zero point\neight one one.\nAlright, now, let's calculate\nthe information from windy.\nSo total information\ncollected from Windy\nequals information taken\nwhen Wendy equal true\nplus information taken\nwhen when D equals false.\nSo we'll calculate the weighted\naverage for each one of them\nand then we'll sum\nit up to finally get the total\ninformation taken from windy.\nSo in this case,\nit equals to 8 by 14 multiplied\nby 0.8 1 1 + 6 y 14 x 1\nwhat is this?\n8 it is total number of yes, and\nno in case when when D\nequals false, right?\nSo when it was false,\nso total number of BS\nthat equals to 6 and total more\nof know that equal to 2\nthat some herbs to 8.\nAll right.\nSo that is why the weighted\naverage results to Aid by\n14 similarly information taken\nwhen windy equals true equals\nto 3 plus 3 that is 3 S\nand 3 no equal 6 divided by\ntotal number of sample space.\nThat is 14 x That\nis entropy of true.\nAll right, so it is a\nby 14 multiplied by 0.8 1 1\nplus 6 by 14 x one\nwhich results to 0.89 to this\nis information taken from Windy.\nAll right.\nNow how much information\nyou are gaining from Wendy.\nSo for that what you will do so\ntotal information gained\nfrom Windy that equals\nto Total entropy -\ninformation taken from Windy.\nAll right, that is 0.94 -\n0.89 to that equals\nto zero point zero four eight.\nAnd so 0.048 is the information\ngained from Windy.\nAll right.\nSimilarly we calculated\nfor the rest to all right.\nSo for Outlook\nas you can see,\nthe information was 0.693.\nAnd it's Information Gain\nwas zero point two four seven\nin case of temperature.\nThe information was around\nzero point nine one one\nand the Information Gain\nthat was equal to 0.02\n9 in case of humidity.\nThe information gained was 0.15\nto and in the case of windy.\nThe information\ngained was 0.048.\nSo what we'll do we'll\nselect the attribute.\nWith a maximum fine.\nNow, we are selected\nOutlook as our root node,\nand it is further subdivided\ninto three different parts\nSunny overcast and rain,\nso in case of overcast\nwe have seen\nthat it consists of all.\nYes, so we can consider\nit as a leaf node,\nbut in case of sunny and rainy,\nit's doubtful as it\nconsists of both.\nYes and both know\nso you need to recalculate\nthe things right again\nfor this node.\nYou have to\nrecalculate the things.\nAll right, you have to again\nselect the attribute.\nIs having the maximum\nInformation Gain.\nAll right, so there's\nhow your complete tree\nwill look like.\nAll right.\nSo, let's see when you can play\nso you can play\nwhen Outlook is overcast.\nAll right, in that case.\nYou can always play\nif the Outlook is sunny.\nYou will further drill down\nto check the humidity condition.\nAll right, if the\nhumidity is normal,\nthen you will play\nif the humidity is high\nthen you won't play right\nwhen the Outlook predicts\nthat it's rainy then\nfurther you will check\nwhether it's windy or not.\nIf it is a week went then\nyou will go and offer.\nSay but if it has strong wind,\nthen you won't play right?\nSo this is how your entire\ndecision tree would look\nlike at the end.\nNow comes the concept\nof pruning say is\nthat what should I do to play?\nWell you have to do\npruning pruning will decide\nhow you will play.\nWhat is this pruning?\nWell, this pruning is nothing\nbut cutting down the nodes\nand order to get\nthe optimal solution.\nAll right.\nSo what pruning does it\nreduces the complexity?\nAll right as are you\ncan see on the screen\nthat it showing only\nthe result for you.\nThat is it showing all\nthe result which says\nthat you can play.\nAll right before we drill down\nto a practical session\na common question\nmight come in your mind.\nYou might think\nthat our tree base model better\nthan cleaner model, right?\nYou can think like if I\ncan use a logistic regression\nfor classification problem\nand linear regression\nfor regression problem.\nThen why there is\na need to use the tree.\nWell many of us have this In\nin their mind and well,\nthere's a valid question too.\nWell, actually as\nI said earlier,\nyou can use any algorithm.\nIt depends on\nthe type of problem.\nYou're solving let's look\nat some key factor,\nwhich will help you to decide\nwhich algorithm to use and\nwhen so the first point being\nif the relationship between\ndependent and independent\nvariable as well approximated\nby a linear model then linear\nregression will outperform\ntree base model second case\nif there is a high\nnon-linearity and complex\nrelationship between Lent\nand independent variables\nat remodel will outperform\na classical regression\nmodel in third case.\nIf you need to build a model\nwhich is easy to explain\nto people a decision tree model\nwill always do better\nthan a linear model\nas the decision tree models\nare simpler to interpret\nthen linear regression.\nAll right.\nNow, let's move on ahead and see\nhow you can write it as\nGentry classifier from scratch\nand python using\nthe card algorithm.\nAll right for this.\nI will be using jupyter notebook\nwith python 3.0.\nOh install on it.\nAlright, so let's\nopen the Anaconda\nand the jupyter notebook.\nWhereas that so this\nis a inner Corner Navigator\nand I will directly jump over\nto jupyter notebook and hit\nthe launch button.\nI guess everyone\nknows that jupyter.\nNotebook is a web-based\ninteractive Computing notebook\nenvironment where you\ncan run your python codes.\nSo my jupyter notebook.\nIt opens on my Local\nHost double 8 9 1\nso I will be using\nthis jupyter notebook\nin order to write\nmy decision tree classifier\nusing python for this\ndecision tree classifier.\nI have already written.\nSet of codes.\nLet me explain you\njust one by one.\nSo we'll start with initializing\nour training data set.\nSo there's our sample data set\nfor which each row\nis an example.\nThe last column is a label\nand the first two columns\nare the features.\nIf you want you can add some\nmore features an example\nfor your practice\ninteresting fact is\nthat this data set\nis designed in a way\nthat the second and fifth\nexample have almost\nthe same features,\nbut they have different labels.\nAll right.\nSo let's move on and see\nhow the tree handles this case\nas you can see here both.\nBoth of them the second\nand the fifth column\nhave the same features.\nWhat did different\nis just their label?\nRight?\nSo let's move ahead.\nSo this is our training data\nset next what we are doing we\nare adding some column labels.\nSo they are used only\nto print the trees fine.\nSo what we'll do we'll add\nheader to the columns\nlike the First Column is\nof color second is of diameter\nand third is a label column.\nAlright, next Road\nwill do will Define\na function as unique values\nin which will pass the rows\nand the columns.\nSo this function\nwhat it will do.\nWe find the unique values\nfor a column in the data set.\nSo this is an example for that.\nSo what we are doing here,\nwe are passing\ntraining data Hazard row\nand column number as 0 so\nwhat we are doing we are finding\nunique values in terms of color.\nAnd in this\nsince the row is training data\nand the column is 1\nso what you are doing here,\nso we are finding\nthe unique values\nin terms of diameter fine.\nSo this is just an example next\nwhat we'll do we'll Define\na function as class count\nand we'll pass zeros into it.\nSo what it does it counts\nthe number of each type\nof Example within data set.\nSo in this function\nwhat we are basically doing\nwe are counting the number\nof each type for example\nin the data set or\nwhat we are doing.\nWe are counting the unique\nvalues for the label\nin the data set as a sample.\nYou can see here.\nWe can pass that entire\ntraining data set\nto this particular function\nas class underscore count\nwhat it will do it will find\nall the different types of label\nwithin the training data set\nas you can see here the unique\nlabel consists of mango grape\nand lemon so next what we'll do\nwe'll Define a function\nis numeric and we'll pass\na value into it.\nSo what it Do it.\nWe'll just test\nif the value is numeric\nor not and it will return\nif the value is\nan integer or a float.\nFor example, you\ncan see is numeric.\nWe are passing 7\nso it is an integer\nso it will return in value and\nif we are passing red it's\nnot a numeric value, right?\nSo moving on ahead\nwhere you define a class\nnamed as question.\nSo what this question does\nthis question is used\nto partition the data set.\nThis class voted does it\njust records a column number?\nFor example 0 for color a light\nand a column value for example,\ngreen Next what we are doing\nwe are defining a match method\nwhich is used to compare\nthe feature value\nin the example.\nThe feature values\nstored in the question.\nLet's see how first of all\nwhat you are doing.\nWe're defining an init\nfunction and inside\nthat we are passing\nthe self column\nand the value as parameter.\nSo next what we do\nwe Define a function\nas match what it does is it\ncompares the feature value\nin an example to the feature\nvalue in this question\nwhen next we'll Define\na function as re PR,\nwhich is just a helper method\nto print the question\nin a readable format.\nNext what we are doing we are\ndefining a function partition.\nWell, this function\nis used to partition\nthe data set each row\nin the data set it checks\nif it matched\nthe question or not\nif it does so it adds it\nto the true rose or if not,\nthen it adds to the false Rose.\nAll right, for example,\nas you can see, it's partition\nthe training data set based on\nwhether the rows\nare ready or not here.\nWe are calling\nthe function question\nand we are passing a value\nof zero and read to it.\nSo what did we do?\nIt will assign all the red rose\nto True underscore Rose.\nAnd everything else\nwill be assigned\nto false underscore rose fine.\nNext what we'll do we'll Define\na gini impurity function\nand inside that will pass\nthe list of rows.\nSo what it will do it will just\ncalculate the dream Purity\nfor the list of rows.\nNext what we are doing\nevery defining a function\nas Information Gain.\nSo what this Information Gain\nfunction does it calculates\nThe Information Gain\nusing the uncertainty\nof the starting node -\nthe weighted impurity\nof the child node.\nThe next function\nis find the best plate.\nWell, this function is used\nto find the best question to ask\nby iterating over\nevery feature of value\nand then calculating\nthe information game.\nFor the detail explanation\non the code.\nYou can find the code\nin the description given below.\nAll right next we'll define\na class as leave\nfor classifying the data.\nIt holds a dictionary of glass\nlike mango for how many times\nit appears in the row\nfrom the training data\nthat reaches the sleeve.\nAlright next is\nthe decision node.\nSo this decision node,\nit will ask a question.\nThis holds a reference\nto the question\nand the two child nodes\non the base of\nthat you are deciding which node\nto add further to which branch.\nAlright so next video.\nWe're defining a function\nof Beltre and inside\nthat we are passing\nour number of rows.\nSo this is the function\nthat is used to build the tree.\nSo initially what we did we\nDefine all the various function\nthat we'll be using\nin order to build a tree.\nSo let's start\nby partitioning the data set\nfor each unique attribute,\nthen we'll calculate\nthe information gain\nand then return the question\nthat produces the highest gain\nand on the basis of that\nwill split the tree.\nSo what we are doing here,\nwe are partitioning\nthe data set calculating\nthe Information Gain.\nAnd then what this is returning\nit is returning the question\nthat is producing\nthe highest gain.\nAll right.\nNow if gain equals\n0 return Leaf Rose,\nso what it will do.\nSo if we are getting\nno for the gain\nthat is gain equals\n0 then in that case\nsince no further question\ncould be asked\nso what it will do it\nwill return a leaf fine now true\nor underscore Rose\nor false underscore Rose\nequal partition with rose\nand the question.\nSo if we are reaching\ntell this position,\nthen you have already\nfound a Value\nwhich will be used\nto partition the data set then\nwhat you will do you\nwill recursively build\nthe true branch\nand similarly recursively\nbuild the false Branch.\nSo return Division\nand Discord node and side\nthat will be passing question\ntrue branch and false Branch.\nSo what it will do it\nwill return a question node.\nThis question node this\nrecalls the best feature\nor the value to ask\nat this point fine.\nNow that we have\nBuilder tree next\nwhat we'll do we'll Define\na print underscore tree function\nwhich will be used\nto print the tree fine.\nSo finally what we are doing\nin this particular function\nthat we are printing our tree\nnext is the classify function\nwhich will use it to decide\nwhether to follow the true\nBranch or the false branch\nand then compared\nto the feature values stored\nin the node to the example.\nWe are considering\nand last what we'll do\nwe'll finally print\nthe production at the leaf.\nSo let's execute\nit and see okay,\nso there's our testing data.\nOnline so we printed\na leaf as well.\nNow that we have trained\nour algorithm is\nour training data set\nnow it's time to test it.\nSo there's our testing data set.\nSo let's finally execute\nit and see what is the result.\nSo this is the result you\nwill get so first question,\nwhich is asked by the algorithm\nis is diameter greater\nthan equal to 3,\nif it is true,\nthen it will further ask\nif the color is yellow again,\nif it is true,\nthen it will predict mango\nas one and lemon with one.\nAnd in case it is false,\nthen it will just\npredict the mango.\nNow.\nThis was the true part.\nNow next coming\nto diameter is not greater\nthan or equal to 3 then\nin that case it's false.\nAnd what did we do?\nIt'll just predict\nthe grape vine.\nOkay.\nSo this was all\nabout the coding part now,\nlet's conclude this session.\nBut before concluding let me\njust show you one more thing.\nNow.\nThere's a scikit-learn\nalgorithm cheat sheet,\nwhich explains you\nwhich algorithm you should use\nand when all right,\nlet's build in\na decision tree format.\nAt let's see how it is Big.\nSo first condition it will check\nwhether you have\n50 samples or not.\nIf your samples\nare greater than 50,\nthen we'll move ahead\nif it is less than 50,\nthen you need\nto collect more data\nif your sample\nis greater than 50,\nthen you have to decide\nwhether you want to predict\na category or not.\nIf you want to\npredict a category,\nthen further you will see\nthat whether you\nhave labeled data or not.\nIf you have label data, then\nthat would be a classification\nalgorithm problem.\nIf you don't have\nthe label data,\nthen it would be\na clustering problem.\nNow if you don't want\nto The category then\nwhat you want to protect\npredict a quantity.\nWell, if you want\nto predict a quantity,\nthen in that case,\nit would be\na regression problem.\nIf you don't want to predict\na quantity and you want\nto keep looking further,\nthen in that case,\nyou should go for dimensionality\nreduction problems and still\nif you don't want to look\nand the predicting structure\nis not working.\nThen you have\ntough luck for that.\nI hope this doesn't recession\nclarifies all your doubt\nover decision tree algorithm.\nNow, we'll try to find out\nthe answer to this particular\nquestion as to why we\nneed random Forest fine.\nSo like human beings learn\nfrom the past experiences.\nSo unlike human beings\na computer does not have\nexperiences then how does\nmachine takes decisions?\nWhere does it learn from?\nWell a computer system actually\nlearns from the data which\nrepresents some past experiences\nof an application domain.\nSo now let's see,\nhow random Forest It's\nin building up in learning model\nwith a very simple use case\nof credit risk detection.\nNow needless to say\nthat credit card companies\nhave a very nested\ninterest in identifying\nFinancial transactions\nthat are illegitimate\nand criminal in nature.\nAnd also I would like\nto mention this point\nthat according to\nthe Federal Reserve payments\nstudy Americans used\ncredit cards to pay\nfor twenty six point\ntwo million purchases in 2012\nand The estimated loss\ndue to unauthorized transactions\nthat here was u.s.\n6 point 1 billion dollars now\nin the banking industry\nmeasuring risk is very critical\nbecause the stakes are too high.\nSo the overall goal is\nactually to figure out\nwho all can be fraudulent\nbefore too much Financial\ndamage has been done.\nSo for this a credit card\ncompany receives thousands\nof applications for new cards\nand each application\ncontains information.\nMission about an\napplicant, right?\nSo so here as you can see\nthat from all those applications\nwhat we can actually\nfigure out is\nthat predictor variables.\nLike what is the marital\nstatus of the person?\nWhat is the gender\nof the person?\nWhat is the age of the person\nand the status which is actually\nwhether it is a default pair\nor non-default pair.\nSo default payments are\nbasically when payments\nare not made in time\nand according to the agreement\nsigned by the cardholder.\nSo now that account is actually\nset to be in the default.\nSo you can easily\nfigure out the history\nof the particular card holder\nfrom this then we can also look\nat the time of payment\nwhether he has been\na regular pair\nor non regular one.\nWhat is the source of income\nfor that particular person\nand so and so forth.\nSo to minimize loss\nthe back actually needs\ncertain decision rule to predict\nwhether to approve\nParticular no one of\nthat particular person or not.\nNow here is\nwhere the random Forest\nactually comes into the picture.\nAll right.\nNow, let's see how random\nForest can actually help us\nin this particular scenario.\nNow, we have taken randomly\ntwo parameters out of all\nthe predictive variables\nthat we saw previously now,\nwe have taken two\npredictor variables here.\nThe first one is the income\nand the second one\nis the H right\nand Hurley parallel\nit to decision trees\nhave been implemented\nupon those predicted variables\nand let's first assume the case\nof the income variable right?\nSo here we have divided\nour income into three categories\nthe first one being the person\nearning over $35,000 second\nfrom 15 to 35 thousand dollars\nthe third one running\nin the range of 0 to\n15 thousand dollars.\nNow if a person\nis earning over $35,000,\nwhich is a pretty Good\nincome pretty decent.\nSo now we'll check out\nfor the credit history.\nAnd here the probability is\nthat if a person is earning\na good amount then\nthere is very low risk\nthat he won't be able to pay\nback already earning good.\nSo the probability is\nthat his application\nof loan will get approved.\nRight?\nSo there is actually low risk\nor moderate risk,\nbut there's no real issue\nof higher risk as such.\nWe can approve\nthe applicants request here.\nNow, let's move on and watch out\nfor the second category\nwhere the person\nis actually earning\nfrom 15 to 35 thousand dollars\nright now here the person may\nor may not pay back.\nSo in such scenarios will look\nfor the credit history as\nto what has been\nhis previous history.\nNow if his previous\nhistory has been bad\nlike he has been a default ER\nin the previous transactions\nwill definitely not Consider\napproving his request\nand he will be\nat the high risk in which\nis not good for the bank.\nIf the previous history\nof that particular\napplicant is really good.\nThen we will just to clarify\na doubt will consider\nanother parameter as well\nthat will be on depth.\nI have his already\nin really high dip then\nthe risks again increases\nand there are chances\nthat he might not pay\nrepay in the future.\nSo here Will.\nNot accept the request\nof the person having high dipped\nif the person is\nin the low depth\nand he has been a good pair\nin his past history.\nThen there are chances\nthat he might be back\nand we can consider\napproving the request\nof this particular applicant.\nAlex look at the third category,\nwhich is a person earning\nfrom 0 to 15 thousand dollars.\nNow, this is something\nwhich actually raises I broke\nand this person\nwill actually lie\nin the category of high risk.\nAll right.\nSo the probability is\nthat his application of loan\nwould probably get rejected now,\nwe'll get one final outcome from\nthis income parameter, right?\nNow let us look\nat our second variable\nthat is H which will lead\ninto the second decision tree.\nNow.\nLet us say\nif the person is Young, right?\nSo now we will look forward to\nif it is a student now\nif it is a student then\nthe chances are high\nthat he won't be\nable to repay back\nbecause he has\nno earning Source, right?\nSo here the risks are too high\nand probability is\nthat his application\nof loan will get rejected fine.\nNow if the person is Young\nand his Not the student\nthen we'll probably go on\nand look for another variable.\nThat is pan balance.\nNow.\nLet's look if the bank balance\nis less than 5 lakhs.\nSo again the risk arises\nand the probabilities\nthat his application\nof loan will get rejected.\nNow if the person\nis Young is not a student\nand his bank balance so\nof greater than 5 lakhs\nis got a pretty good\nand stable and balanced\nthen the probability is\nthat he is sort of application\nwill get approved of Now\nlet us take another scenario\nif he's a senior, right?\nSo if he is a senior\nwill probably go and check out\nfor this credit history.\nHow well has he been\nin his previous transactions?\nWhat kind of a person he is like\nwhether he's a defaulter\nor is Ananda falter.\nNow if he is a very\nfair kind of person\nin his previous transactions\nthen again the risk arises\nand the probability\nof his application\ngetting rejected actually\nincreases right now\nif he has An excellent person as\nper his transactions\nin the previous history.\nSo now again here\nthere is least risk\nand the probabilities\nthat his application\nof loan will get approved.\nSo now here these two variables\nincome and age have led\nto two different decision trees.\nRight and these two different\ndecision trees actually led\nto two different results.\nNow what random forest does is\nit will actually compile\nthese two different results\nfrom these two different.\nGentry's and then finally,\nit will lead\nto a final outcome.\nThat is how random\nForest actually works.\nRight?\nSo that is actually the motive\nof the random Forest.\nNow let us move forward and see\nwhat is random Forest right?\nYou can get an idea\nof the mechanism from the name\nitself random forests.\nSo a collection\nof trees is a fortress\nthat's why I called\nfor is probably and here\nalso the trees are actually\nbecause being trained on subsets\nwhich are being\nselected at random.\nAnd therefore they are called\nrandom forests So Random forests\nis a collection or an insane.\nHumble of decision trees right\nhere decision trees actually\nbuilt using the whole data\nset considering all features,\nbut actually in random Forest\nonly a fraction of the number\nof rows is selected\nand that too at random\nand a particular\nnumber of features,\nwhich are actually selected\nat random are trained\nupon and that is\nhow the decision trees\nare built upon.\nRight?\nSo similarly number\nof decision trees will be grown\nand each decision tree will Salt\ninto a certain final outcome\nand random Forest\nwill do nothing\nbut actually just\ncompiled the results\nof all those decision trees\nto bring up the final result.\nAs you can see\nin this particular figure\nthat a particular instance\nactually has resulted\ninto three different\ndecision trees, right?\nSo not tree one results into\na final outcome called Class A\nand tree to results\ninto class B. Similarly tree\nthree results into class P\nSo Random Forest\nwill compile the results\nof all these Decision trees\nand it will go by the call\nof the majority voting now\nsince head to decision trees\nhave actually voted\ninto the favor of the Class B\nthat is decision tree 2 and 3.\nTherefore the final outcome will\nbe in the favor of the Class B.\nAnd that is how random\nForest actually works upon.\nNow one really\nbeautiful thing about\nthis particular algorithm is\nthat it is one\nof the versatile algorithms\nwhich is capable of Performing\nboth regression as well as Now,\nlet's try to understand\nrandom Forest further\nwith a very beautiful example\nor this is my favorite one.\nSo let's say you want to decide\nif you want to watch edge\nof tomorrow or not, right?\nSo in this particular scenario,\nyou will have two different\nactions to work Bond either.\nYou can just straight away go\nto your best friend\nasked him about.\nAll right,\nwhether should I go for Edge\nof Tomorrow not will I\nlike this movie or you\ncan ask Your friends\nand take their opinion\nconsideration and then based\non the final results\nwho can go out and watch Edge\nof Tomorrow, right?\nSo now let's just take\nthe first scenario.\nSo where you go\nto your best friend asked about\nwhether you should go\nout to watch edge\nof tomorrow or not.\nSo your friend will probably\nask you certain questions\nlike the first one being\nhere Jonah So so let's say\nyour friend asks you\nif you really like\nThe Adventurous kind\nof movies or not.\nSo you say yes,\ndefinitely I would love to watch\nit Venture kind of movie.\nSo the probabilities\nthat you will like edge\nof tomorrow as well.\nSince Age of Tomorrow is\nalso a movie of Adventure\nand sci-fi kind\nof Journal right?\nSo let's say you do not like\nthe adventure John a movie.\nSo then again\nthe probability reduces\nthat you might really\nnot like edge of Morrow right.\nSo from here you can come\nto a certain conclusion right?\nLet's say your best friend puts\nyou into another situation\nwhere he'll ask you\nor a do you like Emily Blunt\nand you see definitely\nI like Emily Blunt and then he\nputs another question to you.\nDo you like Emily Blunt\nto be in the main lead\nand you say yes, then again,\nthe probability arises\nthat you will definitely\nlike edge of tomorrow as\nwell because Edge of Tomorrow\nis Has the Emily plant\nin the main lead cast so\nand if you say oh I do not like\nEmily Blunt then again,\nthe probability reduces\nthat you would like Edge\nof Tomorrow to write.\nSo this is one way\nwhere you have one decision tree\nand your final outcome.\nYour final decision will be\nbased on your one decision tree,\nor you can see your final\noutcome will be based\non just one friend.\nNo, definitely not\nreally convinced.\nYou want to consider the options\nof your other friends also\nso that you can make\nvery precise and crisp\ndecision right you go out\nand you approach some other\nbunch of friends of yours.\nSo now let's say you go\nto three of your friends\nand you ask them\nthe same question\nwhether I would like to watch\nit off tomorrow or not.\nSo you go out and approach\nthree or four friends friend\none friend twin friend three.\nNow, you will consider\neach of their Sport\nand then you will your decision\nnow will be dependent\non the compiled results of all\nof your three friends, right?\nNow here, let's say you go\nto your first friend\nand you ask him\nwhether you would like\nto watch it just tomorrow\nnot and your first friend\nputs you to one question.\nDid you like Top Gun?\nAnd you say yes,\ndefinitely I did like the movie\nTop Gun then the probabilities\nthat you would like\nedge of tomorrow as\nwell because topgun is actually\na military action drama,\nwhich is also Tom Cruise.\nSo now again the probability\nRises that yes,\nyou will like edge\nof tomorrow as well and\nIf you say no I didn't like\nTop Gun then again.\nThe chances are\nthat you wouldn't like Edge\nof Tomorrow, right?\nAnd then another question\nthat he puts you across is\nthat do you really like\nto watch action movies?\nAnd you say yes,\nI would love to watch\nthem that again.\nThe chances are\nthat you would like\nto watch Edge of Tomorrow.\nSo from your friend\nwhen you can come\nto one conclusion now\nhere since the ratio\nof liking the movie\nto don't like is actually 2 is\nto 1 so the final\nresult is Actually,\nyou would like Edge of Tomorrow.\nNow you go to your second friend\nand you ask the same question.\nSo now you are second friend\nasks you did you like far\nand away when we went\nout and did the last time\nwhen we washed it\nand you say no I really\ndidn't like far and away\nthen you would say then\nyou are definitely going\nto like Edge of Tomorrow.\nWhy does so because far\nand away is actually\nsince most of whom\nmight not be knowing it so far\nin a ways Johner of romance\nand it revolves around a girl\nand a guy By falling in love\nwith each other and so on.\nSo the probability is\nthat you wouldn't like\nedge of tomorrow.\nSo he ask you another question.\nDid you like Bolivian\nand to really like\nto watch Tom Cruise?\nAnd you say Yes, again.\nThe probability is\nthat you would like\nto watch Edge of Tomorrow.\nWhy because Oblivion\nagain is a science fiction\ncasting Tom Cruise full\nof strange experiences.\nAnd where Tom Cruise is\nthe savior of the masses.\nKind well,\nthat is the same kind of plot\nin edge of tomorrow as well.\nSo here it is pure yes\nthat you would like\nto watch edge of tomorrow.\nSo you get\nanother second decision\nfrom your second friend.\nNow you go to your third\nfriend and ask him so\nprobably our third friend is\nnot really interesting\nin having any sort\nof conversation with you say,\nit just simply asks you did you\nlike Godzilla and you said\nno I didn't like Godzilla's\nwe said definitely\nyou wouldn't like\nit's of tomorrow why so\nbecause Godzilla is also\nactually sign Fiction movie\nfrom the adventure Jonah.\nSo now you have got\nthree results from\nthree different decision trees\nfrom three different friends.\nNow you compile the results\nof all those friends\nand then you make\na final call that yes,\nwould you like to watch edge\nof tomorrow or not?\nSo this is some very real time\nand very interesting example\nwhere you can actually\nImplement random Forest\ninto ground reality right\nany questions so far.\nSo far, no,\nthat's good, and then\nwe can move forward.\nNow let us look\nat various domains\nwhere random Forest\nis actually used.\nSo because of its diversity\nrandom Forest is actually used\nin various diverse to means\nlike so beat banking beat\nmedicine beat land use\nbeat marketing name it\nand random Forest is there so\nin banking particularly\nrandom Forest is being\nactually used to make it out\nwhether the applicant\nwill be a default a pair\nor it Will be non default of 1\nso that it can accordingly\napprove or reject\nthe applications of loan,\nright?\nSo that is how random Forest\nis being used in banking\ntalking about medicine.\nRandom.\nForest is widely used\nin medicine field\nto predict beforehand.\nWhat is the probability\nif a person will actually have\na particular disease or not?\nRight?\nSo it's actually used to look\nat the various disease Trends.\nLet's say you want to figure\nout what is the probability\nthat a person\nwill have diabetes?\nNot and so what would you do?\nIt'd probably look\nat the medical history\nof the patient and then\nyou will see or read.\nThis has been\nthe glucose concentration.\nWhat was the BMI?\nWhat was the insulin levels\nin the patient in the past\nprevious three months.\nWhat is the age\nof this particular person\nand will make a different\ndecision trees based on each one\nof these predictor variables\nand then you'll finally\ncompiled the results\nof all those variables\nand then you'll make a fine.\nFinal decision as\nto whether the person\nwill have diabetes\nin the near future or not.\nThat is how random\nForest will be used\nin medicine sector now move.\nRandom Forest is also actually\nused to find out the land use.\nFor example, I want to set\nup a particular industry\nin certain area.\nSo what would I probably\nlook for a look for?\nWhat is the\nvegetation over there?\nWhat is the Urban\npopulation over there?\nRight and how much is the Is\nfrom the nearest modes\nof Transport like\nfrom the bus station\nor the railway station\nand accordingly.\nI will split my parameters\nand I will make decision\non each one of these parameters\nand finally I'll compile\nmy decision of all\nthese parameters in that\nwill be my final outcome.\nSo that is how I\nam finally going to predict\nwhether I should put my industry\nat this particular\nlocation or not.\nRight?\nSo these three examples\nhave actually been of majorly\naround classification problem\nbecause we are\ntrying to classify\nwhether or not we're actually\ntrying to answer this question\nwhether or not right now,\nlet's move forward and look\nhow marketing is revolving\naround random Forest.\nSo particularly in marketing\nwe try to identify\nthe customer churn.\nSo this is particularly\nthe regression kind\nof problem right now\nhow let's see so customer churn\nis nothing but actually\nthe number of people\nwhich are actually\nThe number of customers\nwho are losing out.\nSo we're going\nout of your market.\nNow you want to identify\nwhat will be your customer churn\nin near future.\nSo you'll most of them\neCommerce Industries are\nactually using this\nlike Amazon Flipkart Etc.\nSo they particularly look\nat your each Behavior as to\nwhat has been your past history.\nWhat has been\nyour purchasing history.\nWhat do you like\nbased on your activity\naround certain things around\ncertain ads around certain?\nDiscounts or around certain kind\nof materials right?\nIf you like a particular top\nyour activity will be more\naround that particular top.\nSo that is how they track each\nand every particular move\nof yours and then\nthey try to predict\nwhether you will be\nmoving out or not.\nSo that is how they identify\nthe customer churn.\nSo these all are various domains\nwhere random Forest\nis used and this is\nnot the only list so there\nare numerous other examples\nwhich are Chile are using\nrandom forests that makes\nit so special actually.\nNow, let's move\nforward and see how random\nForest actually works.\nRight.\nSo let us start with the random\nForest algorithm first.\nLet's just see it step\nby step as to how random\nForest algorithm works.\nSo the first step is\nto actually select\ncertain M features from T.\nWhere m is less than T.\nSo here T is the total number\nof the predictor variables\nthat you have\nin your data set and out of\nthose total predictor variables.\nYou will select some randomly\nsome Features out of those now\nwhy we are actually selecting\na few features only.\nThe reason is\nthat if you will select all\nthe predictive variables\nor the total predictor variables\nthen each of your decision tree\nwill be same.\nSo the model is not actually\nlearning something new.\nIt is learning\nthe same previous thing\nbecause all those decision trees\nwill be similar,\nright if you actually split\nyour predicted variables\nand you select randomly\na few predicted variables only.\nLet's say there are 14 total\nnumber of variables and out\nof those you randomly\npick just three right?\nSo every time you will get\na new decision tree,\nso there will be variety.\nRight?\nSo the classification model\nwill be actually\nmuch more intelligent\nthan the previous one.\nNow.\nIt has got\nbarrier to experiences.\nSo definitely it will make\ndifferent decisions each time.\nAnd then when you will compile\nall those different decisions,\nit will be a new more accurate.\nAn efficient result right?\nSo the first important step\nis to select certain number\nof features out of all\nthe features now,\nlet's move on to\nthe second step.\nLet's say for any node D. Now.\nThe first step is to calculate\nthe best plate at that point.\nSo, you know that decision tree\nhow decision trees\nactually implemented so\nyou pick up a the most\nsignificant variable right?\nAnd then you will split\nthat particular node\ninto Other child nodes\nthat is how the split\ntakes place, right?\nSo you will do it\nfor M number of variables\nthat you have selected.\nLet's say you\nhave selected three\nso you will implement\nthe split at all.\nThose three nodes\nin one particular decision tree,\nright the third step\nis split up the node\ninto two daughter nodes.\nSo now you can split\nyour root note\ninto as many notes\nas you want to put hair\nwill split our node\ninto 2.2 notes as to this\nor that so it will be an answer\nin terms of You saw that right?\nOur fourth step will be\nto repeat all these 3 steps\nthat we've done previously\nand we'll repeat\nall this splitting\nuntil we have reached all\nthe N number of nodes.\nRight?\nSo we need to repeat\nuntil we have reached\ntill the leaf nodes\nof a decision tree.\nThat is how we will do it right\nnow after these four steps.\nWe will have\nour one decision tree.\nBut random Forest is\nactually about multiple.\nAsian trees.\nSo here our fifth step\nwill come into the picture\nwhich will actually repeat\nall these previous steps\nfor D number of times now\nhit these the D number\nof decision trees.\nLet's say I want to implement\nfive decision trees.\nSo my first step\nwill be to implement all\nthe previous steps 5 times.\nSo the head the eye tration is\n4/5 number of times right now.\nOnce I have created\nthese five decision trees still\nmy task is not complete yet.\nOn my final task will be\nto compile the results\nof all these five\ndifferent decision trees\nand I will make a call\nin the majority\nvoting right here.\nAs you can see in this picture.\nI had in different instances.\nThen I created\nn different decision trees.\nAnd finally I will compile\nthe result of all these n\ndifferent decision trees\nand I will take my call\non the majority voting right.\nSo whatever my\nmajority vote says\nthat will be My final result.\nSo this is basically an overview\nof the random Forest algorithm\nhow it actually works.\nLet's just have a look\nat this example to get\nmuch better understanding\nof what we have learnt.\nSo let's say I have\nthis data set\nwhich consists of four\ndifferent instances, right?\nSo basically it consists\nof the weather information\nof previous 14 days right\nfrom D1 tildy 14,\nand this basically\nOutlook humidity and wind\nis Click gives me\nthe better condition\nof those 14 days.\nAnd finally I have play\nwhich is my target variable\nweather match did take place\non that particular day\nor not right.\nNow.\nMy main goal is to find out\nwhether the match\nwill actually take place\nif I have following\nthese weather conditions\nwith me on any particular day.\nLet's say the Outlook\nis rainy that day\nand humidity is high\nand the wind is very weak.\nSo now I need to predict\nwhether I will be able\nto play in the match.\nThat they are not.\nAll right.\nSo this is\na problem statement fine.\nNow, let's see how random Forest\nis used in this to sort it out.\nNow here the first step\nis to actually split\nmy entire data set\ninto subsets here.\nI have split my entire\n14 variables into further\nsmaller subsets right\nnow these subsets may\nor may not overlap\nlike there is certain\noverlapping between d 1 till D3\nand D3 till D6 fine.\nIs an overlapping of D3\nso it might happen\nthat there might be overlapping\nso you need not really worry\nabout the overlapping\nbut you have to make sure\nthat all those subsets are\nactually different right?\nSo here I have taken\nthree different subsets\nmy first subset consists of D1\ntill D3 Mexican subset\nconsists of D3\ntill D6 and methods subset\nconsists of D7 tildy.\nNow now I will first be focusing\non my first upset now here,\nlet's say that particular day\nthe Outlook was\nOvercast fine if yes,\nit was overcast\nthen the probabilities\nthat the match will take place.\nSo overcast is basically\nwhen your weather is too cloudy.\nSo if that is the condition\nthen definitely the match\nwill take place and let's say\nit wasn't overcast.\nThen you will consider these\nsecond most probable option\nthat will be the wind\nand you will make\na decision based on this now\nwhether wind was weak or strong\nif wind was weak,\nthen you will definitely\ngo out and play them.\nJudge as you would not so\nnow the final outcome\nout of this decision\ntree will be Play\nBecause here the ratio\nbetween the play\nand no play is to is to 1\nso we get to a certain decision\nfrom a first decision tree.\nNow, let us look\nat the second subset now\nsince second subset has\ndifferent number of variables.\nSo that is why this decision\ntrees absolutely different from\nwhat we saw in our four subsets.\nSo let's say if it was overcast\nthen you will play the match\nif It isn't the overcast\nin you would go\nand look out for humidity.\nNow further.\nIt will get split into two\nwhether it was high or normal.\nNow, we'll take the first case\nif the humidity was high\nand when it was week,\nthen you will play\nthe match else\nif humidity was high\nbut wind was too strong,\nthen you would not go out\nand play the match right now.\nLet us look at the second dot\nto node of humidity\nif the humidity was normal.\nThe wind was weak.\nThen you will definitely go out\nand play the match\nas you want go out\nand play the match.\nSo here if you look\nat the final result,\nthen the ratio of placed no play\nis 3 is to 2 then again.\nThe final outcome\nis actually play, right?\nSo from second subset,\nwe get the final\ndecision of play now,\nlet us look at our third subset\nwhich consists of D7\ntill D9 here\nif again the overcast is yes,\nthen you will play a match.\nEach else you will go\nand check out for humidity.\nAnd if the humidity is\nreally high then you\nwon't play the match else.\nYou will play the match\nagain the probability\nof playing the matches.\nYes, because the ratio\nof no play is Twist one, right?\nSo three different subsets\nthree different decision trees\nthree different outcomes\nand one final outcome\nafter compiling all the results\nfrom these three different\ndecision trees are so I This\ngives a better perspective\nbetter understanding\nof random Forest like\nhow it really works.\nAll right.\nSo now let's just have a look\nat various features\nof random Forest Ray.\nSo the first\nand the foremost feature is\nthat it is one\nof the most accurate\nlearning algorithms, right?\nSo why it is so\nbecause single decision trees\nare actually prone\nto having high variance\nor Hive bias and on\nthe contrary actually.\nM4s, it averages\nthe entire variance\nacross the decision trees.\nSo let's say\nif the variances say\nX4 decision tree,\nbut for random Forest,\nlet's say we have\nimplemented n number\nof decision trees parallely.\nSo my entire variance\ngets averaged to upon\nand my final variance\nactually becomes X upon n so\nthat is how the entire variance\nactually goes down\nas compared to other algorithms.\nNow second most\nimportant feature is\nthat it works well\nfor both classification\nand regression problems\nand by far I have come\nacross this is one\nand the only algorithm\nwhich works equally\nwell for both of them\nthese classification kind\nof problem or a regression kind\nof problem, right?\nThen it's really runs efficient\non large databases.\nSo basically it's\nreally scalable.\nEven if you work for\nthe lesser amount of database\nor if you work for a really\nhuge volume of data, right?\nSo that's a very\ngood part about it.\nThen the fourth most\nimportant point is\nthat it requires almost\nno input preparation.\nNow, why am I saying this is\nbecause it has got\ncertain implicit methods,\nwhich actually take care\nand All the outliers\nand all the missing data\nand you really don't have to\ntake care about all that thing\nwhile you are in the stages\nof input preparations.\nSo Random Forest is\nall here to take care\nof everything else and next.\nIs it performs implicit\nfeature selection, right?\nSo while we are implementing\nmultiple decision trees,\nso it has got implicit method\nwhich will automatically pick\nup some random features out.\nOf all your parameters\nand then it will go\non and implementing\ndifferent decision trees.\nSo for example,\nif you just give\none simple command\nthat all right,\nI want to implement\n500 decision trees no matter\nhow so Random Forest\nwill automatically take care\nand it will Implement all\nthose 500 decision trees\nand those all 500 decision trees\nwill be different\nfrom each other and this is\nbecause it has\ngot implicit methods\nwhich will automatically\ncollect different parameters.\nOut of all the variables\nthat you have right?\nThen it can be easily grown\nin parallel why it is so\nbecause we are actually\nimplementing multiple\ndecision trees and all\nthose decision trees are running\nor all those decisions\ntrees are actually\ngetting implemented parallely.\nSo if you say I want thousand\ntrees to be implemented.\nSo all those thousand trees are\ngetting implemented parallely.\nSo that is how the computation\ntime reduces down.\nRight, and the last point is\nthat it has got methods\nfor balancing error\nin unbalanced it\nas it's now what exactly\nunbalanced data sets\nare let me just give\nyou an example of that.\nSo let's say you're working\non a data set fine\nand you create a random\nforest model and get\n90% accuracy immediately.\nFantastic you think right.\nSo now you start diving\ndeep you go a little deeper.\nAnd you discovered\nthat 90% of that data actually\nbelongs to just one class\ndamn your entire data set.\nYour entire decision\nis actually biased\nto just one particular class.\nSo Random Forest actually\ntakes care of this thing\nand it is really not biased\ntowards any particular decision\ntree or any particular variable\nor any class.\nSo it has got methods\nwhich looks after it\nand they does is all the balance\nof errors in your data sets.\nSo that's pretty much\nabout the features\nof random forests.\nWhat is KNN algorithm\nwill K. Nearest neighbor\nis a simple algorithm\nthat stores all\nthe available cases\nand classify the new data\nor case based\non a similarity measure.\nIt suggests that\nif you are similar\nto your neighbors,\nthen you are one of them, right?\nFor example,\nif apple looks more similar\nto banana orange or Melon.\nRather than a monkey rat\nor a cat then most likely Apple\nbelong to the group of fruits.\nAll right.\nWell in general Cayenne is used\nin Search application\nwhere you are looking\nfor similar items\nthat is when your task is\nsome form of fine items\nsimilar to this one.\nThen you call this search\nas a Cayenne search.\nBut what is this KN KN?\nWell this K denotes the number\nof nearest neighbor\nwhich are voting class\nof the new data\nor the testing data.\nFor example,\nif k equal 1 then the testing\ndata are given the same label\nas a close this Ample\nin the training set similarly\nif k equal to 3 the labels\nof the three closes classes\nare checked and the most\ncommon label is assigned\nto then testing data.\nSo this is\nwhat a KN KN algorithm means\nso moving on ahead.\nLet's see some\nof the example of scenarios\nwhere KN is used\nin the industry.\nSo, let's see\nthe industrial application\nof KNN algorithm starting\nwith recommender system.\nWell the biggest use case\nof cayenne and search\nis a recommender system.\nThis recommended system is\nlike an automated form\nof a shop counter guy when you\nasked him for a product.\nNot only shows you the product\nbut also suggest you or displays\nyour relevant set of products,\nwhich are related to the item.\nYou're already interested\nin buying this KNN algorithm\napplies to recommending\nproducts like an Amazon\nor for recommending media,\nlike in case of Netflix or even\nfor recommending advertisement\nto display to a user\nif I'm not wrong almost all\nof you must have used Amazon\nfor shopping, right?\nSo just to tell you more\nthan 35% of amazon.com revenue\nis generated by\nits recommendation engine.\nSo what's their\nstrategy Amazon uses?\nRecommendation as\na targeted marketing tool\nin both the email campaigns\naround most of its website\nPages Amazon will\nrecommend many products\nfrom different categories based\non what you have browser\nand it will pull those products\nin front of you\nwhich you are likely to buy\nlike the frequently\nbought together option\nthat comes at the bottom\nof the product page to tempt you\ninto buying the combo.\nWell, this recommendation\nhas just one main goal\nthat is increase average\norder value or to upsell\nand cross-sell customers\nby providing product suggestion\nbased on items\nin the shopping cart,\nor On the product they are\ncurrently looking at on site.\nSo next industrial\napplication of KNN\nalgorithm is concept search\nor searching semantically\nsimilar documents\nand classifying documents\ncontaining similar topics.\nSo as you know,\nthe data on the Internet\nis increasing exponentially\nevery single second.\nThere are billions and billions\nof documents on the internet\neach document on the internet\ncontains multiple Concepts,\nthat could be\na potential concept.\nNow, this is a situation\nwhere the main problem\nis to extract concept\nfrom a set of documents\nas each page could have\nthousands of combination\nthat could be potential Concepts\nan average document could have\nmillions of concept combined\nthat the vast amount\nof data on the web.\nWell, we are talking\nabout an enormous amount\nof data set and Sample.\nSo what we need is we need\nto find the concept\nfrom the enormous amount\nof data set and samples, right?\nSo for this purpose,\nwe'll be using KNN\nalgorithm more advanced example\ncould include handwriting\ndetection like an OCR\nor image recognization\nor even video recognization.\nAll right.\nSo now that you know\nvarious use cases\nof KNN algorithm,\nlet's proceed and see\nhow does it work.\nSo how does\na KNN algorithm work?\nLet's start by plotting\nthese blue and orange\npoint on our graph.\nSo these Blue Points\nthe belong to class A\nand the orange ones\nthey belong to class B.\nNow you get a star as a new pony\nand your task is to predict\nwhether this new point\nit belongs to class A\nor it belongs to the class B.\nSo to start the production,\nthe very first thing\nthat you have to do is\nselect the value of K,\njust as I told you KN KN\nalgorithm refers to the number\nof nearest neighbors that you\nwant to select for example,\nin this case k equal to 3.\nSo what does it mean it means\nthat I am selecting three points\nwhich are the least distance\nto the new point\nor you can say I am selecting\nthree different points\nwhich are closest to the star.\nWell at this point\nof time you can ask\nhow will you calculate\nthe least distance?\nSo once you\ncalculate the distance,\nyou will get one blue\nand two orange points\nwhich are closest to this star\nnow since in this case\nas we have a majority\nof Inch point so you can see\nthat for k equal 3D star\nbelongs to the class B,\nor you can say\nthat the star is more similar\nto the orange points\nmoving on ahead.\nWell, what if k equal\nto 6 well for this case,\nyou have to look\nfor six different points\nwhich are closest to this star.\nSo in this case\nafter calculating the distance,\nwe find that we have\nfour blue points\nand two Orange Point\nwhich are closest\nto the star now,\nas you can see\nthat the blue points are\nin majority so you can say\nthat for k equals\n6 this star belongs.\nThese two class A or the star\nis more similar to Blue Points.\nSo by now,\nI guess you know\nhow a KNN algorithm work.\nAnd what is the significance\nof gain KNN algorithm.\nSo how will you\nchoose the value of K?\nSo keeping in mind this case\nthe most important parameter\nin KNN algorithm.\nSo, let's see when you build\na k nearest neighbor classifier.\nHow will you choose\na value of K?\nWell, you might have\na specific value of K in mind\nor you could divide up\nyour data and use something\nlike cross-validation technique\nto test several values of K\nin order to determine\nwhich works best for your data.\nExample if n equal\n2,000 cases then\nin that case the optimal value\nof K lies somewhere\nin between 1 to 19.\nBut yes, unless you try it\nyou cannot be sure of it.\nSo, you know how the algorithm\nis working on a higher level.\nLet's move on and see\nhow things are predicted\nusing KNN algorithm.\nRemember I told you\nthe KNN algorithm uses\nthe least distance measure\nin order to find\nits nearest neighbors.\nSo let's see\nhow these distances calculated.\nWell, there are\nseveral distance measure\nwhich can be used.\nSo to start with Will mainly\nfocus on euclidean distance\nin Manhattan distance\nin this session.\nSo what is\nthis euclidean distance?\nWell, this euclidean distance\nis defined as the square root\nof the sum of difference\nbetween a new point x\nand an existing Point why\nso for example here we\nhave Point P1 and P2 Point\nP. 1 is 1 1 and point B 2 is 5\nfor so what is the euclidean\ndistance between both of them?\nSo you can say\nthat euclidean distance is\na direct distance\nbetween two points.\nSo what is the distance\nbetween the point P1 and P2?\nSo we Calculate it as\n5 minus 1 whole square\nplus 4 minus 1 whole square\nand we can route it\nover which results to 5.\nSo next is\nthe Manhattan distance.\nWell, this Manhattan distance is\nused to calculate the distance\nbetween real Vector using\nthe sum of their absolute\ndifference in this case.\nThe Manhattan distance\nbetween the point P1\nand P2 is mod of 5 minus 1\nplus mod value of 4 minus 1,\nwhich results to 3 plus 4.\nThat is 7.\nSo this slide shows\nthe difference between euclidean\nand Manhattan distance\nfrom point A to point B.\nSo euclidean distance is\nnothing but the direct\nor the least possible distance\nbetween A and B.\nWhereas the Manhattan distance\nis a distance between A\nand B measured along the axis\nat right angle.\nLet's take an example and see\nhow things are predicted\nusing KNN algorithm\nor how the cannon\nalgorithm is working suppose.\nWe have data set\nwhich consists of height weight\nand T-shirt size\nof some customers.\nNow when a new customer\ncome we only have is height.\nAnd wait as the information\nnow our task is to predict.\nWhat is the T-shirt size\nof that particular customer?\nSo for this will be using\nthe KNN algorithm.\nSo the very first thing\nwhat we need to do,\nwe need to calculate\nthe euclidean distance.\nSo now that you have a new data\nof height 160 one centimeter\nand weight as 61 kg.\nSo the very first thing\nthat we'll do is we'll calculate\nthe euclidean distance,\nwhich is nothing\nbut the square root\nof 160 1 minus 158 whole square\nplus 61 minus 58 whole square\nand square root of that is 4.24.\nLet's drag and drop it.\nSo these are the various\neuclidean distance\nof other points.\nNow, let's suppose k equal\nto 5 then the algorithm\nwhat it does is it searches\nfor the five customer\nclosest to the new customer\nthat is most similar\nto the new data in terms\nof its attribute for k equal 5.\nLet's find the top five\nminimum euclidian distance.\nSo these are the distance\nwhich we are going\nto use one two,\nthree, four and five.\nSo let's rank them\nin the order first.\nThis is second.\nThis is third then this one\nis Forward and again,\nthis one is five.\nSo there's our order.\nSo for k equal 5 we\nhave for t-shirts\nwhich come under size\nM and one t-shirt\nwhich comes under size l\nso obviously best guess\nfor the best prediction\nfor the T-shirt size of white\n161 centimeters and wait\n60 1 kg is M.\nOr you can say\nthat a new customer fit\ninto size M. Well this was all\nabout the theoretical session.\nBut before we drill down\nto the coding part,\nlet me just tell you why people\ncall KN as a lazy learner.\nWell KN for classification.\nOcean is a very\nsimple algorithm.\nBut that's not why they are\ncalled lazy KN is a lazy learner\nbecause it doesn't have\na discriminative function\nfrom the training data.\nBut what it does it\nmemorizes the training data,\nthere is no learning phase\nof the model and all\nof the work happens at the time.\nYour prediction is requested.\nSo as such there's the reason\nwhy KN is often referred\nto us lazy learning algorithm.\nSo this was all about\nthe theoretical session now,\nlet's move on\nto the coding part.\nSo for the Practical\nimplementation of\nthe Hands-On part,\nI'll be using\nthe artists data set\nso This data set consists\nof 150 observation.\nWe have four features\nand one class label\nthe four features include\nthe sepal length sepal width\npetal length and the petrol head\nwhereas the class label\ndecides which flower belongs\nto which category.\nSo this was the description\nof the data set,\nwhich we are using now,\nlet's move on and see\nwhat are the step\nby step solution\nto perform a KNN algorithm.\nSo first, we'll start\nby handling the data\nwhat we have to do we\nhave to open the data set\nfrom the CSV format\nand split the data set\ninto train and test part next.\nWe'll take the Clarity where we\nhave to calculate the distance\nbetween two data instances.\nOnce we calculate the distance\nnext we'll look for the neighbor\nand select K Neighbors\nwhich are having the least\ndistance from a new point.\nNow once we get our neighbor,\nthen we'll generate a response\nfrom a set of data instances.\nSo this will decide\nwhether the new Point belongs\nto class A or Class B.\nFinally will create\nthe accuracy function\nand in the end.\nWe'll tie it all together\nin the main function.\nSo let's start with our code\nfor implementing KNN\nalgorithm using python.\nI'll be using Java.\nOld book by Don\n3.0 installed on it.\nNow.\nLet's move on and see\nhow can an algorithm\ncan be implemented using python.\nSo there's my jupyter notebook,\nwhich is a web-based interactive\nComputing notebook environment\nwith python 3.0 installed on it.\nSo the launch its launching so\nthere's our jupyter notebook\nand we'll be riding\nour python codes on it.\nSo the first thing\nthat we need to do is\nload our file our data\nis in CSV format\nwithout a header line\nor any code we can open\nthe file the open function\nand read the data line\nusing the reader function.\nIn the CSV module.\nSo let's write a code\nto load our data file.\nLet's execute the Run button.\nSo once you execute\nthe Run button,\nyou can see the entire training\ndata set as the output next.\nWe need to split the data\ninto a training data set\nthat KN can use to make\nprediction and a test data set\nthat we can use to evaluate\nthe accuracy of the model.\nSo we first need to convert\nthe flower measure\nthat will load it as\nstring into numbers\nthat we can work next.\nWe need to split the data set\nrandomly to train and test.\nRatio 67's 233 for test is\nto train as a standard ratio,\nwhich is used for this purpose.\nSo let's define a function\nas load data set\nthat loads a CSV\nwith the provided file\nnamed and split it\nrandomly into training\nand test data set using\nthe provided split ratio.\nSo this is our function load\ndata set which is using filename\nsplit ratio training data set\nand testing data\nset as its input.\nAll right.\nSo let's execute the Run button\nand check for any errors.\nSo it's executed\nwith zero errors.\nLet's test this function.\nSo there's our training\nset testing set load data set.\nSo this is our function\nload data set on inside\nthat we are passing.\nOur file is data\nwith a split ratio of 0.66\nand training data set\nand test data set.\nLet's see what our training data\nset and test data set.\nIt's dividing into so\nit's giving a count\nof training data set\nand testing data set.\nThe total number\nof training data set\nas split into is\n97 and total number\nof test data set we have is 53.\nSo total number of training data\nset we have here is 97 and total\nnumber of test data\nset we have here is 53.\nAll right.\nOkay, so Function load\ndata set is performing.\nWell, so let's move\non to step two\nwhich is similarity.\nSo in order to make prediction,\nwe need to calculate\nthe similarity between\nany two given data instances.\nThis is needed\nso that we can locate the kamo\nsimilar data instances\nin the training data set are\nin turn make a prediction given\nthat all for flower measurement\nare numeric and have same unit.\nWe can directly use\nthe euclidean distance measure.\nThis is nothing\nbut the square root of the sum\nof squared differences\nbetween two areas\nof the number given\nthat all the for flower\nAre numeric and have\nsame unit we can directly use\nthe euclidean distance measure\nwhich is nothing\nbut the square root of the sum\nof squared difference\nbetween two areas\nor the number additionally\nwe want to control\nwhich field to include\nin the distance calculation.\nSo specifically we only want\nto include first for attribute.\nSo our approach will be\nto limit the euclidean distance\nto a fixed length.\nAll right.\nSo let's define\nour euclidean function.\nSo this are euclidean\ndistance function\nwhich takes instance\none instance to and length as\nparameters instance 1 and ends.\nThese two are the two points\nof which you want to calculate\nthe euclidean distance,\nwhereas this length and denote\nthat how many attributes\nyou want to include?\nOkay.\nSo there's our\neuclidean function.\nLet's execute it.\nIt's executing fine\nwithout any errors.\nLet's test the function\nsuppose the data one\nor the first instance consists\nof the data point has two to two\nand it belongs to class A\nand data to consist\nof four for four\nand it belongs to class P.\nSo when we calculate\nthe euclidean distance\nof data one to data to and\nwhat we have to do we\nhave to consider only\nfirst three features of them.\nAll right.\nSo let's print the distance\nas you can see here.\nThe distance comes\nout to be three point\nfour six four now like\nso this is nothing\nbut the square root\nof 4 minus 2 whole Square.\nSo this distance is nothing\nbut the euclidean distance\nand it is calculated as square\nroot of 4 minus 2 whole square\nplus 4 minus 2 whole square\nthat is nothing but 3\ntimes of 4 minus 2 whole square\nthat is 12 + square root\nof 12 is nothing\nbut 3.46 for all right.\nSo now that we have calculated\nthe distance now we need to look\nfor K nearest.\nNeighbors now that we\nhave a similarity measure\nwe can use it to collect\nthe kamo similar instances\nfor a given unseen instance.\nWell, this is\na straightforward process\nof calculating the distance\nfor all the instances\nand selecting a subset with\nthe smallest distance value.\nAnd now what we have\nto do we have to select\nthe smallest distance values.\nSo for that will be\ndefining a function\nas get neighbors.\nSo for that\nwhat we will be doing\nwill be defining a function\nas get neighbors\nwhat it will do it will return\nthe K most similar Neighbors\nFrom the training set\nfor a given test instance.\nAll right, so this is\nhow our get neighbors In look\nlike it takes training data set\nand test instance\nand K as its input here.\nThe K is nothing but the number\nof nearest neighbor\nyou want to check for.\nAll right.\nSo basically what\nyou'll be getting\nfrom this get Mabel's\nfunction is K different points\nhaving least euclidean distance\nfrom the test instance.\nAll right, let's execute it.\nSo the function executed\nwithout any errors.\nSo let's test our function.\nSo suppose the training data set\nincludes the data like to to\nto and it belongs to class A\nand other data includes\nfour four four and it belongs\nto class P and at testing\nand Census 555 or now,\nwe have to predict\nwhether this test instance\nbelongs to class A\nor it belongs to class be.\nAll right for k equal 1\nwe have to predict\nits nearest neighbor and predict\nwhether this test instance\nit will belong to class A\nor will it belong to class be?\nAlright.\nSo let's execute the Run button.\nAll right.\nSo an executing\nthe Run button you can see\nthat we have output\nas for for for\nand be a new instance\n5 5 5 is closes 2.44\nfor which belongs to class be.\nAll right.\nNow once you have located\nthe most similar neighbor\nfor a test instance next task\nis to predict a response based\non those neighbors.\nSo how we can do that.\nWell, we can do this\nby allowing each neighbor\nto vote for the class attribute\nand take the majority vote\nas a prediction.\nLet's see how we can do that.\nSo we are function\nas getresponse with takes\nneighbors as the input.\nWell, this neighbor was nothing\nbut the output of this get me /\nfunction the output\nof get me were function\nwill be fed to get response.\nAll right.\nLet's execute the Run button.\nIt's executed.\nLet's move ahead and test\nour function get response.\nSo we have a But as bun bun\nbun it belongs to class\nA 2 2 2 it belongs to class a33.\nIt belongs to class B.\nSo this response,\nthat's what it will do.\nIt will store the value\nof get response by passing\nthis neighbor value.\nI like so what we want\nto check is we want to predict\nwhether that test instance\nfinal outcome will belongs\nto class A or Class B.\nWhen the neighbors are\n1 1 1 a 2 2 A + 3 3 B.\nSo, let's check our response.\nNow that we have created\nall the different function\nwhich are required\nfor a KNN algorithm.\nSo important main concern is\nhow do you evaluate\nthe accuracy of the prediction\nand easy way to evaluate\nthe accuracy of the model\nis to calculate a ratio\nof the total correct prediction\nto all the protection made.\nSo for this I will\nbe defining function\nas get accuracy and inside\nthat I'll be passing\nmy test data set\nand the predictions get\naccuracy function check\nget executed without any error.\nLet's check it\nfor a sample data set.\nSo we have our test data set as\n1 1 1 It belongs to class A 2/2\nwhich again belongs to class\n3 3 3 which belongs to class B\nand my predictions is\nfor first test data.\nIt predicted latter belongs\nto class A which is true\nfor next it predicted\nthat belongs to class C,\nwhich is again to and for\nthe next again and predictive\nthat it belongs to class A\nwhich is false in this case\ncause the test data\nbelongs to class be.\nAll right.\nSo in total we have to correct\nprediction out of three.\nAll right, so the ratio\nwill be 2 by 3,\nwhich is nothing but 66.6.\nSo our accuracy rate is 66.6.\nIt's so now that you\nhave created all the function\nthat are required\nfor KNN algorithm.\nLet's compile them\ninto one single main function.\nAlright, so this is\nour main function\nand we are using Iris data set\nwith a split of 0.67 and\nthe value of K is 3 Let's see.\nWhat is the accuracy score\nof this check\nhow accurate are modulus so\nin training data set,\nwe have a hundred\nand thirteen values\nand then the test data set.\nWe have 37 values.\nThese are the predicted\nand the actual values\nof the output.\nOkay.\nSo in total we got\nan accuracy of 90s.\nIn point two nine percent,\nwhich is really very good.\nAlright, so I hope the concept\nof this KNN algorithm is\nhere device in a world\nfull of machine learning\nand artificial intelligence\nsurrounding almost everything\naround us classification\nand prediction is one\nof the most important aspects\nof machine learning.\nSo before moving forward,\nlet's have a quick look\nat the agenda.\nI'll start off this video\nby explaining you guys\nwhat exactly is Nave biased\nthen we'll and what\nis Bayes theorem\nwhich serves as a logic\nbehind the name pass\nalgorithm going forward.\nI'll explain the steps involved\nin the neighbors\nalgorithm one by one\nand finally add finish\nof this video with a demo\non the Nave bass using\nthe SQL own package noun\na bass is a simple but\nsurprisingly powerful algorithm\nfrom penetrative analysis.\nIt is a classification technique\nbased on base theorem\nwith an assumption of\nIndependence among predictors.\nIt comprises of two parts,\nwhich is name.\nAnd bias in simple terms\nneighbors classifier assumes\nthat the presence\nof a particular feature\nin a class is unrelated\nto the presence\nof any other feature,\neven if this features\ndepend on each other\nor upon the existence\nof the other features,\nall of these properties\nindependently contribute\nto the probability\nwhether a fruit is an apple\nor an orange or a banana.\nSo that is why it\nis known as naive now naive\nbased model is easy to build\nand particularly useful\nfor very large data sets.\nIn probability Theory\nand statistics based theorem,\nwhich is already\nknown as the base law\nor the base rule describes\nthe probability of an event\nbased on prior knowledge\nof the conditions\nthat might be related\nto the event now paste\ntheorem is a way to figure\nout conditional probability.\nThe conditional probability\nis the probability\nof an event happening given\nthat it has some relationship\nto one or more other events.\nFor example, your probability\nof getting a parking space\nis connected to the time\nof the day you pass.\nWhere you park\nand what conventions are you\ngoing on at that time\nbased Serum is slightly\nmore nuanced in a nutshell.\nIt gives you an actual\nprobability of an event given\ninformation about the tests.\nNow, if you look\nat the definition\nof Bayes theorem,\nwe can see\nthat given a hypothesis H\nand the evidence\ne-base term states that\nthe relationship between the\nprobability of the hypothesis\nbefore getting the evidence\nwhich is the P of H\nand the probability\nof the hypothesis\nafter getting the evidence\nthat P of H given e\nis defined as probability\nof e given H into probability\nof H divided by probability of e\nit's rather confusing, right?\nSo let's take an example\nto understand this theorem.\nSo suppose I have\na deck of cards and\nif a single card is drawn\nfrom the deck of playing cards,\nthe probability that the card\nis a king is for by 52\nsince there are four Kings\nin a standard deck of 52 cards.\nNow if King is an event,\nthis card is a king.\nThe probability of King\nis given as 4 by 52\nthat is equal to 1 by 13.\nNow if the evidence is provided\nfor instance someone looks\nas the That the single card\nis a face card the probability\nof King given\nthat it's a face\ncan be calculated\nusing the base theorem\nby this formula.\nThe since every King\nis also a face card\nthe probability of face given\nthat it's a king is equal to 1\nand since there are\nthree face cards in each suit.\nThat is the chat king and queen.\nThe probability of the face card\nis equal to 12 by 52.\nThat is 3 by 30.\nNow using Bayes theorem we\ncan find out the probability\nof King given that it's a face\nso our final answer\ncomes to 1 by 3,\nwhich is also true.\nSo if you have a deck of cards\nwhich has having only faces now\nthere are three types of phases\nwhich are the chat king\nand queen so the probability\nthat it's the king is 1 by 3.\nNow.\nThis is the simple example\nof how based on works now\nif we look at the proof as in\nhow this Bayes theorem Evolved.\nSo here we have\nprobability of a given p\nand probability of B\ngiven a now for\na joint probability distribution\nover the sets A and B,\nthe probability of\na intersection B,\nthe conditional probability\nof a given B is defined\nas the probability\nof a intersection B divided\nby probability of B,\nand similarly probability of B,\ngiven a is defined as\nprobability of B intersection\na divided by probability\nof a now we can\nEquate probability of\na intersection p and probability\nof B intersection a as\nboth are the same thing now\nfrom this method\nas you can see,\nwe get our final\nbase theorem proof,\nwhich is the probability of a\ngiven b equals probability of B,\ngiven a into probability\nof P divided by\nthe probability of a now\nwhile this is the equation\nthat applies to\nany probability distribution\nover the events A and B.\nIt has a particular nice\ninterpretation in case\nwhere a is represented\nas the hypothesis h\nand B is represented\nas some observed evidence e\nin that case the formula is p\nof H given e is equal\nto P of e given H\ninto probability of H divided\nby probability of e now\nthis relates the probability\nof hypotheses before\ngetting the evidence,\nwhich is p of H\nto the probability\nof the hypothesis\nafter getting the evidence\nwhich is p of H given e\nfor this reason P of H is known\nas the prior probability\nwhile P of Each given e is known\nas the posterior probability\nand the factor\nthat relates the two is known as\nthe likelihood ratio Now using\nthis term space theorem\ncan be rephrased\nas the posterior\nprobability equals.\nThe prior probability\ntimes the likelihood ratio.\nSo now that we know the maths\nwhich is involved\nbehind the baster.\nMm.\nLet's see how we can implement\nthis in real life scenario.\nSo suppose we have a data set.\nIn which we have\nthe Outlook the humidity\nand we need to find out\nwhether we should play\nor not on that day.\nSo the Outlook can be\nsunny overcast rain\nand the humidity high normal\nand the wind are categorized\ninto two phases\nwhich are the weak\nand the strong winds.\nThe first of all will create\na frequency table using\neach attribute of the data set.\nSo the frequency table\nfor the Outlook looks\nlike this we have Sunny overcast\nand rainy the frequency table\nof humidity looks like this\nand Frequency table of when\nlooks like this we have strong\nand weak for wind and high\nand normal ranges for humidity.\nSo for each frequency table,\nwe will generate\na likelihood table now now\nthe likelihood table\ncontains the probability\nof a particular day\nsuppose we take the sunny\nand we take the play as yes\nand no so the probability\nof Sunny given\nthat we play yes is 3 by 10,\nwhich is 0.3 the\nprobability of X,\nwhich is the\nprobability of Sunny\nIs equal to 5 by 14 now,\nthese are all the terms\nwhich are just generated\nfrom the data\nwhich we have a\nand finally the probability\nof yes is 10 out of 14.\nSo if we have a look\nat the likelihood of yes given\nthat it's a sunny we\ncan see using Bayes theorem.\nIt's the probability\nof Sunny given yes\ninto probability of s divided\nby the probability of Sunny.\nSo we have all\nthe values here calculated.\nSo if you put\nthat in our base serum equation,\nwe get the likelihood of yes.\nA 0.59 similarly the likelihood\nof no can also be calculated\nhere is 0.40 now similarly.\nWe are going to create\nthe likelihood table\nfor both the humidity\nand the win there's a\nfor humidity the likelihood\nfor yes given the humidity\nis high is equal to 0.4\nto and the probability\nof playing know\ngiven the vent is high is 0.58.\nThe similarly for table wind\nthe probability of he has given\nthat the wind is week is 0.75\nand the probability of no given\nthat the win is week is 0.25\nnow suppose we have of day\nwhich has high rain\nwhich has high humidity\nand the wind is weak.\nSo should we play or not?\nThat's our for that?\nWe use the base theorem\nhere again the likelihood\nof yes on that day is equal\nto the probability\nof Outlook rain given\nthat it's a yes into probability\nof Magic given that say yes,\nand the probability of\nwhen that is we given\nthat it's we are playing yes\ninto the probability of yes,\nwhich equals to zero\npoint zero one nine\nand similarly the likelihood\nof know on that day is equal\nto zero point zero one six.\nNow if we look\nat the probability\nof yes for that day\nof playing we just\nneed to divide it\nwith the likelihood\nsome of both the yes\nand no so the probability\nof playing tomorrow,\nwhich is yes is 5\nwhereas the probability\nof not playing is equal to 0.45.\nNow.\nThis is based upon the data\nwhich we already have with us.\nSo now that you have an idea\nof what exactly is named bias\nhow it works and we have seen\nhow it can be implemented\non a particular data set.\nLet's see where it\nis used in the industry.\nThe started with our first\nindustrial use case,\nwhich is news categorization\nor we can use\nthe term text classification\nto broaden the spectrum\nof this algorithm news\nin the web are rapidly growing\nin the era of Information Age\nwhere each new site has\nits own different layout\nand categorization\nfor grouping news.\nNow these heterogeneity\nof layout and categorization\ncannot always satisfy\nindividual users need\nto remove these heterogeneity\nand classifying\nthe news articles.\nOwing to the user preference\nis a formidable task companies\nuse web crawler\nto extract useful text\nfrom HTML Pages\nthe news articles\nand each of these news articles\nis then tokenized now\nthese tokens are nothing\nbut the categories\nof the news now\nin order to achieve\nbetter classification result.\nWe remove the less\nsignificant Words,\nwhich are the stop was\nfrom the documents\nor the Articles\nand then we apply\nthe Nave base classifier\nfor classifying the news\ncontents based on the news.\nNow this is by far one\nof the best examples\nof Neighbors classifier,\nwhich is Spam filtering.\nNow.\nIt's the Nave\nBayes classifier are\na popular statistical technique\nfor email filtering.\nThey typically use bag\nof words features to identify\nat the spam email\nand approach commonly used\nin text classification as well.\nNow it works by correlating\nthe use of tokens,\nbut the spam and non-spam emails\nand then the Bayes theorem,\nwhich I explained\nearlier is used to\ncalculate the probability\nthat an email is\nor not a Spam so named\nby a Spam filtering is\na baseline technique\nfor dealing with Spam\nthat container itself\nto the emails need\nof an individual user\nand give low false positive\nspam detection rates\nthat are generally\nacceptable to users.\nIt is one of the oldest ways\nof doing spam filtering\nwith its roots\nin the 1990s particular words\nhave particular probabilities\nof occurring in spam.\nAnd and legitimate email\nas well for instance.\nMost emails users\nwill frequently encounter\nthe world lottery\nor the lucky draw a spam email,\nbut we'll sell them\nsee it in other emails.\nThe filter doesn't know\nthese probabilities in advance\nand must be friends.\nSo it can build them\nup to train the filter.\nThe user must manually indicate\nwhether a new email is Spam\nor not for all the words\nin each straining email.\nThe filter will\nadjust the probability\nthat each word will appear\nin a Spam or legitimate.\nOwl in the database now\nafter training the word\nprobabilities also known\nas the likelihood functions are\nused to compute the probability\nthat an email with a particular\nset of words as in in belongs\nto either category each word\nin the email contributes\nthe email spam probability.\nThis contribution is called\nthe posterior probability\nand is computed again\nusing the base 0\nthen the email spam probability\nis computed over all\nthe verse in the email\nand if the total exceeds\na certain threshold say\nOr 95% the filter will Mark\nthe email as spam.\nNow object detection is\nthe process of finding instances\nof real-world objects\nsuch as faces bicycles\nand buildings in images\nor video now object detection\nalgorithm typically\nuse extracted features\nand learning algorithm\nto recognize instance of\nan object category here again,\na bass plays an important\nrole of categorization\nand classification of object\nnow medical area.\nThis is increasingly voluminous\namount of electronic data,\nwhich are becoming more\nand more complicated.\nThe produced medical data\nhas certain characteristics\nthat make the analysis\nvery challenging and attractive\nas well among all\nthe different approaches.\nThe knave bias is used.\nIt is the most effective\nand efficient classification\nalgorithm and has\nbeen successfully applied\nto many medical problems\nempirical comparison\nof knave bias versus\nfive popular classifiers\non Medical data sets shows\nthat may bias is well suited\nfor medical application and has\nhigh performance in most\nof the examine medical problems.\nNow in the past various\ntesticle methods have been used\nfor modeling in the area\nof disease diagnosis.\nThese methods require\nprior assumptions and are\nless capable of dealing\nwith massive and complicated\nnonlinear and dependent data one\nof the main advantages\nof neighbor as approach\nwhich is appealing\nto Physicians is\nthat all the available\ninformation is used?\nTo explain the decision\nthis explanation seems\nto be natural for medical\ndiagnosis and prognosis.\nThat is it is very\nclose to the way\nhow physician diagnosed patients\nnow weather is one\nof the most influential factor\nin our daily life to an extent\nthat it may affect\nthe economy of a country\nthat depends on occupation\nlike agriculture.\nTherefore as a countermeasure\nto reduce the damage\ncaused by uncertainty\nin whether Behavior,\nthere should be an efficient way\nto print the weather now\nwhether projecting\nhas Challenging problem\nin the meteorological department\nsince ears even\nafter the technology skill\nand scientific\nadvancement the accuracy\nand protection of weather\nhas never been sufficient even\nin current day this domain\nremains as a research topic\nin which scientists\nand mathematicians are working\nto produce a model\nor an algorithm\nthat will accurately\npredict the weather now\na bias in approach\nbased model is created by\nwhere posterior probabilities\nare used to calculate\nthe likelihood of\neach class label for input.\nData instance and the one\nwith the maximum likelihood\nis considered as the resulting\noutput now earlier.\nWe saw a small implementation\nof this algorithm as well\nwhere we predicted\nwhether we should play\nor not based on the data,\nwhich we have collected earlier.\nNow, this is a python Library\nwhich is known as scikit-learn\nit helps to build in a bias\nand model in Python.\nNow, there are three types\nof named by ass model\nunder scikit-learn Library.\nThe first one is the caution.\nIt is used in classification\nand it Assumes\nthat the feature follow\na normal distribution.\nThe next we have is multinomial.\nIt is used for discrete counts.\nFor example, let's say we have\na text classification problem\nand here we\nconsider bernouli trials,\nwhich is one step further\nand instead of word\noccurring in the document.\nWe have count\nhow often word occurs\nin the document you\ncan think of it\nas a number of times\noutcomes number is observed\nin the given number of Trials.\nAnd finally we have\nthe bernouli type.\nOf neighbors.\nThe binomial model is useful\nif your feature vectors are\nbinary bag of words model\nwhere the once\nand the zeros are words occur\nin the document and the verse\nwhich do not occur\nin the document respectively\nbased on their data set.\nYou can choose any of\nthe given discussed model here,\nwhich is the gaussian\nthe multinomial or the bernouli.\nSo let's understand\nhow this algorithm works.\nAnd what are\nthe different steps?\nOne can take to create\na bison model and use knave bias\nto predict the output so\nhere to understand better.\nWe are going to predict\nthe onset of diabetes Now\nthis problem comprises\nof 768 observations\nof medical details\nfor Pima Indian patients.\nThe record describes\ninstantaneous measurement taken\nfrom the patient such as\nthe age the number\nof times pregnant\nand the blood work crew now all\nthe patients are women aged 21\nand Older and all\nthe attributes are numeric\nand the unit's vary\nfrom attribute to attribute.\nEach record has\na class value that indicate\nwhether the patient suffered\non onset of diabetes\nwithin five years\nare the measurements.\nNow.\nThese are classified as 0 now.\nI've broken the whole process\ndown into the following steps.\nThe first step\nis handling the data\nin which we load the data\nfrom the CSV file and split it\ninto training and test it\nas it's the second step\nis summarizing the data.\nIn which we summarize\nthe properties in the training\ndata sets so that we\ncan calculate the probabilities\nand make predictions.\nNow the third step comes is\nmaking a particular prediction.\nWe use the summaries\nof the data set to generate\na single prediction.\nAnd after that we generate\npredictions given a test data\nset and a summarized\ntraining data sets.\nAnd finally we evaluate\nthe accuracy of the predictions\nmade for a test data set\nas the percentage correct\nout of all the predictions made\nand finally We tied\ntogether and form.\nOur own model\nof nape is classifier.\nNow.\nThe first thing we need to do\nis load our data the data is\nin the CSV format\nwithout a header line\nor any codes.\nWe can open the file\nwith the open function\nand read the data lines\nusing the read functions\nin the CSV module.\nNow, we also need\nto convert the attributes\nthat were loaded as\nstrings into numbers\nso that we can work with them.\nSo let me show you\nhow this can be implemented now\nfor that you need to Tall python\non a system and use\nthe jupyter notebook\nor the python shell.\nHey, I'm using\nthe Anaconda Navigator\nwhich has all\nthe things required to do\nthe programming in Python.\nWe have the Jupiter lab.\nWe have the notebook.\nWe have the QT console.\nEven we have a studio as well.\nSo what you need to do is just\ninstall the Anaconda Navigator\nit comes with the pre\ninstalled python also,\nso the moment you click launch\non The jupyter Notebook.\nIt will take you\nto the Jupiter homepage\nin a local system and here you\ncan do programming in Python.\nSo let me just rename it as\nby my India diabetes.\nSo first, we need\nto load the data set.\nSo I'm creating here a function\nload CSV now before that.\nWe need to import\ncertain CSV the math\nand the random method.\nSo as you can see,\nI've created a load CSV function\nwhich will take the pie\nmy Indian diabetes\ndata dot CSV file using\nthe CSV dot read a method\nand then we are converting\nevery element of that data set\ninto float originally all\nthe ants are in string,\nbut we need to convert\nthem into floor\nfor all calculation purposes.\nThe next we need to split\nthe data into training data sets\nthat nay bias can use\nto make the prediction\nand this data set\nthat we can use to evaluate\nthe accuracy of the model.\nWe need to split the data\nset randomly into training\nand testing data set\nin the ratio of usually\nwhich is 7230.\nBut for this example,\nI'm going to use 67\nand 33 now 70 and 30 is a Ratio\nfor testing algorithms\nso you can play around\nwith this number.\nSo this is our split\ndata set function.\nNow the Navy base\nmodel is comprised\nof summary of the data\nin the training data set.\nNow this summary is then used\nwhile making predictions.\nNow the summary\nof the training data\ncollected involves the mean\nthe standard deviation\nof each attribute\nby class value now, for example,\nif there are two class values\nand seven numerical attributes,\nthen we need a mean\nand the standard deviation for\neach of these seven attributes\nand the class value\nwhich makes The 14\nattributes summaries\nso we can break the preparation\nof this summary down\ninto the following sub tasks\nwhich are the separating data\nby class calculating mean\ncalculating standard deviation\nsummarizing the data sets\nand summarizing\nattributes by class.\nSo the first task is to separate\nthe training data set\ninstances by class value\nso that we can calculate\nstatistics for each class.\nWe can do that by creating a map\nof each class value\nto a list of instances\nthat belong to the class.\nClass and sort the entire\ndataset of instances\ninto the appropriate list.\nNow the separate\nby class function just the same.\nSo as you can see\nthe function assumes\nthat the last attribute\nis the class value\nthe function returns a map\nof class value to the list\nof data instances next.\nWe need to calculate\nthe mean of each attribute\nfor a class value.\nNow, the mean is\nthe central middle or\nthe central tendency of the data\nand we use it as a middle\nof our gaussian distribution\nwhen Calculating\nthe probabilities.\nSo this is our function\nfor mean now.\nWe also need to calculate\nthe standard deviation\nof each attribute\nfor a class value.\nThe standard deviation\nis calculated as a square root\nof the variance\nand the variance\nis calculated as the average\nof the squared differences\nfor each attribute value\nfrom the mean now\none thing to note\nthat here is\nthat we are using\nn minus one method\nwhich subtracts one\nfrom the number\nof attributes values\nwhen calculating the variance.\nNow that we have the tools\nto summarize the data\nfor a given list of instances.\nWe can calculate the mean\nand standard deviation\nfor each attribute.\nNow that's if function groups\nthe values for each attribute\nacross our data instances\ninto their own lists\nso that we can compute the mean\nand standard deviation values\nfor each attribute.\nNow next comes the summarizing\nattributes by class.\nWe can pull it all together\nby first separating.\nOur training data sets\ninto instances groped by class\nthen calculating the summaries\nfor each a Should be now.\nWe are ready to make predictions\nusing the summaries prepared\nfrom our training data\nmaking patients involved\ncalculating the probability\nthat a given data instance\nbelong to each class then\nselecting the class\nwith the largest probability\nas a prediction.\nNow we can divide this whole\nmethod into four tasks\nwhich are the calculating\ngaussian probability density\nfunction calculating class\nprobability making a prediction\nand then estimating the accuracy\nnow to calculate the gaussian\nprobability density function.\nWe use the gaussian function\nto estimate the probability\nof a given attribute value\ngiven the node mean\nand the standard deviation\nof the attribute estimated\nfrom the training data.\nAs you can see\nthe parameters RX mean\nand the standard deviation now\nin the calculate\nprobability function,\nwe calculate the exponent first\nthen calculate the main division\nthis lets us fit the equation\nnicely into two lines.\nNow, the next task\nis calculating the\nclass properties now\nthat we had can calculate\nthe probability of an attribute\nbelonging to a class.\nWe can combine the probabilities\nof all the attributes values\nfor a data instance\nand come up with a probability\nof the entire.\nOur data instance\nbelonging to the class.\nSo now that we have calculated\nthe class properties.\nIt's time to finally make\nour first prediction now,\nwe can calculate the probability\nof the data instance belong\nto each class value\nand we can look\nfor the largest probability\nand return the associated class\nand for that we are going\nto use this function predict\nwhich uses the summaries\nand the input Vector which is\nbasically all the probabilities\nwhich are being input\nfor a particular label\nnow finally we can\nAn estimate the accuracy\nof the model\nby making predictions\nfor each data instances\nin our test data for that.\nWe use the get\npredictions method.\nNow this method is used\nto calculate the predictions\nbased upon the test data sets\nand the summary\nof the training data set.\nNow, the predictions\ncan be compared\nto the class values\nin our test data set\nand classification accuracy\ncan be calculated as\nan accuracy ratio\nbetween the zeros\nand the hundred percent.\nNow the get accuracy method will\ncalculate this accuracy ratio.\nNow finally to sum it all up.\nWe Define our main function\nwe call all these methods\nwhich we have defined\nearlier one by one to get\nthe Courtesy of the model\nwhich we have created.\nSo as you can see,\nthis is our main function\nin which we have the file name.\nWe have defined the split ratio.\nWe have the data set.\nWe have the training\nand test data set.\nWe are using the split\ndata set method next.\nWe are using the summarized\nby class function using\nthe get protection and\nthe get accuracy method as well.\nSo guys as you can see\nthe output of this one gives us\nthat we are splitting\nthe 768 Rose into 514\nwhich is the training and 254\nwhich is the test data set rows\nand the accuracy of this model\nis 68% Now we can play\nwith the amount of training\nand test data sets\nwhich are to be used\nso we can change\nthe split ratio to seventies.\n238 is 220 to get\ndifferent sort of accuracy.\nSo suppose I change\nthe split ratio from 0.67 20.8.\nSo as you can see,\nwe get the accuracy\nof 62 percent.\nSo splitting it into 0.67\ngave us a better result\nwhich was 68 percent.\nSo this is how you can Implement\nNavy bias caution classifier.\nThese are the step\nby step methods\nwhich you need to do in case of\nusing the Nave Bayes classifier,\nbut don't worry.\nWe do not need to write\nall this many lines\nof code to make a model\nthis with the second.\nAnd I really comes into picture\nthe scikit-learn library has\na predefined method\nor as say a predefined\nfunction of nape bias,\nwhich converts all\nof these lines,\nof course into merely just\ntwo or three lines of codes.\nSo, let me just open\nanother jupyter notebook.\nSo let me name it\nas sklearn a pass.\nNow here we are going to use\nthe most famous data set\nwhich is the iris De Casa.\nNow, the iris flower data\nset is a multivariate\ndata set introduced by\nthe British statistician\nand biologists Roland Fisher\nand based on this fish is linear\ndiscriminant model this data set\nbecame a typical test case\nfor many statistical\nclassification techniques\nin machine learning.\nSo here we are going to use\nthe caution NB model,\nwhich is already available\nin the sklearn.\nAs I mentioned earlier,\nthere were three\ntypes of Neighbors\nwhich are the question\nmultinomial and the bernouli.\nSo here we are going to use\nthe caution and be model\nwhich is already present\nin the SK loan Library,\nwhich is the cycle in library.\nSo first of all,\nwhat we need to do is\nimport the sklearn data sets\nand the metrics and we also need\nto import the caution NB Now\nonce all these libraries\nare lowered we need\nto load the data set\nwhich is the iris dataset.\nThe next what we need\nto do is fit a Nave\nby a smaller to this data set.\nSo as you can see we have so\neasily defined the model\nwhich is the gaussian\nNB which contains\nall the programming\nwhich I just showed you\nearlier all the methods\nwhich are taking the input\ncalculating the mean\nthe standard deviation\nseparating it bike last\nand finally making predictions.\nCalculating the\nprediction accuracy.\nAll of this comes\nunder the caution and be method\nwhich is inside already present\nin the sklearn library.\nWe just need to fit it\naccording to the data set\nwhich we have so next\nif we print the model we see\nwhich is the gaussian NB model.\nThe next what we need to do\nis make the predictions.\nSo the expected output\nis data set dot Target\nand the projected\nis using the pretend model\nand the model we are using\nis the cause in N be here.\nNow to summarize the model\nwhich created we calculate\nthe confusion Matrix\nand the classification report.\nSo guys, as you can see\nthe classification to provide\nwe have the Precision\nof Point Ninety Six,\nwe have the recall of 0.96.\nWe have the F1 score\nand the support and finally if\nwe print our confusion Matrix,\nas you can see it gives\nus this output.\nSo as you can see\nusing the gaussian\nand we method just\nputting it in the model\nand using any of the data.\nfitting the model\nwhich you created\ninto a particular data set\nand getting the desired\noutput is so easy\nwith the scikit-learn library\nas we Mo support\nVector machine is one\nof the most effective\nmachine learning classifier\nand it has been used\nin various Fields\nsuch as face recognition\ncancer classification\nand so on today's session\nis dedicated to how svm works\nthe various features of svm\nand how it Is used\nin the real world.\nAll right.\nOkay.\nNow let's move on and see\nwhat svm algorithm is all about.\nSo guys s VM\nor support Vector machine is\na supervised learning algorithm,\nwhich is mainly used to classify\ndata into different classes now\nunlike most algorithms svm\nmakes use of a hyperplane\nwhich acts like\na decision boundary\nbetween the various classes\nin general svm can\nbe used to generate\nmultiple separating hyperplanes\nso that the data\nis Divided into segments.\nOkay, and each\nof these segments will contain\nonly one kind of data.\nIt's mainly used\nfor classification purpose\nwearing you want to classify\nor data into two different\nsegments depending\non the features of the data.\nNow before moving any further,\nlet's discuss a few\nfeatures of svm.\nLike I mentioned earlier svm is\na supervised learning algorithm.\nThis means that svm trains\non a set of labeled data svm\nstudies the label training data\nand then classifies\nany new input Data,\ndepending on what it learned\nin the training phase\na main advantage\nof support Vector machine is\nthat it can be used\nfor both classification\nand regression problems.\nAll right.\nNow even though svm is mainly\nknown for classification the svr\nwhich is the support\nVector regressor is used\nfor regression problems.\nAll right, so svm can be used\nboth for classification.\nAnd for regression.\nNow, this is one of the reasons\nwhy a lot of people prefer svm\nbecause it's a very\ngood classifier and along\nThat it is also\nused for regression.\nOkay.\nAnother feature is the svm\nkernel functions svm can be used\nfor classifying nonlinear data\nby using the kernel trick\nthe kernel trick basically\nmeans to transform your data\ninto another dimension\nso that you can easily\ndraw a hyperplane\nbetween the different\nclasses of the data.\nAlright, nonlinear data\nis basically data\nwhich cannot be separated\nwith a straight line.\nAlright, so svm can even be used\non nonlinear data sets.\nYou just have to use\na A kernel functions to do this.\nAll right.\nSo guys, I hope\nyou all are clear\nwith the basic concepts of svm.\nNow, let's move on\nand look at how svm works\nso there's an order\nto understand how svm Works\nlet's consider a small scenario\nnow for a second pretend\nthat you own a firm.\nOkay, and let's say\nthat you have a problem\nand you want to set up a fence\nto protect your rabbits\nfrom the pack of wolves.\nOkay, but where do you\nbuild your films\none way to get around?\nThe problem is to build\na classifier based.\nOn the position of the rabbits\nand words in your pasture.\nSo what I'm telling you is\nyou can classify the group\nof rabbits as one group\nand draw a decision\nboundary between the rabbits\nand the world correct.\nSo if I do that and if I try\nto draw a decision boundary\nbetween the rabbits\nand the Wolves,\nit looks something like this.\nOkay.\nNow you can clearly build\na fence along this line\nin simple terms.\nThis is exactly\nhow SPM work it draws\na decision boundary,\nwhich is a hyperplane\nbetween any New classes\nin order to separate them\nor classify them now.\nI know you're thinking\nhow do you know\nwhere to draw a hyperplane\nthe basic principle behind\nsvm is to draw a hyperplane\nthat best separates\nthe two classes\nin our case the two glasses\nof the rabbits and the Wolves.\nSo you start off by drawing\na random hyperplane\nand then you check the distance\nbetween the hyperplane\nand the closest data points\nfrom each Club these closes\non your is data points\nto the hyperplane are known\nas support vectors.\nAnd that's where the name comes\nfrom support Vector machine.\nSo basically the\nhyperplane is drawn\nbased on these support vectors.\nSo guys an optimal\nhyperplane will have\na maximum distance from each\nof these support vectors.\nAll right.\nSo basically the hyperplane\nwhich has the maximum distance\nfrom the support vectors is\nthe most optimal hyperplane\nand this distance\nbetween the hyperplane\nand the support vectors\nis known as the margin.\nAll right,\nso to sum it up svm\nis used to classify data.\nBy using a hyper plane such\nthat the distance\nbetween the hyperplane and\nthe support vectors is maximum.\nSo basically your margin\nhas to be maximum.\nAll right, that way,\nyou know that you're actually\nseparating your classes or add\nbecause the distance between\nthe two classes is maximum.\nOkay.\nNow, let's try\nto solve a problem.\nOkay.\nSo let's say that I input\na new data point.\nOkay.\nThis is a new data point\nand now I want to draw\na hyper plane such\nthat it best separates\nthe two classes.\nOkay, so I start off\nby drawing a hyperplane.\nLike this and then\nI check the distance\nbetween the hyperplane\nand the support vectors.\nOkay, so I'm trying to check\nif the margin is maximum\nfor this hyper plane,\nbut what if I draw a hyperplane\nwhich is like this?\nAll right.\nNow I'm going to check\nthe support vectors over here.\nThen I'm going\nto check the distance\nfrom the support vectors and for\nthis hyperplane, it's clear\nthat the margin is more red.\nWhen you compare the margin\nof the previous one\nto this hyperplane.\nIt is more.\nSo the reason why I'm choosing\nthis hyperplane is\nbecause the Distance\nbetween the support vectors\nand the hyperplane is maximum\nin this scenario.\nOkay.\nSo guys, this is\nhow you choose a hyperplane.\nYou basically have to make sure\nthat the hyper plane\nhas a maximum.\nMargin.\nAll right, it has to best\nseparate the two classes.\nAll right.\nOkay so far it was quite easy.\nOur data was linearly separable\nwhich means that you\ncould draw a straight line\nto separate the two classes.\nAll right, but what will you do?\nIf the data set is like this\nyou possibly can't draw\na hyperplane like Is on it,\nit doesn't separate\nthe two classes at all.\nSo what do you do\nin such situations now earlier\nin the session I mentioned\nhow a kernel can be used\nto transform data\ninto another dimension\nthat has a clear dividing margin\nbetween the classes of data.\nAlright, so kernel functions\noffer the user this option\nof transforming nonlinear spaces\ninto linear ones.\nNonlinear data set is the one\nthat you can't separate\nusing a straight line.\nAll right.\nIn order to deal\nwith such data sets,\nyou're going to transform them\ninto linear data sets\nand then use svm on them.\nOkay.\nSo simple trick would be\nto transform the two variables\nX and Y into a new\nfeature space involving\na new variable called Z.\nAll right, so guys so far\nwe were plotting our data\non two dimensional space.\nCorrect?\nWe will only using the X\nand the y axis so we had only\nthose two variables X and Y now\nin order to deal with this kind\nof data a simple trick.\nBe to transform\nthe two variables X\nand Y into a new feature space\ninvolving a new variable\ncalled Z. Okay,\nso we're basically\nvisualizing the data\non a three-dimensional space.\nNow when you transform\nthe 2D space into a 3D space\nyou can clearly see\na dividing margin\nbetween the two classes\nof data right now.\nYou can go ahead\nand separate the two classes\nby drawing the best\nhyperplane between them.\nOkay, that's exactly\nwhat we discussed\nin the previous slides.\nSo guys, why don't you\ntry this yourself dried.\nDrawing a hyperplane,\nwhich is the most Optimum\nfor these two classes.\nAll right, so guys,\nI hope you have\na good understanding\nabout nonlinear svm's now.\nLet's look at a real\nworld use case\nif support Vector machines.\nSo guys s VM\nas a classifier has been used\nin cancer classification\nsince the early 2000s.\nSo there was an experiment held\nby a group of professionals\nwho applied svm in a colon\ncancer tissue classification.\nSo the data set\nconsisted of about\nTransmembrane protein samples\nand only about 50 to 200\ngenes samples were input\nInto the svm classifier\nNow this sample\nwhich was input\ninto the svm classifier had\nboth colon cancer tissue samples\nand normal colon tissue\nsamples right now.\nThe main objective of this study\nwas to classify Gene samples\nbased on whether they\nare cancerous or not.\nOkay, so svm was trained\nusing the 50 to 200 samples\nin order to discriminate\nbetween non-tumor\nfrom A tumor specimens.\nSo the performance\nof the svm classifier\nwas very accurate\nfor even a small data set.\nAll right, we had only\n50 to 200 samples and even\nfor the small data set\nsvm was pretty accurate\nwith this results.\nNot only that its\nperformance was compared\nto other classification\nalgorithms like naive Bayes\nand in each case svm\noutperform naive Bayes.\nSo after this experiment\nit was clear\nthat svm classified the data\nmore effectively and it\nworked exceptionally good.\nSmall data sets.\nLet's go ahead\nand understand what exactly\nis unsupervised learning.\nSo sometimes the given data\nis unstructured and unlabeled\nso it becomes difficult\nto classify the data\ninto different categories.\nSo unsupervised learning\nhelps to solve this problem.\nThis learning is used\nto Cluster the input data\nand classes on the basis\nof their statistical properties.\nSo example, we can cluster\nDifferent Bikes based\nupon the speed limit there.\nAcceleration or the average\nthat they are giving so\nand suppose learning is a type\nof machine learning algorithm\nused to draw inferences\nfrom beta sets consisting\nof input data\nwithout labeled responses.\nSo if you have a look\nat the workflow\nor the process flow\nof unsupervised learning,\nso the training data is\ncollection of information\nwithout any label.\nWe have the machine\nlearning algorithm\nand then we have\nthe clustering malls.\nSo what it does is\nthat distributes the data\ninto a different class.\nAnd again, if you provide\nany unreliable new data,\nit will make a prediction\nand find out to which cluster\nthat particular data\nor the data set belongs\nto or the particular data point\nbelongs to so one\nof the most important\nalgorithms in unsupervised\nlearning is clustering.\nSo let's understand exactly\nwhat is clustering.\nSo a clustering\nbasically is the process\nof dividing the data sets\ninto groups consisting\nof similar data points.\nIt means grouping\nof objects based\non the information found in\nthe data describing the object.\nObjects or their relationships\nso clustering malls focus on\nand defying groups\nof similar records\nand labeling records\naccording to the group to which\nthey belong now this is done\nwithout the benefit\nof prior knowledge\nabout the groups\nand their characteristics.\nSo and in fact,\nwe may not even know exactly\nhow many groups are\nthere to look for.\nNow.\nThese models are often\nreferred to as\nunsupervised learning models,\nsince there's no external\nstandard by which to judge.\nOne is\nclassification performance.\nThere are no right or wrong\nanswers to these model.\nAnd if we talk about why\nclustering is used\nso the goal of clustering\nis to determine\nthe intrinsic group in a set\nof unlabeled data sometime.\nThe partitioning is the goal\nor the purpose of clustering\nalgorithm is to make sense\nof and exact value\nfrom the last set of structured\nand unstructured data.\nSo that is why clustering\nis used in the industry and\nif you have a look at the video,\nThese use cases of clustering\nin the industry.\nSo first of all,\nit's being used in marketing.\nSo discovering distinct groups\nin customer databases\nsuch as customers\nwho make a lot\nof long-distance calls customers\nwho use internet more\nthan cause they're also\nusing insurance companies.\nSo like I need to find groups\nof Corporation insurance policy\nholders with high average\nclaim rate Farmers crash cops,\nwhich is profitable.\nThey are using C Smith studies\nand defined problem areas of Oil\nor gas exploration\nBased on seesmic data,\nand they're also used\nin the recommendation of movies.\nIf you would say they\nare also used in Flickr photos.\nThey also used by Amazon\nfor recommending the product\nwhich category it lies in.\nSo basically if we talk\nabout clustering there are\nthree types of clustering.\nSo first of all,\nwe have the exclusive clustering\nwhich is the hard clustering\nso here and item belongs\nexclusively to one cluster\nnot several clusters\nand the data point.\nAlong exclusively\nto one cluster.\nSo an example of this is\nthe k-means clustering\nso k-means clustering\ndoes this exclusive kind\nof clustering so secondly,\nwe have overlapping clustering\nso it is also known as\nsoft clusters in this\nand item can belong\nto multiple clusters as\nits degree of association\nwith each cluster\nis shown and for example,\nwe have fuzzy\nor the c means clustering\nwhich is being used\nfor overlapping clustering\nand finally we have\nThe hierarchical clustering\nso when two clusters have\na parent-child relationship\nor a tree-like structure,\nthen it is known\nas hierarchical cluster.\nSo as you can see here\nfrom the example,\nwe have a parent-child kind\nof relationship in\nthe cluster given here.\nSo let's understand\nwhat exactly is\nK means clustering.\nSo k-means clustering is\nan algorithm whose main goal\nis to group similar elements\nof data points into a cluster\nand it is the process\nby which objects are classified\ninto a predefined number.\nOf groups so that they\nare as much dissimilar as\npossible from one group\nto another group\nbut as much as similar or\npossible within each group now\nif you have a look at the\nalgorithm working here, right?\nSo first of all,\nit starts with and defying\nthe number of clusters,\nwhich is k then I can we find\nthe centroid we find\nthe distance objects\nto the distance object\nto the centroid distance\nof object to the centroid\nthen we find the Dropping based\non the minimum distance has\nthe centroid Converse\nif true then we make\na cluster false.\nWe then I can't find\nthe centroid repeat\nall of the steps\nagain and again,\nso let me show you\nhow exactly clustering was\nwith an example here.\nSo first we need\nto decide the number\nof clusters to be made now\nanother important task here is\nhow to decide the important\nnumber of clusters\nor how to decide the number\nof clusters really get\ninto that later.\nSo first, let's assume\nthat the number Number\nof clusters we have decided\nis 3 so after that then\nwe provide the centroids\nfor all the Clusters\nwhich is guessing\nand the algorithm calculates\nthe euclidean distance\nof the point from each centroid\nand assigns the data point\nto the closest cluster\nnow euclidean distance.\nAll of you know\nis the square root\nof the distance the square root\nof the square of the distance.\nSo next when the center\nis a calculated again,\nwe have our new clusters\nfor each data point.\nAnd again the distance\nfrom the points\nto the new clusters\nare calculated and then again,\nthe points are assigned\nto the closest cluster.\nAnd then again,\nwe have the new centroid\nscattered and now\nthese steps are repeated\nuntil we have\na repetition the centroids\nor the new center eyes are very\nclose to the very previous ones.\nSo antenna and less\noutput gets repeated\nor the outputs are\nvery very close enough.\nWe do not stop this process.\nWe keep on calculating\nthe euclidean distance.\nIt's of all the points\nto the centroids.\nThen we calculate\nthe new centroids\nand that is how clay means\nclustering Works basically,\nso an important part\nhere is to understand\nhow to decide the value of K\nor the number of clusters\nbecause it does\nnot make any sense.\nIf you do not know\nhow many classes\nare you going to make?\nSo to decide\nthe number of clusters,\nwe have the elbow method.\nSo let's assume first\nof all compute\nthe sum squared error,\nwhich is the sse4 some value.\nA for example,\nlet's take two four six\nand eight now the SS e\nwhich is the sum squared\nis defined as a sum\nof the squared distance\nbetween each number member\nof the cluster\nand its centroid\nmathematically and\nif you mathematically it\nis given by the equation\nwhich is provided here.\nAnd if you brought\nthe key against the SSE,\nyou will see\nthat the error decreases\nas K gets large now this is\nbecause the number\nof cluster increases\nthey should be smaller.\nSo does this torsion is\nalso smaller know the idea\nof the elbow method is\nto choose the K at which\nthe SSC decreases abruptly.\nSo for example here\nif we have a look\nat the figure given here.\nWe see that the best number\nof cluster is at the elbow\nas you can see here the graph\nhere changes abruptly\nafter number four.\nSo for this particular example,\nwe're going to use\nfor as a number of cluster.\nSo first of all while working\nwith k-means clustering\nthere are two key points,\nAs to know first of all\nbe careful about various start.\nSo choosing the first center\nat random choosing\nthe second center\nthat is far away from the first\ncenter similarly choosing\nthe NIH Center as far away\nas possible from the closest\nof the all the other centers\nand the second idea\nis to do as many runs\nof k-means each with different\nrandom starting points\nso that you get an idea\nof where exactly\nand how many clusters\nyou need to make and\nwhere exactly the centroid lies.\nAnd how the data\nis getting confused\nnow k-means is not exactly\na very good method.\nSo let's understand the pros and\ncons of clay means clusterings.\nWe know that k-means is simple\nand understandable.\nEveryone loves you\nthat the first go\nthe items automatically assigned\nto the Clusters.\nNow if we have\na look at the cons,\nso first of all one needs to\ndefine the number of clusters,\nthere's a very\nheavy task asks us\nif we have 3/4 or\nif we have 10 categories\nand if we do not know\nwhat the number\nof clusters are going to be.\nIt's Difficult for anyone\nto you know to guess the number\nof clusters not all items\nare forced into clusters\nwhether they are actually belong\nto any other cluster\nor any other category,\nthey are forced to to lie\nin that other category\nin which they are closest\nto this against happens\nbecause of the number\nof clusters with not defining\nthe correct number of clusters\nor not being able to guess\nthe correct number of clusters.\nSo and most of all it's unable\nto handle the noisy data and\nthe outliners because anyway,\nAs machine learning engineers\nand data scientists\nhave to clean the data.\nBut then again it comes down\nto the analysis watch they\nare doing and the method\nthat they are using so typically\npeople do not clean the data\nfor k-means clustering even\nif the clean there's\nsometimes a now see noisy\nand outliners data\nwhich affect the whole model\nso that was all\nfor k-means clustering.\nSo what we're going to do\nis now use k-means clustering\nfor the We data set\nso we have to find out\nthe number of clusters\nand divide it accordingly.\nSo the use case is\nthat first of all,\nwe have a data set\nof five thousand movies.\nAnd what you want\nto do is grip them\nif the movies into clusters\nbased on the Facebook likes,\nso guys, let's have a look\nat the demo here.\nSo first of all,\nwhat we're going to do is\nimport deep copy numpy pandas\nSeaborn the various libraries,\nwhich we're going to use now\nand from map popular videos.\nIn the use ply plot,\nand we're going to use\nthis ggplot and next\nwhat we're going to do\nis import the data set\nand look at the shape\nof the data is it\nso if we have a look at the\nshape of the data set we can see\nthat it has 5043 rows\nwith Twenty Eight columns.\nAnd if you have a look\nat the head of the data set we\ncan see it has 5043 data points,\nso What we're going to do\nis place the data points\nin the plot me take\nthe director Facebook likes\nand we have a look\nat the data columns face number\nand post cars total\nFacebook likes director\nFacebook likes.\nSo what we have done here\nnow is taking the director\nFacebook likes and the actor\nthree Facebook likes, right.\nSo we have five thousand\nforty three rows\nand two columns Now using\nthe k-means from sklearn\nwhat we're going\nto do is import it.\nFirst we're going\nto import k-means\nfrom sklearn dot cluster.\nRemember guys Escalon is\na very important library\nin Python for machine learning.\nSo and the number of cluster\nwhat we're going to do is\nprovide as five now this again,\nthe number of cluster\ndepends upon the SSE,\nwhich is the sum\nof squared errors\nor the we're going\nto use the elbow method.\nSo I'm not going to go\ninto the details of that again.\nSo we're going to fit the data\ninto the k-means to fit and\nif you find the cluster,\nUs then for the\nk-means and printed.\nSo what we find is is\nan array of five clusters\nand Fa print the label\nof the Caymans cluster.\nNow next what we're going\nto do is plot the data\nwhich we have with the Clusters\nwith the new data clusters,\nwhich we have found\nand for this we're going\nto use the si bon and\nas you can see here,\nwe have plotted that car.\nWe have plotted\nthe data into the grid\nand You can see here.\nWe have five clusters.\nSo probably what I would say is\nthat the cluster\n3 and the cluster\nzero are very very close.\nSo it might depend\nsee that's exactly\nwhat I was going to say.\nIs that initially\nthe main Challenge\nand k-means clustering is\nto define the number of centers\nwhich are the K.\nSo as you can see here\nthat the third Center\nand the zeroth cluster\nthe third cluster\nand the zeroth cluster up\nvery very close to each other\nso It probably could have been\nin one another cluster\nand the another disadvantage was\nthat we do not exactly know\nhow the points are\nto be arranged.\nSo it's very difficult to force\nthe data into any other cluster\nwhich makes our analysis\na little different works fine.\nBut sometimes it\nmight be difficult to code\nin the k-means clustering now,\nlet's understand what exactly\nis seems clustering.\nSo the fuzzy c means\nis an extension of the k-means\nclustering the popular simple.\nClustering technique so\nfuzzy clustering also referred\nas soft clustering is a form\nof clustering in which\neach data point can belong\nto more than one cluster.\nSo k-means tries to find\nthe heart clusters\nwhere each point belongs\nto one cluster.\nWhereas the fuzzy c means\ndiscovers the soft clusters\nin a soft cluster\nany point can belong\nto more than one cluster\nat a time with\na certain Affinity value\ntowards each 4zc means assigns\nthe degree of membership,\nwhich Just from 0 to 1\nto an object to a given cluster.\nSo there is a stipulation\nthat the sum of the membership\nof an object to all the cluster.\nIt belongs to must be equal\nto 1 so the degree of membership\nof this particular point to pull\nof these clusters as 0.6 0.4.\nAnd if you add up we get 1\nso that is one of the logic\nbehind the fuzzy c means\nso and and this Affinity\nis proportional to the distance\nfrom the point to the center\nof the cluster now then again\nNow we have the pros\nand cons of fuzzy see means.\nSo first of all,\nit allows a data point to be\nin multiple cluster.\nThat's a pro.\nIt's a more neutral\nrepresentation of the behavior\nof jeans jeans usually are\ninvolved in multiple functions.\nSo it is a very\ngood type of clustering\nwhen we're talking\nabout genes First of and again,\nif we talk about the cons again,\nwe have to Define c\nwhich is the number\nof clusters same as K next.\nWe need to determine the\nmembership cutoff value also,\nso that takes a lot of Time\nand it's time-consuming\nand the Clusters\nare sensitive to initial\nassignment of centroid.\nSo a slight change\nor deviation from the center\nhas it's going to result\nin a very different\nkind of, you know,\na funny kind of output we get\nfrom the fuzzy c means and one\nof the major disadvantage\nof see means clustering is\nthat it's this are\nnon deterministic algorithm.\nSo it does not give you\na particular output\nas in such that's\nthat now let's have a look.\nAt the third type\nof clustering which is\nthe hierarchical clustering.\nSo hierarchical clustering\nis an alternative approach\nwhich builds a hierarchy\nfrom the bottom up\nor the top to bottom\nand does not require\nto specify the number\nof clusters beforehand.\nNow, the algorithm works\nas in first of all,\nwe put each data point\nin its own cluster and\nif I the closest to Cluster\nand combine them into one more\ncluster repeat the above step\ntill the data points are\nin a single cluster.\nNow, there are two types of\nhierarchical clustering one is\nI've number 80 plus string\nand the other one\nis division clustering.\nSo a commemorative\nclustering bills the dendogram\nfrom bottom level\nwhile the division clustering\nit starts all the data points\nin one cluster\nthe fruit cluster now again\nhierarchical clustering also\nhas some sort of pros and cons.\nSo in the pros\ndon't know Assumption\nof a particular number\nof cluster is required\nand it may correspond\nto meaningful taxonomist.\nWhereas if we talk\nabout the cons\nonce a decision is made\nto combine two clusters.\nHas it cannot be undone and one\nof the major disadvantage of\nthese hierarchical clustering is\nthat it becomes very slow.\nIf we talked about very very\nlarge data sets and nowadays.\nI think every industry are using\nlast year as its and collecting\nlarge amounts of data.\nSo hierarchical clustering\nis not the act\nor the best method someone\nmight need to go for so there's\nthat now when we talk\nabout unsupervised learning,\nso we have K means\nclustering and again,\nAnother important term\nwhich people usually Miss while\ntalking about us was running and\nthere's one very\nimportant concept\nof Market Basket analysis.\nNow, it is one\nof the key techniques\nused by large retailers\nto uncover association\nbetween items now\nit works by looking\nfor combination of items\nthat occur together frequently\nin the transactions\nto put it in other way.\nIt allows retailers\nto identify the relationships\nbetween the items\nthat the People by\nfor example people\nwho buy bread also tend to buy\nbutter the marketing team\nat the retail stores\nshould Target customers\nwho buy bread and butter\nand provide them and offer\nso that they buy\na third item like an egg.\nSo if a customer buys bread\nand butter and sees a discount\nor an offer on X,\nhe will be encouraged to spend\nmore money and buy the eggs.\nNow, this is what Market Basket\nanalysis is all about now\nto find the association\nbetween the two items\nand make predictions about\nwhat the customers will buy.\nThere are two Cartoons which are\nthe association rule Mining\nand the ebrary algorithms.\nSo let's discuss each\nof these algorithm\nwith an example.\nFirst of all,\nif we have a look at\nthe association rule mining now,\nit's a technique that's shows\nhow items are associated to\neach other for example customers\nwho purchased spread have\na 60 percent likelihood\nof also purchasing\njam and customers\nwho purchase laptop are more\nlikely to purchase laptop bags.\nNow if you take an example\nof an association rule\nif we have a look\nat the Example here a arrow B.\nIt means that\nif a person buys an atom a then\nhe will also buy an atom P. Now.\nThere are three common ways to\nmeasure a particular Association\nbecause we have to find\nthese rules not on the basis\nof some statistics, right?\nSo what we do is use\nsupport confidence and lift\nnow these three common ways\nand the measures to have a look\nat the association rule\nMining and know exactly\nhow good is that rule.\nSo first of all,\nwe have support So support\ngifts the fraction of the\nWhich contains an item A and B.\nSo it's basically\nthe frequency of the item\nin the whole item set.\nWhere's confidence gifts\nhow often the item\nA and B occurred together\ngiven the number\nof item given the number\nof times a occur.\nSo it's frequency\na comma B divided by\nthe frequency of a now left\nwhat indicates is the strength\nof the rule over the random\nco-occurrence of A and B.\nIf you have a close look\nat the denominator\nof the lift formula here,\nwe have support a into support\nbe and now a major thing\nwhich can be noted from this is\nthat the support of A\nand B are independent here.\nSo if the value of lift\nor the denominator value\nof the lift is more it means\nthat the items are independently\nselling more not together.\nSo that in turn will decrease\nthe value of lift.\nSo what happens is\nthat suppose the value\nof lift is more that implies\nthat the rule which we get.\nIt implies that the rule\nis strong and it\nAnd we used for later purposes\nbecause in that case the support\nin to support P value,\nwhich is the denominator\nof lift will be low\nwhich in turn means\nthat there is a relationship\nbetween the items in the and B.\nSo let's take an example\nof Association rule Mining\nand understand how\nexactly it works.\nSo let's suppose we have\na set of items a b c d\nand e and we have\nthe set of transactions\nwhich are T1 T2,\nT3, T4 and T5\nand what we need to do is\ncreate some sort of Rules,\nfor example, you can see a d\nwhich means that\nif a person buys a he buys D\nif a person by see he buys a\nif a person buys a he by C.\nAnd for the fourth one is\nif a person by B\nand C Hill in turn by a now\nwhat we need to do is calculate\nthe support confidence and lift\nof these rules now here again,\nwe talked about\na priority algorithm.\nSo a priori algorithm\nand the association rule mining\ngo hand in hand.\nSo what a predator\nThis algorithm.\nIt uses the frequent itemsets to\ngenerate the association rules\nand it is based on the concept\nthat a subset\nof a frequent itemsets must also\nbe a frequent Isom set.\nSo let's understand\nwhat is a frequent item set and\nhow all of these work together.\nSo if we take the following\ntransactions of items,\nwe have transaction T\n1 2 T 5 and the items are 1 3 4\n2 3 5 1 2 3 5 2 5 and 1 3 5 now.\nNow another more\nimportant thing about support\nwhich I forgot to mention was\nthat when talking\nabout Association rule mining\nthere is a minimum support count\nwhat we need to do.\nNow.\nThe first step is\nto build a list of items\nthat of size 1 using\nthis transaction data\nset and use the minimum\nsupport count to now,\nlet's see how we do\nthat if we create the table see\nwhen you have a close look\nat the table c 1\nwe have the items at one\nwhich has support three\nbecause it appears\nin the transaction one.\nThree and five similarly\nif you have a look at the item\nset the single item 3.\nSo it has the support\nof for it appears\nin t 1 T 2 T 3 and T 5 but\nif we have a look at the item\nset for it only appears\nin the transaction once\nso it's support value is\n1 now the item set with\nthe support value Which is less\nthan the minimum support value\nthat is to have\nto be eliminated.\nSo the final table\nwhich is a table F1\nhas one two three.\nAnd five it does not\ncontain the for now.\nWhat we're going to do is\ncreate the item list of the size\n2 and all the combination\nof the item sets in F1.\nI used in this iteration.\nSo we're left for behind.\nWe just have 1 2 3 & 5.\nSo the possible item\nsets a 1 2 1 3\n1 5 2 3 2 5 & 3 5 then again.\nWe will calculate the support So\nin this case if we have\na closer look at the table\nc 2 we see\nthat the items at once.\nWhat to do is having\na support value 1\nwhich has to be eliminated.\nSo the final table f 2 does\nnot contain 1 comma 2 similarly\nif we create the item sets\nof size 3 and calculate\nthis support values,\nbut before calculating\nthe support, let's perform\nthe puring on the data set.\nNow what's appearing?\nSo after all the combinations\nare made we divide the table\nc 3 items to check\nif there are another subset\nwhose support is less\nthan the minimum support value.\nThis is a prairie algorithm.\nSo in the item sets one, two,\nthree what we can see\nthat we have one two,\nand in the one to five again,\nwe have one too so build this\ncardboard of these item sets\nand we'll be left\nwith 1 3 5 and 2 3 5.\nSo with one three five,\nwe have three subsets\none five one, three three five,\nwhich are present in table F2.\nThen again.\nWe have two three\nto five and 3/5\nwhich are also present\nin t will f 2\nso we have 2 Move 1 comma\n2 from the table c\n3 and create the table F3 now\nif you're using the items of C3\nto create the atoms of C-4.\nSo what we find is\nthat we have the item set\n1 2 3 5 the support value is\n1 Which is less than\nthe minimum support value of 2.\nSo what we're going\nto do is stop here\nand we're going to return\nto the previous item set.\nThat is the table\nc 3 so the final table.\nWell, if three was\none three five with\nthe support value of 2 and 2 3 5\nwith the support value of 2 now,\nwhat we're gonna do is\ngenerate all the subsets\nof each frequent itemsets.\nSo let's assume\nthat minimum confidence value is\n60% So for every subset s\nof I the output rule is\nthat s gives i2s\nis that s recommends i ns.\nIf the support of I / support\nof s is greater than or equal.\nEqual to the minimum\nconfidence value,\nthen only will proceed further.\nSo keep in mind\nthat we have not used\nleft till now.\nWe are only working\nwith support and confidence.\nSo applying rules\nwith item sets of F3\nwe get rule 1 which is 1 comma\n3 which gives 1 3 5 and 1/3.\nIt means if you buy one\nand three there's a 66% chance\nthat you will buy item 5 also\nsimilarly the rule\n1 comma 5 it means\nthat If you buy one and five,\nthere's a hundred percent chance\nthat you will buy\nthree also similarly\nif we have a look\nat Rule 5 and 6 here\nthe confidence value is\nless than 60 percent which was\nthe assumed confidence value.\nSo what we're going to do is\nwith reject these files now\nan important thing\nto note here is\nthat have a closer look\nto the Rule 5 and root 3,\nyou see it has one five three\none five three three point five.\nIt's very confusing.\nSo one thing to keep in Mine is\nthat the order of the item sets\nis also very important\nthat will help us\nallow create good rules\nand avoid any kind of confusion.\nSo that's that.\nSo now let's learn\nhow Association rule I used in\nMarket Basket analysis problems.\nSo what we'll do\nis we will be using\nthe online transactions data\nof a retail store for\ngenerating Association rules.\nSo first of all,\nwhat you need to do is\nimport pandas MSD ml.\nD&D libraries from the imported\nand read the data.\nSo first of all, what we're\ngoing to do is read the data,\nwhat we're going\nto do is from M LX T\nand E dot frequent patterns.\nWe're going to improve the a\npriori and Association rules.\nAs you can see here.\nWe have the head of the data.\nYou can see we have invoice\nnumber stock code\nthe description quantity\nthe invoice dt8 unit price\ncustomer ID and the country.\nSo in the next step,\nwhat we will do is we\nwill do the data cleanup\nwhich includes removing.\nHis from some\nof the descriptions given\nand what we're going\nto do is drop the rules\nthat do not have\nthe invoice numbers every move\nthe crate transactions.\nSo hey, what what you're\ngoing to do is remove\nwhich do not have\nany invoice number\nif the string tight\nainst Epstein was a number then\nwe're going to remove that.\nThose are the credits remove\nany kind of spaces\nfrom the descriptions.\nSo as you can see here,\nwe have like five hundred\nand thirty-two thousand rows\nwith eight columns.\nSo next one.\nWe're going to do is\nafter the cleanup.\nWe need to consolidate the items\ninto one transaction per row\nwith each product for the sake\nof keeping the data set small.\nWe're going to only look\nat the sales for France.\nSo we're going to use\nthe only France and group\nby invoice number description\nwith the quantity sum\nup and see so\nwhich leaves us\nwith three ninety two rows\nand one thousand five\nhundred sixty three columns.\nNow, there are a lot\nof zeros in the data,\nbut we also need to make sure\nAny positive values\nare converted to a 1 and\nanything less than 0 is set to 0\nso for that we're going\nto use this code defining\nand code units\nif x is less than\n0 return 0 if x\nis greater than 1 returned one.\nSo what we're going to do is map\nand apply it to the whole data\nset we have here.\nSo now that we\nhave structured data properly.\nSo the next step is to generate\nthe frequent item set\nthat has support of at\nleast seven percent.\nNow this number is chosen so\nthat you can get close enough.\nNow, what we're going\nto do is generate the rules\nwith the corresponding\nsupport confidence and lift.\nSo we had given\nthe minimum support a 0.7.\nThe metric is left\nfrequent Island set\nand threshold is 1 so these are\nthe following rules now a few\nrules with a high lift value,\nwhich means that it\noccurs more frequently\nthan would be expected\ngiven the number of transaction\nthe product combinations most\nof the places the confidence.\nIs high as well.\nSo these are few to observations\nwhat we get here.\nIf we filter the data frame\nusing the standard pandas code\nfor large lift six\nand high confidence 0.8.\nThis is what the output\nis going to look like.\nThese are 1 2 3 4 5 6 7 8.\nSo as you can see here,\nwe have the H rules\nwhich are the final rules\nwhich are given by\nthe Association rule Mining\nand this is how all\nthe industries are.\nAre any of these we've talked\nabout largely retailers.\nThey tend to know\nhow their products are used\nand how exactly they\nshould rearrange and provide\nthe offers on the product\nso that people spend\nmore and more money\nand time in the shop.\nSo that was all\nabout Association rule mining.\nSo so guys,\nthat's all for\nunsupervised learning.\nI hope you got to know\nabout the different formulas\nhow unsupervised learning works\nbecause you know,\nwe did not provide\nany label to the data.\nAll we did was create some rules\nand not knowing what the data is\nand we did clusterings\ndifferent types of clusterings\ncame in simi's\nhierarchical clustering.\nThe reinforcement learning\nis a part of machine learning\nwhere an agent is put\nin an environment\nand he learns to behave\nin this environment\nby performing certain actions.\nOkay, so it basically performs\nactions and it either gets\na rewards on the actions\nor It gets a punishment\nand observing the reward\nwhich it gets from those actions\nreinforcement learning is all\nabout taking an appropriate\naction in order\nto maximize the reward\nin a particular situation.\nSo guys in supervised learning\nthe training data comprises\nof the input\nand the expected output\nand so the model is trained\nwith the expected output itself,\nbut when it comes\nto reinforcement learning,\nthere is no expected output here\nthe reinforcement agent decides.\nWhat actions to take in order\nto perform a given task.\nIn the absence of a training\ndata set it is bound to learn\nfrom its experience itself.\nAll right.\nSo reinforcement learning\nis all about an agent\nwho's put in\nan unknown environment\nand he's going to use a hit\nand trial method in order\nto figure out the environment\nand then come up\nwith an outcome.\nOkay.\nNow, let's look\nat reinforcement learning\nwithin an analogy.\nSo consider a scenario\nwhere in a baby is learning\nhow to walk the scenario\ncan go about in two ways.\nNow in the first case\nthe baby starts walking\nand makes it to the candy here.\nThe candy is basically\nthe reward it's going to get so\nsince the candy is the end goal.\nThe baby is happy.\nIt's positive.\nOkay, so the baby is happy\nand it gets rewarded a set\nof candies now another way\nin which this could go is\nthat the baby starts walking\nbut Falls due to some hurdle\nin between the baby gets hurt\nand it doesn't get any candy\nand obviously the baby is sad.\nSo this is a negative reward.\nOkay, or you can say\nthis is a setback.\nSo just like how we humans learn\nfrom our mistakes\nby trial and error.\nLearning is also similar.\nOkay, so we have an agent\nwhich is basically\nthe baby and a reward\nwhich is the candy over here.\nOkay, and with many hurdles\nin between the agent is supposed\nto find the best possible path\nto read through the reward.\nSo guys, I hope\nyou all are clear\nwith the reinforcement learning.\nNow.\nLet's look at the\nreinforcement learning process.\nSo generally a reinforcement\nlearning system has\ntwo main components.\nAll right, the first is an agent\nand the second one is\nan environment now\nin the previous case,\nwe saw that the\nagent was a baby.\nB and the environment\nwas the living room\nwhere in the baby was crawling.\nOkay.\nThe environment is the setting\nthat the agent is acting\non and the agent over here\nrepresents the reinforcement\nlearning algorithm.\nSo guys the reinforcement\nlearning process starts\nwhen the environment\nsends a state to the agent\nand then the agent\nwill take some actions based\non the observations\nin turn the environment\nwill send the next state\nand the respective reward\nback to the agent.\nThe agent will update\nits knowledge with the reward\nreturned by the I meant\nand it uses that to evaluate\nits previous action.\nSo guys this\nLoop keeps continuing\nuntil the environment sends\na terminal state which means\nthat the agent has\naccomplished all his tasks\nand he finally gets the reward.\nOkay.\nThis is exactly\nwhat was depicted\nin this scenario.\nSo the agent keeps\nclimbing up ladders\nuntil he reaches his reward\nto understand this better.\nLet's suppose that our agent is\nlearning to play Counter-Strike.\nOkay, so let's break it down\nnow initially the RL agent\nwhich is Only the player player\n1 let's say it's the player\n1 who is trying to learn\nhow to play the game.\nOkay.\nHe collects some state\nfrom the environment.\nOkay.\nThis could be the first state\nof Counter-Strike now based\non the state the agent\nwill take some action.\nOkay, and this action\ncan be anything\nthat causes a result.\nSo if the player moves left\nor right it's also\nconsidered as an action.\nOkay.\nSo initially the action\nis going to be random\nbecause obviously the first time\nyou pick up Counter-Strike,\nyou're not going\nto be a master at it.\nSo you're going to try\nwith different actions\nand you're just going\nto Up a random action\nin the beginning.\nNow the environment is going\nto give a new state.\nSo after clearing\nthat the environment\nis now going to give a new state\nto the agent or to the player.\nSo maybe he's\nacross stage 1 now.\nHe's in stage 2.\nSo now the player\nwill get a reward\nour one from the environment\nbecause it cleared stage 1.\nSo this reward can be anything.\nIt can be additional points\nor coins or anything like that.\nOkay.\nSo basically this Loop\nkeeps going on\nuntil the player is dead\nor reaches the destination.\nOkay, and it Continuously\noutputs a sequence\nof States actions and rewards.\nSo guys.\nThis was a small example to show\nyou how reinforcement\nlearning process works.\nSo you start\nwith an initial State\nand once a player clothes\nthat state he gets a reward\nafter that the environment will\ngive another stage to the player\nand after it clears that state\nit's going to get another reward\nand it's going to keep happening\nuntil the player\nreaches his destination.\nAll right, so guys,\nI hope this is clear now,\nlet's move on and look\nat the reinforcement\nlearning definition.\nSo there are a few Concepts\nthat you should be aware\nof while studying\nreinforcement learning.\nLet's look at those\ndefinitions over here.\nSo first we have the agent\nnow an agent is basically\nthe reinforcement learning\nalgorithm that learns\nfrom trial and error.\nOkay.\nSo an agent takes actions,\nlike for example a soldier\nin Counter-Strike navigating\nthrough the game.\nThat's also an action.\nOkay, if he moves left right\nor if he shoots at somebody\nthat's also an action.\nOkay.\nSo the agent is responsible\nfor taking actions\nin the environment.\nNow the environment is\nthe whole Counter-Strike game.\nOkay.\nIt's basically the world\nthrough which the agent\nmoves the environment takes\nthe agents current state\nand action as input\nand it Returns the agency reward\nand its next state as output.\nAlright, next we have action\nnow all the possible steps\nthat an agent can take\nare called actions.\nSo like I said,\nit can be moving right left\nor shooting or any of that.\nAlright, then we have\nstate now state is\nbasically the current condition\nreturned by the environment.\nSo Double State you are in\nif you are in state 1 or\nif you're interested\nto that represents\nyour current condition.\nAll right.\nNext we have reward a reward\nis basically an instant return\nfrom the environment\nto appraise Your Last Action.\nOkay, so it can be\nanything like coins\nor it can be additional points.\nSo basically a reward\nis given to an agent\nafter it clears.\nThe specific stages.\nNext we have policy policy is\nbasically the strategy\nthat the agent uses to find\nout his next action.\nIn based on his current\nstate policy is just\nthe strategy with which\nyou approach the game.\nThen we have value.\nNow while you is the expected\nlong-term return with discount\nso value and action value\ncan be a little bit confusing\nfor you right now.\nBut as we move further,\nyou'll understand what\nI'm talking about.\nOkay, so value is basically\nthe long-term return\nthat you get with discount.\nOkay discount, I'll explain\nin the further slides.\nThen we have action value\nnow action value\nis also known as Q value.\nOkay, it's very similar\nto what You except\nthat it takes\nan extra parameter,\nwhich is the current action.\nSo basically here you'll find\nout the Q value depending\non the particular action\nthat you took.\nAll right.\nSo guys don't get confused\nwith value and action value.\nWe look at examples\nin the further slides and you\nwill understand this better.\nOkay, so guys make sure\nthat you're familiar\nwith these terms\nbecause you'll be seeing\na lot of these terms\nin the further slides.\nAll right.\nNow before we move any further,\nI'd like to discuss\na few more Concepts.\nOkay.\nSo first we will discuss\nthe reward maximization.\nSo if you haven't already\nrealize the it the basic aim\nof the RL agent is\nto maximize the reward now,\nhow does that happen?\nLet's try to understand\nthis in depth.\nSo the agent must be\ntrained in such a way\nthat he takes the best action\nso that the reward is maximum\nbecause the end goal\nof reinforcement learning\nis to maximize your reward\nbased on a set of actions.\nSo let me explain this\nwith a small game now\nin the figure you can see there\nis a Forks there's some meat\nand there's a tiger So\nodd agent is basically the fox\nand his end goal is to eat\nthe maximum amount of meat\nbefore being eaten\nby the tiger now\nsince the fox is a clever fellow\nhe eats the meat\nthat is closer to him\nrather than the meat\nwhich is closer to the tiger.\nNow this is because the\ncloser he is to the tiger\nthe higher are his chances\nof getting killed.\nSo because of this the rewards\nwhich are near the tiger,\neven if they are\nbigger meat chunks,\nthey will be discounted.\nSo this is exactly\nwhat discounting means\nso our agent is not going\nto eat the meat chunks\nwhich are Closer to the tiger\nbecause of the risk.\nAll right now even\nthough the meat chunks\nmight be larger.\nHe does not want to take\nthe chances of getting killed.\nOkay.\nThis is called discounting.\nOkay.\nThis is where you discount\nbecause it improvised\nand you just eat the meat\nwhich are closer to you\ninstead of taking risks\nand eating the meat\nwhich are closer\nto your opponent.\nAll right.\nNow the discounting\nof reward Works based\non a value called gamma\nwill be discussing gamma\nin our further slides,\nbut in short the value\nof gamma is between 0 and 1.\nOkay.\nSo the Follow the gamma.\nThe larger is\nthe discount value.\nOkay.\nSo if the gamma value is lesser,\nit means that the agent\nis not going to explore\nand he's not going\nto try and eat the meat chunks\nwhich are closer to the tiger.\nOkay, but if the gamma value\nis closer to 1 it means\nthat our agent is actually\ngoing to explore\nand it's going to dry\nand eat the meat chunks\nwhich are closer to the tiger.\nAll right now,\nI'll be explaining this\nin depth in the further slides.\nSo don't worry\nif you haven't got\na clear concept yet,\nbut just understand\nthat reward maximized.\nAtion is a very important step\nwhen it comes\nto reinforcement learning\nbecause the agent has\nto collect maximum rewards\nby the end of the game.\nAll right.\nNow, let's look\nat another concept\nwhich is called exploration\nand exploitation.\nSo exploration like the name\nsuggests is about exploring\nand capturing more information\nabout an environment\non the other hand exploitation\nis about using the already\nknown exploited information\nto hide in the rewards.\nSo guys consider the fox\nand tiger example\nthat we discussed now here\nthe foxy Only the meat chunks\nwhich are close to him,\nbut he does not eat\nthe meat chunks\nwhich are closer to the tiger.\nOkay, even though they\nmight give him more Awards.\nHe does not eat them\nif the fox only focuses\non the closest rewards,\nhe will never reach\nthe big chunks of meat.\nOkay, this is\nwhat exploitation is\nabout you just going to use\nthe currently known information\nand you're going\nto try and get rewards based\non that information.\nBut if the fox decides\nto explore a bit,\nit can find the bigger award\nwhich is the big chunks of meat.\nThis is exactly\nwhat exploration is.\nSo the agent is not going\nto stick to one corner instead.\nHe's going to explore\nthe entire environment and try\nand collect bigger rewards.\nAll right, so guys,\nI hope you all are clear with\nexploration and exploitation.\nNow, let's look\nat the markers decision process.\nSo guys, this is basically\na mathematical approach\nfor mapping a solution in\nreinforcement learning in a way.\nThe purpose of reinforcement\nlearning is to solve\na Markov decision process.\nOkay, so there are\na few parameters.\nWas that I used to get\nto the solution.\nSo the parameters include\nthe set of actions the set\nof states the rewards the policy\nthat you're taking to approach\nthe problem and the value\nthat you get.\nOkay, so to sum it up\nthe agent must take\nan action a to transition\nfrom a start state\nto the end State s\nwhile doing so the agent\nwill receive a reward are\nfor each action that he takes.\nSo guys a series\nof actions taken by\nthe agent Define the policy\nor a defines the approach.\nAnd the rewards\nthat are collected\nDefine the value.\nSo the main goal here is\nto maximize the rewards\nby choosing the optimum policy.\nAll right.\nNow, let's try to understand\nthis with the help\nof the shortest path problem.\nI'm sure a lot of you might\nhave gone through this problem\nwhen you are in college,\nso guys look\nat the graph over here.\nSo our aim here is\nto find the shortest path\nbetween a and d\nwith minimum possible cost.\nSo the value that you see\non each of these edges\nbasically denotes the cost.\nSo if I want to go from A to see\nit's gonna cost me 15 points.\nOkay.\nSo let's look at\nhow this is done.\nNow before we move\nand look at the problem\nin this problem the set of\nstates are denoted by the nodes,\nwhich is ABCD\nand the action is to Traverse\nfrom one node to the other.\nSo if I'm going from A to B,\nthat's an action\nsimilarly a to see\nthat's an action.\nOkay, the reward is\nbasically the cost\nwhich is represented\nby each Edge over here.\nAll right.\nNow the policy is basically\nthe path that I choose\nto reach the destination,\nso Let's say I choose\na seed be okay,\nthat's one policy\nin order to get to D\nand choosing a CD\nwhich is a policy.\nOkay.\nIt's basically how\nI'm approaching the problem.\nSo guys here you\ncan start off at node a\nand you can take baby steps\nto your destination.\nNow initially you're clueless\nso you can just take\nthe next possible node,\nwhich is visible to you.\nSo guys, if you're smart enough,\nyou're going to choose a\nto see instead of ABCD or ABD.\nAll right.\nSo now if you are at nodes\nsee you want to drive.\nString note D.\nYou must again\nchoose a weisbarth.\nAll right, you just have\nto calculate which path has\nthe highest cost\nor which path will give\nyou the maximum rewards.\nSo guys, this is\na simple problem.\nWe just trying to calculate\nthe shortest path between a\nand d by traversing\nthrough these nodes.\nSo if I Traverse from a CD,\nit gives me the maximum reward.\nOkay, it gives me 65,\nwhich is more than any other\npolicy would give me.\nOkay.\nSo if I go from ABD,\nit would be 40 when you\ncompare this to a CD.\nIt gives me more reward.\nSo obviously I'm going\nto go with a CB.\nOkay, so guys was\na simple problem\nin order to understand how\nMarkov decision process works.\nAll right, so guys,\nI want to ask you a question.\nWhat do you think?\nI did hear did I perform\nexploration or did I\nperform exploitation now\nthe policy for the above example\nis of exploitation\nbecause we didn't explore\nthe other nodes.\nOkay.\nWe just selected three notes\nand we travel through them.\nSo that's why this\nis called exploitation.\nWe must always explore\nthe different notes\nso that we Find\na more optimal policy.\nBut in this case,\nobviously a CD has\nthe highest reward\nand we're going with a CD but\ngenerally it's not so simple.\nThere are a lot of nodes there\nhundreds of notes you Traverse\nand there are like\n50 60 policies.\nOkay, 50 60 different policies.\nSo you make sure you explore\nthrough all the policies\nand then decide\non an Optimum policy\nwhich will give you\na maximum reward the for a robot\nand environment is a place\nwhere It has been\nput to use now.\nRemember this reward is itself\nthe agent for example\nan automobile Factory\nwhere a robot is used\nto move materials\nfrom one place to another now\nthe task we discussed just\nnow have a property in common.\nNow, these tasks involve\nand environment and expect\nthe agent to learn\nfrom the environment.\nNow, this is where traditional\nmachine learning phase\nand hence the need\nfor reinforcement learning now.\nIt is good to have\nan established overview\nof the problem.\nThat is to be Of using\nthe Q learning\nor the reinforcement learning\nso it helps to define\nthe main components\nof a reinforcement\nlearning solution.\nThat is the agent environment\naction rewards and States.\nSo let's suppose we are to build\na few autonomous robots for\nan automobile building Factory.\nNow, these robots will help\nthe factory personal\nby conveying them\nthe necessary parts\nthat they would need\nin order to pull the car.\nNow.\nThese different parts\nare located at\nnine different positions\nwithin the factory warehouse.\nThe car part include\nthe chassis Wheels dashboard\nthe engine and so on\nand the factory workers\nhave prioritized the location\nthat contains the body\nor the chassis to be\nthe topmost but they\nhave provided the priorities\nfor other locations as well,\nwhich will look into the moment.\nNow these locations\nwithin the factory look\nsomewhat like this.\nSo as you can see here,\nwe have L1 L2 L3\nall of these stations now\none thing you might notice here\nthat there Little obstacle\nprison in between the locations.\nSo L6 is the top\npriority location\nthat contains the chassis\nfor preparing the car bodies.\nNow the task is\nto enable the robots\nso that they can find\nthe shortest route\nfrom any given location to\nanother location on their own.\nNow the agents in this case\nare the robots the environment\nis the automobile\nFactory warehouse.\nSo let's talk about the state's\nthe states are the location\nin which a particular robot is\nAnd in the particular\ninstance of time,\nwhich will denote it states\nthe machines understand numbers\nrather than let us so let's map\nthe location codes to number.\nSo as you can see here,\nwe have mapped location\nl 1 to this t 0 L 2 and 1\nand so on we have L8 as\nstate 7 and L line at state.\nSo next what we're going to talk\nabout are the actions.\nSo in our example,\nthe action will be\nthe direct location\nthat a robot can go\nfrom a particular location\nright considering What\nthat is a tel to location\nand the Direct locations to\nwhich it can move rl5 L1 and L3.\nNow the figure here may come\nin handy to visualize this now\nas you might have already\nguessed the set of actions\nhere is nothing but the set\nof all possible states\nof the robot for each location\nthe set of actions\nthat a robot can take\nwill be different.\nFor example, the set\nof actions will change\nif the robot is\nin L1 rather than L2.\nSo if the robot is Is\nin L1 it can only go\nto L 4 and L 2 directly now\nthat we are done with the states\nand the actions.\nLet's talk about the rewards.\nSo the states are\nbasically zero one two,\nthree four and the\nactions are also 0 1\n2 3 4 up to 8.\nNow.\nThe rewards now will\nbe given to a robot.\nIf a location\nwhich is the state\nis directly reachable\nfrom a particular location.\nSo let's take an example suppose\nL line is directly reachable\nfrom L8, right?\nIf a robot goes from LA\nto align and vice versa,\nit will be rewarded by one and\nif I look a shin is\nnot directly reachable\nfrom a particular equation.\nWe do not give any reward\na reward of 0 now the reward\nis just a number\nand nothing else it enables\nthe robots to make sense\nof the movements helping them\nin deciding what locations\nare directly reachable\nand what are not now\nwith this Q. We can\nconstruct a reward table\nwhich contains all\nthe required values mapping\nbetween all possible States.\nSo as you can see here\nin the table the positions\nwhich are marked green\nhave a positive reward.\nAnd as you can see here,\nwe have all the possible rewards\nthat a robot can get by moving\nin between the different states.\nNow comes an\ninteresting decision.\nNow remember that the factory\nadministrator prioritized L6\nto be the topmost.\nSo how do we incorporate\nthis fact in the above table.\nNow, this is done by associating\nthe topmost priority location\nwith a very high reward\nthan the usual ones.\nSo let's put 990.\nAnd in the cell L 6 comma\nand 6 now the table of rewards\nwith a higher reward\nfor the topmost location\nlooks something like this.\nWe have not formally defined\nall the vital components\nfor the solution.\nWe are aiming for\nthe problem discussed.\nNow, you will shift gears\na bit and study some\nof the fundamental concepts\nthat Prevail in the world\nof reinforcement learning\nand q-learning the first\nof all we'll start\nwith the Bellman\nequation now consider\nthe following Square rooms,\nwhich is analogous\nto the actual environment.\nAunt from our original problem,\nbut without the barriers now\nsuppose a robot needs to go\nto the room marked\nin the green promise\ncurrent position a using\nthe specified Direction now,\nhow can we enable the robot\nto do this programmatically\none idea would be introduced\nsome kind of a footprint\nwhich the robot will be able\nto follow now here\na constant value is specified\nin each of the rooms\nwhich will come\nalong the robots way\nif it follows the direction\nspecified above now in this way\nif it starts at A it\nwill be able to scan\nthrough this constant value\nand will move accordingly\nbut this will only work\nif the direction is prefix\nand the robot always starts\nat the location a now\nconsider the robot starts\nat this location rather\nthan its previous one.\nNow the robot\nnow sees Footprints\nin two different directions.\nIt is therefore unable\nto decide which way to go\nin order to get the destination\nwhich is the Green Room.\nIt happens primarily\nbecause the robot\ndoes not have a weight.\nRemember the directions\nto proceed so our job\nnow is to enable\nthe robot with a memory.\nNow, this is where the Bellman\nequation comes into play.\nSo as you can see here,\nthe main reason\nof the Bellman equation\nis to enable the reward\nwith the memory.\nThat's the thing\nwe're going to use.\nSo the equation goes\nsomething like this V\nof s gives maximum a r\nof s comma a plus gamma of vs -\nwhere s is a particular state\nwhich is a ROM a is\nthe Action Moving\nbetween the rooms as -\nis the state to which\nthe robot goes from s\nand gamma is the discount Factor\nnow we'll get\ninto it in a moment\nand obviously R of s comma\na is a reward function\nwhich takes a state as an action\na and outputs the reward now V\nof s is the value of being\nin a particular state\nwhich is the footprint\nnow we consider all\nthe possible actions\nand take the one that yields\nthe maximum value now,\nthere is one constraint however\nregarding the value Footprint,\nthat is the row marked\nin the yellow just\nbelow the Green Room.\nIt will always have\nthe value of 1 to denote\nthat is one of the nearest room\nadjacent to the Green Room\nnot this is also to ensure\nthat a robot gets a reward\nwhen it goes from a yellow room\nto The Green Room.\nLet's see how to make\nsense of the equation\nwhich we have here.\nSo let's assume\na discount factor of 0.9\nas remember gamma is\nthe discount value\nor the discount Factor.\nSo let's take a 0.9\nnow for the room,\nwhich is Just below the one\nor the yellow room, which is\nthe Aztec Mark for this room.\nWhat will be the V of s\nthat is the value of being\nin a particular state?\nSo for this V of s\nwould be something\nlike maximum of a will take 0\nwhich is the initial\nof our s comma.\nHey plus 0.9\nwhich is gamma into 1\nthat gives us zero point\nnine now here the robot\nwill not get any reward\nfor going to a state\nmarked in yellow.\nHence the ER s comma a is 0 here\nbut the robot knows the value\nof being in the yellow room.\nHence V of s Dash is\none following this\nfor the other states.\nWe should get 0.9 then again,\nif we put 0.9 in this equation,\nwe get 0.81 than 0.7 to 9\nand then we again reach\nthe starting point.\nSo this is\nhow the table looks with\nsome value Footprints computed\nfrom the Bellman equation now\na couple of things\nto It is here is\nthat the max function\nhas the robot to always\nchoose the state\nthat gives it the maximum value\nof being in that state.\nNow the discount Factor\ngamma notifies the robot\nabout how far it is\nfrom the destination.\nThis is typically specified by\nthe developer of the algorithm.\nThat would be installed\nin the robot.\nNow, the other states can also\nbe given their respective values\nin a similar way.\nSo as you can see here\nthe boxes adjacent\nto the green one have one and\nif we Move away from 1 we\nget 0.9 0.8 1 0 1 7 to 9\nand finally we reach 0.66.\nNow the robot now\ncan precede its way\nthrough the Green Room utilizing\nthese value Footprints event\nif it's dropped\nat any arbitrary room\nin the given location now,\nif a robot Lance up in\nthe highlighted Sky Blue Area,\nit will still find\ntwo options to choose\nfrom but eventually either\nof the parts will be good enough\nfor the robot to take\nbecause Auto V\nthe value for prints\nand only that out.\nNow one thing to note is\nthat the Bellman equation is one\nof the key equations\nin the world of reinforcement\nlearning and Q learning.\nSo if we think realistically our\nsurroundings do not always work\nin the way we expect\nthere is always a bit\nof stochastic City\ninvolved in it.\nSo this applies\nto robot as well.\nSometimes it might so happen\nthat the robots\nMachinery got corrupted.\nSometimes the robot may come\nacross some hindrance\non its way which it\nmay not be known\nto it beforehand.\nRight and sometimes even\nif the robot knows\nthat it needs to take\nthe right turn it will not so\nhow do we introduce\nthis to cast a city\nin our case now here comes\nthe Markov decision process.\nSo consider the robot is\ncurrently in the Red Room\nand it needs to go\nto the green room.\nNow.\nLet's now consider\nthe robot has a slight chance\nof dysfunctioning and might take\nthe left or the right\nor the bottom turn instead\nof digging the upper turn\nand are Get to the Green Room\nfrom where it is now,\nwhich is the Retro.\nNow the question is,\nhow do we enable the robot\nto handle this when it is out\nin the given environment right.\nNow, this is a situation\nwhere the decision making\nregarding which turn is\nto be taken is partly random\nand partly another control\nof the robot now partly random\nbecause we are not sure\nwhen exactly the robot mind\ndysfunctional and partly under\nthe control of the robot\nbecause it is still\nmaking a decision of taking\na turn right on its own.\nAnd with the help\nof the program embedded into it.\nSo a Markov decision process\nis a discrete time\nstochastic Control process.\nIt provides a mathematical\nframework for modeling\ndecision-making in situations\nwhere the outcomes\nare partly random\nand partly under the control\nof the decision maker.\nNow we need to give this concept\na mathematical shape\nmost likely an equation\nwhich then can be taken further.\nNow you might be surprised\nthat we can do this with the\nhelp of the Bellman equation.\nAction with a few minor tweaks.\nSo if we have a look\nat the original Bellman equation\nV of X is equal\nto maximum of our s comma\na plus gamma V of s -\nwhat needs to be changed\nin the above equation\nso that we can introduce\nsome amount of Randomness\nhere as long as we are not sure\nwhen the robot might not take\nthe expected turn.\nWe are then also not sure\nin which room it might end up\nin which is nothing\nbut the ROM it moves\nfrom its current room\nat this point according.\nTo the equation.\nWe are not sure of the a stash\nwhich is the next state\nor the room,\nbut we do know all the probable\nturns the robot might take now\nin order to incorporate each\nof this probabilities\ninto the above equation.\nWe need to associate\na probability with each\nof the turns to\nquantify the robot.\nIf it has got any expertise\nchance of taking the stern know\nif we do so we get\nPS is equal to maximum\nof RS comma a plus gamma\ninto summation of s -\nPS comma a comma s stash into V\nof his stash now the PS a--\nand a stash is the probability\nof moving from room s\nto establish with the action a\nand the submission\nhere is the expectation\nof the situation.\nThat's a robot in curse,\nwhich is the randomness now,\nlet's take a look\nat this example here.\nSo when we associate\nthe probabilities to each\nof these terms Owns,\nwe essentially mean\nthat there is an 80% chance\nthat the robot will\ntake the upper turn.\nNow, if you put all\nthe required values\nin our equation,\nwe get V of s is equal\nto maximum of R of s comma a +\ncomma of 0.8 into V of room up\nplus zero point 1 into V\nof room down 0.03 into Rome\nof V of from left plus 0.03\ninto V of room right now note\nthat the value footprints.\nNot change due to the fact\nthat we are incorporating\nstochastically here.\nBut this time we\nwill not calculate\nthose values Footprints instead.\nWe will let the robot\nto figure it out.\nNow up until this point.\nWe have not considered\nabout rewarding the robot\nfor its action of going\ninto a particular room.\nWe are only watering the robot\nwhen it gets\nto the destination now,\nideally there should be a reward\nfor each action the robot takes\nto help it better assess\nthe quality of the actions,\nbut the there was need\nnot to be always be the same\nbut it is much better\nthan having some amount\nof reward for the actions\nthan having no rewards at all.\nRight and this idea is known as\nthe living penalty in reality.\nThe reward system\ncan be very complex\nand particularly modeling\nsparse rewards is an active area\nof research in the domain\nof reinforcement learning.\nSo by now we have got\nthe equation which we have a so\nwhat we're going to do is\nnow transition to Q learning.\nSo this equation gives\nus the value of going\nto a particular State\ntaking the stochastic city\nof the environment into account.\nNow, we have also learned\nvery briefly about the idea\nof living penalty\nwhich deals with associating\neach move of the robot\nwith a reward.\nSo Q learning processes\nand idea of assessing\nthe quality of an action\nthat is taken to move to\na state rather than determining\nthe possible value of the state\nwhich is being moved\nto so earlier.\nWe had 0.8 into V. E\nof s 1 0.03 into V\nof S 2 0 point 1 into V\nof S 3 and so on now\nif you incorporate the idea\nof assessing the quality\nof the action for moving\nto a certain state\nso the environment\nwith the agent\nand the quality of the action\nwill look something like this.\nSo instead of 0.8 V\nof s 1 will have q of s\n1 comma a one will have q\nof S 2 comma 2 Q of S 3 now\nthe robot now has food.\nIn states to choose from\nand along with that there are\nfour different actions also for\nthe current state it is in so\nhow do we calculate Q of s comma\na that is the cumulative quality\nof the possible actions\nthe robot might take so\nlet's break it down.\nNow from the equation V of s\nequals maximum a RS comma a +\ncomma summation s -\nPSAs - into V of s -\nif we discard them.\nMaximum function we have is\nof a plus gamma into summation p\nand v now essentially\nin the equation\nthat produces V of s.\nWe are considering\nall possible actions\nand all possible States\nfrom the current state\nthat the robot is\nin and then we are taking\nthe maximum value caused\nby taking a certain action\nand the equation produces\na value footprint,\nwhich is for just\none possible action.\nIn fact, we can think\nof it as the quality\nof the So Q of s comma a\nis equal to RS comma a +\ncomma of summation p and v now\nthat we have got an equation\nto quantify the quality\nof a particular action.\nWe are going to make\na little adjustment\nin the equation we can now say\nthat V of s is the maximum\nof all the possible values\nof Q of s comma a right.\nSo let's utilize this fact\nand replace V of s Dash as\na function of Q. So Q U.s.\nComma a becomes R of s comma a +\ncomma of summation PSAs -\nand maximum of the que\nes - a -\nso the equation of V is now\nturned into an equation of Q,\nwhich is the quality.\nBut why would we do that now?\nThis is done to\nease our calculations\nbecause now we have\nonly one function Q\nwhich is also the core of the\ndynamic programming language.\nWe have only one.\nOcean Q to calculate\nand R of s comma a is\na Quantified metric\nwhich produces reward\nof moving to a certain State.\nNow, the qualities\nof the actions are\ncalled The Q values\nand from now on we will refer\nto the value Footprints\nas the Q values\nan important piece\nof the puzzle is\nthe temporal difference.\nNow temporal difference\nis the component\nthat will help the robot\ncalculate the Q values\nwhich respect to the changes\nin the environment over time.\nSo consider Our robot is\ncurrently in the mark State\nand it wants to move\nto the Upper State.\nOne thing to note that here is\nthat the robot already knows\nthe Q value of making the action\nthat is moving through\nthe Upper State and we know\nthat the environment\nis stochastic in nature\nand the reward\nthat the robot will get\nafter moving to the Upper State\nmight be different\nfrom an earlier observation.\nSo how do we capture\nthis change the real difference?\nWe calculate the new q s comma a\nwith the same formula\nand subtract the Previously\nknown qsa from it.\nSo this will in turn\ngive us the new QA.\nNow the equation\nthat we just derived gifts\nthe temporal difference\nin the Q values\nwhich further helps\nto capture the random changes\nin the environment\nwhich may impose now\nthe name q s comma a\nis updated as the following\nso Q T of s comma is equal\nto QT minus 1 s comma a\nplus Alpha D DT of a comma\ns now here Allah Alpha is\nthe learning rate which controls\nhow quickly the robot adapts\nto the random changes imposed\nby the environment the qts comma\nis the current state q value\nand a QT minus 1 s comma is\nthe previously recorded Q value.\nSo if we replace the TDS comma a\nwith its full form equation,\nwe should get Q T of s\ncomma is equal to QT -\n1 of s comma y\nplus Alpha into R of s\ncomma a plus gamma maximum.\nQ s Dash a dash minus QT\nminus 1 s comma a now\nthat we have all the little\npieces of q line together.\nLet's move forward\nto its implementation part.\nNow, this is the final equation\nof q-learning, right?\nSo, let's see\nhow we can implement this\nand obtain the best path\nfor any robot to take now\nto implement the algorithm.\nWe need to understand\nthe warehouse location\nand how that can be mapped\nto different states.\nSo let's start by reconnecting\nthe sample environment.\nSo as you can see here,\nwe have L1 L2 L3 to align\nand as you can see here,\nwe have certain borders also.\nSo first of all,\nlet's map each of the above\nlocations in the warehouse\ntwo numbers or the states\nso that it will ease\nour calculations, right?\nSo what I'm going to do is\ncreate a new Python 3 file\nin the jupyter notebook\nand I'll name it as q-learning.\nNumber.\nOkay.\nSo let's define the states.\nBut before that what we\nneed to do is import numpy\nbecause we're going to use numpy\nfor this purpose and let's\ninitialize the parameters.\nThat is the gamma\nand Alpha parameters.\nSo gamma is 0.75\nwhich is the discount Factor\nwhereas Alpha is 0.9,\nwhich is the learning rate.\nNow next what we're going to do\nis Define the states and map\nit to numbers.\nSo as I mentioned Earlier l\n1 is 0 and Dylan line.\nWe have defined the states\nin the numerical form.\nNow.\nThe next step is to define\nthe actions which is\nas mentioned above\nrepresents the transition\nto the next state.\nSo as you can see here,\nwe have an array\nof actions from 0 to 8.\nNow, what we're going to do\nis Define the reward table.\nSo as you can see,\nit's the same Matrix\nthat we created just now\nthat I showed you just now now\nif you understood it correctly,\nthere isn't any real\nBarrel limitation\nas depicted in the image,\nfor example, the transitional\nfor tell one is allowed\nbut the reward will be\nzero to discourage\nthat path or in tough situation.\nWhat we do is add\na minus 1 there\nso that it gets\na negative reward.\nSo in the above code snippet\nas you can see here.\nI took each of the states and\nput once in the respective state\nthat are directly reachable\nfrom the certain State now,\nif you refer to that reward\ntable, once again,\nwhat we created the above,\nour reconstruction will\nbe easy to understand\nbut one thing to note here is\nthat we did not consider the top\npriority location L6 yet.\nWe would also need\nan inverse mapping\nfrom the state's back\nto its original location\nand it will be cleaner\nwhen we reach to the utter\ndepths of the algorithms.\nSo for that what we're going\nto do Is have the inverse\nmap location State delegation.\nWe will take the distinct\nState and location\nand convert it back.\nNow.\nWhat we'll do is we will now\nDefine a function get optimal\nwhich is the get optimal route,\nwhich will have a start location\nand an N location.\nDon't worry.\nThe code is pick\nbut I'll explain you each\nand every bit of the code.\nNow the get optimal\nroute function will take\ntwo arguments the style location\nin the warehouse\nand the end location\nin the warehouse recipe lovely\nand it will return\nthe optimal route\nfor reaching the end location\nfrom the starting location\nin the form of an ordered list\ncontaining the letters.\nSo we'll start by defining\nthe function by initializing\nthe Q values to be all zeros.\nSo as you can see here,\nwe have given the Q value\nhas to be 0 but For that\nwhat we need to do is copy\nthe reward Matrix to a new one.\nSo this is the rewards\nnew and next again.\nWhat we need to do is get\nthe ending State corresponding\nto the ending location.\nAnd with this information\nautomatically will set\nthe priority of the given ending\nstay to the highest one\nthat we are not defining it now,\nbut will automatically\nset the priority\nof the given ending\nState as nine nine nine.\nSo what we're going\nto do is initialize\nthe Q values to be 0 and\nin the queue learning process\nwhat you can see See here.\nWe are taking I in range\n1,000 and we're going\nto pick up a state randomly.\nSo we're going to use\nthe MP dot random r + NT\nand for traversing\nthrough the neighbor location\nin the same maze.\nWe're going to iterate\nthrough the new reward\nMatrix and get the actions\nwhich are greater\nthan 0 and after that\nwhat we're going to do is pick\nan action randomly from the list\nof the playable actions\nin years to the next state\nwill going to compute\nthe temporal difference,\nwhich is TD,\nwhich is the rewards plus gamma\ninto The queue of next state\nand will take n p dot ARG Max\nof Q of next eight minus Q\nof the current state.\nWe going to then update\nthe Q values using\nthe Bellman equation\nas you can see here,\nyou have the Bellman equation\nand we're going\nto update the Q values\nand after that we're going\nto initialize the optimal route\nwith a starting location\nnow here we do not know\nwhat the next location yet.\nSo initialize it with the value\nof the starting location,\nwhich again is the random Shh\nnow we do not know\nabout the exact number\nof iteration needed to reach\nto the final location.\nHence while loop will be\na good choice for the iteration.\nSo when you're going to fetch\nthe starting State fetch\nthe highest Q value penetrating\nto the starting State\nwe go to the index\nor the next state,\nbut we need\nthe corresponding letter.\nSo we're going to use that state\nto location function.\nWe just mentioned there\nand after that we're going\nto update the starting location\nfor the next iteration.\nFinally, we'll return the root.\nSo let's take the starting\nlocation of n line\nand and location of L1 and see\nwhat part do we actually get?\nSo as you can see here,\nwe get Airline l8l\nfive L2 and L1.\nAnd if you have a look\nat the image here,\nwe have if we start from L9\nto L1 we got l8l 5 L\n2 l 1 L HL 5 L2 L1.\nThat would yield us the maximum.\nMm value of the maximum\nreward for the robot.\nSo now we have come to the end\nof this Q learning session\nthe past year has seen a lot\nof great examples\nfor machine learning\nand many new high-impact\napplication of machine\nlearning with discovered\nand brought to light especially\nin the healthcare Finance\nthe speech recognition\naugmented reality\nand much more complex 3D\nand video applications.\nThe natural language\nprocessing was easily\nthe most talked about domain\nwithin the community\nwith the likes of you.\nLmf it and but\nbeing open sourced.\nSo let's have a look\nat some of the amazing\nmachine learning projects\nwhich are open sourced\nthe code is available for you.\nAnd those are discussed in\nthis 2018 to nine in Spectrum.\nSo the first and the foremost\nis tensorflow dot DS now\nmachine learning in the browser\nor fictional thought\na few years back.\nBack and a stunning reality.\nNow a lot of us in this field\nare welded to our favorite IDE,\nbut tells of not DOT JS has the\npotential to change your habits.\nIt's become a very popular\nreleased since its release\nearlier this year\nand continues to amaze\nwith its flexibility.\nNow as a repository states,\nthere are primarily\nthree major features\nof terms of rho dot J's\ndevelop machine learning\nand deep learning models\nin your process\nitself run pre-existent as\na flow models within\nthe browser retrain our Gene\nthese prediction models as well.\nAnd if you are familiar with\nKara's the high-level layers\nEPA will seem quite familiar,\nbut there are plenty of examples\navailable on GitHub repository.\nSo do check out those legs\nto Quicken your learning curve.\nAnd as I mentioned earlier,\nI'll leave the links\nto all of these open\nsource machine learning projects\nin the description below.\nThe next what we\nnot discuss is detector\non it is developed by Facebook\nand made a huge Splash\nwhen it was earlier launched in.\nAn 80 those developed by\nFacebook's AI research team,\nwhich is fa ir.\nAnd it implements the state\nof the art object\ndetection frame was it\nis written in Python\nand as help enable\nmultiple projects\nincluding the dance pose.\nNow, we'll know\nwhat exactly is then suppose\nafter this example\nand this repository\ncontains the code\nof over 70 preacher involves.\nSo it's a very good\nopen source small guys.\nSo to check it out now\nthe moment I talked\nabout then suppose.\nThat's the next one.\nI'm going to talk about so\nThat's supposed stents human\npose estimation in the wild,\nbut the code to train\nand evaluate your own dance pose\nusing the our CNN model\nis included here\nand I've given the link\nto the open source code\nin the description below\nand there are notebooks\navailable as well to visualize\ncertain Sports cocoa data set\nthe next on our list.\nWe have D\npainterly harmonization.\nNow, I want you to take\na moment to just admire\nthe above images.\nCan you tell which ones\nwe're done by a human\nand which one by a machine?\nI certainly could not now here.\nThe first frame is\nthe input image the original one\nand a third frame\nas you can see here\nhas been generated by\nthis technique amazing, right?\nThe algorithm has\nan external object\nto your choosing to any image\nand manages it to make it look\nlike nothing touched it now,\nmake sure you check out\nthe code and try to implement it\non different sets\nof images yourself.\nIt is really really fun.\nBut talking about images.\nWe have image out painting now\nwhat if I give you an image and\nask you to extend Its boundaries\nby imagining what it would look\nlike when the entire\nscene was captured.\nYou would understandably turn\nto some image editing software.\nBut here's the awesome news.\nYou can achieve it\nin few lines of code,\nwhich is the image out painting.\nNow.\nThis project is Akira's\nimplementation of Stanford image\nout failing paper,\nwhich is incredibly cool\nand Illustrated paper.\nAnd this is how most\nresearch paper should be.\nI've given the links\nin the description\nbelow to check it out\nguys and see how you can.\nImplement it now.\nLet's talk about audio\nprocessing which is\nan another field\nwhere machine learning\nhas started to make its mark.\nIt is not just limited\nto generate music.\nYou can do tasks like audio\nclassification fingerprinting\nsegmentation tagging and much\nmore and there is a lot\nthat's still yet to be explored\nand who knows perhaps you\ncould use this project\nto Pioneer your way to the top.\nNow what if you want\nto discover your own planner now\nthat might perhaps\nbe overstating things a bit,\nbut the astronaut repository\nwill definitely get you close.\nThe Google brain team discovered\ntwo new planets in the summer\n2017 by applying the astronaut.\nIt's a deep neural network\nmeant for working\nwith astronomical data.\nIt goes to show\nthe far-ranging application\nof machine learning and was\na truly Monumental development.\nAnd now the team\nbehind the technology has\nopen source the entire code,\nso go ahead and check\nout your own planet and\nwho knows you might even have\na planet on your name now,\nI could not possibly\nlet this section.\nPass by without\nmentioning the brt.\nThe Google AI is released\nhas smashed record on his way\nto winning the hearts\nof NLP enthusiasts\nand experts alike following you.\nLmf it and he LMO brt really\nblew away the competition\nwith its performance.\nIt obtained a state\nof art result\non 11 and LP task apart from\nthe official Google repository.\nThere is a python\nimplementation of birth,\nwhich is worth checking out\nwhether it makes a new era\nor not in natural\nlanguage processing.\nThe thing we will soon\nfind out now add on it.\nI'm sure you guys\nmight have heard of it.\nIt is a framework\nfor automatically learning\nhigh quality models without\nrequiring programming expertise\nsince it's a Google invention.\nThe framework is based\non tensorflow and you can build\nand simple models\nusing a Danette\nand even extend it to use\nto train a neural network.\nNow the GitHub page contains\nthe code and example\nthe API documentation\nand other things to get\nyour hands dirty the trust me\nOtto ml is the next big thing.\nNG in our field now\nif you follow a few researchers\non social media,\nyou must have come\nacross some of the images.\nI am showing here\nin a video form a stick human\nrunning across the terrain\nor trying to stand\nup or some sort,\nbut that my friends\nis reinforcement learning\nand action now,\nhere's a signature example\nof it a framework to create\na simulated humanoid to imitate\nmultiple motion skin.\nSo let's have a look\nat the top 10 skills.\nAre required to become\na successful machine\nlearning engineer.\nSo starting with\nprogramming languages python\nis the lingua Franca\nof machine learning.\nYou may have had\nexposure to buy them.\nEven if you weren't previously\nin programming or in a computer\nscience research field.\nHowever, it is important to have\na solid understanding of glasses\nand data structures.\nSometimes python won't\nbe enough often.\nYou'll encounter projects\nthat need to leverage hardware\nfor Speed improvements.\nNow, make sure you are familiar\nwith the basic algorithms as\nwell as the classes.\nMemory management\nand linking now\nif you want a job\nin machine learning,\nyou will probably have\nto learn all of these languages\nat some point C++ can help\nin speeding code up.\nWhereas our works great\nin statistics and plots\nand Hadoop is java-based.\nSo you probably need\nto implement mappers\nand reducers in Java.\nNow next we have linear algebra.\nYou need to be intimately\nfamiliar with mattresses vectors\nand matrix multiplication\nif you have an understanding\nof derivatives and integrals,\nYou should be in the clear.\nOtherwise even simple concept\nlike gradient descent\nwill elude you statistic\nis going to come up a lot at\nleast make sure you are familiar\nwith the caution distributions\nmeans standard deviation\nand much more every bit\nof statistical understanding\nBeyond this helps\nthe theories help in learning\nabout algorithms great\nsamples are naive buys\ngaussian mixture models\nand hidden Markov models.\nYou need to have a firm\nunderstanding of probability\nand stats to understand\nthese these models just go nuts\nand study measure Theory\nand next we have advanced\nsignal processing techniques.\nNow feature extraction is one\nof the most important parts\nof machine learning\ndifferent types of problems\nneed various Solutions.\nYou may be able to utilize\nreally cool Advanced\nsignal processing algorithms\nsuch as wavelets share.\nLet's go blades\nand bandless you need to learn\nabout the time-frequency\nanalysis and try to apply it\nin your problems.\nNow, this skill will give\nyou an edge over all\nthe other skills not this kid.\nWill give you an edge\nwhile you're applying\nfor a machine learning engine\nthe job or others\nor next we have applied\nmaths a lot of machine\nlearning techniques out.\nThere are just fancy types\nof functional approximation.\nNow these often get developed\nby theoretical mathematician\nand then get applied by people\nwho do not understand\nthe theory at all.\nNow the result is\nthat many developers\nmight have a hard time\nfinding the best techniques\nfor the problem.\nSo even a basic understanding\nof numerical analysis will give\nyou a huge Edge having\na firm understanding.\nEnding of algorithm\nTheory and knowing\nhow the algorithm works.\nYou can also discriminate models\nsuch as svm's now you will need\nto understand subjects such as\ngradient descent convex\noptimization LaGrange\nquadratic programming partial\ndifferentiation equations\nand much more now all this math\nmight seem intimidating at first\nif you have been away\nfrom it for a while just\nmachine learning is\nmuch more math intensive\nthan something like\nfront-end developer.\nJust like any other skill\ngetting better at math is a man.\nOur Focus practice\nthe next skill\nin our list is the neural\nnetwork architectures.\nWe need machine learning\nfor tasks that are too complex\nfor human to quote\ndirectly that is tasks\nthat are so complex\nthat it is Impractical now\nneural networks are a class\nof models within the general\nmachine learning literature\nor neural networks are\na specific set of algorithms\nthat have revolutionized\nmachine learning.\nThey're inspired by\nbiological neural networks,\nand the current so-called\ndeep neural networks\nhave proven to work quite well.\nWell, the neural\nnetworks are themselves\nGeneral function approximations,\nwhich is why they\ncan be applied to almost\nany machine learning problem\nabout learning a complex mapping\nfrom the input\nto the output space.\nOf course, there are still\ngood reason for the surge\nin the popularity\nof neural networks,\nbut neural networks have been\nby far the most accurate way of\napproaching many problems like\ntranslation speech recognition\nand image classification now\ncoming to our next point\nwhich is the natural\nlanguage processing now\nsince it combines\ncomputer science and Listed,\nthere are a bunch of libraries\nlike the NLT K chances.\nMm and the techniques\nsuch as sentimental analysis\nand summarization\nthat are unique to NLP now audio\nand video processing\nhas a frequent overlap with\nthe natural language processing.\nHowever, natural language\nprocessing can be applied\nto non audio data\nlike text voice\nand audio analysis involves\nextracting useful information\nfrom the audio signals\nthemselves being well-versed\nin math will get\nyou far in this one\nand you should also be familiar.\nHer with the concepts such as\nthe fast Fourier transforms.\nNow, these were\nthe technical skills\nthat are required\nto become a successful\nmachine learning engineer.\nSo next I'm going to discuss\nsome of the non-technical skills\nor the soft skills,\nwhich are required to become\na machine-learning engineer.\nSo first of all,\nwe have the industry knowledge.\nNow the most successful\nmachine learning projects out.\nThere are going to be those\nthat address real pain points\nwhichever industry we\nare working for you should know\nhow that industry works\nand Will be beneficial\nfor the business\nif a machine learning engineer\ndoes not have business Acumen\nand the know-how of the elements\nthat make up\na successful business model\nor any particular algorithm.\nThen all those technical skills\ncannot be Channel productively,\nyou won't be able\nto discern the problems\nand potential challenges\nthat need solving\nfor the business to sustain\nand grow you won't\nreally be able to help\nyour organization explore\nnew business opportunities.\nSo this is a must-have\nskill now next we\nhave effective communication.\nYou'll need to explain\nthe machine learning\nConcepts to the people\nwith little to no expertise\nin the field chances\nare you'll need to work\nwith a team of Engineers as\nwell as many other teams.\nSo communication is going\nto make all of this much\nmore easier companies searching\nfor a strong machine learning\nengineer looking for someone\nwho can clearly\nand fluently translate\ntheir technical findings\nto a non technical team\nsuch as marketing\nor sales department\nand next on our list.\nWe have rapid prototyping so\nIterating on ideas as quickly as\npossible is mandatory\nfor finding one\nthat works in machine learning\nthis applies to everything\nfrom picking up the right model\nto working on projects\nsuch as A/B Testing\nyou need to do a group\nof techniques used to quickly\nfabricate a scale model\nof a physical part\nor assembly using\nthe three-dimensional\ncomputer aided design,\nwhich is the cat so last\nbut not the least we\nhave the final skill\nand that is to keep updated.\nYou must stay up to date\nwith Any upcoming changes\nevery month new neural\nnetwork models come out\nthat are performed\nthe previous architecture.\nIt also means being aware\nof the news regarding\nthe development of the tools\nthe changelog the conferences\nand much more you\nneed to know about\nthe theories and algorithms.\nNow this you can achieve\nby reading the research papers\nblogs the conference's videos.\nAnd also you need to focus\non the online community\nwith changes very quickly.\nSo expect and cultivate\nthis change now,\nthis is not the Here we have\ncertain skills the bonus skills,\nwhich will give you an edge\nover other competitors\nor the other persons\nwho are applying\nfor a machine-learning engineer\nposition on the bonus point.\nWe have physics.\nNow, you might be in a situation\nwhere you're like to apply\nmachine learning techniques\nto A system\nthat will interact with the real\nworld having some knowledge\nof physics will take\nyou far the next we\nhave reinforcement learning.\nSo this reinforcement learning\nhas been a driver\nbehind many of the most\nexciting developments\nin the Deep learning\nand the AI community.\nT from the alphago zero to\nthe open a is Dota 2 pot.\nThis will be\na critical to understand\nif you want to go\ninto robotics self-driving cars\nor other AI related areas.\nAnd finally we have\ncomputer vision out of all\nthe disciplines out there.\nThere are by far\nthe most resources available\nfor learning computer vision.\nThis field appears to have\nthe lowest barriers to entry\nbut of course this\nlikely means you will face\nslightly more competition.\nSo having a good knowledge\nof computer vision\nhow it rolls will\ngive you an edge.\nOther competitors now.\nI hope you got acquainted\nwith all the skills\nwhich are required\nto become a successful\nmachine learning engineer.\nAs you know,\nwe are living in the worlds\nof humans and machines\nin today's world.\nThese machines are\nthe robots have to be programmed\nbefore they start\nfollowing your instructions.\nBut what if the machine started\nlearning on its own from\ntheir experience work like us\nand feel like us\nand do things more\naccurately than us now?\nWell his machine learning Angela\ncomes into picture to make sure\neverything is working\naccording to the procedures\nand the guidelines.\nSo in my opinion machine\nlearning is one of the most\nrecent and And Technologies,\nthere is you probably use it\nat dozens of times every day\nwithout even knowing it.\nSo before we indulge\ninto the different roles\nthe salary Trends\nand what should be\nthere on the resume\nof a machine learning engineer\nwhile applying for a job.\nLet's understand\nwho exactly a machine learning\nengineering is so machine\nlearning Engineers are\nsophisticated programmers\nwho develop machines and systems\nthat can learn\nand apply knowledge without\nspecific Direction artificial\nintelligence is the goal\nof a machine-learning engineer.\nThey are computer programmers\nbut their focus goes\nbeyond specifically\nprogramming machines to\nperform specific tasks.\nThey create programs\nthat will enable\nmachines to take actions\nwithout being specifically\ndirected to perform those tasks.\nNow if we have a look\nat the job trends\nof machine learning in general.\nSo as you can see\nin Seattle itself,\nwe have 2,000 jobs in New York.\nWe have 1100 San Francisco.\nWe have 1100 in Bengaluru India,\nwe have 1100 and then\nwe have Sunnyvale,\nCalifornia where we have\nIf I were a number of jobs,\nso as you can see the number of\njobs in the market is too much\nand probably with the emergence\nof machine learning\nand artificial intelligence.\nThis number is just\ngoing to get higher now.\nIf you have a look at the job\nopening salary-wise percentage,\nso you can see for the $90,000\nper annum bracket.\nWe have 32.7 percentage\nand that's the maximum.\nSo be assured\nthat if you get a job as\na machine-learning engineer,\nyou'll probably get\naround 90 thousand bucks a year.\nThat's safe to say.\nNow for the hundred and\nten thousand dollars per year.\nWe have 25% $120,000.\nWe have 20 percent\nalmost then we have a hundred\nand thirty thousand dollars\nwhich are the senior\nmachine learning and Jenna's\nthat's a 13 point\n6 7% And finally,\nwe have the most senior\nmachine learning engineer\nor we have\nthe data scientist here,\nwhich have the salary\nof a hundred and forty thousand\ndollars per annum\nand the percentage\nfor that one is really low.\nSo as you can see there is\na great opportunity for people.\nWhat trying to go\ninto machine learning field\nand get started with it?\nSo let's have a look\nat the machine learning\nin junior salary.\nSo the average salary\nin the u.s.\nIs around a hundred eleven\nthousand four hundred\nand ninety dollars\nand the average salary\nin India is around\nseven last nineteen thousand\nsix hundred forty six rupees.\nThat's a very\ngood average salary\nfor any particular profession.\nSo moving forward\nif we have a look\nat the salary of\nan entry-level machine learning.\nYou know, so the salary\nranges from $76,000\nor seventy seven thousand\ndollars two hundred\nand fifty one thousand\ndollars per annum.\nThat's a huge salary.\nAnd if you talk\nabout the bonus here,\nwe have like\nthree thousand dollars to twenty\nfive thousand dollars depending\non the work YouTube and\nthe project you are working on.\nLet's talk about\nthe profit sharing now.\nSo it's around\ntwo thousand dollars\nto fifty thousand dollars.\nNow this again depends\nupon the project you are working\nthe company you are working\nfor and the percentage\nthat Give to the in general\nor the developer\nfor that particular project.\nNow, the total pay comes around\nseventy six thousand dollars\nor seventy-five thousand dollars\ntwo hundred and sixty\ntwo thousand dollars\nand this is just for the entry\nlevel machine learning engineer.\nJust imagine if you become\nan experience machine\nlearning engineer your salary\nis going to go through the roof.\nSo now that we have understood\nwho exactly is\na machine learning engineer\nthe various salary Trends\nthe job Trends in the market\nand how it's rising.\nLet's understand.\nWhat skills it takes to become\na machine learning engine.\nSo first of all,\nwe have programming languages\nnow programming languages are\nbig deal when it comes\nto machine learning\nbecause you don't just\nneed to have Proficiency\nin one language you might\nrequire Proficiency in Python.\nJava are or C++\nbecause you might be working\nin a Hadoop environment\nwhere you require\nJava programming to do\nthe mapreduce Coatings\nand sometimes our is very great\nfor visualization purposes\nand python has you know,\nAnother favorite languages\nwhen comes to machine\nlearning now next scale\nthat particular individual needs\nis calculus and statistics.\nSo a lot of machine learning\nalgorithms are mostly\nmaths and statistics.\nSo and a lot of static\nis required majorly\nthe matrix multiplication\nand all so good understanding\nof calculus as well as\nstatistic is required.\nNow next we have\nsignal processing now Advanced\nsignal processing is something\nthat will give you an upper Edge\nover other machine\nlearning engine is\nif you are Applying\nfor a job anywhere.\nNow the next kill we\nhave is applied maths\nas I mentioned earlier\nmany of the machine\nlearning algorithms here are\npurely mathematical formulas.\nSo a good understanding of maths\nand how the algorithm Works\nwill take you far ahead\nthe next on our list.\nWe have neural networks.\nNo real networks are something\nthat has been emerging\nquite popularly in the recent\nyears and due to its efficiency\nand the extent to which it\ncan walk and get the results\nas soon as possible.\nNeural networks are a must\nfor machine learning engine\nnow moving forward.\nWe have language processing.\nSo a lot of times machine\nlearning Engineers have to deal\nwith text Data the voice data\nas well as video data now\nprocessing any kind\nof language audio\nor the video is something\nthat a machine-learning engineer\nhas to do on a daily basis.\nSo one needs to be proficient\nin this area also now,\nthese are only some\nof the few skills\nwhich are absolutely necessary.\nI would say for\nany machine learning\nand Engineer so let's\nnow discuss the job description\nor the roles\nand responsibilities\nof a particular machine\nlearning engineer now\ndepending on their level\nof expertise machine\nlearning Engineers may have\nto study and transform\ndata science prototypes.\nThey need to design\nmachine Learning Systems.\nThey also need to research and\nImplement appropriate machine\nlearning algorithms and tools\nas it's a very\nimportant part of the job.\nThey need to develop\nnew machine learning application\naccording to the industry\nrequirements the Select\nthe appropriate data sets\nand the data\nrepresentation methods\nbecause if there is a slight\ndeviation in the data set\nand the data representation\nthat's going to\naffect Model A lot.\nThey need to run machine\nlearning tests and experiments.\nThey need to perform\nstatistical analysis\nand fine-tuning using\nthe test results.\nSo sometimes people ask\nwhat exactly is a difference\nbetween a data analyst\nand a machine learning engineer.\nSo so static analysis\njust a small part of of\nmachine learning Engineers job.\nWhereas it is a major part\nor it probably covers a large\npart of a data analyst job\nrather than a machine\nlearning Engineers job.\nSo machine learning\nEngineers might need to train\nand retrain the systems\nwhenever necessary\nand they also need\nto extend the existing\nmachine learning libraries\nand Frameworks to\ntheir full potential\nso that they could make\nthe model Works superbly\nand finally they need to keep\nabreast of the developments\nin the field needless to say\nthat any machine.\nIn general or any particular\nindividual has to stay updated\nto the technologies\nthat are coming in the market\nand every now and then\na new technology arises\nwhich will overthrow\nthe older one.\nSo you need to be\nup to date now coming\nto the resume part\nof a machine learning engineer.\nSo any resume of a particular\nmachine learning Engineers\nshould consist like clear\ncareer objective skills,\nwhich a particular\nindividual possesses\nthe educational qualification\ncertain certification\nthe past experience\nif you are an experienced\nmachine learning and Jen\nare the projects which you\nhave worked on and that's it.\nSo let's have a look\nat the various elements\nthat are required\nin a machine-learning\nEngineers resume.\nSo first of all,\nyou need to have\na clear career objective.\nSo here you will need\nnot stretch it too much\nand keep it as\nprecise as possible.\nSo next we have the skills\nrequired and these skills\ncan be technical as\nwell as non technical.\nSo let's have a look\nat the various Technical and\nnon-technical skills out here.\nSo starting with\nthe technical skills.\nFirst of all,\nwe have programming languages\nas an our Java Python and C++.\nBut the first\nand the foremost requirement\nis to have a good grip\non any programming languages\npreferably python\nas it is easy to learn\nand it's applications are wider\nthan any other language now,\nit is important to have\na good understanding of topics\nlike data structures memory\nmanagement and classes.\nAll the python is\na very good language it\nalone cannot help you\nso you will probably\nhave to learn all\nthese he's languages\nlike C++ are python Java\nand also work on mapreduce\nat some point of time\nthe next on our list.\nWe have calculus and linear\nalgebra and statistics.\nSo you'll need to be\nintimately familiar\nwith matrices the vectors\nand the matrix multiplication.\nSo statistics is going\nto come up a lot\nand at least make sure\nyou are familiar\nwith caution distribution\nmeans standard deviations\nand much more.\nSo you also need to have a firm\nunderstanding of probability.\nStats to understand the machine\nlearning models the next\nas I mentioned earlier, it's\nsignal processing techniques.\nSo feature extraction is one\nof the most important parts\nof machine learning\ndifferent types of problems\nneed various Solutions.\nSo you may be able to utilize\nthe really cool Advanced signal\nprocessing algorithms such as\nwavelengths shallots curve.\nLet's and the ballast\nso try to learn about\nthe time-frequency analysis and\ntry to apply it to your problems\nas it gives you an upper jaw.\nOur other machine\nlearning Engineers,\nso just go for the next we\nhave mathematics and a lot of\nmachine learning techniques out.\nThere are just fancy types\nof function approximation\nhaving a firm understanding\nof algorithm Theory and knowing\nhow the algorithm works\nis really necessary\nand understanding subjects\nlike gradient descent\nconvex optimization\nquadratic programming\nand partial differentiation will\nhelp a lot the neural networks\nas I was talking earlier.\nSo we need machine learning\nfor tasks that are too Flex\nfor humans to quote directly.\nSo that is the tasks\nthat are so complex\nthat it is Impractical neural\nnetworks are a class\nof models within the general\nmachine learning literature.\nThey are specific\nset of algorithms\nthat have revolutionized machine\nlearning deep neural networks\nhave proven to work quite well\nand neural networks\nare themself General\nfunction approximations,\nwhich is why they can be applied\nto almost any machine\nlearning problem out there\nand they help a lot\nabout learning a complex mapping\nfrom the input\nto The output space now next\nwe have language processing\nsince natural language\nprocessing combines two\nof the major areas of work\nthat are linguistic\nand computer science\nand chances are at some point\nyou are going to work\nwith either text\nor audio or the video.\nSo it's necessary to have\na control over libraries\nlike gents mm and ltk\nand techniques like\nwhat to wet sentimental analysis\nand text summarization Now voice\nand audio analysis involves\nextracting useful information\nfrom the Your signals themselves\nvery well versed in maths\nand concept like Fourier\ntransformation will get\nyou far in this one.\nThese were the technical skills\nthat are required but be assured\nthat there are a lot\nof non technical skills.\nAlso that are required\nto land a good job\nin a machine learning industry.\nSo first of all,\nyou need to have\nan industry knowledge.\nSo the most successful\nmachine learning projects out.\nThere are going to be those\nthat address real pain points,\ndon't you agree?\nSo whichever industry\nare working for You should know\nhow that industry works\nand what will be beneficial\nfor the industry.\nNow, if a machine\nlearning engineer\ndoes not have business Acumen\nand the know-how of the elements\nthat make up\na successful business model.\nAll those technical\nskills cannot be\nchanneled productively.\nYou won't be able\nto discern the problems\nand the potential challenges\nthat need solving\nfor the business to sustain\nand grow the next on our list.\nWe have effective communication\nand not this is one\nof the most important parts\nin any job requirements.\nSo you'll need to In machine\nlearning Concepts to people\nwith little to no expertise\nin the field a chances are\nyou will need to work\nwith a team of Engineers\nas well as many other\nteams like marketing\nand the sales team.\nSo communication is going\nto make all of this much\neasier companies searching\nfor the strong machine learning\nengineer looking for someone\nwho can clearly and fluency\ntranslate technical findings\nto a non technical team.\nRapid prototyping\nis another skill,\nwhich is very much required for\nany machine learning engineer.\nSo iterating on ideas as\nquickly as possible is mandatory\nfor finding the one\nthat works in machine learning\nthis applies to everything\nfrom picking the right model\nto working on projects\nsuch as a/b testing\nand much more now you\nneed to do a group\nof techniques used to quickly\nfabricate a scale model\nof a physical part\nor assembly using\nthe three-dimensional\ncomputer aided design,\nwhich is the cat data now coming\nto the final skills,\nwhich will be required\nfor any machine learning agenda\nis to keep updated.\nSo you must stay up to date\nwith any upcoming changes\nevery month new neural\nnetwork models come out\nthat outperformed\nthe previous architecture.\nIt also means being aware of the\nnews regarding the development\nof the tools Theory\nand algorithms through research\npapers blocks conference videos\nand much more.\nNow another part of any machine\nlearning engineer's resume is\nthe education qualification.\nSo a bachelor's\nor master's degree in computer\nscience RIT economics statistics\nor even mathematics can help.\nUp you land a job\nin machine learning plus\nif you are an experienced\nmachine learning engineer,\nso probably some standard\ncompany certifications\nwill help you a lot\nwhen Landing a good job\nin machine learning\nand finally coming\nto the professional experience.\nYou need to have experience in\ncomputer science statistics data\nas is if you are switching\nfrom any other profession into\na machine learning engineer,\nor if you have a previous\nexperience in machine learning\nthat is very well.\nNow finally if we talk\nabout The projects\nso you need to have\nnot just any project\nthat you have worked on you\nneed to have working on machine\nlearning related projects\nthat involve a\ncertain level of AI\nand working on neural networks\nto a certain degree\nto land a good job as\na machine-learning engineer.\nNow if you have a look\nat the company's hiring machine\nlearning Engineers,\nso every other\ncompany is looking\nfor machine learning Engineers\nwho can modify the existing\nmodel to something\nthat did not need much more.\nOf Maintenance and cancel\nsustain so basically working\non artificial intelligence\nand new algorithms\nthat can work on their own is\nwhat every company deserves.\nSo Amazon Facebook.\nWe have Tech giants\nlike Microsoft IBM again\nin the gaming industry,\nwe have or the GPU\nindustry Graphics industry.\nWe have Nvidia\nin banking industry.\nWe have JPMorgan Chase again,\nwe have LinkedIn\nand also we have Walmart.\nSo all of these companies\nrequire machine learning engine\nat some part of the time.\nSo be assured that\nif you are looking for a machine\nlearning engineer post,\nevery other companies be it\na big shot company or even\nthe new startups are looking\nfor machine learning Engineers.\nSo be assured you will get\na job now with this we come\nto an end of this video.\nSo I hope you've got\na good understanding\nof who exactly are\nmachine learning engineer is\nthe way just job Trends\nthe salary Trends.\nWhat are the skills required to\nbecome machine learning engineer\nand once you become\na machine-learning engineer,\nwhat are the roles\nand responsibilities\nor the Job description\nwhat appears to be on the resume\nor the job description\nwhat appears to be\non the job application of\nany machine learning engineers?\nAnd also I hope you got to know\nhow to prepare your resume\nor how to prepare it\nin the correct format.\nAnd what on to keep their\nin the resume the career\nobjectives the skills\nTechnical and non-technical\nprevious experience\neducation qualification\nand certain projects\nwhich are related to it.\nSo that's it guys Ed Rica\nas you know provides\na machine learning.\nEngineer master's program now\nthat is aligned in such a way\nthat will get you acquainted\nin all the skills\nthat are required\nto become a machine\nlearning engine and that too\nin the correct form.\n",
  "words": [
    "sure",
    "agree",
    "machine",
    "learning",
    "one",
    "hottest",
    "trend",
    "today",
    "market",
    "right",
    "gartner",
    "predicts",
    "2022",
    "would",
    "least",
    "40",
    "new",
    "application",
    "development",
    "project",
    "going",
    "market",
    "would",
    "requiring",
    "machine",
    "learning",
    "team",
    "expected",
    "project",
    "generate",
    "revenue",
    "around",
    "three",
    "point",
    "nine",
    "trillion",
    "dollar",
    "cute",
    "looking",
    "huge",
    "upcoming",
    "demand",
    "machine",
    "learning",
    "around",
    "world",
    "guys",
    "eureka",
    "come",
    "designed",
    "machine",
    "learning",
    "full",
    "course",
    "guys",
    "actually",
    "drill",
    "let",
    "introduce",
    "hello",
    "atul",
    "edureka",
    "today",
    "guiding",
    "entire",
    "machine",
    "learning",
    "course",
    "well",
    "course",
    "designed",
    "way",
    "get",
    "slowly",
    "gradually",
    "start",
    "beginner",
    "level",
    "move",
    "towards",
    "advanced",
    "topic",
    "without",
    "delaying",
    "let",
    "start",
    "agenda",
    "today",
    "action",
    "machine",
    "learning",
    "course",
    "segregated",
    "six",
    "different",
    "module",
    "start",
    "first",
    "module",
    "introduction",
    "machine",
    "learning",
    "discuss",
    "things",
    "like",
    "exactly",
    "machine",
    "learning",
    "differs",
    "artificial",
    "intelligence",
    "planning",
    "various",
    "types",
    "dead",
    "space",
    "application",
    "finally",
    "end",
    "first",
    "module",
    "basic",
    "demo",
    "python",
    "okay",
    "second",
    "module",
    "focuses",
    "starts",
    "probability",
    "cover",
    "things",
    "like",
    "descriptive",
    "statistics",
    "inferential",
    "statistics",
    "bob",
    "rarity",
    "theory",
    "third",
    "module",
    "unsupervised",
    "learning",
    "well",
    "supervised",
    "learning",
    "one",
    "type",
    "machine",
    "learning",
    "focuses",
    "mainly",
    "regression",
    "classification",
    "type",
    "problem",
    "deals",
    "label",
    "data",
    "sets",
    "algorithm",
    "part",
    "linear",
    "regression",
    "logistic",
    "regression",
    "napier",
    "random",
    "forest",
    "decision",
    "tree",
    "fourth",
    "module",
    "unsupervised",
    "learning",
    "well",
    "module",
    "focuses",
    "mainly",
    "dealing",
    "unlabeled",
    "data",
    "sets",
    "algorithm",
    "part",
    "offered",
    "algorithm",
    "priori",
    "algorithm",
    "part",
    "fifth",
    "module",
    "reinforcement",
    "learning",
    "going",
    "discuss",
    "reinforcement",
    "learning",
    "depth",
    "also",
    "q",
    "learning",
    "algorithm",
    "finally",
    "end",
    "make",
    "industry",
    "ready",
    "okay",
    "going",
    "discuss",
    "three",
    "different",
    "projects",
    "based",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "finally",
    "end",
    "tell",
    "skills",
    "need",
    "become",
    "machine",
    "learnings",
    "jean",
    "nia",
    "okay",
    "also",
    "discussing",
    "important",
    "questions",
    "asked",
    "interview",
    "fine",
    "come",
    "end",
    "agenda",
    "move",
    "ahead",
    "forget",
    "subscribe",
    "dareka",
    "press",
    "bell",
    "icon",
    "never",
    "miss",
    "update",
    "us",
    "hello",
    "everyone",
    "toll",
    "eureka",
    "welcome",
    "today",
    "session",
    "machine",
    "learning",
    "know",
    "living",
    "world",
    "humans",
    "machines",
    "humans",
    "evolving",
    "learning",
    "past",
    "experience",
    "since",
    "millions",
    "years",
    "hand",
    "era",
    "machines",
    "robots",
    "begun",
    "today",
    "world",
    "machines",
    "rewards",
    "like",
    "need",
    "program",
    "actually",
    "follow",
    "instructions",
    "machine",
    "started",
    "learn",
    "machine",
    "learning",
    "comes",
    "picture",
    "machine",
    "learning",
    "core",
    "many",
    "futuristic",
    "technology",
    "advancement",
    "world",
    "today",
    "see",
    "various",
    "examples",
    "implementation",
    "machine",
    "learning",
    "around",
    "us",
    "tesla",
    "car",
    "apple",
    "siri",
    "sophia",
    "bot",
    "many",
    "exactly",
    "machine",
    "learning",
    "well",
    "machine",
    "learning",
    "subfield",
    "artificial",
    "intelligence",
    "focuses",
    "design",
    "system",
    "learn",
    "make",
    "decisions",
    "predictions",
    "based",
    "experience",
    "data",
    "case",
    "machines",
    "machine",
    "learning",
    "enables",
    "computer",
    "act",
    "make",
    "decisions",
    "rather",
    "explicitly",
    "programmed",
    "carry",
    "certain",
    "task",
    "programs",
    "designed",
    "learn",
    "improve",
    "time",
    "exposed",
    "new",
    "data",
    "let",
    "move",
    "discuss",
    "one",
    "biggest",
    "confusion",
    "people",
    "world",
    "think",
    "three",
    "ai",
    "machine",
    "learning",
    "deep",
    "learning",
    "know",
    "wrong",
    "let",
    "clarify",
    "things",
    "artificial",
    "intelligence",
    "broader",
    "concept",
    "machines",
    "able",
    "carry",
    "tasks",
    "smarter",
    "way",
    "covers",
    "anything",
    "enables",
    "computer",
    "like",
    "humans",
    "think",
    "famous",
    "turing",
    "test",
    "determine",
    "whether",
    "computer",
    "capable",
    "thinking",
    "like",
    "human",
    "talking",
    "siri",
    "phone",
    "get",
    "answer",
    "already",
    "close",
    "artificial",
    "intelligence",
    "coming",
    "machine",
    "learning",
    "part",
    "already",
    "said",
    "machine",
    "learning",
    "subset",
    "current",
    "application",
    "ai",
    "based",
    "idea",
    "able",
    "give",
    "machine",
    "access",
    "data",
    "let",
    "learn",
    "done",
    "cells",
    "subset",
    "artificial",
    "intelligence",
    "deals",
    "extraction",
    "pattern",
    "data",
    "set",
    "means",
    "machine",
    "find",
    "rules",
    "optimal",
    "behavior",
    "also",
    "adapt",
    "changes",
    "world",
    "many",
    "algorithms",
    "involved",
    "known",
    "decades",
    "centuries",
    "even",
    "thanks",
    "advances",
    "computer",
    "science",
    "parallel",
    "computing",
    "scale",
    "massive",
    "data",
    "volumes",
    "machine",
    "learning",
    "part",
    "coming",
    "deep",
    "learning",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "similar",
    "machine",
    "learning",
    "tamar",
    "used",
    "train",
    "deep",
    "neural",
    "network",
    "achieve",
    "better",
    "accuracy",
    "cases",
    "former",
    "performing",
    "mark",
    "right",
    "hope",
    "understood",
    "machine",
    "learning",
    "ai",
    "deep",
    "learning",
    "three",
    "different",
    "okay",
    "moving",
    "ahead",
    "let",
    "see",
    "general",
    "machine",
    "learning",
    "work",
    "one",
    "approaches",
    "machine",
    "learning",
    "algorithm",
    "strained",
    "using",
    "labeled",
    "unlabeled",
    "training",
    "data",
    "set",
    "produce",
    "model",
    "new",
    "input",
    "data",
    "introduced",
    "machine",
    "learning",
    "algorithm",
    "make",
    "prediction",
    "based",
    "model",
    "prediction",
    "evaluated",
    "accuracy",
    "accuracy",
    "acceptable",
    "machine",
    "learning",
    "algorithm",
    "deployed",
    "accuracy",
    "acceptable",
    "machine",
    "learning",
    "algorithm",
    "strained",
    "argument",
    "training",
    "data",
    "set",
    "example",
    "many",
    "factor",
    "steps",
    "involved",
    "let",
    "move",
    "subcategorize",
    "machine",
    "learning",
    "three",
    "different",
    "types",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "let",
    "see",
    "work",
    "work",
    "used",
    "field",
    "banking",
    "healthcare",
    "retail",
    "domains",
    "worry",
    "make",
    "sure",
    "use",
    "enough",
    "examples",
    "implementation",
    "three",
    "give",
    "proper",
    "understanding",
    "starting",
    "supervised",
    "learning",
    "let",
    "see",
    "mathematical",
    "definition",
    "supervised",
    "learning",
    "supervised",
    "learning",
    "input",
    "variables",
    "x",
    "output",
    "variable",
    "use",
    "algorithm",
    "learn",
    "mapping",
    "function",
    "input",
    "output",
    "affects",
    "goal",
    "approximate",
    "mapping",
    "function",
    "well",
    "whenever",
    "new",
    "input",
    "data",
    "x",
    "could",
    "predict",
    "output",
    "variable",
    "data",
    "right",
    "think",
    "confusing",
    "let",
    "simplify",
    "definition",
    "supervised",
    "learning",
    "rephrase",
    "understanding",
    "mathematical",
    "definition",
    "machine",
    "learning",
    "method",
    "instances",
    "training",
    "data",
    "set",
    "composed",
    "different",
    "input",
    "attribute",
    "expected",
    "output",
    "input",
    "attributes",
    "training",
    "data",
    "set",
    "end",
    "data",
    "pixel",
    "image",
    "value",
    "data",
    "base",
    "row",
    "even",
    "audio",
    "frequency",
    "histogram",
    "right",
    "input",
    "instance",
    "expected",
    "output",
    "values",
    "associated",
    "value",
    "discreet",
    "representing",
    "category",
    "real",
    "continuous",
    "value",
    "either",
    "case",
    "algorithm",
    "learns",
    "input",
    "pattern",
    "generate",
    "expected",
    "output",
    "algorithm",
    "strain",
    "used",
    "predict",
    "correct",
    "output",
    "never",
    "seen",
    "input",
    "see",
    "image",
    "screen",
    "right",
    "image",
    "see",
    "feeding",
    "raw",
    "inputs",
    "image",
    "apple",
    "algorithm",
    "part",
    "algorithm",
    "supervisor",
    "keeps",
    "correcting",
    "machine",
    "keeps",
    "training",
    "machine",
    "keeps",
    "telling",
    "yes",
    "apple",
    "apple",
    "things",
    "like",
    "process",
    "keeps",
    "repeating",
    "get",
    "final",
    "train",
    "model",
    "model",
    "ready",
    "easily",
    "predict",
    "correct",
    "output",
    "never",
    "seen",
    "input",
    "slide",
    "see",
    "giving",
    "image",
    "green",
    "apple",
    "machine",
    "machine",
    "easily",
    "identify",
    "yes",
    "apple",
    "giving",
    "correct",
    "result",
    "right",
    "let",
    "make",
    "things",
    "clearer",
    "let",
    "discuss",
    "another",
    "example",
    "slide",
    "image",
    "shows",
    "example",
    "supervised",
    "learning",
    "process",
    "used",
    "produce",
    "model",
    "capable",
    "recognizing",
    "ducks",
    "image",
    "training",
    "data",
    "set",
    "composed",
    "labeled",
    "picture",
    "ducks",
    "non",
    "ducks",
    "result",
    "supervised",
    "learning",
    "process",
    "predictor",
    "model",
    "capable",
    "associating",
    "label",
    "duck",
    "duck",
    "new",
    "image",
    "presented",
    "model",
    "one",
    "strain",
    "resulting",
    "predictive",
    "model",
    "deployed",
    "production",
    "environment",
    "see",
    "mobile",
    "app",
    "example",
    "deployed",
    "ready",
    "recognize",
    "new",
    "pictures",
    "right",
    "might",
    "wondering",
    "category",
    "machine",
    "learning",
    "named",
    "supervised",
    "learning",
    "well",
    "called",
    "supervised",
    "learning",
    "process",
    "algorithm",
    "learning",
    "training",
    "data",
    "set",
    "thought",
    "teacher",
    "supervising",
    "learning",
    "process",
    "know",
    "correct",
    "answers",
    "go",
    "rhythm",
    "iteratively",
    "makes",
    "predicting",
    "training",
    "data",
    "corrected",
    "teacher",
    "learning",
    "stops",
    "algorithm",
    "achieves",
    "acceptable",
    "level",
    "performance",
    "let",
    "move",
    "see",
    "popular",
    "supervised",
    "learning",
    "algorithm",
    "linear",
    "regression",
    "random",
    "forest",
    "support",
    "vector",
    "machines",
    "information",
    "discuss",
    "algorithms",
    "next",
    "video",
    "let",
    "see",
    "popular",
    "use",
    "cases",
    "supervised",
    "learning",
    "donna",
    "codon",
    "speech",
    "automation",
    "mobile",
    "phone",
    "trains",
    "using",
    "voice",
    "one",
    "strain",
    "start",
    "working",
    "based",
    "training",
    "application",
    "supervised",
    "learning",
    "suppose",
    "telling",
    "ok",
    "google",
    "call",
    "sam",
    "say",
    "hey",
    "siri",
    "call",
    "sam",
    "get",
    "answer",
    "action",
    "performed",
    "automatically",
    "call",
    "goes",
    "sam",
    "example",
    "supervised",
    "learning",
    "next",
    "comes",
    "weather",
    "based",
    "prior",
    "knowledge",
    "like",
    "sunny",
    "temperature",
    "high",
    "fire",
    "cloudy",
    "humidity",
    "higher",
    "kind",
    "predict",
    "parameters",
    "given",
    "time",
    "also",
    "example",
    "supervised",
    "learning",
    "feeding",
    "data",
    "machine",
    "telling",
    "whenever",
    "sunny",
    "temperature",
    "higher",
    "whenever",
    "cloudy",
    "humidity",
    "higher",
    "example",
    "supervised",
    "learning",
    "another",
    "example",
    "biometric",
    "attendance",
    "train",
    "machine",
    "couple",
    "inputs",
    "biometric",
    "identity",
    "beat",
    "thumb",
    "iris",
    "yellow",
    "anything",
    "trained",
    "machine",
    "gun",
    "validate",
    "future",
    "input",
    "identify",
    "next",
    "comes",
    "field",
    "banking",
    "sector",
    "banking",
    "sector",
    "supervised",
    "learning",
    "used",
    "predict",
    "credit",
    "worthiness",
    "credit",
    "card",
    "holder",
    "building",
    "machine",
    "learning",
    "model",
    "look",
    "faulty",
    "attributes",
    "providing",
    "data",
    "deliquent",
    "customers",
    "next",
    "comes",
    "healthcare",
    "sector",
    "healthcare",
    "sector",
    "used",
    "predict",
    "patient",
    "readmission",
    "rates",
    "building",
    "regression",
    "model",
    "providing",
    "data",
    "patients",
    "treatment",
    "administration",
    "readmissions",
    "show",
    "variables",
    "best",
    "correlate",
    "readmission",
    "next",
    "comes",
    "retail",
    "sector",
    "retail",
    "sector",
    "used",
    "analyze",
    "product",
    "customer",
    "together",
    "building",
    "supervised",
    "model",
    "identify",
    "frequent",
    "itemsets",
    "association",
    "rule",
    "transactional",
    "data",
    "lets",
    "learn",
    "next",
    "category",
    "machine",
    "learning",
    "unsupervised",
    "part",
    "mathematically",
    "unsupervised",
    "learning",
    "put",
    "data",
    "x",
    "corresponding",
    "output",
    "variable",
    "goal",
    "unsupervised",
    "learning",
    "model",
    "underlying",
    "structure",
    "distribution",
    "data",
    "order",
    "learn",
    "data",
    "let",
    "rephrase",
    "simple",
    "terms",
    "unsupervised",
    "learning",
    "approach",
    "data",
    "instances",
    "training",
    "data",
    "set",
    "expected",
    "output",
    "associated",
    "instead",
    "unsupervised",
    "learning",
    "algorithm",
    "detects",
    "pattern",
    "based",
    "innate",
    "characteristics",
    "input",
    "data",
    "example",
    "machine",
    "learning",
    "tasks",
    "ask",
    "applies",
    "unsupervised",
    "learning",
    "clustering",
    "task",
    "similar",
    "data",
    "instances",
    "grouped",
    "together",
    "order",
    "identify",
    "clusters",
    "data",
    "slide",
    "see",
    "initially",
    "different",
    "varieties",
    "fruits",
    "input",
    "set",
    "fruits",
    "input",
    "x",
    "given",
    "model",
    "model",
    "trained",
    "using",
    "unsupervised",
    "learning",
    "algorithm",
    "model",
    "create",
    "clusters",
    "basis",
    "training",
    "grip",
    "similar",
    "fruits",
    "make",
    "cluster",
    "let",
    "make",
    "things",
    "clearer",
    "let",
    "take",
    "another",
    "example",
    "slide",
    "image",
    "shows",
    "example",
    "unsupervised",
    "learning",
    "process",
    "algorithm",
    "processes",
    "unlabeled",
    "training",
    "data",
    "set",
    "based",
    "characteristics",
    "grips",
    "picture",
    "three",
    "different",
    "clusters",
    "data",
    "despite",
    "ability",
    "grouping",
    "similar",
    "data",
    "clusters",
    "algorithm",
    "capable",
    "add",
    "labels",
    "crow",
    "algorithm",
    "knows",
    "data",
    "instances",
    "similar",
    "identify",
    "meaning",
    "group",
    "might",
    "wondering",
    "category",
    "machine",
    "learning",
    "named",
    "unsupervised",
    "learning",
    "called",
    "unsupervised",
    "learning",
    "unlike",
    "supervised",
    "learning",
    "ever",
    "correct",
    "answer",
    "teacher",
    "algorithms",
    "left",
    "discover",
    "present",
    "interesting",
    "structure",
    "data",
    "let",
    "move",
    "see",
    "popular",
    "unsupervised",
    "learning",
    "algorithm",
    "apriori",
    "algorithm",
    "hierarchical",
    "clustering",
    "let",
    "move",
    "see",
    "examples",
    "learning",
    "suppose",
    "friend",
    "invites",
    "party",
    "meet",
    "totally",
    "strangers",
    "classify",
    "using",
    "unsupervised",
    "learning",
    "prior",
    "knowledge",
    "classification",
    "done",
    "basis",
    "gender",
    "age",
    "group",
    "dressing",
    "education",
    "qualification",
    "whatever",
    "way",
    "might",
    "like",
    "learning",
    "different",
    "supervised",
    "learning",
    "since",
    "use",
    "pasta",
    "prior",
    "knowledge",
    "people",
    "kept",
    "classifying",
    "go",
    "kept",
    "coming",
    "kept",
    "classifying",
    "yeah",
    "category",
    "people",
    "belong",
    "group",
    "category",
    "people",
    "belong",
    "group",
    "okay",
    "let",
    "see",
    "one",
    "example",
    "let",
    "suppose",
    "never",
    "seen",
    "football",
    "match",
    "chance",
    "watch",
    "video",
    "internet",
    "easily",
    "classify",
    "players",
    "basis",
    "different",
    "criterion",
    "like",
    "player",
    "wearing",
    "kind",
    "jersey",
    "one",
    "class",
    "player",
    "wearing",
    "different",
    "kind",
    "jersey",
    "different",
    "class",
    "classify",
    "basis",
    "playing",
    "style",
    "like",
    "guys",
    "attacker",
    "one",
    "class",
    "defender",
    "another",
    "class",
    "classify",
    "whatever",
    "way",
    "observe",
    "things",
    "also",
    "example",
    "unsupervised",
    "learning",
    "let",
    "move",
    "see",
    "unsupervised",
    "learning",
    "used",
    "sectors",
    "banking",
    "healthcare",
    "undertale",
    "starting",
    "banking",
    "sector",
    "banking",
    "sector",
    "used",
    "segment",
    "customers",
    "behavioral",
    "characteristic",
    "surveying",
    "prospects",
    "customers",
    "develop",
    "multiple",
    "segments",
    "using",
    "clustering",
    "healthcare",
    "sector",
    "used",
    "categorize",
    "mri",
    "data",
    "normal",
    "abnormal",
    "ages",
    "uses",
    "deep",
    "learning",
    "techniques",
    "build",
    "model",
    "learns",
    "different",
    "features",
    "images",
    "recognize",
    "different",
    "pattern",
    "next",
    "retail",
    "sector",
    "retail",
    "sector",
    "used",
    "recommend",
    "products",
    "customer",
    "based",
    "past",
    "purchases",
    "building",
    "collaborative",
    "filtering",
    "model",
    "based",
    "past",
    "purchases",
    "assume",
    "guys",
    "proper",
    "idea",
    "unsupervised",
    "learning",
    "means",
    "slightest",
    "doubt",
    "hesitate",
    "add",
    "doubt",
    "section",
    "let",
    "discuss",
    "third",
    "last",
    "type",
    "machine",
    "learning",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "well",
    "reinforcement",
    "learning",
    "type",
    "machine",
    "learning",
    "algorithm",
    "allows",
    "software",
    "agents",
    "machine",
    "automatically",
    "determine",
    "ideal",
    "behavior",
    "within",
    "specific",
    "context",
    "maximize",
    "performance",
    "reinforcement",
    "learning",
    "interaction",
    "two",
    "elements",
    "environment",
    "learning",
    "agent",
    "learning",
    "agent",
    "leverages",
    "mechanism",
    "namely",
    "exploration",
    "exploitation",
    "learning",
    "agent",
    "acts",
    "trial",
    "error",
    "basis",
    "termed",
    "exploration",
    "acts",
    "based",
    "knowledge",
    "gained",
    "environment",
    "referred",
    "exploitation",
    "environment",
    "rewards",
    "agent",
    "correct",
    "actions",
    "reinforcement",
    "signal",
    "leveraging",
    "rewards",
    "obtain",
    "agent",
    "improves",
    "environment",
    "knowledge",
    "select",
    "next",
    "action",
    "image",
    "see",
    "machine",
    "confused",
    "whether",
    "apple",
    "apple",
    "sheena",
    "chain",
    "using",
    "reinforcement",
    "learning",
    "makes",
    "correct",
    "decision",
    "get",
    "rewards",
    "point",
    "case",
    "wrong",
    "gets",
    "penalty",
    "training",
    "done",
    "machine",
    "easily",
    "identify",
    "one",
    "apple",
    "let",
    "see",
    "example",
    "see",
    "agent",
    "judge",
    "environment",
    "find",
    "two",
    "duck",
    "first",
    "task",
    "observe",
    "environment",
    "next",
    "select",
    "action",
    "using",
    "policy",
    "seems",
    "machine",
    "made",
    "wrong",
    "decision",
    "bye",
    "choosing",
    "bunny",
    "duck",
    "machine",
    "get",
    "penalty",
    "example",
    "wrong",
    "answer",
    "right",
    "machine",
    "update",
    "policy",
    "continue",
    "till",
    "machine",
    "gets",
    "optimal",
    "policy",
    "next",
    "time",
    "machine",
    "know",
    "bunny",
    "duck",
    "let",
    "see",
    "use",
    "cases",
    "reinforcement",
    "learning",
    "lets",
    "see",
    "pavlo",
    "trained",
    "dog",
    "using",
    "reinforcement",
    "learning",
    "applied",
    "reinforcement",
    "method",
    "train",
    "dog",
    "babu",
    "integrated",
    "learning",
    "four",
    "stages",
    "initially",
    "pavlo",
    "gave",
    "dog",
    "response",
    "meet",
    "dog",
    "started",
    "salivating",
    "next",
    "created",
    "sound",
    "bell",
    "dog",
    "respond",
    "anything",
    "third",
    "part",
    "tried",
    "condition",
    "dog",
    "using",
    "bell",
    "giving",
    "food",
    "seeing",
    "food",
    "dog",
    "started",
    "salivating",
    "eventually",
    "situation",
    "came",
    "dog",
    "started",
    "salivating",
    "hearing",
    "bell",
    "even",
    "food",
    "given",
    "dog",
    "reinforced",
    "whenever",
    "master",
    "ring",
    "bell",
    "get",
    "food",
    "let",
    "move",
    "see",
    "reinforcement",
    "learning",
    "applied",
    "field",
    "banking",
    "healthcare",
    "retail",
    "sector",
    "starting",
    "banking",
    "sector",
    "banking",
    "sector",
    "reinforcement",
    "learning",
    "used",
    "create",
    "next",
    "best",
    "offer",
    "model",
    "call",
    "center",
    "building",
    "predictive",
    "model",
    "learns",
    "time",
    "user",
    "accept",
    "reject",
    "offer",
    "made",
    "sales",
    "staff",
    "fine",
    "healthcare",
    "sector",
    "used",
    "allocate",
    "scars",
    "resources",
    "handle",
    "different",
    "type",
    "er",
    "cases",
    "building",
    "markov",
    "decision",
    "process",
    "learns",
    "treatment",
    "strategies",
    "type",
    "er",
    "case",
    "next",
    "last",
    "comes",
    "retail",
    "sector",
    "let",
    "see",
    "reinforcement",
    "learning",
    "applied",
    "retail",
    "sector",
    "retail",
    "sector",
    "used",
    "reduce",
    "excess",
    "stock",
    "dynamic",
    "pricing",
    "building",
    "dynamic",
    "pricing",
    "model",
    "price",
    "based",
    "customer",
    "response",
    "offers",
    "hope",
    "attained",
    "understanding",
    "machine",
    "learning",
    "ready",
    "move",
    "move",
    "ahead",
    "welcome",
    "today",
    "topic",
    "discussion",
    "ai",
    "versus",
    "machine",
    "learning",
    "versus",
    "deep",
    "learning",
    "term",
    "confused",
    "lot",
    "people",
    "two",
    "one",
    "among",
    "let",
    "resolve",
    "well",
    "artificial",
    "intelligence",
    "broader",
    "umbrella",
    "machine",
    "learning",
    "deep",
    "learning",
    "come",
    "also",
    "see",
    "diagram",
    "even",
    "deep",
    "learning",
    "subset",
    "machine",
    "learning",
    "say",
    "three",
    "ai",
    "machine",
    "learning",
    "deep",
    "learning",
    "subset",
    "let",
    "move",
    "understand",
    "exactly",
    "differ",
    "let",
    "start",
    "artificial",
    "intelligence",
    "term",
    "artificial",
    "intelligence",
    "first",
    "coined",
    "year",
    "concept",
    "pretty",
    "old",
    "gained",
    "popularity",
    "recently",
    "well",
    "reason",
    "earlier",
    "small",
    "amount",
    "data",
    "data",
    "enough",
    "predict",
    "turret",
    "result",
    "tremendous",
    "increase",
    "amount",
    "data",
    "statistics",
    "suggest",
    "2020",
    "accumulated",
    "volume",
    "data",
    "increase",
    "zettabyte",
    "stew",
    "roughly",
    "around",
    "44",
    "zettabytes",
    "44",
    "trillion",
    "jeebies",
    "data",
    "along",
    "enormous",
    "amount",
    "data",
    "advanced",
    "algorithm",
    "computing",
    "power",
    "storage",
    "deal",
    "large",
    "amount",
    "data",
    "result",
    "expected",
    "70",
    "price",
    "implement",
    "next",
    "12",
    "months",
    "40",
    "percent",
    "2016",
    "51",
    "percent",
    "understanding",
    "ai",
    "well",
    "nothing",
    "technique",
    "enables",
    "machine",
    "act",
    "like",
    "humans",
    "replicating",
    "behavior",
    "nature",
    "ai",
    "possible",
    "machine",
    "learn",
    "experience",
    "machines",
    "responses",
    "based",
    "new",
    "input",
    "performing",
    "tasks",
    "artificial",
    "intelligence",
    "accomplish",
    "specific",
    "tasks",
    "processing",
    "large",
    "amount",
    "data",
    "recognizing",
    "pattern",
    "consider",
    "building",
    "artificial",
    "intelligence",
    "like",
    "building",
    "church",
    "first",
    "church",
    "took",
    "generations",
    "finish",
    "workers",
    "working",
    "never",
    "saw",
    "final",
    "outcome",
    "working",
    "took",
    "pride",
    "craft",
    "building",
    "bricks",
    "chiseling",
    "stone",
    "going",
    "placed",
    "great",
    "structure",
    "ai",
    "researchers",
    "think",
    "humble",
    "brick",
    "makers",
    "job",
    "study",
    "build",
    "components",
    "example",
    "parts",
    "planners",
    "learning",
    "algorithm",
    "etc",
    "anything",
    "someday",
    "someone",
    "somewhere",
    "integrate",
    "intelligent",
    "systems",
    "examples",
    "artificial",
    "intelligence",
    "life",
    "apple",
    "series",
    "computer",
    "tesla",
    "car",
    "many",
    "examples",
    "based",
    "deep",
    "learning",
    "natural",
    "language",
    "processing",
    "well",
    "ai",
    "gains",
    "hype",
    "moving",
    "ahead",
    "let",
    "gus",
    "machine",
    "learning",
    "see",
    "introduced",
    "well",
    "machine",
    "learning",
    "came",
    "existence",
    "late",
    "80s",
    "early",
    "90s",
    "issues",
    "people",
    "made",
    "machine",
    "learning",
    "come",
    "existence",
    "let",
    "us",
    "discuss",
    "one",
    "one",
    "field",
    "statistics",
    "problem",
    "efficiently",
    "train",
    "large",
    "complex",
    "model",
    "field",
    "computer",
    "science",
    "artificial",
    "intelligence",
    "problem",
    "train",
    "robust",
    "version",
    "ai",
    "system",
    "case",
    "neuroscience",
    "problem",
    "faced",
    "researchers",
    "design",
    "operation",
    "model",
    "brain",
    "issues",
    "largest",
    "influence",
    "led",
    "existence",
    "machine",
    "learning",
    "machine",
    "learning",
    "shifted",
    "focus",
    "symbolic",
    "approaches",
    "inherited",
    "ai",
    "move",
    "towards",
    "methods",
    "model",
    "borrowed",
    "statistics",
    "probability",
    "theory",
    "let",
    "proceed",
    "see",
    "exactly",
    "machine",
    "learning",
    "well",
    "machine",
    "learning",
    "subset",
    "ai",
    "enables",
    "computer",
    "act",
    "make",
    "decisions",
    "carry",
    "certain",
    "task",
    "programs",
    "algorithms",
    "designed",
    "way",
    "learn",
    "improve",
    "time",
    "exposed",
    "new",
    "data",
    "let",
    "see",
    "example",
    "machine",
    "learning",
    "let",
    "say",
    "want",
    "create",
    "system",
    "tells",
    "expected",
    "weight",
    "person",
    "based",
    "side",
    "first",
    "thing",
    "collect",
    "data",
    "let",
    "see",
    "data",
    "looks",
    "like",
    "point",
    "graph",
    "represent",
    "one",
    "data",
    "point",
    "start",
    "draw",
    "simple",
    "line",
    "predict",
    "weight",
    "based",
    "height",
    "sample",
    "simple",
    "line",
    "w",
    "equal",
    "x",
    "minus",
    "hundred",
    "w",
    "waiting",
    "kgs",
    "edges",
    "hide",
    "centimeter",
    "line",
    "help",
    "us",
    "make",
    "prediction",
    "main",
    "goal",
    "reduce",
    "difference",
    "estimated",
    "value",
    "actual",
    "value",
    "order",
    "achieve",
    "try",
    "draw",
    "straight",
    "line",
    "fits",
    "different",
    "points",
    "minimize",
    "error",
    "main",
    "goal",
    "minimize",
    "error",
    "make",
    "small",
    "possible",
    "decreasing",
    "error",
    "difference",
    "actual",
    "value",
    "estimated",
    "value",
    "increases",
    "performance",
    "model",
    "data",
    "points",
    "collect",
    "better",
    "model",
    "become",
    "also",
    "improve",
    "model",
    "adding",
    "variables",
    "creating",
    "different",
    "production",
    "lines",
    "line",
    "created",
    "next",
    "time",
    "feed",
    "new",
    "data",
    "example",
    "height",
    "person",
    "model",
    "would",
    "easily",
    "predict",
    "data",
    "tell",
    "predicted",
    "weight",
    "could",
    "hope",
    "got",
    "clear",
    "understanding",
    "machine",
    "learning",
    "moving",
    "ahead",
    "let",
    "learn",
    "deep",
    "learning",
    "deep",
    "learning",
    "consider",
    "deep",
    "learning",
    "model",
    "rocket",
    "engine",
    "fuel",
    "huge",
    "amount",
    "data",
    "feed",
    "algorithms",
    "concept",
    "deep",
    "learning",
    "new",
    "recently",
    "hype",
    "increase",
    "deep",
    "learning",
    "getting",
    "attention",
    "field",
    "particular",
    "kind",
    "machine",
    "learning",
    "inspired",
    "functionality",
    "brain",
    "cells",
    "called",
    "neurons",
    "led",
    "concept",
    "artificial",
    "neural",
    "network",
    "simply",
    "takes",
    "data",
    "connection",
    "artificial",
    "neurons",
    "adjust",
    "according",
    "data",
    "pattern",
    "neurons",
    "added",
    "size",
    "data",
    "large",
    "automatically",
    "features",
    "learning",
    "multiple",
    "levels",
    "abstraction",
    "thereby",
    "allowing",
    "system",
    "learn",
    "complex",
    "function",
    "mapping",
    "without",
    "depending",
    "specific",
    "algorithm",
    "know",
    "one",
    "actually",
    "knows",
    "happens",
    "inside",
    "neural",
    "network",
    "works",
    "well",
    "currently",
    "call",
    "black",
    "box",
    "let",
    "us",
    "discuss",
    "example",
    "deep",
    "learning",
    "understand",
    "better",
    "way",
    "let",
    "start",
    "simple",
    "example",
    "explain",
    "things",
    "conceptual",
    "level",
    "let",
    "us",
    "try",
    "understand",
    "would",
    "recognize",
    "square",
    "shapes",
    "first",
    "thing",
    "check",
    "whether",
    "four",
    "lines",
    "associated",
    "figure",
    "simple",
    "concept",
    "right",
    "yes",
    "check",
    "connected",
    "closed",
    "years",
    "finally",
    "check",
    "whether",
    "perpendicular",
    "sides",
    "equal",
    "correct",
    "everything",
    "fulfills",
    "yes",
    "square",
    "well",
    "nothing",
    "nested",
    "hierarchy",
    "concepts",
    "took",
    "complex",
    "task",
    "identifying",
    "square",
    "case",
    "broken",
    "simpler",
    "tasks",
    "deep",
    "learning",
    "also",
    "thing",
    "larger",
    "scale",
    "let",
    "take",
    "example",
    "machine",
    "recognizes",
    "animal",
    "task",
    "machine",
    "recognize",
    "whether",
    "given",
    "image",
    "cat",
    "dog",
    "asked",
    "resolve",
    "issue",
    "using",
    "concept",
    "machine",
    "learning",
    "would",
    "first",
    "would",
    "define",
    "features",
    "check",
    "whether",
    "animal",
    "whiskers",
    "check",
    "animal",
    "pointed",
    "ears",
    "whether",
    "tail",
    "straight",
    "curved",
    "short",
    "define",
    "facial",
    "features",
    "let",
    "system",
    "identify",
    "features",
    "important",
    "classifying",
    "particular",
    "animal",
    "comes",
    "deep",
    "learning",
    "takes",
    "one",
    "step",
    "ahead",
    "deep",
    "learning",
    "automatically",
    "finds",
    "feature",
    "important",
    "classification",
    "compare",
    "machine",
    "learning",
    "manually",
    "give",
    "features",
    "guess",
    "understood",
    "ai",
    "bigger",
    "picture",
    "machine",
    "learning",
    "deep",
    "learning",
    "apart",
    "let",
    "move",
    "focus",
    "discussion",
    "machine",
    "learning",
    "deep",
    "learning",
    "easiest",
    "way",
    "understand",
    "difference",
    "machine",
    "learning",
    "deep",
    "learning",
    "know",
    "deep",
    "learning",
    "machine",
    "learning",
    "specifically",
    "next",
    "evolution",
    "machine",
    "learning",
    "let",
    "take",
    "important",
    "parameter",
    "compare",
    "machine",
    "learning",
    "deep",
    "learning",
    "starting",
    "data",
    "dependencies",
    "important",
    "difference",
    "deep",
    "learning",
    "machine",
    "learning",
    "performance",
    "volume",
    "data",
    "gets",
    "graph",
    "see",
    "size",
    "data",
    "small",
    "deep",
    "learning",
    "algorithm",
    "perform",
    "well",
    "well",
    "deep",
    "learning",
    "algorithm",
    "needs",
    "large",
    "amount",
    "data",
    "understand",
    "perfectly",
    "hand",
    "machine",
    "learning",
    "algorithm",
    "easily",
    "work",
    "smaller",
    "data",
    "set",
    "fine",
    "next",
    "comes",
    "hardware",
    "dependencies",
    "deep",
    "learning",
    "algorithms",
    "heavily",
    "dependent",
    "machines",
    "machine",
    "learning",
    "algorithm",
    "work",
    "low",
    "machines",
    "well",
    "requirement",
    "deep",
    "learning",
    "algorithm",
    "include",
    "gpus",
    "integral",
    "part",
    "working",
    "deep",
    "learning",
    "algorithm",
    "requires",
    "gpus",
    "large",
    "amount",
    "matrix",
    "multiplication",
    "operations",
    "operations",
    "efficiently",
    "optimized",
    "using",
    "gpu",
    "built",
    "purpose",
    "third",
    "parameter",
    "feature",
    "engineering",
    "well",
    "feature",
    "engineering",
    "process",
    "putting",
    "domain",
    "knowledge",
    "reduce",
    "complexity",
    "data",
    "make",
    "patterns",
    "visible",
    "learning",
    "algorithms",
    "process",
    "difficult",
    "expensive",
    "terms",
    "time",
    "expertise",
    "case",
    "machine",
    "learning",
    "features",
    "needed",
    "identified",
    "expert",
    "hand",
    "coded",
    "per",
    "domain",
    "data",
    "type",
    "example",
    "features",
    "pixel",
    "value",
    "shapes",
    "texture",
    "position",
    "orientation",
    "anything",
    "fine",
    "performance",
    "machine",
    "learning",
    "algorithm",
    "depends",
    "accurately",
    "features",
    "identified",
    "stood",
    "case",
    "deep",
    "learning",
    "algorithms",
    "try",
    "learn",
    "high",
    "level",
    "features",
    "data",
    "distinctive",
    "part",
    "deep",
    "learning",
    "makes",
    "way",
    "ahead",
    "traditional",
    "machine",
    "learning",
    "deep",
    "learning",
    "reduces",
    "task",
    "developing",
    "new",
    "feature",
    "extractor",
    "every",
    "problem",
    "like",
    "case",
    "cnn",
    "algorithm",
    "first",
    "try",
    "learn",
    "features",
    "image",
    "edges",
    "lines",
    "proceeds",
    "parts",
    "faces",
    "people",
    "finally",
    "representation",
    "face",
    "hope",
    "things",
    "getting",
    "clearer",
    "let",
    "move",
    "ahead",
    "see",
    "next",
    "parameter",
    "next",
    "parameter",
    "problem",
    "solving",
    "approach",
    "solving",
    "problem",
    "using",
    "traditional",
    "machine",
    "learning",
    "algorithm",
    "generally",
    "recommended",
    "first",
    "break",
    "problem",
    "different",
    "sub",
    "parts",
    "solve",
    "individually",
    "finally",
    "combine",
    "get",
    "desired",
    "result",
    "machine",
    "learning",
    "algorithm",
    "handles",
    "problem",
    "hand",
    "deep",
    "learning",
    "algorithm",
    "solves",
    "problem",
    "end",
    "end",
    "let",
    "take",
    "example",
    "understand",
    "suppose",
    "task",
    "multiple",
    "object",
    "detection",
    "task",
    "identify",
    "object",
    "present",
    "image",
    "let",
    "see",
    "compare",
    "tackle",
    "issue",
    "using",
    "concept",
    "machine",
    "learning",
    "deep",
    "learning",
    "starting",
    "machine",
    "learning",
    "typical",
    "machine",
    "learning",
    "approach",
    "would",
    "first",
    "divide",
    "problem",
    "two",
    "step",
    "first",
    "object",
    "detection",
    "object",
    "recognization",
    "first",
    "would",
    "use",
    "bounding",
    "box",
    "detection",
    "algorithm",
    "like",
    "grab",
    "could",
    "fight",
    "sample",
    "scan",
    "image",
    "find",
    "possible",
    "objects",
    "objects",
    "recognized",
    "would",
    "use",
    "object",
    "recognization",
    "algorithm",
    "like",
    "svm",
    "hog",
    "recognize",
    "relevant",
    "objects",
    "finally",
    "combine",
    "result",
    "would",
    "able",
    "identify",
    "object",
    "present",
    "image",
    "hand",
    "deep",
    "learning",
    "approach",
    "would",
    "process",
    "end",
    "end",
    "example",
    "yellow",
    "net",
    "type",
    "deep",
    "learning",
    "algorithm",
    "would",
    "pass",
    "image",
    "would",
    "give",
    "location",
    "along",
    "name",
    "object",
    "let",
    "move",
    "fifth",
    "comparison",
    "parameter",
    "execution",
    "time",
    "usually",
    "deep",
    "learning",
    "algorithm",
    "takes",
    "long",
    "time",
    "train",
    "many",
    "parameter",
    "deep",
    "learning",
    "algorithm",
    "makes",
    "training",
    "longer",
    "usual",
    "training",
    "might",
    "even",
    "last",
    "two",
    "weeks",
    "training",
    "completely",
    "scratch",
    "whereas",
    "case",
    "machine",
    "learning",
    "relatively",
    "takes",
    "much",
    "less",
    "time",
    "train",
    "ranging",
    "weeks",
    "arts",
    "execution",
    "time",
    "completely",
    "reversed",
    "comes",
    "testing",
    "data",
    "testing",
    "deep",
    "learning",
    "algorithm",
    "takes",
    "much",
    "less",
    "time",
    "run",
    "whereas",
    "compare",
    "knn",
    "algorithm",
    "type",
    "machine",
    "learning",
    "algorithm",
    "test",
    "time",
    "increases",
    "size",
    "data",
    "increase",
    "last",
    "least",
    "interpretability",
    "factor",
    "comparison",
    "machine",
    "learning",
    "deep",
    "learning",
    "fact",
    "main",
    "reason",
    "deep",
    "learning",
    "still",
    "thought",
    "ten",
    "times",
    "anyone",
    "knew",
    "uses",
    "industry",
    "let",
    "take",
    "example",
    "suppose",
    "use",
    "deep",
    "learning",
    "give",
    "automated",
    "scoring",
    "two",
    "essays",
    "performance",
    "gives",
    "scoring",
    "quite",
    "excellent",
    "near",
    "human",
    "performance",
    "issue",
    "reveal",
    "white",
    "given",
    "score",
    "indeed",
    "mathematically",
    "possible",
    "find",
    "node",
    "deep",
    "neural",
    "network",
    "activated",
    "know",
    "neurons",
    "supposed",
    "model",
    "layers",
    "neurons",
    "collectively",
    "interpret",
    "result",
    "hand",
    "machine",
    "learning",
    "algorithm",
    "like",
    "decision",
    "tree",
    "gives",
    "us",
    "crisp",
    "rule",
    "void",
    "chose",
    "watered",
    "chose",
    "particularly",
    "easy",
    "interpret",
    "reasoning",
    "behind",
    "therefore",
    "algorithms",
    "like",
    "decision",
    "tree",
    "linear",
    "logistic",
    "regression",
    "primarily",
    "used",
    "industry",
    "interpretability",
    "let",
    "summarize",
    "things",
    "machine",
    "learning",
    "uses",
    "algorithm",
    "parse",
    "data",
    "learn",
    "data",
    "make",
    "informed",
    "decision",
    "based",
    "learned",
    "fine",
    "deep",
    "learning",
    "structures",
    "algorithms",
    "layers",
    "create",
    "artificial",
    "neural",
    "network",
    "learn",
    "make",
    "intelligent",
    "decisions",
    "finally",
    "deep",
    "learning",
    "subfield",
    "machine",
    "learning",
    "fall",
    "broad",
    "category",
    "artificial",
    "intelligence",
    "deep",
    "learning",
    "usually",
    "behind",
    "artificial",
    "intelligence",
    "early",
    "days",
    "scientists",
    "used",
    "lab",
    "notebook",
    "test",
    "progress",
    "results",
    "conclusions",
    "jupiter",
    "allows",
    "data",
    "scientists",
    "record",
    "complete",
    "analysis",
    "process",
    "much",
    "way",
    "scientists",
    "use",
    "lab",
    "notebook",
    "jupiter",
    "product",
    "originally",
    "developed",
    "part",
    "ipython",
    "project",
    "ipad",
    "project",
    "used",
    "provide",
    "interactive",
    "online",
    "access",
    "python",
    "time",
    "became",
    "useful",
    "interact",
    "data",
    "analysis",
    "tools",
    "manner",
    "split",
    "python",
    "tool",
    "crew",
    "current",
    "manifestation",
    "jupiter",
    "ipython",
    "still",
    "active",
    "tool",
    "available",
    "use",
    "name",
    "jupiter",
    "derived",
    "combination",
    "julia",
    "python",
    "jupiter",
    "runs",
    "code",
    "many",
    "programming",
    "languages",
    "python",
    "requirement",
    "installing",
    "jupyter",
    "notebook",
    "download",
    "jupyter",
    "notebook",
    "ways",
    "official",
    "website",
    "strongly",
    "recommended",
    "installing",
    "python",
    "jupiter",
    "using",
    "anaconda",
    "distribution",
    "includes",
    "python",
    "know",
    "book",
    "commonly",
    "used",
    "packages",
    "scientific",
    "computing",
    "well",
    "data",
    "science",
    "although",
    "one",
    "also",
    "using",
    "pipe",
    "installation",
    "method",
    "personally",
    "would",
    "suggest",
    "downloading",
    "app",
    "navigator",
    "desktop",
    "graphical",
    "user",
    "interface",
    "included",
    "anaconda",
    "allows",
    "launch",
    "application",
    "easily",
    "manage",
    "conda",
    "packages",
    "environments",
    "channels",
    "without",
    "need",
    "use",
    "command",
    "line",
    "commands",
    "need",
    "go",
    "another",
    "corner",
    "dot",
    "orgy",
    "inside",
    "go",
    "anaconda",
    "navigators",
    "see",
    "conda",
    "installation",
    "code",
    "going",
    "use",
    "install",
    "particular",
    "pc",
    "either",
    "use",
    "installers",
    "download",
    "anaconda",
    "navigator",
    "looks",
    "something",
    "like",
    "see",
    "jupiter",
    "lab",
    "jupyter",
    "notebook",
    "qt",
    "console",
    "ipython",
    "console",
    "spider",
    "somewhat",
    "similar",
    "studio",
    "terms",
    "python",
    "studio",
    "orange",
    "three",
    "glue",
    "vsc",
    "code",
    "focus",
    "today",
    "would",
    "jupyter",
    "notebook",
    "launch",
    "navigator",
    "see",
    "many",
    "options",
    "available",
    "launching",
    "python",
    "well",
    "instances",
    "definition",
    "jupyter",
    "notebook",
    "fundamentally",
    "json",
    "file",
    "number",
    "annotations",
    "three",
    "main",
    "parts",
    "metadata",
    "notebook",
    "format",
    "list",
    "cells",
    "get",
    "acquainted",
    "environment",
    "jupiter",
    "user",
    "interface",
    "number",
    "components",
    "important",
    "know",
    "components",
    "using",
    "daily",
    "basis",
    "get",
    "acquainted",
    "see",
    "focus",
    "today",
    "jupyter",
    "notebook",
    "let",
    "launched",
    "japan",
    "notebook",
    "creates",
    "online",
    "python",
    "instance",
    "use",
    "web",
    "let",
    "launch",
    "see",
    "jupiter",
    "top",
    "left",
    "expected",
    "acts",
    "button",
    "go",
    "home",
    "page",
    "whenever",
    "click",
    "get",
    "back",
    "particular",
    "home",
    "paste",
    "dashboard",
    "three",
    "tabs",
    "displayed",
    "files",
    "running",
    "clusters",
    "understand",
    "three",
    "understand",
    "importance",
    "three",
    "tabs",
    "file",
    "tab",
    "shows",
    "list",
    "current",
    "files",
    "directory",
    "see",
    "many",
    "files",
    "running",
    "tab",
    "presents",
    "another",
    "screen",
    "currently",
    "running",
    "processes",
    "notebooks",
    "list",
    "terminals",
    "notebooks",
    "populated",
    "running",
    "numbers",
    "see",
    "inside",
    "running",
    "terminals",
    "running",
    "notebooks",
    "cluster",
    "tab",
    "presents",
    "another",
    "screen",
    "display",
    "list",
    "clusters",
    "available",
    "see",
    "top",
    "right",
    "corner",
    "screen",
    "three",
    "buttons",
    "upload",
    "new",
    "refresh",
    "button",
    "let",
    "go",
    "back",
    "see",
    "upload",
    "new",
    "refresh",
    "button",
    "upload",
    "button",
    "used",
    "add",
    "files",
    "notebook",
    "space",
    "may",
    "also",
    "drag",
    "drop",
    "would",
    "handling",
    "files",
    "similarly",
    "drag",
    "drop",
    "notebooks",
    "specific",
    "folders",
    "well",
    "menu",
    "new",
    "top",
    "residents",
    "many",
    "text",
    "file",
    "folders",
    "terminal",
    "python",
    "test",
    "file",
    "option",
    "used",
    "add",
    "text",
    "file",
    "current",
    "directory",
    "jupiter",
    "open",
    "new",
    "browser",
    "window",
    "running",
    "new",
    "text",
    "editor",
    "text",
    "entered",
    "automatically",
    "saved",
    "displayed",
    "notebooks",
    "files",
    "display",
    "folder",
    "option",
    "creates",
    "new",
    "folder",
    "name",
    "untitled",
    "folder",
    "remember",
    "files",
    "folder",
    "names",
    "editable",
    "terminal",
    "option",
    "allows",
    "start",
    "ipython",
    "session",
    "node",
    "would",
    "options",
    "available",
    "activated",
    "additional",
    "note",
    "books",
    "available",
    "environment",
    "python",
    "3",
    "option",
    "used",
    "begin",
    "pythons",
    "recession",
    "interactively",
    "note",
    "interface",
    "looks",
    "like",
    "following",
    "screen",
    "shot",
    "full",
    "file",
    "editing",
    "capabilities",
    "script",
    "including",
    "saving",
    "new",
    "file",
    "also",
    "complete",
    "id",
    "python",
    "script",
    "come",
    "refresh",
    "button",
    "refresh",
    "button",
    "used",
    "update",
    "display",
    "really",
    "necessary",
    "display",
    "reactive",
    "changes",
    "underlying",
    "file",
    "structure",
    "talk",
    "files",
    "tab",
    "item",
    "check",
    "box",
    "drop",
    "menu",
    "home",
    "button",
    "see",
    "checkbox",
    "menu",
    "home",
    "button",
    "check",
    "box",
    "used",
    "toggle",
    "checkboxes",
    "item",
    "list",
    "see",
    "select",
    "either",
    "move",
    "either",
    "delete",
    "file",
    "selected",
    "select",
    "deselect",
    "files",
    "wish",
    "drop",
    "menu",
    "presents",
    "list",
    "choices",
    "available",
    "folders",
    "notebooks",
    "running",
    "files",
    "folder",
    "section",
    "select",
    "folders",
    "display",
    "present",
    "account",
    "folders",
    "small",
    "box",
    "see",
    "18",
    "number",
    "folders",
    "notebooks",
    "section",
    "change",
    "count",
    "number",
    "nodes",
    "provide",
    "three",
    "option",
    "see",
    "selected",
    "given",
    "notebooks",
    "number",
    "get",
    "option",
    "either",
    "duplicate",
    "current",
    "notebook",
    "need",
    "move",
    "view",
    "edit",
    "delete",
    "writing",
    "section",
    "select",
    "running",
    "scripts",
    "see",
    "zero",
    "running",
    "scripts",
    "update",
    "count",
    "number",
    "selected",
    "file",
    "section",
    "select",
    "files",
    "notebook",
    "display",
    "update",
    "counts",
    "accordingly",
    "select",
    "files",
    "seven",
    "files",
    "see",
    "seven",
    "files",
    "datasets",
    "csv",
    "files",
    "text",
    "files",
    "home",
    "button",
    "brings",
    "back",
    "home",
    "screen",
    "notebook",
    "click",
    "jupyter",
    "notebook",
    "lower",
    "bring",
    "back",
    "jupiter",
    "notebook",
    "dashboard",
    "see",
    "left",
    "hand",
    "side",
    "every",
    "item",
    "checkbox",
    "items",
    "name",
    "checkbox",
    "used",
    "build",
    "set",
    "files",
    "operate",
    "upon",
    "icon",
    "indicated",
    "type",
    "item",
    "case",
    "items",
    "folder",
    "coming",
    "ring",
    "notebooks",
    "finally",
    "certain",
    "files",
    "text",
    "files",
    "files",
    "typical",
    "workflow",
    "jupyter",
    "notebook",
    "first",
    "create",
    "notebook",
    "project",
    "data",
    "analysis",
    "add",
    "analysis",
    "step",
    "coding",
    "output",
    "surround",
    "analysis",
    "organization",
    "presentation",
    "mark",
    "communicate",
    "entire",
    "story",
    "interactive",
    "notebooks",
    "include",
    "widgets",
    "display",
    "modules",
    "used",
    "others",
    "modifying",
    "parameters",
    "data",
    "note",
    "effects",
    "changes",
    "talk",
    "security",
    "jupyter",
    "notebooks",
    "created",
    "order",
    "shared",
    "users",
    "many",
    "cases",
    "internet",
    "however",
    "jupyter",
    "notebook",
    "execute",
    "arbitrary",
    "code",
    "generate",
    "arbitrary",
    "code",
    "problem",
    "malicious",
    "aspects",
    "placed",
    "note",
    "default",
    "security",
    "mechanism",
    "japan",
    "notebooks",
    "include",
    "raw",
    "html",
    "always",
    "sanitized",
    "check",
    "malicious",
    "coding",
    "another",
    "aspect",
    "run",
    "external",
    "java",
    "scripts",
    "cell",
    "contents",
    "especially",
    "html",
    "javascript",
    "trusted",
    "requires",
    "user",
    "value",
    "nation",
    "continue",
    "output",
    "cell",
    "trusted",
    "html",
    "javascript",
    "never",
    "trusted",
    "clearing",
    "output",
    "cause",
    "notebook",
    "become",
    "trusted",
    "save",
    "notebooks",
    "also",
    "use",
    "security",
    "digest",
    "ensure",
    "correct",
    "user",
    "modifying",
    "contents",
    "need",
    "digest",
    "takes",
    "account",
    "entire",
    "contents",
    "notebook",
    "secret",
    "known",
    "notebook",
    "creator",
    "combination",
    "ensures",
    "malicious",
    "coding",
    "going",
    "added",
    "notebook",
    "add",
    "security",
    "address",
    "notebook",
    "using",
    "following",
    "command",
    "given",
    "jupiter",
    "profile",
    "selected",
    "inside",
    "need",
    "security",
    "notebook",
    "secret",
    "replace",
    "notebooks",
    "secret",
    "putter",
    "secret",
    "act",
    "key",
    "particular",
    "notebook",
    "need",
    "share",
    "particular",
    "key",
    "colleagues",
    "whoever",
    "want",
    "share",
    "particular",
    "notebook",
    "case",
    "keeps",
    "notebooks",
    "geode",
    "away",
    "malicious",
    "coders",
    "aspect",
    "jupiter",
    "configuration",
    "configure",
    "display",
    "parameters",
    "used",
    "presenting",
    "notebooks",
    "configurable",
    "due",
    "use",
    "product",
    "known",
    "code",
    "mirror",
    "present",
    "modify",
    "notebook",
    "cold",
    "mirror",
    "water",
    "basically",
    "javascript",
    "based",
    "editor",
    "within",
    "web",
    "pages",
    "notebooks",
    "code",
    "mirror",
    "see",
    "code",
    "mirror",
    "versatile",
    "text",
    "editor",
    "implemented",
    "javascript",
    "browser",
    "allow",
    "configure",
    "options",
    "jupiter",
    "let",
    "execute",
    "python",
    "code",
    "understand",
    "notebook",
    "better",
    "way",
    "jupiter",
    "interact",
    "scripts",
    "much",
    "executes",
    "script",
    "request",
    "result",
    "think",
    "jupyter",
    "notebooks",
    "extended",
    "languages",
    "besides",
    "python",
    "takes",
    "script",
    "runs",
    "particular",
    "language",
    "engine",
    "across",
    "output",
    "engine",
    "really",
    "knowing",
    "kind",
    "script",
    "executed",
    "new",
    "windows",
    "shows",
    "empty",
    "cell",
    "enter",
    "python",
    "code",
    "know",
    "need",
    "new",
    "select",
    "python",
    "3",
    "open",
    "new",
    "notebook",
    "notebook",
    "untitled",
    "let",
    "give",
    "new",
    "work",
    "area",
    "name",
    "python",
    "code",
    "see",
    "renamed",
    "particular",
    "cell",
    "order",
    "save",
    "option",
    "next",
    "title",
    "see",
    "last",
    "checkpoint",
    "days",
    "ago",
    "unsaved",
    "changes",
    "autosave",
    "option",
    "always",
    "accurate",
    "name",
    "find",
    "selection",
    "particular",
    "notebook",
    "easily",
    "notebook",
    "home",
    "page",
    "select",
    "browser",
    "home",
    "tab",
    "refresh",
    "find",
    "new",
    "window",
    "name",
    "displayed",
    "go",
    "notebook",
    "home",
    "see",
    "mentioned",
    "quotes",
    "running",
    "also",
    "pilot",
    "quotes",
    "let",
    "get",
    "back",
    "particular",
    "page",
    "notebook",
    "one",
    "thing",
    "note",
    "item",
    "icon",
    "versus",
    "folder",
    "icon",
    "though",
    "automatically",
    "assigned",
    "extension",
    "see",
    "ipy",
    "ipython",
    "note",
    "says",
    "item",
    "browser",
    "jupiter",
    "environment",
    "marked",
    "running",
    "answer",
    "file",
    "name",
    "directory",
    "well",
    "go",
    "directory",
    "let",
    "go",
    "check",
    "see",
    "go",
    "users",
    "see",
    "class",
    "projects",
    "python",
    "codes",
    "like",
    "series",
    "automatically",
    "particular",
    "ipython",
    "notebook",
    "created",
    "working",
    "environment",
    "local",
    "disk",
    "space",
    "also",
    "open",
    "ip",
    "b",
    "file",
    "text",
    "editor",
    "see",
    "basic",
    "context",
    "jupiter",
    "code",
    "see",
    "opening",
    "cells",
    "empty",
    "nothing",
    "let",
    "type",
    "code",
    "example",
    "going",
    "put",
    "name",
    "equals",
    "edgy",
    "rekha",
    "next",
    "going",
    "provide",
    "subscribers",
    "equals",
    "seven",
    "hundred",
    "gay",
    "run",
    "particular",
    "cell",
    "need",
    "click",
    "run",
    "icon",
    "see",
    "one",
    "first",
    "set",
    "executed",
    "second",
    "cell",
    "enter",
    "python",
    "code",
    "references",
    "variables",
    "first",
    "cell",
    "see",
    "friend",
    "named",
    "strings",
    "subscribers",
    "let",
    "run",
    "particular",
    "see",
    "note",
    "output",
    "erica",
    "700k",
    "youtube",
    "subscriber",
    "since",
    "700",
    "k",
    "know",
    "jupiter",
    "technologies",
    "subscribe",
    "channel",
    "get",
    "updates",
    "latest",
    "trending",
    "technologies",
    "note",
    "jupiter",
    "color",
    "codes",
    "python",
    "decent",
    "editor",
    "vote",
    "empty",
    "braces",
    "left",
    "code",
    "block",
    "see",
    "execute",
    "cell",
    "results",
    "displayed",
    "line",
    "interesting",
    "jupiter",
    "keeps",
    "output",
    "last",
    "generated",
    "saved",
    "version",
    "file",
    "save",
    "checkpoints",
    "rerun",
    "cells",
    "using",
    "rerun",
    "run",
    "output",
    "would",
    "generated",
    "c8y",
    "autosave",
    "cell",
    "number",
    "incremented",
    "see",
    "rerun",
    "see",
    "cell",
    "number",
    "change",
    "one",
    "three",
    "rerun",
    "selma",
    "change",
    "2",
    "jupiter",
    "keeps",
    "track",
    "latest",
    "version",
    "cell",
    "similarly",
    "close",
    "browser",
    "tab",
    "display",
    "home",
    "tab",
    "find",
    "new",
    "item",
    "created",
    "python",
    "code",
    "notebook",
    "saved",
    "autosaved",
    "see",
    "bracket",
    "autosaved",
    "close",
    "home",
    "button",
    "see",
    "python",
    "codes",
    "see",
    "click",
    "opens",
    "notebook",
    "previously",
    "displayed",
    "items",
    "always",
    "showing",
    "output",
    "sweat",
    "generated",
    "last",
    "run",
    "seen",
    "python",
    "works",
    "jupiter",
    "including",
    "underlying",
    "encoding",
    "python",
    "allows",
    "data",
    "set",
    "data",
    "set",
    "works",
    "jupiter",
    "let",
    "create",
    "another",
    "new",
    "python",
    "notebook",
    "going",
    "name",
    "pandas",
    "read",
    "last",
    "dataset",
    "compute",
    "standard",
    "statistics",
    "data",
    "interested",
    "seeing",
    "use",
    "pandas",
    "jupiter",
    "well",
    "script",
    "performs",
    "information",
    "stored",
    "metadata",
    "especially",
    "large",
    "dataset",
    "python",
    "script",
    "accesses",
    "iris",
    "dataset",
    "built",
    "one",
    "python",
    "packages",
    "looking",
    "read",
    "slightly",
    "large",
    "number",
    "items",
    "calculate",
    "basic",
    "operations",
    "data",
    "set",
    "first",
    "need",
    "sklearn",
    "import",
    "data",
    "set",
    "sklearn",
    "another",
    "library",
    "python",
    "contains",
    "lot",
    "data",
    "sets",
    "machine",
    "learning",
    "algorithms",
    "present",
    "machine",
    "learning",
    "data",
    "sets",
    "import",
    "successful",
    "going",
    "pull",
    "irs",
    "data",
    "going",
    "iris",
    "underscore",
    "data",
    "set",
    "equals",
    "load",
    "screen",
    "sorry",
    "data",
    "set",
    "start",
    "lower",
    "see",
    "number",
    "considered",
    "three",
    "second",
    "drawer",
    "encountered",
    "error",
    "data",
    "set",
    "data",
    "set",
    "going",
    "grab",
    "first",
    "two",
    "corner",
    "data",
    "let",
    "pretend",
    "x",
    "equals",
    "press",
    "tab",
    "automatically",
    "detects",
    "going",
    "write",
    "todd",
    "datasets",
    "dot",
    "data",
    "going",
    "take",
    "first",
    "two",
    "rows",
    "comma",
    "run",
    "keyboard",
    "need",
    "press",
    "shift",
    "enter",
    "next",
    "going",
    "calculate",
    "basic",
    "statistics",
    "going",
    "x",
    "underscore",
    "count",
    "equals",
    "x",
    "going",
    "use",
    "length",
    "function",
    "said",
    "going",
    "use",
    "x",
    "dot",
    "flat",
    "similarly",
    "going",
    "see",
    "x",
    "max",
    "min",
    "display",
    "results",
    "going",
    "play",
    "results",
    "see",
    "counter",
    "300",
    "minimum",
    "value",
    "mean",
    "five",
    "point",
    "eight",
    "four",
    "three",
    "three",
    "three",
    "let",
    "connect",
    "real",
    "life",
    "tell",
    "things",
    "easily",
    "using",
    "concepts",
    "machine",
    "learning",
    "easily",
    "get",
    "answer",
    "questions",
    "like",
    "types",
    "house",
    "lies",
    "segment",
    "market",
    "value",
    "house",
    "male",
    "spam",
    "spam",
    "fraud",
    "well",
    "question",
    "could",
    "ask",
    "machine",
    "getting",
    "answer",
    "need",
    "algorithm",
    "machine",
    "need",
    "train",
    "basis",
    "algorithm",
    "okay",
    "decide",
    "algorithm",
    "choose",
    "okay",
    "best",
    "option",
    "us",
    "explore",
    "one",
    "one",
    "first",
    "classification",
    "algorithm",
    "categories",
    "predicted",
    "using",
    "data",
    "question",
    "like",
    "person",
    "male",
    "female",
    "male",
    "spam",
    "spam",
    "category",
    "question",
    "would",
    "fall",
    "classification",
    "algorithm",
    "classification",
    "supervised",
    "learning",
    "approach",
    "computer",
    "program",
    "learns",
    "input",
    "given",
    "uses",
    "learning",
    "classify",
    "new",
    "observation",
    "examples",
    "classification",
    "problems",
    "speech",
    "organization",
    "handwriting",
    "recognized",
    "biometric",
    "identification",
    "document",
    "classification",
    "etc",
    "next",
    "anomaly",
    "detection",
    "algorithm",
    "identify",
    "unusual",
    "data",
    "point",
    "anomaly",
    "detection",
    "well",
    "technique",
    "used",
    "identify",
    "unusual",
    "pattern",
    "conform",
    "expected",
    "behavior",
    "say",
    "outliers",
    "many",
    "application",
    "business",
    "like",
    "intrusion",
    "detection",
    "like",
    "identifying",
    "strange",
    "patterns",
    "network",
    "traffic",
    "could",
    "signal",
    "hack",
    "system",
    "health",
    "monitoring",
    "sporting",
    "deadly",
    "tumor",
    "mri",
    "scan",
    "even",
    "use",
    "fraud",
    "detection",
    "credit",
    "card",
    "transaction",
    "deal",
    "fault",
    "detection",
    "operating",
    "environment",
    "next",
    "comes",
    "clustering",
    "algorithm",
    "use",
    "clustering",
    "algorithm",
    "group",
    "data",
    "based",
    "similar",
    "condition",
    "get",
    "answer",
    "type",
    "houses",
    "lies",
    "segment",
    "type",
    "customer",
    "buys",
    "product",
    "clustering",
    "task",
    "dividing",
    "population",
    "data",
    "points",
    "number",
    "groups",
    "data",
    "point",
    "groups",
    "hello",
    "data",
    "points",
    "group",
    "groups",
    "simple",
    "words",
    "aim",
    "segregate",
    "groups",
    "similar",
    "trait",
    "assigning",
    "cluster",
    "clustering",
    "task",
    "dividing",
    "population",
    "data",
    "points",
    "number",
    "groups",
    "data",
    "points",
    "x",
    "group",
    "similar",
    "data",
    "points",
    "group",
    "rather",
    "group",
    "words",
    "aim",
    "segregate",
    "groups",
    "similar",
    "traits",
    "assigning",
    "different",
    "clusters",
    "let",
    "understand",
    "example",
    "suppose",
    "head",
    "rental",
    "store",
    "wish",
    "understand",
    "preference",
    "customer",
    "scale",
    "business",
    "possible",
    "look",
    "detail",
    "customer",
    "design",
    "unique",
    "business",
    "strategy",
    "definitely",
    "right",
    "cluster",
    "customer",
    "saying",
    "10",
    "different",
    "groups",
    "based",
    "purchasing",
    "habit",
    "use",
    "separate",
    "strategy",
    "customers",
    "ten",
    "different",
    "groups",
    "call",
    "clustering",
    "next",
    "regression",
    "algorithm",
    "data",
    "predicted",
    "question",
    "may",
    "ask",
    "type",
    "model",
    "like",
    "market",
    "value",
    "house",
    "going",
    "rain",
    "tomorrow",
    "regression",
    "one",
    "important",
    "broadly",
    "used",
    "machine",
    "learning",
    "statistics",
    "tool",
    "allows",
    "make",
    "prediction",
    "data",
    "learning",
    "relationship",
    "features",
    "data",
    "observe",
    "continuous",
    "valued",
    "response",
    "regulation",
    "used",
    "massive",
    "number",
    "application",
    "know",
    "stock",
    "isis",
    "prediction",
    "done",
    "using",
    "regression",
    "know",
    "different",
    "machine",
    "learning",
    "algorithm",
    "decide",
    "algorithm",
    "choose",
    "let",
    "cover",
    "part",
    "using",
    "demo",
    "demo",
    "part",
    "create",
    "six",
    "different",
    "machine",
    "learning",
    "model",
    "pick",
    "best",
    "model",
    "build",
    "confidence",
    "reliable",
    "accuracy",
    "far",
    "demo",
    "part",
    "using",
    "irs",
    "data",
    "set",
    "data",
    "set",
    "quite",
    "famous",
    "considered",
    "one",
    "best",
    "small",
    "project",
    "start",
    "consider",
    "hello",
    "world",
    "data",
    "set",
    "machine",
    "learning",
    "data",
    "set",
    "consists",
    "150",
    "observation",
    "iris",
    "flower",
    "therefore",
    "columns",
    "measurement",
    "flowers",
    "centimeters",
    "fifth",
    "column",
    "species",
    "flower",
    "observe",
    "observed",
    "flowers",
    "belong",
    "one",
    "three",
    "species",
    "iris",
    "setosa",
    "iris",
    "virginica",
    "iris",
    "versicolor",
    "well",
    "good",
    "good",
    "project",
    "well",
    "understand",
    "attributes",
    "numeric",
    "figure",
    "load",
    "handle",
    "data",
    "classification",
    "problem",
    "thereby",
    "allowing",
    "practice",
    "perhaps",
    "easier",
    "type",
    "supervised",
    "learning",
    "algorithm",
    "four",
    "attributes",
    "150",
    "rose",
    "meaning",
    "small",
    "easily",
    "fit",
    "memory",
    "even",
    "numeric",
    "attributes",
    "unit",
    "scale",
    "means",
    "require",
    "special",
    "scaling",
    "transformation",
    "get",
    "started",
    "let",
    "start",
    "coding",
    "told",
    "earlier",
    "using",
    "anaconda",
    "python",
    "install",
    "install",
    "anaconda",
    "navigator",
    "would",
    "look",
    "like",
    "home",
    "page",
    "anaconda",
    "navigator",
    "using",
    "jupyter",
    "notebook",
    "interactive",
    "computing",
    "notebook",
    "environment",
    "help",
    "write",
    "execute",
    "python",
    "codes",
    "let",
    "hit",
    "launch",
    "button",
    "execute",
    "jupyter",
    "notebook",
    "see",
    "jupyter",
    "notebook",
    "starting",
    "localhost",
    "double",
    "eight",
    "nine",
    "zero",
    "okay",
    "jupyter",
    "notebook",
    "select",
    "new",
    "book",
    "python",
    "3",
    "environment",
    "write",
    "execute",
    "python",
    "codes",
    "let",
    "start",
    "checking",
    "version",
    "libraries",
    "order",
    "make",
    "video",
    "short",
    "interactive",
    "informative",
    "already",
    "written",
    "set",
    "code",
    "let",
    "copy",
    "paste",
    "explain",
    "one",
    "one",
    "let",
    "start",
    "checking",
    "version",
    "python",
    "libraries",
    "okay",
    "code",
    "let",
    "copy",
    "copied",
    "let",
    "paste",
    "okay",
    "first",
    "let",
    "summarize",
    "things",
    "checking",
    "version",
    "different",
    "libraries",
    "starting",
    "python",
    "first",
    "check",
    "version",
    "python",
    "working",
    "check",
    "version",
    "using",
    "numpy",
    "matplotlib",
    "panda",
    "okay",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "various",
    "versions",
    "libraries",
    "using",
    "run",
    "working",
    "python",
    "3",
    "point",
    "6",
    "point",
    "4",
    "psi",
    "matplotlib",
    "pandas",
    "version",
    "okay",
    "version",
    "using",
    "ideally",
    "version",
    "recent",
    "match",
    "worry",
    "lack",
    "versions",
    "behind",
    "api",
    "change",
    "quickly",
    "everything",
    "tutorial",
    "likely",
    "still",
    "work",
    "okay",
    "case",
    "getting",
    "error",
    "stop",
    "try",
    "fix",
    "error",
    "case",
    "unable",
    "find",
    "solution",
    "error",
    "feel",
    "free",
    "reach",
    "eureka",
    "even",
    "class",
    "let",
    "tell",
    "able",
    "run",
    "script",
    "properly",
    "able",
    "complete",
    "tutorial",
    "okay",
    "whenever",
    "get",
    "doubt",
    "reach",
    "resolve",
    "everything",
    "working",
    "smoothly",
    "time",
    "load",
    "data",
    "set",
    "said",
    "using",
    "iris",
    "flower",
    "data",
    "set",
    "tutorial",
    "loading",
    "data",
    "set",
    "let",
    "import",
    "modules",
    "function",
    "object",
    "going",
    "use",
    "tutorial",
    "already",
    "written",
    "set",
    "code",
    "let",
    "copy",
    "paste",
    "let",
    "load",
    "libraries",
    "various",
    "libraries",
    "using",
    "tutorial",
    "everything",
    "work",
    "fine",
    "without",
    "error",
    "get",
    "error",
    "stop",
    "need",
    "work",
    "cyber",
    "environment",
    "continue",
    "guess",
    "everything",
    "work",
    "fine",
    "let",
    "hit",
    "run",
    "button",
    "see",
    "okay",
    "worked",
    "let",
    "move",
    "ahead",
    "load",
    "data",
    "load",
    "data",
    "direct",
    "uci",
    "machine",
    "learning",
    "repository",
    "first",
    "let",
    "tell",
    "using",
    "panda",
    "load",
    "data",
    "okay",
    "let",
    "say",
    "url",
    "url",
    "use",
    "machine",
    "learning",
    "repository",
    "downloading",
    "data",
    "set",
    "okay",
    "specify",
    "name",
    "column",
    "loading",
    "data",
    "help",
    "later",
    "explore",
    "data",
    "okay",
    "copy",
    "paste",
    "okay",
    "defining",
    "variable",
    "names",
    "consists",
    "various",
    "parameters",
    "including",
    "sepal",
    "length",
    "sepal",
    "width",
    "petal",
    "length",
    "battle",
    "class",
    "name",
    "column",
    "data",
    "set",
    "okay",
    "let",
    "define",
    "data",
    "set",
    "data",
    "set",
    "equals",
    "panda",
    "dot",
    "read",
    "underscore",
    "csv",
    "inside",
    "defining",
    "url",
    "names",
    "equal",
    "name",
    "already",
    "said",
    "using",
    "panda",
    "load",
    "data",
    "alright",
    "using",
    "panda",
    "dot",
    "read",
    "csv",
    "reading",
    "csv",
    "file",
    "inside",
    "csv",
    "coming",
    "url",
    "url",
    "okay",
    "name",
    "sequel",
    "names",
    "specifying",
    "names",
    "various",
    "columns",
    "particular",
    "csv",
    "file",
    "okay",
    "let",
    "move",
    "forward",
    "execute",
    "even",
    "data",
    "set",
    "loaded",
    "case",
    "network",
    "issues",
    "go",
    "ahead",
    "download",
    "iris",
    "data",
    "file",
    "working",
    "directory",
    "loaded",
    "using",
    "method",
    "make",
    "sure",
    "change",
    "url",
    "local",
    "name",
    "else",
    "might",
    "get",
    "error",
    "okay",
    "yeah",
    "data",
    "set",
    "loaded",
    "let",
    "move",
    "ahead",
    "check",
    "data",
    "set",
    "let",
    "see",
    "many",
    "columns",
    "rows",
    "data",
    "set",
    "okay",
    "let",
    "print",
    "number",
    "rows",
    "columns",
    "data",
    "set",
    "data",
    "set",
    "data",
    "set",
    "dot",
    "shape",
    "give",
    "numbers",
    "total",
    "number",
    "rows",
    "little",
    "column",
    "say",
    "total",
    "number",
    "instances",
    "attributes",
    "data",
    "set",
    "fine",
    "print",
    "data",
    "set",
    "dot",
    "shape",
    "audio",
    "getting",
    "150",
    "150",
    "total",
    "number",
    "rows",
    "data",
    "set",
    "five",
    "total",
    "number",
    "columns",
    "fine",
    "moving",
    "ahead",
    "want",
    "see",
    "sample",
    "data",
    "set",
    "okay",
    "let",
    "print",
    "first",
    "certain",
    "instances",
    "data",
    "set",
    "okay",
    "print",
    "data",
    "set",
    "head",
    "want",
    "first",
    "30",
    "instances",
    "fine",
    "give",
    "first",
    "30",
    "result",
    "data",
    "set",
    "okay",
    "hit",
    "run",
    "button",
    "getting",
    "first",
    "30",
    "result",
    "okay",
    "0",
    "sample",
    "data",
    "set",
    "looks",
    "like",
    "sepal",
    "length",
    "sepal",
    "width",
    "petal",
    "petal",
    "width",
    "class",
    "okay",
    "data",
    "set",
    "looks",
    "like",
    "let",
    "move",
    "look",
    "summary",
    "attribute",
    "want",
    "find",
    "count",
    "mean",
    "minimum",
    "maximum",
    "values",
    "percentiles",
    "well",
    "print",
    "data",
    "set",
    "dot",
    "described",
    "give",
    "let",
    "see",
    "see",
    "numbers",
    "scales",
    "similar",
    "range",
    "0",
    "8",
    "centimeters",
    "right",
    "mean",
    "value",
    "standard",
    "deviation",
    "minimum",
    "value",
    "25",
    "percentile",
    "50",
    "percentile",
    "75",
    "percentile",
    "maximum",
    "value",
    "values",
    "lies",
    "range",
    "0",
    "8",
    "centimeter",
    "okay",
    "took",
    "summary",
    "attribute",
    "let",
    "look",
    "number",
    "instances",
    "belong",
    "class",
    "print",
    "data",
    "set",
    "first",
    "let",
    "print",
    "data",
    "set",
    "want",
    "group",
    "group",
    "using",
    "class",
    "want",
    "size",
    "size",
    "class",
    "fine",
    "let",
    "hit",
    "run",
    "okay",
    "want",
    "want",
    "print",
    "print",
    "data",
    "set",
    "however",
    "want",
    "get",
    "want",
    "class",
    "group",
    "class",
    "okay",
    "want",
    "size",
    "class",
    "find",
    "size",
    "class",
    "group",
    "class",
    "dot",
    "size",
    "skewed",
    "run",
    "see",
    "50",
    "instances",
    "iris",
    "setosa",
    "50",
    "instances",
    "iris",
    "versicolor",
    "50",
    "instances",
    "iris",
    "virginica",
    "okay",
    "data",
    "type",
    "integer",
    "base64",
    "fine",
    "basic",
    "idea",
    "data",
    "let",
    "move",
    "ahead",
    "create",
    "visualization",
    "going",
    "create",
    "two",
    "different",
    "types",
    "plot",
    "first",
    "would",
    "univariate",
    "plot",
    "next",
    "would",
    "multivariate",
    "plot",
    "creating",
    "univariate",
    "plots",
    "better",
    "understand",
    "attribute",
    "next",
    "creating",
    "multivariate",
    "plot",
    "better",
    "understand",
    "relationship",
    "different",
    "attributes",
    "okay",
    "start",
    "univariate",
    "plot",
    "plot",
    "individual",
    "variable",
    "given",
    "input",
    "variables",
    "numeric",
    "create",
    "box",
    "whiskers",
    "plot",
    "okay",
    "let",
    "move",
    "ahead",
    "create",
    "box",
    "whiskers",
    "plot",
    "data",
    "set",
    "dot",
    "plot",
    "kind",
    "want",
    "box",
    "okay",
    "need",
    "subplot",
    "yeah",
    "need",
    "subplots",
    "subplots",
    "equal",
    "type",
    "layout",
    "wo",
    "layout",
    "structure",
    "2",
    "cross",
    "2",
    "next",
    "want",
    "share",
    "coordinates",
    "x",
    "coordinates",
    "want",
    "share",
    "share",
    "x",
    "equal",
    "false",
    "even",
    "share",
    "2",
    "equals",
    "false",
    "okay",
    "data",
    "set",
    "dot",
    "plot",
    "kind",
    "equal",
    "box",
    "subplots",
    "lay",
    "us",
    "want",
    "want",
    "see",
    "plot",
    "show",
    "whatever",
    "created",
    "short",
    "okay",
    "execute",
    "gives",
    "us",
    "much",
    "clearer",
    "idea",
    "distribution",
    "input",
    "attribute",
    "given",
    "layout",
    "2",
    "cross",
    "2",
    "instead",
    "would",
    "given",
    "cross",
    "result",
    "see",
    "fine",
    "everything",
    "would",
    "printed",
    "one",
    "single",
    "row",
    "hold",
    "guys",
    "area",
    "doubt",
    "asking",
    "using",
    "sheriff",
    "share",
    "values",
    "assigned",
    "false",
    "values",
    "okay",
    "ariel",
    "order",
    "resolve",
    "query",
    "need",
    "show",
    "happen",
    "give",
    "true",
    "values",
    "okay",
    "share",
    "go",
    "pull",
    "share",
    "equals",
    "true",
    "let",
    "see",
    "result",
    "get",
    "getting",
    "x",
    "shared",
    "among",
    "visualization",
    "right",
    "see",
    "sepal",
    "length",
    "sepal",
    "width",
    "values",
    "ranging",
    "zero",
    "point",
    "zero",
    "two",
    "seven",
    "point",
    "five",
    "shared",
    "among",
    "visualization",
    "petal",
    "length",
    "shared",
    "value",
    "zero",
    "point",
    "zero",
    "two",
    "seven",
    "point",
    "five",
    "okay",
    "want",
    "share",
    "value",
    "x",
    "giving",
    "us",
    "cluttered",
    "visualization",
    "aria",
    "cause",
    "want",
    "x",
    "coordinates",
    "shared",
    "among",
    "visualization",
    "okay",
    "share",
    "x",
    "share",
    "value",
    "false",
    "okay",
    "let",
    "execute",
    "pretty",
    "much",
    "clear",
    "visualization",
    "gives",
    "clear",
    "idea",
    "distribution",
    "input",
    "attribute",
    "want",
    "also",
    "create",
    "histogram",
    "input",
    "variable",
    "get",
    "clear",
    "idea",
    "distribution",
    "let",
    "create",
    "histogram",
    "data",
    "set",
    "dot",
    "okay",
    "would",
    "need",
    "see",
    "plot",
    "dot",
    "show",
    "let",
    "see",
    "histogram",
    "seems",
    "two",
    "input",
    "variables",
    "go",
    "distribution",
    "useful",
    "note",
    "use",
    "algorithms",
    "exploit",
    "assumption",
    "okay",
    "next",
    "comes",
    "multivariate",
    "lat",
    "created",
    "univariate",
    "plot",
    "understand",
    "attribute",
    "let",
    "move",
    "look",
    "multivariate",
    "plot",
    "see",
    "interaction",
    "different",
    "variables",
    "first",
    "let",
    "look",
    "scatter",
    "plot",
    "attribute",
    "helpful",
    "spot",
    "structured",
    "relationship",
    "input",
    "variables",
    "okay",
    "let",
    "create",
    "scatter",
    "matrix",
    "creating",
    "scatter",
    "plot",
    "need",
    "scatter",
    "matrix",
    "need",
    "pass",
    "data",
    "set",
    "okay",
    "want",
    "want",
    "see",
    "plot",
    "dot",
    "show",
    "scatter",
    "matrix",
    "looks",
    "like",
    "like",
    "diagonal",
    "grouping",
    "pear",
    "right",
    "suggests",
    "high",
    "correlation",
    "predictable",
    "relationship",
    "right",
    "multivariate",
    "plot",
    "let",
    "move",
    "evaluate",
    "algorithm",
    "time",
    "create",
    "model",
    "data",
    "estimate",
    "accuracy",
    "basis",
    "unseen",
    "data",
    "okay",
    "know",
    "data",
    "set",
    "right",
    "know",
    "many",
    "instances",
    "attributes",
    "data",
    "set",
    "know",
    "summary",
    "attribute",
    "guess",
    "seen",
    "much",
    "data",
    "set",
    "let",
    "move",
    "create",
    "algorithm",
    "estimate",
    "accuracy",
    "based",
    "unseen",
    "data",
    "okay",
    "create",
    "model",
    "data",
    "estimate",
    "accuracy",
    "based",
    "unseen",
    "data",
    "okay",
    "first",
    "let",
    "create",
    "validation",
    "data",
    "set",
    "validation",
    "data",
    "set",
    "validation",
    "data",
    "set",
    "training",
    "data",
    "set",
    "using",
    "trainer",
    "model",
    "fine",
    "right",
    "create",
    "validation",
    "data",
    "set",
    "creating",
    "validation",
    "data",
    "set",
    "going",
    "going",
    "split",
    "data",
    "set",
    "two",
    "point",
    "okay",
    "first",
    "thing",
    "create",
    "validation",
    "data",
    "set",
    "even",
    "need",
    "validation",
    "data",
    "set",
    "need",
    "validation",
    "data",
    "set",
    "know",
    "model",
    "created",
    "good",
    "later",
    "use",
    "statistical",
    "method",
    "estimate",
    "accuracy",
    "model",
    "create",
    "unseen",
    "data",
    "also",
    "want",
    "concrete",
    "estimate",
    "accuracy",
    "best",
    "model",
    "unseen",
    "data",
    "evaluating",
    "actual",
    "unseen",
    "data",
    "okay",
    "confused",
    "let",
    "simplify",
    "split",
    "loaded",
    "data",
    "two",
    "parts",
    "first",
    "80",
    "percent",
    "data",
    "user",
    "train",
    "model",
    "rest",
    "20",
    "hold",
    "back",
    "validation",
    "data",
    "set",
    "use",
    "verify",
    "trained",
    "model",
    "okay",
    "fine",
    "let",
    "define",
    "array",
    "ra",
    "water",
    "consist",
    "consist",
    "values",
    "data",
    "set",
    "data",
    "set",
    "dot",
    "values",
    "okay",
    "next",
    "define",
    "variable",
    "x",
    "consist",
    "column",
    "array",
    "0",
    "4",
    "starting",
    "0",
    "4",
    "next",
    "variable",
    "would",
    "consist",
    "array",
    "starting",
    "first",
    "define",
    "variable",
    "x",
    "consist",
    "values",
    "array",
    "starting",
    "beginning",
    "0",
    "del",
    "okay",
    "column",
    "include",
    "x",
    "variable",
    "variable",
    "define",
    "class",
    "output",
    "need",
    "need",
    "fourth",
    "column",
    "class",
    "column",
    "start",
    "beginning",
    "want",
    "fourth",
    "column",
    "okay",
    "define",
    "validation",
    "size",
    "validation",
    "underscore",
    "sighs",
    "define",
    "use",
    "seed",
    "define",
    "cd",
    "equals",
    "method",
    "seed",
    "sets",
    "integers",
    "starting",
    "value",
    "used",
    "generating",
    "random",
    "number",
    "okay",
    "define",
    "value",
    "c",
    "r",
    "equals",
    "tell",
    "importance",
    "later",
    "okay",
    "let",
    "define",
    "first",
    "variables",
    "x",
    "underscore",
    "train",
    "test",
    "underscore",
    "train",
    "underscore",
    "test",
    "okay",
    "want",
    "select",
    "model",
    "okay",
    "module",
    "underscore",
    "selection",
    "split",
    "training",
    "data",
    "set",
    "two",
    "halves",
    "okay",
    "dot",
    "train",
    "underscore",
    "test",
    "underscore",
    "split",
    "want",
    "split",
    "value",
    "x",
    "okay",
    "test",
    "size",
    "equals",
    "validation",
    "size",
    "correct",
    "random",
    "state",
    "equal",
    "seed",
    "city",
    "helping",
    "keep",
    "randomness",
    "training",
    "testing",
    "data",
    "set",
    "fine",
    "let",
    "execute",
    "see",
    "result",
    "executed",
    "next",
    "create",
    "test",
    "harness",
    "use",
    "estimate",
    "accuracy",
    "split",
    "data",
    "set",
    "10",
    "parts",
    "crane",
    "nine",
    "part",
    "test",
    "one",
    "part",
    "repeat",
    "combination",
    "train",
    "test",
    "pilots",
    "okay",
    "let",
    "define",
    "cd",
    "six",
    "already",
    "define",
    "scoring",
    "equals",
    "accuracy",
    "fine",
    "using",
    "metric",
    "accuracy",
    "evaluate",
    "model",
    "ratio",
    "number",
    "correctly",
    "predicted",
    "instances",
    "divided",
    "total",
    "number",
    "instances",
    "data",
    "set",
    "x",
    "hundred",
    "giving",
    "percentage",
    "example",
    "98",
    "accurate",
    "99",
    "accurate",
    "things",
    "like",
    "okay",
    "scoring",
    "variable",
    "run",
    "build",
    "evaluate",
    "model",
    "next",
    "step",
    "next",
    "part",
    "building",
    "model",
    "till",
    "know",
    "algorithm",
    "would",
    "good",
    "problem",
    "configuration",
    "use",
    "let",
    "begin",
    "six",
    "different",
    "algorithm",
    "using",
    "logistic",
    "regression",
    "linear",
    "discriminant",
    "analysis",
    "neighbor",
    "classification",
    "regression",
    "trees",
    "neighbor",
    "buys",
    "vector",
    "machine",
    "well",
    "algorithms",
    "chime",
    "using",
    "good",
    "mixture",
    "simple",
    "linear",
    "algorithms",
    "simple",
    "linear",
    "switch",
    "included",
    "logistic",
    "regression",
    "linear",
    "discriminant",
    "analysis",
    "nonlinear",
    "part",
    "included",
    "knn",
    "algorithm",
    "card",
    "algorithm",
    "neighbor",
    "buys",
    "support",
    "vector",
    "machines",
    "okay",
    "reset",
    "random",
    "number",
    "seed",
    "run",
    "ensure",
    "evaluation",
    "algorithm",
    "performed",
    "using",
    "exactly",
    "data",
    "spreads",
    "ensures",
    "result",
    "directly",
    "comparable",
    "okay",
    "let",
    "copy",
    "paste",
    "okay",
    "building",
    "five",
    "different",
    "types",
    "model",
    "building",
    "logistic",
    "regression",
    "linear",
    "discriminant",
    "analysis",
    "neighbor",
    "decision",
    "tree",
    "ghajini",
    "buys",
    "support",
    "vector",
    "machine",
    "okay",
    "next",
    "evaluate",
    "model",
    "turn",
    "okay",
    "six",
    "different",
    "model",
    "accuracy",
    "estimation",
    "one",
    "need",
    "compare",
    "model",
    "select",
    "accurate",
    "running",
    "script",
    "saw",
    "following",
    "result",
    "see",
    "results",
    "screen",
    "accuracy",
    "score",
    "using",
    "different",
    "set",
    "algorithms",
    "okay",
    "using",
    "logistic",
    "regression",
    "accuracy",
    "rate",
    "using",
    "linear",
    "discriminant",
    "algorithm",
    "accuracy",
    "okay",
    "output",
    "seems",
    "ld",
    "algorithm",
    "accurate",
    "model",
    "tested",
    "want",
    "get",
    "idea",
    "accuracy",
    "model",
    "validation",
    "set",
    "testing",
    "data",
    "set",
    "give",
    "us",
    "independent",
    "final",
    "check",
    "accuracy",
    "best",
    "model",
    "always",
    "valuable",
    "keep",
    "testing",
    "data",
    "set",
    "case",
    "made",
    "overfitting",
    "testing",
    "data",
    "set",
    "made",
    "data",
    "leak",
    "result",
    "overly",
    "optimistic",
    "result",
    "okay",
    "run",
    "ldo",
    "model",
    "directly",
    "validation",
    "set",
    "summarize",
    "result",
    "final",
    "score",
    "confusion",
    "matrix",
    "classification",
    "statistics",
    "probability",
    "essential",
    "disciples",
    "form",
    "basic",
    "foundation",
    "machine",
    "learning",
    "algorithms",
    "deep",
    "learning",
    "social",
    "intelligence",
    "data",
    "science",
    "fact",
    "mathematics",
    "probability",
    "behind",
    "everything",
    "around",
    "us",
    "shapes",
    "patterns",
    "colors",
    "count",
    "petals",
    "flower",
    "mathematics",
    "embedded",
    "every",
    "aspect",
    "lives",
    "going",
    "go",
    "ahead",
    "discuss",
    "agenda",
    "today",
    "going",
    "begin",
    "session",
    "understanding",
    "data",
    "move",
    "look",
    "different",
    "categories",
    "data",
    "like",
    "quantitative",
    "qualitative",
    "data",
    "discuss",
    "exactly",
    "statistics",
    "basic",
    "terminologies",
    "statistics",
    "couple",
    "sampling",
    "techniques",
    "done",
    "discuss",
    "different",
    "types",
    "statistics",
    "involve",
    "descriptive",
    "inferential",
    "statistics",
    "next",
    "session",
    "mainly",
    "focusing",
    "descriptive",
    "statistics",
    "understand",
    "different",
    "measures",
    "center",
    "measures",
    "spread",
    "information",
    "gain",
    "entropy",
    "also",
    "understand",
    "measures",
    "help",
    "user",
    "finally",
    "discuss",
    "exactly",
    "confusion",
    "matrix",
    "covered",
    "entire",
    "descriptive",
    "statistics",
    "module",
    "discuss",
    "probability",
    "module",
    "understand",
    "exactly",
    "probability",
    "different",
    "terminologies",
    "probability",
    "also",
    "study",
    "different",
    "probability",
    "distributions",
    "discuss",
    "types",
    "probability",
    "include",
    "marginal",
    "probability",
    "joint",
    "conditional",
    "probability",
    "move",
    "discuss",
    "use",
    "case",
    "wherein",
    "see",
    "examples",
    "show",
    "us",
    "different",
    "types",
    "probability",
    "work",
    "better",
    "understand",
    "bayes",
    "theorem",
    "look",
    "small",
    "example",
    "also",
    "forgot",
    "mention",
    "end",
    "descriptive",
    "statistics",
    "module",
    "running",
    "small",
    "demo",
    "language",
    "know",
    "much",
    "explaining",
    "every",
    "line",
    "depth",
    "want",
    "understanding",
    "leave",
    "couple",
    "blocks",
    "couple",
    "videos",
    "description",
    "box",
    "definitely",
    "check",
    "content",
    "completed",
    "probability",
    "module",
    "discuss",
    "inferential",
    "statistics",
    "module",
    "start",
    "module",
    "understanding",
    "point",
    "estimation",
    "discuss",
    "confidence",
    "interval",
    "estimate",
    "confidence",
    "interval",
    "also",
    "discuss",
    "margin",
    "error",
    "understand",
    "concepts",
    "looking",
    "small",
    "use",
    "case",
    "finally",
    "end",
    "inferential",
    "real",
    "statistic",
    "module",
    "looking",
    "hypothesis",
    "testing",
    "hypothesis",
    "testing",
    "important",
    "part",
    "inferential",
    "statistics",
    "end",
    "session",
    "looking",
    "use",
    "case",
    "discusses",
    "hypothesis",
    "testing",
    "works",
    "sum",
    "everything",
    "look",
    "demo",
    "explains",
    "inferential",
    "statistics",
    "works",
    "right",
    "guys",
    "lot",
    "cover",
    "today",
    "let",
    "move",
    "ahead",
    "take",
    "look",
    "first",
    "topic",
    "data",
    "quite",
    "simple",
    "question",
    "ask",
    "data",
    "see",
    "set",
    "numbers",
    "sort",
    "documents",
    "stored",
    "computer",
    "data",
    "actually",
    "everything",
    "right",
    "look",
    "around",
    "data",
    "everywhere",
    "click",
    "phone",
    "generates",
    "data",
    "know",
    "generated",
    "data",
    "provides",
    "insights",
    "analysis",
    "helps",
    "us",
    "make",
    "better",
    "business",
    "decisions",
    "data",
    "important",
    "give",
    "formal",
    "definition",
    "data",
    "refers",
    "facts",
    "statistics",
    "collected",
    "together",
    "reference",
    "analysis",
    "right",
    "definition",
    "data",
    "terms",
    "statistics",
    "probability",
    "know",
    "data",
    "collected",
    "measured",
    "analyzed",
    "visualized",
    "using",
    "statistical",
    "models",
    "graphs",
    "data",
    "divided",
    "two",
    "major",
    "subcategories",
    "alright",
    "first",
    "qualitative",
    "data",
    "quantitative",
    "data",
    "two",
    "different",
    "types",
    "data",
    "qualitative",
    "data",
    "nominal",
    "ordinal",
    "data",
    "quantitative",
    "data",
    "discrete",
    "continuous",
    "data",
    "let",
    "focus",
    "qualitative",
    "data",
    "type",
    "data",
    "deals",
    "characteristics",
    "descriptors",
    "ca",
    "easily",
    "measured",
    "observed",
    "subjectively",
    "qualitative",
    "data",
    "divided",
    "nominal",
    "ordinal",
    "data",
    "nominal",
    "data",
    "sort",
    "data",
    "order",
    "ranking",
    "okay",
    "example",
    "nominal",
    "data",
    "gender",
    "ranking",
    "gender",
    "male",
    "female",
    "right",
    "one",
    "two",
    "three",
    "four",
    "sort",
    "ordering",
    "gender",
    "race",
    "another",
    "example",
    "nominal",
    "data",
    "ordinal",
    "data",
    "basically",
    "ordered",
    "series",
    "information",
    "okay",
    "let",
    "say",
    "went",
    "restaurant",
    "okay",
    "information",
    "stored",
    "form",
    "customer",
    "id",
    "right",
    "basically",
    "represented",
    "customer",
    "id",
    "would",
    "rated",
    "service",
    "either",
    "good",
    "average",
    "right",
    "ordinal",
    "data",
    "similarly",
    "record",
    "customers",
    "visit",
    "restaurant",
    "along",
    "ratings",
    "right",
    "data",
    "sort",
    "sequence",
    "sort",
    "order",
    "known",
    "ordinal",
    "data",
    "right",
    "guys",
    "pretty",
    "simple",
    "understand",
    "let",
    "move",
    "look",
    "quantitative",
    "data",
    "quantitative",
    "data",
    "basically",
    "numbers",
    "things",
    "okay",
    "understand",
    "word",
    "quantitative",
    "quantitative",
    "basically",
    "quantity",
    "right",
    "saudis",
    "numbers",
    "deals",
    "anything",
    "measure",
    "objectively",
    "right",
    "two",
    "types",
    "quantitative",
    "data",
    "discrete",
    "continuous",
    "data",
    "discrete",
    "data",
    "also",
    "known",
    "categorical",
    "data",
    "hold",
    "finite",
    "number",
    "possible",
    "values",
    "number",
    "students",
    "class",
    "finite",
    "number",
    "right",
    "ca",
    "infinite",
    "number",
    "students",
    "class",
    "let",
    "say",
    "fifth",
    "grade",
    "hundred",
    "students",
    "class",
    "right",
    "infinite",
    "number",
    "definite",
    "finite",
    "number",
    "students",
    "class",
    "okay",
    "discrete",
    "data",
    "next",
    "continuous",
    "data",
    "type",
    "data",
    "hold",
    "infinite",
    "number",
    "possible",
    "values",
    "okay",
    "say",
    "weight",
    "person",
    "example",
    "continuous",
    "data",
    "mean",
    "see",
    "weight",
    "50",
    "kgs",
    "kgs",
    "one",
    "kgs",
    "one",
    "2",
    "3",
    "right",
    "infinite",
    "number",
    "possible",
    "values",
    "right",
    "mean",
    "continuous",
    "data",
    "right",
    "difference",
    "discrete",
    "continuous",
    "data",
    "also",
    "would",
    "like",
    "mention",
    "things",
    "couple",
    "types",
    "variables",
    "well",
    "discrete",
    "variable",
    "continuous",
    "variable",
    "discrete",
    "variable",
    "also",
    "known",
    "categorical",
    "variable",
    "hold",
    "values",
    "different",
    "categories",
    "let",
    "say",
    "variable",
    "called",
    "message",
    "two",
    "types",
    "values",
    "variable",
    "hold",
    "let",
    "say",
    "message",
    "either",
    "spam",
    "message",
    "non",
    "spam",
    "message",
    "okay",
    "call",
    "variable",
    "discrete",
    "categorical",
    "variable",
    "right",
    "hold",
    "values",
    "represent",
    "different",
    "categories",
    "data",
    "continuous",
    "variables",
    "basically",
    "variables",
    "store",
    "finite",
    "number",
    "values",
    "weight",
    "person",
    "denoted",
    "continuous",
    "variable",
    "right",
    "let",
    "say",
    "variable",
    "called",
    "weight",
    "store",
    "infinite",
    "number",
    "possible",
    "values",
    "call",
    "continuous",
    "variable",
    "guys",
    "basically",
    "variable",
    "anything",
    "store",
    "value",
    "right",
    "associate",
    "sort",
    "data",
    "table",
    "become",
    "either",
    "discrete",
    "variable",
    "continuous",
    "variable",
    "also",
    "dependent",
    "independent",
    "type",
    "variables",
    "wo",
    "discuss",
    "depth",
    "pretty",
    "understandable",
    "sure",
    "know",
    "independent",
    "variable",
    "dependent",
    "variable",
    "right",
    "dependent",
    "variable",
    "variable",
    "whose",
    "value",
    "depends",
    "independent",
    "variable",
    "guys",
    "much",
    "knowledge",
    "expect",
    "right",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "statistics",
    "coming",
    "formal",
    "definition",
    "statistics",
    "statistics",
    "area",
    "applied",
    "mathematics",
    "concerned",
    "data",
    "collection",
    "analysis",
    "interpretation",
    "presentation",
    "usually",
    "speak",
    "statistics",
    "people",
    "think",
    "statistics",
    "analysis",
    "statistics",
    "path",
    "toward",
    "data",
    "collection",
    "also",
    "part",
    "statistics",
    "data",
    "interpretation",
    "presentation",
    "comes",
    "statistics",
    "already",
    "going",
    "use",
    "statistical",
    "methods",
    "visualize",
    "data",
    "collect",
    "data",
    "interpret",
    "data",
    "alright",
    "area",
    "mathematics",
    "deals",
    "understanding",
    "data",
    "used",
    "solve",
    "complex",
    "problems",
    "okay",
    "give",
    "couple",
    "examples",
    "solved",
    "using",
    "statistics",
    "okay",
    "let",
    "say",
    "company",
    "created",
    "new",
    "drug",
    "may",
    "cure",
    "cancer",
    "would",
    "conduct",
    "test",
    "confirm",
    "effectiveness",
    "even",
    "though",
    "sounds",
    "like",
    "biology",
    "problem",
    "solved",
    "statistics",
    "right",
    "create",
    "test",
    "confirm",
    "effectiveness",
    "drum",
    "common",
    "problem",
    "solved",
    "using",
    "statistics",
    "let",
    "give",
    "another",
    "example",
    "friend",
    "baseball",
    "game",
    "blue",
    "offers",
    "bet",
    "neither",
    "team",
    "hit",
    "home",
    "run",
    "game",
    "take",
    "bet",
    "right",
    "discuss",
    "probability",
    "know",
    "win",
    "lose",
    "right",
    "another",
    "problem",
    "comes",
    "statistics",
    "let",
    "look",
    "another",
    "example",
    "latest",
    "sales",
    "data",
    "come",
    "boss",
    "wants",
    "prepare",
    "report",
    "management",
    "places",
    "company",
    "could",
    "improve",
    "business",
    "look",
    "look",
    "problem",
    "involves",
    "lot",
    "data",
    "analysis",
    "look",
    "different",
    "variables",
    "causing",
    "business",
    "go",
    "look",
    "variables",
    "increasing",
    "performance",
    "models",
    "growing",
    "business",
    "alright",
    "involves",
    "lot",
    "data",
    "analysis",
    "basic",
    "idea",
    "behind",
    "data",
    "analysis",
    "use",
    "statistical",
    "techniques",
    "order",
    "figure",
    "relationship",
    "different",
    "variables",
    "different",
    "components",
    "business",
    "okay",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "basic",
    "terminologies",
    "statistics",
    "dive",
    "deep",
    "statistics",
    "important",
    "understand",
    "basic",
    "terminologies",
    "used",
    "statistics",
    "two",
    "important",
    "terminologies",
    "statistics",
    "population",
    "sample",
    "throughout",
    "statistics",
    "course",
    "throughout",
    "problem",
    "trying",
    "stall",
    "statistics",
    "come",
    "across",
    "two",
    "words",
    "population",
    "sample",
    "population",
    "collection",
    "set",
    "individuals",
    "objects",
    "events",
    "events",
    "whose",
    "properties",
    "analyzed",
    "okay",
    "basically",
    "refer",
    "population",
    "subject",
    "trying",
    "analyze",
    "sample",
    "like",
    "word",
    "suggests",
    "subset",
    "population",
    "make",
    "sure",
    "choose",
    "sample",
    "way",
    "represents",
    "entire",
    "population",
    "right",
    "focus",
    "add",
    "one",
    "part",
    "population",
    "instead",
    "represent",
    "entire",
    "population",
    "sample",
    "chosen",
    "well",
    "chosen",
    "sample",
    "contain",
    "information",
    "particular",
    "population",
    "parameter",
    "must",
    "wondering",
    "one",
    "choose",
    "sample",
    "best",
    "represents",
    "entire",
    "population",
    "sampling",
    "statistical",
    "method",
    "deals",
    "selection",
    "individual",
    "observations",
    "within",
    "population",
    "sampling",
    "performed",
    "order",
    "infer",
    "statistical",
    "knowledge",
    "population",
    "right",
    "want",
    "understand",
    "different",
    "statistics",
    "population",
    "like",
    "mean",
    "median",
    "median",
    "mode",
    "standard",
    "deviation",
    "variance",
    "population",
    "going",
    "perform",
    "sampling",
    "right",
    "reasonable",
    "study",
    "large",
    "population",
    "find",
    "mean",
    "median",
    "everything",
    "else",
    "sampling",
    "performed",
    "might",
    "ask",
    "point",
    "sampling",
    "study",
    "entire",
    "population",
    "guys",
    "think",
    "scenario",
    "asked",
    "perform",
    "survey",
    "eating",
    "habits",
    "teenagers",
    "us",
    "present",
    "42",
    "million",
    "teens",
    "us",
    "number",
    "growing",
    "speaking",
    "right",
    "correct",
    "possible",
    "survey",
    "42",
    "million",
    "individuals",
    "health",
    "possible",
    "well",
    "might",
    "possible",
    "take",
    "forever",
    "obviously",
    "reasonable",
    "go",
    "around",
    "knocking",
    "door",
    "asking",
    "teenage",
    "son",
    "eat",
    "right",
    "reasonable",
    "sampling",
    "used",
    "method",
    "wherein",
    "sample",
    "population",
    "studied",
    "order",
    "draw",
    "inferences",
    "entire",
    "population",
    "basically",
    "shortcut",
    "starting",
    "entire",
    "population",
    "instead",
    "taking",
    "entire",
    "population",
    "finding",
    "solutions",
    "going",
    "take",
    "part",
    "population",
    "represents",
    "entire",
    "population",
    "going",
    "perform",
    "statistical",
    "analysis",
    "inferential",
    "statistics",
    "small",
    "sample",
    "right",
    "sample",
    "basically",
    "presents",
    "entire",
    "population",
    "right",
    "sure",
    "made",
    "clear",
    "sample",
    "population",
    "two",
    "main",
    "types",
    "sampling",
    "techniques",
    "discussed",
    "today",
    "probability",
    "sampling",
    "sampling",
    "video",
    "focusing",
    "probability",
    "sampling",
    "techniques",
    "sampling",
    "within",
    "scope",
    "video",
    "right",
    "discuss",
    "probability",
    "part",
    "focusing",
    "statistics",
    "probability",
    "correct",
    "probability",
    "sampling",
    "three",
    "different",
    "types",
    "random",
    "sampling",
    "systematic",
    "stratified",
    "sampling",
    "right",
    "mention",
    "different",
    "types",
    "sampling",
    "zwi",
    "ball",
    "kota",
    "judgment",
    "convenience",
    "sampling",
    "right",
    "guys",
    "session",
    "focusing",
    "probability",
    "let",
    "move",
    "look",
    "different",
    "types",
    "probability",
    "sampling",
    "probability",
    "sampling",
    "sampling",
    "technique",
    "samples",
    "large",
    "population",
    "chosen",
    "using",
    "theory",
    "probability",
    "right",
    "three",
    "types",
    "probability",
    "sampling",
    "right",
    "first",
    "random",
    "sampling",
    "method",
    "member",
    "population",
    "equal",
    "chance",
    "selected",
    "sample",
    "right",
    "every",
    "individual",
    "every",
    "object",
    "population",
    "equal",
    "chance",
    "part",
    "sample",
    "random",
    "sampling",
    "okay",
    "randomly",
    "going",
    "select",
    "individual",
    "object",
    "bay",
    "individual",
    "equal",
    "chance",
    "selected",
    "correct",
    "next",
    "systematic",
    "sampling",
    "systematic",
    "sampling",
    "every",
    "nth",
    "record",
    "chosen",
    "population",
    "part",
    "sample",
    "right",
    "refer",
    "image",
    "shown",
    "six",
    "groups",
    "every",
    "skinned",
    "group",
    "chosen",
    "sample",
    "okay",
    "every",
    "second",
    "record",
    "chosen",
    "systematic",
    "sampling",
    "works",
    "okay",
    "randomly",
    "selecting",
    "nth",
    "record",
    "going",
    "add",
    "sample",
    "next",
    "stratified",
    "sampling",
    "type",
    "technique",
    "stratum",
    "used",
    "form",
    "samples",
    "large",
    "population",
    "stratum",
    "stratum",
    "basically",
    "subset",
    "population",
    "shares",
    "least",
    "one",
    "comment",
    "characteristics",
    "let",
    "say",
    "population",
    "mix",
    "male",
    "female",
    "create",
    "straightens",
    "one",
    "male",
    "subset",
    "female",
    "subset",
    "stratum",
    "basically",
    "subset",
    "population",
    "shares",
    "least",
    "one",
    "common",
    "characteristics",
    "right",
    "example",
    "gender",
    "created",
    "stratum",
    "going",
    "use",
    "random",
    "sampling",
    "stratums",
    "going",
    "choose",
    "final",
    "samba",
    "random",
    "sampling",
    "meaning",
    "individuals",
    "stratum",
    "equal",
    "chance",
    "selected",
    "sample",
    "correct",
    "guys",
    "three",
    "different",
    "types",
    "sampling",
    "techniques",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "different",
    "types",
    "statistics",
    "looking",
    "advanced",
    "concepts",
    "statistics",
    "right",
    "far",
    "discuss",
    "basics",
    "statistics",
    "basically",
    "statistics",
    "different",
    "sampling",
    "techniques",
    "terminologies",
    "statistics",
    "right",
    "look",
    "different",
    "types",
    "statistics",
    "two",
    "major",
    "types",
    "statistics",
    "descriptive",
    "statistics",
    "inferential",
    "statistics",
    "today",
    "session",
    "discussing",
    "types",
    "statistics",
    "depth",
    "right",
    "also",
    "looking",
    "demo",
    "running",
    "language",
    "order",
    "make",
    "understand",
    "exactly",
    "descriptive",
    "inferential",
    "statistics",
    "guys",
    "going",
    "look",
    "600",
    "worry",
    "much",
    "knowledge",
    "explaining",
    "everything",
    "basic",
    "level",
    "right",
    "guys",
    "descriptive",
    "statistics",
    "method",
    "used",
    "describe",
    "understand",
    "features",
    "specific",
    "data",
    "set",
    "giving",
    "short",
    "summary",
    "data",
    "okay",
    "mainly",
    "focused",
    "upon",
    "characteristics",
    "data",
    "also",
    "provides",
    "graphical",
    "summary",
    "data",
    "order",
    "make",
    "understand",
    "descriptive",
    "statistics",
    "let",
    "suppose",
    "pose",
    "want",
    "gift",
    "classmates",
    "study",
    "average",
    "shirt",
    "size",
    "student",
    "classroom",
    "use",
    "descriptive",
    "statistics",
    "study",
    "average",
    "shirt",
    "size",
    "students",
    "classroom",
    "would",
    "would",
    "record",
    "shirt",
    "size",
    "students",
    "class",
    "would",
    "find",
    "maximum",
    "minimum",
    "average",
    "shirt",
    "size",
    "club",
    "okay",
    "coming",
    "inferential",
    "statistics",
    "inferential",
    "statistics",
    "makes",
    "predictions",
    "population",
    "based",
    "sample",
    "data",
    "taken",
    "population",
    "okay",
    "simple",
    "words",
    "generalizes",
    "large",
    "data",
    "set",
    "applies",
    "probability",
    "draw",
    "conclusion",
    "okay",
    "allows",
    "infer",
    "data",
    "parameters",
    "based",
    "statistical",
    "model",
    "using",
    "sample",
    "data",
    "consider",
    "example",
    "finding",
    "average",
    "shirt",
    "size",
    "students",
    "class",
    "infinite",
    "shal",
    "statistics",
    "take",
    "sample",
    "set",
    "class",
    "basically",
    "people",
    "entire",
    "class",
    "right",
    "already",
    "grouped",
    "class",
    "large",
    "medium",
    "small",
    "right",
    "method",
    "basically",
    "build",
    "statistical",
    "model",
    "expand",
    "entire",
    "population",
    "class",
    "guys",
    "brief",
    "understanding",
    "descriptive",
    "inferential",
    "statistics",
    "difference",
    "descriptive",
    "inferential",
    "next",
    "section",
    "go",
    "depth",
    "descriptive",
    "statistics",
    "right",
    "discuss",
    "descriptive",
    "statistics",
    "like",
    "mentioned",
    "earlier",
    "descriptive",
    "statistics",
    "method",
    "used",
    "describe",
    "understand",
    "features",
    "specific",
    "data",
    "set",
    "giving",
    "short",
    "summaries",
    "sample",
    "measures",
    "data",
    "two",
    "important",
    "measures",
    "descriptive",
    "statistics",
    "measure",
    "central",
    "tendency",
    "also",
    "known",
    "measure",
    "center",
    "measures",
    "variability",
    "also",
    "known",
    "measures",
    "spread",
    "ed",
    "measures",
    "center",
    "include",
    "mean",
    "median",
    "mode",
    "measures",
    "center",
    "measures",
    "center",
    "statistical",
    "measures",
    "represent",
    "summary",
    "data",
    "set",
    "okay",
    "three",
    "main",
    "measures",
    "center",
    "mean",
    "median",
    "mode",
    "coming",
    "measures",
    "variability",
    "measures",
    "spread",
    "range",
    "interquartile",
    "range",
    "variance",
    "standard",
    "deviation",
    "right",
    "let",
    "discuss",
    "measures",
    "little",
    "starting",
    "measures",
    "center",
    "sure",
    "know",
    "mean",
    "mean",
    "basically",
    "measure",
    "average",
    "values",
    "sample",
    "okay",
    "basically",
    "average",
    "values",
    "sample",
    "measure",
    "mean",
    "hope",
    "know",
    "main",
    "measured",
    "10",
    "numbers",
    "want",
    "find",
    "mean",
    "10",
    "numbers",
    "add",
    "10",
    "numbers",
    "divide",
    "10",
    "represents",
    "number",
    "samples",
    "data",
    "set",
    "right",
    "since",
    "10",
    "numbers",
    "going",
    "divide",
    "right",
    "give",
    "us",
    "average",
    "mean",
    "better",
    "understand",
    "measures",
    "central",
    "tendency",
    "let",
    "look",
    "example",
    "data",
    "set",
    "basically",
    "cars",
    "data",
    "set",
    "contains",
    "variables",
    "right",
    "something",
    "known",
    "cars",
    "mileage",
    "per",
    "gallon",
    "cylinder",
    "type",
    "displacement",
    "horsepower",
    "roll",
    "axle",
    "ratio",
    "right",
    "measures",
    "related",
    "cars",
    "okay",
    "going",
    "going",
    "use",
    "descriptive",
    "analysis",
    "going",
    "analyze",
    "variables",
    "sample",
    "data",
    "set",
    "mean",
    "standard",
    "deviation",
    "median",
    "mode",
    "let",
    "say",
    "want",
    "find",
    "mean",
    "average",
    "horsepower",
    "cars",
    "among",
    "population",
    "cards",
    "like",
    "mentioned",
    "earlier",
    "check",
    "average",
    "values",
    "case",
    "take",
    "sum",
    "horizontal",
    "horsepower",
    "car",
    "divide",
    "total",
    "number",
    "cards",
    "okay",
    "exactly",
    "done",
    "calculation",
    "part",
    "hundred",
    "ten",
    "basically",
    "represents",
    "horsepower",
    "first",
    "car",
    "alright",
    "similarly",
    "added",
    "values",
    "horsepower",
    "cars",
    "divided",
    "8",
    "8",
    "basically",
    "number",
    "cars",
    "data",
    "set",
    "right",
    "hundred",
    "three",
    "point",
    "six",
    "two",
    "five",
    "mean",
    "average",
    "horsepower",
    "right",
    "let",
    "understand",
    "median",
    "example",
    "okay",
    "define",
    "median",
    "median",
    "basically",
    "measure",
    "central",
    "value",
    "sample",
    "set",
    "called",
    "median",
    "right",
    "see",
    "middle",
    "value",
    "want",
    "find",
    "center",
    "value",
    "mileage",
    "per",
    "gallon",
    "among",
    "population",
    "cars",
    "first",
    "arrange",
    "mgp",
    "values",
    "ascending",
    "descending",
    "order",
    "choose",
    "middle",
    "value",
    "right",
    "case",
    "since",
    "eight",
    "values",
    "right",
    "eight",
    "values",
    "even",
    "entry",
    "whenever",
    "even",
    "number",
    "data",
    "points",
    "samples",
    "data",
    "set",
    "going",
    "take",
    "average",
    "two",
    "middle",
    "values",
    "nine",
    "values",
    "easily",
    "figure",
    "middle",
    "value",
    "know",
    "choose",
    "median",
    "since",
    "even",
    "number",
    "values",
    "going",
    "take",
    "average",
    "two",
    "middle",
    "values",
    "right",
    "eight",
    "twenty",
    "three",
    "two",
    "middle",
    "values",
    "taking",
    "mean",
    "2",
    "hence",
    "get",
    "twenty",
    "two",
    "point",
    "nine",
    "median",
    "right",
    "lastly",
    "let",
    "look",
    "mode",
    "calculated",
    "mode",
    "value",
    "recurrent",
    "sample",
    "set",
    "known",
    "mode",
    "basically",
    "value",
    "occurs",
    "often",
    "okay",
    "known",
    "mode",
    "let",
    "say",
    "want",
    "find",
    "common",
    "type",
    "cylinder",
    "among",
    "population",
    "cards",
    "check",
    "value",
    "repeated",
    "number",
    "times",
    "see",
    "cylinders",
    "come",
    "two",
    "types",
    "cylinder",
    "type",
    "4",
    "cylinder",
    "type",
    "6",
    "right",
    "take",
    "look",
    "data",
    "set",
    "see",
    "recurring",
    "value",
    "6",
    "right",
    "one",
    "two",
    "three",
    "four",
    "five",
    "five",
    "six",
    "one",
    "two",
    "three",
    "yeah",
    "three",
    "four",
    "types",
    "lenders",
    "cylinders",
    "basically",
    "three",
    "four",
    "type",
    "cylinders",
    "five",
    "six",
    "type",
    "cylinders",
    "right",
    "mode",
    "going",
    "6",
    "since",
    "6",
    "recurrent",
    "4",
    "guys",
    "measures",
    "center",
    "measures",
    "central",
    "tendency",
    "let",
    "move",
    "look",
    "measures",
    "spread",
    "right",
    "measure",
    "spread",
    "measure",
    "spread",
    "sometimes",
    "also",
    "called",
    "measure",
    "dispersion",
    "used",
    "describe",
    "variability",
    "sample",
    "population",
    "okay",
    "think",
    "sort",
    "deviation",
    "sample",
    "right",
    "measure",
    "help",
    "different",
    "measure",
    "spreads",
    "range",
    "interquartile",
    "range",
    "variance",
    "standard",
    "deviation",
    "range",
    "pretty",
    "right",
    "given",
    "measure",
    "spread",
    "apart",
    "values",
    "data",
    "set",
    "range",
    "calculated",
    "shown",
    "formula",
    "basically",
    "going",
    "maximum",
    "value",
    "data",
    "set",
    "minimum",
    "value",
    "data",
    "set",
    "calculate",
    "range",
    "data",
    "alright",
    "next",
    "interquartile",
    "range",
    "discuss",
    "interquartile",
    "range",
    "let",
    "understand",
    "quartile",
    "red",
    "quartiles",
    "basically",
    "tell",
    "us",
    "spread",
    "data",
    "set",
    "breaking",
    "data",
    "set",
    "different",
    "quarters",
    "okay",
    "like",
    "median",
    "breaks",
    "data",
    "two",
    "parts",
    "quartile",
    "break",
    "two",
    "different",
    "quarters",
    "better",
    "understand",
    "quartile",
    "interquartile",
    "calculated",
    "let",
    "look",
    "small",
    "example",
    "data",
    "set",
    "basically",
    "represents",
    "marks",
    "hundred",
    "students",
    "ordered",
    "lowest",
    "highest",
    "scores",
    "red",
    "quartiles",
    "lie",
    "following",
    "ranges",
    "first",
    "quartile",
    "also",
    "known",
    "q1",
    "lies",
    "25th",
    "26th",
    "observation",
    "right",
    "look",
    "highlighted",
    "25th",
    "six",
    "observation",
    "calculate",
    "q",
    "1",
    "first",
    "quartile",
    "taking",
    "average",
    "two",
    "values",
    "alright",
    "since",
    "values",
    "45",
    "add",
    "divide",
    "two",
    "still",
    "get",
    "45",
    "second",
    "quartile",
    "q",
    "2",
    "50th",
    "fifty",
    "first",
    "observation",
    "going",
    "take",
    "average",
    "58",
    "59",
    "get",
    "value",
    "second",
    "quarter",
    "third",
    "quartile",
    "q3",
    "75th",
    "76th",
    "observation",
    "take",
    "average",
    "two",
    "values",
    "75th",
    "value",
    "76",
    "value",
    "right",
    "get",
    "value",
    "right",
    "guys",
    "exactly",
    "calculate",
    "different",
    "quarters",
    "let",
    "look",
    "interquartile",
    "range",
    "iqr",
    "interquartile",
    "range",
    "measure",
    "variability",
    "based",
    "dividing",
    "data",
    "set",
    "quartiles",
    "interquartile",
    "range",
    "calculated",
    "subtracting",
    "q1",
    "q3",
    "basically",
    "q3",
    "minus",
    "q1",
    "iq",
    "iqr",
    "q3",
    "minus",
    "q1",
    "right",
    "quartiles",
    "core",
    "tile",
    "represents",
    "quarter",
    "25",
    "right",
    "guys",
    "hope",
    "clear",
    "interquartile",
    "range",
    "quartiles",
    "let",
    "look",
    "variance",
    "covariance",
    "basically",
    "measure",
    "shows",
    "much",
    "variable",
    "first",
    "expected",
    "value",
    "okay",
    "basically",
    "variance",
    "variable",
    "variance",
    "calculated",
    "using",
    "formula",
    "right",
    "x",
    "basically",
    "represents",
    "data",
    "point",
    "data",
    "set",
    "n",
    "total",
    "number",
    "data",
    "points",
    "data",
    "set",
    "x",
    "bar",
    "basically",
    "mean",
    "data",
    "points",
    "right",
    "calculate",
    "variance",
    "variance",
    "basically",
    "computing",
    "squares",
    "deviations",
    "okay",
    "says",
    "square",
    "look",
    "deviation",
    "deviation",
    "difference",
    "element",
    "mean",
    "okay",
    "calculated",
    "using",
    "simple",
    "formula",
    "x",
    "basically",
    "represents",
    "data",
    "point",
    "mu",
    "mean",
    "population",
    "add",
    "exactly",
    "calculate",
    "deviation",
    "population",
    "variance",
    "sample",
    "variance",
    "specific",
    "whether",
    "calculating",
    "variance",
    "population",
    "data",
    "set",
    "sample",
    "data",
    "set",
    "difference",
    "elation",
    "sample",
    "variance",
    "formula",
    "population",
    "variance",
    "pretty",
    "explanatory",
    "x",
    "basically",
    "data",
    "point",
    "mu",
    "mean",
    "population",
    "n",
    "number",
    "samples",
    "data",
    "set",
    "right",
    "let",
    "look",
    "sample",
    "variance",
    "sample",
    "variance",
    "average",
    "squared",
    "differences",
    "mean",
    "right",
    "x",
    "data",
    "point",
    "sample",
    "data",
    "set",
    "x",
    "bar",
    "mean",
    "sample",
    "right",
    "main",
    "population",
    "sample",
    "notice",
    "smaller",
    "n",
    "number",
    "data",
    "points",
    "sample",
    "basically",
    "difference",
    "sample",
    "population",
    "variance",
    "hope",
    "clear",
    "coming",
    "standard",
    "deviation",
    "measure",
    "dispersion",
    "set",
    "data",
    "mean",
    "right",
    "basically",
    "deviation",
    "mean",
    "standard",
    "deviation",
    "better",
    "understand",
    "measures",
    "spread",
    "calculated",
    "let",
    "look",
    "small",
    "use",
    "case",
    "let",
    "say",
    "daenerys",
    "20",
    "dragons",
    "numbers",
    "nine",
    "five",
    "four",
    "shown",
    "screen",
    "work",
    "standard",
    "deviation",
    "order",
    "calculate",
    "standard",
    "deviation",
    "need",
    "know",
    "mean",
    "right",
    "first",
    "going",
    "find",
    "mean",
    "sample",
    "set",
    "calculate",
    "mean",
    "add",
    "numbers",
    "data",
    "set",
    "divided",
    "total",
    "number",
    "samples",
    "data",
    "set",
    "get",
    "value",
    "7",
    "clear",
    "rhs",
    "standard",
    "deviation",
    "formula",
    "right",
    "data",
    "point",
    "going",
    "subtract",
    "mean",
    "going",
    "square",
    "right",
    "get",
    "following",
    "result",
    "basically",
    "get",
    "425",
    "925",
    "finally",
    "find",
    "mean",
    "squared",
    "differences",
    "right",
    "standard",
    "deviation",
    "come",
    "two",
    "point",
    "nine",
    "eight",
    "three",
    "take",
    "square",
    "root",
    "guys",
    "pretty",
    "simple",
    "simple",
    "mathematic",
    "technique",
    "substitute",
    "values",
    "formula",
    "right",
    "hope",
    "clear",
    "let",
    "move",
    "discuss",
    "next",
    "topic",
    "information",
    "gain",
    "entropy",
    "one",
    "favorite",
    "topics",
    "statistics",
    "interesting",
    "topic",
    "mainly",
    "involved",
    "machine",
    "learning",
    "algorithms",
    "like",
    "decision",
    "trees",
    "random",
    "forest",
    "right",
    "important",
    "know",
    "information",
    "gain",
    "entropy",
    "really",
    "work",
    "essential",
    "building",
    "machine",
    "learning",
    "models",
    "focus",
    "statistic",
    "parts",
    "information",
    "gain",
    "entropy",
    "discuss",
    "use",
    "case",
    "see",
    "information",
    "gain",
    "entropy",
    "used",
    "decision",
    "trees",
    "know",
    "decision",
    "tree",
    "basically",
    "machine",
    "learning",
    "algorithm",
    "know",
    "anything",
    "explain",
    "everything",
    "depth",
    "worry",
    "let",
    "look",
    "exactly",
    "entropy",
    "information",
    "gain",
    "entropy",
    "basically",
    "measure",
    "sort",
    "uncertainty",
    "present",
    "data",
    "right",
    "measured",
    "using",
    "formula",
    "set",
    "instances",
    "data",
    "set",
    "although",
    "data",
    "items",
    "data",
    "set",
    "n",
    "different",
    "type",
    "classes",
    "data",
    "set",
    "pi",
    "event",
    "probability",
    "might",
    "seem",
    "little",
    "confusing",
    "go",
    "use",
    "case",
    "understand",
    "terms",
    "even",
    "better",
    "right",
    "cam",
    "information",
    "gain",
    "word",
    "suggests",
    "information",
    "gain",
    "indicates",
    "much",
    "information",
    "particular",
    "feature",
    "particular",
    "variable",
    "gives",
    "us",
    "final",
    "outcome",
    "okay",
    "measured",
    "using",
    "formula",
    "hedge",
    "entropy",
    "whole",
    "data",
    "set",
    "sj",
    "number",
    "instances",
    "j",
    "value",
    "attribute",
    "total",
    "number",
    "instances",
    "data",
    "set",
    "v",
    "set",
    "distinct",
    "values",
    "attribute",
    "hedge",
    "sj",
    "entropy",
    "subsets",
    "instances",
    "hedge",
    "comma",
    "entropy",
    "attribute",
    "even",
    "though",
    "seems",
    "confusing",
    "clear",
    "confusion",
    "right",
    "let",
    "discuss",
    "small",
    "problem",
    "statement",
    "understand",
    "information",
    "gain",
    "entropy",
    "used",
    "study",
    "significance",
    "model",
    "like",
    "said",
    "information",
    "gain",
    "entropy",
    "important",
    "statistical",
    "measures",
    "let",
    "us",
    "understand",
    "significance",
    "predictive",
    "model",
    "okay",
    "get",
    "clear",
    "understanding",
    "let",
    "look",
    "use",
    "case",
    "right",
    "suppose",
    "given",
    "problem",
    "statement",
    "right",
    "statement",
    "predict",
    "whether",
    "match",
    "played",
    "studying",
    "weather",
    "conditions",
    "predictor",
    "variables",
    "outlook",
    "humidity",
    "wind",
    "day",
    "also",
    "predictor",
    "variable",
    "target",
    "variable",
    "basically",
    "played",
    "already",
    "target",
    "variable",
    "variable",
    "trying",
    "protect",
    "okay",
    "value",
    "target",
    "variable",
    "decide",
    "whether",
    "game",
    "played",
    "right",
    "play",
    "two",
    "values",
    "yes",
    "meaning",
    "weather",
    "conditions",
    "good",
    "therefore",
    "play",
    "game",
    "yes",
    "meaning",
    "weather",
    "conditions",
    "good",
    "suitable",
    "play",
    "game",
    "alright",
    "problem",
    "statement",
    "hope",
    "problem",
    "statement",
    "clear",
    "solve",
    "problem",
    "make",
    "use",
    "something",
    "known",
    "decision",
    "trees",
    "guys",
    "think",
    "inverted",
    "tree",
    "branch",
    "tree",
    "denotes",
    "decision",
    "right",
    "branch",
    "known",
    "branch",
    "node",
    "branch",
    "node",
    "going",
    "take",
    "decision",
    "manner",
    "get",
    "outcome",
    "end",
    "branch",
    "right",
    "figure",
    "basically",
    "shows",
    "14",
    "observations",
    "9",
    "observations",
    "result",
    "yes",
    "meaning",
    "14",
    "days",
    "match",
    "played",
    "nine",
    "days",
    "alright",
    "see",
    "day",
    "1",
    "day",
    "2",
    "day",
    "8",
    "day",
    "9",
    "outlook",
    "alright",
    "basically",
    "try",
    "plaster",
    "data",
    "set",
    "depending",
    "outlook",
    "outlook",
    "sunny",
    "data",
    "set",
    "outlook",
    "overcast",
    "outlook",
    "rain",
    "right",
    "sunny",
    "two",
    "yeses",
    "three",
    "nodes",
    "okay",
    "outlook",
    "overcast",
    "four",
    "yes",
    "meaning",
    "four",
    "days",
    "outlook",
    "overcast",
    "play",
    "game",
    "right",
    "comes",
    "drain",
    "three",
    "yeses",
    "two",
    "nodes",
    "right",
    "notice",
    "decision",
    "made",
    "choosing",
    "outlook",
    "variable",
    "root",
    "node",
    "okay",
    "root",
    "node",
    "basically",
    "topmost",
    "node",
    "decision",
    "tree",
    "done",
    "created",
    "decision",
    "tree",
    "starts",
    "outlook",
    "node",
    "right",
    "splitting",
    "decision",
    "tree",
    "depending",
    "parameters",
    "like",
    "sunny",
    "overcast",
    "rain",
    "right",
    "like",
    "know",
    "outlook",
    "three",
    "values",
    "sunny",
    "overcast",
    "brain",
    "let",
    "explain",
    "manner",
    "okay",
    "making",
    "decision",
    "tree",
    "choosing",
    "outlook",
    "variable",
    "root",
    "node",
    "root",
    "note",
    "basically",
    "topmost",
    "node",
    "decision",
    "tree",
    "outlook",
    "node",
    "three",
    "branches",
    "coming",
    "sunny",
    "overcast",
    "rain",
    "basically",
    "outlook",
    "three",
    "values",
    "either",
    "sunny",
    "overcast",
    "rainy",
    "okay",
    "three",
    "values",
    "use",
    "assigned",
    "immediate",
    "branch",
    "nodes",
    "values",
    "possibility",
    "play",
    "equal",
    "yes",
    "calculated",
    "sunny",
    "rain",
    "branches",
    "give",
    "impure",
    "output",
    "meaning",
    "mix",
    "yes",
    "right",
    "two",
    "yeses",
    "three",
    "nodes",
    "three",
    "yeses",
    "two",
    "nodes",
    "comes",
    "overcast",
    "variable",
    "results",
    "hundred",
    "percent",
    "pure",
    "subset",
    "right",
    "shows",
    "overcast",
    "baby",
    "result",
    "definite",
    "certain",
    "output",
    "exactly",
    "entropy",
    "used",
    "measure",
    "right",
    "calculates",
    "impurity",
    "uncertainty",
    "alright",
    "lesser",
    "uncertainty",
    "entropy",
    "variable",
    "significant",
    "variable",
    "comes",
    "overcast",
    "literally",
    "impurity",
    "data",
    "set",
    "hundred",
    "percent",
    "pure",
    "subset",
    "right",
    "want",
    "variables",
    "like",
    "order",
    "build",
    "model",
    "right",
    "always",
    "ways",
    "get",
    "lucky",
    "always",
    "find",
    "variables",
    "result",
    "pure",
    "subsets",
    "measure",
    "entropy",
    "lesser",
    "entropy",
    "particular",
    "variable",
    "significant",
    "variable",
    "decision",
    "tree",
    "root",
    "node",
    "assigned",
    "best",
    "attribute",
    "decision",
    "tree",
    "predict",
    "precise",
    "outcome",
    "meaning",
    "root",
    "note",
    "significant",
    "variable",
    "right",
    "chosen",
    "outlook",
    "might",
    "ask",
    "chosen",
    "overcast",
    "okay",
    "overcast",
    "variable",
    "value",
    "outlook",
    "variable",
    "right",
    "chosen",
    "outlook",
    "hundred",
    "percent",
    "pure",
    "subset",
    "overcast",
    "right",
    "question",
    "head",
    "decide",
    "variable",
    "attribute",
    "best",
    "blitz",
    "data",
    "right",
    "know",
    "looked",
    "data",
    "told",
    "know",
    "hundred",
    "percent",
    "pure",
    "subset",
    "complex",
    "problem",
    "able",
    "understand",
    "variable",
    "best",
    "split",
    "data",
    "guys",
    "comes",
    "decision",
    "tree",
    "information",
    "gain",
    "entropy",
    "help",
    "understand",
    "variable",
    "best",
    "split",
    "data",
    "set",
    "right",
    "variable",
    "assign",
    "root",
    "node",
    "whichever",
    "variable",
    "assigned",
    "dude",
    "node",
    "best",
    "let",
    "data",
    "set",
    "significant",
    "variable",
    "right",
    "need",
    "use",
    "information",
    "gain",
    "entropy",
    "total",
    "14",
    "instances",
    "saw",
    "nine",
    "said",
    "yes",
    "5",
    "instances",
    "said",
    "know",
    "play",
    "particular",
    "day",
    "right",
    "calculate",
    "entropy",
    "formula",
    "substitute",
    "values",
    "formula",
    "substitute",
    "values",
    "formula",
    "get",
    "value",
    "right",
    "entropy",
    "uncertainty",
    "data",
    "present",
    "sample",
    "order",
    "ensure",
    "choose",
    "best",
    "variable",
    "root",
    "node",
    "let",
    "us",
    "look",
    "possible",
    "combinations",
    "use",
    "root",
    "node",
    "okay",
    "possible",
    "combinations",
    "either",
    "outlook",
    "windy",
    "humidity",
    "temperature",
    "okay",
    "four",
    "variables",
    "one",
    "variables",
    "root",
    "node",
    "select",
    "variable",
    "best",
    "fits",
    "root",
    "node",
    "going",
    "see",
    "using",
    "information",
    "gain",
    "entropy",
    "guys",
    "task",
    "hand",
    "find",
    "information",
    "gain",
    "attributes",
    "right",
    "outlook",
    "windy",
    "humidity",
    "temperature",
    "going",
    "find",
    "information",
    "nation",
    "gained",
    "right",
    "point",
    "remember",
    "variable",
    "results",
    "highest",
    "information",
    "gain",
    "must",
    "chosen",
    "give",
    "us",
    "precise",
    "output",
    "information",
    "right",
    "information",
    "gain",
    "attribute",
    "windy",
    "calculate",
    "first",
    "six",
    "instances",
    "true",
    "eight",
    "instances",
    "false",
    "okay",
    "substitute",
    "values",
    "formula",
    "get",
    "value",
    "zero",
    "point",
    "zero",
    "four",
    "eight",
    "get",
    "value",
    "low",
    "value",
    "information",
    "gain",
    "right",
    "information",
    "going",
    "get",
    "windy",
    "attribute",
    "pretty",
    "low",
    "let",
    "calculate",
    "information",
    "gain",
    "attribute",
    "outlook",
    "right",
    "total",
    "14",
    "instances",
    "five",
    "instances",
    "say",
    "sunny",
    "instances",
    "overcast",
    "five",
    "instances",
    "rainy",
    "right",
    "sonny",
    "three",
    "yeses",
    "nose",
    "overcast",
    "yes",
    "three",
    "years",
    "two",
    "nodes",
    "okay",
    "calculate",
    "information",
    "gain",
    "outlook",
    "variable",
    "get",
    "value",
    "zero",
    "point",
    "2",
    "4",
    "7",
    "compare",
    "information",
    "gain",
    "windy",
    "attribute",
    "value",
    "actually",
    "pretty",
    "good",
    "right",
    "zero",
    "point",
    "2",
    "4",
    "7",
    "pretty",
    "good",
    "value",
    "information",
    "gain",
    "let",
    "look",
    "information",
    "gain",
    "attribute",
    "humidity",
    "seven",
    "instances",
    "say",
    "hi",
    "seven",
    "instances",
    "say",
    "normal",
    "right",
    "high",
    "branch",
    "node",
    "three",
    "instances",
    "say",
    "yes",
    "rest",
    "instances",
    "would",
    "say",
    "similarly",
    "normal",
    "branch",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "instances",
    "would",
    "say",
    "yes",
    "one",
    "instance",
    "says",
    "right",
    "calculate",
    "information",
    "gain",
    "humidity",
    "variable",
    "going",
    "get",
    "value",
    "one",
    "also",
    "pretty",
    "decent",
    "value",
    "compare",
    "information",
    "gain",
    "attribute",
    "outlook",
    "less",
    "right",
    "let",
    "look",
    "information",
    "gain",
    "attribute",
    "temperature",
    "right",
    "temperature",
    "hold",
    "repeat",
    "basically",
    "temperature",
    "attribute",
    "hold",
    "hot",
    "mild",
    "cool",
    "okay",
    "hot",
    "two",
    "instances",
    "says",
    "yes",
    "two",
    "instances",
    "mild",
    "four",
    "instances",
    "yes",
    "two",
    "instances",
    "col",
    "three",
    "instances",
    "yes",
    "one",
    "instance",
    "right",
    "calculate",
    "information",
    "gain",
    "attribute",
    "get",
    "value",
    "zero",
    "point",
    "zero",
    "nine",
    "less",
    "summarize",
    "look",
    "information",
    "gain",
    "variable",
    "see",
    "outlook",
    "maximum",
    "gain",
    "right",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "highest",
    "information",
    "gain",
    "value",
    "must",
    "always",
    "choose",
    "variable",
    "highest",
    "information",
    "gain",
    "split",
    "data",
    "root",
    "node",
    "assign",
    "outlook",
    "variable",
    "root",
    "node",
    "right",
    "guys",
    "hope",
    "use",
    "case",
    "clear",
    "doubts",
    "please",
    "keep",
    "commenting",
    "doubts",
    "let",
    "move",
    "look",
    "exactly",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "last",
    "topic",
    "descriptive",
    "statistics",
    "read",
    "running",
    "short",
    "demo",
    "showing",
    "calculate",
    "mean",
    "median",
    "mode",
    "standard",
    "deviation",
    "variance",
    "values",
    "using",
    "okay",
    "let",
    "talk",
    "confusion",
    "matrix",
    "guys",
    "confusion",
    "matrix",
    "get",
    "confused",
    "complex",
    "topic",
    "confusion",
    "matrix",
    "matrix",
    "often",
    "used",
    "describe",
    "performance",
    "model",
    "right",
    "specifically",
    "used",
    "classification",
    "models",
    "classifier",
    "calculate",
    "accuracy",
    "calculate",
    "performance",
    "classifier",
    "comparing",
    "actual",
    "results",
    "predicted",
    "results",
    "right",
    "looks",
    "like",
    "positive",
    "negative",
    "little",
    "confusing",
    "get",
    "back",
    "exactly",
    "true",
    "positive",
    "negative",
    "stands",
    "let",
    "look",
    "example",
    "let",
    "try",
    "understand",
    "exactly",
    "confusion",
    "matrix",
    "guys",
    "made",
    "sure",
    "put",
    "examples",
    "every",
    "topic",
    "important",
    "understand",
    "practical",
    "part",
    "statistics",
    "right",
    "statistics",
    "literally",
    "nothing",
    "theory",
    "need",
    "understand",
    "calculations",
    "done",
    "statistics",
    "okay",
    "done",
    "let",
    "look",
    "small",
    "use",
    "case",
    "okay",
    "let",
    "consider",
    "given",
    "data",
    "hundred",
    "sixty",
    "five",
    "patients",
    "hundred",
    "five",
    "patients",
    "disease",
    "remaining",
    "50",
    "patients",
    "disease",
    "okay",
    "going",
    "build",
    "classifier",
    "predicts",
    "using",
    "hundred",
    "sixty",
    "five",
    "observations",
    "feed",
    "165",
    "observations",
    "classifier",
    "predict",
    "output",
    "every",
    "time",
    "new",
    "patients",
    "detail",
    "fed",
    "classifier",
    "right",
    "165",
    "cases",
    "let",
    "say",
    "classifier",
    "predicted",
    "yes",
    "hundred",
    "ten",
    "times",
    "55",
    "times",
    "alright",
    "yes",
    "basically",
    "stands",
    "yes",
    "person",
    "disease",
    "stands",
    "know",
    "person",
    "disease",
    "right",
    "pretty",
    "yeah",
    "predicted",
    "hundred",
    "ten",
    "times",
    "patient",
    "disease",
    "55",
    "times",
    "know",
    "patient",
    "disease",
    "however",
    "reality",
    "hundred",
    "five",
    "patients",
    "sample",
    "disease",
    "60",
    "patients",
    "disease",
    "right",
    "calculate",
    "accuracy",
    "model",
    "basically",
    "build",
    "confusion",
    "matrix",
    "right",
    "matrix",
    "looks",
    "like",
    "basically",
    "denotes",
    "total",
    "number",
    "observations",
    "165",
    "case",
    "actual",
    "denotes",
    "actual",
    "use",
    "data",
    "set",
    "predicted",
    "denotes",
    "predicted",
    "values",
    "classifier",
    "actual",
    "value",
    "predicted",
    "value",
    "classifier",
    "correctly",
    "able",
    "classify",
    "50",
    "cases",
    "right",
    "since",
    "50",
    "correctly",
    "able",
    "classify",
    "10",
    "cases",
    "incorrectly",
    "classified",
    "meaning",
    "actual",
    "value",
    "classifier",
    "predicted",
    "yes",
    "similarly",
    "wrongly",
    "predicted",
    "five",
    "patients",
    "diseases",
    "whereas",
    "actually",
    "diseases",
    "correctly",
    "predicted",
    "hundred",
    "patients",
    "disease",
    "right",
    "know",
    "little",
    "bit",
    "confusing",
    "look",
    "values",
    "50",
    "meaning",
    "correctly",
    "predicted",
    "50",
    "values",
    "yes",
    "means",
    "wrongly",
    "predicted",
    "yes",
    "values",
    "supposed",
    "predict",
    "right",
    "exactly",
    "true",
    "positive",
    "negative",
    "tell",
    "exactly",
    "true",
    "positive",
    "cases",
    "predicted",
    "yes",
    "actually",
    "disease",
    "right",
    "basically",
    "value",
    "already",
    "predicted",
    "yes",
    "even",
    "though",
    "disease",
    "10",
    "true",
    "positives",
    "right",
    "similarly",
    "predicted",
    "know",
    "disease",
    "meaning",
    "correct",
    "false",
    "positive",
    "predicted",
    "yes",
    "actually",
    "disease",
    "also",
    "known",
    "type",
    "1",
    "error",
    "predicted",
    "actually",
    "disease",
    "guys",
    "basically",
    "true",
    "negatives",
    "basically",
    "correct",
    "classifications",
    "right",
    "confusion",
    "matrix",
    "hope",
    "concept",
    "clear",
    "guys",
    "doubts",
    "please",
    "comment",
    "doubt",
    "comment",
    "section",
    "guys",
    "entire",
    "descriptive",
    "x",
    "module",
    "discuss",
    "probability",
    "okay",
    "understand",
    "exactly",
    "probability",
    "let",
    "clear",
    "common",
    "misconception",
    "people",
    "often",
    "tend",
    "ask",
    "question",
    "relationship",
    "statistics",
    "probability",
    "probability",
    "statistics",
    "related",
    "fields",
    "right",
    "probability",
    "mathematical",
    "method",
    "used",
    "statistical",
    "analysis",
    "therefore",
    "say",
    "probability",
    "statistics",
    "interconnected",
    "launches",
    "mathematics",
    "deal",
    "analyzing",
    "relative",
    "frequency",
    "events",
    "interconnected",
    "feels",
    "probability",
    "makes",
    "use",
    "statistics",
    "statistics",
    "makes",
    "use",
    "probability",
    "interconnected",
    "fields",
    "relationship",
    "statistics",
    "probability",
    "let",
    "understand",
    "exactly",
    "probability",
    "probability",
    "measure",
    "likely",
    "event",
    "occur",
    "precise",
    "ratio",
    "desired",
    "outcome",
    "total",
    "outcomes",
    "probability",
    "outcomes",
    "always",
    "sum",
    "1",
    "probability",
    "always",
    "sum",
    "1",
    "probability",
    "go",
    "beyond",
    "one",
    "okay",
    "either",
    "probability",
    "0",
    "1",
    "form",
    "decimals",
    "like",
    "form",
    "valuable",
    "always",
    "stay",
    "range",
    "0",
    "okay",
    "famous",
    "example",
    "probability",
    "rolling",
    "dice",
    "example",
    "roll",
    "dice",
    "get",
    "six",
    "possible",
    "outcomes",
    "right",
    "get",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "phases",
    "dice",
    "possibility",
    "one",
    "outcome",
    "probability",
    "rolling",
    "dice",
    "get",
    "3",
    "probability",
    "1",
    "6",
    "right",
    "one",
    "phase",
    "number",
    "3",
    "six",
    "phases",
    "one",
    "phase",
    "number",
    "three",
    "probability",
    "getting",
    "3",
    "roll",
    "dice",
    "1",
    "6",
    "similarly",
    "want",
    "find",
    "probability",
    "getting",
    "number",
    "5",
    "probability",
    "going",
    "1",
    "right",
    "sum",
    "right",
    "guys",
    "exactly",
    "probability",
    "simple",
    "concept",
    "learnt",
    "8",
    "standard",
    "onwards",
    "right",
    "let",
    "understand",
    "different",
    "terminologies",
    "related",
    "probability",
    "three",
    "terminologies",
    "often",
    "come",
    "across",
    "talk",
    "probability",
    "something",
    "known",
    "random",
    "experiment",
    "okay",
    "basically",
    "experiment",
    "process",
    "outcomes",
    "predicted",
    "certainty",
    "right",
    "use",
    "probability",
    "going",
    "use",
    "probability",
    "order",
    "predict",
    "outcome",
    "sort",
    "certainty",
    "sample",
    "space",
    "entire",
    "possible",
    "set",
    "outcomes",
    "random",
    "experiment",
    "event",
    "one",
    "outcomes",
    "experiment",
    "consider",
    "example",
    "love",
    "rolling",
    "dice",
    "let",
    "say",
    "want",
    "find",
    "probability",
    "getting",
    "roll",
    "dice",
    "okay",
    "finding",
    "probability",
    "random",
    "experiment",
    "sample",
    "space",
    "basically",
    "entire",
    "possibility",
    "okay",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "phases",
    "need",
    "find",
    "probability",
    "getting",
    "2",
    "right",
    "possible",
    "outcomes",
    "basically",
    "represent",
    "sample",
    "space",
    "okay",
    "1",
    "6",
    "possible",
    "outcomes",
    "represents",
    "sample",
    "space",
    "event",
    "one",
    "outcome",
    "experiment",
    "case",
    "event",
    "get",
    "roll",
    "dice",
    "right",
    "event",
    "probability",
    "getting",
    "roll",
    "dice",
    "guys",
    "basically",
    "random",
    "experiment",
    "sample",
    "space",
    "event",
    "really",
    "means",
    "alright",
    "let",
    "discuss",
    "different",
    "types",
    "events",
    "two",
    "types",
    "events",
    "know",
    "disjoint",
    "non",
    "disjoint",
    "events",
    "disjoint",
    "events",
    "events",
    "common",
    "outcome",
    "example",
    "draw",
    "single",
    "card",
    "deck",
    "cards",
    "king",
    "queen",
    "correct",
    "either",
    "king",
    "queen",
    "non",
    "disjoint",
    "events",
    "events",
    "common",
    "outcomes",
    "example",
    "student",
    "get",
    "hundred",
    "marks",
    "statistics",
    "hundred",
    "marks",
    "probability",
    "right",
    "also",
    "outcome",
    "ball",
    "delibird",
    "ball",
    "6",
    "right",
    "non",
    "disjoint",
    "events",
    "simple",
    "understand",
    "right",
    "let",
    "move",
    "look",
    "different",
    "types",
    "probability",
    "distribution",
    "right",
    "discussing",
    "three",
    "main",
    "probability",
    "distribution",
    "functions",
    "talking",
    "probability",
    "density",
    "function",
    "normal",
    "distribution",
    "central",
    "limit",
    "theorem",
    "okay",
    "probability",
    "density",
    "function",
    "also",
    "known",
    "pdf",
    "concerned",
    "relative",
    "likelihood",
    "continuous",
    "random",
    "variable",
    "take",
    "given",
    "value",
    "right",
    "pdf",
    "gives",
    "probability",
    "variable",
    "lies",
    "range",
    "basically",
    "trying",
    "going",
    "try",
    "find",
    "probability",
    "continuous",
    "random",
    "variable",
    "specified",
    "range",
    "okay",
    "graph",
    "denotes",
    "pdf",
    "continuous",
    "variable",
    "graph",
    "also",
    "known",
    "bell",
    "curve",
    "right",
    "famously",
    "called",
    "bell",
    "curve",
    "shape",
    "three",
    "important",
    "properties",
    "know",
    "probability",
    "density",
    "function",
    "graph",
    "pdf",
    "continuous",
    "range",
    "finding",
    "probability",
    "continuous",
    "variable",
    "lies",
    "ranges",
    "b",
    "right",
    "second",
    "property",
    "area",
    "bounded",
    "curve",
    "density",
    "function",
    "equal",
    "1",
    "basically",
    "area",
    "curve",
    "equal",
    "1",
    "right",
    "denotes",
    "probability",
    "probability",
    "arrange",
    "one",
    "0",
    "1",
    "property",
    "number",
    "three",
    "probability",
    "random",
    "variable",
    "assumes",
    "value",
    "b",
    "equal",
    "area",
    "pdf",
    "bounded",
    "okay",
    "means",
    "probability",
    "value",
    "denoted",
    "area",
    "graph",
    "right",
    "whatever",
    "value",
    "get",
    "basically",
    "one",
    "probability",
    "random",
    "variable",
    "lie",
    "range",
    "right",
    "hope",
    "understood",
    "probability",
    "density",
    "function",
    "basically",
    "probability",
    "finding",
    "value",
    "continuous",
    "random",
    "variable",
    "range",
    "right",
    "let",
    "look",
    "next",
    "distribution",
    "normal",
    "distribution",
    "normal",
    "distribution",
    "also",
    "known",
    "gaussian",
    "distribution",
    "probability",
    "distribution",
    "denotes",
    "symmetric",
    "property",
    "mean",
    "right",
    "meaning",
    "idea",
    "behind",
    "function",
    "data",
    "near",
    "mean",
    "occurs",
    "frequently",
    "data",
    "away",
    "mean",
    "means",
    "say",
    "data",
    "around",
    "mean",
    "represents",
    "entire",
    "data",
    "set",
    "okay",
    "take",
    "sample",
    "data",
    "around",
    "mean",
    "represent",
    "entire",
    "data",
    "set",
    "similar",
    "probability",
    "density",
    "function",
    "normal",
    "distribution",
    "appears",
    "bell",
    "curve",
    "right",
    "comes",
    "normal",
    "distribution",
    "two",
    "important",
    "factors",
    "right",
    "mean",
    "population",
    "standard",
    "deviation",
    "okay",
    "mean",
    "graph",
    "determines",
    "location",
    "center",
    "graph",
    "right",
    "standard",
    "deviation",
    "determines",
    "height",
    "graph",
    "okay",
    "standard",
    "deviation",
    "large",
    "curve",
    "going",
    "look",
    "something",
    "like",
    "right",
    "short",
    "wide",
    "standard",
    "deviation",
    "small",
    "curve",
    "tall",
    "narrow",
    "right",
    "normal",
    "distribution",
    "let",
    "look",
    "central",
    "limit",
    "theorem",
    "central",
    "limit",
    "theorem",
    "states",
    "sampling",
    "distribution",
    "mean",
    "independent",
    "random",
    "variable",
    "normal",
    "nearly",
    "normal",
    "sample",
    "size",
    "large",
    "enough",
    "little",
    "confusing",
    "okay",
    "let",
    "break",
    "simple",
    "terms",
    "large",
    "population",
    "divided",
    "many",
    "samples",
    "mean",
    "samples",
    "population",
    "almost",
    "equal",
    "mean",
    "entire",
    "population",
    "right",
    "meaning",
    "sample",
    "normally",
    "distributed",
    "right",
    "compare",
    "mean",
    "sample",
    "almost",
    "equal",
    "mean",
    "population",
    "right",
    "graph",
    "basically",
    "shows",
    "clear",
    "understanding",
    "central",
    "limit",
    "theorem",
    "red",
    "see",
    "sample",
    "mean",
    "sample",
    "almost",
    "along",
    "line",
    "right",
    "okay",
    "exactly",
    "central",
    "limit",
    "theorem",
    "states",
    "accuracy",
    "resemblance",
    "normal",
    "distribution",
    "depends",
    "two",
    "main",
    "factors",
    "right",
    "first",
    "number",
    "sample",
    "points",
    "consider",
    "right",
    "second",
    "shape",
    "underlying",
    "population",
    "shape",
    "obviously",
    "depends",
    "standard",
    "deviation",
    "mean",
    "sample",
    "correct",
    "guys",
    "central",
    "limit",
    "theorem",
    "basically",
    "states",
    "sample",
    "normally",
    "distributed",
    "way",
    "mean",
    "sample",
    "coincide",
    "mean",
    "actual",
    "population",
    "right",
    "short",
    "terms",
    "central",
    "limit",
    "theorem",
    "states",
    "alright",
    "holds",
    "true",
    "large",
    "mostly",
    "small",
    "data",
    "set",
    "deviations",
    "compared",
    "large",
    "data",
    "set",
    "scaling",
    "factor",
    "right",
    "small",
    "deviation",
    "small",
    "data",
    "set",
    "change",
    "value",
    "drastically",
    "large",
    "data",
    "set",
    "small",
    "deviation",
    "matter",
    "let",
    "move",
    "look",
    "next",
    "topic",
    "different",
    "types",
    "probability",
    "important",
    "topic",
    "problems",
    "solved",
    "understanding",
    "type",
    "probability",
    "use",
    "solve",
    "problem",
    "right",
    "three",
    "important",
    "types",
    "probability",
    "marginal",
    "joint",
    "conditional",
    "probability",
    "let",
    "discuss",
    "probability",
    "event",
    "occurring",
    "unconditioned",
    "event",
    "known",
    "marginal",
    "probability",
    "unconditional",
    "probability",
    "let",
    "say",
    "want",
    "find",
    "probability",
    "card",
    "drawn",
    "heart",
    "right",
    "want",
    "find",
    "probability",
    "card",
    "drawn",
    "heart",
    "prophet",
    "b13",
    "52",
    "since",
    "52",
    "cards",
    "deck",
    "13",
    "hearts",
    "deck",
    "cards",
    "right",
    "52",
    "cards",
    "turtleneck",
    "marginal",
    "probability",
    "13",
    "marginal",
    "probability",
    "let",
    "understand",
    "joint",
    "probability",
    "joint",
    "probability",
    "measure",
    "two",
    "events",
    "happening",
    "time",
    "okay",
    "let",
    "say",
    "two",
    "events",
    "probability",
    "event",
    "b",
    "occurring",
    "dissection",
    "example",
    "want",
    "find",
    "probability",
    "card",
    "four",
    "red",
    "would",
    "joint",
    "probability",
    "right",
    "finding",
    "card",
    "4",
    "card",
    "red",
    "color",
    "answer",
    "2",
    "52",
    "heart",
    "diamonds",
    "correct",
    "red",
    "color",
    "therefore",
    "probability",
    "52",
    "1",
    "26",
    "right",
    "joint",
    "probability",
    "moving",
    "let",
    "look",
    "exactly",
    "conditional",
    "probability",
    "probability",
    "event",
    "outcome",
    "based",
    "occurrence",
    "previous",
    "event",
    "outcome",
    "call",
    "conditional",
    "probability",
    "okay",
    "conditional",
    "probability",
    "event",
    "b",
    "probability",
    "event",
    "occur",
    "given",
    "event",
    "already",
    "occurred",
    "right",
    "b",
    "dependent",
    "events",
    "expression",
    "conditional",
    "probability",
    "given",
    "first",
    "term",
    "left",
    "hand",
    "side",
    "p",
    "b",
    "basically",
    "probability",
    "event",
    "b",
    "occurring",
    "given",
    "event",
    "already",
    "occurred",
    "right",
    "like",
    "said",
    "b",
    "dependent",
    "events",
    "expression",
    "b",
    "independent",
    "events",
    "expression",
    "conditional",
    "probability",
    "like",
    "right",
    "guys",
    "p",
    "b",
    "b",
    "obviously",
    "probability",
    "probability",
    "b",
    "right",
    "let",
    "move",
    "order",
    "understand",
    "conditional",
    "probability",
    "joint",
    "probability",
    "marginal",
    "probability",
    "let",
    "look",
    "small",
    "use",
    "case",
    "okay",
    "basically",
    "going",
    "take",
    "data",
    "set",
    "examines",
    "salary",
    "package",
    "training",
    "undergone",
    "candidates",
    "okay",
    "60",
    "candidates",
    "without",
    "training",
    "forty",
    "five",
    "candidates",
    "enrolled",
    "adder",
    "curse",
    "training",
    "right",
    "task",
    "assess",
    "training",
    "salary",
    "package",
    "okay",
    "let",
    "look",
    "little",
    "depth",
    "total",
    "hundred",
    "five",
    "candidates",
    "60",
    "enrolled",
    "frederick",
    "training",
    "45",
    "enrolled",
    "deer",
    "acres",
    "training",
    "small",
    "survey",
    "conducted",
    "rating",
    "package",
    "salary",
    "got",
    "right",
    "read",
    "data",
    "understand",
    "five",
    "candidates",
    "without",
    "education",
    "training",
    "got",
    "poor",
    "salary",
    "package",
    "okay",
    "similarly",
    "30",
    "candidates",
    "ed",
    "eureka",
    "training",
    "got",
    "good",
    "package",
    "right",
    "guys",
    "basically",
    "comparing",
    "salary",
    "package",
    "person",
    "depending",
    "whether",
    "enrolled",
    "director",
    "training",
    "right",
    "data",
    "set",
    "let",
    "look",
    "problem",
    "statement",
    "find",
    "probability",
    "candidate",
    "undergone",
    "drake",
    "training",
    "quite",
    "simple",
    "type",
    "probability",
    "marginal",
    "probability",
    "right",
    "probability",
    "candidate",
    "undergone",
    "edger",
    "acres",
    "training",
    "obviously",
    "45",
    "divided",
    "hundred",
    "five",
    "since",
    "45",
    "number",
    "candidates",
    "eddie",
    "record",
    "raining",
    "hundred",
    "five",
    "total",
    "number",
    "candidates",
    "get",
    "value",
    "approximately",
    "right",
    "probability",
    "candidate",
    "undergone",
    "educate",
    "girl",
    "straining",
    "next",
    "question",
    "find",
    "probability",
    "candidate",
    "attended",
    "edger",
    "acres",
    "training",
    "also",
    "good",
    "package",
    "obviously",
    "joint",
    "probability",
    "problem",
    "right",
    "calculate",
    "since",
    "table",
    "quite",
    "formatted",
    "directly",
    "find",
    "people",
    "gotten",
    "good",
    "package",
    "along",
    "eddie",
    "record",
    "raining",
    "30",
    "right",
    "hundred",
    "five",
    "people",
    "30",
    "people",
    "education",
    "training",
    "good",
    "package",
    "right",
    "specifically",
    "asking",
    "people",
    "eddie",
    "record",
    "raining",
    "remember",
    "night",
    "question",
    "find",
    "probability",
    "gang",
    "today",
    "attended",
    "editor",
    "acres",
    "training",
    "also",
    "good",
    "package",
    "right",
    "need",
    "consider",
    "two",
    "factors",
    "candidate",
    "addenda",
    "deaderick",
    "training",
    "good",
    "package",
    "clearly",
    "number",
    "30",
    "30",
    "divided",
    "total",
    "number",
    "candidates",
    "right",
    "get",
    "answer",
    "clearly",
    "next",
    "find",
    "probability",
    "candidate",
    "good",
    "package",
    "given",
    "undergone",
    "training",
    "okay",
    "early",
    "conditional",
    "probability",
    "defining",
    "condition",
    "saying",
    "want",
    "find",
    "probability",
    "candidate",
    "good",
    "package",
    "given",
    "undergone",
    "training",
    "right",
    "condition",
    "undergone",
    "training",
    "right",
    "number",
    "people",
    "undergone",
    "training",
    "60",
    "five",
    "got",
    "good",
    "package",
    "phi",
    "60",
    "five",
    "hundred",
    "five",
    "clearly",
    "mentioned",
    "good",
    "pack",
    "given",
    "undergone",
    "training",
    "consider",
    "people",
    "undergone",
    "training",
    "right",
    "five",
    "people",
    "undergone",
    "training",
    "gotten",
    "good",
    "package",
    "right",
    "5",
    "divided",
    "60",
    "get",
    "probability",
    "around",
    "pretty",
    "low",
    "right",
    "okay",
    "different",
    "types",
    "probability",
    "let",
    "move",
    "look",
    "last",
    "topic",
    "probability",
    "base",
    "theorem",
    "guys",
    "base",
    "room",
    "important",
    "concept",
    "comes",
    "statistics",
    "probability",
    "majorly",
    "used",
    "knife",
    "bias",
    "algorithm",
    "aware",
    "bias",
    "supervised",
    "learning",
    "classification",
    "algorithm",
    "mainly",
    "used",
    "gmail",
    "spam",
    "filtering",
    "right",
    "lot",
    "might",
    "noticed",
    "open",
    "gmail",
    "see",
    "folder",
    "called",
    "spam",
    "right",
    "carried",
    "machine",
    "learning",
    "algorithm",
    "use",
    "knife",
    "bias",
    "right",
    "let",
    "discuss",
    "exactly",
    "bayes",
    "theorem",
    "denotes",
    "bias",
    "theorem",
    "used",
    "show",
    "relation",
    "one",
    "conditional",
    "probability",
    "inverse",
    "right",
    "basically",
    "nothing",
    "probability",
    "event",
    "occurring",
    "based",
    "prior",
    "knowledge",
    "conditions",
    "might",
    "related",
    "event",
    "okay",
    "mathematically",
    "bell",
    "theorem",
    "represented",
    "like",
    "right",
    "shown",
    "equation",
    "term",
    "referred",
    "likelihood",
    "ratio",
    "measures",
    "probability",
    "occurrence",
    "event",
    "given",
    "event",
    "okay",
    "left",
    "hand",
    "side",
    "known",
    "posterior",
    "right",
    "referred",
    "posterior",
    "means",
    "probability",
    "occurrence",
    "given",
    "event",
    "right",
    "second",
    "term",
    "referred",
    "likelihood",
    "ratio",
    "measures",
    "probability",
    "occurrence",
    "b",
    "given",
    "event",
    "p",
    "also",
    "known",
    "prior",
    "refers",
    "actual",
    "probability",
    "distribution",
    "p",
    "b",
    "probability",
    "b",
    "right",
    "bias",
    "theorem",
    "order",
    "better",
    "understand",
    "base",
    "theorem",
    "let",
    "look",
    "small",
    "example",
    "let",
    "say",
    "three",
    "bowels",
    "bow",
    "bow",
    "bouncy",
    "okay",
    "barley",
    "contains",
    "two",
    "blue",
    "balls",
    "red",
    "balls",
    "bowel",
    "b",
    "contains",
    "eight",
    "blue",
    "balls",
    "red",
    "balls",
    "wow",
    "zeke",
    "games",
    "one",
    "blue",
    "ball",
    "three",
    "red",
    "balls",
    "draw",
    "one",
    "ball",
    "bowl",
    "probability",
    "draw",
    "blue",
    "ball",
    "bowel",
    "know",
    "drew",
    "exactly",
    "total",
    "two",
    "blue",
    "balls",
    "right",
    "understand",
    "question",
    "please",
    "read",
    "shall",
    "pause",
    "second",
    "two",
    "right",
    "hope",
    "understood",
    "question",
    "okay",
    "going",
    "going",
    "draw",
    "blueprint",
    "tell",
    "exactly",
    "solve",
    "problem",
    "want",
    "give",
    "solution",
    "problem",
    "right",
    "draw",
    "blueprint",
    "tell",
    "exactly",
    "steps",
    "want",
    "come",
    "solution",
    "right",
    "formula",
    "also",
    "given",
    "everything",
    "given",
    "come",
    "final",
    "answer",
    "right",
    "let",
    "look",
    "solve",
    "problem",
    "first",
    "let",
    "consider",
    "right",
    "let",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "let",
    "x",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "right",
    "two",
    "events",
    "need",
    "calculate",
    "probability",
    "two",
    "probabilities",
    "need",
    "consider",
    "one",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "okay",
    "two",
    "represented",
    "x",
    "respectively",
    "want",
    "probability",
    "occurrence",
    "event",
    "given",
    "x",
    "means",
    "given",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "probability",
    "picking",
    "blue",
    "ball",
    "bag",
    "definition",
    "conditional",
    "probability",
    "exactly",
    "equation",
    "look",
    "like",
    "correct",
    "basically",
    "occurrence",
    "event",
    "given",
    "element",
    "x",
    "probability",
    "x",
    "probability",
    "x",
    "alone",
    "correct",
    "need",
    "need",
    "find",
    "two",
    "probabilities",
    "probability",
    "x",
    "occurring",
    "together",
    "probability",
    "okay",
    "entire",
    "solution",
    "find",
    "p",
    "probability",
    "x",
    "three",
    "ways",
    "first",
    "white",
    "ball",
    "either",
    "white",
    "read",
    "see",
    "first",
    "find",
    "probability",
    "x",
    "x",
    "basically",
    "represents",
    "event",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "right",
    "three",
    "ways",
    "possible",
    "pick",
    "one",
    "blue",
    "ball",
    "bowel",
    "one",
    "bowel",
    "second",
    "case",
    "pick",
    "one",
    "another",
    "blue",
    "ball",
    "see",
    "third",
    "case",
    "pick",
    "blue",
    "ball",
    "bagby",
    "blue",
    "ball",
    "bagsy",
    "right",
    "three",
    "ways",
    "possible",
    "need",
    "find",
    "probability",
    "step",
    "need",
    "find",
    "probability",
    "x",
    "occurring",
    "together",
    "sum",
    "terms",
    "one",
    "two",
    "okay",
    "events",
    "picking",
    "ball",
    "bag",
    "correct",
    "find",
    "probability",
    "let",
    "know",
    "answer",
    "comment",
    "section",
    "right",
    "see",
    "get",
    "answer",
    "right",
    "gave",
    "entire",
    "solution",
    "substitute",
    "value",
    "right",
    "want",
    "second",
    "two",
    "going",
    "pause",
    "screen",
    "go",
    "clearer",
    "way",
    "right",
    "remember",
    "need",
    "calculate",
    "two",
    "first",
    "probability",
    "need",
    "calculate",
    "event",
    "picking",
    "blue",
    "ball",
    "bag",
    "given",
    "picking",
    "exactly",
    "two",
    "blue",
    "balls",
    "okay",
    "ii",
    "probability",
    "need",
    "calculate",
    "event",
    "picking",
    "exactly",
    "bluebirds",
    "right",
    "two",
    "probabilities",
    "need",
    "calculate",
    "remember",
    "solution",
    "right",
    "guys",
    "make",
    "sure",
    "mention",
    "answers",
    "comment",
    "section",
    "let",
    "move",
    "get",
    "next",
    "topic",
    "inferential",
    "statistics",
    "guys",
    "completed",
    "probability",
    "module",
    "right",
    "discuss",
    "inferential",
    "statistics",
    "second",
    "type",
    "statistics",
    "discussed",
    "descriptive",
    "statistics",
    "earlier",
    "right",
    "like",
    "mentioned",
    "earlier",
    "inferential",
    "statistics",
    "also",
    "known",
    "statistical",
    "inference",
    "branch",
    "statistics",
    "deals",
    "forming",
    "inferences",
    "predictions",
    "population",
    "based",
    "sample",
    "data",
    "taken",
    "population",
    "right",
    "question",
    "ask",
    "one",
    "form",
    "inferences",
    "predictions",
    "sample",
    "answer",
    "use",
    "point",
    "estimation",
    "okay",
    "must",
    "wondering",
    "point",
    "estimation",
    "one",
    "estimation",
    "concerned",
    "use",
    "sample",
    "data",
    "measure",
    "single",
    "value",
    "serves",
    "approximate",
    "value",
    "best",
    "estimate",
    "unknown",
    "population",
    "parameter",
    "little",
    "confusing",
    "let",
    "break",
    "camping",
    "order",
    "calculate",
    "mean",
    "huge",
    "population",
    "first",
    "draw",
    "sample",
    "population",
    "find",
    "sample",
    "mean",
    "right",
    "sample",
    "mean",
    "used",
    "estimate",
    "population",
    "mean",
    "basically",
    "point",
    "estimate",
    "estimating",
    "value",
    "one",
    "parameters",
    "population",
    "right",
    "basically",
    "main",
    "trying",
    "estimate",
    "value",
    "mean",
    "point",
    "estimation",
    "two",
    "main",
    "terms",
    "point",
    "estimation",
    "something",
    "known",
    "estimator",
    "something",
    "known",
    "estimate",
    "estimator",
    "function",
    "sample",
    "used",
    "find",
    "estimate",
    "alright",
    "example",
    "basically",
    "sample",
    "mean",
    "right",
    "function",
    "calculates",
    "sample",
    "mean",
    "known",
    "estimator",
    "realized",
    "value",
    "estimator",
    "estimate",
    "right",
    "hope",
    "point",
    "estimation",
    "clear",
    "find",
    "estimates",
    "four",
    "common",
    "ways",
    "first",
    "one",
    "method",
    "moment",
    "yo",
    "form",
    "equation",
    "sample",
    "data",
    "set",
    "analyze",
    "similar",
    "equation",
    "population",
    "data",
    "set",
    "well",
    "like",
    "population",
    "mean",
    "population",
    "variance",
    "simple",
    "terms",
    "taking",
    "known",
    "facts",
    "population",
    "extending",
    "ideas",
    "sample",
    "alright",
    "analyze",
    "sample",
    "estimate",
    "essential",
    "complex",
    "values",
    "right",
    "next",
    "maximum",
    "likelihood",
    "method",
    "basically",
    "uses",
    "model",
    "estimate",
    "value",
    "right",
    "maximum",
    "likelihood",
    "majorly",
    "based",
    "probability",
    "lot",
    "probability",
    "involved",
    "method",
    "next",
    "base",
    "estimator",
    "works",
    "minimizing",
    "errors",
    "average",
    "risk",
    "okay",
    "base",
    "estimator",
    "lot",
    "bayes",
    "theorem",
    "right",
    "let",
    "get",
    "depth",
    "estimation",
    "methods",
    "finally",
    "best",
    "unbiased",
    "estimators",
    "method",
    "seven",
    "unbiased",
    "estimators",
    "used",
    "approximate",
    "parameter",
    "okay",
    "guys",
    "couple",
    "methods",
    "used",
    "find",
    "estimate",
    "method",
    "find",
    "estimate",
    "known",
    "interval",
    "estimation",
    "okay",
    "one",
    "important",
    "estimation",
    "methods",
    "right",
    "confidence",
    "interval",
    "also",
    "comes",
    "picture",
    "right",
    "apart",
    "interval",
    "estimation",
    "also",
    "something",
    "known",
    "margin",
    "error",
    "discussing",
    "upcoming",
    "slides",
    "first",
    "let",
    "understand",
    "interval",
    "estimate",
    "okay",
    "interval",
    "range",
    "values",
    "used",
    "estimate",
    "population",
    "parameter",
    "known",
    "interval",
    "estimation",
    "right",
    "understandable",
    "basically",
    "trying",
    "see",
    "going",
    "estimate",
    "value",
    "parameter",
    "let",
    "say",
    "trying",
    "find",
    "mean",
    "population",
    "going",
    "going",
    "build",
    "range",
    "value",
    "lie",
    "range",
    "interval",
    "alright",
    "way",
    "output",
    "going",
    "accurate",
    "predicted",
    "point",
    "estimation",
    "instead",
    "estimated",
    "interval",
    "within",
    "value",
    "might",
    "occur",
    "right",
    "okay",
    "image",
    "clearly",
    "shows",
    "point",
    "estimate",
    "interval",
    "estimate",
    "different",
    "guys",
    "interval",
    "estimate",
    "obviously",
    "accurate",
    "focusing",
    "particular",
    "value",
    "particular",
    "point",
    "order",
    "predict",
    "probability",
    "instead",
    "saying",
    "value",
    "might",
    "within",
    "range",
    "lower",
    "confidence",
    "limit",
    "upper",
    "confidence",
    "limit",
    "right",
    "denotes",
    "range",
    "interval",
    "okay",
    "still",
    "confused",
    "interval",
    "estimation",
    "let",
    "give",
    "small",
    "example",
    "stated",
    "take",
    "30",
    "minutes",
    "reach",
    "theater",
    "known",
    "point",
    "estimation",
    "okay",
    "stated",
    "take",
    "45",
    "minutes",
    "hour",
    "reach",
    "theater",
    "example",
    "estimation",
    "right",
    "hope",
    "clear",
    "interval",
    "estimation",
    "gives",
    "rise",
    "two",
    "important",
    "statistical",
    "terminologies",
    "one",
    "known",
    "confidence",
    "interval",
    "known",
    "margin",
    "error",
    "right",
    "important",
    "pay",
    "attention",
    "terminologies",
    "confidence",
    "interval",
    "one",
    "significant",
    "measures",
    "used",
    "check",
    "essential",
    "machine",
    "learning",
    "model",
    "right",
    "confidence",
    "interval",
    "confidence",
    "interval",
    "measure",
    "confidence",
    "interval",
    "estimated",
    "contains",
    "population",
    "parameter",
    "population",
    "mean",
    "parameters",
    "right",
    "statisticians",
    "use",
    "confidence",
    "interval",
    "describe",
    "amount",
    "uncertainty",
    "associated",
    "sample",
    "estimate",
    "population",
    "parameter",
    "guys",
    "lot",
    "definition",
    "let",
    "make",
    "understand",
    "confidence",
    "interval",
    "small",
    "example",
    "okay",
    "let",
    "say",
    "perform",
    "survey",
    "survey",
    "group",
    "cat",
    "owners",
    "see",
    "many",
    "cans",
    "cat",
    "food",
    "purchase",
    "one",
    "year",
    "okay",
    "test",
    "statistics",
    "99",
    "percent",
    "confidence",
    "level",
    "get",
    "confidence",
    "interval",
    "hundred",
    "comma",
    "200",
    "means",
    "think",
    "cat",
    "owners",
    "hundred",
    "two",
    "hundred",
    "cans",
    "year",
    "also",
    "since",
    "confidence",
    "level",
    "99",
    "shows",
    "confident",
    "results",
    "correct",
    "okay",
    "hope",
    "clear",
    "alright",
    "confidence",
    "interval",
    "hundred",
    "two",
    "hundred",
    "confidence",
    "level",
    "99",
    "right",
    "difference",
    "confidence",
    "interval",
    "confidence",
    "level",
    "within",
    "confidence",
    "interval",
    "value",
    "going",
    "lie",
    "confidence",
    "level",
    "show",
    "confident",
    "estimation",
    "right",
    "hope",
    "clear",
    "let",
    "look",
    "margin",
    "error",
    "margin",
    "error",
    "given",
    "level",
    "confidence",
    "greatest",
    "possible",
    "distance",
    "point",
    "estimate",
    "value",
    "parameter",
    "estimating",
    "say",
    "deviation",
    "actual",
    "point",
    "estimate",
    "right",
    "margin",
    "error",
    "calculated",
    "using",
    "formula",
    "zc",
    "denotes",
    "critical",
    "value",
    "confidence",
    "interval",
    "x",
    "standard",
    "deviation",
    "divided",
    "root",
    "sample",
    "size",
    "right",
    "n",
    "basically",
    "sample",
    "size",
    "let",
    "understand",
    "estimate",
    "confidence",
    "intervals",
    "guys",
    "level",
    "confidence",
    "denoted",
    "c",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "let",
    "say",
    "trying",
    "estimate",
    "mean",
    "right",
    "level",
    "confidence",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "interval",
    "minus",
    "z",
    "z",
    "area",
    "beneath",
    "curve",
    "nothing",
    "probability",
    "interval",
    "estimate",
    "contains",
    "population",
    "parameter",
    "right",
    "basically",
    "contain",
    "value",
    "predicting",
    "right",
    "known",
    "critical",
    "values",
    "basically",
    "lower",
    "limit",
    "higher",
    "limit",
    "confidence",
    "level",
    "also",
    "something",
    "known",
    "z",
    "score",
    "court",
    "calculated",
    "using",
    "standard",
    "normal",
    "table",
    "right",
    "look",
    "anywhere",
    "google",
    "find",
    "table",
    "standard",
    "normal",
    "table",
    "understand",
    "done",
    "let",
    "look",
    "small",
    "example",
    "okay",
    "let",
    "say",
    "level",
    "confidence",
    "vince",
    "90",
    "means",
    "90",
    "confident",
    "interval",
    "contains",
    "population",
    "mean",
    "okay",
    "remaining",
    "10",
    "hundred",
    "percent",
    "remaining",
    "10",
    "equally",
    "distributed",
    "tail",
    "regions",
    "okay",
    "right",
    "either",
    "side",
    "see",
    "distribute",
    "leftover",
    "percentage",
    "z",
    "scores",
    "calculated",
    "table",
    "mentioned",
    "right",
    "one",
    "6",
    "4",
    "5",
    "get",
    "collated",
    "standard",
    "normal",
    "table",
    "okay",
    "guys",
    "estimate",
    "level",
    "confidence",
    "sum",
    "let",
    "tell",
    "steps",
    "involved",
    "constructing",
    "confidence",
    "interval",
    "first",
    "would",
    "start",
    "identifying",
    "sample",
    "statistic",
    "okay",
    "statistic",
    "use",
    "estimate",
    "population",
    "parameter",
    "anything",
    "like",
    "mean",
    "sample",
    "next",
    "select",
    "confidence",
    "level",
    "confidence",
    "level",
    "describes",
    "uncertainty",
    "sampling",
    "method",
    "right",
    "find",
    "something",
    "known",
    "margin",
    "error",
    "right",
    "discussed",
    "margin",
    "error",
    "earlier",
    "find",
    "based",
    "equation",
    "explained",
    "previous",
    "slide",
    "finally",
    "specify",
    "confidence",
    "interval",
    "right",
    "let",
    "look",
    "problem",
    "statement",
    "better",
    "understand",
    "concept",
    "random",
    "sample",
    "32",
    "textbook",
    "prices",
    "taken",
    "local",
    "college",
    "bookstore",
    "mean",
    "sample",
    "sample",
    "standard",
    "deviation",
    "use",
    "95",
    "confident",
    "level",
    "find",
    "margin",
    "error",
    "mean",
    "price",
    "text",
    "books",
    "bookstore",
    "okay",
    "straightforward",
    "question",
    "want",
    "read",
    "question",
    "substitute",
    "values",
    "equation",
    "right",
    "guys",
    "know",
    "formula",
    "margin",
    "error",
    "take",
    "z",
    "score",
    "table",
    "deviation",
    "madrid",
    "right",
    "standard",
    "deviation",
    "n",
    "stands",
    "number",
    "samples",
    "number",
    "samples",
    "32",
    "basically",
    "32",
    "textbooks",
    "approximately",
    "margin",
    "error",
    "going",
    "around",
    "pretty",
    "simple",
    "question",
    "right",
    "hope",
    "understood",
    "know",
    "idea",
    "behind",
    "confidence",
    "interval",
    "let",
    "move",
    "ahead",
    "one",
    "important",
    "topics",
    "statistical",
    "inference",
    "hypothesis",
    "testing",
    "right",
    "ugly",
    "statisticians",
    "use",
    "hypothesis",
    "testing",
    "formally",
    "check",
    "whether",
    "hypothesis",
    "accepted",
    "rejected",
    "okay",
    "hypothesis",
    "testing",
    "inferential",
    "statistical",
    "technique",
    "used",
    "determine",
    "whether",
    "enough",
    "evidence",
    "data",
    "sample",
    "infer",
    "certain",
    "condition",
    "holds",
    "true",
    "entire",
    "population",
    "understand",
    "characteristics",
    "general",
    "population",
    "take",
    "random",
    "sample",
    "analyze",
    "properties",
    "sample",
    "right",
    "test",
    "whether",
    "identified",
    "conclusion",
    "represents",
    "population",
    "accurately",
    "finally",
    "interpret",
    "results",
    "whether",
    "accept",
    "hypothesis",
    "depends",
    "upon",
    "percentage",
    "value",
    "get",
    "hypothesis",
    "okay",
    "better",
    "understand",
    "let",
    "look",
    "small",
    "example",
    "steps",
    "followed",
    "hypothesis",
    "testing",
    "begin",
    "stating",
    "null",
    "alternative",
    "hypothesis",
    "right",
    "tell",
    "exactly",
    "terms",
    "formulate",
    "analysis",
    "plan",
    "right",
    "analyze",
    "sample",
    "data",
    "finally",
    "interpret",
    "results",
    "right",
    "understand",
    "entire",
    "hypothesis",
    "testing",
    "look",
    "good",
    "example",
    "okay",
    "consider",
    "boys",
    "nick",
    "harry",
    "boys",
    "caught",
    "bunking",
    "class",
    "asked",
    "stay",
    "back",
    "school",
    "clean",
    "classroom",
    "punishment",
    "right",
    "john",
    "decided",
    "four",
    "would",
    "take",
    "turns",
    "clean",
    "classrooms",
    "came",
    "plan",
    "writing",
    "names",
    "chits",
    "putting",
    "bowl",
    "every",
    "day",
    "pick",
    "name",
    "bowel",
    "person",
    "play",
    "clock",
    "right",
    "sounds",
    "pretty",
    "fair",
    "enough",
    "three",
    "days",
    "everybody",
    "name",
    "come",
    "except",
    "john",
    "assuming",
    "event",
    "completely",
    "random",
    "free",
    "bias",
    "probability",
    "john",
    "cheating",
    "right",
    "probability",
    "actually",
    "cheating",
    "solved",
    "using",
    "hypothesis",
    "testing",
    "okay",
    "begin",
    "calculating",
    "probability",
    "john",
    "picked",
    "day",
    "alright",
    "going",
    "assume",
    "event",
    "free",
    "bias",
    "need",
    "find",
    "probability",
    "john",
    "cheating",
    "right",
    "first",
    "find",
    "probability",
    "john",
    "picked",
    "day",
    "right",
    "get",
    "3",
    "4",
    "basically",
    "75",
    "75",
    "fairly",
    "high",
    "john",
    "picked",
    "three",
    "days",
    "row",
    "probability",
    "drop",
    "approximately",
    "42",
    "okay",
    "three",
    "days",
    "row",
    "meaning",
    "probability",
    "drops",
    "42",
    "percent",
    "let",
    "consider",
    "situation",
    "john",
    "picked",
    "12",
    "days",
    "row",
    "probability",
    "drops",
    "three",
    "point",
    "two",
    "percent",
    "okay",
    "probability",
    "john",
    "cheating",
    "becomes",
    "fairly",
    "high",
    "right",
    "order",
    "statisticians",
    "come",
    "conclusion",
    "define",
    "known",
    "threshold",
    "value",
    "right",
    "considering",
    "situation",
    "threshold",
    "value",
    "set",
    "5",
    "percent",
    "would",
    "indicate",
    "probability",
    "lies",
    "5",
    "john",
    "cheating",
    "way",
    "detention",
    "probability",
    "threshold",
    "value",
    "john",
    "lucky",
    "name",
    "getting",
    "picked",
    "probability",
    "hypothesis",
    "testing",
    "give",
    "rise",
    "two",
    "important",
    "components",
    "hypothesis",
    "testing",
    "null",
    "hypothesis",
    "alternative",
    "hypothesis",
    "null",
    "hypothesis",
    "based",
    "basically",
    "approving",
    "assumption",
    "alternate",
    "hypothesis",
    "result",
    "disapproves",
    "assumption",
    "right",
    "therefore",
    "example",
    "probability",
    "event",
    "occurring",
    "less",
    "5",
    "event",
    "biased",
    "hence",
    "proves",
    "alternate",
    "hypothesis",
    "guys",
    "come",
    "end",
    "session",
    "let",
    "go",
    "ahead",
    "understand",
    "exactly",
    "learning",
    "supervised",
    "learning",
    "input",
    "variable",
    "x",
    "output",
    "variable",
    "use",
    "algorithm",
    "learn",
    "map",
    "egg",
    "function",
    "input",
    "output",
    "mentioned",
    "earlier",
    "example",
    "face",
    "detection",
    "called",
    "supervised",
    "learning",
    "process",
    "algorithm",
    "learning",
    "training",
    "data",
    "set",
    "thought",
    "teacher",
    "supervising",
    "learning",
    "process",
    "look",
    "supervised",
    "learning",
    "steps",
    "would",
    "rather",
    "say",
    "workflow",
    "model",
    "used",
    "see",
    "historic",
    "data",
    "random",
    "sampling",
    "split",
    "data",
    "train",
    "asset",
    "testing",
    "data",
    "set",
    "using",
    "training",
    "data",
    "set",
    "help",
    "machine",
    "learning",
    "supervised",
    "machine",
    "learning",
    "create",
    "statistical",
    "model",
    "mod",
    "generated",
    "help",
    "training",
    "data",
    "set",
    "use",
    "testing",
    "data",
    "set",
    "production",
    "testing",
    "get",
    "output",
    "finally",
    "model",
    "validation",
    "outcome",
    "training",
    "testing",
    "look",
    "prediction",
    "part",
    "particular",
    "supervised",
    "learning",
    "algorithm",
    "model",
    "used",
    "operating",
    "outcome",
    "new",
    "data",
    "set",
    "whenever",
    "performance",
    "model",
    "degraded",
    "model",
    "retrained",
    "performance",
    "issues",
    "model",
    "retained",
    "help",
    "new",
    "data",
    "talk",
    "supervisor",
    "one",
    "quite",
    "algorithms",
    "linear",
    "regression",
    "logistic",
    "regression",
    "entry",
    "random",
    "forest",
    "made",
    "classifiers",
    "linear",
    "regression",
    "used",
    "estimate",
    "real",
    "values",
    "example",
    "cost",
    "houses",
    "number",
    "calls",
    "total",
    "sales",
    "based",
    "continuous",
    "variables",
    "reading",
    "regression",
    "talk",
    "logistic",
    "regression",
    "used",
    "estimate",
    "discrete",
    "values",
    "example",
    "binary",
    "values",
    "like",
    "0",
    "1",
    "yes",
    "true",
    "false",
    "based",
    "given",
    "set",
    "independent",
    "variables",
    "example",
    "talking",
    "something",
    "like",
    "chances",
    "winning",
    "talk",
    "winning",
    "either",
    "true",
    "false",
    "rain",
    "today",
    "yes",
    "like",
    "output",
    "particular",
    "algorithm",
    "particular",
    "question",
    "either",
    "yes",
    "banner",
    "e",
    "use",
    "large",
    "stick",
    "regression",
    "next",
    "decision",
    "trees",
    "used",
    "classification",
    "problems",
    "work",
    "x",
    "categorical",
    "continuous",
    "dependent",
    "variables",
    "talk",
    "random",
    "forest",
    "random",
    "forest",
    "symbol",
    "decision",
    "tree",
    "gives",
    "better",
    "prediction",
    "accuracy",
    "decision",
    "tree",
    "another",
    "type",
    "supervised",
    "learning",
    "algorithm",
    "finally",
    "need",
    "based",
    "classifier",
    "classification",
    "technique",
    "based",
    "bayes",
    "theorem",
    "assumption",
    "independence",
    "predictors",
    "linear",
    "regression",
    "one",
    "easiest",
    "algorithm",
    "machine",
    "learning",
    "statistical",
    "model",
    "attempts",
    "show",
    "relationship",
    "two",
    "variables",
    "linear",
    "equation",
    "drill",
    "linear",
    "regression",
    "algorithm",
    "depth",
    "give",
    "quick",
    "overview",
    "today",
    "agenda",
    "start",
    "session",
    "quick",
    "overview",
    "regression",
    "linear",
    "regression",
    "one",
    "type",
    "regression",
    "algorithm",
    "learn",
    "regression",
    "use",
    "case",
    "various",
    "types",
    "next",
    "learn",
    "algorithm",
    "scratch",
    "teach",
    "mathematical",
    "implementation",
    "first",
    "drill",
    "coding",
    "part",
    "implement",
    "linear",
    "regression",
    "using",
    "python",
    "today",
    "session",
    "deal",
    "linear",
    "regression",
    "algorithm",
    "using",
    "least",
    "square",
    "method",
    "check",
    "goodness",
    "fit",
    "close",
    "data",
    "fitted",
    "regression",
    "line",
    "using",
    "r",
    "square",
    "method",
    "finally",
    "optimize",
    "using",
    "gradient",
    "decent",
    "method",
    "last",
    "part",
    "coding",
    "session",
    "teach",
    "implement",
    "linear",
    "regression",
    "using",
    "python",
    "coding",
    "session",
    "would",
    "divided",
    "two",
    "parts",
    "first",
    "part",
    "would",
    "consist",
    "linear",
    "regression",
    "using",
    "python",
    "scratch",
    "use",
    "mathematical",
    "algorithm",
    "learned",
    "session",
    "next",
    "part",
    "coding",
    "session",
    "using",
    "direct",
    "implementation",
    "linear",
    "regression",
    "let",
    "begin",
    "session",
    "regression",
    "well",
    "regression",
    "analysis",
    "form",
    "predictive",
    "modeling",
    "technique",
    "investigates",
    "relationship",
    "dependent",
    "independent",
    "variable",
    "regression",
    "analysis",
    "vols",
    "graphing",
    "line",
    "set",
    "data",
    "points",
    "closely",
    "fits",
    "overall",
    "shape",
    "data",
    "regression",
    "shows",
    "changes",
    "dependent",
    "variable",
    "changes",
    "explanation",
    "variable",
    "fine",
    "would",
    "ask",
    "uses",
    "regression",
    "well",
    "major",
    "three",
    "uses",
    "regression",
    "analysis",
    "first",
    "determining",
    "strength",
    "predicates",
    "errs",
    "regression",
    "might",
    "used",
    "identify",
    "strength",
    "effect",
    "independent",
    "variables",
    "dependent",
    "variable",
    "ask",
    "question",
    "like",
    "strength",
    "relationship",
    "sales",
    "marketing",
    "spending",
    "relationship",
    "age",
    "income",
    "second",
    "forecasting",
    "effect",
    "regression",
    "used",
    "forecast",
    "effects",
    "impact",
    "changes",
    "regression",
    "analysis",
    "help",
    "us",
    "understand",
    "much",
    "dependent",
    "variable",
    "changes",
    "change",
    "one",
    "independent",
    "variable",
    "fine",
    "example",
    "ask",
    "question",
    "like",
    "much",
    "additional",
    "say",
    "lancome",
    "get",
    "thousand",
    "dollars",
    "spent",
    "marketing",
    "trend",
    "forecasting",
    "regression",
    "analysis",
    "predict",
    "trends",
    "future",
    "values",
    "regression",
    "analysis",
    "used",
    "get",
    "point",
    "estimates",
    "ask",
    "questions",
    "like",
    "price",
    "bitcoin",
    "next",
    "six",
    "months",
    "right",
    "next",
    "topic",
    "linear",
    "versus",
    "logistic",
    "regression",
    "hope",
    "know",
    "regression",
    "let",
    "move",
    "understand",
    "type",
    "various",
    "kinds",
    "regression",
    "like",
    "linear",
    "regression",
    "logistic",
    "regression",
    "polynomial",
    "regression",
    "others",
    "session",
    "focusing",
    "linear",
    "logistic",
    "regression",
    "let",
    "move",
    "let",
    "tell",
    "linear",
    "regression",
    "logistic",
    "regression",
    "compare",
    "right",
    "starting",
    "linear",
    "regression",
    "simple",
    "linear",
    "regression",
    "interested",
    "things",
    "like",
    "equal",
    "mx",
    "plus",
    "trying",
    "find",
    "correlation",
    "x",
    "variable",
    "means",
    "every",
    "value",
    "x",
    "corresponding",
    "value",
    "continuous",
    "right",
    "however",
    "logistic",
    "regression",
    "fitting",
    "data",
    "straight",
    "line",
    "like",
    "linear",
    "regression",
    "instead",
    "mapping",
    "versus",
    "x",
    "sigmoid",
    "function",
    "logistic",
    "regression",
    "find",
    "1",
    "0",
    "particular",
    "value",
    "x",
    "thus",
    "essentially",
    "deciding",
    "true",
    "false",
    "value",
    "given",
    "value",
    "x",
    "fine",
    "core",
    "concept",
    "linear",
    "regression",
    "say",
    "data",
    "modeled",
    "using",
    "straight",
    "case",
    "logistic",
    "regression",
    "data",
    "module",
    "using",
    "sigmoid",
    "function",
    "linear",
    "regression",
    "used",
    "continuous",
    "variables",
    "hand",
    "logistic",
    "regression",
    "used",
    "categorical",
    "variable",
    "output",
    "prediction",
    "linear",
    "regression",
    "value",
    "variable",
    "hand",
    "output",
    "production",
    "logistic",
    "regression",
    "probability",
    "occurrence",
    "event",
    "check",
    "accuracy",
    "goodness",
    "fit",
    "case",
    "linear",
    "regression",
    "various",
    "methods",
    "like",
    "measured",
    "loss",
    "r",
    "square",
    "adjusted",
    "r",
    "squared",
    "etc",
    "case",
    "logistic",
    "regression",
    "accuracy",
    "precision",
    "recall",
    "f1",
    "score",
    "nothing",
    "harmonic",
    "mean",
    "precision",
    "recall",
    "next",
    "roc",
    "curve",
    "determining",
    "probability",
    "threshold",
    "classification",
    "confusion",
    "matrix",
    "etc",
    "many",
    "right",
    "summarizing",
    "difference",
    "linear",
    "logistic",
    "regression",
    "say",
    "type",
    "function",
    "mapping",
    "main",
    "point",
    "difference",
    "linear",
    "logistic",
    "regression",
    "linear",
    "regression",
    "model",
    "continuous",
    "x2",
    "continuous",
    "file",
    "hand",
    "logistic",
    "regression",
    "maps",
    "continuous",
    "x",
    "bindery",
    "use",
    "logistic",
    "regression",
    "make",
    "category",
    "true",
    "false",
    "decisions",
    "data",
    "find",
    "let",
    "move",
    "ahead",
    "next",
    "linear",
    "regression",
    "selection",
    "criteria",
    "say",
    "use",
    "linear",
    "regression",
    "first",
    "classification",
    "regression",
    "capabilities",
    "regression",
    "models",
    "predict",
    "continuous",
    "variable",
    "sales",
    "made",
    "day",
    "predict",
    "temperature",
    "city",
    "reliance",
    "polynomial",
    "like",
    "straight",
    "line",
    "fit",
    "data",
    "set",
    "poses",
    "real",
    "challenge",
    "comes",
    "towards",
    "building",
    "classification",
    "capability",
    "let",
    "imagine",
    "fit",
    "line",
    "train",
    "points",
    "imagine",
    "add",
    "data",
    "points",
    "order",
    "fit",
    "change",
    "existing",
    "model",
    "maybe",
    "change",
    "threshold",
    "happen",
    "new",
    "data",
    "point",
    "model",
    "hence",
    "linear",
    "regression",
    "good",
    "classification",
    "models",
    "fine",
    "next",
    "data",
    "quality",
    "missing",
    "value",
    "removes",
    "one",
    "data",
    "point",
    "could",
    "optimize",
    "regression",
    "simple",
    "linear",
    "regression",
    "outliers",
    "significantly",
    "disrupt",
    "outcome",
    "know",
    "remove",
    "outliers",
    "model",
    "become",
    "good",
    "right",
    "data",
    "quality",
    "next",
    "computational",
    "complexity",
    "linear",
    "regression",
    "often",
    "computationally",
    "expensive",
    "compared",
    "decision",
    "tree",
    "clustering",
    "algorithm",
    "order",
    "complexity",
    "n",
    "training",
    "example",
    "x",
    "features",
    "usually",
    "falls",
    "either",
    "big",
    "x",
    "bigger",
    "xn",
    "next",
    "comprehensible",
    "transparent",
    "linear",
    "regression",
    "easily",
    "comprehensible",
    "transparent",
    "nature",
    "represented",
    "simple",
    "mathematical",
    "notation",
    "anyone",
    "understood",
    "easily",
    "criteria",
    "based",
    "select",
    "linear",
    "regression",
    "algorithm",
    "right",
    "next",
    "linear",
    "regression",
    "used",
    "first",
    "evaluating",
    "trans",
    "sales",
    "estimate",
    "well",
    "linear",
    "regression",
    "used",
    "business",
    "evaluate",
    "trends",
    "make",
    "estimates",
    "forecast",
    "example",
    "company",
    "sales",
    "increased",
    "steadily",
    "every",
    "month",
    "past",
    "years",
    "conducting",
    "linear",
    "analysis",
    "sales",
    "data",
    "monthly",
    "sales",
    "axis",
    "time",
    "x",
    "axis",
    "give",
    "line",
    "predicts",
    "upward",
    "trends",
    "sale",
    "creating",
    "trendline",
    "company",
    "could",
    "use",
    "slope",
    "lines",
    "focused",
    "sale",
    "future",
    "months",
    "next",
    "analyzing",
    "impact",
    "price",
    "changes",
    "linear",
    "regression",
    "used",
    "analyze",
    "effect",
    "pricing",
    "omer",
    "behavior",
    "instance",
    "company",
    "changes",
    "price",
    "certain",
    "product",
    "several",
    "times",
    "record",
    "quantity",
    "price",
    "level",
    "perform",
    "linear",
    "regression",
    "sold",
    "quantity",
    "dependent",
    "variable",
    "price",
    "independent",
    "variable",
    "would",
    "result",
    "line",
    "depicts",
    "extent",
    "customer",
    "reduce",
    "consumption",
    "product",
    "prices",
    "increasing",
    "result",
    "would",
    "help",
    "us",
    "future",
    "pricing",
    "decisions",
    "next",
    "assessment",
    "risk",
    "financial",
    "services",
    "insurance",
    "domain",
    "linear",
    "regression",
    "used",
    "analyze",
    "risk",
    "example",
    "health",
    "insurance",
    "company",
    "might",
    "conduct",
    "linear",
    "regression",
    "algorithm",
    "plotting",
    "number",
    "claims",
    "per",
    "customer",
    "age",
    "might",
    "discover",
    "old",
    "customers",
    "tend",
    "make",
    "health",
    "insurance",
    "claim",
    "well",
    "result",
    "analysis",
    "might",
    "guide",
    "important",
    "business",
    "decisions",
    "right",
    "rough",
    "idea",
    "linear",
    "regression",
    "algorithm",
    "like",
    "used",
    "use",
    "early",
    "let",
    "move",
    "understand",
    "algorithm",
    "depth",
    "suppose",
    "independent",
    "variable",
    "dependent",
    "variable",
    "right",
    "suppose",
    "data",
    "point",
    "x",
    "axis",
    "independent",
    "variable",
    "increasing",
    "dependent",
    "variable",
    "kind",
    "linear",
    "regression",
    "line",
    "would",
    "get",
    "would",
    "get",
    "positive",
    "linear",
    "regression",
    "line",
    "right",
    "slope",
    "would",
    "positive",
    "next",
    "suppose",
    "independent",
    "variable",
    "x",
    "axis",
    "increasing",
    "hand",
    "dependent",
    "variable",
    "decreasing",
    "kind",
    "line",
    "get",
    "case",
    "get",
    "negative",
    "regression",
    "line",
    "case",
    "slope",
    "line",
    "negative",
    "particular",
    "line",
    "line",
    "equal",
    "mx",
    "plus",
    "c",
    "line",
    "linear",
    "regression",
    "shows",
    "relationship",
    "independent",
    "variable",
    "dependent",
    "variable",
    "line",
    "known",
    "line",
    "linear",
    "regression",
    "okay",
    "let",
    "add",
    "data",
    "points",
    "graph",
    "observation",
    "data",
    "points",
    "graph",
    "let",
    "plot",
    "okay",
    "data",
    "points",
    "plotted",
    "task",
    "create",
    "regression",
    "line",
    "best",
    "fit",
    "line",
    "right",
    "regression",
    "line",
    "drawn",
    "task",
    "production",
    "suppose",
    "estimated",
    "value",
    "predicted",
    "value",
    "actual",
    "value",
    "okay",
    "main",
    "goal",
    "reduce",
    "error",
    "reduce",
    "distance",
    "estimated",
    "predicted",
    "value",
    "actual",
    "value",
    "best",
    "fit",
    "line",
    "would",
    "one",
    "least",
    "error",
    "least",
    "difference",
    "estimated",
    "value",
    "actual",
    "value",
    "right",
    "words",
    "minimize",
    "error",
    "brief",
    "understanding",
    "linear",
    "regression",
    "algorithm",
    "soon",
    "jump",
    "towards",
    "mathematical",
    "implementation",
    "let",
    "tell",
    "suppose",
    "draw",
    "graph",
    "speed",
    "distance",
    "covered",
    "axis",
    "time",
    "domain",
    "constant",
    "plot",
    "graph",
    "speed",
    "travel",
    "vehicle",
    "distance",
    "traveled",
    "fixed",
    "unit",
    "time",
    "get",
    "positive",
    "relationship",
    "right",
    "suppose",
    "equation",
    "line",
    "equal",
    "mx",
    "plus",
    "case",
    "distance",
    "traveled",
    "fixed",
    "duration",
    "time",
    "x",
    "speed",
    "vehicle",
    "positive",
    "slope",
    "line",
    "see",
    "line",
    "right",
    "suppose",
    "distance",
    "remaining",
    "constant",
    "plot",
    "graph",
    "speed",
    "vehicle",
    "time",
    "taken",
    "travel",
    "fixed",
    "distance",
    "case",
    "get",
    "line",
    "negative",
    "relationship",
    "right",
    "slope",
    "line",
    "negative",
    "equation",
    "line",
    "changes",
    "equal",
    "minus",
    "mx",
    "plus",
    "c",
    "time",
    "taken",
    "travel",
    "fixed",
    "distance",
    "x",
    "speed",
    "vehicle",
    "negative",
    "slope",
    "line",
    "see",
    "line",
    "right",
    "let",
    "get",
    "back",
    "independent",
    "dependent",
    "variable",
    "term",
    "dependent",
    "variable",
    "x",
    "independent",
    "variable",
    "let",
    "move",
    "see",
    "magical",
    "implementation",
    "things",
    "alright",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "5",
    "let",
    "plot",
    "0",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "align",
    "3",
    "4",
    "2",
    "4",
    "right",
    "let",
    "plot",
    "1",
    "2",
    "3",
    "4",
    "5",
    "let",
    "plot",
    "coordinates",
    "1",
    "1",
    "x",
    "equal",
    "1",
    "equal",
    "3",
    "x",
    "equal",
    "1",
    "equal",
    "3",
    "point",
    "1",
    "comma",
    "3",
    "similarly",
    "1",
    "3",
    "2",
    "4",
    "3",
    "2",
    "4",
    "4",
    "5",
    "alright",
    "moving",
    "ahead",
    "let",
    "calculate",
    "mean",
    "x",
    "plot",
    "graph",
    "right",
    "mean",
    "x",
    "1",
    "plus",
    "2",
    "plus",
    "3",
    "plus",
    "4",
    "plus",
    "5",
    "divided",
    "right",
    "similarly",
    "mean",
    "3",
    "plus",
    "4",
    "plus",
    "2",
    "plus",
    "4",
    "plus",
    "5",
    "10",
    "divided",
    "nothing",
    "alright",
    "next",
    "plot",
    "mean",
    "3",
    "comma",
    "3",
    "graph",
    "okay",
    "point",
    "3",
    "comma",
    "3",
    "see",
    "goal",
    "find",
    "predict",
    "best",
    "fit",
    "line",
    "using",
    "least",
    "square",
    "method",
    "right",
    "order",
    "find",
    "first",
    "need",
    "find",
    "equation",
    "line",
    "let",
    "find",
    "equation",
    "regression",
    "line",
    "alright",
    "let",
    "suppose",
    "regression",
    "line",
    "equal",
    "mx",
    "plus",
    "equation",
    "line",
    "need",
    "find",
    "value",
    "wear",
    "equals",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "upon",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "get",
    "confused",
    "let",
    "resolve",
    "right",
    "moving",
    "ahead",
    "part",
    "formula",
    "going",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "1",
    "minus",
    "x",
    "bar",
    "3",
    "1",
    "minus",
    "3",
    "minus",
    "2",
    "next",
    "x",
    "equal",
    "minus",
    "mean",
    "3",
    "minus",
    "1",
    "similarly",
    "3",
    "3",
    "0",
    "4",
    "minus",
    "3",
    "1",
    "5",
    "3",
    "right",
    "x",
    "minus",
    "x",
    "bar",
    "nothing",
    "distance",
    "point",
    "line",
    "equal",
    "3",
    "minus",
    "bar",
    "implies",
    "implies",
    "distance",
    "point",
    "line",
    "x",
    "equal",
    "3",
    "fine",
    "let",
    "calculate",
    "value",
    "minus",
    "bar",
    "starting",
    "equal",
    "3",
    "value",
    "bar",
    "three",
    "minus",
    "three",
    "much",
    "next",
    "4",
    "minus",
    "next",
    "minus",
    "next",
    "4",
    "minus",
    "5",
    "minus",
    "alright",
    "done",
    "minus",
    "bar",
    "fine",
    "next",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "let",
    "calculate",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "2",
    "whole",
    "square",
    "4",
    "minus",
    "1",
    "whole",
    "square",
    "1",
    "0",
    "squared",
    "0",
    "1",
    "square",
    "1",
    "2",
    "square",
    "fine",
    "table",
    "x",
    "minus",
    "x",
    "bar",
    "minus",
    "bar",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "need",
    "need",
    "product",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "alright",
    "let",
    "see",
    "product",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "minus",
    "2",
    "x",
    "minus",
    "minus",
    "1",
    "x",
    "0",
    "point",
    "minus",
    "zero",
    "point",
    "4",
    "0",
    "x",
    "minus",
    "0",
    "1",
    "multiplied",
    "zero",
    "point",
    "four",
    "next",
    "2",
    "multiplied",
    "1",
    "point",
    "right",
    "almost",
    "parts",
    "formula",
    "done",
    "need",
    "get",
    "summation",
    "last",
    "two",
    "columns",
    "right",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "whole",
    "square",
    "10",
    "summation",
    "x",
    "minus",
    "x",
    "bar",
    "x",
    "minus",
    "bar",
    "value",
    "equal",
    "4",
    "10",
    "fine",
    "let",
    "put",
    "value",
    "equals",
    "zero",
    "point",
    "4",
    "line",
    "equal",
    "mx",
    "plus",
    "let",
    "file",
    "points",
    "equation",
    "find",
    "value",
    "remember",
    "mean",
    "calculated",
    "x",
    "mean",
    "value",
    "x",
    "3",
    "equation",
    "3",
    "point",
    "6",
    "equals",
    "0",
    "applied",
    "3",
    "plus",
    "alright",
    "equal",
    "1",
    "point",
    "2",
    "plus",
    "value",
    "c",
    "minus",
    "right",
    "equals",
    "zero",
    "point",
    "four",
    "c",
    "finally",
    "calculate",
    "equation",
    "regression",
    "line",
    "get",
    "equal",
    "zero",
    "point",
    "four",
    "times",
    "x",
    "plus",
    "two",
    "point",
    "four",
    "regression",
    "line",
    "right",
    "plotting",
    "points",
    "actual",
    "point",
    "right",
    "given",
    "equals",
    "zero",
    "point",
    "four",
    "sql",
    "let",
    "predict",
    "value",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "x",
    "equal",
    "1",
    "predicted",
    "value",
    "zero",
    "point",
    "four",
    "x",
    "one",
    "plus",
    "two",
    "point",
    "four",
    "similarly",
    "x",
    "equal",
    "predicted",
    "value",
    "zero",
    "point",
    "4",
    "x",
    "2",
    "2",
    "point",
    "4",
    "equals",
    "3",
    "point",
    "2",
    "similarly",
    "x",
    "equal",
    "3",
    "3",
    "x",
    "equals",
    "4",
    "4",
    "point",
    "0",
    "x",
    "equal",
    "5",
    "four",
    "point",
    "four",
    "let",
    "plot",
    "graph",
    "line",
    "passing",
    "predicting",
    "point",
    "cutting",
    "line",
    "regression",
    "task",
    "calculate",
    "distance",
    "actual",
    "predicted",
    "value",
    "job",
    "reduce",
    "distance",
    "right",
    "words",
    "reduce",
    "error",
    "actual",
    "predicted",
    "value",
    "line",
    "least",
    "error",
    "line",
    "linear",
    "regression",
    "chicken",
    "regression",
    "line",
    "also",
    "best",
    "fit",
    "line",
    "right",
    "things",
    "work",
    "computer",
    "performs",
    "n",
    "number",
    "iteration",
    "different",
    "values",
    "different",
    "values",
    "calculate",
    "equation",
    "line",
    "equals",
    "mx",
    "plus",
    "right",
    "value",
    "changes",
    "line",
    "changing",
    "iteration",
    "start",
    "one",
    "right",
    "perform",
    "number",
    "iteration",
    "every",
    "iteration",
    "calculate",
    "predicted",
    "value",
    "according",
    "line",
    "compare",
    "distance",
    "actual",
    "value",
    "predicted",
    "value",
    "value",
    "distance",
    "actual",
    "predicted",
    "value",
    "minimum",
    "selected",
    "best",
    "fit",
    "line",
    "right",
    "calculated",
    "best",
    "fit",
    "line",
    "time",
    "check",
    "goodness",
    "fit",
    "check",
    "good",
    "model",
    "performing",
    "order",
    "method",
    "called",
    "r",
    "square",
    "method",
    "r",
    "square",
    "well",
    "value",
    "statistical",
    "measure",
    "close",
    "data",
    "fitted",
    "regression",
    "line",
    "general",
    "considered",
    "high",
    "value",
    "model",
    "good",
    "model",
    "also",
    "lower",
    "squared",
    "value",
    "good",
    "model",
    "well",
    "higher",
    "squared",
    "value",
    "model",
    "fit",
    "like",
    "also",
    "known",
    "coefficient",
    "determination",
    "coefficient",
    "multiple",
    "determination",
    "let",
    "move",
    "see",
    "square",
    "calculated",
    "actual",
    "values",
    "plotted",
    "graph",
    "calculated",
    "predicted",
    "values",
    "remember",
    "calculated",
    "predicted",
    "values",
    "equation",
    "predicted",
    "equals",
    "0",
    "1",
    "4",
    "x",
    "x",
    "plus",
    "two",
    "point",
    "four",
    "every",
    "x",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "5",
    "got",
    "ed",
    "values",
    "phi",
    "right",
    "let",
    "plot",
    "graph",
    "point",
    "line",
    "passing",
    "points",
    "nothing",
    "regression",
    "line",
    "right",
    "need",
    "check",
    "compare",
    "distance",
    "actual",
    "mean",
    "versus",
    "distance",
    "predicted",
    "mean",
    "alike",
    "basically",
    "calculating",
    "distance",
    "actual",
    "value",
    "mean",
    "distance",
    "predicted",
    "value",
    "mean",
    "like",
    "nothing",
    "square",
    "mathematically",
    "represent",
    "school",
    "whereas",
    "summation",
    "predicted",
    "values",
    "minus",
    "bar",
    "whole",
    "square",
    "divided",
    "summation",
    "minus",
    "bar",
    "whole",
    "square",
    "actual",
    "value",
    "p",
    "predicted",
    "value",
    "bar",
    "mean",
    "value",
    "nothing",
    "remember",
    "formula",
    "next",
    "calculate",
    "minus",
    "y1",
    "3y",
    "bar",
    "3",
    "point",
    "6",
    "calculate",
    "3",
    "minus",
    "nothing",
    "minus",
    "similarly",
    "equal",
    "4",
    "bar",
    "equal",
    "minus",
    "bar",
    "zero",
    "point",
    "4",
    "2",
    "minus",
    "1",
    "point",
    "6",
    "4",
    "minus",
    "zero",
    "point",
    "four",
    "five",
    "minus",
    "got",
    "value",
    "minus",
    "bar",
    "take",
    "square",
    "minus",
    "square",
    "square",
    "square",
    "square",
    "squared",
    "part",
    "formula",
    "need",
    "need",
    "yp",
    "minus",
    "bar",
    "value",
    "vip",
    "values",
    "subtract",
    "2",
    "minus",
    "minus",
    "similarly",
    "get",
    "minus",
    "minus",
    "0",
    "1",
    "0",
    "minus",
    "4",
    "minus",
    "calculated",
    "value",
    "yp",
    "minus",
    "bar",
    "turn",
    "calculate",
    "value",
    "b",
    "minus",
    "bar",
    "whole",
    "square",
    "next",
    "square",
    "point",
    "four",
    "square",
    "square",
    "0",
    "0",
    "point",
    "4",
    "square",
    "square",
    "right",
    "part",
    "formula",
    "suggests",
    "suggests",
    "take",
    "summation",
    "p",
    "minus",
    "bar",
    "whole",
    "square",
    "summation",
    "minus",
    "bar",
    "whole",
    "square",
    "right",
    "let",
    "see",
    "submitting",
    "minus",
    "bar",
    "whole",
    "square",
    "get",
    "five",
    "point",
    "two",
    "summation",
    "p",
    "minus",
    "bar",
    "whole",
    "square",
    "get",
    "one",
    "point",
    "six",
    "value",
    "r",
    "square",
    "calculated",
    "1",
    "point",
    "6",
    "upon",
    "fine",
    "result",
    "get",
    "approximately",
    "equal",
    "well",
    "good",
    "fit",
    "right",
    "suggests",
    "data",
    "points",
    "far",
    "away",
    "regression",
    "line",
    "alright",
    "graph",
    "look",
    "like",
    "r",
    "square",
    "increase",
    "value",
    "r",
    "square",
    "see",
    "actual",
    "value",
    "would",
    "like",
    "closer",
    "regression",
    "line",
    "reaches",
    "comes",
    "clothes",
    "value",
    "approximately",
    "equals",
    "1",
    "actual",
    "values",
    "lies",
    "regression",
    "line",
    "example",
    "case",
    "get",
    "low",
    "value",
    "r",
    "square",
    "suppose",
    "case",
    "see",
    "actual",
    "values",
    "far",
    "away",
    "regression",
    "line",
    "say",
    "many",
    "outliers",
    "data",
    "focus",
    "thing",
    "data",
    "right",
    "calculation",
    "square",
    "might",
    "get",
    "question",
    "like",
    "low",
    "values",
    "square",
    "always",
    "bad",
    "well",
    "field",
    "entirely",
    "expected",
    "ask",
    "value",
    "low",
    "example",
    "field",
    "attempts",
    "predict",
    "human",
    "behavior",
    "psychology",
    "typically",
    "values",
    "lower",
    "around",
    "50",
    "conclude",
    "humans",
    "simply",
    "harder",
    "predict",
    "physical",
    "process",
    "furthermore",
    "ask",
    "value",
    "low",
    "statistically",
    "significant",
    "predictors",
    "still",
    "draw",
    "important",
    "conclusion",
    "changes",
    "predicator",
    "values",
    "associated",
    "created",
    "changes",
    "response",
    "value",
    "regardless",
    "significant",
    "coefficient",
    "still",
    "represent",
    "mean",
    "change",
    "response",
    "one",
    "unit",
    "change",
    "predicator",
    "holding",
    "predicated",
    "model",
    "constant",
    "obviously",
    "type",
    "information",
    "extremely",
    "valuable",
    "right",
    "right",
    "theoretical",
    "concept",
    "let",
    "move",
    "coding",
    "part",
    "understand",
    "code",
    "depth",
    "implementing",
    "linear",
    "regression",
    "using",
    "python",
    "using",
    "anaconda",
    "jupyter",
    "notebook",
    "installed",
    "like",
    "jupyter",
    "notebook",
    "using",
    "python",
    "alright",
    "going",
    "use",
    "data",
    "set",
    "consisting",
    "head",
    "size",
    "human",
    "brain",
    "different",
    "people",
    "right",
    "let",
    "import",
    "data",
    "set",
    "percent",
    "matplotlib",
    "line",
    "importing",
    "numpy",
    "np",
    "pandas",
    "speedy",
    "matplotlib",
    "matplotlib",
    "importing",
    "pipe",
    "lot",
    "plt",
    "alright",
    "next",
    "import",
    "data",
    "brain",
    "dot",
    "csv",
    "store",
    "database",
    "table",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "armor",
    "task",
    "symbol",
    "symbolizes",
    "still",
    "executing",
    "output",
    "data",
    "set",
    "consists",
    "two",
    "thirty",
    "seven",
    "rows",
    "4",
    "columns",
    "columns",
    "gender",
    "age",
    "range",
    "head",
    "size",
    "centimeter",
    "cube",
    "brain",
    "weights",
    "graham",
    "fine",
    "sample",
    "data",
    "set",
    "looks",
    "consists",
    "data",
    "set",
    "imported",
    "data",
    "see",
    "237",
    "values",
    "training",
    "set",
    "find",
    "linear",
    "relationship",
    "head",
    "size",
    "brain",
    "weights",
    "collect",
    "x",
    "x",
    "would",
    "consist",
    "head",
    "size",
    "values",
    "would",
    "consist",
    "brain",
    "values",
    "collecting",
    "x",
    "let",
    "execute",
    "run",
    "done",
    "next",
    "need",
    "find",
    "values",
    "b",
    "1",
    "b",
    "say",
    "need",
    "mean",
    "x",
    "values",
    "first",
    "calculate",
    "mean",
    "x",
    "mean",
    "x",
    "equal",
    "np",
    "dot",
    "min",
    "mean",
    "predefined",
    "function",
    "numb",
    "similarly",
    "mean",
    "underscore",
    "equal",
    "np",
    "dot",
    "mean",
    "return",
    "return",
    "mean",
    "values",
    "next",
    "check",
    "total",
    "number",
    "values",
    "equals",
    "well",
    "length",
    "alright",
    "use",
    "formula",
    "calculate",
    "values",
    "b",
    "1",
    "b",
    "naught",
    "mnc",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "result",
    "see",
    "screen",
    "got",
    "1",
    "0",
    "point",
    "2",
    "6",
    "3",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "alright",
    "coefficient",
    "comparing",
    "equation",
    "equal",
    "mx",
    "plus",
    "say",
    "brain",
    "weight",
    "equals",
    "zero",
    "point",
    "2",
    "6",
    "3",
    "x",
    "head",
    "size",
    "plus",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "say",
    "value",
    "zero",
    "point",
    "2",
    "6",
    "3",
    "value",
    "three",
    "twenty",
    "five",
    "point",
    "five",
    "seven",
    "right",
    "linear",
    "model",
    "let",
    "plot",
    "see",
    "graphically",
    "let",
    "execute",
    "plot",
    "looks",
    "like",
    "model",
    "bad",
    "need",
    "find",
    "good",
    "model",
    "order",
    "find",
    "many",
    "methods",
    "like",
    "root",
    "mean",
    "square",
    "method",
    "coefficient",
    "determination",
    "square",
    "method",
    "tutorial",
    "told",
    "score",
    "method",
    "let",
    "focus",
    "see",
    "good",
    "model",
    "let",
    "calculate",
    "r",
    "square",
    "value",
    "right",
    "ss",
    "underscore",
    "total",
    "sum",
    "square",
    "ss",
    "total",
    "sum",
    "square",
    "residuals",
    "r",
    "square",
    "formula",
    "1",
    "minus",
    "total",
    "sum",
    "squares",
    "upon",
    "total",
    "sum",
    "square",
    "residuals",
    "right",
    "next",
    "execute",
    "get",
    "value",
    "r",
    "square",
    "pretty",
    "good",
    "implemented",
    "simple",
    "linear",
    "regression",
    "model",
    "using",
    "least",
    "square",
    "method",
    "let",
    "move",
    "see",
    "implement",
    "model",
    "using",
    "machine",
    "learning",
    "library",
    "called",
    "right",
    "simple",
    "machine",
    "owning",
    "library",
    "python",
    "welding",
    "machine",
    "learning",
    "model",
    "easy",
    "using",
    "suppose",
    "python",
    "code",
    "using",
    "libraries",
    "code",
    "shortens",
    "length",
    "like",
    "let",
    "execute",
    "run",
    "button",
    "see",
    "get",
    "score",
    "today",
    "discussing",
    "logistic",
    "regression",
    "let",
    "move",
    "forward",
    "understand",
    "logistic",
    "regression",
    "algorithm",
    "widely",
    "used",
    "dependent",
    "variable",
    "see",
    "output",
    "binary",
    "format",
    "need",
    "predict",
    "outcome",
    "categorical",
    "dependent",
    "variable",
    "outcome",
    "always",
    "discreet",
    "categorical",
    "nature",
    "discrete",
    "mean",
    "value",
    "binary",
    "say",
    "two",
    "values",
    "either",
    "0",
    "1",
    "either",
    "yes",
    "either",
    "true",
    "false",
    "high",
    "low",
    "outcomes",
    "value",
    "need",
    "protect",
    "discrete",
    "say",
    "categorical",
    "nature",
    "whereas",
    "linear",
    "regression",
    "value",
    "say",
    "value",
    "two",
    "predictors",
    "range",
    "difference",
    "linear",
    "regression",
    "logistic",
    "regression",
    "must",
    "question",
    "linear",
    "regression",
    "guys",
    "linear",
    "regression",
    "value",
    "buyer",
    "value",
    "need",
    "predict",
    "range",
    "case",
    "logistic",
    "regression",
    "two",
    "values",
    "either",
    "0",
    "one",
    "entertain",
    "values",
    "zero",
    "one",
    "linear",
    "regression",
    "value",
    "range",
    "order",
    "implement",
    "logic",
    "regression",
    "need",
    "clip",
    "part",
    "need",
    "value",
    "zero",
    "need",
    "value",
    "1",
    "since",
    "value",
    "0",
    "1",
    "main",
    "rule",
    "logistic",
    "regression",
    "linear",
    "line",
    "clipped",
    "zero",
    "one",
    "clip",
    "graph",
    "would",
    "look",
    "somewhat",
    "like",
    "getting",
    "curve",
    "nothing",
    "three",
    "different",
    "straight",
    "lines",
    "need",
    "make",
    "new",
    "way",
    "solve",
    "problem",
    "formulated",
    "equation",
    "hence",
    "come",
    "logistic",
    "regression",
    "outcome",
    "either",
    "0",
    "main",
    "rule",
    "logistic",
    "regression",
    "resulting",
    "curve",
    "formulated",
    "hence",
    "main",
    "aim",
    "bring",
    "values",
    "0",
    "1",
    "fulfilled",
    "came",
    "large",
    "stick",
    "regression",
    "gets",
    "formulated",
    "equation",
    "looks",
    "somewhat",
    "like",
    "guys",
    "nothing",
    "curve",
    "say",
    "sigmoid",
    "curve",
    "sigmoid",
    "function",
    "curve",
    "sigmoid",
    "function",
    "basically",
    "converts",
    "value",
    "minus",
    "infinity",
    "infinity",
    "pure",
    "discrete",
    "values",
    "logitech",
    "regression",
    "wants",
    "say",
    "values",
    "binary",
    "format",
    "either",
    "0",
    "see",
    "values",
    "either",
    "0",
    "1",
    "nothing",
    "transition",
    "guys",
    "catch",
    "let",
    "say",
    "data",
    "point",
    "decide",
    "whether",
    "value",
    "0",
    "1",
    "concept",
    "threshold",
    "basically",
    "divides",
    "line",
    "threshold",
    "value",
    "basically",
    "indicates",
    "probability",
    "either",
    "winning",
    "losing",
    "winning",
    "mean",
    "value",
    "equals",
    "losing",
    "mean",
    "values",
    "equal",
    "0",
    "let",
    "data",
    "point",
    "let",
    "say",
    "cursor",
    "check",
    "whether",
    "value",
    "less",
    "threshold",
    "value",
    "let",
    "say",
    "threshold",
    "value",
    "give",
    "result",
    "1",
    "less",
    "give",
    "result",
    "zero",
    "threshold",
    "value",
    "need",
    "define",
    "value",
    "let",
    "value",
    "rounded",
    "let",
    "see",
    "less",
    "let",
    "value",
    "reduce",
    "zero",
    "use",
    "concept",
    "threshold",
    "value",
    "find",
    "output",
    "discreet",
    "either",
    "0",
    "one",
    "hope",
    "caught",
    "curve",
    "logistic",
    "regression",
    "guys",
    "sigmoid",
    "curve",
    "make",
    "curve",
    "need",
    "make",
    "equation",
    "let",
    "address",
    "part",
    "well",
    "let",
    "see",
    "equation",
    "formed",
    "imitate",
    "functionality",
    "equation",
    "straight",
    "line",
    "equal",
    "mx",
    "plus",
    "case",
    "one",
    "independent",
    "variable",
    "let",
    "say",
    "many",
    "independent",
    "variable",
    "equation",
    "becomes",
    "1",
    "x",
    "1",
    "plus",
    "2",
    "x",
    "2",
    "plus",
    "3",
    "x",
    "3",
    "till",
    "nx",
    "n",
    "let",
    "us",
    "put",
    "b",
    "equation",
    "becomes",
    "equal",
    "b",
    "1",
    "x",
    "1",
    "plus",
    "beta",
    "2",
    "x",
    "2",
    "plus",
    "b",
    "3",
    "x",
    "3",
    "till",
    "nxn",
    "plus",
    "guys",
    "equation",
    "straight",
    "line",
    "range",
    "minus",
    "infinity",
    "infinity",
    "case",
    "say",
    "largest",
    "equation",
    "value",
    "need",
    "predict",
    "say",
    "value",
    "range",
    "0",
    "case",
    "need",
    "transform",
    "equation",
    "done",
    "divide",
    "equation",
    "1",
    "minus",
    "equal",
    "0",
    "0",
    "1",
    "minus",
    "0",
    "equal",
    "1",
    "0",
    "1",
    "0",
    "take",
    "equals",
    "1",
    "1",
    "1",
    "minus",
    "1",
    "0",
    "1",
    "0",
    "infinity",
    "range",
    "know",
    "infinity",
    "want",
    "range",
    "minus",
    "infinity",
    "infinity",
    "log",
    "equation",
    "let",
    "go",
    "ahead",
    "logarithmic",
    "equation",
    "transform",
    "get",
    "range",
    "minus",
    "infinity",
    "infinity",
    "log",
    "1",
    "minus",
    "1",
    "final",
    "logistic",
    "regression",
    "equation",
    "guys",
    "worry",
    "write",
    "formula",
    "memorize",
    "formula",
    "python",
    "need",
    "call",
    "function",
    "logistic",
    "regression",
    "everything",
    "automatically",
    "want",
    "scare",
    "maths",
    "formulas",
    "behind",
    "always",
    "good",
    "know",
    "formula",
    "generated",
    "hope",
    "guys",
    "clear",
    "logistic",
    "regression",
    "comes",
    "picture",
    "next",
    "let",
    "us",
    "see",
    "major",
    "differences",
    "linear",
    "regression",
    "logistic",
    "regression",
    "first",
    "linear",
    "regression",
    "value",
    "continuous",
    "variable",
    "variable",
    "need",
    "predict",
    "continuous",
    "nature",
    "whereas",
    "logistic",
    "regression",
    "categorical",
    "variable",
    "value",
    "need",
    "predict",
    "creating",
    "nature",
    "either",
    "0",
    "1",
    "two",
    "values",
    "example",
    "whether",
    "raining",
    "raining",
    "humid",
    "outside",
    "humid",
    "outside",
    "going",
    "snow",
    "going",
    "snow",
    "example",
    "need",
    "predict",
    "values",
    "discrete",
    "predict",
    "whether",
    "happening",
    "next",
    "linear",
    "equation",
    "solves",
    "regression",
    "problems",
    "concept",
    "independent",
    "variable",
    "dependent",
    "variable",
    "calculate",
    "value",
    "need",
    "predict",
    "using",
    "x",
    "variable",
    "see",
    "value",
    "need",
    "predict",
    "range",
    "whereas",
    "logistic",
    "regression",
    "discrete",
    "values",
    "logistic",
    "regression",
    "basically",
    "solves",
    "classification",
    "problem",
    "basically",
    "classify",
    "give",
    "result",
    "whether",
    "event",
    "happening",
    "hope",
    "pretty",
    "much",
    "clear",
    "till",
    "next",
    "linear",
    "equation",
    "graph",
    "seen",
    "straight",
    "line",
    "graph",
    "calculate",
    "value",
    "respect",
    "value",
    "x",
    "logistic",
    "regression",
    "got",
    "escobar",
    "see",
    "sigmoid",
    "curve",
    "using",
    "sigmoid",
    "function",
    "predict",
    "moving",
    "let",
    "us",
    "see",
    "various",
    "use",
    "cases",
    "logistic",
    "regression",
    "implemented",
    "real",
    "life",
    "first",
    "weather",
    "prediction",
    "largest",
    "aggression",
    "helps",
    "predict",
    "weather",
    "example",
    "used",
    "predict",
    "whether",
    "raining",
    "whether",
    "sunny",
    "cloudy",
    "things",
    "predicted",
    "using",
    "logistic",
    "regression",
    "need",
    "keep",
    "mind",
    "linear",
    "regression",
    "logistic",
    "regression",
    "used",
    "predicting",
    "weather",
    "case",
    "linear",
    "equation",
    "helps",
    "predict",
    "temperature",
    "tomorrow",
    "whereas",
    "logistic",
    "regression",
    "tell",
    "going",
    "rain",
    "whether",
    "cloudy",
    "going",
    "snow",
    "values",
    "discrete",
    "whereas",
    "apply",
    "linear",
    "regression",
    "predicting",
    "things",
    "like",
    "temperature",
    "tomorrow",
    "temperature",
    "day",
    "tomorrow",
    "thing",
    "slight",
    "differences",
    "linear",
    "regression",
    "logistic",
    "regression",
    "moving",
    "ahead",
    "classification",
    "problem",
    "sighs",
    "performs",
    "classification",
    "help",
    "tell",
    "whether",
    "bird",
    "bird",
    "classify",
    "different",
    "kind",
    "mammals",
    "let",
    "say",
    "whether",
    "dog",
    "dog",
    "similarly",
    "check",
    "reptile",
    "whether",
    "reptile",
    "reptile",
    "logistic",
    "regression",
    "perform",
    "classification",
    "point",
    "already",
    "discussed",
    "used",
    "classification",
    "problems",
    "next",
    "also",
    "helps",
    "determine",
    "illness",
    "well",
    "let",
    "take",
    "example",
    "let",
    "say",
    "patient",
    "goes",
    "routine",
    "check",
    "hospital",
    "doctor",
    "perform",
    "various",
    "tests",
    "patient",
    "check",
    "whether",
    "patient",
    "actually",
    "l",
    "features",
    "doctor",
    "check",
    "sugar",
    "level",
    "blood",
    "pressure",
    "age",
    "patient",
    "small",
    "old",
    "person",
    "previous",
    "medical",
    "history",
    "patient",
    "features",
    "recorded",
    "doctor",
    "finally",
    "doctor",
    "checks",
    "patient",
    "data",
    "data",
    "outcome",
    "illness",
    "severity",
    "illness",
    "using",
    "data",
    "doctor",
    "identify",
    "patient",
    "ill",
    "various",
    "use",
    "cases",
    "use",
    "logistic",
    "regression",
    "guess",
    "enough",
    "theory",
    "part",
    "let",
    "move",
    "ahead",
    "see",
    "practical",
    "implementation",
    "logistic",
    "regression",
    "implementing",
    "two",
    "projects",
    "data",
    "set",
    "titanic",
    "predict",
    "factors",
    "made",
    "people",
    "likely",
    "survive",
    "sinking",
    "titanic",
    "ship",
    "second",
    "project",
    "see",
    "data",
    "analysis",
    "suv",
    "cars",
    "data",
    "suv",
    "cars",
    "purchase",
    "factors",
    "made",
    "people",
    "interested",
    "buying",
    "suv",
    "major",
    "questions",
    "implement",
    "logistic",
    "regression",
    "output",
    "get",
    "let",
    "start",
    "first",
    "project",
    "titanic",
    "data",
    "analysis",
    "might",
    "know",
    "ship",
    "called",
    "titanic",
    "basically",
    "hit",
    "iceberg",
    "sunk",
    "bottom",
    "ocean",
    "big",
    "disaster",
    "time",
    "first",
    "voyage",
    "ship",
    "supposed",
    "really",
    "really",
    "strongly",
    "built",
    "one",
    "best",
    "ships",
    "time",
    "big",
    "disaster",
    "time",
    "course",
    "movie",
    "well",
    "many",
    "might",
    "washed",
    "data",
    "passengers",
    "survived",
    "survive",
    "particular",
    "tragedy",
    "look",
    "data",
    "analyze",
    "factors",
    "would",
    "contributed",
    "chances",
    "person",
    "survival",
    "ship",
    "using",
    "logistic",
    "regression",
    "predict",
    "whether",
    "person",
    "survived",
    "person",
    "died",
    "apart",
    "also",
    "look",
    "various",
    "features",
    "along",
    "first",
    "let",
    "us",
    "explore",
    "data",
    "set",
    "index",
    "value",
    "first",
    "column",
    "passenger",
    "id",
    "next",
    "column",
    "survived",
    "two",
    "values",
    "0",
    "1",
    "0",
    "stands",
    "survive",
    "one",
    "stands",
    "survive",
    "column",
    "categorical",
    "values",
    "discrete",
    "next",
    "passenger",
    "class",
    "three",
    "values",
    "1",
    "2",
    "basically",
    "tells",
    "whether",
    "passengers",
    "travelling",
    "first",
    "class",
    "second",
    "class",
    "third",
    "class",
    "name",
    "six",
    "see",
    "gender",
    "passenger",
    "passenger",
    "male",
    "female",
    "age",
    "sip",
    "sp",
    "basically",
    "means",
    "number",
    "siblings",
    "spouses",
    "aboard",
    "titanic",
    "values",
    "1",
    "0",
    "parts",
    "apart",
    "basically",
    "number",
    "parents",
    "children",
    "aboard",
    "titanic",
    "also",
    "values",
    "ticket",
    "number",
    "fair",
    "table",
    "number",
    "embarked",
    "column",
    "inbox",
    "column",
    "three",
    "values",
    "sc",
    "basically",
    "stands",
    "southampton",
    "c",
    "stands",
    "cherbourg",
    "q",
    "stands",
    "cubans",
    "features",
    "applying",
    "model",
    "perform",
    "various",
    "steps",
    "implementing",
    "logistic",
    "regression",
    "various",
    "steps",
    "required",
    "implement",
    "algorithm",
    "case",
    "implementing",
    "logistic",
    "regression",
    "soft",
    "first",
    "step",
    "collect",
    "data",
    "import",
    "libraries",
    "used",
    "collecting",
    "data",
    "taking",
    "forward",
    "second",
    "step",
    "analyze",
    "data",
    "go",
    "various",
    "fields",
    "analyze",
    "data",
    "check",
    "females",
    "children",
    "survive",
    "better",
    "males",
    "rich",
    "passenger",
    "survived",
    "poor",
    "passenger",
    "money",
    "matter",
    "paid",
    "mode",
    "get",
    "ship",
    "evacuated",
    "first",
    "workers",
    "worker",
    "survived",
    "survival",
    "rate",
    "worker",
    "ship",
    "traveling",
    "passenger",
    "questions",
    "would",
    "going",
    "one",
    "one",
    "stage",
    "need",
    "analyze",
    "data",
    "explore",
    "data",
    "much",
    "third",
    "step",
    "wrangle",
    "data",
    "data",
    "wrangling",
    "basically",
    "means",
    "cleaning",
    "data",
    "simply",
    "remove",
    "unnecessary",
    "items",
    "null",
    "values",
    "data",
    "set",
    "clear",
    "data",
    "take",
    "forward",
    "step",
    "build",
    "model",
    "using",
    "train",
    "data",
    "set",
    "test",
    "using",
    "test",
    "performing",
    "split",
    "basically",
    "get",
    "data",
    "set",
    "training",
    "testing",
    "data",
    "set",
    "find",
    "check",
    "accuracy",
    "ensure",
    "much",
    "accurate",
    "values",
    "hope",
    "guys",
    "got",
    "five",
    "steps",
    "going",
    "implement",
    "logistic",
    "regression",
    "let",
    "go",
    "steps",
    "detail",
    "number",
    "one",
    "collect",
    "data",
    "say",
    "import",
    "libraries",
    "may",
    "show",
    "implementation",
    "part",
    "well",
    "open",
    "jupyter",
    "notebook",
    "implement",
    "steps",
    "side",
    "side",
    "guys",
    "jupyter",
    "notebook",
    "first",
    "let",
    "rename",
    "jupyter",
    "notebook",
    "let",
    "say",
    "titanic",
    "data",
    "analysis",
    "full",
    "step",
    "import",
    "libraries",
    "collect",
    "data",
    "let",
    "import",
    "library",
    "first",
    "first",
    "import",
    "pandas",
    "pandas",
    "used",
    "data",
    "analysis",
    "say",
    "import",
    "pandas",
    "pd",
    "importing",
    "numpy",
    "say",
    "import",
    "numpy",
    "np",
    "number",
    "library",
    "python",
    "basically",
    "stands",
    "numerical",
    "python",
    "widely",
    "used",
    "perform",
    "scientific",
    "computation",
    "next",
    "importing",
    "seaborn",
    "c",
    "1",
    "library",
    "statistical",
    "plotting",
    "say",
    "import",
    "seaborn",
    "sns",
    "also",
    "import",
    "matplotlib",
    "matplotlib",
    "library",
    "plotting",
    "say",
    "import",
    "matplotlib",
    "dot",
    "pi",
    "plot",
    "plt",
    "run",
    "library",
    "jupyter",
    "notebook",
    "write",
    "percentage",
    "matplotlib",
    "line",
    "next",
    "importing",
    "one",
    "module",
    "well",
    "calculate",
    "basic",
    "mathematical",
    "functions",
    "say",
    "import",
    "maths",
    "libraries",
    "needing",
    "titanic",
    "data",
    "analysis",
    "let",
    "import",
    "data",
    "set",
    "take",
    "variable",
    "let",
    "say",
    "titanic",
    "data",
    "using",
    "pandas",
    "read",
    "csv",
    "see",
    "data",
    "set",
    "like",
    "name",
    "data",
    "set",
    "titanic",
    "dot",
    "csv",
    "already",
    "showed",
    "data",
    "set",
    "let",
    "bring",
    "top",
    "10",
    "rows",
    "say",
    "take",
    "variable",
    "titanic",
    "data",
    "dot",
    "head",
    "say",
    "top",
    "ten",
    "rules",
    "run",
    "run",
    "style",
    "press",
    "shift",
    "enter",
    "else",
    "directly",
    "click",
    "cell",
    "index",
    "passenger",
    "id",
    "nothing",
    "index",
    "starting",
    "1",
    "survived",
    "column",
    "category",
    "call",
    "values",
    "say",
    "discrete",
    "values",
    "form",
    "0",
    "passenger",
    "class",
    "name",
    "passenger",
    "sex",
    "age",
    "data",
    "set",
    "going",
    "forward",
    "next",
    "let",
    "us",
    "print",
    "number",
    "passengers",
    "original",
    "data",
    "frame",
    "simply",
    "type",
    "print",
    "say",
    "number",
    "passengers",
    "using",
    "length",
    "function",
    "calculate",
    "total",
    "length",
    "say",
    "length",
    "inside",
    "passing",
    "variable",
    "titanic",
    "data",
    "copy",
    "paste",
    "dot",
    "index",
    "next",
    "set",
    "bring",
    "one",
    "number",
    "passengers",
    "original",
    "data",
    "set",
    "891",
    "around",
    "number",
    "would",
    "traveling",
    "titanic",
    "ship",
    "first",
    "step",
    "done",
    "collected",
    "data",
    "imported",
    "libraries",
    "find",
    "total",
    "number",
    "passengers",
    "titanic",
    "let",
    "go",
    "back",
    "presentation",
    "let",
    "see",
    "next",
    "step",
    "done",
    "collecting",
    "data",
    "next",
    "step",
    "analyze",
    "data",
    "creating",
    "different",
    "plots",
    "check",
    "relationship",
    "variables",
    "one",
    "variable",
    "affecting",
    "simply",
    "explore",
    "data",
    "set",
    "making",
    "use",
    "various",
    "columns",
    "plot",
    "graph",
    "either",
    "plot",
    "correlation",
    "graph",
    "plot",
    "distribution",
    "curve",
    "guys",
    "let",
    "go",
    "back",
    "jupyter",
    "notebook",
    "let",
    "analyze",
    "data",
    "second",
    "part",
    "analyze",
    "data",
    "put",
    "headed",
    "put",
    "go",
    "code",
    "click",
    "mark",
    "run",
    "first",
    "let",
    "us",
    "plot",
    "account",
    "plot",
    "pay",
    "passengers",
    "survived",
    "survive",
    "using",
    "seabourn",
    "library",
    "imported",
    "seaborn",
    "sns",
    "write",
    "whole",
    "name",
    "simply",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "say",
    "axis",
    "survive",
    "data",
    "using",
    "titanic",
    "data",
    "say",
    "name",
    "variable",
    "store",
    "data",
    "set",
    "let",
    "run",
    "see",
    "survived",
    "column",
    "x",
    "axis",
    "axis",
    "count",
    "zero",
    "basically",
    "stands",
    "survive",
    "one",
    "stands",
    "passengers",
    "survive",
    "see",
    "around",
    "550",
    "passengers",
    "survive",
    "around",
    "350",
    "passengers",
    "survive",
    "basically",
    "conclude",
    "less",
    "survivors",
    "survivors",
    "first",
    "plot",
    "another",
    "plot",
    "compare",
    "sex",
    "whether",
    "passengers",
    "survived",
    "survive",
    "many",
    "men",
    "many",
    "female",
    "simply",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "add",
    "hue",
    "six",
    "want",
    "know",
    "many",
    "females",
    "many",
    "male",
    "survive",
    "specifying",
    "data",
    "using",
    "titanic",
    "data",
    "set",
    "let",
    "run",
    "done",
    "mistake",
    "see",
    "survived",
    "column",
    "count",
    "color",
    "stands",
    "male",
    "passengers",
    "orange",
    "stands",
    "female",
    "see",
    "passengers",
    "survive",
    "value",
    "0",
    "see",
    "majority",
    "males",
    "survive",
    "see",
    "people",
    "survived",
    "see",
    "majority",
    "female",
    "survive",
    "basically",
    "concludes",
    "gender",
    "survival",
    "rate",
    "appears",
    "average",
    "women",
    "three",
    "times",
    "likely",
    "survive",
    "men",
    "next",
    "let",
    "us",
    "plot",
    "another",
    "plot",
    "hue",
    "passenger",
    "class",
    "see",
    "class",
    "passenger",
    "traveling",
    "whether",
    "traveling",
    "class",
    "1",
    "2",
    "arrived",
    "command",
    "say",
    "soon",
    "plot",
    "gave",
    "family",
    "change",
    "hue",
    "passenger",
    "class",
    "variable",
    "named",
    "pe",
    "class",
    "data",
    "said",
    "using",
    "titanic",
    "data",
    "result",
    "see",
    "blue",
    "orange",
    "second",
    "class",
    "green",
    "third",
    "class",
    "passengers",
    "survive",
    "majorly",
    "third",
    "class",
    "say",
    "lowest",
    "class",
    "cheapest",
    "class",
    "get",
    "dynamic",
    "people",
    "survive",
    "majorly",
    "belong",
    "higher",
    "classes",
    "1",
    "2",
    "eyes",
    "passenger",
    "traveling",
    "third",
    "class",
    "computed",
    "passengers",
    "survive",
    "majorly",
    "third",
    "see",
    "lowest",
    "class",
    "passengers",
    "traveling",
    "first",
    "second",
    "class",
    "would",
    "tend",
    "survive",
    "mode",
    "next",
    "got",
    "graph",
    "age",
    "distribution",
    "simply",
    "use",
    "data",
    "using",
    "pandas",
    "library",
    "declare",
    "array",
    "pass",
    "column",
    "plot",
    "want",
    "histogram",
    "see",
    "plot",
    "da",
    "test",
    "notice",
    "young",
    "passengers",
    "see",
    "children",
    "ages",
    "0",
    "10",
    "average",
    "people",
    "go",
    "ahead",
    "lester",
    "would",
    "population",
    "analysis",
    "age",
    "column",
    "saw",
    "young",
    "passengers",
    "video",
    "courage",
    "passengers",
    "traveling",
    "titanic",
    "next",
    "let",
    "plot",
    "graph",
    "fare",
    "well",
    "say",
    "titanic",
    "data",
    "say",
    "fair",
    "got",
    "histogram",
    "say",
    "hissed",
    "see",
    "fair",
    "size",
    "zero",
    "hundred",
    "let",
    "add",
    "bin",
    "size",
    "make",
    "clear",
    "say",
    "ben",
    "equals",
    "let",
    "say",
    "20",
    "increase",
    "figure",
    "size",
    "well",
    "say",
    "fixed",
    "size",
    "let",
    "say",
    "give",
    "dimensions",
    "10",
    "bins",
    "clear",
    "next",
    "analyzed",
    "columns",
    "well",
    "type",
    "titanic",
    "data",
    "want",
    "information",
    "columns",
    "left",
    "passenger",
    "id",
    "guess",
    "use",
    "see",
    "many",
    "passengers",
    "survived",
    "many",
    "also",
    "analysis",
    "gender",
    "basis",
    "saw",
    "female",
    "tend",
    "survive",
    "maintain",
    "survive",
    "saw",
    "passenger",
    "class",
    "passenger",
    "traveling",
    "first",
    "class",
    "second",
    "class",
    "third",
    "class",
    "name",
    "name",
    "analysis",
    "saw",
    "sex",
    "saw",
    "ages",
    "well",
    "sea",
    "bass",
    "stands",
    "number",
    "siblings",
    "spouse",
    "aboard",
    "titanic",
    "let",
    "us",
    "well",
    "say",
    "sns",
    "dot",
    "count",
    "plot",
    "mentioned",
    "x",
    "sc",
    "sp",
    "using",
    "titanic",
    "data",
    "see",
    "plot",
    "conclude",
    "maximum",
    "value",
    "zero",
    "conclude",
    "neither",
    "children",
    "spouse",
    "board",
    "titanic",
    "second",
    "highest",
    "value",
    "1",
    "various",
    "values",
    "2",
    "3",
    "4",
    "next",
    "go",
    "store",
    "column",
    "well",
    "similarly",
    "four",
    "parts",
    "next",
    "part",
    "see",
    "number",
    "parents",
    "children",
    "titanic",
    "similarly",
    "israel",
    "ticket",
    "number",
    "think",
    "analysis",
    "required",
    "ticket",
    "fears",
    "already",
    "discussed",
    "people",
    "would",
    "tend",
    "travel",
    "first",
    "class",
    "pay",
    "highest",
    "view",
    "cable",
    "number",
    "embarked",
    "columns",
    "data",
    "wrangling",
    "analyzed",
    "data",
    "seen",
    "quite",
    "graphs",
    "conclude",
    "variable",
    "better",
    "another",
    "relationship",
    "whole",
    "third",
    "step",
    "data",
    "wrangling",
    "data",
    "wrangling",
    "basically",
    "means",
    "cleaning",
    "data",
    "large",
    "data",
    "set",
    "might",
    "null",
    "values",
    "say",
    "n",
    "values",
    "important",
    "remove",
    "unnecessary",
    "items",
    "present",
    "data",
    "set",
    "removing",
    "directly",
    "affects",
    "accuracy",
    "go",
    "ahead",
    "clean",
    "data",
    "removing",
    "nan",
    "values",
    "unnecessary",
    "columns",
    "null",
    "value",
    "data",
    "set",
    "next",
    "time",
    "performing",
    "data",
    "wrangling",
    "supposed",
    "fall",
    "check",
    "whether",
    "dataset",
    "null",
    "say",
    "titanic",
    "data",
    "name",
    "data",
    "set",
    "say",
    "null",
    "basically",
    "tell",
    "values",
    "null",
    "return",
    "boolean",
    "result",
    "basically",
    "checks",
    "missing",
    "data",
    "result",
    "boolean",
    "format",
    "result",
    "true",
    "false",
    "falls",
    "mean",
    "null",
    "true",
    "means",
    "null",
    "let",
    "run",
    "see",
    "values",
    "false",
    "true",
    "falls",
    "value",
    "null",
    "true",
    "value",
    "none",
    "see",
    "cabin",
    "column",
    "first",
    "value",
    "null",
    "something",
    "see",
    "large",
    "data",
    "set",
    "counting",
    "stop",
    "actually",
    "see",
    "actually",
    "print",
    "number",
    "passengers",
    "nan",
    "value",
    "column",
    "say",
    "titanic",
    "underscore",
    "data",
    "null",
    "want",
    "sum",
    "got",
    "basically",
    "print",
    "number",
    "passengers",
    "n",
    "n",
    "values",
    "column",
    "see",
    "missing",
    "values",
    "column",
    "maximum",
    "value",
    "cave",
    "column",
    "less",
    "embark",
    "column",
    "2",
    "want",
    "see",
    "numbers",
    "also",
    "plot",
    "heat",
    "map",
    "visually",
    "analyze",
    "let",
    "well",
    "say",
    "snsd",
    "heat",
    "map",
    "say",
    "tick",
    "labels",
    "false",
    "child",
    "run",
    "already",
    "seen",
    "three",
    "columns",
    "missing",
    "data",
    "value",
    "present",
    "might",
    "age",
    "almost",
    "20",
    "column",
    "missing",
    "value",
    "caping",
    "columns",
    "quite",
    "large",
    "value",
    "two",
    "values",
    "embark",
    "column",
    "well",
    "add",
    "see",
    "map",
    "color",
    "coding",
    "say",
    "see",
    "map",
    "graph",
    "becomes",
    "attractive",
    "yellow",
    "stands",
    "drew",
    "say",
    "values",
    "null",
    "computed",
    "missing",
    "value",
    "lot",
    "missing",
    "values",
    "cabin",
    "column",
    "less",
    "value",
    "even",
    "visible",
    "embark",
    "column",
    "well",
    "remove",
    "missing",
    "values",
    "either",
    "replace",
    "values",
    "put",
    "dummy",
    "values",
    "simply",
    "drop",
    "column",
    "let",
    "us",
    "suppose",
    "pick",
    "age",
    "column",
    "first",
    "let",
    "plot",
    "box",
    "plot",
    "analyze",
    "column",
    "age",
    "say",
    "sns",
    "dot",
    "box",
    "plot",
    "say",
    "x",
    "equals",
    "passenger",
    "class",
    "pe",
    "class",
    "say",
    "equal",
    "h",
    "data",
    "set",
    "using",
    "titanic",
    "side",
    "say",
    "data",
    "goes",
    "titanic",
    "data",
    "see",
    "edge",
    "first",
    "class",
    "second",
    "class",
    "tends",
    "older",
    "rather",
    "third",
    "place",
    "well",
    "depends",
    "experience",
    "much",
    "earn",
    "might",
    "number",
    "reasons",
    "concluded",
    "passengers",
    "traveling",
    "class",
    "one",
    "class",
    "two",
    "tend",
    "older",
    "class",
    "3",
    "found",
    "missing",
    "values",
    "em",
    "one",
    "way",
    "either",
    "drop",
    "column",
    "simply",
    "fill",
    "values",
    "method",
    "called",
    "imputation",
    "perform",
    "data",
    "wrangling",
    "cleaning",
    "spring",
    "head",
    "data",
    "set",
    "say",
    "titanic",
    "head",
    "titanic",
    "data",
    "let",
    "say",
    "want",
    "five",
    "rows",
    "survived",
    "categorical",
    "particular",
    "column",
    "apply",
    "logic",
    "progression",
    "value",
    "value",
    "need",
    "predict",
    "passenger",
    "class",
    "name",
    "ticket",
    "number",
    "fair",
    "given",
    "seen",
    "keeping",
    "lot",
    "null",
    "values",
    "say",
    "invalid",
    "quite",
    "visible",
    "well",
    "first",
    "drop",
    "column",
    "dropping",
    "say",
    "titanic",
    "underscore",
    "data",
    "simply",
    "type",
    "drop",
    "column",
    "need",
    "drop",
    "drop",
    "cable",
    "column",
    "mention",
    "access",
    "equals",
    "1",
    "say",
    "place",
    "also",
    "true",
    "print",
    "head",
    "see",
    "whether",
    "column",
    "removed",
    "data",
    "set",
    "say",
    "titanic",
    "dot",
    "head",
    "see",
    "given",
    "column",
    "anymore",
    "also",
    "drop",
    "na",
    "values",
    "say",
    "titanic",
    "data",
    "dot",
    "drop",
    "values",
    "say",
    "nan",
    "number",
    "say",
    "place",
    "equal",
    "true",
    "let",
    "titanic",
    "let",
    "plot",
    "heat",
    "map",
    "let",
    "say",
    "values",
    "showing",
    "lot",
    "null",
    "values",
    "removed",
    "say",
    "snsd",
    "heat",
    "map",
    "pass",
    "data",
    "set",
    "check",
    "null",
    "say",
    "dick",
    "labels",
    "equal",
    "false",
    "want",
    "color",
    "coding",
    "say",
    "false",
    "basically",
    "help",
    "check",
    "whether",
    "values",
    "removed",
    "data",
    "set",
    "see",
    "null",
    "values",
    "entirely",
    "black",
    "actually",
    "know",
    "well",
    "go",
    "copy",
    "part",
    "use",
    "sum",
    "function",
    "calculate",
    "sum",
    "tells",
    "data",
    "set",
    "green",
    "data",
    "set",
    "contain",
    "null",
    "value",
    "n",
    "value",
    "r",
    "angela",
    "data",
    "see",
    "cleaner",
    "data",
    "done",
    "one",
    "step",
    "data",
    "wrangling",
    "removing",
    "one",
    "column",
    "lot",
    "things",
    "actually",
    "fill",
    "values",
    "values",
    "calculate",
    "mean",
    "fit",
    "null",
    "values",
    "see",
    "data",
    "set",
    "say",
    "titanic",
    "data",
    "dot",
    "head",
    "see",
    "lot",
    "string",
    "values",
    "converted",
    "categorical",
    "variables",
    "order",
    "implement",
    "logistic",
    "regression",
    "convert",
    "categorical",
    "variable",
    "dummy",
    "variables",
    "done",
    "using",
    "pandas",
    "logistic",
    "regression",
    "take",
    "two",
    "values",
    "whenever",
    "apply",
    "machine",
    "learning",
    "need",
    "make",
    "sure",
    "string",
    "values",
    "present",
    "wo",
    "taking",
    "input",
    "variables",
    "using",
    "string",
    "predict",
    "anything",
    "case",
    "survived",
    "columns",
    "2210",
    "many",
    "people",
    "tend",
    "survive",
    "men",
    "0",
    "stands",
    "survive",
    "one",
    "stands",
    "survive",
    "let",
    "convert",
    "variables",
    "dummy",
    "variables",
    "use",
    "pandas",
    "say",
    "pd",
    "get",
    "dummies",
    "simply",
    "press",
    "tab",
    "autocomplete",
    "say",
    "titanic",
    "data",
    "pass",
    "sex",
    "simply",
    "click",
    "shift",
    "tab",
    "get",
    "information",
    "type",
    "data",
    "frame",
    "passenger",
    "id",
    "survived",
    "passenger",
    "class",
    "run",
    "see",
    "0",
    "basically",
    "stands",
    "female",
    "female",
    "similarly",
    "male",
    "zero",
    "stanford",
    "made",
    "one",
    "stanford",
    "main",
    "require",
    "columns",
    "one",
    "column",
    "enough",
    "tell",
    "us",
    "whether",
    "male",
    "say",
    "female",
    "let",
    "say",
    "want",
    "keep",
    "mail",
    "say",
    "value",
    "mail",
    "1",
    "definitely",
    "maid",
    "female",
    "need",
    "values",
    "remove",
    "first",
    "column",
    "let",
    "say",
    "female",
    "say",
    "drop",
    "first",
    "andrew",
    "given",
    "one",
    "column",
    "male",
    "value",
    "0",
    "let",
    "set",
    "variable",
    "hsx",
    "say",
    "sex",
    "dot",
    "head",
    "want",
    "see",
    "first",
    "pie",
    "sorry",
    "dot",
    "data",
    "looks",
    "like",
    "done",
    "sex",
    "numerical",
    "values",
    "age",
    "numerical",
    "values",
    "spouses",
    "ticket",
    "number",
    "pair",
    "embarked",
    "well",
    "embark",
    "values",
    "sc",
    "also",
    "apply",
    "get",
    "dummy",
    "function",
    "let",
    "say",
    "take",
    "variable",
    "let",
    "say",
    "embark",
    "use",
    "pandas",
    "library",
    "need",
    "column",
    "name",
    "embarked",
    "let",
    "print",
    "head",
    "say",
    "embark",
    "dot",
    "head",
    "c",
    "q",
    "also",
    "drop",
    "first",
    "column",
    "two",
    "values",
    "enough",
    "passenger",
    "either",
    "traveling",
    "q",
    "toonstone",
    "s4",
    "sound",
    "time",
    "values",
    "0",
    "definitely",
    "passenger",
    "cherbourg",
    "third",
    "value",
    "drop",
    "first",
    "value",
    "say",
    "drop",
    "let",
    "run",
    "output",
    "looks",
    "like",
    "similarly",
    "passenger",
    "class",
    "well",
    "also",
    "three",
    "classes",
    "one",
    "two",
    "three",
    "copy",
    "whole",
    "statement",
    "let",
    "say",
    "want",
    "variable",
    "name",
    "let",
    "say",
    "pcl",
    "pass",
    "column",
    "name",
    "pe",
    "class",
    "drop",
    "first",
    "column",
    "also",
    "values",
    "1",
    "2",
    "3",
    "remove",
    "first",
    "column",
    "left",
    "two",
    "three",
    "values",
    "0",
    "definitely",
    "passengers",
    "traveling",
    "first",
    "class",
    "made",
    "values",
    "categorical",
    "next",
    "step",
    "would",
    "concatenate",
    "new",
    "rows",
    "data",
    "set",
    "see",
    "titanic",
    "data",
    "using",
    "pandas",
    "concatenate",
    "columns",
    "say",
    "p",
    "dot",
    "one",
    "cat",
    "say",
    "concatenate",
    "sex",
    "concatenate",
    "embark",
    "pcl",
    "mention",
    "access",
    "one",
    "run",
    "print",
    "head",
    "see",
    "columns",
    "added",
    "mail",
    "column",
    "basically",
    "tells",
    "person",
    "male",
    "female",
    "embark",
    "basically",
    "q",
    "traveling",
    "queenstown",
    "value",
    "would",
    "one",
    "else",
    "would",
    "0",
    "values",
    "zeroed",
    "definitely",
    "traveling",
    "cherbourg",
    "passenger",
    "class",
    "2",
    "value",
    "0",
    "passengers",
    "travelling",
    "class",
    "one",
    "hope",
    "got",
    "till",
    "irrelevant",
    "columns",
    "done",
    "drop",
    "columns",
    "drop",
    "pe",
    "class",
    "embarked",
    "column",
    "sex",
    "column",
    "type",
    "titanic",
    "data",
    "dot",
    "drop",
    "mention",
    "columns",
    "want",
    "drop",
    "say",
    "even",
    "lead",
    "passenger",
    "id",
    "nothing",
    "index",
    "value",
    "starting",
    "one",
    "drop",
    "well",
    "want",
    "name",
    "well",
    "delete",
    "name",
    "well",
    "else",
    "drop",
    "drop",
    "ticket",
    "well",
    "mention",
    "axis",
    "l",
    "say",
    "place",
    "equal",
    "true",
    "okay",
    "column",
    "name",
    "starts",
    "uppercase",
    "dropped",
    "let",
    "bring",
    "data",
    "set",
    "final",
    "leadership",
    "guys",
    "survived",
    "column",
    "value",
    "zero",
    "one",
    "passenger",
    "class",
    "forgot",
    "drop",
    "well",
    "worries",
    "drop",
    "let",
    "run",
    "survive",
    "h",
    "sp",
    "parts",
    "fair",
    "mail",
    "converted",
    "performed",
    "data",
    "angling",
    "see",
    "clean",
    "data",
    "converted",
    "values",
    "gender",
    "male",
    "embarked",
    "qns",
    "passenger",
    "class",
    "2",
    "2",
    "data",
    "wrangling",
    "cleaning",
    "data",
    "next",
    "training",
    "testing",
    "data",
    "split",
    "data",
    "set",
    "train",
    "subset",
    "test",
    "steps",
    "build",
    "model",
    "train",
    "data",
    "predict",
    "output",
    "test",
    "data",
    "set",
    "let",
    "go",
    "back",
    "jupiter",
    "implement",
    "well",
    "need",
    "train",
    "data",
    "set",
    "put",
    "indeed",
    "heading",
    "need",
    "define",
    "dependent",
    "variable",
    "independent",
    "variable",
    "output",
    "say",
    "value",
    "need",
    "predict",
    "write",
    "titanic",
    "data",
    "take",
    "column",
    "survive",
    "basically",
    "predict",
    "column",
    "whether",
    "passenger",
    "survived",
    "see",
    "discrete",
    "outcome",
    "form",
    "0",
    "1",
    "rest",
    "things",
    "take",
    "features",
    "say",
    "independent",
    "variable",
    "say",
    "titanic",
    "data",
    "drop",
    "simply",
    "drop",
    "survive",
    "columns",
    "independent",
    "variable",
    "everything",
    "else",
    "features",
    "leads",
    "survival",
    "rate",
    "defined",
    "independent",
    "variable",
    "dependent",
    "variable",
    "next",
    "step",
    "split",
    "data",
    "training",
    "testing",
    "subset",
    "using",
    "sk",
    "loan",
    "type",
    "sklearn",
    "dot",
    "cross",
    "validation",
    "import",
    "train",
    "display",
    "click",
    "shift",
    "tab",
    "go",
    "documentation",
    "see",
    "examples",
    "blast",
    "open",
    "go",
    "examples",
    "see",
    "split",
    "data",
    "extra",
    "next",
    "test",
    "drain",
    "test",
    "using",
    "string",
    "test",
    "platelet",
    "passing",
    "independent",
    "variable",
    "dependent",
    "variable",
    "define",
    "size",
    "random",
    "straight",
    "let",
    "copy",
    "paste",
    "train",
    "test",
    "dependent",
    "variable",
    "train",
    "test",
    "using",
    "split",
    "function",
    "pass",
    "independent",
    "dependent",
    "variable",
    "set",
    "split",
    "size",
    "let",
    "say",
    "put",
    "basically",
    "means",
    "data",
    "set",
    "divided",
    "ratio",
    "add",
    "random",
    "straight",
    "let",
    "say",
    "applying",
    "one",
    "necessary",
    "want",
    "result",
    "mine",
    "add",
    "random",
    "stream",
    "would",
    "basically",
    "take",
    "exactly",
    "sample",
    "every",
    "next",
    "train",
    "predict",
    "creating",
    "model",
    "logistic",
    "regression",
    "graph",
    "linear",
    "regression",
    "next",
    "type",
    "sk",
    "loan",
    "dot",
    "linear",
    "model",
    "import",
    "logistic",
    "regression",
    "next",
    "create",
    "instance",
    "logistic",
    "regression",
    "model",
    "say",
    "log",
    "model",
    "equals",
    "largest",
    "aggression",
    "need",
    "fit",
    "model",
    "say",
    "log",
    "model",
    "dot",
    "fit",
    "pass",
    "ex",
    "train",
    "rain",
    "gives",
    "details",
    "logistic",
    "regression",
    "gives",
    "class",
    "made",
    "dual",
    "fit",
    "intercept",
    "things",
    "need",
    "need",
    "make",
    "prediction",
    "take",
    "variable",
    "insect",
    "addictions",
    "pass",
    "model",
    "say",
    "log",
    "model",
    "dot",
    "protect",
    "pass",
    "value",
    "x",
    "test",
    "created",
    "model",
    "fit",
    "model",
    "made",
    "predictions",
    "evaluate",
    "model",
    "performing",
    "simply",
    "calculate",
    "accuracy",
    "also",
    "calculate",
    "classification",
    "report",
    "worry",
    "guys",
    "showing",
    "methods",
    "say",
    "sklearn",
    "dot",
    "matrix",
    "input",
    "classification",
    "report",
    "start",
    "fishing",
    "report",
    "inside",
    "passing",
    "test",
    "predictions",
    "guys",
    "classification",
    "report",
    "precision",
    "recall",
    "advanced",
    "code",
    "support",
    "value",
    "decision",
    "75",
    "72",
    "73",
    "bad",
    "order",
    "calculate",
    "accuracy",
    "well",
    "also",
    "use",
    "concept",
    "confusion",
    "matrix",
    "want",
    "print",
    "confusion",
    "matrix",
    "simply",
    "say",
    "sklearn",
    "dot",
    "matrix",
    "import",
    "confusion",
    "matrix",
    "first",
    "print",
    "function",
    "imported",
    "successfully",
    "confusion",
    "matrix",
    "passing",
    "variables",
    "test",
    "predictions",
    "hope",
    "guys",
    "already",
    "know",
    "concept",
    "confusion",
    "matrix",
    "guys",
    "give",
    "quick",
    "confirmation",
    "whether",
    "guys",
    "remember",
    "confusion",
    "matrix",
    "concept",
    "quickly",
    "summarize",
    "well",
    "okay",
    "charged",
    "say",
    "yes",
    "okay",
    "clear",
    "tell",
    "brief",
    "confusion",
    "matrix",
    "confusion",
    "matrix",
    "nothing",
    "2",
    "2",
    "matrix",
    "four",
    "outcomes",
    "basic",
    "tells",
    "us",
    "accurate",
    "values",
    "column",
    "predicted",
    "predicted",
    "actual",
    "know",
    "actual",
    "yes",
    "concept",
    "confusion",
    "matrix",
    "let",
    "fade",
    "values",
    "calculated",
    "105",
    "105",
    "2125",
    "63",
    "see",
    "got",
    "four",
    "outcomes",
    "105",
    "value",
    "model",
    "predicted",
    "reality",
    "also",
    "predicted",
    "know",
    "actual",
    "know",
    "similarly",
    "63",
    "predicted",
    "yes",
    "model",
    "predicted",
    "yes",
    "actually",
    "also",
    "yes",
    "order",
    "calculate",
    "accuracy",
    "need",
    "add",
    "sum",
    "two",
    "values",
    "divide",
    "whole",
    "two",
    "values",
    "tells",
    "order",
    "predicted",
    "correct",
    "output",
    "value",
    "also",
    "called",
    "called",
    "false",
    "positive",
    "called",
    "true",
    "positive",
    "called",
    "false",
    "negative",
    "order",
    "calculate",
    "accuracy",
    "manually",
    "python",
    "import",
    "accuracy",
    "score",
    "function",
    "get",
    "results",
    "well",
    "say",
    "sklearn",
    "import",
    "accuracy",
    "score",
    "simply",
    "print",
    "accuracy",
    "passing",
    "variables",
    "test",
    "predictions",
    "tells",
    "accuracy",
    "78",
    "quite",
    "good",
    "want",
    "manually",
    "2",
    "plus",
    "two",
    "numbers",
    "105",
    "comes",
    "almost",
    "168",
    "divide",
    "sum",
    "phone",
    "numbers",
    "105",
    "plus",
    "63",
    "plus",
    "21",
    "plus",
    "25",
    "gives",
    "result",
    "divide",
    "two",
    "number",
    "get",
    "accuracy",
    "98",
    "say",
    "calculate",
    "accuracy",
    "let",
    "go",
    "back",
    "presentation",
    "let",
    "see",
    "covered",
    "till",
    "first",
    "data",
    "data",
    "train",
    "test",
    "subset",
    "build",
    "model",
    "train",
    "data",
    "predicted",
    "output",
    "test",
    "data",
    "set",
    "fifth",
    "step",
    "check",
    "accuracy",
    "calculator",
    "accuracy",
    "almost",
    "seventy",
    "eight",
    "percent",
    "quite",
    "good",
    "say",
    "accuracy",
    "bad",
    "tells",
    "accurate",
    "results",
    "accuracy",
    "skoda",
    "finds",
    "enhanced",
    "got",
    "good",
    "accuracy",
    "moving",
    "ahead",
    "let",
    "us",
    "see",
    "second",
    "project",
    "suv",
    "data",
    "analysis",
    "car",
    "company",
    "released",
    "new",
    "suv",
    "market",
    "using",
    "previous",
    "data",
    "sales",
    "suv",
    "want",
    "predict",
    "category",
    "people",
    "might",
    "interested",
    "buying",
    "using",
    "logistic",
    "regression",
    "need",
    "find",
    "factors",
    "made",
    "people",
    "interested",
    "buying",
    "suv",
    "let",
    "us",
    "hear",
    "data",
    "set",
    "user",
    "id",
    "gender",
    "male",
    "female",
    "age",
    "estimated",
    "salary",
    "purchased",
    "column",
    "discreet",
    "column",
    "see",
    "categorical",
    "column",
    "value",
    "0",
    "1",
    "column",
    "need",
    "predict",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "based",
    "factors",
    "deciding",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "know",
    "salary",
    "person",
    "know",
    "age",
    "using",
    "predict",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "let",
    "go",
    "jupyter",
    "notebook",
    "implemented",
    "logistic",
    "regression",
    "guys",
    "going",
    "details",
    "data",
    "cleaning",
    "analyzing",
    "part",
    "start",
    "part",
    "leave",
    "go",
    "ahead",
    "practice",
    "much",
    "alright",
    "second",
    "project",
    "suv",
    "predictions",
    "alright",
    "first",
    "import",
    "libraries",
    "say",
    "import",
    "numpy",
    "snp",
    "similarly",
    "rest",
    "alright",
    "let",
    "bring",
    "head",
    "data",
    "set",
    "give",
    "already",
    "seen",
    "columns",
    "user",
    "id",
    "gender",
    "age",
    "salary",
    "calculate",
    "whether",
    "person",
    "actually",
    "purchase",
    "suv",
    "let",
    "us",
    "simply",
    "go",
    "algorithm",
    "part",
    "directly",
    "start",
    "logistic",
    "regression",
    "train",
    "model",
    "things",
    "first",
    "need",
    "define",
    "independent",
    "variable",
    "dependent",
    "variable",
    "case",
    "want",
    "ex",
    "independent",
    "variable",
    "data",
    "set",
    "lock",
    "specify",
    "sighing",
    "rows",
    "cool",
    "basically",
    "stands",
    "columns",
    "want",
    "two",
    "three",
    "dot",
    "values",
    "fetch",
    "rows",
    "second",
    "third",
    "column",
    "age",
    "estimated",
    "salary",
    "factors",
    "used",
    "predict",
    "dependent",
    "variable",
    "purchase",
    "dependent",
    "variable",
    "purchase",
    "dependent",
    "variable",
    "age",
    "salary",
    "say",
    "later",
    "said",
    "dot",
    "log",
    "rows",
    "add",
    "one",
    "column",
    "position",
    "column",
    "values",
    "right",
    "forgot",
    "one",
    "square",
    "bracket",
    "alright",
    "defined",
    "independent",
    "variable",
    "dependent",
    "variable",
    "independent",
    "variable",
    "age",
    "salary",
    "dependent",
    "variable",
    "column",
    "purchase",
    "must",
    "wondering",
    "lock",
    "function",
    "look",
    "function",
    "basically",
    "index",
    "panda",
    "data",
    "frame",
    "used",
    "integer",
    "based",
    "indexing",
    "also",
    "say",
    "selection",
    "index",
    "let",
    "bring",
    "independent",
    "variables",
    "dependent",
    "variable",
    "bring",
    "independent",
    "variable",
    "aged",
    "well",
    "salary",
    "next",
    "let",
    "print",
    "dependent",
    "variable",
    "well",
    "see",
    "values",
    "0",
    "1",
    "0",
    "stands",
    "purchase",
    "next",
    "let",
    "divide",
    "data",
    "set",
    "training",
    "test",
    "subset",
    "simply",
    "write",
    "sk",
    "loaned",
    "cross",
    "plate",
    "dot",
    "cross",
    "validation",
    "import",
    "rain",
    "test",
    "next",
    "press",
    "shift",
    "tab",
    "go",
    "examples",
    "copy",
    "line",
    "copy",
    "move",
    "points",
    "want",
    "text",
    "size",
    "let",
    "see",
    "25",
    "divided",
    "trained",
    "tested",
    "ratio",
    "let",
    "say",
    "take",
    "random",
    "set",
    "0",
    "random",
    "state",
    "basically",
    "ensures",
    "result",
    "say",
    "samples",
    "taken",
    "whenever",
    "run",
    "code",
    "let",
    "run",
    "also",
    "scale",
    "input",
    "values",
    "better",
    "performing",
    "done",
    "using",
    "standard",
    "scale",
    "oh",
    "let",
    "well",
    "say",
    "sklearn",
    "import",
    "standard",
    "scalar",
    "scale",
    "see",
    "data",
    "set",
    "dealing",
    "large",
    "numbers",
    "well",
    "although",
    "using",
    "small",
    "data",
    "set",
    "whenever",
    "working",
    "prod",
    "environment",
    "working",
    "large",
    "data",
    "set",
    "using",
    "thousands",
    "hundred",
    "thousands",
    "people",
    "scaling",
    "definitely",
    "affect",
    "performance",
    "large",
    "extent",
    "let",
    "show",
    "scale",
    "input",
    "values",
    "contains",
    "methods",
    "functionality",
    "required",
    "transform",
    "data",
    "let",
    "us",
    "scale",
    "tests",
    "well",
    "training",
    "data",
    "set",
    "else",
    "first",
    "make",
    "instance",
    "say",
    "standard",
    "scalar",
    "xtreme",
    "sasc",
    "dot",
    "fit",
    "fit",
    "underscore",
    "transform",
    "pass",
    "xtreme",
    "variable",
    "similarly",
    "test",
    "wherein",
    "pass",
    "x",
    "test",
    "right",
    "next",
    "step",
    "import",
    "logistic",
    "regression",
    "simply",
    "apply",
    "logistically",
    "creation",
    "first",
    "importing",
    "say",
    "sklearn",
    "sklearn",
    "linear",
    "model",
    "import",
    "logistic",
    "regression",
    "using",
    "classifier",
    "classifier",
    "dot",
    "equals",
    "largest",
    "aggression",
    "make",
    "instance",
    "say",
    "logistic",
    "regression",
    "pass",
    "random",
    "state",
    "0",
    "simply",
    "fit",
    "model",
    "simply",
    "pass",
    "x",
    "train",
    "white",
    "rain",
    "tells",
    "details",
    "logistic",
    "regression",
    "predict",
    "value",
    "say",
    "prayed",
    "equals",
    "classifier",
    "predict",
    "function",
    "pass",
    "x",
    "test",
    "created",
    "model",
    "scale",
    "input",
    "values",
    "applied",
    "logistic",
    "regression",
    "predicted",
    "values",
    "want",
    "know",
    "accuracy",
    "accuracy",
    "first",
    "need",
    "import",
    "accuracy",
    "scores",
    "say",
    "sklearn",
    "import",
    "actually",
    "see",
    "school",
    "using",
    "function",
    "calculate",
    "accuracy",
    "manually",
    "creating",
    "confusion",
    "matrix",
    "pass",
    "lightest",
    "predicted",
    "right",
    "get",
    "accuracy",
    "89",
    "want",
    "know",
    "accuracy",
    "percentage",
    "multiply",
    "hundred",
    "run",
    "gives",
    "89",
    "hope",
    "guys",
    "clear",
    "whatever",
    "taught",
    "today",
    "taken",
    "independent",
    "variables",
    "age",
    "salary",
    "calculated",
    "many",
    "people",
    "purchase",
    "suv",
    "calculated",
    "model",
    "checking",
    "accuracy",
    "get",
    "accuracy",
    "89",
    "great",
    "alright",
    "guys",
    "today",
    "discuss",
    "covered",
    "today",
    "training",
    "first",
    "quick",
    "introduction",
    "regression",
    "aggression",
    "actually",
    "use",
    "understood",
    "types",
    "regression",
    "got",
    "details",
    "logistic",
    "regression",
    "compared",
    "linear",
    "logistic",
    "regression",
    "also",
    "seen",
    "various",
    "use",
    "cases",
    "implement",
    "logistic",
    "regression",
    "real",
    "life",
    "picked",
    "two",
    "projects",
    "titanic",
    "data",
    "analysis",
    "suv",
    "prediction",
    "seen",
    "collect",
    "data",
    "analyze",
    "data",
    "perform",
    "modeling",
    "date",
    "train",
    "data",
    "test",
    "data",
    "finally",
    "calculated",
    "accuracy",
    "suv",
    "prediction",
    "actually",
    "analyze",
    "clean",
    "data",
    "lot",
    "things",
    "go",
    "ahead",
    "pick",
    "data",
    "set",
    "explore",
    "much",
    "classification",
    "hope",
    "every",
    "one",
    "must",
    "used",
    "gmail",
    "think",
    "male",
    "getting",
    "classified",
    "spam",
    "spam",
    "mail",
    "well",
    "classification",
    "well",
    "classification",
    "process",
    "dividing",
    "data",
    "set",
    "different",
    "categories",
    "groups",
    "adding",
    "label",
    "way",
    "say",
    "technique",
    "categorizing",
    "observation",
    "different",
    "category",
    "basically",
    "taking",
    "data",
    "analyzing",
    "basis",
    "condition",
    "finely",
    "divided",
    "various",
    "categories",
    "classify",
    "well",
    "classify",
    "perform",
    "predictive",
    "analysis",
    "like",
    "get",
    "mail",
    "machine",
    "predicts",
    "spam",
    "spam",
    "mail",
    "basis",
    "prediction",
    "add",
    "irrelevant",
    "spam",
    "mail",
    "respective",
    "folder",
    "general",
    "classification",
    "algorithm",
    "handled",
    "questions",
    "like",
    "data",
    "belongs",
    "category",
    "b",
    "category",
    "like",
    "male",
    "female",
    "something",
    "like",
    "getting",
    "okay",
    "fine",
    "question",
    "arises",
    "use",
    "well",
    "use",
    "protection",
    "order",
    "check",
    "whether",
    "transaction",
    "genuine",
    "suppose",
    "using",
    "credit",
    "india",
    "due",
    "reason",
    "fly",
    "dubai",
    "using",
    "credit",
    "card",
    "get",
    "notification",
    "alert",
    "regarding",
    "transaction",
    "would",
    "ask",
    "confirm",
    "transaction",
    "also",
    "kind",
    "predictive",
    "analysis",
    "machine",
    "predicts",
    "something",
    "fishy",
    "transaction",
    "ago",
    "made",
    "transaction",
    "using",
    "credit",
    "card",
    "india",
    "24",
    "hour",
    "later",
    "credit",
    "card",
    "used",
    "payment",
    "dubai",
    "machine",
    "texts",
    "something",
    "fishy",
    "going",
    "transaction",
    "order",
    "confirm",
    "sends",
    "notification",
    "alert",
    "right",
    "well",
    "one",
    "use",
    "case",
    "classification",
    "even",
    "use",
    "classify",
    "different",
    "items",
    "like",
    "fruits",
    "base",
    "taste",
    "color",
    "size",
    "weight",
    "machine",
    "well",
    "trained",
    "using",
    "classification",
    "algorithm",
    "easily",
    "predict",
    "class",
    "type",
    "fruit",
    "whenever",
    "new",
    "data",
    "given",
    "fruit",
    "item",
    "car",
    "house",
    "signboard",
    "anything",
    "noticed",
    "visit",
    "sites",
    "try",
    "login",
    "get",
    "picture",
    "capture",
    "right",
    "identify",
    "whether",
    "given",
    "image",
    "car",
    "pole",
    "select",
    "example",
    "10",
    "images",
    "selecting",
    "three",
    "mages",
    "way",
    "training",
    "machine",
    "right",
    "telling",
    "three",
    "picture",
    "car",
    "rest",
    "knows",
    "training",
    "something",
    "big",
    "right",
    "moving",
    "ahead",
    "let",
    "discuss",
    "types",
    "education",
    "online",
    "well",
    "several",
    "different",
    "ways",
    "perform",
    "tasks",
    "like",
    "order",
    "predict",
    "whether",
    "given",
    "person",
    "male",
    "female",
    "machine",
    "trained",
    "first",
    "right",
    "multiple",
    "ways",
    "train",
    "machine",
    "choose",
    "one",
    "predictive",
    "analytics",
    "many",
    "different",
    "techniques",
    "common",
    "decision",
    "tree",
    "cover",
    "depth",
    "today",
    "session",
    "part",
    "classification",
    "algorithm",
    "decision",
    "tree",
    "random",
    "forest",
    "name",
    "buys",
    "neighbor",
    "lodge",
    "regression",
    "linear",
    "regression",
    "support",
    "vector",
    "machines",
    "many",
    "alright",
    "let",
    "give",
    "idea",
    "starting",
    "decision",
    "tree",
    "well",
    "decision",
    "tree",
    "graphical",
    "representation",
    "possible",
    "solution",
    "decision",
    "decisions",
    "made",
    "explained",
    "easily",
    "example",
    "task",
    "says",
    "go",
    "restaurant",
    "buy",
    "hamburger",
    "confused",
    "artboard",
    "create",
    "dish",
    "entry",
    "starting",
    "root",
    "node",
    "first",
    "check",
    "whether",
    "hungry",
    "right",
    "hungry",
    "go",
    "back",
    "sleep",
    "right",
    "hungry",
    "25",
    "decide",
    "go",
    "restaurant",
    "hungry",
    "25",
    "go",
    "buy",
    "hamburger",
    "right",
    "decision",
    "tree",
    "moving",
    "ahead",
    "let",
    "see",
    "random",
    "forest",
    "well",
    "random",
    "forest",
    "build",
    "multiple",
    "decision",
    "trees",
    "merges",
    "together",
    "get",
    "accurate",
    "stable",
    "production",
    "right",
    "time",
    "random",
    "forest",
    "trained",
    "bagging",
    "method",
    "bragging",
    "method",
    "based",
    "idea",
    "combination",
    "learning",
    "module",
    "increases",
    "overall",
    "result",
    "combining",
    "learning",
    "different",
    "models",
    "clubbing",
    "together",
    "increase",
    "overall",
    "result",
    "fine",
    "one",
    "thing",
    "size",
    "data",
    "set",
    "huge",
    "case",
    "one",
    "single",
    "decision",
    "tree",
    "would",
    "lead",
    "offutt",
    "model",
    "way",
    "like",
    "single",
    "person",
    "might",
    "perspective",
    "complete",
    "population",
    "population",
    "huge",
    "right",
    "however",
    "implement",
    "voting",
    "system",
    "ask",
    "different",
    "individual",
    "interpret",
    "data",
    "would",
    "able",
    "cover",
    "pattern",
    "much",
    "meticulous",
    "way",
    "even",
    "diagram",
    "see",
    "section",
    "howard",
    "large",
    "training",
    "data",
    "set",
    "first",
    "divide",
    "training",
    "data",
    "set",
    "n",
    "create",
    "decision",
    "tree",
    "cell",
    "sample",
    "b",
    "part",
    "take",
    "vote",
    "every",
    "decision",
    "made",
    "every",
    "decision",
    "tree",
    "finally",
    "club",
    "vote",
    "get",
    "random",
    "forest",
    "dition",
    "fine",
    "let",
    "move",
    "ahead",
    "next",
    "neighbor",
    "buys",
    "name",
    "bias",
    "classification",
    "technique",
    "based",
    "bayes",
    "theorem",
    "assumes",
    "particular",
    "feature",
    "class",
    "completely",
    "unrelated",
    "presence",
    "feature",
    "named",
    "buys",
    "simple",
    "easy",
    "implement",
    "algorithm",
    "due",
    "simplicity",
    "algorithm",
    "might",
    "perform",
    "complex",
    "model",
    "size",
    "data",
    "set",
    "large",
    "enough",
    "right",
    "classical",
    "use",
    "case",
    "navy",
    "bias",
    "document",
    "classification",
    "determine",
    "whether",
    "given",
    "text",
    "corresponds",
    "one",
    "categories",
    "texas",
    "case",
    "features",
    "used",
    "might",
    "presence",
    "absence",
    "absence",
    "keyword",
    "nev",
    "diagram",
    "see",
    "using",
    "neighbor",
    "buys",
    "decide",
    "whether",
    "disease",
    "first",
    "check",
    "probability",
    "disease",
    "disease",
    "right",
    "probability",
    "disease",
    "hand",
    "probability",
    "disease",
    "okay",
    "first",
    "let",
    "see",
    "disease",
    "go",
    "doctor",
    "right",
    "visited",
    "doctor",
    "test",
    "positive",
    "adjective",
    "probability",
    "positive",
    "test",
    "disease",
    "0",
    "probability",
    "negative",
    "test",
    "already",
    "disease",
    "also",
    "false",
    "negative",
    "statement",
    "test",
    "detecting",
    "negative",
    "still",
    "disease",
    "right",
    "false",
    "negative",
    "statement",
    "let",
    "move",
    "ahead",
    "disease",
    "probability",
    "disease",
    "visit",
    "doctor",
    "doctor",
    "like",
    "yes",
    "disease",
    "already",
    "know",
    "disease",
    "false",
    "positive",
    "statement",
    "probability",
    "disease",
    "actually",
    "know",
    "disease",
    "probability",
    "disease",
    "actually",
    "know",
    "disease",
    "probability",
    "around",
    "fine",
    "probability",
    "disease",
    "even",
    "test",
    "showing",
    "results",
    "true",
    "positive",
    "statement",
    "right",
    "let",
    "move",
    "ahead",
    "discuss",
    "kn",
    "n",
    "algorithm",
    "knn",
    "algorithm",
    "neighbor",
    "stores",
    "available",
    "cases",
    "classifies",
    "new",
    "cases",
    "based",
    "similarity",
    "measure",
    "k",
    "knn",
    "algorithm",
    "nearest",
    "neighbor",
    "wish",
    "take",
    "vote",
    "example",
    "k",
    "equal",
    "1",
    "object",
    "simply",
    "assigned",
    "class",
    "single",
    "nearest",
    "neighbor",
    "diagram",
    "see",
    "difference",
    "image",
    "k",
    "equal",
    "1",
    "k",
    "equal",
    "3",
    "k",
    "equal",
    "5",
    "right",
    "well",
    "systems",
    "able",
    "use",
    "neighbor",
    "visual",
    "pattern",
    "recognization",
    "scan",
    "detect",
    "hidden",
    "packages",
    "bottom",
    "bin",
    "shopping",
    "cart",
    "checkout",
    "object",
    "detected",
    "matches",
    "exactly",
    "object",
    "listed",
    "database",
    "price",
    "spotted",
    "product",
    "could",
    "even",
    "automatically",
    "added",
    "customers",
    "bill",
    "automated",
    "billing",
    "practice",
    "used",
    "extensively",
    "time",
    "technology",
    "developed",
    "available",
    "use",
    "want",
    "use",
    "yeah",
    "one",
    "thing",
    "neighbor",
    "also",
    "used",
    "retail",
    "detect",
    "patterns",
    "credit",
    "card",
    "users",
    "many",
    "new",
    "transaction",
    "scrutinizing",
    "software",
    "application",
    "use",
    "cayenne",
    "algorithms",
    "analyze",
    "register",
    "data",
    "spot",
    "unusual",
    "pattern",
    "indicates",
    "suspicious",
    "activity",
    "example",
    "register",
    "data",
    "indicates",
    "lot",
    "customers",
    "information",
    "entered",
    "manually",
    "rather",
    "automated",
    "scanning",
    "swapping",
    "case",
    "could",
    "indicate",
    "employees",
    "using",
    "register",
    "fact",
    "stealing",
    "customers",
    "personal",
    "information",
    "register",
    "data",
    "indicates",
    "particular",
    "good",
    "returned",
    "exchanged",
    "multiple",
    "times",
    "could",
    "indicate",
    "employees",
    "misusing",
    "return",
    "policy",
    "trying",
    "make",
    "money",
    "fake",
    "returns",
    "right",
    "knn",
    "algorithm",
    "starting",
    "decision",
    "tree",
    "first",
    "let",
    "tell",
    "choose",
    "gentry",
    "start",
    "well",
    "decision",
    "tree",
    "really",
    "easy",
    "read",
    "understand",
    "belongs",
    "one",
    "models",
    "interpretable",
    "understand",
    "exactly",
    "classifier",
    "made",
    "particular",
    "decision",
    "right",
    "let",
    "tell",
    "fact",
    "given",
    "data",
    "set",
    "say",
    "algorithm",
    "performs",
    "better",
    "like",
    "say",
    "decision",
    "trees",
    "better",
    "buys",
    "name",
    "biases",
    "performing",
    "better",
    "decision",
    "tree",
    "depends",
    "data",
    "set",
    "right",
    "apply",
    "hit",
    "trial",
    "method",
    "algorithms",
    "one",
    "one",
    "compare",
    "result",
    "model",
    "gives",
    "best",
    "result",
    "order",
    "use",
    "better",
    "accuracy",
    "data",
    "set",
    "right",
    "let",
    "start",
    "decision",
    "tree",
    "well",
    "decision",
    "tree",
    "graphical",
    "representation",
    "possible",
    "solution",
    "decision",
    "based",
    "certain",
    "conditions",
    "might",
    "wondering",
    "thing",
    "called",
    "decision",
    "tree",
    "well",
    "called",
    "starts",
    "root",
    "branches",
    "number",
    "solution",
    "like",
    "tree",
    "right",
    "even",
    "tree",
    "starts",
    "roux",
    "starts",
    "growing",
    "branches",
    "gets",
    "bigger",
    "bigger",
    "similarly",
    "decision",
    "tree",
    "roux",
    "keeps",
    "growing",
    "increasing",
    "number",
    "decision",
    "conditions",
    "let",
    "tell",
    "real",
    "life",
    "scenario",
    "wo",
    "say",
    "must",
    "used",
    "remember",
    "whenever",
    "dial",
    "number",
    "credit",
    "card",
    "company",
    "redirects",
    "intelligent",
    "computerised",
    "assistant",
    "asks",
    "questions",
    "like",
    "press",
    "one",
    "english",
    "press",
    "2",
    "henry",
    "press",
    "3",
    "press",
    "4",
    "right",
    "select",
    "one",
    "redirects",
    "certain",
    "set",
    "questions",
    "like",
    "press",
    "1",
    "press",
    "1",
    "similarly",
    "right",
    "keeps",
    "repeating",
    "finally",
    "get",
    "right",
    "person",
    "right",
    "might",
    "think",
    "caught",
    "voicemail",
    "hell",
    "company",
    "actually",
    "using",
    "decision",
    "tree",
    "get",
    "right",
    "person",
    "lied",
    "like",
    "focus",
    "particular",
    "image",
    "moment",
    "particular",
    "slide",
    "see",
    "image",
    "task",
    "accept",
    "new",
    "job",
    "offer",
    "alright",
    "decide",
    "created",
    "decision",
    "tree",
    "starting",
    "base",
    "condition",
    "root",
    "node",
    "basic",
    "salary",
    "minimum",
    "salary",
    "accepting",
    "offer",
    "right",
    "salary",
    "greater",
    "check",
    "whether",
    "commute",
    "one",
    "hour",
    "one",
    "decline",
    "offer",
    "less",
    "one",
    "hour",
    "getting",
    "closer",
    "accepting",
    "job",
    "offer",
    "check",
    "whether",
    "company",
    "offering",
    "free",
    "coffee",
    "right",
    "company",
    "offering",
    "free",
    "coffee",
    "decline",
    "offer",
    "fit",
    "offering",
    "free",
    "coffee",
    "yeah",
    "happily",
    "accept",
    "offer",
    "right",
    "example",
    "decision",
    "tree",
    "let",
    "move",
    "ahead",
    "understand",
    "decision",
    "tree",
    "well",
    "sample",
    "data",
    "set",
    "using",
    "explain",
    "decision",
    "tree",
    "right",
    "data",
    "set",
    "row",
    "example",
    "first",
    "two",
    "columns",
    "provide",
    "features",
    "attributes",
    "describes",
    "data",
    "last",
    "column",
    "gives",
    "label",
    "class",
    "want",
    "predict",
    "like",
    "modify",
    "data",
    "adding",
    "additional",
    "features",
    "example",
    "program",
    "work",
    "exactly",
    "way",
    "fine",
    "data",
    "set",
    "pretty",
    "straightforward",
    "except",
    "one",
    "thing",
    "hope",
    "noticed",
    "perfectly",
    "separable",
    "let",
    "tell",
    "something",
    "second",
    "fifth",
    "examples",
    "features",
    "different",
    "labels",
    "yellow",
    "colour",
    "diameter",
    "three",
    "labels",
    "mango",
    "lemon",
    "right",
    "let",
    "move",
    "see",
    "decision",
    "tree",
    "handles",
    "case",
    "right",
    "order",
    "build",
    "tree",
    "use",
    "decision",
    "tree",
    "algorithm",
    "called",
    "card",
    "card",
    "algorithm",
    "stands",
    "classification",
    "regression",
    "tree",
    "algorithm",
    "online",
    "let",
    "see",
    "preview",
    "works",
    "right",
    "begin",
    "add",
    "root",
    "node",
    "tree",
    "nodes",
    "receive",
    "list",
    "rows",
    "input",
    "route",
    "receive",
    "entire",
    "training",
    "data",
    "set",
    "node",
    "ask",
    "true",
    "false",
    "question",
    "one",
    "feature",
    "response",
    "question",
    "split",
    "partition",
    "data",
    "set",
    "two",
    "different",
    "subsets",
    "subsets",
    "become",
    "input",
    "child",
    "node",
    "tree",
    "goal",
    "question",
    "finally",
    "unmix",
    "labels",
    "proceed",
    "words",
    "produce",
    "purest",
    "possible",
    "distribution",
    "labels",
    "node",
    "example",
    "input",
    "node",
    "contains",
    "one",
    "single",
    "type",
    "label",
    "could",
    "say",
    "perfectly",
    "unmixed",
    "uncertainty",
    "type",
    "label",
    "consists",
    "grapes",
    "right",
    "hand",
    "labels",
    "node",
    "still",
    "mixed",
    "would",
    "ask",
    "another",
    "question",
    "drill",
    "right",
    "need",
    "understand",
    "question",
    "ask",
    "need",
    "conduct",
    "much",
    "question",
    "helps",
    "unmix",
    "label",
    "quantify",
    "amount",
    "uncertainty",
    "single",
    "node",
    "using",
    "metric",
    "called",
    "gini",
    "impurity",
    "quantify",
    "much",
    "question",
    "reduces",
    "uncertainty",
    "using",
    "concept",
    "called",
    "information",
    "game",
    "use",
    "select",
    "best",
    "question",
    "ask",
    "point",
    "iterate",
    "steps",
    "recursively",
    "build",
    "tree",
    "new",
    "node",
    "continue",
    "dividing",
    "data",
    "question",
    "ask",
    "finally",
    "reach",
    "leaf",
    "alright",
    "alright",
    "decision",
    "tree",
    "order",
    "create",
    "diversion",
    "first",
    "identify",
    "different",
    "set",
    "questions",
    "ask",
    "tree",
    "like",
    "color",
    "green",
    "question",
    "question",
    "decided",
    "data",
    "set",
    "like",
    "colored",
    "green",
    "diameter",
    "greater",
    "equal",
    "3",
    "color",
    "yellow",
    "right",
    "questions",
    "resembles",
    "data",
    "set",
    "remember",
    "right",
    "color",
    "green",
    "divide",
    "two",
    "part",
    "first",
    "green",
    "mango",
    "true",
    "false",
    "lemon",
    "map",
    "right",
    "color",
    "green",
    "diameter",
    "greater",
    "equal",
    "3",
    "color",
    "yellow",
    "let",
    "move",
    "understand",
    "decision",
    "tree",
    "terminologies",
    "alright",
    "starting",
    "root",
    "node",
    "root",
    "node",
    "base",
    "node",
    "tree",
    "entire",
    "tree",
    "starts",
    "root",
    "node",
    "words",
    "first",
    "node",
    "tree",
    "represents",
    "entire",
    "population",
    "sample",
    "entire",
    "population",
    "segregated",
    "divided",
    "two",
    "homogeneous",
    "set",
    "fine",
    "next",
    "leaf",
    "node",
    "well",
    "leaf",
    "node",
    "one",
    "reach",
    "end",
    "tree",
    "right",
    "segregated",
    "level",
    "leaf",
    "node",
    "next",
    "splitting",
    "splitting",
    "dividing",
    "root",
    "node",
    "node",
    "different",
    "sub",
    "part",
    "basis",
    "condition",
    "right",
    "comes",
    "branch",
    "sub",
    "tree",
    "well",
    "branch",
    "subtree",
    "gets",
    "formed",
    "split",
    "tree",
    "suppose",
    "split",
    "root",
    "node",
    "gets",
    "divided",
    "two",
    "branches",
    "two",
    "subtrees",
    "right",
    "next",
    "concept",
    "pruning",
    "well",
    "say",
    "pruning",
    "opposite",
    "splitting",
    "removing",
    "sub",
    "node",
    "decision",
    "tree",
    "see",
    "pruning",
    "later",
    "session",
    "right",
    "let",
    "move",
    "ahead",
    "next",
    "parent",
    "child",
    "node",
    "well",
    "first",
    "root",
    "node",
    "always",
    "parent",
    "node",
    "nodes",
    "associated",
    "known",
    "child",
    "node",
    "well",
    "understand",
    "way",
    "top",
    "node",
    "belongs",
    "parent",
    "node",
    "bottom",
    "node",
    "derived",
    "top",
    "node",
    "zhi",
    "node",
    "node",
    "producing",
    "note",
    "child",
    "node",
    "node",
    "producing",
    "parent",
    "node",
    "simple",
    "concept",
    "right",
    "let",
    "use",
    "cartel",
    "gotham",
    "design",
    "tree",
    "manually",
    "first",
    "decide",
    "question",
    "ask",
    "let",
    "first",
    "visualize",
    "decision",
    "tree",
    "decision",
    "tree",
    "creating",
    "manually",
    "like",
    "first",
    "let",
    "look",
    "data",
    "set",
    "outlook",
    "temperature",
    "humidity",
    "windy",
    "different",
    "attributes",
    "basis",
    "predict",
    "whether",
    "play",
    "one",
    "among",
    "pick",
    "first",
    "answer",
    "determine",
    "best",
    "attribute",
    "classifies",
    "training",
    "data",
    "right",
    "choose",
    "best",
    "attribute",
    "tree",
    "decide",
    "split",
    "tree",
    "decide",
    "root",
    "node",
    "well",
    "move",
    "split",
    "tree",
    "terminologies",
    "know",
    "right",
    "first",
    "gini",
    "index",
    "x",
    "gini",
    "index",
    "gini",
    "index",
    "measure",
    "impurity",
    "purity",
    "used",
    "building",
    "decision",
    "tree",
    "cartel",
    "gotham",
    "right",
    "next",
    "information",
    "gain",
    "information",
    "gain",
    "decrease",
    "entropy",
    "data",
    "set",
    "split",
    "basis",
    "attribute",
    "constructing",
    "decision",
    "tree",
    "finding",
    "attribute",
    "returns",
    "highest",
    "information",
    "gain",
    "right",
    "selecting",
    "node",
    "would",
    "give",
    "highest",
    "information",
    "gain",
    "alright",
    "next",
    "reduction",
    "variance",
    "reduction",
    "variance",
    "algorithm",
    "used",
    "continuous",
    "target",
    "variable",
    "regression",
    "problems",
    "split",
    "lower",
    "variance",
    "selected",
    "criteria",
    "let",
    "population",
    "see",
    "general",
    "term",
    "mean",
    "variance",
    "variance",
    "much",
    "data",
    "wearing",
    "right",
    "data",
    "less",
    "impure",
    "pure",
    "case",
    "variation",
    "would",
    "less",
    "data",
    "almost",
    "similar",
    "right",
    "also",
    "way",
    "setting",
    "tree",
    "split",
    "lower",
    "variance",
    "selected",
    "criteria",
    "split",
    "population",
    "right",
    "next",
    "chi",
    "square",
    "square",
    "algorithm",
    "used",
    "find",
    "statistical",
    "significance",
    "differences",
    "sub",
    "nodes",
    "parent",
    "nodes",
    "fine",
    "let",
    "move",
    "ahead",
    "main",
    "question",
    "decide",
    "best",
    "attribute",
    "understand",
    "need",
    "calculate",
    "something",
    "known",
    "information",
    "game",
    "attribute",
    "highest",
    "information",
    "gain",
    "considered",
    "best",
    "yeah",
    "know",
    "next",
    "question",
    "might",
    "like",
    "information",
    "move",
    "see",
    "exactly",
    "information",
    "gain",
    "let",
    "first",
    "introduce",
    "term",
    "called",
    "entropy",
    "term",
    "used",
    "calculating",
    "information",
    "gain",
    "well",
    "entropy",
    "metric",
    "measures",
    "impurity",
    "something",
    "words",
    "say",
    "first",
    "step",
    "solve",
    "problem",
    "decision",
    "tree",
    "mentioned",
    "something",
    "impurity",
    "let",
    "move",
    "understand",
    "impurity",
    "suppose",
    "basket",
    "full",
    "apples",
    "another",
    "bowl",
    "full",
    "label",
    "says",
    "apple",
    "asked",
    "pick",
    "one",
    "item",
    "basket",
    "ball",
    "probability",
    "getting",
    "apple",
    "correct",
    "label",
    "1",
    "case",
    "say",
    "impurities",
    "zero",
    "right",
    "four",
    "different",
    "fruits",
    "basket",
    "four",
    "different",
    "labels",
    "ball",
    "probability",
    "matching",
    "fruit",
    "label",
    "obviously",
    "one",
    "something",
    "less",
    "well",
    "could",
    "possible",
    "picked",
    "banana",
    "basket",
    "randomly",
    "picked",
    "level",
    "ball",
    "says",
    "cherry",
    "random",
    "permutation",
    "combination",
    "possible",
    "case",
    "say",
    "impurities",
    "nonzero",
    "hope",
    "concept",
    "impurities",
    "coming",
    "back",
    "entropy",
    "said",
    "entropy",
    "measure",
    "impurity",
    "graph",
    "left",
    "see",
    "probability",
    "zero",
    "one",
    "either",
    "highly",
    "impure",
    "highly",
    "pure",
    "case",
    "value",
    "entropy",
    "zero",
    "probability",
    "value",
    "entropy",
    "maximum",
    "well",
    "impurity",
    "impurities",
    "degree",
    "randomness",
    "random",
    "data",
    "data",
    "completely",
    "pure",
    "case",
    "randomness",
    "equals",
    "zero",
    "data",
    "completely",
    "empty",
    "even",
    "case",
    "value",
    "impurity",
    "zero",
    "question",
    "like",
    "value",
    "entropy",
    "maximum",
    "might",
    "arise",
    "mine",
    "right",
    "let",
    "discuss",
    "let",
    "derive",
    "mathematically",
    "see",
    "slide",
    "mathematical",
    "formula",
    "entropy",
    "probability",
    "yes",
    "let",
    "move",
    "see",
    "graph",
    "say",
    "mathematically",
    "suppose",
    "total",
    "sample",
    "space",
    "divided",
    "two",
    "parts",
    "yes",
    "like",
    "data",
    "set",
    "result",
    "playing",
    "divided",
    "two",
    "parts",
    "yes",
    "predict",
    "either",
    "play",
    "right",
    "particular",
    "case",
    "define",
    "formula",
    "entropy",
    "entropy",
    "total",
    "sample",
    "space",
    "equals",
    "negative",
    "probability",
    "e",
    "multiplied",
    "log",
    "probability",
    "yes",
    "whether",
    "base",
    "2",
    "minus",
    "probability",
    "x",
    "log",
    "probability",
    "base",
    "total",
    "sample",
    "space",
    "p",
    "v",
    "probability",
    "e",
    "p",
    "know",
    "probability",
    "well",
    "number",
    "bs",
    "equal",
    "number",
    "know",
    "probability",
    "equals",
    "right",
    "since",
    "equal",
    "number",
    "bs",
    "know",
    "case",
    "value",
    "entropy",
    "one",
    "put",
    "value",
    "right",
    "let",
    "move",
    "next",
    "slide",
    "show",
    "alright",
    "next",
    "contains",
    "yes",
    "know",
    "probability",
    "sample",
    "space",
    "either",
    "1",
    "0",
    "case",
    "entropy",
    "equal",
    "0",
    "let",
    "see",
    "mathematically",
    "one",
    "one",
    "let",
    "start",
    "first",
    "condition",
    "probability",
    "formula",
    "entropy",
    "right",
    "first",
    "case",
    "right",
    "discuss",
    "art",
    "probability",
    "vs",
    "equal",
    "probability",
    "node",
    "data",
    "set",
    "rule",
    "number",
    "yes",
    "right",
    "probability",
    "yes",
    "equal",
    "probability",
    "equals",
    "words",
    "say",
    "yes",
    "plus",
    "equal",
    "total",
    "sample",
    "space",
    "right",
    "since",
    "probability",
    "put",
    "values",
    "formula",
    "get",
    "something",
    "like",
    "calculate",
    "get",
    "entropy",
    "total",
    "sample",
    "space",
    "one",
    "right",
    "let",
    "see",
    "next",
    "case",
    "next",
    "case",
    "either",
    "totally",
    "us",
    "total",
    "yes",
    "let",
    "see",
    "formula",
    "total",
    "yes",
    "yes",
    "0",
    "fine",
    "probability",
    "e",
    "equal",
    "one",
    "yes",
    "total",
    "sample",
    "space",
    "obviously",
    "formula",
    "put",
    "thing",
    "get",
    "entropy",
    "sample",
    "space",
    "equal",
    "negative",
    "x",
    "1",
    "multiplied",
    "log",
    "1",
    "value",
    "log",
    "1",
    "equals",
    "total",
    "thing",
    "result",
    "0",
    "similarly",
    "case",
    "even",
    "case",
    "get",
    "entropy",
    "total",
    "sample",
    "case",
    "0",
    "entropy",
    "right",
    "next",
    "information",
    "gain",
    "well",
    "information",
    "gain",
    "measures",
    "reduction",
    "entropy",
    "decides",
    "attribute",
    "selected",
    "decision",
    "node",
    "total",
    "collection",
    "information",
    "gain",
    "equals",
    "entropy",
    "calculated",
    "weighted",
    "average",
    "multiplied",
    "entropy",
    "feature",
    "worry",
    "see",
    "calculate",
    "example",
    "right",
    "let",
    "manually",
    "build",
    "decision",
    "tree",
    "data",
    "set",
    "data",
    "set",
    "consists",
    "14",
    "different",
    "instances",
    "nine",
    "yes",
    "five",
    "know",
    "like",
    "formula",
    "entropy",
    "put",
    "since",
    "9",
    "years",
    "total",
    "probability",
    "e",
    "equals",
    "9",
    "14",
    "total",
    "probability",
    "equals",
    "phi",
    "14",
    "put",
    "value",
    "calculate",
    "result",
    "get",
    "value",
    "oh",
    "entropy",
    "right",
    "first",
    "step",
    "compute",
    "entropy",
    "entire",
    "data",
    "set",
    "right",
    "select",
    "outlook",
    "temperature",
    "humidity",
    "windy",
    "node",
    "select",
    "root",
    "node",
    "big",
    "question",
    "right",
    "decide",
    "particular",
    "node",
    "chosen",
    "base",
    "note",
    "basis",
    "creating",
    "entire",
    "tree",
    "select",
    "let",
    "see",
    "one",
    "one",
    "calculate",
    "entropy",
    "information",
    "gain",
    "front",
    "note",
    "starting",
    "outlook",
    "outlook",
    "three",
    "different",
    "parameters",
    "sunny",
    "overcast",
    "rainy",
    "first",
    "select",
    "many",
    "number",
    "years",
    "case",
    "sunny",
    "like",
    "sunny",
    "many",
    "number",
    "years",
    "many",
    "number",
    "nodes",
    "total",
    "yes",
    "three",
    "nos",
    "case",
    "sunny",
    "case",
    "overcast",
    "yes",
    "overcast",
    "surely",
    "go",
    "play",
    "like",
    "alright",
    "next",
    "rainy",
    "total",
    "number",
    "vs",
    "equal",
    "three",
    "total",
    "number",
    "equals",
    "2",
    "fine",
    "next",
    "calculate",
    "entropy",
    "feature",
    "calculating",
    "entropy",
    "outlook",
    "equals",
    "sunny",
    "first",
    "assuming",
    "outlook",
    "root",
    "node",
    "calculating",
    "information",
    "gain",
    "alright",
    "order",
    "calculate",
    "information",
    "gain",
    "remember",
    "formula",
    "entropy",
    "total",
    "sample",
    "space",
    "weighted",
    "average",
    "x",
    "entropy",
    "feature",
    "right",
    "calculating",
    "entropy",
    "look",
    "sunny",
    "total",
    "number",
    "yes",
    "sunny",
    "total",
    "number",
    "know",
    "three",
    "fine",
    "let",
    "put",
    "formula",
    "since",
    "probability",
    "yes",
    "2",
    "5",
    "probability",
    "3",
    "get",
    "something",
    "like",
    "alright",
    "getting",
    "entropy",
    "sunny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "fine",
    "next",
    "calculate",
    "entropy",
    "overcast",
    "overcast",
    "remember",
    "yes",
    "right",
    "probability",
    "yes",
    "equal",
    "1",
    "put",
    "get",
    "value",
    "entropy",
    "0",
    "fine",
    "rainy",
    "rainy",
    "3s",
    "nose",
    "probability",
    "e",
    "case",
    "sonny",
    "3",
    "5",
    "probability",
    "know",
    "case",
    "sonny",
    "2",
    "add",
    "value",
    "probability",
    "vs",
    "probability",
    "formula",
    "get",
    "entropy",
    "sunny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "point",
    "calculate",
    "much",
    "information",
    "getting",
    "outlook",
    "equals",
    "weighted",
    "average",
    "right",
    "diverge",
    "total",
    "number",
    "years",
    "total",
    "number",
    "fine",
    "information",
    "outlook",
    "equals",
    "5",
    "14",
    "5",
    "came",
    "calculating",
    "total",
    "number",
    "sample",
    "space",
    "within",
    "particular",
    "outlook",
    "sunny",
    "right",
    "case",
    "sunny",
    "two",
    "years",
    "three",
    "nos",
    "right",
    "weighted",
    "average",
    "sonny",
    "would",
    "equal",
    "5",
    "right",
    "since",
    "formula",
    "five",
    "14",
    "x",
    "entropy",
    "feature",
    "right",
    "calculated",
    "entropy",
    "sonny",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "right",
    "multiply",
    "5",
    "14",
    "one",
    "right",
    "well",
    "calculation",
    "information",
    "outlook",
    "equal",
    "sunny",
    "outlook",
    "even",
    "equals",
    "overcast",
    "rainy",
    "case",
    "similarly",
    "calculate",
    "everything",
    "overcast",
    "sunny",
    "overcast",
    "weighted",
    "averages",
    "14",
    "multiplied",
    "entropy",
    "0",
    "sonny",
    "phi",
    "yes",
    "knows",
    "x",
    "entropy",
    "zero",
    "point",
    "nine",
    "seven",
    "one",
    "finally",
    "take",
    "sum",
    "equals",
    "right",
    "next",
    "calculate",
    "information",
    "gained",
    "earlier",
    "information",
    "taken",
    "outlook",
    "calculating",
    "information",
    "gaining",
    "outlook",
    "right",
    "information",
    "gain",
    "equals",
    "total",
    "entropy",
    "minus",
    "information",
    "taken",
    "outlook",
    "right",
    "total",
    "entropy",
    "information",
    "took",
    "outlook",
    "value",
    "information",
    "gained",
    "outlook",
    "results",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "right",
    "next",
    "let",
    "assume",
    "wendy",
    "root",
    "node",
    "wendy",
    "consists",
    "two",
    "parameters",
    "false",
    "true",
    "let",
    "see",
    "many",
    "years",
    "many",
    "nodes",
    "case",
    "true",
    "false",
    "wendy",
    "falls",
    "parameter",
    "case",
    "six",
    "years",
    "knows",
    "true",
    "parameter",
    "3",
    "3",
    "nodes",
    "right",
    "let",
    "move",
    "ahead",
    "similarly",
    "calculate",
    "information",
    "taken",
    "wendy",
    "finally",
    "calculate",
    "information",
    "gained",
    "wendy",
    "alright",
    "first",
    "calculate",
    "entropy",
    "feature",
    "starting",
    "windy",
    "equal",
    "true",
    "case",
    "true",
    "equal",
    "number",
    "yes",
    "equal",
    "number",
    "remember",
    "graph",
    "probability",
    "total",
    "number",
    "years",
    "equal",
    "total",
    "number",
    "know",
    "case",
    "entropy",
    "equals",
    "1",
    "directly",
    "write",
    "entropy",
    "room",
    "windy",
    "one",
    "already",
    "proved",
    "probability",
    "equals",
    "entropy",
    "maximum",
    "equals",
    "right",
    "next",
    "entropy",
    "false",
    "windy",
    "right",
    "similarly",
    "put",
    "probability",
    "yes",
    "formula",
    "calculate",
    "result",
    "since",
    "six",
    "years",
    "two",
    "nodes",
    "total",
    "get",
    "probability",
    "e",
    "s6",
    "8",
    "probability",
    "know",
    "two",
    "eight",
    "right",
    "calculate",
    "get",
    "entropy",
    "false",
    "zero",
    "point",
    "eight",
    "one",
    "one",
    "alright",
    "let",
    "calculate",
    "information",
    "windy",
    "total",
    "information",
    "collected",
    "windy",
    "equals",
    "information",
    "taken",
    "wendy",
    "equal",
    "true",
    "plus",
    "information",
    "taken",
    "equals",
    "false",
    "calculate",
    "weighted",
    "average",
    "one",
    "sum",
    "finally",
    "get",
    "total",
    "information",
    "taken",
    "windy",
    "case",
    "equals",
    "8",
    "14",
    "multiplied",
    "1",
    "1",
    "6",
    "14",
    "x",
    "1",
    "8",
    "total",
    "number",
    "yes",
    "case",
    "equals",
    "false",
    "right",
    "false",
    "total",
    "number",
    "bs",
    "equals",
    "6",
    "total",
    "know",
    "equal",
    "2",
    "herbs",
    "right",
    "weighted",
    "average",
    "results",
    "aid",
    "14",
    "similarly",
    "information",
    "taken",
    "windy",
    "equals",
    "true",
    "equals",
    "3",
    "plus",
    "3",
    "3",
    "3",
    "equal",
    "6",
    "divided",
    "total",
    "number",
    "sample",
    "space",
    "14",
    "x",
    "entropy",
    "true",
    "right",
    "14",
    "multiplied",
    "1",
    "1",
    "plus",
    "6",
    "14",
    "x",
    "one",
    "results",
    "information",
    "taken",
    "windy",
    "right",
    "much",
    "information",
    "gaining",
    "wendy",
    "total",
    "information",
    "gained",
    "windy",
    "equals",
    "total",
    "entropy",
    "information",
    "taken",
    "windy",
    "right",
    "equals",
    "zero",
    "point",
    "zero",
    "four",
    "eight",
    "information",
    "gained",
    "windy",
    "right",
    "similarly",
    "calculated",
    "rest",
    "right",
    "outlook",
    "see",
    "information",
    "information",
    "gain",
    "zero",
    "point",
    "two",
    "four",
    "seven",
    "case",
    "temperature",
    "information",
    "around",
    "zero",
    "point",
    "nine",
    "one",
    "one",
    "information",
    "gain",
    "equal",
    "9",
    "case",
    "humidity",
    "information",
    "gained",
    "case",
    "windy",
    "information",
    "gained",
    "select",
    "attribute",
    "maximum",
    "fine",
    "selected",
    "outlook",
    "root",
    "node",
    "subdivided",
    "three",
    "different",
    "parts",
    "sunny",
    "overcast",
    "rain",
    "case",
    "overcast",
    "seen",
    "consists",
    "yes",
    "consider",
    "leaf",
    "node",
    "case",
    "sunny",
    "rainy",
    "doubtful",
    "consists",
    "yes",
    "know",
    "need",
    "recalculate",
    "things",
    "right",
    "node",
    "recalculate",
    "things",
    "right",
    "select",
    "attribute",
    "maximum",
    "information",
    "gain",
    "right",
    "complete",
    "tree",
    "look",
    "like",
    "right",
    "let",
    "see",
    "play",
    "play",
    "outlook",
    "overcast",
    "right",
    "case",
    "always",
    "play",
    "outlook",
    "sunny",
    "drill",
    "check",
    "humidity",
    "condition",
    "right",
    "humidity",
    "normal",
    "play",
    "humidity",
    "high",
    "wo",
    "play",
    "right",
    "outlook",
    "predicts",
    "rainy",
    "check",
    "whether",
    "windy",
    "week",
    "went",
    "go",
    "offer",
    "say",
    "strong",
    "wind",
    "wo",
    "play",
    "right",
    "entire",
    "decision",
    "tree",
    "would",
    "look",
    "like",
    "end",
    "comes",
    "concept",
    "pruning",
    "say",
    "play",
    "well",
    "pruning",
    "pruning",
    "decide",
    "play",
    "pruning",
    "well",
    "pruning",
    "nothing",
    "cutting",
    "nodes",
    "order",
    "get",
    "optimal",
    "solution",
    "right",
    "pruning",
    "reduces",
    "complexity",
    "right",
    "see",
    "screen",
    "showing",
    "result",
    "showing",
    "result",
    "says",
    "play",
    "right",
    "drill",
    "practical",
    "session",
    "common",
    "question",
    "might",
    "come",
    "mind",
    "might",
    "think",
    "tree",
    "base",
    "model",
    "better",
    "cleaner",
    "model",
    "right",
    "think",
    "like",
    "use",
    "logistic",
    "regression",
    "classification",
    "problem",
    "linear",
    "regression",
    "regression",
    "problem",
    "need",
    "use",
    "tree",
    "well",
    "many",
    "us",
    "mind",
    "well",
    "valid",
    "question",
    "well",
    "actually",
    "said",
    "earlier",
    "use",
    "algorithm",
    "depends",
    "type",
    "problem",
    "solving",
    "let",
    "look",
    "key",
    "factor",
    "help",
    "decide",
    "algorithm",
    "use",
    "first",
    "point",
    "relationship",
    "dependent",
    "independent",
    "variable",
    "well",
    "approximated",
    "linear",
    "model",
    "linear",
    "regression",
    "outperform",
    "tree",
    "base",
    "model",
    "second",
    "case",
    "high",
    "complex",
    "relationship",
    "lent",
    "independent",
    "variables",
    "remodel",
    "outperform",
    "classical",
    "regression",
    "model",
    "third",
    "case",
    "need",
    "build",
    "model",
    "easy",
    "explain",
    "people",
    "decision",
    "tree",
    "model",
    "always",
    "better",
    "linear",
    "model",
    "decision",
    "tree",
    "models",
    "simpler",
    "interpret",
    "linear",
    "regression",
    "right",
    "let",
    "move",
    "ahead",
    "see",
    "write",
    "gentry",
    "classifier",
    "scratch",
    "python",
    "using",
    "card",
    "algorithm",
    "right",
    "using",
    "jupyter",
    "notebook",
    "python",
    "oh",
    "install",
    "alright",
    "let",
    "open",
    "anaconda",
    "jupyter",
    "notebook",
    "whereas",
    "inner",
    "corner",
    "navigator",
    "directly",
    "jump",
    "jupyter",
    "notebook",
    "hit",
    "launch",
    "button",
    "guess",
    "everyone",
    "knows",
    "jupyter",
    "notebook",
    "interactive",
    "computing",
    "notebook",
    "environment",
    "run",
    "python",
    "codes",
    "jupyter",
    "notebook",
    "opens",
    "local",
    "host",
    "double",
    "8",
    "9",
    "1",
    "using",
    "jupyter",
    "notebook",
    "order",
    "write",
    "decision",
    "tree",
    "classifier",
    "using",
    "python",
    "decision",
    "tree",
    "classifier",
    "already",
    "written",
    "set",
    "codes",
    "let",
    "explain",
    "one",
    "one",
    "start",
    "initializing",
    "training",
    "data",
    "set",
    "sample",
    "data",
    "set",
    "row",
    "example",
    "last",
    "column",
    "label",
    "first",
    "two",
    "columns",
    "features",
    "want",
    "add",
    "features",
    "example",
    "practice",
    "interesting",
    "fact",
    "data",
    "set",
    "designed",
    "way",
    "second",
    "fifth",
    "example",
    "almost",
    "features",
    "different",
    "labels",
    "right",
    "let",
    "move",
    "see",
    "tree",
    "handles",
    "case",
    "see",
    "second",
    "fifth",
    "column",
    "features",
    "different",
    "label",
    "right",
    "let",
    "move",
    "ahead",
    "training",
    "data",
    "set",
    "next",
    "adding",
    "column",
    "labels",
    "used",
    "print",
    "trees",
    "fine",
    "add",
    "header",
    "columns",
    "like",
    "first",
    "column",
    "color",
    "second",
    "diameter",
    "third",
    "label",
    "column",
    "alright",
    "next",
    "road",
    "define",
    "function",
    "unique",
    "values",
    "pass",
    "rows",
    "columns",
    "function",
    "find",
    "unique",
    "values",
    "column",
    "data",
    "set",
    "example",
    "passing",
    "training",
    "data",
    "hazard",
    "row",
    "column",
    "number",
    "0",
    "finding",
    "unique",
    "values",
    "terms",
    "color",
    "since",
    "row",
    "training",
    "data",
    "column",
    "1",
    "finding",
    "unique",
    "values",
    "terms",
    "diameter",
    "fine",
    "example",
    "next",
    "define",
    "function",
    "class",
    "count",
    "pass",
    "zeros",
    "counts",
    "number",
    "type",
    "example",
    "within",
    "data",
    "set",
    "function",
    "basically",
    "counting",
    "number",
    "type",
    "example",
    "data",
    "set",
    "counting",
    "unique",
    "values",
    "label",
    "data",
    "set",
    "sample",
    "see",
    "pass",
    "entire",
    "training",
    "data",
    "set",
    "particular",
    "function",
    "class",
    "underscore",
    "count",
    "find",
    "different",
    "types",
    "label",
    "within",
    "training",
    "data",
    "set",
    "see",
    "unique",
    "label",
    "consists",
    "mango",
    "grape",
    "lemon",
    "next",
    "define",
    "function",
    "numeric",
    "pass",
    "value",
    "test",
    "value",
    "numeric",
    "return",
    "value",
    "integer",
    "float",
    "example",
    "see",
    "numeric",
    "passing",
    "7",
    "integer",
    "return",
    "value",
    "passing",
    "red",
    "numeric",
    "value",
    "right",
    "moving",
    "ahead",
    "define",
    "class",
    "named",
    "question",
    "question",
    "question",
    "used",
    "partition",
    "data",
    "set",
    "class",
    "voted",
    "records",
    "column",
    "number",
    "example",
    "0",
    "color",
    "light",
    "column",
    "value",
    "example",
    "green",
    "next",
    "defining",
    "match",
    "method",
    "used",
    "compare",
    "feature",
    "value",
    "example",
    "feature",
    "values",
    "stored",
    "question",
    "let",
    "see",
    "first",
    "defining",
    "init",
    "function",
    "inside",
    "passing",
    "self",
    "column",
    "value",
    "parameter",
    "next",
    "define",
    "function",
    "match",
    "compares",
    "feature",
    "value",
    "example",
    "feature",
    "value",
    "question",
    "next",
    "define",
    "function",
    "pr",
    "helper",
    "method",
    "print",
    "question",
    "readable",
    "format",
    "next",
    "defining",
    "function",
    "partition",
    "well",
    "function",
    "used",
    "partition",
    "data",
    "set",
    "row",
    "data",
    "set",
    "checks",
    "matched",
    "question",
    "adds",
    "true",
    "rose",
    "adds",
    "false",
    "rose",
    "right",
    "example",
    "see",
    "partition",
    "training",
    "data",
    "set",
    "based",
    "whether",
    "rows",
    "ready",
    "calling",
    "function",
    "question",
    "passing",
    "value",
    "zero",
    "read",
    "assign",
    "red",
    "rose",
    "true",
    "underscore",
    "rose",
    "everything",
    "else",
    "assigned",
    "false",
    "underscore",
    "rose",
    "fine",
    "next",
    "define",
    "gini",
    "impurity",
    "function",
    "inside",
    "pass",
    "list",
    "rows",
    "calculate",
    "dream",
    "purity",
    "list",
    "rows",
    "next",
    "every",
    "defining",
    "function",
    "information",
    "gain",
    "information",
    "gain",
    "function",
    "calculates",
    "information",
    "gain",
    "using",
    "uncertainty",
    "starting",
    "node",
    "weighted",
    "impurity",
    "child",
    "node",
    "next",
    "function",
    "find",
    "best",
    "plate",
    "well",
    "function",
    "used",
    "find",
    "best",
    "question",
    "ask",
    "iterating",
    "every",
    "feature",
    "value",
    "calculating",
    "information",
    "game",
    "detail",
    "explanation",
    "code",
    "find",
    "code",
    "description",
    "given",
    "right",
    "next",
    "define",
    "class",
    "leave",
    "classifying",
    "data",
    "holds",
    "dictionary",
    "glass",
    "like",
    "mango",
    "many",
    "times",
    "appears",
    "row",
    "training",
    "data",
    "reaches",
    "sleeve",
    "alright",
    "next",
    "decision",
    "node",
    "decision",
    "node",
    "ask",
    "question",
    "holds",
    "reference",
    "question",
    "two",
    "child",
    "nodes",
    "base",
    "deciding",
    "node",
    "add",
    "branch",
    "alright",
    "next",
    "video",
    "defining",
    "function",
    "beltre",
    "inside",
    "passing",
    "number",
    "rows",
    "function",
    "used",
    "build",
    "tree",
    "initially",
    "define",
    "various",
    "function",
    "using",
    "order",
    "build",
    "tree",
    "let",
    "start",
    "partitioning",
    "data",
    "set",
    "unique",
    "attribute",
    "calculate",
    "information",
    "gain",
    "return",
    "question",
    "produces",
    "highest",
    "gain",
    "basis",
    "split",
    "tree",
    "partitioning",
    "data",
    "set",
    "calculating",
    "information",
    "gain",
    "returning",
    "returning",
    "question",
    "producing",
    "highest",
    "gain",
    "right",
    "gain",
    "equals",
    "0",
    "return",
    "leaf",
    "rose",
    "getting",
    "gain",
    "gain",
    "equals",
    "0",
    "case",
    "since",
    "question",
    "could",
    "asked",
    "return",
    "leaf",
    "fine",
    "true",
    "underscore",
    "rose",
    "false",
    "underscore",
    "rose",
    "equal",
    "partition",
    "rose",
    "question",
    "reaching",
    "tell",
    "position",
    "already",
    "found",
    "value",
    "used",
    "partition",
    "data",
    "set",
    "recursively",
    "build",
    "true",
    "branch",
    "similarly",
    "recursively",
    "build",
    "false",
    "branch",
    "return",
    "division",
    "discord",
    "node",
    "side",
    "passing",
    "question",
    "true",
    "branch",
    "false",
    "branch",
    "return",
    "question",
    "node",
    "question",
    "node",
    "recalls",
    "best",
    "feature",
    "value",
    "ask",
    "point",
    "fine",
    "builder",
    "tree",
    "next",
    "define",
    "print",
    "underscore",
    "tree",
    "function",
    "used",
    "print",
    "tree",
    "fine",
    "finally",
    "particular",
    "function",
    "printing",
    "tree",
    "next",
    "classify",
    "function",
    "use",
    "decide",
    "whether",
    "follow",
    "true",
    "branch",
    "false",
    "branch",
    "compared",
    "feature",
    "values",
    "stored",
    "node",
    "example",
    "considering",
    "last",
    "finally",
    "print",
    "production",
    "leaf",
    "let",
    "execute",
    "see",
    "okay",
    "testing",
    "data",
    "online",
    "printed",
    "leaf",
    "well",
    "trained",
    "algorithm",
    "training",
    "data",
    "set",
    "time",
    "test",
    "testing",
    "data",
    "set",
    "let",
    "finally",
    "execute",
    "see",
    "result",
    "result",
    "get",
    "first",
    "question",
    "asked",
    "algorithm",
    "diameter",
    "greater",
    "equal",
    "3",
    "true",
    "ask",
    "color",
    "yellow",
    "true",
    "predict",
    "mango",
    "one",
    "lemon",
    "one",
    "case",
    "false",
    "predict",
    "mango",
    "true",
    "part",
    "next",
    "coming",
    "diameter",
    "greater",
    "equal",
    "3",
    "case",
    "false",
    "predict",
    "grape",
    "vine",
    "okay",
    "coding",
    "part",
    "let",
    "conclude",
    "session",
    "concluding",
    "let",
    "show",
    "one",
    "thing",
    "algorithm",
    "cheat",
    "sheet",
    "explains",
    "algorithm",
    "use",
    "right",
    "let",
    "build",
    "decision",
    "tree",
    "format",
    "let",
    "see",
    "big",
    "first",
    "condition",
    "check",
    "whether",
    "50",
    "samples",
    "samples",
    "greater",
    "50",
    "move",
    "ahead",
    "less",
    "50",
    "need",
    "collect",
    "data",
    "sample",
    "greater",
    "50",
    "decide",
    "whether",
    "want",
    "predict",
    "category",
    "want",
    "predict",
    "category",
    "see",
    "whether",
    "labeled",
    "data",
    "label",
    "data",
    "would",
    "classification",
    "algorithm",
    "problem",
    "label",
    "data",
    "would",
    "clustering",
    "problem",
    "want",
    "category",
    "want",
    "protect",
    "predict",
    "quantity",
    "well",
    "want",
    "predict",
    "quantity",
    "case",
    "would",
    "regression",
    "problem",
    "want",
    "predict",
    "quantity",
    "want",
    "keep",
    "looking",
    "case",
    "go",
    "dimensionality",
    "reduction",
    "problems",
    "still",
    "want",
    "look",
    "predicting",
    "structure",
    "working",
    "tough",
    "luck",
    "hope",
    "recession",
    "clarifies",
    "doubt",
    "decision",
    "tree",
    "algorithm",
    "try",
    "find",
    "answer",
    "particular",
    "question",
    "need",
    "random",
    "forest",
    "fine",
    "like",
    "human",
    "beings",
    "learn",
    "past",
    "experiences",
    "unlike",
    "human",
    "beings",
    "computer",
    "experiences",
    "machine",
    "takes",
    "decisions",
    "learn",
    "well",
    "computer",
    "system",
    "actually",
    "learns",
    "data",
    "represents",
    "past",
    "experiences",
    "application",
    "domain",
    "let",
    "see",
    "random",
    "forest",
    "building",
    "learning",
    "model",
    "simple",
    "use",
    "case",
    "credit",
    "risk",
    "detection",
    "needless",
    "say",
    "credit",
    "card",
    "companies",
    "nested",
    "interest",
    "identifying",
    "financial",
    "transactions",
    "illegitimate",
    "criminal",
    "nature",
    "also",
    "would",
    "like",
    "mention",
    "point",
    "according",
    "federal",
    "reserve",
    "payments",
    "study",
    "americans",
    "used",
    "credit",
    "cards",
    "pay",
    "twenty",
    "six",
    "point",
    "two",
    "million",
    "purchases",
    "2012",
    "estimated",
    "loss",
    "due",
    "unauthorized",
    "transactions",
    "6",
    "point",
    "1",
    "billion",
    "dollars",
    "banking",
    "industry",
    "measuring",
    "risk",
    "critical",
    "stakes",
    "high",
    "overall",
    "goal",
    "actually",
    "figure",
    "fraudulent",
    "much",
    "financial",
    "damage",
    "done",
    "credit",
    "card",
    "company",
    "receives",
    "thousands",
    "applications",
    "new",
    "cards",
    "application",
    "contains",
    "information",
    "mission",
    "applicant",
    "right",
    "see",
    "applications",
    "actually",
    "figure",
    "predictor",
    "variables",
    "like",
    "marital",
    "status",
    "person",
    "gender",
    "person",
    "age",
    "person",
    "status",
    "actually",
    "whether",
    "default",
    "pair",
    "pair",
    "default",
    "payments",
    "basically",
    "payments",
    "made",
    "time",
    "according",
    "agreement",
    "signed",
    "cardholder",
    "account",
    "actually",
    "set",
    "default",
    "easily",
    "figure",
    "history",
    "particular",
    "card",
    "holder",
    "also",
    "look",
    "time",
    "payment",
    "whether",
    "regular",
    "pair",
    "non",
    "regular",
    "one",
    "source",
    "income",
    "particular",
    "person",
    "forth",
    "minimize",
    "loss",
    "back",
    "actually",
    "needs",
    "certain",
    "decision",
    "rule",
    "predict",
    "whether",
    "approve",
    "particular",
    "one",
    "particular",
    "person",
    "random",
    "forest",
    "actually",
    "comes",
    "picture",
    "right",
    "let",
    "see",
    "random",
    "forest",
    "actually",
    "help",
    "us",
    "particular",
    "scenario",
    "taken",
    "randomly",
    "two",
    "parameters",
    "predictive",
    "variables",
    "saw",
    "previously",
    "taken",
    "two",
    "predictor",
    "variables",
    "first",
    "one",
    "income",
    "second",
    "one",
    "h",
    "right",
    "hurley",
    "parallel",
    "decision",
    "trees",
    "implemented",
    "upon",
    "predicted",
    "variables",
    "let",
    "first",
    "assume",
    "case",
    "income",
    "variable",
    "right",
    "divided",
    "income",
    "three",
    "categories",
    "first",
    "one",
    "person",
    "earning",
    "second",
    "15",
    "35",
    "thousand",
    "dollars",
    "third",
    "one",
    "running",
    "range",
    "0",
    "15",
    "thousand",
    "dollars",
    "person",
    "earning",
    "pretty",
    "good",
    "income",
    "pretty",
    "decent",
    "check",
    "credit",
    "history",
    "probability",
    "person",
    "earning",
    "good",
    "amount",
    "low",
    "risk",
    "wo",
    "able",
    "pay",
    "back",
    "already",
    "earning",
    "good",
    "probability",
    "application",
    "loan",
    "get",
    "approved",
    "right",
    "actually",
    "low",
    "risk",
    "moderate",
    "risk",
    "real",
    "issue",
    "higher",
    "risk",
    "approve",
    "applicants",
    "request",
    "let",
    "move",
    "watch",
    "second",
    "category",
    "person",
    "actually",
    "earning",
    "15",
    "35",
    "thousand",
    "dollars",
    "right",
    "person",
    "may",
    "may",
    "pay",
    "back",
    "scenarios",
    "look",
    "credit",
    "history",
    "previous",
    "history",
    "previous",
    "history",
    "bad",
    "like",
    "default",
    "er",
    "previous",
    "transactions",
    "definitely",
    "consider",
    "approving",
    "request",
    "high",
    "risk",
    "good",
    "bank",
    "previous",
    "history",
    "particular",
    "applicant",
    "really",
    "good",
    "clarify",
    "doubt",
    "consider",
    "another",
    "parameter",
    "well",
    "depth",
    "already",
    "really",
    "high",
    "dip",
    "risks",
    "increases",
    "chances",
    "might",
    "pay",
    "repay",
    "future",
    "accept",
    "request",
    "person",
    "high",
    "dipped",
    "person",
    "low",
    "depth",
    "good",
    "pair",
    "past",
    "history",
    "chances",
    "might",
    "back",
    "consider",
    "approving",
    "request",
    "particular",
    "applicant",
    "alex",
    "look",
    "third",
    "category",
    "person",
    "earning",
    "0",
    "15",
    "thousand",
    "dollars",
    "something",
    "actually",
    "raises",
    "broke",
    "person",
    "actually",
    "lie",
    "category",
    "high",
    "risk",
    "right",
    "probability",
    "application",
    "loan",
    "would",
    "probably",
    "get",
    "rejected",
    "get",
    "one",
    "final",
    "outcome",
    "income",
    "parameter",
    "right",
    "let",
    "us",
    "look",
    "second",
    "variable",
    "h",
    "lead",
    "second",
    "decision",
    "tree",
    "let",
    "us",
    "say",
    "person",
    "young",
    "right",
    "look",
    "forward",
    "student",
    "student",
    "chances",
    "high",
    "wo",
    "able",
    "repay",
    "back",
    "earning",
    "source",
    "right",
    "risks",
    "high",
    "probability",
    "application",
    "loan",
    "get",
    "rejected",
    "fine",
    "person",
    "young",
    "student",
    "probably",
    "go",
    "look",
    "another",
    "variable",
    "pan",
    "balance",
    "let",
    "look",
    "bank",
    "balance",
    "less",
    "5",
    "lakhs",
    "risk",
    "arises",
    "probabilities",
    "application",
    "loan",
    "get",
    "rejected",
    "person",
    "young",
    "student",
    "bank",
    "balance",
    "greater",
    "5",
    "lakhs",
    "got",
    "pretty",
    "good",
    "stable",
    "balanced",
    "probability",
    "sort",
    "application",
    "get",
    "approved",
    "let",
    "us",
    "take",
    "another",
    "scenario",
    "senior",
    "right",
    "senior",
    "probably",
    "go",
    "check",
    "credit",
    "history",
    "well",
    "previous",
    "transactions",
    "kind",
    "person",
    "like",
    "whether",
    "defaulter",
    "ananda",
    "falter",
    "fair",
    "kind",
    "person",
    "previous",
    "transactions",
    "risk",
    "arises",
    "probability",
    "application",
    "getting",
    "rejected",
    "actually",
    "increases",
    "right",
    "excellent",
    "person",
    "per",
    "transactions",
    "previous",
    "history",
    "least",
    "risk",
    "probabilities",
    "application",
    "loan",
    "get",
    "approved",
    "two",
    "variables",
    "income",
    "age",
    "led",
    "two",
    "different",
    "decision",
    "trees",
    "right",
    "two",
    "different",
    "decision",
    "trees",
    "actually",
    "led",
    "two",
    "different",
    "results",
    "random",
    "forest",
    "actually",
    "compile",
    "two",
    "different",
    "results",
    "two",
    "different",
    "gentry",
    "finally",
    "lead",
    "final",
    "outcome",
    "random",
    "forest",
    "actually",
    "works",
    "right",
    "actually",
    "motive",
    "random",
    "forest",
    "let",
    "us",
    "move",
    "forward",
    "see",
    "random",
    "forest",
    "right",
    "get",
    "idea",
    "mechanism",
    "name",
    "random",
    "forests",
    "collection",
    "trees",
    "fortress",
    "called",
    "probably",
    "also",
    "trees",
    "actually",
    "trained",
    "subsets",
    "selected",
    "random",
    "therefore",
    "called",
    "random",
    "forests",
    "random",
    "forests",
    "collection",
    "insane",
    "humble",
    "decision",
    "trees",
    "right",
    "decision",
    "trees",
    "actually",
    "built",
    "using",
    "whole",
    "data",
    "set",
    "considering",
    "features",
    "actually",
    "random",
    "forest",
    "fraction",
    "number",
    "rows",
    "selected",
    "random",
    "particular",
    "number",
    "features",
    "actually",
    "selected",
    "random",
    "trained",
    "upon",
    "decision",
    "trees",
    "built",
    "upon",
    "right",
    "similarly",
    "number",
    "decision",
    "trees",
    "grown",
    "decision",
    "tree",
    "salt",
    "certain",
    "final",
    "outcome",
    "random",
    "forest",
    "nothing",
    "actually",
    "compiled",
    "results",
    "decision",
    "trees",
    "bring",
    "final",
    "result",
    "see",
    "particular",
    "figure",
    "particular",
    "instance",
    "actually",
    "resulted",
    "three",
    "different",
    "decision",
    "trees",
    "right",
    "tree",
    "one",
    "results",
    "final",
    "outcome",
    "called",
    "class",
    "tree",
    "results",
    "class",
    "similarly",
    "tree",
    "three",
    "results",
    "class",
    "p",
    "random",
    "forest",
    "compile",
    "results",
    "decision",
    "trees",
    "go",
    "call",
    "majority",
    "voting",
    "since",
    "head",
    "decision",
    "trees",
    "actually",
    "voted",
    "favor",
    "class",
    "b",
    "decision",
    "tree",
    "2",
    "therefore",
    "final",
    "outcome",
    "favor",
    "class",
    "random",
    "forest",
    "actually",
    "works",
    "upon",
    "one",
    "really",
    "beautiful",
    "thing",
    "particular",
    "algorithm",
    "one",
    "versatile",
    "algorithms",
    "capable",
    "performing",
    "regression",
    "well",
    "let",
    "try",
    "understand",
    "random",
    "forest",
    "beautiful",
    "example",
    "favorite",
    "one",
    "let",
    "say",
    "want",
    "decide",
    "want",
    "watch",
    "edge",
    "tomorrow",
    "right",
    "particular",
    "scenario",
    "two",
    "different",
    "actions",
    "work",
    "bond",
    "either",
    "straight",
    "away",
    "go",
    "best",
    "friend",
    "asked",
    "right",
    "whether",
    "go",
    "edge",
    "tomorrow",
    "like",
    "movie",
    "ask",
    "friends",
    "take",
    "opinion",
    "consideration",
    "based",
    "final",
    "results",
    "go",
    "watch",
    "edge",
    "tomorrow",
    "right",
    "let",
    "take",
    "first",
    "scenario",
    "go",
    "best",
    "friend",
    "asked",
    "whether",
    "go",
    "watch",
    "edge",
    "tomorrow",
    "friend",
    "probably",
    "ask",
    "certain",
    "questions",
    "like",
    "first",
    "one",
    "jonah",
    "let",
    "say",
    "friend",
    "asks",
    "really",
    "like",
    "adventurous",
    "kind",
    "movies",
    "say",
    "yes",
    "definitely",
    "would",
    "love",
    "watch",
    "venture",
    "kind",
    "movie",
    "probabilities",
    "like",
    "edge",
    "tomorrow",
    "well",
    "since",
    "age",
    "tomorrow",
    "also",
    "movie",
    "adventure",
    "kind",
    "journal",
    "right",
    "let",
    "say",
    "like",
    "adventure",
    "john",
    "movie",
    "probability",
    "reduces",
    "might",
    "really",
    "like",
    "edge",
    "morrow",
    "right",
    "come",
    "certain",
    "conclusion",
    "right",
    "let",
    "say",
    "best",
    "friend",
    "puts",
    "another",
    "situation",
    "ask",
    "like",
    "emily",
    "blunt",
    "see",
    "definitely",
    "like",
    "emily",
    "blunt",
    "puts",
    "another",
    "question",
    "like",
    "emily",
    "blunt",
    "main",
    "lead",
    "say",
    "yes",
    "probability",
    "arises",
    "definitely",
    "like",
    "edge",
    "tomorrow",
    "well",
    "edge",
    "tomorrow",
    "emily",
    "plant",
    "main",
    "lead",
    "cast",
    "say",
    "oh",
    "like",
    "emily",
    "blunt",
    "probability",
    "reduces",
    "would",
    "like",
    "edge",
    "tomorrow",
    "write",
    "one",
    "way",
    "one",
    "decision",
    "tree",
    "final",
    "outcome",
    "final",
    "decision",
    "based",
    "one",
    "decision",
    "tree",
    "see",
    "final",
    "outcome",
    "based",
    "one",
    "friend",
    "definitely",
    "really",
    "convinced",
    "want",
    "consider",
    "options",
    "friends",
    "also",
    "make",
    "precise",
    "crisp",
    "decision",
    "right",
    "go",
    "approach",
    "bunch",
    "friends",
    "let",
    "say",
    "go",
    "three",
    "friends",
    "ask",
    "question",
    "whether",
    "would",
    "like",
    "watch",
    "tomorrow",
    "go",
    "approach",
    "three",
    "four",
    "friends",
    "friend",
    "one",
    "friend",
    "twin",
    "friend",
    "three",
    "consider",
    "sport",
    "decision",
    "dependent",
    "compiled",
    "results",
    "three",
    "friends",
    "right",
    "let",
    "say",
    "go",
    "first",
    "friend",
    "ask",
    "whether",
    "would",
    "like",
    "watch",
    "tomorrow",
    "first",
    "friend",
    "puts",
    "one",
    "question",
    "like",
    "top",
    "gun",
    "say",
    "yes",
    "definitely",
    "like",
    "movie",
    "top",
    "gun",
    "probabilities",
    "would",
    "like",
    "edge",
    "tomorrow",
    "well",
    "topgun",
    "actually",
    "military",
    "action",
    "drama",
    "also",
    "tom",
    "cruise",
    "probability",
    "rises",
    "yes",
    "like",
    "edge",
    "tomorrow",
    "well",
    "say",
    "like",
    "top",
    "gun",
    "chances",
    "would",
    "like",
    "edge",
    "tomorrow",
    "right",
    "another",
    "question",
    "puts",
    "across",
    "really",
    "like",
    "watch",
    "action",
    "movies",
    "say",
    "yes",
    "would",
    "love",
    "watch",
    "chances",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "friend",
    "come",
    "one",
    "conclusion",
    "since",
    "ratio",
    "liking",
    "movie",
    "like",
    "actually",
    "2",
    "1",
    "final",
    "result",
    "actually",
    "would",
    "like",
    "edge",
    "tomorrow",
    "go",
    "second",
    "friend",
    "ask",
    "question",
    "second",
    "friend",
    "asks",
    "like",
    "far",
    "away",
    "went",
    "last",
    "time",
    "washed",
    "say",
    "really",
    "like",
    "far",
    "away",
    "would",
    "say",
    "definitely",
    "going",
    "like",
    "edge",
    "tomorrow",
    "far",
    "away",
    "actually",
    "since",
    "might",
    "knowing",
    "far",
    "ways",
    "johner",
    "romance",
    "revolves",
    "around",
    "girl",
    "guy",
    "falling",
    "love",
    "probability",
    "would",
    "like",
    "edge",
    "tomorrow",
    "ask",
    "another",
    "question",
    "like",
    "bolivian",
    "really",
    "like",
    "watch",
    "tom",
    "cruise",
    "say",
    "yes",
    "probability",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "oblivion",
    "science",
    "fiction",
    "casting",
    "tom",
    "cruise",
    "full",
    "strange",
    "experiences",
    "tom",
    "cruise",
    "savior",
    "masses",
    "kind",
    "well",
    "kind",
    "plot",
    "edge",
    "tomorrow",
    "well",
    "pure",
    "yes",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "get",
    "another",
    "second",
    "decision",
    "second",
    "friend",
    "go",
    "third",
    "friend",
    "ask",
    "probably",
    "third",
    "friend",
    "really",
    "interesting",
    "sort",
    "conversation",
    "say",
    "simply",
    "asks",
    "like",
    "godzilla",
    "said",
    "like",
    "godzilla",
    "said",
    "definitely",
    "would",
    "like",
    "tomorrow",
    "godzilla",
    "also",
    "actually",
    "sign",
    "fiction",
    "movie",
    "adventure",
    "jonah",
    "got",
    "three",
    "results",
    "three",
    "different",
    "decision",
    "trees",
    "three",
    "different",
    "friends",
    "compile",
    "results",
    "friends",
    "make",
    "final",
    "call",
    "yes",
    "would",
    "like",
    "watch",
    "edge",
    "tomorrow",
    "real",
    "time",
    "interesting",
    "example",
    "actually",
    "implement",
    "random",
    "forest",
    "ground",
    "reality",
    "right",
    "questions",
    "far",
    "far",
    "good",
    "move",
    "forward",
    "let",
    "us",
    "look",
    "various",
    "domains",
    "random",
    "forest",
    "actually",
    "used",
    "diversity",
    "random",
    "forest",
    "actually",
    "used",
    "various",
    "diverse",
    "means",
    "like",
    "beat",
    "banking",
    "beat",
    "medicine",
    "beat",
    "land",
    "use",
    "beat",
    "marketing",
    "name",
    "random",
    "forest",
    "banking",
    "particularly",
    "random",
    "forest",
    "actually",
    "used",
    "make",
    "whether",
    "applicant",
    "default",
    "pair",
    "non",
    "default",
    "1",
    "accordingly",
    "approve",
    "reject",
    "applications",
    "loan",
    "right",
    "random",
    "forest",
    "used",
    "banking",
    "talking",
    "medicine",
    "random",
    "forest",
    "widely",
    "used",
    "medicine",
    "field",
    "predict",
    "beforehand",
    "probability",
    "person",
    "actually",
    "particular",
    "disease",
    "right",
    "actually",
    "used",
    "look",
    "various",
    "disease",
    "trends",
    "let",
    "say",
    "want",
    "figure",
    "probability",
    "person",
    "diabetes",
    "would",
    "probably",
    "look",
    "medical",
    "history",
    "patient",
    "see",
    "read",
    "glucose",
    "concentration",
    "bmi",
    "insulin",
    "levels",
    "patient",
    "past",
    "previous",
    "three",
    "months",
    "age",
    "particular",
    "person",
    "make",
    "different",
    "decision",
    "trees",
    "based",
    "one",
    "predictor",
    "variables",
    "finally",
    "compiled",
    "results",
    "variables",
    "make",
    "fine",
    "final",
    "decision",
    "whether",
    "person",
    "diabetes",
    "near",
    "future",
    "random",
    "forest",
    "used",
    "medicine",
    "sector",
    "move",
    "random",
    "forest",
    "also",
    "actually",
    "used",
    "find",
    "land",
    "use",
    "example",
    "want",
    "set",
    "particular",
    "industry",
    "certain",
    "area",
    "would",
    "probably",
    "look",
    "look",
    "vegetation",
    "urban",
    "population",
    "right",
    "much",
    "nearest",
    "modes",
    "transport",
    "like",
    "bus",
    "station",
    "railway",
    "station",
    "accordingly",
    "split",
    "parameters",
    "make",
    "decision",
    "one",
    "parameters",
    "finally",
    "compile",
    "decision",
    "parameters",
    "final",
    "outcome",
    "finally",
    "going",
    "predict",
    "whether",
    "put",
    "industry",
    "particular",
    "location",
    "right",
    "three",
    "examples",
    "actually",
    "majorly",
    "around",
    "classification",
    "problem",
    "trying",
    "classify",
    "whether",
    "actually",
    "trying",
    "answer",
    "question",
    "whether",
    "right",
    "let",
    "move",
    "forward",
    "look",
    "marketing",
    "revolving",
    "around",
    "random",
    "forest",
    "particularly",
    "marketing",
    "try",
    "identify",
    "customer",
    "churn",
    "particularly",
    "regression",
    "kind",
    "problem",
    "right",
    "let",
    "see",
    "customer",
    "churn",
    "nothing",
    "actually",
    "number",
    "people",
    "actually",
    "number",
    "customers",
    "losing",
    "going",
    "market",
    "want",
    "identify",
    "customer",
    "churn",
    "near",
    "future",
    "ecommerce",
    "industries",
    "actually",
    "using",
    "like",
    "amazon",
    "flipkart",
    "etc",
    "particularly",
    "look",
    "behavior",
    "past",
    "history",
    "purchasing",
    "history",
    "like",
    "based",
    "activity",
    "around",
    "certain",
    "things",
    "around",
    "certain",
    "ads",
    "around",
    "certain",
    "discounts",
    "around",
    "certain",
    "kind",
    "materials",
    "right",
    "like",
    "particular",
    "top",
    "activity",
    "around",
    "particular",
    "top",
    "track",
    "every",
    "particular",
    "move",
    "try",
    "predict",
    "whether",
    "moving",
    "identify",
    "customer",
    "churn",
    "various",
    "domains",
    "random",
    "forest",
    "used",
    "list",
    "numerous",
    "examples",
    "chile",
    "using",
    "random",
    "forests",
    "makes",
    "special",
    "actually",
    "let",
    "move",
    "forward",
    "see",
    "random",
    "forest",
    "actually",
    "works",
    "right",
    "let",
    "us",
    "start",
    "random",
    "forest",
    "algorithm",
    "first",
    "let",
    "see",
    "step",
    "step",
    "random",
    "forest",
    "algorithm",
    "works",
    "first",
    "step",
    "actually",
    "select",
    "certain",
    "features",
    "less",
    "total",
    "number",
    "predictor",
    "variables",
    "data",
    "set",
    "total",
    "predictor",
    "variables",
    "select",
    "randomly",
    "features",
    "actually",
    "selecting",
    "features",
    "reason",
    "select",
    "predictive",
    "variables",
    "total",
    "predictor",
    "variables",
    "decision",
    "tree",
    "model",
    "actually",
    "learning",
    "something",
    "new",
    "learning",
    "previous",
    "thing",
    "decision",
    "trees",
    "similar",
    "right",
    "actually",
    "split",
    "predicted",
    "variables",
    "select",
    "randomly",
    "predicted",
    "variables",
    "let",
    "say",
    "14",
    "total",
    "number",
    "variables",
    "randomly",
    "pick",
    "three",
    "right",
    "every",
    "time",
    "get",
    "new",
    "decision",
    "tree",
    "variety",
    "right",
    "classification",
    "model",
    "actually",
    "much",
    "intelligent",
    "previous",
    "one",
    "got",
    "barrier",
    "experiences",
    "definitely",
    "make",
    "different",
    "decisions",
    "time",
    "compile",
    "different",
    "decisions",
    "new",
    "accurate",
    "efficient",
    "result",
    "right",
    "first",
    "important",
    "step",
    "select",
    "certain",
    "number",
    "features",
    "features",
    "let",
    "move",
    "second",
    "step",
    "let",
    "say",
    "node",
    "first",
    "step",
    "calculate",
    "best",
    "plate",
    "point",
    "know",
    "decision",
    "tree",
    "decision",
    "trees",
    "actually",
    "implemented",
    "pick",
    "significant",
    "variable",
    "right",
    "split",
    "particular",
    "node",
    "child",
    "nodes",
    "split",
    "takes",
    "place",
    "right",
    "number",
    "variables",
    "selected",
    "let",
    "say",
    "selected",
    "three",
    "implement",
    "split",
    "three",
    "nodes",
    "one",
    "particular",
    "decision",
    "tree",
    "right",
    "third",
    "step",
    "split",
    "node",
    "two",
    "daughter",
    "nodes",
    "split",
    "root",
    "note",
    "many",
    "notes",
    "want",
    "put",
    "hair",
    "split",
    "node",
    "notes",
    "answer",
    "terms",
    "saw",
    "right",
    "fourth",
    "step",
    "repeat",
    "3",
    "steps",
    "done",
    "previously",
    "repeat",
    "splitting",
    "reached",
    "n",
    "number",
    "nodes",
    "right",
    "need",
    "repeat",
    "reached",
    "till",
    "leaf",
    "nodes",
    "decision",
    "tree",
    "right",
    "four",
    "steps",
    "one",
    "decision",
    "tree",
    "random",
    "forest",
    "actually",
    "multiple",
    "asian",
    "trees",
    "fifth",
    "step",
    "come",
    "picture",
    "actually",
    "repeat",
    "previous",
    "steps",
    "number",
    "times",
    "hit",
    "number",
    "decision",
    "trees",
    "let",
    "say",
    "want",
    "implement",
    "five",
    "decision",
    "trees",
    "first",
    "step",
    "implement",
    "previous",
    "steps",
    "5",
    "times",
    "head",
    "eye",
    "tration",
    "number",
    "times",
    "right",
    "created",
    "five",
    "decision",
    "trees",
    "still",
    "task",
    "complete",
    "yet",
    "final",
    "task",
    "compile",
    "results",
    "five",
    "different",
    "decision",
    "trees",
    "make",
    "call",
    "majority",
    "voting",
    "right",
    "see",
    "picture",
    "different",
    "instances",
    "created",
    "n",
    "different",
    "decision",
    "trees",
    "finally",
    "compile",
    "result",
    "n",
    "different",
    "decision",
    "trees",
    "take",
    "call",
    "majority",
    "voting",
    "right",
    "whatever",
    "majority",
    "vote",
    "says",
    "final",
    "result",
    "basically",
    "overview",
    "random",
    "forest",
    "algorithm",
    "actually",
    "works",
    "let",
    "look",
    "example",
    "get",
    "much",
    "better",
    "understanding",
    "learnt",
    "let",
    "say",
    "data",
    "set",
    "consists",
    "four",
    "different",
    "instances",
    "right",
    "basically",
    "consists",
    "weather",
    "information",
    "previous",
    "14",
    "days",
    "right",
    "d1",
    "tildy",
    "14",
    "basically",
    "outlook",
    "humidity",
    "wind",
    "click",
    "gives",
    "better",
    "condition",
    "14",
    "days",
    "finally",
    "play",
    "target",
    "variable",
    "weather",
    "match",
    "take",
    "place",
    "particular",
    "day",
    "right",
    "main",
    "goal",
    "find",
    "whether",
    "match",
    "actually",
    "take",
    "place",
    "following",
    "weather",
    "conditions",
    "particular",
    "day",
    "let",
    "say",
    "outlook",
    "rainy",
    "day",
    "humidity",
    "high",
    "wind",
    "weak",
    "need",
    "predict",
    "whether",
    "able",
    "play",
    "match",
    "right",
    "problem",
    "statement",
    "fine",
    "let",
    "see",
    "random",
    "forest",
    "used",
    "sort",
    "first",
    "step",
    "actually",
    "split",
    "entire",
    "data",
    "set",
    "subsets",
    "split",
    "entire",
    "14",
    "variables",
    "smaller",
    "subsets",
    "right",
    "subsets",
    "may",
    "may",
    "overlap",
    "like",
    "certain",
    "overlapping",
    "1",
    "till",
    "d3",
    "d3",
    "till",
    "d6",
    "fine",
    "overlapping",
    "d3",
    "might",
    "happen",
    "might",
    "overlapping",
    "need",
    "really",
    "worry",
    "overlapping",
    "make",
    "sure",
    "subsets",
    "actually",
    "different",
    "right",
    "taken",
    "three",
    "different",
    "subsets",
    "first",
    "subset",
    "consists",
    "d1",
    "till",
    "d3",
    "mexican",
    "subset",
    "consists",
    "d3",
    "till",
    "d6",
    "methods",
    "subset",
    "consists",
    "d7",
    "tildy",
    "first",
    "focusing",
    "first",
    "upset",
    "let",
    "say",
    "particular",
    "day",
    "outlook",
    "overcast",
    "fine",
    "yes",
    "overcast",
    "probabilities",
    "match",
    "take",
    "place",
    "overcast",
    "basically",
    "weather",
    "cloudy",
    "condition",
    "definitely",
    "match",
    "take",
    "place",
    "let",
    "say",
    "overcast",
    "consider",
    "second",
    "probable",
    "option",
    "wind",
    "make",
    "decision",
    "based",
    "whether",
    "wind",
    "weak",
    "strong",
    "wind",
    "weak",
    "definitely",
    "go",
    "play",
    "judge",
    "would",
    "final",
    "outcome",
    "decision",
    "tree",
    "play",
    "ratio",
    "play",
    "play",
    "1",
    "get",
    "certain",
    "decision",
    "first",
    "decision",
    "tree",
    "let",
    "us",
    "look",
    "second",
    "subset",
    "since",
    "second",
    "subset",
    "different",
    "number",
    "variables",
    "decision",
    "trees",
    "absolutely",
    "different",
    "saw",
    "four",
    "subsets",
    "let",
    "say",
    "overcast",
    "play",
    "match",
    "overcast",
    "would",
    "go",
    "look",
    "humidity",
    "get",
    "split",
    "two",
    "whether",
    "high",
    "normal",
    "take",
    "first",
    "case",
    "humidity",
    "high",
    "week",
    "play",
    "match",
    "else",
    "humidity",
    "high",
    "wind",
    "strong",
    "would",
    "go",
    "play",
    "match",
    "right",
    "let",
    "us",
    "look",
    "second",
    "dot",
    "node",
    "humidity",
    "humidity",
    "normal",
    "wind",
    "weak",
    "definitely",
    "go",
    "play",
    "match",
    "want",
    "go",
    "play",
    "match",
    "look",
    "final",
    "result",
    "ratio",
    "placed",
    "play",
    "3",
    "2",
    "final",
    "outcome",
    "actually",
    "play",
    "right",
    "second",
    "subset",
    "get",
    "final",
    "decision",
    "play",
    "let",
    "us",
    "look",
    "third",
    "subset",
    "consists",
    "d7",
    "till",
    "d9",
    "overcast",
    "yes",
    "play",
    "match",
    "else",
    "go",
    "check",
    "humidity",
    "humidity",
    "really",
    "high",
    "wo",
    "play",
    "match",
    "else",
    "play",
    "match",
    "probability",
    "playing",
    "matches",
    "yes",
    "ratio",
    "play",
    "twist",
    "one",
    "right",
    "three",
    "different",
    "subsets",
    "three",
    "different",
    "decision",
    "trees",
    "three",
    "different",
    "outcomes",
    "one",
    "final",
    "outcome",
    "compiling",
    "results",
    "three",
    "different",
    "decision",
    "trees",
    "gives",
    "better",
    "perspective",
    "better",
    "understanding",
    "random",
    "forest",
    "like",
    "really",
    "works",
    "right",
    "let",
    "look",
    "various",
    "features",
    "random",
    "forest",
    "ray",
    "first",
    "foremost",
    "feature",
    "one",
    "accurate",
    "learning",
    "algorithms",
    "right",
    "single",
    "decision",
    "trees",
    "actually",
    "prone",
    "high",
    "variance",
    "hive",
    "bias",
    "contrary",
    "actually",
    "m4s",
    "averages",
    "entire",
    "variance",
    "across",
    "decision",
    "trees",
    "let",
    "say",
    "variances",
    "say",
    "x4",
    "decision",
    "tree",
    "random",
    "forest",
    "let",
    "say",
    "implemented",
    "n",
    "number",
    "decision",
    "trees",
    "parallely",
    "entire",
    "variance",
    "gets",
    "averaged",
    "upon",
    "final",
    "variance",
    "actually",
    "becomes",
    "x",
    "upon",
    "n",
    "entire",
    "variance",
    "actually",
    "goes",
    "compared",
    "algorithms",
    "second",
    "important",
    "feature",
    "works",
    "well",
    "classification",
    "regression",
    "problems",
    "far",
    "come",
    "across",
    "one",
    "algorithm",
    "works",
    "equally",
    "well",
    "classification",
    "kind",
    "problem",
    "regression",
    "kind",
    "problem",
    "right",
    "really",
    "runs",
    "efficient",
    "large",
    "databases",
    "basically",
    "really",
    "scalable",
    "even",
    "work",
    "lesser",
    "amount",
    "database",
    "work",
    "really",
    "huge",
    "volume",
    "data",
    "right",
    "good",
    "part",
    "fourth",
    "important",
    "point",
    "requires",
    "almost",
    "input",
    "preparation",
    "saying",
    "got",
    "certain",
    "implicit",
    "methods",
    "actually",
    "take",
    "care",
    "outliers",
    "missing",
    "data",
    "really",
    "take",
    "care",
    "thing",
    "stages",
    "input",
    "preparations",
    "random",
    "forest",
    "take",
    "care",
    "everything",
    "else",
    "next",
    "performs",
    "implicit",
    "feature",
    "selection",
    "right",
    "implementing",
    "multiple",
    "decision",
    "trees",
    "got",
    "implicit",
    "method",
    "automatically",
    "pick",
    "random",
    "features",
    "parameters",
    "go",
    "implementing",
    "different",
    "decision",
    "trees",
    "example",
    "give",
    "one",
    "simple",
    "command",
    "right",
    "want",
    "implement",
    "500",
    "decision",
    "trees",
    "matter",
    "random",
    "forest",
    "automatically",
    "take",
    "care",
    "implement",
    "500",
    "decision",
    "trees",
    "500",
    "decision",
    "trees",
    "different",
    "got",
    "implicit",
    "methods",
    "automatically",
    "collect",
    "different",
    "parameters",
    "variables",
    "right",
    "easily",
    "grown",
    "parallel",
    "actually",
    "implementing",
    "multiple",
    "decision",
    "trees",
    "decision",
    "trees",
    "running",
    "decisions",
    "trees",
    "actually",
    "getting",
    "implemented",
    "parallely",
    "say",
    "want",
    "thousand",
    "trees",
    "implemented",
    "thousand",
    "trees",
    "getting",
    "implemented",
    "parallely",
    "computation",
    "time",
    "reduces",
    "right",
    "last",
    "point",
    "got",
    "methods",
    "balancing",
    "error",
    "unbalanced",
    "exactly",
    "unbalanced",
    "data",
    "sets",
    "let",
    "give",
    "example",
    "let",
    "say",
    "working",
    "data",
    "set",
    "fine",
    "create",
    "random",
    "forest",
    "model",
    "get",
    "90",
    "accuracy",
    "immediately",
    "fantastic",
    "think",
    "right",
    "start",
    "diving",
    "deep",
    "go",
    "little",
    "deeper",
    "discovered",
    "90",
    "data",
    "actually",
    "belongs",
    "one",
    "class",
    "damn",
    "entire",
    "data",
    "set",
    "entire",
    "decision",
    "actually",
    "biased",
    "one",
    "particular",
    "class",
    "random",
    "forest",
    "actually",
    "takes",
    "care",
    "thing",
    "really",
    "biased",
    "towards",
    "particular",
    "decision",
    "tree",
    "particular",
    "variable",
    "class",
    "got",
    "methods",
    "looks",
    "balance",
    "errors",
    "data",
    "sets",
    "pretty",
    "much",
    "features",
    "random",
    "forests",
    "knn",
    "algorithm",
    "nearest",
    "neighbor",
    "simple",
    "algorithm",
    "stores",
    "available",
    "cases",
    "classify",
    "new",
    "data",
    "case",
    "based",
    "similarity",
    "measure",
    "suggests",
    "similar",
    "neighbors",
    "one",
    "right",
    "example",
    "apple",
    "looks",
    "similar",
    "banana",
    "orange",
    "melon",
    "rather",
    "monkey",
    "rat",
    "cat",
    "likely",
    "apple",
    "belong",
    "group",
    "fruits",
    "right",
    "well",
    "general",
    "cayenne",
    "used",
    "search",
    "application",
    "looking",
    "similar",
    "items",
    "task",
    "form",
    "fine",
    "items",
    "similar",
    "one",
    "call",
    "search",
    "cayenne",
    "search",
    "kn",
    "kn",
    "well",
    "k",
    "denotes",
    "number",
    "nearest",
    "neighbor",
    "voting",
    "class",
    "new",
    "data",
    "testing",
    "data",
    "example",
    "k",
    "equal",
    "1",
    "testing",
    "data",
    "given",
    "label",
    "close",
    "ample",
    "training",
    "set",
    "similarly",
    "k",
    "equal",
    "3",
    "labels",
    "three",
    "closes",
    "classes",
    "checked",
    "common",
    "label",
    "assigned",
    "testing",
    "data",
    "kn",
    "kn",
    "algorithm",
    "means",
    "moving",
    "ahead",
    "let",
    "see",
    "example",
    "scenarios",
    "kn",
    "used",
    "industry",
    "let",
    "see",
    "industrial",
    "application",
    "knn",
    "algorithm",
    "starting",
    "recommender",
    "system",
    "well",
    "biggest",
    "use",
    "case",
    "cayenne",
    "search",
    "recommender",
    "system",
    "recommended",
    "system",
    "like",
    "automated",
    "form",
    "shop",
    "counter",
    "guy",
    "asked",
    "product",
    "shows",
    "product",
    "also",
    "suggest",
    "displays",
    "relevant",
    "set",
    "products",
    "related",
    "item",
    "already",
    "interested",
    "buying",
    "knn",
    "algorithm",
    "applies",
    "recommending",
    "products",
    "like",
    "amazon",
    "recommending",
    "media",
    "like",
    "case",
    "netflix",
    "even",
    "recommending",
    "advertisement",
    "display",
    "user",
    "wrong",
    "almost",
    "must",
    "used",
    "amazon",
    "shopping",
    "right",
    "tell",
    "35",
    "revenue",
    "generated",
    "recommendation",
    "engine",
    "strategy",
    "amazon",
    "uses",
    "recommendation",
    "targeted",
    "marketing",
    "tool",
    "email",
    "campaigns",
    "around",
    "website",
    "pages",
    "amazon",
    "recommend",
    "many",
    "products",
    "different",
    "categories",
    "based",
    "browser",
    "pull",
    "products",
    "front",
    "likely",
    "buy",
    "like",
    "frequently",
    "bought",
    "together",
    "option",
    "comes",
    "bottom",
    "product",
    "page",
    "tempt",
    "buying",
    "combo",
    "well",
    "recommendation",
    "one",
    "main",
    "goal",
    "increase",
    "average",
    "order",
    "value",
    "upsell",
    "customers",
    "providing",
    "product",
    "suggestion",
    "based",
    "items",
    "shopping",
    "cart",
    "product",
    "currently",
    "looking",
    "site",
    "next",
    "industrial",
    "application",
    "knn",
    "algorithm",
    "concept",
    "search",
    "searching",
    "semantically",
    "similar",
    "documents",
    "classifying",
    "documents",
    "containing",
    "similar",
    "topics",
    "know",
    "data",
    "internet",
    "increasing",
    "exponentially",
    "every",
    "single",
    "second",
    "billions",
    "billions",
    "documents",
    "internet",
    "document",
    "internet",
    "contains",
    "multiple",
    "concepts",
    "could",
    "potential",
    "concept",
    "situation",
    "main",
    "problem",
    "extract",
    "concept",
    "set",
    "documents",
    "page",
    "could",
    "thousands",
    "combination",
    "could",
    "potential",
    "concepts",
    "average",
    "document",
    "could",
    "millions",
    "concept",
    "combined",
    "vast",
    "amount",
    "data",
    "web",
    "well",
    "talking",
    "enormous",
    "amount",
    "data",
    "set",
    "sample",
    "need",
    "need",
    "find",
    "concept",
    "enormous",
    "amount",
    "data",
    "set",
    "samples",
    "right",
    "purpose",
    "using",
    "knn",
    "algorithm",
    "advanced",
    "example",
    "could",
    "include",
    "handwriting",
    "detection",
    "like",
    "ocr",
    "image",
    "recognization",
    "even",
    "video",
    "recognization",
    "right",
    "know",
    "various",
    "use",
    "cases",
    "knn",
    "algorithm",
    "let",
    "proceed",
    "see",
    "work",
    "knn",
    "algorithm",
    "work",
    "let",
    "start",
    "plotting",
    "blue",
    "orange",
    "point",
    "graph",
    "blue",
    "points",
    "belong",
    "class",
    "orange",
    "ones",
    "belong",
    "class",
    "get",
    "star",
    "new",
    "pony",
    "task",
    "predict",
    "whether",
    "new",
    "point",
    "belongs",
    "class",
    "belongs",
    "class",
    "start",
    "production",
    "first",
    "thing",
    "select",
    "value",
    "k",
    "told",
    "kn",
    "kn",
    "algorithm",
    "refers",
    "number",
    "nearest",
    "neighbors",
    "want",
    "select",
    "example",
    "case",
    "k",
    "equal",
    "mean",
    "means",
    "selecting",
    "three",
    "points",
    "least",
    "distance",
    "new",
    "point",
    "say",
    "selecting",
    "three",
    "different",
    "points",
    "closest",
    "star",
    "well",
    "point",
    "time",
    "ask",
    "calculate",
    "least",
    "distance",
    "calculate",
    "distance",
    "get",
    "one",
    "blue",
    "two",
    "orange",
    "points",
    "closest",
    "star",
    "since",
    "case",
    "majority",
    "inch",
    "point",
    "see",
    "k",
    "equal",
    "3d",
    "star",
    "belongs",
    "class",
    "b",
    "say",
    "star",
    "similar",
    "orange",
    "points",
    "moving",
    "ahead",
    "well",
    "k",
    "equal",
    "6",
    "well",
    "case",
    "look",
    "six",
    "different",
    "points",
    "closest",
    "star",
    "case",
    "calculating",
    "distance",
    "find",
    "four",
    "blue",
    "points",
    "two",
    "orange",
    "point",
    "closest",
    "star",
    "see",
    "blue",
    "points",
    "majority",
    "say",
    "k",
    "equals",
    "6",
    "star",
    "belongs",
    "two",
    "class",
    "star",
    "similar",
    "blue",
    "points",
    "guess",
    "know",
    "knn",
    "algorithm",
    "work",
    "significance",
    "gain",
    "knn",
    "algorithm",
    "choose",
    "value",
    "k",
    "keeping",
    "mind",
    "case",
    "important",
    "parameter",
    "knn",
    "algorithm",
    "let",
    "see",
    "build",
    "k",
    "nearest",
    "neighbor",
    "classifier",
    "choose",
    "value",
    "k",
    "well",
    "might",
    "specific",
    "value",
    "k",
    "mind",
    "could",
    "divide",
    "data",
    "use",
    "something",
    "like",
    "technique",
    "test",
    "several",
    "values",
    "k",
    "order",
    "determine",
    "works",
    "best",
    "data",
    "example",
    "n",
    "equal",
    "cases",
    "case",
    "optimal",
    "value",
    "k",
    "lies",
    "somewhere",
    "1",
    "yes",
    "unless",
    "try",
    "sure",
    "know",
    "algorithm",
    "working",
    "higher",
    "level",
    "let",
    "move",
    "see",
    "things",
    "predicted",
    "using",
    "knn",
    "algorithm",
    "remember",
    "told",
    "knn",
    "algorithm",
    "uses",
    "least",
    "distance",
    "measure",
    "order",
    "find",
    "nearest",
    "neighbors",
    "let",
    "see",
    "distances",
    "calculated",
    "well",
    "several",
    "distance",
    "measure",
    "used",
    "start",
    "mainly",
    "focus",
    "euclidean",
    "distance",
    "manhattan",
    "distance",
    "session",
    "euclidean",
    "distance",
    "well",
    "euclidean",
    "distance",
    "defined",
    "square",
    "root",
    "sum",
    "difference",
    "new",
    "point",
    "x",
    "existing",
    "point",
    "example",
    "point",
    "p1",
    "p2",
    "point",
    "1",
    "1",
    "1",
    "point",
    "b",
    "2",
    "5",
    "euclidean",
    "distance",
    "say",
    "euclidean",
    "distance",
    "direct",
    "distance",
    "two",
    "points",
    "distance",
    "point",
    "p1",
    "p2",
    "calculate",
    "5",
    "minus",
    "1",
    "whole",
    "square",
    "plus",
    "4",
    "minus",
    "1",
    "whole",
    "square",
    "route",
    "results",
    "next",
    "manhattan",
    "distance",
    "well",
    "manhattan",
    "distance",
    "used",
    "calculate",
    "distance",
    "real",
    "vector",
    "using",
    "sum",
    "absolute",
    "difference",
    "case",
    "manhattan",
    "distance",
    "point",
    "p1",
    "p2",
    "mod",
    "5",
    "minus",
    "1",
    "plus",
    "mod",
    "value",
    "4",
    "minus",
    "1",
    "results",
    "3",
    "plus",
    "slide",
    "shows",
    "difference",
    "euclidean",
    "manhattan",
    "distance",
    "point",
    "point",
    "euclidean",
    "distance",
    "nothing",
    "direct",
    "least",
    "possible",
    "distance",
    "whereas",
    "manhattan",
    "distance",
    "distance",
    "b",
    "measured",
    "along",
    "axis",
    "right",
    "angle",
    "let",
    "take",
    "example",
    "see",
    "things",
    "predicted",
    "using",
    "knn",
    "algorithm",
    "cannon",
    "algorithm",
    "working",
    "suppose",
    "data",
    "set",
    "consists",
    "height",
    "weight",
    "size",
    "customers",
    "new",
    "customer",
    "come",
    "height",
    "wait",
    "information",
    "task",
    "predict",
    "size",
    "particular",
    "customer",
    "using",
    "knn",
    "algorithm",
    "first",
    "thing",
    "need",
    "need",
    "calculate",
    "euclidean",
    "distance",
    "new",
    "data",
    "height",
    "160",
    "one",
    "centimeter",
    "weight",
    "61",
    "kg",
    "first",
    "thing",
    "calculate",
    "euclidean",
    "distance",
    "nothing",
    "square",
    "root",
    "160",
    "1",
    "minus",
    "158",
    "whole",
    "square",
    "plus",
    "61",
    "minus",
    "58",
    "whole",
    "square",
    "square",
    "root",
    "let",
    "drag",
    "drop",
    "various",
    "euclidean",
    "distance",
    "points",
    "let",
    "suppose",
    "k",
    "equal",
    "5",
    "algorithm",
    "searches",
    "five",
    "customer",
    "closest",
    "new",
    "customer",
    "similar",
    "new",
    "data",
    "terms",
    "attribute",
    "k",
    "equal",
    "let",
    "find",
    "top",
    "five",
    "minimum",
    "euclidian",
    "distance",
    "distance",
    "going",
    "use",
    "one",
    "two",
    "three",
    "four",
    "five",
    "let",
    "rank",
    "order",
    "first",
    "second",
    "third",
    "one",
    "forward",
    "one",
    "five",
    "order",
    "k",
    "equal",
    "5",
    "come",
    "size",
    "one",
    "comes",
    "size",
    "l",
    "obviously",
    "best",
    "guess",
    "best",
    "prediction",
    "size",
    "white",
    "161",
    "centimeters",
    "wait",
    "60",
    "1",
    "kg",
    "say",
    "new",
    "customer",
    "fit",
    "size",
    "well",
    "theoretical",
    "session",
    "drill",
    "coding",
    "part",
    "let",
    "tell",
    "people",
    "call",
    "kn",
    "lazy",
    "learner",
    "well",
    "kn",
    "classification",
    "ocean",
    "simple",
    "algorithm",
    "called",
    "lazy",
    "kn",
    "lazy",
    "learner",
    "discriminative",
    "function",
    "training",
    "data",
    "memorizes",
    "training",
    "data",
    "learning",
    "phase",
    "model",
    "work",
    "happens",
    "time",
    "prediction",
    "requested",
    "reason",
    "kn",
    "often",
    "referred",
    "us",
    "lazy",
    "learning",
    "algorithm",
    "theoretical",
    "session",
    "let",
    "move",
    "coding",
    "part",
    "practical",
    "implementation",
    "part",
    "using",
    "artists",
    "data",
    "set",
    "data",
    "set",
    "consists",
    "150",
    "observation",
    "four",
    "features",
    "one",
    "class",
    "label",
    "four",
    "features",
    "include",
    "sepal",
    "length",
    "sepal",
    "width",
    "petal",
    "length",
    "petrol",
    "head",
    "whereas",
    "class",
    "label",
    "decides",
    "flower",
    "belongs",
    "category",
    "description",
    "data",
    "set",
    "using",
    "let",
    "move",
    "see",
    "step",
    "step",
    "solution",
    "perform",
    "knn",
    "algorithm",
    "first",
    "start",
    "handling",
    "data",
    "open",
    "data",
    "set",
    "csv",
    "format",
    "split",
    "data",
    "set",
    "train",
    "test",
    "part",
    "next",
    "take",
    "clarity",
    "calculate",
    "distance",
    "two",
    "data",
    "instances",
    "calculate",
    "distance",
    "next",
    "look",
    "neighbor",
    "select",
    "k",
    "neighbors",
    "least",
    "distance",
    "new",
    "point",
    "get",
    "neighbor",
    "generate",
    "response",
    "set",
    "data",
    "instances",
    "decide",
    "whether",
    "new",
    "point",
    "belongs",
    "class",
    "class",
    "finally",
    "create",
    "accuracy",
    "function",
    "end",
    "tie",
    "together",
    "main",
    "function",
    "let",
    "start",
    "code",
    "implementing",
    "knn",
    "algorithm",
    "using",
    "python",
    "using",
    "java",
    "old",
    "book",
    "installed",
    "let",
    "move",
    "see",
    "algorithm",
    "implemented",
    "using",
    "python",
    "jupyter",
    "notebook",
    "interactive",
    "computing",
    "notebook",
    "environment",
    "python",
    "installed",
    "launch",
    "launching",
    "jupyter",
    "notebook",
    "riding",
    "python",
    "codes",
    "first",
    "thing",
    "need",
    "load",
    "file",
    "data",
    "csv",
    "format",
    "without",
    "header",
    "line",
    "code",
    "open",
    "file",
    "open",
    "function",
    "read",
    "data",
    "line",
    "using",
    "reader",
    "function",
    "csv",
    "module",
    "let",
    "write",
    "code",
    "load",
    "data",
    "file",
    "let",
    "execute",
    "run",
    "button",
    "execute",
    "run",
    "button",
    "see",
    "entire",
    "training",
    "data",
    "set",
    "output",
    "next",
    "need",
    "split",
    "data",
    "training",
    "data",
    "set",
    "kn",
    "use",
    "make",
    "prediction",
    "test",
    "data",
    "set",
    "use",
    "evaluate",
    "accuracy",
    "model",
    "first",
    "need",
    "convert",
    "flower",
    "measure",
    "load",
    "string",
    "numbers",
    "work",
    "next",
    "need",
    "split",
    "data",
    "set",
    "randomly",
    "train",
    "test",
    "ratio",
    "67",
    "233",
    "test",
    "train",
    "standard",
    "ratio",
    "used",
    "purpose",
    "let",
    "define",
    "function",
    "load",
    "data",
    "set",
    "loads",
    "csv",
    "provided",
    "file",
    "named",
    "split",
    "randomly",
    "training",
    "test",
    "data",
    "set",
    "using",
    "provided",
    "split",
    "ratio",
    "function",
    "load",
    "data",
    "set",
    "using",
    "filename",
    "split",
    "ratio",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "input",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "check",
    "errors",
    "executed",
    "zero",
    "errors",
    "let",
    "test",
    "function",
    "training",
    "set",
    "testing",
    "set",
    "load",
    "data",
    "set",
    "function",
    "load",
    "data",
    "set",
    "inside",
    "passing",
    "file",
    "data",
    "split",
    "ratio",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "let",
    "see",
    "training",
    "data",
    "set",
    "test",
    "data",
    "set",
    "dividing",
    "giving",
    "count",
    "training",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "total",
    "number",
    "training",
    "data",
    "set",
    "split",
    "97",
    "total",
    "number",
    "test",
    "data",
    "set",
    "total",
    "number",
    "training",
    "data",
    "set",
    "97",
    "total",
    "number",
    "test",
    "data",
    "set",
    "right",
    "okay",
    "function",
    "load",
    "data",
    "set",
    "performing",
    "well",
    "let",
    "move",
    "step",
    "two",
    "similarity",
    "order",
    "make",
    "prediction",
    "need",
    "calculate",
    "similarity",
    "two",
    "given",
    "data",
    "instances",
    "needed",
    "locate",
    "kamo",
    "similar",
    "data",
    "instances",
    "training",
    "data",
    "set",
    "turn",
    "make",
    "prediction",
    "given",
    "flower",
    "measurement",
    "numeric",
    "unit",
    "directly",
    "use",
    "euclidean",
    "distance",
    "measure",
    "nothing",
    "square",
    "root",
    "sum",
    "squared",
    "differences",
    "two",
    "areas",
    "number",
    "given",
    "flower",
    "numeric",
    "unit",
    "directly",
    "use",
    "euclidean",
    "distance",
    "measure",
    "nothing",
    "square",
    "root",
    "sum",
    "squared",
    "difference",
    "two",
    "areas",
    "number",
    "additionally",
    "want",
    "control",
    "field",
    "include",
    "distance",
    "calculation",
    "specifically",
    "want",
    "include",
    "first",
    "attribute",
    "approach",
    "limit",
    "euclidean",
    "distance",
    "fixed",
    "length",
    "right",
    "let",
    "define",
    "euclidean",
    "function",
    "euclidean",
    "distance",
    "function",
    "takes",
    "instance",
    "one",
    "instance",
    "length",
    "parameters",
    "instance",
    "1",
    "ends",
    "two",
    "two",
    "points",
    "want",
    "calculate",
    "euclidean",
    "distance",
    "whereas",
    "length",
    "denote",
    "many",
    "attributes",
    "want",
    "include",
    "okay",
    "euclidean",
    "function",
    "let",
    "execute",
    "executing",
    "fine",
    "without",
    "errors",
    "let",
    "test",
    "function",
    "suppose",
    "data",
    "one",
    "first",
    "instance",
    "consists",
    "data",
    "point",
    "two",
    "two",
    "belongs",
    "class",
    "data",
    "consist",
    "four",
    "four",
    "belongs",
    "class",
    "calculate",
    "euclidean",
    "distance",
    "data",
    "one",
    "data",
    "consider",
    "first",
    "three",
    "features",
    "right",
    "let",
    "print",
    "distance",
    "see",
    "distance",
    "comes",
    "three",
    "point",
    "four",
    "six",
    "four",
    "like",
    "nothing",
    "square",
    "root",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "distance",
    "nothing",
    "euclidean",
    "distance",
    "calculated",
    "square",
    "root",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "plus",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "nothing",
    "3",
    "times",
    "4",
    "minus",
    "2",
    "whole",
    "square",
    "12",
    "square",
    "root",
    "12",
    "nothing",
    "right",
    "calculated",
    "distance",
    "need",
    "look",
    "k",
    "nearest",
    "neighbors",
    "similarity",
    "measure",
    "use",
    "collect",
    "kamo",
    "similar",
    "instances",
    "given",
    "unseen",
    "instance",
    "well",
    "straightforward",
    "process",
    "calculating",
    "distance",
    "instances",
    "selecting",
    "subset",
    "smallest",
    "distance",
    "value",
    "select",
    "smallest",
    "distance",
    "values",
    "defining",
    "function",
    "get",
    "neighbors",
    "defining",
    "function",
    "get",
    "neighbors",
    "return",
    "k",
    "similar",
    "neighbors",
    "training",
    "set",
    "given",
    "test",
    "instance",
    "right",
    "get",
    "neighbors",
    "look",
    "like",
    "takes",
    "training",
    "data",
    "set",
    "test",
    "instance",
    "k",
    "input",
    "k",
    "nothing",
    "number",
    "nearest",
    "neighbor",
    "want",
    "check",
    "right",
    "basically",
    "getting",
    "get",
    "mabel",
    "function",
    "k",
    "different",
    "points",
    "least",
    "euclidean",
    "distance",
    "test",
    "instance",
    "right",
    "let",
    "execute",
    "function",
    "executed",
    "without",
    "errors",
    "let",
    "test",
    "function",
    "suppose",
    "training",
    "data",
    "set",
    "includes",
    "data",
    "like",
    "belongs",
    "class",
    "data",
    "includes",
    "four",
    "four",
    "four",
    "belongs",
    "class",
    "p",
    "testing",
    "census",
    "555",
    "predict",
    "whether",
    "test",
    "instance",
    "belongs",
    "class",
    "belongs",
    "class",
    "right",
    "k",
    "equal",
    "1",
    "predict",
    "nearest",
    "neighbor",
    "predict",
    "whether",
    "test",
    "instance",
    "belong",
    "class",
    "belong",
    "class",
    "alright",
    "let",
    "execute",
    "run",
    "button",
    "right",
    "executing",
    "run",
    "button",
    "see",
    "output",
    "new",
    "instance",
    "5",
    "5",
    "5",
    "closes",
    "belongs",
    "class",
    "right",
    "located",
    "similar",
    "neighbor",
    "test",
    "instance",
    "next",
    "task",
    "predict",
    "response",
    "based",
    "neighbors",
    "well",
    "allowing",
    "neighbor",
    "vote",
    "class",
    "attribute",
    "take",
    "majority",
    "vote",
    "prediction",
    "let",
    "see",
    "function",
    "getresponse",
    "takes",
    "neighbors",
    "input",
    "well",
    "neighbor",
    "nothing",
    "output",
    "get",
    "function",
    "output",
    "get",
    "function",
    "fed",
    "get",
    "response",
    "right",
    "let",
    "execute",
    "run",
    "button",
    "executed",
    "let",
    "move",
    "ahead",
    "test",
    "function",
    "get",
    "response",
    "bun",
    "bun",
    "bun",
    "belongs",
    "class",
    "2",
    "2",
    "2",
    "belongs",
    "class",
    "a33",
    "belongs",
    "class",
    "response",
    "store",
    "value",
    "get",
    "response",
    "passing",
    "neighbor",
    "value",
    "like",
    "want",
    "check",
    "want",
    "predict",
    "whether",
    "test",
    "instance",
    "final",
    "outcome",
    "belongs",
    "class",
    "class",
    "neighbors",
    "1",
    "1",
    "1",
    "2",
    "2",
    "3",
    "3",
    "let",
    "check",
    "response",
    "created",
    "different",
    "function",
    "required",
    "knn",
    "algorithm",
    "important",
    "main",
    "concern",
    "evaluate",
    "accuracy",
    "prediction",
    "easy",
    "way",
    "evaluate",
    "accuracy",
    "model",
    "calculate",
    "ratio",
    "total",
    "correct",
    "prediction",
    "protection",
    "made",
    "defining",
    "function",
    "get",
    "accuracy",
    "inside",
    "passing",
    "test",
    "data",
    "set",
    "predictions",
    "get",
    "accuracy",
    "function",
    "check",
    "get",
    "executed",
    "without",
    "error",
    "let",
    "check",
    "sample",
    "data",
    "set",
    "test",
    "data",
    "set",
    "1",
    "1",
    "1",
    "belongs",
    "class",
    "belongs",
    "class",
    "3",
    "3",
    "3",
    "belongs",
    "class",
    "b",
    "predictions",
    "first",
    "test",
    "data",
    "predicted",
    "latter",
    "belongs",
    "class",
    "true",
    "next",
    "predicted",
    "belongs",
    "class",
    "c",
    "next",
    "predictive",
    "belongs",
    "class",
    "false",
    "case",
    "cause",
    "test",
    "data",
    "belongs",
    "class",
    "right",
    "total",
    "correct",
    "prediction",
    "three",
    "right",
    "ratio",
    "2",
    "3",
    "nothing",
    "accuracy",
    "rate",
    "created",
    "function",
    "required",
    "knn",
    "algorithm",
    "let",
    "compile",
    "one",
    "single",
    "main",
    "function",
    "alright",
    "main",
    "function",
    "using",
    "iris",
    "data",
    "set",
    "split",
    "value",
    "k",
    "3",
    "let",
    "see",
    "accuracy",
    "score",
    "check",
    "accurate",
    "modulus",
    "training",
    "data",
    "set",
    "hundred",
    "thirteen",
    "values",
    "test",
    "data",
    "set",
    "37",
    "values",
    "predicted",
    "actual",
    "values",
    "output",
    "okay",
    "total",
    "got",
    "accuracy",
    "90s",
    "point",
    "two",
    "nine",
    "percent",
    "really",
    "good",
    "alright",
    "hope",
    "concept",
    "knn",
    "algorithm",
    "device",
    "world",
    "full",
    "machine",
    "learning",
    "artificial",
    "intelligence",
    "surrounding",
    "almost",
    "everything",
    "around",
    "us",
    "classification",
    "prediction",
    "one",
    "important",
    "aspects",
    "machine",
    "learning",
    "moving",
    "forward",
    "let",
    "quick",
    "look",
    "agenda",
    "start",
    "video",
    "explaining",
    "guys",
    "exactly",
    "nave",
    "biased",
    "bayes",
    "theorem",
    "serves",
    "logic",
    "behind",
    "name",
    "pass",
    "algorithm",
    "going",
    "forward",
    "explain",
    "steps",
    "involved",
    "neighbors",
    "algorithm",
    "one",
    "one",
    "finally",
    "add",
    "finish",
    "video",
    "demo",
    "nave",
    "bass",
    "using",
    "sql",
    "package",
    "noun",
    "bass",
    "simple",
    "surprisingly",
    "powerful",
    "algorithm",
    "penetrative",
    "analysis",
    "classification",
    "technique",
    "based",
    "base",
    "theorem",
    "assumption",
    "independence",
    "among",
    "predictors",
    "comprises",
    "two",
    "parts",
    "name",
    "bias",
    "simple",
    "terms",
    "neighbors",
    "classifier",
    "assumes",
    "presence",
    "particular",
    "feature",
    "class",
    "unrelated",
    "presence",
    "feature",
    "even",
    "features",
    "depend",
    "upon",
    "existence",
    "features",
    "properties",
    "independently",
    "contribute",
    "probability",
    "whether",
    "fruit",
    "apple",
    "orange",
    "banana",
    "known",
    "naive",
    "naive",
    "based",
    "model",
    "easy",
    "build",
    "particularly",
    "useful",
    "large",
    "data",
    "sets",
    "probability",
    "theory",
    "statistics",
    "based",
    "theorem",
    "already",
    "known",
    "base",
    "law",
    "base",
    "rule",
    "describes",
    "probability",
    "event",
    "based",
    "prior",
    "knowledge",
    "conditions",
    "might",
    "related",
    "event",
    "paste",
    "theorem",
    "way",
    "figure",
    "conditional",
    "probability",
    "conditional",
    "probability",
    "probability",
    "event",
    "happening",
    "given",
    "relationship",
    "one",
    "events",
    "example",
    "probability",
    "getting",
    "parking",
    "space",
    "connected",
    "time",
    "day",
    "pass",
    "park",
    "conventions",
    "going",
    "time",
    "based",
    "serum",
    "slightly",
    "nuanced",
    "nutshell",
    "gives",
    "actual",
    "probability",
    "event",
    "given",
    "information",
    "tests",
    "look",
    "definition",
    "bayes",
    "theorem",
    "see",
    "given",
    "hypothesis",
    "h",
    "evidence",
    "term",
    "states",
    "relationship",
    "probability",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "probability",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "given",
    "e",
    "defined",
    "probability",
    "e",
    "given",
    "h",
    "probability",
    "h",
    "divided",
    "probability",
    "e",
    "rather",
    "confusing",
    "right",
    "let",
    "take",
    "example",
    "understand",
    "theorem",
    "suppose",
    "deck",
    "cards",
    "single",
    "card",
    "drawn",
    "deck",
    "playing",
    "cards",
    "probability",
    "card",
    "king",
    "52",
    "since",
    "four",
    "kings",
    "standard",
    "deck",
    "52",
    "cards",
    "king",
    "event",
    "card",
    "king",
    "probability",
    "king",
    "given",
    "4",
    "52",
    "equal",
    "1",
    "evidence",
    "provided",
    "instance",
    "someone",
    "looks",
    "single",
    "card",
    "face",
    "card",
    "probability",
    "king",
    "given",
    "face",
    "calculated",
    "using",
    "base",
    "theorem",
    "formula",
    "since",
    "every",
    "king",
    "also",
    "face",
    "card",
    "probability",
    "face",
    "given",
    "king",
    "equal",
    "1",
    "since",
    "three",
    "face",
    "cards",
    "suit",
    "chat",
    "king",
    "queen",
    "probability",
    "face",
    "card",
    "equal",
    "12",
    "3",
    "using",
    "bayes",
    "theorem",
    "find",
    "probability",
    "king",
    "given",
    "face",
    "final",
    "answer",
    "comes",
    "1",
    "3",
    "also",
    "true",
    "deck",
    "cards",
    "faces",
    "three",
    "types",
    "phases",
    "chat",
    "king",
    "queen",
    "probability",
    "king",
    "1",
    "simple",
    "example",
    "based",
    "works",
    "look",
    "proof",
    "bayes",
    "theorem",
    "evolved",
    "probability",
    "given",
    "p",
    "probability",
    "b",
    "given",
    "joint",
    "probability",
    "distribution",
    "sets",
    "b",
    "probability",
    "intersection",
    "b",
    "conditional",
    "probability",
    "given",
    "b",
    "defined",
    "probability",
    "intersection",
    "b",
    "divided",
    "probability",
    "b",
    "similarly",
    "probability",
    "b",
    "given",
    "defined",
    "probability",
    "b",
    "intersection",
    "divided",
    "probability",
    "equate",
    "probability",
    "intersection",
    "p",
    "probability",
    "b",
    "intersection",
    "thing",
    "method",
    "see",
    "get",
    "final",
    "base",
    "theorem",
    "proof",
    "probability",
    "given",
    "b",
    "equals",
    "probability",
    "b",
    "given",
    "probability",
    "p",
    "divided",
    "probability",
    "equation",
    "applies",
    "probability",
    "distribution",
    "events",
    "particular",
    "nice",
    "interpretation",
    "case",
    "represented",
    "hypothesis",
    "h",
    "b",
    "represented",
    "observed",
    "evidence",
    "e",
    "case",
    "formula",
    "p",
    "h",
    "given",
    "e",
    "equal",
    "p",
    "e",
    "given",
    "h",
    "probability",
    "h",
    "divided",
    "probability",
    "e",
    "relates",
    "probability",
    "hypotheses",
    "getting",
    "evidence",
    "p",
    "h",
    "probability",
    "hypothesis",
    "getting",
    "evidence",
    "p",
    "h",
    "given",
    "e",
    "reason",
    "p",
    "h",
    "known",
    "prior",
    "probability",
    "p",
    "given",
    "e",
    "known",
    "posterior",
    "probability",
    "factor",
    "relates",
    "two",
    "known",
    "likelihood",
    "ratio",
    "using",
    "term",
    "space",
    "theorem",
    "rephrased",
    "posterior",
    "probability",
    "equals",
    "prior",
    "probability",
    "times",
    "likelihood",
    "ratio",
    "know",
    "maths",
    "involved",
    "behind",
    "baster",
    "mm",
    "let",
    "see",
    "implement",
    "real",
    "life",
    "scenario",
    "suppose",
    "data",
    "set",
    "outlook",
    "humidity",
    "need",
    "find",
    "whether",
    "play",
    "day",
    "outlook",
    "sunny",
    "overcast",
    "rain",
    "humidity",
    "high",
    "normal",
    "wind",
    "categorized",
    "two",
    "phases",
    "weak",
    "strong",
    "winds",
    "first",
    "create",
    "frequency",
    "table",
    "using",
    "attribute",
    "data",
    "set",
    "frequency",
    "table",
    "outlook",
    "looks",
    "like",
    "sunny",
    "overcast",
    "rainy",
    "frequency",
    "table",
    "humidity",
    "looks",
    "like",
    "frequency",
    "table",
    "looks",
    "like",
    "strong",
    "weak",
    "wind",
    "high",
    "normal",
    "ranges",
    "humidity",
    "frequency",
    "table",
    "generate",
    "likelihood",
    "table",
    "likelihood",
    "table",
    "contains",
    "probability",
    "particular",
    "day",
    "suppose",
    "take",
    "sunny",
    "take",
    "play",
    "yes",
    "probability",
    "sunny",
    "given",
    "play",
    "yes",
    "3",
    "10",
    "probability",
    "x",
    "probability",
    "sunny",
    "equal",
    "5",
    "14",
    "terms",
    "generated",
    "data",
    "finally",
    "probability",
    "yes",
    "10",
    "look",
    "likelihood",
    "yes",
    "given",
    "sunny",
    "see",
    "using",
    "bayes",
    "theorem",
    "probability",
    "sunny",
    "given",
    "yes",
    "probability",
    "divided",
    "probability",
    "sunny",
    "values",
    "calculated",
    "put",
    "base",
    "serum",
    "equation",
    "get",
    "likelihood",
    "yes",
    "similarly",
    "likelihood",
    "also",
    "calculated",
    "similarly",
    "going",
    "create",
    "likelihood",
    "table",
    "humidity",
    "win",
    "humidity",
    "likelihood",
    "yes",
    "given",
    "humidity",
    "high",
    "equal",
    "probability",
    "playing",
    "know",
    "given",
    "vent",
    "high",
    "similarly",
    "table",
    "wind",
    "probability",
    "given",
    "wind",
    "week",
    "probability",
    "given",
    "win",
    "week",
    "suppose",
    "day",
    "high",
    "rain",
    "high",
    "humidity",
    "wind",
    "weak",
    "play",
    "use",
    "base",
    "theorem",
    "likelihood",
    "yes",
    "day",
    "equal",
    "probability",
    "outlook",
    "rain",
    "given",
    "yes",
    "probability",
    "magic",
    "given",
    "say",
    "yes",
    "probability",
    "given",
    "playing",
    "yes",
    "probability",
    "yes",
    "equals",
    "zero",
    "point",
    "zero",
    "one",
    "nine",
    "similarly",
    "likelihood",
    "know",
    "day",
    "equal",
    "zero",
    "point",
    "zero",
    "one",
    "six",
    "look",
    "probability",
    "yes",
    "day",
    "playing",
    "need",
    "divide",
    "likelihood",
    "yes",
    "probability",
    "playing",
    "tomorrow",
    "yes",
    "5",
    "whereas",
    "probability",
    "playing",
    "equal",
    "based",
    "upon",
    "data",
    "already",
    "us",
    "idea",
    "exactly",
    "named",
    "bias",
    "works",
    "seen",
    "implemented",
    "particular",
    "data",
    "set",
    "let",
    "see",
    "used",
    "industry",
    "started",
    "first",
    "industrial",
    "use",
    "case",
    "news",
    "categorization",
    "use",
    "term",
    "text",
    "classification",
    "broaden",
    "spectrum",
    "algorithm",
    "news",
    "web",
    "rapidly",
    "growing",
    "era",
    "information",
    "age",
    "new",
    "site",
    "different",
    "layout",
    "categorization",
    "grouping",
    "news",
    "heterogeneity",
    "layout",
    "categorization",
    "always",
    "satisfy",
    "individual",
    "users",
    "need",
    "remove",
    "heterogeneity",
    "classifying",
    "news",
    "articles",
    "owing",
    "user",
    "preference",
    "formidable",
    "task",
    "companies",
    "use",
    "web",
    "crawler",
    "extract",
    "useful",
    "text",
    "html",
    "pages",
    "news",
    "articles",
    "news",
    "articles",
    "tokenized",
    "tokens",
    "nothing",
    "categories",
    "news",
    "order",
    "achieve",
    "better",
    "classification",
    "result",
    "remove",
    "less",
    "significant",
    "words",
    "stop",
    "documents",
    "articles",
    "apply",
    "nave",
    "base",
    "classifier",
    "classifying",
    "news",
    "contents",
    "based",
    "news",
    "far",
    "one",
    "best",
    "examples",
    "neighbors",
    "classifier",
    "spam",
    "filtering",
    "nave",
    "bayes",
    "classifier",
    "popular",
    "statistical",
    "technique",
    "email",
    "filtering",
    "typically",
    "use",
    "bag",
    "words",
    "features",
    "identify",
    "spam",
    "email",
    "approach",
    "commonly",
    "used",
    "text",
    "classification",
    "well",
    "works",
    "correlating",
    "use",
    "tokens",
    "spam",
    "emails",
    "bayes",
    "theorem",
    "explained",
    "earlier",
    "used",
    "calculate",
    "probability",
    "email",
    "spam",
    "named",
    "spam",
    "filtering",
    "baseline",
    "technique",
    "dealing",
    "spam",
    "container",
    "emails",
    "need",
    "individual",
    "user",
    "give",
    "low",
    "false",
    "positive",
    "spam",
    "detection",
    "rates",
    "generally",
    "acceptable",
    "users",
    "one",
    "oldest",
    "ways",
    "spam",
    "filtering",
    "roots",
    "1990s",
    "particular",
    "words",
    "particular",
    "probabilities",
    "occurring",
    "spam",
    "legitimate",
    "email",
    "well",
    "instance",
    "emails",
    "users",
    "frequently",
    "encounter",
    "world",
    "lottery",
    "lucky",
    "draw",
    "spam",
    "email",
    "sell",
    "see",
    "emails",
    "filter",
    "know",
    "probabilities",
    "advance",
    "must",
    "friends",
    "build",
    "train",
    "filter",
    "user",
    "must",
    "manually",
    "indicate",
    "whether",
    "new",
    "email",
    "spam",
    "words",
    "straining",
    "email",
    "filter",
    "adjust",
    "probability",
    "word",
    "appear",
    "spam",
    "legitimate",
    "owl",
    "database",
    "training",
    "word",
    "probabilities",
    "also",
    "known",
    "likelihood",
    "functions",
    "used",
    "compute",
    "probability",
    "email",
    "particular",
    "set",
    "words",
    "belongs",
    "either",
    "category",
    "word",
    "email",
    "contributes",
    "email",
    "spam",
    "probability",
    "contribution",
    "called",
    "posterior",
    "probability",
    "computed",
    "using",
    "base",
    "0",
    "email",
    "spam",
    "probability",
    "computed",
    "verse",
    "email",
    "total",
    "exceeds",
    "certain",
    "threshold",
    "say",
    "95",
    "filter",
    "mark",
    "email",
    "spam",
    "object",
    "detection",
    "process",
    "finding",
    "instances",
    "objects",
    "faces",
    "bicycles",
    "buildings",
    "images",
    "video",
    "object",
    "detection",
    "algorithm",
    "typically",
    "use",
    "extracted",
    "features",
    "learning",
    "algorithm",
    "recognize",
    "instance",
    "object",
    "category",
    "bass",
    "plays",
    "important",
    "role",
    "categorization",
    "classification",
    "object",
    "medical",
    "area",
    "increasingly",
    "voluminous",
    "amount",
    "electronic",
    "data",
    "becoming",
    "complicated",
    "produced",
    "medical",
    "data",
    "certain",
    "characteristics",
    "make",
    "analysis",
    "challenging",
    "attractive",
    "well",
    "among",
    "different",
    "approaches",
    "knave",
    "bias",
    "used",
    "effective",
    "efficient",
    "classification",
    "algorithm",
    "successfully",
    "applied",
    "many",
    "medical",
    "problems",
    "empirical",
    "comparison",
    "knave",
    "bias",
    "versus",
    "five",
    "popular",
    "classifiers",
    "medical",
    "data",
    "sets",
    "shows",
    "may",
    "bias",
    "well",
    "suited",
    "medical",
    "application",
    "high",
    "performance",
    "examine",
    "medical",
    "problems",
    "past",
    "various",
    "testicle",
    "methods",
    "used",
    "modeling",
    "area",
    "disease",
    "diagnosis",
    "methods",
    "require",
    "prior",
    "assumptions",
    "less",
    "capable",
    "dealing",
    "massive",
    "complicated",
    "nonlinear",
    "dependent",
    "data",
    "one",
    "main",
    "advantages",
    "neighbor",
    "approach",
    "appealing",
    "physicians",
    "available",
    "information",
    "used",
    "explain",
    "decision",
    "explanation",
    "seems",
    "natural",
    "medical",
    "diagnosis",
    "prognosis",
    "close",
    "way",
    "physician",
    "diagnosed",
    "patients",
    "weather",
    "one",
    "influential",
    "factor",
    "daily",
    "life",
    "extent",
    "may",
    "affect",
    "economy",
    "country",
    "depends",
    "occupation",
    "like",
    "agriculture",
    "therefore",
    "countermeasure",
    "reduce",
    "damage",
    "caused",
    "uncertainty",
    "whether",
    "behavior",
    "efficient",
    "way",
    "print",
    "weather",
    "whether",
    "projecting",
    "challenging",
    "problem",
    "meteorological",
    "department",
    "since",
    "ears",
    "even",
    "technology",
    "skill",
    "scientific",
    "advancement",
    "accuracy",
    "protection",
    "weather",
    "never",
    "sufficient",
    "even",
    "current",
    "day",
    "domain",
    "remains",
    "research",
    "topic",
    "scientists",
    "mathematicians",
    "working",
    "produce",
    "model",
    "algorithm",
    "accurately",
    "predict",
    "weather",
    "bias",
    "approach",
    "based",
    "model",
    "created",
    "posterior",
    "probabilities",
    "used",
    "calculate",
    "likelihood",
    "class",
    "label",
    "input",
    "data",
    "instance",
    "one",
    "maximum",
    "likelihood",
    "considered",
    "resulting",
    "output",
    "earlier",
    "saw",
    "small",
    "implementation",
    "algorithm",
    "well",
    "predicted",
    "whether",
    "play",
    "based",
    "data",
    "collected",
    "earlier",
    "python",
    "library",
    "known",
    "helps",
    "build",
    "bias",
    "model",
    "python",
    "three",
    "types",
    "named",
    "ass",
    "model",
    "library",
    "first",
    "one",
    "caution",
    "used",
    "classification",
    "assumes",
    "feature",
    "follow",
    "normal",
    "distribution",
    "next",
    "multinomial",
    "used",
    "discrete",
    "counts",
    "example",
    "let",
    "say",
    "text",
    "classification",
    "problem",
    "consider",
    "bernouli",
    "trials",
    "one",
    "step",
    "instead",
    "word",
    "occurring",
    "document",
    "count",
    "often",
    "word",
    "occurs",
    "document",
    "think",
    "number",
    "times",
    "outcomes",
    "number",
    "observed",
    "given",
    "number",
    "trials",
    "finally",
    "bernouli",
    "type",
    "neighbors",
    "binomial",
    "model",
    "useful",
    "feature",
    "vectors",
    "binary",
    "bag",
    "words",
    "model",
    "zeros",
    "words",
    "occur",
    "document",
    "verse",
    "occur",
    "document",
    "respectively",
    "based",
    "data",
    "set",
    "choose",
    "given",
    "discussed",
    "model",
    "gaussian",
    "multinomial",
    "bernouli",
    "let",
    "understand",
    "algorithm",
    "works",
    "different",
    "steps",
    "one",
    "take",
    "create",
    "bison",
    "model",
    "use",
    "knave",
    "bias",
    "predict",
    "output",
    "understand",
    "better",
    "going",
    "predict",
    "onset",
    "diabetes",
    "problem",
    "comprises",
    "768",
    "observations",
    "medical",
    "details",
    "pima",
    "indian",
    "patients",
    "record",
    "describes",
    "instantaneous",
    "measurement",
    "taken",
    "patient",
    "age",
    "number",
    "times",
    "pregnant",
    "blood",
    "work",
    "crew",
    "patients",
    "women",
    "aged",
    "21",
    "older",
    "attributes",
    "numeric",
    "unit",
    "vary",
    "attribute",
    "attribute",
    "record",
    "class",
    "value",
    "indicate",
    "whether",
    "patient",
    "suffered",
    "onset",
    "diabetes",
    "within",
    "five",
    "years",
    "measurements",
    "classified",
    "0",
    "broken",
    "whole",
    "process",
    "following",
    "steps",
    "first",
    "step",
    "handling",
    "data",
    "load",
    "data",
    "csv",
    "file",
    "split",
    "training",
    "test",
    "second",
    "step",
    "summarizing",
    "data",
    "summarize",
    "properties",
    "training",
    "data",
    "sets",
    "calculate",
    "probabilities",
    "make",
    "predictions",
    "third",
    "step",
    "comes",
    "making",
    "particular",
    "prediction",
    "use",
    "summaries",
    "data",
    "set",
    "generate",
    "single",
    "prediction",
    "generate",
    "predictions",
    "given",
    "test",
    "data",
    "set",
    "summarized",
    "training",
    "data",
    "sets",
    "finally",
    "evaluate",
    "accuracy",
    "predictions",
    "made",
    "test",
    "data",
    "set",
    "percentage",
    "correct",
    "predictions",
    "made",
    "finally",
    "tied",
    "together",
    "form",
    "model",
    "nape",
    "classifier",
    "first",
    "thing",
    "need",
    "load",
    "data",
    "data",
    "csv",
    "format",
    "without",
    "header",
    "line",
    "codes",
    "open",
    "file",
    "open",
    "function",
    "read",
    "data",
    "lines",
    "using",
    "read",
    "functions",
    "csv",
    "module",
    "also",
    "need",
    "convert",
    "attributes",
    "loaded",
    "strings",
    "numbers",
    "work",
    "let",
    "show",
    "implemented",
    "need",
    "tall",
    "python",
    "system",
    "use",
    "jupyter",
    "notebook",
    "python",
    "shell",
    "hey",
    "using",
    "anaconda",
    "navigator",
    "things",
    "required",
    "programming",
    "python",
    "jupiter",
    "lab",
    "notebook",
    "qt",
    "console",
    "even",
    "studio",
    "well",
    "need",
    "install",
    "anaconda",
    "navigator",
    "comes",
    "pre",
    "installed",
    "python",
    "also",
    "moment",
    "click",
    "launch",
    "jupyter",
    "notebook",
    "take",
    "jupiter",
    "homepage",
    "local",
    "system",
    "programming",
    "python",
    "let",
    "rename",
    "india",
    "diabetes",
    "first",
    "need",
    "load",
    "data",
    "set",
    "creating",
    "function",
    "load",
    "csv",
    "need",
    "import",
    "certain",
    "csv",
    "math",
    "random",
    "method",
    "see",
    "created",
    "load",
    "csv",
    "function",
    "take",
    "pie",
    "indian",
    "diabetes",
    "data",
    "dot",
    "csv",
    "file",
    "using",
    "csv",
    "dot",
    "read",
    "method",
    "converting",
    "every",
    "element",
    "data",
    "set",
    "float",
    "originally",
    "ants",
    "string",
    "need",
    "convert",
    "floor",
    "calculation",
    "purposes",
    "next",
    "need",
    "split",
    "data",
    "training",
    "data",
    "sets",
    "nay",
    "bias",
    "use",
    "make",
    "prediction",
    "data",
    "set",
    "use",
    "evaluate",
    "accuracy",
    "model",
    "need",
    "split",
    "data",
    "set",
    "randomly",
    "training",
    "testing",
    "data",
    "set",
    "ratio",
    "usually",
    "example",
    "going",
    "use",
    "67",
    "33",
    "70",
    "30",
    "ratio",
    "testing",
    "algorithms",
    "play",
    "around",
    "number",
    "split",
    "data",
    "set",
    "function",
    "navy",
    "base",
    "model",
    "comprised",
    "summary",
    "data",
    "training",
    "data",
    "set",
    "summary",
    "used",
    "making",
    "predictions",
    "summary",
    "training",
    "data",
    "collected",
    "involves",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "class",
    "value",
    "example",
    "two",
    "class",
    "values",
    "seven",
    "numerical",
    "attributes",
    "need",
    "mean",
    "standard",
    "deviation",
    "seven",
    "attributes",
    "class",
    "value",
    "makes",
    "14",
    "attributes",
    "summaries",
    "break",
    "preparation",
    "summary",
    "following",
    "sub",
    "tasks",
    "separating",
    "data",
    "class",
    "calculating",
    "mean",
    "calculating",
    "standard",
    "deviation",
    "summarizing",
    "data",
    "sets",
    "summarizing",
    "attributes",
    "class",
    "first",
    "task",
    "separate",
    "training",
    "data",
    "set",
    "instances",
    "class",
    "value",
    "calculate",
    "statistics",
    "class",
    "creating",
    "map",
    "class",
    "value",
    "list",
    "instances",
    "belong",
    "class",
    "class",
    "sort",
    "entire",
    "dataset",
    "instances",
    "appropriate",
    "list",
    "separate",
    "class",
    "function",
    "see",
    "function",
    "assumes",
    "last",
    "attribute",
    "class",
    "value",
    "function",
    "returns",
    "map",
    "class",
    "value",
    "list",
    "data",
    "instances",
    "next",
    "need",
    "calculate",
    "mean",
    "attribute",
    "class",
    "value",
    "mean",
    "central",
    "middle",
    "central",
    "tendency",
    "data",
    "use",
    "middle",
    "gaussian",
    "distribution",
    "calculating",
    "probabilities",
    "function",
    "mean",
    "also",
    "need",
    "calculate",
    "standard",
    "deviation",
    "attribute",
    "class",
    "value",
    "standard",
    "deviation",
    "calculated",
    "square",
    "root",
    "variance",
    "variance",
    "calculated",
    "average",
    "squared",
    "differences",
    "attribute",
    "value",
    "mean",
    "one",
    "thing",
    "note",
    "using",
    "n",
    "minus",
    "one",
    "method",
    "subtracts",
    "one",
    "number",
    "attributes",
    "values",
    "calculating",
    "variance",
    "tools",
    "summarize",
    "data",
    "given",
    "list",
    "instances",
    "calculate",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "function",
    "groups",
    "values",
    "attribute",
    "across",
    "data",
    "instances",
    "lists",
    "compute",
    "mean",
    "standard",
    "deviation",
    "values",
    "attribute",
    "next",
    "comes",
    "summarizing",
    "attributes",
    "class",
    "pull",
    "together",
    "first",
    "separating",
    "training",
    "data",
    "sets",
    "instances",
    "groped",
    "class",
    "calculating",
    "summaries",
    "ready",
    "make",
    "predictions",
    "using",
    "summaries",
    "prepared",
    "training",
    "data",
    "making",
    "patients",
    "involved",
    "calculating",
    "probability",
    "given",
    "data",
    "instance",
    "belong",
    "class",
    "selecting",
    "class",
    "largest",
    "probability",
    "prediction",
    "divide",
    "whole",
    "method",
    "four",
    "tasks",
    "calculating",
    "gaussian",
    "probability",
    "density",
    "function",
    "calculating",
    "class",
    "probability",
    "making",
    "prediction",
    "estimating",
    "accuracy",
    "calculate",
    "gaussian",
    "probability",
    "density",
    "function",
    "use",
    "gaussian",
    "function",
    "estimate",
    "probability",
    "given",
    "attribute",
    "value",
    "given",
    "node",
    "mean",
    "standard",
    "deviation",
    "attribute",
    "estimated",
    "training",
    "data",
    "see",
    "parameters",
    "rx",
    "mean",
    "standard",
    "deviation",
    "calculate",
    "probability",
    "function",
    "calculate",
    "exponent",
    "first",
    "calculate",
    "main",
    "division",
    "lets",
    "us",
    "fit",
    "equation",
    "nicely",
    "two",
    "lines",
    "next",
    "task",
    "calculating",
    "class",
    "properties",
    "calculate",
    "probability",
    "attribute",
    "belonging",
    "class",
    "combine",
    "probabilities",
    "attributes",
    "values",
    "data",
    "instance",
    "come",
    "probability",
    "entire",
    "data",
    "instance",
    "belonging",
    "class",
    "calculated",
    "class",
    "properties",
    "time",
    "finally",
    "make",
    "first",
    "prediction",
    "calculate",
    "probability",
    "data",
    "instance",
    "belong",
    "class",
    "value",
    "look",
    "largest",
    "probability",
    "return",
    "associated",
    "class",
    "going",
    "use",
    "function",
    "predict",
    "uses",
    "summaries",
    "input",
    "vector",
    "basically",
    "probabilities",
    "input",
    "particular",
    "label",
    "finally",
    "estimate",
    "accuracy",
    "model",
    "making",
    "predictions",
    "data",
    "instances",
    "test",
    "data",
    "use",
    "get",
    "predictions",
    "method",
    "method",
    "used",
    "calculate",
    "predictions",
    "based",
    "upon",
    "test",
    "data",
    "sets",
    "summary",
    "training",
    "data",
    "set",
    "predictions",
    "compared",
    "class",
    "values",
    "test",
    "data",
    "set",
    "classification",
    "accuracy",
    "calculated",
    "accuracy",
    "ratio",
    "zeros",
    "hundred",
    "percent",
    "get",
    "accuracy",
    "method",
    "calculate",
    "accuracy",
    "ratio",
    "finally",
    "sum",
    "define",
    "main",
    "function",
    "call",
    "methods",
    "defined",
    "earlier",
    "one",
    "one",
    "get",
    "courtesy",
    "model",
    "created",
    "see",
    "main",
    "function",
    "file",
    "name",
    "defined",
    "split",
    "ratio",
    "data",
    "set",
    "training",
    "test",
    "data",
    "set",
    "using",
    "split",
    "data",
    "set",
    "method",
    "next",
    "using",
    "summarized",
    "class",
    "function",
    "using",
    "get",
    "protection",
    "get",
    "accuracy",
    "method",
    "well",
    "guys",
    "see",
    "output",
    "one",
    "gives",
    "us",
    "splitting",
    "768",
    "rose",
    "514",
    "training",
    "254",
    "test",
    "data",
    "set",
    "rows",
    "accuracy",
    "model",
    "68",
    "play",
    "amount",
    "training",
    "test",
    "data",
    "sets",
    "used",
    "change",
    "split",
    "ratio",
    "seventies",
    "238",
    "220",
    "get",
    "different",
    "sort",
    "accuracy",
    "suppose",
    "change",
    "split",
    "ratio",
    "see",
    "get",
    "accuracy",
    "62",
    "percent",
    "splitting",
    "gave",
    "us",
    "better",
    "result",
    "68",
    "percent",
    "implement",
    "navy",
    "bias",
    "caution",
    "classifier",
    "step",
    "step",
    "methods",
    "need",
    "case",
    "using",
    "nave",
    "bayes",
    "classifier",
    "worry",
    "need",
    "write",
    "many",
    "lines",
    "code",
    "make",
    "model",
    "second",
    "really",
    "comes",
    "picture",
    "library",
    "predefined",
    "method",
    "say",
    "predefined",
    "function",
    "nape",
    "bias",
    "converts",
    "lines",
    "course",
    "merely",
    "two",
    "three",
    "lines",
    "codes",
    "let",
    "open",
    "another",
    "jupyter",
    "notebook",
    "let",
    "name",
    "sklearn",
    "pass",
    "going",
    "use",
    "famous",
    "data",
    "set",
    "iris",
    "de",
    "casa",
    "iris",
    "flower",
    "data",
    "set",
    "multivariate",
    "data",
    "set",
    "introduced",
    "british",
    "statistician",
    "biologists",
    "roland",
    "fisher",
    "based",
    "fish",
    "linear",
    "discriminant",
    "model",
    "data",
    "set",
    "became",
    "typical",
    "test",
    "case",
    "many",
    "statistical",
    "classification",
    "techniques",
    "machine",
    "learning",
    "going",
    "use",
    "caution",
    "nb",
    "model",
    "already",
    "available",
    "sklearn",
    "mentioned",
    "earlier",
    "three",
    "types",
    "neighbors",
    "question",
    "multinomial",
    "bernouli",
    "going",
    "use",
    "caution",
    "model",
    "already",
    "present",
    "sk",
    "loan",
    "library",
    "cycle",
    "library",
    "first",
    "need",
    "import",
    "sklearn",
    "data",
    "sets",
    "metrics",
    "also",
    "need",
    "import",
    "caution",
    "nb",
    "libraries",
    "lowered",
    "need",
    "load",
    "data",
    "set",
    "iris",
    "dataset",
    "next",
    "need",
    "fit",
    "nave",
    "smaller",
    "data",
    "set",
    "see",
    "easily",
    "defined",
    "model",
    "gaussian",
    "nb",
    "contains",
    "programming",
    "showed",
    "earlier",
    "methods",
    "taking",
    "input",
    "calculating",
    "mean",
    "standard",
    "deviation",
    "separating",
    "bike",
    "last",
    "finally",
    "making",
    "predictions",
    "calculating",
    "prediction",
    "accuracy",
    "comes",
    "caution",
    "method",
    "inside",
    "already",
    "present",
    "sklearn",
    "library",
    "need",
    "fit",
    "according",
    "data",
    "set",
    "next",
    "print",
    "model",
    "see",
    "gaussian",
    "nb",
    "model",
    "next",
    "need",
    "make",
    "predictions",
    "expected",
    "output",
    "data",
    "set",
    "dot",
    "target",
    "projected",
    "using",
    "pretend",
    "model",
    "model",
    "using",
    "cause",
    "n",
    "summarize",
    "model",
    "created",
    "calculate",
    "confusion",
    "matrix",
    "classification",
    "report",
    "guys",
    "see",
    "classification",
    "provide",
    "precision",
    "point",
    "ninety",
    "six",
    "recall",
    "f1",
    "score",
    "support",
    "finally",
    "print",
    "confusion",
    "matrix",
    "see",
    "gives",
    "us",
    "output",
    "see",
    "using",
    "gaussian",
    "method",
    "putting",
    "model",
    "using",
    "data",
    "fitting",
    "model",
    "created",
    "particular",
    "data",
    "set",
    "getting",
    "desired",
    "output",
    "easy",
    "library",
    "mo",
    "support",
    "vector",
    "machine",
    "one",
    "effective",
    "machine",
    "learning",
    "classifier",
    "used",
    "various",
    "fields",
    "face",
    "recognition",
    "cancer",
    "classification",
    "today",
    "session",
    "dedicated",
    "svm",
    "works",
    "various",
    "features",
    "svm",
    "used",
    "real",
    "world",
    "right",
    "okay",
    "let",
    "move",
    "see",
    "svm",
    "algorithm",
    "guys",
    "vm",
    "support",
    "vector",
    "machine",
    "supervised",
    "learning",
    "algorithm",
    "mainly",
    "used",
    "classify",
    "data",
    "different",
    "classes",
    "unlike",
    "algorithms",
    "svm",
    "makes",
    "use",
    "hyperplane",
    "acts",
    "like",
    "decision",
    "boundary",
    "various",
    "classes",
    "general",
    "svm",
    "used",
    "generate",
    "multiple",
    "separating",
    "hyperplanes",
    "data",
    "divided",
    "segments",
    "okay",
    "segments",
    "contain",
    "one",
    "kind",
    "data",
    "mainly",
    "used",
    "classification",
    "purpose",
    "wearing",
    "want",
    "classify",
    "data",
    "two",
    "different",
    "segments",
    "depending",
    "features",
    "data",
    "moving",
    "let",
    "discuss",
    "features",
    "svm",
    "like",
    "mentioned",
    "earlier",
    "svm",
    "supervised",
    "learning",
    "algorithm",
    "means",
    "svm",
    "trains",
    "set",
    "labeled",
    "data",
    "svm",
    "studies",
    "label",
    "training",
    "data",
    "classifies",
    "new",
    "input",
    "data",
    "depending",
    "learned",
    "training",
    "phase",
    "main",
    "advantage",
    "support",
    "vector",
    "machine",
    "used",
    "classification",
    "regression",
    "problems",
    "right",
    "even",
    "though",
    "svm",
    "mainly",
    "known",
    "classification",
    "svr",
    "support",
    "vector",
    "regressor",
    "used",
    "regression",
    "problems",
    "right",
    "svm",
    "used",
    "classification",
    "regression",
    "one",
    "reasons",
    "lot",
    "people",
    "prefer",
    "svm",
    "good",
    "classifier",
    "along",
    "also",
    "used",
    "regression",
    "okay",
    "another",
    "feature",
    "svm",
    "kernel",
    "functions",
    "svm",
    "used",
    "classifying",
    "nonlinear",
    "data",
    "using",
    "kernel",
    "trick",
    "kernel",
    "trick",
    "basically",
    "means",
    "transform",
    "data",
    "another",
    "dimension",
    "easily",
    "draw",
    "hyperplane",
    "different",
    "classes",
    "data",
    "alright",
    "nonlinear",
    "data",
    "basically",
    "data",
    "separated",
    "straight",
    "line",
    "alright",
    "svm",
    "even",
    "used",
    "nonlinear",
    "data",
    "sets",
    "use",
    "kernel",
    "functions",
    "right",
    "guys",
    "hope",
    "clear",
    "basic",
    "concepts",
    "svm",
    "let",
    "move",
    "look",
    "svm",
    "works",
    "order",
    "understand",
    "svm",
    "works",
    "let",
    "consider",
    "small",
    "scenario",
    "second",
    "pretend",
    "firm",
    "okay",
    "let",
    "say",
    "problem",
    "want",
    "set",
    "fence",
    "protect",
    "rabbits",
    "pack",
    "wolves",
    "okay",
    "build",
    "films",
    "one",
    "way",
    "get",
    "around",
    "problem",
    "build",
    "classifier",
    "based",
    "position",
    "rabbits",
    "words",
    "pasture",
    "telling",
    "classify",
    "group",
    "rabbits",
    "one",
    "group",
    "draw",
    "decision",
    "boundary",
    "rabbits",
    "world",
    "correct",
    "try",
    "draw",
    "decision",
    "boundary",
    "rabbits",
    "wolves",
    "looks",
    "something",
    "like",
    "okay",
    "clearly",
    "build",
    "fence",
    "along",
    "line",
    "simple",
    "terms",
    "exactly",
    "spm",
    "work",
    "draws",
    "decision",
    "boundary",
    "hyperplane",
    "new",
    "classes",
    "order",
    "separate",
    "classify",
    "know",
    "thinking",
    "know",
    "draw",
    "hyperplane",
    "basic",
    "principle",
    "behind",
    "svm",
    "draw",
    "hyperplane",
    "best",
    "separates",
    "two",
    "classes",
    "case",
    "two",
    "glasses",
    "rabbits",
    "wolves",
    "start",
    "drawing",
    "random",
    "hyperplane",
    "check",
    "distance",
    "hyperplane",
    "closest",
    "data",
    "points",
    "club",
    "closes",
    "data",
    "points",
    "hyperplane",
    "known",
    "support",
    "vectors",
    "name",
    "comes",
    "support",
    "vector",
    "machine",
    "basically",
    "hyperplane",
    "drawn",
    "based",
    "support",
    "vectors",
    "guys",
    "optimal",
    "hyperplane",
    "maximum",
    "distance",
    "support",
    "vectors",
    "right",
    "basically",
    "hyperplane",
    "maximum",
    "distance",
    "support",
    "vectors",
    "optimal",
    "hyperplane",
    "distance",
    "hyperplane",
    "support",
    "vectors",
    "known",
    "margin",
    "right",
    "sum",
    "svm",
    "used",
    "classify",
    "data",
    "using",
    "hyper",
    "plane",
    "distance",
    "hyperplane",
    "support",
    "vectors",
    "maximum",
    "basically",
    "margin",
    "maximum",
    "right",
    "way",
    "know",
    "actually",
    "separating",
    "classes",
    "add",
    "distance",
    "two",
    "classes",
    "maximum",
    "okay",
    "let",
    "try",
    "solve",
    "problem",
    "okay",
    "let",
    "say",
    "input",
    "new",
    "data",
    "point",
    "okay",
    "new",
    "data",
    "point",
    "want",
    "draw",
    "hyper",
    "plane",
    "best",
    "separates",
    "two",
    "classes",
    "okay",
    "start",
    "drawing",
    "hyperplane",
    "like",
    "check",
    "distance",
    "hyperplane",
    "support",
    "vectors",
    "okay",
    "trying",
    "check",
    "margin",
    "maximum",
    "hyper",
    "plane",
    "draw",
    "hyperplane",
    "like",
    "right",
    "going",
    "check",
    "support",
    "vectors",
    "going",
    "check",
    "distance",
    "support",
    "vectors",
    "hyperplane",
    "clear",
    "margin",
    "red",
    "compare",
    "margin",
    "previous",
    "one",
    "hyperplane",
    "reason",
    "choosing",
    "hyperplane",
    "distance",
    "support",
    "vectors",
    "hyperplane",
    "maximum",
    "scenario",
    "okay",
    "guys",
    "choose",
    "hyperplane",
    "basically",
    "make",
    "sure",
    "hyper",
    "plane",
    "maximum",
    "margin",
    "right",
    "best",
    "separate",
    "two",
    "classes",
    "right",
    "okay",
    "far",
    "quite",
    "easy",
    "data",
    "linearly",
    "separable",
    "means",
    "could",
    "draw",
    "straight",
    "line",
    "separate",
    "two",
    "classes",
    "right",
    "data",
    "set",
    "like",
    "possibly",
    "ca",
    "draw",
    "hyperplane",
    "like",
    "separate",
    "two",
    "classes",
    "situations",
    "earlier",
    "session",
    "mentioned",
    "kernel",
    "used",
    "transform",
    "data",
    "another",
    "dimension",
    "clear",
    "dividing",
    "margin",
    "classes",
    "data",
    "alright",
    "kernel",
    "functions",
    "offer",
    "user",
    "option",
    "transforming",
    "nonlinear",
    "spaces",
    "linear",
    "ones",
    "nonlinear",
    "data",
    "set",
    "one",
    "ca",
    "separate",
    "using",
    "straight",
    "line",
    "right",
    "order",
    "deal",
    "data",
    "sets",
    "going",
    "transform",
    "linear",
    "data",
    "sets",
    "use",
    "svm",
    "okay",
    "simple",
    "trick",
    "would",
    "transform",
    "two",
    "variables",
    "x",
    "new",
    "feature",
    "space",
    "involving",
    "new",
    "variable",
    "called",
    "right",
    "guys",
    "far",
    "plotting",
    "data",
    "two",
    "dimensional",
    "space",
    "correct",
    "using",
    "x",
    "axis",
    "two",
    "variables",
    "x",
    "order",
    "deal",
    "kind",
    "data",
    "simple",
    "trick",
    "transform",
    "two",
    "variables",
    "x",
    "new",
    "feature",
    "space",
    "involving",
    "new",
    "variable",
    "called",
    "okay",
    "basically",
    "visualizing",
    "data",
    "space",
    "transform",
    "2d",
    "space",
    "3d",
    "space",
    "clearly",
    "see",
    "dividing",
    "margin",
    "two",
    "classes",
    "data",
    "right",
    "go",
    "ahead",
    "separate",
    "two",
    "classes",
    "drawing",
    "best",
    "hyperplane",
    "okay",
    "exactly",
    "discussed",
    "previous",
    "slides",
    "guys",
    "try",
    "dried",
    "drawing",
    "hyperplane",
    "optimum",
    "two",
    "classes",
    "right",
    "guys",
    "hope",
    "good",
    "understanding",
    "nonlinear",
    "svm",
    "let",
    "look",
    "real",
    "world",
    "use",
    "case",
    "support",
    "vector",
    "machines",
    "guys",
    "vm",
    "classifier",
    "used",
    "cancer",
    "classification",
    "since",
    "early",
    "2000s",
    "experiment",
    "held",
    "group",
    "professionals",
    "applied",
    "svm",
    "colon",
    "cancer",
    "tissue",
    "classification",
    "data",
    "set",
    "consisted",
    "transmembrane",
    "protein",
    "samples",
    "50",
    "200",
    "genes",
    "samples",
    "input",
    "svm",
    "classifier",
    "sample",
    "input",
    "svm",
    "classifier",
    "colon",
    "cancer",
    "tissue",
    "samples",
    "normal",
    "colon",
    "tissue",
    "samples",
    "right",
    "main",
    "objective",
    "study",
    "classify",
    "gene",
    "samples",
    "based",
    "whether",
    "cancerous",
    "okay",
    "svm",
    "trained",
    "using",
    "50",
    "200",
    "samples",
    "order",
    "discriminate",
    "tumor",
    "specimens",
    "performance",
    "svm",
    "classifier",
    "accurate",
    "even",
    "small",
    "data",
    "set",
    "right",
    "50",
    "200",
    "samples",
    "even",
    "small",
    "data",
    "set",
    "svm",
    "pretty",
    "accurate",
    "results",
    "performance",
    "compared",
    "classification",
    "algorithms",
    "like",
    "naive",
    "bayes",
    "case",
    "svm",
    "outperform",
    "naive",
    "bayes",
    "experiment",
    "clear",
    "svm",
    "classified",
    "data",
    "effectively",
    "worked",
    "exceptionally",
    "good",
    "small",
    "data",
    "sets",
    "let",
    "go",
    "ahead",
    "understand",
    "exactly",
    "unsupervised",
    "learning",
    "sometimes",
    "given",
    "data",
    "unstructured",
    "unlabeled",
    "becomes",
    "difficult",
    "classify",
    "data",
    "different",
    "categories",
    "unsupervised",
    "learning",
    "helps",
    "solve",
    "problem",
    "learning",
    "used",
    "cluster",
    "input",
    "data",
    "classes",
    "basis",
    "statistical",
    "properties",
    "example",
    "cluster",
    "different",
    "bikes",
    "based",
    "upon",
    "speed",
    "limit",
    "acceleration",
    "average",
    "giving",
    "suppose",
    "learning",
    "type",
    "machine",
    "learning",
    "algorithm",
    "used",
    "draw",
    "inferences",
    "beta",
    "sets",
    "consisting",
    "input",
    "data",
    "without",
    "labeled",
    "responses",
    "look",
    "workflow",
    "process",
    "flow",
    "unsupervised",
    "learning",
    "training",
    "data",
    "collection",
    "information",
    "without",
    "label",
    "machine",
    "learning",
    "algorithm",
    "clustering",
    "malls",
    "distributes",
    "data",
    "different",
    "class",
    "provide",
    "unreliable",
    "new",
    "data",
    "make",
    "prediction",
    "find",
    "cluster",
    "particular",
    "data",
    "data",
    "set",
    "belongs",
    "particular",
    "data",
    "point",
    "belongs",
    "one",
    "important",
    "algorithms",
    "unsupervised",
    "learning",
    "clustering",
    "let",
    "understand",
    "exactly",
    "clustering",
    "clustering",
    "basically",
    "process",
    "dividing",
    "data",
    "sets",
    "groups",
    "consisting",
    "similar",
    "data",
    "points",
    "means",
    "grouping",
    "objects",
    "based",
    "information",
    "found",
    "data",
    "describing",
    "object",
    "objects",
    "relationships",
    "clustering",
    "malls",
    "focus",
    "defying",
    "groups",
    "similar",
    "records",
    "labeling",
    "records",
    "according",
    "group",
    "belong",
    "done",
    "without",
    "benefit",
    "prior",
    "knowledge",
    "groups",
    "characteristics",
    "fact",
    "may",
    "even",
    "know",
    "exactly",
    "many",
    "groups",
    "look",
    "models",
    "often",
    "referred",
    "unsupervised",
    "learning",
    "models",
    "since",
    "external",
    "standard",
    "judge",
    "one",
    "classification",
    "performance",
    "right",
    "wrong",
    "answers",
    "model",
    "talk",
    "clustering",
    "used",
    "goal",
    "clustering",
    "determine",
    "intrinsic",
    "group",
    "set",
    "unlabeled",
    "data",
    "sometime",
    "partitioning",
    "goal",
    "purpose",
    "clustering",
    "algorithm",
    "make",
    "sense",
    "exact",
    "value",
    "last",
    "set",
    "structured",
    "unstructured",
    "data",
    "clustering",
    "used",
    "industry",
    "look",
    "video",
    "use",
    "cases",
    "clustering",
    "industry",
    "first",
    "used",
    "marketing",
    "discovering",
    "distinct",
    "groups",
    "customer",
    "databases",
    "customers",
    "make",
    "lot",
    "calls",
    "customers",
    "use",
    "internet",
    "cause",
    "also",
    "using",
    "insurance",
    "companies",
    "like",
    "need",
    "find",
    "groups",
    "corporation",
    "insurance",
    "policy",
    "holders",
    "high",
    "average",
    "claim",
    "rate",
    "farmers",
    "crash",
    "cops",
    "profitable",
    "using",
    "c",
    "smith",
    "studies",
    "defined",
    "problem",
    "areas",
    "oil",
    "gas",
    "exploration",
    "based",
    "seesmic",
    "data",
    "also",
    "used",
    "recommendation",
    "movies",
    "would",
    "say",
    "also",
    "used",
    "flickr",
    "photos",
    "also",
    "used",
    "amazon",
    "recommending",
    "product",
    "category",
    "lies",
    "basically",
    "talk",
    "clustering",
    "three",
    "types",
    "clustering",
    "first",
    "exclusive",
    "clustering",
    "hard",
    "clustering",
    "item",
    "belongs",
    "exclusively",
    "one",
    "cluster",
    "several",
    "clusters",
    "data",
    "point",
    "along",
    "exclusively",
    "one",
    "cluster",
    "example",
    "clustering",
    "clustering",
    "exclusive",
    "kind",
    "clustering",
    "secondly",
    "overlapping",
    "clustering",
    "also",
    "known",
    "soft",
    "clusters",
    "item",
    "belong",
    "multiple",
    "clusters",
    "degree",
    "association",
    "cluster",
    "shown",
    "example",
    "fuzzy",
    "c",
    "means",
    "clustering",
    "used",
    "overlapping",
    "clustering",
    "finally",
    "hierarchical",
    "clustering",
    "two",
    "clusters",
    "relationship",
    "structure",
    "known",
    "hierarchical",
    "cluster",
    "see",
    "example",
    "kind",
    "relationship",
    "cluster",
    "given",
    "let",
    "understand",
    "exactly",
    "k",
    "means",
    "clustering",
    "clustering",
    "algorithm",
    "whose",
    "main",
    "goal",
    "group",
    "similar",
    "elements",
    "data",
    "points",
    "cluster",
    "process",
    "objects",
    "classified",
    "predefined",
    "number",
    "groups",
    "much",
    "dissimilar",
    "possible",
    "one",
    "group",
    "another",
    "group",
    "much",
    "similar",
    "possible",
    "within",
    "group",
    "look",
    "algorithm",
    "working",
    "right",
    "first",
    "starts",
    "defying",
    "number",
    "clusters",
    "k",
    "find",
    "centroid",
    "find",
    "distance",
    "objects",
    "distance",
    "object",
    "centroid",
    "distance",
    "object",
    "centroid",
    "find",
    "dropping",
    "based",
    "minimum",
    "distance",
    "centroid",
    "converse",
    "true",
    "make",
    "cluster",
    "false",
    "ca",
    "find",
    "centroid",
    "repeat",
    "steps",
    "let",
    "show",
    "exactly",
    "clustering",
    "example",
    "first",
    "need",
    "decide",
    "number",
    "clusters",
    "made",
    "another",
    "important",
    "task",
    "decide",
    "important",
    "number",
    "clusters",
    "decide",
    "number",
    "clusters",
    "really",
    "get",
    "later",
    "first",
    "let",
    "assume",
    "number",
    "number",
    "clusters",
    "decided",
    "3",
    "provide",
    "centroids",
    "clusters",
    "guessing",
    "algorithm",
    "calculates",
    "euclidean",
    "distance",
    "point",
    "centroid",
    "assigns",
    "data",
    "point",
    "closest",
    "cluster",
    "euclidean",
    "distance",
    "know",
    "square",
    "root",
    "distance",
    "square",
    "root",
    "square",
    "distance",
    "next",
    "center",
    "calculated",
    "new",
    "clusters",
    "data",
    "point",
    "distance",
    "points",
    "new",
    "clusters",
    "calculated",
    "points",
    "assigned",
    "closest",
    "cluster",
    "new",
    "centroid",
    "scattered",
    "steps",
    "repeated",
    "repetition",
    "centroids",
    "new",
    "center",
    "eyes",
    "close",
    "previous",
    "ones",
    "antenna",
    "less",
    "output",
    "gets",
    "repeated",
    "outputs",
    "close",
    "enough",
    "stop",
    "process",
    "keep",
    "calculating",
    "euclidean",
    "distance",
    "points",
    "centroids",
    "calculate",
    "new",
    "centroids",
    "clay",
    "means",
    "clustering",
    "works",
    "basically",
    "important",
    "part",
    "understand",
    "decide",
    "value",
    "k",
    "number",
    "clusters",
    "make",
    "sense",
    "know",
    "many",
    "classes",
    "going",
    "make",
    "decide",
    "number",
    "clusters",
    "elbow",
    "method",
    "let",
    "assume",
    "first",
    "compute",
    "sum",
    "squared",
    "error",
    "sse4",
    "value",
    "example",
    "let",
    "take",
    "two",
    "four",
    "six",
    "eight",
    "ss",
    "e",
    "sum",
    "squared",
    "defined",
    "sum",
    "squared",
    "distance",
    "number",
    "member",
    "cluster",
    "centroid",
    "mathematically",
    "mathematically",
    "given",
    "equation",
    "provided",
    "brought",
    "key",
    "sse",
    "see",
    "error",
    "decreases",
    "k",
    "gets",
    "large",
    "number",
    "cluster",
    "increases",
    "smaller",
    "torsion",
    "also",
    "smaller",
    "know",
    "idea",
    "elbow",
    "method",
    "choose",
    "k",
    "ssc",
    "decreases",
    "abruptly",
    "example",
    "look",
    "figure",
    "given",
    "see",
    "best",
    "number",
    "cluster",
    "elbow",
    "see",
    "graph",
    "changes",
    "abruptly",
    "number",
    "four",
    "particular",
    "example",
    "going",
    "use",
    "number",
    "cluster",
    "first",
    "working",
    "clustering",
    "two",
    "key",
    "points",
    "know",
    "first",
    "careful",
    "various",
    "start",
    "choosing",
    "first",
    "center",
    "random",
    "choosing",
    "second",
    "center",
    "far",
    "away",
    "first",
    "center",
    "similarly",
    "choosing",
    "nih",
    "center",
    "far",
    "away",
    "possible",
    "closest",
    "centers",
    "second",
    "idea",
    "many",
    "runs",
    "different",
    "random",
    "starting",
    "points",
    "get",
    "idea",
    "exactly",
    "many",
    "clusters",
    "need",
    "make",
    "exactly",
    "centroid",
    "lies",
    "data",
    "getting",
    "confused",
    "exactly",
    "good",
    "method",
    "let",
    "understand",
    "pros",
    "cons",
    "clay",
    "means",
    "clusterings",
    "know",
    "simple",
    "understandable",
    "everyone",
    "loves",
    "first",
    "go",
    "items",
    "automatically",
    "assigned",
    "clusters",
    "look",
    "cons",
    "first",
    "one",
    "needs",
    "define",
    "number",
    "clusters",
    "heavy",
    "task",
    "asks",
    "us",
    "10",
    "categories",
    "know",
    "number",
    "clusters",
    "going",
    "difficult",
    "anyone",
    "know",
    "guess",
    "number",
    "clusters",
    "items",
    "forced",
    "clusters",
    "whether",
    "actually",
    "belong",
    "cluster",
    "category",
    "forced",
    "lie",
    "category",
    "closest",
    "happens",
    "number",
    "clusters",
    "defining",
    "correct",
    "number",
    "clusters",
    "able",
    "guess",
    "correct",
    "number",
    "clusters",
    "unable",
    "handle",
    "noisy",
    "data",
    "outliners",
    "anyway",
    "machine",
    "learning",
    "engineers",
    "data",
    "scientists",
    "clean",
    "data",
    "comes",
    "analysis",
    "watch",
    "method",
    "using",
    "typically",
    "people",
    "clean",
    "data",
    "clustering",
    "even",
    "clean",
    "sometimes",
    "see",
    "noisy",
    "outliners",
    "data",
    "affect",
    "whole",
    "model",
    "clustering",
    "going",
    "use",
    "clustering",
    "data",
    "set",
    "find",
    "number",
    "clusters",
    "divide",
    "accordingly",
    "use",
    "case",
    "first",
    "data",
    "set",
    "five",
    "thousand",
    "movies",
    "want",
    "grip",
    "movies",
    "clusters",
    "based",
    "facebook",
    "likes",
    "guys",
    "let",
    "look",
    "demo",
    "first",
    "going",
    "import",
    "deep",
    "copy",
    "numpy",
    "pandas",
    "seaborn",
    "various",
    "libraries",
    "going",
    "use",
    "map",
    "popular",
    "videos",
    "use",
    "ply",
    "plot",
    "going",
    "use",
    "ggplot",
    "next",
    "going",
    "import",
    "data",
    "set",
    "look",
    "shape",
    "data",
    "look",
    "shape",
    "data",
    "set",
    "see",
    "5043",
    "rows",
    "twenty",
    "eight",
    "columns",
    "look",
    "head",
    "data",
    "set",
    "see",
    "5043",
    "data",
    "points",
    "going",
    "place",
    "data",
    "points",
    "plot",
    "take",
    "director",
    "facebook",
    "likes",
    "look",
    "data",
    "columns",
    "face",
    "number",
    "post",
    "cars",
    "total",
    "facebook",
    "likes",
    "director",
    "facebook",
    "likes",
    "done",
    "taking",
    "director",
    "facebook",
    "likes",
    "actor",
    "three",
    "facebook",
    "likes",
    "right",
    "five",
    "thousand",
    "forty",
    "three",
    "rows",
    "two",
    "columns",
    "using",
    "sklearn",
    "going",
    "import",
    "first",
    "going",
    "import",
    "sklearn",
    "dot",
    "cluster",
    "remember",
    "guys",
    "escalon",
    "important",
    "library",
    "python",
    "machine",
    "learning",
    "number",
    "cluster",
    "going",
    "provide",
    "five",
    "number",
    "cluster",
    "depends",
    "upon",
    "sse",
    "sum",
    "squared",
    "errors",
    "going",
    "use",
    "elbow",
    "method",
    "going",
    "go",
    "details",
    "going",
    "fit",
    "data",
    "fit",
    "find",
    "cluster",
    "us",
    "printed",
    "find",
    "array",
    "five",
    "clusters",
    "fa",
    "print",
    "label",
    "caymans",
    "cluster",
    "next",
    "going",
    "plot",
    "data",
    "clusters",
    "new",
    "data",
    "clusters",
    "found",
    "going",
    "use",
    "si",
    "bon",
    "see",
    "plotted",
    "car",
    "plotted",
    "data",
    "grid",
    "see",
    "five",
    "clusters",
    "probably",
    "would",
    "say",
    "cluster",
    "3",
    "cluster",
    "zero",
    "close",
    "might",
    "depend",
    "see",
    "exactly",
    "going",
    "say",
    "initially",
    "main",
    "challenge",
    "clustering",
    "define",
    "number",
    "centers",
    "see",
    "third",
    "center",
    "zeroth",
    "cluster",
    "third",
    "cluster",
    "zeroth",
    "cluster",
    "close",
    "probably",
    "could",
    "one",
    "another",
    "cluster",
    "another",
    "disadvantage",
    "exactly",
    "know",
    "points",
    "arranged",
    "difficult",
    "force",
    "data",
    "cluster",
    "makes",
    "analysis",
    "little",
    "different",
    "works",
    "fine",
    "sometimes",
    "might",
    "difficult",
    "code",
    "clustering",
    "let",
    "understand",
    "exactly",
    "seems",
    "clustering",
    "fuzzy",
    "c",
    "means",
    "extension",
    "clustering",
    "popular",
    "simple",
    "clustering",
    "technique",
    "fuzzy",
    "clustering",
    "also",
    "referred",
    "soft",
    "clustering",
    "form",
    "clustering",
    "data",
    "point",
    "belong",
    "one",
    "cluster",
    "tries",
    "find",
    "heart",
    "clusters",
    "point",
    "belongs",
    "one",
    "cluster",
    "whereas",
    "fuzzy",
    "c",
    "means",
    "discovers",
    "soft",
    "clusters",
    "soft",
    "cluster",
    "point",
    "belong",
    "one",
    "cluster",
    "time",
    "certain",
    "affinity",
    "value",
    "towards",
    "4zc",
    "means",
    "assigns",
    "degree",
    "membership",
    "0",
    "1",
    "object",
    "given",
    "cluster",
    "stipulation",
    "sum",
    "membership",
    "object",
    "cluster",
    "belongs",
    "must",
    "equal",
    "1",
    "degree",
    "membership",
    "particular",
    "point",
    "pull",
    "clusters",
    "add",
    "get",
    "1",
    "one",
    "logic",
    "behind",
    "fuzzy",
    "c",
    "means",
    "affinity",
    "proportional",
    "distance",
    "point",
    "center",
    "cluster",
    "pros",
    "cons",
    "fuzzy",
    "see",
    "means",
    "first",
    "allows",
    "data",
    "point",
    "multiple",
    "cluster",
    "pro",
    "neutral",
    "representation",
    "behavior",
    "jeans",
    "jeans",
    "usually",
    "involved",
    "multiple",
    "functions",
    "good",
    "type",
    "clustering",
    "talking",
    "genes",
    "first",
    "talk",
    "cons",
    "define",
    "c",
    "number",
    "clusters",
    "k",
    "next",
    "need",
    "determine",
    "membership",
    "cutoff",
    "value",
    "also",
    "takes",
    "lot",
    "time",
    "clusters",
    "sensitive",
    "initial",
    "assignment",
    "centroid",
    "slight",
    "change",
    "deviation",
    "center",
    "going",
    "result",
    "different",
    "kind",
    "know",
    "funny",
    "kind",
    "output",
    "get",
    "fuzzy",
    "c",
    "means",
    "one",
    "major",
    "disadvantage",
    "see",
    "means",
    "clustering",
    "non",
    "deterministic",
    "algorithm",
    "give",
    "particular",
    "output",
    "let",
    "look",
    "third",
    "type",
    "clustering",
    "hierarchical",
    "clustering",
    "hierarchical",
    "clustering",
    "alternative",
    "approach",
    "builds",
    "hierarchy",
    "bottom",
    "top",
    "bottom",
    "require",
    "specify",
    "number",
    "clusters",
    "beforehand",
    "algorithm",
    "works",
    "first",
    "put",
    "data",
    "point",
    "cluster",
    "closest",
    "cluster",
    "combine",
    "one",
    "cluster",
    "repeat",
    "step",
    "till",
    "data",
    "points",
    "single",
    "cluster",
    "two",
    "types",
    "hierarchical",
    "clustering",
    "one",
    "number",
    "80",
    "plus",
    "string",
    "one",
    "division",
    "clustering",
    "commemorative",
    "clustering",
    "bills",
    "dendogram",
    "bottom",
    "level",
    "division",
    "clustering",
    "starts",
    "data",
    "points",
    "one",
    "cluster",
    "fruit",
    "cluster",
    "hierarchical",
    "clustering",
    "also",
    "sort",
    "pros",
    "cons",
    "pros",
    "know",
    "assumption",
    "particular",
    "number",
    "cluster",
    "required",
    "may",
    "correspond",
    "meaningful",
    "taxonomist",
    "whereas",
    "talk",
    "cons",
    "decision",
    "made",
    "combine",
    "two",
    "clusters",
    "undone",
    "one",
    "major",
    "disadvantage",
    "hierarchical",
    "clustering",
    "becomes",
    "slow",
    "talked",
    "large",
    "data",
    "sets",
    "nowadays",
    "think",
    "every",
    "industry",
    "using",
    "last",
    "year",
    "collecting",
    "large",
    "amounts",
    "data",
    "hierarchical",
    "clustering",
    "act",
    "best",
    "method",
    "someone",
    "might",
    "need",
    "go",
    "talk",
    "unsupervised",
    "learning",
    "k",
    "means",
    "clustering",
    "another",
    "important",
    "term",
    "people",
    "usually",
    "miss",
    "talking",
    "us",
    "running",
    "one",
    "important",
    "concept",
    "market",
    "basket",
    "analysis",
    "one",
    "key",
    "techniques",
    "used",
    "large",
    "retailers",
    "uncover",
    "association",
    "items",
    "works",
    "looking",
    "combination",
    "items",
    "occur",
    "together",
    "frequently",
    "transactions",
    "put",
    "way",
    "allows",
    "retailers",
    "identify",
    "relationships",
    "items",
    "people",
    "example",
    "people",
    "buy",
    "bread",
    "also",
    "tend",
    "buy",
    "butter",
    "marketing",
    "team",
    "retail",
    "stores",
    "target",
    "customers",
    "buy",
    "bread",
    "butter",
    "provide",
    "offer",
    "buy",
    "third",
    "item",
    "like",
    "egg",
    "customer",
    "buys",
    "bread",
    "butter",
    "sees",
    "discount",
    "offer",
    "x",
    "encouraged",
    "spend",
    "money",
    "buy",
    "eggs",
    "market",
    "basket",
    "analysis",
    "find",
    "association",
    "two",
    "items",
    "make",
    "predictions",
    "customers",
    "buy",
    "two",
    "cartoons",
    "association",
    "rule",
    "mining",
    "ebrary",
    "algorithms",
    "let",
    "discuss",
    "algorithm",
    "example",
    "first",
    "look",
    "association",
    "rule",
    "mining",
    "technique",
    "shows",
    "items",
    "associated",
    "example",
    "customers",
    "purchased",
    "spread",
    "60",
    "percent",
    "likelihood",
    "also",
    "purchasing",
    "jam",
    "customers",
    "purchase",
    "laptop",
    "likely",
    "purchase",
    "laptop",
    "bags",
    "take",
    "example",
    "association",
    "rule",
    "look",
    "example",
    "arrow",
    "means",
    "person",
    "buys",
    "atom",
    "also",
    "buy",
    "atom",
    "three",
    "common",
    "ways",
    "measure",
    "particular",
    "association",
    "find",
    "rules",
    "basis",
    "statistics",
    "right",
    "use",
    "support",
    "confidence",
    "lift",
    "three",
    "common",
    "ways",
    "measures",
    "look",
    "association",
    "rule",
    "mining",
    "know",
    "exactly",
    "good",
    "rule",
    "first",
    "support",
    "support",
    "gifts",
    "fraction",
    "contains",
    "item",
    "basically",
    "frequency",
    "item",
    "whole",
    "item",
    "set",
    "confidence",
    "gifts",
    "often",
    "item",
    "b",
    "occurred",
    "together",
    "given",
    "number",
    "item",
    "given",
    "number",
    "times",
    "occur",
    "frequency",
    "comma",
    "b",
    "divided",
    "frequency",
    "left",
    "indicates",
    "strength",
    "rule",
    "random",
    "close",
    "look",
    "denominator",
    "lift",
    "formula",
    "support",
    "support",
    "major",
    "thing",
    "noted",
    "support",
    "b",
    "independent",
    "value",
    "lift",
    "denominator",
    "value",
    "lift",
    "means",
    "items",
    "independently",
    "selling",
    "together",
    "turn",
    "decrease",
    "value",
    "lift",
    "happens",
    "suppose",
    "value",
    "lift",
    "implies",
    "rule",
    "get",
    "implies",
    "rule",
    "strong",
    "used",
    "later",
    "purposes",
    "case",
    "support",
    "support",
    "p",
    "value",
    "denominator",
    "lift",
    "low",
    "turn",
    "means",
    "relationship",
    "items",
    "let",
    "take",
    "example",
    "association",
    "rule",
    "mining",
    "understand",
    "exactly",
    "works",
    "let",
    "suppose",
    "set",
    "items",
    "b",
    "c",
    "e",
    "set",
    "transactions",
    "t1",
    "t2",
    "t3",
    "t4",
    "t5",
    "need",
    "create",
    "sort",
    "rules",
    "example",
    "see",
    "means",
    "person",
    "buys",
    "buys",
    "person",
    "see",
    "buys",
    "person",
    "buys",
    "fourth",
    "one",
    "person",
    "b",
    "c",
    "hill",
    "turn",
    "need",
    "calculate",
    "support",
    "confidence",
    "lift",
    "rules",
    "talked",
    "priority",
    "algorithm",
    "priori",
    "algorithm",
    "association",
    "rule",
    "mining",
    "go",
    "hand",
    "hand",
    "predator",
    "algorithm",
    "uses",
    "frequent",
    "itemsets",
    "generate",
    "association",
    "rules",
    "based",
    "concept",
    "subset",
    "frequent",
    "itemsets",
    "must",
    "also",
    "frequent",
    "isom",
    "set",
    "let",
    "understand",
    "frequent",
    "item",
    "set",
    "work",
    "together",
    "take",
    "following",
    "transactions",
    "items",
    "transaction",
    "1",
    "2",
    "5",
    "items",
    "1",
    "3",
    "4",
    "2",
    "3",
    "5",
    "1",
    "2",
    "3",
    "5",
    "2",
    "5",
    "1",
    "3",
    "5",
    "another",
    "important",
    "thing",
    "support",
    "forgot",
    "mention",
    "talking",
    "association",
    "rule",
    "mining",
    "minimum",
    "support",
    "count",
    "need",
    "first",
    "step",
    "build",
    "list",
    "items",
    "size",
    "1",
    "using",
    "transaction",
    "data",
    "set",
    "use",
    "minimum",
    "support",
    "count",
    "let",
    "see",
    "create",
    "table",
    "see",
    "close",
    "look",
    "table",
    "c",
    "1",
    "items",
    "one",
    "support",
    "three",
    "appears",
    "transaction",
    "one",
    "three",
    "five",
    "similarly",
    "look",
    "item",
    "set",
    "single",
    "item",
    "support",
    "appears",
    "1",
    "2",
    "3",
    "5",
    "look",
    "item",
    "set",
    "appears",
    "transaction",
    "support",
    "value",
    "1",
    "item",
    "set",
    "support",
    "value",
    "less",
    "minimum",
    "support",
    "value",
    "eliminated",
    "final",
    "table",
    "table",
    "f1",
    "one",
    "two",
    "three",
    "five",
    "contain",
    "going",
    "create",
    "item",
    "list",
    "size",
    "2",
    "combination",
    "item",
    "sets",
    "f1",
    "used",
    "iteration",
    "left",
    "behind",
    "1",
    "2",
    "3",
    "possible",
    "item",
    "sets",
    "1",
    "2",
    "1",
    "3",
    "1",
    "5",
    "2",
    "3",
    "2",
    "5",
    "3",
    "5",
    "calculate",
    "support",
    "case",
    "closer",
    "look",
    "table",
    "c",
    "2",
    "see",
    "items",
    "support",
    "value",
    "1",
    "eliminated",
    "final",
    "table",
    "f",
    "2",
    "contain",
    "1",
    "comma",
    "2",
    "similarly",
    "create",
    "item",
    "sets",
    "size",
    "3",
    "calculate",
    "support",
    "values",
    "calculating",
    "support",
    "let",
    "perform",
    "puring",
    "data",
    "set",
    "appearing",
    "combinations",
    "made",
    "divide",
    "table",
    "c",
    "3",
    "items",
    "check",
    "another",
    "subset",
    "whose",
    "support",
    "less",
    "minimum",
    "support",
    "value",
    "prairie",
    "algorithm",
    "item",
    "sets",
    "one",
    "two",
    "three",
    "see",
    "one",
    "two",
    "one",
    "five",
    "one",
    "build",
    "cardboard",
    "item",
    "sets",
    "left",
    "1",
    "3",
    "5",
    "2",
    "3",
    "one",
    "three",
    "five",
    "three",
    "subsets",
    "one",
    "five",
    "one",
    "three",
    "three",
    "five",
    "present",
    "table",
    "f2",
    "two",
    "three",
    "five",
    "also",
    "present",
    "f",
    "2",
    "2",
    "move",
    "1",
    "comma",
    "2",
    "table",
    "c",
    "3",
    "create",
    "table",
    "f3",
    "using",
    "items",
    "c3",
    "create",
    "atoms",
    "find",
    "item",
    "set",
    "1",
    "2",
    "3",
    "5",
    "support",
    "value",
    "1",
    "less",
    "minimum",
    "support",
    "value",
    "going",
    "stop",
    "going",
    "return",
    "previous",
    "item",
    "set",
    "table",
    "c",
    "3",
    "final",
    "table",
    "well",
    "three",
    "one",
    "three",
    "five",
    "support",
    "value",
    "2",
    "2",
    "3",
    "5",
    "support",
    "value",
    "2",
    "gon",
    "na",
    "generate",
    "subsets",
    "frequent",
    "itemsets",
    "let",
    "assume",
    "minimum",
    "confidence",
    "value",
    "60",
    "every",
    "subset",
    "output",
    "rule",
    "gives",
    "i2s",
    "recommends",
    "ns",
    "support",
    "support",
    "greater",
    "equal",
    "equal",
    "minimum",
    "confidence",
    "value",
    "proceed",
    "keep",
    "mind",
    "used",
    "left",
    "till",
    "working",
    "support",
    "confidence",
    "applying",
    "rules",
    "item",
    "sets",
    "f3",
    "get",
    "rule",
    "1",
    "1",
    "comma",
    "3",
    "gives",
    "1",
    "3",
    "5",
    "means",
    "buy",
    "one",
    "three",
    "66",
    "chance",
    "buy",
    "item",
    "5",
    "also",
    "similarly",
    "rule",
    "1",
    "comma",
    "5",
    "means",
    "buy",
    "one",
    "five",
    "hundred",
    "percent",
    "chance",
    "buy",
    "three",
    "also",
    "similarly",
    "look",
    "rule",
    "5",
    "6",
    "confidence",
    "value",
    "less",
    "60",
    "percent",
    "assumed",
    "confidence",
    "value",
    "going",
    "reject",
    "files",
    "important",
    "thing",
    "note",
    "closer",
    "look",
    "rule",
    "5",
    "root",
    "3",
    "see",
    "one",
    "five",
    "three",
    "one",
    "five",
    "three",
    "three",
    "point",
    "five",
    "confusing",
    "one",
    "thing",
    "keep",
    "mine",
    "order",
    "item",
    "sets",
    "also",
    "important",
    "help",
    "us",
    "allow",
    "create",
    "good",
    "rules",
    "avoid",
    "kind",
    "confusion",
    "let",
    "learn",
    "association",
    "rule",
    "used",
    "market",
    "basket",
    "analysis",
    "problems",
    "using",
    "online",
    "transactions",
    "data",
    "retail",
    "store",
    "generating",
    "association",
    "rules",
    "first",
    "need",
    "import",
    "pandas",
    "msd",
    "ml",
    "libraries",
    "imported",
    "read",
    "data",
    "first",
    "going",
    "read",
    "data",
    "going",
    "lx",
    "e",
    "dot",
    "frequent",
    "patterns",
    "going",
    "improve",
    "priori",
    "association",
    "rules",
    "see",
    "head",
    "data",
    "see",
    "invoice",
    "number",
    "stock",
    "code",
    "description",
    "quantity",
    "invoice",
    "dt8",
    "unit",
    "price",
    "customer",
    "id",
    "country",
    "next",
    "step",
    "data",
    "cleanup",
    "includes",
    "removing",
    "descriptions",
    "given",
    "going",
    "drop",
    "rules",
    "invoice",
    "numbers",
    "every",
    "move",
    "crate",
    "transactions",
    "hey",
    "going",
    "remove",
    "invoice",
    "number",
    "string",
    "tight",
    "ainst",
    "epstein",
    "number",
    "going",
    "remove",
    "credits",
    "remove",
    "kind",
    "spaces",
    "descriptions",
    "see",
    "like",
    "five",
    "hundred",
    "thousand",
    "rows",
    "eight",
    "columns",
    "next",
    "one",
    "going",
    "cleanup",
    "need",
    "consolidate",
    "items",
    "one",
    "transaction",
    "per",
    "row",
    "product",
    "sake",
    "keeping",
    "data",
    "set",
    "small",
    "going",
    "look",
    "sales",
    "france",
    "going",
    "use",
    "france",
    "group",
    "invoice",
    "number",
    "description",
    "quantity",
    "sum",
    "see",
    "leaves",
    "us",
    "three",
    "ninety",
    "two",
    "rows",
    "one",
    "thousand",
    "five",
    "hundred",
    "sixty",
    "three",
    "columns",
    "lot",
    "zeros",
    "data",
    "also",
    "need",
    "make",
    "sure",
    "positive",
    "values",
    "converted",
    "1",
    "anything",
    "less",
    "0",
    "set",
    "0",
    "going",
    "use",
    "code",
    "defining",
    "code",
    "units",
    "x",
    "less",
    "0",
    "return",
    "0",
    "x",
    "greater",
    "1",
    "returned",
    "one",
    "going",
    "map",
    "apply",
    "whole",
    "data",
    "set",
    "structured",
    "data",
    "properly",
    "next",
    "step",
    "generate",
    "frequent",
    "item",
    "set",
    "support",
    "least",
    "seven",
    "percent",
    "number",
    "chosen",
    "get",
    "close",
    "enough",
    "going",
    "generate",
    "rules",
    "corresponding",
    "support",
    "confidence",
    "lift",
    "given",
    "minimum",
    "support",
    "metric",
    "left",
    "frequent",
    "island",
    "set",
    "threshold",
    "1",
    "following",
    "rules",
    "rules",
    "high",
    "lift",
    "value",
    "means",
    "occurs",
    "frequently",
    "would",
    "expected",
    "given",
    "number",
    "transaction",
    "product",
    "combinations",
    "places",
    "confidence",
    "high",
    "well",
    "observations",
    "get",
    "filter",
    "data",
    "frame",
    "using",
    "standard",
    "pandas",
    "code",
    "large",
    "lift",
    "six",
    "high",
    "confidence",
    "output",
    "going",
    "look",
    "like",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "see",
    "h",
    "rules",
    "final",
    "rules",
    "given",
    "association",
    "rule",
    "mining",
    "industries",
    "talked",
    "largely",
    "retailers",
    "tend",
    "know",
    "products",
    "used",
    "exactly",
    "rearrange",
    "provide",
    "offers",
    "product",
    "people",
    "spend",
    "money",
    "time",
    "shop",
    "association",
    "rule",
    "mining",
    "guys",
    "unsupervised",
    "learning",
    "hope",
    "got",
    "know",
    "different",
    "formulas",
    "unsupervised",
    "learning",
    "works",
    "know",
    "provide",
    "label",
    "data",
    "create",
    "rules",
    "knowing",
    "data",
    "clusterings",
    "different",
    "types",
    "clusterings",
    "came",
    "simi",
    "hierarchical",
    "clustering",
    "reinforcement",
    "learning",
    "part",
    "machine",
    "learning",
    "agent",
    "put",
    "environment",
    "learns",
    "behave",
    "environment",
    "performing",
    "certain",
    "actions",
    "okay",
    "basically",
    "performs",
    "actions",
    "either",
    "gets",
    "rewards",
    "actions",
    "gets",
    "punishment",
    "observing",
    "reward",
    "gets",
    "actions",
    "reinforcement",
    "learning",
    "taking",
    "appropriate",
    "action",
    "order",
    "maximize",
    "reward",
    "particular",
    "situation",
    "guys",
    "supervised",
    "learning",
    "training",
    "data",
    "comprises",
    "input",
    "expected",
    "output",
    "model",
    "trained",
    "expected",
    "output",
    "comes",
    "reinforcement",
    "learning",
    "expected",
    "output",
    "reinforcement",
    "agent",
    "decides",
    "actions",
    "take",
    "order",
    "perform",
    "given",
    "task",
    "absence",
    "training",
    "data",
    "set",
    "bound",
    "learn",
    "experience",
    "right",
    "reinforcement",
    "learning",
    "agent",
    "put",
    "unknown",
    "environment",
    "going",
    "use",
    "hit",
    "trial",
    "method",
    "order",
    "figure",
    "environment",
    "come",
    "outcome",
    "okay",
    "let",
    "look",
    "reinforcement",
    "learning",
    "within",
    "analogy",
    "consider",
    "scenario",
    "baby",
    "learning",
    "walk",
    "scenario",
    "go",
    "two",
    "ways",
    "first",
    "case",
    "baby",
    "starts",
    "walking",
    "makes",
    "candy",
    "candy",
    "basically",
    "reward",
    "going",
    "get",
    "since",
    "candy",
    "end",
    "goal",
    "baby",
    "happy",
    "positive",
    "okay",
    "baby",
    "happy",
    "gets",
    "rewarded",
    "set",
    "candies",
    "another",
    "way",
    "could",
    "go",
    "baby",
    "starts",
    "walking",
    "falls",
    "due",
    "hurdle",
    "baby",
    "gets",
    "hurt",
    "get",
    "candy",
    "obviously",
    "baby",
    "sad",
    "negative",
    "reward",
    "okay",
    "say",
    "setback",
    "like",
    "humans",
    "learn",
    "mistakes",
    "trial",
    "error",
    "learning",
    "also",
    "similar",
    "okay",
    "agent",
    "basically",
    "baby",
    "reward",
    "candy",
    "okay",
    "many",
    "hurdles",
    "agent",
    "supposed",
    "find",
    "best",
    "possible",
    "path",
    "read",
    "reward",
    "guys",
    "hope",
    "clear",
    "reinforcement",
    "learning",
    "let",
    "look",
    "reinforcement",
    "learning",
    "process",
    "generally",
    "reinforcement",
    "learning",
    "system",
    "two",
    "main",
    "components",
    "right",
    "first",
    "agent",
    "second",
    "one",
    "environment",
    "previous",
    "case",
    "saw",
    "agent",
    "baby",
    "b",
    "environment",
    "living",
    "room",
    "baby",
    "crawling",
    "okay",
    "environment",
    "setting",
    "agent",
    "acting",
    "agent",
    "represents",
    "reinforcement",
    "learning",
    "algorithm",
    "guys",
    "reinforcement",
    "learning",
    "process",
    "starts",
    "environment",
    "sends",
    "state",
    "agent",
    "agent",
    "take",
    "actions",
    "based",
    "observations",
    "turn",
    "environment",
    "send",
    "next",
    "state",
    "respective",
    "reward",
    "back",
    "agent",
    "agent",
    "update",
    "knowledge",
    "reward",
    "returned",
    "meant",
    "uses",
    "evaluate",
    "previous",
    "action",
    "guys",
    "loop",
    "keeps",
    "continuing",
    "environment",
    "sends",
    "terminal",
    "state",
    "means",
    "agent",
    "accomplished",
    "tasks",
    "finally",
    "gets",
    "reward",
    "okay",
    "exactly",
    "depicted",
    "scenario",
    "agent",
    "keeps",
    "climbing",
    "ladders",
    "reaches",
    "reward",
    "understand",
    "better",
    "let",
    "suppose",
    "agent",
    "learning",
    "play",
    "okay",
    "let",
    "break",
    "initially",
    "rl",
    "agent",
    "player",
    "player",
    "1",
    "let",
    "say",
    "player",
    "1",
    "trying",
    "learn",
    "play",
    "game",
    "okay",
    "collects",
    "state",
    "environment",
    "okay",
    "could",
    "first",
    "state",
    "based",
    "state",
    "agent",
    "take",
    "action",
    "okay",
    "action",
    "anything",
    "causes",
    "result",
    "player",
    "moves",
    "left",
    "right",
    "also",
    "considered",
    "action",
    "okay",
    "initially",
    "action",
    "going",
    "random",
    "obviously",
    "first",
    "time",
    "pick",
    "going",
    "master",
    "going",
    "try",
    "different",
    "actions",
    "going",
    "random",
    "action",
    "beginning",
    "environment",
    "going",
    "give",
    "new",
    "state",
    "clearing",
    "environment",
    "going",
    "give",
    "new",
    "state",
    "agent",
    "player",
    "maybe",
    "across",
    "stage",
    "1",
    "stage",
    "player",
    "get",
    "reward",
    "one",
    "environment",
    "cleared",
    "stage",
    "reward",
    "anything",
    "additional",
    "points",
    "coins",
    "anything",
    "like",
    "okay",
    "basically",
    "loop",
    "keeps",
    "going",
    "player",
    "dead",
    "reaches",
    "destination",
    "okay",
    "continuously",
    "outputs",
    "sequence",
    "states",
    "actions",
    "rewards",
    "guys",
    "small",
    "example",
    "show",
    "reinforcement",
    "learning",
    "process",
    "works",
    "start",
    "initial",
    "state",
    "player",
    "clothes",
    "state",
    "gets",
    "reward",
    "environment",
    "give",
    "another",
    "stage",
    "player",
    "clears",
    "state",
    "going",
    "get",
    "another",
    "reward",
    "going",
    "keep",
    "happening",
    "player",
    "reaches",
    "destination",
    "right",
    "guys",
    "hope",
    "clear",
    "let",
    "move",
    "look",
    "reinforcement",
    "learning",
    "definition",
    "concepts",
    "aware",
    "studying",
    "reinforcement",
    "learning",
    "let",
    "look",
    "definitions",
    "first",
    "agent",
    "agent",
    "basically",
    "reinforcement",
    "learning",
    "algorithm",
    "learns",
    "trial",
    "error",
    "okay",
    "agent",
    "takes",
    "actions",
    "like",
    "example",
    "soldier",
    "navigating",
    "game",
    "also",
    "action",
    "okay",
    "moves",
    "left",
    "right",
    "shoots",
    "somebody",
    "also",
    "action",
    "okay",
    "agent",
    "responsible",
    "taking",
    "actions",
    "environment",
    "environment",
    "whole",
    "game",
    "okay",
    "basically",
    "world",
    "agent",
    "moves",
    "environment",
    "takes",
    "agents",
    "current",
    "state",
    "action",
    "input",
    "returns",
    "agency",
    "reward",
    "next",
    "state",
    "output",
    "alright",
    "next",
    "action",
    "possible",
    "steps",
    "agent",
    "take",
    "called",
    "actions",
    "like",
    "said",
    "moving",
    "right",
    "left",
    "shooting",
    "alright",
    "state",
    "state",
    "basically",
    "current",
    "condition",
    "returned",
    "environment",
    "double",
    "state",
    "state",
    "1",
    "interested",
    "represents",
    "current",
    "condition",
    "right",
    "next",
    "reward",
    "reward",
    "basically",
    "instant",
    "return",
    "environment",
    "appraise",
    "last",
    "action",
    "okay",
    "anything",
    "like",
    "coins",
    "additional",
    "points",
    "basically",
    "reward",
    "given",
    "agent",
    "clears",
    "specific",
    "stages",
    "next",
    "policy",
    "policy",
    "basically",
    "strategy",
    "agent",
    "uses",
    "find",
    "next",
    "action",
    "based",
    "current",
    "state",
    "policy",
    "strategy",
    "approach",
    "game",
    "value",
    "expected",
    "return",
    "discount",
    "value",
    "action",
    "value",
    "little",
    "bit",
    "confusing",
    "right",
    "move",
    "understand",
    "talking",
    "okay",
    "value",
    "basically",
    "return",
    "get",
    "discount",
    "okay",
    "discount",
    "explain",
    "slides",
    "action",
    "value",
    "action",
    "value",
    "also",
    "known",
    "q",
    "value",
    "okay",
    "similar",
    "except",
    "takes",
    "extra",
    "parameter",
    "current",
    "action",
    "basically",
    "find",
    "q",
    "value",
    "depending",
    "particular",
    "action",
    "took",
    "right",
    "guys",
    "get",
    "confused",
    "value",
    "action",
    "value",
    "look",
    "examples",
    "slides",
    "understand",
    "better",
    "okay",
    "guys",
    "make",
    "sure",
    "familiar",
    "terms",
    "seeing",
    "lot",
    "terms",
    "slides",
    "right",
    "move",
    "like",
    "discuss",
    "concepts",
    "okay",
    "first",
    "discuss",
    "reward",
    "maximization",
    "already",
    "realize",
    "basic",
    "aim",
    "rl",
    "agent",
    "maximize",
    "reward",
    "happen",
    "let",
    "try",
    "understand",
    "depth",
    "agent",
    "must",
    "trained",
    "way",
    "takes",
    "best",
    "action",
    "reward",
    "maximum",
    "end",
    "goal",
    "reinforcement",
    "learning",
    "maximize",
    "reward",
    "based",
    "set",
    "actions",
    "let",
    "explain",
    "small",
    "game",
    "figure",
    "see",
    "forks",
    "meat",
    "tiger",
    "odd",
    "agent",
    "basically",
    "fox",
    "end",
    "goal",
    "eat",
    "maximum",
    "amount",
    "meat",
    "eaten",
    "tiger",
    "since",
    "fox",
    "clever",
    "fellow",
    "eats",
    "meat",
    "closer",
    "rather",
    "meat",
    "closer",
    "tiger",
    "closer",
    "tiger",
    "higher",
    "chances",
    "getting",
    "killed",
    "rewards",
    "near",
    "tiger",
    "even",
    "bigger",
    "meat",
    "chunks",
    "discounted",
    "exactly",
    "discounting",
    "means",
    "agent",
    "going",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "risk",
    "right",
    "even",
    "though",
    "meat",
    "chunks",
    "might",
    "larger",
    "want",
    "take",
    "chances",
    "getting",
    "killed",
    "okay",
    "called",
    "discounting",
    "okay",
    "discount",
    "improvised",
    "eat",
    "meat",
    "closer",
    "instead",
    "taking",
    "risks",
    "eating",
    "meat",
    "closer",
    "opponent",
    "right",
    "discounting",
    "reward",
    "works",
    "based",
    "value",
    "called",
    "gamma",
    "discussing",
    "gamma",
    "slides",
    "short",
    "value",
    "gamma",
    "0",
    "okay",
    "follow",
    "gamma",
    "larger",
    "discount",
    "value",
    "okay",
    "gamma",
    "value",
    "lesser",
    "means",
    "agent",
    "going",
    "explore",
    "going",
    "try",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "okay",
    "gamma",
    "value",
    "closer",
    "1",
    "means",
    "agent",
    "actually",
    "going",
    "explore",
    "going",
    "dry",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "right",
    "explaining",
    "depth",
    "slides",
    "worry",
    "got",
    "clear",
    "concept",
    "yet",
    "understand",
    "reward",
    "maximized",
    "ation",
    "important",
    "step",
    "comes",
    "reinforcement",
    "learning",
    "agent",
    "collect",
    "maximum",
    "rewards",
    "end",
    "game",
    "right",
    "let",
    "look",
    "another",
    "concept",
    "called",
    "exploration",
    "exploitation",
    "exploration",
    "like",
    "name",
    "suggests",
    "exploring",
    "capturing",
    "information",
    "environment",
    "hand",
    "exploitation",
    "using",
    "already",
    "known",
    "exploited",
    "information",
    "hide",
    "rewards",
    "guys",
    "consider",
    "fox",
    "tiger",
    "example",
    "discussed",
    "foxy",
    "meat",
    "chunks",
    "close",
    "eat",
    "meat",
    "chunks",
    "closer",
    "tiger",
    "okay",
    "even",
    "though",
    "might",
    "give",
    "awards",
    "eat",
    "fox",
    "focuses",
    "closest",
    "rewards",
    "never",
    "reach",
    "big",
    "chunks",
    "meat",
    "okay",
    "exploitation",
    "going",
    "use",
    "currently",
    "known",
    "information",
    "going",
    "try",
    "get",
    "rewards",
    "based",
    "information",
    "fox",
    "decides",
    "explore",
    "bit",
    "find",
    "bigger",
    "award",
    "big",
    "chunks",
    "meat",
    "exactly",
    "exploration",
    "agent",
    "going",
    "stick",
    "one",
    "corner",
    "instead",
    "going",
    "explore",
    "entire",
    "environment",
    "try",
    "collect",
    "bigger",
    "rewards",
    "right",
    "guys",
    "hope",
    "clear",
    "exploration",
    "exploitation",
    "let",
    "look",
    "markers",
    "decision",
    "process",
    "guys",
    "basically",
    "mathematical",
    "approach",
    "mapping",
    "solution",
    "reinforcement",
    "learning",
    "way",
    "purpose",
    "reinforcement",
    "learning",
    "solve",
    "markov",
    "decision",
    "process",
    "okay",
    "parameters",
    "used",
    "get",
    "solution",
    "parameters",
    "include",
    "set",
    "actions",
    "set",
    "states",
    "rewards",
    "policy",
    "taking",
    "approach",
    "problem",
    "value",
    "get",
    "okay",
    "sum",
    "agent",
    "must",
    "take",
    "action",
    "transition",
    "start",
    "state",
    "end",
    "state",
    "agent",
    "receive",
    "reward",
    "action",
    "takes",
    "guys",
    "series",
    "actions",
    "taken",
    "agent",
    "define",
    "policy",
    "defines",
    "approach",
    "rewards",
    "collected",
    "define",
    "value",
    "main",
    "goal",
    "maximize",
    "rewards",
    "choosing",
    "optimum",
    "policy",
    "right",
    "let",
    "try",
    "understand",
    "help",
    "shortest",
    "path",
    "problem",
    "sure",
    "lot",
    "might",
    "gone",
    "problem",
    "college",
    "guys",
    "look",
    "graph",
    "aim",
    "find",
    "shortest",
    "path",
    "minimum",
    "possible",
    "cost",
    "value",
    "see",
    "edges",
    "basically",
    "denotes",
    "cost",
    "want",
    "go",
    "see",
    "gon",
    "na",
    "cost",
    "15",
    "points",
    "okay",
    "let",
    "look",
    "done",
    "move",
    "look",
    "problem",
    "problem",
    "set",
    "states",
    "denoted",
    "nodes",
    "abcd",
    "action",
    "traverse",
    "one",
    "node",
    "going",
    "b",
    "action",
    "similarly",
    "see",
    "action",
    "okay",
    "reward",
    "basically",
    "cost",
    "represented",
    "edge",
    "right",
    "policy",
    "basically",
    "path",
    "choose",
    "reach",
    "destination",
    "let",
    "say",
    "choose",
    "seed",
    "okay",
    "one",
    "policy",
    "order",
    "get",
    "choosing",
    "cd",
    "policy",
    "okay",
    "basically",
    "approaching",
    "problem",
    "guys",
    "start",
    "node",
    "take",
    "baby",
    "steps",
    "destination",
    "initially",
    "clueless",
    "take",
    "next",
    "possible",
    "node",
    "visible",
    "guys",
    "smart",
    "enough",
    "going",
    "choose",
    "see",
    "instead",
    "abcd",
    "abd",
    "right",
    "nodes",
    "see",
    "want",
    "drive",
    "string",
    "note",
    "must",
    "choose",
    "weisbarth",
    "right",
    "calculate",
    "path",
    "highest",
    "cost",
    "path",
    "give",
    "maximum",
    "rewards",
    "guys",
    "simple",
    "problem",
    "trying",
    "calculate",
    "shortest",
    "path",
    "traversing",
    "nodes",
    "traverse",
    "cd",
    "gives",
    "maximum",
    "reward",
    "okay",
    "gives",
    "65",
    "policy",
    "would",
    "give",
    "okay",
    "go",
    "abd",
    "would",
    "40",
    "compare",
    "cd",
    "gives",
    "reward",
    "obviously",
    "going",
    "go",
    "cb",
    "okay",
    "guys",
    "simple",
    "problem",
    "order",
    "understand",
    "markov",
    "decision",
    "process",
    "works",
    "right",
    "guys",
    "want",
    "ask",
    "question",
    "think",
    "hear",
    "perform",
    "exploration",
    "perform",
    "exploitation",
    "policy",
    "example",
    "exploitation",
    "explore",
    "nodes",
    "okay",
    "selected",
    "three",
    "notes",
    "travel",
    "called",
    "exploitation",
    "must",
    "always",
    "explore",
    "different",
    "notes",
    "find",
    "optimal",
    "policy",
    "case",
    "obviously",
    "cd",
    "highest",
    "reward",
    "going",
    "cd",
    "generally",
    "simple",
    "lot",
    "nodes",
    "hundreds",
    "notes",
    "traverse",
    "like",
    "50",
    "60",
    "policies",
    "okay",
    "50",
    "60",
    "different",
    "policies",
    "make",
    "sure",
    "explore",
    "policies",
    "decide",
    "optimum",
    "policy",
    "give",
    "maximum",
    "reward",
    "robot",
    "environment",
    "place",
    "put",
    "use",
    "remember",
    "reward",
    "agent",
    "example",
    "automobile",
    "factory",
    "robot",
    "used",
    "move",
    "materials",
    "one",
    "place",
    "another",
    "task",
    "discussed",
    "property",
    "common",
    "tasks",
    "involve",
    "environment",
    "expect",
    "agent",
    "learn",
    "environment",
    "traditional",
    "machine",
    "learning",
    "phase",
    "hence",
    "need",
    "reinforcement",
    "learning",
    "good",
    "established",
    "overview",
    "problem",
    "using",
    "q",
    "learning",
    "reinforcement",
    "learning",
    "helps",
    "define",
    "main",
    "components",
    "reinforcement",
    "learning",
    "solution",
    "agent",
    "environment",
    "action",
    "rewards",
    "states",
    "let",
    "suppose",
    "build",
    "autonomous",
    "robots",
    "automobile",
    "building",
    "factory",
    "robots",
    "help",
    "factory",
    "personal",
    "conveying",
    "necessary",
    "parts",
    "would",
    "need",
    "order",
    "pull",
    "car",
    "different",
    "parts",
    "located",
    "nine",
    "different",
    "positions",
    "within",
    "factory",
    "warehouse",
    "car",
    "part",
    "include",
    "chassis",
    "wheels",
    "dashboard",
    "engine",
    "factory",
    "workers",
    "prioritized",
    "location",
    "contains",
    "body",
    "chassis",
    "topmost",
    "provided",
    "priorities",
    "locations",
    "well",
    "look",
    "moment",
    "locations",
    "within",
    "factory",
    "look",
    "somewhat",
    "like",
    "see",
    "l1",
    "l2",
    "l3",
    "stations",
    "one",
    "thing",
    "might",
    "notice",
    "little",
    "obstacle",
    "prison",
    "locations",
    "l6",
    "top",
    "priority",
    "location",
    "contains",
    "chassis",
    "preparing",
    "car",
    "bodies",
    "task",
    "enable",
    "robots",
    "find",
    "shortest",
    "route",
    "given",
    "location",
    "another",
    "location",
    "agents",
    "case",
    "robots",
    "environment",
    "automobile",
    "factory",
    "warehouse",
    "let",
    "talk",
    "state",
    "states",
    "location",
    "particular",
    "robot",
    "particular",
    "instance",
    "time",
    "denote",
    "states",
    "machines",
    "understand",
    "numbers",
    "rather",
    "let",
    "us",
    "let",
    "map",
    "location",
    "codes",
    "number",
    "see",
    "mapped",
    "location",
    "l",
    "1",
    "0",
    "l",
    "2",
    "1",
    "l8",
    "state",
    "7",
    "l",
    "line",
    "state",
    "next",
    "going",
    "talk",
    "actions",
    "example",
    "action",
    "direct",
    "location",
    "robot",
    "go",
    "particular",
    "location",
    "right",
    "considering",
    "tel",
    "location",
    "direct",
    "locations",
    "move",
    "rl5",
    "l1",
    "l3",
    "figure",
    "may",
    "come",
    "handy",
    "visualize",
    "might",
    "already",
    "guessed",
    "set",
    "actions",
    "nothing",
    "set",
    "possible",
    "states",
    "robot",
    "location",
    "set",
    "actions",
    "robot",
    "take",
    "different",
    "example",
    "set",
    "actions",
    "change",
    "robot",
    "l1",
    "rather",
    "l2",
    "robot",
    "l1",
    "go",
    "l",
    "4",
    "l",
    "2",
    "directly",
    "done",
    "states",
    "actions",
    "let",
    "talk",
    "rewards",
    "states",
    "basically",
    "zero",
    "one",
    "two",
    "three",
    "four",
    "actions",
    "also",
    "0",
    "1",
    "2",
    "3",
    "4",
    "rewards",
    "given",
    "robot",
    "location",
    "state",
    "directly",
    "reachable",
    "particular",
    "location",
    "let",
    "take",
    "example",
    "suppose",
    "l",
    "line",
    "directly",
    "reachable",
    "l8",
    "right",
    "robot",
    "goes",
    "la",
    "align",
    "vice",
    "versa",
    "rewarded",
    "one",
    "look",
    "shin",
    "directly",
    "reachable",
    "particular",
    "equation",
    "give",
    "reward",
    "reward",
    "0",
    "reward",
    "number",
    "nothing",
    "else",
    "enables",
    "robots",
    "make",
    "sense",
    "movements",
    "helping",
    "deciding",
    "locations",
    "directly",
    "reachable",
    "construct",
    "reward",
    "table",
    "contains",
    "required",
    "values",
    "mapping",
    "possible",
    "states",
    "see",
    "table",
    "positions",
    "marked",
    "green",
    "positive",
    "reward",
    "see",
    "possible",
    "rewards",
    "robot",
    "get",
    "moving",
    "different",
    "states",
    "comes",
    "interesting",
    "decision",
    "remember",
    "factory",
    "administrator",
    "prioritized",
    "l6",
    "topmost",
    "incorporate",
    "fact",
    "table",
    "done",
    "associating",
    "topmost",
    "priority",
    "location",
    "high",
    "reward",
    "usual",
    "ones",
    "let",
    "put",
    "cell",
    "l",
    "6",
    "comma",
    "6",
    "table",
    "rewards",
    "higher",
    "reward",
    "topmost",
    "location",
    "looks",
    "something",
    "like",
    "formally",
    "defined",
    "vital",
    "components",
    "solution",
    "aiming",
    "problem",
    "discussed",
    "shift",
    "gears",
    "bit",
    "study",
    "fundamental",
    "concepts",
    "prevail",
    "world",
    "reinforcement",
    "learning",
    "first",
    "start",
    "bellman",
    "equation",
    "consider",
    "following",
    "square",
    "rooms",
    "analogous",
    "actual",
    "environment",
    "aunt",
    "original",
    "problem",
    "without",
    "barriers",
    "suppose",
    "robot",
    "needs",
    "go",
    "room",
    "marked",
    "green",
    "promise",
    "current",
    "position",
    "using",
    "specified",
    "direction",
    "enable",
    "robot",
    "programmatically",
    "one",
    "idea",
    "would",
    "introduced",
    "kind",
    "footprint",
    "robot",
    "able",
    "follow",
    "constant",
    "value",
    "specified",
    "rooms",
    "come",
    "along",
    "robots",
    "way",
    "follows",
    "direction",
    "specified",
    "way",
    "starts",
    "able",
    "scan",
    "constant",
    "value",
    "move",
    "accordingly",
    "work",
    "direction",
    "prefix",
    "robot",
    "always",
    "starts",
    "location",
    "consider",
    "robot",
    "starts",
    "location",
    "rather",
    "previous",
    "one",
    "robot",
    "sees",
    "footprints",
    "two",
    "different",
    "directions",
    "therefore",
    "unable",
    "decide",
    "way",
    "go",
    "order",
    "get",
    "destination",
    "green",
    "room",
    "happens",
    "primarily",
    "robot",
    "weight",
    "remember",
    "directions",
    "proceed",
    "job",
    "enable",
    "robot",
    "memory",
    "bellman",
    "equation",
    "comes",
    "play",
    "see",
    "main",
    "reason",
    "bellman",
    "equation",
    "enable",
    "reward",
    "memory",
    "thing",
    "going",
    "use",
    "equation",
    "goes",
    "something",
    "like",
    "v",
    "gives",
    "maximum",
    "r",
    "comma",
    "plus",
    "gamma",
    "vs",
    "particular",
    "state",
    "rom",
    "action",
    "moving",
    "rooms",
    "state",
    "robot",
    "goes",
    "gamma",
    "discount",
    "factor",
    "get",
    "moment",
    "obviously",
    "r",
    "comma",
    "reward",
    "function",
    "takes",
    "state",
    "action",
    "outputs",
    "reward",
    "v",
    "value",
    "particular",
    "state",
    "footprint",
    "consider",
    "possible",
    "actions",
    "take",
    "one",
    "yields",
    "maximum",
    "value",
    "one",
    "constraint",
    "however",
    "regarding",
    "value",
    "footprint",
    "row",
    "marked",
    "yellow",
    "green",
    "room",
    "always",
    "value",
    "1",
    "denote",
    "one",
    "nearest",
    "room",
    "adjacent",
    "green",
    "room",
    "also",
    "ensure",
    "robot",
    "gets",
    "reward",
    "goes",
    "yellow",
    "room",
    "green",
    "room",
    "let",
    "see",
    "make",
    "sense",
    "equation",
    "let",
    "assume",
    "discount",
    "factor",
    "remember",
    "gamma",
    "discount",
    "value",
    "discount",
    "factor",
    "let",
    "take",
    "room",
    "one",
    "yellow",
    "room",
    "aztec",
    "mark",
    "room",
    "v",
    "value",
    "particular",
    "state",
    "v",
    "would",
    "something",
    "like",
    "maximum",
    "take",
    "0",
    "initial",
    "comma",
    "hey",
    "plus",
    "gamma",
    "1",
    "gives",
    "us",
    "zero",
    "point",
    "nine",
    "robot",
    "get",
    "reward",
    "going",
    "state",
    "marked",
    "yellow",
    "hence",
    "er",
    "comma",
    "0",
    "robot",
    "knows",
    "value",
    "yellow",
    "room",
    "hence",
    "v",
    "dash",
    "one",
    "following",
    "states",
    "get",
    "put",
    "equation",
    "get",
    "9",
    "reach",
    "starting",
    "point",
    "table",
    "looks",
    "value",
    "footprints",
    "computed",
    "bellman",
    "equation",
    "couple",
    "things",
    "max",
    "function",
    "robot",
    "always",
    "choose",
    "state",
    "gives",
    "maximum",
    "value",
    "state",
    "discount",
    "factor",
    "gamma",
    "notifies",
    "robot",
    "far",
    "destination",
    "typically",
    "specified",
    "developer",
    "algorithm",
    "would",
    "installed",
    "robot",
    "states",
    "also",
    "given",
    "respective",
    "values",
    "similar",
    "way",
    "see",
    "boxes",
    "adjacent",
    "green",
    "one",
    "one",
    "move",
    "away",
    "1",
    "get",
    "1",
    "0",
    "1",
    "7",
    "9",
    "finally",
    "reach",
    "robot",
    "precede",
    "way",
    "green",
    "room",
    "utilizing",
    "value",
    "footprints",
    "event",
    "dropped",
    "arbitrary",
    "room",
    "given",
    "location",
    "robot",
    "lance",
    "highlighted",
    "sky",
    "blue",
    "area",
    "still",
    "find",
    "two",
    "options",
    "choose",
    "eventually",
    "either",
    "parts",
    "good",
    "enough",
    "robot",
    "take",
    "auto",
    "v",
    "value",
    "prints",
    "one",
    "thing",
    "note",
    "bellman",
    "equation",
    "one",
    "key",
    "equations",
    "world",
    "reinforcement",
    "learning",
    "q",
    "learning",
    "think",
    "realistically",
    "surroundings",
    "always",
    "work",
    "way",
    "expect",
    "always",
    "bit",
    "stochastic",
    "city",
    "involved",
    "applies",
    "robot",
    "well",
    "sometimes",
    "might",
    "happen",
    "robots",
    "machinery",
    "got",
    "corrupted",
    "sometimes",
    "robot",
    "may",
    "come",
    "across",
    "hindrance",
    "way",
    "may",
    "known",
    "beforehand",
    "right",
    "sometimes",
    "even",
    "robot",
    "knows",
    "needs",
    "take",
    "right",
    "turn",
    "introduce",
    "cast",
    "city",
    "case",
    "comes",
    "markov",
    "decision",
    "process",
    "consider",
    "robot",
    "currently",
    "red",
    "room",
    "needs",
    "go",
    "green",
    "room",
    "let",
    "consider",
    "robot",
    "slight",
    "chance",
    "dysfunctioning",
    "might",
    "take",
    "left",
    "right",
    "bottom",
    "turn",
    "instead",
    "digging",
    "upper",
    "turn",
    "get",
    "green",
    "room",
    "retro",
    "question",
    "enable",
    "robot",
    "handle",
    "given",
    "environment",
    "right",
    "situation",
    "decision",
    "making",
    "regarding",
    "turn",
    "taken",
    "partly",
    "random",
    "partly",
    "another",
    "control",
    "robot",
    "partly",
    "random",
    "sure",
    "exactly",
    "robot",
    "mind",
    "dysfunctional",
    "partly",
    "control",
    "robot",
    "still",
    "making",
    "decision",
    "taking",
    "turn",
    "right",
    "help",
    "program",
    "embedded",
    "markov",
    "decision",
    "process",
    "discrete",
    "time",
    "stochastic",
    "control",
    "process",
    "provides",
    "mathematical",
    "framework",
    "modeling",
    "situations",
    "outcomes",
    "partly",
    "random",
    "partly",
    "control",
    "decision",
    "maker",
    "need",
    "give",
    "concept",
    "mathematical",
    "shape",
    "likely",
    "equation",
    "taken",
    "might",
    "surprised",
    "help",
    "bellman",
    "equation",
    "action",
    "minor",
    "tweaks",
    "look",
    "original",
    "bellman",
    "equation",
    "v",
    "x",
    "equal",
    "maximum",
    "comma",
    "plus",
    "gamma",
    "v",
    "needs",
    "changed",
    "equation",
    "introduce",
    "amount",
    "randomness",
    "long",
    "sure",
    "robot",
    "might",
    "take",
    "expected",
    "turn",
    "also",
    "sure",
    "room",
    "might",
    "end",
    "nothing",
    "rom",
    "moves",
    "current",
    "room",
    "point",
    "according",
    "equation",
    "sure",
    "stash",
    "next",
    "state",
    "room",
    "know",
    "probable",
    "turns",
    "robot",
    "might",
    "take",
    "order",
    "incorporate",
    "probabilities",
    "equation",
    "need",
    "associate",
    "probability",
    "turns",
    "quantify",
    "robot",
    "got",
    "expertise",
    "chance",
    "taking",
    "stern",
    "know",
    "get",
    "ps",
    "equal",
    "maximum",
    "rs",
    "comma",
    "plus",
    "gamma",
    "summation",
    "ps",
    "comma",
    "comma",
    "stash",
    "v",
    "stash",
    "ps",
    "stash",
    "probability",
    "moving",
    "room",
    "establish",
    "action",
    "submission",
    "expectation",
    "situation",
    "robot",
    "curse",
    "randomness",
    "let",
    "take",
    "look",
    "example",
    "associate",
    "probabilities",
    "terms",
    "owns",
    "essentially",
    "mean",
    "80",
    "chance",
    "robot",
    "take",
    "upper",
    "turn",
    "put",
    "required",
    "values",
    "equation",
    "get",
    "v",
    "equal",
    "maximum",
    "r",
    "comma",
    "comma",
    "v",
    "room",
    "plus",
    "zero",
    "point",
    "1",
    "v",
    "room",
    "rome",
    "v",
    "left",
    "plus",
    "v",
    "room",
    "right",
    "note",
    "value",
    "footprints",
    "change",
    "due",
    "fact",
    "incorporating",
    "stochastically",
    "time",
    "calculate",
    "values",
    "footprints",
    "instead",
    "let",
    "robot",
    "figure",
    "point",
    "considered",
    "rewarding",
    "robot",
    "action",
    "going",
    "particular",
    "room",
    "watering",
    "robot",
    "gets",
    "destination",
    "ideally",
    "reward",
    "action",
    "robot",
    "takes",
    "help",
    "better",
    "assess",
    "quality",
    "actions",
    "need",
    "always",
    "much",
    "better",
    "amount",
    "reward",
    "actions",
    "rewards",
    "right",
    "idea",
    "known",
    "living",
    "penalty",
    "reality",
    "reward",
    "system",
    "complex",
    "particularly",
    "modeling",
    "sparse",
    "rewards",
    "active",
    "area",
    "research",
    "domain",
    "reinforcement",
    "learning",
    "got",
    "equation",
    "going",
    "transition",
    "q",
    "learning",
    "equation",
    "gives",
    "us",
    "value",
    "going",
    "particular",
    "state",
    "taking",
    "stochastic",
    "city",
    "environment",
    "account",
    "also",
    "learned",
    "briefly",
    "idea",
    "living",
    "penalty",
    "deals",
    "associating",
    "move",
    "robot",
    "reward",
    "q",
    "learning",
    "processes",
    "idea",
    "assessing",
    "quality",
    "action",
    "taken",
    "move",
    "state",
    "rather",
    "determining",
    "possible",
    "value",
    "state",
    "moved",
    "earlier",
    "e",
    "1",
    "v",
    "2",
    "0",
    "point",
    "1",
    "v",
    "3",
    "incorporate",
    "idea",
    "assessing",
    "quality",
    "action",
    "moving",
    "certain",
    "state",
    "environment",
    "agent",
    "quality",
    "action",
    "look",
    "something",
    "like",
    "instead",
    "v",
    "1",
    "q",
    "1",
    "comma",
    "one",
    "q",
    "2",
    "comma",
    "2",
    "q",
    "3",
    "robot",
    "food",
    "states",
    "choose",
    "along",
    "four",
    "different",
    "actions",
    "also",
    "current",
    "state",
    "calculate",
    "q",
    "comma",
    "cumulative",
    "quality",
    "possible",
    "actions",
    "robot",
    "might",
    "take",
    "let",
    "break",
    "equation",
    "v",
    "equals",
    "maximum",
    "rs",
    "comma",
    "comma",
    "summation",
    "psas",
    "v",
    "discard",
    "maximum",
    "function",
    "plus",
    "gamma",
    "summation",
    "p",
    "v",
    "essentially",
    "equation",
    "produces",
    "v",
    "considering",
    "possible",
    "actions",
    "possible",
    "states",
    "current",
    "state",
    "robot",
    "taking",
    "maximum",
    "value",
    "caused",
    "taking",
    "certain",
    "action",
    "equation",
    "produces",
    "value",
    "footprint",
    "one",
    "possible",
    "action",
    "fact",
    "think",
    "quality",
    "q",
    "comma",
    "equal",
    "rs",
    "comma",
    "comma",
    "summation",
    "p",
    "v",
    "got",
    "equation",
    "quantify",
    "quality",
    "particular",
    "action",
    "going",
    "make",
    "little",
    "adjustment",
    "equation",
    "say",
    "v",
    "maximum",
    "possible",
    "values",
    "q",
    "comma",
    "right",
    "let",
    "utilize",
    "fact",
    "replace",
    "v",
    "dash",
    "function",
    "q",
    "comma",
    "becomes",
    "r",
    "comma",
    "comma",
    "summation",
    "psas",
    "maximum",
    "que",
    "es",
    "equation",
    "v",
    "turned",
    "equation",
    "q",
    "quality",
    "would",
    "done",
    "ease",
    "calculations",
    "one",
    "function",
    "q",
    "also",
    "core",
    "dynamic",
    "programming",
    "language",
    "one",
    "ocean",
    "q",
    "calculate",
    "r",
    "comma",
    "quantified",
    "metric",
    "produces",
    "reward",
    "moving",
    "certain",
    "state",
    "qualities",
    "actions",
    "called",
    "q",
    "values",
    "refer",
    "value",
    "footprints",
    "q",
    "values",
    "important",
    "piece",
    "puzzle",
    "temporal",
    "difference",
    "temporal",
    "difference",
    "component",
    "help",
    "robot",
    "calculate",
    "q",
    "values",
    "respect",
    "changes",
    "environment",
    "time",
    "consider",
    "robot",
    "currently",
    "mark",
    "state",
    "wants",
    "move",
    "upper",
    "state",
    "one",
    "thing",
    "note",
    "robot",
    "already",
    "knows",
    "q",
    "value",
    "making",
    "action",
    "moving",
    "upper",
    "state",
    "know",
    "environment",
    "stochastic",
    "nature",
    "reward",
    "robot",
    "get",
    "moving",
    "upper",
    "state",
    "might",
    "different",
    "earlier",
    "observation",
    "capture",
    "change",
    "real",
    "difference",
    "calculate",
    "new",
    "q",
    "comma",
    "formula",
    "subtract",
    "previously",
    "known",
    "qsa",
    "turn",
    "give",
    "us",
    "new",
    "qa",
    "equation",
    "derived",
    "gifts",
    "temporal",
    "difference",
    "q",
    "values",
    "helps",
    "capture",
    "random",
    "changes",
    "environment",
    "may",
    "impose",
    "name",
    "q",
    "comma",
    "updated",
    "following",
    "q",
    "comma",
    "equal",
    "qt",
    "minus",
    "1",
    "comma",
    "plus",
    "alpha",
    "dt",
    "comma",
    "allah",
    "alpha",
    "learning",
    "rate",
    "controls",
    "quickly",
    "robot",
    "adapts",
    "random",
    "changes",
    "imposed",
    "environment",
    "qts",
    "comma",
    "current",
    "state",
    "q",
    "value",
    "qt",
    "minus",
    "1",
    "comma",
    "previously",
    "recorded",
    "q",
    "value",
    "replace",
    "tds",
    "comma",
    "full",
    "form",
    "equation",
    "get",
    "q",
    "comma",
    "equal",
    "qt",
    "1",
    "comma",
    "plus",
    "alpha",
    "r",
    "comma",
    "plus",
    "gamma",
    "maximum",
    "q",
    "dash",
    "dash",
    "minus",
    "qt",
    "minus",
    "1",
    "comma",
    "little",
    "pieces",
    "q",
    "line",
    "together",
    "let",
    "move",
    "forward",
    "implementation",
    "part",
    "final",
    "equation",
    "right",
    "let",
    "see",
    "implement",
    "obtain",
    "best",
    "path",
    "robot",
    "take",
    "implement",
    "algorithm",
    "need",
    "understand",
    "warehouse",
    "location",
    "mapped",
    "different",
    "states",
    "let",
    "start",
    "reconnecting",
    "sample",
    "environment",
    "see",
    "l1",
    "l2",
    "l3",
    "align",
    "see",
    "certain",
    "borders",
    "also",
    "first",
    "let",
    "map",
    "locations",
    "warehouse",
    "two",
    "numbers",
    "states",
    "ease",
    "calculations",
    "right",
    "going",
    "create",
    "new",
    "python",
    "3",
    "file",
    "jupyter",
    "notebook",
    "name",
    "number",
    "okay",
    "let",
    "define",
    "states",
    "need",
    "import",
    "numpy",
    "going",
    "use",
    "numpy",
    "purpose",
    "let",
    "initialize",
    "parameters",
    "gamma",
    "alpha",
    "parameters",
    "gamma",
    "discount",
    "factor",
    "whereas",
    "alpha",
    "learning",
    "rate",
    "next",
    "going",
    "define",
    "states",
    "map",
    "numbers",
    "mentioned",
    "earlier",
    "l",
    "1",
    "0",
    "dylan",
    "line",
    "defined",
    "states",
    "numerical",
    "form",
    "next",
    "step",
    "define",
    "actions",
    "mentioned",
    "represents",
    "transition",
    "next",
    "state",
    "see",
    "array",
    "actions",
    "0",
    "going",
    "define",
    "reward",
    "table",
    "see",
    "matrix",
    "created",
    "showed",
    "understood",
    "correctly",
    "real",
    "barrel",
    "limitation",
    "depicted",
    "image",
    "example",
    "transitional",
    "tell",
    "one",
    "allowed",
    "reward",
    "zero",
    "discourage",
    "path",
    "tough",
    "situation",
    "add",
    "minus",
    "1",
    "gets",
    "negative",
    "reward",
    "code",
    "snippet",
    "see",
    "took",
    "states",
    "put",
    "respective",
    "state",
    "directly",
    "reachable",
    "certain",
    "state",
    "refer",
    "reward",
    "table",
    "created",
    "reconstruction",
    "easy",
    "understand",
    "one",
    "thing",
    "note",
    "consider",
    "top",
    "priority",
    "location",
    "l6",
    "yet",
    "would",
    "also",
    "need",
    "inverse",
    "mapping",
    "state",
    "back",
    "original",
    "location",
    "cleaner",
    "reach",
    "utter",
    "depths",
    "algorithms",
    "going",
    "inverse",
    "map",
    "location",
    "state",
    "delegation",
    "take",
    "distinct",
    "state",
    "location",
    "convert",
    "back",
    "define",
    "function",
    "get",
    "optimal",
    "get",
    "optimal",
    "route",
    "start",
    "location",
    "n",
    "location",
    "worry",
    "code",
    "pick",
    "explain",
    "every",
    "bit",
    "code",
    "get",
    "optimal",
    "route",
    "function",
    "take",
    "two",
    "arguments",
    "style",
    "location",
    "warehouse",
    "end",
    "location",
    "warehouse",
    "recipe",
    "lovely",
    "return",
    "optimal",
    "route",
    "reaching",
    "end",
    "location",
    "starting",
    "location",
    "form",
    "ordered",
    "list",
    "containing",
    "letters",
    "start",
    "defining",
    "function",
    "initializing",
    "q",
    "values",
    "zeros",
    "see",
    "given",
    "q",
    "value",
    "0",
    "need",
    "copy",
    "reward",
    "matrix",
    "new",
    "one",
    "rewards",
    "new",
    "next",
    "need",
    "get",
    "ending",
    "state",
    "corresponding",
    "ending",
    "location",
    "information",
    "automatically",
    "set",
    "priority",
    "given",
    "ending",
    "stay",
    "highest",
    "one",
    "defining",
    "automatically",
    "set",
    "priority",
    "given",
    "ending",
    "state",
    "nine",
    "nine",
    "nine",
    "going",
    "initialize",
    "q",
    "values",
    "0",
    "queue",
    "learning",
    "process",
    "see",
    "see",
    "taking",
    "range",
    "going",
    "pick",
    "state",
    "randomly",
    "going",
    "use",
    "mp",
    "dot",
    "random",
    "r",
    "nt",
    "traversing",
    "neighbor",
    "location",
    "maze",
    "going",
    "iterate",
    "new",
    "reward",
    "matrix",
    "get",
    "actions",
    "greater",
    "0",
    "going",
    "pick",
    "action",
    "randomly",
    "list",
    "playable",
    "actions",
    "years",
    "next",
    "state",
    "going",
    "compute",
    "temporal",
    "difference",
    "td",
    "rewards",
    "plus",
    "gamma",
    "queue",
    "next",
    "state",
    "take",
    "n",
    "p",
    "dot",
    "arg",
    "max",
    "q",
    "next",
    "eight",
    "minus",
    "q",
    "current",
    "state",
    "going",
    "update",
    "q",
    "values",
    "using",
    "bellman",
    "equation",
    "see",
    "bellman",
    "equation",
    "going",
    "update",
    "q",
    "values",
    "going",
    "initialize",
    "optimal",
    "route",
    "starting",
    "location",
    "know",
    "next",
    "location",
    "yet",
    "initialize",
    "value",
    "starting",
    "location",
    "random",
    "shh",
    "know",
    "exact",
    "number",
    "iteration",
    "needed",
    "reach",
    "final",
    "location",
    "hence",
    "loop",
    "good",
    "choice",
    "iteration",
    "going",
    "fetch",
    "starting",
    "state",
    "fetch",
    "highest",
    "q",
    "value",
    "penetrating",
    "starting",
    "state",
    "go",
    "index",
    "next",
    "state",
    "need",
    "corresponding",
    "letter",
    "going",
    "use",
    "state",
    "location",
    "function",
    "mentioned",
    "going",
    "update",
    "starting",
    "location",
    "next",
    "iteration",
    "finally",
    "return",
    "root",
    "let",
    "take",
    "starting",
    "location",
    "n",
    "line",
    "location",
    "l1",
    "see",
    "part",
    "actually",
    "get",
    "see",
    "get",
    "airline",
    "l8l",
    "five",
    "l2",
    "l1",
    "look",
    "image",
    "start",
    "l9",
    "l1",
    "got",
    "l8l",
    "5",
    "l",
    "2",
    "l",
    "1",
    "l",
    "hl",
    "5",
    "l2",
    "l1",
    "would",
    "yield",
    "us",
    "maximum",
    "mm",
    "value",
    "maximum",
    "reward",
    "robot",
    "come",
    "end",
    "q",
    "learning",
    "session",
    "past",
    "year",
    "seen",
    "lot",
    "great",
    "examples",
    "machine",
    "learning",
    "many",
    "new",
    "application",
    "machine",
    "learning",
    "discovered",
    "brought",
    "light",
    "especially",
    "healthcare",
    "finance",
    "speech",
    "recognition",
    "augmented",
    "reality",
    "much",
    "complex",
    "3d",
    "video",
    "applications",
    "natural",
    "language",
    "processing",
    "easily",
    "talked",
    "domain",
    "within",
    "community",
    "likes",
    "lmf",
    "open",
    "sourced",
    "let",
    "look",
    "amazing",
    "machine",
    "learning",
    "projects",
    "open",
    "sourced",
    "code",
    "available",
    "discussed",
    "2018",
    "nine",
    "spectrum",
    "first",
    "foremost",
    "tensorflow",
    "dot",
    "ds",
    "machine",
    "learning",
    "browser",
    "fictional",
    "thought",
    "years",
    "back",
    "back",
    "stunning",
    "reality",
    "lot",
    "us",
    "field",
    "welded",
    "favorite",
    "ide",
    "tells",
    "dot",
    "js",
    "potential",
    "change",
    "habits",
    "become",
    "popular",
    "released",
    "since",
    "release",
    "earlier",
    "year",
    "continues",
    "amaze",
    "flexibility",
    "repository",
    "states",
    "primarily",
    "three",
    "major",
    "features",
    "terms",
    "rho",
    "dot",
    "j",
    "develop",
    "machine",
    "learning",
    "deep",
    "learning",
    "models",
    "process",
    "run",
    "flow",
    "models",
    "within",
    "browser",
    "retrain",
    "gene",
    "prediction",
    "models",
    "well",
    "familiar",
    "kara",
    "layers",
    "epa",
    "seem",
    "quite",
    "familiar",
    "plenty",
    "examples",
    "available",
    "github",
    "repository",
    "check",
    "legs",
    "quicken",
    "learning",
    "curve",
    "mentioned",
    "earlier",
    "leave",
    "links",
    "open",
    "source",
    "machine",
    "learning",
    "projects",
    "description",
    "next",
    "discuss",
    "detector",
    "developed",
    "facebook",
    "made",
    "huge",
    "splash",
    "earlier",
    "launched",
    "80",
    "developed",
    "facebook",
    "ai",
    "research",
    "team",
    "fa",
    "ir",
    "implements",
    "state",
    "art",
    "object",
    "detection",
    "frame",
    "written",
    "python",
    "help",
    "enable",
    "multiple",
    "projects",
    "including",
    "dance",
    "pose",
    "know",
    "exactly",
    "suppose",
    "example",
    "repository",
    "contains",
    "code",
    "70",
    "preacher",
    "involves",
    "good",
    "open",
    "source",
    "small",
    "guys",
    "check",
    "moment",
    "talked",
    "suppose",
    "next",
    "one",
    "going",
    "talk",
    "supposed",
    "stents",
    "human",
    "pose",
    "estimation",
    "wild",
    "code",
    "train",
    "evaluate",
    "dance",
    "pose",
    "using",
    "cnn",
    "model",
    "included",
    "given",
    "link",
    "open",
    "source",
    "code",
    "description",
    "notebooks",
    "available",
    "well",
    "visualize",
    "certain",
    "sports",
    "cocoa",
    "data",
    "set",
    "next",
    "list",
    "painterly",
    "harmonization",
    "want",
    "take",
    "moment",
    "admire",
    "images",
    "tell",
    "ones",
    "done",
    "human",
    "one",
    "machine",
    "certainly",
    "could",
    "first",
    "frame",
    "input",
    "image",
    "original",
    "one",
    "third",
    "frame",
    "see",
    "generated",
    "technique",
    "amazing",
    "right",
    "algorithm",
    "external",
    "object",
    "choosing",
    "image",
    "manages",
    "make",
    "look",
    "like",
    "nothing",
    "touched",
    "make",
    "sure",
    "check",
    "code",
    "try",
    "implement",
    "different",
    "sets",
    "images",
    "really",
    "really",
    "fun",
    "talking",
    "images",
    "image",
    "painting",
    "give",
    "image",
    "ask",
    "extend",
    "boundaries",
    "imagining",
    "would",
    "look",
    "like",
    "entire",
    "scene",
    "captured",
    "would",
    "understandably",
    "turn",
    "image",
    "editing",
    "software",
    "awesome",
    "news",
    "achieve",
    "lines",
    "code",
    "image",
    "painting",
    "project",
    "akira",
    "implementation",
    "stanford",
    "image",
    "failing",
    "paper",
    "incredibly",
    "cool",
    "illustrated",
    "paper",
    "research",
    "paper",
    "given",
    "links",
    "description",
    "check",
    "guys",
    "see",
    "implement",
    "let",
    "talk",
    "audio",
    "processing",
    "another",
    "field",
    "machine",
    "learning",
    "started",
    "make",
    "mark",
    "limited",
    "generate",
    "music",
    "tasks",
    "like",
    "audio",
    "classification",
    "fingerprinting",
    "segmentation",
    "tagging",
    "much",
    "lot",
    "still",
    "yet",
    "explored",
    "knows",
    "perhaps",
    "could",
    "use",
    "project",
    "pioneer",
    "way",
    "top",
    "want",
    "discover",
    "planner",
    "might",
    "perhaps",
    "overstating",
    "things",
    "bit",
    "astronaut",
    "repository",
    "definitely",
    "get",
    "close",
    "google",
    "brain",
    "team",
    "discovered",
    "two",
    "new",
    "planets",
    "summer",
    "2017",
    "applying",
    "astronaut",
    "deep",
    "neural",
    "network",
    "meant",
    "working",
    "astronomical",
    "data",
    "goes",
    "show",
    "application",
    "machine",
    "learning",
    "truly",
    "monumental",
    "development",
    "team",
    "behind",
    "technology",
    "open",
    "source",
    "entire",
    "code",
    "go",
    "ahead",
    "check",
    "planet",
    "knows",
    "might",
    "even",
    "planet",
    "name",
    "could",
    "possibly",
    "let",
    "section",
    "pass",
    "without",
    "mentioning",
    "brt",
    "google",
    "ai",
    "released",
    "smashed",
    "record",
    "way",
    "winning",
    "hearts",
    "nlp",
    "enthusiasts",
    "experts",
    "alike",
    "following",
    "lmf",
    "lmo",
    "brt",
    "really",
    "blew",
    "away",
    "competition",
    "performance",
    "obtained",
    "state",
    "art",
    "result",
    "11",
    "lp",
    "task",
    "apart",
    "official",
    "google",
    "repository",
    "python",
    "implementation",
    "birth",
    "worth",
    "checking",
    "whether",
    "makes",
    "new",
    "era",
    "natural",
    "language",
    "processing",
    "thing",
    "soon",
    "find",
    "add",
    "sure",
    "guys",
    "might",
    "heard",
    "framework",
    "automatically",
    "learning",
    "high",
    "quality",
    "models",
    "without",
    "requiring",
    "programming",
    "expertise",
    "since",
    "google",
    "invention",
    "framework",
    "based",
    "tensorflow",
    "build",
    "simple",
    "models",
    "using",
    "danette",
    "even",
    "extend",
    "use",
    "train",
    "neural",
    "network",
    "github",
    "page",
    "contains",
    "code",
    "example",
    "api",
    "documentation",
    "things",
    "get",
    "hands",
    "dirty",
    "trust",
    "otto",
    "ml",
    "next",
    "big",
    "thing",
    "ng",
    "field",
    "follow",
    "researchers",
    "social",
    "media",
    "must",
    "come",
    "across",
    "images",
    "showing",
    "video",
    "form",
    "stick",
    "human",
    "running",
    "across",
    "terrain",
    "trying",
    "stand",
    "sort",
    "friends",
    "reinforcement",
    "learning",
    "action",
    "signature",
    "example",
    "framework",
    "create",
    "simulated",
    "humanoid",
    "imitate",
    "multiple",
    "motion",
    "skin",
    "let",
    "look",
    "top",
    "10",
    "skills",
    "required",
    "become",
    "successful",
    "machine",
    "learning",
    "engineer",
    "starting",
    "programming",
    "languages",
    "python",
    "lingua",
    "franca",
    "machine",
    "learning",
    "may",
    "exposure",
    "buy",
    "even",
    "previously",
    "programming",
    "computer",
    "science",
    "research",
    "field",
    "however",
    "important",
    "solid",
    "understanding",
    "glasses",
    "data",
    "structures",
    "sometimes",
    "python",
    "wo",
    "enough",
    "often",
    "encounter",
    "projects",
    "need",
    "leverage",
    "hardware",
    "speed",
    "improvements",
    "make",
    "sure",
    "familiar",
    "basic",
    "algorithms",
    "well",
    "classes",
    "memory",
    "management",
    "linking",
    "want",
    "job",
    "machine",
    "learning",
    "probably",
    "learn",
    "languages",
    "point",
    "help",
    "speeding",
    "code",
    "whereas",
    "works",
    "great",
    "statistics",
    "plots",
    "hadoop",
    "probably",
    "need",
    "implement",
    "mappers",
    "reducers",
    "java",
    "next",
    "linear",
    "algebra",
    "need",
    "intimately",
    "familiar",
    "mattresses",
    "vectors",
    "matrix",
    "multiplication",
    "understanding",
    "derivatives",
    "integrals",
    "clear",
    "otherwise",
    "even",
    "simple",
    "concept",
    "like",
    "gradient",
    "descent",
    "elude",
    "statistic",
    "going",
    "come",
    "lot",
    "least",
    "make",
    "sure",
    "familiar",
    "caution",
    "distributions",
    "means",
    "standard",
    "deviation",
    "much",
    "every",
    "bit",
    "statistical",
    "understanding",
    "beyond",
    "helps",
    "theories",
    "help",
    "learning",
    "algorithms",
    "great",
    "samples",
    "naive",
    "buys",
    "gaussian",
    "mixture",
    "models",
    "hidden",
    "markov",
    "models",
    "need",
    "firm",
    "understanding",
    "probability",
    "stats",
    "understand",
    "models",
    "go",
    "nuts",
    "study",
    "measure",
    "theory",
    "next",
    "advanced",
    "signal",
    "processing",
    "techniques",
    "feature",
    "extraction",
    "one",
    "important",
    "parts",
    "machine",
    "learning",
    "different",
    "types",
    "problems",
    "need",
    "various",
    "solutions",
    "may",
    "able",
    "utilize",
    "really",
    "cool",
    "advanced",
    "signal",
    "processing",
    "algorithms",
    "wavelets",
    "share",
    "let",
    "go",
    "blades",
    "bandless",
    "need",
    "learn",
    "analysis",
    "try",
    "apply",
    "problems",
    "skill",
    "give",
    "edge",
    "skills",
    "kid",
    "give",
    "edge",
    "applying",
    "machine",
    "learning",
    "engine",
    "job",
    "others",
    "next",
    "applied",
    "maths",
    "lot",
    "machine",
    "learning",
    "techniques",
    "fancy",
    "types",
    "functional",
    "approximation",
    "often",
    "get",
    "developed",
    "theoretical",
    "mathematician",
    "get",
    "applied",
    "people",
    "understand",
    "theory",
    "result",
    "many",
    "developers",
    "might",
    "hard",
    "time",
    "finding",
    "best",
    "techniques",
    "problem",
    "even",
    "basic",
    "understanding",
    "numerical",
    "analysis",
    "give",
    "huge",
    "edge",
    "firm",
    "understanding",
    "ending",
    "algorithm",
    "theory",
    "knowing",
    "algorithm",
    "works",
    "also",
    "discriminate",
    "models",
    "svm",
    "need",
    "understand",
    "subjects",
    "gradient",
    "descent",
    "convex",
    "optimization",
    "lagrange",
    "quadratic",
    "programming",
    "partial",
    "differentiation",
    "equations",
    "much",
    "math",
    "might",
    "seem",
    "intimidating",
    "first",
    "away",
    "machine",
    "learning",
    "much",
    "math",
    "intensive",
    "something",
    "like",
    "developer",
    "like",
    "skill",
    "getting",
    "better",
    "math",
    "man",
    "focus",
    "practice",
    "next",
    "skill",
    "list",
    "neural",
    "network",
    "architectures",
    "need",
    "machine",
    "learning",
    "tasks",
    "complex",
    "human",
    "quote",
    "directly",
    "tasks",
    "complex",
    "impractical",
    "neural",
    "networks",
    "class",
    "models",
    "within",
    "general",
    "machine",
    "learning",
    "literature",
    "neural",
    "networks",
    "specific",
    "set",
    "algorithms",
    "revolutionized",
    "machine",
    "learning",
    "inspired",
    "biological",
    "neural",
    "networks",
    "current",
    "deep",
    "neural",
    "networks",
    "proven",
    "work",
    "quite",
    "well",
    "well",
    "neural",
    "networks",
    "general",
    "function",
    "approximations",
    "applied",
    "almost",
    "machine",
    "learning",
    "problem",
    "learning",
    "complex",
    "mapping",
    "input",
    "output",
    "space",
    "course",
    "still",
    "good",
    "reason",
    "surge",
    "popularity",
    "neural",
    "networks",
    "neural",
    "networks",
    "far",
    "accurate",
    "way",
    "approaching",
    "many",
    "problems",
    "like",
    "translation",
    "speech",
    "recognition",
    "image",
    "classification",
    "coming",
    "next",
    "point",
    "natural",
    "language",
    "processing",
    "since",
    "combines",
    "computer",
    "science",
    "listed",
    "bunch",
    "libraries",
    "like",
    "nlt",
    "k",
    "chances",
    "mm",
    "techniques",
    "sentimental",
    "analysis",
    "summarization",
    "unique",
    "nlp",
    "audio",
    "video",
    "processing",
    "frequent",
    "overlap",
    "natural",
    "language",
    "processing",
    "however",
    "natural",
    "language",
    "processing",
    "applied",
    "non",
    "audio",
    "data",
    "like",
    "text",
    "voice",
    "audio",
    "analysis",
    "involves",
    "extracting",
    "useful",
    "information",
    "audio",
    "signals",
    "math",
    "get",
    "far",
    "one",
    "also",
    "familiar",
    "concepts",
    "fast",
    "fourier",
    "transforms",
    "technical",
    "skills",
    "required",
    "become",
    "successful",
    "machine",
    "learning",
    "engineer",
    "next",
    "going",
    "discuss",
    "skills",
    "soft",
    "skills",
    "required",
    "become",
    "engineer",
    "first",
    "industry",
    "knowledge",
    "successful",
    "machine",
    "learning",
    "projects",
    "going",
    "address",
    "real",
    "pain",
    "points",
    "whichever",
    "industry",
    "working",
    "know",
    "industry",
    "works",
    "beneficial",
    "business",
    "machine",
    "learning",
    "engineer",
    "business",
    "acumen",
    "elements",
    "make",
    "successful",
    "business",
    "model",
    "particular",
    "algorithm",
    "technical",
    "skills",
    "channel",
    "productively",
    "wo",
    "able",
    "discern",
    "problems",
    "potential",
    "challenges",
    "need",
    "solving",
    "business",
    "sustain",
    "grow",
    "wo",
    "really",
    "able",
    "help",
    "organization",
    "explore",
    "new",
    "business",
    "opportunities",
    "skill",
    "next",
    "effective",
    "communication",
    "need",
    "explain",
    "machine",
    "learning",
    "concepts",
    "people",
    "little",
    "expertise",
    "field",
    "chances",
    "need",
    "work",
    "team",
    "engineers",
    "well",
    "many",
    "teams",
    "communication",
    "going",
    "make",
    "much",
    "easier",
    "companies",
    "searching",
    "strong",
    "machine",
    "learning",
    "engineer",
    "looking",
    "someone",
    "clearly",
    "fluently",
    "translate",
    "technical",
    "findings",
    "non",
    "technical",
    "team",
    "marketing",
    "sales",
    "department",
    "next",
    "list",
    "rapid",
    "prototyping",
    "iterating",
    "ideas",
    "quickly",
    "possible",
    "mandatory",
    "finding",
    "one",
    "works",
    "machine",
    "learning",
    "applies",
    "everything",
    "picking",
    "right",
    "model",
    "working",
    "projects",
    "testing",
    "need",
    "group",
    "techniques",
    "used",
    "quickly",
    "fabricate",
    "scale",
    "model",
    "physical",
    "part",
    "assembly",
    "using",
    "computer",
    "aided",
    "design",
    "cat",
    "last",
    "least",
    "final",
    "skill",
    "keep",
    "updated",
    "must",
    "stay",
    "date",
    "upcoming",
    "changes",
    "every",
    "month",
    "new",
    "neural",
    "network",
    "models",
    "come",
    "performed",
    "previous",
    "architecture",
    "also",
    "means",
    "aware",
    "news",
    "regarding",
    "development",
    "tools",
    "changelog",
    "conferences",
    "much",
    "need",
    "know",
    "theories",
    "algorithms",
    "achieve",
    "reading",
    "research",
    "papers",
    "blogs",
    "conference",
    "videos",
    "also",
    "need",
    "focus",
    "online",
    "community",
    "changes",
    "quickly",
    "expect",
    "cultivate",
    "change",
    "certain",
    "skills",
    "bonus",
    "skills",
    "give",
    "edge",
    "competitors",
    "persons",
    "applying",
    "engineer",
    "position",
    "bonus",
    "point",
    "physics",
    "might",
    "situation",
    "like",
    "apply",
    "machine",
    "learning",
    "techniques",
    "system",
    "interact",
    "real",
    "world",
    "knowledge",
    "physics",
    "take",
    "far",
    "next",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "driver",
    "behind",
    "many",
    "exciting",
    "developments",
    "deep",
    "learning",
    "ai",
    "community",
    "alphago",
    "zero",
    "open",
    "dota",
    "2",
    "pot",
    "critical",
    "understand",
    "want",
    "go",
    "robotics",
    "cars",
    "ai",
    "related",
    "areas",
    "finally",
    "computer",
    "vision",
    "disciplines",
    "far",
    "resources",
    "available",
    "learning",
    "computer",
    "vision",
    "field",
    "appears",
    "lowest",
    "barriers",
    "entry",
    "course",
    "likely",
    "means",
    "face",
    "slightly",
    "competition",
    "good",
    "knowledge",
    "computer",
    "vision",
    "rolls",
    "give",
    "edge",
    "competitors",
    "hope",
    "got",
    "acquainted",
    "skills",
    "required",
    "become",
    "successful",
    "machine",
    "learning",
    "engineer",
    "know",
    "living",
    "worlds",
    "humans",
    "machines",
    "today",
    "world",
    "machines",
    "robots",
    "programmed",
    "start",
    "following",
    "instructions",
    "machine",
    "started",
    "learning",
    "experience",
    "work",
    "like",
    "us",
    "feel",
    "like",
    "us",
    "things",
    "accurately",
    "us",
    "well",
    "machine",
    "learning",
    "angela",
    "comes",
    "picture",
    "make",
    "sure",
    "everything",
    "working",
    "according",
    "procedures",
    "guidelines",
    "opinion",
    "machine",
    "learning",
    "one",
    "recent",
    "technologies",
    "probably",
    "use",
    "dozens",
    "times",
    "every",
    "day",
    "without",
    "even",
    "knowing",
    "indulge",
    "different",
    "roles",
    "salary",
    "trends",
    "resume",
    "machine",
    "learning",
    "engineer",
    "applying",
    "job",
    "let",
    "understand",
    "exactly",
    "machine",
    "learning",
    "engineering",
    "machine",
    "learning",
    "engineers",
    "sophisticated",
    "programmers",
    "develop",
    "machines",
    "systems",
    "learn",
    "apply",
    "knowledge",
    "without",
    "specific",
    "direction",
    "artificial",
    "intelligence",
    "goal",
    "engineer",
    "computer",
    "programmers",
    "focus",
    "goes",
    "beyond",
    "specifically",
    "programming",
    "machines",
    "perform",
    "specific",
    "tasks",
    "create",
    "programs",
    "enable",
    "machines",
    "take",
    "actions",
    "without",
    "specifically",
    "directed",
    "perform",
    "tasks",
    "look",
    "job",
    "trends",
    "machine",
    "learning",
    "general",
    "see",
    "seattle",
    "jobs",
    "new",
    "york",
    "1100",
    "san",
    "francisco",
    "1100",
    "bengaluru",
    "india",
    "1100",
    "sunnyvale",
    "california",
    "number",
    "jobs",
    "see",
    "number",
    "jobs",
    "market",
    "much",
    "probably",
    "emergence",
    "machine",
    "learning",
    "artificial",
    "intelligence",
    "number",
    "going",
    "get",
    "higher",
    "look",
    "job",
    "opening",
    "percentage",
    "see",
    "per",
    "annum",
    "bracket",
    "percentage",
    "maximum",
    "assured",
    "get",
    "job",
    "engineer",
    "probably",
    "get",
    "around",
    "90",
    "thousand",
    "bucks",
    "year",
    "safe",
    "say",
    "hundred",
    "ten",
    "thousand",
    "dollars",
    "per",
    "year",
    "25",
    "20",
    "percent",
    "almost",
    "hundred",
    "thirty",
    "thousand",
    "dollars",
    "senior",
    "machine",
    "learning",
    "jenna",
    "13",
    "point",
    "6",
    "7",
    "finally",
    "senior",
    "machine",
    "learning",
    "engineer",
    "data",
    "scientist",
    "salary",
    "hundred",
    "forty",
    "thousand",
    "dollars",
    "per",
    "annum",
    "percentage",
    "one",
    "really",
    "low",
    "see",
    "great",
    "opportunity",
    "people",
    "trying",
    "go",
    "machine",
    "learning",
    "field",
    "get",
    "started",
    "let",
    "look",
    "machine",
    "learning",
    "junior",
    "salary",
    "average",
    "salary",
    "around",
    "hundred",
    "eleven",
    "thousand",
    "four",
    "hundred",
    "ninety",
    "dollars",
    "average",
    "salary",
    "india",
    "around",
    "seven",
    "last",
    "nineteen",
    "thousand",
    "six",
    "hundred",
    "forty",
    "six",
    "rupees",
    "good",
    "average",
    "salary",
    "particular",
    "profession",
    "moving",
    "forward",
    "look",
    "salary",
    "machine",
    "learning",
    "know",
    "salary",
    "ranges",
    "seventy",
    "seven",
    "thousand",
    "dollars",
    "two",
    "hundred",
    "fifty",
    "one",
    "thousand",
    "dollars",
    "per",
    "annum",
    "huge",
    "salary",
    "talk",
    "bonus",
    "like",
    "three",
    "thousand",
    "dollars",
    "twenty",
    "five",
    "thousand",
    "dollars",
    "depending",
    "work",
    "youtube",
    "project",
    "working",
    "let",
    "talk",
    "profit",
    "sharing",
    "around",
    "two",
    "thousand",
    "dollars",
    "fifty",
    "thousand",
    "dollars",
    "depends",
    "upon",
    "project",
    "working",
    "company",
    "working",
    "percentage",
    "give",
    "general",
    "developer",
    "particular",
    "project",
    "total",
    "pay",
    "comes",
    "around",
    "seventy",
    "six",
    "thousand",
    "dollars",
    "thousand",
    "dollars",
    "two",
    "hundred",
    "sixty",
    "two",
    "thousand",
    "dollars",
    "entry",
    "level",
    "machine",
    "learning",
    "engineer",
    "imagine",
    "become",
    "experience",
    "machine",
    "learning",
    "engineer",
    "salary",
    "going",
    "go",
    "roof",
    "understood",
    "exactly",
    "machine",
    "learning",
    "engineer",
    "various",
    "salary",
    "trends",
    "job",
    "trends",
    "market",
    "rising",
    "let",
    "understand",
    "skills",
    "takes",
    "become",
    "machine",
    "learning",
    "engine",
    "first",
    "programming",
    "languages",
    "programming",
    "languages",
    "big",
    "deal",
    "comes",
    "machine",
    "learning",
    "need",
    "proficiency",
    "one",
    "language",
    "might",
    "require",
    "proficiency",
    "python",
    "java",
    "might",
    "working",
    "hadoop",
    "environment",
    "require",
    "java",
    "programming",
    "mapreduce",
    "coatings",
    "sometimes",
    "great",
    "visualization",
    "purposes",
    "python",
    "know",
    "another",
    "favorite",
    "languages",
    "comes",
    "machine",
    "learning",
    "next",
    "scale",
    "particular",
    "individual",
    "needs",
    "calculus",
    "statistics",
    "lot",
    "machine",
    "learning",
    "algorithms",
    "mostly",
    "maths",
    "statistics",
    "lot",
    "static",
    "required",
    "majorly",
    "matrix",
    "multiplication",
    "good",
    "understanding",
    "calculus",
    "well",
    "statistic",
    "required",
    "next",
    "signal",
    "processing",
    "advanced",
    "signal",
    "processing",
    "something",
    "give",
    "upper",
    "edge",
    "machine",
    "learning",
    "engine",
    "applying",
    "job",
    "anywhere",
    "next",
    "kill",
    "applied",
    "maths",
    "mentioned",
    "earlier",
    "many",
    "machine",
    "learning",
    "algorithms",
    "purely",
    "mathematical",
    "formulas",
    "good",
    "understanding",
    "maths",
    "algorithm",
    "works",
    "take",
    "far",
    "ahead",
    "next",
    "list",
    "neural",
    "networks",
    "real",
    "networks",
    "something",
    "emerging",
    "quite",
    "popularly",
    "recent",
    "years",
    "due",
    "efficiency",
    "extent",
    "walk",
    "get",
    "results",
    "soon",
    "possible",
    "neural",
    "networks",
    "must",
    "machine",
    "learning",
    "engine",
    "moving",
    "forward",
    "language",
    "processing",
    "lot",
    "times",
    "machine",
    "learning",
    "engineers",
    "deal",
    "text",
    "data",
    "voice",
    "data",
    "well",
    "video",
    "data",
    "processing",
    "kind",
    "language",
    "audio",
    "video",
    "something",
    "engineer",
    "daily",
    "basis",
    "one",
    "needs",
    "proficient",
    "area",
    "also",
    "skills",
    "absolutely",
    "necessary",
    "would",
    "say",
    "machine",
    "learning",
    "engineer",
    "let",
    "discuss",
    "job",
    "description",
    "roles",
    "responsibilities",
    "particular",
    "machine",
    "learning",
    "engineer",
    "depending",
    "level",
    "expertise",
    "machine",
    "learning",
    "engineers",
    "may",
    "study",
    "transform",
    "data",
    "science",
    "prototypes",
    "need",
    "design",
    "machine",
    "learning",
    "systems",
    "also",
    "need",
    "research",
    "implement",
    "appropriate",
    "machine",
    "learning",
    "algorithms",
    "tools",
    "important",
    "part",
    "job",
    "need",
    "develop",
    "new",
    "machine",
    "learning",
    "application",
    "according",
    "industry",
    "requirements",
    "select",
    "appropriate",
    "data",
    "sets",
    "data",
    "representation",
    "methods",
    "slight",
    "deviation",
    "data",
    "set",
    "data",
    "representation",
    "going",
    "affect",
    "model",
    "lot",
    "need",
    "run",
    "machine",
    "learning",
    "tests",
    "experiments",
    "need",
    "perform",
    "statistical",
    "analysis",
    "using",
    "test",
    "results",
    "sometimes",
    "people",
    "ask",
    "exactly",
    "difference",
    "data",
    "analyst",
    "machine",
    "learning",
    "engineer",
    "static",
    "analysis",
    "small",
    "part",
    "machine",
    "learning",
    "engineers",
    "job",
    "whereas",
    "major",
    "part",
    "probably",
    "covers",
    "large",
    "part",
    "data",
    "analyst",
    "job",
    "rather",
    "machine",
    "learning",
    "engineers",
    "job",
    "machine",
    "learning",
    "engineers",
    "might",
    "need",
    "train",
    "retrain",
    "systems",
    "whenever",
    "necessary",
    "also",
    "need",
    "extend",
    "existing",
    "machine",
    "learning",
    "libraries",
    "frameworks",
    "full",
    "potential",
    "could",
    "make",
    "model",
    "works",
    "superbly",
    "finally",
    "need",
    "keep",
    "abreast",
    "developments",
    "field",
    "needless",
    "say",
    "machine",
    "general",
    "particular",
    "individual",
    "stay",
    "updated",
    "technologies",
    "coming",
    "market",
    "every",
    "new",
    "technology",
    "arises",
    "overthrow",
    "older",
    "one",
    "need",
    "date",
    "coming",
    "resume",
    "part",
    "machine",
    "learning",
    "engineer",
    "resume",
    "particular",
    "machine",
    "learning",
    "engineers",
    "consist",
    "like",
    "clear",
    "career",
    "objective",
    "skills",
    "particular",
    "individual",
    "possesses",
    "educational",
    "qualification",
    "certain",
    "certification",
    "past",
    "experience",
    "experienced",
    "machine",
    "learning",
    "jen",
    "projects",
    "worked",
    "let",
    "look",
    "various",
    "elements",
    "required",
    "engineers",
    "resume",
    "first",
    "need",
    "clear",
    "career",
    "objective",
    "need",
    "stretch",
    "much",
    "keep",
    "precise",
    "possible",
    "next",
    "skills",
    "required",
    "skills",
    "technical",
    "well",
    "non",
    "technical",
    "let",
    "look",
    "various",
    "technical",
    "skills",
    "starting",
    "technical",
    "skills",
    "first",
    "programming",
    "languages",
    "java",
    "python",
    "first",
    "foremost",
    "requirement",
    "good",
    "grip",
    "programming",
    "languages",
    "preferably",
    "python",
    "easy",
    "learn",
    "applications",
    "wider",
    "language",
    "important",
    "good",
    "understanding",
    "topics",
    "like",
    "data",
    "structures",
    "memory",
    "management",
    "classes",
    "python",
    "good",
    "language",
    "alone",
    "help",
    "probably",
    "learn",
    "languages",
    "like",
    "python",
    "java",
    "also",
    "work",
    "mapreduce",
    "point",
    "time",
    "next",
    "list",
    "calculus",
    "linear",
    "algebra",
    "statistics",
    "need",
    "intimately",
    "familiar",
    "matrices",
    "vectors",
    "matrix",
    "multiplication",
    "statistics",
    "going",
    "come",
    "lot",
    "least",
    "make",
    "sure",
    "familiar",
    "caution",
    "distribution",
    "means",
    "standard",
    "deviations",
    "much",
    "also",
    "need",
    "firm",
    "understanding",
    "probability",
    "stats",
    "understand",
    "machine",
    "learning",
    "models",
    "next",
    "mentioned",
    "earlier",
    "signal",
    "processing",
    "techniques",
    "feature",
    "extraction",
    "one",
    "important",
    "parts",
    "machine",
    "learning",
    "different",
    "types",
    "problems",
    "need",
    "various",
    "solutions",
    "may",
    "able",
    "utilize",
    "really",
    "cool",
    "advanced",
    "signal",
    "processing",
    "algorithms",
    "wavelengths",
    "shallots",
    "curve",
    "let",
    "ballast",
    "try",
    "learn",
    "analysis",
    "try",
    "apply",
    "problems",
    "gives",
    "upper",
    "jaw",
    "machine",
    "learning",
    "engineers",
    "go",
    "next",
    "mathematics",
    "lot",
    "machine",
    "learning",
    "techniques",
    "fancy",
    "types",
    "function",
    "approximation",
    "firm",
    "understanding",
    "algorithm",
    "theory",
    "knowing",
    "algorithm",
    "works",
    "really",
    "necessary",
    "understanding",
    "subjects",
    "like",
    "gradient",
    "descent",
    "convex",
    "optimization",
    "quadratic",
    "programming",
    "partial",
    "differentiation",
    "help",
    "lot",
    "neural",
    "networks",
    "talking",
    "earlier",
    "need",
    "machine",
    "learning",
    "tasks",
    "flex",
    "humans",
    "quote",
    "directly",
    "tasks",
    "complex",
    "impractical",
    "neural",
    "networks",
    "class",
    "models",
    "within",
    "general",
    "machine",
    "learning",
    "literature",
    "specific",
    "set",
    "algorithms",
    "revolutionized",
    "machine",
    "learning",
    "deep",
    "neural",
    "networks",
    "proven",
    "work",
    "quite",
    "well",
    "neural",
    "networks",
    "themself",
    "general",
    "function",
    "approximations",
    "applied",
    "almost",
    "machine",
    "learning",
    "problem",
    "help",
    "lot",
    "learning",
    "complex",
    "mapping",
    "input",
    "output",
    "space",
    "next",
    "language",
    "processing",
    "since",
    "natural",
    "language",
    "processing",
    "combines",
    "two",
    "major",
    "areas",
    "work",
    "linguistic",
    "computer",
    "science",
    "chances",
    "point",
    "going",
    "work",
    "either",
    "text",
    "audio",
    "video",
    "necessary",
    "control",
    "libraries",
    "like",
    "gents",
    "mm",
    "ltk",
    "techniques",
    "like",
    "wet",
    "sentimental",
    "analysis",
    "text",
    "summarization",
    "voice",
    "audio",
    "analysis",
    "involves",
    "extracting",
    "useful",
    "information",
    "signals",
    "well",
    "versed",
    "maths",
    "concept",
    "like",
    "fourier",
    "transformation",
    "get",
    "far",
    "one",
    "technical",
    "skills",
    "required",
    "assured",
    "lot",
    "non",
    "technical",
    "skills",
    "also",
    "required",
    "land",
    "good",
    "job",
    "machine",
    "learning",
    "industry",
    "first",
    "need",
    "industry",
    "knowledge",
    "successful",
    "machine",
    "learning",
    "projects",
    "going",
    "address",
    "real",
    "pain",
    "points",
    "agree",
    "whichever",
    "industry",
    "working",
    "know",
    "industry",
    "works",
    "beneficial",
    "industry",
    "machine",
    "learning",
    "engineer",
    "business",
    "acumen",
    "elements",
    "make",
    "successful",
    "business",
    "model",
    "technical",
    "skills",
    "channeled",
    "productively",
    "wo",
    "able",
    "discern",
    "problems",
    "potential",
    "challenges",
    "need",
    "solving",
    "business",
    "sustain",
    "grow",
    "next",
    "list",
    "effective",
    "communication",
    "one",
    "important",
    "parts",
    "job",
    "requirements",
    "need",
    "machine",
    "learning",
    "concepts",
    "people",
    "little",
    "expertise",
    "field",
    "chances",
    "need",
    "work",
    "team",
    "engineers",
    "well",
    "many",
    "teams",
    "like",
    "marketing",
    "sales",
    "team",
    "communication",
    "going",
    "make",
    "much",
    "easier",
    "companies",
    "searching",
    "strong",
    "machine",
    "learning",
    "engineer",
    "looking",
    "someone",
    "clearly",
    "fluency",
    "translate",
    "technical",
    "findings",
    "non",
    "technical",
    "team",
    "rapid",
    "prototyping",
    "another",
    "skill",
    "much",
    "required",
    "machine",
    "learning",
    "engineer",
    "iterating",
    "ideas",
    "quickly",
    "possible",
    "mandatory",
    "finding",
    "one",
    "works",
    "machine",
    "learning",
    "applies",
    "everything",
    "picking",
    "right",
    "model",
    "working",
    "projects",
    "testing",
    "much",
    "need",
    "group",
    "techniques",
    "used",
    "quickly",
    "fabricate",
    "scale",
    "model",
    "physical",
    "part",
    "assembly",
    "using",
    "computer",
    "aided",
    "design",
    "cat",
    "data",
    "coming",
    "final",
    "skills",
    "required",
    "machine",
    "learning",
    "agenda",
    "keep",
    "updated",
    "must",
    "stay",
    "date",
    "upcoming",
    "changes",
    "every",
    "month",
    "new",
    "neural",
    "network",
    "models",
    "come",
    "outperformed",
    "previous",
    "architecture",
    "also",
    "means",
    "aware",
    "news",
    "regarding",
    "development",
    "tools",
    "theory",
    "algorithms",
    "research",
    "papers",
    "blocks",
    "conference",
    "videos",
    "much",
    "another",
    "part",
    "machine",
    "learning",
    "engineer",
    "resume",
    "education",
    "qualification",
    "bachelor",
    "master",
    "degree",
    "computer",
    "science",
    "rit",
    "economics",
    "statistics",
    "even",
    "mathematics",
    "help",
    "land",
    "job",
    "machine",
    "learning",
    "plus",
    "experienced",
    "machine",
    "learning",
    "engineer",
    "probably",
    "standard",
    "company",
    "certifications",
    "help",
    "lot",
    "landing",
    "good",
    "job",
    "machine",
    "learning",
    "finally",
    "coming",
    "professional",
    "experience",
    "need",
    "experience",
    "computer",
    "science",
    "statistics",
    "data",
    "switching",
    "profession",
    "machine",
    "learning",
    "engineer",
    "previous",
    "experience",
    "machine",
    "learning",
    "well",
    "finally",
    "talk",
    "projects",
    "need",
    "project",
    "worked",
    "need",
    "working",
    "machine",
    "learning",
    "related",
    "projects",
    "involve",
    "certain",
    "level",
    "ai",
    "working",
    "neural",
    "networks",
    "certain",
    "degree",
    "land",
    "good",
    "job",
    "engineer",
    "look",
    "company",
    "hiring",
    "machine",
    "learning",
    "engineers",
    "every",
    "company",
    "looking",
    "machine",
    "learning",
    "engineers",
    "modify",
    "existing",
    "model",
    "something",
    "need",
    "much",
    "maintenance",
    "cancel",
    "sustain",
    "basically",
    "working",
    "artificial",
    "intelligence",
    "new",
    "algorithms",
    "work",
    "every",
    "company",
    "deserves",
    "amazon",
    "facebook",
    "tech",
    "giants",
    "like",
    "microsoft",
    "ibm",
    "gaming",
    "industry",
    "gpu",
    "industry",
    "graphics",
    "industry",
    "nvidia",
    "banking",
    "industry",
    "jpmorgan",
    "chase",
    "linkedin",
    "also",
    "walmart",
    "companies",
    "require",
    "machine",
    "learning",
    "engine",
    "part",
    "time",
    "assured",
    "looking",
    "machine",
    "learning",
    "engineer",
    "post",
    "every",
    "companies",
    "big",
    "shot",
    "company",
    "even",
    "new",
    "startups",
    "looking",
    "machine",
    "learning",
    "engineers",
    "assured",
    "get",
    "job",
    "come",
    "end",
    "video",
    "hope",
    "got",
    "good",
    "understanding",
    "exactly",
    "machine",
    "learning",
    "engineer",
    "way",
    "job",
    "trends",
    "salary",
    "trends",
    "skills",
    "required",
    "become",
    "machine",
    "learning",
    "engineer",
    "become",
    "engineer",
    "roles",
    "responsibilities",
    "job",
    "description",
    "appears",
    "resume",
    "job",
    "description",
    "appears",
    "job",
    "application",
    "machine",
    "learning",
    "engineers",
    "also",
    "hope",
    "got",
    "know",
    "prepare",
    "resume",
    "prepare",
    "correct",
    "format",
    "keep",
    "resume",
    "career",
    "objectives",
    "skills",
    "technical",
    "previous",
    "experience",
    "education",
    "qualification",
    "certain",
    "projects",
    "related",
    "guys",
    "ed",
    "rica",
    "know",
    "provides",
    "machine",
    "learning",
    "engineer",
    "master",
    "program",
    "aligned",
    "way",
    "get",
    "acquainted",
    "skills",
    "required",
    "become",
    "machine",
    "learning",
    "engine",
    "correct",
    "form"
  ],
  "keywords": [
    "sure",
    "machine",
    "learning",
    "one",
    "today",
    "market",
    "right",
    "would",
    "least",
    "new",
    "application",
    "project",
    "going",
    "team",
    "expected",
    "generate",
    "around",
    "three",
    "point",
    "nine",
    "looking",
    "world",
    "guys",
    "come",
    "actually",
    "let",
    "entire",
    "well",
    "way",
    "get",
    "start",
    "level",
    "move",
    "topic",
    "without",
    "action",
    "six",
    "different",
    "module",
    "first",
    "discuss",
    "things",
    "like",
    "exactly",
    "artificial",
    "intelligence",
    "various",
    "types",
    "space",
    "finally",
    "end",
    "basic",
    "demo",
    "python",
    "okay",
    "second",
    "starts",
    "probability",
    "descriptive",
    "statistics",
    "inferential",
    "theory",
    "third",
    "unsupervised",
    "supervised",
    "type",
    "mainly",
    "regression",
    "classification",
    "problem",
    "label",
    "data",
    "sets",
    "algorithm",
    "part",
    "linear",
    "logistic",
    "random",
    "forest",
    "decision",
    "tree",
    "reinforcement",
    "depth",
    "also",
    "q",
    "make",
    "industry",
    "projects",
    "based",
    "tell",
    "skills",
    "need",
    "become",
    "important",
    "questions",
    "asked",
    "fine",
    "ahead",
    "press",
    "us",
    "session",
    "know",
    "machines",
    "past",
    "experience",
    "since",
    "years",
    "hand",
    "rewards",
    "learn",
    "comes",
    "picture",
    "many",
    "see",
    "examples",
    "implementation",
    "car",
    "apple",
    "system",
    "decisions",
    "predictions",
    "case",
    "computer",
    "rather",
    "certain",
    "task",
    "time",
    "confusion",
    "people",
    "think",
    "ai",
    "deep",
    "concept",
    "able",
    "tasks",
    "anything",
    "test",
    "whether",
    "human",
    "talking",
    "answer",
    "already",
    "close",
    "coming",
    "said",
    "subset",
    "current",
    "idea",
    "give",
    "done",
    "pattern",
    "set",
    "means",
    "find",
    "rules",
    "optimal",
    "changes",
    "algorithms",
    "involved",
    "known",
    "even",
    "science",
    "scale",
    "similar",
    "used",
    "train",
    "neural",
    "network",
    "better",
    "accuracy",
    "cases",
    "performing",
    "hope",
    "moving",
    "general",
    "work",
    "using",
    "training",
    "model",
    "input",
    "prediction",
    "example",
    "factor",
    "steps",
    "field",
    "banking",
    "retail",
    "worry",
    "use",
    "enough",
    "understanding",
    "starting",
    "mathematical",
    "definition",
    "variables",
    "x",
    "output",
    "variable",
    "mapping",
    "function",
    "goal",
    "whenever",
    "could",
    "predict",
    "confusing",
    "method",
    "instances",
    "attribute",
    "attributes",
    "image",
    "value",
    "base",
    "row",
    "audio",
    "frequency",
    "instance",
    "values",
    "category",
    "real",
    "continuous",
    "either",
    "correct",
    "seen",
    "screen",
    "keeps",
    "yes",
    "process",
    "final",
    "easily",
    "green",
    "identify",
    "result",
    "another",
    "shows",
    "non",
    "predictive",
    "environment",
    "might",
    "named",
    "called",
    "go",
    "makes",
    "performance",
    "support",
    "vector",
    "information",
    "next",
    "video",
    "working",
    "suppose",
    "call",
    "say",
    "automatically",
    "goes",
    "weather",
    "prior",
    "knowledge",
    "sunny",
    "temperature",
    "high",
    "humidity",
    "higher",
    "kind",
    "parameters",
    "given",
    "iris",
    "yellow",
    "trained",
    "sector",
    "credit",
    "card",
    "building",
    "look",
    "customers",
    "patient",
    "patients",
    "show",
    "best",
    "analyze",
    "product",
    "customer",
    "together",
    "frequent",
    "association",
    "rule",
    "put",
    "distribution",
    "order",
    "simple",
    "terms",
    "approach",
    "instead",
    "ask",
    "clustering",
    "clusters",
    "create",
    "basis",
    "cluster",
    "take",
    "add",
    "labels",
    "knows",
    "meaning",
    "group",
    "left",
    "present",
    "hierarchical",
    "friend",
    "classify",
    "gender",
    "age",
    "belong",
    "match",
    "chance",
    "watch",
    "player",
    "class",
    "multiple",
    "normal",
    "uses",
    "techniques",
    "build",
    "features",
    "section",
    "last",
    "within",
    "specific",
    "two",
    "agent",
    "error",
    "gained",
    "actions",
    "select",
    "gets",
    "policy",
    "made",
    "choosing",
    "till",
    "dog",
    "applied",
    "four",
    "response",
    "created",
    "condition",
    "situation",
    "offer",
    "center",
    "user",
    "sales",
    "reduce",
    "price",
    "term",
    "lot",
    "among",
    "understand",
    "pretty",
    "earlier",
    "small",
    "amount",
    "along",
    "large",
    "implement",
    "percent",
    "nothing",
    "technique",
    "possible",
    "processing",
    "consider",
    "saw",
    "outcome",
    "job",
    "study",
    "parts",
    "language",
    "complex",
    "version",
    "brain",
    "focus",
    "methods",
    "want",
    "tells",
    "weight",
    "person",
    "thing",
    "collect",
    "looks",
    "graph",
    "draw",
    "line",
    "sample",
    "equal",
    "minus",
    "hundred",
    "help",
    "main",
    "difference",
    "estimated",
    "actual",
    "try",
    "straight",
    "points",
    "creating",
    "lines",
    "predicted",
    "got",
    "clear",
    "engine",
    "getting",
    "particular",
    "simply",
    "takes",
    "size",
    "inside",
    "works",
    "box",
    "explain",
    "square",
    "check",
    "figure",
    "everything",
    "concepts",
    "define",
    "step",
    "feature",
    "compare",
    "guess",
    "parameter",
    "perform",
    "dependent",
    "low",
    "include",
    "matrix",
    "per",
    "depends",
    "every",
    "face",
    "solve",
    "object",
    "detection",
    "divide",
    "svm",
    "pass",
    "location",
    "name",
    "whereas",
    "much",
    "less",
    "testing",
    "run",
    "knn",
    "fact",
    "still",
    "times",
    "gives",
    "quite",
    "score",
    "node",
    "easy",
    "behind",
    "therefore",
    "days",
    "notebook",
    "results",
    "jupiter",
    "record",
    "analysis",
    "provide",
    "split",
    "available",
    "code",
    "programming",
    "languages",
    "jupyter",
    "ways",
    "anaconda",
    "dot",
    "something",
    "orange",
    "file",
    "number",
    "format",
    "list",
    "top",
    "button",
    "home",
    "click",
    "back",
    "files",
    "running",
    "tab",
    "notebooks",
    "numbers",
    "display",
    "may",
    "drop",
    "similarly",
    "text",
    "option",
    "open",
    "remember",
    "note",
    "3",
    "following",
    "id",
    "really",
    "talk",
    "item",
    "selected",
    "change",
    "count",
    "nodes",
    "zero",
    "seven",
    "csv",
    "items",
    "upon",
    "coding",
    "execute",
    "always",
    "cell",
    "share",
    "away",
    "basically",
    "implemented",
    "across",
    "area",
    "accurate",
    "mentioned",
    "assigned",
    "codes",
    "b",
    "equals",
    "k",
    "color",
    "2",
    "pandas",
    "read",
    "standard",
    "calculate",
    "sklearn",
    "import",
    "library",
    "contains",
    "underscore",
    "load",
    "write",
    "rows",
    "comma",
    "length",
    "play",
    "minimum",
    "mean",
    "five",
    "eight",
    "lies",
    "male",
    "spam",
    "question",
    "decide",
    "choose",
    "explore",
    "categories",
    "female",
    "observation",
    "problems",
    "business",
    "transaction",
    "buys",
    "dividing",
    "population",
    "groups",
    "words",
    "head",
    "definitely",
    "10",
    "rain",
    "tomorrow",
    "relationship",
    "pick",
    "confidence",
    "far",
    "consists",
    "columns",
    "column",
    "good",
    "numeric",
    "rose",
    "fit",
    "hit",
    "libraries",
    "copy",
    "6",
    "4",
    "solution",
    "reach",
    "defining",
    "alright",
    "forward",
    "else",
    "print",
    "total",
    "little",
    "30",
    "0",
    "summary",
    "maximum",
    "range",
    "8",
    "deviation",
    "50",
    "plot",
    "individual",
    "wo",
    "false",
    "single",
    "true",
    "evaluate",
    "estimate",
    "validation",
    "statistical",
    "consist",
    "c",
    "r",
    "state",
    "keep",
    "ratio",
    "divided",
    "percentage",
    "neighbor",
    "trees",
    "directly",
    "turn",
    "estimation",
    "independent",
    "form",
    "terminologies",
    "sampling",
    "measures",
    "spread",
    "gain",
    "entropy",
    "conditional",
    "bayes",
    "theorem",
    "mention",
    "description",
    "interval",
    "margin",
    "hypothesis",
    "sum",
    "sort",
    "helps",
    "models",
    "major",
    "discrete",
    "average",
    "measure",
    "categorical",
    "table",
    "path",
    "company",
    "common",
    "game",
    "blue",
    "trying",
    "events",
    "represents",
    "chosen",
    "must",
    "median",
    "mode",
    "variance",
    "scenario",
    "obviously",
    "taking",
    "finding",
    "discussed",
    "ball",
    "samples",
    "randomly",
    "taken",
    "central",
    "cars",
    "cards",
    "calculated",
    "often",
    "sometimes",
    "formula",
    "red",
    "highest",
    "1",
    "n",
    "bar",
    "calculating",
    "squared",
    "root",
    "uncertainty",
    "classes",
    "event",
    "whole",
    "v",
    "subsets",
    "statement",
    "outlook",
    "wind",
    "day",
    "branch",
    "denotes",
    "14",
    "overcast",
    "making",
    "rainy",
    "pure",
    "baby",
    "impurity",
    "5",
    "windy",
    "classifier",
    "positive",
    "negative",
    "stands",
    "disease",
    "60",
    "outcomes",
    "king",
    "limit",
    "likelihood",
    "curve",
    "gaussian",
    "states",
    "almost",
    "previous",
    "p",
    "salary",
    "package",
    "undergone",
    "room",
    "bias",
    "equation",
    "balls",
    "picking",
    "probabilities",
    "risk",
    "purchase",
    "distance",
    "null",
    "john",
    "threshold",
    "map",
    "chances",
    "e",
    "marketing",
    "thousand",
    "dollars",
    "trends",
    "plus",
    "quality",
    "missing",
    "remove",
    "big",
    "axis",
    "summation",
    "passing",
    "closer",
    "return",
    "infinity",
    "transform",
    "log",
    "apply",
    "l",
    "medical",
    "history",
    "titanic",
    "survive",
    "suv",
    "passengers",
    "survived",
    "index",
    "passenger",
    "required",
    "traveling",
    "h",
    "edge",
    "place",
    "defined",
    "belongs",
    "buy",
    "kn",
    "nearest",
    "greater",
    "leaf",
    "transactions",
    "probably",
    "friends",
    "neighbors",
    "email",
    "closest",
    "euclidean",
    "news",
    "vectors",
    "hyperplane",
    "centroid",
    "engineers",
    "discount",
    "lift",
    "reward",
    "meat",
    "tiger",
    "gamma",
    "robot",
    "engineer",
    "networks",
    "technical"
  ]
}