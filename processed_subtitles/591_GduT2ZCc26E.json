{
  "text": "hey guys it's Greg welcome back today we\nhave an awesome video pretty much every\nfeature engineering technique for\ntraining machine learning models I'll\nexplain how to do that in Python code we\nare doing this in a co-op notebook so\ntake a look in the video description if\nyou want to follow along with that but\neither way we've got one dimensionality\nreduction using PCA two pre-processing\nand scaling methods three categorical\nencoding so dummy and one hot variables\nfour we've got bidding grouping\naggregating features together five\nclustering so here's the map of\nCalifornia for example we can cluster\nthe latitude and longitude and get this\nkind of pretty picture out of it it\ndoesn't have to be that geographical\ndata but that makes the most sense\nvisually and finally we've got feature\nselection combining all of this\ndifferent stuff together and only taking\nwhat you want to okay if you only want\nto learn one or a particular subset of\nthese ideas I do have marked in the\nchapters Down Below on all these\ndifferent uh topics so just go and learn\nwhat it is that you want to learn or if\nyou want to follow everything along with\nme I've got it here at the start with\ninitial setup but before you go off and\ndo your own thing with this video I want\nto point out that we are using the\nCalifornia Housing data set that is in\nthe link in the description down below\nso that's the particular Kangle data set\nwhether you're doing it like this or you\nclick that link and download it it's the\nsame data set but the point is we're not\nusing this one that is in Sample data\nCalifornia Housing over here we actually\nhave to download it straight from kagle\nlike this okay so with that being said\num I put a lot of effort into this video\nto code everything up and explain it so\nif you don't mind dropping a like in\nthis video video I'd really appreciate\nit I know this is going to help a lot of\ndifferent people and that would be great\nif you could support me and others that\nway H cuz we're all learning together\nand if you're not subscribed to the\nchannel this is a really good time to do\nthat but with that being said let's get\nstarted with our initial setup so\nfirstly we have to grab that California\nHousing data set there's two ways to do\nit you go to the video description click\nthat link download the zip folder and\nthen you can actually go ahead and\nupload that zip folder up here or what I\ndo is I use my kagle API and so I go to\nmy kle profile generate API token it\ndownloads this k. Json thing and then\nyou can upload that or put it in the\nworking directory of all the stuff that\nyou're doing whatever environment that\nyou are using and then you can just run\nthis block of code here in collab and\nit'll do all of the downloading and UNS\nit for you so I am going to do that it\ncomplains at me because I've already\ndone it before but I can go over here\nand click yes just to kind of redo this\nstuff it inflates or whatever housing.\nCSV so the important part is we've got\nthis California housing data set right\nthere now now that we've got that we can\nreaden the CSV into a pandas data frame\nso import pandas aspd we'll make this\nvariable DF equal to the read pandas\nread CSV of housing. CSV we're going to\nimmediately drop any rows that have any\nnull values that's what this part does\nand then we can just call DF to Output\nthat we see we've got\n20433 rows by 10 columns just to explain\nthe data set if you don't AR aren't\nalready familiar with it if you are skip\nover over the section for sure but\nbasically each of these rows are a\nparticular area in California and we\nhave a bunch of different information\nfor each of those areas so for each of\nthese different areas we've got the\nlongitude the latitude the housing\nmedian age so of all the houses in that\narea we can uh get all their ages get\nthe median of that and so that happens\nto be 41 for this one uh we can sum up\nthe total number of rooms so 880 total\nbedrooms is 129 population household the\nincome uh the median income in that area\nand then finally up and two we have\nmedian house value which is generally\nwhat this data set is used to predict we\ngenerally use a bunch of these different\nvariables to try and learn a function\nthat Maps these variables into the\npredicted median house value okay so\nwe'll do that for everything that we do\nin this video we're also going to use\nthis is why we got the extra data set\nthe different data set here is it has\nthis ocean proximity this categorical\nvariable here which we will do in the uh\nthe categorical encoding section okay so\nwe have our DF here the first thing that\nwe're going to do since we're training a\nmachine learning model or many of them\nthat are going to learn uh basically a\ntransformation of these features into\ntrying to learn the median house value\nfrom these features well we're going to\nsplit into a train and test data frame\nso that we can train our machine\nlearning models on that dat on that data\nframe and then we can test in uh our\nmodels on our test data frame as well\nokay so with this what this does here DF\nis equal to DF do sample set Frack equal\nto one what this does here is basically\nit shuffles the rows here and we set\nrandom state to two that's uh that's if\nyou want to reproduce the same results\nyou should use the same uh random seed\nvalue here or if you want a different\nresults you could change the value there\nokay and then what I do is just make\ntrain and test the first uh train is\ngoing to be the first 177,000 rows\nbecause remember it has a the the data\nframe itself has 20,00 433 rows so I can\ndo a decent split of roughly like 80 80%\nin the train and maybe 20% or so in the\ntest data frame I get the first 17,000\nin train I have to do this reset index\nthing don't worry too much about that it\njust kind of it just makes it so that\nthis index is still correct and then\nwhat we do is get test DF is 17,000\nonward and then we can reset that's\nindex as well so we got our train data\nframe is all the columns and then the\nfirst 177,000 rows and then test data\nframe is all of this stuff and the the\nrandomization randomly split um the the\ntrain and test information okay so now\nwhat we're going to do all our machine\nmodels are going to be about using the\nmedian house value to predict that and\nso the training the training output\nbasically we say we use y for output y\ntrain is just going to be that\nparticular column so we get the column\nthis actually returns a panda series and\nso we can do train DF sub the median\nhouse value we convert that to a numpy\narray which is just going to be a\nflattened all 177,000 values and we do\nthat for both y train and Y test and we\ncan output their numpy shapes okay so\nthese we are going to use very heavily\nthese are all of our observed or values\nor outputs we have 177,000 uh observed\noutputs for the train and 3,433 for the\ntest so the first thing we'll do here is\ncreate something called a baseline model\nand we're going to do that by making it\njust the average median house value in\nin the train data frame okay so the\npoint of what we're generally trying to\ndo here is learn just some sort of\nmapping between these inputs and this uh\nthis output over here the median house\nvalue we're trying to predict that and\nso usually we'll try and learn from\nthese features and use those but we\ndon't necessarily have to we can\nactually just make a baseline model so a\nvery simple model for terms of\ncomparison uh where we just calculate\nthe average of everything so this is the\ntrain data frame if we calculate the\naverage of all of these median house\nvalues will be left with some number and\nit's it's not a very good prediction\nbecause obviously we should be using\nsome of the features that we have\navailable to us like longitude and\nmagnitude but we don't have to so we'll\nmake our first Baseline model which is\nsimply the average median house value in\nthe train data frame and for all of our\nmodels we're going to be evaluating them\nbased off of the error metric the mean\nabsolute error and we'll import that\nfrom sklearn or pyit learnmetrics\nokay so we get the average median house\nvalue in Train by this is a panda Series\nso we take the train data frame sub\nmedian house value and that has a panda\nseries have an object called or a\nfunction called mean and so we can call\nthat to get some value here and then we\ncan get the test predictions which is\njust all of our predictions for the test\nset and that's what we're always trying\nto do here when we we train models off\nof the train data frame that's what we\ndid by taking the average and then we\ncompare our prediction one by one to all\nthese different actual test values okay\nso the first prediction we are trying to\nguess this that was our best possible uh\nbut what we instead predicted is\nwhatever this average actually is and\njust to show what this average is uh\nvery quickly we can do that with this so\nevery time we're predicting simply\n20,000 uh sorry\n27,1 189 as is the house value and we\ncan see that's wrong in many places it's\nactually pretty close to this value but\nit's far off the majority for sure but\nwhat we do is we we make the list we\njust get the list of that average value\nand then we multiply that by the length\nof the test data frame so that we're\ngoing to have and we know that's sorry\nabout that we know that it's going to\nhave 3,433 predictions and so it's\nreally just the average value uh 3,433\ntimes and so that'll be our predictions\nfor everything we get the mean absolute\nerror of those test predictions right\nright there and Y test so we're\ncomparing one by one the difference\nbetween the Baseline model which is just\none value over and over again versus the\nactual y test which is again just these\nvalues here in the test data frame and\nwe come up with some number which as we\ncan see is not defined clearly I'm not\nrunning this stuff as much as I have to\nrun that run that and we get\n90,9 52 it means on average we off by\nthe absolute value of $90,000 which is\nnot terrible I mean it's definitely not\na very good model uh but we want to do a\nlot of different methods to improve upon\nthat we're going to start with\ndimensionality reduction in PCA and by\nthe way the point of this video is not\nnecessarily to actually get uh better\nvalues than this although we definitely\nwill I'm not trying to show you how to\nmake the best machine learning model on\nthis particular data set I'm showing you\nall the different techniques so that you\ncan do what you think is right and you\nknow all the different techniques\navailable so you can try and get the\nbest model for whatever your problem is\nokay so firstly we're going to start\nwith dimensionality reduction with PCA\nobserve the correlation between total\nrooms total bedrooms and households if\nwe get to the train DF all of these\ncolumns so total rooms total bedrooms\nand households were left with just a\nsmaller data frame and then we can get\nthe correlation between those where the\ncorrelation is always going to be a\nvalue between negative one and one these\nare actually all positives and this\nmakes sense because if it is a positive\nvalue that means for example as the\nnumber of total rooms increases the\nnumber of total bedrooms tends to\nincrease as well so if we look at total\nrooms with total bedrooms we get 9\n31023 and that value is the same as that\none over here because it's total\nbedrooms and total rooms total bedrooms\nand total rooms it's the same thing so\nwhat we're seeing here is there's a very\nhigh positive correlation it's almost\nit's very close to this perfect linear\npattern of when the total bedroom is\ngoing to increase the total rooms\nincreases as well why am I telling you\nthis well that's because maybe we don't\nactually need all of these different\nvariables maybe we don't need total\nrooms and total bedrooms and households\nto try and predict uh the the median\nhouse value because there's so much\nrelationship and information captured in\nthem already so one option here and of\ncourse you could just simply use one of\nthese different features um you could\njust only use total rooms for example\nbut something that's most likely a\nlittle bit better is to do something\ncalled PCA which we'll look at very\nshortly so what we're going to do first\nhere is set up our Matrix to make a\nmachine learning model of our input with\njust total rooms total bedrooms and\nhouseholds as inputs so without PCA at\nall for now we import numpy as NP and\nwe'll make xtrain 3 equal to that uh\nthat same data frame there and the the\ntrain so that's just those three columns\nand then we convert that to numpy and we\nwhat we're left over with is a 17 ,000\nrow by three column nump array that\nmakes sense now we do that exact same\nthing uh for X tests and I just put a\nthree on it to remind you that it's with\nthe with these three variables and so we\nget our test information as well now we\nhave an input Matrix where it's the\nnumber of examples by the number of\ncolumns the number of examples by the\nnumber of columns so 177,000 in the\ntrain and 3,433 in the test what we're\ngoing to do now is get an error of a\nrandom forest model and it doesn't\nmatter if you know much about ROM force\nor not uh but on just XT train 3 y train\nand then testing that on X test 3 and Y\ntest okay so that's a lot of words but\nbasically we're going to uh import the\nrain and Forest regression model it's\ngoing to we're going to be doing\nregression that's a it's it's predicting\nthe median house value so it's\npredicting continuous value that's what\nregression means we get the forest base\nI'm just writing Bas as in a base model\nis equal to the random Forest regressor\nwhere we'll just make these variabl for\nfun the N estimators is equal to 50 Max\ndeps is five don't worry about don't\nworry about that stuff if you don't\nunderstand it and then we fit it with\nxtrain 3 so we feed it in our input\nMatrix uh for Te for train sorry we\ntrain it uh we feed it in the\ncorresponding inputs in the training and\nthen we can then now that we have a\nmodel okay so we have our forest base as\na model trained one and we can get\nForest based test prediction so the same\nthing we are making for the average\nmodel where we just getting\nuh instead of the the average just every\nsingle time we're actually going to be\nusing a random Forest to make a\nprediction now and so we feeded in our X\ntest information we feed it in our input\nand then we can get the mean absolute V\nvalue or mean absolute error on y test\ncomparing that to the forest models\nbased predictions okay so if we run that\nwe're going to see it's not defined I\nkeep forgetting to run stuff X test and\nthen X train 3 is now to find I guess\nI'll run that\none funny okay so we run that model and\nthen we get\n81,9 196 so on average our our model was\noff by about\n$882,000 instead of $90,000 like our\naverage Baseline model was so clearly we\nlearned 91,000 actually where that was\noff okay so our random Forest did a lot\nmuch better obviously but now let's\nactually do a PCA and see if this helps\nfor this remember for this particular\ndata set and this particular model that\nwe happen to use does it help well we\ncan train PCA which is just a linear uh\ndimensionality reduction technique you\ncan also do something called\nautoencoders and some other weird ones\nif you want to look at them uh so PCA is\na linear dimensionality reduction\ntechnique from SK learn. decomposition\nwe'll import PCA and we'll train a PCA\nwhere we set it's number of components\nequal to two so that means uh whatever\nyou feed me in here I'm I'm feeding you\nin uh three different columns that's why\nI call it xtrain three here and we're\ngetting back just two columns which is\nsupposedly just better information more\nconcise information into two columns we\nthen we don't need these three columns\nwe can just get it in two two better\nones so we can use PCA to transform\nxtrain 3 and X test 3 and xtrain PCA and\nX test PCA and so as we see if we call\nthe pca. transform on those inputs we\nwill be left over with 17,000 X3 but\n17,2 and 3,433 by 2 instead let's see if\nthat does any better this random Forest\nis doing the exact same thing as before\nwe'll get a random forest and then we'll\nfit it with extrem PCA instead on the\nPCA data feed in that y train Forest PCA\ntest predictions is equal to Forest pca.\npredict we feed it in that X test PCA\nand back we get we happen to have and I\nalways forget what I'm actually running\nhere but we get the absolute error on\nthis model is 79 something okay so the\nother one was 82 for this particular\nmodel and maybe it would change things\nif we change this value or this to a\nlinear regression or whatever but the\nPCA did happen to help for this model so\nthat is one particular way of doing uh\nfeature engineering techniques is with\ndimensionality reduction that's how to\ndo it with PCA in particular okay I\nquickly added in xra three here to show\nyou what happens if we pre-process or\nscale it beforehand from SK to\npre-processing will import standard\nscaler normalizer and minmax scaler\nthere're three different tools to do\nthat and we'll also get our graphing\nLibrary here if we make a scaler equal\nto for example the standard scaler and\nwe fit that with xtrain 3 we we fit it\nfeed it in this we'll get xtrain 3\nscaled by transforming using that scaler\nto transform xtrain 3 we transform this\nthing and then we get this thing out\ncalled Xtreme 3 scaled and it's it looks\nexactly the same except just these the\nvalues look different and in particular\nwe see two things well one they they can\nbe negative now and that's just that\nthat happens to happen from standard\nscale or these ones might not do that um\nbut in general they are all every single\nvalue here is pretty much on a closer\nrange to each other and what I mean by\nthat is these are all in the thousands\nas we see and these are all in the\nhundreds as we see that is a big problem\nfor many machine learning models not all\nof them some SK learned stuff might not\neven care but any deep learning\nalgorithm is going to really care that\nthese values are just so much bigger\nthan these ones and that's because you\nknow if we change from 2,700 to 1200\nthat's a 1500 value drop that's huge\ncompared to 574 versus 214 we only drop\nthis by 3 350 or so 360 so why do we\ncare that this change is so much bigger\nthan that one well that just says that\nthis feature this this column is more\nimportant than this other column and if\nwe really wanted to do that maybe we\ncould keep it but in general we assume\nin setup that all features are pretty\nmuch equally important and so to do that\nwe have to do for example this standard\nscaler thing where now all these all\nthese values here are between -3 and 3\nor so these are all between 3 and 3 or\nso why am I saying three and not like 7\nand negative. 7 that's because we are\ndoing what standard scaler is is uh\nperforming a standardization thing which\nmeans we are subtracting we calculate\nthe mean of this column and we calculate\nstandard deviation of this column we\ntake this value and we subtract it by\nits mean divide by standard deviation we\ntake this value we subtract it by its\nmean standard de divide by standard\ndeviation and we do that for all of the\nvalues in that column we do it for this\ncolumn where we take this we calculate\nthe mean for the column and then we\nsubtract by the standard deviation of\nthe column we do that for all of those\nvalues and then they will all be roughly\nin if we were to graph it this is just\nthe First Column after the\ntransformation these are between as we\ncan see -3 and three or so almost\neverything is that and if we were to do\nsay look at the first uh second column\nactually they again they moved a little\nbit more to the negatives but most of\nthem are between -3 and 3 not everything\nbut most are and so that makes it a lot\neasier for these machine learning\nalgorithms to figure out what the heck\nis going on it treats these treats these\nColum equally basically okay so we can\nuse standard scaler uh that that does\nthis particular transformation we don't\ntechnically need between ne3 and three\nor so but we could do normalizer instead\nwhich for this particular data set uh\nwe'll see this is just looking at the\nFirst Column uh this makes the values\nbetween zero and one we might get\ndifferent Behavior if our values were\nallowed to be negative in the first\nplace because note that all these are\npositive uh or we could do a mmax scaler\nand I'm not really going to show you\nuh the different ones like all the\ndifferences between these uh that's what\nSK learn is for and so I have this over\nhere if you want to read the\ndocumentation on why you might want to\nuse one over the other and what exactly\nthey're doing uh then you can read about\nit there or I might make a future video\non that um but the idea is that most\nvalues you are going to be uh\nstandardizing and uh if you happen to\nknow that there's already a maximum out\nthere so for pictur\nfor example for picture data all the\nvalues are going to be between 0 and 255\nand so your pre-processing could just be\ndivide everything by\n255 note that in that particular example\nI just said uh all the values are\nbetween 0 and 255 they're all already on\nthe same sort of range why are we\ndividing by anything why aren't we just\nhappy with the values in general nural\nnetworks are still and many machine\nlearning Al algorithms are still happier\nwith values between zero and one or so\nand negative3 and three or so or\nsomething small like that it's just\neasier for a lot of things okay so those\nare our different options uh each of\nthese produce a different uh algorithm\nand you can produce a different output\nyou can look here I'll include this in\nthe do in the description down below if\nyou want to see which one you want to\nuse but that's the idea you just kind of\ntry them out and see what works best\noverall okay it's important to note that\nI did actually rerun The Notebook above\nso if you see slightly different numbers\nthat would be why um but anyway for now\nnote that we were doing the exact same\ntransformation at least the same\nTransformer object on every column that\nwe were using we picked standard scaler\nand here we fed in the whole xtrain 3\nhere there is times when you might want\nto pick um you know maybe du only the\nFirst Column a standard scal on that one\nmaybe a normalizer on the second column\nyou can feel free to play around with\nthat it takes a little bit of\nmanipulation and numpy to to kind of get\nthose arrays separated and then joined\nback again later I just wanted to let\nyou know that's an option that you can\ndo and sometimes we we apply different\ntransformations to different columns\nlike that but for now we're going to do\na standard scaler on xtrain 3 and we do\nthat with we'll get X test 3 scaled as\nwell so that we have X train down here\nwe have XT train scaled and X test\nscaled I don't know why I didn't include\nthe three on that but I guess that's\nokay and then we make a random forest\nwith the scaled information to be that\nsame random Forest we fit in XT train 3\nscaled uh and Y train we get the test\npredictions for that model with that\nrandom Forest there do predict with the\nX test 3 scaled and then we check the\nabsolute error comparative y test and\nthose predictions which gives us\n82270 uh for this example okay and so if\nwe look up above and compare this to\nexactly the same model but without any\nSt scaling being done this is 824 that\none the other one is uh 82 to okay so\nthey're very very similar it turns out\nthat just this random forest model\ndoesn't really care too much about the\nscaling but I promise you neural\nnetworks care a lot so we might want to\ndo that for those and some other machine\nlearning models as well now it turns out\nthat that PCA thing we did earlier\nactually requires uh at least it prefers\nsome sort of standard scale or some sort\nof scaling operation like that before\nthe data is fed in before we just fed it\nin the Raw values uh now we want to\nactually scale the data then do PCA then\ntrain a random Forest on that and now\nthat we have three steps here it's\nprobably the best uh best choice to\nactually use this psychic learn pipeline\nobject from SK learn. pipeline import\npipeline where we'll make a scale then\nPCA then pipe uh then Forest pipeline so\nwe do standardization then PCA then\nrandom Forest to be the pipeline with\nstandard scaler and then PCA with number\nof components of two and then a random\nforest regressor with that same thing as\nbefore note how we're passing in these\nobjects which is just like bracket\nbracket here we're not fitting we're not\npassing any sort of inputs this is just\ndefining the pipeline of what can be\ntrained and then and then predicted with\nlater so we can fit it so we get this\nscale PCA pipe forest and we call fit on\nthat with it still before X train 3 and\nY train okay so we feed in those RW\ninputs what it does is it learns this\nstandard scaler okay it knows what to do\nuh to it it learned its parameters so\nthat it can perform that transformation\nthen we did a PCA so it can perform that\nPCA transformation with that data\nactually being scaled because when we\ncall fit what it does is it fits this\nobject and then it passes it in to this\nobject which it then fits that and then\nit passes into this object which then\nfits that so it fits all of these in the\nfull pipeline so we call fit and then we\ncan get the predictions with it but now\nthat we fit with just that thing.\npredict with the XS 3 that input matrix\nit's going to do the all those\nTransformations so on predict it's going\nto scale that data it's going to apply\nPCA to that data to make it two columns\nand then we're going to actually apply\nthis previously learned random Forest to\nthat to get the mean absolute error\nwhich totals uh that is not defined\nbecause I haven't run that up there and\nthat is going to get an error of 80\n8,224 which as we look above is uh not\nvery different from the other ones still\nwhat is going on here uh that is just\nbecause standard scaler may not be the\nbest possible object you might want to\nuse normalizer instead so let's try\nnormalizer and see if we get a better\nresult we do this and although this\ncomment is technically wrong right now\nit doesn't really matter now we get 77\nokay that is that is better than all the\nother ones so it turns out that\nnormalizer happens to be a better uh\ntransformation\nto perform on this data okay so I'll\njust change the comment again to that\nnormalization and above to normalization\nas well okay now we're on to something\ncalled categorical encoding which\ngenerally we want to convert this into\ndummy or one hot variables dummy is for\nthe pandas version and one hot is for\nthe numpy version although it's doing\nthe same thing we'll look at that\nshortly there is other ways of encoding\ncategorical variables what this assumes\nis that each different category oral\nvariable for example red green and blue\nare treated entirely differently and\nthat actually makes sense for RGB that's\nwhy we make colors out of RGB because\nthe there are three primary colors but\nthen instead think about more colors if\nwe had yellow and green well they\nactually have similarities to red and\nblue and and the primary colors so what\nI'm trying to say here is that we might\nnot necessarily want to do this dumy\nencoding for everything for example for\nwords words have similarities to each\nother we use something called embeddings\nfor words but we're going to use dummy\nencoding uh for for this because it does\nmake sense we have low dimensionality\nmeaning we have uh what we're looking at\nis if we get the train data frame sub\nocean proximity that's that categorical\ncolumn uh from way above and we look at\nthis column uh it's just it's just that\ncolumn there and then hopefully I can\nfind this again very quickly here we\nhave uh we pd. getet dummies on that\ncolumn and that returns a data frame\nwhen we get the head of that that\nreturns this object right here where it\nis going to have since we called that on\ntrain DF it'll be 177,000 rows and for\neach of these different columns it has\nuh well it has it now has one column for\neach of these different variables that\nwere in that list so all those different\ncolumns are we have one for near Bay we\nhave one for Island and all of the\ndifferent options we have right there\nokay so why do we want this well now we\ncan say hey this row is it's one of\nthese one of these possible things and\nthe one that is it's a near Bay this row\nis one of these many possible things the\none that it is is an inland so we call\nor Inland whatever um so we call this\neither dummies because uh actually I\ndon't know the term but I think it's a\nstatistical thing and uh one hot when\nwe're when we are to convert this thing\nto a numpy array really this is just one\nVector here which is a this is one hot\nthis is hot as in it's the thing that's\nnot cold this is cold this is cold this\nis cold this is cold and this is hot and\nso it's one hot because this is this is\na one okay so that's the idea it's just\na way of decoding to this row that this\nis this is this type of thing and this\nrow is this type of thing and this row\nis this type of thing now importantly uh\nhow many different values do we have for\neach of these different things well\nfirstly before we look at this code If\nYou observe the frequency of categories\nwe can look at the value counts of train\nDF Sub sub ocean proximity that's just\nthat categorical variable of above we\nlook at the value counts which shows how\nmany this this total value should be the\nnumber of rows that we have in the data\nframe and so we have\n7,522 of this one which is an hour less\nthan the ocean or like an hours drive to\nthe ocean or something like that we have\n5,48 inlands we have 2,172 NE I\n1,895 near Bas we only have three\nislands so what we're actually going to\ndo here after we just do a simple\nconcatenate to append these new columns\nto our train data frame so we train DF\nPD duck and cat train DF train dummies\nacross the uh the column AIS AIS is one\nwe combine all of those to get uh\nbasically just the dummies or one hot\nfrom this this variable here so we\nappend that and then since we don't have\nmany isets here we're going to draw drop\nIsland train DF drop Island in place is\ntrue meaning we don't have to do uh we\ndon't have to do train DF is equal to\nthat thing if we do uh in place is true\nwe do it on AIS equals 1 and we train DF\ndo head we output that and we can see it\nis all of these columns except uh we\nremoved Island okay so this uh this data\nframe isn't overly useful as it is uh\nwe'll change that very shortly for now\nwe're just going to do exactly the same\nthing on the test data set so we'll get\nthe test dummies which is the data frame\nof that gets the pd. get dummies of the\ntest DF sub ocean proximity test\ndummies. head gets that data frame there\nand then we append the dummies and drop\nIsland on the test data frame so test\ndummies is H that same thing I don't\nknow why I would need that I definitely\ndon't need that so test DF is equal to\nPD do and Cat of test DF test dummies\nacross axis equals one test do DF drop\nIsland in placees true accesses one test\nDF head I haven't ran a few of these\nthings for a while so I'm actually just\ngoing to go back from the start and\nstart running these\noperations and now that we have um so\nnow that we've concatenated that we have\nthose columns on the train test uh train\nand test data frames so uh we can get\nthis we're just going to try and make a\nmachine learning model based off of only\nthose columns of course in the end we\nwill use some other columns as well but\nfor now we're just going to make a model\nthat only looks at these different\ncategories and see how that does so we\nget X train dummies which is just that\ntrain DF to numpy this part's a little\nbit weird so we get all rows and then we\nwant -4 up until the end well if we\ncount back here we get uh min-1 -2 -3 -4\nwe get this column so it starts at Min\n-4 that column and then we want to go to\nthe end so we say all rows which is all\nof this information but then all only\ncolumns we want is just this chunk here\nso it's just this chunk here uh except\nthis is the head so it's all that\ninformation and so if we were to look at\nthe shape of this thing it is 17,000 by4\nso we did that for the training\ninformation we can do that for the test\nand so we do test DF that's Den by all\nthe rows ne4 up to the end and that's\nthe test dummies. shape we will make a\nlinear regression model just for fun uh\nwe don't have to do that but I'm\nchoosing to do a linear regression on\nthe X train dummies and Y train we make\nthat our linear dummy model we get the\nlinear dummy test predictions which is\nthat model predict with the X test\ndummies and we get the mean absolute\nerror with Y test and linear dummy test\npredictions to show that it is in total\nI don't know why I'm not just running\nthe stuff as I go um 77,000 195 okay so\nthat's actually a pretty good model like\nthat's actually a really good model that\nonly uses uh the a one hot encoding or\ndummy encoding of these variables okay\nso it just use the ocean proximity in\nits fullest capacity to return a pretty\ngood model next up is something called\nbinning and I'm going to use other terms\nfor it like grouping and aggregating\nreally just referring to the same thing\nwhich is if we're to look at train DFS\nup housing median age for example not\nthe median value this is one of our\ninputs we range from about 1 three to\nlike 55 years or so okay so why is this\nimportant well there's no reason that we\nhave to use this column as it is as just\nits values or even just scaling of it uh\nwhat we can do is maybe sort of move\nthings into bins and so if I was to draw\na line right down the middle which is uh\nsay 30 or so okay so if I draw a line\nhere what I can do is basically shove\neverything in here into one big bin and\nsay that it's less than 30 or it's\nbigger than 30 or equivalently bigger\nthan or equal to 30 so we get two big\nbins out of this which is everything is\neither less than 30 or it is greater\nthan 30 if I do train sub train DF sub\nmedian age less than 30 this is adding a\nnew column into the data frame we make\nthat equal to train DF sub housing\nmedian age is less than 30 so it applies\nthat operation uh to every single every\nsingle Row in there and then it returns\na Boolean okay so what this thing what\nthis thing is here is a panda series of\nbooleans where it's either this thing\nthis first one is happens to be less\nthan 30 the second one happens to be\nless than 30 third one is bigger than 30\nokay so we get this Boolean series out\nof it and then we convert that into an\nINT and so that'll make the falses zero\nand the trues uh will be one and then\ntrain DF sub uh uh train DF do head\nafter that we now included this column\nhere so we basically just bin uh so\nthere's no reason we could have just\nmade this a binary variable we could\nhave of course uh made this multiple\nvariables like say between zero and 15\n15 and 30 and 30 and so on then we'd\nhave to do that one hot encoding thing\nagain um that's that's the main idea\nthough is basically we just kind of\ngroup these things into different bins\nand treat them as different ideas now\nhousing median age is bigger than 30 is\nkind of something I made up uh but for\npretend for now that that's actually\nmakes sense that uh some realtor went\nover some really uh famous real realtor\nsaid something stupid like uh any\nanything less than anything older than\n30 is just completely useless like it's\njust so much worse um and so that would\nactually have a strong effect and so\nthis is something reasonable to do to\ngive uh the Zero versus the one value\nvery uh different feelings and it's not\njust about this kind of this gradual\nincrease it's more about the fact that\nit's all either less than 30 or it's all\nbigger than 30 that's why we're doing\nthis it's probably not going to work\nsuper well but it would better for other\nexamples but that's the idea so again uh\nwe could of course combine this with\nother features but for now we're going\nto make a model just out of that\nvariable and see how that does we'll get\nthe X train with just median AG that\nshould be really median age less than 30\nbut that's fine uh so we get that we\nconvert that to numpy and for these SK\nlearn models they always have to have\nthis kind of other column shape like\nnumber of columns so if if we just\ngrabbed this technically it would have\nbeen 177,000 flat we reshape it into\nthis is the same as doing uh 17,000 by 1\nbut I'm going to leave it as negative 1\nin case the number of values uh happens\nto change over time so we do that we\nreshape it and we get 70,000 by 1 which\nis all either zeros uh sorry I must have\nforgotten to do this and then we get\nthis which is all either zeros or ones\nthat same thing as we had before we you\ncan do the exact same thing with test\nwhich uh not really going to show how to\ndo that but it's the exact same\noperation but on the test\ninformation and then we train a model\nwith just that information I'm going to\nstart to gloss over what I'm doing here\nbecause you've seen this a bunch of\ntimes now I'm just whatever training and\ntesting uh data set I'm setting up here\nuh that's the one that I'm using for the\nmodel okay so we do that and then we\nmake a model out of that we're going to\ndo a linear regression just for fun and\nwe can see we got\n90,000 uh\n90.7 which if we compare that to the\nBaseline model at the very top that\nmodel so it's 90.7 compared to 90.9 okay\nso it's ever so slightly a better guess\nthan the average so it's clearly it's\nit's means something to to do that so\nthat is the idea of uh bidding and\ngrouping you could do it into more more\nvariables you could of course combine\nthis with other variables uh but that's\nthat's the idea now we're moving on to\nclustering where first we're going to\nplot a map of California uh I'm very\ncertain we are that is definitely doing\nnothing so I'm going to kill that and\nwe'll do a scatter plot of we'll do the\nlongitude on the X and the latitude on\nthe Y and this is California right there\nit's shaped like that so there's are all\nthe points we can make a k means model\nfrom longitude and latitude and get the\nClusters so what are clusters well\nthey're just groups and so clusters it's\nreally like a label it's saying No this\ngroup is different than that group and\nit's different than that group if we\nfrom K learn. cluster import K means\nthat's just one algorithm for clustering\nand it's important for this because it\nactually has a predict method you can\nlearn a k means and then you can predict\nthat uh on a different data set than you\nlearned from a lot of algorithms uh\nclustering algorithms can't do that um\nbut we get XT Trin lat long which is\njust train DF subat longitude latitude\nthat's numpy we make a k means to be a k\nmeans with the number clusters you know\nI put seven but there's no reason I\ncouldn't do three and then we we fit\nthat with uh the input just the latitude\nand the longitude and we can get the\nlabels K means. labels uncore and so\nthis is saying the first one belong to\nthis group and the second one belong to\nthis group and the third one belong to\nthis group the numbers have no meaning\nother than they are different groups\nthan each other there's it's not like uh\nthree group three is closer to group two\nuh than it is to group zero like it's\njust different values are different\ngroups entirely now let's plot a colored\nmap of California we'll do uh P plot.\nExpress is PX my favorite Library px.\nscatter with the x is train DF sub\nlongitude Y is train DF sub latitude and\nthe color so this is so much easier to\ndo this in plotly rather than map plot\nlip we make color equal to K means.\nlabels under score so what that does is\nit colors here each of the different\ngroups so here these are all the fours\nthese are all the ones these are all the\nsixes these are all the the twos these\nare the fives okay so it makes this it\nmakes this uh this grouping here and I\nforgot I hadn't run that again because\nwe actually only made three groups and\nso here this is the these are the twos\nthese are the ones these are zeros and\nso it grouped them according to these\nvalues here which is really really cool\nso you can do this uh clustering idea\nwith you know other variables as well\nwhich won't produce as as cool\nvisualizations as something like a\nlatitude and a longitude uh but I just\nwanted to show you this so it really\nmade sense what we were doing um and\noften by the way you might also want to\ndo uh the same sort of scaling stuff\nbefore running a k means or some other\nclustering algorithms as well okay so\nnow we're going to make X train\nclustering uh which uses just the one\nhot encoding from the cluster viel so\nhow do we take this and make a model out\nof this well really these are\ncategorical variables this is a two\nthese are all twos these are all ones\nand these are all zeros so what we're\ngoing to do is that same sort of dummies\nthing uh where we don't have that many\nfor this we have three columns and so\nall of these this is if it's zero this\nis if it's one this is if it's two these\nare the zeros these are the ones these\nare the twos so we do that same thing\nextreme clustering is pd. getet dummies\non the that a series so that's just a\nway of transforming that into a column\nnot even sure if I need that but um so\nwe we get a a pandas data frame out of\nthose labels and then we convert that to\nnumpy so that this is that one hot\nencoding thing and I definitely call\nthat one hot here when I'm looking at it\nat in a numpy format like this so that\nis how we make uh the Clusters turned\ninto at least one way there's others um\nbut one way to get uh turn clusters into\nmeaningful\ndescriptions so we'll now predict the\nclusters of the test data and create X\ntest clustering using one hot encoding\nso we get X test latitude longitude or\ntest DF which is test DF sub longitude\nlatitude I don't know why it's kind of\nconfusing how I said lat long and then\nlong lat but um we get that to numpy\nwhich is just the longitude and the\nlatitudes we get the clustering uh which\nis pd. get dummies of the series I\nshould have organized this organized\nthis a little better but basically we do\ncing St predict on the X test last long\nand then we do the series of that and\nthen we do the get dummies on that so\nwhat gets the labels it turns it into a\nseries it converts that into a data\nframe which is one hot and then we do\ntumpi to get the full shape is going to\nbe that by three instead okay so now\nwe'll check the error of a linear model\nthat uses only the cluster one hot\nencodings how does a linear model do\nwith just knowing that it's kind of\norganized like this is this even related\nto the median house value uh let's find\nout so we make a a model and it's got 80\n88 so it's not bad uh but what if we\nwere to increase the the cluster the\namount of clusters what if we actually\ngot more information out of saying uh\nmaybe these ones versus these ones\nversus these ones so if we were to go\nback to that seven if we were to do\nseven\nhere uh so train it plot it on the train\nand\nthen uh make the matrices out of it and\nthen we were to make another model we\ngot way down to 75 okay so that's how\nimportant this information is is we can\nmake this is just a linear model out of\nall just out of these different clusters\nof information we' learned so much about\nthe median house value based off of this\nthese areas and feel free to play around\nwith that value uh of number of clusters\nif you want to so that really cool you\nof course you don't have to do\ngeographical data U that's just a\nusually you won't uh but if you do have\ngeographical data that can be uh a\nuseful way to get better predictions out\nof it uh or at least different\npredictions there's it it we didn't even\nmake a model that just used the vitude\nand longitude as is you could do that\nand maybe it did better I don't know\nthis is just a cool clustering technique\nthat will have its applications for many\nother variables okay so finally we're\ngoing to do feature selection which is\njust a combination of features so we'll\nobserve the shapes of XT train\nclustering XT train scaled and dummy X\ntrain so all this is if if you didn't\nwatch this part basically this is uh\ndata that's from clustering this is data\nthat's uh some variables scaled and this\nis uh the dummies from that'll actually\nbe the dummies from the geographical\ninformation okay so what we can see here\nis clustering has the seven columns the\nscaled has those three columns and the\ndummies those are those four different\nuh categorical\nvariables so we conat concatenate the\ntraining arrays side by side to make one\nbig xra full input Matrix so we do the\nnumpy do concatenate of X train\nclustering xra 3 scaled and ex string\ndummies across axis one to get this full\nthing so we can see these are 70,000\nrows 70k rows 17K rows so is that and\nthen we just kind of concatenate uh we\njust put these seven and then we put\nthese three and then we put these four\nso you can see that the shape is like\nthis so it's this is a First Column\nsecond column third column up until the\nfirst seven are this the next three are\nthis and the next last four are these\nones here okay so that is our full\ninformation here this is just one way\nwhy why am I doing this well this is\njust one way that we can combine\nmultiple features uh and to make a model\nokay we could of course drop one of\nthese columns at a different one I just\npicked some random stuff because that's\nwhat feature selection or combining\nfeatures is about just play around with\nstuff figure out what works well and\nonly use what you need to use okay so we\nget our full Matrix there I'm going to\nmake sure yes that's already run we can\nlook at the shapes of the testing\ninformation so it's the same thing but\nthe tests and then the test as well this\nfull testing array we check the eror of\na random Forest this is the combination\nof these features we'll make this random\nForest we'll fit it with that train full\nand compare that to White train we'll do\ntest predictions where we predict the uh\nrandom Forest clustering that's not the\nright model guys so uh okay you're going\nto watch me fix this which is basically\nmaking a a random Forest we'll call that\nfull this is going to be full based off\nof random Forest full and this is going\nto be full test\npredictions and yeah so it didn't\nactually matter what the name was but\nthat was definitely not that was\ndefinitely not the right name and in\ntotal here we get a model that seems to\nbe uh at 67k okay so that is easily\nbetter than anything we looked at so far\nuh it's better than our Baseline\ncoverage it's better than uh just the\nclustering just the dummies uh so we\ncombine all of this stuff together to\nmake that and just for fun here let's\neven uh try to make an even better model\nyou know what let's not use these\nfeatures let's try and let's try and\nfigure out uh some new ones so I'll do\nmaybe just an initial one can we do\nbetter than\n67,000 uh I bet you we can if we try and\nchange those parameters and we get bum\nbum bum 60k okay so immediately uh you\nknow this these are things you always\nhave to do which is train the parameters\nof a model we could fit that a lot\nbetter as well but that's the idea so uh\nif you aren't subscribed I think you\nprobably figured out right now that\nthat's a good idea drop a like if you\ndid get value from this I'd really\nappreciate it and yeah let me know what\nyou want to see in the future I hope\nthis one was a good one CU a lot of\npeople are asking for it and so I I put\nin a lot of work to make sure that I got\nthat out and I look forward to uploading\nthis I I'm really happy if you to see it\nI hope it does well and yeah I'll see\nyou later guys\n",
  "words": [
    "hey",
    "guys",
    "greg",
    "welcome",
    "back",
    "today",
    "awesome",
    "video",
    "pretty",
    "much",
    "every",
    "feature",
    "engineering",
    "technique",
    "training",
    "machine",
    "learning",
    "models",
    "explain",
    "python",
    "code",
    "notebook",
    "take",
    "look",
    "video",
    "description",
    "want",
    "follow",
    "along",
    "either",
    "way",
    "got",
    "one",
    "dimensionality",
    "reduction",
    "using",
    "pca",
    "two",
    "scaling",
    "methods",
    "three",
    "categorical",
    "encoding",
    "dummy",
    "one",
    "hot",
    "variables",
    "four",
    "got",
    "bidding",
    "grouping",
    "aggregating",
    "features",
    "together",
    "five",
    "clustering",
    "map",
    "california",
    "example",
    "cluster",
    "latitude",
    "longitude",
    "get",
    "kind",
    "pretty",
    "picture",
    "geographical",
    "data",
    "makes",
    "sense",
    "visually",
    "finally",
    "got",
    "feature",
    "selection",
    "combining",
    "different",
    "stuff",
    "together",
    "taking",
    "want",
    "okay",
    "want",
    "learn",
    "one",
    "particular",
    "subset",
    "ideas",
    "marked",
    "chapters",
    "different",
    "uh",
    "topics",
    "go",
    "learn",
    "want",
    "learn",
    "want",
    "follow",
    "everything",
    "along",
    "got",
    "start",
    "initial",
    "setup",
    "go",
    "thing",
    "video",
    "want",
    "point",
    "using",
    "california",
    "housing",
    "data",
    "set",
    "link",
    "description",
    "particular",
    "kangle",
    "data",
    "set",
    "whether",
    "like",
    "click",
    "link",
    "download",
    "data",
    "set",
    "point",
    "using",
    "one",
    "sample",
    "data",
    "california",
    "housing",
    "actually",
    "download",
    "straight",
    "kagle",
    "like",
    "okay",
    "said",
    "um",
    "put",
    "lot",
    "effort",
    "video",
    "code",
    "everything",
    "explain",
    "mind",
    "dropping",
    "like",
    "video",
    "video",
    "really",
    "appreciate",
    "know",
    "going",
    "help",
    "lot",
    "different",
    "people",
    "would",
    "great",
    "could",
    "support",
    "others",
    "way",
    "h",
    "cuz",
    "learning",
    "together",
    "subscribed",
    "channel",
    "really",
    "good",
    "time",
    "said",
    "let",
    "get",
    "started",
    "initial",
    "setup",
    "firstly",
    "grab",
    "california",
    "housing",
    "data",
    "set",
    "two",
    "ways",
    "go",
    "video",
    "description",
    "click",
    "link",
    "download",
    "zip",
    "folder",
    "actually",
    "go",
    "ahead",
    "upload",
    "zip",
    "folder",
    "use",
    "kagle",
    "api",
    "go",
    "kle",
    "profile",
    "generate",
    "api",
    "token",
    "downloads",
    "json",
    "thing",
    "upload",
    "put",
    "working",
    "directory",
    "stuff",
    "whatever",
    "environment",
    "using",
    "run",
    "block",
    "code",
    "collab",
    "downloading",
    "uns",
    "going",
    "complains",
    "already",
    "done",
    "go",
    "click",
    "yes",
    "kind",
    "redo",
    "stuff",
    "inflates",
    "whatever",
    "housing",
    "csv",
    "important",
    "part",
    "got",
    "california",
    "housing",
    "data",
    "set",
    "right",
    "got",
    "readen",
    "csv",
    "pandas",
    "data",
    "frame",
    "import",
    "pandas",
    "aspd",
    "make",
    "variable",
    "df",
    "equal",
    "read",
    "pandas",
    "read",
    "csv",
    "housing",
    "csv",
    "going",
    "immediately",
    "drop",
    "rows",
    "null",
    "values",
    "part",
    "call",
    "df",
    "output",
    "see",
    "got",
    "20433",
    "rows",
    "10",
    "columns",
    "explain",
    "data",
    "set",
    "ar",
    "already",
    "familiar",
    "skip",
    "section",
    "sure",
    "basically",
    "rows",
    "particular",
    "area",
    "california",
    "bunch",
    "different",
    "information",
    "areas",
    "different",
    "areas",
    "got",
    "longitude",
    "latitude",
    "housing",
    "median",
    "age",
    "houses",
    "area",
    "uh",
    "get",
    "ages",
    "get",
    "median",
    "happens",
    "41",
    "one",
    "uh",
    "sum",
    "total",
    "number",
    "rooms",
    "880",
    "total",
    "bedrooms",
    "129",
    "population",
    "household",
    "income",
    "uh",
    "median",
    "income",
    "area",
    "finally",
    "two",
    "median",
    "house",
    "value",
    "generally",
    "data",
    "set",
    "used",
    "predict",
    "generally",
    "use",
    "bunch",
    "different",
    "variables",
    "try",
    "learn",
    "function",
    "maps",
    "variables",
    "predicted",
    "median",
    "house",
    "value",
    "okay",
    "everything",
    "video",
    "also",
    "going",
    "use",
    "got",
    "extra",
    "data",
    "set",
    "different",
    "data",
    "set",
    "ocean",
    "proximity",
    "categorical",
    "variable",
    "uh",
    "categorical",
    "encoding",
    "section",
    "okay",
    "df",
    "first",
    "thing",
    "going",
    "since",
    "training",
    "machine",
    "learning",
    "model",
    "many",
    "going",
    "learn",
    "uh",
    "basically",
    "transformation",
    "features",
    "trying",
    "learn",
    "median",
    "house",
    "value",
    "features",
    "well",
    "going",
    "split",
    "train",
    "test",
    "data",
    "frame",
    "train",
    "machine",
    "learning",
    "models",
    "dat",
    "data",
    "frame",
    "test",
    "uh",
    "models",
    "test",
    "data",
    "frame",
    "well",
    "okay",
    "df",
    "equal",
    "df",
    "sample",
    "set",
    "frack",
    "equal",
    "one",
    "basically",
    "shuffles",
    "rows",
    "set",
    "random",
    "state",
    "two",
    "uh",
    "want",
    "reproduce",
    "results",
    "use",
    "uh",
    "random",
    "seed",
    "value",
    "want",
    "different",
    "results",
    "could",
    "change",
    "value",
    "okay",
    "make",
    "train",
    "test",
    "first",
    "uh",
    "train",
    "going",
    "first",
    "rows",
    "remember",
    "data",
    "frame",
    "433",
    "rows",
    "decent",
    "split",
    "roughly",
    "like",
    "80",
    "80",
    "train",
    "maybe",
    "20",
    "test",
    "data",
    "frame",
    "get",
    "first",
    "train",
    "reset",
    "index",
    "thing",
    "worry",
    "much",
    "kind",
    "makes",
    "index",
    "still",
    "correct",
    "get",
    "test",
    "df",
    "onward",
    "reset",
    "index",
    "well",
    "got",
    "train",
    "data",
    "frame",
    "columns",
    "first",
    "rows",
    "test",
    "data",
    "frame",
    "stuff",
    "randomization",
    "randomly",
    "split",
    "um",
    "train",
    "test",
    "information",
    "okay",
    "going",
    "machine",
    "models",
    "going",
    "using",
    "median",
    "house",
    "value",
    "predict",
    "training",
    "training",
    "output",
    "basically",
    "say",
    "use",
    "output",
    "train",
    "going",
    "particular",
    "column",
    "get",
    "column",
    "actually",
    "returns",
    "panda",
    "series",
    "train",
    "df",
    "sub",
    "median",
    "house",
    "value",
    "convert",
    "numpy",
    "array",
    "going",
    "flattened",
    "values",
    "train",
    "test",
    "output",
    "numpy",
    "shapes",
    "okay",
    "going",
    "use",
    "heavily",
    "observed",
    "values",
    "outputs",
    "uh",
    "observed",
    "outputs",
    "train",
    "test",
    "first",
    "thing",
    "create",
    "something",
    "called",
    "baseline",
    "model",
    "going",
    "making",
    "average",
    "median",
    "house",
    "value",
    "train",
    "data",
    "frame",
    "okay",
    "point",
    "generally",
    "trying",
    "learn",
    "sort",
    "mapping",
    "inputs",
    "uh",
    "output",
    "median",
    "house",
    "value",
    "trying",
    "predict",
    "usually",
    "try",
    "learn",
    "features",
    "use",
    "necessarily",
    "actually",
    "make",
    "baseline",
    "model",
    "simple",
    "model",
    "terms",
    "comparison",
    "uh",
    "calculate",
    "average",
    "everything",
    "train",
    "data",
    "frame",
    "calculate",
    "average",
    "median",
    "house",
    "values",
    "left",
    "number",
    "good",
    "prediction",
    "obviously",
    "using",
    "features",
    "available",
    "us",
    "like",
    "longitude",
    "magnitude",
    "make",
    "first",
    "baseline",
    "model",
    "simply",
    "average",
    "median",
    "house",
    "value",
    "train",
    "data",
    "frame",
    "models",
    "going",
    "evaluating",
    "based",
    "error",
    "metric",
    "mean",
    "absolute",
    "error",
    "import",
    "sklearn",
    "pyit",
    "learnmetrics",
    "okay",
    "get",
    "average",
    "median",
    "house",
    "value",
    "train",
    "panda",
    "series",
    "take",
    "train",
    "data",
    "frame",
    "sub",
    "median",
    "house",
    "value",
    "panda",
    "series",
    "object",
    "called",
    "function",
    "called",
    "mean",
    "call",
    "get",
    "value",
    "get",
    "test",
    "predictions",
    "predictions",
    "test",
    "set",
    "always",
    "trying",
    "train",
    "models",
    "train",
    "data",
    "frame",
    "taking",
    "average",
    "compare",
    "prediction",
    "one",
    "one",
    "different",
    "actual",
    "test",
    "values",
    "okay",
    "first",
    "prediction",
    "trying",
    "guess",
    "best",
    "possible",
    "uh",
    "instead",
    "predicted",
    "whatever",
    "average",
    "actually",
    "show",
    "average",
    "uh",
    "quickly",
    "every",
    "time",
    "predicting",
    "simply",
    "uh",
    "sorry",
    "189",
    "house",
    "value",
    "see",
    "wrong",
    "many",
    "places",
    "actually",
    "pretty",
    "close",
    "value",
    "far",
    "majority",
    "sure",
    "make",
    "list",
    "get",
    "list",
    "average",
    "value",
    "multiply",
    "length",
    "test",
    "data",
    "frame",
    "going",
    "know",
    "sorry",
    "know",
    "going",
    "predictions",
    "really",
    "average",
    "value",
    "uh",
    "times",
    "predictions",
    "everything",
    "get",
    "mean",
    "absolute",
    "error",
    "test",
    "predictions",
    "right",
    "right",
    "test",
    "comparing",
    "one",
    "one",
    "difference",
    "baseline",
    "model",
    "one",
    "value",
    "versus",
    "actual",
    "test",
    "values",
    "test",
    "data",
    "frame",
    "come",
    "number",
    "see",
    "defined",
    "clearly",
    "running",
    "stuff",
    "much",
    "run",
    "run",
    "get",
    "52",
    "means",
    "average",
    "absolute",
    "value",
    "terrible",
    "mean",
    "definitely",
    "good",
    "model",
    "uh",
    "want",
    "lot",
    "different",
    "methods",
    "improve",
    "upon",
    "going",
    "start",
    "dimensionality",
    "reduction",
    "pca",
    "way",
    "point",
    "video",
    "necessarily",
    "actually",
    "get",
    "uh",
    "better",
    "values",
    "although",
    "definitely",
    "trying",
    "show",
    "make",
    "best",
    "machine",
    "learning",
    "model",
    "particular",
    "data",
    "set",
    "showing",
    "different",
    "techniques",
    "think",
    "right",
    "know",
    "different",
    "techniques",
    "available",
    "try",
    "get",
    "best",
    "model",
    "whatever",
    "problem",
    "okay",
    "firstly",
    "going",
    "start",
    "dimensionality",
    "reduction",
    "pca",
    "observe",
    "correlation",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "households",
    "get",
    "train",
    "df",
    "columns",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "households",
    "left",
    "smaller",
    "data",
    "frame",
    "get",
    "correlation",
    "correlation",
    "always",
    "going",
    "value",
    "negative",
    "one",
    "one",
    "actually",
    "positives",
    "makes",
    "sense",
    "positive",
    "value",
    "means",
    "example",
    "number",
    "total",
    "rooms",
    "increases",
    "number",
    "total",
    "bedrooms",
    "tends",
    "increase",
    "well",
    "look",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "get",
    "9",
    "31023",
    "value",
    "one",
    "total",
    "bedrooms",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "total",
    "rooms",
    "thing",
    "seeing",
    "high",
    "positive",
    "correlation",
    "almost",
    "close",
    "perfect",
    "linear",
    "pattern",
    "total",
    "bedroom",
    "going",
    "increase",
    "total",
    "rooms",
    "increases",
    "well",
    "telling",
    "well",
    "maybe",
    "actually",
    "need",
    "different",
    "variables",
    "maybe",
    "need",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "households",
    "try",
    "predict",
    "uh",
    "median",
    "house",
    "value",
    "much",
    "relationship",
    "information",
    "captured",
    "already",
    "one",
    "option",
    "course",
    "could",
    "simply",
    "use",
    "one",
    "different",
    "features",
    "um",
    "could",
    "use",
    "total",
    "rooms",
    "example",
    "something",
    "likely",
    "little",
    "bit",
    "better",
    "something",
    "called",
    "pca",
    "look",
    "shortly",
    "going",
    "first",
    "set",
    "matrix",
    "make",
    "machine",
    "learning",
    "model",
    "input",
    "total",
    "rooms",
    "total",
    "bedrooms",
    "households",
    "inputs",
    "without",
    "pca",
    "import",
    "numpy",
    "np",
    "make",
    "xtrain",
    "3",
    "equal",
    "uh",
    "data",
    "frame",
    "train",
    "three",
    "columns",
    "convert",
    "numpy",
    "left",
    "17",
    "row",
    "three",
    "column",
    "nump",
    "array",
    "makes",
    "sense",
    "exact",
    "thing",
    "uh",
    "x",
    "tests",
    "put",
    "three",
    "remind",
    "three",
    "variables",
    "get",
    "test",
    "information",
    "well",
    "input",
    "matrix",
    "number",
    "examples",
    "number",
    "columns",
    "number",
    "examples",
    "number",
    "columns",
    "train",
    "test",
    "going",
    "get",
    "error",
    "random",
    "forest",
    "model",
    "matter",
    "know",
    "much",
    "rom",
    "force",
    "uh",
    "xt",
    "train",
    "3",
    "train",
    "testing",
    "x",
    "test",
    "3",
    "test",
    "okay",
    "lot",
    "words",
    "basically",
    "going",
    "uh",
    "import",
    "rain",
    "forest",
    "regression",
    "model",
    "going",
    "going",
    "regression",
    "predicting",
    "median",
    "house",
    "value",
    "predicting",
    "continuous",
    "value",
    "regression",
    "means",
    "get",
    "forest",
    "base",
    "writing",
    "bas",
    "base",
    "model",
    "equal",
    "random",
    "forest",
    "regressor",
    "make",
    "variabl",
    "fun",
    "n",
    "estimators",
    "equal",
    "50",
    "max",
    "deps",
    "five",
    "worry",
    "worry",
    "stuff",
    "understand",
    "fit",
    "xtrain",
    "3",
    "feed",
    "input",
    "matrix",
    "uh",
    "te",
    "train",
    "sorry",
    "train",
    "uh",
    "feed",
    "corresponding",
    "inputs",
    "training",
    "model",
    "okay",
    "forest",
    "base",
    "model",
    "trained",
    "one",
    "get",
    "forest",
    "based",
    "test",
    "prediction",
    "thing",
    "making",
    "average",
    "model",
    "getting",
    "uh",
    "instead",
    "average",
    "every",
    "single",
    "time",
    "actually",
    "going",
    "using",
    "random",
    "forest",
    "make",
    "prediction",
    "feeded",
    "x",
    "test",
    "information",
    "feed",
    "input",
    "get",
    "mean",
    "absolute",
    "v",
    "value",
    "mean",
    "absolute",
    "error",
    "test",
    "comparing",
    "forest",
    "models",
    "based",
    "predictions",
    "okay",
    "run",
    "going",
    "see",
    "defined",
    "keep",
    "forgetting",
    "run",
    "stuff",
    "x",
    "test",
    "x",
    "train",
    "3",
    "find",
    "guess",
    "run",
    "one",
    "funny",
    "okay",
    "run",
    "model",
    "get",
    "196",
    "average",
    "model",
    "instead",
    "like",
    "average",
    "baseline",
    "model",
    "clearly",
    "learned",
    "actually",
    "okay",
    "random",
    "forest",
    "lot",
    "much",
    "better",
    "obviously",
    "let",
    "actually",
    "pca",
    "see",
    "helps",
    "remember",
    "particular",
    "data",
    "set",
    "particular",
    "model",
    "happen",
    "use",
    "help",
    "well",
    "train",
    "pca",
    "linear",
    "uh",
    "dimensionality",
    "reduction",
    "technique",
    "also",
    "something",
    "called",
    "autoencoders",
    "weird",
    "ones",
    "want",
    "look",
    "uh",
    "pca",
    "linear",
    "dimensionality",
    "reduction",
    "technique",
    "sk",
    "learn",
    "decomposition",
    "import",
    "pca",
    "train",
    "pca",
    "set",
    "number",
    "components",
    "equal",
    "two",
    "means",
    "uh",
    "whatever",
    "feed",
    "feeding",
    "uh",
    "three",
    "different",
    "columns",
    "call",
    "xtrain",
    "three",
    "getting",
    "back",
    "two",
    "columns",
    "supposedly",
    "better",
    "information",
    "concise",
    "information",
    "two",
    "columns",
    "need",
    "three",
    "columns",
    "get",
    "two",
    "two",
    "better",
    "ones",
    "use",
    "pca",
    "transform",
    "xtrain",
    "3",
    "x",
    "test",
    "3",
    "xtrain",
    "pca",
    "x",
    "test",
    "pca",
    "see",
    "call",
    "pca",
    "transform",
    "inputs",
    "left",
    "x3",
    "2",
    "instead",
    "let",
    "see",
    "better",
    "random",
    "forest",
    "exact",
    "thing",
    "get",
    "random",
    "forest",
    "fit",
    "extrem",
    "pca",
    "instead",
    "pca",
    "data",
    "feed",
    "train",
    "forest",
    "pca",
    "test",
    "predictions",
    "equal",
    "forest",
    "pca",
    "predict",
    "feed",
    "x",
    "test",
    "pca",
    "back",
    "get",
    "happen",
    "always",
    "forget",
    "actually",
    "running",
    "get",
    "absolute",
    "error",
    "model",
    "79",
    "something",
    "okay",
    "one",
    "82",
    "particular",
    "model",
    "maybe",
    "would",
    "change",
    "things",
    "change",
    "value",
    "linear",
    "regression",
    "whatever",
    "pca",
    "happen",
    "help",
    "model",
    "one",
    "particular",
    "way",
    "uh",
    "feature",
    "engineering",
    "techniques",
    "dimensionality",
    "reduction",
    "pca",
    "particular",
    "okay",
    "quickly",
    "added",
    "xra",
    "three",
    "show",
    "happens",
    "scale",
    "beforehand",
    "sk",
    "import",
    "standard",
    "scaler",
    "normalizer",
    "minmax",
    "scaler",
    "three",
    "different",
    "tools",
    "also",
    "get",
    "graphing",
    "library",
    "make",
    "scaler",
    "equal",
    "example",
    "standard",
    "scaler",
    "fit",
    "xtrain",
    "3",
    "fit",
    "feed",
    "get",
    "xtrain",
    "3",
    "scaled",
    "transforming",
    "using",
    "scaler",
    "transform",
    "xtrain",
    "3",
    "transform",
    "thing",
    "get",
    "thing",
    "called",
    "xtreme",
    "3",
    "scaled",
    "looks",
    "exactly",
    "except",
    "values",
    "look",
    "different",
    "particular",
    "see",
    "two",
    "things",
    "well",
    "one",
    "negative",
    "happens",
    "happen",
    "standard",
    "scale",
    "ones",
    "might",
    "um",
    "general",
    "every",
    "single",
    "value",
    "pretty",
    "much",
    "closer",
    "range",
    "mean",
    "thousands",
    "see",
    "hundreds",
    "see",
    "big",
    "problem",
    "many",
    "machine",
    "learning",
    "models",
    "sk",
    "learned",
    "stuff",
    "might",
    "even",
    "care",
    "deep",
    "learning",
    "algorithm",
    "going",
    "really",
    "care",
    "values",
    "much",
    "bigger",
    "ones",
    "know",
    "change",
    "1200",
    "1500",
    "value",
    "drop",
    "huge",
    "compared",
    "574",
    "versus",
    "214",
    "drop",
    "3",
    "350",
    "360",
    "care",
    "change",
    "much",
    "bigger",
    "one",
    "well",
    "says",
    "feature",
    "column",
    "important",
    "column",
    "really",
    "wanted",
    "maybe",
    "could",
    "keep",
    "general",
    "assume",
    "setup",
    "features",
    "pretty",
    "much",
    "equally",
    "important",
    "example",
    "standard",
    "scaler",
    "thing",
    "values",
    "3",
    "3",
    "3",
    "saying",
    "three",
    "like",
    "7",
    "negative",
    "7",
    "standard",
    "scaler",
    "uh",
    "performing",
    "standardization",
    "thing",
    "means",
    "subtracting",
    "calculate",
    "mean",
    "column",
    "calculate",
    "standard",
    "deviation",
    "column",
    "take",
    "value",
    "subtract",
    "mean",
    "divide",
    "standard",
    "deviation",
    "take",
    "value",
    "subtract",
    "mean",
    "standard",
    "de",
    "divide",
    "standard",
    "deviation",
    "values",
    "column",
    "column",
    "take",
    "calculate",
    "mean",
    "column",
    "subtract",
    "standard",
    "deviation",
    "column",
    "values",
    "roughly",
    "graph",
    "first",
    "column",
    "transformation",
    "see",
    "three",
    "almost",
    "everything",
    "say",
    "look",
    "first",
    "uh",
    "second",
    "column",
    "actually",
    "moved",
    "little",
    "bit",
    "negatives",
    "3",
    "everything",
    "makes",
    "lot",
    "easier",
    "machine",
    "learning",
    "algorithms",
    "figure",
    "heck",
    "going",
    "treats",
    "treats",
    "colum",
    "equally",
    "basically",
    "okay",
    "use",
    "standard",
    "scaler",
    "uh",
    "particular",
    "transformation",
    "technically",
    "need",
    "ne3",
    "three",
    "could",
    "normalizer",
    "instead",
    "particular",
    "data",
    "set",
    "uh",
    "see",
    "looking",
    "first",
    "column",
    "uh",
    "makes",
    "values",
    "zero",
    "one",
    "might",
    "get",
    "different",
    "behavior",
    "values",
    "allowed",
    "negative",
    "first",
    "place",
    "note",
    "positive",
    "uh",
    "could",
    "mmax",
    "scaler",
    "really",
    "going",
    "show",
    "uh",
    "different",
    "ones",
    "like",
    "differences",
    "uh",
    "sk",
    "learn",
    "want",
    "read",
    "documentation",
    "might",
    "want",
    "use",
    "one",
    "exactly",
    "uh",
    "read",
    "might",
    "make",
    "future",
    "video",
    "um",
    "idea",
    "values",
    "going",
    "uh",
    "standardizing",
    "uh",
    "happen",
    "know",
    "already",
    "maximum",
    "pictur",
    "example",
    "picture",
    "data",
    "values",
    "going",
    "0",
    "255",
    "could",
    "divide",
    "everything",
    "255",
    "note",
    "particular",
    "example",
    "said",
    "uh",
    "values",
    "0",
    "255",
    "already",
    "sort",
    "range",
    "dividing",
    "anything",
    "happy",
    "values",
    "general",
    "nural",
    "networks",
    "still",
    "many",
    "machine",
    "learning",
    "al",
    "algorithms",
    "still",
    "happier",
    "values",
    "zero",
    "one",
    "negative3",
    "three",
    "something",
    "small",
    "like",
    "easier",
    "lot",
    "things",
    "okay",
    "different",
    "options",
    "uh",
    "produce",
    "different",
    "uh",
    "algorithm",
    "produce",
    "different",
    "output",
    "look",
    "include",
    "description",
    "want",
    "see",
    "one",
    "want",
    "use",
    "idea",
    "kind",
    "try",
    "see",
    "works",
    "best",
    "overall",
    "okay",
    "important",
    "note",
    "actually",
    "rerun",
    "notebook",
    "see",
    "slightly",
    "different",
    "numbers",
    "would",
    "um",
    "anyway",
    "note",
    "exact",
    "transformation",
    "least",
    "transformer",
    "object",
    "every",
    "column",
    "using",
    "picked",
    "standard",
    "scaler",
    "fed",
    "whole",
    "xtrain",
    "3",
    "times",
    "might",
    "want",
    "pick",
    "um",
    "know",
    "maybe",
    "du",
    "first",
    "column",
    "standard",
    "scal",
    "one",
    "maybe",
    "normalizer",
    "second",
    "column",
    "feel",
    "free",
    "play",
    "around",
    "takes",
    "little",
    "bit",
    "manipulation",
    "numpy",
    "kind",
    "get",
    "arrays",
    "separated",
    "joined",
    "back",
    "later",
    "wanted",
    "let",
    "know",
    "option",
    "sometimes",
    "apply",
    "different",
    "transformations",
    "different",
    "columns",
    "like",
    "going",
    "standard",
    "scaler",
    "xtrain",
    "3",
    "get",
    "x",
    "test",
    "3",
    "scaled",
    "well",
    "x",
    "train",
    "xt",
    "train",
    "scaled",
    "x",
    "test",
    "scaled",
    "know",
    "include",
    "three",
    "guess",
    "okay",
    "make",
    "random",
    "forest",
    "scaled",
    "information",
    "random",
    "forest",
    "fit",
    "xt",
    "train",
    "3",
    "scaled",
    "uh",
    "train",
    "get",
    "test",
    "predictions",
    "model",
    "random",
    "forest",
    "predict",
    "x",
    "test",
    "3",
    "scaled",
    "check",
    "absolute",
    "error",
    "comparative",
    "test",
    "predictions",
    "gives",
    "us",
    "82270",
    "uh",
    "example",
    "okay",
    "look",
    "compare",
    "exactly",
    "model",
    "without",
    "st",
    "scaling",
    "done",
    "824",
    "one",
    "one",
    "uh",
    "82",
    "okay",
    "similar",
    "turns",
    "random",
    "forest",
    "model",
    "really",
    "care",
    "much",
    "scaling",
    "promise",
    "neural",
    "networks",
    "care",
    "lot",
    "might",
    "want",
    "machine",
    "learning",
    "models",
    "well",
    "turns",
    "pca",
    "thing",
    "earlier",
    "actually",
    "requires",
    "uh",
    "least",
    "prefers",
    "sort",
    "standard",
    "scale",
    "sort",
    "scaling",
    "operation",
    "like",
    "data",
    "fed",
    "fed",
    "raw",
    "values",
    "uh",
    "want",
    "actually",
    "scale",
    "data",
    "pca",
    "train",
    "random",
    "forest",
    "three",
    "steps",
    "probably",
    "best",
    "uh",
    "best",
    "choice",
    "actually",
    "use",
    "psychic",
    "learn",
    "pipeline",
    "object",
    "sk",
    "learn",
    "pipeline",
    "import",
    "pipeline",
    "make",
    "scale",
    "pca",
    "pipe",
    "uh",
    "forest",
    "pipeline",
    "standardization",
    "pca",
    "random",
    "forest",
    "pipeline",
    "standard",
    "scaler",
    "pca",
    "number",
    "components",
    "two",
    "random",
    "forest",
    "regressor",
    "thing",
    "note",
    "passing",
    "objects",
    "like",
    "bracket",
    "bracket",
    "fitting",
    "passing",
    "sort",
    "inputs",
    "defining",
    "pipeline",
    "trained",
    "predicted",
    "later",
    "fit",
    "get",
    "scale",
    "pca",
    "pipe",
    "forest",
    "call",
    "fit",
    "still",
    "x",
    "train",
    "3",
    "train",
    "okay",
    "feed",
    "rw",
    "inputs",
    "learns",
    "standard",
    "scaler",
    "okay",
    "knows",
    "uh",
    "learned",
    "parameters",
    "perform",
    "transformation",
    "pca",
    "perform",
    "pca",
    "transformation",
    "data",
    "actually",
    "scaled",
    "call",
    "fit",
    "fits",
    "object",
    "passes",
    "object",
    "fits",
    "passes",
    "object",
    "fits",
    "fits",
    "full",
    "pipeline",
    "call",
    "fit",
    "get",
    "predictions",
    "fit",
    "thing",
    "predict",
    "xs",
    "3",
    "input",
    "matrix",
    "going",
    "transformations",
    "predict",
    "going",
    "scale",
    "data",
    "going",
    "apply",
    "pca",
    "data",
    "make",
    "two",
    "columns",
    "going",
    "actually",
    "apply",
    "previously",
    "learned",
    "random",
    "forest",
    "get",
    "mean",
    "absolute",
    "error",
    "totals",
    "uh",
    "defined",
    "run",
    "going",
    "get",
    "error",
    "80",
    "look",
    "uh",
    "different",
    "ones",
    "still",
    "going",
    "uh",
    "standard",
    "scaler",
    "may",
    "best",
    "possible",
    "object",
    "might",
    "want",
    "use",
    "normalizer",
    "instead",
    "let",
    "try",
    "normalizer",
    "see",
    "get",
    "better",
    "result",
    "although",
    "comment",
    "technically",
    "wrong",
    "right",
    "really",
    "matter",
    "get",
    "77",
    "okay",
    "better",
    "ones",
    "turns",
    "normalizer",
    "happens",
    "better",
    "uh",
    "transformation",
    "perform",
    "data",
    "okay",
    "change",
    "comment",
    "normalization",
    "normalization",
    "well",
    "okay",
    "something",
    "called",
    "categorical",
    "encoding",
    "generally",
    "want",
    "convert",
    "dummy",
    "one",
    "hot",
    "variables",
    "dummy",
    "pandas",
    "version",
    "one",
    "hot",
    "numpy",
    "version",
    "although",
    "thing",
    "look",
    "shortly",
    "ways",
    "encoding",
    "categorical",
    "variables",
    "assumes",
    "different",
    "category",
    "oral",
    "variable",
    "example",
    "red",
    "green",
    "blue",
    "treated",
    "entirely",
    "differently",
    "actually",
    "makes",
    "sense",
    "rgb",
    "make",
    "colors",
    "rgb",
    "three",
    "primary",
    "colors",
    "instead",
    "think",
    "colors",
    "yellow",
    "green",
    "well",
    "actually",
    "similarities",
    "red",
    "blue",
    "primary",
    "colors",
    "trying",
    "say",
    "might",
    "necessarily",
    "want",
    "dumy",
    "encoding",
    "everything",
    "example",
    "words",
    "words",
    "similarities",
    "use",
    "something",
    "called",
    "embeddings",
    "words",
    "going",
    "use",
    "dummy",
    "encoding",
    "uh",
    "make",
    "sense",
    "low",
    "dimensionality",
    "meaning",
    "uh",
    "looking",
    "get",
    "train",
    "data",
    "frame",
    "sub",
    "ocean",
    "proximity",
    "categorical",
    "column",
    "uh",
    "way",
    "look",
    "column",
    "uh",
    "column",
    "hopefully",
    "find",
    "quickly",
    "uh",
    "pd",
    "getet",
    "dummies",
    "column",
    "returns",
    "data",
    "frame",
    "get",
    "head",
    "returns",
    "object",
    "right",
    "going",
    "since",
    "called",
    "train",
    "df",
    "rows",
    "different",
    "columns",
    "uh",
    "well",
    "one",
    "column",
    "different",
    "variables",
    "list",
    "different",
    "columns",
    "one",
    "near",
    "bay",
    "one",
    "island",
    "different",
    "options",
    "right",
    "okay",
    "want",
    "well",
    "say",
    "hey",
    "row",
    "one",
    "one",
    "possible",
    "things",
    "one",
    "near",
    "bay",
    "row",
    "one",
    "many",
    "possible",
    "things",
    "one",
    "inland",
    "call",
    "inland",
    "whatever",
    "um",
    "call",
    "either",
    "dummies",
    "uh",
    "actually",
    "know",
    "term",
    "think",
    "statistical",
    "thing",
    "uh",
    "one",
    "hot",
    "convert",
    "thing",
    "numpy",
    "array",
    "really",
    "one",
    "vector",
    "one",
    "hot",
    "hot",
    "thing",
    "cold",
    "cold",
    "cold",
    "cold",
    "cold",
    "hot",
    "one",
    "hot",
    "one",
    "okay",
    "idea",
    "way",
    "decoding",
    "row",
    "type",
    "thing",
    "row",
    "type",
    "thing",
    "row",
    "type",
    "thing",
    "importantly",
    "uh",
    "many",
    "different",
    "values",
    "different",
    "things",
    "well",
    "firstly",
    "look",
    "code",
    "observe",
    "frequency",
    "categories",
    "look",
    "value",
    "counts",
    "train",
    "df",
    "sub",
    "sub",
    "ocean",
    "proximity",
    "categorical",
    "variable",
    "look",
    "value",
    "counts",
    "shows",
    "many",
    "total",
    "value",
    "number",
    "rows",
    "data",
    "frame",
    "one",
    "hour",
    "less",
    "ocean",
    "like",
    "hours",
    "drive",
    "ocean",
    "something",
    "like",
    "inlands",
    "ne",
    "near",
    "bas",
    "three",
    "islands",
    "actually",
    "going",
    "simple",
    "concatenate",
    "append",
    "new",
    "columns",
    "train",
    "data",
    "frame",
    "train",
    "df",
    "pd",
    "duck",
    "cat",
    "train",
    "df",
    "train",
    "dummies",
    "across",
    "uh",
    "column",
    "ais",
    "ais",
    "one",
    "combine",
    "get",
    "uh",
    "basically",
    "dummies",
    "one",
    "hot",
    "variable",
    "append",
    "since",
    "many",
    "isets",
    "going",
    "draw",
    "drop",
    "island",
    "train",
    "df",
    "drop",
    "island",
    "place",
    "true",
    "meaning",
    "uh",
    "train",
    "df",
    "equal",
    "thing",
    "uh",
    "place",
    "true",
    "ais",
    "equals",
    "1",
    "train",
    "df",
    "head",
    "output",
    "see",
    "columns",
    "except",
    "uh",
    "removed",
    "island",
    "okay",
    "uh",
    "data",
    "frame",
    "overly",
    "useful",
    "uh",
    "change",
    "shortly",
    "going",
    "exactly",
    "thing",
    "test",
    "data",
    "set",
    "get",
    "test",
    "dummies",
    "data",
    "frame",
    "gets",
    "pd",
    "get",
    "dummies",
    "test",
    "df",
    "sub",
    "ocean",
    "proximity",
    "test",
    "dummies",
    "head",
    "gets",
    "data",
    "frame",
    "append",
    "dummies",
    "drop",
    "island",
    "test",
    "data",
    "frame",
    "test",
    "dummies",
    "h",
    "thing",
    "know",
    "would",
    "need",
    "definitely",
    "need",
    "test",
    "df",
    "equal",
    "pd",
    "cat",
    "test",
    "df",
    "test",
    "dummies",
    "across",
    "axis",
    "equals",
    "one",
    "test",
    "df",
    "drop",
    "island",
    "placees",
    "true",
    "accesses",
    "one",
    "test",
    "df",
    "head",
    "ran",
    "things",
    "actually",
    "going",
    "go",
    "back",
    "start",
    "start",
    "running",
    "operations",
    "um",
    "concatenated",
    "columns",
    "train",
    "test",
    "uh",
    "train",
    "test",
    "data",
    "frames",
    "uh",
    "get",
    "going",
    "try",
    "make",
    "machine",
    "learning",
    "model",
    "based",
    "columns",
    "course",
    "end",
    "use",
    "columns",
    "well",
    "going",
    "make",
    "model",
    "looks",
    "different",
    "categories",
    "see",
    "get",
    "x",
    "train",
    "dummies",
    "train",
    "df",
    "numpy",
    "part",
    "little",
    "bit",
    "weird",
    "get",
    "rows",
    "want",
    "end",
    "well",
    "count",
    "back",
    "get",
    "uh",
    "get",
    "column",
    "starts",
    "min",
    "column",
    "want",
    "go",
    "end",
    "say",
    "rows",
    "information",
    "columns",
    "want",
    "chunk",
    "chunk",
    "uh",
    "except",
    "head",
    "information",
    "look",
    "shape",
    "thing",
    "by4",
    "training",
    "information",
    "test",
    "test",
    "df",
    "den",
    "rows",
    "ne4",
    "end",
    "test",
    "dummies",
    "shape",
    "make",
    "linear",
    "regression",
    "model",
    "fun",
    "uh",
    "choosing",
    "linear",
    "regression",
    "x",
    "train",
    "dummies",
    "train",
    "make",
    "linear",
    "dummy",
    "model",
    "get",
    "linear",
    "dummy",
    "test",
    "predictions",
    "model",
    "predict",
    "x",
    "test",
    "dummies",
    "get",
    "mean",
    "absolute",
    "error",
    "test",
    "linear",
    "dummy",
    "test",
    "predictions",
    "show",
    "total",
    "know",
    "running",
    "stuff",
    "go",
    "um",
    "195",
    "okay",
    "actually",
    "pretty",
    "good",
    "model",
    "like",
    "actually",
    "really",
    "good",
    "model",
    "uses",
    "uh",
    "one",
    "hot",
    "encoding",
    "dummy",
    "encoding",
    "variables",
    "okay",
    "use",
    "ocean",
    "proximity",
    "fullest",
    "capacity",
    "return",
    "pretty",
    "good",
    "model",
    "next",
    "something",
    "called",
    "binning",
    "going",
    "use",
    "terms",
    "like",
    "grouping",
    "aggregating",
    "really",
    "referring",
    "thing",
    "look",
    "train",
    "dfs",
    "housing",
    "median",
    "age",
    "example",
    "median",
    "value",
    "one",
    "inputs",
    "range",
    "1",
    "three",
    "like",
    "55",
    "years",
    "okay",
    "important",
    "well",
    "reason",
    "use",
    "column",
    "values",
    "even",
    "scaling",
    "uh",
    "maybe",
    "sort",
    "move",
    "things",
    "bins",
    "draw",
    "line",
    "right",
    "middle",
    "uh",
    "say",
    "30",
    "okay",
    "draw",
    "line",
    "basically",
    "shove",
    "everything",
    "one",
    "big",
    "bin",
    "say",
    "less",
    "30",
    "bigger",
    "30",
    "equivalently",
    "bigger",
    "equal",
    "30",
    "get",
    "two",
    "big",
    "bins",
    "everything",
    "either",
    "less",
    "30",
    "greater",
    "30",
    "train",
    "sub",
    "train",
    "df",
    "sub",
    "median",
    "age",
    "less",
    "30",
    "adding",
    "new",
    "column",
    "data",
    "frame",
    "make",
    "equal",
    "train",
    "df",
    "sub",
    "housing",
    "median",
    "age",
    "less",
    "30",
    "applies",
    "operation",
    "uh",
    "every",
    "single",
    "every",
    "single",
    "row",
    "returns",
    "boolean",
    "okay",
    "thing",
    "thing",
    "panda",
    "series",
    "booleans",
    "either",
    "thing",
    "first",
    "one",
    "happens",
    "less",
    "30",
    "second",
    "one",
    "happens",
    "less",
    "30",
    "third",
    "one",
    "bigger",
    "30",
    "okay",
    "get",
    "boolean",
    "series",
    "convert",
    "int",
    "make",
    "falses",
    "zero",
    "trues",
    "uh",
    "one",
    "train",
    "df",
    "sub",
    "uh",
    "uh",
    "train",
    "df",
    "head",
    "included",
    "column",
    "basically",
    "bin",
    "uh",
    "reason",
    "could",
    "made",
    "binary",
    "variable",
    "could",
    "course",
    "uh",
    "made",
    "multiple",
    "variables",
    "like",
    "say",
    "zero",
    "15",
    "15",
    "30",
    "30",
    "one",
    "hot",
    "encoding",
    "thing",
    "um",
    "main",
    "idea",
    "though",
    "basically",
    "kind",
    "group",
    "things",
    "different",
    "bins",
    "treat",
    "different",
    "ideas",
    "housing",
    "median",
    "age",
    "bigger",
    "30",
    "kind",
    "something",
    "made",
    "uh",
    "pretend",
    "actually",
    "makes",
    "sense",
    "uh",
    "realtor",
    "went",
    "really",
    "uh",
    "famous",
    "real",
    "realtor",
    "said",
    "something",
    "stupid",
    "like",
    "uh",
    "anything",
    "less",
    "anything",
    "older",
    "30",
    "completely",
    "useless",
    "like",
    "much",
    "worse",
    "um",
    "would",
    "actually",
    "strong",
    "effect",
    "something",
    "reasonable",
    "give",
    "uh",
    "zero",
    "versus",
    "one",
    "value",
    "uh",
    "different",
    "feelings",
    "kind",
    "gradual",
    "increase",
    "fact",
    "either",
    "less",
    "30",
    "bigger",
    "30",
    "probably",
    "going",
    "work",
    "super",
    "well",
    "would",
    "better",
    "examples",
    "idea",
    "uh",
    "could",
    "course",
    "combine",
    "features",
    "going",
    "make",
    "model",
    "variable",
    "see",
    "get",
    "x",
    "train",
    "median",
    "ag",
    "really",
    "median",
    "age",
    "less",
    "30",
    "fine",
    "uh",
    "get",
    "convert",
    "numpy",
    "sk",
    "learn",
    "models",
    "always",
    "kind",
    "column",
    "shape",
    "like",
    "number",
    "columns",
    "grabbed",
    "technically",
    "would",
    "flat",
    "reshape",
    "uh",
    "1",
    "going",
    "leave",
    "negative",
    "1",
    "case",
    "number",
    "values",
    "uh",
    "happens",
    "change",
    "time",
    "reshape",
    "get",
    "1",
    "either",
    "zeros",
    "uh",
    "sorry",
    "must",
    "forgotten",
    "get",
    "either",
    "zeros",
    "ones",
    "thing",
    "exact",
    "thing",
    "test",
    "uh",
    "really",
    "going",
    "show",
    "exact",
    "operation",
    "test",
    "information",
    "train",
    "model",
    "information",
    "going",
    "start",
    "gloss",
    "seen",
    "bunch",
    "times",
    "whatever",
    "training",
    "testing",
    "uh",
    "data",
    "set",
    "setting",
    "uh",
    "one",
    "using",
    "model",
    "okay",
    "make",
    "model",
    "going",
    "linear",
    "regression",
    "fun",
    "see",
    "got",
    "uh",
    "compare",
    "baseline",
    "model",
    "top",
    "model",
    "compared",
    "okay",
    "ever",
    "slightly",
    "better",
    "guess",
    "average",
    "clearly",
    "means",
    "something",
    "idea",
    "uh",
    "bidding",
    "grouping",
    "could",
    "variables",
    "could",
    "course",
    "combine",
    "variables",
    "uh",
    "idea",
    "moving",
    "clustering",
    "first",
    "going",
    "plot",
    "map",
    "california",
    "uh",
    "certain",
    "definitely",
    "nothing",
    "going",
    "kill",
    "scatter",
    "plot",
    "longitude",
    "x",
    "latitude",
    "california",
    "right",
    "shaped",
    "like",
    "points",
    "make",
    "k",
    "means",
    "model",
    "longitude",
    "latitude",
    "get",
    "clusters",
    "clusters",
    "well",
    "groups",
    "clusters",
    "really",
    "like",
    "label",
    "saying",
    "group",
    "different",
    "group",
    "different",
    "group",
    "k",
    "learn",
    "cluster",
    "import",
    "k",
    "means",
    "one",
    "algorithm",
    "clustering",
    "important",
    "actually",
    "predict",
    "method",
    "learn",
    "k",
    "means",
    "predict",
    "uh",
    "different",
    "data",
    "set",
    "learned",
    "lot",
    "algorithms",
    "uh",
    "clustering",
    "algorithms",
    "ca",
    "um",
    "get",
    "xt",
    "trin",
    "lat",
    "long",
    "train",
    "df",
    "subat",
    "longitude",
    "latitude",
    "numpy",
    "make",
    "k",
    "means",
    "k",
    "means",
    "number",
    "clusters",
    "know",
    "put",
    "seven",
    "reason",
    "could",
    "three",
    "fit",
    "uh",
    "input",
    "latitude",
    "longitude",
    "get",
    "labels",
    "k",
    "means",
    "labels",
    "uncore",
    "saying",
    "first",
    "one",
    "belong",
    "group",
    "second",
    "one",
    "belong",
    "group",
    "third",
    "one",
    "belong",
    "group",
    "numbers",
    "meaning",
    "different",
    "groups",
    "like",
    "uh",
    "three",
    "group",
    "three",
    "closer",
    "group",
    "two",
    "uh",
    "group",
    "zero",
    "like",
    "different",
    "values",
    "different",
    "groups",
    "entirely",
    "let",
    "plot",
    "colored",
    "map",
    "california",
    "uh",
    "p",
    "plot",
    "express",
    "px",
    "favorite",
    "library",
    "px",
    "scatter",
    "x",
    "train",
    "df",
    "sub",
    "longitude",
    "train",
    "df",
    "sub",
    "latitude",
    "color",
    "much",
    "easier",
    "plotly",
    "rather",
    "map",
    "plot",
    "lip",
    "make",
    "color",
    "equal",
    "k",
    "means",
    "labels",
    "score",
    "colors",
    "different",
    "groups",
    "fours",
    "ones",
    "sixes",
    "twos",
    "fives",
    "okay",
    "makes",
    "makes",
    "uh",
    "grouping",
    "forgot",
    "run",
    "actually",
    "made",
    "three",
    "groups",
    "twos",
    "ones",
    "zeros",
    "grouped",
    "according",
    "values",
    "really",
    "really",
    "cool",
    "uh",
    "clustering",
    "idea",
    "know",
    "variables",
    "well",
    "wo",
    "produce",
    "cool",
    "visualizations",
    "something",
    "like",
    "latitude",
    "longitude",
    "uh",
    "wanted",
    "show",
    "really",
    "made",
    "sense",
    "um",
    "often",
    "way",
    "might",
    "also",
    "want",
    "uh",
    "sort",
    "scaling",
    "stuff",
    "running",
    "k",
    "means",
    "clustering",
    "algorithms",
    "well",
    "okay",
    "going",
    "make",
    "x",
    "train",
    "clustering",
    "uh",
    "uses",
    "one",
    "hot",
    "encoding",
    "cluster",
    "viel",
    "take",
    "make",
    "model",
    "well",
    "really",
    "categorical",
    "variables",
    "two",
    "twos",
    "ones",
    "zeros",
    "going",
    "sort",
    "dummies",
    "thing",
    "uh",
    "many",
    "three",
    "columns",
    "zero",
    "one",
    "two",
    "zeros",
    "ones",
    "twos",
    "thing",
    "extreme",
    "clustering",
    "pd",
    "getet",
    "dummies",
    "series",
    "way",
    "transforming",
    "column",
    "even",
    "sure",
    "need",
    "um",
    "get",
    "pandas",
    "data",
    "frame",
    "labels",
    "convert",
    "numpy",
    "one",
    "hot",
    "encoding",
    "thing",
    "definitely",
    "call",
    "one",
    "hot",
    "looking",
    "numpy",
    "format",
    "like",
    "make",
    "uh",
    "clusters",
    "turned",
    "least",
    "one",
    "way",
    "others",
    "um",
    "one",
    "way",
    "get",
    "uh",
    "turn",
    "clusters",
    "meaningful",
    "descriptions",
    "predict",
    "clusters",
    "test",
    "data",
    "create",
    "x",
    "test",
    "clustering",
    "using",
    "one",
    "hot",
    "encoding",
    "get",
    "x",
    "test",
    "latitude",
    "longitude",
    "test",
    "df",
    "test",
    "df",
    "sub",
    "longitude",
    "latitude",
    "know",
    "kind",
    "confusing",
    "said",
    "lat",
    "long",
    "long",
    "lat",
    "um",
    "get",
    "numpy",
    "longitude",
    "latitudes",
    "get",
    "clustering",
    "uh",
    "pd",
    "get",
    "dummies",
    "series",
    "organized",
    "organized",
    "little",
    "better",
    "basically",
    "cing",
    "st",
    "predict",
    "x",
    "test",
    "last",
    "long",
    "series",
    "get",
    "dummies",
    "gets",
    "labels",
    "turns",
    "series",
    "converts",
    "data",
    "frame",
    "one",
    "hot",
    "tumpi",
    "get",
    "full",
    "shape",
    "going",
    "three",
    "instead",
    "okay",
    "check",
    "error",
    "linear",
    "model",
    "uses",
    "cluster",
    "one",
    "hot",
    "encodings",
    "linear",
    "model",
    "knowing",
    "kind",
    "organized",
    "like",
    "even",
    "related",
    "median",
    "house",
    "value",
    "uh",
    "let",
    "find",
    "make",
    "model",
    "got",
    "80",
    "88",
    "bad",
    "uh",
    "increase",
    "cluster",
    "amount",
    "clusters",
    "actually",
    "got",
    "information",
    "saying",
    "uh",
    "maybe",
    "ones",
    "versus",
    "ones",
    "versus",
    "ones",
    "go",
    "back",
    "seven",
    "seven",
    "uh",
    "train",
    "plot",
    "train",
    "uh",
    "make",
    "matrices",
    "make",
    "another",
    "model",
    "got",
    "way",
    "75",
    "okay",
    "important",
    "information",
    "make",
    "linear",
    "model",
    "different",
    "clusters",
    "information",
    "learned",
    "much",
    "median",
    "house",
    "value",
    "based",
    "areas",
    "feel",
    "free",
    "play",
    "around",
    "value",
    "uh",
    "number",
    "clusters",
    "want",
    "really",
    "cool",
    "course",
    "geographical",
    "data",
    "u",
    "usually",
    "wo",
    "uh",
    "geographical",
    "data",
    "uh",
    "useful",
    "way",
    "get",
    "better",
    "predictions",
    "uh",
    "least",
    "different",
    "predictions",
    "even",
    "make",
    "model",
    "used",
    "vitude",
    "longitude",
    "could",
    "maybe",
    "better",
    "know",
    "cool",
    "clustering",
    "technique",
    "applications",
    "many",
    "variables",
    "okay",
    "finally",
    "going",
    "feature",
    "selection",
    "combination",
    "features",
    "observe",
    "shapes",
    "xt",
    "train",
    "clustering",
    "xt",
    "train",
    "scaled",
    "dummy",
    "x",
    "train",
    "watch",
    "part",
    "basically",
    "uh",
    "data",
    "clustering",
    "data",
    "uh",
    "variables",
    "scaled",
    "uh",
    "dummies",
    "actually",
    "dummies",
    "geographical",
    "information",
    "okay",
    "see",
    "clustering",
    "seven",
    "columns",
    "scaled",
    "three",
    "columns",
    "dummies",
    "four",
    "different",
    "uh",
    "categorical",
    "variables",
    "conat",
    "concatenate",
    "training",
    "arrays",
    "side",
    "side",
    "make",
    "one",
    "big",
    "xra",
    "full",
    "input",
    "matrix",
    "numpy",
    "concatenate",
    "x",
    "train",
    "clustering",
    "xra",
    "3",
    "scaled",
    "ex",
    "string",
    "dummies",
    "across",
    "axis",
    "one",
    "get",
    "full",
    "thing",
    "see",
    "rows",
    "70k",
    "rows",
    "17k",
    "rows",
    "kind",
    "concatenate",
    "uh",
    "put",
    "seven",
    "put",
    "three",
    "put",
    "four",
    "see",
    "shape",
    "like",
    "first",
    "column",
    "second",
    "column",
    "third",
    "column",
    "first",
    "seven",
    "next",
    "three",
    "next",
    "last",
    "four",
    "ones",
    "okay",
    "full",
    "information",
    "one",
    "way",
    "well",
    "one",
    "way",
    "combine",
    "multiple",
    "features",
    "uh",
    "make",
    "model",
    "okay",
    "could",
    "course",
    "drop",
    "one",
    "columns",
    "different",
    "one",
    "picked",
    "random",
    "stuff",
    "feature",
    "selection",
    "combining",
    "features",
    "play",
    "around",
    "stuff",
    "figure",
    "works",
    "well",
    "use",
    "need",
    "use",
    "okay",
    "get",
    "full",
    "matrix",
    "going",
    "make",
    "sure",
    "yes",
    "already",
    "run",
    "look",
    "shapes",
    "testing",
    "information",
    "thing",
    "tests",
    "test",
    "well",
    "full",
    "testing",
    "array",
    "check",
    "eror",
    "random",
    "forest",
    "combination",
    "features",
    "make",
    "random",
    "forest",
    "fit",
    "train",
    "full",
    "compare",
    "white",
    "train",
    "test",
    "predictions",
    "predict",
    "uh",
    "random",
    "forest",
    "clustering",
    "right",
    "model",
    "guys",
    "uh",
    "okay",
    "going",
    "watch",
    "fix",
    "basically",
    "making",
    "random",
    "forest",
    "call",
    "full",
    "going",
    "full",
    "based",
    "random",
    "forest",
    "full",
    "going",
    "full",
    "test",
    "predictions",
    "yeah",
    "actually",
    "matter",
    "name",
    "definitely",
    "definitely",
    "right",
    "name",
    "total",
    "get",
    "model",
    "seems",
    "uh",
    "67k",
    "okay",
    "easily",
    "better",
    "anything",
    "looked",
    "far",
    "uh",
    "better",
    "baseline",
    "coverage",
    "better",
    "uh",
    "clustering",
    "dummies",
    "uh",
    "combine",
    "stuff",
    "together",
    "make",
    "fun",
    "let",
    "even",
    "uh",
    "try",
    "make",
    "even",
    "better",
    "model",
    "know",
    "let",
    "use",
    "features",
    "let",
    "try",
    "let",
    "try",
    "figure",
    "uh",
    "new",
    "ones",
    "maybe",
    "initial",
    "one",
    "better",
    "uh",
    "bet",
    "try",
    "change",
    "parameters",
    "get",
    "bum",
    "bum",
    "bum",
    "60k",
    "okay",
    "immediately",
    "uh",
    "know",
    "things",
    "always",
    "train",
    "parameters",
    "model",
    "could",
    "fit",
    "lot",
    "better",
    "well",
    "idea",
    "uh",
    "subscribed",
    "think",
    "probably",
    "figured",
    "right",
    "good",
    "idea",
    "drop",
    "like",
    "get",
    "value",
    "really",
    "appreciate",
    "yeah",
    "let",
    "know",
    "want",
    "see",
    "future",
    "hope",
    "one",
    "good",
    "one",
    "cu",
    "lot",
    "people",
    "asking",
    "put",
    "lot",
    "work",
    "make",
    "sure",
    "got",
    "look",
    "forward",
    "uploading",
    "really",
    "happy",
    "see",
    "hope",
    "well",
    "yeah",
    "see",
    "later",
    "guys"
  ],
  "keywords": [
    "back",
    "video",
    "pretty",
    "much",
    "every",
    "feature",
    "training",
    "machine",
    "learning",
    "models",
    "take",
    "look",
    "want",
    "either",
    "way",
    "got",
    "one",
    "dimensionality",
    "reduction",
    "using",
    "pca",
    "two",
    "scaling",
    "three",
    "categorical",
    "encoding",
    "dummy",
    "hot",
    "variables",
    "features",
    "clustering",
    "california",
    "example",
    "cluster",
    "latitude",
    "longitude",
    "get",
    "kind",
    "data",
    "makes",
    "sense",
    "different",
    "stuff",
    "okay",
    "learn",
    "particular",
    "uh",
    "go",
    "everything",
    "start",
    "thing",
    "housing",
    "set",
    "like",
    "actually",
    "said",
    "um",
    "put",
    "lot",
    "really",
    "know",
    "going",
    "would",
    "could",
    "good",
    "let",
    "use",
    "whatever",
    "run",
    "already",
    "important",
    "right",
    "pandas",
    "frame",
    "import",
    "make",
    "variable",
    "df",
    "equal",
    "drop",
    "rows",
    "values",
    "call",
    "output",
    "see",
    "columns",
    "sure",
    "basically",
    "information",
    "median",
    "age",
    "happens",
    "total",
    "number",
    "rooms",
    "bedrooms",
    "house",
    "value",
    "predict",
    "try",
    "ocean",
    "proximity",
    "first",
    "model",
    "many",
    "transformation",
    "trying",
    "well",
    "train",
    "test",
    "random",
    "change",
    "maybe",
    "still",
    "say",
    "column",
    "series",
    "sub",
    "convert",
    "numpy",
    "something",
    "called",
    "baseline",
    "average",
    "sort",
    "inputs",
    "calculate",
    "prediction",
    "based",
    "error",
    "mean",
    "absolute",
    "object",
    "predictions",
    "always",
    "best",
    "instead",
    "show",
    "versus",
    "running",
    "means",
    "definitely",
    "better",
    "negative",
    "linear",
    "need",
    "course",
    "little",
    "matrix",
    "input",
    "xtrain",
    "3",
    "row",
    "exact",
    "x",
    "forest",
    "xt",
    "regression",
    "fit",
    "feed",
    "learned",
    "happen",
    "ones",
    "sk",
    "things",
    "scale",
    "standard",
    "scaler",
    "normalizer",
    "scaled",
    "might",
    "even",
    "care",
    "bigger",
    "second",
    "algorithms",
    "zero",
    "note",
    "idea",
    "pipeline",
    "full",
    "colors",
    "pd",
    "dummies",
    "head",
    "island",
    "cold",
    "less",
    "combine",
    "1",
    "shape",
    "30",
    "made",
    "group",
    "zeros",
    "plot",
    "k",
    "clusters",
    "groups",
    "seven",
    "labels"
  ]
}