{
  "text": "hello and welcome everyone so this will\nbe sort of a session where we'll not\ndiscuss the usual uh Tex stuff that we\nusually do uh so usually we discuss\nabout uh the technical details and we go\nin depth so today's session is dedicated\nas I said there's a long due uh session\nwhere I said that I'll be talking about\nuh generative AI in general and we'll\ntake llms in particular so again I'll I\nwould love to start with questions if\nyou people have questions so straight\naway shoot them and then we can start\nfrom there and then we'll go a little\nbit deeper also into these LMS this will\nbe a generic session not on the\narchitecture details and how do we train\nand maybe we'll touch some of the\naspects but more or less it will be a\ngeneric session any questions on Genera\nair and how does it work or something\nlike\nthat sir yes\n[Music]\nllm\ntrain so what specifications we need and\nwhat hardware and this softwares we do\nhave so we can at least not a big llm\nbut even a small\n170 100 billion parameters so 100\nbillion is a lot yes yes I I'll tell you\nthe hardware specifications\nusually uh it depends on the parameters\nthat you have okay it depends on the\nparameters okay I'll have this question\nand I'll answer this question and first\nwe'll start about a bit about what\ngenerative AI is and then afterwards\nmaybe we'll have questions but keep this\ndiscussion engaging ask as many\nquestions as possible and uh this will\nbe a generic session it might help you a\nlot because you might be using geni\ntools and then you should have a right\ncontext what to use them for because\nmany people have this notion that they\ncan be used for this this that and and\nthen sometimes it gives nonsensical\nanswers and then we claim that this tool\nis not working good so if I talk about\ngen okay let me just shift the screen\nbecause I won't write much things so if\nI talk about generate your\nAI so what is this generation doing okay\nso what are what are we generating\nbasically so the point here is that uh\nhumans do generate humans do generate\ndata so it is\nhumans okay uh who generally generate\ndata let's suppose if you speak you\ngenerate data and other devic also\ngenerate data let's suppose you capture\nuh uh photos by using the cameras and\nyou have sensors there are a lot there\nare a lot of tools but humans guide\nthese tools and then they generate this\ndata generate the\ndata that's the point so but now what is\nthe meaning of generate AI so we want uh\nsomething that can be generated without\nlet's let's suppose having that\nequipment let me talk about the image if\nI if you see image okay I need a camera\nor something for that either I can do it\nby camera or I can have an artist who\ncan just draw uh draw an image and maybe\nsome other means can be there so what we\nwant is we actually want machines to do\nthat we actually want machines to\ngenerate those images because you know\nif you go into a little bit of detail so\nif you see an image it is just pixels\nbunch of\npixels okay so can we somehow uh have\nthose pixels arranged in a certain\nfashion which will make sense to a human\nokay if you let suppose want to generate\nimage of a cat image of a dog image of\nsome human being okay because the task\nnow is to find the pixel values and\narrange them in a certain way so that it\nmakes sense to other humans it makes\nsense to the uh people who are seeing it\nand they should say this is a face okay\nthat's the task of generation now the\nGenera AI basically it uh generates new\ncontent including text images audio\nvideo based on the pattern it learns\nfrom the data okay it has a data and it\nlearns patterns from that data and then\nit tries to generate the data from\nwhat's called a same distribution okay\nwe have talked about talked a bit about\ndistributions so data is coming from\nsome underlying distribution so what\nthese gen algorithms try to do is they\ntry to mimic that distribution they TR\nto find those patterns from that\ndistribution if you have a data okay you\nhave data and then you they analyze this\ndata and they see these different\npattern they try to figure out the\npatterns and try to come up with the\nsame pattern okay try to come up with\nthe same things and we'll slowly\nformulate towards it so this is in a\nbroader sense what generative AI means\nthat is you don't have physical\nequipments right now you don't have a\ncamera by which you can take a\nphotograph or you don't have a human\nspeaking uh so what will happen it'll be\nmachines who will now generate the data\nand each generation task is different\ngenerating language is different\ngenerating image is different generating\naudio is different so they work on\ndifferent principles and there are a lot\nof algorithms and lot of architectures\nthat came along the way by this uh in\nthis Genera AI domain if you see it\nstarted with very basic things and then\ncame up with\ngenerator adversarial networks and then\nwe have the diffusion models and after\nwe had Transformer networks and now they\ntook the whole world by storm okay and\nuh there were a lot of things that the\ntransform architecture can do that is a\nbit about these uh this what Genera AI\nmeans Genera AI is just generating new\ntypes of data and and I'm saying it new\nwhy because it should somehow mix and\nmatch the previous data whatever the\npattern there were it should not just\nhave the sample which already is there\nthen it just sampling problem let's\nsuppose you gave generate a some data\nand it is just showing up the sample\nthat was already in the data that is not\ngeneration that is just sampling but\ngeneration basically means that thing\ndid not exist that thing did not exist\nwithin your data so let me show you an\nexample uh where I got interested into\nthe generat give me a second I'll bring\nup that website\nup so if you see there's a website which\nis called as I think these people don't\nexist.com if I just type it uh these\npeople do not exist so that is this the\nperson this person does not exist.com so\nif you see this image so this person\ndoes not exist in the world but you can\nsee the face okay and you can see pretty\nmuch all the details in there okay it is\nit looks a realistic picture if you see\nit okay if I just refresh this again and\nit will pop up a new\npicture and in reality these people do\nnot exist it is not that it is just\nhaving the bunch of data and it is\nsampling from that and it is showing it\nno it is actually generating these\nsamples okay maybe it has generated\nthose samples and kept it somewhere now\nfor the website purpose it is just\nshowing them up but all these people\nthey do not\nexist okay this is where I got sort of\ninterested into this Genera Ai and I\nthink this is based on Genera\nadversarial networks and we might maybe\nspeak about their architecture and stuff\nand now we have even uh in the meta also\nin let's suppose CLA and all of these\nplatforms which give services to the\npeople uh they have this image\ngeneration part also so also they do\ngenerate text okay so I showed you this\nI hope that uh you guys can relate to\nand you know the problem of generation\nsometimes we get confused that\ngeneration sometimes basic might mean\nthat it is just sampling it is not just\nsampling what is it doing is it is\ntrying to come up with new samples\nrather not having the existing samples\nin the data so you will give it some\ndata and it won't just sample and uh\ndata point from the data rather what\nwill it do is it'll try to see the\ndistribution analyze the distribution\nand try to remember the distribution and\nafter that it will generate a new sample\nfrom the distribution okay let me speak\nsome things about distribution we we'll\nsee that in probability also but here\nI'll for the people who don't know what\ndistribution is and let's suppose if I\ntake height of people okay if I take\nheight of\npeople so height of people you you see\nthat it'll range it'll range a lot so\nyou'll see that height let's suppose\nstarts from 0.5\nM say this is 0.5 M and let's suppose\nsome 10 million people are in the range\nof 0.5 m\nand then let's suppose 0.6 M and\nsomething like that let's suppose the\nmax height is 1.7 m let's say okay so\nthese are the different values that uh\nwill be for the height let's call that H\nokay H will be uh the height of the\nperson and height of person may vary\nfrom 0.5 M to 1.7 m okay so if this is\nthe case so what it'll do is so we might\nsee that if you see let's suppose if you\nhave total let's suppose 1 billion of\nthe people let's suppose 100 people\nwe'll just maybe 100 people so you might\nsee that let's say some 50 people have\nthis much height I should not say 50\nlet's suppose some 10 people some 12\npeople have this height so we'll\nbasically distribute okay we'll see uh\nhow is the height distributed so how\nmany people have this how much\npercentage of people have this and\nusually what comes up is like like this\nokay let's suppose this is the mean\nheight okay so this is mean\nheight and this is let's the least\nheight 0.5 and this is the max height 1\n.7 M okay and here we have the count of\nuh the people so if you see something\nlike this so you will see that most of\nthe people will fall into this average\nheight okay so this is one of the type\nof distribution so we call this a normal\ndistribution which is symmetrical\ndistribution so by distribution I mean\nthat you have certain values okay so now\nhow uh is the population so this is\nusually called as population statistics\nso how is the population distributed now\nso what amount of population what\npercentage of population takes this\nvalue this value if you look at around\nif you look for the whole population\nthen if you arrange them in a certain\norder let's in here we arrange them from\n0.5 to 1.7 height okay this was the\nheight so here we had on this axis we\nhad the height now here we had let's\nsuppose the number of people so if you\nsee in in that aspect so we had uh the\ntotal number of the the height of the\npeople and then we see how much so this\nthis density basically shows that how\nmuch people you have there so this is\ncalled as a distribution similar will be\nfor the words also so if you have if you\nlet's suppose have English words okay so\nif I write all the English words that do\nexist in the dictionary there and then\nif I collect all the text documents so I\nhave\nwords okay then I'll collect Corpus so\nCorpus basically means a lot of text\ndocuments will be there then I can find\nthe distribution of words I can say okay\nthis is the word exists in let's suppose\nall the documents and this and that I\ncan find the distribution I can find\ndistribution from this Corpus as in the\nfirst example we had Heights of the\npeople and then we were saying that\nwe'll find the distribution of heights\nhow is this height distri distributed\nokay if I take a normal person so what\nis the chance that he'll he'll be in\nthat this is whole delal in probability\nand stats we'll see them in detail but I\njust wanted to give you a sort of glance\ninto what what I mean by distrib when I\nsay a distribution so if you take all\nthe words from the English language okay\nand then if you take a lot of data lot\nof Corpus a lot of documents from the\ninternet and they'll follow a specific\ndistribution they'll follow some\ndistribution they'll be distributed\nsomehow that will let's if you only have\n100 documents so you can easily say that\nthis word the exist in these many\ndocuments this word exists in these many\ndocument and then you can come up with\nthe distribution okay so this is uh this\nis some sort of like uh I say minuscule\nversion of what a distribution is in\nintely and we'll go in detail in while\nstudying probability and stats we'll see\nwhat distributions actually mean so can\nI assume that now you at least have some\nclarity of what distributions\nare okay I particularly for the people\nwho are in undergrad so you might not\nhave heard about these terms ASF and uh\nAran might be there bomas so you you\npeople at least got an idea what\ndistribution is so distribution is\nnothing but basically you have certain\nvalues so how are these values\ndistributed okay how are these values\ntaken okay as in the height the height\nis from 0.5 to Let's suppose uh 1.7 and\nit's a continuous range of values now\nhow do people take these values makes it\na distribution similarly in the context\nof natural language you have words\nbasically it's called as vocabulary so\nin natural language it's called a\nvocabulary so in the vocabulary you have\nthis vocabulary size and then in the\ndocuments what the distribution how how\nare these words distributed in the\ndocuments so that's called the\ndistribution for these words okay so\nwhat I'm saying is that these uh\nwhatever networks you call them or\nmodels you call them these they try to\nlearn this distribution they try to\nlearn this underlying distribution okay\nthis is just for the height of the\npeople so once they learn this\ndistribution what will they do is now\nthey'll sample a point from here so\nthey'll pick a point from here and\nthey'll show it to the people but this\ndistribution is learned from the sample\ndata whatever data you have is this\ndistribution is learned and then you\njust sample the thing from the\ndistribution and show it to the p and\nsimilarly for this image also if you see\nif you ask me what is the distribution\nso we'll see uh this basically is\ndistribution of pixels\ndistribution of pixels how are how are\nthese pixels distributed okay now if I\njust have to create an image which is 28\ncross 28 image so that means it has 784\npixels okay we I have to generate each\npixel here so I have to generate this\nthis this each pixel has to be generated\nand then I have to arrange them in a\ncertain order also okay that makes the\nsearch space very huge okay how how to\nsearch this if you if you see it as a\ncombination combinal problem so it makes\nit makes the space huge so each of these\n784 values have to be filled let's\nsuppose and then if you see RGB values\nand then it has take colors so basically\nnow what it tried to it tries to learn\nthe distribution of each pixel it it'll\ntake a pixel from a distribution which\nit has learned from data okay and then\nit will put them together basically it\nis time to learn distribution\nand giving some once it's asked to\ngenerate a new thing it is basically\ngenerating a new sample from that\ndistribution okay and this distribution\nright now is 784 dimensional so in\nheight the distribution was uh one\ndimensional because we're just sampling\nheight so here it was 784 Dimension it\nhas to basically generate\n784 in that space it has to navigate the\nspace and generate a sample okay so this\nis how they do it so they this is called\nas uh because they are learning the\ndistribution and in particular in these\nllms okay so this is one way of learning\nand there are other rule based learnings\nbut we'll restrict our discussion on\nthis why they learn distribution and by\ndistribution I mean they try to find a\npattern once they found the pattern and\nwhen asked to generate something new so\nthey'll just have this distribution at\nthe place and then they'll just uh give\nthe sample from the distribution any\nquestions on the distribution now we'll\nhave this was the sort of heavy part\nokay because we have not covered prob\nand stuff that I try to keep my language\nvery flexed so that uh uh you people do\nunderstand it okay and this is how\ngenerative air Works mostly if you see\nthese gener ad networks if you see the\nvariational auto encoders if you see\ndiffusion models if you see the\nTransformer models mostly that's how\nthey try to learn the distribution uh\nfrom the data from the large Corpus\nthat's why they data hungry they require\na lot a lot a lot of data in order to\nfind those patterns is it is it is it\nall right can I move\nforward\nokay now we'll see what these llms are\nokay okay now coming to the core part\nwhat these LMS are as you know I don't\nwant to Define it what these are large\nlanguage models and whatever so people\nare now thinking that they're just\nextension of what's called as NR models\nokay NR\nmodels and what are these engr models so\nif I talk about the engram models okay\nlet's speak about Bagram model Bagram\nmodel so let's suppose if I uh show you\na lot of data so what we can do is in\nthat data there will be lot of sentences\nthere will be sent let's suppose uh\ncat's hat on the\nmat on the\nmat okay what these vagr models\nbasically are so if I let's suppose give\nyou this cat Okay and then we'll see\nwhich word follows this cat okay which\nword follows this cat how much\nprobability is there for sat how much\nprobability how much probability is\nthere for on the mat and let's suppose\nwe have a vocabulary size so what we\nhave we'll have vocabulary and uh we'll\nhave a lot of words let's call them now\nI'll generalize this thing so I'll\ngeneralize it so let's suppose I have\nthis I have word W1 till w n words in my\nvocabulary so basically my dictionary\ncontains n words let's suppose so I'm\ntrying to explain what a Bagram model is\nand how does it work so I have words\nfrom W1 to WN so what I can do is I'll\nsay if I have this word WN okay so which\nword has the highest probability of\ncoming uh because I have huge carpus I\ncan I can count the frequencies I can\ncount the frequency of words I can say I\ncan count the pairs so W1 W2 okay W1 uh\nW3 W1 W1 even okay and uh W1 W four and\nso on till W1 WN so I can count the\nfrequency of all of these things I can\ncount frequency by frequency I mean how\nmany times do they coexist in every\ndocument let's suppose I have all the\ndocuments the world so if I somehow come\nup with if this is a W1 word okay and\nhow what's the Frequency that this W1 is\nfollow of W2 okay so if I have large\ncorpor let's suppose this is 100 times\nokay and let's suppose this is some\n2,000 times this is some 40 times this\nis some 50 times accordingly this is\nsome 10,000 times something like that so\nfrom this we can convert into\nprobability so are you understanding\nthis what I'm saying can you understand\nthis so what I'm saying is if we have a\nword W1 and then uh we have the\nvocabulary also so I want to see because\nI'm restricting myself to the Bagram\nmodels I'm saying that what I'll do is\nI'll search the whole documents and I'll\nsee how many times that this word does\nthese two words occer together this W1\nW2 W1 W2 this is W1 W1 and then W1 W3\nsomething like that okay so I'll check\nall the frequencies and these\nfrequencies can be converted into the\nprobabilities so what what I can do is I\ncan make a diagram like this W1 W2 till\nWN okay and then W1 W2 till\nWN and here I'll I'll have the\nprobability scores so here I'll have\nprobabilities if you go like this so\nI'll have probabilities so let's suppose\nthis W1 W2 the the probability is 0.1\nthe probability is uh 01 and the\nprobability is. 5 05 something like that\nbut now you know the probabilities that\nall of these should sum up to one okay\nso basically from the occurrences I will\nconvert it into probability okay let's\nsuppose this occurred some thousand\ntimes in the whole document this\noccurred some 10,000 times something\nlike this and they occurred together I'm\nsaying this is 10,000 this is thousand\ntimes I decrease the probability so if\nyou see this so from this I have the\nprobability now what I can do is so if I\nhave word W1 so I can look into this\nprobability distribution and I can say\nthat okay so this word is W1 then now\nwhat I can do is with point1 probability\nI'll pick up this word with 0 Z on\nprobability I'll pick up this word okay\nI can pick up and uh I'll pick up W1 and\nthen I'll sample from this basically\nthat's called as picking up from the\ndistribution so I'll pick up the word\nlet's suppose that came up W3 and now I\nhave W3 so corresponding W3 I also again\nhas the same column no sorry same row of\nprobabilities so again I'll sample from\nthat from W3 I let's suppose sample WK\nokay I have WK over here now so from\nthis I'll have probabilities also\nmentioned over here from the\ndocuments and after this W1 WK and once\nI see that I'll again pick up some with\nthe probability because each will have\nsome probability assigned to that and\nwe'll sample that thing and let's\nsuppose we got some word w Dash okay\nthis is how we'll generate this\nsentence is it clear is it clear how\nwill how how how a Bagram model will\nwork so we'll have W1 to w n words and\nwhat we'll do is we'll count the\nfrequency of their occurrences we'll see\nhow many of times they occur together\nokay once we have the count so along\nthis row we can convert them into\nprobability a simplest way of converting\ninto probabilities is sum all of these\ntogether and let's suppose it was uh\nth000 here it was 10,000 so then divide\nit by the all the sums then you uh sum\nthis whole thing first and then divide\neach entry so that you convert them into\nprobabilities okay once you convert them\ninto probabilities then randomly\ninitially you will start your sentence\nyou just start your sentence with\nsomething or maybe you have a start\ntoken also so you'll say this is my\nstart token how many times is this word\nokay which word is is the starting word\nmost of the time you can just sample\nthat and after that what you can do is\nuh you can follow this you can just see\nthe row corresponding row and according\nto the probabilities pick up those words\nand you can complete the sentence\ntogether okay I hope that this is this\nmakes sense to you and this is what is\ncalled as a Byram model and similarly\nthis can be made this can be seen for\nengrams also so we'll make an n\nbasically we'll have so we'll have n\nminus one thing and then we'll predict\nthe N uh token based on what is the n\nminus one and this is a simplest\nproblemistic approach once we go this\nwe'll code this so I'm promising you\npeople that we'll Cote it okay once we\ngo to the Deep larning part we'll Cote\nthat I'll show you that how to basically\nwe'll make this at least Bagram model we\nwe'll make it and I'll show you that it\ndoes perform it does show you some\nresults okay so I might have a notebook\nwith me also where I can just show you I\nwon't explain the code I'll just show\nyou the notebook whatever steps I showed\nyou I'll show you those let me give me a\nsecond or\nso okay so but I want to explain the\ncode um let me just create a new\nnotebook I'll write it Byram\nmodel so this I'll explain in detail\nonce we go to deep learning part once we\ngo to language model parts so we'll go\nin details so is the code window visible\nis it visible\nyeah yes yes it is visible so I'll show\nyou these things so let's suppose I'll\nimport this m plot lib isor yes\neverything is there so what I can do is\nI can generate a corpus so let's\nconsider these\ndocuments okay so this is my these are\nmy documents some some words a and\nPowerful a problems creative data and I\nhave just created some junk data\nconsider this as my Corpus so this is my\nwhole list of documents okay which I was\nspeaking about now there are a lot of\nwords so what I need to do is I need to\ntokenize this so what is the meaning of\ntokenize I'll take each word I'll see\nhow many words are there so this is what\ntoken I am separating out I'm separating\nout basically each of the words so that\nI tokenize the data I'll show you the\ntokenization also if you see this so\nthis is tokenize data there is a word\nthere is a word and Powerful AI problems\ncreativity these are all the words okay\nI basically took all the sentences I did\nWord level tokenization so I separated\nout all the words and then what I can do\nis I can create byrams okay okay so I\ncan create byrams so let's\nsuppose I'll create Bagram so if you see\nthis so a and and they exist 25 times\ntogether and and Powerful they exist 20\ntimes together powerful and AI they\nexist 10 times okay similarly you can\nsee for all the value you can see that\nwe have a d we have a whole dictionary\nof these values beautiful is and\nprogramming came eight times and future\nand a came nine times so I'm just\nkeeping the\ncount okay so once I have this count I\ncan show you the Matrix also but it\nmakes no sense\nI can also print the Matrix because in\nthe code that I had written I think long\nway back so if you see this by this is\nthe Matrix a and a they exist together\neight times a and AI exist 15 times A\nand N exist 25 times so this is the\ncount and this is a huge Matrix so\nusually we have here we have 32 by 32\nthat is our vocabulary size is 32 we\nhave only 32 words we can have more this\nis just a mockup data so now after after\nwe have this so what we have to do is uh\nnow we'll calculate the probabilities\nwe'll convert them into probabilities\nokay so this is how to convert these\nbyrams into probabilities so if I show\nyou now this is the\nprobability with and has probability\nlike a will be followed by and has\nprobability of 0.09 okay a with\napplications has probability of 0.03\nsimilarly a with innovation has this\nprobability and similarly you can go\nthrough all of these I just computed the\nprobability of each word corresponding\nto this now let's suppose I already\ngenerate function by generate I'll\ngenerate the sample so in there I'll\nstart with a random thing so that that's\nwhy you see this import random okay so\nwhat I do here is I import random and\ngenerate this so once I generate and\nthen I'll just pick up the choices from\nthe random and I'll generate the\nsentence this will be generate sentences\nso if you see this I applications python\nbeautiful I love Innovation I love I\nlearning language something like that it\nlooks jber but because the Corpus size\nis less okay and what is doing is it's\nseeing this I and then it has to sample\nthe next word so it is sampling\napplications once application is sampled\nit will sample the next word it is\nsampling python if I run it again\nbecause it's random it should\nchange okay so you can see this I signs\nfuture solution and create and this is a\nBagram model so you saw that we got a\ncorpus and the first step was we\ntokenized the data we took each the word\nstep that's called as Word level\ntokenization now we have character level\ntokenization we have different types of\ntokenization how do you separate out\nthese words and we have in bite level\ntokenization also and after we have that\nthen in Bagram model what we usually do\nis we'll see the occurrence of these\nbyrams together how let's suppose we\nhave a word is it followed by some other\nword so the frequency those frequencies\nare convert into probabilities and then\nrandomly generate the first word and\nafter once we have that word we slowly\ngenerate the other words based on what\nwas happening previously I hope that\nthis Bagram model is clear is it\nclear okay beautiful that it and there\nis no magic there is no magic in this\nwe'll see these language models so now\nlanguage models are engram models on\nsteroids okay and they have done a lot\nof architectural change this and that\nand that's how they generate these\nthings and there they have attention\nthey have do this do that and we'll see\nall of these and we'll start from the\nBagram models so now what I told you is\nthat if you consider this let me just uh\nhide this code\nwindow okay so now if you see these\nlarge L model just as engram model so\nthey just are can be viewed as an\nextension but there AR change and there\nare different training methods and they\nare given more capabilities but\nunderneath the H it is doing this it is\nbasically trying to uh see what's called\nas context it is seeing let's suppose\npast tokens so they are called tokens\nusually so whatever I call as a word\nthey are called tokens that's why it's\ncalled tokenization you break them into\ntokens so it is seeing the previous\ncontext it is seeing the previous\ncontext and based on the previous\ncontext it is generating the new token\nthe next token and they're called as\nAuto regressive models okay Auto\nregressive usually whatever you have\nthese\nchat GPT llama and Claud all of these\nare Auto regress model what do the\nmeaning of Auto regressive because they\ntake into consideration what happened\npreviously okay let's suppose I the\nfirst thing it generated was C and then\nafter seeing C it generate a then T okay\nthen\nspace whatever happened previously it\nwill generate that and then space then\nset then on then then mat and then let's\nit will stop let's suppose to uh\ngenerate this on it'll take whole of\nthis into Picture This is called as\ncontext okay you might have seen the in\nnews articles that Chad GPD supports\nthis much Contex length Contex lens\nbasically means how many tokens uh can\nit attend to how many tokens Can it keep\nin let's suppose uh I should not use\nthis word but how many uh tokens Can can\nit keep in the memory okay this is\ncalled as context so context is what\nwhat came before it and once it generate\nthis on now the new thing became like\nthis so based on this it generated this\nbased on this it generated this mat as\nwe saw in the Byram models okay so I\nhope this is clear is it\nclear is it clear now now they have\nthese Branch okay now how do you\ngenerate there are different techniques\nof generation so different techniques of\ngeneration okay we won't go in detail\nthere is something called as greedy\napproach there is something called as\nbeam search there are a lot of search\nmethods how do you basically because it\ndoes not generate only one thing now\nonce you have the sample which thing to\npick now becomes a question because if\nif I just say left\nand okay okay what is the usual\nthings here let's suppose the words can\nbe right left and right people usually\nuse let's it has a probability of 0.1 or\nmaybe I think more 0.2 left and Center\nCenter also will have probability some\nprobability 0 point uh let's suppose 18\nsomething like that and there will be a\nlot of words now which words select and\nput it here that is how do we how do we\nwrite different ALG like really search\nlike I said mem search okay and they'll\nselect this word and put it here and it\nwon't happen at Word level it will\nhappen basically at token level and\ntoken cannot be a word token may be a\nword less than a word as I said the\ndifferent methods of tokenization are\nthere so let's suppose this word is high\nprobability if you take if you choose\nthe highest probability word so what\nwill happen is you are using greedy\napproach so whichever word has the\nhighest probability once it sees this\ncontext and these are the vocabulary\nwords and you are choosing the highest\nuh probability word that's called as\ngreedy search you are becoming greedy\nand taking that and it does not let\nmodel get more creative okay and\nsometimes what do you have beam search\nalso so it basically creates a lot of\nbeams so you have left and and then\nthere's a lot of possibilities which\nwords to select okay so it let's suppose\nit'll have top four beams they usually n\nbeams so from here it'll again have a\nlot of possibilities and from here it'll\nagain have a lot of\npossibilities okay so once it has the\npossibility and then we see let's after\ntwo steps we're saying after two steps\nwhichever branch has the maximum\nprobability I'll generate those words\nokay usually this is how beam search\nworks like I'm dumbing it down a lot but\nI think you got the idea so now you have\nthis and and you're keeping beams alive\nyou're saying these four words I'm\ntaking and after that there are\ndifferent beams again and then you're\nComputing the probability at let step\ntwo okay so as it becomes more creative\nthis is how we uh sample from the and\nthese are generation modes in which mode\ndo you want to generate so I just\nexplained two one is greedy and then\nthere are topk sampling or there there\nare a lot of things that people do in\nthis so if you see language model it's\nnothing but angram model on many people\nwill disagree with me no this is not\njust Ang grammar it is not just but you\ncan uh uh maybe think it is is angr\nmodel because you have n minus one\ntokens and you are generating nth token\nbased on what is there in N minus one\nokay so that's that's how it does but as\nwe did with the Bagram model it is\ntotally different uh with how these\nlarge language models like GPT and Lama\nclaw do it so they do it separately\nbecause they have their own uh uh what's\ncalled is architecture and then they use\nsome people use self attention and they\nuse different types of attention also\nokay is it clear so far now what's it\ngood for what's it good for the question\nbecomes so what is it good for the good\nit good for distribution things so\nwhatever if you have a distribution okay\nlet's supp language language has a\ndistribution it has a clear distribution\nthat's why you will never see GPT or\nthese large language models they making\na grammatical mistake why because there\nis a pattern in the language there is a\nparticular distribution there is a\npattern in the language okay it captures\nthat pattern okay so the main problem\ncomes with the content there is two\nthere are two things content and\nstyle okay it captures the style in a\ngreat way it is great at capturing style\nokay but this content I'm not sure about\nokay how how much does can capture the\ncontent let's suppose if somebody has a\nsomebody can speak eloquently but his\ncontent is totally rubbish okay that's\nwhat these language models should be\nseen as they are really good at\ncapturing style why because capturing\nstyle is a distribution okay but content\nis not a distribution why do they get\nthese fact facts wrong because facts at\nan instance it's not a distribution okay\nlet's suppose uh we have we have\ndifferent facts uh facts like what is\nthe radius of sun what is the radius of\nEarth they don't come from a\ndistribution okay they have fact they're\ncalled instance level things they have\nan instance let's who is the president\nof uh United States who is the prime\nminister of India if it is in data it\nprobably try to replicate it but it\nsometimes may get these facts wrong if\nyou see a mathematical problem that is\nnot a distribution problem that is a\nfactual problem rather there's a there's\na whole procedure for that okay so what\nis it good at it is good at generating\nthese distributions it's good at\nmimicking those distributions okay in\nthat one thing comes is copying style so\nit it can write beautifully maybe\nwhatever the content it is maybe that\ndoes not have an intrinsic value if you\nif you tell it if you have to tell it to\nmake a mistake you have to give it\nprompt speak like an let's suppose a\nperson who who speaks in broken English\nthen it'll give those things otherwise\nit won't make make a grammatical mistake\nwhy because it has learned the pattern\nokay there a difference between a formal\nlanguage and informal language okay if\nyou see uh formal language like python\nC++ or other language so they have an\ninterpreter okay The Interpreter is that\nbecause you can create a p tree and even\nCC they have a compiler so you can check\nthese because there are these semantic\nrules and everything is there and there\nis some validity but if you see this\nformal language you you don't have an\ninterpreter okay but basically the whole\ninterpreter is the whole world how you\nput the things whole you can interpret\nin any any way possible okay that that's\nwhy it's very hard uh to come up with\nthe grammar first and then with the\nlanguage so rather what happens is\nlanguage comes first then grammar comes\nspontaneously and in the computer\nlanguage it's revers you come with the\ngrammar first you say these are the\nrules and after that you make a language\nokay and once you give that okay now\nmany people confuse it with that they\nhave capability of basically reasoning a\nlot of things they ask them questions\nokay and many of many of people have\nshown that it does not possess reasoning\ncapabilities not they shown it is a fact\nnow it does not have because people what\nthey see is when they see some\nit with a reasoning and it's very hard\nto distinguish between uh is somebody\nreasoning or is he saying something from\nthe root memory let's suppose if I ask\nyou a question and that's a problem with\nthese exams also so when you go for\nthese IIT J need coachings so what they\nbasically teach you there is patterns of\nthe questions they don't tell you to\nreason okay they're saying you this is\nthe pattern of question this is the\nformulation to solve it you apply this\nformula you'll get this answer and fill\nfill in this thing okay so now it is\nvery hard to distinguish between a\nperson who actually reasoned and came up\nwith the answer and the person who just\nfollowed the step who just did the root\nmemory and just came up with the answer\nit's very hard to give uh tell the\ndifference between reasoning and what\nlooks like reasoning and because\nreasoning is usually from first\nprinciples okay you should first take a\nstep and then learn a different step and\nthen infer something from that infer\nsomething that but these language models\ndo not do that okay they generate these\nthings and I showed you this and after\nthat uh let's suppose they they'll give\nyou some things any questions till now I\nI at least getting a bigger picture of\nhow language models work\nit's looking like I'm talking to myself\nonly you should also ask me questions\nyou you divert the conversations\notherwise whatever comes to my head I'll\nspeak\nthat\nyes it will be no that's what I'm saying\nit will be novel In Style the style is\ngood sty style will be good but the\ncontent is the problem it can generate\nany jish it can generate the things\nwhich may look it may give sense but the\ncontent is totally nonsensical okay\nsometimes it generate sentences because\nit puts them in a right way now The\nInterpreter you are not interpreter is\nthere is no restriction how it can\ngenerate because we have no uh rules for\nthese language how to interpret as I\nsaid that formal language they do have a\nparticular procedure how to check are\nthey syntactically correct are they\nsemantically correct and now with the\nwith the informal language the natural\nlanguage the problem is that the whole\nworld is interpreter so you want to\ninterpret you can interpret it that's\nwhy sometimes you will see GPD a\nbrilliant answer but the problem is it\nactually gave a brilliant style and the\ncontent came out to be brilliant but\nsometimes uh it just generates something\nrubbish and uh it gives something and we\nfeel like oh probably this is this is\nfactual but it's what it's trying to do\nit just mimicking the language okay so\nsecond second thing that you can see it\nis uh it is basically trying to simulate\nthe things you can you can see it as a\nsimulation machine if you see a human\nlet me make a difference between this if\nyou see a human who have persistent\nmemory and let's suppose we are speaking\nright now whatever I spoke I also have\nthose things in the head and if you can\nrefer to that uh I'll let's suppose I'll\nanswer accordingly and we we'll fit the\nsituation but what these language models\nare you can think them let's you give\none prompt okay and they generate the\nthings right there so if you want to go\nback again probably you won't be able to\naccess it okay let me let me show you a\nsimple analogy so I'll go to chat GPT\nand I'll maybe\nokay let me let me go to chat GPD I I'll\nshow you something amazing at least it\nshould it should look amazing to many of\nyou what I what I want to do is I'll\nshow you how how different they do the\nwork from us normal\nhumans and uh let me bring up the code\nwindow no no sorry I should have not\nbrought Chrome window is\nthere\nso yes Chrome window is not Vis can you\ncan you see it no yeah yes sir let's\nplay a game with GPD okay so I'll\nexplain you the game and then uh what I\ncan do is let's suppose uh you might\nhave heard about this game uh you have\nto think about some object and now I'll\nguess that object but I'll I'll I'll ask\nyou a question uh let's suppose if you\nthought about uh say pen okay if you\nthought about this pen I'll say you okay\nthink about an object you'll think about\nno then I'll cury you and I'll just give\nyes no answers you just give me yes no\nI'll tell you okay uh tell me uh is it\nused by humans you can say yes is it\nused by animals you can say no okay I'll\ncury you this and I'll basically\ndecrease my search space till I get the\nright answer I'll say uh is it a pen so\nif it was a pen you'll say yes is it a\nlet's suppose is it a dog no say is it a\nfood so I'll just ask you yes no because\nnow you have the thing in the mind you\nhave the object that I asked you keep an\nobject in your mind and then I'll ask\nyou yes no questions and get the answer\nhow many of you understood the game the\ngame is that you'll have to think of an\nobject and then you'll have to ask yes\nno questions until it basically gets the\nright answer is is the setting of The\nGame\nclear is it clear somebody speak I\ncannot see the\nscreen yes sir it's CLE yes so I can say\nI can play the same game with GPT okay I\nI I'll play the same game I'll say uh\nlet's play a game let us play a game in\nwhich\nin\nwhich you have\nto select an\nobject and I will\nask yes SL no questions I'll ask\nquestions questions\nwhere you will answer no or\nyes and then\nI will accordingly guess the\nobject guess the object something like\nthis okay I hope that it picks\nup okay sounds fun I have picked an\nobject so it has picked an object and go\nahead ask you a question okay so I'll\nask it the question I'll ask it uh let's\nsuppose the question is uh is it used by\nhumans is it used by humans I'm now\ntalking about the object that it has Tau\nin the memory is it used by humans it's\nsaying yes it is uh is it a living\nthing it is saying no it is not a living\nthing uh basically it means it is used\nby humans and it is a living thing is it\nedible can we eat it something like that\nI'm making spelling mistakes no it's not\nedible\nso can\nwe is does it move does it move\nsomething like this\nokay saying no it does not move and\nlet's suppose uh now I'll say okay I\ndon't know please tell me the answer\nplease tell me the\nanswer\nokay the object I picked is chair okay\nnow it show showed this okay if it was\nsort of saying the right thing so I'll\njust change this prompt so I said does\nit move it said no okay so I'll say can\npeople sit on it okay can people because\nI know that it is chair can 10 people\nsit on it and I again send\nit it is saying yes\nso I again prompt it I again prompt it\nokay is it made up of wood up of\nwood yes but usually plastic chair can\nbe plastic also so\nis uh I I'm right now what I'm trying to\ndo is I'm trying to go outside the chair\nso that it does not generate a chair\nonce I ask the response it should not\nstick with the chair I giving it a\nresponse so that it it changed the\nanswer so is its color\nyellow something like\nthis no and uh does\nit have an\nengine so does it have legs does it have\nlegs it does have\nwhat is the object can does it have five\nlegs okay\nlet's does it have five\nlegs no does it have four legs does it\nhave four\nlegs no now it deviated now it deviated\nbecause now chair has a chair has four\nlegs okay now I I'll ask it okay what is\nthe object what is the object\nnow you you can see it here does it have\nfour legs it said no and it said chair\nuh let me again change this\nprompt uh is it\nbigger is it bigger than\nhouse okay let me let me just give it a\npromp it deviates many times so that it\nChang the object is it bigger than house\nuh is\nit bigger than a chair now I'll prompt\nit according is it bigger than a chair\nyes\nuh is\nit\nuh is it kept outside the\nhouse this conversion is becoming long\njust kept outside the\nhouse is it a\nchair no it is not a chair you can see\nthis okay what is it what is the object\nthe object I was thinking is a picnic\ntable you see how many of you are able\nto see it now it is basically nothing\nkeeping in keeping in memory it's not\nkeeping in the memory so whatever prompt\nI'm giving based on previously it is\ngenerating the thing okay so how many of\nyou understood this\nthing so the point here is what I try to\ndo is I try to give it a prompt try to\ngive a prompt and if it was thinking up\nfront if it was it had that memory part\nokay it kept in the memory and then uh\nit basically will just keep it on chair\nit will keep chair and it'll never\nchange the answer now what is it doing\nbasically is once I give a prompt it is\ngiving some specification so now it is\nsettling the things according to the\nspecification okay here I said is it a\nchair it said no it is not a chair okay\nwhat is the object then the object I was\nthinking of picnic table but initially\nyou you saw that before some prompts it\nwas saying it is a chair so now how\nhumans work and the GPD work is entirely\ndifferent okay these basically just\ngenerate Things based on what was\npreviously done if I if I would have\nasked you you would have never changed\nthe answer you would have given me\nspecific is it a chair you would have\nsaid yes it is a chair I was thinking of\nthe chair but here you saw that\ninitially it said that yes I was sing of\nthe chair and once I changed the prompt\nand I made it devate I made it devate\nfrom the chair okay it thought of some\nother object okay Bas that means that it\nis doing what is called as impr prompto\nit is just thinking at that time not\nthinking basically it is just generating\nwhatever is the previous context based\non that it is generating the the thing\nit is not keeping those things in the\nview as humans do it okay so this is one\nof the examples so where uh I showed you\nthat you can just see these tree of\nthoughts what they have and then it just\npicks one of them okay as as you saw\nthat initially it said chair and then\nsuddenly it changed the tree and you\nshould not basically ask uh such\nquestions which are what is called as\nout of distribution which do not follow\nad like the factual questions or if\nthere is an algorithm let's suppose and\nit needs computation to perform should\nnot ask the chat do it so what is it\nbetter for then I said it is better for\ncopying style so if you have a style you\ncan copy the style and any questions on\nthis any\nquestions sir\nwhat yes yes yes go\nahead I think can you raise your hand so\nthat I can call out\nnames yes Aran go\nahead what about logical reasoning like\nit does not do it it does not do it it\ndoes not reason it rather what it does\nas I showed you I from this point I try\nto tell you that it is not basically\nreasoning anything thing so it is seeing\nthe previous context and based on that\nit is generating the next token okay\nsame for the coding so what you\nbasically do when you ask it for a\nprompt generate me a code which adds the\nnumber so there are a lot of\ndocumentations written on uh internet\nwhen it was web was scra so here then in\nthe comments it saw that so it saw that\nuh when people write add two numbers add\ntwo\nnumbers okay and then they put a comment\nor something and then they write the\nprogram Def and something like this so\nwhen you gave this prompt okay this was\na prompt and it will this thing what\nfollowed it okay it basically is tring\nto just generate the next token it is\nnot seeing your logic so that's why\nusually people say that they don't\nunderstand what they speak so usually it\nbecause on the internet there'll be lot\nof corpuses which have in the code this\nline Okay add a number so what I'll do\nis it'll pick up from the documentation\nand God knows from where so it'll\nbasically take this as a prompt and\ngenerate next tokens as this that is\nsometime it makes so such a small\nmistake if you ask it to do something it\nmakes a small sometimes misses let's\nsuppose uh variable name it sometimes\ngive a variable name hello and it will\nsometimes use some other hle L something\nlike this it makes this minuscule\nmistake but the structure is right but\nif you see it makes these minute\nmistakes so in the code it does not\ngenerate the logic so rather what it\ndoes is whatever context you gave it\nwill generate it auto regressively it\nwill generate the next token and after\nnext token next token without giving in\nthe view that there is a logic there is\nsome constraints as you said it does it\nlook for the L it does not look for the\nlogic so if you ask it Noel problem\nwhich let's suppose is\nnowhere in the internet then it should\nbe able to solve that it should have\nbeen which it should have reasoned from\nthe first principles it should have said\nokay they want me to do this so first\nstep should be this then should be this\nthen should be this okay but it does not\ndo like that so what it basically does\nwithout constraints whatever is the\nprevious context seeing the context and\nit generates the next word and if you\nsee uh these code language are rather\neasy to replicate so if your if your\nassignment is uh if chat GPT is able to\ndo an assignment that means it is no\nnovel problem so if you are doing a\nnovel problem that chat GPT will\nstruggle because it will have no\ninstance okay no instance of this so if\nI again why will it struggle so if I\nlet's suppose tell you how to multiply\nhow to multiply two numbers okay so I'll\nshow you a generic method I'll give you\nlet's suppose two digit numbers first\ntwo digit numbers okay then I'll show\nyou okay this is all this is a generic\ntechnique you can apply it to three\ndigit I'll say 12 into 14 if you have to\nmultiply this is the mechanism take this\nfour and multiply with these two so\nyou'll get it like this then put a cross\nover here then write it and multiply\nthis number with these two so and then\nadd them together\nokay oops this will be 8 six and this\nwill one and this is how it how I I\nshowed you this so I showed a mechanism\nto do this but if you somehow gave this\nto chat so you'll have to tell it two\ndigit thing is done like this then also\nthree digit three digigit some three\ndigigit product is done like this so you\nhave to show it each instance it's not\nlearning at all it is trying to find the\npattern it's not trying to see The\nLogical reason Okay the reason is this\nif you multiply these things together it\nwill happen like this it is not trying\nto do that it is rather find the pattern\nthat's why if you ask it if you convince\nit if you say what is 9 plus 2 let's\nsuppose it will give the right answer 9\nplus 2 because there will be a lot of\nCorpus on the internet where 9 9\nmultiplied by two will be given as 18 so\nif you Tred no no no no my wife said it\nis 19 so it'll it will say yes it is 19\nthe the question is it's basically\nconversing with it is not rather doing\nthe logical reasoning okay you see the\ndifference many people have uh they put\nnonsensical prompts on this they they\nsaying oh look CH GP is making a mistake\nit is not it is not\ndoing that so that shows the ignorance\nthat you don't know the fact that how is\nit generating the thing so if you are\nforcing it to do something it will\nbecause it is trained on whatever your\ncontext you give it is generated the\nnext word based on that okay so one\nexample of that is so Caesar Cipher\npeople have written papers on this uh so\nif I have a CD okay if I want to do a c\nCER that means just substitute it so a\ncan be replaced by let's suppose a + 2\nthat means uh replace it with C okay A\nplus 2 basically means so if a to z are\ngiven some numbers like 1 to 26 okay so\nif I say A+ 2 this will be a scheme not\na plus2 maybe letter plus two so okay\nwhatever letter appears just go two\nforward and replace that with the letter\nso a will be replaced with c b with d\nand c with e something like this so This\nis called as substitution you substitute\nthese words and these are called as\ncyper text and I hope everybody knows\nthis if you are from CS background so\nyou know this uh you have a plain text\nand you can substitute the other words\nand this is what is uh and you have a\nletter so you can make a foury old kid\nyou can ask them okay if I have word\nhello okay if you do the sub plus two\njust make these words plus two let's\nsuppose h g h i so g h i and J so H\nshould be replaced with J this will be\nreplaced by J uh and then e plus 2 is so\nc a b c d e and f so this will be\nreplaced by F uh so JK L MN so this will\nbe replaced by n and n m n o PQ this\nwill be replaced by P Q so this is my C\ntext this was my plain thing and now\nthis is my Cipher okay if I now give\nlet's suppose any kid I'll say okay I'm\nsaying smart can you just make it the\ncipher text or can you let's if I give a\ncipher text the coding scheme used is\nplus two okay he can easily decode code\nand this is very because it hardly took\nme a minute or so to explain you people\nit'll hardly take me couple of minutes\nto uh teach it a four year five year kid\nand he'll be do he'll be able to\nprecisely do it but when GPD was was\ngiven this it terribly failed okay and\nit was say okay this is how these things\nare but because it learns the\ndistribution okay this actually is not\ndistri and now it actually performed\ngood on what's called as Road 13 if you\nget plus 13 that is A+ 13 so whatever is\nthe 13th character after a pick up that\nwhy is it performing good on Road 13 not\non rotation 2 rotation 5 or rotation 26\nsomething like that why is it not\nperforming good on these but on this\nbecause Linux actually supports this so\nwhen people were writing this C Cipher\nthey somehow many of the people wrote\nthis rotation 13 and now they have a lot\nof data for this Road 13 okay and from\nthe internet it learned the pattern and\nit was performing good on Road 13 but on\nthis road two Road Three it was not\nperforming good at all there are Papers\nwritten on this okay now this tells you\none more thing that it learns the\ndistribution rather than reasoning it\ndoes not reason at all okay whatever is\nthe context given the next word is\npredict based on the context so now if\nyou see any post given by any person and\nyou ask it for a weather it'll it'll see\na distribution it it does not give the\nfactual information you see if you see\nsaying 9 into 3 is somewhat let's supp\n34 all right no problem why because\nmaybe there there is data and something\nhappened and from all that context it\ngave this it generated this 34 because\nit's not trained to do The Logical\ncalculations or do the planning part of\nthe things now people came up with\nagentic Behavior now if you give it a\ntask which is mathematical so it will\ninvoke an agent which will perform\nmathematical calculations and if you\ngive it a task specific which specific\ntools are used so LM will in internally\ninvoke some other tool okay for the\ncalculation it will generate it will\ninvoke calculators for the let's suppose\nuh doing of image generation it'll\ninvoke some other model but in turn if\nyou see these things are not even\nexpected because of the architecture and\nbecause of what we expect from language\nmodels and we try because it is trying\nto uh see the distribution and the\nbetter point is that it should be used\nfor idea generation okay idea generation\nwhy why this because idea generation\nrequires a lot of knowledge okay it\nrequires a lot of birth knowledge maybe\nin-depth knowledge is a separate thing\nbut if you have a lot of breadth\nknowledge and uh then you can come up\nwith good ideas and but it should never\nbe used as a\nverifier okay verifier it does not\nperform good why because for\nverification you need to have\nconstraints of logic and reasoning so\nhere you need\nconstraints so let's suppose if you are\nif you want to build something just take\nidea from it because it will have a lot\nof domain know different domains and one\nway of seeing the large language models\nis compressed version of Internet so\nit's compressed\nversion so people might be thinking that\noh there are so many parameters we have\nbillions of parameters you should be\nthankful that there are only billions of\nparameters think of the data data that\nis on the internet it is huge it is huge\nand then we have a basically loss lossy\ncompression there is a lot of loss in\ncomparation but you only got let's\nsuppose 400 billion parameter model and\nit is performing really nice so what is\ntrying to do is compressing all that\nknowledge together and then giving us\nthe answer so it is a smaller version so\nif you if you compare it the human\nprobably human will have a lot more\nparameters maybe hundreds of more in the\npower in the magnitude of 100x of\nparameters than just the state of the\nart GPT that we might have state of the\nart large language model that we have\nyou can think it as a compressed version\nof whatever was the intern whatever data\nit was given it tried to compress that\ninto its weights and B into those\nparameters okay and then I'm saying is\nbecause now it has the vast knowled it\nknows things about it does not know\nbasically it uh just have the patterns\nit has patterns about biology it has\npatterns about mathematics it has\npatterns about astronomy everything so\nif you want to work on something so the\nbetter could be you could ask it to\ngenerate things the idea generation and\nthen you can be a discriminator you can\nbe a verifier okay this thing is wrong\npick up this this thing is right do do\nthis now people tell you it will reason\nprobably some other models will re but\nthis language model if you don't come up\nwith with new artical chains and you\nstill try to increase the parameter and\nuh expect that they will come near to\naren intelligence or they will do\nsomething I don't I'm on the other party\nI'm thinking that you should come up\nwith new techniques you should not\nrather rely on only attention and then\nkeep on increasing the parameters keep\non increasing data and feed them and you\nmagically think that they'll be able to\nprobably and many people do think that\nthis is right now if you see these\nlanguage models this is their limit so\nabout that they cannot do and you have\nto come up with the new techniques so as\nto incorporate with these things so I'm\nokay with that people are saying that uh\nthere will be some new techniques some\nnew methods new architectures which\nprobably will take us towards reasoning\nor towards planning towards these things\nbut if you right now tell me that these\nlanguage models if given more data and\nmore parameters we be able to achieve\nthat so I would cly disagree probably\nany\nquestions I think MF you had a\nquestion yes sir\nright it depends on the context Contex\nwindow so if context now context window\nis 1 million tokens 1 million is huge\nthing\nokay let suppose you're the context it\nwill miss the context it will have no\nidea about that okay but you should see\nthe contest length contest length how\nmuch is the contest length and after\nthat if you exceed that contest length\nso what will happen is it won't know\nwhat came before that so it will\ngenerate it is basically sliding window\nsort of thing so let's suppose I'll\nrestrict let's suppose Contex window\nlength is 10 so what will happen is so a\nb c d something like this so after this\nis after this let's suppose till here it\nwas 10 now okay once it came to the 11\ntoken so it will shift this it will now\ntake this much as context once you again\ngenerate it'll take this much as context\nokay it will keep on forgetting what was\npreviously what was not what was\nbasically in the context but now it it\ncontest length was uh that was a cap\nit's contest 10 so it'll keep on sliding\nokay it won't take these two tokens into\nconsideration now so after that it'll\nkeep on moving that these are called as\nlong form long range dependencies which\nprobably because they came up after RNN\nRNN have this problem of long range\ndependencies that's why now they have at\nleast 1 million is very easy people are\nnow giving a lot of tokens and I don't\nknow what's the current state of the art\nyou might just search the internet and\nask it how what's the largest contest L\nthat a language model supports okay and\nin millions easily in millions okay but\nif you exceed that so uh the theory and\nthe practice is that it won't remember\nwhat you what you said before that and\nit it won't generate it won't take\nbasically those things in the account\nwhile generating the next token okay is\nit\nclear yes second\nquestion right\nthinking yes yes it is very hard now and\npeople are trying to come up with tools\nyes and there's a good thing so if you\nknow there there's a facility where\npeople are keeping all these crops what\nsub so they thinking that if this whole\ncivil D destructed so we should\nsomewhere have this thing so that they\ncan start the civilization and there\nthere is that I exactly don't know the\nname I forgot the name so that is that\nthing where they keep these different\ntypes of crop seeds and sort of things\nand they preserve them so that if let's\nsuppose some catastrophe happened if\ncatastrophe happened everything got lost\nand only that facility is there and\nthey're making the facility such that it\nwould not be distracted by that and\nafter that humans are there they should\nbe able from there they should take\nthese seeds and sort of thing they\nshould start the thing again so same\nshould be done with the internet now now\ntill suppose large language models were\nthere so we had all the data was from\nthe humans at least more than 95% data\nwas from humans and now your models were\ntrained by using this data okay once\nthey were trained using this data what\nhappened is and now you'll mix this data\nfrom the humans and from this and now\nyou don't actually internet should be\nsnapshot there should be a snapshot of\nthe internet kept uh once these models\ncame and people started training them\nnow each of the message emails they're\nbasically written by these and co-pilot\nand code is even written so so now we\nhave What's called the self referential\nproblem so a student can be as good as a\nteacher if the student doesn't have the\nintelligence the same thing happens with\nthe GPT because they have this self\nreferential problem so whatever humans\ndid they can do it best up to only that\nlevel okay LM cannot be better than the\nbest speaker in the world it cannot be\nbetter it cannot be better than the best\nwriter in the world why because if that\nis the best writer that is the cap\nbecause it's trained on human data and\nit tried to learn the human distribution\nokay as I said usually I usually say\nthis that it is trying to simulate\nintelligence it is not being intelligent\nso if it was intelligent if you gave it\nthings till relativity till gravitation\nso all the physics was given to\ngravitation if it was able to predict\nrelativity okay if it was able to come\nup with these concept then it was\nintelligent now why I said it's just\nsimulation it is not intelligent\nBehavior it is simulation so what it\ntries to do whatever knowledge it get it\nwill generate just hul Spokes and\nmixture of those things and it might\nlook intelligent it might look like that\nbecause as I said it's very hard for a\nhuman to differenti what is intent what\nis not okay and because it is mimicking\nthose things but if you see real in real\nshould lie here so if you give it uh\nsuppose knowledge of physics still\ngravitation it should come up with the\nrelativity but it does not do that okay\nso I hope that you got this notion at\nleast that it is not being intelligent\nit is simulating intelligence okay so\nnow I believe that internet should be\nsnapshot it and I think that people will\nhave the data set it will be pure for\nhumans so because language model will be\nas good as human the best human yeah I\nthink father is a\nquestion uh sir as you have said\nmultiple times previously that the top\nscientists are working on solving this\nproblem of intelligence like if large\nlanguage models are just trying to\nsimulate intelligence then how can they\nexpect them to uh be self-aware and\nintelligent not with that's what I said\nin if you just heard me carefully a\ncouple of minutes ago I was saying that\nthe techniques are used in large English\nmodels they have this limitation but but\nif you have to come up with something\nyou have to come with new techniques so\nif you have this concept of\nConsciousness if you want to come to\nclose to AGI and things like that you\nshould come up with new techniques now\nthe problem many of the people are\nworking in uh just increasing the\nparameters and using the same techniques\nbut if they come up with new techniques\nthen I have no problem of thinking this\nyes they can come closer but by using\nthis what they saying is there many\nother people they're saying if we\nincrease the parameters and then use the\nsame architecture probably it will learn\nto reason but that's not the case if you\ncome up with new techniques then maybe\nand then all better is maybe yes you\nhave a followup question sir I want to\nask is intelligence learned or it is\npresent inside us right from when we are\nborn like intelligence is that's what\nthat's that's the issue intelligence is\nnot learned so every human is\nintelligent okay now there will be\nlevels of intelligence okay and it is\nvery hard to Define intelligence\naltogether okay that's why we are\nstruggling to tell if there was a\nspecific definition of what intelligence\nis we have directly said that jgpt or\nthe LM they not intelligent they're just\nsimulating elligence because there is no\ndefinition and even that's the problem\nwe are facing in education system also\nso should we now ask people to solve as\nmany problems because the point is now\nyou have coaching centers and this is\nvery unfortunate to say that these\npeople are more trained they are like\nGPT they're simulating they're not being\nintelligent and I have seen people I'll\njust give an instance I have seen I\nstudied among these rankers IJ rankers\nto what happened is I'll tell you an\nexample from a physics guy so and most\nof you will laugh at this\nthe the physics guy so if you see this\nthis is a pressure\ncooker okay and we're cooking something\nand now you know that pressure cooker\nonce it boils it has a lot of pressure\ninside and if you try to open it and it\nsomehow you opened it because of the\npressure it'll go upwards and it'll\nthere will be a huge blast and it'll\ncreate a mess so that guy knew how to\nsolve the equations and because it was\nhard CED in him so he tried to open the\npressure cooker and I was asking hey man\nyou are a physics major and that to\nphysics copper so don't you think that\nif you open this that blast okay that is\nwhere the intelligence do kicken because\nhe was so trained in formal thinking he\nwas so trained in give me an equation\nand this is how it should be solved and\nthis is how it can be done okay that's\nthe problem with these training programs\nso if you uh take these people and then\nask them to uh do training for ID let's\nsuppose for three couple of years three\nfour five years and then they basically\nsee the patterns usually what I say is\nit is not difficult doing assignment the\ndifficult is come with an assignment\ncome with new questions okay there are\ndifferent websites which have all these\nquestions uh previous questions now the\nactual task is done by the professor who\ncomes up with new questions who comes up\nwith new question which was not\npreviously asked which was not previous\nin any pattern okay if somebody can do\nthis so that is one level of\nintelligence because he has to\nunderstand every material okay that's\nwhy doing research is hard why because\nyou have to first identify the problem\nitself you don't know the problem okay\nand you read all things and you see a\nproblem first and coming up with this\ngood problem is a huge task sometimes it\ntakes years to do this and you we have\nPhD students here and they know MF will\ntell you how hard it is to come with\nactual problems this is the problem and\nthis is what this is we want to solve\nokay and then people take any problem\nthey okay I want to do this thing so it\nis better so coming up with a good\nproblem and coming up with let's suppose\nif you have knowledge of something and\ncoming up with good questions about that\nis actually a hard task so that is an\nintelligent Behavior but very this is at\na vague level but I can't Define what\nincy if I defined them then problem\nsolved I wouldn't have con yes I wanted\nto say\nsomething\nresearch resarch\n[Music]\nto humans idea I said I told you that\nidea generation is it is good for that\nbecause idea generation it has a lot of\nknowledge about the world it has all\nthese mathematics physics to IDE but now\nyou have to validate are these ideas\ngood you have to validate that okay but\nif let's suppose if you ask it this is\nthe physics problem okay knowledge now\nbecause it knows it knows this stuff and\nit can come up with the ideas but let's\nsuppose if it has to find an actual\nnovel thing it won't be able to do that\nokay but if you now have a human let's\nsuppose take Einstein Einstein studied\nabout light he had the information give\nthe same information to GPT this is\nthere a property of light this is the\nmaterial about light okay what can you\nsay about light it will say nothing it\nit'll basically say something but they\nwon't have the content value they have\nstyle value but if you give a human\nbecause human has a think ability to\ncome up with new things so they'll come\nup with these new things so they'll come\nup with because they are not simulating\nintelligence they are actually being\nintelligent but as I said idea\ngeneration in the longer context can be\nseen and yes and even those ideas might\nbe written by somebody on internet\nbecause it has now a cap of this you\nthink like this you are a human over\nhere okay how many connections you have\nprobably you are less connected you may\nask me you may ask some friends okay\nwhat but chat GPT has the access to the\nentire internet's knowledge whatever\nthere was it basically means it can\ncontact 8 billion people okay this is\nnot a Fair competition at all if you ask\ngenerate ideas and gener ideas to human\nbecause human are restricted by whatever\ntheir knowledge but if you give them\ntime let's suppose you give ji time and\nyou humans you give hum one month then\nyou will humans will come up with\nsomething novel which basically they did\nnot verify from somewhere but it was\njust out of thinking what they thought\nokay but if you give the chat GPT you\nwill see that chat GP did give some idea\nGenerations but they were not not as\nNoel as humans did it because it just\ntried to uh just simulate whatever was\nthere whatever was on the web and just\ntried to from the context tried to\ngenerate whatever the knowledge it had\nand it it tried to generate those tokens\nas I said but humans actually having\nintelligence will will keep on thinking\nabout it we'll keep on thinking but\nthat's the problem with these language\nmodel you ask them they stop you train\nthem they stop stop but let's suppose I\nspeaking to you right now because humans\nnever shut down till death okay even\nwhile you are sleeping you're still\nthinking about something and God knows\nwhat you're thinking God knows how your\nthoughts are processing but these models\nlet's suppose uh if you train them and\nif you deploy them in a system and if\nyou don't retrain them continuously they\ndon't do anything on their own but\nhumans if you see right now we're\ntalking about things so what will happen\nafter a while is maybe subconsciously\nyou have your own brain and in the in\nthe dreams also you'll think about the\nand there'll be there's a complex\nprocess going on and that is what uh\nseems as intelligent Behavior to me\nthat's what leads to intelligent\nBehavior but I don't think the LMS do\nthat okay and they don't uh process the\ninformation like that and then there\nshould a mechanism of doing that now\npeople are doing reinforcement learning\nhuman feedback and they're keeping these\nmachines and whatever they generate and\nthey should learn from it and again it\nwill be a self problem because it does\nnot have that I think ask a question did\nI answer your question\nbuf yes sir it can give you a pro\nproblem but it cannot tell you the\nfeasibility of the problem yes yeah huh\nyes yes that's that's a good point so\nlet's suppose if you ask it to generate\na beautiful house it a beautiful house\nbut that house because of structural\nproblems won't ever exist okay it let\nsuppose make a tajal it will say you can\nput to over here you can put to over\nhere and then you can color this and\nthat but that tajal won't you won't be\nable to make why because it's not\nfeasible in the real world can because\nit did not it did no calcul but if you\nask hum humans will think they have\nconstraint okay if I put a structure\nmaybe the structure is not feasible at\nall so it may generate things which are\nnot feasible which are not realistic and\nthey might look good that that's the\nissue they might look good and people\nmight get convinced oh it looks good\nshould I try it with this no because why\nit generated because the way it is it\nwas trained it will generate something\nvery very good looking but that won't be\nthe reality yes as had a question yes\nyes you are I didn't get\nyou humans are very Superior very\nSuperior yes as\nI had a question how these models are\ncontinuously trained like sometimes\nthere are new research paper or new\nresearch coming up how the how the\nVersion Control or how these models are\ncontinuously trained so yes I'll tell\nyou the things like uh what happens is\nusually when Transformers came they do\nwhat's called as architectural\nchange okay AR change are done so they\ndo different let's suppose if you see\nGPT architecture and if you see Lama\narchitecture both of them are would\nallow to discuss with you once we go to\nlanguage modeling part so you will see\nthat the framework they designed on is\npretty much similar but the internals\nare quite different let's suppose this\nalso used embeddings it us embeddings\nokay and this use something called as it\nuses positional embeddings this use\nsomething called as rotational\nembeddings okay here also it uses\nattention this called as self attention\nhere it uses something called as\nattention with cavic Cas group multi\nattention Okay now if you see indivual\nParts you will see corresponding thing\nthere but they have complicated a bit\nand and if you see here they use the\nsimple Norm layer and here they use\nsomething called as RMS Norm that's one\naspect so they do some Aral chain and\nthey replace the things with this okay\nand other thing is that they come up\nwith some novel algorithms to infuse\nlet's suppose they'll introduce some\nother things in here they'll introduce\nsome other algorithms okay that's one\ndimension the one dimension is actually\nchange then you can have some other you\ncan you can come with new techniques\nalso come up with new techniques and do\nthat and maybe there are a lot of\nimprovements you can change the weight\nrange you can change it the way it INF\nfirst you can uh try to come up with a\nlot of uh Mumbo and\njumbo and let's there two ways to think\nabout it one thing is from experiment\npoint of view experimentalist will think\nthat we We'll add on these things and\nwe'll experiment and from a theorist\npoint of view he'll see okay let me go\nwith the theory so if I do this and what\nwill happen to this and he'll try to\nFirst understand From thetical\nperspective and then try to do it now\nthat AI has become like this it has\nbecome like\nmarketing very sorry to say this but it\nhas become like that if you you should\nbe the first person to get it with the\nmarket rather science should be like\nthis doesn't matter that if somebody\nelse published it if I'm not convinced\nthat this will give something it should\nnot be published at all but now people\nare in the race whatever they find they\ndo publish it okay but actually it it\nshould have been verified and now the\nproblem is there so somebody does a\nresearch and they just found one example\nthat it is doing something like this and\nthey publish it and people try to pick\nup that research and they cite it and it\nbecomes a whole line and someday some\nguy claims the root thing was wrong okay\nthis is not how it how it does whole the\nresearch go goes down to the gutters\nthat's how it is happening right now why\nbecause the funds associated with it\nbecause of the grants and because of\nthis AI people want to get the papers\naccept this and that because if you\nwrite something about LMS it's a there's\na high chance that people will accept it\nokay but this is how the research\nbasically should be done so it usually\nshould have some oil change and they\nshould have different mechanism of\ntraining also and maybe within within\neach component can be modified so you\ncan think of modification of each\ncomponent maybe all together you can\nscrap the whole architecture of this you\ncan come up with new things all together\nokay that's how they are doing it now if\nyou see these uh The major portion\nthat's happening right now they're\nincreasing data and increas number of\nparameters and doing a lot of\ncomputation so that's why Sam Alman\nrecently asked some 7 trillion dollar\ndollars it's not even economy of many\ncountries is less than that okay so he\nwas asking because why because they want\nto get bigger gpus because they're\nthinking if you feed more data to it and\nwith these AR everything and we can come\nup with this but if you see humans\nhumans are ined because they want to\nmimic human brain okay I I just try to\ngive you the sort of birds view what are\nthe different things that people do they\ndo AR change They do change in training\nand they try to come up with new\nalgorithms they try to come up with new\ndata and the best part is for me is the\ncoming up with new techniques if you\ncome up with new Techni suppose\nattention was a technique that transform\nintroduced so whatever you see language\nmodels performing that technique was\nintroduced in 2017 okay that's the paper\nis titled that attention is all that you\nneed attention is all that you\nneed and uh it was published by\nGoogle okay and then open a basically\nused it and the open a was funded by\nMicrosoft and basically the rivals did\nit so they publish research and they did\nnot know how much amazing this could be\nso opena used in their gpts and then\nMicrosoft is basically partially funding\nopener and uh they made these and even\nSatya said that we made Google dance\nbecause Google had a project running so\nthey had something called as Lambda okay\nand they were not anticipating that some\nsomebody else would use these techniques\nand come up with the model first so\nopeni basically released and there was\nno prior notice Google did not know that\npeople are doing this and then in that\nwave they released that b and it was a\ndisaster okay and because they were not\nready to because they had a plan usually\ncompanies do have a plan okay in one\nyear we'll do this in two years we'll do\nthis in three years they had a plan but\nonce this open a released chat GPT and\nthese people Google people were forced\nand there was a code r that time every\nof my professor who was working in\nGoogle because I was at that time doing\nMasters so and they were asked to come\nto the office okay because they wanted\nto see because they had a threat because\nif GPT was performing and they didn't\nknow what happened people were coming\nwith perplex and this and then they were\nthinking because our main stream of\nincome is search so if somehow GPT and\nthese language models can be used for\nsearch then our business model is gone\nand they had the code alert and you can\nsee the beauty of the research so if you\ndo research on this front because they\ncome with new technique I was talking\nabout techniques so they came up with\ntechnique of attention Okay and after\nthat GPT came with just one concept of\nattention or from the Transformers they\ncame up with these large language models\nso if you come up with good Concepts\ngood uh sort of uh things that can be\nused somewhere else and that's that's\nthe way research should be conducted not\nthat you just got something you want to\npublish it okay you can do oranic change\nand you can learn about a lot of stuff\nand then come up with new techniques so\nas to revolutionize a field rather than\njust seeing what is it taking and if you\njust get a result maybe we'll get some\ngrants we'll get a result accepted\nthat's a very wrong way to look at\nit I think fad is a question yes\nfad sir I wanted to ask this question a\nlong time ago like in this lecture I\nwanted to ask that um is intelligence\njust the way the structure of our brain\nthe structure of our neurons and how\nthey are arrang their Arrangement is it\nwhat constitutes intelligence or what\nand if it is then how why can't we\nreplicate it oh very hard question\nbecause we don't know about brain we\ndon't know anything we know very little\nabout brain and that's why we're trying\nto mimic so whatever we're doing in deep\nlearning whatever we're trying to do in\nthis uh recurrent neural networks feed\nfor they're all inspired from brain okay\nbut if I if you ask me uh we have have\nnot understood the brain fully we have\nunderstood many very minuscal parts of\nthat that's why people are not\ninterested in biology Neuroscience they\nwant to see different aspects and they\nwant to do it computationally can we\nperform the computations on this so if\nyou see all of these things are inspired\nfrom the brain but as humans the\nlimitation is that we don't understand\nthe brain itself and there is a huge\nresearch going on on the brain on brain\nof rodents and different things what\nconut intelligence how these neurons are\nconnected and that's what led to the\nCuriosity and after that Deep larning\nField came into existence and it took\noff recently by recently I mean in I\nthink 2006 onwards it took off okay and\nafter 2012 it became uncontrollable and\nnow we are still in that bubble where\npeople are learning about these things\nif many of the uh and people do contest\nagainst this Nobel Prize giving to the\nAI people so they're saying that they\nare stopping this Frontier research all\nof this happened because of Frontier\nresearch so by Frontier research I mean\nyou should not see just applications\nresearch will find some application\nsomething will not but if you give a\nprize noal priz to the AI for chemistry\nyou gave it to the baker lab and\nfor uh physics you gave it to the Hinton\nand hfield Hinton came up with the back\npropagation and hfield came with what's\ncalled as hfield networks and they they\nhave their links in the in the competen\naspect they were computer scientists and\nnow people are saying if you give PRI\nyou sending a wrong message the message\nis that you should pursue these fields\nwhere there is money because of\ncapitalist society and you should not\npursue the frontier research the\nfrontier research is rather that where\nyou you just are curious and you you\nwant to discover it because you because\nyou have curiosity not that there is\nsome implication direct implication a\nit's good thing to find applications but\nthe frontier research is not necessary\nthat you should find it so if you see\nmathematics and Mathematics is just a\ntool and people should not study\nmathematics because if you just ask okay\nwhy is Abra algebra probably I won't\nknow why it get use and maybe someday\nsomebody will use it but this should not\nbe the case that people will just uh\ngive the prizes and fund the things that\njust seem practically to them okay and\nregarding the human brain that's why\npeople are thinking okay will they stop\nthe research funding in this also so if\nsome people want to study about brain\nbecause it does not probably see uh the\ndirect implications in the real world so\nwill it will it be the case that they'll\nstop the frontier research and should\nshould not be the case so short answer\nis that we know very DET about the\nbrains and the more we know and probably\nmuch techniques more techniques will\ndevelop and where intelligence comes\nfrom nobody knows it has some links with\nConsciousness and what Consciousness is\nwe don't know and to to quote the\nscripture and it is said that we know\nvery little about the\nrule any other\nquestions Anar has not asked any\nquestion yes\nno I don't have any\nquestion okay did you at least got some\nperspec about the Genera Ai and in in\nparticular language models at least some\nsort of okay what you should expect from\nthese\nmodels yes okay you should not now if\nsomebody puts a LinkedIn post hey I\nasked JG this you should laugh at that\npost so but I'm saying what we should\ntalk about so\nI yes Yes actually I can stop the\nrecording now\n",
  "words": [
    "hello",
    "welcome",
    "everyone",
    "sort",
    "session",
    "discuss",
    "usual",
    "uh",
    "tex",
    "stuff",
    "usually",
    "uh",
    "usually",
    "discuss",
    "uh",
    "technical",
    "details",
    "go",
    "depth",
    "today",
    "session",
    "dedicated",
    "said",
    "long",
    "due",
    "uh",
    "session",
    "said",
    "talking",
    "uh",
    "generative",
    "ai",
    "general",
    "take",
    "llms",
    "particular",
    "would",
    "love",
    "start",
    "questions",
    "people",
    "questions",
    "straight",
    "away",
    "shoot",
    "start",
    "go",
    "little",
    "bit",
    "deeper",
    "also",
    "lms",
    "generic",
    "session",
    "architecture",
    "details",
    "train",
    "maybe",
    "touch",
    "aspects",
    "less",
    "generic",
    "session",
    "questions",
    "genera",
    "air",
    "work",
    "something",
    "like",
    "sir",
    "yes",
    "music",
    "llm",
    "train",
    "specifications",
    "need",
    "hardware",
    "softwares",
    "least",
    "big",
    "llm",
    "even",
    "small",
    "170",
    "100",
    "billion",
    "parameters",
    "100",
    "billion",
    "lot",
    "yes",
    "yes",
    "tell",
    "hardware",
    "specifications",
    "usually",
    "uh",
    "depends",
    "parameters",
    "okay",
    "depends",
    "parameters",
    "okay",
    "question",
    "answer",
    "question",
    "first",
    "start",
    "bit",
    "generative",
    "ai",
    "afterwards",
    "maybe",
    "questions",
    "keep",
    "discussion",
    "engaging",
    "ask",
    "many",
    "questions",
    "possible",
    "uh",
    "generic",
    "session",
    "might",
    "help",
    "lot",
    "might",
    "using",
    "geni",
    "tools",
    "right",
    "context",
    "use",
    "many",
    "people",
    "notion",
    "used",
    "sometimes",
    "gives",
    "nonsensical",
    "answers",
    "claim",
    "tool",
    "working",
    "good",
    "talk",
    "gen",
    "okay",
    "let",
    "shift",
    "screen",
    "wo",
    "write",
    "much",
    "things",
    "talk",
    "generate",
    "ai",
    "generation",
    "okay",
    "generating",
    "basically",
    "point",
    "uh",
    "humans",
    "generate",
    "humans",
    "generate",
    "data",
    "humans",
    "okay",
    "uh",
    "generally",
    "generate",
    "data",
    "let",
    "suppose",
    "speak",
    "generate",
    "data",
    "devic",
    "also",
    "generate",
    "data",
    "let",
    "suppose",
    "capture",
    "uh",
    "uh",
    "photos",
    "using",
    "cameras",
    "sensors",
    "lot",
    "lot",
    "tools",
    "humans",
    "guide",
    "tools",
    "generate",
    "data",
    "generate",
    "data",
    "point",
    "meaning",
    "generate",
    "ai",
    "want",
    "uh",
    "something",
    "generated",
    "without",
    "let",
    "let",
    "suppose",
    "equipment",
    "let",
    "talk",
    "image",
    "see",
    "image",
    "okay",
    "need",
    "camera",
    "something",
    "either",
    "camera",
    "artist",
    "draw",
    "uh",
    "draw",
    "image",
    "maybe",
    "means",
    "want",
    "actually",
    "want",
    "machines",
    "actually",
    "want",
    "machines",
    "generate",
    "images",
    "know",
    "go",
    "little",
    "bit",
    "detail",
    "see",
    "image",
    "pixels",
    "bunch",
    "pixels",
    "okay",
    "somehow",
    "uh",
    "pixels",
    "arranged",
    "certain",
    "fashion",
    "make",
    "sense",
    "human",
    "okay",
    "let",
    "suppose",
    "want",
    "generate",
    "image",
    "cat",
    "image",
    "dog",
    "image",
    "human",
    "okay",
    "task",
    "find",
    "pixel",
    "values",
    "arrange",
    "certain",
    "way",
    "makes",
    "sense",
    "humans",
    "makes",
    "sense",
    "uh",
    "people",
    "seeing",
    "say",
    "face",
    "okay",
    "task",
    "generation",
    "genera",
    "ai",
    "basically",
    "uh",
    "generates",
    "new",
    "content",
    "including",
    "text",
    "images",
    "audio",
    "video",
    "based",
    "pattern",
    "learns",
    "data",
    "okay",
    "data",
    "learns",
    "patterns",
    "data",
    "tries",
    "generate",
    "data",
    "called",
    "distribution",
    "okay",
    "talked",
    "talked",
    "bit",
    "distributions",
    "data",
    "coming",
    "underlying",
    "distribution",
    "gen",
    "algorithms",
    "try",
    "try",
    "mimic",
    "distribution",
    "tr",
    "find",
    "patterns",
    "distribution",
    "data",
    "okay",
    "data",
    "analyze",
    "data",
    "see",
    "different",
    "pattern",
    "try",
    "figure",
    "patterns",
    "try",
    "come",
    "pattern",
    "okay",
    "try",
    "come",
    "things",
    "slowly",
    "formulate",
    "towards",
    "broader",
    "sense",
    "generative",
    "ai",
    "means",
    "physical",
    "equipments",
    "right",
    "camera",
    "take",
    "photograph",
    "human",
    "speaking",
    "uh",
    "happen",
    "machines",
    "generate",
    "data",
    "generation",
    "task",
    "different",
    "generating",
    "language",
    "different",
    "generating",
    "image",
    "different",
    "generating",
    "audio",
    "different",
    "work",
    "different",
    "principles",
    "lot",
    "algorithms",
    "lot",
    "architectures",
    "came",
    "along",
    "way",
    "uh",
    "genera",
    "ai",
    "domain",
    "see",
    "started",
    "basic",
    "things",
    "came",
    "generator",
    "adversarial",
    "networks",
    "diffusion",
    "models",
    "transformer",
    "networks",
    "took",
    "whole",
    "world",
    "storm",
    "okay",
    "uh",
    "lot",
    "things",
    "transform",
    "architecture",
    "bit",
    "uh",
    "genera",
    "ai",
    "means",
    "genera",
    "ai",
    "generating",
    "new",
    "types",
    "data",
    "saying",
    "new",
    "somehow",
    "mix",
    "match",
    "previous",
    "data",
    "whatever",
    "pattern",
    "sample",
    "already",
    "sampling",
    "problem",
    "let",
    "suppose",
    "gave",
    "generate",
    "data",
    "showing",
    "sample",
    "already",
    "data",
    "generation",
    "sampling",
    "generation",
    "basically",
    "means",
    "thing",
    "exist",
    "thing",
    "exist",
    "within",
    "data",
    "let",
    "show",
    "example",
    "uh",
    "got",
    "interested",
    "generat",
    "give",
    "second",
    "bring",
    "website",
    "see",
    "website",
    "called",
    "think",
    "people",
    "type",
    "uh",
    "people",
    "exist",
    "person",
    "person",
    "see",
    "image",
    "person",
    "exist",
    "world",
    "see",
    "face",
    "okay",
    "see",
    "pretty",
    "much",
    "details",
    "okay",
    "looks",
    "realistic",
    "picture",
    "see",
    "okay",
    "refresh",
    "pop",
    "new",
    "picture",
    "reality",
    "people",
    "exist",
    "bunch",
    "data",
    "sampling",
    "showing",
    "actually",
    "generating",
    "samples",
    "okay",
    "maybe",
    "generated",
    "samples",
    "kept",
    "somewhere",
    "website",
    "purpose",
    "showing",
    "people",
    "exist",
    "okay",
    "got",
    "sort",
    "interested",
    "genera",
    "ai",
    "think",
    "based",
    "genera",
    "adversarial",
    "networks",
    "might",
    "maybe",
    "speak",
    "architecture",
    "stuff",
    "even",
    "uh",
    "meta",
    "also",
    "let",
    "suppose",
    "cla",
    "platforms",
    "give",
    "services",
    "people",
    "uh",
    "image",
    "generation",
    "part",
    "also",
    "also",
    "generate",
    "text",
    "okay",
    "showed",
    "hope",
    "uh",
    "guys",
    "relate",
    "know",
    "problem",
    "generation",
    "sometimes",
    "get",
    "confused",
    "generation",
    "sometimes",
    "basic",
    "might",
    "mean",
    "sampling",
    "sampling",
    "trying",
    "come",
    "new",
    "samples",
    "rather",
    "existing",
    "samples",
    "data",
    "give",
    "data",
    "wo",
    "sample",
    "uh",
    "data",
    "point",
    "data",
    "rather",
    "try",
    "see",
    "distribution",
    "analyze",
    "distribution",
    "try",
    "remember",
    "distribution",
    "generate",
    "new",
    "sample",
    "distribution",
    "okay",
    "let",
    "speak",
    "things",
    "distribution",
    "see",
    "probability",
    "also",
    "people",
    "know",
    "distribution",
    "let",
    "suppose",
    "take",
    "height",
    "people",
    "okay",
    "take",
    "height",
    "people",
    "height",
    "people",
    "see",
    "range",
    "range",
    "lot",
    "see",
    "height",
    "let",
    "suppose",
    "starts",
    "say",
    "let",
    "suppose",
    "10",
    "million",
    "people",
    "range",
    "let",
    "suppose",
    "something",
    "like",
    "let",
    "suppose",
    "max",
    "height",
    "let",
    "say",
    "okay",
    "different",
    "values",
    "uh",
    "height",
    "let",
    "call",
    "h",
    "okay",
    "h",
    "uh",
    "height",
    "person",
    "height",
    "person",
    "may",
    "vary",
    "okay",
    "case",
    "might",
    "see",
    "see",
    "let",
    "suppose",
    "total",
    "let",
    "suppose",
    "1",
    "billion",
    "people",
    "let",
    "suppose",
    "100",
    "people",
    "maybe",
    "100",
    "people",
    "might",
    "see",
    "let",
    "say",
    "50",
    "people",
    "much",
    "height",
    "say",
    "50",
    "let",
    "suppose",
    "10",
    "people",
    "12",
    "people",
    "height",
    "basically",
    "distribute",
    "okay",
    "see",
    "uh",
    "height",
    "distributed",
    "many",
    "people",
    "much",
    "percentage",
    "people",
    "usually",
    "comes",
    "like",
    "like",
    "okay",
    "let",
    "suppose",
    "mean",
    "height",
    "okay",
    "mean",
    "height",
    "let",
    "least",
    "height",
    "max",
    "height",
    "1",
    "okay",
    "count",
    "uh",
    "people",
    "see",
    "something",
    "like",
    "see",
    "people",
    "fall",
    "average",
    "height",
    "okay",
    "one",
    "type",
    "distribution",
    "call",
    "normal",
    "distribution",
    "symmetrical",
    "distribution",
    "distribution",
    "mean",
    "certain",
    "values",
    "okay",
    "uh",
    "population",
    "usually",
    "called",
    "population",
    "statistics",
    "population",
    "distributed",
    "amount",
    "population",
    "percentage",
    "population",
    "takes",
    "value",
    "value",
    "look",
    "around",
    "look",
    "whole",
    "population",
    "arrange",
    "certain",
    "order",
    "let",
    "arrange",
    "height",
    "okay",
    "height",
    "axis",
    "height",
    "let",
    "suppose",
    "number",
    "people",
    "see",
    "aspect",
    "uh",
    "total",
    "number",
    "height",
    "people",
    "see",
    "much",
    "density",
    "basically",
    "shows",
    "much",
    "people",
    "called",
    "distribution",
    "similar",
    "words",
    "also",
    "let",
    "suppose",
    "english",
    "words",
    "okay",
    "write",
    "english",
    "words",
    "exist",
    "dictionary",
    "collect",
    "text",
    "documents",
    "words",
    "okay",
    "collect",
    "corpus",
    "corpus",
    "basically",
    "means",
    "lot",
    "text",
    "documents",
    "find",
    "distribution",
    "words",
    "say",
    "okay",
    "word",
    "exists",
    "let",
    "suppose",
    "documents",
    "find",
    "distribution",
    "find",
    "distribution",
    "corpus",
    "first",
    "example",
    "heights",
    "people",
    "saying",
    "find",
    "distribution",
    "heights",
    "height",
    "distri",
    "distributed",
    "okay",
    "take",
    "normal",
    "person",
    "chance",
    "whole",
    "delal",
    "probability",
    "stats",
    "see",
    "detail",
    "wanted",
    "give",
    "sort",
    "glance",
    "mean",
    "distrib",
    "say",
    "distribution",
    "take",
    "words",
    "english",
    "language",
    "okay",
    "take",
    "lot",
    "data",
    "lot",
    "corpus",
    "lot",
    "documents",
    "internet",
    "follow",
    "specific",
    "distribution",
    "follow",
    "distribution",
    "distributed",
    "somehow",
    "let",
    "100",
    "documents",
    "easily",
    "say",
    "word",
    "exist",
    "many",
    "documents",
    "word",
    "exists",
    "many",
    "document",
    "come",
    "distribution",
    "okay",
    "uh",
    "sort",
    "like",
    "uh",
    "say",
    "minuscule",
    "version",
    "distribution",
    "intely",
    "go",
    "detail",
    "studying",
    "probability",
    "stats",
    "see",
    "distributions",
    "actually",
    "mean",
    "assume",
    "least",
    "clarity",
    "distributions",
    "okay",
    "particularly",
    "people",
    "undergrad",
    "might",
    "heard",
    "terms",
    "asf",
    "uh",
    "aran",
    "might",
    "bomas",
    "people",
    "least",
    "got",
    "idea",
    "distribution",
    "distribution",
    "nothing",
    "basically",
    "certain",
    "values",
    "values",
    "distributed",
    "okay",
    "values",
    "taken",
    "okay",
    "height",
    "height",
    "let",
    "suppose",
    "uh",
    "continuous",
    "range",
    "values",
    "people",
    "take",
    "values",
    "makes",
    "distribution",
    "similarly",
    "context",
    "natural",
    "language",
    "words",
    "basically",
    "called",
    "vocabulary",
    "natural",
    "language",
    "called",
    "vocabulary",
    "vocabulary",
    "vocabulary",
    "size",
    "documents",
    "distribution",
    "words",
    "distributed",
    "documents",
    "called",
    "distribution",
    "words",
    "okay",
    "saying",
    "uh",
    "whatever",
    "networks",
    "call",
    "models",
    "call",
    "try",
    "learn",
    "distribution",
    "try",
    "learn",
    "underlying",
    "distribution",
    "okay",
    "height",
    "people",
    "learn",
    "distribution",
    "sample",
    "point",
    "pick",
    "point",
    "show",
    "people",
    "distribution",
    "learned",
    "sample",
    "data",
    "whatever",
    "data",
    "distribution",
    "learned",
    "sample",
    "thing",
    "distribution",
    "show",
    "p",
    "similarly",
    "image",
    "also",
    "see",
    "ask",
    "distribution",
    "see",
    "uh",
    "basically",
    "distribution",
    "pixels",
    "distribution",
    "pixels",
    "pixels",
    "distributed",
    "okay",
    "create",
    "image",
    "28",
    "cross",
    "28",
    "image",
    "means",
    "784",
    "pixels",
    "okay",
    "generate",
    "pixel",
    "generate",
    "pixel",
    "generated",
    "arrange",
    "certain",
    "order",
    "also",
    "okay",
    "makes",
    "search",
    "space",
    "huge",
    "okay",
    "search",
    "see",
    "combination",
    "combinal",
    "problem",
    "makes",
    "makes",
    "space",
    "huge",
    "784",
    "values",
    "filled",
    "let",
    "suppose",
    "see",
    "rgb",
    "values",
    "take",
    "colors",
    "basically",
    "tried",
    "tries",
    "learn",
    "distribution",
    "pixel",
    "take",
    "pixel",
    "distribution",
    "learned",
    "data",
    "okay",
    "put",
    "together",
    "basically",
    "time",
    "learn",
    "distribution",
    "giving",
    "asked",
    "generate",
    "new",
    "thing",
    "basically",
    "generating",
    "new",
    "sample",
    "distribution",
    "okay",
    "distribution",
    "right",
    "784",
    "dimensional",
    "height",
    "distribution",
    "uh",
    "one",
    "dimensional",
    "sampling",
    "height",
    "784",
    "dimension",
    "basically",
    "generate",
    "784",
    "space",
    "navigate",
    "space",
    "generate",
    "sample",
    "okay",
    "called",
    "uh",
    "learning",
    "distribution",
    "particular",
    "llms",
    "okay",
    "one",
    "way",
    "learning",
    "rule",
    "based",
    "learnings",
    "restrict",
    "discussion",
    "learn",
    "distribution",
    "distribution",
    "mean",
    "try",
    "find",
    "pattern",
    "found",
    "pattern",
    "asked",
    "generate",
    "something",
    "new",
    "distribution",
    "place",
    "uh",
    "give",
    "sample",
    "distribution",
    "questions",
    "distribution",
    "sort",
    "heavy",
    "part",
    "okay",
    "covered",
    "prob",
    "stuff",
    "try",
    "keep",
    "language",
    "flexed",
    "uh",
    "uh",
    "people",
    "understand",
    "okay",
    "generative",
    "air",
    "works",
    "mostly",
    "see",
    "gener",
    "ad",
    "networks",
    "see",
    "variational",
    "auto",
    "encoders",
    "see",
    "diffusion",
    "models",
    "see",
    "transformer",
    "models",
    "mostly",
    "try",
    "learn",
    "distribution",
    "uh",
    "data",
    "large",
    "corpus",
    "data",
    "hungry",
    "require",
    "lot",
    "lot",
    "lot",
    "data",
    "order",
    "find",
    "patterns",
    "right",
    "move",
    "forward",
    "okay",
    "see",
    "llms",
    "okay",
    "okay",
    "coming",
    "core",
    "part",
    "lms",
    "know",
    "want",
    "define",
    "large",
    "language",
    "models",
    "whatever",
    "people",
    "thinking",
    "extension",
    "called",
    "nr",
    "models",
    "okay",
    "nr",
    "models",
    "engr",
    "models",
    "talk",
    "engram",
    "models",
    "okay",
    "let",
    "speak",
    "bagram",
    "model",
    "bagram",
    "model",
    "let",
    "suppose",
    "uh",
    "show",
    "lot",
    "data",
    "data",
    "lot",
    "sentences",
    "sent",
    "let",
    "suppose",
    "uh",
    "cat",
    "hat",
    "mat",
    "mat",
    "okay",
    "vagr",
    "models",
    "basically",
    "let",
    "suppose",
    "give",
    "cat",
    "okay",
    "see",
    "word",
    "follows",
    "cat",
    "okay",
    "word",
    "follows",
    "cat",
    "much",
    "probability",
    "sat",
    "much",
    "probability",
    "much",
    "probability",
    "mat",
    "let",
    "suppose",
    "vocabulary",
    "size",
    "vocabulary",
    "uh",
    "lot",
    "words",
    "let",
    "call",
    "generalize",
    "thing",
    "generalize",
    "let",
    "suppose",
    "word",
    "w1",
    "till",
    "w",
    "n",
    "words",
    "vocabulary",
    "basically",
    "dictionary",
    "contains",
    "n",
    "words",
    "let",
    "suppose",
    "trying",
    "explain",
    "bagram",
    "model",
    "work",
    "words",
    "w1",
    "wn",
    "say",
    "word",
    "wn",
    "okay",
    "word",
    "highest",
    "probability",
    "coming",
    "uh",
    "huge",
    "carpus",
    "count",
    "frequencies",
    "count",
    "frequency",
    "words",
    "say",
    "count",
    "pairs",
    "w1",
    "w2",
    "okay",
    "w1",
    "uh",
    "w3",
    "w1",
    "w1",
    "even",
    "okay",
    "uh",
    "w1",
    "w",
    "four",
    "till",
    "w1",
    "wn",
    "count",
    "frequency",
    "things",
    "count",
    "frequency",
    "frequency",
    "mean",
    "many",
    "times",
    "coexist",
    "every",
    "document",
    "let",
    "suppose",
    "documents",
    "world",
    "somehow",
    "come",
    "w1",
    "word",
    "okay",
    "frequency",
    "w1",
    "follow",
    "w2",
    "okay",
    "large",
    "corpor",
    "let",
    "suppose",
    "100",
    "times",
    "okay",
    "let",
    "suppose",
    "times",
    "40",
    "times",
    "50",
    "times",
    "accordingly",
    "times",
    "something",
    "like",
    "convert",
    "probability",
    "understanding",
    "saying",
    "understand",
    "saying",
    "word",
    "w1",
    "uh",
    "vocabulary",
    "also",
    "want",
    "see",
    "restricting",
    "bagram",
    "models",
    "saying",
    "search",
    "whole",
    "documents",
    "see",
    "many",
    "times",
    "word",
    "two",
    "words",
    "occer",
    "together",
    "w1",
    "w2",
    "w1",
    "w2",
    "w1",
    "w1",
    "w1",
    "w3",
    "something",
    "like",
    "okay",
    "check",
    "frequencies",
    "frequencies",
    "converted",
    "probabilities",
    "make",
    "diagram",
    "like",
    "w1",
    "w2",
    "till",
    "wn",
    "okay",
    "w1",
    "w2",
    "till",
    "wn",
    "probability",
    "scores",
    "probabilities",
    "go",
    "like",
    "probabilities",
    "let",
    "suppose",
    "w1",
    "w2",
    "probability",
    "probability",
    "uh",
    "01",
    "probability",
    "5",
    "05",
    "something",
    "like",
    "know",
    "probabilities",
    "sum",
    "one",
    "okay",
    "basically",
    "occurrences",
    "convert",
    "probability",
    "okay",
    "let",
    "suppose",
    "occurred",
    "thousand",
    "times",
    "whole",
    "document",
    "occurred",
    "times",
    "something",
    "like",
    "occurred",
    "together",
    "saying",
    "thousand",
    "times",
    "decrease",
    "probability",
    "see",
    "probability",
    "word",
    "w1",
    "look",
    "probability",
    "distribution",
    "say",
    "okay",
    "word",
    "w1",
    "point1",
    "probability",
    "pick",
    "word",
    "0",
    "z",
    "probability",
    "pick",
    "word",
    "okay",
    "pick",
    "uh",
    "pick",
    "w1",
    "sample",
    "basically",
    "called",
    "picking",
    "distribution",
    "pick",
    "word",
    "let",
    "suppose",
    "came",
    "w3",
    "w3",
    "corresponding",
    "w3",
    "also",
    "column",
    "sorry",
    "row",
    "probabilities",
    "sample",
    "w3",
    "let",
    "suppose",
    "sample",
    "wk",
    "okay",
    "wk",
    "probabilities",
    "also",
    "mentioned",
    "documents",
    "w1",
    "wk",
    "see",
    "pick",
    "probability",
    "probability",
    "assigned",
    "sample",
    "thing",
    "let",
    "suppose",
    "got",
    "word",
    "w",
    "dash",
    "okay",
    "generate",
    "sentence",
    "clear",
    "clear",
    "bagram",
    "model",
    "work",
    "w1",
    "w",
    "n",
    "words",
    "count",
    "frequency",
    "occurrences",
    "see",
    "many",
    "times",
    "occur",
    "together",
    "okay",
    "count",
    "along",
    "row",
    "convert",
    "probability",
    "simplest",
    "way",
    "converting",
    "probabilities",
    "sum",
    "together",
    "let",
    "suppose",
    "uh",
    "th000",
    "divide",
    "sums",
    "uh",
    "sum",
    "whole",
    "thing",
    "first",
    "divide",
    "entry",
    "convert",
    "probabilities",
    "okay",
    "convert",
    "probabilities",
    "randomly",
    "initially",
    "start",
    "sentence",
    "start",
    "sentence",
    "something",
    "maybe",
    "start",
    "token",
    "also",
    "say",
    "start",
    "token",
    "many",
    "times",
    "word",
    "okay",
    "word",
    "starting",
    "word",
    "time",
    "sample",
    "uh",
    "follow",
    "see",
    "row",
    "corresponding",
    "row",
    "according",
    "probabilities",
    "pick",
    "words",
    "complete",
    "sentence",
    "together",
    "okay",
    "hope",
    "makes",
    "sense",
    "called",
    "byram",
    "model",
    "similarly",
    "made",
    "seen",
    "engrams",
    "also",
    "make",
    "n",
    "basically",
    "n",
    "minus",
    "one",
    "thing",
    "predict",
    "n",
    "uh",
    "token",
    "based",
    "n",
    "minus",
    "one",
    "simplest",
    "problemistic",
    "approach",
    "go",
    "code",
    "promising",
    "people",
    "cote",
    "okay",
    "go",
    "deep",
    "larning",
    "part",
    "cote",
    "show",
    "basically",
    "make",
    "least",
    "bagram",
    "model",
    "make",
    "show",
    "perform",
    "show",
    "results",
    "okay",
    "might",
    "notebook",
    "also",
    "show",
    "wo",
    "explain",
    "code",
    "show",
    "notebook",
    "whatever",
    "steps",
    "showed",
    "show",
    "let",
    "give",
    "second",
    "okay",
    "want",
    "explain",
    "code",
    "um",
    "let",
    "create",
    "new",
    "notebook",
    "write",
    "byram",
    "model",
    "explain",
    "detail",
    "go",
    "deep",
    "learning",
    "part",
    "go",
    "language",
    "model",
    "parts",
    "go",
    "details",
    "code",
    "window",
    "visible",
    "visible",
    "yeah",
    "yes",
    "yes",
    "visible",
    "show",
    "things",
    "let",
    "suppose",
    "import",
    "plot",
    "lib",
    "isor",
    "yes",
    "everything",
    "generate",
    "corpus",
    "let",
    "consider",
    "documents",
    "okay",
    "documents",
    "words",
    "powerful",
    "problems",
    "creative",
    "data",
    "created",
    "junk",
    "data",
    "consider",
    "corpus",
    "whole",
    "list",
    "documents",
    "okay",
    "speaking",
    "lot",
    "words",
    "need",
    "need",
    "tokenize",
    "meaning",
    "tokenize",
    "take",
    "word",
    "see",
    "many",
    "words",
    "token",
    "separating",
    "separating",
    "basically",
    "words",
    "tokenize",
    "data",
    "show",
    "tokenization",
    "also",
    "see",
    "tokenize",
    "data",
    "word",
    "word",
    "powerful",
    "ai",
    "problems",
    "creativity",
    "words",
    "okay",
    "basically",
    "took",
    "sentences",
    "word",
    "level",
    "tokenization",
    "separated",
    "words",
    "create",
    "byrams",
    "okay",
    "okay",
    "create",
    "byrams",
    "let",
    "suppose",
    "create",
    "bagram",
    "see",
    "exist",
    "25",
    "times",
    "together",
    "powerful",
    "exist",
    "20",
    "times",
    "together",
    "powerful",
    "ai",
    "exist",
    "10",
    "times",
    "okay",
    "similarly",
    "see",
    "value",
    "see",
    "whole",
    "dictionary",
    "values",
    "beautiful",
    "programming",
    "came",
    "eight",
    "times",
    "future",
    "came",
    "nine",
    "times",
    "keeping",
    "count",
    "okay",
    "count",
    "show",
    "matrix",
    "also",
    "makes",
    "sense",
    "also",
    "print",
    "matrix",
    "code",
    "written",
    "think",
    "long",
    "way",
    "back",
    "see",
    "matrix",
    "exist",
    "together",
    "eight",
    "times",
    "ai",
    "exist",
    "15",
    "times",
    "n",
    "exist",
    "25",
    "times",
    "count",
    "huge",
    "matrix",
    "usually",
    "32",
    "32",
    "vocabulary",
    "size",
    "32",
    "32",
    "words",
    "mockup",
    "data",
    "uh",
    "calculate",
    "probabilities",
    "convert",
    "probabilities",
    "okay",
    "convert",
    "byrams",
    "probabilities",
    "show",
    "probability",
    "probability",
    "like",
    "followed",
    "probability",
    "okay",
    "applications",
    "probability",
    "similarly",
    "innovation",
    "probability",
    "similarly",
    "go",
    "computed",
    "probability",
    "word",
    "corresponding",
    "let",
    "suppose",
    "already",
    "generate",
    "function",
    "generate",
    "generate",
    "sample",
    "start",
    "random",
    "thing",
    "see",
    "import",
    "random",
    "okay",
    "import",
    "random",
    "generate",
    "generate",
    "pick",
    "choices",
    "random",
    "generate",
    "sentence",
    "generate",
    "sentences",
    "see",
    "applications",
    "python",
    "beautiful",
    "love",
    "innovation",
    "love",
    "learning",
    "language",
    "something",
    "like",
    "looks",
    "jber",
    "corpus",
    "size",
    "less",
    "okay",
    "seeing",
    "sample",
    "next",
    "word",
    "sampling",
    "applications",
    "application",
    "sampled",
    "sample",
    "next",
    "word",
    "sampling",
    "python",
    "run",
    "random",
    "change",
    "okay",
    "see",
    "signs",
    "future",
    "solution",
    "create",
    "bagram",
    "model",
    "saw",
    "got",
    "corpus",
    "first",
    "step",
    "tokenized",
    "data",
    "took",
    "word",
    "step",
    "called",
    "word",
    "level",
    "tokenization",
    "character",
    "level",
    "tokenization",
    "different",
    "types",
    "tokenization",
    "separate",
    "words",
    "bite",
    "level",
    "tokenization",
    "also",
    "bagram",
    "model",
    "usually",
    "see",
    "occurrence",
    "byrams",
    "together",
    "let",
    "suppose",
    "word",
    "followed",
    "word",
    "frequency",
    "frequencies",
    "convert",
    "probabilities",
    "randomly",
    "generate",
    "first",
    "word",
    "word",
    "slowly",
    "generate",
    "words",
    "based",
    "happening",
    "previously",
    "hope",
    "bagram",
    "model",
    "clear",
    "clear",
    "okay",
    "beautiful",
    "magic",
    "magic",
    "see",
    "language",
    "models",
    "language",
    "models",
    "engram",
    "models",
    "steroids",
    "okay",
    "done",
    "lot",
    "architectural",
    "change",
    "generate",
    "things",
    "attention",
    "see",
    "start",
    "bagram",
    "models",
    "told",
    "consider",
    "let",
    "uh",
    "hide",
    "code",
    "window",
    "okay",
    "see",
    "large",
    "l",
    "model",
    "engram",
    "model",
    "viewed",
    "extension",
    "ar",
    "change",
    "different",
    "training",
    "methods",
    "given",
    "capabilities",
    "underneath",
    "h",
    "basically",
    "trying",
    "uh",
    "see",
    "called",
    "context",
    "seeing",
    "let",
    "suppose",
    "past",
    "tokens",
    "called",
    "tokens",
    "usually",
    "whatever",
    "call",
    "word",
    "called",
    "tokens",
    "called",
    "tokenization",
    "break",
    "tokens",
    "seeing",
    "previous",
    "context",
    "seeing",
    "previous",
    "context",
    "based",
    "previous",
    "context",
    "generating",
    "new",
    "token",
    "next",
    "token",
    "called",
    "auto",
    "regressive",
    "models",
    "okay",
    "auto",
    "regressive",
    "usually",
    "whatever",
    "chat",
    "gpt",
    "llama",
    "claud",
    "auto",
    "regress",
    "model",
    "meaning",
    "auto",
    "regressive",
    "take",
    "consideration",
    "happened",
    "previously",
    "okay",
    "let",
    "suppose",
    "first",
    "thing",
    "generated",
    "c",
    "seeing",
    "c",
    "generate",
    "okay",
    "space",
    "whatever",
    "happened",
    "previously",
    "generate",
    "space",
    "set",
    "mat",
    "let",
    "stop",
    "let",
    "suppose",
    "uh",
    "generate",
    "take",
    "whole",
    "picture",
    "called",
    "context",
    "okay",
    "might",
    "seen",
    "news",
    "articles",
    "chad",
    "gpd",
    "supports",
    "much",
    "contex",
    "length",
    "contex",
    "lens",
    "basically",
    "means",
    "many",
    "tokens",
    "uh",
    "attend",
    "many",
    "tokens",
    "keep",
    "let",
    "suppose",
    "uh",
    "use",
    "word",
    "many",
    "uh",
    "tokens",
    "keep",
    "memory",
    "okay",
    "called",
    "context",
    "context",
    "came",
    "generate",
    "new",
    "thing",
    "became",
    "like",
    "based",
    "generated",
    "based",
    "generated",
    "mat",
    "saw",
    "byram",
    "models",
    "okay",
    "hope",
    "clear",
    "clear",
    "clear",
    "branch",
    "okay",
    "generate",
    "different",
    "techniques",
    "generation",
    "different",
    "techniques",
    "generation",
    "okay",
    "wo",
    "go",
    "detail",
    "something",
    "called",
    "greedy",
    "approach",
    "something",
    "called",
    "beam",
    "search",
    "lot",
    "search",
    "methods",
    "basically",
    "generate",
    "one",
    "thing",
    "sample",
    "thing",
    "pick",
    "becomes",
    "question",
    "say",
    "left",
    "okay",
    "okay",
    "usual",
    "things",
    "let",
    "suppose",
    "words",
    "right",
    "left",
    "right",
    "people",
    "usually",
    "use",
    "let",
    "probability",
    "maybe",
    "think",
    "left",
    "center",
    "center",
    "also",
    "probability",
    "probability",
    "0",
    "point",
    "uh",
    "let",
    "suppose",
    "18",
    "something",
    "like",
    "lot",
    "words",
    "words",
    "select",
    "put",
    "write",
    "different",
    "alg",
    "like",
    "really",
    "search",
    "like",
    "said",
    "mem",
    "search",
    "okay",
    "select",
    "word",
    "put",
    "wo",
    "happen",
    "word",
    "level",
    "happen",
    "basically",
    "token",
    "level",
    "token",
    "word",
    "token",
    "may",
    "word",
    "less",
    "word",
    "said",
    "different",
    "methods",
    "tokenization",
    "let",
    "suppose",
    "word",
    "high",
    "probability",
    "take",
    "choose",
    "highest",
    "probability",
    "word",
    "happen",
    "using",
    "greedy",
    "approach",
    "whichever",
    "word",
    "highest",
    "probability",
    "sees",
    "context",
    "vocabulary",
    "words",
    "choosing",
    "highest",
    "uh",
    "probability",
    "word",
    "called",
    "greedy",
    "search",
    "becoming",
    "greedy",
    "taking",
    "let",
    "model",
    "get",
    "creative",
    "okay",
    "sometimes",
    "beam",
    "search",
    "also",
    "basically",
    "creates",
    "lot",
    "beams",
    "left",
    "lot",
    "possibilities",
    "words",
    "select",
    "okay",
    "let",
    "suppose",
    "top",
    "four",
    "beams",
    "usually",
    "n",
    "beams",
    "lot",
    "possibilities",
    "lot",
    "possibilities",
    "okay",
    "possibility",
    "see",
    "let",
    "two",
    "steps",
    "saying",
    "two",
    "steps",
    "whichever",
    "branch",
    "maximum",
    "probability",
    "generate",
    "words",
    "okay",
    "usually",
    "beam",
    "search",
    "works",
    "like",
    "dumbing",
    "lot",
    "think",
    "got",
    "idea",
    "keeping",
    "beams",
    "alive",
    "saying",
    "four",
    "words",
    "taking",
    "different",
    "beams",
    "computing",
    "probability",
    "let",
    "step",
    "two",
    "okay",
    "becomes",
    "creative",
    "uh",
    "sample",
    "generation",
    "modes",
    "mode",
    "want",
    "generate",
    "explained",
    "two",
    "one",
    "greedy",
    "topk",
    "sampling",
    "lot",
    "things",
    "people",
    "see",
    "language",
    "model",
    "nothing",
    "angram",
    "model",
    "many",
    "people",
    "disagree",
    "ang",
    "grammar",
    "uh",
    "uh",
    "maybe",
    "think",
    "angr",
    "model",
    "n",
    "minus",
    "one",
    "tokens",
    "generating",
    "nth",
    "token",
    "based",
    "n",
    "minus",
    "one",
    "okay",
    "bagram",
    "model",
    "totally",
    "different",
    "uh",
    "large",
    "language",
    "models",
    "like",
    "gpt",
    "lama",
    "claw",
    "separately",
    "uh",
    "uh",
    "called",
    "architecture",
    "use",
    "people",
    "use",
    "self",
    "attention",
    "use",
    "different",
    "types",
    "attention",
    "also",
    "okay",
    "clear",
    "far",
    "good",
    "good",
    "question",
    "becomes",
    "good",
    "good",
    "good",
    "distribution",
    "things",
    "whatever",
    "distribution",
    "okay",
    "let",
    "supp",
    "language",
    "language",
    "distribution",
    "clear",
    "distribution",
    "never",
    "see",
    "gpt",
    "large",
    "language",
    "models",
    "making",
    "grammatical",
    "mistake",
    "pattern",
    "language",
    "particular",
    "distribution",
    "pattern",
    "language",
    "okay",
    "captures",
    "pattern",
    "okay",
    "main",
    "problem",
    "comes",
    "content",
    "two",
    "two",
    "things",
    "content",
    "style",
    "okay",
    "captures",
    "style",
    "great",
    "way",
    "great",
    "capturing",
    "style",
    "okay",
    "content",
    "sure",
    "okay",
    "much",
    "capture",
    "content",
    "let",
    "suppose",
    "somebody",
    "somebody",
    "speak",
    "eloquently",
    "content",
    "totally",
    "rubbish",
    "okay",
    "language",
    "models",
    "seen",
    "really",
    "good",
    "capturing",
    "style",
    "capturing",
    "style",
    "distribution",
    "okay",
    "content",
    "distribution",
    "get",
    "fact",
    "facts",
    "wrong",
    "facts",
    "instance",
    "distribution",
    "okay",
    "let",
    "suppose",
    "uh",
    "different",
    "facts",
    "uh",
    "facts",
    "like",
    "radius",
    "sun",
    "radius",
    "earth",
    "come",
    "distribution",
    "okay",
    "fact",
    "called",
    "instance",
    "level",
    "things",
    "instance",
    "let",
    "president",
    "uh",
    "united",
    "states",
    "prime",
    "minister",
    "india",
    "data",
    "probably",
    "try",
    "replicate",
    "sometimes",
    "may",
    "get",
    "facts",
    "wrong",
    "see",
    "mathematical",
    "problem",
    "distribution",
    "problem",
    "factual",
    "problem",
    "rather",
    "whole",
    "procedure",
    "okay",
    "good",
    "good",
    "generating",
    "distributions",
    "good",
    "mimicking",
    "distributions",
    "okay",
    "one",
    "thing",
    "comes",
    "copying",
    "style",
    "write",
    "beautifully",
    "maybe",
    "whatever",
    "content",
    "maybe",
    "intrinsic",
    "value",
    "tell",
    "tell",
    "make",
    "mistake",
    "give",
    "prompt",
    "speak",
    "like",
    "let",
    "suppose",
    "person",
    "speaks",
    "broken",
    "english",
    "give",
    "things",
    "otherwise",
    "wo",
    "make",
    "make",
    "grammatical",
    "mistake",
    "learned",
    "pattern",
    "okay",
    "difference",
    "formal",
    "language",
    "informal",
    "language",
    "okay",
    "see",
    "uh",
    "formal",
    "language",
    "like",
    "python",
    "language",
    "interpreter",
    "okay",
    "interpreter",
    "create",
    "p",
    "tree",
    "even",
    "cc",
    "compiler",
    "check",
    "semantic",
    "rules",
    "everything",
    "validity",
    "see",
    "formal",
    "language",
    "interpreter",
    "okay",
    "basically",
    "whole",
    "interpreter",
    "whole",
    "world",
    "put",
    "things",
    "whole",
    "interpret",
    "way",
    "possible",
    "okay",
    "hard",
    "uh",
    "come",
    "grammar",
    "first",
    "language",
    "rather",
    "happens",
    "language",
    "comes",
    "first",
    "grammar",
    "comes",
    "spontaneously",
    "computer",
    "language",
    "revers",
    "come",
    "grammar",
    "first",
    "say",
    "rules",
    "make",
    "language",
    "okay",
    "give",
    "okay",
    "many",
    "people",
    "confuse",
    "capability",
    "basically",
    "reasoning",
    "lot",
    "things",
    "ask",
    "questions",
    "okay",
    "many",
    "many",
    "people",
    "shown",
    "possess",
    "reasoning",
    "capabilities",
    "shown",
    "fact",
    "people",
    "see",
    "see",
    "reasoning",
    "hard",
    "distinguish",
    "uh",
    "somebody",
    "reasoning",
    "saying",
    "something",
    "root",
    "memory",
    "let",
    "suppose",
    "ask",
    "question",
    "problem",
    "exams",
    "also",
    "go",
    "iit",
    "j",
    "need",
    "coachings",
    "basically",
    "teach",
    "patterns",
    "questions",
    "tell",
    "reason",
    "okay",
    "saying",
    "pattern",
    "question",
    "formulation",
    "solve",
    "apply",
    "formula",
    "get",
    "answer",
    "fill",
    "fill",
    "thing",
    "okay",
    "hard",
    "distinguish",
    "person",
    "actually",
    "reasoned",
    "came",
    "answer",
    "person",
    "followed",
    "step",
    "root",
    "memory",
    "came",
    "answer",
    "hard",
    "give",
    "uh",
    "tell",
    "difference",
    "reasoning",
    "looks",
    "like",
    "reasoning",
    "reasoning",
    "usually",
    "first",
    "principles",
    "okay",
    "first",
    "take",
    "step",
    "learn",
    "different",
    "step",
    "infer",
    "something",
    "infer",
    "something",
    "language",
    "models",
    "okay",
    "generate",
    "things",
    "showed",
    "uh",
    "let",
    "suppose",
    "give",
    "things",
    "questions",
    "till",
    "least",
    "getting",
    "bigger",
    "picture",
    "language",
    "models",
    "work",
    "looking",
    "like",
    "talking",
    "also",
    "ask",
    "questions",
    "divert",
    "conversations",
    "otherwise",
    "whatever",
    "comes",
    "head",
    "speak",
    "yes",
    "saying",
    "novel",
    "style",
    "style",
    "good",
    "sty",
    "style",
    "good",
    "content",
    "problem",
    "generate",
    "jish",
    "generate",
    "things",
    "may",
    "look",
    "may",
    "give",
    "sense",
    "content",
    "totally",
    "nonsensical",
    "okay",
    "sometimes",
    "generate",
    "sentences",
    "puts",
    "right",
    "way",
    "interpreter",
    "interpreter",
    "restriction",
    "generate",
    "uh",
    "rules",
    "language",
    "interpret",
    "said",
    "formal",
    "language",
    "particular",
    "procedure",
    "check",
    "syntactically",
    "correct",
    "semantically",
    "correct",
    "informal",
    "language",
    "natural",
    "language",
    "problem",
    "whole",
    "world",
    "interpreter",
    "want",
    "interpret",
    "interpret",
    "sometimes",
    "see",
    "gpd",
    "brilliant",
    "answer",
    "problem",
    "actually",
    "gave",
    "brilliant",
    "style",
    "content",
    "came",
    "brilliant",
    "sometimes",
    "uh",
    "generates",
    "something",
    "rubbish",
    "uh",
    "gives",
    "something",
    "feel",
    "like",
    "oh",
    "probably",
    "factual",
    "trying",
    "mimicking",
    "language",
    "okay",
    "second",
    "second",
    "thing",
    "see",
    "uh",
    "basically",
    "trying",
    "simulate",
    "things",
    "see",
    "simulation",
    "machine",
    "see",
    "human",
    "let",
    "make",
    "difference",
    "see",
    "human",
    "persistent",
    "memory",
    "let",
    "suppose",
    "speaking",
    "right",
    "whatever",
    "spoke",
    "also",
    "things",
    "head",
    "refer",
    "uh",
    "let",
    "suppose",
    "answer",
    "accordingly",
    "fit",
    "situation",
    "language",
    "models",
    "think",
    "let",
    "give",
    "one",
    "prompt",
    "okay",
    "generate",
    "things",
    "right",
    "want",
    "go",
    "back",
    "probably",
    "wo",
    "able",
    "access",
    "okay",
    "let",
    "let",
    "show",
    "simple",
    "analogy",
    "go",
    "chat",
    "gpt",
    "maybe",
    "okay",
    "let",
    "let",
    "go",
    "chat",
    "gpd",
    "show",
    "something",
    "amazing",
    "least",
    "look",
    "amazing",
    "many",
    "want",
    "show",
    "different",
    "work",
    "us",
    "normal",
    "humans",
    "uh",
    "let",
    "bring",
    "code",
    "window",
    "sorry",
    "brought",
    "chrome",
    "window",
    "yes",
    "chrome",
    "window",
    "vis",
    "see",
    "yeah",
    "yes",
    "sir",
    "let",
    "play",
    "game",
    "gpd",
    "okay",
    "explain",
    "game",
    "uh",
    "let",
    "suppose",
    "uh",
    "might",
    "heard",
    "game",
    "uh",
    "think",
    "object",
    "guess",
    "object",
    "ask",
    "question",
    "uh",
    "let",
    "suppose",
    "thought",
    "uh",
    "say",
    "pen",
    "okay",
    "thought",
    "pen",
    "say",
    "okay",
    "think",
    "object",
    "think",
    "cury",
    "give",
    "yes",
    "answers",
    "give",
    "yes",
    "tell",
    "okay",
    "uh",
    "tell",
    "uh",
    "used",
    "humans",
    "say",
    "yes",
    "used",
    "animals",
    "say",
    "okay",
    "cury",
    "basically",
    "decrease",
    "search",
    "space",
    "till",
    "get",
    "right",
    "answer",
    "say",
    "uh",
    "pen",
    "pen",
    "say",
    "yes",
    "let",
    "suppose",
    "dog",
    "say",
    "food",
    "ask",
    "yes",
    "thing",
    "mind",
    "object",
    "asked",
    "keep",
    "object",
    "mind",
    "ask",
    "yes",
    "questions",
    "get",
    "answer",
    "many",
    "understood",
    "game",
    "game",
    "think",
    "object",
    "ask",
    "yes",
    "questions",
    "basically",
    "gets",
    "right",
    "answer",
    "setting",
    "game",
    "clear",
    "clear",
    "somebody",
    "speak",
    "see",
    "screen",
    "yes",
    "sir",
    "cle",
    "yes",
    "say",
    "play",
    "game",
    "gpt",
    "okay",
    "play",
    "game",
    "say",
    "uh",
    "let",
    "play",
    "game",
    "let",
    "us",
    "play",
    "game",
    "select",
    "object",
    "ask",
    "yes",
    "sl",
    "questions",
    "ask",
    "questions",
    "questions",
    "answer",
    "yes",
    "accordingly",
    "guess",
    "object",
    "guess",
    "object",
    "something",
    "like",
    "okay",
    "hope",
    "picks",
    "okay",
    "sounds",
    "fun",
    "picked",
    "object",
    "picked",
    "object",
    "go",
    "ahead",
    "ask",
    "question",
    "okay",
    "ask",
    "question",
    "ask",
    "uh",
    "let",
    "suppose",
    "question",
    "uh",
    "used",
    "humans",
    "used",
    "humans",
    "talking",
    "object",
    "tau",
    "memory",
    "used",
    "humans",
    "saying",
    "yes",
    "uh",
    "living",
    "thing",
    "saying",
    "living",
    "thing",
    "uh",
    "basically",
    "means",
    "used",
    "humans",
    "living",
    "thing",
    "edible",
    "eat",
    "something",
    "like",
    "making",
    "spelling",
    "mistakes",
    "edible",
    "move",
    "move",
    "something",
    "like",
    "okay",
    "saying",
    "move",
    "let",
    "suppose",
    "uh",
    "say",
    "okay",
    "know",
    "please",
    "tell",
    "answer",
    "please",
    "tell",
    "answer",
    "okay",
    "object",
    "picked",
    "chair",
    "okay",
    "show",
    "showed",
    "okay",
    "sort",
    "saying",
    "right",
    "thing",
    "change",
    "prompt",
    "said",
    "move",
    "said",
    "okay",
    "say",
    "people",
    "sit",
    "okay",
    "people",
    "know",
    "chair",
    "10",
    "people",
    "sit",
    "send",
    "saying",
    "yes",
    "prompt",
    "prompt",
    "okay",
    "made",
    "wood",
    "wood",
    "yes",
    "usually",
    "plastic",
    "chair",
    "plastic",
    "also",
    "uh",
    "right",
    "trying",
    "trying",
    "go",
    "outside",
    "chair",
    "generate",
    "chair",
    "ask",
    "response",
    "stick",
    "chair",
    "giving",
    "response",
    "changed",
    "answer",
    "color",
    "yellow",
    "something",
    "like",
    "uh",
    "engine",
    "legs",
    "legs",
    "object",
    "five",
    "legs",
    "okay",
    "let",
    "five",
    "legs",
    "four",
    "legs",
    "four",
    "legs",
    "deviated",
    "deviated",
    "chair",
    "chair",
    "four",
    "legs",
    "okay",
    "ask",
    "okay",
    "object",
    "object",
    "see",
    "four",
    "legs",
    "said",
    "said",
    "chair",
    "uh",
    "let",
    "change",
    "prompt",
    "uh",
    "bigger",
    "bigger",
    "house",
    "okay",
    "let",
    "let",
    "give",
    "promp",
    "deviates",
    "many",
    "times",
    "chang",
    "object",
    "bigger",
    "house",
    "uh",
    "bigger",
    "chair",
    "prompt",
    "according",
    "bigger",
    "chair",
    "yes",
    "uh",
    "uh",
    "kept",
    "outside",
    "house",
    "conversion",
    "becoming",
    "long",
    "kept",
    "outside",
    "house",
    "chair",
    "chair",
    "see",
    "okay",
    "object",
    "object",
    "thinking",
    "picnic",
    "table",
    "see",
    "many",
    "able",
    "see",
    "basically",
    "nothing",
    "keeping",
    "keeping",
    "memory",
    "keeping",
    "memory",
    "whatever",
    "prompt",
    "giving",
    "based",
    "previously",
    "generating",
    "thing",
    "okay",
    "many",
    "understood",
    "thing",
    "point",
    "try",
    "try",
    "give",
    "prompt",
    "try",
    "give",
    "prompt",
    "thinking",
    "front",
    "memory",
    "part",
    "okay",
    "kept",
    "memory",
    "uh",
    "basically",
    "keep",
    "chair",
    "keep",
    "chair",
    "never",
    "change",
    "answer",
    "basically",
    "give",
    "prompt",
    "giving",
    "specification",
    "settling",
    "things",
    "according",
    "specification",
    "okay",
    "said",
    "chair",
    "said",
    "chair",
    "okay",
    "object",
    "object",
    "thinking",
    "picnic",
    "table",
    "initially",
    "saw",
    "prompts",
    "saying",
    "chair",
    "humans",
    "work",
    "gpd",
    "work",
    "entirely",
    "different",
    "okay",
    "basically",
    "generate",
    "things",
    "based",
    "previously",
    "done",
    "would",
    "asked",
    "would",
    "never",
    "changed",
    "answer",
    "would",
    "given",
    "specific",
    "chair",
    "would",
    "said",
    "yes",
    "chair",
    "thinking",
    "chair",
    "saw",
    "initially",
    "said",
    "yes",
    "sing",
    "chair",
    "changed",
    "prompt",
    "made",
    "devate",
    "made",
    "devate",
    "chair",
    "okay",
    "thought",
    "object",
    "okay",
    "bas",
    "means",
    "called",
    "impr",
    "prompto",
    "thinking",
    "time",
    "thinking",
    "basically",
    "generating",
    "whatever",
    "previous",
    "context",
    "based",
    "generating",
    "thing",
    "keeping",
    "things",
    "view",
    "humans",
    "okay",
    "one",
    "examples",
    "uh",
    "showed",
    "see",
    "tree",
    "thoughts",
    "picks",
    "one",
    "okay",
    "saw",
    "initially",
    "said",
    "chair",
    "suddenly",
    "changed",
    "tree",
    "basically",
    "ask",
    "uh",
    "questions",
    "called",
    "distribution",
    "follow",
    "ad",
    "like",
    "factual",
    "questions",
    "algorithm",
    "let",
    "suppose",
    "needs",
    "computation",
    "perform",
    "ask",
    "chat",
    "better",
    "said",
    "better",
    "copying",
    "style",
    "style",
    "copy",
    "style",
    "questions",
    "questions",
    "sir",
    "yes",
    "yes",
    "yes",
    "go",
    "ahead",
    "think",
    "raise",
    "hand",
    "call",
    "names",
    "yes",
    "aran",
    "go",
    "ahead",
    "logical",
    "reasoning",
    "like",
    "reason",
    "rather",
    "showed",
    "point",
    "try",
    "tell",
    "basically",
    "reasoning",
    "anything",
    "thing",
    "seeing",
    "previous",
    "context",
    "based",
    "generating",
    "next",
    "token",
    "okay",
    "coding",
    "basically",
    "ask",
    "prompt",
    "generate",
    "code",
    "adds",
    "number",
    "lot",
    "documentations",
    "written",
    "uh",
    "internet",
    "web",
    "scra",
    "comments",
    "saw",
    "saw",
    "uh",
    "people",
    "write",
    "add",
    "two",
    "numbers",
    "add",
    "two",
    "numbers",
    "okay",
    "put",
    "comment",
    "something",
    "write",
    "program",
    "def",
    "something",
    "like",
    "gave",
    "prompt",
    "okay",
    "prompt",
    "thing",
    "followed",
    "okay",
    "basically",
    "tring",
    "generate",
    "next",
    "token",
    "seeing",
    "logic",
    "usually",
    "people",
    "say",
    "understand",
    "speak",
    "usually",
    "internet",
    "lot",
    "corpuses",
    "code",
    "line",
    "okay",
    "add",
    "number",
    "pick",
    "documentation",
    "god",
    "knows",
    "basically",
    "take",
    "prompt",
    "generate",
    "next",
    "tokens",
    "sometime",
    "makes",
    "small",
    "mistake",
    "ask",
    "something",
    "makes",
    "small",
    "sometimes",
    "misses",
    "let",
    "suppose",
    "uh",
    "variable",
    "name",
    "sometimes",
    "give",
    "variable",
    "name",
    "hello",
    "sometimes",
    "use",
    "hle",
    "l",
    "something",
    "like",
    "makes",
    "minuscule",
    "mistake",
    "structure",
    "right",
    "see",
    "makes",
    "minute",
    "mistakes",
    "code",
    "generate",
    "logic",
    "rather",
    "whatever",
    "context",
    "gave",
    "generate",
    "auto",
    "regressively",
    "generate",
    "next",
    "token",
    "next",
    "token",
    "next",
    "token",
    "without",
    "giving",
    "view",
    "logic",
    "constraints",
    "said",
    "look",
    "l",
    "look",
    "logic",
    "ask",
    "noel",
    "problem",
    "let",
    "suppose",
    "nowhere",
    "internet",
    "able",
    "solve",
    "reasoned",
    "first",
    "principles",
    "said",
    "okay",
    "want",
    "first",
    "step",
    "okay",
    "like",
    "basically",
    "without",
    "constraints",
    "whatever",
    "previous",
    "context",
    "seeing",
    "context",
    "generates",
    "next",
    "word",
    "see",
    "uh",
    "code",
    "language",
    "rather",
    "easy",
    "replicate",
    "assignment",
    "uh",
    "chat",
    "gpt",
    "able",
    "assignment",
    "means",
    "novel",
    "problem",
    "novel",
    "problem",
    "chat",
    "gpt",
    "struggle",
    "instance",
    "okay",
    "instance",
    "struggle",
    "let",
    "suppose",
    "tell",
    "multiply",
    "multiply",
    "two",
    "numbers",
    "okay",
    "show",
    "generic",
    "method",
    "give",
    "let",
    "suppose",
    "two",
    "digit",
    "numbers",
    "first",
    "two",
    "digit",
    "numbers",
    "okay",
    "show",
    "okay",
    "generic",
    "technique",
    "apply",
    "three",
    "digit",
    "say",
    "12",
    "14",
    "multiply",
    "mechanism",
    "take",
    "four",
    "multiply",
    "two",
    "get",
    "like",
    "put",
    "cross",
    "write",
    "multiply",
    "number",
    "two",
    "add",
    "together",
    "okay",
    "oops",
    "8",
    "six",
    "one",
    "showed",
    "showed",
    "mechanism",
    "somehow",
    "gave",
    "chat",
    "tell",
    "two",
    "digit",
    "thing",
    "done",
    "like",
    "also",
    "three",
    "digit",
    "three",
    "digigit",
    "three",
    "digigit",
    "product",
    "done",
    "like",
    "show",
    "instance",
    "learning",
    "trying",
    "find",
    "pattern",
    "trying",
    "see",
    "logical",
    "reason",
    "okay",
    "reason",
    "multiply",
    "things",
    "together",
    "happen",
    "like",
    "trying",
    "rather",
    "find",
    "pattern",
    "ask",
    "convince",
    "say",
    "9",
    "plus",
    "2",
    "let",
    "suppose",
    "give",
    "right",
    "answer",
    "9",
    "plus",
    "2",
    "lot",
    "corpus",
    "internet",
    "9",
    "9",
    "multiplied",
    "two",
    "given",
    "18",
    "tred",
    "wife",
    "said",
    "19",
    "say",
    "yes",
    "19",
    "question",
    "basically",
    "conversing",
    "rather",
    "logical",
    "reasoning",
    "okay",
    "see",
    "difference",
    "many",
    "people",
    "uh",
    "put",
    "nonsensical",
    "prompts",
    "saying",
    "oh",
    "look",
    "ch",
    "gp",
    "making",
    "mistake",
    "shows",
    "ignorance",
    "know",
    "fact",
    "generating",
    "thing",
    "forcing",
    "something",
    "trained",
    "whatever",
    "context",
    "give",
    "generated",
    "next",
    "word",
    "based",
    "okay",
    "one",
    "example",
    "caesar",
    "cipher",
    "people",
    "written",
    "papers",
    "uh",
    "cd",
    "okay",
    "want",
    "c",
    "cer",
    "means",
    "substitute",
    "replaced",
    "let",
    "suppose",
    "2",
    "means",
    "uh",
    "replace",
    "c",
    "okay",
    "plus",
    "2",
    "basically",
    "means",
    "z",
    "given",
    "numbers",
    "like",
    "1",
    "26",
    "okay",
    "say",
    "2",
    "scheme",
    "plus2",
    "maybe",
    "letter",
    "plus",
    "two",
    "okay",
    "whatever",
    "letter",
    "appears",
    "go",
    "two",
    "forward",
    "replace",
    "letter",
    "replaced",
    "c",
    "b",
    "c",
    "e",
    "something",
    "like",
    "called",
    "substitution",
    "substitute",
    "words",
    "called",
    "cyper",
    "text",
    "hope",
    "everybody",
    "knows",
    "cs",
    "background",
    "know",
    "uh",
    "plain",
    "text",
    "substitute",
    "words",
    "uh",
    "letter",
    "make",
    "foury",
    "old",
    "kid",
    "ask",
    "okay",
    "word",
    "hello",
    "okay",
    "sub",
    "plus",
    "two",
    "make",
    "words",
    "plus",
    "two",
    "let",
    "suppose",
    "h",
    "g",
    "h",
    "g",
    "h",
    "j",
    "h",
    "replaced",
    "j",
    "replaced",
    "j",
    "uh",
    "e",
    "plus",
    "2",
    "c",
    "b",
    "c",
    "e",
    "f",
    "replaced",
    "f",
    "uh",
    "jk",
    "l",
    "mn",
    "replaced",
    "n",
    "n",
    "n",
    "pq",
    "replaced",
    "p",
    "q",
    "c",
    "text",
    "plain",
    "thing",
    "cipher",
    "okay",
    "give",
    "let",
    "suppose",
    "kid",
    "say",
    "okay",
    "saying",
    "smart",
    "make",
    "cipher",
    "text",
    "let",
    "give",
    "cipher",
    "text",
    "coding",
    "scheme",
    "used",
    "plus",
    "two",
    "okay",
    "easily",
    "decode",
    "code",
    "hardly",
    "took",
    "minute",
    "explain",
    "people",
    "hardly",
    "take",
    "couple",
    "minutes",
    "uh",
    "teach",
    "four",
    "year",
    "five",
    "year",
    "kid",
    "able",
    "precisely",
    "gpd",
    "given",
    "terribly",
    "failed",
    "okay",
    "say",
    "okay",
    "things",
    "learns",
    "distribution",
    "okay",
    "actually",
    "distri",
    "actually",
    "performed",
    "good",
    "called",
    "road",
    "13",
    "get",
    "plus",
    "13",
    "13",
    "whatever",
    "13th",
    "character",
    "pick",
    "performing",
    "good",
    "road",
    "13",
    "rotation",
    "2",
    "rotation",
    "5",
    "rotation",
    "26",
    "something",
    "like",
    "performing",
    "good",
    "linux",
    "actually",
    "supports",
    "people",
    "writing",
    "c",
    "cipher",
    "somehow",
    "many",
    "people",
    "wrote",
    "rotation",
    "13",
    "lot",
    "data",
    "road",
    "13",
    "okay",
    "internet",
    "learned",
    "pattern",
    "performing",
    "good",
    "road",
    "13",
    "road",
    "two",
    "road",
    "three",
    "performing",
    "good",
    "papers",
    "written",
    "okay",
    "tells",
    "one",
    "thing",
    "learns",
    "distribution",
    "rather",
    "reasoning",
    "reason",
    "okay",
    "whatever",
    "context",
    "given",
    "next",
    "word",
    "predict",
    "based",
    "context",
    "see",
    "post",
    "given",
    "person",
    "ask",
    "weather",
    "see",
    "distribution",
    "give",
    "factual",
    "information",
    "see",
    "see",
    "saying",
    "9",
    "3",
    "somewhat",
    "let",
    "supp",
    "34",
    "right",
    "problem",
    "maybe",
    "data",
    "something",
    "happened",
    "context",
    "gave",
    "generated",
    "34",
    "trained",
    "logical",
    "calculations",
    "planning",
    "part",
    "things",
    "people",
    "came",
    "agentic",
    "behavior",
    "give",
    "task",
    "mathematical",
    "invoke",
    "agent",
    "perform",
    "mathematical",
    "calculations",
    "give",
    "task",
    "specific",
    "specific",
    "tools",
    "used",
    "lm",
    "internally",
    "invoke",
    "tool",
    "okay",
    "calculation",
    "generate",
    "invoke",
    "calculators",
    "let",
    "suppose",
    "uh",
    "image",
    "generation",
    "invoke",
    "model",
    "turn",
    "see",
    "things",
    "even",
    "expected",
    "architecture",
    "expect",
    "language",
    "models",
    "try",
    "trying",
    "uh",
    "see",
    "distribution",
    "better",
    "point",
    "used",
    "idea",
    "generation",
    "okay",
    "idea",
    "generation",
    "idea",
    "generation",
    "requires",
    "lot",
    "knowledge",
    "okay",
    "requires",
    "lot",
    "birth",
    "knowledge",
    "maybe",
    "knowledge",
    "separate",
    "thing",
    "lot",
    "breadth",
    "knowledge",
    "uh",
    "come",
    "good",
    "ideas",
    "never",
    "used",
    "verifier",
    "okay",
    "verifier",
    "perform",
    "good",
    "verification",
    "need",
    "constraints",
    "logic",
    "reasoning",
    "need",
    "constraints",
    "let",
    "suppose",
    "want",
    "build",
    "something",
    "take",
    "idea",
    "lot",
    "domain",
    "know",
    "different",
    "domains",
    "one",
    "way",
    "seeing",
    "large",
    "language",
    "models",
    "compressed",
    "version",
    "internet",
    "compressed",
    "version",
    "people",
    "might",
    "thinking",
    "oh",
    "many",
    "parameters",
    "billions",
    "parameters",
    "thankful",
    "billions",
    "parameters",
    "think",
    "data",
    "data",
    "internet",
    "huge",
    "huge",
    "basically",
    "loss",
    "lossy",
    "compression",
    "lot",
    "loss",
    "comparation",
    "got",
    "let",
    "suppose",
    "400",
    "billion",
    "parameter",
    "model",
    "performing",
    "really",
    "nice",
    "trying",
    "compressing",
    "knowledge",
    "together",
    "giving",
    "us",
    "answer",
    "smaller",
    "version",
    "compare",
    "human",
    "probably",
    "human",
    "lot",
    "parameters",
    "maybe",
    "hundreds",
    "power",
    "magnitude",
    "100x",
    "parameters",
    "state",
    "art",
    "gpt",
    "might",
    "state",
    "art",
    "large",
    "language",
    "model",
    "think",
    "compressed",
    "version",
    "whatever",
    "intern",
    "whatever",
    "data",
    "given",
    "tried",
    "compress",
    "weights",
    "b",
    "parameters",
    "okay",
    "saying",
    "vast",
    "knowled",
    "knows",
    "things",
    "know",
    "basically",
    "uh",
    "patterns",
    "patterns",
    "biology",
    "patterns",
    "mathematics",
    "patterns",
    "astronomy",
    "everything",
    "want",
    "work",
    "something",
    "better",
    "could",
    "could",
    "ask",
    "generate",
    "things",
    "idea",
    "generation",
    "discriminator",
    "verifier",
    "okay",
    "thing",
    "wrong",
    "pick",
    "thing",
    "right",
    "people",
    "tell",
    "reason",
    "probably",
    "models",
    "language",
    "model",
    "come",
    "new",
    "artical",
    "chains",
    "still",
    "try",
    "increase",
    "parameter",
    "uh",
    "expect",
    "come",
    "near",
    "intelligence",
    "something",
    "party",
    "thinking",
    "come",
    "new",
    "techniques",
    "rather",
    "rely",
    "attention",
    "keep",
    "increasing",
    "parameters",
    "keep",
    "increasing",
    "data",
    "feed",
    "magically",
    "think",
    "able",
    "probably",
    "many",
    "people",
    "think",
    "right",
    "see",
    "language",
    "models",
    "limit",
    "come",
    "new",
    "techniques",
    "incorporate",
    "things",
    "okay",
    "people",
    "saying",
    "uh",
    "new",
    "techniques",
    "new",
    "methods",
    "new",
    "architectures",
    "probably",
    "take",
    "us",
    "towards",
    "reasoning",
    "towards",
    "planning",
    "towards",
    "things",
    "right",
    "tell",
    "language",
    "models",
    "given",
    "data",
    "parameters",
    "able",
    "achieve",
    "would",
    "cly",
    "disagree",
    "probably",
    "questions",
    "think",
    "mf",
    "question",
    "yes",
    "sir",
    "right",
    "depends",
    "context",
    "contex",
    "window",
    "context",
    "context",
    "window",
    "1",
    "million",
    "tokens",
    "1",
    "million",
    "huge",
    "thing",
    "okay",
    "let",
    "suppose",
    "context",
    "miss",
    "context",
    "idea",
    "okay",
    "see",
    "contest",
    "length",
    "contest",
    "length",
    "much",
    "contest",
    "length",
    "exceed",
    "contest",
    "length",
    "happen",
    "wo",
    "know",
    "came",
    "generate",
    "basically",
    "sliding",
    "window",
    "sort",
    "thing",
    "let",
    "suppose",
    "restrict",
    "let",
    "suppose",
    "contex",
    "window",
    "length",
    "10",
    "happen",
    "b",
    "c",
    "something",
    "like",
    "let",
    "suppose",
    "till",
    "10",
    "okay",
    "came",
    "11",
    "token",
    "shift",
    "take",
    "much",
    "context",
    "generate",
    "take",
    "much",
    "context",
    "okay",
    "keep",
    "forgetting",
    "previously",
    "basically",
    "context",
    "contest",
    "length",
    "uh",
    "cap",
    "contest",
    "10",
    "keep",
    "sliding",
    "okay",
    "wo",
    "take",
    "two",
    "tokens",
    "consideration",
    "keep",
    "moving",
    "called",
    "long",
    "form",
    "long",
    "range",
    "dependencies",
    "probably",
    "came",
    "rnn",
    "rnn",
    "problem",
    "long",
    "range",
    "dependencies",
    "least",
    "1",
    "million",
    "easy",
    "people",
    "giving",
    "lot",
    "tokens",
    "know",
    "current",
    "state",
    "art",
    "might",
    "search",
    "internet",
    "ask",
    "largest",
    "contest",
    "l",
    "language",
    "model",
    "supports",
    "okay",
    "millions",
    "easily",
    "millions",
    "okay",
    "exceed",
    "uh",
    "theory",
    "practice",
    "wo",
    "remember",
    "said",
    "wo",
    "generate",
    "wo",
    "take",
    "basically",
    "things",
    "account",
    "generating",
    "next",
    "token",
    "okay",
    "clear",
    "yes",
    "second",
    "question",
    "right",
    "thinking",
    "yes",
    "yes",
    "hard",
    "people",
    "trying",
    "come",
    "tools",
    "yes",
    "good",
    "thing",
    "know",
    "facility",
    "people",
    "keeping",
    "crops",
    "sub",
    "thinking",
    "whole",
    "civil",
    "destructed",
    "somewhere",
    "thing",
    "start",
    "civilization",
    "exactly",
    "know",
    "name",
    "forgot",
    "name",
    "thing",
    "keep",
    "different",
    "types",
    "crop",
    "seeds",
    "sort",
    "things",
    "preserve",
    "let",
    "suppose",
    "catastrophe",
    "happened",
    "catastrophe",
    "happened",
    "everything",
    "got",
    "lost",
    "facility",
    "making",
    "facility",
    "would",
    "distracted",
    "humans",
    "able",
    "take",
    "seeds",
    "sort",
    "thing",
    "start",
    "thing",
    "done",
    "internet",
    "till",
    "suppose",
    "large",
    "language",
    "models",
    "data",
    "humans",
    "least",
    "95",
    "data",
    "humans",
    "models",
    "trained",
    "using",
    "data",
    "okay",
    "trained",
    "using",
    "data",
    "happened",
    "mix",
    "data",
    "humans",
    "actually",
    "internet",
    "snapshot",
    "snapshot",
    "internet",
    "kept",
    "uh",
    "models",
    "came",
    "people",
    "started",
    "training",
    "message",
    "emails",
    "basically",
    "written",
    "code",
    "even",
    "written",
    "called",
    "self",
    "referential",
    "problem",
    "student",
    "good",
    "teacher",
    "student",
    "intelligence",
    "thing",
    "happens",
    "gpt",
    "self",
    "referential",
    "problem",
    "whatever",
    "humans",
    "best",
    "level",
    "okay",
    "lm",
    "better",
    "best",
    "speaker",
    "world",
    "better",
    "better",
    "best",
    "writer",
    "world",
    "best",
    "writer",
    "cap",
    "trained",
    "human",
    "data",
    "tried",
    "learn",
    "human",
    "distribution",
    "okay",
    "said",
    "usually",
    "usually",
    "say",
    "trying",
    "simulate",
    "intelligence",
    "intelligent",
    "intelligent",
    "gave",
    "things",
    "till",
    "relativity",
    "till",
    "gravitation",
    "physics",
    "given",
    "gravitation",
    "able",
    "predict",
    "relativity",
    "okay",
    "able",
    "come",
    "concept",
    "intelligent",
    "said",
    "simulation",
    "intelligent",
    "behavior",
    "simulation",
    "tries",
    "whatever",
    "knowledge",
    "get",
    "generate",
    "hul",
    "spokes",
    "mixture",
    "things",
    "might",
    "look",
    "intelligent",
    "might",
    "look",
    "like",
    "said",
    "hard",
    "human",
    "differenti",
    "intent",
    "okay",
    "mimicking",
    "things",
    "see",
    "real",
    "real",
    "lie",
    "give",
    "uh",
    "suppose",
    "knowledge",
    "physics",
    "still",
    "gravitation",
    "come",
    "relativity",
    "okay",
    "hope",
    "got",
    "notion",
    "least",
    "intelligent",
    "simulating",
    "intelligence",
    "okay",
    "believe",
    "internet",
    "snapshot",
    "think",
    "people",
    "data",
    "set",
    "pure",
    "humans",
    "language",
    "model",
    "good",
    "human",
    "best",
    "human",
    "yeah",
    "think",
    "father",
    "question",
    "uh",
    "sir",
    "said",
    "multiple",
    "times",
    "previously",
    "top",
    "scientists",
    "working",
    "solving",
    "problem",
    "intelligence",
    "like",
    "large",
    "language",
    "models",
    "trying",
    "simulate",
    "intelligence",
    "expect",
    "uh",
    "intelligent",
    "said",
    "heard",
    "carefully",
    "couple",
    "minutes",
    "ago",
    "saying",
    "techniques",
    "used",
    "large",
    "english",
    "models",
    "limitation",
    "come",
    "something",
    "come",
    "new",
    "techniques",
    "concept",
    "consciousness",
    "want",
    "come",
    "close",
    "agi",
    "things",
    "like",
    "come",
    "new",
    "techniques",
    "problem",
    "many",
    "people",
    "working",
    "uh",
    "increasing",
    "parameters",
    "using",
    "techniques",
    "come",
    "new",
    "techniques",
    "problem",
    "thinking",
    "yes",
    "come",
    "closer",
    "using",
    "saying",
    "many",
    "people",
    "saying",
    "increase",
    "parameters",
    "use",
    "architecture",
    "probably",
    "learn",
    "reason",
    "case",
    "come",
    "new",
    "techniques",
    "maybe",
    "better",
    "maybe",
    "yes",
    "followup",
    "question",
    "sir",
    "want",
    "ask",
    "intelligence",
    "learned",
    "present",
    "inside",
    "us",
    "right",
    "born",
    "like",
    "intelligence",
    "issue",
    "intelligence",
    "learned",
    "every",
    "human",
    "intelligent",
    "okay",
    "levels",
    "intelligence",
    "okay",
    "hard",
    "define",
    "intelligence",
    "altogether",
    "okay",
    "struggling",
    "tell",
    "specific",
    "definition",
    "intelligence",
    "directly",
    "said",
    "jgpt",
    "lm",
    "intelligent",
    "simulating",
    "elligence",
    "definition",
    "even",
    "problem",
    "facing",
    "education",
    "system",
    "also",
    "ask",
    "people",
    "solve",
    "many",
    "problems",
    "point",
    "coaching",
    "centers",
    "unfortunate",
    "say",
    "people",
    "trained",
    "like",
    "gpt",
    "simulating",
    "intelligent",
    "seen",
    "people",
    "give",
    "instance",
    "seen",
    "studied",
    "among",
    "rankers",
    "ij",
    "rankers",
    "happened",
    "tell",
    "example",
    "physics",
    "guy",
    "laugh",
    "physics",
    "guy",
    "see",
    "pressure",
    "cooker",
    "okay",
    "cooking",
    "something",
    "know",
    "pressure",
    "cooker",
    "boils",
    "lot",
    "pressure",
    "inside",
    "try",
    "open",
    "somehow",
    "opened",
    "pressure",
    "go",
    "upwards",
    "huge",
    "blast",
    "create",
    "mess",
    "guy",
    "knew",
    "solve",
    "equations",
    "hard",
    "ced",
    "tried",
    "open",
    "pressure",
    "cooker",
    "asking",
    "hey",
    "man",
    "physics",
    "major",
    "physics",
    "copper",
    "think",
    "open",
    "blast",
    "okay",
    "intelligence",
    "kicken",
    "trained",
    "formal",
    "thinking",
    "trained",
    "give",
    "equation",
    "solved",
    "done",
    "okay",
    "problem",
    "training",
    "programs",
    "uh",
    "take",
    "people",
    "ask",
    "uh",
    "training",
    "id",
    "let",
    "suppose",
    "three",
    "couple",
    "years",
    "three",
    "four",
    "five",
    "years",
    "basically",
    "see",
    "patterns",
    "usually",
    "say",
    "difficult",
    "assignment",
    "difficult",
    "come",
    "assignment",
    "come",
    "new",
    "questions",
    "okay",
    "different",
    "websites",
    "questions",
    "uh",
    "previous",
    "questions",
    "actual",
    "task",
    "done",
    "professor",
    "comes",
    "new",
    "questions",
    "comes",
    "new",
    "question",
    "previously",
    "asked",
    "previous",
    "pattern",
    "okay",
    "somebody",
    "one",
    "level",
    "intelligence",
    "understand",
    "every",
    "material",
    "okay",
    "research",
    "hard",
    "first",
    "identify",
    "problem",
    "know",
    "problem",
    "okay",
    "read",
    "things",
    "see",
    "problem",
    "first",
    "coming",
    "good",
    "problem",
    "huge",
    "task",
    "sometimes",
    "takes",
    "years",
    "phd",
    "students",
    "know",
    "mf",
    "tell",
    "hard",
    "come",
    "actual",
    "problems",
    "problem",
    "want",
    "solve",
    "okay",
    "people",
    "take",
    "problem",
    "okay",
    "want",
    "thing",
    "better",
    "coming",
    "good",
    "problem",
    "coming",
    "let",
    "suppose",
    "knowledge",
    "something",
    "coming",
    "good",
    "questions",
    "actually",
    "hard",
    "task",
    "intelligent",
    "behavior",
    "vague",
    "level",
    "ca",
    "define",
    "incy",
    "defined",
    "problem",
    "solved",
    "would",
    "con",
    "yes",
    "wanted",
    "say",
    "something",
    "research",
    "resarch",
    "music",
    "humans",
    "idea",
    "said",
    "told",
    "idea",
    "generation",
    "good",
    "idea",
    "generation",
    "lot",
    "knowledge",
    "world",
    "mathematics",
    "physics",
    "ide",
    "validate",
    "ideas",
    "good",
    "validate",
    "okay",
    "let",
    "suppose",
    "ask",
    "physics",
    "problem",
    "okay",
    "knowledge",
    "knows",
    "knows",
    "stuff",
    "come",
    "ideas",
    "let",
    "suppose",
    "find",
    "actual",
    "novel",
    "thing",
    "wo",
    "able",
    "okay",
    "human",
    "let",
    "suppose",
    "take",
    "einstein",
    "einstein",
    "studied",
    "light",
    "information",
    "give",
    "information",
    "gpt",
    "property",
    "light",
    "material",
    "light",
    "okay",
    "say",
    "light",
    "say",
    "nothing",
    "basically",
    "say",
    "something",
    "wo",
    "content",
    "value",
    "style",
    "value",
    "give",
    "human",
    "human",
    "think",
    "ability",
    "come",
    "new",
    "things",
    "come",
    "new",
    "things",
    "come",
    "simulating",
    "intelligence",
    "actually",
    "intelligent",
    "said",
    "idea",
    "generation",
    "longer",
    "context",
    "seen",
    "yes",
    "even",
    "ideas",
    "might",
    "written",
    "somebody",
    "internet",
    "cap",
    "think",
    "like",
    "human",
    "okay",
    "many",
    "connections",
    "probably",
    "less",
    "connected",
    "may",
    "ask",
    "may",
    "ask",
    "friends",
    "okay",
    "chat",
    "gpt",
    "access",
    "entire",
    "internet",
    "knowledge",
    "whatever",
    "basically",
    "means",
    "contact",
    "8",
    "billion",
    "people",
    "okay",
    "fair",
    "competition",
    "ask",
    "generate",
    "ideas",
    "gener",
    "ideas",
    "human",
    "human",
    "restricted",
    "whatever",
    "knowledge",
    "give",
    "time",
    "let",
    "suppose",
    "give",
    "ji",
    "time",
    "humans",
    "give",
    "hum",
    "one",
    "month",
    "humans",
    "come",
    "something",
    "novel",
    "basically",
    "verify",
    "somewhere",
    "thinking",
    "thought",
    "okay",
    "give",
    "chat",
    "gpt",
    "see",
    "chat",
    "gp",
    "give",
    "idea",
    "generations",
    "noel",
    "humans",
    "tried",
    "uh",
    "simulate",
    "whatever",
    "whatever",
    "web",
    "tried",
    "context",
    "tried",
    "generate",
    "whatever",
    "knowledge",
    "tried",
    "generate",
    "tokens",
    "said",
    "humans",
    "actually",
    "intelligence",
    "keep",
    "thinking",
    "keep",
    "thinking",
    "problem",
    "language",
    "model",
    "ask",
    "stop",
    "train",
    "stop",
    "stop",
    "let",
    "suppose",
    "speaking",
    "right",
    "humans",
    "never",
    "shut",
    "till",
    "death",
    "okay",
    "even",
    "sleeping",
    "still",
    "thinking",
    "something",
    "god",
    "knows",
    "thinking",
    "god",
    "knows",
    "thoughts",
    "processing",
    "models",
    "let",
    "suppose",
    "uh",
    "train",
    "deploy",
    "system",
    "retrain",
    "continuously",
    "anything",
    "humans",
    "see",
    "right",
    "talking",
    "things",
    "happen",
    "maybe",
    "subconsciously",
    "brain",
    "dreams",
    "also",
    "think",
    "complex",
    "process",
    "going",
    "uh",
    "seems",
    "intelligent",
    "behavior",
    "leads",
    "intelligent",
    "behavior",
    "think",
    "lms",
    "okay",
    "uh",
    "process",
    "information",
    "like",
    "mechanism",
    "people",
    "reinforcement",
    "learning",
    "human",
    "feedback",
    "keeping",
    "machines",
    "whatever",
    "generate",
    "learn",
    "self",
    "problem",
    "think",
    "ask",
    "question",
    "answer",
    "question",
    "buf",
    "yes",
    "sir",
    "give",
    "pro",
    "problem",
    "tell",
    "feasibility",
    "problem",
    "yes",
    "yeah",
    "huh",
    "yes",
    "yes",
    "good",
    "point",
    "let",
    "suppose",
    "ask",
    "generate",
    "beautiful",
    "house",
    "beautiful",
    "house",
    "house",
    "structural",
    "problems",
    "wo",
    "ever",
    "exist",
    "okay",
    "let",
    "suppose",
    "make",
    "tajal",
    "say",
    "put",
    "put",
    "color",
    "tajal",
    "wo",
    "wo",
    "able",
    "make",
    "feasible",
    "real",
    "world",
    "calcul",
    "ask",
    "hum",
    "humans",
    "think",
    "constraint",
    "okay",
    "put",
    "structure",
    "maybe",
    "structure",
    "feasible",
    "may",
    "generate",
    "things",
    "feasible",
    "realistic",
    "might",
    "look",
    "good",
    "issue",
    "might",
    "look",
    "good",
    "people",
    "might",
    "get",
    "convinced",
    "oh",
    "looks",
    "good",
    "try",
    "generated",
    "way",
    "trained",
    "generate",
    "something",
    "good",
    "looking",
    "wo",
    "reality",
    "yes",
    "question",
    "yes",
    "yes",
    "get",
    "humans",
    "superior",
    "superior",
    "yes",
    "question",
    "models",
    "continuously",
    "trained",
    "like",
    "sometimes",
    "new",
    "research",
    "paper",
    "new",
    "research",
    "coming",
    "version",
    "control",
    "models",
    "continuously",
    "trained",
    "yes",
    "tell",
    "things",
    "like",
    "uh",
    "happens",
    "usually",
    "transformers",
    "came",
    "called",
    "architectural",
    "change",
    "okay",
    "ar",
    "change",
    "done",
    "different",
    "let",
    "suppose",
    "see",
    "gpt",
    "architecture",
    "see",
    "lama",
    "architecture",
    "would",
    "allow",
    "discuss",
    "go",
    "language",
    "modeling",
    "part",
    "see",
    "framework",
    "designed",
    "pretty",
    "much",
    "similar",
    "internals",
    "quite",
    "different",
    "let",
    "suppose",
    "also",
    "used",
    "embeddings",
    "us",
    "embeddings",
    "okay",
    "use",
    "something",
    "called",
    "uses",
    "positional",
    "embeddings",
    "use",
    "something",
    "called",
    "rotational",
    "embeddings",
    "okay",
    "also",
    "uses",
    "attention",
    "called",
    "self",
    "attention",
    "uses",
    "something",
    "called",
    "attention",
    "cavic",
    "cas",
    "group",
    "multi",
    "attention",
    "okay",
    "see",
    "indivual",
    "parts",
    "see",
    "corresponding",
    "thing",
    "complicated",
    "bit",
    "see",
    "use",
    "simple",
    "norm",
    "layer",
    "use",
    "something",
    "called",
    "rms",
    "norm",
    "one",
    "aspect",
    "aral",
    "chain",
    "replace",
    "things",
    "okay",
    "thing",
    "come",
    "novel",
    "algorithms",
    "infuse",
    "let",
    "suppose",
    "introduce",
    "things",
    "introduce",
    "algorithms",
    "okay",
    "one",
    "dimension",
    "one",
    "dimension",
    "actually",
    "change",
    "come",
    "new",
    "techniques",
    "also",
    "come",
    "new",
    "techniques",
    "maybe",
    "lot",
    "improvements",
    "change",
    "weight",
    "range",
    "change",
    "way",
    "inf",
    "first",
    "uh",
    "try",
    "come",
    "lot",
    "uh",
    "mumbo",
    "jumbo",
    "let",
    "two",
    "ways",
    "think",
    "one",
    "thing",
    "experiment",
    "point",
    "view",
    "experimentalist",
    "think",
    "add",
    "things",
    "experiment",
    "theorist",
    "point",
    "view",
    "see",
    "okay",
    "let",
    "go",
    "theory",
    "happen",
    "try",
    "first",
    "understand",
    "thetical",
    "perspective",
    "try",
    "ai",
    "become",
    "like",
    "become",
    "like",
    "marketing",
    "sorry",
    "say",
    "become",
    "like",
    "first",
    "person",
    "get",
    "market",
    "rather",
    "science",
    "like",
    "matter",
    "somebody",
    "else",
    "published",
    "convinced",
    "give",
    "something",
    "published",
    "people",
    "race",
    "whatever",
    "find",
    "publish",
    "okay",
    "actually",
    "verified",
    "problem",
    "somebody",
    "research",
    "found",
    "one",
    "example",
    "something",
    "like",
    "publish",
    "people",
    "try",
    "pick",
    "research",
    "cite",
    "becomes",
    "whole",
    "line",
    "someday",
    "guy",
    "claims",
    "root",
    "thing",
    "wrong",
    "okay",
    "whole",
    "research",
    "go",
    "goes",
    "gutters",
    "happening",
    "right",
    "funds",
    "associated",
    "grants",
    "ai",
    "people",
    "want",
    "get",
    "papers",
    "accept",
    "write",
    "something",
    "lms",
    "high",
    "chance",
    "people",
    "accept",
    "okay",
    "research",
    "basically",
    "done",
    "usually",
    "oil",
    "change",
    "different",
    "mechanism",
    "training",
    "also",
    "maybe",
    "within",
    "within",
    "component",
    "modified",
    "think",
    "modification",
    "component",
    "maybe",
    "together",
    "scrap",
    "whole",
    "architecture",
    "come",
    "new",
    "things",
    "together",
    "okay",
    "see",
    "uh",
    "major",
    "portion",
    "happening",
    "right",
    "increasing",
    "data",
    "increas",
    "number",
    "parameters",
    "lot",
    "computation",
    "sam",
    "alman",
    "recently",
    "asked",
    "7",
    "trillion",
    "dollar",
    "dollars",
    "even",
    "economy",
    "many",
    "countries",
    "less",
    "okay",
    "asking",
    "want",
    "get",
    "bigger",
    "gpus",
    "thinking",
    "feed",
    "data",
    "ar",
    "everything",
    "come",
    "see",
    "humans",
    "humans",
    "ined",
    "want",
    "mimic",
    "human",
    "brain",
    "okay",
    "try",
    "give",
    "sort",
    "birds",
    "view",
    "different",
    "things",
    "people",
    "ar",
    "change",
    "change",
    "training",
    "try",
    "come",
    "new",
    "algorithms",
    "try",
    "come",
    "new",
    "data",
    "best",
    "part",
    "coming",
    "new",
    "techniques",
    "come",
    "new",
    "techni",
    "suppose",
    "attention",
    "technique",
    "transform",
    "introduced",
    "whatever",
    "see",
    "language",
    "models",
    "performing",
    "technique",
    "introduced",
    "2017",
    "okay",
    "paper",
    "titled",
    "attention",
    "need",
    "attention",
    "need",
    "uh",
    "published",
    "google",
    "okay",
    "open",
    "basically",
    "used",
    "open",
    "funded",
    "microsoft",
    "basically",
    "rivals",
    "publish",
    "research",
    "know",
    "much",
    "amazing",
    "could",
    "opena",
    "used",
    "gpts",
    "microsoft",
    "basically",
    "partially",
    "funding",
    "opener",
    "uh",
    "made",
    "even",
    "satya",
    "said",
    "made",
    "google",
    "dance",
    "google",
    "project",
    "running",
    "something",
    "called",
    "lambda",
    "okay",
    "anticipating",
    "somebody",
    "else",
    "would",
    "use",
    "techniques",
    "come",
    "model",
    "first",
    "openi",
    "basically",
    "released",
    "prior",
    "notice",
    "google",
    "know",
    "people",
    "wave",
    "released",
    "b",
    "disaster",
    "okay",
    "ready",
    "plan",
    "usually",
    "companies",
    "plan",
    "okay",
    "one",
    "year",
    "two",
    "years",
    "three",
    "years",
    "plan",
    "open",
    "released",
    "chat",
    "gpt",
    "people",
    "google",
    "people",
    "forced",
    "code",
    "r",
    "time",
    "every",
    "professor",
    "working",
    "google",
    "time",
    "masters",
    "asked",
    "come",
    "office",
    "okay",
    "wanted",
    "see",
    "threat",
    "gpt",
    "performing",
    "know",
    "happened",
    "people",
    "coming",
    "perplex",
    "thinking",
    "main",
    "stream",
    "income",
    "search",
    "somehow",
    "gpt",
    "language",
    "models",
    "used",
    "search",
    "business",
    "model",
    "gone",
    "code",
    "alert",
    "see",
    "beauty",
    "research",
    "research",
    "front",
    "come",
    "new",
    "technique",
    "talking",
    "techniques",
    "came",
    "technique",
    "attention",
    "okay",
    "gpt",
    "came",
    "one",
    "concept",
    "attention",
    "transformers",
    "came",
    "large",
    "language",
    "models",
    "come",
    "good",
    "concepts",
    "good",
    "uh",
    "sort",
    "uh",
    "things",
    "used",
    "somewhere",
    "else",
    "way",
    "research",
    "conducted",
    "got",
    "something",
    "want",
    "publish",
    "okay",
    "oranic",
    "change",
    "learn",
    "lot",
    "stuff",
    "come",
    "new",
    "techniques",
    "revolutionize",
    "field",
    "rather",
    "seeing",
    "taking",
    "get",
    "result",
    "maybe",
    "get",
    "grants",
    "get",
    "result",
    "accepted",
    "wrong",
    "way",
    "look",
    "think",
    "fad",
    "question",
    "yes",
    "fad",
    "sir",
    "wanted",
    "ask",
    "question",
    "long",
    "time",
    "ago",
    "like",
    "lecture",
    "wanted",
    "ask",
    "um",
    "intelligence",
    "way",
    "structure",
    "brain",
    "structure",
    "neurons",
    "arrang",
    "arrangement",
    "constitutes",
    "intelligence",
    "ca",
    "replicate",
    "oh",
    "hard",
    "question",
    "know",
    "brain",
    "know",
    "anything",
    "know",
    "little",
    "brain",
    "trying",
    "mimic",
    "whatever",
    "deep",
    "learning",
    "whatever",
    "trying",
    "uh",
    "recurrent",
    "neural",
    "networks",
    "feed",
    "inspired",
    "brain",
    "okay",
    "ask",
    "uh",
    "understood",
    "brain",
    "fully",
    "understood",
    "many",
    "minuscal",
    "parts",
    "people",
    "interested",
    "biology",
    "neuroscience",
    "want",
    "see",
    "different",
    "aspects",
    "want",
    "computationally",
    "perform",
    "computations",
    "see",
    "things",
    "inspired",
    "brain",
    "humans",
    "limitation",
    "understand",
    "brain",
    "huge",
    "research",
    "going",
    "brain",
    "brain",
    "rodents",
    "different",
    "things",
    "conut",
    "intelligence",
    "neurons",
    "connected",
    "led",
    "curiosity",
    "deep",
    "larning",
    "field",
    "came",
    "existence",
    "took",
    "recently",
    "recently",
    "mean",
    "think",
    "2006",
    "onwards",
    "took",
    "okay",
    "2012",
    "became",
    "uncontrollable",
    "still",
    "bubble",
    "people",
    "learning",
    "things",
    "many",
    "uh",
    "people",
    "contest",
    "nobel",
    "prize",
    "giving",
    "ai",
    "people",
    "saying",
    "stopping",
    "frontier",
    "research",
    "happened",
    "frontier",
    "research",
    "frontier",
    "research",
    "mean",
    "see",
    "applications",
    "research",
    "find",
    "application",
    "something",
    "give",
    "prize",
    "noal",
    "priz",
    "ai",
    "chemistry",
    "gave",
    "baker",
    "lab",
    "uh",
    "physics",
    "gave",
    "hinton",
    "hfield",
    "hinton",
    "came",
    "back",
    "propagation",
    "hfield",
    "came",
    "called",
    "hfield",
    "networks",
    "links",
    "competen",
    "aspect",
    "computer",
    "scientists",
    "people",
    "saying",
    "give",
    "pri",
    "sending",
    "wrong",
    "message",
    "message",
    "pursue",
    "fields",
    "money",
    "capitalist",
    "society",
    "pursue",
    "frontier",
    "research",
    "frontier",
    "research",
    "rather",
    "curious",
    "want",
    "discover",
    "curiosity",
    "implication",
    "direct",
    "implication",
    "good",
    "thing",
    "find",
    "applications",
    "frontier",
    "research",
    "necessary",
    "find",
    "see",
    "mathematics",
    "mathematics",
    "tool",
    "people",
    "study",
    "mathematics",
    "ask",
    "okay",
    "abra",
    "algebra",
    "probably",
    "wo",
    "know",
    "get",
    "use",
    "maybe",
    "someday",
    "somebody",
    "use",
    "case",
    "people",
    "uh",
    "give",
    "prizes",
    "fund",
    "things",
    "seem",
    "practically",
    "okay",
    "regarding",
    "human",
    "brain",
    "people",
    "thinking",
    "okay",
    "stop",
    "research",
    "funding",
    "also",
    "people",
    "want",
    "study",
    "brain",
    "probably",
    "see",
    "uh",
    "direct",
    "implications",
    "real",
    "world",
    "case",
    "stop",
    "frontier",
    "research",
    "case",
    "short",
    "answer",
    "know",
    "det",
    "brains",
    "know",
    "probably",
    "much",
    "techniques",
    "techniques",
    "develop",
    "intelligence",
    "comes",
    "nobody",
    "knows",
    "links",
    "consciousness",
    "consciousness",
    "know",
    "quote",
    "scripture",
    "said",
    "know",
    "little",
    "rule",
    "questions",
    "anar",
    "asked",
    "question",
    "yes",
    "question",
    "okay",
    "least",
    "got",
    "perspec",
    "genera",
    "ai",
    "particular",
    "language",
    "models",
    "least",
    "sort",
    "okay",
    "expect",
    "models",
    "yes",
    "okay",
    "somebody",
    "puts",
    "linkedin",
    "post",
    "hey",
    "asked",
    "jg",
    "laugh",
    "post",
    "saying",
    "talk",
    "yes",
    "yes",
    "actually",
    "stop",
    "recording"
  ],
  "keywords": [
    "sort",
    "session",
    "uh",
    "stuff",
    "usually",
    "go",
    "said",
    "long",
    "talking",
    "ai",
    "take",
    "particular",
    "would",
    "start",
    "questions",
    "people",
    "bit",
    "also",
    "generic",
    "architecture",
    "maybe",
    "less",
    "genera",
    "work",
    "something",
    "like",
    "sir",
    "yes",
    "need",
    "least",
    "even",
    "100",
    "billion",
    "parameters",
    "lot",
    "tell",
    "okay",
    "question",
    "answer",
    "first",
    "keep",
    "ask",
    "many",
    "might",
    "using",
    "tools",
    "right",
    "context",
    "use",
    "used",
    "sometimes",
    "good",
    "talk",
    "let",
    "wo",
    "write",
    "much",
    "things",
    "generate",
    "generation",
    "generating",
    "basically",
    "point",
    "humans",
    "data",
    "suppose",
    "speak",
    "want",
    "generated",
    "image",
    "see",
    "means",
    "actually",
    "know",
    "detail",
    "pixels",
    "somehow",
    "certain",
    "make",
    "sense",
    "human",
    "cat",
    "task",
    "find",
    "pixel",
    "values",
    "way",
    "makes",
    "seeing",
    "say",
    "new",
    "content",
    "text",
    "based",
    "pattern",
    "patterns",
    "called",
    "distribution",
    "distributions",
    "coming",
    "algorithms",
    "try",
    "different",
    "come",
    "happen",
    "language",
    "came",
    "networks",
    "models",
    "took",
    "whole",
    "world",
    "saying",
    "previous",
    "whatever",
    "sample",
    "sampling",
    "problem",
    "gave",
    "thing",
    "exist",
    "show",
    "example",
    "got",
    "give",
    "second",
    "think",
    "person",
    "kept",
    "part",
    "showed",
    "hope",
    "get",
    "mean",
    "trying",
    "rather",
    "probability",
    "height",
    "range",
    "10",
    "call",
    "h",
    "may",
    "case",
    "1",
    "distributed",
    "comes",
    "count",
    "one",
    "population",
    "value",
    "look",
    "number",
    "words",
    "english",
    "documents",
    "corpus",
    "word",
    "wanted",
    "internet",
    "follow",
    "specific",
    "version",
    "idea",
    "similarly",
    "vocabulary",
    "learn",
    "pick",
    "learned",
    "create",
    "784",
    "search",
    "space",
    "huge",
    "tried",
    "put",
    "together",
    "time",
    "giving",
    "asked",
    "learning",
    "understand",
    "auto",
    "large",
    "move",
    "thinking",
    "bagram",
    "model",
    "mat",
    "w1",
    "till",
    "n",
    "explain",
    "wn",
    "frequency",
    "w2",
    "w3",
    "four",
    "times",
    "convert",
    "two",
    "probabilities",
    "sentence",
    "clear",
    "token",
    "made",
    "seen",
    "code",
    "perform",
    "window",
    "everything",
    "problems",
    "tokenization",
    "level",
    "beautiful",
    "keeping",
    "written",
    "applications",
    "random",
    "next",
    "change",
    "saw",
    "step",
    "previously",
    "done",
    "attention",
    "l",
    "training",
    "given",
    "tokens",
    "chat",
    "gpt",
    "happened",
    "c",
    "stop",
    "gpd",
    "length",
    "memory",
    "techniques",
    "greedy",
    "beams",
    "self",
    "never",
    "mistake",
    "style",
    "somebody",
    "facts",
    "wrong",
    "instance",
    "probably",
    "prompt",
    "formal",
    "interpreter",
    "hard",
    "reasoning",
    "reason",
    "solve",
    "bigger",
    "novel",
    "oh",
    "able",
    "us",
    "play",
    "game",
    "object",
    "chair",
    "legs",
    "house",
    "view",
    "better",
    "add",
    "numbers",
    "logic",
    "knows",
    "structure",
    "multiply",
    "digit",
    "technique",
    "three",
    "9",
    "plus",
    "2",
    "trained",
    "cipher",
    "replaced",
    "b",
    "road",
    "13",
    "performing",
    "behavior",
    "knowledge",
    "ideas",
    "mathematics",
    "intelligence",
    "contest",
    "best",
    "intelligent",
    "physics",
    "pressure",
    "open",
    "years",
    "research",
    "brain",
    "google",
    "frontier"
  ]
}