{
  "text": "hello everyone and welcome back to my\nanother video well this video right now\nwill be a little different from previous\nones well uh recently I was\ninvestigating and checking the war to\nvac 2 framework for solves for wise\nlearning of speech representation well\nthat's kind of paper from 2020 and you\nif you are related or doing something in\nspeech recognition test probably you\nalready heard it well this is a second\nversion of this kind of paper there was\nof course the first version War to vac 2\nsimply and this is release paper from uh\nFacebook and basically uh what is what\nthis is is that uh kind of paper that\ntells us how they released a model that\nis right now is one of the best in\nspeech recognition models overall well\nit's three years already for for this\nMundo but still it's one of the best\nmodels we can use so basically uh in\nthis tutorial I'll demonstrate you how\neasy we can fine tune this model with\nour own data for speech recognition\ntasks so I'm not going to cover the\ntheory and the modal specifications\nitself I'm mainly gonna give you a code\nthat I use to fine tune our model on my\nown data with myself my own tokenizer\nand Etc the this will be pretty simple\nand we are not going deep into the\ntheory or the coding stuff I'll simply\ngonna cover it in short so basically uh\nthat's pretty nice model because uh what\nwe need to do is simply we read about\nyour data we receive this kind of raw\naudio and we simply feed this data\nstraight into this model and it has a\nconclusion neural networks inside\nTransformer encoder layer and there it\ngives us a output well basically this\nmodel is focused on CTC loss and that's\nwhat I'm gonna use in this kind of\ntutorial and demonstrate how to use it\nas simple as it is and we don't need to\nthrow in this form model from scratch we\ncan use the pertrained model and we\nfine-tune on this model so basically\nthis paper demonstrates uh what they did\nand they explained why they receive such\ngreat results and Etc and we're gonna\nuse it basically we we're gonna find\ntunics of course there is some results\nof data that are labeled labeled they do\nthe some comparison on it but I don't\nthink that you are so interested on this\nso basically this model is trained and\npublished on hugging face and we can use\nit from this and this is the base and\nyou can see that's the Facebook and it\ngives us some explanations and of course\nin in hugging phase you can find some\nexplanations how they train this model\nand Etc so I'll give you a video\ntutorial how to train this model and how\nto run the inference when you want to\ndeploy it and if you don't want to use\niport or pytorch installation itself\nthere so what we got to do is simply\njump into the code on of my ml2 package\nthat I was working recently and so on so\nyou might see that I have what worked\ntwo torch tutorial here and you might\nheard it right right now I'm not going\nto use tensorflow and I will use a pie\ntorch and at the end of this tutorial I\nwill explain you why and that's going to\nbe pretty simple so what are the\nrequirements from this tutorial is that\nyou need torch at least this version\nbecause I tested on it and I'm not sure\nif it will work on higher versions and\nyou need Transformers so you could\ndownload this pre-trained model you need\nthis mltu package that I recently\nreleased but I'll update the version\nthat you're gonna use and of course on\nthe next and there might be on an extra\non time\nokay let's import this that's right and\nlet's go straight to the training code\nuh here and I'll give you explanation\nwhat I do here so basically uh I import\nmany stuff most of the stuff is from my\nmltu package as you may see there is a\nmodel CTC law that I use data provider\nmetrics to track character error rate\nand board error rate and of course some\ncallbacks are really stopping bottom of\ncosine model 2 on the next answer board\nso we could track the metrics how we\ntrain them Etc and model checkpoint and\nof course there are some augmentations\nrelated to audio So Random audio noise\nrandom Audio Pitch and Time Stretch I\nimplemented these few augmentation to\nimprove uh our data to scale it so our\nmodel would uh fit it better on it so\nbasically what we do here I still have\nas you can see above to vac 2 for CTC\nsee that basically will be from\nTransformers library that we should have\ninstalled while using this and there's a\nfew more functions from pytorch so but\nyou don't need to focus on it right now\nso what I'm doing here as you might see\nI download a simple kind of LG speech\ndata set uh that's pretty huge data set\nthat has 30 000 uh samples of speech\nthat are labeled and what we need to do\nwe simply need to download them and I\nuse this kind of function in to download\nit so it will be placed in dataset ldot\nspeech metadata and so on so if I open\nmy data sets you can see that I have\nthis data already downloaded and\nit's pretty simple so let's move on\nso next I have some here a vocabulary\nthat this is the usual stuff that have\nthis kind of characters inside this uh\ndata set so what I need to do I need to\npre-process this data set so I read this\nmetadata path method there's a file that\nuses separator of the following and I\nsimply read the strings of\ntranscriptions in and what I do here so\nuh I joined this kind of labels and I\nuse a lower as you can see this this\nmeans that I don't want to have a\ncapitalized characters I lower all of\nthem that's for Simplicity reason\nbecause of course when we are trying to\nrecognize speech we don't care if it's a\ncapital or not so simply it's way easier\nand to train the model when all\ncharacters are lower and this simplifies\nour vocabulary that we need to use\nso great so then I use a data provider\nthat's very simple to tensorflow data\nprovider and uh actually I inherited\neverything from the tensorflow data\nprovider but you might see that I it has\nsome additional parameters as workers\nuse multi-processing or not and Etc so\nthis time when we are working with a\nsound data it's really hard on the CPU\nside because it needs to load all this\naudio from our disk and if we need to\naugment it it's even harder so usually\nif you have a strong CPU so I recommend\nto use it with multi-processing but if\nit if it's there's something wrong it it\ncan't you be used on multiple on CPU it\nwill be used as threadpool executor and\nnext and Etc it simply iterates all our\ndata\nso what we do here so we skip the\nvalidation I have the configurations\nhere that you can dig in and check what\nit has and how many apple I want to\ntrain what will be my batch size would\nbe learning rates and Etc uh what will\nbe warm up epochs uh whether I want to\nuse mixed Precision or not and Etc\nthere's many different things and this\nconfig is saved along the model\ntrainable mode so let's move on and as\nyou might see I I'm gonna read use this\ndata preprocessors this is audio reader\nthis means that it will read out you\nhave a sample rate of 16 000 and it will\ncreate a specific audio object of my\nmltu object then we need to use label\nindexer that will label our characters\ninto the integer representation and then\nwe need to use this batch post\nprocessors so when we are training our\nmodel we need to have\naudios for example it Audios in one\nbatch so when we want to train our model\nall these audio should be the same\nlength this means that we use audio\npadding and I recommend use to use this\nor on batch and\nthen it will be padded to maximum size\nof possible length in the audios and the\nsame we apply for labels and we don't\nneed to pad for example if we have\nsentences with uh 20 characters we don't\nneed to add it to 100 characters\nit will be a little harder to train our\nmodel so for efficiency it's better to\npad only to the maximum length that\nexist in our batch so this is the\npurpose of this and this is as I\nmentioned is whether we want to use\nmultiprocessing or not\nand if we are training our model on some\nweaker CPU it might not handle this very\nmuch for example on my computer\nI can't use multi-processing on windows\nat least but if I try to run this on\nLinux it works so it depends on what\noperating system you have what you have\nand Etc and this is kind of very good\nworker that when we work with audio\nbecause it will work in a background\nload everything for a trainable model\nfor us in the background that's pretty\namazing how it works and there of course\nwe split our data into validation and\ntraining and we're gonna save this along\nthe model what our training and volition\ndata sets so this mean this is simply\nfor validating our trainable model and\nif we want we can use augmentation and I\nam not using it because it eats a lot of\nCPU Papa in this it's pretty hard to\ntrain it takes way more time to train\naugmented audios so the crucial part\nright now is here we need to create this\nuh about to vac model from a pytorch\nTransformers so basically we've used\nboth to Vector for CTC and we use from\npre-trainer and I as you see I see I I'm\nusing per trained name and this is the\nspecific name from hugging faces and\nwe're simply gonna download this model\nand load it on and what I'm doing here\ndifferently is that hidden States I'm\nchanging my book but the vocabulary size\nand I ignore mismatches this means that\nI'm changing the head of this model and\nsimply put my own head that will be\ntrainable for classification and when we\nfeed the forward pass here we input our\naudio data audio batches and Etc and we\nsimply\nreceive the outputs and we use the\nlogins from these outputs into our into\nthe logsoft max function and whole\noutput will go to the CTC loss of course\nyou can implement this differently but I\nfor me I found this is pretty good\nimplementation so I follow with it so\nhere we create this custom model of\npytorch and we have it here and I put\nthis on GPU because I'm totally not\nrecommend to train try to train it on\nCPU because you're gonna need to wait a\nmonth to train it I believe and of\ncourse if you want to train it on pretty\nlarge data set you need kind of a very\nstrong GPU or use multiple gpus and of\ncourse pretty strong CPU remember that\ntrain or out Transformers on how the\nit's it takes a lot of time so it's up\nto you so now next we defined a bar map\ncosine design and I'll explain what it\ndoes but it's pretty necessary to to use\nwith Transformers so it means that we\nstart from really low learning rate and\nevery Epoch we increase this learning\nrate to faster and better train our\nmotor so then I use the tensorboard\nthat's pretty self-explainable we use\nearly stopping we track the validation\ncharacter erroring and we of course we\ncould track the word error read but I\nchose to use Charter error rate it's up\nto you you can you what you want to do\nso next I use the model checkpoint this\nmeans that I'll save my models or as\nMundo PT this is the weights and I will\nsave the best weights according to\nvalidation character error rate that's\nwhat I'm gonna use\nand of course at the end uh I wanna say\nmy best Moodle into on the next format\nso I don't need to save the pytorch mode\nbecause it's pretty hard when you I'm\nusing a pie torch to load back the\ntrainer model on another platform for\nexample if I want to use this on\nRaspberry Pi it's pretty impossible to\nload this bag without knowing the\narchitecture or without installing the\npie torch so I chose to convert the\nmodel to uninx and then I it's very\nsimply for me to run this on any other\ndevice I I want so that's it and here\nyou might see that I have a custom model\nobject that it handles all the training\nstuff the callbacks the metrics and Etc\nso here what I do here I feed to this\nmodel are a model as you can see custom\nmodel that's my buff to back model I use\nCTC lows and\nuh that's kind of custom lost and we\nhere use my our vocabulary for for this\nkind of data set and then I use Adam B\nOptimizer you can use Simple Adam uh it\nwork both for well well but it was shown\nthat Adam W Works slightly better so why\nnot using it and then I want to track my\ncharacter error rate and what error rate\nthat's kind of usual when we are working\nthe towards uh audio or any recognition\nrelated to speech words characters\ndoesn't matter then I use a mixed\nPrecision we can find it using configs\nand usually if you if you have good GPU\nit will dramatically increase the\ntraining for you you two times least and\nit will take less memory on GPU so we\ncan use larger batches but it's up to\nyou I I chose to use this mixer\nprecision as true and then I I saved is\nare train and validation CSV files from\nmy split so later I could\ntest my how my module works that I would\ntrain and convert on the next so I want\nto use this on the same validation data\nbecause of course on train data it will\nwork well and that's it we call the fit\nand we waited to finish well I didn't\nwant to train it right now because if I\nstart training my CPU jumps to 100 and I\nmight face a blue screen or simply uh\nthe video recording might start lagging\ninsanely but to prove that it works I\ncan start it immediately and kill it\nwhen it starts streaming so let's run\nand let's see how it keeps loading\nsomething here okay and as you can see\nit takes one half\n1474 batches to train and that's great\nand as you can see it doesn't handle\nmultiple processing so it's switching to\nmulti threading that's as I expected\nbecause I know that it doesn't work on\nmy machine like that and within the\nfirst batch it will be pretty good\nokay\nand as you can see it it keeps training\nand it showed us chart the error rate\nword error rate and we simply need to\nwait until it completes this stuff but\nright now because I'm recording it's way\nslower and I believe it might be lagging\non my side so I don't want this to\ncontinue training while I'm recording so\ngreat I'll kill it because I already\ntrained this model and I already tested\neverything out\nthat skill kill\nthis stuff okay I'll simply kill this\none so great and I believe you're\ninterested how it trained how you can\nprove it okay uh I have a pre-trained\nmodel and if I go to my models I have\nmoved to work too and here is my train\nmodel that already has a PT weights and\nmodel on next let's train motor but\nfirst before going to the test part\nlet's look at the tensorboard how it\ntrained and I have attention board for\nexactly that what I showed you and let's\nmaximize this one and you might see that\nit started training and there was 10\nepoths of warm up and it wasn't\nimproving at all so that's the worst\npart with this model because we when we\nit's training for such epos and it's not\ndecreasing we are not sure whether it's\ntraining or not but look at this uh\nafter 12 epos it dramatically dropped\nsomewhere here and you might see that\ntrain is you checked Android of training\nis 0.21 and validation is only 0.01 so\nthat's a huge difference and if we\nscroll to the end you might see that uh\nit dramatically decreased and let's look\nat the word array because that's really\ntowards and you might see that at the\nend our training uh was only two percent\nerror and validation was 1.8 error so\nthis means that it works on our eight\naudio data and our loss looks exactly\nthe same so now let's look at our\nlearning right so that's exactly what I\nwas talking what is this uh uh warm up\nso this means that you might see that it\nstarts increasing the learning rate to\nsome learning rate we Define so this is\none point one e minus five is my\nlearning rate that it should have\nachieved during the warm up and it\nachieved this and it will train it to\ncontinue training and it was decreasing\nour word our rate was decreasing\neverything was just great\nand its training actually lost looks\npretty the same and we don't need to\nlook at it it was simply also with high\nhigh and then dropped dramatically so\ngreat it works so right now you might\nask what prove it that it works okay\ngreat why not so if I go to my\nmoodles here I have a test dot Pi script\nthat I created and here is my own X\ninference model and CTC decoder and\ncharacter and what are calculation\nscripts so what I do here so simply here\nI have my model trainer you can see four\nthree four uh and I load this model on\nthe next with my following object and\nwhat we do here how we do this\nprediction\nso uh here is our audio path and the\ntrue label so maybe we are interested to\nto see what is the label great not\nproblem let's print it out here print\nlabor and it will print in terminal for\nus and right now I read this volition\nCSP file that it was saved along the\nmodel here and we're gonna iterate\nthrough it to see whether it works or\nnot and we'll see what are the true\nlabels of it and we are not interested\nin the prediction because uh this error\nrate is pretty low so why not we might\nknow not to notice the difference\naccurately without having time into it\nso great and if I I run this for all\nthis data set and we will see what is\nthe charter and what error rate for my\nlabels through labels and Etc so let's\nrun this simply and you'll see that it\nit's pretty nice\nand it works so simply what we do we use\nour live librosa to load this up and we\nload this audio we have this audio raw\ndata that we put into this kind of\nprediction model and it expands them or\nruns the inference and use a CTC decoder\nto decode the text and that's it and you\nmight see here is the output of my uh\nmood off and you might see that there is\nCharter error rate word error rate what\nare the results and Etc and let's stop\nthis right now I'm not interested\nactually and let's move on\nuh and we can see what are the true\nlabels uh how I see it's not the perfect\nI see it cropped me uh capitalized\nletters because all all sentences start\nfrom uh from a capital so that's my\nproblem I need to change this train\nscript to prove it that it works and\nbasically you might see that I that's my\nmistake I teached more model to predict\nuh Speech without capitalized letters so\nuh yeah that's my mistake I need to look\nat it closer so anyway I'll fix this\ntraining code for you\nuh I don't know maybe I'll train a model\nmodel but I'm not recording another\ntutorial right now because I'm not\ninterested to do so\nand of course\num I'll publish all the model VR into my\nGitHub repository and Etc you can find\nthe link in the video description below\nif you want and there everything is but\nhere is the idea how simple it is to\nfine tune this buff to Vector model\nand you can use the audio whatever you\nwant so basically\nit works as you can see and if you can\ntry to record your audio to and use the\nprediction but it will be pretty hard\nand it will be not that accurate because\nuh this kind of data set that I used to\ntrain is very specific with a accent of\npeople that were reading books and it\nmight be not that great for you but\nstill it works and you can try to use\nyour own data to fine-tune this model\nand you will see that it works pretty\nnicely so but I don't want to invest a\nlot of time into explaining this stuff\nhere so basically it works and if you\nlike this video please don't say\nhesitate to smash the like button\nsubscribe and if you have any questions\ndrop the comment below and you will see\nhow to solve this so basically let's go\nback to one stuff if you remember I\nasked I'm not I do not recognize to\ntrain this on tensorflow\nand there is a huge uh problem with this\nbecause we also we can load the\nTransformers the same model\nuh that we use in pytorch but there's a\nminor difference that training takes\naround five times longer I don't know\nwhy\nit used the same uh CPU same GPU uh as\nas that by torch but it trains way\nslower and I can't explain this\nso I don't know why we would like to\ntrain this model on tensorflow if we can\nfaster train it on pytotch and we if we\nanyway are going to deploy this into\nsome on an X format and use it so uh\nit's up to you you can try to train this\non tensorflow but I do not recommend to\ndo so I recommend using only pytorch so\nit's up to you so that's it about this\nintroduction with war to vac and this\ncode you can find on my GitHub Link in\nthe description and try to train your\nown monal or throw you can try to run my\nown model that well I'll open upload\nlink also into our description so that's\nit about this and\nand that's it we'll see you in our next\nvideo tutorial thank you again for\nwatching and we'll see you next time bye\n",
  "words": [
    "hello",
    "everyone",
    "welcome",
    "back",
    "another",
    "video",
    "well",
    "video",
    "right",
    "little",
    "different",
    "previous",
    "ones",
    "well",
    "uh",
    "recently",
    "investigating",
    "checking",
    "war",
    "vac",
    "2",
    "framework",
    "solves",
    "wise",
    "learning",
    "speech",
    "representation",
    "well",
    "kind",
    "paper",
    "2020",
    "related",
    "something",
    "speech",
    "recognition",
    "test",
    "probably",
    "already",
    "heard",
    "well",
    "second",
    "version",
    "kind",
    "paper",
    "course",
    "first",
    "version",
    "war",
    "vac",
    "2",
    "simply",
    "release",
    "paper",
    "uh",
    "facebook",
    "basically",
    "uh",
    "uh",
    "kind",
    "paper",
    "tells",
    "us",
    "released",
    "model",
    "right",
    "one",
    "best",
    "speech",
    "recognition",
    "models",
    "overall",
    "well",
    "three",
    "years",
    "already",
    "mundo",
    "still",
    "one",
    "best",
    "models",
    "use",
    "basically",
    "uh",
    "tutorial",
    "demonstrate",
    "easy",
    "fine",
    "tune",
    "model",
    "data",
    "speech",
    "recognition",
    "tasks",
    "going",
    "cover",
    "theory",
    "modal",
    "specifications",
    "mainly",
    "gon",
    "na",
    "give",
    "code",
    "use",
    "fine",
    "tune",
    "model",
    "data",
    "tokenizer",
    "etc",
    "pretty",
    "simple",
    "going",
    "deep",
    "theory",
    "coding",
    "stuff",
    "simply",
    "gon",
    "na",
    "cover",
    "short",
    "basically",
    "uh",
    "pretty",
    "nice",
    "model",
    "uh",
    "need",
    "simply",
    "read",
    "data",
    "receive",
    "kind",
    "raw",
    "audio",
    "simply",
    "feed",
    "data",
    "straight",
    "model",
    "conclusion",
    "neural",
    "networks",
    "inside",
    "transformer",
    "encoder",
    "layer",
    "gives",
    "us",
    "output",
    "well",
    "basically",
    "model",
    "focused",
    "ctc",
    "loss",
    "gon",
    "na",
    "use",
    "kind",
    "tutorial",
    "demonstrate",
    "use",
    "simple",
    "need",
    "throw",
    "form",
    "model",
    "scratch",
    "use",
    "pertrained",
    "model",
    "model",
    "basically",
    "paper",
    "demonstrates",
    "uh",
    "explained",
    "receive",
    "great",
    "results",
    "etc",
    "gon",
    "na",
    "use",
    "basically",
    "gon",
    "na",
    "find",
    "tunics",
    "course",
    "results",
    "data",
    "labeled",
    "labeled",
    "comparison",
    "think",
    "interested",
    "basically",
    "model",
    "trained",
    "published",
    "hugging",
    "face",
    "use",
    "base",
    "see",
    "facebook",
    "gives",
    "us",
    "explanations",
    "course",
    "hugging",
    "phase",
    "find",
    "explanations",
    "train",
    "model",
    "etc",
    "give",
    "video",
    "tutorial",
    "train",
    "model",
    "run",
    "inference",
    "want",
    "deploy",
    "want",
    "use",
    "iport",
    "pytorch",
    "installation",
    "got",
    "simply",
    "jump",
    "code",
    "ml2",
    "package",
    "working",
    "recently",
    "might",
    "see",
    "worked",
    "two",
    "torch",
    "tutorial",
    "might",
    "heard",
    "right",
    "right",
    "going",
    "use",
    "tensorflow",
    "use",
    "pie",
    "torch",
    "end",
    "tutorial",
    "explain",
    "going",
    "pretty",
    "simple",
    "requirements",
    "tutorial",
    "need",
    "torch",
    "least",
    "version",
    "tested",
    "sure",
    "work",
    "higher",
    "versions",
    "need",
    "transformers",
    "could",
    "download",
    "model",
    "need",
    "mltu",
    "package",
    "recently",
    "released",
    "update",
    "version",
    "gon",
    "na",
    "use",
    "course",
    "next",
    "might",
    "extra",
    "time",
    "okay",
    "let",
    "import",
    "right",
    "let",
    "go",
    "straight",
    "training",
    "code",
    "uh",
    "give",
    "explanation",
    "basically",
    "uh",
    "import",
    "many",
    "stuff",
    "stuff",
    "mltu",
    "package",
    "may",
    "see",
    "model",
    "ctc",
    "law",
    "use",
    "data",
    "provider",
    "metrics",
    "track",
    "character",
    "error",
    "rate",
    "board",
    "error",
    "rate",
    "course",
    "callbacks",
    "really",
    "stopping",
    "bottom",
    "cosine",
    "model",
    "2",
    "next",
    "answer",
    "board",
    "could",
    "track",
    "metrics",
    "train",
    "etc",
    "model",
    "checkpoint",
    "course",
    "augmentations",
    "related",
    "audio",
    "random",
    "audio",
    "noise",
    "random",
    "audio",
    "pitch",
    "time",
    "stretch",
    "implemented",
    "augmentation",
    "improve",
    "uh",
    "data",
    "scale",
    "model",
    "would",
    "uh",
    "fit",
    "better",
    "basically",
    "still",
    "see",
    "vac",
    "2",
    "ctc",
    "see",
    "basically",
    "transformers",
    "library",
    "installed",
    "using",
    "functions",
    "pytorch",
    "need",
    "focus",
    "right",
    "might",
    "see",
    "download",
    "simple",
    "kind",
    "lg",
    "speech",
    "data",
    "set",
    "uh",
    "pretty",
    "huge",
    "data",
    "set",
    "30",
    "000",
    "uh",
    "samples",
    "speech",
    "labeled",
    "need",
    "simply",
    "need",
    "download",
    "use",
    "kind",
    "function",
    "download",
    "placed",
    "dataset",
    "ldot",
    "speech",
    "metadata",
    "open",
    "data",
    "sets",
    "see",
    "data",
    "already",
    "downloaded",
    "pretty",
    "simple",
    "let",
    "move",
    "next",
    "vocabulary",
    "usual",
    "stuff",
    "kind",
    "characters",
    "inside",
    "uh",
    "data",
    "set",
    "need",
    "need",
    "data",
    "set",
    "read",
    "metadata",
    "path",
    "method",
    "file",
    "uses",
    "separator",
    "following",
    "simply",
    "read",
    "strings",
    "transcriptions",
    "uh",
    "joined",
    "kind",
    "labels",
    "use",
    "lower",
    "see",
    "means",
    "want",
    "capitalized",
    "characters",
    "lower",
    "simplicity",
    "reason",
    "course",
    "trying",
    "recognize",
    "speech",
    "care",
    "capital",
    "simply",
    "way",
    "easier",
    "train",
    "model",
    "characters",
    "lower",
    "simplifies",
    "vocabulary",
    "need",
    "use",
    "great",
    "use",
    "data",
    "provider",
    "simple",
    "tensorflow",
    "data",
    "provider",
    "uh",
    "actually",
    "inherited",
    "everything",
    "tensorflow",
    "data",
    "provider",
    "might",
    "see",
    "additional",
    "parameters",
    "workers",
    "use",
    "etc",
    "time",
    "working",
    "sound",
    "data",
    "really",
    "hard",
    "cpu",
    "side",
    "needs",
    "load",
    "audio",
    "disk",
    "need",
    "augment",
    "even",
    "harder",
    "usually",
    "strong",
    "cpu",
    "recommend",
    "use",
    "something",
    "wrong",
    "ca",
    "used",
    "multiple",
    "cpu",
    "used",
    "threadpool",
    "executor",
    "next",
    "etc",
    "simply",
    "iterates",
    "data",
    "skip",
    "validation",
    "configurations",
    "dig",
    "check",
    "many",
    "apple",
    "want",
    "train",
    "batch",
    "size",
    "would",
    "learning",
    "rates",
    "etc",
    "uh",
    "warm",
    "epochs",
    "uh",
    "whether",
    "want",
    "use",
    "mixed",
    "precision",
    "etc",
    "many",
    "different",
    "things",
    "config",
    "saved",
    "along",
    "model",
    "trainable",
    "mode",
    "let",
    "move",
    "might",
    "see",
    "gon",
    "na",
    "read",
    "use",
    "data",
    "preprocessors",
    "audio",
    "reader",
    "means",
    "read",
    "sample",
    "rate",
    "16",
    "000",
    "create",
    "specific",
    "audio",
    "object",
    "mltu",
    "object",
    "need",
    "use",
    "label",
    "indexer",
    "label",
    "characters",
    "integer",
    "representation",
    "need",
    "use",
    "batch",
    "post",
    "processors",
    "training",
    "model",
    "need",
    "audios",
    "example",
    "audios",
    "one",
    "batch",
    "want",
    "train",
    "model",
    "audio",
    "length",
    "means",
    "use",
    "audio",
    "padding",
    "recommend",
    "use",
    "use",
    "batch",
    "padded",
    "maximum",
    "size",
    "possible",
    "length",
    "audios",
    "apply",
    "labels",
    "need",
    "pad",
    "example",
    "sentences",
    "uh",
    "20",
    "characters",
    "need",
    "add",
    "100",
    "characters",
    "little",
    "harder",
    "train",
    "model",
    "efficiency",
    "better",
    "pad",
    "maximum",
    "length",
    "exist",
    "batch",
    "purpose",
    "mentioned",
    "whether",
    "want",
    "use",
    "multiprocessing",
    "training",
    "model",
    "weaker",
    "cpu",
    "might",
    "handle",
    "much",
    "example",
    "computer",
    "ca",
    "use",
    "windows",
    "least",
    "try",
    "run",
    "linux",
    "works",
    "depends",
    "operating",
    "system",
    "etc",
    "kind",
    "good",
    "worker",
    "work",
    "audio",
    "work",
    "background",
    "load",
    "everything",
    "trainable",
    "model",
    "us",
    "background",
    "pretty",
    "amazing",
    "works",
    "course",
    "split",
    "data",
    "validation",
    "training",
    "gon",
    "na",
    "save",
    "along",
    "model",
    "training",
    "volition",
    "data",
    "sets",
    "mean",
    "simply",
    "validating",
    "trainable",
    "model",
    "want",
    "use",
    "augmentation",
    "using",
    "eats",
    "lot",
    "cpu",
    "papa",
    "pretty",
    "hard",
    "train",
    "takes",
    "way",
    "time",
    "train",
    "augmented",
    "audios",
    "crucial",
    "part",
    "right",
    "need",
    "create",
    "uh",
    "vac",
    "model",
    "pytorch",
    "transformers",
    "basically",
    "used",
    "vector",
    "ctc",
    "use",
    "see",
    "see",
    "using",
    "per",
    "trained",
    "name",
    "specific",
    "name",
    "hugging",
    "faces",
    "simply",
    "gon",
    "na",
    "download",
    "model",
    "load",
    "differently",
    "hidden",
    "states",
    "changing",
    "book",
    "vocabulary",
    "size",
    "ignore",
    "mismatches",
    "means",
    "changing",
    "head",
    "model",
    "simply",
    "put",
    "head",
    "trainable",
    "classification",
    "feed",
    "forward",
    "pass",
    "input",
    "audio",
    "data",
    "audio",
    "batches",
    "etc",
    "simply",
    "receive",
    "outputs",
    "use",
    "logins",
    "outputs",
    "logsoft",
    "max",
    "function",
    "whole",
    "output",
    "go",
    "ctc",
    "loss",
    "course",
    "implement",
    "differently",
    "found",
    "pretty",
    "good",
    "implementation",
    "follow",
    "create",
    "custom",
    "model",
    "pytorch",
    "put",
    "gpu",
    "totally",
    "recommend",
    "train",
    "try",
    "train",
    "cpu",
    "gon",
    "na",
    "need",
    "wait",
    "month",
    "train",
    "believe",
    "course",
    "want",
    "train",
    "pretty",
    "large",
    "data",
    "set",
    "need",
    "kind",
    "strong",
    "gpu",
    "use",
    "multiple",
    "gpus",
    "course",
    "pretty",
    "strong",
    "cpu",
    "remember",
    "train",
    "transformers",
    "takes",
    "lot",
    "time",
    "next",
    "defined",
    "bar",
    "map",
    "cosine",
    "design",
    "explain",
    "pretty",
    "necessary",
    "use",
    "transformers",
    "means",
    "start",
    "really",
    "low",
    "learning",
    "rate",
    "every",
    "epoch",
    "increase",
    "learning",
    "rate",
    "faster",
    "better",
    "train",
    "motor",
    "use",
    "tensorboard",
    "pretty",
    "use",
    "early",
    "stopping",
    "track",
    "validation",
    "character",
    "erroring",
    "course",
    "could",
    "track",
    "word",
    "error",
    "read",
    "chose",
    "use",
    "charter",
    "error",
    "rate",
    "want",
    "next",
    "use",
    "model",
    "checkpoint",
    "means",
    "save",
    "models",
    "mundo",
    "pt",
    "weights",
    "save",
    "best",
    "weights",
    "according",
    "validation",
    "character",
    "error",
    "rate",
    "gon",
    "na",
    "use",
    "course",
    "end",
    "uh",
    "wan",
    "na",
    "say",
    "best",
    "moodle",
    "next",
    "format",
    "need",
    "save",
    "pytorch",
    "mode",
    "pretty",
    "hard",
    "using",
    "pie",
    "torch",
    "load",
    "back",
    "trainer",
    "model",
    "another",
    "platform",
    "example",
    "want",
    "use",
    "raspberry",
    "pi",
    "pretty",
    "impossible",
    "load",
    "bag",
    "without",
    "knowing",
    "architecture",
    "without",
    "installing",
    "pie",
    "torch",
    "chose",
    "convert",
    "model",
    "uninx",
    "simply",
    "run",
    "device",
    "want",
    "might",
    "see",
    "custom",
    "model",
    "object",
    "handles",
    "training",
    "stuff",
    "callbacks",
    "metrics",
    "etc",
    "feed",
    "model",
    "model",
    "see",
    "custom",
    "model",
    "buff",
    "back",
    "model",
    "use",
    "ctc",
    "lows",
    "uh",
    "kind",
    "custom",
    "lost",
    "use",
    "vocabulary",
    "kind",
    "data",
    "set",
    "use",
    "adam",
    "b",
    "optimizer",
    "use",
    "simple",
    "adam",
    "uh",
    "work",
    "well",
    "well",
    "shown",
    "adam",
    "w",
    "works",
    "slightly",
    "better",
    "using",
    "want",
    "track",
    "character",
    "error",
    "rate",
    "error",
    "rate",
    "kind",
    "usual",
    "working",
    "towards",
    "uh",
    "audio",
    "recognition",
    "related",
    "speech",
    "words",
    "characters",
    "matter",
    "use",
    "mixed",
    "precision",
    "find",
    "using",
    "configs",
    "usually",
    "good",
    "gpu",
    "dramatically",
    "increase",
    "training",
    "two",
    "times",
    "least",
    "take",
    "less",
    "memory",
    "gpu",
    "use",
    "larger",
    "batches",
    "chose",
    "use",
    "mixer",
    "precision",
    "true",
    "saved",
    "train",
    "validation",
    "csv",
    "files",
    "split",
    "later",
    "could",
    "test",
    "module",
    "works",
    "would",
    "train",
    "convert",
    "next",
    "want",
    "use",
    "validation",
    "data",
    "course",
    "train",
    "data",
    "work",
    "well",
    "call",
    "fit",
    "waited",
    "finish",
    "well",
    "want",
    "train",
    "right",
    "start",
    "training",
    "cpu",
    "jumps",
    "100",
    "might",
    "face",
    "blue",
    "screen",
    "simply",
    "uh",
    "video",
    "recording",
    "might",
    "start",
    "lagging",
    "insanely",
    "prove",
    "works",
    "start",
    "immediately",
    "kill",
    "starts",
    "streaming",
    "let",
    "run",
    "let",
    "see",
    "keeps",
    "loading",
    "something",
    "okay",
    "see",
    "takes",
    "one",
    "half",
    "1474",
    "batches",
    "train",
    "great",
    "see",
    "handle",
    "multiple",
    "processing",
    "switching",
    "multi",
    "threading",
    "expected",
    "know",
    "work",
    "machine",
    "like",
    "within",
    "first",
    "batch",
    "pretty",
    "good",
    "okay",
    "see",
    "keeps",
    "training",
    "showed",
    "us",
    "chart",
    "error",
    "rate",
    "word",
    "error",
    "rate",
    "simply",
    "need",
    "wait",
    "completes",
    "stuff",
    "right",
    "recording",
    "way",
    "slower",
    "believe",
    "might",
    "lagging",
    "side",
    "want",
    "continue",
    "training",
    "recording",
    "great",
    "kill",
    "already",
    "trained",
    "model",
    "already",
    "tested",
    "everything",
    "skill",
    "kill",
    "stuff",
    "okay",
    "simply",
    "kill",
    "one",
    "great",
    "believe",
    "interested",
    "trained",
    "prove",
    "okay",
    "uh",
    "model",
    "go",
    "models",
    "moved",
    "work",
    "train",
    "model",
    "already",
    "pt",
    "weights",
    "model",
    "next",
    "let",
    "train",
    "motor",
    "first",
    "going",
    "test",
    "part",
    "let",
    "look",
    "tensorboard",
    "trained",
    "attention",
    "board",
    "exactly",
    "showed",
    "let",
    "maximize",
    "one",
    "might",
    "see",
    "started",
    "training",
    "10",
    "epoths",
    "warm",
    "improving",
    "worst",
    "part",
    "model",
    "training",
    "epos",
    "decreasing",
    "sure",
    "whether",
    "training",
    "look",
    "uh",
    "12",
    "epos",
    "dramatically",
    "dropped",
    "somewhere",
    "might",
    "see",
    "train",
    "checked",
    "android",
    "training",
    "validation",
    "huge",
    "difference",
    "scroll",
    "end",
    "might",
    "see",
    "uh",
    "dramatically",
    "decreased",
    "let",
    "look",
    "word",
    "array",
    "really",
    "towards",
    "might",
    "see",
    "end",
    "training",
    "uh",
    "two",
    "percent",
    "error",
    "validation",
    "error",
    "means",
    "works",
    "eight",
    "audio",
    "data",
    "loss",
    "looks",
    "exactly",
    "let",
    "look",
    "learning",
    "right",
    "exactly",
    "talking",
    "uh",
    "uh",
    "warm",
    "means",
    "might",
    "see",
    "starts",
    "increasing",
    "learning",
    "rate",
    "learning",
    "rate",
    "define",
    "one",
    "point",
    "one",
    "e",
    "minus",
    "five",
    "learning",
    "rate",
    "achieved",
    "warm",
    "achieved",
    "train",
    "continue",
    "training",
    "decreasing",
    "word",
    "rate",
    "decreasing",
    "everything",
    "great",
    "training",
    "actually",
    "lost",
    "looks",
    "pretty",
    "need",
    "look",
    "simply",
    "also",
    "high",
    "high",
    "dropped",
    "dramatically",
    "great",
    "works",
    "right",
    "might",
    "ask",
    "prove",
    "works",
    "okay",
    "great",
    "go",
    "moodles",
    "test",
    "dot",
    "pi",
    "script",
    "created",
    "x",
    "inference",
    "model",
    "ctc",
    "decoder",
    "character",
    "calculation",
    "scripts",
    "simply",
    "model",
    "trainer",
    "see",
    "four",
    "three",
    "four",
    "uh",
    "load",
    "model",
    "next",
    "following",
    "object",
    "prediction",
    "uh",
    "audio",
    "path",
    "true",
    "label",
    "maybe",
    "interested",
    "see",
    "label",
    "great",
    "problem",
    "let",
    "print",
    "print",
    "labor",
    "print",
    "terminal",
    "us",
    "right",
    "read",
    "volition",
    "csp",
    "file",
    "saved",
    "along",
    "model",
    "gon",
    "na",
    "iterate",
    "see",
    "whether",
    "works",
    "see",
    "true",
    "labels",
    "interested",
    "prediction",
    "uh",
    "error",
    "rate",
    "pretty",
    "low",
    "might",
    "know",
    "notice",
    "difference",
    "accurately",
    "without",
    "time",
    "great",
    "run",
    "data",
    "set",
    "see",
    "charter",
    "error",
    "rate",
    "labels",
    "labels",
    "etc",
    "let",
    "run",
    "simply",
    "see",
    "pretty",
    "nice",
    "works",
    "simply",
    "use",
    "live",
    "librosa",
    "load",
    "load",
    "audio",
    "audio",
    "raw",
    "data",
    "put",
    "kind",
    "prediction",
    "model",
    "expands",
    "runs",
    "inference",
    "use",
    "ctc",
    "decoder",
    "decode",
    "text",
    "might",
    "see",
    "output",
    "uh",
    "mood",
    "might",
    "see",
    "charter",
    "error",
    "rate",
    "word",
    "error",
    "rate",
    "results",
    "etc",
    "let",
    "stop",
    "right",
    "interested",
    "actually",
    "let",
    "move",
    "uh",
    "see",
    "true",
    "labels",
    "uh",
    "see",
    "perfect",
    "see",
    "cropped",
    "uh",
    "capitalized",
    "letters",
    "sentences",
    "start",
    "uh",
    "capital",
    "problem",
    "need",
    "change",
    "train",
    "script",
    "prove",
    "works",
    "basically",
    "might",
    "see",
    "mistake",
    "teached",
    "model",
    "predict",
    "uh",
    "speech",
    "without",
    "capitalized",
    "letters",
    "uh",
    "yeah",
    "mistake",
    "need",
    "look",
    "closer",
    "anyway",
    "fix",
    "training",
    "code",
    "uh",
    "know",
    "maybe",
    "train",
    "model",
    "model",
    "recording",
    "another",
    "tutorial",
    "right",
    "interested",
    "course",
    "um",
    "publish",
    "model",
    "vr",
    "github",
    "repository",
    "etc",
    "find",
    "link",
    "video",
    "description",
    "want",
    "everything",
    "idea",
    "simple",
    "fine",
    "tune",
    "buff",
    "vector",
    "model",
    "use",
    "audio",
    "whatever",
    "want",
    "basically",
    "works",
    "see",
    "try",
    "record",
    "audio",
    "use",
    "prediction",
    "pretty",
    "hard",
    "accurate",
    "uh",
    "kind",
    "data",
    "set",
    "used",
    "train",
    "specific",
    "accent",
    "people",
    "reading",
    "books",
    "might",
    "great",
    "still",
    "works",
    "try",
    "use",
    "data",
    "model",
    "see",
    "works",
    "pretty",
    "nicely",
    "want",
    "invest",
    "lot",
    "time",
    "explaining",
    "stuff",
    "basically",
    "works",
    "like",
    "video",
    "please",
    "say",
    "hesitate",
    "smash",
    "like",
    "button",
    "subscribe",
    "questions",
    "drop",
    "comment",
    "see",
    "solve",
    "basically",
    "let",
    "go",
    "back",
    "one",
    "stuff",
    "remember",
    "asked",
    "recognize",
    "train",
    "tensorflow",
    "huge",
    "uh",
    "problem",
    "also",
    "load",
    "transformers",
    "model",
    "uh",
    "use",
    "pytorch",
    "minor",
    "difference",
    "training",
    "takes",
    "around",
    "five",
    "times",
    "longer",
    "know",
    "used",
    "uh",
    "cpu",
    "gpu",
    "uh",
    "torch",
    "trains",
    "way",
    "slower",
    "ca",
    "explain",
    "know",
    "would",
    "like",
    "train",
    "model",
    "tensorflow",
    "faster",
    "train",
    "pytotch",
    "anyway",
    "going",
    "deploy",
    "x",
    "format",
    "use",
    "uh",
    "try",
    "train",
    "tensorflow",
    "recommend",
    "recommend",
    "using",
    "pytorch",
    "introduction",
    "war",
    "vac",
    "code",
    "find",
    "github",
    "link",
    "description",
    "try",
    "train",
    "monal",
    "throw",
    "try",
    "run",
    "model",
    "well",
    "open",
    "upload",
    "link",
    "also",
    "description",
    "see",
    "next",
    "video",
    "tutorial",
    "thank",
    "watching",
    "see",
    "next",
    "time",
    "bye"
  ],
  "keywords": [
    "back",
    "another",
    "video",
    "well",
    "right",
    "uh",
    "recently",
    "war",
    "vac",
    "2",
    "learning",
    "speech",
    "kind",
    "paper",
    "related",
    "something",
    "recognition",
    "test",
    "already",
    "version",
    "course",
    "first",
    "simply",
    "basically",
    "us",
    "model",
    "one",
    "best",
    "models",
    "still",
    "use",
    "tutorial",
    "fine",
    "tune",
    "data",
    "going",
    "gon",
    "na",
    "give",
    "code",
    "etc",
    "pretty",
    "simple",
    "stuff",
    "need",
    "read",
    "receive",
    "audio",
    "feed",
    "output",
    "ctc",
    "loss",
    "great",
    "results",
    "find",
    "labeled",
    "interested",
    "trained",
    "hugging",
    "see",
    "train",
    "run",
    "inference",
    "want",
    "pytorch",
    "package",
    "working",
    "might",
    "two",
    "torch",
    "tensorflow",
    "pie",
    "end",
    "explain",
    "least",
    "work",
    "transformers",
    "could",
    "download",
    "mltu",
    "next",
    "time",
    "okay",
    "let",
    "go",
    "training",
    "many",
    "provider",
    "metrics",
    "track",
    "character",
    "error",
    "rate",
    "board",
    "really",
    "would",
    "better",
    "using",
    "set",
    "huge",
    "move",
    "vocabulary",
    "characters",
    "labels",
    "lower",
    "means",
    "capitalized",
    "way",
    "actually",
    "everything",
    "hard",
    "cpu",
    "load",
    "strong",
    "recommend",
    "ca",
    "used",
    "multiple",
    "validation",
    "batch",
    "size",
    "warm",
    "whether",
    "precision",
    "saved",
    "along",
    "trainable",
    "create",
    "specific",
    "object",
    "label",
    "audios",
    "example",
    "length",
    "try",
    "works",
    "good",
    "save",
    "lot",
    "takes",
    "part",
    "put",
    "batches",
    "custom",
    "gpu",
    "believe",
    "start",
    "word",
    "chose",
    "charter",
    "weights",
    "without",
    "adam",
    "dramatically",
    "true",
    "recording",
    "prove",
    "kill",
    "know",
    "like",
    "look",
    "exactly",
    "decreasing",
    "difference",
    "also",
    "prediction",
    "problem",
    "print",
    "link",
    "description"
  ]
}