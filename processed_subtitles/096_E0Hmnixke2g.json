{
  "text": "in the next 17 minutes I will give you\nan overview of the most important\nmachine learning algorithms to help you\ndecide which one is right for your\nproblem my name is Tim and I have been a\ndata scientist for over 10 years and\ntaught all of these algorithms to\nhundreds of students in real life\nmachine learning boot camps there is a\nsimple strategy for picking the right\nalgorithm for your problem in 17 minutes\nyou will know how to pick the right one\nfor any problem and get a basic\nintuition of each algorithm and how they\nrelate to each other my goal is to give\nas many of you as possible an intuitive\nunderstanding of the major machine\nlearning algorithms to make you stop\nfeeling overwhelmed according to\nWikipedia machine learning is a field of\nstudy in artificial intelligence\nconcerned with the development and study\nof statistical algorithms that can learn\nfrom data and generalize to unseen data\nand thus perform tasks without explicit\ninstructions much of the recent\nadvancements in AI are driven by neural\nnetworks which I hope to give you an\nintuitive understanding of by the end of\nthis video Let's divide machine learning\ninto its subfields generally machine\nlearning is divided into two areas\nsupervised learning and unsupervised\nlearning supervised learning is when we\nhave a data set with any number of\nindependent variables also called\nfeatures or input variables and a\ndependent variable also called Target or\noutput variable that is supposed to be\npredicted we have a so-called training\ndata set where we know the True Values\nfor the output variable also called\nlabels that we can train our algorithm\non to later predict the output variable\nfor new unknown data examples could be\npredicting the price of a house the\noutput variable based on features of the\nhouse say square footage location year\nof construction Etc categorizing an\nobject as a cat or a dog the output\nvariable or label based on features of\nthe object say height weight size of the\nears color of the eyes Etc unsupervised\nlearning is basically any learning\nproblem that is not supervised so where\nno truth about the data is known so\nwhere a supervised algorithm would be\nlike showing a little kid what a typical\ncat looks like and what a typical dog\nlooks like and then giving it a new\npicture and asking it what animal it\nsees an unsupervised algorithm would be\ngiving a kid with no idea of what cats\nand dogs are a pile of pictures of\nanimals and asking it to group by\nsimilarity without any further\ninstructions examples of unsupervised\nproblems might be to sort all of your\nemails into three unspecified categories\nwhich you can then later inspect and\nname as you wish the algorithm will\ndecide on its own how it will create\nthose categories also called clusters\nlet's start with supervised learning\narguably the bigger and more important\nbranch of machine learning there are\nbroadly two subcategories in regression\nwe want to predict a continuous numeric\nTarget variable for a given input\nvariable using the example from before\nit could be predicting the price of a\nhouse given any number features of a\nhouse and determining their relationship\nto the final price of the house we might\nfor example find out that square footage\nis directly proportional to the price\nlinear dependence but that the age of\nthe house has no influence on the price\nof the house in classification we try to\nassign a discrete categorical label also\ncalled a class to a data point for\nexample we may want to assign the label\nspam or no spam to an email based on its\ncontent sender and so on but we could\nalso have more than two classes for\nexample junk primary social promotions\nand updates as Gmail does by default now\nlet's dive into the actual algorithms\nstarting with the mother of all machine\nlearning algorithms linear regression in\ngeneral supervised learning algorithms\ntry to determine the relationship\nbetween two variables we try to find the\nfunction that Maps one to the other\nlinear regression in its simplest form\nis trying to determine a linear\nrelationship between two variables\nnamely the input and the output we want\nto fit a linear equation to the data by\nminimizing the sum of squares of the\ndistances between data points and the\nregression Line This simply minimizes\nthe average distance of the real data to\nour predictive model in this case the\nregression line and should therefore\nminimize prediction errors for new data\npoints a simple example of a linear\nrelationship might be the height and\nshoe size of a person where the\nregression fit might tell us that for\nevery one unit of shoe size increase the\nperson will be on average 2 Ines taller\nyou can make your model more complex and\nfit multi-dimensional data to an output\nvariable in the example of the shoe size\nyou might for example want to include\nthe gender age and ethnicity of the\nperson to get an even better model many\nof the very fancy machine learning\nalgorithms including neural networks are\njust extensions of this very simple idea\nas I will show you later in the video\nlogistic regression is a variant of\nlinear regression and probably the most\nbasic classification algorithm instead\nof fitting a line to two numerical\nvariables with a presumably linear\nrelationship you now try to predict a\ncategorical output variable using\ncategorical or numerical input variables\nlet's look at an example we now want to\npredict one of two classes for example\nthe gender of a person based on height\nand weight so a linear regression\nwouldn't make much sense anymore instead\nof fitting a line to the data we now fit\na so-called sigmoid function to the data\nwhich looks like this the equation will\nnow not tell us about a linear\nrelationship between two variables but\nwill now conveniently tell us the\nprobability of a data point falling into\na certain class given the value of the\ninput variable so for example the\nlikelihood of an adult person with a\nheight of 180 cm being a man would be\n80% this is completely made up of course\nthe K nearest neighbors algorithm or KNN\nis a very simple and intuitive algorithm\nthat can be used for both regression and\nclass ification it is a so-called\nnon-parametric algorithm the name means\nthat we don't try to fit any equations\nand thus find any parameters of a model\nso no true model fitting is necessary\nthe idea of KNN is simply that for any\ngiven new data point we will predict the\ntarget to be the average of its K\nnearest neighbors while this might seem\nvery simple this is actually a very\npowerful predictive algorithm especially\nwhen relationships are more complicated\nthan a simple linear relationship in a\nclassification example we might say that\nthe gender of a person will be the same\nas the majority of the five people\nclosest in weight and height to the\nperson in question in a regression\nexample we might say that the weight of\na person is the average weight of the\nthree people closest in height and of\nchest circumference this makes a ton of\nintuitive sense you might realize that\nthe number three seems a bit arbitrary\nand it is K is called a hyperparameter\nof the algorithm and choosing the right\nK is an art choosing a very small number\nof K say one or two will lead to your\nmodel predicting your training data set\nvery well but not generalizing well to\nunseen data this is called overfitting\nchoosing a very large number say 1,000\nwill lead to a worst fit over overall\nthis is called underfitting the best\nnumber is somewhere in between and\nDepends a lot on the problem at hand\nmethods for finding the right\nhyperparameters include cross validation\nbut are beyond the scope of this video\nsupport Vector machine is a supervised\nmachine learning algorithm originally\ndesigned for classification tasks but it\ncan also be used for regression tasks\nthe core concept of the algorithm is to\ndraw a decision boundary between data\npoints that separates data points of the\ntraining set as well as possible as the\nname suggests a new unseen data point\nwill be classified according to where it\nfalls with respect to the decision\nboundary let's take this arbitrary\nexample of trying to classify animals by\ntheir weight and the length of their\nnose in this simple case of trying to\nclassify cats and elephants the decision\nboundary is a straight line the svm\nalgorithm tries to find the line that\nseparates the classes with the largest\nmargin possible that is maximizing the\nspace between the different classes this\nmakes the decision boundary generalize\nwell and less sensitive to noise and\noutliers in the training data the\nso-called support vectors are the data\npoints that sit on the edge of the\nmargin knowing the support vectors is\nenough to classify new data points which\noften makes the algorithm very memory\nefficient one of the benefits of SPM is\nthat it is very powerful in high\nDimensions that is if the number of\nfeatures is large compared to the size\nof the data in those higher dimensional\ncases the decision boundary is called a\nhyperplane another feature that makes\nsvms extremely powerful is the use of\nso-called kernel functions which allow\nfor the identification of Highly complex\nnonlinear decision boundaries kernel\nfunctions are an implicit way to turn\nyour original features into new more\ncomplex features using the so-called\nkernel trick which is beyond the scope\nof this video this allows for efficient\ncreation of nonlinear decision\nboundaries by creating complex new\nfeatures such as weight divided by\nheight squared also called the BMI this\nis called implicit feature engineering\nneural networks take the idea of\nimplicit feature engineering to the next\nlevel as I will explain later possible\nkernel functions for svms are the linear\nthe polinomial the RBF and the sigmoid\nkernel another fairly simple classifier\nis the naive Bas classifier that gets\nits name from B theorem which looks like\nthis I believe it's easiest to\nunderstand naive Bay with an example use\ncase that it is often used for spam\nfilters we can train our algorithm with\na number of spam and non-spam emails and\ncount the occurrences of different words\nin each class and thereby calculate the\nprobability of certain words appearing\nin spam emails and non-spam emails we\ncan then quickly classify a new email\nbased on the words it contains by by\nusing base theorem we simply multiply\nthe different probabilities of all words\nin the email together this algorithm\nmakes the false assumption that the\nprobabilities of the different words\nappearing are independent of each other\nwhich is why we call this classifier\nnaive this makes it very computation\nAlly efficient while still being a good\napproximation for many use cases such as\nspam classification and other text-based\nclassification tasks decision trees are\nthe basis of a number of more complex\nsupervised learning algorithms in its\nsimplest form a decision tree looks\nsomewhat like this the decision tree is\nbasically a series of yes no questions\nthat allow us to partition a data set in\nseveral Dimensions here is an example\ndecision tree for classifying people\ninto high and lowrisk patients for heart\nattacks the goal of the decision tree\nalgorithm is to create so-called Leaf\nnodes at the bottom of the tree that are\nas pure as possible meaning instead of\nrandomly splitting the data we try to\nfind splits that lead to the resulting\ngroups or leaves to be as pure as\npossible which is to say that as few\ndata points as possible are\nmisclassified while this might seem like\na very basic and simple algorithm which\nit is we can turn it into a very\npowerful algorithm by combining many\ndecision trees together combining many\nsimple models to a more powerful complex\nmodel is called an ensemble algorithm\none form of ensembling is bagging where\nwe train multiple models on different\nsubsets of the training data using a\nmethod called bootstrap\na famous version of this idea is called\na random Forest where many decision\ntrees vote on the classification of your\ndata by majority vote of the different\ntrees in the random Forest random\nforests are very powerful estimators\nthat can be used both for classification\nand regression the randomness comes from\nrandomly excluding features for\ndifferent trees in the forest which\nprevents overfitting and makes it much\nmore robust because it removes\ncorrelation between the trees another\ntype of Ensemble method is called\nboosting where instead of running many\ndecision trees in parallel like for\nrandom forests we train models in\nsequence where each model focuses on\nfixing the errors made by the previous\nmodel we combine a series of weak models\nin sequence thus becoming a strong model\nbecause each sequential model tries to\nfix the errors of the previous model\nboosted trees often get to higher\naccuracies than random forests but are\nalso more prone to overfitting its\nsequential nature makes it slower to\ntrain than random forests famous\nexamples of boosted trees are Ada boost\ngradient boosting and XG boost the\ndetails of which are beyond the scope of\nthis video now let's get to the reigning\nking of AI neural networks to to\nunderstand neural networks let's look at\nlogistic regression again say we have a\nnumber of features and are trying to\npredict a target class the features\nmight be pixel intensities of a digital\nimage and the target might be\nclassifying the image as one of the\ndigits from 0 to 9 now for this\nparticular case you might see why this\nmight be difficult to do with logistic\nregression because say the number one\ndoesn't look the same when different\npeople write it and even if the same\nperson writes it several times it will\nlook slightly different each time and it\nwon't be the exact same pixels\nilluminated for every instance of the\nnumber one all of the instances of the\nnumber one have commonality however like\nthey all have a dominating vertical line\nand usually no Crossing Lines as other\ndigits might have and usually there are\nno circular shapes in the number one as\nthere would be in the number eight or or\nnine however the computer doesn't\ninitially know about these more complex\nfeatures but only the pixel intensities\nwe could manually engineer these\nfeatures by measuring some of these\nthings and explicitly adding them as new\nfeatures but artificial neural networks\nsimilarly to using a kernel function\nwith a support Vector machine are\ndesigned to implicitly and automatically\ndesign these features for us without any\nguidance from humans we do this by\nadding additional layers of unknown\nvariables between the input and output\nvariables in its simplest form this is\ncalled a single layer percep chop which\nis basically just a multi-feature\nregression task now if we add a hidden\nlayer the hidden variables in the middle\nlayer represent some hidden unknown\nfeatures and instead of predicting the\ntarget variable directly we try to\npredict these hidden features with our\ninput features and then try to predict\nthe target variables with our new hidden\nfeatures in our specific example we\nmight be able to say that every time\nseveral pixels are illuminated next to\neach other they represent a horizontal\nline which can be a new feature to try\nand predict the digit in question even\nthough we never explicitly defined a\nfeature called horizontal line This is a\nmuch simplified view of what is actually\ngoing on but hopefully this gets the\npoint across we don't usually know what\nthe hidden features represent we just\ntrain the neural network to predict the\nfinal Target as well as possible the\nhidden features we can Design This Way\nare limited in the case of the single\nhidden layer but what if we add a layer\nand have the hidden layer predict\nanother hidden layer what if we now had\neven more layers this is called Deep\nlearning and can result in very complex\nhidden features so that might represent\nall kinds of complex information in the\npictures like the fact that there is a\nface in the picture however we will\nusually not know what the hidden\nfeatures mean we just know that they\nresult in good predictions all we have\ntalked about so far is supervised\nlearning where we wanted to predict a\nspecific Target variable using some\ninput variables however sometimes we\ndon't have anything specific to predict\nand just want to find some underlying\nstructure in our data that's where\nunsupervised learning comes in a very\ncommon unsupervised problem is\nclustering it's easy to confuse\nclustering with classification but they\nare conceptually very different\nclassification is when we know the\nclasses we want to predict and have\ntraining data with true labels available\nshown as colors here like pictures of\ncats and dogs clustering is when we\ndon't have any labels and want to find\nunknown clusters just by looking at the\noverall structure of the data and trying\nto find potential clusters in the data\nfor example we might look at a\ntwo-dimensional data set that looks like\nthis any human will probably easily see\nthree clusters here but it's not always\nas straightforward as your data might\nmight also look like this we don't know\nhow many clusters there are because the\nproblem is unsupervised the most famous\nclustering algorithm is called K means\nclustering just like for KNN K is a\nhyperparameter and stands for the number\nof clusters you are looking for finding\nthe right number of clusters again is an\nart and has a lot to do with your\nspecific problem and some trial and\nerror in domain knowledge might be\nrequired this is beyond the scope of\nthis video K means is very simple you\nstart by randomly selecting centers for\nyour K clusters and assigning all data\npoints to the cluster center closest to\nthem the Clusters here are shown in blue\nand green you then recalculate the\ncluster centers based on the data points\nnow assigned to them you can see the\ncenters moving closer to the actual\nclusters you then assign the data points\nagain to the new cluster centers\nfollowed by recalculating the cluster\ncenters you repeat this process until\nthe centers of the Clusters have\nstabilized while K means is the most\nfamous and most common clustering\nalgorithm other algorithms exist\nincluding some where you don't need to\nspecify the number of clusters like\nhierarchical clustering and DB scan\nwhich can find clusters of arbitrary\nshape but I won't discuss them here the\nlast type of algorithm I will leave you\nwith is dimensionality reduction the\nidea of dimensionality reduction is to\nreduce the number of features or\ndimensions of your data set keeping as\nmuch information as possible usually\nthis group of algorithms does this by\nfinding correlations between existing\nfeatures and removing potentially\nredundant Dimensions without losing much\ninformation for example do you really\nneed a picture in high resolution to\nrecognize the airplane in the picture or\ncan you reduce the number of pixels in\nthe image as such dimensionality\nreduction will give you information\nabout the relationships within your\nexisting features and it can also be\nused as a pre-processing step in your\nsupervised learning algorithm to reduce\nthe number of features in your data set\nand make the algorithm more efficient\nand robust an example algorithm is\nprincipal component analysis or PCA\nlet's say we are trying to predict types\nof fish based on several features like\nlength height color and number of teeth\nwhen looking at the correlations of the\ndifferent features we might find that\nheight and length are strongly\ncorrelated and including both both won't\nhelp the algorithm much and might in\nfact hurt it by introducing noise we can\nsimply include a shape feature that is a\ncombination of the two this is actually\nextremely common in large data sets and\nallows us to reduce the number of\nfeatures dramatically and still get good\nresults PCA does this by finding the\ndirections in which most variance in the\ndata set is retained in this example the\ndirection of most variant is a diagonal\nthis is called the first principal\ncomponent or PC and can become our new\nshape feature the second principal\ncomponent is orthogonal to the first and\nonly explains a small fr C of the\nvariant of the data set and can thus be\nexcluded from our data set in this case\nin large data sets we can do this for\nall features and rank them by explain\nvariants and exclude any principal\ncomponents that don't contribute much to\nthe variant and thus wouldn't help much\nin our ml model this was all common\nmachine learning algorithms explained if\nyou are overwhelmed and don't know which\nalgorithm you need here is a great cheat\nsheet by syit learn that will help you\ndecide which algorithm is right for\nwhich type of problem if you want a road\nmap on how to learn machine learning\ncheck out my video on that\n",
  "words": [
    "next",
    "17",
    "minutes",
    "give",
    "overview",
    "important",
    "machine",
    "learning",
    "algorithms",
    "help",
    "decide",
    "one",
    "right",
    "problem",
    "name",
    "tim",
    "data",
    "scientist",
    "10",
    "years",
    "taught",
    "algorithms",
    "hundreds",
    "students",
    "real",
    "life",
    "machine",
    "learning",
    "boot",
    "camps",
    "simple",
    "strategy",
    "picking",
    "right",
    "algorithm",
    "problem",
    "17",
    "minutes",
    "know",
    "pick",
    "right",
    "one",
    "problem",
    "get",
    "basic",
    "intuition",
    "algorithm",
    "relate",
    "goal",
    "give",
    "many",
    "possible",
    "intuitive",
    "understanding",
    "major",
    "machine",
    "learning",
    "algorithms",
    "make",
    "stop",
    "feeling",
    "overwhelmed",
    "according",
    "wikipedia",
    "machine",
    "learning",
    "field",
    "study",
    "artificial",
    "intelligence",
    "concerned",
    "development",
    "study",
    "statistical",
    "algorithms",
    "learn",
    "data",
    "generalize",
    "unseen",
    "data",
    "thus",
    "perform",
    "tasks",
    "without",
    "explicit",
    "instructions",
    "much",
    "recent",
    "advancements",
    "ai",
    "driven",
    "neural",
    "networks",
    "hope",
    "give",
    "intuitive",
    "understanding",
    "end",
    "video",
    "let",
    "divide",
    "machine",
    "learning",
    "subfields",
    "generally",
    "machine",
    "learning",
    "divided",
    "two",
    "areas",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "supervised",
    "learning",
    "data",
    "set",
    "number",
    "independent",
    "variables",
    "also",
    "called",
    "features",
    "input",
    "variables",
    "dependent",
    "variable",
    "also",
    "called",
    "target",
    "output",
    "variable",
    "supposed",
    "predicted",
    "training",
    "data",
    "set",
    "know",
    "true",
    "values",
    "output",
    "variable",
    "also",
    "called",
    "labels",
    "train",
    "algorithm",
    "later",
    "predict",
    "output",
    "variable",
    "new",
    "unknown",
    "data",
    "examples",
    "could",
    "predicting",
    "price",
    "house",
    "output",
    "variable",
    "based",
    "features",
    "house",
    "say",
    "square",
    "footage",
    "location",
    "year",
    "construction",
    "etc",
    "categorizing",
    "object",
    "cat",
    "dog",
    "output",
    "variable",
    "label",
    "based",
    "features",
    "object",
    "say",
    "height",
    "weight",
    "size",
    "ears",
    "color",
    "eyes",
    "etc",
    "unsupervised",
    "learning",
    "basically",
    "learning",
    "problem",
    "supervised",
    "truth",
    "data",
    "known",
    "supervised",
    "algorithm",
    "would",
    "like",
    "showing",
    "little",
    "kid",
    "typical",
    "cat",
    "looks",
    "like",
    "typical",
    "dog",
    "looks",
    "like",
    "giving",
    "new",
    "picture",
    "asking",
    "animal",
    "sees",
    "unsupervised",
    "algorithm",
    "would",
    "giving",
    "kid",
    "idea",
    "cats",
    "dogs",
    "pile",
    "pictures",
    "animals",
    "asking",
    "group",
    "similarity",
    "without",
    "instructions",
    "examples",
    "unsupervised",
    "problems",
    "might",
    "sort",
    "emails",
    "three",
    "unspecified",
    "categories",
    "later",
    "inspect",
    "name",
    "wish",
    "algorithm",
    "decide",
    "create",
    "categories",
    "also",
    "called",
    "clusters",
    "let",
    "start",
    "supervised",
    "learning",
    "arguably",
    "bigger",
    "important",
    "branch",
    "machine",
    "learning",
    "broadly",
    "two",
    "subcategories",
    "regression",
    "want",
    "predict",
    "continuous",
    "numeric",
    "target",
    "variable",
    "given",
    "input",
    "variable",
    "using",
    "example",
    "could",
    "predicting",
    "price",
    "house",
    "given",
    "number",
    "features",
    "house",
    "determining",
    "relationship",
    "final",
    "price",
    "house",
    "might",
    "example",
    "find",
    "square",
    "footage",
    "directly",
    "proportional",
    "price",
    "linear",
    "dependence",
    "age",
    "house",
    "influence",
    "price",
    "house",
    "classification",
    "try",
    "assign",
    "discrete",
    "categorical",
    "label",
    "also",
    "called",
    "class",
    "data",
    "point",
    "example",
    "may",
    "want",
    "assign",
    "label",
    "spam",
    "spam",
    "email",
    "based",
    "content",
    "sender",
    "could",
    "also",
    "two",
    "classes",
    "example",
    "junk",
    "primary",
    "social",
    "promotions",
    "updates",
    "gmail",
    "default",
    "let",
    "dive",
    "actual",
    "algorithms",
    "starting",
    "mother",
    "machine",
    "learning",
    "algorithms",
    "linear",
    "regression",
    "general",
    "supervised",
    "learning",
    "algorithms",
    "try",
    "determine",
    "relationship",
    "two",
    "variables",
    "try",
    "find",
    "function",
    "maps",
    "one",
    "linear",
    "regression",
    "simplest",
    "form",
    "trying",
    "determine",
    "linear",
    "relationship",
    "two",
    "variables",
    "namely",
    "input",
    "output",
    "want",
    "fit",
    "linear",
    "equation",
    "data",
    "minimizing",
    "sum",
    "squares",
    "distances",
    "data",
    "points",
    "regression",
    "line",
    "simply",
    "minimizes",
    "average",
    "distance",
    "real",
    "data",
    "predictive",
    "model",
    "case",
    "regression",
    "line",
    "therefore",
    "minimize",
    "prediction",
    "errors",
    "new",
    "data",
    "points",
    "simple",
    "example",
    "linear",
    "relationship",
    "might",
    "height",
    "shoe",
    "size",
    "person",
    "regression",
    "fit",
    "might",
    "tell",
    "us",
    "every",
    "one",
    "unit",
    "shoe",
    "size",
    "increase",
    "person",
    "average",
    "2",
    "ines",
    "taller",
    "make",
    "model",
    "complex",
    "fit",
    "data",
    "output",
    "variable",
    "example",
    "shoe",
    "size",
    "might",
    "example",
    "want",
    "include",
    "gender",
    "age",
    "ethnicity",
    "person",
    "get",
    "even",
    "better",
    "model",
    "many",
    "fancy",
    "machine",
    "learning",
    "algorithms",
    "including",
    "neural",
    "networks",
    "extensions",
    "simple",
    "idea",
    "show",
    "later",
    "video",
    "logistic",
    "regression",
    "variant",
    "linear",
    "regression",
    "probably",
    "basic",
    "classification",
    "algorithm",
    "instead",
    "fitting",
    "line",
    "two",
    "numerical",
    "variables",
    "presumably",
    "linear",
    "relationship",
    "try",
    "predict",
    "categorical",
    "output",
    "variable",
    "using",
    "categorical",
    "numerical",
    "input",
    "variables",
    "let",
    "look",
    "example",
    "want",
    "predict",
    "one",
    "two",
    "classes",
    "example",
    "gender",
    "person",
    "based",
    "height",
    "weight",
    "linear",
    "regression",
    "would",
    "make",
    "much",
    "sense",
    "anymore",
    "instead",
    "fitting",
    "line",
    "data",
    "fit",
    "sigmoid",
    "function",
    "data",
    "looks",
    "like",
    "equation",
    "tell",
    "us",
    "linear",
    "relationship",
    "two",
    "variables",
    "conveniently",
    "tell",
    "us",
    "probability",
    "data",
    "point",
    "falling",
    "certain",
    "class",
    "given",
    "value",
    "input",
    "variable",
    "example",
    "likelihood",
    "adult",
    "person",
    "height",
    "180",
    "cm",
    "man",
    "would",
    "80",
    "completely",
    "made",
    "course",
    "k",
    "nearest",
    "neighbors",
    "algorithm",
    "knn",
    "simple",
    "intuitive",
    "algorithm",
    "used",
    "regression",
    "class",
    "ification",
    "algorithm",
    "name",
    "means",
    "try",
    "fit",
    "equations",
    "thus",
    "find",
    "parameters",
    "model",
    "true",
    "model",
    "fitting",
    "necessary",
    "idea",
    "knn",
    "simply",
    "given",
    "new",
    "data",
    "point",
    "predict",
    "target",
    "average",
    "k",
    "nearest",
    "neighbors",
    "might",
    "seem",
    "simple",
    "actually",
    "powerful",
    "predictive",
    "algorithm",
    "especially",
    "relationships",
    "complicated",
    "simple",
    "linear",
    "relationship",
    "classification",
    "example",
    "might",
    "say",
    "gender",
    "person",
    "majority",
    "five",
    "people",
    "closest",
    "weight",
    "height",
    "person",
    "question",
    "regression",
    "example",
    "might",
    "say",
    "weight",
    "person",
    "average",
    "weight",
    "three",
    "people",
    "closest",
    "height",
    "chest",
    "circumference",
    "makes",
    "ton",
    "intuitive",
    "sense",
    "might",
    "realize",
    "number",
    "three",
    "seems",
    "bit",
    "arbitrary",
    "k",
    "called",
    "hyperparameter",
    "algorithm",
    "choosing",
    "right",
    "k",
    "art",
    "choosing",
    "small",
    "number",
    "k",
    "say",
    "one",
    "two",
    "lead",
    "model",
    "predicting",
    "training",
    "data",
    "set",
    "well",
    "generalizing",
    "well",
    "unseen",
    "data",
    "called",
    "overfitting",
    "choosing",
    "large",
    "number",
    "say",
    "lead",
    "worst",
    "fit",
    "overall",
    "called",
    "underfitting",
    "best",
    "number",
    "somewhere",
    "depends",
    "lot",
    "problem",
    "hand",
    "methods",
    "finding",
    "right",
    "hyperparameters",
    "include",
    "cross",
    "validation",
    "beyond",
    "scope",
    "video",
    "support",
    "vector",
    "machine",
    "supervised",
    "machine",
    "learning",
    "algorithm",
    "originally",
    "designed",
    "classification",
    "tasks",
    "also",
    "used",
    "regression",
    "tasks",
    "core",
    "concept",
    "algorithm",
    "draw",
    "decision",
    "boundary",
    "data",
    "points",
    "separates",
    "data",
    "points",
    "training",
    "set",
    "well",
    "possible",
    "name",
    "suggests",
    "new",
    "unseen",
    "data",
    "point",
    "classified",
    "according",
    "falls",
    "respect",
    "decision",
    "boundary",
    "let",
    "take",
    "arbitrary",
    "example",
    "trying",
    "classify",
    "animals",
    "weight",
    "length",
    "nose",
    "simple",
    "case",
    "trying",
    "classify",
    "cats",
    "elephants",
    "decision",
    "boundary",
    "straight",
    "line",
    "svm",
    "algorithm",
    "tries",
    "find",
    "line",
    "separates",
    "classes",
    "largest",
    "margin",
    "possible",
    "maximizing",
    "space",
    "different",
    "classes",
    "makes",
    "decision",
    "boundary",
    "generalize",
    "well",
    "less",
    "sensitive",
    "noise",
    "outliers",
    "training",
    "data",
    "support",
    "vectors",
    "data",
    "points",
    "sit",
    "edge",
    "margin",
    "knowing",
    "support",
    "vectors",
    "enough",
    "classify",
    "new",
    "data",
    "points",
    "often",
    "makes",
    "algorithm",
    "memory",
    "efficient",
    "one",
    "benefits",
    "spm",
    "powerful",
    "high",
    "dimensions",
    "number",
    "features",
    "large",
    "compared",
    "size",
    "data",
    "higher",
    "dimensional",
    "cases",
    "decision",
    "boundary",
    "called",
    "hyperplane",
    "another",
    "feature",
    "makes",
    "svms",
    "extremely",
    "powerful",
    "use",
    "kernel",
    "functions",
    "allow",
    "identification",
    "highly",
    "complex",
    "nonlinear",
    "decision",
    "boundaries",
    "kernel",
    "functions",
    "implicit",
    "way",
    "turn",
    "original",
    "features",
    "new",
    "complex",
    "features",
    "using",
    "kernel",
    "trick",
    "beyond",
    "scope",
    "video",
    "allows",
    "efficient",
    "creation",
    "nonlinear",
    "decision",
    "boundaries",
    "creating",
    "complex",
    "new",
    "features",
    "weight",
    "divided",
    "height",
    "squared",
    "also",
    "called",
    "bmi",
    "called",
    "implicit",
    "feature",
    "engineering",
    "neural",
    "networks",
    "take",
    "idea",
    "implicit",
    "feature",
    "engineering",
    "next",
    "level",
    "explain",
    "later",
    "possible",
    "kernel",
    "functions",
    "svms",
    "linear",
    "polinomial",
    "rbf",
    "sigmoid",
    "kernel",
    "another",
    "fairly",
    "simple",
    "classifier",
    "naive",
    "bas",
    "classifier",
    "gets",
    "name",
    "b",
    "theorem",
    "looks",
    "like",
    "believe",
    "easiest",
    "understand",
    "naive",
    "bay",
    "example",
    "use",
    "case",
    "often",
    "used",
    "spam",
    "filters",
    "train",
    "algorithm",
    "number",
    "spam",
    "emails",
    "count",
    "occurrences",
    "different",
    "words",
    "class",
    "thereby",
    "calculate",
    "probability",
    "certain",
    "words",
    "appearing",
    "spam",
    "emails",
    "emails",
    "quickly",
    "classify",
    "new",
    "email",
    "based",
    "words",
    "contains",
    "using",
    "base",
    "theorem",
    "simply",
    "multiply",
    "different",
    "probabilities",
    "words",
    "email",
    "together",
    "algorithm",
    "makes",
    "false",
    "assumption",
    "probabilities",
    "different",
    "words",
    "appearing",
    "independent",
    "call",
    "classifier",
    "naive",
    "makes",
    "computation",
    "ally",
    "efficient",
    "still",
    "good",
    "approximation",
    "many",
    "use",
    "cases",
    "spam",
    "classification",
    "classification",
    "tasks",
    "decision",
    "trees",
    "basis",
    "number",
    "complex",
    "supervised",
    "learning",
    "algorithms",
    "simplest",
    "form",
    "decision",
    "tree",
    "looks",
    "somewhat",
    "like",
    "decision",
    "tree",
    "basically",
    "series",
    "yes",
    "questions",
    "allow",
    "us",
    "partition",
    "data",
    "set",
    "several",
    "dimensions",
    "example",
    "decision",
    "tree",
    "classifying",
    "people",
    "high",
    "lowrisk",
    "patients",
    "heart",
    "attacks",
    "goal",
    "decision",
    "tree",
    "algorithm",
    "create",
    "leaf",
    "nodes",
    "bottom",
    "tree",
    "pure",
    "possible",
    "meaning",
    "instead",
    "randomly",
    "splitting",
    "data",
    "try",
    "find",
    "splits",
    "lead",
    "resulting",
    "groups",
    "leaves",
    "pure",
    "possible",
    "say",
    "data",
    "points",
    "possible",
    "misclassified",
    "might",
    "seem",
    "like",
    "basic",
    "simple",
    "algorithm",
    "turn",
    "powerful",
    "algorithm",
    "combining",
    "many",
    "decision",
    "trees",
    "together",
    "combining",
    "many",
    "simple",
    "models",
    "powerful",
    "complex",
    "model",
    "called",
    "ensemble",
    "algorithm",
    "one",
    "form",
    "ensembling",
    "bagging",
    "train",
    "multiple",
    "models",
    "different",
    "subsets",
    "training",
    "data",
    "using",
    "method",
    "called",
    "bootstrap",
    "famous",
    "version",
    "idea",
    "called",
    "random",
    "forest",
    "many",
    "decision",
    "trees",
    "vote",
    "classification",
    "data",
    "majority",
    "vote",
    "different",
    "trees",
    "random",
    "forest",
    "random",
    "forests",
    "powerful",
    "estimators",
    "used",
    "classification",
    "regression",
    "randomness",
    "comes",
    "randomly",
    "excluding",
    "features",
    "different",
    "trees",
    "forest",
    "prevents",
    "overfitting",
    "makes",
    "much",
    "robust",
    "removes",
    "correlation",
    "trees",
    "another",
    "type",
    "ensemble",
    "method",
    "called",
    "boosting",
    "instead",
    "running",
    "many",
    "decision",
    "trees",
    "parallel",
    "like",
    "random",
    "forests",
    "train",
    "models",
    "sequence",
    "model",
    "focuses",
    "fixing",
    "errors",
    "made",
    "previous",
    "model",
    "combine",
    "series",
    "weak",
    "models",
    "sequence",
    "thus",
    "becoming",
    "strong",
    "model",
    "sequential",
    "model",
    "tries",
    "fix",
    "errors",
    "previous",
    "model",
    "boosted",
    "trees",
    "often",
    "get",
    "higher",
    "accuracies",
    "random",
    "forests",
    "also",
    "prone",
    "overfitting",
    "sequential",
    "nature",
    "makes",
    "slower",
    "train",
    "random",
    "forests",
    "famous",
    "examples",
    "boosted",
    "trees",
    "ada",
    "boost",
    "gradient",
    "boosting",
    "xg",
    "boost",
    "details",
    "beyond",
    "scope",
    "video",
    "let",
    "get",
    "reigning",
    "king",
    "ai",
    "neural",
    "networks",
    "understand",
    "neural",
    "networks",
    "let",
    "look",
    "logistic",
    "regression",
    "say",
    "number",
    "features",
    "trying",
    "predict",
    "target",
    "class",
    "features",
    "might",
    "pixel",
    "intensities",
    "digital",
    "image",
    "target",
    "might",
    "classifying",
    "image",
    "one",
    "digits",
    "0",
    "9",
    "particular",
    "case",
    "might",
    "see",
    "might",
    "difficult",
    "logistic",
    "regression",
    "say",
    "number",
    "one",
    "look",
    "different",
    "people",
    "write",
    "even",
    "person",
    "writes",
    "several",
    "times",
    "look",
    "slightly",
    "different",
    "time",
    "wo",
    "exact",
    "pixels",
    "illuminated",
    "every",
    "instance",
    "number",
    "one",
    "instances",
    "number",
    "one",
    "commonality",
    "however",
    "like",
    "dominating",
    "vertical",
    "line",
    "usually",
    "crossing",
    "lines",
    "digits",
    "might",
    "usually",
    "circular",
    "shapes",
    "number",
    "one",
    "would",
    "number",
    "eight",
    "nine",
    "however",
    "computer",
    "initially",
    "know",
    "complex",
    "features",
    "pixel",
    "intensities",
    "could",
    "manually",
    "engineer",
    "features",
    "measuring",
    "things",
    "explicitly",
    "adding",
    "new",
    "features",
    "artificial",
    "neural",
    "networks",
    "similarly",
    "using",
    "kernel",
    "function",
    "support",
    "vector",
    "machine",
    "designed",
    "implicitly",
    "automatically",
    "design",
    "features",
    "us",
    "without",
    "guidance",
    "humans",
    "adding",
    "additional",
    "layers",
    "unknown",
    "variables",
    "input",
    "output",
    "variables",
    "simplest",
    "form",
    "called",
    "single",
    "layer",
    "percep",
    "chop",
    "basically",
    "regression",
    "task",
    "add",
    "hidden",
    "layer",
    "hidden",
    "variables",
    "middle",
    "layer",
    "represent",
    "hidden",
    "unknown",
    "features",
    "instead",
    "predicting",
    "target",
    "variable",
    "directly",
    "try",
    "predict",
    "hidden",
    "features",
    "input",
    "features",
    "try",
    "predict",
    "target",
    "variables",
    "new",
    "hidden",
    "features",
    "specific",
    "example",
    "might",
    "able",
    "say",
    "every",
    "time",
    "several",
    "pixels",
    "illuminated",
    "next",
    "represent",
    "horizontal",
    "line",
    "new",
    "feature",
    "try",
    "predict",
    "digit",
    "question",
    "even",
    "though",
    "never",
    "explicitly",
    "defined",
    "feature",
    "called",
    "horizontal",
    "line",
    "much",
    "simplified",
    "view",
    "actually",
    "going",
    "hopefully",
    "gets",
    "point",
    "across",
    "usually",
    "know",
    "hidden",
    "features",
    "represent",
    "train",
    "neural",
    "network",
    "predict",
    "final",
    "target",
    "well",
    "possible",
    "hidden",
    "features",
    "design",
    "way",
    "limited",
    "case",
    "single",
    "hidden",
    "layer",
    "add",
    "layer",
    "hidden",
    "layer",
    "predict",
    "another",
    "hidden",
    "layer",
    "even",
    "layers",
    "called",
    "deep",
    "learning",
    "result",
    "complex",
    "hidden",
    "features",
    "might",
    "represent",
    "kinds",
    "complex",
    "information",
    "pictures",
    "like",
    "fact",
    "face",
    "picture",
    "however",
    "usually",
    "know",
    "hidden",
    "features",
    "mean",
    "know",
    "result",
    "good",
    "predictions",
    "talked",
    "far",
    "supervised",
    "learning",
    "wanted",
    "predict",
    "specific",
    "target",
    "variable",
    "using",
    "input",
    "variables",
    "however",
    "sometimes",
    "anything",
    "specific",
    "predict",
    "want",
    "find",
    "underlying",
    "structure",
    "data",
    "unsupervised",
    "learning",
    "comes",
    "common",
    "unsupervised",
    "problem",
    "clustering",
    "easy",
    "confuse",
    "clustering",
    "classification",
    "conceptually",
    "different",
    "classification",
    "know",
    "classes",
    "want",
    "predict",
    "training",
    "data",
    "true",
    "labels",
    "available",
    "shown",
    "colors",
    "like",
    "pictures",
    "cats",
    "dogs",
    "clustering",
    "labels",
    "want",
    "find",
    "unknown",
    "clusters",
    "looking",
    "overall",
    "structure",
    "data",
    "trying",
    "find",
    "potential",
    "clusters",
    "data",
    "example",
    "might",
    "look",
    "data",
    "set",
    "looks",
    "like",
    "human",
    "probably",
    "easily",
    "see",
    "three",
    "clusters",
    "always",
    "straightforward",
    "data",
    "might",
    "might",
    "also",
    "look",
    "like",
    "know",
    "many",
    "clusters",
    "problem",
    "unsupervised",
    "famous",
    "clustering",
    "algorithm",
    "called",
    "k",
    "means",
    "clustering",
    "like",
    "knn",
    "k",
    "hyperparameter",
    "stands",
    "number",
    "clusters",
    "looking",
    "finding",
    "right",
    "number",
    "clusters",
    "art",
    "lot",
    "specific",
    "problem",
    "trial",
    "error",
    "domain",
    "knowledge",
    "might",
    "required",
    "beyond",
    "scope",
    "video",
    "k",
    "means",
    "simple",
    "start",
    "randomly",
    "selecting",
    "centers",
    "k",
    "clusters",
    "assigning",
    "data",
    "points",
    "cluster",
    "center",
    "closest",
    "clusters",
    "shown",
    "blue",
    "green",
    "recalculate",
    "cluster",
    "centers",
    "based",
    "data",
    "points",
    "assigned",
    "see",
    "centers",
    "moving",
    "closer",
    "actual",
    "clusters",
    "assign",
    "data",
    "points",
    "new",
    "cluster",
    "centers",
    "followed",
    "recalculating",
    "cluster",
    "centers",
    "repeat",
    "process",
    "centers",
    "clusters",
    "stabilized",
    "k",
    "means",
    "famous",
    "common",
    "clustering",
    "algorithm",
    "algorithms",
    "exist",
    "including",
    "need",
    "specify",
    "number",
    "clusters",
    "like",
    "hierarchical",
    "clustering",
    "db",
    "scan",
    "find",
    "clusters",
    "arbitrary",
    "shape",
    "wo",
    "discuss",
    "last",
    "type",
    "algorithm",
    "leave",
    "dimensionality",
    "reduction",
    "idea",
    "dimensionality",
    "reduction",
    "reduce",
    "number",
    "features",
    "dimensions",
    "data",
    "set",
    "keeping",
    "much",
    "information",
    "possible",
    "usually",
    "group",
    "algorithms",
    "finding",
    "correlations",
    "existing",
    "features",
    "removing",
    "potentially",
    "redundant",
    "dimensions",
    "without",
    "losing",
    "much",
    "information",
    "example",
    "really",
    "need",
    "picture",
    "high",
    "resolution",
    "recognize",
    "airplane",
    "picture",
    "reduce",
    "number",
    "pixels",
    "image",
    "dimensionality",
    "reduction",
    "give",
    "information",
    "relationships",
    "within",
    "existing",
    "features",
    "also",
    "used",
    "step",
    "supervised",
    "learning",
    "algorithm",
    "reduce",
    "number",
    "features",
    "data",
    "set",
    "make",
    "algorithm",
    "efficient",
    "robust",
    "example",
    "algorithm",
    "principal",
    "component",
    "analysis",
    "pca",
    "let",
    "say",
    "trying",
    "predict",
    "types",
    "fish",
    "based",
    "several",
    "features",
    "like",
    "length",
    "height",
    "color",
    "number",
    "teeth",
    "looking",
    "correlations",
    "different",
    "features",
    "might",
    "find",
    "height",
    "length",
    "strongly",
    "correlated",
    "including",
    "wo",
    "help",
    "algorithm",
    "much",
    "might",
    "fact",
    "hurt",
    "introducing",
    "noise",
    "simply",
    "include",
    "shape",
    "feature",
    "combination",
    "two",
    "actually",
    "extremely",
    "common",
    "large",
    "data",
    "sets",
    "allows",
    "us",
    "reduce",
    "number",
    "features",
    "dramatically",
    "still",
    "get",
    "good",
    "results",
    "pca",
    "finding",
    "directions",
    "variance",
    "data",
    "set",
    "retained",
    "example",
    "direction",
    "variant",
    "diagonal",
    "called",
    "first",
    "principal",
    "component",
    "pc",
    "become",
    "new",
    "shape",
    "feature",
    "second",
    "principal",
    "component",
    "orthogonal",
    "first",
    "explains",
    "small",
    "fr",
    "c",
    "variant",
    "data",
    "set",
    "thus",
    "excluded",
    "data",
    "set",
    "case",
    "large",
    "data",
    "sets",
    "features",
    "rank",
    "explain",
    "variants",
    "exclude",
    "principal",
    "components",
    "contribute",
    "much",
    "variant",
    "thus",
    "would",
    "help",
    "much",
    "ml",
    "model",
    "common",
    "machine",
    "learning",
    "algorithms",
    "explained",
    "overwhelmed",
    "know",
    "algorithm",
    "need",
    "great",
    "cheat",
    "sheet",
    "syit",
    "learn",
    "help",
    "decide",
    "algorithm",
    "right",
    "type",
    "problem",
    "want",
    "road",
    "map",
    "learn",
    "machine",
    "learning",
    "check",
    "video"
  ],
  "keywords": [
    "next",
    "give",
    "machine",
    "learning",
    "algorithms",
    "help",
    "decide",
    "one",
    "right",
    "problem",
    "name",
    "data",
    "simple",
    "algorithm",
    "know",
    "get",
    "basic",
    "many",
    "possible",
    "intuitive",
    "make",
    "learn",
    "unseen",
    "thus",
    "tasks",
    "without",
    "much",
    "neural",
    "networks",
    "video",
    "let",
    "two",
    "supervised",
    "unsupervised",
    "set",
    "number",
    "variables",
    "also",
    "called",
    "features",
    "input",
    "variable",
    "target",
    "output",
    "training",
    "true",
    "labels",
    "train",
    "later",
    "predict",
    "new",
    "unknown",
    "examples",
    "could",
    "predicting",
    "price",
    "house",
    "based",
    "say",
    "label",
    "height",
    "weight",
    "size",
    "basically",
    "would",
    "like",
    "looks",
    "picture",
    "idea",
    "cats",
    "pictures",
    "might",
    "emails",
    "three",
    "clusters",
    "regression",
    "want",
    "given",
    "using",
    "example",
    "relationship",
    "find",
    "linear",
    "classification",
    "try",
    "assign",
    "categorical",
    "class",
    "point",
    "spam",
    "email",
    "classes",
    "function",
    "simplest",
    "form",
    "trying",
    "fit",
    "points",
    "line",
    "simply",
    "average",
    "model",
    "case",
    "errors",
    "shoe",
    "person",
    "tell",
    "us",
    "every",
    "complex",
    "include",
    "gender",
    "even",
    "including",
    "logistic",
    "variant",
    "instead",
    "fitting",
    "look",
    "k",
    "knn",
    "used",
    "means",
    "actually",
    "powerful",
    "people",
    "closest",
    "makes",
    "arbitrary",
    "choosing",
    "lead",
    "well",
    "overfitting",
    "large",
    "finding",
    "beyond",
    "scope",
    "support",
    "decision",
    "boundary",
    "classify",
    "length",
    "different",
    "often",
    "efficient",
    "high",
    "dimensions",
    "another",
    "feature",
    "use",
    "kernel",
    "functions",
    "implicit",
    "classifier",
    "naive",
    "words",
    "good",
    "trees",
    "tree",
    "several",
    "randomly",
    "models",
    "famous",
    "random",
    "forest",
    "forests",
    "type",
    "image",
    "see",
    "wo",
    "pixels",
    "however",
    "usually",
    "layer",
    "hidden",
    "represent",
    "specific",
    "information",
    "common",
    "clustering",
    "looking",
    "centers",
    "cluster",
    "need",
    "shape",
    "dimensionality",
    "reduction",
    "reduce",
    "principal",
    "component"
  ]
}