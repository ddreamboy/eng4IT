{
  "text": "So far in our deep learning tutorial\nseries we looked at\nArtificial Neural Network and\nConvolutional Neural Network which is\nmainly used for image processing.\nIn this video we will talk about\nRecurrent Neural Network which is used\nmainly for\nnatural language processing tasks so if\nyou think about deep learning overall,\nCNNs are mainly for images, RNNs are\nmainly for\nNLP. There are other use cases as well so\nwe'll understand\nhow Recurrent Neural Network works and\nwe'll look at different\napplications of RNN in the field of NLP\nas well as some other domains.\nWe will be looking at some real-life use\ncases where\nsequence models are useful. You must have\nused\nGoogle mail-Gmail. Here, when you type in\na sentence it will auto complete it. So\njust see,\nwhen I type \"not interested at this time\"\nis something it auto completed.\nSo google has this RRN or Recurrent\nNeural Network\nembedded into it where, when you type in\na sentence \"not interested at\"\nit will auto complete with \"this time\". If\nyou say \"we'll let you know if it changes\"\nit will also say \"in the future\" so this\nsaves you time.\nIt will write the sentence for you.\nAnother use case is translation.\nYou must have used Google Translate\nwhere you can translate\nsentence from one to another language\neasily.\nThird use case is Named Entity\nRecognization\nwhere in the X you know you give Neural\nNetwork a statement\nand in the Y Neural Network will tell\nyou the person name\nthe company and time. Rudolph Smith must\nbe a millionaire with Tesla's\nprices skyrocketing.\nSo these are various use cases where\nusing sequence models or\nRNN-Recurrent Neural Network helps.\nThe fourth use case is Sentiment\nAnalysis where you have a paragraph and\nit will tell you\nthe sentiment whether this product\nreview is One star, Two star\nand so on. Now you would think - \nWhy can't we use a simple Neural Network\nto solve this problem? See all these\nproblems\nthey are called Sequence Modeling\nproblem\nbecause the sequence is important. When\nit comes to human language\nsequence is very important. For example\nwhen you say, \"how are you?\" versus\n\"you are how\" doesn't make sense,\nright? So the sequence is important here\nand you would\nthink - Why don't we use simple neural\nnetwork for that? Well,\nlet's try it. So for language translation\nhow about we build this kind of neural\nnetwork\nwe know where input is the English\nstatement and the output could be Hindi\nstatement\nOnce I build this network, what if my\nsentence size changes? So i might be\ninputting\ndifferent sentence size and with a fixed\nneural network architecture it's not\ngoing to work\nbecause you have to decide how many\nneurons are there in the input and\noutput layer.\nSo with language translation, number of\nneurons\nbecomes a problem. Like what do you\ndecide as a size of neurons?\nNow one would argue okay I would decide\nlet's say a\nhuge size let's say 100 neurons and\nremaining\nif I am saying, did you eat biryani? So it\nwill occupy 4 neuron.\nRemaining 96 I will just say 0\nor you know blank statement. That might\nwork\nbut still it's not ideal. The second\nissue is too much computation.\nYou all know neural networks work\non numbers, they don't work on string.\nSo you have to convert your word into\na vector. So one of the ways of\nconverting that into a vector\nis -l et's say there are 25000 words\nin your vocabulary and you will do one\nhot encoding\nwhere you know \"how\" let's say is at 46th\nposition\n\"are\" is let's say second position \"you\" is\nlet's say\nat 17000th position. So at that position\nyou put 1,\nremaining position you put 0 and that's\ncalled\none hot encoding. You have to do similar\nthing for output as well.\nBut you realize this will increase too\nmuch computation.\nEach of the word when you convert it to\na vector\nyou know how many neurons you need in\nthe input layer itself.\nIts humongous. The third issue\nis this - Sometimes when you translate\nlanguage\nyou for let's say two different English\nstatements you might have a\nHindi statement. So in this case\nwhen I say \"On sunday I ate golgappa\"\nlet's say\nI train this network based on this\nstatement\nand then for 'On Sunday' let's say it will\nadjust the weights of\nall these edges which I have highlighted\nin yellow color.\nSame statement I can say differently. I\ncan say \"I\nate golgappa on Sunday\". So now on Sunday\nthe meaning of on Sunday\nis same but here neural network has to\nlearn different set of edges you see all\nthese edges are in yellow color.\nSo the parameters are not shared.\nWe looked at in our Convolutional Neural\nNetwork tutorial as well that by\nusing convolution operation we can share\nthe parameters.\nHere, the use of\nANN or Artificial Neural Network doesn't\nallow you to\ndo that okay. Also\nthe most important part in all this\ndiscussion is the sequence.\nSee when you have structured data, for\nexample you're trying to figure out\nif the transaction is fraud or not and\nlet's say\nyour features are transaction amount,\nwhether the transaction was made out of\ncountry\nor whether the SSN that customer\nprovided is correct or not.\nNow here if you change the order of this\nfeatures, let's say 'ssn correct?' I supply\nyou know\nmy first neuron\nit's not going to affect anything\nyou know because \nthe sequence in which you supply the\ninput doesn't matter.\nWhereas if you have\nEnglish to Hindi\ntranslation\nand instead of saying \"I ate golgappa\non sunday'\nand if I say\n181\n00:06:25,360 --> 00:06:31,600\n\"I ate Sunday on golgappa\" the meaning\nbecomes totally different. So now you\ncannot say that\nthe Hindi translation is 'ravivar ko mene golgappe khaye\"\nbecause it becomes invalid so sequence\nis very very important that's why\nArtificial Neural Network doesn't work\nin this case.\nJust to summarize these are the three\nmajor problems\nwith using ANN for sequence problems.\nLet's once again talk about Named Entity\nRecognition.\nLet's say 'Dhaval loves baby yoda'\nI love my baby grogu. I love Mandalorian\nseries and\nwe have got this nice baby grogu at\nour home\nwhich actually talks with us. In this\nstatement\nDhaval and baby yoda are person names,\nokay.\nSo the whole purpose of Named Entity\nRecognization is\nto find out the\nentity you know like 'Dhaval' as an entity\nis a person 'baby yoda'\nas an entity as a person so that's the\nwhole\ngoal of NER.\nNow you can represent this as ones and\nzero. So if the\nword is person's name you would mark it as\none\nand if it is not a person's name \nyou would mark it as zero.\nso let's see how RNN works here. RNN is also called Recurrent Neural Network.\nso first of all you have to convert Dhaval into some vector. \nIt doesn't matter how you convert\n216\n00:07:57,199 --> 00:08:01,680\nit you can take a vocabulary and use one\nhot encoding and there are other ways\nof\nvectorizing a word. Then\nyou have a layer of neurons. So these are\nall individual neurons.\nLet's say this is one layer. It's a\nhidden layer\nyou supply that and you get one output\nokay.\nSo each neuron all you know has a sigma\nfunction\nand activation function. So now while\nprocessing the statement 'Dhaval loves\nbaby yoda'\nnow I will process it word by word.\nSo I supply 'Dhaval', get the output and\nthen I\ngo back again. Now I supply\n'loves' converted into vector and the\nprevious output which I\ngot which was y Dhaval I now supply\nthat as an input to this\nlayer. So you see\nthe input of the layer is not only the\nnext word but the previous\noutput because\nthe language makes sense. Language needs\nto carry the context\nif I have just a word loss and if I\ndon't have Dhaval in front of it\nit might mean a different thing. So there\nis a context that you need\nand this kind of architecture provides\nyour context or a memory.\nIn the third word again you supply 'baby'\nto the same network right. So our network\nhas only one layer.\nit has only one layer, so there is input\nlayer output layer and the hidden layer\nis just one\nand it has bunch of neurons. In that we\nare repeatedly processing word\none by one okay\nand you keep on doing this. Now the\nbenefit of this is\nwhen i'm processing 'baby' when i get why\nloves\nthat 'why loves' carries the state, the\nprevious state or previous memory\nof 'Dhaval loves' you know the whole\nstatement.\nNow i'm presenting this in a different\nway make sure\nthese are not four different hidden\nlayers.\nThis is a time travel\nokay so actual hidden layer is only one.\nI am just doing a time travel. So first\nwhen I supplied word 'Dhaval'\ni got this output and output was nothing\nbut the activation function which I am\ndenoting with a1\nand you need some previous activation a0\nas well.\nLet's say it's a vector of all zeros\nthen you supply second word\n'loves' and use the previous output which\nwas yDhaval\nso yDhaval and a1 they are\nboth same here\nand then you get another output a2\nwhere\nthat you supply along with the third\nword 'baby'\nto the same network. So these four\nneurons it's the same\nsingle layer. I am just showing\nthe status of it at different times okay.\nSo you have to be very clear on this\nthat these are not four different layers.\nIt's just one layer just because I am\nshowing different time steps that's why\nI'm showing you almost a time travel\nhere\nand once the network is trained\nof course it will output like 'Dhaval' is\none 'loves' is zero 'baby' is one and so on\nokay. So you get your NER output\nindividually here. One other way of\nrepresenting the same network\nokay because just to avoid confusion and\nto\nmake presentation little more clear. Many\ntimes\nin literature you will see presentation\nlike this - \nWhere each word which is an input comes\nfrom the bottom\nand there is activation. So again this\nand these two diagrams are exactly same\nokay\nI'm just putting this word at the bottom.\nGeneric Representation of RNN is this.\nSo this is the real representation. You have\nonly one layer\nand you are you are kind of almost in a\nloop. You are\nsupplying the output of previous\nword as an input to the second word.\nSo now let's talk about training. So\nagain\nthe problem we are talking about is NER\nwhere these are my training samples okay\nx and y.  'x' is a statement 'y' is whether\na given word is person name or not\nso we are processing first training\nsample 'Dhaval loves baby yoda'\nso this one I will first\ninitialize my neural network weights\nwith some random values,\nthen I supply each word, then I calculate\ny hat which is predicted\ny, then I compare with the real y so real\ny here\nis 1 0 1 1 so I compare that with here\nso 1\n0 1 1 I compare that with y hat\nand I find out the loss okay\nand then I sum the loss.\nSo that will be my total loss. You all\nknow about grade and descent right. So we\ncompute the loss then we back propagate\nthe loss\nand we adjust the weights. So now I take\nthe second statement\n'Iron man punched on hulk's face'\nhe was very angry with hulk. Again i\ncalculate all the losses\nthen I find total loss and\nthen I do grid and decent to reduce the\nloss.\nSo i keep on doing this for all my\ntraining samples. Let's I have 100 training\nsamples.\nPassing all hundred training samples\nthrough this network will be one epoch.\nWe might do let's say 20 epochs and at\nthe end of the 20 epoch\nmy loss might become very minimum. At\nthat point we can say my neural network\nis trained.\nLet's take a look at language\ntranslation. So in language translation\nwhat happens is\nyou supply first word to your\nnetwork then you get the output\nthen again same network you supply\nsecond word and the output from previous\nstep\nas an input and of course when you\nsupply first where you have to\npass in some activation values let's say\nall\na vector of all zeros.\nThen you supply third word for fourth\nword and so on\nand when you're done with all the words\nthat's when the network starts to\ntranslate it because\nyou cannot translate one word by one,\nbecause after\nthe statement I can push maybe one more\nword and that will just totally change\nmy translation.\nThat's why for language translation you\nhave to supply all the words and only\nthen\nthe network can translate for you.\nSo the network will translate it like\nthis\nand the first part is called encoder\nthe second part is called decoder.\nWe will go\nmore in depth into all this but I want\nto quickly demonstrate how\nthe neural network looks in the case of\nlanguage translation.\nNow this layer doesn't have to be just\nsingle layer. It can be a deep RNN as\nwell\nwhere the actual network might have\nmultiple hidden layers\nokay. So I hope that clarifies\nthe architecture behind RNN and you\nunderstand why you can't use simple\nneural network here and you have to use\nspecialized\nneural network called RNN which can\nmemorize for you, which can\nremember previous state because language\nis\nall about sequence. If you change the\nsequence the meaning changes\nso if you like this video please give\nit a thumbs up\nand we'll be having more Recurrent\nNeural Network and NLP type\ntutorials in the future videos\nThank you.\n",
  "words": [
    "far",
    "deep",
    "learning",
    "tutorial",
    "series",
    "looked",
    "artificial",
    "neural",
    "network",
    "convolutional",
    "neural",
    "network",
    "mainly",
    "used",
    "image",
    "processing",
    "video",
    "talk",
    "recurrent",
    "neural",
    "network",
    "used",
    "mainly",
    "natural",
    "language",
    "processing",
    "tasks",
    "think",
    "deep",
    "learning",
    "overall",
    "cnns",
    "mainly",
    "images",
    "rnns",
    "mainly",
    "nlp",
    "use",
    "cases",
    "well",
    "understand",
    "recurrent",
    "neural",
    "network",
    "works",
    "look",
    "different",
    "applications",
    "rnn",
    "field",
    "nlp",
    "well",
    "domains",
    "looking",
    "use",
    "cases",
    "sequence",
    "models",
    "useful",
    "must",
    "used",
    "google",
    "type",
    "sentence",
    "auto",
    "complete",
    "see",
    "type",
    "interested",
    "time",
    "something",
    "auto",
    "completed",
    "google",
    "rrn",
    "recurrent",
    "neural",
    "network",
    "embedded",
    "type",
    "sentence",
    "interested",
    "auto",
    "complete",
    "time",
    "say",
    "let",
    "know",
    "changes",
    "also",
    "say",
    "future",
    "saves",
    "time",
    "write",
    "sentence",
    "another",
    "use",
    "case",
    "translation",
    "must",
    "used",
    "google",
    "translate",
    "translate",
    "sentence",
    "one",
    "another",
    "language",
    "easily",
    "third",
    "use",
    "case",
    "named",
    "entity",
    "recognization",
    "x",
    "know",
    "give",
    "neural",
    "network",
    "statement",
    "neural",
    "network",
    "tell",
    "person",
    "name",
    "company",
    "time",
    "rudolph",
    "smith",
    "must",
    "millionaire",
    "tesla",
    "prices",
    "skyrocketing",
    "various",
    "use",
    "cases",
    "using",
    "sequence",
    "models",
    "neural",
    "network",
    "helps",
    "fourth",
    "use",
    "case",
    "sentiment",
    "analysis",
    "paragraph",
    "tell",
    "sentiment",
    "whether",
    "product",
    "review",
    "one",
    "star",
    "two",
    "star",
    "would",
    "think",
    "ca",
    "use",
    "simple",
    "neural",
    "network",
    "solve",
    "problem",
    "see",
    "problems",
    "called",
    "sequence",
    "modeling",
    "problem",
    "sequence",
    "important",
    "comes",
    "human",
    "language",
    "sequence",
    "important",
    "example",
    "say",
    "versus",
    "make",
    "sense",
    "right",
    "sequence",
    "important",
    "would",
    "think",
    "use",
    "simple",
    "neural",
    "network",
    "well",
    "let",
    "try",
    "language",
    "translation",
    "build",
    "kind",
    "neural",
    "network",
    "know",
    "input",
    "english",
    "statement",
    "output",
    "could",
    "hindi",
    "statement",
    "build",
    "network",
    "sentence",
    "size",
    "changes",
    "might",
    "inputting",
    "different",
    "sentence",
    "size",
    "fixed",
    "neural",
    "network",
    "architecture",
    "going",
    "work",
    "decide",
    "many",
    "neurons",
    "input",
    "output",
    "layer",
    "language",
    "translation",
    "number",
    "neurons",
    "becomes",
    "problem",
    "like",
    "decide",
    "size",
    "neurons",
    "one",
    "would",
    "argue",
    "okay",
    "would",
    "decide",
    "let",
    "say",
    "huge",
    "size",
    "let",
    "say",
    "100",
    "neurons",
    "remaining",
    "saying",
    "eat",
    "biryani",
    "occupy",
    "4",
    "neuron",
    "remaining",
    "96",
    "say",
    "0",
    "know",
    "blank",
    "statement",
    "might",
    "work",
    "still",
    "ideal",
    "second",
    "issue",
    "much",
    "computation",
    "know",
    "neural",
    "networks",
    "work",
    "numbers",
    "work",
    "string",
    "convert",
    "word",
    "vector",
    "one",
    "ways",
    "converting",
    "vector",
    "et",
    "say",
    "25000",
    "words",
    "vocabulary",
    "one",
    "hot",
    "encoding",
    "know",
    "let",
    "say",
    "46th",
    "position",
    "let",
    "say",
    "second",
    "position",
    "let",
    "say",
    "17000th",
    "position",
    "position",
    "put",
    "1",
    "remaining",
    "position",
    "put",
    "0",
    "called",
    "one",
    "hot",
    "encoding",
    "similar",
    "thing",
    "output",
    "well",
    "realize",
    "increase",
    "much",
    "computation",
    "word",
    "convert",
    "vector",
    "know",
    "many",
    "neurons",
    "need",
    "input",
    "layer",
    "humongous",
    "third",
    "issue",
    "sometimes",
    "translate",
    "language",
    "let",
    "say",
    "two",
    "different",
    "english",
    "statements",
    "might",
    "hindi",
    "statement",
    "case",
    "say",
    "sunday",
    "ate",
    "golgappa",
    "let",
    "say",
    "train",
    "network",
    "based",
    "statement",
    "sunday",
    "let",
    "say",
    "adjust",
    "weights",
    "edges",
    "highlighted",
    "yellow",
    "color",
    "statement",
    "say",
    "differently",
    "say",
    "ate",
    "golgappa",
    "sunday",
    "sunday",
    "meaning",
    "sunday",
    "neural",
    "network",
    "learn",
    "different",
    "set",
    "edges",
    "see",
    "edges",
    "yellow",
    "color",
    "parameters",
    "shared",
    "looked",
    "convolutional",
    "neural",
    "network",
    "tutorial",
    "well",
    "using",
    "convolution",
    "operation",
    "share",
    "parameters",
    "use",
    "ann",
    "artificial",
    "neural",
    "network",
    "allow",
    "okay",
    "also",
    "important",
    "part",
    "discussion",
    "sequence",
    "see",
    "structured",
    "data",
    "example",
    "trying",
    "figure",
    "transaction",
    "fraud",
    "let",
    "say",
    "features",
    "transaction",
    "amount",
    "whether",
    "transaction",
    "made",
    "country",
    "whether",
    "ssn",
    "customer",
    "provided",
    "correct",
    "change",
    "order",
    "features",
    "let",
    "say",
    "correct",
    "supply",
    "know",
    "first",
    "neuron",
    "going",
    "affect",
    "anything",
    "know",
    "sequence",
    "supply",
    "input",
    "matter",
    "whereas",
    "english",
    "hindi",
    "translation",
    "instead",
    "saying",
    "ate",
    "golgappa",
    "sunday",
    "say",
    "181",
    "ate",
    "sunday",
    "golgappa",
    "meaning",
    "becomes",
    "totally",
    "different",
    "say",
    "hindi",
    "translation",
    "ko",
    "mene",
    "golgappe",
    "khaye",
    "becomes",
    "invalid",
    "sequence",
    "important",
    "artificial",
    "neural",
    "network",
    "work",
    "case",
    "summarize",
    "three",
    "major",
    "problems",
    "using",
    "ann",
    "sequence",
    "problems",
    "let",
    "talk",
    "named",
    "entity",
    "recognition",
    "let",
    "say",
    "loves",
    "baby",
    "yoda",
    "love",
    "baby",
    "grogu",
    "love",
    "mandalorian",
    "series",
    "got",
    "nice",
    "baby",
    "grogu",
    "home",
    "actually",
    "talks",
    "us",
    "statement",
    "dhaval",
    "baby",
    "yoda",
    "person",
    "names",
    "okay",
    "whole",
    "purpose",
    "named",
    "entity",
    "recognization",
    "find",
    "entity",
    "know",
    "like",
    "entity",
    "person",
    "yoda",
    "entity",
    "person",
    "whole",
    "goal",
    "ner",
    "represent",
    "ones",
    "zero",
    "word",
    "person",
    "name",
    "would",
    "mark",
    "one",
    "person",
    "name",
    "would",
    "mark",
    "zero",
    "let",
    "see",
    "rnn",
    "works",
    "rnn",
    "also",
    "called",
    "recurrent",
    "neural",
    "network",
    "first",
    "convert",
    "dhaval",
    "vector",
    "matter",
    "convert",
    "216",
    "take",
    "vocabulary",
    "use",
    "one",
    "hot",
    "encoding",
    "ways",
    "vectorizing",
    "word",
    "layer",
    "neurons",
    "individual",
    "neurons",
    "let",
    "say",
    "one",
    "layer",
    "hidden",
    "layer",
    "supply",
    "get",
    "one",
    "output",
    "okay",
    "neuron",
    "know",
    "sigma",
    "function",
    "activation",
    "function",
    "processing",
    "statement",
    "loves",
    "baby",
    "yoda",
    "process",
    "word",
    "word",
    "supply",
    "get",
    "output",
    "go",
    "back",
    "supply",
    "converted",
    "vector",
    "previous",
    "output",
    "got",
    "dhaval",
    "supply",
    "input",
    "layer",
    "see",
    "input",
    "layer",
    "next",
    "word",
    "previous",
    "output",
    "language",
    "makes",
    "sense",
    "language",
    "needs",
    "carry",
    "context",
    "word",
    "loss",
    "dhaval",
    "front",
    "might",
    "mean",
    "different",
    "thing",
    "context",
    "need",
    "kind",
    "architecture",
    "provides",
    "context",
    "memory",
    "third",
    "word",
    "supply",
    "network",
    "right",
    "network",
    "one",
    "layer",
    "one",
    "layer",
    "input",
    "layer",
    "output",
    "layer",
    "hidden",
    "layer",
    "one",
    "bunch",
    "neurons",
    "repeatedly",
    "processing",
    "word",
    "one",
    "one",
    "okay",
    "keep",
    "benefit",
    "processing",
    "get",
    "loves",
    "loves",
    "carries",
    "state",
    "previous",
    "state",
    "previous",
    "memory",
    "loves",
    "know",
    "whole",
    "statement",
    "presenting",
    "different",
    "way",
    "make",
    "sure",
    "four",
    "different",
    "hidden",
    "layers",
    "time",
    "travel",
    "okay",
    "actual",
    "hidden",
    "layer",
    "one",
    "time",
    "travel",
    "first",
    "supplied",
    "word",
    "got",
    "output",
    "output",
    "nothing",
    "activation",
    "function",
    "denoting",
    "a1",
    "need",
    "previous",
    "activation",
    "a0",
    "well",
    "let",
    "say",
    "vector",
    "zeros",
    "supply",
    "second",
    "word",
    "use",
    "previous",
    "output",
    "ydhaval",
    "ydhaval",
    "a1",
    "get",
    "another",
    "output",
    "a2",
    "supply",
    "along",
    "third",
    "word",
    "network",
    "four",
    "neurons",
    "single",
    "layer",
    "showing",
    "status",
    "different",
    "times",
    "okay",
    "clear",
    "four",
    "different",
    "layers",
    "one",
    "layer",
    "showing",
    "different",
    "time",
    "steps",
    "showing",
    "almost",
    "time",
    "travel",
    "network",
    "trained",
    "course",
    "output",
    "like",
    "one",
    "zero",
    "one",
    "okay",
    "get",
    "ner",
    "output",
    "individually",
    "one",
    "way",
    "representing",
    "network",
    "okay",
    "avoid",
    "confusion",
    "make",
    "presentation",
    "little",
    "clear",
    "many",
    "times",
    "literature",
    "see",
    "presentation",
    "like",
    "word",
    "input",
    "comes",
    "bottom",
    "activation",
    "two",
    "diagrams",
    "exactly",
    "okay",
    "putting",
    "word",
    "bottom",
    "generic",
    "representation",
    "rnn",
    "real",
    "representation",
    "one",
    "layer",
    "kind",
    "almost",
    "loop",
    "supplying",
    "output",
    "previous",
    "word",
    "input",
    "second",
    "word",
    "let",
    "talk",
    "training",
    "problem",
    "talking",
    "ner",
    "training",
    "samples",
    "okay",
    "x",
    "x",
    "statement",
    "whether",
    "given",
    "word",
    "person",
    "name",
    "processing",
    "first",
    "training",
    "sample",
    "loves",
    "baby",
    "yoda",
    "one",
    "first",
    "initialize",
    "neural",
    "network",
    "weights",
    "random",
    "values",
    "supply",
    "word",
    "calculate",
    "hat",
    "predicted",
    "compare",
    "real",
    "real",
    "1",
    "0",
    "1",
    "1",
    "compare",
    "1",
    "0",
    "1",
    "1",
    "compare",
    "hat",
    "find",
    "loss",
    "okay",
    "sum",
    "loss",
    "total",
    "loss",
    "know",
    "grade",
    "descent",
    "right",
    "compute",
    "loss",
    "back",
    "propagate",
    "loss",
    "adjust",
    "weights",
    "take",
    "second",
    "statement",
    "man",
    "punched",
    "hulk",
    "face",
    "angry",
    "hulk",
    "calculate",
    "losses",
    "find",
    "total",
    "loss",
    "grid",
    "decent",
    "reduce",
    "loss",
    "keep",
    "training",
    "samples",
    "let",
    "100",
    "training",
    "samples",
    "passing",
    "hundred",
    "training",
    "samples",
    "network",
    "one",
    "epoch",
    "might",
    "let",
    "say",
    "20",
    "epochs",
    "end",
    "20",
    "epoch",
    "loss",
    "might",
    "become",
    "minimum",
    "point",
    "say",
    "neural",
    "network",
    "trained",
    "let",
    "take",
    "look",
    "language",
    "translation",
    "language",
    "translation",
    "happens",
    "supply",
    "first",
    "word",
    "network",
    "get",
    "output",
    "network",
    "supply",
    "second",
    "word",
    "output",
    "previous",
    "step",
    "input",
    "course",
    "supply",
    "first",
    "pass",
    "activation",
    "values",
    "let",
    "say",
    "vector",
    "zeros",
    "supply",
    "third",
    "word",
    "fourth",
    "word",
    "done",
    "words",
    "network",
    "starts",
    "translate",
    "translate",
    "one",
    "word",
    "one",
    "statement",
    "push",
    "maybe",
    "one",
    "word",
    "totally",
    "change",
    "translation",
    "language",
    "translation",
    "supply",
    "words",
    "network",
    "translate",
    "network",
    "translate",
    "like",
    "first",
    "part",
    "called",
    "encoder",
    "second",
    "part",
    "called",
    "decoder",
    "go",
    "depth",
    "want",
    "quickly",
    "demonstrate",
    "neural",
    "network",
    "looks",
    "case",
    "language",
    "translation",
    "layer",
    "single",
    "layer",
    "deep",
    "rnn",
    "well",
    "actual",
    "network",
    "might",
    "multiple",
    "hidden",
    "layers",
    "okay",
    "hope",
    "clarifies",
    "architecture",
    "behind",
    "rnn",
    "understand",
    "ca",
    "use",
    "simple",
    "neural",
    "network",
    "use",
    "specialized",
    "neural",
    "network",
    "called",
    "rnn",
    "memorize",
    "remember",
    "previous",
    "state",
    "language",
    "sequence",
    "change",
    "sequence",
    "meaning",
    "changes",
    "like",
    "video",
    "please",
    "give",
    "thumbs",
    "recurrent",
    "neural",
    "network",
    "nlp",
    "type",
    "tutorials",
    "future",
    "videos",
    "thank"
  ],
  "keywords": [
    "deep",
    "artificial",
    "neural",
    "network",
    "mainly",
    "used",
    "processing",
    "talk",
    "recurrent",
    "language",
    "think",
    "nlp",
    "use",
    "cases",
    "well",
    "different",
    "rnn",
    "sequence",
    "must",
    "google",
    "type",
    "sentence",
    "auto",
    "see",
    "time",
    "say",
    "let",
    "know",
    "changes",
    "also",
    "another",
    "case",
    "translation",
    "translate",
    "one",
    "third",
    "named",
    "entity",
    "x",
    "statement",
    "person",
    "name",
    "using",
    "whether",
    "two",
    "would",
    "simple",
    "problem",
    "problems",
    "called",
    "important",
    "make",
    "right",
    "kind",
    "input",
    "english",
    "output",
    "hindi",
    "size",
    "might",
    "architecture",
    "work",
    "decide",
    "many",
    "neurons",
    "layer",
    "becomes",
    "like",
    "okay",
    "remaining",
    "neuron",
    "0",
    "second",
    "convert",
    "word",
    "vector",
    "words",
    "hot",
    "encoding",
    "position",
    "1",
    "need",
    "sunday",
    "ate",
    "golgappa",
    "weights",
    "edges",
    "meaning",
    "part",
    "transaction",
    "change",
    "supply",
    "first",
    "loves",
    "baby",
    "yoda",
    "got",
    "dhaval",
    "whole",
    "find",
    "ner",
    "zero",
    "take",
    "hidden",
    "get",
    "function",
    "activation",
    "previous",
    "context",
    "loss",
    "state",
    "four",
    "layers",
    "travel",
    "showing",
    "real",
    "training",
    "samples",
    "compare"
  ]
}