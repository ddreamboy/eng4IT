{
  "text": "Long short-term\nmemories. I've got them both and so does this network. Hooray! StatQuest!\nHello, i'm Josh Starmer and welcome to StatQuest. Today we're going to talk about\na Long Short-Term Memory, LSTM, and it's going to be clearly explained! Lightning, yeah!\nGonna deploy your models in just a few days not months. Yeah! This StatQuest is also\nbrought to you by the letters A, B and C. A always. B be. C curious. Always be curious.\nNote: This StatQuest assumes you are already familiar with recurrent neural networks\nand the vanishing exploding gradient problem. If not, check out the Quest. Also note:\nAlthough Long Short-Term Memory totally awesome. It is also a stepping stone to learning\nabout Transformers which we will talk about in future StatQuests. In other words,\ntoday we're taking the second step in our Quest. Now, in the StatQuest on basic,\nvanilla recurrent neural networks we saw how we can use a feedback loop to unroll\na network that works well with different amounts of sequential data. However, we\nalso saw that when we plug in the numbers, when the weight on the feedback loop is\ngreater than one, and in this example the weight is 2, then when we do the math we\nend up multiplying the input by the weight, which in this case is 2, raised to the\nnumber of times we unrolled the network. And thus if we had 50 sequential data points,\nlike 50 days of stock market data, which isn't really that much, then we would raise 2 by 50. And 2 to the 50th power is a huge\nnumber. And this  huge number would cause the gradient, which we need for gradient\ndescent, to explode. Kaboom! Alternatively, we saw that if the weight on the feedback\nloop was less than 1, and now we have it set to 0.5, then we'll end up multiplying\nthe input value by 0.5 raised to the 50th power. And 0.5 raised to the 50th power\nis a number super close to zero. And this number super close to 0 would cause the\ngradient, which we need for gradient descent to vanish. Poof!\nIn summary, basic vanilla recurrent neural networks are hard to train because the gradients can explode. Kaboom!\nOr vanish. Poof!\nThe good news is that it doesn't take much to extend the basic vanilla recurrent neural\nnetwork so that we can avoid this problem. So today we're to talk about Long Short-Term\nMemory,  LSTM, which is a type of recurrent neural network that is designed to avoid\nthe exploding / vanishing gradient problem.\nHooray!\nThe main idea behind how Long Short-Term Memory works is that instead of using the\nsame feedback loop connection for events that happened long ago and events that just\nhappened yesterday to make a prediction about tomorrow, Long Short-Term Memory uses\ntwo separate paths to make predictions about tomorrow. One path is for long-term\nmemories, and one is for short-term memories.\nBam! Now that we understand the main idea behind Long Short-Term Memory, that it uses\ndifferent paths for long and short-term memories, let's talk about the details.\nThe bad news is that compared to a basic vanilla recurrent neural network, which unrolls\nfrom a relatively simple unit, Long Short-Term Memory is based on a much more complicated unit.\nHoly smokes, this looks really complicated!\nDon't worry Squatch, we will go through this one step at a time so that you can easily understand each part.\nBam!\nNote: Unlike the network's we've used before in this series, Long Short-Term Memory uses sigmoid activation functions and tan-h\nactivation functions. So let's quickly talk about sigmoid and tan-h activation functions.\nIn a nutshell, the sigmoid activation function takes any x-axis coordinate and turns it into a y-axis\ncoordinate between 0 and 1. For example, when we plug in this x-axis coordinate, 10, into the equation for the sigmoid activation function, we get 0.99995 as the y-axis\ncoordinate. And if we plug in this x-axis coordinate, -5, then we get 0.01 as the y-axis coordinate.\nIn contrast, the tan-h, or hyperbolic tangent activation function takes any x-axis coordinate and turns it into a y-axis\ncoordinate between -1 and 1. For example, if we plug this x-axis coordinate, 2, into the equation for the tan-h activation function, we get 0.96 as the y-axis\ncoordinate. And if we plug in this x-axis coordinate, -5, we get -1 as the y-axis\ncoordinate. So, now that we know that the sigmoid activation function turns any input into a number between 0 and 1 and the tan-h\nactivation function turns any input into a number between -1 and 1, let's talk about\nhow the Long Short-Term Memory unit works.\nFirst, this green line that runs all the way across the top of the unit is called\nthe cell state and represents the long-term memory. Although the long-term memory\ncan be modified by this multiplication and then later by this addition, you'll  notice\nthat there are no weights and biases that can modify it directly. This lack of weights\nallows the long-term memories to flow through a series of unrolled units without\ncausing the gradient to explode or vanish. Now, this pink line, called the hidden\nstate, represents the short-term memories. And as we can see, the short-term memories\nare directly connected to weights that can modify them. To understand how the long\nand short-term memories interact and result in predictions, let's run some numbers\nthrough this unit. First, for the sake of making the math interesting, let's just\nassume that the previous long-term memory is 2, and the previous short-term memory\nis 1,  and let's set the input value to 1. Now that we have plugged in some numbers,\nlet's do the math to see what happens in the first stage of a Long Short-Term Memory\nUnit. We'll start with the short-term memory, 1, times its weight, 2.7. Then we multiply\nthe input, 1, by its weight, 1.63. And then we add those two terms together. And, lastly, we add this bias, 1.62, to get 5.95, an x-axis\ncoordinate for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation and function and we get the y-axis\ncoordinate 0.997. Lastly, we multiply the long-term memory, 2, by the y-axis coordinate,\n0.997, and the result is 1.99. So this first stage of the Long Short-Term Memory\nunit reduced the long-term memory by a little bit.\nIn  contrast, if the input to the LSTM was a relatively large negative number, like -10,\nthen, after calculating the x-axis coordinate, the output from the sigmoid activation\nfunction will be 0. And that means the long-term memory would be completely forgotten\nbecause anything multiplied by 0 is 0. Thus, because the  sigmoid activation function\nturns any input into a number between 0 and 1, the  output determines what percentage\nof the long-term memory is remembered. To summarize, the first stage in a Long Short-Term\nMemory unit determines what percentage of the long-term memory is remembered.\nBam!\nOh no, it's the dreaded terminology alert. Even though  this part of the Long Short-Term\nMemory unit determines what percentage of the long-term memory will be remembered, it is usually called the Forget Gate.\nSmall bam.\nNow that we know with the first part of the LSTM unit does, it determines what percentage\nof the long-term memory will be remembered, let's go back to when the input was 1\nand talk about what the second stage does. In a nutshell, the block on the right\ncombines the short-term memory and the input to create a potential long-term memory.\nAnd the block on the left determines what percentage of that potential memory to add\nto the long-term memory. So let's plug the numbers in and do the math to see how\na potential memory is created and how much of it is added to the the long-term memory.\nStarting with the block furthest to the right, we multiply the short term memory\nand the input by their respective weights. Then we add those values together and add a bias term to get 2.03, the input value for the tan-h\nactivation function. Now we plug 2.03 into the equation for the tan-h activation function and we get the y-axis coordinate,\n0.97.\nRemember the tan-h activation function turns any input into a number between -1 and 1.\nAnd in this case, when the input to the LSTM is 1, then after calculating the x-axis coordinate, the tan-h\nactivation function gives us an output close to 1. In contrast, if the input to the LSTM was -10, then after calculating the x-axis coordinate, the output from the tan-h activation function would be -1.\nGoing back to when the input to the LSTM was 1, we have a potential memory, 0.97,\nbased on the short-term memory and the input. Now the LSTM has to decide how much\nof this potential memory to save. And this is done using the exact same method we\nused earlier when we determined what percentage of the long-term memory to remember. In other words, after multiplying\nthe short-term memory and the input  by weights and adding those products together and adding a bias, we get 4.27, the x-axis\ninput value for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation function and we get the y-axis\ncoordinate 1.0. And that means the entire potential long-term memory is retained,\nbecause multiplying it by 1 doesn't change it. Note:\nIf the original input value was -10, then the percentage of the potential memory to\nremember would be 0, so we would not add anything to the long-term memory. Now, going\nback to when the original input value was 1, we add 0.97 to the existing long-term\nmemory and we get a new long-term memory, 2.96. Double bam! Oh no, it's the dreaded\nterminology alert! Even though this part of the Long Short-Term Memory unit determines\nhow we should update the long-term memory, it's usually called the input gate. Tiny bam.\nNow that we have a new long-term memory, we're ready to talk about the final stage\nin the LSTM. This final stage updates the short-term memory. We start with the new long-term memory and use it as input to the tan-h\nactivation function. After plugging 2.96 into the tan-h activation function, we get 0.99.\n0.99 represents a potential short-term memory. Now, the LSTM has to decide how much\nof this potential short-term memory to pass on. And this is done using the exact\nsame method we used two times earlier: When we determined, what percentage of the\noriginal long-term memory to remember and when we determined what percentage of the\npotential long-term memory to remember. In all three cases, we use a sigmoid activation\nfunction to determine what percent the LSTM remembers. In this case when we do the\nmath, we get 0.99. And we create the new short-term memory by multiplying 0.99 by\n0.99 to get 0.98. This new short-term memory, 0.98, is also the output from this\nentire LSTM unit. Oh no, it's the dreaded  terminology alert again.\nBecause the new short-term memory is the output from this entire  LSTM unit, this stage is called the  Output gate.\nAnd at long last, the common terminology seems reasonable to me. Triple bam! Now that\nwe understand how all three stages in a single LSTM unit work, let's see them in\naction with real data. Here we have stock prices for two companies Company A and Company B.\nOn the  y-axis we have the stock value and on the x-axis we have the day the value was recorded. Note:\nIf we overlap the data from the two companies, we see that the only differences occur\non day 1 and on day 5. On day 1, Company A is at 0 and Company B is at 1. And on\nday five, Company A returns to 0 and Company B returns to 1. On  all of the other\ndays, days 2, 3 and 4, both companies have the exact same values. Given this sequential\ndata, we want the LSTM to remember what happened on day 1 so it can correctly predict\nwhat will happen on day 5. In other words, we're going to sequentially run the data\nfrom days 1 through 4 through an unrolled LSTM and see if it can correctly predict\nthe values for day 5 for both companies. So let's go back to the LSTM and initialize\nthe long and short-term memories to 0. Now, because this single LSTM unit is taking\nup the whole screen, let's shrink it down to this smaller simplified diagram. Now,\nif we want to sequentially run Company A's values from days 1 through 4 through this LSTM,\nthen we'll start by plugging the value for day 1, which is 0, into the  Input.\nNow, just like before we do the math.\nBoop be doop boop boop boop boop boop.\nAnd after doing the math, we see that the new or updated long-term memory is -0.20\nand the new updated short-term memory is -0.13. So we plug in -0.2 for the updated\nlong-term memory and -0.1, rounded, for the updated short-term memory. Now we unroll\nthe LSTM using the updated memories and plug the  from day 2, 0.5, into the input.\nThen the LSTM does its math using the exact same weights and biases as before, and\nwe end up with these updated long and short-term memories.\nNote: If you can't remember the StatQuest on recurrent neural networks very well,\nthe reason the LSTM reuses the exact same weights and biases is so that it can handle\ndata sequences of different lengths. Small bam.\nAnyway, we unroll the LSTM again and plug in the value for day 3. Then the LSTM does\nthe math again using the exact same weights and biases and gives us these updated\nmemories. Then we unroll the LSTM one last time and plug in the value for day 4.\nAnd the LSTM does the math again using the exact same weights and biases and gives\nus the final memories. And the final short-term memory, 0.0, is the output from the\nunrolled LSTM. And that means the  Output from the LSTM correctly predicts Company A's value for day 5.\nBam!\nNow that we have shown that the LSTM can correctly predict the value on day 5 for\nCompany A, let's show how the same LSTM, with the same weights and biases can correctly\npredict the value on day 5 for Company B.\nNote:  Remember, on days 1 through 4, the only difference between the companies occurs\non day 1, and that means the LSTM has to remember what happened on day 1 in order\nto correctly predict the different output values on day 5. So let's start by initializing\nthe long and short-term memories to 0. Now, let's plug in the value for day one from Company B,\n1.\nAnd the LSTM does the math, just like before using the exact same weights and biases.\nBeep boop.\nAfter doing the math, we see that the updated long-term memory is 0.5 and the updated\nshort-term memory is 0.28. So we plug in 0.5 for the updated long-term memory and\n0.3, rounded, for the updated short-term memory. Now we unroll the LSTM and do the math with the remaining input values.\nAnd the final short-term memory, 1.0, is the output from the unrolled LSTM. And that\nmeans the output from the LSTM correctly predicts Company B's value for day 5. Double\nbam! In summary, using separate paths for long-term memories and short-term memories,\nLong Short-Term Memory networks avoid the exploding/vanishing gradient problem, and\nthat means we can unroll them more times to accommodate longer sequences of input\ndata than a vanilla recurrent neural network.\nAt first, i was scared of how complicated the LSTM was, but now I understand. Triple Bam!\nNow it's time for  some Shameless Self-Promotion. If you want to review statistics\nand machine learning offline, check out the StatQuest PDF study guides and my book\nthe StatQuest Illustrated Guide to Machine Learning at statquest.org. There's something\nfor everyone! Hooray! We've made it to the end of another exciting StatQuest. If\nyou liked this StatQuest and want to see more, please subscribe. And if you want\nto support StatQuest, consider contributing to my Patreon campaign becoming a channel\nmember, buying one or two of my original songs or a t-shirt or a hoodie or just donate,\nthe links are in the description below. Alright, until next time Quest on!\n",
  "words": [
    "long",
    "memories",
    "got",
    "network",
    "hooray",
    "statquest",
    "hello",
    "josh",
    "starmer",
    "welcome",
    "statquest",
    "today",
    "going",
    "talk",
    "long",
    "memory",
    "lstm",
    "going",
    "clearly",
    "explained",
    "lightning",
    "yeah",
    "gon",
    "na",
    "deploy",
    "models",
    "days",
    "months",
    "yeah",
    "statquest",
    "also",
    "brought",
    "letters",
    "b",
    "always",
    "b",
    "c",
    "curious",
    "always",
    "curious",
    "note",
    "statquest",
    "assumes",
    "already",
    "familiar",
    "recurrent",
    "neural",
    "networks",
    "vanishing",
    "exploding",
    "gradient",
    "problem",
    "check",
    "quest",
    "also",
    "note",
    "although",
    "long",
    "memory",
    "totally",
    "awesome",
    "also",
    "stepping",
    "stone",
    "learning",
    "transformers",
    "talk",
    "future",
    "statquests",
    "words",
    "today",
    "taking",
    "second",
    "step",
    "quest",
    "statquest",
    "basic",
    "vanilla",
    "recurrent",
    "neural",
    "networks",
    "saw",
    "use",
    "feedback",
    "loop",
    "unroll",
    "network",
    "works",
    "well",
    "different",
    "amounts",
    "sequential",
    "data",
    "however",
    "also",
    "saw",
    "plug",
    "numbers",
    "weight",
    "feedback",
    "loop",
    "greater",
    "one",
    "example",
    "weight",
    "2",
    "math",
    "end",
    "multiplying",
    "input",
    "weight",
    "case",
    "2",
    "raised",
    "number",
    "times",
    "unrolled",
    "network",
    "thus",
    "50",
    "sequential",
    "data",
    "points",
    "like",
    "50",
    "days",
    "stock",
    "market",
    "data",
    "really",
    "much",
    "would",
    "raise",
    "2",
    "2",
    "50th",
    "power",
    "huge",
    "number",
    "huge",
    "number",
    "would",
    "cause",
    "gradient",
    "need",
    "gradient",
    "descent",
    "explode",
    "kaboom",
    "alternatively",
    "saw",
    "weight",
    "feedback",
    "loop",
    "less",
    "1",
    "set",
    "end",
    "multiplying",
    "input",
    "value",
    "raised",
    "50th",
    "power",
    "raised",
    "50th",
    "power",
    "number",
    "super",
    "close",
    "zero",
    "number",
    "super",
    "close",
    "0",
    "would",
    "cause",
    "gradient",
    "need",
    "gradient",
    "descent",
    "vanish",
    "poof",
    "summary",
    "basic",
    "vanilla",
    "recurrent",
    "neural",
    "networks",
    "hard",
    "train",
    "gradients",
    "explode",
    "kaboom",
    "vanish",
    "poof",
    "good",
    "news",
    "take",
    "much",
    "extend",
    "basic",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "avoid",
    "problem",
    "today",
    "talk",
    "long",
    "memory",
    "lstm",
    "type",
    "recurrent",
    "neural",
    "network",
    "designed",
    "avoid",
    "exploding",
    "vanishing",
    "gradient",
    "problem",
    "hooray",
    "main",
    "idea",
    "behind",
    "long",
    "memory",
    "works",
    "instead",
    "using",
    "feedback",
    "loop",
    "connection",
    "events",
    "happened",
    "long",
    "ago",
    "events",
    "happened",
    "yesterday",
    "make",
    "prediction",
    "tomorrow",
    "long",
    "memory",
    "uses",
    "two",
    "separate",
    "paths",
    "make",
    "predictions",
    "tomorrow",
    "one",
    "path",
    "memories",
    "one",
    "memories",
    "bam",
    "understand",
    "main",
    "idea",
    "behind",
    "long",
    "memory",
    "uses",
    "different",
    "paths",
    "long",
    "memories",
    "let",
    "talk",
    "details",
    "bad",
    "news",
    "compared",
    "basic",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "unrolls",
    "relatively",
    "simple",
    "unit",
    "long",
    "memory",
    "based",
    "much",
    "complicated",
    "unit",
    "holy",
    "smokes",
    "looks",
    "really",
    "complicated",
    "worry",
    "squatch",
    "go",
    "one",
    "step",
    "time",
    "easily",
    "understand",
    "part",
    "bam",
    "note",
    "unlike",
    "network",
    "used",
    "series",
    "long",
    "memory",
    "uses",
    "sigmoid",
    "activation",
    "functions",
    "activation",
    "functions",
    "let",
    "quickly",
    "talk",
    "sigmoid",
    "activation",
    "functions",
    "nutshell",
    "sigmoid",
    "activation",
    "function",
    "takes",
    "coordinate",
    "turns",
    "coordinate",
    "0",
    "example",
    "plug",
    "coordinate",
    "10",
    "equation",
    "sigmoid",
    "activation",
    "function",
    "get",
    "coordinate",
    "plug",
    "coordinate",
    "get",
    "coordinate",
    "contrast",
    "hyperbolic",
    "tangent",
    "activation",
    "function",
    "takes",
    "coordinate",
    "turns",
    "coordinate",
    "example",
    "plug",
    "coordinate",
    "2",
    "equation",
    "activation",
    "function",
    "get",
    "coordinate",
    "plug",
    "coordinate",
    "get",
    "coordinate",
    "know",
    "sigmoid",
    "activation",
    "function",
    "turns",
    "input",
    "number",
    "0",
    "1",
    "activation",
    "function",
    "turns",
    "input",
    "number",
    "1",
    "let",
    "talk",
    "long",
    "memory",
    "unit",
    "works",
    "first",
    "green",
    "line",
    "runs",
    "way",
    "across",
    "top",
    "unit",
    "called",
    "cell",
    "state",
    "represents",
    "memory",
    "although",
    "memory",
    "modified",
    "multiplication",
    "later",
    "addition",
    "notice",
    "weights",
    "biases",
    "modify",
    "directly",
    "lack",
    "weights",
    "allows",
    "memories",
    "flow",
    "series",
    "unrolled",
    "units",
    "without",
    "causing",
    "gradient",
    "explode",
    "vanish",
    "pink",
    "line",
    "called",
    "hidden",
    "state",
    "represents",
    "memories",
    "see",
    "memories",
    "directly",
    "connected",
    "weights",
    "modify",
    "understand",
    "long",
    "memories",
    "interact",
    "result",
    "predictions",
    "let",
    "run",
    "numbers",
    "unit",
    "first",
    "sake",
    "making",
    "math",
    "interesting",
    "let",
    "assume",
    "previous",
    "memory",
    "2",
    "previous",
    "memory",
    "1",
    "let",
    "set",
    "input",
    "value",
    "plugged",
    "numbers",
    "let",
    "math",
    "see",
    "happens",
    "first",
    "stage",
    "long",
    "memory",
    "unit",
    "start",
    "memory",
    "1",
    "times",
    "weight",
    "multiply",
    "input",
    "1",
    "weight",
    "add",
    "two",
    "terms",
    "together",
    "lastly",
    "add",
    "bias",
    "get",
    "coordinate",
    "sigmoid",
    "activation",
    "function",
    "plug",
    "coordinate",
    "equation",
    "sigmoid",
    "activation",
    "function",
    "get",
    "coordinate",
    "lastly",
    "multiply",
    "memory",
    "2",
    "coordinate",
    "result",
    "first",
    "stage",
    "long",
    "memory",
    "unit",
    "reduced",
    "memory",
    "little",
    "bit",
    "contrast",
    "input",
    "lstm",
    "relatively",
    "large",
    "negative",
    "number",
    "like",
    "calculating",
    "coordinate",
    "output",
    "sigmoid",
    "activation",
    "function",
    "means",
    "memory",
    "would",
    "completely",
    "forgotten",
    "anything",
    "multiplied",
    "0",
    "thus",
    "sigmoid",
    "activation",
    "function",
    "turns",
    "input",
    "number",
    "0",
    "1",
    "output",
    "determines",
    "percentage",
    "memory",
    "remembered",
    "summarize",
    "first",
    "stage",
    "long",
    "memory",
    "unit",
    "determines",
    "percentage",
    "memory",
    "remembered",
    "bam",
    "oh",
    "dreaded",
    "terminology",
    "alert",
    "even",
    "though",
    "part",
    "long",
    "memory",
    "unit",
    "determines",
    "percentage",
    "memory",
    "remembered",
    "usually",
    "called",
    "forget",
    "gate",
    "small",
    "bam",
    "know",
    "first",
    "part",
    "lstm",
    "unit",
    "determines",
    "percentage",
    "memory",
    "remembered",
    "let",
    "go",
    "back",
    "input",
    "1",
    "talk",
    "second",
    "stage",
    "nutshell",
    "block",
    "right",
    "combines",
    "memory",
    "input",
    "create",
    "potential",
    "memory",
    "block",
    "left",
    "determines",
    "percentage",
    "potential",
    "memory",
    "add",
    "memory",
    "let",
    "plug",
    "numbers",
    "math",
    "see",
    "potential",
    "memory",
    "created",
    "much",
    "added",
    "memory",
    "starting",
    "block",
    "furthest",
    "right",
    "multiply",
    "short",
    "term",
    "memory",
    "input",
    "respective",
    "weights",
    "add",
    "values",
    "together",
    "add",
    "bias",
    "term",
    "get",
    "input",
    "value",
    "activation",
    "function",
    "plug",
    "equation",
    "activation",
    "function",
    "get",
    "coordinate",
    "remember",
    "activation",
    "function",
    "turns",
    "input",
    "number",
    "case",
    "input",
    "lstm",
    "1",
    "calculating",
    "coordinate",
    "activation",
    "function",
    "gives",
    "us",
    "output",
    "close",
    "contrast",
    "input",
    "lstm",
    "calculating",
    "coordinate",
    "output",
    "activation",
    "function",
    "would",
    "going",
    "back",
    "input",
    "lstm",
    "1",
    "potential",
    "memory",
    "based",
    "memory",
    "input",
    "lstm",
    "decide",
    "much",
    "potential",
    "memory",
    "save",
    "done",
    "using",
    "exact",
    "method",
    "used",
    "earlier",
    "determined",
    "percentage",
    "memory",
    "remember",
    "words",
    "multiplying",
    "memory",
    "input",
    "weights",
    "adding",
    "products",
    "together",
    "adding",
    "bias",
    "get",
    "input",
    "value",
    "sigmoid",
    "activation",
    "function",
    "plug",
    "coordinate",
    "equation",
    "sigmoid",
    "activation",
    "function",
    "get",
    "coordinate",
    "means",
    "entire",
    "potential",
    "memory",
    "retained",
    "multiplying",
    "1",
    "change",
    "note",
    "original",
    "input",
    "value",
    "percentage",
    "potential",
    "memory",
    "remember",
    "would",
    "0",
    "would",
    "add",
    "anything",
    "memory",
    "going",
    "back",
    "original",
    "input",
    "value",
    "1",
    "add",
    "existing",
    "memory",
    "get",
    "new",
    "memory",
    "double",
    "bam",
    "oh",
    "dreaded",
    "terminology",
    "alert",
    "even",
    "though",
    "part",
    "long",
    "memory",
    "unit",
    "determines",
    "update",
    "memory",
    "usually",
    "called",
    "input",
    "gate",
    "tiny",
    "bam",
    "new",
    "memory",
    "ready",
    "talk",
    "final",
    "stage",
    "lstm",
    "final",
    "stage",
    "updates",
    "memory",
    "start",
    "new",
    "memory",
    "use",
    "input",
    "activation",
    "function",
    "plugging",
    "activation",
    "function",
    "get",
    "represents",
    "potential",
    "memory",
    "lstm",
    "decide",
    "much",
    "potential",
    "memory",
    "pass",
    "done",
    "using",
    "exact",
    "method",
    "used",
    "two",
    "times",
    "earlier",
    "determined",
    "percentage",
    "original",
    "memory",
    "remember",
    "determined",
    "percentage",
    "potential",
    "memory",
    "remember",
    "three",
    "cases",
    "use",
    "sigmoid",
    "activation",
    "function",
    "determine",
    "percent",
    "lstm",
    "remembers",
    "case",
    "math",
    "get",
    "create",
    "new",
    "memory",
    "multiplying",
    "get",
    "new",
    "memory",
    "also",
    "output",
    "entire",
    "lstm",
    "unit",
    "oh",
    "dreaded",
    "terminology",
    "alert",
    "new",
    "memory",
    "output",
    "entire",
    "lstm",
    "unit",
    "stage",
    "called",
    "output",
    "gate",
    "long",
    "last",
    "common",
    "terminology",
    "seems",
    "reasonable",
    "triple",
    "bam",
    "understand",
    "three",
    "stages",
    "single",
    "lstm",
    "unit",
    "work",
    "let",
    "see",
    "action",
    "real",
    "data",
    "stock",
    "prices",
    "two",
    "companies",
    "company",
    "company",
    "stock",
    "value",
    "day",
    "value",
    "recorded",
    "note",
    "overlap",
    "data",
    "two",
    "companies",
    "see",
    "differences",
    "occur",
    "day",
    "1",
    "day",
    "day",
    "1",
    "company",
    "0",
    "company",
    "b",
    "day",
    "five",
    "company",
    "returns",
    "0",
    "company",
    "b",
    "returns",
    "days",
    "days",
    "2",
    "3",
    "4",
    "companies",
    "exact",
    "values",
    "given",
    "sequential",
    "data",
    "want",
    "lstm",
    "remember",
    "happened",
    "day",
    "1",
    "correctly",
    "predict",
    "happen",
    "day",
    "words",
    "going",
    "sequentially",
    "run",
    "data",
    "days",
    "1",
    "4",
    "unrolled",
    "lstm",
    "see",
    "correctly",
    "predict",
    "values",
    "day",
    "5",
    "companies",
    "let",
    "go",
    "back",
    "lstm",
    "initialize",
    "long",
    "memories",
    "single",
    "lstm",
    "unit",
    "taking",
    "whole",
    "screen",
    "let",
    "shrink",
    "smaller",
    "simplified",
    "diagram",
    "want",
    "sequentially",
    "run",
    "company",
    "values",
    "days",
    "1",
    "4",
    "lstm",
    "start",
    "plugging",
    "value",
    "day",
    "1",
    "0",
    "input",
    "like",
    "math",
    "boop",
    "doop",
    "boop",
    "boop",
    "boop",
    "boop",
    "boop",
    "math",
    "see",
    "new",
    "updated",
    "memory",
    "new",
    "updated",
    "memory",
    "plug",
    "updated",
    "memory",
    "rounded",
    "updated",
    "memory",
    "unroll",
    "lstm",
    "using",
    "updated",
    "memories",
    "plug",
    "day",
    "2",
    "input",
    "lstm",
    "math",
    "using",
    "exact",
    "weights",
    "biases",
    "end",
    "updated",
    "long",
    "memories",
    "note",
    "ca",
    "remember",
    "statquest",
    "recurrent",
    "neural",
    "networks",
    "well",
    "reason",
    "lstm",
    "reuses",
    "exact",
    "weights",
    "biases",
    "handle",
    "data",
    "sequences",
    "different",
    "lengths",
    "small",
    "bam",
    "anyway",
    "unroll",
    "lstm",
    "plug",
    "value",
    "day",
    "lstm",
    "math",
    "using",
    "exact",
    "weights",
    "biases",
    "gives",
    "us",
    "updated",
    "memories",
    "unroll",
    "lstm",
    "one",
    "last",
    "time",
    "plug",
    "value",
    "day",
    "lstm",
    "math",
    "using",
    "exact",
    "weights",
    "biases",
    "gives",
    "us",
    "final",
    "memories",
    "final",
    "memory",
    "output",
    "unrolled",
    "lstm",
    "means",
    "output",
    "lstm",
    "correctly",
    "predicts",
    "company",
    "value",
    "day",
    "bam",
    "shown",
    "lstm",
    "correctly",
    "predict",
    "value",
    "day",
    "5",
    "company",
    "let",
    "show",
    "lstm",
    "weights",
    "biases",
    "correctly",
    "predict",
    "value",
    "day",
    "5",
    "company",
    "note",
    "remember",
    "days",
    "1",
    "4",
    "difference",
    "companies",
    "occurs",
    "day",
    "1",
    "means",
    "lstm",
    "remember",
    "happened",
    "day",
    "1",
    "order",
    "correctly",
    "predict",
    "different",
    "output",
    "values",
    "day",
    "let",
    "start",
    "initializing",
    "long",
    "memories",
    "let",
    "plug",
    "value",
    "day",
    "one",
    "company",
    "b",
    "lstm",
    "math",
    "like",
    "using",
    "exact",
    "weights",
    "biases",
    "beep",
    "boop",
    "math",
    "see",
    "updated",
    "memory",
    "updated",
    "memory",
    "plug",
    "updated",
    "memory",
    "rounded",
    "updated",
    "memory",
    "unroll",
    "lstm",
    "math",
    "remaining",
    "input",
    "values",
    "final",
    "memory",
    "output",
    "unrolled",
    "lstm",
    "means",
    "output",
    "lstm",
    "correctly",
    "predicts",
    "company",
    "b",
    "value",
    "day",
    "double",
    "bam",
    "summary",
    "using",
    "separate",
    "paths",
    "memories",
    "memories",
    "long",
    "memory",
    "networks",
    "avoid",
    "gradient",
    "problem",
    "means",
    "unroll",
    "times",
    "accommodate",
    "longer",
    "sequences",
    "input",
    "data",
    "vanilla",
    "recurrent",
    "neural",
    "network",
    "first",
    "scared",
    "complicated",
    "lstm",
    "understand",
    "triple",
    "bam",
    "time",
    "shameless",
    "want",
    "review",
    "statistics",
    "machine",
    "learning",
    "offline",
    "check",
    "statquest",
    "pdf",
    "study",
    "guides",
    "book",
    "statquest",
    "illustrated",
    "guide",
    "machine",
    "learning",
    "something",
    "everyone",
    "hooray",
    "made",
    "end",
    "another",
    "exciting",
    "statquest",
    "liked",
    "statquest",
    "want",
    "see",
    "please",
    "subscribe",
    "want",
    "support",
    "statquest",
    "consider",
    "contributing",
    "patreon",
    "campaign",
    "becoming",
    "channel",
    "member",
    "buying",
    "one",
    "two",
    "original",
    "songs",
    "hoodie",
    "donate",
    "links",
    "description",
    "alright",
    "next",
    "time",
    "quest"
  ],
  "keywords": [
    "long",
    "memories",
    "network",
    "statquest",
    "going",
    "talk",
    "memory",
    "lstm",
    "days",
    "also",
    "b",
    "note",
    "recurrent",
    "neural",
    "networks",
    "gradient",
    "problem",
    "basic",
    "vanilla",
    "feedback",
    "loop",
    "unroll",
    "different",
    "data",
    "plug",
    "numbers",
    "weight",
    "one",
    "2",
    "math",
    "end",
    "multiplying",
    "input",
    "number",
    "times",
    "unrolled",
    "like",
    "much",
    "would",
    "1",
    "value",
    "0",
    "using",
    "happened",
    "two",
    "bam",
    "understand",
    "let",
    "unit",
    "time",
    "part",
    "sigmoid",
    "activation",
    "function",
    "coordinate",
    "turns",
    "equation",
    "get",
    "first",
    "called",
    "weights",
    "biases",
    "see",
    "stage",
    "start",
    "add",
    "output",
    "means",
    "determines",
    "percentage",
    "remembered",
    "terminology",
    "back",
    "potential",
    "values",
    "remember",
    "exact",
    "original",
    "new",
    "final",
    "companies",
    "company",
    "day",
    "4",
    "want",
    "correctly",
    "predict",
    "boop",
    "updated"
  ]
}