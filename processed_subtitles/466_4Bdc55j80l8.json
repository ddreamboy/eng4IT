{
  "text": "transformers are taking the natural\nlanguage processing world by storm these\nincredible models are breaking multiple\nNLP records and pushing the\nstate-of-the-art they are used in many\napplications like machine language\ntranslation conversational chat BOTS and\neven a power better search engines\ntransformers are the rage and deep\nlearning nowadays but how do they work\nwhy are they outperformed a previous\nking of sequence problems like recurrent\nneural networks gr use and LS tiens\nyou've probably heard of different\nfamous transformer models like Burt CBT\nand GB t2 in this video we'll focus on\nthe one paper that started it all\nattention is all you need to understand\ntransformers we first must understand\nthe attention mechanism to get an\nintuitive understanding of the attention\nmechanism let's start with a fun text\ngeneration model that's capable of\nwriting its own sci-fi novel we'll need\nto prime in a model with an arbitrary\ninput and a model will generate the rest\nokay\nlet's make the story interesting as\naliens entered our planet and began to\ncolonize earth a certain group of\nextraterrestrials begin to manipulate\nour society through their influence of a\ncertain number of the elite of the\ncountry to keep an iron grip over the\npopulace by the way I then just make\nthis up this was actually generated by\nopen AI is GPT to transformer model\nshout out to hugging face for an awesome\ninterface to play with I'll provide a\nlink in description okay so the model is\na little dark but what's interesting is\nhow it works as a model generate tax\nword by word it has the ability to\nreference or tend to words that is\nrelevant to the generated word how the\nmodel knows which were to attend to is\nall learned while training with\nbackpropagation our intends are also\ncapable of looking at previous inputs\ntoo but the power of the attention\nmechanism is that it doesn't suffer from\nshort-term memory rnns have a shorter\nwindow to reference from so when a story\ngets longer rnns can't access word\ngenerated earlier in the sequence\nthis is still true for gr use and L\nSTM's although they do have a bigger\ncapacity to achieve longer term memory\ntherefore having a longer window to\nreference from the attention mechanism\nin theory and given enough compute\nresources have an infinite window to\nreference from therefore being capable\nof using the entire context of the story\nwhile generating the text this power was\ndemonstrated in the paper attention is\nall you need when the author's introduce\na new novel neural network called the\nTransformers which is an attention based\nencoder decoder type architecture on a\nhigh level the encoder Maps an input\nsequence into an abstract continuous\nrepresentation that holds all the\nlearned information of that input to\ndecoder then takes our continuous\nrepresentation and step by step\ngenerates a single output while also\nbeing fed to previous output let's walk\nthrough an example\nthe attention is all you need paper\napplied to transformer model on a neuro\nmachine translation problem our\ndemonstration of the transformer model\nwould be a conversational chat bot the\nexample with taking an input tax hi how\nare you and generate the response I am\nfine\nlet's break down the mechanics of the\nnetwork step by step the first step is\nfeeding our input into a word embedded\nlayer a word embedding layer can be\nthought of as a lookup table to grab a\nlearn factor of representation of each\nword neural networks learned through\nnumbers so each word maps to a vector\nwith continuous values to represent that\nword\nnext step is to inject positional\ninformation into the embeddings because\na transformer encoder has no recurrence\nlike recurrent known networks we must\nadd information about the positions into\nthe input embeddings\nthis is done using positional encoding\nthe authors came up with a clever trick\nusing sine and cosine functions we won't\ngo into the mathematical details of the\npositional codings in this video but\nhere are the basics for every odd time\nstep create a vector using the cosine\nfunction for every even time step create\na vector using the sine function then\nadd those vectors to their corresponding\nembedding vector this successfully gives\nthe network information on two positions\nof each vector the sine and cosine\nfunctions were chosen in tandem because\nthey have linear properties the model\ncan easily learn to attend to now we\nhave the encoder layer the encoder\nlayers job is to map all input sequence\ninto an abstract continuous\nrepresentation that holds the learned\ninformation for that entire sequence it\ncontains two sub modules multi-headed\nattention followed by a fully connected\nnetwork there are also residual\nconnections around each of the two sub\nmodules followed by a layer\nnormalization to break this down let's\nlook at the multi headed attention\nmodule multi-headed attention Indian\ncode applies a specific attention\nmechanism called self attention self\nattention allows a model to associate\neach individual word in the input to\nother words in the input so in our\nexample it's possible that our model can\nlearn to associate the word you with how\nM are it's also possible that the model\nlearns that word structured in this\npattern are typically a question\nso respond appropriately to achieve self\nattention we feed the input into three\ndistinct fully connected layers to\ncreate the query key and value vectors\nwhat are these vectors exactly I found a\ngood explanation on stock-exchange\nstating the query key and value concept\ncomes from the retrieval system for\nexample when you type a query to search\nfor some video on YouTube\nthe search engine will map your query\nagainst a set of keys for example video\ntitle description etc associated with\ncandidate videos in the database then\npresent you with the best match video\nlet's see how this relates to self\nattention the queries and keys undergoes\na dot product matrix multiplication to\nproduce a score matrix the score matrix\ndetermines how much focus should a word\nbe put on other words so each word will\nhave a score to correspond to other\nwords in the time step the higher score\nthe more the focus this is how queries\nare mapped to keys then the scores get\nscaled down by getting divided by the\nsquare root of the dimension of the\nqueries and the keys this is to allow\nfor more stable gradients as multiplying\nvalues can have exploding effects next\nyou take the softmax the scaled score to\nget the attention weights which gives\nyou probability values between 0 & 1\nby doing the softmax the higher scores\nget heightened and the lower scores are\ndepressed this allows the model to be\nmore confident on which words to attend\nto then you take the attention weights\nand multiply it by your value vector to\nget an output vector the higher softmax\nscores will keep the value of the words\nthe model learn is more important the\nlower scores will drown out their\nirrelevant words you feed the output\nvector into a linear layer to process to\nmake this a multi-headed attention\ncomputation you need to split the query\nkey in value into adding vectors before\napplying self attention to split vectors\nthat goes through the same self\nattention process individually each self\nattention process is called a head each\nhead produces an output vector that gets\nconcatenated into a single vector before\ngo through in a final linear layer in\ntheory each head would learn something\ndifferent therefore giving the encounter\nmodel more representation power okay so\nthat's multi-headed attention to sum it\nup multi-headed attention is a module in\na transformer network that\nyou to the attention waits for the input\nand produces an output vector with\nencoded information on how each word\nshould attend to all other words in a\nsequence\nnext step the multi-headed attention\noutput vector is added to the original\ninput this is called a residual\nconnection the output of the residual\nconnection goes through a layer\nnormalization the normalized residual\noutput gets fed into a point-wise\nfeed-forward network for further\nprocessing the point-wise feed-forward\nnetwork are a couple of linear layers\nwith a relict evasion in between the\noutput of that is again added to the\ninput of the point-wise feed-forward\nnetwork and further normalized the\nresidual connections helps the network\ntrain by allowing gradients to flow\nthrough the networks directly the layer\nnormalizations are used to stabilize the\nnetwork which results in sustained\nproducing the training time necessary\nand a point-wise feed-forward layer are\nused to further process the attention\noutput potentially giving it a richer\nrepresentation\nand that wraps up the encoded layer all\nthese operations is for the purpose of\nencoding the input to a continuous\nrepresentation with attention\ninformation this will help the decoder\nfocus on the appropriate words in the\ninput during the decoding process you\ncan stack the encoder and times to\nfurther encode the information where\neach layer has the opportunity to learn\ndifferent attention representations\ntherefore potentially boosting the\npredictive power of the transformer\nnetwork now we move on to the decoder\nthe decoders job is to generate text\nsequences the decoder has similar sub\nlayers as the encoder it has two\nmulti-headed attention layers a\npoint-wise feed-forward layer with\nresidual connections and layer\nnormalization after each sub layer these\nsub layers behave similarly to layers in\nthe encoder but each multi-headed\nattention layer has a different job it's\ncapped off with a linear layer that acts\nlike a classifier and a soft Max to get\nthe word probabilities the decoder is\nauto regressive it takes in the list of\nprevious outputs as inputs as well as\nthe encoder outputs that contains the\nattention information from the input the\ndecoder stops decoding when it generates\nan end token as an output let's walk\nthrough the decoding steps the input\ngoes through an embedding layer in a\nposition on coding layer to get\npositional embeddings the positional\nembeddings gets fed into the first\nmulti-headed attention layer which\ncomputes the attention score for the\ndecoders input this multi-headed\nattention layer operates slightly\ndifferent since the decoders\nautoregressive and generates the\nsequence word-by-word you need to\nprevent it from condition into future\ntokens for example when computing\nattention scores on the word am you\nshould not have access to the word fine\nbecause our word is a future word that\nwas generated after the word am should\nonly have access to itself and the words\nbefore this is true for all other words\nwhere they can only attend to previous\nwords we need a method to prevent\ncomputing attention scores for future\nwords this method is called masking\nto prevent the decoder from looking at\nfuture tokens you apply a look-ahead\nmask the mask is added before\ncalculating the softmax and after\nscaling the scores let's take a look at\nhow this works the mask is a matrix\nthat's the same size as the attention\nscores filled with values of materials\nand negative infinity x' when you add\nthe mask to the scale attention scores\nyou get a matrix of scores with the top\nright triangle filled with negative\ninfinity x' the reason for this is once\nyou take the softmax of the mask scores\nthe negative infinity is get zeroed out\nleaving a zero attention score for\nfuture tokens as you can see the\nattention scores for M have values for\nitself and all other words before it but\nzero for the word fine this essentially\ntells the model to put no focus on those\nwords this masking is the only\ndifference on how the attention scores\nare calculated in the first multi-headed\nattention layer this layers still have\nmultiple heads that the masks are being\napplied to before getting concatenated\nand fed through a linear layer for\nfurther processing the output of the\nfirst multi-headed attention is a mask\noutput vector with information on how\nthe model should attend on the decoders\ninputs\nnow on to the second multi-headed\nattention layer for this layer the\nencoders output are the queries in the\nkeys in the first multi-headed attention\nlayer outputs are the values this\nprocess matches the encoders input to\nthe decoders input allowing the decoder\nto decide which encoder input is\nrelevant to put focus on the output of\nthe second multi-headed attention goes\nthrough a point wise feed-forward layer\nfor further processing the output of the\nfinal point wise feed-forward layer goes\nthrough a final linear layer that access\na classifier the classifier is as\nbiggest number of classes you have for\nexample if you have 10,000 classes for\n10,000 words the output of that\nclassifier will be of size 10,000 the\noutput of the classifier again gets fed\ninto a soft max layer the soft max layer\nproduced probability scores between 0\nand 1 for each class we take the index\nof the highest probability score and\nthat equals our predicted word the\ndecoder didn't taste the output and adds\nit to the list of decoder inputs and\ncontinue decoding again until end token\nis predicted for our case the highest\nprobability prediction is the final\nclass which is assigned to the end token\nthis is how the decoder generates the\noutput the decoder can be stacked n\nlayers high each layer taking in inputs\nfrom the encoder and the layers before\nit by stacking layers the model can\nlearn to extract and focus on different\ncombinations of attention from its\nattention heads potentially boosting its\npredictive power and that's it that's\nthe mechanics of the transformers\ntransformers leverage the power of the\nattention mechanism to make better\npredictions recur known networks trying\nto achieve similar things but because\nthey suffer from short term memory\ntransformers are usually better\nespecially if you want to encode or\ngenerate longer sequences because of the\ntransformer architecture the natural\nlanguage processing industry can now\nachieve unprecedented results if you\nfound this helpful\nhit that like and subscribe button also\nlet me know in comments what you'd like\nto see next and until next time thanks\nfor watching\n",
  "words": [
    "transformers",
    "taking",
    "natural",
    "language",
    "processing",
    "world",
    "storm",
    "incredible",
    "models",
    "breaking",
    "multiple",
    "nlp",
    "records",
    "pushing",
    "used",
    "many",
    "applications",
    "like",
    "machine",
    "language",
    "translation",
    "conversational",
    "chat",
    "bots",
    "even",
    "power",
    "better",
    "search",
    "engines",
    "transformers",
    "rage",
    "deep",
    "learning",
    "nowadays",
    "work",
    "outperformed",
    "previous",
    "king",
    "sequence",
    "problems",
    "like",
    "recurrent",
    "neural",
    "networks",
    "gr",
    "use",
    "ls",
    "tiens",
    "probably",
    "heard",
    "different",
    "famous",
    "transformer",
    "models",
    "like",
    "burt",
    "cbt",
    "gb",
    "t2",
    "video",
    "focus",
    "one",
    "paper",
    "started",
    "attention",
    "need",
    "understand",
    "transformers",
    "first",
    "must",
    "understand",
    "attention",
    "mechanism",
    "get",
    "intuitive",
    "understanding",
    "attention",
    "mechanism",
    "let",
    "start",
    "fun",
    "text",
    "generation",
    "model",
    "capable",
    "writing",
    "novel",
    "need",
    "prime",
    "model",
    "arbitrary",
    "input",
    "model",
    "generate",
    "rest",
    "okay",
    "let",
    "make",
    "story",
    "interesting",
    "aliens",
    "entered",
    "planet",
    "began",
    "colonize",
    "earth",
    "certain",
    "group",
    "extraterrestrials",
    "begin",
    "manipulate",
    "society",
    "influence",
    "certain",
    "number",
    "elite",
    "country",
    "keep",
    "iron",
    "grip",
    "populace",
    "way",
    "make",
    "actually",
    "generated",
    "open",
    "ai",
    "gpt",
    "transformer",
    "model",
    "shout",
    "hugging",
    "face",
    "awesome",
    "interface",
    "play",
    "provide",
    "link",
    "description",
    "okay",
    "model",
    "little",
    "dark",
    "interesting",
    "works",
    "model",
    "generate",
    "tax",
    "word",
    "word",
    "ability",
    "reference",
    "tend",
    "words",
    "relevant",
    "generated",
    "word",
    "model",
    "knows",
    "attend",
    "learned",
    "training",
    "backpropagation",
    "intends",
    "also",
    "capable",
    "looking",
    "previous",
    "inputs",
    "power",
    "attention",
    "mechanism",
    "suffer",
    "memory",
    "rnns",
    "shorter",
    "window",
    "reference",
    "story",
    "gets",
    "longer",
    "rnns",
    "ca",
    "access",
    "word",
    "generated",
    "earlier",
    "sequence",
    "still",
    "true",
    "gr",
    "use",
    "l",
    "stm",
    "although",
    "bigger",
    "capacity",
    "achieve",
    "longer",
    "term",
    "memory",
    "therefore",
    "longer",
    "window",
    "reference",
    "attention",
    "mechanism",
    "theory",
    "given",
    "enough",
    "compute",
    "resources",
    "infinite",
    "window",
    "reference",
    "therefore",
    "capable",
    "using",
    "entire",
    "context",
    "story",
    "generating",
    "text",
    "power",
    "demonstrated",
    "paper",
    "attention",
    "need",
    "author",
    "introduce",
    "new",
    "novel",
    "neural",
    "network",
    "called",
    "transformers",
    "attention",
    "based",
    "encoder",
    "decoder",
    "type",
    "architecture",
    "high",
    "level",
    "encoder",
    "maps",
    "input",
    "sequence",
    "abstract",
    "continuous",
    "representation",
    "holds",
    "learned",
    "information",
    "input",
    "decoder",
    "takes",
    "continuous",
    "representation",
    "step",
    "step",
    "generates",
    "single",
    "output",
    "also",
    "fed",
    "previous",
    "output",
    "let",
    "walk",
    "example",
    "attention",
    "need",
    "paper",
    "applied",
    "transformer",
    "model",
    "neuro",
    "machine",
    "translation",
    "problem",
    "demonstration",
    "transformer",
    "model",
    "would",
    "conversational",
    "chat",
    "bot",
    "example",
    "taking",
    "input",
    "tax",
    "hi",
    "generate",
    "response",
    "fine",
    "let",
    "break",
    "mechanics",
    "network",
    "step",
    "step",
    "first",
    "step",
    "feeding",
    "input",
    "word",
    "embedded",
    "layer",
    "word",
    "embedding",
    "layer",
    "thought",
    "lookup",
    "table",
    "grab",
    "learn",
    "factor",
    "representation",
    "word",
    "neural",
    "networks",
    "learned",
    "numbers",
    "word",
    "maps",
    "vector",
    "continuous",
    "values",
    "represent",
    "word",
    "next",
    "step",
    "inject",
    "positional",
    "information",
    "embeddings",
    "transformer",
    "encoder",
    "recurrence",
    "like",
    "recurrent",
    "known",
    "networks",
    "must",
    "add",
    "information",
    "positions",
    "input",
    "embeddings",
    "done",
    "using",
    "positional",
    "encoding",
    "authors",
    "came",
    "clever",
    "trick",
    "using",
    "sine",
    "cosine",
    "functions",
    "wo",
    "go",
    "mathematical",
    "details",
    "positional",
    "codings",
    "video",
    "basics",
    "every",
    "odd",
    "time",
    "step",
    "create",
    "vector",
    "using",
    "cosine",
    "function",
    "every",
    "even",
    "time",
    "step",
    "create",
    "vector",
    "using",
    "sine",
    "function",
    "add",
    "vectors",
    "corresponding",
    "embedding",
    "vector",
    "successfully",
    "gives",
    "network",
    "information",
    "two",
    "positions",
    "vector",
    "sine",
    "cosine",
    "functions",
    "chosen",
    "tandem",
    "linear",
    "properties",
    "model",
    "easily",
    "learn",
    "attend",
    "encoder",
    "layer",
    "encoder",
    "layers",
    "job",
    "map",
    "input",
    "sequence",
    "abstract",
    "continuous",
    "representation",
    "holds",
    "learned",
    "information",
    "entire",
    "sequence",
    "contains",
    "two",
    "sub",
    "modules",
    "attention",
    "followed",
    "fully",
    "connected",
    "network",
    "also",
    "residual",
    "connections",
    "around",
    "two",
    "sub",
    "modules",
    "followed",
    "layer",
    "normalization",
    "break",
    "let",
    "look",
    "multi",
    "headed",
    "attention",
    "module",
    "attention",
    "indian",
    "code",
    "applies",
    "specific",
    "attention",
    "mechanism",
    "called",
    "self",
    "attention",
    "self",
    "attention",
    "allows",
    "model",
    "associate",
    "individual",
    "word",
    "input",
    "words",
    "input",
    "example",
    "possible",
    "model",
    "learn",
    "associate",
    "word",
    "also",
    "possible",
    "model",
    "learns",
    "word",
    "structured",
    "pattern",
    "typically",
    "question",
    "respond",
    "appropriately",
    "achieve",
    "self",
    "attention",
    "feed",
    "input",
    "three",
    "distinct",
    "fully",
    "connected",
    "layers",
    "create",
    "query",
    "key",
    "value",
    "vectors",
    "vectors",
    "exactly",
    "found",
    "good",
    "explanation",
    "stating",
    "query",
    "key",
    "value",
    "concept",
    "comes",
    "retrieval",
    "system",
    "example",
    "type",
    "query",
    "search",
    "video",
    "youtube",
    "search",
    "engine",
    "map",
    "query",
    "set",
    "keys",
    "example",
    "video",
    "title",
    "description",
    "etc",
    "associated",
    "candidate",
    "videos",
    "database",
    "present",
    "best",
    "match",
    "video",
    "let",
    "see",
    "relates",
    "self",
    "attention",
    "queries",
    "keys",
    "undergoes",
    "dot",
    "product",
    "matrix",
    "multiplication",
    "produce",
    "score",
    "matrix",
    "score",
    "matrix",
    "determines",
    "much",
    "focus",
    "word",
    "put",
    "words",
    "word",
    "score",
    "correspond",
    "words",
    "time",
    "step",
    "higher",
    "score",
    "focus",
    "queries",
    "mapped",
    "keys",
    "scores",
    "get",
    "scaled",
    "getting",
    "divided",
    "square",
    "root",
    "dimension",
    "queries",
    "keys",
    "allow",
    "stable",
    "gradients",
    "multiplying",
    "values",
    "exploding",
    "effects",
    "next",
    "take",
    "softmax",
    "scaled",
    "score",
    "get",
    "attention",
    "weights",
    "gives",
    "probability",
    "values",
    "0",
    "1",
    "softmax",
    "higher",
    "scores",
    "get",
    "heightened",
    "lower",
    "scores",
    "depressed",
    "allows",
    "model",
    "confident",
    "words",
    "attend",
    "take",
    "attention",
    "weights",
    "multiply",
    "value",
    "vector",
    "get",
    "output",
    "vector",
    "higher",
    "softmax",
    "scores",
    "keep",
    "value",
    "words",
    "model",
    "learn",
    "important",
    "lower",
    "scores",
    "drown",
    "irrelevant",
    "words",
    "feed",
    "output",
    "vector",
    "linear",
    "layer",
    "process",
    "make",
    "attention",
    "computation",
    "need",
    "split",
    "query",
    "key",
    "value",
    "adding",
    "vectors",
    "applying",
    "self",
    "attention",
    "split",
    "vectors",
    "goes",
    "self",
    "attention",
    "process",
    "individually",
    "self",
    "attention",
    "process",
    "called",
    "head",
    "head",
    "produces",
    "output",
    "vector",
    "gets",
    "concatenated",
    "single",
    "vector",
    "go",
    "final",
    "linear",
    "layer",
    "theory",
    "head",
    "would",
    "learn",
    "something",
    "different",
    "therefore",
    "giving",
    "encounter",
    "model",
    "representation",
    "power",
    "okay",
    "attention",
    "sum",
    "attention",
    "module",
    "transformer",
    "network",
    "attention",
    "waits",
    "input",
    "produces",
    "output",
    "vector",
    "encoded",
    "information",
    "word",
    "attend",
    "words",
    "sequence",
    "next",
    "step",
    "attention",
    "output",
    "vector",
    "added",
    "original",
    "input",
    "called",
    "residual",
    "connection",
    "output",
    "residual",
    "connection",
    "goes",
    "layer",
    "normalization",
    "normalized",
    "residual",
    "output",
    "gets",
    "fed",
    "network",
    "processing",
    "network",
    "couple",
    "linear",
    "layers",
    "relict",
    "evasion",
    "output",
    "added",
    "input",
    "network",
    "normalized",
    "residual",
    "connections",
    "helps",
    "network",
    "train",
    "allowing",
    "gradients",
    "flow",
    "networks",
    "directly",
    "layer",
    "normalizations",
    "used",
    "stabilize",
    "network",
    "results",
    "sustained",
    "producing",
    "training",
    "time",
    "necessary",
    "layer",
    "used",
    "process",
    "attention",
    "output",
    "potentially",
    "giving",
    "richer",
    "representation",
    "wraps",
    "encoded",
    "layer",
    "operations",
    "purpose",
    "encoding",
    "input",
    "continuous",
    "representation",
    "attention",
    "information",
    "help",
    "decoder",
    "focus",
    "appropriate",
    "words",
    "input",
    "decoding",
    "process",
    "stack",
    "encoder",
    "times",
    "encode",
    "information",
    "layer",
    "opportunity",
    "learn",
    "different",
    "attention",
    "representations",
    "therefore",
    "potentially",
    "boosting",
    "predictive",
    "power",
    "transformer",
    "network",
    "move",
    "decoder",
    "decoders",
    "job",
    "generate",
    "text",
    "sequences",
    "decoder",
    "similar",
    "sub",
    "layers",
    "encoder",
    "two",
    "attention",
    "layers",
    "layer",
    "residual",
    "connections",
    "layer",
    "normalization",
    "sub",
    "layer",
    "sub",
    "layers",
    "behave",
    "similarly",
    "layers",
    "encoder",
    "attention",
    "layer",
    "different",
    "job",
    "capped",
    "linear",
    "layer",
    "acts",
    "like",
    "classifier",
    "soft",
    "max",
    "get",
    "word",
    "probabilities",
    "decoder",
    "auto",
    "regressive",
    "takes",
    "list",
    "previous",
    "outputs",
    "inputs",
    "well",
    "encoder",
    "outputs",
    "contains",
    "attention",
    "information",
    "input",
    "decoder",
    "stops",
    "decoding",
    "generates",
    "end",
    "token",
    "output",
    "let",
    "walk",
    "decoding",
    "steps",
    "input",
    "goes",
    "embedding",
    "layer",
    "position",
    "coding",
    "layer",
    "get",
    "positional",
    "embeddings",
    "positional",
    "embeddings",
    "gets",
    "fed",
    "first",
    "attention",
    "layer",
    "computes",
    "attention",
    "score",
    "decoders",
    "input",
    "attention",
    "layer",
    "operates",
    "slightly",
    "different",
    "since",
    "decoders",
    "autoregressive",
    "generates",
    "sequence",
    "need",
    "prevent",
    "condition",
    "future",
    "tokens",
    "example",
    "computing",
    "attention",
    "scores",
    "word",
    "access",
    "word",
    "fine",
    "word",
    "future",
    "word",
    "generated",
    "word",
    "access",
    "words",
    "true",
    "words",
    "attend",
    "previous",
    "words",
    "need",
    "method",
    "prevent",
    "computing",
    "attention",
    "scores",
    "future",
    "words",
    "method",
    "called",
    "masking",
    "prevent",
    "decoder",
    "looking",
    "future",
    "tokens",
    "apply",
    "mask",
    "mask",
    "added",
    "calculating",
    "softmax",
    "scaling",
    "scores",
    "let",
    "take",
    "look",
    "works",
    "mask",
    "matrix",
    "size",
    "attention",
    "scores",
    "filled",
    "values",
    "materials",
    "negative",
    "infinity",
    "x",
    "add",
    "mask",
    "scale",
    "attention",
    "scores",
    "get",
    "matrix",
    "scores",
    "top",
    "right",
    "triangle",
    "filled",
    "negative",
    "infinity",
    "x",
    "reason",
    "take",
    "softmax",
    "mask",
    "scores",
    "negative",
    "infinity",
    "get",
    "zeroed",
    "leaving",
    "zero",
    "attention",
    "score",
    "future",
    "tokens",
    "see",
    "attention",
    "scores",
    "values",
    "words",
    "zero",
    "word",
    "fine",
    "essentially",
    "tells",
    "model",
    "put",
    "focus",
    "words",
    "masking",
    "difference",
    "attention",
    "scores",
    "calculated",
    "first",
    "attention",
    "layer",
    "layers",
    "still",
    "multiple",
    "heads",
    "masks",
    "applied",
    "getting",
    "concatenated",
    "fed",
    "linear",
    "layer",
    "processing",
    "output",
    "first",
    "attention",
    "mask",
    "output",
    "vector",
    "information",
    "model",
    "attend",
    "decoders",
    "inputs",
    "second",
    "attention",
    "layer",
    "layer",
    "encoders",
    "output",
    "queries",
    "keys",
    "first",
    "attention",
    "layer",
    "outputs",
    "values",
    "process",
    "matches",
    "encoders",
    "input",
    "decoders",
    "input",
    "allowing",
    "decoder",
    "decide",
    "encoder",
    "input",
    "relevant",
    "put",
    "focus",
    "output",
    "second",
    "attention",
    "goes",
    "point",
    "wise",
    "layer",
    "processing",
    "output",
    "final",
    "point",
    "wise",
    "layer",
    "goes",
    "final",
    "linear",
    "layer",
    "access",
    "classifier",
    "classifier",
    "biggest",
    "number",
    "classes",
    "example",
    "classes",
    "words",
    "output",
    "classifier",
    "size",
    "output",
    "classifier",
    "gets",
    "fed",
    "soft",
    "max",
    "layer",
    "soft",
    "max",
    "layer",
    "produced",
    "probability",
    "scores",
    "0",
    "1",
    "class",
    "take",
    "index",
    "highest",
    "probability",
    "score",
    "equals",
    "predicted",
    "word",
    "decoder",
    "taste",
    "output",
    "adds",
    "list",
    "decoder",
    "inputs",
    "continue",
    "decoding",
    "end",
    "token",
    "predicted",
    "case",
    "highest",
    "probability",
    "prediction",
    "final",
    "class",
    "assigned",
    "end",
    "token",
    "decoder",
    "generates",
    "output",
    "decoder",
    "stacked",
    "n",
    "layers",
    "high",
    "layer",
    "taking",
    "inputs",
    "encoder",
    "layers",
    "stacking",
    "layers",
    "model",
    "learn",
    "extract",
    "focus",
    "different",
    "combinations",
    "attention",
    "attention",
    "heads",
    "potentially",
    "boosting",
    "predictive",
    "power",
    "mechanics",
    "transformers",
    "transformers",
    "leverage",
    "power",
    "attention",
    "mechanism",
    "make",
    "better",
    "predictions",
    "recur",
    "known",
    "networks",
    "trying",
    "achieve",
    "similar",
    "things",
    "suffer",
    "short",
    "term",
    "memory",
    "transformers",
    "usually",
    "better",
    "especially",
    "want",
    "encode",
    "generate",
    "longer",
    "sequences",
    "transformer",
    "architecture",
    "natural",
    "language",
    "processing",
    "industry",
    "achieve",
    "unprecedented",
    "results",
    "found",
    "helpful",
    "hit",
    "like",
    "subscribe",
    "button",
    "also",
    "let",
    "know",
    "comments",
    "like",
    "see",
    "next",
    "next",
    "time",
    "thanks",
    "watching"
  ],
  "keywords": [
    "transformers",
    "taking",
    "language",
    "processing",
    "used",
    "like",
    "power",
    "better",
    "search",
    "previous",
    "sequence",
    "neural",
    "networks",
    "different",
    "transformer",
    "video",
    "focus",
    "paper",
    "attention",
    "need",
    "first",
    "mechanism",
    "get",
    "let",
    "text",
    "model",
    "capable",
    "input",
    "generate",
    "okay",
    "make",
    "story",
    "generated",
    "word",
    "reference",
    "words",
    "attend",
    "learned",
    "also",
    "inputs",
    "memory",
    "window",
    "gets",
    "longer",
    "access",
    "achieve",
    "therefore",
    "using",
    "network",
    "called",
    "encoder",
    "decoder",
    "continuous",
    "representation",
    "information",
    "step",
    "generates",
    "output",
    "fed",
    "example",
    "fine",
    "layer",
    "embedding",
    "learn",
    "vector",
    "values",
    "next",
    "positional",
    "embeddings",
    "add",
    "sine",
    "cosine",
    "time",
    "create",
    "vectors",
    "two",
    "linear",
    "layers",
    "job",
    "sub",
    "residual",
    "connections",
    "normalization",
    "self",
    "query",
    "key",
    "value",
    "keys",
    "see",
    "queries",
    "matrix",
    "score",
    "put",
    "higher",
    "scores",
    "take",
    "softmax",
    "probability",
    "process",
    "goes",
    "head",
    "final",
    "added",
    "potentially",
    "decoding",
    "decoders",
    "classifier",
    "soft",
    "max",
    "outputs",
    "end",
    "token",
    "prevent",
    "future",
    "tokens",
    "mask",
    "negative",
    "infinity"
  ]
}