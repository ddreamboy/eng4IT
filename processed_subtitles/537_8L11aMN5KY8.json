{
  "text": "[Music]\nhello i'm luis serrano and this video is\nabout generative adversarial networks or\nGanz for short develop IDN good fellow\nin our researches in montreal ganz are a\ngreat advance in machine learning and\nthey have numerous applications perhaps\nnot the fanciest applications of Ganz's\nface generation if you go to this page\nthis person does not exist calm you'll\nsee it in action these images you see\nhere are of people who don't exist they\nhave been fully generated by a neural\nnetwork this is fascinating\nconsidering how detailed our expressions\nare in this video we learn how to\ngenerate faces with again in a very\nsimple way and great news for those of\nyou who like to sing along to the videos\nas you type code in this video we'll be\ncoding a pair of one layer neural\nnetworks which will generate some very\nsimple images and it's all in github\nunder this repo which you can find in\nthe links but if you don't want to write\ncode this video is for you as well since\nwe develop the intuition and the\nequations by hand as well first let me\ngive you a general idea of what ganzar\nganz consists of a pair of neural\nnetworks that fight with each other one\nis called a generator and the other one\nthe discriminator and they behave a lot\nlike a counter feeder and a cup the\ncounter feeder is constantly trying to\nmake fake paintings and the cop is\nconstantly trying to catch the counter\nfeeder the counter feeder is a generator\nand the cop is a discriminator so as he\ngets caught by the cop the counter\nfeeder keeps improving and improving his\npaintings until one day he learns to\nfinally paint a perfect one that\ncompletely fools the cop a neural\nnetwork language what happens is that we\nhave a pair of neural networks the\ngenerator and the discriminator and we\ntrain them with a set of real images and\na set of fake images generated by the\ngenerator the discriminator is tend to\nidentify which images are real or which\nones come from the generator and the\ngenerator is strained to fool the\ndiscriminator into classifying its\nimages as real images\nin this video we'll build a very very\nsimple pair of guns so simple that we'll\nbe able to code them straight in Python\nwithout any deep learning packages this\nis our setting we live in a world called\nslanted land where everybody looks\nslightly elongated and walks at a 45\ndegree angle this world has technology\nbut not as developed as ours they have\ncomputer screens which display images\nbut the best resolution they've been\nable to work out is 2 times 2 so they\nhave black and white 2 pixels by 2 pixel\nscreens also they have developed neural\nnetworks but only very simple ones as a\nmatter of fact they only know neural\nnetworks of one layer but I will show\nyou that in this simple world we can\nstill create a pair of gas that will\ngenerate faces of the people who live in\nslanted land here are four faces of\npeople who live in slanted land notice\nthat everybody is elongated and tilted\n45 degrees to the left\nsince the screens are only 2 pixels by 2\npixels this is how the pictures of the\npeople look in the screen they SMO\nsucked like a diagonal or a backward\nslash and this is how noisy images look\nnotice that these are not faces since\nthey don't look like a backlash and\nthese are mostly generated randomly so\nthe goal for networks is to be able to\ndistinguish faces like these from noisy\nimages or non faces like these ones\nlet's attach some numbers here we'll\nhave a scale where a white pixel has\nvalues 0 and a black pixel has a value\nof 1 you may have seen this in the\nopposite way but we'll do it like this\nfor clarity this way we can attach a\nvalue to every pixel in the 2 x 2\nscreens\n ok we're ready to build our networks\nfirst I'll build them by hand and then\nI'll show you how to get the computer to\ntrain them let's start by building the\ndiscriminator the first question is how\nare we gonna tell faces apart from non\nfaces easy notice that in the faces the\ntop left and the bottom right corners\nhave large\nvalues because or pixels are dark\nwhereas the other two corners have small\nvalues because their pixels are light on\nthe other hand in noisy images anything\ncan happen\ntherefore the way to tell faces apart is\nby adding the two values corresponding\nto the top left and bottom right corners\nand subtracting the values corresponding\nto the other two corners and faces this\nresult will be very high whereas in\nnoisy images it will be low for example\nfor this face the value is 2 and the\nvalue for the noise image is minus 0.5\nwe can add a cutoff or a threshold of\nsay 1 and say that any image that's\ncourse 1 or higher is a face and any\nimage 2 is course less than 1 is not a\nface or it's a noisy image a neural\nnetwork lingo this is how things look\nthe values of our 4 pixels get\nmultiplied by plus or minus 1 depending\non what diagonal they are on and then we\nsubtract a total value of 1 or the bias\nwe add these 4 numbers and if the score\nis 1 or more than the image is\nclassified as a face and if it's less\nthan 1 that is classified as another\nface in this case the images face\nbecause it gets the score of 1 we can\nalso put the probability that something\nis a face by using the sigmoid function\nwe apply the sigmoid which is this\nfunction that sends high numbers to\nnumbers close to 1 and low numbers the\nnumbers close to 0 and we get sigmoid of\n1 which is 0.73 the discriminator\nnetwork then assigns to this image a\nprobability of 73% that it is a face\nsince it is a high probability greater\nthan 50% we conclude that the\ndiscriminator thinks that the image is a\nface which is correct notice that in\nthis neural network the thick edges are\npositive and that thin ones are negative\nand this is a convention we'll be using\nthroughout this video now if we enter\nthe second image which is not a face\ninto a discriminator then we do the same\ncalculation\nwe get negative 0.5 for the score\nsigmoid of negative 0.5 is 0.37 this is\nlower than 50% so we conclude that this\ndiscriminator thinks that this image is\nnot a face the discriminator is correct\nagain now in the same way let's build a\ngenerator this is a neural network that\nwill output faces to build a generator\nwill again take into account that along\nfaces these two corners are high value\nthese two corners are low value whereas\nin noisy images or non faces anything\ncan happen so this is how the generator\nworks first we start by picking an input\nset which is a random number between 0 &\n1 in this case let's say will be 0.7 in\ngeneral the input will be a vector that\ncomes from some fixed distribution now\nlet's build a neural network what we\nreally want is to have some large values\nand some small ones the larger ones are\nrepresented by thick edges and the small\nones by thin edges because remember that\nwe want large values for the top left\nand bottom right corner and small values\nfor the top right and bottom left corner\nso since the top output has to be large\nwe want these weights coming in to be\nlarge so let's make them plus 1 now what\ndo we get for the output here well first\nwe're gonna get a score of plus 1 times\n0.7 plus 1 which is 1 point 7 now let's\nlook at the second note it corresponds\nto the top-right corner so it has to be\na small value so let's put in negative\nnumbers here what do we get for the\nscore we get minus 1 times 0.7 minus 1\nwhich is minus 1.7 in the same way we\nwant a small value here so we put\nweights of minus 1 and we get minus 1.7\nand for the last one we're on a higher\nvalue so we put plus ones and we get\nplus one times zero point seven plus one\nwhich is one point seven now those are\njust the scores we need to apply sigmoid\nto find the probabilities so we apply\nthe sigmoid and we get 0.85 0.15 0.15 10\n0.85 those are the values that will go\nin our pixels\nand notice that our image looks like a\ndiagonal which is how we define our\nfaces noticed by the way we built it\nthis neural leopard will always generate\nlarge values for the top left and bottom\nright corners and small values for the\ntop right and bottom left corners no\nmatter what values that we input because\nremember that is between 0 & 1\ntherefore this neural network will\nalways generate a face that means it's a\ngood generator of course we built this\nneural networks by eyeballing the\nweights but that's not how it's normally\ndone in general we have to train the\nneural networks to get the best possible\nweights for this let me tell you a\nlittle bit about error functions and\nerror or cost function is a way to tell\nthe network how it's doing in order for\nit to improve if the error is large then\nthe network is not doing well and it\nneeds to reduce the error to improve the\nerror function that we'll use to train\nthese games is the log loss which is the\nnatural logarithm this is the logarithm\nbase e Y the logarithm well a logarithm\nappears a lot of error functions from\nmany deeper reasons which will not cover\nbut we can think of it for now so very\nconvenient function let's first say that\nwe have a label of 1 and our neural\nnetwork predicted is as 0.1 this is a\nbad prediction and we should produce a\nlarge error because 0.1 is very far from\nwhat on the other hand it was label is 1\nand the prediction is 0.9 then that's a\ngood prediction because the prediction\nis very close to the label so this\nshould produce a small error how then\ncan we find a formula for this error\nwell notice that the negative natural\nlogarithm of 0.1 is 2.3 which is large\nwhile the negative logarithm of 0.9 is\n0.1 which is low as a matter of fact the\ncloser number is to 1 the smaller is\nnegative logarithm gets therefore when\nthe label is 1 the function negative\nlogarithm of the prediction is a good\nerror function now let's go in the other\nextreme when the label is 0 in this case\nif we had a prediction of 0.1 it would\nbe good because it's close to the label\ntherefore the error should be small on\nthe other hand a prediction of 0.9 is\nterrible\n so the error should be large the\nfunction that we need here is similar to\nthe previous one with it with slight\ndifference it is the negative logarithm\nof 1 minus the prediction notice that in\nthe first case the error is the negative\nlogarithm of 0.9 which is 0.1 and in the\nsecond case is the negative logarithm is\nyour appointment 1 which is 2 points to\nbe and this match is what we want it\nwhich is that the first error is small\nthe second is large to summarize if the\nlabel is 1 which means we want the\nprediction to be 1 we define the law\nclass to be negative logarithm of the\nprediction notice that from the graph of\nthe negative logarithm of X on the right\nthis is big when the prediction is close\nto 0 and small when the prediction is\nclose to 1 and then when the label is 0\nwhich means I want the prediction to be\nclose to 0 then we use as an error\nfunction the negative logarithm of 1\nminus the prediction from the graph of\nnegative logarithm of 1 minus X in the\nright we see that when the prediction is\nclose to zero there's a low error and\nwhen the prediction is close to 1 there\nis a high error so these two are the\nerror functions that we're going to use\nfor training the generator and the\ndiscriminator based on if we want the\nprediction to be 0 or 1 now that we have\nour functions we get to the meat of our\ntraining process which is back\npropagation I will explain it very\nbriefly the way we train your networks\nby first taking a data point and\nperforming a forward pass calculating\nthe prediction and then calculating the\nerror based on the log loss that we\npreviously defined then we proceed to\ntake the derivative of the error based\non all the weights using the chain rule\nthis will tell us how much to obtain\neach weight in order to best decrease\nthe error the way we do the error using\na derivative is a process called\ngradient descent long story short what\nyou do is you plot the arrows back to\nthe\nthen calculate gradient which is the\ndirection of greatest growth and then\ntake a tiny step in the direction of the\nnegative of this gradient in order to\nfind new parameters that decrease this\nerror as much as possible now we're\nready to train the generator and the\ndiscriminator and what we have to do is\nput the right error functions on the\nright places here's our parent neural\nnetwork and notice that the weights are\nnot yet defined so we start by defining\nthem as random in numbers now we select\nset some random number between 0 and 1\nwhich is going to serve as the input to\nthe generator we do a forward pass of\nthe generator to obtain some image which\nis probably in our face since the\nweights are random this is gonna be our\ngenerated image now we pass that\ngenerated image through the\ndiscriminator so the discriminator can\ntell if it's fake or not the\ndiscriminator outputs a probability so\nlet's say that it's for example 0.68 now\npay close attention because this is\nwhere the rubber meets the rope\nthis is where we define the correct\nerror functions first let's think what\ndoes the discriminator want to do here\nin other words if the discriminator was\ngreat\nwhat should it output well since the\nimage is not a face but it's a generated\nimage from the generator then the\ndiscriminator should be saying that it's\nfake that means that the discriminator\nshould be outputting a 0 if we remember\nthe error function the way we measure an\nerror when we want the neural network to\noutput a 0 is the negative logarithm of\n1 minus the predation this is an error\nthat will help us train the\ndiscriminator now that we've figured out\nwhat the discriminate at once let's go\nto the generator what does the generator\nwant well the generators wildest dreams\nare to generate an image so good so real\nthat the discriminator classifies it as\nreal therefore the generator wants this\nwhole neural network the connection of\nthe two to output a 1\nthat means that the error function from\nthe generator is negative logarithm of\nthe prediction so that is the error\nfunction that will help us train the\nweights of the generator in other words\nif G of Z is the output of the generator\nand D F G of Z is the output of the\ndiscriminator then the error function\nfor the generator is negative logarithm\nof D of G of Z and the error function\nfrom the discriminator is negative\nlogarithm of 1 minus D of G of Z the\nderivatives of these two are what will\nhelp us update the weights of both\nneural networks in order to improve that\nparticular prediction notice that these\ntwo error functions fight against each\nother but that is okay because the read\nerror function only changes the\ngenerator weights and the blue error\nfunction only changes the discriminator\nweights therefore they don't collide\nthey simply improve both neural networks\nas one to produce different outputs\nwhich is fascinating so what we have to\ndo now is repeat this process many times\nwe pick a random value for Z we apply\nthe generator to produce a fake image\napply the discriminator to that image\nand use back propagation to update the\nweights on both the generator and the\ndiscriminator then we take a real image\nplug it into a discriminator on updated\nweights again using back propagation so\nwhat happens after many of these\niterations or epochs well we left the\ntraining in the notebook and we got\nthese values for the weights to make it\neasier I've drawn thick edges for the\npositive weights and thin edges for the\nways that are negative or zero feel free\nto pause the video and convince yourself\nthat these two neural networks actually\nwork really well both the generator for\ngenerating realistic with them faces and\nthe discriminator for being able to tell\napart the faces from the non-faces\nand the reason for this is that if we\nremember from the beginning of the video\nthe top left and bottom right corners\noff a face should be big and the other\ntwo should be small so let's just\nconvince ourselves\nlet's look at the generator notice that\nsince the input is between 0 & 1 and the\ntube top edges are positive then the\nsigmoid value of this output is large\nwhich is the value this pixel over here\ntherefore that pixel has a large value\nsimilarly these 2 values are also\npositive and they give us a large value\nfor this pixel over here now these two\nover here are negative so they give us a\nsmall value for this pixel and these 2\nover here negative so they give us a\nsmall value for this one and therefore\nour image looks a lot like a diagonal\nand given the resolution of the screens\nand slanted land that are all two\npictures by two pixels and this picture\nlooks and candidly like this resident of\nslanted land over here thus we have\nbuilt a network that generates faces now\nas promised here's the code for you to\nsing along to the video it's in this\nrepo called Yuans and they're my github\nfirst we have the faces that we\nhard-code and the random noise the\nimages that we generate that are not\nnecessarily faces then we also develop\nthe derivatives carefully for the\ndiscriminator based on faces then for\nthe discriminator based on noisy images\nboth with respect to the weights and\nrespect to the biases these are all\ncoded in this discriminated class we\nalso work out the derivatives\ncorresponding to the error functions for\nthe generator again with respect to the\nweights and the bias and these are all\ncoded in the generated class we also\nhave error function plots we can plot\nthe error function for the generator and\nthe error function for the discriminator\nnotice that the generator error function\ngoes down and stabilizes but since the\ngenerator and so following the\ndiscriminator then the discriminative\nfunction doesn't do so well and actually\ngoes up at the end and finally we ask\nour generator to generate some random\nimages and here they are notice that\nthey all look like faces and slanted\nland which is what we wanted from the\nbeginning therefore we have successfully\ncreated a pair of gas that generate\nfaces in slanted lead\nnow time for some acknowledgments this\nvideo would not be the same if not for\nthe help of my friend so a big thanks to\nDiego Sahil\nand Alejandra who helped me in various\nways either encourage me to learn Gans\nmore seriously or helping me with\nendless questions or gave me great\nfeedback on my code Diego in particular\nhas a great series of blog posts where\nhe cold scans from scratch and by torch\nand tensor flow I use them as an\ninspiration for this video so I highly\nrecommend them and that's all for today\n I hope you've enjoyed it I'd like to\nremind you that I have a\nmachine-learning book called rocking\nmachine learning in which I explain the\nconcepts of machine learning and\ndown-to-earth way with real examples for\neverybody to understand in the\ndescription you can find the link to the\nbook and a very special 40% discount for\nthe viewers of this channel and as usual\nif you enjoyed this video please\nsubscribe to my channel for more content\nor hit like or share amongst your\nfriends and feel free write comment I\nreally enjoy reading your comment\nespecially those with suggestions for\nfuture topics and if you'd like to tweet\nat me my twitter handle is luis likes\nmath all the informations videos\nwritings etc can be found at this link\nserrano academy so check it out thank\nyou very much for your attention and see\nyou in the next video\nyou\n\n",
  "sentences": [
    "[Music]\nhello i'm luis serrano and this video is\nabout generative adversarial networks or\nGanz for short develop IDN good fellow\nin our researches in montreal ganz are a\ngreat advance in machine learning and\nthey have numerous applications perhaps\nnot the fanciest applications of Ganz's\nface generation if you go to this page\nthis person does not exist calm you'll\nsee it in action these images you see\nhere are of people who don't exist they\nhave been fully generated by a neural\nnetwork this is fascinating\nconsidering how detailed our expressions\nare in this video we learn how to\ngenerate faces with again in a very\nsimple way and great news for those of\nyou who like to sing along to the videos\nas you type code in this video we'll be\ncoding a pair of one layer neural\nnetworks which will generate some very\nsimple images and it's all in github\nunder this repo which you can find in\nthe links but if you don't want to write\ncode this video is for you as well since\nwe develop the intuition and the\nequations by hand as well first let me\ngive you a general idea of what ganzar\nganz consists of a pair of neural\nnetworks that fight with each other one\nis called a generator and the other one\nthe discriminator and they behave a lot\nlike a counter feeder and a cup the\ncounter feeder is constantly trying to\nmake fake paintings and the cop is\nconstantly trying to catch the counter\nfeeder the counter feeder is a generator\nand the cop is a discriminator so as he\ngets caught by the cop the counter\nfeeder keeps improving and improving his\npaintings until one day he learns to\nfinally paint a perfect one that\ncompletely fools the cop a neural\nnetwork language what happens is that we\nhave a pair of neural networks the\ngenerator and the discriminator and we\ntrain them with a set of real images and\na set of fake images generated by the\ngenerator the discriminator is tend to\nidentify which images are real or which\nones come from the generator and the\ngenerator is strained to fool the\ndiscriminator into classifying its\nimages as real images\nin this video we'll build a very very\nsimple pair of guns so simple that we'll\nbe able to code them straight in Python\nwithout any deep learning packages this\nis our setting we live in a world called\nslanted land where everybody looks\nslightly elongated and walks at a 45\ndegree angle this world has technology\nbut not as developed as ours they have\ncomputer screens which display images\nbut the best resolution they've been\nable to work out is 2 times 2 so they\nhave black and white 2 pixels by 2 pixel\nscreens also they have developed neural\nnetworks but only very simple ones as a\nmatter of fact they only know neural\nnetworks of one layer but I will show\nyou that in this simple world we can\nstill create a pair of gas that will\ngenerate faces of the people who live in\nslanted land here are four faces of\npeople who live in slanted land notice\nthat everybody is elongated and tilted\n45 degrees to the left\nsince the screens are only 2 pixels by 2\npixels this is how the pictures of the\npeople look in the screen they SMO\nsucked like a diagonal or a backward\nslash and this is how noisy images look\nnotice that these are not faces since\nthey don't look like a backlash and\nthese are mostly generated randomly so\nthe goal for networks is to be able to\ndistinguish faces like these from noisy\nimages or non faces like these ones\nlet's attach some numbers here we'll\nhave a scale where a white pixel has\nvalues 0 and a black pixel has a value\nof 1 you may have seen this in the\nopposite way but we'll do it like this\nfor clarity this way we can attach a\nvalue to every pixel in the 2 x 2\nscreens\n ok we're ready to build our networks\nfirst I'll build them by hand and then\nI'll show you how to get the computer to\ntrain them let's start by building the\ndiscriminator the first question is how\nare we gonna tell faces apart from non\nfaces easy notice that in the faces the\ntop left and the bottom right corners\nhave large\nvalues because or pixels are dark\nwhereas the other two corners have small\nvalues because their pixels are light on\nthe other hand in noisy images anything\ncan happen\ntherefore the way to tell faces apart is\nby adding the two values corresponding\nto the top left and bottom right corners\nand subtracting the values corresponding\nto the other two corners and faces this\nresult will be very high whereas in\nnoisy images it will be low for example\nfor this face the value is 2 and the\nvalue for the noise image is minus 0.5\nwe can add a cutoff or a threshold of\nsay 1 and say that any image that's\ncourse 1 or higher is a face and any\nimage 2 is course less than 1 is not a\nface or it's a noisy image a neural\nnetwork lingo this is how things look\nthe values of our 4 pixels get\nmultiplied by plus or minus 1 depending\non what diagonal they are on and then we\nsubtract a total value of 1 or the bias\nwe add these 4 numbers and if the score\nis 1 or more than the image is\nclassified as a face and if it's less\nthan 1 that is classified as another\nface in this case the images face\nbecause it gets the score of 1 we can\nalso put the probability that something\nis a face by using the sigmoid function\nwe apply the sigmoid which is this\nfunction that sends high numbers to\nnumbers close to 1 and low numbers the\nnumbers close to 0 and we get sigmoid of\n1 which is 0.73 the discriminator\nnetwork then assigns to this image a\nprobability of 73% that it is a face\nsince it is a high probability greater\nthan 50% we conclude that the\ndiscriminator thinks that the image is a\nface which is correct notice that in\nthis neural network the thick edges are\npositive and that thin ones are negative\nand this is a convention we'll be using\nthroughout this video now if we enter\nthe second image which is not a face\ninto a discriminator then we do the same\ncalculation\nwe get negative 0.5 for the score\nsigmoid of negative 0.5 is 0.37 this is\nlower than 50% so we conclude that this\ndiscriminator thinks that this image is\nnot a face the discriminator is correct\nagain now in the same way let's build a\ngenerator this is a neural network that\nwill output faces to build a generator\nwill again take into account that along\nfaces these two corners are high value\nthese two corners are low value whereas\nin noisy images or non faces anything\ncan happen so this is how the generator\nworks first we start by picking an input\nset which is a random number between 0 &\n1 in this case let's say will be 0.7 in\ngeneral the input will be a vector that\ncomes from some fixed distribution now\nlet's build a neural network what we\nreally want is to have some large values\nand some small ones the larger ones are\nrepresented by thick edges and the small\nones by thin edges because remember that\nwe want large values for the top left\nand bottom right corner and small values\nfor the top right and bottom left corner\nso since the top output has to be large\nwe want these weights coming in to be\nlarge so let's make them plus 1 now what\ndo we get for the output here well first\nwe're gonna get a score of plus 1 times\n0.7 plus 1 which is 1 point 7 now let's\nlook at the second note it corresponds\nto the top-right corner so it has to be\na small value so let's put in negative\nnumbers here what do we get for the\nscore we get minus 1 times 0.7 minus 1\nwhich is minus 1.7 in the same way we\nwant a small value here so we put\nweights of minus 1 and we get minus 1.7\nand for the last one we're on a higher\nvalue so we put plus ones and we get\nplus one times zero point seven plus one\nwhich is one point seven now those are\njust the scores we need to apply sigmoid\nto find the probabilities so we apply\nthe sigmoid and we get 0.85 0.15 0.15 10\n0.85 those are the values that will go\nin our pixels\nand notice that our image looks like a\ndiagonal which is how we define our\nfaces noticed by the way we built it\nthis neural leopard will always generate\nlarge values for the top left and bottom\nright corners and small values for the\ntop right and bottom left corners no\nmatter what values that we input because\nremember that is between 0 & 1\ntherefore this neural network will\nalways generate a face that means it's a\ngood generator of course we built this\nneural networks by eyeballing the\nweights but that's not how it's normally\ndone in general we have to train the\nneural networks to get the best possible\nweights for this let me tell you a\nlittle bit about error functions and\nerror or cost function is a way to tell\nthe network how it's doing in order for\nit to improve if the error is large then\nthe network is not doing well and it\nneeds to reduce the error to improve the\nerror function that we'll use to train\nthese games is the log loss which is the\nnatural logarithm this is the logarithm\nbase e Y the logarithm well a logarithm\nappears a lot of error functions from\nmany deeper reasons which will not cover\nbut we can think of it for now so very\nconvenient function let's first say that\nwe have a label of 1 and our neural\nnetwork predicted is as 0.1 this is a\nbad prediction and we should produce a\nlarge error because 0.1 is very far from\nwhat on the other hand it was label is 1\nand the prediction is 0.9 then that's a\ngood prediction because the prediction\nis very close to the label so this\nshould produce a small error how then\ncan we find a formula for this error\nwell notice that the negative natural\nlogarithm of 0.1 is 2.3 which is large\nwhile the negative logarithm of 0.9 is\n0.1 which is low as a matter of fact the\ncloser number is to 1 the smaller is\nnegative logarithm gets therefore when\nthe label is 1 the function negative\nlogarithm of the prediction is a good\nerror function now let's go in the other\nextreme when the label is 0 in this case\nif we had a prediction of 0.1 it would\nbe good because it's close to the label\ntherefore the error should be small on\nthe other hand a prediction of 0.9 is\nterrible\n so the error should be large the\nfunction that we need here is similar to\nthe previous one with it with slight\ndifference it is the negative logarithm\nof 1 minus the prediction notice that in\nthe first case the error is the negative\nlogarithm of 0.9 which is 0.1 and in the\nsecond case is the negative logarithm is\nyour appointment 1 which is 2 points to\nbe and this match is what we want it\nwhich is that the first error is small\nthe second is large to summarize if the\nlabel is 1 which means we want the\nprediction to be 1 we define the law\nclass to be negative logarithm of the\nprediction notice that from the graph of\nthe negative logarithm of X on the right\nthis is big when the prediction is close\nto 0 and small when the prediction is\nclose to 1 and then when the label is 0\nwhich means I want the prediction to be\nclose to 0 then we use as an error\nfunction the negative logarithm of 1\nminus the prediction from the graph of\nnegative logarithm of 1 minus X in the\nright we see that when the prediction is\nclose to zero there's a low error and\nwhen the prediction is close to 1 there\nis a high error so these two are the\nerror functions that we're going to use\nfor training the generator and the\ndiscriminator based on if we want the\nprediction to be 0 or 1 now that we have\nour functions we get to the meat of our\ntraining process which is back\npropagation I will explain it very\nbriefly the way we train your networks\nby first taking a data point and\nperforming a forward pass calculating\nthe prediction and then calculating the\nerror based on the log loss that we\npreviously defined then we proceed to\ntake the derivative of the error based\non all the weights using the chain rule\nthis will tell us how much to obtain\neach weight in order to best decrease\nthe error the way we do the error using\na derivative is a process called\ngradient descent long story short what\nyou do is you plot the arrows back to\nthe\nthen calculate gradient which is the\ndirection of greatest growth and then\ntake a tiny step in the direction of the\nnegative of this gradient in order to\nfind new parameters that decrease this\nerror as much as possible now we're\nready to train the generator and the\ndiscriminator and what we have to do is\nput the right error functions on the\nright places here's our parent neural\nnetwork and notice that the weights are\nnot yet defined so we start by defining\nthem as random in numbers now we select\nset some random number between 0 and 1\nwhich is going to serve as the input to\nthe generator we do a forward pass of\nthe generator to obtain some image which\nis probably in our face since the\nweights are random this is gonna be our\ngenerated image now we pass that\ngenerated image through the\ndiscriminator so the discriminator can\ntell if it's fake or not the\ndiscriminator outputs a probability so\nlet's say that it's for example 0.68 now\npay close attention because this is\nwhere the rubber meets the rope\nthis is where we define the correct\nerror functions first let's think what\ndoes the discriminator want to do here\nin other words if the discriminator was\ngreat\nwhat should it output well since the\nimage is not a face but it's a generated\nimage from the generator then the\ndiscriminator should be saying that it's\nfake that means that the discriminator\nshould be outputting a 0 if we remember\nthe error function the way we measure an\nerror when we want the neural network to\noutput a 0 is the negative logarithm of\n1 minus the predation this is an error\nthat will help us train the\ndiscriminator now that we've figured out\nwhat the discriminate at once let's go\nto the generator what does the generator\nwant well the generators wildest dreams\nare to generate an image so good so real\nthat the discriminator classifies it as\nreal therefore the generator wants this\nwhole neural network the connection of\nthe two to output a 1\nthat means that the error function from\nthe generator is negative logarithm of\nthe prediction so that is the error\nfunction that will help us train the\nweights of the generator in other words\nif G of Z is the output of the generator\nand D F G of Z is the output of the\ndiscriminator then the error function\nfor the generator is negative logarithm\nof D of G of Z and the error function\nfrom the discriminator is negative\nlogarithm of 1 minus D of G of Z the\nderivatives of these two are what will\nhelp us update the weights of both\nneural networks in order to improve that\nparticular prediction notice that these\ntwo error functions fight against each\nother but that is okay because the read\nerror function only changes the\ngenerator weights and the blue error\nfunction only changes the discriminator\nweights therefore they don't collide\nthey simply improve both neural networks\nas one to produce different outputs\nwhich is fascinating so what we have to\ndo now is repeat this process many times\nwe pick a random value for Z we apply\nthe generator to produce a fake image\napply the discriminator to that image\nand use back propagation to update the\nweights on both the generator and the\ndiscriminator then we take a real image\nplug it into a discriminator on updated\nweights again using back propagation so\nwhat happens after many of these\niterations or epochs well we left the\ntraining in the notebook and we got\nthese values for the weights to make it\neasier I've drawn thick edges for the\npositive weights and thin edges for the\nways that are negative or zero feel free\nto pause the video and convince yourself\nthat these two neural networks actually\nwork really well both the generator for\ngenerating realistic with them faces and\nthe discriminator for being able to tell\napart the faces from the non-faces\nand the reason for this is that if we\nremember from the beginning of the video\nthe top left and bottom right corners\noff a face should be big and the other\ntwo should be small so let's just\nconvince ourselves\nlet's look at the generator notice that\nsince the input is between 0 & 1 and the\ntube top edges are positive then the\nsigmoid value of this output is large\nwhich is the value this pixel over here\ntherefore that pixel has a large value\nsimilarly these 2 values are also\npositive and they give us a large value\nfor this pixel over here now these two\nover here are negative so they give us a\nsmall value for this pixel and these 2\nover here negative so they give us a\nsmall value for this one and therefore\nour image looks a lot like a diagonal\nand given the resolution of the screens\nand slanted land that are all two\npictures by two pixels and this picture\nlooks and candidly like this resident of\nslanted land over here thus we have\nbuilt a network that generates faces now\nas promised here's the code for you to\nsing along to the video it's in this\nrepo called Yuans and they're my github\nfirst we have the faces that we\nhard-code and the random noise the\nimages that we generate that are not\nnecessarily faces then we also develop\nthe derivatives carefully for the\ndiscriminator based on faces then for\nthe discriminator based on noisy images\nboth with respect to the weights and\nrespect to the biases these are all\ncoded in this discriminated class we\nalso work out the derivatives\ncorresponding to the error functions for\nthe generator again with respect to the\nweights and the bias and these are all\ncoded in the generated class we also\nhave error function plots we can plot\nthe error function for the generator and\nthe error function for the discriminator\nnotice that the generator error function\ngoes down and stabilizes but since the\ngenerator and so following the\ndiscriminator then the discriminative\nfunction doesn't do so well and actually\ngoes up at the end and finally we ask\nour generator to generate some random\nimages and here they are notice that\nthey all look like faces and slanted\nland which is what we wanted from the\nbeginning therefore we have successfully\ncreated a pair of gas that generate\nfaces in slanted lead\nnow time for some acknowledgments this\nvideo would not be the same if not for\nthe help of my friend so a big thanks to\nDiego Sahil\nand Alejandra who helped me in various\nways either encourage me to learn Gans\nmore seriously or helping me with\nendless questions or gave me great\nfeedback on my code Diego in particular\nhas a great series of blog posts where\nhe cold scans from scratch and by torch\nand tensor flow I use them as an\ninspiration for this video so I highly\nrecommend them and that's all for today\n I hope you've enjoyed it I'd like to\nremind you that I have a\nmachine-learning book called rocking\nmachine learning in which I explain the\nconcepts of machine learning and\ndown-to-earth way with real examples for\neverybody to understand in the\ndescription you can find the link to the\nbook and a very special 40% discount for\nthe viewers of this channel and as usual\nif you enjoyed this video please\nsubscribe to my channel for more content\nor hit like or share amongst your\nfriends and feel free write comment I\nreally enjoy reading your comment\nespecially those with suggestions for\nfuture topics and if you'd like to tweet\nat me my twitter handle is luis likes\nmath all the informations videos\nwritings etc can be found at this link\nserrano academy so check it out thank\nyou very much for your attention and see\nyou in the next video\nyou"
  ],
  "paragraphs": [
    "[Music]",
    "hello i'm luis serrano and this video is",
    "about generative adversarial networks or",
    "Ganz for short develop IDN good fellow",
    "in our researches in montreal ganz are a",
    "great advance in machine learning and",
    "they have numerous applications perhaps",
    "not the fanciest applications of Ganz's",
    "face generation if you go to this page",
    "this person does not exist calm you'll",
    "see it in action these images you see",
    "here are of people who don't exist they",
    "have been fully generated by a neural",
    "network this is fascinating",
    "considering how detailed our expressions",
    "are in this video we learn how to",
    "generate faces with again in a very",
    "simple way and great news for those of",
    "you who like to sing along to the videos",
    "as you type code in this video we'll be",
    "coding a pair of one layer neural",
    "networks which will generate some very",
    "simple images and it's all in github",
    "under this repo which you can find in",
    "the links but if you don't want to write",
    "code this video is for you as well since",
    "we develop the intuition and the",
    "equations by hand as well first let me",
    "give you a general idea of what ganzar",
    "ganz consists of a pair of neural",
    "networks that fight with each other one",
    "is called a generator and the other one",
    "the discriminator and they behave a lot",
    "like a counter feeder and a cup the",
    "counter feeder is constantly trying to",
    "make fake paintings and the cop is",
    "constantly trying to catch the counter",
    "feeder the counter feeder is a generator",
    "and the cop is a discriminator so as he",
    "gets caught by the cop the counter",
    "feeder keeps improving and improving his",
    "paintings until one day he learns to",
    "finally paint a perfect one that",
    "completely fools the cop a neural",
    "network language what happens is that we",
    "have a pair of neural networks the",
    "generator and the discriminator and we",
    "train them with a set of real images and",
    "a set of fake images generated by the",
    "generator the discriminator is tend to",
    "identify which images are real or which",
    "ones come from the generator and the",
    "generator is strained to fool the",
    "discriminator into classifying its",
    "images as real images",
    "in this video we'll build a very very",
    "simple pair of guns so simple that we'll",
    "be able to code them straight in Python",
    "without any deep learning packages this",
    "is our setting we live in a world called",
    "slanted land where everybody looks",
    "slightly elongated and walks at a 45",
    "degree angle this world has technology",
    "but not as developed as ours they have",
    "computer screens which display images",
    "but the best resolution they've been",
    "able to work out is 2 times 2 so they",
    "have black and white 2 pixels by 2 pixel",
    "screens also they have developed neural",
    "networks but only very simple ones as a",
    "matter of fact they only know neural",
    "networks of one layer but I will show",
    "you that in this simple world we can",
    "still create a pair of gas that will",
    "generate faces of the people who live in",
    "slanted land here are four faces of",
    "people who live in slanted land notice",
    "that everybody is elongated and tilted",
    "45 degrees to the left",
    "since the screens are only 2 pixels by 2",
    "pixels this is how the pictures of the",
    "people look in the screen they SMO",
    "sucked like a diagonal or a backward",
    "slash and this is how noisy images look",
    "notice that these are not faces since",
    "they don't look like a backlash and",
    "these are mostly generated randomly so",
    "the goal for networks is to be able to",
    "distinguish faces like these from noisy",
    "images or non faces like these ones",
    "let's attach some numbers here we'll",
    "have a scale where a white pixel has",
    "values 0 and a black pixel has a value",
    "of 1 you may have seen this in the",
    "opposite way but we'll do it like this",
    "for clarity this way we can attach a",
    "value to every pixel in the 2 x 2",
    "screens",
    "ok we're ready to build our networks",
    "first I'll build them by hand and then",
    "I'll show you how to get the computer to",
    "train them let's start by building the",
    "discriminator the first question is how",
    "are we gonna tell faces apart from non",
    "faces easy notice that in the faces the",
    "top left and the bottom right corners",
    "have large",
    "values because or pixels are dark",
    "whereas the other two corners have small",
    "values because their pixels are light on",
    "the other hand in noisy images anything",
    "can happen",
    "therefore the way to tell faces apart is",
    "by adding the two values corresponding",
    "to the top left and bottom right corners",
    "and subtracting the values corresponding",
    "to the other two corners and faces this",
    "result will be very high whereas in",
    "noisy images it will be low for example",
    "for this face the value is 2 and the",
    "value for the noise image is minus 0.5",
    "we can add a cutoff or a threshold of",
    "say 1 and say that any image that's",
    "course 1 or higher is a face and any",
    "image 2 is course less than 1 is not a",
    "face or it's a noisy image a neural",
    "network lingo this is how things look",
    "the values of our 4 pixels get",
    "multiplied by plus or minus 1 depending",
    "on what diagonal they are on and then we",
    "subtract a total value of 1 or the bias",
    "we add these 4 numbers and if the score",
    "is 1 or more than the image is",
    "classified as a face and if it's less",
    "than 1 that is classified as another",
    "face in this case the images face",
    "because it gets the score of 1 we can",
    "also put the probability that something",
    "is a face by using the sigmoid function",
    "we apply the sigmoid which is this",
    "function that sends high numbers to",
    "numbers close to 1 and low numbers the",
    "numbers close to 0 and we get sigmoid of",
    "1 which is 0.73 the discriminator",
    "network then assigns to this image a",
    "probability of 73% that it is a face",
    "since it is a high probability greater",
    "than 50% we conclude that the",
    "discriminator thinks that the image is a",
    "face which is correct notice that in",
    "this neural network the thick edges are",
    "positive and that thin ones are negative",
    "and this is a convention we'll be using",
    "throughout this video now if we enter",
    "the second image which is not a face",
    "into a discriminator then we do the same",
    "calculation",
    "we get negative 0.5 for the score",
    "sigmoid of negative 0.5 is 0.37 this is",
    "lower than 50% so we conclude that this",
    "discriminator thinks that this image is",
    "not a face the discriminator is correct",
    "again now in the same way let's build a",
    "generator this is a neural network that",
    "will output faces to build a generator",
    "will again take into account that along",
    "faces these two corners are high value",
    "these two corners are low value whereas",
    "in noisy images or non faces anything",
    "can happen so this is how the generator",
    "works first we start by picking an input",
    "set which is a random number between 0 &",
    "1 in this case let's say will be 0.7 in",
    "general the input will be a vector that",
    "comes from some fixed distribution now",
    "let's build a neural network what we",
    "really want is to have some large values",
    "and some small ones the larger ones are",
    "represented by thick edges and the small",
    "ones by thin edges because remember that",
    "we want large values for the top left",
    "and bottom right corner and small values",
    "for the top right and bottom left corner",
    "so since the top output has to be large",
    "we want these weights coming in to be",
    "large so let's make them plus 1 now what",
    "do we get for the output here well first",
    "we're gonna get a score of plus 1 times",
    "0.7 plus 1 which is 1 point 7 now let's",
    "look at the second note it corresponds",
    "to the top-right corner so it has to be",
    "a small value so let's put in negative",
    "numbers here what do we get for the",
    "score we get minus 1 times 0.7 minus 1",
    "which is minus 1.7 in the same way we",
    "want a small value here so we put",
    "weights of minus 1 and we get minus 1.7",
    "and for the last one we're on a higher",
    "value so we put plus ones and we get",
    "plus one times zero point seven plus one",
    "which is one point seven now those are",
    "just the scores we need to apply sigmoid",
    "to find the probabilities so we apply",
    "the sigmoid and we get 0.85 0.15 0.15 10",
    "0.85 those are the values that will go",
    "in our pixels",
    "and notice that our image looks like a",
    "diagonal which is how we define our",
    "faces noticed by the way we built it",
    "this neural leopard will always generate",
    "large values for the top left and bottom",
    "right corners and small values for the",
    "top right and bottom left corners no",
    "matter what values that we input because",
    "remember that is between 0 & 1",
    "therefore this neural network will",
    "always generate a face that means it's a",
    "good generator of course we built this",
    "neural networks by eyeballing the",
    "weights but that's not how it's normally",
    "done in general we have to train the",
    "neural networks to get the best possible",
    "weights for this let me tell you a",
    "little bit about error functions and",
    "error or cost function is a way to tell",
    "the network how it's doing in order for",
    "it to improve if the error is large then",
    "the network is not doing well and it",
    "needs to reduce the error to improve the",
    "error function that we'll use to train",
    "these games is the log loss which is the",
    "natural logarithm this is the logarithm",
    "base e Y the logarithm well a logarithm",
    "appears a lot of error functions from",
    "many deeper reasons which will not cover",
    "but we can think of it for now so very",
    "convenient function let's first say that",
    "we have a label of 1 and our neural",
    "network predicted is as 0.1 this is a",
    "bad prediction and we should produce a",
    "large error because 0.1 is very far from",
    "what on the other hand it was label is 1",
    "and the prediction is 0.9 then that's a",
    "good prediction because the prediction",
    "is very close to the label so this",
    "should produce a small error how then",
    "can we find a formula for this error",
    "well notice that the negative natural",
    "logarithm of 0.1 is 2.3 which is large",
    "while the negative logarithm of 0.9 is",
    "0.1 which is low as a matter of fact the",
    "closer number is to 1 the smaller is",
    "negative logarithm gets therefore when",
    "the label is 1 the function negative",
    "logarithm of the prediction is a good",
    "error function now let's go in the other",
    "extreme when the label is 0 in this case",
    "if we had a prediction of 0.1 it would",
    "be good because it's close to the label",
    "therefore the error should be small on",
    "the other hand a prediction of 0.9 is",
    "terrible",
    "so the error should be large the",
    "function that we need here is similar to",
    "the previous one with it with slight",
    "difference it is the negative logarithm",
    "of 1 minus the prediction notice that in",
    "the first case the error is the negative",
    "logarithm of 0.9 which is 0.1 and in the",
    "second case is the negative logarithm is",
    "your appointment 1 which is 2 points to",
    "be and this match is what we want it",
    "which is that the first error is small",
    "the second is large to summarize if the",
    "label is 1 which means we want the",
    "prediction to be 1 we define the law",
    "class to be negative logarithm of the",
    "prediction notice that from the graph of",
    "the negative logarithm of X on the right",
    "this is big when the prediction is close",
    "to 0 and small when the prediction is",
    "close to 1 and then when the label is 0",
    "which means I want the prediction to be",
    "close to 0 then we use as an error",
    "function the negative logarithm of 1",
    "minus the prediction from the graph of",
    "negative logarithm of 1 minus X in the",
    "right we see that when the prediction is",
    "close to zero there's a low error and",
    "when the prediction is close to 1 there",
    "is a high error so these two are the",
    "error functions that we're going to use",
    "for training the generator and the",
    "discriminator based on if we want the",
    "prediction to be 0 or 1 now that we have",
    "our functions we get to the meat of our",
    "training process which is back",
    "propagation I will explain it very",
    "briefly the way we train your networks",
    "by first taking a data point and",
    "performing a forward pass calculating",
    "the prediction and then calculating the",
    "error based on the log loss that we",
    "previously defined then we proceed to",
    "take the derivative of the error based",
    "on all the weights using the chain rule",
    "this will tell us how much to obtain",
    "each weight in order to best decrease",
    "the error the way we do the error using",
    "a derivative is a process called",
    "gradient descent long story short what",
    "you do is you plot the arrows back to",
    "the",
    "then calculate gradient which is the",
    "direction of greatest growth and then",
    "take a tiny step in the direction of the",
    "negative of this gradient in order to",
    "find new parameters that decrease this",
    "error as much as possible now we're",
    "ready to train the generator and the",
    "discriminator and what we have to do is",
    "put the right error functions on the",
    "right places here's our parent neural",
    "network and notice that the weights are",
    "not yet defined so we start by defining",
    "them as random in numbers now we select",
    "set some random number between 0 and 1",
    "which is going to serve as the input to",
    "the generator we do a forward pass of",
    "the generator to obtain some image which",
    "is probably in our face since the",
    "weights are random this is gonna be our",
    "generated image now we pass that",
    "generated image through the",
    "discriminator so the discriminator can",
    "tell if it's fake or not the",
    "discriminator outputs a probability so",
    "let's say that it's for example 0.68 now",
    "pay close attention because this is",
    "where the rubber meets the rope",
    "this is where we define the correct",
    "error functions first let's think what",
    "does the discriminator want to do here",
    "in other words if the discriminator was",
    "great",
    "what should it output well since the",
    "image is not a face but it's a generated",
    "image from the generator then the",
    "discriminator should be saying that it's",
    "fake that means that the discriminator",
    "should be outputting a 0 if we remember",
    "the error function the way we measure an",
    "error when we want the neural network to",
    "output a 0 is the negative logarithm of",
    "1 minus the predation this is an error",
    "that will help us train the",
    "discriminator now that we've figured out",
    "what the discriminate at once let's go",
    "to the generator what does the generator",
    "want well the generators wildest dreams",
    "are to generate an image so good so real",
    "that the discriminator classifies it as",
    "real therefore the generator wants this",
    "whole neural network the connection of",
    "the two to output a 1",
    "that means that the error function from",
    "the generator is negative logarithm of",
    "the prediction so that is the error",
    "function that will help us train the",
    "weights of the generator in other words",
    "if G of Z is the output of the generator",
    "and D F G of Z is the output of the",
    "discriminator then the error function",
    "for the generator is negative logarithm",
    "of D of G of Z and the error function",
    "from the discriminator is negative",
    "logarithm of 1 minus D of G of Z the",
    "derivatives of these two are what will",
    "help us update the weights of both",
    "neural networks in order to improve that",
    "particular prediction notice that these",
    "two error functions fight against each",
    "other but that is okay because the read",
    "error function only changes the",
    "generator weights and the blue error",
    "function only changes the discriminator",
    "weights therefore they don't collide",
    "they simply improve both neural networks",
    "as one to produce different outputs",
    "which is fascinating so what we have to",
    "do now is repeat this process many times",
    "we pick a random value for Z we apply",
    "the generator to produce a fake image",
    "apply the discriminator to that image",
    "and use back propagation to update the",
    "weights on both the generator and the",
    "discriminator then we take a real image",
    "plug it into a discriminator on updated",
    "weights again using back propagation so",
    "what happens after many of these",
    "iterations or epochs well we left the",
    "training in the notebook and we got",
    "these values for the weights to make it",
    "easier I've drawn thick edges for the",
    "positive weights and thin edges for the",
    "ways that are negative or zero feel free",
    "to pause the video and convince yourself",
    "that these two neural networks actually",
    "work really well both the generator for",
    "generating realistic with them faces and",
    "the discriminator for being able to tell",
    "apart the faces from the non-faces",
    "and the reason for this is that if we",
    "remember from the beginning of the video",
    "the top left and bottom right corners",
    "off a face should be big and the other",
    "two should be small so let's just",
    "convince ourselves",
    "let's look at the generator notice that",
    "since the input is between 0 & 1 and the",
    "tube top edges are positive then the",
    "sigmoid value of this output is large",
    "which is the value this pixel over here",
    "therefore that pixel has a large value",
    "similarly these 2 values are also",
    "positive and they give us a large value",
    "for this pixel over here now these two",
    "over here are negative so they give us a",
    "small value for this pixel and these 2",
    "over here negative so they give us a",
    "small value for this one and therefore",
    "our image looks a lot like a diagonal",
    "and given the resolution of the screens",
    "and slanted land that are all two",
    "pictures by two pixels and this picture",
    "looks and candidly like this resident of",
    "slanted land over here thus we have",
    "built a network that generates faces now",
    "as promised here's the code for you to",
    "sing along to the video it's in this",
    "repo called Yuans and they're my github",
    "first we have the faces that we",
    "hard-code and the random noise the",
    "images that we generate that are not",
    "necessarily faces then we also develop",
    "the derivatives carefully for the",
    "discriminator based on faces then for",
    "the discriminator based on noisy images",
    "both with respect to the weights and",
    "respect to the biases these are all",
    "coded in this discriminated class we",
    "also work out the derivatives",
    "corresponding to the error functions for",
    "the generator again with respect to the",
    "weights and the bias and these are all",
    "coded in the generated class we also",
    "have error function plots we can plot",
    "the error function for the generator and",
    "the error function for the discriminator",
    "notice that the generator error function",
    "goes down and stabilizes but since the",
    "generator and so following the",
    "discriminator then the discriminative",
    "function doesn't do so well and actually",
    "goes up at the end and finally we ask",
    "our generator to generate some random",
    "images and here they are notice that",
    "they all look like faces and slanted",
    "land which is what we wanted from the",
    "beginning therefore we have successfully",
    "created a pair of gas that generate",
    "faces in slanted lead",
    "now time for some acknowledgments this",
    "video would not be the same if not for",
    "the help of my friend so a big thanks to",
    "Diego Sahil",
    "and Alejandra who helped me in various",
    "ways either encourage me to learn Gans",
    "more seriously or helping me with",
    "endless questions or gave me great",
    "feedback on my code Diego in particular",
    "has a great series of blog posts where",
    "he cold scans from scratch and by torch",
    "and tensor flow I use them as an",
    "inspiration for this video so I highly",
    "recommend them and that's all for today",
    "I hope you've enjoyed it I'd like to",
    "remind you that I have a",
    "machine-learning book called rocking",
    "machine learning in which I explain the",
    "concepts of machine learning and",
    "down-to-earth way with real examples for",
    "everybody to understand in the",
    "description you can find the link to the",
    "book and a very special 40% discount for",
    "the viewers of this channel and as usual",
    "if you enjoyed this video please",
    "subscribe to my channel for more content",
    "or hit like or share amongst your",
    "friends and feel free write comment I",
    "really enjoy reading your comment",
    "especially those with suggestions for",
    "future topics and if you'd like to tweet",
    "at me my twitter handle is luis likes",
    "math all the informations videos",
    "writings etc can be found at this link",
    "serrano academy so check it out thank",
    "you very much for your attention and see",
    "you in the next video",
    "you"
  ],
  "keywords": [
    "difference",
    "descent",
    "everybody",
    "anything",
    "subtract",
    "opposite",
    "guns",
    "sucked",
    "reading",
    "scores",
    "build",
    "beginning",
    "book",
    "considering",
    "generate",
    "best",
    "subscribe",
    "distinguish",
    "probabilities",
    "45",
    "friend",
    "weight",
    "connection",
    "total",
    "parent",
    "viewers",
    "throughout",
    "catch",
    "represented",
    "machine",
    "generating",
    "end",
    "tend",
    "alejandra",
    "previously",
    "find",
    "ready",
    "along",
    "goal",
    "fixed",
    "special",
    "rubber",
    "likes",
    "predation",
    "forward",
    "needs",
    "blog",
    "neural",
    "gradient",
    "ganz",
    "normally",
    "close",
    "yuans",
    "built",
    "slash",
    "attention",
    "constantly",
    "coding",
    "obtain",
    "slanted",
    "seven",
    "positive",
    "na",
    "picking",
    "7",
    "computer",
    "learning",
    "content",
    "give",
    "videos",
    "encourage",
    "examples",
    "serve",
    "training",
    "simple",
    "random",
    "given",
    "tilted",
    "need",
    "intuition",
    "negative",
    "helped",
    "value",
    "github",
    "repo",
    "without",
    "share",
    "page",
    "generated",
    "two",
    "improving",
    "f",
    "thus",
    "lot",
    "predicted",
    "walks",
    "outputs",
    "update",
    "simply",
    "four",
    "briefly",
    "data",
    "fools",
    "generators",
    "wanted",
    "1",
    "places",
    "conclude",
    "error",
    "think",
    "world",
    "handle",
    "bias",
    "works",
    "updated",
    "got",
    "parameters",
    "one",
    "feeder",
    "summarize",
    "discriminated",
    "person",
    "cold",
    "resolution",
    "friends",
    "example",
    "discriminator",
    "course",
    "resident",
    "caught",
    "point",
    "well",
    "73",
    "necessarily",
    "free",
    "python",
    "define",
    "etc",
    "reasons",
    "easier",
    "strained",
    "setting",
    "found",
    "wants",
    "greater",
    "many",
    "measure",
    "functions",
    "read",
    "corresponding",
    "language",
    "second",
    "posts",
    "expressions",
    "gans",
    "whereas",
    "behave",
    "show",
    "appointment",
    "plots",
    "montreal",
    "slightly",
    "today",
    "decrease",
    "packages",
    "slight",
    "come",
    "story",
    "gon",
    "something",
    "big",
    "land",
    "video",
    "discriminative",
    "create",
    "gas",
    "faces",
    "take",
    "backward",
    "suggestions",
    "let",
    "using",
    "convince",
    "gets",
    "derivatives",
    "time",
    "4",
    "sing",
    "really",
    "great",
    "e",
    "add",
    "code",
    "cop",
    "result",
    "right",
    "log",
    "equations",
    "edges",
    "future",
    "layer",
    "check",
    "previous",
    "number",
    "label",
    "advance",
    "fake",
    "tiny",
    "discriminate",
    "black",
    "screen",
    "reason",
    "yet",
    "hand",
    "fight",
    "repeat",
    "coded",
    "ganzar",
    "may",
    "sigmoid",
    "look",
    "10",
    "respect",
    "goes",
    "graph",
    "things",
    "deep",
    "assigns",
    "type",
    "cutoff",
    "promised",
    "please",
    "tweet",
    "general",
    "classifies",
    "amongst",
    "also",
    "terrible",
    "especially",
    "generative",
    "apart",
    "cover",
    "greatest",
    "writings",
    "learns",
    "fellow",
    "remind",
    "perfect",
    "recommend",
    "dark",
    "subtracting",
    "series",
    "reduce",
    "go",
    "comment",
    "action",
    "see",
    "next",
    "want",
    "changes",
    "either",
    "corner",
    "pay",
    "math",
    "flow",
    "good",
    "small",
    "note",
    "idea",
    "hope",
    "convenient",
    "face",
    "x",
    "correct",
    "weights",
    "put",
    "proceed",
    "developed",
    "values",
    "ok",
    "40",
    "light",
    "scans",
    "generator",
    "understand",
    "scale",
    "usual",
    "channel",
    "straight",
    "always",
    "cost",
    "technology",
    "way",
    "hit",
    "get",
    "noticed",
    "back",
    "image",
    "meets",
    "happen",
    "feel",
    "elongated",
    "fully",
    "helping",
    "discount",
    "ways",
    "thank",
    "still",
    "smo",
    "twitter",
    "logarithm",
    "much",
    "possible",
    "case",
    "calculate",
    "identify",
    "counter",
    "50",
    "paint",
    "feedback",
    "generation",
    "bottom",
    "people",
    "chain",
    "randomly",
    "base",
    "direction",
    "collide",
    "network",
    "lingo",
    "explain",
    "easy",
    "mostly",
    "generates",
    "created",
    "say",
    "plot",
    "non",
    "match",
    "another",
    "gave",
    "endless",
    "corners",
    "ones",
    "learn",
    "enjoyed",
    "output",
    "degrees",
    "train",
    "extreme",
    "deeper",
    "luis",
    "since",
    "derivative",
    "able",
    "pixel",
    "short",
    "sahil",
    "various",
    "depending",
    "process",
    "actually",
    "consists",
    "clarity",
    "adversarial",
    "new",
    "called",
    "improve",
    "following",
    "thanks",
    "candidly",
    "calm",
    "link",
    "iterations",
    "fact",
    "probability",
    "less",
    "seriously",
    "music",
    "networks",
    "links",
    "diagonal",
    "thinks",
    "use",
    "know",
    "law",
    "function",
    "cup",
    "exist",
    "ask",
    "zero",
    "detailed",
    "pixels",
    "convention",
    "coming",
    "live",
    "performing",
    "matter",
    "enjoy",
    "2",
    "z",
    "real",
    "applications",
    "us",
    "defining",
    "description",
    "okay",
    "input",
    "g",
    "tell",
    "pair",
    "make",
    "backlash",
    "sends",
    "happens",
    "realistic",
    "meat",
    "therefore",
    "whole",
    "numbers",
    "appears",
    "top",
    "done",
    "plug",
    "outputting",
    "saying",
    "develop",
    "step",
    "rope",
    "numerous",
    "wildest",
    "thin",
    "notebook",
    "angle",
    "last",
    "probably",
    "like",
    "acknowledgments",
    "inspiration",
    "high",
    "formula",
    "write",
    "pictures",
    "particular",
    "calculation",
    "figured",
    "paintings",
    "work",
    "would",
    "drawn",
    "looks",
    "biases",
    "images",
    "far",
    "plus",
    "thick",
    "fool",
    "smaller",
    "tensor",
    "class",
    "loss",
    "every",
    "long",
    "0",
    "means",
    "games",
    "select",
    "keeps",
    "attach",
    "points",
    "perhaps",
    "set",
    "white",
    "first",
    "noisy",
    "minus",
    "diego",
    "degree",
    "successfully",
    "topics",
    "seen",
    "carefully",
    "based",
    "score",
    "serrano",
    "account",
    "lower",
    "distribution",
    "help",
    "higher",
    "fanciest",
    "order",
    "going",
    "highly",
    "remember",
    "hello",
    "closer",
    "apply",
    "dreams",
    "prediction",
    "eyeballing",
    "day",
    "screens",
    "pick",
    "taking",
    "informations",
    "finally",
    "fascinating",
    "leopard",
    "pause",
    "vector",
    "trying",
    "propagation",
    "scratch",
    "academy",
    "threshold",
    "start",
    "building",
    "similar",
    "large",
    "defined",
    "picture",
    "words",
    "adding",
    "similarly",
    "stabilizes",
    "corresponds",
    "bad",
    "rule",
    "question",
    "noise",
    "questions",
    "times",
    "left",
    "notice",
    "comes",
    "produce",
    "classifying",
    "larger",
    "completely",
    "idn",
    "enter",
    "display",
    "different",
    "natural",
    "growth",
    "low",
    "blue",
    "news",
    "arrows",
    "rocking",
    "epochs",
    "little",
    "bit",
    "torch",
    "tube",
    "pass",
    "concepts",
    "classified",
    "lead",
    "researches",
    "calculating",
    "multiplied"
  ]
}