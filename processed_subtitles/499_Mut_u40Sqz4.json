{
  "text": "hey nick what you've been working on oh\nman i've been working on some awesome\nstuff i'm\nactually using reinforcement learning to\ntrain a race car to race around the\ntrack\noh really how's that going yeah it's\ngoing great\nyup great at doing burnouts\n[Music]\ni promise guys it does get better than\nthis let's get to it\nwhat's happening guys my name is\nnicholas chernotte and welcome to the\nreinforcement learning course\nin this video we're going to be covering\na bunch of stuff but basically the core\ngoal is to be able to allow you to go\nfrom\nabsolute beginner to being able to go\nand leverage reinforcement learning\nwe're going to cover a ton of stuff\nspecifically how to set up your\nenvironment\nhow to work with different algorithms\nwe'll also test out on some pre-built\nenvironments using open ai\ngym so you'll be able to balance a cart\npole you'll be able to build your own\nself-driving car\nand then last but not least we're also\ngoing to take a look at how to build\ncustom\nenvironments something which is so so\nimportant when it comes to being able to\nleverage reinforcement learning\nfor a use case which is relevant for you\nbut all in all by the end of this video\nyou should be able to take away those\nskill sets and be\nable to leverage reinforcement learning\nin a practical manner\nready to do it let's get to it alrighty\nguys\nwelcome to the full-blown reinforcement\nlearning course\nnow this course is intended to be a\npractical guide\nin terms of getting up and running with\nreinforcement learning so ideally it\naims to bridge the gap\nbetween a lot of the theory that you see\nout there and practical implementation\nnow we're going to be covering a ton of\nstuff in this course\nso let's take a look at our game plan so\nfirst up what we're going to be doing is\nwe're going to be taking a look at rl in\na nutshell\nand this really talks about and\nspecifically in this section\nwe're going to be talking about how\nreinforcement learning works and learns\nsome of the applications around rl as\nwell as some of the limitations\nthen we're going to take a look at how\nyou can set up your environment to work\nwith reinforcement learning\nand then we're going to be using a\nlibrary called stable baselines\nthen under step 2 we're going to be\ntaking a look at environments so\nenvironments are one half of the\nequation when it comes to working with\nreinforcement learning so we need to be\nable\nto set up an environment and\nspecifically open ai gym environments to\nbe able to work with reinforcement\nlearning\nthen we're going to kick off our\ntraining so there's a whole bunch of\ndifferent types of algorithms available\ninside of stable baselines\nso we're going to take a look at how we\ncan set up some algorithms to be able to\ntrain a reinforcement learning agent\nthen under step 4 we're then so once\nwe've trained our model we're then going\nto test it out and evaluate it\nso this is easier than it sounds so you\ncan set up an environment and test it\nout and see what your agent actually\nlooks like\nthen we're also going to take a look at\nevaluation as well as how you can take a\nlook at different metrics how to\nunderstand those metrics and we'll also\ntake a look at how we can open them up\ninside a tensorboard something which i\nreally really like\nthen we'll take it one step further so\nstep five we'll take a look at how we\ncan leverage callbacks\nto stop our model trading once we hit a\ncertain threshold\nwe'll see how we can use different\nalgorithms so there's a whole bunch of\nalgorithms available in reinforcement\nlearning you don't need to write them\nyourself there's a whole bunch\nalready written for you that you can use\nand we'll take a look how we can use\nthose\nand then we'll also take a look at\ndifferent architectures so say for\nexample you wanted to change the neural\nnetwork that sits behind a particular\nagent\nyou can do that as well but this\nwouldn't be a full-blown course unless\nwe had some projects as well so we're\ngoing to be taking a look at\nthree different projects so we're going\nto take a look at how we can solve\nthe breakout environment which is an\natari game so it's it's sort of like\npong a little bit but\nnot really we'll also take a look at how\nwe can solve a self-driving environment\nso that's a car racing environment\nand how we can train our model to only\nhave a picture as an input\nand train our car to drive along a\nracetrack which i think is pretty\nawesome\nand then we'll also take a look at\ncustom environments something which i\nthink is\nso so often overlooked so this will\nallow you to get a better understanding\nof how to build an environment to work\nwith reinforcement learning\nnow the framework that we're going to be\nusing when we build our customer\nenvironment is going to be open ai gym\nso i'm going to show you all the\ndifferent types\nof spaces don't worry if you don't\nunderstand that yet or if you're not\ntoo sure what i'm talking about we'll go\nthrough it in great detail\nokay that's a game plan in a nutshell\nnow it's time to take a look\nat irl in a nutshell so i wanted to\ninclude this section to give you a\nlittle bit of context about what\nreinforcement learning is\nhow it's meant to be used and some of\nits applications as well as some of its\nlimitations this is\nnot going to be a full deep dive into\nthe theory and the maths behind it\nit's just a high level overview so you\nget an idea as to where rl fits in in\nthe big world of machine learning and\ndata science\nso first up what is reinforcement\nlearning well\nreinforcement learning focuses on\nteaching agents through trial and error\nthat's a really really high level\nstatement now i know there's probably a\nlot of hardcore deep learning engineers\nthat will probably go nick that's not\nquite right\nbut it sort of gives you an idea as to\nhow reinforcement learning learns\nideally you've got an agent and it\nlearns based on the reward that it gets\nso try something out if it doesn't get a\nreward then it tries something else if\nit doesn't get a reward or it gets a\nbigger reward it might try doing that\nmultiple times\nwe've also got this thing called the\nexploration exploitation trade-off so\nagain i'll talk about that a little bit\nlater\nbut you sort of get the idea\nreinforcement learning is learning and\nbased on actively engaging with an\nenvironment\nnow that brings us to how the framework\nactually fits together\nwell there's four key things or well\nfive key things that you need to\nconsider whenever you're working within\nreinforcement learning or there's four\nfundamental concepts\nso they are the agent the environment\nthe action and then reward plus\nobservations\nso think of your agent as something\nwhich is operating within an environment\nso this might be a machine learning\nmodel\nmight also be a person or a player if\nyou're working in a game environment\nyour environment is where that\nparticular agent is actually operating\nin so in this case say for example if we\ntake a game\nso your player is operating within the\ngame environment so\nit's getting reward based on what it\nactually does there\nnow your agent will see what's happening\nwithin that environment so say for\nexample\nwe're taking a look at a game your\nplayer will be able to see\nwhat's around them so in terms of the\nobservation so it'll see what the game\nenvironment actually looks like\nand then it'll also see what reward it\naccrues based on the actions it takes\nso ideally your agent might walk around\nthe environment it might do something\nand might accumulate a point\nmight do something else it might not\naccumulate a point it might even lose a\nlife that might be a negative reward\na really really good way to sort of get\nyour head around this is to think of how\nyou might go about training a dog\nsay for example you wanted to teach your\ndog how to sit or how to lay down\nwell your agent in this case is going to\nbe your dog because you're trying to\ntrain your agent to be able to take the\nright action\nnow the reward in this case is you\ngiving your dog a treat every time they\ndo the right thing\nso what your dog might try to do is take\nan action so\ninitially you might say sit and the dog\nmight not actually do anything so in\nthis case it hasn't actually taken it or\nit's taken an action of doing nothing\nand in this particular case the\nenvironment that it's working with is\nthe environment with\nyourself in it so in it's trying to get\na reward\nor trying to get a treat from doing a\nparticular thing\nnow your dog will eventually see that it\ngets no reward because it didn't sit\ndown\nso it might try something else so in\nthis case you might say sit again\nit might then sit and then it'll say\nthat it'll get a reward so ideally\nit will then start to learn what action\nto take in response to the environment\nin order to maximize the reward\nso it's observing the command that\nyou're giving to be able to take the\nright action\nso this in a nutshell is how\nreinforcement learning works your agent\ntries to take an\naction in order to maximize its rewards\nin response to the observations\nwithin the environment now again i just\nwanted to give you a little bit of\ntheory we're not going to delve into\nthis too much but you sort of get the\nidea as to how reinforcement learning\nworks\nit's a little bit different in terms of\nhow you might work with tabular deep\nlearning and machine learning\nbecause your agent is actively engaging\nwith a simulated or a real\nenvironment now in this case we're going\nto be dealing with simulated\nenvironments but i'll talk a little bit\nabout that later\nso what are some applications or\npractical applications\nof reinforcement learning well there's a\nwhole heap out there and there's only\nbecoming more reinforcement learning is\nreally really popular right now because\nthere's a\nwhole heap of open world environments\nthat people are trying to solve using\nmachine learning and deep learning\none of which is autonomous driving so\nyou can see\nthis picture up here this is actually\nfrom an environment called kala\nso kala is a really really popular\ndriving simulation which\nallows you to actually train autonomous\nagents or perform reinforcement learning\non it\nnow you can actually train a car to be\nable to navigate through an open world\nusing reinforcement learning it's pretty\npretty cool right now another great\napplication of reinforcement learning is\nsecurities trading so again\nthink of this so your agent in this case\nwill be like an autonomous trader your\nenvironment is going to be the\nsecurities trading environment so\nideally what you're going to do is\nyou're going to try to train your agent\nto make trades that are going to make\nyou profit so ideally it wants to\nbuy low and sell high and sell high and\nbuy low so if it's short selling\nagain this is really really popular at\nthe moment there's a heap of stuff\nhappening\nin that space another one which i'm\npersonally fascinated by is a neural\nnetwork architecture search\nso what you can actually do is use\nreinforcement learning to build up a\nneural network for you and\nfind an optimal neural network which i\nthink is absolutely crazy\nso say for example you're trying to\nbuild a deep neural network to solve a\nparticular use case you might not know\nwhat the best type of architecture\nin terms of layers in terms of number of\nunits or in times of activations is\nyou could actually use reinforcement\nlearning to try to solve this problem\nfor you now this is obviously super\nadvanced\nbut it sort of gives you an idea as to\nwhat's possible with the tech\nanother place that reinforcement\nlearning is super popular right now is\nin robotics so training agents or\ntraining robots in\nreal life can often be quite expensive\nbecause say for example you've only got\none robot can be hard to train\non a lot of tasks so what you can\nactually do is build up\nsimulated environments of that\nparticular robot and train that robot to\ndo\na particular thing now in this case the\nagent is going to be\nthe autonomous model which is training\nthe robot\nthe environment that it's operating in\nin this case this agent i believe is\ntrying to move a ball to the correct\nposition\nthis is actually based on a simulation\nenvironment called mujocho\nso again i'll show you that a little bit\nlater but we're not going to be solving\nthat one today\nbut you sort of get the idea so we can\nactually train the robot the environment\nis going to be moving the ball to the\nright place\nand the reward is going to be how close\nor how far that ball is from its optimal\nposition\nso again there are a whole heap of\napplications i've only sort of shown\nfour there but there are a ton out there\nanother place where it's really really\npopular is in gaming so again\ngaming is an open world environment so\nthe reward function can be really really\ndifferent each and every time\nyou can sort of see how it can start to\napply into different environments\nokay so what about some limitations and\nconsiderations for reinforcement\nlearning\nso again reinforcement learning is\nabsolutely amazing and i'm fascinated by\nit but there are\nsome limitations specifically for simple\nproblems reinforcement learning can\nsometimes be overkill\nso say for example we're taking a look\nat hyper parameter optimization there's\nalready really really powerful models\nfor that\nparticularly when you're dealing with\nsimple models but if you're getting to\nsuper advanced problems reinforcement\nlearning could help you out in that\nspace\nanother thing that it assumes is that\nthe environment is markovian that means\nyour future states for your environment\nare based on your current observations\nand there's no random acts but we know\nin real life that there's going to be\nrandom events that happen that\ninfluence our particular model so say\nfor example you were training your\nmujoko robot right\nin this particular case your environment\nmight not cater to people walking past\nthe robot or knocking the robot so\nyou never really know what's going to\nhappen in real life you can only train\nin your best case scenario so again\nwe can sort of deal with this because in\nour reinforcement learning model we're\ngoing to sort of isolate our environment\nbut again it's just something to take in\nmind when somebody asks you that\nquestion\nanother thing to note is that training\ncan take a long time and is not always\nstable so\nwe've got this concept called the\nexploration and exploitation trade-off\nso ideally what your model tries to do\nis explore the environment when it's\nstarting\nout and then it tries to exploit it to\nbe able to get the best possible rewards\nbut sometimes what might happen is your\nmodel might not have enough time to\nexplore\nand might start exploiting too early so\nsometimes we need to tune hyper\nparameters\nto be able to get our model to truly\nexplore the environment and truly\nunderstand it\nsometimes because we don't get this\nquite right our model might not all be\nthat stable so we might\nget to a certain point we might reach a\ncap in terms of our maximum reward but\nnow another thing to note as well is\nthat training can take a long time so if\nyou've got a really really open\nenvironment so say for example you're\ntrying to train\na reinforcement learning model for grand\ntheft auto because it's such a huge\nenvironment\ntraining a model to sort of work out\nwhat to do in that particular case is\ngoing to take a long\nlong time all right now\nnot to be down i just sort of wanted to\nbring up some of those limitations and\nconsiderations now\non that note let's start getting onto\nour setup\nso step number one is going to be setup\nwhat we're first i'm going to do is\ninstall our required dependencies now in\nthis case it's really really simple to\nget up and running with this\njust a single pip install all you need\nto do is run exclamation mark pip\ninstall stable dash baselines three and\nthen\ninsider square brackets pass through\nextra so stable baselines is a\nreinforcement learning\nlibrary that allows you to work with\nmodel free algorithms but again we'll\ntalk about that later\nso we can work with stable baselines to\nbuild up a reinforcement learning agent\nto be able to train against a specific\nenvironment\nnow the cool thing about stable\nbaselines is that it's actually based\noff an original library called\nbaselines which was built by open ai the\ngreat thing about stable bass lines is\nthat there's a\nwhole heap of really really useful\nhelpers now i've got the documentation\non the screen so this is the full\nthis is the migration link but if you\nwanted to go into stable baselines i'm\ngoing to include these links in the\ndescription below as well as\nall the code that you're seeing in here\nbut you can see here that stable\nbaselines\nactually is or this is the documentation\nthere's a whole heap\nof guides and it's a really really well\nsupported environment or\nreally well supported library as well so\nagain really really really useful\num there's a whole bunch of getting\nstarted information if you want to be\nable to leverage that\nsort of shows you how to get started\nreally really quickly so this here is\none\nsingle reinforcement learning\nenvironment and training in a single\nwhat is that like 40 20 lines of code so\nagain you can get started really really\nquickly with this\nbut we're going to be going through all\nof this in great detail as we're walking\nthrough it\nso let's kick things off and start by\ninstalling stable baselines\nso i'm going to be working inside of a\njupiter notebook environment for this\nand i'm going to give you the baseline\ncode or the starter code as well as the\ncompleted code as well\ninside of the github repo in the\ndescription below so you'll be able to\npick up all of this and work with it\nat your own pace so first things first\nwe're going to have\n10 different steps that we're going to\nbe going through for our main\ntutorial and then we're going to have\nour projects as well so the first thing\nthat we need to do\nwell let's actually take a look at these\n10 steps so first up what we're going to\ndo is import our dependencies\nthen we're going to load up our\nenvironment so in this case we're going\nto be solving a reasonably simple\nenvironment called cartpole and i'll\nshow you that in a sec\nwe're going to take a look at how to\nunderstand an environment because that\nis so\nso important then we'll train a\nreinforcement learning model i'll show\nyou how to save it down to disk and\nreload it so if you wanted to go\nand move it elsewhere or go and deploy\nit you could do that\nwe'll take a look at how to evaluate it\nhow to test it how to view our logs\ninside a tensorboard\nhow to add a call back to the training\nstage so this allows you to stop your\ntraining at a certain point once you're\nhappy with it\nhow to change policies as well as how to\nuse an alternate algorithm\nso we're going to be covering quite a\nfair bit but again take it\nat your own pace and if you get stuck or\nif you have any questions at all\nhit me up in the comments below or join\nthe discord server again link will be in\nthe description below\nalways happy to chat there as well all\nright enough on that\nlet's actually kick this thing off and\nwrite some code so the first thing that\nwe're going to do\nis install our dependencies and import\nthem so in this case we're going to be\ninstalling stable baselines three so\nremember we had\nexclamation mark pip install stable dash\nbaseline street and then in square\nbrackets extra so let's go ahead and\nwrite that alrighty that looks like it's\nall installed successfully so you can\nsee we don't we've got a warning there\nthat says to upgrade pip\nthat's fine don't worry about it but it\nlooks like we're all good to go\nnow in this case that is now done so\nthat again really\nsimple to get started with stable bass\nlines it's a single pip install\nbut again there's so much you can do\nwith it which makes it pretty cool\nso the next thing that we want to do oh\nlet's actually take a look at that line\nso we've written exclamation mark\npip install and then stable dash\nwhich i'm just going to screw that up\nstable dash\nbaselines and then three and then extra\nnow the reason that we're passing\nthrough three is that stable baselines\nhas gone through a number of iterations\nso\nthere was a stable bass lines one and\nthen a stable bass lines two we're now\nup to stable bass lines\nthree so this is the latest package\nagain which runs on tensorflow and\npytorch we're going to be using pytorch\nfor this\nbut just uh something to keep in mind so\nthat's the reason that we're passing\nthrough the three\nall right now that's our installation\ndone we're all good to go\nnow the next thing that we want to do is\nactually import some stuff so let's go\nahead and\nimport some dependencies and then i'll\ntalk you through each one of those\nokay so those are our main dependencies\nnow imported so we've gotten written\nfive lines of code there\nso first up what we've written is import\nos so os is just an operating system\nlibrary that's going to make it a little\nbit easier later on when we go to define\nour paths\nto save our model as well as where to\nlog out\nthen we've imported jim so jim is for\nopen ai gym but i'm going to talk about\nthis a little more once we get into our\nenvironment section of our slides\nso jim allows us to build environments\nand work with pre-existing environments\nreally really easily\nthen we've imported our first algorithm\nso we've actually imported ppo\nso to do that we've written from stable\nunderscore baselines three\nimport ppo and again there's a whole\nheap of different types of algorithms so\nif we actually take a look\nwe're actually taking a look at the\nstable baselines package that should be\nstable baselines three\nso if you actually take a look there's a\nwhole heap of different algorithms there\nso there's a2c\nddpg dqn her ppo\nsac and td3 now again there's a whole\nbunch of stuff here so i'm actually\ngoing to talk about\nwhen to use which algorithm um and under\nwhich circumstances so again\ndon't fret if you've seen this and\nyou're like oh my god there's so much\nwe're actually going to go through this\nand i'll actually give you a little bit\nof a guide or\nat least some guide rails as to when to\nuse which particular type of algorithm\nbut in this case we're going to be using\nthis one here so ppo so again if you\nwant to see\nthe documentation it's all there and you\ncan take a look at the performance of\nthat particular\nalgorithm cool right\nokay so that's this line here so from\nstable underscore baselines import ppo\nand then the next one that we've written\nis from stable underscore baselines\nthree\ndot common dot vec underscore env import\ndummy vec env\nnow i'll talk about this a little more\nonce we get to the breakout tutorial but\nbasically\nstable baselines allows you to vectorize\nenvironments this means that it allows\nyou to train\nyour machine learning model or train\nyour reinforcement learning agent on\nmultiple agents at the same or multiple\nenvironments at the same time\nthis means that you can get a huge boost\nin\nyour training speed by doing that now in\nthis case we're not going to be\nvectorizing our environment so we're\ngoing to be able to use\nthis dummy vec env wrapper instead\nyou'll see what it actually looks like\nwhen we do vectorize in the breakout\nproject but again for now just think of\nthis as a wrapper around your\nenvironment\nmakes it easier to work with stable bass\nlines then the next thing that we've\nwritten\nis from stable underscore baselines\nthree dot common dot\nevaluation import evaluate underscore\npolicy so evaluate underscore policy\nmakes it easier to test out how model is\nactually performing so what you'll\nactually get when we run this\nis the average reward over a certain\nnumber of episodes again i'll talk about\nit more later and you'll also get the\nstandard deviation\nfor that particular agent that you're\ntraining\nso again five lines of code so import os\nimport gym import our algorithm which is\nppo import our dummy vec wrapper and\nimport our evaluate policy helper\nwhich will be used somewhere around here\ncool that\nis pretty much it in terms of our\ndependency so we've written five lines\nof code\nnow on to step two environments\nso i think a key thing to call that is\nthe difference between simulated and\nreal environments now this is why we're\nusing open ai gym so open ai gym allows\nyou to build\nsimulated environments really really\neasily so there's a whole heap of\nhelpers\nit's a really well supported library and\nthat's particularly why it's\nreally really popular when it comes to\nworking with reinforcement learning\nnow a key thing to call out is that when\nwe're working with\nreinforcement learning often one of the\nbenefits is that\nby using simulated environments we're\nable to reduce cost and we're able to\nproduce better models a whole heap\nfaster\nnow say for example you're working for\nan engineering company and your\nengineering company wants to build an\nautonomous\nagent to go and train this robot over\nhere to be able to move\na particular ball to a certain position\nnow this robot is actually called a\nfetch row but it's actually a real robot\nso you can actually take a look at what\nit looks like\nnow they might only be able to afford a\nsingle robot\nso this sort of limits how fast they may\nbe able to train that particular robot\nand obviously there's costs involved\nwith actually training that robot so\nyou're going to be wearing down the\njoints\nyou're going to be using electricity\nit's going to take a lot more time and\ncost to be able to train\nthat robot if you're doing it in a real\nenvironment now one of the amazing\nthings about reinforcement learning is\nthat you can try to simulate that\nenvironment to be able to train the\nagent in the same way\nover here you can see that this is\nactually a replica\nof this robot and it's actually built\ninside of a simulation tool called\nmojoko so again i talked about this a\nlittle bit earlier but this makes it\na whole heap easier to train and a whole\nheap more cost effective to be able to\ngo ahead and train\nyour agent which is actually pretty cool\nbecause i mean this technology hasn't\nbeen around for a\nwhole heap of time but it obviously\nimproves the ability for\npeople to leverage reinforcement\nlearning so rather than having to go and\ndo it in real or\nin real time on that particular agent\nthey can do it in a simulated\nenvironment\nand run it on there but again ultimately\nwhat you may find is that\nwhilst we may train on a simulated\nenvironment the end goal is to take that\nagent and go and deploy it on a\nproduction-like environment\nwhich would be a real robot likewise if\nyou're doing it on a game you might\ntrain on\na testing version of the game and you\nmight deploy on a real version of the\ngame so you sort of get the idea between\na simulated and a real environment\nthis is simulated this is going to be\nreal\nnow this is where open ai gym comes in\nso open ai gym gives you a really\nlightweight environment but really\nfeature packed to be able to go\nand build out a reinforcement learning\nenvironment now in this case\nyou can actually take a look at the docs\nso it's at https colon forward slash\nforward slash\njim.openai forward slash docs so if we\nactually go to that link\nwhich is over here we can go to docs you\ncan see that there's a whole heap\nof documentation around how to actually\nuse open ai gym\nand the nice thing about this and\nparticularly why i've used this\nparticular environment or framework\nis that there's a whole heap of support\nso it's really really well supported a\nlot of people are using this when it\ncomes to open air gym\nso know when you're looking at cutting\nedge stuff or you're looking at\nwhat skills to learn if you wanted to go\nand do this for your career\nopen ai gym tends to be the standard in\nthis particular space\nnow there's a whole heap of pre-built\nenvironments that you can actually use\ninside of openai gym so remember i was\ntalking about mujoko for that particular\nrobot so you can actually oh it's might\nnot be mujo\nit might be under robotics so you can\nsee that we've actually got\nour fetch robot over here there's also\nthis shadow hand\nrobot now these are actually based on\nreal robots so if you actually google\nfetch robot that's not what i meant to\ntype a\nfetch robot\nyou can actually see so it's actually a\nreal robot that i'm actually showing you\nhere so\nthis robot is exactly mimicking this\nthis robot this hand one is actually\ncalled a shadow hand\nrobot shadow and robot\ni believe there you go so you can\nactually see these are actually based on\nreal robots that are out there in the\nreal world so people are trying to train\nthem\nusing open ai gym now there's also a\nbunch of environments around algorithms\naround\natari which we'll do a little bit later\naround box 2d\nso we're actually going to be testing\nout this one classic control\nso we're going to be testing out cartpol\nmujoko\nrobotics toy text so on and so forth\nthere's a whole heap\nthere's also a whole heap of third-party\nenvironments so if you wanted to do\nsomething really really\nhardcore you could definitely take a\nlook at those as well so i\nremember i was talking about carla i\nbelieve there's one over here so there's\nactually a wrapper to be able to\nleverage color as part of\nopen ai gym now in this case we are\ngoing to be dealing with classic control\nto begin with so we're going to keep\nthis relatively simple and try to solve\nthe carpol environment\nso if we actually take a look at this\nthe goal in this particular case\nis to get this little robot down here to\nbe able to balance\nthis beam now you can see right now it's\nsort of bumping around side to side and\nthe beam is sort of falling over\nnow there's two actions that we can\nreally take we can move it to the left\nor we can move the cut to the right\nbut again i'll delve into this a little\nbit more so what we're going to be able\nto do is train a reinforcement learning\nagent to be able to solve that\nparticular problem\nnow what we're going to do next up is\nwe're actually going to be taking a look\nat what that environment actually looks\nlike\nnow key thing to note is that when\nyou're actually taking a look at open ai\ngym\nenvironments is that these environments\nare represented by something called\nspaces\nthere are a number of different types of\nspaces\nthat open ai gym supports now the names\nmight be a little bit tricky when it\ncomes to actually leveraging them but\nlet me sort of walk you through them so\nthe first one is\nbox now this is a range of values so\nthink of\nsay for example you wanted a continuous\nvalue you're going to want to use a box\nspace so the way to instantiate a box\nspace is by using box\nand then passing through the low value\nthe high value and the shape of the\nspace\nagain i'm going to delve into this a\nwhole heap more when we actually take a\nlook at\nour environment and we're actually going\nto use some of these spaces to actually\nbuild up our own custom environment in\nproject\n3. the next type of space is discrete so\nthis is just a set of items\nso if i type in discrete and then pass\nthrough the value three\nwhat you're actually going to get back\nin terms of your space is the values 0\n1 and 2. so it's actually going to give\nyou discrete numbers\nthat represent specific mappings to\nsomething so typically you'll see\ndiscrete actions used\nfor or typically you'll see discrete\nspaces used for actions\nso action zero will be something action\none will be something and action two\nwill be something else\nuh you've also got tuples so tuple\nallows you to combine\nspaces together so you can see we can\nuse tuple and then pass through discrete\nand box\nso this just allows you to join them key\nthing to note is that stable baselines\ndoesn't support tupor so again good to\nknow but you're not going to use it all\nthat much\nyou've also got dict spaces so this is\njust a dictionary of spaces so really\nsimilar to two balls but in this case\nwe're just declaring dict\nand then we're passing through a\ndictionary of spaces the other two types\nof spaces these are ones that i haven't\ndealt with too much but it's important\nto note that they're there so you've got\na multi-binary space so this is a one\nhot encoded\nset of binary values so if you pass\nthrough multi binary and pass through\nthe value four\nwhat you're going to get is a list of\nvalues and you're going to have\nfour positions so you'll have zero one\ntwo three so ideally four values\nand you're just effectively going to\nhave binary flags so ones or zeros\nrepresented in those positions so it's a\none hot encoded vector\nof different actions or different spaces\nyou've also got\nmulti-discrete so this is very similar\nto our discrete space but in this case\nyou can\nhave uh multiple sets of values so\nyou'll have 0 1 so if i pass through 5 2\n2 what i'll get back is a range of\nvalues between\n0 and 4 for the first position 0 and 1\nfor the second position\nand 0 and one for the third position so\nagain you can start to see how these\nspaces sort of play\nbut enough on that let's actually take a\nlook and start building our environment\nso back to our notebook what we're going\nto do now is start loading up our\nenvironment so first up what we're going\nto do\nis we're going to use open ai gym to\ninstantiate our environment and then\nwe'll actually test it out and take a\nlook at it so\nlet's first upload our environment\nokay that is two lines of code to be\nable to go and create our environment\nnow i've gone and separated it out into\ntwo lines of code but you can make it\none and i'll\nexplain this so the first line of code\nthat we've written is environment\nunderscore name\nequals cart pole dash v0 now this is\ncase sensitive so make sure you get the\ncase correct so\nwe've got a capital c-a-r-t p-o-l-e\ndash v-0 so this environment name is\njust\na mapping to the open or pre-installed\nopen ai gym environments\nthen what we're doing is we're actually\nmaking our environments we've written\nemv\nequals gym dot make and then to that\nwill pass through our environment name\nvariable so again if we actually just\nprinted out the environment name\nvariable it's just going to be a string\nright\nenvironment name\njust a string right nothing crazy there\nnow what we can actually do is we can\nactually test out this environment so\nremember\nwhat we're going to do initially is just\ntake random actions in that environment\nbut eventually what we're going to do is\nwe're going to take our agent and\nspecifically our reinforcement learning\nagent\nand try to get it to take the right\nactions in that particular environment\nto maximize our reward that's what\nreinforcement learning is all about\nso what we want to do first up is sort\nof get an understanding of the\nenvironment it's really really important\nto understand what's actually happening\nin that environment\nbefore you try to do anything because\nyou might be trying the wrong algorithms\nyou might be doing\na whole bunch of random stuff but\nunderstanding the environment is going\nto make your life\nso much easier trust me on this so let's\ngo on ahead and write a bit of a loop\nto test out our environment so let's go\ndo this\nalrighty so i've written a lot of code\nthere but i'm going to take it step by\nstep with you so again we're going to\ntake this step by step whenever we're\ngoing through this stuff\nso what is that so one two three four\nfive\nsix seven eight 9 10 11 12\n12 lines of code now again all of this\ncode including the beginner as well as\nthe completed tutorials are going to be\navailable in the github description\nbelow\nso if you want to take this and walk\nthrough it and compare your code you can\ndo that\nfirst up what we're going to do is walk\nthrough each step or each line of this\ncode\nso what we're actually going to be doing\nis we're going to be\ntrying to test out the carpol\nenvironment five\ntimes now what we've actually gone and\ndone is created a variable called\nepisodes and we've set that to five so\nthis means that we're going to try to\nloop through our environment\nfive times to see how we can operate\nwithin it so we've written episodes\nequals five and then we're looping\nthrough each one of those episodes\nso for episode in range\nand then we're starting off at one and\nthen we're going episode plus one so\nthis is effectively just looping through\neach one of the episodes\nif we actually write this out to\nepisodes\nequals fives for episode in\nrange one comma episodes\nplus one let's print out our episode\nso you can see that it's just going to\nbe looping through one to five right so\nthat's all\nthese two lines are doing here then what\nwe're doing is we're resetting our\nenvironment so by running env.reset\nwe're going to get an initial set of\nobservations remember\nthere was those five key components to\nany environment or four key components\nthere was the agent the action the\nenvironment and then the observations\nplus the rewards\nso by running env.reset we're going to\nget our initial set of observations so\nif i type in emb.reset\nyou can see that these are the\nobservations for a particular\nenvironment now i'll talk about what\nthese values mean in a second\nonce we actually get to understanding\nthe environment but for now just\nunderstand that these\nare the observations that we get for our\nparticular pole right\nso we're getting these four values now\nwhat we're effectively going to be doing\nis passing\nthese observations or later on we'll\npass these observations to our\nreinforcement learning agent\nto determine what the best type of\naction is to be able to maximize our\nreward\nso our agents going to see these values\nand it's going to go hey i've got these\nvalues\nwhat should i do or what action should i\ntake to be able to maximize my reward\nand get that bar\nin the straightest possible position\nthen over here we're just setting up\nsome temporary variables so we're\nsetting whether or not the episode is\ndone\nso you've got a maximum number of steps\nwithin this particular environment\nand we're also setting up a running\nscore counter across the episodes\nthen we've got a while loop so while not\ndone we're then going to render our\nenvironment so\nthe render function allows us to view\nthe environment or view the graphical\nrepresentation of that environment\nkey thing to call out is if you're\nrunning this inside a collab\nthe render function is not going to work\nlike this you've got to do a little bit\nof extra work so hit me up in the\ncomments below\nif you want a little bit of help with\nthat then what we're doing is we're\ngenerating a random action so rather\nthan taking\nin our observations and actually\ngenerating an action which is actually\nuseful\nwe're just going to take a random one so\nthis is akin to doing this so i can just\ntype in let's actually move this down\nso what would be doing is emv dot action\nspace\ndot sample so we're just generating a\nrandom action\nthis is actually really good to note as\nwell so if i actually take off sample\nremember how i was talking about the\ndifferent types of action spaces\nin this case here we've got discrete is\ntwo so this will mean that we get two\ndifferent types of action so we've got\nzero or one so if we actually type in\ndot sample\nyou can see we've got one this time got\none got one\ngot zero so you can see our action space\nis just going to have those two\ndifferent actions zero or one this is\nwhat discrete two\nwhich is what our action space looks\nlike represents\nnow we can actually take a look at our\nobservation space as well this is a key\nthing to call that\nso there's going to be two different\nspaces within any environment\nyour action space so these are the\nactions you can take on that environment\nand your observation space so this is\nwhat your observations actually look\nlike for that particular environment so\nit's a partial view\nso if we type in observation space you\ncan see\nthat we've actually got a box\nenvironment so i've got these values\nhere so that's going to be a lower bound\nand this is going to be our upper bound\nand then we've got four comma so this\nmeans that we're going to have four\nvalues so zero\noh zero one two and one two three\nfour and then they're gonna be in the d\ntype float32 so again you can start to\nsee how\nour environment is actually built up now\nagain we can sample this if we wanted to\nand this is going to look almost\nidentical to what we get from enb\ndot reset up here then\nwhat we can actually do is we can\nactually pass through our random action\nso this is the next line that we've\nwritten to our environment so we can do\nthis using a and v\ndot step so if we actually did that so\namv step and pass through the values\n1 you can see we're going to get our\nobservation back so again we can keep\ndoing this\nand this is really just us passing\nthrough our action\nnow what we actually get back from this\nis actually really really interesting\nso we're going to get back our next set\nof observations which are what you can\nsee\nthere and we're also going to get a\nreward\nso this is whether or not we're getting\na positive value or a negative value so\none is obviously\nincrement zero is going to be a\ndecrement or negative one is going to be\na decrement\nand then true is basically specifying\nwhether or not our episode is done so\nremember\nwe've got this done statement here and\nthis done statement up here so once our\nepisode is done\nwe're going to stop it so that full line\nof code is\nn underscore state comma reward comma\ndone\ncomma info so this is just unpacking the\nvalues that we get from env.step\nand then the next line of code is just\naccumulating a reward so score plus\nequals reward\nand then we're just printing out the\nresults that we actually get from taking\nthat step\nso we've written print and then open\nquotes episode\ncolon squiggly brackets score colon\nsquiggly brackets i call them squiggly\nbrackets\ndot format and then we're passing\nthrough our episode and our score and\nthen last but not least we're closing\nour environment so when we use\nenv.render you'll get this python pop-up\nto close it down you just need to run\nenv.close\ncool so that is all well and good let's\nactually test this out so if we run this\nnow\nyou should see down the bottom that's\nour environment testing itself out now\nif we didn't want to close it we can\njust comment this line down here so we\ncan actually see it\nand there you sort of go so again it's\nrunning really really quickly and it's\njust sort of moving the bar around\nwhen we actually go to test it out we'll\nsee it run a little bit more slowly\nbut you can sort of see so if we keep\nrunning it\nlooks like we've screwed it up let's\nactually close it say amv dot close\nand then it's closed down so let's run\nit again\nthen we can close it you can start to\nsee what's sort of happening there right\nour actions are moving this black box to\nthe left and to the right\nand our bar is effectively swaying based\non\nresponse to that so ideally the goal is\nto hold that bar as straight as possible\nfor as long as possible\ncool all right so that is a whole bunch\nof stuff now\ndone now we've taken a look at how we\ncan sample our environment so let's\nactually take a look at that in a little\nbit more detail\nso remember there's two parts to our\nenvironment there's our action space and\nour observation space so if we type in\nenv dot action space\nthat's going to be our actions and then\nwe can type in emv dot observation space\nand those are going to be our\nobservations now you're probably\nthinking well nick\nwhat are these values that we get from\nthese observation spaces so\nand action spaces so let's actually type\nthis so if we write dot sample\nand dot sample to the end of these\nyou can see that we've got these values\nso let's actually duplicate this so\nwe've got both so amv\ndot action space\nand emb dot observation space\nright so this is describing the type of\naction space this is actually an example\ntype of observation space and then this\nis an actual example\nnow we can actually take a look at what\nthese represent so i've actually got\nthis link here\nwhich actually gives you a little bit\nmore detail so\nin terms of that observation so this is\nactually from the open ai gym\ndocumentation so you can actually zoom\ninto this\nso in terms of our observation space\nremember we've got a box 4\nwhich is this down here so box\nand then four the first position\nrepresents the cuts position\nand it's got a minimum value of minus\n4.8 and a maximum value of 4.8\nwe've also got the cut velocity which is\ngoing to be this value here\nwe've then got the pole angle which is\ngoing to be this value here\nand then we've got the pole angular\nvelocities i'm guessing this is sort of\nthe speed at which\nthe pole is moving up or down so again\nthese observation spaces\njust map to this now not every\nenvironment is going to be well\ndocumented like this but i sort of\nwanted to give you an idea\nso cut position cut velocity pole angle\npole velocity\ncut position cut velocity pole angle\npole velocity then in terms of our\naction spaces remember we've got two\npossible actions zero or one\nthese are the descriptions for our\nactions so action zero\nis going to push our cart to the left\naction one is going to push our cart to\nthe right so you can sort of see how\nthese actions\nand observations sort of play together\nnow\nthat is our environment in a nutshell so\nyou can see that we've gone and done a\nfew things there so we went and defined\nour environment we then went and tested\nit out\nand then we went and actually took a\nlook in granular detail to be able to\nactually understand how this environment\nactually fits together\nand i think this is really really\nimportant because it gives you an idea\nas to what the hell you're trying to\nsolve\nbut keep in mind that whenever you're\nsolving one of these environments you're\ntypically going to have an\naction space and an observation space\nand it's a good idea to try to\nunderstand what each one of those means\nbut on that note that is our environment\nnow set up so we can actually close this\nso let's go and take a look at what's\nnext\nso this brings us to step three training\nso a key thing to call out is that there\nare a\nheap of different types of algorithms\nwhen it comes to reinforcement learning\nnow typically\nthese are grouped into model based rl\nand model free based reinforcement\nlearning algorithms\nnow we're mainly going to be focusing on\nmodel freebase reinforcement learning\nalgorithms because that's where a lot of\ndevelopment is happening\nbut that's not to say that model based\nreinforcement learning isn't\nuseful as well so a core thing to note\nin terms of model-free rl so the\nwhole idea between model-free rl is that\nit only uses\nthe current state values to try to make\na prediction\nwith model-based reinforcement learning\nwhat's actually happening is it's trying\nto make a prediction about the future\nstate of the model\nto try to generate a best possible\naction\nso there are a whole heap of advantages\nfor and again so i'm not going to go\nthrough it in great detail\nthere is a great document on this down\nat the open ai website so\nunder https colon forward slash forward\nslash spinningup.openai.com\nthere's a really really good explanation\nof model 3 versus model based rl\nnow if you so i guess a key thing to\ncall that is that\nstable baselines really only deals with\nmodel 3 based rl\nthere are a number of other libraries\nthat look at model base rl as well i\nbelieve rl lib\nis one of them so we're going to be\nfocusing on a model free rl and\nspecifically we're going to be taking a\nlook at\nthe a2c algorithm ppo and we'll also\nprobably use i think we'll probably only\nuse those two to begin with but again\nwe'll also use dqn over here as well so\nagain we'll use a few different types of\nalgorithms so you can sort of see what\nthey look like\nbut that sort of gives you an idea as to\nwhat's sort of out there there's a bunch\nof algorithms\nbroadly grouped into model 3 versus\nmodel based reinforcement learning\nnow a core thing to note is choosing the\nbest possible algorithm for your use\ncase so we've talked a little bit about\ndifferent types of action\nand observation spaces so far\nnow the algorithm that you're going to\ntry to use or the algorithm that i'd\nsuggest you use\nshould ideally be one that maps\nappropriately to your particular type of\naction space\nso you can see here that in terms of\nstable bass lines there's a whole bunch\nof different types of algorithms so\nwe've got a2c\nddpg dqn her ppo\nsac and td3 and i'll show you how to\nactually use these different algorithms\na little bit later in the main tutorial\nbut a key thing to note is that\ncertain algorithms can only work on\ncertain types of action spaces\nso you can see here the a2c works on\nboxes discrete multi-discrete and\nmulti-binary\nddpg works on box spaces only\ndqn works on discrete spaces only and\nthis is in reference\nto the action space so a key thing to\ncall that is that it's based on the\naction space not so much the observation\nspace\nso remember if we go back to our main\ntutorial so it's this\nover here so if you type in emb dot\naction space and you say that it's\ndiscrete\nthen you know let's scroll back then you\nknow that you can use\nany one of the models down here that has\na green tick under discrete so we could\nuse a2c to solve this\ndqn her and ppo\nnow if you had a box environment so\nremember if we take a look at our\nobservation space\nassume our action space had a shape of\nbox\nwell then you'd be looking at using one\nof these ones a2c\nddpg her ppo sac\nor td3 i've got a little bit of a guide\ndown here so discrete single process use\na dqn\ndiscrete multi-process use ppo or a2c\nusing a continuous single process sac or\ntd3\ncontinuous multi-process ppo or a2c\na key thing to call out guys is that\ntreat these algorithms as commodities so\nyou can choose to use whichever one you\nwant for your particular use case\nsome are going to perform better than\nothers it's good to know how they work\nit\nis better to know in detail how they're\nput together but again you don't really\nneed to know that or that level of\ndetail to be able to try this out or try\nyour hand at it\nit's just important to know which\nalgorithm you should use for which type\nof action space\nand again all of them are available\ninside of the stable baselines\ndocumentation so you can see that we've\ngot all of those here\nso remember to get to this you can go to\nstable dash\nbaselines3 dot read the docs dot io\nforward en forward slash master forward\nslash models and then\nthis is if you want to look at the ppo\nalgorithm modules forward\nppo dot html but again all the links are\ngoing to be in the description below so\nyou can grab that and pick it up\ncool so we've talked a little bit about\ndifferent types of algorithms and when\nto use which one\nnow another thing to note is that you\nneed to be able to understand\nyour training metric now which type of\nalgorithm you use is going to determine\nwhat type of metrics you get during\ntraining but broadly you should get\nsomething that looks a little bit like\nthis and you'll see this once we kick\noff our training\nso we can break this up into evaluation\nmetrics time metrics\nloss metrics and then we've got other\nmetrics so our evaluation metrics are\nall to do with our episode length and\nour episode reward\nmean well so these are our averages so\nour length is how long our episode\nactually went for so if you're playing a\ngame think of it as one single game\nwhen we're trying to balance our cart\npoll at one episode is\nthe number of steps the maximum number\nof steps we're allowed to take\na time metrics so in this case we've got\nframes per second so this is how fast\nyou're processing\niterations so that means how many times\nyou've actually gone through\ntime elapsed how long it's been running\nand total time steps so that's how many\nsteps you've actually taken in an\nepisode\nyou've also got some loss metrics so\nyou've got entropy loss policy loss\nvalue loss\nagain if you want a greater detail or if\nyou want a greater explanation on that\nhit me up in the comments below\nwe've also got some other metrics as\nwell so we've got the explained variance\nso this is how much of the variance in\nthe environment\nyour agent is able to explain you've\nalso got your learning rate so how fast\nour policy is actually updating\nand you've also got n updates which is\nhow many updates we've actually made\nto our agent now a core thing to call\nout\nis that by default when we actually go\nand install stable baselines using the\npip install command\nwe're only going to be installing that\nwithout gpu acceleration now if you\nwanted to use\ngpu acceleration you could all you need\nto do is just go and install the\nappropriate pi torch version\nso say for example i wanted to leverage\ngpu acceleration on my particular\nmachine which i'll show you in a sec\nall i would need to do is go to\npytorch.org so if we go to our baseline\ninstall page hit install and then if we\nscroll on down\nyou can see that down here it sort of\ngives you the steps to go and install\nthis\nso i could choose the stable install i'm\nworking on a windows machine but if i\ncould\nif i was on a mac i could hit math linux\ni could hit linux\nso i'm going to choose windows and then\nin this case i'd say for example i\nwanted to install using pip i could just\nhit pip\nand then i could choose the language\nthat i wanted to install for so if i\nwanted java i could choose that if i\nwanted python in this case we're working\nin python so choose python\nand then i need to choose the compute\nplatform this is really really important\nhere\nso cool thing to call that is that\ncuda and cu dnn are only supported on\nnvidia gpu so\nif you want to leverage gpu acceleration\nyou have to have\nan nvidia gpu to use cuda now over here\nyou've also got rock\nm now rock m is the beta package which\nis available for amd gpus\nnow i believe this is only available on\nlinux at the moment so if you wanted to\nuse an\namd gpu to be able to do this you'd need\nto be able or you'd need to be on the\nlinux os\nin this case i'm on windows so you can\nsee it's not available on windows so i'd\nbe using cuda or\nin this case cuda 10.2 or cuda 11.1\nnow this is really only needed if you\nwant to use gpu acceleration to be\nhonest\nwith reinforcement learning you're not\ngoing to see as much of a performance\nboost in training as you would with\ntraditional deep learning by using a gpu\nso if you don't have a gpu\ndon't stress don't worry about it you\ndon't need to do this step i just wanted\nto call it out for people that do have a\ngpu that wanted to do that\nbut in this case we'll probably do that\nin one of our other projects and take a\nlook at how to\ninstall that so for now you can sort of\nskip this it's just a good thing to note\nalrighty so on that note though let's go\non ahead and let's go and\ntrain our agent so i'm going to skip\nback into our notebook and what we're\ngoing to do now is start\ntraining our reinforcement learning\nmodel so first up what i'm going to do\nis i'm going to define a log path and\nthis is going to be where we\nsave our tensorboard log so if we wanted\nto go and monitor our training we'd be\nable to take a look inside of this log\ndirectory\nand view how our model is actually\nperforming so i'll show you how to do\nthat down here\nso let's go on ahead and first up to\nfind our log path so i'm going to type\nin log path or log\nunderscore path and then we'll specify\nthat\nnow a key thing to call that is that\nthis path needs to\nexist so we can also create it as part\nof our code but i've just gone and done\nit manually because it's reasonably\nstraightforward so what i'm going to do\nis inside of the folder that we're\nworking with i'm going to create a\nfolder called training\nand then inside of that i'm going to\ncreate two additional folders one called\nlogs and one chord saved models let's\nzoom in on this\nso you can see that we've got a training\nfolder and then we've got one called\nlogs one called saved models\nso inside of our logs folder we're going\nto save our logs so in this case you can\nsee i've got a bunch let's delete them\nbecause we don't need those\nand inside of saved models we've got a\nbunch of models as well so i'm just\ngoing to delete\nthese ones because we don't need those\nfor now\nso our logs is going to be where we save\nour logs for our model\nand our saved models are going to be\nwhere we save our\nsaved model so our trained model so\nagain we'll take a look at those a\nlittle bit later\nonce we actually go and do it okay\nso that is that now done so again\nwhen you're doing this i'm going to add\na comment so\nmake your directories\nfirst all right so we're going to define\nour log path\nso again this is going to give us a path\nto our logs\ntraining backwards backwards logs and\nbecause i'm on a windows machine the\npath is represented by a double backward\nslash\nif you're on a mac or a linux machine i\nbelieve it's a forward slash\ncool so that is our log path now defined\nnow the next thing that we need to do is\ninstantiate our algorithm\nand specifically our agent now remember\nwhen we went and imported our\ndependencies we went and imported ppo so\nin this case\nppo is going to be the algorithm that\nwe're going to be using\nfor this particular environment so\nlet's go ahead and define that and then\nwe'll take a look\nokay that is our algorithm\nnow set up now you can see here that\nit's printed out using cuda device this\nis because\ni do currently have gpu acceleration set\nup for this particular environment\nif you were not using the pytorch cuda\nversion or the pytorch gpu accelerated\nversion\nwhat you would see here is using cpu\ndevice so again no need to stress if\nyou're not using gpu acceleration i'll\nshow you how to set it up later\nif it says using cpu device that's\nperfectly fine as well you're still good\nto go\nall right so in order to do that we've\nwritten three lines of code\nhere so written so we've gone and\nrecreated our environment here this is\njust to keep it all\nencapsulated so i've written emv equals\njim dot make and then to that we'll pass\nyou our environment name\nso this line over here is no different\nto\nthis line over here so again exact same\nthing\nthen what we've done is we've wrapped\nour environment inside of that dummy\nvec env wrapper so remember up here we\nimported this\nthis is where we're actually wrapping it\nso to do that we've written env\nequals dummy vec env and then we've\ncreated a\nlambda function so this is going to be\nan environment creation function\nso inside of square brackets i've\nwritten lambda colon and then env so\nthis is going to allow us to work with\nour environment that's wrapped inside of\nthat dummy vectorized environment\nso again just think of it as a wrapper\nfor a non-vectorized environment\ni'll show you a real vectorized\nenvironment when we get to our project\none\nand then we've actually gone and defined\nour model so think of this as defining\nour agent so we've written model\nequals ppo so again this is the\nalgorithm that we've gone and imported\nover here\nand then to that we've passed through\nwhat is that uh two\narguments and two keyword arguments so\nthe first one is defining the policy\nthat we're going to use so in this case\nit's a\nmlp policy this stands for multi-layer\nperceptron policy\nnow in this case this means that we're\ngoing to be using a neural network which\nis just using\nstandard sort of neural network units\nwe're not using lstm layers and we're\nnot using cnn layers\nwhat we will do inside of project 1 and\nproject 2 is we'll actually use a\ncnn policy core thing to call out as\nwell\nis stable baselines two actually had one\nadvantage over stable baselines three in\nthat it actually had an\nmlp lstm policies so if you wanted to\nuse\nwindowed data sets which are\nparticularly useful for trading or\nfinance\nas well as certain gaming applications\nthat particular policy\nisn't unfortunately available in stable\nbaselines 3 as far as i know so again\nif that changes i'll mention it in the\npinned comment below\nfor now it supports mlp policy and i\nbelieve cnn policy\nyou'll see cnn policy inside of project\none\nthe next argument that we've passed\nthrough is our environment so this is\ngoing to be this\nvec dummy vectorized environment here we\nspecified verbose equals one\nbecause we want it well we want to log\nout the results for that particular\nmodel and then we'll specified our\ntensorboard log path so tensorboard\nunderscore log and we've specified it as\nthis log path here so if we actually go\nand take a look at this algorithm\nppo there are a whole\nheap of arguments that we can actually\npass through here so you can see that we\ncan pass through\nthe policy the environment the learning\nrate the number of steps the batch size\nthe number of epochs gamma gae lambdas\nso there's a whole bunch of different\ntypes of piper parameters that you can\nactually train on here as well\nand a whole bunch of different things\nthat you can actually train so again if\nthere's a whole bunch of documentation\non this particular environment\nand you can see all of that there so\nagain we're keeping this pretty simple\nand we're using the standard hyper\nparameters in this particular case\nso now that our agent is now set up the\nnext thing that we need to do is just go\non ahead and train it so again this is\npretty\nsimple from here on out so we just need\nto use model.learn to be able to go and\ntrain it so let's do it\nso if we type in model.learn and then we\njust need to pass through the number of\ntime steps that we want to train it for\nso again i'm\njust going to pass that through and\ninitially i'm just going to set that to\n20 000. so the full line is model\ndot learn and then we're passing through\na keyword argument so total underscore\ntime steps\nequals 20 000. now you can play around\nwith this number and\nin terms of how long you want to train\nso for a simple environment you're\nprobably going to be able to get away\nwith a lower number of total time steps\nfor a sophisticated environment say for\nexample breakout or the self-driving\nenvironment you're probably gonna need a\nheap more\nso for example for cardpole i've managed\nto solve it more often than not in under\n20 000 steps\nfor the breakout and the self-driving\ntutorial well breakout doesn't actually\nhave\na end goal per se but that actually\ntook around about 300 400 000 time steps\nas did the self-driving tutorial so\nagain the complexity in the environment\nis going to define how many time steps\nyou need to train for so in this case\nwe're pretty happy with 20 000 so we can\ngo on ahead and kick that off\nand what you'll see eventually is once\nthis model starts training it looks like\nwe've got a bit of an error there\nokay it looks like it might have just\nbeen a warning\nokay so you can see our model is now\nstarting to train so we're getting our\ntime\nmetrics and we're also getting a whole\nbunch of additional training metrics\nso let's let this go on ahead and run\nand then as soon as it's done we'll be\nable to test it out\nokay so we can see that our model has\nfinished training and if we take a look\nso it looks like we've got an explained\nvariance of 0.231\nwe've got an entropy loss of minus 0.599\na learning rate of 0.000\nloss of 57.6 looks like it wasn't all\nthat stable to the end but that's fine\nlet's test it out and see what this\nactually looks like\nso that's our model now trained or at\nleast trained for 20 000 steps now if we\nwanted to we could go and train this for\nlonger so all we need to do is go and\nrun it again\nit's going to start training again\nso you can see it's kicking off training\nand it's going on so again if you wanted\nto train it for longer all you need to\ndo is go and run that again\nnow now that we've kicked it off let's\nlet that finish and then we'll actually\ntest it out\nokay so that is our next round of\ntraining now\ndone looks like our explained variance\nis a little bit higher learning rate's\nstill the same\nwe've gone through a total of 20 480\ntime steps so again this is just for\nthis latest run\nnow more often than not what you're\ngoing to want to do is you're going to\nwant to\nsave down this model and move it around\nif you wanted to go and deploy it you'd\nwant to be able to save it so let's take\na look at how we might\nsave and reload our model first up and\nthen we'll go and evaluate it\nso we're going to define a path and\nwe're just going to call it a ppo path\nsimilar to what we did for our log path\ncool so that's our path defined so i've\njust written\nppo underscore path so that's going to\nbe our path variable\nequals os dot path dot join\nand then we're going to be saving it\ninside of our effectively our saved\nmodels folder so training\nand then saved models and then our file\nname is actually going to be ppo\nunderscore model underscore cart poll so\nthis is going to save our model\ninside of this folder so reinforcement\nlearning course well this is my current\nfolder\ntraining and then saved models so it's\ngoing to be saved in here\nso if we go and save it now\nso you can see that our model is now\nsaved there so ppo underscore model\nunderscore carpol\nso again to save it all i've written is\nmodel dot save and then i've passed\nthrough this ppo path\nnow if we wanted to we could actually go\nand delete this model and reload it so\nlet's go ahead and do that because this\nsort of simulates deployment right\nyou're going to be reloading\nfrom your saved model each time so let's\ndo it\nso i'll just write del model to delete\nour model and then what we can do is we\ncan reload this model back into memory\nso if i type in model\nso we're just going to define a variable\ncalled model and then we can actually\nreload it so to do that we're just\nwriting ppo\ndot load and then we just pass through\nour path\nor the path to your actual model so if\nyou save it somewhere else you're going\nto make sure or make sure that you pass\nthrough the full path to the models\nand then we're going to pass through our\nenvironment as well so the full line is\nmodel\nequals ppo dot load and then to that we\npass through the path where our model is\nactually saved so remember\nppo underscore path is just going to be\nwhere our model is\nso in this case it's in training saved\nmodels ppo model carpol\nit's exactly this so training\nsaved models ppo underscore model\nunderscore carpal so that's the same\nfile that we're working with\nso let's load it so right now so before\ni run this cell so you can see if i type\nin model\ndot learn for example total time steps\nequals a thousand so this would be our\ntraining step\nso you can see that we've got name model\nis not defined because remember we\ndeleted our model over here\nnow if we actually went and loaded it\nyou can see that we've now loaded our\nmodel and if we go and run this\nyou can see that we're now training\nagain\nright so you sort of get the idea so you\ncan go and train your model you can save\nit using model.save\nand then you can reload it using\nppo.load so remember\nmodel.save and then you actually use the\nalgorithm.load to be able to load it\nback up\ncool that is our training now done so in\na nutshell we've done quite a fair bit\nthere so we've ridden\nour so we've actually created our\nalgorithm or our agent so ppo and then\nwe'll pass through our parameters\nwe've used model.learn to trade our\nmodel then use model.save to save our\nmodel and then\nppo or whatever algorithm you're using\ndot load\nto be able to go and reload it into\nmemory so again those four key\ncomponents are really really important\nso use the algorithm to find the hyper\nparameters\nmodel.learn to train it model.save to\nsave it and then whatever the algorithm\nis\ndot load to reload it\non to step four testing and evaluation\nso so far what we've gone and done\nis we've set up our environment we've\ngone and trained it but we haven't\nactually done anything with our trained\nmodel as of yet\nwell what we're going to want to do is\nwe're actually going to want to train\nour model to see how it's actually\nperforming\nnow you would have noticed that when we\nactually went and trained that model\nusing the ppo algorithm so let's\nactually go back and take a look\nwe didn't actually get those training\nmetrics now the training metrics\nthat i was sort of showing up here or\nthe rollout metrics that you can see\nthere\nare very much dependent on the algorithm\nthat you're using so with the a2c\nalgorithm i believe you get these\nrollout metrics but with ppo you don't\nso what you're going to want to do is\nyou're going to want to evaluate the\nmodel itself to see what the performance\nis actually like\nnow we can actually use the evaluate\npolicy method that we imported\nright up at the start to be able to see\nwhat that actually looks like\nbut a key thing to call that is if you\ndo get these metrics it's a great thing\nso the two key ones that you need to pay\nattention to are the\nepisode mean length or episode or ep\nunderscore len underscore mean so this\nis how long each episode actually lasted\non average so say for example you're\nplaying breakout\nit's how many times your model was able\nto play or hit the ball or how many\nframes it was able to go through\nbefore the model eventually died so with\ngaming this is particularly important\nthe reward mean is effectively the your\naverage reward\nso remember think back to our dog\nenvironment so it's how many times or\nit's\non average how many times your dog got a\ntrade or your average reward in this\nparticular case now we can actually get\nmetrics similar to these by using that\nevaluate policy method\nand we can also monitor those training\nmetrics inside of tensorboard so\nremember when we actually set up our\nmodel we actually pass through\ntensorboard underscore log\nand we specified our log path so if we\ngo back to that\nso you can see over here when we defined\nppo we actually specified this\ntensorboard log path\nso we can then go and actually take a\nlook at those metrics and those are\ngoing to be our training metrics\ncool so let's go on ahead and do that\nand this is how you start tensorboard\nbut again i'm going to show you how to\ndo this in a second\nso let's do it\nso we are now up to evaluation so\nlet's go ahead and do this so we're\ngoing to be using the\nevaluate underscore policy method from\nup here so remember this is going to be\na method that allows us to test how well\na model is actually performing\nnow the ppo model in this particular\ncase is considered\nsolved if you get on average a score of\n200 and higher\nso ideally we want to see that our model\nis scoring 200 on average to determine\nwhether or not the environment is\nactually being solved\nnow certain environments are going to\nsort of have a cap as to where it's\nconsidered solved\nothers are just going to be continuous\nwhatever highest score you get is the\nbest\nso breakout and the self-driving\ntutorial i don't believe have\ncaps but in this case the carpol\nenvironment does\nso let's go ahead and test this out\nokay so we've gone and written our line\nto go and test out our policy or\nevaluate it and the line that we've\nwritten is evaluate underscore policy\nand then to that we've gone and passed\nthrough two arguments and two keyword\narguments\nso we've gone and passed through our\nmodel our environment\nhow many episodes we want to test it for\nso in this case we've passed through\nn underscore eval underscore episodes\nequals 10\nand then we've gone and specified render\nequals true so passing through render\nequals true determines whether or not we\nactually visualize it in real time\nso if you're evaluating this policy on\ncolab then you want to specify\nrender equals false because you don't\nwant to visualize it it's not going to\nwork\nat least with the default evaluation so\nlet's go ahead and test this out and see\nwhat our model actually looks like\nso you can see it's way more stable this\ntime so remember when\ntesting it out at the pa at the start it\nwas sort of falling down and going sort\nof all over the place\nnow it's perfectly stable right so you\ncan see that it's balancing it almost\nexactly and so it's going to do this 10\ntimes so it's going to go through 10\ndifferent episodes and take the average\nreward and we'll actually see that in a\nsecond\npretty cool right so in just a couple of\nlines of code you've been able to build\na reinforcement learning agent\nnow again the training speed is going to\nbe pretty much very similar when you're\ntraining on a gpu or not on a gpu\nit's going to be very very similar so\ndon't fret if you don't have a gp on\nyour machine\ntest this out regardless cool so that's\nnow done\nand you can see on average our reward is\n200 so this environment is now\nconsidered solved so we're good\nso these two values that you get out of\nevaluate policy\nare the average reward over the number\nof episodes\nand the standard deviation in that\nreward so in this case we're getting a\naverage score of 200 with a standard\ndeviation of zero\nso we're perfect we're absolutely bang\non in this particular case\nnow the next thing that we want to do is\nactually close down that environment so\nagain we have it over here so if we\nwanted to close it we can just type in\nemv.close\nand that is going to close it down now\nright now we've gone and evaluated it\nbut if we actually wanted to go and\ndeploy this\nhow would we actually go about doing it\nso this is sort of just testing out our\nmodel in an\nencapsulated function what would happen\nif we actually wanted to go and rather\nthan doing it like this we actually\nwanted to do it\nsort of similar to what we did up here\nwell we can actually do that\nthe core difference is that rather than\nusing env.actionspace.sample\nwe're actually going to pass through our\nenvironment observations to our agent\nnow to try to predict the best\ntype of action because that's what\nreinforcement learning is all about\nremember\nwe're going to take our observations\npass it to our agent our agent is going\nto try to determine the best type of\naction to take to our environment to\nmaximize our reward\nso the flow is going to be very much\nsimilar so we'll\ntake a look at our environment so we'll\nuse env.reset to reset our environment\nand get our observations\nwe're then going to use model.predict on\nthose observations to try to get the\nbest possible type of action\nand then we're actually going to take\nthat step so we can actually copy this\nentire block of code here\nand right down here let's go ahead and\ntest out our model\nnow to this we're going to make a few\nkey changes so rather than using\nenv.actionspace.sample\nwe're going to change this to model dot\npredict\nand then to our model.predict function\nwe need to pass through our observations\nnow right now we've got our observations\nnamed two different things so we want to\nchange this\nso env.reset we're going to change that\nvariable to be equal to obs\nand then down here in env.step.action\nrather than having n\nunderscore state we're going to change\nthis to obs as well\nso ideally what we're going to change is\nthis line over here so before it was\nstate\nequals e and b dot reset we're going to\nchange our action line\nso rather than having\nenv.actionspace.sample we're going to\nchange it to model.predict\nand to that we're going to pass through\nour observations\nand then in our env.step line which is\nwhere our action is taken\nwe're going to change that first\nparameter to obs as well\nso now if we run this rather than taking\nrandom steps we're actually going to be\nusing a model to take steps so remember\nwe've now subbed out our model so we're\nnow now\nusing model here\nso if we go and run this looks like\nwe've got a bit of an error let's take a\nlook\nand we might need to oh we're actually\ngoing to get two parts from our model so\nif we actually uh let's actually take a\nlook at this\nwe use model.predict\nobs we actually get back two values so\nwe get this array and we get this none\nvalue so our\naction is actually the first bit and the\nsecond component\nare our states\nwe actually don't need that second\ncomponent so we just make that\nunderscore so if we actually do that now\nand we take a look at our action\nthat's looking better all right cool so\nwe're just going to make this one change\nso we're going to unpack this value\nand our environment is still open so\nlet's go ahead and close it\nit's closed all right let's try this\nagain\nthere you go so you can see performing\nway better than before it's now\nbalancing that\nthat pole way better than what we had\ninitially when we were just taking\nrandom steps and you can see the score\nbeing printed down below so\nwe're getting 200 pretty much every\nsingle time which means\nwe're smack bang on solving that model\nand there you go so you can see that\nwe've now gone and done that and again\nwe can close this now if you wanted to\nrun this continuously you could as well\nbut in this case we're doing it in a\nnice sort of loop\nand then and again we can go and do this\nagain so let's try running it pretty\ncool right so\nwe're now actively using our\nreinforcement learning agent to be able\nto go and\ninteract with our open ai gym\nenvironment so it's now balancing the\npoll a whole heap better\nnow let's actually take a look at what\nwe did there so we went and\nlet's actually delete this so if you\ncast your mind\nright back up to section two where we're\nloading our environment\nand we're playing around with how we can\nactually play with it now\nwhat we actually did is we had one\nreally important function which was emv\ndot reset now remember when we type in\nenv dot reset we're going to get the\nobservations for our observation space\nwhat we can actually do is we can take\nthese observations or what we're\nactually doing is we're taking these\nobservations here\nand we're passing them to our model so\nmodel.predict\nobs now what you're actually getting\nback here is two values so let's\nactually\ntake a look at what we're getting back\nto do so we are going to get back the\nmodel action and the next state so\nthat's used in recurrent policy so again\nbecause we're not using our current\npolicy we're not getting that next state\nso what we actually get back in this\nparticular case that is relevant to us\nis this first value here which is our\narray\nnow remember in terms of our action\nspace\nspace\nremember there were two different types\nof action so zero\nlet's see if we get one zero and one\nnow what we're basically getting here is\nrather than just getting a random action\nwe're using our model\ndot predict function on our observations\nfrom our environment to generate this\naction here\nso you can see that rather than getting\nenv.actionspace.sample\nour model is actually predicting that\nbased on the observations of our current\nenvironment\nright now you should take action one in\norder to get the best possible reward\nso this is effectively what\nreinforcement learning is all about so\nif you cast your mind back to that\ndiagram\nso we've got our agent we've got our\nenvironment we've got our action and\nwe've got our reward let's actually go\nback to that slide\nright so we've got our agent so in this\ncase our\nagent is this model we've got our action\noh that's actually so we've got our\nagent we've got our environment so\nremember\nour environment is emv this variable\nhere\nwe've got our action in this particular\ncase which is\nwhat we're just generating here so this\none and we've also got our observations\nwhich is this value here so remember our\nobservation so we can print that out\nis these four values now if you cast\nyour mind back we actually took a look\nat what each one of these observations\nmeant over here\nso our observations are our car position\nour cart velocity\nour pole angle and our pole angular\nvelocity\nso again you can start to see how this\nis all sort of fitting together you've\ngot those four key components you've got\nyour\nyou've got your agent you've got your\nenvironment you've got your action and\nyou've also\ngot your observations now a core thing\nthat i haven't called out yet\nis the reward right so we saw that we\ngot our model.predict\nnow how do we actually determine what\nour reward is well we get our reward\nwhen we run env dot step so if we\nactually do that now\nin b dot step and remember our model\njust predicted take this action so if we\ngo and take that extract that out\nand if we pass our action to this\nwhat we're actually getting back is\nthose values that are relevant to us so\nwe're going to get our state\nso this is the state after taking our\naction on it\nthen this value over here is actually\nour reward so you can see our reward in\nthis particular case\nis a value of one now let's actually\ntake a look does it talk about reward\nuh reward there you go so reward is one\nfor every step\ntaken including the termination steps so\nthis basically means that\nwe haven't let up hole four down\ncompletely which means that we get a\nreward of one\nif you pass a certain threshold and your\npole starts to fall down then you don't\nget that reward\nso by basically keeping our pole in the\nupright position and not falling down\nwe're getting\naccumulating a value of one every single\ntime which is how we've got this value\nof 200 here\nso that in a nutshell sort of shows you\nthe theory\nall the way through to the practical so\nthese five steps are getting or what is\nwhat are we up to step six\nthese six steps sort of show you how to\ndefine an environment how to train a\nmodel how to evaluate it as well as how\nto test it out as well so we've done\nquite a fair bit there now\nwhile you're training so this obviously\ntrained really really quickly right so\nwe're able to spin it up train it really\nquickly and get it up and running\nnow if you are training a way larger and\nway more sophisticated environment what\nyou might want to do is view the\ntraining logs inside of tensorboard\nso what we can actually do is do exactly\nthat now i'm going to start it up from\nwithin jupiter notebooks but\nideally you would want to run this from\na command prompt so that you're not\nlocking up your notebook because\nonce you run this it's going to run\ncontinuously it's not going to unlock\nyour notebook you're not going to be\nable to run anything else so i'll sort\nof show you how to do this and then\nwe'll continue onwards\nso what we first need to do is we need\nto get the\nlog directory that we want to view so if\nwe go back to\nour folders so if i go into so this is\nour root folder so if i go into training\nlogs you can see that we've got three\ndifferent training sets\nnow remember we kicked off our training\nthree times so this is where we've got\nthree different sets of logs\nso our one was our i believe\nwere they all the same no our second one\nour first and our second one were the\nlongest our third one was only that\n1000 training steps so let's actually\ntake a look at ppo2\nso what we're going to do is we're going\nto go into that folder and we're going\nto specify tensorboard to run from that\nfolder so first up what we need to do is\ngive it a path\nto that ppo2 folder so let's specify\nthat first up\nokay so we've gone and specified our\ntraining log part so if we go and take a\nlook at that\nso you can see this is giving us a path\nto our ppo2 folder so training\nlogs and then ppo2 so this is\neffectively\nwhere we gone so training\nand then logs and then ppo2 so this file\nover here is our\ntensorboard log file that we're going to\nbe able to use\nnow all we need to do to kick off\ntensorboard from within that folder\nand you're going to need tensorboard lot\ninstalled so i believe it's just a pip\ninstall tensorboard to go and do that\nto go and run this we just need to run\nexclamation mark tensorboard dash dash\nlog dear\nand then we need to specify our training\nlog path\nyeah that looks about right so written\nexclamation mark tensorboard\ndash dash log dear equals and then\ninside of squiggly brackets\ntraining underscore log path we've\nwritten that wrong\nso let me quickly explain what this line\nis doing so i think i've had some\ncomments on this before so\nusing an exclamation mark inside of a\njupiter notebook is known as using a\nmagic command\nso this allows you to run command line\ncommands from within your notebook so by\nme putting through exclamation mark this\nis akin to me going\nto a command prompt or to a terminal and\nwriting tensorboard dash dash log\nblah blah blah whatever so in this\nparticular case what i've actually\nwritten is\nexclamation mark tensorboard dash dash\nlogged let me actually show you it's\nprobably going to make more sense\nso if i went to d drive cd youtube\ncd reinforcement learning uh so\nlet's actually go into let's actually\nspecify the exact same thing\nso written training log pass that's\ngoing to go into training logs\nokay so this is akin to me doing this so\ntensorboard\ndash dash log dear equals\ntraining slash logs slash ppo2\nso right you can sort of see how this is\nactually running inside\nof a command prompt and eventually you\nshould get a line that says it's running\nat http localhost 6006\nthis line over here that i've written\ninside of a command prompt is exactly\nthe same\nas what we would be running over here\nso what i can do is i can go to this\nlink over here which is being created by\ntensorboard\nand you'll get all of your training and\ndoesn't look like we've got any training\nmetrics what's happened there\nokay let's just go directly into the\nfolder\nso we'll go into training\nthen we're going to go into ppo2\nand then we'll run tensorboard\ndash dash log dear equals dot\nthat times the charm let's see if this\nworks okay so it should be running at\nhttp\ndash or colon dash dash local host and\nthen six\nand then six thousand and six let's\nrefresh now\nokay way better so what we went and did\nis i went\nand just seeded into the folder i'm\nguessing i'm getting this\npath that i was specifying incorrect but\nthat's fine you can sort of see how to\nrun it there\nall right so from here you are going to\nget a whole heap of different metrics\nnow specifically you're going to get\ntrain metrics so this sort of shows you\nthe frames per second\nand you're also going to get a number of\ntrain metrics so you're going to get\nclip fraction\napprox underscore k and if you want to\ndeeper dive into what these metrics mean\nhow to evaluate them by all means\ndo hit me up in the comments below i'll\nprobably have a little blue box\nsomewhere in the corner up here that\nsort of explains them as well but you\ncan start to see that you're getting all\nof your different training metrics so\nyou're getting your entropy loss\nyour explained variance which should go\nhigher your learning rate which looks\nlike it stayed pretty stable\nyour loss which looks like it's going\ndown your policy gradient loss as well\nas your value loss so again you're\ngetting\na whole bunch of different types of\ntraining metrics that you can view in\ntensorboard\nnow this is obviously run from the\ncommand prompt which we had to do a\nlittle bit of reducing to get to work\nnow rather than doing that you can just\nrun it from the notebook as well so if i\nstop this now\nall right and close down this command\nprompt what we can actually do is run\nthis command and it'll effectively do\nthe same for us\nright so this is currently running you\ncan see the little asterisks there\nover here now if i go to localhost\n6006 that gives us tensorboard all up\nand running now so you can start to see\nhow to view those logs\nas well now in this particular case\nthat is our set of metrics\nnow done uh if anyone has any feedback\non any of this or has\num a better way to launch tensorboard by\nall means do hit me up in the comments\nbelow i'm always\nwelcome to feedback but that sort of\nbrings us to the end of our testing and\nevaluation step so what we went and did\nis we went and evaluated our model using\nevaluate underscore policy we went and\ntested it out\nand we also went and viewed our login\ntensorboard so it looks like it still\nruns even though you end the cell so\nthat's something i might need to dig\ninto if you do have any problems with\nthat do hit me up in the comments below\nthough\nnow a quick word on performance tuning\nand performance in general\nwhen you are training your model the\ncore metric that you should be looking\nat\nis your average reward so this gives you\nan indication as to how well your model\nis going to perform\nin that particular environment using\nthat particular reward function\nnow the other metric that you want to be\ntaking a look at is your average episode\nlength so this\nideally aims to identify how long your\nagent is actually lasting in that\nparticular environment now this is\nparticularly important\nwhen you have environments that don't\nhave a fixed environment length\nso when we take a look at the breakout\nenvironment and when we take a look at\nthe self-driving environment those are\nreally really good indicators to be\ntaking a look at\nnow what you can actually do is if your\nmodel is not performing that well\nthere's\nthree key strategies that i'd suggest\nyou start taking a look at\nso these strategies are one train for\nlonger so if you've only trained for say\nfor example\nten thousand twenty thousand or a\nhundred thousand steps try training your\nmodel for longer and see if that\nimproves performance the other thing\nthat you can also take a look at is\nhyper parameter tuning so again\nwhen you're dealing with deep learning\nmodels or even traditional statistical\nmachine learning models\nhyper parameter tuning can yield\nsignificant results\nnow stable baselines supports hyper\nparameter tuning using a package called\noptuna\nso we're not going to show it in this\ncourse but if you'd like to see a little\nbit more on that do let me know\nthe last thing that i'd suggest you take\na look at is take a look at the\ndifferent algorithms that people are\nusing\nto perform state of the art training as\nwell so this can be another thing that\nhelps you out when it comes to improving\nyour performance\nokay on to our next topic so what we're\ngoing to do are we skipped all the way\nthrough so let's go\nback to where were we we are now\nup to step five so call backs alternate\nalgorithms and\narchitectures so what we're going to be\ndoing in this particular step is we're\ngoing to be\nrecreating our model but this time what\nwe're going to do is we're going to\nspecify a reward threshold so this means\nthat our training is going to stop\nonce it hits a certain benchmark now\nthis is really really useful when you've\ngot\nreally really large models that you're\ntrying to train and you want to stop\nthem before your model starts getting\nunstable\nnow what we can actually do is we can\nuse some of the helpers from stable bass\nlines to do this so we can use\nthe eval callback and the stop training\non reward\nthreshold callback to do that now the\nnice thing about this is that you can\nactually save your model as part of this\nas well so\nit will automatically save your best\nmodel\nwe're also going to take a look at how\nwe can define a different neural network\narchitecture so\nremember we specified the mlp policy but\nwe can actually pass through a different\nneural network architecture as well\nand then last but not least we're going\nto take a look at how we can use a\ndifferent algorithm so that's\nthe last thing that we should be doing\nin that particular section so\nlet's kick this off and do it\nso the first thing that we're going to\nbe taking a look at is how we can add a\ncallback to our training stage now the\ncool thing about this is that\nif you need to stop your training after\na certain reward threshold\nthis gives you the automated capability\nto do that now this is really really\nimportant\nparticularly when you're training huge\nmodels or models that are going to take\na long time to train so say for example\nyou're training\nthe breakout tutorial the self-driving\ntutorial you might want to use this\nbut again not mandatory just gives you\nthe ability to extend\nout your training step so in order to do\nthis we first up need to install a\ncouple of additional dependencies\nnamely some helpers from stable\nbaselines so let's go ahead and import\nthese\nokay so we've gone and written one\nadditional line of code there so the\nline that i've written is from\nstable underscore baselines three dot\ncommon dot callbacks\nimport eval call back comma and then\nstop\ntraining on reward threshold so our eval\ncallback is going to be the callback\nthat runs\nduring our training stage and our stop\ntraining on reward threshold\nis going to be think of it like a\nchecker so basically once our model\npasses a certain reward threshold so\nremember our\nreward for our carpool environment was\nour the reward\nwhich indicates that solved for the\ncarpool environment is 200\nso we'd basically be stopping our\ntraining once it receives or once it\nachieves that 200 score on average\nso that's now set up now what we need to\ndo is actually set up these callbacks so\nlet's go ahead and set them up and then\nwe'll kick off another training run\nusing it\nokay so those are our two callbacks\nsort of set up so there's one additional\nthing that we need to do and we need to\nspecify\nuh where our best model is going to be\nbut i'll come back to that in a second\nlet's actually take a look at what we\nwrote\nso first up what we're doing is we're\nsetting up our stop training on reward\nthreshold callback this is the callback\nthat's basically going to stop our\ntraining once we pass a certain reward\nthreshold\nso in order to do that written stop\nunderscore callback\nequals stop training on reward threshold\nwhich is this\nthat we imported up here and then we're\npassing through our reward threshold so\nthis basically specifies\nafter which average reward we want to\nstop our training on so in this case\ni've set it to 200.\nand then i've also specified verbose\nequals one so we get some additional\nlogging\nthen the next callback that i've\nactually written is the eval callback so\nthis is the callback that's going to be\ntriggered after each training run\nnow in this particular case i've written\neval underscore callback\nequals eval callback and then two that\nwill pass through a number of arguments\nso first up we're passing through our\nenvironment then\nwe're passing through the callback that\nwe want to run on the new best model\nso in this case i've written callback on\nnew best and then we've specified stop\ncallbacks so this basically means\nthat every time there's a new best model\nit's going to run that stop callback\nand if the stop callback realizes that\nthe reward threshold is above 200 then\nit's going to stop\ntraining overall now we can also specify\nhow frequently we want to run our\nevaluation callback and in this case\ni've set it to 10 000 time steps\nand then i've also specified the best\nunderscore model we actually need to do\nthis\nso what we can actually do is have this\neval callback\nsave the model every time there's a new\nbest model what we do need to do however\nis specify what we want that model to be\ncalled so let's specify that\nso in this case all i've gone and\ndefined is the save path so this is\nwhere we want to save that best model so\ni've written save\nunderscore path equals os dot path dot\njoin\nand then i've just specified the same\nsaved model folder so\ntraining and then comma saved space\nmodels\nand then what we're going to do is we're\ngoing to specify our best model\nas our save path so this means that\nafter every 10\n000 runs it's going to double check\nwhether or not it's past the 200\nreward threshold if it has it'll stop\nthe training\nand it'll also save out the best model\nto this save pass so so you'll actually\nsee this when we actually trigger it\nso if we run this cell now it looks like\nwe've written this\nthis should be best model save path my\nbad\nthere we go cool so we just needed to\nupdate that parameter there so it should\nbe best underscore model\nunderscore save underscore path and then\nwe've specified our save path\nthat's all good to go now so those are\nour two training callbacks now what we\nnow need to do is associate this to our\nmodel so we're going to create a new ppo\nmodel\nand assign these callbacks\nso that's our new model created again\nthis line is exactly the same as what we\ndid\nwhen creating an initial model under\nstep three so again exactly the same as\nthis line\nwhat's different now is that when we run\nour training command so model.learn\nwe're going to pass through our callback\nso let's do that\nokay so what we've then gone and written\nis model.learn\nand then rather than just passing\nthrough the total time steps we're also\npassing through the callback that we\nwant to run\nand in this case we're passing through\nour eval callback so this is going to be\nthe callback that checks every 10\n000 steps and again on every 10 000\nsteps it's going to save the best new\nmodel if it's got it\ninto that save path and it's also going\nto check whether or not it's past that\naverage reward threshold\nso if we go on and kick this off now we\nshould see our training kick off so\nlet's do that\nand so after 10 000 steps you should see\nthe fact that it's evaluating whether or\nnot it's past our reward threshold so\nyou can see it's about 8 000 right now\nso it should check on the next one so\nyou can see there so it's gone and\nevaluated it\nit's checked the episode length so it\nlooks on average it's 198.8 so\nif we keep on going after another 10 000\nwe'll see that eval callback run again\nand there you go so because it's now hit\nthe 200 it stopped our training and\nagain we only had\n20 000 but if we train for longer it\nwould stop it regardless because it's\nnow hit that 200 score threshold\nit's gonna stop the training pretty cool\nright so this gives you a lot of\nflexibility when you're actually going\nout and training really large models and\nyou want to cap it off before it\njust sort of runs wild now another thing\nto note is that this will have also have\nsaved our model so if we go into\nreinforcement learning\ngo into our training folder in our saved\nmodels folder you can see this\nbest model folder or this best model is\nnow saved as well so this is a\nas a result of actually having our\ncallback saved\nit actually goes about and saves the\nbest model as well\nokay so that is our callback now the\nnext thing that we can do is also change\nour policy so say for example we wanted\nto use a different neural network what\nwe can do is do that as well so let's\ntake a look at how you might do that so\nin order to change our policies we can\nactually specify a new neural network\narchitecture now in order to do this we\nneed to specify\na network architecture for our custom\nactor as well as our value function\nso i'll show you how to do this so this\nis akin to just changing the number of\nunits\nand the number of layers inside of your\nneural network so again pretty simple to\ndo and then we can pass it on to our\nmodel so let's do it\nokay so that is our new neural network\narchitecture defined\nnow what we need to do is actually\nassociate it to our algorithm\nso what i've actually written here is\nnew underscore arch so new arch\nequals and then inside of square\nbrackets i've defined a dictionary\nnow the first neural network\narchitecture that i've defined is for\nour custom actor\nso in order to do that we need to pass\nthrough pi and then we're specifying\nthat we're going to have a new neural\nnetwork with 128 units in each one of\nthose layers\nfour layers 128 units 128 128 128 and\n128\nand then we need to specify the same for\nour value functions again\nfour layers with 128 units in each\nneural network layer so you can see that\nthere so vf\nequals and then inside of square\nbrackets 128 128 128\nand 128 now you get again you might only\ndo this if you had a really specific\nreason\nwhat i've actually found is the neural\nnetworks inside of the baseline\nalgorithms work pretty well\nbut again this sort of shows you how you\nmight go about doing that so\nlet's go ahead and associate this to our\nmodel and then kick off our training\nagain\noh this should actually be net arch my\nbad\nthere you go alrighty cool so that is\nour new neural network now associated\nto our ppo model and specifically we've\ngone and updated the policy\nso what i've gone and written is model\nequals ppo and then mlp policy because\nagain we're using the multi-layer\nperceptron policy\nthen our pass through our environment\nverbose equals one tensorboard\nlog log path so again no no real change\nfrom here onwards\nbut then in order to specify the new\nneural network and specifically the new\npolicy i've written policy underscored\nkw\nargs equals and then i've passed through\na dictionary\nand then to that dictionary we've\nspecified the net underscore\narch value and then we've set that equal\nto this over here so netarch\nso again this is just defining a new\nneural network or a new neural network\npolicy attached to our model now what we\ncan do is again type in model.learn\nand again we can apply our eval callback\nto this as well so again if we\nrun that\nthis is now using our new neural network\narchitecture and specifically our new\npolicy\nanother thing to call that is that\ninside of stable baselines\nyou've actually got a few different\npolicies so if you go to the\ndocumentation\nand go to custom policy network there's\na whole heap\nof information on how to actually do\nthis as well\nso you can define custom feature\nextractors so on and so forth so again\npretty pretty cool what you can actually\ndo with this and it can actually get\nreally really sophisticated\nnow in this case we've got our model\nlooks like it's all training\nsufficiently and again our eval callback\nhas kicked in\nso it looks like our episode length has\nhit 200.\nlooks like we're all good there and\nwe've stopped all righty now the next\nthing that we want to now\ndo is actually take a look at how we\nmight go about using an alternate\nalgorithm\nso rather than using our ppo model which\nwe've been using so far we might want to\nuse a dqn for example\nso how might we go about doing that well\nfirst up we need to import the dqn\nalgorithm so let's do that\nokay so that's our dqn now imported so\ni've written from\nstable underscore baselines import dqn\nand then what we can do is just go and\nuse this in a really similar manner to\nhow we used our\nppo models we could actually copy this\nover\npaste it down here and all we really\nneed to do is sub out ppo for dqn\nwe're going to get rid of our policy\nkeyword arguments\nand so this has instantiated our dqm\nmodel and again model.learn\nand we'll pass through total time steps\nset it to 20 000.\nand there you go so this is now training\na dqn model\nrather than a ppo model so really really\nquickly that shows how to apply a\ndifferent algorithm\nremember also in stable baselines you've\ngot a bunch of different types of\nalgorithms so you've got\na2c ddpg dqn her ppo\nsac and td3 in stable baselines 2\nthere's even more algorithms in that as\nwell but because that is now in\nmaintenance mode i figured i'd show\nstable baselines\nthree cool so that is our dqm\nmodel and now done now again to save and\nexport these it's a very similar manner\nso we just type in model.save\nthe only difference when you're loading\nfrom a dqn is that rather than typing in\nppo.load you'd now type in dqn.load\nso that in a nutshell really covers how\nto add a callback to our trading stage\nhow to change our policy as well as how\nto use an alternate\nalgorithm now on that note that\nbrings us to our projects so step six\nwe're now going to go through our\ndifferent projects\nso we've got three specific projects\nthat we're going to take a look at so\nproject one we're going to take a look\nat reinforcement learning for atari\ngames and we're specifically going to be\ntrying to solve\nthe breakout problem then project two\nwe're going to try to leverage\nreinforcement learning to build a racing\ncar and this is sort of like\nalmost going down the path of autonomous\ndriving and then project three we're\ngoing to take a look at how we can build\nour own custom environments using\nthe open ai gym spaces that we talked\nabout\na little bit earlier on so first things\nfirst\nlet's take a look at reinforcement\nlearning for atari games\nokay so inside of the github repository\nyou're also going to have\na couple of additional projects so you\ncan have project one\nproject two and project three so project\none is gonna be breakout so if i\nactually go\nto the github repo so you can see you\ncan have project one which is breakout\nproject two which is self-driving and\nproject three which is custom\nenvironment\nnow in this case what i've gone and done\nis i've started off with project one\nwhich is breakout now again\neven though we're working on different\nenvironments how we actually go about\ntraining them is going to be very very\nsimilar so whilst we've spent a lot of\ntime going through the basics in the\nmain\ncourse how we actually go about applying\nthis to\nour different environments is going to\nbe pretty much the same so let's start\noff by importing our dependencies first\nup\nokay so i've gone and written six\ndifferent lines of code there so these\nare all going to be\npretty familiar to you from the previous\ntutorial\nso the first one that i've written or\nthe first line that i've written is\nimporting jim again no difference there\nthen from stable baselines three i'm\nimporting a2c this is just a different\nalgorithm so again remember how we\nimport a ppo\nand then we import a dqn now this time\nwe're going to be importing a2c just a\ndifferent algorithm\nthen we're importing from stable\nunderscore baselines three\ndot common.vec underscore emv import\nvec frame stack so remember how in the\nmain tutorial we didn't vectorize our\nenvironment so we only trained on one\nenvironment at a time what we're going\nto do for breakout is we're actually\ngoing to train on four environments at\nthe same time so this should allow us to\nspeed up our training\nthen what i've written is from stable\nunderscore baselines three\ndot dot evaluation import evaluate\nunderscore policy\nno change there again so we use that\npreviously and then from\nstable underscore baselines three dot\ncommon dot\nenv underscore util import make\nunderscore atari underscore env\nso this line is a little bit different\nand just helps us work with the atari\nenvironment so atari\nenvironments are the environments that\nallow us to play atari game so if we\nactually go to the\ngym documentation and take a look at our\nenvironments\nyou can see under atari you've got the\nability to try out a lot of these games\nnow\nwe're specifically going to be training\non\nbreakouts so let's take a look at that\none so it's going to look like this\nyeah it's actually this one so basically\nthe goal is just to maximize the score\nthat you can see\nup here and you've got a maximum number\nof lives as well actually this is your\nnumber of lives this is your number of\nscores\nso again the goal is to just maximize\nthat score so there's no real cap that\nyou can get to to completely solve the\nenvironment\nit's just about getting the highest\npossible score so\nlet's go on ahead so what else would we\nwrite there and then import os so again\nthis is going to allow us to work with\nour operating system\nnow another thing that i wanted to call\nthat so say for example we wanted to use\ngpu acceleration so i said i'd show how\nto do that well\nall we need to do is go back to our pi\ntorch\nlink and in this case i'm going to\nchoose the build that i want so stable\nwindows pip python and then i've already\ngot cuda 11 installed on this machine so\nif you don't you're gonna need to do\nthat in order for this to work\nso what i'll do is i'll just copy this\nlink\nand then bring it into my notebook so\ni'll add in an exclamation mark\npaste that in and i just need to get rid\nof this three here so if i run that\nthis is now going to install the cuda\naccelerated version of pi torch so\nthen what typically what you need to do\nis just restart your kernel so just hit\nkernel\nand then restart hit restart\nand you should be good to go and then\nwhat we need to do because we restarted\nour kernel is just re-import those\ndependencies so once that's done\nwe should be okay now as of late there\nhas been a change to the entire\nenvironment so previously you used to\njust be able to import them and they\nused to sort of just work but now you\ngot to do something a slight bit\ndifferent so what you actually need to\ndo\nis download the raw files\nfrom let me just grab this link you need\nto download the raw files\nfrom this particular link here so it's\natarimania.com\nforward slash roms forward slash roms.ra\nso you can see that that's now\ndownloading\nand i'll paste that in i'll make that\navailable in the notebook as well so\nit'll be this\nso you can see http colon forward slash\nforward slash www.atarimania.com\nforward slash roms forward slash roms\ndot ra\nwithout the one you don't need that so\nthat's going to download\nall of the files that you need to be\nable to work with the atari environment\nso you don't need to do that or you\ndidn't need to do this previously but i\nthink as of late this is a change to the\nentire environments that you need to do\nin order to use them so once that's\ndownloaded\nyou'll have a file called roms.ra so\nlet's wait for that to download and then\ni'll show you what to do with it\nokay it looks like that has finished\ndownloading let's go and take a look at\nit so you can see that we've got roms.ra\nso i'm just going to copy this and paste\nit into the same directory that i'm\ncurrently working with\nso you can see i've already got roms and\ni've got hc rom so these files so i can\nactually delete these\nand what you need to do is just paste in\nthat roms.ra file\nand then unzip it so i'm just going to\nextract it\nand you can see i've now got hc roms and\nrom let me zoom in on this\nso i've now got hc roms and roms so what\nwe'll then do is extract these\ninto the same folder\nand so that's hc roms let's do roms as\nwell\nand so once those have extracted there's\njust a command that you need to run\nin order to install these so it's pretty\nstraightforward but once that's done\nyou should be able to leverage the atari\nenvironment so let's let that finish and\nthen we should be good to go\nand if you get a warning you can just\nskip those\ncool that's good so you should now so we\ncan actually delete these now so we can\ndelete hc roms roms.ra\nand roms so we don't need those we just\nneed the extracted folders which you can\nsee there\nokay so that's all well and good now we\nneed to go and ahead and install those\nso if we go back\nwe just need to run a simple command to\ngo and install those into our\nenvironment so let's go ahead and do\nthat\nand there you go so we're all good so\nthe command to run it is exclamation\nmark\npython dash m and then atari underscore\npi\ndot import underscore roms and then you\njust need to pass through the path to\nthose roms\nso again if i actually show you so those\nfiles\nare inside of a folder called roms and\nthen roms again so you need to point to\nthis particular file path here\nso in this case what i've written is\nexclamation mark python dash\nm atari underscore pi dot import\nunderscore roms\ndot backwards roms backwards rom so\nagain if you're on\na mac the file path might be a little\nbit different i believe it's forward\nslash rather than backward slash\nbut you sort of get the idea so once\nthat's done you should be all good to go\nahead and test this environment so let's\ngo ahead\nset up our environment and we'll\nactually take a look at it\nokay so that's our environment now made\nnow if we type in emv.reset we should\nget our\nobservation oh my bad\nso there you go so we've got our\nobservations and if we type in\nuh what's the other one so emv dot\naction space\ni'll leave that\nenv dot action underscore space\nso you can see our action space is\ndiscrete and we've got four different\nactions that we can take\nwe can take a look at our observation\nspace in v dot observation says\nyou can see that our observation space\nis going to be a box\nwith the values ranging from 0 to 255\nand the dimensions are going to be 210\nin terms of height\n160 in terms of width and 3. so this\nmeans it looks like it's going to be an\nimage which in this case\nit's an image based model now what we\ncan do is we can actually go on ahead\nand test out this model so remember\nif we cast our minds back\nto step 6 where we actually tested out\nour model we can actually copy this\nblock of code\nand run the same thing here\nso remember this is just going to go on\nahead and test out our particular model\nactually this is the wrong code let's\nactually write it from scratch\nso what we can do is go through a number\nof episodes and actually play breakout\nso let's give this a crack\nokay so let's take a look at what we\nwrote so this code is really really\nsimilar\nto what we used in step two where we\nloaded and tested out our environment\nso again we're setting up the number of\nepisodes that we want to play\nwe're looping through each one of those\nepisodes and then we're basically going\non ahead and taking random actions on\nthat environment\nto see what it looks like so if we run\nthis now should get a little pop-up\nand you can see we're effectively\nplaying breakouts again it went really\nreally quickly\nif we didn't want to close our\nenvironment we can just comment at that\nlast line\nand you're going to see it play now you\ncan see it's sort of just playing\nrandomly and it's not exactly getting\nthe highest score so it's what\ncapping added around two four it looks\nlike the highest that it got was four\nnow it gets a point for each block it\nbreaks so you can see they've got one\ntwo three in that case we want to try to\ntrain a model that's able to play a\nlittle bit better now again\ntraining this model can take a long time\nso we'll give it a crack and\nif you want to take this further train\nit for longer let me know how you go in\nthe comments below\nnow what we're going to do here is a\nlittle bit different to what we did in\nthe main tutorial because what we're\ngoing to do is we're actually going to\nvectorize our environment and train\nfour different environments at the same\ntime so let's go on ahead and test this\nout\nokay there you go so that's our\nenvironment at the moment so now if we\ntype in\nemv.reset\nand env.render\nyou can see that we're actually playing\nwith four different environments at once\nso this means that when we actually go\nand train\nwe're going to be training four\nenvironments at the same time so\nhopefully\nthis should give us a little bit of a\nspeed up now we can type in\nemb.close to close that down you can see\nit doesn't look like it's closed down\nin this case let's try that again\nsometimes it's not going to want to\nclose down and you're just going to have\nthis for shut it but i don't want to\nforeshadow it because sometimes it'll\ncrash the kernel\nthat's fine for now just leave it open\nso\nthat is our environment now set up so\nall well and good\nnow what we can do is actually set up\nour model to actually go\nahead and train this so let's do that\nand kick off our training\noh let's actually take a look at what we\nwrote to vectorize our environment i\ncompletely skipped over that\nso what i wrote was emv equals make\nunderscore atari underscore emv\nand then to that we pass through the\ntype of environment that we want to run\nso in this case the environment that\nwe're actually running is breakout dash\nv0 so this is the breakout game that\nwe're actually working with\nnow a key thing to call that is if you\nactually take a look at the gym\nenvironments\nthere's actually a breakout ram version\nand a breakout\ndash v0 version so this one is going to\ntrain using\nimages this one is actually going to\ntrain using ram we want the\nimage based model because we're going to\nbe using a cnn policy\nso then to that we've also passed n\nunderscore e and v's equals to 4\nbecause we're going to use four\nenvironments at the same time and we're\ngoing to specify\nseed as zero to get some reproducible\nresults\nthen what we've actually gone is we've\ngone and stacked those environments\ntogether so to do that written emv\nequals vec frame stack so this is that\nwrapper that we\nimported up here and then we've passed\nthrough our environment and we've\nspecified\nn underscore stack equals four so this\nbasically stacks our environments\ntogether\nthen we're going to go and specify\nmodels so let's go and do it\nokay so that is our\nmodel now set up so we've gone and\nwritten two lines of code there\nso first up we've written log underscore\npath equals os\ndot path dot join and then we'll specify\ntraining and log so again it's similar\nto how we set up our log path\nin the main tutorial then we've gone and\nspecified our model so model equals a2c\nso remember we're just using\na different model in this particular\ncase so this is the different algorithm\nthat we're using so rather than ppo\nor dqm then we'll specify the type of\npolicy that we want to use\nthis is a key differentiator so\npreviously we used the mlp policy which\nis great\nfor tabular data or tabular observations\nbut because our image and specifically\nour observations are an\nimage in this case our cnn policy is\nactually going to be a lot faster to\ntrain so we've specified\ncnn policy so this basically uses a\nconvolutional neural network\nas part of the policy rather than just a\nmulti-layer perceptron\nthen we've gone and specified our\nenvironment which is\ncoming from here specified verbose\nequals one because we want logging and\nwe've also specified a tensorboard log\npath\nnow what we can go and do is go on ahead\nand train this now we're going to train\nthis\nfor a little bit longer so what i might\ndo is might stop the training if it runs\nfor too long\nand then we'll actually load up one that\ni pre-trained and see how that performs\nbut for now let's train this on about\nlet's give it 200 000 steps so again if\nyou want to get a really really high\nperforming model you might need to train\nup to a million or even two million\nsteps\nbut let's give it a hundred thousand and\nsee how long that takes\nideally we should be able to get a score\nhigher than four from what you can see\nup here\nwhich is just random actions\nokay so no real difference there so what\ni've written is model\ndot learn and then pass through total\nunderscore time steps\nand we'll specify that as a hundred\nthousand so again no different to what\nwe did for our previous tutorial where\nwe're\ntraining a model so we're gonna run this\nand we'll be right back\nokay so you can see that our training is\nkicked off so again\nwe'll let this train and then we'll be\nright back as soon as it's done so it's\ngonna train for a hundred thousand steps\nso we'll give it a little bit of time\nokay so that is our breakout model\nnow finished training so you can see\nthat after a hundred thousand steps\nwe've got an episode reward mean or\naverage episode reward of\n5.84 and an episode length of about 479\nframes so not too bad overall\nnow what we want to do is save this\nmodel and reload it before we do\nanything else so let's go ahead and do\nthat\nnow this is going to be really similar\nto how we've saved models before so\nagain nothing too crazy there\nso that is our model now saved so if we\ngo\nand take a look so you can see that\nwe've gone and saved a2c breakout model\nnow i've also got an a2c model that's\nbeen trained for 300 000 steps for\nbreakouts so if you wanted to take a\nlook at that one\ni'll include that in the github repo as\nwell so you can take a look and try that\none out\nbut in this case we are going to test\nout our own model so let's go on ahead\nand delete our model\nand then reload it just to make sure it\nall works\nokay so that's our model reloaded now\nagain what we can do is use the\nevaluate policy method which we had\nover here to test it out so remember\nthat the max score that we got after\ntesting out our five episodes up here\nwas\nfour so ideally this model should\nideally try to get a little bit better\nthan that so let's try that out\nso what we're going to do is we're going\nto use evaluate policy\nand we're going to pass through our\nmodel environment and then the number of\neval steps\nso we're going to do let's do 10 and\nlet's do\nrender equals true\nand let's take a look you must only pass\nokay so this\nis actually pretty common so when we\nactually go and evaluate\nwe can only go and evaluate on a single\nenvironment now remember correctly when\nwe went and created our environment\nright here we had four environments so\nwe vectorized them and trained them a\nwhole heap faster\nnow what we can do is we can actually\nsingle this down and leverage our\nvectorized\nmodel on a non-vectorized environment or\nan environment that only has\na single particular environment inside\nof it so let's go ahead and create one\nof those first up\nokay so we've now gone and recreated our\nenvironment now this time rather than\npassing through four\nenvironments in our make atari\nenvironment function we've only passed\nthrough one but we're still stacking it\nup as though that there's four\nenvironments so this is going to allow\nus to leverage it so now if we go and\nrun our evaluate policy method\nyou can see it should all be running\nwell so you can see it's playing\nway better now it's looking like got a\nfive six seven\nsix they're still playing way better\nthan the random agent was so you can see\nthere that on average we're getting a\nvalue of 6.1\nwith a standard deviation of 1.9 now\nwhat we could also do is we could also\ntest out that bigger model that i had in\nthere so again i can't remember\nhow well that was performing but let's\ngo ahead and test that out\nso the model path to that model is going\nto be\na2c 300k models we can copy that name\nand try loading that up so i'm just\ngoing to update the a2c path\nand then load that one and then recreate\nour environment you don't need to\nrecreate it\nbut then let's try this one out\nso if you get your environment sort of\nfreezing like this sometimes what you\nmight need to do is restart your\nnotebook so you can see there that it\ndoesn't look like it's\nopening up so ideally what you should do\nis just hit restart on your kernel for\nthis but make sure you save down your\nmodel before you do this i'm just going\nto hit restart\nhit restart again this should ideally\nclose oh we want to stop that yep so\nthat's good that's from another kernel\nnow what we're going to do is re-import\nour dependencies so just import that\nand then scroll on down and what we're\ngoing to do is define our a2c path\nload up that model we need to recreate\nour environment from down here\nthen load up that model and then try it\nagain\nso there you go so that looks like it's\nperforming way better already and you\ncan see that this model obviously it's\nbeen trained a lot longer but it's\ngetting into the tens and possibly the\n20s when it's actually playing\nso again the longer that you train the\nbetter that this model is actually going\nto get\nnow you could also try using some of the\nrecurrent policies\nbut at the moment they're not\nimplemented in stable baselines 3. i\nwill let you know once they are in the\npinned comment but that sort of gives\nyou an idea as to how you can go about\ntraining a reinforcement learning agent\nfor breakout now what we'll do is we'll\njust clean this up so if we type in enb\ndot close that'll close our atari\nenvironment it's just this one left\nso that is all well and good so we went\nand did a ton of stuff here so what we\ndid\nis we went and imported our dependencies\nand we imported a couple of new ones\nto work with atari we also went and\ninstalled the atari roms so remember\nyou've got to download them from atari\nmania and again i'll include this link\nin the description below\nwe vectorize our environment so rather\nthan using a single environment\nwe trained on four so this gives us a\nbit of a speed boost\nand then we also went and trained it up\nand then we went\nand saved it and evaluated it at the\nbottom and i also showed you the 300k\nmodel which you can see here\nit was getting an average score of 12.7\nwith a standard deviation of five so\noverall it was better but there was a\nhigher standard deviation\nso that sort of gives you an idea of\nwhat's possible by just training a\nlittle bit longer\nhey guys editing nick here before we\njump over to the next project i wanted\nto let you know that i ended up training\nthe breakout model\nfor an additional two million steps just\nto see what it would actually take a\nlook like\nnow after training for around about two\nmillion steps what we ended up with\nis an average reward around about the 20\npoint range\nnow this obviously is a markedly\nimproved\nresult over what we had in our original\nmodel so ideally you can see the impact\nof training for longer this is what it\nlooked like so as i was saying i ended\nup training the model for a\nwhole heap of additional time steps so\nall up i ended up training\nthe breakout model specifically with the\na2c algorithm for around about\ntwo million time steps now when i\nevaluated this model it looked like we\nwere getting around about an\naverage score of 21 which is again still\nway better than our random model still\nbetter than\nour 100 000 time step model so you can\nstart to see the impact of trading for\nlonger\nnow i'll also make this model available\ninside of the github repository so if\nyou want to test it out for yourself you\ncan start to see what that actually\nlooks like\nso the model name is a2c underscore 2m\nmodel so a2c trained for 2 million steps\nso what we can do is as per usual\nload this up into our environment and\nwhat we'll do is we'll load it into our\na2c\nalgorithm we'll create our environment\nthat has a single frame at a particular\ntime\nand then rather than evaluating for 10\ntime steps what we'll go on ahead and do\nis evaluate for 50.\nso what i'll do is i'll run this and\nthen i'll leave you to it so you can\nstart to see the boost in performance\nand then we'll come back at the end\nand take a look at what our end score\nwas so again we're going to run it for\n50 evaluation episodes so let's go on\nahead and do this and you can take a\nlook\n[Music]\nso already you can start to see that\nthis is performing way better so we're\nclearing\nthe tens we're clearing the 20s and\nevery now and then\nthe ultimate score is hitting 30. so\nagain way better than what we had in our\nprevious models\nbut again i'll leave you to it so you\ncan enjoy the performance and take a\nlook at how it's actually running\nuh\n[Music]\nup\nuh\n[Music]\nokay so that is 50 episodes now done so\nit looked like we actually cleared\n50 around about halfway through there so\nagain significantly better performance\nthan our other two models now if we\nactually take a look at our scores you\ncan see that our\nmean reward over 50 episodes is 22.22 so\nagain\nway better than the other models that we\nwere taking a look at and overall our\nstandard deviation\nwas 9.1 so again way better than what we\nwere taking a look at before\nand ideally this begins to show you what\nis possible when you go and train your\nmodel for a little bit longer\non to our next project now that is\nproject one now done now we're on to\nproject two so reinforcement learning\nfor autonomous driving\nso for this particular environment we're\ngoing to be using the racing car\nenvironment so this is\ntrying to get a car to drive around a\nrandomly generated race track\nso let's go on ahead and take a look at\nthis one\nso again still the same five steps that\nwe're going to be going through but in\nthis case\nslight bit different in terms of how\nwe're going to set it up now the first\nthing to note is that in order to\nleverage\nthe racing car environment you do need\nto install swig\nso in order to do that just take a look\nat installing squig\nand it's going to vary depending on\nwhether or not you're installing on a\nwindows machine or on a\nlinux machine so for windows i believe\nall you need to do is download the swig\nfile and then extract it and add it to\nyour path\nfor a mac i believe all you need to do\nis use homebrew to install it\nlet's take a look yep so you can\nactually use homebrew so brew install\nswig so way easier if you're doing it on\na mac\ncool so once you've got swig installed\nso again for windows you just download\nit extract it and then add it to your\npath and then you should be good to go\nfor mac you just got to use brew install\nbut again if you need a hand with that\nhit me up in the comments below then\nwhat we're going to do is install\ntwo new dependencies so we're going to\nneed the box 2d\nenvironment and we're also going to need\npiglet so let's go ahead and install\nthese\nup we've typed in jim wrong that should\nbe jim\nokay all good so what i've gone and\nwritten there is exclamation mark pip\ninstall gym and then inside of square\nbrackets box\n2d so when using the racing car\nenvironment you need to have box 2d\ninstalled that's what the racing car\nenvironment is built on top of\nso once you've got that installed you\nshould be good to go and then we're also\ninstalling piglet so again this is the\ndependency of that particular\nenvironment\nonce that's done all you need to do is\nagain import your dependencies so let's\ngo ahead and do that\nalrighty so we've gone and written five\nlines of code there so the first line is\nimporting open ai gym as per usual so\nimport gym then the second one is from\nstable\nunderscore baselines three import ppo\nnext one from stable underscore\nbaselines three dot common dot vec\nunderscore env import dummy vec env so\nagain this is really similar to what we\ndid\nover here in our main tutorial so again\nwe're going to be\nwrapping up our environment exactly as\nwe had down there\nthen the next line is importing our\nevaluate policy function and then\nlast but not least we're importing os\nalrighty now the next thing that we're\ngoing to do is test out our environment\nas per usual so let's go ahead and do\nthat\nokay that is our environment created so\nthis is just a warning so you can sort\nof ignore that\nnow what we can do is take a look at our\nenvironment again so emv\ndot reset so you can see that this is\ngenerating our\ntrack and we'll talk about that a little\nbit more\nwe can take a look at emv.actionspace as\nper usual\nand you can see it's going to be a box\nand we've got a three\ndifferent values between minus one and\none\nif we take a look at our observation\nspace\nyou can see again it's going to be a box\nand it's going to be values between 0\nand 255\nand it looks like it's going to be an\nimage so 96 by 96 by 3.\nso this means that we're going to have\nan image to be able to go ahead and\ntrain our racing environment\nnow if we type in emv.render we can take\na look at the\nenvironment itself\nyou can see it's not popping up let's\nbring it up\nthat's sort of what our racetrack looks\nlike so you can see that we've got the\nentire racetrack there\nnow we're actually going to test this\nout so we can type in emb.close so\nenv.render should probably talk about\nthis a little bit more\nso env.render allows you to render the\nactual environment that you're working\nin so this is an optional thing you\ndon't need to render\nit does slow down training if you're\nrendering while you're training but it\ngives you the ability to see your agent\nin action\nso we can type emv.close to close that\ndown\nso that should close down that\nenvironment so that's the old one cool\nnow what we're going to do is we're\ngoing to go ahead and test out our\nenvironment again so again we can just\ncopy this\nfrom our breakout tutorial so what we\ncan do is just copy this testing code\nand bring it in and again this is\nalmost identical right so we can\nuncomment our emv.close and go ahead and\nrun this so this is going to test out\nour environment\nso you can see that we're actually\ntrying to drive this car\nalong this racetrack now ideally you're\ngoing to get more points the longer it\nstays on\ninside of the track and the more turns\nit does now because this is just taking\nrandom actions it doesn't actually know\nwhere the track is at the moment so it's\njust going to go straight\nyou can see it's making a lot of\nmovement it's\nkind of performing okay but it's not\nable to take the turn\nso the first time it gets up to that\nturn it's failing right\nall right we don't need to watch so you\nsort of get the idea so the goal is to\nget this car to go around the racetrack\nnow\nwe can actually stop this and hit in\nvr.close to close it\nthis just sort of cleans it up that's\nfine we can leave that one open\nand now what we're going to do is go\nahead and train our model again so again\nsame sort of process\nthis time we're going to be using the\nppo algorithm so rather than using the\nother algorithm we're going to use a\nslightly\ndifferent one so let's go ahead oh so\nrather than using a2c\nlike we did for breakout we're going to\nuse ppo here\nso let's go ahead and train up our model\nwe should also take a look at what the\ndifferent actions are so if we take a\nlook uh\nwhat can we do mv\ndoesn't look like we've got it let's\ntake a look at how what we can pass a\ncar racing\nenvironment open ai gym\nso let's take a look at what we've\nactually got here if there's any\ndocumentation on the different actions\nit doesn't look like it so again\nsometimes you're going to get\nbetter better explanations of what's\nactually happening\nin different environments\nit looks like we've got something here\nso the raw actually this is useful\nso the reward is minus let's make this a\nbit bigger so the reward is minus\nzero or negative point one for every\nframe\nand plus one thousand divided by n for\nevery tractile visited\nso this means that for every track tile\nvisited you're going to get plus 1000\ndivided by n\nwhere n is the number total number of\ntiles it says slightly\nconfusing or complicated a reward\nfunction but you sort of get the idea\nso you get more rewards or you get more\npoints for being able to go down each\nand every frame as long as you're on the\ntiles\nso the game is solved when the agent\nconsistently gets 900 plus points so\nagain this is going to take some time to\nbe able to get to that point\num so remember it's a powerful rear\nwheel drive car don't press the\naccelerator\nsome indicators are shown at the bottom\nso we've got the true speed for abs\nsensors the steering wheel position and\nthe gyroscope\nso again pretty cool so we've got a\nwhole bunch of information here\nnow you can see there that this one does\nhave a finite\nset of reward statements which dictate\nwhether or not it's solved so\nideally you want to get over 900 points\nthat's going to take a long time so i\ntrained for 438 000 steps and i think i\nwas getting in the realm\nof 50 40 sort of points so again\ncan take a while to solve now what we're\ngoing to do in this case is again we're\ngoing to try to solve it\nand see how we go so let's do that so\nwe're going to go and train our model so\nwe're going to instantiate\nour environment and then we're going to\ngo on ahead and train it\nokay so that is our environment now set\nup so we've gotten written e env\nequals jim dot make and then environment\nname\nand then again we're wrapping it inside\nof our dummy vectorize environment\nwrapper because we're not actually going\nto vectorize this one\nit's again pretty similar to what we did\nin the main tutorial\nthen what we can go and do is set up our\nagent and our model so let's do it\nokay so that is our model set up so\nagain we've gone and specified our\nlogging path and this is where we're\ngoing to log out our tensorboard logs so\nwe've gone written os.path.join\nand then specified training and then\nspecified logs so that's going to be our\ndirectory\nand then we've actually gone and\nspecified our agent so model equals ppo\nso again we're going to be using the ppo\nalgorithm here and then we'll pass\nthrough cnn policy\npass through our environment pass\nthrough verbose equals one\nand specified the tensorboard log path\nnow again we're going to train but we're\nonly going to train for\n100 000 steps you might want to train\nfor a whole heap longer if you want to\ntry to hit that 900 score and if you do\nhit that 900 score\ndo let me know in the comments below and\nshare it out on twitter and linkedin i'd\nlove to see it\nso in this case we are going to go ahead\nand train our model for a hundred\nthousand steps so let's go on ahead and\ndo that\nso again to train our model we're just\ngonna write model model.learn so you're\ngoing to start to see there's a\nrepeatable process to this so you\ninstantiate your environment you create\nthe environment\nvectorize it if you need to and then set\nup your model and then go ahead and\ntrain it so in this case we're going to\ngo ahead and train it\nso let's go ahead kick this off and we\nwill be\nright back so let's just wait for the\ntraining to kick off successfully\nand there you go so you can see that\nwe're starting to get our output from\nour algorithm so we'll let that train\nfor a hundred thousand steps and we'll\nbe right back\nokay so that is our self-driving\nracing car now train so again we've gone\nand trained it for\na hundred thousand steps 100 352 to be\nexact\nnow again as per usual what we're going\nto go ahead and do is save our model and\nthen test it out so let's do it\nokay so that's our model now saved so\nwhat we've gone and written is ppo\nunderscore path to set up our path\nvariable so\nin order to do that written os dot path\ndot join\nand then specified that we want it in\nour training folder and then\nour saved underscore mod or saved models\nfolder and then we're gonna name it ppo\ndriving model\nand then again we've used model.save to\nbe able to go and save that down\nso if we now take a look we've now got\nthis model here ppo\nunderscore driving underscore model now\ni've also got another model that i\ntrained for 428 000 steps so we'll take\na look at that one as well\nbut for now let's go ahead and delete\nour model as per usual just to double\ncheck this all works\nand then let's load it back up so model\nequals ppo\ndot load and then we're going to pass\nthrough our ppo path\nand our environment cool so that's all\ngood\nnow what we can do is go ahead and\nevaluate as per usual so let's go ahead\nand do that\nso we're going to pass through evaluate\nunderscore policy and then we're going\nto pass through our model\nour environment and then the number of\nsteps\nso we're going to do 10 steps and then\nwe are going to\npass through render equals true\nso it looks like our car's ripping it\naround the track\ndoing a bunch of donuts\nso a key thing to note is that the car\nis high powered so it doesn't always so\nyou can see it's going around the corner\nbut it's having a bit of trouble\nsort of getting there this is great\nso you can see that because the car is\nso high powered it starts to lack\ntraction so in this case it can get\nstuck in this loop\nwhereas instead of actually driving\nforward it just goes and does\ndonuts okay so it sort of gave up there\ngot around the first corner\nup spinned out\nokay i think it's just gonna keep doing\nthis so\nall right so that's sort of what a\nhundred thousand steps gets to you so\nnot\nexactly the best uh racing car driver\njust kind of all right cool so let's\nstop this because it's clearly uh\nit's driving me and saying that it's\njust going all over the place and we're\njust gonna type in env\ndot close\nall right so that's now closed we've\nonly got that one open and rather than\nusing that one let's go ahead and load\nup the one that i trained\nfor i think it was 438 000 steps\n428 so let's go ahead and load up this\nmodel and again i'll make this\nmodel available in the github repo\nso i'm just going to change the name of\nthe ppo path and then load this one up\nand let's go ahead and test with this\nmodel\nso you can see it is a lot slower now\nbut it is at least\nsort of sticking to the track it's just\ncut the corner\nit's fine it's back on\nso you can see it's getting the score\nthat it's getting is much higher so it's\nwe're up to 190 200.\nso ideally you want to be able to get up\nto 900 so that means that it's going to\nhave to accelerate off the turn so this\nis obviously trained for 438\n000 steps so if you actually train for a\nlot longer you'd get a car that's\nactually rips it down the straights\ntakes the corners appropriately in this\ncase it's starting to sort of\nit's a little bit hesitant on the\nthrottle which you can see there so it's\nit's going but it's maybe not going as\nfast as it could there you go it's just\naccelerated\nit's back on track\nthere you go but you can see that this\nis obviously way better than the one\nthat we trained for 100 000 steps so\nthat sort of shows you the difference\nthe training for a lot longer and i did\nnothing different apart from just\ntrained for a longer period of time\nso again when you're training these\nreinforcement learning agents training\nfor a longer period of time can\nobviously help you out\na lot more and ideally produce a much\nbetter model so ideally for this i'd be\nlooking at something in the realm of\nlike\na million to two million steps to be\nable to get something great so if\nsomeone does have the time\nand if you do run it for that long by\nall means do let me know if you'd like\nto see me do it\ndo hit me up in the comments below i'd\nlove to take a look at this again\nbut for now that is our self-driving car\nsort of done it is a little bit\nglitchy on the throttle but you sort of\nget the idea\nnow we can go ahead and close this so\nstop that environment\nand then run emb.close to close it now\nagain remember\nin our main tutorial we also went\nthrough the ability to test it like this\nso rather than going through and using\nthe evaluate policy we could also do\nthis as well so if i copy step\n6 from the main tutorial we could\nactually plug this in\nso in this case we've got our\nenvironment that's all good our model\nthat's all good we could actually just\ntest this out so let's run it\nthere you go so you can see rather than\nusing evaluate policy we're now\nusing the i know what do you call it\nflow to be able to go and train this\nand you can see the car is getting\naround turn so we're up to what 250 now\ngod i don't know how you can watch this\nafter too many times it does get a\nlittle bit glitchy\nbut it's going around its corners it's\nmoving around and keep in mind we've\nonly trained this on\nthe image right so like we don't have\nany additional information but the image\nthat's actually coming out of this\nwhich is actually pretty cool right\nlet's get oh it actually took the corner\nthat's pretty cool\ncome on 270. not bad so again the the\nmax score to consider this solved is 900\nso ideally you'd want to train it\nfor a lot longer to be able to get this\nperforming way better\nand you can actually see our scores\nbeing logged down here so 255\n181 276 it just got 214.\nbut this is obviously way better than\nwhat we had in that random agent which\nwas getting like negative values\nalso i noticed that if it v is too far\noff the track and it's not able to see\nthe track anymore it sort of gets stuck\nand just stops there\nthis is on the 438k model but again you\ncould train it longer and you'd get\nbetter results so that sort of gives you\nan idea as to how you can leverage\nreinforcement learning\nfor autonomous driving and in this case\nracing\nhey guys editing nick here again so i\nalso ended up training the self-driving\ntutorial for a whole heap more steps\nnow again i trained this model for about\ntwo million steps\nand this significantly improved the\nperformance of this particular model\nso in the actual tutorial we got around\nin the range of about 200 to 300 in\nterms of our reward estimate\nnow when i went and trained it for a\nwhole heap longer we were heading\ntowards\nthe range of around about 700 not quite\ncompletely solved\nbased on the environment metrics but\nideally you can see again\nit's performing a whole heap better this\nis what that looked like\nso as i was saying i ended up training\nthe self-driving model\nfor a whole heap longer all up i ended\nup training it for two million\nadditional steps now the reason that i\nwanted to show you this is just so you\ncan see the impact of training for\nlonger so this is obviously one\ntechnique\nthat you can go about leveraging in\norder to improve the performance of your\nmodels\nso again all up two million steps and\ni'll make this model available in the\ngithub repository so you can pick this\nup\nso if you class your minds back we had\nthree different models all up now so we\ntrained the first model which was just\ndoing burnouts\nit's not really making it past the first\ncorner we had the second model which was\nsuper jittery and then we've got this\nmodel now as well so i'll make this one\navailable\nso in order to load this one up all i\nneeded to do is again\nsimilar to what we did for our previous\nmodels i can just load it using the ppo\npath and model.load or ppo.load\nand then we can go and run this model\nnow what you should see is that this\nmodel performs a\nwhole heap better than the previous\nmodels it'll still spin out on a corner\nevery now and then\nbut again it's getting a lot further and\nscoring a lot higher than those other\nmodels that we train\nso let's go ahead and take a look at\nthis one\n[Music]\nso you can see there it got up to about\n800 so not too bad so every now and then\nit's gonna lose focus and\nsort of veer off the track but you can\nsee it's performing a whole heap better\nthan the other models that we had\ntrained so it'll\nspin out but then it works its way back\nto the track and it eventually starts\ntaking the corners pretty well again\nso every now and then you'll see one\nthat performs not so well but you can\nsee that this is performing\nsignificantly better than what we had\nbefore\nso again getting into the 700 range\nthere we go that was another 770 score\nso you can see down the bottom again\nwhen we're evaluating our model what our\nperformance is looking like so if we\nbring that a little bit further up\n[Music]\nand open it up it doesn't look like\nwe're printing out so let's let these 10\nepisodes run and then you'll eventually\nsee the total score\nor the automated sum of the results so\ni'll be quiet now and i'll let you enjoy\nthis\n[Music]\n[Music]\n[Music]\nand that is all 10 episodes complete so\nyou can see that we had a\nsignificant boost in terms of our\nperformance simply by training for\nlonger now if we actually take a look at\nthe results of our evaluate policy\nyou can see that our final score down\nhere our average score over 10 episodes\nwas 741 so again\nnot quite hitting that golden 900 mark\nbut again still\nway better than what we had in our\nprevious models this also had a standard\ndeviation of about 123 so again\na reasonably high standard deviation in\nthis particular case\nbut this sort of shows you what's\npossible when you ideally go and tune\nyour model and train\nfor a little bit longer on to our next\nproject\nnow on that note that is project two\nnow done now the last project that we're\ngoing to be taking a look at is\nreinforcement learning for custom\nenvironments now\nif you've watched my shower environment\nor shower custom environment tutorial\nthis is going to be that same\nenvironment but we're going to be using\nstable bass lines as the algorithms to\nbe able to solve this so without further\nado let's kick off project three\nso again all of these notebooks are\ngoing to be available\ninside of the github repository so if\nyou want to pick these up\nby all means do grab them let me know\nhow you go with them and if you get\nstuck\nplease do reach out to me i'm more than\nhappy to help\nso let's go on ahead and do this so\nthere's a bunch of dependencies that\nwe're going to be importing here\nnamely because we're going to be\ndefining our own environment in this\ncase\nso let's go ahead and import these\ndependencies and then we'll take a step\nback and take a look at those\nokay so we've gone and written nine\nlines of code there so there's quite a\nfair bit\nnow in this case what i've gone and\nwritten is i've broken it up into three\nspecific sections\nso these are our gym dependencies or\nopen ai gym dependencies\nthese are some of our helpers that we're\ngoing to need and then down here we've\ngot our stable baseline stuff\nso first up from jim we're importing\nmore than just the gym\npackage this time so written import gym\nwhich is going to give us a pretty\nstandard import\nthen what we're doing is we're importing\nthe gym environment class so to do that\nwe've written from\ngym import env so this is going to be\nthe\nsuper class that we're going to be able\nto use to build our own environment\nthen what we've written is from gym dot\nspaces\nimport discrete box dict tuple multi\nbinary and multi-discrete\nso each one of these represents all of\nthe different types of spaces that are\navailable inside of openai gym so i\nwanted to sort of show you what each one\nof these different types of spaces looks\nlike\nand how to actually use them we'll\nprobably only use\nthe two common ones discrete and box in\nour environment but i wanted to give you\nan idea as to how these all fit together\nthen we've gone and brought in some\nhelpers so we've imported numpy so\nimport\nnumpy as np we've imported random so\nimport random\nwe've imported operating system so\nimport os\nand then we've gone and imported our\nstandard stable baseline stuff so from\nstable underscore baselines three\nimport ppo from stable underscore\nbaselines three\nimport common dot vec underscore emv\nimport\ndummy vec nv again pretty standard and\nthen we've imported our evaluate policy\nfunction so again\nthis is really really common most of\nthis is pretty common and jim is pretty\ncommon the new stuff is\nthese couple of lines here so\nlet's go and have a look at our\ndifferent types of spaces so we've got a\nbunch that we've\nimported over here let's take a look at\neach one of these so\nfirst up is discrete so we can dive in\ndiscrete\nand then say we wanted three different\nactions we can do that so that's going\nto give us our discrete space now we can\nactually sample it and take a look at\nall the different types of values so\nagain 0 1\nand we should get up to 2. so you can\nsee this is going to give you a value\nbetween 0 1 and 2 by passing through\ndiscrete equals 3.\nso if you had an action and an action\nmapped or each one of those actions\nmapped to a specific value so 0 1 or 2\nthat's how you'd use that then we've got\na box\nso that's our box space so to do that\nwe're in box and then we'll pass through\nzero\nand then comma one so this is our low\nvalue this is our upper value and then\nthis this is the shape of the output\nthat we're going to get\nso we're going to get an array that's\nthree by three so ideally you'll have a\nlist of lists so if we take a look at\nthat\nby sampling it so again you've got an\narray\nand inside of that array you've got\nthree individual rays and those arrays\nhave three values so again\nexactly the same formatting so we've got\nthose values between zero\nand one so again you might use this if\nyou were trying to\nlook at different types of sensors or if\nyou had continuous variables\nyou'd use a box now again if you just\nwanted three values you could do that\nand that's just going to give you three\nvalues as well so all i've done is i've\nreduced it\ninto an array of three values\nthen what we've got is a tuple now at\nthe moment stable baselines doesn't\nsupport a tuple but if you wanted to use\nit you could still take a look\nso to that we can pass through a\ndiscrete environment or a discrete space\nand really your tuple space just allows\nyou to combine different spaces so if we\ndo that\nyou can see our tuple is now combined of\nour discrete environment and our\ndiscrete box space so if we sample it\nyou can see we're getting a discrete\nvalue first up and then we're getting\nour box second\nokay on to our next one so again so far\nwe've done discrete\nbox and tuple next one that we want to\ntake a look at is\ndict so this is really similar to a\ntuple the only difference is that\nrather than passing through a 2-port or\ntuple you pass through a dictionary so\nlet's do it\nokay so that is our dict space so what\ni've written is dict\nand then open braces and then to that\nwe've actually passed through this\ndictionary here\nnow this dictionary has two keys so\nheight and then\nheight is equal to discrete two so\nreally it's no different to typing\ndiscrete two\nup here and then we've created another\nkey which is speed\nand we've set that equal to box and then\ntwo box remember you're going to pass\nthrough\nthree key arguments so you're going to\npass through your lower value\nyour upper value and then the shape that\nyou want so in this case i've specified\na shape of\n1 comma which means i'm only going to\nget a single value back\nbetween the values of 0 and 100. so if\nwe actually go and sample this\nyou can see we've got our height key\nwhich is represented as 0\nbecause remember it's going to be\nbetween 0 1 and 2 and then we've also\ngot our speed which in this case is a\nvalue between 0\nand 100 pretty cool right so that gives\nus a dict space\nnow the next space that we want to take\na look at is multi-binary\nso in order to create that space we've\njust written multi-binary and then\npassed through the number of\npositions that we want in our\nmulti-binary space so multi-binary 4 is\ngoing to give us 4 positions so we can\ngo and sample it\nand in this case you can see that we've\ngot 0 1 2\n3 and 4 positions and in this case it's\ngoing to be a binary set of values\nso either zeros or one so if we go and\nsample it multiple times you can see\nit's just different combinations of\nzeros and ones in those four positions\ncool now the last type of space that we\nwant to take a look at is multi-discrete\nso again\ngoing to be pretty similar to\nmulti-binary except rather than being\nbinary values\nthey're going to be discrete values\nbetween any value that we want now so\nlet's go ahead and do it\nokay so that is our multi-discrete\nspace so to do that written\nmulti-discrete and then two that i've\npassed through a list\nand the values are passed are five two\nand two so if we go and hit sample\nso again you're going to get three\ndifferent values and these values are\ngoing to vary\ndepending on what parameters you've\npassed through to the list so because\ni've passed through 5\nthe first value is going to vary between\n0 and 4\nbecause i pass through 2 it's going to\nvary between 0 and two\nactually is this going let's actually\ntake a look\ni believe max it's going to get up to\nyeah yeah so it's going to be\nzero to four and then zero to one and\nthen zero to one again so again this is\nthe upper cap so it starts at zero\ncool so we can keep going through that\nand you can sort of see what happens so\nif we go and pass through another value\nnow\nwe're just going to get another discrete\nvalue appended onto the end of that so\nthat's really a summary of all the\ndifferent types of spaces\nthat you've got available inside of open\nai gym so you've got a discrete space a\nbox space\ntuple dict multi-binary and\nmulti-discrete so discrete is when you'd\nhave a discrete number of actions and\nthose mapped through to a single integer\nbox gives you continuous variables\nremember with your box you just pass\nthrough your lower value\nyour upper value and then the shape of\nthe box that you actually want\nyour tuple allows you to combine\ndifferent types of spaces together\nas a tuple but is not currently\nsupported by a\nstable baseline so something to keep in\nmind so remember to your tuple you just\npass through\na set of braces and then the two\ndifferent types or whatever types of\nspaces you want so we've passed you\ndiscrete here and then\nbox here as well so again this is inside\nof braces there\nthen we've got our dick space so again\nto our dick we just pass through a\ndictionary of different types of spaces\nso we've got our discrete and we've also\ngot our box space there\nwe've got our multi-binary space so\nagain we've got that now\nkeep in mind you could actually grab\nthat multi-binary space and add it to\nyour tuple as well so if we do that\nwe are just in there so again\nnow we've gone and added another type of\nspace to our tuple so again we could do\nthis with that dict\nso say for example i could call this um\ncolor i don't know\nso again you can add multiple versions\nto the two-point dick they're sort of\nlike\ngrouping spaces right so we've got\nmulti-binary and then to that you pass\nthrough the number of positions that you\nwant in your binary space and we've also\ngot multi-discrete and this gives you a\nbunch of different discrete\ntypes of values so again there's not a\nlot of documentation out there on these\nso i figured i'd do a little bit of a\ncrash course on them\nif you'd like to see more on that by all\nmeans do hit me up in the comments below\nbut now we're going to be building our\nown environment now\nthe goals of this environment are to\nbasically build an agent\nto give us the best shower possible\nnow what's going to happen is randomly\nthe temperature is going to fluctuate\nbecause there's other people in the\nbuilding so it's going to randomly\ngo up and down now we know that our\noptimal temperature is between\n37 and 39 degrees\nso we want to be able to train an agent\nto automatically respond to changes in\ntemperature\nand get it within that 37 and 39 degrees\nrange now keep in mind that our agent\ndoesn't actually know that we prefer\nour temperature to be within 37 and 39\ndegrees\nso it's going to need to learn what\ntypes of adjustments it can make\nto get it to within that range just\nsomething to keep in mind\nso remember this is a simulated\nenvironment so our agent doesn't\nactually\nknow how it accumulates its reward it\njust knows that by doing certain actions\nit's going to get a reward now we know\nit\nwe want to get it between 37 and 39 but\nour agent doesn't\nso let's go ahead and build this\nenvironment so there's a few different\nfunctions that we need to implement in\nthis environment to get it valid\nso let's go ahead let's build a shell\nand then we'll fill it up\nokay so at a high level that is our\nshower environment now we obviously\nhaven't gone and implemented the\ndifferent components into it\nbut these are the four key functions\nthat you need to have inside of your\nshower environment class\nso let's take a look at what we've got\nso far so what i've gone and read in\nthis class\nand then inside a capitals or camel case\ni've got shower env\nand then to that we're passing through\nour env class which is from our gm\nenvironment up there\nand then colon then we've got four\ndifferent functions that we've gone and\nimplemented so we've got the init\nfunction so which\ntriggers when we create our class the\nstep function the render function and\nthe reset function\nso to do that we're in def underscore\nunderscore init and then two that were\npassing through self inside a pair of\nbrackets and then a colon and then right\nnow we've just written pass\nthis allows us to define it without\nhaving any errors for now\nthen we've defined a step function so\ndef step\nand then to that we'll pass through self\nand then we're passing through the\naction that we're actually going to pass\nthrough to our\nenvironment so remember when we use\nenv.step we pass through our action and\nit actually does something\nthen we've gone and defined a render\nfunction so def render and then two that\nwill pass through self and then colon\nand then pass so we're not going to do\nanything in our render function for now\ni actually as part of building this\ncourse i actually built out a\ngiant or started building out a giant\npie game environment but it was sort of\nblowing out of proportion so if you'd\nlike to see a video on reinforcement\nlearning for gaming\nwhich involves building a custom\nenvironment using pi game please do let\nme know in the comments below i'd love\nto hear your thoughts\nand then our last function is reset so\ndef reset and then to that again we're\ngoing to be passing through self\ncolon and then right now we've got past\nso let's go ahead and initially\nset up our init function and then we'll\nkeep going\nokay that is our initialization function\nand now\ndone so we went and wrote four lines of\ncode there\nso first up we defined our action space\nso to do that we wrote self\ndot action space and we set that equal\nto discrete\nthree so remember this is no different\nto saying discrete\nthree and the three actions that we're\ngoing to have\nare whether or not we turn the tap up\nwhether or not we turn the tap down or\nwhether or not we leave the tap\nunchanged so this basically gives us\nthree discrete actions now you could\nchange this and have it actually as a\nbox type action space where you actually\nturn the tap by a certain amount or by a\ncertain number of degrees\nbut in this case we're going to keep it\npretty simple and say up down or hold\nthen we've gone and defined our\nobservation space so to do that written\nself\ndot observation underscore space equals\nbox\nand then we've set it to equal to two\nnumpy arrays so in this case we've got\na low value so low equals mp dot array\nand then pass through zero and then\nwe've gone and specified a high value so\nhigh\nequals mp.array and then to that we've\npassed through the number 100\nso this means that our observation space\nlet's actually extract that so we can\ntake a look\nso this means that our observation space\nis going to be a value between 0\nand 100 and have the value of 1. so if\nwe type in dot sample\nwe can do that so you can see that\nthat's going to be the value that we get\nback now we can actually change this so\nwe can just make it 0\nand 100 and pass through shape\nequals 1 comma that should ideally give\nus the same\ntype of output so delete that\nso there you go so same sort of output\nand if we type in dot sample\nagain we're going to get the same type\nof output so again two different ways of\ndefining it in this case i've just\nswapped it out but you can sort of see\nthat you've got multiple ways of\ndefining that box space\nthen we've gone and defined as initial\nstate so this is going to set our\ninitial state so we'll set that equal to\n38 plus a random integer between\n-3 and 3. so this means that our shower\nis going to start out at 38 degrees plus\nor minus 3.\nand the goal of our agent is going to be\nto get it within that magic range\nof 37 and 39 degrees\nnow we've also set another variable so\nthis is going to effectively represent\nour episode length\nin this case it's our shower link so\nwe're only going to shower for 60\nseconds it's a fast shot i know\nso what we've gone and written is self\ndot shower underscore length equals 60.\nso what we're going to do inside of our\nstep function\nis decrease that by one second every\ntime we go through and take an\naction so let's go on ahead and now\ndefine\nour step function\nokay so we've now gone and filled out\nour step function so\nin this particular case what we've got\nis let's say one two\nthree four five six six different code\nblocks\nso the first one is applying the impact\nof our action on our state so remember\nwe had\nthree different actions so zero one or\ntwo so zero is going to represent\ndecreasing the temperature of our shower\nby one degree\none is going to represent no change and\nthen two is going to represent\nincreasing the temperature of our shower\nby one degree so in order to do that\nreasonably simply we've written self dot\nstate\nplus equals the action minus one so\nremember our action is going to be\ndiscrete what was it three\nso if we go and sample that\nso in this case we've got one so by\nminusing one we're going to get the\nvalue\nso actually let's actually print it out\nso in this case if we take the action of\none that is going to be\nakin to leaving the temperature the same\nso if we minus one\nagain it's going to apply zero change to\nour temperature so self.state is going\nto stay the same\nif we get a different value say for\nexample we get 2\nby minusing 1 which is what we're doing\nhere we're going to increase the\ntemperature of the shower by 1 degree\nand if we get 0 we're effectively going\nto be subtracting one\ncool so now the next thing that we've\ngone and done\nis then decrease the shower time so\nevery time we take a step or take an\naction we're going to decrease the\nlength of our shower by one so remember\nwe\ndefined it up here initially to 60\nseconds you could change this to\nsomething different if you wanted to\nso we've gone and defined self dot\nshower underscore length\nminus equals one so that's going to\ndecrease it and then this is\nreally really important so this is where\nwe actually define our reward so again\nif you had a really complicated reward\nschema this is where you'd be doing it\nso what we've gone and done is we've\nwritten if self.state\nso remember state is our temperature is\ngreater than or equal to 37. so remember\nthe magic ratio it's got to be between\n37 and 39 degrees\nand self.state is less than 39 degrees\nthen the reward is one in all other\ncases so say for example if our shower\nis completely out of that range\nour reward is going to be negative one\nnow you could also make the reward zero\nas well\nthen what we're also doing is we're\nchecking whether or not a shower is done\nbecause if our episode is done then we\nwant to stop that particular episode so\nif\nself.shower underscore length is less\nthan equal to zero\nthen done is set to true let's fix that\nwe've gone\nscrewed that up then else done equals\nfalse so again if it's\nnot if we haven't fully consumed the 60\nseconds then the ash hour is not done\nthen we're creating a blank info\ndictionary so if we wanted to pass\nadditional stuff\nout of here we could do that in there\nand then out of this we're returning our\nself.state which is going to be our\ntemperature\nour reward for that particular episode\nwhether or not we're done and our info\nso again\nout of this we're returning all of these\nbits of information that we've gone and\ncalculated in our step function\nnow in this case our render function\nwe're not actually going to do anything\nin here so we could implement biz if we\nwanted to\nwe're not going to do anything there but\nif you wanted to you definitely could\nthen the last function that we need to\nimplement is our reset function\nkey thing to know is that if you wanted\na more detailed tutorial on how to\nimplement render and again pygame\ni'd love to do something on that and if\nyou've got a specific use case hit me up\nin the comments below\nbecause i'd love to hear some ideas as\nwell in this case let's go ahead and\nwrap up this environment so for our\nreset function we just need to reset our\ninitial temperature and we also need to\nreset our shower time to 60 seconds\nso let's go ahead and do it\nokay i think that is our environment\nnow done so for our reset function what\nwe're effectively doing we could\nactually potentially drop this\nself.state up here\nbecause we're going and re-initializing\nit inside of our reset but that's fine\nfor now\nso what we've gone and written in\nself.state equals\nnp dot array and then to that we're\npassing through our\nsame random initialization function so\ninside of a set of square brackets\nwe've passed through 38 degrees plus\nrandom dot rand int between -3 and 3. so\nagain you could\nchoose a broader random initialization\nif you wanted to\nin this case i've just chosen three and\nthen we've specified as type float\nbecause remember our box is going to be\nwe haven't specified that it's going to\nbe an integer\nwe could do that as well so you could\nspecify d types in this case we're going\nto leave it as a float\nnow what we're also doing is we're\nresetting our shower length to 60 so\nself dot shower underscore length\nequals 60 and then we're returning our\nself dot state\nso that should be our environment all\nwell and good now now what we can do is\nwe can actually test out this\nenvironment so emv\nequals shower env\nand then we can run inv dot observation\nspace as per usual\nand you can see we're getting our box\nback and if we type in dot sample\nthis is our initial temperature and if\nwe keep doing that you can sort of see\nthat there\nand we can type in emb dot action space\nand again we've got our discrete space\ndot sample\nand there you go so you can see that\nwe've gone through the breakout tutorial\nthe self-driving tutorial and you're\nprobably thinking how these space is\nbuilt well this is exactly how they're\ndone\nwhen you're dealing with gaming\nimplementations there's a lot more work\ndone around the render\naround the observation space as well as\naround the reward space because it's a\nlittle bit more sophisticated\nand again if you'd like to see that done\nlet me know i've started i've already\ngot the template code sort of built\nlove to do your tutorial if you guys are\ninterested now in this case\nwhat we're actually going to go ahead\nand do is test our environment and train\nit as per usual so again\nwhat we can go ahead and do is let's\njust copy the exact same testing code\nthat we used for our driving tutorial\nwhich is this here\nand this is under step two test\nenvironment we can actually paste this\nhere now\nthe cool thing is that we've actually\ngone and defined an environment to a\nstate that we can actually use it as\npart of a template code so if we go and\nrun this\nyou can see that it's automatically gone\nand smashed through all of those\nepisodes so it's got our score printed\nout\nnow remember our score is going to\nincrement by one\nif we've got the shower between 37 and\n39\nand it's gonna decrease by one if it's\noutside that range so you can see that\njust by running those five episodes\nwe've got a\nhigh score of 26 and the lowest of minus\n60. so again we can keep running this\nand you can see it's very quick and this\nis because we don't have a sophisticated\nrender function and it's just\nit's all text based so again it's going\nto go very very quickly\nnow in this case what we're going to go\nahead and do is train our model and then\nsave it so you can sort of see how to do\nthis\non a custom environment so let's go\nahead and do that\nokay so we've gone and initialized our\nmodel and it's automatically wrapped\ninside of the dummy vec nv so even\nthough we've gone and imported it over\nhere\nlooks like it's automatically wrapped so\nwe're good to go so we've written log\nunderscore path equals os dot path dot\njoin and then true that will pass\nthrough training and then log so\nremember\nit's going to follow that same sort of\nlogging directory that we set up\nand then we specified model equals ppo\npass through the policy that we want to\nuse in this case mlp policy\nthis is different because in our\nbreakout tutorial in our self-driving\ntutorial we had an\nimage returned now we've got sort of\ntabular data or tensorbase data or\nactually well\nis sort of the same but we've actually\ngot a array of values rather than an\nimage so we're going to use the mlp\npolicy\nthen we're passing through our\nenvironment specifying verbose equals\none\nand then specifying a tensorboard a log\npath\ncool now the next thing that we need to\ndo is just go on ahead and train so\nagain this is going to train really\nreally quickly so you don't need to do\na super long training run so let's just\ntry it out so model dot\nlearn to train and then total time steps\ni don't know let's set this to 4000 for\nexample so let's go ahead kick this off\nand let it train so this should train\nreasonably quickly because again it's\nusing an mlp policy and it's all\ntabular data i mean you can see the\nframes per second is five and that\nthat's actually done\nso it literally went that quick so let's\nactually run it for longer so rather\nthan 4000 let's give it 40 000.\nall right so you can see that that's now\nrunning it's doing about 600 frames per\nsecond so really really quickly\nand that's one thing to call that so\nwith the game environments they're going\nto take longer to train\nversus like a simple environment like\nthis so again the more sophisticated the\nenvironment the longer it's going to\ntake to train so just sort of keep that\nin mind\nwhen you're planning your projects and\nwhen you're sort of committing to\nclients when you're building this type\nof stuff\nand again if you need help by all means\ndo call me out i'm more than happy to\nhelp\nyou can start to see that we're getting\nour episode reward mean so in this case\nminus 28.2\nminus 25.9 so it looks like it's\ndropping so it should ideally get into\nthe positives\nminus 21.7 minus 14.9 it's getting close\nminus 16.9 and again the episode length\nmean is going to be the same pretty much\nall the time it's going to be 60 seconds\nbecause that's the maximum remember\ni might need to train this for a little\nlonger looks like it's getting close but\nit's not into the positives yet all\nright it's minus 15.5 let's actually\ngive it another i don't know\n20 000 steps let's run that\nso minus 4.12 so again you can see it's\nstarting to get close into the positives\nminus 5.18\n9.8\nlet's let that run and we'll be right\nback\nokay so it got pretty close it's episode\nreward mean over here got to about minus\n4.22 so i guess this depends on the\nstarting point and how the model\nactually\ndevelops from there so let's actually go\nand test it out and see how we actually\ngo\nso again we can use evaluate policy here\nor we should save our model so model dot\nsave let's define our path\nuh what are we going to call this shower\nmodel\nlet's just double check our directory\nname again\nso it's training saved models so let's\nspecify that and we're going to call\nthis\nshower model\nppo cool so we can type in model.save\nshower path and again if we go and take\na look that should be\n[Music]\nshallow model underscore ppo so again\nthat is now saved so we're good\nagain we can delete our model and if we\nwanted to reload it we can just type in\nmodel equals ppo dot load pass through\nour path\npass through our environment and then if\nwe wanted to test it out we can run\nevaluate policy\npass through our model pass through our\nenvironment pass through the number of\neval episodes\nand we don't need a render this time\nbecause we don't have a render function\nso if we type in render equals true\nshould throw an error it might actually\nnot throw an error because we've got the\n[Music]\nwhat did we have because we've got past\nso we're all good\nnow in this case we've got a mean\nepisode reward of 12 with a standard\ndeviation of 58.78 so again it's\ngetting there but it's very it's got\nwide variance so again you could train\nthis for a whole lot longer tighten up\nthe environment make it a little bit\nmore realistic than only being able to\nadjust\nup down and sort of steady state but\nthat sort of gives you an idea as to how\nto bring this all together\nso in this tutorial we went through a\nbunch of stuff as well so we took\nuh so we imported all of our\ndependencies we took a look at all of\nour different types of spaces\nremember there's a discrete space box\ntuple dict\nmulti binary and multi-discrete and just\nkeep in mind that stable baselines\ndoesn't support tuples yet\nwe also took a look at how to build our\nenvironment so remember we had to define\nour init function our step function\nour render function and last but not\nleast our reset function\nand then again in terms of testing and\ntraining and saving the model\nit's all very much the same but again if\nyou've got a really sophisticated model\nthat you'd like to build by all means\nhit me up in the comments below i'd love\nto help you out and if you do build some\nreally cool environments do let me know\ni'd love to see them\nas well on that note\nwe have now finished project number\nthree so it comes to our wrap-up\nso hopefully you've enjoyed this course\nso in it we've gone and covered a whole\nheap of stuff and remember the core\npurpose of this course\nis to bridge the gap between a lot of\nthe theoretical stuff that you see\nfloating around there in terms of\nreinforcement learning\nand show you a practical set of\nimplementation steps\nso we went through rl in a nutshell and\ntalked about what reinforcement learning\nis and how it works\nwe took a look at how to set up our\nenvironment with stable baselines we\nthen went and built and took a look at\nsome different types of environments\nusing open ai gym in step number two\nwe then went and trained a model we then\nwent and tested\nand evaluated it so we took a look at\nhow we can view it inside a tensorboard\nwe then extended out some of our\nalgorithms and specifically we went and\nimplemented callbacks\nwe went and used different algorithms so\nremember we all up we use ppo\na2c and we used a dqn algorithm as well\nwe even went and changed our policy\narchitecture so again some pretty cool\nstuff happening there\nand then we went through our three\ndifferent projects so remember we went\nand trained a model to play\nbreakout we went and trained a model to\nrace a car around a racing track and we\nalso\nwent and built our own custom shower\nenvironment so all up we did quite a\nfair bit\nnow i want to leave you with some\nadditional resources so if you haven't\ngone through david silva's reinforcement\nlearning course\ni'd highly recommend you do his team are\nthe team behind deepmind and the guys\nthat actually built\nthe alpha go model so obviously super\nsmart dude\ngot some awesome theories for floating\naround there and by all means i do\nrecommend you check it out\nthere's also a great book called\nreinforcement learning and\nintroduction by richard sutton and\nandrew bartos some of the pioneers in\nthe reinforcement learning field\ni highly recommend you go and check\nthose that book out it's got some\nawesome stuff in it as well\nnow in terms of what to learn next i\nlove it when people give me ideas as to\nwhere to go from here\nand i want to give you the same so one\nof the things that we didn't cover in\nthis course is hyper parameter tuning so\none of the ways that you can\nimprove how you train your models is to\ntune the hyper parameters\nthat you start and you progress your\nalgorithms with so again that might be\nsomething\nthat you take a deeper look into and if\nyou'd like to see a tutorial on that or\na course on that by all means\ndo hit me up building detailed custom\nenvironments for example we talked about\nthis a little bit in terms of\nimplementing a render function with pi\ngame as well as integration with other\nsimulation systems like mojoko and unity\nand then last but not least i think one\nof the coolest things that you could\npotentially take a look at learning\nis how to do an end-to-end\nimplementation so say for example\nyou actually went and built a cart pole\nrobot and actually got went and trained\nin a simulated environment\nand then implemented it on a real\nenvironment\nperhaps using a raspberry pi based robot\ni think that would be an awesome thing\nto go and take a look at next\nbut on that note that about wraps it up\nhopefully you've enjoyed this and thanks\nagain for tuning in\nthanks so much for tuning in guys if you\nenjoyed this video be sure to give it a\nthumbs up hit subscribe and tick that\nbell and let me know\nany feedback or anything that you'd like\nto see going on from this and if you get\nstuck\nat all by all means do hit me up in the\ncomments below i'm happy to help you out\nthanks again for tuning in peace\n",
  "words": [
    "hey",
    "nick",
    "working",
    "oh",
    "man",
    "working",
    "awesome",
    "stuff",
    "actually",
    "using",
    "reinforcement",
    "learning",
    "train",
    "race",
    "car",
    "race",
    "around",
    "track",
    "oh",
    "really",
    "going",
    "yeah",
    "going",
    "great",
    "yup",
    "great",
    "burnouts",
    "music",
    "promise",
    "guys",
    "get",
    "better",
    "let",
    "get",
    "happening",
    "guys",
    "name",
    "nicholas",
    "chernotte",
    "welcome",
    "reinforcement",
    "learning",
    "course",
    "video",
    "going",
    "covering",
    "bunch",
    "stuff",
    "basically",
    "core",
    "goal",
    "able",
    "allow",
    "go",
    "absolute",
    "beginner",
    "able",
    "go",
    "leverage",
    "reinforcement",
    "learning",
    "going",
    "cover",
    "ton",
    "stuff",
    "specifically",
    "set",
    "environment",
    "work",
    "different",
    "algorithms",
    "also",
    "test",
    "environments",
    "using",
    "open",
    "ai",
    "gym",
    "able",
    "balance",
    "cart",
    "pole",
    "able",
    "build",
    "car",
    "last",
    "least",
    "also",
    "going",
    "take",
    "look",
    "build",
    "custom",
    "environments",
    "something",
    "important",
    "comes",
    "able",
    "leverage",
    "reinforcement",
    "learning",
    "use",
    "case",
    "relevant",
    "end",
    "video",
    "able",
    "take",
    "away",
    "skill",
    "sets",
    "able",
    "leverage",
    "reinforcement",
    "learning",
    "practical",
    "manner",
    "ready",
    "let",
    "get",
    "alrighty",
    "guys",
    "welcome",
    "reinforcement",
    "learning",
    "course",
    "course",
    "intended",
    "practical",
    "guide",
    "terms",
    "getting",
    "running",
    "reinforcement",
    "learning",
    "ideally",
    "aims",
    "bridge",
    "gap",
    "lot",
    "theory",
    "see",
    "practical",
    "implementation",
    "going",
    "covering",
    "ton",
    "stuff",
    "course",
    "let",
    "take",
    "look",
    "game",
    "plan",
    "first",
    "going",
    "going",
    "taking",
    "look",
    "rl",
    "nutshell",
    "really",
    "talks",
    "specifically",
    "section",
    "going",
    "talking",
    "reinforcement",
    "learning",
    "works",
    "learns",
    "applications",
    "around",
    "rl",
    "well",
    "limitations",
    "going",
    "take",
    "look",
    "set",
    "environment",
    "work",
    "reinforcement",
    "learning",
    "going",
    "using",
    "library",
    "called",
    "stable",
    "baselines",
    "step",
    "2",
    "going",
    "taking",
    "look",
    "environments",
    "environments",
    "one",
    "half",
    "equation",
    "comes",
    "working",
    "reinforcement",
    "learning",
    "need",
    "able",
    "set",
    "environment",
    "specifically",
    "open",
    "ai",
    "gym",
    "environments",
    "able",
    "work",
    "reinforcement",
    "learning",
    "going",
    "kick",
    "training",
    "whole",
    "bunch",
    "different",
    "types",
    "algorithms",
    "available",
    "inside",
    "stable",
    "baselines",
    "going",
    "take",
    "look",
    "set",
    "algorithms",
    "able",
    "train",
    "reinforcement",
    "learning",
    "agent",
    "step",
    "4",
    "trained",
    "model",
    "going",
    "test",
    "evaluate",
    "easier",
    "sounds",
    "set",
    "environment",
    "test",
    "see",
    "agent",
    "actually",
    "looks",
    "like",
    "also",
    "going",
    "take",
    "look",
    "evaluation",
    "well",
    "take",
    "look",
    "different",
    "metrics",
    "understand",
    "metrics",
    "also",
    "take",
    "look",
    "open",
    "inside",
    "tensorboard",
    "something",
    "really",
    "really",
    "like",
    "take",
    "one",
    "step",
    "step",
    "five",
    "take",
    "look",
    "leverage",
    "callbacks",
    "stop",
    "model",
    "trading",
    "hit",
    "certain",
    "threshold",
    "see",
    "use",
    "different",
    "algorithms",
    "whole",
    "bunch",
    "algorithms",
    "available",
    "reinforcement",
    "learning",
    "need",
    "write",
    "whole",
    "bunch",
    "already",
    "written",
    "use",
    "take",
    "look",
    "use",
    "also",
    "take",
    "look",
    "different",
    "architectures",
    "say",
    "example",
    "wanted",
    "change",
    "neural",
    "network",
    "sits",
    "behind",
    "particular",
    "agent",
    "well",
    "would",
    "course",
    "unless",
    "projects",
    "well",
    "going",
    "taking",
    "look",
    "three",
    "different",
    "projects",
    "going",
    "take",
    "look",
    "solve",
    "breakout",
    "environment",
    "atari",
    "game",
    "sort",
    "like",
    "pong",
    "little",
    "bit",
    "really",
    "also",
    "take",
    "look",
    "solve",
    "environment",
    "car",
    "racing",
    "environment",
    "train",
    "model",
    "picture",
    "input",
    "train",
    "car",
    "drive",
    "along",
    "racetrack",
    "think",
    "pretty",
    "awesome",
    "also",
    "take",
    "look",
    "custom",
    "environments",
    "something",
    "think",
    "often",
    "overlooked",
    "allow",
    "get",
    "better",
    "understanding",
    "build",
    "environment",
    "work",
    "reinforcement",
    "learning",
    "framework",
    "going",
    "using",
    "build",
    "customer",
    "environment",
    "going",
    "open",
    "ai",
    "gym",
    "going",
    "show",
    "different",
    "types",
    "spaces",
    "worry",
    "understand",
    "yet",
    "sure",
    "talking",
    "go",
    "great",
    "detail",
    "okay",
    "game",
    "plan",
    "nutshell",
    "time",
    "take",
    "look",
    "irl",
    "nutshell",
    "wanted",
    "include",
    "section",
    "give",
    "little",
    "bit",
    "context",
    "reinforcement",
    "learning",
    "meant",
    "used",
    "applications",
    "well",
    "limitations",
    "going",
    "full",
    "deep",
    "dive",
    "theory",
    "maths",
    "behind",
    "high",
    "level",
    "overview",
    "get",
    "idea",
    "rl",
    "fits",
    "big",
    "world",
    "machine",
    "learning",
    "data",
    "science",
    "first",
    "reinforcement",
    "learning",
    "well",
    "reinforcement",
    "learning",
    "focuses",
    "teaching",
    "agents",
    "trial",
    "error",
    "really",
    "really",
    "high",
    "level",
    "statement",
    "know",
    "probably",
    "lot",
    "hardcore",
    "deep",
    "learning",
    "engineers",
    "probably",
    "go",
    "nick",
    "quite",
    "right",
    "sort",
    "gives",
    "idea",
    "reinforcement",
    "learning",
    "learns",
    "ideally",
    "got",
    "agent",
    "learns",
    "based",
    "reward",
    "gets",
    "try",
    "something",
    "get",
    "reward",
    "tries",
    "something",
    "else",
    "get",
    "reward",
    "gets",
    "bigger",
    "reward",
    "might",
    "try",
    "multiple",
    "times",
    "also",
    "got",
    "thing",
    "called",
    "exploration",
    "exploitation",
    "talk",
    "little",
    "bit",
    "later",
    "sort",
    "get",
    "idea",
    "reinforcement",
    "learning",
    "learning",
    "based",
    "actively",
    "engaging",
    "environment",
    "brings",
    "us",
    "framework",
    "actually",
    "fits",
    "together",
    "well",
    "four",
    "key",
    "things",
    "well",
    "five",
    "key",
    "things",
    "need",
    "consider",
    "whenever",
    "working",
    "within",
    "reinforcement",
    "learning",
    "four",
    "fundamental",
    "concepts",
    "agent",
    "environment",
    "action",
    "reward",
    "plus",
    "observations",
    "think",
    "agent",
    "something",
    "operating",
    "within",
    "environment",
    "might",
    "machine",
    "learning",
    "model",
    "might",
    "also",
    "person",
    "player",
    "working",
    "game",
    "environment",
    "environment",
    "particular",
    "agent",
    "actually",
    "operating",
    "case",
    "say",
    "example",
    "take",
    "game",
    "player",
    "operating",
    "within",
    "game",
    "environment",
    "getting",
    "reward",
    "based",
    "actually",
    "agent",
    "see",
    "happening",
    "within",
    "environment",
    "say",
    "example",
    "taking",
    "look",
    "game",
    "player",
    "able",
    "see",
    "around",
    "terms",
    "observation",
    "see",
    "game",
    "environment",
    "actually",
    "looks",
    "like",
    "also",
    "see",
    "reward",
    "accrues",
    "based",
    "actions",
    "takes",
    "ideally",
    "agent",
    "might",
    "walk",
    "around",
    "environment",
    "might",
    "something",
    "might",
    "accumulate",
    "point",
    "might",
    "something",
    "else",
    "might",
    "accumulate",
    "point",
    "might",
    "even",
    "lose",
    "life",
    "might",
    "negative",
    "reward",
    "really",
    "really",
    "good",
    "way",
    "sort",
    "get",
    "head",
    "around",
    "think",
    "might",
    "go",
    "training",
    "dog",
    "say",
    "example",
    "wanted",
    "teach",
    "dog",
    "sit",
    "lay",
    "well",
    "agent",
    "case",
    "going",
    "dog",
    "trying",
    "train",
    "agent",
    "able",
    "take",
    "right",
    "action",
    "reward",
    "case",
    "giving",
    "dog",
    "treat",
    "every",
    "time",
    "right",
    "thing",
    "dog",
    "might",
    "try",
    "take",
    "action",
    "initially",
    "might",
    "say",
    "sit",
    "dog",
    "might",
    "actually",
    "anything",
    "case",
    "actually",
    "taken",
    "taken",
    "action",
    "nothing",
    "particular",
    "case",
    "environment",
    "working",
    "environment",
    "trying",
    "get",
    "reward",
    "trying",
    "get",
    "treat",
    "particular",
    "thing",
    "dog",
    "eventually",
    "see",
    "gets",
    "reward",
    "sit",
    "might",
    "try",
    "something",
    "else",
    "case",
    "might",
    "say",
    "sit",
    "might",
    "sit",
    "say",
    "get",
    "reward",
    "ideally",
    "start",
    "learn",
    "action",
    "take",
    "response",
    "environment",
    "order",
    "maximize",
    "reward",
    "observing",
    "command",
    "giving",
    "able",
    "take",
    "right",
    "action",
    "nutshell",
    "reinforcement",
    "learning",
    "works",
    "agent",
    "tries",
    "take",
    "action",
    "order",
    "maximize",
    "rewards",
    "response",
    "observations",
    "within",
    "environment",
    "wanted",
    "give",
    "little",
    "bit",
    "theory",
    "going",
    "delve",
    "much",
    "sort",
    "get",
    "idea",
    "reinforcement",
    "learning",
    "works",
    "little",
    "bit",
    "different",
    "terms",
    "might",
    "work",
    "tabular",
    "deep",
    "learning",
    "machine",
    "learning",
    "agent",
    "actively",
    "engaging",
    "simulated",
    "real",
    "environment",
    "case",
    "going",
    "dealing",
    "simulated",
    "environments",
    "talk",
    "little",
    "bit",
    "later",
    "applications",
    "practical",
    "applications",
    "reinforcement",
    "learning",
    "well",
    "whole",
    "heap",
    "becoming",
    "reinforcement",
    "learning",
    "really",
    "really",
    "popular",
    "right",
    "whole",
    "heap",
    "open",
    "world",
    "environments",
    "people",
    "trying",
    "solve",
    "using",
    "machine",
    "learning",
    "deep",
    "learning",
    "one",
    "autonomous",
    "driving",
    "see",
    "picture",
    "actually",
    "environment",
    "called",
    "kala",
    "kala",
    "really",
    "really",
    "popular",
    "driving",
    "simulation",
    "allows",
    "actually",
    "train",
    "autonomous",
    "agents",
    "perform",
    "reinforcement",
    "learning",
    "actually",
    "train",
    "car",
    "able",
    "navigate",
    "open",
    "world",
    "using",
    "reinforcement",
    "learning",
    "pretty",
    "pretty",
    "cool",
    "right",
    "another",
    "great",
    "application",
    "reinforcement",
    "learning",
    "securities",
    "trading",
    "think",
    "agent",
    "case",
    "like",
    "autonomous",
    "trader",
    "environment",
    "going",
    "securities",
    "trading",
    "environment",
    "ideally",
    "going",
    "going",
    "try",
    "train",
    "agent",
    "make",
    "trades",
    "going",
    "make",
    "profit",
    "ideally",
    "wants",
    "buy",
    "low",
    "sell",
    "high",
    "sell",
    "high",
    "buy",
    "low",
    "short",
    "selling",
    "really",
    "really",
    "popular",
    "moment",
    "heap",
    "stuff",
    "happening",
    "space",
    "another",
    "one",
    "personally",
    "fascinated",
    "neural",
    "network",
    "architecture",
    "search",
    "actually",
    "use",
    "reinforcement",
    "learning",
    "build",
    "neural",
    "network",
    "find",
    "optimal",
    "neural",
    "network",
    "think",
    "absolutely",
    "crazy",
    "say",
    "example",
    "trying",
    "build",
    "deep",
    "neural",
    "network",
    "solve",
    "particular",
    "use",
    "case",
    "might",
    "know",
    "best",
    "type",
    "architecture",
    "terms",
    "layers",
    "terms",
    "number",
    "units",
    "times",
    "activations",
    "could",
    "actually",
    "use",
    "reinforcement",
    "learning",
    "try",
    "solve",
    "problem",
    "obviously",
    "super",
    "advanced",
    "sort",
    "gives",
    "idea",
    "possible",
    "tech",
    "another",
    "place",
    "reinforcement",
    "learning",
    "super",
    "popular",
    "right",
    "robotics",
    "training",
    "agents",
    "training",
    "robots",
    "real",
    "life",
    "often",
    "quite",
    "expensive",
    "say",
    "example",
    "got",
    "one",
    "robot",
    "hard",
    "train",
    "lot",
    "tasks",
    "actually",
    "build",
    "simulated",
    "environments",
    "particular",
    "robot",
    "train",
    "robot",
    "particular",
    "thing",
    "case",
    "agent",
    "going",
    "autonomous",
    "model",
    "training",
    "robot",
    "environment",
    "operating",
    "case",
    "agent",
    "believe",
    "trying",
    "move",
    "ball",
    "correct",
    "position",
    "actually",
    "based",
    "simulation",
    "environment",
    "called",
    "mujocho",
    "show",
    "little",
    "bit",
    "later",
    "going",
    "solving",
    "one",
    "today",
    "sort",
    "get",
    "idea",
    "actually",
    "train",
    "robot",
    "environment",
    "going",
    "moving",
    "ball",
    "right",
    "place",
    "reward",
    "going",
    "close",
    "far",
    "ball",
    "optimal",
    "position",
    "whole",
    "heap",
    "applications",
    "sort",
    "shown",
    "four",
    "ton",
    "another",
    "place",
    "really",
    "really",
    "popular",
    "gaming",
    "gaming",
    "open",
    "world",
    "environment",
    "reward",
    "function",
    "really",
    "really",
    "different",
    "every",
    "time",
    "sort",
    "see",
    "start",
    "apply",
    "different",
    "environments",
    "okay",
    "limitations",
    "considerations",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "absolutely",
    "amazing",
    "fascinated",
    "limitations",
    "specifically",
    "simple",
    "problems",
    "reinforcement",
    "learning",
    "sometimes",
    "overkill",
    "say",
    "example",
    "taking",
    "look",
    "hyper",
    "parameter",
    "optimization",
    "already",
    "really",
    "really",
    "powerful",
    "models",
    "particularly",
    "dealing",
    "simple",
    "models",
    "getting",
    "super",
    "advanced",
    "problems",
    "reinforcement",
    "learning",
    "could",
    "help",
    "space",
    "another",
    "thing",
    "assumes",
    "environment",
    "markovian",
    "means",
    "future",
    "states",
    "environment",
    "based",
    "current",
    "observations",
    "random",
    "acts",
    "know",
    "real",
    "life",
    "going",
    "random",
    "events",
    "happen",
    "influence",
    "particular",
    "model",
    "say",
    "example",
    "training",
    "mujoko",
    "robot",
    "right",
    "particular",
    "case",
    "environment",
    "might",
    "cater",
    "people",
    "walking",
    "past",
    "robot",
    "knocking",
    "robot",
    "never",
    "really",
    "know",
    "going",
    "happen",
    "real",
    "life",
    "train",
    "best",
    "case",
    "scenario",
    "sort",
    "deal",
    "reinforcement",
    "learning",
    "model",
    "going",
    "sort",
    "isolate",
    "environment",
    "something",
    "take",
    "mind",
    "somebody",
    "asks",
    "question",
    "another",
    "thing",
    "note",
    "training",
    "take",
    "long",
    "time",
    "always",
    "stable",
    "got",
    "concept",
    "called",
    "exploration",
    "exploitation",
    "ideally",
    "model",
    "tries",
    "explore",
    "environment",
    "starting",
    "tries",
    "exploit",
    "able",
    "get",
    "best",
    "possible",
    "rewards",
    "sometimes",
    "might",
    "happen",
    "model",
    "might",
    "enough",
    "time",
    "explore",
    "might",
    "start",
    "exploiting",
    "early",
    "sometimes",
    "need",
    "tune",
    "hyper",
    "parameters",
    "able",
    "get",
    "model",
    "truly",
    "explore",
    "environment",
    "truly",
    "understand",
    "sometimes",
    "get",
    "quite",
    "right",
    "model",
    "might",
    "stable",
    "might",
    "get",
    "certain",
    "point",
    "might",
    "reach",
    "cap",
    "terms",
    "maximum",
    "reward",
    "another",
    "thing",
    "note",
    "well",
    "training",
    "take",
    "long",
    "time",
    "got",
    "really",
    "really",
    "open",
    "environment",
    "say",
    "example",
    "trying",
    "train",
    "reinforcement",
    "learning",
    "model",
    "grand",
    "theft",
    "auto",
    "huge",
    "environment",
    "training",
    "model",
    "sort",
    "work",
    "particular",
    "case",
    "going",
    "take",
    "long",
    "long",
    "time",
    "right",
    "sort",
    "wanted",
    "bring",
    "limitations",
    "considerations",
    "note",
    "let",
    "start",
    "getting",
    "onto",
    "setup",
    "step",
    "number",
    "one",
    "going",
    "setup",
    "first",
    "going",
    "install",
    "required",
    "dependencies",
    "case",
    "really",
    "really",
    "simple",
    "get",
    "running",
    "single",
    "pip",
    "install",
    "need",
    "run",
    "exclamation",
    "mark",
    "pip",
    "install",
    "stable",
    "dash",
    "baselines",
    "three",
    "insider",
    "square",
    "brackets",
    "pass",
    "extra",
    "stable",
    "baselines",
    "reinforcement",
    "learning",
    "library",
    "allows",
    "work",
    "model",
    "free",
    "algorithms",
    "talk",
    "later",
    "work",
    "stable",
    "baselines",
    "build",
    "reinforcement",
    "learning",
    "agent",
    "able",
    "train",
    "specific",
    "environment",
    "cool",
    "thing",
    "stable",
    "baselines",
    "actually",
    "based",
    "original",
    "library",
    "called",
    "baselines",
    "built",
    "open",
    "ai",
    "great",
    "thing",
    "stable",
    "bass",
    "lines",
    "whole",
    "heap",
    "really",
    "really",
    "useful",
    "helpers",
    "got",
    "documentation",
    "screen",
    "full",
    "migration",
    "link",
    "wanted",
    "go",
    "stable",
    "baselines",
    "going",
    "include",
    "links",
    "description",
    "well",
    "code",
    "seeing",
    "see",
    "stable",
    "baselines",
    "actually",
    "documentation",
    "whole",
    "heap",
    "guides",
    "really",
    "really",
    "well",
    "supported",
    "environment",
    "really",
    "well",
    "supported",
    "library",
    "well",
    "really",
    "really",
    "really",
    "useful",
    "um",
    "whole",
    "bunch",
    "getting",
    "started",
    "information",
    "want",
    "able",
    "leverage",
    "sort",
    "shows",
    "get",
    "started",
    "really",
    "really",
    "quickly",
    "one",
    "single",
    "reinforcement",
    "learning",
    "environment",
    "training",
    "single",
    "like",
    "40",
    "20",
    "lines",
    "code",
    "get",
    "started",
    "really",
    "really",
    "quickly",
    "going",
    "going",
    "great",
    "detail",
    "walking",
    "let",
    "kick",
    "things",
    "start",
    "installing",
    "stable",
    "baselines",
    "going",
    "working",
    "inside",
    "jupiter",
    "notebook",
    "environment",
    "going",
    "give",
    "baseline",
    "code",
    "starter",
    "code",
    "well",
    "completed",
    "code",
    "well",
    "inside",
    "github",
    "repo",
    "description",
    "able",
    "pick",
    "work",
    "pace",
    "first",
    "things",
    "first",
    "going",
    "10",
    "different",
    "steps",
    "going",
    "going",
    "main",
    "tutorial",
    "going",
    "projects",
    "well",
    "first",
    "thing",
    "need",
    "well",
    "let",
    "actually",
    "take",
    "look",
    "10",
    "steps",
    "first",
    "going",
    "import",
    "dependencies",
    "going",
    "load",
    "environment",
    "case",
    "going",
    "solving",
    "reasonably",
    "simple",
    "environment",
    "called",
    "cartpole",
    "show",
    "sec",
    "going",
    "take",
    "look",
    "understand",
    "environment",
    "important",
    "train",
    "reinforcement",
    "learning",
    "model",
    "show",
    "save",
    "disk",
    "reload",
    "wanted",
    "go",
    "move",
    "elsewhere",
    "go",
    "deploy",
    "could",
    "take",
    "look",
    "evaluate",
    "test",
    "view",
    "logs",
    "inside",
    "tensorboard",
    "add",
    "call",
    "back",
    "training",
    "stage",
    "allows",
    "stop",
    "training",
    "certain",
    "point",
    "happy",
    "change",
    "policies",
    "well",
    "use",
    "alternate",
    "algorithm",
    "going",
    "covering",
    "quite",
    "fair",
    "bit",
    "take",
    "pace",
    "get",
    "stuck",
    "questions",
    "hit",
    "comments",
    "join",
    "discord",
    "server",
    "link",
    "description",
    "always",
    "happy",
    "chat",
    "well",
    "right",
    "enough",
    "let",
    "actually",
    "kick",
    "thing",
    "write",
    "code",
    "first",
    "thing",
    "going",
    "install",
    "dependencies",
    "import",
    "case",
    "going",
    "installing",
    "stable",
    "baselines",
    "three",
    "remember",
    "exclamation",
    "mark",
    "pip",
    "install",
    "stable",
    "dash",
    "baseline",
    "street",
    "square",
    "brackets",
    "extra",
    "let",
    "go",
    "ahead",
    "write",
    "alrighty",
    "looks",
    "like",
    "installed",
    "successfully",
    "see",
    "got",
    "warning",
    "says",
    "upgrade",
    "pip",
    "fine",
    "worry",
    "looks",
    "like",
    "good",
    "go",
    "case",
    "done",
    "really",
    "simple",
    "get",
    "started",
    "stable",
    "bass",
    "lines",
    "single",
    "pip",
    "install",
    "much",
    "makes",
    "pretty",
    "cool",
    "next",
    "thing",
    "want",
    "oh",
    "let",
    "actually",
    "take",
    "look",
    "line",
    "written",
    "exclamation",
    "mark",
    "pip",
    "install",
    "stable",
    "dash",
    "going",
    "screw",
    "stable",
    "dash",
    "baselines",
    "three",
    "extra",
    "reason",
    "passing",
    "three",
    "stable",
    "baselines",
    "gone",
    "number",
    "iterations",
    "stable",
    "bass",
    "lines",
    "one",
    "stable",
    "bass",
    "lines",
    "two",
    "stable",
    "bass",
    "lines",
    "three",
    "latest",
    "package",
    "runs",
    "tensorflow",
    "pytorch",
    "going",
    "using",
    "pytorch",
    "uh",
    "something",
    "keep",
    "mind",
    "reason",
    "passing",
    "three",
    "right",
    "installation",
    "done",
    "good",
    "go",
    "next",
    "thing",
    "want",
    "actually",
    "import",
    "stuff",
    "let",
    "go",
    "ahead",
    "import",
    "dependencies",
    "talk",
    "one",
    "okay",
    "main",
    "dependencies",
    "imported",
    "gotten",
    "written",
    "five",
    "lines",
    "code",
    "first",
    "written",
    "import",
    "os",
    "os",
    "operating",
    "system",
    "library",
    "going",
    "make",
    "little",
    "bit",
    "easier",
    "later",
    "go",
    "define",
    "paths",
    "save",
    "model",
    "well",
    "log",
    "imported",
    "jim",
    "jim",
    "open",
    "ai",
    "gym",
    "going",
    "talk",
    "little",
    "get",
    "environment",
    "section",
    "slides",
    "jim",
    "allows",
    "us",
    "build",
    "environments",
    "work",
    "environments",
    "really",
    "really",
    "easily",
    "imported",
    "first",
    "algorithm",
    "actually",
    "imported",
    "ppo",
    "written",
    "stable",
    "underscore",
    "baselines",
    "three",
    "import",
    "ppo",
    "whole",
    "heap",
    "different",
    "types",
    "algorithms",
    "actually",
    "take",
    "look",
    "actually",
    "taking",
    "look",
    "stable",
    "baselines",
    "package",
    "stable",
    "baselines",
    "three",
    "actually",
    "take",
    "look",
    "whole",
    "heap",
    "different",
    "algorithms",
    "a2c",
    "ddpg",
    "dqn",
    "ppo",
    "sac",
    "td3",
    "whole",
    "bunch",
    "stuff",
    "actually",
    "going",
    "talk",
    "use",
    "algorithm",
    "um",
    "circumstances",
    "fret",
    "seen",
    "like",
    "oh",
    "god",
    "much",
    "actually",
    "going",
    "go",
    "actually",
    "give",
    "little",
    "bit",
    "guide",
    "least",
    "guide",
    "rails",
    "use",
    "particular",
    "type",
    "algorithm",
    "case",
    "going",
    "using",
    "one",
    "ppo",
    "want",
    "see",
    "documentation",
    "take",
    "look",
    "performance",
    "particular",
    "algorithm",
    "cool",
    "right",
    "okay",
    "line",
    "stable",
    "underscore",
    "baselines",
    "import",
    "ppo",
    "next",
    "one",
    "written",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "common",
    "dot",
    "vec",
    "underscore",
    "env",
    "import",
    "dummy",
    "vec",
    "env",
    "talk",
    "little",
    "get",
    "breakout",
    "tutorial",
    "basically",
    "stable",
    "baselines",
    "allows",
    "vectorize",
    "environments",
    "means",
    "allows",
    "train",
    "machine",
    "learning",
    "model",
    "train",
    "reinforcement",
    "learning",
    "agent",
    "multiple",
    "agents",
    "multiple",
    "environments",
    "time",
    "means",
    "get",
    "huge",
    "boost",
    "training",
    "speed",
    "case",
    "going",
    "vectorizing",
    "environment",
    "going",
    "able",
    "use",
    "dummy",
    "vec",
    "env",
    "wrapper",
    "instead",
    "see",
    "actually",
    "looks",
    "like",
    "vectorize",
    "breakout",
    "project",
    "think",
    "wrapper",
    "around",
    "environment",
    "makes",
    "easier",
    "work",
    "stable",
    "bass",
    "lines",
    "next",
    "thing",
    "written",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "common",
    "dot",
    "evaluation",
    "import",
    "evaluate",
    "underscore",
    "policy",
    "evaluate",
    "underscore",
    "policy",
    "makes",
    "easier",
    "test",
    "model",
    "actually",
    "performing",
    "actually",
    "get",
    "run",
    "average",
    "reward",
    "certain",
    "number",
    "episodes",
    "talk",
    "later",
    "also",
    "get",
    "standard",
    "deviation",
    "particular",
    "agent",
    "training",
    "five",
    "lines",
    "code",
    "import",
    "os",
    "import",
    "gym",
    "import",
    "algorithm",
    "ppo",
    "import",
    "dummy",
    "vec",
    "wrapper",
    "import",
    "evaluate",
    "policy",
    "helper",
    "used",
    "somewhere",
    "around",
    "cool",
    "pretty",
    "much",
    "terms",
    "dependency",
    "written",
    "five",
    "lines",
    "code",
    "step",
    "two",
    "environments",
    "think",
    "key",
    "thing",
    "call",
    "difference",
    "simulated",
    "real",
    "environments",
    "using",
    "open",
    "ai",
    "gym",
    "open",
    "ai",
    "gym",
    "allows",
    "build",
    "simulated",
    "environments",
    "really",
    "really",
    "easily",
    "whole",
    "heap",
    "helpers",
    "really",
    "well",
    "supported",
    "library",
    "particularly",
    "really",
    "really",
    "popular",
    "comes",
    "working",
    "reinforcement",
    "learning",
    "key",
    "thing",
    "call",
    "working",
    "reinforcement",
    "learning",
    "often",
    "one",
    "benefits",
    "using",
    "simulated",
    "environments",
    "able",
    "reduce",
    "cost",
    "able",
    "produce",
    "better",
    "models",
    "whole",
    "heap",
    "faster",
    "say",
    "example",
    "working",
    "engineering",
    "company",
    "engineering",
    "company",
    "wants",
    "build",
    "autonomous",
    "agent",
    "go",
    "train",
    "robot",
    "able",
    "move",
    "particular",
    "ball",
    "certain",
    "position",
    "robot",
    "actually",
    "called",
    "fetch",
    "row",
    "actually",
    "real",
    "robot",
    "actually",
    "take",
    "look",
    "looks",
    "like",
    "might",
    "able",
    "afford",
    "single",
    "robot",
    "sort",
    "limits",
    "fast",
    "may",
    "able",
    "train",
    "particular",
    "robot",
    "obviously",
    "costs",
    "involved",
    "actually",
    "training",
    "robot",
    "going",
    "wearing",
    "joints",
    "going",
    "using",
    "electricity",
    "going",
    "take",
    "lot",
    "time",
    "cost",
    "able",
    "train",
    "robot",
    "real",
    "environment",
    "one",
    "amazing",
    "things",
    "reinforcement",
    "learning",
    "try",
    "simulate",
    "environment",
    "able",
    "train",
    "agent",
    "way",
    "see",
    "actually",
    "replica",
    "robot",
    "actually",
    "built",
    "inside",
    "simulation",
    "tool",
    "called",
    "mojoko",
    "talked",
    "little",
    "bit",
    "earlier",
    "makes",
    "whole",
    "heap",
    "easier",
    "train",
    "whole",
    "heap",
    "cost",
    "effective",
    "able",
    "go",
    "ahead",
    "train",
    "agent",
    "actually",
    "pretty",
    "cool",
    "mean",
    "technology",
    "around",
    "whole",
    "heap",
    "time",
    "obviously",
    "improves",
    "ability",
    "people",
    "leverage",
    "reinforcement",
    "learning",
    "rather",
    "go",
    "real",
    "real",
    "time",
    "particular",
    "agent",
    "simulated",
    "environment",
    "run",
    "ultimately",
    "may",
    "find",
    "whilst",
    "may",
    "train",
    "simulated",
    "environment",
    "end",
    "goal",
    "take",
    "agent",
    "go",
    "deploy",
    "environment",
    "would",
    "real",
    "robot",
    "likewise",
    "game",
    "might",
    "train",
    "testing",
    "version",
    "game",
    "might",
    "deploy",
    "real",
    "version",
    "game",
    "sort",
    "get",
    "idea",
    "simulated",
    "real",
    "environment",
    "simulated",
    "going",
    "real",
    "open",
    "ai",
    "gym",
    "comes",
    "open",
    "ai",
    "gym",
    "gives",
    "really",
    "lightweight",
    "environment",
    "really",
    "feature",
    "packed",
    "able",
    "go",
    "build",
    "reinforcement",
    "learning",
    "environment",
    "case",
    "actually",
    "take",
    "look",
    "docs",
    "https",
    "colon",
    "forward",
    "slash",
    "forward",
    "slash",
    "forward",
    "slash",
    "docs",
    "actually",
    "go",
    "link",
    "go",
    "docs",
    "see",
    "whole",
    "heap",
    "documentation",
    "around",
    "actually",
    "use",
    "open",
    "ai",
    "gym",
    "nice",
    "thing",
    "particularly",
    "used",
    "particular",
    "environment",
    "framework",
    "whole",
    "heap",
    "support",
    "really",
    "really",
    "well",
    "supported",
    "lot",
    "people",
    "using",
    "comes",
    "open",
    "air",
    "gym",
    "know",
    "looking",
    "cutting",
    "edge",
    "stuff",
    "looking",
    "skills",
    "learn",
    "wanted",
    "go",
    "career",
    "open",
    "ai",
    "gym",
    "tends",
    "standard",
    "particular",
    "space",
    "whole",
    "heap",
    "environments",
    "actually",
    "use",
    "inside",
    "openai",
    "gym",
    "remember",
    "talking",
    "mujoko",
    "particular",
    "robot",
    "actually",
    "oh",
    "might",
    "mujo",
    "might",
    "robotics",
    "see",
    "actually",
    "got",
    "fetch",
    "robot",
    "also",
    "shadow",
    "hand",
    "robot",
    "actually",
    "based",
    "real",
    "robots",
    "actually",
    "google",
    "fetch",
    "robot",
    "meant",
    "type",
    "fetch",
    "robot",
    "actually",
    "see",
    "actually",
    "real",
    "robot",
    "actually",
    "showing",
    "robot",
    "exactly",
    "mimicking",
    "robot",
    "hand",
    "one",
    "actually",
    "called",
    "shadow",
    "hand",
    "robot",
    "shadow",
    "robot",
    "believe",
    "go",
    "actually",
    "see",
    "actually",
    "based",
    "real",
    "robots",
    "real",
    "world",
    "people",
    "trying",
    "train",
    "using",
    "open",
    "ai",
    "gym",
    "also",
    "bunch",
    "environments",
    "around",
    "algorithms",
    "around",
    "atari",
    "little",
    "bit",
    "later",
    "around",
    "box",
    "2d",
    "actually",
    "going",
    "testing",
    "one",
    "classic",
    "control",
    "going",
    "testing",
    "cartpol",
    "mujoko",
    "robotics",
    "toy",
    "text",
    "forth",
    "whole",
    "heap",
    "also",
    "whole",
    "heap",
    "environments",
    "wanted",
    "something",
    "really",
    "really",
    "hardcore",
    "could",
    "definitely",
    "take",
    "look",
    "well",
    "remember",
    "talking",
    "carla",
    "believe",
    "one",
    "actually",
    "wrapper",
    "able",
    "leverage",
    "color",
    "part",
    "open",
    "ai",
    "gym",
    "case",
    "going",
    "dealing",
    "classic",
    "control",
    "begin",
    "going",
    "keep",
    "relatively",
    "simple",
    "try",
    "solve",
    "carpol",
    "environment",
    "actually",
    "take",
    "look",
    "goal",
    "particular",
    "case",
    "get",
    "little",
    "robot",
    "able",
    "balance",
    "beam",
    "see",
    "right",
    "sort",
    "bumping",
    "around",
    "side",
    "side",
    "beam",
    "sort",
    "falling",
    "two",
    "actions",
    "really",
    "take",
    "move",
    "left",
    "move",
    "cut",
    "right",
    "delve",
    "little",
    "bit",
    "going",
    "able",
    "train",
    "reinforcement",
    "learning",
    "agent",
    "able",
    "solve",
    "particular",
    "problem",
    "going",
    "next",
    "actually",
    "going",
    "taking",
    "look",
    "environment",
    "actually",
    "looks",
    "like",
    "key",
    "thing",
    "note",
    "actually",
    "taking",
    "look",
    "open",
    "ai",
    "gym",
    "environments",
    "environments",
    "represented",
    "something",
    "called",
    "spaces",
    "number",
    "different",
    "types",
    "spaces",
    "open",
    "ai",
    "gym",
    "supports",
    "names",
    "might",
    "little",
    "bit",
    "tricky",
    "comes",
    "actually",
    "leveraging",
    "let",
    "sort",
    "walk",
    "first",
    "one",
    "box",
    "range",
    "values",
    "think",
    "say",
    "example",
    "wanted",
    "continuous",
    "value",
    "going",
    "want",
    "use",
    "box",
    "space",
    "way",
    "instantiate",
    "box",
    "space",
    "using",
    "box",
    "passing",
    "low",
    "value",
    "high",
    "value",
    "shape",
    "space",
    "going",
    "delve",
    "whole",
    "heap",
    "actually",
    "take",
    "look",
    "environment",
    "actually",
    "going",
    "use",
    "spaces",
    "actually",
    "build",
    "custom",
    "environment",
    "project",
    "next",
    "type",
    "space",
    "discrete",
    "set",
    "items",
    "type",
    "discrete",
    "pass",
    "value",
    "three",
    "actually",
    "going",
    "get",
    "back",
    "terms",
    "space",
    "values",
    "0",
    "1",
    "actually",
    "going",
    "give",
    "discrete",
    "numbers",
    "represent",
    "specific",
    "mappings",
    "something",
    "typically",
    "see",
    "discrete",
    "actions",
    "used",
    "typically",
    "see",
    "discrete",
    "spaces",
    "used",
    "actions",
    "action",
    "zero",
    "something",
    "action",
    "one",
    "something",
    "action",
    "two",
    "something",
    "else",
    "uh",
    "also",
    "got",
    "tuples",
    "tuple",
    "allows",
    "combine",
    "spaces",
    "together",
    "see",
    "use",
    "tuple",
    "pass",
    "discrete",
    "box",
    "allows",
    "join",
    "key",
    "thing",
    "note",
    "stable",
    "baselines",
    "support",
    "tupor",
    "good",
    "know",
    "going",
    "use",
    "much",
    "also",
    "got",
    "dict",
    "spaces",
    "dictionary",
    "spaces",
    "really",
    "similar",
    "two",
    "balls",
    "case",
    "declaring",
    "dict",
    "passing",
    "dictionary",
    "spaces",
    "two",
    "types",
    "spaces",
    "ones",
    "dealt",
    "much",
    "important",
    "note",
    "got",
    "space",
    "one",
    "hot",
    "encoded",
    "set",
    "binary",
    "values",
    "pass",
    "multi",
    "binary",
    "pass",
    "value",
    "four",
    "going",
    "get",
    "list",
    "values",
    "going",
    "four",
    "positions",
    "zero",
    "one",
    "two",
    "three",
    "ideally",
    "four",
    "values",
    "effectively",
    "going",
    "binary",
    "flags",
    "ones",
    "zeros",
    "represented",
    "positions",
    "one",
    "hot",
    "encoded",
    "vector",
    "different",
    "actions",
    "different",
    "spaces",
    "also",
    "got",
    "similar",
    "discrete",
    "space",
    "case",
    "uh",
    "multiple",
    "sets",
    "values",
    "0",
    "1",
    "pass",
    "5",
    "2",
    "2",
    "get",
    "back",
    "range",
    "values",
    "0",
    "4",
    "first",
    "position",
    "0",
    "1",
    "second",
    "position",
    "0",
    "one",
    "third",
    "position",
    "start",
    "see",
    "spaces",
    "sort",
    "play",
    "enough",
    "let",
    "actually",
    "take",
    "look",
    "start",
    "building",
    "environment",
    "back",
    "notebook",
    "going",
    "start",
    "loading",
    "environment",
    "first",
    "going",
    "going",
    "use",
    "open",
    "ai",
    "gym",
    "instantiate",
    "environment",
    "actually",
    "test",
    "take",
    "look",
    "let",
    "first",
    "upload",
    "environment",
    "okay",
    "two",
    "lines",
    "code",
    "able",
    "go",
    "create",
    "environment",
    "gone",
    "separated",
    "two",
    "lines",
    "code",
    "make",
    "one",
    "explain",
    "first",
    "line",
    "code",
    "written",
    "environment",
    "underscore",
    "name",
    "equals",
    "cart",
    "pole",
    "dash",
    "v0",
    "case",
    "sensitive",
    "make",
    "sure",
    "get",
    "case",
    "correct",
    "got",
    "capital",
    "dash",
    "environment",
    "name",
    "mapping",
    "open",
    "open",
    "ai",
    "gym",
    "environments",
    "actually",
    "making",
    "environments",
    "written",
    "emv",
    "equals",
    "gym",
    "dot",
    "make",
    "pass",
    "environment",
    "name",
    "variable",
    "actually",
    "printed",
    "environment",
    "name",
    "variable",
    "going",
    "string",
    "right",
    "environment",
    "name",
    "string",
    "right",
    "nothing",
    "crazy",
    "actually",
    "actually",
    "test",
    "environment",
    "remember",
    "going",
    "initially",
    "take",
    "random",
    "actions",
    "environment",
    "eventually",
    "going",
    "going",
    "take",
    "agent",
    "specifically",
    "reinforcement",
    "learning",
    "agent",
    "try",
    "get",
    "take",
    "right",
    "actions",
    "particular",
    "environment",
    "maximize",
    "reward",
    "reinforcement",
    "learning",
    "want",
    "first",
    "sort",
    "get",
    "understanding",
    "environment",
    "really",
    "really",
    "important",
    "understand",
    "actually",
    "happening",
    "environment",
    "try",
    "anything",
    "might",
    "trying",
    "wrong",
    "algorithms",
    "might",
    "whole",
    "bunch",
    "random",
    "stuff",
    "understanding",
    "environment",
    "going",
    "make",
    "life",
    "much",
    "easier",
    "trust",
    "let",
    "go",
    "ahead",
    "write",
    "bit",
    "loop",
    "test",
    "environment",
    "let",
    "go",
    "alrighty",
    "written",
    "lot",
    "code",
    "going",
    "take",
    "step",
    "step",
    "going",
    "take",
    "step",
    "step",
    "whenever",
    "going",
    "stuff",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "seven",
    "eight",
    "9",
    "10",
    "11",
    "12",
    "12",
    "lines",
    "code",
    "code",
    "including",
    "beginner",
    "well",
    "completed",
    "tutorials",
    "going",
    "available",
    "github",
    "description",
    "want",
    "take",
    "walk",
    "compare",
    "code",
    "first",
    "going",
    "walk",
    "step",
    "line",
    "code",
    "actually",
    "going",
    "going",
    "trying",
    "test",
    "carpol",
    "environment",
    "five",
    "times",
    "actually",
    "gone",
    "done",
    "created",
    "variable",
    "called",
    "episodes",
    "set",
    "five",
    "means",
    "going",
    "try",
    "loop",
    "environment",
    "five",
    "times",
    "see",
    "operate",
    "within",
    "written",
    "episodes",
    "equals",
    "five",
    "looping",
    "one",
    "episodes",
    "episode",
    "range",
    "starting",
    "one",
    "going",
    "episode",
    "plus",
    "one",
    "effectively",
    "looping",
    "one",
    "episodes",
    "actually",
    "write",
    "episodes",
    "equals",
    "fives",
    "episode",
    "range",
    "one",
    "comma",
    "episodes",
    "plus",
    "one",
    "let",
    "print",
    "episode",
    "see",
    "going",
    "looping",
    "one",
    "five",
    "right",
    "two",
    "lines",
    "resetting",
    "environment",
    "running",
    "going",
    "get",
    "initial",
    "set",
    "observations",
    "remember",
    "five",
    "key",
    "components",
    "environment",
    "four",
    "key",
    "components",
    "agent",
    "action",
    "environment",
    "observations",
    "plus",
    "rewards",
    "running",
    "going",
    "get",
    "initial",
    "set",
    "observations",
    "type",
    "see",
    "observations",
    "particular",
    "environment",
    "talk",
    "values",
    "mean",
    "second",
    "actually",
    "get",
    "understanding",
    "environment",
    "understand",
    "observations",
    "get",
    "particular",
    "pole",
    "right",
    "getting",
    "four",
    "values",
    "effectively",
    "going",
    "passing",
    "observations",
    "later",
    "pass",
    "observations",
    "reinforcement",
    "learning",
    "agent",
    "determine",
    "best",
    "type",
    "action",
    "able",
    "maximize",
    "reward",
    "agents",
    "going",
    "see",
    "values",
    "going",
    "go",
    "hey",
    "got",
    "values",
    "action",
    "take",
    "able",
    "maximize",
    "reward",
    "get",
    "bar",
    "straightest",
    "possible",
    "position",
    "setting",
    "temporary",
    "variables",
    "setting",
    "whether",
    "episode",
    "done",
    "got",
    "maximum",
    "number",
    "steps",
    "within",
    "particular",
    "environment",
    "also",
    "setting",
    "running",
    "score",
    "counter",
    "across",
    "episodes",
    "got",
    "loop",
    "done",
    "going",
    "render",
    "environment",
    "render",
    "function",
    "allows",
    "us",
    "view",
    "environment",
    "view",
    "graphical",
    "representation",
    "environment",
    "key",
    "thing",
    "call",
    "running",
    "inside",
    "collab",
    "render",
    "function",
    "going",
    "work",
    "like",
    "got",
    "little",
    "bit",
    "extra",
    "work",
    "hit",
    "comments",
    "want",
    "little",
    "bit",
    "help",
    "generating",
    "random",
    "action",
    "rather",
    "taking",
    "observations",
    "actually",
    "generating",
    "action",
    "actually",
    "useful",
    "going",
    "take",
    "random",
    "one",
    "akin",
    "type",
    "let",
    "actually",
    "move",
    "would",
    "emv",
    "dot",
    "action",
    "space",
    "dot",
    "sample",
    "generating",
    "random",
    "action",
    "actually",
    "really",
    "good",
    "note",
    "well",
    "actually",
    "take",
    "sample",
    "remember",
    "talking",
    "different",
    "types",
    "action",
    "spaces",
    "case",
    "got",
    "discrete",
    "two",
    "mean",
    "get",
    "two",
    "different",
    "types",
    "action",
    "got",
    "zero",
    "one",
    "actually",
    "type",
    "dot",
    "sample",
    "see",
    "got",
    "one",
    "time",
    "got",
    "one",
    "got",
    "one",
    "got",
    "zero",
    "see",
    "action",
    "space",
    "going",
    "two",
    "different",
    "actions",
    "zero",
    "one",
    "discrete",
    "two",
    "action",
    "space",
    "looks",
    "like",
    "represents",
    "actually",
    "take",
    "look",
    "observation",
    "space",
    "well",
    "key",
    "thing",
    "call",
    "going",
    "two",
    "different",
    "spaces",
    "within",
    "environment",
    "action",
    "space",
    "actions",
    "take",
    "environment",
    "observation",
    "space",
    "observations",
    "actually",
    "look",
    "like",
    "particular",
    "environment",
    "partial",
    "view",
    "type",
    "observation",
    "space",
    "see",
    "actually",
    "got",
    "box",
    "environment",
    "got",
    "values",
    "going",
    "lower",
    "bound",
    "going",
    "upper",
    "bound",
    "got",
    "four",
    "comma",
    "means",
    "going",
    "four",
    "values",
    "zero",
    "oh",
    "zero",
    "one",
    "two",
    "one",
    "two",
    "three",
    "four",
    "gon",
    "na",
    "type",
    "float32",
    "start",
    "see",
    "environment",
    "actually",
    "built",
    "sample",
    "wanted",
    "going",
    "look",
    "almost",
    "identical",
    "get",
    "enb",
    "dot",
    "reset",
    "actually",
    "actually",
    "pass",
    "random",
    "action",
    "next",
    "line",
    "written",
    "environment",
    "using",
    "v",
    "dot",
    "step",
    "actually",
    "amv",
    "step",
    "pass",
    "values",
    "1",
    "see",
    "going",
    "get",
    "observation",
    "back",
    "keep",
    "really",
    "us",
    "passing",
    "action",
    "actually",
    "get",
    "back",
    "actually",
    "really",
    "really",
    "interesting",
    "going",
    "get",
    "back",
    "next",
    "set",
    "observations",
    "see",
    "also",
    "going",
    "get",
    "reward",
    "whether",
    "getting",
    "positive",
    "value",
    "negative",
    "value",
    "one",
    "obviously",
    "increment",
    "zero",
    "going",
    "decrement",
    "negative",
    "one",
    "going",
    "decrement",
    "true",
    "basically",
    "specifying",
    "whether",
    "episode",
    "done",
    "remember",
    "got",
    "done",
    "statement",
    "done",
    "statement",
    "episode",
    "done",
    "going",
    "stop",
    "full",
    "line",
    "code",
    "n",
    "underscore",
    "state",
    "comma",
    "reward",
    "comma",
    "done",
    "comma",
    "info",
    "unpacking",
    "values",
    "get",
    "next",
    "line",
    "code",
    "accumulating",
    "reward",
    "score",
    "plus",
    "equals",
    "reward",
    "printing",
    "results",
    "actually",
    "get",
    "taking",
    "step",
    "written",
    "print",
    "open",
    "quotes",
    "episode",
    "colon",
    "squiggly",
    "brackets",
    "score",
    "colon",
    "squiggly",
    "brackets",
    "call",
    "squiggly",
    "brackets",
    "dot",
    "format",
    "passing",
    "episode",
    "score",
    "last",
    "least",
    "closing",
    "environment",
    "use",
    "get",
    "python",
    "close",
    "need",
    "run",
    "cool",
    "well",
    "good",
    "let",
    "actually",
    "test",
    "run",
    "see",
    "bottom",
    "environment",
    "testing",
    "want",
    "close",
    "comment",
    "line",
    "actually",
    "see",
    "sort",
    "go",
    "running",
    "really",
    "really",
    "quickly",
    "sort",
    "moving",
    "bar",
    "around",
    "actually",
    "go",
    "test",
    "see",
    "run",
    "little",
    "bit",
    "slowly",
    "sort",
    "see",
    "keep",
    "running",
    "looks",
    "like",
    "screwed",
    "let",
    "actually",
    "close",
    "say",
    "amv",
    "dot",
    "close",
    "closed",
    "let",
    "run",
    "close",
    "start",
    "see",
    "sort",
    "happening",
    "right",
    "actions",
    "moving",
    "black",
    "box",
    "left",
    "right",
    "bar",
    "effectively",
    "swaying",
    "based",
    "response",
    "ideally",
    "goal",
    "hold",
    "bar",
    "straight",
    "possible",
    "long",
    "possible",
    "cool",
    "right",
    "whole",
    "bunch",
    "stuff",
    "done",
    "taken",
    "look",
    "sample",
    "environment",
    "let",
    "actually",
    "take",
    "look",
    "little",
    "bit",
    "detail",
    "remember",
    "two",
    "parts",
    "environment",
    "action",
    "space",
    "observation",
    "space",
    "type",
    "env",
    "dot",
    "action",
    "space",
    "going",
    "actions",
    "type",
    "emv",
    "dot",
    "observation",
    "space",
    "going",
    "observations",
    "probably",
    "thinking",
    "well",
    "nick",
    "values",
    "get",
    "observation",
    "spaces",
    "action",
    "spaces",
    "let",
    "actually",
    "type",
    "write",
    "dot",
    "sample",
    "dot",
    "sample",
    "end",
    "see",
    "got",
    "values",
    "let",
    "actually",
    "duplicate",
    "got",
    "amv",
    "dot",
    "action",
    "space",
    "emb",
    "dot",
    "observation",
    "space",
    "right",
    "describing",
    "type",
    "action",
    "space",
    "actually",
    "example",
    "type",
    "observation",
    "space",
    "actual",
    "example",
    "actually",
    "take",
    "look",
    "represent",
    "actually",
    "got",
    "link",
    "actually",
    "gives",
    "little",
    "bit",
    "detail",
    "terms",
    "observation",
    "actually",
    "open",
    "ai",
    "gym",
    "documentation",
    "actually",
    "zoom",
    "terms",
    "observation",
    "space",
    "remember",
    "got",
    "box",
    "4",
    "box",
    "four",
    "first",
    "position",
    "represents",
    "cuts",
    "position",
    "got",
    "minimum",
    "value",
    "minus",
    "maximum",
    "value",
    "also",
    "got",
    "cut",
    "velocity",
    "going",
    "value",
    "got",
    "pole",
    "angle",
    "going",
    "value",
    "got",
    "pole",
    "angular",
    "velocities",
    "guessing",
    "sort",
    "speed",
    "pole",
    "moving",
    "observation",
    "spaces",
    "map",
    "every",
    "environment",
    "going",
    "well",
    "documented",
    "like",
    "sort",
    "wanted",
    "give",
    "idea",
    "cut",
    "position",
    "cut",
    "velocity",
    "pole",
    "angle",
    "pole",
    "velocity",
    "cut",
    "position",
    "cut",
    "velocity",
    "pole",
    "angle",
    "pole",
    "velocity",
    "terms",
    "action",
    "spaces",
    "remember",
    "got",
    "two",
    "possible",
    "actions",
    "zero",
    "one",
    "descriptions",
    "actions",
    "action",
    "zero",
    "going",
    "push",
    "cart",
    "left",
    "action",
    "one",
    "going",
    "push",
    "cart",
    "right",
    "sort",
    "see",
    "actions",
    "observations",
    "sort",
    "play",
    "together",
    "environment",
    "nutshell",
    "see",
    "gone",
    "done",
    "things",
    "went",
    "defined",
    "environment",
    "went",
    "tested",
    "went",
    "actually",
    "took",
    "look",
    "granular",
    "detail",
    "able",
    "actually",
    "understand",
    "environment",
    "actually",
    "fits",
    "together",
    "think",
    "really",
    "really",
    "important",
    "gives",
    "idea",
    "hell",
    "trying",
    "solve",
    "keep",
    "mind",
    "whenever",
    "solving",
    "one",
    "environments",
    "typically",
    "going",
    "action",
    "space",
    "observation",
    "space",
    "good",
    "idea",
    "try",
    "understand",
    "one",
    "means",
    "note",
    "environment",
    "set",
    "actually",
    "close",
    "let",
    "go",
    "take",
    "look",
    "next",
    "brings",
    "us",
    "step",
    "three",
    "training",
    "key",
    "thing",
    "call",
    "heap",
    "different",
    "types",
    "algorithms",
    "comes",
    "reinforcement",
    "learning",
    "typically",
    "grouped",
    "model",
    "based",
    "rl",
    "model",
    "free",
    "based",
    "reinforcement",
    "learning",
    "algorithms",
    "mainly",
    "going",
    "focusing",
    "model",
    "freebase",
    "reinforcement",
    "learning",
    "algorithms",
    "lot",
    "development",
    "happening",
    "say",
    "model",
    "based",
    "reinforcement",
    "learning",
    "useful",
    "well",
    "core",
    "thing",
    "note",
    "terms",
    "rl",
    "whole",
    "idea",
    "rl",
    "uses",
    "current",
    "state",
    "values",
    "try",
    "make",
    "prediction",
    "reinforcement",
    "learning",
    "actually",
    "happening",
    "trying",
    "make",
    "prediction",
    "future",
    "state",
    "model",
    "try",
    "generate",
    "best",
    "possible",
    "action",
    "whole",
    "heap",
    "advantages",
    "going",
    "go",
    "great",
    "detail",
    "great",
    "document",
    "open",
    "ai",
    "website",
    "https",
    "colon",
    "forward",
    "slash",
    "forward",
    "slash",
    "really",
    "really",
    "good",
    "explanation",
    "model",
    "3",
    "versus",
    "model",
    "based",
    "rl",
    "guess",
    "key",
    "thing",
    "call",
    "stable",
    "baselines",
    "really",
    "deals",
    "model",
    "3",
    "based",
    "rl",
    "number",
    "libraries",
    "look",
    "model",
    "base",
    "rl",
    "well",
    "believe",
    "rl",
    "lib",
    "one",
    "going",
    "focusing",
    "model",
    "free",
    "rl",
    "specifically",
    "going",
    "taking",
    "look",
    "a2c",
    "algorithm",
    "ppo",
    "also",
    "probably",
    "use",
    "think",
    "probably",
    "use",
    "two",
    "begin",
    "also",
    "use",
    "dqn",
    "well",
    "use",
    "different",
    "types",
    "algorithms",
    "sort",
    "see",
    "look",
    "like",
    "sort",
    "gives",
    "idea",
    "sort",
    "bunch",
    "algorithms",
    "broadly",
    "grouped",
    "model",
    "3",
    "versus",
    "model",
    "based",
    "reinforcement",
    "learning",
    "core",
    "thing",
    "note",
    "choosing",
    "best",
    "possible",
    "algorithm",
    "use",
    "case",
    "talked",
    "little",
    "bit",
    "different",
    "types",
    "action",
    "observation",
    "spaces",
    "far",
    "algorithm",
    "going",
    "try",
    "use",
    "algorithm",
    "suggest",
    "use",
    "ideally",
    "one",
    "maps",
    "appropriately",
    "particular",
    "type",
    "action",
    "space",
    "see",
    "terms",
    "stable",
    "bass",
    "lines",
    "whole",
    "bunch",
    "different",
    "types",
    "algorithms",
    "got",
    "a2c",
    "ddpg",
    "dqn",
    "ppo",
    "sac",
    "td3",
    "show",
    "actually",
    "use",
    "different",
    "algorithms",
    "little",
    "bit",
    "later",
    "main",
    "tutorial",
    "key",
    "thing",
    "note",
    "certain",
    "algorithms",
    "work",
    "certain",
    "types",
    "action",
    "spaces",
    "see",
    "a2c",
    "works",
    "boxes",
    "discrete",
    "ddpg",
    "works",
    "box",
    "spaces",
    "dqn",
    "works",
    "discrete",
    "spaces",
    "reference",
    "action",
    "space",
    "key",
    "thing",
    "call",
    "based",
    "action",
    "space",
    "much",
    "observation",
    "space",
    "remember",
    "go",
    "back",
    "main",
    "tutorial",
    "type",
    "emb",
    "dot",
    "action",
    "space",
    "say",
    "discrete",
    "know",
    "let",
    "scroll",
    "back",
    "know",
    "use",
    "one",
    "models",
    "green",
    "tick",
    "discrete",
    "could",
    "use",
    "a2c",
    "solve",
    "dqn",
    "ppo",
    "box",
    "environment",
    "remember",
    "take",
    "look",
    "observation",
    "space",
    "assume",
    "action",
    "space",
    "shape",
    "box",
    "well",
    "looking",
    "using",
    "one",
    "ones",
    "a2c",
    "ddpg",
    "ppo",
    "sac",
    "td3",
    "got",
    "little",
    "bit",
    "guide",
    "discrete",
    "single",
    "process",
    "use",
    "dqn",
    "discrete",
    "use",
    "ppo",
    "a2c",
    "using",
    "continuous",
    "single",
    "process",
    "sac",
    "td3",
    "continuous",
    "ppo",
    "a2c",
    "key",
    "thing",
    "call",
    "guys",
    "treat",
    "algorithms",
    "commodities",
    "choose",
    "use",
    "whichever",
    "one",
    "want",
    "particular",
    "use",
    "case",
    "going",
    "perform",
    "better",
    "others",
    "good",
    "know",
    "work",
    "better",
    "know",
    "detail",
    "put",
    "together",
    "really",
    "need",
    "know",
    "level",
    "detail",
    "able",
    "try",
    "try",
    "hand",
    "important",
    "know",
    "algorithm",
    "use",
    "type",
    "action",
    "space",
    "available",
    "inside",
    "stable",
    "baselines",
    "documentation",
    "see",
    "got",
    "remember",
    "get",
    "go",
    "stable",
    "dash",
    "baselines3",
    "dot",
    "read",
    "docs",
    "dot",
    "io",
    "forward",
    "en",
    "forward",
    "slash",
    "master",
    "forward",
    "slash",
    "models",
    "want",
    "look",
    "ppo",
    "algorithm",
    "modules",
    "forward",
    "ppo",
    "dot",
    "html",
    "links",
    "going",
    "description",
    "grab",
    "pick",
    "cool",
    "talked",
    "little",
    "bit",
    "different",
    "types",
    "algorithms",
    "use",
    "one",
    "another",
    "thing",
    "note",
    "need",
    "able",
    "understand",
    "training",
    "metric",
    "type",
    "algorithm",
    "use",
    "going",
    "determine",
    "type",
    "metrics",
    "get",
    "training",
    "broadly",
    "get",
    "something",
    "looks",
    "little",
    "bit",
    "like",
    "see",
    "kick",
    "training",
    "break",
    "evaluation",
    "metrics",
    "time",
    "metrics",
    "loss",
    "metrics",
    "got",
    "metrics",
    "evaluation",
    "metrics",
    "episode",
    "length",
    "episode",
    "reward",
    "mean",
    "well",
    "averages",
    "length",
    "long",
    "episode",
    "actually",
    "went",
    "playing",
    "game",
    "think",
    "one",
    "single",
    "game",
    "trying",
    "balance",
    "cart",
    "poll",
    "one",
    "episode",
    "number",
    "steps",
    "maximum",
    "number",
    "steps",
    "allowed",
    "take",
    "time",
    "metrics",
    "case",
    "got",
    "frames",
    "per",
    "second",
    "fast",
    "processing",
    "iterations",
    "means",
    "many",
    "times",
    "actually",
    "gone",
    "time",
    "elapsed",
    "long",
    "running",
    "total",
    "time",
    "steps",
    "many",
    "steps",
    "actually",
    "taken",
    "episode",
    "also",
    "got",
    "loss",
    "metrics",
    "got",
    "entropy",
    "loss",
    "policy",
    "loss",
    "value",
    "loss",
    "want",
    "greater",
    "detail",
    "want",
    "greater",
    "explanation",
    "hit",
    "comments",
    "also",
    "got",
    "metrics",
    "well",
    "got",
    "explained",
    "variance",
    "much",
    "variance",
    "environment",
    "agent",
    "able",
    "explain",
    "also",
    "got",
    "learning",
    "rate",
    "fast",
    "policy",
    "actually",
    "updating",
    "also",
    "got",
    "n",
    "updates",
    "many",
    "updates",
    "actually",
    "made",
    "agent",
    "core",
    "thing",
    "call",
    "default",
    "actually",
    "go",
    "install",
    "stable",
    "baselines",
    "using",
    "pip",
    "install",
    "command",
    "going",
    "installing",
    "without",
    "gpu",
    "acceleration",
    "wanted",
    "use",
    "gpu",
    "acceleration",
    "could",
    "need",
    "go",
    "install",
    "appropriate",
    "pi",
    "torch",
    "version",
    "say",
    "example",
    "wanted",
    "leverage",
    "gpu",
    "acceleration",
    "particular",
    "machine",
    "show",
    "sec",
    "would",
    "need",
    "go",
    "go",
    "baseline",
    "install",
    "page",
    "hit",
    "install",
    "scroll",
    "see",
    "sort",
    "gives",
    "steps",
    "go",
    "install",
    "could",
    "choose",
    "stable",
    "install",
    "working",
    "windows",
    "machine",
    "could",
    "mac",
    "could",
    "hit",
    "math",
    "linux",
    "could",
    "hit",
    "linux",
    "going",
    "choose",
    "windows",
    "case",
    "say",
    "example",
    "wanted",
    "install",
    "using",
    "pip",
    "could",
    "hit",
    "pip",
    "could",
    "choose",
    "language",
    "wanted",
    "install",
    "wanted",
    "java",
    "could",
    "choose",
    "wanted",
    "python",
    "case",
    "working",
    "python",
    "choose",
    "python",
    "need",
    "choose",
    "compute",
    "platform",
    "really",
    "really",
    "important",
    "cool",
    "thing",
    "call",
    "cuda",
    "cu",
    "dnn",
    "supported",
    "nvidia",
    "gpu",
    "want",
    "leverage",
    "gpu",
    "acceleration",
    "nvidia",
    "gpu",
    "use",
    "cuda",
    "also",
    "got",
    "rock",
    "rock",
    "beta",
    "package",
    "available",
    "amd",
    "gpus",
    "believe",
    "available",
    "linux",
    "moment",
    "wanted",
    "use",
    "amd",
    "gpu",
    "able",
    "need",
    "able",
    "need",
    "linux",
    "os",
    "case",
    "windows",
    "see",
    "available",
    "windows",
    "using",
    "cuda",
    "case",
    "cuda",
    "cuda",
    "really",
    "needed",
    "want",
    "use",
    "gpu",
    "acceleration",
    "honest",
    "reinforcement",
    "learning",
    "going",
    "see",
    "much",
    "performance",
    "boost",
    "training",
    "would",
    "traditional",
    "deep",
    "learning",
    "using",
    "gpu",
    "gpu",
    "stress",
    "worry",
    "need",
    "step",
    "wanted",
    "call",
    "people",
    "gpu",
    "wanted",
    "case",
    "probably",
    "one",
    "projects",
    "take",
    "look",
    "install",
    "sort",
    "skip",
    "good",
    "thing",
    "note",
    "alrighty",
    "note",
    "though",
    "let",
    "go",
    "ahead",
    "let",
    "go",
    "train",
    "agent",
    "going",
    "skip",
    "back",
    "notebook",
    "going",
    "start",
    "training",
    "reinforcement",
    "learning",
    "model",
    "first",
    "going",
    "going",
    "define",
    "log",
    "path",
    "going",
    "save",
    "tensorboard",
    "log",
    "wanted",
    "go",
    "monitor",
    "training",
    "able",
    "take",
    "look",
    "inside",
    "log",
    "directory",
    "view",
    "model",
    "actually",
    "performing",
    "show",
    "let",
    "go",
    "ahead",
    "first",
    "find",
    "log",
    "path",
    "going",
    "type",
    "log",
    "path",
    "log",
    "underscore",
    "path",
    "specify",
    "key",
    "thing",
    "call",
    "path",
    "needs",
    "exist",
    "also",
    "create",
    "part",
    "code",
    "gone",
    "done",
    "manually",
    "reasonably",
    "straightforward",
    "going",
    "inside",
    "folder",
    "working",
    "going",
    "create",
    "folder",
    "called",
    "training",
    "inside",
    "going",
    "create",
    "two",
    "additional",
    "folders",
    "one",
    "called",
    "logs",
    "one",
    "chord",
    "saved",
    "models",
    "let",
    "zoom",
    "see",
    "got",
    "training",
    "folder",
    "got",
    "one",
    "called",
    "logs",
    "one",
    "called",
    "saved",
    "models",
    "inside",
    "logs",
    "folder",
    "going",
    "save",
    "logs",
    "case",
    "see",
    "got",
    "bunch",
    "let",
    "delete",
    "need",
    "inside",
    "saved",
    "models",
    "got",
    "bunch",
    "models",
    "well",
    "going",
    "delete",
    "ones",
    "need",
    "logs",
    "going",
    "save",
    "logs",
    "model",
    "saved",
    "models",
    "going",
    "save",
    "saved",
    "model",
    "trained",
    "model",
    "take",
    "look",
    "little",
    "bit",
    "later",
    "actually",
    "go",
    "okay",
    "done",
    "going",
    "add",
    "comment",
    "make",
    "directories",
    "first",
    "right",
    "going",
    "define",
    "log",
    "path",
    "going",
    "give",
    "us",
    "path",
    "logs",
    "training",
    "backwards",
    "backwards",
    "logs",
    "windows",
    "machine",
    "path",
    "represented",
    "double",
    "backward",
    "slash",
    "mac",
    "linux",
    "machine",
    "believe",
    "forward",
    "slash",
    "cool",
    "log",
    "path",
    "defined",
    "next",
    "thing",
    "need",
    "instantiate",
    "algorithm",
    "specifically",
    "agent",
    "remember",
    "went",
    "imported",
    "dependencies",
    "went",
    "imported",
    "ppo",
    "case",
    "ppo",
    "going",
    "algorithm",
    "going",
    "using",
    "particular",
    "environment",
    "let",
    "go",
    "ahead",
    "define",
    "take",
    "look",
    "okay",
    "algorithm",
    "set",
    "see",
    "printed",
    "using",
    "cuda",
    "device",
    "currently",
    "gpu",
    "acceleration",
    "set",
    "particular",
    "environment",
    "using",
    "pytorch",
    "cuda",
    "version",
    "pytorch",
    "gpu",
    "accelerated",
    "version",
    "would",
    "see",
    "using",
    "cpu",
    "device",
    "need",
    "stress",
    "using",
    "gpu",
    "acceleration",
    "show",
    "set",
    "later",
    "says",
    "using",
    "cpu",
    "device",
    "perfectly",
    "fine",
    "well",
    "still",
    "good",
    "go",
    "right",
    "order",
    "written",
    "three",
    "lines",
    "code",
    "written",
    "gone",
    "recreated",
    "environment",
    "keep",
    "encapsulated",
    "written",
    "emv",
    "equals",
    "jim",
    "dot",
    "make",
    "pass",
    "environment",
    "name",
    "line",
    "different",
    "line",
    "exact",
    "thing",
    "done",
    "wrapped",
    "environment",
    "inside",
    "dummy",
    "vec",
    "env",
    "wrapper",
    "remember",
    "imported",
    "actually",
    "wrapping",
    "written",
    "env",
    "equals",
    "dummy",
    "vec",
    "env",
    "created",
    "lambda",
    "function",
    "going",
    "environment",
    "creation",
    "function",
    "inside",
    "square",
    "brackets",
    "written",
    "lambda",
    "colon",
    "env",
    "going",
    "allow",
    "us",
    "work",
    "environment",
    "wrapped",
    "inside",
    "dummy",
    "vectorized",
    "environment",
    "think",
    "wrapper",
    "environment",
    "show",
    "real",
    "vectorized",
    "environment",
    "get",
    "project",
    "one",
    "actually",
    "gone",
    "defined",
    "model",
    "think",
    "defining",
    "agent",
    "written",
    "model",
    "equals",
    "ppo",
    "algorithm",
    "gone",
    "imported",
    "passed",
    "uh",
    "two",
    "arguments",
    "two",
    "keyword",
    "arguments",
    "first",
    "one",
    "defining",
    "policy",
    "going",
    "use",
    "case",
    "mlp",
    "policy",
    "stands",
    "perceptron",
    "policy",
    "case",
    "means",
    "going",
    "using",
    "neural",
    "network",
    "using",
    "standard",
    "sort",
    "neural",
    "network",
    "units",
    "using",
    "lstm",
    "layers",
    "using",
    "cnn",
    "layers",
    "inside",
    "project",
    "1",
    "project",
    "2",
    "actually",
    "use",
    "cnn",
    "policy",
    "core",
    "thing",
    "call",
    "well",
    "stable",
    "baselines",
    "two",
    "actually",
    "one",
    "advantage",
    "stable",
    "baselines",
    "three",
    "actually",
    "mlp",
    "lstm",
    "policies",
    "wanted",
    "use",
    "windowed",
    "data",
    "sets",
    "particularly",
    "useful",
    "trading",
    "finance",
    "well",
    "certain",
    "gaming",
    "applications",
    "particular",
    "policy",
    "unfortunately",
    "available",
    "stable",
    "baselines",
    "3",
    "far",
    "know",
    "changes",
    "mention",
    "pinned",
    "comment",
    "supports",
    "mlp",
    "policy",
    "believe",
    "cnn",
    "policy",
    "see",
    "cnn",
    "policy",
    "inside",
    "project",
    "one",
    "next",
    "argument",
    "passed",
    "environment",
    "going",
    "vec",
    "dummy",
    "vectorized",
    "environment",
    "specified",
    "verbose",
    "equals",
    "one",
    "want",
    "well",
    "want",
    "log",
    "results",
    "particular",
    "model",
    "specified",
    "tensorboard",
    "log",
    "path",
    "tensorboard",
    "underscore",
    "log",
    "specified",
    "log",
    "path",
    "actually",
    "go",
    "take",
    "look",
    "algorithm",
    "ppo",
    "whole",
    "heap",
    "arguments",
    "actually",
    "pass",
    "see",
    "pass",
    "policy",
    "environment",
    "learning",
    "rate",
    "number",
    "steps",
    "batch",
    "size",
    "number",
    "epochs",
    "gamma",
    "gae",
    "lambdas",
    "whole",
    "bunch",
    "different",
    "types",
    "piper",
    "parameters",
    "actually",
    "train",
    "well",
    "whole",
    "bunch",
    "different",
    "things",
    "actually",
    "train",
    "whole",
    "bunch",
    "documentation",
    "particular",
    "environment",
    "see",
    "keeping",
    "pretty",
    "simple",
    "using",
    "standard",
    "hyper",
    "parameters",
    "particular",
    "case",
    "agent",
    "set",
    "next",
    "thing",
    "need",
    "go",
    "ahead",
    "train",
    "pretty",
    "simple",
    "need",
    "use",
    "able",
    "go",
    "train",
    "let",
    "type",
    "need",
    "pass",
    "number",
    "time",
    "steps",
    "want",
    "train",
    "going",
    "pass",
    "initially",
    "going",
    "set",
    "20",
    "full",
    "line",
    "model",
    "dot",
    "learn",
    "passing",
    "keyword",
    "argument",
    "total",
    "underscore",
    "time",
    "steps",
    "equals",
    "20",
    "play",
    "around",
    "number",
    "terms",
    "long",
    "want",
    "train",
    "simple",
    "environment",
    "probably",
    "going",
    "able",
    "get",
    "away",
    "lower",
    "number",
    "total",
    "time",
    "steps",
    "sophisticated",
    "environment",
    "say",
    "example",
    "breakout",
    "environment",
    "probably",
    "gon",
    "na",
    "need",
    "heap",
    "example",
    "cardpole",
    "managed",
    "solve",
    "often",
    "20",
    "000",
    "steps",
    "breakout",
    "tutorial",
    "well",
    "breakout",
    "actually",
    "end",
    "goal",
    "per",
    "se",
    "actually",
    "took",
    "around",
    "300",
    "400",
    "000",
    "time",
    "steps",
    "tutorial",
    "complexity",
    "environment",
    "going",
    "define",
    "many",
    "time",
    "steps",
    "need",
    "train",
    "case",
    "pretty",
    "happy",
    "20",
    "000",
    "go",
    "ahead",
    "kick",
    "see",
    "eventually",
    "model",
    "starts",
    "training",
    "looks",
    "like",
    "got",
    "bit",
    "error",
    "okay",
    "looks",
    "like",
    "might",
    "warning",
    "okay",
    "see",
    "model",
    "starting",
    "train",
    "getting",
    "time",
    "metrics",
    "also",
    "getting",
    "whole",
    "bunch",
    "additional",
    "training",
    "metrics",
    "let",
    "let",
    "go",
    "ahead",
    "run",
    "soon",
    "done",
    "able",
    "test",
    "okay",
    "see",
    "model",
    "finished",
    "training",
    "take",
    "look",
    "looks",
    "like",
    "got",
    "explained",
    "variance",
    "got",
    "entropy",
    "loss",
    "minus",
    "learning",
    "rate",
    "loss",
    "looks",
    "like",
    "stable",
    "end",
    "fine",
    "let",
    "test",
    "see",
    "actually",
    "looks",
    "like",
    "model",
    "trained",
    "least",
    "trained",
    "20",
    "000",
    "steps",
    "wanted",
    "could",
    "go",
    "train",
    "longer",
    "need",
    "go",
    "run",
    "going",
    "start",
    "training",
    "see",
    "kicking",
    "training",
    "going",
    "wanted",
    "train",
    "longer",
    "need",
    "go",
    "run",
    "kicked",
    "let",
    "let",
    "finish",
    "actually",
    "test",
    "okay",
    "next",
    "round",
    "training",
    "done",
    "looks",
    "like",
    "explained",
    "variance",
    "little",
    "bit",
    "higher",
    "learning",
    "rate",
    "still",
    "gone",
    "total",
    "20",
    "480",
    "time",
    "steps",
    "latest",
    "run",
    "often",
    "going",
    "want",
    "going",
    "want",
    "save",
    "model",
    "move",
    "around",
    "wanted",
    "go",
    "deploy",
    "want",
    "able",
    "save",
    "let",
    "take",
    "look",
    "might",
    "save",
    "reload",
    "model",
    "first",
    "go",
    "evaluate",
    "going",
    "define",
    "path",
    "going",
    "call",
    "ppo",
    "path",
    "similar",
    "log",
    "path",
    "cool",
    "path",
    "defined",
    "written",
    "ppo",
    "underscore",
    "path",
    "going",
    "path",
    "variable",
    "equals",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "going",
    "saving",
    "inside",
    "effectively",
    "saved",
    "models",
    "folder",
    "training",
    "saved",
    "models",
    "file",
    "name",
    "actually",
    "going",
    "ppo",
    "underscore",
    "model",
    "underscore",
    "cart",
    "poll",
    "going",
    "save",
    "model",
    "inside",
    "folder",
    "reinforcement",
    "learning",
    "course",
    "well",
    "current",
    "folder",
    "training",
    "saved",
    "models",
    "going",
    "saved",
    "go",
    "save",
    "see",
    "model",
    "saved",
    "ppo",
    "underscore",
    "model",
    "underscore",
    "carpol",
    "save",
    "written",
    "model",
    "dot",
    "save",
    "passed",
    "ppo",
    "path",
    "wanted",
    "could",
    "actually",
    "go",
    "delete",
    "model",
    "reload",
    "let",
    "go",
    "ahead",
    "sort",
    "simulates",
    "deployment",
    "right",
    "going",
    "reloading",
    "saved",
    "model",
    "time",
    "let",
    "write",
    "del",
    "model",
    "delete",
    "model",
    "reload",
    "model",
    "back",
    "memory",
    "type",
    "model",
    "going",
    "define",
    "variable",
    "called",
    "model",
    "actually",
    "reload",
    "writing",
    "ppo",
    "dot",
    "load",
    "pass",
    "path",
    "path",
    "actual",
    "model",
    "save",
    "somewhere",
    "else",
    "going",
    "make",
    "sure",
    "make",
    "sure",
    "pass",
    "full",
    "path",
    "models",
    "going",
    "pass",
    "environment",
    "well",
    "full",
    "line",
    "model",
    "equals",
    "ppo",
    "dot",
    "load",
    "pass",
    "path",
    "model",
    "actually",
    "saved",
    "remember",
    "ppo",
    "underscore",
    "path",
    "going",
    "model",
    "case",
    "training",
    "saved",
    "models",
    "ppo",
    "model",
    "carpol",
    "exactly",
    "training",
    "saved",
    "models",
    "ppo",
    "underscore",
    "model",
    "underscore",
    "carpal",
    "file",
    "working",
    "let",
    "load",
    "right",
    "run",
    "cell",
    "see",
    "type",
    "model",
    "dot",
    "learn",
    "example",
    "total",
    "time",
    "steps",
    "equals",
    "thousand",
    "would",
    "training",
    "step",
    "see",
    "got",
    "name",
    "model",
    "defined",
    "remember",
    "deleted",
    "model",
    "actually",
    "went",
    "loaded",
    "see",
    "loaded",
    "model",
    "go",
    "run",
    "see",
    "training",
    "right",
    "sort",
    "get",
    "idea",
    "go",
    "train",
    "model",
    "save",
    "using",
    "reload",
    "using",
    "remember",
    "actually",
    "use",
    "able",
    "load",
    "back",
    "cool",
    "training",
    "done",
    "nutshell",
    "done",
    "quite",
    "fair",
    "bit",
    "ridden",
    "actually",
    "created",
    "algorithm",
    "agent",
    "ppo",
    "pass",
    "parameters",
    "used",
    "trade",
    "model",
    "use",
    "save",
    "model",
    "ppo",
    "whatever",
    "algorithm",
    "using",
    "dot",
    "load",
    "able",
    "go",
    "reload",
    "memory",
    "four",
    "key",
    "components",
    "really",
    "really",
    "important",
    "use",
    "algorithm",
    "find",
    "hyper",
    "parameters",
    "train",
    "save",
    "whatever",
    "algorithm",
    "dot",
    "load",
    "reload",
    "step",
    "four",
    "testing",
    "evaluation",
    "far",
    "gone",
    "done",
    "set",
    "environment",
    "gone",
    "trained",
    "actually",
    "done",
    "anything",
    "trained",
    "model",
    "yet",
    "well",
    "going",
    "want",
    "actually",
    "going",
    "want",
    "train",
    "model",
    "see",
    "actually",
    "performing",
    "would",
    "noticed",
    "actually",
    "went",
    "trained",
    "model",
    "using",
    "ppo",
    "algorithm",
    "let",
    "actually",
    "go",
    "back",
    "take",
    "look",
    "actually",
    "get",
    "training",
    "metrics",
    "training",
    "metrics",
    "sort",
    "showing",
    "rollout",
    "metrics",
    "see",
    "much",
    "dependent",
    "algorithm",
    "using",
    "a2c",
    "algorithm",
    "believe",
    "get",
    "rollout",
    "metrics",
    "ppo",
    "going",
    "want",
    "going",
    "want",
    "evaluate",
    "model",
    "see",
    "performance",
    "actually",
    "like",
    "actually",
    "use",
    "evaluate",
    "policy",
    "method",
    "imported",
    "right",
    "start",
    "able",
    "see",
    "actually",
    "looks",
    "like",
    "key",
    "thing",
    "call",
    "get",
    "metrics",
    "great",
    "thing",
    "two",
    "key",
    "ones",
    "need",
    "pay",
    "attention",
    "episode",
    "mean",
    "length",
    "episode",
    "ep",
    "underscore",
    "len",
    "underscore",
    "mean",
    "long",
    "episode",
    "actually",
    "lasted",
    "average",
    "say",
    "example",
    "playing",
    "breakout",
    "many",
    "times",
    "model",
    "able",
    "play",
    "hit",
    "ball",
    "many",
    "frames",
    "able",
    "go",
    "model",
    "eventually",
    "died",
    "gaming",
    "particularly",
    "important",
    "reward",
    "mean",
    "effectively",
    "average",
    "reward",
    "remember",
    "think",
    "back",
    "dog",
    "environment",
    "many",
    "times",
    "average",
    "many",
    "times",
    "dog",
    "got",
    "trade",
    "average",
    "reward",
    "particular",
    "case",
    "actually",
    "get",
    "metrics",
    "similar",
    "using",
    "evaluate",
    "policy",
    "method",
    "also",
    "monitor",
    "training",
    "metrics",
    "inside",
    "tensorboard",
    "remember",
    "actually",
    "set",
    "model",
    "actually",
    "pass",
    "tensorboard",
    "underscore",
    "log",
    "specified",
    "log",
    "path",
    "go",
    "back",
    "see",
    "defined",
    "ppo",
    "actually",
    "specified",
    "tensorboard",
    "log",
    "path",
    "go",
    "actually",
    "take",
    "look",
    "metrics",
    "going",
    "training",
    "metrics",
    "cool",
    "let",
    "go",
    "ahead",
    "start",
    "tensorboard",
    "going",
    "show",
    "second",
    "let",
    "evaluation",
    "let",
    "go",
    "ahead",
    "going",
    "using",
    "evaluate",
    "underscore",
    "policy",
    "method",
    "remember",
    "going",
    "method",
    "allows",
    "us",
    "test",
    "well",
    "model",
    "actually",
    "performing",
    "ppo",
    "model",
    "particular",
    "case",
    "considered",
    "solved",
    "get",
    "average",
    "score",
    "200",
    "higher",
    "ideally",
    "want",
    "see",
    "model",
    "scoring",
    "200",
    "average",
    "determine",
    "whether",
    "environment",
    "actually",
    "solved",
    "certain",
    "environments",
    "going",
    "sort",
    "cap",
    "considered",
    "solved",
    "others",
    "going",
    "continuous",
    "whatever",
    "highest",
    "score",
    "get",
    "best",
    "breakout",
    "tutorial",
    "believe",
    "caps",
    "case",
    "carpol",
    "environment",
    "let",
    "go",
    "ahead",
    "test",
    "okay",
    "gone",
    "written",
    "line",
    "go",
    "test",
    "policy",
    "evaluate",
    "line",
    "written",
    "evaluate",
    "underscore",
    "policy",
    "gone",
    "passed",
    "two",
    "arguments",
    "two",
    "keyword",
    "arguments",
    "gone",
    "passed",
    "model",
    "environment",
    "many",
    "episodes",
    "want",
    "test",
    "case",
    "passed",
    "n",
    "underscore",
    "eval",
    "underscore",
    "episodes",
    "equals",
    "10",
    "gone",
    "specified",
    "render",
    "equals",
    "true",
    "passing",
    "render",
    "equals",
    "true",
    "determines",
    "whether",
    "actually",
    "visualize",
    "real",
    "time",
    "evaluating",
    "policy",
    "colab",
    "want",
    "specify",
    "render",
    "equals",
    "false",
    "want",
    "visualize",
    "going",
    "work",
    "least",
    "default",
    "evaluation",
    "let",
    "go",
    "ahead",
    "test",
    "see",
    "model",
    "actually",
    "looks",
    "like",
    "see",
    "way",
    "stable",
    "time",
    "remember",
    "testing",
    "pa",
    "start",
    "sort",
    "falling",
    "going",
    "sort",
    "place",
    "perfectly",
    "stable",
    "right",
    "see",
    "balancing",
    "almost",
    "exactly",
    "going",
    "10",
    "times",
    "going",
    "go",
    "10",
    "different",
    "episodes",
    "take",
    "average",
    "reward",
    "actually",
    "see",
    "second",
    "pretty",
    "cool",
    "right",
    "couple",
    "lines",
    "code",
    "able",
    "build",
    "reinforcement",
    "learning",
    "agent",
    "training",
    "speed",
    "going",
    "pretty",
    "much",
    "similar",
    "training",
    "gpu",
    "gpu",
    "going",
    "similar",
    "fret",
    "gp",
    "machine",
    "test",
    "regardless",
    "cool",
    "done",
    "see",
    "average",
    "reward",
    "200",
    "environment",
    "considered",
    "solved",
    "good",
    "two",
    "values",
    "get",
    "evaluate",
    "policy",
    "average",
    "reward",
    "number",
    "episodes",
    "standard",
    "deviation",
    "reward",
    "case",
    "getting",
    "average",
    "score",
    "200",
    "standard",
    "deviation",
    "zero",
    "perfect",
    "absolutely",
    "bang",
    "particular",
    "case",
    "next",
    "thing",
    "want",
    "actually",
    "close",
    "environment",
    "wanted",
    "close",
    "type",
    "going",
    "close",
    "right",
    "gone",
    "evaluated",
    "actually",
    "wanted",
    "go",
    "deploy",
    "would",
    "actually",
    "go",
    "sort",
    "testing",
    "model",
    "encapsulated",
    "function",
    "would",
    "happen",
    "actually",
    "wanted",
    "go",
    "rather",
    "like",
    "actually",
    "wanted",
    "sort",
    "similar",
    "well",
    "actually",
    "core",
    "difference",
    "rather",
    "using",
    "actually",
    "going",
    "pass",
    "environment",
    "observations",
    "agent",
    "try",
    "predict",
    "best",
    "type",
    "action",
    "reinforcement",
    "learning",
    "remember",
    "going",
    "take",
    "observations",
    "pass",
    "agent",
    "agent",
    "going",
    "try",
    "determine",
    "best",
    "type",
    "action",
    "take",
    "environment",
    "maximize",
    "reward",
    "flow",
    "going",
    "much",
    "similar",
    "take",
    "look",
    "environment",
    "use",
    "reset",
    "environment",
    "get",
    "observations",
    "going",
    "use",
    "observations",
    "try",
    "get",
    "best",
    "possible",
    "type",
    "action",
    "actually",
    "going",
    "take",
    "step",
    "actually",
    "copy",
    "entire",
    "block",
    "code",
    "right",
    "let",
    "go",
    "ahead",
    "test",
    "model",
    "going",
    "make",
    "key",
    "changes",
    "rather",
    "using",
    "going",
    "change",
    "model",
    "dot",
    "predict",
    "function",
    "need",
    "pass",
    "observations",
    "right",
    "got",
    "observations",
    "named",
    "two",
    "different",
    "things",
    "want",
    "change",
    "going",
    "change",
    "variable",
    "equal",
    "obs",
    "rather",
    "n",
    "underscore",
    "state",
    "going",
    "change",
    "obs",
    "well",
    "ideally",
    "going",
    "change",
    "line",
    "state",
    "equals",
    "e",
    "b",
    "dot",
    "reset",
    "going",
    "change",
    "action",
    "line",
    "rather",
    "going",
    "change",
    "going",
    "pass",
    "observations",
    "line",
    "action",
    "taken",
    "going",
    "change",
    "first",
    "parameter",
    "obs",
    "well",
    "run",
    "rather",
    "taking",
    "random",
    "steps",
    "actually",
    "going",
    "using",
    "model",
    "take",
    "steps",
    "remember",
    "subbed",
    "model",
    "using",
    "model",
    "go",
    "run",
    "looks",
    "like",
    "got",
    "bit",
    "error",
    "let",
    "take",
    "look",
    "might",
    "need",
    "oh",
    "actually",
    "going",
    "get",
    "two",
    "parts",
    "model",
    "actually",
    "uh",
    "let",
    "actually",
    "take",
    "look",
    "use",
    "obs",
    "actually",
    "get",
    "back",
    "two",
    "values",
    "get",
    "array",
    "get",
    "none",
    "value",
    "action",
    "actually",
    "first",
    "bit",
    "second",
    "component",
    "states",
    "actually",
    "need",
    "second",
    "component",
    "make",
    "underscore",
    "actually",
    "take",
    "look",
    "action",
    "looking",
    "better",
    "right",
    "cool",
    "going",
    "make",
    "one",
    "change",
    "going",
    "unpack",
    "value",
    "environment",
    "still",
    "open",
    "let",
    "go",
    "ahead",
    "close",
    "closed",
    "right",
    "let",
    "try",
    "go",
    "see",
    "performing",
    "way",
    "better",
    "balancing",
    "pole",
    "way",
    "better",
    "initially",
    "taking",
    "random",
    "steps",
    "see",
    "score",
    "printed",
    "getting",
    "200",
    "pretty",
    "much",
    "every",
    "single",
    "time",
    "means",
    "smack",
    "bang",
    "solving",
    "model",
    "go",
    "see",
    "gone",
    "done",
    "close",
    "wanted",
    "run",
    "continuously",
    "could",
    "well",
    "case",
    "nice",
    "sort",
    "loop",
    "go",
    "let",
    "try",
    "running",
    "pretty",
    "cool",
    "right",
    "actively",
    "using",
    "reinforcement",
    "learning",
    "agent",
    "able",
    "go",
    "interact",
    "open",
    "ai",
    "gym",
    "environment",
    "balancing",
    "poll",
    "whole",
    "heap",
    "better",
    "let",
    "actually",
    "take",
    "look",
    "went",
    "let",
    "actually",
    "delete",
    "cast",
    "mind",
    "right",
    "back",
    "section",
    "two",
    "loading",
    "environment",
    "playing",
    "around",
    "actually",
    "play",
    "actually",
    "one",
    "really",
    "important",
    "function",
    "emv",
    "dot",
    "reset",
    "remember",
    "type",
    "env",
    "dot",
    "reset",
    "going",
    "get",
    "observations",
    "observation",
    "space",
    "actually",
    "take",
    "observations",
    "actually",
    "taking",
    "observations",
    "passing",
    "model",
    "obs",
    "actually",
    "getting",
    "back",
    "two",
    "values",
    "let",
    "actually",
    "take",
    "look",
    "getting",
    "back",
    "going",
    "get",
    "back",
    "model",
    "action",
    "next",
    "state",
    "used",
    "recurrent",
    "policy",
    "using",
    "current",
    "policy",
    "getting",
    "next",
    "state",
    "actually",
    "get",
    "back",
    "particular",
    "case",
    "relevant",
    "us",
    "first",
    "value",
    "array",
    "remember",
    "terms",
    "action",
    "space",
    "space",
    "remember",
    "two",
    "different",
    "types",
    "action",
    "zero",
    "let",
    "see",
    "get",
    "one",
    "zero",
    "one",
    "basically",
    "getting",
    "rather",
    "getting",
    "random",
    "action",
    "using",
    "model",
    "dot",
    "predict",
    "function",
    "observations",
    "environment",
    "generate",
    "action",
    "see",
    "rather",
    "getting",
    "model",
    "actually",
    "predicting",
    "based",
    "observations",
    "current",
    "environment",
    "right",
    "take",
    "action",
    "one",
    "order",
    "get",
    "best",
    "possible",
    "reward",
    "effectively",
    "reinforcement",
    "learning",
    "cast",
    "mind",
    "back",
    "diagram",
    "got",
    "agent",
    "got",
    "environment",
    "got",
    "action",
    "got",
    "reward",
    "let",
    "actually",
    "go",
    "back",
    "slide",
    "right",
    "got",
    "agent",
    "case",
    "agent",
    "model",
    "got",
    "action",
    "oh",
    "actually",
    "got",
    "agent",
    "got",
    "environment",
    "remember",
    "environment",
    "emv",
    "variable",
    "got",
    "action",
    "particular",
    "case",
    "generating",
    "one",
    "also",
    "got",
    "observations",
    "value",
    "remember",
    "observation",
    "print",
    "four",
    "values",
    "cast",
    "mind",
    "back",
    "actually",
    "took",
    "look",
    "one",
    "observations",
    "meant",
    "observations",
    "car",
    "position",
    "cart",
    "velocity",
    "pole",
    "angle",
    "pole",
    "angular",
    "velocity",
    "start",
    "see",
    "sort",
    "fitting",
    "together",
    "got",
    "four",
    "key",
    "components",
    "got",
    "got",
    "agent",
    "got",
    "environment",
    "got",
    "action",
    "also",
    "got",
    "observations",
    "core",
    "thing",
    "called",
    "yet",
    "reward",
    "right",
    "saw",
    "got",
    "actually",
    "determine",
    "reward",
    "well",
    "get",
    "reward",
    "run",
    "env",
    "dot",
    "step",
    "actually",
    "b",
    "dot",
    "step",
    "remember",
    "model",
    "predicted",
    "take",
    "action",
    "go",
    "take",
    "extract",
    "pass",
    "action",
    "actually",
    "getting",
    "back",
    "values",
    "relevant",
    "us",
    "going",
    "get",
    "state",
    "state",
    "taking",
    "action",
    "value",
    "actually",
    "reward",
    "see",
    "reward",
    "particular",
    "case",
    "value",
    "one",
    "let",
    "actually",
    "take",
    "look",
    "talk",
    "reward",
    "uh",
    "reward",
    "go",
    "reward",
    "one",
    "every",
    "step",
    "taken",
    "including",
    "termination",
    "steps",
    "basically",
    "means",
    "let",
    "hole",
    "four",
    "completely",
    "means",
    "get",
    "reward",
    "one",
    "pass",
    "certain",
    "threshold",
    "pole",
    "starts",
    "fall",
    "get",
    "reward",
    "basically",
    "keeping",
    "pole",
    "upright",
    "position",
    "falling",
    "getting",
    "accumulating",
    "value",
    "one",
    "every",
    "single",
    "time",
    "got",
    "value",
    "200",
    "nutshell",
    "sort",
    "shows",
    "theory",
    "way",
    "practical",
    "five",
    "steps",
    "getting",
    "step",
    "six",
    "six",
    "steps",
    "sort",
    "show",
    "define",
    "environment",
    "train",
    "model",
    "evaluate",
    "well",
    "test",
    "well",
    "done",
    "quite",
    "fair",
    "bit",
    "training",
    "obviously",
    "trained",
    "really",
    "really",
    "quickly",
    "right",
    "able",
    "spin",
    "train",
    "really",
    "quickly",
    "get",
    "running",
    "training",
    "way",
    "larger",
    "way",
    "sophisticated",
    "environment",
    "might",
    "want",
    "view",
    "training",
    "logs",
    "inside",
    "tensorboard",
    "actually",
    "exactly",
    "going",
    "start",
    "within",
    "jupiter",
    "notebooks",
    "ideally",
    "would",
    "want",
    "run",
    "command",
    "prompt",
    "locking",
    "notebook",
    "run",
    "going",
    "run",
    "continuously",
    "going",
    "unlock",
    "notebook",
    "going",
    "able",
    "run",
    "anything",
    "else",
    "sort",
    "show",
    "continue",
    "onwards",
    "first",
    "need",
    "need",
    "get",
    "log",
    "directory",
    "want",
    "view",
    "go",
    "back",
    "folders",
    "go",
    "root",
    "folder",
    "go",
    "training",
    "logs",
    "see",
    "got",
    "three",
    "different",
    "training",
    "sets",
    "remember",
    "kicked",
    "training",
    "three",
    "times",
    "got",
    "three",
    "different",
    "sets",
    "logs",
    "one",
    "believe",
    "second",
    "one",
    "first",
    "second",
    "one",
    "longest",
    "third",
    "one",
    "1000",
    "training",
    "steps",
    "let",
    "actually",
    "take",
    "look",
    "ppo2",
    "going",
    "going",
    "go",
    "folder",
    "going",
    "specify",
    "tensorboard",
    "run",
    "folder",
    "first",
    "need",
    "give",
    "path",
    "ppo2",
    "folder",
    "let",
    "specify",
    "first",
    "okay",
    "gone",
    "specified",
    "training",
    "log",
    "part",
    "go",
    "take",
    "look",
    "see",
    "giving",
    "us",
    "path",
    "ppo2",
    "folder",
    "training",
    "logs",
    "ppo2",
    "effectively",
    "gone",
    "training",
    "logs",
    "ppo2",
    "file",
    "tensorboard",
    "log",
    "file",
    "going",
    "able",
    "use",
    "need",
    "kick",
    "tensorboard",
    "within",
    "folder",
    "going",
    "need",
    "tensorboard",
    "lot",
    "installed",
    "believe",
    "pip",
    "install",
    "tensorboard",
    "go",
    "go",
    "run",
    "need",
    "run",
    "exclamation",
    "mark",
    "tensorboard",
    "dash",
    "dash",
    "log",
    "dear",
    "need",
    "specify",
    "training",
    "log",
    "path",
    "yeah",
    "looks",
    "right",
    "written",
    "exclamation",
    "mark",
    "tensorboard",
    "dash",
    "dash",
    "log",
    "dear",
    "equals",
    "inside",
    "squiggly",
    "brackets",
    "training",
    "underscore",
    "log",
    "path",
    "written",
    "wrong",
    "let",
    "quickly",
    "explain",
    "line",
    "think",
    "comments",
    "using",
    "exclamation",
    "mark",
    "inside",
    "jupiter",
    "notebook",
    "known",
    "using",
    "magic",
    "command",
    "allows",
    "run",
    "command",
    "line",
    "commands",
    "within",
    "notebook",
    "putting",
    "exclamation",
    "mark",
    "akin",
    "going",
    "command",
    "prompt",
    "terminal",
    "writing",
    "tensorboard",
    "dash",
    "dash",
    "log",
    "blah",
    "blah",
    "blah",
    "whatever",
    "particular",
    "case",
    "actually",
    "written",
    "exclamation",
    "mark",
    "tensorboard",
    "dash",
    "dash",
    "logged",
    "let",
    "actually",
    "show",
    "probably",
    "going",
    "make",
    "sense",
    "went",
    "drive",
    "cd",
    "youtube",
    "cd",
    "reinforcement",
    "learning",
    "uh",
    "let",
    "actually",
    "go",
    "let",
    "actually",
    "specify",
    "exact",
    "thing",
    "written",
    "training",
    "log",
    "pass",
    "going",
    "go",
    "training",
    "logs",
    "okay",
    "akin",
    "tensorboard",
    "dash",
    "dash",
    "log",
    "dear",
    "equals",
    "training",
    "slash",
    "logs",
    "slash",
    "ppo2",
    "right",
    "sort",
    "see",
    "actually",
    "running",
    "inside",
    "command",
    "prompt",
    "eventually",
    "get",
    "line",
    "says",
    "running",
    "http",
    "localhost",
    "6006",
    "line",
    "written",
    "inside",
    "command",
    "prompt",
    "exactly",
    "would",
    "running",
    "go",
    "link",
    "created",
    "tensorboard",
    "get",
    "training",
    "look",
    "like",
    "got",
    "training",
    "metrics",
    "happened",
    "okay",
    "let",
    "go",
    "directly",
    "folder",
    "go",
    "training",
    "going",
    "go",
    "ppo2",
    "run",
    "tensorboard",
    "dash",
    "dash",
    "log",
    "dear",
    "equals",
    "dot",
    "times",
    "charm",
    "let",
    "see",
    "works",
    "okay",
    "running",
    "http",
    "dash",
    "colon",
    "dash",
    "dash",
    "local",
    "host",
    "six",
    "six",
    "thousand",
    "six",
    "let",
    "refresh",
    "okay",
    "way",
    "better",
    "went",
    "went",
    "seeded",
    "folder",
    "guessing",
    "getting",
    "path",
    "specifying",
    "incorrect",
    "fine",
    "sort",
    "see",
    "run",
    "right",
    "going",
    "get",
    "whole",
    "heap",
    "different",
    "metrics",
    "specifically",
    "going",
    "get",
    "train",
    "metrics",
    "sort",
    "shows",
    "frames",
    "per",
    "second",
    "also",
    "going",
    "get",
    "number",
    "train",
    "metrics",
    "going",
    "get",
    "clip",
    "fraction",
    "approx",
    "underscore",
    "k",
    "want",
    "deeper",
    "dive",
    "metrics",
    "mean",
    "evaluate",
    "means",
    "hit",
    "comments",
    "probably",
    "little",
    "blue",
    "box",
    "somewhere",
    "corner",
    "sort",
    "explains",
    "well",
    "start",
    "see",
    "getting",
    "different",
    "training",
    "metrics",
    "getting",
    "entropy",
    "loss",
    "explained",
    "variance",
    "go",
    "higher",
    "learning",
    "rate",
    "looks",
    "like",
    "stayed",
    "pretty",
    "stable",
    "loss",
    "looks",
    "like",
    "going",
    "policy",
    "gradient",
    "loss",
    "well",
    "value",
    "loss",
    "getting",
    "whole",
    "bunch",
    "different",
    "types",
    "training",
    "metrics",
    "view",
    "tensorboard",
    "obviously",
    "run",
    "command",
    "prompt",
    "little",
    "bit",
    "reducing",
    "get",
    "work",
    "rather",
    "run",
    "notebook",
    "well",
    "stop",
    "right",
    "close",
    "command",
    "prompt",
    "actually",
    "run",
    "command",
    "effectively",
    "us",
    "right",
    "currently",
    "running",
    "see",
    "little",
    "asterisks",
    "go",
    "localhost",
    "6006",
    "gives",
    "us",
    "tensorboard",
    "running",
    "start",
    "see",
    "view",
    "logs",
    "well",
    "particular",
    "case",
    "set",
    "metrics",
    "done",
    "uh",
    "anyone",
    "feedback",
    "um",
    "better",
    "way",
    "launch",
    "tensorboard",
    "means",
    "hit",
    "comments",
    "always",
    "welcome",
    "feedback",
    "sort",
    "brings",
    "us",
    "end",
    "testing",
    "evaluation",
    "step",
    "went",
    "went",
    "evaluated",
    "model",
    "using",
    "evaluate",
    "underscore",
    "policy",
    "went",
    "tested",
    "also",
    "went",
    "viewed",
    "login",
    "tensorboard",
    "looks",
    "like",
    "still",
    "runs",
    "even",
    "though",
    "end",
    "cell",
    "something",
    "might",
    "need",
    "dig",
    "problems",
    "hit",
    "comments",
    "though",
    "quick",
    "word",
    "performance",
    "tuning",
    "performance",
    "general",
    "training",
    "model",
    "core",
    "metric",
    "looking",
    "average",
    "reward",
    "gives",
    "indication",
    "well",
    "model",
    "going",
    "perform",
    "particular",
    "environment",
    "using",
    "particular",
    "reward",
    "function",
    "metric",
    "want",
    "taking",
    "look",
    "average",
    "episode",
    "length",
    "ideally",
    "aims",
    "identify",
    "long",
    "agent",
    "actually",
    "lasting",
    "particular",
    "environment",
    "particularly",
    "important",
    "environments",
    "fixed",
    "environment",
    "length",
    "take",
    "look",
    "breakout",
    "environment",
    "take",
    "look",
    "environment",
    "really",
    "really",
    "good",
    "indicators",
    "taking",
    "look",
    "actually",
    "model",
    "performing",
    "well",
    "three",
    "key",
    "strategies",
    "suggest",
    "start",
    "taking",
    "look",
    "strategies",
    "one",
    "train",
    "longer",
    "trained",
    "say",
    "example",
    "ten",
    "thousand",
    "twenty",
    "thousand",
    "hundred",
    "thousand",
    "steps",
    "try",
    "training",
    "model",
    "longer",
    "see",
    "improves",
    "performance",
    "thing",
    "also",
    "take",
    "look",
    "hyper",
    "parameter",
    "tuning",
    "dealing",
    "deep",
    "learning",
    "models",
    "even",
    "traditional",
    "statistical",
    "machine",
    "learning",
    "models",
    "hyper",
    "parameter",
    "tuning",
    "yield",
    "significant",
    "results",
    "stable",
    "baselines",
    "supports",
    "hyper",
    "parameter",
    "tuning",
    "using",
    "package",
    "called",
    "optuna",
    "going",
    "show",
    "course",
    "like",
    "see",
    "little",
    "bit",
    "let",
    "know",
    "last",
    "thing",
    "suggest",
    "take",
    "look",
    "take",
    "look",
    "different",
    "algorithms",
    "people",
    "using",
    "perform",
    "state",
    "art",
    "training",
    "well",
    "another",
    "thing",
    "helps",
    "comes",
    "improving",
    "performance",
    "okay",
    "next",
    "topic",
    "going",
    "skipped",
    "way",
    "let",
    "go",
    "back",
    "step",
    "five",
    "call",
    "backs",
    "alternate",
    "algorithms",
    "architectures",
    "going",
    "particular",
    "step",
    "going",
    "recreating",
    "model",
    "time",
    "going",
    "going",
    "specify",
    "reward",
    "threshold",
    "means",
    "training",
    "going",
    "stop",
    "hits",
    "certain",
    "benchmark",
    "really",
    "really",
    "useful",
    "got",
    "really",
    "really",
    "large",
    "models",
    "trying",
    "train",
    "want",
    "stop",
    "model",
    "starts",
    "getting",
    "unstable",
    "actually",
    "use",
    "helpers",
    "stable",
    "bass",
    "lines",
    "use",
    "eval",
    "callback",
    "stop",
    "training",
    "reward",
    "threshold",
    "callback",
    "nice",
    "thing",
    "actually",
    "save",
    "model",
    "part",
    "well",
    "automatically",
    "save",
    "best",
    "model",
    "also",
    "going",
    "take",
    "look",
    "define",
    "different",
    "neural",
    "network",
    "architecture",
    "remember",
    "specified",
    "mlp",
    "policy",
    "actually",
    "pass",
    "different",
    "neural",
    "network",
    "architecture",
    "well",
    "last",
    "least",
    "going",
    "take",
    "look",
    "use",
    "different",
    "algorithm",
    "last",
    "thing",
    "particular",
    "section",
    "let",
    "kick",
    "first",
    "thing",
    "going",
    "taking",
    "look",
    "add",
    "callback",
    "training",
    "stage",
    "cool",
    "thing",
    "need",
    "stop",
    "training",
    "certain",
    "reward",
    "threshold",
    "gives",
    "automated",
    "capability",
    "really",
    "really",
    "important",
    "particularly",
    "training",
    "huge",
    "models",
    "models",
    "going",
    "take",
    "long",
    "time",
    "train",
    "say",
    "example",
    "training",
    "breakout",
    "tutorial",
    "tutorial",
    "might",
    "want",
    "use",
    "mandatory",
    "gives",
    "ability",
    "extend",
    "training",
    "step",
    "order",
    "first",
    "need",
    "install",
    "couple",
    "additional",
    "dependencies",
    "namely",
    "helpers",
    "stable",
    "baselines",
    "let",
    "go",
    "ahead",
    "import",
    "okay",
    "gone",
    "written",
    "one",
    "additional",
    "line",
    "code",
    "line",
    "written",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "common",
    "dot",
    "callbacks",
    "import",
    "eval",
    "call",
    "back",
    "comma",
    "stop",
    "training",
    "reward",
    "threshold",
    "eval",
    "callback",
    "going",
    "callback",
    "runs",
    "training",
    "stage",
    "stop",
    "training",
    "reward",
    "threshold",
    "going",
    "think",
    "like",
    "checker",
    "basically",
    "model",
    "passes",
    "certain",
    "reward",
    "threshold",
    "remember",
    "reward",
    "carpool",
    "environment",
    "reward",
    "indicates",
    "solved",
    "carpool",
    "environment",
    "200",
    "basically",
    "stopping",
    "training",
    "receives",
    "achieves",
    "200",
    "score",
    "average",
    "set",
    "need",
    "actually",
    "set",
    "callbacks",
    "let",
    "go",
    "ahead",
    "set",
    "kick",
    "another",
    "training",
    "run",
    "using",
    "okay",
    "two",
    "callbacks",
    "sort",
    "set",
    "one",
    "additional",
    "thing",
    "need",
    "need",
    "specify",
    "uh",
    "best",
    "model",
    "going",
    "come",
    "back",
    "second",
    "let",
    "actually",
    "take",
    "look",
    "wrote",
    "first",
    "setting",
    "stop",
    "training",
    "reward",
    "threshold",
    "callback",
    "callback",
    "basically",
    "going",
    "stop",
    "training",
    "pass",
    "certain",
    "reward",
    "threshold",
    "order",
    "written",
    "stop",
    "underscore",
    "callback",
    "equals",
    "stop",
    "training",
    "reward",
    "threshold",
    "imported",
    "passing",
    "reward",
    "threshold",
    "basically",
    "specifies",
    "average",
    "reward",
    "want",
    "stop",
    "training",
    "case",
    "set",
    "also",
    "specified",
    "verbose",
    "equals",
    "one",
    "get",
    "additional",
    "logging",
    "next",
    "callback",
    "actually",
    "written",
    "eval",
    "callback",
    "callback",
    "going",
    "triggered",
    "training",
    "run",
    "particular",
    "case",
    "written",
    "eval",
    "underscore",
    "callback",
    "equals",
    "eval",
    "callback",
    "two",
    "pass",
    "number",
    "arguments",
    "first",
    "passing",
    "environment",
    "passing",
    "callback",
    "want",
    "run",
    "new",
    "best",
    "model",
    "case",
    "written",
    "callback",
    "new",
    "best",
    "specified",
    "stop",
    "callbacks",
    "basically",
    "means",
    "every",
    "time",
    "new",
    "best",
    "model",
    "going",
    "run",
    "stop",
    "callback",
    "stop",
    "callback",
    "realizes",
    "reward",
    "threshold",
    "200",
    "going",
    "stop",
    "training",
    "overall",
    "also",
    "specify",
    "frequently",
    "want",
    "run",
    "evaluation",
    "callback",
    "case",
    "set",
    "10",
    "000",
    "time",
    "steps",
    "also",
    "specified",
    "best",
    "underscore",
    "model",
    "actually",
    "need",
    "actually",
    "eval",
    "callback",
    "save",
    "model",
    "every",
    "time",
    "new",
    "best",
    "model",
    "need",
    "however",
    "specify",
    "want",
    "model",
    "called",
    "let",
    "specify",
    "case",
    "gone",
    "defined",
    "save",
    "path",
    "want",
    "save",
    "best",
    "model",
    "written",
    "save",
    "underscore",
    "path",
    "equals",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "specified",
    "saved",
    "model",
    "folder",
    "training",
    "comma",
    "saved",
    "space",
    "models",
    "going",
    "going",
    "specify",
    "best",
    "model",
    "save",
    "path",
    "means",
    "every",
    "10",
    "000",
    "runs",
    "going",
    "double",
    "check",
    "whether",
    "past",
    "200",
    "reward",
    "threshold",
    "stop",
    "training",
    "also",
    "save",
    "best",
    "model",
    "save",
    "pass",
    "actually",
    "see",
    "actually",
    "trigger",
    "run",
    "cell",
    "looks",
    "like",
    "written",
    "best",
    "model",
    "save",
    "path",
    "bad",
    "go",
    "cool",
    "needed",
    "update",
    "parameter",
    "best",
    "underscore",
    "model",
    "underscore",
    "save",
    "underscore",
    "path",
    "specified",
    "save",
    "path",
    "good",
    "go",
    "two",
    "training",
    "callbacks",
    "need",
    "associate",
    "model",
    "going",
    "create",
    "new",
    "ppo",
    "model",
    "assign",
    "callbacks",
    "new",
    "model",
    "created",
    "line",
    "exactly",
    "creating",
    "initial",
    "model",
    "step",
    "three",
    "exactly",
    "line",
    "different",
    "run",
    "training",
    "command",
    "going",
    "pass",
    "callback",
    "let",
    "okay",
    "gone",
    "written",
    "rather",
    "passing",
    "total",
    "time",
    "steps",
    "also",
    "passing",
    "callback",
    "want",
    "run",
    "case",
    "passing",
    "eval",
    "callback",
    "going",
    "callback",
    "checks",
    "every",
    "10",
    "000",
    "steps",
    "every",
    "10",
    "000",
    "steps",
    "going",
    "save",
    "best",
    "new",
    "model",
    "got",
    "save",
    "path",
    "also",
    "going",
    "check",
    "whether",
    "past",
    "average",
    "reward",
    "threshold",
    "go",
    "kick",
    "see",
    "training",
    "kick",
    "let",
    "10",
    "000",
    "steps",
    "see",
    "fact",
    "evaluating",
    "whether",
    "past",
    "reward",
    "threshold",
    "see",
    "8",
    "000",
    "right",
    "check",
    "next",
    "one",
    "see",
    "gone",
    "evaluated",
    "checked",
    "episode",
    "length",
    "looks",
    "average",
    "keep",
    "going",
    "another",
    "10",
    "000",
    "see",
    "eval",
    "callback",
    "run",
    "go",
    "hit",
    "200",
    "stopped",
    "training",
    "20",
    "000",
    "train",
    "longer",
    "would",
    "stop",
    "regardless",
    "hit",
    "200",
    "score",
    "threshold",
    "gon",
    "na",
    "stop",
    "training",
    "pretty",
    "cool",
    "right",
    "gives",
    "lot",
    "flexibility",
    "actually",
    "going",
    "training",
    "really",
    "large",
    "models",
    "want",
    "cap",
    "sort",
    "runs",
    "wild",
    "another",
    "thing",
    "note",
    "also",
    "saved",
    "model",
    "go",
    "reinforcement",
    "learning",
    "go",
    "training",
    "folder",
    "saved",
    "models",
    "folder",
    "see",
    "best",
    "model",
    "folder",
    "best",
    "model",
    "saved",
    "well",
    "result",
    "actually",
    "callback",
    "saved",
    "actually",
    "goes",
    "saves",
    "best",
    "model",
    "well",
    "okay",
    "callback",
    "next",
    "thing",
    "also",
    "change",
    "policy",
    "say",
    "example",
    "wanted",
    "use",
    "different",
    "neural",
    "network",
    "well",
    "let",
    "take",
    "look",
    "might",
    "order",
    "change",
    "policies",
    "actually",
    "specify",
    "new",
    "neural",
    "network",
    "architecture",
    "order",
    "need",
    "specify",
    "network",
    "architecture",
    "custom",
    "actor",
    "well",
    "value",
    "function",
    "show",
    "akin",
    "changing",
    "number",
    "units",
    "number",
    "layers",
    "inside",
    "neural",
    "network",
    "pretty",
    "simple",
    "pass",
    "model",
    "let",
    "okay",
    "new",
    "neural",
    "network",
    "architecture",
    "defined",
    "need",
    "actually",
    "associate",
    "algorithm",
    "actually",
    "written",
    "new",
    "underscore",
    "arch",
    "new",
    "arch",
    "equals",
    "inside",
    "square",
    "brackets",
    "defined",
    "dictionary",
    "first",
    "neural",
    "network",
    "architecture",
    "defined",
    "custom",
    "actor",
    "order",
    "need",
    "pass",
    "pi",
    "specifying",
    "going",
    "new",
    "neural",
    "network",
    "128",
    "units",
    "one",
    "layers",
    "four",
    "layers",
    "128",
    "units",
    "128",
    "128",
    "128",
    "128",
    "need",
    "specify",
    "value",
    "functions",
    "four",
    "layers",
    "128",
    "units",
    "neural",
    "network",
    "layer",
    "see",
    "vf",
    "equals",
    "inside",
    "square",
    "brackets",
    "128",
    "128",
    "128",
    "128",
    "get",
    "might",
    "really",
    "specific",
    "reason",
    "actually",
    "found",
    "neural",
    "networks",
    "inside",
    "baseline",
    "algorithms",
    "work",
    "pretty",
    "well",
    "sort",
    "shows",
    "might",
    "go",
    "let",
    "go",
    "ahead",
    "associate",
    "model",
    "kick",
    "training",
    "oh",
    "actually",
    "net",
    "arch",
    "bad",
    "go",
    "alrighty",
    "cool",
    "new",
    "neural",
    "network",
    "associated",
    "ppo",
    "model",
    "specifically",
    "gone",
    "updated",
    "policy",
    "gone",
    "written",
    "model",
    "equals",
    "ppo",
    "mlp",
    "policy",
    "using",
    "perceptron",
    "policy",
    "pass",
    "environment",
    "verbose",
    "equals",
    "one",
    "tensorboard",
    "log",
    "log",
    "path",
    "real",
    "change",
    "onwards",
    "order",
    "specify",
    "new",
    "neural",
    "network",
    "specifically",
    "new",
    "policy",
    "written",
    "policy",
    "underscored",
    "kw",
    "args",
    "equals",
    "passed",
    "dictionary",
    "dictionary",
    "specified",
    "net",
    "underscore",
    "arch",
    "value",
    "set",
    "equal",
    "netarch",
    "defining",
    "new",
    "neural",
    "network",
    "new",
    "neural",
    "network",
    "policy",
    "attached",
    "model",
    "type",
    "apply",
    "eval",
    "callback",
    "well",
    "run",
    "using",
    "new",
    "neural",
    "network",
    "architecture",
    "specifically",
    "new",
    "policy",
    "another",
    "thing",
    "call",
    "inside",
    "stable",
    "baselines",
    "actually",
    "got",
    "different",
    "policies",
    "go",
    "documentation",
    "go",
    "custom",
    "policy",
    "network",
    "whole",
    "heap",
    "information",
    "actually",
    "well",
    "define",
    "custom",
    "feature",
    "extractors",
    "forth",
    "pretty",
    "pretty",
    "cool",
    "actually",
    "actually",
    "get",
    "really",
    "really",
    "sophisticated",
    "case",
    "got",
    "model",
    "looks",
    "like",
    "training",
    "sufficiently",
    "eval",
    "callback",
    "kicked",
    "looks",
    "like",
    "episode",
    "length",
    "hit",
    "looks",
    "like",
    "good",
    "stopped",
    "righty",
    "next",
    "thing",
    "want",
    "actually",
    "take",
    "look",
    "might",
    "go",
    "using",
    "alternate",
    "algorithm",
    "rather",
    "using",
    "ppo",
    "model",
    "using",
    "far",
    "might",
    "want",
    "use",
    "dqn",
    "example",
    "might",
    "go",
    "well",
    "first",
    "need",
    "import",
    "dqn",
    "algorithm",
    "let",
    "okay",
    "dqn",
    "imported",
    "written",
    "stable",
    "underscore",
    "baselines",
    "import",
    "dqn",
    "go",
    "use",
    "really",
    "similar",
    "manner",
    "used",
    "ppo",
    "models",
    "could",
    "actually",
    "copy",
    "paste",
    "really",
    "need",
    "sub",
    "ppo",
    "dqn",
    "going",
    "get",
    "rid",
    "policy",
    "keyword",
    "arguments",
    "instantiated",
    "dqm",
    "model",
    "pass",
    "total",
    "time",
    "steps",
    "set",
    "20",
    "go",
    "training",
    "dqn",
    "model",
    "rather",
    "ppo",
    "model",
    "really",
    "really",
    "quickly",
    "shows",
    "apply",
    "different",
    "algorithm",
    "remember",
    "also",
    "stable",
    "baselines",
    "got",
    "bunch",
    "different",
    "types",
    "algorithms",
    "got",
    "a2c",
    "ddpg",
    "dqn",
    "ppo",
    "sac",
    "td3",
    "stable",
    "baselines",
    "2",
    "even",
    "algorithms",
    "well",
    "maintenance",
    "mode",
    "figured",
    "show",
    "stable",
    "baselines",
    "three",
    "cool",
    "dqm",
    "model",
    "done",
    "save",
    "export",
    "similar",
    "manner",
    "type",
    "difference",
    "loading",
    "dqn",
    "rather",
    "typing",
    "type",
    "nutshell",
    "really",
    "covers",
    "add",
    "callback",
    "trading",
    "stage",
    "change",
    "policy",
    "well",
    "use",
    "alternate",
    "algorithm",
    "note",
    "brings",
    "us",
    "projects",
    "step",
    "six",
    "going",
    "go",
    "different",
    "projects",
    "got",
    "three",
    "specific",
    "projects",
    "going",
    "take",
    "look",
    "project",
    "one",
    "going",
    "take",
    "look",
    "reinforcement",
    "learning",
    "atari",
    "games",
    "specifically",
    "going",
    "trying",
    "solve",
    "breakout",
    "problem",
    "project",
    "two",
    "going",
    "try",
    "leverage",
    "reinforcement",
    "learning",
    "build",
    "racing",
    "car",
    "sort",
    "like",
    "almost",
    "going",
    "path",
    "autonomous",
    "driving",
    "project",
    "three",
    "going",
    "take",
    "look",
    "build",
    "custom",
    "environments",
    "using",
    "open",
    "ai",
    "gym",
    "spaces",
    "talked",
    "little",
    "bit",
    "earlier",
    "first",
    "things",
    "first",
    "let",
    "take",
    "look",
    "reinforcement",
    "learning",
    "atari",
    "games",
    "okay",
    "inside",
    "github",
    "repository",
    "also",
    "going",
    "couple",
    "additional",
    "projects",
    "project",
    "one",
    "project",
    "two",
    "project",
    "three",
    "project",
    "one",
    "gon",
    "na",
    "breakout",
    "actually",
    "go",
    "github",
    "repo",
    "see",
    "project",
    "one",
    "breakout",
    "project",
    "two",
    "project",
    "three",
    "custom",
    "environment",
    "case",
    "gone",
    "done",
    "started",
    "project",
    "one",
    "breakout",
    "even",
    "though",
    "working",
    "different",
    "environments",
    "actually",
    "go",
    "training",
    "going",
    "similar",
    "whilst",
    "spent",
    "lot",
    "time",
    "going",
    "basics",
    "main",
    "course",
    "actually",
    "go",
    "applying",
    "different",
    "environments",
    "going",
    "pretty",
    "much",
    "let",
    "start",
    "importing",
    "dependencies",
    "first",
    "okay",
    "gone",
    "written",
    "six",
    "different",
    "lines",
    "code",
    "going",
    "pretty",
    "familiar",
    "previous",
    "tutorial",
    "first",
    "one",
    "written",
    "first",
    "line",
    "written",
    "importing",
    "jim",
    "difference",
    "stable",
    "baselines",
    "three",
    "importing",
    "a2c",
    "different",
    "algorithm",
    "remember",
    "import",
    "ppo",
    "import",
    "dqn",
    "time",
    "going",
    "importing",
    "a2c",
    "different",
    "algorithm",
    "importing",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "underscore",
    "emv",
    "import",
    "vec",
    "frame",
    "stack",
    "remember",
    "main",
    "tutorial",
    "vectorize",
    "environment",
    "trained",
    "one",
    "environment",
    "time",
    "going",
    "breakout",
    "actually",
    "going",
    "train",
    "four",
    "environments",
    "time",
    "allow",
    "us",
    "speed",
    "training",
    "written",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "dot",
    "evaluation",
    "import",
    "evaluate",
    "underscore",
    "policy",
    "change",
    "use",
    "previously",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "common",
    "dot",
    "env",
    "underscore",
    "util",
    "import",
    "make",
    "underscore",
    "atari",
    "underscore",
    "env",
    "line",
    "little",
    "bit",
    "different",
    "helps",
    "us",
    "work",
    "atari",
    "environment",
    "atari",
    "environments",
    "environments",
    "allow",
    "us",
    "play",
    "atari",
    "game",
    "actually",
    "go",
    "gym",
    "documentation",
    "take",
    "look",
    "environments",
    "see",
    "atari",
    "got",
    "ability",
    "try",
    "lot",
    "games",
    "specifically",
    "going",
    "training",
    "breakouts",
    "let",
    "take",
    "look",
    "one",
    "going",
    "look",
    "like",
    "yeah",
    "actually",
    "one",
    "basically",
    "goal",
    "maximize",
    "score",
    "see",
    "got",
    "maximum",
    "number",
    "lives",
    "well",
    "actually",
    "number",
    "lives",
    "number",
    "scores",
    "goal",
    "maximize",
    "score",
    "real",
    "cap",
    "get",
    "completely",
    "solve",
    "environment",
    "getting",
    "highest",
    "possible",
    "score",
    "let",
    "go",
    "ahead",
    "else",
    "would",
    "write",
    "import",
    "os",
    "going",
    "allow",
    "us",
    "work",
    "operating",
    "system",
    "another",
    "thing",
    "wanted",
    "call",
    "say",
    "example",
    "wanted",
    "use",
    "gpu",
    "acceleration",
    "said",
    "show",
    "well",
    "need",
    "go",
    "back",
    "pi",
    "torch",
    "link",
    "case",
    "going",
    "choose",
    "build",
    "want",
    "stable",
    "windows",
    "pip",
    "python",
    "already",
    "got",
    "cuda",
    "11",
    "installed",
    "machine",
    "gon",
    "na",
    "need",
    "order",
    "work",
    "copy",
    "link",
    "bring",
    "notebook",
    "add",
    "exclamation",
    "mark",
    "paste",
    "need",
    "get",
    "rid",
    "three",
    "run",
    "going",
    "install",
    "cuda",
    "accelerated",
    "version",
    "pi",
    "torch",
    "typically",
    "need",
    "restart",
    "kernel",
    "hit",
    "kernel",
    "restart",
    "hit",
    "restart",
    "good",
    "go",
    "need",
    "restarted",
    "kernel",
    "dependencies",
    "done",
    "okay",
    "late",
    "change",
    "entire",
    "environment",
    "previously",
    "used",
    "able",
    "import",
    "used",
    "sort",
    "work",
    "got",
    "something",
    "slight",
    "bit",
    "different",
    "actually",
    "need",
    "download",
    "raw",
    "files",
    "let",
    "grab",
    "link",
    "need",
    "download",
    "raw",
    "files",
    "particular",
    "link",
    "forward",
    "slash",
    "roms",
    "forward",
    "slash",
    "see",
    "downloading",
    "paste",
    "make",
    "available",
    "notebook",
    "well",
    "see",
    "http",
    "colon",
    "forward",
    "slash",
    "forward",
    "slash",
    "forward",
    "slash",
    "roms",
    "forward",
    "slash",
    "roms",
    "dot",
    "ra",
    "without",
    "one",
    "need",
    "going",
    "download",
    "files",
    "need",
    "able",
    "work",
    "atari",
    "environment",
    "need",
    "need",
    "previously",
    "think",
    "late",
    "change",
    "entire",
    "environments",
    "need",
    "order",
    "use",
    "downloaded",
    "file",
    "called",
    "let",
    "wait",
    "download",
    "show",
    "okay",
    "looks",
    "like",
    "finished",
    "downloading",
    "let",
    "go",
    "take",
    "look",
    "see",
    "got",
    "going",
    "copy",
    "paste",
    "directory",
    "currently",
    "working",
    "see",
    "already",
    "got",
    "roms",
    "got",
    "hc",
    "rom",
    "files",
    "actually",
    "delete",
    "need",
    "paste",
    "file",
    "unzip",
    "going",
    "extract",
    "see",
    "got",
    "hc",
    "roms",
    "rom",
    "let",
    "zoom",
    "got",
    "hc",
    "roms",
    "roms",
    "extract",
    "folder",
    "hc",
    "roms",
    "let",
    "roms",
    "well",
    "extracted",
    "command",
    "need",
    "run",
    "order",
    "install",
    "pretty",
    "straightforward",
    "done",
    "able",
    "leverage",
    "atari",
    "environment",
    "let",
    "let",
    "finish",
    "good",
    "go",
    "get",
    "warning",
    "skip",
    "cool",
    "good",
    "actually",
    "delete",
    "delete",
    "hc",
    "roms",
    "roms",
    "need",
    "need",
    "extracted",
    "folders",
    "see",
    "okay",
    "well",
    "good",
    "need",
    "go",
    "ahead",
    "install",
    "go",
    "back",
    "need",
    "run",
    "simple",
    "command",
    "go",
    "install",
    "environment",
    "let",
    "go",
    "ahead",
    "go",
    "good",
    "command",
    "run",
    "exclamation",
    "mark",
    "python",
    "dash",
    "atari",
    "underscore",
    "pi",
    "dot",
    "import",
    "underscore",
    "roms",
    "need",
    "pass",
    "path",
    "roms",
    "actually",
    "show",
    "files",
    "inside",
    "folder",
    "called",
    "roms",
    "roms",
    "need",
    "point",
    "particular",
    "file",
    "path",
    "case",
    "written",
    "exclamation",
    "mark",
    "python",
    "dash",
    "atari",
    "underscore",
    "pi",
    "dot",
    "import",
    "underscore",
    "roms",
    "dot",
    "backwards",
    "roms",
    "backwards",
    "rom",
    "mac",
    "file",
    "path",
    "might",
    "little",
    "bit",
    "different",
    "believe",
    "forward",
    "slash",
    "rather",
    "backward",
    "slash",
    "sort",
    "get",
    "idea",
    "done",
    "good",
    "go",
    "ahead",
    "test",
    "environment",
    "let",
    "go",
    "ahead",
    "set",
    "environment",
    "actually",
    "take",
    "look",
    "okay",
    "environment",
    "made",
    "type",
    "get",
    "observation",
    "oh",
    "bad",
    "go",
    "got",
    "observations",
    "type",
    "uh",
    "one",
    "emv",
    "dot",
    "action",
    "space",
    "leave",
    "env",
    "dot",
    "action",
    "underscore",
    "space",
    "see",
    "action",
    "space",
    "discrete",
    "got",
    "four",
    "different",
    "actions",
    "take",
    "take",
    "look",
    "observation",
    "space",
    "v",
    "dot",
    "observation",
    "says",
    "see",
    "observation",
    "space",
    "going",
    "box",
    "values",
    "ranging",
    "0",
    "255",
    "dimensions",
    "going",
    "210",
    "terms",
    "height",
    "160",
    "terms",
    "width",
    "means",
    "looks",
    "like",
    "going",
    "image",
    "case",
    "image",
    "based",
    "model",
    "actually",
    "go",
    "ahead",
    "test",
    "model",
    "remember",
    "cast",
    "minds",
    "back",
    "step",
    "6",
    "actually",
    "tested",
    "model",
    "actually",
    "copy",
    "block",
    "code",
    "run",
    "thing",
    "remember",
    "going",
    "go",
    "ahead",
    "test",
    "particular",
    "model",
    "actually",
    "wrong",
    "code",
    "let",
    "actually",
    "write",
    "scratch",
    "go",
    "number",
    "episodes",
    "actually",
    "play",
    "breakout",
    "let",
    "give",
    "crack",
    "okay",
    "let",
    "take",
    "look",
    "wrote",
    "code",
    "really",
    "really",
    "similar",
    "used",
    "step",
    "two",
    "loaded",
    "tested",
    "environment",
    "setting",
    "number",
    "episodes",
    "want",
    "play",
    "looping",
    "one",
    "episodes",
    "basically",
    "going",
    "ahead",
    "taking",
    "random",
    "actions",
    "environment",
    "see",
    "looks",
    "like",
    "run",
    "get",
    "little",
    "see",
    "effectively",
    "playing",
    "breakouts",
    "went",
    "really",
    "really",
    "quickly",
    "want",
    "close",
    "environment",
    "comment",
    "last",
    "line",
    "going",
    "see",
    "play",
    "see",
    "sort",
    "playing",
    "randomly",
    "exactly",
    "getting",
    "highest",
    "score",
    "capping",
    "added",
    "around",
    "two",
    "four",
    "looks",
    "like",
    "highest",
    "got",
    "four",
    "gets",
    "point",
    "block",
    "breaks",
    "see",
    "got",
    "one",
    "two",
    "three",
    "case",
    "want",
    "try",
    "train",
    "model",
    "able",
    "play",
    "little",
    "bit",
    "better",
    "training",
    "model",
    "take",
    "long",
    "time",
    "give",
    "crack",
    "want",
    "take",
    "train",
    "longer",
    "let",
    "know",
    "go",
    "comments",
    "going",
    "little",
    "bit",
    "different",
    "main",
    "tutorial",
    "going",
    "actually",
    "going",
    "vectorize",
    "environment",
    "train",
    "four",
    "different",
    "environments",
    "time",
    "let",
    "go",
    "ahead",
    "test",
    "okay",
    "go",
    "environment",
    "moment",
    "type",
    "see",
    "actually",
    "playing",
    "four",
    "different",
    "environments",
    "means",
    "actually",
    "go",
    "train",
    "going",
    "training",
    "four",
    "environments",
    "time",
    "hopefully",
    "give",
    "us",
    "little",
    "bit",
    "speed",
    "type",
    "close",
    "see",
    "look",
    "like",
    "closed",
    "case",
    "let",
    "try",
    "sometimes",
    "going",
    "want",
    "close",
    "going",
    "shut",
    "want",
    "foreshadow",
    "sometimes",
    "crash",
    "kernel",
    "fine",
    "leave",
    "open",
    "environment",
    "set",
    "well",
    "good",
    "actually",
    "set",
    "model",
    "actually",
    "go",
    "ahead",
    "train",
    "let",
    "kick",
    "training",
    "oh",
    "let",
    "actually",
    "take",
    "look",
    "wrote",
    "vectorize",
    "environment",
    "completely",
    "skipped",
    "wrote",
    "emv",
    "equals",
    "make",
    "underscore",
    "atari",
    "underscore",
    "emv",
    "pass",
    "type",
    "environment",
    "want",
    "run",
    "case",
    "environment",
    "actually",
    "running",
    "breakout",
    "dash",
    "v0",
    "breakout",
    "game",
    "actually",
    "working",
    "key",
    "thing",
    "call",
    "actually",
    "take",
    "look",
    "gym",
    "environments",
    "actually",
    "breakout",
    "ram",
    "version",
    "breakout",
    "dash",
    "v0",
    "version",
    "one",
    "going",
    "train",
    "using",
    "images",
    "one",
    "actually",
    "going",
    "train",
    "using",
    "ram",
    "want",
    "image",
    "based",
    "model",
    "going",
    "using",
    "cnn",
    "policy",
    "also",
    "passed",
    "n",
    "underscore",
    "e",
    "v",
    "equals",
    "4",
    "going",
    "use",
    "four",
    "environments",
    "time",
    "going",
    "specify",
    "seed",
    "zero",
    "get",
    "reproducible",
    "results",
    "actually",
    "gone",
    "gone",
    "stacked",
    "environments",
    "together",
    "written",
    "emv",
    "equals",
    "vec",
    "frame",
    "stack",
    "wrapper",
    "imported",
    "passed",
    "environment",
    "specified",
    "n",
    "underscore",
    "stack",
    "equals",
    "four",
    "basically",
    "stacks",
    "environments",
    "together",
    "going",
    "go",
    "specify",
    "models",
    "let",
    "go",
    "okay",
    "model",
    "set",
    "gone",
    "written",
    "two",
    "lines",
    "code",
    "first",
    "written",
    "log",
    "underscore",
    "path",
    "equals",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "specify",
    "training",
    "log",
    "similar",
    "set",
    "log",
    "path",
    "main",
    "tutorial",
    "gone",
    "specified",
    "model",
    "model",
    "equals",
    "a2c",
    "remember",
    "using",
    "different",
    "model",
    "particular",
    "case",
    "different",
    "algorithm",
    "using",
    "rather",
    "ppo",
    "dqm",
    "specify",
    "type",
    "policy",
    "want",
    "use",
    "key",
    "differentiator",
    "previously",
    "used",
    "mlp",
    "policy",
    "great",
    "tabular",
    "data",
    "tabular",
    "observations",
    "image",
    "specifically",
    "observations",
    "image",
    "case",
    "cnn",
    "policy",
    "actually",
    "going",
    "lot",
    "faster",
    "train",
    "specified",
    "cnn",
    "policy",
    "basically",
    "uses",
    "convolutional",
    "neural",
    "network",
    "part",
    "policy",
    "rather",
    "perceptron",
    "gone",
    "specified",
    "environment",
    "coming",
    "specified",
    "verbose",
    "equals",
    "one",
    "want",
    "logging",
    "also",
    "specified",
    "tensorboard",
    "log",
    "path",
    "go",
    "go",
    "ahead",
    "train",
    "going",
    "train",
    "little",
    "bit",
    "longer",
    "might",
    "might",
    "stop",
    "training",
    "runs",
    "long",
    "actually",
    "load",
    "one",
    "see",
    "performs",
    "let",
    "train",
    "let",
    "give",
    "200",
    "000",
    "steps",
    "want",
    "get",
    "really",
    "really",
    "high",
    "performing",
    "model",
    "might",
    "need",
    "train",
    "million",
    "even",
    "two",
    "million",
    "steps",
    "let",
    "give",
    "hundred",
    "thousand",
    "see",
    "long",
    "takes",
    "ideally",
    "able",
    "get",
    "score",
    "higher",
    "four",
    "see",
    "random",
    "actions",
    "okay",
    "real",
    "difference",
    "written",
    "model",
    "dot",
    "learn",
    "pass",
    "total",
    "underscore",
    "time",
    "steps",
    "specify",
    "hundred",
    "thousand",
    "different",
    "previous",
    "tutorial",
    "training",
    "model",
    "gon",
    "na",
    "run",
    "right",
    "back",
    "okay",
    "see",
    "training",
    "kicked",
    "let",
    "train",
    "right",
    "back",
    "soon",
    "done",
    "gon",
    "na",
    "train",
    "hundred",
    "thousand",
    "steps",
    "give",
    "little",
    "bit",
    "time",
    "okay",
    "breakout",
    "model",
    "finished",
    "training",
    "see",
    "hundred",
    "thousand",
    "steps",
    "got",
    "episode",
    "reward",
    "mean",
    "average",
    "episode",
    "reward",
    "episode",
    "length",
    "479",
    "frames",
    "bad",
    "overall",
    "want",
    "save",
    "model",
    "reload",
    "anything",
    "else",
    "let",
    "go",
    "ahead",
    "going",
    "really",
    "similar",
    "saved",
    "models",
    "nothing",
    "crazy",
    "model",
    "saved",
    "go",
    "take",
    "look",
    "see",
    "gone",
    "saved",
    "a2c",
    "breakout",
    "model",
    "also",
    "got",
    "a2c",
    "model",
    "trained",
    "300",
    "000",
    "steps",
    "breakouts",
    "wanted",
    "take",
    "look",
    "one",
    "include",
    "github",
    "repo",
    "well",
    "take",
    "look",
    "try",
    "one",
    "case",
    "going",
    "test",
    "model",
    "let",
    "go",
    "ahead",
    "delete",
    "model",
    "reload",
    "make",
    "sure",
    "works",
    "okay",
    "model",
    "reloaded",
    "use",
    "evaluate",
    "policy",
    "method",
    "test",
    "remember",
    "max",
    "score",
    "got",
    "testing",
    "five",
    "episodes",
    "four",
    "ideally",
    "model",
    "ideally",
    "try",
    "get",
    "little",
    "bit",
    "better",
    "let",
    "try",
    "going",
    "going",
    "use",
    "evaluate",
    "policy",
    "going",
    "pass",
    "model",
    "environment",
    "number",
    "eval",
    "steps",
    "going",
    "let",
    "10",
    "let",
    "render",
    "equals",
    "true",
    "let",
    "take",
    "look",
    "must",
    "pass",
    "okay",
    "actually",
    "pretty",
    "common",
    "actually",
    "go",
    "evaluate",
    "go",
    "evaluate",
    "single",
    "environment",
    "remember",
    "correctly",
    "went",
    "created",
    "environment",
    "right",
    "four",
    "environments",
    "vectorized",
    "trained",
    "whole",
    "heap",
    "faster",
    "actually",
    "single",
    "leverage",
    "vectorized",
    "model",
    "environment",
    "environment",
    "single",
    "particular",
    "environment",
    "inside",
    "let",
    "go",
    "ahead",
    "create",
    "one",
    "first",
    "okay",
    "gone",
    "recreated",
    "environment",
    "time",
    "rather",
    "passing",
    "four",
    "environments",
    "make",
    "atari",
    "environment",
    "function",
    "passed",
    "one",
    "still",
    "stacking",
    "though",
    "four",
    "environments",
    "going",
    "allow",
    "us",
    "leverage",
    "go",
    "run",
    "evaluate",
    "policy",
    "method",
    "see",
    "running",
    "well",
    "see",
    "playing",
    "way",
    "better",
    "looking",
    "like",
    "got",
    "five",
    "six",
    "seven",
    "six",
    "still",
    "playing",
    "way",
    "better",
    "random",
    "agent",
    "see",
    "average",
    "getting",
    "value",
    "standard",
    "deviation",
    "could",
    "also",
    "could",
    "also",
    "test",
    "bigger",
    "model",
    "ca",
    "remember",
    "well",
    "performing",
    "let",
    "go",
    "ahead",
    "test",
    "model",
    "path",
    "model",
    "going",
    "a2c",
    "300k",
    "models",
    "copy",
    "name",
    "try",
    "loading",
    "going",
    "update",
    "a2c",
    "path",
    "load",
    "one",
    "recreate",
    "environment",
    "need",
    "recreate",
    "let",
    "try",
    "one",
    "get",
    "environment",
    "sort",
    "freezing",
    "like",
    "sometimes",
    "might",
    "need",
    "restart",
    "notebook",
    "see",
    "look",
    "like",
    "opening",
    "ideally",
    "hit",
    "restart",
    "kernel",
    "make",
    "sure",
    "save",
    "model",
    "going",
    "hit",
    "restart",
    "hit",
    "restart",
    "ideally",
    "close",
    "oh",
    "want",
    "stop",
    "yep",
    "good",
    "another",
    "kernel",
    "going",
    "dependencies",
    "import",
    "scroll",
    "going",
    "define",
    "a2c",
    "path",
    "load",
    "model",
    "need",
    "recreate",
    "environment",
    "load",
    "model",
    "try",
    "go",
    "looks",
    "like",
    "performing",
    "way",
    "better",
    "already",
    "see",
    "model",
    "obviously",
    "trained",
    "lot",
    "longer",
    "getting",
    "tens",
    "possibly",
    "20s",
    "actually",
    "playing",
    "longer",
    "train",
    "better",
    "model",
    "actually",
    "going",
    "get",
    "could",
    "also",
    "try",
    "using",
    "recurrent",
    "policies",
    "moment",
    "implemented",
    "stable",
    "baselines",
    "let",
    "know",
    "pinned",
    "comment",
    "sort",
    "gives",
    "idea",
    "go",
    "training",
    "reinforcement",
    "learning",
    "agent",
    "breakout",
    "clean",
    "type",
    "enb",
    "dot",
    "close",
    "close",
    "atari",
    "environment",
    "one",
    "left",
    "well",
    "good",
    "went",
    "ton",
    "stuff",
    "went",
    "imported",
    "dependencies",
    "imported",
    "couple",
    "new",
    "ones",
    "work",
    "atari",
    "also",
    "went",
    "installed",
    "atari",
    "roms",
    "remember",
    "got",
    "download",
    "atari",
    "mania",
    "include",
    "link",
    "description",
    "vectorize",
    "environment",
    "rather",
    "using",
    "single",
    "environment",
    "trained",
    "four",
    "gives",
    "us",
    "bit",
    "speed",
    "boost",
    "also",
    "went",
    "trained",
    "went",
    "saved",
    "evaluated",
    "bottom",
    "also",
    "showed",
    "300k",
    "model",
    "see",
    "getting",
    "average",
    "score",
    "standard",
    "deviation",
    "five",
    "overall",
    "better",
    "higher",
    "standard",
    "deviation",
    "sort",
    "gives",
    "idea",
    "possible",
    "training",
    "little",
    "bit",
    "longer",
    "hey",
    "guys",
    "editing",
    "nick",
    "jump",
    "next",
    "project",
    "wanted",
    "let",
    "know",
    "ended",
    "training",
    "breakout",
    "model",
    "additional",
    "two",
    "million",
    "steps",
    "see",
    "would",
    "actually",
    "take",
    "look",
    "like",
    "training",
    "around",
    "two",
    "million",
    "steps",
    "ended",
    "average",
    "reward",
    "around",
    "20",
    "point",
    "range",
    "obviously",
    "markedly",
    "improved",
    "result",
    "original",
    "model",
    "ideally",
    "see",
    "impact",
    "training",
    "longer",
    "looked",
    "like",
    "saying",
    "ended",
    "training",
    "model",
    "whole",
    "heap",
    "additional",
    "time",
    "steps",
    "ended",
    "training",
    "breakout",
    "model",
    "specifically",
    "a2c",
    "algorithm",
    "around",
    "two",
    "million",
    "time",
    "steps",
    "evaluated",
    "model",
    "looked",
    "like",
    "getting",
    "around",
    "average",
    "score",
    "21",
    "still",
    "way",
    "better",
    "random",
    "model",
    "still",
    "better",
    "100",
    "000",
    "time",
    "step",
    "model",
    "start",
    "see",
    "impact",
    "trading",
    "longer",
    "also",
    "make",
    "model",
    "available",
    "inside",
    "github",
    "repository",
    "want",
    "test",
    "start",
    "see",
    "actually",
    "looks",
    "like",
    "model",
    "name",
    "a2c",
    "underscore",
    "2m",
    "model",
    "a2c",
    "trained",
    "2",
    "million",
    "steps",
    "per",
    "usual",
    "load",
    "environment",
    "load",
    "a2c",
    "algorithm",
    "create",
    "environment",
    "single",
    "frame",
    "particular",
    "time",
    "rather",
    "evaluating",
    "10",
    "time",
    "steps",
    "go",
    "ahead",
    "evaluate",
    "run",
    "leave",
    "start",
    "see",
    "boost",
    "performance",
    "come",
    "back",
    "end",
    "take",
    "look",
    "end",
    "score",
    "going",
    "run",
    "50",
    "evaluation",
    "episodes",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "music",
    "already",
    "start",
    "see",
    "performing",
    "way",
    "better",
    "clearing",
    "tens",
    "clearing",
    "20s",
    "every",
    "ultimate",
    "score",
    "hitting",
    "way",
    "better",
    "previous",
    "models",
    "leave",
    "enjoy",
    "performance",
    "take",
    "look",
    "actually",
    "running",
    "uh",
    "music",
    "uh",
    "music",
    "okay",
    "50",
    "episodes",
    "done",
    "looked",
    "like",
    "actually",
    "cleared",
    "50",
    "around",
    "halfway",
    "significantly",
    "better",
    "performance",
    "two",
    "models",
    "actually",
    "take",
    "look",
    "scores",
    "see",
    "mean",
    "reward",
    "50",
    "episodes",
    "way",
    "better",
    "models",
    "taking",
    "look",
    "overall",
    "standard",
    "deviation",
    "way",
    "better",
    "taking",
    "look",
    "ideally",
    "begins",
    "show",
    "possible",
    "go",
    "train",
    "model",
    "little",
    "bit",
    "longer",
    "next",
    "project",
    "project",
    "one",
    "done",
    "project",
    "two",
    "reinforcement",
    "learning",
    "autonomous",
    "driving",
    "particular",
    "environment",
    "going",
    "using",
    "racing",
    "car",
    "environment",
    "trying",
    "get",
    "car",
    "drive",
    "around",
    "randomly",
    "generated",
    "race",
    "track",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "one",
    "still",
    "five",
    "steps",
    "going",
    "going",
    "case",
    "slight",
    "bit",
    "different",
    "terms",
    "going",
    "set",
    "first",
    "thing",
    "note",
    "order",
    "leverage",
    "racing",
    "car",
    "environment",
    "need",
    "install",
    "swig",
    "order",
    "take",
    "look",
    "installing",
    "squig",
    "going",
    "vary",
    "depending",
    "whether",
    "installing",
    "windows",
    "machine",
    "linux",
    "machine",
    "windows",
    "believe",
    "need",
    "download",
    "swig",
    "file",
    "extract",
    "add",
    "path",
    "mac",
    "believe",
    "need",
    "use",
    "homebrew",
    "install",
    "let",
    "take",
    "look",
    "yep",
    "actually",
    "use",
    "homebrew",
    "brew",
    "install",
    "swig",
    "way",
    "easier",
    "mac",
    "cool",
    "got",
    "swig",
    "installed",
    "windows",
    "download",
    "extract",
    "add",
    "path",
    "good",
    "go",
    "mac",
    "got",
    "use",
    "brew",
    "install",
    "need",
    "hand",
    "hit",
    "comments",
    "going",
    "install",
    "two",
    "new",
    "dependencies",
    "going",
    "need",
    "box",
    "2d",
    "environment",
    "also",
    "going",
    "need",
    "piglet",
    "let",
    "go",
    "ahead",
    "install",
    "typed",
    "jim",
    "wrong",
    "jim",
    "okay",
    "good",
    "gone",
    "written",
    "exclamation",
    "mark",
    "pip",
    "install",
    "gym",
    "inside",
    "square",
    "brackets",
    "box",
    "2d",
    "using",
    "racing",
    "car",
    "environment",
    "need",
    "box",
    "2d",
    "installed",
    "racing",
    "car",
    "environment",
    "built",
    "top",
    "got",
    "installed",
    "good",
    "go",
    "also",
    "installing",
    "piglet",
    "dependency",
    "particular",
    "environment",
    "done",
    "need",
    "import",
    "dependencies",
    "let",
    "go",
    "ahead",
    "alrighty",
    "gone",
    "written",
    "five",
    "lines",
    "code",
    "first",
    "line",
    "importing",
    "open",
    "ai",
    "gym",
    "per",
    "usual",
    "import",
    "gym",
    "second",
    "one",
    "stable",
    "underscore",
    "baselines",
    "three",
    "import",
    "ppo",
    "next",
    "one",
    "stable",
    "underscore",
    "baselines",
    "three",
    "dot",
    "common",
    "dot",
    "vec",
    "underscore",
    "env",
    "import",
    "dummy",
    "vec",
    "env",
    "really",
    "similar",
    "main",
    "tutorial",
    "going",
    "wrapping",
    "environment",
    "exactly",
    "next",
    "line",
    "importing",
    "evaluate",
    "policy",
    "function",
    "last",
    "least",
    "importing",
    "os",
    "alrighty",
    "next",
    "thing",
    "going",
    "test",
    "environment",
    "per",
    "usual",
    "let",
    "go",
    "ahead",
    "okay",
    "environment",
    "created",
    "warning",
    "sort",
    "ignore",
    "take",
    "look",
    "environment",
    "emv",
    "dot",
    "reset",
    "see",
    "generating",
    "track",
    "talk",
    "little",
    "bit",
    "take",
    "look",
    "per",
    "usual",
    "see",
    "going",
    "box",
    "got",
    "three",
    "different",
    "values",
    "minus",
    "one",
    "one",
    "take",
    "look",
    "observation",
    "space",
    "see",
    "going",
    "box",
    "going",
    "values",
    "0",
    "255",
    "looks",
    "like",
    "going",
    "image",
    "96",
    "96",
    "means",
    "going",
    "image",
    "able",
    "go",
    "ahead",
    "train",
    "racing",
    "environment",
    "type",
    "take",
    "look",
    "environment",
    "see",
    "popping",
    "let",
    "bring",
    "sort",
    "racetrack",
    "looks",
    "like",
    "see",
    "got",
    "entire",
    "racetrack",
    "actually",
    "going",
    "test",
    "type",
    "probably",
    "talk",
    "little",
    "bit",
    "allows",
    "render",
    "actual",
    "environment",
    "working",
    "optional",
    "thing",
    "need",
    "render",
    "slow",
    "training",
    "rendering",
    "training",
    "gives",
    "ability",
    "see",
    "agent",
    "action",
    "type",
    "close",
    "close",
    "environment",
    "old",
    "one",
    "cool",
    "going",
    "going",
    "go",
    "ahead",
    "test",
    "environment",
    "copy",
    "breakout",
    "tutorial",
    "copy",
    "testing",
    "code",
    "bring",
    "almost",
    "identical",
    "right",
    "uncomment",
    "go",
    "ahead",
    "run",
    "going",
    "test",
    "environment",
    "see",
    "actually",
    "trying",
    "drive",
    "car",
    "along",
    "racetrack",
    "ideally",
    "going",
    "get",
    "points",
    "longer",
    "stays",
    "inside",
    "track",
    "turns",
    "taking",
    "random",
    "actions",
    "actually",
    "know",
    "track",
    "moment",
    "going",
    "go",
    "straight",
    "see",
    "making",
    "lot",
    "movement",
    "kind",
    "performing",
    "okay",
    "able",
    "take",
    "turn",
    "first",
    "time",
    "gets",
    "turn",
    "failing",
    "right",
    "right",
    "need",
    "watch",
    "sort",
    "get",
    "idea",
    "goal",
    "get",
    "car",
    "go",
    "around",
    "racetrack",
    "actually",
    "stop",
    "hit",
    "close",
    "sort",
    "cleans",
    "fine",
    "leave",
    "one",
    "open",
    "going",
    "go",
    "ahead",
    "train",
    "model",
    "sort",
    "process",
    "time",
    "going",
    "using",
    "ppo",
    "algorithm",
    "rather",
    "using",
    "algorithm",
    "going",
    "use",
    "slightly",
    "different",
    "one",
    "let",
    "go",
    "ahead",
    "oh",
    "rather",
    "using",
    "a2c",
    "like",
    "breakout",
    "going",
    "use",
    "ppo",
    "let",
    "go",
    "ahead",
    "train",
    "model",
    "also",
    "take",
    "look",
    "different",
    "actions",
    "take",
    "look",
    "uh",
    "mv",
    "look",
    "like",
    "got",
    "let",
    "take",
    "look",
    "pass",
    "car",
    "racing",
    "environment",
    "open",
    "ai",
    "gym",
    "let",
    "take",
    "look",
    "actually",
    "got",
    "documentation",
    "different",
    "actions",
    "look",
    "like",
    "sometimes",
    "going",
    "get",
    "better",
    "better",
    "explanations",
    "actually",
    "happening",
    "different",
    "environments",
    "looks",
    "like",
    "got",
    "something",
    "raw",
    "actually",
    "useful",
    "reward",
    "minus",
    "let",
    "make",
    "bit",
    "bigger",
    "reward",
    "minus",
    "zero",
    "negative",
    "point",
    "one",
    "every",
    "frame",
    "plus",
    "one",
    "thousand",
    "divided",
    "n",
    "every",
    "tractile",
    "visited",
    "means",
    "every",
    "track",
    "tile",
    "visited",
    "going",
    "get",
    "plus",
    "1000",
    "divided",
    "n",
    "n",
    "number",
    "total",
    "number",
    "tiles",
    "says",
    "slightly",
    "confusing",
    "complicated",
    "reward",
    "function",
    "sort",
    "get",
    "idea",
    "get",
    "rewards",
    "get",
    "points",
    "able",
    "go",
    "every",
    "frame",
    "long",
    "tiles",
    "game",
    "solved",
    "agent",
    "consistently",
    "gets",
    "900",
    "plus",
    "points",
    "going",
    "take",
    "time",
    "able",
    "get",
    "point",
    "um",
    "remember",
    "powerful",
    "rear",
    "wheel",
    "drive",
    "car",
    "press",
    "accelerator",
    "indicators",
    "shown",
    "bottom",
    "got",
    "true",
    "speed",
    "abs",
    "sensors",
    "steering",
    "wheel",
    "position",
    "gyroscope",
    "pretty",
    "cool",
    "got",
    "whole",
    "bunch",
    "information",
    "see",
    "one",
    "finite",
    "set",
    "reward",
    "statements",
    "dictate",
    "whether",
    "solved",
    "ideally",
    "want",
    "get",
    "900",
    "points",
    "going",
    "take",
    "long",
    "time",
    "trained",
    "438",
    "000",
    "steps",
    "think",
    "getting",
    "realm",
    "50",
    "40",
    "sort",
    "points",
    "take",
    "solve",
    "going",
    "case",
    "going",
    "try",
    "solve",
    "see",
    "go",
    "let",
    "going",
    "go",
    "train",
    "model",
    "going",
    "instantiate",
    "environment",
    "going",
    "go",
    "ahead",
    "train",
    "okay",
    "environment",
    "set",
    "gotten",
    "written",
    "e",
    "env",
    "equals",
    "jim",
    "dot",
    "make",
    "environment",
    "name",
    "wrapping",
    "inside",
    "dummy",
    "vectorize",
    "environment",
    "wrapper",
    "actually",
    "going",
    "vectorize",
    "one",
    "pretty",
    "similar",
    "main",
    "tutorial",
    "go",
    "set",
    "agent",
    "model",
    "let",
    "okay",
    "model",
    "set",
    "gone",
    "specified",
    "logging",
    "path",
    "going",
    "log",
    "tensorboard",
    "logs",
    "gone",
    "written",
    "specified",
    "training",
    "specified",
    "logs",
    "going",
    "directory",
    "actually",
    "gone",
    "specified",
    "agent",
    "model",
    "equals",
    "ppo",
    "going",
    "using",
    "ppo",
    "algorithm",
    "pass",
    "cnn",
    "policy",
    "pass",
    "environment",
    "pass",
    "verbose",
    "equals",
    "one",
    "specified",
    "tensorboard",
    "log",
    "path",
    "going",
    "train",
    "going",
    "train",
    "100",
    "000",
    "steps",
    "might",
    "want",
    "train",
    "whole",
    "heap",
    "longer",
    "want",
    "try",
    "hit",
    "900",
    "score",
    "hit",
    "900",
    "score",
    "let",
    "know",
    "comments",
    "share",
    "twitter",
    "linkedin",
    "love",
    "see",
    "case",
    "going",
    "go",
    "ahead",
    "train",
    "model",
    "hundred",
    "thousand",
    "steps",
    "let",
    "go",
    "ahead",
    "train",
    "model",
    "gon",
    "na",
    "write",
    "model",
    "going",
    "start",
    "see",
    "repeatable",
    "process",
    "instantiate",
    "environment",
    "create",
    "environment",
    "vectorize",
    "need",
    "set",
    "model",
    "go",
    "ahead",
    "train",
    "case",
    "going",
    "go",
    "ahead",
    "train",
    "let",
    "go",
    "ahead",
    "kick",
    "right",
    "back",
    "let",
    "wait",
    "training",
    "kick",
    "successfully",
    "go",
    "see",
    "starting",
    "get",
    "output",
    "algorithm",
    "let",
    "train",
    "hundred",
    "thousand",
    "steps",
    "right",
    "back",
    "okay",
    "racing",
    "car",
    "train",
    "gone",
    "trained",
    "hundred",
    "thousand",
    "steps",
    "100",
    "352",
    "exact",
    "per",
    "usual",
    "going",
    "go",
    "ahead",
    "save",
    "model",
    "test",
    "let",
    "okay",
    "model",
    "saved",
    "gone",
    "written",
    "ppo",
    "underscore",
    "path",
    "set",
    "path",
    "variable",
    "order",
    "written",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "specified",
    "want",
    "training",
    "folder",
    "saved",
    "underscore",
    "mod",
    "saved",
    "models",
    "folder",
    "gon",
    "na",
    "name",
    "ppo",
    "driving",
    "model",
    "used",
    "able",
    "go",
    "save",
    "take",
    "look",
    "got",
    "model",
    "ppo",
    "underscore",
    "driving",
    "underscore",
    "model",
    "also",
    "got",
    "another",
    "model",
    "trained",
    "428",
    "000",
    "steps",
    "take",
    "look",
    "one",
    "well",
    "let",
    "go",
    "ahead",
    "delete",
    "model",
    "per",
    "usual",
    "double",
    "check",
    "works",
    "let",
    "load",
    "back",
    "model",
    "equals",
    "ppo",
    "dot",
    "load",
    "going",
    "pass",
    "ppo",
    "path",
    "environment",
    "cool",
    "good",
    "go",
    "ahead",
    "evaluate",
    "per",
    "usual",
    "let",
    "go",
    "ahead",
    "going",
    "pass",
    "evaluate",
    "underscore",
    "policy",
    "going",
    "pass",
    "model",
    "environment",
    "number",
    "steps",
    "going",
    "10",
    "steps",
    "going",
    "pass",
    "render",
    "equals",
    "true",
    "looks",
    "like",
    "car",
    "ripping",
    "around",
    "track",
    "bunch",
    "donuts",
    "key",
    "thing",
    "note",
    "car",
    "high",
    "powered",
    "always",
    "see",
    "going",
    "around",
    "corner",
    "bit",
    "trouble",
    "sort",
    "getting",
    "great",
    "see",
    "car",
    "high",
    "powered",
    "starts",
    "lack",
    "traction",
    "case",
    "get",
    "stuck",
    "loop",
    "whereas",
    "instead",
    "actually",
    "driving",
    "forward",
    "goes",
    "donuts",
    "okay",
    "sort",
    "gave",
    "got",
    "around",
    "first",
    "corner",
    "spinned",
    "okay",
    "think",
    "gon",
    "na",
    "keep",
    "right",
    "sort",
    "hundred",
    "thousand",
    "steps",
    "gets",
    "exactly",
    "best",
    "uh",
    "racing",
    "car",
    "driver",
    "kind",
    "right",
    "cool",
    "let",
    "stop",
    "clearly",
    "uh",
    "driving",
    "saying",
    "going",
    "place",
    "gon",
    "na",
    "type",
    "env",
    "dot",
    "close",
    "right",
    "closed",
    "got",
    "one",
    "open",
    "rather",
    "using",
    "one",
    "let",
    "go",
    "ahead",
    "load",
    "one",
    "trained",
    "think",
    "438",
    "000",
    "steps",
    "428",
    "let",
    "go",
    "ahead",
    "load",
    "model",
    "make",
    "model",
    "available",
    "github",
    "repo",
    "going",
    "change",
    "name",
    "ppo",
    "path",
    "load",
    "one",
    "let",
    "go",
    "ahead",
    "test",
    "model",
    "see",
    "lot",
    "slower",
    "least",
    "sort",
    "sticking",
    "track",
    "cut",
    "corner",
    "fine",
    "back",
    "see",
    "getting",
    "score",
    "getting",
    "much",
    "higher",
    "190",
    "ideally",
    "want",
    "able",
    "get",
    "900",
    "means",
    "going",
    "accelerate",
    "turn",
    "obviously",
    "trained",
    "438",
    "000",
    "steps",
    "actually",
    "train",
    "lot",
    "longer",
    "get",
    "car",
    "actually",
    "rips",
    "straights",
    "takes",
    "corners",
    "appropriately",
    "case",
    "starting",
    "sort",
    "little",
    "bit",
    "hesitant",
    "throttle",
    "see",
    "going",
    "maybe",
    "going",
    "fast",
    "could",
    "go",
    "accelerated",
    "back",
    "track",
    "go",
    "see",
    "obviously",
    "way",
    "better",
    "one",
    "trained",
    "100",
    "000",
    "steps",
    "sort",
    "shows",
    "difference",
    "training",
    "lot",
    "longer",
    "nothing",
    "different",
    "apart",
    "trained",
    "longer",
    "period",
    "time",
    "training",
    "reinforcement",
    "learning",
    "agents",
    "training",
    "longer",
    "period",
    "time",
    "obviously",
    "help",
    "lot",
    "ideally",
    "produce",
    "much",
    "better",
    "model",
    "ideally",
    "looking",
    "something",
    "realm",
    "like",
    "million",
    "two",
    "million",
    "steps",
    "able",
    "get",
    "something",
    "great",
    "someone",
    "time",
    "run",
    "long",
    "means",
    "let",
    "know",
    "like",
    "see",
    "hit",
    "comments",
    "love",
    "take",
    "look",
    "car",
    "sort",
    "done",
    "little",
    "bit",
    "glitchy",
    "throttle",
    "sort",
    "get",
    "idea",
    "go",
    "ahead",
    "close",
    "stop",
    "environment",
    "run",
    "close",
    "remember",
    "main",
    "tutorial",
    "also",
    "went",
    "ability",
    "test",
    "like",
    "rather",
    "going",
    "using",
    "evaluate",
    "policy",
    "could",
    "also",
    "well",
    "copy",
    "step",
    "6",
    "main",
    "tutorial",
    "could",
    "actually",
    "plug",
    "case",
    "got",
    "environment",
    "good",
    "model",
    "good",
    "could",
    "actually",
    "test",
    "let",
    "run",
    "go",
    "see",
    "rather",
    "using",
    "evaluate",
    "policy",
    "using",
    "know",
    "call",
    "flow",
    "able",
    "go",
    "train",
    "see",
    "car",
    "getting",
    "around",
    "turn",
    "250",
    "god",
    "know",
    "watch",
    "many",
    "times",
    "get",
    "little",
    "bit",
    "glitchy",
    "going",
    "around",
    "corners",
    "moving",
    "around",
    "keep",
    "mind",
    "trained",
    "image",
    "right",
    "like",
    "additional",
    "information",
    "image",
    "actually",
    "coming",
    "actually",
    "pretty",
    "cool",
    "right",
    "let",
    "get",
    "oh",
    "actually",
    "took",
    "corner",
    "pretty",
    "cool",
    "come",
    "bad",
    "max",
    "score",
    "consider",
    "solved",
    "900",
    "ideally",
    "want",
    "train",
    "lot",
    "longer",
    "able",
    "get",
    "performing",
    "way",
    "better",
    "actually",
    "see",
    "scores",
    "logged",
    "255",
    "181",
    "276",
    "got",
    "obviously",
    "way",
    "better",
    "random",
    "agent",
    "getting",
    "like",
    "negative",
    "values",
    "also",
    "noticed",
    "v",
    "far",
    "track",
    "able",
    "see",
    "track",
    "anymore",
    "sort",
    "gets",
    "stuck",
    "stops",
    "438k",
    "model",
    "could",
    "train",
    "longer",
    "get",
    "better",
    "results",
    "sort",
    "gives",
    "idea",
    "leverage",
    "reinforcement",
    "learning",
    "autonomous",
    "driving",
    "case",
    "racing",
    "hey",
    "guys",
    "editing",
    "nick",
    "also",
    "ended",
    "training",
    "tutorial",
    "whole",
    "heap",
    "steps",
    "trained",
    "model",
    "two",
    "million",
    "steps",
    "significantly",
    "improved",
    "performance",
    "particular",
    "model",
    "actual",
    "tutorial",
    "got",
    "around",
    "range",
    "200",
    "300",
    "terms",
    "reward",
    "estimate",
    "went",
    "trained",
    "whole",
    "heap",
    "longer",
    "heading",
    "towards",
    "range",
    "around",
    "700",
    "quite",
    "completely",
    "solved",
    "based",
    "environment",
    "metrics",
    "ideally",
    "see",
    "performing",
    "whole",
    "heap",
    "better",
    "looked",
    "like",
    "saying",
    "ended",
    "training",
    "model",
    "whole",
    "heap",
    "longer",
    "ended",
    "training",
    "two",
    "million",
    "additional",
    "steps",
    "reason",
    "wanted",
    "show",
    "see",
    "impact",
    "training",
    "longer",
    "obviously",
    "one",
    "technique",
    "go",
    "leveraging",
    "order",
    "improve",
    "performance",
    "models",
    "two",
    "million",
    "steps",
    "make",
    "model",
    "available",
    "github",
    "repository",
    "pick",
    "class",
    "minds",
    "back",
    "three",
    "different",
    "models",
    "trained",
    "first",
    "model",
    "burnouts",
    "really",
    "making",
    "past",
    "first",
    "corner",
    "second",
    "model",
    "super",
    "jittery",
    "got",
    "model",
    "well",
    "make",
    "one",
    "available",
    "order",
    "load",
    "one",
    "needed",
    "similar",
    "previous",
    "models",
    "load",
    "using",
    "ppo",
    "path",
    "go",
    "run",
    "model",
    "see",
    "model",
    "performs",
    "whole",
    "heap",
    "better",
    "previous",
    "models",
    "still",
    "spin",
    "corner",
    "every",
    "getting",
    "lot",
    "scoring",
    "lot",
    "higher",
    "models",
    "train",
    "let",
    "go",
    "ahead",
    "take",
    "look",
    "one",
    "music",
    "see",
    "got",
    "800",
    "bad",
    "every",
    "gon",
    "na",
    "lose",
    "focus",
    "sort",
    "veer",
    "track",
    "see",
    "performing",
    "whole",
    "heap",
    "better",
    "models",
    "trained",
    "spin",
    "works",
    "way",
    "back",
    "track",
    "eventually",
    "starts",
    "taking",
    "corners",
    "pretty",
    "well",
    "every",
    "see",
    "one",
    "performs",
    "well",
    "see",
    "performing",
    "significantly",
    "better",
    "getting",
    "700",
    "range",
    "go",
    "another",
    "770",
    "score",
    "see",
    "bottom",
    "evaluating",
    "model",
    "performance",
    "looking",
    "like",
    "bring",
    "little",
    "bit",
    "music",
    "open",
    "look",
    "like",
    "printing",
    "let",
    "let",
    "10",
    "episodes",
    "run",
    "eventually",
    "see",
    "total",
    "score",
    "automated",
    "sum",
    "results",
    "quiet",
    "let",
    "enjoy",
    "music",
    "music",
    "music",
    "10",
    "episodes",
    "complete",
    "see",
    "significant",
    "boost",
    "terms",
    "performance",
    "simply",
    "training",
    "longer",
    "actually",
    "take",
    "look",
    "results",
    "evaluate",
    "policy",
    "see",
    "final",
    "score",
    "average",
    "score",
    "10",
    "episodes",
    "741",
    "quite",
    "hitting",
    "golden",
    "900",
    "mark",
    "still",
    "way",
    "better",
    "previous",
    "models",
    "also",
    "standard",
    "deviation",
    "123",
    "reasonably",
    "high",
    "standard",
    "deviation",
    "particular",
    "case",
    "sort",
    "shows",
    "possible",
    "ideally",
    "go",
    "tune",
    "model",
    "train",
    "little",
    "bit",
    "longer",
    "next",
    "project",
    "note",
    "project",
    "two",
    "done",
    "last",
    "project",
    "going",
    "taking",
    "look",
    "reinforcement",
    "learning",
    "custom",
    "environments",
    "watched",
    "shower",
    "environment",
    "shower",
    "custom",
    "environment",
    "tutorial",
    "going",
    "environment",
    "going",
    "using",
    "stable",
    "bass",
    "lines",
    "algorithms",
    "able",
    "solve",
    "without",
    "ado",
    "let",
    "kick",
    "project",
    "three",
    "notebooks",
    "going",
    "available",
    "inside",
    "github",
    "repository",
    "want",
    "pick",
    "means",
    "grab",
    "let",
    "know",
    "go",
    "get",
    "stuck",
    "please",
    "reach",
    "happy",
    "help",
    "let",
    "go",
    "ahead",
    "bunch",
    "dependencies",
    "going",
    "importing",
    "namely",
    "going",
    "defining",
    "environment",
    "case",
    "let",
    "go",
    "ahead",
    "import",
    "dependencies",
    "take",
    "step",
    "back",
    "take",
    "look",
    "okay",
    "gone",
    "written",
    "nine",
    "lines",
    "code",
    "quite",
    "fair",
    "bit",
    "case",
    "gone",
    "written",
    "broken",
    "three",
    "specific",
    "sections",
    "gym",
    "dependencies",
    "open",
    "ai",
    "gym",
    "dependencies",
    "helpers",
    "going",
    "need",
    "got",
    "stable",
    "baseline",
    "stuff",
    "first",
    "jim",
    "importing",
    "gym",
    "package",
    "time",
    "written",
    "import",
    "gym",
    "going",
    "give",
    "us",
    "pretty",
    "standard",
    "import",
    "importing",
    "gym",
    "environment",
    "class",
    "written",
    "gym",
    "import",
    "env",
    "going",
    "super",
    "class",
    "going",
    "able",
    "use",
    "build",
    "environment",
    "written",
    "gym",
    "dot",
    "spaces",
    "import",
    "discrete",
    "box",
    "dict",
    "tuple",
    "multi",
    "binary",
    "one",
    "represents",
    "different",
    "types",
    "spaces",
    "available",
    "inside",
    "openai",
    "gym",
    "wanted",
    "sort",
    "show",
    "one",
    "different",
    "types",
    "spaces",
    "looks",
    "like",
    "actually",
    "use",
    "probably",
    "use",
    "two",
    "common",
    "ones",
    "discrete",
    "box",
    "environment",
    "wanted",
    "give",
    "idea",
    "fit",
    "together",
    "gone",
    "brought",
    "helpers",
    "imported",
    "numpy",
    "import",
    "numpy",
    "np",
    "imported",
    "random",
    "import",
    "random",
    "imported",
    "operating",
    "system",
    "import",
    "os",
    "gone",
    "imported",
    "standard",
    "stable",
    "baseline",
    "stuff",
    "stable",
    "underscore",
    "baselines",
    "three",
    "import",
    "ppo",
    "stable",
    "underscore",
    "baselines",
    "three",
    "import",
    "common",
    "dot",
    "vec",
    "underscore",
    "emv",
    "import",
    "dummy",
    "vec",
    "nv",
    "pretty",
    "standard",
    "imported",
    "evaluate",
    "policy",
    "function",
    "really",
    "really",
    "common",
    "pretty",
    "common",
    "jim",
    "pretty",
    "common",
    "new",
    "stuff",
    "couple",
    "lines",
    "let",
    "go",
    "look",
    "different",
    "types",
    "spaces",
    "got",
    "bunch",
    "imported",
    "let",
    "take",
    "look",
    "one",
    "first",
    "discrete",
    "dive",
    "discrete",
    "say",
    "wanted",
    "three",
    "different",
    "actions",
    "going",
    "give",
    "us",
    "discrete",
    "space",
    "actually",
    "sample",
    "take",
    "look",
    "different",
    "types",
    "values",
    "0",
    "1",
    "get",
    "see",
    "going",
    "give",
    "value",
    "0",
    "1",
    "2",
    "passing",
    "discrete",
    "equals",
    "action",
    "action",
    "mapped",
    "one",
    "actions",
    "mapped",
    "specific",
    "value",
    "0",
    "1",
    "2",
    "use",
    "got",
    "box",
    "box",
    "space",
    "box",
    "pass",
    "zero",
    "comma",
    "one",
    "low",
    "value",
    "upper",
    "value",
    "shape",
    "output",
    "going",
    "get",
    "going",
    "get",
    "array",
    "three",
    "three",
    "ideally",
    "list",
    "lists",
    "take",
    "look",
    "sampling",
    "got",
    "array",
    "inside",
    "array",
    "got",
    "three",
    "individual",
    "rays",
    "arrays",
    "three",
    "values",
    "exactly",
    "formatting",
    "got",
    "values",
    "zero",
    "one",
    "might",
    "use",
    "trying",
    "look",
    "different",
    "types",
    "sensors",
    "continuous",
    "variables",
    "use",
    "box",
    "wanted",
    "three",
    "values",
    "could",
    "going",
    "give",
    "three",
    "values",
    "well",
    "done",
    "reduced",
    "array",
    "three",
    "values",
    "got",
    "tuple",
    "moment",
    "stable",
    "baselines",
    "support",
    "tuple",
    "wanted",
    "use",
    "could",
    "still",
    "take",
    "look",
    "pass",
    "discrete",
    "environment",
    "discrete",
    "space",
    "really",
    "tuple",
    "space",
    "allows",
    "combine",
    "different",
    "spaces",
    "see",
    "tuple",
    "combined",
    "discrete",
    "environment",
    "discrete",
    "box",
    "space",
    "sample",
    "see",
    "getting",
    "discrete",
    "value",
    "first",
    "getting",
    "box",
    "second",
    "okay",
    "next",
    "one",
    "far",
    "done",
    "discrete",
    "box",
    "tuple",
    "next",
    "one",
    "want",
    "take",
    "look",
    "dict",
    "really",
    "similar",
    "tuple",
    "difference",
    "rather",
    "passing",
    "tuple",
    "pass",
    "dictionary",
    "let",
    "okay",
    "dict",
    "space",
    "written",
    "dict",
    "open",
    "braces",
    "actually",
    "passed",
    "dictionary",
    "dictionary",
    "two",
    "keys",
    "height",
    "height",
    "equal",
    "discrete",
    "two",
    "really",
    "different",
    "typing",
    "discrete",
    "two",
    "created",
    "another",
    "key",
    "speed",
    "set",
    "equal",
    "box",
    "two",
    "box",
    "remember",
    "going",
    "pass",
    "three",
    "key",
    "arguments",
    "going",
    "pass",
    "lower",
    "value",
    "upper",
    "value",
    "shape",
    "want",
    "case",
    "specified",
    "shape",
    "1",
    "comma",
    "means",
    "going",
    "get",
    "single",
    "value",
    "back",
    "values",
    "0",
    "actually",
    "go",
    "sample",
    "see",
    "got",
    "height",
    "key",
    "represented",
    "0",
    "remember",
    "going",
    "0",
    "1",
    "2",
    "also",
    "got",
    "speed",
    "case",
    "value",
    "0",
    "100",
    "pretty",
    "cool",
    "right",
    "gives",
    "us",
    "dict",
    "space",
    "next",
    "space",
    "want",
    "take",
    "look",
    "order",
    "create",
    "space",
    "written",
    "passed",
    "number",
    "positions",
    "want",
    "space",
    "4",
    "going",
    "give",
    "us",
    "4",
    "positions",
    "go",
    "sample",
    "case",
    "see",
    "got",
    "0",
    "1",
    "2",
    "3",
    "4",
    "positions",
    "case",
    "going",
    "binary",
    "set",
    "values",
    "either",
    "zeros",
    "one",
    "go",
    "sample",
    "multiple",
    "times",
    "see",
    "different",
    "combinations",
    "zeros",
    "ones",
    "four",
    "positions",
    "cool",
    "last",
    "type",
    "space",
    "want",
    "take",
    "look",
    "going",
    "pretty",
    "similar",
    "except",
    "rather",
    "binary",
    "values",
    "going",
    "discrete",
    "values",
    "value",
    "want",
    "let",
    "go",
    "ahead",
    "okay",
    "space",
    "written",
    "two",
    "passed",
    "list",
    "values",
    "passed",
    "five",
    "two",
    "two",
    "go",
    "hit",
    "sample",
    "going",
    "get",
    "three",
    "different",
    "values",
    "values",
    "going",
    "vary",
    "depending",
    "parameters",
    "passed",
    "list",
    "passed",
    "5",
    "first",
    "value",
    "going",
    "vary",
    "0",
    "4",
    "pass",
    "2",
    "going",
    "vary",
    "0",
    "two",
    "actually",
    "going",
    "let",
    "actually",
    "take",
    "look",
    "believe",
    "max",
    "going",
    "get",
    "yeah",
    "yeah",
    "going",
    "zero",
    "four",
    "zero",
    "one",
    "zero",
    "one",
    "upper",
    "cap",
    "starts",
    "zero",
    "cool",
    "keep",
    "going",
    "sort",
    "see",
    "happens",
    "go",
    "pass",
    "another",
    "value",
    "going",
    "get",
    "another",
    "discrete",
    "value",
    "appended",
    "onto",
    "end",
    "really",
    "summary",
    "different",
    "types",
    "spaces",
    "got",
    "available",
    "inside",
    "open",
    "ai",
    "gym",
    "got",
    "discrete",
    "space",
    "box",
    "space",
    "tuple",
    "dict",
    "discrete",
    "discrete",
    "number",
    "actions",
    "mapped",
    "single",
    "integer",
    "box",
    "gives",
    "continuous",
    "variables",
    "remember",
    "box",
    "pass",
    "lower",
    "value",
    "upper",
    "value",
    "shape",
    "box",
    "actually",
    "want",
    "tuple",
    "allows",
    "combine",
    "different",
    "types",
    "spaces",
    "together",
    "tuple",
    "currently",
    "supported",
    "stable",
    "baseline",
    "something",
    "keep",
    "mind",
    "remember",
    "tuple",
    "pass",
    "set",
    "braces",
    "two",
    "different",
    "types",
    "whatever",
    "types",
    "spaces",
    "want",
    "passed",
    "discrete",
    "box",
    "well",
    "inside",
    "braces",
    "got",
    "dick",
    "space",
    "dick",
    "pass",
    "dictionary",
    "different",
    "types",
    "spaces",
    "got",
    "discrete",
    "also",
    "got",
    "box",
    "space",
    "got",
    "space",
    "got",
    "keep",
    "mind",
    "could",
    "actually",
    "grab",
    "space",
    "add",
    "tuple",
    "well",
    "gone",
    "added",
    "another",
    "type",
    "space",
    "tuple",
    "could",
    "dict",
    "say",
    "example",
    "could",
    "call",
    "um",
    "color",
    "know",
    "add",
    "multiple",
    "versions",
    "dick",
    "sort",
    "like",
    "grouping",
    "spaces",
    "right",
    "got",
    "pass",
    "number",
    "positions",
    "want",
    "binary",
    "space",
    "also",
    "got",
    "gives",
    "bunch",
    "different",
    "discrete",
    "types",
    "values",
    "lot",
    "documentation",
    "figured",
    "little",
    "bit",
    "crash",
    "course",
    "like",
    "see",
    "means",
    "hit",
    "comments",
    "going",
    "building",
    "environment",
    "goals",
    "environment",
    "basically",
    "build",
    "agent",
    "give",
    "us",
    "best",
    "shower",
    "possible",
    "going",
    "happen",
    "randomly",
    "temperature",
    "going",
    "fluctuate",
    "people",
    "building",
    "going",
    "randomly",
    "go",
    "know",
    "optimal",
    "temperature",
    "37",
    "39",
    "degrees",
    "want",
    "able",
    "train",
    "agent",
    "automatically",
    "respond",
    "changes",
    "temperature",
    "get",
    "within",
    "37",
    "39",
    "degrees",
    "range",
    "keep",
    "mind",
    "agent",
    "actually",
    "know",
    "prefer",
    "temperature",
    "within",
    "37",
    "39",
    "degrees",
    "going",
    "need",
    "learn",
    "types",
    "adjustments",
    "make",
    "get",
    "within",
    "range",
    "something",
    "keep",
    "mind",
    "remember",
    "simulated",
    "environment",
    "agent",
    "actually",
    "know",
    "accumulates",
    "reward",
    "knows",
    "certain",
    "actions",
    "going",
    "get",
    "reward",
    "know",
    "want",
    "get",
    "37",
    "39",
    "agent",
    "let",
    "go",
    "ahead",
    "build",
    "environment",
    "different",
    "functions",
    "need",
    "implement",
    "environment",
    "get",
    "valid",
    "let",
    "go",
    "ahead",
    "let",
    "build",
    "shell",
    "fill",
    "okay",
    "high",
    "level",
    "shower",
    "environment",
    "obviously",
    "gone",
    "implemented",
    "different",
    "components",
    "four",
    "key",
    "functions",
    "need",
    "inside",
    "shower",
    "environment",
    "class",
    "let",
    "take",
    "look",
    "got",
    "far",
    "gone",
    "read",
    "class",
    "inside",
    "capitals",
    "camel",
    "case",
    "got",
    "shower",
    "env",
    "passing",
    "env",
    "class",
    "gm",
    "environment",
    "colon",
    "got",
    "four",
    "different",
    "functions",
    "gone",
    "implemented",
    "got",
    "init",
    "function",
    "triggers",
    "create",
    "class",
    "step",
    "function",
    "render",
    "function",
    "reset",
    "function",
    "def",
    "underscore",
    "underscore",
    "init",
    "two",
    "passing",
    "self",
    "inside",
    "pair",
    "brackets",
    "colon",
    "right",
    "written",
    "pass",
    "allows",
    "us",
    "define",
    "without",
    "errors",
    "defined",
    "step",
    "function",
    "def",
    "step",
    "pass",
    "self",
    "passing",
    "action",
    "actually",
    "going",
    "pass",
    "environment",
    "remember",
    "use",
    "pass",
    "action",
    "actually",
    "something",
    "gone",
    "defined",
    "render",
    "function",
    "def",
    "render",
    "two",
    "pass",
    "self",
    "colon",
    "pass",
    "going",
    "anything",
    "render",
    "function",
    "actually",
    "part",
    "building",
    "course",
    "actually",
    "built",
    "giant",
    "started",
    "building",
    "giant",
    "pie",
    "game",
    "environment",
    "sort",
    "blowing",
    "proportion",
    "like",
    "see",
    "video",
    "reinforcement",
    "learning",
    "gaming",
    "involves",
    "building",
    "custom",
    "environment",
    "using",
    "pi",
    "game",
    "please",
    "let",
    "know",
    "comments",
    "love",
    "hear",
    "thoughts",
    "last",
    "function",
    "reset",
    "def",
    "reset",
    "going",
    "passing",
    "self",
    "colon",
    "right",
    "got",
    "past",
    "let",
    "go",
    "ahead",
    "initially",
    "set",
    "init",
    "function",
    "keep",
    "going",
    "okay",
    "initialization",
    "function",
    "done",
    "went",
    "wrote",
    "four",
    "lines",
    "code",
    "first",
    "defined",
    "action",
    "space",
    "wrote",
    "self",
    "dot",
    "action",
    "space",
    "set",
    "equal",
    "discrete",
    "three",
    "remember",
    "different",
    "saying",
    "discrete",
    "three",
    "three",
    "actions",
    "going",
    "whether",
    "turn",
    "tap",
    "whether",
    "turn",
    "tap",
    "whether",
    "leave",
    "tap",
    "unchanged",
    "basically",
    "gives",
    "us",
    "three",
    "discrete",
    "actions",
    "could",
    "change",
    "actually",
    "box",
    "type",
    "action",
    "space",
    "actually",
    "turn",
    "tap",
    "certain",
    "amount",
    "certain",
    "number",
    "degrees",
    "case",
    "going",
    "keep",
    "pretty",
    "simple",
    "say",
    "hold",
    "gone",
    "defined",
    "observation",
    "space",
    "written",
    "self",
    "dot",
    "observation",
    "underscore",
    "space",
    "equals",
    "box",
    "set",
    "equal",
    "two",
    "numpy",
    "arrays",
    "case",
    "got",
    "low",
    "value",
    "low",
    "equals",
    "mp",
    "dot",
    "array",
    "pass",
    "zero",
    "gone",
    "specified",
    "high",
    "value",
    "high",
    "equals",
    "passed",
    "number",
    "100",
    "means",
    "observation",
    "space",
    "let",
    "actually",
    "extract",
    "take",
    "look",
    "means",
    "observation",
    "space",
    "going",
    "value",
    "0",
    "100",
    "value",
    "type",
    "dot",
    "sample",
    "see",
    "going",
    "value",
    "get",
    "back",
    "actually",
    "change",
    "make",
    "0",
    "100",
    "pass",
    "shape",
    "equals",
    "1",
    "comma",
    "ideally",
    "give",
    "us",
    "type",
    "output",
    "delete",
    "go",
    "sort",
    "output",
    "type",
    "dot",
    "sample",
    "going",
    "get",
    "type",
    "output",
    "two",
    "different",
    "ways",
    "defining",
    "case",
    "swapped",
    "sort",
    "see",
    "got",
    "multiple",
    "ways",
    "defining",
    "box",
    "space",
    "gone",
    "defined",
    "initial",
    "state",
    "going",
    "set",
    "initial",
    "state",
    "set",
    "equal",
    "38",
    "plus",
    "random",
    "integer",
    "means",
    "shower",
    "going",
    "start",
    "38",
    "degrees",
    "plus",
    "minus",
    "goal",
    "agent",
    "going",
    "get",
    "within",
    "magic",
    "range",
    "37",
    "39",
    "degrees",
    "also",
    "set",
    "another",
    "variable",
    "going",
    "effectively",
    "represent",
    "episode",
    "length",
    "case",
    "shower",
    "link",
    "going",
    "shower",
    "60",
    "seconds",
    "fast",
    "shot",
    "know",
    "gone",
    "written",
    "self",
    "dot",
    "shower",
    "underscore",
    "length",
    "equals",
    "going",
    "inside",
    "step",
    "function",
    "decrease",
    "one",
    "second",
    "every",
    "time",
    "go",
    "take",
    "action",
    "let",
    "go",
    "ahead",
    "define",
    "step",
    "function",
    "okay",
    "gone",
    "filled",
    "step",
    "function",
    "particular",
    "case",
    "got",
    "let",
    "say",
    "one",
    "two",
    "three",
    "four",
    "five",
    "six",
    "six",
    "different",
    "code",
    "blocks",
    "first",
    "one",
    "applying",
    "impact",
    "action",
    "state",
    "remember",
    "three",
    "different",
    "actions",
    "zero",
    "one",
    "two",
    "zero",
    "going",
    "represent",
    "decreasing",
    "temperature",
    "shower",
    "one",
    "degree",
    "one",
    "going",
    "represent",
    "change",
    "two",
    "going",
    "represent",
    "increasing",
    "temperature",
    "shower",
    "one",
    "degree",
    "order",
    "reasonably",
    "simply",
    "written",
    "self",
    "dot",
    "state",
    "plus",
    "equals",
    "action",
    "minus",
    "one",
    "remember",
    "action",
    "going",
    "discrete",
    "three",
    "go",
    "sample",
    "case",
    "got",
    "one",
    "minusing",
    "one",
    "going",
    "get",
    "value",
    "actually",
    "let",
    "actually",
    "print",
    "case",
    "take",
    "action",
    "one",
    "going",
    "akin",
    "leaving",
    "temperature",
    "minus",
    "one",
    "going",
    "apply",
    "zero",
    "change",
    "temperature",
    "going",
    "stay",
    "get",
    "different",
    "value",
    "say",
    "example",
    "get",
    "2",
    "minusing",
    "1",
    "going",
    "increase",
    "temperature",
    "shower",
    "1",
    "degree",
    "get",
    "0",
    "effectively",
    "going",
    "subtracting",
    "one",
    "cool",
    "next",
    "thing",
    "gone",
    "done",
    "decrease",
    "shower",
    "time",
    "every",
    "time",
    "take",
    "step",
    "take",
    "action",
    "going",
    "decrease",
    "length",
    "shower",
    "one",
    "remember",
    "defined",
    "initially",
    "60",
    "seconds",
    "could",
    "change",
    "something",
    "different",
    "wanted",
    "gone",
    "defined",
    "self",
    "dot",
    "shower",
    "underscore",
    "length",
    "minus",
    "equals",
    "one",
    "going",
    "decrease",
    "really",
    "really",
    "important",
    "actually",
    "define",
    "reward",
    "really",
    "complicated",
    "reward",
    "schema",
    "gone",
    "done",
    "written",
    "remember",
    "state",
    "temperature",
    "greater",
    "equal",
    "remember",
    "magic",
    "ratio",
    "got",
    "37",
    "39",
    "degrees",
    "less",
    "39",
    "degrees",
    "reward",
    "one",
    "cases",
    "say",
    "example",
    "shower",
    "completely",
    "range",
    "reward",
    "going",
    "negative",
    "one",
    "could",
    "also",
    "make",
    "reward",
    "zero",
    "well",
    "also",
    "checking",
    "whether",
    "shower",
    "done",
    "episode",
    "done",
    "want",
    "stop",
    "particular",
    "episode",
    "underscore",
    "length",
    "less",
    "equal",
    "zero",
    "done",
    "set",
    "true",
    "let",
    "fix",
    "gone",
    "screwed",
    "else",
    "done",
    "equals",
    "false",
    "fully",
    "consumed",
    "60",
    "seconds",
    "ash",
    "hour",
    "done",
    "creating",
    "blank",
    "info",
    "dictionary",
    "wanted",
    "pass",
    "additional",
    "stuff",
    "could",
    "returning",
    "going",
    "temperature",
    "reward",
    "particular",
    "episode",
    "whether",
    "done",
    "info",
    "returning",
    "bits",
    "information",
    "gone",
    "calculated",
    "step",
    "function",
    "case",
    "render",
    "function",
    "actually",
    "going",
    "anything",
    "could",
    "implement",
    "biz",
    "wanted",
    "going",
    "anything",
    "wanted",
    "definitely",
    "could",
    "last",
    "function",
    "need",
    "implement",
    "reset",
    "function",
    "key",
    "thing",
    "know",
    "wanted",
    "detailed",
    "tutorial",
    "implement",
    "render",
    "pygame",
    "love",
    "something",
    "got",
    "specific",
    "use",
    "case",
    "hit",
    "comments",
    "love",
    "hear",
    "ideas",
    "well",
    "case",
    "let",
    "go",
    "ahead",
    "wrap",
    "environment",
    "reset",
    "function",
    "need",
    "reset",
    "initial",
    "temperature",
    "also",
    "need",
    "reset",
    "shower",
    "time",
    "60",
    "seconds",
    "let",
    "go",
    "ahead",
    "okay",
    "think",
    "environment",
    "done",
    "reset",
    "function",
    "effectively",
    "could",
    "actually",
    "potentially",
    "drop",
    "going",
    "inside",
    "reset",
    "fine",
    "gone",
    "written",
    "equals",
    "np",
    "dot",
    "array",
    "passing",
    "random",
    "initialization",
    "function",
    "inside",
    "set",
    "square",
    "brackets",
    "passed",
    "38",
    "degrees",
    "plus",
    "random",
    "dot",
    "rand",
    "int",
    "could",
    "choose",
    "broader",
    "random",
    "initialization",
    "wanted",
    "case",
    "chosen",
    "three",
    "specified",
    "type",
    "float",
    "remember",
    "box",
    "going",
    "specified",
    "going",
    "integer",
    "could",
    "well",
    "could",
    "specify",
    "types",
    "case",
    "going",
    "leave",
    "float",
    "also",
    "resetting",
    "shower",
    "length",
    "60",
    "self",
    "dot",
    "shower",
    "underscore",
    "length",
    "equals",
    "60",
    "returning",
    "self",
    "dot",
    "state",
    "environment",
    "well",
    "good",
    "actually",
    "test",
    "environment",
    "emv",
    "equals",
    "shower",
    "env",
    "run",
    "inv",
    "dot",
    "observation",
    "space",
    "per",
    "usual",
    "see",
    "getting",
    "box",
    "back",
    "type",
    "dot",
    "sample",
    "initial",
    "temperature",
    "keep",
    "sort",
    "see",
    "type",
    "emb",
    "dot",
    "action",
    "space",
    "got",
    "discrete",
    "space",
    "dot",
    "sample",
    "go",
    "see",
    "gone",
    "breakout",
    "tutorial",
    "tutorial",
    "probably",
    "thinking",
    "space",
    "built",
    "well",
    "exactly",
    "done",
    "dealing",
    "gaming",
    "implementations",
    "lot",
    "work",
    "done",
    "around",
    "render",
    "around",
    "observation",
    "space",
    "well",
    "around",
    "reward",
    "space",
    "little",
    "bit",
    "sophisticated",
    "like",
    "see",
    "done",
    "let",
    "know",
    "started",
    "already",
    "got",
    "template",
    "code",
    "sort",
    "built",
    "love",
    "tutorial",
    "guys",
    "interested",
    "case",
    "actually",
    "going",
    "go",
    "ahead",
    "test",
    "environment",
    "train",
    "per",
    "usual",
    "go",
    "ahead",
    "let",
    "copy",
    "exact",
    "testing",
    "code",
    "used",
    "driving",
    "tutorial",
    "step",
    "two",
    "test",
    "environment",
    "actually",
    "paste",
    "cool",
    "thing",
    "actually",
    "gone",
    "defined",
    "environment",
    "state",
    "actually",
    "use",
    "part",
    "template",
    "code",
    "go",
    "run",
    "see",
    "automatically",
    "gone",
    "smashed",
    "episodes",
    "got",
    "score",
    "printed",
    "remember",
    "score",
    "going",
    "increment",
    "one",
    "got",
    "shower",
    "37",
    "39",
    "gon",
    "na",
    "decrease",
    "one",
    "outside",
    "range",
    "see",
    "running",
    "five",
    "episodes",
    "got",
    "high",
    "score",
    "26",
    "lowest",
    "minus",
    "keep",
    "running",
    "see",
    "quick",
    "sophisticated",
    "render",
    "function",
    "text",
    "based",
    "going",
    "go",
    "quickly",
    "case",
    "going",
    "go",
    "ahead",
    "train",
    "model",
    "save",
    "sort",
    "see",
    "custom",
    "environment",
    "let",
    "go",
    "ahead",
    "okay",
    "gone",
    "initialized",
    "model",
    "automatically",
    "wrapped",
    "inside",
    "dummy",
    "vec",
    "nv",
    "even",
    "though",
    "gone",
    "imported",
    "looks",
    "like",
    "automatically",
    "wrapped",
    "good",
    "go",
    "written",
    "log",
    "underscore",
    "path",
    "equals",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "true",
    "pass",
    "training",
    "log",
    "remember",
    "going",
    "follow",
    "sort",
    "logging",
    "directory",
    "set",
    "specified",
    "model",
    "equals",
    "ppo",
    "pass",
    "policy",
    "want",
    "use",
    "case",
    "mlp",
    "policy",
    "different",
    "breakout",
    "tutorial",
    "tutorial",
    "image",
    "returned",
    "got",
    "sort",
    "tabular",
    "data",
    "tensorbase",
    "data",
    "actually",
    "well",
    "sort",
    "actually",
    "got",
    "array",
    "values",
    "rather",
    "image",
    "going",
    "use",
    "mlp",
    "policy",
    "passing",
    "environment",
    "specifying",
    "verbose",
    "equals",
    "one",
    "specifying",
    "tensorboard",
    "log",
    "path",
    "cool",
    "next",
    "thing",
    "need",
    "go",
    "ahead",
    "train",
    "going",
    "train",
    "really",
    "really",
    "quickly",
    "need",
    "super",
    "long",
    "training",
    "run",
    "let",
    "try",
    "model",
    "dot",
    "learn",
    "train",
    "total",
    "time",
    "steps",
    "know",
    "let",
    "set",
    "4000",
    "example",
    "let",
    "go",
    "ahead",
    "kick",
    "let",
    "train",
    "train",
    "reasonably",
    "quickly",
    "using",
    "mlp",
    "policy",
    "tabular",
    "data",
    "mean",
    "see",
    "frames",
    "per",
    "second",
    "five",
    "actually",
    "done",
    "literally",
    "went",
    "quick",
    "let",
    "actually",
    "run",
    "longer",
    "rather",
    "4000",
    "let",
    "give",
    "40",
    "right",
    "see",
    "running",
    "600",
    "frames",
    "per",
    "second",
    "really",
    "really",
    "quickly",
    "one",
    "thing",
    "call",
    "game",
    "environments",
    "going",
    "take",
    "longer",
    "train",
    "versus",
    "like",
    "simple",
    "environment",
    "like",
    "sophisticated",
    "environment",
    "longer",
    "going",
    "take",
    "train",
    "sort",
    "keep",
    "mind",
    "planning",
    "projects",
    "sort",
    "committing",
    "clients",
    "building",
    "type",
    "stuff",
    "need",
    "help",
    "means",
    "call",
    "happy",
    "help",
    "start",
    "see",
    "getting",
    "episode",
    "reward",
    "mean",
    "case",
    "minus",
    "minus",
    "looks",
    "like",
    "dropping",
    "ideally",
    "get",
    "positives",
    "minus",
    "minus",
    "getting",
    "close",
    "minus",
    "episode",
    "length",
    "mean",
    "going",
    "pretty",
    "much",
    "time",
    "going",
    "60",
    "seconds",
    "maximum",
    "remember",
    "might",
    "need",
    "train",
    "little",
    "longer",
    "looks",
    "like",
    "getting",
    "close",
    "positives",
    "yet",
    "right",
    "minus",
    "let",
    "actually",
    "give",
    "another",
    "know",
    "20",
    "000",
    "steps",
    "let",
    "run",
    "minus",
    "see",
    "starting",
    "get",
    "close",
    "positives",
    "minus",
    "let",
    "let",
    "run",
    "right",
    "back",
    "okay",
    "got",
    "pretty",
    "close",
    "episode",
    "reward",
    "mean",
    "got",
    "minus",
    "guess",
    "depends",
    "starting",
    "point",
    "model",
    "actually",
    "develops",
    "let",
    "actually",
    "go",
    "test",
    "see",
    "actually",
    "go",
    "use",
    "evaluate",
    "policy",
    "save",
    "model",
    "model",
    "dot",
    "save",
    "let",
    "define",
    "path",
    "uh",
    "going",
    "call",
    "shower",
    "model",
    "let",
    "double",
    "check",
    "directory",
    "name",
    "training",
    "saved",
    "models",
    "let",
    "specify",
    "going",
    "call",
    "shower",
    "model",
    "ppo",
    "cool",
    "type",
    "shower",
    "path",
    "go",
    "take",
    "look",
    "music",
    "shallow",
    "model",
    "underscore",
    "ppo",
    "saved",
    "good",
    "delete",
    "model",
    "wanted",
    "reload",
    "type",
    "model",
    "equals",
    "ppo",
    "dot",
    "load",
    "pass",
    "path",
    "pass",
    "environment",
    "wanted",
    "test",
    "run",
    "evaluate",
    "policy",
    "pass",
    "model",
    "pass",
    "environment",
    "pass",
    "number",
    "eval",
    "episodes",
    "need",
    "render",
    "time",
    "render",
    "function",
    "type",
    "render",
    "equals",
    "true",
    "throw",
    "error",
    "might",
    "actually",
    "throw",
    "error",
    "got",
    "music",
    "got",
    "past",
    "good",
    "case",
    "got",
    "mean",
    "episode",
    "reward",
    "12",
    "standard",
    "deviation",
    "getting",
    "got",
    "wide",
    "variance",
    "could",
    "train",
    "whole",
    "lot",
    "longer",
    "tighten",
    "environment",
    "make",
    "little",
    "bit",
    "realistic",
    "able",
    "adjust",
    "sort",
    "steady",
    "state",
    "sort",
    "gives",
    "idea",
    "bring",
    "together",
    "tutorial",
    "went",
    "bunch",
    "stuff",
    "well",
    "took",
    "uh",
    "imported",
    "dependencies",
    "took",
    "look",
    "different",
    "types",
    "spaces",
    "remember",
    "discrete",
    "space",
    "box",
    "tuple",
    "dict",
    "multi",
    "binary",
    "keep",
    "mind",
    "stable",
    "baselines",
    "support",
    "tuples",
    "yet",
    "also",
    "took",
    "look",
    "build",
    "environment",
    "remember",
    "define",
    "init",
    "function",
    "step",
    "function",
    "render",
    "function",
    "last",
    "least",
    "reset",
    "function",
    "terms",
    "testing",
    "training",
    "saving",
    "model",
    "much",
    "got",
    "really",
    "sophisticated",
    "model",
    "like",
    "build",
    "means",
    "hit",
    "comments",
    "love",
    "help",
    "build",
    "really",
    "cool",
    "environments",
    "let",
    "know",
    "love",
    "see",
    "well",
    "note",
    "finished",
    "project",
    "number",
    "three",
    "comes",
    "hopefully",
    "enjoyed",
    "course",
    "gone",
    "covered",
    "whole",
    "heap",
    "stuff",
    "remember",
    "core",
    "purpose",
    "course",
    "bridge",
    "gap",
    "lot",
    "theoretical",
    "stuff",
    "see",
    "floating",
    "around",
    "terms",
    "reinforcement",
    "learning",
    "show",
    "practical",
    "set",
    "implementation",
    "steps",
    "went",
    "rl",
    "nutshell",
    "talked",
    "reinforcement",
    "learning",
    "works",
    "took",
    "look",
    "set",
    "environment",
    "stable",
    "baselines",
    "went",
    "built",
    "took",
    "look",
    "different",
    "types",
    "environments",
    "using",
    "open",
    "ai",
    "gym",
    "step",
    "number",
    "two",
    "went",
    "trained",
    "model",
    "went",
    "tested",
    "evaluated",
    "took",
    "look",
    "view",
    "inside",
    "tensorboard",
    "extended",
    "algorithms",
    "specifically",
    "went",
    "implemented",
    "callbacks",
    "went",
    "used",
    "different",
    "algorithms",
    "remember",
    "use",
    "ppo",
    "a2c",
    "used",
    "dqn",
    "algorithm",
    "well",
    "even",
    "went",
    "changed",
    "policy",
    "architecture",
    "pretty",
    "cool",
    "stuff",
    "happening",
    "went",
    "three",
    "different",
    "projects",
    "remember",
    "went",
    "trained",
    "model",
    "play",
    "breakout",
    "went",
    "trained",
    "model",
    "race",
    "car",
    "around",
    "racing",
    "track",
    "also",
    "went",
    "built",
    "custom",
    "shower",
    "environment",
    "quite",
    "fair",
    "bit",
    "want",
    "leave",
    "additional",
    "resources",
    "gone",
    "david",
    "silva",
    "reinforcement",
    "learning",
    "course",
    "highly",
    "recommend",
    "team",
    "team",
    "behind",
    "deepmind",
    "guys",
    "actually",
    "built",
    "alpha",
    "go",
    "model",
    "obviously",
    "super",
    "smart",
    "dude",
    "got",
    "awesome",
    "theories",
    "floating",
    "around",
    "means",
    "recommend",
    "check",
    "also",
    "great",
    "book",
    "called",
    "reinforcement",
    "learning",
    "introduction",
    "richard",
    "sutton",
    "andrew",
    "bartos",
    "pioneers",
    "reinforcement",
    "learning",
    "field",
    "highly",
    "recommend",
    "go",
    "check",
    "book",
    "got",
    "awesome",
    "stuff",
    "well",
    "terms",
    "learn",
    "next",
    "love",
    "people",
    "give",
    "ideas",
    "go",
    "want",
    "give",
    "one",
    "things",
    "cover",
    "course",
    "hyper",
    "parameter",
    "tuning",
    "one",
    "ways",
    "improve",
    "train",
    "models",
    "tune",
    "hyper",
    "parameters",
    "start",
    "progress",
    "algorithms",
    "might",
    "something",
    "take",
    "deeper",
    "look",
    "like",
    "see",
    "tutorial",
    "course",
    "means",
    "hit",
    "building",
    "detailed",
    "custom",
    "environments",
    "example",
    "talked",
    "little",
    "bit",
    "terms",
    "implementing",
    "render",
    "function",
    "pi",
    "game",
    "well",
    "integration",
    "simulation",
    "systems",
    "like",
    "mojoko",
    "unity",
    "last",
    "least",
    "think",
    "one",
    "coolest",
    "things",
    "could",
    "potentially",
    "take",
    "look",
    "learning",
    "implementation",
    "say",
    "example",
    "actually",
    "went",
    "built",
    "cart",
    "pole",
    "robot",
    "actually",
    "got",
    "went",
    "trained",
    "simulated",
    "environment",
    "implemented",
    "real",
    "environment",
    "perhaps",
    "using",
    "raspberry",
    "pi",
    "based",
    "robot",
    "think",
    "would",
    "awesome",
    "thing",
    "go",
    "take",
    "look",
    "next",
    "note",
    "wraps",
    "hopefully",
    "enjoyed",
    "thanks",
    "tuning",
    "thanks",
    "much",
    "tuning",
    "guys",
    "enjoyed",
    "video",
    "sure",
    "give",
    "thumbs",
    "hit",
    "subscribe",
    "tick",
    "bell",
    "let",
    "know",
    "feedback",
    "anything",
    "like",
    "see",
    "going",
    "get",
    "stuck",
    "means",
    "hit",
    "comments",
    "happy",
    "help",
    "thanks",
    "tuning",
    "peace"
  ],
  "keywords": [
    "working",
    "oh",
    "stuff",
    "actually",
    "using",
    "reinforcement",
    "learning",
    "train",
    "car",
    "around",
    "track",
    "really",
    "going",
    "great",
    "music",
    "get",
    "better",
    "let",
    "name",
    "course",
    "bunch",
    "basically",
    "able",
    "go",
    "leverage",
    "specifically",
    "set",
    "environment",
    "work",
    "different",
    "algorithms",
    "also",
    "test",
    "environments",
    "open",
    "ai",
    "gym",
    "pole",
    "build",
    "last",
    "least",
    "take",
    "look",
    "custom",
    "something",
    "important",
    "use",
    "case",
    "end",
    "terms",
    "getting",
    "running",
    "ideally",
    "lot",
    "see",
    "game",
    "first",
    "taking",
    "rl",
    "works",
    "well",
    "called",
    "stable",
    "baselines",
    "step",
    "2",
    "one",
    "need",
    "kick",
    "training",
    "whole",
    "types",
    "available",
    "inside",
    "agent",
    "trained",
    "model",
    "evaluate",
    "looks",
    "like",
    "evaluation",
    "metrics",
    "tensorboard",
    "five",
    "stop",
    "hit",
    "certain",
    "threshold",
    "write",
    "written",
    "say",
    "example",
    "wanted",
    "change",
    "neural",
    "network",
    "particular",
    "would",
    "projects",
    "three",
    "solve",
    "breakout",
    "atari",
    "sort",
    "little",
    "bit",
    "racing",
    "think",
    "pretty",
    "show",
    "spaces",
    "okay",
    "time",
    "give",
    "used",
    "high",
    "idea",
    "machine",
    "know",
    "probably",
    "quite",
    "right",
    "gives",
    "got",
    "based",
    "reward",
    "try",
    "might",
    "times",
    "thing",
    "talk",
    "later",
    "us",
    "together",
    "four",
    "key",
    "things",
    "within",
    "action",
    "plus",
    "observations",
    "observation",
    "actions",
    "point",
    "good",
    "way",
    "trying",
    "every",
    "start",
    "order",
    "command",
    "much",
    "simulated",
    "real",
    "heap",
    "driving",
    "allows",
    "cool",
    "another",
    "make",
    "space",
    "architecture",
    "best",
    "type",
    "number",
    "could",
    "obviously",
    "possible",
    "robot",
    "believe",
    "position",
    "close",
    "function",
    "simple",
    "models",
    "means",
    "random",
    "mind",
    "note",
    "long",
    "install",
    "dependencies",
    "single",
    "pip",
    "run",
    "exclamation",
    "mark",
    "dash",
    "brackets",
    "pass",
    "built",
    "lines",
    "documentation",
    "link",
    "code",
    "want",
    "quickly",
    "20",
    "notebook",
    "10",
    "steps",
    "main",
    "tutorial",
    "import",
    "load",
    "save",
    "reload",
    "view",
    "logs",
    "call",
    "back",
    "algorithm",
    "comments",
    "remember",
    "ahead",
    "done",
    "next",
    "line",
    "passing",
    "gone",
    "two",
    "uh",
    "keep",
    "imported",
    "os",
    "define",
    "log",
    "jim",
    "ppo",
    "underscore",
    "a2c",
    "dqn",
    "performance",
    "dot",
    "common",
    "vec",
    "env",
    "dummy",
    "project",
    "policy",
    "performing",
    "average",
    "episodes",
    "standard",
    "deviation",
    "mean",
    "rather",
    "testing",
    "colon",
    "forward",
    "slash",
    "exactly",
    "box",
    "range",
    "values",
    "value",
    "discrete",
    "0",
    "1",
    "zero",
    "tuple",
    "dict",
    "dictionary",
    "similar",
    "effectively",
    "second",
    "play",
    "create",
    "equals",
    "emv",
    "six",
    "episode",
    "comma",
    "whether",
    "score",
    "render",
    "sample",
    "gon",
    "na",
    "reset",
    "state",
    "minus",
    "went",
    "defined",
    "took",
    "loss",
    "length",
    "per",
    "many",
    "total",
    "gpu",
    "path",
    "specify",
    "folder",
    "additional",
    "saved",
    "delete",
    "still",
    "passed",
    "specified",
    "000",
    "longer",
    "thousand",
    "200",
    "eval",
    "copy",
    "callback",
    "new",
    "128",
    "importing",
    "roms",
    "image",
    "million",
    "shower",
    "temperature",
    "self"
  ]
}