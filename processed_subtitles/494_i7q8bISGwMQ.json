{
  "text": "welcome back so i've started this video lecture \nseries on reinforcement learning and the last  \nthree videos were at a very high level kind of \nwhat is reinforcement learning how does it work uh  \nwhat are some of the applications but we really \ndidn't dig into too many details on the actual  \nalgorithms of how you implement reinforcement \nlearning in practice and so that's what i'm  \nactually going to do today and in this next part \nof this series is something i hope is going to be  \nreally really useful kind of for all of you which \nis the  \nThe first thing is i'm going to kind of organize the different approaches of reinforcement learning \nThis is a massive field that's about 100 years old\nThis merges neuroscience, behavioral science like Pavlov's dog, optimization theory, optimal control\nthink Bellman's equation \nand the Hamilton Jacobi Bellman equation  \nall the way to modern day deep reinforcement learning\nwhich is kind of how to use powerful machine learning techniques to solve these optimization problems\nand you'll remember that in my view of reinforcement learning\nthis is really at the intersection of machine learning \nand control theory\nso we're essentially machine learning good effective control strategies to interact with in a an environment\nSo in this first lecture what i'm gonna do and i think i'm hoping that this is actually super useful for some of you  \nis i'm going to talk through the organization \nof these different decisions you have to make  \nand kind of how you can think about the landscape \nof reinforcement learning.\nBefore going on I want to mention this is actually a chapter in the \nnew second edition of our book data driven science  \nand engineering with myself and Nathan Kutz and \nreinforcement learning was one of the new chapters  \nI decided to write so this is a great excuse \nfor me to get to learn more about reinforcement learning\nand it's also a nice opportunity for me \nto kind of get to communicate more details to you  \nso if you want to download this chapter the link \nis here, and I'll also put it in the comments below  \nand i'll have a link to the second edition of the \nbook uh up soon as well probably in the comments  \ngood so um a new chapter you can \nfollow along with all of the videos  \nand each video kind of um you \nknow follows follows the chapter  \ngood so before i get into that organizational \nchart of how you know all of these different  \ntypes of reinforcement learning can be thought \nof i want to just do a really really quick recap  \nof what is the reinforcement learning problem \nso in reinforcement learning you have an agent  \nthat gets to interact with the world or the \nenvironment through a set of actions sometimes  \nthese are discrete actions sometimes these are \ncontinuous actions if i have a robot i might have  \na continuous action space whereas if i'm playing \na game if i'm the you know the white pieces on a  \nchess board then i have a discrete set of actions \neven though it might be kind of high dimensional  \nand i observe the state of the system at each \ntime step i get to observe the state of the system  \nand use that information uh to change my actions \nto try to maximize my current or future rewards  \nuh through through playing and i'll mention that \nin lots of applications for example in chess  \nthe reward structure might be quite sparse i might \nnot get any feedback on whether or not i'm making  \ngood moves until the very end when i either \nwin or lose tic-tac-toe backgammon checkers  \ngo are all kind of the same way and that delayed \nreward structure is one of the things that makes  \nthis reinforcement learning problem really really \nchallenging it's what makes uh you know learning  \nin animal systems also challenging if you want to \nteach your dog a trick you know they have to know  \nkind of step by step what you want them to do \nand so you actually sometimes have to give them  \nrewards at intermediate steps to train a behavior \nand so the agent their control strategy or their  \ntheir policy is typically called pi and it \nbasically is a probability of taking action a  \ngiven a state s a current state s so this could be \na deterministic policy it could be a probabilistic  \npolicy but essentially it's a set of rules that \ndetermines what actions i as the agent take given  \nuh what i sense uh in the environment to maximize \nmy future rewards so that's the policy and again  \nusually this is written in a probabilistic \nframework because typically the environment  \nis written as a probabilistic model and there \nis something called a value function so given  \num you know some policy pi that i take then i can \nassociate a value with being in each of the states  \nof this system essentially by what is my expected \nfuture reward add up all of my future rewards  \nwhat's the expectation of that and we'll put in \nthis little discount factor because future rewards  \nmight be less uh advantageous to me than than \ncurrent rewards this is just something that people  \ndo in economic theory it's kind of like a you \nknow utility function and so for every policy pi  \nthere is a value associated with being in each \nof the given states s now again i'll point out  \nfor for even reasonably sophisticated problems you \ncan do this for tic-tac-toe you can enumerate all  \npossible states and all possible actions and you \ncan compute this value function kind of through  \nbrute force but even for moderately complicated \ngames even like checkers let alone back game in  \nher chess or go this state space the the the \nspace of all possible states you could observe  \nyour system in is astronomically large i think \nit's estimated that there's 10 to the 80 plus um  \nmaybe it's 10 to 180 africa it's a huge number of \npossible chess boards even more possible go boards  \nand so you can't really actually enumerate \nthis value function but it's a good um kind  \nof abstract function that we can think about are \nthese policy functions and these value functions  \nand at least in simplistic dynamic programming \nwe often assume that we know a model of of our  \nenvironment and i'll get to that in a minute so \nthe goal here the entire goal of reinforcement  \nlearning is to learn through trial and error \nthrough experience what is the optimal policy  \nto maximize your future rewards okay so notice \nthat this value function is a function of policy  \npi i want to learn the best possible policy that \nalways gives me the most value out of every board  \nposition out of every state and that's a really \nhard problem it's easy to state the goal and it  \nis really really hard to solve this problem that's \nwhy it's been uh you know this growing field for  \n100 years that's why it's still a growing field \nbecause we have more powerful emerging techniques  \nin machine learning to start to solve this problem \nso that's the framework i need you to know what  \nthe the policy is that is the set of kind of rules \nor or uh you know controllers that i as an agent  \nget to take to manipulate my environment the value \nfunction tells me how valuable it is to be in a  \nparticular state so i might want to move myself \ninto that state so i need you to know kind of this  \nnomenclature so now i can kind of show you how all \nof these techniques are organized okay so that's  \nwhat i'm going to do for the rest of the video \nis we're going to talk through kind of the key  \norganization of all of the different uh like \nlike mainstream types of reinforcement learning  \nokay so the first biggest dichotomy is between \nmodel-based and model-free reinforcement learning  \nso if you actually have a good model of your \nenvironment you have some you know markov decision  \nprocess or some differential equation if you have \na good model to start with then you can work in  \nthis model based reinforcement learning world now \nsome people don't actually consider what i'm about  \nto talk about reinforcement learning but but i \ndo okay so for example if my environment is a  \nmarkov decision process which means that there is \na probability kind of a deterministic probability  \nthat sounds like an oxymoron but if there is \na specified probability of moving from state s  \nto the next state s prime given action a and \nthis probability function is known so it's a  \nit doesn't depend on the history of your actions \nand states it only depends on the current state  \nand the current action determines a probability of \ngoing to a next state s prime then uh two really  \nreally powerful techniques that i'm going to tell \nyou about to optimize the the policy function pi  \nis policy iteration and value iteration these \nallow you to essentially iteratively walk through  \num the game or or the markov decision process \ntaking actions that you think are going to  \nbe the best and then assessing what the \nvalue of that action and state actually  \nare and then kind of refining and iterating \nthe policy function and the value function  \nso that is a really really powerful approach \nif you have a model of your system you can  \nrun this kind of on a computer and and kind of \ndetermine learn what the best policy and value is  \nand this is kind of a special case of dynamic \nprogramming that relies on the bellman optimality  \ncondition for the value function \nso i'm going to do a whole lecture  \non this blue part right here we're going to \ntalk about policy iteration and value iteration  \nand how they are essentially dynamic using dynamic \nprogramming on the value function which satisfies  \nbellman's optimality condition now that was for \nprobabilistic uh processes things where maybe  \nyou know like backgammon where there's a dice \nroll at every turn for deterministic systems like  \na robot system or a self-driving car if i think \nabout a human you know my reinforcement learning  \nproblem this is much more of a continuous control \nproblem in which case i might have some nonlinear  \ndifferential equation x dot equals f of x comma u \nand so the linear optimal control that we studied  \ni guess in chapter 8 of the book so linear \nquadratic regulators common filters things like  \nthat optimal linear control problems are special \ncases of this optimal non-linear control problem  \nwith the hamilton jacobi bellman equation \nagain this relies on bellman optimality and  \nyou can use kind of dynamic programming ideas \nto solve optimal nonlinear control problems  \nlike this now i'll point out mathematically \nthis is a beautiful theory it's powerful  \nit's been around for decades and it's you know \nkind of the textbook way of thinking about how to  \ndesign optimal policies and optimal controllers \nyou know for markov decision processes and for  \nnon-linear control systems in practice actually \nsolving these things with dynamic programming  \nends up usually amounting to a brute force search \nand it's usually not scalable to high dimensional  \nsystems so typically it's hard to do this optimal \nhamilton jacobi bellman type non-linear control  \nfor an even moderately high dimensional system \nyou know you can do this for a three-dimensional  \nsystem sometimes a five-dimensional system maybe \nyou know i've heard special cases with with  \nmachine learning you can do this maybe for a 10 or \n100 dimensional system but you can't do this for  \nthe nonlinear fluid flow equations which might \nhave you know a hundred thousand or a million  \ndimensional differential equation when you write \nit down on your computer so important caveat there  \nbut that's model based control and a lot of what \nwe're going to do in model free control uses ideas  \nthat we learned from model based control so even \nthough you know i don't actually do a lot of this  \nin my daily life with reinforcement learning \nmost of the time we don't have a good model  \nof our system for example in chess i don't have \na model of my opponent for example or at least i  \ncan't write it down mathematically as a markov \ndecision process so i can't really use these  \ntechniques but a lot of what model free control \nreinforcement learning is going to do is kind of  \napproximate dynamic programming where you're \nsimultaneously learning kind of the dynamics  \nor learning to update these these functions \nthrough trial and error without actually having  \na model and so in model-free reinforcement \nlearning kind of the major dichotomy here  \nis between gradient-free and gradient-based \nmethods and i'll tell you what this means in a  \nlittle bit but for example if i can parameterize \nmy policy pi by some variables theta and i know  \nkind of what the dependency with those variables \ntheta are i might be able to take the gradient  \nof my reward function or my value function \nwith respect to those parameters directly  \nand speed up the optimization okay so gradient \nbased if you if you can use it is usually going  \nto be the fastest most efficient way to do \nthings but oftentimes again we don't have  \ngradient information we're just playing games \nwe're playing chess we're playing go and i can't  \ncompute the derivative of one game with respect \nto another that's hard for me at least to do  \nand so within gradient free okay there's a lot \nof dichotomies here there's a dichotomy of a  \ndichotomy of a dichotomy within gradient free \ncontrol there is this idea of sometimes you can  \nbe off policy or on policy and it's a really \nimportant uh distinction what on policy means  \nis that let's say i'm playing a bunch of games \nof chess i'm trying to learn an optimal policy  \nfunction or an optimal value function or both by \nplaying games of chess and iteratively kind of  \nrefining my estimate of pi or a v what on policy \nmeans is that i always play my best game possible  \nwhatever i think the value function is and \nwhatever i think my best policy possible is  \ni'm always going to use that best policy as i play \nmy game and i'm going to always try to kind of get  \nthe most reward out of my system every game \ni play that's what it means to be on policy  \noff policy means well maybe i'll try some things \nmaybe maybe i know that my policy is suboptimal  \nand so i'm just going to do some like random moves \noccasionally that is called off policy because i  \nthink they're sub-optimal but they might be really \nvaluable for learning information about the system  \nuh so on policy methods include this \nsarsa state action reward state action  \nand there's all of these variants of the sarsa \nalgorithm this on policy reinforcement learning  \nand these tds mean temporal difference and mc \nis monte carlo and so there's this whole family  \nof kind of gradient-free optimization techniques \nthat use different kind of amounts of history i'll  \ntalk all about that that's going to be a whole \nother lecture is this this red box gradient free  \nmodel free reinforcement learning and so the off \npolicy version of sarsa kind of this on policy  \nset of algorithms there is an off policy variant \ncalled q learning and so this quality function  \nq is kind of the joint value if you \nlike of being in a particular state  \nand taking a particular action a so this quality \nfunction contains all of the information of my  \nmy optimal policy and the value function and both \nof these can be derived from the quality function  \nbut the really important distinction is that when \nwe learn based on the quality function we don't  \nneed a model for what my next state is going to be \nthis quality function kind of implicitly defines  \nthe value of you know based on where you're going \nto go in the future and so q learning is a really  \nnice way of learning when you have no model and \nyou can take off policy information and learn  \nfrom that you can take a sub-optimal controller \njust to see what happens and still learn and get  \nbetter policies and better value functions in the \nfuture and that's also really important if you  \nwant to do imitation learning if i want to just \nwatch other people play games of chess even though  \ni don't know what their value function is or what \ntheir policy is with these off policy learning  \nalgorithms you can accumulate that information \ninto your estimate of the world and every bit of  \ninformation you get improves your quality function \nand it improves the next game you're going to play  \nso really powerful and i would say most of what we \ndo nowadays you know is kind of in this q learning  \nworld a lot a lot a lot of machine learning is \nq learning reinforcement learning is q-learning  \nand then the gradient-based algorithms i'm not \ngoing to talk about it too much here but it's  \nessentially where you would actually update the \nparameters of your policy or your value function  \nor your q function directly using some kind of a \ngradient optimization so if i can sum up all of my  \nfuture rewards and it's a function of the current \nparameters theta that parameterize my policy  \nthen i might be able to use gradient optimization \nthings like newton's steps and steepest descent  \nthings like that to get a good estimate \nand this when i have the ability to do that  \nis going to be way way faster uh than any of \nthese uh these gradient free methods and and  \neven in term uh will be faster than dynamic \nprogramming and so the last piece of this  \nis kind of in the last 10 years we've had this \nmassive explosion of deep reinforcement learning  \na lot of this has been because of deep mind and \nalphago you know demonstrating that machines  \ncomputers can play atari games at human level \nperformance they can beat grand masters that go  \njust incredibly impressive demonstrations \nof reinforcement learning that now use deep  \nneural networks either to learn a model where you \ncan then use model-based reinforcement learning  \nor to represent these kind of model-free concepts \nso you can have like a deep neural network for  \nthe quality function you can have a deep neural \nnetwork for the policy and then differentiate  \nwith respect to those network parameters uh using \nkind of you know auto diff and back propagation  \nuh to do gradient based optimization on your \npolicy network i would say that deep model  \npredictive control this doesn't exactly fit into \nthe reinforcement learning world but i would say  \nyou know it's it's morally very closely related \ndeep model predictive control allows you to solve  \nthese kind of hard optimal nonlinear problems \nand then you can actually learn a policy based  \non what your model predictive controller actually \ndoes you can essentially kind of codify that model  \npredictive controller into a control policy and \nfinally uh actor critic methods um actor critic  \nmethods existed long before deep reinforcement \nlearning but nowadays they have kind of a renewed  \ninterest uh because you can you can uh you \ncan train these with with deep neural networks  \nokay so that is the mile-high view as i see it of \nthe different categories of reinforcement learning  \nis this comprehensive absolutely \nnot is it a hundred percent  \nfactually correct definitely not this is you \nknow a rough sketch of the main divides and  \nthings you need to think about when you're \nchoosing a reinforcement learning algorithm  \nif you have a model of your system you can use \ndynamic programming based on bellman optimality  \nif you don't have a model of the system you can \neither use gradient free or gradient based methods  \nand then there's on policy and off policy variants \ndepending on you know your specific needs it tends  \nout to be that sarsa methods are more conservative \nand q learning will tend to converge faster  \nand then for all of these methods there are \nways of kind of making them more powerful  \nand more flexible representations using uh deep \nneural networks in kind of different focused ways  \nokay so in the next few videos we'll zoom into you \nknow this part here for markup decision processes  \nhow we do policy iteration and value iteration \nwe'll actually derive the quality function uh here  \nwe'll talk about model free control these kind \nof gradient free methods on policy and off policy  \ncue learning is one of the most important ones and \ntemporal difference learning actually has a lot of  \nneuro science analog so how we learn in in our \nanimal brains people think you know is very  \nclosely related to these td learning policies \nwe'll talk about how you do optimal nonlinear  \ncontrol with the hamilton jacobi bellman equation \nuh we'll talk very briefly about policy gradient  \noptimization and then you know all of these \nthere are kind of deep learning things uh  \nwe'll pepper it throughout with deep learning \nor maybe i'll have a whole lecture on on these  \ndeep learning methods so that's all coming up \nreally excited to walk you through this thank you\n",
  "words": [
    "welcome",
    "back",
    "started",
    "video",
    "lecture",
    "series",
    "reinforcement",
    "learning",
    "last",
    "three",
    "videos",
    "high",
    "level",
    "kind",
    "reinforcement",
    "learning",
    "work",
    "uh",
    "applications",
    "really",
    "dig",
    "many",
    "details",
    "actual",
    "algorithms",
    "implement",
    "reinforcement",
    "learning",
    "practice",
    "actually",
    "going",
    "today",
    "next",
    "part",
    "series",
    "something",
    "hope",
    "going",
    "really",
    "really",
    "useful",
    "kind",
    "first",
    "thing",
    "going",
    "kind",
    "organize",
    "different",
    "approaches",
    "reinforcement",
    "learning",
    "massive",
    "field",
    "100",
    "years",
    "old",
    "merges",
    "neuroscience",
    "behavioral",
    "science",
    "like",
    "pavlov",
    "dog",
    "optimization",
    "theory",
    "optimal",
    "control",
    "think",
    "bellman",
    "equation",
    "hamilton",
    "jacobi",
    "bellman",
    "equation",
    "way",
    "modern",
    "day",
    "deep",
    "reinforcement",
    "learning",
    "kind",
    "use",
    "powerful",
    "machine",
    "learning",
    "techniques",
    "solve",
    "optimization",
    "problems",
    "remember",
    "view",
    "reinforcement",
    "learning",
    "really",
    "intersection",
    "machine",
    "learning",
    "control",
    "theory",
    "essentially",
    "machine",
    "learning",
    "good",
    "effective",
    "control",
    "strategies",
    "interact",
    "environment",
    "first",
    "lecture",
    "gon",
    "na",
    "think",
    "hoping",
    "actually",
    "super",
    "useful",
    "going",
    "talk",
    "organization",
    "different",
    "decisions",
    "make",
    "kind",
    "think",
    "landscape",
    "reinforcement",
    "learning",
    "going",
    "want",
    "mention",
    "actually",
    "chapter",
    "new",
    "second",
    "edition",
    "book",
    "data",
    "driven",
    "science",
    "engineering",
    "nathan",
    "kutz",
    "reinforcement",
    "learning",
    "one",
    "new",
    "chapters",
    "decided",
    "write",
    "great",
    "excuse",
    "get",
    "learn",
    "reinforcement",
    "learning",
    "also",
    "nice",
    "opportunity",
    "kind",
    "get",
    "communicate",
    "details",
    "want",
    "download",
    "chapter",
    "link",
    "also",
    "put",
    "comments",
    "link",
    "second",
    "edition",
    "book",
    "uh",
    "soon",
    "well",
    "probably",
    "comments",
    "good",
    "um",
    "new",
    "chapter",
    "follow",
    "along",
    "videos",
    "video",
    "kind",
    "um",
    "know",
    "follows",
    "follows",
    "chapter",
    "good",
    "get",
    "organizational",
    "chart",
    "know",
    "different",
    "types",
    "reinforcement",
    "learning",
    "thought",
    "want",
    "really",
    "really",
    "quick",
    "recap",
    "reinforcement",
    "learning",
    "problem",
    "reinforcement",
    "learning",
    "agent",
    "gets",
    "interact",
    "world",
    "environment",
    "set",
    "actions",
    "sometimes",
    "discrete",
    "actions",
    "sometimes",
    "continuous",
    "actions",
    "robot",
    "might",
    "continuous",
    "action",
    "space",
    "whereas",
    "playing",
    "game",
    "know",
    "white",
    "pieces",
    "chess",
    "board",
    "discrete",
    "set",
    "actions",
    "even",
    "though",
    "might",
    "kind",
    "high",
    "dimensional",
    "observe",
    "state",
    "system",
    "time",
    "step",
    "get",
    "observe",
    "state",
    "system",
    "use",
    "information",
    "uh",
    "change",
    "actions",
    "try",
    "maximize",
    "current",
    "future",
    "rewards",
    "uh",
    "playing",
    "mention",
    "lots",
    "applications",
    "example",
    "chess",
    "reward",
    "structure",
    "might",
    "quite",
    "sparse",
    "might",
    "get",
    "feedback",
    "whether",
    "making",
    "good",
    "moves",
    "end",
    "either",
    "win",
    "lose",
    "backgammon",
    "checkers",
    "go",
    "kind",
    "way",
    "delayed",
    "reward",
    "structure",
    "one",
    "things",
    "makes",
    "reinforcement",
    "learning",
    "problem",
    "really",
    "really",
    "challenging",
    "makes",
    "uh",
    "know",
    "learning",
    "animal",
    "systems",
    "also",
    "challenging",
    "want",
    "teach",
    "dog",
    "trick",
    "know",
    "know",
    "kind",
    "step",
    "step",
    "want",
    "actually",
    "sometimes",
    "give",
    "rewards",
    "intermediate",
    "steps",
    "train",
    "behavior",
    "agent",
    "control",
    "strategy",
    "policy",
    "typically",
    "called",
    "pi",
    "basically",
    "probability",
    "taking",
    "action",
    "given",
    "state",
    "current",
    "state",
    "could",
    "deterministic",
    "policy",
    "could",
    "probabilistic",
    "policy",
    "essentially",
    "set",
    "rules",
    "determines",
    "actions",
    "agent",
    "take",
    "given",
    "uh",
    "sense",
    "uh",
    "environment",
    "maximize",
    "future",
    "rewards",
    "policy",
    "usually",
    "written",
    "probabilistic",
    "framework",
    "typically",
    "environment",
    "written",
    "probabilistic",
    "model",
    "something",
    "called",
    "value",
    "function",
    "given",
    "um",
    "know",
    "policy",
    "pi",
    "take",
    "associate",
    "value",
    "states",
    "system",
    "essentially",
    "expected",
    "future",
    "reward",
    "add",
    "future",
    "rewards",
    "expectation",
    "put",
    "little",
    "discount",
    "factor",
    "future",
    "rewards",
    "might",
    "less",
    "uh",
    "advantageous",
    "current",
    "rewards",
    "something",
    "people",
    "economic",
    "theory",
    "kind",
    "like",
    "know",
    "utility",
    "function",
    "every",
    "policy",
    "pi",
    "value",
    "associated",
    "given",
    "states",
    "point",
    "even",
    "reasonably",
    "sophisticated",
    "problems",
    "enumerate",
    "possible",
    "states",
    "possible",
    "actions",
    "compute",
    "value",
    "function",
    "kind",
    "brute",
    "force",
    "even",
    "moderately",
    "complicated",
    "games",
    "even",
    "like",
    "checkers",
    "let",
    "alone",
    "back",
    "game",
    "chess",
    "go",
    "state",
    "space",
    "space",
    "possible",
    "states",
    "could",
    "observe",
    "system",
    "astronomically",
    "large",
    "think",
    "estimated",
    "10",
    "80",
    "plus",
    "um",
    "maybe",
    "10",
    "180",
    "africa",
    "huge",
    "number",
    "possible",
    "chess",
    "boards",
    "even",
    "possible",
    "go",
    "boards",
    "ca",
    "really",
    "actually",
    "enumerate",
    "value",
    "function",
    "good",
    "um",
    "kind",
    "abstract",
    "function",
    "think",
    "policy",
    "functions",
    "value",
    "functions",
    "least",
    "simplistic",
    "dynamic",
    "programming",
    "often",
    "assume",
    "know",
    "model",
    "environment",
    "get",
    "minute",
    "goal",
    "entire",
    "goal",
    "reinforcement",
    "learning",
    "learn",
    "trial",
    "error",
    "experience",
    "optimal",
    "policy",
    "maximize",
    "future",
    "rewards",
    "okay",
    "notice",
    "value",
    "function",
    "function",
    "policy",
    "pi",
    "want",
    "learn",
    "best",
    "possible",
    "policy",
    "always",
    "gives",
    "value",
    "every",
    "board",
    "position",
    "every",
    "state",
    "really",
    "hard",
    "problem",
    "easy",
    "state",
    "goal",
    "really",
    "really",
    "hard",
    "solve",
    "problem",
    "uh",
    "know",
    "growing",
    "field",
    "100",
    "years",
    "still",
    "growing",
    "field",
    "powerful",
    "emerging",
    "techniques",
    "machine",
    "learning",
    "start",
    "solve",
    "problem",
    "framework",
    "need",
    "know",
    "policy",
    "set",
    "kind",
    "rules",
    "uh",
    "know",
    "controllers",
    "agent",
    "get",
    "take",
    "manipulate",
    "environment",
    "value",
    "function",
    "tells",
    "valuable",
    "particular",
    "state",
    "might",
    "want",
    "move",
    "state",
    "need",
    "know",
    "kind",
    "nomenclature",
    "kind",
    "show",
    "techniques",
    "organized",
    "okay",
    "going",
    "rest",
    "video",
    "going",
    "talk",
    "kind",
    "key",
    "organization",
    "different",
    "uh",
    "like",
    "like",
    "mainstream",
    "types",
    "reinforcement",
    "learning",
    "okay",
    "first",
    "biggest",
    "dichotomy",
    "reinforcement",
    "learning",
    "actually",
    "good",
    "model",
    "environment",
    "know",
    "markov",
    "decision",
    "process",
    "differential",
    "equation",
    "good",
    "model",
    "start",
    "work",
    "model",
    "based",
    "reinforcement",
    "learning",
    "world",
    "people",
    "actually",
    "consider",
    "talk",
    "reinforcement",
    "learning",
    "okay",
    "example",
    "environment",
    "markov",
    "decision",
    "process",
    "means",
    "probability",
    "kind",
    "deterministic",
    "probability",
    "sounds",
    "like",
    "oxymoron",
    "specified",
    "probability",
    "moving",
    "state",
    "next",
    "state",
    "prime",
    "given",
    "action",
    "probability",
    "function",
    "known",
    "depend",
    "history",
    "actions",
    "states",
    "depends",
    "current",
    "state",
    "current",
    "action",
    "determines",
    "probability",
    "going",
    "next",
    "state",
    "prime",
    "uh",
    "two",
    "really",
    "really",
    "powerful",
    "techniques",
    "going",
    "tell",
    "optimize",
    "policy",
    "function",
    "pi",
    "policy",
    "iteration",
    "value",
    "iteration",
    "allow",
    "essentially",
    "iteratively",
    "walk",
    "um",
    "game",
    "markov",
    "decision",
    "process",
    "taking",
    "actions",
    "think",
    "going",
    "best",
    "assessing",
    "value",
    "action",
    "state",
    "actually",
    "kind",
    "refining",
    "iterating",
    "policy",
    "function",
    "value",
    "function",
    "really",
    "really",
    "powerful",
    "approach",
    "model",
    "system",
    "run",
    "kind",
    "computer",
    "kind",
    "determine",
    "learn",
    "best",
    "policy",
    "value",
    "kind",
    "special",
    "case",
    "dynamic",
    "programming",
    "relies",
    "bellman",
    "optimality",
    "condition",
    "value",
    "function",
    "going",
    "whole",
    "lecture",
    "blue",
    "part",
    "right",
    "going",
    "talk",
    "policy",
    "iteration",
    "value",
    "iteration",
    "essentially",
    "dynamic",
    "using",
    "dynamic",
    "programming",
    "value",
    "function",
    "satisfies",
    "bellman",
    "optimality",
    "condition",
    "probabilistic",
    "uh",
    "processes",
    "things",
    "maybe",
    "know",
    "like",
    "backgammon",
    "dice",
    "roll",
    "every",
    "turn",
    "deterministic",
    "systems",
    "like",
    "robot",
    "system",
    "car",
    "think",
    "human",
    "know",
    "reinforcement",
    "learning",
    "problem",
    "much",
    "continuous",
    "control",
    "problem",
    "case",
    "might",
    "nonlinear",
    "differential",
    "equation",
    "x",
    "dot",
    "equals",
    "f",
    "x",
    "comma",
    "u",
    "linear",
    "optimal",
    "control",
    "studied",
    "guess",
    "chapter",
    "8",
    "book",
    "linear",
    "quadratic",
    "regulators",
    "common",
    "filters",
    "things",
    "like",
    "optimal",
    "linear",
    "control",
    "problems",
    "special",
    "cases",
    "optimal",
    "control",
    "problem",
    "hamilton",
    "jacobi",
    "bellman",
    "equation",
    "relies",
    "bellman",
    "optimality",
    "use",
    "kind",
    "dynamic",
    "programming",
    "ideas",
    "solve",
    "optimal",
    "nonlinear",
    "control",
    "problems",
    "like",
    "point",
    "mathematically",
    "beautiful",
    "theory",
    "powerful",
    "around",
    "decades",
    "know",
    "kind",
    "textbook",
    "way",
    "thinking",
    "design",
    "optimal",
    "policies",
    "optimal",
    "controllers",
    "know",
    "markov",
    "decision",
    "processes",
    "control",
    "systems",
    "practice",
    "actually",
    "solving",
    "things",
    "dynamic",
    "programming",
    "ends",
    "usually",
    "amounting",
    "brute",
    "force",
    "search",
    "usually",
    "scalable",
    "high",
    "dimensional",
    "systems",
    "typically",
    "hard",
    "optimal",
    "hamilton",
    "jacobi",
    "bellman",
    "type",
    "control",
    "even",
    "moderately",
    "high",
    "dimensional",
    "system",
    "know",
    "system",
    "sometimes",
    "system",
    "maybe",
    "know",
    "heard",
    "special",
    "cases",
    "machine",
    "learning",
    "maybe",
    "10",
    "100",
    "dimensional",
    "system",
    "ca",
    "nonlinear",
    "fluid",
    "flow",
    "equations",
    "might",
    "know",
    "hundred",
    "thousand",
    "million",
    "dimensional",
    "differential",
    "equation",
    "write",
    "computer",
    "important",
    "caveat",
    "model",
    "based",
    "control",
    "lot",
    "going",
    "model",
    "free",
    "control",
    "uses",
    "ideas",
    "learned",
    "model",
    "based",
    "control",
    "even",
    "though",
    "know",
    "actually",
    "lot",
    "daily",
    "life",
    "reinforcement",
    "learning",
    "time",
    "good",
    "model",
    "system",
    "example",
    "chess",
    "model",
    "opponent",
    "example",
    "least",
    "ca",
    "write",
    "mathematically",
    "markov",
    "decision",
    "process",
    "ca",
    "really",
    "use",
    "techniques",
    "lot",
    "model",
    "free",
    "control",
    "reinforcement",
    "learning",
    "going",
    "kind",
    "approximate",
    "dynamic",
    "programming",
    "simultaneously",
    "learning",
    "kind",
    "dynamics",
    "learning",
    "update",
    "functions",
    "trial",
    "error",
    "without",
    "actually",
    "model",
    "reinforcement",
    "learning",
    "kind",
    "major",
    "dichotomy",
    "methods",
    "tell",
    "means",
    "little",
    "bit",
    "example",
    "parameterize",
    "policy",
    "pi",
    "variables",
    "theta",
    "know",
    "kind",
    "dependency",
    "variables",
    "theta",
    "might",
    "able",
    "take",
    "gradient",
    "reward",
    "function",
    "value",
    "function",
    "respect",
    "parameters",
    "directly",
    "speed",
    "optimization",
    "okay",
    "gradient",
    "based",
    "use",
    "usually",
    "going",
    "fastest",
    "efficient",
    "way",
    "things",
    "oftentimes",
    "gradient",
    "information",
    "playing",
    "games",
    "playing",
    "chess",
    "playing",
    "go",
    "ca",
    "compute",
    "derivative",
    "one",
    "game",
    "respect",
    "another",
    "hard",
    "least",
    "within",
    "gradient",
    "free",
    "okay",
    "lot",
    "dichotomies",
    "dichotomy",
    "dichotomy",
    "dichotomy",
    "within",
    "gradient",
    "free",
    "control",
    "idea",
    "sometimes",
    "policy",
    "policy",
    "really",
    "important",
    "uh",
    "distinction",
    "policy",
    "means",
    "let",
    "say",
    "playing",
    "bunch",
    "games",
    "chess",
    "trying",
    "learn",
    "optimal",
    "policy",
    "function",
    "optimal",
    "value",
    "function",
    "playing",
    "games",
    "chess",
    "iteratively",
    "kind",
    "refining",
    "estimate",
    "pi",
    "v",
    "policy",
    "means",
    "always",
    "play",
    "best",
    "game",
    "possible",
    "whatever",
    "think",
    "value",
    "function",
    "whatever",
    "think",
    "best",
    "policy",
    "possible",
    "always",
    "going",
    "use",
    "best",
    "policy",
    "play",
    "game",
    "going",
    "always",
    "try",
    "kind",
    "get",
    "reward",
    "system",
    "every",
    "game",
    "play",
    "means",
    "policy",
    "policy",
    "means",
    "well",
    "maybe",
    "try",
    "things",
    "maybe",
    "maybe",
    "know",
    "policy",
    "suboptimal",
    "going",
    "like",
    "random",
    "moves",
    "occasionally",
    "called",
    "policy",
    "think",
    "might",
    "really",
    "valuable",
    "learning",
    "information",
    "system",
    "uh",
    "policy",
    "methods",
    "include",
    "sarsa",
    "state",
    "action",
    "reward",
    "state",
    "action",
    "variants",
    "sarsa",
    "algorithm",
    "policy",
    "reinforcement",
    "learning",
    "tds",
    "mean",
    "temporal",
    "difference",
    "mc",
    "monte",
    "carlo",
    "whole",
    "family",
    "kind",
    "optimization",
    "techniques",
    "use",
    "different",
    "kind",
    "amounts",
    "history",
    "talk",
    "going",
    "whole",
    "lecture",
    "red",
    "box",
    "gradient",
    "free",
    "model",
    "free",
    "reinforcement",
    "learning",
    "policy",
    "version",
    "sarsa",
    "kind",
    "policy",
    "set",
    "algorithms",
    "policy",
    "variant",
    "called",
    "q",
    "learning",
    "quality",
    "function",
    "q",
    "kind",
    "joint",
    "value",
    "like",
    "particular",
    "state",
    "taking",
    "particular",
    "action",
    "quality",
    "function",
    "contains",
    "information",
    "optimal",
    "policy",
    "value",
    "function",
    "derived",
    "quality",
    "function",
    "really",
    "important",
    "distinction",
    "learn",
    "based",
    "quality",
    "function",
    "need",
    "model",
    "next",
    "state",
    "going",
    "quality",
    "function",
    "kind",
    "implicitly",
    "defines",
    "value",
    "know",
    "based",
    "going",
    "go",
    "future",
    "q",
    "learning",
    "really",
    "nice",
    "way",
    "learning",
    "model",
    "take",
    "policy",
    "information",
    "learn",
    "take",
    "controller",
    "see",
    "happens",
    "still",
    "learn",
    "get",
    "better",
    "policies",
    "better",
    "value",
    "functions",
    "future",
    "also",
    "really",
    "important",
    "want",
    "imitation",
    "learning",
    "want",
    "watch",
    "people",
    "play",
    "games",
    "chess",
    "even",
    "though",
    "know",
    "value",
    "function",
    "policy",
    "policy",
    "learning",
    "algorithms",
    "accumulate",
    "information",
    "estimate",
    "world",
    "every",
    "bit",
    "information",
    "get",
    "improves",
    "quality",
    "function",
    "improves",
    "next",
    "game",
    "going",
    "play",
    "really",
    "powerful",
    "would",
    "say",
    "nowadays",
    "know",
    "kind",
    "q",
    "learning",
    "world",
    "lot",
    "lot",
    "lot",
    "machine",
    "learning",
    "q",
    "learning",
    "reinforcement",
    "learning",
    "algorithms",
    "going",
    "talk",
    "much",
    "essentially",
    "would",
    "actually",
    "update",
    "parameters",
    "policy",
    "value",
    "function",
    "q",
    "function",
    "directly",
    "using",
    "kind",
    "gradient",
    "optimization",
    "sum",
    "future",
    "rewards",
    "function",
    "current",
    "parameters",
    "theta",
    "parameterize",
    "policy",
    "might",
    "able",
    "use",
    "gradient",
    "optimization",
    "things",
    "like",
    "newton",
    "steps",
    "steepest",
    "descent",
    "things",
    "like",
    "get",
    "good",
    "estimate",
    "ability",
    "going",
    "way",
    "way",
    "faster",
    "uh",
    "uh",
    "gradient",
    "free",
    "methods",
    "even",
    "term",
    "uh",
    "faster",
    "dynamic",
    "programming",
    "last",
    "piece",
    "kind",
    "last",
    "10",
    "years",
    "massive",
    "explosion",
    "deep",
    "reinforcement",
    "learning",
    "lot",
    "deep",
    "mind",
    "alphago",
    "know",
    "demonstrating",
    "machines",
    "computers",
    "play",
    "atari",
    "games",
    "human",
    "level",
    "performance",
    "beat",
    "grand",
    "masters",
    "go",
    "incredibly",
    "impressive",
    "demonstrations",
    "reinforcement",
    "learning",
    "use",
    "deep",
    "neural",
    "networks",
    "either",
    "learn",
    "model",
    "use",
    "reinforcement",
    "learning",
    "represent",
    "kind",
    "concepts",
    "like",
    "deep",
    "neural",
    "network",
    "quality",
    "function",
    "deep",
    "neural",
    "network",
    "policy",
    "differentiate",
    "respect",
    "network",
    "parameters",
    "uh",
    "using",
    "kind",
    "know",
    "auto",
    "diff",
    "back",
    "propagation",
    "uh",
    "gradient",
    "based",
    "optimization",
    "policy",
    "network",
    "would",
    "say",
    "deep",
    "model",
    "predictive",
    "control",
    "exactly",
    "fit",
    "reinforcement",
    "learning",
    "world",
    "would",
    "say",
    "know",
    "morally",
    "closely",
    "related",
    "deep",
    "model",
    "predictive",
    "control",
    "allows",
    "solve",
    "kind",
    "hard",
    "optimal",
    "nonlinear",
    "problems",
    "actually",
    "learn",
    "policy",
    "based",
    "model",
    "predictive",
    "controller",
    "actually",
    "essentially",
    "kind",
    "codify",
    "model",
    "predictive",
    "controller",
    "control",
    "policy",
    "finally",
    "uh",
    "actor",
    "critic",
    "methods",
    "um",
    "actor",
    "critic",
    "methods",
    "existed",
    "long",
    "deep",
    "reinforcement",
    "learning",
    "nowadays",
    "kind",
    "renewed",
    "interest",
    "uh",
    "uh",
    "train",
    "deep",
    "neural",
    "networks",
    "okay",
    "view",
    "see",
    "different",
    "categories",
    "reinforcement",
    "learning",
    "comprehensive",
    "absolutely",
    "hundred",
    "percent",
    "factually",
    "correct",
    "definitely",
    "know",
    "rough",
    "sketch",
    "main",
    "divides",
    "things",
    "need",
    "think",
    "choosing",
    "reinforcement",
    "learning",
    "algorithm",
    "model",
    "system",
    "use",
    "dynamic",
    "programming",
    "based",
    "bellman",
    "optimality",
    "model",
    "system",
    "either",
    "use",
    "gradient",
    "free",
    "gradient",
    "based",
    "methods",
    "policy",
    "policy",
    "variants",
    "depending",
    "know",
    "specific",
    "needs",
    "tends",
    "sarsa",
    "methods",
    "conservative",
    "q",
    "learning",
    "tend",
    "converge",
    "faster",
    "methods",
    "ways",
    "kind",
    "making",
    "powerful",
    "flexible",
    "representations",
    "using",
    "uh",
    "deep",
    "neural",
    "networks",
    "kind",
    "different",
    "focused",
    "ways",
    "okay",
    "next",
    "videos",
    "zoom",
    "know",
    "part",
    "markup",
    "decision",
    "processes",
    "policy",
    "iteration",
    "value",
    "iteration",
    "actually",
    "derive",
    "quality",
    "function",
    "uh",
    "talk",
    "model",
    "free",
    "control",
    "kind",
    "gradient",
    "free",
    "methods",
    "policy",
    "policy",
    "cue",
    "learning",
    "one",
    "important",
    "ones",
    "temporal",
    "difference",
    "learning",
    "actually",
    "lot",
    "neuro",
    "science",
    "analog",
    "learn",
    "animal",
    "brains",
    "people",
    "think",
    "know",
    "closely",
    "related",
    "td",
    "learning",
    "policies",
    "talk",
    "optimal",
    "nonlinear",
    "control",
    "hamilton",
    "jacobi",
    "bellman",
    "equation",
    "uh",
    "talk",
    "briefly",
    "policy",
    "gradient",
    "optimization",
    "know",
    "kind",
    "deep",
    "learning",
    "things",
    "uh",
    "pepper",
    "throughout",
    "deep",
    "learning",
    "maybe",
    "whole",
    "lecture",
    "deep",
    "learning",
    "methods",
    "coming",
    "really",
    "excited",
    "walk",
    "thank"
  ],
  "keywords": [
    "lecture",
    "reinforcement",
    "learning",
    "high",
    "kind",
    "uh",
    "really",
    "algorithms",
    "actually",
    "going",
    "next",
    "different",
    "like",
    "optimization",
    "theory",
    "optimal",
    "control",
    "think",
    "bellman",
    "equation",
    "hamilton",
    "jacobi",
    "way",
    "deep",
    "use",
    "powerful",
    "machine",
    "techniques",
    "solve",
    "problems",
    "essentially",
    "good",
    "environment",
    "talk",
    "want",
    "chapter",
    "one",
    "get",
    "learn",
    "also",
    "um",
    "know",
    "problem",
    "agent",
    "world",
    "set",
    "actions",
    "sometimes",
    "might",
    "action",
    "playing",
    "game",
    "chess",
    "even",
    "dimensional",
    "state",
    "system",
    "information",
    "current",
    "future",
    "rewards",
    "example",
    "reward",
    "go",
    "things",
    "systems",
    "policy",
    "called",
    "pi",
    "probability",
    "given",
    "probabilistic",
    "take",
    "usually",
    "model",
    "value",
    "function",
    "states",
    "people",
    "every",
    "possible",
    "games",
    "10",
    "maybe",
    "ca",
    "functions",
    "dynamic",
    "programming",
    "okay",
    "best",
    "always",
    "hard",
    "need",
    "dichotomy",
    "markov",
    "decision",
    "process",
    "based",
    "means",
    "iteration",
    "optimality",
    "whole",
    "using",
    "nonlinear",
    "important",
    "lot",
    "free",
    "methods",
    "gradient",
    "parameters",
    "say",
    "play",
    "sarsa",
    "q",
    "quality",
    "would",
    "neural",
    "network",
    "predictive"
  ]
}