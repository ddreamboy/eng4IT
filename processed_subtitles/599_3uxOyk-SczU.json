{
  "text": "Let's talk about\nDimensionality Reduction.\nSo, here,\nI've got a bunch of points, okay?\nAnd these points are in 2D, all right?\nIt's called kind of green and\nred, but that's sort of a joke.\nWe're not going to worry too\nmuch about why it was done that.\nWhat you should notice is\nthat in the middle here\nare these orangeish yellowish points.\nDo you see why it's R and G?\nRed plus green makes sort of yellowish,\nor so, so\nalong that diagonal\nit'll be kind of yellow.\nI don't know who did this originally,\nbut it's kind of cool.\nAll right,\nif you think about these orange points,\nlet's think about how\nthey're distributed, okay?\nSo, they have a have a mean sit\nsomewhere, and they've got a.\nSet of eigenvectors, right?\nThey've got a co-variants\nmatrix that describes them.\nThey have a axis of least inertia and\nthen the next axis.\nSo here you're seeing the points,\nthe orange points are x, x bar here\nis supposed to be their, their mean.\nAnd then we've got the big eigenvector\nv1 and then the smaller one v2.\nAnd the idea is.\nWe can represent those orange points\nby only their v1 coordinates,\nplus the mean.\nAnd what that would mean is haha.\nEssentially that we're going to\nthink of all the orange points\nas just being on that line.\nAnd all I'm going to tell you\nis where on the line they are.\nAnd we're going to essentially ignore\nthe amount that they're off that line.\nSo we've just reduced\nthe dimensions from how many?\nTwo.\nTo how many?\nOne.\nOkay, that doesn't sound like\nthat big a deal, nor is it.\nBut in higher dimensions,\nthis could be a huge deal.\nImagine you've got something in\n10,000-dimensional space, and yes,\nin just a minute, we're going to\ndo a 10,000-dimensional space.\nIf you said, well, I'm going to\nrepresent them by one number, or\neven 30, that would be a huge reduction.\nSo if I say, well,\nwhat direction does it vary the most in?\nAnd I just give you that value.\nIf that's good enough for\nwhat we want to do, you've reduced\nthe description from being a lot of\nnumbers to being a much smaller number.\nIn fact, we can sort of express that\nhere algebraically in terms of just,\nthinking about it,\nwhatever dimension x happens to be in.\nSo if I've got a whole bunch of data\npoints in some N-dimensional space,\nwhat I want to know is\nthe direction of projection.\nAnd we'll just say it's v.\nAll right,\nthat if I projected those points,\nafter subtracting out the mean,\nthat I'd have the greatest amount.\nRight, the greatest variation.\nAnd that's what that says here, right?\nSo take x, subtract out the mean,\ndotted with the v,\nsummed over all the x's, take the norm.\nTake the square.\nWell that can be written as\nan expression like this of just v\ntranspose Av,\nwhere A is just this outer product.\nOkay, that's the co-variants matrix\nthat we're, we're familiar with before.\nAnd as we said before.\nThe eigenvector with\nthe largest eigenvalue lambda\nis going to be the one that\ncaptures that greatest variation.\nIn a minute I'll give you a little\nargument about why it's the largest\neigenvector with the largest eigenvalue.\nOr you can just take my word for it.\nAnd, in fact, the smallest eigenvalue\nwould be the least amount of dimension,\nso basically, what we're\ngoing to have to do at some point\nis take the eigenvectors\nof this co-variance matrix.\n",
  "words": [
    "let",
    "talk",
    "dimensionality",
    "reduction",
    "got",
    "bunch",
    "points",
    "okay",
    "points",
    "2d",
    "right",
    "called",
    "kind",
    "green",
    "red",
    "sort",
    "joke",
    "going",
    "worry",
    "much",
    "done",
    "notice",
    "middle",
    "orangeish",
    "yellowish",
    "points",
    "see",
    "r",
    "g",
    "red",
    "plus",
    "green",
    "makes",
    "sort",
    "yellowish",
    "along",
    "diagonal",
    "kind",
    "yellow",
    "know",
    "originally",
    "kind",
    "cool",
    "right",
    "think",
    "orange",
    "points",
    "let",
    "think",
    "distributed",
    "okay",
    "mean",
    "sit",
    "somewhere",
    "got",
    "set",
    "eigenvectors",
    "right",
    "got",
    "matrix",
    "describes",
    "axis",
    "least",
    "inertia",
    "next",
    "axis",
    "seeing",
    "points",
    "orange",
    "points",
    "x",
    "x",
    "bar",
    "supposed",
    "mean",
    "got",
    "big",
    "eigenvector",
    "v1",
    "smaller",
    "one",
    "v2",
    "idea",
    "represent",
    "orange",
    "points",
    "v1",
    "coordinates",
    "plus",
    "mean",
    "would",
    "mean",
    "haha",
    "essentially",
    "going",
    "think",
    "orange",
    "points",
    "line",
    "going",
    "tell",
    "line",
    "going",
    "essentially",
    "ignore",
    "amount",
    "line",
    "reduced",
    "dimensions",
    "many",
    "two",
    "many",
    "one",
    "okay",
    "sound",
    "like",
    "big",
    "deal",
    "higher",
    "dimensions",
    "could",
    "huge",
    "deal",
    "imagine",
    "got",
    "something",
    "space",
    "yes",
    "minute",
    "going",
    "space",
    "said",
    "well",
    "going",
    "represent",
    "one",
    "number",
    "even",
    "30",
    "would",
    "huge",
    "reduction",
    "say",
    "well",
    "direction",
    "vary",
    "give",
    "value",
    "good",
    "enough",
    "want",
    "reduced",
    "description",
    "lot",
    "numbers",
    "much",
    "smaller",
    "number",
    "fact",
    "sort",
    "express",
    "algebraically",
    "terms",
    "thinking",
    "whatever",
    "dimension",
    "x",
    "happens",
    "got",
    "whole",
    "bunch",
    "data",
    "points",
    "space",
    "want",
    "know",
    "direction",
    "projection",
    "say",
    "right",
    "projected",
    "points",
    "subtracting",
    "mean",
    "greatest",
    "amount",
    "right",
    "greatest",
    "variation",
    "says",
    "right",
    "take",
    "x",
    "subtract",
    "mean",
    "dotted",
    "v",
    "summed",
    "x",
    "take",
    "norm",
    "take",
    "square",
    "well",
    "written",
    "expression",
    "like",
    "v",
    "transpose",
    "av",
    "outer",
    "product",
    "okay",
    "matrix",
    "familiar",
    "said",
    "eigenvector",
    "largest",
    "eigenvalue",
    "lambda",
    "going",
    "one",
    "captures",
    "greatest",
    "variation",
    "minute",
    "give",
    "little",
    "argument",
    "largest",
    "eigenvector",
    "largest",
    "eigenvalue",
    "take",
    "word",
    "fact",
    "smallest",
    "eigenvalue",
    "would",
    "least",
    "amount",
    "dimension",
    "basically",
    "going",
    "point",
    "take",
    "eigenvectors",
    "matrix"
  ],
  "keywords": [
    "let",
    "reduction",
    "got",
    "bunch",
    "points",
    "okay",
    "right",
    "kind",
    "green",
    "red",
    "sort",
    "going",
    "much",
    "yellowish",
    "plus",
    "know",
    "think",
    "orange",
    "mean",
    "eigenvectors",
    "matrix",
    "axis",
    "least",
    "x",
    "big",
    "eigenvector",
    "v1",
    "smaller",
    "one",
    "represent",
    "would",
    "essentially",
    "line",
    "amount",
    "reduced",
    "dimensions",
    "many",
    "like",
    "deal",
    "huge",
    "space",
    "minute",
    "said",
    "well",
    "number",
    "say",
    "direction",
    "give",
    "want",
    "fact",
    "dimension",
    "greatest",
    "variation",
    "take",
    "v",
    "largest",
    "eigenvalue"
  ]
}