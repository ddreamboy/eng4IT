{
  "text": "Principal component analysis is a\ntechnique used in machine learning to\nreduce dimension. In this video we are\ngoing to look at what it is, we'll write\nsome python code, and in the end there\nwill be an interesting exercise for you\nto work on.\nLet's say you're working on a building a\nmachine learning model to predict\nproperty prices.\nHere all the columns on the left hand\nside are the\nfeatures\nor the attributes of the property that\ndecides the price. So the column in green\nis actually your target variable.\nYou probably already know that the price\nof the home is mainly dependent on area,\nwhich town it is in, plot, etc.\nIt depends on how many bathrooms you\nhave in a property but not as much. For\nexample,\n2600 square foot home with two bathroom\nversus three bathroom, price won't be\nthat different.\nBut if you go from 2600 to 3000 square\nfeet price will be\nsignificantly different. So clearly area\nplot plays a more important role in\ndetermining the last price,\nbathroom plays a little bit of a role,\nand how about this particular column\ntrees nearby!\nWhether you have two trees nearby your\nhome or three trees nearby your home,\nyou don't care that much. So that column\nprobably doesn't impact\nthe last price at all or or it impacts\nit only a little bit.\nWhen you're solving\nreal life machine learning problems you\nwill have many columns- hundred, thousand,\ntwo thousand columns or features,\nand you need to\ndo something to identify the features\nwhich are very important.\nYou're working on let's say handwritten\ndigit classification;\nwhere you know you have digits which are\nwritten by hand and then you try to\nclassify as\none of the numbers from 0 to 9.\nHere\nthis image is represented as pixel. Let's\nsay this is a grade of 8 by 8 and every\nnumber presents the\ncolor. So 0 means black, 16 means highest\nwhite you know\nand since it's an 8 by 8 grid there are\ntotal 64 pixels or numbers that helps to\ndetermine what number it is. So this 64\npixels are called features and if you\nrepresent....\nNow if you think about uh\nsome of the pixels in these images you\nwill find that those pixels don't play\nany role at all,\nuh\nin figuring what is it what digit is it.\nFor example, these two pixels\nhere or pretty much any pixel in this\nlast column.\nNo matter, what number it is these pixels\nare always black. So we can say that\nthese pixels are not important feature.\nAnd what if we get rid of these features?\nWe get two benefits out of that. First\nthe training is faster. You know machine\nlearning training takes a lot of time,\nlot of compute resources. So you want to\nsave the training, you want to make it\nlittle lightweight\nand your inference can also be faster\nand data visualization becomes easier.\nLet's say you have 100 features and\nsomehow you reduce those 100 features\ninto only two features or three features,\nthen as a human\nyou can plot it on a 2D or 3D graph you\ncan visualize it and data visualization\nhelps\na great\nData visualization helps you a lot in\nterms of your final decision making\nuh with regards to what kind of model\nyou want to build.\nPCA is a process of figuring out the\nmost important features\nor the principal component that has a\nmaximum impact on a target variable.\nPCA will\ncreate in fact the new feature called\nprincipal component. You know PC1, PC2, and\nso on. \nSo again going back to the digits\nexample,\nLet's say I have 64\nfeatures out of that I am plotting only\ntwo features corner pixel and central\npixel.\nNow, here these cluster represent\ndifferent digits.\nYou immediately notice that the corner\npixel is not playing an important role.\nMaximum variation is on the y-axis or\nmaximum variance is on the y-axis, which\nis a central pixel. So if I ask you to\nreduce this two-dimension into one\ndimension, you can easily do so by\ngetting rid of corner pixel.\nSo this graph that I have drawn on the\nright hand side it is one dimension. I\nmean I know, the graph is not perfect\nperfect it still looks like two\ndimension but you get my point- to reduce\nthis from two dimension to one dimension\nis\neasier.\nLet's look at the\niris flower data set where sepal width\nand sepal height determine what kind of\nflower it is.\nIf you have a scatter plot like this\nyou can draw a line, which covers the\nmaximum variance. So this line covers the\nmaximum variance or the\nmaximum information in terms of features,\nand you can draw a perpendicular line\nwhich covers the second most\nvariance you know\nand these are called principal component.\nSo here PC1 is this\naxis which covers the most of the\nvariance, PC2 is the axis that covers\nthe second most variance.\nSo when you apply PCA, \nyou get a chart like this and I know\nI have done only\nthis graph for two dimension but really\nif you have 100 dimension, and if you\napply PCA you can figure out let's say\n10 most important piece uh principal\ncomponents.\nSo for 100 features, you can actually\ncreate 100 principal components in the\ndescending order of their impact on the\ntarget variable.\nSo for digits\nif I have to load this in a data frame,\npanda data frame will look like this.\nThere's a little error it's like pixel\n63 because I start with pixel 0,\nbut you already see\npixel 0 and 1 has\nall values is 0 they are not kind of\nimportant.\nSo when you use SKLearn library and\ncall PCA method where you say okay, n\ncomponents is six; you're basically\nasking PCA to extract the sixth most\nimportant component,\nand that will look something like this.\nSo what this is doing is calculating new\nfeatures,\nand these features are such that PC1\ncovers the most\nvariance in terms of features, you know\nin terms of information extraction from\nyour dataset. PC2 is the second most\nhighest component\nthat contains a lot of information about\nthe features of your data set.\nHere I give six, you can give it anything\nyou can have two three four five; you\nknow trial and error.\nYou can also\ngive a different\nparameter to this method, which will be\nlike 0.95, which which means you know\nget me 95 percent of information in\nterms of features, you know get me 95\npercent of variance. So you can do that\nyou will see that in the actual coding\nsection, but few things to keep in mind\nbefore using PCA is you need to scale\nthe features. Because if you don't scale\nit,\nit's not going to work okay. For example,\nhere is a chart,\nand if this graph is in millions and\nthis graph is in a normal value,\nyou know the graph might become skewed\nand PC and the PCA will not work as per\nyour expectation.\nAccuracy might drop. So it's a trade-off\nwhen you're trying to reduce.\nLet's say you have 100 features and if\nall 100 features are important and\nthey're trying to contribute to\nend target variable.\nIf you reduce from 100 to 5 you're going\nto lose a lot of information. So we'll\nsee all of that in the coding session,\nbut just to summarize PCA is called a\ndimensionality reduction technique\nas\nit helps us reduce dimension. We already\nsaw here in this case we had 64\ndimension.\nThis helped us to\nget to six dimension and when you have\nless columns your\ncomputation becomes much faster.\nAnd it also helps you to address the\ndimensionality curse problem. In machine\nlearning dimensionality curse is of\nis a big problem so many data sets,\nhaving so many\nso many columns, you know so many\ndimensions and that makes our model\nreally complex hard to visualize and PCA\nis an excellent technique that help you\ndeal with dimensionality curse.\nLet's get to python coding now.\nI opened my Jupyter notebook and\nimported some important libraries. I'm\ngoing to use the handwritten digits\ndataset from SKLearn\ndata set module.\nLet's\nload the data set by calling load digits\nfunction.\nWhen you do that,\nit loads the data set and you can\ncall\ndataset.keys to get an idea on you know\nwhat the dataset is all about.\nYou can see that dataset.data will\ncontain\nall the pixels\nand these are the feature names and this\nis the target array.\nRight!\nNow,\nlet's\nlook at dataset.data.shape.\nSo there are\n17 almost 1700 or 1800 samples. Every\nsample has\n64 columns. So if you look at the\nfirst sample\nit's a flat one-dimensional array of 64\npixels and if you want to visualize this\ndata using matplotlib library, you need\nto convert it into two dimensional array\nand the way you do that is by calling\nreshape function, which is because this\nis numpy array and when you do eight by\neight it just converts one dimensional\narray into\ntwo dimensional\narray of pixels.\nNow\nyou can import\nmatplotlib\nand\nuse\nthe plot, you know I'm gonna just plot a\ngray image\nand when you do plot dot mat show like\nmetric show,\nit will\nshow that it will show a picture of that\narray. What is my array? This is my array,\nright! See two dimensional array so I'm\ngonna just put ctrl c ctrl v\nand my first digit\nlooks like this. This is clearly zero. If\nI look at my\nsecond data sample,\nsee\nit is one,\nthen two\nand so on. You know like 50th sample will\nbe\nI think this is two.\nOkay,\nso this is how you visualize it.\nNow,\nlet's look at\nthe target. When you do data set dot\ntarget\nuh\nit's a huge array but it is your end\nclass you know if you do\nunique,\nyou will find\nthe way you do unique is by calling\nnp.unique\nand you see\nthe numbers are between range zero to\nnine. \nWe already saw let's say our\neighth number here\nis \nwhich number it is okay I can check it\nhere.\nSo if you\ndo eight,\nthis is the eighth number I mean I know\nit looks weird but let's do the tenth\nnumber.\nSo then number is nine.\nSee nine.\nSo this dataset.data\nis your feature and dataset.target is\nyour class basically. We are classifying\nthis\nin 10 classes, 0\nto 9.\nOkay, let me create a data frame now, so I\ncan do pd dot DataFrame\nand just supply data set dot data, and\nthat will create a data frame for\nyou. But\nthe\ncolumn\nheaders are you know randomly assigned.\nSo I'm going to\ndo\ncolumns,\nyou can say data set dot feature names\nand the feature names are all the pixels,\nyou know\nlike zero throw zero pixel zero throw\nfirst pixel and so on\nand I'll just save that\ninto a data frame\nobject.\nOkay, my data frame object is ready.\nNow\nuh\nI mean I know from this data set the\nvalues are between 0 to 16, but if you\nwant to confirm you can do df dot\ndescribe.\nAnd for each of the columns if you look\nat min and max;\nI mean this column is useless but if you\nlook at mean and max for most of the\ngood pixels,\nit will be between 0 and 16. \nOkay\num so\n0 is my minimum value 16 is my highest\nvalue.\nNow I'm going to\nstore this in x and y. So df is my x and\nmy y\nis my,\nyou know target set.\nSo again what is my x? x is my data frame.\nwhat is my y? is my\ntarget\nclasses, you know the digit which tells\nyou 0 to 9. \nNow,\nI would like to scale these features\nbefore building my machine learning\nmodel, so I'm just going to use the\nstandard scalar. You know scaling is a\ncommon thing that I do before training\nmy model,\nand\nskeleton provides very convenient API to\ndo that. When you do scalar dot\nfit transform it will just scale that x\nthing and you can\nstore this into x scale. So when you\nprint now\nx\nscale,\nyou will see the values\nare\nstand their scale you know using the\nstandard scalar. I think negative one to\none. You can use min max scalar as well.\nOkay,\nnow we are ready to train the model. So\nwe are going to do our usual thing which\nis train this split. You already know\nthis by this by this point\nand\nI'm going to\ncall train test split\non my x scale.\nSo x\nscale\nand\ny, and I will keep my test size to be 20%,\nyou can do 30% whatever and I'm just\nkeeping random state for reproducibility.\nIf you don't want to keep it is fine\nand in the result what I get is x train\nx test\ny train y test.\nThis should be in your muscle memory\nfriends, if you're doing machine learning\nfor some time. \nNow, \nI am ready to train my model. Well I'm\ngoing to use logistic regression. You can\nuse\nrandom forest decision tree. There are so\nmany classification techniques but i'll\nstart with logistic\nregression. I will create a model, which\nis an object of this class\nand then model dot fit,\nAgain pretty standard\nand model or score. So I do model score\nto\nmeasure the accuracy.\n97 percent! See this is pretty amazing.\nI get 97% accuracy\nuh by training on all these\nfeatures. But now let's do PCA. So to do\nPCA you can import PCA from SKLearn on\ndecomposition,\nand then\nI will create a PCA variable. So let me\nincrease my font size. It's very simple\nPCA and here you supply two type of\narguments, you can either say\nhow many components you want. You can say\ntwo five trial and error you can figure\nit out. But what I'm going to do is I'm\ngoing to supply 0.95.\nWhat this will tell PCA is that,\nretain 95 percent of useful features\nand then\ncreate new dimensions.\nYou're not explicitly saying I want only\ntwo dimensional as an output, you're\nsaying\ngive me retain 95% of information,\nretain 95 percent variation and then\nwhatever features you come up with I'm\nhappy with that,\nand then\nPCA dot\nfit\ntransform.\nThe API is quite simple.\nYou supply your data frame here, you get\na new data frame with new feature, new\nprincipal components\nand when you do\nxpca dot shape, see\nnow you've got only 29 columns.\nIf you\nlook at your original data frame,\nyou had 64 columns. So 64 columns to 29.\nIt probably got rid of\nunnecessary things, such as this guy for\nexample.\nZeroth pixel! All values are zero. See\nlook at here this feature is not needed\nat all. So it probably got rid of that\nand it\nit did not, by the way just to make it\nclear it's not like it will pick 29\ncolumns out of it. It will calculate\nnew columns okay. So if you do x\npca you see,\nit\nactually calculated\nnew columns\nfor you.\nAnd if you do\nPCA variable dot\nexplained\nvariance\nratio, \nit will tell you that my first column is\ncapturing you know 14 percent of\nvariation or 14 of\nuseful information from my data set.\nPC2 second component\nis capturing 13 percent of information\nand so on.\nSo this is how\nyou\nprint explained plane variation ratio. There is\nanother thing called n components. It\nwill tell you how many components you\ngot, which is basically your columns you\nknow 29.\nSo here PCA help me reduce my columns\nfrom 64 to 29. All these 29\ncolumns are computed column. They're the\nnew feature,\nbut good news is you can now use\nthis\nuh new data frame to train your model. So\nI'm going to do trend test split once\nagain, but this time I will supply xpca,\ny remains for high then my test size is\n0.2\nand I'll just\nkeep my random state to be\nthis\nand as a result what you get is\nyour train and test\ndata sets.\nAnd I'm going to use again same linear\nregression\nlogistic regression sorry\nand I will say\nlogistic regression.\nThis time I will fit on PCA\nand then model\ndots or\nand y is\nsay. I got pretty good accuracy 97\npercent but let's say\nthis failed to converge because\ntotal number of iterations\nlimit reached. So I need to supply some\nmaximum iteration, so I will say\nmax\noter\nso I need to increase my iteration\nbasically, to\nso that the\ngradient descent can converge.\nSee almost same accuracy. So you are\nrealizing that\ndespite dropping lot of features my\naccuracy is almost same. Here is\n97.22, here is\n96.994. So it's\nalmost\nsimilar.\nOkay.\nNow you can\num create PCA by supplying the\ncomponents explicitly. So I can say okay\nPCA,\nlet's say\nmy\nnumber of components is 2.\nNow this is what is going to happen when\nyou explicitly say my components are 2.\nSo I'm going to do PCA transform with my\nx\nxpca.shape.\nNow\nwe are telling\nthat\nwe want two components.\nOkay so it\nit will identify the two most important\nfeatures\nbut\nhere,\nsee let me see,\nso see every data point is just two\nfeatures. Now did it capture enough\ninformation.\nWe are not sure. When you print explain\nvariation variance ratio,\nyou are capturing 14% and 30% around 27 28\npercent information. So naturally when\nyou use\nthis data set to train your model,\nthe accuracy is going to be low. So I'm\njust going to\nlike same code but I'm using the new PCA,\nand the new data frame and when you\ntrain the model you see the accuracy is\nreduced to 60, 60.83. So\nyou take a hit, but\nthe benefit you got here is\nyour\nuh features are only two. So your\ncomputers can be computation can be very\nfast.\nSo you can decide what accuracy do you\nwant and then you can\nreduce your dimensions accordingly.\nSo that's all I had for this video. Now\nlet's move on to the most important part\nof\nthis\ntutorial, which is an exercise. I'm going\nto provide a link of this exercise below\nbut it's very important friends that you\nwork on exercise and I have given a very\ngood exercise, there is a data set\nwhich you can download from Kaggle or you\ncan see if you click on here, there is\nthis heart csv and so there is a data\nset for\nheart disease and using this data set\nyou need to predict\nif the person has heart disease or not\nand you are going to use PCA. So work on\nall these six aspects of\nthe exercise and let me know how it goes\nin the comment post below; your solution,\nyour github link, whatever and if you\nhave any question\npost in a comment below, and if you like\nthis video\nshare it with your friends. You know so\nthat they can learn, everything is for\nfree friends exercise explanation\nall my youtube videos are free, so you\ncan have\nlearning at you know at almost no cost.\nSo please share it with your friends and\ngive it a thumbs up if you like this.\nThank you\n",
  "words": [
    "principal",
    "component",
    "analysis",
    "technique",
    "used",
    "machine",
    "learning",
    "reduce",
    "dimension",
    "video",
    "going",
    "look",
    "write",
    "python",
    "code",
    "end",
    "interesting",
    "exercise",
    "work",
    "let",
    "say",
    "working",
    "building",
    "machine",
    "learning",
    "model",
    "predict",
    "property",
    "prices",
    "columns",
    "left",
    "hand",
    "side",
    "features",
    "attributes",
    "property",
    "decides",
    "price",
    "column",
    "green",
    "actually",
    "target",
    "variable",
    "probably",
    "already",
    "know",
    "price",
    "home",
    "mainly",
    "dependent",
    "area",
    "town",
    "plot",
    "etc",
    "depends",
    "many",
    "bathrooms",
    "property",
    "much",
    "example",
    "2600",
    "square",
    "foot",
    "home",
    "two",
    "bathroom",
    "versus",
    "three",
    "bathroom",
    "price",
    "wo",
    "different",
    "go",
    "2600",
    "3000",
    "square",
    "feet",
    "price",
    "significantly",
    "different",
    "clearly",
    "area",
    "plot",
    "plays",
    "important",
    "role",
    "determining",
    "last",
    "price",
    "bathroom",
    "plays",
    "little",
    "bit",
    "role",
    "particular",
    "column",
    "trees",
    "nearby",
    "whether",
    "two",
    "trees",
    "nearby",
    "home",
    "three",
    "trees",
    "nearby",
    "home",
    "care",
    "much",
    "column",
    "probably",
    "impact",
    "last",
    "price",
    "impacts",
    "little",
    "bit",
    "solving",
    "real",
    "life",
    "machine",
    "learning",
    "problems",
    "many",
    "hundred",
    "thousand",
    "two",
    "thousand",
    "columns",
    "features",
    "need",
    "something",
    "identify",
    "features",
    "important",
    "working",
    "let",
    "say",
    "handwritten",
    "digit",
    "classification",
    "know",
    "digits",
    "written",
    "hand",
    "try",
    "classify",
    "one",
    "numbers",
    "0",
    "image",
    "represented",
    "pixel",
    "let",
    "say",
    "grade",
    "8",
    "8",
    "every",
    "number",
    "presents",
    "color",
    "0",
    "means",
    "black",
    "16",
    "means",
    "highest",
    "white",
    "know",
    "since",
    "8",
    "8",
    "grid",
    "total",
    "64",
    "pixels",
    "numbers",
    "helps",
    "determine",
    "number",
    "64",
    "pixels",
    "called",
    "features",
    "represent",
    "think",
    "uh",
    "pixels",
    "images",
    "find",
    "pixels",
    "play",
    "role",
    "uh",
    "figuring",
    "digit",
    "example",
    "two",
    "pixels",
    "pretty",
    "much",
    "pixel",
    "last",
    "column",
    "matter",
    "number",
    "pixels",
    "always",
    "black",
    "say",
    "pixels",
    "important",
    "feature",
    "get",
    "rid",
    "features",
    "get",
    "two",
    "benefits",
    "first",
    "training",
    "faster",
    "know",
    "machine",
    "learning",
    "training",
    "takes",
    "lot",
    "time",
    "lot",
    "compute",
    "resources",
    "want",
    "save",
    "training",
    "want",
    "make",
    "little",
    "lightweight",
    "inference",
    "also",
    "faster",
    "data",
    "visualization",
    "becomes",
    "easier",
    "let",
    "say",
    "100",
    "features",
    "somehow",
    "reduce",
    "100",
    "features",
    "two",
    "features",
    "three",
    "features",
    "human",
    "plot",
    "2d",
    "3d",
    "graph",
    "visualize",
    "data",
    "visualization",
    "helps",
    "great",
    "data",
    "visualization",
    "helps",
    "lot",
    "terms",
    "final",
    "decision",
    "making",
    "uh",
    "regards",
    "kind",
    "model",
    "want",
    "build",
    "pca",
    "process",
    "figuring",
    "important",
    "features",
    "principal",
    "component",
    "maximum",
    "impact",
    "target",
    "variable",
    "pca",
    "create",
    "fact",
    "new",
    "feature",
    "called",
    "principal",
    "component",
    "know",
    "pc1",
    "pc2",
    "going",
    "back",
    "digits",
    "example",
    "let",
    "say",
    "64",
    "features",
    "plotting",
    "two",
    "features",
    "corner",
    "pixel",
    "central",
    "pixel",
    "cluster",
    "represent",
    "different",
    "digits",
    "immediately",
    "notice",
    "corner",
    "pixel",
    "playing",
    "important",
    "role",
    "maximum",
    "variation",
    "maximum",
    "variance",
    "central",
    "pixel",
    "ask",
    "reduce",
    "one",
    "dimension",
    "easily",
    "getting",
    "rid",
    "corner",
    "pixel",
    "graph",
    "drawn",
    "right",
    "hand",
    "side",
    "one",
    "dimension",
    "mean",
    "know",
    "graph",
    "perfect",
    "perfect",
    "still",
    "looks",
    "like",
    "two",
    "dimension",
    "get",
    "reduce",
    "two",
    "dimension",
    "one",
    "dimension",
    "easier",
    "let",
    "look",
    "iris",
    "flower",
    "data",
    "set",
    "sepal",
    "width",
    "sepal",
    "height",
    "determine",
    "kind",
    "flower",
    "scatter",
    "plot",
    "like",
    "draw",
    "line",
    "covers",
    "maximum",
    "variance",
    "line",
    "covers",
    "maximum",
    "variance",
    "maximum",
    "information",
    "terms",
    "features",
    "draw",
    "perpendicular",
    "line",
    "covers",
    "second",
    "variance",
    "know",
    "called",
    "principal",
    "component",
    "pc1",
    "axis",
    "covers",
    "variance",
    "pc2",
    "axis",
    "covers",
    "second",
    "variance",
    "apply",
    "pca",
    "get",
    "chart",
    "like",
    "know",
    "done",
    "graph",
    "two",
    "dimension",
    "really",
    "100",
    "dimension",
    "apply",
    "pca",
    "figure",
    "let",
    "say",
    "10",
    "important",
    "piece",
    "uh",
    "principal",
    "components",
    "100",
    "features",
    "actually",
    "create",
    "100",
    "principal",
    "components",
    "descending",
    "order",
    "impact",
    "target",
    "variable",
    "digits",
    "load",
    "data",
    "frame",
    "panda",
    "data",
    "frame",
    "look",
    "like",
    "little",
    "error",
    "like",
    "pixel",
    "63",
    "start",
    "pixel",
    "0",
    "already",
    "see",
    "pixel",
    "0",
    "1",
    "values",
    "0",
    "kind",
    "important",
    "use",
    "sklearn",
    "library",
    "call",
    "pca",
    "method",
    "say",
    "okay",
    "n",
    "components",
    "six",
    "basically",
    "asking",
    "pca",
    "extract",
    "sixth",
    "important",
    "component",
    "look",
    "something",
    "like",
    "calculating",
    "new",
    "features",
    "features",
    "pc1",
    "covers",
    "variance",
    "terms",
    "features",
    "know",
    "terms",
    "information",
    "extraction",
    "dataset",
    "pc2",
    "second",
    "highest",
    "component",
    "contains",
    "lot",
    "information",
    "features",
    "data",
    "set",
    "give",
    "six",
    "give",
    "anything",
    "two",
    "three",
    "four",
    "five",
    "know",
    "trial",
    "error",
    "also",
    "give",
    "different",
    "parameter",
    "method",
    "like",
    "means",
    "know",
    "get",
    "95",
    "percent",
    "information",
    "terms",
    "features",
    "know",
    "get",
    "95",
    "percent",
    "variance",
    "see",
    "actual",
    "coding",
    "section",
    "things",
    "keep",
    "mind",
    "using",
    "pca",
    "need",
    "scale",
    "features",
    "scale",
    "going",
    "work",
    "okay",
    "example",
    "chart",
    "graph",
    "millions",
    "graph",
    "normal",
    "value",
    "know",
    "graph",
    "might",
    "become",
    "skewed",
    "pc",
    "pca",
    "work",
    "per",
    "expectation",
    "accuracy",
    "might",
    "drop",
    "trying",
    "reduce",
    "let",
    "say",
    "100",
    "features",
    "100",
    "features",
    "important",
    "trying",
    "contribute",
    "end",
    "target",
    "variable",
    "reduce",
    "100",
    "5",
    "going",
    "lose",
    "lot",
    "information",
    "see",
    "coding",
    "session",
    "summarize",
    "pca",
    "called",
    "dimensionality",
    "reduction",
    "technique",
    "helps",
    "us",
    "reduce",
    "dimension",
    "already",
    "saw",
    "case",
    "64",
    "dimension",
    "helped",
    "us",
    "get",
    "six",
    "dimension",
    "less",
    "columns",
    "computation",
    "becomes",
    "much",
    "faster",
    "also",
    "helps",
    "address",
    "dimensionality",
    "curse",
    "problem",
    "machine",
    "learning",
    "dimensionality",
    "curse",
    "big",
    "problem",
    "many",
    "data",
    "sets",
    "many",
    "many",
    "columns",
    "know",
    "many",
    "dimensions",
    "makes",
    "model",
    "really",
    "complex",
    "hard",
    "visualize",
    "pca",
    "excellent",
    "technique",
    "help",
    "deal",
    "dimensionality",
    "curse",
    "let",
    "get",
    "python",
    "coding",
    "opened",
    "jupyter",
    "notebook",
    "imported",
    "important",
    "libraries",
    "going",
    "use",
    "handwritten",
    "digits",
    "dataset",
    "sklearn",
    "data",
    "set",
    "module",
    "let",
    "load",
    "data",
    "set",
    "calling",
    "load",
    "digits",
    "function",
    "loads",
    "data",
    "set",
    "call",
    "get",
    "idea",
    "know",
    "dataset",
    "see",
    "contain",
    "pixels",
    "feature",
    "names",
    "target",
    "array",
    "right",
    "let",
    "look",
    "17",
    "almost",
    "1700",
    "1800",
    "samples",
    "every",
    "sample",
    "64",
    "columns",
    "look",
    "first",
    "sample",
    "flat",
    "array",
    "64",
    "pixels",
    "want",
    "visualize",
    "data",
    "using",
    "matplotlib",
    "library",
    "need",
    "convert",
    "two",
    "dimensional",
    "array",
    "way",
    "calling",
    "reshape",
    "function",
    "numpy",
    "array",
    "eight",
    "eight",
    "converts",
    "one",
    "dimensional",
    "array",
    "two",
    "dimensional",
    "array",
    "pixels",
    "import",
    "matplotlib",
    "use",
    "plot",
    "know",
    "gon",
    "na",
    "plot",
    "gray",
    "image",
    "plot",
    "dot",
    "mat",
    "show",
    "like",
    "metric",
    "show",
    "show",
    "show",
    "picture",
    "array",
    "array",
    "array",
    "right",
    "see",
    "two",
    "dimensional",
    "array",
    "gon",
    "na",
    "put",
    "ctrl",
    "c",
    "ctrl",
    "v",
    "first",
    "digit",
    "looks",
    "like",
    "clearly",
    "zero",
    "look",
    "second",
    "data",
    "sample",
    "see",
    "one",
    "two",
    "know",
    "like",
    "50th",
    "sample",
    "think",
    "two",
    "okay",
    "visualize",
    "let",
    "look",
    "target",
    "data",
    "set",
    "dot",
    "target",
    "uh",
    "huge",
    "array",
    "end",
    "class",
    "know",
    "unique",
    "find",
    "way",
    "unique",
    "calling",
    "see",
    "numbers",
    "range",
    "zero",
    "nine",
    "already",
    "saw",
    "let",
    "say",
    "eighth",
    "number",
    "number",
    "okay",
    "check",
    "eight",
    "eighth",
    "number",
    "mean",
    "know",
    "looks",
    "weird",
    "let",
    "tenth",
    "number",
    "number",
    "nine",
    "see",
    "nine",
    "feature",
    "class",
    "basically",
    "classifying",
    "10",
    "classes",
    "0",
    "okay",
    "let",
    "create",
    "data",
    "frame",
    "pd",
    "dot",
    "dataframe",
    "supply",
    "data",
    "set",
    "dot",
    "data",
    "create",
    "data",
    "frame",
    "column",
    "headers",
    "know",
    "randomly",
    "assigned",
    "going",
    "columns",
    "say",
    "data",
    "set",
    "dot",
    "feature",
    "names",
    "feature",
    "names",
    "pixels",
    "know",
    "like",
    "zero",
    "throw",
    "zero",
    "pixel",
    "zero",
    "throw",
    "first",
    "pixel",
    "save",
    "data",
    "frame",
    "object",
    "okay",
    "data",
    "frame",
    "object",
    "ready",
    "uh",
    "mean",
    "know",
    "data",
    "set",
    "values",
    "0",
    "16",
    "want",
    "confirm",
    "df",
    "dot",
    "describe",
    "columns",
    "look",
    "min",
    "max",
    "mean",
    "column",
    "useless",
    "look",
    "mean",
    "max",
    "good",
    "pixels",
    "0",
    "okay",
    "um",
    "0",
    "minimum",
    "value",
    "16",
    "highest",
    "value",
    "going",
    "store",
    "x",
    "df",
    "x",
    "know",
    "target",
    "set",
    "x",
    "x",
    "data",
    "frame",
    "target",
    "classes",
    "know",
    "digit",
    "tells",
    "0",
    "would",
    "like",
    "scale",
    "features",
    "building",
    "machine",
    "learning",
    "model",
    "going",
    "use",
    "standard",
    "scalar",
    "know",
    "scaling",
    "common",
    "thing",
    "training",
    "model",
    "skeleton",
    "provides",
    "convenient",
    "api",
    "scalar",
    "dot",
    "fit",
    "transform",
    "scale",
    "x",
    "thing",
    "store",
    "x",
    "scale",
    "print",
    "x",
    "scale",
    "see",
    "values",
    "stand",
    "scale",
    "know",
    "using",
    "standard",
    "scalar",
    "think",
    "negative",
    "one",
    "one",
    "use",
    "min",
    "max",
    "scalar",
    "well",
    "okay",
    "ready",
    "train",
    "model",
    "going",
    "usual",
    "thing",
    "train",
    "split",
    "already",
    "know",
    "point",
    "going",
    "call",
    "train",
    "test",
    "split",
    "x",
    "scale",
    "x",
    "scale",
    "keep",
    "test",
    "size",
    "20",
    "30",
    "whatever",
    "keeping",
    "random",
    "state",
    "reproducibility",
    "want",
    "keep",
    "fine",
    "result",
    "get",
    "x",
    "train",
    "x",
    "test",
    "train",
    "test",
    "muscle",
    "memory",
    "friends",
    "machine",
    "learning",
    "time",
    "ready",
    "train",
    "model",
    "well",
    "going",
    "use",
    "logistic",
    "regression",
    "use",
    "random",
    "forest",
    "decision",
    "tree",
    "many",
    "classification",
    "techniques",
    "start",
    "logistic",
    "regression",
    "create",
    "model",
    "object",
    "class",
    "model",
    "dot",
    "fit",
    "pretty",
    "standard",
    "model",
    "score",
    "model",
    "score",
    "measure",
    "accuracy",
    "97",
    "percent",
    "see",
    "pretty",
    "amazing",
    "get",
    "97",
    "accuracy",
    "uh",
    "training",
    "features",
    "let",
    "pca",
    "pca",
    "import",
    "pca",
    "sklearn",
    "decomposition",
    "create",
    "pca",
    "variable",
    "let",
    "increase",
    "font",
    "size",
    "simple",
    "pca",
    "supply",
    "two",
    "type",
    "arguments",
    "either",
    "say",
    "many",
    "components",
    "want",
    "say",
    "two",
    "five",
    "trial",
    "error",
    "figure",
    "going",
    "going",
    "supply",
    "tell",
    "pca",
    "retain",
    "95",
    "percent",
    "useful",
    "features",
    "create",
    "new",
    "dimensions",
    "explicitly",
    "saying",
    "want",
    "two",
    "dimensional",
    "output",
    "saying",
    "give",
    "retain",
    "95",
    "information",
    "retain",
    "95",
    "percent",
    "variation",
    "whatever",
    "features",
    "come",
    "happy",
    "pca",
    "dot",
    "fit",
    "transform",
    "api",
    "quite",
    "simple",
    "supply",
    "data",
    "frame",
    "get",
    "new",
    "data",
    "frame",
    "new",
    "feature",
    "new",
    "principal",
    "components",
    "xpca",
    "dot",
    "shape",
    "see",
    "got",
    "29",
    "columns",
    "look",
    "original",
    "data",
    "frame",
    "64",
    "columns",
    "64",
    "columns",
    "probably",
    "got",
    "rid",
    "unnecessary",
    "things",
    "guy",
    "example",
    "zeroth",
    "pixel",
    "values",
    "zero",
    "see",
    "look",
    "feature",
    "needed",
    "probably",
    "got",
    "rid",
    "way",
    "make",
    "clear",
    "like",
    "pick",
    "29",
    "columns",
    "calculate",
    "new",
    "columns",
    "okay",
    "x",
    "pca",
    "see",
    "actually",
    "calculated",
    "new",
    "columns",
    "pca",
    "variable",
    "dot",
    "explained",
    "variance",
    "ratio",
    "tell",
    "first",
    "column",
    "capturing",
    "know",
    "14",
    "percent",
    "variation",
    "14",
    "useful",
    "information",
    "data",
    "set",
    "pc2",
    "second",
    "component",
    "capturing",
    "13",
    "percent",
    "information",
    "print",
    "explained",
    "plane",
    "variation",
    "ratio",
    "another",
    "thing",
    "called",
    "n",
    "components",
    "tell",
    "many",
    "components",
    "got",
    "basically",
    "columns",
    "know",
    "pca",
    "help",
    "reduce",
    "columns",
    "64",
    "29",
    "columns",
    "computed",
    "column",
    "new",
    "feature",
    "good",
    "news",
    "use",
    "uh",
    "new",
    "data",
    "frame",
    "train",
    "model",
    "going",
    "trend",
    "test",
    "split",
    "time",
    "supply",
    "xpca",
    "remains",
    "high",
    "test",
    "size",
    "keep",
    "random",
    "state",
    "result",
    "get",
    "train",
    "test",
    "data",
    "sets",
    "going",
    "use",
    "linear",
    "regression",
    "logistic",
    "regression",
    "sorry",
    "say",
    "logistic",
    "regression",
    "time",
    "fit",
    "pca",
    "model",
    "dots",
    "say",
    "got",
    "pretty",
    "good",
    "accuracy",
    "97",
    "percent",
    "let",
    "say",
    "failed",
    "converge",
    "total",
    "number",
    "iterations",
    "limit",
    "reached",
    "need",
    "supply",
    "maximum",
    "iteration",
    "say",
    "max",
    "oter",
    "need",
    "increase",
    "iteration",
    "basically",
    "gradient",
    "descent",
    "converge",
    "see",
    "almost",
    "accuracy",
    "realizing",
    "despite",
    "dropping",
    "lot",
    "features",
    "accuracy",
    "almost",
    "almost",
    "similar",
    "okay",
    "um",
    "create",
    "pca",
    "supplying",
    "components",
    "explicitly",
    "say",
    "okay",
    "pca",
    "let",
    "say",
    "number",
    "components",
    "going",
    "happen",
    "explicitly",
    "say",
    "components",
    "going",
    "pca",
    "transform",
    "x",
    "telling",
    "want",
    "two",
    "components",
    "okay",
    "identify",
    "two",
    "important",
    "features",
    "see",
    "let",
    "see",
    "see",
    "every",
    "data",
    "point",
    "two",
    "features",
    "capture",
    "enough",
    "information",
    "sure",
    "print",
    "explain",
    "variation",
    "variance",
    "ratio",
    "capturing",
    "14",
    "30",
    "around",
    "27",
    "28",
    "percent",
    "information",
    "naturally",
    "use",
    "data",
    "set",
    "train",
    "model",
    "accuracy",
    "going",
    "low",
    "going",
    "like",
    "code",
    "using",
    "new",
    "pca",
    "new",
    "data",
    "frame",
    "train",
    "model",
    "see",
    "accuracy",
    "reduced",
    "60",
    "take",
    "hit",
    "benefit",
    "got",
    "uh",
    "features",
    "two",
    "computers",
    "computation",
    "fast",
    "decide",
    "accuracy",
    "want",
    "reduce",
    "dimensions",
    "accordingly",
    "video",
    "let",
    "move",
    "important",
    "part",
    "tutorial",
    "exercise",
    "going",
    "provide",
    "link",
    "exercise",
    "important",
    "friends",
    "work",
    "exercise",
    "given",
    "good",
    "exercise",
    "data",
    "set",
    "download",
    "kaggle",
    "see",
    "click",
    "heart",
    "csv",
    "data",
    "set",
    "heart",
    "disease",
    "using",
    "data",
    "set",
    "need",
    "predict",
    "person",
    "heart",
    "disease",
    "going",
    "use",
    "pca",
    "work",
    "six",
    "aspects",
    "exercise",
    "let",
    "know",
    "goes",
    "comment",
    "post",
    "solution",
    "github",
    "link",
    "whatever",
    "question",
    "post",
    "comment",
    "like",
    "video",
    "share",
    "friends",
    "know",
    "learn",
    "everything",
    "free",
    "friends",
    "exercise",
    "explanation",
    "youtube",
    "videos",
    "free",
    "learning",
    "know",
    "almost",
    "cost",
    "please",
    "share",
    "friends",
    "give",
    "thumbs",
    "like",
    "thank"
  ],
  "keywords": [
    "principal",
    "component",
    "technique",
    "machine",
    "learning",
    "reduce",
    "dimension",
    "video",
    "going",
    "look",
    "end",
    "exercise",
    "work",
    "let",
    "say",
    "model",
    "property",
    "columns",
    "hand",
    "features",
    "price",
    "column",
    "actually",
    "target",
    "variable",
    "probably",
    "already",
    "know",
    "home",
    "plot",
    "many",
    "much",
    "example",
    "two",
    "bathroom",
    "three",
    "different",
    "important",
    "role",
    "last",
    "little",
    "trees",
    "nearby",
    "impact",
    "need",
    "digit",
    "digits",
    "one",
    "numbers",
    "0",
    "pixel",
    "8",
    "every",
    "number",
    "means",
    "16",
    "highest",
    "64",
    "pixels",
    "helps",
    "called",
    "think",
    "uh",
    "pretty",
    "feature",
    "get",
    "rid",
    "first",
    "training",
    "faster",
    "lot",
    "time",
    "want",
    "also",
    "data",
    "visualization",
    "100",
    "graph",
    "visualize",
    "terms",
    "kind",
    "pca",
    "maximum",
    "create",
    "new",
    "pc1",
    "pc2",
    "corner",
    "variation",
    "variance",
    "right",
    "mean",
    "looks",
    "like",
    "set",
    "line",
    "covers",
    "information",
    "second",
    "components",
    "load",
    "frame",
    "error",
    "see",
    "values",
    "use",
    "sklearn",
    "call",
    "okay",
    "six",
    "basically",
    "dataset",
    "give",
    "95",
    "percent",
    "coding",
    "keep",
    "using",
    "scale",
    "value",
    "accuracy",
    "dimensionality",
    "curse",
    "dimensions",
    "calling",
    "names",
    "array",
    "almost",
    "sample",
    "dimensional",
    "way",
    "eight",
    "dot",
    "show",
    "zero",
    "class",
    "nine",
    "supply",
    "object",
    "ready",
    "max",
    "good",
    "x",
    "standard",
    "scalar",
    "thing",
    "fit",
    "transform",
    "print",
    "train",
    "split",
    "test",
    "size",
    "whatever",
    "random",
    "friends",
    "logistic",
    "regression",
    "97",
    "tell",
    "retain",
    "explicitly",
    "got",
    "29",
    "ratio",
    "capturing",
    "14",
    "heart"
  ]
}