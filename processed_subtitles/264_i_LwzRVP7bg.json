{
  "text": "Kylie Ying has worked at many interesting places such as MIT, CERN, and Free Code Camp.\nShe's a physicist, engineer, and basically a genius. And now she's going to teach you\nabout machine learning in a way that is accessible to absolute beginners.\nWhat's up you guys? So welcome to Machine Learning for Everyone. If you are someone who\nis interested in machine learning and you think you are considered as everyone, then this video\nis for you. In this video, we'll talk about supervised and unsupervised learning models,\nwe'll go through maybe a little bit of the logic or math behind them, and then we'll also see how\nwe can program it on Google CoLab. If there are certain things that I have done, and you know,\nyou're somebody with more experience than me, please feel free to correct me in the comments\nand we can all as a community learn from this together. So with that, let's just dive right in.\nWithout wasting any time, let's just dive straight into the code and I will be teaching you guys\nconcepts as we go. So this here is the UCI machine learning repository. And basically,\nthey just have a ton of data sets that we can access. And I found this really cool one called\nthe magic gamma telescope data set. So in this data set, if you want to read all this information,\nto summarize what I what I think is going on, is there's this gamma telescope, and we have all\nthese high energy particles hitting the telescope. Now there's a camera, there's a detector that\nactually records certain patterns of you know, how this light hits the camera. And we can use\nproperties of those patterns in order to predict what type of particle caused that radiation. So\nwhether it was a gamma particle, or some other head, like hadron. Down here, these are all of\nthe attributes of those patterns that we collect in the camera. So you can see that there's, you\nknow, some length, width, size, asymmetry, etc. Now we're going to use all these properties to\nhelp us discriminate the patterns and whether or not they came from a gamma particle or hadron.\nSo in order to do this, we're going to come up here, go to the data folder. And you're going\nto click this magic zero for data, and we're going to download that. Now over here, I have a colab\nnotebook open. So you go to colab dot research dot google.com, you start a new notebook. And\nI'm just going to call this the magic data set. So actually, I'm going to call this for code camp\nmagic example. Okay. So with that, I'm going to first start with some imports. So I will import,\nyou know, I always import NumPy, I always import pandas. And I always import matplotlib.\nAnd then we'll import other things as we go. So yeah,\nwe run that in order to run the cell, you can either click this play button here, or you can\non my computer, it's just shift enter and that that will run the cell. And here, I'm just going\nto order I'm just going to, you know, let you guys know, okay, this is where I found the data set.\nSo I've copied and pasted this actually, but this is just where I found the data set.\nAnd in order to import that downloaded file that we we got from the computer, we're going to go\nover here to this folder thing. And I am literally just going to drag and drop that file into here.\nOkay. So in order to take a look at, you know, what does this file consist of,\ndo we have the labels? Do we not? I mean, we could open it on our computer, but we can also just do\npandas read CSV. And we can pass in the name of this file.\nAnd let's see what it returns. So it doesn't seem like we have the label. So let's go back to here.\nI'm just going to make the columns, the column labels, all of these attribute names over here.\nSo I'm just going to take these values and make that the column names.\nAll right, how do I do that? So basically, I will come back here, and I will create a list called\ncalls. And I will type in all of those things. With f size, f conk. And we also have f conk one.\nWe have f symmetry, f m three long, f m three trans, f alpha. Let's see, we have f dist and class.\nOkay, great. Now in order to label those as these columns down here in our data frame.\nSo basically, this command here just reads some CSV file that you pass in CSV has come about comma\nseparated values, and turns that into a pandas data frame object. So now if I pass in a names here,\nthen it basically assigns these labels to the columns of this data set. So I'm going to set\nthis data frame equal to DF. And then if we call the head is just like, give me the first five things,\ngive me the first five things. Now you'll see that we have labels for all of these. Okay.\nAll right, great. So one thing that you might notice is that over here, the class labels,\nwe have G and H. So if I actually go down here, and I do data frame class unique,\nyou'll see that I have either G's or H's, and these stand for gammas or hadrons.\nAnd our computer is not so good at understanding letters, right? Our computer is really good at\nunderstanding numbers. So what we're going to do is we're going to convert this to zero for G and\none for H. So here, I'm going to set this equal to this, whether or not that equals G. And then\nI'm just going to say as type int. So what this should do is convert this entire column,\nif it equals G, then this is true. So I guess that would be one. And then if it's H, it would\nbe false. So that would be zero, but I'm just converting G and H to one and zero, it doesn't\nreally matter. Like, if G is one and H is zero or vice versa. Let me just take a step back right\nnow and talk about this data set. So here I have some data frame, and I have all of these different\nvalues for each entry. Now this is a you know, each of these is one sample, it's one example,\nit's one item in our data set, it's one data point, all of these things are kind of the same\nthing when I mentioned, oh, this is one example, or this is one sample or whatever. Now, each of\nthese samples, they have, you know, one quality for each or one value for each of these labels\nup here, and then it has the class. Now what we're going to do in this specific example is try to\npredict for future, you know, samples, whether the class is G for gamma or H for hadron. And\nthat is something known as classification. Now, all of these up here, these are known as our features,\nand features are just things that we're going to pass into our model in order to help us predict\nthe label, which in this case is the class column. So for you know, sample zero, I have\n10 different features. So I have 10 different values that I can pass into some model.\nAnd I can spit out, you know, the class the label, and I know the true label here is G. So this is\nthis is actually supervised learning. All right. So before I move on, let me just give you a quick\nlittle crash course on what I just said. This is machine learning for everyone. Well, the first\nquestion is, what is machine learning? Well, machine learning is a sub domain of computer science\nthat focuses on certain algorithms, which might help a computer learn from data, without a\nprogrammer being there telling the computer exactly what to do. That's what we call explicit\nprogramming. So you might have heard of AI and ML and data science, what is the difference between\nall of these. So AI is artificial intelligence. And that's an area of computer science, where the\ngoal is to enable computers and machines to perform human like tasks and simulate human behavior.\nNow machine learning is a subset of AI that tries to solve one specific problem and make predictions\nusing certain data. And data science is a field that attempts to find patterns and draw insights\nfrom data. And that might mean we're using machine learning. So all of these fields kind of overlap,\nand all of them might use machine learning. So there are a few types of machine learning.\nThe first one is supervised learning. And in supervised learning, we're using labeled inputs.\nSo this means whatever input we get, we have a corresponding output label, in order to train\nmodels and to learn outputs of different new inputs that we might feed our model. So for example,\nI might have these pictures, okay, to a computer, all these pictures are are pixels, they're pixels\nwith a certain color. Now in supervised learning, all of these inputs have a label associated with\nthem, this is the output that we might want the computer to be able to predict. So for example,\nover here, this picture is a cat, this picture is a dog, and this picture is a lizard.\nNow there's also unsupervised learning. And in unsupervised learning, we use unlabeled data\nto learn about patterns in the data. So here are here are my input data points. Again, they're just\nimages, they're just pixels. Well, okay, let's say I have a bunch of these different pictures.\nAnd what I can do is I can feed all these to my computer. And I might not, you know,\nmy computer is not going to be able to say, Oh, this is a cat, dog and lizard in terms of,\nyou know, the output. But it might be able to cluster all these pictures, it might say,\nHey, all of these have something in common. All of these have something in common. And then these\ndown here have something in common, that's finding some sort of structure in our unlabeled data.\nAnd finally, we have reinforcement learning. And reinforcement learning. Well, they usually\nthere's an agent that is learning in some sort of interactive environment, based on rewards and\npenalties. So let's think of a dog, we can train our dog, but there's not necessarily, you know,\nany wrong or right output at any given moment, right? Well, let's pretend that dog is a computer.\nEssentially, what we're doing is we're giving rewards to our computer, and tell your computer,\nHey, this is probably something good that you want to keep doing. Well, computer agent terminology.\nBut in this class today, we'll be focusing on supervised learning and unsupervised learning\nand learning different models for each of those. Alright, so let's talk about supervised learning\nfirst. So this is kind of what a machine learning model looks like you have a bunch of inputs\nthat are going into some model. And then the model is spitting out an output, which is our prediction.\nSo all these inputs, this is what we call the feature vector. Now there are different types\nof features that we can have, we might have qualitative features. And qualitative means\ncategorical data, there's either a finite number of categories or groups. So one example of a\nqualitative feature might be gender. And in this case, there's only two here, it's for the sake of\nthe example, I know this might be a little bit outdated. Here we have a girl and a boy, there are\ntwo genders, there are two different categories. That's a piece of qualitative data. Another\nexample might be okay, we have, you know, a bunch of different nationalities, maybe a nationality or\na nation or a location, that might also be an example of categorical data. Now, in both of\nthese, there's no inherent order. It's not like, you know, we can rate us one and France to Japan\nthree, etc. Right? There's not really any inherent order built into either of these categorical\ndata sets. That's why we call this nominal data. Now, for nominal data, the way that we want\nto feed it into our computer is using something called one hot encoding. So let's say that, you\nknow, I have a data set, some of the items in our data, some of the inputs might be from the US,\nsome might be from India, then Canada, then France. Now, how do we get our computer to recognize that\nwe have to do something called one hot encoding. And basically, one hot encoding is saying, okay,\nwell, if it matches some category, make that a one. And if it doesn't just make that a zero.\nSo for example, if your input were from the US, you would you might have 1000. India, you know,\n0100. Canada, okay, well, the item representing Canada is one and then France, the item representing\nFrance is one. And then you can see that the rest are zeros, that's one hot encoding.\nNow, there are also a different type of qualitative feature. So here on the left,\nthere are different age groups, there's babies, toddlers, teenagers, young adults,\nadults, and so on, right. And on the right hand side, we might have different ratings. So maybe\nbad, not so good, mediocre, good, and then like, great. Now, these are known as ordinal pieces of\ndata, because they have some sort of inherent order, right? Like, being a toddler is a lot closer to\nbeing a baby than being an elderly person, right? Or good is closer to great than it is to really\nbad. So these have some sort of inherent ordering system. And so for these types of data sets,\nwe can actually just mark them from, you know, one to five, or we can just say, hey, for each of these,\nlet's give it a number. And this makes sense. Because, like, for example, the thing that I\njust said, how good is closer to great, then good is close to not good at all. Well, four is closer\nto five, then four is close to one. So this actually kind of makes sense. And it'll make sense for the\ncomputer as well. Alright, there are also quantitative pieces of data and quantitative\npieces of data are numerical valued pieces of data. So this could be discrete, which means,\nyou know, they might be integers, or it could be continuous, which means all real numbers.\nSo for example, the length of something is a quantitative piece of data, it's a quantitative\nfeature, the temperature of something is a quantitative feature. And then maybe how many\nEaster eggs I collected in my basket, this Easter egg hunt, that is an example of discrete quantitative\nfeature. Okay, so these are continuous. And this over here is the screen. So those are the things\nthat go into our feature vector, those are our features that we're feeding this model, because\nour computers are really, really good at understanding math, right at understanding numbers,\nthey're not so good at understanding things that humans might be able to understand.\nWell, what are the types of predictions that our model can output? So in supervised learning,\nthere are some different tasks, there's one classification, and basically classification,\njust saying, okay, predict discrete classes. And that might mean, you know, this is a hot dog,\nthis is a pizza, and this is ice cream. Okay, so there are three distinct classes and any other\npictures of hot dogs, pizza or ice cream, I can put under these labels. Hot dog, pizza, ice cream.\nHot dog, pizza, ice cream. This is something known as multi class classification. But there's also\nbinary classification. And binary classification, you might have hot dog, or not hot dog. So there's\nonly two categories that you're working with something that is something and something that's\nisn't binary classification. Okay, so yeah, other examples. So if something has positive or negative\nsentiment, that's binary classification. Maybe you're predicting your pictures of their cats or\ndogs. That's binary classification. Maybe, you know, you are writing an email filter, and you're\ntrying to figure out if an email spam or not spam. So that's also binary classification.\nNow for multi class classification, you might have, you know, cat, dog, lizard, dolphin, shark,\nrabbit, etc. We might have different types of fruits like orange, apple, pear, etc. And then\nmaybe different plant species. But multi class classification just means more than two. Okay,\nand binary means we're predicting between two things. There's also something called regression\nwhen we talk about supervised learning. And this just means we're trying to predict continuous\nvalues. So instead of just trying to predict different categories, we're trying to come up\nwith a number that you know, is on some sort of scale. So some examples. So some examples might\nbe the price of aetherium tomorrow, or it might be okay, what is going to be the temperature?\nOr it might be what is the price of this house? Right? So these things don't really fit into\ndiscrete classes. We're trying to predict a number that's as close to the true value as possible\nusing different features of our data set. So that's exactly what our model looks like in\nsupervised learning. Now let's talk about the model itself. How do we make this model learn?\nOr how can we tell whether or not it's even learning? So before we talk about the models,\nlet's talk about how can we actually like evaluate these models? Or how can we tell\nwhether something is a good model or bad model? So let's take a look at this data set. So this data\nset has this is from a diabetes, a Pima Indian diabetes data set. And here we have different\nnumber of pregnancies, different glucose levels, blood pressure, skin thickness, insulin, BMI,\nage, and then the outcome whether or not they have diabetes one for they do zero for they don't.\nSo here, all of these are quantitative features, right, because they're all on some scale.\nSo each row is a different sample in the data. So it's a different example, it's one person's data,\nand each row represents one person in this data set. Now this column, each column represents a\ndifferent feature. So this one here is some measure of blood pressure levels. And this one\nover here, as we mentioned is the output label. So this one is whether or not they have diabetes.\nAnd as I mentioned, this is what we would call a feature vector, because these are all of our\nfeatures in one sample. And this is what's known as the target, or the output for that feature\nvector. That's what we're trying to predict. And all of these together is our features matrix x.\nAnd over here, this is our labels or targets vector y. So I've condensed this to a chocolate\nbar to kind of talk about some of the other concepts in machine learning. So over here,\nwe have our x, our features matrix, and over here, this is our label y. So each row of this\nwill be fed into our model, right. And our model will make some sort of prediction. And what we do\nis we compare that prediction to the actual value of y that we have in our label data set, because\nthat's the whole point of supervised learning is we can compare what our model is outputting to,\noh, what is the truth, actually, and then we can go back and we can adjust some things. So the next\niteration, we get closer to what the true value is. So that whole process here, the tinkering that,\nokay, what's the difference? Where did we go wrong? That's what's known as training the model.\nAlright, so take this whole, you know, chunk right here, do we want to really put our entire\nchocolate bar into the model to train our model? Not really, right? Because if we did that, then\nhow do we know that our model can do well on new data that we haven't seen? Like, if I were to\ncreate a model to predict whether or not someone has diabetes, let's say that I just train all my\ndata, and I see that all my training data does well, I go to some hospital, I'm like, here's my\nmodel. I think you can use this to predict if somebody has diabetes. Do we think that would\nbe effective or not? Probably not, right? Because we haven't assessed how well our model can\ngeneralize. Okay, it might do well after you know, our model has seen this data over and over and\nover again. But what about new data? Can our model handle new data? Well, how do we how do we get our\nmodel to assess that? So we actually break up our whole data set that we have into three different\ntypes of data sets, we call it the training data set, the validation data set and the testing data\nset. And you know, you might have 60% here 20% and 20% or 80 10 and 10. It really depends on how\nmany statistics you have, I think either of those would be acceptable. So what we do is then we feed\nthe training data set into our model, we come up with, you know, this might be a vector of predictions\ncorresponding with each sample that we put into our model, we figure out, okay, what's the difference\nbetween our prediction and the true values, this is something known as loss, losses, you know,\nwhat's the difference here, in some numerical quantity, of course. And then we make adjustments,\nand that's what we call training. Okay. So then, once you know, we've made a bunch of adjustments,\nwe can put our validation set through this model. And the validation set is kind of used as a reality\ncheck during or after training to ensure that the model can handle unseen data still. So every\nsingle time after we train one iteration, we might stick the validation set in and see, hey, what's\nthe loss there. And then after our training is over, we can assess the validation set and ask,\nhey, what's the loss there. But one key difference here is that we don't have that training step,\nthis loss never gets fed back into the model, right, that feedback loop is not closed.\nAlright, so let's talk about loss really quickly. So here, I have four different types of models,\nI have some sort of data that's being fed into the model, and then some output. Okay, so this output\nhere is pretty far from you know, this truth that we want. And so this loss is going to be high. In\nmodel B, again, this is pretty far from what we want. So this loss is also going to be high,\nlet's give it 1.5. Now this one here, it's pretty close, I mean, maybe not almost, but pretty close\nto this one. So that might have a loss of 0.5. And then this one here is maybe further than this,\nbut still better than these two. So that loss might be 0.9. Okay, so which of these model\nperforms the best? Well, model C has a smallest loss, so it's probably model C. Okay, now let's\ntake model C. After you know, we've come up with these, all these models, and we've seen, okay, model\nC is probably the best model. We take model C, and we run our test set through this model. And this\ntest set is used as a final check to see how generalizable that chosen model is. So if I,\nyou know, finish training my diabetes data set, then I could run it through some chunk of the\ndata and I can say, oh, like, this is how we perform on data that it's never seen before at\nany point during the training process. Okay. And that loss, that's the final reported performance\nof my test set, or this would be the final reported performance of my model. Okay.\nSo let's talk about this thing called loss, because I think I kind of just glossed over it,\nright? So loss is the difference between your prediction and the actual, like, label.\nSo this would give a slightly higher loss than this. And this would even give a higher loss,\nbecause it's even more off. In computer science, we like formulas, right? We like formulaic ways\nof describing things. So here are some examples of loss functions and how we can actually come\nup with numbers. This here is known as L one loss. And basically, L one loss just takes the\nabsolute value of whatever your you know, real value is, whatever the real output label is,\nsubtracts the predicted value, and takes the absolute value of that. Okay. So the absolute\nvalue is a function that looks something like this. So the further off you are, the greater your losses,\nright in either direction. So if your real value is off from your predicted value by 10,\nthen your loss for that point would be 10. And then this sum here just means, hey,\nwe're taking all the points in our data set. And we're trying to figure out the sum of how far\neverything is. Now, we also have something called L two loss. So this loss function is quadratic,\nwhich means that if it's close, the penalty is very minimal. And if it's off by a lot,\nthen the penalty is much, much higher. Okay. And this instead of the absolute value, we just square\nthe the difference between the two. Now, there's also something called binary cross entropy loss.\nIt looks something like this. And this is for binary classification, this this might be the\nloss that we use. So this loss, you know, I'm not going to really go through it too much.\nBut you just need to know that loss decreases as the performance gets better. So there are some\nother measures of accurate or performance as well. So for example, accuracy, what is accuracy?\nSo let's say that these are pictures that I'm feeding my model, okay. And these predictions\nmight be apple, orange, orange, apple, okay, but the actual is apple, orange, apple, apple. So\nthree of them were correct. And one of them was incorrect. So the accuracy of this model is\nthree quarters or 75%. Alright, coming back to our colab notebook, I'm going to close this a little\nbit. Again, we've imported stuff up here. And we've already created our data frame right here. And\nthis is this is all of our data. This is what we're going to use to train our models. So down here,\nagain, if we now take a look at our data set, you'll see that our classes are now zeros and ones.\nSo now this is all numerical, which is good, because our computer can now understand that.\nOkay. And you know, it would probably be a good idea to maybe kind of plot, hey, do these things\nhave anything to do with the class. So here, I'm going to go through all the labels. So for label\nin the columns of this data frame. So this just gets me the list. Actually, we have the list,\nright? It's called so let's just use that might be less confusing of everything up to the last\nthing, which is the class. So I'm going to take all these 10 different features. And I'm going\nto plot them as a histogram. So and now I'm going to plot them as a histogram. So basically, if I\ntake that data frame, and I say, okay, for everything where the class is equal to one, so these are all\nof our gammas, remember, now, for that portion of the data frame, if I look at this label, so now\nthese, okay, what this part here is saying is, inside the data frame, get me everything where\nthe class is equal to one. So that's all all of these would fit into that category, right?\nAnd now let's just look at the label column. So the first label would be f length, which would\nbe this column. So this command here is getting me all the different values that belong to class one\nfor this specific label. And that's exactly what I'm going to put into the histogram. And now I'm\njust going to tell you know, matplotlib make the color blue, make this label this as you know, gamma\nset alpha, why do I keep doing that, alpha equal to 0.7. So that's just like the transparency.\nAnd then I'm going to set density equal to true, so that when we compare it to\nthe hadrons here, we'll have a baseline for comparing them. Okay, so the density being true\njust basically normalizes these distributions. So you know, if you have 200 in of one type,\nand then 50 of another type, well, if you drew the histograms, it would be hard to compare because\none of them would be a lot bigger than the other, right. But by normalizing them, we kind of are\ndistributing them over how many samples there are. Alright, and then I'm just going to put a title\non here and make that the label, the y label. So because it's density, the y label is probability.\nAnd the x label is just going to be the label.\nWhat is going on. And I'm going to include a legend and PLT dot show just means okay, display\nthe plot. So if I run that, just be up to the last item. So we want a list, right, not just the last\nitem. And now we can see that we're plotting all of these. So here we have the length. Oh, and I\nmade this gamma. So this should be hadron. Okay, so the gammas in blue, the hadrons are in red. So\nhere we can already see that, you know, maybe if the length is smaller, it's probably more likely\nto be gamma, right. And we can kind of you know, these all look somewhat similar. But here, okay,\nclearly, if there's more asymmetry, or if you know, this asymmetry measure is larger, then it's\nprobably hadron. Okay, oh, this one's a good one. So f alpha seems like hadrons are pretty evenly\ndistributed. Whereas if this is smaller, it looks like there's more gammas in that area.\nOkay, so this is kind of what the data that we're working with, we can kind of see what's going on.\nOkay, so the next thing that we're going to do here is we are going to create our train,\nour validation, and our test data sets. I'm going to set train valid and test to be equal to\nthis. So NumPy dot split, I'm just splitting up the data frame. And if I do this sample,\nwhere I'm sampling everything, this will basically shuffle my data. Now, if I I want to pass in where\nexactly I'm splitting my data set, so the first split is going to be maybe at 60%. So I'm going\nto say 0.6 times the length of this data frame. So and then cast that 10 integer, that's going\nto be the first place where you know, I cut it off, and that'll be my training data. Now, if I\nthen go to 0.8, this basically means everything between 60% and 80% of the length of the data\nset will go towards validation. And then, like everything from 80 to 100, I'm going to pass\nmy test data. So I can run that. And now, if we go up here, and we inspect this data, we'll see that\nthese columns seem to have values in like the 100s, whereas this one is 0.03. Right? So the scale of\nall these numbers is way off. And sometimes that will affect our results. So I'm going to run this\nis way off. And sometimes that will affect our results. So one thing that we would want to do\nis scale these so that they are, you know, so that it's now relative to maybe the mean and the\nstandard deviation of that specific column. I'm going to create a function called scale data set.\nAnd I'm going to pass in the data frame. And that's what I'll do for now. Okay, so the x values are\ngoing to be, you know, I take the data frame. And let's assume that the columns are going to be,\nyou know, that the label will always be the last thing in the data frame. So what I can do is say\ndata frame, dot columns all the way up to the last item, and get those values. Now for my y,\nwell, it's the last column. So I can just do this, I can just index into that last column,\nand then get those values. Now, in, so I'm actually going to import something known as\nthe standard scalar from sk learn. So if I come up here, I can go to sk learn dot pre processing.\nAnd I'm going to import standard scalar, I have to run that cell, I'm going to come back down here.\nAnd now I'm going to create a scalar and use that skip or so standard scalar.\nAnd with the scalar, what I can do is actually just fit and transform x. So here, I can say x\nis equal to scalar dot fit, fit, transform x. So what that's doing is saying, okay, take x and\nfit the standard scalar to x, and then transform all those values. And what would it be? And that's\ngoing to be our new x. Alright. And then I'm also going to just create, you know, the whole data as\none huge 2d NumPy array. And in order to do that, I'm going to call H stack. So H stack is saying,\nokay, take an array, and another array and horizontally stack them together. That's what\nthe H stands for. So by horizontally stacked them together, just like put them side by side,\nokay, not on top of each other. So what am I stacking? Well, I have to pass in something\nso that it can stack x and y. And now, okay, so NumPy is very particular about dimensions,\nright? So in this specific case, our x is a two dimensional object, but y is only a one dimensional\nthing, it's only a vector of values. So in order to now reshape it into a 2d item, we have to call\nNumPy dot reshape. And we can pass in the dimensions of its reshape. So if I pass in negative\none comma one, that just means okay, make this a 2d array, where the negative one just means infer\nwhat what this dimension value would be, which ends up being the length of y, this would be the\nsame as literally doing this. But the negative one is easier because we're making the computer\ndo the hard work. So if I stack that, I'm going to then return the data x and y. Okay. So one more\nthing is that if we go into our training data set, okay, again, this is our training data set.\nAnd we get the length of the training data set. But where the training data sets class is one,\nso remember that this is the gammas. And then if we print that, and we do the same thing, but zero,\nwe'll see that, you know, there's around 7000 of the gammas, but only around 4000 of the hadrons.\nSo that might actually become an issue. And instead, what we want to do is we want to oversample\nour our training data set. So that means that we want to increase the number of these values,\nso that these kind of match better. And surprise, surprise, there is something that we can import\nthat will help us do that. It's so I'm going to go to from in the learn dot oversampling. And I'm\ngoing to import this random oversampler, run that cell, and come back down here. So I will actually\nadd in this parameter called oversample, and set that to false for default. And if I do want to\noversample, then what I'm going to do, and by oversample, so if I do want to oversample,\nthen I'm going to create this ROS and set it equal to this random oversampler. And then for x and y,\nI'm just going to say, okay, just fit and resample x and y. And what that's doing is saying, okay,\ntake more of the less class. So take take the less class and keep sampling from there to increase\nthe size of our data set of that smaller class so that they now match. So if I do this, and I scale\ndata set, and I pass in the training data set where oversample is true. So this let's say this\nis train and then x train, y train. Oops, what's going on? These should be columns. So basically,\nwhat I'm doing now is I'm just saying, okay, what is the length of y train? Okay, now it's\n14,800, whatever. And now let's take a look at how many of these are type one. So actually,\nwe can just sum that up. And then we'll also see that if we instead switch the label and ask how\nmany of them are the other type, it's the same value. So now these have been evenly, you know,\nrebalanced. Okay, well, okay. So here, I'm just going to make this the validation data set. And\nthen the next one, I'm going to make this the test data set. Alright, and we're actually going to\nswitch oversample here to false. Now, the reason why I'm switching that to false is because my\nvalidation and my test sets are for the purpose of you know, if I have data that I haven't seen yet,\nhow does my sample perform on those? And I don't want to oversample for that right now. Like,\nI don't care about balancing those I'm, I want to know if I have a random set of data that's\nunlabeled, can I trust my model, right? So that's why I'm not oversampling. I run that. And again,\nwhat is going on? Oh, it's because we already have this train. So I have to go come up here and split\nthat data frame again. And now let's run these. Okay. So now we have our data properly formatted.\nAnd we're going to move on to different models now. And I'm going to tell you guys a little bit\nabout each of these models. And then I'm going to show you how we can do that in our code. So the\nfirst model that we're going to learn about is KNN or K nearest neighbors. Okay, so here, I've\nalready drawn a plot on the y axis, I have the number of kids that a family might have. And then\non the x axis, I have their income in terms of 1000s per year. So, you know, if if someone's\nmaking 40,000 a year, that's where this would be. And if somebody making 320, that's where that\nwould be somebody has zero kids, it'd be somewhere along this axis. Somebody has five, it'd be\nsomewhere over here. Okay. And now I have these plus signs and these minus signs on here. So what\nI'm going to represent here is the plus sign means that they own a car. And the minus sign is going\nto represent no car. Okay. So your initial thought should be okay, I think this is binary\nclassification because all of our points all of our samples have labels. So this is a sample with\nthe plus label. And this here is another sample with the minus label. This is an abbreviation for\nwidth that I'll use. Alright, so we have this entire data set. And maybe around half the people\nown a car and maybe around half the people don't own a car. Okay, well, what if I had some new\npoint, let me use choose a different color, I'll use this nice green. Well, what if I have a new\npoint over here? So let's say that somebody makes 40,000 a year and has two kids. What do we think\nthat would be? Well, just logically looking at this plot, you might think, okay, it seems like\nthey wouldn't have a car, right? Because that kind of matches the pattern of everybody else around\nthem. So that's a whole concept of this nearest neighbors is you look at, okay, what's around you.\nAnd then you're basically like, okay, I'm going to take the label of the majority that's around me.\nSo the first thing that we have to do is we have to define a distance function. And a lot of times\nin, you know, 2d plots like this, our distance function is something known as Euclidean distance.\nAnd Euclidean distance is basically just this straight line distance like this. Okay. So this\nwould be the Euclidean distance, it seems like there's this point, there's this point, there's\nthat point, etc. So the length of this line, this green line that I just drew, that is what's known\nas Euclidean distance. If we want to get technical with that, this exact formula is the distance here,\nlet me zoom in. The distance is equal to the square root of one point x minus the other points x\nsquared plus extend that square root, the same thing for y. So y one of one minus y two of the\nother squared. Okay, so we're basically trying to find the length, the distances, the difference\nbetween x and y, and then square each of those sum it up and take the square root. Okay, so I'm\ngoing to erase this so it doesn't clutter my drawing. But anyways, now going back to this plot,\nso here in the nearest neighbor algorithm, we see that there is a K, right? And this K is basically\ntelling us, okay, how many neighbors do we use in order to judge what the label is? So usually,\nwe use a K of maybe, you know, three or five, depends on how big our data set is. But here,\nI would say, maybe a logical number would be three or five. So let's say that we take K to be equal\nto three. Okay, well, of this data point that I drew over here, let me use green to highlight this.\nOkay, so of this data point that I drew over here, it looks like the three closest points are definitely\nthis one, this one. And then this one has a length of four. And this one seems like it'd be a little\nbit further than four. So actually, this would be these would be our three points. Well, all those\npoints are blue. So chances are, my prediction for this point is going to be blue, it's going to be\nprobably don't have a car. All right, now what if my point is somewhere? What if my point is\nsomewhere over here, let's say that a couple has four kids, and they make 240,000 a year. All right,\nwell, now my closest points are this one, probably a little bit over that one. And then this one,\nright? Okay, still all pluses. Well, this one is more than likely to be plus. Right? Now,\nlet me get rid of some of these just so that it looks a little bit more clear. All right,\nlet's go through one more. What about a point that might be right here? Okay, let's see. Well,\ndefinitely this is the closest, right? This one's also closest. And then it's really close between\nthe two of these. But if we actually do the mathematics, it seems like if we zoom in,\nthis one is right here. And this one is in between these two. So this one here is actually shorter\nthan this one. And that means that that top one is the one that we're going to take. Now,\nwhat is the majority of the points that are close by? Well, we have one plus here, we have one plus\nhere, and we have one minus here, which means that the pluses are the majority. And that means\nthat this label is probably somebody with a car. Okay. So this is how K nearest neighbors would\nwork. It's that simple. And this can be extrapolated to further dimensions to higher dimensions. You\nknow, if you have here, we have two different features, we have the income, and then we have\nthe number of kids. But let's say we have 10 different features, we can expand our distance\nfunction so that it includes all 10 of those dimensions, we take the square root of everything,\nand then we figure out which one is the closest to the point that we desire to classify. Okay. So\nthat's K nearest neighbors. So now we've learned about K nearest neighbors. Let's see how we would\nbe able to do that within our code. So here, I'm going to label the section K nearest neighbors.\nAnd we're actually going to use a package from SK learn. So the reason why we, you know, use these\npackages and so that we don't have to manually code all these things ourselves, because it would\nbe really difficult. And chances are the way that we would code it, either would have bugs,\nor it'd be really slow, or I don't know a whole bunch of issues. So what we're going to do is\nhand it off to the pros. From here, I can say, okay, from SK learn, which is this package dot\nneighbors, I'm going to import K neighbors classifier, because we're classifying. Okay,\nso I run that. And our KNN model is going to be this K neighbors classifier. And we can pass in\na parameter of how many neighbors, you know, we want to use. So first, let's see what happens if\nwe just use one. So now if I do K, and then model dot fit, I can pass in my x training set and my\nweight y train data. Okay. So that effectively fits this model. And let's get all the predictions. So\nwhy can and I guess yeah, let's do y predictions. And my y predictions are going to be cannon model\ndot predict. So let's use the test set x test. Okay. Alright, so if I call y predict, you'll see\nthat we have those. But if I get my truth values for that test set, you'll see that this is what\nwe actually do. So just looking at this, we got five out of six of them. Okay, great. So let's\nactually take a look at something called the classification report that's offered by SK learn.\nSo if I go to from SK learn dot metrics, import classification report, what I can actually do is\nsay, hey, print out this classification report for me. And let's check, you know, I'm giving you the\ny test and the y prediction. We run this and we see we get this whole entire chart. So I'm going\nto tell you guys a few things on this chart. Alright, this accuracy is 82%, which is actually\npretty good. That's just saying, hey, if we just look at, you know, what each of these new points,\nwhat it's closest to, then we actually get an 82% accuracy, which means how many do we get right\nversus how many total are there. Now, precision is saying, okay, you might see that we have it\nfor class one, or class zero and class one. What precision is saying was, let's go to this Wikipedia\ndiagram over here, because I actually kind of like this diagram. So here, this is our entire data set.\nAnd on the left over here, we have everything that we know is positive. So everything that is\nactually truly positive, that we've labeled positive in our original data set. And over here,\nthis is everything that's truly negative. Now in the circle, we have things that are positive that\nwere labeled positive by our model. On the left here, we have things that are truly positive,\nbecause you know, this side is the positive side and the side is the negative side. So these are\ntruly positive. Whereas all these ones out here, well, they should have been positive, but they\nare labeled as negative. And in here, these are the ones that we've labeled positive, but they're\nactually negative. And out here, these are truly negative. So precision is saying, okay, out of all\nthe ones we've labeled as positive, how many of them are true positives? And recall is saying,\nokay, out of all the ones that we know are truly positive, how many do we actually get right? Okay,\nso going back to this over here, our precision score, so again, precision, out of all the ones\nthat we've labeled as the specific class, how many of them are actually that class, it's 7784%. Now,\nrecall how out of all the ones that are actually this class, how many of those that we get, this\nis 68% and 89%. Alright, so not too shabby, we can clearly see that this recall and precision for\nlike this, the class zero is worse than class one. Right? So that means for hadron, it's worked for\nhadrons and for our gammas. This f1 score over here is kind of a combination of the precision and\nrecall score. So we're actually going to mostly look at this one because we have an unbalanced\ntest data set. So here we have a measure of 72 and 87 or point seven two and point eight seven,\nwhich is not too shabby. All right. Well, what if we, you know, made this three. So we actually see\nthat, okay, so what was it originally with one? We see that our f1 score, you know, is now it was\npoint seven two and then point eight seven. And then our accuracy was 82%. So if I change that to\nthree. Alright, so we've kind of increased zero at the cost of one and then our overall accuracy\nis 81. So let's actually just make this five. Alright, so you know, again, very similar numbers,\nwe have 82% accuracy, which is pretty decent for a model that's relatively simple. Okay,\nthe next type of model that we're going to talk about is something known as naive Bayes. Now,\nin order to understand the concepts behind naive Bayes, we have to be able to understand\nconditional probability and Bayes rule. So let's say I have some sort of data set that's shown in\nthis table right here. People who have COVID are over here in this red row. And people who do not\nhave COVID are down here in this green row. Now, what about the COVID test? Well, people who have\ntested positive are over here in this column. And people who have tested negative are over here in\nthis column. Okay. Yeah, so basically, our categories are people who have COVID and test positive,\npeople who don't have COVID, but test positive, so a false false positive, people who have COVID\nand test negative, which is a false negative, and people who don't have COVID and test negative,\nwhich good means you don't have COVID. Okay, so let's make this slightly more legible. And here,\nin the margins, I've written down the sums of whatever it's referring to. So this here is the\nsum of this entire row. And this here might be the sum of this column over here. Okay. So the first\nquestion that I have is, what is the probability of having COVID given that you have a positive\ntest? And in probability, we write that out like this. So the probability of COVID given, so this\nline, that vertical line means given that, you know, some condition, so given a positive test,\nokay, so what is the probability of having COVID given a positive test? So what this is asking is\nsaying, okay, let's go into this condition. So the condition of having a positive test, that is this\nslice of the data, right? That means if you're in this slice of data, you have a positive test. So\ngiven that we have a positive test, given in this condition, in this circumstance, we have a positive\ntest. So what's the probability that we have COVID? Well, if we're just using this data, the number\nof people that have COVID is 531. So I'm gonna say that there's 531 people that have COVID. And then\nnow we divide that by the total number of people that have a positive test, which is 551. Okay,\nso that's the probability and doing a quick division, we get that this is equal to around\n96.4%. So according to this data set, which is data that I made up off the top of my head, so it's\nnot actually real COVID data. But according to this data, the probability of having COVID given\nthat you tested positive is 96.4%. Alright, now with that, let's talk about Bayes rule, which is\nthis section here. Let's ignore this bottom part for now. So Bayes rule is asking, okay, what is\nthe probability of some event A happening, given that B happened. So this, we already know has\nhappened. This is our condition, right? Well, what if we don't have data for that, right? Like, what\nif we don't know what the probability of A given B is? Well, Bayes rule is saying, okay, well, you\ncan actually go and calculate it, as long as you have a probability of B given A, the probability\nof A and the probability of B. Okay. And this is just a mathematical formula for that. Alright,\nso here we have Bayes rule. And let's actually see Bayes rule in action. Let's use it on an example.\nSo here, let's say that we have some disease statistics, okay. So not COVID different disease.\nAnd we know that the probability of obtaining a false positive is 0.05 probability of obtaining a\nfalse negative is 0.01. And the probability of the disease is 0.1. Okay, what is the probability of\nthe disease given that we got a positive test? Hmm, how do we even go about solving this? So\nwhat what do I mean by false positive? What's a different way to rewrite that? A false positive\nis when you test positive, but you don't actually have the disease. So this here is a probability\nthat you have a positive test given no disease, right? And similarly for the false negative,\nit's a probability that you test negative given that you actually have the disease. So if I put\nthat into a chart, for example, and this might be my positive and negative tests, and this might\nbe my diseases, disease and no disease. Well, the probability that I test positive, but actually\nhave no disease, okay, that's 0.05 over here. And then the false negatives up here for 0.01. So I'm\ntesting negative, but I don't actually have the disease. This so the probability that you test\npositive, and you don't have the disease, plus a probability that you test negative, given that you\ndon't have the disease, that should sum up to one. Okay, because if you don't have the disease,\nthen you should have some probability that you're testing positive and some probability that you're\ntesting negative. But that probability, in total should be one. So that means that the probability\nnegative and no disease, this should be the reciprocal, this should be the opposite. So it\nshould be 0.95 because it's one minus whatever this probability is. And then similarly, oops,\nup here, this should be 0.99 because the probability that we, you know,\ntest negative and have the disease plus the probability that we test positive and have the\ndisease should equal one. So this is our probability chart. And now, this probability of disease\nbeing point 0.1 just means I have 10% probability of actually of having the disease, right? Like,\nin the general population, the probability that I have the disease is 0.1. Okay, so what is the\nprobability that I have the disease given that I got a positive test? Well, remember that we\ncan write this out in terms of Bayes rule, right? So if I use this rule up here, this is the\nprobability of a positive test given that I have the disease times the probability of the disease\ndivided by the probability of the evidence, which is my positive test.\nAlright, now let's plug in some numbers for that. The probability of having a positive test given\nthat I have the disease is 0.99. And then the probability that I have the disease is this value\nover here 0.1. Okay. And then the probability that I have a positive test at all should be okay,\nwhat is the probability that I have a positive test given that I actually have the disease\nand then having having the disease. And then the other case, where the probability of me having a\nnegative test given or sorry, positive test giving no disease times the probability of not actually\nhaving a disease. Okay, so I can expand that probability of having a positive test out into\nthese two different cases, I have a disease, and then I don't. And then what's the probability of\nhaving positive tests in either one of those cases. So that expression would become 0.99 times 0.1\nplus 0.05. So that's the probability that I'm testing positive, but don't have the disease.\nAnd the times the probability that I don't actually have the disease. So that's one minus\n0.1 probability that the population doesn't have the disease is 90%. So 0.9. And let's do that\nmultiplication. And I get an answer of 0.6875 or 68.75%. Okay. All right, so we can actually expand\nthat we can expand Bayes rule and apply it to classification. And this is what we call naive\nbase. So first, a little terminology. So the posterior is this over here, because it's asking,\nHey, what is the probability of some class CK? So by CK, I just mean, you know, the different\ncategories, so C for category or class or whatever. So category one might be cats, category two,\ndogs, category three, lizards, all the way, we have k categories, k is just some number. Okay.\nSo what is the probability of having of this specific sample x, so this is our feature vector\nof this one sample. What is the probability of x fitting into category 123 for whatever, right,\nso that that's what this is asking, what is the probability that, you know, it's actually from\nthis class, given all this evidence that we see the x's. So the likelihood is this quantity over\nhere, it's saying, Okay, well, given that, you know, assume, assume we are, assume that this\nclass is class CK, okay, assume that this is a category. Well, what is the likelihood of\nactually seeing x, all these different features from that category. And then this here is the\nprior. So like in the entire population of things, what are the probabilities? What is the\nprobability of this class in general? Like if I have, you know, in my entire data set, what is the\npercentage? What is the chance that this image is a cat? How many cats do I have? Right. And then this\ndown here is called the evidence because what we're trying to do is we're changing our prior,\nwe're creating this new posterior probability built upon the prior by using some sort of evidence,\nright? And that evidence is a probability of x. So that's some vocab. And this here\nis a rule for naive Bayes. Whoa, okay, let's digest that a little bit. Okay. So what is\nlet me use a different color. What is this side of the equation asking? It's asking,\nwhat is the probability that we are in some class K, CK, given that, you know, this is my first\ninput, this is my second input, this is, you know, my third, fourth, this is my nth input. So let's\nsay that our classification is, do we play soccer today or not? Okay, and let's say our x's are,\nokay, is it how much wind is there? How much rain is there? And what day of the week is it? So let's\nSo let's say that it's raining, it's not windy, but it's Wednesday, do we play soccer? Do we not?\nSo let's use Bayes rule on this. So this here\nis equal to the probability of x one, x two, all these joint probabilities, given class K\ntimes the probability of that class, all over the probability of this evidence.\nOkay. So what is this fancy symbol over here, this means proportional to\nso how our equal sign means it's equal to this like little squiggly sign means that this is\nproportional to okay, and this denominator over here, you might notice that it has no impact on\nthe class like this, that number doesn't depend on the class, right? So this is going to be constant\nfor all of our different classes. So what I'm going to do is make things simpler. So I'm just\ngoing to say that this probability x one, x two, all the way to x n, this is going to be proportional\nto the numerator, I don't care about the denominator, because it's the same for every\nsingle class. So this is proportional to x one, x two, x n given class K times the probability of\nthat class. Okay. All right. So in naive Bayes, the point of it being naive, is that we're actually\nthis joint probability, we're just assuming that all of these different things\nare all independent. So in my soccer example, you know, the probability that we're playing soccer,\nor the probability that, you know, it's windy, and it's rainy, and, and it's Wednesday, all these\nthings are independent, we're assuming that they're independent. So that means that I can\nactually write this part of the equation here as this. So each term in here, I can just multiply\nall of them together. So the probability of the first feature, given that it's class K,\ntimes the probability of the second feature and given this problem, like class K all the way up\nall the way up until, you know, the nth feature of given that it's class K. So this expands to\nall of this. All right, which means that this here is now proportional to the thing that we just\nexpanded times this. So I'm going to write that out. So the probability of that class.\nAnd I'm actually going to use this symbol. So what this means is it's a huge multiplication,\nit means multiply everything to the right of this. So this probability x, given some class K,\nbut do it for all the i's. So I, what is I, okay, we're going to go from the first\nthe first x i all the way to the nth. So that means for every single i, we're just multiplying\nthese probabilities together. And that's where this up here comes from. So to wrap this up,\noops, this should be a line to wrap this up in plain English. Basically, what this is saying\nis a probability that you know, we're in some category, given that we have all these different\nfeatures is proportional to the probability of that class in general, times the probability of\neach of those features, given that we're in this one class that we're testing. So the probability\nof it, you know, of us playing soccer today, given that it's rainy, not windy, and and it's\nWednesday, is proportional to Okay, well, what is what is the probability that we play soccer\nanyways, and then times the probability that it's rainy, given that we're playing soccer,\ntimes the probability that it's not windy, given that we're playing soccer. So how many times are\nwe playing soccer when it's windy, how you know, and then how many times are what's the probability\nthat's Wednesday, given that we're playing soccer. Okay. So how do we use this in order to make a\nclassification. So that's where this comes in our y hat, our predicted y is going to be equal to\nsomething called the arg max. And then this expression over here, because we want to take\nthe arg max. Well, we want. So okay, if I write out this, again, this means the probability of\nbeing in some class CK given all of our evidence. Well, we're going to take the K that maximizes\nthis expression on the right. That's what arc max means. So if K is in zero, oops,\none through K, so this is how many categories are, we're going to go through each K. And we're going\nto solve this expression over here and find the K that makes that the largest. Okay. And remember\nthat instead of writing this, we have now a formula, thanks to Bayes rule for helping us\napproximate that right in something that maybe we can we maybe we have like the evidence for that,\nwe have the answers for that based on our training set. So this principle of going through each of\nthese and finding whatever class whatever category maximizes this expression on the right,\nthis is something known as MAP for short, or maximum a posteriori.\nPick the hypothesis. So pick the K that is the most probable so that we minimize the probability\nof misclassification. Right. So that is MAP. That is naive Bayes. Back to the notebook. So\njust like how I imported k nearest neighbor, k neighbors classifier up here for naive Bayes,\nI can go to SK learn naive Bayes. And I can import Gaussian naive Bayes.\nRight. And here I'm going to say my naive Bayes model is equal. This is very similar to what we\nhad above. And I'm just going to say with this model, we are going to fit x train and y train.\nAll right, just like above. So this, I might actually, so I'm going to set that. And\nexactly, just like above, I'm going to make my prediction. So here, I'm going to instead use my\nnaive Bayes model. And of course, I'm going to run the classification report again. So I'm actually\njust going to put these in the same cell. But here we have the y the new y prediction and then y test\nis still our original test data set. So if I run this, you'll see that. Okay, what's going on here,\nwe get worse scores, right? Our precision, for all of them, they look slightly worse. And our,\nyou know, for our precision, our recall, our f1 score, they look slightly worse for all the different\ncategories. And our total accuracy, I mean, it's still 72%, which is not too shabby. But it's still\n72%. Okay. Which, you know, is not not that great. Okay, so let's move on to logistic regression.\nHere, I've drawn a plot, I have y. So this is my label on one axis. And then this is maybe one of\nmy features. So let's just say I only have one feature in this case, text zero, right? Well,\nwe see that, you know, I have a few of one class type down here. And we know it's one class type\nbecause it's zero. And then we have our other class type one up here. And then we have our\ny. Okay. So many of you guys are familiar with regression. So let's start there. If I were to\ndraw a regression line through this, it might look something like like this. Right? Well, this\ndoesn't seem to be a very good model. Like, why would we use this specific line to predict why?\nRight? It's, it's iffy. Okay. For example, we might say, okay, well, it seems like, you know,\neverything from here downwards would be one class type in here, upwards would be another class type.\nBut when you look at this, you're just you, you visually can tell, okay, like, that line doesn't\nmake sense. Things are not those dots are not along that line. And the reason is because we\nare doing classification, not regression. Okay. Well, first of all, let's start here, we know that\nthis model, if we just use this line, it equals m x. So whatever this let's just say it's x plus b,\nwhich is the y intercept, right? And m is the slope. But when we use a linear regression,\nis it actually y hat? No, it's not right. So when we're working with linear regression,\nwhat we're actually estimating in our model is a probability, what's a probability between zero\nand one, that is class zero or class one. So here, let's rewrite this as p equals m x plus b.\nOkay, well, m x plus b, that can range, you know, from negative infinity to infinity,\nright? For any for any value of x, it goes from negative infinity to infinity.\nBut probability, we know probably one of the rules of probability is that probability has to stay\nbetween zero and one. So how do we fix this? Well, maybe instead of just setting the probability\nequal to that, we can set the odds equal to this. So by that, I mean, okay, let's do probability\ndivided by one minus the probability. Okay, so now becomes this ratio. Now this ratio is allowed to\ntake on infinite values. But there's still one issue here. Let me move this over a bit.\nThe one issue here is that m x plus b, that can still be negative, right? Like if you know,\nI have a negative slope, if I have a negative b, if I have some negative x's in there, I don't know,\nbut that can be that's allowed to be negative. So how do we fix that? We do that by actually taking\nthe log of the odds. Okay. So now I have the log of you know, some probability divided by one minus\nthe probability. And now that is on a range of negative infinity to infinity, which is good\nbecause the range of log should be negative infinity to infinity. Now how do I solve for P\nthe probability? Well, the first thing I can do is take, you know, I can remove the log by taking\nthe not the e to the whatever is on both sides. So that gives me the probability\nover the one minus the probability is now equal to e to the m x plus b. Okay. So let's multiply\nthat out. So the probability is equal to one minus probability e to the m x plus b. So P is equal to\ne to the m x plus b minus P times e to the m x plus b. And now we have we can move like terms to\none side. So if I do P, so basically, I'm moving this over, so I'm adding P. So now P one plus e\nto the m x plus b is equal to e to the m x plus b and let me change this parentheses make it a\nlittle bigger. So now my probability can be e to the m x plus b divided by one plus e to the m x plus b.\nOkay, well, let me just rewrite this really quickly, I want a numerator of one on top.\nOkay, so what I'm going to do is I'm going to multiply this by negative m x plus b,\nand then also the bottom by negative m x plus b, and I'm allowed to do that because\nthis over this is one. So now my probability is equal to one over\none plus e to the negative m x plus b. And now why did I rewrite it like that?\nIt's because this is actually a form of a special function, which is called the sigmoid\nfunction. And for the sigmoid function, it looks something like this. So s of x sigmoid, you know,\nthat some x is equal to one over one plus e to the negative x. So essentially, what I just did up here\nis rewrite this in some sigmoid function, where the x value is actually m x plus b.\nSo maybe I'll change this to y just to make that a bit more clear, it doesn't matter what\nthe variable name is. But this is our sigmoid function. And visually, what our sigmoid function\nlooks like is it goes from zero. So this here is zero to one. And it looks something like this\ncurved s, which I didn't draw too well. Let me try that again. It's hard to draw\nsomething if I can draw this right. Like that. Okay, so it goes in between zero and one.\nAnd you might notice that this form fits our shape up here.\nOops, let's draw it sharper. But if it's our shape up there a lot better, right?\nAlright, so that is what we call logistic regression, we're basically trying to fit our data\nto the sigmoid function. Okay. And when we only have, you know, one data point, so if we only have\none feature x, and that's what we call simple logistic regression. But then if we have, you know,\nso that's only x zero, but then if we have x zero, x one, all the way to x n, we call this\nmultiple logistic regression, because there are multiple features that we're considering\nwhen we're building our model, logistic regression. So I'm going to put that here.\nAnd again, from SK learn this linear model, we can import logistic regression. All right.\nAnd just like how we did above, we can repeat all of this. So here, instead of NB, I'm going to call\nthis log model, or LG logistic regression. I'm going to change this to logistic regression.\nSo I'm just going to use the default logistic regression. But actually, if you look here,\nyou see that you can use different penalties. So right now we're using\nan L2 penalty. But L2 is our quadratic formula. Okay, so that means that for,\nyou know, outliers, it would really penalize that. For all these other things, you know,\nyou can toggle these different parameters, and you might get slightly different results.\nIf I were building a production level logistic regression model, then I would want to go and I\nwould want to figure out how to do that. So I'm going to go ahead and I'm going to go ahead and\nI would want to figure out, you know, what are the best parameters to pass into here,\nbased on my validation data. But for now, we'll just we'll just use this out of the box.\nSo again, I'm going to fit the X train and the Y train. And I'm just going to predict again,\nso I can just call this again. And instead of LG, NB, I'm going to use LG. So here, this is decent\nprecision 65% recall 71, f 168, or 82 total accuracy of 77. Okay, so it performs slightly\nbetter than I base, but it's still not as good as K and N. Alright, so the last model for\nclassification that I wanted to talk about is something called support vector machines,\nor SVMs for short. So what exactly is an SVM model, I have two different features x zero and\nx one on the axes. And then I've told you if it's you know, class zero or class one based on the\nblue and red labels, my goal is to find some sort of line between these two labels that best divides\nthe data. Alright, so this line is our SVM model. So I call it a line here because in 2d, it's a\nline, but in 3d, it would be a plane and then you can also have more and more dimensions. So the\nproper term is actually I want to find the hyperplane that best differentiates these two\nclasses. Let's see a few examples. Okay, so first, between these three lines, let's say A, B, and C,\nand C, which one is the best divider of the data, which one has you know, all the data on one side\nor the other, or at least if it doesn't, which one divides it the most, right, like which one\nis has the most defined boundary between the two different groups. So this this question should be\npretty straightforward. It should be a right because a has a clear distinct line between where you\nknow, everything on this side of a is one label, it's negative and everything on this side of a\nis the other label, it's positive. So what if I have a but then what if I had drawn my B\nlike this, and my C, maybe like this, sorry, they're kind of the labels are kind of close together.\nBut now which one is the best? So I would argue that it's still a, right? And why is it still a?\nRight? And why is it still a? Because in these other two, look at how close this is to that,\nto these points. Right? So if I had some new point that I wanted to estimate, okay,\nsay I didn't have A or B. So let's say we're just working with C. Let's say I have some new point\nthat's right here. Or maybe a new point that's right there. Well, it seems like just logically\nlooking at this. I mean, without the boundary, that would probably go under the positives,\nright? I mean, it's pretty close to that other positive. So one thing that we care about in SVM\nis something known as the margin. Okay, so not only do we want to separate the two classes really\nwell, we also care about the boundary in between where the points in those classes in our data set\nare, and the line that we're drawing. So in a line like this, the closest values to this line\nmight be like here. And I'm trying to draw these perpendicular. Right? And so this effectively,\nif I switch over to these dotted lines, if I can draw this right. So these effectively\nare what's known as the margins. Okay, so these both here, these are our margins in our SVMs.\nAnd our goal is to maximize those margins. So not only do we want the line that best separates the\ntwo different classes, we want the line that has the largest margin. And the data points that lie\non the margin lines, the data. So basically, these are the data points that's helping us define our\ndivider. These are what we call support vectors. Hence the name support vector machines. Okay,\nso the issue with SVM sometimes is that they're not so robust to outliers. Right? So for example,\nif I had one outlier, like this up here, that would totally change where I want my support\nvector to be, even though that might be my only outlier. Okay. So that's just something to keep\nin mind. As you know, when you're working with SVM is, it might not be the best model if there\nare outliers in your data set. Okay, so another example of SVMs might be, let's say that we have\ndata like this, I'm just going to use a one dimensional data set for this example. Let's\nsay we have a data set that looks like this. Well, our, you know, separators should be\nperpendicular to this line. But it should be somewhere along this line. So it could be\nanywhere like this. You might argue, okay, well, there's one here. And then you could also just\ndraw another one over here, right? And then maybe you can have two SVMs. But that's not really how\nSVMs work. But one thing that we can do is we can create some sort of projection. So I realize here\nthat one thing I forgot to do was to label where zero was. So let's just say zero is here.\nNow, what I'm going to do is I'm going to say, okay, I'm going to have x, and then I'm going to\nhave x, sorry, x zero and x one. So x zero is just going to be my original x. But I'm going to make\nx one equal to let's say, x squared. So whatever is this squared, right? So now, my natives would be,\nyou know, maybe somewhere here, here, just pretend that it's somewhere up here.\nRight. And now my pluses might be something like\nthat. And I'm going to run out of space over here. So I'm just going to draw these together,\nuse your imagination. But once I draw it like this, well, it's a lot easier to apply a boundary,\nright? Now our SVM could be maybe something like this, this. And now you see that we've divided\nour data set. Now it's separable where one class is this way. And the other class is that way.\nOkay, so that's known as SVMs. I do highly suggest that, you know, any of these models that we just\nmentioned, if you're interested in them, do go more in depth mathematically into them. Like how\ndo we how do we find this hyperplane? Right? I'm not going to go over that in this specific course,\nbecause you're just learning what an SVM is. But it's a good idea to know, oh, okay, this is the\ntechnique behind finding, you know, what exactly are the are the how do you define the hyperplane\nthat we're going to use. So anyways, this transformation that we did down here, this is known\nas the kernel trick. So when we go from x to some coordinate x, and then x squared,\nwhat we're doing is we are applying a kernel. So that's why it's called the kernel trick.\nSo SVMs are actually really powerful. And you'll see that here. So from sk learn.svm, we are going\nto import SVC. And SVC is our support vector classifier. So with this, so with our SVM model,\nwe are going to, you know, create SVC model. And we are going to, again, fit this to X train, I\ncould have just copied and pasted this, I should be able to do that. So we're going to create SVC\nagain, fit this to X train, I could have just copied and pasted this, I should have probably\ndone that. Okay, taking a bit longer. All right. Let's predict using RSVM model. And here,\nlet's see if I can hover over this. Right. So again, you see a lot of these different\nparameters here that you can go back and change if you were creating a production level model. Okay,\nbut in this specific case, we'll just use it out of the box again. So if I make predictions,\nyou'll note that Wow, the accuracy actually jumps to 87% with the SVM. And even with class zero,\nthere's nothing less than, you know, point eight, which is great. And for class one,\nI mean, everything's at 0.9, which is higher than anything that we had seen to this point.\nSo so far, we've gone over four different classification models, we've done SVM,\nlogistic regression, naive Bayes and cannon. And these are just simple ways on how to implement\nthem. Each of these they have different, you know, they have different hyper parameters that you can\ngo and you can toggle. And you can try to see if that helps later on or not. But for the most part,\nthey perform, they give us around 70 to 80% accuracy. Okay, with SVM being the best. Now,\nlet's see if we can actually beat that using a neural net. Now the final type of model that\nI wanted to talk about is known as a neural net or neural network. And neural nets look something\nlike this. So you have an input layer, this is where all your features would go. And they have\nall these arrows pointing to some sort of hidden layer. And then all these arrows point to some\nsort of output layer. So what is what is all this mean? Each of these layers in here, this is\nsomething known as a neuron. Okay, so that's a neuron. In a neural net. These are all of our\nfeatures that we're inputting into the neural net. So that might be x zero x one all the way through\nx n. Right. And these are the features that we talked about there, they might be you know,\nthe pregnancy, the BMI, the age, etc. Now all of these get weighted by some value. So they\nare multiplied by some w number that applies to that one specific category that one specific\nfeature. So these two get multiplied. And the sum of all of these goes into that neuron. Okay,\nso basically, I'm taking w zero times x zero. And then I'm adding x one times w one and then\nI'm adding you know, x two times w two, etc, all the way to x n times w n. And that's getting\ninput into the neuron. Now I'm also adding this bias term, which just means okay, I might want\nto shift this by a little bit. So I might add five or I might add 0.1 or I might subtract 100,\nI don't know. But we're going to add this bias term. And the output of all these things. So\nthe sum of this, this, this and this, go into something known as an activation function,\nokay. And then after applying this activation function, we get an output. And this is what a\nneuron would look like. Now a whole network of them would look something like this.\nSo I kind of gloss over this activation function. What exactly is that? This is how a neural net\nlooks like if we have all our inputs here. And let's say all of these arrows represent some sort\nof addition, right? Then what's going on is we're just adding a bunch of times, right? We're adding\nthe some sort of weight times these input layer a bunch of times. And then if we were to go back\nand factor that all out, then this entire neural net is just a linear combination of these input\nlayers, which I don't know about you, but that just seems kind of useless, right? Because we could\nliterally just write that out in a formula, why would we need to set up this entire neural network,\nwe wouldn't. So the activation function is introduced, right? So without an activation\nfunction, this just becomes a linear model. An activation function might look something like\nthis. And as you can tell, these are not linear. And the reason why we introduce these is so that\nour entire model doesn't collapse on itself and become a linear model. So over here, this is\nsomething known as a sigmoid function, it runs between zero and one, tanh runs between negative\none all the way to one. And this is ReLU, which anything less than zero is zero, and then anything\ngreater than zero is linear. So with these activation functions, every single output of a neuron\nis no longer just the linear combination of these, it's some sort of altered linear state, which means\nthat the input into the next neuron is, you know, it doesn't it doesn't collapse on itself, it doesn't\nbecome linear, because we've introduced all these nonlinearities. So this is a training set, the\nmodel, the loss, right? And then we do this thing called training, where we have to feed the loss\nback into the model, and make certain adjustments to the model to improve this predicted output.\nLet's talk a little bit about the training, what exactly goes on during that step.\nLet's go back and take a look at our L2 loss function. This is what our L2 loss function\nlooks like it's a quadratic formula, right? Well, up here, the error is really, really, really, really\nlarge. And our goal is to get somewhere down here, where the loss is decreased, right? Because that\nmeans that our predicted value is closer to our true value. So that means that we want to go\nthis way. Okay. And thanks to a lot of properties of math, something that we can do is called\ngradient descent, in order to follow this slope down this way. This quadratic is, it has different\ndifferent slopes with respect to some value. Okay, so the loss with respect to some weight\nw zero, versus w one versus w n, they might all be different. Right? So some way that I kind of\nthink about it is, to what extent is this value contributing to our loss. And we can actually\nfigure that out through some calculus, which we're not going to touch up on in this specific course.\nBut if you want to learn more about neural nets, you should probably also learn some calculus\nand figure out what exactly back propagation is doing, in order to actually calculate, you know,\nhow much do we have to backstep by. So the thing is here, you might notice that this follows\nthis curve at all of these different points. And the closer we get to the bottom, the smaller\nthis step becomes. Now stick with me here. So my new value, this is what we call a weight update,\nI'm going to take w zero, and I'm going to set some new value for w zero. And what I'm going to\nset for that is the old value of w zero, plus some factor, which I'll just call alpha for now,\ntimes whatever this arrow is. So that's basically saying, okay, take our old w zero, our old weight,\nand just decrease it this way. So I guess increase it in this direction, right, like take a step in\nthis direction. But this alpha here is telling us, okay, don't don't take a huge step, right,\njust in case we're wrong, take a small step, take a small step in that direction, see if we get any\ncloser. And for those of you who, you know, do want to look more into the mathematics of things,\nthe reason why I use a plus here is because this here is the negative gradient, right, if this were\njust the if you were to use the actual gradient, this should be a minus.\nNow this alpha is something that we call the learning rate. Okay, and that adjusts how quickly\nwe're taking steps. And that might, you know, tell our that that will ultimately control\nhow long it takes for our neural net to converge. Or sometimes if you set it too high, it might even\ndiverge. But with all of these weights, so here I have w zero, w one, and then w n. We make the same\nupdate to all of them after we calculate the loss, the gradient of the loss with respect to that\nweight. So that's how back propagation works. And that is everything that's going on here. After we\ncalculate the loss, we're calculating gradients, making adjustments in the model. So we're setting\nall the all the weights to something adjusted slightly. And then we're going to calculate the\ngradient. And then we're saying, Okay, let's take the training set and run it through the model\nagain, and go through this loop all over again. So for machine learning, we already have seen some\nlibraries that we use, right, we've already seen SK learn. But when we start going into neural\nnetworks, this is kind of what we're trying to program. And it's not very fun to try to\ndo this from scratch, because not only will we probably have a lot of bugs, but also probably\nnot going to be fast enough, right? Wouldn't it be great if there are just some, you know,\nfull time professionals that are dedicated to solving this problem, and they could literally\njust give us their code that's already running really fast? Well, the answer is, yes, that exists.\nAnd that's why we use TensorFlow. So TensorFlow makes it really easy to define these models. But\nwe also have enough control over what exactly we're feeding into this model. So for example,\nthis line here is basically saying, Okay, let's create a sequential neural net. So sequential is\njust, you know, what we've seen here, it just goes one layer to the next. And a dense layer means that\na dense layer means that all of them are interconnected. So here, this is interconnected with all of these\nnodes, and this one's all these, and then this one gets connected to all of the next ones, and so on.\nSo we're going to create 16 dense nodes with relu activation functions. And then we're going\nto create another layer of 16 dense nodes with relu activation. And then our output layer is going\nto be just one node. Okay. And that's how easy it is to define something in TensorFlow. So TensorFlow\nis an open source library that helps you develop and train your ML models. Let's implement this\nfor a neural net. So we're using a neural net for classification. Now, so our neural net model,\nwe are going to use TensorFlow, and I don't think I imported that up here. So we are going to import\nthat down here. So I'm going to import TensorFlow as TF. And enter. Cool. So my neural net model\nis going to be, I'm going to use this. So essentially, this is saying layer all these\nthings that I'm about to pass in. So yeah, layer them linear stack of layers, layer them as a model.\nAnd what that means, nope, not that. So what that means is I can pass in\nsome sort of layer, and I'm just going to use a dense layer.\nOops, dot dense. And let's say we have 32 units. Okay, I will also\nset the activation as really. And at first we have to specify the input shape. So here we have 10,\nand comma. Alright. Alright, so that's our first layer. Now our next layer, I'm just going to have\nanother dense layer of 32 units all using relu. And that's it. So for the final layer, this is\njust going to be my output layer, it's going to just be one node. And the activation is going to\nbe sigmoid. So if you recall from our logistic regression, what happened there was when we had\na sigmoid, it looks something like this, right? So by creating a sigmoid activation on our last layer,\nwe're essentially projecting our predictions to be zero or one, just like in logistic regression.\nAnd that's going to help us, you know, we can just round to zero or one and classify that way.\nOkay. So this is my neural net model. And I'm going to compile this. So in TensorFlow,\nwe have to compile it. It's really cool, because I can just literally pass in what type of optimizer\nI want, and it'll do it. So here, if I go to optimizers, I'm actually going to use atom.\nAnd you'll see that, you know, the learning rate is 0.001. So I'm just going to use that default.\nSo 0.001. And my loss is going to be binary cross entropy. And the metrics that I'm also going to\ninclude on here, so it already will consider loss, but I'm, I'm also going to tack on accuracy.\nSo we can actually see that in a plot later on. Alright, so I'm going to run this.\nAnd one thing that I'm going to also do is I'm going to define these plot definitions. So I'm\nactually copying and pasting this, I got these from TensorFlow. So if you go on to some TensorFlow\ntutorial, they actually have these, this like, defined. And that's exactly what I'm doing here.\nSo I'm actually going to move this cell up, run that. So we're basically plotting the loss\nover all the different epochs. epochs means like training cycles. And we're going to run that. So\nmeans like training cycles. And we're going to plot the accuracy over all the epochs.\nAlright, so we have our model. And now all that's left is, let's train it. Okay.\nSo I'm going to say history. So TensorFlow is great, because it keeps track of the history\nof the training, which is why we can go and plot it later on. Now I'm going to set that equal to\nthis neural net model. And fit that with x train, y train, I'm going to make the number of epochs\nequal to let's say just let's just use 100 for now. And the batch size, I'm going to set equal to,\nlet's say 32. Alright. And the validation split. So what the validation split does, if it's down\nhere somewhere. Okay, so yeah, this validation split is just the fraction of the training data\nto be used as validation data. So essentially, every single epoch, what's going on is TensorFlow\nsaying, leave certain if this is point two, then leave 20% out. And we're going to test how the\nmodel performs on that 20% that we've left out. Okay, so it's basically like our validation data\nset. But TensorFlow does it on our training data set during the training. So we have now a measure\noutside of just our validation data set to see, you know, what's going on. So validation split,\nI'm going to make that 0.2. And we can run this. So if I run that, all right, and I'm actually going\nto set verbose equal to zero, which means, okay, don't print anything, because printing something\nfor 100 epochs might get kind of annoying. So I'm just going to let it run, let it train,\nand then we'll see what happens. Cool, so it finished training. And now what I can do is\nbecause you know, I've already defined these two functions, I can go ahead and I can plot the loss,\noops, loss of that history. And I can also plot the accuracy throughout the training.\nSo this is a little bit ish what we're looking for. We definitely are looking for a steadily\ndecreasing loss and an increasing accuracy. So here we do see that, you know, our validation\naccuracy improves from around point seven, seven or something all the way up to somewhere around\npoint, maybe eight one. And our loss is decreasing. So this is good. It is expected that the validation\nloss and accuracy is performing worse than the training loss or accuracy. And that's because\nour model is training on that data. So it's adapting to that data. Whereas the validation stuff is,\nyou know, stuff that it hasn't seen yet. So, so that's why. So in machine learning, as we saw above,\nwe could change a bunch of the parameters, right? Like I could change this to 64. So now it'd be\na row of 64 nodes, and then 32, and then one. So I can change some of these parameters.\nAnd a lot of machine learning is trying to find, hey, what do we set these hyper parameters to?\nSo what I'm actually going to do is I'm going to rewrite this so that we can do something what's\nknown as a grid search. So we can search through an entire space of hey, what happens if, you know,\nwe have 64 nodes and 64 nodes, or 16 nodes and 16 nodes, and so on. And then on top of all that,\nwe can, you know, we can change this learning rate, we can change how many epochs we can change,\nyou know, the batch size, all these things might affect our training. And just for kicks,\nI'm also going to add what's known as a dropout layer in here. And what dropout is doing is\nsaying, hey, randomly choose with at this rate, certain nodes, and don't train them in, you know,\nin a certain iteration. So this helps prevent overfitting. Okay, so I'm actually going to\ndefine this as a function called train model, we're going to pass in x train, y train,\nthe number of nodes, the dropout, you know, the probability that we just talked about\nlearning rate. So I'm actually going to say lr batch size. And we can also pass in number epochs,\nright? I mentioned that as a parameter. So indent this, so it goes under here. And with these two,\nI'm going to set this equal to number of nodes. And now with the two dropout layers, I'm going\nto set dropout prob. So now you know, the probability of turning off a node during the training\nis equal to dropout prob. And I'm going to keep the output layer the same. Now I'm compiling it,\nbut this here is now going to be my learning rate. And I still want binary cross entropy and\naccuracy. We are actually going to train our model inside of this function. But here we can do the\nepochs equal epochs, and this is equal to whatever, you know, we're passing in x train,\ny train belong right here. Okay, so those are getting passed in as well. And finally, at the\nend, I'm going to return this model and the history of that model. Okay. So now what I'll do\nis let's just go through all of these. So let's say let's keep epochs at 100. And now what I can\ndo is I can say, hey, for a number of nodes in, let's say, let's do 1632 and 64, to see what\nhappens for the different dropout probabilities. And I mean, zero would be nothing. Let's use 0.2.\nAlso, to see what happens. You know, for the learning rate in 0.005, 0.001. And you know,\nmaybe we want to throw on 0.1 in there as well. And then for the batch size, let's do 1632,\n64 as well. Actually, and let's also throw in 128. Actually, let's get rid of 16. Sorry,\nso 128 in there. That should be 01. I'm going to record the model and history using this\ntrain model here. So we're going to do x train y train, the number of nodes is going to be,\nyou know, the number of nodes that we've defined here, dropout, prob, LR, batch size, and epochs.\nOkay. And then now we have both the model and the history. And what I'm going to do is again,\nI want to plot the loss for the history. I'm also going to plot the accuracy.\nProbably should have done them side by side, that probably would have been easier.\nOkay, so what I'm going to do is split up, split this up. And that will be\nthe subplots. So now this is just saying, okay, I want one row and two columns in that row for my\nplots. Okay, so I'm going to plot on my axis one, the loss. I don't actually know this is going to\nwork. Okay, we don't care about the grid. Yeah, let's let's keep the grid. And then now my other.\nSo now on here, I'm going to plot all the accuracies on the second plot.\nI might have to debug this a bit.\nWe should be able to get rid of that. If we run this, we already have history saved as a variable\nin here. So if I just run it on this, okay, it has no attribute x label. Oh, I think it's because\nit's like set x label or something. Okay, yeah, so it's, it's set instead of just x label, y label.\nSo let's see if that works. All right, cool. Um, and let's actually make this a bit larger.\nOkay, so we can actually change the figure size that I'm gonna set. Let's see what happens if I\nset that to. Oh, that's not the way I wanted it. Okay, so that looks reasonable.\nAnd that's just going to be my plot history function. So now I can plot them side by side.\nHere, I'm going to plot the history. And what I'm actually going to do is I so here, first,\nI'm going to print out all these parameters. So I'm going to print out\nthe F string to print out all of this stuff. So here, I'm going to print out all these parameters.\nUh, all of this stuff. So here, I'm printing out how many nodes, um, the dropout probability,\nuh, the learning rate.\nAnd we already know how many you found, so I'm not even going to bother with that.\nSo once we plot this, uh, let's actually also figure out what the, um, what the validation\nlosses on our validation set that we have that we created all the way back up here.\nAlright, so remember, we created three data sets. Let's call our model and evaluate what the\nvalidation data with the validation data sets loss would be. And I actually want to record,\nlet's say I want to record whatever model has the least validation loss. So\nfirst, I'm going to initialize that to infinity so that you know, any model will beat that score.\nSo if I do float infinity, that will set that to infinity. And maybe I'll keep\ntrack of the parameters. Actually, it doesn't really matter. I'm just going to keep track of\nthe model. And I'm gonna set that to none. So now down here, if the validation loss is ever\nless than the least validation loss, then I am going to simply come down here and say,\nHey, this validation for this least validation loss is now equal to the validation loss.\nAnd the least loss model is whatever this model is that just earned that validation loss. Okay.\nSo we are actually just going to let this run for a while. And then we're going to get our least\nlast model after that. So let's just run. All right, and now we wait.\nAll right, so we've finally finished training. And you'll notice that okay, down here, the loss\nactually gets to like 0.29. The accuracy is around 88%, which is pretty good. So you might be wondering,\nokay, why is this accuracy in this? Like, these are both the validation. So this accuracy here\nis on the validation data set that we've defined at the beginning, right? And this one here,\nthis is actually taking 20% of our tests, our training set every time during the training,\nand saying, Okay, how much of it do I get right now? You know, after this one step where I didn't\ntrain with any of that. So they're slightly different. And actually, I realized later on\nthat I probably you know, probably what I should have done is over here, when we were defining\nthe model fit, instead of the validation split, you can define the validation data.\nAnd you can pass in the validation data, I don't know if this is the proper syntax. But\nthat's probably what I should have done. But instead, you know, we'll just stick with what\nwe have here. So you'll see at the end, you know, with the 64 nodes, it seems like this is our best\nperformance 64 nodes with a dropout of 0.2, a learning rate of 0.001, and a batch size of 64.\nAnd it does seem like yes, the validation, you know, the fake validation, but the validation\nloss is decreasing, and then the accuracy is increasing, which is a good sign. Okay,\nso finally, what I'm going to do is I'm actually just going to predict. So I'm going to take\nthis model, which we've called our least loss model, I'm going to take this model,\nand I'm going to predict x test on that. And you'll see that it gives me some values that\nare really close to zero and some that are really close to one. And that's because we have a sigmoid\noutput. So if I do this, and what I can do is I can cast them. So I'm going to say anything that's\ngreater than 0.5, set that to one. So if I actually, I think what happens if I do this?\nOh, okay, so I have to cast that as type. And so now you'll see that it's ones and zeros. And I'm\nactually going to transform this into a column as well. So here I'm going to Oh, oops, I didn't\nI didn't mean to do that. Okay, no, I wanted to just reshape it to that. So now it's one dimensional.\nOkay. And using that we can actually just rerun the classification report based on these this\nneural net output. And you'll see that okay, the the F ones are the accuracy gives us 87%. So it\nseems like what happened here is the precision on class zero. So the hadrons has increased a bit,\nbut the recall decreased. But the F one score is still at a good point eight one. And for the other\nclass, it looked like the precision decreased a bit the recall increased for an overall F one score.\nThat's also been increased. I think I interpreted that properly. I mean, we went through all this\nwork and we got a model that performs actually very, very similarly to the SVM model that we\nhad earlier. And the whole point of this exercise was to demonstrate, okay, these are how you can\ndefine your models. But it's also to say, hey, maybe, you know, neural nets are very, very\npowerful, as you can tell. But sometimes, you know, an SVM or some other model might actually be more\nappropriate. But in this case, I guess it didn't really matter which one we use at the end. An 87%\naccuracy score is still pretty good. So yeah, let's now move on to regression.\nWe just saw a bunch of different classification models. Now let's shift gears into regression,\nthe other type of supervised learning. If we look at this plot over here, we see a bunch of scattered\ndata points. And here we have our x value for those data points. And then we have the corresponding y\nvalue, which is now our label. And when we look at this plot, well, our goal in regression is to find\nthe line of best fit that best models this data. Essentially, we're trying to let's say we're given\nsome new value of x that we don't have in our sample, we're trying to say, okay, what would my\nprediction for y be for that given x value. So that, you know, might be somewhere around there.\nI don't know. But remember, in regression that, you know, given certain features,\nwe're trying to predict some continuous numerical value for y.\nIn linear regression, we want to take our data and fit a linear model to this data. So in this case,\nour linear model might look something along the lines of here. Right. So this here would be\nconsidered as maybe our line of best fit. And this line is modeled by the equation, I'm going to write\nit down here, y equals b zero, plus b one x. Now b zero just means it's this y intercept. So if we\nextend this y down here, this value here is b zero, and then b one defines the source of the\nline, defines the slope of this line. Okay. All right. So that's the that's the formula\nfor linear regression. And how exactly do we come up with that formula? What are we trying to do\nwith this linear regression? You know, we could just eyeball where the line be, but humans are\nnot very good at eyeballing certain things like that. I mean, we can get close, but a computer is\nbetter at giving us a precise value for b zero and b one. Well, let's introduce the concept of\nsomething known as a residual. Okay, so residual, you might also hear this being called the error.\nAnd what that means is, let's take some data point in our data set. And we're going to evaluate how\nfar off is our prediction from a data point that we already have. So this here is our y, let's say,\nthis is 12345678. So this is y eight, let's call it, you'll see that I use this y i in order to\nI in order to represent, hey, just one of these points. Okay. So this here is why and this here\nwould be the prediction. Oops, this here would be the prediction for y eight, which I've labeled\nwith this hat. Okay, if it has a hat on it, that means hey, this is what this is my guess this is\nmy prediction for you know, this specific value of x. Okay. Now the residual would be this distance\nhere between y eight and y hat eight. So y eight minus y hat eight. All right, because that would\ngive us this here. And I'm just going to take the absolute value of this. Because what if it's below\nthe line, right, then you would get a negative value, but distance can't be negative. So we're\njust going to put a little hat, or we're going to put a little absolute value around this quantity.\nAnd that gives us the residual or the error. So let me rewrite that. And you know, to generalize\nto all the points, I'm going to say the residual can be calculated as y i minus y hat of i. Okay.\nSo this just means the distance between some given point, and its prediction, its corresponding\nprediction on the line. So now, with this residual, this line of best fit is generally trying to\ndecrease these residuals as much as possible. So now that we have some value for the error,\nour line of best fit is trying to decrease the error as much as possible for all of the different\ndata points. And that might mean, you know, minimizing the sum of all the residuals. So this\nhere, this is the sum symbol. And if I just stick the residual calculation in there,\nit looks something like that, right. And I'm just going to say, okay, for all of the eyes in our\ndata set, so for all the different points, we're going to sum up all the residuals. And I'm going\nto try to decrease that with my line of best fit. So I'm going to find the B0 and B1, which gives\nme the lowest value of this. Okay. Now in other, you know, sometimes in different circumstances,\nwe might attach a squared to that. So we're trying to decrease the sum of the squared residuals.\nAnd what that does is it just, you know, it adds a higher penalty for how far off we are from,\nyou know, points that are further off. So that is linear regression, we're trying to find\nthis equation, some line of best fit that will help us decrease this measure of error\nwith respect to all the data points that we have in our data set, and try to come up with\nthe best prediction for all of them. This is known as simple linear regression.\nAnd basically, that means, you know, our equation looks something like this. Now, there's also\nmultiple linear regression, which just means that hey, if we have more than one value for x, so like\nthink of our feature vectors, we have multiple values in our x vector, then our predictor might\nlook something more like this. Actually, I'm just going to say etc, plus b n, x n. So now I'm coming\nup with some coefficient for all of the different x values that I have in my vector. Now you guys\nmight have noticed that I have some assumptions over here. And you might be asking, okay, Kylie,\nwhat in the world do these assumptions mean? So let's go over them.\nSo let's go over them. The first one is linearity.\nAnd what that means is, let's say I have a data set. Okay.\nLinearity just means, okay, my does my data follow a linear pattern? Does y increase as x\nincreases? Or does y decrease at as x increases? Does so if y increases or decreases at a constant\nrate as x increases, then you're probably looking at something linear. So what's the example of a\nnonlinear data set? Let's say I had data that might look something like that. Okay. So now just\nvisually judging this, you might say, okay, seems like the line of best fit might actually be some\ncurve like this. Right. And in this case, we don't satisfy that linearity assumption anymore.\nSo with linearity, we basically just want our data set to follow some sort of linear trajectory.\nAnd independence, our second assumption\njust means this point over here, it should have no influence on this point over here,\nor this point over here, or this point over here. So in other words, all the points,\nall the samples in our data set should be independent. Okay, they should not rely on\none another, they should not affect one another.\nOkay, now, normality and homoscedasticity, those are concepts which use this residual. Okay. So if\nI have a plot that looks something like this, and I have a plot that looks like this. Okay,\nsomething like this. And my line of best fit is somewhere here, maybe it's something like that.\nIn order to look at these normality and homoscedasticity assumptions, let's look at\nthe residual plot. Okay. And what that means is I'm going to keep my same x axis. But instead\nof plotting now where they are relative to this y, I'm going to plot these errors. So now I'm\ngoing to plot y minus y hat like this. Okay. And now you know, this one is slightly positive,\nso it might be here, this one down here is negative, it might be here. So our residual plot,\nit's literally just a plot of how you know, the values are distributed around our line of best\nfit. So it looks like it might, you know, look something like this. Okay. So this might be our\nresidual plot. And what normality means, so our assumptions are normality and homoscedasticity,\nI might have butchered that spelling, I don't really know. But what normality is saying is\nsaying, okay, these residuals should be normally distributed. Okay, around this line of best fit,\nit should follow a normal distribution. And now what homoscedasticity says, okay, our variants\nof these points should remain constant throughout. So this spread here should be approximately the\nsame as this spread over here. Now, what's an example of where you know, homoscedasticity is\nnot held? Well, let's say that our original plot actually looks something like this.\nOkay, so now if we looked at the residuals for that, it might look something\nlike that. And now if we look at this spread of the points, it decreases, right? So now the spread\nis not constant, which means that homoscedasticity, this assumption would not be fulfilled, and it\nmight not be appropriate to use linear regression. So that's just linear regression. Basically,\nwe have a bunch of data points, we want to predict some y value for those. And we're trying to come\nup with this line of best fit that best describes, hey, given some value x, what would be my best\nguess of what y is. So let's move on to how do we evaluate a linear regression model. So the first\nmeasure that I'm going to talk about is known as mean absolute error, or MAE\nfor short, okay. And mean absolute error is basically saying, all right, let's take\nall the errors. So all these residuals that we talked about, let's sum up the distance\nfor all of them, and then take the average. And then that can describe, you know, how far off are\nwe. So the mathematical formula for that would be, okay, let's take all the residuals.\nAlright, so this is the distance. Actually, let me redraw a plot down here. So\nsuppose I have a data set, look like this. And here are all my data points, right. And now let's\nsay my line looks something like that. So my mean absolute error would be summing up all of these\nvalues. This was a mistake. So summing up all of these, and then dividing by how many data points\nI have. So what would be all the residuals, it would be y i, right, so every single point,\nminus y hat i, so the prediction for that on here. And then we're going to sum over all of\nall of the different i's in our data set. Right, so i, and then we divide by the number of points\nwe have. So actually, I'm going to rewrite this to make it a little clearer. So i is equal to\nwhatever the first data point is all the way through the nth data point. And then we divide\nit by n, which is how many points there are. Okay, so this is our measure of mae. And this is basically\ntelling us, okay, in on average, this is the distance between our predicted value and the\nactual value in our training set. Okay. And mae is good because it allows us to, you know, when we\nget this value here, we can literally directly compare it to whatever units the y value is in.\nSo let's say y is we're talking, you know, the prediction of the price of a house, right, in\ndollars. Once we have once we calculate the mae, we can literally say, oh, the average, you know,\nprice, the average, how much we're off by is literally this many dollars. Okay. So that's the\nmean absolute error. An evaluation technique that's also closely related to that is called the mean\nsquared error. And this is MSE for short. Okay. Now, if I take this plot again, and I duplicated\nand move it down here, well, the gist of mean squared error is kind of the same, but instead\nof the absolute value, we're going to square. So now the MSE is something along the lines of,\nokay, let's sum up something, right, so we're going to sum up all of our errors.\nSo now I'm going to do y i minus y hat i. But instead of absolute valuing them,\nI'm going to square them all. And then I'm going to divide by n in order to find the mean. So\nbasically, now I'm taking all of these different values, and I'm squaring them first before I add\nthem to one another. And then I divide by n. And the reason why we like using mean squared error\nis that it helps us punish large errors in the prediction. And later on, MSE might be important\nbecause of differentiability, right? So a quadratic equation is differentiable, you know,\nif you're familiar with calculus, a quadratic equation is differentiable, whereas the absolute\nvalue function is not totally differentiable everywhere. But if you don't understand that,\ndon't worry about it, you won't really need it right now. And now one downside of mean squared\nerror is that once I calculate the mean squared error over here, and I go back over to y, and I\nwant to compare the values. Well, it gets a little bit trickier to do that because now my mean squared\nerror is in terms of y squared, right? It's this is now squared. So instead of just dollars, how,\nyou know, how many dollars off am I I'm talking how many dollars squared off am I. And that,\nyou know, to humans, it doesn't really make that much sense. Which is why we have created\nsomething known as the root mean squared error. And I'm just going to copy this diagram over here\nbecause it's very, very similar to mean squared error. Except now we take a big squared root.\nOkay, so this is our messy, and we take the square root of that mean squared error. And so now the\nterm in which you know, we're defining our error is now in terms of that dollar sign symbol again.\nSo that's a pro of root mean squared error is that now we can say, okay, our error according\nto this metric is this many dollar signs off from our predictor. Okay, so it's in the same unit,\nwhich is one of the pros of root mean squared error. And now finally, there is the coefficient\nof determination, or r squared. And this is a formula for r squared. So r squared is equal\nto one minus RSS over TSS. Okay, so what does that mean? Basically, RSS stands for the sum\nof the squared residuals. So maybe it should be SSR instead, but\nRSS sum of the squared residuals, and this is equal to if I take the sum of all the values,\nand I take y i minus y hat, i, and square that, that is my RSS, right, it's a sum of the squared\nresiduals. Now TSS, let me actually use a different color for that.\nSo TSS is the total sum of squares.\nAnd what that means is that instead of being with respect to this prediction,\nwe are instead going to\ntake each y value and just subtract the mean of all the y values, and square that.\nOkay, so if I drew this out,\nand if this were my\nactually, let's use a different color. Let's use green. If this were my predictor,\nso RSS is giving me this measure here, right? It's giving me some estimate of how far off we are from\nour regressor that we predicted. Actually, I'm gonna take this one, and I'm gonna take this one,\nand actually, I'm going to use red for that. Well, TSS, on the other hand, is saying, okay,\nhow far off are these values from the mean. So if we literally didn't do any calculations for the\nline of best fit, if we just took all the y values and average all of them, and said, hey,\nthis is the average value for every single x value, I'm just going to predict that average value\ninstead, then it's asking, okay, how far off are all these points from that line?\nOkay, and remember that this square means that we're punishing larger errors, right? So even if\nthey look somewhat close in terms of distance, the further a few data points are, then the further\nthe larger our total sum of squares is going to be. Sorry, that was my dog. So the total sum of\nsquares is taking all of these values and saying, okay, what is the sum of squares, if I didn't do\nany regressor, and I literally just calculated the average of all the y values in my data set,\nand for every single x value, I'm just going to predict that average, which means that okay,\nlike, that means that maybe y and x aren't associated with each other at all. Like the\nbest thing that I can do for any new x value, just predict, hey, this is the average of my data set.\nAnd this total sum of squares is saying, okay, well, with respect to that average,\nwhat is our error? Right? So up here, the sum of the squared residuals, this is telling us what is\nour what what is our error with respect to this line of best fit? Well, our total sum of squares\nsaying what is the error with respect to, you know, just the average y value. And if our line\nof best fit is a better fit, then this total sum of squares, that means that you know, this numerator,\nthat means that this numerator is going to be smaller than this denominator, right?\nAnd if our errors in our line of best fit are much smaller, then that means that this ratio\nof the RSS over TSS is going to be very small, which means that R squared is going to go towards\none. And now when R squared is towards one, that means that that's usually a sign that we have a\ngood predictor. It's one of the signs, not the only one. So over here, I also have, you know,\nthat there's this adjusted R squared. And what that does, it just adjusts for the number of terms.\nSo x1, x2, x3, etc. It adjusts for how many extra terms we add, because usually when we,\nyou know, add an extra term, the R squared value will increase because that'll help us predict\ny some more. But the value for the adjusted R squared increase if the new term actually\nimproves this model fit more than expected, you know, by chance. So that's what adjusted\nR squared is. I'm not, you know, it's out of the scope of this one specific course.\nAnd now that's linear regression. Basically, I've covered the concept of residuals or errors.\nAnd, you know, how do we use that in order to find the line of best fit? And you know,\nour computer can do all the calculations for us, which is nice. But behind the scenes,\nit's trying to minimize that error, right? And then we've gone through all the different\nways of actually evaluating a linear regression model and the pros and cons of each one.\nSo now let's look at an example. So we're still on supervised learning. But now we're just going to\ntalk about regression. So what happens when you don't just want to predict, you know, type 123?\nWhat happens if you actually want to predict a certain value? So again, I'm on the UCI machine\nlearning repository. And here I found this data set about bike sharing in Seoul, South Korea.\nSo this data set is predicting rental bike count. And here it's the kind of bikes rented at each\nhour. So what we're going to do, again, you're going to go into the data folder, and you're going\nto download this CSV file. And we're going to move over to collab again. And here I'm going to name\nthis FCC bikes and regression. I don't remember what I called the last one. But yeah, FCC bikes\nregression. Now I'm going to import a bunch of the same things that I did earlier. And, you know,\nI'm going to also continue to import the oversampler and the standard scaler. And then I'm actually\nalso just going to let you guys know that I have a few more things I wanted import. So this is a\nlibrary that lets us copy things. Seaborn is a wrapper over a matplotlib. So it also allows us\nto plot certain things. And then just letting you know that we're also going to be using\nTensorFlow. Okay, so one more thing that we're also going to be using, we're going to use the\nsklearn linear model library. Actually, let me make my screen a little bit bigger. So yeah,\nawesome. Run this and that'll import all the things that we need. So again, I'm just going to,\nyou know, give some credit to where we got this data set. So let me copy and paste this UCI thing.\nAnd I will also give credit to this here.\nOkay, cool. All right, cool. So this is our data set. And again, it tells us all the different\nattributes that we have right here. So I'm actually going to go ahead and paste this in here.\nFeel free to copy and paste this if you want me to read it out loud, so you can type it.\nIt's byte count, hour, temp, humidity, wind, visibility, dew point, temp, radiation, rain,\nsnow, and functional, whatever that means. Okay, so I'm going to come over here and import my data\nby dragging and dropping. All right. Now, one thing that you guys might actually need to do is\nyou might actually have to open up the CSV because there were, at first, a few like forbidding\ncharacters in mine, at least. So you might have to get rid of like, I think there was a degree here,\nbut my computer wasn't recognizing it. So I got rid of that. So you might have to go through\nand get rid of some of those labels that are incorrect. I'm going to do this. Okay. But\nafter we've done that, we've imported in here, I'm going to create a data a data frame from that. So,\nall right, so now what I can do is I can read that CSV file and I can get the data into here.\nSo so like data dot CSV. Okay, so now if I call data dot head, you'll see that I have all the\nvarious labels, right? And then I have the data in there. So I'm going to from here, I'm actually\ngoing to get rid of some of these columns that, you know, I don't really care about. So here,\nI'm going to, when I when I type this in, I'm going to drop maybe the date, whether or not it's a\nholiday, and the various seasons. So I'm just not going to care about these things. Access equals\none means drop it from the columns. So now you'll see that okay, we still have, I mean,\nI guess you don't really notice it. But if I set the data frames columns equal to data set calls,\nand I look at, you know, the first five things, then you'll see that this is now our data set.\nIt's a lot easier to read. So another thing is, I'm actually going to\ndf functional. And we're going to create this. So remember that our computers are not very good\nat language, we want it to be in zeros and ones. So here, I will convert that.\nWell, if this is equal to yes, then that that gets mapped as one. So then set type integer. All right.\nGreat. Cool. So the thing is, right now, these by counts are for whatever hour. So\nto make this example simpler, I'm just going to index on an hour, and I'm gonna say, okay,\nwe're only going to use that specific hour. So I'm just going to index on an hour, and I'm\ngoing to use an hour. So here, let's say. So this data frame is only going to be data frame where\nthe hour, let's say it equals 12. Okay, so it's noon. All right. So now you'll see that all the\nequal to 12. And I'm actually going to now drop that column. Our access equals one. Alright,\nso we run this cell. Okay, so now we got rid of the hour in here. And we just have the by count,\nthe temperature, humidity, wind, visibility, and yada, yada, yada. Alright, so what I want to do\nis I'm going to actually plot all of these. So for i in all the columns, so the range, length of\nwhatever its data frame is, and all the columns, because I don't have by count as\nactually, it's my first thing. So what I'm going to do is say for a label in data frame,\ncolumns, everything after the first thing, so that would give me the temperature and\nonwards. So these are all my features, right? I'm going to just scatter. So I want to see how that\nlabel how that specific data, how that affects the by count. So I'm going to plot the bike count on\nthe y axis. And I'm going to plot, you know, whatever the specific label is on the x axis.\nAnd I'm going to title this, whatever the label is. And, you know, make my y label, the bike count\nat noon. And the x label as just the label. Okay, now, I guess we don't even need the legend.\nWe don't even need the legend. So just show that plot. All right. So it seems like functional is\nnot really doesn't really give us any utility. So then snow rain seems like this radiation,\nyou know, is fairly linear dew point temperature, visibility, wind doesn't really seem like it does\nmuch humidity, kind of maybe like an inverse relationship. But the temperature definitely\nlooks like there's a relationship between that and the number of bikes, right. So what I'm actually\ngoing to do is I'm going to drop some of the ones that don't don't seem like they really matter. So\nmaybe wind, you know, visibility. Yeah, so I'm going to get rid of when visibility and functional.\nSo now data frame, and I'm going to drop wind, visibility, and functional. All right. And the\naxis again is the column. So that's one. So if I look at my data set, now, I have just the\ntemperature, the humidity, the dew point temperature, radiation, rain, and snow. So again,\nwhat I want to do is I want to split this into my training, my validation and my test data set,\njust as we talked before. Here, we can use the exact same thing that we just did. And we can say\nnumpy dot split, and sample, you know that the whole sample, and then create our splits\nof the data frame. And we're going to do that. But now set this to eight. Okay.\nSo I don't really care about, you know, the the full grid, the full array. So I'm just going to\nuse an underscore for that variable. But I will get my training x and y's. And actually, I don't\nhave a function for getting the x and y's. So here, I'm going to write a function defined,\nget x y. And I'm going to pass in the data frame. And I'm actually going to pass in what the name\nof the y label is, and what the x what specific x labels I want to look at. So here, if that's none,\nthen I'm just like, like, I'm only going to I'm going to get everything from the data set. That's\nnot the wildlife. So here, I'm actually going to make first a deep copy of my data frame.\nAnd that basically means I'm just copying everything over. If, if like x labels is none,\nso if not x labels, then all I'm going to do is say, all right, x is going to be whatever this\ndata frame is. And I'm just going to take all the columns. So C for C, and data frame, dot columns,\nif C does not equal the y label, right, and I'm going to get the values from that. But if there\nis the x labels, well, okay, so in order to index only one thing, so like, let's say I pass in only\none thing in here, then my data frame is, so let me make a case for that. So if the length of x\nlabels is equal to one, then what I'm going to do is just say that this is going to be x labels,\nand add that just that label values, and I actually need to reshape to make this 2d.\nSo I'm going to pass in negative one comma one there. Now, otherwise, if I have like a list of\nspecific x labels that I want to use, then I'm actually just going to say x is equal to data\nframe of those x labels, dot values. And that should suffice. Alright, so now that's just me\nextracting x. And in order to get my y, I'm going to do y equals data frame, and then passing the y\nlabel. And at the very end, I'm going to say data equals NP dot h stack. So I'm stacking them horizontally\none next to each other. And I'll take x and y, and return that. Oh, but this needs to be values.\nAnd I'm actually going to reshape this to make it 2d as well so that we can do this h stack.\nAnd I will return data x, y. So now I should be able to say, okay, get x, y, and take that data\nframe. And the y label, so my y label is byte count. And actually, so for the x label, I'm actually\ngoing to let's just do like one dimension right now. And earlier, I got rid of the plots, but we\nhad seen that maybe, you know, the temperature dimension does really well. And we might be able\nto use that to predict why. So I'm going to label this also that, you know, it's just using the\ntemperature. And I am also going to do this again for, oh, this should be train. And this should be\nvalidation. And this should be a test. Because oh, that's Val. Right. But here, it should be Val.\nAnd this should be test. Alright, so we run this and now we have our training validation and test\ndata sets for just the temperature. So if I look at x train temp, it's literally just the temperature.\nOkay, and I'm doing this first to show you simple linear regression. Alright, so right now I can\ncreate a regressor. So I can say the temp regressor here. And then I'm going to, you know, make a\nlinear regression model. And just like before, I can simply fix fit my x train temp, y train temp\nin order to train train this linear regression model. Alright, and then I can also, I can print\nthis regressor is coefficients and the intercept. So if I do that, okay, this is the coefficient\nfor whatever the temperature is, and then the the x intercept, okay, or the y intercept, sorry. All\nright. And I can, you know, score, so I can get the the r squared score. So I can score x test\nand y test. All right, so it's an r squared of around point three eight, which is better than\nzero, which would mean, hey, there's absolutely no association. But it's also not, you know, like,\ngood, it depends on the context. But, you know, the higher that number, it means the higher that\nthe two variables would be correlated, right? Which here, it's all right. It just means there's\nmaybe some association between the two. But the reason why I want to do this one D was to show\nyou, you know, if we plotted this, this is what it would look like. So if I create a scatterplot,\nand let's take the training. So this is our data. And then let's make it blue. And then if I\nalso plotted, so something that I can do is say, you know, the x range, I'm going to plot it,\nis when space, and this goes from negative 20 to 40, this piece of data. So I'm going to just say,\nlet's take 100 things from there. So I'm going to plot x, and I'm going to take this temper,\nthis, like, regressor, and predict x with that. Okay, and this label, I'm going to label that\nthe fit. And this color, let's make this red. And let's actually set the line with, so I can,\nI can change how thick that value is. Okay. Now at the very end, let's create a legend. And let's,\nall right, let's also create, you know, title, all these things that matter, in some sense. So\nhere, let's just say, this would be the bikes, versus the temperature, right? And the y label\nwould be number of bikes. And the x label would be the temperature. So I actually think that this\nmight cause an error. Yeah. So it's expecting a 2d array. So we actually have to reshape this.\nOkay, there we go. So I just had to make this an array and then reshape it. So it was 2d. Now,\nwe see that, all right, this increases. But again, remember those assumptions that we had about\nlinear regression, like this, I don't really know if this fits those assumptions, right? I just\nwanted to show you guys though, that like, all right, this is what a line of s fit through this\ndata would look like. Okay. Now, we can do multiple linear regression, right. So I'm going to go ahead\nand do that as well. Now, if I take my data set, and instead of the labels, it's actually what's\nmy current data set right now. Alright, so let's just use all of these except for the byte count,\nright. So I'm going to just say for the x labels, let's just take the data frames columns and just\nremove the byte count. So does that work? So if this part should be of x labels is none. And then\nthis should work now. Oops, sorry. Okay, so I have Oh, but this here, because it's not just the\ntemperature anymore, we should actually do this, let's say all, right. So I'm just going to quickly\nrerun this piece here so that we have our temperature only data set. And now we have our\nall data set. Okay. And this regressor, I can do the same thing. So I can do the all regressor.\nAnd I'm going to make this the linear regression. And I'm going to fit this to x train all and y\ntrain all. Okay. Alright, so let's go ahead and also score this regressor. And let's see how the\nR squared performs now. So if I test this on the test data set, what happens? Alright, so our R\nsquare seems to improve it went from point four to point five, two, which is a good sign. Okay.\nAnd I can't necessarily plot, you know, every single dimension. But this just this is just\nto say, okay, this is this is improved, right? Alright, so one cool thing that you can do with\ntensorflow is you can actually do regression, but with the neural net. So here, I'm going\nto we already have our our training data for just the temperature and just, you know, for all the\ndifferent columns. So I'm not going to bother with splitting up the data again, I'm just going to go\nahead and start building the model. So in this linear regression model, typically, you know,\nit does help if we normalize it. So that's very easy to do with tensorflow, I can just create some\nnormalizer layer. So I'm going to do tensorflow Keras layers, and get the normalization layer.\nAnd the input shape for that will just be one because let's just do it again on just the\ntemperature and the access I will make none. Now for this temp normalizer, and I should have had\nan equal sign there. I'm going to adapt this to X train temp, and reshape this to just a single vector.\nSo that should work great. Now with this model, so temp neural net model, what I can do is I can do,\nyou know, dot keras, sequential. And I'm going to pass in this normalizer layer. And then I'm\ngoing to say, hey, just give me one single dense layer with one single unit. And what that's doing\nis saying, all right, well, one single node just means that it's linear. And if you don't add any\nsort of activation function to it, the output is also linear. So here, I'm going to have tensorflow\nKeras layers dot dense. And I'm just going to have one unit. And that's going to be my model. Okay.\nSo with this model, let's compile. And for our optimizer, let's use,\nlet's use the atom again, dot atom, and we have to pass in the learning rate. So learning rate,\nand our learning rate, let's do 0.01. And now, the loss, we actually let's get this one 0.1. And the\nloss, I'm going to do mean squared error. Okay, so we run that we've compiled it, okay, great.\nAnd just like before, we can call history. And I'm going to fit this model. So here,\nif I call fit, I can just fit it, and I'm going to take the x train with the temperature,\nbut reshape it. Y train for the temperature. And I'm going to set verbose equal to zero so\nthat it doesn't, you know, display stuff. I'm actually going to set epochs equal to, let's do\n1000. And the validation data should be let's pass in the validation data set here\nas a tuple. And I know I spelled that wrong. So let's just run this.\nAnd up here, I've copied and pasted the plot loss from our previous but changed the y label\nto MSC. Because now we're talking we're dealing with mean squared error. And I'm going to plot\nthe loss of this history after it's done. So let's just wait for this to finish training and then to\nplot. Okay, so this actually looks pretty good. We see that the value is still the same. So\nthis actually looks pretty good. We see that the values are converging. So now what I can do is\nI'm going to go back up and take this plot. And we are going to just run that plot again. So\nhere, instead of this temperature regressor, I'm going to use the neural net regressor.\nThis neural net model.\nAnd if I run that, I can see that, you know, this also gives me a linear regressor,\nyou'll notice that this this fit is not entirely the same as the one\nup here. And that's due to the training process of, you know, of this neural net. So just two\ndifferent ways to try and try to find the best linear regressor. Okay, but here we're using back\npropagation to train a neural net node, whereas in the other one, they probably are not doing that.\nOkay, they're probably just trying to actually compute the line of s fit. So, okay, given this,\nwell, we can repeat the exact same exercise with our with our multiple linear regressions. Okay,\nbut I'm actually going to skip that part. I will leave that as an exercise to the viewer. Okay,\nso now what would happen if we use a neural net, a real neural net instead of just, you know,\none single node in order to predict this. So let's start on that code, we already have our\nnormalizer. So I'm actually going to take the same setup here. But instead of, you know, this\none dense layer, I'm going to set this equal to 32 units. And for my activation, I'm going to use\nRelu. And now let's duplicate that. And for the final output, I just want one answer. So I just\nwant one cell. And this activation is also going to be Relu, because I can't ever have less than\nzero bytes. So I'm just going to set that as Relu. I'm just going to name this the neural net model.\nOkay. And at the bottom, I'm going to have this neural net model. I'm going to have this neural\nnet model, I'm going to compile. And I will actually use the same compiler here. But instead of\ninstead of a learning rate of 0.01, I'll use 0.001. Okay. And I'm going to train this here.\nSo the history is this neural net model. And I'm going to fit that against x train temp, y train\ntemp, and valid validation data, I'm going to set this again equal to x val temp, and y val temp.\nNow, for the verbose, I'm going to say equal to zero epochs, let's do 100. And here for the batch\nsize, actually, let's just not do a batch size right now. Let's just try it. Let's see what happens\nhere. And again, we can plot the loss of this history after it's done training. So let's just\nrun this. And that's not what we're supposed to get. So what is going on? Here is sequential,\nwe have our temperature normalizer, which I'm wondering now if we have to redo that.\nDo that. Okay, so we do see this decline, it's an interesting curve, but we do we do see it eventually.\nSo this is our loss, which all right, if decreasing, that's a good sign.\nAnd actually, what's interesting is let's just let's plot this model again. So here instead of that.\nAnd you'll see that we actually have this like, curve that looks something like this. So actually,\nwhat if I got rid of this activation? Let's train this again. And see what happens.\nAlright, so even even when I got rid of that really at the end, it kind of knows, hey, you know, if\nit's not the best model, if we had maybe one more layer in here, these are just things that you have\nto play around with. When you're, you know, working with machine learning, it's like, you don't really\nknow what the best model is going to be. For example, this also is not brilliant. But I guess\nit's okay. So my point is, though, that with a neural net, I mean, this is not brilliant, but also\nthere's like no data down here, right? So it's kind of hard for our model to predict. In fact,\nwe probably should have started the prediction somewhere around here. My point, though, is that\nwith this neural net model, you can see that this is no longer a linear predictor, but yet we still\nget an estimate of the value, right? And we can repeat this exact same exercise, right? So let's\ndo that. Right. And we can repeat this exact same exercise with the multiple inputs. So here,\nif I now pass in all of the data, so this is my all normalizer,\nand I should just be able to pass in that. So let's move this to the next cell. Here,\nI'm going to pass in my all normalizer. And let's compile it. Yeah, those parameters look good.\nGreat. So here with the history, when we're trying to fit this model, instead of temp,\nwe're going to use our larger data set with all the features. And let's just train that.\nAnd of course, we want to plot the loss.\nOkay, so that's what our loss looks like. So an interesting curve, but it's decreasing.\nSo before we saw that our R squared score was around point five, two. Well, we don't really have\nthat with a neural net anymore. But one thing that we can measure is hey, what is the mean squared\nerror, right? So if I come down here, and I compare the two mean squared errors, so\nso I can predict x test all right. So these are my predictions using that linear regressor,\nwill linear multiple multiple linear regressor. So these are my live predictions, linear regression.\nOkay. I'm actually going to do that at the bottom. So let me just copy and paste that cell and bring\nit down here. So now I'm going to calculate the mean squared error for both the linear regressor\nand the neural net. Okay, so this is my linear and this is my neural net. So if I do my neural net\nmodel, and I predict x test all, I get my two, you know, different y predictions. And I can calculate\nthe mean squared error, right? So if I want to get the mean squared error, and I have y prediction\nand y real, I can do numpy dot square, and then I would need the y prediction minus, you know, the\nreal. So this this is basically squaring everything. And this should be a vector. So if I just take\nthis entire thing and take the mean of that, that should give me the MSC. So let's just try that out.\nAnd the y real is y test all, right? So that's my mean squared error for the linear regressor.\nAnd this is my mean squared error for the neural net. So that's interesting. I will debug this live,\nI guess. So my guess is that it's probably coming from this normalization layer. Because this input\nshape is probably just six. And okay, so that works now. And the reason why is because, like,\nmy inputs are only for every vector, it's only a one dimensional vector of length six. So I should\nhave I should have just had six, comma, which is a tuple of size six from the start, or it's a it's\na tuple containing one element, which is a six. Okay, so it's actually interesting that my neural\nnet results seem like they they have a larger mean squared error than my linear regressor.\nOne thing that we can look at is, we can actually plot the real versus, you know, the the actual\nresults versus what the predictions are. So if I say, some access, and I use plt dot axes, and make\naxes and make these equal, then I can scatter the the y, you know, the test. So what the actual\nvalues are on the x axis, and then what the prediction are on the x axis. Okay. And I can\nlabel this as the linear regression predictions. Okay, so then let me just label my axes. So the\nx axis, I'm going to say is the true values. The y axis is going to be my linear regression predictions.\nOr actually, let's plot. Let's just make this predictions.\nAnd then at the end, I'm going to plot. Oh, let's set some limits.\nBecause I think that's like approximately the max number of bikes.\nSo I'm going to set my x limit to this and my y limit to this.\nSo here, I'm going to pass that in here too. And all right, this is what we actually get for our\nlinear regressor. You see that actually, they align quite well, I mean, to some extent. So 2000 is\nprobably too much 2500. I mean, looks like maybe like 1800 would be enough here for our limits.\nAnd I'm actually going to label something else, the neural net predictions.\nLet's add a legend. So you can see that our neural net for the larger values, it seems like\nit's a little bit more spread out. And it seems like we tend to underestimate a little bit down\nhere in this area. Okay. And for some reason, these are way off as well.\nBut yeah, so we've basically used a linear regressor and a neural net. Honestly, there are\nsometimes where a neural net is more appropriate and a linear regressor is more appropriate.\nI think that it just comes with time and trying to figure out, you know, and just literally seeing\nlike, hey, what works better, like here, a linear, a multiple linear regressor might actually work\nbetter than a neural net. But for example, with the one dimensional case, a linear regressor would\nnever be able to see this curve. Okay. I mean, I'm not saying this is a great model either, but I'm\njust saying like, hey, you know, sometimes it might be more appropriate to use something that's not\nlinear. So yeah, I will leave regression at that. Okay, so we just talked about supervised learning.\nAnd in supervised learning, we have data, we have some a bunch of features and for a bunch of\ndifferent samples. But each of those samples has some sort of label on it, whether that's a number,\na category, a class, etc. Right, we were able to use that label in order to try to predict\nright, we were able to use that label in order to try to predict new labels of other points that\nwe haven't seen yet. Well, now let's move on to unsupervised learning. So with unsupervised\nlearning, we have a bunch of unlabeled data. And what can we do with that? You know, can we learn\nanything from this data? So the first algorithm that we're going to discuss is known as k means\nclustering. What k means clustering is trying to do is it's trying to compute k clusters from the data.\nSo in this example below, I have a bunch of scattered points. And you'll see that this\nis x zero and x one on the two axes, which means I'm actually plotting two different features,\nright of each point, but we don't know what the y label is for those points. And now, just looking\nat these scattered points, we can kind of see how there are different clusters in the data set,\nright. So depending on what we pick for k, we might have different clusters. Let's say k equals two,\nright, then we might pick, okay, this seems like it could be one cluster, but this here is also\nanother cluster. So those might be our two different clusters. If we have k equals three,\nfor example, then okay, this seems like it could be a cluster. This seems like it could be a\ncluster. And maybe this could be a cluster, right. So we could have three different clusters in the\ndata set. Now, this k here is predefined, if I can spell that correctly, by the person who's running\nthe model. So that would be you. All right. And let's discuss how you know, the computer actually\ngoes through and computes the k clusters. So I'm going to write those steps down here.\nNow, the first step that happens is we actually choose well, the computer chooses three random\npoints on this plot to be the centroids. And by centuries, I just mean the center of the clusters.\nOkay. So three random points, let's say we're doing k equals three, so we're choosing three\nrandom points to be the centroids of the three clusters. If it were two, we'd be choosing two\nrandom points. Okay. So maybe the three random points I'm choosing might be here.\nHere, here, and here. All right. So we have three different points. And the second thing that we do\nis we actually calculate\nthe distance for each point to those centroids. So between all the points and the centroid.\nSo basically, I'm saying, all right, this is this distance, this distance, this distance,\nall of these distances, I'm computing between oops, not those two, between the points, not the\ncentroids themselves. So I'm computing the distances for all of these plots to each of the centroids.\nOkay. And that comes with also assigning those points to the closest centroid.\nWhat do I mean by that? So let's take this point here, for example, so I'm computing\nthis distance, this distance, and this distance. And I'm saying, okay, it seems like the red one\nis the closest. So I'm actually going to put this into the red centroid. So if I do that for\nall of these points, it seems slightly closer to red, and this one seems slightly closer to red,\nright? Now for the blue, I actually wouldn't put any blue ones in here, but we would probably\nactually, that first one is closer to red. And now it seems like the rest of them are probably\ncloser to green. So let's just put all of these into green here, like that. And cool. So now we\nhave, you know, our two, three, technically centroid. So there's this group here, there's\nthis group here. And then blue is kind of just this group here, it hasn't really touched any\nof the points yet. So the next step, three that we do is we actually go and we recalculate the\ncentroid. So we compute new centroids based on the points that we have in all the centroids.\nAnd by that, I just mean, okay, well, let's take the average of all these points. And where is that\nnew centroid? That's probably going to be somewhere around here, right? The blue one, we don't have\nany points in there. So we won't touch and then the screen one, we can put that probably somewhere\nover here, oops, somewhere over here. Right. So now if I erase all of the previously computed centroids,\nI can go and I can actually redo step two over here, this calculation.\nAlright, so I'm going to go back and I'm going to iterate through everything again,\nand I'm going to recompute my three centroids. So let's see, we're going to take this red point,\nthese are definitely all red, right? This one still looks a bit red. Now,\nthis part, we actually start getting closer to the blues.\nSo this one still seems closer to a blue than a green, this one as well. And I think the rest\nwould belong to green. Okay, so now our three centroids are three, sorry, our three clusters\nwould be this, this, and then this, right? Those are our three centroids. And so now we go back\nand we compute the new sorry, those would be the three clusters. So now we go back and we compute\nthe three centroids. So I'm going to get rid of this, this and this. And now where would this\nred be centered, probably closer, you know, to this point here, this blue might be closer to\nup here. And then this green would probably be somewhere. It's pretty similar to what we had\nbefore. But it seems like it'd be pulled down a bit. So probably somewhere around there for green.\nAll right. And now, again, we go back and we compute the distance between all the points\nand the centroids. And then we assign them to the closest centroid. Okay. So the reds are all here,\nit's very clear. Actually, let me just circle that. And this it actually seems like this point is\nit actually seemed like this point is closer to this blue now. So the blues seem like they would\nbe maybe this point looks like it'd be blue. So all these look like they would be blue now.\nAnd the greens would probably be this cluster right here. So we go back, we compute the centroids,\nbam. This one probably like almost here, bam. And then the green looks like it would be probably\nhere ish. Okay. And now we go back and we compute the we compute the clusters again.\nSo red, still this blue, I would argue is now this cluster here. And green is this cluster here.\nOkay, so we go and we recompute the centroids, bam, bam. And, you know, bam. And now if I were\nto go and assign all the points to clusters again, I would get the exact same thing. Right. And so\nthat's when we know that we can stop iterating between steps two and three is when we've\nconverged on some solution when we've reached some stable point. And so now because none of\nthese points are really changing out of their clusters anymore, we can go back to the user\nand say, Hey, these are our three clusters. Okay. And this process, something known as\nexpectation maximization. This part where we're assigning the points to the closest centroid,\nthis is something this is our expectation step. And this part where we're computing the new\ncentroids, this is our maximization step. Okay, so that's expectation maximization.\nAnd we use this in order to compute the centroids, assign all the points to clusters,\naccording to those centroids. And then we're recomputing all that over again, until we reach\nsome stable point where nothing is changing anymore. Alright, so that's our first example\nof unsupervised learning. And basically, what this is doing is trying to find some structure,\nsome pattern in the data. So if I came up with another point, you know, might be somewhere here,\nI can say, Oh, it looks like that's closer to if this is a, b, c, it looks like that's closest to\ncluster B. And so I would probably put it in cluster B. Okay, so we can find some structure\nin the data based on just how, how the points are scattered relative to one another. Now,\nthe second unsupervised learning technique that I'm going to discuss with you guys, something noted,\nprincipal component analysis. And the point of principal component analysis is very often it's\nused as a dimensionality reduction technique. So let me write that down. It's used for dimensionality\nreduction. And what do I mean by dimensionality reduction is if I have a bunch of features like\nx1 x2 x3 x4, etc. Can I just reduce that down to one dimension that gives me the most information\nabout how all these points are spread relative to one another. And that's what PCA is for. So PCA\nprincipal component analysis. Let's say I have some points in the x zero and x one feature space.\nOkay, so these points might be spread, you know, something like this.\nOkay. So for example, if this were something to do with housing prices, right,\nthis here might be x zero might be hey, years since built, right, since the house was built,\nand x one might be square footage of the house. Alright, so like years since built, I mean, like\nright now it's been, you know, 22 years since a house in 2000 was built. Now principal component\nanalysis is just saying, alright, let's say we want to build a model, or let's say we want to,\nyou know, display something about our data, but we don't we don't have two axes to show it on.\nHow do we display, you know, how do we how do we demonstrate that this point is a further away from\nthis point than this point. And we can do that using principal component analysis. So\ntake what you know about linear regression and just forget about it for a second. Otherwise,\nyou might get confused. PCA is a way of trying to find direction in the space with the largest\nvariance. So this principal component, what that means is basically the component.\nSo some direction in this space with the largest variance, okay, it tells us the most about our\ndata set without the two different dimensions. Like, let's say we have these two different\nmentions, and somebody's telling us, hey, you only get one dimension in order to show your data set.\nWhat dimension do you want to show us? Okay, so let's say we want to show our data set,\nwhat dimension like what do we do, we want to project our data onto a single dimension.\nAlright, so that in this case might be a dimension that looks something like\nthis. And you might say, okay, we're not going to talk about linear regression, okay.\nWe don't have a y value. So linear regression, this would be why this is not why, okay, we don't\nhave a label for that. Instead, what we're doing is we're taking the right angle projection. So\nall of these take that's not very visible. But take this right angle projection onto this line.\nAnd what PCA is doing is saying, okay, map all of these points onto this one dimensional space.\nSo the transformed data set would be here.\nThis one's on the data sets are on the line. So we just put that there. But now this would be our\nnew one dimensional data set. Okay, it's not our prediction or anything. This is our new data set.\nIf somebody came to us said you only get one dimension, you only get one number to represent\neach of these 2d points. What number would you give us? What number would you give us?\nSo this would be our new one dimensional data set. Okay, it's not our prediction or anything.\nWhat number would you give me? This would be the number that we gave. Okay, this in this direction,\nthis is where our points are the most spread out. Right? If I took this plot,\nand let me actually duplicate this so I don't have to rewrite anything.\nOr so I don't have to erase and then redraw anything. Let me get rid of some of this stuff.\nAnd I just got rid of a point there too. So let me draw that back.\nAlright, so if this were my original data point, what if I had taken, you know, this to be\nthe PCA dimension? Okay, well, I then would have points that let me actually do that in different\ncolor. So if I were to draw a right angle to this for every point, my points would look something\nlike this. And so just intuitively looking at these two different plots, this top one and this one,\nwe can see that the points are squished a little bit closer together. Right? Which means that the\nvariance that's not the space with the largest variance. The thing about the largest variance\nis that this will give us the most discrimination between all of these points. The larger the\nvariance, the further spread out these points will likely be. Now, and so that's the that's the\ndimension that we should project it on a different way to actually look at that, like what is the\ndimension with the largest variance. It's actually it also happens to be the dimension that decreases\nto be the dimension that decreases that minimizes the residuals. So if we take all the points, and\nwe take the residual from that the XY residual, so in linear regression, in linear regression,\nwe were looking only at this residual, the differences between the predictions right between\ny and y hat, it's not that here in principal component analysis, we're taking the difference\nfrom our current point in two dimensional space, and then it's projected point. Okay, so we're\ntaking that dimension. And we're saying, alright, how much, you know, how much distance is there\nbetween that projection residual, and we're trying to minimize that for all of these points. So that\nactually equates to this largest variance dimension, this dimension here, the PCA dimension,\nyou can either look at it as minimizing, minimize, let me get rid of this,\nthe projection residuals. So that's the stuff in orange.\nOr to maximizing the variance between the points.\nOkay. And we're not really going to talk about, you know, the method that we need in order to\ncalculate out the principal components, or like what that projection would be, because you will\nneed to understand linear algebra for that, especially eigenvectors and eigenvalues, which\nI'm not going to cover in this class. But that's how you would find the principal components. Okay,\nnow, with this two dimensional data set here, sorry, this one dimensional data set, we started\nfrom a 2d data set, and we now boil it down to one dimension. Well, we can go and take that\ndimension, and we can do other things with it. Right, we can, like if there were a y label,\nthen we can now show x versus y, rather than x zero and x one in different plots with that y.\nNow we can just say, oh, this is a principal component. And we're going to plot that with\nthe y. Or for example, if there were 100 different dimensions, and you only wanted to take five of\nthem, well, you could go and you could find the top five PCA dimensions. And that might be a lot\nmore useful to you than 100 different feature vector values. Right. So that's principal component\nanalysis. Again, we're taking, you know, certain data that's unlabeled, and we're trying to make\nsome sort of estimation, like some guess about its structure from that original data set, if we\nwanted to take, you know, a 3d thing, so like a sphere, but we only have a 2d surface to draw it\non. Well, what's the best approximation that we can make? Oh, it's a circle. Right PCA is kind of\nthe same thing. It's saying if we have something with all these different dimensions, but we can't\nshow all of them, how do we boil it down to just one dimension? How do we extract the most\ninformation from that multiple dimensions? And that is exactly either you minimize the projection\nresiduals, or you maximize the variance. And that is PCA. So we'll go through an example of that.\nNow, finally, let's move on to implementing the unsupervised learning part of this class.\nHere, again, I'm on the UCI machine learning repository. And I have a seeds data set where,\nyou know, I have a bunch of kernels that belong to three different types of wheat. So there's\ncomma, Rosa and Canadian. And the different features that we have access to are, you know,\ngeometric parameters of those wheat kernels. So the area perimeter, compactness, length, width,\nwidth, asymmetry, and the length of the kernel groove. Okay, so all of these are real values,\nwhich is easy to work with. And what we're going to do is we're going to try to predict,\nor I guess we're going to try to cluster the different varieties of the wheat.\nSo let's get started. I have a colab notebook open again. Oh, you're gonna have to, you know,\ngo to the data folder, download this. And so I'm going to go to the data folder, download this,\nand let's get started. So the first thing to do is to import our seeds data set into our colab\nnotebook. So I've done that here. Okay, and then we're going to import all the classics again,\nso pandas. And then I'm also going to import seedborn because I'm going to want that for this\nspecific class. Okay. Great. So now our columns that we have in our seed data set are the area,\nthe perimeter, the compactness, the length, with asymmetry, groove, length, I mean, I'm just going\nto call it groove. And then the class, right, the wheat kernels class. So now we have to import this,\nI'm going to do that using pandas read CSV. And it's called seeds data.csv. So I'm going to turn\nthat into a data frame. And the names are equal to the columns over here. So what happens if I just\ndo that? Oops, what did I call this seeds data set text? Alright, so if we actually look at our\ndata frame right now, you'll notice something funky. Okay. And here, you know, we have all the\nstuff under area. And these are all our numbers with some dash t. So the reason is because we\nhaven't actually told pandas what the separator is, which we can do like this. And this t that's\njust a tab. So in order to ensure that like all whitespace gets recognized as a separator,\nwe can actually this is for like a space. So any spaces are going to get recognized as data\nseparators. So if I run that, now our this, you know, this is a lot better. Okay. Okay.\nSo now let's actually go and like visualize this data. So what I'm actually going to do is plot\neach of these against one another. So in this case, pretend that we don't have access to the\nclass, right? Pretend that so this class here, I'm just going to show you in this example,\nthat like, hey, we can predict our classes using unsupervised learning. But for this example,\nin unsupervised learning, we don't actually have access to the class. So I'm going to just try to\nplot these against one another and see what happens. So for some I in range, you know,\nthe columns minus one because the classes in the columns. And I'm just going to say for j in range,\nso take everything from I onwards, you know, so I like the next thing after I until the end of this.\nSo this will give us basically a grid of all the different like combinations. And our x label is\ngoing to be columns I our y label is going to be the columns j. So those are our labels up here.\nAnd I'm going to use seaborne this time. And I'm going to say scatter my data. So our x is going\nto be our x label. Or y is going to be our y label. And our data is going to be the data frame that\nwe're passing in. So what's interesting here is that we can say hue. And what this will do is say,\nlike if I give this class, it's going to separate the three different classes into three different\nhues. So now what we're doing is we're basically comparing the area and the perimeter or the area\nand the compactness. But we're going to visualize, you know, what classes they're in. So let's go\nahead and I might have to show. So great. So basically, we can see perimeter and area we give\nwe get these three groups. The area compactness, we get these three groups, and so on. So these all\nkind of look honestly like somewhat similar. Right, so Wow, look at this one. So this one,\nwe have the compactness and the asymmetry. And it looks like there's not really I mean,\nit just looks like they're blobs, right? Sure, maybe class three is over here more, but\none and two kind of look like they're on top of each other. Okay. I mean, there are some that\nmight look slightly better in terms of clustering. But let's go through some of the some of the\nclustering examples that we talked about, and try to implement those. The first thing that we're\ngoing to do is just straight up clustering. So what we learned about was k means clustering.\nSo from SK learn, I'm going to import k means. Okay. And just for the sake of being able to run,\nyou know, any x and any y, I'm just going to say, hey, let's use some x. What's a good one, maybe.\nI mean, perimeter asymmetry could be a good one. So x could be perimeter, y could be asymmetry.\nOkay. And for this, the x values, I'm going to just extract those specific values.\nAlright, well, let's make a k means algorithm, or let's, you know, define this. So k means,\nand in this specific case, we know that the number of clusters is three. So let's just use that. And\nI'm going to fit this against this x that I've just defined right here. Right. So, you know, if I\ncreate this clusters, so one thing, one cool thing is I can actually go to this clusters, and I can\nsay k mean dot labels. And it'll give give me if I can type correctly, it'll give me what its\npredictions for all the clusters are. And our actual, oops, not that. If we go to the data frame,\nand we get the class, and the values from those, we can actually compare these two and say, hey,\nlike, you know, everything in general, most of the zeros that it's predicted, are the ones, right.\nAnd in general, the twos are the twos here. And then this third class one, okay, that corresponds\nto three. Now remember, these are separate classes. So the labels, what we actually call them don't\nreally matter. We can say a map zero to one map two to two and map one to three. Okay, and our,\nyou know, our mapping would do fairly well. But we can actually visualize this. And in order to do\nthat, I'm going to create this cluster cluster data frame. So I'm going to create a data frame.\nAnd I'm going to pass in a horizontally stacked array with x, so my values for x and y. And then\nthe clusters that I have here, but I'm going to reshape them. So it's 2d.\nOkay. And the columns, the labels for that are going to be x, y, and plus. Okay. So I'm going\nto go ahead and do that same seaborne scatter plot. Again, where x is x, y is y. And now,\nthe hue is again the class. And the data is now this cluster data frame. Alright, so this here,\nthis here is my k means like, I guess classes.\nSo k means kind of looks like this. If I come down here and I plot, you know, my original data frame,\nthis is my original classes with respect to this specific x and y. And you'll see that, honestly,\nlike it doesn't do too poorly. Yeah, there's I mean, the colors are different, but that's fine.\nFor the most part, it gets information of the clusters, right. And now we can do that with\nhigher dimensions. So with the higher dimensions, if we make x equal to, you know, all the columns,\nexcept for the last one, which is our class, we can do the exact same thing.\nWe can do the exact same thing. So here, and we can\npredict this. But now, our columns are equal to our data frame columns all the way to the last one.\nAnd then with this class, actually, so we can literally just say data frame columns.\nAnd we can fit all of this. And now, if I want to plot the k means classes.\nAlright, so this was my that's my clustered and my original. So actually, let me see if I can\nget these on the same page. So yeah, I mean, pretty similar to what we just saw. But what's\nactually really cool is even something like, you know, if we change. So what's one of them\nwhere they were like on top of each other? Okay, so compactness and asymmetry, this one's messy.\nRight. So if I come down here, and I say compactness and asymmetry, and I'm trying to do this in 2d,\nthis is what my scatterplot. So this is what you know, my k means is telling me for these two\ndimensions for compactness and asymmetry, if we just look at those two, these are our three classes,\nright? And we know that the original looks something like this. And are these two remotely\nalike? No. Okay, so now if I come back down here, and I rerun this higher dimensions one,\nbut actually, this clusters, I need to get the labels of the k means again.\nOkay, so if I rerun this with higher dimensions,\nwell, if we zoom out, and we take a look at these two, sure, the colors are mixed up. But in general,\nthere are the three groups are there, right? This does a much better job at assessing, okay,\nwhat group is what. So, for example, we could relabel the one in the original class to two.\nAnd then we could make sorry, okay, this is kind of confusing. But for example, if this light pink\nwere projected onto this darker pink here, and then this dark one was actually the light pink,\nand this light one was this dark one, then you kind of see like these correspond to one another,\nright? Like even these two up here are the same class as all the other ones over here, which are\nthe same in the same color. So you don't want to compare the two colors between the plots,\nyou want to compare which points are in what colors in each of the plots. So that's one cool\napplication. So this is how k means functions, it's basically taking all the data sets and saying,\nAll right, where are my clusters given these pieces of data? And then the next thing that we\ntalked about is PCA. So PCA, we're reducing the dimension, but we're mapping all these like,\nyou know, seven dimensions. I don't know if there are seven, I made that number up, but we're\nmapping multiple dimensions into a lower dimension number. Right. And so let's see how that works.\nSo from SK learn decomposition, I can import PCA and that will be my PCA model.\nSo if I do PCA component, so this is how many dimensions you want to map it into.\nAnd you know, for this exercise, let's do two. Okay, so now I'm taking the top two dimensions.\nAnd my transformed x is going to be PCA dot fit transform, and the same x that I had up here.\nAnd the same x that I had up here. Okay, so all the other all the values basically, area,\nperimeter, compactness, length, width, asymmetry, groove. Okay. So let's run that. And we've\ntransformed it. So let's look at what the shape of x used to be. So they're okay. So seven was right,\nI had 210 samples, each seven, seven features long, basically. And now my transformed x\nis 210 samples, but only of length two, which means that I only have two dimensions now that\nI'm plotting. And we can actually even take a look at, you know, the first five things.\nOkay, so now we see each each one is a two dimensional point,\neach sample is now a two dimensional point in our new in our new dimensions.\nSo what's cool is I can actually scatter these\nzero and transformed x. So I actually have to\ntake the columns here. And if I show that,\nbasically, we've just taken this like seven dimensional thing, and we've made it into a\nsingle or I guess to a two dimensional representation. So that's a point of PCA.\nAnd actually, let's go ahead and do the same clustering exercise as we did up here. If I take\nthe k means this PCA data frame, I can let's construct data frame out of that. And the data\nframe is going to be H stack. I'm going to take this transformed x and the clusters that reshape.\nSo actually, instead of clusters, I'm going to use k means dot labels. And I need to reshape this.\nSo it's 2d. So we can do the H stack. And for the columns, I'm going to set this to PCA one PCA two,\nand the class. All right. So now if I take this, I can also do the same for the truth.\nBut instead of the k means labels, I want from the data frame the original classes.\nAnd I'm just going to take the values from that. And so now I have a data frame for the k means\nwith PCA and then a data frame for the truth with also the PCA. And I can now plot these similarly\nto how I plotted these up here. So let me actually take these two.\nInstead of the cluster data frame, I want the this is the k means PCA data frame. This is still going\nto be class, but now x and y are going to be the two PCA dimensions. Okay. So these are my two PCA\ndimensions. And you can see that the data frame is going to be the same as the cluster data frame.\nSo these are my two PCA dimensions. And you can see that, you know, they're, they're pretty spread\nout. And then here, I'm going to go to my truth classes. Again, it's PCA one PCA two, but instead\nof k means this should be truth PCA data frame. So you can see that like in the truth data frame\nalong these two dimensions, we actually are doing fairly well in terms of separation, right? It does\nseem like this is slightly more separable than the other like dimensions that we had been looking at\nup here. So that's a good sign. And up here, you can see that hey, some of these correspond to one\nanother. I mean, for the most part, our algorithm or unsupervised clustering algorithm is able to\nto give us is able to spit out, you know, what the proper labels are. I mean, if you map these\nspecific labels to the different types of kernels. But for example, this one might all be the comma\nkernel kernels and same here. And then these might all be the Canadian kernels. And these might all\nbe the Canadian kernels. So it does struggle a little bit with, you know, where they overlap.\nBut for the most part, our algorithm is able to find the three different categories, and do a\nfairly good job at predicting them without without any information from us, we haven't given our\nalgorithm any labels. So that's a gist of unsupervised learning. I hope you guys enjoyed\nthis course. I hope you know, a lot of these examples made sense. If there are certain things\nthat I have done, and you know, you're somebody with more experience than me, please let me know\nin the comments and we can all as a community learn from this together. So thank you all for watching.\n",
  "words": [
    "kylie",
    "ying",
    "worked",
    "many",
    "interesting",
    "places",
    "mit",
    "cern",
    "free",
    "code",
    "camp",
    "physicist",
    "engineer",
    "basically",
    "genius",
    "going",
    "teach",
    "machine",
    "learning",
    "way",
    "accessible",
    "absolute",
    "beginners",
    "guys",
    "welcome",
    "machine",
    "learning",
    "everyone",
    "someone",
    "interested",
    "machine",
    "learning",
    "think",
    "considered",
    "everyone",
    "video",
    "video",
    "talk",
    "supervised",
    "unsupervised",
    "learning",
    "models",
    "go",
    "maybe",
    "little",
    "bit",
    "logic",
    "math",
    "behind",
    "also",
    "see",
    "program",
    "google",
    "colab",
    "certain",
    "things",
    "done",
    "know",
    "somebody",
    "experience",
    "please",
    "feel",
    "free",
    "correct",
    "comments",
    "community",
    "learn",
    "together",
    "let",
    "dive",
    "right",
    "without",
    "wasting",
    "time",
    "let",
    "dive",
    "straight",
    "code",
    "teaching",
    "guys",
    "concepts",
    "go",
    "uci",
    "machine",
    "learning",
    "repository",
    "basically",
    "ton",
    "data",
    "sets",
    "access",
    "found",
    "really",
    "cool",
    "one",
    "called",
    "magic",
    "gamma",
    "telescope",
    "data",
    "set",
    "data",
    "set",
    "want",
    "read",
    "information",
    "summarize",
    "think",
    "going",
    "gamma",
    "telescope",
    "high",
    "energy",
    "particles",
    "hitting",
    "telescope",
    "camera",
    "detector",
    "actually",
    "records",
    "certain",
    "patterns",
    "know",
    "light",
    "hits",
    "camera",
    "use",
    "properties",
    "patterns",
    "order",
    "predict",
    "type",
    "particle",
    "caused",
    "radiation",
    "whether",
    "gamma",
    "particle",
    "head",
    "like",
    "hadron",
    "attributes",
    "patterns",
    "collect",
    "camera",
    "see",
    "know",
    "length",
    "width",
    "size",
    "asymmetry",
    "etc",
    "going",
    "use",
    "properties",
    "help",
    "us",
    "discriminate",
    "patterns",
    "whether",
    "came",
    "gamma",
    "particle",
    "hadron",
    "order",
    "going",
    "come",
    "go",
    "data",
    "folder",
    "going",
    "click",
    "magic",
    "zero",
    "data",
    "going",
    "download",
    "colab",
    "notebook",
    "open",
    "go",
    "colab",
    "dot",
    "research",
    "dot",
    "start",
    "new",
    "notebook",
    "going",
    "call",
    "magic",
    "data",
    "set",
    "actually",
    "going",
    "call",
    "code",
    "camp",
    "magic",
    "example",
    "okay",
    "going",
    "first",
    "start",
    "imports",
    "import",
    "know",
    "always",
    "import",
    "numpy",
    "always",
    "import",
    "pandas",
    "always",
    "import",
    "matplotlib",
    "import",
    "things",
    "go",
    "yeah",
    "run",
    "order",
    "run",
    "cell",
    "either",
    "click",
    "play",
    "button",
    "computer",
    "shift",
    "enter",
    "run",
    "cell",
    "going",
    "order",
    "going",
    "know",
    "let",
    "guys",
    "know",
    "okay",
    "found",
    "data",
    "set",
    "copied",
    "pasted",
    "actually",
    "found",
    "data",
    "set",
    "order",
    "import",
    "downloaded",
    "file",
    "got",
    "computer",
    "going",
    "go",
    "folder",
    "thing",
    "literally",
    "going",
    "drag",
    "drop",
    "file",
    "okay",
    "order",
    "take",
    "look",
    "know",
    "file",
    "consist",
    "labels",
    "mean",
    "could",
    "open",
    "computer",
    "also",
    "pandas",
    "read",
    "csv",
    "pass",
    "name",
    "file",
    "let",
    "see",
    "returns",
    "seem",
    "like",
    "label",
    "let",
    "go",
    "back",
    "going",
    "make",
    "columns",
    "column",
    "labels",
    "attribute",
    "names",
    "going",
    "take",
    "values",
    "make",
    "column",
    "names",
    "right",
    "basically",
    "come",
    "back",
    "create",
    "list",
    "called",
    "calls",
    "type",
    "things",
    "f",
    "size",
    "f",
    "conk",
    "also",
    "f",
    "conk",
    "one",
    "f",
    "symmetry",
    "f",
    "three",
    "long",
    "f",
    "three",
    "trans",
    "f",
    "alpha",
    "let",
    "see",
    "f",
    "dist",
    "class",
    "okay",
    "great",
    "order",
    "label",
    "columns",
    "data",
    "frame",
    "basically",
    "command",
    "reads",
    "csv",
    "file",
    "pass",
    "csv",
    "come",
    "comma",
    "separated",
    "values",
    "turns",
    "pandas",
    "data",
    "frame",
    "object",
    "pass",
    "names",
    "basically",
    "assigns",
    "labels",
    "columns",
    "data",
    "set",
    "going",
    "set",
    "data",
    "frame",
    "equal",
    "df",
    "call",
    "head",
    "like",
    "give",
    "first",
    "five",
    "things",
    "give",
    "first",
    "five",
    "things",
    "see",
    "labels",
    "okay",
    "right",
    "great",
    "one",
    "thing",
    "might",
    "notice",
    "class",
    "labels",
    "g",
    "actually",
    "go",
    "data",
    "frame",
    "class",
    "unique",
    "see",
    "either",
    "g",
    "h",
    "stand",
    "gammas",
    "hadrons",
    "computer",
    "good",
    "understanding",
    "letters",
    "right",
    "computer",
    "really",
    "good",
    "understanding",
    "numbers",
    "going",
    "going",
    "convert",
    "zero",
    "g",
    "one",
    "going",
    "set",
    "equal",
    "whether",
    "equals",
    "going",
    "say",
    "type",
    "int",
    "convert",
    "entire",
    "column",
    "equals",
    "g",
    "true",
    "guess",
    "would",
    "one",
    "h",
    "would",
    "false",
    "would",
    "zero",
    "converting",
    "g",
    "h",
    "one",
    "zero",
    "really",
    "matter",
    "like",
    "g",
    "one",
    "h",
    "zero",
    "vice",
    "versa",
    "let",
    "take",
    "step",
    "back",
    "right",
    "talk",
    "data",
    "set",
    "data",
    "frame",
    "different",
    "values",
    "entry",
    "know",
    "one",
    "sample",
    "one",
    "example",
    "one",
    "item",
    "data",
    "set",
    "one",
    "data",
    "point",
    "things",
    "kind",
    "thing",
    "mentioned",
    "oh",
    "one",
    "example",
    "one",
    "sample",
    "whatever",
    "samples",
    "know",
    "one",
    "quality",
    "one",
    "value",
    "labels",
    "class",
    "going",
    "specific",
    "example",
    "try",
    "predict",
    "future",
    "know",
    "samples",
    "whether",
    "class",
    "g",
    "gamma",
    "h",
    "hadron",
    "something",
    "known",
    "classification",
    "known",
    "features",
    "features",
    "things",
    "going",
    "pass",
    "model",
    "order",
    "help",
    "us",
    "predict",
    "label",
    "case",
    "class",
    "column",
    "know",
    "sample",
    "zero",
    "10",
    "different",
    "features",
    "10",
    "different",
    "values",
    "pass",
    "model",
    "spit",
    "know",
    "class",
    "label",
    "know",
    "true",
    "label",
    "actually",
    "supervised",
    "learning",
    "right",
    "move",
    "let",
    "give",
    "quick",
    "little",
    "crash",
    "course",
    "said",
    "machine",
    "learning",
    "everyone",
    "well",
    "first",
    "question",
    "machine",
    "learning",
    "well",
    "machine",
    "learning",
    "sub",
    "domain",
    "computer",
    "science",
    "focuses",
    "certain",
    "algorithms",
    "might",
    "help",
    "computer",
    "learn",
    "data",
    "without",
    "programmer",
    "telling",
    "computer",
    "exactly",
    "call",
    "explicit",
    "programming",
    "might",
    "heard",
    "ai",
    "ml",
    "data",
    "science",
    "difference",
    "ai",
    "artificial",
    "intelligence",
    "area",
    "computer",
    "science",
    "goal",
    "enable",
    "computers",
    "machines",
    "perform",
    "human",
    "like",
    "tasks",
    "simulate",
    "human",
    "behavior",
    "machine",
    "learning",
    "subset",
    "ai",
    "tries",
    "solve",
    "one",
    "specific",
    "problem",
    "make",
    "predictions",
    "using",
    "certain",
    "data",
    "data",
    "science",
    "field",
    "attempts",
    "find",
    "patterns",
    "draw",
    "insights",
    "data",
    "might",
    "mean",
    "using",
    "machine",
    "learning",
    "fields",
    "kind",
    "overlap",
    "might",
    "use",
    "machine",
    "learning",
    "types",
    "machine",
    "learning",
    "first",
    "one",
    "supervised",
    "learning",
    "supervised",
    "learning",
    "using",
    "labeled",
    "inputs",
    "means",
    "whatever",
    "input",
    "get",
    "corresponding",
    "output",
    "label",
    "order",
    "train",
    "models",
    "learn",
    "outputs",
    "different",
    "new",
    "inputs",
    "might",
    "feed",
    "model",
    "example",
    "might",
    "pictures",
    "okay",
    "computer",
    "pictures",
    "pixels",
    "pixels",
    "certain",
    "color",
    "supervised",
    "learning",
    "inputs",
    "label",
    "associated",
    "output",
    "might",
    "want",
    "computer",
    "able",
    "predict",
    "example",
    "picture",
    "cat",
    "picture",
    "dog",
    "picture",
    "lizard",
    "also",
    "unsupervised",
    "learning",
    "unsupervised",
    "learning",
    "use",
    "unlabeled",
    "data",
    "learn",
    "patterns",
    "data",
    "input",
    "data",
    "points",
    "images",
    "pixels",
    "well",
    "okay",
    "let",
    "say",
    "bunch",
    "different",
    "pictures",
    "feed",
    "computer",
    "might",
    "know",
    "computer",
    "going",
    "able",
    "say",
    "oh",
    "cat",
    "dog",
    "lizard",
    "terms",
    "know",
    "output",
    "might",
    "able",
    "cluster",
    "pictures",
    "might",
    "say",
    "hey",
    "something",
    "common",
    "something",
    "common",
    "something",
    "common",
    "finding",
    "sort",
    "structure",
    "unlabeled",
    "data",
    "finally",
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "well",
    "usually",
    "agent",
    "learning",
    "sort",
    "interactive",
    "environment",
    "based",
    "rewards",
    "penalties",
    "let",
    "think",
    "dog",
    "train",
    "dog",
    "necessarily",
    "know",
    "wrong",
    "right",
    "output",
    "given",
    "moment",
    "right",
    "well",
    "let",
    "pretend",
    "dog",
    "computer",
    "essentially",
    "giving",
    "rewards",
    "computer",
    "tell",
    "computer",
    "hey",
    "probably",
    "something",
    "good",
    "want",
    "keep",
    "well",
    "computer",
    "agent",
    "terminology",
    "class",
    "today",
    "focusing",
    "supervised",
    "learning",
    "unsupervised",
    "learning",
    "learning",
    "different",
    "models",
    "alright",
    "let",
    "talk",
    "supervised",
    "learning",
    "first",
    "kind",
    "machine",
    "learning",
    "model",
    "looks",
    "like",
    "bunch",
    "inputs",
    "going",
    "model",
    "model",
    "spitting",
    "output",
    "prediction",
    "inputs",
    "call",
    "feature",
    "vector",
    "different",
    "types",
    "features",
    "might",
    "qualitative",
    "features",
    "qualitative",
    "means",
    "categorical",
    "data",
    "either",
    "finite",
    "number",
    "categories",
    "groups",
    "one",
    "example",
    "qualitative",
    "feature",
    "might",
    "gender",
    "case",
    "two",
    "sake",
    "example",
    "know",
    "might",
    "little",
    "bit",
    "outdated",
    "girl",
    "boy",
    "two",
    "genders",
    "two",
    "different",
    "categories",
    "piece",
    "qualitative",
    "data",
    "another",
    "example",
    "might",
    "okay",
    "know",
    "bunch",
    "different",
    "nationalities",
    "maybe",
    "nationality",
    "nation",
    "location",
    "might",
    "also",
    "example",
    "categorical",
    "data",
    "inherent",
    "order",
    "like",
    "know",
    "rate",
    "us",
    "one",
    "france",
    "japan",
    "three",
    "etc",
    "right",
    "really",
    "inherent",
    "order",
    "built",
    "either",
    "categorical",
    "data",
    "sets",
    "call",
    "nominal",
    "data",
    "nominal",
    "data",
    "way",
    "want",
    "feed",
    "computer",
    "using",
    "something",
    "called",
    "one",
    "hot",
    "encoding",
    "let",
    "say",
    "know",
    "data",
    "set",
    "items",
    "data",
    "inputs",
    "might",
    "us",
    "might",
    "india",
    "canada",
    "france",
    "get",
    "computer",
    "recognize",
    "something",
    "called",
    "one",
    "hot",
    "encoding",
    "basically",
    "one",
    "hot",
    "encoding",
    "saying",
    "okay",
    "well",
    "matches",
    "category",
    "make",
    "one",
    "make",
    "zero",
    "example",
    "input",
    "us",
    "would",
    "might",
    "india",
    "know",
    "canada",
    "okay",
    "well",
    "item",
    "representing",
    "canada",
    "one",
    "france",
    "item",
    "representing",
    "france",
    "one",
    "see",
    "rest",
    "zeros",
    "one",
    "hot",
    "encoding",
    "also",
    "different",
    "type",
    "qualitative",
    "feature",
    "left",
    "different",
    "age",
    "groups",
    "babies",
    "toddlers",
    "teenagers",
    "young",
    "adults",
    "adults",
    "right",
    "right",
    "hand",
    "side",
    "might",
    "different",
    "ratings",
    "maybe",
    "bad",
    "good",
    "mediocre",
    "good",
    "like",
    "great",
    "known",
    "ordinal",
    "pieces",
    "data",
    "sort",
    "inherent",
    "order",
    "right",
    "like",
    "toddler",
    "lot",
    "closer",
    "baby",
    "elderly",
    "person",
    "right",
    "good",
    "closer",
    "great",
    "really",
    "bad",
    "sort",
    "inherent",
    "ordering",
    "system",
    "types",
    "data",
    "sets",
    "actually",
    "mark",
    "know",
    "one",
    "five",
    "say",
    "hey",
    "let",
    "give",
    "number",
    "makes",
    "sense",
    "like",
    "example",
    "thing",
    "said",
    "good",
    "closer",
    "great",
    "good",
    "close",
    "good",
    "well",
    "four",
    "closer",
    "five",
    "four",
    "close",
    "one",
    "actually",
    "kind",
    "makes",
    "sense",
    "make",
    "sense",
    "computer",
    "well",
    "alright",
    "also",
    "quantitative",
    "pieces",
    "data",
    "quantitative",
    "pieces",
    "data",
    "numerical",
    "valued",
    "pieces",
    "data",
    "could",
    "discrete",
    "means",
    "know",
    "might",
    "integers",
    "could",
    "continuous",
    "means",
    "real",
    "numbers",
    "example",
    "length",
    "something",
    "quantitative",
    "piece",
    "data",
    "quantitative",
    "feature",
    "temperature",
    "something",
    "quantitative",
    "feature",
    "maybe",
    "many",
    "easter",
    "eggs",
    "collected",
    "basket",
    "easter",
    "egg",
    "hunt",
    "example",
    "discrete",
    "quantitative",
    "feature",
    "okay",
    "continuous",
    "screen",
    "things",
    "go",
    "feature",
    "vector",
    "features",
    "feeding",
    "model",
    "computers",
    "really",
    "really",
    "good",
    "understanding",
    "math",
    "right",
    "understanding",
    "numbers",
    "good",
    "understanding",
    "things",
    "humans",
    "might",
    "able",
    "understand",
    "well",
    "types",
    "predictions",
    "model",
    "output",
    "supervised",
    "learning",
    "different",
    "tasks",
    "one",
    "classification",
    "basically",
    "classification",
    "saying",
    "okay",
    "predict",
    "discrete",
    "classes",
    "might",
    "mean",
    "know",
    "hot",
    "dog",
    "pizza",
    "ice",
    "cream",
    "okay",
    "three",
    "distinct",
    "classes",
    "pictures",
    "hot",
    "dogs",
    "pizza",
    "ice",
    "cream",
    "put",
    "labels",
    "hot",
    "dog",
    "pizza",
    "ice",
    "cream",
    "hot",
    "dog",
    "pizza",
    "ice",
    "cream",
    "something",
    "known",
    "multi",
    "class",
    "classification",
    "also",
    "binary",
    "classification",
    "binary",
    "classification",
    "might",
    "hot",
    "dog",
    "hot",
    "dog",
    "two",
    "categories",
    "working",
    "something",
    "something",
    "something",
    "binary",
    "classification",
    "okay",
    "yeah",
    "examples",
    "something",
    "positive",
    "negative",
    "sentiment",
    "binary",
    "classification",
    "maybe",
    "predicting",
    "pictures",
    "cats",
    "dogs",
    "binary",
    "classification",
    "maybe",
    "know",
    "writing",
    "email",
    "filter",
    "trying",
    "figure",
    "email",
    "spam",
    "spam",
    "also",
    "binary",
    "classification",
    "multi",
    "class",
    "classification",
    "might",
    "know",
    "cat",
    "dog",
    "lizard",
    "dolphin",
    "shark",
    "rabbit",
    "etc",
    "might",
    "different",
    "types",
    "fruits",
    "like",
    "orange",
    "apple",
    "pear",
    "etc",
    "maybe",
    "different",
    "plant",
    "species",
    "multi",
    "class",
    "classification",
    "means",
    "two",
    "okay",
    "binary",
    "means",
    "predicting",
    "two",
    "things",
    "also",
    "something",
    "called",
    "regression",
    "talk",
    "supervised",
    "learning",
    "means",
    "trying",
    "predict",
    "continuous",
    "values",
    "instead",
    "trying",
    "predict",
    "different",
    "categories",
    "trying",
    "come",
    "number",
    "know",
    "sort",
    "scale",
    "examples",
    "examples",
    "might",
    "price",
    "aetherium",
    "tomorrow",
    "might",
    "okay",
    "going",
    "temperature",
    "might",
    "price",
    "house",
    "right",
    "things",
    "really",
    "fit",
    "discrete",
    "classes",
    "trying",
    "predict",
    "number",
    "close",
    "true",
    "value",
    "possible",
    "using",
    "different",
    "features",
    "data",
    "set",
    "exactly",
    "model",
    "looks",
    "like",
    "supervised",
    "learning",
    "let",
    "talk",
    "model",
    "make",
    "model",
    "learn",
    "tell",
    "whether",
    "even",
    "learning",
    "talk",
    "models",
    "let",
    "talk",
    "actually",
    "like",
    "evaluate",
    "models",
    "tell",
    "whether",
    "something",
    "good",
    "model",
    "bad",
    "model",
    "let",
    "take",
    "look",
    "data",
    "set",
    "data",
    "set",
    "diabetes",
    "pima",
    "indian",
    "diabetes",
    "data",
    "set",
    "different",
    "number",
    "pregnancies",
    "different",
    "glucose",
    "levels",
    "blood",
    "pressure",
    "skin",
    "thickness",
    "insulin",
    "bmi",
    "age",
    "outcome",
    "whether",
    "diabetes",
    "one",
    "zero",
    "quantitative",
    "features",
    "right",
    "scale",
    "row",
    "different",
    "sample",
    "data",
    "different",
    "example",
    "one",
    "person",
    "data",
    "row",
    "represents",
    "one",
    "person",
    "data",
    "set",
    "column",
    "column",
    "represents",
    "different",
    "feature",
    "one",
    "measure",
    "blood",
    "pressure",
    "levels",
    "one",
    "mentioned",
    "output",
    "label",
    "one",
    "whether",
    "diabetes",
    "mentioned",
    "would",
    "call",
    "feature",
    "vector",
    "features",
    "one",
    "sample",
    "known",
    "target",
    "output",
    "feature",
    "vector",
    "trying",
    "predict",
    "together",
    "features",
    "matrix",
    "labels",
    "targets",
    "vector",
    "condensed",
    "chocolate",
    "bar",
    "kind",
    "talk",
    "concepts",
    "machine",
    "learning",
    "x",
    "features",
    "matrix",
    "label",
    "row",
    "fed",
    "model",
    "right",
    "model",
    "make",
    "sort",
    "prediction",
    "compare",
    "prediction",
    "actual",
    "value",
    "label",
    "data",
    "set",
    "whole",
    "point",
    "supervised",
    "learning",
    "compare",
    "model",
    "outputting",
    "oh",
    "truth",
    "actually",
    "go",
    "back",
    "adjust",
    "things",
    "next",
    "iteration",
    "get",
    "closer",
    "true",
    "value",
    "whole",
    "process",
    "tinkering",
    "okay",
    "difference",
    "go",
    "wrong",
    "known",
    "training",
    "model",
    "alright",
    "take",
    "whole",
    "know",
    "chunk",
    "right",
    "want",
    "really",
    "put",
    "entire",
    "chocolate",
    "bar",
    "model",
    "train",
    "model",
    "really",
    "right",
    "know",
    "model",
    "well",
    "new",
    "data",
    "seen",
    "like",
    "create",
    "model",
    "predict",
    "whether",
    "someone",
    "diabetes",
    "let",
    "say",
    "train",
    "data",
    "see",
    "training",
    "data",
    "well",
    "go",
    "hospital",
    "like",
    "model",
    "think",
    "use",
    "predict",
    "somebody",
    "diabetes",
    "think",
    "would",
    "effective",
    "probably",
    "right",
    "assessed",
    "well",
    "model",
    "generalize",
    "okay",
    "might",
    "well",
    "know",
    "model",
    "seen",
    "data",
    "new",
    "data",
    "model",
    "handle",
    "new",
    "data",
    "well",
    "get",
    "model",
    "assess",
    "actually",
    "break",
    "whole",
    "data",
    "set",
    "three",
    "different",
    "types",
    "data",
    "sets",
    "call",
    "training",
    "data",
    "set",
    "validation",
    "data",
    "set",
    "testing",
    "data",
    "set",
    "know",
    "might",
    "60",
    "20",
    "20",
    "80",
    "10",
    "really",
    "depends",
    "many",
    "statistics",
    "think",
    "either",
    "would",
    "acceptable",
    "feed",
    "training",
    "data",
    "set",
    "model",
    "come",
    "know",
    "might",
    "vector",
    "predictions",
    "corresponding",
    "sample",
    "put",
    "model",
    "figure",
    "okay",
    "difference",
    "prediction",
    "true",
    "values",
    "something",
    "known",
    "loss",
    "losses",
    "know",
    "difference",
    "numerical",
    "quantity",
    "course",
    "make",
    "adjustments",
    "call",
    "training",
    "okay",
    "know",
    "made",
    "bunch",
    "adjustments",
    "put",
    "validation",
    "set",
    "model",
    "validation",
    "set",
    "kind",
    "used",
    "reality",
    "check",
    "training",
    "ensure",
    "model",
    "handle",
    "unseen",
    "data",
    "still",
    "every",
    "single",
    "time",
    "train",
    "one",
    "iteration",
    "might",
    "stick",
    "validation",
    "set",
    "see",
    "hey",
    "loss",
    "training",
    "assess",
    "validation",
    "set",
    "ask",
    "hey",
    "loss",
    "one",
    "key",
    "difference",
    "training",
    "step",
    "loss",
    "never",
    "gets",
    "fed",
    "back",
    "model",
    "right",
    "feedback",
    "loop",
    "closed",
    "alright",
    "let",
    "talk",
    "loss",
    "really",
    "quickly",
    "four",
    "different",
    "types",
    "models",
    "sort",
    "data",
    "fed",
    "model",
    "output",
    "okay",
    "output",
    "pretty",
    "far",
    "know",
    "truth",
    "want",
    "loss",
    "going",
    "high",
    "model",
    "b",
    "pretty",
    "far",
    "want",
    "loss",
    "also",
    "going",
    "high",
    "let",
    "give",
    "one",
    "pretty",
    "close",
    "mean",
    "maybe",
    "almost",
    "pretty",
    "close",
    "one",
    "might",
    "loss",
    "one",
    "maybe",
    "still",
    "better",
    "two",
    "loss",
    "might",
    "okay",
    "model",
    "performs",
    "best",
    "well",
    "model",
    "c",
    "smallest",
    "loss",
    "probably",
    "model",
    "okay",
    "let",
    "take",
    "model",
    "know",
    "come",
    "models",
    "seen",
    "okay",
    "model",
    "c",
    "probably",
    "best",
    "model",
    "take",
    "model",
    "c",
    "run",
    "test",
    "set",
    "model",
    "test",
    "set",
    "used",
    "final",
    "check",
    "see",
    "generalizable",
    "chosen",
    "model",
    "know",
    "finish",
    "training",
    "diabetes",
    "data",
    "set",
    "could",
    "run",
    "chunk",
    "data",
    "say",
    "oh",
    "like",
    "perform",
    "data",
    "never",
    "seen",
    "point",
    "training",
    "process",
    "okay",
    "loss",
    "final",
    "reported",
    "performance",
    "test",
    "set",
    "would",
    "final",
    "reported",
    "performance",
    "model",
    "okay",
    "let",
    "talk",
    "thing",
    "called",
    "loss",
    "think",
    "kind",
    "glossed",
    "right",
    "loss",
    "difference",
    "prediction",
    "actual",
    "like",
    "label",
    "would",
    "give",
    "slightly",
    "higher",
    "loss",
    "would",
    "even",
    "give",
    "higher",
    "loss",
    "even",
    "computer",
    "science",
    "like",
    "formulas",
    "right",
    "like",
    "formulaic",
    "ways",
    "describing",
    "things",
    "examples",
    "loss",
    "functions",
    "actually",
    "come",
    "numbers",
    "known",
    "l",
    "one",
    "loss",
    "basically",
    "l",
    "one",
    "loss",
    "takes",
    "absolute",
    "value",
    "whatever",
    "know",
    "real",
    "value",
    "whatever",
    "real",
    "output",
    "label",
    "subtracts",
    "predicted",
    "value",
    "takes",
    "absolute",
    "value",
    "okay",
    "absolute",
    "value",
    "function",
    "looks",
    "something",
    "like",
    "greater",
    "losses",
    "right",
    "either",
    "direction",
    "real",
    "value",
    "predicted",
    "value",
    "10",
    "loss",
    "point",
    "would",
    "sum",
    "means",
    "hey",
    "taking",
    "points",
    "data",
    "set",
    "trying",
    "figure",
    "sum",
    "far",
    "everything",
    "also",
    "something",
    "called",
    "l",
    "two",
    "loss",
    "loss",
    "function",
    "quadratic",
    "means",
    "close",
    "penalty",
    "minimal",
    "lot",
    "penalty",
    "much",
    "much",
    "higher",
    "okay",
    "instead",
    "absolute",
    "value",
    "square",
    "difference",
    "two",
    "also",
    "something",
    "called",
    "binary",
    "cross",
    "entropy",
    "loss",
    "looks",
    "something",
    "like",
    "binary",
    "classification",
    "might",
    "loss",
    "use",
    "loss",
    "know",
    "going",
    "really",
    "go",
    "much",
    "need",
    "know",
    "loss",
    "decreases",
    "performance",
    "gets",
    "better",
    "measures",
    "accurate",
    "performance",
    "well",
    "example",
    "accuracy",
    "accuracy",
    "let",
    "say",
    "pictures",
    "feeding",
    "model",
    "okay",
    "predictions",
    "might",
    "apple",
    "orange",
    "orange",
    "apple",
    "okay",
    "actual",
    "apple",
    "orange",
    "apple",
    "apple",
    "three",
    "correct",
    "one",
    "incorrect",
    "accuracy",
    "model",
    "three",
    "quarters",
    "75",
    "alright",
    "coming",
    "back",
    "colab",
    "notebook",
    "going",
    "close",
    "little",
    "bit",
    "imported",
    "stuff",
    "already",
    "created",
    "data",
    "frame",
    "right",
    "data",
    "going",
    "use",
    "train",
    "models",
    "take",
    "look",
    "data",
    "set",
    "see",
    "classes",
    "zeros",
    "ones",
    "numerical",
    "good",
    "computer",
    "understand",
    "okay",
    "know",
    "would",
    "probably",
    "good",
    "idea",
    "maybe",
    "kind",
    "plot",
    "hey",
    "things",
    "anything",
    "class",
    "going",
    "go",
    "labels",
    "label",
    "columns",
    "data",
    "frame",
    "gets",
    "list",
    "actually",
    "list",
    "right",
    "called",
    "let",
    "use",
    "might",
    "less",
    "confusing",
    "everything",
    "last",
    "thing",
    "class",
    "going",
    "take",
    "10",
    "different",
    "features",
    "going",
    "plot",
    "histogram",
    "going",
    "plot",
    "histogram",
    "basically",
    "take",
    "data",
    "frame",
    "say",
    "okay",
    "everything",
    "class",
    "equal",
    "one",
    "gammas",
    "remember",
    "portion",
    "data",
    "frame",
    "look",
    "label",
    "okay",
    "part",
    "saying",
    "inside",
    "data",
    "frame",
    "get",
    "everything",
    "class",
    "equal",
    "one",
    "would",
    "fit",
    "category",
    "right",
    "let",
    "look",
    "label",
    "column",
    "first",
    "label",
    "would",
    "f",
    "length",
    "would",
    "column",
    "command",
    "getting",
    "different",
    "values",
    "belong",
    "class",
    "one",
    "specific",
    "label",
    "exactly",
    "going",
    "put",
    "histogram",
    "going",
    "tell",
    "know",
    "matplotlib",
    "make",
    "color",
    "blue",
    "make",
    "label",
    "know",
    "gamma",
    "set",
    "alpha",
    "keep",
    "alpha",
    "equal",
    "like",
    "transparency",
    "going",
    "set",
    "density",
    "equal",
    "true",
    "compare",
    "hadrons",
    "baseline",
    "comparing",
    "okay",
    "density",
    "true",
    "basically",
    "normalizes",
    "distributions",
    "know",
    "200",
    "one",
    "type",
    "50",
    "another",
    "type",
    "well",
    "drew",
    "histograms",
    "would",
    "hard",
    "compare",
    "one",
    "would",
    "lot",
    "bigger",
    "right",
    "normalizing",
    "kind",
    "distributing",
    "many",
    "samples",
    "alright",
    "going",
    "put",
    "title",
    "make",
    "label",
    "label",
    "density",
    "label",
    "probability",
    "x",
    "label",
    "going",
    "label",
    "going",
    "going",
    "include",
    "legend",
    "plt",
    "dot",
    "show",
    "means",
    "okay",
    "display",
    "plot",
    "run",
    "last",
    "item",
    "want",
    "list",
    "right",
    "last",
    "item",
    "see",
    "plotting",
    "length",
    "oh",
    "made",
    "gamma",
    "hadron",
    "okay",
    "gammas",
    "blue",
    "hadrons",
    "red",
    "already",
    "see",
    "know",
    "maybe",
    "length",
    "smaller",
    "probably",
    "likely",
    "gamma",
    "right",
    "kind",
    "know",
    "look",
    "somewhat",
    "similar",
    "okay",
    "clearly",
    "asymmetry",
    "know",
    "asymmetry",
    "measure",
    "larger",
    "probably",
    "hadron",
    "okay",
    "oh",
    "one",
    "good",
    "one",
    "f",
    "alpha",
    "seems",
    "like",
    "hadrons",
    "pretty",
    "evenly",
    "distributed",
    "whereas",
    "smaller",
    "looks",
    "like",
    "gammas",
    "area",
    "okay",
    "kind",
    "data",
    "working",
    "kind",
    "see",
    "going",
    "okay",
    "next",
    "thing",
    "going",
    "going",
    "create",
    "train",
    "validation",
    "test",
    "data",
    "sets",
    "going",
    "set",
    "train",
    "valid",
    "test",
    "equal",
    "numpy",
    "dot",
    "split",
    "splitting",
    "data",
    "frame",
    "sample",
    "sampling",
    "everything",
    "basically",
    "shuffle",
    "data",
    "want",
    "pass",
    "exactly",
    "splitting",
    "data",
    "set",
    "first",
    "split",
    "going",
    "maybe",
    "60",
    "going",
    "say",
    "times",
    "length",
    "data",
    "frame",
    "cast",
    "10",
    "integer",
    "going",
    "first",
    "place",
    "know",
    "cut",
    "training",
    "data",
    "go",
    "basically",
    "means",
    "everything",
    "60",
    "80",
    "length",
    "data",
    "set",
    "go",
    "towards",
    "validation",
    "like",
    "everything",
    "80",
    "100",
    "going",
    "pass",
    "test",
    "data",
    "run",
    "go",
    "inspect",
    "data",
    "see",
    "columns",
    "seem",
    "values",
    "like",
    "100s",
    "whereas",
    "one",
    "right",
    "scale",
    "numbers",
    "way",
    "sometimes",
    "affect",
    "results",
    "going",
    "run",
    "way",
    "sometimes",
    "affect",
    "results",
    "one",
    "thing",
    "would",
    "want",
    "scale",
    "know",
    "relative",
    "maybe",
    "mean",
    "standard",
    "deviation",
    "specific",
    "column",
    "going",
    "create",
    "function",
    "called",
    "scale",
    "data",
    "set",
    "going",
    "pass",
    "data",
    "frame",
    "okay",
    "x",
    "values",
    "going",
    "know",
    "take",
    "data",
    "frame",
    "let",
    "assume",
    "columns",
    "going",
    "know",
    "label",
    "always",
    "last",
    "thing",
    "data",
    "frame",
    "say",
    "data",
    "frame",
    "dot",
    "columns",
    "way",
    "last",
    "item",
    "get",
    "values",
    "well",
    "last",
    "column",
    "index",
    "last",
    "column",
    "get",
    "values",
    "actually",
    "going",
    "import",
    "something",
    "known",
    "standard",
    "scalar",
    "sk",
    "learn",
    "come",
    "go",
    "sk",
    "learn",
    "dot",
    "pre",
    "processing",
    "going",
    "import",
    "standard",
    "scalar",
    "run",
    "cell",
    "going",
    "come",
    "back",
    "going",
    "create",
    "scalar",
    "use",
    "skip",
    "standard",
    "scalar",
    "scalar",
    "actually",
    "fit",
    "transform",
    "say",
    "x",
    "equal",
    "scalar",
    "dot",
    "fit",
    "fit",
    "transform",
    "saying",
    "okay",
    "take",
    "x",
    "fit",
    "standard",
    "scalar",
    "x",
    "transform",
    "values",
    "would",
    "going",
    "new",
    "alright",
    "also",
    "going",
    "create",
    "know",
    "whole",
    "data",
    "one",
    "huge",
    "2d",
    "numpy",
    "array",
    "order",
    "going",
    "call",
    "h",
    "stack",
    "h",
    "stack",
    "saying",
    "okay",
    "take",
    "array",
    "another",
    "array",
    "horizontally",
    "stack",
    "together",
    "h",
    "stands",
    "horizontally",
    "stacked",
    "together",
    "like",
    "put",
    "side",
    "side",
    "okay",
    "top",
    "stacking",
    "well",
    "pass",
    "something",
    "stack",
    "x",
    "okay",
    "numpy",
    "particular",
    "dimensions",
    "right",
    "specific",
    "case",
    "x",
    "two",
    "dimensional",
    "object",
    "one",
    "dimensional",
    "thing",
    "vector",
    "values",
    "order",
    "reshape",
    "2d",
    "item",
    "call",
    "numpy",
    "dot",
    "reshape",
    "pass",
    "dimensions",
    "reshape",
    "pass",
    "negative",
    "one",
    "comma",
    "one",
    "means",
    "okay",
    "make",
    "2d",
    "array",
    "negative",
    "one",
    "means",
    "infer",
    "dimension",
    "value",
    "would",
    "ends",
    "length",
    "would",
    "literally",
    "negative",
    "one",
    "easier",
    "making",
    "computer",
    "hard",
    "work",
    "stack",
    "going",
    "return",
    "data",
    "x",
    "okay",
    "one",
    "thing",
    "go",
    "training",
    "data",
    "set",
    "okay",
    "training",
    "data",
    "set",
    "get",
    "length",
    "training",
    "data",
    "set",
    "training",
    "data",
    "sets",
    "class",
    "one",
    "remember",
    "gammas",
    "print",
    "thing",
    "zero",
    "see",
    "know",
    "around",
    "7000",
    "gammas",
    "around",
    "4000",
    "hadrons",
    "might",
    "actually",
    "become",
    "issue",
    "instead",
    "want",
    "want",
    "oversample",
    "training",
    "data",
    "set",
    "means",
    "want",
    "increase",
    "number",
    "values",
    "kind",
    "match",
    "better",
    "surprise",
    "surprise",
    "something",
    "import",
    "help",
    "us",
    "going",
    "go",
    "learn",
    "dot",
    "oversampling",
    "going",
    "import",
    "random",
    "oversampler",
    "run",
    "cell",
    "come",
    "back",
    "actually",
    "add",
    "parameter",
    "called",
    "oversample",
    "set",
    "false",
    "default",
    "want",
    "oversample",
    "going",
    "oversample",
    "want",
    "oversample",
    "going",
    "create",
    "ros",
    "set",
    "equal",
    "random",
    "oversampler",
    "x",
    "going",
    "say",
    "okay",
    "fit",
    "resample",
    "x",
    "saying",
    "okay",
    "take",
    "less",
    "class",
    "take",
    "take",
    "less",
    "class",
    "keep",
    "sampling",
    "increase",
    "size",
    "data",
    "set",
    "smaller",
    "class",
    "match",
    "scale",
    "data",
    "set",
    "pass",
    "training",
    "data",
    "set",
    "oversample",
    "true",
    "let",
    "say",
    "train",
    "x",
    "train",
    "train",
    "oops",
    "going",
    "columns",
    "basically",
    "saying",
    "okay",
    "length",
    "train",
    "okay",
    "whatever",
    "let",
    "take",
    "look",
    "many",
    "type",
    "one",
    "actually",
    "sum",
    "also",
    "see",
    "instead",
    "switch",
    "label",
    "ask",
    "many",
    "type",
    "value",
    "evenly",
    "know",
    "rebalanced",
    "okay",
    "well",
    "okay",
    "going",
    "make",
    "validation",
    "data",
    "set",
    "next",
    "one",
    "going",
    "make",
    "test",
    "data",
    "set",
    "alright",
    "actually",
    "going",
    "switch",
    "oversample",
    "false",
    "reason",
    "switching",
    "false",
    "validation",
    "test",
    "sets",
    "purpose",
    "know",
    "data",
    "seen",
    "yet",
    "sample",
    "perform",
    "want",
    "oversample",
    "right",
    "like",
    "care",
    "balancing",
    "want",
    "know",
    "random",
    "set",
    "data",
    "unlabeled",
    "trust",
    "model",
    "right",
    "oversampling",
    "run",
    "going",
    "oh",
    "already",
    "train",
    "go",
    "come",
    "split",
    "data",
    "frame",
    "let",
    "run",
    "okay",
    "data",
    "properly",
    "formatted",
    "going",
    "move",
    "different",
    "models",
    "going",
    "tell",
    "guys",
    "little",
    "bit",
    "models",
    "going",
    "show",
    "code",
    "first",
    "model",
    "going",
    "learn",
    "knn",
    "k",
    "nearest",
    "neighbors",
    "okay",
    "already",
    "drawn",
    "plot",
    "axis",
    "number",
    "kids",
    "family",
    "might",
    "x",
    "axis",
    "income",
    "terms",
    "1000s",
    "per",
    "year",
    "know",
    "someone",
    "making",
    "year",
    "would",
    "somebody",
    "making",
    "320",
    "would",
    "somebody",
    "zero",
    "kids",
    "somewhere",
    "along",
    "axis",
    "somebody",
    "five",
    "somewhere",
    "okay",
    "plus",
    "signs",
    "minus",
    "signs",
    "going",
    "represent",
    "plus",
    "sign",
    "means",
    "car",
    "minus",
    "sign",
    "going",
    "represent",
    "car",
    "okay",
    "initial",
    "thought",
    "okay",
    "think",
    "binary",
    "classification",
    "points",
    "samples",
    "labels",
    "sample",
    "plus",
    "label",
    "another",
    "sample",
    "minus",
    "label",
    "abbreviation",
    "width",
    "use",
    "alright",
    "entire",
    "data",
    "set",
    "maybe",
    "around",
    "half",
    "people",
    "car",
    "maybe",
    "around",
    "half",
    "people",
    "car",
    "okay",
    "well",
    "new",
    "point",
    "let",
    "use",
    "choose",
    "different",
    "color",
    "use",
    "nice",
    "green",
    "well",
    "new",
    "point",
    "let",
    "say",
    "somebody",
    "makes",
    "year",
    "two",
    "kids",
    "think",
    "would",
    "well",
    "logically",
    "looking",
    "plot",
    "might",
    "think",
    "okay",
    "seems",
    "like",
    "would",
    "car",
    "right",
    "kind",
    "matches",
    "pattern",
    "everybody",
    "else",
    "around",
    "whole",
    "concept",
    "nearest",
    "neighbors",
    "look",
    "okay",
    "around",
    "basically",
    "like",
    "okay",
    "going",
    "take",
    "label",
    "majority",
    "around",
    "first",
    "thing",
    "define",
    "distance",
    "function",
    "lot",
    "times",
    "know",
    "2d",
    "plots",
    "like",
    "distance",
    "function",
    "something",
    "known",
    "euclidean",
    "distance",
    "euclidean",
    "distance",
    "basically",
    "straight",
    "line",
    "distance",
    "like",
    "okay",
    "would",
    "euclidean",
    "distance",
    "seems",
    "like",
    "point",
    "point",
    "point",
    "etc",
    "length",
    "line",
    "green",
    "line",
    "drew",
    "known",
    "euclidean",
    "distance",
    "want",
    "get",
    "technical",
    "exact",
    "formula",
    "distance",
    "let",
    "zoom",
    "distance",
    "equal",
    "square",
    "root",
    "one",
    "point",
    "x",
    "minus",
    "points",
    "x",
    "squared",
    "plus",
    "extend",
    "square",
    "root",
    "thing",
    "one",
    "one",
    "minus",
    "two",
    "squared",
    "okay",
    "basically",
    "trying",
    "find",
    "length",
    "distances",
    "difference",
    "x",
    "square",
    "sum",
    "take",
    "square",
    "root",
    "okay",
    "going",
    "erase",
    "clutter",
    "drawing",
    "anyways",
    "going",
    "back",
    "plot",
    "nearest",
    "neighbor",
    "algorithm",
    "see",
    "k",
    "right",
    "k",
    "basically",
    "telling",
    "us",
    "okay",
    "many",
    "neighbors",
    "use",
    "order",
    "judge",
    "label",
    "usually",
    "use",
    "k",
    "maybe",
    "know",
    "three",
    "five",
    "depends",
    "big",
    "data",
    "set",
    "would",
    "say",
    "maybe",
    "logical",
    "number",
    "would",
    "three",
    "five",
    "let",
    "say",
    "take",
    "k",
    "equal",
    "three",
    "okay",
    "well",
    "data",
    "point",
    "drew",
    "let",
    "use",
    "green",
    "highlight",
    "okay",
    "data",
    "point",
    "drew",
    "looks",
    "like",
    "three",
    "closest",
    "points",
    "definitely",
    "one",
    "one",
    "one",
    "length",
    "four",
    "one",
    "seems",
    "like",
    "little",
    "bit",
    "four",
    "actually",
    "would",
    "would",
    "three",
    "points",
    "well",
    "points",
    "blue",
    "chances",
    "prediction",
    "point",
    "going",
    "blue",
    "going",
    "probably",
    "car",
    "right",
    "point",
    "somewhere",
    "point",
    "somewhere",
    "let",
    "say",
    "couple",
    "four",
    "kids",
    "make",
    "year",
    "right",
    "well",
    "closest",
    "points",
    "one",
    "probably",
    "little",
    "bit",
    "one",
    "one",
    "right",
    "okay",
    "still",
    "pluses",
    "well",
    "one",
    "likely",
    "plus",
    "right",
    "let",
    "get",
    "rid",
    "looks",
    "little",
    "bit",
    "clear",
    "right",
    "let",
    "go",
    "one",
    "point",
    "might",
    "right",
    "okay",
    "let",
    "see",
    "well",
    "definitely",
    "closest",
    "right",
    "one",
    "also",
    "closest",
    "really",
    "close",
    "two",
    "actually",
    "mathematics",
    "seems",
    "like",
    "zoom",
    "one",
    "right",
    "one",
    "two",
    "one",
    "actually",
    "shorter",
    "one",
    "means",
    "top",
    "one",
    "one",
    "going",
    "take",
    "majority",
    "points",
    "close",
    "well",
    "one",
    "plus",
    "one",
    "plus",
    "one",
    "minus",
    "means",
    "pluses",
    "majority",
    "means",
    "label",
    "probably",
    "somebody",
    "car",
    "okay",
    "k",
    "nearest",
    "neighbors",
    "would",
    "work",
    "simple",
    "extrapolated",
    "dimensions",
    "higher",
    "dimensions",
    "know",
    "two",
    "different",
    "features",
    "income",
    "number",
    "kids",
    "let",
    "say",
    "10",
    "different",
    "features",
    "expand",
    "distance",
    "function",
    "includes",
    "10",
    "dimensions",
    "take",
    "square",
    "root",
    "everything",
    "figure",
    "one",
    "closest",
    "point",
    "desire",
    "classify",
    "okay",
    "k",
    "nearest",
    "neighbors",
    "learned",
    "k",
    "nearest",
    "neighbors",
    "let",
    "see",
    "would",
    "able",
    "within",
    "code",
    "going",
    "label",
    "section",
    "k",
    "nearest",
    "neighbors",
    "actually",
    "going",
    "use",
    "package",
    "sk",
    "learn",
    "reason",
    "know",
    "use",
    "packages",
    "manually",
    "code",
    "things",
    "would",
    "really",
    "difficult",
    "chances",
    "way",
    "would",
    "code",
    "either",
    "would",
    "bugs",
    "really",
    "slow",
    "know",
    "whole",
    "bunch",
    "issues",
    "going",
    "hand",
    "pros",
    "say",
    "okay",
    "sk",
    "learn",
    "package",
    "dot",
    "neighbors",
    "going",
    "import",
    "k",
    "neighbors",
    "classifier",
    "classifying",
    "okay",
    "run",
    "knn",
    "model",
    "going",
    "k",
    "neighbors",
    "classifier",
    "pass",
    "parameter",
    "many",
    "neighbors",
    "know",
    "want",
    "use",
    "first",
    "let",
    "see",
    "happens",
    "use",
    "one",
    "k",
    "model",
    "dot",
    "fit",
    "pass",
    "x",
    "training",
    "set",
    "weight",
    "train",
    "data",
    "okay",
    "effectively",
    "fits",
    "model",
    "let",
    "get",
    "predictions",
    "guess",
    "yeah",
    "let",
    "predictions",
    "predictions",
    "going",
    "cannon",
    "model",
    "dot",
    "predict",
    "let",
    "use",
    "test",
    "set",
    "x",
    "test",
    "okay",
    "alright",
    "call",
    "predict",
    "see",
    "get",
    "truth",
    "values",
    "test",
    "set",
    "see",
    "actually",
    "looking",
    "got",
    "five",
    "six",
    "okay",
    "great",
    "let",
    "actually",
    "take",
    "look",
    "something",
    "called",
    "classification",
    "report",
    "offered",
    "sk",
    "learn",
    "go",
    "sk",
    "learn",
    "dot",
    "metrics",
    "import",
    "classification",
    "report",
    "actually",
    "say",
    "hey",
    "print",
    "classification",
    "report",
    "let",
    "check",
    "know",
    "giving",
    "test",
    "prediction",
    "run",
    "see",
    "get",
    "whole",
    "entire",
    "chart",
    "going",
    "tell",
    "guys",
    "things",
    "chart",
    "alright",
    "accuracy",
    "82",
    "actually",
    "pretty",
    "good",
    "saying",
    "hey",
    "look",
    "know",
    "new",
    "points",
    "closest",
    "actually",
    "get",
    "82",
    "accuracy",
    "means",
    "many",
    "get",
    "right",
    "versus",
    "many",
    "total",
    "precision",
    "saying",
    "okay",
    "might",
    "see",
    "class",
    "one",
    "class",
    "zero",
    "class",
    "one",
    "precision",
    "saying",
    "let",
    "go",
    "wikipedia",
    "diagram",
    "actually",
    "kind",
    "like",
    "diagram",
    "entire",
    "data",
    "set",
    "left",
    "everything",
    "know",
    "positive",
    "everything",
    "actually",
    "truly",
    "positive",
    "labeled",
    "positive",
    "original",
    "data",
    "set",
    "everything",
    "truly",
    "negative",
    "circle",
    "things",
    "positive",
    "labeled",
    "positive",
    "model",
    "left",
    "things",
    "truly",
    "positive",
    "know",
    "side",
    "positive",
    "side",
    "side",
    "negative",
    "side",
    "truly",
    "positive",
    "whereas",
    "ones",
    "well",
    "positive",
    "labeled",
    "negative",
    "ones",
    "labeled",
    "positive",
    "actually",
    "negative",
    "truly",
    "negative",
    "precision",
    "saying",
    "okay",
    "ones",
    "labeled",
    "positive",
    "many",
    "true",
    "positives",
    "recall",
    "saying",
    "okay",
    "ones",
    "know",
    "truly",
    "positive",
    "many",
    "actually",
    "get",
    "right",
    "okay",
    "going",
    "back",
    "precision",
    "score",
    "precision",
    "ones",
    "labeled",
    "specific",
    "class",
    "many",
    "actually",
    "class",
    "7784",
    "recall",
    "ones",
    "actually",
    "class",
    "many",
    "get",
    "68",
    "89",
    "alright",
    "shabby",
    "clearly",
    "see",
    "recall",
    "precision",
    "like",
    "class",
    "zero",
    "worse",
    "class",
    "one",
    "right",
    "means",
    "hadron",
    "worked",
    "hadrons",
    "gammas",
    "f1",
    "score",
    "kind",
    "combination",
    "precision",
    "recall",
    "score",
    "actually",
    "going",
    "mostly",
    "look",
    "one",
    "unbalanced",
    "test",
    "data",
    "set",
    "measure",
    "72",
    "87",
    "point",
    "seven",
    "two",
    "point",
    "eight",
    "seven",
    "shabby",
    "right",
    "well",
    "know",
    "made",
    "three",
    "actually",
    "see",
    "okay",
    "originally",
    "one",
    "see",
    "f1",
    "score",
    "know",
    "point",
    "seven",
    "two",
    "point",
    "eight",
    "seven",
    "accuracy",
    "82",
    "change",
    "three",
    "alright",
    "kind",
    "increased",
    "zero",
    "cost",
    "one",
    "overall",
    "accuracy",
    "let",
    "actually",
    "make",
    "five",
    "alright",
    "know",
    "similar",
    "numbers",
    "82",
    "accuracy",
    "pretty",
    "decent",
    "model",
    "relatively",
    "simple",
    "okay",
    "next",
    "type",
    "model",
    "going",
    "talk",
    "something",
    "known",
    "naive",
    "bayes",
    "order",
    "understand",
    "concepts",
    "behind",
    "naive",
    "bayes",
    "able",
    "understand",
    "conditional",
    "probability",
    "bayes",
    "rule",
    "let",
    "say",
    "sort",
    "data",
    "set",
    "shown",
    "table",
    "right",
    "people",
    "covid",
    "red",
    "row",
    "people",
    "covid",
    "green",
    "row",
    "covid",
    "test",
    "well",
    "people",
    "tested",
    "positive",
    "column",
    "people",
    "tested",
    "negative",
    "column",
    "okay",
    "yeah",
    "basically",
    "categories",
    "people",
    "covid",
    "test",
    "positive",
    "people",
    "covid",
    "test",
    "positive",
    "false",
    "false",
    "positive",
    "people",
    "covid",
    "test",
    "negative",
    "false",
    "negative",
    "people",
    "covid",
    "test",
    "negative",
    "good",
    "means",
    "covid",
    "okay",
    "let",
    "make",
    "slightly",
    "legible",
    "margins",
    "written",
    "sums",
    "whatever",
    "referring",
    "sum",
    "entire",
    "row",
    "might",
    "sum",
    "column",
    "okay",
    "first",
    "question",
    "probability",
    "covid",
    "given",
    "positive",
    "test",
    "probability",
    "write",
    "like",
    "probability",
    "covid",
    "given",
    "line",
    "vertical",
    "line",
    "means",
    "given",
    "know",
    "condition",
    "given",
    "positive",
    "test",
    "okay",
    "probability",
    "covid",
    "given",
    "positive",
    "test",
    "asking",
    "saying",
    "okay",
    "let",
    "go",
    "condition",
    "condition",
    "positive",
    "test",
    "slice",
    "data",
    "right",
    "means",
    "slice",
    "data",
    "positive",
    "test",
    "given",
    "positive",
    "test",
    "given",
    "condition",
    "circumstance",
    "positive",
    "test",
    "probability",
    "covid",
    "well",
    "using",
    "data",
    "number",
    "people",
    "covid",
    "gon",
    "na",
    "say",
    "531",
    "people",
    "covid",
    "divide",
    "total",
    "number",
    "people",
    "positive",
    "test",
    "okay",
    "probability",
    "quick",
    "division",
    "get",
    "equal",
    "around",
    "according",
    "data",
    "set",
    "data",
    "made",
    "top",
    "head",
    "actually",
    "real",
    "covid",
    "data",
    "according",
    "data",
    "probability",
    "covid",
    "given",
    "tested",
    "positive",
    "alright",
    "let",
    "talk",
    "bayes",
    "rule",
    "section",
    "let",
    "ignore",
    "bottom",
    "part",
    "bayes",
    "rule",
    "asking",
    "okay",
    "probability",
    "event",
    "happening",
    "given",
    "b",
    "happened",
    "already",
    "know",
    "happened",
    "condition",
    "right",
    "well",
    "data",
    "right",
    "like",
    "know",
    "probability",
    "given",
    "b",
    "well",
    "bayes",
    "rule",
    "saying",
    "okay",
    "well",
    "actually",
    "go",
    "calculate",
    "long",
    "probability",
    "b",
    "given",
    "probability",
    "probability",
    "okay",
    "mathematical",
    "formula",
    "alright",
    "bayes",
    "rule",
    "let",
    "actually",
    "see",
    "bayes",
    "rule",
    "action",
    "let",
    "use",
    "example",
    "let",
    "say",
    "disease",
    "statistics",
    "okay",
    "covid",
    "different",
    "disease",
    "know",
    "probability",
    "obtaining",
    "false",
    "positive",
    "probability",
    "obtaining",
    "false",
    "negative",
    "probability",
    "disease",
    "okay",
    "probability",
    "disease",
    "given",
    "got",
    "positive",
    "test",
    "hmm",
    "even",
    "go",
    "solving",
    "mean",
    "false",
    "positive",
    "different",
    "way",
    "rewrite",
    "false",
    "positive",
    "test",
    "positive",
    "actually",
    "disease",
    "probability",
    "positive",
    "test",
    "given",
    "disease",
    "right",
    "similarly",
    "false",
    "negative",
    "probability",
    "test",
    "negative",
    "given",
    "actually",
    "disease",
    "put",
    "chart",
    "example",
    "might",
    "positive",
    "negative",
    "tests",
    "might",
    "diseases",
    "disease",
    "disease",
    "well",
    "probability",
    "test",
    "positive",
    "actually",
    "disease",
    "okay",
    "false",
    "negatives",
    "testing",
    "negative",
    "actually",
    "disease",
    "probability",
    "test",
    "positive",
    "disease",
    "plus",
    "probability",
    "test",
    "negative",
    "given",
    "disease",
    "sum",
    "one",
    "okay",
    "disease",
    "probability",
    "testing",
    "positive",
    "probability",
    "testing",
    "negative",
    "probability",
    "total",
    "one",
    "means",
    "probability",
    "negative",
    "disease",
    "reciprocal",
    "opposite",
    "one",
    "minus",
    "whatever",
    "probability",
    "similarly",
    "oops",
    "probability",
    "know",
    "test",
    "negative",
    "disease",
    "plus",
    "probability",
    "test",
    "positive",
    "disease",
    "equal",
    "one",
    "probability",
    "chart",
    "probability",
    "disease",
    "point",
    "means",
    "10",
    "probability",
    "actually",
    "disease",
    "right",
    "like",
    "general",
    "population",
    "probability",
    "disease",
    "okay",
    "probability",
    "disease",
    "given",
    "got",
    "positive",
    "test",
    "well",
    "remember",
    "write",
    "terms",
    "bayes",
    "rule",
    "right",
    "use",
    "rule",
    "probability",
    "positive",
    "test",
    "given",
    "disease",
    "times",
    "probability",
    "disease",
    "divided",
    "probability",
    "evidence",
    "positive",
    "test",
    "alright",
    "let",
    "plug",
    "numbers",
    "probability",
    "positive",
    "test",
    "given",
    "disease",
    "probability",
    "disease",
    "value",
    "okay",
    "probability",
    "positive",
    "test",
    "okay",
    "probability",
    "positive",
    "test",
    "given",
    "actually",
    "disease",
    "disease",
    "case",
    "probability",
    "negative",
    "test",
    "given",
    "sorry",
    "positive",
    "test",
    "giving",
    "disease",
    "times",
    "probability",
    "actually",
    "disease",
    "okay",
    "expand",
    "probability",
    "positive",
    "test",
    "two",
    "different",
    "cases",
    "disease",
    "probability",
    "positive",
    "tests",
    "either",
    "one",
    "cases",
    "expression",
    "would",
    "become",
    "times",
    "plus",
    "probability",
    "testing",
    "positive",
    "disease",
    "times",
    "probability",
    "actually",
    "disease",
    "one",
    "minus",
    "probability",
    "population",
    "disease",
    "90",
    "let",
    "multiplication",
    "get",
    "answer",
    "okay",
    "right",
    "actually",
    "expand",
    "expand",
    "bayes",
    "rule",
    "apply",
    "classification",
    "call",
    "naive",
    "base",
    "first",
    "little",
    "terminology",
    "posterior",
    "asking",
    "hey",
    "probability",
    "class",
    "ck",
    "ck",
    "mean",
    "know",
    "different",
    "categories",
    "c",
    "category",
    "class",
    "whatever",
    "category",
    "one",
    "might",
    "cats",
    "category",
    "two",
    "dogs",
    "category",
    "three",
    "lizards",
    "way",
    "k",
    "categories",
    "k",
    "number",
    "okay",
    "probability",
    "specific",
    "sample",
    "x",
    "feature",
    "vector",
    "one",
    "sample",
    "probability",
    "x",
    "fitting",
    "category",
    "123",
    "whatever",
    "right",
    "asking",
    "probability",
    "know",
    "actually",
    "class",
    "given",
    "evidence",
    "see",
    "x",
    "likelihood",
    "quantity",
    "saying",
    "okay",
    "well",
    "given",
    "know",
    "assume",
    "assume",
    "assume",
    "class",
    "class",
    "ck",
    "okay",
    "assume",
    "category",
    "well",
    "likelihood",
    "actually",
    "seeing",
    "x",
    "different",
    "features",
    "category",
    "prior",
    "like",
    "entire",
    "population",
    "things",
    "probabilities",
    "probability",
    "class",
    "general",
    "like",
    "know",
    "entire",
    "data",
    "set",
    "percentage",
    "chance",
    "image",
    "cat",
    "many",
    "cats",
    "right",
    "called",
    "evidence",
    "trying",
    "changing",
    "prior",
    "creating",
    "new",
    "posterior",
    "probability",
    "built",
    "upon",
    "prior",
    "using",
    "sort",
    "evidence",
    "right",
    "evidence",
    "probability",
    "vocab",
    "rule",
    "naive",
    "bayes",
    "whoa",
    "okay",
    "let",
    "digest",
    "little",
    "bit",
    "okay",
    "let",
    "use",
    "different",
    "color",
    "side",
    "equation",
    "asking",
    "asking",
    "probability",
    "class",
    "k",
    "ck",
    "given",
    "know",
    "first",
    "input",
    "second",
    "input",
    "know",
    "third",
    "fourth",
    "nth",
    "input",
    "let",
    "say",
    "classification",
    "play",
    "soccer",
    "today",
    "okay",
    "let",
    "say",
    "x",
    "okay",
    "much",
    "wind",
    "much",
    "rain",
    "day",
    "week",
    "let",
    "let",
    "say",
    "raining",
    "windy",
    "wednesday",
    "play",
    "soccer",
    "let",
    "use",
    "bayes",
    "rule",
    "equal",
    "probability",
    "x",
    "one",
    "x",
    "two",
    "joint",
    "probabilities",
    "given",
    "class",
    "k",
    "times",
    "probability",
    "class",
    "probability",
    "evidence",
    "okay",
    "fancy",
    "symbol",
    "means",
    "proportional",
    "equal",
    "sign",
    "means",
    "equal",
    "like",
    "little",
    "squiggly",
    "sign",
    "means",
    "proportional",
    "okay",
    "denominator",
    "might",
    "notice",
    "impact",
    "class",
    "like",
    "number",
    "depend",
    "class",
    "right",
    "going",
    "constant",
    "different",
    "classes",
    "going",
    "make",
    "things",
    "simpler",
    "going",
    "say",
    "probability",
    "x",
    "one",
    "x",
    "two",
    "way",
    "x",
    "n",
    "going",
    "proportional",
    "numerator",
    "care",
    "denominator",
    "every",
    "single",
    "class",
    "proportional",
    "x",
    "one",
    "x",
    "two",
    "x",
    "n",
    "given",
    "class",
    "k",
    "times",
    "probability",
    "class",
    "okay",
    "right",
    "naive",
    "bayes",
    "point",
    "naive",
    "actually",
    "joint",
    "probability",
    "assuming",
    "different",
    "things",
    "independent",
    "soccer",
    "example",
    "know",
    "probability",
    "playing",
    "soccer",
    "probability",
    "know",
    "windy",
    "rainy",
    "wednesday",
    "things",
    "independent",
    "assuming",
    "independent",
    "means",
    "actually",
    "write",
    "part",
    "equation",
    "term",
    "multiply",
    "together",
    "probability",
    "first",
    "feature",
    "given",
    "class",
    "k",
    "times",
    "probability",
    "second",
    "feature",
    "given",
    "problem",
    "like",
    "class",
    "k",
    "way",
    "way",
    "know",
    "nth",
    "feature",
    "given",
    "class",
    "expands",
    "right",
    "means",
    "proportional",
    "thing",
    "expanded",
    "times",
    "going",
    "write",
    "probability",
    "class",
    "actually",
    "going",
    "use",
    "symbol",
    "means",
    "huge",
    "multiplication",
    "means",
    "multiply",
    "everything",
    "right",
    "probability",
    "x",
    "given",
    "class",
    "k",
    "okay",
    "going",
    "go",
    "first",
    "first",
    "x",
    "way",
    "nth",
    "means",
    "every",
    "single",
    "multiplying",
    "probabilities",
    "together",
    "comes",
    "wrap",
    "oops",
    "line",
    "wrap",
    "plain",
    "english",
    "basically",
    "saying",
    "probability",
    "know",
    "category",
    "given",
    "different",
    "features",
    "proportional",
    "probability",
    "class",
    "general",
    "times",
    "probability",
    "features",
    "given",
    "one",
    "class",
    "testing",
    "probability",
    "know",
    "us",
    "playing",
    "soccer",
    "today",
    "given",
    "rainy",
    "windy",
    "wednesday",
    "proportional",
    "okay",
    "well",
    "probability",
    "play",
    "soccer",
    "anyways",
    "times",
    "probability",
    "rainy",
    "given",
    "playing",
    "soccer",
    "times",
    "probability",
    "windy",
    "given",
    "playing",
    "soccer",
    "many",
    "times",
    "playing",
    "soccer",
    "windy",
    "know",
    "many",
    "times",
    "probability",
    "wednesday",
    "given",
    "playing",
    "soccer",
    "okay",
    "use",
    "order",
    "make",
    "classification",
    "comes",
    "hat",
    "predicted",
    "going",
    "equal",
    "something",
    "called",
    "arg",
    "max",
    "expression",
    "want",
    "take",
    "arg",
    "max",
    "well",
    "want",
    "okay",
    "write",
    "means",
    "probability",
    "class",
    "ck",
    "given",
    "evidence",
    "well",
    "going",
    "take",
    "k",
    "maximizes",
    "expression",
    "right",
    "arc",
    "max",
    "means",
    "k",
    "zero",
    "oops",
    "one",
    "k",
    "many",
    "categories",
    "going",
    "go",
    "going",
    "solve",
    "expression",
    "find",
    "k",
    "makes",
    "largest",
    "okay",
    "remember",
    "instead",
    "writing",
    "formula",
    "thanks",
    "bayes",
    "rule",
    "helping",
    "us",
    "approximate",
    "right",
    "something",
    "maybe",
    "maybe",
    "like",
    "evidence",
    "answers",
    "based",
    "training",
    "set",
    "principle",
    "going",
    "finding",
    "whatever",
    "class",
    "whatever",
    "category",
    "maximizes",
    "expression",
    "right",
    "something",
    "known",
    "map",
    "short",
    "maximum",
    "posteriori",
    "pick",
    "hypothesis",
    "pick",
    "k",
    "probable",
    "minimize",
    "probability",
    "misclassification",
    "right",
    "map",
    "naive",
    "bayes",
    "back",
    "notebook",
    "like",
    "imported",
    "k",
    "nearest",
    "neighbor",
    "k",
    "neighbors",
    "classifier",
    "naive",
    "bayes",
    "go",
    "sk",
    "learn",
    "naive",
    "bayes",
    "import",
    "gaussian",
    "naive",
    "bayes",
    "right",
    "going",
    "say",
    "naive",
    "bayes",
    "model",
    "equal",
    "similar",
    "going",
    "say",
    "model",
    "going",
    "fit",
    "x",
    "train",
    "train",
    "right",
    "like",
    "might",
    "actually",
    "going",
    "set",
    "exactly",
    "like",
    "going",
    "make",
    "prediction",
    "going",
    "instead",
    "use",
    "naive",
    "bayes",
    "model",
    "course",
    "going",
    "run",
    "classification",
    "report",
    "actually",
    "going",
    "put",
    "cell",
    "new",
    "prediction",
    "test",
    "still",
    "original",
    "test",
    "data",
    "set",
    "run",
    "see",
    "okay",
    "going",
    "get",
    "worse",
    "scores",
    "right",
    "precision",
    "look",
    "slightly",
    "worse",
    "know",
    "precision",
    "recall",
    "f1",
    "score",
    "look",
    "slightly",
    "worse",
    "different",
    "categories",
    "total",
    "accuracy",
    "mean",
    "still",
    "72",
    "shabby",
    "still",
    "72",
    "okay",
    "know",
    "great",
    "okay",
    "let",
    "move",
    "logistic",
    "regression",
    "drawn",
    "plot",
    "label",
    "one",
    "axis",
    "maybe",
    "one",
    "features",
    "let",
    "say",
    "one",
    "feature",
    "case",
    "text",
    "zero",
    "right",
    "well",
    "see",
    "know",
    "one",
    "class",
    "type",
    "know",
    "one",
    "class",
    "type",
    "zero",
    "class",
    "type",
    "one",
    "okay",
    "many",
    "guys",
    "familiar",
    "regression",
    "let",
    "start",
    "draw",
    "regression",
    "line",
    "might",
    "look",
    "something",
    "like",
    "like",
    "right",
    "well",
    "seem",
    "good",
    "model",
    "like",
    "would",
    "use",
    "specific",
    "line",
    "predict",
    "right",
    "iffy",
    "okay",
    "example",
    "might",
    "say",
    "okay",
    "well",
    "seems",
    "like",
    "know",
    "everything",
    "downwards",
    "would",
    "one",
    "class",
    "type",
    "upwards",
    "would",
    "another",
    "class",
    "type",
    "look",
    "visually",
    "tell",
    "okay",
    "like",
    "line",
    "make",
    "sense",
    "things",
    "dots",
    "along",
    "line",
    "reason",
    "classification",
    "regression",
    "okay",
    "well",
    "first",
    "let",
    "start",
    "know",
    "model",
    "use",
    "line",
    "equals",
    "whatever",
    "let",
    "say",
    "x",
    "plus",
    "b",
    "intercept",
    "right",
    "slope",
    "use",
    "linear",
    "regression",
    "actually",
    "hat",
    "right",
    "working",
    "linear",
    "regression",
    "actually",
    "estimating",
    "model",
    "probability",
    "probability",
    "zero",
    "one",
    "class",
    "zero",
    "class",
    "one",
    "let",
    "rewrite",
    "p",
    "equals",
    "x",
    "plus",
    "okay",
    "well",
    "x",
    "plus",
    "b",
    "range",
    "know",
    "negative",
    "infinity",
    "infinity",
    "right",
    "value",
    "x",
    "goes",
    "negative",
    "infinity",
    "infinity",
    "probability",
    "know",
    "probably",
    "one",
    "rules",
    "probability",
    "probability",
    "stay",
    "zero",
    "one",
    "fix",
    "well",
    "maybe",
    "instead",
    "setting",
    "probability",
    "equal",
    "set",
    "odds",
    "equal",
    "mean",
    "okay",
    "let",
    "probability",
    "divided",
    "one",
    "minus",
    "probability",
    "okay",
    "becomes",
    "ratio",
    "ratio",
    "allowed",
    "take",
    "infinite",
    "values",
    "still",
    "one",
    "issue",
    "let",
    "move",
    "bit",
    "one",
    "issue",
    "x",
    "plus",
    "b",
    "still",
    "negative",
    "right",
    "like",
    "know",
    "negative",
    "slope",
    "negative",
    "b",
    "negative",
    "x",
    "know",
    "allowed",
    "negative",
    "fix",
    "actually",
    "taking",
    "log",
    "odds",
    "okay",
    "log",
    "know",
    "probability",
    "divided",
    "one",
    "minus",
    "probability",
    "range",
    "negative",
    "infinity",
    "infinity",
    "good",
    "range",
    "log",
    "negative",
    "infinity",
    "infinity",
    "solve",
    "p",
    "probability",
    "well",
    "first",
    "thing",
    "take",
    "know",
    "remove",
    "log",
    "taking",
    "e",
    "whatever",
    "sides",
    "gives",
    "probability",
    "one",
    "minus",
    "probability",
    "equal",
    "e",
    "x",
    "plus",
    "okay",
    "let",
    "multiply",
    "probability",
    "equal",
    "one",
    "minus",
    "probability",
    "e",
    "x",
    "plus",
    "p",
    "equal",
    "e",
    "x",
    "plus",
    "b",
    "minus",
    "p",
    "times",
    "e",
    "x",
    "plus",
    "move",
    "like",
    "terms",
    "one",
    "side",
    "p",
    "basically",
    "moving",
    "adding",
    "p",
    "one",
    "plus",
    "e",
    "x",
    "plus",
    "b",
    "equal",
    "e",
    "x",
    "plus",
    "b",
    "let",
    "change",
    "parentheses",
    "make",
    "little",
    "bigger",
    "probability",
    "e",
    "x",
    "plus",
    "b",
    "divided",
    "one",
    "plus",
    "e",
    "x",
    "plus",
    "okay",
    "well",
    "let",
    "rewrite",
    "really",
    "quickly",
    "want",
    "numerator",
    "one",
    "top",
    "okay",
    "going",
    "going",
    "multiply",
    "negative",
    "x",
    "plus",
    "b",
    "also",
    "bottom",
    "negative",
    "x",
    "plus",
    "b",
    "allowed",
    "one",
    "probability",
    "equal",
    "one",
    "one",
    "plus",
    "e",
    "negative",
    "x",
    "plus",
    "rewrite",
    "like",
    "actually",
    "form",
    "special",
    "function",
    "called",
    "sigmoid",
    "function",
    "sigmoid",
    "function",
    "looks",
    "something",
    "like",
    "x",
    "sigmoid",
    "know",
    "x",
    "equal",
    "one",
    "one",
    "plus",
    "e",
    "negative",
    "essentially",
    "rewrite",
    "sigmoid",
    "function",
    "x",
    "value",
    "actually",
    "x",
    "plus",
    "maybe",
    "change",
    "make",
    "bit",
    "clear",
    "matter",
    "variable",
    "name",
    "sigmoid",
    "function",
    "visually",
    "sigmoid",
    "function",
    "looks",
    "like",
    "goes",
    "zero",
    "zero",
    "one",
    "looks",
    "something",
    "like",
    "curved",
    "draw",
    "well",
    "let",
    "try",
    "hard",
    "draw",
    "something",
    "draw",
    "right",
    "like",
    "okay",
    "goes",
    "zero",
    "one",
    "might",
    "notice",
    "form",
    "fits",
    "shape",
    "oops",
    "let",
    "draw",
    "sharper",
    "shape",
    "lot",
    "better",
    "right",
    "alright",
    "call",
    "logistic",
    "regression",
    "basically",
    "trying",
    "fit",
    "data",
    "sigmoid",
    "function",
    "okay",
    "know",
    "one",
    "data",
    "point",
    "one",
    "feature",
    "x",
    "call",
    "simple",
    "logistic",
    "regression",
    "know",
    "x",
    "zero",
    "x",
    "zero",
    "x",
    "one",
    "way",
    "x",
    "n",
    "call",
    "multiple",
    "logistic",
    "regression",
    "multiple",
    "features",
    "considering",
    "building",
    "model",
    "logistic",
    "regression",
    "going",
    "put",
    "sk",
    "learn",
    "linear",
    "model",
    "import",
    "logistic",
    "regression",
    "right",
    "like",
    "repeat",
    "instead",
    "nb",
    "going",
    "call",
    "log",
    "model",
    "lg",
    "logistic",
    "regression",
    "going",
    "change",
    "logistic",
    "regression",
    "going",
    "use",
    "default",
    "logistic",
    "regression",
    "actually",
    "look",
    "see",
    "use",
    "different",
    "penalties",
    "right",
    "using",
    "l2",
    "penalty",
    "l2",
    "quadratic",
    "formula",
    "okay",
    "means",
    "know",
    "outliers",
    "would",
    "really",
    "penalize",
    "things",
    "know",
    "toggle",
    "different",
    "parameters",
    "might",
    "get",
    "slightly",
    "different",
    "results",
    "building",
    "production",
    "level",
    "logistic",
    "regression",
    "model",
    "would",
    "want",
    "go",
    "would",
    "want",
    "figure",
    "going",
    "go",
    "ahead",
    "going",
    "go",
    "ahead",
    "would",
    "want",
    "figure",
    "know",
    "best",
    "parameters",
    "pass",
    "based",
    "validation",
    "data",
    "use",
    "box",
    "going",
    "fit",
    "x",
    "train",
    "train",
    "going",
    "predict",
    "call",
    "instead",
    "lg",
    "nb",
    "going",
    "use",
    "lg",
    "decent",
    "precision",
    "65",
    "recall",
    "71",
    "f",
    "168",
    "82",
    "total",
    "accuracy",
    "okay",
    "performs",
    "slightly",
    "better",
    "base",
    "still",
    "good",
    "k",
    "alright",
    "last",
    "model",
    "classification",
    "wanted",
    "talk",
    "something",
    "called",
    "support",
    "vector",
    "machines",
    "svms",
    "short",
    "exactly",
    "svm",
    "model",
    "two",
    "different",
    "features",
    "x",
    "zero",
    "x",
    "one",
    "axes",
    "told",
    "know",
    "class",
    "zero",
    "class",
    "one",
    "based",
    "blue",
    "red",
    "labels",
    "goal",
    "find",
    "sort",
    "line",
    "two",
    "labels",
    "best",
    "divides",
    "data",
    "alright",
    "line",
    "svm",
    "model",
    "call",
    "line",
    "2d",
    "line",
    "3d",
    "would",
    "plane",
    "also",
    "dimensions",
    "proper",
    "term",
    "actually",
    "want",
    "find",
    "hyperplane",
    "best",
    "differentiates",
    "two",
    "classes",
    "let",
    "see",
    "examples",
    "okay",
    "first",
    "three",
    "lines",
    "let",
    "say",
    "b",
    "c",
    "c",
    "one",
    "best",
    "divider",
    "data",
    "one",
    "know",
    "data",
    "one",
    "side",
    "least",
    "one",
    "divides",
    "right",
    "like",
    "one",
    "defined",
    "boundary",
    "two",
    "different",
    "groups",
    "question",
    "pretty",
    "straightforward",
    "right",
    "clear",
    "distinct",
    "line",
    "know",
    "everything",
    "side",
    "one",
    "label",
    "negative",
    "everything",
    "side",
    "label",
    "positive",
    "drawn",
    "b",
    "like",
    "c",
    "maybe",
    "like",
    "sorry",
    "kind",
    "labels",
    "kind",
    "close",
    "together",
    "one",
    "best",
    "would",
    "argue",
    "still",
    "right",
    "still",
    "right",
    "still",
    "two",
    "look",
    "close",
    "points",
    "right",
    "new",
    "point",
    "wanted",
    "estimate",
    "okay",
    "say",
    "let",
    "say",
    "working",
    "let",
    "say",
    "new",
    "point",
    "right",
    "maybe",
    "new",
    "point",
    "right",
    "well",
    "seems",
    "like",
    "logically",
    "looking",
    "mean",
    "without",
    "boundary",
    "would",
    "probably",
    "go",
    "positives",
    "right",
    "mean",
    "pretty",
    "close",
    "positive",
    "one",
    "thing",
    "care",
    "svm",
    "something",
    "known",
    "margin",
    "okay",
    "want",
    "separate",
    "two",
    "classes",
    "really",
    "well",
    "also",
    "care",
    "boundary",
    "points",
    "classes",
    "data",
    "set",
    "line",
    "drawing",
    "line",
    "like",
    "closest",
    "values",
    "line",
    "might",
    "like",
    "trying",
    "draw",
    "perpendicular",
    "right",
    "effectively",
    "switch",
    "dotted",
    "lines",
    "draw",
    "right",
    "effectively",
    "known",
    "margins",
    "okay",
    "margins",
    "svms",
    "goal",
    "maximize",
    "margins",
    "want",
    "line",
    "best",
    "separates",
    "two",
    "different",
    "classes",
    "want",
    "line",
    "largest",
    "margin",
    "data",
    "points",
    "lie",
    "margin",
    "lines",
    "data",
    "basically",
    "data",
    "points",
    "helping",
    "us",
    "define",
    "divider",
    "call",
    "support",
    "vectors",
    "hence",
    "name",
    "support",
    "vector",
    "machines",
    "okay",
    "issue",
    "svm",
    "sometimes",
    "robust",
    "outliers",
    "right",
    "example",
    "one",
    "outlier",
    "like",
    "would",
    "totally",
    "change",
    "want",
    "support",
    "vector",
    "even",
    "though",
    "might",
    "outlier",
    "okay",
    "something",
    "keep",
    "mind",
    "know",
    "working",
    "svm",
    "might",
    "best",
    "model",
    "outliers",
    "data",
    "set",
    "okay",
    "another",
    "example",
    "svms",
    "might",
    "let",
    "say",
    "data",
    "like",
    "going",
    "use",
    "one",
    "dimensional",
    "data",
    "set",
    "example",
    "let",
    "say",
    "data",
    "set",
    "looks",
    "like",
    "well",
    "know",
    "separators",
    "perpendicular",
    "line",
    "somewhere",
    "along",
    "line",
    "could",
    "anywhere",
    "like",
    "might",
    "argue",
    "okay",
    "well",
    "one",
    "could",
    "also",
    "draw",
    "another",
    "one",
    "right",
    "maybe",
    "two",
    "svms",
    "really",
    "svms",
    "work",
    "one",
    "thing",
    "create",
    "sort",
    "projection",
    "realize",
    "one",
    "thing",
    "forgot",
    "label",
    "zero",
    "let",
    "say",
    "zero",
    "going",
    "going",
    "say",
    "okay",
    "going",
    "x",
    "going",
    "x",
    "sorry",
    "x",
    "zero",
    "x",
    "one",
    "x",
    "zero",
    "going",
    "original",
    "going",
    "make",
    "x",
    "one",
    "equal",
    "let",
    "say",
    "x",
    "squared",
    "whatever",
    "squared",
    "right",
    "natives",
    "would",
    "know",
    "maybe",
    "somewhere",
    "pretend",
    "somewhere",
    "right",
    "pluses",
    "might",
    "something",
    "like",
    "going",
    "run",
    "space",
    "going",
    "draw",
    "together",
    "use",
    "imagination",
    "draw",
    "like",
    "well",
    "lot",
    "easier",
    "apply",
    "boundary",
    "right",
    "svm",
    "could",
    "maybe",
    "something",
    "like",
    "see",
    "divided",
    "data",
    "set",
    "separable",
    "one",
    "class",
    "way",
    "class",
    "way",
    "okay",
    "known",
    "svms",
    "highly",
    "suggest",
    "know",
    "models",
    "mentioned",
    "interested",
    "go",
    "depth",
    "mathematically",
    "like",
    "find",
    "hyperplane",
    "right",
    "going",
    "go",
    "specific",
    "course",
    "learning",
    "svm",
    "good",
    "idea",
    "know",
    "oh",
    "okay",
    "technique",
    "behind",
    "finding",
    "know",
    "exactly",
    "define",
    "hyperplane",
    "going",
    "use",
    "anyways",
    "transformation",
    "known",
    "kernel",
    "trick",
    "go",
    "x",
    "coordinate",
    "x",
    "x",
    "squared",
    "applying",
    "kernel",
    "called",
    "kernel",
    "trick",
    "svms",
    "actually",
    "really",
    "powerful",
    "see",
    "sk",
    "going",
    "import",
    "svc",
    "svc",
    "support",
    "vector",
    "classifier",
    "svm",
    "model",
    "going",
    "know",
    "create",
    "svc",
    "model",
    "going",
    "fit",
    "x",
    "train",
    "could",
    "copied",
    "pasted",
    "able",
    "going",
    "create",
    "svc",
    "fit",
    "x",
    "train",
    "could",
    "copied",
    "pasted",
    "probably",
    "done",
    "okay",
    "taking",
    "bit",
    "longer",
    "right",
    "let",
    "predict",
    "using",
    "rsvm",
    "model",
    "let",
    "see",
    "hover",
    "right",
    "see",
    "lot",
    "different",
    "parameters",
    "go",
    "back",
    "change",
    "creating",
    "production",
    "level",
    "model",
    "okay",
    "specific",
    "case",
    "use",
    "box",
    "make",
    "predictions",
    "note",
    "wow",
    "accuracy",
    "actually",
    "jumps",
    "87",
    "svm",
    "even",
    "class",
    "zero",
    "nothing",
    "less",
    "know",
    "point",
    "eight",
    "great",
    "class",
    "one",
    "mean",
    "everything",
    "higher",
    "anything",
    "seen",
    "point",
    "far",
    "gone",
    "four",
    "different",
    "classification",
    "models",
    "done",
    "svm",
    "logistic",
    "regression",
    "naive",
    "bayes",
    "cannon",
    "simple",
    "ways",
    "implement",
    "different",
    "know",
    "different",
    "hyper",
    "parameters",
    "go",
    "toggle",
    "try",
    "see",
    "helps",
    "later",
    "part",
    "perform",
    "give",
    "us",
    "around",
    "70",
    "80",
    "accuracy",
    "okay",
    "svm",
    "best",
    "let",
    "see",
    "actually",
    "beat",
    "using",
    "neural",
    "net",
    "final",
    "type",
    "model",
    "wanted",
    "talk",
    "known",
    "neural",
    "net",
    "neural",
    "network",
    "neural",
    "nets",
    "look",
    "something",
    "like",
    "input",
    "layer",
    "features",
    "would",
    "go",
    "arrows",
    "pointing",
    "sort",
    "hidden",
    "layer",
    "arrows",
    "point",
    "sort",
    "output",
    "layer",
    "mean",
    "layers",
    "something",
    "known",
    "neuron",
    "okay",
    "neuron",
    "neural",
    "net",
    "features",
    "inputting",
    "neural",
    "net",
    "might",
    "x",
    "zero",
    "x",
    "one",
    "way",
    "x",
    "right",
    "features",
    "talked",
    "might",
    "know",
    "pregnancy",
    "bmi",
    "age",
    "etc",
    "get",
    "weighted",
    "value",
    "multiplied",
    "w",
    "number",
    "applies",
    "one",
    "specific",
    "category",
    "one",
    "specific",
    "feature",
    "two",
    "get",
    "multiplied",
    "sum",
    "goes",
    "neuron",
    "okay",
    "basically",
    "taking",
    "w",
    "zero",
    "times",
    "x",
    "zero",
    "adding",
    "x",
    "one",
    "times",
    "w",
    "one",
    "adding",
    "know",
    "x",
    "two",
    "times",
    "w",
    "two",
    "etc",
    "way",
    "x",
    "n",
    "times",
    "w",
    "getting",
    "input",
    "neuron",
    "also",
    "adding",
    "bias",
    "term",
    "means",
    "okay",
    "might",
    "want",
    "shift",
    "little",
    "bit",
    "might",
    "add",
    "five",
    "might",
    "add",
    "might",
    "subtract",
    "100",
    "know",
    "going",
    "add",
    "bias",
    "term",
    "output",
    "things",
    "sum",
    "go",
    "something",
    "known",
    "activation",
    "function",
    "okay",
    "applying",
    "activation",
    "function",
    "get",
    "output",
    "neuron",
    "would",
    "look",
    "like",
    "whole",
    "network",
    "would",
    "look",
    "something",
    "like",
    "kind",
    "gloss",
    "activation",
    "function",
    "exactly",
    "neural",
    "net",
    "looks",
    "like",
    "inputs",
    "let",
    "say",
    "arrows",
    "represent",
    "sort",
    "addition",
    "right",
    "going",
    "adding",
    "bunch",
    "times",
    "right",
    "adding",
    "sort",
    "weight",
    "times",
    "input",
    "layer",
    "bunch",
    "times",
    "go",
    "back",
    "factor",
    "entire",
    "neural",
    "net",
    "linear",
    "combination",
    "input",
    "layers",
    "know",
    "seems",
    "kind",
    "useless",
    "right",
    "could",
    "literally",
    "write",
    "formula",
    "would",
    "need",
    "set",
    "entire",
    "neural",
    "network",
    "would",
    "activation",
    "function",
    "introduced",
    "right",
    "without",
    "activation",
    "function",
    "becomes",
    "linear",
    "model",
    "activation",
    "function",
    "might",
    "look",
    "something",
    "like",
    "tell",
    "linear",
    "reason",
    "introduce",
    "entire",
    "model",
    "collapse",
    "become",
    "linear",
    "model",
    "something",
    "known",
    "sigmoid",
    "function",
    "runs",
    "zero",
    "one",
    "tanh",
    "runs",
    "negative",
    "one",
    "way",
    "one",
    "relu",
    "anything",
    "less",
    "zero",
    "zero",
    "anything",
    "greater",
    "zero",
    "linear",
    "activation",
    "functions",
    "every",
    "single",
    "output",
    "neuron",
    "longer",
    "linear",
    "combination",
    "sort",
    "altered",
    "linear",
    "state",
    "means",
    "input",
    "next",
    "neuron",
    "know",
    "collapse",
    "become",
    "linear",
    "introduced",
    "nonlinearities",
    "training",
    "set",
    "model",
    "loss",
    "right",
    "thing",
    "called",
    "training",
    "feed",
    "loss",
    "back",
    "model",
    "make",
    "certain",
    "adjustments",
    "model",
    "improve",
    "predicted",
    "output",
    "let",
    "talk",
    "little",
    "bit",
    "training",
    "exactly",
    "goes",
    "step",
    "let",
    "go",
    "back",
    "take",
    "look",
    "l2",
    "loss",
    "function",
    "l2",
    "loss",
    "function",
    "looks",
    "like",
    "quadratic",
    "formula",
    "right",
    "well",
    "error",
    "really",
    "really",
    "really",
    "really",
    "large",
    "goal",
    "get",
    "somewhere",
    "loss",
    "decreased",
    "right",
    "means",
    "predicted",
    "value",
    "closer",
    "true",
    "value",
    "means",
    "want",
    "go",
    "way",
    "okay",
    "thanks",
    "lot",
    "properties",
    "math",
    "something",
    "called",
    "gradient",
    "descent",
    "order",
    "follow",
    "slope",
    "way",
    "quadratic",
    "different",
    "different",
    "slopes",
    "respect",
    "value",
    "okay",
    "loss",
    "respect",
    "weight",
    "w",
    "zero",
    "versus",
    "w",
    "one",
    "versus",
    "w",
    "n",
    "might",
    "different",
    "right",
    "way",
    "kind",
    "think",
    "extent",
    "value",
    "contributing",
    "loss",
    "actually",
    "figure",
    "calculus",
    "going",
    "touch",
    "specific",
    "course",
    "want",
    "learn",
    "neural",
    "nets",
    "probably",
    "also",
    "learn",
    "calculus",
    "figure",
    "exactly",
    "back",
    "propagation",
    "order",
    "actually",
    "calculate",
    "know",
    "much",
    "backstep",
    "thing",
    "might",
    "notice",
    "follows",
    "curve",
    "different",
    "points",
    "closer",
    "get",
    "bottom",
    "smaller",
    "step",
    "becomes",
    "stick",
    "new",
    "value",
    "call",
    "weight",
    "update",
    "going",
    "take",
    "w",
    "zero",
    "going",
    "set",
    "new",
    "value",
    "w",
    "zero",
    "going",
    "set",
    "old",
    "value",
    "w",
    "zero",
    "plus",
    "factor",
    "call",
    "alpha",
    "times",
    "whatever",
    "arrow",
    "basically",
    "saying",
    "okay",
    "take",
    "old",
    "w",
    "zero",
    "old",
    "weight",
    "decrease",
    "way",
    "guess",
    "increase",
    "direction",
    "right",
    "like",
    "take",
    "step",
    "direction",
    "alpha",
    "telling",
    "us",
    "okay",
    "take",
    "huge",
    "step",
    "right",
    "case",
    "wrong",
    "take",
    "small",
    "step",
    "take",
    "small",
    "step",
    "direction",
    "see",
    "get",
    "closer",
    "know",
    "want",
    "look",
    "mathematics",
    "things",
    "reason",
    "use",
    "plus",
    "negative",
    "gradient",
    "right",
    "use",
    "actual",
    "gradient",
    "minus",
    "alpha",
    "something",
    "call",
    "learning",
    "rate",
    "okay",
    "adjusts",
    "quickly",
    "taking",
    "steps",
    "might",
    "know",
    "tell",
    "ultimately",
    "control",
    "long",
    "takes",
    "neural",
    "net",
    "converge",
    "sometimes",
    "set",
    "high",
    "might",
    "even",
    "diverge",
    "weights",
    "w",
    "zero",
    "w",
    "one",
    "w",
    "make",
    "update",
    "calculate",
    "loss",
    "gradient",
    "loss",
    "respect",
    "weight",
    "back",
    "propagation",
    "works",
    "everything",
    "going",
    "calculate",
    "loss",
    "calculating",
    "gradients",
    "making",
    "adjustments",
    "model",
    "setting",
    "weights",
    "something",
    "adjusted",
    "slightly",
    "going",
    "calculate",
    "gradient",
    "saying",
    "okay",
    "let",
    "take",
    "training",
    "set",
    "run",
    "model",
    "go",
    "loop",
    "machine",
    "learning",
    "already",
    "seen",
    "libraries",
    "use",
    "right",
    "already",
    "seen",
    "sk",
    "learn",
    "start",
    "going",
    "neural",
    "networks",
    "kind",
    "trying",
    "program",
    "fun",
    "try",
    "scratch",
    "probably",
    "lot",
    "bugs",
    "also",
    "probably",
    "going",
    "fast",
    "enough",
    "right",
    "would",
    "great",
    "know",
    "full",
    "time",
    "professionals",
    "dedicated",
    "solving",
    "problem",
    "could",
    "literally",
    "give",
    "us",
    "code",
    "already",
    "running",
    "really",
    "fast",
    "well",
    "answer",
    "yes",
    "exists",
    "use",
    "tensorflow",
    "tensorflow",
    "makes",
    "really",
    "easy",
    "define",
    "models",
    "also",
    "enough",
    "control",
    "exactly",
    "feeding",
    "model",
    "example",
    "line",
    "basically",
    "saying",
    "okay",
    "let",
    "create",
    "sequential",
    "neural",
    "net",
    "sequential",
    "know",
    "seen",
    "goes",
    "one",
    "layer",
    "next",
    "dense",
    "layer",
    "means",
    "dense",
    "layer",
    "means",
    "interconnected",
    "interconnected",
    "nodes",
    "one",
    "one",
    "gets",
    "connected",
    "next",
    "ones",
    "going",
    "create",
    "16",
    "dense",
    "nodes",
    "relu",
    "activation",
    "functions",
    "going",
    "create",
    "another",
    "layer",
    "16",
    "dense",
    "nodes",
    "relu",
    "activation",
    "output",
    "layer",
    "going",
    "one",
    "node",
    "okay",
    "easy",
    "define",
    "something",
    "tensorflow",
    "tensorflow",
    "open",
    "source",
    "library",
    "helps",
    "develop",
    "train",
    "ml",
    "models",
    "let",
    "implement",
    "neural",
    "net",
    "using",
    "neural",
    "net",
    "classification",
    "neural",
    "net",
    "model",
    "going",
    "use",
    "tensorflow",
    "think",
    "imported",
    "going",
    "import",
    "going",
    "import",
    "tensorflow",
    "tf",
    "enter",
    "cool",
    "neural",
    "net",
    "model",
    "going",
    "going",
    "use",
    "essentially",
    "saying",
    "layer",
    "things",
    "pass",
    "yeah",
    "layer",
    "linear",
    "stack",
    "layers",
    "layer",
    "model",
    "means",
    "nope",
    "means",
    "pass",
    "sort",
    "layer",
    "going",
    "use",
    "dense",
    "layer",
    "oops",
    "dot",
    "dense",
    "let",
    "say",
    "32",
    "units",
    "okay",
    "also",
    "set",
    "activation",
    "really",
    "first",
    "specify",
    "input",
    "shape",
    "10",
    "comma",
    "alright",
    "alright",
    "first",
    "layer",
    "next",
    "layer",
    "going",
    "another",
    "dense",
    "layer",
    "32",
    "units",
    "using",
    "relu",
    "final",
    "layer",
    "going",
    "output",
    "layer",
    "going",
    "one",
    "node",
    "activation",
    "going",
    "sigmoid",
    "recall",
    "logistic",
    "regression",
    "happened",
    "sigmoid",
    "looks",
    "something",
    "like",
    "right",
    "creating",
    "sigmoid",
    "activation",
    "last",
    "layer",
    "essentially",
    "projecting",
    "predictions",
    "zero",
    "one",
    "like",
    "logistic",
    "regression",
    "going",
    "help",
    "us",
    "know",
    "round",
    "zero",
    "one",
    "classify",
    "way",
    "okay",
    "neural",
    "net",
    "model",
    "going",
    "compile",
    "tensorflow",
    "compile",
    "really",
    "cool",
    "literally",
    "pass",
    "type",
    "optimizer",
    "want",
    "go",
    "optimizers",
    "actually",
    "going",
    "use",
    "atom",
    "see",
    "know",
    "learning",
    "rate",
    "going",
    "use",
    "default",
    "loss",
    "going",
    "binary",
    "cross",
    "entropy",
    "metrics",
    "also",
    "going",
    "include",
    "already",
    "consider",
    "loss",
    "also",
    "going",
    "tack",
    "accuracy",
    "actually",
    "see",
    "plot",
    "later",
    "alright",
    "going",
    "run",
    "one",
    "thing",
    "going",
    "also",
    "going",
    "define",
    "plot",
    "definitions",
    "actually",
    "copying",
    "pasting",
    "got",
    "tensorflow",
    "go",
    "tensorflow",
    "tutorial",
    "actually",
    "like",
    "defined",
    "exactly",
    "actually",
    "going",
    "move",
    "cell",
    "run",
    "basically",
    "plotting",
    "loss",
    "different",
    "epochs",
    "epochs",
    "means",
    "like",
    "training",
    "cycles",
    "going",
    "run",
    "means",
    "like",
    "training",
    "cycles",
    "going",
    "plot",
    "accuracy",
    "epochs",
    "alright",
    "model",
    "left",
    "let",
    "train",
    "okay",
    "going",
    "say",
    "history",
    "tensorflow",
    "great",
    "keeps",
    "track",
    "history",
    "training",
    "go",
    "plot",
    "later",
    "going",
    "set",
    "equal",
    "neural",
    "net",
    "model",
    "fit",
    "x",
    "train",
    "train",
    "going",
    "make",
    "number",
    "epochs",
    "equal",
    "let",
    "say",
    "let",
    "use",
    "100",
    "batch",
    "size",
    "going",
    "set",
    "equal",
    "let",
    "say",
    "alright",
    "validation",
    "split",
    "validation",
    "split",
    "somewhere",
    "okay",
    "yeah",
    "validation",
    "split",
    "fraction",
    "training",
    "data",
    "used",
    "validation",
    "data",
    "essentially",
    "every",
    "single",
    "epoch",
    "going",
    "tensorflow",
    "saying",
    "leave",
    "certain",
    "point",
    "two",
    "leave",
    "20",
    "going",
    "test",
    "model",
    "performs",
    "20",
    "left",
    "okay",
    "basically",
    "like",
    "validation",
    "data",
    "set",
    "tensorflow",
    "training",
    "data",
    "set",
    "training",
    "measure",
    "outside",
    "validation",
    "data",
    "set",
    "see",
    "know",
    "going",
    "validation",
    "split",
    "going",
    "make",
    "run",
    "run",
    "right",
    "actually",
    "going",
    "set",
    "verbose",
    "equal",
    "zero",
    "means",
    "okay",
    "print",
    "anything",
    "printing",
    "something",
    "100",
    "epochs",
    "might",
    "get",
    "kind",
    "annoying",
    "going",
    "let",
    "run",
    "let",
    "train",
    "see",
    "happens",
    "cool",
    "finished",
    "training",
    "know",
    "already",
    "defined",
    "two",
    "functions",
    "go",
    "ahead",
    "plot",
    "loss",
    "oops",
    "loss",
    "history",
    "also",
    "plot",
    "accuracy",
    "throughout",
    "training",
    "little",
    "bit",
    "ish",
    "looking",
    "definitely",
    "looking",
    "steadily",
    "decreasing",
    "loss",
    "increasing",
    "accuracy",
    "see",
    "know",
    "validation",
    "accuracy",
    "improves",
    "around",
    "point",
    "seven",
    "seven",
    "something",
    "way",
    "somewhere",
    "around",
    "point",
    "maybe",
    "eight",
    "one",
    "loss",
    "decreasing",
    "good",
    "expected",
    "validation",
    "loss",
    "accuracy",
    "performing",
    "worse",
    "training",
    "loss",
    "accuracy",
    "model",
    "training",
    "data",
    "adapting",
    "data",
    "whereas",
    "validation",
    "stuff",
    "know",
    "stuff",
    "seen",
    "yet",
    "machine",
    "learning",
    "saw",
    "could",
    "change",
    "bunch",
    "parameters",
    "right",
    "like",
    "could",
    "change",
    "row",
    "64",
    "nodes",
    "32",
    "one",
    "change",
    "parameters",
    "lot",
    "machine",
    "learning",
    "trying",
    "find",
    "hey",
    "set",
    "hyper",
    "parameters",
    "actually",
    "going",
    "going",
    "rewrite",
    "something",
    "known",
    "grid",
    "search",
    "search",
    "entire",
    "space",
    "hey",
    "happens",
    "know",
    "64",
    "nodes",
    "64",
    "nodes",
    "16",
    "nodes",
    "16",
    "nodes",
    "top",
    "know",
    "change",
    "learning",
    "rate",
    "change",
    "many",
    "epochs",
    "change",
    "know",
    "batch",
    "size",
    "things",
    "might",
    "affect",
    "training",
    "kicks",
    "also",
    "going",
    "add",
    "known",
    "dropout",
    "layer",
    "dropout",
    "saying",
    "hey",
    "randomly",
    "choose",
    "rate",
    "certain",
    "nodes",
    "train",
    "know",
    "certain",
    "iteration",
    "helps",
    "prevent",
    "overfitting",
    "okay",
    "actually",
    "going",
    "define",
    "function",
    "called",
    "train",
    "model",
    "going",
    "pass",
    "x",
    "train",
    "train",
    "number",
    "nodes",
    "dropout",
    "know",
    "probability",
    "talked",
    "learning",
    "rate",
    "actually",
    "going",
    "say",
    "lr",
    "batch",
    "size",
    "also",
    "pass",
    "number",
    "epochs",
    "right",
    "mentioned",
    "parameter",
    "indent",
    "goes",
    "two",
    "going",
    "set",
    "equal",
    "number",
    "nodes",
    "two",
    "dropout",
    "layers",
    "going",
    "set",
    "dropout",
    "prob",
    "know",
    "probability",
    "turning",
    "node",
    "training",
    "equal",
    "dropout",
    "prob",
    "going",
    "keep",
    "output",
    "layer",
    "compiling",
    "going",
    "learning",
    "rate",
    "still",
    "want",
    "binary",
    "cross",
    "entropy",
    "accuracy",
    "actually",
    "going",
    "train",
    "model",
    "inside",
    "function",
    "epochs",
    "equal",
    "epochs",
    "equal",
    "whatever",
    "know",
    "passing",
    "x",
    "train",
    "train",
    "belong",
    "right",
    "okay",
    "getting",
    "passed",
    "well",
    "finally",
    "end",
    "going",
    "return",
    "model",
    "history",
    "model",
    "okay",
    "let",
    "go",
    "let",
    "say",
    "let",
    "keep",
    "epochs",
    "say",
    "hey",
    "number",
    "nodes",
    "let",
    "say",
    "let",
    "1632",
    "64",
    "see",
    "happens",
    "different",
    "dropout",
    "probabilities",
    "mean",
    "zero",
    "would",
    "nothing",
    "let",
    "use",
    "also",
    "see",
    "happens",
    "know",
    "learning",
    "rate",
    "know",
    "maybe",
    "want",
    "throw",
    "well",
    "batch",
    "size",
    "let",
    "1632",
    "64",
    "well",
    "actually",
    "let",
    "also",
    "throw",
    "actually",
    "let",
    "get",
    "rid",
    "sorry",
    "128",
    "going",
    "record",
    "model",
    "history",
    "using",
    "train",
    "model",
    "going",
    "x",
    "train",
    "train",
    "number",
    "nodes",
    "going",
    "know",
    "number",
    "nodes",
    "defined",
    "dropout",
    "prob",
    "lr",
    "batch",
    "size",
    "epochs",
    "okay",
    "model",
    "history",
    "going",
    "want",
    "plot",
    "loss",
    "history",
    "also",
    "going",
    "plot",
    "accuracy",
    "probably",
    "done",
    "side",
    "side",
    "probably",
    "would",
    "easier",
    "okay",
    "going",
    "split",
    "split",
    "subplots",
    "saying",
    "okay",
    "want",
    "one",
    "row",
    "two",
    "columns",
    "row",
    "plots",
    "okay",
    "going",
    "plot",
    "axis",
    "one",
    "loss",
    "actually",
    "know",
    "going",
    "work",
    "okay",
    "care",
    "grid",
    "yeah",
    "let",
    "let",
    "keep",
    "grid",
    "going",
    "plot",
    "accuracies",
    "second",
    "plot",
    "might",
    "debug",
    "bit",
    "able",
    "get",
    "rid",
    "run",
    "already",
    "history",
    "saved",
    "variable",
    "run",
    "okay",
    "attribute",
    "x",
    "label",
    "oh",
    "think",
    "like",
    "set",
    "x",
    "label",
    "something",
    "okay",
    "yeah",
    "set",
    "instead",
    "x",
    "label",
    "label",
    "let",
    "see",
    "works",
    "right",
    "cool",
    "um",
    "let",
    "actually",
    "make",
    "bit",
    "larger",
    "okay",
    "actually",
    "change",
    "figure",
    "size",
    "gon",
    "na",
    "set",
    "let",
    "see",
    "happens",
    "set",
    "oh",
    "way",
    "wanted",
    "okay",
    "looks",
    "reasonable",
    "going",
    "plot",
    "history",
    "function",
    "plot",
    "side",
    "side",
    "going",
    "plot",
    "history",
    "actually",
    "going",
    "first",
    "going",
    "print",
    "parameters",
    "going",
    "print",
    "f",
    "string",
    "print",
    "stuff",
    "going",
    "print",
    "parameters",
    "uh",
    "stuff",
    "printing",
    "many",
    "nodes",
    "um",
    "dropout",
    "probability",
    "uh",
    "learning",
    "rate",
    "already",
    "know",
    "many",
    "found",
    "even",
    "going",
    "bother",
    "plot",
    "uh",
    "let",
    "actually",
    "also",
    "figure",
    "um",
    "validation",
    "losses",
    "validation",
    "set",
    "created",
    "way",
    "back",
    "alright",
    "remember",
    "created",
    "three",
    "data",
    "sets",
    "let",
    "call",
    "model",
    "evaluate",
    "validation",
    "data",
    "validation",
    "data",
    "sets",
    "loss",
    "would",
    "actually",
    "want",
    "record",
    "let",
    "say",
    "want",
    "record",
    "whatever",
    "model",
    "least",
    "validation",
    "loss",
    "first",
    "going",
    "initialize",
    "infinity",
    "know",
    "model",
    "beat",
    "score",
    "float",
    "infinity",
    "set",
    "infinity",
    "maybe",
    "keep",
    "track",
    "parameters",
    "actually",
    "really",
    "matter",
    "going",
    "keep",
    "track",
    "model",
    "gon",
    "na",
    "set",
    "none",
    "validation",
    "loss",
    "ever",
    "less",
    "least",
    "validation",
    "loss",
    "going",
    "simply",
    "come",
    "say",
    "hey",
    "validation",
    "least",
    "validation",
    "loss",
    "equal",
    "validation",
    "loss",
    "least",
    "loss",
    "model",
    "whatever",
    "model",
    "earned",
    "validation",
    "loss",
    "okay",
    "actually",
    "going",
    "let",
    "run",
    "going",
    "get",
    "least",
    "last",
    "model",
    "let",
    "run",
    "right",
    "wait",
    "right",
    "finally",
    "finished",
    "training",
    "notice",
    "okay",
    "loss",
    "actually",
    "gets",
    "like",
    "accuracy",
    "around",
    "88",
    "pretty",
    "good",
    "might",
    "wondering",
    "okay",
    "accuracy",
    "like",
    "validation",
    "accuracy",
    "validation",
    "data",
    "set",
    "defined",
    "beginning",
    "right",
    "one",
    "actually",
    "taking",
    "20",
    "tests",
    "training",
    "set",
    "every",
    "time",
    "training",
    "saying",
    "okay",
    "much",
    "get",
    "right",
    "know",
    "one",
    "step",
    "train",
    "slightly",
    "different",
    "actually",
    "realized",
    "later",
    "probably",
    "know",
    "probably",
    "done",
    "defining",
    "model",
    "fit",
    "instead",
    "validation",
    "split",
    "define",
    "validation",
    "data",
    "pass",
    "validation",
    "data",
    "know",
    "proper",
    "syntax",
    "probably",
    "done",
    "instead",
    "know",
    "stick",
    "see",
    "end",
    "know",
    "64",
    "nodes",
    "seems",
    "like",
    "best",
    "performance",
    "64",
    "nodes",
    "dropout",
    "learning",
    "rate",
    "batch",
    "size",
    "seem",
    "like",
    "yes",
    "validation",
    "know",
    "fake",
    "validation",
    "validation",
    "loss",
    "decreasing",
    "accuracy",
    "increasing",
    "good",
    "sign",
    "okay",
    "finally",
    "going",
    "actually",
    "going",
    "predict",
    "going",
    "take",
    "model",
    "called",
    "least",
    "loss",
    "model",
    "going",
    "take",
    "model",
    "going",
    "predict",
    "x",
    "test",
    "see",
    "gives",
    "values",
    "really",
    "close",
    "zero",
    "really",
    "close",
    "one",
    "sigmoid",
    "output",
    "cast",
    "going",
    "say",
    "anything",
    "greater",
    "set",
    "one",
    "actually",
    "think",
    "happens",
    "oh",
    "okay",
    "cast",
    "type",
    "see",
    "ones",
    "zeros",
    "actually",
    "going",
    "transform",
    "column",
    "well",
    "going",
    "oh",
    "oops",
    "mean",
    "okay",
    "wanted",
    "reshape",
    "one",
    "dimensional",
    "okay",
    "using",
    "actually",
    "rerun",
    "classification",
    "report",
    "based",
    "neural",
    "net",
    "output",
    "see",
    "okay",
    "f",
    "ones",
    "accuracy",
    "gives",
    "us",
    "87",
    "seems",
    "like",
    "happened",
    "precision",
    "class",
    "zero",
    "hadrons",
    "increased",
    "bit",
    "recall",
    "decreased",
    "f",
    "one",
    "score",
    "still",
    "good",
    "point",
    "eight",
    "one",
    "class",
    "looked",
    "like",
    "precision",
    "decreased",
    "bit",
    "recall",
    "increased",
    "overall",
    "f",
    "one",
    "score",
    "also",
    "increased",
    "think",
    "interpreted",
    "properly",
    "mean",
    "went",
    "work",
    "got",
    "model",
    "performs",
    "actually",
    "similarly",
    "svm",
    "model",
    "earlier",
    "whole",
    "point",
    "exercise",
    "demonstrate",
    "okay",
    "define",
    "models",
    "also",
    "say",
    "hey",
    "maybe",
    "know",
    "neural",
    "nets",
    "powerful",
    "tell",
    "sometimes",
    "know",
    "svm",
    "model",
    "might",
    "actually",
    "appropriate",
    "case",
    "guess",
    "really",
    "matter",
    "one",
    "use",
    "end",
    "87",
    "accuracy",
    "score",
    "still",
    "pretty",
    "good",
    "yeah",
    "let",
    "move",
    "regression",
    "saw",
    "bunch",
    "different",
    "classification",
    "models",
    "let",
    "shift",
    "gears",
    "regression",
    "type",
    "supervised",
    "learning",
    "look",
    "plot",
    "see",
    "bunch",
    "scattered",
    "data",
    "points",
    "x",
    "value",
    "data",
    "points",
    "corresponding",
    "value",
    "label",
    "look",
    "plot",
    "well",
    "goal",
    "regression",
    "find",
    "line",
    "best",
    "fit",
    "best",
    "models",
    "data",
    "essentially",
    "trying",
    "let",
    "say",
    "given",
    "new",
    "value",
    "x",
    "sample",
    "trying",
    "say",
    "okay",
    "would",
    "prediction",
    "given",
    "x",
    "value",
    "know",
    "might",
    "somewhere",
    "around",
    "know",
    "remember",
    "regression",
    "know",
    "given",
    "certain",
    "features",
    "trying",
    "predict",
    "continuous",
    "numerical",
    "value",
    "linear",
    "regression",
    "want",
    "take",
    "data",
    "fit",
    "linear",
    "model",
    "data",
    "case",
    "linear",
    "model",
    "might",
    "look",
    "something",
    "along",
    "lines",
    "right",
    "would",
    "considered",
    "maybe",
    "line",
    "best",
    "fit",
    "line",
    "modeled",
    "equation",
    "going",
    "write",
    "equals",
    "b",
    "zero",
    "plus",
    "b",
    "one",
    "b",
    "zero",
    "means",
    "intercept",
    "extend",
    "value",
    "b",
    "zero",
    "b",
    "one",
    "defines",
    "source",
    "line",
    "defines",
    "slope",
    "line",
    "okay",
    "right",
    "formula",
    "linear",
    "regression",
    "exactly",
    "come",
    "formula",
    "trying",
    "linear",
    "regression",
    "know",
    "could",
    "eyeball",
    "line",
    "humans",
    "good",
    "eyeballing",
    "certain",
    "things",
    "like",
    "mean",
    "get",
    "close",
    "computer",
    "better",
    "giving",
    "us",
    "precise",
    "value",
    "b",
    "zero",
    "b",
    "one",
    "well",
    "let",
    "introduce",
    "concept",
    "something",
    "known",
    "residual",
    "okay",
    "residual",
    "might",
    "also",
    "hear",
    "called",
    "error",
    "means",
    "let",
    "take",
    "data",
    "point",
    "data",
    "set",
    "going",
    "evaluate",
    "far",
    "prediction",
    "data",
    "point",
    "already",
    "let",
    "say",
    "eight",
    "let",
    "call",
    "see",
    "use",
    "order",
    "order",
    "represent",
    "hey",
    "one",
    "points",
    "okay",
    "would",
    "prediction",
    "oops",
    "would",
    "prediction",
    "eight",
    "labeled",
    "hat",
    "okay",
    "hat",
    "means",
    "hey",
    "guess",
    "prediction",
    "know",
    "specific",
    "value",
    "okay",
    "residual",
    "would",
    "distance",
    "eight",
    "hat",
    "eight",
    "eight",
    "minus",
    "hat",
    "eight",
    "right",
    "would",
    "give",
    "us",
    "going",
    "take",
    "absolute",
    "value",
    "line",
    "right",
    "would",
    "get",
    "negative",
    "value",
    "distance",
    "ca",
    "negative",
    "going",
    "put",
    "little",
    "hat",
    "going",
    "put",
    "little",
    "absolute",
    "value",
    "around",
    "quantity",
    "gives",
    "us",
    "residual",
    "error",
    "let",
    "rewrite",
    "know",
    "generalize",
    "points",
    "going",
    "say",
    "residual",
    "calculated",
    "minus",
    "hat",
    "okay",
    "means",
    "distance",
    "given",
    "point",
    "prediction",
    "corresponding",
    "prediction",
    "line",
    "residual",
    "line",
    "best",
    "fit",
    "generally",
    "trying",
    "decrease",
    "residuals",
    "much",
    "possible",
    "value",
    "error",
    "line",
    "best",
    "fit",
    "trying",
    "decrease",
    "error",
    "much",
    "possible",
    "different",
    "data",
    "points",
    "might",
    "mean",
    "know",
    "minimizing",
    "sum",
    "residuals",
    "sum",
    "symbol",
    "stick",
    "residual",
    "calculation",
    "looks",
    "something",
    "like",
    "right",
    "going",
    "say",
    "okay",
    "eyes",
    "data",
    "set",
    "different",
    "points",
    "going",
    "sum",
    "residuals",
    "going",
    "try",
    "decrease",
    "line",
    "best",
    "fit",
    "going",
    "find",
    "b0",
    "b1",
    "gives",
    "lowest",
    "value",
    "okay",
    "know",
    "sometimes",
    "different",
    "circumstances",
    "might",
    "attach",
    "squared",
    "trying",
    "decrease",
    "sum",
    "squared",
    "residuals",
    "know",
    "adds",
    "higher",
    "penalty",
    "far",
    "know",
    "points",
    "linear",
    "regression",
    "trying",
    "find",
    "equation",
    "line",
    "best",
    "fit",
    "help",
    "us",
    "decrease",
    "measure",
    "error",
    "respect",
    "data",
    "points",
    "data",
    "set",
    "try",
    "come",
    "best",
    "prediction",
    "known",
    "simple",
    "linear",
    "regression",
    "basically",
    "means",
    "know",
    "equation",
    "looks",
    "something",
    "like",
    "also",
    "multiple",
    "linear",
    "regression",
    "means",
    "hey",
    "one",
    "value",
    "x",
    "like",
    "think",
    "feature",
    "vectors",
    "multiple",
    "values",
    "x",
    "vector",
    "predictor",
    "might",
    "look",
    "something",
    "like",
    "actually",
    "going",
    "say",
    "etc",
    "plus",
    "b",
    "n",
    "x",
    "coming",
    "coefficient",
    "different",
    "x",
    "values",
    "vector",
    "guys",
    "might",
    "noticed",
    "assumptions",
    "might",
    "asking",
    "okay",
    "kylie",
    "world",
    "assumptions",
    "mean",
    "let",
    "go",
    "let",
    "go",
    "first",
    "one",
    "linearity",
    "means",
    "let",
    "say",
    "data",
    "set",
    "okay",
    "linearity",
    "means",
    "okay",
    "data",
    "follow",
    "linear",
    "pattern",
    "increase",
    "x",
    "increases",
    "decrease",
    "x",
    "increases",
    "increases",
    "decreases",
    "constant",
    "rate",
    "x",
    "increases",
    "probably",
    "looking",
    "something",
    "linear",
    "example",
    "nonlinear",
    "data",
    "set",
    "let",
    "say",
    "data",
    "might",
    "look",
    "something",
    "like",
    "okay",
    "visually",
    "judging",
    "might",
    "say",
    "okay",
    "seems",
    "like",
    "line",
    "best",
    "fit",
    "might",
    "actually",
    "curve",
    "like",
    "right",
    "case",
    "satisfy",
    "linearity",
    "assumption",
    "anymore",
    "linearity",
    "basically",
    "want",
    "data",
    "set",
    "follow",
    "sort",
    "linear",
    "trajectory",
    "independence",
    "second",
    "assumption",
    "means",
    "point",
    "influence",
    "point",
    "point",
    "point",
    "words",
    "points",
    "samples",
    "data",
    "set",
    "independent",
    "okay",
    "rely",
    "one",
    "another",
    "affect",
    "one",
    "another",
    "okay",
    "normality",
    "homoscedasticity",
    "concepts",
    "use",
    "residual",
    "okay",
    "plot",
    "looks",
    "something",
    "like",
    "plot",
    "looks",
    "like",
    "okay",
    "something",
    "like",
    "line",
    "best",
    "fit",
    "somewhere",
    "maybe",
    "something",
    "like",
    "order",
    "look",
    "normality",
    "homoscedasticity",
    "assumptions",
    "let",
    "look",
    "residual",
    "plot",
    "okay",
    "means",
    "going",
    "keep",
    "x",
    "axis",
    "instead",
    "plotting",
    "relative",
    "going",
    "plot",
    "errors",
    "going",
    "plot",
    "minus",
    "hat",
    "like",
    "okay",
    "know",
    "one",
    "slightly",
    "positive",
    "might",
    "one",
    "negative",
    "might",
    "residual",
    "plot",
    "literally",
    "plot",
    "know",
    "values",
    "distributed",
    "around",
    "line",
    "best",
    "fit",
    "looks",
    "like",
    "might",
    "know",
    "look",
    "something",
    "like",
    "okay",
    "might",
    "residual",
    "plot",
    "normality",
    "means",
    "assumptions",
    "normality",
    "homoscedasticity",
    "might",
    "butchered",
    "spelling",
    "really",
    "know",
    "normality",
    "saying",
    "saying",
    "okay",
    "residuals",
    "normally",
    "distributed",
    "okay",
    "around",
    "line",
    "best",
    "fit",
    "follow",
    "normal",
    "distribution",
    "homoscedasticity",
    "says",
    "okay",
    "variants",
    "points",
    "remain",
    "constant",
    "throughout",
    "spread",
    "approximately",
    "spread",
    "example",
    "know",
    "homoscedasticity",
    "held",
    "well",
    "let",
    "say",
    "original",
    "plot",
    "actually",
    "looks",
    "something",
    "like",
    "okay",
    "looked",
    "residuals",
    "might",
    "look",
    "something",
    "like",
    "look",
    "spread",
    "points",
    "decreases",
    "right",
    "spread",
    "constant",
    "means",
    "homoscedasticity",
    "assumption",
    "would",
    "fulfilled",
    "might",
    "appropriate",
    "use",
    "linear",
    "regression",
    "linear",
    "regression",
    "basically",
    "bunch",
    "data",
    "points",
    "want",
    "predict",
    "value",
    "trying",
    "come",
    "line",
    "best",
    "fit",
    "best",
    "describes",
    "hey",
    "given",
    "value",
    "x",
    "would",
    "best",
    "guess",
    "let",
    "move",
    "evaluate",
    "linear",
    "regression",
    "model",
    "first",
    "measure",
    "going",
    "talk",
    "known",
    "mean",
    "absolute",
    "error",
    "mae",
    "short",
    "okay",
    "mean",
    "absolute",
    "error",
    "basically",
    "saying",
    "right",
    "let",
    "take",
    "errors",
    "residuals",
    "talked",
    "let",
    "sum",
    "distance",
    "take",
    "average",
    "describe",
    "know",
    "far",
    "mathematical",
    "formula",
    "would",
    "okay",
    "let",
    "take",
    "residuals",
    "alright",
    "distance",
    "actually",
    "let",
    "redraw",
    "plot",
    "suppose",
    "data",
    "set",
    "look",
    "like",
    "data",
    "points",
    "right",
    "let",
    "say",
    "line",
    "looks",
    "something",
    "like",
    "mean",
    "absolute",
    "error",
    "would",
    "summing",
    "values",
    "mistake",
    "summing",
    "dividing",
    "many",
    "data",
    "points",
    "would",
    "residuals",
    "would",
    "right",
    "every",
    "single",
    "point",
    "minus",
    "hat",
    "prediction",
    "going",
    "sum",
    "different",
    "data",
    "set",
    "right",
    "divide",
    "number",
    "points",
    "actually",
    "going",
    "rewrite",
    "make",
    "little",
    "clearer",
    "equal",
    "whatever",
    "first",
    "data",
    "point",
    "way",
    "nth",
    "data",
    "point",
    "divide",
    "n",
    "many",
    "points",
    "okay",
    "measure",
    "mae",
    "basically",
    "telling",
    "us",
    "okay",
    "average",
    "distance",
    "predicted",
    "value",
    "actual",
    "value",
    "training",
    "set",
    "okay",
    "mae",
    "good",
    "allows",
    "us",
    "know",
    "get",
    "value",
    "literally",
    "directly",
    "compare",
    "whatever",
    "units",
    "value",
    "let",
    "say",
    "talking",
    "know",
    "prediction",
    "price",
    "house",
    "right",
    "dollars",
    "calculate",
    "mae",
    "literally",
    "say",
    "oh",
    "average",
    "know",
    "price",
    "average",
    "much",
    "literally",
    "many",
    "dollars",
    "okay",
    "mean",
    "absolute",
    "error",
    "evaluation",
    "technique",
    "also",
    "closely",
    "related",
    "called",
    "mean",
    "squared",
    "error",
    "mse",
    "short",
    "okay",
    "take",
    "plot",
    "duplicated",
    "move",
    "well",
    "gist",
    "mean",
    "squared",
    "error",
    "kind",
    "instead",
    "absolute",
    "value",
    "going",
    "square",
    "mse",
    "something",
    "along",
    "lines",
    "okay",
    "let",
    "sum",
    "something",
    "right",
    "going",
    "sum",
    "errors",
    "going",
    "minus",
    "hat",
    "instead",
    "absolute",
    "valuing",
    "going",
    "square",
    "going",
    "divide",
    "n",
    "order",
    "find",
    "mean",
    "basically",
    "taking",
    "different",
    "values",
    "squaring",
    "first",
    "add",
    "one",
    "another",
    "divide",
    "reason",
    "like",
    "using",
    "mean",
    "squared",
    "error",
    "helps",
    "us",
    "punish",
    "large",
    "errors",
    "prediction",
    "later",
    "mse",
    "might",
    "important",
    "differentiability",
    "right",
    "quadratic",
    "equation",
    "differentiable",
    "know",
    "familiar",
    "calculus",
    "quadratic",
    "equation",
    "differentiable",
    "whereas",
    "absolute",
    "value",
    "function",
    "totally",
    "differentiable",
    "everywhere",
    "understand",
    "worry",
    "wo",
    "really",
    "need",
    "right",
    "one",
    "downside",
    "mean",
    "squared",
    "error",
    "calculate",
    "mean",
    "squared",
    "error",
    "go",
    "back",
    "want",
    "compare",
    "values",
    "well",
    "gets",
    "little",
    "bit",
    "trickier",
    "mean",
    "squared",
    "error",
    "terms",
    "squared",
    "right",
    "squared",
    "instead",
    "dollars",
    "know",
    "many",
    "dollars",
    "talking",
    "many",
    "dollars",
    "squared",
    "know",
    "humans",
    "really",
    "make",
    "much",
    "sense",
    "created",
    "something",
    "known",
    "root",
    "mean",
    "squared",
    "error",
    "going",
    "copy",
    "diagram",
    "similar",
    "mean",
    "squared",
    "error",
    "except",
    "take",
    "big",
    "squared",
    "root",
    "okay",
    "messy",
    "take",
    "square",
    "root",
    "mean",
    "squared",
    "error",
    "term",
    "know",
    "defining",
    "error",
    "terms",
    "dollar",
    "sign",
    "symbol",
    "pro",
    "root",
    "mean",
    "squared",
    "error",
    "say",
    "okay",
    "error",
    "according",
    "metric",
    "many",
    "dollar",
    "signs",
    "predictor",
    "okay",
    "unit",
    "one",
    "pros",
    "root",
    "mean",
    "squared",
    "error",
    "finally",
    "coefficient",
    "determination",
    "r",
    "squared",
    "formula",
    "r",
    "squared",
    "r",
    "squared",
    "equal",
    "one",
    "minus",
    "rss",
    "tss",
    "okay",
    "mean",
    "basically",
    "rss",
    "stands",
    "sum",
    "squared",
    "residuals",
    "maybe",
    "ssr",
    "instead",
    "rss",
    "sum",
    "squared",
    "residuals",
    "equal",
    "take",
    "sum",
    "values",
    "take",
    "minus",
    "hat",
    "square",
    "rss",
    "right",
    "sum",
    "squared",
    "residuals",
    "tss",
    "let",
    "actually",
    "use",
    "different",
    "color",
    "tss",
    "total",
    "sum",
    "squares",
    "means",
    "instead",
    "respect",
    "prediction",
    "instead",
    "going",
    "take",
    "value",
    "subtract",
    "mean",
    "values",
    "square",
    "okay",
    "drew",
    "actually",
    "let",
    "use",
    "different",
    "color",
    "let",
    "use",
    "green",
    "predictor",
    "rss",
    "giving",
    "measure",
    "right",
    "giving",
    "estimate",
    "far",
    "regressor",
    "predicted",
    "actually",
    "gon",
    "na",
    "take",
    "one",
    "gon",
    "na",
    "take",
    "one",
    "actually",
    "going",
    "use",
    "red",
    "well",
    "tss",
    "hand",
    "saying",
    "okay",
    "far",
    "values",
    "mean",
    "literally",
    "calculations",
    "line",
    "best",
    "fit",
    "took",
    "values",
    "average",
    "said",
    "hey",
    "average",
    "value",
    "every",
    "single",
    "x",
    "value",
    "going",
    "predict",
    "average",
    "value",
    "instead",
    "asking",
    "okay",
    "far",
    "points",
    "line",
    "okay",
    "remember",
    "square",
    "means",
    "punishing",
    "larger",
    "errors",
    "right",
    "even",
    "look",
    "somewhat",
    "close",
    "terms",
    "distance",
    "data",
    "points",
    "larger",
    "total",
    "sum",
    "squares",
    "going",
    "sorry",
    "dog",
    "total",
    "sum",
    "squares",
    "taking",
    "values",
    "saying",
    "okay",
    "sum",
    "squares",
    "regressor",
    "literally",
    "calculated",
    "average",
    "values",
    "data",
    "set",
    "every",
    "single",
    "x",
    "value",
    "going",
    "predict",
    "average",
    "means",
    "okay",
    "like",
    "means",
    "maybe",
    "x",
    "associated",
    "like",
    "best",
    "thing",
    "new",
    "x",
    "value",
    "predict",
    "hey",
    "average",
    "data",
    "set",
    "total",
    "sum",
    "squares",
    "saying",
    "okay",
    "well",
    "respect",
    "average",
    "error",
    "right",
    "sum",
    "squared",
    "residuals",
    "telling",
    "us",
    "error",
    "respect",
    "line",
    "best",
    "fit",
    "well",
    "total",
    "sum",
    "squares",
    "saying",
    "error",
    "respect",
    "know",
    "average",
    "value",
    "line",
    "best",
    "fit",
    "better",
    "fit",
    "total",
    "sum",
    "squares",
    "means",
    "know",
    "numerator",
    "means",
    "numerator",
    "going",
    "smaller",
    "denominator",
    "right",
    "errors",
    "line",
    "best",
    "fit",
    "much",
    "smaller",
    "means",
    "ratio",
    "rss",
    "tss",
    "going",
    "small",
    "means",
    "r",
    "squared",
    "going",
    "go",
    "towards",
    "one",
    "r",
    "squared",
    "towards",
    "one",
    "means",
    "usually",
    "sign",
    "good",
    "predictor",
    "one",
    "signs",
    "one",
    "also",
    "know",
    "adjusted",
    "r",
    "squared",
    "adjusts",
    "number",
    "terms",
    "x1",
    "x2",
    "x3",
    "etc",
    "adjusts",
    "many",
    "extra",
    "terms",
    "add",
    "usually",
    "know",
    "add",
    "extra",
    "term",
    "r",
    "squared",
    "value",
    "increase",
    "help",
    "us",
    "predict",
    "value",
    "adjusted",
    "r",
    "squared",
    "increase",
    "new",
    "term",
    "actually",
    "improves",
    "model",
    "fit",
    "expected",
    "know",
    "chance",
    "adjusted",
    "r",
    "squared",
    "know",
    "scope",
    "one",
    "specific",
    "course",
    "linear",
    "regression",
    "basically",
    "covered",
    "concept",
    "residuals",
    "errors",
    "know",
    "use",
    "order",
    "find",
    "line",
    "best",
    "fit",
    "know",
    "computer",
    "calculations",
    "us",
    "nice",
    "behind",
    "scenes",
    "trying",
    "minimize",
    "error",
    "right",
    "gone",
    "different",
    "ways",
    "actually",
    "evaluating",
    "linear",
    "regression",
    "model",
    "pros",
    "cons",
    "one",
    "let",
    "look",
    "example",
    "still",
    "supervised",
    "learning",
    "going",
    "talk",
    "regression",
    "happens",
    "want",
    "predict",
    "know",
    "type",
    "123",
    "happens",
    "actually",
    "want",
    "predict",
    "certain",
    "value",
    "uci",
    "machine",
    "learning",
    "repository",
    "found",
    "data",
    "set",
    "bike",
    "sharing",
    "seoul",
    "south",
    "korea",
    "data",
    "set",
    "predicting",
    "rental",
    "bike",
    "count",
    "kind",
    "bikes",
    "rented",
    "hour",
    "going",
    "going",
    "go",
    "data",
    "folder",
    "going",
    "download",
    "csv",
    "file",
    "going",
    "move",
    "collab",
    "going",
    "name",
    "fcc",
    "bikes",
    "regression",
    "remember",
    "called",
    "last",
    "one",
    "yeah",
    "fcc",
    "bikes",
    "regression",
    "going",
    "import",
    "bunch",
    "things",
    "earlier",
    "know",
    "going",
    "also",
    "continue",
    "import",
    "oversampler",
    "standard",
    "scaler",
    "actually",
    "also",
    "going",
    "let",
    "guys",
    "know",
    "things",
    "wanted",
    "import",
    "library",
    "lets",
    "us",
    "copy",
    "things",
    "seaborn",
    "wrapper",
    "matplotlib",
    "also",
    "allows",
    "us",
    "plot",
    "certain",
    "things",
    "letting",
    "know",
    "also",
    "going",
    "using",
    "tensorflow",
    "okay",
    "one",
    "thing",
    "also",
    "going",
    "using",
    "going",
    "use",
    "sklearn",
    "linear",
    "model",
    "library",
    "actually",
    "let",
    "make",
    "screen",
    "little",
    "bit",
    "bigger",
    "yeah",
    "awesome",
    "run",
    "import",
    "things",
    "need",
    "going",
    "know",
    "give",
    "credit",
    "got",
    "data",
    "set",
    "let",
    "copy",
    "paste",
    "uci",
    "thing",
    "also",
    "give",
    "credit",
    "okay",
    "cool",
    "right",
    "cool",
    "data",
    "set",
    "tells",
    "us",
    "different",
    "attributes",
    "right",
    "actually",
    "going",
    "go",
    "ahead",
    "paste",
    "feel",
    "free",
    "copy",
    "paste",
    "want",
    "read",
    "loud",
    "type",
    "byte",
    "count",
    "hour",
    "temp",
    "humidity",
    "wind",
    "visibility",
    "dew",
    "point",
    "temp",
    "radiation",
    "rain",
    "snow",
    "functional",
    "whatever",
    "means",
    "okay",
    "going",
    "come",
    "import",
    "data",
    "dragging",
    "dropping",
    "right",
    "one",
    "thing",
    "guys",
    "might",
    "actually",
    "need",
    "might",
    "actually",
    "open",
    "csv",
    "first",
    "like",
    "forbidding",
    "characters",
    "mine",
    "least",
    "might",
    "get",
    "rid",
    "like",
    "think",
    "degree",
    "computer",
    "recognizing",
    "got",
    "rid",
    "might",
    "go",
    "get",
    "rid",
    "labels",
    "incorrect",
    "going",
    "okay",
    "done",
    "imported",
    "going",
    "create",
    "data",
    "data",
    "frame",
    "right",
    "read",
    "csv",
    "file",
    "get",
    "data",
    "like",
    "data",
    "dot",
    "csv",
    "okay",
    "call",
    "data",
    "dot",
    "head",
    "see",
    "various",
    "labels",
    "right",
    "data",
    "going",
    "actually",
    "going",
    "get",
    "rid",
    "columns",
    "know",
    "really",
    "care",
    "going",
    "type",
    "going",
    "drop",
    "maybe",
    "date",
    "whether",
    "holiday",
    "various",
    "seasons",
    "going",
    "care",
    "things",
    "access",
    "equals",
    "one",
    "means",
    "drop",
    "columns",
    "see",
    "okay",
    "still",
    "mean",
    "guess",
    "really",
    "notice",
    "set",
    "data",
    "frames",
    "columns",
    "equal",
    "data",
    "set",
    "calls",
    "look",
    "know",
    "first",
    "five",
    "things",
    "see",
    "data",
    "set",
    "lot",
    "easier",
    "read",
    "another",
    "thing",
    "actually",
    "going",
    "df",
    "functional",
    "going",
    "create",
    "remember",
    "computers",
    "good",
    "language",
    "want",
    "zeros",
    "ones",
    "convert",
    "well",
    "equal",
    "yes",
    "gets",
    "mapped",
    "one",
    "set",
    "type",
    "integer",
    "right",
    "great",
    "cool",
    "thing",
    "right",
    "counts",
    "whatever",
    "hour",
    "make",
    "example",
    "simpler",
    "going",
    "index",
    "hour",
    "gon",
    "na",
    "say",
    "okay",
    "going",
    "use",
    "specific",
    "hour",
    "going",
    "index",
    "hour",
    "going",
    "use",
    "hour",
    "let",
    "say",
    "data",
    "frame",
    "going",
    "data",
    "frame",
    "hour",
    "let",
    "say",
    "equals",
    "okay",
    "noon",
    "right",
    "see",
    "equal",
    "actually",
    "going",
    "drop",
    "column",
    "access",
    "equals",
    "one",
    "alright",
    "run",
    "cell",
    "okay",
    "got",
    "rid",
    "hour",
    "count",
    "temperature",
    "humidity",
    "wind",
    "visibility",
    "yada",
    "yada",
    "yada",
    "alright",
    "want",
    "going",
    "actually",
    "plot",
    "columns",
    "range",
    "length",
    "whatever",
    "data",
    "frame",
    "columns",
    "count",
    "actually",
    "first",
    "thing",
    "going",
    "say",
    "label",
    "data",
    "frame",
    "columns",
    "everything",
    "first",
    "thing",
    "would",
    "give",
    "temperature",
    "onwards",
    "features",
    "right",
    "going",
    "scatter",
    "want",
    "see",
    "label",
    "specific",
    "data",
    "affects",
    "count",
    "going",
    "plot",
    "bike",
    "count",
    "axis",
    "going",
    "plot",
    "know",
    "whatever",
    "specific",
    "label",
    "x",
    "axis",
    "going",
    "title",
    "whatever",
    "label",
    "know",
    "make",
    "label",
    "bike",
    "count",
    "noon",
    "x",
    "label",
    "label",
    "okay",
    "guess",
    "even",
    "need",
    "legend",
    "even",
    "need",
    "legend",
    "show",
    "plot",
    "right",
    "seems",
    "like",
    "functional",
    "really",
    "really",
    "give",
    "us",
    "utility",
    "snow",
    "rain",
    "seems",
    "like",
    "radiation",
    "know",
    "fairly",
    "linear",
    "dew",
    "point",
    "temperature",
    "visibility",
    "wind",
    "really",
    "seem",
    "like",
    "much",
    "humidity",
    "kind",
    "maybe",
    "like",
    "inverse",
    "relationship",
    "temperature",
    "definitely",
    "looks",
    "like",
    "relationship",
    "number",
    "bikes",
    "right",
    "actually",
    "going",
    "going",
    "drop",
    "ones",
    "seem",
    "like",
    "really",
    "matter",
    "maybe",
    "wind",
    "know",
    "visibility",
    "yeah",
    "going",
    "get",
    "rid",
    "visibility",
    "functional",
    "data",
    "frame",
    "going",
    "drop",
    "wind",
    "visibility",
    "functional",
    "right",
    "axis",
    "column",
    "one",
    "look",
    "data",
    "set",
    "temperature",
    "humidity",
    "dew",
    "point",
    "temperature",
    "radiation",
    "rain",
    "snow",
    "want",
    "want",
    "split",
    "training",
    "validation",
    "test",
    "data",
    "set",
    "talked",
    "use",
    "exact",
    "thing",
    "say",
    "numpy",
    "dot",
    "split",
    "sample",
    "know",
    "whole",
    "sample",
    "create",
    "splits",
    "data",
    "frame",
    "going",
    "set",
    "eight",
    "okay",
    "really",
    "care",
    "know",
    "full",
    "grid",
    "full",
    "array",
    "going",
    "use",
    "underscore",
    "variable",
    "get",
    "training",
    "x",
    "actually",
    "function",
    "getting",
    "x",
    "going",
    "write",
    "function",
    "defined",
    "get",
    "x",
    "going",
    "pass",
    "data",
    "frame",
    "actually",
    "going",
    "pass",
    "name",
    "label",
    "x",
    "specific",
    "x",
    "labels",
    "want",
    "look",
    "none",
    "like",
    "like",
    "going",
    "going",
    "get",
    "everything",
    "data",
    "set",
    "wildlife",
    "actually",
    "going",
    "make",
    "first",
    "deep",
    "copy",
    "data",
    "frame",
    "basically",
    "means",
    "copying",
    "everything",
    "like",
    "x",
    "labels",
    "none",
    "x",
    "labels",
    "going",
    "say",
    "right",
    "x",
    "going",
    "whatever",
    "data",
    "frame",
    "going",
    "take",
    "columns",
    "c",
    "c",
    "data",
    "frame",
    "dot",
    "columns",
    "c",
    "equal",
    "label",
    "right",
    "going",
    "get",
    "values",
    "x",
    "labels",
    "well",
    "okay",
    "order",
    "index",
    "one",
    "thing",
    "like",
    "let",
    "say",
    "pass",
    "one",
    "thing",
    "data",
    "frame",
    "let",
    "make",
    "case",
    "length",
    "x",
    "labels",
    "equal",
    "one",
    "going",
    "say",
    "going",
    "x",
    "labels",
    "add",
    "label",
    "values",
    "actually",
    "need",
    "reshape",
    "make",
    "2d",
    "going",
    "pass",
    "negative",
    "one",
    "comma",
    "one",
    "otherwise",
    "like",
    "list",
    "specific",
    "x",
    "labels",
    "want",
    "use",
    "actually",
    "going",
    "say",
    "x",
    "equal",
    "data",
    "frame",
    "x",
    "labels",
    "dot",
    "values",
    "suffice",
    "alright",
    "extracting",
    "order",
    "get",
    "going",
    "equals",
    "data",
    "frame",
    "passing",
    "label",
    "end",
    "going",
    "say",
    "data",
    "equals",
    "np",
    "dot",
    "h",
    "stack",
    "stacking",
    "horizontally",
    "one",
    "next",
    "take",
    "x",
    "return",
    "oh",
    "needs",
    "values",
    "actually",
    "going",
    "reshape",
    "make",
    "2d",
    "well",
    "h",
    "stack",
    "return",
    "data",
    "x",
    "able",
    "say",
    "okay",
    "get",
    "x",
    "take",
    "data",
    "frame",
    "label",
    "label",
    "byte",
    "count",
    "actually",
    "x",
    "label",
    "actually",
    "going",
    "let",
    "like",
    "one",
    "dimension",
    "right",
    "earlier",
    "got",
    "rid",
    "plots",
    "seen",
    "maybe",
    "know",
    "temperature",
    "dimension",
    "really",
    "well",
    "might",
    "able",
    "use",
    "predict",
    "going",
    "label",
    "also",
    "know",
    "using",
    "temperature",
    "also",
    "going",
    "oh",
    "train",
    "validation",
    "test",
    "oh",
    "val",
    "right",
    "val",
    "test",
    "alright",
    "run",
    "training",
    "validation",
    "test",
    "data",
    "sets",
    "temperature",
    "look",
    "x",
    "train",
    "temp",
    "literally",
    "temperature",
    "okay",
    "first",
    "show",
    "simple",
    "linear",
    "regression",
    "alright",
    "right",
    "create",
    "regressor",
    "say",
    "temp",
    "regressor",
    "going",
    "know",
    "make",
    "linear",
    "regression",
    "model",
    "like",
    "simply",
    "fix",
    "fit",
    "x",
    "train",
    "temp",
    "train",
    "temp",
    "order",
    "train",
    "train",
    "linear",
    "regression",
    "model",
    "alright",
    "also",
    "print",
    "regressor",
    "coefficients",
    "intercept",
    "okay",
    "coefficient",
    "whatever",
    "temperature",
    "x",
    "intercept",
    "okay",
    "intercept",
    "sorry",
    "right",
    "know",
    "score",
    "get",
    "r",
    "squared",
    "score",
    "score",
    "x",
    "test",
    "test",
    "right",
    "r",
    "squared",
    "around",
    "point",
    "three",
    "eight",
    "better",
    "zero",
    "would",
    "mean",
    "hey",
    "absolutely",
    "association",
    "also",
    "know",
    "like",
    "good",
    "depends",
    "context",
    "know",
    "higher",
    "number",
    "means",
    "higher",
    "two",
    "variables",
    "would",
    "correlated",
    "right",
    "right",
    "means",
    "maybe",
    "association",
    "two",
    "reason",
    "want",
    "one",
    "show",
    "know",
    "plotted",
    "would",
    "look",
    "like",
    "create",
    "scatterplot",
    "let",
    "take",
    "training",
    "data",
    "let",
    "make",
    "blue",
    "also",
    "plotted",
    "something",
    "say",
    "know",
    "x",
    "range",
    "going",
    "plot",
    "space",
    "goes",
    "negative",
    "20",
    "40",
    "piece",
    "data",
    "going",
    "say",
    "let",
    "take",
    "100",
    "things",
    "going",
    "plot",
    "x",
    "going",
    "take",
    "temper",
    "like",
    "regressor",
    "predict",
    "x",
    "okay",
    "label",
    "going",
    "label",
    "fit",
    "color",
    "let",
    "make",
    "red",
    "let",
    "actually",
    "set",
    "line",
    "change",
    "thick",
    "value",
    "okay",
    "end",
    "let",
    "create",
    "legend",
    "let",
    "right",
    "let",
    "also",
    "create",
    "know",
    "title",
    "things",
    "matter",
    "sense",
    "let",
    "say",
    "would",
    "bikes",
    "versus",
    "temperature",
    "right",
    "label",
    "would",
    "number",
    "bikes",
    "x",
    "label",
    "would",
    "temperature",
    "actually",
    "think",
    "might",
    "cause",
    "error",
    "yeah",
    "expecting",
    "2d",
    "array",
    "actually",
    "reshape",
    "okay",
    "go",
    "make",
    "array",
    "reshape",
    "2d",
    "see",
    "right",
    "increases",
    "remember",
    "assumptions",
    "linear",
    "regression",
    "like",
    "really",
    "know",
    "fits",
    "assumptions",
    "right",
    "wanted",
    "show",
    "guys",
    "though",
    "like",
    "right",
    "line",
    "fit",
    "data",
    "would",
    "look",
    "like",
    "okay",
    "multiple",
    "linear",
    "regression",
    "right",
    "going",
    "go",
    "ahead",
    "well",
    "take",
    "data",
    "set",
    "instead",
    "labels",
    "actually",
    "current",
    "data",
    "set",
    "right",
    "alright",
    "let",
    "use",
    "except",
    "byte",
    "count",
    "right",
    "going",
    "say",
    "x",
    "labels",
    "let",
    "take",
    "data",
    "frames",
    "columns",
    "remove",
    "byte",
    "count",
    "work",
    "part",
    "x",
    "labels",
    "none",
    "work",
    "oops",
    "sorry",
    "okay",
    "oh",
    "temperature",
    "anymore",
    "actually",
    "let",
    "say",
    "right",
    "going",
    "quickly",
    "rerun",
    "piece",
    "temperature",
    "data",
    "set",
    "data",
    "set",
    "okay",
    "regressor",
    "thing",
    "regressor",
    "going",
    "make",
    "linear",
    "regression",
    "going",
    "fit",
    "x",
    "train",
    "train",
    "okay",
    "alright",
    "let",
    "go",
    "ahead",
    "also",
    "score",
    "regressor",
    "let",
    "see",
    "r",
    "squared",
    "performs",
    "test",
    "test",
    "data",
    "set",
    "happens",
    "alright",
    "r",
    "square",
    "seems",
    "improve",
    "went",
    "point",
    "four",
    "point",
    "five",
    "two",
    "good",
    "sign",
    "okay",
    "ca",
    "necessarily",
    "plot",
    "know",
    "every",
    "single",
    "dimension",
    "say",
    "okay",
    "improved",
    "right",
    "alright",
    "one",
    "cool",
    "thing",
    "tensorflow",
    "actually",
    "regression",
    "neural",
    "net",
    "going",
    "already",
    "training",
    "data",
    "temperature",
    "know",
    "different",
    "columns",
    "going",
    "bother",
    "splitting",
    "data",
    "going",
    "go",
    "ahead",
    "start",
    "building",
    "model",
    "linear",
    "regression",
    "model",
    "typically",
    "know",
    "help",
    "normalize",
    "easy",
    "tensorflow",
    "create",
    "normalizer",
    "layer",
    "going",
    "tensorflow",
    "keras",
    "layers",
    "get",
    "normalization",
    "layer",
    "input",
    "shape",
    "one",
    "let",
    "temperature",
    "access",
    "make",
    "none",
    "temp",
    "normalizer",
    "equal",
    "sign",
    "going",
    "adapt",
    "x",
    "train",
    "temp",
    "reshape",
    "single",
    "vector",
    "work",
    "great",
    "model",
    "temp",
    "neural",
    "net",
    "model",
    "know",
    "dot",
    "keras",
    "sequential",
    "going",
    "pass",
    "normalizer",
    "layer",
    "going",
    "say",
    "hey",
    "give",
    "one",
    "single",
    "dense",
    "layer",
    "one",
    "single",
    "unit",
    "saying",
    "right",
    "well",
    "one",
    "single",
    "node",
    "means",
    "linear",
    "add",
    "sort",
    "activation",
    "function",
    "output",
    "also",
    "linear",
    "going",
    "tensorflow",
    "keras",
    "layers",
    "dot",
    "dense",
    "going",
    "one",
    "unit",
    "going",
    "model",
    "okay",
    "model",
    "let",
    "compile",
    "optimizer",
    "let",
    "use",
    "let",
    "use",
    "atom",
    "dot",
    "atom",
    "pass",
    "learning",
    "rate",
    "learning",
    "rate",
    "learning",
    "rate",
    "let",
    "loss",
    "actually",
    "let",
    "get",
    "one",
    "loss",
    "going",
    "mean",
    "squared",
    "error",
    "okay",
    "run",
    "compiled",
    "okay",
    "great",
    "like",
    "call",
    "history",
    "going",
    "fit",
    "model",
    "call",
    "fit",
    "fit",
    "going",
    "take",
    "x",
    "train",
    "temperature",
    "reshape",
    "train",
    "temperature",
    "going",
    "set",
    "verbose",
    "equal",
    "zero",
    "know",
    "display",
    "stuff",
    "actually",
    "going",
    "set",
    "epochs",
    "equal",
    "let",
    "validation",
    "data",
    "let",
    "pass",
    "validation",
    "data",
    "set",
    "tuple",
    "know",
    "spelled",
    "wrong",
    "let",
    "run",
    "copied",
    "pasted",
    "plot",
    "loss",
    "previous",
    "changed",
    "label",
    "msc",
    "talking",
    "dealing",
    "mean",
    "squared",
    "error",
    "going",
    "plot",
    "loss",
    "history",
    "done",
    "let",
    "wait",
    "finish",
    "training",
    "plot",
    "okay",
    "actually",
    "looks",
    "pretty",
    "good",
    "see",
    "value",
    "still",
    "actually",
    "looks",
    "pretty",
    "good",
    "see",
    "values",
    "converging",
    "going",
    "go",
    "back",
    "take",
    "plot",
    "going",
    "run",
    "plot",
    "instead",
    "temperature",
    "regressor",
    "going",
    "use",
    "neural",
    "net",
    "regressor",
    "neural",
    "net",
    "model",
    "run",
    "see",
    "know",
    "also",
    "gives",
    "linear",
    "regressor",
    "notice",
    "fit",
    "entirely",
    "one",
    "due",
    "training",
    "process",
    "know",
    "neural",
    "net",
    "two",
    "different",
    "ways",
    "try",
    "try",
    "find",
    "best",
    "linear",
    "regressor",
    "okay",
    "using",
    "back",
    "propagation",
    "train",
    "neural",
    "net",
    "node",
    "whereas",
    "one",
    "probably",
    "okay",
    "probably",
    "trying",
    "actually",
    "compute",
    "line",
    "fit",
    "okay",
    "given",
    "well",
    "repeat",
    "exact",
    "exercise",
    "multiple",
    "linear",
    "regressions",
    "okay",
    "actually",
    "going",
    "skip",
    "part",
    "leave",
    "exercise",
    "viewer",
    "okay",
    "would",
    "happen",
    "use",
    "neural",
    "net",
    "real",
    "neural",
    "net",
    "instead",
    "know",
    "one",
    "single",
    "node",
    "order",
    "predict",
    "let",
    "start",
    "code",
    "already",
    "normalizer",
    "actually",
    "going",
    "take",
    "setup",
    "instead",
    "know",
    "one",
    "dense",
    "layer",
    "going",
    "set",
    "equal",
    "32",
    "units",
    "activation",
    "going",
    "use",
    "relu",
    "let",
    "duplicate",
    "final",
    "output",
    "want",
    "one",
    "answer",
    "want",
    "one",
    "cell",
    "activation",
    "also",
    "going",
    "relu",
    "ca",
    "ever",
    "less",
    "zero",
    "bytes",
    "going",
    "set",
    "relu",
    "going",
    "name",
    "neural",
    "net",
    "model",
    "okay",
    "bottom",
    "going",
    "neural",
    "net",
    "model",
    "going",
    "neural",
    "net",
    "model",
    "going",
    "compile",
    "actually",
    "use",
    "compiler",
    "instead",
    "instead",
    "learning",
    "rate",
    "use",
    "okay",
    "going",
    "train",
    "history",
    "neural",
    "net",
    "model",
    "going",
    "fit",
    "x",
    "train",
    "temp",
    "train",
    "temp",
    "valid",
    "validation",
    "data",
    "going",
    "set",
    "equal",
    "x",
    "val",
    "temp",
    "val",
    "temp",
    "verbose",
    "going",
    "say",
    "equal",
    "zero",
    "epochs",
    "let",
    "batch",
    "size",
    "actually",
    "let",
    "batch",
    "size",
    "right",
    "let",
    "try",
    "let",
    "see",
    "happens",
    "plot",
    "loss",
    "history",
    "done",
    "training",
    "let",
    "run",
    "supposed",
    "get",
    "going",
    "sequential",
    "temperature",
    "normalizer",
    "wondering",
    "redo",
    "okay",
    "see",
    "decline",
    "interesting",
    "curve",
    "see",
    "eventually",
    "loss",
    "right",
    "decreasing",
    "good",
    "sign",
    "actually",
    "interesting",
    "let",
    "let",
    "plot",
    "model",
    "instead",
    "see",
    "actually",
    "like",
    "curve",
    "looks",
    "something",
    "like",
    "actually",
    "got",
    "rid",
    "activation",
    "let",
    "train",
    "see",
    "happens",
    "alright",
    "even",
    "even",
    "got",
    "rid",
    "really",
    "end",
    "kind",
    "knows",
    "hey",
    "know",
    "best",
    "model",
    "maybe",
    "one",
    "layer",
    "things",
    "play",
    "around",
    "know",
    "working",
    "machine",
    "learning",
    "like",
    "really",
    "know",
    "best",
    "model",
    "going",
    "example",
    "also",
    "brilliant",
    "guess",
    "okay",
    "point",
    "though",
    "neural",
    "net",
    "mean",
    "brilliant",
    "also",
    "like",
    "data",
    "right",
    "kind",
    "hard",
    "model",
    "predict",
    "fact",
    "probably",
    "started",
    "prediction",
    "somewhere",
    "around",
    "point",
    "though",
    "neural",
    "net",
    "model",
    "see",
    "longer",
    "linear",
    "predictor",
    "yet",
    "still",
    "get",
    "estimate",
    "value",
    "right",
    "repeat",
    "exact",
    "exercise",
    "right",
    "let",
    "right",
    "repeat",
    "exact",
    "exercise",
    "multiple",
    "inputs",
    "pass",
    "data",
    "normalizer",
    "able",
    "pass",
    "let",
    "move",
    "next",
    "cell",
    "going",
    "pass",
    "normalizer",
    "let",
    "compile",
    "yeah",
    "parameters",
    "look",
    "good",
    "great",
    "history",
    "trying",
    "fit",
    "model",
    "instead",
    "temp",
    "going",
    "use",
    "larger",
    "data",
    "set",
    "features",
    "let",
    "train",
    "course",
    "want",
    "plot",
    "loss",
    "okay",
    "loss",
    "looks",
    "like",
    "interesting",
    "curve",
    "decreasing",
    "saw",
    "r",
    "squared",
    "score",
    "around",
    "point",
    "five",
    "two",
    "well",
    "really",
    "neural",
    "net",
    "anymore",
    "one",
    "thing",
    "measure",
    "hey",
    "mean",
    "squared",
    "error",
    "right",
    "come",
    "compare",
    "two",
    "mean",
    "squared",
    "errors",
    "predict",
    "x",
    "test",
    "right",
    "predictions",
    "using",
    "linear",
    "regressor",
    "linear",
    "multiple",
    "multiple",
    "linear",
    "regressor",
    "live",
    "predictions",
    "linear",
    "regression",
    "okay",
    "actually",
    "going",
    "bottom",
    "let",
    "copy",
    "paste",
    "cell",
    "bring",
    "going",
    "calculate",
    "mean",
    "squared",
    "error",
    "linear",
    "regressor",
    "neural",
    "net",
    "okay",
    "linear",
    "neural",
    "net",
    "neural",
    "net",
    "model",
    "predict",
    "x",
    "test",
    "get",
    "two",
    "know",
    "different",
    "predictions",
    "calculate",
    "mean",
    "squared",
    "error",
    "right",
    "want",
    "get",
    "mean",
    "squared",
    "error",
    "prediction",
    "real",
    "numpy",
    "dot",
    "square",
    "would",
    "need",
    "prediction",
    "minus",
    "know",
    "real",
    "basically",
    "squaring",
    "everything",
    "vector",
    "take",
    "entire",
    "thing",
    "take",
    "mean",
    "give",
    "msc",
    "let",
    "try",
    "real",
    "test",
    "right",
    "mean",
    "squared",
    "error",
    "linear",
    "regressor",
    "mean",
    "squared",
    "error",
    "neural",
    "net",
    "interesting",
    "debug",
    "live",
    "guess",
    "guess",
    "probably",
    "coming",
    "normalization",
    "layer",
    "input",
    "shape",
    "probably",
    "six",
    "okay",
    "works",
    "reason",
    "like",
    "inputs",
    "every",
    "vector",
    "one",
    "dimensional",
    "vector",
    "length",
    "six",
    "six",
    "comma",
    "tuple",
    "size",
    "six",
    "start",
    "tuple",
    "containing",
    "one",
    "element",
    "six",
    "okay",
    "actually",
    "interesting",
    "neural",
    "net",
    "results",
    "seem",
    "like",
    "larger",
    "mean",
    "squared",
    "error",
    "linear",
    "regressor",
    "one",
    "thing",
    "look",
    "actually",
    "plot",
    "real",
    "versus",
    "know",
    "actual",
    "results",
    "versus",
    "predictions",
    "say",
    "access",
    "use",
    "plt",
    "dot",
    "axes",
    "make",
    "axes",
    "make",
    "equal",
    "scatter",
    "know",
    "test",
    "actual",
    "values",
    "x",
    "axis",
    "prediction",
    "x",
    "axis",
    "okay",
    "label",
    "linear",
    "regression",
    "predictions",
    "okay",
    "let",
    "label",
    "axes",
    "x",
    "axis",
    "going",
    "say",
    "true",
    "values",
    "axis",
    "going",
    "linear",
    "regression",
    "predictions",
    "actually",
    "let",
    "plot",
    "let",
    "make",
    "predictions",
    "end",
    "going",
    "plot",
    "oh",
    "let",
    "set",
    "limits",
    "think",
    "like",
    "approximately",
    "max",
    "number",
    "bikes",
    "going",
    "set",
    "x",
    "limit",
    "limit",
    "going",
    "pass",
    "right",
    "actually",
    "get",
    "linear",
    "regressor",
    "see",
    "actually",
    "align",
    "quite",
    "well",
    "mean",
    "extent",
    "2000",
    "probably",
    "much",
    "mean",
    "looks",
    "like",
    "maybe",
    "like",
    "1800",
    "would",
    "enough",
    "limits",
    "actually",
    "going",
    "label",
    "something",
    "else",
    "neural",
    "net",
    "predictions",
    "let",
    "add",
    "legend",
    "see",
    "neural",
    "net",
    "larger",
    "values",
    "seems",
    "like",
    "little",
    "bit",
    "spread",
    "seems",
    "like",
    "tend",
    "underestimate",
    "little",
    "bit",
    "area",
    "okay",
    "reason",
    "way",
    "well",
    "yeah",
    "basically",
    "used",
    "linear",
    "regressor",
    "neural",
    "net",
    "honestly",
    "sometimes",
    "neural",
    "net",
    "appropriate",
    "linear",
    "regressor",
    "appropriate",
    "think",
    "comes",
    "time",
    "trying",
    "figure",
    "know",
    "literally",
    "seeing",
    "like",
    "hey",
    "works",
    "better",
    "like",
    "linear",
    "multiple",
    "linear",
    "regressor",
    "might",
    "actually",
    "work",
    "better",
    "neural",
    "net",
    "example",
    "one",
    "dimensional",
    "case",
    "linear",
    "regressor",
    "would",
    "never",
    "able",
    "see",
    "curve",
    "okay",
    "mean",
    "saying",
    "great",
    "model",
    "either",
    "saying",
    "like",
    "hey",
    "know",
    "sometimes",
    "might",
    "appropriate",
    "use",
    "something",
    "linear",
    "yeah",
    "leave",
    "regression",
    "okay",
    "talked",
    "supervised",
    "learning",
    "supervised",
    "learning",
    "data",
    "bunch",
    "features",
    "bunch",
    "different",
    "samples",
    "samples",
    "sort",
    "label",
    "whether",
    "number",
    "category",
    "class",
    "etc",
    "right",
    "able",
    "use",
    "label",
    "order",
    "try",
    "predict",
    "right",
    "able",
    "use",
    "label",
    "order",
    "try",
    "predict",
    "new",
    "labels",
    "points",
    "seen",
    "yet",
    "well",
    "let",
    "move",
    "unsupervised",
    "learning",
    "unsupervised",
    "learning",
    "bunch",
    "unlabeled",
    "data",
    "know",
    "learn",
    "anything",
    "data",
    "first",
    "algorithm",
    "going",
    "discuss",
    "known",
    "k",
    "means",
    "clustering",
    "k",
    "means",
    "clustering",
    "trying",
    "trying",
    "compute",
    "k",
    "clusters",
    "data",
    "example",
    "bunch",
    "scattered",
    "points",
    "see",
    "x",
    "zero",
    "x",
    "one",
    "two",
    "axes",
    "means",
    "actually",
    "plotting",
    "two",
    "different",
    "features",
    "right",
    "point",
    "know",
    "label",
    "points",
    "looking",
    "scattered",
    "points",
    "kind",
    "see",
    "different",
    "clusters",
    "data",
    "set",
    "right",
    "depending",
    "pick",
    "k",
    "might",
    "different",
    "clusters",
    "let",
    "say",
    "k",
    "equals",
    "two",
    "right",
    "might",
    "pick",
    "okay",
    "seems",
    "like",
    "could",
    "one",
    "cluster",
    "also",
    "another",
    "cluster",
    "might",
    "two",
    "different",
    "clusters",
    "k",
    "equals",
    "three",
    "example",
    "okay",
    "seems",
    "like",
    "could",
    "cluster",
    "seems",
    "like",
    "could",
    "cluster",
    "maybe",
    "could",
    "cluster",
    "right",
    "could",
    "three",
    "different",
    "clusters",
    "data",
    "set",
    "k",
    "predefined",
    "spell",
    "correctly",
    "person",
    "running",
    "model",
    "would",
    "right",
    "let",
    "discuss",
    "know",
    "computer",
    "actually",
    "goes",
    "computes",
    "k",
    "clusters",
    "going",
    "write",
    "steps",
    "first",
    "step",
    "happens",
    "actually",
    "choose",
    "well",
    "computer",
    "chooses",
    "three",
    "random",
    "points",
    "plot",
    "centroids",
    "centuries",
    "mean",
    "center",
    "clusters",
    "okay",
    "three",
    "random",
    "points",
    "let",
    "say",
    "k",
    "equals",
    "three",
    "choosing",
    "three",
    "random",
    "points",
    "centroids",
    "three",
    "clusters",
    "two",
    "choosing",
    "two",
    "random",
    "points",
    "okay",
    "maybe",
    "three",
    "random",
    "points",
    "choosing",
    "might",
    "right",
    "three",
    "different",
    "points",
    "second",
    "thing",
    "actually",
    "calculate",
    "distance",
    "point",
    "centroids",
    "points",
    "centroid",
    "basically",
    "saying",
    "right",
    "distance",
    "distance",
    "distance",
    "distances",
    "computing",
    "oops",
    "two",
    "points",
    "centroids",
    "computing",
    "distances",
    "plots",
    "centroids",
    "okay",
    "comes",
    "also",
    "assigning",
    "points",
    "closest",
    "centroid",
    "mean",
    "let",
    "take",
    "point",
    "example",
    "computing",
    "distance",
    "distance",
    "distance",
    "saying",
    "okay",
    "seems",
    "like",
    "red",
    "one",
    "closest",
    "actually",
    "going",
    "put",
    "red",
    "centroid",
    "points",
    "seems",
    "slightly",
    "closer",
    "red",
    "one",
    "seems",
    "slightly",
    "closer",
    "red",
    "right",
    "blue",
    "actually",
    "would",
    "put",
    "blue",
    "ones",
    "would",
    "probably",
    "actually",
    "first",
    "one",
    "closer",
    "red",
    "seems",
    "like",
    "rest",
    "probably",
    "closer",
    "green",
    "let",
    "put",
    "green",
    "like",
    "cool",
    "know",
    "two",
    "three",
    "technically",
    "centroid",
    "group",
    "group",
    "blue",
    "kind",
    "group",
    "really",
    "touched",
    "points",
    "yet",
    "next",
    "step",
    "three",
    "actually",
    "go",
    "recalculate",
    "centroid",
    "compute",
    "new",
    "centroids",
    "based",
    "points",
    "centroids",
    "mean",
    "okay",
    "well",
    "let",
    "take",
    "average",
    "points",
    "new",
    "centroid",
    "probably",
    "going",
    "somewhere",
    "around",
    "right",
    "blue",
    "one",
    "points",
    "wo",
    "touch",
    "screen",
    "one",
    "put",
    "probably",
    "somewhere",
    "oops",
    "somewhere",
    "right",
    "erase",
    "previously",
    "computed",
    "centroids",
    "go",
    "actually",
    "redo",
    "step",
    "two",
    "calculation",
    "alright",
    "going",
    "go",
    "back",
    "going",
    "iterate",
    "everything",
    "going",
    "recompute",
    "three",
    "centroids",
    "let",
    "see",
    "going",
    "take",
    "red",
    "point",
    "definitely",
    "red",
    "right",
    "one",
    "still",
    "looks",
    "bit",
    "red",
    "part",
    "actually",
    "start",
    "getting",
    "closer",
    "blues",
    "one",
    "still",
    "seems",
    "closer",
    "blue",
    "green",
    "one",
    "well",
    "think",
    "rest",
    "would",
    "belong",
    "green",
    "okay",
    "three",
    "centroids",
    "three",
    "sorry",
    "three",
    "clusters",
    "would",
    "right",
    "three",
    "centroids",
    "go",
    "back",
    "compute",
    "new",
    "sorry",
    "would",
    "three",
    "clusters",
    "go",
    "back",
    "compute",
    "three",
    "centroids",
    "going",
    "get",
    "rid",
    "would",
    "red",
    "centered",
    "probably",
    "closer",
    "know",
    "point",
    "blue",
    "might",
    "closer",
    "green",
    "would",
    "probably",
    "somewhere",
    "pretty",
    "similar",
    "seems",
    "like",
    "pulled",
    "bit",
    "probably",
    "somewhere",
    "around",
    "green",
    "right",
    "go",
    "back",
    "compute",
    "distance",
    "points",
    "centroids",
    "assign",
    "closest",
    "centroid",
    "okay",
    "reds",
    "clear",
    "actually",
    "let",
    "circle",
    "actually",
    "seems",
    "like",
    "point",
    "actually",
    "seemed",
    "like",
    "point",
    "closer",
    "blue",
    "blues",
    "seem",
    "like",
    "would",
    "maybe",
    "point",
    "looks",
    "like",
    "blue",
    "look",
    "like",
    "would",
    "blue",
    "greens",
    "would",
    "probably",
    "cluster",
    "right",
    "go",
    "back",
    "compute",
    "centroids",
    "bam",
    "one",
    "probably",
    "like",
    "almost",
    "bam",
    "green",
    "looks",
    "like",
    "would",
    "probably",
    "ish",
    "okay",
    "go",
    "back",
    "compute",
    "compute",
    "clusters",
    "red",
    "still",
    "blue",
    "would",
    "argue",
    "cluster",
    "green",
    "cluster",
    "okay",
    "go",
    "recompute",
    "centroids",
    "bam",
    "bam",
    "know",
    "bam",
    "go",
    "assign",
    "points",
    "clusters",
    "would",
    "get",
    "exact",
    "thing",
    "right",
    "know",
    "stop",
    "iterating",
    "steps",
    "two",
    "three",
    "converged",
    "solution",
    "reached",
    "stable",
    "point",
    "none",
    "points",
    "really",
    "changing",
    "clusters",
    "anymore",
    "go",
    "back",
    "user",
    "say",
    "hey",
    "three",
    "clusters",
    "okay",
    "process",
    "something",
    "known",
    "expectation",
    "maximization",
    "part",
    "assigning",
    "points",
    "closest",
    "centroid",
    "something",
    "expectation",
    "step",
    "part",
    "computing",
    "new",
    "centroids",
    "maximization",
    "step",
    "okay",
    "expectation",
    "maximization",
    "use",
    "order",
    "compute",
    "centroids",
    "assign",
    "points",
    "clusters",
    "according",
    "centroids",
    "recomputing",
    "reach",
    "stable",
    "point",
    "nothing",
    "changing",
    "anymore",
    "alright",
    "first",
    "example",
    "unsupervised",
    "learning",
    "basically",
    "trying",
    "find",
    "structure",
    "pattern",
    "data",
    "came",
    "another",
    "point",
    "know",
    "might",
    "somewhere",
    "say",
    "oh",
    "looks",
    "like",
    "closer",
    "b",
    "c",
    "looks",
    "like",
    "closest",
    "cluster",
    "would",
    "probably",
    "put",
    "cluster",
    "okay",
    "find",
    "structure",
    "data",
    "based",
    "points",
    "scattered",
    "relative",
    "one",
    "another",
    "second",
    "unsupervised",
    "learning",
    "technique",
    "going",
    "discuss",
    "guys",
    "something",
    "noted",
    "principal",
    "component",
    "analysis",
    "point",
    "principal",
    "component",
    "analysis",
    "often",
    "used",
    "dimensionality",
    "reduction",
    "technique",
    "let",
    "write",
    "used",
    "dimensionality",
    "reduction",
    "mean",
    "dimensionality",
    "reduction",
    "bunch",
    "features",
    "like",
    "x1",
    "x2",
    "x3",
    "x4",
    "etc",
    "reduce",
    "one",
    "dimension",
    "gives",
    "information",
    "points",
    "spread",
    "relative",
    "one",
    "another",
    "pca",
    "pca",
    "principal",
    "component",
    "analysis",
    "let",
    "say",
    "points",
    "x",
    "zero",
    "x",
    "one",
    "feature",
    "space",
    "okay",
    "points",
    "might",
    "spread",
    "know",
    "something",
    "like",
    "okay",
    "example",
    "something",
    "housing",
    "prices",
    "right",
    "might",
    "x",
    "zero",
    "might",
    "hey",
    "years",
    "since",
    "built",
    "right",
    "since",
    "house",
    "built",
    "x",
    "one",
    "might",
    "square",
    "footage",
    "house",
    "alright",
    "like",
    "years",
    "since",
    "built",
    "mean",
    "like",
    "right",
    "know",
    "22",
    "years",
    "since",
    "house",
    "2000",
    "built",
    "principal",
    "component",
    "analysis",
    "saying",
    "alright",
    "let",
    "say",
    "want",
    "build",
    "model",
    "let",
    "say",
    "want",
    "know",
    "display",
    "something",
    "data",
    "two",
    "axes",
    "show",
    "display",
    "know",
    "demonstrate",
    "point",
    "away",
    "point",
    "point",
    "using",
    "principal",
    "component",
    "analysis",
    "take",
    "know",
    "linear",
    "regression",
    "forget",
    "second",
    "otherwise",
    "might",
    "get",
    "confused",
    "pca",
    "way",
    "trying",
    "find",
    "direction",
    "space",
    "largest",
    "variance",
    "principal",
    "component",
    "means",
    "basically",
    "component",
    "direction",
    "space",
    "largest",
    "variance",
    "okay",
    "tells",
    "us",
    "data",
    "set",
    "without",
    "two",
    "different",
    "dimensions",
    "like",
    "let",
    "say",
    "two",
    "different",
    "mentions",
    "somebody",
    "telling",
    "us",
    "hey",
    "get",
    "one",
    "dimension",
    "order",
    "show",
    "data",
    "set",
    "dimension",
    "want",
    "show",
    "us",
    "okay",
    "let",
    "say",
    "want",
    "show",
    "data",
    "set",
    "dimension",
    "like",
    "want",
    "project",
    "data",
    "onto",
    "single",
    "dimension",
    "alright",
    "case",
    "might",
    "dimension",
    "looks",
    "something",
    "like",
    "might",
    "say",
    "okay",
    "going",
    "talk",
    "linear",
    "regression",
    "okay",
    "value",
    "linear",
    "regression",
    "would",
    "okay",
    "label",
    "instead",
    "taking",
    "right",
    "angle",
    "projection",
    "take",
    "visible",
    "take",
    "right",
    "angle",
    "projection",
    "onto",
    "line",
    "pca",
    "saying",
    "okay",
    "map",
    "points",
    "onto",
    "one",
    "dimensional",
    "space",
    "transformed",
    "data",
    "set",
    "would",
    "one",
    "data",
    "sets",
    "line",
    "put",
    "would",
    "new",
    "one",
    "dimensional",
    "data",
    "set",
    "okay",
    "prediction",
    "anything",
    "new",
    "data",
    "set",
    "somebody",
    "came",
    "us",
    "said",
    "get",
    "one",
    "dimension",
    "get",
    "one",
    "number",
    "represent",
    "2d",
    "points",
    "number",
    "would",
    "give",
    "us",
    "number",
    "would",
    "give",
    "us",
    "would",
    "new",
    "one",
    "dimensional",
    "data",
    "set",
    "okay",
    "prediction",
    "anything",
    "number",
    "would",
    "give",
    "would",
    "number",
    "gave",
    "okay",
    "direction",
    "points",
    "spread",
    "right",
    "took",
    "plot",
    "let",
    "actually",
    "duplicate",
    "rewrite",
    "anything",
    "erase",
    "redraw",
    "anything",
    "let",
    "get",
    "rid",
    "stuff",
    "got",
    "rid",
    "point",
    "let",
    "draw",
    "back",
    "alright",
    "original",
    "data",
    "point",
    "taken",
    "know",
    "pca",
    "dimension",
    "okay",
    "well",
    "would",
    "points",
    "let",
    "actually",
    "different",
    "color",
    "draw",
    "right",
    "angle",
    "every",
    "point",
    "points",
    "would",
    "look",
    "something",
    "like",
    "intuitively",
    "looking",
    "two",
    "different",
    "plots",
    "top",
    "one",
    "one",
    "see",
    "points",
    "squished",
    "little",
    "bit",
    "closer",
    "together",
    "right",
    "means",
    "variance",
    "space",
    "largest",
    "variance",
    "thing",
    "largest",
    "variance",
    "give",
    "us",
    "discrimination",
    "points",
    "larger",
    "variance",
    "spread",
    "points",
    "likely",
    "dimension",
    "project",
    "different",
    "way",
    "actually",
    "look",
    "like",
    "dimension",
    "largest",
    "variance",
    "actually",
    "also",
    "happens",
    "dimension",
    "decreases",
    "dimension",
    "decreases",
    "minimizes",
    "residuals",
    "take",
    "points",
    "take",
    "residual",
    "xy",
    "residual",
    "linear",
    "regression",
    "linear",
    "regression",
    "looking",
    "residual",
    "differences",
    "predictions",
    "right",
    "hat",
    "principal",
    "component",
    "analysis",
    "taking",
    "difference",
    "current",
    "point",
    "two",
    "dimensional",
    "space",
    "projected",
    "point",
    "okay",
    "taking",
    "dimension",
    "saying",
    "alright",
    "much",
    "know",
    "much",
    "distance",
    "projection",
    "residual",
    "trying",
    "minimize",
    "points",
    "actually",
    "equates",
    "largest",
    "variance",
    "dimension",
    "dimension",
    "pca",
    "dimension",
    "either",
    "look",
    "minimizing",
    "minimize",
    "let",
    "get",
    "rid",
    "projection",
    "residuals",
    "stuff",
    "orange",
    "maximizing",
    "variance",
    "points",
    "okay",
    "really",
    "going",
    "talk",
    "know",
    "method",
    "need",
    "order",
    "calculate",
    "principal",
    "components",
    "like",
    "projection",
    "would",
    "need",
    "understand",
    "linear",
    "algebra",
    "especially",
    "eigenvectors",
    "eigenvalues",
    "going",
    "cover",
    "class",
    "would",
    "find",
    "principal",
    "components",
    "okay",
    "two",
    "dimensional",
    "data",
    "set",
    "sorry",
    "one",
    "dimensional",
    "data",
    "set",
    "started",
    "2d",
    "data",
    "set",
    "boil",
    "one",
    "dimension",
    "well",
    "go",
    "take",
    "dimension",
    "things",
    "right",
    "like",
    "label",
    "show",
    "x",
    "versus",
    "rather",
    "x",
    "zero",
    "x",
    "one",
    "different",
    "plots",
    "say",
    "oh",
    "principal",
    "component",
    "going",
    "plot",
    "example",
    "100",
    "different",
    "dimensions",
    "wanted",
    "take",
    "five",
    "well",
    "could",
    "go",
    "could",
    "find",
    "top",
    "five",
    "pca",
    "dimensions",
    "might",
    "lot",
    "useful",
    "100",
    "different",
    "feature",
    "vector",
    "values",
    "right",
    "principal",
    "component",
    "analysis",
    "taking",
    "know",
    "certain",
    "data",
    "unlabeled",
    "trying",
    "make",
    "sort",
    "estimation",
    "like",
    "guess",
    "structure",
    "original",
    "data",
    "set",
    "wanted",
    "take",
    "know",
    "3d",
    "thing",
    "like",
    "sphere",
    "2d",
    "surface",
    "draw",
    "well",
    "best",
    "approximation",
    "make",
    "oh",
    "circle",
    "right",
    "pca",
    "kind",
    "thing",
    "saying",
    "something",
    "different",
    "dimensions",
    "ca",
    "show",
    "boil",
    "one",
    "dimension",
    "extract",
    "information",
    "multiple",
    "dimensions",
    "exactly",
    "either",
    "minimize",
    "projection",
    "residuals",
    "maximize",
    "variance",
    "pca",
    "go",
    "example",
    "finally",
    "let",
    "move",
    "implementing",
    "unsupervised",
    "learning",
    "part",
    "class",
    "uci",
    "machine",
    "learning",
    "repository",
    "seeds",
    "data",
    "set",
    "know",
    "bunch",
    "kernels",
    "belong",
    "three",
    "different",
    "types",
    "wheat",
    "comma",
    "rosa",
    "canadian",
    "different",
    "features",
    "access",
    "know",
    "geometric",
    "parameters",
    "wheat",
    "kernels",
    "area",
    "perimeter",
    "compactness",
    "length",
    "width",
    "width",
    "asymmetry",
    "length",
    "kernel",
    "groove",
    "okay",
    "real",
    "values",
    "easy",
    "work",
    "going",
    "going",
    "try",
    "predict",
    "guess",
    "going",
    "try",
    "cluster",
    "different",
    "varieties",
    "wheat",
    "let",
    "get",
    "started",
    "colab",
    "notebook",
    "open",
    "oh",
    "gon",
    "na",
    "know",
    "go",
    "data",
    "folder",
    "download",
    "going",
    "go",
    "data",
    "folder",
    "download",
    "let",
    "get",
    "started",
    "first",
    "thing",
    "import",
    "seeds",
    "data",
    "set",
    "colab",
    "notebook",
    "done",
    "okay",
    "going",
    "import",
    "classics",
    "pandas",
    "also",
    "going",
    "import",
    "seedborn",
    "going",
    "want",
    "specific",
    "class",
    "okay",
    "great",
    "columns",
    "seed",
    "data",
    "set",
    "area",
    "perimeter",
    "compactness",
    "length",
    "asymmetry",
    "groove",
    "length",
    "mean",
    "going",
    "call",
    "groove",
    "class",
    "right",
    "wheat",
    "kernels",
    "class",
    "import",
    "going",
    "using",
    "pandas",
    "read",
    "csv",
    "called",
    "seeds",
    "going",
    "turn",
    "data",
    "frame",
    "names",
    "equal",
    "columns",
    "happens",
    "oops",
    "call",
    "seeds",
    "data",
    "set",
    "text",
    "alright",
    "actually",
    "look",
    "data",
    "frame",
    "right",
    "notice",
    "something",
    "funky",
    "okay",
    "know",
    "stuff",
    "area",
    "numbers",
    "dash",
    "reason",
    "actually",
    "told",
    "pandas",
    "separator",
    "like",
    "tab",
    "order",
    "ensure",
    "like",
    "whitespace",
    "gets",
    "recognized",
    "separator",
    "actually",
    "like",
    "space",
    "spaces",
    "going",
    "get",
    "recognized",
    "data",
    "separators",
    "run",
    "know",
    "lot",
    "better",
    "okay",
    "okay",
    "let",
    "actually",
    "go",
    "like",
    "visualize",
    "data",
    "actually",
    "going",
    "plot",
    "one",
    "another",
    "case",
    "pretend",
    "access",
    "class",
    "right",
    "pretend",
    "class",
    "going",
    "show",
    "example",
    "like",
    "hey",
    "predict",
    "classes",
    "using",
    "unsupervised",
    "learning",
    "example",
    "unsupervised",
    "learning",
    "actually",
    "access",
    "class",
    "going",
    "try",
    "plot",
    "one",
    "another",
    "see",
    "happens",
    "range",
    "know",
    "columns",
    "minus",
    "one",
    "classes",
    "columns",
    "going",
    "say",
    "j",
    "range",
    "take",
    "everything",
    "onwards",
    "know",
    "like",
    "next",
    "thing",
    "end",
    "give",
    "us",
    "basically",
    "grid",
    "different",
    "like",
    "combinations",
    "x",
    "label",
    "going",
    "columns",
    "label",
    "going",
    "columns",
    "labels",
    "going",
    "use",
    "seaborne",
    "time",
    "going",
    "say",
    "scatter",
    "data",
    "x",
    "going",
    "x",
    "label",
    "going",
    "label",
    "data",
    "going",
    "data",
    "frame",
    "passing",
    "interesting",
    "say",
    "hue",
    "say",
    "like",
    "give",
    "class",
    "going",
    "separate",
    "three",
    "different",
    "classes",
    "three",
    "different",
    "hues",
    "basically",
    "comparing",
    "area",
    "perimeter",
    "area",
    "compactness",
    "going",
    "visualize",
    "know",
    "classes",
    "let",
    "go",
    "ahead",
    "might",
    "show",
    "great",
    "basically",
    "see",
    "perimeter",
    "area",
    "give",
    "get",
    "three",
    "groups",
    "area",
    "compactness",
    "get",
    "three",
    "groups",
    "kind",
    "look",
    "honestly",
    "like",
    "somewhat",
    "similar",
    "right",
    "wow",
    "look",
    "one",
    "one",
    "compactness",
    "asymmetry",
    "looks",
    "like",
    "really",
    "mean",
    "looks",
    "like",
    "blobs",
    "right",
    "sure",
    "maybe",
    "class",
    "three",
    "one",
    "two",
    "kind",
    "look",
    "like",
    "top",
    "okay",
    "mean",
    "might",
    "look",
    "slightly",
    "better",
    "terms",
    "clustering",
    "let",
    "go",
    "clustering",
    "examples",
    "talked",
    "try",
    "implement",
    "first",
    "thing",
    "going",
    "straight",
    "clustering",
    "learned",
    "k",
    "means",
    "clustering",
    "sk",
    "learn",
    "going",
    "import",
    "k",
    "means",
    "okay",
    "sake",
    "able",
    "run",
    "know",
    "x",
    "going",
    "say",
    "hey",
    "let",
    "use",
    "good",
    "one",
    "maybe",
    "mean",
    "perimeter",
    "asymmetry",
    "could",
    "good",
    "one",
    "x",
    "could",
    "perimeter",
    "could",
    "asymmetry",
    "okay",
    "x",
    "values",
    "going",
    "extract",
    "specific",
    "values",
    "alright",
    "well",
    "let",
    "make",
    "k",
    "means",
    "algorithm",
    "let",
    "know",
    "define",
    "k",
    "means",
    "specific",
    "case",
    "know",
    "number",
    "clusters",
    "three",
    "let",
    "use",
    "going",
    "fit",
    "x",
    "defined",
    "right",
    "right",
    "know",
    "create",
    "clusters",
    "one",
    "thing",
    "one",
    "cool",
    "thing",
    "actually",
    "go",
    "clusters",
    "say",
    "k",
    "mean",
    "dot",
    "labels",
    "give",
    "give",
    "type",
    "correctly",
    "give",
    "predictions",
    "clusters",
    "actual",
    "oops",
    "go",
    "data",
    "frame",
    "get",
    "class",
    "values",
    "actually",
    "compare",
    "two",
    "say",
    "hey",
    "like",
    "know",
    "everything",
    "general",
    "zeros",
    "predicted",
    "ones",
    "right",
    "general",
    "twos",
    "twos",
    "third",
    "class",
    "one",
    "okay",
    "corresponds",
    "three",
    "remember",
    "separate",
    "classes",
    "labels",
    "actually",
    "call",
    "really",
    "matter",
    "say",
    "map",
    "zero",
    "one",
    "map",
    "two",
    "two",
    "map",
    "one",
    "three",
    "okay",
    "know",
    "mapping",
    "would",
    "fairly",
    "well",
    "actually",
    "visualize",
    "order",
    "going",
    "create",
    "cluster",
    "cluster",
    "data",
    "frame",
    "going",
    "create",
    "data",
    "frame",
    "going",
    "pass",
    "horizontally",
    "stacked",
    "array",
    "x",
    "values",
    "x",
    "clusters",
    "going",
    "reshape",
    "2d",
    "okay",
    "columns",
    "labels",
    "going",
    "x",
    "plus",
    "okay",
    "going",
    "go",
    "ahead",
    "seaborne",
    "scatter",
    "plot",
    "x",
    "x",
    "hue",
    "class",
    "data",
    "cluster",
    "data",
    "frame",
    "alright",
    "k",
    "means",
    "like",
    "guess",
    "classes",
    "k",
    "means",
    "kind",
    "looks",
    "like",
    "come",
    "plot",
    "know",
    "original",
    "data",
    "frame",
    "original",
    "classes",
    "respect",
    "specific",
    "x",
    "see",
    "honestly",
    "like",
    "poorly",
    "yeah",
    "mean",
    "colors",
    "different",
    "fine",
    "part",
    "gets",
    "information",
    "clusters",
    "right",
    "higher",
    "dimensions",
    "higher",
    "dimensions",
    "make",
    "x",
    "equal",
    "know",
    "columns",
    "except",
    "last",
    "one",
    "class",
    "exact",
    "thing",
    "exact",
    "thing",
    "predict",
    "columns",
    "equal",
    "data",
    "frame",
    "columns",
    "way",
    "last",
    "one",
    "class",
    "actually",
    "literally",
    "say",
    "data",
    "frame",
    "columns",
    "fit",
    "want",
    "plot",
    "k",
    "means",
    "classes",
    "alright",
    "clustered",
    "original",
    "actually",
    "let",
    "see",
    "get",
    "page",
    "yeah",
    "mean",
    "pretty",
    "similar",
    "saw",
    "actually",
    "really",
    "cool",
    "even",
    "something",
    "like",
    "know",
    "change",
    "one",
    "like",
    "top",
    "okay",
    "compactness",
    "asymmetry",
    "one",
    "messy",
    "right",
    "come",
    "say",
    "compactness",
    "asymmetry",
    "trying",
    "2d",
    "scatterplot",
    "know",
    "k",
    "means",
    "telling",
    "two",
    "dimensions",
    "compactness",
    "asymmetry",
    "look",
    "two",
    "three",
    "classes",
    "right",
    "know",
    "original",
    "looks",
    "something",
    "like",
    "two",
    "remotely",
    "alike",
    "okay",
    "come",
    "back",
    "rerun",
    "higher",
    "dimensions",
    "one",
    "actually",
    "clusters",
    "need",
    "get",
    "labels",
    "k",
    "means",
    "okay",
    "rerun",
    "higher",
    "dimensions",
    "well",
    "zoom",
    "take",
    "look",
    "two",
    "sure",
    "colors",
    "mixed",
    "general",
    "three",
    "groups",
    "right",
    "much",
    "better",
    "job",
    "assessing",
    "okay",
    "group",
    "example",
    "could",
    "relabel",
    "one",
    "original",
    "class",
    "two",
    "could",
    "make",
    "sorry",
    "okay",
    "kind",
    "confusing",
    "example",
    "light",
    "pink",
    "projected",
    "onto",
    "darker",
    "pink",
    "dark",
    "one",
    "actually",
    "light",
    "pink",
    "light",
    "one",
    "dark",
    "one",
    "kind",
    "see",
    "like",
    "correspond",
    "one",
    "another",
    "right",
    "like",
    "even",
    "two",
    "class",
    "ones",
    "color",
    "want",
    "compare",
    "two",
    "colors",
    "plots",
    "want",
    "compare",
    "points",
    "colors",
    "plots",
    "one",
    "cool",
    "application",
    "k",
    "means",
    "functions",
    "basically",
    "taking",
    "data",
    "sets",
    "saying",
    "right",
    "clusters",
    "given",
    "pieces",
    "data",
    "next",
    "thing",
    "talked",
    "pca",
    "pca",
    "reducing",
    "dimension",
    "mapping",
    "like",
    "know",
    "seven",
    "dimensions",
    "know",
    "seven",
    "made",
    "number",
    "mapping",
    "multiple",
    "dimensions",
    "lower",
    "dimension",
    "number",
    "right",
    "let",
    "see",
    "works",
    "sk",
    "learn",
    "decomposition",
    "import",
    "pca",
    "pca",
    "model",
    "pca",
    "component",
    "many",
    "dimensions",
    "want",
    "map",
    "know",
    "exercise",
    "let",
    "two",
    "okay",
    "taking",
    "top",
    "two",
    "dimensions",
    "transformed",
    "x",
    "going",
    "pca",
    "dot",
    "fit",
    "transform",
    "x",
    "x",
    "okay",
    "values",
    "basically",
    "area",
    "perimeter",
    "compactness",
    "length",
    "width",
    "asymmetry",
    "groove",
    "okay",
    "let",
    "run",
    "transformed",
    "let",
    "look",
    "shape",
    "x",
    "used",
    "okay",
    "seven",
    "right",
    "210",
    "samples",
    "seven",
    "seven",
    "features",
    "long",
    "basically",
    "transformed",
    "x",
    "210",
    "samples",
    "length",
    "two",
    "means",
    "two",
    "dimensions",
    "plotting",
    "actually",
    "even",
    "take",
    "look",
    "know",
    "first",
    "five",
    "things",
    "okay",
    "see",
    "one",
    "two",
    "dimensional",
    "point",
    "sample",
    "two",
    "dimensional",
    "point",
    "new",
    "new",
    "dimensions",
    "cool",
    "actually",
    "scatter",
    "zero",
    "transformed",
    "actually",
    "take",
    "columns",
    "show",
    "basically",
    "taken",
    "like",
    "seven",
    "dimensional",
    "thing",
    "made",
    "single",
    "guess",
    "two",
    "dimensional",
    "representation",
    "point",
    "pca",
    "actually",
    "let",
    "go",
    "ahead",
    "clustering",
    "exercise",
    "take",
    "k",
    "means",
    "pca",
    "data",
    "frame",
    "let",
    "construct",
    "data",
    "frame",
    "data",
    "frame",
    "going",
    "h",
    "stack",
    "going",
    "take",
    "transformed",
    "x",
    "clusters",
    "reshape",
    "actually",
    "instead",
    "clusters",
    "going",
    "use",
    "k",
    "means",
    "dot",
    "labels",
    "need",
    "reshape",
    "2d",
    "h",
    "stack",
    "columns",
    "going",
    "set",
    "pca",
    "one",
    "pca",
    "two",
    "class",
    "right",
    "take",
    "also",
    "truth",
    "instead",
    "k",
    "means",
    "labels",
    "want",
    "data",
    "frame",
    "original",
    "classes",
    "going",
    "take",
    "values",
    "data",
    "frame",
    "k",
    "means",
    "pca",
    "data",
    "frame",
    "truth",
    "also",
    "pca",
    "plot",
    "similarly",
    "plotted",
    "let",
    "actually",
    "take",
    "two",
    "instead",
    "cluster",
    "data",
    "frame",
    "want",
    "k",
    "means",
    "pca",
    "data",
    "frame",
    "still",
    "going",
    "class",
    "x",
    "going",
    "two",
    "pca",
    "dimensions",
    "okay",
    "two",
    "pca",
    "dimensions",
    "see",
    "data",
    "frame",
    "going",
    "cluster",
    "data",
    "frame",
    "two",
    "pca",
    "dimensions",
    "see",
    "know",
    "pretty",
    "spread",
    "going",
    "go",
    "truth",
    "classes",
    "pca",
    "one",
    "pca",
    "two",
    "instead",
    "k",
    "means",
    "truth",
    "pca",
    "data",
    "frame",
    "see",
    "like",
    "truth",
    "data",
    "frame",
    "along",
    "two",
    "dimensions",
    "actually",
    "fairly",
    "well",
    "terms",
    "separation",
    "right",
    "seem",
    "like",
    "slightly",
    "separable",
    "like",
    "dimensions",
    "looking",
    "good",
    "sign",
    "see",
    "hey",
    "correspond",
    "one",
    "another",
    "mean",
    "part",
    "algorithm",
    "unsupervised",
    "clustering",
    "algorithm",
    "able",
    "give",
    "us",
    "able",
    "spit",
    "know",
    "proper",
    "labels",
    "mean",
    "map",
    "specific",
    "labels",
    "different",
    "types",
    "kernels",
    "example",
    "one",
    "might",
    "comma",
    "kernel",
    "kernels",
    "might",
    "canadian",
    "kernels",
    "might",
    "canadian",
    "kernels",
    "struggle",
    "little",
    "bit",
    "know",
    "overlap",
    "part",
    "algorithm",
    "able",
    "find",
    "three",
    "different",
    "categories",
    "fairly",
    "good",
    "job",
    "predicting",
    "without",
    "without",
    "information",
    "us",
    "given",
    "algorithm",
    "labels",
    "gist",
    "unsupervised",
    "learning",
    "hope",
    "guys",
    "enjoyed",
    "course",
    "hope",
    "know",
    "lot",
    "examples",
    "made",
    "sense",
    "certain",
    "things",
    "done",
    "know",
    "somebody",
    "experience",
    "please",
    "let",
    "know",
    "comments",
    "community",
    "learn",
    "together",
    "thank",
    "watching"
  ],
  "keywords": [
    "many",
    "code",
    "basically",
    "going",
    "machine",
    "learning",
    "way",
    "absolute",
    "guys",
    "think",
    "talk",
    "supervised",
    "unsupervised",
    "models",
    "go",
    "maybe",
    "little",
    "bit",
    "also",
    "see",
    "certain",
    "things",
    "done",
    "know",
    "somebody",
    "learn",
    "together",
    "let",
    "right",
    "data",
    "sets",
    "really",
    "cool",
    "one",
    "called",
    "set",
    "want",
    "actually",
    "use",
    "order",
    "predict",
    "type",
    "whether",
    "like",
    "length",
    "size",
    "asymmetry",
    "etc",
    "us",
    "come",
    "zero",
    "dot",
    "start",
    "new",
    "call",
    "example",
    "okay",
    "first",
    "import",
    "yeah",
    "run",
    "cell",
    "either",
    "computer",
    "got",
    "thing",
    "literally",
    "take",
    "look",
    "labels",
    "mean",
    "could",
    "pass",
    "seem",
    "label",
    "back",
    "make",
    "columns",
    "column",
    "values",
    "create",
    "f",
    "three",
    "class",
    "great",
    "frame",
    "equal",
    "give",
    "five",
    "might",
    "h",
    "good",
    "equals",
    "say",
    "entire",
    "true",
    "guess",
    "would",
    "false",
    "step",
    "different",
    "sample",
    "point",
    "kind",
    "oh",
    "whatever",
    "samples",
    "value",
    "specific",
    "try",
    "something",
    "known",
    "classification",
    "features",
    "model",
    "case",
    "10",
    "move",
    "well",
    "exactly",
    "difference",
    "area",
    "predictions",
    "using",
    "find",
    "draw",
    "types",
    "inputs",
    "means",
    "input",
    "get",
    "output",
    "train",
    "color",
    "able",
    "dog",
    "points",
    "bunch",
    "terms",
    "cluster",
    "hey",
    "sort",
    "given",
    "tell",
    "probably",
    "keep",
    "alright",
    "looks",
    "prediction",
    "feature",
    "vector",
    "number",
    "categories",
    "two",
    "another",
    "rate",
    "hot",
    "saying",
    "category",
    "side",
    "lot",
    "closer",
    "close",
    "real",
    "temperature",
    "classes",
    "put",
    "binary",
    "positive",
    "negative",
    "trying",
    "figure",
    "regression",
    "instead",
    "fit",
    "even",
    "row",
    "measure",
    "x",
    "compare",
    "whole",
    "next",
    "training",
    "seen",
    "validation",
    "loss",
    "still",
    "every",
    "single",
    "gets",
    "pretty",
    "far",
    "b",
    "better",
    "best",
    "c",
    "test",
    "slightly",
    "higher",
    "function",
    "sum",
    "taking",
    "everything",
    "much",
    "square",
    "need",
    "accuracy",
    "stuff",
    "already",
    "ones",
    "plot",
    "anything",
    "last",
    "remember",
    "part",
    "blue",
    "probability",
    "show",
    "red",
    "seems",
    "split",
    "times",
    "sk",
    "2d",
    "stack",
    "top",
    "dimensions",
    "dimensional",
    "reshape",
    "dimension",
    "work",
    "around",
    "add",
    "oops",
    "reason",
    "k",
    "neighbors",
    "axis",
    "somewhere",
    "plus",
    "minus",
    "sign",
    "people",
    "green",
    "looking",
    "define",
    "distance",
    "line",
    "formula",
    "root",
    "squared",
    "closest",
    "rid",
    "happens",
    "total",
    "precision",
    "original",
    "recall",
    "score",
    "seven",
    "eight",
    "change",
    "naive",
    "bayes",
    "rule",
    "covid",
    "write",
    "calculate",
    "disease",
    "rewrite",
    "sorry",
    "soccer",
    "hat",
    "logistic",
    "linear",
    "infinity",
    "goes",
    "e",
    "sigmoid",
    "multiple",
    "parameters",
    "ahead",
    "wanted",
    "svm",
    "space",
    "neural",
    "net",
    "layer",
    "w",
    "activation",
    "error",
    "respect",
    "tensorflow",
    "dense",
    "nodes",
    "epochs",
    "history",
    "dropout",
    "residual",
    "residuals",
    "spread",
    "average",
    "r",
    "regressor",
    "count",
    "hour",
    "temp",
    "compute",
    "clusters",
    "centroids",
    "principal",
    "component",
    "pca",
    "variance",
    "compactness"
  ]
}