{
  "text": "hi everybody welcome to a new PI torch\ntutorial in this tutorial we will talk\nabout transfer learning and how it can\nbe applied in PI torch transfer learning\nis a machine learning method where a\nmodel developed for a first task is then\nreused as the starting point for a model\non a second task for example we can\ntrain a model to classify birds and cats\nand then use the same model modified\nonly a little bit in the last layer and\nthen used a new model to classify bees\nand docks so it's a popular approach in\ndeep learning that allows rapid\ngeneration of new models and this is\nsuper important because training of a\ncompletely new model can be very time\nconsuming it can take multiple days or\neven weeks so if you use a pre trained\nmodel then we typically exchange only\nthe last layer and then do not need to\ntrain the whole model again however\ntransfer learning can achieve pretty\ngood performance results and that's why\nit's so popular nowadays so let's have a\nlook at this picture here we have a\ntypical CNN architecture that I already\nshowed you in the last tutorial and this\nlet's say this has been already trained\non a lot of data and we have the\noptimized weights and now we only want\nto take the last fully connected layer\nso this one here and then modify it and\ntrain the last layer on our new data so\nthen we have a new model that has been\ntrained and tweaked in the last layer\nand yeah this is the concept of transfer\nlearning and now let's have a look at a\nconcrete example in pi torch so in this\nexample we want we are using the pre\ntrained ResNet 18 CNN this is a network\nthat is trained on more than a million\nimages from the image net database and\nthis network is 18 layers deep and can\nclassify images into 1,000 object\ncategories and now in our example we\nhave only two classes so we only want to\ndetect bees and ants\nand yeah so let's start so in this\nsession I already I also want to show\nyou two other new things so first the\ndata sets image folder how we can use\nthis and how use a scapula to change the\nlearning rate and then of course how\ntransfer learning is used so I already\nimported the things that we need and now\nwe set up the data and the last time we\nuse the built-in data sets from the\ntorch vision data sets and now here we\nuse the data sets dot image folder\nbecause we saved our data in a folder\nand this has to have the structure like\nthis so we have the folder here and then\nwe have a training and a validation\nfolder so train and Val and in each one\nwe have folders for each class so here\nwe have ants and ants and peace and also\nin the validation folder we have ants\nand bees and now in each folder we have\nthe images here so for example here we\nhave some ants and also let's have a\nlook at some piece so here we have a bee\nand yeah some you must structure your\nfolder like this and then you can call\nthe datasets dot image folder and give\nit the path and we also give it some\ntransforms here and then we get the\nclasses the class names by calling image\nsets image data sets dot classes and\nyeah then here I define the training\nmodel where I did the loop um and did\nthe training and the evaluation I will\nnot go into detail here\nyou should already know this from the\nlast tutorials how a typical training\nand evaluation loop looks like you can\nalso check the whole code on github so I\nwill provide the link in the description\nso have a look at this yourself and now\nlet's use transfer learning so first of\nall we want to import the pre trained\nmodel so let's set up this model so we\ncan do this by saying model so model\nequals and this is available in the\ntorch vision thought models module so I\nimported torch vision models already and\nthen I can call models dot rest net 16\nor sorry resonate 18 here and then I can\nsay pre-trained equals true so this is\nalready the optimized weights that are\ntrained on the image net data and now\nwhat we want to do is we want to\nexchange the last fully connected layer\nso first of all let's get the number of\ninput features from the last layer so\nlet's say num features equals model and\nwe can get this by calling dot FC fully\nconnected and then the input features\nthis is the number of input features for\nthe last layer that we need and then\nlet's create a new layer and assign it\nto the last layer so let's say model dot\nFC equals and now we give it a new fully\nconnected layer and n dot Lin ER and\nthis gets the number of input features\nthat we have and then as new output\nfeatures number of outputs we have to\nbecause we have two classes now and now\nwe send our model\nto the device if we have GPU support so\nwe created our device in the beginning\nas always so this is CUDA or simply CPU\nand now that we have our new model we\ncan again as always define our loss and\noptimize us so we say criterion equals n\nn dot cross entropy loss and then let's\nsay the optimizer equals this is from\nthe optimization module Optim dot SGD\nstochastic gradient descent which has to\noptimize the model parameters and we\nhave to specify the learning rate equals\nlet's say point zero zero one and now as\na new thing let's use a scheduler this\nwill update the learning rate so for\nthis we can say we can create this by\nsaying it's called a step L our schedule\nax equals and L our schedule ax is\navailable also in the torch optimization\nmodule so we already imported this and\nthen we can say L our schedule ax dot\nstep L R and then here we have to give\nit the optimizer so here we say\noptimizer and then we say step size step\nsize equals 7 and then we say gamma\nequals let's say point 1 this means that\nevery 7 epochs our learning rate is\nmultiplied by this value so every 7\nepochs our learning rate has\nonly 10 is now only updated to 10% so\nyeah this is how you use a scapula and\nthen typically what we want to do is in\nour loop in our loop over the epoch so\nfor epoch in range let's say 100 and\nthen typically here we use the training\nwhere we also do the the optimizer dot\nstep optimizer dot step then we want to\nevaluate it evaluate it and then we also\nhave to call schedule a step scheduler\nstep so this is how we use a scapula\nplease have a look at the whole loop\nhere yourself\nso yeah now we set up the scheduler and\nlet's call the training function so here\nwe say model equals and then train model\nso this is the function that I created\nand then I have to pass the model the\ncriterion the optimizer the scheduler\nand also the number of epochs so num\nepochs let's say 20 and yeah so this is\nhow we use how we can use transfer\nlearning so in this case we use a\ntechnique that is called fine tuning\nbecause here we train the whole model\nagain but only a little bit so we\nfine-tune all the weights spaced on the\nnew data and with the new last layer so\nthis is one option and the second one is\nfor this I copy and paste the same thing\nlet's see where does it start so here\nand then as a second option what we can\ndo is we can freeze all the all the\nlayers in the beginning and only train\nthe very last layer so for this we have\nto loop over all the parameters here\nafter we got our model so we say for\nparam in model dot parameters and then\nwe can set the require scratch attribute\nto false so we can say param dot\nrequires grat and then say re sorry dot\nrequire Scrat requires scratch equals\nfalse now we have it and this will\nfreeze all the layers in the beginning\nand now we set up the new last layer we\ncreate a new layer here and by default\nthis has require Scrat equals true and\nthen again we set up the loss and\noptimize and the scheduler in this case\nand then we do the training function\nagain and so yeah so this is even more\nfaster and let's run this and then have\na look at both the evaluations and I\nalso print out the time that it took so\nyeah let's save this and let's run this\nby saying python transfer dot pi and\n[Music]\nthis might offers it will download all\nthe images and this might take a couple\nof seconds because I don't have GPU\nsupport here on my macbook so I will\nskip this and then I will see you in a\nsecond\nall right so now I'm back so this took\nsuper long on my computer so I reset the\nnumber of epochs to just 2 in this\nexample so let's have a look at the\nresults so after only 2 epochs so this\nis the first training where we did the\nfine-tuning of the whole model so this\ntook three and a half minutes and the\nbest accuracy now is 0.92 so 92% and\nthen this is the second training where\nwe only trained the last layer so this\ntook only one and a half minutes\napproximately\nand.yeah curacy is also and it's already\nover 80% so of course it's not as good\nas in when we trained the whole training\nbut still pretty good for only two\nepochs and now let's imagine if we set\nthe number of epochs even higher so yeah\nthis is why transfer learning is so\npowerful because we have a pre trained\nmodel and then we only find unit a\nlittle bit and do a completely new task\nand then achieve pretty good results too\nso yeah so now I hope you understood how\ntransfer learning can be applied in PI\ntorch if you enjoyed this tutorial\nplease subscribe to the channel and see\nyou next time bye\n",
  "words": [
    "hi",
    "everybody",
    "welcome",
    "new",
    "pi",
    "torch",
    "tutorial",
    "tutorial",
    "talk",
    "transfer",
    "learning",
    "applied",
    "pi",
    "torch",
    "transfer",
    "learning",
    "machine",
    "learning",
    "method",
    "model",
    "developed",
    "first",
    "task",
    "reused",
    "starting",
    "point",
    "model",
    "second",
    "task",
    "example",
    "train",
    "model",
    "classify",
    "birds",
    "cats",
    "use",
    "model",
    "modified",
    "little",
    "bit",
    "last",
    "layer",
    "used",
    "new",
    "model",
    "classify",
    "bees",
    "docks",
    "popular",
    "approach",
    "deep",
    "learning",
    "allows",
    "rapid",
    "generation",
    "new",
    "models",
    "super",
    "important",
    "training",
    "completely",
    "new",
    "model",
    "time",
    "consuming",
    "take",
    "multiple",
    "days",
    "even",
    "weeks",
    "use",
    "pre",
    "trained",
    "model",
    "typically",
    "exchange",
    "last",
    "layer",
    "need",
    "train",
    "whole",
    "model",
    "however",
    "transfer",
    "learning",
    "achieve",
    "pretty",
    "good",
    "performance",
    "results",
    "popular",
    "nowadays",
    "let",
    "look",
    "picture",
    "typical",
    "cnn",
    "architecture",
    "already",
    "showed",
    "last",
    "tutorial",
    "let",
    "say",
    "already",
    "trained",
    "lot",
    "data",
    "optimized",
    "weights",
    "want",
    "take",
    "last",
    "fully",
    "connected",
    "layer",
    "one",
    "modify",
    "train",
    "last",
    "layer",
    "new",
    "data",
    "new",
    "model",
    "trained",
    "tweaked",
    "last",
    "layer",
    "yeah",
    "concept",
    "transfer",
    "learning",
    "let",
    "look",
    "concrete",
    "example",
    "pi",
    "torch",
    "example",
    "want",
    "using",
    "pre",
    "trained",
    "resnet",
    "18",
    "cnn",
    "network",
    "trained",
    "million",
    "images",
    "image",
    "net",
    "database",
    "network",
    "18",
    "layers",
    "deep",
    "classify",
    "images",
    "object",
    "categories",
    "example",
    "two",
    "classes",
    "want",
    "detect",
    "bees",
    "ants",
    "yeah",
    "let",
    "start",
    "session",
    "already",
    "also",
    "want",
    "show",
    "two",
    "new",
    "things",
    "first",
    "data",
    "sets",
    "image",
    "folder",
    "use",
    "use",
    "scapula",
    "change",
    "learning",
    "rate",
    "course",
    "transfer",
    "learning",
    "used",
    "already",
    "imported",
    "things",
    "need",
    "set",
    "data",
    "last",
    "time",
    "use",
    "data",
    "sets",
    "torch",
    "vision",
    "data",
    "sets",
    "use",
    "data",
    "sets",
    "dot",
    "image",
    "folder",
    "saved",
    "data",
    "folder",
    "structure",
    "like",
    "folder",
    "training",
    "validation",
    "folder",
    "train",
    "val",
    "one",
    "folders",
    "class",
    "ants",
    "ants",
    "peace",
    "also",
    "validation",
    "folder",
    "ants",
    "bees",
    "folder",
    "images",
    "example",
    "ants",
    "also",
    "let",
    "look",
    "piece",
    "bee",
    "yeah",
    "must",
    "structure",
    "folder",
    "like",
    "call",
    "datasets",
    "dot",
    "image",
    "folder",
    "give",
    "path",
    "also",
    "give",
    "transforms",
    "get",
    "classes",
    "class",
    "names",
    "calling",
    "image",
    "sets",
    "image",
    "data",
    "sets",
    "dot",
    "classes",
    "yeah",
    "define",
    "training",
    "model",
    "loop",
    "um",
    "training",
    "evaluation",
    "go",
    "detail",
    "already",
    "know",
    "last",
    "tutorials",
    "typical",
    "training",
    "evaluation",
    "loop",
    "looks",
    "like",
    "also",
    "check",
    "whole",
    "code",
    "github",
    "provide",
    "link",
    "description",
    "look",
    "let",
    "use",
    "transfer",
    "learning",
    "first",
    "want",
    "import",
    "pre",
    "trained",
    "model",
    "let",
    "set",
    "model",
    "saying",
    "model",
    "model",
    "equals",
    "available",
    "torch",
    "vision",
    "thought",
    "models",
    "module",
    "imported",
    "torch",
    "vision",
    "models",
    "already",
    "call",
    "models",
    "dot",
    "rest",
    "net",
    "16",
    "sorry",
    "resonate",
    "18",
    "say",
    "equals",
    "true",
    "already",
    "optimized",
    "weights",
    "trained",
    "image",
    "net",
    "data",
    "want",
    "want",
    "exchange",
    "last",
    "fully",
    "connected",
    "layer",
    "first",
    "let",
    "get",
    "number",
    "input",
    "features",
    "last",
    "layer",
    "let",
    "say",
    "num",
    "features",
    "equals",
    "model",
    "get",
    "calling",
    "dot",
    "fc",
    "fully",
    "connected",
    "input",
    "features",
    "number",
    "input",
    "features",
    "last",
    "layer",
    "need",
    "let",
    "create",
    "new",
    "layer",
    "assign",
    "last",
    "layer",
    "let",
    "say",
    "model",
    "dot",
    "fc",
    "equals",
    "give",
    "new",
    "fully",
    "connected",
    "layer",
    "n",
    "dot",
    "lin",
    "er",
    "gets",
    "number",
    "input",
    "features",
    "new",
    "output",
    "features",
    "number",
    "outputs",
    "two",
    "classes",
    "send",
    "model",
    "device",
    "gpu",
    "support",
    "created",
    "device",
    "beginning",
    "always",
    "cuda",
    "simply",
    "cpu",
    "new",
    "model",
    "always",
    "define",
    "loss",
    "optimize",
    "us",
    "say",
    "criterion",
    "equals",
    "n",
    "n",
    "dot",
    "cross",
    "entropy",
    "loss",
    "let",
    "say",
    "optimizer",
    "equals",
    "optimization",
    "module",
    "optim",
    "dot",
    "sgd",
    "stochastic",
    "gradient",
    "descent",
    "optimize",
    "model",
    "parameters",
    "specify",
    "learning",
    "rate",
    "equals",
    "let",
    "say",
    "point",
    "zero",
    "zero",
    "one",
    "new",
    "thing",
    "let",
    "use",
    "scheduler",
    "update",
    "learning",
    "rate",
    "say",
    "create",
    "saying",
    "called",
    "step",
    "l",
    "schedule",
    "ax",
    "equals",
    "l",
    "schedule",
    "ax",
    "available",
    "also",
    "torch",
    "optimization",
    "module",
    "already",
    "imported",
    "say",
    "l",
    "schedule",
    "ax",
    "dot",
    "step",
    "l",
    "r",
    "give",
    "optimizer",
    "say",
    "optimizer",
    "say",
    "step",
    "size",
    "step",
    "size",
    "equals",
    "7",
    "say",
    "gamma",
    "equals",
    "let",
    "say",
    "point",
    "1",
    "means",
    "every",
    "7",
    "epochs",
    "learning",
    "rate",
    "multiplied",
    "value",
    "every",
    "7",
    "epochs",
    "learning",
    "rate",
    "10",
    "updated",
    "10",
    "yeah",
    "use",
    "scapula",
    "typically",
    "want",
    "loop",
    "loop",
    "epoch",
    "epoch",
    "range",
    "let",
    "say",
    "100",
    "typically",
    "use",
    "training",
    "also",
    "optimizer",
    "dot",
    "step",
    "optimizer",
    "dot",
    "step",
    "want",
    "evaluate",
    "evaluate",
    "also",
    "call",
    "schedule",
    "step",
    "scheduler",
    "step",
    "use",
    "scapula",
    "please",
    "look",
    "whole",
    "loop",
    "yeah",
    "set",
    "scheduler",
    "let",
    "call",
    "training",
    "function",
    "say",
    "model",
    "equals",
    "train",
    "model",
    "function",
    "created",
    "pass",
    "model",
    "criterion",
    "optimizer",
    "scheduler",
    "also",
    "number",
    "epochs",
    "num",
    "epochs",
    "let",
    "say",
    "20",
    "yeah",
    "use",
    "use",
    "transfer",
    "learning",
    "case",
    "use",
    "technique",
    "called",
    "fine",
    "tuning",
    "train",
    "whole",
    "model",
    "little",
    "bit",
    "weights",
    "spaced",
    "new",
    "data",
    "new",
    "last",
    "layer",
    "one",
    "option",
    "second",
    "one",
    "copy",
    "paste",
    "thing",
    "let",
    "see",
    "start",
    "second",
    "option",
    "freeze",
    "layers",
    "beginning",
    "train",
    "last",
    "layer",
    "loop",
    "parameters",
    "got",
    "model",
    "say",
    "param",
    "model",
    "dot",
    "parameters",
    "set",
    "require",
    "scratch",
    "attribute",
    "false",
    "say",
    "param",
    "dot",
    "requires",
    "grat",
    "say",
    "sorry",
    "dot",
    "require",
    "scrat",
    "requires",
    "scratch",
    "equals",
    "false",
    "freeze",
    "layers",
    "beginning",
    "set",
    "new",
    "last",
    "layer",
    "create",
    "new",
    "layer",
    "default",
    "require",
    "scrat",
    "equals",
    "true",
    "set",
    "loss",
    "optimize",
    "scheduler",
    "case",
    "training",
    "function",
    "yeah",
    "even",
    "faster",
    "let",
    "run",
    "look",
    "evaluations",
    "also",
    "print",
    "time",
    "took",
    "yeah",
    "let",
    "save",
    "let",
    "run",
    "saying",
    "python",
    "transfer",
    "dot",
    "pi",
    "music",
    "might",
    "offers",
    "download",
    "images",
    "might",
    "take",
    "couple",
    "seconds",
    "gpu",
    "support",
    "macbook",
    "skip",
    "see",
    "second",
    "right",
    "back",
    "took",
    "super",
    "long",
    "computer",
    "reset",
    "number",
    "epochs",
    "2",
    "example",
    "let",
    "look",
    "results",
    "2",
    "epochs",
    "first",
    "training",
    "whole",
    "model",
    "took",
    "three",
    "half",
    "minutes",
    "best",
    "accuracy",
    "92",
    "second",
    "training",
    "trained",
    "last",
    "layer",
    "took",
    "one",
    "half",
    "minutes",
    "approximately",
    "curacy",
    "also",
    "already",
    "80",
    "course",
    "good",
    "trained",
    "whole",
    "training",
    "still",
    "pretty",
    "good",
    "two",
    "epochs",
    "let",
    "imagine",
    "set",
    "number",
    "epochs",
    "even",
    "higher",
    "yeah",
    "transfer",
    "learning",
    "powerful",
    "pre",
    "trained",
    "model",
    "find",
    "unit",
    "little",
    "bit",
    "completely",
    "new",
    "task",
    "achieve",
    "pretty",
    "good",
    "results",
    "yeah",
    "hope",
    "understood",
    "transfer",
    "learning",
    "applied",
    "pi",
    "torch",
    "enjoyed",
    "tutorial",
    "please",
    "subscribe",
    "channel",
    "see",
    "next",
    "time",
    "bye"
  ],
  "keywords": [
    "new",
    "pi",
    "torch",
    "tutorial",
    "transfer",
    "learning",
    "model",
    "first",
    "task",
    "point",
    "second",
    "example",
    "train",
    "classify",
    "use",
    "little",
    "bit",
    "last",
    "layer",
    "bees",
    "models",
    "training",
    "time",
    "take",
    "even",
    "pre",
    "trained",
    "typically",
    "need",
    "whole",
    "pretty",
    "good",
    "results",
    "let",
    "look",
    "already",
    "say",
    "data",
    "weights",
    "want",
    "fully",
    "connected",
    "one",
    "yeah",
    "18",
    "images",
    "image",
    "net",
    "layers",
    "two",
    "classes",
    "ants",
    "also",
    "sets",
    "folder",
    "scapula",
    "rate",
    "imported",
    "set",
    "vision",
    "dot",
    "like",
    "call",
    "give",
    "get",
    "loop",
    "saying",
    "equals",
    "module",
    "number",
    "input",
    "features",
    "create",
    "n",
    "beginning",
    "loss",
    "optimize",
    "optimizer",
    "parameters",
    "scheduler",
    "step",
    "l",
    "schedule",
    "ax",
    "7",
    "epochs",
    "function",
    "see",
    "require",
    "took"
  ]
}