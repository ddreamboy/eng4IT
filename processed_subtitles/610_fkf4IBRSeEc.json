{
  "text": "[Music]\nwelcome back so now we're going to talk\nabout how you can use the singular value\ndecomposition to compute the principal\ncomponent analysis or PCA PCA is the\nbedrock dimensionality reduction\ntechnique for probability and statistics\nand it's still very very commonly used\nin data science and machine learning\napplications when you have big data that\nmight have some statistical distribution\nand you want to uncover the low\ndimensional patterns to build models off\nof it\nPCA has been around for a long time\nsince 1901 pearson paper okay so it's a\nreally really old method very\nwell-established ton of theory about the\nstatistics of this and I'm going to\nessentially pigeonhole this as what\nwe're gonna call the statistical\ninterpretation interpretation of the SVD\nand principal component analysis is in\nparticular going to provide us with a\ndata-driven hierarchical coordinate\nsystem so it gives us a hierarchical\nhigh or that's not how you spell\nhierarchical a hierarchical coordinate\nsystem hierarchical coordinate system\nbased on data to represent the\nstatistical variations in your data sets\nokay so it's a coordinate system based\nin terms of directions in your data that\ncaptured the maximum amounts of the\nvariance in your data okay and so I'm\ngoing to the notation here is gonna be a\nlittle different than what we're used to\nand that's because the PCA literature\nand the SVD literature how kind of\ndifferent conventions about what the\nmatrix looks like so in the PCA\nliterature we still have a data matrix X\nand it still has a bunch of measurements\nfrom experiments independent experiments\nbut here we're going to represent those\nindependent experiments as big row\nfactors x1 x2 and so on and so forth\nokay so each each row vector X are\nessentially measurements from a single\nexperiment measurements from a single\nexperiment and what we're hoping is that\nthese are kind of individual experiments\nso this measurement might be the\ndemographic information from a specific\nhuman you know age weight sex race etc\netc etc and then X 1 would be person 1 X\n2 is person 2 and so on and so forth so\nthe same basic idea as before with our\ndata matrix X except now instead of\ncolumns having the information for a\nspecific individual kind of that that\nmeasurement from a single experiment now\nwe're gonna have those B rows and that's\nbecause that's consistent with the PCA\nliterature so I just want to show you\nhow it looks this way okay and the idea\nhere is that we're going to try to find\nand we're going to assume that this data\nX has some statistical distribution it's\nnot deterministic there are some\nstatistical variability to this\ninformation and we're going to try to\nuncover the dominant kind of\ncombinations of features that that\ndescribe as much of the data as possible\nokay so we're gonna do it using the SVD\nbut we're gonna write it a little bit\ndifferently okay so because this is the\nstatistical interpretation of the SVD\nthere's a few steps that are extra that\nwe don't normally do with SVD that we're\ngonna do here so kind of step one in\nthis procedure is that we're gonna\ncompute the mean the row-wise mean the\naverage row so we're gonna compute the\nmean row and we're just gonna call this\nX bar equals one over I'm gonna say that\nI still have n rows 1 over n sum of each\nof these X J's from J equals 1 to n ok\nso this is the average row I just\naverage all of the rows\nthe next thing I'm going to do is I'm\ngoing to build an average matrix so the\naverage matrix is going to be obtained\nby taking a vector of ones and\nmultiplying it by that X bar vector okay\nso I just literally create n copies of X\nbar and that's my X bar average matrix\nand so step two is I'm going to subtract\nthe mean from my data matrix okay so I'm\ngoing to subtract the mean so now I have\nB equals X minus X bar and the way we\nsay this in in PCA language is that this\nis the mean centered data so if I have\nsome distribution of data where there's\nsome you know average value to all of\nthis when I subtract that out it brings\neverything down so that the center of my\ndistribution is at the origin okay so\nwe're going to be modeling this data\nmatrix X assuming that it is a zero mean\nGaussian and so this is where we we\nsubtract off the mean so that it's zero\nmean okay now what we're gonna do is\nwe're going to compute the covariance\nmatrix of this mean Center data so the\ncovariance matrix again this is kind of\njust the correlation matrix from the SVD\nbut we're calling it a covariance matrix\nin this context of the covariance matrix\nof the rows of B and we're going to call\nthat matrix C C is equal to B transpose\nB okay good so at this point all we've\ndone is essentially take our data matrix\nwe've written it in a transpose from how\nwe normally do it we've subtracted off\nthe mean and we've computed this\ncorrelation matrix or this covariance\nmatrix so this looks a lot like the X\ntranspose X from before but we've\nsubtracted off the mean now what we're\ngoing to do is we're going to compute\nthe eigenvectors the leading\neigenvectors of this correlation matrix\nand that's going to be related\nboth to the singular vectors of X and\nalso to its principal components okay\nand I'm gonna try my best to get the\nnotation and the terminology correct\nagain this is in section 1.5 in our book\nso you can go there to refer for more\ndetails good so now what we're going to\ndo is we're going to compute the eigen\ndecomposition we're going to compute the\neigenvalues and eigenvectors so the Ides\nof C and in particular we're going to\ncompute for example let's call it V 1\ntranspose B transpose B V 1 that would\nbe the biggest eigenvector of this\nmatrix B transpose B is V 1 then I would\ncompute V 2 then V 3 then V 4 and so on\nand so forth just like in the SVD and\nthere's corresponding eigenvalues just\nlike in the SVD and essentially what\nwe're gonna get is this matrix C times V\nequals V times D where these are my\neigen values and these are my eigen\nvectors okay good so all we've done is\nwe've computed the singular sorry the\neigen value decomposition of this\ncovariance matrix that you could\nactually compute it using the SVD I'll\nshow you that in a minute\nand you get these eigen values and eigen\nvectors and here's where the principal\ncomponents come in so if I take this\nmatrix T which is equal to my my mean\nsubtracted data B times these eigen\nvectors V these are called my principal\ncomponents these are my principal\ncomponents okay principal components and\nthis vector V of these eigen vectors are\ncalled the loadings so essentially what\nyou do is you decompose this matrix into\nkind of directions of maximal variance\njust like in the singular value\ndecomposition called the principal\ncomponents\nand the loadings are kind of how much of\neach of those principle components each\nof each of the experiments has the\nloadings in a particular experiment of\nthose principal components columns okay\nand oftentimes in terms of the singular\nvalue decomposition language so let's\nsay X was equal to u Sigma V transpose\nhere then what we would say is that T is\nsimply equal to u times Sigma okay\nbecause and if I write B as sorry I\nshould be a little bit more careful if I\ntook the singular value decomposition of\nthe mean subtracted data B equals u\nSigma V transpose then T would be either\nB times V and B times V is simply u\ntimes Sigma okay so these are also a\nrepresentation of the principal\ncomponents okay so you can get the\nprincipal components and the loadings\ndirectly from the SVD of the mean\nsubtracted data I guess that's the\nheadline here is that this very\nimportant statistical representation of\nyour data can be achieved just by\ncomputing the SVD of your mean\nsubtracted data it's the same is finding\nthese eigen vectors of the covariance\nmatrix which is what you would kind of\ndo in the classical principal component\nanalysis good ok now what's also\nimportant is these eigen values here or\nthe singular values in Sigma give you an\nindication of the amount of the variance\nof this data set that these principal\ncomponents capture or these loadings\ncapture so if I only want to describe\nthis high dimensional data in terms of\nthe first two principal components or\nthe and the first two vectors of\nloadings I would be able to compute how\nmuch of the variance is captured by\ncomputing kind of how much energy or\nvariances in those first two eigen\nvalues of this this D matrix and so what\nI could literally do I want to make sure\nI'm being careful here these eigen\nvalues lambda are equal to the square of\nthe singular values\nit's literally equal to the variance of\nthat principal component in the data and\nso if I want to know how much variance\nis being captured in the first let's say\nfour modes I would take the sum from k\nequals 1 to R of lambda K divided by the\nsum of all n of my lambdas so I would\nbasically see how what's the fraction of\nvariance captured by my first are\nlambdas divided by the total of all of\nmy eigen values all of the variants in\nthe data and so for example I might\ndecide to keep only as many principal\ncomponents as are needed to explain 95%\nof the variance and so that would give\nyou a criterion for how many principal\ncomponents to keep okay again we're\ngoing to code this up I'm actually going\nto create a couple of data matrices real\ndata matrices that have distributions\none of them will be a random data matrix\nanother one will be a data matrix\nconsisting of genetic markers for people\nwith and without ovarian cancer and\nwe'll compute this principal component\nanalysis and look at the results so I'll\npoint out that again in MATLAB it's\npretty easy to compute this so it's\nsomething very simple like V score and\nthen some extra variable s 2 equals PCA\nof the B matrix okay so really really\neasy to compute in MATLAB also easy to\ncompute in R and Python and so we'll do\nexamples of that the last thing I want\nto do is just show you a picture so\nagain if I have data that is you know\nsome high dimensional data and it has\nsome distribution you're hoping that it\nhas some Gaussian kind of white noise\ndistribution or some some normal\ndistribution not drawing a great normal\ndistribution here then what the\nprincipal component analysis is going to\ndo is it's essentially going to find\nthese ellipsoids of maximal variance in\nterms of 1 & 2 & 3 standard deviations\nso you can actually quantify if I have a\nnew point how\nis it given the distribution of the old\npoints and not only that but that prints\nthe singular value decomposition in the\nprincipal component analysis will tell\nyou exactly what directions are account\nfor the most variance the second most\nvariants the third most variance in the\ndata and so on and so forth okay so a\nvery useful statistical technique to\nbuild models statistical models from\nyour data okay thank you\n",
  "words": [
    "music",
    "welcome",
    "back",
    "going",
    "talk",
    "use",
    "singular",
    "value",
    "decomposition",
    "compute",
    "principal",
    "component",
    "analysis",
    "pca",
    "pca",
    "bedrock",
    "dimensionality",
    "reduction",
    "technique",
    "probability",
    "statistics",
    "still",
    "commonly",
    "used",
    "data",
    "science",
    "machine",
    "learning",
    "applications",
    "big",
    "data",
    "might",
    "statistical",
    "distribution",
    "want",
    "uncover",
    "low",
    "dimensional",
    "patterns",
    "build",
    "models",
    "pca",
    "around",
    "long",
    "time",
    "since",
    "1901",
    "pearson",
    "paper",
    "okay",
    "really",
    "really",
    "old",
    "method",
    "ton",
    "theory",
    "statistics",
    "going",
    "essentially",
    "pigeonhole",
    "gon",
    "na",
    "call",
    "statistical",
    "interpretation",
    "interpretation",
    "svd",
    "principal",
    "component",
    "analysis",
    "particular",
    "going",
    "provide",
    "us",
    "hierarchical",
    "coordinate",
    "system",
    "gives",
    "us",
    "hierarchical",
    "high",
    "spell",
    "hierarchical",
    "hierarchical",
    "coordinate",
    "system",
    "hierarchical",
    "coordinate",
    "system",
    "based",
    "data",
    "represent",
    "statistical",
    "variations",
    "data",
    "sets",
    "okay",
    "coordinate",
    "system",
    "based",
    "terms",
    "directions",
    "data",
    "captured",
    "maximum",
    "amounts",
    "variance",
    "data",
    "okay",
    "going",
    "notation",
    "gon",
    "na",
    "little",
    "different",
    "used",
    "pca",
    "literature",
    "svd",
    "literature",
    "kind",
    "different",
    "conventions",
    "matrix",
    "looks",
    "like",
    "pca",
    "literature",
    "still",
    "data",
    "matrix",
    "x",
    "still",
    "bunch",
    "measurements",
    "experiments",
    "independent",
    "experiments",
    "going",
    "represent",
    "independent",
    "experiments",
    "big",
    "row",
    "factors",
    "x1",
    "x2",
    "forth",
    "okay",
    "row",
    "vector",
    "x",
    "essentially",
    "measurements",
    "single",
    "experiment",
    "measurements",
    "single",
    "experiment",
    "hoping",
    "kind",
    "individual",
    "experiments",
    "measurement",
    "might",
    "demographic",
    "information",
    "specific",
    "human",
    "know",
    "age",
    "weight",
    "sex",
    "race",
    "etc",
    "etc",
    "etc",
    "x",
    "1",
    "would",
    "person",
    "1",
    "x",
    "2",
    "person",
    "2",
    "forth",
    "basic",
    "idea",
    "data",
    "matrix",
    "x",
    "except",
    "instead",
    "columns",
    "information",
    "specific",
    "individual",
    "kind",
    "measurement",
    "single",
    "experiment",
    "gon",
    "na",
    "b",
    "rows",
    "consistent",
    "pca",
    "literature",
    "want",
    "show",
    "looks",
    "way",
    "okay",
    "idea",
    "going",
    "try",
    "find",
    "going",
    "assume",
    "data",
    "x",
    "statistical",
    "distribution",
    "deterministic",
    "statistical",
    "variability",
    "information",
    "going",
    "try",
    "uncover",
    "dominant",
    "kind",
    "combinations",
    "features",
    "describe",
    "much",
    "data",
    "possible",
    "okay",
    "gon",
    "na",
    "using",
    "svd",
    "gon",
    "na",
    "write",
    "little",
    "bit",
    "differently",
    "okay",
    "statistical",
    "interpretation",
    "svd",
    "steps",
    "extra",
    "normally",
    "svd",
    "gon",
    "na",
    "kind",
    "step",
    "one",
    "procedure",
    "gon",
    "na",
    "compute",
    "mean",
    "mean",
    "average",
    "row",
    "gon",
    "na",
    "compute",
    "mean",
    "row",
    "gon",
    "na",
    "call",
    "x",
    "bar",
    "equals",
    "one",
    "gon",
    "na",
    "say",
    "still",
    "n",
    "rows",
    "1",
    "n",
    "sum",
    "x",
    "j",
    "j",
    "equals",
    "1",
    "n",
    "ok",
    "average",
    "row",
    "average",
    "rows",
    "next",
    "thing",
    "going",
    "going",
    "build",
    "average",
    "matrix",
    "average",
    "matrix",
    "going",
    "obtained",
    "taking",
    "vector",
    "ones",
    "multiplying",
    "x",
    "bar",
    "vector",
    "okay",
    "literally",
    "create",
    "n",
    "copies",
    "x",
    "bar",
    "x",
    "bar",
    "average",
    "matrix",
    "step",
    "two",
    "going",
    "subtract",
    "mean",
    "data",
    "matrix",
    "okay",
    "going",
    "subtract",
    "mean",
    "b",
    "equals",
    "x",
    "minus",
    "x",
    "bar",
    "way",
    "say",
    "pca",
    "language",
    "mean",
    "centered",
    "data",
    "distribution",
    "data",
    "know",
    "average",
    "value",
    "subtract",
    "brings",
    "everything",
    "center",
    "distribution",
    "origin",
    "okay",
    "going",
    "modeling",
    "data",
    "matrix",
    "x",
    "assuming",
    "zero",
    "mean",
    "gaussian",
    "subtract",
    "mean",
    "zero",
    "mean",
    "okay",
    "gon",
    "na",
    "going",
    "compute",
    "covariance",
    "matrix",
    "mean",
    "center",
    "data",
    "covariance",
    "matrix",
    "kind",
    "correlation",
    "matrix",
    "svd",
    "calling",
    "covariance",
    "matrix",
    "context",
    "covariance",
    "matrix",
    "rows",
    "b",
    "going",
    "call",
    "matrix",
    "c",
    "c",
    "equal",
    "b",
    "transpose",
    "b",
    "okay",
    "good",
    "point",
    "done",
    "essentially",
    "take",
    "data",
    "matrix",
    "written",
    "transpose",
    "normally",
    "subtracted",
    "mean",
    "computed",
    "correlation",
    "matrix",
    "covariance",
    "matrix",
    "looks",
    "lot",
    "like",
    "x",
    "transpose",
    "x",
    "subtracted",
    "mean",
    "going",
    "going",
    "compute",
    "eigenvectors",
    "leading",
    "eigenvectors",
    "correlation",
    "matrix",
    "going",
    "related",
    "singular",
    "vectors",
    "x",
    "also",
    "principal",
    "components",
    "okay",
    "gon",
    "na",
    "try",
    "best",
    "get",
    "notation",
    "terminology",
    "correct",
    "section",
    "book",
    "go",
    "refer",
    "details",
    "good",
    "going",
    "going",
    "compute",
    "eigen",
    "decomposition",
    "going",
    "compute",
    "eigenvalues",
    "eigenvectors",
    "ides",
    "c",
    "particular",
    "going",
    "compute",
    "example",
    "let",
    "call",
    "v",
    "1",
    "transpose",
    "b",
    "transpose",
    "b",
    "v",
    "1",
    "would",
    "biggest",
    "eigenvector",
    "matrix",
    "b",
    "transpose",
    "b",
    "v",
    "1",
    "would",
    "compute",
    "v",
    "2",
    "v",
    "3",
    "v",
    "4",
    "forth",
    "like",
    "svd",
    "corresponding",
    "eigenvalues",
    "like",
    "svd",
    "essentially",
    "gon",
    "na",
    "get",
    "matrix",
    "c",
    "times",
    "v",
    "equals",
    "v",
    "times",
    "eigen",
    "values",
    "eigen",
    "vectors",
    "okay",
    "good",
    "done",
    "computed",
    "singular",
    "sorry",
    "eigen",
    "value",
    "decomposition",
    "covariance",
    "matrix",
    "could",
    "actually",
    "compute",
    "using",
    "svd",
    "show",
    "minute",
    "get",
    "eigen",
    "values",
    "eigen",
    "vectors",
    "principal",
    "components",
    "come",
    "take",
    "matrix",
    "equal",
    "mean",
    "subtracted",
    "data",
    "b",
    "times",
    "eigen",
    "vectors",
    "v",
    "called",
    "principal",
    "components",
    "principal",
    "components",
    "okay",
    "principal",
    "components",
    "vector",
    "v",
    "eigen",
    "vectors",
    "called",
    "loadings",
    "essentially",
    "decompose",
    "matrix",
    "kind",
    "directions",
    "maximal",
    "variance",
    "like",
    "singular",
    "value",
    "decomposition",
    "called",
    "principal",
    "components",
    "loadings",
    "kind",
    "much",
    "principle",
    "components",
    "experiments",
    "loadings",
    "particular",
    "experiment",
    "principal",
    "components",
    "columns",
    "okay",
    "oftentimes",
    "terms",
    "singular",
    "value",
    "decomposition",
    "language",
    "let",
    "say",
    "x",
    "equal",
    "u",
    "sigma",
    "v",
    "transpose",
    "would",
    "say",
    "simply",
    "equal",
    "u",
    "times",
    "sigma",
    "okay",
    "write",
    "b",
    "sorry",
    "little",
    "bit",
    "careful",
    "took",
    "singular",
    "value",
    "decomposition",
    "mean",
    "subtracted",
    "data",
    "b",
    "equals",
    "u",
    "sigma",
    "v",
    "transpose",
    "would",
    "either",
    "b",
    "times",
    "v",
    "b",
    "times",
    "v",
    "simply",
    "u",
    "times",
    "sigma",
    "okay",
    "also",
    "representation",
    "principal",
    "components",
    "okay",
    "get",
    "principal",
    "components",
    "loadings",
    "directly",
    "svd",
    "mean",
    "subtracted",
    "data",
    "guess",
    "headline",
    "important",
    "statistical",
    "representation",
    "data",
    "achieved",
    "computing",
    "svd",
    "mean",
    "subtracted",
    "data",
    "finding",
    "eigen",
    "vectors",
    "covariance",
    "matrix",
    "would",
    "kind",
    "classical",
    "principal",
    "component",
    "analysis",
    "good",
    "ok",
    "also",
    "important",
    "eigen",
    "values",
    "singular",
    "values",
    "sigma",
    "give",
    "indication",
    "amount",
    "variance",
    "data",
    "set",
    "principal",
    "components",
    "capture",
    "loadings",
    "capture",
    "want",
    "describe",
    "high",
    "dimensional",
    "data",
    "terms",
    "first",
    "two",
    "principal",
    "components",
    "first",
    "two",
    "vectors",
    "loadings",
    "would",
    "able",
    "compute",
    "much",
    "variance",
    "captured",
    "computing",
    "kind",
    "much",
    "energy",
    "variances",
    "first",
    "two",
    "eigen",
    "values",
    "matrix",
    "could",
    "literally",
    "want",
    "make",
    "sure",
    "careful",
    "eigen",
    "values",
    "lambda",
    "equal",
    "square",
    "singular",
    "values",
    "literally",
    "equal",
    "variance",
    "principal",
    "component",
    "data",
    "want",
    "know",
    "much",
    "variance",
    "captured",
    "first",
    "let",
    "say",
    "four",
    "modes",
    "would",
    "take",
    "sum",
    "k",
    "equals",
    "1",
    "r",
    "lambda",
    "k",
    "divided",
    "sum",
    "n",
    "lambdas",
    "would",
    "basically",
    "see",
    "fraction",
    "variance",
    "captured",
    "first",
    "lambdas",
    "divided",
    "total",
    "eigen",
    "values",
    "variants",
    "data",
    "example",
    "might",
    "decide",
    "keep",
    "many",
    "principal",
    "components",
    "needed",
    "explain",
    "95",
    "variance",
    "would",
    "give",
    "criterion",
    "many",
    "principal",
    "components",
    "keep",
    "okay",
    "going",
    "code",
    "actually",
    "going",
    "create",
    "couple",
    "data",
    "matrices",
    "real",
    "data",
    "matrices",
    "distributions",
    "one",
    "random",
    "data",
    "matrix",
    "another",
    "one",
    "data",
    "matrix",
    "consisting",
    "genetic",
    "markers",
    "people",
    "without",
    "ovarian",
    "cancer",
    "compute",
    "principal",
    "component",
    "analysis",
    "look",
    "results",
    "point",
    "matlab",
    "pretty",
    "easy",
    "compute",
    "something",
    "simple",
    "like",
    "v",
    "score",
    "extra",
    "variable",
    "2",
    "equals",
    "pca",
    "b",
    "matrix",
    "okay",
    "really",
    "really",
    "easy",
    "compute",
    "matlab",
    "also",
    "easy",
    "compute",
    "r",
    "python",
    "examples",
    "last",
    "thing",
    "want",
    "show",
    "picture",
    "data",
    "know",
    "high",
    "dimensional",
    "data",
    "distribution",
    "hoping",
    "gaussian",
    "kind",
    "white",
    "noise",
    "distribution",
    "normal",
    "distribution",
    "drawing",
    "great",
    "normal",
    "distribution",
    "principal",
    "component",
    "analysis",
    "going",
    "essentially",
    "going",
    "find",
    "ellipsoids",
    "maximal",
    "variance",
    "terms",
    "1",
    "2",
    "3",
    "standard",
    "deviations",
    "actually",
    "quantify",
    "new",
    "point",
    "given",
    "distribution",
    "old",
    "points",
    "prints",
    "singular",
    "value",
    "decomposition",
    "principal",
    "component",
    "analysis",
    "tell",
    "exactly",
    "directions",
    "account",
    "variance",
    "second",
    "variants",
    "third",
    "variance",
    "data",
    "forth",
    "okay",
    "useful",
    "statistical",
    "technique",
    "build",
    "models",
    "statistical",
    "models",
    "data",
    "okay",
    "thank"
  ],
  "keywords": [
    "going",
    "singular",
    "value",
    "decomposition",
    "compute",
    "principal",
    "component",
    "analysis",
    "pca",
    "still",
    "data",
    "might",
    "statistical",
    "distribution",
    "want",
    "dimensional",
    "build",
    "models",
    "okay",
    "really",
    "essentially",
    "gon",
    "na",
    "call",
    "interpretation",
    "svd",
    "particular",
    "hierarchical",
    "coordinate",
    "system",
    "high",
    "terms",
    "directions",
    "captured",
    "variance",
    "little",
    "literature",
    "kind",
    "matrix",
    "looks",
    "like",
    "x",
    "measurements",
    "experiments",
    "row",
    "forth",
    "vector",
    "single",
    "experiment",
    "information",
    "know",
    "etc",
    "1",
    "would",
    "2",
    "b",
    "rows",
    "show",
    "try",
    "much",
    "one",
    "mean",
    "average",
    "bar",
    "equals",
    "say",
    "n",
    "sum",
    "literally",
    "two",
    "subtract",
    "covariance",
    "correlation",
    "c",
    "equal",
    "transpose",
    "good",
    "point",
    "take",
    "subtracted",
    "eigenvectors",
    "vectors",
    "also",
    "components",
    "get",
    "eigen",
    "let",
    "v",
    "times",
    "values",
    "actually",
    "called",
    "loadings",
    "u",
    "sigma",
    "first",
    "easy"
  ]
}