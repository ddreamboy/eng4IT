{
  "text": "hi everyone thanks for joining us today\nthis event is brought to you brought to\nyou by data talks club which is a\ncommunity of people who love data we\nhave weekly events this is one of such\nevents if you want to find out more\nabout the events we have there is a link\nin the description go to the description\ncheck it out it has quite a few events\nthat we plan so check it out\nif you see something interesting\nregister sign up\nthen if you haven't subscribed to our\nyoutube channel for whatever reason now\nit's the time the best time to fix it\nthere is this red button below the video\nclick on this and you will get notified\nabout all our future videos\nand we have an amazing slack community\nif you join it you can hang out with\nother data enthusiasts so i do suggest\nto do this you're missing out if you\nare not there\nlast but not least we have an amazing\ncourse starting soon in september so\nthis course covers the basics of machine\nlearning engineering there is also a\nlink in the description so if you're\nwatching it right now and you want to\nlearn want to learn more about machine\nlearning engineering check it out it's\ntotally free\nyeah\nyou'll like it\nduring today's presentation you can ask\nany question you want and i will be\nasking these questions and by the way\nhow do you prefer should i\nask you questions at the end or should i\ninterrupt you if there is a relevant\nquestion and ask you the question um i'd\nprefer at the end if that's okay\nyeah that is totally okay so we will\nkeep the questions at the end so there\nwill be a q a session and there is a\nlink been linked in the live chat that\nyou can use for asking questions\ni think that's all you're probably here\nnot for me talking about this stuff but\nfor the\nfun stuff so now the floor is yours\nplease start\ngreat uh just before i start i've\nactually got a fan on in the background\nbecause it's very warm here i just wanna\nmake sure it's not interrupting the\naudio at all\ni cannot hear anything but also i have a\nfun that's why maybe i can hear it okay\ncool if anyone uh\nwatching live can hear a fan in the\nbackground let me know and i will turn\nit off\num great so firstly i'd like to thank\nboth alexia alexian france for inviting\nme to to give this talk with datatalks\nclub it's my first time hopefully not my\nlast\num and so today i'll be talking about\nfeature engineering for time series\nforecasting\nand so this talk\nbefore we go on let me just quickly\nintroduce myself\nso my name is kishan and i work as a\ndata science manager for a luxury\nfashion e-commerce company called\nfarfetch where i work on time series\nforecasting and pricing optimization\nbefore i entered data science i did a\nphd in physics working on building\nmodels\nof abnormal heart rhythms and doing\nlarge scale time series analysis on that\ni also like to contribute to open source\nwhen possible my most relevant recent\ncontributions would be\nto stats models where i contributed some\nadditional functionality to the\ntime series decomposition methods there\nin collaboration with soledad girly i'm\nalso building an online course on the\nsame topic feature engineering for\nforecasting if you're interested please\ncheck it out and you can find all the\nslides today already on github in the\nlink there\nmaybe you can send me the links and i\nwill post them in the live chat in the\ndescription so people can check them out\nright now\nlet me\njust\nopen and send that\nat least link to the slides and then\nwhere people will be able to so\ni've just sent the slides there in the\nchat\ndid you do great uh\nyes have you received it in the live\nchat you should have seen someone ah in\nthe live chat okay yeah maybe you should\nsend it in zoom because in the live chat\nyoutube might block links that are not\ncoming from the stream owner okay one\nsecond sorry everyone but i think you\nmight want these links now because later\nyou will have to come back\nand then find it okay now i have the\nlink and i will post it\nto live chat sorry for the interruption\nnow you have the link\nokay perfect now on to the actual topic\nso what does this talk about this talk\nis all about how do we convert a time\nseries forecasting task when we want to\npredict the future value of a sequence\nor a set of sequences and convert that\ninto a tabular machine learning\nregression task where we have a table of\nfeatures and a target variable and this\nallows us to use some of our favorite\nmodels like the new aggression random\nforest and so on and so there are two\nmain themes to this talk\none is how just how do we do forecasting\nusing traditional machine learning\nmodels and then secondly how do we build\ngood features when we start doing that\nand so we'll start about talking about\nhow we can do time series forecasting\nusing machine learning models\nso i just want to motivate why we'd want\nto use machine learning for forecasting\nby way of an example\nso a couple of years ago there was a\nkaggle competition called the m5\nforecasting\ncompetition where the objective was to\npredict\nthe sales of a set of goods from walmart\nand this was an interesting problem\nbecause of some characteristics of the\ndata set there was a very large number\nof correlated time series that we had to\nforecast\nthere was over 30 000 product store\ncombinations and therefore over 30 000\ntime series to forecast the\nproducts lied in a hierarchical\nstructure so a given product could\nbelong to a particular department which\nbelonged to a category the sales of a\nproduct belong to a store which could\nbelong to a state and so on so there's a\nhierarchical structure in the data\nyou can see examples of the time series\nhere on the bottom right and as you can\nsee they have a varying length so each\ntime series can come online as it were\nat different points in time\nand just from looking at the time series\nyou can see that there's a high degree\nof sparsity and intermittency there's no\nobvious seasonality or trend when\nlooking at this data\nalso we would imagine that exogenous\nvariables or features like the price\npromotional activity would also be\nimportant in forecasting the sales of\nthese products and then we would also\nexpect multiple seasonal patterns so you\nmight expect a weekly pattern because\nthe weekend would be different from\nweekdays the summer will be different to\nthe winter and so we have multiple\nseasonalities\nnow what was interesting is for this\ntype of data set it turned out that all\nof the top performing methods that won\nthe competition were actually quote\nunquote pure machine learning models and\nthey were better than all of the\ntraditional statistical benchmarks used\nfor time series forecasting such as\nexponential smoothing arima and so on\nand so in a lot of the top comp in the\nthe top winning solutions we saw light\ngbm and it turns out that uh machine\nlearning models like light gbm were able\nto learn simultaneously across multiple\ndifferent time series and integrate\nthese exogenous features and overall\nthat allowed it to give you a better\nforecasting accuracy\nthe other advantage of using machine\nlearning is that it gives you access to\nother\ntools and options for example you could\nspecify the sample weights which would\nallow you for example to give more\nweight to recent data than data in the\nfar past\nyou can also cust specify custom loss\nfunctions and actually in the in the\nwinning solution\na custom loss function was actually used\nand so you get all this for free just by\nusing a machine learning models here\nthat being said you should not neglect\nour simple traditional statistical\nbaselines over 90 percent of the\nparticipating teams did not beat a\nsimple statistical baseline model\nexponential smoothing and that means we\nshould still use these as benchmarks and\neven the top 50 solutions only improved\non exponential smoothing by between 15\nto 20 percent depending on the\ngranularity of the time series we were\nlooking at and so as a result we should\nstill continue to benchmark against\nsimple methods and not rule them out\nentirely\nnow overall my advice is that we should\nuse kind of simpler more traditional\nmethods and we're dealing with what i\nwould kind of call easy time series that\nis where you see a very strong\nseasonality or trend you have a very\nsmall number of time series they're not\ncorrelated with each other so you could\njust learn one model per time series you\ndon't see a lot of sparsity or\nintermittency there's very little\nexogenous features in which case it's\nyou're probably better off using\nsomething like arima or exponential\nsmoothing or profit\nbut you should consider using machine\nlearning models when you have a much\nmore complex problem that you might\nencounter in a business setting such as\nthis and so that that kind of hopefully\ngives you some motivation of when to use\nwhat kind of method\nso how do we go about doing forecasting\nwith machine learning\nso we start off with our time series\nhere it's just a fake time series which\ni'm going to call sales and we have all\nof this information up to time t and we\nwant to predict the sales at time t plus\none\nand so we want to convert this time\nseries into a table of features and a\ntarget variable so how do we create our\ntarget variable in this case we're just\ndirectly going to use our time series\nbecause that's what we want to predict\nand when we create these feature vectors\nwe want to create features which only\nuse information in the past and so we're\nonly using the data that's known at the\ntime of the target and this will allow\nus to avoid what is called lookahead\nbias this is where we use information\nfrom the future\nto predict the future which would result\nin data leakage\nand so i can show you what we do here so\nwhen we're constructing the feature\nvector for this uh target here perhaps\nin the future we're basically going to\ncreate some function from the previous\nvalues and the simplest thing we can do\nis just to directly use those past\nvalues and so we can create a feature\nvector here from the previous values of\nour time series and so here we're using\nwhat's called a lag of one a lag of two\nand a lag of three so lag one lag two\nleg three and so we can populate our\nfeature vectors and associate them with\nour target in this way and it doesn't\ncreate any data leakage\nand so we go on and of course we're\ngoing to have some missing data at the\nstart of the time series because we\ncan't lag any further as we go back\nand so here we've derived some features\nfrom the past value\nof our target time series and these are\ncalled lag features and you can also\ncreate more complex features from the\nprevious values and we'll get on to\nthose later another class of features\nare those where you know the value both\nin the past and in the future one\nexample might be advertising spend you\nmight have observed the advertising\nspend in the past and you observe the\nsales and then you might have a budget\nand so you know what the advertising\nspend is in the future and so that's\nquite easy to create a feature from you\njust use it directly\nyou might also have features where you\nknow the value in the past but you don't\nknow it in the future\nexamples would be weather related\nfeatures you might have observed\nrainfall in the past and that might\naffect the sales of your product maybe\nyou're selling ice cream i don't know um\nand then in the future how do you\npopulate this feature well there are a\nfew things you can do one is you have to\nuse an alternative forecast so you use a\nweather forecast um and you pull that\nfrom somewhere else and you use that as\nyour feature\nalternatively you have to create your\nown naive forecast so you might\nextrapolate from the previous value and\nproject that outwards\nand if neither of those work for you\nthen you might have to also use lag\nfeatures for these type of features so\nrather than projecting it out you'd use\nthe lag value of rainfall so to predict\nthe sales tomorrow you'd look at the\nrainfall today for example\nlastly we have a set of features which\nprobably isn't discussed as much one\nwhich we'll call static features\nthey're called static features because\nthey typically represent metadata about\nyour time series and they don't change\nfor a given time series so you might\nhave the sales in a particular country\nlike the uk you might ask why is this\nuseful\nit's useful when you start dealing with\nmultiple time series because then that\nmetadata can vary between time series so\nyou might also have the sales in germany\nor in other countries and then you can\ntry and have your model learn\ninformation about different groups of\ntime series and that's where static\nfeatures come into play\nand so now we have a group of features\norganized in a table and we have a\ntarget variable\nand so we can form our training data\nhere we'll come on to later around how\nwe can\nuh process our categorical features and\ndealing with missing data and that kind\nof stuff but essentially we have our\ntraining data which allows us to take\nwhatever standard sk learn type model we\nhave run dot fit and then we can predict\none step into the future\nnow in practice we will want to do\nmulti-step forecasting we don't want to\nforecast just one step into the future\ntypically we forecast multiple steps\ninto the future\nand so there are two main methods that\nallow us to do this one is called direct\nforecasting and the other one is called\nrecursive forecasting\nin direct forecasting we're going to\ndirectly predict the future values\neffectively we're going to use the same\nfeatures built from the the past values\nbut we'll we'll build models on\ndifferent target variables which means\nwe'll have multiple models for each\ndifferent target variable but\neffectively the same features and so\nyou'll have many models per forecast\ntime step so let's just illustrate that\nso let's say you're creating your\nfeature vector and you start at this\nthis time point here you create your\nfeature vector of lag values and you can\nassociate that with your target variable\nfor one step ahead now you also want to\nbuild a model that predicts two steps\nahead so you use the target variable\nwhich is two steps after the time frame\nyou're considering here and likewise a\nthird time step so now we have the same\nfeature vector but multiple different\ntarget variables\nand so we can then drag our time\nline forward here and construct our\nfeature vectors and our target variables\nwe can then effectively pass the same\nfeature vectors to these different\nmodels and give them different target\nvariables during training and so when we\npass our input feature vector\nrepresenting our most recent\ntime horizon\nwhen we pass that input vector to our\nmodels you will then get a forecast for\neach model giving you a different\nforecast into the future so they have a\nmodel that gives you a one-step forecast\nanother model that gives you a two-step\nforecast and so on this has the obvious\ndownside that you're now going to train\nmultiple models per forecasting step if\nyou want to do 28 days into the future\nyou'll have 28 models and that\nintroduces a lot of complexity so is\nthere a simpler way of doing this well\nthis is what recursive forecasting tries\nto take into account and so with\nrecursive forecasting you build your\nfeatures and you fit the model once and\nthen you're going to recursively use\nthis model and i'll show you how so you\nbuild your model and you predict your\none step ahead forecast as normal now\nyou're going to append that forecast\nback to your original training data\nand now you recreate your features on\nthe target variable with your forecast\nand then you plug that back into the\nmodel to get you your next forecast and\nso on and this way you can use one\nfitted model to predict multiple steps\ninto the future\nnow let me show you exactly how that\nworks on our tabular data set so let's\nassume that we've already processed our\nstatic features our future unknown\nfeatures and our feature future known\nfeatures\nand so we're just dealing with our lag\nfeatures here\nand now we have to build these\niteratively so in the first step ahead\nwe're just going to build our features\nfrom the past values as we as we've\nshown\nand then we predict one step into the\nfuture\nand now we append that prediction back\nto our target variable and now we\nrecreate\nthe feature vector from this augmented\ntarget variable here with our forecast\nand now we can do dot predict again on\nthis feature vector append the forecast\nand continue and in this way we can do\nmulti-step forecasting using just one\nmodel so let's just talk about the pros\nand cons of these methods so one of the\nadvantages of direct forecasting is that\neach model you're building is directly\noptimized for each forecasting step if\nyou're going to build a 10 step ahead\nforecast then you've built a model that\ndoes precisely that\nand this is to\ncontrast it with recursive forecasting\nwhere you only built one model which\noptimized on building a one step ahead\nforecast and you might imagine that the\nway a model uses features to do a\nforecast in the near term might be\ndifferent than in the long term and so\nthat's\nsome of the the advantages and\ndisadvantages on that point\nthe cons of direct forecasting obviously\nyou're now having to use multiple models\nwhich involves a computational cost it's\nharder to maintain\nand also these forecasts are all\nindependent the forecasting model to\nforecast one step ahead is independent\nfrom the second step ahead and third\nstep ahead and so on so you can't\nguarantee that they'll be correlated or\nnear one another\nso some of the advantages of recursive\nforecasting is that you only have one\nmodel which means it's less\ncomputational time and the forecasts by\nby definition they are dependent because\nyou're using the forecast to generate\nyour next forecast and so they're\nthey're correlated\none of the disadvantages is that you now\nhave a lot of additional code complexity\num to iteratively create your features\nappend them to target variable and so on\nand i'll show you how you can overcome\nthat with some open source libraries\nlater\nalso you'll have the propagation of\nerrors if you're one step ahead forecast\nhas a significant error then that will\nobviously propagate to your second step\nahead your third step ahead and so on\nand so in that way these errors\npropagate and as we've mentioned it's\nonly optimized for one step ahead and so\nthese are some of the things to to keep\nin mind when you're considering whether\nor not to use direct or recursive\nforecasting\nnow i just want to talk a little bit\nabout how we would validate our\nforecasting model so traditionally when\nwe work with tabular data and we want to\ndo some kind of cross-validation or a\ntrain test split\nwe could just randomly shuffle the data\nfrom our original data to a training and\ntest set and we could do that in a\nrandom manner because we can assume that\neach row is independent and that way we\ncan therefore randomly split the data\nthe problem now is when we're dealing\nwith time series data and despite the\nfact we're still working with tabular\ndata you cannot split randomly because\nthe time ordering in your data means\neach row is no longer independent from\none another\nif you were to randomly shuffle this\ndata set you might have values in the\nfuture land in your training data values\nfrom the past landing in your test data\nand then you're using the future to\npredict the past and so you will have a\nvery inaccurate idea of how well your\nmodel actually performs in practice\nso instead you need to split by time to\nreplicate the actual forecasting process\nwhat does that look like so in time\nseries cross validation you'll arrange\nyour data by time and you have your full\ndata set and then you'll split the data\nby certain time horizons and then you'll\ntrain your data you'll train your model\nusing data prior to that horizon and\nyou'll forecast to some horizon in the\nfuture and then you move that window\nforward and so in this way you do get\nmultiple folds that you're able to train\nyour model and also test and so you have\nto take the time ordering into account\nthat's how you do time series cross\nvalidation\nso just before we move on to feature\nengineering i really want to highlight\nthe differences in the machine learning\nworkflow between doing your standard\nregression and classification tasks\nversus doing forecasting whilst we're\nstill operating with tabular data in\nboth cases the workflow when it comes to\nwriting your code will actually be very\ndifferent\nso for a start when we want to do our\ntrust our train test split\num we could do random allocation when\nwe're doing our typical random\nregression and classification tasks when\nwe're doing forecasting we need to split\nby time\nwhen we're creating the feature and\ntarget variable we could actually\npre-compute our features and target\nvariable before predict time before we\neven touch the model when we're doing\ntypical regression and classification\ntasks\nnow some of our features are actually\nbuilt from the target variable on demand\nat predict time those lag features when\nwe did recursive for\nforecasting and this means that you\ncan't just pre-compute all of your\nfeatures prior to predict time when\nyou're doing recursive forecasting for\nexample\nthis also changes the way that the model\nbehaves at predict time so\nwhen you're doing regression and\nclassification tasks you only need the\ntrained model and then you could provide\nan input and then you get a prediction\nwhen we're doing forecasting you need\nthe train model but you also need the\ntraining set at predict time because you\nneed information from the training set\nto construct any feature that is built\nfrom the target variable and so that\nmeans that you can't just have the\ntraining mode the trained model when you\nput your model into production you also\nneed to pass your training set as well\nso it changes the way that you put\nmodels into production\nand lastly the feature engineering looks\nquite different there are a lot of time\nseries specific feature engineering\nissues which can cause data leakage\nwhere you have information flowing from\nthe past from the future into the past\nthat you want to avoid and i'll come on\nto that in the next section of this talk\nso now i'm going to talk about feature\nengineering\nso there's a lot of future engineering\nwhen it comes to time series forecasting\nand a lot of things just specific for\ntime series\nand so this covers a wide area all the\nway from how do we impute missing data\nhow do we identify outliers can we\ntransform the data our time series to\nmake it more forecastable for example\nby applying certain transforms like the\nlog transform or deseasonalizing or\ndetrending our data before\npassing it to our forecasting model can\nwe encode how do we encode categorical\nvariables now we're dealing with time\nseries we can also extract a lot of\ninformation just from the temporal\naspect of the problem for example from\nthe calendar like the day the week the\nmonth are there any particular holidays\nwe also extract a lot of features from\npast values of both the target and other\nfeatures\nand lastly there's also a lot of\ninformation from the trend and\nseasonality in a time series for example\nare there change points is there a step\nchange somewhere in your data is there\nsome regularity that you can pick up\nfrom\nfrom the seasonality and project that\nforward so there's a whole host of\nmethods to create feature features for\ntime series forecasting and a specific\nto time series and we're going to cover\nsome of the most important ones and\nthese were also used in the\nm5 forecasting competition and so let's\ntalk a bit about those and so one thing\nyou might be asking is well what data\ncan i use to build features and the\nanswer is well any data that you know at\nthe time of making a prediction\nincluding any knowledge you have about\nfuture values of a feature let's say for\nexample your marketing spend but you\nhave to be very careful not to leak any\ndata from the future into the past and\ni'll show you how it's very easy for\nthat to that to occur it's easier to say\nthan it is to do in practice so i'll\nshow you\nsome points to be careful about\nso let's let's look at a concrete\nexample imagine that we have multiple\nproducts and we want to which means that\nwe have multiple time series and we have\nsome global advertising spend and we\nwant to predict the future value of\nthese products now i'm going to\nrepresent that as a table here so you\nhave your your time index you have your\nproduct id and for each time series we\nhave a different product id we have our\nadvertising spend which is the same\nacross both product ids and we have our\ntarget variable which is just the sales\nand so we want to predict future values\nof our products and we might know the\nadvertising spend going into the future\nlet's say\nso the first question is is how do we\nbuild our target variable for this\nexample so now we're actually dealing\nwith multiple time series\nand what you can do is if the table is\nif your time series are concatenated in\nthis way you can directly just use your\nsales value here as your target variable\none thing to note now is that the target\nvariable actually contains time\nsorry sales values from multiple\ndifferent time series all within one\ntarget a vector and the model without\nwithout features doesn't know that one\nobservation belongs to the same time\nseries and it doesn't know that two\ndifferent values belong to two different\ntime series you have to try and encode\nthat kind of information in your\nfeatures\nso let's talk about the most simple\nfeature that you can build and it's one\nthat i showed earlier which are lag\nfeatures and this is where you're\ndirectly using the past values of the\ntarget variable or feature to predict\nthe future values the intuition here is\nthat recent values of the target are\nlikely to be predictive of future values\nlikewise another trick you can do here\nis something which are called seasonal\nlags let's say you know that your time\nseries has weekly seasonality\nthen you can introduce this by just\nusing a lag of seven days for example so\nit's a very quick and dirty and very\neasy way of capturing weekly seasonality\nand i'll show you in a concrete example\nwhere this works quite well later in the\ntalk\nso you could also use the lagged version\nof other target time series to predict\nother ones so you could use previous\nvalues of one product to predict the\nfuture values of a different product\nnow the only disadvantage here is that\nif you have lots of products you will\nend up creating lots of additional\nfeatures so really this should only be\nconsidered where you have a small number\nof additional time series that you know\nto be correlated and this is a very easy\nway to model the codependency between\nyour various different target time\nseries\nand lastly we might also want to use\nlagged values of our exogenous features\ni'll give you a concrete example of that\none might be for example advertising\nspend you the advertising spend might\nhave an impact on your sales today but\nalso tomorrow and the day after and so\non\nthe effect is distributed in time\nand therefore you might want to use\nlagged versions of your advertising\nspend so the advertising spend today\nyesterday the day before and so on and\nthat's known as distributed lags because\nyou're trying to capture this\ndistributed effect in time and so that's\nanother use case for for creating lag\nfeatures from other features\nand so what does this look like when\nyou're building that on on this kind of\ntable so when you're creating your lag\nfeatures from your target variable it\nlooks very similar to what i showed\nbefore you're just using the the\nprevious values and likewise for your\nadvertising spend and so here we have\nthe lag of one and lag of two for our\nadvertising spend and then you create\nyour feature vector you're associating\nit with your target value and you\ncontinue in this way and so on and then\nyou can build out your uh your feature\nmatrix like this now it might be very\ntempting looking at this that you might\njust use pandas and then just shift your\ntime series to create the lagged\nfeatures if your table does look like\nthis however you just have to bear in\nmind that if you do just use shift then\nyou might accidentally introduce values\nfrom which belong to one time series one\nproduct id and assign them to the wrong\nproduct id at the edges so ideally you\nwant to group by your product id and\nthen apply any transformation to an\nindividual time series rather than\noperating on this table as a whole so\njust to warn you so that's one way that\nyou might accidentally\nscrew up your feature engineering there\num now a question i had in the past was\nyou end up building lots of lag features\nif you\ndo this so how do you know which flag\nfeatures to build\nand so there are three main methods that\nwe can do to try and narrow down which\nlag features we build uh what is using\ndomain knowledge another thing we can do\nis use feature selection and modeling\ntechniques and lastly we could use what\nare called time series correlation\nmethods\nso let's give a specific example where\ndomain knowledge can be quite helpful so\nlet's say we want to lag our target\nvariable what should we do well the\nsimple one is if you know the\nseasonality a priori then that will help\nyou build your seasonal lag if you're\nworking on a retail sales data set and\nyou have monthly granularity you know\nthat you will have yearly seasonality\nyou have christmas you have black friday\nyou have events which\nrepeat each year and therefore using a\nlag of one year would be an obviously\npredictive candidate to create\nanother example might be electricity\ndemand here we have an hourly\nelectricity demand data set and you'll\nhave multiple seasonalities daily weekly\nand yearly\nand so in this case you might build a\nlag feature which can is that which has\na lag of one day one week and one year\nso you can use any information you know\nabout the seasonality ahead of time and\nalso you might have some intuition about\nhow many recent lags to use you might\nknow that the your time series is highly\ncorrelated to the sales over the past\none or two days but not to say 20 days\nin the past and so that kind of gives\nyou some limit and that can take you so\nfar using some domain knowledge\nbut what if we want to use a more\nautomatic method well another approach\nis using feature selection and modeling\ntechniques so in this scenario you just\ncreate a bunch of different lag features\nfor both your target variable and any\nfeatures which you think might be\nreasonable for example if you are using\nadvertising spend it's highly unlikely\nthat the advertising spend from a year\nago is going to predict your sales today\nand so you probably don't want to create\nlag features up to a year ago but maybe\nyou choose several weeks\nand you create say i don't know 14 or so\nlag features for advertising spend and\nyou do the same thing for other features\nand your sales\nwhat you can do is then just use\nstandard feature selection and modeling\ntechniques like greedy feature forward\nselection and those methods those would\nbe very time consuming other things you\ncould do is use modeling techniques like\nlasso so just build a lasso model to do\na one step ahead forecast and let that\nautomatically determine some of your\nmost important features which also\noptimize on forecasting one step ahead\nso that's quite another easy way to try\nand narrow down which features you want\nto use\nand lastly we have what are called these\ntime series correlation methods\nand this the main idea here is you want\nto measure how correlated are these lag\nfeatures with your target variable and\nif a given lag feature is very\ncorrelated to a target variable then it\nmight be helpful\nand so\nthere are three main methods here one is\ncalled the autocorrelation function and\nanother one is called the partial\nautocorrelation function and what they\ndo is they measure how correlated your\ntime series is to a lagged version of\nitself at different lagged values so\nhere we have a retail sales data set\nwhere you have yearly\nseasonality and on this graph here we\nhave what's called an autocorrelation\nplot on the y-axis we see the\ncorrelation of the original time series\nto a lagged version of that time series\nat different lags and you see these\nlittle peaks which occur at a lag of 12\nand a lag of 24 and what that means is\nthat the time series at a lag of 12 is\nmuch more highly correlated to your\noriginal time series than other lags and\nso you can use these plots to try and\nidentify is a given lag a potentially a\ngood candidate to use for forecasting\nand then we also have the cross\ncorrelation function which allows you to\nmeasure how correlated the lag of\ndifferent features are to your target\ntime series so not measuring the\ncorrelation with the time series to\nlagged versions of itself but to lagged\nversions of other time series and so you\nend up getting similar types of plots\nwhere you look at these and say is the\ncorrelation high for a given lag okay\nthen maybe i might want to use that as a\nfeature\nso that can help you narrow down make\nsure you don't build too many features\nlet's move on to another type of feature\nwhat we call window features and this\ngeneralizes the idea of lag features\nwhere rather than directly using past\nvalues we're computing some function or\nsummary statistic over a window of past\ndata so rather than directly using the\npast three lags we're going to look at\nthe mean of the past three values for\nexample or the standard deviation which\ngives us a measure of the volatility of\nour data and use that to predict future\nvalues\nand this can be helpful because for\nexample by taking the mean you're also\nsmoothing the data so if there's a lot\nof volatility in your data then that can\nalso be helpful\nso what does this look like in practice\nso what you're going to do is you have a\nwindow of some size\nand then you apply a set of summary\nstatistics for example the mean and the\nstandard deviation and you roll that\nthrough your data\nto create your feature vectors one thing\nto note here is that we're actually\nlagging our rolling statistic we're not\nsimply using pandas taking the rolling\noperator and then applying dot mean\nwe're also doing a lag of one because if\nwe were to not take the lag of one we\nwould have then assigned the feature\nvector to this row over here i don't\nknow can you see my mouse by the way\nyeah\nwe would have assigned it over here\nwhich means that you're then lagging\nleaking information because you're using\ninformation at the time of prediction to\ncreate your features and so you create\nleakage so you also have to do some\nlagging here once again another easy way\nthat you might accidentally\nleak data from the future into the past\nso\none way of picking the window sizes\nis a technique called nested window\nfeatures and this is where you'll look\nat window sizes on different orders of\nmagnitude so you might look at window\nsizes on the orders of days weeks and\nmonths and in that way you create a\nfeature which captures some information\nat different time scales and you let the\nmodel learn about some behaviors which\noccur in different time scales there\nand so next let's move on to static\nfeatures so as i described before static\nfeatures effectively represent metadata\naround your time series so we might have\nsomething like the product id and so we\nhave different time series here which\nmeans that the static feature is\ndifferent amongst these two different\ngroup of time series\nand we might also have other types of\nstatic data let's say we have some other\nmetadata about say the product category\nmaybe one of these time series\nrepresents shorts another time series\nrepresents watches\nand then if you have a whole group of\ntime series you might realize that say\nthe time series which are of type shorts\nhave some kind of seasonality but the\nwatches time series don't it'd be great\nif the model is able to learn these\ndifferences between these groups of time\nseries\nand likewise to pass information that a\ntime series that the data in the target\nvector comes from the same time series\nand so you can do this by finding ways\nof encoding these static features\nso the most common way of dealing with\nthese kind of categorical variables in\ntabular data sets is what's called\ntarget encoding and this is where you\ngroup by your categorical variable and\nyou simply take the average of the\ntarget variable\nfor each category over the training data\nset here\nand so what we'll do here is we take for\nsku1 we'll take the mean of our training\ndata and then we'll pass that value as\nour encoded value here and we'll do the\nlikewise for sku2\nnow this may look okay however there's a\nsubtle data leakage going here so whilst\nthere is no leakage between the training\nset and the test set because we only\ncomputed the encoding on the training\nset the target is being leaked from the\nfuture values to the past values in the\ntraining set and this can actually cause\nsome degree of overfitting to this\nfeature here and i'm going to show you\nwhy\nso if we plot our time series like this\nand see what it means to create the\ntarget encoding it'll become more\nobvious\nso imagine this is our time series we've\nsplit it into our training data and our\ntest data\nif we take the target encoding by just\ntaking the mean of our training data\nlike we do here and let's say the mean\nis 14.4\nand now we impute our target our product\nid by 14.4\nwell that means that the model when it's\npredicting values let's say\nat this value here at this point in time\nit has access to information in the\nfuture because the future values were\nused when computing the the mean\nand so now that the model is able to\ndetermine that oh well the time series\nis going to increase because there's an\nincreasing trend and so there is still\nleakage from future values to past\nvalues by taking the target encoding in\nthis manner so how do you overcome this\nwell one\nsimple adjustment you can make is to\nensure that you only use the values you\nknow at that point in time\nso when you create this uh this target\nencoding\nat this time point two\nyou only use the values before time step\ntwo to create that encoding\nand then at time step three use all the\nvalues prior to that so effectively what\nyou have is this expanding window going\nacross your data set and you're taking\nthe mean across that expanding window\nand in this case now there is no data\nleakage within this feature\nnow you might have a question about how\ndo we compute the encoding for future\nvalues\nso\nif you're doing one step ahead\nforecasting this is not a problem\nbecause we can still take the mean of\nthe target\nto all uh at all points prior to that\nand so that's what we can do there\nif we're using recursive forecasting\nthen we need to recommit recompute the\nencoding at predict time to ensure\nthat the way that you've created your\ntarget at your um your encoding is\nconsistent with how you've trained the\nmodel so\nin this case we would make a one step\nahead forecast you append that to your\ntarget vector and then you recompute the\nencoding\nwith your forecast in the data in this\nway so you have to adjust the way that\nyou compute your encoding\nand likewise you'd have to make an\nadjustment for for direct forecasting as\nwell so the way that you compute your\nyour encoding depends on the method that\nyou the forecasting method that you use\nso what are the key takeaways so far\nwell data leakage is a huge risk when\ncreating features from the target\nvariable and future unknown variables\nand therefore you should only use data\nthat you're going to know at the time of\nthat target variable and this also means\nthat handling features at predict time\ncan vary depending on the forecasting\nmethod that we use and this introduces a\nlot of code complexity when implementing\nthis in practice which is what would be\nhelpful if there were some good open\nsource libraries to help us with that\nwhich is what we're going to talk about\nnow\nso what libraries are out there\nso if you're interested\nin just doing feature engineering that\nis just creating the features\non some data set and not doing\nforecasting then there's some cool\nlibraries out there ts fresh and feature\nengine are some and ts fresh computes a\nwhole host of various different time\nseries statistics so one time series\ngoes in multiple statistics come out\nnow if you're interested in that whole\nforecasting workflow on and using\ntabular data and machine learning to do\nthat well both darts and sk time help\nyou do that so these packages have a\nmuch larger scope than just doing\nforecasting with machine learning but\nthey also offer these services and\nthat's really really cool so they\nimplement these direct recursive\nforecasting strategies they also contain\ntransformers to help you build pipelines\nand so and so the advantage that they\nbring is all of the complexity in for\nexample creating lag features\ndynamically recomputing your feature at\npredict time it takes care of all of\nthat for you under the hood so that you\ndon't have to worry about that and so\nthat's one reason to use these open\nsource libraries rather than\nimplementing that logic yourself i just\nwant to walk through a concrete example\nwhich i've implemented in darts\nno reason for that i would also highly\nrecommend looking into sk time both of\nthem are fantastic libraries to use\nnow how do we do this with darts so\nimagine this is our panda series that we\nhave we have our index we have our\nvalues\nand what do we do\nso from darts we have to use a wrapper\naround our pandas series of data frames\nand so we have to import\na class called the time series class\nwhich is just a lightweight wrapper\naround a data frame\nand then what does the heavy lifting is\nwhat's called this regression model\nclass\nbuilt into darts and that's the the\nclass that will do the construction of\nyour lag features and all this kind of\nthings under the hood\nand then we can use whatever regression\nmodel we like we could use random forest\nnew regression whatever we want from\nsklearn\nyou can also use gradient boosted trees\nfrom extreme boost and light gbm and so\non so we import that here\nso we start off by uh converting our\npandas series here to a time series\nobject\nwe might want to hold out the last 24\ndata points so that we can treat that as\nour our test data\nand\nhere we specify uh and train the model\nso we have our regression model and here\nis where we specify the lag features\nwe're not creating the lag features\nseparately and then passing them to the\nmodel we're specifying them as part of\nthe model definition so here i'm saying\nuh lag the target by 1 2 and 12. and\nhere we're going to see where we have a\nyearly seasonality um a lag of 12 be\nquite helpful the data is monthly here\nwhich is why it's a lag of 12. here\nwe're going to pass a linear regression\nmodel as i said before it could also be\na random forest whatever you want and\nthen we fit the model and then we can\npredict 24 time steps into the future\nand as mentioned previously you now also\nhave to pass the training set at the\nsame time when you want to predict\nand so\nthis is the forecast here and it's it's\nvery fairly reasonable here this is the\nairline's passenger data set which is\njust a linear regression built off\nfeatures which is the lag of 1 2 and 12.\nso that's how you do recursive\nforecasting using linear regression on\none time series and only lag features\nwhat if you have uh external uh\nvariables or other other features so\nlet's say you've got advertising spend\nor you might also want to use the month\nor year as a feature as an additional\nway of capturing seasonality\nwell you just create another\ntime series object\nfrom your features so here let's say our\nfeatures is the ad spend month and year\nwe filter our data frame and then create\na time series object and we store that\nin this variable called futurecon which\nstands for future covariates which is\nthe parlance that\ndartz uses\nand then we just pass future covariates\nto our model.fit method\none detail i'll mention is that you\ncould also lag your future coverage so\nif you want to use the lag of the\nadvertising spend you can specify those\nlags as part of the model definition\nhere as well\nand over here i'm choosing not to like\nthe advertising spend then we can just\nhit model.fit\nmodel.predict we pass our future\ncovariates as well as our training data\nand then we can also get a forecast out\nso now we know how to do recursive\nforecasting using linear regression on a\nsingle time series uh with both lag\nfeatures and exogenous features\nnow let's deal with our last example\nhere which is what we have a much more\ncomplex example we have multiple time\nseries\nand we have exogenous features and we\nwant to do lag features\nso here we have our timestamp a time\nseries is defined by both country and\nproduct id so you have\none time series per country product id\ncombination our target variable is here\ny and we have our features\nso darts has a handy method called from\ngroup data frames which will convert a\npandas data frame which looks like this\ninto a set of time series objects so you\nspecify\nthe columns which define your time\nseries the time index and the target\nvariable and it returns a list of of\ntime series and that's how darts handles\nmultiple time series it needs to take a\nlist of time series and we do the same\nthing for our features so we have a list\nof features and there's a one-to-one\ncorrespondence here so this time series\nhere corresponds to the features\nassociated with that um in this list\nhere\nand then we can simply pass all of our\nfuture covariates now in our model.fit\nmethod and if we want to just for\nexample predict a subset of our time\nseries then we can do that so here we're\ngoing to say only predict the first two\ntime series that we passed in our\ntraining set and therefore you're only\ngoing to need the future covariates for\nthese first two time series and now we\nhave one model which was trained across\nmultiple time series simultaneously and\nthen at predict time it was able to\nproduce multiple forecasts for each time\nseries\nwhich is pretty cool and also these time\nseries they didn't need to be aligned\nthe time series could have been um uh\ndefined at different time points so here\nyou can see this time series didn't even\nexist during this period of time and so\nthese indexes don't need to be fine and\nso this is some of the advantages once\nagain of using a tabular machine\nlearning based approach\nso i'm just going to conclude now so\nthat we can answer answer some questions\nso forecasting can be treated as a\ntabular machine learning task and it can\nbe competitive with our traditional\nforecasting models\nthe feature engineering and machine\nlearning workflow is very different for\ntime series forecasting even though\nwe're still working with tabular data\nforecasting comes with its own set of\nfeature engineering methods and concerns\naround data leakage\nand more and more support is\nincreasingly becoming available for time\nseries related tasks in the python\necosystem so i would continue to watch\nthat space\nand if you'd like to learn more about\nthis topic then do check out this course\nthat i've created with solid garlic it's\ngoing to be launched in october feature\nengineering for forecasting\num these are some of the references for\nsome of the materials used to create\nthis talk\nand i'll now stop for any questions\nyou're probably tired of talking right\n100\nyeah i i do have a question um\nso yeah you need a small break right\nso if you can go\ncan you go please to slide 24. so in\nthis slide 24 you were talking about\nit's going to take a while to get back\nthere yeah\nso you were talking about uh like using\npredictions for um some features you\ncall them x so i don't remember this is\na very complex word\nexactly yeah yeah so i i remember this\nfrom my econometrics class but in\nmachine learning we just called them\nteachers right yeah\nuh yeah you're still coming back\nsorry one second yeah okay i'm gonna\nprobably go over and i've pressed back\ntoo many times and my computer is now\nlagging sorry there we go\nokay cool can you still see why can you\nstill see my screen\nyes\nokay perfect let's go to slide 24 okay\nyeah 24.\nokay this rainfall feature right so here\nyou\nuse a feature and then you don't know\nthe value and then what you do is you\npredict the next value right using some\nforecasting tool so what i have trouble\nunderstanding uh here or i have no\ntroubles understanding but like trying\nto figure out if it's\nokay or not so what happens here is that\nduring train time we use the actual\nvalue but then during the predict time\nwe use the prediction right wouldn't it\nbe better during to use\nthe prediction also the forecast during\nthe train time\nyeah so that you're absolutely correct\nand this is this comes a very tricky\ntopic because then the question is do\nyou want to use lagged versions because\nthat's what you're using\nat predict time\num you can do that but it depends as\nwell if you're trying to capture some\ndegree of causality as well because then\nyou know the advertising spend on the\nday or the rainfall on the day had an\nimpact on the sales so if you're quite\nconfident\nabout what that value is in the future\num then it should be okay if you're\nlacking a lot of confidence so other\nareas i see this is for example using\npage views right you might have page\nviews of some product and you want to\npredict the future values but of course\nyou don't know what the page views are\ngoing to be in like four weeks time\nuh in which case you're better off using\nthe lag values because you don't want to\nbuild a whole other model to predict\npage views on the other hand if it's\nsomething like weather you might have\nmore confidence about what that might be\nin the future um\nand therefore in that scenario it might\nbe better off also it depends if you\nwant to do scenario forecasting here as\nwell that you're trying to answer the\nquestion um i want to know uh what my\nsales are going to be tomorrow uh if\nit's raining versus if it's not raining\nif it's sunny or cloudy and so you might\nactually\nplug in different values to try and\nassess different scenarios as well in\nthat case you would also want to use the\nfeature\nas as it was on the day rather than\nnecessarily lagging it\nand yeah coming back to this wearer for\nthe casting right so i i remember i was\ntrying to\nthink how to solve this problem and then\nit's very easy to find the actual\nweather on that day on the internet\nbut it's very difficult to find the\nprediction the forecast that was for\nthis day let's say one one day before\nand then like if you want to use uh\nyou know to\nuse the prediction for training your\nmodel then how do you get data\nyeah this is this is a very tricky topic\nokay so maybe what i'll do now is i will\nshare my screen we have a lot of\nquestions and it's probably easier for\nyou if i just\nshare my screen and then you can see\nthem too\nokay and by the way sorry for the spam\nattack uh while during the\nstream i was googling like how to stop\nit so now there is a\nslow motion comments so now it's easier\nto handle spam anyways so the first\nquestion is do you have any resources um\nto learn time series forecasting both\ntraditional and deep learning algorithms\nso one resource obviously is the course\nyou just mentioned so the actually there\nis another question like how do i find\nthe course so there's the link in the\ndescription i put the link it's the\nfirst link there click on this and there\nthere will be a link to the course\nbut do you do you have other resources\nthat you could recommend\nabsolutely yeah i mean there's also a\nfantastic review paper which came out\nrelatively recently over the past few\nmonths which basically covers\num forecasting using machine learning\nand forecasting more generally not just\nmachine learnings also traditional\nstatistical models it's a very chunky\nreview paper several hundred pages long\nbut if you're looking at just on deep\ndiving into specific areas of\nforecasting i can\nlink that um review page i think the\nreview paper is also actually uh in the\nreferences section of this talk um if\nnot i will i will add some references um\ni don't know is it possible for me to\npost additional links\nuh back to the youtube channel once we\nfinish streaming yeah or you can just we\nhave to link to this thing right so this\nis uh just to show people how to find it\nso this is the first link here\nand then you can just update your\nrepo and put\nlike all the links here yeah sure yeah\ndefinitely so what i can do is i'll i\ncan update the repo there to\ntry and provide some more of those\nresources um on deep learning for time\nseries forecasting specifically i've not\nseen great introductory material\ntypically you need to have had some like\nprior knowledge of deep learning rather\nthan taking you from the very beginnings\nand therefore\nonce again it's kind of more paper\noriented\nthere are some links which i found\nuseful tutorial wise for example\num google recently released today time\nseries forecasting method called\ntemporal fusion transformers i think\nlike over some time over the past year\nor so that was quite interesting and\nthey've got quite a nice little tutorial\num explaining that um so that can also\nhelp a little bit with the deep learning\nstuff um\nas always i i if you're just getting\ninto forecasting to begin with i'd\nrecommend rob heinmann's book\nforecasting and i've got the full name\nof the book it's probably considered\nkind of the bible entry textbook for uh\nfor forecasting um\nthere we go forecasting principles\npractice yeah yeah and then it's free\nit's online uh yes it's an r but like r\nactually has a very nice ecosystem\nuh of like all these tools of libraries\nlong before python had anything similar\ni think it's still like a few steps\nahead i i definitely agree which is why\ni had to make some contributions to\nstats models because there'd be new\nstuff coming out in r which didn't exist\nin stats models so you have to implement\nthem yourself if you want them in python\nokay thanks um\nif you forecast once a month should you\nretain every month before you run your\nforecast\nit's a good question it depends how much\nyou expect the distribution the data to\nchange month and month if it's only\nmonthly i would still encourage it um\njust because you can then refresh the\nmodel\nso it picks up any recent changes in in\nyour training distribution um i don't\nthink it's computationally going to be\ntoo demanding to do that\nthe only downsides is that you should at\nleast check for model stability do your\nmodel coefficients jump around\nsignificantly every month by ending up\nwith very different models if so you\nwant to examine why and which features\nare moving around\nbecause you don't want to end up um\nusing a very different forecasting model\neffectively every month by retraining um\nand so that's just a way of just kind of\nsense checking whilst your machine\nlearning model is in production\nokay yeah and if you run it every day\nthen maybe it's too cumbersome to\nretrain it right\nmaybe you can retrain it like every\nmonth absolutely and also if you're if\nyou're forecasting let's say one month\nahead um your data may not have changed\nsignificantly i'm not sure having an\nadditional forecast is actually going to\nbe beneficial\nand that's actually a question from me\nso while you were talking about\ni think it was midway through your talk\nand then\n[Music]\neven during the half of the talk you\nsaid so many times that you should be\ncareful you can introduce data leaks\nlike you can accidentally use the data\nfrom the future\nand my question was like how to guard\nagainst that and i guess you've partly\nanswered that you can use a library that\nyou know i guess yeah\ni yeah i mean i guess from a practical\nperspective um there are some libraries\nwhich will kind of handle so okay when\nwhen does that data leakage appear um\nwhen you create new features it's\ntypically when you're doing some kind of\nlagging or rolling window and using the\npast values right and so um there are\nmethods both in for example feature\nengine uh sk time\nuh darts where you know this kind of\nbehavior is covered for you in the\ntransformers that are provided or in the\nmodel objects so that will hopefully\nhelp guard to that guard you against\nthat to some extent um\nin other cases also for crossband the\nother example i was going to mention as\nwell is when you're doing cross\nvalidation\nyou have to rebuild your training set um\nas you're moving your time step forward\nduring cross foundation right so that's\nanother area where it can\ncome into play but once again these\nlibraries also help you with cross\nvalidation so i guess i'd say the\neasiest way is to use other libraries um\nif not and you're going to build your\nown package\nmy other advice is use a lot of unit\ntesting when you're building these so\nbuild unit tests for any transformations\nyou're building where you're going to\ncreate some kind of feature that uses\nlike values and then lagging them\num\ni think there was another comment as\nwell i had um\nsorry i've not had enough coffee today\nif it comes back to me i'll come back to\nthis topic\nokay so in the meantime i think you will\nlike this question oh yes okay i think i\nhad okay it came back to me\nyes\nyeah absolutely so the other one is um\nif you notice that your model is\nunreasonably good\nif you notice that um your forecasts are\njust very accurate\nchances are you haven't suddenly built a\ncrystal ball you've somehow leaked data\nfrom the future into the past and that's\none reason why your forecast looks very\nvery good\nand that's another sign that you might\nhave actually leaked data from the the\nfuture into the past so that's another\nsanity check don't expect your\nforecasting models to look very good\nyeah thanks i accidentally marked one\ninteresting question as answered so i\nclicked on this so this question was how\nwould you deal with effects of events\nlike covet or financial crisis where\nbehavior of the time series going\nforward might be very difficult\ni think this is such a an interesting\nquestion and i think something that\neverybody is facing in machine learning\nwhere you're having to train on on past\ndata where the distribution of the data\nhas changed significantly so\num what can you do some simple things\nyou could try and do so with time series\nforecasting you would hope that the most\nimportant data is your recent data\nand so one thing you can do is\npotentially truncate your training data\nto only train over those time periods\nnow you come into a more complex\nsituation you had covered covet is kind\nof the behaviors of code probably are\nnot as prominent now as it was during\nthat period so would you be under\nforecasting if for example\ncovid had\nwas still having an impact\nand this is just very challenging to\nhandle some things you can do is build\ntwo forecasting models one with covered\ndata one without\nand then kind of take an average between\nthe two you have to just be very\npragmatic and caveat the outputs that\nyou provide to your stakeholders other\nways you can handle this is potentially\nby introducing a new feature\num or just giving more weight to recent\ntime series to recent data so i\nmentioned earlier you could use the\nsample weights option so one thing you\ncan do is just give more weight to\nrecent time periods um also i wouldn't\nbe adverse to adding\nadditional business logic on top of your\nmachine learning output like review the\nforecast does it make sense has the\nbusiness been growing this fast in the\npast are you sure that if your forecast\nsays it's going to grow 200 and it's\nonly ever grown 30 year and year\nmaybe your forecast isn't so realistic\nand you might need to\nartificially constrain your forecast\nsomehow right um\nso yeah this is the most practical\nadvice i can give them on that on that\nquestion\nyeah this one i think you answered the\ncourse will be available in october am i\nright yeah so in october we're hoping to\npre-launch with most of the material we\nhave up until then which will hopefully\nbe over 12 hours of material um it's\nquite a wide scape and we want to\ncontinue adding to the course so even\nafter those those 12 hours of materials\nwe'll still be adding more so we'll have\na pre-launch in october which takes you\nquite a long way but even after that\nwe're going to continue adding materials\num on a monthly basis but um you can\nalready subscribe to this and then be\nnotified when it goes live right yes\nthat's why you have this email box yeah\nexactly so since it's live you can get\naccess\nso a question from video guido i have a\nproblem like your example predict\nmultiple series and i need to predict\nonly one step but i have a lead time of\n30\nhow would you hit predict and lead time\nlike maybe i i'm not sure what lead time\nis maybe\nwe can answer that\num i'm trying to interpret the question\nso i can uh properly answer it so a lead\ntimes 30 days i'm assuming that means\nyou're trying to predict\n30 days into the future\nor maybe it's a delay if it's a delay um\ni mean the only advice if it's a delay\nthen you should i mean the the easiest\nthings you do then is you have to lag\nyour features right so you know the the\nbasic premise is with the information\nyou have at predict time if you need to\npredict um let's say t equals 31\nand beyond but you don't care about time\nt equals zero to 30.\num then you will have to lag a lot of\nyour features um and use um a lag\nfeature let's say you're predicting\nwhatever you have now up until 30 days\nand beyond\ni've seen this in practice as well so\nlet's say you're doing recursive\nforecasting\nand let's say you're interested in\npredicting time t equals 30 to 60 so one\nmonth into the future you could still\nuse recursive forecasting from time t\nequal 1 to 60 discard the first 30 for\nthe first 30 predictions and then just\nuse the remaining 30\nor just to use direct forecasting where\nyou directly build the target as being\nday 31\ninto the future as long as you're\nkeeping to that principle that you're\nonly using the information that you're\ngoing to have at time equals zero and\nthen you're not using information\nbetween zero and 30 you'll ensure that\nyou avoid leakage\nyeah i'm afraid we're running out of\ntime but we have quite a few questions\nso maybe what i can propose i don't know\nif uh\nif you can do this uh but maybe we can\nmove these questions to slack and then\ntake them offline\nwould it work for you yeah more than\nhappy to do so okay so i will in a\ncouple of hours uh i will\nput all these questions to slack we have\na special channel i think it's called\nevents q a something like this and i\nwill make an announcement in our\nannouncements channel so you will be\nable to see it\nand i'll put the questions there and we\ncan take them offline great okay\nyeah amazing presentation packed with a\nlot of knowledge\ni am i'm sure many people will\nbe watch it and\nuse it and i will certainly do\num yeah thanks a lot thanks for joining\nus today thanks for sharing all this\nknowledge and thanks everyone for\njoining us today as well for asking\nquestions for being active i apologize\nthat we couldn't cover all your\nquestions but we will try to answer them\nanyways\nokay thank you\nyeah so yeah see you soon\n",
  "words": [
    "hi",
    "everyone",
    "thanks",
    "joining",
    "us",
    "today",
    "event",
    "brought",
    "brought",
    "data",
    "talks",
    "club",
    "community",
    "people",
    "love",
    "data",
    "weekly",
    "events",
    "one",
    "events",
    "want",
    "find",
    "events",
    "link",
    "description",
    "go",
    "description",
    "check",
    "quite",
    "events",
    "plan",
    "check",
    "see",
    "something",
    "interesting",
    "register",
    "sign",
    "subscribed",
    "youtube",
    "channel",
    "whatever",
    "reason",
    "time",
    "best",
    "time",
    "fix",
    "red",
    "button",
    "video",
    "click",
    "get",
    "notified",
    "future",
    "videos",
    "amazing",
    "slack",
    "community",
    "join",
    "hang",
    "data",
    "enthusiasts",
    "suggest",
    "missing",
    "last",
    "least",
    "amazing",
    "course",
    "starting",
    "soon",
    "september",
    "course",
    "covers",
    "basics",
    "machine",
    "learning",
    "engineering",
    "also",
    "link",
    "description",
    "watching",
    "right",
    "want",
    "learn",
    "want",
    "learn",
    "machine",
    "learning",
    "engineering",
    "check",
    "totally",
    "free",
    "yeah",
    "like",
    "today",
    "presentation",
    "ask",
    "question",
    "want",
    "asking",
    "questions",
    "way",
    "prefer",
    "ask",
    "questions",
    "end",
    "interrupt",
    "relevant",
    "question",
    "ask",
    "question",
    "um",
    "prefer",
    "end",
    "okay",
    "yeah",
    "totally",
    "okay",
    "keep",
    "questions",
    "end",
    "q",
    "session",
    "link",
    "linked",
    "live",
    "chat",
    "use",
    "asking",
    "questions",
    "think",
    "probably",
    "talking",
    "stuff",
    "fun",
    "stuff",
    "floor",
    "please",
    "start",
    "great",
    "uh",
    "start",
    "actually",
    "got",
    "fan",
    "background",
    "warm",
    "wan",
    "na",
    "make",
    "sure",
    "interrupting",
    "audio",
    "hear",
    "anything",
    "also",
    "fun",
    "maybe",
    "hear",
    "okay",
    "cool",
    "anyone",
    "uh",
    "watching",
    "live",
    "hear",
    "fan",
    "background",
    "let",
    "know",
    "turn",
    "um",
    "great",
    "firstly",
    "like",
    "thank",
    "alexia",
    "alexian",
    "france",
    "inviting",
    "give",
    "talk",
    "datatalks",
    "club",
    "first",
    "time",
    "hopefully",
    "last",
    "um",
    "today",
    "talking",
    "feature",
    "engineering",
    "time",
    "series",
    "forecasting",
    "talk",
    "go",
    "let",
    "quickly",
    "introduce",
    "name",
    "kishan",
    "work",
    "data",
    "science",
    "manager",
    "luxury",
    "fashion",
    "company",
    "called",
    "farfetch",
    "work",
    "time",
    "series",
    "forecasting",
    "pricing",
    "optimization",
    "entered",
    "data",
    "science",
    "phd",
    "physics",
    "working",
    "building",
    "models",
    "abnormal",
    "heart",
    "rhythms",
    "large",
    "scale",
    "time",
    "series",
    "analysis",
    "also",
    "like",
    "contribute",
    "open",
    "source",
    "possible",
    "relevant",
    "recent",
    "contributions",
    "would",
    "stats",
    "models",
    "contributed",
    "additional",
    "functionality",
    "time",
    "series",
    "decomposition",
    "methods",
    "collaboration",
    "soledad",
    "girly",
    "also",
    "building",
    "online",
    "course",
    "topic",
    "feature",
    "engineering",
    "forecasting",
    "interested",
    "please",
    "check",
    "find",
    "slides",
    "today",
    "already",
    "github",
    "link",
    "maybe",
    "send",
    "links",
    "post",
    "live",
    "chat",
    "description",
    "people",
    "check",
    "right",
    "let",
    "open",
    "send",
    "least",
    "link",
    "slides",
    "people",
    "able",
    "sent",
    "slides",
    "chat",
    "great",
    "uh",
    "yes",
    "received",
    "live",
    "chat",
    "seen",
    "someone",
    "ah",
    "live",
    "chat",
    "okay",
    "yeah",
    "maybe",
    "send",
    "zoom",
    "live",
    "chat",
    "youtube",
    "might",
    "block",
    "links",
    "coming",
    "stream",
    "owner",
    "okay",
    "one",
    "second",
    "sorry",
    "everyone",
    "think",
    "might",
    "want",
    "links",
    "later",
    "come",
    "back",
    "find",
    "okay",
    "link",
    "post",
    "live",
    "chat",
    "sorry",
    "interruption",
    "link",
    "okay",
    "perfect",
    "actual",
    "topic",
    "talk",
    "talk",
    "convert",
    "time",
    "series",
    "forecasting",
    "task",
    "want",
    "predict",
    "future",
    "value",
    "sequence",
    "set",
    "sequences",
    "convert",
    "tabular",
    "machine",
    "learning",
    "regression",
    "task",
    "table",
    "features",
    "target",
    "variable",
    "allows",
    "us",
    "use",
    "favorite",
    "models",
    "like",
    "new",
    "aggression",
    "random",
    "forest",
    "two",
    "main",
    "themes",
    "talk",
    "one",
    "forecasting",
    "using",
    "traditional",
    "machine",
    "learning",
    "models",
    "secondly",
    "build",
    "good",
    "features",
    "start",
    "start",
    "talking",
    "time",
    "series",
    "forecasting",
    "using",
    "machine",
    "learning",
    "models",
    "want",
    "motivate",
    "want",
    "use",
    "machine",
    "learning",
    "forecasting",
    "way",
    "example",
    "couple",
    "years",
    "ago",
    "kaggle",
    "competition",
    "called",
    "m5",
    "forecasting",
    "competition",
    "objective",
    "predict",
    "sales",
    "set",
    "goods",
    "walmart",
    "interesting",
    "problem",
    "characteristics",
    "data",
    "set",
    "large",
    "number",
    "correlated",
    "time",
    "series",
    "forecast",
    "30",
    "000",
    "product",
    "store",
    "combinations",
    "therefore",
    "30",
    "000",
    "time",
    "series",
    "forecast",
    "products",
    "lied",
    "hierarchical",
    "structure",
    "given",
    "product",
    "could",
    "belong",
    "particular",
    "department",
    "belonged",
    "category",
    "sales",
    "product",
    "belong",
    "store",
    "could",
    "belong",
    "state",
    "hierarchical",
    "structure",
    "data",
    "see",
    "examples",
    "time",
    "series",
    "bottom",
    "right",
    "see",
    "varying",
    "length",
    "time",
    "series",
    "come",
    "online",
    "different",
    "points",
    "time",
    "looking",
    "time",
    "series",
    "see",
    "high",
    "degree",
    "sparsity",
    "intermittency",
    "obvious",
    "seasonality",
    "trend",
    "looking",
    "data",
    "also",
    "would",
    "imagine",
    "exogenous",
    "variables",
    "features",
    "like",
    "price",
    "promotional",
    "activity",
    "would",
    "also",
    "important",
    "forecasting",
    "sales",
    "products",
    "would",
    "also",
    "expect",
    "multiple",
    "seasonal",
    "patterns",
    "might",
    "expect",
    "weekly",
    "pattern",
    "weekend",
    "would",
    "different",
    "weekdays",
    "summer",
    "different",
    "winter",
    "multiple",
    "seasonalities",
    "interesting",
    "type",
    "data",
    "set",
    "turned",
    "top",
    "performing",
    "methods",
    "competition",
    "actually",
    "quote",
    "unquote",
    "pure",
    "machine",
    "learning",
    "models",
    "better",
    "traditional",
    "statistical",
    "benchmarks",
    "used",
    "time",
    "series",
    "forecasting",
    "exponential",
    "smoothing",
    "arima",
    "lot",
    "top",
    "comp",
    "top",
    "winning",
    "solutions",
    "saw",
    "light",
    "gbm",
    "turns",
    "uh",
    "machine",
    "learning",
    "models",
    "like",
    "light",
    "gbm",
    "able",
    "learn",
    "simultaneously",
    "across",
    "multiple",
    "different",
    "time",
    "series",
    "integrate",
    "exogenous",
    "features",
    "overall",
    "allowed",
    "give",
    "better",
    "forecasting",
    "accuracy",
    "advantage",
    "using",
    "machine",
    "learning",
    "gives",
    "access",
    "tools",
    "options",
    "example",
    "could",
    "specify",
    "sample",
    "weights",
    "would",
    "allow",
    "example",
    "give",
    "weight",
    "recent",
    "data",
    "data",
    "far",
    "past",
    "also",
    "cust",
    "specify",
    "custom",
    "loss",
    "functions",
    "actually",
    "winning",
    "solution",
    "custom",
    "loss",
    "function",
    "actually",
    "used",
    "get",
    "free",
    "using",
    "machine",
    "learning",
    "models",
    "said",
    "neglect",
    "simple",
    "traditional",
    "statistical",
    "baselines",
    "90",
    "percent",
    "participating",
    "teams",
    "beat",
    "simple",
    "statistical",
    "baseline",
    "model",
    "exponential",
    "smoothing",
    "means",
    "still",
    "use",
    "benchmarks",
    "even",
    "top",
    "50",
    "solutions",
    "improved",
    "exponential",
    "smoothing",
    "15",
    "20",
    "percent",
    "depending",
    "granularity",
    "time",
    "series",
    "looking",
    "result",
    "still",
    "continue",
    "benchmark",
    "simple",
    "methods",
    "rule",
    "entirely",
    "overall",
    "advice",
    "use",
    "kind",
    "simpler",
    "traditional",
    "methods",
    "dealing",
    "would",
    "kind",
    "call",
    "easy",
    "time",
    "series",
    "see",
    "strong",
    "seasonality",
    "trend",
    "small",
    "number",
    "time",
    "series",
    "correlated",
    "could",
    "learn",
    "one",
    "model",
    "per",
    "time",
    "series",
    "see",
    "lot",
    "sparsity",
    "intermittency",
    "little",
    "exogenous",
    "features",
    "case",
    "probably",
    "better",
    "using",
    "something",
    "like",
    "arima",
    "exponential",
    "smoothing",
    "profit",
    "consider",
    "using",
    "machine",
    "learning",
    "models",
    "much",
    "complex",
    "problem",
    "might",
    "encounter",
    "business",
    "setting",
    "kind",
    "hopefully",
    "gives",
    "motivation",
    "use",
    "kind",
    "method",
    "go",
    "forecasting",
    "machine",
    "learning",
    "start",
    "time",
    "series",
    "fake",
    "time",
    "series",
    "going",
    "call",
    "sales",
    "information",
    "time",
    "want",
    "predict",
    "sales",
    "time",
    "plus",
    "one",
    "want",
    "convert",
    "time",
    "series",
    "table",
    "features",
    "target",
    "variable",
    "create",
    "target",
    "variable",
    "case",
    "directly",
    "going",
    "use",
    "time",
    "series",
    "want",
    "predict",
    "create",
    "feature",
    "vectors",
    "want",
    "create",
    "features",
    "use",
    "information",
    "past",
    "using",
    "data",
    "known",
    "time",
    "target",
    "allow",
    "us",
    "avoid",
    "called",
    "lookahead",
    "bias",
    "use",
    "information",
    "future",
    "predict",
    "future",
    "would",
    "result",
    "data",
    "leakage",
    "show",
    "constructing",
    "feature",
    "vector",
    "uh",
    "target",
    "perhaps",
    "future",
    "basically",
    "going",
    "create",
    "function",
    "previous",
    "values",
    "simplest",
    "thing",
    "directly",
    "use",
    "past",
    "values",
    "create",
    "feature",
    "vector",
    "previous",
    "values",
    "time",
    "series",
    "using",
    "called",
    "lag",
    "one",
    "lag",
    "two",
    "lag",
    "three",
    "lag",
    "one",
    "lag",
    "two",
    "leg",
    "three",
    "populate",
    "feature",
    "vectors",
    "associate",
    "target",
    "way",
    "create",
    "data",
    "leakage",
    "go",
    "course",
    "going",
    "missing",
    "data",
    "start",
    "time",
    "series",
    "ca",
    "lag",
    "go",
    "back",
    "derived",
    "features",
    "past",
    "value",
    "target",
    "time",
    "series",
    "called",
    "lag",
    "features",
    "also",
    "create",
    "complex",
    "features",
    "previous",
    "values",
    "get",
    "later",
    "another",
    "class",
    "features",
    "know",
    "value",
    "past",
    "future",
    "one",
    "example",
    "might",
    "advertising",
    "spend",
    "might",
    "observed",
    "advertising",
    "spend",
    "past",
    "observe",
    "sales",
    "might",
    "budget",
    "know",
    "advertising",
    "spend",
    "future",
    "quite",
    "easy",
    "create",
    "feature",
    "use",
    "directly",
    "might",
    "also",
    "features",
    "know",
    "value",
    "past",
    "know",
    "future",
    "examples",
    "would",
    "weather",
    "related",
    "features",
    "might",
    "observed",
    "rainfall",
    "past",
    "might",
    "affect",
    "sales",
    "product",
    "maybe",
    "selling",
    "ice",
    "cream",
    "know",
    "um",
    "future",
    "populate",
    "feature",
    "well",
    "things",
    "one",
    "use",
    "alternative",
    "forecast",
    "use",
    "weather",
    "forecast",
    "um",
    "pull",
    "somewhere",
    "else",
    "use",
    "feature",
    "alternatively",
    "create",
    "naive",
    "forecast",
    "might",
    "extrapolate",
    "previous",
    "value",
    "project",
    "outwards",
    "neither",
    "work",
    "might",
    "also",
    "use",
    "lag",
    "features",
    "type",
    "features",
    "rather",
    "projecting",
    "use",
    "lag",
    "value",
    "rainfall",
    "predict",
    "sales",
    "tomorrow",
    "look",
    "rainfall",
    "today",
    "example",
    "lastly",
    "set",
    "features",
    "probably",
    "discussed",
    "much",
    "one",
    "call",
    "static",
    "features",
    "called",
    "static",
    "features",
    "typically",
    "represent",
    "metadata",
    "time",
    "series",
    "change",
    "given",
    "time",
    "series",
    "might",
    "sales",
    "particular",
    "country",
    "like",
    "uk",
    "might",
    "ask",
    "useful",
    "useful",
    "start",
    "dealing",
    "multiple",
    "time",
    "series",
    "metadata",
    "vary",
    "time",
    "series",
    "might",
    "also",
    "sales",
    "germany",
    "countries",
    "try",
    "model",
    "learn",
    "information",
    "different",
    "groups",
    "time",
    "series",
    "static",
    "features",
    "come",
    "play",
    "group",
    "features",
    "organized",
    "table",
    "target",
    "variable",
    "form",
    "training",
    "data",
    "come",
    "later",
    "around",
    "uh",
    "process",
    "categorical",
    "features",
    "dealing",
    "missing",
    "data",
    "kind",
    "stuff",
    "essentially",
    "training",
    "data",
    "allows",
    "us",
    "take",
    "whatever",
    "standard",
    "sk",
    "learn",
    "type",
    "model",
    "run",
    "dot",
    "fit",
    "predict",
    "one",
    "step",
    "future",
    "practice",
    "want",
    "forecasting",
    "want",
    "forecast",
    "one",
    "step",
    "future",
    "typically",
    "forecast",
    "multiple",
    "steps",
    "future",
    "two",
    "main",
    "methods",
    "allow",
    "us",
    "one",
    "called",
    "direct",
    "forecasting",
    "one",
    "called",
    "recursive",
    "forecasting",
    "direct",
    "forecasting",
    "going",
    "directly",
    "predict",
    "future",
    "values",
    "effectively",
    "going",
    "use",
    "features",
    "built",
    "past",
    "values",
    "build",
    "models",
    "different",
    "target",
    "variables",
    "means",
    "multiple",
    "models",
    "different",
    "target",
    "variable",
    "effectively",
    "features",
    "many",
    "models",
    "per",
    "forecast",
    "time",
    "step",
    "let",
    "illustrate",
    "let",
    "say",
    "creating",
    "feature",
    "vector",
    "start",
    "time",
    "point",
    "create",
    "feature",
    "vector",
    "lag",
    "values",
    "associate",
    "target",
    "variable",
    "one",
    "step",
    "ahead",
    "also",
    "want",
    "build",
    "model",
    "predicts",
    "two",
    "steps",
    "ahead",
    "use",
    "target",
    "variable",
    "two",
    "steps",
    "time",
    "frame",
    "considering",
    "likewise",
    "third",
    "time",
    "step",
    "feature",
    "vector",
    "multiple",
    "different",
    "target",
    "variables",
    "drag",
    "time",
    "line",
    "forward",
    "construct",
    "feature",
    "vectors",
    "target",
    "variables",
    "effectively",
    "pass",
    "feature",
    "vectors",
    "different",
    "models",
    "give",
    "different",
    "target",
    "variables",
    "training",
    "pass",
    "input",
    "feature",
    "vector",
    "representing",
    "recent",
    "time",
    "horizon",
    "pass",
    "input",
    "vector",
    "models",
    "get",
    "forecast",
    "model",
    "giving",
    "different",
    "forecast",
    "future",
    "model",
    "gives",
    "forecast",
    "another",
    "model",
    "gives",
    "forecast",
    "obvious",
    "downside",
    "going",
    "train",
    "multiple",
    "models",
    "per",
    "forecasting",
    "step",
    "want",
    "28",
    "days",
    "future",
    "28",
    "models",
    "introduces",
    "lot",
    "complexity",
    "simpler",
    "way",
    "well",
    "recursive",
    "forecasting",
    "tries",
    "take",
    "account",
    "recursive",
    "forecasting",
    "build",
    "features",
    "fit",
    "model",
    "going",
    "recursively",
    "use",
    "model",
    "show",
    "build",
    "model",
    "predict",
    "one",
    "step",
    "ahead",
    "forecast",
    "normal",
    "going",
    "append",
    "forecast",
    "back",
    "original",
    "training",
    "data",
    "recreate",
    "features",
    "target",
    "variable",
    "forecast",
    "plug",
    "back",
    "model",
    "get",
    "next",
    "forecast",
    "way",
    "use",
    "one",
    "fitted",
    "model",
    "predict",
    "multiple",
    "steps",
    "future",
    "let",
    "show",
    "exactly",
    "works",
    "tabular",
    "data",
    "set",
    "let",
    "assume",
    "already",
    "processed",
    "static",
    "features",
    "future",
    "unknown",
    "features",
    "feature",
    "future",
    "known",
    "features",
    "dealing",
    "lag",
    "features",
    "build",
    "iteratively",
    "first",
    "step",
    "ahead",
    "going",
    "build",
    "features",
    "past",
    "values",
    "shown",
    "predict",
    "one",
    "step",
    "future",
    "append",
    "prediction",
    "back",
    "target",
    "variable",
    "recreate",
    "feature",
    "vector",
    "augmented",
    "target",
    "variable",
    "forecast",
    "dot",
    "predict",
    "feature",
    "vector",
    "append",
    "forecast",
    "continue",
    "way",
    "forecasting",
    "using",
    "one",
    "model",
    "let",
    "talk",
    "pros",
    "cons",
    "methods",
    "one",
    "advantages",
    "direct",
    "forecasting",
    "model",
    "building",
    "directly",
    "optimized",
    "forecasting",
    "step",
    "going",
    "build",
    "10",
    "step",
    "ahead",
    "forecast",
    "built",
    "model",
    "precisely",
    "contrast",
    "recursive",
    "forecasting",
    "built",
    "one",
    "model",
    "optimized",
    "building",
    "one",
    "step",
    "ahead",
    "forecast",
    "might",
    "imagine",
    "way",
    "model",
    "uses",
    "features",
    "forecast",
    "near",
    "term",
    "might",
    "different",
    "long",
    "term",
    "advantages",
    "disadvantages",
    "point",
    "cons",
    "direct",
    "forecasting",
    "obviously",
    "use",
    "multiple",
    "models",
    "involves",
    "computational",
    "cost",
    "harder",
    "maintain",
    "also",
    "forecasts",
    "independent",
    "forecasting",
    "model",
    "forecast",
    "one",
    "step",
    "ahead",
    "independent",
    "second",
    "step",
    "ahead",
    "third",
    "step",
    "ahead",
    "ca",
    "guarantee",
    "correlated",
    "near",
    "one",
    "another",
    "advantages",
    "recursive",
    "forecasting",
    "one",
    "model",
    "means",
    "less",
    "computational",
    "time",
    "forecasts",
    "definition",
    "dependent",
    "using",
    "forecast",
    "generate",
    "next",
    "forecast",
    "correlated",
    "one",
    "disadvantages",
    "lot",
    "additional",
    "code",
    "complexity",
    "um",
    "iteratively",
    "create",
    "features",
    "append",
    "target",
    "variable",
    "show",
    "overcome",
    "open",
    "source",
    "libraries",
    "later",
    "also",
    "propagation",
    "errors",
    "one",
    "step",
    "ahead",
    "forecast",
    "significant",
    "error",
    "obviously",
    "propagate",
    "second",
    "step",
    "ahead",
    "third",
    "step",
    "ahead",
    "way",
    "errors",
    "propagate",
    "mentioned",
    "optimized",
    "one",
    "step",
    "ahead",
    "things",
    "keep",
    "mind",
    "considering",
    "whether",
    "use",
    "direct",
    "recursive",
    "forecasting",
    "want",
    "talk",
    "little",
    "bit",
    "would",
    "validate",
    "forecasting",
    "model",
    "traditionally",
    "work",
    "tabular",
    "data",
    "want",
    "kind",
    "train",
    "test",
    "split",
    "could",
    "randomly",
    "shuffle",
    "data",
    "original",
    "data",
    "training",
    "test",
    "set",
    "could",
    "random",
    "manner",
    "assume",
    "row",
    "independent",
    "way",
    "therefore",
    "randomly",
    "split",
    "data",
    "problem",
    "dealing",
    "time",
    "series",
    "data",
    "despite",
    "fact",
    "still",
    "working",
    "tabular",
    "data",
    "split",
    "randomly",
    "time",
    "ordering",
    "data",
    "means",
    "row",
    "longer",
    "independent",
    "one",
    "another",
    "randomly",
    "shuffle",
    "data",
    "set",
    "might",
    "values",
    "future",
    "land",
    "training",
    "data",
    "values",
    "past",
    "landing",
    "test",
    "data",
    "using",
    "future",
    "predict",
    "past",
    "inaccurate",
    "idea",
    "well",
    "model",
    "actually",
    "performs",
    "practice",
    "instead",
    "need",
    "split",
    "time",
    "replicate",
    "actual",
    "forecasting",
    "process",
    "look",
    "like",
    "time",
    "series",
    "cross",
    "validation",
    "arrange",
    "data",
    "time",
    "full",
    "data",
    "set",
    "split",
    "data",
    "certain",
    "time",
    "horizons",
    "train",
    "data",
    "train",
    "model",
    "using",
    "data",
    "prior",
    "horizon",
    "forecast",
    "horizon",
    "future",
    "move",
    "window",
    "forward",
    "way",
    "get",
    "multiple",
    "folds",
    "able",
    "train",
    "model",
    "also",
    "test",
    "take",
    "time",
    "ordering",
    "account",
    "time",
    "series",
    "cross",
    "validation",
    "move",
    "feature",
    "engineering",
    "really",
    "want",
    "highlight",
    "differences",
    "machine",
    "learning",
    "workflow",
    "standard",
    "regression",
    "classification",
    "tasks",
    "versus",
    "forecasting",
    "whilst",
    "still",
    "operating",
    "tabular",
    "data",
    "cases",
    "workflow",
    "comes",
    "writing",
    "code",
    "actually",
    "different",
    "start",
    "want",
    "trust",
    "train",
    "test",
    "split",
    "um",
    "could",
    "random",
    "allocation",
    "typical",
    "random",
    "regression",
    "classification",
    "tasks",
    "forecasting",
    "need",
    "split",
    "time",
    "creating",
    "feature",
    "target",
    "variable",
    "could",
    "actually",
    "features",
    "target",
    "variable",
    "predict",
    "time",
    "even",
    "touch",
    "model",
    "typical",
    "regression",
    "classification",
    "tasks",
    "features",
    "actually",
    "built",
    "target",
    "variable",
    "demand",
    "predict",
    "time",
    "lag",
    "features",
    "recursive",
    "forecasting",
    "means",
    "ca",
    "features",
    "prior",
    "predict",
    "time",
    "recursive",
    "forecasting",
    "example",
    "also",
    "changes",
    "way",
    "model",
    "behaves",
    "predict",
    "time",
    "regression",
    "classification",
    "tasks",
    "need",
    "trained",
    "model",
    "could",
    "provide",
    "input",
    "get",
    "prediction",
    "forecasting",
    "need",
    "train",
    "model",
    "also",
    "need",
    "training",
    "set",
    "predict",
    "time",
    "need",
    "information",
    "training",
    "set",
    "construct",
    "feature",
    "built",
    "target",
    "variable",
    "means",
    "ca",
    "training",
    "mode",
    "trained",
    "model",
    "put",
    "model",
    "production",
    "also",
    "need",
    "pass",
    "training",
    "set",
    "well",
    "changes",
    "way",
    "put",
    "models",
    "production",
    "lastly",
    "feature",
    "engineering",
    "looks",
    "quite",
    "different",
    "lot",
    "time",
    "series",
    "specific",
    "feature",
    "engineering",
    "issues",
    "cause",
    "data",
    "leakage",
    "information",
    "flowing",
    "past",
    "future",
    "past",
    "want",
    "avoid",
    "come",
    "next",
    "section",
    "talk",
    "going",
    "talk",
    "feature",
    "engineering",
    "lot",
    "future",
    "engineering",
    "comes",
    "time",
    "series",
    "forecasting",
    "lot",
    "things",
    "specific",
    "time",
    "series",
    "covers",
    "wide",
    "area",
    "way",
    "impute",
    "missing",
    "data",
    "identify",
    "outliers",
    "transform",
    "data",
    "time",
    "series",
    "make",
    "forecastable",
    "example",
    "applying",
    "certain",
    "transforms",
    "like",
    "log",
    "transform",
    "deseasonalizing",
    "detrending",
    "data",
    "passing",
    "forecasting",
    "model",
    "encode",
    "encode",
    "categorical",
    "variables",
    "dealing",
    "time",
    "series",
    "also",
    "extract",
    "lot",
    "information",
    "temporal",
    "aspect",
    "problem",
    "example",
    "calendar",
    "like",
    "day",
    "week",
    "month",
    "particular",
    "holidays",
    "also",
    "extract",
    "lot",
    "features",
    "past",
    "values",
    "target",
    "features",
    "lastly",
    "also",
    "lot",
    "information",
    "trend",
    "seasonality",
    "time",
    "series",
    "example",
    "change",
    "points",
    "step",
    "change",
    "somewhere",
    "data",
    "regularity",
    "pick",
    "seasonality",
    "project",
    "forward",
    "whole",
    "host",
    "methods",
    "create",
    "feature",
    "features",
    "time",
    "series",
    "forecasting",
    "specific",
    "time",
    "series",
    "going",
    "cover",
    "important",
    "ones",
    "also",
    "used",
    "m5",
    "forecasting",
    "competition",
    "let",
    "talk",
    "bit",
    "one",
    "thing",
    "might",
    "asking",
    "well",
    "data",
    "use",
    "build",
    "features",
    "answer",
    "well",
    "data",
    "know",
    "time",
    "making",
    "prediction",
    "including",
    "knowledge",
    "future",
    "values",
    "feature",
    "let",
    "say",
    "example",
    "marketing",
    "spend",
    "careful",
    "leak",
    "data",
    "future",
    "past",
    "show",
    "easy",
    "occur",
    "easier",
    "say",
    "practice",
    "show",
    "points",
    "careful",
    "let",
    "let",
    "look",
    "concrete",
    "example",
    "imagine",
    "multiple",
    "products",
    "want",
    "means",
    "multiple",
    "time",
    "series",
    "global",
    "advertising",
    "spend",
    "want",
    "predict",
    "future",
    "value",
    "products",
    "going",
    "represent",
    "table",
    "time",
    "index",
    "product",
    "id",
    "time",
    "series",
    "different",
    "product",
    "id",
    "advertising",
    "spend",
    "across",
    "product",
    "ids",
    "target",
    "variable",
    "sales",
    "want",
    "predict",
    "future",
    "values",
    "products",
    "might",
    "know",
    "advertising",
    "spend",
    "going",
    "future",
    "let",
    "say",
    "first",
    "question",
    "build",
    "target",
    "variable",
    "example",
    "actually",
    "dealing",
    "multiple",
    "time",
    "series",
    "table",
    "time",
    "series",
    "concatenated",
    "way",
    "directly",
    "use",
    "sales",
    "value",
    "target",
    "variable",
    "one",
    "thing",
    "note",
    "target",
    "variable",
    "actually",
    "contains",
    "time",
    "sorry",
    "sales",
    "values",
    "multiple",
    "different",
    "time",
    "series",
    "within",
    "one",
    "target",
    "vector",
    "model",
    "without",
    "without",
    "features",
    "know",
    "one",
    "observation",
    "belongs",
    "time",
    "series",
    "know",
    "two",
    "different",
    "values",
    "belong",
    "two",
    "different",
    "time",
    "series",
    "try",
    "encode",
    "kind",
    "information",
    "features",
    "let",
    "talk",
    "simple",
    "feature",
    "build",
    "one",
    "showed",
    "earlier",
    "lag",
    "features",
    "directly",
    "using",
    "past",
    "values",
    "target",
    "variable",
    "feature",
    "predict",
    "future",
    "values",
    "intuition",
    "recent",
    "values",
    "target",
    "likely",
    "predictive",
    "future",
    "values",
    "likewise",
    "another",
    "trick",
    "something",
    "called",
    "seasonal",
    "lags",
    "let",
    "say",
    "know",
    "time",
    "series",
    "weekly",
    "seasonality",
    "introduce",
    "using",
    "lag",
    "seven",
    "days",
    "example",
    "quick",
    "dirty",
    "easy",
    "way",
    "capturing",
    "weekly",
    "seasonality",
    "show",
    "concrete",
    "example",
    "works",
    "quite",
    "well",
    "later",
    "talk",
    "could",
    "also",
    "use",
    "lagged",
    "version",
    "target",
    "time",
    "series",
    "predict",
    "ones",
    "could",
    "use",
    "previous",
    "values",
    "one",
    "product",
    "predict",
    "future",
    "values",
    "different",
    "product",
    "disadvantage",
    "lots",
    "products",
    "end",
    "creating",
    "lots",
    "additional",
    "features",
    "really",
    "considered",
    "small",
    "number",
    "additional",
    "time",
    "series",
    "know",
    "correlated",
    "easy",
    "way",
    "model",
    "codependency",
    "various",
    "different",
    "target",
    "time",
    "series",
    "lastly",
    "might",
    "also",
    "want",
    "use",
    "lagged",
    "values",
    "exogenous",
    "features",
    "give",
    "concrete",
    "example",
    "one",
    "might",
    "example",
    "advertising",
    "spend",
    "advertising",
    "spend",
    "might",
    "impact",
    "sales",
    "today",
    "also",
    "tomorrow",
    "day",
    "effect",
    "distributed",
    "time",
    "therefore",
    "might",
    "want",
    "use",
    "lagged",
    "versions",
    "advertising",
    "spend",
    "advertising",
    "spend",
    "today",
    "yesterday",
    "day",
    "known",
    "distributed",
    "lags",
    "trying",
    "capture",
    "distributed",
    "effect",
    "time",
    "another",
    "use",
    "case",
    "creating",
    "lag",
    "features",
    "features",
    "look",
    "like",
    "building",
    "kind",
    "table",
    "creating",
    "lag",
    "features",
    "target",
    "variable",
    "looks",
    "similar",
    "showed",
    "using",
    "previous",
    "values",
    "likewise",
    "advertising",
    "spend",
    "lag",
    "one",
    "lag",
    "two",
    "advertising",
    "spend",
    "create",
    "feature",
    "vector",
    "associating",
    "target",
    "value",
    "continue",
    "way",
    "build",
    "uh",
    "feature",
    "matrix",
    "like",
    "might",
    "tempting",
    "looking",
    "might",
    "use",
    "pandas",
    "shift",
    "time",
    "series",
    "create",
    "lagged",
    "features",
    "table",
    "look",
    "like",
    "however",
    "bear",
    "mind",
    "use",
    "shift",
    "might",
    "accidentally",
    "introduce",
    "values",
    "belong",
    "one",
    "time",
    "series",
    "one",
    "product",
    "id",
    "assign",
    "wrong",
    "product",
    "id",
    "edges",
    "ideally",
    "want",
    "group",
    "product",
    "id",
    "apply",
    "transformation",
    "individual",
    "time",
    "series",
    "rather",
    "operating",
    "table",
    "whole",
    "warn",
    "one",
    "way",
    "might",
    "accidentally",
    "screw",
    "feature",
    "engineering",
    "um",
    "question",
    "past",
    "end",
    "building",
    "lots",
    "lag",
    "features",
    "know",
    "flag",
    "features",
    "build",
    "three",
    "main",
    "methods",
    "try",
    "narrow",
    "lag",
    "features",
    "build",
    "uh",
    "using",
    "domain",
    "knowledge",
    "another",
    "thing",
    "use",
    "feature",
    "selection",
    "modeling",
    "techniques",
    "lastly",
    "could",
    "use",
    "called",
    "time",
    "series",
    "correlation",
    "methods",
    "let",
    "give",
    "specific",
    "example",
    "domain",
    "knowledge",
    "quite",
    "helpful",
    "let",
    "say",
    "want",
    "lag",
    "target",
    "variable",
    "well",
    "simple",
    "one",
    "know",
    "seasonality",
    "priori",
    "help",
    "build",
    "seasonal",
    "lag",
    "working",
    "retail",
    "sales",
    "data",
    "set",
    "monthly",
    "granularity",
    "know",
    "yearly",
    "seasonality",
    "christmas",
    "black",
    "friday",
    "events",
    "repeat",
    "year",
    "therefore",
    "using",
    "lag",
    "one",
    "year",
    "would",
    "obviously",
    "predictive",
    "candidate",
    "create",
    "another",
    "example",
    "might",
    "electricity",
    "demand",
    "hourly",
    "electricity",
    "demand",
    "data",
    "set",
    "multiple",
    "seasonalities",
    "daily",
    "weekly",
    "yearly",
    "case",
    "might",
    "build",
    "lag",
    "feature",
    "lag",
    "one",
    "day",
    "one",
    "week",
    "one",
    "year",
    "use",
    "information",
    "know",
    "seasonality",
    "ahead",
    "time",
    "also",
    "might",
    "intuition",
    "many",
    "recent",
    "lags",
    "use",
    "might",
    "know",
    "time",
    "series",
    "highly",
    "correlated",
    "sales",
    "past",
    "one",
    "two",
    "days",
    "say",
    "20",
    "days",
    "past",
    "kind",
    "gives",
    "limit",
    "take",
    "far",
    "using",
    "domain",
    "knowledge",
    "want",
    "use",
    "automatic",
    "method",
    "well",
    "another",
    "approach",
    "using",
    "feature",
    "selection",
    "modeling",
    "techniques",
    "scenario",
    "create",
    "bunch",
    "different",
    "lag",
    "features",
    "target",
    "variable",
    "features",
    "think",
    "might",
    "reasonable",
    "example",
    "using",
    "advertising",
    "spend",
    "highly",
    "unlikely",
    "advertising",
    "spend",
    "year",
    "ago",
    "going",
    "predict",
    "sales",
    "today",
    "probably",
    "want",
    "create",
    "lag",
    "features",
    "year",
    "ago",
    "maybe",
    "choose",
    "several",
    "weeks",
    "create",
    "say",
    "know",
    "14",
    "lag",
    "features",
    "advertising",
    "spend",
    "thing",
    "features",
    "sales",
    "use",
    "standard",
    "feature",
    "selection",
    "modeling",
    "techniques",
    "like",
    "greedy",
    "feature",
    "forward",
    "selection",
    "methods",
    "would",
    "time",
    "consuming",
    "things",
    "could",
    "use",
    "modeling",
    "techniques",
    "like",
    "lasso",
    "build",
    "lasso",
    "model",
    "one",
    "step",
    "ahead",
    "forecast",
    "let",
    "automatically",
    "determine",
    "important",
    "features",
    "also",
    "optimize",
    "forecasting",
    "one",
    "step",
    "ahead",
    "quite",
    "another",
    "easy",
    "way",
    "try",
    "narrow",
    "features",
    "want",
    "use",
    "lastly",
    "called",
    "time",
    "series",
    "correlation",
    "methods",
    "main",
    "idea",
    "want",
    "measure",
    "correlated",
    "lag",
    "features",
    "target",
    "variable",
    "given",
    "lag",
    "feature",
    "correlated",
    "target",
    "variable",
    "might",
    "helpful",
    "three",
    "main",
    "methods",
    "one",
    "called",
    "autocorrelation",
    "function",
    "another",
    "one",
    "called",
    "partial",
    "autocorrelation",
    "function",
    "measure",
    "correlated",
    "time",
    "series",
    "lagged",
    "version",
    "different",
    "lagged",
    "values",
    "retail",
    "sales",
    "data",
    "set",
    "yearly",
    "seasonality",
    "graph",
    "called",
    "autocorrelation",
    "plot",
    "see",
    "correlation",
    "original",
    "time",
    "series",
    "lagged",
    "version",
    "time",
    "series",
    "different",
    "lags",
    "see",
    "little",
    "peaks",
    "occur",
    "lag",
    "12",
    "lag",
    "24",
    "means",
    "time",
    "series",
    "lag",
    "12",
    "much",
    "highly",
    "correlated",
    "original",
    "time",
    "series",
    "lags",
    "use",
    "plots",
    "try",
    "identify",
    "given",
    "lag",
    "potentially",
    "good",
    "candidate",
    "use",
    "forecasting",
    "also",
    "cross",
    "correlation",
    "function",
    "allows",
    "measure",
    "correlated",
    "lag",
    "different",
    "features",
    "target",
    "time",
    "series",
    "measuring",
    "correlation",
    "time",
    "series",
    "lagged",
    "versions",
    "lagged",
    "versions",
    "time",
    "series",
    "end",
    "getting",
    "similar",
    "types",
    "plots",
    "look",
    "say",
    "correlation",
    "high",
    "given",
    "lag",
    "okay",
    "maybe",
    "might",
    "want",
    "use",
    "feature",
    "help",
    "narrow",
    "make",
    "sure",
    "build",
    "many",
    "features",
    "let",
    "move",
    "another",
    "type",
    "feature",
    "call",
    "window",
    "features",
    "generalizes",
    "idea",
    "lag",
    "features",
    "rather",
    "directly",
    "using",
    "past",
    "values",
    "computing",
    "function",
    "summary",
    "statistic",
    "window",
    "past",
    "data",
    "rather",
    "directly",
    "using",
    "past",
    "three",
    "lags",
    "going",
    "look",
    "mean",
    "past",
    "three",
    "values",
    "example",
    "standard",
    "deviation",
    "gives",
    "us",
    "measure",
    "volatility",
    "data",
    "use",
    "predict",
    "future",
    "values",
    "helpful",
    "example",
    "taking",
    "mean",
    "also",
    "smoothing",
    "data",
    "lot",
    "volatility",
    "data",
    "also",
    "helpful",
    "look",
    "like",
    "practice",
    "going",
    "window",
    "size",
    "apply",
    "set",
    "summary",
    "statistics",
    "example",
    "mean",
    "standard",
    "deviation",
    "roll",
    "data",
    "create",
    "feature",
    "vectors",
    "one",
    "thing",
    "note",
    "actually",
    "lagging",
    "rolling",
    "statistic",
    "simply",
    "using",
    "pandas",
    "taking",
    "rolling",
    "operator",
    "applying",
    "dot",
    "mean",
    "also",
    "lag",
    "one",
    "take",
    "lag",
    "one",
    "would",
    "assigned",
    "feature",
    "vector",
    "row",
    "know",
    "see",
    "mouse",
    "way",
    "yeah",
    "would",
    "assigned",
    "means",
    "lagging",
    "leaking",
    "information",
    "using",
    "information",
    "time",
    "prediction",
    "create",
    "features",
    "create",
    "leakage",
    "also",
    "lagging",
    "another",
    "easy",
    "way",
    "might",
    "accidentally",
    "leak",
    "data",
    "future",
    "past",
    "one",
    "way",
    "picking",
    "window",
    "sizes",
    "technique",
    "called",
    "nested",
    "window",
    "features",
    "look",
    "window",
    "sizes",
    "different",
    "orders",
    "magnitude",
    "might",
    "look",
    "window",
    "sizes",
    "orders",
    "days",
    "weeks",
    "months",
    "way",
    "create",
    "feature",
    "captures",
    "information",
    "different",
    "time",
    "scales",
    "let",
    "model",
    "learn",
    "behaviors",
    "occur",
    "different",
    "time",
    "scales",
    "next",
    "let",
    "move",
    "static",
    "features",
    "described",
    "static",
    "features",
    "effectively",
    "represent",
    "metadata",
    "around",
    "time",
    "series",
    "might",
    "something",
    "like",
    "product",
    "id",
    "different",
    "time",
    "series",
    "means",
    "static",
    "feature",
    "different",
    "amongst",
    "two",
    "different",
    "group",
    "time",
    "series",
    "might",
    "also",
    "types",
    "static",
    "data",
    "let",
    "say",
    "metadata",
    "say",
    "product",
    "category",
    "maybe",
    "one",
    "time",
    "series",
    "represents",
    "shorts",
    "another",
    "time",
    "series",
    "represents",
    "watches",
    "whole",
    "group",
    "time",
    "series",
    "might",
    "realize",
    "say",
    "time",
    "series",
    "type",
    "shorts",
    "kind",
    "seasonality",
    "watches",
    "time",
    "series",
    "great",
    "model",
    "able",
    "learn",
    "differences",
    "groups",
    "time",
    "series",
    "likewise",
    "pass",
    "information",
    "time",
    "series",
    "data",
    "target",
    "vector",
    "comes",
    "time",
    "series",
    "finding",
    "ways",
    "encoding",
    "static",
    "features",
    "common",
    "way",
    "dealing",
    "kind",
    "categorical",
    "variables",
    "tabular",
    "data",
    "sets",
    "called",
    "target",
    "encoding",
    "group",
    "categorical",
    "variable",
    "simply",
    "take",
    "average",
    "target",
    "variable",
    "category",
    "training",
    "data",
    "set",
    "take",
    "sku1",
    "take",
    "mean",
    "training",
    "data",
    "pass",
    "value",
    "encoded",
    "value",
    "likewise",
    "sku2",
    "may",
    "look",
    "okay",
    "however",
    "subtle",
    "data",
    "leakage",
    "going",
    "whilst",
    "leakage",
    "training",
    "set",
    "test",
    "set",
    "computed",
    "encoding",
    "training",
    "set",
    "target",
    "leaked",
    "future",
    "values",
    "past",
    "values",
    "training",
    "set",
    "actually",
    "cause",
    "degree",
    "overfitting",
    "feature",
    "going",
    "show",
    "plot",
    "time",
    "series",
    "like",
    "see",
    "means",
    "create",
    "target",
    "encoding",
    "become",
    "obvious",
    "imagine",
    "time",
    "series",
    "split",
    "training",
    "data",
    "test",
    "data",
    "take",
    "target",
    "encoding",
    "taking",
    "mean",
    "training",
    "data",
    "like",
    "let",
    "say",
    "mean",
    "impute",
    "target",
    "product",
    "id",
    "well",
    "means",
    "model",
    "predicting",
    "values",
    "let",
    "say",
    "value",
    "point",
    "time",
    "access",
    "information",
    "future",
    "future",
    "values",
    "used",
    "computing",
    "mean",
    "model",
    "able",
    "determine",
    "oh",
    "well",
    "time",
    "series",
    "going",
    "increase",
    "increasing",
    "trend",
    "still",
    "leakage",
    "future",
    "values",
    "past",
    "values",
    "taking",
    "target",
    "encoding",
    "manner",
    "overcome",
    "well",
    "one",
    "simple",
    "adjustment",
    "make",
    "ensure",
    "use",
    "values",
    "know",
    "point",
    "time",
    "create",
    "uh",
    "target",
    "encoding",
    "time",
    "point",
    "two",
    "use",
    "values",
    "time",
    "step",
    "two",
    "create",
    "encoding",
    "time",
    "step",
    "three",
    "use",
    "values",
    "prior",
    "effectively",
    "expanding",
    "window",
    "going",
    "across",
    "data",
    "set",
    "taking",
    "mean",
    "across",
    "expanding",
    "window",
    "case",
    "data",
    "leakage",
    "within",
    "feature",
    "might",
    "question",
    "compute",
    "encoding",
    "future",
    "values",
    "one",
    "step",
    "ahead",
    "forecasting",
    "problem",
    "still",
    "take",
    "mean",
    "target",
    "uh",
    "points",
    "prior",
    "using",
    "recursive",
    "forecasting",
    "need",
    "recommit",
    "recompute",
    "encoding",
    "predict",
    "time",
    "ensure",
    "way",
    "created",
    "target",
    "um",
    "encoding",
    "consistent",
    "trained",
    "model",
    "case",
    "would",
    "make",
    "one",
    "step",
    "ahead",
    "forecast",
    "append",
    "target",
    "vector",
    "recompute",
    "encoding",
    "forecast",
    "data",
    "way",
    "adjust",
    "way",
    "compute",
    "encoding",
    "likewise",
    "make",
    "adjustment",
    "direct",
    "forecasting",
    "well",
    "way",
    "compute",
    "encoding",
    "depends",
    "method",
    "forecasting",
    "method",
    "use",
    "key",
    "takeaways",
    "far",
    "well",
    "data",
    "leakage",
    "huge",
    "risk",
    "creating",
    "features",
    "target",
    "variable",
    "future",
    "unknown",
    "variables",
    "therefore",
    "use",
    "data",
    "going",
    "know",
    "time",
    "target",
    "variable",
    "also",
    "means",
    "handling",
    "features",
    "predict",
    "time",
    "vary",
    "depending",
    "forecasting",
    "method",
    "use",
    "introduces",
    "lot",
    "code",
    "complexity",
    "implementing",
    "practice",
    "would",
    "helpful",
    "good",
    "open",
    "source",
    "libraries",
    "help",
    "us",
    "going",
    "talk",
    "libraries",
    "interested",
    "feature",
    "engineering",
    "creating",
    "features",
    "data",
    "set",
    "forecasting",
    "cool",
    "libraries",
    "ts",
    "fresh",
    "feature",
    "engine",
    "ts",
    "fresh",
    "computes",
    "whole",
    "host",
    "various",
    "different",
    "time",
    "series",
    "statistics",
    "one",
    "time",
    "series",
    "goes",
    "multiple",
    "statistics",
    "come",
    "interested",
    "whole",
    "forecasting",
    "workflow",
    "using",
    "tabular",
    "data",
    "machine",
    "learning",
    "well",
    "darts",
    "sk",
    "time",
    "help",
    "packages",
    "much",
    "larger",
    "scope",
    "forecasting",
    "machine",
    "learning",
    "also",
    "offer",
    "services",
    "really",
    "really",
    "cool",
    "implement",
    "direct",
    "recursive",
    "forecasting",
    "strategies",
    "also",
    "contain",
    "transformers",
    "help",
    "build",
    "pipelines",
    "advantage",
    "bring",
    "complexity",
    "example",
    "creating",
    "lag",
    "features",
    "dynamically",
    "recomputing",
    "feature",
    "predict",
    "time",
    "takes",
    "care",
    "hood",
    "worry",
    "one",
    "reason",
    "use",
    "open",
    "source",
    "libraries",
    "rather",
    "implementing",
    "logic",
    "want",
    "walk",
    "concrete",
    "example",
    "implemented",
    "darts",
    "reason",
    "would",
    "also",
    "highly",
    "recommend",
    "looking",
    "sk",
    "time",
    "fantastic",
    "libraries",
    "use",
    "darts",
    "imagine",
    "panda",
    "series",
    "index",
    "values",
    "darts",
    "use",
    "wrapper",
    "around",
    "pandas",
    "series",
    "data",
    "frames",
    "import",
    "class",
    "called",
    "time",
    "series",
    "class",
    "lightweight",
    "wrapper",
    "around",
    "data",
    "frame",
    "heavy",
    "lifting",
    "called",
    "regression",
    "model",
    "class",
    "built",
    "darts",
    "class",
    "construction",
    "lag",
    "features",
    "kind",
    "things",
    "hood",
    "use",
    "whatever",
    "regression",
    "model",
    "like",
    "could",
    "use",
    "random",
    "forest",
    "new",
    "regression",
    "whatever",
    "want",
    "sklearn",
    "also",
    "use",
    "gradient",
    "boosted",
    "trees",
    "extreme",
    "boost",
    "light",
    "gbm",
    "import",
    "start",
    "uh",
    "converting",
    "pandas",
    "series",
    "time",
    "series",
    "object",
    "might",
    "want",
    "hold",
    "last",
    "24",
    "data",
    "points",
    "treat",
    "test",
    "data",
    "specify",
    "uh",
    "train",
    "model",
    "regression",
    "model",
    "specify",
    "lag",
    "features",
    "creating",
    "lag",
    "features",
    "separately",
    "passing",
    "model",
    "specifying",
    "part",
    "model",
    "definition",
    "saying",
    "uh",
    "lag",
    "target",
    "1",
    "2",
    "going",
    "see",
    "yearly",
    "seasonality",
    "um",
    "lag",
    "12",
    "quite",
    "helpful",
    "data",
    "monthly",
    "lag",
    "going",
    "pass",
    "linear",
    "regression",
    "model",
    "said",
    "could",
    "also",
    "random",
    "forest",
    "whatever",
    "want",
    "fit",
    "model",
    "predict",
    "24",
    "time",
    "steps",
    "future",
    "mentioned",
    "previously",
    "also",
    "pass",
    "training",
    "set",
    "time",
    "want",
    "predict",
    "forecast",
    "fairly",
    "reasonable",
    "airline",
    "passenger",
    "data",
    "set",
    "linear",
    "regression",
    "built",
    "features",
    "lag",
    "1",
    "2",
    "recursive",
    "forecasting",
    "using",
    "linear",
    "regression",
    "one",
    "time",
    "series",
    "lag",
    "features",
    "uh",
    "external",
    "uh",
    "variables",
    "features",
    "let",
    "say",
    "got",
    "advertising",
    "spend",
    "might",
    "also",
    "want",
    "use",
    "month",
    "year",
    "feature",
    "additional",
    "way",
    "capturing",
    "seasonality",
    "well",
    "create",
    "another",
    "time",
    "series",
    "object",
    "features",
    "let",
    "say",
    "features",
    "ad",
    "spend",
    "month",
    "year",
    "filter",
    "data",
    "frame",
    "create",
    "time",
    "series",
    "object",
    "store",
    "variable",
    "called",
    "futurecon",
    "stands",
    "future",
    "covariates",
    "parlance",
    "dartz",
    "uses",
    "pass",
    "future",
    "covariates",
    "method",
    "one",
    "detail",
    "mention",
    "could",
    "also",
    "lag",
    "future",
    "coverage",
    "want",
    "use",
    "lag",
    "advertising",
    "spend",
    "specify",
    "lags",
    "part",
    "model",
    "definition",
    "well",
    "choosing",
    "like",
    "advertising",
    "spend",
    "hit",
    "pass",
    "future",
    "covariates",
    "well",
    "training",
    "data",
    "also",
    "get",
    "forecast",
    "know",
    "recursive",
    "forecasting",
    "using",
    "linear",
    "regression",
    "single",
    "time",
    "series",
    "uh",
    "lag",
    "features",
    "exogenous",
    "features",
    "let",
    "deal",
    "last",
    "example",
    "much",
    "complex",
    "example",
    "multiple",
    "time",
    "series",
    "exogenous",
    "features",
    "want",
    "lag",
    "features",
    "timestamp",
    "time",
    "series",
    "defined",
    "country",
    "product",
    "id",
    "one",
    "time",
    "series",
    "per",
    "country",
    "product",
    "id",
    "combination",
    "target",
    "variable",
    "features",
    "darts",
    "handy",
    "method",
    "called",
    "group",
    "data",
    "frames",
    "convert",
    "pandas",
    "data",
    "frame",
    "looks",
    "like",
    "set",
    "time",
    "series",
    "objects",
    "specify",
    "columns",
    "define",
    "time",
    "series",
    "time",
    "index",
    "target",
    "variable",
    "returns",
    "list",
    "time",
    "series",
    "darts",
    "handles",
    "multiple",
    "time",
    "series",
    "needs",
    "take",
    "list",
    "time",
    "series",
    "thing",
    "features",
    "list",
    "features",
    "correspondence",
    "time",
    "series",
    "corresponds",
    "features",
    "associated",
    "um",
    "list",
    "simply",
    "pass",
    "future",
    "covariates",
    "method",
    "want",
    "example",
    "predict",
    "subset",
    "time",
    "series",
    "going",
    "say",
    "predict",
    "first",
    "two",
    "time",
    "series",
    "passed",
    "training",
    "set",
    "therefore",
    "going",
    "need",
    "future",
    "covariates",
    "first",
    "two",
    "time",
    "series",
    "one",
    "model",
    "trained",
    "across",
    "multiple",
    "time",
    "series",
    "simultaneously",
    "predict",
    "time",
    "able",
    "produce",
    "multiple",
    "forecasts",
    "time",
    "series",
    "pretty",
    "cool",
    "also",
    "time",
    "series",
    "need",
    "aligned",
    "time",
    "series",
    "could",
    "um",
    "uh",
    "defined",
    "different",
    "time",
    "points",
    "see",
    "time",
    "series",
    "even",
    "exist",
    "period",
    "time",
    "indexes",
    "need",
    "fine",
    "advantages",
    "using",
    "tabular",
    "machine",
    "learning",
    "based",
    "approach",
    "going",
    "conclude",
    "answer",
    "answer",
    "questions",
    "forecasting",
    "treated",
    "tabular",
    "machine",
    "learning",
    "task",
    "competitive",
    "traditional",
    "forecasting",
    "models",
    "feature",
    "engineering",
    "machine",
    "learning",
    "workflow",
    "different",
    "time",
    "series",
    "forecasting",
    "even",
    "though",
    "still",
    "working",
    "tabular",
    "data",
    "forecasting",
    "comes",
    "set",
    "feature",
    "engineering",
    "methods",
    "concerns",
    "around",
    "data",
    "leakage",
    "support",
    "increasingly",
    "becoming",
    "available",
    "time",
    "series",
    "related",
    "tasks",
    "python",
    "ecosystem",
    "would",
    "continue",
    "watch",
    "space",
    "like",
    "learn",
    "topic",
    "check",
    "course",
    "created",
    "solid",
    "garlic",
    "going",
    "launched",
    "october",
    "feature",
    "engineering",
    "forecasting",
    "um",
    "references",
    "materials",
    "used",
    "create",
    "talk",
    "stop",
    "questions",
    "probably",
    "tired",
    "talking",
    "right",
    "100",
    "yeah",
    "question",
    "um",
    "yeah",
    "need",
    "small",
    "break",
    "right",
    "go",
    "go",
    "please",
    "slide",
    "slide",
    "24",
    "talking",
    "going",
    "take",
    "get",
    "back",
    "yeah",
    "talking",
    "uh",
    "like",
    "using",
    "predictions",
    "um",
    "features",
    "call",
    "x",
    "remember",
    "complex",
    "word",
    "exactly",
    "yeah",
    "yeah",
    "remember",
    "econometrics",
    "class",
    "machine",
    "learning",
    "called",
    "teachers",
    "right",
    "yeah",
    "uh",
    "yeah",
    "still",
    "coming",
    "back",
    "sorry",
    "one",
    "second",
    "yeah",
    "okay",
    "gon",
    "na",
    "probably",
    "go",
    "pressed",
    "back",
    "many",
    "times",
    "computer",
    "lagging",
    "sorry",
    "go",
    "okay",
    "cool",
    "still",
    "see",
    "still",
    "see",
    "screen",
    "yes",
    "okay",
    "perfect",
    "let",
    "go",
    "slide",
    "24",
    "okay",
    "yeah",
    "okay",
    "rainfall",
    "feature",
    "right",
    "use",
    "feature",
    "know",
    "value",
    "predict",
    "next",
    "value",
    "right",
    "using",
    "forecasting",
    "tool",
    "trouble",
    "understanding",
    "uh",
    "troubles",
    "understanding",
    "like",
    "trying",
    "figure",
    "okay",
    "happens",
    "train",
    "time",
    "use",
    "actual",
    "value",
    "predict",
    "time",
    "use",
    "prediction",
    "right",
    "would",
    "better",
    "use",
    "prediction",
    "also",
    "forecast",
    "train",
    "time",
    "yeah",
    "absolutely",
    "correct",
    "comes",
    "tricky",
    "topic",
    "question",
    "want",
    "use",
    "lagged",
    "versions",
    "using",
    "predict",
    "time",
    "um",
    "depends",
    "well",
    "trying",
    "capture",
    "degree",
    "causality",
    "well",
    "know",
    "advertising",
    "spend",
    "day",
    "rainfall",
    "day",
    "impact",
    "sales",
    "quite",
    "confident",
    "value",
    "future",
    "um",
    "okay",
    "lacking",
    "lot",
    "confidence",
    "areas",
    "see",
    "example",
    "using",
    "page",
    "views",
    "right",
    "might",
    "page",
    "views",
    "product",
    "want",
    "predict",
    "future",
    "values",
    "course",
    "know",
    "page",
    "views",
    "going",
    "like",
    "four",
    "weeks",
    "time",
    "uh",
    "case",
    "better",
    "using",
    "lag",
    "values",
    "want",
    "build",
    "whole",
    "model",
    "predict",
    "page",
    "views",
    "hand",
    "something",
    "like",
    "weather",
    "might",
    "confidence",
    "might",
    "future",
    "um",
    "therefore",
    "scenario",
    "might",
    "better",
    "also",
    "depends",
    "want",
    "scenario",
    "forecasting",
    "well",
    "trying",
    "answer",
    "question",
    "um",
    "want",
    "know",
    "uh",
    "sales",
    "going",
    "tomorrow",
    "uh",
    "raining",
    "versus",
    "raining",
    "sunny",
    "cloudy",
    "might",
    "actually",
    "plug",
    "different",
    "values",
    "try",
    "assess",
    "different",
    "scenarios",
    "well",
    "case",
    "would",
    "also",
    "want",
    "use",
    "feature",
    "day",
    "rather",
    "necessarily",
    "lagging",
    "yeah",
    "coming",
    "back",
    "wearer",
    "casting",
    "right",
    "remember",
    "trying",
    "think",
    "solve",
    "problem",
    "easy",
    "find",
    "actual",
    "weather",
    "day",
    "internet",
    "difficult",
    "find",
    "prediction",
    "forecast",
    "day",
    "let",
    "say",
    "one",
    "one",
    "day",
    "like",
    "want",
    "use",
    "uh",
    "know",
    "use",
    "prediction",
    "training",
    "model",
    "get",
    "data",
    "yeah",
    "tricky",
    "topic",
    "okay",
    "maybe",
    "share",
    "screen",
    "lot",
    "questions",
    "probably",
    "easier",
    "share",
    "screen",
    "see",
    "okay",
    "way",
    "sorry",
    "spam",
    "attack",
    "uh",
    "stream",
    "googling",
    "like",
    "stop",
    "slow",
    "motion",
    "comments",
    "easier",
    "handle",
    "spam",
    "anyways",
    "first",
    "question",
    "resources",
    "um",
    "learn",
    "time",
    "series",
    "forecasting",
    "traditional",
    "deep",
    "learning",
    "algorithms",
    "one",
    "resource",
    "obviously",
    "course",
    "mentioned",
    "actually",
    "another",
    "question",
    "like",
    "find",
    "course",
    "link",
    "description",
    "put",
    "link",
    "first",
    "link",
    "click",
    "link",
    "course",
    "resources",
    "could",
    "recommend",
    "absolutely",
    "yeah",
    "mean",
    "also",
    "fantastic",
    "review",
    "paper",
    "came",
    "relatively",
    "recently",
    "past",
    "months",
    "basically",
    "covers",
    "um",
    "forecasting",
    "using",
    "machine",
    "learning",
    "forecasting",
    "generally",
    "machine",
    "learnings",
    "also",
    "traditional",
    "statistical",
    "models",
    "chunky",
    "review",
    "paper",
    "several",
    "hundred",
    "pages",
    "long",
    "looking",
    "deep",
    "diving",
    "specific",
    "areas",
    "forecasting",
    "link",
    "um",
    "review",
    "page",
    "think",
    "review",
    "paper",
    "also",
    "actually",
    "uh",
    "references",
    "section",
    "talk",
    "um",
    "add",
    "references",
    "um",
    "know",
    "possible",
    "post",
    "additional",
    "links",
    "uh",
    "back",
    "youtube",
    "channel",
    "finish",
    "streaming",
    "yeah",
    "link",
    "thing",
    "right",
    "uh",
    "show",
    "people",
    "find",
    "first",
    "link",
    "update",
    "repo",
    "put",
    "like",
    "links",
    "yeah",
    "sure",
    "yeah",
    "definitely",
    "update",
    "repo",
    "try",
    "provide",
    "resources",
    "um",
    "deep",
    "learning",
    "time",
    "series",
    "forecasting",
    "specifically",
    "seen",
    "great",
    "introductory",
    "material",
    "typically",
    "need",
    "like",
    "prior",
    "knowledge",
    "deep",
    "learning",
    "rather",
    "taking",
    "beginnings",
    "therefore",
    "kind",
    "paper",
    "oriented",
    "links",
    "found",
    "useful",
    "tutorial",
    "wise",
    "example",
    "um",
    "google",
    "recently",
    "released",
    "today",
    "time",
    "series",
    "forecasting",
    "method",
    "called",
    "temporal",
    "fusion",
    "transformers",
    "think",
    "like",
    "time",
    "past",
    "year",
    "quite",
    "interesting",
    "got",
    "quite",
    "nice",
    "little",
    "tutorial",
    "um",
    "explaining",
    "um",
    "also",
    "help",
    "little",
    "bit",
    "deep",
    "learning",
    "stuff",
    "um",
    "always",
    "getting",
    "forecasting",
    "begin",
    "recommend",
    "rob",
    "heinmann",
    "book",
    "forecasting",
    "got",
    "full",
    "name",
    "book",
    "probably",
    "considered",
    "kind",
    "bible",
    "entry",
    "textbook",
    "uh",
    "forecasting",
    "um",
    "go",
    "forecasting",
    "principles",
    "practice",
    "yeah",
    "yeah",
    "free",
    "online",
    "uh",
    "yes",
    "r",
    "like",
    "r",
    "actually",
    "nice",
    "ecosystem",
    "uh",
    "like",
    "tools",
    "libraries",
    "long",
    "python",
    "anything",
    "similar",
    "think",
    "still",
    "like",
    "steps",
    "ahead",
    "definitely",
    "agree",
    "make",
    "contributions",
    "stats",
    "models",
    "new",
    "stuff",
    "coming",
    "r",
    "exist",
    "stats",
    "models",
    "implement",
    "want",
    "python",
    "okay",
    "thanks",
    "um",
    "forecast",
    "month",
    "retain",
    "every",
    "month",
    "run",
    "forecast",
    "good",
    "question",
    "depends",
    "much",
    "expect",
    "distribution",
    "data",
    "change",
    "month",
    "month",
    "monthly",
    "would",
    "still",
    "encourage",
    "um",
    "refresh",
    "model",
    "picks",
    "recent",
    "changes",
    "training",
    "distribution",
    "um",
    "think",
    "computationally",
    "going",
    "demanding",
    "downsides",
    "least",
    "check",
    "model",
    "stability",
    "model",
    "coefficients",
    "jump",
    "around",
    "significantly",
    "every",
    "month",
    "ending",
    "different",
    "models",
    "want",
    "examine",
    "features",
    "moving",
    "around",
    "want",
    "end",
    "um",
    "using",
    "different",
    "forecasting",
    "model",
    "effectively",
    "every",
    "month",
    "retraining",
    "um",
    "way",
    "kind",
    "sense",
    "checking",
    "whilst",
    "machine",
    "learning",
    "model",
    "production",
    "okay",
    "yeah",
    "run",
    "every",
    "day",
    "maybe",
    "cumbersome",
    "retrain",
    "right",
    "maybe",
    "retrain",
    "like",
    "every",
    "month",
    "absolutely",
    "also",
    "forecasting",
    "let",
    "say",
    "one",
    "month",
    "ahead",
    "um",
    "data",
    "may",
    "changed",
    "significantly",
    "sure",
    "additional",
    "forecast",
    "actually",
    "going",
    "beneficial",
    "actually",
    "question",
    "talking",
    "think",
    "midway",
    "talk",
    "music",
    "even",
    "half",
    "talk",
    "said",
    "many",
    "times",
    "careful",
    "introduce",
    "data",
    "leaks",
    "like",
    "accidentally",
    "use",
    "data",
    "future",
    "question",
    "like",
    "guard",
    "guess",
    "partly",
    "answered",
    "use",
    "library",
    "know",
    "guess",
    "yeah",
    "yeah",
    "mean",
    "guess",
    "practical",
    "perspective",
    "um",
    "libraries",
    "kind",
    "handle",
    "okay",
    "data",
    "leakage",
    "appear",
    "um",
    "create",
    "new",
    "features",
    "typically",
    "kind",
    "lagging",
    "rolling",
    "window",
    "using",
    "past",
    "values",
    "right",
    "um",
    "methods",
    "example",
    "feature",
    "engine",
    "uh",
    "sk",
    "time",
    "uh",
    "darts",
    "know",
    "kind",
    "behavior",
    "covered",
    "transformers",
    "provided",
    "model",
    "objects",
    "hopefully",
    "help",
    "guard",
    "guard",
    "extent",
    "um",
    "cases",
    "also",
    "crossband",
    "example",
    "going",
    "mention",
    "well",
    "cross",
    "validation",
    "rebuild",
    "training",
    "set",
    "um",
    "moving",
    "time",
    "step",
    "forward",
    "cross",
    "foundation",
    "right",
    "another",
    "area",
    "come",
    "play",
    "libraries",
    "also",
    "help",
    "cross",
    "validation",
    "guess",
    "say",
    "easiest",
    "way",
    "use",
    "libraries",
    "um",
    "going",
    "build",
    "package",
    "advice",
    "use",
    "lot",
    "unit",
    "testing",
    "building",
    "build",
    "unit",
    "tests",
    "transformations",
    "building",
    "going",
    "create",
    "kind",
    "feature",
    "uses",
    "like",
    "values",
    "lagging",
    "um",
    "think",
    "another",
    "comment",
    "well",
    "um",
    "sorry",
    "enough",
    "coffee",
    "today",
    "comes",
    "back",
    "come",
    "back",
    "topic",
    "okay",
    "meantime",
    "think",
    "like",
    "question",
    "oh",
    "yes",
    "okay",
    "think",
    "okay",
    "came",
    "back",
    "yes",
    "yeah",
    "absolutely",
    "one",
    "um",
    "notice",
    "model",
    "unreasonably",
    "good",
    "notice",
    "um",
    "forecasts",
    "accurate",
    "chances",
    "suddenly",
    "built",
    "crystal",
    "ball",
    "somehow",
    "leaked",
    "data",
    "future",
    "past",
    "one",
    "reason",
    "forecast",
    "looks",
    "good",
    "another",
    "sign",
    "might",
    "actually",
    "leaked",
    "data",
    "future",
    "past",
    "another",
    "sanity",
    "check",
    "expect",
    "forecasting",
    "models",
    "look",
    "good",
    "yeah",
    "thanks",
    "accidentally",
    "marked",
    "one",
    "interesting",
    "question",
    "answered",
    "clicked",
    "question",
    "would",
    "deal",
    "effects",
    "events",
    "like",
    "covet",
    "financial",
    "crisis",
    "behavior",
    "time",
    "series",
    "going",
    "forward",
    "might",
    "difficult",
    "think",
    "interesting",
    "question",
    "think",
    "something",
    "everybody",
    "facing",
    "machine",
    "learning",
    "train",
    "past",
    "data",
    "distribution",
    "data",
    "changed",
    "significantly",
    "um",
    "simple",
    "things",
    "could",
    "try",
    "time",
    "series",
    "forecasting",
    "would",
    "hope",
    "important",
    "data",
    "recent",
    "data",
    "one",
    "thing",
    "potentially",
    "truncate",
    "training",
    "data",
    "train",
    "time",
    "periods",
    "come",
    "complex",
    "situation",
    "covered",
    "covet",
    "kind",
    "behaviors",
    "code",
    "probably",
    "prominent",
    "period",
    "would",
    "forecasting",
    "example",
    "covid",
    "still",
    "impact",
    "challenging",
    "handle",
    "things",
    "build",
    "two",
    "forecasting",
    "models",
    "one",
    "covered",
    "data",
    "one",
    "without",
    "kind",
    "take",
    "average",
    "two",
    "pragmatic",
    "caveat",
    "outputs",
    "provide",
    "stakeholders",
    "ways",
    "handle",
    "potentially",
    "introducing",
    "new",
    "feature",
    "um",
    "giving",
    "weight",
    "recent",
    "time",
    "series",
    "recent",
    "data",
    "mentioned",
    "earlier",
    "could",
    "use",
    "sample",
    "weights",
    "option",
    "one",
    "thing",
    "give",
    "weight",
    "recent",
    "time",
    "periods",
    "um",
    "also",
    "would",
    "adverse",
    "adding",
    "additional",
    "business",
    "logic",
    "top",
    "machine",
    "learning",
    "output",
    "like",
    "review",
    "forecast",
    "make",
    "sense",
    "business",
    "growing",
    "fast",
    "past",
    "sure",
    "forecast",
    "says",
    "going",
    "grow",
    "200",
    "ever",
    "grown",
    "30",
    "year",
    "year",
    "maybe",
    "forecast",
    "realistic",
    "might",
    "need",
    "artificially",
    "constrain",
    "forecast",
    "somehow",
    "right",
    "um",
    "yeah",
    "practical",
    "advice",
    "give",
    "question",
    "yeah",
    "one",
    "think",
    "answered",
    "course",
    "available",
    "october",
    "right",
    "yeah",
    "october",
    "hoping",
    "material",
    "hopefully",
    "12",
    "hours",
    "material",
    "um",
    "quite",
    "wide",
    "scape",
    "want",
    "continue",
    "adding",
    "course",
    "even",
    "12",
    "hours",
    "materials",
    "still",
    "adding",
    "october",
    "takes",
    "quite",
    "long",
    "way",
    "even",
    "going",
    "continue",
    "adding",
    "materials",
    "um",
    "monthly",
    "basis",
    "um",
    "already",
    "subscribe",
    "notified",
    "goes",
    "live",
    "right",
    "yes",
    "email",
    "box",
    "yeah",
    "exactly",
    "since",
    "live",
    "get",
    "access",
    "question",
    "video",
    "guido",
    "problem",
    "like",
    "example",
    "predict",
    "multiple",
    "series",
    "need",
    "predict",
    "one",
    "step",
    "lead",
    "time",
    "30",
    "would",
    "hit",
    "predict",
    "lead",
    "time",
    "like",
    "maybe",
    "sure",
    "lead",
    "time",
    "maybe",
    "answer",
    "um",
    "trying",
    "interpret",
    "question",
    "uh",
    "properly",
    "answer",
    "lead",
    "times",
    "30",
    "days",
    "assuming",
    "means",
    "trying",
    "predict",
    "30",
    "days",
    "future",
    "maybe",
    "delay",
    "delay",
    "um",
    "mean",
    "advice",
    "delay",
    "mean",
    "easiest",
    "things",
    "lag",
    "features",
    "right",
    "know",
    "basic",
    "premise",
    "information",
    "predict",
    "time",
    "need",
    "predict",
    "um",
    "let",
    "say",
    "equals",
    "31",
    "beyond",
    "care",
    "time",
    "equals",
    "zero",
    "um",
    "lag",
    "lot",
    "features",
    "um",
    "use",
    "um",
    "lag",
    "feature",
    "let",
    "say",
    "predicting",
    "whatever",
    "30",
    "days",
    "beyond",
    "seen",
    "practice",
    "well",
    "let",
    "say",
    "recursive",
    "forecasting",
    "let",
    "say",
    "interested",
    "predicting",
    "time",
    "equals",
    "30",
    "60",
    "one",
    "month",
    "future",
    "could",
    "still",
    "use",
    "recursive",
    "forecasting",
    "time",
    "equal",
    "1",
    "60",
    "discard",
    "first",
    "30",
    "first",
    "30",
    "predictions",
    "use",
    "remaining",
    "30",
    "use",
    "direct",
    "forecasting",
    "directly",
    "build",
    "target",
    "day",
    "31",
    "future",
    "long",
    "keeping",
    "principle",
    "using",
    "information",
    "going",
    "time",
    "equals",
    "zero",
    "using",
    "information",
    "zero",
    "30",
    "ensure",
    "avoid",
    "leakage",
    "yeah",
    "afraid",
    "running",
    "time",
    "quite",
    "questions",
    "maybe",
    "propose",
    "know",
    "uh",
    "uh",
    "maybe",
    "move",
    "questions",
    "slack",
    "take",
    "offline",
    "would",
    "work",
    "yeah",
    "happy",
    "okay",
    "couple",
    "hours",
    "uh",
    "put",
    "questions",
    "slack",
    "special",
    "channel",
    "think",
    "called",
    "events",
    "q",
    "something",
    "like",
    "make",
    "announcement",
    "announcements",
    "channel",
    "able",
    "see",
    "put",
    "questions",
    "take",
    "offline",
    "great",
    "okay",
    "yeah",
    "amazing",
    "presentation",
    "packed",
    "lot",
    "knowledge",
    "sure",
    "many",
    "people",
    "watch",
    "use",
    "certainly",
    "um",
    "yeah",
    "thanks",
    "lot",
    "thanks",
    "joining",
    "us",
    "today",
    "thanks",
    "sharing",
    "knowledge",
    "thanks",
    "everyone",
    "joining",
    "us",
    "today",
    "well",
    "asking",
    "questions",
    "active",
    "apologize",
    "could",
    "cover",
    "questions",
    "try",
    "answer",
    "anyways",
    "okay",
    "thank",
    "yeah",
    "yeah",
    "see",
    "soon"
  ],
  "keywords": [
    "thanks",
    "us",
    "today",
    "data",
    "people",
    "weekly",
    "events",
    "one",
    "want",
    "find",
    "link",
    "description",
    "go",
    "check",
    "quite",
    "see",
    "something",
    "interesting",
    "whatever",
    "time",
    "get",
    "future",
    "course",
    "machine",
    "learning",
    "engineering",
    "also",
    "right",
    "learn",
    "yeah",
    "like",
    "question",
    "questions",
    "way",
    "end",
    "um",
    "okay",
    "live",
    "chat",
    "use",
    "think",
    "probably",
    "talking",
    "stuff",
    "start",
    "great",
    "uh",
    "actually",
    "make",
    "sure",
    "maybe",
    "cool",
    "let",
    "know",
    "give",
    "talk",
    "first",
    "feature",
    "series",
    "forecasting",
    "work",
    "called",
    "building",
    "models",
    "open",
    "recent",
    "would",
    "additional",
    "methods",
    "topic",
    "links",
    "able",
    "yes",
    "might",
    "sorry",
    "later",
    "come",
    "back",
    "predict",
    "value",
    "set",
    "tabular",
    "regression",
    "table",
    "features",
    "target",
    "variable",
    "new",
    "random",
    "two",
    "main",
    "using",
    "traditional",
    "build",
    "good",
    "example",
    "sales",
    "problem",
    "correlated",
    "forecast",
    "30",
    "product",
    "therefore",
    "products",
    "given",
    "could",
    "belong",
    "different",
    "points",
    "looking",
    "seasonality",
    "imagine",
    "exogenous",
    "variables",
    "multiple",
    "type",
    "top",
    "better",
    "used",
    "smoothing",
    "lot",
    "across",
    "gives",
    "specify",
    "past",
    "function",
    "simple",
    "model",
    "means",
    "still",
    "even",
    "continue",
    "kind",
    "dealing",
    "call",
    "easy",
    "little",
    "case",
    "much",
    "complex",
    "method",
    "going",
    "information",
    "create",
    "directly",
    "vectors",
    "leakage",
    "show",
    "vector",
    "previous",
    "values",
    "thing",
    "lag",
    "three",
    "another",
    "class",
    "advertising",
    "spend",
    "rainfall",
    "well",
    "things",
    "rather",
    "look",
    "lastly",
    "static",
    "try",
    "group",
    "training",
    "around",
    "take",
    "standard",
    "step",
    "practice",
    "steps",
    "direct",
    "recursive",
    "effectively",
    "built",
    "many",
    "say",
    "creating",
    "point",
    "ahead",
    "likewise",
    "forward",
    "pass",
    "train",
    "days",
    "append",
    "next",
    "prediction",
    "long",
    "libraries",
    "test",
    "split",
    "need",
    "cross",
    "prior",
    "move",
    "window",
    "tasks",
    "comes",
    "put",
    "specific",
    "day",
    "month",
    "whole",
    "answer",
    "knowledge",
    "id",
    "lags",
    "lagged",
    "trying",
    "pandas",
    "accidentally",
    "correlation",
    "helpful",
    "help",
    "year",
    "12",
    "24",
    "mean",
    "taking",
    "lagging",
    "encoding",
    "darts",
    "covariates",
    "page",
    "deep",
    "review",
    "every"
  ]
}