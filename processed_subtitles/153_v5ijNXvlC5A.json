{
  "text": "okay so good morning everyone and thanks\nfor coming to my tutorial on modern time\nseries analysis this is sort of my\ninterpretation of modern time series\nanalysis I suppose everyone would have a\ndifferent interpretation but to me\nmainly what that means is we're not\ngoing to cover auto regressive models\nwe're going to talk about them briefly\nand then we're gonna mainly talk about\nmethods that are more computationally\ntaxing some of them were invented many\ndecades ago but I would still classify\nthem as modern given that they've been\nfar more successful sort of with the era\nof big data and far more successful with\nsort of readily available computing\nresources but that doesn't necessarily\nmean the ideas are very new but I mean\nthat's also true for even neural\nnetworks right the ideas are also not\nnew a lot of the modernity comes from\nthe computing resources so that's how\nwe'll be thinking about this a little\nbit about me and my background in this\nsubject so a few years ago I actually\ngave another tutorial\non time series analysis at Syfy and that\none covered more exploratory data\nanalysis data processing and also more\ntraditional methods like ARIMA models\nauto regressive models that sort of\nthing so if you're interested in that\nand you don't have a background that is\none resource of many resources that's\njust the one that I've prepared you\ndon't need to be familiar with that\nmaterial for today but it is definitely\nmaterial you should be familiar with and\nthat should really be your bread and\nbutter if you are working with\ntime-series professionally even though\nthose men tear up those methods are sort\nof low-tech or less modern they're very\nmuch relevant today they still perform\nexceptionally well and they also give\nyou a really good conceptual foundation\nand similarly it's good to be\ncomfortable with data cleaning of time\nseries and all of that so that's a\nlittle bit about me I'm also just a\nlittle bit curious about people in this\nroom so I guess I'll just do a quick\nsurvey show of hands if you already work\nwith time series data at work okay I\nhope there will still be stuff for you\nif not no hard feelings and you can\ncertainly pipe in with your own\nsuggestions and input as well\nand how many people have never worked\nwith time series data okay or very\nlimited how about very limited okay fair\nenough and how many people work in a\nscientific sector broadly defined okay\nand how many people don't work in\nsomething you would call remotely\nscientific okay\nfair enough so good for me to know and\nthen just out of curiosity who thinks\nthey came the farthest to get here can\nanybody beat Singapore that's really\ntough yeah that's okay there was someone\non my site who claimed they'd been\ntraveling for two days but they didn't\ncome to this conference they might have\nyou beat though as far as like time or\nwhatever okay okay so here's an outline\nof what we're gonna be talking about\njust a very brief overview of sort of\nwhat makes time series data special some\nthings you want to keep in your head\nwhen you're looking at time series data\nespecially if you're new to it or\nhaven't worked with it for a while and\nthen we're gonna divide our discussion\ninto three components we're going to be\nlooking at state state space models for\ntime series just an overview of what\nthose are and we're going to cover two\nmethods specific to state state state\nspace methods we're going to talk a bit\nabout applying traditional missional\nmachine learning methods specifically\nfor time series and what approaches you\ncan take there and what you want to look\nout for and we're gonna wrap up looking\nat some deep learning for time series\nokay so time series generally what our\ntime series it seems like everybody has\na good idea of what those are given that\nyou work with them right so the classic\nexample I think in every time series\nbook almost always you will run into\nmany many examples of something like\nstock prices there are readily available\nusually free fairly chaotic time series\nso they're really interesting to look at\nand of course nobody is all that good at\nmodeling them because if they were\nthey'd be filthy rich so it remains sort\nof an open problem which is also\ninteresting but I want to point out that\nmany other things are time series that\ndon't get called time series and even\nget analyzed in their respective\ndisciplines without anyone using that\nword even though that's what they are so\non the upper right hand\nof this slide I have an EKG diagram I've\nnever heard a doctor talk about time\nseries data but actually that's often\nwhat they're looking at right they are\ntaking some sort of measurement over\ntime when you're hooked up to any device\nin the hospital it has some kind of\ndigital or analog curve that's a time\nseries right so I think data science or\ntime series analysis or whatever you\nwant to call it has a lot to contribute\nto this field and while we keep hearing\nabout the benefits of healthcare for\ntrying it for for data science or deep\nlearning we don't hear about it so much\nin the time series context even though\nthere's actually a lot of time series\ndata in the health field as well as many\nothers similarly on the bottom I think\nthis is an NMR but I might be wrong if a\nchemist wants to correct me and tell me\nthis is some other kind of spectroscopy\nbut this is some kind of NMR looking at\nmolecular structure right this is also a\ntime series by my definition even though\nhere the x axis is not time it's\nactually just wavelength because you can\nuse many of the same methods on it and\nwhat you have is you have a well-ordered\nwell spaced temporal axis so we are even\ngoing to be working with some data today\nwhere your temporal axis is not time per\nse but it's something with this sort of\nwell ordered metric between different\npoints in time right so that's what\nmakes time series different from\nanything that's cross-sectional analysis\nand here's an example so what might I do\nwith something like that right if I\ndon't have time as an axis am I really\ngonna do prediction well probably not\nright prediction might not be the most\nuseful thing although maybe you can\ncontrive an example but you can\ncertainly do something like time series\nclustering right you might have multiple\nNMR spectra and you want to find a way\nto classify them without having hundreds\nof grad students leaving over it right\nif you can find a way to replace those\ngrad students even better okay so tasks\nfor time series analysis what are things\npeople do well as usual visualization\nand exploratory data analysis when you\ndo this for time series often you have\nspecific time related temporal questions\nsuch as understanding the temporal\nbehavior of the data\nespecially if your time axis is actually\na time axis such as seasonality right is\nthere some kind of recurrence in your\ndata some kind of seasonality you can\nalso look for cyclical data not quite\nthis time the same as seasonal data\nright so cyclical data will have some\nsort of recurrence but it can be damped\nand it can also sort of change its\nperiod over time so an example of\nseasonal would be something like the\nweather and an example of an example of\ncyclical would be something like people\nhypothesize there's a business cycle\nright some sort of boom-and-bust cycle\nand banking or stock prices that sort of\nthing and what's another thing we\nusually want to do is sort of identify\nunderlying distributions and the nature\nof the temporal processes producing the\ndata right we want to get a sense of I\nhave a hypothesis that I see this sort\nof river level over time and I want to\nget at sort of what describes that river\nlevel more fundamentally that's another\nthing I might want to do not just\npredict but also have a way of\ndescribing the dynamics what else do we\nneed to do well we can do estimation of\npast present and future values right so\nthere's also this distinction between\nsay something like filtering and\nforecasting or also smoothing we'll talk\nabout all of that in the context of\nstate space but the idea is if you have\nnoisy measurements and you can think of\nall sorts of things as a noisy\nmeasurement such as stock prices where\nmaybe you think the market got it wrong\nand there's a real price that's separate\nfrom what the market says or it could be\na medical sensor that has some sort of\nknown plus or minus so let's say even if\nyou measure your blood sugar within a\nminute on the same device you will still\nhave you know 5% discrepancy and that's\nsomething that's actually accepted by\nthe FDA right now for devices used by\npeople with diabetes so there's an\nexample of you know you can't get it\nperfect and so you could have a time\nseries with just lots of measurement\nerror so you can have tasks of figuring\nout well what was the true value at all\nthese time steps right there was a value\nI measured versus the value that is\nactually true and is there a way of\ngetting at that\nnext up classification that's another\ncommon time series task this could be\nboth in something like the medical field\nright can I\nto find what kind of EKG I'm looking at\nis is showing me normal heart or some\nkind of arrhythmia this could be\nclassification of NMR spectra like we\ntalked about this could be\nclassification of retail behavior do we\nhave sort of distinctive trajectories\nthrough our website right things like\nthat can also be time series analysis\nand then another major task is anomaly\ndetection right can we figure out which\npoints are the problematic points right\nand the better we can do that the less\noften say a bank will have to cancel\ncredit cards or a doctor will have to\nrun tests that weren't really necessary\nversus run the ones that we really need\nwe don't always think of these things as\ntime series analysis but increasingly we\nhave panel data for all these things\nright increasingly for things we think\nof as cross-sectional data right surveys\nor whatever we're better able to track\npeople and so we have a sense of how\nthings are evolving over time okay\ntime series data versus cross-sectional\ndata what are some of the challenges or\nthings to keep in mind well number one\nthere's more opportunities for missing\ndata points it can be quite challenging\nfollowing the same people or even\nsamples through time and I imagine\nanyone who has worked especially in sort\nof human related Sciences knows about\nthis problem right good luck finding the\nsame survey respondents year in and year\nout as sort of an epic task even for\nreally simple scientific data that can\nbe a problem for example when I was\nputting together the data for this\ntutorial I was trying to use some\nColorado River data from the federal\ngovernment and I kept getting bombarded\nwith this sort of bright red text\nindicating what measurements they had\ndiscontinued due to funding cuts right\nand so time series especially anything\nthat looks at government data sets is\neven affected by politics to some extent\nwhich is kind of funny right you'll just\nrun into it more than you might if you\nwere just doing panel data second major\npoint there's a high degree of\ncorrelation between data points values\nin the past almost always and we hope\npredict values in the future right so\nyou have to think about what your errors\nlook like you have to think about what\nyour relation to points in time looks\nlike as opposed to correlations where\ncross-sectional data where you tend to\nthrow things in and these are different\npeople and hopefully to first order\nthey're not super correlated with one\nanother so this is good for prediction\nbut it can be bad for models that assume\nindependent\ninputs right so for example when we are\nlooking at some machine learning\ntechniques or when we're thinking about\nhow can I apply sort of non time-series\nstatistics to time series analysis we\nmight very readily violate the\nassumptions whereas if we're sort of\ncareless in general about running linear\nregression that's not so likely to be\ntrue it's very likely to be true in time\nseries analysis and then there's also\njust the the hassle of dealing with\ntimestamps or other measures of whatever\nyour distance metric is along your\ntemporal access right you have things\nlike time zones frequency irregularities\nand so on even if you only work with\nsort of scientific data where things\nlike the exact timestamp like day of the\nweek doesn't matter because as far as I\nknow that doesn't affect physics you\nmight somehow find that for example your\ntiming measurement is a little bit off\nright you have some sort of bias problem\nso it doesn't go away even in that case\nokay\nother characteristics data is collected\nsequentially one axis is monotonically\nincreasing right so as I mentioned\nthat's my definition of a time series\nit's got to have a meaningful axis with\nmeasurable distance it doesn't have to\nbe time we do want to characterize\nstructure across data points right so\nagain seasonality in cycles\nautocorrelation and trends who is\nfamiliar with the concept of\nautocorrelation I just want to make sure\nthat's fairly familiar so just a quick\noverview for those who aren't\nautocorrelation is just how a point at\ntime T is correlated say with at the\ntime T minus one for the entire time\nseries right so at time T how do I\nrelate like this value how does it\nalways relate to the value before it\nright in a probabilistic sense that's\nautocorrelation and then finally\nstochastic behavior even as the\nbehavioral regime right so in\ncross-sectional data we're used to\nthinking about oh if I see sort of\nqualitatively different behavior I might\nbe identifying sort of different\nsubgroups in my population right maybe\nI'm looking at two molecules or sort of\ntwo types of consumer or whatever but in\ntime series you can just have within the\nsame series some kind of stochastic\nshift to a different regime and you need\nto think about how to identify that that\nsort of change point identification okay\nhere's a slide on autocorrelation and in\ncase anyone's not familiar\nwith it even if you are I guess the main\npoint I want to make is that you don't\nalways know from eyeballing your data\nwhat it's going to look like right so we\nmight think oh I'm just sort of looking\nat this top panel I have a sense of how\ncorrelated these things are but I\npersonally when I then look at the lower\npanels this series is much more sort of\nself correlated and correlated than I\nwould have thought from just looking at\nit right it looks kind of noisy and\nmaybe there's some kind of seasonality\nbut not to the extent then that I see\nwith the ACS so that could be good to\nkeep in mind to your data is never clean\nenough or never sort of noisy enough or\nwhite noise like enough or or beautiful\nenough that you should not run basic\nDiagnostics ok special concerns for time\nseries data so correlated errors right\nand on the right-hand side on the upper\nright-hand side this is actually the\nerrors or the residuals of a time series\nmodel so at this point I say already\nwrote a model that I thought I counted\nfor the seasonality and the self\ncorrelation but what we can see is on\nthese blue lines give us the\nsignificance levels you can see that\nthis model actually is failing to\ndescribe all the autocorrelation of the\ntime series right so this is something\nyou need to keep in mind for time series\nthere's special Diagnostics you need to\ndo that our time aware in addition to\nhaving time aware modeling we need to\nhave time aware Diagnostics\ncross-validation also will usually look\ndifferent in a time series context right\nso there I'll draw your attention to\nthis plot in the lower right hand corner\nfor cross-validation it depends on what\nyou are modeling and the nature of your\ndata but often if you are trying to\npredict the future you want to make sure\nthat future information does not leak\nbackwards in time influence your model\nand make your model look a lot better\nthan it does that sounds like common\nsense right we're all professionals we\nwould never ever do that right and yet\nmany people myself included will find\nthemselves very embarrassed when\nsomething does much better in training\nand you think you properly cross\nvalidated it and you roll it out to\nproduction which is really the ground\ntruth and it doesn't and so you can\nreally never ever ever be careful enough\nabout the cross validation and the next\nbullet\nwe'll look at so one thing you can do as\nis illustrated in this lower right hand\ncorner is you sort of roll your training\ndata and your testing data forward in\ntime and the idea is is you never want\nto test on data that is in any way older\nthan what you trained on because you\ndon't want information propagating\nbackwards that's another thing about\ntime series and temporal axes most of\nthe time right the information should\nreally only propagate in one direction\nyou can write models that are the\nopposite right you can have causal time\nseries and you can have the opposite\nwhere information goes backwards in time\nthat doesn't tend to be interesting for\nmodeling though right that doesn't tend\nto be the questions we're trying to\nanswer at work or in research um few\nother notes on look ahead right so this\nis just for cross validation just\nbecause you've covered this does not in\nany way mean you've avoided look ahead\njust because you've properly crossed\nvalidated there are all sorts of other\nthings to keep in mind time stamping\nright just because you have data\navailable time stamped at a particular\ntime as an input into your model doesn't\nmean that that time stamp reflects say\nwhen you would actually have that data\nright so you might have for example\nlet's say you're trying to predict the\njobs numbers and you might have some\nsort of survey of how people are feeling\nand you have the date of the survey when\nit was taken but you forget that maybe\nit doesn't get input into the system\navailable to you for a week\nand that's the sort of thing that can\nhappen when you move from research to\nproduction and suddenly you realize that\nyour inputs that you thought were\navailable actually are it right and\nmaybe it took place in the past but it\nhasn't been registered yet so there's\nanother example of look ahead there are\nmany more and as far as I know there's\nnot sort of an automated way to diagnose\nlook ahead you just have to really keep\nyour eyes open and it depends a lot on\nwhat your field is and sometimes it's\nnot even an issue depending again on on\nthe field but especially tricky with\nhuman data\nokay so just brief overview any\nquestions or comments or anything people\nwith experience would want to throw in\non top of that I know it's like 8:30 in\nthe morning oh yes somebody does\nonly in the statistical sense so I have\nno frequency domain examples here\nalthough I can speak about it briefly\nwhere we get to deep learning because\ncertainly there's a lot done there and\nthat's a good thing to highlight right\nso there's a whole sort of other domain\nof time-series analysis that looks at\ntime series in the frequency domain very\nsuccessful especially in certain fields\nand actually for those who are\ninterested in a brief tutorial I have\nthat in my first tutorial posted so\nthat's good pointer is yes no I won't be\ndiscussing data imputation although all\nof the methods we use can be applied to\ndata imputation so I'll make sure to\ncomment on that as we go but there are\nno examples and that's that's another\ngiven that what I highlighted about the\nmissing data this can often be important\nokay okay so let's talk about\nstate-space models we'll start with some\nbackground on Box Jenkins ARIMA modeling\nso ARIMA modeling if you haven't used it\nyou've probably at least seen this term\nARIMA or maybe you've seen ARMA or AR or\nma you see these are all sort of the\nsame class of models statistical models\ndeveloped a very long time ago at this\npoint sort of early early to mid 20th\ncentury as you can see though they can\nhave despite you know sort of being old\ntraditional they have excellent\nperformance especially on small data\nsets so here I have the example of the\nairline passenger data set very famous\ndata set if you look at time series\ntextbooks this is always there you can\nsee so the forecast versus the observed\nare pretty close right this is already\npretty good and so one of the\ninteresting things about modern time\nseries analysis right are sort of\nnon-traditional methods however you want\nto call them is you always want to make\nsure that you are actually contributing\nsomething that you couldn't get say with\nan ARIMA\nand that is surprisingly difficult not\njust that it's also surprisingly\ndifficult sometimes for an arena model\nto those to beat an even simpler model\nwhich is just sort of your know model\nwhich could be something like I will\npredict for time t plus 1 the value I\nhave at T and just leave it at that and\nthat itself can be an extremely good\nmodel so I just want to point out that\nwe are starting with a fairly high bar\nespecially in certain domains right if\nyou have certain kinds of data sets that\ndon't have too much noise or they have\nsay a strong seasonal component you can\ndo very well with this sort of model so\nwhat does an ARIMA model look like well\nI just want to show you sort of one way\nof thinking about it if L is your lag\noperator a lag operator means if I apply\nL to X sub T X at time T I get X at t\nminus 1 if I apply l squared to X sub T\nI get X sub t minus 2 right so it's just\na way of expressing that we're moving\nsomething back in time in this case now\nmy alpha is my alpha sub I on the\nleft-hand side those are my auto\nregressive coefficients and my theta is\non my right hand side are my moving\naverage components so this is sort of\nthe traditional way to express an ARIMA\nmodel it's some sort of or this is an\nARMA model rather it's some sort of on\nthe left way of operating on the data\nitself so this is going to have\ncoefficients that apply to pass values\nand then on the right you're going to\nhave coefficients that apply to sort of\npast errors right and those could be\nnoise and the system stochastic noise\nthey could be measurement noise you can\ninterpret it in a number of ways but the\nidea is that your future value is in\nsome way going to depend on your past\nvalues and your past errors and what\nsort of values you choose for P and Q\nyour summation are part of how you do\nyour model tuning now if you go to an\nARIMA model now you have all of that\nplus a difference so all that means is\nyou have this additional 1 minus L to\nthe D which means you're going to\ndifference your time series so that\nmeans instead of drawing my original\ntime series like my air passengers with\nthe absolute number at each time stamp\nI'm going to convert my time series for\na first difference to the Delta so the\ntime series becomes a series of delta T\nDelta X's instead of X's right so you\nhave the exact same in\nMeishan you haven't lost any information\nexcept maybe your offset and what that\ndoes is it often converts your time\nseries to a more tractable form maybe it\nwon't have such differences in\nvolatility over time maybe it won't have\nthe trend maybe it will now be eligible\nfor models to apply that wouldn't\notherwise right for example or EMI needs\nstationary data so often you're going to\nbe different saying to get that\nstationary data so that that's the model\nthat's as much as I'm going to get into\nthis model for those who don't know it\nyet that all you sort of need to know is\nthis describes future data in terms of\npast data and past errors it's as simple\nas that and I should also mention\nfitting these things is not easy fitting\nthese things can be a bit of an art or\nit can be a matter of sort of optimizing\nyour aka information criterion depending\non who you talk to they're sort of\ndifferent schools of thought but\neveryone agrees this is a fairly\ndifficult problem and it's easy to sort\nof over fit it's easy to specify an\noverly complicated model and so on and\nthat's the sort of thing where you can\nthen get embarrassed later when you roll\nit into production and it turns out to\nnot be as good as you thought it was\nokay so why are people not happy with\nARIMA\ndespite for example this beautiful plot\nI showed you right to me this already\nseems pretty good right I'd say if I\ncould get this on on everything I'm\ntrying to predict in the world that's\nalready you know enough to buy a house\nor buy some stocks or something like\nthat but there's a lot plenty of\nproblems right so I think the first one\nyou probably noticed as I took you\nthrough it is it's not especially\nintuitive right I just showed you this\nequation the equation itself is a little\nconfusing if you haven't worked with\nthese sort of forever but on top of that\nwhen you get coefficients that's also\nstill kind of confusing right what does\nit mean that you know X sub t minus 1\nthe coefficient is 0.3 or 0.7 I don't\nlike what what's the difference between\nthe point 3 and the point 7 I don't\nreally know and especially when you have\nmultiple inputs right so if I'm saying X\nsub T is going to be a function of X sub\nT minus 1 X sub 2 minus 2 X sub 2 minus\n3 and they all have coefficients and it\nreally becomes kind of difficult to\nthink about what that dynamic looks like\nand you'll see this in time series\ntextbooks mostly when they give you\nexamples they'll stick to something with\njust one or two inputs right so like an\nAR 1 or 2 model precisely because the\nthe more calm\nit gets the more difficult it gets to\neven think about where different\nbehaviors are coming from there's also\nno way to build in an underlying\nunderstanding about how it works right\nso especially if you come from a\ndiscipline with sort of a well developed\nset of time series data maybe people\nhave been staring at it for five years\nor a hundred years right depending on\nwhat discipline you're in usually\nthere's some knowledge about that system\nand there's also some theories people\nwant to test right so they might even\nsay well it's not enough to be able to\npredict or describe this model with some\nkind of dynamic that's a RMA whatever\nthat doesn't mean much to me but I'm\naware that say there's a random walk\nelement to this kind of data or I'm\naware that there's a cyclical element\nwhich can't really be described with an\nARIMA or there are external regressors\nI'd like to include and then you know\nyou get into more complicated models or\nyou start looking for models that are a\nlittle bit intuitive also some systems\ncycle more slowly or stochastically then\ncan be easily described with an ARIMA\nmodel so ARIMA models do really well\nwith sort of short cycles right maybe\nseven days on daily data things like\nthat but when you're already even\ngetting to sort of 24 hours and hourly\ndata\nARIMA models can get really ugly and\nit's unlike machine learning where you\nsort of throw more data at it and you\nmight get a better result\nARIMA models sort of level out so they\ndo really well with small data sets\narguably they still beat everything else\nbecause they're just so you can give it\njust a 40-point time series and it will\ndo really well and good luck doing that\nwith a tree or a neural network but on\nthe other hand when you give it 40,000\npoints instead of 40 it's not clear that\nyou get much better performance okay so\nenter structural time series so these\nhave been around for quite a while\nthey're not new but they're sort of much\neasier to implement them they used to be\na few decades ago right you get much\nmore computing power now and what that\nmeans is you can have a way that's a\nlittle bit more intuitive of\nunderstanding your system so here's an\nexample of a model that has a level\ncomponent and a seasonal component and\nthen there's a component they call\nirregular is just sort of everything\nelse that can't be explained that sort\nof the error term we're going to and add\nat the end and so he\narguably we already get a much better\nsense right so compare this sorry to\nflip back but compare this to that right\nthis I don't really have a sense of why\nmy models doing well or what's\ncontributing to it right I can see that\nmy model is describing the seasonal data\nI could look at my coefficients from my\nARIMA model and try to puzzle it out but\nthere might be quite a few of them so\nit's difficult to see where as here I\nfeel like I can look at this and say\nwell you know we have this underlying\nlevel that sort of stays quite steady up\nuntil like sort of 83 and then it takes\na dip and I feel like I know a little\nbit more about what's going on and I can\npossibly use this as supporting evidence\nfor a hypothesis or maybe evidence that\ngoes against my hypothesis about how my\ndata is behaving so these are how we use\nstructural time series interesting thing\nthey can also be expressed in a Rhema\nform so in a way we're not actually\ndoing anything novel we're not actually\nif you're going to be like sort of a\nvery formal mathematician about it\nwe're not contributing anything new but\nto my mind we really are because we have\na way of inspecting and enhancing sort\nof the human intelligence rather than\nthe statistics these are fit via maximum\nlikelihood or a common filter right so\nyou can have an MLA interpretation of\nthat you can have a Bayesian\ninterpretation of that interestingly\ncommon filters again not super modern\nI believe they go back to the time of\nthe Apollo mission at least that's the\nstory everyone likes to tell at\nconferences that they actually wanted to\nuse a different way to estimate the\ntrajectory of the spaceship but they\nrealize they didn't have the computing\nstorage space where they'd need to store\nmany points in time to continue their\nestimation so they had to roll out\nsomething where you only need to store\none data point at a time and you just\nupdate and so that's part of the beauty\nof something like a common filter method\nis you estimate your whole time series\nand you just keep a couple of numbers\naround so even if you're estimating a\ntime series that's millions of points or\nyou know 10,000 points something more\nreasonable you're only keeping one\nnumber you're not storing the whole\nthing so this can be really good on on\nsmall machineries such as the machinery\nof you know the computers of the 60s and\n70s still useful today these are largely\ndeveloped in econometrics so if you have\nan econ background you probably have\nseen the\nquite a bit whereas they are otherwise\nunfortunately fairly underutilized they\noffer insights into the underlying\nstructure as we just illustrated and\nit's also possible not in the package\nwe're using but there are packages where\nyou can inject Bayesian analysis via\npriors onto your parameter so you can\nsort of build these out quite\nextensively state seus models generally\nright what what's useful about states\nstate space model it's also compared to\ntraditional ARIMA models they don't just\noffer forecasting and a set of\ncoefficients they sort of offer three\ndifferent stages and these are all\ninherent to fitting them stage number\none is filtering filtering is this\nremember I have this underlying\nassumption I have some sort of noise in\nmy data or are these underlying\ncomponents but I only get to measure the\none thing right filtering is a way of at\ntime T I take all the data from zero up\nto time T including time T and I use\nthat to make an estimate of sort of what\nis the true value right now right\nthere's a value I'm seeing but that\nmight be fairly noisy versus a true\nvalue and so you can see why that would\nbe useful on the Apollo mission right\nwe're actually is our spaceship right\nare our sensors say this but is that\nactually where they are because it's\npretty important to have a good idea of\nwhere they are next step is prediction\nright so in this case at time T I'm\ngonna look T plus 1 or maybe even T plus\nK into the future and that's something\nelse I can do with a state space model\nright so I can both update where I think\nI am now I can think about where I am in\nthe future and then finally I can do\nsmoothing which essentially is you have\nall of your data or all the data of\nrelevance up to say Big T and I want to\ngo back to some earlier time point and\nsay well at time 10 whatever at time 10\nI thought I was at you know this point\nbut now that I see where I ended up at\nthe end I realized I even get additional\ninformation I can sort of back propagate\nthat and get an even better estimation\nof where I was at that time so that's\ncalled smoothing so this is all\nfunctionality the filtering and the\nsmoothing that you don't get with an\nARIMA model right so you actually do\nhave new technology from having this\nperspective common filter I thought\nabout having everyone code this up but I\ndecided this was not the best use of our\ntime so this is just to sort of give you\nan overview\nis a just sort of a high level overview\nwiki graphic of how this works you start\nwith some sort of prior knowledge\nusually just some sort of vanilla\nGaussian distribution you have your\nunderlying state and you have some model\nfor how you should update that\nunderlying state so you make a\nprediction as to where you think is\ngoing based on your knowledge of the\nsystem dynamics and its current\ntrajectory once you make the prediction\nyou use that to do your update right you\ncompare how your prediction right at\ntime T before I measured at time T or at\ntime t minus 1 the state of the world I\nsaw was such that I predicted my time\nseries would go this way or my\nmeasurement right my measurement would\ngo this way but at time T when I got to\ntime T and got my measurement my\nmeasurement had actually gone the other\nway right and so this becomes a way of\nbalancing well my forecast wasn't\ntotally meaningless right so I don't\njust discard that because I have new\ndata what I do is balance why does what\nI'm predicting versus what I actually\nmeasured why are they so different and\nhow should I combine them to have\nsomething reasonable rather than just\npicking one or the other so you do your\nupdate step and then you go back to do\nyour next time step and then so this is\nsort of the forward propagation and then\ngoing backward in time smoothing is very\nsimilar computationally but it's a\nseparate cycle so there's your common\nfilter the underlying model for what\nsomething looks like is something like\nthis so with a state space you have an\nunderlying state and then you have what\nyou can measure right so your underlying\nstate X is going to be some sort of\nmodel based on on your right-hand side\nof your equation you have F sub K times\nX times K minus 1 so how do you update\nyour state based on what was earlier\nthat looks like a lot like an auto\nregressive model but on the other hand\nyou also have the option for inputs such\nas be K times UK that would be what to\nyour knowledge was happening at time T\nnot time t minus 1 right so for example\nin the case of Apollo X sub K minus 1 is\nwhere they thought Apollo was at time K\nminus 1 but u sub K was well what are we\ndoing with our motor or our rocket like\nhow are we positioned in our propulsion\nsystem at this moment or is it even on\nthat's what's described by u sub K right\nso you can also if you know sort of what\nyour astron\nthey're doing up there you can factor\nthat in as well and then you have some\nsort of error terms so there's there's\nyour description of your underlying\nstate and then you have your measurement\nwhich will cause these sub K here right\nyou recognize that in some way your\nmeasurement might not be directly the\nsame as what your X sub K is right so in\nthe Apollo example your state where you\nare and your position that you measure\nyou know should be the same thing we\ndon't have some sort of translation but\nit could also be that these things are\nnot in the same space right it could be\nthat I'm actually X maybe I'm measuring\nonly velocity and acceleration not\npositioned and then somehow with my\nobservation I only get position and I'm\ntrying to find a way to translate\nbetween them so the point I want to make\nhere is just that these are really\nflexible you can describe all sorts of\ndynamics but it does require that you\nhave some kind of hypothesis about the\ndynamics of your system to really be\ninteresting right so you might not want\nto throw this out a model of say really\nnoisy stock data and you're sort of like\nI have no idea what this is so like\nlet's make up a bottle and just put it\nin because you probably won't get\nsensible outputs but a model like a\nrocket where you have Newtonian physics\nand you have a flight plan makes a\nreally good case for this okay and this\nis just spelled out more in detail you\nusually won't have to code this up\nyourself but it's good to feel empowered\nand able to do so it looks really awful\nbut actually it's just a bunch of matrix\nmultiplication and inversion so good to\nremember that you can look it up and\ncode it if you want okay and then how do\nthese models tend to get evaluated\nusually with your K key information\ncriterion your AIC so just for anyone\nwho's not familiar with this I want to\nput it up it's two times your number of\nparameters minus two times your log\nlikelihood right so you want it to be\nmore negative when you're evaluating\nmodels this is just one way you would\nmight evaluate models such as\nstate-space models okay so here are some\nmodels that we're going to look at in a\nin a jupiter notebook right after this\nso we're gonna look at things like local\nlinear trend or smooth trends so this I\npulled directly out of the stats models\npackage for the unobserved components\nmodel so let's see can you see this no\nyes here's my MA so okay so it can give\nyou either two ways to select these\neither of these can be your inputs and\nthen\nthis tells you sort of the underlying\nmodel that it's using so in this case we\nhave an underlying model where our\nobservable is some function of the\nunderlying state plus epsilon some kind\nof error and the underlying state itself\nis a function of its former state and\nalso some sort of velocity type terms\nsome kind of trend term of where it's\nmoving that's our beta plus some sort of\nerror and then our beta itself can\nchange so that's our local linear trend\nand then take a look at the smooth trend\nand see if you spot the difference they\nlook quite similar but there is one\ndifference so what's the difference\nYeah right so no no stochastic term here\nand the smooth trend in the middle\nequation there's no stochastic term\nright so that's just to give you a sense\nof the distinctions that people make\npeople sort of make these slight tweaks\nand they have different names but it's\nalways best to just go look at the\nequations to see what they look like\nokay so now that I have bored your pants\noff with a bunch of slides I think it's\ntime to open the first Jupiter notebook\nokay so people should feel free to use\neither the instructor or the student\nversion student version is if you don't\nwant to be able to cheat you know that\ncan be a good way to discipline but you\nshould you can be your own judge of what\nmakes the most sense okay so the first\nthing we need to do is obtain and\nvisualize our data so in this case I I\nhad downloaded some two different\nmeasurements of global temperature data\nyes yes okay so thank you excellent\npoint\nso from the base folder these are in the\nstate state state space models folder\nokay I'm sorry so for anyone who walked\nin late no worries the set of Jupiter\nnotebook slides are available on the\nslack Channel and a zip file and\notherwise someone has helpfully posted\nthem here I see that's the older version\nsorry about that okay I really\nappreciate that so just want to confirm\ndo you most people have the student\ninstructor version let's see so if you\nscroll up in the slack this was sent 913\np.m. last night it's the second version\nrather than the one that's pinned and I\ncan upload it again so it will be the\nnewest thing\nokay so that's the newest thing\nso in that case you should email Syfy at\nan calm and they will just tell them\nthat and they will almost instantly\nreply\nI think someone's just Manning that\ncontinuously other questions okay\nokay so for those downloading it's a\npretty small zip file so I think you\nshould have it pretty quickly I'm just\ngonna just slowly look at this data and\nyou should be caught up pretty quick Oh\ngithub is updated okay great thank you\nso github is also updated if you find\nthat more useful\nthat's this okay okay so we're just at\nobtain and visualize data so this CSV\nshould be available in the zip file and\nI have the source listed here this is\njust two sources of global temperature\ndata obviously climate change data is a\ngreat source of arguing about time\nseries a great source of possibly\nambiguous time series depending on your\npolitics but it's a very rich source of\ndebate so I grabbed some of that and the\npoint of this data set is they have sort\nof two sources of the data now we can\ncompare so if we load that data and take\na look at it the first thing I do is\njust plot you know just basic time\nseries plot\nI also recommend sometimes that you\ndon't plot the whole dataset all at once\nright so for example here I just looked\nat the first hundred data points anyone\nhave an idea why I recommend that or why\nyou might feel the same that versus\nlet's look at this one yes let's see\nmaybe I was a man how's that\nokay so I'm not especially satisfied\nwith these plots does anyone feel\nsatisfied with these plots I hope not\nokay so that's actually the first\nexercise I have for you I'm not gonna\nscroll up for those who want to have it\nbe a secret until you figure it out but\nso its first coding exercise for\neveryone I'd like you to take a couple\nof minutes to figure out what's wrong\nwith the way the data looks now and the\nplot and how can we fix it because it's\nnot even just an aesthetic problem\nthere's an error right now so this would\nbe one of these if I presented this to\nmy boss I should be really embarrassing\nkind of situations so think about what\nthat is and how you could correct it\nokay so I think we had already covered\nthe problems right we've got data types\nthat are not especially helpful we've\nactually combined different measurements\ninto one time series when actually that\ndoesn't make any sense I should have\nbeen a little wary of that given that\nalready even in my plot I don't have\ntime displaying right I have just sort\nof a random index so the first thing\nwe're gonna do is we're just gonna pivot\nour data and then if we plot our data\nincidentally it seems like that has\nreordered our date index appropriately\nand now I'm seeing something more like\nwhat I expect and now I'm looking only\nat one type one time series instead of\ntwo right I actually have two time\nseries in my data set\nI should not plot them as one and as you\nsee here right unfortunately often and\nthis is something that's tricky about\ntime series right you can be plotting it\nthe wrong way and it will still look\nreasonable right especially if I didn't\nhave domain knowledge about sort of\nglobal warming that would have looked\nfine to me and maybe I would have\nfoolishly carried on another problem\nthough is I still so now my my dates\nseem to be an order right I could check\nmore extensively but they're still only\nuseful as an index whereas I would like\nsomething a little more useful so let's\njust go through this we won't do this as\nan exercise how can we make the index\nmore time aware well nicely pandas\nprovides date time and also period time\nindexes so we're gonna use date time and\nif we do this we see we now have a date\ntime index instead of a base index class\nso what do I get from that well already\nnotice compared to this plot which I\nalready thought was\nokay I've now got a pot where these are\nnot being treated as strings they're\nbeing treated as actual dates and so\npandas can fill in a little bit of\nbackground knowledge about what sort of\nlabeling would be reasonable and then I\nalso get sort of Handy label indexing\nlike this so now I can now index a by\nyear right just the 1880 is going to\nshow me all the 1880s which also is just\ngoing to make my life easier I can do\nsome plotting like this so now I plot\nboth series from 1880 to 1950 and get a\nsense of how correlated they are or I\ncan just plot everything from 1950\nonwards and again get all that nice\nfunctionality with the date time so this\nis just a reminder that that's available\nto you and it can be very helpful and\nobviously always look carefully at your\ndata before you get started okay so\nquick exercise how strongly do these\nmeasurements correlate contemporaneously\nwhat about with a time lag so now we've\ngot two groups of data just quickly two\nminutes how helpful are they for\npredicting one another or for\nunderstanding one another\nwe can it can depend a lot on from one\npackage to another I guess one critique\nI would even have of the Python spaces\nthere's not a lot of uniformity around\nthis because the functionality is spread\nout among many packages and not to be\nanti Python because Python is my first\nlove but are actually does a better job\nof having more of a unified interface\nokay so how strongly do these\nmeasurements correlate contemporaneously\nthere's all sorts of ways to think about\nthat but just want to point out we can\nyou know just do simple things like\nscatter plots right so now I'm plotting\nthe two measurements the GC a G and the\nG is temp against one another for the\nexact same time period so at any given\ntime they seem to correlate pretty\nstrongly if I you know if I saw\nsomething like that it might work more\noften I would be delighted similarly we\ncan ask what about whether years offset\nright what do these have any sort of\npredictive value for each other so in\nthis case I'm plotting 1880 to 1899 for\none variable versus 1881 to 1900 for the\nother so I've just offset it by a year\nwhat does that look like having inste\nlook more like the real world and what I\nsee at my job anyway but is it terrible\nwell it depends on your domain right\nmaybe for predicting air passengers this\nwould be terrible maybe for predicting\nhousing prices it's not so bad we can\nlook at the Pearson R and we see it's\nnot zero right I mean actually this\nprobably looks worse than it is so it\ncan also be good to have different\nmeasures visual and numeric and again\njust as a reminder we can sort of look\nat the data with standard standard tools\nfrom pandas and then if we want to get\nan idea of our date range we can look at\nour min and Max for our index right so\nall sorts of standard operations okay so\nnow we're going to throw this into an\nunobserved component model I'm only\ninterested in training on the data from\n1960 onwards just personal choice I\nthought that's a little bit more\ninteresting that's sort of where we see\nthings taking off we're going to define\nour model so we have our model per M\nand all I'm gonna stay say is I want a\nsmooth trend for my level I want no\ncycle and no seasonality that's sort of\nmy first pass based on these clots we\ndid here right so looking sort of at\n1960 onwards I'm like eh I don't know if\nI seem much cycling or seasonality\narguably I could be more careful about\nthis but this is gonna be my first pass\nso I build my model with a dictionary I\nfit my model so I'm gonna make an\nunobserved components object from stats\nmodels right so stats models has a whole\ntime series analysis API including a\nstate space API with unobserved\ncomponents I give it my data and I just\ngive it the model to unpack right and\nthere's other sort of parameters you can\ntweak and you can read up about those in\nthe documentation but this is as simple\nas it is for a first pass I'm then gonna\nfit my model and I'm gonna plot the\ncomponents right because this is what I\nwas talking about as being like the\ngreat thing as compared to an ARIMA\nokay so I've plotted my components what\ndo I see here I've got my predicted\nversus observed plot ah so how do we how\ndo we feel about this fill seems like\nwe're always within our confidence\ninterval it seems to follow pretty well\nthis is a one step ahead prediction\nright so you also would want to think\nabout whether that's sort of interesting\nto you is it interesting to predict\nglobal temperatures one month in advance\nor would we like to try a more will try\nmore below we also can look at our two\ncomponents right so we have a level\ncomponent and a trend component what do\nwe think about these how do these look\ncompared to what I showed you in the\nPowerPoint I would say not great right\nthey they both sort of look like Wiggly\nlines I would say that I don't get a\nwhole lot of insight out of this when I\nlook at it I say hmm level versus trend\nI mean they both it looks like hmm okay\nthe model mostly mostly gives the\nabsolute value of the time series to the\nlevel and then it's almost like the\ntrend is just whatever was left over but\nit doesn't look like a meaningful trend\nso thinking about that I think okay well\nlet me revisit\nthat and we will revisit that in a\ncouple of steps we might also want to\nplot our predictions so to do that we\ncan also set a greater time horizon to\ndo at the end to do dynamic prediction\nso if you move down to the next cell\nI set the number of steps I want to\npredict forward as being 20 so 20 months\narbitrarily chosen so now I want to get\npredictions and I want to do the last 20\nsteps is dynamic meaning that now I'm\nnot going to update at each step right\nthat's when we do our one step ahead\nforecasting now for my last 20 steps\nit's just gonna roll with those 20 steps\nit's just gonna take what it predicted\nand that will be its new input take what\nit predicted that will be its new input\nit's gonna move forward so if I do that\nthis is what I get and I want to draw\nyour attention to the end what do you\nnotice at the end sort of this upper\nright-hand corner it's just sort of a\nflat line right so that's another thing\nyou're gonna notice with these models to\nbe aware of right when you're doing sort\nof multi-step horizons they're not so\ngreat in the sense of since there's no\nerror they'll just assume there is no\nerror and they're just gonna sort of\nkeep going along whatever smooth\ntrajectory is established by the\nunderlying model that can be a good\nthing or a bad thing depending on what\nyou want to do this certainly will look\na little bit different say from an ARIMA\nmodel where you will sort of see more\nwiggles and things like that okay and\nthen how does it do how does this model\ndo well it looks pretty good for one\nstep ahead right but can I can I really\ndecide if it's really good just based on\none one plot well no especially if I'm\nnot comparing this to what would my null\nmodel look like right I need to think\nabout what my null model would look like\nyou can also do a cleaner plot so this\nis just to illustrate if you want to put\nin some confidence bounds again this red\nnow is showing your dynamic prediction\nyour dotted lines are showing your\nconfidence bounds they don't look\nfantastic either right so here's another\ndifference say it compared to if you're\nused to an ARIMA model your confidence\nbounds will rapidly diverge because of\nthis unincorporated error that you no\nlonger able to handle and we can look at\nthis also more up\nto get a sense and as you can see what\nwe discussed right your your forward\nmoving prediction is just going to be a\nflat line for these simple models okay\nso next exercise for you all is consider\nadding a seasonal term for twelve\nperiods to the model fit above and does\nthis improve the fit of the model so we\nhad initially rejected seasonality but\nshould we revisit that and how do you do\nit if you're not sure go to the stats\nmodels API you can google that and look\nfor the unobserved components\ndocumentation so let's take a couple\nminutes to look at that so also I've\njust posted the slides I had a request\nto post slides so those are now also in\nthe time series Channel ok so how do we\nadd a seasonal term here we keep a local\nlinear trend and now we add a seasonal\nparameter which we set to twelve by\ntwelve its monthly data right so what\nwhat if anything else would I expect\nright so if the if the earth has a\nseasonality that would be my guess as to\nwhat it is and if we now apply our\ncomponents we see something interesting\nnow alright so we see what looks like a\nvery regular seasonal component which\nagain shows how silly I was not to have\ndone more exploration of my data at the\nstart when beginning to think about what\nkind of model would be appropriate we\nalso see now that sort of the trend\ncomponent has gone to just this\nnegligible thing it's not really adding\nto the model we have a level component\nand a seasonal component and that\nalready seems to do a pretty good job so\nhow does this compare to the original\nmodel if we wanted to compare them we\nmight think about for example comparing\nthe correlation of the two models with\nthe data and we see these are sort of\nindistinguishable right so that to my\nmind is not a very helpful metric and\nshows how we should be skeptical because\nvisually clearly one was doing a better\njob of describing the data are offering\nmore intuition to us than the other so\nthat shows sometimes that these\nnumerical measurements might not be very\ninformative\nand what about if we look at the mean\nabsolute error in that case in both\ncases we see that the new model does\nslightly better but yes okay so if you\nlook at the I'll go up and hopefully you\nhave this in your notebook as well it's\nthis is the seasonal component so the\nseasonal component is not zero it's it's\nclearly significant and it's showing\nsort of a pattern that makes sense based\non our domain knowledge of some kind of\n12-month cycle of the earth's\ntemperature okay I'm glad people are\njust interrupting me when whenever is\nhelpful because that's what this is\nabout\nokay so point here is especially with\nstructural models people sometimes try\nto optimize them just looking at\nsomething like the AIC and that's great\nand it's at least some sort of measure\nbut arguably especially because\nstructural models are mainly just sort\nof helping your intuition and helping\nyou understand the underlying dynamics\nyou might find that a model that looks\nmuch worse visually that isn't offering\nintuition doesn't do that much worse\nright but then it's not offering any\nvalue so a big part of thinking about\nwhen to use these models and how to use\nthem is what are they teaching you about\nthe data and it's really interesting to\nme that in this case we see an\nillustration of how just exploring\nmodels pointed out something that we\nsort of initially missed about this data\nbecause we did insufficient exploration\nthat's another way this can be better\nthan a Rhema for example with inner\nRhema you might notice that your model\nisn't fitting your data very well but\nyou won't get such easy insights into\nwhy versus here we say oh look boom I\nadded a seasonal term and I can actually\nsee the seasonality rather than just\nseeing a coefficient okay so now let's\nexplore the seasonality more what's\nsomething else we see here we see that\nour trend component wasn't especially\nuseful so why don't we get you why don't\nwe get rid of it all together how do we\ndo that we switched to a local level\nmodel instead of a local trend model and\nwe keep our seasonality so if we do that\nwe really now we get something that\nlooks really beautiful in terms of not\nhaving extraneous portions of our model\nlike a whole trend component that we\ndon't really need we also see the\nseasonal component again very strong\nsignal interestingly it is not\nI'm varying right into a sort of uniform\nand we see a sort of a level drifting\nattending to drift in the positive\ndirection right so to the extent that we\nwant to have debates about climate\nchange this is one insight of like oh if\nI fit this kind of model I see gradually\nthat the level is going up and I see a\nseasonality and that can be sort of\nfirst-order way to talk about this data\nwhen you're first getting into it\nquestion in the back\nNo okay okay and then we can again sort\nof look at the mean absolute error and\nwe see that we are continuing to improve\nthese are sort of small incremental\nimprovements not massive improvements\nbut mainly we are having a more\nparsimonious model that gives us better\nintuition into our data okay and then\nonce we see this we realize we really\nshould have revisited our data and\nthought about the shape of it right so\nif we sort of plot here we can see more\nseasonality by looking more closely\nalthough we see that this is not as\nclean of seasonality picture as we saw\nsay with the air passengers data right\nso it's a noisier seasonality and yet\nour structural model did a pretty good\njob of capturing it nonetheless okay so\nfinal exercise and then we're going to\ntake a break a common null model for\ntime series is to predict the value at\ntime t minus one for the value at time T\nso how does such a model compare to the\nmodels we fit here just take two minutes\nto compare those right have we actually\ndone anything with sort of our fancy\nstructural time series model or are we\njust kidding ourselves something you\nalways have to be asking yourself with\nthat fancy methods\nyeah exactly so basically it will just\nhave a coefficient 4 it will have 12 11\ndifferent coefficients right you don't\nneed 12 right you need n minus 1 for n\nseasons and then you'll have a\ncoefficient per and you'll say Oh at\nthis point we add or subtract this much\nand that's where you get a seasonal\ncomponent yeah so it can handle\nsomething like an incomplete cycle well\nI should I should really be saying an\nincomplete seasonal cycle as compared to\na psycho because structural time series\ncan also handle a cycle what a cycle is\nis when you have some sort of repeated\npattern but it's not as regular as a\nseason so for a season the way that\nthese models are fit is they're sort of\na different component per season so\nthere will be 11 components to describe\nthe 12 season model right because one of\nthem will just give a null component\nlike everything every other kind of\nmodel that does this and then if you\nwant to fit cyclical data what that will\nusually fit to is sort of a sine curve\nwhere what it's then trying to fit to is\nto determine the appropriate frequency\nto model the data and you can also add\nthings like a damping parameter so you\nhave sort of a start with a cycle but\nthen usually what you want to do is you\nwant to both frequency will be time\ndependent unlike a season where the\nfrequency is fixed and it will also have\nthe option for those who are either\nphysicists or remember of some sort of\nlike damping envelope is something else\nyou can fit too but that's not what we\nsee in our data right we in this data\narguably we see some kind of seasonal so\nin that case so for seasonal you will\nreally have like S sub 1 S sub 2\nS sub 3 and so on and it will just\nassume that F sub 1 is what you start\nwith\nit doesn't actually care what we\nconsider the first one it will just take\nthat and as long as it goes around in a\ncircle that's good\nokay so how does our null model compare\nto what we did so yes\nso absolutely so this is even a problem\nwith ARIMA models right which seems sort\nof more traditional statistical it's\nwhat's interesting for example just to\nback out for a Rhema and then I'll come\nback to this case you can actually have\nmodels that look different they look\nlike they have different parameters but\nactually if you do the polynomial math\nyou can factor out and you realize you\nhave like sort of extra terms that\nliterally just cancel out and you are\nliterally fitting a model that math\ntells you is too complicated like your\nmodel makes no sense mathematically so\neven with a simpler method like that\nwith structural time series I I consider\nthem more useful sort of as an\nexploratory tool it's not something I\nwouldn't fly an airplane or treat cancer\nbased on the outcome of this now that's\njust me many Khan Academy Trish ins\nmight feel a little bit differently the\nmore data you have the better if you\nmove this into a Bayesian model where\nyou have a much stronger prior that's\nalso better and actually Google has a\ngreat Bayesian structural time series\npackage because then you at least have\nstronger inputs into what you want so if\nyou inject say a strong prior from a\nBayesian sense and I would trust these a\nlittle bit more versus if you are just\nusing your default Gaussian and rolling\nwith what it is absolutely and these can\nbe very easy to overfit like many sort\nof data intensive computation intensive\nmodels and there is no guarantee also\nthat you get to the optimum fit for your\ndata but arguably that's even a good\nthing right because getting to your\noptimum fit for your data could be\noverfitting as well yes\nso this depends on the discipline\ncertain disciplines will have very\nstrong priors not in the Bayesian sense\nof the shape of the distribution but in\nthe definition of the model so for\nexample if you're modeling the stock\nmarket you have to make a very strong\ncase for using anything stronger than\nsay a random walk and you'd have to have\na justification another example I know\nof is people modeling hydrology feel\nvery strongly that that should just be a\nlocal level model without a trend so\nthat tends to come from other domain\nknowledge or theories about causation of\nthe data you're seeing yeah so in this\ncase we're just exploring because we are\nnot climate change experts so for\nexample I would I would be very\nskeptical if someone went to Congress\nwith this and said look I figured it out\nfor example okay okay so very quickly\nand then we'll take a quick break so if\nwe're looking at our null model compared\nto this model if we consider the\ncorrelation they're not measurably\ndifferent right so we've done all this\nwork fancy model but it's not it's not\nclear we've done much better maybe we've\ndone a tiny bit better and maybe that\nmatters but as we were just discussing\nsince it's possible to overfit our\nmodels or there's so many knobs to turn\nwe have to look at skepticism right so\nwhether this is a good model will depend\non your purposes whether you're just\ntrying to interpret it are you trying to\nmake predictions and what sort of\naccuracy do you need right is half a\npercentage point improvement in\ncorrelation meaningful in your field or\nno it's very context dependent but\notherwise I mean if you're someone's\nboss and they come to you with this\nobviously the first thing you should do\nis shoot them down and say do you really\nneed this because you've barely improved\non your null model so explain to me why\nwhy you haven't and what about in terms\nof me and absolute error well here we\nhave quite a difference right so so\nmaybe this is what's something we're\nthinking about right and maybe we should\nsay okay well well why is that and and\nhow can we improve that and what\ninsights does that offer us right so\nagain just a reminder to use different\nmetrics and think about your context\nokay so those are structural models\nwe're going to take a short break and\nthen we're going to come back and do a\nhidden Markov model\nand that will be a wrap on state space\nso let's just call it a five-minute\nbreak and come back at 9:22 okay so our\nbreak is over I just dumped my email\naddress in the slack also in case that's\nnot available on Syfy's so if anyone has\nquestions after the tutorial or\nfollow-up comments great to hear from\nyou yes so the slides are in the slack\nas a zip file and then for anyone who\nwasn't here at the beginning I am\nunfortunately locked out of github at\nthe moment\nfor very it's a very long story so as\nsoon as I'm back in github they'll also\nbe on git and and someone has nicely\nposted them I get now but ultimately I\nwould like to you know have the sides\nand they get the repository altogether\nso and also if you email me once I get\nthat together hopefully in a few days\nyou can also have that does that answer\nyour question yes okay any other\nquestions okay okay and then just so\nfolks know we'll definitely do another\nbreak before they stop serving breakfast\nat 10:30 in case anyone needs to pick me\nup okay so we're going to be talking\nabout hidden Markov models now this is\nanother technique it's not especially\nnovel or considered cutting-edge but\nit's increasingly useful because now you\ncan actually get decent computing power\nas opposed to when it was invented you\nknow and maybe if you were at IBM and\nhad a really fancy computer or whatever\nyou might be able to fit it now we can\noh you know just fit it on our laptops\nso much more relevant than it used to be\nwhat is a hidden Markov model why is\nthis the state's today's state space\nmodel well it's the same idea in the\nsense of having the idea that there is\nan underlying state that you cannot\nobserve and there is the output of that\nstate which you can observe in the\nstructural time series models we tend to\nhave the model that the state and the\nmeasured quantity are basically the same\nthing with error but we can also broaden\nour perspective with hidden Markov\nmodels and think well maybe what we\nobserve might not even be in\nsame physical or conceptual category as\nthe underlying state that's producing it\nso in this case what we have we have a\nsequence of measurements as depicted\nhere where we envision I feel strongly\nthat people should be able to come and\ngo whenever they want so no one no one\nis imprisoned here okay or locked out on\nthe way back so in this sense what we\nenvision is we have a sequence of states\nand for whatever reason the dynamics of\nthat system the states are evolving over\ntime we conceptualize this in a\ndiscrete-time framework but this can\nalso be in continuous time and you can\ndo hidden Markov models and continuous\ntime we won't look at those but if those\ndescribe your data you can think about\nthat so we have a sequence of discrete\ntimes where step the underlying state is\nevolving we can't see that directly what\nwe can observe is whatever the\nobservable is why also at those times\nand somehow X the underlying state is\nwhat produces a certain kind of Y so how\ndo we make this more physical well one\nexample where this is widely used and\nactually where you don't even need to\nhave the fiction of discrete time steps\nis in DNA analysis so when people look\nat DNA sequences and they're trying to\nthink about why do I see maybe certain\npeptides or why do I see a certain\nsequence and what does that tell me\nabout the underlying state of the DNA\nI'm not a biologist so I'm probably\nslaughtering this but the point is is\nyou have these DNA chunks that really\nare coming in discrete measures and\nthere's an example where it is a time\nseries in the sense of having an axis\nthat is well ordered and has a good\ndefinition of sort of moving forward or\nbackward along an axis so that's an\nexample where maybe your underlying\nstate again don't don't hold me to it on\nthe biology maybe your underlying state\nis something like how methylated is this\nchunk of DNA and what you observe maybe\nis something like over time like what\nkind of enzymes are being produced at\nthis time what kind of distribution so\nnotice that X itself the underlying\nstate that's usually going to be\nas discreetly defined right I have a\ncountable number of states these are\ndiscrete states I'm not going to be in\ntwo at the same time although again you\ncan model anything right you can make\nthings infinitely complicated but that's\nnot the traditional hmm and then your\noutputs these can either be discrete or\nthey can be continuous right so in the\ndiscrete case it could be like what kind\nof sequence am i observing where that\nsequence is some other kind of indicator\nbut not the state or it can be some kind\nof continuous variable so an example of\nthat and the one we're gonna model in\nour notebook is we can think of rivers\nas being sort of in high flow or low\nflow States to first-order and then we\ncan think about well if you're in a high\nflow or a low flow state what kind of\nflow are you going to see that's the Y\nand the Y will be a continuous variable\nright so it's not like I'll see you\nexactly the same number for low state\nversus high state and there's a good\nchance that the distributions will\noverlap but if I sort of look at the\nwhole sequence I might get a sense of oh\nthese were low and these were high and\nthese were low so why might I use this\nsay as opposed to an ARIMA model and I\ndon't mean to say that every mamada\nloves the be-all-end-all but we should\nthink well why would I choose this model\nover say another one that's known to\nperform well well there's a couple of\nreasons so again just like with\nstructural time series this offers us\nsome kind of intuition and some kind of\nsort of model of the underlying dynamic\nso to the extent we have a model like\nthat for a particular system like this\nif it fits then that's a good thing\nbecause it can offer some kind of\ninsight another thing for example in the\ncase of river flow and this I learned\njust from reading a hydrologist blog who\nis blogging about HMMs is that they find\nthat ARIMA models don't capture these\nsort of nonlinear dynamics of it's not\nthat you know time T minus one predicts\ntime T it's more of this sort of regime\nswitching where you know kind of the\nprobability of the regime switching but\nwhen you last switch two regimes doesn't\nnecessarily tell you when you will next\nmature regimes and that's a feature of\nMarkov processes in general right the\nwhole point of a Markov process is that\nwhen I'm at t minus 1 if I know my state\nat t minus 1 I don't even really care\nabout King before so that's quite\ndifferent from an ARIMA model where I\nsay oh I'm looking at my whole history\nto determine where I'm going next with a\nhidden Markov model I'll say as long as\nI know XT minus 1\nI don't need to look backwards in time\nsort of all the information I need is in\nthat data point so those are just a few\nobservations about hidden Markov models\nand when you might want to use them okay\nso there are state space model\nobservations are an indicator of the\nunderlying state but we posit some\nseparate underlying state and in this\ncase you really wouldn't posit that your\nunderlying state is just your observable\nwith error that's not sort of what\nyou're doing with a hidden Markov model\nunlike the structural time series in a\nMarkov process the past doesn't matter\nif the present status is known right so\nthe most informative thing is just your\nmost recent measurement and nothing else\ngives you new information so these\nthings the two aspects computationally I\nwant to talk a bit about before we fit\nour parameter estimation with the bound\nwelch algorithm and smoothing / state\nlabeling with the Viterbi algorithm so\nI'm guessing you've seen these words\nfloating around I just want to give you\nan overview or an intuition for what\nthey are before we use them okay so the\nbound Welch algorithm is how we\ndetermine parameters so when would we\nuse this this is at the very start this\nis like I have a time series I have a\nsense that there's some kind of regime\nswitching but more than that I really\nhave no idea like I have no priors\nthere's nothing I know about sort of\nwhether you know the mean of one state\nis 3 or 17 I have no idea so this is\nreally amazing in the sense of you can\njust say here's here's my data and\nhere's how many sort of underlying\nstates I think I have go and and it will\nsort of figure out a set of parameters\nnow of course what what does that entail\nit's the same concern we talked about\nearlier you have a lot of knobs to turn\nthis is a very complicated situations\nright when you start thinking about how\nyou would code this up if you just had\nto code this yourself it's sort of a\nnightmare\nso basically what that means it it's not\nguaranteed to converge to a global\nmaximum you're not guaranteed to get the\nabsolute best parameters to describe\nyour data what this algorithm does\nguarantee is that you will always get a\nlittle bit better with each cycle of\nthis algorithm so it's a forward\nbackward expectation maximization\nalgorithm right which means you sort of\nfirst figure out what your likelihood\nexpectation is given your data that's\nyour\nexpectation step and then your\nmaximization step is you sort of update\nyour estimate of your parameters to\nmaximize the likelihood given that\nexpression of the likelihood form and\nthen you do it again right and you just\nsort of do it and do it and do it until\nyou get to what you think is an\nacceptable convergence level or an\nacceptable likelihood but that will only\nget you to a local maximum so that's\nanother thing to keep in mind with these\nmodels right again there there's not\nsome sort of unique well-defined closed\nform solution and often what people will\nrecommend is that you run it many many\ntimes before you sort of make any\npronouncements on what this is showing\nyou right so you don't just run it once\nmaybe you run it a hundred times and\nlook at what your your parameter\nestimations are and see if some sort of\nlike narrative or some form is emerging\nfrom your many many attempts and of\ncourse keep in mind you can over fit the\ndata right so it's quite easy to maybe\nchoose a set of parameters that will do\nmuch better on your training data than\non some sort of validation holdout so\nthat's always a risk with these sorts of\nthings okay so just a brief overview of\nthe details right what's the problem\nyou're trying to estimate basically\nthree things you're trying to estimate a\nwhich is sort of your transition matrix\nprobability so what's that if we go back\nyou're at the next time step including\nthe same state so a describes from XT\nminus 1 to XT what does my transition\nlook like am i likely to stay in the\nsame state am I likely to go to a\ndifferent state and let's quantify that\nright it's not just you don't get an\nanswer of a little likely not likely\nright you actually get a matrix saying\nto go from from state I to state J your\nprobability is point four to go from\nstate I to state K your probability is\npoint two or whatever and so on so\nthat's the first thing you're trying to\nfit your B is so you're seeing another\nmatrix there or another sort of set of\nparameters indexed off your state how\nlikely is a particular Y assuming a\nparticular underlying state and then\nwhat's the final thing you need to\ndescribe this model pie for the priors\nthat's telling you how likely you are to\nsort of begin in\na particular state so this model also\nposits some sort of beginning which is\nsomething you want to think about maybe\nif you're taking the same time series\nand just slicing it up over and over\nagain the idea of a prior is saying well\nno we sort of think you're likely to\nstart in some states you need to think a\nlittle bit about what is starting mean\nand does my data sort of acceptable fit\nthat so on your forward step essentially\nwhat you're doing is you're developing\nthe probability of being at time T where\nT can be any time the probability of\nbeing in state I at XT and at the same\ntime right so this is a combined\nprobability the probability of seeing\nthe whole sequence of your actual\nobservations at y1 all the way up to YT\nso that's your forward step you're sort\nof moving forward in time how probable\nat XT is it that I'm in state I and that\nI've seen all this data at the same time\nand then you go backwards in the other\ndirection and then you say okay on the\nother hand how probable is it if you\nknow so in this case conditional on\nbeing in state I at time T how probable\nis it that I see then the sequence from\nt plus 1 all the way to t right so\nyou're sort of effectively picking a\npoint in the middle your forward step is\nhow probable is it that I am in steps\nstate I and I've seen this data and your\nbackward step is again I'm still a state\nI how probable is the data that comes\nafter it given that I'm in step I right\nso you can see if some sort of measure\nof we're converging on this measure of\nhow probable is this whole sequence from\nthe point of view of this point in time\nand then you estimate two other\nparameters right which essentially\nbecome our just expressions of these so\nwe calculated this Alpha and the beta\nright the forward and the backward and\nfrom these you can express so sort of\nmore generic quantities such as above\ngamma sub I is the probability of being\nin state I at time T even all the data\nyou've observed the full time series and\nyour estimation of theta right your\nparameters and you can also estimate\nfinal component of the behavior which is\nthe transitioning given the data you've\nseen in the theta so it's a lot of\nprogramming as you can\nand this is just to make you aware of\nsort of what's going on under the hood\nand from this you can then estimate\nthose three quantities right the pie for\nthe prior how likely you are at the\nbeginning of a sequence to be in any\ngiven starting state because that will\ninfluence where you go from there\nalpha sub IJ how likely you are to\ntransition from state I to state J at a\nparticular time step and finally beta\nwhich is how likely you are to see a\nparticular observed value given you're\nin a particular underlying state okay so\nthat's how we would estimate the\nparameters this is really\ncomputationally taxing it's really\ndifficult and like I said you're not\nguaranteed to get to the perfect\nparameters you will get you one\niteration of parameters that are a\nglobal maximum optimization final slide\nbefore we go to some code what's the\nother thing we want to do so first we\nwant to have the parameters to sort of\nexplain our underlying sequence right if\nwe posit a certain number of states what\nparameters can we come up with to\ndescribe the behavior okay now we've got\nthose parameters let's say we accept\nthem and want to move forward then what\nwe need to do is figure out if we posit\nthese underlying parameters as\ndescribing our state what actually were\nmy states in my time series at each time\nstep I can actually get a label and how\ndo I do this I do this via the Viterbi\nViterbi algorithm for determining\nsequence so this has to come afterwards\nto use this I need to have those\nparameters describing my time series if\nI have that this is a dynamic\nprogramming problem for those who don't\nknow the term dynamic programming is\nbasically when you realize that your\nlarger problem is actually composed of a\nsmaller problem you can that sounds like\nrecursion but it's different because you\ncan also memorize it it's sort of a\nlimited countable ordered set of\nproblems right so why does that describe\nthis why is this not just recursion and\ndynamic programming because I have a set\nnumber of steps in my whole path I have\na set number of options I can explore so\nif you think about it I could actually\nsummarize all of that in a matrix right\nso to get from say\nwhat's the best way to do that and I\ncould explore all of the possible states\npermute those states versus the cost and\nI can ultimately sort of come up with\nthe most probable way to go through\nthose states if that sounds complicated\nand like I'm waving my hands I\ndefinitely am you can read the code it's\nactually not that bad but it's a pain in\nthe butt to code right but you sort of\nget the sense of like all the things\nyou'd need to think about to code this\nup properly and make sure you found the\nmost probable path through your data so\nleave it at that this is a great example\nthough of something that could be\naccessible if you did want to code it up\non your own okay so now we're gonna open\nthe second the second notebook as soon\nas I find my notebook so again in state\nspace model is now we're looking at the\nGaussian hmm so a Gaussian hidden hidden\nMarkov model is where you're gonna have\ncontinuous observables you can also have\na multinomial hidden markov model where\nboth your underlying state and your\nobservable are sort of these discrete\nstates so that's another thing if that\nbetter describes your data you could\nlook into okay so in this case as I\nmentioned I was trying to get Colorado\nRiver data but the website wasn't\nworking so well I don't know if that's\nall so budget cuts for whatever but\nwe're gonna look at the Nile instead\nthis is a very famous data set if you\nread traditional stats textbooks this is\nall over so if we look at the nile data\nwe see that we don't have the problem\nhold on I need to clear this start over\nwe don't want to be giving away the\nanswers we don't have the problem of\nsort of multiple groups of observations\nwe just have one value for each year\njust the flow for that year we don't\neven know when this was taken or if it's\na cumulative value but what we do see is\nwe see this plot right so this is kind\nof interesting given what the\nhydrologists told us based on the one\nhydrology blog I've ever read they're\nsort of a high flow and a low flow state\nto first order if you want to model a\nriver system that could be due to things\nlike\nEl Nino or whatever the equivalent is in\nthe Nile region right but these sort of\nglobal larger global cycles or patterns\nor states that are affecting things like\nRiver flow that are not things like just\none year predicts the next but more like\nyou are in one regime and at some point\nyou switch to another but it's not\ndeterministic some kind of nonlinear\nstochastic switching so if I sort of\neyeball this to me I'm like wow this\nalready looks like sort of two states\nright like I've sort of got this high\nthing up here and this low thing up here\nthe one bit of background research I did\nis I thought oh man is this one the\nSoviets built that dam on the Nile maybe\nthat's just building the dam but then if\nyou look at the the time stamp it's\naround 1900 which was definitely not\nwhen the Soviets were building the the\ndam on the Nile so we know it wasn't\nthat so we say okay I'm gonna assume\nthis is some sort of climatological\nphenomenon we're looking at so let's\ntake a look at the API for hmm learn so\na little bit of history about hmm learn\nI believe this used to be part of\nscikit-learn and then it split off to be\nits own project and was a continuation\nso just be aware of that when you look\ninto hidden Markov models it was\nsufficiently complex or interesting to\nbe its own thing although it has a\nsimilar API so first tricky thing is\nactually they just sort of require this\nshape they require two dimensions for\ntheir input so that's why there's this\nexpand ins if you run it without that\nyou'll get an error and you'll just have\nto put it back in so I very generously\njust gave that to you we're gonna start\nwith saying there's two states right\nthere's gonna be a low flow state and a\nhigh flow state I just read that in my\nhydrology blog so it must be good we set\nup the model as a Gaussian hmm and very\nnicely see we only have to set the\nnumber of components the number of\niterations and then I just want to fit\nit with my values and I want to get the\nhidden state so I run a model predict so\nlet's just run that and let's see what\ndo we even get for hidden states we get\na numpy array and what's the shape the\nshape is the same thing we put in and\nwhat does it look like well it's it's a\nbunch of numbers and actually they're\nall the same number which is not\nespecially interesting so far if I look\nat it sort of it looks like I have two\nvalues one has 72 count the other has 20\naccount that's not telling me that much\nwhat if I plot it so I'm plotting my\nhidden state note that a hidden state is\nnot something it necessarily make sense\nto plot in the sense of like the higher\nstate is not necessarily to higher value\nso just keep in mind this is just to\nshow sort of a label and a really quick\nand easy way it is the number on the\nhidden state means absolutely nothing\nso the fact that I sort of go from high\nto low doesn't mean anything by itself\njust looking at ones and zeros but what\ndo we notice here this verse is what I\nsaw above is it is it telling me\nanything I didn't know none especially\nright because all it said is oh yeah you\nwere in one state here and then it seems\nlike you took a nosedive and you're in\nanother state there's two states and it\nsort of agrees with me there so we're\ngonna talk a little bit about how to fix\nthis or what might be more interesting\nand informative but before we do that I\nwould say since we now realize we're\ngonna want to experiment a bit is I\nwould ask you to just take a couple of\nminutes and think how can we package\nthis API a little more conveniently so\ngo ahead and read up on the hmm learn\nGaussian hmm API and also think\nbasically I want you to turn this into a\nfunction so that I can just call it\nreally easily instead of having to sort\nof write this over and over here's one\nsolution I imagine folks had other ideas\nbut the main goal would be you'd want\nsomething that accepts both your values\nand also your states it could be\nconvenient to have that reshape in case\nyou are often passing in data that lacks\nthat second dimension right better to\njust get that stuff out of the way take\nthe mental tax off in this case I'm\ngonna have the same the same model right\nso the number of components which I now\nwant to make configurable I chose not to\nmake the number of iterations\nconfigurable but that's sort of up to\nyou to decide number of iterations tends\nto depend more on the quality of your\ndata rather than or the nature of your\ndata rather than the number of states to\nfirst order so this wouldn't be\nsomething you'd have to adjust that much\nfor the same data set we grab the hidden\nstates we also probably want to extract\nsome more information right just knowing\nthe state label by itself is not that\ninteresting I also want to know what the\nBOM Welch algorithm found to be sort of\nthe mean and standard deviation for the\nGaussian model that I'm developing right\nso in this case these these describe\nsort of what I expect to see coming out\nof my observable data and then so that's\nthe amuse and the Sigma's right are the\nthe Gaussian distributions for the Y\nit's given the underlying state and then\ntransmat is the transition matrix this\nis the thing that describes how likely\nare you to go from state I to state J I\nwant to see that as well because that's\ngoing to give me some information about\nwhat the underlying dynamics look like\nand does this match my theory of how\nRiver flow works so if I were to only do\nthat that would be alright but I\nactually also like personally I like to\nreorder the output because the output\ncomes out and sort of an arbitrary order\nright like as I said the hidden state\nlabels don't mean anything\nthey're just arbitrary labels they\ncontinue to be arbitrary labels no\nmatter what or what I like to do is at\nleast label them in sort of an order\nfrom like lowest to highest so lowest\nmean has the lowest hidden state label\nhighest mean has the highest hidden\nstate label and not just at least to me\noffer some way of saying oh that's a\nbigger one that's a smaller one but it\ndoesn't mean more than that so I have\nthis code to just re permute everything\nincluding the transition matrix and if I\ndo that we can now fit a method that\nwill give us the hidden states the Meuse\nthe Sigma is the transition matrix and\nthe model should we wish to do more with\nthe model so we run that again and then\nI won't leave this as an exercise I\nthink we'll just go through it but how\nmight we want to plot this well what\nthings do we want to plot we want to\nplot the real value and then we want to\nplot the hidden state in some way right\nI'd like to have these overlaid in some\nway this is a matter of personal\npreference some people will do things\nlike sort of color the background\naccording to the state you're in like I\nsaid I have this idiosyncratic way of I\nlike to just plot the state as a line\ngraph so long as I preserve in my mind\nthe fact that this does not mean\nanything except as an indicator of\ndifferent values right so it's not\nactually indicating anything about the\nsize of those values\nyes thank you\nwhich part this one yes right because I\nthought I would run that first thank you\nokay so now my states are in are in the\norder that I want them to be in right I\nlike just the highest mean to be the\nhighest labeled state again does not\nmean anything more than that it just\nmeans they're sort of an ordinal order\nso here I still got I have the same\nuninformative oh yeah you're in some\nsort of like higher flow regime and now\nyou're in some sort of lower level or\nlower flow regime but this hasn't taught\nme much so the next exercise for you\nguys now that we have this in the form\nwhere we can experiment and plot our\nresults easily I'd like you to try two\ndifferent ways of maybe improving on\nthis that I would recommend one of them\nis to cut off this sort of earlier set\nof time points which might not be that\ninformative or might just be two\nfundamentally different to be that\ninteresting because I'm not interested\nin an outcome that just says oh for\ntwenty years you were in one state and\nthen for 80 years you're in another\nstate not super interesting so can I cut\noff the region of interest and rerun the\nanalysis and then the second method is\nto think well maybe I'm not looking at\ntwo underlying States should I try it\nwith a different number of underlying\nstates and see what that looks like so\ngo ahead and take a few minutes to work\non those two options the first\npossibility right we can cut off the\nspecial region which to my mind means we\nlike this first state it seems to be so\ndifferent right it just takes over that\nfirst day and I think well maybe that\nwas some sort of extraordinary event I\nwant to rule that out I'm interested in\nsort of more typical dynamics of the\nriver so why don't I just cut off those\nfirst 2728 values and model that and if\nI do that I didn't put it into my fancy\nplotting function but I can I can see\nmore switching now and this is sort of\nmore what I would expect based on the\nvery little I know about hydrology right\nnow I'm not looking for some regime\nshift of like 20 years and then 80 years\nright that's that sounds like almost\nlike a geological event or something I'm\ninterested in sort of more every few\nyears El Nino type stuff so that's one\nway to do it but do I really need to do\nthat because that feels like\nthen maybe my model doesn't describe my\ndata very well if I have to sort of cut\noff whole periods and and maybe that's\nfine for my\npurposes but I also want to see well\nmaybe can this just be a little more\nrobust so what if we instead just add\nanother hidden state right what if I say\nwell maybe there was this extraordinary\nstate and then there's the normal two\nstate dynamics so can this accommodate\nthat so if I put in three states\nand plot that well this does seem to be\naccommodating this right I have I have\nmy one state that's sort of the initial\nextraordinary state I have another state\nand another state so I have my three\nstates I seem to have gone wrong\nsomewhere in my ordering of my states so\nI'll have to fix that bug\nduring our break ideally the first state\nwould sort of be up high and then it\nwould jump low and high and low so I\napologize for that but what we see is we\nsee more switching now right so we see\nthat this model is robust enough that it\ncan sort of describe a state that was\nsort of very long and very different and\nthen accommodate regime shifting as well\nand if I look at my muse what do I see I\nsee I know what I did sorry let me fix\nmy plotting if I can no sorry\nso I have I have three sort of means for\nmy y observable right two of them are\nlow eight one around 800 880 and then I\nhave this one around 1100 which is sort\nof this extraordinary regime and I can\nalso look at my transition matrix which\nis going to describe how one state\ntravels to another and what do I see\nhere if I look at this right so why is\nit three by three because I have three\nstates and the transition matrix\ndescribes the probability of\ntransitioning from any one of those\nthree states to any one of the other\nthree states interestingly here I can\nsee I have a very challenging data set\nbecause some of these transition\nprobabilities are zero right and in\nparticular the possibility of\ntransitioning from either of these top\ntwo states to the third state is 0 why\nis that because here's the third state\nand it's it only observes in one\ndirection right it only observes going\nfrom that top state down and sort of\nexiting that regime it doesn't observe\nan end\nto that regime so we would need to have\ndomain knowledge to know is that\nreasonable or if it's not reasonable is\nthere some sort of constraint on our\nfitting outer then we would want to put\nin to not permit things to go to zero so\nthose are sort of more advanced things\nyou might do on your own with domain\nknowledge but that's not something\nyou're going to get out of the box so\nI'll also keep in mind with all these\nmethods there are ways you can tweak\nthem to reflect your data dynamics but\notherwise as a general solution they're\ngonna fit the data they see so if it\nnever ever ever sees that event it can\nvery well go to zero if you don't\nconstrain it not to and maybe that makes\nsense so on the other hand you can say\nwell that's very flexible because what\nit shows is it can accommodate both sort\nof normal regime shifting and also\nextraordinary events and maybe that's\ninteresting and helpful to know unlike\nsay in a Rhema model or even a\nstructural time series which will\nstruggle more with that sort of sudden\nregime shift we can also see that from\nthere if we look at the first two rows\nand columns we can see sort of the\nprobability of switching between the\nsort of two more normal states and that\nsort of seems to flicker back and forth\nwith you know directionality favoring\none more than the other it's not 50/50\nand that's sort of an interesting\ninsight maybe if we are into hydrology\nwe might say oh yeah that reflects the\ndata and that's something I can pursue\nin my research and so on so we get some\ninsights here what I talked about this\nis just a local maximum not a global\nmaximum so you're not guaranteed to get\nthe same fit if you initialize\ndifferently and so on it's just one set\nof parameters you would want to run this\nsimulation many many times with\ndifferent starting conditions to really\nhave a better sense of how you're\ndescribing this data and arguably with\njust a hundred data points there's only\nso much you can get out of it anyway\nright and this is this is where some of\nthese modern methods they don't always\nwork as well as you might like with just\na hundred data points but it's a simple\nsimple toy example okay so we're gonna\ngo ahead and take a break we'll make\nthis I want people to have enough time\nto go get some snacks so I think we'll\nmake it like a 10-minute break\nit's the big snack break and there's\nalso an exercise should you wish to\nattempt it so I will quickly show you\nguys this when we get back and then we\nwill move on to machine learning methods\nso let's see it's 10 o'clock exactly so\nwe'll restart at 10 10\nand just to clarify the snacks are just\nwhere breakfast was so across the\ncourtyard sort of that way okay so we're\ngonna get started again first I'll just\nquickly walk us through the last\nexercise from the hidden Markov model\ncomponent and then we're gonna move on\nto an overview of machine learning for\nan hour and then last hour we'll be on\ndeep learning and I keep panicking when\nI see my East Coast I'm Stan so psycho\ncrap it's 11:00 already no okay okay so\njust one other function I wanted to make\nyou aware of is that you can sample from\na hidden Markov model once you have fit\nit and you don't even need to fit one\nyou can also feed in parameters and\ncreate a hidden Markov model with a\ncertain set of parameters so that can be\nuseful if you just want to see well what\nwould something look like and run many\niterations of it to test out a theory so\nin this case I sample from the model we\njust developed and then I can plot it\nand think well that that's what I would\nlook like where that that's what one\nsort of sampling of this would look like\nbut then if we were to refit it notice\nthat this looks very different from our\ndata right even though it's the same\nhypothesized underlying process so keep\nin mind that a process with a certain\nset of Meuse and sigmaz to describe your\ny sub t probabilities and your\ntransition matrix can still give you\ndata that looks very different so if we\nwere to then to refit we we see it like\na totally different model where the\nthree states are different as well and\nin this case we're actually seeing what\nit really looks like here's what\nhappened is that our two sort of normal\nstates were the ones that really got\nrepeated say sampled from repeatedly we\ndidn't really get to that third\nextraordinary state that characterized\nthe beginning of our flow model so in\nthis case we're seeing a transition\nmatrix where the probabilities don't\nlook at all like what we saw in our\nfirst case mainly because now it's\ntaking what was our two states out of\nour three states and it's remodeling\njust those two states as three so here's\nan example of it's not necessarily wrong\nor a bad thing but just to keep in mind\nthat the same hidden Markov model that\ncan describe one set of data can look\nvery different with a different set of\ndata\nsuch as if one of your states is not\neven there right so if you give it too\nmany states or too few states relative\nto ground truth you can end up with\nthings that don't make a great deal of\nsense there aren't very insightful so\njust keep that in mind when you're\nfitting these models especially if you\nhave these sort of outlier states that\nyou don't see very often okay\nso any questions or comments on hidden\nMarkov models before we switch gears\nokay so those are sort of your overview\nof state space models\nso as a recap\nonce I load this as a recap for our\nstate space models they can be really\ngood for when we have some sort of\nunderlying sense of what we think the\ndynamics of the system are they can sort\nof add evidence to those dynamics or at\nthe same time they can take away\nevidence right they can tend to refute\nwhat we think for example we saw that\nhaving a trend in the earth temperature\ndata actually wasn't that interesting\nand more just a local level model\nsufficed and it also helped us discover\nseasonality that we had been too lazy to\ndiscover on our own and then with our\nhidden Markov model we saw this model\ncan accommodate things like say an\nextraordinary state change but in such\ncases we will run into problems with\nsimulation so there are problems these\nmethods need to be treated carefully and\nwith knowledge of how they work you\ncan't just roll them out out of the box\nbut they do offer a lot of things that\nsave traditional statistical models\ndon't offer such as a way to sort of\nbreak down the system and try to get\ninside right so even though we only have\na univariate measurement we can actually\nget at very complex dynamics underneath\nthat univariate method and also just to\npoint out all of these methods can also\naccommodate multivariate time series\ndata as well so if you have parallel\ntemperature measurements and you want to\nincorporate them or parallel\nmeasurements and a hidden Markov process\nthat are all indicative of the same\nunderlying state or maybe a multivariate\nstate of some kind that can all be\naccommodated as well with standard\nsoftware and then somebody was asking me\nduring the break well what about you\nknow your underlying restrictions such\nas I mentioned you might want to have\nyou might want to have a transition\nmatrix that does not permit a zero\ntransition probability because you know\nthat's just not true\nin your system you can build in things\nthat in my experience most of the time\nthat will require hacking the source\ncode or writing your own source code the\ngood news is that these are open source\ntools so you have a lot of inspiration\nslash code you can steal from to like\nmake your own thing and the other good\nnews is that the underlying algorithms\nsuch as the Viterbi algorithm or the\nBrown Welch algorithm I think they're\ncomplicated in the sense that there are\nmany many indices but they're not rocket\nscience in the sense of it's all fairly\nintuitive and easy to reason about so it\nis fairly straightforward to hack this\nonce you've sort of developed that sense\nof how it's working okay so now we're\ngoing to talk about machine learning for\ntime series switching emphases a little\nbit but a different way of understanding\nour data or processing our data ok so\nthe first thing we need to think about\nwhen we do machine learning for time\nseries is the fact that I'm not aware of\ncall my attention to if you can think of\none a machine learning method that's\nreally sort of developed for time series\nand my experience when you apply machine\nlearning to time series you are applying\nmore general methods and finding a way\nto make them reflect or accept time\nseries data as opposed to the models\nwe've just been looking at which were\nbuilt from their core around the idea of\na time series right so traditional\nmodels like ARIMA but also models like\nhidden Markov model and structural time\nseries at their core and vision sort of\ntemporarily ordered data on the other\nhand when we get to things like decision\ntrees those do not fundamentally build\nthe model around this idea of temporal\nnotions right actually that is\ncompletely absent from the model so if\nwe are going to use models like that we\nneed to provide a way of translating\nthis temporal data into something that\nmakes sense and can be accepted as an\ninput to that algorithm but also makes\nsense right so we don't want to get\noverly focused on the inputs we also\nwant to make sure that these are\nsensible inputs and we're not just\nthrowing data at it so that's why for\nexample we would never just say well\nlet's make our time series the inputs to\nour decision tree right let's have a\ndecision tree and maybe I have a really\nshort time series it's you know it's\njust 10 points it's from time one to\ntime ten so I have plenty of machine\nlearning models that take more than 10\ninputs so why don't I just put the\ninputs\nand see what happens it's very very rare\nthat that's going to get you anything\nlike what you want if you have that sort\nof time series data arguably it's so\npredictable and so clean and so easy\nthat you don't need machine learning\nanyway you could probably just eyeball\nit right so if you have that kind of\ndata with so little noise and such clear\nstructure your there's no reason to be\ndoing that so any case where that would\nwork it's not a case where you would\nneed to do it on the other hand what are\nsome things that could be difficult\nright your time series can be different\nlengths and also things don't sort of\ngot to say even a really simple example\nmaybe we always have many time series of\na volcano erupting so we know there's\nalways interruptions somewhere in the\ndata that's how we've defined this data\nset that eruption won't occur at the\nexact same moment though right in one\ntime series it might be at time step\nthree at another it might be a time step\n13 so how could you expect a machine\nlearning algorithm with no notion of\ntime to sort of understand that this\neruption can occur at any of these times\nbut that once it does we expect whatever\ncollateral damage results or whatever\nshows up in the time series so again\nmain message you can't just feed your\ntime series to a machine learning\nnetwork I have never seen a case where\nthat makes sense so though if someone\nhas one that would be great to know\nabout so what we do instead is we have\nto do feature generation right and this\nis no different you have to do that in\ncross-sectional data too as well\nsometimes you can generate you know data\noff of your raw data but with time\nseries it's basically required and so\nwhat sorts of features do you want right\nwhat are we doing when we do feature\ngeneration we're finding away to take\nsomething like this curve and just have\na few numbers that just describe what it\nlooks like and that's no different from\nany other time we'd want to do feature\ngeneration it's no different from\nwhenever we want to summarize a complex\nsituation with a couple of words or a\ncouple of numbers or a mix and some\nheterogeneous data so in this case we're\nlooking at a time series and what are we\ngoing to use well we might use the\nmaximum value the minimum value the\nmedian the mean the number of peaks as\nyou may be noticing these already if you\nhave sufficiently long time series and\nnow remember we're sort of each time\nseries is almost like its own data point\nnow so for each time series you're going\nto need to go through each\ntimestep in that time series and compute\nthese things and if you think about that\nthat actually gets to be quite\ncomputationally taxing right because to\neven just look at the mean or the min or\nthe max I need to go through every data\npoint so if I have really long time\nseries the you know order N or order N\nsquared gets to be quite a drag and then\nif I'm doing things like number of peeps\nwell that's this is one of these things\nwhere you know a human eyeball is great\nat this a deep learning network of the\nright kind might be great at this but if\nI'm just trying to write simple Python\nroutines and especially efficient ones I\ndon't know of a very quick and fast and\nsuper reliable way to identify these\nPeaks right so that's a sort of thing\nthat would be really helpful if we could\nput it into a feature but we need to do\nthat sort of cost-benefit analysis of\nhow well for my data can I analyze this\nyou know and also the computational cost\nso that's always something you're\nthinking about with feature generation\nis just how expensive it can be and\nwhether it's worth it I also want to\npoint out that times theory is well\nstudied very complicated domain\ndependent sort of thing as far as what\nturns out to be useful when let's go\nback so this set I just wanted to\nhighlight it's one of many canonical\nsets it's called the catch-22 canonical\nset so this is sort of an ongoing really\ninteresting project it's part of this\nproject called the highly comparative\ntime series analysis project and what\nthey are doing is collecting every kind\nof time series data they can get their\nhands on at all frequencies from\nwhatever domains are available and you\ncan donate datasets and the idea is to\nuse machine learning itself to sort of\nidentify what features perform well\nacross many kinds of datasets for\nclassification and prediction tasks as\nappropriate and so this is the list of\n22 features that these researchers came\nup with there's also sort of an\nalternative version where it's down to\n17 features rather than 22 but anyway\nthe point is is if you're interested in\nsort of what's going on sort of\ncutting-edge feature generation\nresearch or what people might use out of\nthe box you could have something like\nthis now on the other hand I might say\n22 is a really high number so sure if\nyou're doing sort of high-throughput\nfeature generation of many datasets just\nto sort of try to develop insights about\nwhat kind of features apply it's\nuniversally great on the other hand if\nyou have a very specific youth case like\nan EKG or some kind of seismological\nmeasurement over time in LA when they\nhave a massive earthquake or whatever\nthis should not be your first go to\nright so this is for a very general time\nseries but almost always if you have\ndomain knowledge or just have looked at\nyour data you can do better than just a\ngeneric feature set but it's there if\nyou need it it's also as I was just\nmentioning very disciplined specific and\nsort of the more you know about the\nunderlying processes the more meaningful\nyou can identify features so on the Left\nI have an EKG again I love this example\nbecause it's periodic it has all sorts\nof interesting features so these are\nsorts of things for example that doctors\nsee in their medical textbooks and this\nis how they learn to read this time\nseries right it's actually a time series\nand part of what they're doing when they\ndo Diagnostics it's to do future\ngeneration right effectively what\nthey're doing is identifying things like\noh the tall peak how tall is that and\nyou know how long before the first peak\nand the tall peak and what's the\ntemporal distance between like that last\nlittle dip and the first peak and so on\nand that's part of how they diagnose\nheart illnesses right so then they know\nwhat's relevant and so if you were\nworking on data like this you would\nprobably want to work closely with a\nphysician rather than you know\ndeveloping all of these from scratch\nright that would that would sort of be a\nwaste of time and then on the right I\nhave a case of astronomical data that's\nanother discipline where time series\nfeatures are actually very well\ndeveloped they're whole Python packages\njust on astronomical time series feature\ngenerations so if that's something you\ndo that's where you'd want to start out\nlooking so you should always start with\nyour discipline specific stuff once you\nhave your time series features and we're\ngoing to compute some features what can\nyou do with them well one thing you can\ndo is classify your time series data so\nhere's an example from the time series\nclassification database they show sort\nof twenty-four different classes of time\nseries data\nand so one thing you can think of is if\nI have a whole sample of different time\nseries how would I separate them out\nlike this and so we're gonna be looking\nat two ways of thinking about that we're\ngonna use random forests and we're gonna\nuse gradient boosted trees but just in\ncase anyone's not familiar what is a\ndecision tree a decision tree looks like\nthis right a decision tree is something\nlike you feed in your features of your\ndata and you look at one feature at a\ntime to sort of make decisions branching\non this decision tree branch down branch\ndown branch down and at the end after\nyou've made a series of decisions that\nare sort of binary yes/no about various\nfeatures you get to either a\nclassification label or a regression\nprediction right so these are why our\ntree is helpful in general they can\ncapture nonlinear dynamics they are also\noften fairly resistant sort of\nextraneous garbage inputs so you can\nalso be a little bit less judicious\nabout sort of feeding everything in and\nsometimes you won't be too punished but\nof course no one's going to actively\nadvocate for that it's just good to\nremember and they can also give you give\nyou a sense you can read them right and\nstart getting sort of a sense of how\nthings work so I imagine even a doctor\nwhen they're looking at an EKG that's\nsort of how they think right that's how\nthey're trained even they have these\nheuristic decision trees and that's just\nlike our decision trees and machine\nlearning so the two techniques we're\ngoing to use that are based on trees is\none the random forest right where we\njust build a whole bunch of trees each\none sort of built off a subset of\nfeatures and a subset of data and we do\nmajority voting and the winner is you\nknow the majority winner and we're also\ngoing to use XG boost\nI'm guessing folks have at least heard\nof XG boost or gradient boosted trees\nthe idea is that with a random forest\nyou build however many trees you're\ngonna build and they're all in parallel\nwith some sort of random subset of the\ndata and the features and you build off\nof that gradient boosted trees are built\nsequentially so first you build your\nbest decision tree with whatever your\nparameters are your second decision tree\nis then built off the errors of the\nfirst decision tree so each decision\ntree is sort of trying to correct the\nerrors of the previous one with the idea\nthat you're going to add the inputs\nright so it's not sort of a majority\nconsensus is that everyone contributes\nthere a little bit you add up the trees\nand you're good\nthere's no reason at least in my mind\nwhen I\nabout why this should work so well that\nit does for time series specific data\nbut empirically in the last sort of\nthree years maybe even a little bit more\nit's become clear that XG boost is very\nsuccessful at time series tasks much\nmore so than sort of earlier\napplications of machine learning to time\nseries so that's why I highlight it it's\njust empirically true when you look at\ncompetitions or industry use cases also\nclustering this is the other thing we're\ngonna do in this notebook so again I'm\nguessing folks are familiar with the\nconcept but just in case you're not\ngenerally speaking what is clustering\nlook like it looks like you you look at\nsort of the feature space of your data\nbe a time series data or cross-sectional\ndata and you hope somehow that the\nfeatures sort of cluster in this way\nthat indicate they're sort of specific\ntypes of data points and you hope that\nthat's somehow means something more\nfundamental about what has produced that\ndata that's a beautiful picture does\nanyone actually see that picture at work\nthough I mean I I want your job if you\ndo right that's that's not really what\nyou see well you usually see is\nsomething on the right and you're sort\nof squinting at it and running your\nanalysis and that also tends to be true\nfor time series data so we also have to\nkeep in mind the messiness of real world\ndata this I think this picture is a\nsomewhat fair version of a really nice\ntime series right so if we were to\ncluster this is what it looks like in\nthe time series case right like we can\nsee that those top two curves twelve and\neleven they look similar right they've\nboth got three bumps they're sort of in\nthe same space positionally they seem to\nhave approximately the same through a\nvolatility over time if we look at ten\nand nine the gray that's an even better\ncase of yeah they look pretty similar\nbut you know some of them even have\nfeatures that the other one does in so\nagain it gets a little more complicated\nand you start thinking well what's good\nenough what isn't good enough if we look\nat the green seven eight seven eight and\nsix those also are another good case\nwhere you start wondering you know how\ngood is good enough that might be a\nlittle bit more like real life where you\nknow there's there's there's some sort\nof commonality you don't even know if\nyou could write code to identify what\nthat commonality is so you get you see\nwhere it can be difficult to generate\nfeatures especially if we want to have\nfeatures that are robust for all of\nthese time series\nthe same time right because of course\nall of these are jumbled together we\nneed to identify features that would\ndistinguish among these not really easy\nright like number of Peaks well number\nof Peaks my blue versus my brown they\nmight end up being the same or even my\nbrown versus my grey at the top you know\ndepending on what sort of peak finding\ncode I write the grey and the brown\ncould both have three peaks and come out\nbeing very similar and what does that\nmean so you have to have some domain\nknowledge again and also sort of look at\nyour data so time series clustering is\nsurprisingly difficult right\nconceptually I think it can also be\ndifficult sometimes because we're we\nmight be looking at it via the features\nbut ultimately I think what we most want\nis we want to look at the original raw\nseries and make sure that looks similar\nright we we have this intuitive sense of\nif I can eyeball it and it looks similar\nthat's what I want to come up together\nin a cluster it's very computationally\ncostly as I mentioned the feature\ngeneration itself is very\ncomputationally costly because you have\nto go through the whole time series to\ncome up with a few features and then\nthat's just one data point because now\nremember again your time series a whole\ntime series like one of those curves is\njust one data point right so if each of\nthese curves is actually a thousand\nmeasurements that's you know thousands\nand thousands to only produce actually\ntwelve inputs right twelve samples so\nkeep that in mind as opposed to\ncross-sectional data one pitfall I want\nto point out is you might say well\nforget feature generation why don't I\njust measure the distance between curves\nright and I could do something like use\nEuclidean distance you almost never want\nto use euclidean distance and we're\nactually going to cover a really great\nexample of why and I'll introduce\nanother distance metric that works a lot\nbetter and then time series clustering\nis used across many disciplines so\nthat's good to be aware of as I\nmentioned medicine right looking at an\nEKG also in finance especially sort of\n1980s style finance people used to plot\nthings and then say oh does this month\nlook like that month from the 60s and\nmaybe the same thing is going to happen\nwith that kodak stock or whatever that\ndefinitely happens a lot less now but it\nwas a tried-and-true method back in the\nday chemistry also like I talked about\nthe NMR spectra right it's effectively\nwhen chemistry professors and there were\ngrad students out and tell them to go\ntake those spectra that's what they're\ndoing right does it look more like this\none or more like that one and how can we\ncluster and and so on there are more\nexamples as well\nhere's a practical example this is I\ndon't even know this person but I've\nalways admired this blog post they did\nso basically what they did is they\nlooked at Washington DC they looked at\nthe bike used per hour daily for all\ndays of the week for all bike rental\nspots so I don't know I'm guessing\nAustin has something like this too but\nyou know you sort of dock the bike\nyou're a member you take the bike to\nanother dock and they looked at the\ntraffic for one bikes were being taken\nout of different spots and they\nbasically identified three kinds of\nstations right so there was the one that\nwas busy both at the beginning of the\nday and the end there was one that was\nbusy only at the end of the day and\nthere was one that was busy only at the\nbeginning of the day and what they\nconcluded was this sort of showed some\nkind of pattern of people's movements\nboth to go to work and also sort of\nwhere they go after work because\napparently often they don't go straight\nhome or we would have only the double\nbump situation so there's plenty of\nexamples of fairly clean time-series\nclustering in the wild I think that's\nespecially true for sort of human\nbehavior because we have these it turns\nout really boring patterns like we go to\nwork we come home from work it's\nactually really boring if you look at\nhuman data a little bit noisier if you\nlook at things like astronomical data or\nfinancial time series and that sort of\nthing but sometimes you get really\nbeautiful clustering and then finally\nyes okay finally last overview so I just\nwant to talk about the distance metric\nwe are going to look at for time series\nwhich is called dynamic time warping\nthis is going to be an alternative\nsolution to that clustering problem\nright so I highlighted how you might\ngenerate features and features might\nwork well if you're looking at sort of\none particular class of time series but\nit can be very difficult to generate\nfeatures that are robust across many\ndifferent classes of time series within\nthe same data set so another option that\npeople have come up with that's quite\ncomputationally taxing but works pretty\nwell is called dynamic time warping so\nwhat you can see here\nimagine these solid curves are my two\ntime series I'm trying to line up\nbasically what it does is it tries to\nfind it tries to keep them parallel but\nfind a way to maximize we have the\ngoodness of fit so you can almost\nand the nice thing about this is that\nthis tends to correspond pretty well\nwith what we do visually without having\nto actually do the image analysis right\nso this looks at it and just like we can\nsort of squint and we can see the two\nthe two peaks at the beginning and in\nour brains we want to match them up that\ndoes that computationally so we're going\nto revisit that and see what that looks\nlike so that's an overview of what we're\ngonna do in the notebook so now let's go\nto notebook number three so we're gonna\ngo out of the state space into the\nmachine learning and we'll start with\ntrees for clustering or classification\nand prediction okay so the data set\nwe're going to start with is a data set\nthat's available via the cesium library\nand it is an EEG data set so that's for\nthe brain rather than the heart and I'm\nsure somebody in this room knows a ton\nabout it so feel free to pipe up if you\nhave any sort of input always\ninteresting alright so we visualized\nthis and we see that there we can even\nsort of qualitatively see a difference\nbetween three samples we can see at the\nbottom there sort of one highly volatile\nsample this is sort of all over the\nplace and then the top two are sort of\nless volatile it's not just the\nvolatility though if we look we also\nspot just differences in the values\nright so the top plot looks like it\nmight be sort of Gaussian around zero\nand it has sort of a range of like minus\none hundred to one hundred the second\nplot if we were to just look at it\nvisually it doesn't look that different\nfrom the top plot but if we look at the\nvalues this one only ranges from zero to\nminus a hundred right so the actual\nvalues in this case seem to be some kind\nof tip-off of the classes and then again\nif we look at the last one this might be\nsort of Gaussian and hard to know from\nthis but we see the range is also quite\ndifferent 500 to minus 500 right so\nalready we can spot that we'd want to\nfind ways to sort of preserve\ninformation about the value\nif we were thinking about generating\nfeatures as far as what kind of data\nwe're looking at we're looking at numpy\narray is served within a dictionary and\nthey're giving us times measurements and\nclasses so that's something to be aware\nof the measurements themselves are a\nlist and it looks like we've got 500\nsamples and then if we look at just one\nof those samples it's four thousand and\nninety seven points long so you already\nget a sense of oh my goodness so if I\nhave five hundred time series and each\nof them has four thousand and ninety\nseven points you see already how it gets\nto be a little bit computationally\ntaxing just to go through all of that so\nnext if we generate features feel free\nto go ahead and hit that button if you\nwant at least on my rinky-dink laptop\nthis takes a really long time so I'm\njust gonna load it up from prepared data\nso that's the code I did right and what\ndid I what did I extract I extracted the\namplitude the percent that's beyond one\nstandard deviation right so a measure of\nlike how many outliers we have and how\nspread out they are the percent close to\nthe media and write some sort of measure\nof how tightly is hewing to those\ncentral values this skew and the\ndistribution and also the max slope\nwhich is sort of how much it jumped up\nor down absolute value from one data\npoint to another how did I find these\nfeatures well I just looked I looked at\nmy data a little bit saw what will fit\nreasonably well and then I looked at the\ncaesium documentation and I generated\nthose so here you can see if you did\njust load it up from the CSV this is\nwhat you see and what do we see we see\nsome sort of reasonable variation and if\nwe look at the shape what is the shape\nnow it's just 500 by 6 and actually one\nof those is just the channel so it's not\neven relevant but I've digested 5,500 x\n4,000 points down to 500 by 6 points\nright so if I'm in a rinky-dink laptop\nsituation in particular or if I'm not\nbut I have some enormous time series\ndatabase and still need to compress it\ndown this is a way to do that right I've\nreally shrunk my data down we're gonna\nskip this exercise of validating slash\ncalculating these features but I just\nwant\npoint out that you care right so cesium\ncesium is this feature generation\nlibrary it is not the only one there are\nplenty of Python feature generation\nlibraries some of them are\ngeneral-purpose like I talked about that\ncatch-22 dataset they have this\ngeneral-purpose library and some of them\nare specialized such as I mentioned\nthere are astronomical time series\nfeature generation libraries it's good\nto know these libraries are available\nespecially for things like these\nfeatures I can absolutely write out the\ncode for these features which I did here\nbut it's a pain in the butt right for\nsort of standard things like calculating\nthe skew or calculating the max loop\nwhy should I hand code those alright so\nespecially in the exploratory component\nI might want to just farm that out to an\nexisting library so you can check on\nyour own time that indeed the features\nwe calculate match the features that are\nproduced by the algorithm I just check\nfor one data point and we can also sort\nof think okay well are these meaningful\nso one thing I did for example is did a\nhistogram of the amplitude feature by\nclass and it looks like at least for one\nclass right one class at least\ndistinguishes itself compared to the\nother so okay there's at least some\npreliminary evidence that some of these\nfeatures will help to distinguish\nclasses which is what I want to do okay\nso these are these are some other\nexamples of plotting histograms to again\ncheck are these features I generated\nsomewhat useful and in fact you can go\nthrough quite a bit of feature\nimportance analysis by the time you're\ndoing that for time series it's just\nlike how you would do it for\ncross-sectional data right do the\nfeatures I'm producing tend in some way\nto correlate with the outcome I'm\ninterested in B that a classifier or a\nforecast or whatever it is so you\ndefinitely would want to go through and\ncheck that they're sensible you also\nultimately if you have a well-developed\nset of features you probably at some\npoint would want to code them yourself\nright so I mentioned cesium or other\nfeature generation libraries are useful\nin the sense of why should I code up the\nsame things repeatedly there's a few\nsolutions to that one is if you do this\nenough you'll just have your own library\nwhich is even better because you really\nknow it line by line and you know how it\nworks\nultimately if you have a really well\ndeveloped set of features you also don't\nwant to\nout-of-the-box solution because that's\ncoded to be very general if I on the\nother hand know the same three features\nI always calculate one thing to keep in\nmind is sometimes features can be\ncalculated together to cut down on\ncomputational cost right so if I'm\nlooking at min and Max there's no reason\nto do one pass through the data to find\nthe max and then another pass through\nthe data to find the men right so you\nmight want to group all things that go\nthrough the whole data in the same way\nand do them all together that's not\nsomething a future generation library\nwill do for you I have not yet seen an\nexample though I love if anyone has one\nof somebody sort of coding things up to\ntake advantage of those that would be a\ngreat contribution to the open source\ncommunity if anyone wants to but in the\nmeantime be aware of that because even\nin this really tiny data set I found I\nhad a weak quite a while to generate the\nfeatures right so this can blow up very\nquickly and you don't want to spend\nwhole afternoons at work sort of sitting\nthere waiting for features that may not\neven be very useful okay so that's the\nfuture generation it can be quite\nstraightforward should be\ndomain-specific but in this case we sort\nof went with a few common-sense features\nand we're going to see how they work so\nwe prepare a training and a test set and\nlet's roll out a random forest\nclassifier we're going to use ten\nestimators a max depth of three set a\nseed for no particular reason we're just\nsaying okay how does this random forest\ndo so if we fit it and score it on our\ntraining data we've got like a 62\npercent sixty-three percent accuracy the\ngood news is our test data is not much\nlower so it doesn't seem like we over\nfit it seems like we have a reasonably\nreasonably generalized model and if we\nlook sort of at what came back it also\nlooks reasonably well distributed so we\nhave five classes they're all sort of\ndistributed in our underlying test data\nso that's something interesting to know\nwhen we're thinking about our possible\nnull model right so when you think about\nis 62% a good accuracy of course you\nwant to think about well what would even\na really dumb model do right so if we\nhave five five classes a dumb model\nwould not be doing 60% accuracy right so\nis just some sort of sanity test that\nour data is indeed doing something so\nthat's our random forest right it's just\nto point out okay you can put some\nfeature\nand get a classification that is far\nbetter than a random classification with\njust five features so I boil it down\nessentially 4,100 data points to five\ndata points without even much\nexploration and I could already do\nsomething right so time series\nclassification is not that hard to get\nsome kind of result that is better than\nrandom and that really boils a whole\ndataset down to just a few numbers and\nit depends of course on the quality of\nthe data and what you're looking at okay\nso now we're going to run through the\nexact same analysis just with XG boost\ninstead right so now I'm going to use an\nextra boost classifier I'm going to\nagain use ten estimators and max depth\nof three I'm gonna fit that and I'm\ngonna score that and what I see now is I\nhave improved right away on my random\nforest model this is as a rule of thumb\nquite expected with time series so as I\nwas mentioning extra boost and gradient\nboosted trees tend to do extremely well\non time series data relative to what had\nbeen there before so that's something to\nkeep in mind although of course here we\ndo see a little bit of evidence of\noverfitting so I might want to prune\nthat back and as a reminder what you get\nwith extra boost that you don't get\nautomatically with a random forest is\nsome measure of future importance so if\nyou were iterating and thinking about\nokay well what features turned out to be\nuseful especially if you don't want to\ngo through all the time series and plot\nthem all if you don't have time for that\nor if the time series are just so noisy\nthat the visual is not giving you much\ninformation you can use something like\nfeature importance as one way of\nthinking about what kinds of features\nare useful and then you might look and\nsay okay so feature 0 is useful and what\nwas feature 0 anyway if I go all the way\nback up what was that did it so that was\nthe amplitude right and that was\nactually one of the things that jumped\nout at us so if it says amplitude is\nuseful right these indicators of sort of\nthe numerical range is useful one thing\nI might do to improve this model is say\nokay well what other sorts of features\nprovide similar complimentary\ninformation right so maybe in addition\nto the amplitude maybe I need to add the\nmean or something like that so provide\nmore information so you can use this as\na way of sort of learning also for this\nkind of data what is useful\nso that's classification we also want to\nthink about forecasting though right so\nin this case we're going to take a look\nat the air passengers data set which I\nhighlighted in a slide at the beginning\nof the lecture so we're gonna load the\nair passengers data set take a look at\nwhat this looks like so here we can see\nwe've got monthly data going back to\n1949 set our index and take a look at\nwhat that looks like let's plot it and\nso here we can see the plot there's\ndefinitely some seasonal component and\nthere's also some trend component it's\ngoing up now if we were doing say ARIMA\nmodeling we would definitely need to\nmake this stationary that's not a per se\nrequirement with something like machine\nlearning but it is generally a good idea\nright so some of the same considerations\nabout what your data looks like for\ntraditional statistical models are still\nthings you can do to make your data\ncleaner and easier to digest for a\nmachine learning model so that's thing\nabout log transforming this and also\ndipping it so if we do that we get a\ntime series that is more uniform in its\nvariants and values right which has to\nbe a good thing because basically what\nwe're doing is making all the sub series\nfrom this time series more comparable\nright because what we're going to do\nmany different options and many\ndifferent samples by having a sliding\nwindow right so this could be one sample\nthis could be another sample this could\nbe another sample because in this case I\nhave one baseline time series I'm gonna\nconvert that into many samples for my\nmachine learning algorithm so here's\nwhere we see another difference compared\nto a statistical model or a state space\nmodel in the statistical or state space\nmodel the model for inputting your data\nis you have one long time series and you\nthrow that in on the other hand with\nmachine learning the model has nothing\nto do with being aware of this temporal\ncomponent of the data so instead we take\ndifferent time windows we chop up our\ntime series and those become different\nsamples so that becomes a way to use one\ntime series and just model that one time\nseries for machine learning so why is\nthis different from what we just did my\nclassification because we\ntrying to forecast and we're trying to\nforecast one series in particular if we\nhad many time series and that would be a\ndifferent consideration do we want each\ntime series to just be one sample or\neven in that case do we want to chop up\neach of those time series into sub\nsamples and use them all but in any case\nwhat you want to make sure you do since\nyou're going to be doing a sliding\nwindow across here is you would like to\nmake all these data points as comparable\nto one another as possible because if\nyou think about a machine learning\nperspective right the the machine\nlearning algorithm just sort of sees a\nslice here and a slice here and it has\nno way of sort of knowing which dynamics\nshould be carried around forward or\nwhich dynamics are unique to a\nparticular aspect whereas if you give it\nthis they're all clearly more comparable\nto one another thanks to those\ntransformations and then you would just\nneed to make sure to transform back at\nthe end when you wanted to check your\npredictions so we're going to have a\ntime series that is the diff of the log\nof these values and now there's an\nexercise I'm going to ask you to think\nabout now that we have one time series\nand we have it in a form where sort of\nall components of the full range of time\nlook very similar how can we convert\nthat to many samples so take a couple of\nminutes and think about how you would\nchop this up to create different sub\ntime series as samples okay so let's\ntake a look at this together\nwhat I decided to do and there's no\nreason that this is especially great or\nanything but I decided to break it up\ninto twelve months so to sort of look at\na year at a time a slice now arguably\nthat's not so great because you can't\nreally capture the seasonality if you\njust have one sample of the season but I\nsaid what the heck that's what I'm gonna\ndo so um twelve steps I think what\ntwelve steps does give you the\nopportunity to do those maybe at least\nbe able to pinpoint where and where in\nthe seasonal cycle you are when it's\nlooking at many many examples whereas if\nyou have too few months you probably\ncan't spot that at all so that was part\nof the rationale and then what I did was\nI wanted first to just give it I pre\nallocated this array of Val so if we\nlook at this it's a hundred and forty\nthree by twelve so basically I've just\nstacked the time series twelve times\nbecause I'm gonna want twelve time steps\nand then what I do is I'm just gonna lag\nit and I'm sure there's a more elegant\nway to do this I used a for loop just\nyou know my Genki solution and I don't\nhave to think too hard about it and\nbasically I just for each column I shift\nit up by however many lags I want right\nbecause I want the first value to be 12\nmonths in the past but relative to the\nlast value because I want to move\nforward in time so if I do this then I\nget this valve so now I have only 132 by\n12 so why did I go from a hundred and\nforty three samples to 132 samples well\nI lost the samples where I don't have a\nfull 12 months right so at the very\nbeginning of the time series I don't\nhave those even of 12 months but I have\nto go 12 months into the time series to\neven have one sub sample of a time\nseries so if I look at that I can then\nlook at say the last and steps of my\ntime series right so that's that's this\ntime series I depicted up above right so\nmy last my last twelve steps are these\nlast values right the twelve last values\nof the global time series and that\nbecomes the last sample now that I have\nbroken my time series into individual\nwindows right so vallis minus one is\njust my last sample that I'm going to in\nsome way process for a machine learning\nalgorithm and notice these are the same\nvalues right so I basically I converted\nthis box of say the last twelve samples\nthat's now sort of one sample time\nseries one thing I hope you noticed is\nthat I could actually have more samples\nright now I only have a hundred and\nthirty-two what would be some ways to\nhave more or at least I can think of one\nway let's see so two they do overlap so\nif you look see I have minus one six\nseven - oh nine and then minus one six\nseven - oh nine here but actually that's\na fantastic point and when I do my deep\nlearning example that was like my trick\nthere I was that we don't have\noverlapping and we could\nso if I have chopped it up to have\nnon-overlapping time windows that's kind\nof silly if I think I want more time\nseries I should just have a sliding\nwindow which is actually what I've done\nin this case the way it was coded up\njust by lagging one at a time rather\nthan chopping them up whereas if I had\ndone say a reshape then I certainly\nwouldn't have had overlap and I would\nhave had to put it put that back in so I\ndo have I don't I do have overlap\nalready so I'm using each point as much\nas I can I guess I think I want to point\nout is if I reduce my number of steps I\ncould get a couple of more time points\nout of there right so if I had only six\nsteps instead of twelve I would get six\nmore sequences most of the time that\nwon't matter maybe in such a small set\nit might matter so that's something to\nthink about another thing you could\nthink about less used in machine\nlearning compared to deep learning but\nyou could think about doing some data\naugmentation if you have some domain\nknowledge to think about ways to augment\nyour data in a way that's realistic okay\nso that's how I prepare my data so I've\njust created shorter time series I\nhaven't actually created features yet\nright so with forecasting we have this\nadditional step of first we take our\nlong series and we break it up into\nsmaller series I still need to feature\neyes right I still need to do the same\nthing I did earlier for the\nclassification so I'm going to do that\nhere so I convert my measures to a list\nbecause that's what cesium expects right\nso I've got my list of measures and each\nmeasure is this 11 times to 11 times\nsteps long and then I'm gonna feature\neyes this one I'm gonna run through just\nso you guys can see this one is small\nenough I can actually do it but as you\ncan see at least on my rinky-dink laptop\nit still takes a while right so even\nthis really tiny data set that should be\ninstant but it isn't in part because\nthis is not a very general library and\nin part because time series feature\ngeneration is just that slow so if I\nlook at these columns what have I got I\nthink these are the same features I used\nabove so sort of a generic set of\nfeatures if I look at the histogram of\nthese let's see oh well I see it's sort\nof multimodal I'm like wow that that's\ncool maybe there's these underlying\npopulations I can get all excited about\nsome possible structure\nsimilarly if I look at percent amplitude\nI see something that might potentially\nbe multimodal so that seems like maybe\nit could be good when I'm going to use a\ntree to do some kind of regression task\nI like multimodal it suggests there's\nsome sort of meaningful data underneath\nso I'm gonna run now I'm gonna run\ninstead of an XG boost classifier I'm\ngonna run a regresar so it's actually\nthe same process now that I have\nconverted my inputs to features it's the\nsame except now I'm looking for a\nnumerical output to forecast a value and\nmy Y instead of being a label is going\nto be the forecasted value so if I do\nthat and I run my model oh well my RMS\nhe is dropping that's great on the other\nhand our MSE can be hard to digest right\nwhat does that what does that really\nmean\nso I can do with a few things I can do a\nscatter plot of my test data notice I\nhave very little test data because this\nis a very small data set you never\nreally use machine learning on such a\ntiny data set right so keep in mind and\nthere you know your real-world\napplication you would have more points\nbut that also means you could\npotentially be waiting many hours to\ngenerate your feature so obviously we're\nnot doing that here but do keep in mind\nyour future generation will just sort of\nblow up so if we look at our testing and\nour even our training doesn't look\nespecially good here this is really bad\nright I was just telling you guys how\namazing XG boost is and how it has\nreally contributed to machine learning\nfor time series I even have a negative\ncorrelation between my prediction and my\nactual value that's really bad right\nthat means I'm sort of actively doing\nworse even than chance I would do better\nto just sort of predict 0 always rather\nthan run my model I certainly don't want\nto tell my boss this so exercise for the\nreader although you can scroll down and\nsee the answer but just think for a\nminute what what ones so terribly wrong\nhere is this remember this is this is\nactually a really easy data set right I\nshowed you at the beginning of this\nwhole tutorial the air passengers data\nand traditional ARIMA did great so why\ncan't I do anything so certainly\ntransforming makes it harder right and\nthat's actually it's a frequent sort of\nmistake\nsee both in time-series research papers\nand also blogs is sometimes people get\nthese amazing r-squared values or\namazing correlations but that's mainly\nbecause they haven't transformed it and\nsure like most of the prediction just\ngoes into the magnitude where is\nactually the utility of the forecast is\nsort of that Delta so especially when we\ndipped it that makes it into a much\nharder task because once you diff a time\nseries predicting the last value for the\nnext one is not a great strategy so we\nhave turned it into a harder problem but\nactually when you transform it and do an\nARIMA model you'll still get great\nresults so that is not why that's maybe\nwhy we don't get impressive sounding\nnumbers but that it still should do\nbetter than it's doing so what what else\ncould have gone wrong so think about\nwhat we did we sort of we took a sliding\nwindow of basically a year's worth of\ndata and then we summarized it with\nthings like amplitude what else did we\nuse let's go back up percent beyond one\nstandard deviation skew max slope etc\nwhat else could have gone wrong there\nso I might buy that and this is where it\ncan be helpful to have an owl model I\nmight buy this thing I was like oh well\nmaybe this data is just not amenable to\nthis analysis like it's it's evolving so\nmuch over time that it's not reasonable\nto forecast but I guess I'd have to\nrebuttals to that right one rebuttal is\neven with our eyes we can sort of see\nsomething so we should be able to find\nsome technique that can do that and also\nas I mentioned the ARIMA can do it so if\nwe even though we have a model already\nwe know we can do it\nthat's not apparent once you have the\nlog transform but any any algorithm\nworth its salt should not just not work\nbecause we made a transform that makes\nit more uniform but absolutely we should\nthink about those things I did de trend\nit yes\nso you could say removing the trend may\nbe removed all the signal I think that's\none way of framing what he said so\nthat's what I'm saying that could be one\npossibility if we weren't aware that\nthere is a model that works and this is\nespecially if you work in fields where\nactually even the best model is pretty\nbad you can have these debates all day\nand sometimes you just don't know right\nis it that there's just no signal left\nor is it that we have a bad model and\nthat you know for some people that's\ntheir whole job is figuring that out or\ntrying to figure that out and it happens\na lot in time serious in this case\nthough there actually let me call your\nattention to this so we we didn't really\nlook at our future set in this case once\nwe computed it so here's the feature set\nfor my first five data points and what\ndo you notice here a lot of the values\nare the same right so I mean if this was\nwhat I was looking at rather than let's\nsay the picture of the time series I\nmean really what am I supposed to do\nwith this the percent amplitude only has\ntwo different numbers here the max slope\nhas the same number throughout and if we\nthink about why right let's let's plot\nthree samples of the same data right\nremember it's a sliding window that's\nthe problem is we have created these\npoints out of sliding windows but then\nthe data that\nhave produced tends to just look at\nthings like the amplitude the numerical\nqualities rather than say positional\nqualities right so if I look with my eye\nat these three curves I can see they're\ndifferent they're sort of at different\nphases of the same cycle so if I can see\nsort of that phase information which is\npositional information I'm in a great\npoint but my algorithm right now is not\nseeing anything that sort of positional\ninformation like telling me where a peak\nis it's only seeing information about\nnumbers right but if I'm only looking at\nthese time series along this axis I'm\nactually not seeing anything that\ndifferentiates these three right along\nthis axis the value axis they're no\ndifferent and that's part of what I did\nwrong here is I only used features that\ndescribe sort of the underlying values\nproduced by the process without\nproviding features that describe the\ntemporal structure rate the structure\nalong this x-axis that turned out to be\nreasonably okay for the EEG data because\nthere was already so much variation on\nthe numerical axis we didn't even really\nneed to treat it as a time series right\nwe could just sort of summarize the\nvalues we saw and we got a reasonable\nclassification but in this data set\nthat's not going to cut it right the\ndifferent samples are not different\nalong their numerical axis their\ndifferent along their positional axis so\nlet's revisit this and generate features\nthat encode some kind of positional\ninformation so this is just one stab at\ndoing that basically we're going to\ngenerate six features and what are these\nfeatures that I generated let's just\nread them okay so there the first one is\nNP dot where the values is equal to the\nmax so now I want to know at which time\nstep does the max occur my next feature\nis NP dot where values is equal to the\nmen so now I want to know at which time\nstep does the minimum occur right so now\ngoing back here those two features are\ngoing to give me information about this\naxis rather than this axis because\nthat's what I need to differentiate\nthese samples what have I got here I've\ngot the distance between the min and the\nmax and notice it's actually more than\ndistance its positional right because if\nthe min occurs before the Max versus\nafter this will be positive or negative\nright so this is giving me\ndirectionality\nthe relationship between the men and the\nmax I'm also still gonna keep the max in\nthis might not be that useful but I\nfigure what the half just in case that\nmatters and now I'm gonna take what am I\ntaking here I'm taking the last one -\nthe next - last one\nwhy am I doing that well in this case I\nhave a really short time series right\njust 11 steps and it seems like if I\nlook at the end the end last few points\nare where these three distinguished\nthemselves even though they're sort of\nneighboring sliding windows the part\nwhere they're most different is at the\nend so I'm just saying I want some\nfeatures to sort of focus on the\nbehavior that occurs here at the end so\nthat's what I'm doing with these again\nI'm sort of blindly throwing these in\nother than this idea that I need to\nunderstand this positional relevance so\nlet me do this and then let me make sure\nthese features are different because if\nthey're not I'm really in trouble but\nthese do appear to be at least a little\nbit different actually frame it just\nprints a little prettier so you know\nit's not great I'd have ten nine eight\nbut ten nine eight is a heck of a lot\nbetter than 10 10 10 right so we at\nleast go from having columns that are\njust absolutely identical which are\ntotal garbage for our decision tree into\none where there are at least some\nmeaningful differences even between\nquite similar time series because at the\nend those time series have different\noutputs okay so here we do that so we're\ngonna divide it up again let's fit this\nagain notice here we've still got our\nrmse is about the same point 1 3 5 what\nwas it up here where is it dude - dude\n0.138 so I'm not doing that much better\non my rmse front let's do a scatterplot\nthough so here's my test and let's look\nat the correlation on my test okay so\nhow do I adjust this so this is my test\nplot let's remember what the first test\nplot looked like with the total garbage\nfeatures I mean clearly this plot is a\nlot better right so our old plot with\nthe garbage features it's almost like\nthey're just on independent axes it's\nalmost like there is no relationship at\nall between our prediction and our a\ntrue value here that's not true at\nall right so I think we're seeing a case\nagain where our MSE won't tell you the\nfull story and that's something you need\nto keep in mind in general with your\ntime series models no particular metric\nis going to get it right all the time\nyou want to take a more holistic view\nand it will also depend on your\napplications right so our MSE gives me\nthe same thing for a model that has\nalmost no relationship to my outcome\nversus one that does right if I go by\nplotting and I would also point out here\nespecially if you ignore these two\noutlier points and you seem to have even\nmore of a relationship right so if I do\nmy Pearson R versus my Spearman are my\nspearmint R to get rid of a bit of the\noutliers the correlation is a little bit\nless negative although it's still\nnegative right so this model still needs\nwork but at least now we're bringing\nsome sense to the data versus before\nbecause our inputs were identical for\ndifferent outcomes there was just no way\nto get started for the algorithm and if\nwe do a scatter plot for the training we\nsee that we do even better right so is\nagain if we ignore that outlier in\nparticular this begins to look like\nsomething of a relationship and actually\nif we ignore the outlier we might even\nfinally be into the realm of positive\ncorrelation okay so not super impressive\nresults because again we're dealing with\na small data set so actually this\nexercise is here to show you a few\nthings right we saw how we can take one\ntime series and turn it into many\nsliding windows to produce enough\nseparate samples to fit an algorithm\nsafe for prediction but on the other\nhand I think what we see here is a it\ncan be very easy to generate the wrong\nfeature set and even a nonsense feature\nset which is a reason you should be\ncareful about using packages that are\nautomated feature generation it could be\nthat your features are not in any way\nmeaningfully distinguishing between say\nneighboring time series which is\nsomething you want to do but you also\nwant to remember that especially for low\ndata situations machine learning is\nreally not the way to go so if you're\ndoing some sort of forecasting that\ninvolves low quantities of data that's\nnot really what machine learning is\nbuilt for right versus something like\nthe area my model is so part of using\nmachine learning for time series is\nknowing when to use it judiciously and\nwhen you just don't have enough data to\njustify it so all of these techniques\nshould really be used on much much\nlarger data sets and if you use them on\nthe larger data sets that's where you're\ngoing to see some really excellent\nperformance and the\nyour data set the more likely that\nmachine learning is the way to go rather\nthan the traditional statistical models\nbecause as I mentioned those do well on\nsmall data sets and then they sort of\nmax out your estimation of your\nparameters can only get so good when you\ndon't have that money first is when you\nhave many many parameters in a more\ncomplex model if you have the data to\njustify it you can get really excellent\nresults and you can see that for example\nin cago competitions and industry\nresearch papers in academic modeling\ncompetitions of timeseriesforecasting XG\nboost is performing very well on large\nvolume data ok so any questions about\nmachine learning and time series ok so\ncan I look at your computer while we\ntake a break as we're about to take a\nbreak ok great so we're gonna take a\nbreak let's reconvene at 11:20 it's\n11:11 now and we will cover deep\nlearning as our last component so we\nhave we're gonna cover two more\nnotebooks I do want to briefly cover\ntime series clustering so I'm going to\nfly through this and then we'll get to\nthe deep learning example there are two\ndeep learning examples in the notebooks\none on electricity forecasting and one\non stock forecasting the stock\nforecasting one is sort of supposed to\nbe like a you know do it on your own\nafter the tutorial thing that was\nbecause I wanted you to have a tensor\nflow example as well as a mixed net\nexample just because I like those two\nlibraries but the concepts are the same\nso we're going to go through the more\ninteresting example and then the stock\nprediction example is just a second\nexample with noisier data and with a\ndifferent framework so that's just the\nbackground but let's talk about\nclustering let me just again briefly\nremind you of the dynamic time warping\njust because that's the thing I want to\ncover ok so what we just did we used we\nlooked at how we generate features for a\ntime series what can go wrong there what\ncan go right there or why we might not\nwant to do it sort of blindly why we\nmight want to put some thought into it\nwe saw that feature generation can be\nreally time consuming we learned how to\nextract many samples from a machine\nlearning\nperspective samples rather than just one\ntime series to accommodate the nature of\nhow much team learning works and how it\nthinks in terms of samples rather than\nin terms of time series so now briefly\nwe're gonna look at clustering and\nmainly I want to show you the value of\ndynamic time warping so again as a\nreminder this is one way to map time\nseries to one another in a way that\nlooks sort of more at their shapes\nrather than at features so we're gonna\ncompare those briefly and see see how\nthey work okay so in this case what we\nare looking at this is actually from the\nother tutorial I gave a few years ago\nthis is sort of a test subset of what\nare some word projections this is based\noff a paper from I think the 1990s or\nearly aughts before we had all this\nlovely deep learning and where people\nwere thinking okay like we scan\nhistorical documents is there a way to\nsort of identify which words within the\ndocuments are the same and what they\ncame up with is let's look at these\ndocuments look at a word in handwriting\nright we're looking at sort of\nhistorical stuff let's project that onto\na 1d axis right so sort of all the\nletters on the word we just do a bin\ncount of the density convert that to a\n1d axis that is actually an ordered\nevenly spaced axis so it's like a\ntemporal axis even though it's not per\nse time so if we do that they basically\nboiled it down to about two hundred and\nseven data points per word and if we\nlooked at what some of the words looked\nlike and they don't tell us what word in\nthe classification corresponds to\ncertain handwriting but we can see there\nare sort of distinctive shapes right\ndifferent numbers of Peaks different\nlocations of the peaks and I also just\nwanted to point out sometimes you can\njust have a different way of summarizing\nthe data and you almost create a new\ntime series so this on the left is sort\nof the time series of the projection of\nthe word and then on the right I took a\nhistogram of the values so basically I\ntook this summarised it numerically but\nyou can even think of a histogram as a\ntime series in the sense of your x axis\nis again evenly spaced right so once you\nstart realizing that time series\ntemporal axis can be a way to think\nabout shapes of curves more generally\nyou can think of all sorts of ways to\ngenerate features one really nice\nsort of visualize time series when you\nwant to see many examples of the same\nsort of class of a time series is you\ncan think about things like producing a\n2d histogram so in this case we're\nlooking at the word 23 so let's see I\nbelieve this this is word 23 yeah this\nis where 23 here but that's just one\nexample right what if we want to make\nsure that the features we see here are\nmore general we can do something like a\n2d histogram and we see oh yeah it looks\nlike there are sort of two peaks here\nalthough the location is not set that\nspecifically and a couple of Peaks going\non here and this is actually a good\nexample of part of the reason we can't\njust feed the raw time series as the\ninputs or as the features right because\nthings like Peaks their location jumps\naround whereas if we were to just feed\nit in that behavior won't really be\ndocumented so we're gonna just generate\nsome features again off of them off of\nwhat we've seen recently briefly so\nagain I used cesium and again this is\nsort of time consuming so I would\nrecommend just reading it off the CSV\nyou could on your own time possibly\nconsider generating them but it can be\nquite time consuming and again we sort\nof look at a histogram to get a sense of\nwhat these features looks like we at\nleast want to make sure we've got some\nsort of spread we're gonna generate some\nfeatures for the histogram itself as\nwell treating that as a sort of\ndirectional time series we're also going\nto find the location of the Max value so\nwe sort of learned our lesson from that\nlast example we did with the air\npassengers let's put in some sort of\npositional information that we found\nourselves in this case where the max is\nand then finally for clustering we want\nto make sure all the features sort of\ncount equally right so we want to make\nsure that it's not like amplitude just\nswapped out percent beyond one standard\nD right so what's a quick way to do this\nthis is not time serious specific but in\ngeneral we want to pre-process our\nfeatures and then we're gonna cluster\nthem and in this case I'm using\nhierarchical clustering why am i doing\nthat because I don't really have a good\nsense of what my distribution looks like\nso I don't want to use something like\nk-means where I'm assuming that I\nnecessarily want to minimize variance\nand that I have these sort of well\nshaped spheres I don't want to\nanything like that at all so if I do\nthat when I'm going to see this is just\nsort of the result of my clustering it\nlooks pretty messy like I'm not really\nfinding a good pattern and one measure\nof that is like the homogeneity score\nright so like how homogeneous are any of\nthe clusters I made in terms of the\noriginal class labels the punchline is\nnot very good right so the clustering I\ndo based on features of my time series\nat least for these features not great\nnow I could put in more work and try to\nfind good features or I could remember\noh wait dynamic time warping I heard\nabout this magical distance metric\nthat's really fantastic let me check\nthat out right so let's first look at a\ntoy example here's my toy example where\nI prove to you that euclidean distance\nis so terrible right so here I have two\nsine curves and just a flat line I think\nwe would all agree that we would rather\nmatch the two sine curves even if they\nhave sort of different frequencies we'd\nrather think of them as being more\nsimilar to each other rather than\nthinking that either is sort of more\nsimilar to a flat line right so if I\nwere going to group these say into only\ntwo groups we'd probably put the two\nsine curves together right there's more\nfundamental process and common for those\ntwo relative to just the flat line so as\nan exercise I would recommend on your\nown time considering calculating these\nbut I'll just point out that if we do\ncalculate these both of the sine curves\nend up having a shorter distance to the\nflat line than they do to each other\nthat you know there's no magic behind it\nthat's just you know sum of squares and\ntake the square root that's the behavior\nyou're going to get super undesirable\nbehavior so this has two components that\nconcern me firstly it's not super\ncomputationally efficient any way to do\nEuclidean distance right you still need\nto go through all the data points it's\nnot like it's really fast and it's\nreally crap right so there's two reasons\nnot to use it it's almost always true\nthat you wouldn't want to use Euclidean\ndistance another measure that has been\nrecommended is a correlation measure\nright like how sort of similar are they\nhow correlated are they that seems like\nmaybe it could do better than Euclidean\ndistance so we're going to do the same\nthing in this case you see that\nI am gonna add a little bit of random\nnoise to my time series 3 which is the\nflatline just so that I don't have an\nundefined correlation right because if\nthere's no variants that measure will\njust come back as an an so let's do the\nsame thing let's look at the correlation\nso ha interesting again the sine curves\nare sort of negatively correlated with\neach other versus they both have a\npositive correlation with the flatline\nso again if I use correlation a it's not\na super efficient metric anyway and via\ndoesn't do an especially good job\nrelative to my desired behavior so what\nI would recommend is that we do dynamic\ntime warping here is one simple\nimplementation you can definitely code\nthis on your own so I would definitely\nrecommend this if you're ever going to\nwork with dynamic time warping so that\nyou feel more comfortable with the\ndefinition the meat of it is here right\nso I'm using this this is the dynamic\nprogramming aspect is I have a matrix\nbecause basically what I'm trying to do\nis for each set of points right for\ngoing up to the eius point on one time\nseries and the J thought another and\nthey don't need to be unified I want to\nfigure out what's the way to align them\nthat results in the shortest difference\nin their values right that best aligns\ntheir difference so I do that by\niterating I start at the beginning of\ntime series 1 and the beginning of time\nseries 2 and then I calculate the\ndistance between them so this is for\nexample one thing you can play with when\nyou want an even dynamic time warping\ncan have many definitions right here I'm\njust gonna use the square of the\ndifference as my distance and then I\nwant to think okay so the min preceding\ndistance would either be the distance\nfrom I minus 1 and J or the distance\nfrom I J minus 1 or the difference with\nI minus 1 J minus 1 remember these eyes\nand J's are how I'm moving along the\ntemporal axis and remember I have\ndisconnected the 2 axis of my curves so\nI can sort of move along one curve but\nnot the other or move along both only\none step the way I've defined it but I'm\nsort of trying to move along the axis in\na way having a path such that my square\ndistance here is at a minimum so it's\nsort of this iterative way of working\nthrough the data so what if I move one\nhere but not here\nor to here and but not here and how can\nI sort of warp these two curves on to\neach other in the way that makes them\nfit the best that's what we're doing and\nthen ultimately I returned that minimum\ndistance I take the square root of that\nthing and that's what I'm defining as my\ntime dynamic time warming distance so\nnow I sure hope this fixes the problem\nor I just wasted a lot of time\nso now TS 1/2 T s 2 is 3.7 TS 1 2 ts 3\nis also 3 point 7 and T s 2 2 TS 3 is 4\npoint 1 6 ah so notice it's still not\nperfect right I still have a problem of\nmy flat line not being as different from\nmy sine curve as I would like but it's a\nlot better at least it's not telling me\nthat my two sine curves are way\ndifferent from each other compared to\nthe flat line so firstly dynamic warm at\ndynamic time warping has a lot of\nparameters you could tweak so one thing\nyou might want to do for a particular\ndataset is explore sort of different\nways of stepping back and forth right so\nwhat sort of time steps you'll allow and\ndifferent distance metrics to see if\nmaybe you could improve the distinction\ndepending on your underlying data and\nthen the other thing you might want to\ndo is just accept it at some point in\nthe sense of it is extremely difficult\nto define differences between time\nseries and this is where if if this is\nnot good enough you might even begin to\nlook into deep learning because at some\npoint you say okay it's just an image\nanalysis problem instead and lets you\nknow ratchet it up a notch so that's\nsomething to keep in mind to to the\nextent we want to keep this as a\nclustering problem this is how we want\nto do it I also wanted to point out that\nthere are libraries that code this up so\nyou won't find dynamic time warping in\nany of the sort of major libraries like\nSK learn or sci-fi unfortunately but\nthere are spelled all sorts of DIY\ngithub such as this one here so you can\nsee that actually we have the same\ndefinition and they have but this is a\nmuch more sophisticated implementation\nwhere they're more knobs to turn so\nthese things are available but you do\nhave to do a bit of work now if I were\nto compute this pairwise distance it is\nreally computationally taxing so we\nwould be here awhile just sitting around\nwe're not going to do that but we are\ngoing to run through the clustering and\nif we look at how the clustering perform\nnow now we have fantastic\nhomogeneity and completeness scores\nright so the extent to which one class\nis covered by one cluster is really good\nand the extent to which one cluster is\ncomposed of only one class is also\nreally good so now we have a really nice\none-to-one mapping versus if you\nremember the homogeneity score for just\nthe features was more like 50% so your\ndistance metric can make a tremendous\ndifference for time series clustering\nand similarly you can also use these\nsorts of things for forecasting or\nclassification right so for\nclassification I think it's obvious you\nwould just sort of map something on to\nthe cluster and if your clusters are\nquite homogeneous that's already useful\nyou can also use this for forecasting in\nthe sense of if you have a partial curve\nyou can say what curve or what time\nseries is this closest to and then use\nthat time series and its outcome to\npredict the outcome for your new feature\nso that's something you see in finance\nyou see that in health as well where\nyou're just sort of saying for example\nthis patient up to time T who are they\nmost similar to and maybe that is you\nknow a likely outcome for that patient\ntoo and that's really interesting\nbecause then you can use quite\nheterogeneous data you can use really\nmessy data and you'll get a pretty good\nresult here as compared to trying to\ncome up with a feature set which can be\nquite challenging so this is an\nalternative way to do both forecasting\nand classification any questions okay\ngreat so we are on to our last component\nwhich is deep learning for time series\nso we just have a couple of slides to\ncover okay so she'll have hands if you\nhave worked with a deep learning\nframework of some kind before at least\non a toy example okay so for those who\nhaven't I'm gonna give like a very brief\nrundown but this is in no way a good\nsubstitute for reading up on it and\nlearning these packages but I just want\nto give you a sense of how this would\nlook in a time series context so we\nstart with the simplest of neural\nnetwork examples which would be\nsomething like a fully connected model\nso neural networks are really just a\nform of machine learning in the sense\nthat they expect a series of inputs and\nwill give you a series of outputs and\nthey are not necessarily time aware\nright\nthey're not looking for give me your\ntime series and I will fit a model to a\ntime series they are looking for samples\nright X and outputs and the beautiful\nthing just like machine learning is deep\nlearning is like fairly agnostic it\ndoesn't really care what inputs you give\nit right to some extent these have been\nbuilt as you throw anything you want at\nit and you may very well get a nice\nanswer if you sort of tweak your\nparameters and you have your training\nand you know that aspect is a bit of an\nart but what does this look like in the\nsimplest model you have an input layer\nwhich would be for example could be\nfeatures so this looks a lot like a\ndecision tree right where we would\ncreate our features and then we would\ninput them and the difference between\nsay a decision tree and a neural network\nit's in the neural network will look\nlike this it will for example if you\nwere using a fully connected model then\nevery input would go through some kind\nof multiplication with a weight and\npossibly some sort of activation\nfunction an activation function is\nessentially just a way of introducing\nnon-linearity so you would essentially\nhave like a matrix multiplication\ncoupled with a non-linearity to produce\nyour next set of inputs into this layer\nthis layer would do the exact same thing\nright some sort of matrix multiplication\nfollowed by some nonlinear activation\nand output it and then at the end you\nmight have a bunch of new features that\nhave come out through your model and you\nfind some way of combining them to\nproduce your output that's like you know\nthe very very high-level view for those\nwho really have not encountered this\nbefore and obviously there's a lot that\ngoes into figuring out how you should do\nthat properly how to initialize it you\nvery quickly get into the millions of\nparameters so we're gonna leave all that\naside and there are great tutorials\noutside Syfy this week on this topic so\nyou can learn more but the main point\nhere is that a fully connected model\nfrom a time series perspective is just\nlike another machine learning model it\nwill still require future generation and\na lot of thought about how to sort of\ntranslate your time series into\nsomething that this kind of model can\ndigest so it may very well do better\nwith say than a decision tree right\nthat's sort of an empirical question for\na particular kind of data but this is\nnot in any way time aware just like\nmachine learning techniques are not in\nany way time aware and we have to sort\nof cut up and pre process our time into\na format that this\nsince there are other options though so\nthe classic example of how you would put\na time series into a neural network is\ncalled a recurrent neural network or are\nan annual mostly CR NN and the idea here\nis what this does\nrecurrent recognizes that your data will\ncome again and again your data recurs so\nyou build one cell here we go one cell\nbut basically what you do is unroll it\nand you reapply it at each stage with\nyour new data so this provides a\nunifying slash temporally aware way of\nlooking at your data because you have a\nmodel that understands it's going to be\nrolled out and used on the same data so\nwhat does this look like for a\nunivariate or multivariate case and each\neither example doesn't matter you have\nyour existing neural network model first\nyou put in your first value of your time\nseries right chug-chug-chug it produces\na hidden state you don't need to worry\nabout that but that's part of it's sort\nof internal accounting why does it need\na hidden State well that's what makes it\ntemporarily aware that's what gives it\nthe ability to sort of remember things\nfrom state to state right rather than in\nthis example we're just get something\nand chugs out one response and there's\nno time here this model itself has a\ntemporal axis right it produces its\nhidden state then you put in your next\nvalue it remembers its hidden state that\naffects what goes on in here and out\ncomes a new hidden state so your model\nhas some form of memory or some way of\nevolving over time recognizing that your\ndata wants something that recognizes\ndynamics that are evolving over time so\nrecurrent neural networks just like\neverything else we've talked about today\nnot a new idea I don't know the exact\ntime but I believe it's early 1980s that\nthese were envisioned and even things\nlike you probably know the term grew or\nLST M my understanding is even in LST M\nwas envisioned in the 80s although maybe\nI'm wrong in it's the 90s but it was\ncertainly a long time ago even though\nthese are still considered relatively\ncutting-edge exciting new technologies\nwhat's really exciting is a we've\ndeveloped much more of an art as to how\nto fit these you know how do we\ninitialize these things what does\nbackpropagation looks like and of course\nmuch more exciting is that we've all\ngotten now fancy GPUs and things that\ncan actually chug through this right so\nthe ability to imagine something\nthis was there in the 60s right but the\nability to actually do it is what's\nfairly new and also the presence of\nenough data to get good results is\nfairly new so the the two of variants of\nrecurrent neural networks that have\nturned out to be quite successful one is\ncalled a GRU so you see an example here\nso what we're looking at now is if we\nwere to actually unpack the guts of this\nthing so if we unpack the guts this is\nwhat we've got we've if we've got our\nour XT that comes in we track our hidden\nStates it goes through several different\nvariants of transformation and people\nhave sort of liked the update gate the\nforget gate the idea is is you have\ndifferent components one is sort of\nsupposed to help the model to decide how\nmuch is to sort of update its hidden\nstate based on new information another\nsort of looks at the new information\ncomes it coming in and sort of\ntransforms it before it even sort of\ntalks to the hidden state and so you\nhave all these parameters that can sort\nof specialize to describe different\ndifferent properties of the temporal\ndynamics of your data depending on how\nthey allow the updating now the\ninteresting thing is in a way this\nsounds a bit like what we talked about\nwith structural time series and\nstate-space models generally right it is\ndeveloping some sort of internal model\njust like the common filter of how it\nshould adapt to new information given\nits prior expectations although in this\ncase it's prior expectations are a\nhidden state that is not sort of a well\ndescribed statistical thing it's more\nlike a set of parameters and a matrix\nand actually the the theory of this is\nnot as well developed as I'm sure it\nwill be you know in a decade from now\nand we'll eventually figure out you know\nthese are these are still models they\ncan still have statistical properties\nit's just so much harder to think about\nthem but that's what a grue is a\ngroovers is an LS TM you've probably\nheard of these two and L STM tends to be\na little bit more complicated it has one\nmore gate compared to a grew in many\ncases it doesn't perform any better but\nit's sort of worth looking at both of\nthem depending on your dataset\nLS TM people say have sort of a longer\nmemory compared to a grew but again\ndepends very much on what kind of data\nyou're looking at okay so convolutional\nneural networks you might be surprised\nto see this here because you're thinking\noh I heard orang ins are actually rnns\nare time aware but\nactually CNN's can also be time aware in\nthe sense that an image is very much\nlike some of the seeing of things we've\ndone today right such as time series\nclassification time series clustering\ndoes not look all that different from\nimage analysis right so we can also use\nconvolutional neural networks as a way\nof understanding our time series as a\npicture that can be on its own already a\ngood way to classify a time series we\ncan also do modifications to\nconvolutional neural networks that make\nthem temporarily aware so one example of\nthat is what's called a causal\nconvolution so in a standard convolution\nif you do image analysis it sort of\ntreats all directions as the same and\nall areas are equally spaced but you can\ndo a very easy modification a causal\nconvolution where convolution sort of\nonly goes in one direction temporarily\nand in that case you have convolution\nthat is also time aware that can be used\nas an input to other components of a\nmodel or it can just be its own model\nwhich of these performs better will very\nmuch depend on your data what kind of\npatterns are in your data and what kind\nof tasks you're trying to do are you\ntrying to predict or are you trying to\nclassify cnn's tend to be more\nsuccessful for time series\nclassification rather than prediction\nbut of course that's just a very broad\ngeneralization in a field where we're\nstill sort of working out the science of\nhow these things work and so one\nactually really cool architecture I\nwanted to highlight this was published\nabout two years ago I think\nEllis T net so these researchers\nactually said well why why do we have to\npick and choose between convolutional\nand recurrent behaviors there's actually\nno reason we shouldn't use both and in a\nway that makes sense right because if we\nthink about something like the airline\npassengers data they're sort of bold\naspects right there's this sort of\naspect of we see a trend and a\ndifference and that sort of more like\nRNN ish right to see a trend I'm not\nsure convolutional understands that but\non the other hand we have this seasonal\ncomponent right where they're sort of\nwinter fall summer etc etc and that part\nseems a bit more like an image analysis\ntype of component right so they had this\nidea they also said we can use the\nconvolutional bit to understand better\nhow to use multivariate time series\nright so to the extent we want to relate\ndifferent inputs right if I have\ndifferent inputs into my model and it's\nnot just a univariate\ntime-series do I want to sort of\nconvolve these there are a few\nadvantages to that one is maybe you'll\ndiscover their relationships between\nthem especially if we convolve on time\nand input right and we sort of take\nalmost like a sliding window image of\nour time series but we can also think\nmaybe the convolution itself might\nidentify some seasonality right because\nthat's sort of image like they also said\nwell why do we always do sort of\nrecurrent where we just sort of feed it\nin one at a time what we might also want\nto do to design our architecture to\nbetter reflect certain realities is we\nmight want to have something that\nreflects the reality of seasonality so\nwe might also want to have something\nwhere we skip right so maybe if I have\n24-hour data I might want to have both\nan RNN that looks at all the hours but I\nmight want to have an RNN that in\naddition to looking at all the hours\nonly looks at the same local hour for\nprevious day right so if I'm at 3 a.m.\ntrying to predict the electricity for 4\nam maybe I want to look at 1 2 3 4 or\nyou know 0 whatever up to 3 a.m. you\nknow that sort of slice but I might want\nto look at 3 a.m. from the previous day\nand the previous day in previous day and\nthink of that as its own time series and\nhave a recurrent neural network that\nruns only through that or if not that\nmaybe only look at the states of the\nrecurrent neural network at those points\nright so for those of you who have\nworked with neural networks you know\nthat there's actually sort of infinite\narchitectural permutations you can make\nanything you can imagine you can build\nin a fairly straightforward way\nalthough figuring out how to train it\nproperly of course is always where you\nrun into trouble and figuring out if you\nactually have enough data to train it\nbut the wonderful thing about this as\ncompared to some of the other models\nwe've looked at today is you can imagine\nany sort of dynamics and describe it\nwith some kind of neural network and see\nif you can improve your forecasting or\nclassification or what-have-you\nah so that that is prediction so let's\nsee hopefully it will come back up ok so\nin the meantime I would ask everyone to\nopen your notebooks so we're gonna look\nat notebook 5\nsupposed to come back on electricity\nokay I'm going to see if I can Oh\nexcellent thank goodness okay remind me\nnot to move at all until for the next 13\nminutes okay so we're gonna try to\nforecast electricity this is one of the\noriginal data sets that was used when\nthe LST night model was published the\noztent\nt net model when it was published they\nsaw pretty big gains compared to say\nsomething like just using a grue or just\nusing a convolutional neural network and\nthey made this point of you know\nbuilding a an architecture that reflects\nespecially three of the temporal\nCadence's of human activity works really\nwell and also they were making the point\nthat with multivariate time series it\ncan be interesting to find ways to\ncombine those information channels so\nthose those were what they pointed out\nso one of the model one of the data sets\nthey use with this was this electric\ndata set that we're going to use now so\nthe first thing we're gonna do is look\nat the data let's look at this together\nso if we read it in and look at the head\nas you can see this is like very\nmulti-channel this is three hundred and\ntwenty one parallel time series of\nelectric use hourly Electric use at 321\ndifferent sites I want to say it's the\nstate of Alabama but I don't remember\nyou'd have to check the reference to the\ndata they do mention but it's hourly\ntimes hourly electric use in some state\nin the US and if we plot this as always\nthere are you know sort of better and\nworse ways to plot but if we look this\nthis is really interesting right this is\njust that one site for the full range of\ntime it's interesting to me because you\nhave sort of drastically different\nregimes so if I were doing say a\ntraditional statistical model or even a\nmachine learning model I would be really\nhard-pressed to think oh my goodness\nlike how am I gonna make all of these\ndifferent points in time comparable\nright I mean this this looks really\ndifferent and it would it would take a\nlot of thought where's with a neural\nnetwork sometimes because we don't for\nnow have those assumptions built in we\ncan also expect it to more flexibly\nhandle really varying data and maybe\neven notice things that we wouldn't\nnotice but this this long-term plot this\ngives us one view it tells us that we've\ngone through sort of different regimes\nof electric use at this one site and\nthis is just one of 321 inputs and\nunless I we're doing this as a full-time\njob for years I'm not sure I'd be able\nto look at all three\ntwenty-one and know them although great\nif you can write but also let's remember\nif we look at it at a smaller time scale\nwill spot other data so if we look at it\nat this smaller time scale what we spot\nis some kind of recurring pattern right\nwhich makes sense because this is our\nearly electric use now you might be\nthinking oh well I would have expected\nto see better use than this and this is\nbecause we have I have pre difference to\nthis data for you so actually the\noriginal paper fit to the original time\nseries which looks more like the air\npassengers it's sort of account but a\nthat's too easy and B that tends to over\nrepresent how well you're doing you're\nlike oh man look at me I got 99%\ncorrelation they match so well when in\nfact most of that could just be done\nwith a null model so if we difference it\nout we are being really strict with\nourselves that like the model has to add\nsomething useful that can predict from\nhour to hour rather than just sort of\nglobally getting it right so I encourage\nyou whenever you can to use difference\ndata\nokay so handy data structure this is not\nnothing to do with time series but I\nlike to use something like a ring buffer\nwhen I'm sort of thinking about a way to\ndo early stopping so another thing for\nthose new to neural networks there's no\nsort of predefined limit to how long you\nshould train right you sort of want to\ntrain as long as you're getting better\nbut you want to make sure you don't over\nfit so that's similar to other\ntechniques we've discussed today the\ndifference being is that now we could\npotentially have way more parameters\nthan even the most complicated model\nwe've looked at right we talked about\nhow structural time series there's so\nmany knobs to turn looks like you could\nreally game it if you wanted to that is\nlike orders of magnitude different from\nthis this is really a lot of knobs to\nturn so we need to make sure to be\ndisciplined in our fitting so that's\njust one way that I did early stopping\nok so data preparation I want to call\nyour attention to this because this part\nactually looks a little bit like what we\nhad done before right where we take one\nlong time series and we reshape it into\nmany slices of a time series in this\ncase we have multi channels so we're not\njust slicing along one axis we actually\nhave to slice all of our multivariate\ntime series and make sure that all of\nthem sort of are these sliding windows\nand keep them in sync so that's what\nthis data does I would encourage you to\nlook at it on your\nas well to see how that works we also\nbuild data iterators so that we can\nprepare batches of data okay so here's\nthe interesting part\nso I provide examples of how you could\nuse a fully connected model or a CNN\nmodel I would encourage you to try those\nout on your own you'll find that they\ndon't tend to do as well as the RNN\nmodel or the simple LST net model that\nI've introduced here so just briefly the\nRNN model how do you build an RNN model\nand mix nut well you need to have a cell\nand then you need to add that cell you\ndecide how many hidden units you have\nwhich is basically saying how large of\nlike a hidden state matrix do I want to\nhave right like how many different\nparameters do I want to have to describe\nthis behavior of how it should update\nand how it should retain information so\nwith an RNN model that's essentially\njust one layer of this RNN rollout that\nwe did already now let's look at this\nLST net model this is really interesting\nand this corresponds to that\narchitecture I showed you the first\nthing we're gonna do in this case is\nwe're going to apply a convolutional\nfilter to our inputs so we're going to\ntake our inputs which is multi-channel\nright so actually in this case we have\ntime by batch number by tnc time by\nbatch number by channel where channel is\nis like 320 right so the first thing\nwe're gonna do is run that through a\nfilter where we're basically like\nlooking at a sliding window as an image\nso the window slides both over time but\nalso over all of our features we then\ntake the convolutional output and that\nbecomes the input to our neural network\nRNN the recurrent neural network so in\nthis case we've said instead of just\nfeeding in our raw values we could also\nconsider processing those with other\nneural components it's almost a way of\nsaying the convolutional component is\ngoing to produce features from our raw\nfeatures it's going to boil those down\nto fewer features and then those\nfeatures are going to go into the\nrecurrent bit so we're sort of divvying\nup the task we're not going to do the\nfuture generation but some portion of\nour network is going to do the future\ngeneration before we feed it into the\nrecurrent neural network and if you\nthink about that that makes sense right\nbecause why should I be putting in 320\nparallel measurements into the recurrent\nneural network which then a has\ndecide how the measurements are related\nto each other and how they're\ninteresting but B has to keep track of\nthe temporal component so if I can sort\nof shrink down the multivariate aspect\nso that the RN and can concentrate on\nthe temporal component that can be\nreally helpful now there are two RN n\nthere are 2 RN n models right there's\none that sort of looks at every data\npoint and then there's one that sort of\nskips seasonality but here I have left\nthat out and I just have the regular RN\nn and the reason for that was I just\nfound that for this particular data set\nit wasn't especially important so that's\nsomething else you want to keep in mind\nis what architecture actually improves\nyour process for a particular model and\nthen finally what I didn't mention but\nwhat's really interesting is the authors\nincluded an autoregressive element this\nis just an element that uses the passed\nvalues to predict future values right\nthis is the same as an ARIMA model so\nactually what we're seeing here also is\ncombining a statistical approach with\nneural networks and this is something\nthat is increasingly common and that has\nactually proven quite successful in time\nseries in particular much more than in\nother areas of machine learning so those\nare our models we can define our\ntraining and we can run this let's see\nam i running my screen is well my screen\nis frozen so we must be running because\nthat's what happens on my rinky-dink\nlaptop obviously you don't usually\nreally want to do deep learning on a\nlaptop and you don't usually want to do\nit on a small data set so keeping both\nof those in mind let's see okay well I\nwould encourage you to check out the\noutput and experiment with this on your\nown but the the punchline that I wish we\nhad time to train on is that the LSD nap\nmodel does quite a bit better than the\nrecurrent neural network alone so it's\nreally important to design structures\nthat are like savvy about how they\nallocate the labor so in this case the\nlabor of the convolutional bit the\nfuture generation is separated out from\nthe temporal analysis that's one thing\nthat makes this model really successful\nthe other thing that makes this model\nreally successful is the inclusion of\nthis AR component the autoroute\nrecive component and in fact if you\nremove that component you take an\nenormous hit to performance so this is I\nthink is great inspiration for how you\ncan take your traditional statistical\nknowledge and work it into your neural\nnetworks and you will find that that\ngets you a really great outcome and in\nfact so most recently there was an\nacademic research competition on machine\non on timeseriesforecasting and both the\nnumber one and number two winners of\nthis competition which took place on a\nhundred thousand different time series\nover many domains turned out to be\nintegrating machine learning and\nstatistical statistical analyses that\nare quite traditional in one case\ncombining statistical analyses with deep\nlearning and then another case using X G\nboost to choose the coefficients to\ncombine statistical models so also the\nways that you can sort of permute these\nthings are quite varied gosh I could\nhave sworn this this ran faster when I\nwas oh here we go here we go okay so in\nthis case you can see I'm printing out\nthe correlation which metric you like\ncan be a matter of personal preference\nit can also be a matter of what is\nmeaningful for your problem but\nbasically what we want to see here is\nthat it's improving especially on the\nvalidation I think we will stop it here\nif we can yes okay so stopped it here I\nalso have the results from the last time\nI ran this already loaded no no I'm\nlying\nsuckster\noh I see because I only went up to okay\nlet's do this okay so in this case we\ncan see what we're looking at after only\nfour iterations and we can see so far\nfor this first column it doesn't look\nespecially fantastic let's look at this\ncolumn quite different right and this\nthis actually looks a little bit more\npromising especially if we ignore\noutliers but we can see here that we've\nactually managed to fit many time series\nright we're fitting 321 time series in\nparallel with one generalized model and\nso we can sort of expect results to vary\nbetween one and the other so if you let\nthis go for about 20 to 25 iterations\nyou get really excellent results but not\nafter after 5 epochs so I'd encourage\nyou to keep this running on your own\ncomputers but takeaways here are this is\nnot even an enormous time series data\nset right I can fit this all in memory\non my rinky-dink laptop so I actually in\nmy experience have found that deep\nlearning works even better than like\nsort of regression trees and things like\nthat for cases of smallish\nbut not tiny data sets for deep learning\ncompared to statistical models and also\nthat you want to be creative about\ncombining your statistical models and\nyour deep learning so this is where you\nsee especially for time series analysis\nhaving a broad domain knowledge of many\ndifferent sort of classes of analysis\nwill also help you to build more\ncreative models so even when you want to\ndo something more cutting-edge like deep\nlearning having that traditional\nknowledge will really help you ok so I\nhave one slide to wrap up because I did\nwant to talk about ok so just to wrap up\nto to highlight some things that we\nhaven't talked about but that are\nimportant also as being on the horizon\nor just active areas in modern time\nseries analysis I think the huge one\nthat we didn't have time for but that\nyou can very much address with what we\ntalked about today is anomaly detection\nright so maybe even some of you work in\nthis field\nthings like structural time series\nhidden Markov models regression trees\nexcuse and deep learning can all be\napplied to the question of anomaly\ndetection so anomaly detection is not\nsomething where you need\none specific technique actually all of\nthese are available to you and may even\nbe combined successfully there's also so\nmany new and old libraries for time\nseries analysis especially for more\nmodern methods there are literally\nhundreds of packages definitely this is\nan environment where ours options are\nricher than Python so you might also\nwant to get more comfortable especially\nlike with at least being able to access\nour packages through Python to have\naccess to those time series analysis\nobviously is an area of active research\nboth in industry and in academia but\nmost people who are doing cutting-edge\nresearch work in our so I'm a bit of a\ndowner on that front that if you want to\nhave access to those methods you also\nwant to be looking at the our ecosystem\nas well as the Python ecosystem and then\nthe two things I would highlight that we\ndidn't talk about but you would also\nwant to look into firstly as automated\nforecasting at scale so somebody had\nbrought up Facebook's profit package\nthere's also a Google package there's\nalso a Twitter package I'm probably\nforgetting some others but there are\nsome massive tech companies uber looking\nat really massive data sets of\ntime-series developing ideas for their\nown research and then sometimes either\nopen sourcing them or sometimes offering\nforecasting as a service so an example\nof that is Amazon now offers via AWS is\noffering forecasting as a service using\nsort of their expertise that they're at\ntheir own data right so if you you have\ndata that looks like Amazon's data right\nlike many many parallel time series or\nhighly variable time series such as\nrolling out new products you might want\nto look into that the extent to which\nthey are open about what they're doing\nversus a little bit cagey varies by\ncompany so some of these packages are\ncompletely open-source some of them are\ncompletely closed off what is clear when\nyou read the literature is they're\nbarely using deep learning it's\nprimarily traditional ARIMA or\ntraditional models that have few\nparameters because the goal for them is\nto just have a fairly reliable\ntransparent forecast rather than the\nperfect forecast and then finally as I\nmentioned to some extent it looks like\nthe future is combining machine learning\nand statistical approaches so you very\nmuch do still want to have that\ntraditional timeseriesforecasting\nbackground and ARIMA and that sort of\nthing because that is not going away and\nit continues to be part of even the most\ncutting\nresults in time series analysis okay so\nI'm available for questions afterwards\nthanks very much for coming to the\ntutorial and please be in touch if you\nhave questions or comments\n[Applause]\n",
  "words": [
    "okay",
    "good",
    "morning",
    "everyone",
    "thanks",
    "coming",
    "tutorial",
    "modern",
    "time",
    "series",
    "analysis",
    "sort",
    "interpretation",
    "modern",
    "time",
    "series",
    "analysis",
    "suppose",
    "everyone",
    "would",
    "different",
    "interpretation",
    "mainly",
    "means",
    "going",
    "cover",
    "auto",
    "regressive",
    "models",
    "going",
    "talk",
    "briefly",
    "gon",
    "na",
    "mainly",
    "talk",
    "methods",
    "computationally",
    "taxing",
    "invented",
    "many",
    "decades",
    "ago",
    "would",
    "still",
    "classify",
    "modern",
    "given",
    "far",
    "successful",
    "sort",
    "era",
    "big",
    "data",
    "far",
    "successful",
    "sort",
    "readily",
    "available",
    "computing",
    "resources",
    "necessarily",
    "mean",
    "ideas",
    "new",
    "mean",
    "also",
    "true",
    "even",
    "neural",
    "networks",
    "right",
    "ideas",
    "also",
    "new",
    "lot",
    "modernity",
    "comes",
    "computing",
    "resources",
    "thinking",
    "little",
    "bit",
    "background",
    "subject",
    "years",
    "ago",
    "actually",
    "gave",
    "another",
    "tutorial",
    "time",
    "series",
    "analysis",
    "syfy",
    "one",
    "covered",
    "exploratory",
    "data",
    "analysis",
    "data",
    "processing",
    "also",
    "traditional",
    "methods",
    "like",
    "arima",
    "models",
    "auto",
    "regressive",
    "models",
    "sort",
    "thing",
    "interested",
    "background",
    "one",
    "resource",
    "many",
    "resources",
    "one",
    "prepared",
    "need",
    "familiar",
    "material",
    "today",
    "definitely",
    "material",
    "familiar",
    "really",
    "bread",
    "butter",
    "working",
    "professionally",
    "even",
    "though",
    "men",
    "tear",
    "methods",
    "sort",
    "less",
    "modern",
    "much",
    "relevant",
    "today",
    "still",
    "perform",
    "exceptionally",
    "well",
    "also",
    "give",
    "really",
    "good",
    "conceptual",
    "foundation",
    "similarly",
    "good",
    "comfortable",
    "data",
    "cleaning",
    "time",
    "series",
    "little",
    "bit",
    "also",
    "little",
    "bit",
    "curious",
    "people",
    "room",
    "guess",
    "quick",
    "survey",
    "show",
    "hands",
    "already",
    "work",
    "time",
    "series",
    "data",
    "work",
    "okay",
    "hope",
    "still",
    "stuff",
    "hard",
    "feelings",
    "certainly",
    "pipe",
    "suggestions",
    "input",
    "well",
    "many",
    "people",
    "never",
    "worked",
    "time",
    "series",
    "data",
    "okay",
    "limited",
    "limited",
    "okay",
    "fair",
    "enough",
    "many",
    "people",
    "work",
    "scientific",
    "sector",
    "broadly",
    "defined",
    "okay",
    "many",
    "people",
    "work",
    "something",
    "would",
    "call",
    "remotely",
    "scientific",
    "okay",
    "fair",
    "enough",
    "good",
    "know",
    "curiosity",
    "thinks",
    "came",
    "farthest",
    "get",
    "anybody",
    "beat",
    "singapore",
    "really",
    "tough",
    "yeah",
    "okay",
    "someone",
    "site",
    "claimed",
    "traveling",
    "two",
    "days",
    "come",
    "conference",
    "might",
    "beat",
    "though",
    "far",
    "like",
    "time",
    "whatever",
    "okay",
    "okay",
    "outline",
    "gon",
    "na",
    "talking",
    "brief",
    "overview",
    "sort",
    "makes",
    "time",
    "series",
    "data",
    "special",
    "things",
    "want",
    "keep",
    "head",
    "looking",
    "time",
    "series",
    "data",
    "especially",
    "new",
    "worked",
    "gon",
    "na",
    "divide",
    "discussion",
    "three",
    "components",
    "going",
    "looking",
    "state",
    "state",
    "space",
    "models",
    "time",
    "series",
    "overview",
    "going",
    "cover",
    "two",
    "methods",
    "specific",
    "state",
    "state",
    "state",
    "space",
    "methods",
    "going",
    "talk",
    "bit",
    "applying",
    "traditional",
    "missional",
    "machine",
    "learning",
    "methods",
    "specifically",
    "time",
    "series",
    "approaches",
    "take",
    "want",
    "look",
    "gon",
    "na",
    "wrap",
    "looking",
    "deep",
    "learning",
    "time",
    "series",
    "okay",
    "time",
    "series",
    "generally",
    "time",
    "series",
    "seems",
    "like",
    "everybody",
    "good",
    "idea",
    "given",
    "work",
    "right",
    "classic",
    "example",
    "think",
    "every",
    "time",
    "series",
    "book",
    "almost",
    "always",
    "run",
    "many",
    "many",
    "examples",
    "something",
    "like",
    "stock",
    "prices",
    "readily",
    "available",
    "usually",
    "free",
    "fairly",
    "chaotic",
    "time",
    "series",
    "really",
    "interesting",
    "look",
    "course",
    "nobody",
    "good",
    "modeling",
    "filthy",
    "rich",
    "remains",
    "sort",
    "open",
    "problem",
    "also",
    "interesting",
    "want",
    "point",
    "many",
    "things",
    "time",
    "series",
    "get",
    "called",
    "time",
    "series",
    "even",
    "get",
    "analyzed",
    "respective",
    "disciplines",
    "without",
    "anyone",
    "using",
    "word",
    "even",
    "though",
    "upper",
    "right",
    "hand",
    "slide",
    "ekg",
    "diagram",
    "never",
    "heard",
    "doctor",
    "talk",
    "time",
    "series",
    "data",
    "actually",
    "often",
    "looking",
    "right",
    "taking",
    "sort",
    "measurement",
    "time",
    "hooked",
    "device",
    "hospital",
    "kind",
    "digital",
    "analog",
    "curve",
    "time",
    "series",
    "right",
    "think",
    "data",
    "science",
    "time",
    "series",
    "analysis",
    "whatever",
    "want",
    "call",
    "lot",
    "contribute",
    "field",
    "keep",
    "hearing",
    "benefits",
    "healthcare",
    "trying",
    "data",
    "science",
    "deep",
    "learning",
    "hear",
    "much",
    "time",
    "series",
    "context",
    "even",
    "though",
    "actually",
    "lot",
    "time",
    "series",
    "data",
    "health",
    "field",
    "well",
    "many",
    "others",
    "similarly",
    "bottom",
    "think",
    "nmr",
    "might",
    "wrong",
    "chemist",
    "wants",
    "correct",
    "tell",
    "kind",
    "spectroscopy",
    "kind",
    "nmr",
    "looking",
    "molecular",
    "structure",
    "right",
    "also",
    "time",
    "series",
    "definition",
    "even",
    "though",
    "x",
    "axis",
    "time",
    "actually",
    "wavelength",
    "use",
    "many",
    "methods",
    "well",
    "spaced",
    "temporal",
    "axis",
    "even",
    "going",
    "working",
    "data",
    "today",
    "temporal",
    "axis",
    "time",
    "per",
    "se",
    "something",
    "sort",
    "well",
    "ordered",
    "metric",
    "different",
    "points",
    "time",
    "right",
    "makes",
    "time",
    "series",
    "different",
    "anything",
    "analysis",
    "example",
    "might",
    "something",
    "like",
    "right",
    "time",
    "axis",
    "really",
    "gon",
    "na",
    "prediction",
    "well",
    "probably",
    "right",
    "prediction",
    "might",
    "useful",
    "thing",
    "although",
    "maybe",
    "contrive",
    "example",
    "certainly",
    "something",
    "like",
    "time",
    "series",
    "clustering",
    "right",
    "might",
    "multiple",
    "nmr",
    "spectra",
    "want",
    "find",
    "way",
    "classify",
    "without",
    "hundreds",
    "grad",
    "students",
    "leaving",
    "right",
    "find",
    "way",
    "replace",
    "grad",
    "students",
    "even",
    "better",
    "okay",
    "tasks",
    "time",
    "series",
    "analysis",
    "things",
    "people",
    "well",
    "usual",
    "visualization",
    "exploratory",
    "data",
    "analysis",
    "time",
    "series",
    "often",
    "specific",
    "time",
    "related",
    "temporal",
    "questions",
    "understanding",
    "temporal",
    "behavior",
    "data",
    "especially",
    "time",
    "axis",
    "actually",
    "time",
    "axis",
    "seasonality",
    "right",
    "kind",
    "recurrence",
    "data",
    "kind",
    "seasonality",
    "also",
    "look",
    "cyclical",
    "data",
    "quite",
    "time",
    "seasonal",
    "data",
    "right",
    "cyclical",
    "data",
    "sort",
    "recurrence",
    "damped",
    "also",
    "sort",
    "change",
    "period",
    "time",
    "example",
    "seasonal",
    "would",
    "something",
    "like",
    "weather",
    "example",
    "example",
    "cyclical",
    "would",
    "something",
    "like",
    "people",
    "hypothesize",
    "business",
    "cycle",
    "right",
    "sort",
    "cycle",
    "banking",
    "stock",
    "prices",
    "sort",
    "thing",
    "another",
    "thing",
    "usually",
    "want",
    "sort",
    "identify",
    "underlying",
    "distributions",
    "nature",
    "temporal",
    "processes",
    "producing",
    "data",
    "right",
    "want",
    "get",
    "sense",
    "hypothesis",
    "see",
    "sort",
    "river",
    "level",
    "time",
    "want",
    "get",
    "sort",
    "describes",
    "river",
    "level",
    "fundamentally",
    "another",
    "thing",
    "might",
    "want",
    "predict",
    "also",
    "way",
    "describing",
    "dynamics",
    "else",
    "need",
    "well",
    "estimation",
    "past",
    "present",
    "future",
    "values",
    "right",
    "also",
    "distinction",
    "say",
    "something",
    "like",
    "filtering",
    "forecasting",
    "also",
    "smoothing",
    "talk",
    "context",
    "state",
    "space",
    "idea",
    "noisy",
    "measurements",
    "think",
    "sorts",
    "things",
    "noisy",
    "measurement",
    "stock",
    "prices",
    "maybe",
    "think",
    "market",
    "got",
    "wrong",
    "real",
    "price",
    "separate",
    "market",
    "says",
    "could",
    "medical",
    "sensor",
    "sort",
    "known",
    "plus",
    "minus",
    "let",
    "say",
    "even",
    "measure",
    "blood",
    "sugar",
    "within",
    "minute",
    "device",
    "still",
    "know",
    "5",
    "discrepancy",
    "something",
    "actually",
    "accepted",
    "fda",
    "right",
    "devices",
    "used",
    "people",
    "diabetes",
    "example",
    "know",
    "ca",
    "get",
    "perfect",
    "could",
    "time",
    "series",
    "lots",
    "measurement",
    "error",
    "tasks",
    "figuring",
    "well",
    "true",
    "value",
    "time",
    "steps",
    "right",
    "value",
    "measured",
    "versus",
    "value",
    "actually",
    "true",
    "way",
    "getting",
    "next",
    "classification",
    "another",
    "common",
    "time",
    "series",
    "task",
    "could",
    "something",
    "like",
    "medical",
    "field",
    "right",
    "find",
    "kind",
    "ekg",
    "looking",
    "showing",
    "normal",
    "heart",
    "kind",
    "arrhythmia",
    "could",
    "classification",
    "nmr",
    "spectra",
    "like",
    "talked",
    "could",
    "classification",
    "retail",
    "behavior",
    "sort",
    "distinctive",
    "trajectories",
    "website",
    "right",
    "things",
    "like",
    "also",
    "time",
    "series",
    "analysis",
    "another",
    "major",
    "task",
    "anomaly",
    "detection",
    "right",
    "figure",
    "points",
    "problematic",
    "points",
    "right",
    "better",
    "less",
    "often",
    "say",
    "bank",
    "cancel",
    "credit",
    "cards",
    "doctor",
    "run",
    "tests",
    "really",
    "necessary",
    "versus",
    "run",
    "ones",
    "really",
    "need",
    "always",
    "think",
    "things",
    "time",
    "series",
    "analysis",
    "increasingly",
    "panel",
    "data",
    "things",
    "right",
    "increasingly",
    "things",
    "think",
    "data",
    "right",
    "surveys",
    "whatever",
    "better",
    "able",
    "track",
    "people",
    "sense",
    "things",
    "evolving",
    "time",
    "okay",
    "time",
    "series",
    "data",
    "versus",
    "data",
    "challenges",
    "things",
    "keep",
    "mind",
    "well",
    "number",
    "one",
    "opportunities",
    "missing",
    "data",
    "points",
    "quite",
    "challenging",
    "following",
    "people",
    "even",
    "samples",
    "time",
    "imagine",
    "anyone",
    "worked",
    "especially",
    "sort",
    "human",
    "related",
    "sciences",
    "knows",
    "problem",
    "right",
    "good",
    "luck",
    "finding",
    "survey",
    "respondents",
    "year",
    "year",
    "sort",
    "epic",
    "task",
    "even",
    "really",
    "simple",
    "scientific",
    "data",
    "problem",
    "example",
    "putting",
    "together",
    "data",
    "tutorial",
    "trying",
    "use",
    "colorado",
    "river",
    "data",
    "federal",
    "government",
    "kept",
    "getting",
    "bombarded",
    "sort",
    "bright",
    "red",
    "text",
    "indicating",
    "measurements",
    "discontinued",
    "due",
    "funding",
    "cuts",
    "right",
    "time",
    "series",
    "especially",
    "anything",
    "looks",
    "government",
    "data",
    "sets",
    "even",
    "affected",
    "politics",
    "extent",
    "kind",
    "funny",
    "right",
    "run",
    "might",
    "panel",
    "data",
    "second",
    "major",
    "point",
    "high",
    "degree",
    "correlation",
    "data",
    "points",
    "values",
    "past",
    "almost",
    "always",
    "hope",
    "predict",
    "values",
    "future",
    "right",
    "think",
    "errors",
    "look",
    "like",
    "think",
    "relation",
    "points",
    "time",
    "looks",
    "like",
    "opposed",
    "correlations",
    "data",
    "tend",
    "throw",
    "things",
    "different",
    "people",
    "hopefully",
    "first",
    "order",
    "super",
    "correlated",
    "one",
    "another",
    "good",
    "prediction",
    "bad",
    "models",
    "assume",
    "independent",
    "inputs",
    "right",
    "example",
    "looking",
    "machine",
    "learning",
    "techniques",
    "thinking",
    "apply",
    "sort",
    "non",
    "statistics",
    "time",
    "series",
    "analysis",
    "might",
    "readily",
    "violate",
    "assumptions",
    "whereas",
    "sort",
    "careless",
    "general",
    "running",
    "linear",
    "regression",
    "likely",
    "true",
    "likely",
    "true",
    "time",
    "series",
    "analysis",
    "also",
    "hassle",
    "dealing",
    "timestamps",
    "measures",
    "whatever",
    "distance",
    "metric",
    "along",
    "temporal",
    "access",
    "right",
    "things",
    "like",
    "time",
    "zones",
    "frequency",
    "irregularities",
    "even",
    "work",
    "sort",
    "scientific",
    "data",
    "things",
    "like",
    "exact",
    "timestamp",
    "like",
    "day",
    "week",
    "matter",
    "far",
    "know",
    "affect",
    "physics",
    "might",
    "somehow",
    "find",
    "example",
    "timing",
    "measurement",
    "little",
    "bit",
    "right",
    "sort",
    "bias",
    "problem",
    "go",
    "away",
    "even",
    "case",
    "okay",
    "characteristics",
    "data",
    "collected",
    "sequentially",
    "one",
    "axis",
    "monotonically",
    "increasing",
    "right",
    "mentioned",
    "definition",
    "time",
    "series",
    "got",
    "meaningful",
    "axis",
    "measurable",
    "distance",
    "time",
    "want",
    "characterize",
    "structure",
    "across",
    "data",
    "points",
    "right",
    "seasonality",
    "cycles",
    "autocorrelation",
    "trends",
    "familiar",
    "concept",
    "autocorrelation",
    "want",
    "make",
    "sure",
    "fairly",
    "familiar",
    "quick",
    "overview",
    "autocorrelation",
    "point",
    "time",
    "correlated",
    "say",
    "time",
    "minus",
    "one",
    "entire",
    "time",
    "series",
    "right",
    "time",
    "relate",
    "like",
    "value",
    "always",
    "relate",
    "value",
    "right",
    "probabilistic",
    "sense",
    "autocorrelation",
    "finally",
    "stochastic",
    "behavior",
    "even",
    "behavioral",
    "regime",
    "right",
    "data",
    "used",
    "thinking",
    "oh",
    "see",
    "sort",
    "qualitatively",
    "different",
    "behavior",
    "might",
    "identifying",
    "sort",
    "different",
    "subgroups",
    "population",
    "right",
    "maybe",
    "looking",
    "two",
    "molecules",
    "sort",
    "two",
    "types",
    "consumer",
    "whatever",
    "time",
    "series",
    "within",
    "series",
    "kind",
    "stochastic",
    "shift",
    "different",
    "regime",
    "need",
    "think",
    "identify",
    "sort",
    "change",
    "point",
    "identification",
    "okay",
    "slide",
    "autocorrelation",
    "case",
    "anyone",
    "familiar",
    "even",
    "guess",
    "main",
    "point",
    "want",
    "make",
    "always",
    "know",
    "eyeballing",
    "data",
    "going",
    "look",
    "like",
    "right",
    "might",
    "think",
    "oh",
    "sort",
    "looking",
    "top",
    "panel",
    "sense",
    "correlated",
    "things",
    "personally",
    "look",
    "lower",
    "panels",
    "series",
    "much",
    "sort",
    "self",
    "correlated",
    "correlated",
    "would",
    "thought",
    "looking",
    "right",
    "looks",
    "kind",
    "noisy",
    "maybe",
    "kind",
    "seasonality",
    "extent",
    "see",
    "acs",
    "could",
    "good",
    "keep",
    "mind",
    "data",
    "never",
    "clean",
    "enough",
    "never",
    "sort",
    "noisy",
    "enough",
    "white",
    "noise",
    "like",
    "enough",
    "beautiful",
    "enough",
    "run",
    "basic",
    "diagnostics",
    "ok",
    "special",
    "concerns",
    "time",
    "series",
    "data",
    "correlated",
    "errors",
    "right",
    "side",
    "upper",
    "side",
    "actually",
    "errors",
    "residuals",
    "time",
    "series",
    "model",
    "point",
    "say",
    "already",
    "wrote",
    "model",
    "thought",
    "counted",
    "seasonality",
    "self",
    "correlation",
    "see",
    "blue",
    "lines",
    "give",
    "us",
    "significance",
    "levels",
    "see",
    "model",
    "actually",
    "failing",
    "describe",
    "autocorrelation",
    "time",
    "series",
    "right",
    "something",
    "need",
    "keep",
    "mind",
    "time",
    "series",
    "special",
    "diagnostics",
    "need",
    "time",
    "aware",
    "addition",
    "time",
    "aware",
    "modeling",
    "need",
    "time",
    "aware",
    "diagnostics",
    "also",
    "usually",
    "look",
    "different",
    "time",
    "series",
    "context",
    "right",
    "draw",
    "attention",
    "plot",
    "lower",
    "right",
    "hand",
    "corner",
    "depends",
    "modeling",
    "nature",
    "data",
    "often",
    "trying",
    "predict",
    "future",
    "want",
    "make",
    "sure",
    "future",
    "information",
    "leak",
    "backwards",
    "time",
    "influence",
    "model",
    "make",
    "model",
    "look",
    "lot",
    "better",
    "sounds",
    "like",
    "common",
    "sense",
    "right",
    "professionals",
    "would",
    "never",
    "ever",
    "right",
    "yet",
    "many",
    "people",
    "included",
    "find",
    "embarrassed",
    "something",
    "much",
    "better",
    "training",
    "think",
    "properly",
    "cross",
    "validated",
    "roll",
    "production",
    "really",
    "ground",
    "truth",
    "really",
    "never",
    "ever",
    "ever",
    "careful",
    "enough",
    "cross",
    "validation",
    "next",
    "bullet",
    "look",
    "one",
    "thing",
    "illustrated",
    "lower",
    "right",
    "hand",
    "corner",
    "sort",
    "roll",
    "training",
    "data",
    "testing",
    "data",
    "forward",
    "time",
    "idea",
    "never",
    "want",
    "test",
    "data",
    "way",
    "older",
    "trained",
    "want",
    "information",
    "propagating",
    "backwards",
    "another",
    "thing",
    "time",
    "series",
    "temporal",
    "axes",
    "time",
    "right",
    "information",
    "really",
    "propagate",
    "one",
    "direction",
    "write",
    "models",
    "opposite",
    "right",
    "causal",
    "time",
    "series",
    "opposite",
    "information",
    "goes",
    "backwards",
    "time",
    "tend",
    "interesting",
    "modeling",
    "though",
    "right",
    "tend",
    "questions",
    "trying",
    "answer",
    "work",
    "research",
    "um",
    "notes",
    "look",
    "ahead",
    "right",
    "cross",
    "validation",
    "covered",
    "way",
    "mean",
    "avoided",
    "look",
    "ahead",
    "properly",
    "crossed",
    "validated",
    "sorts",
    "things",
    "keep",
    "mind",
    "time",
    "stamping",
    "right",
    "data",
    "available",
    "time",
    "stamped",
    "particular",
    "time",
    "input",
    "model",
    "mean",
    "time",
    "stamp",
    "reflects",
    "say",
    "would",
    "actually",
    "data",
    "right",
    "might",
    "example",
    "let",
    "say",
    "trying",
    "predict",
    "jobs",
    "numbers",
    "might",
    "sort",
    "survey",
    "people",
    "feeling",
    "date",
    "survey",
    "taken",
    "forget",
    "maybe",
    "get",
    "input",
    "system",
    "available",
    "week",
    "sort",
    "thing",
    "happen",
    "move",
    "research",
    "production",
    "suddenly",
    "realize",
    "inputs",
    "thought",
    "available",
    "actually",
    "right",
    "maybe",
    "took",
    "place",
    "past",
    "registered",
    "yet",
    "another",
    "example",
    "look",
    "ahead",
    "many",
    "far",
    "know",
    "sort",
    "automated",
    "way",
    "diagnose",
    "look",
    "ahead",
    "really",
    "keep",
    "eyes",
    "open",
    "depends",
    "lot",
    "field",
    "sometimes",
    "even",
    "issue",
    "depending",
    "field",
    "especially",
    "tricky",
    "human",
    "data",
    "okay",
    "brief",
    "overview",
    "questions",
    "comments",
    "anything",
    "people",
    "experience",
    "would",
    "want",
    "throw",
    "top",
    "know",
    "like",
    "morning",
    "oh",
    "yes",
    "somebody",
    "statistical",
    "sense",
    "frequency",
    "domain",
    "examples",
    "although",
    "speak",
    "briefly",
    "get",
    "deep",
    "learning",
    "certainly",
    "lot",
    "done",
    "good",
    "thing",
    "highlight",
    "right",
    "whole",
    "sort",
    "domain",
    "analysis",
    "looks",
    "time",
    "series",
    "frequency",
    "domain",
    "successful",
    "especially",
    "certain",
    "fields",
    "actually",
    "interested",
    "brief",
    "tutorial",
    "first",
    "tutorial",
    "posted",
    "good",
    "pointer",
    "yes",
    "wo",
    "discussing",
    "data",
    "imputation",
    "although",
    "methods",
    "use",
    "applied",
    "data",
    "imputation",
    "make",
    "sure",
    "comment",
    "go",
    "examples",
    "another",
    "given",
    "highlighted",
    "missing",
    "data",
    "often",
    "important",
    "okay",
    "okay",
    "let",
    "talk",
    "models",
    "start",
    "background",
    "box",
    "jenkins",
    "arima",
    "modeling",
    "arima",
    "modeling",
    "used",
    "probably",
    "least",
    "seen",
    "term",
    "arima",
    "maybe",
    "seen",
    "arma",
    "ar",
    "see",
    "sort",
    "class",
    "models",
    "statistical",
    "models",
    "developed",
    "long",
    "time",
    "ago",
    "point",
    "sort",
    "early",
    "early",
    "mid",
    "20th",
    "century",
    "see",
    "though",
    "despite",
    "know",
    "sort",
    "old",
    "traditional",
    "excellent",
    "performance",
    "especially",
    "small",
    "data",
    "sets",
    "example",
    "airline",
    "passenger",
    "data",
    "set",
    "famous",
    "data",
    "set",
    "look",
    "time",
    "series",
    "textbooks",
    "always",
    "see",
    "forecast",
    "versus",
    "observed",
    "pretty",
    "close",
    "right",
    "already",
    "pretty",
    "good",
    "one",
    "interesting",
    "things",
    "modern",
    "time",
    "series",
    "analysis",
    "right",
    "sort",
    "methods",
    "however",
    "want",
    "call",
    "always",
    "want",
    "make",
    "sure",
    "actually",
    "contributing",
    "something",
    "could",
    "get",
    "say",
    "arima",
    "surprisingly",
    "difficult",
    "also",
    "surprisingly",
    "difficult",
    "sometimes",
    "arena",
    "model",
    "beat",
    "even",
    "simpler",
    "model",
    "sort",
    "know",
    "model",
    "could",
    "something",
    "like",
    "predict",
    "time",
    "plus",
    "1",
    "value",
    "leave",
    "extremely",
    "good",
    "model",
    "want",
    "point",
    "starting",
    "fairly",
    "high",
    "bar",
    "especially",
    "certain",
    "domains",
    "right",
    "certain",
    "kinds",
    "data",
    "sets",
    "much",
    "noise",
    "say",
    "strong",
    "seasonal",
    "component",
    "well",
    "sort",
    "model",
    "arima",
    "model",
    "look",
    "like",
    "well",
    "want",
    "show",
    "sort",
    "one",
    "way",
    "thinking",
    "l",
    "lag",
    "operator",
    "lag",
    "operator",
    "means",
    "apply",
    "l",
    "x",
    "sub",
    "x",
    "time",
    "get",
    "x",
    "minus",
    "1",
    "apply",
    "l",
    "squared",
    "x",
    "sub",
    "get",
    "x",
    "sub",
    "minus",
    "2",
    "right",
    "way",
    "expressing",
    "moving",
    "something",
    "back",
    "time",
    "case",
    "alpha",
    "alpha",
    "sub",
    "side",
    "auto",
    "regressive",
    "coefficients",
    "theta",
    "right",
    "hand",
    "side",
    "moving",
    "average",
    "components",
    "sort",
    "traditional",
    "way",
    "express",
    "arima",
    "model",
    "sort",
    "arma",
    "model",
    "rather",
    "sort",
    "left",
    "way",
    "operating",
    "data",
    "going",
    "coefficients",
    "apply",
    "pass",
    "values",
    "right",
    "going",
    "coefficients",
    "apply",
    "sort",
    "past",
    "errors",
    "right",
    "could",
    "noise",
    "system",
    "stochastic",
    "noise",
    "could",
    "measurement",
    "noise",
    "interpret",
    "number",
    "ways",
    "idea",
    "future",
    "value",
    "way",
    "going",
    "depend",
    "past",
    "values",
    "past",
    "errors",
    "sort",
    "values",
    "choose",
    "p",
    "q",
    "summation",
    "part",
    "model",
    "tuning",
    "go",
    "arima",
    "model",
    "plus",
    "difference",
    "means",
    "additional",
    "1",
    "minus",
    "l",
    "means",
    "going",
    "difference",
    "time",
    "series",
    "means",
    "instead",
    "drawing",
    "original",
    "time",
    "series",
    "like",
    "air",
    "passengers",
    "absolute",
    "number",
    "time",
    "stamp",
    "going",
    "convert",
    "time",
    "series",
    "first",
    "difference",
    "delta",
    "time",
    "series",
    "becomes",
    "series",
    "delta",
    "delta",
    "x",
    "instead",
    "x",
    "right",
    "exact",
    "meishan",
    "lost",
    "information",
    "except",
    "maybe",
    "offset",
    "often",
    "converts",
    "time",
    "series",
    "tractable",
    "form",
    "maybe",
    "wo",
    "differences",
    "volatility",
    "time",
    "maybe",
    "wo",
    "trend",
    "maybe",
    "eligible",
    "models",
    "apply",
    "would",
    "otherwise",
    "right",
    "example",
    "emi",
    "needs",
    "stationary",
    "data",
    "often",
    "going",
    "different",
    "saying",
    "get",
    "stationary",
    "data",
    "model",
    "much",
    "going",
    "get",
    "model",
    "know",
    "yet",
    "sort",
    "need",
    "know",
    "describes",
    "future",
    "data",
    "terms",
    "past",
    "data",
    "past",
    "errors",
    "simple",
    "also",
    "mention",
    "fitting",
    "things",
    "easy",
    "fitting",
    "things",
    "bit",
    "art",
    "matter",
    "sort",
    "optimizing",
    "aka",
    "information",
    "criterion",
    "depending",
    "talk",
    "sort",
    "different",
    "schools",
    "thought",
    "everyone",
    "agrees",
    "fairly",
    "difficult",
    "problem",
    "easy",
    "sort",
    "fit",
    "easy",
    "specify",
    "overly",
    "complicated",
    "model",
    "sort",
    "thing",
    "get",
    "embarrassed",
    "later",
    "roll",
    "production",
    "turns",
    "good",
    "thought",
    "okay",
    "people",
    "happy",
    "arima",
    "despite",
    "example",
    "beautiful",
    "plot",
    "showed",
    "right",
    "already",
    "seems",
    "pretty",
    "good",
    "right",
    "say",
    "could",
    "get",
    "everything",
    "trying",
    "predict",
    "world",
    "already",
    "know",
    "enough",
    "buy",
    "house",
    "buy",
    "stocks",
    "something",
    "like",
    "lot",
    "plenty",
    "problems",
    "right",
    "think",
    "first",
    "one",
    "probably",
    "noticed",
    "took",
    "especially",
    "intuitive",
    "right",
    "showed",
    "equation",
    "equation",
    "little",
    "confusing",
    "worked",
    "sort",
    "forever",
    "top",
    "get",
    "coefficients",
    "also",
    "still",
    "kind",
    "confusing",
    "right",
    "mean",
    "know",
    "x",
    "sub",
    "minus",
    "1",
    "coefficient",
    "like",
    "difference",
    "point",
    "3",
    "point",
    "7",
    "really",
    "know",
    "especially",
    "multiple",
    "inputs",
    "right",
    "saying",
    "x",
    "sub",
    "going",
    "function",
    "x",
    "sub",
    "minus",
    "1",
    "x",
    "sub",
    "2",
    "minus",
    "2",
    "x",
    "sub",
    "2",
    "minus",
    "3",
    "coefficients",
    "really",
    "becomes",
    "kind",
    "difficult",
    "think",
    "dynamic",
    "looks",
    "like",
    "see",
    "time",
    "series",
    "textbooks",
    "mostly",
    "give",
    "examples",
    "stick",
    "something",
    "one",
    "two",
    "inputs",
    "right",
    "like",
    "ar",
    "1",
    "2",
    "model",
    "precisely",
    "calm",
    "gets",
    "difficult",
    "gets",
    "even",
    "think",
    "different",
    "behaviors",
    "coming",
    "also",
    "way",
    "build",
    "underlying",
    "understanding",
    "works",
    "right",
    "especially",
    "come",
    "discipline",
    "sort",
    "well",
    "developed",
    "set",
    "time",
    "series",
    "data",
    "maybe",
    "people",
    "staring",
    "five",
    "years",
    "hundred",
    "years",
    "right",
    "depending",
    "discipline",
    "usually",
    "knowledge",
    "system",
    "also",
    "theories",
    "people",
    "want",
    "test",
    "right",
    "might",
    "even",
    "say",
    "well",
    "enough",
    "able",
    "predict",
    "describe",
    "model",
    "kind",
    "dynamic",
    "rma",
    "whatever",
    "mean",
    "much",
    "aware",
    "say",
    "random",
    "walk",
    "element",
    "kind",
    "data",
    "aware",
    "cyclical",
    "element",
    "ca",
    "really",
    "described",
    "arima",
    "external",
    "regressors",
    "like",
    "include",
    "know",
    "get",
    "complicated",
    "models",
    "start",
    "looking",
    "models",
    "little",
    "bit",
    "intuitive",
    "also",
    "systems",
    "cycle",
    "slowly",
    "stochastically",
    "easily",
    "described",
    "arima",
    "model",
    "arima",
    "models",
    "really",
    "well",
    "sort",
    "short",
    "cycles",
    "right",
    "maybe",
    "seven",
    "days",
    "daily",
    "data",
    "things",
    "like",
    "already",
    "even",
    "getting",
    "sort",
    "24",
    "hours",
    "hourly",
    "data",
    "arima",
    "models",
    "get",
    "really",
    "ugly",
    "unlike",
    "machine",
    "learning",
    "sort",
    "throw",
    "data",
    "might",
    "get",
    "better",
    "result",
    "arima",
    "models",
    "sort",
    "level",
    "really",
    "well",
    "small",
    "data",
    "sets",
    "arguably",
    "still",
    "beat",
    "everything",
    "else",
    "give",
    "time",
    "series",
    "really",
    "well",
    "good",
    "luck",
    "tree",
    "neural",
    "network",
    "hand",
    "give",
    "points",
    "instead",
    "40",
    "clear",
    "get",
    "much",
    "better",
    "performance",
    "okay",
    "enter",
    "structural",
    "time",
    "series",
    "around",
    "quite",
    "new",
    "sort",
    "much",
    "easier",
    "implement",
    "used",
    "decades",
    "ago",
    "right",
    "get",
    "much",
    "computing",
    "power",
    "means",
    "way",
    "little",
    "bit",
    "intuitive",
    "understanding",
    "system",
    "example",
    "model",
    "level",
    "component",
    "seasonal",
    "component",
    "component",
    "call",
    "irregular",
    "sort",
    "everything",
    "else",
    "ca",
    "explained",
    "sort",
    "error",
    "term",
    "going",
    "add",
    "end",
    "arguably",
    "already",
    "get",
    "much",
    "better",
    "sense",
    "right",
    "compare",
    "sorry",
    "flip",
    "back",
    "compare",
    "right",
    "really",
    "sense",
    "models",
    "well",
    "contributing",
    "right",
    "see",
    "model",
    "describing",
    "seasonal",
    "data",
    "could",
    "look",
    "coefficients",
    "arima",
    "model",
    "try",
    "puzzle",
    "might",
    "quite",
    "difficult",
    "see",
    "feel",
    "like",
    "look",
    "say",
    "well",
    "know",
    "underlying",
    "level",
    "sort",
    "stays",
    "quite",
    "steady",
    "like",
    "sort",
    "83",
    "takes",
    "dip",
    "feel",
    "like",
    "know",
    "little",
    "bit",
    "going",
    "possibly",
    "use",
    "supporting",
    "evidence",
    "hypothesis",
    "maybe",
    "evidence",
    "goes",
    "hypothesis",
    "data",
    "behaving",
    "use",
    "structural",
    "time",
    "series",
    "interesting",
    "thing",
    "also",
    "expressed",
    "rhema",
    "form",
    "way",
    "actually",
    "anything",
    "novel",
    "actually",
    "going",
    "like",
    "sort",
    "formal",
    "mathematician",
    "contributing",
    "anything",
    "new",
    "mind",
    "really",
    "way",
    "inspecting",
    "enhancing",
    "sort",
    "human",
    "intelligence",
    "rather",
    "statistics",
    "fit",
    "via",
    "maximum",
    "likelihood",
    "common",
    "filter",
    "right",
    "mla",
    "interpretation",
    "bayesian",
    "interpretation",
    "interestingly",
    "common",
    "filters",
    "super",
    "modern",
    "believe",
    "go",
    "back",
    "time",
    "apollo",
    "mission",
    "least",
    "story",
    "everyone",
    "likes",
    "tell",
    "conferences",
    "actually",
    "wanted",
    "use",
    "different",
    "way",
    "estimate",
    "trajectory",
    "spaceship",
    "realize",
    "computing",
    "storage",
    "space",
    "need",
    "store",
    "many",
    "points",
    "time",
    "continue",
    "estimation",
    "roll",
    "something",
    "need",
    "store",
    "one",
    "data",
    "point",
    "time",
    "update",
    "part",
    "beauty",
    "something",
    "like",
    "common",
    "filter",
    "method",
    "estimate",
    "whole",
    "time",
    "series",
    "keep",
    "couple",
    "numbers",
    "around",
    "even",
    "estimating",
    "time",
    "series",
    "millions",
    "points",
    "know",
    "points",
    "something",
    "reasonable",
    "keeping",
    "one",
    "number",
    "storing",
    "whole",
    "thing",
    "really",
    "good",
    "small",
    "machineries",
    "machinery",
    "know",
    "computers",
    "60s",
    "70s",
    "still",
    "useful",
    "today",
    "largely",
    "developed",
    "econometrics",
    "econ",
    "background",
    "probably",
    "seen",
    "quite",
    "bit",
    "whereas",
    "otherwise",
    "unfortunately",
    "fairly",
    "underutilized",
    "offer",
    "insights",
    "underlying",
    "structure",
    "illustrated",
    "also",
    "possible",
    "package",
    "using",
    "packages",
    "inject",
    "bayesian",
    "analysis",
    "via",
    "priors",
    "onto",
    "parameter",
    "sort",
    "build",
    "quite",
    "extensively",
    "state",
    "seus",
    "models",
    "generally",
    "right",
    "useful",
    "states",
    "state",
    "space",
    "model",
    "also",
    "compared",
    "traditional",
    "arima",
    "models",
    "offer",
    "forecasting",
    "set",
    "coefficients",
    "sort",
    "offer",
    "three",
    "different",
    "stages",
    "inherent",
    "fitting",
    "stage",
    "number",
    "one",
    "filtering",
    "filtering",
    "remember",
    "underlying",
    "assumption",
    "sort",
    "noise",
    "data",
    "underlying",
    "components",
    "get",
    "measure",
    "one",
    "thing",
    "right",
    "filtering",
    "way",
    "time",
    "take",
    "data",
    "zero",
    "time",
    "including",
    "time",
    "use",
    "make",
    "estimate",
    "sort",
    "true",
    "value",
    "right",
    "right",
    "value",
    "seeing",
    "might",
    "fairly",
    "noisy",
    "versus",
    "true",
    "value",
    "see",
    "would",
    "useful",
    "apollo",
    "mission",
    "right",
    "actually",
    "spaceship",
    "right",
    "sensors",
    "say",
    "actually",
    "pretty",
    "important",
    "good",
    "idea",
    "next",
    "step",
    "prediction",
    "right",
    "case",
    "time",
    "gon",
    "na",
    "look",
    "plus",
    "1",
    "maybe",
    "even",
    "plus",
    "k",
    "future",
    "something",
    "else",
    "state",
    "space",
    "model",
    "right",
    "update",
    "think",
    "think",
    "future",
    "finally",
    "smoothing",
    "essentially",
    "data",
    "data",
    "relevance",
    "say",
    "big",
    "want",
    "go",
    "back",
    "earlier",
    "time",
    "point",
    "say",
    "well",
    "time",
    "10",
    "whatever",
    "time",
    "10",
    "thought",
    "know",
    "point",
    "see",
    "ended",
    "end",
    "realized",
    "even",
    "get",
    "additional",
    "information",
    "sort",
    "back",
    "propagate",
    "get",
    "even",
    "better",
    "estimation",
    "time",
    "called",
    "smoothing",
    "functionality",
    "filtering",
    "smoothing",
    "get",
    "arima",
    "model",
    "right",
    "actually",
    "new",
    "technology",
    "perspective",
    "common",
    "filter",
    "thought",
    "everyone",
    "code",
    "decided",
    "best",
    "use",
    "time",
    "sort",
    "give",
    "overview",
    "sort",
    "high",
    "level",
    "overview",
    "wiki",
    "graphic",
    "works",
    "start",
    "sort",
    "prior",
    "knowledge",
    "usually",
    "sort",
    "vanilla",
    "gaussian",
    "distribution",
    "underlying",
    "state",
    "model",
    "update",
    "underlying",
    "state",
    "make",
    "prediction",
    "think",
    "going",
    "based",
    "knowledge",
    "system",
    "dynamics",
    "current",
    "trajectory",
    "make",
    "prediction",
    "use",
    "update",
    "right",
    "compare",
    "prediction",
    "right",
    "time",
    "measured",
    "time",
    "time",
    "minus",
    "1",
    "state",
    "world",
    "saw",
    "predicted",
    "time",
    "series",
    "would",
    "go",
    "way",
    "measurement",
    "right",
    "measurement",
    "would",
    "go",
    "way",
    "time",
    "got",
    "time",
    "got",
    "measurement",
    "measurement",
    "actually",
    "gone",
    "way",
    "right",
    "becomes",
    "way",
    "balancing",
    "well",
    "forecast",
    "totally",
    "meaningless",
    "right",
    "discard",
    "new",
    "data",
    "balance",
    "predicting",
    "versus",
    "actually",
    "measured",
    "different",
    "combine",
    "something",
    "reasonable",
    "rather",
    "picking",
    "one",
    "update",
    "step",
    "go",
    "back",
    "next",
    "time",
    "step",
    "sort",
    "forward",
    "propagation",
    "going",
    "backward",
    "time",
    "smoothing",
    "similar",
    "computationally",
    "separate",
    "cycle",
    "common",
    "filter",
    "underlying",
    "model",
    "something",
    "looks",
    "like",
    "something",
    "like",
    "state",
    "space",
    "underlying",
    "state",
    "measure",
    "right",
    "underlying",
    "state",
    "x",
    "going",
    "sort",
    "model",
    "based",
    "side",
    "equation",
    "f",
    "sub",
    "k",
    "times",
    "x",
    "times",
    "k",
    "minus",
    "1",
    "update",
    "state",
    "based",
    "earlier",
    "looks",
    "like",
    "lot",
    "like",
    "auto",
    "regressive",
    "model",
    "hand",
    "also",
    "option",
    "inputs",
    "k",
    "times",
    "uk",
    "would",
    "knowledge",
    "happening",
    "time",
    "time",
    "minus",
    "1",
    "right",
    "example",
    "case",
    "apollo",
    "x",
    "sub",
    "k",
    "minus",
    "1",
    "thought",
    "apollo",
    "time",
    "k",
    "minus",
    "1",
    "u",
    "sub",
    "k",
    "well",
    "motor",
    "rocket",
    "like",
    "positioned",
    "propulsion",
    "system",
    "moment",
    "even",
    "described",
    "u",
    "sub",
    "k",
    "right",
    "also",
    "know",
    "sort",
    "astron",
    "factor",
    "well",
    "sort",
    "error",
    "terms",
    "description",
    "underlying",
    "state",
    "measurement",
    "cause",
    "sub",
    "k",
    "right",
    "recognize",
    "way",
    "measurement",
    "might",
    "directly",
    "x",
    "sub",
    "k",
    "right",
    "apollo",
    "example",
    "state",
    "position",
    "measure",
    "know",
    "thing",
    "sort",
    "translation",
    "could",
    "also",
    "things",
    "space",
    "right",
    "could",
    "actually",
    "x",
    "maybe",
    "measuring",
    "velocity",
    "acceleration",
    "positioned",
    "somehow",
    "observation",
    "get",
    "position",
    "trying",
    "find",
    "way",
    "translate",
    "point",
    "want",
    "make",
    "really",
    "flexible",
    "describe",
    "sorts",
    "dynamics",
    "require",
    "kind",
    "hypothesis",
    "dynamics",
    "system",
    "really",
    "interesting",
    "right",
    "might",
    "want",
    "throw",
    "model",
    "say",
    "really",
    "noisy",
    "stock",
    "data",
    "sort",
    "like",
    "idea",
    "like",
    "let",
    "make",
    "bottle",
    "put",
    "probably",
    "wo",
    "get",
    "sensible",
    "outputs",
    "model",
    "like",
    "rocket",
    "newtonian",
    "physics",
    "flight",
    "plan",
    "makes",
    "really",
    "good",
    "case",
    "okay",
    "spelled",
    "detail",
    "usually",
    "wo",
    "code",
    "good",
    "feel",
    "empowered",
    "able",
    "looks",
    "really",
    "awful",
    "actually",
    "bunch",
    "matrix",
    "multiplication",
    "inversion",
    "good",
    "remember",
    "look",
    "code",
    "want",
    "okay",
    "models",
    "tend",
    "get",
    "evaluated",
    "usually",
    "k",
    "key",
    "information",
    "criterion",
    "aic",
    "anyone",
    "familiar",
    "want",
    "put",
    "two",
    "times",
    "number",
    "parameters",
    "minus",
    "two",
    "times",
    "log",
    "likelihood",
    "right",
    "want",
    "negative",
    "evaluating",
    "models",
    "one",
    "way",
    "would",
    "might",
    "evaluate",
    "models",
    "models",
    "okay",
    "models",
    "going",
    "look",
    "jupiter",
    "notebook",
    "right",
    "gon",
    "na",
    "look",
    "things",
    "like",
    "local",
    "linear",
    "trend",
    "smooth",
    "trends",
    "pulled",
    "directly",
    "stats",
    "models",
    "package",
    "unobserved",
    "components",
    "model",
    "let",
    "see",
    "see",
    "yes",
    "okay",
    "give",
    "either",
    "two",
    "ways",
    "select",
    "either",
    "inputs",
    "tells",
    "sort",
    "underlying",
    "model",
    "using",
    "case",
    "underlying",
    "model",
    "observable",
    "function",
    "underlying",
    "state",
    "plus",
    "epsilon",
    "kind",
    "error",
    "underlying",
    "state",
    "function",
    "former",
    "state",
    "also",
    "sort",
    "velocity",
    "type",
    "terms",
    "kind",
    "trend",
    "term",
    "moving",
    "beta",
    "plus",
    "sort",
    "error",
    "beta",
    "change",
    "local",
    "linear",
    "trend",
    "take",
    "look",
    "smooth",
    "trend",
    "see",
    "spot",
    "difference",
    "look",
    "quite",
    "similar",
    "one",
    "difference",
    "difference",
    "yeah",
    "right",
    "stochastic",
    "term",
    "smooth",
    "trend",
    "middle",
    "equation",
    "stochastic",
    "term",
    "right",
    "give",
    "sense",
    "distinctions",
    "people",
    "make",
    "people",
    "sort",
    "make",
    "slight",
    "tweaks",
    "different",
    "names",
    "always",
    "best",
    "go",
    "look",
    "equations",
    "see",
    "look",
    "like",
    "okay",
    "bored",
    "pants",
    "bunch",
    "slides",
    "think",
    "time",
    "open",
    "first",
    "jupiter",
    "notebook",
    "okay",
    "people",
    "feel",
    "free",
    "use",
    "either",
    "instructor",
    "student",
    "version",
    "student",
    "version",
    "want",
    "able",
    "cheat",
    "know",
    "good",
    "way",
    "discipline",
    "judge",
    "makes",
    "sense",
    "okay",
    "first",
    "thing",
    "need",
    "obtain",
    "visualize",
    "data",
    "case",
    "downloaded",
    "two",
    "different",
    "measurements",
    "global",
    "temperature",
    "data",
    "yes",
    "yes",
    "okay",
    "thank",
    "excellent",
    "point",
    "base",
    "folder",
    "state",
    "state",
    "state",
    "space",
    "models",
    "folder",
    "okay",
    "sorry",
    "anyone",
    "walked",
    "late",
    "worries",
    "set",
    "jupiter",
    "notebook",
    "slides",
    "available",
    "slack",
    "channel",
    "zip",
    "file",
    "otherwise",
    "someone",
    "helpfully",
    "posted",
    "see",
    "older",
    "version",
    "sorry",
    "okay",
    "really",
    "appreciate",
    "want",
    "confirm",
    "people",
    "student",
    "instructor",
    "version",
    "let",
    "see",
    "scroll",
    "slack",
    "sent",
    "913",
    "last",
    "night",
    "second",
    "version",
    "rather",
    "one",
    "pinned",
    "upload",
    "newest",
    "thing",
    "okay",
    "newest",
    "thing",
    "case",
    "email",
    "syfy",
    "calm",
    "tell",
    "almost",
    "instantly",
    "reply",
    "think",
    "someone",
    "manning",
    "continuously",
    "questions",
    "okay",
    "okay",
    "downloading",
    "pretty",
    "small",
    "zip",
    "file",
    "think",
    "pretty",
    "quickly",
    "gon",
    "na",
    "slowly",
    "look",
    "data",
    "caught",
    "pretty",
    "quick",
    "oh",
    "github",
    "updated",
    "okay",
    "great",
    "thank",
    "github",
    "also",
    "updated",
    "find",
    "useful",
    "okay",
    "okay",
    "obtain",
    "visualize",
    "data",
    "csv",
    "available",
    "zip",
    "file",
    "source",
    "listed",
    "two",
    "sources",
    "global",
    "temperature",
    "data",
    "obviously",
    "climate",
    "change",
    "data",
    "great",
    "source",
    "arguing",
    "time",
    "series",
    "great",
    "source",
    "possibly",
    "ambiguous",
    "time",
    "series",
    "depending",
    "politics",
    "rich",
    "source",
    "debate",
    "grabbed",
    "point",
    "data",
    "set",
    "sort",
    "two",
    "sources",
    "data",
    "compare",
    "load",
    "data",
    "take",
    "look",
    "first",
    "thing",
    "plot",
    "know",
    "basic",
    "time",
    "series",
    "plot",
    "also",
    "recommend",
    "sometimes",
    "plot",
    "whole",
    "dataset",
    "right",
    "example",
    "looked",
    "first",
    "hundred",
    "data",
    "points",
    "anyone",
    "idea",
    "recommend",
    "might",
    "feel",
    "versus",
    "let",
    "look",
    "one",
    "yes",
    "let",
    "see",
    "maybe",
    "man",
    "okay",
    "especially",
    "satisfied",
    "plots",
    "anyone",
    "feel",
    "satisfied",
    "plots",
    "hope",
    "okay",
    "actually",
    "first",
    "exercise",
    "gon",
    "na",
    "scroll",
    "want",
    "secret",
    "figure",
    "first",
    "coding",
    "exercise",
    "everyone",
    "like",
    "take",
    "couple",
    "minutes",
    "figure",
    "wrong",
    "way",
    "data",
    "looks",
    "plot",
    "fix",
    "even",
    "aesthetic",
    "problem",
    "error",
    "right",
    "would",
    "one",
    "presented",
    "boss",
    "really",
    "embarrassing",
    "kind",
    "situations",
    "think",
    "could",
    "correct",
    "okay",
    "think",
    "already",
    "covered",
    "problems",
    "right",
    "got",
    "data",
    "types",
    "especially",
    "helpful",
    "actually",
    "combined",
    "different",
    "measurements",
    "one",
    "time",
    "series",
    "actually",
    "make",
    "sense",
    "little",
    "wary",
    "given",
    "already",
    "even",
    "plot",
    "time",
    "displaying",
    "right",
    "sort",
    "random",
    "index",
    "first",
    "thing",
    "gon",
    "na",
    "gon",
    "na",
    "pivot",
    "data",
    "plot",
    "data",
    "incidentally",
    "seems",
    "like",
    "reordered",
    "date",
    "index",
    "appropriately",
    "seeing",
    "something",
    "like",
    "expect",
    "looking",
    "one",
    "type",
    "one",
    "time",
    "series",
    "instead",
    "two",
    "right",
    "actually",
    "two",
    "time",
    "series",
    "data",
    "set",
    "plot",
    "one",
    "see",
    "right",
    "unfortunately",
    "often",
    "something",
    "tricky",
    "time",
    "series",
    "right",
    "plotting",
    "wrong",
    "way",
    "still",
    "look",
    "reasonable",
    "right",
    "especially",
    "domain",
    "knowledge",
    "sort",
    "global",
    "warming",
    "would",
    "looked",
    "fine",
    "maybe",
    "would",
    "foolishly",
    "carried",
    "another",
    "problem",
    "though",
    "still",
    "dates",
    "seem",
    "order",
    "right",
    "could",
    "check",
    "extensively",
    "still",
    "useful",
    "index",
    "whereas",
    "would",
    "like",
    "something",
    "little",
    "useful",
    "let",
    "go",
    "wo",
    "exercise",
    "make",
    "index",
    "time",
    "aware",
    "well",
    "nicely",
    "pandas",
    "provides",
    "date",
    "time",
    "also",
    "period",
    "time",
    "indexes",
    "gon",
    "na",
    "use",
    "date",
    "time",
    "see",
    "date",
    "time",
    "index",
    "instead",
    "base",
    "index",
    "class",
    "get",
    "well",
    "already",
    "notice",
    "compared",
    "plot",
    "already",
    "thought",
    "okay",
    "got",
    "pot",
    "treated",
    "strings",
    "treated",
    "actual",
    "dates",
    "pandas",
    "fill",
    "little",
    "bit",
    "background",
    "knowledge",
    "sort",
    "labeling",
    "would",
    "reasonable",
    "also",
    "get",
    "sort",
    "handy",
    "label",
    "indexing",
    "like",
    "index",
    "year",
    "right",
    "1880",
    "going",
    "show",
    "1880s",
    "also",
    "going",
    "make",
    "life",
    "easier",
    "plotting",
    "like",
    "plot",
    "series",
    "1880",
    "1950",
    "get",
    "sense",
    "correlated",
    "plot",
    "everything",
    "1950",
    "onwards",
    "get",
    "nice",
    "functionality",
    "date",
    "time",
    "reminder",
    "available",
    "helpful",
    "obviously",
    "always",
    "look",
    "carefully",
    "data",
    "get",
    "started",
    "okay",
    "quick",
    "exercise",
    "strongly",
    "measurements",
    "correlate",
    "contemporaneously",
    "time",
    "lag",
    "got",
    "two",
    "groups",
    "data",
    "quickly",
    "two",
    "minutes",
    "helpful",
    "predicting",
    "one",
    "another",
    "understanding",
    "one",
    "another",
    "depend",
    "lot",
    "one",
    "package",
    "another",
    "guess",
    "one",
    "critique",
    "would",
    "even",
    "python",
    "spaces",
    "lot",
    "uniformity",
    "around",
    "functionality",
    "spread",
    "among",
    "many",
    "packages",
    "anti",
    "python",
    "python",
    "first",
    "love",
    "actually",
    "better",
    "job",
    "unified",
    "interface",
    "okay",
    "strongly",
    "measurements",
    "correlate",
    "contemporaneously",
    "sorts",
    "ways",
    "think",
    "want",
    "point",
    "know",
    "simple",
    "things",
    "like",
    "scatter",
    "plots",
    "right",
    "plotting",
    "two",
    "measurements",
    "gc",
    "g",
    "g",
    "temp",
    "one",
    "another",
    "exact",
    "time",
    "period",
    "given",
    "time",
    "seem",
    "correlate",
    "pretty",
    "strongly",
    "know",
    "saw",
    "something",
    "like",
    "might",
    "work",
    "often",
    "would",
    "delighted",
    "similarly",
    "ask",
    "whether",
    "years",
    "offset",
    "right",
    "sort",
    "predictive",
    "value",
    "case",
    "plotting",
    "1880",
    "1899",
    "one",
    "variable",
    "versus",
    "1881",
    "1900",
    "offset",
    "year",
    "look",
    "like",
    "inste",
    "look",
    "like",
    "real",
    "world",
    "see",
    "job",
    "anyway",
    "terrible",
    "well",
    "depends",
    "domain",
    "right",
    "maybe",
    "predicting",
    "air",
    "passengers",
    "would",
    "terrible",
    "maybe",
    "predicting",
    "housing",
    "prices",
    "bad",
    "look",
    "pearson",
    "r",
    "see",
    "zero",
    "right",
    "mean",
    "actually",
    "probably",
    "looks",
    "worse",
    "also",
    "good",
    "different",
    "measures",
    "visual",
    "numeric",
    "reminder",
    "sort",
    "look",
    "data",
    "standard",
    "standard",
    "tools",
    "pandas",
    "want",
    "get",
    "idea",
    "date",
    "range",
    "look",
    "min",
    "max",
    "index",
    "right",
    "sorts",
    "standard",
    "operations",
    "okay",
    "going",
    "throw",
    "unobserved",
    "component",
    "model",
    "interested",
    "training",
    "data",
    "1960",
    "onwards",
    "personal",
    "choice",
    "thought",
    "little",
    "bit",
    "interesting",
    "sort",
    "see",
    "things",
    "taking",
    "going",
    "define",
    "model",
    "model",
    "per",
    "gon",
    "na",
    "stay",
    "say",
    "want",
    "smooth",
    "trend",
    "level",
    "want",
    "cycle",
    "seasonality",
    "sort",
    "first",
    "pass",
    "based",
    "clots",
    "right",
    "looking",
    "sort",
    "1960",
    "onwards",
    "like",
    "eh",
    "know",
    "seem",
    "much",
    "cycling",
    "seasonality",
    "arguably",
    "could",
    "careful",
    "gon",
    "na",
    "first",
    "pass",
    "build",
    "model",
    "dictionary",
    "fit",
    "model",
    "gon",
    "na",
    "make",
    "unobserved",
    "components",
    "object",
    "stats",
    "models",
    "right",
    "stats",
    "models",
    "whole",
    "time",
    "series",
    "analysis",
    "api",
    "including",
    "state",
    "space",
    "api",
    "unobserved",
    "components",
    "give",
    "data",
    "give",
    "model",
    "unpack",
    "right",
    "sort",
    "parameters",
    "tweak",
    "read",
    "documentation",
    "simple",
    "first",
    "pass",
    "gon",
    "na",
    "fit",
    "model",
    "gon",
    "na",
    "plot",
    "components",
    "right",
    "talking",
    "like",
    "great",
    "thing",
    "compared",
    "arima",
    "okay",
    "plotted",
    "components",
    "see",
    "got",
    "predicted",
    "versus",
    "observed",
    "plot",
    "ah",
    "feel",
    "fill",
    "seems",
    "like",
    "always",
    "within",
    "confidence",
    "interval",
    "seems",
    "follow",
    "pretty",
    "well",
    "one",
    "step",
    "ahead",
    "prediction",
    "right",
    "also",
    "would",
    "want",
    "think",
    "whether",
    "sort",
    "interesting",
    "interesting",
    "predict",
    "global",
    "temperatures",
    "one",
    "month",
    "advance",
    "would",
    "like",
    "try",
    "try",
    "also",
    "look",
    "two",
    "components",
    "right",
    "level",
    "component",
    "trend",
    "component",
    "think",
    "look",
    "compared",
    "showed",
    "powerpoint",
    "would",
    "say",
    "great",
    "right",
    "sort",
    "look",
    "like",
    "wiggly",
    "lines",
    "would",
    "say",
    "get",
    "whole",
    "lot",
    "insight",
    "look",
    "say",
    "hmm",
    "level",
    "versus",
    "trend",
    "mean",
    "looks",
    "like",
    "hmm",
    "okay",
    "model",
    "mostly",
    "mostly",
    "gives",
    "absolute",
    "value",
    "time",
    "series",
    "level",
    "almost",
    "like",
    "trend",
    "whatever",
    "left",
    "look",
    "like",
    "meaningful",
    "trend",
    "thinking",
    "think",
    "okay",
    "well",
    "let",
    "revisit",
    "revisit",
    "couple",
    "steps",
    "might",
    "also",
    "want",
    "plot",
    "predictions",
    "also",
    "set",
    "greater",
    "time",
    "horizon",
    "end",
    "dynamic",
    "prediction",
    "move",
    "next",
    "cell",
    "set",
    "number",
    "steps",
    "want",
    "predict",
    "forward",
    "20",
    "20",
    "months",
    "arbitrarily",
    "chosen",
    "want",
    "get",
    "predictions",
    "want",
    "last",
    "20",
    "steps",
    "dynamic",
    "meaning",
    "going",
    "update",
    "step",
    "right",
    "one",
    "step",
    "ahead",
    "forecasting",
    "last",
    "20",
    "steps",
    "gon",
    "na",
    "roll",
    "20",
    "steps",
    "gon",
    "na",
    "take",
    "predicted",
    "new",
    "input",
    "take",
    "predicted",
    "new",
    "input",
    "gon",
    "na",
    "move",
    "forward",
    "get",
    "want",
    "draw",
    "attention",
    "end",
    "notice",
    "end",
    "sort",
    "upper",
    "corner",
    "sort",
    "flat",
    "line",
    "right",
    "another",
    "thing",
    "gon",
    "na",
    "notice",
    "models",
    "aware",
    "right",
    "sort",
    "horizons",
    "great",
    "sense",
    "since",
    "error",
    "assume",
    "error",
    "gon",
    "na",
    "sort",
    "keep",
    "going",
    "along",
    "whatever",
    "smooth",
    "trajectory",
    "established",
    "underlying",
    "model",
    "good",
    "thing",
    "bad",
    "thing",
    "depending",
    "want",
    "certainly",
    "look",
    "little",
    "bit",
    "different",
    "say",
    "arima",
    "model",
    "sort",
    "see",
    "wiggles",
    "things",
    "like",
    "okay",
    "model",
    "well",
    "looks",
    "pretty",
    "good",
    "one",
    "step",
    "ahead",
    "right",
    "really",
    "decide",
    "really",
    "good",
    "based",
    "one",
    "one",
    "plot",
    "well",
    "especially",
    "comparing",
    "would",
    "null",
    "model",
    "look",
    "like",
    "right",
    "need",
    "think",
    "null",
    "model",
    "would",
    "look",
    "like",
    "also",
    "cleaner",
    "plot",
    "illustrate",
    "want",
    "put",
    "confidence",
    "bounds",
    "red",
    "showing",
    "dynamic",
    "prediction",
    "dotted",
    "lines",
    "showing",
    "confidence",
    "bounds",
    "look",
    "fantastic",
    "either",
    "right",
    "another",
    "difference",
    "say",
    "compared",
    "used",
    "arima",
    "model",
    "confidence",
    "bounds",
    "rapidly",
    "diverge",
    "unincorporated",
    "error",
    "longer",
    "able",
    "handle",
    "look",
    "also",
    "get",
    "sense",
    "see",
    "discussed",
    "right",
    "forward",
    "moving",
    "prediction",
    "going",
    "flat",
    "line",
    "simple",
    "models",
    "okay",
    "next",
    "exercise",
    "consider",
    "adding",
    "seasonal",
    "term",
    "twelve",
    "periods",
    "model",
    "fit",
    "improve",
    "fit",
    "model",
    "initially",
    "rejected",
    "seasonality",
    "revisit",
    "sure",
    "go",
    "stats",
    "models",
    "api",
    "google",
    "look",
    "unobserved",
    "components",
    "documentation",
    "let",
    "take",
    "couple",
    "minutes",
    "look",
    "also",
    "posted",
    "slides",
    "request",
    "post",
    "slides",
    "also",
    "time",
    "series",
    "channel",
    "ok",
    "add",
    "seasonal",
    "term",
    "keep",
    "local",
    "linear",
    "trend",
    "add",
    "seasonal",
    "parameter",
    "set",
    "twelve",
    "twelve",
    "monthly",
    "data",
    "right",
    "anything",
    "else",
    "would",
    "expect",
    "right",
    "earth",
    "seasonality",
    "would",
    "guess",
    "apply",
    "components",
    "see",
    "something",
    "interesting",
    "alright",
    "see",
    "looks",
    "like",
    "regular",
    "seasonal",
    "component",
    "shows",
    "silly",
    "done",
    "exploration",
    "data",
    "start",
    "beginning",
    "think",
    "kind",
    "model",
    "would",
    "appropriate",
    "also",
    "see",
    "sort",
    "trend",
    "component",
    "gone",
    "negligible",
    "thing",
    "really",
    "adding",
    "model",
    "level",
    "component",
    "seasonal",
    "component",
    "already",
    "seems",
    "pretty",
    "good",
    "job",
    "compare",
    "original",
    "model",
    "wanted",
    "compare",
    "might",
    "think",
    "example",
    "comparing",
    "correlation",
    "two",
    "models",
    "data",
    "see",
    "sort",
    "indistinguishable",
    "right",
    "mind",
    "helpful",
    "metric",
    "shows",
    "skeptical",
    "visually",
    "clearly",
    "one",
    "better",
    "job",
    "describing",
    "data",
    "offering",
    "intuition",
    "us",
    "shows",
    "sometimes",
    "numerical",
    "measurements",
    "might",
    "informative",
    "look",
    "mean",
    "absolute",
    "error",
    "case",
    "cases",
    "see",
    "new",
    "model",
    "slightly",
    "better",
    "yes",
    "okay",
    "look",
    "go",
    "hopefully",
    "notebook",
    "well",
    "seasonal",
    "component",
    "seasonal",
    "component",
    "zero",
    "clearly",
    "significant",
    "showing",
    "sort",
    "pattern",
    "makes",
    "sense",
    "based",
    "domain",
    "knowledge",
    "kind",
    "cycle",
    "earth",
    "temperature",
    "okay",
    "glad",
    "people",
    "interrupting",
    "whenever",
    "helpful",
    "okay",
    "point",
    "especially",
    "structural",
    "models",
    "people",
    "sometimes",
    "try",
    "optimize",
    "looking",
    "something",
    "like",
    "aic",
    "great",
    "least",
    "sort",
    "measure",
    "arguably",
    "especially",
    "structural",
    "models",
    "mainly",
    "sort",
    "helping",
    "intuition",
    "helping",
    "understand",
    "underlying",
    "dynamics",
    "might",
    "find",
    "model",
    "looks",
    "much",
    "worse",
    "visually",
    "offering",
    "intuition",
    "much",
    "worse",
    "right",
    "offering",
    "value",
    "big",
    "part",
    "thinking",
    "use",
    "models",
    "use",
    "teaching",
    "data",
    "really",
    "interesting",
    "case",
    "see",
    "illustration",
    "exploring",
    "models",
    "pointed",
    "something",
    "sort",
    "initially",
    "missed",
    "data",
    "insufficient",
    "exploration",
    "another",
    "way",
    "better",
    "rhema",
    "example",
    "inner",
    "rhema",
    "might",
    "notice",
    "model",
    "fitting",
    "data",
    "well",
    "wo",
    "get",
    "easy",
    "insights",
    "versus",
    "say",
    "oh",
    "look",
    "boom",
    "added",
    "seasonal",
    "term",
    "actually",
    "see",
    "seasonality",
    "rather",
    "seeing",
    "coefficient",
    "okay",
    "let",
    "explore",
    "seasonality",
    "something",
    "else",
    "see",
    "see",
    "trend",
    "component",
    "especially",
    "useful",
    "get",
    "get",
    "rid",
    "together",
    "switched",
    "local",
    "level",
    "model",
    "instead",
    "local",
    "trend",
    "model",
    "keep",
    "seasonality",
    "really",
    "get",
    "something",
    "looks",
    "really",
    "beautiful",
    "terms",
    "extraneous",
    "portions",
    "model",
    "like",
    "whole",
    "trend",
    "component",
    "really",
    "need",
    "also",
    "see",
    "seasonal",
    "component",
    "strong",
    "signal",
    "interestingly",
    "varying",
    "right",
    "sort",
    "uniform",
    "see",
    "sort",
    "level",
    "drifting",
    "attending",
    "drift",
    "positive",
    "direction",
    "right",
    "extent",
    "want",
    "debates",
    "climate",
    "change",
    "one",
    "insight",
    "like",
    "oh",
    "fit",
    "kind",
    "model",
    "see",
    "gradually",
    "level",
    "going",
    "see",
    "seasonality",
    "sort",
    "way",
    "talk",
    "data",
    "first",
    "getting",
    "question",
    "back",
    "okay",
    "okay",
    "sort",
    "look",
    "mean",
    "absolute",
    "error",
    "see",
    "continuing",
    "improve",
    "sort",
    "small",
    "incremental",
    "improvements",
    "massive",
    "improvements",
    "mainly",
    "parsimonious",
    "model",
    "gives",
    "us",
    "better",
    "intuition",
    "data",
    "okay",
    "see",
    "realize",
    "really",
    "revisited",
    "data",
    "thought",
    "shape",
    "right",
    "sort",
    "plot",
    "see",
    "seasonality",
    "looking",
    "closely",
    "although",
    "see",
    "clean",
    "seasonality",
    "picture",
    "saw",
    "say",
    "air",
    "passengers",
    "data",
    "right",
    "noisier",
    "seasonality",
    "yet",
    "structural",
    "model",
    "pretty",
    "good",
    "job",
    "capturing",
    "nonetheless",
    "okay",
    "final",
    "exercise",
    "going",
    "take",
    "break",
    "common",
    "null",
    "model",
    "time",
    "series",
    "predict",
    "value",
    "time",
    "minus",
    "one",
    "value",
    "time",
    "model",
    "compare",
    "models",
    "fit",
    "take",
    "two",
    "minutes",
    "compare",
    "right",
    "actually",
    "done",
    "anything",
    "sort",
    "fancy",
    "structural",
    "time",
    "series",
    "model",
    "kidding",
    "something",
    "always",
    "asking",
    "fancy",
    "methods",
    "yeah",
    "exactly",
    "basically",
    "coefficient",
    "4",
    "12",
    "11",
    "different",
    "coefficients",
    "right",
    "need",
    "12",
    "right",
    "need",
    "n",
    "minus",
    "1",
    "n",
    "seasons",
    "coefficient",
    "per",
    "say",
    "oh",
    "point",
    "add",
    "subtract",
    "much",
    "get",
    "seasonal",
    "component",
    "yeah",
    "handle",
    "something",
    "like",
    "incomplete",
    "cycle",
    "well",
    "really",
    "saying",
    "incomplete",
    "seasonal",
    "cycle",
    "compared",
    "psycho",
    "structural",
    "time",
    "series",
    "also",
    "handle",
    "cycle",
    "cycle",
    "sort",
    "repeated",
    "pattern",
    "regular",
    "season",
    "season",
    "way",
    "models",
    "fit",
    "sort",
    "different",
    "component",
    "per",
    "season",
    "11",
    "components",
    "describe",
    "12",
    "season",
    "model",
    "right",
    "one",
    "give",
    "null",
    "component",
    "like",
    "everything",
    "every",
    "kind",
    "model",
    "want",
    "fit",
    "cyclical",
    "data",
    "usually",
    "fit",
    "sort",
    "sine",
    "curve",
    "trying",
    "fit",
    "determine",
    "appropriate",
    "frequency",
    "model",
    "data",
    "also",
    "add",
    "things",
    "like",
    "damping",
    "parameter",
    "sort",
    "start",
    "cycle",
    "usually",
    "want",
    "want",
    "frequency",
    "time",
    "dependent",
    "unlike",
    "season",
    "frequency",
    "fixed",
    "also",
    "option",
    "either",
    "physicists",
    "remember",
    "sort",
    "like",
    "damping",
    "envelope",
    "something",
    "else",
    "fit",
    "see",
    "data",
    "right",
    "data",
    "arguably",
    "see",
    "kind",
    "seasonal",
    "case",
    "seasonal",
    "really",
    "like",
    "sub",
    "1",
    "sub",
    "2",
    "sub",
    "3",
    "assume",
    "f",
    "sub",
    "1",
    "start",
    "actually",
    "care",
    "consider",
    "first",
    "one",
    "take",
    "long",
    "goes",
    "around",
    "circle",
    "good",
    "okay",
    "null",
    "model",
    "compare",
    "yes",
    "absolutely",
    "even",
    "problem",
    "arima",
    "models",
    "right",
    "seems",
    "sort",
    "traditional",
    "statistical",
    "interesting",
    "example",
    "back",
    "rhema",
    "come",
    "back",
    "case",
    "actually",
    "models",
    "look",
    "different",
    "look",
    "like",
    "different",
    "parameters",
    "actually",
    "polynomial",
    "math",
    "factor",
    "realize",
    "like",
    "sort",
    "extra",
    "terms",
    "literally",
    "cancel",
    "literally",
    "fitting",
    "model",
    "math",
    "tells",
    "complicated",
    "like",
    "model",
    "makes",
    "sense",
    "mathematically",
    "even",
    "simpler",
    "method",
    "like",
    "structural",
    "time",
    "series",
    "consider",
    "useful",
    "sort",
    "exploratory",
    "tool",
    "something",
    "would",
    "fly",
    "airplane",
    "treat",
    "cancer",
    "based",
    "outcome",
    "many",
    "khan",
    "academy",
    "trish",
    "ins",
    "might",
    "feel",
    "little",
    "bit",
    "differently",
    "data",
    "better",
    "move",
    "bayesian",
    "model",
    "much",
    "stronger",
    "prior",
    "also",
    "better",
    "actually",
    "google",
    "great",
    "bayesian",
    "structural",
    "time",
    "series",
    "package",
    "least",
    "stronger",
    "inputs",
    "want",
    "inject",
    "say",
    "strong",
    "prior",
    "bayesian",
    "sense",
    "would",
    "trust",
    "little",
    "bit",
    "versus",
    "using",
    "default",
    "gaussian",
    "rolling",
    "absolutely",
    "easy",
    "overfit",
    "like",
    "many",
    "sort",
    "data",
    "intensive",
    "computation",
    "intensive",
    "models",
    "guarantee",
    "also",
    "get",
    "optimum",
    "fit",
    "data",
    "arguably",
    "even",
    "good",
    "thing",
    "right",
    "getting",
    "optimum",
    "fit",
    "data",
    "could",
    "overfitting",
    "well",
    "yes",
    "depends",
    "discipline",
    "certain",
    "disciplines",
    "strong",
    "priors",
    "bayesian",
    "sense",
    "shape",
    "distribution",
    "definition",
    "model",
    "example",
    "modeling",
    "stock",
    "market",
    "make",
    "strong",
    "case",
    "using",
    "anything",
    "stronger",
    "say",
    "random",
    "walk",
    "justification",
    "another",
    "example",
    "know",
    "people",
    "modeling",
    "hydrology",
    "feel",
    "strongly",
    "local",
    "level",
    "model",
    "without",
    "trend",
    "tends",
    "come",
    "domain",
    "knowledge",
    "theories",
    "causation",
    "data",
    "seeing",
    "yeah",
    "case",
    "exploring",
    "climate",
    "change",
    "experts",
    "example",
    "would",
    "would",
    "skeptical",
    "someone",
    "went",
    "congress",
    "said",
    "look",
    "figured",
    "example",
    "okay",
    "okay",
    "quickly",
    "take",
    "quick",
    "break",
    "looking",
    "null",
    "model",
    "compared",
    "model",
    "consider",
    "correlation",
    "measurably",
    "different",
    "right",
    "done",
    "work",
    "fancy",
    "model",
    "clear",
    "done",
    "much",
    "better",
    "maybe",
    "done",
    "tiny",
    "bit",
    "better",
    "maybe",
    "matters",
    "discussing",
    "since",
    "possible",
    "overfit",
    "models",
    "many",
    "knobs",
    "turn",
    "look",
    "skepticism",
    "right",
    "whether",
    "good",
    "model",
    "depend",
    "purposes",
    "whether",
    "trying",
    "interpret",
    "trying",
    "make",
    "predictions",
    "sort",
    "accuracy",
    "need",
    "right",
    "half",
    "percentage",
    "point",
    "improvement",
    "correlation",
    "meaningful",
    "field",
    "context",
    "dependent",
    "otherwise",
    "mean",
    "someone",
    "boss",
    "come",
    "obviously",
    "first",
    "thing",
    "shoot",
    "say",
    "really",
    "need",
    "barely",
    "improved",
    "null",
    "model",
    "explain",
    "terms",
    "absolute",
    "error",
    "well",
    "quite",
    "difference",
    "right",
    "maybe",
    "something",
    "thinking",
    "right",
    "maybe",
    "say",
    "okay",
    "well",
    "well",
    "improve",
    "insights",
    "offer",
    "us",
    "right",
    "reminder",
    "use",
    "different",
    "metrics",
    "think",
    "context",
    "okay",
    "structural",
    "models",
    "going",
    "take",
    "short",
    "break",
    "going",
    "come",
    "back",
    "hidden",
    "markov",
    "model",
    "wrap",
    "state",
    "space",
    "let",
    "call",
    "break",
    "come",
    "back",
    "okay",
    "break",
    "dumped",
    "email",
    "address",
    "slack",
    "also",
    "case",
    "available",
    "syfy",
    "anyone",
    "questions",
    "tutorial",
    "comments",
    "great",
    "hear",
    "yes",
    "slides",
    "slack",
    "zip",
    "file",
    "anyone",
    "beginning",
    "unfortunately",
    "locked",
    "github",
    "moment",
    "long",
    "story",
    "soon",
    "back",
    "github",
    "also",
    "git",
    "someone",
    "nicely",
    "posted",
    "get",
    "ultimately",
    "would",
    "like",
    "know",
    "sides",
    "get",
    "repository",
    "altogether",
    "also",
    "email",
    "get",
    "together",
    "hopefully",
    "days",
    "also",
    "answer",
    "question",
    "yes",
    "okay",
    "questions",
    "okay",
    "okay",
    "folks",
    "know",
    "definitely",
    "another",
    "break",
    "stop",
    "serving",
    "breakfast",
    "case",
    "anyone",
    "needs",
    "pick",
    "okay",
    "going",
    "talking",
    "hidden",
    "markov",
    "models",
    "another",
    "technique",
    "especially",
    "novel",
    "considered",
    "increasingly",
    "useful",
    "actually",
    "get",
    "decent",
    "computing",
    "power",
    "opposed",
    "invented",
    "know",
    "maybe",
    "ibm",
    "really",
    "fancy",
    "computer",
    "whatever",
    "might",
    "able",
    "fit",
    "oh",
    "know",
    "fit",
    "laptops",
    "much",
    "relevant",
    "used",
    "hidden",
    "markov",
    "model",
    "state",
    "today",
    "state",
    "space",
    "model",
    "well",
    "idea",
    "sense",
    "idea",
    "underlying",
    "state",
    "observe",
    "output",
    "state",
    "observe",
    "structural",
    "time",
    "series",
    "models",
    "tend",
    "model",
    "state",
    "measured",
    "quantity",
    "basically",
    "thing",
    "error",
    "also",
    "broaden",
    "perspective",
    "hidden",
    "markov",
    "models",
    "think",
    "well",
    "maybe",
    "observe",
    "might",
    "even",
    "physical",
    "conceptual",
    "category",
    "underlying",
    "state",
    "producing",
    "case",
    "sequence",
    "measurements",
    "depicted",
    "envision",
    "feel",
    "strongly",
    "people",
    "able",
    "come",
    "go",
    "whenever",
    "want",
    "one",
    "one",
    "imprisoned",
    "okay",
    "locked",
    "way",
    "back",
    "sense",
    "envision",
    "sequence",
    "states",
    "whatever",
    "reason",
    "dynamics",
    "system",
    "states",
    "evolving",
    "time",
    "conceptualize",
    "framework",
    "also",
    "continuous",
    "time",
    "hidden",
    "markov",
    "models",
    "continuous",
    "time",
    "wo",
    "look",
    "describe",
    "data",
    "think",
    "sequence",
    "discrete",
    "times",
    "step",
    "underlying",
    "state",
    "evolving",
    "ca",
    "see",
    "directly",
    "observe",
    "whatever",
    "observable",
    "also",
    "times",
    "somehow",
    "x",
    "underlying",
    "state",
    "produces",
    "certain",
    "kind",
    "make",
    "physical",
    "well",
    "one",
    "example",
    "widely",
    "used",
    "actually",
    "even",
    "need",
    "fiction",
    "discrete",
    "time",
    "steps",
    "dna",
    "analysis",
    "people",
    "look",
    "dna",
    "sequences",
    "trying",
    "think",
    "see",
    "maybe",
    "certain",
    "peptides",
    "see",
    "certain",
    "sequence",
    "tell",
    "underlying",
    "state",
    "dna",
    "biologist",
    "probably",
    "slaughtering",
    "point",
    "dna",
    "chunks",
    "really",
    "coming",
    "discrete",
    "measures",
    "example",
    "time",
    "series",
    "sense",
    "axis",
    "well",
    "ordered",
    "good",
    "definition",
    "sort",
    "moving",
    "forward",
    "backward",
    "along",
    "axis",
    "example",
    "maybe",
    "underlying",
    "state",
    "hold",
    "biology",
    "maybe",
    "underlying",
    "state",
    "something",
    "like",
    "methylated",
    "chunk",
    "dna",
    "observe",
    "maybe",
    "something",
    "like",
    "time",
    "like",
    "kind",
    "enzymes",
    "produced",
    "time",
    "kind",
    "distribution",
    "notice",
    "x",
    "underlying",
    "state",
    "usually",
    "going",
    "discreetly",
    "defined",
    "right",
    "countable",
    "number",
    "states",
    "discrete",
    "states",
    "going",
    "two",
    "time",
    "although",
    "model",
    "anything",
    "right",
    "make",
    "things",
    "infinitely",
    "complicated",
    "traditional",
    "hmm",
    "outputs",
    "either",
    "discrete",
    "continuous",
    "right",
    "discrete",
    "case",
    "could",
    "like",
    "kind",
    "sequence",
    "observing",
    "sequence",
    "kind",
    "indicator",
    "state",
    "kind",
    "continuous",
    "variable",
    "example",
    "one",
    "gon",
    "na",
    "model",
    "notebook",
    "think",
    "rivers",
    "sort",
    "high",
    "flow",
    "low",
    "flow",
    "states",
    "think",
    "well",
    "high",
    "flow",
    "low",
    "flow",
    "state",
    "kind",
    "flow",
    "going",
    "see",
    "continuous",
    "variable",
    "right",
    "like",
    "see",
    "exactly",
    "number",
    "low",
    "state",
    "versus",
    "high",
    "state",
    "good",
    "chance",
    "distributions",
    "overlap",
    "sort",
    "look",
    "whole",
    "sequence",
    "might",
    "get",
    "sense",
    "oh",
    "low",
    "high",
    "low",
    "might",
    "use",
    "say",
    "opposed",
    "arima",
    "model",
    "mean",
    "say",
    "every",
    "mamada",
    "loves",
    "think",
    "well",
    "would",
    "choose",
    "model",
    "say",
    "another",
    "one",
    "known",
    "perform",
    "well",
    "well",
    "couple",
    "reasons",
    "like",
    "structural",
    "time",
    "series",
    "offers",
    "us",
    "kind",
    "intuition",
    "kind",
    "sort",
    "model",
    "underlying",
    "dynamic",
    "extent",
    "model",
    "like",
    "particular",
    "system",
    "like",
    "fits",
    "good",
    "thing",
    "offer",
    "kind",
    "insight",
    "another",
    "thing",
    "example",
    "case",
    "river",
    "flow",
    "learned",
    "reading",
    "hydrologist",
    "blog",
    "blogging",
    "hmms",
    "find",
    "arima",
    "models",
    "capture",
    "sort",
    "nonlinear",
    "dynamics",
    "know",
    "time",
    "minus",
    "one",
    "predicts",
    "time",
    "sort",
    "regime",
    "switching",
    "know",
    "kind",
    "probability",
    "regime",
    "switching",
    "last",
    "switch",
    "two",
    "regimes",
    "necessarily",
    "tell",
    "next",
    "mature",
    "regimes",
    "feature",
    "markov",
    "processes",
    "general",
    "right",
    "whole",
    "point",
    "markov",
    "process",
    "minus",
    "1",
    "know",
    "state",
    "minus",
    "1",
    "even",
    "really",
    "care",
    "king",
    "quite",
    "different",
    "arima",
    "model",
    "say",
    "oh",
    "looking",
    "whole",
    "history",
    "determine",
    "going",
    "next",
    "hidden",
    "markov",
    "model",
    "say",
    "long",
    "know",
    "xt",
    "minus",
    "1",
    "need",
    "look",
    "backwards",
    "time",
    "sort",
    "information",
    "need",
    "data",
    "point",
    "observations",
    "hidden",
    "markov",
    "models",
    "might",
    "want",
    "use",
    "okay",
    "state",
    "space",
    "model",
    "observations",
    "indicator",
    "underlying",
    "state",
    "posit",
    "separate",
    "underlying",
    "state",
    "case",
    "really",
    "would",
    "posit",
    "underlying",
    "state",
    "observable",
    "error",
    "sort",
    "hidden",
    "markov",
    "model",
    "unlike",
    "structural",
    "time",
    "series",
    "markov",
    "process",
    "past",
    "matter",
    "present",
    "status",
    "known",
    "right",
    "informative",
    "thing",
    "recent",
    "measurement",
    "nothing",
    "else",
    "gives",
    "new",
    "information",
    "things",
    "two",
    "aspects",
    "computationally",
    "want",
    "talk",
    "bit",
    "fit",
    "parameter",
    "estimation",
    "bound",
    "welch",
    "algorithm",
    "smoothing",
    "state",
    "labeling",
    "viterbi",
    "algorithm",
    "guessing",
    "seen",
    "words",
    "floating",
    "around",
    "want",
    "give",
    "overview",
    "intuition",
    "use",
    "okay",
    "bound",
    "welch",
    "algorithm",
    "determine",
    "parameters",
    "would",
    "use",
    "start",
    "like",
    "time",
    "series",
    "sense",
    "kind",
    "regime",
    "switching",
    "really",
    "idea",
    "like",
    "priors",
    "nothing",
    "know",
    "sort",
    "whether",
    "know",
    "mean",
    "one",
    "state",
    "3",
    "17",
    "idea",
    "really",
    "amazing",
    "sense",
    "say",
    "data",
    "many",
    "sort",
    "underlying",
    "states",
    "think",
    "go",
    "sort",
    "figure",
    "set",
    "parameters",
    "course",
    "entail",
    "concern",
    "talked",
    "earlier",
    "lot",
    "knobs",
    "turn",
    "complicated",
    "situations",
    "right",
    "start",
    "thinking",
    "would",
    "code",
    "code",
    "sort",
    "nightmare",
    "basically",
    "means",
    "guaranteed",
    "converge",
    "global",
    "maximum",
    "guaranteed",
    "get",
    "absolute",
    "best",
    "parameters",
    "describe",
    "data",
    "algorithm",
    "guarantee",
    "always",
    "get",
    "little",
    "bit",
    "better",
    "cycle",
    "algorithm",
    "forward",
    "backward",
    "expectation",
    "maximization",
    "algorithm",
    "right",
    "means",
    "sort",
    "first",
    "figure",
    "likelihood",
    "expectation",
    "given",
    "data",
    "expectation",
    "step",
    "maximization",
    "step",
    "sort",
    "update",
    "estimate",
    "parameters",
    "maximize",
    "likelihood",
    "given",
    "expression",
    "likelihood",
    "form",
    "right",
    "sort",
    "get",
    "think",
    "acceptable",
    "convergence",
    "level",
    "acceptable",
    "likelihood",
    "get",
    "local",
    "maximum",
    "another",
    "thing",
    "keep",
    "mind",
    "models",
    "right",
    "sort",
    "unique",
    "closed",
    "form",
    "solution",
    "often",
    "people",
    "recommend",
    "run",
    "many",
    "many",
    "times",
    "sort",
    "make",
    "pronouncements",
    "showing",
    "right",
    "run",
    "maybe",
    "run",
    "hundred",
    "times",
    "look",
    "parameter",
    "estimations",
    "see",
    "sort",
    "like",
    "narrative",
    "form",
    "emerging",
    "many",
    "many",
    "attempts",
    "course",
    "keep",
    "mind",
    "fit",
    "data",
    "right",
    "quite",
    "easy",
    "maybe",
    "choose",
    "set",
    "parameters",
    "much",
    "better",
    "training",
    "data",
    "sort",
    "validation",
    "holdout",
    "always",
    "risk",
    "sorts",
    "things",
    "okay",
    "brief",
    "overview",
    "details",
    "right",
    "problem",
    "trying",
    "estimate",
    "basically",
    "three",
    "things",
    "trying",
    "estimate",
    "sort",
    "transition",
    "matrix",
    "probability",
    "go",
    "back",
    "next",
    "time",
    "step",
    "including",
    "state",
    "describes",
    "xt",
    "minus",
    "1",
    "xt",
    "transition",
    "look",
    "like",
    "likely",
    "stay",
    "state",
    "likely",
    "go",
    "different",
    "state",
    "let",
    "quantify",
    "right",
    "get",
    "answer",
    "little",
    "likely",
    "likely",
    "right",
    "actually",
    "get",
    "matrix",
    "saying",
    "go",
    "state",
    "state",
    "j",
    "probability",
    "point",
    "four",
    "go",
    "state",
    "state",
    "k",
    "probability",
    "point",
    "two",
    "whatever",
    "first",
    "thing",
    "trying",
    "fit",
    "b",
    "seeing",
    "another",
    "matrix",
    "another",
    "sort",
    "set",
    "parameters",
    "indexed",
    "state",
    "likely",
    "particular",
    "assuming",
    "particular",
    "underlying",
    "state",
    "final",
    "thing",
    "need",
    "describe",
    "model",
    "pie",
    "priors",
    "telling",
    "likely",
    "sort",
    "begin",
    "particular",
    "state",
    "model",
    "also",
    "posits",
    "sort",
    "beginning",
    "something",
    "want",
    "think",
    "maybe",
    "taking",
    "time",
    "series",
    "slicing",
    "idea",
    "prior",
    "saying",
    "well",
    "sort",
    "think",
    "likely",
    "start",
    "states",
    "need",
    "think",
    "little",
    "bit",
    "starting",
    "mean",
    "data",
    "sort",
    "acceptable",
    "fit",
    "forward",
    "step",
    "essentially",
    "developing",
    "probability",
    "time",
    "time",
    "probability",
    "state",
    "xt",
    "time",
    "right",
    "combined",
    "probability",
    "probability",
    "seeing",
    "whole",
    "sequence",
    "actual",
    "observations",
    "y1",
    "way",
    "yt",
    "forward",
    "step",
    "sort",
    "moving",
    "forward",
    "time",
    "probable",
    "xt",
    "state",
    "seen",
    "data",
    "time",
    "go",
    "backwards",
    "direction",
    "say",
    "okay",
    "hand",
    "probable",
    "know",
    "case",
    "conditional",
    "state",
    "time",
    "probable",
    "see",
    "sequence",
    "plus",
    "1",
    "way",
    "right",
    "sort",
    "effectively",
    "picking",
    "point",
    "middle",
    "forward",
    "step",
    "probable",
    "steps",
    "state",
    "seen",
    "data",
    "backward",
    "step",
    "still",
    "state",
    "probable",
    "data",
    "comes",
    "given",
    "step",
    "right",
    "see",
    "sort",
    "measure",
    "converging",
    "measure",
    "probable",
    "whole",
    "sequence",
    "point",
    "view",
    "point",
    "time",
    "estimate",
    "two",
    "parameters",
    "right",
    "essentially",
    "become",
    "expressions",
    "calculated",
    "alpha",
    "beta",
    "right",
    "forward",
    "backward",
    "express",
    "sort",
    "generic",
    "quantities",
    "gamma",
    "sub",
    "probability",
    "state",
    "time",
    "even",
    "data",
    "observed",
    "full",
    "time",
    "series",
    "estimation",
    "theta",
    "right",
    "parameters",
    "also",
    "estimate",
    "final",
    "component",
    "behavior",
    "transitioning",
    "given",
    "data",
    "seen",
    "theta",
    "lot",
    "programming",
    "make",
    "aware",
    "sort",
    "going",
    "hood",
    "estimate",
    "three",
    "quantities",
    "right",
    "pie",
    "prior",
    "likely",
    "beginning",
    "sequence",
    "given",
    "starting",
    "state",
    "influence",
    "go",
    "alpha",
    "sub",
    "ij",
    "likely",
    "transition",
    "state",
    "state",
    "j",
    "particular",
    "time",
    "step",
    "finally",
    "beta",
    "likely",
    "see",
    "particular",
    "observed",
    "value",
    "given",
    "particular",
    "underlying",
    "state",
    "okay",
    "would",
    "estimate",
    "parameters",
    "really",
    "computationally",
    "taxing",
    "really",
    "difficult",
    "like",
    "said",
    "guaranteed",
    "get",
    "perfect",
    "parameters",
    "get",
    "one",
    "iteration",
    "parameters",
    "global",
    "maximum",
    "optimization",
    "final",
    "slide",
    "go",
    "code",
    "thing",
    "want",
    "first",
    "want",
    "parameters",
    "sort",
    "explain",
    "underlying",
    "sequence",
    "right",
    "posit",
    "certain",
    "number",
    "states",
    "parameters",
    "come",
    "describe",
    "behavior",
    "okay",
    "got",
    "parameters",
    "let",
    "say",
    "accept",
    "want",
    "move",
    "forward",
    "need",
    "figure",
    "posit",
    "underlying",
    "parameters",
    "describing",
    "state",
    "actually",
    "states",
    "time",
    "series",
    "time",
    "step",
    "actually",
    "get",
    "label",
    "via",
    "viterbi",
    "viterbi",
    "algorithm",
    "determining",
    "sequence",
    "come",
    "afterwards",
    "use",
    "need",
    "parameters",
    "describing",
    "time",
    "series",
    "dynamic",
    "programming",
    "problem",
    "know",
    "term",
    "dynamic",
    "programming",
    "basically",
    "realize",
    "larger",
    "problem",
    "actually",
    "composed",
    "smaller",
    "problem",
    "sounds",
    "like",
    "recursion",
    "different",
    "also",
    "memorize",
    "sort",
    "limited",
    "countable",
    "ordered",
    "set",
    "problems",
    "right",
    "describe",
    "recursion",
    "dynamic",
    "programming",
    "set",
    "number",
    "steps",
    "whole",
    "path",
    "set",
    "number",
    "options",
    "explore",
    "think",
    "could",
    "actually",
    "summarize",
    "matrix",
    "right",
    "get",
    "say",
    "best",
    "way",
    "could",
    "explore",
    "possible",
    "states",
    "permute",
    "states",
    "versus",
    "cost",
    "ultimately",
    "sort",
    "come",
    "probable",
    "way",
    "go",
    "states",
    "sounds",
    "complicated",
    "like",
    "waving",
    "hands",
    "definitely",
    "read",
    "code",
    "actually",
    "bad",
    "pain",
    "butt",
    "code",
    "right",
    "sort",
    "get",
    "sense",
    "like",
    "things",
    "need",
    "think",
    "code",
    "properly",
    "make",
    "sure",
    "found",
    "probable",
    "path",
    "data",
    "leave",
    "great",
    "example",
    "though",
    "something",
    "could",
    "accessible",
    "want",
    "code",
    "okay",
    "gon",
    "na",
    "open",
    "second",
    "second",
    "notebook",
    "soon",
    "find",
    "notebook",
    "state",
    "space",
    "model",
    "looking",
    "gaussian",
    "hmm",
    "gaussian",
    "hidden",
    "hidden",
    "markov",
    "model",
    "gon",
    "na",
    "continuous",
    "observables",
    "also",
    "multinomial",
    "hidden",
    "markov",
    "model",
    "underlying",
    "state",
    "observable",
    "sort",
    "discrete",
    "states",
    "another",
    "thing",
    "better",
    "describes",
    "data",
    "could",
    "look",
    "okay",
    "case",
    "mentioned",
    "trying",
    "get",
    "colorado",
    "river",
    "data",
    "website",
    "working",
    "well",
    "know",
    "budget",
    "cuts",
    "whatever",
    "gon",
    "na",
    "look",
    "nile",
    "instead",
    "famous",
    "data",
    "set",
    "read",
    "traditional",
    "stats",
    "textbooks",
    "look",
    "nile",
    "data",
    "see",
    "problem",
    "hold",
    "need",
    "clear",
    "start",
    "want",
    "giving",
    "away",
    "answers",
    "problem",
    "sort",
    "multiple",
    "groups",
    "observations",
    "one",
    "value",
    "year",
    "flow",
    "year",
    "even",
    "know",
    "taken",
    "cumulative",
    "value",
    "see",
    "see",
    "plot",
    "right",
    "kind",
    "interesting",
    "given",
    "hydrologists",
    "told",
    "us",
    "based",
    "one",
    "hydrology",
    "blog",
    "ever",
    "read",
    "sort",
    "high",
    "flow",
    "low",
    "flow",
    "state",
    "first",
    "order",
    "want",
    "model",
    "river",
    "system",
    "could",
    "due",
    "things",
    "like",
    "el",
    "nino",
    "whatever",
    "equivalent",
    "nile",
    "region",
    "right",
    "sort",
    "global",
    "larger",
    "global",
    "cycles",
    "patterns",
    "states",
    "affecting",
    "things",
    "like",
    "river",
    "flow",
    "things",
    "like",
    "one",
    "year",
    "predicts",
    "next",
    "like",
    "one",
    "regime",
    "point",
    "switch",
    "another",
    "deterministic",
    "kind",
    "nonlinear",
    "stochastic",
    "switching",
    "sort",
    "eyeball",
    "like",
    "wow",
    "already",
    "looks",
    "like",
    "sort",
    "two",
    "states",
    "right",
    "like",
    "sort",
    "got",
    "high",
    "thing",
    "low",
    "thing",
    "one",
    "bit",
    "background",
    "research",
    "thought",
    "oh",
    "man",
    "one",
    "soviets",
    "built",
    "dam",
    "nile",
    "maybe",
    "building",
    "dam",
    "look",
    "time",
    "stamp",
    "around",
    "1900",
    "definitely",
    "soviets",
    "building",
    "dam",
    "nile",
    "know",
    "say",
    "okay",
    "gon",
    "na",
    "assume",
    "sort",
    "climatological",
    "phenomenon",
    "looking",
    "let",
    "take",
    "look",
    "api",
    "hmm",
    "learn",
    "little",
    "bit",
    "history",
    "hmm",
    "learn",
    "believe",
    "used",
    "part",
    "split",
    "project",
    "continuation",
    "aware",
    "look",
    "hidden",
    "markov",
    "models",
    "sufficiently",
    "complex",
    "interesting",
    "thing",
    "although",
    "similar",
    "api",
    "first",
    "tricky",
    "thing",
    "actually",
    "sort",
    "require",
    "shape",
    "require",
    "two",
    "dimensions",
    "input",
    "expand",
    "ins",
    "run",
    "without",
    "get",
    "error",
    "put",
    "back",
    "generously",
    "gave",
    "gon",
    "na",
    "start",
    "saying",
    "two",
    "states",
    "right",
    "gon",
    "na",
    "low",
    "flow",
    "state",
    "high",
    "flow",
    "state",
    "read",
    "hydrology",
    "blog",
    "must",
    "good",
    "set",
    "model",
    "gaussian",
    "hmm",
    "nicely",
    "see",
    "set",
    "number",
    "components",
    "number",
    "iterations",
    "want",
    "fit",
    "values",
    "want",
    "get",
    "hidden",
    "state",
    "run",
    "model",
    "predict",
    "let",
    "run",
    "let",
    "see",
    "even",
    "get",
    "hidden",
    "states",
    "get",
    "numpy",
    "array",
    "shape",
    "shape",
    "thing",
    "put",
    "look",
    "like",
    "well",
    "bunch",
    "numbers",
    "actually",
    "number",
    "especially",
    "interesting",
    "far",
    "look",
    "sort",
    "looks",
    "like",
    "two",
    "values",
    "one",
    "72",
    "count",
    "20",
    "account",
    "telling",
    "much",
    "plot",
    "plotting",
    "hidden",
    "state",
    "note",
    "hidden",
    "state",
    "something",
    "necessarily",
    "make",
    "sense",
    "plot",
    "sense",
    "like",
    "higher",
    "state",
    "necessarily",
    "higher",
    "value",
    "keep",
    "mind",
    "show",
    "sort",
    "label",
    "really",
    "quick",
    "easy",
    "way",
    "number",
    "hidden",
    "state",
    "means",
    "absolutely",
    "nothing",
    "fact",
    "sort",
    "go",
    "high",
    "low",
    "mean",
    "anything",
    "looking",
    "ones",
    "zeros",
    "notice",
    "verse",
    "saw",
    "telling",
    "anything",
    "know",
    "none",
    "especially",
    "right",
    "said",
    "oh",
    "yeah",
    "one",
    "state",
    "seems",
    "like",
    "took",
    "nosedive",
    "another",
    "state",
    "two",
    "states",
    "sort",
    "agrees",
    "gon",
    "na",
    "talk",
    "little",
    "bit",
    "fix",
    "might",
    "interesting",
    "informative",
    "would",
    "say",
    "since",
    "realize",
    "gon",
    "na",
    "want",
    "experiment",
    "bit",
    "would",
    "ask",
    "take",
    "couple",
    "minutes",
    "think",
    "package",
    "api",
    "little",
    "conveniently",
    "go",
    "ahead",
    "read",
    "hmm",
    "learn",
    "gaussian",
    "hmm",
    "api",
    "also",
    "think",
    "basically",
    "want",
    "turn",
    "function",
    "call",
    "really",
    "easily",
    "instead",
    "sort",
    "write",
    "one",
    "solution",
    "imagine",
    "folks",
    "ideas",
    "main",
    "goal",
    "would",
    "want",
    "something",
    "accepts",
    "values",
    "also",
    "states",
    "could",
    "convenient",
    "reshape",
    "case",
    "often",
    "passing",
    "data",
    "lacks",
    "second",
    "dimension",
    "right",
    "better",
    "get",
    "stuff",
    "way",
    "take",
    "mental",
    "tax",
    "case",
    "gon",
    "na",
    "model",
    "right",
    "number",
    "components",
    "want",
    "make",
    "configurable",
    "chose",
    "make",
    "number",
    "iterations",
    "configurable",
    "sort",
    "decide",
    "number",
    "iterations",
    "tends",
    "depend",
    "quality",
    "data",
    "rather",
    "nature",
    "data",
    "rather",
    "number",
    "states",
    "first",
    "order",
    "would",
    "something",
    "adjust",
    "much",
    "data",
    "set",
    "grab",
    "hidden",
    "states",
    "also",
    "probably",
    "want",
    "extract",
    "information",
    "right",
    "knowing",
    "state",
    "label",
    "interesting",
    "also",
    "want",
    "know",
    "bom",
    "welch",
    "algorithm",
    "found",
    "sort",
    "mean",
    "standard",
    "deviation",
    "gaussian",
    "model",
    "developing",
    "right",
    "case",
    "describe",
    "sort",
    "expect",
    "see",
    "coming",
    "observable",
    "data",
    "amuse",
    "sigma",
    "right",
    "gaussian",
    "distributions",
    "given",
    "underlying",
    "state",
    "transmat",
    "transition",
    "matrix",
    "thing",
    "describes",
    "likely",
    "go",
    "state",
    "state",
    "j",
    "want",
    "see",
    "well",
    "going",
    "give",
    "information",
    "underlying",
    "dynamics",
    "look",
    "like",
    "match",
    "theory",
    "river",
    "flow",
    "works",
    "would",
    "alright",
    "actually",
    "also",
    "like",
    "personally",
    "like",
    "reorder",
    "output",
    "output",
    "comes",
    "sort",
    "arbitrary",
    "order",
    "right",
    "like",
    "said",
    "hidden",
    "state",
    "labels",
    "mean",
    "anything",
    "arbitrary",
    "labels",
    "continue",
    "arbitrary",
    "labels",
    "matter",
    "like",
    "least",
    "label",
    "sort",
    "order",
    "like",
    "lowest",
    "highest",
    "lowest",
    "mean",
    "lowest",
    "hidden",
    "state",
    "label",
    "highest",
    "mean",
    "highest",
    "hidden",
    "state",
    "label",
    "least",
    "offer",
    "way",
    "saying",
    "oh",
    "bigger",
    "one",
    "smaller",
    "one",
    "mean",
    "code",
    "permute",
    "everything",
    "including",
    "transition",
    "matrix",
    "fit",
    "method",
    "give",
    "us",
    "hidden",
    "states",
    "meuse",
    "sigma",
    "transition",
    "matrix",
    "model",
    "wish",
    "model",
    "run",
    "wo",
    "leave",
    "exercise",
    "think",
    "go",
    "might",
    "want",
    "plot",
    "well",
    "things",
    "want",
    "plot",
    "want",
    "plot",
    "real",
    "value",
    "want",
    "plot",
    "hidden",
    "state",
    "way",
    "right",
    "like",
    "overlaid",
    "way",
    "matter",
    "personal",
    "preference",
    "people",
    "things",
    "like",
    "sort",
    "color",
    "background",
    "according",
    "state",
    "like",
    "said",
    "idiosyncratic",
    "way",
    "like",
    "plot",
    "state",
    "line",
    "graph",
    "long",
    "preserve",
    "mind",
    "fact",
    "mean",
    "anything",
    "except",
    "indicator",
    "different",
    "values",
    "right",
    "actually",
    "indicating",
    "anything",
    "size",
    "values",
    "yes",
    "thank",
    "part",
    "one",
    "yes",
    "right",
    "thought",
    "would",
    "run",
    "first",
    "thank",
    "okay",
    "states",
    "order",
    "want",
    "right",
    "like",
    "highest",
    "mean",
    "highest",
    "labeled",
    "state",
    "mean",
    "anything",
    "means",
    "sort",
    "ordinal",
    "order",
    "still",
    "got",
    "uninformative",
    "oh",
    "yeah",
    "sort",
    "like",
    "higher",
    "flow",
    "regime",
    "sort",
    "lower",
    "level",
    "lower",
    "flow",
    "regime",
    "taught",
    "much",
    "next",
    "exercise",
    "guys",
    "form",
    "experiment",
    "plot",
    "results",
    "easily",
    "like",
    "try",
    "two",
    "different",
    "ways",
    "maybe",
    "improving",
    "would",
    "recommend",
    "one",
    "cut",
    "sort",
    "earlier",
    "set",
    "time",
    "points",
    "might",
    "informative",
    "might",
    "two",
    "fundamentally",
    "different",
    "interesting",
    "interested",
    "outcome",
    "says",
    "oh",
    "twenty",
    "years",
    "one",
    "state",
    "80",
    "years",
    "another",
    "state",
    "super",
    "interesting",
    "cut",
    "region",
    "interest",
    "rerun",
    "analysis",
    "second",
    "method",
    "think",
    "well",
    "maybe",
    "looking",
    "two",
    "underlying",
    "states",
    "try",
    "different",
    "number",
    "underlying",
    "states",
    "see",
    "looks",
    "like",
    "go",
    "ahead",
    "take",
    "minutes",
    "work",
    "two",
    "options",
    "first",
    "possibility",
    "right",
    "cut",
    "special",
    "region",
    "mind",
    "means",
    "like",
    "first",
    "state",
    "seems",
    "different",
    "right",
    "takes",
    "first",
    "day",
    "think",
    "well",
    "maybe",
    "sort",
    "extraordinary",
    "event",
    "want",
    "rule",
    "interested",
    "sort",
    "typical",
    "dynamics",
    "river",
    "cut",
    "first",
    "2728",
    "values",
    "model",
    "put",
    "fancy",
    "plotting",
    "function",
    "see",
    "switching",
    "sort",
    "would",
    "expect",
    "based",
    "little",
    "know",
    "hydrology",
    "right",
    "looking",
    "regime",
    "shift",
    "like",
    "20",
    "years",
    "80",
    "years",
    "right",
    "sounds",
    "like",
    "almost",
    "like",
    "geological",
    "event",
    "something",
    "interested",
    "sort",
    "every",
    "years",
    "el",
    "nino",
    "type",
    "stuff",
    "one",
    "way",
    "really",
    "need",
    "feels",
    "like",
    "maybe",
    "model",
    "describe",
    "data",
    "well",
    "sort",
    "cut",
    "whole",
    "periods",
    "maybe",
    "fine",
    "purposes",
    "also",
    "want",
    "see",
    "well",
    "maybe",
    "little",
    "robust",
    "instead",
    "add",
    "another",
    "hidden",
    "state",
    "right",
    "say",
    "well",
    "maybe",
    "extraordinary",
    "state",
    "normal",
    "two",
    "state",
    "dynamics",
    "accommodate",
    "put",
    "three",
    "states",
    "plot",
    "well",
    "seem",
    "accommodating",
    "right",
    "one",
    "state",
    "sort",
    "initial",
    "extraordinary",
    "state",
    "another",
    "state",
    "another",
    "state",
    "three",
    "states",
    "seem",
    "gone",
    "wrong",
    "somewhere",
    "ordering",
    "states",
    "fix",
    "bug",
    "break",
    "ideally",
    "first",
    "state",
    "would",
    "sort",
    "high",
    "would",
    "jump",
    "low",
    "high",
    "low",
    "apologize",
    "see",
    "see",
    "switching",
    "right",
    "see",
    "model",
    "robust",
    "enough",
    "sort",
    "describe",
    "state",
    "sort",
    "long",
    "different",
    "accommodate",
    "regime",
    "shifting",
    "well",
    "look",
    "muse",
    "see",
    "see",
    "know",
    "sorry",
    "let",
    "fix",
    "plotting",
    "sorry",
    "three",
    "sort",
    "means",
    "observable",
    "right",
    "two",
    "low",
    "eight",
    "one",
    "around",
    "800",
    "880",
    "one",
    "around",
    "1100",
    "sort",
    "extraordinary",
    "regime",
    "also",
    "look",
    "transition",
    "matrix",
    "going",
    "describe",
    "one",
    "state",
    "travels",
    "another",
    "see",
    "look",
    "right",
    "three",
    "three",
    "three",
    "states",
    "transition",
    "matrix",
    "describes",
    "probability",
    "transitioning",
    "one",
    "three",
    "states",
    "one",
    "three",
    "states",
    "interestingly",
    "see",
    "challenging",
    "data",
    "set",
    "transition",
    "probabilities",
    "zero",
    "right",
    "particular",
    "possibility",
    "transitioning",
    "either",
    "top",
    "two",
    "states",
    "third",
    "state",
    "0",
    "third",
    "state",
    "observes",
    "one",
    "direction",
    "right",
    "observes",
    "going",
    "top",
    "state",
    "sort",
    "exiting",
    "regime",
    "observe",
    "end",
    "regime",
    "would",
    "need",
    "domain",
    "knowledge",
    "know",
    "reasonable",
    "reasonable",
    "sort",
    "constraint",
    "fitting",
    "outer",
    "would",
    "want",
    "put",
    "permit",
    "things",
    "go",
    "zero",
    "sort",
    "advanced",
    "things",
    "might",
    "domain",
    "knowledge",
    "something",
    "going",
    "get",
    "box",
    "also",
    "keep",
    "mind",
    "methods",
    "ways",
    "tweak",
    "reflect",
    "data",
    "dynamics",
    "otherwise",
    "general",
    "solution",
    "gon",
    "na",
    "fit",
    "data",
    "see",
    "never",
    "ever",
    "ever",
    "sees",
    "event",
    "well",
    "go",
    "zero",
    "constrain",
    "maybe",
    "makes",
    "sense",
    "hand",
    "say",
    "well",
    "flexible",
    "shows",
    "accommodate",
    "sort",
    "normal",
    "regime",
    "shifting",
    "also",
    "extraordinary",
    "events",
    "maybe",
    "interesting",
    "helpful",
    "know",
    "unlike",
    "say",
    "rhema",
    "model",
    "even",
    "structural",
    "time",
    "series",
    "struggle",
    "sort",
    "sudden",
    "regime",
    "shift",
    "also",
    "see",
    "look",
    "first",
    "two",
    "rows",
    "columns",
    "see",
    "sort",
    "probability",
    "switching",
    "sort",
    "two",
    "normal",
    "states",
    "sort",
    "seems",
    "flicker",
    "back",
    "forth",
    "know",
    "directionality",
    "favoring",
    "one",
    "sort",
    "interesting",
    "insight",
    "maybe",
    "hydrology",
    "might",
    "say",
    "oh",
    "yeah",
    "reflects",
    "data",
    "something",
    "pursue",
    "research",
    "get",
    "insights",
    "talked",
    "local",
    "maximum",
    "global",
    "maximum",
    "guaranteed",
    "get",
    "fit",
    "initialize",
    "differently",
    "one",
    "set",
    "parameters",
    "would",
    "want",
    "run",
    "simulation",
    "many",
    "many",
    "times",
    "different",
    "starting",
    "conditions",
    "really",
    "better",
    "sense",
    "describing",
    "data",
    "arguably",
    "hundred",
    "data",
    "points",
    "much",
    "get",
    "anyway",
    "right",
    "modern",
    "methods",
    "always",
    "work",
    "well",
    "might",
    "like",
    "hundred",
    "data",
    "points",
    "simple",
    "simple",
    "toy",
    "example",
    "okay",
    "gon",
    "na",
    "go",
    "ahead",
    "take",
    "break",
    "make",
    "want",
    "people",
    "enough",
    "time",
    "go",
    "get",
    "snacks",
    "think",
    "make",
    "like",
    "break",
    "big",
    "snack",
    "break",
    "also",
    "exercise",
    "wish",
    "attempt",
    "quickly",
    "show",
    "guys",
    "get",
    "back",
    "move",
    "machine",
    "learning",
    "methods",
    "let",
    "see",
    "10",
    "exactly",
    "restart",
    "10",
    "10",
    "clarify",
    "snacks",
    "breakfast",
    "across",
    "courtyard",
    "sort",
    "way",
    "okay",
    "gon",
    "na",
    "get",
    "started",
    "first",
    "quickly",
    "walk",
    "us",
    "last",
    "exercise",
    "hidden",
    "markov",
    "model",
    "component",
    "gon",
    "na",
    "move",
    "overview",
    "machine",
    "learning",
    "hour",
    "last",
    "hour",
    "deep",
    "learning",
    "keep",
    "panicking",
    "see",
    "east",
    "coast",
    "stan",
    "psycho",
    "crap",
    "already",
    "okay",
    "okay",
    "one",
    "function",
    "wanted",
    "make",
    "aware",
    "sample",
    "hidden",
    "markov",
    "model",
    "fit",
    "even",
    "need",
    "fit",
    "one",
    "also",
    "feed",
    "parameters",
    "create",
    "hidden",
    "markov",
    "model",
    "certain",
    "set",
    "parameters",
    "useful",
    "want",
    "see",
    "well",
    "would",
    "something",
    "look",
    "like",
    "run",
    "many",
    "iterations",
    "test",
    "theory",
    "case",
    "sample",
    "model",
    "developed",
    "plot",
    "think",
    "well",
    "would",
    "look",
    "like",
    "one",
    "sort",
    "sampling",
    "would",
    "look",
    "like",
    "refit",
    "notice",
    "looks",
    "different",
    "data",
    "right",
    "even",
    "though",
    "hypothesized",
    "underlying",
    "process",
    "keep",
    "mind",
    "process",
    "certain",
    "set",
    "meuse",
    "sigmaz",
    "describe",
    "sub",
    "probabilities",
    "transition",
    "matrix",
    "still",
    "give",
    "data",
    "looks",
    "different",
    "refit",
    "see",
    "like",
    "totally",
    "different",
    "model",
    "three",
    "states",
    "different",
    "well",
    "case",
    "actually",
    "seeing",
    "really",
    "looks",
    "like",
    "happened",
    "two",
    "sort",
    "normal",
    "states",
    "ones",
    "really",
    "got",
    "repeated",
    "say",
    "sampled",
    "repeatedly",
    "really",
    "get",
    "third",
    "extraordinary",
    "state",
    "characterized",
    "beginning",
    "flow",
    "model",
    "case",
    "seeing",
    "transition",
    "matrix",
    "probabilities",
    "look",
    "like",
    "saw",
    "first",
    "case",
    "mainly",
    "taking",
    "two",
    "states",
    "three",
    "states",
    "remodeling",
    "two",
    "states",
    "three",
    "example",
    "necessarily",
    "wrong",
    "bad",
    "thing",
    "keep",
    "mind",
    "hidden",
    "markov",
    "model",
    "describe",
    "one",
    "set",
    "data",
    "look",
    "different",
    "different",
    "set",
    "data",
    "one",
    "states",
    "even",
    "right",
    "give",
    "many",
    "states",
    "states",
    "relative",
    "ground",
    "truth",
    "end",
    "things",
    "make",
    "great",
    "deal",
    "sense",
    "insightful",
    "keep",
    "mind",
    "fitting",
    "models",
    "especially",
    "sort",
    "outlier",
    "states",
    "see",
    "often",
    "okay",
    "questions",
    "comments",
    "hidden",
    "markov",
    "models",
    "switch",
    "gears",
    "okay",
    "sort",
    "overview",
    "state",
    "space",
    "models",
    "recap",
    "load",
    "recap",
    "state",
    "space",
    "models",
    "really",
    "good",
    "sort",
    "underlying",
    "sense",
    "think",
    "dynamics",
    "system",
    "sort",
    "add",
    "evidence",
    "dynamics",
    "time",
    "take",
    "away",
    "evidence",
    "right",
    "tend",
    "refute",
    "think",
    "example",
    "saw",
    "trend",
    "earth",
    "temperature",
    "data",
    "actually",
    "interesting",
    "local",
    "level",
    "model",
    "sufficed",
    "also",
    "helped",
    "us",
    "discover",
    "seasonality",
    "lazy",
    "discover",
    "hidden",
    "markov",
    "model",
    "saw",
    "model",
    "accommodate",
    "things",
    "like",
    "say",
    "extraordinary",
    "state",
    "change",
    "cases",
    "run",
    "problems",
    "simulation",
    "problems",
    "methods",
    "need",
    "treated",
    "carefully",
    "knowledge",
    "work",
    "ca",
    "roll",
    "box",
    "offer",
    "lot",
    "things",
    "save",
    "traditional",
    "statistical",
    "models",
    "offer",
    "way",
    "sort",
    "break",
    "system",
    "try",
    "get",
    "inside",
    "right",
    "even",
    "though",
    "univariate",
    "measurement",
    "actually",
    "get",
    "complex",
    "dynamics",
    "underneath",
    "univariate",
    "method",
    "also",
    "point",
    "methods",
    "also",
    "accommodate",
    "multivariate",
    "time",
    "series",
    "data",
    "well",
    "parallel",
    "temperature",
    "measurements",
    "want",
    "incorporate",
    "parallel",
    "measurements",
    "hidden",
    "markov",
    "process",
    "indicative",
    "underlying",
    "state",
    "maybe",
    "multivariate",
    "state",
    "kind",
    "accommodated",
    "well",
    "standard",
    "software",
    "somebody",
    "asking",
    "break",
    "well",
    "know",
    "underlying",
    "restrictions",
    "mentioned",
    "might",
    "want",
    "might",
    "want",
    "transition",
    "matrix",
    "permit",
    "zero",
    "transition",
    "probability",
    "know",
    "true",
    "system",
    "build",
    "things",
    "experience",
    "time",
    "require",
    "hacking",
    "source",
    "code",
    "writing",
    "source",
    "code",
    "good",
    "news",
    "open",
    "source",
    "tools",
    "lot",
    "inspiration",
    "slash",
    "code",
    "steal",
    "like",
    "make",
    "thing",
    "good",
    "news",
    "underlying",
    "algorithms",
    "viterbi",
    "algorithm",
    "brown",
    "welch",
    "algorithm",
    "think",
    "complicated",
    "sense",
    "many",
    "many",
    "indices",
    "rocket",
    "science",
    "sense",
    "fairly",
    "intuitive",
    "easy",
    "reason",
    "fairly",
    "straightforward",
    "hack",
    "sort",
    "developed",
    "sense",
    "working",
    "okay",
    "going",
    "talk",
    "machine",
    "learning",
    "time",
    "series",
    "switching",
    "emphases",
    "little",
    "bit",
    "different",
    "way",
    "understanding",
    "data",
    "processing",
    "data",
    "ok",
    "first",
    "thing",
    "need",
    "think",
    "machine",
    "learning",
    "time",
    "series",
    "fact",
    "aware",
    "call",
    "attention",
    "think",
    "one",
    "machine",
    "learning",
    "method",
    "really",
    "sort",
    "developed",
    "time",
    "series",
    "experience",
    "apply",
    "machine",
    "learning",
    "time",
    "series",
    "applying",
    "general",
    "methods",
    "finding",
    "way",
    "make",
    "reflect",
    "accept",
    "time",
    "series",
    "data",
    "opposed",
    "models",
    "looking",
    "built",
    "core",
    "around",
    "idea",
    "time",
    "series",
    "right",
    "traditional",
    "models",
    "like",
    "arima",
    "also",
    "models",
    "like",
    "hidden",
    "markov",
    "model",
    "structural",
    "time",
    "series",
    "core",
    "vision",
    "sort",
    "temporarily",
    "ordered",
    "data",
    "hand",
    "get",
    "things",
    "like",
    "decision",
    "trees",
    "fundamentally",
    "build",
    "model",
    "around",
    "idea",
    "temporal",
    "notions",
    "right",
    "actually",
    "completely",
    "absent",
    "model",
    "going",
    "use",
    "models",
    "like",
    "need",
    "provide",
    "way",
    "translating",
    "temporal",
    "data",
    "something",
    "makes",
    "sense",
    "accepted",
    "input",
    "algorithm",
    "also",
    "makes",
    "sense",
    "right",
    "want",
    "get",
    "overly",
    "focused",
    "inputs",
    "also",
    "want",
    "make",
    "sure",
    "sensible",
    "inputs",
    "throwing",
    "data",
    "example",
    "would",
    "never",
    "say",
    "well",
    "let",
    "make",
    "time",
    "series",
    "inputs",
    "decision",
    "tree",
    "right",
    "let",
    "decision",
    "tree",
    "maybe",
    "really",
    "short",
    "time",
    "series",
    "know",
    "10",
    "points",
    "time",
    "one",
    "time",
    "ten",
    "plenty",
    "machine",
    "learning",
    "models",
    "take",
    "10",
    "inputs",
    "put",
    "inputs",
    "see",
    "happens",
    "rare",
    "going",
    "get",
    "anything",
    "like",
    "want",
    "sort",
    "time",
    "series",
    "data",
    "arguably",
    "predictable",
    "clean",
    "easy",
    "need",
    "machine",
    "learning",
    "anyway",
    "could",
    "probably",
    "eyeball",
    "right",
    "kind",
    "data",
    "little",
    "noise",
    "clear",
    "structure",
    "reason",
    "case",
    "would",
    "work",
    "case",
    "would",
    "need",
    "hand",
    "things",
    "could",
    "difficult",
    "right",
    "time",
    "series",
    "different",
    "lengths",
    "also",
    "things",
    "sort",
    "got",
    "say",
    "even",
    "really",
    "simple",
    "example",
    "maybe",
    "always",
    "many",
    "time",
    "series",
    "volcano",
    "erupting",
    "know",
    "always",
    "interruptions",
    "somewhere",
    "data",
    "defined",
    "data",
    "set",
    "eruption",
    "wo",
    "occur",
    "exact",
    "moment",
    "though",
    "right",
    "one",
    "time",
    "series",
    "might",
    "time",
    "step",
    "three",
    "another",
    "might",
    "time",
    "step",
    "13",
    "could",
    "expect",
    "machine",
    "learning",
    "algorithm",
    "notion",
    "time",
    "sort",
    "understand",
    "eruption",
    "occur",
    "times",
    "expect",
    "whatever",
    "collateral",
    "damage",
    "results",
    "whatever",
    "shows",
    "time",
    "series",
    "main",
    "message",
    "ca",
    "feed",
    "time",
    "series",
    "machine",
    "learning",
    "network",
    "never",
    "seen",
    "case",
    "makes",
    "sense",
    "though",
    "someone",
    "one",
    "would",
    "great",
    "know",
    "instead",
    "feature",
    "generation",
    "right",
    "different",
    "data",
    "well",
    "sometimes",
    "generate",
    "know",
    "data",
    "raw",
    "data",
    "time",
    "series",
    "basically",
    "required",
    "sorts",
    "features",
    "want",
    "right",
    "feature",
    "generation",
    "finding",
    "away",
    "take",
    "something",
    "like",
    "curve",
    "numbers",
    "describe",
    "looks",
    "like",
    "different",
    "time",
    "want",
    "feature",
    "generation",
    "different",
    "whenever",
    "want",
    "summarize",
    "complex",
    "situation",
    "couple",
    "words",
    "couple",
    "numbers",
    "mix",
    "heterogeneous",
    "data",
    "case",
    "looking",
    "time",
    "series",
    "going",
    "use",
    "well",
    "might",
    "use",
    "maximum",
    "value",
    "minimum",
    "value",
    "median",
    "mean",
    "number",
    "peaks",
    "may",
    "noticing",
    "already",
    "sufficiently",
    "long",
    "time",
    "series",
    "remember",
    "sort",
    "time",
    "series",
    "almost",
    "like",
    "data",
    "point",
    "time",
    "series",
    "going",
    "need",
    "go",
    "timestep",
    "time",
    "series",
    "compute",
    "things",
    "think",
    "actually",
    "gets",
    "quite",
    "computationally",
    "taxing",
    "right",
    "even",
    "look",
    "mean",
    "min",
    "max",
    "need",
    "go",
    "every",
    "data",
    "point",
    "really",
    "long",
    "time",
    "series",
    "know",
    "order",
    "n",
    "order",
    "n",
    "squared",
    "gets",
    "quite",
    "drag",
    "things",
    "like",
    "number",
    "peeps",
    "well",
    "one",
    "things",
    "know",
    "human",
    "eyeball",
    "great",
    "deep",
    "learning",
    "network",
    "right",
    "kind",
    "might",
    "great",
    "trying",
    "write",
    "simple",
    "python",
    "routines",
    "especially",
    "efficient",
    "ones",
    "know",
    "quick",
    "fast",
    "super",
    "reliable",
    "way",
    "identify",
    "peaks",
    "right",
    "sort",
    "thing",
    "would",
    "really",
    "helpful",
    "could",
    "put",
    "feature",
    "need",
    "sort",
    "analysis",
    "well",
    "data",
    "analyze",
    "know",
    "also",
    "computational",
    "cost",
    "always",
    "something",
    "thinking",
    "feature",
    "generation",
    "expensive",
    "whether",
    "worth",
    "also",
    "want",
    "point",
    "times",
    "theory",
    "well",
    "studied",
    "complicated",
    "domain",
    "dependent",
    "sort",
    "thing",
    "far",
    "turns",
    "useful",
    "let",
    "go",
    "back",
    "set",
    "wanted",
    "highlight",
    "one",
    "many",
    "canonical",
    "sets",
    "called",
    "canonical",
    "set",
    "sort",
    "ongoing",
    "really",
    "interesting",
    "project",
    "part",
    "project",
    "called",
    "highly",
    "comparative",
    "time",
    "series",
    "analysis",
    "project",
    "collecting",
    "every",
    "kind",
    "time",
    "series",
    "data",
    "get",
    "hands",
    "frequencies",
    "whatever",
    "domains",
    "available",
    "donate",
    "datasets",
    "idea",
    "use",
    "machine",
    "learning",
    "sort",
    "identify",
    "features",
    "perform",
    "well",
    "across",
    "many",
    "kinds",
    "datasets",
    "classification",
    "prediction",
    "tasks",
    "appropriate",
    "list",
    "22",
    "features",
    "researchers",
    "came",
    "also",
    "sort",
    "alternative",
    "version",
    "17",
    "features",
    "rather",
    "22",
    "anyway",
    "point",
    "interested",
    "sort",
    "going",
    "sort",
    "feature",
    "generation",
    "research",
    "people",
    "might",
    "use",
    "box",
    "could",
    "something",
    "like",
    "hand",
    "might",
    "say",
    "22",
    "really",
    "high",
    "number",
    "sure",
    "sort",
    "feature",
    "generation",
    "many",
    "datasets",
    "sort",
    "try",
    "develop",
    "insights",
    "kind",
    "features",
    "apply",
    "universally",
    "great",
    "hand",
    "specific",
    "youth",
    "case",
    "like",
    "ekg",
    "kind",
    "seismological",
    "measurement",
    "time",
    "la",
    "massive",
    "earthquake",
    "whatever",
    "first",
    "go",
    "right",
    "general",
    "time",
    "series",
    "almost",
    "always",
    "domain",
    "knowledge",
    "looked",
    "data",
    "better",
    "generic",
    "feature",
    "set",
    "need",
    "also",
    "mentioning",
    "disciplined",
    "specific",
    "sort",
    "know",
    "underlying",
    "processes",
    "meaningful",
    "identify",
    "features",
    "left",
    "ekg",
    "love",
    "example",
    "periodic",
    "sorts",
    "interesting",
    "features",
    "sorts",
    "things",
    "example",
    "doctors",
    "see",
    "medical",
    "textbooks",
    "learn",
    "read",
    "time",
    "series",
    "right",
    "actually",
    "time",
    "series",
    "part",
    "diagnostics",
    "future",
    "generation",
    "right",
    "effectively",
    "identifying",
    "things",
    "like",
    "oh",
    "tall",
    "peak",
    "tall",
    "know",
    "long",
    "first",
    "peak",
    "tall",
    "peak",
    "temporal",
    "distance",
    "like",
    "last",
    "little",
    "dip",
    "first",
    "peak",
    "part",
    "diagnose",
    "heart",
    "illnesses",
    "right",
    "know",
    "relevant",
    "working",
    "data",
    "like",
    "would",
    "probably",
    "want",
    "work",
    "closely",
    "physician",
    "rather",
    "know",
    "developing",
    "scratch",
    "right",
    "would",
    "would",
    "sort",
    "waste",
    "time",
    "right",
    "case",
    "astronomical",
    "data",
    "another",
    "discipline",
    "time",
    "series",
    "features",
    "actually",
    "well",
    "developed",
    "whole",
    "python",
    "packages",
    "astronomical",
    "time",
    "series",
    "feature",
    "generations",
    "something",
    "want",
    "start",
    "looking",
    "always",
    "start",
    "discipline",
    "specific",
    "stuff",
    "time",
    "series",
    "features",
    "going",
    "compute",
    "features",
    "well",
    "one",
    "thing",
    "classify",
    "time",
    "series",
    "data",
    "example",
    "time",
    "series",
    "classification",
    "database",
    "show",
    "sort",
    "different",
    "classes",
    "time",
    "series",
    "data",
    "one",
    "thing",
    "think",
    "whole",
    "sample",
    "different",
    "time",
    "series",
    "would",
    "separate",
    "like",
    "gon",
    "na",
    "looking",
    "two",
    "ways",
    "thinking",
    "gon",
    "na",
    "use",
    "random",
    "forests",
    "gon",
    "na",
    "use",
    "gradient",
    "boosted",
    "trees",
    "case",
    "anyone",
    "familiar",
    "decision",
    "tree",
    "decision",
    "tree",
    "looks",
    "like",
    "right",
    "decision",
    "tree",
    "something",
    "like",
    "feed",
    "features",
    "data",
    "look",
    "one",
    "feature",
    "time",
    "sort",
    "make",
    "decisions",
    "branching",
    "decision",
    "tree",
    "branch",
    "branch",
    "branch",
    "end",
    "made",
    "series",
    "decisions",
    "sort",
    "binary",
    "various",
    "features",
    "get",
    "either",
    "classification",
    "label",
    "regression",
    "prediction",
    "right",
    "tree",
    "helpful",
    "general",
    "capture",
    "nonlinear",
    "dynamics",
    "also",
    "often",
    "fairly",
    "resistant",
    "sort",
    "extraneous",
    "garbage",
    "inputs",
    "also",
    "little",
    "bit",
    "less",
    "judicious",
    "sort",
    "feeding",
    "everything",
    "sometimes",
    "wo",
    "punished",
    "course",
    "one",
    "going",
    "actively",
    "advocate",
    "good",
    "remember",
    "also",
    "give",
    "give",
    "sense",
    "read",
    "right",
    "start",
    "getting",
    "sort",
    "sense",
    "things",
    "work",
    "imagine",
    "even",
    "doctor",
    "looking",
    "ekg",
    "sort",
    "think",
    "right",
    "trained",
    "even",
    "heuristic",
    "decision",
    "trees",
    "like",
    "decision",
    "trees",
    "machine",
    "learning",
    "two",
    "techniques",
    "going",
    "use",
    "based",
    "trees",
    "one",
    "random",
    "forest",
    "right",
    "build",
    "whole",
    "bunch",
    "trees",
    "one",
    "sort",
    "built",
    "subset",
    "features",
    "subset",
    "data",
    "majority",
    "voting",
    "winner",
    "know",
    "majority",
    "winner",
    "also",
    "going",
    "use",
    "xg",
    "boost",
    "guessing",
    "folks",
    "least",
    "heard",
    "xg",
    "boost",
    "gradient",
    "boosted",
    "trees",
    "idea",
    "random",
    "forest",
    "build",
    "however",
    "many",
    "trees",
    "gon",
    "na",
    "build",
    "parallel",
    "sort",
    "random",
    "subset",
    "data",
    "features",
    "build",
    "gradient",
    "boosted",
    "trees",
    "built",
    "sequentially",
    "first",
    "build",
    "best",
    "decision",
    "tree",
    "whatever",
    "parameters",
    "second",
    "decision",
    "tree",
    "built",
    "errors",
    "first",
    "decision",
    "tree",
    "decision",
    "tree",
    "sort",
    "trying",
    "correct",
    "errors",
    "previous",
    "one",
    "idea",
    "going",
    "add",
    "inputs",
    "right",
    "sort",
    "majority",
    "consensus",
    "everyone",
    "contributes",
    "little",
    "bit",
    "add",
    "trees",
    "good",
    "reason",
    "least",
    "mind",
    "work",
    "well",
    "time",
    "series",
    "specific",
    "data",
    "empirically",
    "last",
    "sort",
    "three",
    "years",
    "maybe",
    "even",
    "little",
    "bit",
    "become",
    "clear",
    "xg",
    "boost",
    "successful",
    "time",
    "series",
    "tasks",
    "much",
    "sort",
    "earlier",
    "applications",
    "machine",
    "learning",
    "time",
    "series",
    "highlight",
    "empirically",
    "true",
    "look",
    "competitions",
    "industry",
    "use",
    "cases",
    "also",
    "clustering",
    "thing",
    "gon",
    "na",
    "notebook",
    "guessing",
    "folks",
    "familiar",
    "concept",
    "case",
    "generally",
    "speaking",
    "clustering",
    "look",
    "like",
    "looks",
    "like",
    "look",
    "sort",
    "feature",
    "space",
    "data",
    "time",
    "series",
    "data",
    "data",
    "hope",
    "somehow",
    "features",
    "sort",
    "cluster",
    "way",
    "indicate",
    "sort",
    "specific",
    "types",
    "data",
    "points",
    "hope",
    "somehow",
    "means",
    "something",
    "fundamental",
    "produced",
    "data",
    "beautiful",
    "picture",
    "anyone",
    "actually",
    "see",
    "picture",
    "work",
    "though",
    "mean",
    "want",
    "job",
    "right",
    "really",
    "see",
    "well",
    "usually",
    "see",
    "something",
    "right",
    "sort",
    "squinting",
    "running",
    "analysis",
    "also",
    "tends",
    "true",
    "time",
    "series",
    "data",
    "also",
    "keep",
    "mind",
    "messiness",
    "real",
    "world",
    "data",
    "think",
    "picture",
    "somewhat",
    "fair",
    "version",
    "really",
    "nice",
    "time",
    "series",
    "right",
    "cluster",
    "looks",
    "like",
    "time",
    "series",
    "case",
    "right",
    "like",
    "see",
    "top",
    "two",
    "curves",
    "twelve",
    "eleven",
    "look",
    "similar",
    "right",
    "got",
    "three",
    "bumps",
    "sort",
    "space",
    "positionally",
    "seem",
    "approximately",
    "volatility",
    "time",
    "look",
    "ten",
    "nine",
    "gray",
    "even",
    "better",
    "case",
    "yeah",
    "look",
    "pretty",
    "similar",
    "know",
    "even",
    "features",
    "one",
    "gets",
    "little",
    "complicated",
    "start",
    "thinking",
    "well",
    "good",
    "enough",
    "good",
    "enough",
    "look",
    "green",
    "seven",
    "eight",
    "seven",
    "eight",
    "six",
    "also",
    "another",
    "good",
    "case",
    "start",
    "wondering",
    "know",
    "good",
    "good",
    "enough",
    "might",
    "little",
    "bit",
    "like",
    "real",
    "life",
    "know",
    "sort",
    "commonality",
    "even",
    "know",
    "could",
    "write",
    "code",
    "identify",
    "commonality",
    "get",
    "see",
    "difficult",
    "generate",
    "features",
    "especially",
    "want",
    "features",
    "robust",
    "time",
    "series",
    "time",
    "right",
    "course",
    "jumbled",
    "together",
    "need",
    "identify",
    "features",
    "would",
    "distinguish",
    "among",
    "really",
    "easy",
    "right",
    "like",
    "number",
    "peaks",
    "well",
    "number",
    "peaks",
    "blue",
    "versus",
    "brown",
    "might",
    "end",
    "even",
    "brown",
    "versus",
    "grey",
    "top",
    "know",
    "depending",
    "sort",
    "peak",
    "finding",
    "code",
    "write",
    "grey",
    "brown",
    "could",
    "three",
    "peaks",
    "come",
    "similar",
    "mean",
    "domain",
    "knowledge",
    "also",
    "sort",
    "look",
    "data",
    "time",
    "series",
    "clustering",
    "surprisingly",
    "difficult",
    "right",
    "conceptually",
    "think",
    "also",
    "difficult",
    "sometimes",
    "might",
    "looking",
    "via",
    "features",
    "ultimately",
    "think",
    "want",
    "want",
    "look",
    "original",
    "raw",
    "series",
    "make",
    "sure",
    "looks",
    "similar",
    "right",
    "intuitive",
    "sense",
    "eyeball",
    "looks",
    "similar",
    "want",
    "come",
    "together",
    "cluster",
    "computationally",
    "costly",
    "mentioned",
    "feature",
    "generation",
    "computationally",
    "costly",
    "go",
    "whole",
    "time",
    "series",
    "come",
    "features",
    "one",
    "data",
    "point",
    "remember",
    "time",
    "series",
    "whole",
    "time",
    "series",
    "like",
    "one",
    "curves",
    "one",
    "data",
    "point",
    "right",
    "curves",
    "actually",
    "thousand",
    "measurements",
    "know",
    "thousands",
    "thousands",
    "produce",
    "actually",
    "twelve",
    "inputs",
    "right",
    "twelve",
    "samples",
    "keep",
    "mind",
    "opposed",
    "data",
    "one",
    "pitfall",
    "want",
    "point",
    "might",
    "say",
    "well",
    "forget",
    "feature",
    "generation",
    "measure",
    "distance",
    "curves",
    "right",
    "could",
    "something",
    "like",
    "use",
    "euclidean",
    "distance",
    "almost",
    "never",
    "want",
    "use",
    "euclidean",
    "distance",
    "actually",
    "going",
    "cover",
    "really",
    "great",
    "example",
    "introduce",
    "another",
    "distance",
    "metric",
    "works",
    "lot",
    "better",
    "time",
    "series",
    "clustering",
    "used",
    "across",
    "many",
    "disciplines",
    "good",
    "aware",
    "mentioned",
    "medicine",
    "right",
    "looking",
    "ekg",
    "also",
    "finance",
    "especially",
    "sort",
    "1980s",
    "style",
    "finance",
    "people",
    "used",
    "plot",
    "things",
    "say",
    "oh",
    "month",
    "look",
    "like",
    "month",
    "60s",
    "maybe",
    "thing",
    "going",
    "happen",
    "kodak",
    "stock",
    "whatever",
    "definitely",
    "happens",
    "lot",
    "less",
    "method",
    "back",
    "day",
    "chemistry",
    "also",
    "like",
    "talked",
    "nmr",
    "spectra",
    "right",
    "effectively",
    "chemistry",
    "professors",
    "grad",
    "students",
    "tell",
    "go",
    "take",
    "spectra",
    "right",
    "look",
    "like",
    "one",
    "like",
    "one",
    "cluster",
    "examples",
    "well",
    "practical",
    "example",
    "even",
    "know",
    "person",
    "always",
    "admired",
    "blog",
    "post",
    "basically",
    "looked",
    "washington",
    "dc",
    "looked",
    "bike",
    "used",
    "per",
    "hour",
    "daily",
    "days",
    "week",
    "bike",
    "rental",
    "spots",
    "know",
    "guessing",
    "austin",
    "something",
    "like",
    "know",
    "sort",
    "dock",
    "bike",
    "member",
    "take",
    "bike",
    "another",
    "dock",
    "looked",
    "traffic",
    "one",
    "bikes",
    "taken",
    "different",
    "spots",
    "basically",
    "identified",
    "three",
    "kinds",
    "stations",
    "right",
    "one",
    "busy",
    "beginning",
    "day",
    "end",
    "one",
    "busy",
    "end",
    "day",
    "one",
    "busy",
    "beginning",
    "day",
    "concluded",
    "sort",
    "showed",
    "kind",
    "pattern",
    "people",
    "movements",
    "go",
    "work",
    "also",
    "sort",
    "go",
    "work",
    "apparently",
    "often",
    "go",
    "straight",
    "home",
    "would",
    "double",
    "bump",
    "situation",
    "plenty",
    "examples",
    "fairly",
    "clean",
    "clustering",
    "wild",
    "think",
    "especially",
    "true",
    "sort",
    "human",
    "behavior",
    "turns",
    "really",
    "boring",
    "patterns",
    "like",
    "go",
    "work",
    "come",
    "home",
    "work",
    "actually",
    "really",
    "boring",
    "look",
    "human",
    "data",
    "little",
    "bit",
    "noisier",
    "look",
    "things",
    "like",
    "astronomical",
    "data",
    "financial",
    "time",
    "series",
    "sort",
    "thing",
    "sometimes",
    "get",
    "really",
    "beautiful",
    "clustering",
    "finally",
    "yes",
    "okay",
    "finally",
    "last",
    "overview",
    "want",
    "talk",
    "distance",
    "metric",
    "going",
    "look",
    "time",
    "series",
    "called",
    "dynamic",
    "time",
    "warping",
    "going",
    "alternative",
    "solution",
    "clustering",
    "problem",
    "right",
    "highlighted",
    "might",
    "generate",
    "features",
    "features",
    "might",
    "work",
    "well",
    "looking",
    "sort",
    "one",
    "particular",
    "class",
    "time",
    "series",
    "difficult",
    "generate",
    "features",
    "robust",
    "across",
    "many",
    "different",
    "classes",
    "time",
    "series",
    "within",
    "data",
    "set",
    "another",
    "option",
    "people",
    "come",
    "quite",
    "computationally",
    "taxing",
    "works",
    "pretty",
    "well",
    "called",
    "dynamic",
    "time",
    "warping",
    "see",
    "imagine",
    "solid",
    "curves",
    "two",
    "time",
    "series",
    "trying",
    "line",
    "basically",
    "tries",
    "find",
    "tries",
    "keep",
    "parallel",
    "find",
    "way",
    "maximize",
    "goodness",
    "fit",
    "almost",
    "nice",
    "thing",
    "tends",
    "correspond",
    "pretty",
    "well",
    "visually",
    "without",
    "actually",
    "image",
    "analysis",
    "right",
    "looks",
    "like",
    "sort",
    "squint",
    "see",
    "two",
    "two",
    "peaks",
    "beginning",
    "brains",
    "want",
    "match",
    "computationally",
    "going",
    "revisit",
    "see",
    "looks",
    "like",
    "overview",
    "gon",
    "na",
    "notebook",
    "let",
    "go",
    "notebook",
    "number",
    "three",
    "gon",
    "na",
    "go",
    "state",
    "space",
    "machine",
    "learning",
    "start",
    "trees",
    "clustering",
    "classification",
    "prediction",
    "okay",
    "data",
    "set",
    "going",
    "start",
    "data",
    "set",
    "available",
    "via",
    "cesium",
    "library",
    "eeg",
    "data",
    "set",
    "brain",
    "rather",
    "heart",
    "sure",
    "somebody",
    "room",
    "knows",
    "ton",
    "feel",
    "free",
    "pipe",
    "sort",
    "input",
    "always",
    "interesting",
    "alright",
    "visualized",
    "see",
    "even",
    "sort",
    "qualitatively",
    "see",
    "difference",
    "three",
    "samples",
    "see",
    "bottom",
    "sort",
    "one",
    "highly",
    "volatile",
    "sample",
    "sort",
    "place",
    "top",
    "two",
    "sort",
    "less",
    "volatile",
    "volatility",
    "though",
    "look",
    "also",
    "spot",
    "differences",
    "values",
    "right",
    "top",
    "plot",
    "looks",
    "like",
    "might",
    "sort",
    "gaussian",
    "around",
    "zero",
    "sort",
    "range",
    "like",
    "minus",
    "one",
    "hundred",
    "one",
    "hundred",
    "second",
    "plot",
    "look",
    "visually",
    "look",
    "different",
    "top",
    "plot",
    "look",
    "values",
    "one",
    "ranges",
    "zero",
    "minus",
    "hundred",
    "right",
    "actual",
    "values",
    "case",
    "seem",
    "kind",
    "classes",
    "look",
    "last",
    "one",
    "might",
    "sort",
    "gaussian",
    "hard",
    "know",
    "see",
    "range",
    "also",
    "quite",
    "different",
    "500",
    "minus",
    "500",
    "right",
    "already",
    "spot",
    "want",
    "find",
    "ways",
    "sort",
    "preserve",
    "information",
    "value",
    "thinking",
    "generating",
    "features",
    "far",
    "kind",
    "data",
    "looking",
    "looking",
    "numpy",
    "array",
    "served",
    "within",
    "dictionary",
    "giving",
    "us",
    "times",
    "measurements",
    "classes",
    "something",
    "aware",
    "measurements",
    "list",
    "looks",
    "like",
    "got",
    "500",
    "samples",
    "look",
    "one",
    "samples",
    "four",
    "thousand",
    "ninety",
    "seven",
    "points",
    "long",
    "already",
    "get",
    "sense",
    "oh",
    "goodness",
    "five",
    "hundred",
    "time",
    "series",
    "four",
    "thousand",
    "ninety",
    "seven",
    "points",
    "see",
    "already",
    "gets",
    "little",
    "bit",
    "computationally",
    "taxing",
    "go",
    "next",
    "generate",
    "features",
    "feel",
    "free",
    "go",
    "ahead",
    "hit",
    "button",
    "want",
    "least",
    "laptop",
    "takes",
    "really",
    "long",
    "time",
    "gon",
    "na",
    "load",
    "prepared",
    "data",
    "code",
    "right",
    "extract",
    "extracted",
    "amplitude",
    "percent",
    "beyond",
    "one",
    "standard",
    "deviation",
    "right",
    "measure",
    "like",
    "many",
    "outliers",
    "spread",
    "percent",
    "close",
    "media",
    "write",
    "sort",
    "measure",
    "tightly",
    "hewing",
    "central",
    "values",
    "skew",
    "distribution",
    "also",
    "max",
    "slope",
    "sort",
    "much",
    "jumped",
    "absolute",
    "value",
    "one",
    "data",
    "point",
    "another",
    "find",
    "features",
    "well",
    "looked",
    "looked",
    "data",
    "little",
    "bit",
    "saw",
    "fit",
    "reasonably",
    "well",
    "looked",
    "caesium",
    "documentation",
    "generated",
    "see",
    "load",
    "csv",
    "see",
    "see",
    "see",
    "sort",
    "reasonable",
    "variation",
    "look",
    "shape",
    "shape",
    "500",
    "6",
    "actually",
    "one",
    "channel",
    "even",
    "relevant",
    "digested",
    "x",
    "points",
    "500",
    "6",
    "points",
    "right",
    "laptop",
    "situation",
    "particular",
    "enormous",
    "time",
    "series",
    "database",
    "still",
    "need",
    "compress",
    "way",
    "right",
    "really",
    "shrunk",
    "data",
    "gon",
    "na",
    "skip",
    "exercise",
    "validating",
    "slash",
    "calculating",
    "features",
    "want",
    "point",
    "care",
    "right",
    "cesium",
    "cesium",
    "feature",
    "generation",
    "library",
    "one",
    "plenty",
    "python",
    "feature",
    "generation",
    "libraries",
    "like",
    "talked",
    "dataset",
    "library",
    "specialized",
    "mentioned",
    "astronomical",
    "time",
    "series",
    "feature",
    "generation",
    "libraries",
    "good",
    "know",
    "libraries",
    "available",
    "especially",
    "things",
    "like",
    "features",
    "absolutely",
    "write",
    "code",
    "features",
    "pain",
    "butt",
    "right",
    "sort",
    "standard",
    "things",
    "like",
    "calculating",
    "skew",
    "calculating",
    "max",
    "loop",
    "hand",
    "code",
    "alright",
    "especially",
    "exploratory",
    "component",
    "might",
    "want",
    "farm",
    "existing",
    "library",
    "check",
    "time",
    "indeed",
    "features",
    "calculate",
    "match",
    "features",
    "produced",
    "algorithm",
    "check",
    "one",
    "data",
    "point",
    "also",
    "sort",
    "think",
    "okay",
    "well",
    "meaningful",
    "one",
    "thing",
    "example",
    "histogram",
    "amplitude",
    "feature",
    "class",
    "looks",
    "like",
    "least",
    "one",
    "class",
    "right",
    "one",
    "class",
    "least",
    "distinguishes",
    "compared",
    "okay",
    "least",
    "preliminary",
    "evidence",
    "features",
    "help",
    "distinguish",
    "classes",
    "want",
    "okay",
    "examples",
    "plotting",
    "histograms",
    "check",
    "features",
    "generated",
    "somewhat",
    "useful",
    "fact",
    "go",
    "quite",
    "bit",
    "feature",
    "importance",
    "analysis",
    "time",
    "time",
    "series",
    "like",
    "would",
    "data",
    "right",
    "features",
    "producing",
    "tend",
    "way",
    "correlate",
    "outcome",
    "interested",
    "b",
    "classifier",
    "forecast",
    "whatever",
    "definitely",
    "would",
    "want",
    "go",
    "check",
    "sensible",
    "also",
    "ultimately",
    "set",
    "features",
    "probably",
    "point",
    "would",
    "want",
    "code",
    "right",
    "mentioned",
    "cesium",
    "feature",
    "generation",
    "libraries",
    "useful",
    "sense",
    "code",
    "things",
    "repeatedly",
    "solutions",
    "one",
    "enough",
    "library",
    "even",
    "better",
    "really",
    "know",
    "line",
    "line",
    "know",
    "works",
    "ultimately",
    "really",
    "well",
    "developed",
    "set",
    "features",
    "also",
    "want",
    "solution",
    "coded",
    "general",
    "hand",
    "know",
    "three",
    "features",
    "always",
    "calculate",
    "one",
    "thing",
    "keep",
    "mind",
    "sometimes",
    "features",
    "calculated",
    "together",
    "cut",
    "computational",
    "cost",
    "right",
    "looking",
    "min",
    "max",
    "reason",
    "one",
    "pass",
    "data",
    "find",
    "max",
    "another",
    "pass",
    "data",
    "find",
    "men",
    "right",
    "might",
    "want",
    "group",
    "things",
    "go",
    "whole",
    "data",
    "way",
    "together",
    "something",
    "future",
    "generation",
    "library",
    "yet",
    "seen",
    "example",
    "though",
    "love",
    "anyone",
    "one",
    "somebody",
    "sort",
    "coding",
    "things",
    "take",
    "advantage",
    "would",
    "great",
    "contribution",
    "open",
    "source",
    "community",
    "anyone",
    "wants",
    "meantime",
    "aware",
    "even",
    "really",
    "tiny",
    "data",
    "set",
    "found",
    "weak",
    "quite",
    "generate",
    "features",
    "right",
    "blow",
    "quickly",
    "want",
    "spend",
    "whole",
    "afternoons",
    "work",
    "sort",
    "sitting",
    "waiting",
    "features",
    "may",
    "even",
    "useful",
    "okay",
    "future",
    "generation",
    "quite",
    "straightforward",
    "case",
    "sort",
    "went",
    "features",
    "going",
    "see",
    "work",
    "prepare",
    "training",
    "test",
    "set",
    "let",
    "roll",
    "random",
    "forest",
    "classifier",
    "going",
    "use",
    "ten",
    "estimators",
    "max",
    "depth",
    "three",
    "set",
    "seed",
    "particular",
    "reason",
    "saying",
    "okay",
    "random",
    "forest",
    "fit",
    "score",
    "training",
    "data",
    "got",
    "like",
    "62",
    "percent",
    "percent",
    "accuracy",
    "good",
    "news",
    "test",
    "data",
    "much",
    "lower",
    "seem",
    "like",
    "fit",
    "seems",
    "like",
    "reasonably",
    "reasonably",
    "generalized",
    "model",
    "look",
    "sort",
    "came",
    "back",
    "also",
    "looks",
    "reasonably",
    "well",
    "distributed",
    "five",
    "classes",
    "sort",
    "distributed",
    "underlying",
    "test",
    "data",
    "something",
    "interesting",
    "know",
    "thinking",
    "possible",
    "null",
    "model",
    "right",
    "think",
    "62",
    "good",
    "accuracy",
    "course",
    "want",
    "think",
    "well",
    "would",
    "even",
    "really",
    "dumb",
    "model",
    "right",
    "five",
    "five",
    "classes",
    "dumb",
    "model",
    "would",
    "60",
    "accuracy",
    "right",
    "sort",
    "sanity",
    "test",
    "data",
    "indeed",
    "something",
    "random",
    "forest",
    "right",
    "point",
    "okay",
    "put",
    "feature",
    "get",
    "classification",
    "far",
    "better",
    "random",
    "classification",
    "five",
    "features",
    "boil",
    "essentially",
    "data",
    "points",
    "five",
    "data",
    "points",
    "without",
    "even",
    "much",
    "exploration",
    "could",
    "already",
    "something",
    "right",
    "time",
    "series",
    "classification",
    "hard",
    "get",
    "kind",
    "result",
    "better",
    "random",
    "really",
    "boils",
    "whole",
    "dataset",
    "numbers",
    "depends",
    "course",
    "quality",
    "data",
    "looking",
    "okay",
    "going",
    "run",
    "exact",
    "analysis",
    "xg",
    "boost",
    "instead",
    "right",
    "going",
    "use",
    "extra",
    "boost",
    "classifier",
    "going",
    "use",
    "ten",
    "estimators",
    "max",
    "depth",
    "three",
    "gon",
    "na",
    "fit",
    "gon",
    "na",
    "score",
    "see",
    "improved",
    "right",
    "away",
    "random",
    "forest",
    "model",
    "rule",
    "thumb",
    "quite",
    "expected",
    "time",
    "series",
    "mentioning",
    "extra",
    "boost",
    "gradient",
    "boosted",
    "trees",
    "tend",
    "extremely",
    "well",
    "time",
    "series",
    "data",
    "relative",
    "something",
    "keep",
    "mind",
    "although",
    "course",
    "see",
    "little",
    "bit",
    "evidence",
    "overfitting",
    "might",
    "want",
    "prune",
    "back",
    "reminder",
    "get",
    "extra",
    "boost",
    "get",
    "automatically",
    "random",
    "forest",
    "measure",
    "future",
    "importance",
    "iterating",
    "thinking",
    "okay",
    "well",
    "features",
    "turned",
    "useful",
    "especially",
    "want",
    "go",
    "time",
    "series",
    "plot",
    "time",
    "time",
    "series",
    "noisy",
    "visual",
    "giving",
    "much",
    "information",
    "use",
    "something",
    "like",
    "feature",
    "importance",
    "one",
    "way",
    "thinking",
    "kinds",
    "features",
    "useful",
    "might",
    "look",
    "say",
    "okay",
    "feature",
    "0",
    "useful",
    "feature",
    "0",
    "anyway",
    "go",
    "way",
    "back",
    "amplitude",
    "right",
    "actually",
    "one",
    "things",
    "jumped",
    "us",
    "says",
    "amplitude",
    "useful",
    "right",
    "indicators",
    "sort",
    "numerical",
    "range",
    "useful",
    "one",
    "thing",
    "might",
    "improve",
    "model",
    "say",
    "okay",
    "well",
    "sorts",
    "features",
    "provide",
    "similar",
    "complimentary",
    "information",
    "right",
    "maybe",
    "addition",
    "amplitude",
    "maybe",
    "need",
    "add",
    "mean",
    "something",
    "like",
    "provide",
    "information",
    "use",
    "way",
    "sort",
    "learning",
    "also",
    "kind",
    "data",
    "useful",
    "classification",
    "also",
    "want",
    "think",
    "forecasting",
    "though",
    "right",
    "case",
    "going",
    "take",
    "look",
    "air",
    "passengers",
    "data",
    "set",
    "highlighted",
    "slide",
    "beginning",
    "lecture",
    "gon",
    "na",
    "load",
    "air",
    "passengers",
    "data",
    "set",
    "take",
    "look",
    "looks",
    "like",
    "see",
    "got",
    "monthly",
    "data",
    "going",
    "back",
    "1949",
    "set",
    "index",
    "take",
    "look",
    "looks",
    "like",
    "let",
    "plot",
    "see",
    "plot",
    "definitely",
    "seasonal",
    "component",
    "also",
    "trend",
    "component",
    "going",
    "say",
    "arima",
    "modeling",
    "would",
    "definitely",
    "need",
    "make",
    "stationary",
    "per",
    "se",
    "requirement",
    "something",
    "like",
    "machine",
    "learning",
    "generally",
    "good",
    "idea",
    "right",
    "considerations",
    "data",
    "looks",
    "like",
    "traditional",
    "statistical",
    "models",
    "still",
    "things",
    "make",
    "data",
    "cleaner",
    "easier",
    "digest",
    "machine",
    "learning",
    "model",
    "thing",
    "log",
    "transforming",
    "also",
    "dipping",
    "get",
    "time",
    "series",
    "uniform",
    "variants",
    "values",
    "right",
    "good",
    "thing",
    "basically",
    "making",
    "sub",
    "series",
    "time",
    "series",
    "comparable",
    "right",
    "going",
    "many",
    "different",
    "options",
    "many",
    "different",
    "samples",
    "sliding",
    "window",
    "right",
    "could",
    "one",
    "sample",
    "could",
    "another",
    "sample",
    "could",
    "another",
    "sample",
    "case",
    "one",
    "baseline",
    "time",
    "series",
    "gon",
    "na",
    "convert",
    "many",
    "samples",
    "machine",
    "learning",
    "algorithm",
    "see",
    "another",
    "difference",
    "compared",
    "statistical",
    "model",
    "state",
    "space",
    "model",
    "statistical",
    "state",
    "space",
    "model",
    "model",
    "inputting",
    "data",
    "one",
    "long",
    "time",
    "series",
    "throw",
    "hand",
    "machine",
    "learning",
    "model",
    "nothing",
    "aware",
    "temporal",
    "component",
    "data",
    "instead",
    "take",
    "different",
    "time",
    "windows",
    "chop",
    "time",
    "series",
    "become",
    "different",
    "samples",
    "becomes",
    "way",
    "use",
    "one",
    "time",
    "series",
    "model",
    "one",
    "time",
    "series",
    "machine",
    "learning",
    "different",
    "classification",
    "trying",
    "forecast",
    "trying",
    "forecast",
    "one",
    "series",
    "particular",
    "many",
    "time",
    "series",
    "would",
    "different",
    "consideration",
    "want",
    "time",
    "series",
    "one",
    "sample",
    "even",
    "case",
    "want",
    "chop",
    "time",
    "series",
    "sub",
    "samples",
    "use",
    "case",
    "want",
    "make",
    "sure",
    "since",
    "going",
    "sliding",
    "window",
    "across",
    "would",
    "like",
    "make",
    "data",
    "points",
    "comparable",
    "one",
    "another",
    "possible",
    "think",
    "machine",
    "learning",
    "perspective",
    "right",
    "machine",
    "learning",
    "algorithm",
    "sort",
    "sees",
    "slice",
    "slice",
    "way",
    "sort",
    "knowing",
    "dynamics",
    "carried",
    "around",
    "forward",
    "dynamics",
    "unique",
    "particular",
    "aspect",
    "whereas",
    "give",
    "clearly",
    "comparable",
    "one",
    "another",
    "thanks",
    "transformations",
    "would",
    "need",
    "make",
    "sure",
    "transform",
    "back",
    "end",
    "wanted",
    "check",
    "predictions",
    "going",
    "time",
    "series",
    "diff",
    "log",
    "values",
    "exercise",
    "going",
    "ask",
    "think",
    "one",
    "time",
    "series",
    "form",
    "sort",
    "components",
    "full",
    "range",
    "time",
    "look",
    "similar",
    "convert",
    "many",
    "samples",
    "take",
    "couple",
    "minutes",
    "think",
    "would",
    "chop",
    "create",
    "different",
    "sub",
    "time",
    "series",
    "samples",
    "okay",
    "let",
    "take",
    "look",
    "together",
    "decided",
    "reason",
    "especially",
    "great",
    "anything",
    "decided",
    "break",
    "twelve",
    "months",
    "sort",
    "look",
    "year",
    "time",
    "slice",
    "arguably",
    "great",
    "ca",
    "really",
    "capture",
    "seasonality",
    "one",
    "sample",
    "season",
    "said",
    "heck",
    "gon",
    "na",
    "um",
    "twelve",
    "steps",
    "think",
    "twelve",
    "steps",
    "give",
    "opportunity",
    "maybe",
    "least",
    "able",
    "pinpoint",
    "seasonal",
    "cycle",
    "looking",
    "many",
    "many",
    "examples",
    "whereas",
    "months",
    "probably",
    "ca",
    "spot",
    "part",
    "rationale",
    "wanted",
    "first",
    "give",
    "pre",
    "allocated",
    "array",
    "val",
    "look",
    "hundred",
    "forty",
    "three",
    "twelve",
    "basically",
    "stacked",
    "time",
    "series",
    "twelve",
    "times",
    "gon",
    "na",
    "want",
    "twelve",
    "time",
    "steps",
    "gon",
    "na",
    "lag",
    "sure",
    "elegant",
    "way",
    "used",
    "loop",
    "know",
    "genki",
    "solution",
    "think",
    "hard",
    "basically",
    "column",
    "shift",
    "however",
    "many",
    "lags",
    "want",
    "right",
    "want",
    "first",
    "value",
    "12",
    "months",
    "past",
    "relative",
    "last",
    "value",
    "want",
    "move",
    "forward",
    "time",
    "get",
    "valve",
    "132",
    "12",
    "go",
    "hundred",
    "forty",
    "three",
    "samples",
    "132",
    "samples",
    "well",
    "lost",
    "samples",
    "full",
    "12",
    "months",
    "right",
    "beginning",
    "time",
    "series",
    "even",
    "12",
    "months",
    "go",
    "12",
    "months",
    "time",
    "series",
    "even",
    "one",
    "sub",
    "sample",
    "time",
    "series",
    "look",
    "look",
    "say",
    "last",
    "steps",
    "time",
    "series",
    "right",
    "time",
    "series",
    "depicted",
    "right",
    "last",
    "last",
    "twelve",
    "steps",
    "last",
    "values",
    "right",
    "twelve",
    "last",
    "values",
    "global",
    "time",
    "series",
    "becomes",
    "last",
    "sample",
    "broken",
    "time",
    "series",
    "individual",
    "windows",
    "right",
    "vallis",
    "minus",
    "one",
    "last",
    "sample",
    "going",
    "way",
    "process",
    "machine",
    "learning",
    "algorithm",
    "notice",
    "values",
    "right",
    "basically",
    "converted",
    "box",
    "say",
    "last",
    "twelve",
    "samples",
    "sort",
    "one",
    "sample",
    "time",
    "series",
    "one",
    "thing",
    "hope",
    "noticed",
    "could",
    "actually",
    "samples",
    "right",
    "hundred",
    "would",
    "ways",
    "least",
    "think",
    "one",
    "way",
    "let",
    "see",
    "two",
    "overlap",
    "look",
    "see",
    "minus",
    "one",
    "six",
    "seven",
    "oh",
    "nine",
    "minus",
    "one",
    "six",
    "seven",
    "oh",
    "nine",
    "actually",
    "fantastic",
    "point",
    "deep",
    "learning",
    "example",
    "like",
    "trick",
    "overlapping",
    "could",
    "chopped",
    "time",
    "windows",
    "kind",
    "silly",
    "think",
    "want",
    "time",
    "series",
    "sliding",
    "window",
    "actually",
    "done",
    "case",
    "way",
    "coded",
    "lagging",
    "one",
    "time",
    "rather",
    "chopping",
    "whereas",
    "done",
    "say",
    "reshape",
    "certainly",
    "would",
    "overlap",
    "would",
    "put",
    "put",
    "back",
    "overlap",
    "already",
    "using",
    "point",
    "much",
    "guess",
    "think",
    "want",
    "point",
    "reduce",
    "number",
    "steps",
    "could",
    "get",
    "couple",
    "time",
    "points",
    "right",
    "six",
    "steps",
    "instead",
    "twelve",
    "would",
    "get",
    "six",
    "sequences",
    "time",
    "wo",
    "matter",
    "maybe",
    "small",
    "set",
    "might",
    "matter",
    "something",
    "think",
    "another",
    "thing",
    "could",
    "think",
    "less",
    "used",
    "machine",
    "learning",
    "compared",
    "deep",
    "learning",
    "could",
    "think",
    "data",
    "augmentation",
    "domain",
    "knowledge",
    "think",
    "ways",
    "augment",
    "data",
    "way",
    "realistic",
    "okay",
    "prepare",
    "data",
    "created",
    "shorter",
    "time",
    "series",
    "actually",
    "created",
    "features",
    "yet",
    "right",
    "forecasting",
    "additional",
    "step",
    "first",
    "take",
    "long",
    "series",
    "break",
    "smaller",
    "series",
    "still",
    "need",
    "feature",
    "eyes",
    "right",
    "still",
    "need",
    "thing",
    "earlier",
    "classification",
    "going",
    "convert",
    "measures",
    "list",
    "cesium",
    "expects",
    "right",
    "got",
    "list",
    "measures",
    "measure",
    "11",
    "times",
    "11",
    "times",
    "steps",
    "long",
    "gon",
    "na",
    "feature",
    "eyes",
    "one",
    "gon",
    "na",
    "run",
    "guys",
    "see",
    "one",
    "small",
    "enough",
    "actually",
    "see",
    "least",
    "laptop",
    "still",
    "takes",
    "right",
    "even",
    "really",
    "tiny",
    "data",
    "set",
    "instant",
    "part",
    "general",
    "library",
    "part",
    "time",
    "series",
    "feature",
    "generation",
    "slow",
    "look",
    "columns",
    "got",
    "think",
    "features",
    "used",
    "sort",
    "generic",
    "set",
    "features",
    "look",
    "histogram",
    "let",
    "see",
    "oh",
    "well",
    "see",
    "sort",
    "multimodal",
    "like",
    "wow",
    "cool",
    "maybe",
    "underlying",
    "populations",
    "get",
    "excited",
    "possible",
    "structure",
    "similarly",
    "look",
    "percent",
    "amplitude",
    "see",
    "something",
    "might",
    "potentially",
    "multimodal",
    "seems",
    "like",
    "maybe",
    "could",
    "good",
    "going",
    "use",
    "tree",
    "kind",
    "regression",
    "task",
    "like",
    "multimodal",
    "suggests",
    "sort",
    "meaningful",
    "data",
    "underneath",
    "gon",
    "na",
    "run",
    "gon",
    "na",
    "run",
    "instead",
    "xg",
    "boost",
    "classifier",
    "gon",
    "na",
    "run",
    "regresar",
    "actually",
    "process",
    "converted",
    "inputs",
    "features",
    "except",
    "looking",
    "numerical",
    "output",
    "forecast",
    "value",
    "instead",
    "label",
    "going",
    "forecasted",
    "value",
    "run",
    "model",
    "oh",
    "well",
    "rms",
    "dropping",
    "great",
    "hand",
    "mse",
    "hard",
    "digest",
    "right",
    "really",
    "mean",
    "things",
    "scatter",
    "plot",
    "test",
    "data",
    "notice",
    "little",
    "test",
    "data",
    "small",
    "data",
    "set",
    "never",
    "really",
    "use",
    "machine",
    "learning",
    "tiny",
    "data",
    "set",
    "right",
    "keep",
    "mind",
    "know",
    "application",
    "would",
    "points",
    "also",
    "means",
    "could",
    "potentially",
    "waiting",
    "many",
    "hours",
    "generate",
    "feature",
    "obviously",
    "keep",
    "mind",
    "future",
    "generation",
    "sort",
    "blow",
    "look",
    "testing",
    "even",
    "training",
    "look",
    "especially",
    "good",
    "really",
    "bad",
    "right",
    "telling",
    "guys",
    "amazing",
    "xg",
    "boost",
    "really",
    "contributed",
    "machine",
    "learning",
    "time",
    "series",
    "even",
    "negative",
    "correlation",
    "prediction",
    "actual",
    "value",
    "really",
    "bad",
    "right",
    "means",
    "sort",
    "actively",
    "worse",
    "even",
    "chance",
    "would",
    "better",
    "sort",
    "predict",
    "0",
    "always",
    "rather",
    "run",
    "model",
    "certainly",
    "want",
    "tell",
    "boss",
    "exercise",
    "reader",
    "although",
    "scroll",
    "see",
    "answer",
    "think",
    "minute",
    "ones",
    "terribly",
    "wrong",
    "remember",
    "actually",
    "really",
    "easy",
    "data",
    "set",
    "right",
    "showed",
    "beginning",
    "whole",
    "tutorial",
    "air",
    "passengers",
    "data",
    "traditional",
    "arima",
    "great",
    "ca",
    "anything",
    "certainly",
    "transforming",
    "makes",
    "harder",
    "right",
    "actually",
    "frequent",
    "sort",
    "mistake",
    "see",
    "research",
    "papers",
    "also",
    "blogs",
    "sometimes",
    "people",
    "get",
    "amazing",
    "values",
    "amazing",
    "correlations",
    "mainly",
    "transformed",
    "sure",
    "like",
    "prediction",
    "goes",
    "magnitude",
    "actually",
    "utility",
    "forecast",
    "sort",
    "delta",
    "especially",
    "dipped",
    "makes",
    "much",
    "harder",
    "task",
    "diff",
    "time",
    "series",
    "predicting",
    "last",
    "value",
    "next",
    "one",
    "great",
    "strategy",
    "turned",
    "harder",
    "problem",
    "actually",
    "transform",
    "arima",
    "model",
    "still",
    "get",
    "great",
    "results",
    "maybe",
    "get",
    "impressive",
    "sounding",
    "numbers",
    "still",
    "better",
    "else",
    "could",
    "gone",
    "wrong",
    "think",
    "sort",
    "took",
    "sliding",
    "window",
    "basically",
    "year",
    "worth",
    "data",
    "summarized",
    "things",
    "like",
    "amplitude",
    "else",
    "use",
    "let",
    "go",
    "back",
    "percent",
    "beyond",
    "one",
    "standard",
    "deviation",
    "skew",
    "max",
    "slope",
    "etc",
    "else",
    "could",
    "gone",
    "wrong",
    "might",
    "buy",
    "helpful",
    "owl",
    "model",
    "might",
    "buy",
    "thing",
    "like",
    "oh",
    "well",
    "maybe",
    "data",
    "amenable",
    "analysis",
    "like",
    "evolving",
    "much",
    "time",
    "reasonable",
    "forecast",
    "guess",
    "rebuttals",
    "right",
    "one",
    "rebuttal",
    "even",
    "eyes",
    "sort",
    "see",
    "something",
    "able",
    "find",
    "technique",
    "also",
    "mentioned",
    "arima",
    "even",
    "though",
    "model",
    "already",
    "know",
    "apparent",
    "log",
    "transform",
    "algorithm",
    "worth",
    "salt",
    "work",
    "made",
    "transform",
    "makes",
    "uniform",
    "absolutely",
    "think",
    "things",
    "de",
    "trend",
    "yes",
    "could",
    "say",
    "removing",
    "trend",
    "may",
    "removed",
    "signal",
    "think",
    "one",
    "way",
    "framing",
    "said",
    "saying",
    "could",
    "one",
    "possibility",
    "aware",
    "model",
    "works",
    "especially",
    "work",
    "fields",
    "actually",
    "even",
    "best",
    "model",
    "pretty",
    "bad",
    "debates",
    "day",
    "sometimes",
    "know",
    "right",
    "signal",
    "left",
    "bad",
    "model",
    "know",
    "people",
    "whole",
    "job",
    "figuring",
    "trying",
    "figure",
    "happens",
    "lot",
    "time",
    "serious",
    "case",
    "though",
    "actually",
    "let",
    "call",
    "attention",
    "really",
    "look",
    "future",
    "set",
    "case",
    "computed",
    "feature",
    "set",
    "first",
    "five",
    "data",
    "points",
    "notice",
    "lot",
    "values",
    "right",
    "mean",
    "looking",
    "rather",
    "let",
    "say",
    "picture",
    "time",
    "series",
    "mean",
    "really",
    "supposed",
    "percent",
    "amplitude",
    "two",
    "different",
    "numbers",
    "max",
    "slope",
    "number",
    "throughout",
    "think",
    "right",
    "let",
    "let",
    "plot",
    "three",
    "samples",
    "data",
    "right",
    "remember",
    "sliding",
    "window",
    "problem",
    "created",
    "points",
    "sliding",
    "windows",
    "data",
    "produced",
    "tends",
    "look",
    "things",
    "like",
    "amplitude",
    "numerical",
    "qualities",
    "rather",
    "say",
    "positional",
    "qualities",
    "right",
    "look",
    "eye",
    "three",
    "curves",
    "see",
    "different",
    "sort",
    "different",
    "phases",
    "cycle",
    "see",
    "sort",
    "phase",
    "information",
    "positional",
    "information",
    "great",
    "point",
    "algorithm",
    "right",
    "seeing",
    "anything",
    "sort",
    "positional",
    "information",
    "like",
    "telling",
    "peak",
    "seeing",
    "information",
    "numbers",
    "right",
    "looking",
    "time",
    "series",
    "along",
    "axis",
    "actually",
    "seeing",
    "anything",
    "differentiates",
    "three",
    "right",
    "along",
    "axis",
    "value",
    "axis",
    "different",
    "part",
    "wrong",
    "used",
    "features",
    "describe",
    "sort",
    "underlying",
    "values",
    "produced",
    "process",
    "without",
    "providing",
    "features",
    "describe",
    "temporal",
    "structure",
    "rate",
    "structure",
    "along",
    "turned",
    "reasonably",
    "okay",
    "eeg",
    "data",
    "already",
    "much",
    "variation",
    "numerical",
    "axis",
    "even",
    "really",
    "need",
    "treat",
    "time",
    "series",
    "right",
    "could",
    "sort",
    "summarize",
    "values",
    "saw",
    "got",
    "reasonable",
    "classification",
    "data",
    "set",
    "going",
    "cut",
    "right",
    "different",
    "samples",
    "different",
    "along",
    "numerical",
    "axis",
    "different",
    "along",
    "positional",
    "axis",
    "let",
    "revisit",
    "generate",
    "features",
    "encode",
    "kind",
    "positional",
    "information",
    "one",
    "stab",
    "basically",
    "going",
    "generate",
    "six",
    "features",
    "features",
    "generated",
    "let",
    "read",
    "okay",
    "first",
    "one",
    "np",
    "dot",
    "values",
    "equal",
    "max",
    "want",
    "know",
    "time",
    "step",
    "max",
    "occur",
    "next",
    "feature",
    "np",
    "dot",
    "values",
    "equal",
    "men",
    "want",
    "know",
    "time",
    "step",
    "minimum",
    "occur",
    "right",
    "going",
    "back",
    "two",
    "features",
    "going",
    "give",
    "information",
    "axis",
    "rather",
    "axis",
    "need",
    "differentiate",
    "samples",
    "got",
    "got",
    "distance",
    "min",
    "max",
    "notice",
    "actually",
    "distance",
    "positional",
    "right",
    "min",
    "occurs",
    "max",
    "versus",
    "positive",
    "negative",
    "right",
    "giving",
    "directionality",
    "relationship",
    "men",
    "max",
    "also",
    "still",
    "gon",
    "na",
    "keep",
    "max",
    "might",
    "useful",
    "figure",
    "half",
    "case",
    "matters",
    "gon",
    "na",
    "take",
    "taking",
    "taking",
    "last",
    "one",
    "next",
    "last",
    "one",
    "well",
    "case",
    "really",
    "short",
    "time",
    "series",
    "right",
    "11",
    "steps",
    "seems",
    "like",
    "look",
    "end",
    "end",
    "last",
    "points",
    "three",
    "distinguished",
    "even",
    "though",
    "sort",
    "neighboring",
    "sliding",
    "windows",
    "part",
    "different",
    "end",
    "saying",
    "want",
    "features",
    "sort",
    "focus",
    "behavior",
    "occurs",
    "end",
    "sort",
    "blindly",
    "throwing",
    "idea",
    "need",
    "understand",
    "positional",
    "relevance",
    "let",
    "let",
    "make",
    "sure",
    "features",
    "different",
    "really",
    "trouble",
    "appear",
    "least",
    "little",
    "bit",
    "different",
    "actually",
    "frame",
    "prints",
    "little",
    "prettier",
    "know",
    "great",
    "ten",
    "nine",
    "eight",
    "ten",
    "nine",
    "eight",
    "heck",
    "lot",
    "better",
    "10",
    "10",
    "10",
    "right",
    "least",
    "go",
    "columns",
    "absolutely",
    "identical",
    "total",
    "garbage",
    "decision",
    "tree",
    "one",
    "least",
    "meaningful",
    "differences",
    "even",
    "quite",
    "similar",
    "time",
    "series",
    "end",
    "time",
    "series",
    "different",
    "outputs",
    "okay",
    "gon",
    "na",
    "divide",
    "let",
    "fit",
    "notice",
    "still",
    "got",
    "rmse",
    "point",
    "1",
    "3",
    "5",
    "dude",
    "dude",
    "much",
    "better",
    "rmse",
    "front",
    "let",
    "scatterplot",
    "though",
    "test",
    "let",
    "look",
    "correlation",
    "test",
    "okay",
    "adjust",
    "test",
    "plot",
    "let",
    "remember",
    "first",
    "test",
    "plot",
    "looked",
    "like",
    "total",
    "garbage",
    "features",
    "mean",
    "clearly",
    "plot",
    "lot",
    "better",
    "right",
    "old",
    "plot",
    "garbage",
    "features",
    "almost",
    "like",
    "independent",
    "axes",
    "almost",
    "like",
    "relationship",
    "prediction",
    "true",
    "value",
    "true",
    "right",
    "think",
    "seeing",
    "case",
    "mse",
    "wo",
    "tell",
    "full",
    "story",
    "something",
    "need",
    "keep",
    "mind",
    "general",
    "time",
    "series",
    "models",
    "particular",
    "metric",
    "going",
    "get",
    "right",
    "time",
    "want",
    "take",
    "holistic",
    "view",
    "also",
    "depend",
    "applications",
    "right",
    "mse",
    "gives",
    "thing",
    "model",
    "almost",
    "relationship",
    "outcome",
    "versus",
    "one",
    "right",
    "go",
    "plotting",
    "would",
    "also",
    "point",
    "especially",
    "ignore",
    "two",
    "outlier",
    "points",
    "seem",
    "even",
    "relationship",
    "right",
    "pearson",
    "r",
    "versus",
    "spearman",
    "spearmint",
    "r",
    "get",
    "rid",
    "bit",
    "outliers",
    "correlation",
    "little",
    "bit",
    "less",
    "negative",
    "although",
    "still",
    "negative",
    "right",
    "model",
    "still",
    "needs",
    "work",
    "least",
    "bringing",
    "sense",
    "data",
    "versus",
    "inputs",
    "identical",
    "different",
    "outcomes",
    "way",
    "get",
    "started",
    "algorithm",
    "scatter",
    "plot",
    "training",
    "see",
    "even",
    "better",
    "right",
    "ignore",
    "outlier",
    "particular",
    "begins",
    "look",
    "like",
    "something",
    "relationship",
    "actually",
    "ignore",
    "outlier",
    "might",
    "even",
    "finally",
    "realm",
    "positive",
    "correlation",
    "okay",
    "super",
    "impressive",
    "results",
    "dealing",
    "small",
    "data",
    "set",
    "actually",
    "exercise",
    "show",
    "things",
    "right",
    "saw",
    "take",
    "one",
    "time",
    "series",
    "turn",
    "many",
    "sliding",
    "windows",
    "produce",
    "enough",
    "separate",
    "samples",
    "fit",
    "algorithm",
    "safe",
    "prediction",
    "hand",
    "think",
    "see",
    "easy",
    "generate",
    "wrong",
    "feature",
    "set",
    "even",
    "nonsense",
    "feature",
    "set",
    "reason",
    "careful",
    "using",
    "packages",
    "automated",
    "feature",
    "generation",
    "could",
    "features",
    "way",
    "meaningfully",
    "distinguishing",
    "say",
    "neighboring",
    "time",
    "series",
    "something",
    "want",
    "also",
    "want",
    "remember",
    "especially",
    "low",
    "data",
    "situations",
    "machine",
    "learning",
    "really",
    "way",
    "go",
    "sort",
    "forecasting",
    "involves",
    "low",
    "quantities",
    "data",
    "really",
    "machine",
    "learning",
    "built",
    "right",
    "versus",
    "something",
    "like",
    "area",
    "model",
    "part",
    "using",
    "machine",
    "learning",
    "time",
    "series",
    "knowing",
    "use",
    "judiciously",
    "enough",
    "data",
    "justify",
    "techniques",
    "really",
    "used",
    "much",
    "much",
    "larger",
    "data",
    "sets",
    "use",
    "larger",
    "data",
    "sets",
    "going",
    "see",
    "really",
    "excellent",
    "performance",
    "data",
    "set",
    "likely",
    "machine",
    "learning",
    "way",
    "go",
    "rather",
    "traditional",
    "statistical",
    "models",
    "mentioned",
    "well",
    "small",
    "data",
    "sets",
    "sort",
    "max",
    "estimation",
    "parameters",
    "get",
    "good",
    "money",
    "first",
    "many",
    "many",
    "parameters",
    "complex",
    "model",
    "data",
    "justify",
    "get",
    "really",
    "excellent",
    "results",
    "see",
    "example",
    "cago",
    "competitions",
    "industry",
    "research",
    "papers",
    "academic",
    "modeling",
    "competitions",
    "timeseriesforecasting",
    "xg",
    "boost",
    "performing",
    "well",
    "large",
    "volume",
    "data",
    "ok",
    "questions",
    "machine",
    "learning",
    "time",
    "series",
    "ok",
    "look",
    "computer",
    "take",
    "break",
    "take",
    "break",
    "ok",
    "great",
    "gon",
    "na",
    "take",
    "break",
    "let",
    "reconvene",
    "cover",
    "deep",
    "learning",
    "last",
    "component",
    "gon",
    "na",
    "cover",
    "two",
    "notebooks",
    "want",
    "briefly",
    "cover",
    "time",
    "series",
    "clustering",
    "going",
    "fly",
    "get",
    "deep",
    "learning",
    "example",
    "two",
    "deep",
    "learning",
    "examples",
    "notebooks",
    "one",
    "electricity",
    "forecasting",
    "one",
    "stock",
    "forecasting",
    "stock",
    "forecasting",
    "one",
    "sort",
    "supposed",
    "like",
    "know",
    "tutorial",
    "thing",
    "wanted",
    "tensor",
    "flow",
    "example",
    "well",
    "mixed",
    "net",
    "example",
    "like",
    "two",
    "libraries",
    "concepts",
    "going",
    "go",
    "interesting",
    "example",
    "stock",
    "prediction",
    "example",
    "second",
    "example",
    "noisier",
    "data",
    "different",
    "framework",
    "background",
    "let",
    "talk",
    "clustering",
    "let",
    "briefly",
    "remind",
    "dynamic",
    "time",
    "warping",
    "thing",
    "want",
    "cover",
    "ok",
    "used",
    "looked",
    "generate",
    "features",
    "time",
    "series",
    "go",
    "wrong",
    "go",
    "right",
    "might",
    "want",
    "sort",
    "blindly",
    "might",
    "want",
    "put",
    "thought",
    "saw",
    "feature",
    "generation",
    "really",
    "time",
    "consuming",
    "learned",
    "extract",
    "many",
    "samples",
    "machine",
    "learning",
    "perspective",
    "samples",
    "rather",
    "one",
    "time",
    "series",
    "accommodate",
    "nature",
    "much",
    "team",
    "learning",
    "works",
    "thinks",
    "terms",
    "samples",
    "rather",
    "terms",
    "time",
    "series",
    "briefly",
    "gon",
    "na",
    "look",
    "clustering",
    "mainly",
    "want",
    "show",
    "value",
    "dynamic",
    "time",
    "warping",
    "reminder",
    "one",
    "way",
    "map",
    "time",
    "series",
    "one",
    "another",
    "way",
    "looks",
    "sort",
    "shapes",
    "rather",
    "features",
    "gon",
    "na",
    "compare",
    "briefly",
    "see",
    "see",
    "work",
    "okay",
    "case",
    "looking",
    "actually",
    "tutorial",
    "gave",
    "years",
    "ago",
    "sort",
    "test",
    "subset",
    "word",
    "projections",
    "based",
    "paper",
    "think",
    "1990s",
    "early",
    "aughts",
    "lovely",
    "deep",
    "learning",
    "people",
    "thinking",
    "okay",
    "like",
    "scan",
    "historical",
    "documents",
    "way",
    "sort",
    "identify",
    "words",
    "within",
    "documents",
    "came",
    "let",
    "look",
    "documents",
    "look",
    "word",
    "handwriting",
    "right",
    "looking",
    "sort",
    "historical",
    "stuff",
    "let",
    "project",
    "onto",
    "1d",
    "axis",
    "right",
    "sort",
    "letters",
    "word",
    "bin",
    "count",
    "density",
    "convert",
    "1d",
    "axis",
    "actually",
    "ordered",
    "evenly",
    "spaced",
    "axis",
    "like",
    "temporal",
    "axis",
    "even",
    "though",
    "per",
    "se",
    "time",
    "basically",
    "boiled",
    "two",
    "hundred",
    "seven",
    "data",
    "points",
    "per",
    "word",
    "looked",
    "words",
    "looked",
    "like",
    "tell",
    "us",
    "word",
    "classification",
    "corresponds",
    "certain",
    "handwriting",
    "see",
    "sort",
    "distinctive",
    "shapes",
    "right",
    "different",
    "numbers",
    "peaks",
    "different",
    "locations",
    "peaks",
    "also",
    "wanted",
    "point",
    "sometimes",
    "different",
    "way",
    "summarizing",
    "data",
    "almost",
    "create",
    "new",
    "time",
    "series",
    "left",
    "sort",
    "time",
    "series",
    "projection",
    "word",
    "right",
    "took",
    "histogram",
    "values",
    "basically",
    "took",
    "summarised",
    "numerically",
    "even",
    "think",
    "histogram",
    "time",
    "series",
    "sense",
    "x",
    "axis",
    "evenly",
    "spaced",
    "right",
    "start",
    "realizing",
    "time",
    "series",
    "temporal",
    "axis",
    "way",
    "think",
    "shapes",
    "curves",
    "generally",
    "think",
    "sorts",
    "ways",
    "generate",
    "features",
    "one",
    "really",
    "nice",
    "sort",
    "visualize",
    "time",
    "series",
    "want",
    "see",
    "many",
    "examples",
    "sort",
    "class",
    "time",
    "series",
    "think",
    "things",
    "like",
    "producing",
    "2d",
    "histogram",
    "case",
    "looking",
    "word",
    "23",
    "let",
    "see",
    "believe",
    "word",
    "23",
    "yeah",
    "23",
    "one",
    "example",
    "right",
    "want",
    "make",
    "sure",
    "features",
    "see",
    "general",
    "something",
    "like",
    "2d",
    "histogram",
    "see",
    "oh",
    "yeah",
    "looks",
    "like",
    "sort",
    "two",
    "peaks",
    "although",
    "location",
    "set",
    "specifically",
    "couple",
    "peaks",
    "going",
    "actually",
    "good",
    "example",
    "part",
    "reason",
    "ca",
    "feed",
    "raw",
    "time",
    "series",
    "inputs",
    "features",
    "right",
    "things",
    "like",
    "peaks",
    "location",
    "jumps",
    "around",
    "whereas",
    "feed",
    "behavior",
    "wo",
    "really",
    "documented",
    "gon",
    "na",
    "generate",
    "features",
    "seen",
    "recently",
    "briefly",
    "used",
    "cesium",
    "sort",
    "time",
    "consuming",
    "would",
    "recommend",
    "reading",
    "csv",
    "could",
    "time",
    "possibly",
    "consider",
    "generating",
    "quite",
    "time",
    "consuming",
    "sort",
    "look",
    "histogram",
    "get",
    "sense",
    "features",
    "looks",
    "like",
    "least",
    "want",
    "make",
    "sure",
    "got",
    "sort",
    "spread",
    "gon",
    "na",
    "generate",
    "features",
    "histogram",
    "well",
    "treating",
    "sort",
    "directional",
    "time",
    "series",
    "also",
    "going",
    "find",
    "location",
    "max",
    "value",
    "sort",
    "learned",
    "lesson",
    "last",
    "example",
    "air",
    "passengers",
    "let",
    "put",
    "sort",
    "positional",
    "information",
    "found",
    "case",
    "max",
    "finally",
    "clustering",
    "want",
    "make",
    "sure",
    "features",
    "sort",
    "count",
    "equally",
    "right",
    "want",
    "make",
    "sure",
    "like",
    "amplitude",
    "swapped",
    "percent",
    "beyond",
    "one",
    "standard",
    "right",
    "quick",
    "way",
    "time",
    "serious",
    "specific",
    "general",
    "want",
    "features",
    "gon",
    "na",
    "cluster",
    "case",
    "using",
    "hierarchical",
    "clustering",
    "really",
    "good",
    "sense",
    "distribution",
    "looks",
    "like",
    "want",
    "use",
    "something",
    "like",
    "assuming",
    "necessarily",
    "want",
    "minimize",
    "variance",
    "sort",
    "well",
    "shaped",
    "spheres",
    "want",
    "anything",
    "like",
    "going",
    "see",
    "sort",
    "result",
    "clustering",
    "looks",
    "pretty",
    "messy",
    "like",
    "really",
    "finding",
    "good",
    "pattern",
    "one",
    "measure",
    "like",
    "homogeneity",
    "score",
    "right",
    "like",
    "homogeneous",
    "clusters",
    "made",
    "terms",
    "original",
    "class",
    "labels",
    "punchline",
    "good",
    "right",
    "clustering",
    "based",
    "features",
    "time",
    "series",
    "least",
    "features",
    "great",
    "could",
    "put",
    "work",
    "try",
    "find",
    "good",
    "features",
    "could",
    "remember",
    "oh",
    "wait",
    "dynamic",
    "time",
    "warping",
    "heard",
    "magical",
    "distance",
    "metric",
    "really",
    "fantastic",
    "let",
    "check",
    "right",
    "let",
    "first",
    "look",
    "toy",
    "example",
    "toy",
    "example",
    "prove",
    "euclidean",
    "distance",
    "terrible",
    "right",
    "two",
    "sine",
    "curves",
    "flat",
    "line",
    "think",
    "would",
    "agree",
    "would",
    "rather",
    "match",
    "two",
    "sine",
    "curves",
    "even",
    "sort",
    "different",
    "frequencies",
    "rather",
    "think",
    "similar",
    "rather",
    "thinking",
    "either",
    "sort",
    "similar",
    "flat",
    "line",
    "right",
    "going",
    "group",
    "say",
    "two",
    "groups",
    "probably",
    "put",
    "two",
    "sine",
    "curves",
    "together",
    "right",
    "fundamental",
    "process",
    "common",
    "two",
    "relative",
    "flat",
    "line",
    "exercise",
    "would",
    "recommend",
    "time",
    "considering",
    "calculating",
    "point",
    "calculate",
    "sine",
    "curves",
    "end",
    "shorter",
    "distance",
    "flat",
    "line",
    "know",
    "magic",
    "behind",
    "know",
    "sum",
    "squares",
    "take",
    "square",
    "root",
    "behavior",
    "going",
    "get",
    "super",
    "undesirable",
    "behavior",
    "two",
    "components",
    "concern",
    "firstly",
    "super",
    "computationally",
    "efficient",
    "way",
    "euclidean",
    "distance",
    "right",
    "still",
    "need",
    "go",
    "data",
    "points",
    "like",
    "really",
    "fast",
    "really",
    "crap",
    "right",
    "two",
    "reasons",
    "use",
    "almost",
    "always",
    "true",
    "would",
    "want",
    "use",
    "euclidean",
    "distance",
    "another",
    "measure",
    "recommended",
    "correlation",
    "measure",
    "right",
    "like",
    "sort",
    "similar",
    "correlated",
    "seems",
    "like",
    "maybe",
    "could",
    "better",
    "euclidean",
    "distance",
    "going",
    "thing",
    "case",
    "see",
    "gon",
    "na",
    "add",
    "little",
    "bit",
    "random",
    "noise",
    "time",
    "series",
    "3",
    "flatline",
    "undefined",
    "correlation",
    "right",
    "variants",
    "measure",
    "come",
    "back",
    "let",
    "thing",
    "let",
    "look",
    "correlation",
    "ha",
    "interesting",
    "sine",
    "curves",
    "sort",
    "negatively",
    "correlated",
    "versus",
    "positive",
    "correlation",
    "flatline",
    "use",
    "correlation",
    "super",
    "efficient",
    "metric",
    "anyway",
    "via",
    "especially",
    "good",
    "job",
    "relative",
    "desired",
    "behavior",
    "would",
    "recommend",
    "dynamic",
    "time",
    "warping",
    "one",
    "simple",
    "implementation",
    "definitely",
    "code",
    "would",
    "definitely",
    "recommend",
    "ever",
    "going",
    "work",
    "dynamic",
    "time",
    "warping",
    "feel",
    "comfortable",
    "definition",
    "meat",
    "right",
    "using",
    "dynamic",
    "programming",
    "aspect",
    "matrix",
    "basically",
    "trying",
    "set",
    "points",
    "right",
    "going",
    "eius",
    "point",
    "one",
    "time",
    "series",
    "j",
    "thought",
    "another",
    "need",
    "unified",
    "want",
    "figure",
    "way",
    "align",
    "results",
    "shortest",
    "difference",
    "values",
    "right",
    "best",
    "aligns",
    "difference",
    "iterating",
    "start",
    "beginning",
    "time",
    "series",
    "1",
    "beginning",
    "time",
    "series",
    "2",
    "calculate",
    "distance",
    "example",
    "one",
    "thing",
    "play",
    "want",
    "even",
    "dynamic",
    "time",
    "warping",
    "many",
    "definitions",
    "right",
    "gon",
    "na",
    "use",
    "square",
    "difference",
    "distance",
    "want",
    "think",
    "okay",
    "min",
    "preceding",
    "distance",
    "would",
    "either",
    "distance",
    "minus",
    "1",
    "j",
    "distance",
    "j",
    "minus",
    "1",
    "difference",
    "minus",
    "1",
    "j",
    "minus",
    "1",
    "remember",
    "eyes",
    "j",
    "moving",
    "along",
    "temporal",
    "axis",
    "remember",
    "disconnected",
    "2",
    "axis",
    "curves",
    "sort",
    "move",
    "along",
    "one",
    "curve",
    "move",
    "along",
    "one",
    "step",
    "way",
    "defined",
    "sort",
    "trying",
    "move",
    "along",
    "axis",
    "way",
    "path",
    "square",
    "distance",
    "minimum",
    "sort",
    "iterative",
    "way",
    "working",
    "data",
    "move",
    "one",
    "sort",
    "warp",
    "two",
    "curves",
    "way",
    "makes",
    "fit",
    "best",
    "ultimately",
    "returned",
    "minimum",
    "distance",
    "take",
    "square",
    "root",
    "thing",
    "defining",
    "time",
    "dynamic",
    "time",
    "warming",
    "distance",
    "sure",
    "hope",
    "fixes",
    "problem",
    "wasted",
    "lot",
    "time",
    "ts",
    "2",
    "ts",
    "1",
    "2",
    "ts",
    "3",
    "also",
    "3",
    "point",
    "7",
    "2",
    "2",
    "ts",
    "3",
    "4",
    "point",
    "1",
    "6",
    "ah",
    "notice",
    "still",
    "perfect",
    "right",
    "still",
    "problem",
    "flat",
    "line",
    "different",
    "sine",
    "curve",
    "would",
    "like",
    "lot",
    "better",
    "least",
    "telling",
    "two",
    "sine",
    "curves",
    "way",
    "different",
    "compared",
    "flat",
    "line",
    "firstly",
    "dynamic",
    "warm",
    "dynamic",
    "time",
    "warping",
    "lot",
    "parameters",
    "could",
    "tweak",
    "one",
    "thing",
    "might",
    "want",
    "particular",
    "dataset",
    "explore",
    "sort",
    "different",
    "ways",
    "stepping",
    "back",
    "forth",
    "right",
    "sort",
    "time",
    "steps",
    "allow",
    "different",
    "distance",
    "metrics",
    "see",
    "maybe",
    "could",
    "improve",
    "distinction",
    "depending",
    "underlying",
    "data",
    "thing",
    "might",
    "want",
    "accept",
    "point",
    "sense",
    "extremely",
    "difficult",
    "define",
    "differences",
    "time",
    "series",
    "good",
    "enough",
    "might",
    "even",
    "begin",
    "look",
    "deep",
    "learning",
    "point",
    "say",
    "okay",
    "image",
    "analysis",
    "problem",
    "instead",
    "lets",
    "know",
    "ratchet",
    "notch",
    "something",
    "keep",
    "mind",
    "extent",
    "want",
    "keep",
    "clustering",
    "problem",
    "want",
    "also",
    "wanted",
    "point",
    "libraries",
    "code",
    "wo",
    "find",
    "dynamic",
    "time",
    "warping",
    "sort",
    "major",
    "libraries",
    "like",
    "sk",
    "learn",
    "unfortunately",
    "spelled",
    "sorts",
    "diy",
    "github",
    "one",
    "see",
    "actually",
    "definition",
    "much",
    "sophisticated",
    "implementation",
    "knobs",
    "turn",
    "things",
    "available",
    "bit",
    "work",
    "compute",
    "pairwise",
    "distance",
    "really",
    "computationally",
    "taxing",
    "would",
    "awhile",
    "sitting",
    "around",
    "going",
    "going",
    "run",
    "clustering",
    "look",
    "clustering",
    "perform",
    "fantastic",
    "homogeneity",
    "completeness",
    "scores",
    "right",
    "extent",
    "one",
    "class",
    "covered",
    "one",
    "cluster",
    "really",
    "good",
    "extent",
    "one",
    "cluster",
    "composed",
    "one",
    "class",
    "also",
    "really",
    "good",
    "really",
    "nice",
    "mapping",
    "versus",
    "remember",
    "homogeneity",
    "score",
    "features",
    "like",
    "50",
    "distance",
    "metric",
    "make",
    "tremendous",
    "difference",
    "time",
    "series",
    "clustering",
    "similarly",
    "also",
    "use",
    "sorts",
    "things",
    "forecasting",
    "classification",
    "right",
    "classification",
    "think",
    "obvious",
    "would",
    "sort",
    "map",
    "something",
    "cluster",
    "clusters",
    "quite",
    "homogeneous",
    "already",
    "useful",
    "also",
    "use",
    "forecasting",
    "sense",
    "partial",
    "curve",
    "say",
    "curve",
    "time",
    "series",
    "closest",
    "use",
    "time",
    "series",
    "outcome",
    "predict",
    "outcome",
    "new",
    "feature",
    "something",
    "see",
    "finance",
    "see",
    "health",
    "well",
    "sort",
    "saying",
    "example",
    "patient",
    "time",
    "similar",
    "maybe",
    "know",
    "likely",
    "outcome",
    "patient",
    "really",
    "interesting",
    "use",
    "quite",
    "heterogeneous",
    "data",
    "use",
    "really",
    "messy",
    "data",
    "get",
    "pretty",
    "good",
    "result",
    "compared",
    "trying",
    "come",
    "feature",
    "set",
    "quite",
    "challenging",
    "alternative",
    "way",
    "forecasting",
    "classification",
    "questions",
    "okay",
    "great",
    "last",
    "component",
    "deep",
    "learning",
    "time",
    "series",
    "couple",
    "slides",
    "cover",
    "okay",
    "hands",
    "worked",
    "deep",
    "learning",
    "framework",
    "kind",
    "least",
    "toy",
    "example",
    "okay",
    "gon",
    "na",
    "give",
    "like",
    "brief",
    "rundown",
    "way",
    "good",
    "substitute",
    "reading",
    "learning",
    "packages",
    "want",
    "give",
    "sense",
    "would",
    "look",
    "time",
    "series",
    "context",
    "start",
    "simplest",
    "neural",
    "network",
    "examples",
    "would",
    "something",
    "like",
    "fully",
    "connected",
    "model",
    "neural",
    "networks",
    "really",
    "form",
    "machine",
    "learning",
    "sense",
    "expect",
    "series",
    "inputs",
    "give",
    "series",
    "outputs",
    "necessarily",
    "time",
    "aware",
    "right",
    "looking",
    "give",
    "time",
    "series",
    "fit",
    "model",
    "time",
    "series",
    "looking",
    "samples",
    "right",
    "x",
    "outputs",
    "beautiful",
    "thing",
    "like",
    "machine",
    "learning",
    "deep",
    "learning",
    "like",
    "fairly",
    "agnostic",
    "really",
    "care",
    "inputs",
    "give",
    "right",
    "extent",
    "built",
    "throw",
    "anything",
    "want",
    "may",
    "well",
    "get",
    "nice",
    "answer",
    "sort",
    "tweak",
    "parameters",
    "training",
    "know",
    "aspect",
    "bit",
    "art",
    "look",
    "like",
    "simplest",
    "model",
    "input",
    "layer",
    "would",
    "example",
    "could",
    "features",
    "looks",
    "lot",
    "like",
    "decision",
    "tree",
    "right",
    "would",
    "create",
    "features",
    "would",
    "input",
    "difference",
    "say",
    "decision",
    "tree",
    "neural",
    "network",
    "neural",
    "network",
    "look",
    "like",
    "example",
    "using",
    "fully",
    "connected",
    "model",
    "every",
    "input",
    "would",
    "go",
    "kind",
    "multiplication",
    "weight",
    "possibly",
    "sort",
    "activation",
    "function",
    "activation",
    "function",
    "essentially",
    "way",
    "introducing",
    "would",
    "essentially",
    "like",
    "matrix",
    "multiplication",
    "coupled",
    "produce",
    "next",
    "set",
    "inputs",
    "layer",
    "layer",
    "would",
    "exact",
    "thing",
    "right",
    "sort",
    "matrix",
    "multiplication",
    "followed",
    "nonlinear",
    "activation",
    "output",
    "end",
    "might",
    "bunch",
    "new",
    "features",
    "come",
    "model",
    "find",
    "way",
    "combining",
    "produce",
    "output",
    "like",
    "know",
    "view",
    "really",
    "encountered",
    "obviously",
    "lot",
    "goes",
    "figuring",
    "properly",
    "initialize",
    "quickly",
    "get",
    "millions",
    "parameters",
    "gon",
    "na",
    "leave",
    "aside",
    "great",
    "tutorials",
    "outside",
    "syfy",
    "week",
    "topic",
    "learn",
    "main",
    "point",
    "fully",
    "connected",
    "model",
    "time",
    "series",
    "perspective",
    "like",
    "another",
    "machine",
    "learning",
    "model",
    "still",
    "require",
    "future",
    "generation",
    "lot",
    "thought",
    "sort",
    "translate",
    "time",
    "series",
    "something",
    "kind",
    "model",
    "digest",
    "may",
    "well",
    "better",
    "say",
    "decision",
    "tree",
    "right",
    "sort",
    "empirical",
    "question",
    "particular",
    "kind",
    "data",
    "way",
    "time",
    "aware",
    "like",
    "machine",
    "learning",
    "techniques",
    "way",
    "time",
    "aware",
    "sort",
    "cut",
    "pre",
    "process",
    "time",
    "format",
    "since",
    "options",
    "though",
    "classic",
    "example",
    "would",
    "put",
    "time",
    "series",
    "neural",
    "network",
    "called",
    "recurrent",
    "neural",
    "network",
    "annual",
    "mostly",
    "cr",
    "nn",
    "idea",
    "recurrent",
    "recognizes",
    "data",
    "come",
    "data",
    "recurs",
    "build",
    "one",
    "cell",
    "go",
    "one",
    "cell",
    "basically",
    "unroll",
    "reapply",
    "stage",
    "new",
    "data",
    "provides",
    "unifying",
    "slash",
    "temporally",
    "aware",
    "way",
    "looking",
    "data",
    "model",
    "understands",
    "going",
    "rolled",
    "used",
    "data",
    "look",
    "like",
    "univariate",
    "multivariate",
    "case",
    "either",
    "example",
    "matter",
    "existing",
    "neural",
    "network",
    "model",
    "first",
    "put",
    "first",
    "value",
    "time",
    "series",
    "right",
    "produces",
    "hidden",
    "state",
    "need",
    "worry",
    "part",
    "sort",
    "internal",
    "accounting",
    "need",
    "hidden",
    "state",
    "well",
    "makes",
    "temporarily",
    "aware",
    "gives",
    "ability",
    "sort",
    "remember",
    "things",
    "state",
    "state",
    "right",
    "rather",
    "example",
    "get",
    "something",
    "chugs",
    "one",
    "response",
    "time",
    "model",
    "temporal",
    "axis",
    "right",
    "produces",
    "hidden",
    "state",
    "put",
    "next",
    "value",
    "remembers",
    "hidden",
    "state",
    "affects",
    "goes",
    "comes",
    "new",
    "hidden",
    "state",
    "model",
    "form",
    "memory",
    "way",
    "evolving",
    "time",
    "recognizing",
    "data",
    "wants",
    "something",
    "recognizes",
    "dynamics",
    "evolving",
    "time",
    "recurrent",
    "neural",
    "networks",
    "like",
    "everything",
    "else",
    "talked",
    "today",
    "new",
    "idea",
    "know",
    "exact",
    "time",
    "believe",
    "early",
    "1980s",
    "envisioned",
    "even",
    "things",
    "like",
    "probably",
    "know",
    "term",
    "grew",
    "lst",
    "understanding",
    "even",
    "lst",
    "envisioned",
    "80s",
    "although",
    "maybe",
    "wrong",
    "90s",
    "certainly",
    "long",
    "time",
    "ago",
    "even",
    "though",
    "still",
    "considered",
    "relatively",
    "exciting",
    "new",
    "technologies",
    "really",
    "exciting",
    "developed",
    "much",
    "art",
    "fit",
    "know",
    "initialize",
    "things",
    "backpropagation",
    "looks",
    "like",
    "course",
    "much",
    "exciting",
    "gotten",
    "fancy",
    "gpus",
    "things",
    "actually",
    "chug",
    "right",
    "ability",
    "imagine",
    "something",
    "60s",
    "right",
    "ability",
    "actually",
    "fairly",
    "new",
    "also",
    "presence",
    "enough",
    "data",
    "get",
    "good",
    "results",
    "fairly",
    "new",
    "two",
    "variants",
    "recurrent",
    "neural",
    "networks",
    "turned",
    "quite",
    "successful",
    "one",
    "called",
    "gru",
    "see",
    "example",
    "looking",
    "actually",
    "unpack",
    "guts",
    "thing",
    "unpack",
    "guts",
    "got",
    "got",
    "xt",
    "comes",
    "track",
    "hidden",
    "states",
    "goes",
    "several",
    "different",
    "variants",
    "transformation",
    "people",
    "sort",
    "liked",
    "update",
    "gate",
    "forget",
    "gate",
    "idea",
    "different",
    "components",
    "one",
    "sort",
    "supposed",
    "help",
    "model",
    "decide",
    "much",
    "sort",
    "update",
    "hidden",
    "state",
    "based",
    "new",
    "information",
    "another",
    "sort",
    "looks",
    "new",
    "information",
    "comes",
    "coming",
    "sort",
    "transforms",
    "even",
    "sort",
    "talks",
    "hidden",
    "state",
    "parameters",
    "sort",
    "specialize",
    "describe",
    "different",
    "different",
    "properties",
    "temporal",
    "dynamics",
    "data",
    "depending",
    "allow",
    "updating",
    "interesting",
    "thing",
    "way",
    "sounds",
    "bit",
    "like",
    "talked",
    "structural",
    "time",
    "series",
    "models",
    "generally",
    "right",
    "developing",
    "sort",
    "internal",
    "model",
    "like",
    "common",
    "filter",
    "adapt",
    "new",
    "information",
    "given",
    "prior",
    "expectations",
    "although",
    "case",
    "prior",
    "expectations",
    "hidden",
    "state",
    "sort",
    "well",
    "described",
    "statistical",
    "thing",
    "like",
    "set",
    "parameters",
    "matrix",
    "actually",
    "theory",
    "well",
    "developed",
    "sure",
    "know",
    "decade",
    "eventually",
    "figure",
    "know",
    "still",
    "models",
    "still",
    "statistical",
    "properties",
    "much",
    "harder",
    "think",
    "grue",
    "groovers",
    "ls",
    "tm",
    "probably",
    "heard",
    "two",
    "l",
    "stm",
    "tends",
    "little",
    "bit",
    "complicated",
    "one",
    "gate",
    "compared",
    "grew",
    "many",
    "cases",
    "perform",
    "better",
    "sort",
    "worth",
    "looking",
    "depending",
    "dataset",
    "ls",
    "tm",
    "people",
    "say",
    "sort",
    "longer",
    "memory",
    "compared",
    "grew",
    "depends",
    "much",
    "kind",
    "data",
    "looking",
    "okay",
    "convolutional",
    "neural",
    "networks",
    "might",
    "surprised",
    "see",
    "thinking",
    "oh",
    "heard",
    "orang",
    "ins",
    "actually",
    "rnns",
    "time",
    "aware",
    "actually",
    "cnn",
    "also",
    "time",
    "aware",
    "sense",
    "image",
    "much",
    "like",
    "seeing",
    "things",
    "done",
    "today",
    "right",
    "time",
    "series",
    "classification",
    "time",
    "series",
    "clustering",
    "look",
    "different",
    "image",
    "analysis",
    "right",
    "also",
    "use",
    "convolutional",
    "neural",
    "networks",
    "way",
    "understanding",
    "time",
    "series",
    "picture",
    "already",
    "good",
    "way",
    "classify",
    "time",
    "series",
    "also",
    "modifications",
    "convolutional",
    "neural",
    "networks",
    "make",
    "temporarily",
    "aware",
    "one",
    "example",
    "called",
    "causal",
    "convolution",
    "standard",
    "convolution",
    "image",
    "analysis",
    "sort",
    "treats",
    "directions",
    "areas",
    "equally",
    "spaced",
    "easy",
    "modification",
    "causal",
    "convolution",
    "convolution",
    "sort",
    "goes",
    "one",
    "direction",
    "temporarily",
    "case",
    "convolution",
    "also",
    "time",
    "aware",
    "used",
    "input",
    "components",
    "model",
    "model",
    "performs",
    "better",
    "much",
    "depend",
    "data",
    "kind",
    "patterns",
    "data",
    "kind",
    "tasks",
    "trying",
    "trying",
    "predict",
    "trying",
    "classify",
    "cnn",
    "tend",
    "successful",
    "time",
    "series",
    "classification",
    "rather",
    "prediction",
    "course",
    "broad",
    "generalization",
    "field",
    "still",
    "sort",
    "working",
    "science",
    "things",
    "work",
    "one",
    "actually",
    "really",
    "cool",
    "architecture",
    "wanted",
    "highlight",
    "published",
    "two",
    "years",
    "ago",
    "think",
    "ellis",
    "net",
    "researchers",
    "actually",
    "said",
    "well",
    "pick",
    "choose",
    "convolutional",
    "recurrent",
    "behaviors",
    "actually",
    "reason",
    "use",
    "way",
    "makes",
    "sense",
    "right",
    "think",
    "something",
    "like",
    "airline",
    "passengers",
    "data",
    "sort",
    "bold",
    "aspects",
    "right",
    "sort",
    "aspect",
    "see",
    "trend",
    "difference",
    "sort",
    "like",
    "rnn",
    "ish",
    "right",
    "see",
    "trend",
    "sure",
    "convolutional",
    "understands",
    "hand",
    "seasonal",
    "component",
    "right",
    "sort",
    "winter",
    "fall",
    "summer",
    "etc",
    "etc",
    "part",
    "seems",
    "bit",
    "like",
    "image",
    "analysis",
    "type",
    "component",
    "right",
    "idea",
    "also",
    "said",
    "use",
    "convolutional",
    "bit",
    "understand",
    "better",
    "use",
    "multivariate",
    "time",
    "series",
    "right",
    "extent",
    "want",
    "relate",
    "different",
    "inputs",
    "right",
    "different",
    "inputs",
    "model",
    "univariate",
    "want",
    "sort",
    "convolve",
    "advantages",
    "one",
    "maybe",
    "discover",
    "relationships",
    "especially",
    "convolve",
    "time",
    "input",
    "right",
    "sort",
    "take",
    "almost",
    "like",
    "sliding",
    "window",
    "image",
    "time",
    "series",
    "also",
    "think",
    "maybe",
    "convolution",
    "might",
    "identify",
    "seasonality",
    "right",
    "sort",
    "image",
    "like",
    "also",
    "said",
    "well",
    "always",
    "sort",
    "recurrent",
    "sort",
    "feed",
    "one",
    "time",
    "might",
    "also",
    "want",
    "design",
    "architecture",
    "better",
    "reflect",
    "certain",
    "realities",
    "might",
    "want",
    "something",
    "reflects",
    "reality",
    "seasonality",
    "might",
    "also",
    "want",
    "something",
    "skip",
    "right",
    "maybe",
    "data",
    "might",
    "want",
    "rnn",
    "looks",
    "hours",
    "might",
    "want",
    "rnn",
    "addition",
    "looking",
    "hours",
    "looks",
    "local",
    "hour",
    "previous",
    "day",
    "right",
    "3",
    "trying",
    "predict",
    "electricity",
    "4",
    "maybe",
    "want",
    "look",
    "1",
    "2",
    "3",
    "4",
    "know",
    "0",
    "whatever",
    "3",
    "know",
    "sort",
    "slice",
    "might",
    "want",
    "look",
    "3",
    "previous",
    "day",
    "previous",
    "day",
    "previous",
    "day",
    "think",
    "time",
    "series",
    "recurrent",
    "neural",
    "network",
    "runs",
    "maybe",
    "look",
    "states",
    "recurrent",
    "neural",
    "network",
    "points",
    "right",
    "worked",
    "neural",
    "networks",
    "know",
    "actually",
    "sort",
    "infinite",
    "architectural",
    "permutations",
    "make",
    "anything",
    "imagine",
    "build",
    "fairly",
    "straightforward",
    "way",
    "although",
    "figuring",
    "train",
    "properly",
    "course",
    "always",
    "run",
    "trouble",
    "figuring",
    "actually",
    "enough",
    "data",
    "train",
    "wonderful",
    "thing",
    "compared",
    "models",
    "looked",
    "today",
    "imagine",
    "sort",
    "dynamics",
    "describe",
    "kind",
    "neural",
    "network",
    "see",
    "improve",
    "forecasting",
    "classification",
    "ah",
    "prediction",
    "let",
    "see",
    "hopefully",
    "come",
    "back",
    "ok",
    "meantime",
    "would",
    "ask",
    "everyone",
    "open",
    "notebooks",
    "gon",
    "na",
    "look",
    "notebook",
    "5",
    "supposed",
    "come",
    "back",
    "electricity",
    "okay",
    "going",
    "see",
    "oh",
    "excellent",
    "thank",
    "goodness",
    "okay",
    "remind",
    "move",
    "next",
    "13",
    "minutes",
    "okay",
    "gon",
    "na",
    "try",
    "forecast",
    "electricity",
    "one",
    "original",
    "data",
    "sets",
    "used",
    "lst",
    "night",
    "model",
    "published",
    "oztent",
    "net",
    "model",
    "published",
    "saw",
    "pretty",
    "big",
    "gains",
    "compared",
    "say",
    "something",
    "like",
    "using",
    "grue",
    "using",
    "convolutional",
    "neural",
    "network",
    "made",
    "point",
    "know",
    "building",
    "architecture",
    "reflects",
    "especially",
    "three",
    "temporal",
    "cadence",
    "human",
    "activity",
    "works",
    "really",
    "well",
    "also",
    "making",
    "point",
    "multivariate",
    "time",
    "series",
    "interesting",
    "find",
    "ways",
    "combine",
    "information",
    "channels",
    "pointed",
    "one",
    "model",
    "one",
    "data",
    "sets",
    "use",
    "electric",
    "data",
    "set",
    "going",
    "use",
    "first",
    "thing",
    "gon",
    "na",
    "look",
    "data",
    "let",
    "look",
    "together",
    "read",
    "look",
    "head",
    "see",
    "like",
    "three",
    "hundred",
    "twenty",
    "one",
    "parallel",
    "time",
    "series",
    "electric",
    "use",
    "hourly",
    "electric",
    "use",
    "321",
    "different",
    "sites",
    "want",
    "say",
    "state",
    "alabama",
    "remember",
    "check",
    "reference",
    "data",
    "mention",
    "hourly",
    "times",
    "hourly",
    "electric",
    "use",
    "state",
    "us",
    "plot",
    "always",
    "know",
    "sort",
    "better",
    "worse",
    "ways",
    "plot",
    "look",
    "really",
    "interesting",
    "right",
    "one",
    "site",
    "full",
    "range",
    "time",
    "interesting",
    "sort",
    "drastically",
    "different",
    "regimes",
    "say",
    "traditional",
    "statistical",
    "model",
    "even",
    "machine",
    "learning",
    "model",
    "would",
    "really",
    "think",
    "oh",
    "goodness",
    "like",
    "gon",
    "na",
    "make",
    "different",
    "points",
    "time",
    "comparable",
    "right",
    "mean",
    "looks",
    "really",
    "different",
    "would",
    "would",
    "take",
    "lot",
    "thought",
    "neural",
    "network",
    "sometimes",
    "assumptions",
    "built",
    "also",
    "expect",
    "flexibly",
    "handle",
    "really",
    "varying",
    "data",
    "maybe",
    "even",
    "notice",
    "things",
    "would",
    "notice",
    "plot",
    "gives",
    "us",
    "one",
    "view",
    "tells",
    "us",
    "gone",
    "sort",
    "different",
    "regimes",
    "electric",
    "use",
    "one",
    "site",
    "one",
    "321",
    "inputs",
    "unless",
    "job",
    "years",
    "sure",
    "able",
    "look",
    "three",
    "know",
    "although",
    "great",
    "write",
    "also",
    "let",
    "remember",
    "look",
    "smaller",
    "time",
    "scale",
    "spot",
    "data",
    "look",
    "smaller",
    "time",
    "scale",
    "spot",
    "kind",
    "recurring",
    "pattern",
    "right",
    "makes",
    "sense",
    "early",
    "electric",
    "use",
    "might",
    "thinking",
    "oh",
    "well",
    "would",
    "expected",
    "see",
    "better",
    "use",
    "pre",
    "difference",
    "data",
    "actually",
    "original",
    "paper",
    "fit",
    "original",
    "time",
    "series",
    "looks",
    "like",
    "air",
    "passengers",
    "sort",
    "account",
    "easy",
    "b",
    "tends",
    "represent",
    "well",
    "like",
    "oh",
    "man",
    "look",
    "got",
    "99",
    "correlation",
    "match",
    "well",
    "fact",
    "could",
    "done",
    "null",
    "model",
    "difference",
    "really",
    "strict",
    "like",
    "model",
    "add",
    "something",
    "useful",
    "predict",
    "hour",
    "hour",
    "rather",
    "sort",
    "globally",
    "getting",
    "right",
    "encourage",
    "whenever",
    "use",
    "difference",
    "data",
    "okay",
    "handy",
    "data",
    "structure",
    "nothing",
    "time",
    "series",
    "like",
    "use",
    "something",
    "like",
    "ring",
    "buffer",
    "sort",
    "thinking",
    "way",
    "early",
    "stopping",
    "another",
    "thing",
    "new",
    "neural",
    "networks",
    "sort",
    "predefined",
    "limit",
    "long",
    "train",
    "right",
    "sort",
    "want",
    "train",
    "long",
    "getting",
    "better",
    "want",
    "make",
    "sure",
    "fit",
    "similar",
    "techniques",
    "discussed",
    "today",
    "difference",
    "could",
    "potentially",
    "way",
    "parameters",
    "even",
    "complicated",
    "model",
    "looked",
    "right",
    "talked",
    "structural",
    "time",
    "series",
    "many",
    "knobs",
    "turn",
    "looks",
    "like",
    "could",
    "really",
    "game",
    "wanted",
    "like",
    "orders",
    "magnitude",
    "different",
    "really",
    "lot",
    "knobs",
    "turn",
    "need",
    "make",
    "sure",
    "disciplined",
    "fitting",
    "one",
    "way",
    "early",
    "stopping",
    "ok",
    "data",
    "preparation",
    "want",
    "call",
    "attention",
    "part",
    "actually",
    "looks",
    "little",
    "bit",
    "like",
    "done",
    "right",
    "take",
    "one",
    "long",
    "time",
    "series",
    "reshape",
    "many",
    "slices",
    "time",
    "series",
    "case",
    "multi",
    "channels",
    "slicing",
    "along",
    "one",
    "axis",
    "actually",
    "slice",
    "multivariate",
    "time",
    "series",
    "make",
    "sure",
    "sort",
    "sliding",
    "windows",
    "keep",
    "sync",
    "data",
    "would",
    "encourage",
    "look",
    "well",
    "see",
    "works",
    "also",
    "build",
    "data",
    "iterators",
    "prepare",
    "batches",
    "data",
    "okay",
    "interesting",
    "part",
    "provide",
    "examples",
    "could",
    "use",
    "fully",
    "connected",
    "model",
    "cnn",
    "model",
    "would",
    "encourage",
    "try",
    "find",
    "tend",
    "well",
    "rnn",
    "model",
    "simple",
    "lst",
    "net",
    "model",
    "introduced",
    "briefly",
    "rnn",
    "model",
    "build",
    "rnn",
    "model",
    "mix",
    "nut",
    "well",
    "need",
    "cell",
    "need",
    "add",
    "cell",
    "decide",
    "many",
    "hidden",
    "units",
    "basically",
    "saying",
    "large",
    "like",
    "hidden",
    "state",
    "matrix",
    "want",
    "right",
    "like",
    "many",
    "different",
    "parameters",
    "want",
    "describe",
    "behavior",
    "update",
    "retain",
    "information",
    "rnn",
    "model",
    "essentially",
    "one",
    "layer",
    "rnn",
    "rollout",
    "already",
    "let",
    "look",
    "lst",
    "net",
    "model",
    "really",
    "interesting",
    "corresponds",
    "architecture",
    "showed",
    "first",
    "thing",
    "gon",
    "na",
    "case",
    "going",
    "apply",
    "convolutional",
    "filter",
    "inputs",
    "going",
    "take",
    "inputs",
    "right",
    "actually",
    "case",
    "time",
    "batch",
    "number",
    "tnc",
    "time",
    "batch",
    "number",
    "channel",
    "channel",
    "like",
    "320",
    "right",
    "first",
    "thing",
    "gon",
    "na",
    "run",
    "filter",
    "basically",
    "like",
    "looking",
    "sliding",
    "window",
    "image",
    "window",
    "slides",
    "time",
    "also",
    "features",
    "take",
    "convolutional",
    "output",
    "becomes",
    "input",
    "neural",
    "network",
    "rnn",
    "recurrent",
    "neural",
    "network",
    "case",
    "said",
    "instead",
    "feeding",
    "raw",
    "values",
    "could",
    "also",
    "consider",
    "processing",
    "neural",
    "components",
    "almost",
    "way",
    "saying",
    "convolutional",
    "component",
    "going",
    "produce",
    "features",
    "raw",
    "features",
    "going",
    "boil",
    "fewer",
    "features",
    "features",
    "going",
    "go",
    "recurrent",
    "bit",
    "sort",
    "divvying",
    "task",
    "going",
    "future",
    "generation",
    "portion",
    "network",
    "going",
    "future",
    "generation",
    "feed",
    "recurrent",
    "neural",
    "network",
    "think",
    "makes",
    "sense",
    "right",
    "putting",
    "320",
    "parallel",
    "measurements",
    "recurrent",
    "neural",
    "network",
    "decide",
    "measurements",
    "related",
    "interesting",
    "b",
    "keep",
    "track",
    "temporal",
    "component",
    "sort",
    "shrink",
    "multivariate",
    "aspect",
    "rn",
    "concentrate",
    "temporal",
    "component",
    "really",
    "helpful",
    "two",
    "rn",
    "n",
    "2",
    "rn",
    "n",
    "models",
    "right",
    "one",
    "sort",
    "looks",
    "every",
    "data",
    "point",
    "one",
    "sort",
    "skips",
    "seasonality",
    "left",
    "regular",
    "rn",
    "n",
    "reason",
    "found",
    "particular",
    "data",
    "set",
    "especially",
    "important",
    "something",
    "else",
    "want",
    "keep",
    "mind",
    "architecture",
    "actually",
    "improves",
    "process",
    "particular",
    "model",
    "finally",
    "mention",
    "really",
    "interesting",
    "authors",
    "included",
    "autoregressive",
    "element",
    "element",
    "uses",
    "passed",
    "values",
    "predict",
    "future",
    "values",
    "right",
    "arima",
    "model",
    "actually",
    "seeing",
    "also",
    "combining",
    "statistical",
    "approach",
    "neural",
    "networks",
    "something",
    "increasingly",
    "common",
    "actually",
    "proven",
    "quite",
    "successful",
    "time",
    "series",
    "particular",
    "much",
    "areas",
    "machine",
    "learning",
    "models",
    "define",
    "training",
    "run",
    "let",
    "see",
    "running",
    "screen",
    "well",
    "screen",
    "frozen",
    "must",
    "running",
    "happens",
    "laptop",
    "obviously",
    "usually",
    "really",
    "want",
    "deep",
    "learning",
    "laptop",
    "usually",
    "want",
    "small",
    "data",
    "set",
    "keeping",
    "mind",
    "let",
    "see",
    "okay",
    "well",
    "would",
    "encourage",
    "check",
    "output",
    "experiment",
    "punchline",
    "wish",
    "time",
    "train",
    "lsd",
    "nap",
    "model",
    "quite",
    "bit",
    "better",
    "recurrent",
    "neural",
    "network",
    "alone",
    "really",
    "important",
    "design",
    "structures",
    "like",
    "savvy",
    "allocate",
    "labor",
    "case",
    "labor",
    "convolutional",
    "bit",
    "future",
    "generation",
    "separated",
    "temporal",
    "analysis",
    "one",
    "thing",
    "makes",
    "model",
    "really",
    "successful",
    "thing",
    "makes",
    "model",
    "really",
    "successful",
    "inclusion",
    "ar",
    "component",
    "autoroute",
    "recive",
    "component",
    "fact",
    "remove",
    "component",
    "take",
    "enormous",
    "hit",
    "performance",
    "think",
    "great",
    "inspiration",
    "take",
    "traditional",
    "statistical",
    "knowledge",
    "work",
    "neural",
    "networks",
    "find",
    "gets",
    "really",
    "great",
    "outcome",
    "fact",
    "recently",
    "academic",
    "research",
    "competition",
    "machine",
    "timeseriesforecasting",
    "number",
    "one",
    "number",
    "two",
    "winners",
    "competition",
    "took",
    "place",
    "hundred",
    "thousand",
    "different",
    "time",
    "series",
    "many",
    "domains",
    "turned",
    "integrating",
    "machine",
    "learning",
    "statistical",
    "statistical",
    "analyses",
    "quite",
    "traditional",
    "one",
    "case",
    "combining",
    "statistical",
    "analyses",
    "deep",
    "learning",
    "another",
    "case",
    "using",
    "x",
    "g",
    "boost",
    "choose",
    "coefficients",
    "combine",
    "statistical",
    "models",
    "also",
    "ways",
    "sort",
    "permute",
    "things",
    "quite",
    "varied",
    "gosh",
    "could",
    "sworn",
    "ran",
    "faster",
    "oh",
    "go",
    "go",
    "okay",
    "case",
    "see",
    "printing",
    "correlation",
    "metric",
    "like",
    "matter",
    "personal",
    "preference",
    "also",
    "matter",
    "meaningful",
    "problem",
    "basically",
    "want",
    "see",
    "improving",
    "especially",
    "validation",
    "think",
    "stop",
    "yes",
    "okay",
    "stopped",
    "also",
    "results",
    "last",
    "time",
    "ran",
    "already",
    "loaded",
    "lying",
    "suckster",
    "oh",
    "see",
    "went",
    "okay",
    "let",
    "okay",
    "case",
    "see",
    "looking",
    "four",
    "iterations",
    "see",
    "far",
    "first",
    "column",
    "look",
    "especially",
    "fantastic",
    "let",
    "look",
    "column",
    "quite",
    "different",
    "right",
    "actually",
    "looks",
    "little",
    "bit",
    "promising",
    "especially",
    "ignore",
    "outliers",
    "see",
    "actually",
    "managed",
    "fit",
    "many",
    "time",
    "series",
    "right",
    "fitting",
    "321",
    "time",
    "series",
    "parallel",
    "one",
    "generalized",
    "model",
    "sort",
    "expect",
    "results",
    "vary",
    "one",
    "let",
    "go",
    "20",
    "25",
    "iterations",
    "get",
    "really",
    "excellent",
    "results",
    "5",
    "epochs",
    "encourage",
    "keep",
    "running",
    "computers",
    "takeaways",
    "even",
    "enormous",
    "time",
    "series",
    "data",
    "set",
    "right",
    "fit",
    "memory",
    "laptop",
    "actually",
    "experience",
    "found",
    "deep",
    "learning",
    "works",
    "even",
    "better",
    "like",
    "sort",
    "regression",
    "trees",
    "things",
    "like",
    "cases",
    "smallish",
    "tiny",
    "data",
    "sets",
    "deep",
    "learning",
    "compared",
    "statistical",
    "models",
    "also",
    "want",
    "creative",
    "combining",
    "statistical",
    "models",
    "deep",
    "learning",
    "see",
    "especially",
    "time",
    "series",
    "analysis",
    "broad",
    "domain",
    "knowledge",
    "many",
    "different",
    "sort",
    "classes",
    "analysis",
    "also",
    "help",
    "build",
    "creative",
    "models",
    "even",
    "want",
    "something",
    "like",
    "deep",
    "learning",
    "traditional",
    "knowledge",
    "really",
    "help",
    "ok",
    "one",
    "slide",
    "wrap",
    "want",
    "talk",
    "ok",
    "wrap",
    "highlight",
    "things",
    "talked",
    "important",
    "also",
    "horizon",
    "active",
    "areas",
    "modern",
    "time",
    "series",
    "analysis",
    "think",
    "huge",
    "one",
    "time",
    "much",
    "address",
    "talked",
    "today",
    "anomaly",
    "detection",
    "right",
    "maybe",
    "even",
    "work",
    "field",
    "things",
    "like",
    "structural",
    "time",
    "series",
    "hidden",
    "markov",
    "models",
    "regression",
    "trees",
    "excuse",
    "deep",
    "learning",
    "applied",
    "question",
    "anomaly",
    "detection",
    "anomaly",
    "detection",
    "something",
    "need",
    "one",
    "specific",
    "technique",
    "actually",
    "available",
    "may",
    "even",
    "combined",
    "successfully",
    "also",
    "many",
    "new",
    "old",
    "libraries",
    "time",
    "series",
    "analysis",
    "especially",
    "modern",
    "methods",
    "literally",
    "hundreds",
    "packages",
    "definitely",
    "environment",
    "options",
    "richer",
    "python",
    "might",
    "also",
    "want",
    "get",
    "comfortable",
    "especially",
    "like",
    "least",
    "able",
    "access",
    "packages",
    "python",
    "access",
    "time",
    "series",
    "analysis",
    "obviously",
    "area",
    "active",
    "research",
    "industry",
    "academia",
    "people",
    "research",
    "work",
    "bit",
    "downer",
    "front",
    "want",
    "access",
    "methods",
    "also",
    "want",
    "looking",
    "ecosystem",
    "well",
    "python",
    "ecosystem",
    "two",
    "things",
    "would",
    "highlight",
    "talk",
    "would",
    "also",
    "want",
    "look",
    "firstly",
    "automated",
    "forecasting",
    "scale",
    "somebody",
    "brought",
    "facebook",
    "profit",
    "package",
    "also",
    "google",
    "package",
    "also",
    "twitter",
    "package",
    "probably",
    "forgetting",
    "others",
    "massive",
    "tech",
    "companies",
    "uber",
    "looking",
    "really",
    "massive",
    "data",
    "sets",
    "developing",
    "ideas",
    "research",
    "sometimes",
    "either",
    "open",
    "sourcing",
    "sometimes",
    "offering",
    "forecasting",
    "service",
    "example",
    "amazon",
    "offers",
    "via",
    "aws",
    "offering",
    "forecasting",
    "service",
    "using",
    "sort",
    "expertise",
    "data",
    "right",
    "data",
    "looks",
    "like",
    "amazon",
    "data",
    "right",
    "like",
    "many",
    "many",
    "parallel",
    "time",
    "series",
    "highly",
    "variable",
    "time",
    "series",
    "rolling",
    "new",
    "products",
    "might",
    "want",
    "look",
    "extent",
    "open",
    "versus",
    "little",
    "bit",
    "cagey",
    "varies",
    "company",
    "packages",
    "completely",
    "completely",
    "closed",
    "clear",
    "read",
    "literature",
    "barely",
    "using",
    "deep",
    "learning",
    "primarily",
    "traditional",
    "arima",
    "traditional",
    "models",
    "parameters",
    "goal",
    "fairly",
    "reliable",
    "transparent",
    "forecast",
    "rather",
    "perfect",
    "forecast",
    "finally",
    "mentioned",
    "extent",
    "looks",
    "like",
    "future",
    "combining",
    "machine",
    "learning",
    "statistical",
    "approaches",
    "much",
    "still",
    "want",
    "traditional",
    "timeseriesforecasting",
    "background",
    "arima",
    "sort",
    "thing",
    "going",
    "away",
    "continues",
    "part",
    "even",
    "cutting",
    "results",
    "time",
    "series",
    "analysis",
    "okay",
    "available",
    "questions",
    "afterwards",
    "thanks",
    "much",
    "coming",
    "tutorial",
    "please",
    "touch",
    "questions",
    "comments",
    "applause"
  ],
  "keywords": [
    "okay",
    "good",
    "everyone",
    "tutorial",
    "modern",
    "time",
    "series",
    "analysis",
    "sort",
    "would",
    "different",
    "mainly",
    "means",
    "going",
    "cover",
    "models",
    "talk",
    "briefly",
    "gon",
    "na",
    "methods",
    "computationally",
    "many",
    "ago",
    "still",
    "given",
    "far",
    "successful",
    "data",
    "available",
    "necessarily",
    "mean",
    "new",
    "also",
    "true",
    "even",
    "neural",
    "networks",
    "right",
    "lot",
    "thinking",
    "little",
    "bit",
    "background",
    "years",
    "actually",
    "another",
    "one",
    "traditional",
    "like",
    "arima",
    "thing",
    "interested",
    "need",
    "familiar",
    "today",
    "definitely",
    "really",
    "working",
    "though",
    "less",
    "much",
    "well",
    "give",
    "people",
    "quick",
    "show",
    "already",
    "work",
    "hope",
    "certainly",
    "input",
    "never",
    "enough",
    "something",
    "call",
    "know",
    "get",
    "yeah",
    "someone",
    "two",
    "come",
    "might",
    "whatever",
    "overview",
    "makes",
    "things",
    "want",
    "keep",
    "looking",
    "especially",
    "three",
    "components",
    "state",
    "space",
    "specific",
    "machine",
    "learning",
    "take",
    "look",
    "deep",
    "seems",
    "idea",
    "example",
    "think",
    "every",
    "almost",
    "always",
    "run",
    "examples",
    "stock",
    "usually",
    "fairly",
    "interesting",
    "course",
    "modeling",
    "open",
    "problem",
    "point",
    "called",
    "without",
    "anyone",
    "using",
    "word",
    "hand",
    "often",
    "measurement",
    "kind",
    "curve",
    "field",
    "trying",
    "wrong",
    "tell",
    "structure",
    "x",
    "axis",
    "use",
    "temporal",
    "per",
    "metric",
    "points",
    "anything",
    "prediction",
    "probably",
    "useful",
    "although",
    "maybe",
    "clustering",
    "find",
    "way",
    "better",
    "questions",
    "understanding",
    "behavior",
    "seasonality",
    "quite",
    "seasonal",
    "change",
    "cycle",
    "identify",
    "underlying",
    "sense",
    "see",
    "river",
    "level",
    "predict",
    "dynamics",
    "else",
    "past",
    "future",
    "values",
    "say",
    "forecasting",
    "noisy",
    "measurements",
    "sorts",
    "got",
    "could",
    "plus",
    "minus",
    "let",
    "measure",
    "used",
    "ca",
    "error",
    "value",
    "steps",
    "versus",
    "getting",
    "next",
    "classification",
    "common",
    "talked",
    "figure",
    "able",
    "mind",
    "number",
    "samples",
    "imagine",
    "human",
    "year",
    "simple",
    "together",
    "looks",
    "sets",
    "extent",
    "second",
    "high",
    "correlation",
    "errors",
    "tend",
    "throw",
    "first",
    "order",
    "super",
    "correlated",
    "bad",
    "inputs",
    "apply",
    "whereas",
    "general",
    "likely",
    "distance",
    "along",
    "exact",
    "day",
    "matter",
    "go",
    "case",
    "mentioned",
    "meaningful",
    "make",
    "sure",
    "finally",
    "regime",
    "oh",
    "top",
    "thought",
    "noise",
    "ok",
    "model",
    "us",
    "describe",
    "aware",
    "plot",
    "information",
    "ever",
    "training",
    "roll",
    "forward",
    "test",
    "write",
    "goes",
    "research",
    "ahead",
    "particular",
    "numbers",
    "date",
    "system",
    "move",
    "took",
    "sometimes",
    "depending",
    "yes",
    "statistical",
    "domain",
    "done",
    "whole",
    "certain",
    "wo",
    "start",
    "least",
    "seen",
    "term",
    "class",
    "developed",
    "long",
    "early",
    "small",
    "set",
    "forecast",
    "pretty",
    "difficult",
    "1",
    "component",
    "sub",
    "2",
    "moving",
    "back",
    "coefficients",
    "rather",
    "ways",
    "part",
    "difference",
    "instead",
    "original",
    "air",
    "passengers",
    "absolute",
    "form",
    "trend",
    "saying",
    "terms",
    "fitting",
    "easy",
    "fit",
    "complicated",
    "everything",
    "3",
    "function",
    "dynamic",
    "gets",
    "build",
    "works",
    "five",
    "hundred",
    "knowledge",
    "random",
    "seven",
    "arguably",
    "tree",
    "network",
    "structural",
    "around",
    "add",
    "end",
    "compare",
    "try",
    "feel",
    "via",
    "maximum",
    "filter",
    "wanted",
    "estimate",
    "update",
    "method",
    "couple",
    "reasonable",
    "offer",
    "package",
    "packages",
    "states",
    "compared",
    "remember",
    "zero",
    "seeing",
    "step",
    "k",
    "essentially",
    "10",
    "code",
    "best",
    "prior",
    "gaussian",
    "based",
    "saw",
    "similar",
    "times",
    "put",
    "matrix",
    "parameters",
    "notebook",
    "local",
    "either",
    "slides",
    "version",
    "global",
    "last",
    "quickly",
    "great",
    "source",
    "obviously",
    "recommend",
    "looked",
    "exercise",
    "minutes",
    "helpful",
    "index",
    "expect",
    "plotting",
    "seem",
    "check",
    "notice",
    "label",
    "python",
    "job",
    "standard",
    "max",
    "api",
    "read",
    "hmm",
    "20",
    "months",
    "flat",
    "line",
    "null",
    "twelve",
    "beginning",
    "shape",
    "break",
    "basically",
    "12",
    "n",
    "sine",
    "outcome",
    "tends",
    "said",
    "turn",
    "hidden",
    "markov",
    "output",
    "sequence",
    "reason",
    "discrete",
    "flow",
    "low",
    "switching",
    "probability",
    "feature",
    "process",
    "algorithm",
    "transition",
    "j",
    "probable",
    "built",
    "fact",
    "results",
    "cut",
    "extraordinary",
    "sample",
    "feed",
    "multivariate",
    "parallel",
    "decision",
    "trees",
    "generation",
    "generate",
    "features",
    "peaks",
    "classes",
    "forest",
    "xg",
    "boost",
    "cluster",
    "curves",
    "warping",
    "image",
    "library",
    "amplitude",
    "percent",
    "libraries",
    "histogram",
    "sliding",
    "window",
    "windows",
    "positional",
    "recurrent",
    "convolutional",
    "rnn"
  ]
}