{
  "text": "if you have an interview coming up and\nyou want to revise 10 most important\nmachine learning algorithms real quick\nyou will not find a better video than\nthis let's go ahead and do the revision\nof 10 most frequent used ml algorithms\nthese are the 10 algorithms I am going\nto explain you how they work and what\nare their pros and cons okay and as you\ncan see first five algorithms is in one\ncolor next three is in a different color\nand last two is in a different color\nthere is a reason for that guys I will\ntell you in a moment but before that\nlet's try to answer two basic questions\nokay let's try to answer what is machine\nlearning and what are algorithms okay so\nI'll start with a non-bookish definition\nand I will give you one simple example\nsuppose you want to travel from\nBangalore to Hyderabad okay where you\nwant to go you want to go from Bangalore\nto Hyderabad\nfor this you can either take a train or\nyou can either take a flight or you can\ntake a bus as well or maybe you can\ndrive your own car as well okay so two\nthings we have to understand here guys\nwhat is the task okay and what is the\napproach\nfine\nso the task in hand is we have to go\nfrom Bangalore to Hyderabad okay and the\napproach is all these three options that\nI told you just now\nnow related to the world of machine\nlearning in machine learning the task\ncan be different kinds of tasks okay for\nexample it can be a regression task\nokay or it can be a classification task\nokay\nor it can be a unsupervised learning\nproblem I will just write unsupervised\nokay so in approach section we can have\ndifferent different approaches based on\nif we are solving a regression problem\nor we are solving a classification or we\nare solving a particular case of\nunsupervised learning okay in regression\nalso we can take many approaches for\nexample in regression there is not only\none approach in regression I can take\napproach one approach two approach 3\napproach 4 approach five in\nclassification I can take this approach\nthis approach this approach in\nunsupervised also I can take multiple\napproaches so that is why this color\ncoding is there\nthe first five algorithms that you see\nhere\nwill solve I will explain you for\nregression use case Okay so there we\nwill take a regression use case and try\nto understand how to solve that using\nthese five algorithms okay the next\nthree that you see I am going to explain\nyou with a classification use case so\nthese approaches are for classification\nproblem okay\nand last two I am going to explain you\nfor a unsupervised learning problem how\nthat will be this these algorithms will\nbe used to solve unsupervised learning\nproblem okay\nso let's go ahead guys and try to\nunderstand with a simple input data I\nhave taken a sample input data here and\nlet's without any delay start on the\nfirst algorithm known as linear\nregression so machine learning is all\nabout learning pattern from the data\nusing algorithms okay so if we are using\na algorithm known as linear regression\nthen what will happen let's try to\nunderstand that so first algorithm of\nour list linear regression okay now\nsuppose this is the employee data of an\norganization you have a age column you\nhave a salary column fine so 22 years\nperson earns 23 000 and so on and so\nforth suppose we using the linear\nregression approach to solve this\nregression problem now as I told you\nfirst five problems will be regression\nproblems first five algorithms you will\nunderstand using regression problem okay\ncome here this is your data so what\nlinear regression will do is\nit will just take this data and it will\nsee how the data is plotted on a XY\nplane like this for example on one axis\nwe can take salary okay on y axis\nand on x axis we can take Edge okay and\nI am just roughly pointing these points\nokay first point 22 and 23 000 maybe it\ncan come somewhere here on x axis if you\nput h on Y axis salary I am just putting\nhere second data point can come\nsomewhere here let's say 41 and 80 000\ndata points and third data point 58 and\n150k this data point can come maybe\nsomewhere here I can say okay\nso what linear regression will do is it\nwill try to plot a line okay ideally\nwhat the assumption is all these points\nshould fall on same line\na line like this can be plotted or a\nline like this can be plotted but the\nAssumption here is\nideally in an Ideal World all these\npoints will fall in the same line but it\nwill never happen in the real world so\nwhat logistic linear regression will do\nis it will try to fit something known as\na best fit line okay so this is your\nbest fit line let's assume that how this\nbest fit line is computed it will try to\nminimize the distance from all these\npoints together so distance from this\npoint is this distance from this point\nis this parallel to Y axis distance from\nthis point is this okay so you can call\nthis even you can call this E2 you can\ncall this E3 okay so what linear\nregression will do is it will try to\nminimize even Square\nplus E2 square plus E3 Square\nfor whichever line it finds the minimum\neven Square E2 Square E3 Square it will\ncall that line as the model\nokay it will call that line as the model\nnow as you know from your normal\nunderstanding of mathematics\nthis straight line will have a equation\nin the form of mostly simplest we can\nwrite Y is equal to MX plus C right in\nour case I can say salary is equal to M\ntimes h m times of H this is\nmultiplication plus c c can be an\nintercept let's give some number here\nsome random number I will give let's say\n2000 okay\nso imagine this line which is the model\nfor linear regression has this formula\nokay now the next question comes\ntomorrow when the pattern has been\nlearned and a new age comes let's say\nage is 50.\nso what will be the salary for that\nperson so very simple the model will\ncome here and put the numbers here for\nexample if for M we can put any number\nlet's say 0.2\nthen age will be 50 and then salary will\nbe intercept will be 2000 whatever this\ncalculation comes that will be the\nprediction of the salary for this 50\nokay very simple very simple\nmathematical model the assumption is\nthere is a linear relation between\nindependent variable and Target variable\nokay that's the Assumption so what it\nwill do it will try to plot a line what\nit will call as a best fit line wherever\nit finds this value as minimum once the\nbest fit line comes then how the\nprediction happens like this okay\nobviously there will be pros and cons of\nall the algorithms all the models so\nwhat is the pros and cons of linear\nregression the the pluses or Pros for\nthis model will be it's a simple to\nunderstand model it's a mathematical\nmodel you can explain to someone but the\ncons will be\num it's not necessary that your data\nwill always be this simple that can be\nfit in a line right or close to a line\nso it's a simple model hence lot of real\nworld problems it may be difficult to\nsolve with simple linear regression\nthere can be a varieties in linear\nregression that\num I have created videos you can watch\nthrough those videos but simply linear\nregression works like this okay this is\none first approach\nfirst approach means first algorithm now\nlet's go ahead and try to see how\ndecision tree will approach the same\nproblem okay how decision tree will\napproach this same problem\nso if you give this same data okay if\nyou give the same data to decision tree\nand you ask hey learn pattern from this\ndata what decision tree will do is\nit will just try to break the data how\nit will break the data is it will create\na rule like this okay so I can write a\nrule here for example I can say is less\nthan equals to 30 this is a rule okay so\nsome records will satisfy this rule okay\nsome records will satisfy and some\nrecords will not satisfy this way data\nwill break okay if you come here is less\nthan 30 how many records only one record\nis more than 30 two records so how many\nrecords will come this side only one\nrecord will come okay so let's say that\nrecord is\nI should not write the wrong Numbers 22\n23k 4180k\nso I will write here 22 and 23 K and\nhere I will write 41 and 80k okay and\nthere is one more record let me take the\nnumbers 58 and 150k\n58 and\n150k understand this carefully guys\nbecause for next next algorithms this is\nthe base okay\nso decision tree will split your data\nlike this so you had total how many\nrecords in the beginning three records\nhere how many records you are having one\nrecord here how many records you are\nhaving two records okay so this is first\nlevel of split now definitely can split\nit one more time okay\nso tree can make here there are limited\nnumber of Records but imagine if there\nare more records there can be one more\nsplit here saying you know another\nfilter is is maybe less than 40 or\nsomething like this okay but I will not\ntake that now that will make the tree\ncomplex okay so this is your model\nbreaking your data based on some\nconditions is nothing but your model so\nsomebody asks you what is a model in\ndecision tree this is your model now the\nimportant question is suppose tomorrow\nsomebody comes and asks for a person\nwith age 50 what is your prediction for\na person with age 50 what is your\nprediction very very important concept\nto understand guys decision tree will\ncome and check what is this for age 50\nokay so age 50 will come in which\ncategory will come in this line okay in\nthis line how many records are there two\nrecords so decision tree will go ahead\nand take the average of these two\nsalaries so for age 50 your prediction\nwill be what will be the prediction guys\nfor age 50 prediction will be 80k plus\n150k divided by 2. okay this is how\ndecision tree will be making the\nprediction\nsuppose you ask through this entry hey\nwhat will be the salary of a person with\nage 21 so it will not go to right hand\nside it will go to left hand side\nbecause this is the tree branch in which\nit should go it will directly say 23k in\nthis case because there is only one\nrecord Suppose there are two records it\nwill take the average okay so you see\nhow these two approaches are different\nfor solving same regression problem here\na mathematical line will be fit and here\na decision tree you know data will be\nbroken into multiple pieces and\nprediction will be made okay remember\nguys decision tree is based for many\nother Advanced algorithms and our third\nalgorithm in the list is something non\nas a random Forest okay a random Forest\nwhat random Forest will do is it will\nsay decision tree okay you have done a\ngood job but\nuh there is a chances of overfitting of\nthe data so we did not discuss pros and\ncons of this process it's a simple model\nyou know you don't need to do a lot of\nmathematics Etc and cons is there is a\nchances of overfitting because you know\nif there is a little change in the data\nyour model may change totally that's a\nrisk here in decision tree so\noverfitting\nSo Random Forest will come and say Hey\nyou are taking a right approach but\nthere is a chances of overfitting so why\ndon't you fit multiple trees so what\nrandom Forest will do is it will come\nand create multiple trees this is your\ntree one okay like the way we saw\ndecision tree this is your for example\ntree one okay this is your for example\ntree two\nokay\nand similarly there can be n number of\ntrees okay similarly there can be n\nnumber of trees so we will call this as\nT1 we will call this as T2 and that\nthere can be you know 500 trees for\nexample\nso what random Forest will do is it will\nsay two deficiently hey if you are\nfitting one tree there is a chance of\nresult being biased or there is a chance\nof overfitting or there is a chance of\nmodel not being stable but what I will\ndo is I will fit 500 trees okay and how\nI will make the prediction is very\nimportant to understand here guys\nprediction of random Forest will be\naverage of all these prediction for\nexample if we are trying to predict for\nthe age 50 right for the age 50 what\nwill be the salary if we are trying to\npredict okay then in random Forest it\nwill take prediction from tree one plus\nprediction from 3 2. Plus\nprediction from tree 500 okay it will\ntake all the predictions and it will\ntake average of that\nwhat is the what is the thing that we\nare trying to achieve here suppose in\none decision tree your tree is\noverfitting or not performing well or is\nbiased okay so what may happen in\ndiffusion trees since you are taking a\nfeedback from 500 different trees so\nthat overfitting problem or model in\nstability problem may not be there okay\nso this is how random Forest is\ndifferent from decision tree remember\nall these individual trees will not be\nusing all the data for example\nsuppose in your data there is one\nthousand rows and 10 columns okay just\nan example I am giving so all these all\nthese trees will not use necessarily all\nthe records it may be possible that tree\nOne is using 100 records and three\ncolumns randomly selected three two T2\nis using three two hundred records and\nthree columns randomly selected okay and\nthat is the advantage of this random\nForest that all these trees Will May\nlearn a different kind of pattern and\nwhen you take a aggregated result then\nyou will have all the flavors okay this\nkind of learning that I just explained\nyou is known as and Sample learning okay\nremember guys at unfold data science you\nwill find a big playlist explaining all\nthe algorithms of Ensemble learning in\ndetail I will paste the link in the\ndescription you must check if you have\nany confusion on how and simple learning\nworks okay\nbut there is more to Ensemble learning\nwhat happened just now in random Forest\nis known as parallel way of learning\nokay parallel way of learning\nparallel way of learning why parallel\nway of learning guys because here tree\none and three two and three three are\nindependent of each other when you call\na random forest model 31 can start\nbuilding by taking a sub sample of the\ndata 3 2 can start building by taking a\nsubsample of the data they are not\ndependent on each other okay so all\nthese things can happen parallely hence\nwe call it a parallel learning now the\nquestion is is there another way of\nlearning in Ensemble yes there comes our\nnext algorithm known as add a boost okay\nAda boost standing for adaptive boosting\nso what Ada boost will do is\nlet me write the data here let me write\nthe data one more time and I may be\nwriting some different numbers\nso that's not important just\nunderstanding the concept is important\nokay so 42 I will write 50 000 and let's\nsay 58 I will write 150 000 just as an\nexample this is your input data\nso boosting is another technique\nboosting is another technique of\nEnsemble category okay in boosting\nespecially at a boost what will happen\nis it will assign a weight to all your\nobservations okay suppose this is your\noriginal data for training salary being\nyour target column so initial weights\ninitial weights\nokay\nand\nwhat the initial weights will be it will\nbe the same weight for all your records\nfor example there are three records so\none by three I am saying one by three I\nam saying one by three I am saying so\nall the rows are equally important okay\ntry to understand the concept guys in\nAda boost in the beginning first\niteration all the rows are equally\nimportant okay but how Ada boost works\nis in the name only there is adaptive it\nadapts to the mistakes of the previous\nmodel now why I am saying a previous\nmodel and next model is one thing you\nhave to always remember at a boost is a\nsequential learning process you you\nremember how I just now told random\nForest is a parallel learning process\nso in random Forest\ntree one and three two are independent\nof each other okay it will take a sub\nsample and create it will take a sub\nsample and create nothing to do with\neach other\nbut in adoboost or other boosting\ntechniques\nit's a sequential model so there will be\na multiple models in this so there will\nbe multiple models fitted to the data I\nwill tell you in a moment what these\nmodels will be model 1 model 2 model 3\nModel 4 and so on and so forth how many\never model comes but it will not happen\nparallely okay it will happen in\nsequence\nnow the important thing to understand is\nhow this sequence will be generated okay\nso what will happen is this model one\nyou can think of as a base model this\nmodel one you can think of as a base\nmodel and remember in Ada boost your\ndecision trees will look like stumps\nstumps means there will be a tree like\nthis and there will be another tree like\nthis so it will the depth of the tree\nwill not be Beyond one level okay so\nthis is called stumps in the language of\nmachine learning\nso multiple stems will be created now\nsuppose your model 1 is this first stump\nwhat is your model one guys this first\nstump okay\nmodel one comes and make some prediction\nabout the salary model one comes and\nmake some predictions about this salary\nokay so what we will have is another\ncolumn called as salary underscore\nprediction and where from this\nprediction Comes This prediction comes\nfrom model one the first model okay so\nobviously there will be some mistakes so\n22 000 may be said as 21 900 and 50 and\n150 can be said as 50 can be said as\nlet's say 52 000 okay and 150 can be\nsaid as let's say two hundred thousand\nbased on this first model first decision\ntree that it is creating which I am\ncalling a system so there will be some\ndifferences between actual and predicted\nand from this there will be a residual\ncoming residual means errors right\nresidual means errors okay so what will\nbe the errors 21 900 minus 22 000 right\nso it will be for example I can say a\nhundred\nactual minus predicted it is minus two\nthousand and it is minus minus 50 000\nbecause we have put okay so this is the\nerrors these are the actual values and\nthe first model what it predicts right\nthose are the errors from the first\nmodel OKAY twenty two thousand minus\ntwenty one nine hundred is one hundred\nand so on and so forth\nnow\nthese are the initial weights okay\nso what will happen in the next model\nwhen the M2 is fitted right these\ninitial weights will be changed and more\npreference will be given to the\nobservations where these residuals are\nmore okay I am repeating one more time\nguys M1 will predict this and then\nresiduals or errors will come when the\nM2 is trained right then the weights\nwill not be same for all these three\nrecords rather weight will be increased\nfor this because you are getting more\nerrors here and weight will be decreased\nfor this because you are getting less\nerror here okay\nand so on and so forth M2 will come\ncompute create the residual then again\nweights will be adjusted M3 will come\npredict residual will be calculated\nweights will be adjusted and finally\nwhat you will get is a combination of\nwhat will be your final model your final\nmodel will be a combination of base\nmodel I am calling it the first model\nokay plus M1 plus M2 plus M3 plus so on\nand so forth remember this this is not a\nmathematical equation this is just\nindicative equation I am giving you okay\nif you want to understand more\nmathematics behind it please go ahead\nand click on the link I'm giving you in\nthe description okay and all these\nthings will not have equal say in the\nfinal output their say also will be\ndifferent in the final output for\nexample in random Forest you saw all the\nmodels have equal C in the final output\nwe are dividing by 500 okay\nbut here all these models will not have\nequal say they will have an equal say\nokay\nlet's move ahead to another what is the\npros and cons for this model again this\nmodel will give you a may give you a\nbetter result than most of the models\nbecause it is adapting to the changes\nbut if you have a larger data side it\nmay it may need more resources to train\nand also it is a one kind of Black Box\nmodel some kind of Black Box model means\nyou don't have much explanation of what\nis going on inside apart from some hyper\nparameters okay\nlet's move ahead to the last algorithm\nintegration category known as gradient\nboost okay what is the last algorithm\nintegration category gradient boost\nremember guys all these algorithms that\nI'm explaining you I have not taken\nanything that is used less all are used\nmore only okay\nso I will take a simple data age\nsalary is 21 salary let's say 20K is 40\nsalaries let's say 42k is 58 salary is\nlet's say 60k this is your input data\nand you want to run a gradient boost on\nthis\nwhat will happen is understand guys this\nis again a sequential learning not a\nparallel learning okay so there will be\na base prediction\nfor all these data base prediction okay\nbase prediction\nwhat is the base prediction guys base\nprediction is nothing but it's a kind of\ndumb model it will assume that for all\nthese guys it will be a average of you\nknow all these three records so what is\nthe average of this uh 80 plus 42.\n80 plus 42 divided by 3 right so 2 1 1\n2. right let's say assume for Simplicity\nthis is 36k okay so the base prediction\nwill be put here 36k 36k 36k one is the\nbase prediction comes then there will be\na residual computed okay residual will\nbe the difference between actual and\npredicted values whatever these numbers\nare\nfine now comes the interesting part how\ngradient boost is different from Ada\nboost or other algorithms so what\ngradient boost will do is it will try to\nfit a model on this residual okay it\nwill try to fit a model on this residual\nand try to minimize these residuals so\nthat will be called as a base model okay\nand then there will be next model you\ncan call it residual model one okay and\nthen there will be a next model you can\ncall it residual model 2 and so on and\nso forth okay so what will happen is\nresiduals will be computed and then\nwhatever the residual comes based on\nthat base prediction will be updated so\nfor example let's say your residual here\nis how much 20 minus 36 minus 16 is your\nresidual right\nso this will act as a independent column\nand this residual will act as a Target\ncolumn\nand then let's say in the prediction\nthis minus 16 is is comes as let's say\nminus 10. so what will happen is this\nbase prediction will get updated by this\nthis base prediction will get updated\nagain it's a complicated model if you\nwant to understand more details there\nare links in the description please\nclick on that it will be very clear to\nyou okay so what will happen base model\nplus residual model 1 plus residual\nmodel 2 so on and so forth and there\nwill be some parameters which will\nassign weight to all these models so as\nI say all these models will not have\nequal vote in the final output there\nwill be a different votes in this fine\nso this is about gradient boost one of\nthe famous algorithm for winning kaggle\ncompetitions and most of the things so\ngradient boost and there is another\nvariant of gradient boost known as xgb\nextreme gradient boost please go ahead\nand read about this algorithm guys I am\nnot covering because there is a slight\ndifference between gradient boost and\nsgb you can read about that as well fine\nlet's move ahead to the second category\nof algorithms known as classification\nalgorithms so in classification\nalgorithms the first algorithm that I am\ngoing to cover is logistic regression\nnow very very important guys please pay\nattention here and try to understand how\nlogistic regression is going to work for\nany given scenario it's a mathematical\nmodel hence it is important for you to\nunderstand okay suppose this is an\nemployee data and you have 21 22k\nwhether the employee leaves the\norganization or does not leave the\norganization just I am saying 1 0 okay\nand then 40 year guy makes let's say 42k\nleave 0 no 58 year guy makes let's say\n60k just for example leaves know one so\nthis is a classification problem where\nwe are trying to predict whether a\nemployee will leave the organization or\ndoes not leave the organization the last\ncolumn that you see is your target\ncolumn the last column that you see is\nyour target column this type of problem\nis called a classification problem\nbecause what this what the objective of\nthis model is tomorrow I give you age of\nthe employee for example 31 salary of\nthe employee for example 34k and I asked\nto the model hey Will the guy leave or\nnot leave the organization okay so this\nis a classification problem how logistic\nregression will take this problem is we\nhave we have to understand some\nmathematical Concepts here so if you see\nhere the target column is 1 0 only so\nthat is either one or zero one or zero\nokay so which means that Y which is our\nTarget can be understand this is very\nimportant concept guys\ncan be either 0 or 1 it cannot be\nanything else your target cannot be\nanything else apart from 0 or 1 but your\nage and salary can take any real number\nX can be\nany value between minus infinity to plus\ninfinity right\nso X can be any value between minus\ninfinity 2 plus infinity y can be only 0\nor 1 okay\nso what we have to understand here is we\nhave to somehow create a relation that\nwill enable us to predict y given X okay\nthe problem here is on the left hand\nside we have minus infinity to plus\ninfinity range that is X range okay so I\nwill write here x x means independent\nfeatures on the right hand side your\nvalues can be only 0 to 1 0 or 1 not 0\nto 1 okay\nso what we do is we do not directly\npredict y rather we predict something\nelse what is that something else that we\npredict so in place of predicting y we\npredict probabilities okay probabilities\nof an observation falling in y\nprobabilities\nl i t i e s Okay so\nwhat we will do is we will predict\nprobabilities then the range will be 0\nto 1 as you know probability can take\nthe range between 0 to 1 okay\nnow this range is also not what we are\nlooking for minus infinity to plus\ninfinity so what we will do is we will\ndo one more transformation and we will\nmake this as a odds so what is the range\nof odds 0 to Infinity okay but still we\nare not minus infinity 2 plus infinity\nrange so what we will do is we will take\nlog of odds okay log of odds\nokay and then the range will become\nminus infinity to plus infinity\nhow your equation will look like here is\nwhen you say log of odds right so on the\nright hand side it will be log of P by 1\nminus p\nokay on the left hand side you will have\nbeta node plus beta 1 x 1 plus so on and\nso forth okay this equation that you see\nin front of you now is called the base\nequation for the logistic regression now\none important concept to understand here\nguys this is a logic function okay and\ninverse of logit function H sigmoid\nfunction okay support suppose you take\nthe inverse of this or sigmoid of this\nso what will happen is if you apply\nSigma at both sides so if you don't know\nwhat is sigmoid function then sigmoid\nfunction f x looks like this 1 by 1 plus\ne to the power minus X this is your\nsigmoid function on XY plane how it will\nlook like is this suppose this is your 0\nthis is your 1 and this is your 0.5 okay\nso sigmat will look like this so it will\nalways be between 0 to 1 okay so your\nlogistic regression this equation will\nbe changed in the form of sigmoid\nfunction so your f x or P okay P will\nlook like if you take if you take\nsigmoid on both sides right then on the\nright hand side you will just have p and\nhere we will have 1 by 1 plus e to the\npower minus\nbeta0 plus beta 1 x 1 okay remember guys\nthis equation is equation 1 and this\nequation is equation to both the\nequations are same the difference is\nthis is a logit equation and this is a\nsigmoid equation okay now take it if you\nif you take a inverse of logit that is\nnothing but sigmoid okay understand this\ncarefully and now this equation from our\nexample how it can be written is 1 by 1\nplus e to the power minus beta 0\nplus beta 1 into age okay plus beta 2\ninto salary\nthis is nothing but your logistic\nregression equation okay and as you know\nas I told you this is a sigmoid function\nso the output of what you see here\noutput of this will always be between 0\nto 1 which means you can get a\nprobability and then you can say that\nbased on this probability I can say\nwhether the employee leaves or does not\nleave okay logistic regression is again\na very important and not easy to\nunderstand concept okay so as you can\nsee we are modeling a categorical\nvariable against real numbers hence we\nneed to do certain Transformations these\nare the Transformations that we need to\ndo and how it relates to the probability\nI just explained you now okay pros and\ncons mathematical model not very\ndifficult to understand cons it again\nassumes a lot of things about the data\nwhich may or may not be correct hence it\nmay not give a great result all the time\nokay but very famous and very important\nalgorithm to understand\nnext algorithm in the category in the\nclassification category one simple one I\nwant to cover here that is known as gear\nnearest neighbor okay it's a pretty\nsimple algorithm suppose in the same\ndata on this data you want to build a k\nn algorithm okay so since I have data\nhere so I will explain here only so what\ncan happen is it will plot a x-ray plane\nlike this okay and it's a\nthree-dimensional data so you can have\none more axis for salary\nor you can have two access only because\nfrom two axis we can we can predict okay\nso let's come here age and let's say\nhere salary okay\nout of these three employees let's say\none employees 21 22 employees Falls here\nand second employee 40 Falls here and 58\nFalls somewhere here okay so what K\nnearest neighbor will do is it will try\nto allocate neighbors to all these\nindividual observations for example this\nis your observation one this is your\nobservation two and this is your\nobservation three okay so one does not\nhas any neighbors but 2 is the neighbor\nof 3 and 3 is the neighbor of two okay\nso tomorrow some prediction comes for\nlet's say age 50 again I will take 50\nexample\n50 example\nso what it will do is it will try to see\nand I will take salary also because in\nthis case salary is also there so salary\nis let's say 61k okay so what it will do\nis it will try to see where can I fit\nthis 58 percent and salary 61k Maybe\nwho are the nearest neighbor to that guy\nso the nearest neighbor to that guy may\nbe this guy and this guy suppose that\nnew guy comes somewhere here okay so who\nare the neighbors for this this is the\nfirst neighbor this is the second\nneighbor okay so it will simply go ahead\nand take the you know mode of results\nfor example these these two guys are the\nare the second neighbors right I mean\ntwo neighbors of that so it will take 0\nand 1 which is maximum so in this case\nthere is no mode of the data but\nobviously if you take a larger data\nthere will be modes of the data okay so\nwhichever mode for example Suppose there\nare 30 records out of that 20 is 1 and\n10 is 0. so the prediction for this guy\nwill be whatever is maximum or whatever\nis mode so if the mode is one or zero\nwhatever it is that will be the\nprediction for k n okay so as I told you\nCannon is a pretty simple algorithm it\nwill just plot your data try to find the\nnearest neighbors and then when a new\nobservation comes you give how many how\nmany Observer how many neighbors you\nwant for that record and it will create\none based on that okay so Canon is a\nsimple to understand algorithm nothing\ncomplex in that so I covered quickly in\nthat that slide itself okay now let's\ntry to understand another classification\ntechnique known as support Vector\nmachines or svms\nso what svms will do is it will plot\nyour data in whatever axis you have\nsuppose age is one axis and salary is\none axis okay\nand your data points I will take little\nmore data points okay your data points\nlook like this so these are some data\npoints and this is these are some more\ndata points okay\nso what sbm will do is it will try to\ncreate something known as a decision\nboundary okay how this decision boundary\nis different from linear regression\ndecision boundary is in any integration\nthere is a pure mathematical equation\ninvolved here there is a concept of\nsomething known as a hyper plane okay\nfor example if I draw a line between\nthis right so all these guys black guys\nyou can think leaves or Target column is\none\nall these guys you can think does not\nleave or Target column is zero does not\nleave\nokay\nsuppose your data is like this so what\nwill happen is your svm will plot this\nis called in the language of sbm this is\ncalled a decision boundary okay decision\nboundary so in this case your data looks\npretty simple pretty separated hence the\ndecision boundary can be as simple as a\nline okay but in most of the scenarios\nreal world scenarios decision boundary\ncannot be as simple as this okay so\nthere will be some black dots here there\nwill be some black circles here okay and\nthere can be some this Cross Blue Cross\nthis side right so in this case decision\nboundary is not doing Justice so\ndecision boundary need to change and\nthat is where the concept of hyper\nplanes and kernels two very important\nConcept in svm guys if you want to\nexplore more on sbm hyper planes okay\nand kernels\nso when your data become complex then\nsimple decision boundaries cannot\npredict it well okay so you need to have\na have a complex decision boundary and\nthat is where hyperplane and kernels\nconcept come but just to give you an\nidea of how svm works it will create a\ndecision boundary and tomorrow any\nprediction any new result come for\nexample somebody asks what is the um you\nknow for a person with for a person with\nage 50\nand salary is 60k whether the person\nwill leave or not leave so this svm\nmodel will see on which side of decision\nboundary this guy is falling if this guy\nfalls on this side of decision boundary\nit says do not leave if this guy falls\non this side of decision boundary it\nsays leaves okay so in svm remember\nconcept of decision boundaries hyper\nplanes kernels and kernel tricks okay\nso we have covered three things from the\nclassification scenarios and five things\nfrom the regression scenarios let's go\nahead and try to see some unsupervised\nlearning problems okay so what is the\nmeaning of unsupervised learning till\nnow we are having a Target column but in\nunsupervised learning we may not have a\nTarget column okay suppose for the same\nemployee data we have age and salary\nbut somebody comes to you and tells you\nthat hey can you tell me if there are\ndifferent buckets of employees existing\nin my organization\ndifferent buckets means some people with\nless age and more salary some people\nwith more is endless salary so are there\ndifferent buckets somebody can can come\nand ask you okay\nso how you will solve that problem is by\nusing something knowledge clustering or\nsegmentation okay so suppose the task in\nhand is here there are three records\nonly but there can be more records right\nin the real world scenario what I am\ninterested in knowing is if there are\nnatural clusters in my organization so\nthis is my organization data on one axis\nI have is on other axis I have salary\nokay and I have multiple data points\nhere three data points only but I am\nplotting more data points just for\ndemonstration okay so there is nothing\nto predict but employed is interested in\nknowing if there are buckets means if\nfew employees are closer to each other\nin terms of their characteristics so for\nexample these employees are closer you\ncan call bucket one these employees are\ncloser you can call bucket two or\nsegment 2. okay but how this will be\nimplemented is in K means clustering so\none technique for implementing bucketing\nis K means clustering okay there can be\nother techniques also for segmentation\nor bucketing one technique is K means\nclustering in this technique what will\nhappen is the distance between the\nvarious employees will be computed for\nexample this is your employee one and\nthis is your employee two okay suppose I\nask you how similar is employee one from\nemployee two so there can be different\nsimilarity metric that you can compute\nfor example euclidean distance or\nManhattan distance or cosine similarity\nEtc I have detailed video on these\nthings as well I will link it in the\ndescription but suppose I tell you a\nsimple uh you know how the distance how\nthe similar Sim how these two employees\nare similar or different so you will say\n21 minus 40 whole Square\nplus 20K 20K\nminus 42k whole Square so on all the\ndimensions you are taking the distance\nbetween them squaring it and under\nrooting it this is called euclidean\ndistance between E1 and U2 whatever\nnumber you get it okay\nso suppose E1 and E2 equilibrium\ndistance is less and E1 and E3\nequilibrium distance is more so in that\ncase you say E1 and E2 are closer to\neach other okay and in the similar way\nyou start finding the employees which\nare closer to each other and then you\ncall this as one bucket similarly this\nscore you call is at an another bucket\nokay remember I have explained you in\nsimple terms but there is a very\nimportant Concept in k-means known as\ncentroid concept okay so please go ahead\nand watch unfold data science detailed\nthe video on k-means clustering you will\nunderstand all the details of how\ncentroid is defined and how this\nalgorithm works at a mathematical level\nokay I will link that video please\nensure you watch that\nso this is about k-means clustering now\nlast but not the least guys you might\nhave seen in Amazon and Flipkart that\nthere are different different uh\nproducts that is recommended to you for\nexample if you buy a laptop then it will\ntell you hey go ahead and buy this\nlaptop bag as well so this is nothing\nbut a recommendation okay\nin the Netflix if you watch let's say\none movie one action movie let's say if\nyou watch Mission Impossible then it\nwill go and recommend you Jack Ryan\nseries maybe okay\nso this is called a recommendation\nsystem that is running in background\nokay so how this system works one simple\nuh yet powerful technique for\nrecommender system is known as\ncollaborative filtering collaborative\nfiltering\nokay\nso what collaborative filtering does is\nit will take users okay users\nand it will take items\ntry to understand this simple concept\nEdge it's pretty simple to understand so\nusers can be a month\nand users can be John\nand users can be do okay and in the\nitems we can have let's say Mission\nImpossible\nin the atoms we can have Jack Ryan in\nthe atoms we can have another any movie\nof James Bond Series in the atoms we can\nhave Spiderman okay in the atoms we can\nhave any comedy movies for example home\nalone you can say okay\nso Aman which movie Aman watches or\nwhich movie Aman has watched for example\nMission Impossible Aman has watched Jack\nRyan he has watched but he has not\nwatched let's say this movie Zero I will\nsay okay James one movie and this movie\nhe has not not uh watched okay\nSpider-Man movie there is another guy\nJohn who has watched Mission Impossible\nJack Ryan James Bond movie and\nSpider-Man movie as well\nthere is another guy doe who has not\nwatched any of these movies but has\nwatched Home Alone the comedy movie okay\nso\nthe which users are similar to which\nuser will be computed based on one of\nthe user similarity metric so what are\nthe user similarity metric I told you\ncosine similarity it can be different\nkind of distance metric so as you can\nthink from the common sense also here\nAman watches action movies if you can\nsee here and John also watches action\nmovies more\nMission Impossible and Jack Ryan but\nAman has not watched James one movie and\nAman has not watched Spider-Man movie so\nwhat will happen is since Aman and Jon\nare similar to each other so go ahead\nand watch the movies that Jon has\nwatched but Aman has not watched because\nAman and Jon both tastes are similar so\ngo ahead and recommend what John has\nwatched but Aman has not watched so what\nwill be the recommendation going to Aman\nJames Bond movie and Spider-Man movie\nOkay now imagine this is a large metric\nof large users and large items so it\nwill be seen which users tastes are\nsimilar to each other okay and then\nthe other user which has not watched\nthat movie will be recommended the\nmovies or series based on the similar\nusers watching history okay this is\npretty simple but powerful technique\nknown as collaborative filtering\nso let's revise once guys what all we\ndiscussed long discussion but very very\nfruitful for you to revise few\nfundamental concepts linear regression\ndecision tree random Forest data boost\ngradient boost for segregation we\ndiscussed classification I explained you\nlogistic regression how svm works and\nhow k n works and I explained you two\nunsupervised technique came instant\ncollaborative filtering now not in too\nmuch detail I went because it's not\npossible to go in all the details of 10\nalgorithms in short time but read this\nas a refresher and please go ahead and\nclick all the links of whichever\nalgorithm you are more interested in\nlearning all the videos are there on all\nfour data science okay\nI request you guys please press the like\nbutton if you like this video and please\npress the Subscribe button and the bell\nicon\nif you want me to create more videos\nlike this see you all in the next video\nguys wherever you are stay safe and take\ncare\n",
  "words": [
    "interview",
    "coming",
    "want",
    "revise",
    "10",
    "important",
    "machine",
    "learning",
    "algorithms",
    "real",
    "quick",
    "find",
    "better",
    "video",
    "let",
    "go",
    "ahead",
    "revision",
    "10",
    "frequent",
    "used",
    "ml",
    "algorithms",
    "10",
    "algorithms",
    "going",
    "explain",
    "work",
    "pros",
    "cons",
    "okay",
    "see",
    "first",
    "five",
    "algorithms",
    "one",
    "color",
    "next",
    "three",
    "different",
    "color",
    "last",
    "two",
    "different",
    "color",
    "reason",
    "guys",
    "tell",
    "moment",
    "let",
    "try",
    "answer",
    "two",
    "basic",
    "questions",
    "okay",
    "let",
    "try",
    "answer",
    "machine",
    "learning",
    "algorithms",
    "okay",
    "start",
    "definition",
    "give",
    "one",
    "simple",
    "example",
    "suppose",
    "want",
    "travel",
    "bangalore",
    "hyderabad",
    "okay",
    "want",
    "go",
    "want",
    "go",
    "bangalore",
    "hyderabad",
    "either",
    "take",
    "train",
    "either",
    "take",
    "flight",
    "take",
    "bus",
    "well",
    "maybe",
    "drive",
    "car",
    "well",
    "okay",
    "two",
    "things",
    "understand",
    "guys",
    "task",
    "okay",
    "approach",
    "fine",
    "task",
    "hand",
    "go",
    "bangalore",
    "hyderabad",
    "okay",
    "approach",
    "three",
    "options",
    "told",
    "related",
    "world",
    "machine",
    "learning",
    "machine",
    "learning",
    "task",
    "different",
    "kinds",
    "tasks",
    "okay",
    "example",
    "regression",
    "task",
    "okay",
    "classification",
    "task",
    "okay",
    "unsupervised",
    "learning",
    "problem",
    "write",
    "unsupervised",
    "okay",
    "approach",
    "section",
    "different",
    "different",
    "approaches",
    "based",
    "solving",
    "regression",
    "problem",
    "solving",
    "classification",
    "solving",
    "particular",
    "case",
    "unsupervised",
    "learning",
    "okay",
    "regression",
    "also",
    "take",
    "many",
    "approaches",
    "example",
    "regression",
    "one",
    "approach",
    "regression",
    "take",
    "approach",
    "one",
    "approach",
    "two",
    "approach",
    "3",
    "approach",
    "4",
    "approach",
    "five",
    "classification",
    "take",
    "approach",
    "approach",
    "approach",
    "unsupervised",
    "also",
    "take",
    "multiple",
    "approaches",
    "color",
    "coding",
    "first",
    "five",
    "algorithms",
    "see",
    "solve",
    "explain",
    "regression",
    "use",
    "case",
    "okay",
    "take",
    "regression",
    "use",
    "case",
    "try",
    "understand",
    "solve",
    "using",
    "five",
    "algorithms",
    "okay",
    "next",
    "three",
    "see",
    "going",
    "explain",
    "classification",
    "use",
    "case",
    "approaches",
    "classification",
    "problem",
    "okay",
    "last",
    "two",
    "going",
    "explain",
    "unsupervised",
    "learning",
    "problem",
    "algorithms",
    "used",
    "solve",
    "unsupervised",
    "learning",
    "problem",
    "okay",
    "let",
    "go",
    "ahead",
    "guys",
    "try",
    "understand",
    "simple",
    "input",
    "data",
    "taken",
    "sample",
    "input",
    "data",
    "let",
    "without",
    "delay",
    "start",
    "first",
    "algorithm",
    "known",
    "linear",
    "regression",
    "machine",
    "learning",
    "learning",
    "pattern",
    "data",
    "using",
    "algorithms",
    "okay",
    "using",
    "algorithm",
    "known",
    "linear",
    "regression",
    "happen",
    "let",
    "try",
    "understand",
    "first",
    "algorithm",
    "list",
    "linear",
    "regression",
    "okay",
    "suppose",
    "employee",
    "data",
    "organization",
    "age",
    "column",
    "salary",
    "column",
    "fine",
    "22",
    "years",
    "person",
    "earns",
    "23",
    "000",
    "forth",
    "suppose",
    "using",
    "linear",
    "regression",
    "approach",
    "solve",
    "regression",
    "problem",
    "told",
    "first",
    "five",
    "problems",
    "regression",
    "problems",
    "first",
    "five",
    "algorithms",
    "understand",
    "using",
    "regression",
    "problem",
    "okay",
    "come",
    "data",
    "linear",
    "regression",
    "take",
    "data",
    "see",
    "data",
    "plotted",
    "xy",
    "plane",
    "like",
    "example",
    "one",
    "axis",
    "take",
    "salary",
    "okay",
    "axis",
    "x",
    "axis",
    "take",
    "edge",
    "okay",
    "roughly",
    "pointing",
    "points",
    "okay",
    "first",
    "point",
    "22",
    "23",
    "000",
    "maybe",
    "come",
    "somewhere",
    "x",
    "axis",
    "put",
    "h",
    "axis",
    "salary",
    "putting",
    "second",
    "data",
    "point",
    "come",
    "somewhere",
    "let",
    "say",
    "41",
    "80",
    "000",
    "data",
    "points",
    "third",
    "data",
    "point",
    "58",
    "150k",
    "data",
    "point",
    "come",
    "maybe",
    "somewhere",
    "say",
    "okay",
    "linear",
    "regression",
    "try",
    "plot",
    "line",
    "okay",
    "ideally",
    "assumption",
    "points",
    "fall",
    "line",
    "line",
    "like",
    "plotted",
    "line",
    "like",
    "plotted",
    "assumption",
    "ideally",
    "ideal",
    "world",
    "points",
    "fall",
    "line",
    "never",
    "happen",
    "real",
    "world",
    "logistic",
    "linear",
    "regression",
    "try",
    "fit",
    "something",
    "known",
    "best",
    "fit",
    "line",
    "okay",
    "best",
    "fit",
    "line",
    "let",
    "assume",
    "best",
    "fit",
    "line",
    "computed",
    "try",
    "minimize",
    "distance",
    "points",
    "together",
    "distance",
    "point",
    "distance",
    "point",
    "parallel",
    "axis",
    "distance",
    "point",
    "okay",
    "call",
    "even",
    "call",
    "e2",
    "call",
    "e3",
    "okay",
    "linear",
    "regression",
    "try",
    "minimize",
    "even",
    "square",
    "plus",
    "e2",
    "square",
    "plus",
    "e3",
    "square",
    "whichever",
    "line",
    "finds",
    "minimum",
    "even",
    "square",
    "e2",
    "square",
    "e3",
    "square",
    "call",
    "line",
    "model",
    "okay",
    "call",
    "line",
    "model",
    "know",
    "normal",
    "understanding",
    "mathematics",
    "straight",
    "line",
    "equation",
    "form",
    "mostly",
    "simplest",
    "write",
    "equal",
    "mx",
    "plus",
    "c",
    "right",
    "case",
    "say",
    "salary",
    "equal",
    "times",
    "h",
    "times",
    "h",
    "multiplication",
    "plus",
    "c",
    "c",
    "intercept",
    "let",
    "give",
    "number",
    "random",
    "number",
    "give",
    "let",
    "say",
    "2000",
    "okay",
    "imagine",
    "line",
    "model",
    "linear",
    "regression",
    "formula",
    "okay",
    "next",
    "question",
    "comes",
    "tomorrow",
    "pattern",
    "learned",
    "new",
    "age",
    "comes",
    "let",
    "say",
    "age",
    "salary",
    "person",
    "simple",
    "model",
    "come",
    "put",
    "numbers",
    "example",
    "put",
    "number",
    "let",
    "say",
    "age",
    "50",
    "salary",
    "intercept",
    "2000",
    "whatever",
    "calculation",
    "comes",
    "prediction",
    "salary",
    "50",
    "okay",
    "simple",
    "simple",
    "mathematical",
    "model",
    "assumption",
    "linear",
    "relation",
    "independent",
    "variable",
    "target",
    "variable",
    "okay",
    "assumption",
    "try",
    "plot",
    "line",
    "call",
    "best",
    "fit",
    "line",
    "wherever",
    "finds",
    "value",
    "minimum",
    "best",
    "fit",
    "line",
    "comes",
    "prediction",
    "happens",
    "like",
    "okay",
    "obviously",
    "pros",
    "cons",
    "algorithms",
    "models",
    "pros",
    "cons",
    "linear",
    "regression",
    "pluses",
    "pros",
    "model",
    "simple",
    "understand",
    "model",
    "mathematical",
    "model",
    "explain",
    "someone",
    "cons",
    "um",
    "necessary",
    "data",
    "always",
    "simple",
    "fit",
    "line",
    "right",
    "close",
    "line",
    "simple",
    "model",
    "hence",
    "lot",
    "real",
    "world",
    "problems",
    "may",
    "difficult",
    "solve",
    "simple",
    "linear",
    "regression",
    "varieties",
    "linear",
    "regression",
    "um",
    "created",
    "videos",
    "watch",
    "videos",
    "simply",
    "linear",
    "regression",
    "works",
    "like",
    "okay",
    "one",
    "first",
    "approach",
    "first",
    "approach",
    "means",
    "first",
    "algorithm",
    "let",
    "go",
    "ahead",
    "try",
    "see",
    "decision",
    "tree",
    "approach",
    "problem",
    "okay",
    "decision",
    "tree",
    "approach",
    "problem",
    "give",
    "data",
    "okay",
    "give",
    "data",
    "decision",
    "tree",
    "ask",
    "hey",
    "learn",
    "pattern",
    "data",
    "decision",
    "tree",
    "try",
    "break",
    "data",
    "break",
    "data",
    "create",
    "rule",
    "like",
    "okay",
    "write",
    "rule",
    "example",
    "say",
    "less",
    "equals",
    "30",
    "rule",
    "okay",
    "records",
    "satisfy",
    "rule",
    "okay",
    "records",
    "satisfy",
    "records",
    "satisfy",
    "way",
    "data",
    "break",
    "okay",
    "come",
    "less",
    "30",
    "many",
    "records",
    "one",
    "record",
    "30",
    "two",
    "records",
    "many",
    "records",
    "come",
    "side",
    "one",
    "record",
    "come",
    "okay",
    "let",
    "say",
    "record",
    "write",
    "wrong",
    "numbers",
    "22",
    "23k",
    "4180k",
    "write",
    "22",
    "23",
    "k",
    "write",
    "41",
    "80k",
    "okay",
    "one",
    "record",
    "let",
    "take",
    "numbers",
    "58",
    "150k",
    "58",
    "150k",
    "understand",
    "carefully",
    "guys",
    "next",
    "next",
    "algorithms",
    "base",
    "okay",
    "decision",
    "tree",
    "split",
    "data",
    "like",
    "total",
    "many",
    "records",
    "beginning",
    "three",
    "records",
    "many",
    "records",
    "one",
    "record",
    "many",
    "records",
    "two",
    "records",
    "okay",
    "first",
    "level",
    "split",
    "definitely",
    "split",
    "one",
    "time",
    "okay",
    "tree",
    "make",
    "limited",
    "number",
    "records",
    "imagine",
    "records",
    "one",
    "split",
    "saying",
    "know",
    "another",
    "filter",
    "maybe",
    "less",
    "40",
    "something",
    "like",
    "okay",
    "take",
    "make",
    "tree",
    "complex",
    "okay",
    "model",
    "breaking",
    "data",
    "based",
    "conditions",
    "nothing",
    "model",
    "somebody",
    "asks",
    "model",
    "decision",
    "tree",
    "model",
    "important",
    "question",
    "suppose",
    "tomorrow",
    "somebody",
    "comes",
    "asks",
    "person",
    "age",
    "50",
    "prediction",
    "person",
    "age",
    "50",
    "prediction",
    "important",
    "concept",
    "understand",
    "guys",
    "decision",
    "tree",
    "come",
    "check",
    "age",
    "50",
    "okay",
    "age",
    "50",
    "come",
    "category",
    "come",
    "line",
    "okay",
    "line",
    "many",
    "records",
    "two",
    "records",
    "decision",
    "tree",
    "go",
    "ahead",
    "take",
    "average",
    "two",
    "salaries",
    "age",
    "50",
    "prediction",
    "prediction",
    "guys",
    "age",
    "50",
    "prediction",
    "80k",
    "plus",
    "150k",
    "divided",
    "okay",
    "decision",
    "tree",
    "making",
    "prediction",
    "suppose",
    "ask",
    "entry",
    "hey",
    "salary",
    "person",
    "age",
    "21",
    "go",
    "right",
    "hand",
    "side",
    "go",
    "left",
    "hand",
    "side",
    "tree",
    "branch",
    "go",
    "directly",
    "say",
    "23k",
    "case",
    "one",
    "record",
    "suppose",
    "two",
    "records",
    "take",
    "average",
    "okay",
    "see",
    "two",
    "approaches",
    "different",
    "solving",
    "regression",
    "problem",
    "mathematical",
    "line",
    "fit",
    "decision",
    "tree",
    "know",
    "data",
    "broken",
    "multiple",
    "pieces",
    "prediction",
    "made",
    "okay",
    "remember",
    "guys",
    "decision",
    "tree",
    "based",
    "many",
    "advanced",
    "algorithms",
    "third",
    "algorithm",
    "list",
    "something",
    "non",
    "random",
    "forest",
    "okay",
    "random",
    "forest",
    "random",
    "forest",
    "say",
    "decision",
    "tree",
    "okay",
    "done",
    "good",
    "job",
    "uh",
    "chances",
    "overfitting",
    "data",
    "discuss",
    "pros",
    "cons",
    "process",
    "simple",
    "model",
    "know",
    "need",
    "lot",
    "mathematics",
    "etc",
    "cons",
    "chances",
    "overfitting",
    "know",
    "little",
    "change",
    "data",
    "model",
    "may",
    "change",
    "totally",
    "risk",
    "decision",
    "tree",
    "overfitting",
    "random",
    "forest",
    "come",
    "say",
    "hey",
    "taking",
    "right",
    "approach",
    "chances",
    "overfitting",
    "fit",
    "multiple",
    "trees",
    "random",
    "forest",
    "come",
    "create",
    "multiple",
    "trees",
    "tree",
    "one",
    "okay",
    "like",
    "way",
    "saw",
    "decision",
    "tree",
    "example",
    "tree",
    "one",
    "okay",
    "example",
    "tree",
    "two",
    "okay",
    "similarly",
    "n",
    "number",
    "trees",
    "okay",
    "similarly",
    "n",
    "number",
    "trees",
    "call",
    "t1",
    "call",
    "t2",
    "know",
    "500",
    "trees",
    "example",
    "random",
    "forest",
    "say",
    "two",
    "deficiently",
    "hey",
    "fitting",
    "one",
    "tree",
    "chance",
    "result",
    "biased",
    "chance",
    "overfitting",
    "chance",
    "model",
    "stable",
    "fit",
    "500",
    "trees",
    "okay",
    "make",
    "prediction",
    "important",
    "understand",
    "guys",
    "prediction",
    "random",
    "forest",
    "average",
    "prediction",
    "example",
    "trying",
    "predict",
    "age",
    "50",
    "right",
    "age",
    "50",
    "salary",
    "trying",
    "predict",
    "okay",
    "random",
    "forest",
    "take",
    "prediction",
    "tree",
    "one",
    "plus",
    "prediction",
    "3",
    "plus",
    "prediction",
    "tree",
    "500",
    "okay",
    "take",
    "predictions",
    "take",
    "average",
    "thing",
    "trying",
    "achieve",
    "suppose",
    "one",
    "decision",
    "tree",
    "tree",
    "overfitting",
    "performing",
    "well",
    "biased",
    "okay",
    "may",
    "happen",
    "diffusion",
    "trees",
    "since",
    "taking",
    "feedback",
    "500",
    "different",
    "trees",
    "overfitting",
    "problem",
    "model",
    "stability",
    "problem",
    "may",
    "okay",
    "random",
    "forest",
    "different",
    "decision",
    "tree",
    "remember",
    "individual",
    "trees",
    "using",
    "data",
    "example",
    "suppose",
    "data",
    "one",
    "thousand",
    "rows",
    "10",
    "columns",
    "okay",
    "example",
    "giving",
    "trees",
    "use",
    "necessarily",
    "records",
    "may",
    "possible",
    "tree",
    "one",
    "using",
    "100",
    "records",
    "three",
    "columns",
    "randomly",
    "selected",
    "three",
    "two",
    "t2",
    "using",
    "three",
    "two",
    "hundred",
    "records",
    "three",
    "columns",
    "randomly",
    "selected",
    "okay",
    "advantage",
    "random",
    "forest",
    "trees",
    "may",
    "learn",
    "different",
    "kind",
    "pattern",
    "take",
    "aggregated",
    "result",
    "flavors",
    "okay",
    "kind",
    "learning",
    "explained",
    "known",
    "sample",
    "learning",
    "okay",
    "remember",
    "guys",
    "unfold",
    "data",
    "science",
    "find",
    "big",
    "playlist",
    "explaining",
    "algorithms",
    "ensemble",
    "learning",
    "detail",
    "paste",
    "link",
    "description",
    "must",
    "check",
    "confusion",
    "simple",
    "learning",
    "works",
    "okay",
    "ensemble",
    "learning",
    "happened",
    "random",
    "forest",
    "known",
    "parallel",
    "way",
    "learning",
    "okay",
    "parallel",
    "way",
    "learning",
    "parallel",
    "way",
    "learning",
    "parallel",
    "way",
    "learning",
    "guys",
    "tree",
    "one",
    "three",
    "two",
    "three",
    "three",
    "independent",
    "call",
    "random",
    "forest",
    "model",
    "31",
    "start",
    "building",
    "taking",
    "sub",
    "sample",
    "data",
    "3",
    "2",
    "start",
    "building",
    "taking",
    "subsample",
    "data",
    "dependent",
    "okay",
    "things",
    "happen",
    "parallely",
    "hence",
    "call",
    "parallel",
    "learning",
    "question",
    "another",
    "way",
    "learning",
    "ensemble",
    "yes",
    "comes",
    "next",
    "algorithm",
    "known",
    "add",
    "boost",
    "okay",
    "ada",
    "boost",
    "standing",
    "adaptive",
    "boosting",
    "ada",
    "boost",
    "let",
    "write",
    "data",
    "let",
    "write",
    "data",
    "one",
    "time",
    "may",
    "writing",
    "different",
    "numbers",
    "important",
    "understanding",
    "concept",
    "important",
    "okay",
    "42",
    "write",
    "50",
    "000",
    "let",
    "say",
    "58",
    "write",
    "150",
    "000",
    "example",
    "input",
    "data",
    "boosting",
    "another",
    "technique",
    "boosting",
    "another",
    "technique",
    "ensemble",
    "category",
    "okay",
    "boosting",
    "especially",
    "boost",
    "happen",
    "assign",
    "weight",
    "observations",
    "okay",
    "suppose",
    "original",
    "data",
    "training",
    "salary",
    "target",
    "column",
    "initial",
    "weights",
    "initial",
    "weights",
    "okay",
    "initial",
    "weights",
    "weight",
    "records",
    "example",
    "three",
    "records",
    "one",
    "three",
    "saying",
    "one",
    "three",
    "saying",
    "one",
    "three",
    "saying",
    "rows",
    "equally",
    "important",
    "okay",
    "try",
    "understand",
    "concept",
    "guys",
    "ada",
    "boost",
    "beginning",
    "first",
    "iteration",
    "rows",
    "equally",
    "important",
    "okay",
    "ada",
    "boost",
    "works",
    "name",
    "adaptive",
    "adapts",
    "mistakes",
    "previous",
    "model",
    "saying",
    "previous",
    "model",
    "next",
    "model",
    "one",
    "thing",
    "always",
    "remember",
    "boost",
    "sequential",
    "learning",
    "process",
    "remember",
    "told",
    "random",
    "forest",
    "parallel",
    "learning",
    "process",
    "random",
    "forest",
    "tree",
    "one",
    "three",
    "two",
    "independent",
    "okay",
    "take",
    "sub",
    "sample",
    "create",
    "take",
    "sub",
    "sample",
    "create",
    "nothing",
    "adoboost",
    "boosting",
    "techniques",
    "sequential",
    "model",
    "multiple",
    "models",
    "multiple",
    "models",
    "fitted",
    "data",
    "tell",
    "moment",
    "models",
    "model",
    "1",
    "model",
    "2",
    "model",
    "3",
    "model",
    "4",
    "forth",
    "many",
    "ever",
    "model",
    "comes",
    "happen",
    "parallely",
    "okay",
    "happen",
    "sequence",
    "important",
    "thing",
    "understand",
    "sequence",
    "generated",
    "okay",
    "happen",
    "model",
    "one",
    "think",
    "base",
    "model",
    "model",
    "one",
    "think",
    "base",
    "model",
    "remember",
    "ada",
    "boost",
    "decision",
    "trees",
    "look",
    "like",
    "stumps",
    "stumps",
    "means",
    "tree",
    "like",
    "another",
    "tree",
    "like",
    "depth",
    "tree",
    "beyond",
    "one",
    "level",
    "okay",
    "called",
    "stumps",
    "language",
    "machine",
    "learning",
    "multiple",
    "stems",
    "created",
    "suppose",
    "model",
    "1",
    "first",
    "stump",
    "model",
    "one",
    "guys",
    "first",
    "stump",
    "okay",
    "model",
    "one",
    "comes",
    "make",
    "prediction",
    "salary",
    "model",
    "one",
    "comes",
    "make",
    "predictions",
    "salary",
    "okay",
    "another",
    "column",
    "called",
    "salary",
    "underscore",
    "prediction",
    "prediction",
    "comes",
    "prediction",
    "comes",
    "model",
    "one",
    "first",
    "model",
    "okay",
    "obviously",
    "mistakes",
    "22",
    "000",
    "may",
    "said",
    "21",
    "900",
    "50",
    "150",
    "said",
    "50",
    "said",
    "let",
    "say",
    "52",
    "000",
    "okay",
    "150",
    "said",
    "let",
    "say",
    "two",
    "hundred",
    "thousand",
    "based",
    "first",
    "model",
    "first",
    "decision",
    "tree",
    "creating",
    "calling",
    "system",
    "differences",
    "actual",
    "predicted",
    "residual",
    "coming",
    "residual",
    "means",
    "errors",
    "right",
    "residual",
    "means",
    "errors",
    "okay",
    "errors",
    "21",
    "900",
    "minus",
    "22",
    "000",
    "right",
    "example",
    "say",
    "hundred",
    "actual",
    "minus",
    "predicted",
    "minus",
    "two",
    "thousand",
    "minus",
    "minus",
    "50",
    "000",
    "put",
    "okay",
    "errors",
    "actual",
    "values",
    "first",
    "model",
    "predicts",
    "right",
    "errors",
    "first",
    "model",
    "okay",
    "twenty",
    "two",
    "thousand",
    "minus",
    "twenty",
    "one",
    "nine",
    "hundred",
    "one",
    "hundred",
    "forth",
    "initial",
    "weights",
    "okay",
    "happen",
    "next",
    "model",
    "m2",
    "fitted",
    "right",
    "initial",
    "weights",
    "changed",
    "preference",
    "given",
    "observations",
    "residuals",
    "okay",
    "repeating",
    "one",
    "time",
    "guys",
    "m1",
    "predict",
    "residuals",
    "errors",
    "come",
    "m2",
    "trained",
    "right",
    "weights",
    "three",
    "records",
    "rather",
    "weight",
    "increased",
    "getting",
    "errors",
    "weight",
    "decreased",
    "getting",
    "less",
    "error",
    "okay",
    "forth",
    "m2",
    "come",
    "compute",
    "create",
    "residual",
    "weights",
    "adjusted",
    "m3",
    "come",
    "predict",
    "residual",
    "calculated",
    "weights",
    "adjusted",
    "finally",
    "get",
    "combination",
    "final",
    "model",
    "final",
    "model",
    "combination",
    "base",
    "model",
    "calling",
    "first",
    "model",
    "okay",
    "plus",
    "m1",
    "plus",
    "m2",
    "plus",
    "m3",
    "plus",
    "forth",
    "remember",
    "mathematical",
    "equation",
    "indicative",
    "equation",
    "giving",
    "okay",
    "want",
    "understand",
    "mathematics",
    "behind",
    "please",
    "go",
    "ahead",
    "click",
    "link",
    "giving",
    "description",
    "okay",
    "things",
    "equal",
    "say",
    "final",
    "output",
    "say",
    "also",
    "different",
    "final",
    "output",
    "example",
    "random",
    "forest",
    "saw",
    "models",
    "equal",
    "c",
    "final",
    "output",
    "dividing",
    "500",
    "okay",
    "models",
    "equal",
    "say",
    "equal",
    "say",
    "okay",
    "let",
    "move",
    "ahead",
    "another",
    "pros",
    "cons",
    "model",
    "model",
    "give",
    "may",
    "give",
    "better",
    "result",
    "models",
    "adapting",
    "changes",
    "larger",
    "data",
    "side",
    "may",
    "may",
    "need",
    "resources",
    "train",
    "also",
    "one",
    "kind",
    "black",
    "box",
    "model",
    "kind",
    "black",
    "box",
    "model",
    "means",
    "much",
    "explanation",
    "going",
    "inside",
    "apart",
    "hyper",
    "parameters",
    "okay",
    "let",
    "move",
    "ahead",
    "last",
    "algorithm",
    "integration",
    "category",
    "known",
    "gradient",
    "boost",
    "okay",
    "last",
    "algorithm",
    "integration",
    "category",
    "gradient",
    "boost",
    "remember",
    "guys",
    "algorithms",
    "explaining",
    "taken",
    "anything",
    "used",
    "less",
    "used",
    "okay",
    "take",
    "simple",
    "data",
    "age",
    "salary",
    "21",
    "salary",
    "let",
    "say",
    "20k",
    "40",
    "salaries",
    "let",
    "say",
    "42k",
    "58",
    "salary",
    "let",
    "say",
    "60k",
    "input",
    "data",
    "want",
    "run",
    "gradient",
    "boost",
    "happen",
    "understand",
    "guys",
    "sequential",
    "learning",
    "parallel",
    "learning",
    "okay",
    "base",
    "prediction",
    "data",
    "base",
    "prediction",
    "okay",
    "base",
    "prediction",
    "base",
    "prediction",
    "guys",
    "base",
    "prediction",
    "nothing",
    "kind",
    "dumb",
    "model",
    "assume",
    "guys",
    "average",
    "know",
    "three",
    "records",
    "average",
    "uh",
    "80",
    "plus",
    "42",
    "80",
    "plus",
    "42",
    "divided",
    "3",
    "right",
    "2",
    "1",
    "1",
    "right",
    "let",
    "say",
    "assume",
    "simplicity",
    "36k",
    "okay",
    "base",
    "prediction",
    "put",
    "36k",
    "36k",
    "36k",
    "one",
    "base",
    "prediction",
    "comes",
    "residual",
    "computed",
    "okay",
    "residual",
    "difference",
    "actual",
    "predicted",
    "values",
    "whatever",
    "numbers",
    "fine",
    "comes",
    "interesting",
    "part",
    "gradient",
    "boost",
    "different",
    "ada",
    "boost",
    "algorithms",
    "gradient",
    "boost",
    "try",
    "fit",
    "model",
    "residual",
    "okay",
    "try",
    "fit",
    "model",
    "residual",
    "try",
    "minimize",
    "residuals",
    "called",
    "base",
    "model",
    "okay",
    "next",
    "model",
    "call",
    "residual",
    "model",
    "one",
    "okay",
    "next",
    "model",
    "call",
    "residual",
    "model",
    "2",
    "forth",
    "okay",
    "happen",
    "residuals",
    "computed",
    "whatever",
    "residual",
    "comes",
    "based",
    "base",
    "prediction",
    "updated",
    "example",
    "let",
    "say",
    "residual",
    "much",
    "20",
    "minus",
    "36",
    "minus",
    "16",
    "residual",
    "right",
    "act",
    "independent",
    "column",
    "residual",
    "act",
    "target",
    "column",
    "let",
    "say",
    "prediction",
    "minus",
    "16",
    "comes",
    "let",
    "say",
    "minus",
    "happen",
    "base",
    "prediction",
    "get",
    "updated",
    "base",
    "prediction",
    "get",
    "updated",
    "complicated",
    "model",
    "want",
    "understand",
    "details",
    "links",
    "description",
    "please",
    "click",
    "clear",
    "okay",
    "happen",
    "base",
    "model",
    "plus",
    "residual",
    "model",
    "1",
    "plus",
    "residual",
    "model",
    "2",
    "forth",
    "parameters",
    "assign",
    "weight",
    "models",
    "say",
    "models",
    "equal",
    "vote",
    "final",
    "output",
    "different",
    "votes",
    "fine",
    "gradient",
    "boost",
    "one",
    "famous",
    "algorithm",
    "winning",
    "kaggle",
    "competitions",
    "things",
    "gradient",
    "boost",
    "another",
    "variant",
    "gradient",
    "boost",
    "known",
    "xgb",
    "extreme",
    "gradient",
    "boost",
    "please",
    "go",
    "ahead",
    "read",
    "algorithm",
    "guys",
    "covering",
    "slight",
    "difference",
    "gradient",
    "boost",
    "sgb",
    "read",
    "well",
    "fine",
    "let",
    "move",
    "ahead",
    "second",
    "category",
    "algorithms",
    "known",
    "classification",
    "algorithms",
    "classification",
    "algorithms",
    "first",
    "algorithm",
    "going",
    "cover",
    "logistic",
    "regression",
    "important",
    "guys",
    "please",
    "pay",
    "attention",
    "try",
    "understand",
    "logistic",
    "regression",
    "going",
    "work",
    "given",
    "scenario",
    "mathematical",
    "model",
    "hence",
    "important",
    "understand",
    "okay",
    "suppose",
    "employee",
    "data",
    "21",
    "22k",
    "whether",
    "employee",
    "leaves",
    "organization",
    "leave",
    "organization",
    "saying",
    "1",
    "0",
    "okay",
    "40",
    "year",
    "guy",
    "makes",
    "let",
    "say",
    "42k",
    "leave",
    "0",
    "58",
    "year",
    "guy",
    "makes",
    "let",
    "say",
    "60k",
    "example",
    "leaves",
    "know",
    "one",
    "classification",
    "problem",
    "trying",
    "predict",
    "whether",
    "employee",
    "leave",
    "organization",
    "leave",
    "organization",
    "last",
    "column",
    "see",
    "target",
    "column",
    "last",
    "column",
    "see",
    "target",
    "column",
    "type",
    "problem",
    "called",
    "classification",
    "problem",
    "objective",
    "model",
    "tomorrow",
    "give",
    "age",
    "employee",
    "example",
    "31",
    "salary",
    "employee",
    "example",
    "34k",
    "asked",
    "model",
    "hey",
    "guy",
    "leave",
    "leave",
    "organization",
    "okay",
    "classification",
    "problem",
    "logistic",
    "regression",
    "take",
    "problem",
    "understand",
    "mathematical",
    "concepts",
    "see",
    "target",
    "column",
    "1",
    "0",
    "either",
    "one",
    "zero",
    "one",
    "zero",
    "okay",
    "means",
    "target",
    "understand",
    "important",
    "concept",
    "guys",
    "either",
    "0",
    "1",
    "anything",
    "else",
    "target",
    "anything",
    "else",
    "apart",
    "0",
    "1",
    "age",
    "salary",
    "take",
    "real",
    "number",
    "x",
    "value",
    "minus",
    "infinity",
    "plus",
    "infinity",
    "right",
    "x",
    "value",
    "minus",
    "infinity",
    "2",
    "plus",
    "infinity",
    "0",
    "1",
    "okay",
    "understand",
    "somehow",
    "create",
    "relation",
    "enable",
    "us",
    "predict",
    "given",
    "x",
    "okay",
    "problem",
    "left",
    "hand",
    "side",
    "minus",
    "infinity",
    "plus",
    "infinity",
    "range",
    "x",
    "range",
    "okay",
    "write",
    "x",
    "x",
    "means",
    "independent",
    "features",
    "right",
    "hand",
    "side",
    "values",
    "0",
    "1",
    "0",
    "1",
    "0",
    "1",
    "okay",
    "directly",
    "predict",
    "rather",
    "predict",
    "something",
    "else",
    "something",
    "else",
    "predict",
    "place",
    "predicting",
    "predict",
    "probabilities",
    "okay",
    "probabilities",
    "observation",
    "falling",
    "probabilities",
    "l",
    "e",
    "okay",
    "predict",
    "probabilities",
    "range",
    "0",
    "1",
    "know",
    "probability",
    "take",
    "range",
    "0",
    "1",
    "okay",
    "range",
    "also",
    "looking",
    "minus",
    "infinity",
    "plus",
    "infinity",
    "one",
    "transformation",
    "make",
    "odds",
    "range",
    "odds",
    "0",
    "infinity",
    "okay",
    "still",
    "minus",
    "infinity",
    "2",
    "plus",
    "infinity",
    "range",
    "take",
    "log",
    "odds",
    "okay",
    "log",
    "odds",
    "okay",
    "range",
    "become",
    "minus",
    "infinity",
    "plus",
    "infinity",
    "equation",
    "look",
    "like",
    "say",
    "log",
    "odds",
    "right",
    "right",
    "hand",
    "side",
    "log",
    "p",
    "1",
    "minus",
    "p",
    "okay",
    "left",
    "hand",
    "side",
    "beta",
    "node",
    "plus",
    "beta",
    "1",
    "x",
    "1",
    "plus",
    "forth",
    "okay",
    "equation",
    "see",
    "front",
    "called",
    "base",
    "equation",
    "logistic",
    "regression",
    "one",
    "important",
    "concept",
    "understand",
    "guys",
    "logic",
    "function",
    "okay",
    "inverse",
    "logit",
    "function",
    "h",
    "sigmoid",
    "function",
    "okay",
    "support",
    "suppose",
    "take",
    "inverse",
    "sigmoid",
    "happen",
    "apply",
    "sigma",
    "sides",
    "know",
    "sigmoid",
    "function",
    "sigmoid",
    "function",
    "f",
    "x",
    "looks",
    "like",
    "1",
    "1",
    "plus",
    "e",
    "power",
    "minus",
    "x",
    "sigmoid",
    "function",
    "xy",
    "plane",
    "look",
    "like",
    "suppose",
    "0",
    "1",
    "okay",
    "sigmat",
    "look",
    "like",
    "always",
    "0",
    "1",
    "okay",
    "logistic",
    "regression",
    "equation",
    "changed",
    "form",
    "sigmoid",
    "function",
    "f",
    "x",
    "p",
    "okay",
    "p",
    "look",
    "like",
    "take",
    "take",
    "sigmoid",
    "sides",
    "right",
    "right",
    "hand",
    "side",
    "p",
    "1",
    "1",
    "plus",
    "e",
    "power",
    "minus",
    "beta0",
    "plus",
    "beta",
    "1",
    "x",
    "1",
    "okay",
    "remember",
    "guys",
    "equation",
    "equation",
    "1",
    "equation",
    "equation",
    "equations",
    "difference",
    "logit",
    "equation",
    "sigmoid",
    "equation",
    "okay",
    "take",
    "take",
    "inverse",
    "logit",
    "nothing",
    "sigmoid",
    "okay",
    "understand",
    "carefully",
    "equation",
    "example",
    "written",
    "1",
    "1",
    "plus",
    "e",
    "power",
    "minus",
    "beta",
    "0",
    "plus",
    "beta",
    "1",
    "age",
    "okay",
    "plus",
    "beta",
    "2",
    "salary",
    "nothing",
    "logistic",
    "regression",
    "equation",
    "okay",
    "know",
    "told",
    "sigmoid",
    "function",
    "output",
    "see",
    "output",
    "always",
    "0",
    "1",
    "means",
    "get",
    "probability",
    "say",
    "based",
    "probability",
    "say",
    "whether",
    "employee",
    "leaves",
    "leave",
    "okay",
    "logistic",
    "regression",
    "important",
    "easy",
    "understand",
    "concept",
    "okay",
    "see",
    "modeling",
    "categorical",
    "variable",
    "real",
    "numbers",
    "hence",
    "need",
    "certain",
    "transformations",
    "transformations",
    "need",
    "relates",
    "probability",
    "explained",
    "okay",
    "pros",
    "cons",
    "mathematical",
    "model",
    "difficult",
    "understand",
    "cons",
    "assumes",
    "lot",
    "things",
    "data",
    "may",
    "may",
    "correct",
    "hence",
    "may",
    "give",
    "great",
    "result",
    "time",
    "okay",
    "famous",
    "important",
    "algorithm",
    "understand",
    "next",
    "algorithm",
    "category",
    "classification",
    "category",
    "one",
    "simple",
    "one",
    "want",
    "cover",
    "known",
    "gear",
    "nearest",
    "neighbor",
    "okay",
    "pretty",
    "simple",
    "algorithm",
    "suppose",
    "data",
    "data",
    "want",
    "build",
    "k",
    "n",
    "algorithm",
    "okay",
    "since",
    "data",
    "explain",
    "happen",
    "plot",
    "plane",
    "like",
    "okay",
    "data",
    "one",
    "axis",
    "salary",
    "two",
    "access",
    "two",
    "axis",
    "predict",
    "okay",
    "let",
    "come",
    "age",
    "let",
    "say",
    "salary",
    "okay",
    "three",
    "employees",
    "let",
    "say",
    "one",
    "employees",
    "21",
    "22",
    "employees",
    "falls",
    "second",
    "employee",
    "40",
    "falls",
    "58",
    "falls",
    "somewhere",
    "okay",
    "k",
    "nearest",
    "neighbor",
    "try",
    "allocate",
    "neighbors",
    "individual",
    "observations",
    "example",
    "observation",
    "one",
    "observation",
    "two",
    "observation",
    "three",
    "okay",
    "one",
    "neighbors",
    "2",
    "neighbor",
    "3",
    "3",
    "neighbor",
    "two",
    "okay",
    "tomorrow",
    "prediction",
    "comes",
    "let",
    "say",
    "age",
    "50",
    "take",
    "50",
    "example",
    "50",
    "example",
    "try",
    "see",
    "take",
    "salary",
    "also",
    "case",
    "salary",
    "also",
    "salary",
    "let",
    "say",
    "61k",
    "okay",
    "try",
    "see",
    "fit",
    "58",
    "percent",
    "salary",
    "61k",
    "maybe",
    "nearest",
    "neighbor",
    "guy",
    "nearest",
    "neighbor",
    "guy",
    "may",
    "guy",
    "guy",
    "suppose",
    "new",
    "guy",
    "comes",
    "somewhere",
    "okay",
    "neighbors",
    "first",
    "neighbor",
    "second",
    "neighbor",
    "okay",
    "simply",
    "go",
    "ahead",
    "take",
    "know",
    "mode",
    "results",
    "example",
    "two",
    "guys",
    "second",
    "neighbors",
    "right",
    "mean",
    "two",
    "neighbors",
    "take",
    "0",
    "1",
    "maximum",
    "case",
    "mode",
    "data",
    "obviously",
    "take",
    "larger",
    "data",
    "modes",
    "data",
    "okay",
    "whichever",
    "mode",
    "example",
    "suppose",
    "30",
    "records",
    "20",
    "1",
    "10",
    "prediction",
    "guy",
    "whatever",
    "maximum",
    "whatever",
    "mode",
    "mode",
    "one",
    "zero",
    "whatever",
    "prediction",
    "k",
    "n",
    "okay",
    "told",
    "cannon",
    "pretty",
    "simple",
    "algorithm",
    "plot",
    "data",
    "try",
    "find",
    "nearest",
    "neighbors",
    "new",
    "observation",
    "comes",
    "give",
    "many",
    "many",
    "observer",
    "many",
    "neighbors",
    "want",
    "record",
    "create",
    "one",
    "based",
    "okay",
    "canon",
    "simple",
    "understand",
    "algorithm",
    "nothing",
    "complex",
    "covered",
    "quickly",
    "slide",
    "okay",
    "let",
    "try",
    "understand",
    "another",
    "classification",
    "technique",
    "known",
    "support",
    "vector",
    "machines",
    "svms",
    "svms",
    "plot",
    "data",
    "whatever",
    "axis",
    "suppose",
    "age",
    "one",
    "axis",
    "salary",
    "one",
    "axis",
    "okay",
    "data",
    "points",
    "take",
    "little",
    "data",
    "points",
    "okay",
    "data",
    "points",
    "look",
    "like",
    "data",
    "points",
    "data",
    "points",
    "okay",
    "sbm",
    "try",
    "create",
    "something",
    "known",
    "decision",
    "boundary",
    "okay",
    "decision",
    "boundary",
    "different",
    "linear",
    "regression",
    "decision",
    "boundary",
    "integration",
    "pure",
    "mathematical",
    "equation",
    "involved",
    "concept",
    "something",
    "known",
    "hyper",
    "plane",
    "okay",
    "example",
    "draw",
    "line",
    "right",
    "guys",
    "black",
    "guys",
    "think",
    "leaves",
    "target",
    "column",
    "one",
    "guys",
    "think",
    "leave",
    "target",
    "column",
    "zero",
    "leave",
    "okay",
    "suppose",
    "data",
    "like",
    "happen",
    "svm",
    "plot",
    "called",
    "language",
    "sbm",
    "called",
    "decision",
    "boundary",
    "okay",
    "decision",
    "boundary",
    "case",
    "data",
    "looks",
    "pretty",
    "simple",
    "pretty",
    "separated",
    "hence",
    "decision",
    "boundary",
    "simple",
    "line",
    "okay",
    "scenarios",
    "real",
    "world",
    "scenarios",
    "decision",
    "boundary",
    "simple",
    "okay",
    "black",
    "dots",
    "black",
    "circles",
    "okay",
    "cross",
    "blue",
    "cross",
    "side",
    "right",
    "case",
    "decision",
    "boundary",
    "justice",
    "decision",
    "boundary",
    "need",
    "change",
    "concept",
    "hyper",
    "planes",
    "kernels",
    "two",
    "important",
    "concept",
    "svm",
    "guys",
    "want",
    "explore",
    "sbm",
    "hyper",
    "planes",
    "okay",
    "kernels",
    "data",
    "become",
    "complex",
    "simple",
    "decision",
    "boundaries",
    "predict",
    "well",
    "okay",
    "need",
    "complex",
    "decision",
    "boundary",
    "hyperplane",
    "kernels",
    "concept",
    "come",
    "give",
    "idea",
    "svm",
    "works",
    "create",
    "decision",
    "boundary",
    "tomorrow",
    "prediction",
    "new",
    "result",
    "come",
    "example",
    "somebody",
    "asks",
    "um",
    "know",
    "person",
    "person",
    "age",
    "50",
    "salary",
    "60k",
    "whether",
    "person",
    "leave",
    "leave",
    "svm",
    "model",
    "see",
    "side",
    "decision",
    "boundary",
    "guy",
    "falling",
    "guy",
    "falls",
    "side",
    "decision",
    "boundary",
    "says",
    "leave",
    "guy",
    "falls",
    "side",
    "decision",
    "boundary",
    "says",
    "leaves",
    "okay",
    "svm",
    "remember",
    "concept",
    "decision",
    "boundaries",
    "hyper",
    "planes",
    "kernels",
    "kernel",
    "tricks",
    "okay",
    "covered",
    "three",
    "things",
    "classification",
    "scenarios",
    "five",
    "things",
    "regression",
    "scenarios",
    "let",
    "go",
    "ahead",
    "try",
    "see",
    "unsupervised",
    "learning",
    "problems",
    "okay",
    "meaning",
    "unsupervised",
    "learning",
    "till",
    "target",
    "column",
    "unsupervised",
    "learning",
    "may",
    "target",
    "column",
    "okay",
    "suppose",
    "employee",
    "data",
    "age",
    "salary",
    "somebody",
    "comes",
    "tells",
    "hey",
    "tell",
    "different",
    "buckets",
    "employees",
    "existing",
    "organization",
    "different",
    "buckets",
    "means",
    "people",
    "less",
    "age",
    "salary",
    "people",
    "endless",
    "salary",
    "different",
    "buckets",
    "somebody",
    "come",
    "ask",
    "okay",
    "solve",
    "problem",
    "using",
    "something",
    "knowledge",
    "clustering",
    "segmentation",
    "okay",
    "suppose",
    "task",
    "hand",
    "three",
    "records",
    "records",
    "right",
    "real",
    "world",
    "scenario",
    "interested",
    "knowing",
    "natural",
    "clusters",
    "organization",
    "organization",
    "data",
    "one",
    "axis",
    "axis",
    "salary",
    "okay",
    "multiple",
    "data",
    "points",
    "three",
    "data",
    "points",
    "plotting",
    "data",
    "points",
    "demonstration",
    "okay",
    "nothing",
    "predict",
    "employed",
    "interested",
    "knowing",
    "buckets",
    "means",
    "employees",
    "closer",
    "terms",
    "characteristics",
    "example",
    "employees",
    "closer",
    "call",
    "bucket",
    "one",
    "employees",
    "closer",
    "call",
    "bucket",
    "two",
    "segment",
    "okay",
    "implemented",
    "k",
    "means",
    "clustering",
    "one",
    "technique",
    "implementing",
    "bucketing",
    "k",
    "means",
    "clustering",
    "okay",
    "techniques",
    "also",
    "segmentation",
    "bucketing",
    "one",
    "technique",
    "k",
    "means",
    "clustering",
    "technique",
    "happen",
    "distance",
    "various",
    "employees",
    "computed",
    "example",
    "employee",
    "one",
    "employee",
    "two",
    "okay",
    "suppose",
    "ask",
    "similar",
    "employee",
    "one",
    "employee",
    "two",
    "different",
    "similarity",
    "metric",
    "compute",
    "example",
    "euclidean",
    "distance",
    "manhattan",
    "distance",
    "cosine",
    "similarity",
    "etc",
    "detailed",
    "video",
    "things",
    "well",
    "link",
    "description",
    "suppose",
    "tell",
    "simple",
    "uh",
    "know",
    "distance",
    "similar",
    "sim",
    "two",
    "employees",
    "similar",
    "different",
    "say",
    "21",
    "minus",
    "40",
    "whole",
    "square",
    "plus",
    "20k",
    "20k",
    "minus",
    "42k",
    "whole",
    "square",
    "dimensions",
    "taking",
    "distance",
    "squaring",
    "rooting",
    "called",
    "euclidean",
    "distance",
    "e1",
    "u2",
    "whatever",
    "number",
    "get",
    "okay",
    "suppose",
    "e1",
    "e2",
    "equilibrium",
    "distance",
    "less",
    "e1",
    "e3",
    "equilibrium",
    "distance",
    "case",
    "say",
    "e1",
    "e2",
    "closer",
    "okay",
    "similar",
    "way",
    "start",
    "finding",
    "employees",
    "closer",
    "call",
    "one",
    "bucket",
    "similarly",
    "score",
    "call",
    "another",
    "bucket",
    "okay",
    "remember",
    "explained",
    "simple",
    "terms",
    "important",
    "concept",
    "known",
    "centroid",
    "concept",
    "okay",
    "please",
    "go",
    "ahead",
    "watch",
    "unfold",
    "data",
    "science",
    "detailed",
    "video",
    "clustering",
    "understand",
    "details",
    "centroid",
    "defined",
    "algorithm",
    "works",
    "mathematical",
    "level",
    "okay",
    "link",
    "video",
    "please",
    "ensure",
    "watch",
    "clustering",
    "last",
    "least",
    "guys",
    "might",
    "seen",
    "amazon",
    "flipkart",
    "different",
    "different",
    "uh",
    "products",
    "recommended",
    "example",
    "buy",
    "laptop",
    "tell",
    "hey",
    "go",
    "ahead",
    "buy",
    "laptop",
    "bag",
    "well",
    "nothing",
    "recommendation",
    "okay",
    "netflix",
    "watch",
    "let",
    "say",
    "one",
    "movie",
    "one",
    "action",
    "movie",
    "let",
    "say",
    "watch",
    "mission",
    "impossible",
    "go",
    "recommend",
    "jack",
    "ryan",
    "series",
    "maybe",
    "okay",
    "called",
    "recommendation",
    "system",
    "running",
    "background",
    "okay",
    "system",
    "works",
    "one",
    "simple",
    "uh",
    "yet",
    "powerful",
    "technique",
    "recommender",
    "system",
    "known",
    "collaborative",
    "filtering",
    "collaborative",
    "filtering",
    "okay",
    "collaborative",
    "filtering",
    "take",
    "users",
    "okay",
    "users",
    "take",
    "items",
    "try",
    "understand",
    "simple",
    "concept",
    "edge",
    "pretty",
    "simple",
    "understand",
    "users",
    "month",
    "users",
    "john",
    "users",
    "okay",
    "items",
    "let",
    "say",
    "mission",
    "impossible",
    "atoms",
    "jack",
    "ryan",
    "atoms",
    "another",
    "movie",
    "james",
    "bond",
    "series",
    "atoms",
    "spiderman",
    "okay",
    "atoms",
    "comedy",
    "movies",
    "example",
    "home",
    "alone",
    "say",
    "okay",
    "aman",
    "movie",
    "aman",
    "watches",
    "movie",
    "aman",
    "watched",
    "example",
    "mission",
    "impossible",
    "aman",
    "watched",
    "jack",
    "ryan",
    "watched",
    "watched",
    "let",
    "say",
    "movie",
    "zero",
    "say",
    "okay",
    "james",
    "one",
    "movie",
    "movie",
    "uh",
    "watched",
    "okay",
    "movie",
    "another",
    "guy",
    "john",
    "watched",
    "mission",
    "impossible",
    "jack",
    "ryan",
    "james",
    "bond",
    "movie",
    "movie",
    "well",
    "another",
    "guy",
    "doe",
    "watched",
    "movies",
    "watched",
    "home",
    "alone",
    "comedy",
    "movie",
    "okay",
    "users",
    "similar",
    "user",
    "computed",
    "based",
    "one",
    "user",
    "similarity",
    "metric",
    "user",
    "similarity",
    "metric",
    "told",
    "cosine",
    "similarity",
    "different",
    "kind",
    "distance",
    "metric",
    "think",
    "common",
    "sense",
    "also",
    "aman",
    "watches",
    "action",
    "movies",
    "see",
    "john",
    "also",
    "watches",
    "action",
    "movies",
    "mission",
    "impossible",
    "jack",
    "ryan",
    "aman",
    "watched",
    "james",
    "one",
    "movie",
    "aman",
    "watched",
    "movie",
    "happen",
    "since",
    "aman",
    "jon",
    "similar",
    "go",
    "ahead",
    "watch",
    "movies",
    "jon",
    "watched",
    "aman",
    "watched",
    "aman",
    "jon",
    "tastes",
    "similar",
    "go",
    "ahead",
    "recommend",
    "john",
    "watched",
    "aman",
    "watched",
    "recommendation",
    "going",
    "aman",
    "james",
    "bond",
    "movie",
    "movie",
    "okay",
    "imagine",
    "large",
    "metric",
    "large",
    "users",
    "large",
    "items",
    "seen",
    "users",
    "tastes",
    "similar",
    "okay",
    "user",
    "watched",
    "movie",
    "recommended",
    "movies",
    "series",
    "based",
    "similar",
    "users",
    "watching",
    "history",
    "okay",
    "pretty",
    "simple",
    "powerful",
    "technique",
    "known",
    "collaborative",
    "filtering",
    "let",
    "revise",
    "guys",
    "discussed",
    "long",
    "discussion",
    "fruitful",
    "revise",
    "fundamental",
    "concepts",
    "linear",
    "regression",
    "decision",
    "tree",
    "random",
    "forest",
    "data",
    "boost",
    "gradient",
    "boost",
    "segregation",
    "discussed",
    "classification",
    "explained",
    "logistic",
    "regression",
    "svm",
    "works",
    "k",
    "n",
    "works",
    "explained",
    "two",
    "unsupervised",
    "technique",
    "came",
    "instant",
    "collaborative",
    "filtering",
    "much",
    "detail",
    "went",
    "possible",
    "go",
    "details",
    "10",
    "algorithms",
    "short",
    "time",
    "read",
    "refresher",
    "please",
    "go",
    "ahead",
    "click",
    "links",
    "whichever",
    "algorithm",
    "interested",
    "learning",
    "videos",
    "four",
    "data",
    "science",
    "okay",
    "request",
    "guys",
    "please",
    "press",
    "like",
    "button",
    "like",
    "video",
    "please",
    "press",
    "subscribe",
    "button",
    "bell",
    "icon",
    "want",
    "create",
    "videos",
    "like",
    "see",
    "next",
    "video",
    "guys",
    "wherever",
    "stay",
    "safe",
    "take",
    "care"
  ],
  "keywords": [
    "want",
    "10",
    "important",
    "machine",
    "learning",
    "algorithms",
    "real",
    "video",
    "let",
    "go",
    "ahead",
    "going",
    "explain",
    "pros",
    "cons",
    "okay",
    "see",
    "first",
    "five",
    "one",
    "next",
    "three",
    "different",
    "last",
    "two",
    "guys",
    "try",
    "give",
    "simple",
    "example",
    "suppose",
    "take",
    "well",
    "maybe",
    "things",
    "understand",
    "task",
    "approach",
    "hand",
    "told",
    "world",
    "regression",
    "classification",
    "unsupervised",
    "problem",
    "write",
    "based",
    "case",
    "also",
    "many",
    "3",
    "multiple",
    "solve",
    "using",
    "data",
    "algorithm",
    "known",
    "linear",
    "happen",
    "employee",
    "organization",
    "age",
    "column",
    "salary",
    "22",
    "person",
    "000",
    "forth",
    "come",
    "like",
    "axis",
    "x",
    "points",
    "point",
    "say",
    "58",
    "plot",
    "line",
    "logistic",
    "fit",
    "something",
    "distance",
    "parallel",
    "call",
    "square",
    "plus",
    "model",
    "know",
    "equation",
    "equal",
    "right",
    "number",
    "random",
    "comes",
    "numbers",
    "50",
    "whatever",
    "prediction",
    "mathematical",
    "target",
    "models",
    "hence",
    "may",
    "watch",
    "works",
    "means",
    "decision",
    "tree",
    "hey",
    "create",
    "less",
    "records",
    "way",
    "record",
    "side",
    "k",
    "base",
    "make",
    "saying",
    "another",
    "nothing",
    "concept",
    "category",
    "average",
    "21",
    "remember",
    "forest",
    "uh",
    "overfitting",
    "need",
    "trees",
    "predict",
    "kind",
    "2",
    "boost",
    "ada",
    "technique",
    "weights",
    "1",
    "look",
    "called",
    "residual",
    "errors",
    "minus",
    "final",
    "please",
    "output",
    "gradient",
    "leave",
    "0",
    "guy",
    "infinity",
    "range",
    "beta",
    "function",
    "sigmoid",
    "neighbor",
    "pretty",
    "employees",
    "neighbors",
    "boundary",
    "svm",
    "clustering",
    "similar",
    "movie",
    "users",
    "movies",
    "aman",
    "watched"
  ]
}