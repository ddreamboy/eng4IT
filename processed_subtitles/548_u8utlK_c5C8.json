{
  "text": "hello all my name is Krishna Graham\nwelcome to my youtube channel today in\nthis particular video we'll be\nunderstanding about recurrent neural\nnetwork architecture and you'll also\nunderstand the forward propagation over\ntime now why I am saying forward\npropagation over time because that is\nthe main functionality behind RNN behind\nthe working of the RNA so we'll try to\nunderstand what exactly it is guys if\nyou want to know about RN and first of\nall you need to know a general\narchitecture of RN and want exactly is\nan RN it so this is how an RN and\narchitecture looks like here this is my\ninput okay and this input may be of any\nnumber of dimensions okay any number of\ndimensions basically means I can have\nany number of features right and then\nthis is my hidden layer here I can have\nany number of hidden neurons okay and\nfinally I have my output now along with\nthis particular output we also get\noutput with respect to time that\nbasically means suppose I want to solve\na NLP use case which is of sentiment\nanalysis now in sentiment analysis we\nhave a sentence and we try to determine\nwhether that is a positive sentence or a\nnegative sentence or a positive review\nor a negative review so usually whenever\nI consider a sentence suppose this is my\nsentence 1 and inside the sentence\nsuppose I have four words like this food\nis bad ok and according to this I have\nmy output which will definitely say it\nas zero because this particular review\nis a negative review right now if I take\nthis particular example our RNN act each\nand every time like at t is equal to 1\nwill pre process one word then at t is\nequal to 2 will pre process the second\nword now what will happen as we are\nprocessing suppose in the first case\nwill pass one word right the pre\nprocessing will happen over here and\nthen it will give you an output and\napart from that for the next word right\nwhatever was the output for the first\nword that will also be sent to this\nparticular neuron and recurrent neural\nnetwork and because of this the sequence\ninformation is kept ok now I'll make you\nunderstand what exactly I'm telling you\nI'll try to expand\nthis whole diagram over here okay this\nis how my whole diagram will look like\nnow see this this is my general\narchitecture of a recurrent neural\nnetwork so I told you that I'm trying to\nsolve this NLP uske sentiment analysis\nand I'm discussing about the forward\npropagation how forward propagation will\ntake place okay so let us go ahead and\ntry to understand now suppose I have\nthese four words okay and this is an\noutput now at time T is equal to 1 my\nfirst word goes inside this particular\nhidden layer now suppose in this\nparticular hidden layer I have 100\nneurons ok I have 100 neurons now what\nwill happen the first thing that will\nhappen is that this input this word and\nI can represent this word into some\ndimensions that is of Rd so I can\nrepresent this with some vector you know\nthis word I can represent with some\nvector and how to do that I'll show you\nthat in the practical application this\nyou know about word to ik tf-idf bag of\nwords there each and every word we try\nto represent in the form of vectors\nright so similarly this particular word\nwill be represented for some D dimension\nvector then we assign weights because\nthis is how in a NN also happens right\nwe have our input then we multiply the\nweights and then we give it to the\nhidden layer so this hidden layer is\nhaving 100 neurons right so suppose this\nis having 100 neurons so I will try to\nmultiply my input layer that is whatever\nthe input data I'm coming along with\nthis particular weights and I will get\nthe output over okay now this is at time\nis equal to 1 at time is equal to 1 I am\njust passing one input right now ok so\nlet us go ahead and write the first\nfunction of oh one so oh one can be\nwritten over here as ok so here I'm\nactually now and understand one more\nthing guys here is all my hidden neurons\nand you know that in hidden neurons I\napply some activation function right\nlike Ray Lu like the tan H like sigmoid\nactivation functions so this oh one will\nbasically be some function right I'll\napply some kind of function over here\nand in this function will try to\nmultiply the blue and x11 that is what\nwe\nright in enn we'll multiply the inputs\nalong with the weights in each and every\nhidden neuron so here I can basically\nwrite it as x i1 x w okay so this will\nbe my output 1 okay this will be my\noutput 1 pretty much clear okay and this\nis important this is what is happening\nin the forward propagation right now\nonce this output 1 is computed now get\ngo back to this architecture now this\noutput 1 has to be given to the same\nhidden neuron right so this hidden you\nrun again will come over here okay and\nsame number of attend 180 ada neurons\nwill be coming but that time is equal to\nt is equal to 2\nmy next word that is x12 will get passed\nnow when it is getting passed the same\nwaves will get assigned over here\nremember guys weights will get updated\nonly in the back propagation not in the\nforward propagation so in my t is equal\nto 2 the same weights will get assigned\nmy new input will be x12 but I also have\nan output right this output is also\ngoing to the Nira and when I am passing\nthis output I will assign some different\nweight okay and they have a lot of great\ninitialization techniques guys which I\nhave already discussed in my artificial\nneural network so this is my W dash\nwhich is get initialized for this\nparticular output now how will my\nfunction look when I am using this data\nin this data and I am getting the output\nOh - now my Oh - will be nothing but\nfunction first of all and multiply X 1/2\ninto W so this will be my X 1/2 into W\nplus C and doing the addition operation\nfor this one also now right so oh one\nmultiplied by W 1 and this is very very\nimportant guide because this is what\nhappens in the forward propagation in\nthe backward propagation again the\nderivative of all this will get\ncalculated so that is how you know\nunremember oh here Oh 2 is completely\ndependent on this x1 2 and O 1 and that\nis how sequence is actually kept the\nsequence information is actually cared\nnow at time is equal to 3 that is T is\nequal to 3 again\nnow this particular output will go back\nto the same hidden neuron so o2 is going\nover here again this weight will get\ninitialized over here okay that blew up\n- and it'll be same okay and in this\ncase in the third time T is equal to 3 X\n1 3 is getting passed right now in when\nX 1 3 is getting passed again this will\nbe the same weights over here and this\nwill continue till the end of the\nstatement\nso my oh 3 again I can write it as my oh\n3 will nothing beber function of X 1 3 w\nplus o 2 into the blue - okay so this is\nmy W - okay and this is my oh 3 now\nsimilarly finally my Oh 4 which is this\noutput will be nothing but function of\nor X 1/4 x w+ o 3 into w 1 and this is\nwhere you are getting all your output 1\noutput 2 output 3 output 4 in the\nforward propagation this is how the\nfunction is getting computed and that's\nit we'll just stop in this 4 cycle right\nT is equal to 4 and sentences may have\nany lengths right so for the first eight\nsentence that you see over here it will\ngo till t is equal to 4 now you can see\nover here T is equal to 4 right now\nafter you get this output function then\nfinally we pass through this sigmoid\nbecause since this is an suppose this\nsentiment analysis is having an output\nas 0 and 1 so I have to use a soft max\nright over here so this will be my\nsoftmax activation function and then I'm\ndoing this and another way it will get\nassigned to this like w double dash okay\nand then this will classify and this\nwill give us our Y hat predicted value\nso this is my Y hat predicted value and\nthen I go and compute my loss function\nwhere I will try to subtract this two\nvalue and our main aim is to reduce this\nparticular value now once I have done\nthis in the backward propagation what\nwill happen that will be discussed in\nthe next video but just understand that\nhow the forward propagation has happened\nwe have started from here we have gone\ntill the end you know so just with\nrespect to one statement each and every\nword is can be\ninto vectors of some D dimensions and\nthat depends on the type of vocabulary\nthe number of words that are present in\nthat vocabulary so it is very very\nimportant how you can actually implement\nthat but in forward propagation this is\nhow it goes this main output basically\nmeans that unless and until the\nstatement does not get over this will be\ngetting passed to the same middle layer\nand always remember guys over here the\noutput phone will be dependent on X 1/4\nand output 3 output 3 will be dependent\non X 1 3 and output 2 output 2 will\ndependent on this and this value so by\nthis the sequence information is always\nmaintained you know it is not discarded\nwhich was the problem in some of the\ntechniques that we had like in\nbag-of-words tf-idf in other techniques\nbut in this technique the sequence\ninformation is not removed so this is\nbasically being used in many many\nimportant applications where you know\nthe output from that particular chatbot\nfrom that Google assistance is very very\nimportant\nbased on our statement that we usually\nspeak you know and this functions that\nwe have derived over here this is very\nvery important again guys because in the\nback propagation will try to find out\nthe derivatives of this right so I hope\nyou understood this particular video one\nmore change now I have to do is that\nguys always remember for each and every\nhidden neurons when time is equal to 2 3\n& 4 you can see that one output is going\nso to in order to change this in the\ninput also and make an output 0 and here\nthe weights are initialized this output\n0 is nothing but it will be some 0\npadded values or randomly initialized\nvalues initially ok then I'll try to\nmultiply this along with this I try to\nadd this also in my first name so here I\ncan basically write it as x11 w+ o 0w -\nright\nso i can write like this so yes this was\nall about this particular video I hope\nyou liked it please do subscribe the\nchannel share with all the friends Oh\never require this kind of health it is\nvery important to understand all these\nthings guys so that you'll be able to\nimplement a whole lot of things in\nrecord\nin our network so yes that's really\nintellectually they have a great day\nthank you one at all\n",
  "words": [
    "hello",
    "name",
    "krishna",
    "graham",
    "welcome",
    "youtube",
    "channel",
    "today",
    "particular",
    "video",
    "understanding",
    "recurrent",
    "neural",
    "network",
    "architecture",
    "also",
    "understand",
    "forward",
    "propagation",
    "time",
    "saying",
    "forward",
    "propagation",
    "time",
    "main",
    "functionality",
    "behind",
    "rnn",
    "behind",
    "working",
    "rna",
    "try",
    "understand",
    "exactly",
    "guys",
    "want",
    "know",
    "rn",
    "first",
    "need",
    "know",
    "general",
    "architecture",
    "rn",
    "want",
    "exactly",
    "rn",
    "rn",
    "architecture",
    "looks",
    "like",
    "input",
    "okay",
    "input",
    "may",
    "number",
    "dimensions",
    "okay",
    "number",
    "dimensions",
    "basically",
    "means",
    "number",
    "features",
    "right",
    "hidden",
    "layer",
    "number",
    "hidden",
    "neurons",
    "okay",
    "finally",
    "output",
    "along",
    "particular",
    "output",
    "also",
    "get",
    "output",
    "respect",
    "time",
    "basically",
    "means",
    "suppose",
    "want",
    "solve",
    "nlp",
    "use",
    "case",
    "sentiment",
    "analysis",
    "sentiment",
    "analysis",
    "sentence",
    "try",
    "determine",
    "whether",
    "positive",
    "sentence",
    "negative",
    "sentence",
    "positive",
    "review",
    "negative",
    "review",
    "usually",
    "whenever",
    "consider",
    "sentence",
    "suppose",
    "sentence",
    "1",
    "inside",
    "sentence",
    "suppose",
    "four",
    "words",
    "like",
    "food",
    "bad",
    "ok",
    "according",
    "output",
    "definitely",
    "say",
    "zero",
    "particular",
    "review",
    "negative",
    "review",
    "right",
    "take",
    "particular",
    "example",
    "rnn",
    "act",
    "every",
    "time",
    "like",
    "equal",
    "1",
    "pre",
    "process",
    "one",
    "word",
    "equal",
    "2",
    "pre",
    "process",
    "second",
    "word",
    "happen",
    "processing",
    "suppose",
    "first",
    "case",
    "pass",
    "one",
    "word",
    "right",
    "pre",
    "processing",
    "happen",
    "give",
    "output",
    "apart",
    "next",
    "word",
    "right",
    "whatever",
    "output",
    "first",
    "word",
    "also",
    "sent",
    "particular",
    "neuron",
    "recurrent",
    "neural",
    "network",
    "sequence",
    "information",
    "kept",
    "ok",
    "make",
    "understand",
    "exactly",
    "telling",
    "try",
    "expand",
    "whole",
    "diagram",
    "okay",
    "whole",
    "diagram",
    "look",
    "like",
    "see",
    "general",
    "architecture",
    "recurrent",
    "neural",
    "network",
    "told",
    "trying",
    "solve",
    "nlp",
    "uske",
    "sentiment",
    "analysis",
    "discussing",
    "forward",
    "propagation",
    "forward",
    "propagation",
    "take",
    "place",
    "okay",
    "let",
    "us",
    "go",
    "ahead",
    "try",
    "understand",
    "suppose",
    "four",
    "words",
    "okay",
    "output",
    "time",
    "equal",
    "1",
    "first",
    "word",
    "goes",
    "inside",
    "particular",
    "hidden",
    "layer",
    "suppose",
    "particular",
    "hidden",
    "layer",
    "100",
    "neurons",
    "ok",
    "100",
    "neurons",
    "happen",
    "first",
    "thing",
    "happen",
    "input",
    "word",
    "represent",
    "word",
    "dimensions",
    "rd",
    "represent",
    "vector",
    "know",
    "word",
    "represent",
    "vector",
    "show",
    "practical",
    "application",
    "know",
    "word",
    "ik",
    "bag",
    "words",
    "every",
    "word",
    "try",
    "represent",
    "form",
    "vectors",
    "right",
    "similarly",
    "particular",
    "word",
    "represented",
    "dimension",
    "vector",
    "assign",
    "weights",
    "nn",
    "also",
    "happens",
    "right",
    "input",
    "multiply",
    "weights",
    "give",
    "hidden",
    "layer",
    "hidden",
    "layer",
    "100",
    "neurons",
    "right",
    "suppose",
    "100",
    "neurons",
    "try",
    "multiply",
    "input",
    "layer",
    "whatever",
    "input",
    "data",
    "coming",
    "along",
    "particular",
    "weights",
    "get",
    "output",
    "okay",
    "time",
    "equal",
    "1",
    "time",
    "equal",
    "1",
    "passing",
    "one",
    "input",
    "right",
    "ok",
    "let",
    "us",
    "go",
    "ahead",
    "write",
    "first",
    "function",
    "oh",
    "one",
    "oh",
    "one",
    "written",
    "ok",
    "actually",
    "understand",
    "one",
    "thing",
    "guys",
    "hidden",
    "neurons",
    "know",
    "hidden",
    "neurons",
    "apply",
    "activation",
    "function",
    "right",
    "like",
    "ray",
    "lu",
    "like",
    "tan",
    "h",
    "like",
    "sigmoid",
    "activation",
    "functions",
    "oh",
    "one",
    "basically",
    "function",
    "right",
    "apply",
    "kind",
    "function",
    "function",
    "try",
    "multiply",
    "blue",
    "x11",
    "right",
    "enn",
    "multiply",
    "inputs",
    "along",
    "weights",
    "every",
    "hidden",
    "neuron",
    "basically",
    "write",
    "x",
    "i1",
    "x",
    "w",
    "okay",
    "output",
    "1",
    "okay",
    "output",
    "1",
    "pretty",
    "much",
    "clear",
    "okay",
    "important",
    "happening",
    "forward",
    "propagation",
    "right",
    "output",
    "1",
    "computed",
    "get",
    "go",
    "back",
    "architecture",
    "output",
    "1",
    "given",
    "hidden",
    "neuron",
    "right",
    "hidden",
    "run",
    "come",
    "okay",
    "number",
    "attend",
    "180",
    "ada",
    "neurons",
    "coming",
    "time",
    "equal",
    "equal",
    "2",
    "next",
    "word",
    "x12",
    "get",
    "passed",
    "getting",
    "passed",
    "waves",
    "get",
    "assigned",
    "remember",
    "guys",
    "weights",
    "get",
    "updated",
    "back",
    "propagation",
    "forward",
    "propagation",
    "equal",
    "2",
    "weights",
    "get",
    "assigned",
    "new",
    "input",
    "x12",
    "also",
    "output",
    "right",
    "output",
    "also",
    "going",
    "nira",
    "passing",
    "output",
    "assign",
    "different",
    "weight",
    "okay",
    "lot",
    "great",
    "initialization",
    "techniques",
    "guys",
    "already",
    "discussed",
    "artificial",
    "neural",
    "network",
    "w",
    "dash",
    "get",
    "initialized",
    "particular",
    "output",
    "function",
    "look",
    "using",
    "data",
    "data",
    "getting",
    "output",
    "oh",
    "oh",
    "nothing",
    "function",
    "first",
    "multiply",
    "x",
    "w",
    "x",
    "w",
    "plus",
    "c",
    "addition",
    "operation",
    "one",
    "also",
    "right",
    "oh",
    "one",
    "multiplied",
    "w",
    "1",
    "important",
    "guide",
    "happens",
    "forward",
    "propagation",
    "backward",
    "propagation",
    "derivative",
    "get",
    "calculated",
    "know",
    "unremember",
    "oh",
    "oh",
    "2",
    "completely",
    "dependent",
    "x1",
    "2",
    "1",
    "sequence",
    "actually",
    "kept",
    "sequence",
    "information",
    "actually",
    "cared",
    "time",
    "equal",
    "3",
    "equal",
    "3",
    "particular",
    "output",
    "go",
    "back",
    "hidden",
    "neuron",
    "o2",
    "going",
    "weight",
    "get",
    "initialized",
    "okay",
    "blew",
    "okay",
    "case",
    "third",
    "time",
    "equal",
    "3",
    "x",
    "1",
    "3",
    "getting",
    "passed",
    "right",
    "x",
    "1",
    "3",
    "getting",
    "passed",
    "weights",
    "continue",
    "till",
    "end",
    "statement",
    "oh",
    "3",
    "write",
    "oh",
    "3",
    "nothing",
    "beber",
    "function",
    "x",
    "1",
    "3",
    "w",
    "plus",
    "2",
    "blue",
    "okay",
    "w",
    "okay",
    "oh",
    "3",
    "similarly",
    "finally",
    "oh",
    "4",
    "output",
    "nothing",
    "function",
    "x",
    "x",
    "3",
    "w",
    "1",
    "getting",
    "output",
    "1",
    "output",
    "2",
    "output",
    "3",
    "output",
    "4",
    "forward",
    "propagation",
    "function",
    "getting",
    "computed",
    "stop",
    "4",
    "cycle",
    "right",
    "equal",
    "4",
    "sentences",
    "may",
    "lengths",
    "right",
    "first",
    "eight",
    "sentence",
    "see",
    "go",
    "till",
    "equal",
    "4",
    "see",
    "equal",
    "4",
    "right",
    "get",
    "output",
    "function",
    "finally",
    "pass",
    "sigmoid",
    "since",
    "suppose",
    "sentiment",
    "analysis",
    "output",
    "0",
    "1",
    "use",
    "soft",
    "max",
    "right",
    "softmax",
    "activation",
    "function",
    "another",
    "way",
    "get",
    "assigned",
    "like",
    "w",
    "double",
    "dash",
    "okay",
    "classify",
    "give",
    "us",
    "hat",
    "predicted",
    "value",
    "hat",
    "predicted",
    "value",
    "go",
    "compute",
    "loss",
    "function",
    "try",
    "subtract",
    "two",
    "value",
    "main",
    "aim",
    "reduce",
    "particular",
    "value",
    "done",
    "backward",
    "propagation",
    "happen",
    "discussed",
    "next",
    "video",
    "understand",
    "forward",
    "propagation",
    "happened",
    "started",
    "gone",
    "till",
    "end",
    "know",
    "respect",
    "one",
    "statement",
    "every",
    "word",
    "vectors",
    "dimensions",
    "depends",
    "type",
    "vocabulary",
    "number",
    "words",
    "present",
    "vocabulary",
    "important",
    "actually",
    "implement",
    "forward",
    "propagation",
    "goes",
    "main",
    "output",
    "basically",
    "means",
    "unless",
    "statement",
    "get",
    "getting",
    "passed",
    "middle",
    "layer",
    "always",
    "remember",
    "guys",
    "output",
    "phone",
    "dependent",
    "x",
    "output",
    "3",
    "output",
    "3",
    "dependent",
    "x",
    "1",
    "3",
    "output",
    "2",
    "output",
    "2",
    "dependent",
    "value",
    "sequence",
    "information",
    "always",
    "maintained",
    "know",
    "discarded",
    "problem",
    "techniques",
    "like",
    "techniques",
    "technique",
    "sequence",
    "information",
    "removed",
    "basically",
    "used",
    "many",
    "many",
    "important",
    "applications",
    "know",
    "output",
    "particular",
    "chatbot",
    "google",
    "assistance",
    "important",
    "based",
    "statement",
    "usually",
    "speak",
    "know",
    "functions",
    "derived",
    "important",
    "guys",
    "back",
    "propagation",
    "try",
    "find",
    "derivatives",
    "right",
    "hope",
    "understood",
    "particular",
    "video",
    "one",
    "change",
    "guys",
    "always",
    "remember",
    "every",
    "hidden",
    "neurons",
    "time",
    "equal",
    "2",
    "3",
    "4",
    "see",
    "one",
    "output",
    "going",
    "order",
    "change",
    "input",
    "also",
    "make",
    "output",
    "0",
    "weights",
    "initialized",
    "output",
    "0",
    "nothing",
    "0",
    "padded",
    "values",
    "randomly",
    "initialized",
    "values",
    "initially",
    "ok",
    "try",
    "multiply",
    "along",
    "try",
    "add",
    "also",
    "first",
    "name",
    "basically",
    "write",
    "x11",
    "0w",
    "right",
    "write",
    "like",
    "yes",
    "particular",
    "video",
    "hope",
    "liked",
    "please",
    "subscribe",
    "channel",
    "share",
    "friends",
    "oh",
    "ever",
    "require",
    "kind",
    "health",
    "important",
    "understand",
    "things",
    "guys",
    "able",
    "implement",
    "whole",
    "lot",
    "things",
    "record",
    "network",
    "yes",
    "really",
    "intellectually",
    "great",
    "day",
    "thank",
    "one"
  ],
  "keywords": [
    "particular",
    "video",
    "recurrent",
    "neural",
    "network",
    "architecture",
    "also",
    "understand",
    "forward",
    "propagation",
    "time",
    "main",
    "try",
    "exactly",
    "guys",
    "want",
    "know",
    "rn",
    "first",
    "like",
    "input",
    "okay",
    "number",
    "dimensions",
    "basically",
    "means",
    "right",
    "hidden",
    "layer",
    "neurons",
    "finally",
    "output",
    "along",
    "get",
    "suppose",
    "case",
    "sentiment",
    "analysis",
    "sentence",
    "negative",
    "review",
    "1",
    "words",
    "ok",
    "every",
    "equal",
    "pre",
    "one",
    "word",
    "2",
    "happen",
    "give",
    "next",
    "neuron",
    "sequence",
    "information",
    "whole",
    "see",
    "us",
    "go",
    "100",
    "represent",
    "vector",
    "weights",
    "multiply",
    "data",
    "write",
    "function",
    "oh",
    "actually",
    "activation",
    "x",
    "w",
    "important",
    "back",
    "passed",
    "getting",
    "assigned",
    "remember",
    "going",
    "techniques",
    "initialized",
    "nothing",
    "dependent",
    "3",
    "till",
    "statement",
    "4",
    "0",
    "value",
    "always"
  ]
}