{
  "text": "hi everyone this is yml and today you\nare going to talk about two of the most\nused methods in assemble learning\nbegging and boosting we'll see how each\nalgorithm works and what are the\nsimilarities and the difference between\nthe two so let's not waste any more time\nand let's dig in\nto start with let's suppose that we have\na data set what begging does with this\ndata set is quite simple you create and\nsub data set of equal size by sampling\nwith the replacement from the original\ndata set a technique known as\nbootstrapping and train a classifier on\neach sub data set then at the end we use\nall these models to make a prediction in\nan assembler classifier so we know\nnutshell begging consists of two steps\none is bootstrapping the data set and\ntwo is aggregating the results\nhenceforth its name begging when used\nboosting on the other hand the way you\ncreate this models changes quite a bit\nso let's suppose that you have that same\ndata set the first thing you do is to\ntrain a classifier on this data set and\nsee which samples were correctly\nclassified by the model and which ones\nwere incorrectly classified then you use\nthis information to weight up the\nsamples which were incorrectly\nclassified by the model so that when you\ntrain the next model it pays more\nattention to those samples and hopefully\nlearns to correctly classify them then\nyou look again at the incorrectly\nclassified samples weigh them up drain a\nnew model look at its predictions weigh\nup the misclassified samples and so on\nand so on until you get the desired\nnumber of models at the end as in the\nbeginning case you use all these models\nin an assemble to make predictions on\nnew data now let's see what are the\nsimilarities and the differences between\nthe two by firstly looking at how they\nwork so at a high level both Metals\nBuilds an example of models but begging\nbuilds them in parallel and boosting\nboost them sequentially knowing this may\nhelp us in choosing which one to use\ndepending on the Computing resources and\ndevelopment time we have at our disposal\nif we have a lot of computing then due\nto its parallel nature begin may be a\nmore suitable algorithm since it may\ntake a lot less time to train the models\nwhile we might get no significant\nImprovement in training for boosting due\nto its sequential nature another 13 that\nwe might consider when looking at these\ntwo Ensemble learning methods is the\ndata set on which they train the\nclassifiers so both models builds a\nseparate data for each model but they do\nit differently begging uses a subset of\nthe original data set that is generated\nby same thing with replacement while\nboosting uses the same samples as in the\noriginal data set also another important\ndistinction is that in back in the\nsamples are unweighted while in boosting\nthey are weighted in regards to the\npredictions given by the previous\nclassifier how each Ensemble makes the\npredictions is yet another important\nDimension to analyze so both methods\nmake predictions by taking the average\nof the models butting back in the\nclassifiers are equally weighted while\nin boosting the models are weighted in\nThe Ensemble based on their training\nperformance\nas in any machine learning problem the\nbias and variance of the system plays an\nimportant role in the final performance\nin our case because they are an enzyme\npod backing and boosting are good at\nreducing the variance however begging\nhas closely zero bias reduction the\nreason being that because we don't\nchange the voiding of our data when\nsampling the bias of the individual\nmodel is transferred to the assemble\nthis doesn't happen in the boosting case\nsince the samples are waiting from one\nmodel to another but unfortunately this\nmakes boosting more prone to overheating\nin comparison with pegging I know that I\nmay have lesser questions unanswered in\nthis video and things like why do we\nsample with replacement in begging or\nwhy is boosting prone to overheating\nmore exactly may have popped in your\nmind well I've done that on purpose\nmostly because you may notice a thing\nthat this question have in common the\nwhy which is the main theme on this\nchannel so I intend to make videos about\nthese subjects in the future this was\nthe video for today I hope you enjoyed\nit please leave a like if you did share\nwith me your thoughts in the comment\nsection subscribe to be up to date with\nthe new content and until next time I\nwish you a wonderful time bye bye\n",
  "words": [
    "hi",
    "everyone",
    "yml",
    "today",
    "going",
    "talk",
    "two",
    "used",
    "methods",
    "assemble",
    "learning",
    "begging",
    "boosting",
    "see",
    "algorithm",
    "works",
    "similarities",
    "difference",
    "two",
    "let",
    "waste",
    "time",
    "let",
    "dig",
    "start",
    "let",
    "suppose",
    "data",
    "set",
    "begging",
    "data",
    "set",
    "quite",
    "simple",
    "create",
    "sub",
    "data",
    "set",
    "equal",
    "size",
    "sampling",
    "replacement",
    "original",
    "data",
    "set",
    "technique",
    "known",
    "bootstrapping",
    "train",
    "classifier",
    "sub",
    "data",
    "set",
    "end",
    "use",
    "models",
    "make",
    "prediction",
    "assembler",
    "classifier",
    "know",
    "nutshell",
    "begging",
    "consists",
    "two",
    "steps",
    "one",
    "bootstrapping",
    "data",
    "set",
    "two",
    "aggregating",
    "results",
    "henceforth",
    "name",
    "begging",
    "used",
    "boosting",
    "hand",
    "way",
    "create",
    "models",
    "changes",
    "quite",
    "bit",
    "let",
    "suppose",
    "data",
    "set",
    "first",
    "thing",
    "train",
    "classifier",
    "data",
    "set",
    "see",
    "samples",
    "correctly",
    "classified",
    "model",
    "ones",
    "incorrectly",
    "classified",
    "use",
    "information",
    "weight",
    "samples",
    "incorrectly",
    "classified",
    "model",
    "train",
    "next",
    "model",
    "pays",
    "attention",
    "samples",
    "hopefully",
    "learns",
    "correctly",
    "classify",
    "look",
    "incorrectly",
    "classified",
    "samples",
    "weigh",
    "drain",
    "new",
    "model",
    "look",
    "predictions",
    "weigh",
    "misclassified",
    "samples",
    "get",
    "desired",
    "number",
    "models",
    "end",
    "beginning",
    "case",
    "use",
    "models",
    "assemble",
    "make",
    "predictions",
    "new",
    "data",
    "let",
    "see",
    "similarities",
    "differences",
    "two",
    "firstly",
    "looking",
    "work",
    "high",
    "level",
    "metals",
    "builds",
    "example",
    "models",
    "begging",
    "builds",
    "parallel",
    "boosting",
    "boost",
    "sequentially",
    "knowing",
    "may",
    "help",
    "us",
    "choosing",
    "one",
    "use",
    "depending",
    "computing",
    "resources",
    "development",
    "time",
    "disposal",
    "lot",
    "computing",
    "due",
    "parallel",
    "nature",
    "begin",
    "may",
    "suitable",
    "algorithm",
    "since",
    "may",
    "take",
    "lot",
    "less",
    "time",
    "train",
    "models",
    "might",
    "get",
    "significant",
    "improvement",
    "training",
    "boosting",
    "due",
    "sequential",
    "nature",
    "another",
    "13",
    "might",
    "consider",
    "looking",
    "two",
    "ensemble",
    "learning",
    "methods",
    "data",
    "set",
    "train",
    "classifiers",
    "models",
    "builds",
    "separate",
    "data",
    "model",
    "differently",
    "begging",
    "uses",
    "subset",
    "original",
    "data",
    "set",
    "generated",
    "thing",
    "replacement",
    "boosting",
    "uses",
    "samples",
    "original",
    "data",
    "set",
    "also",
    "another",
    "important",
    "distinction",
    "back",
    "samples",
    "unweighted",
    "boosting",
    "weighted",
    "regards",
    "predictions",
    "given",
    "previous",
    "classifier",
    "ensemble",
    "makes",
    "predictions",
    "yet",
    "another",
    "important",
    "dimension",
    "analyze",
    "methods",
    "make",
    "predictions",
    "taking",
    "average",
    "models",
    "butting",
    "back",
    "classifiers",
    "equally",
    "weighted",
    "boosting",
    "models",
    "weighted",
    "ensemble",
    "based",
    "training",
    "performance",
    "machine",
    "learning",
    "problem",
    "bias",
    "variance",
    "system",
    "plays",
    "important",
    "role",
    "final",
    "performance",
    "case",
    "enzyme",
    "pod",
    "backing",
    "boosting",
    "good",
    "reducing",
    "variance",
    "however",
    "begging",
    "closely",
    "zero",
    "bias",
    "reduction",
    "reason",
    "change",
    "voiding",
    "data",
    "sampling",
    "bias",
    "individual",
    "model",
    "transferred",
    "assemble",
    "happen",
    "boosting",
    "case",
    "since",
    "samples",
    "waiting",
    "one",
    "model",
    "another",
    "unfortunately",
    "makes",
    "boosting",
    "prone",
    "overheating",
    "comparison",
    "pegging",
    "know",
    "may",
    "lesser",
    "questions",
    "unanswered",
    "video",
    "things",
    "like",
    "sample",
    "replacement",
    "begging",
    "boosting",
    "prone",
    "overheating",
    "exactly",
    "may",
    "popped",
    "mind",
    "well",
    "done",
    "purpose",
    "mostly",
    "may",
    "notice",
    "thing",
    "question",
    "common",
    "main",
    "theme",
    "channel",
    "intend",
    "make",
    "videos",
    "subjects",
    "future",
    "video",
    "today",
    "hope",
    "enjoyed",
    "please",
    "leave",
    "like",
    "share",
    "thoughts",
    "comment",
    "section",
    "subscribe",
    "date",
    "new",
    "content",
    "next",
    "time",
    "wish",
    "wonderful",
    "time",
    "bye",
    "bye"
  ],
  "keywords": [
    "today",
    "two",
    "used",
    "methods",
    "assemble",
    "learning",
    "begging",
    "boosting",
    "see",
    "algorithm",
    "similarities",
    "let",
    "time",
    "suppose",
    "data",
    "set",
    "quite",
    "create",
    "sub",
    "sampling",
    "replacement",
    "original",
    "bootstrapping",
    "train",
    "classifier",
    "end",
    "use",
    "models",
    "make",
    "know",
    "one",
    "thing",
    "samples",
    "correctly",
    "classified",
    "model",
    "incorrectly",
    "next",
    "look",
    "weigh",
    "new",
    "predictions",
    "get",
    "case",
    "looking",
    "builds",
    "parallel",
    "may",
    "computing",
    "lot",
    "due",
    "nature",
    "since",
    "might",
    "training",
    "another",
    "ensemble",
    "classifiers",
    "uses",
    "important",
    "back",
    "weighted",
    "makes",
    "performance",
    "bias",
    "variance",
    "prone",
    "overheating",
    "video",
    "like",
    "bye"
  ]
}