{
  "text": "greetings fellow Learners before we\nbegin our titular tale of transfer\nlearning I have a thought-provoking\nquestion for\nyou what activity or skill was easy for\nyou to pick up but not necessarily for\nanyone else for me I would say it was\nteaching and I attribute a lot of this\nto the public speaking i' had been doing\nsince I was growing up by the time I was\nin 10th Grade I was speaking in front of\nthousands of people hosting events when\nI was in school studying in India and\neventually I started hosting other\nevents which you can still find on my\nchannel till this day so public speaking\nis definitely a transferable skill I\nused for teaching on YouTube and so\nteaching came a little bit quick to me\nbut please comment down below what your\nsuperpower skill is and I would love to\nknow you better now this video is going\nto be divided into three passes where we\nstart with an overview of transfer\nlearning followed by some detailed\nexample and how we would exactly solve a\ntransfer learning problem and then we're\ngoing to code that same thing out in\npass three of the explanation so stay\ntuned we're going to learn a\nlot this is a neural network so let's\nsay that we want to train this network\nto take in a given question and produce\nan answer to do so we construct a data\nset of about 1 million questions along\nwith their\nanswers now during the training phase of\nthe network we will pass question answer\npairs to the model the model updates its\nparameters and we repeat this process\nuntil the model eventually learns to\nanswer\nquestions now during the inference phase\nthe model can take an unseen question\nand then it can produce an answer so\nthis is great now let's say that we want\nto perform another related task where we\ntranslate a given English sentence to a\nFrench sentence so to do so we will\nconstruct a data set of like let's say\nagain 1 million English sentences with\ntheir French\ntranslations we start with an untrained\nNetwork and during the training phase of\nthe network we pass English French pairs\nto the model and update the model\nparameters now during the inference\nphase the model can take an unseen\nEnglish sentence and then produce a\nFrench\ntranslation this is is great\nagain but now one major pain Point here\nis that building data sets of this 1\nmillion size can be very difficult and\nif we wanted to now solve the problem of\nsay translating English to Spanish this\ncan be difficult because we would need\nto collect 1 million examples again from\nscratch now transfer learning helps\nmitigate this problem so let's say that\nwe want to build an English Spanish\ntranslator we can first train the model\non one problem like English to French\nand then we can fine-tune this model on\nthe problem we want to solve which is\nEnglish to Spanish in this way knowledge\ncan be transferred using a model trained\non one problem as a starting point and\nhence we don't need as much English to\nSpanish data for the model to actually\nlearn quiz\n[Music]\ntime have you been paying attention\nlet's quiz you to find out what is the\nbenefit of transfer learning a it\nreduces the need for computational\nresources B it overcomes data\nlimitations C it enhances model\ninterpretability or d none of the above\nplease comment your answer down below\nand let's have a discussion if you think\nI deserve and you love learning please\ndo consider hitting that like button\nbecause it will help me a lot that's\nthat's going to do it for pass one in\nquiz time for now but keep paying\nattention because I will be back to quiz\nyou let's illustrate transfer learning\nby teaching a model to perform an NLP\ntask which is question answering in the\nsimplest form of question answering the\nnetwork is given a context and a\nquestion and the output answer can be\nextracted from the context so the output\nis essentially just going to be two\nnumbers the first number is going to be\nthe index of the start position and the\nsecond number is going to be the index\nof the end position for this type of\nnetwork we will use Bert with transfer\nlearning for details on the architecture\nof Bert you can check out this video and\nfor now just know that Bert is a stack\nof the encoder part of the Transformer\nneural\nnetwork and it is trained in two phases\nwe have a pre-training phase and a fine\ntuning phase so let's talk about each of\nthese so during the pre-training phase\nwe will take a dumb Network and we'll\ntrain it on two problems Mass language\nmodeling and next sentence prediction so\nin Mass language modeling the model will\ntake in a mask input and determine what\nthose masks are in next sentence\nprediction the model will take in two\nsentences and determine whether the\nsecond sentence logically follows the\nfirst and once it is trained on these\ntwo problems The Bert model is set to be\npre-trained now you as a user don't\nusually need to pre-train a model\nyourself and these models can be\ndownloaded online and then fine tuned\nfor your use\ncase during the fine-tuning phase we\nwill take this pre-train network and\ntrain it further on question answering\nso the data set would have a question\nplus context we concatenate these\ntogether this stream of text is then\nbroken down into individual units called\na token we then pass these tokens into\nBert and in code we're going to use a\nversion of Bert called distill Bert that\nconverts each token into a\n768 dimensional\nembedding when I say embedding they are\nbasically vectors that is a set of 768\nnumbers that represent the meaning of a\nword now each of these 384 tokens are\nthen mapped into a two-dimensional\nVector now the first number will\ndetermine the probability that this\ntoken is the start token of the answer\nand the second number determines the\nprobability that this token is the end\ntoken of the answer so they are numbers\nbetween 0o and\none and we have\n384 of these two-dimensional vectors\nwe can then take the maximum values\nacross these columns to get the start\nposition and end\nposition and there's a little bit of\npostprocessing to handle some edge cases\noverall the big picture here is birch\nrequires substantially less question\nanswer data than if we had trained a\nmodel from\nscratch squeeze\ntime\nit's that time of video again have you\nbeen paying attention let's quiz you to\nfind out which of the following\narchitectures can make use of transfer\nlearning a Bert b\nGPT C feed forward neural networks or D\nall of the\nabove comment your answer down below and\nlet's have a discussion that's going to\ndo it for quiz time for now but keep\npaying attention because I will be back\nto quiz\nyou in this pass we are going to\nfine-tune Bert on the question answering\ndata set this notebook we describe a few\nprocesses the first is loading up the\ntraining data then we're going to have\nto pre-process that training data so\nthat it can be fed to a model then\nloading and training of the model itself\nand then some postprocessing to be able\nto interpret the results from the model\nand then performance evaluation so let's\ngo through each starting with just\nloading the data set in this case we're\ngoing to use Squad version one which\nmeans that the answers will always be\npresent within the context that is\nprovided so as to simplify the process\nwe're going to use distilled Bert and\nthe batch size is 16 which means that we\ncan pass 16 examples to the network all\nin parallel and we can get 16 results in\nparallel this data set has 87,000\ntraining examples and 10,000 test\nexamples and so this shows that we only\nneed so many examples to actually\nfine-tune our model instead of the\nmillions that we would potentially need\nif training from scratch and here is a\nrecord of how exactly that training data\nlooks the important part is the context\nthe question and then the answer where\nthe answer is we can see that the text\nthat the answer respond corresponds to\nand this text is directly present within\nthe context itself so they consider to\nbe unscriptural doctrines that is\npresent right over here and then its\nstarting position is also given over\nhere which is the position of they in\nthis case step one of loading the data\nset is complete now we want to go to\nstep two of actually pre-processing the\ntraining data so first thing we want to\ndo is concatenate the question and the\ncontext and then we will break that text\ndown into individual tokens via an auto\ntokenizer so you could see that here\nthis is the question this is the context\nwe have broken it down into individual\ntokens and then we have padded it using\na padding token this is required because\nwe want some fix size inputs\nmathematical inputs to be passed into\nthe model at a given time and then we\nalso want to fetch the start position\nthis will indicate the index of the\nstart of the answer and then the index\nof the end of the answer because it's\nalready present in the context so for\nexample 33 to 39 for this fifth example\nright over here which is the same one\nright over here it just basically says\nthat I think a golden statue of the\nVirgin Mary answers this question and\nthis a golden I think this is the 33rd\ntoken whereas this is the 39th\ntoken so how do we go from this to\ncreating this tokenized version plus\npadding plus you know getting the start\nand end positions right over here of the\nanswer well we have a function that does\nall of that right over here which is\ngoing to be called prepare training\nfeatures so we were able to load the\ndata set and we are now able to\npre-process our input data and our\noutput data now we need to load and\nfine-tune our model and this here is the\narchitecture so what happens here is we\nnow have the input tokens which were the\n384 tokens each of those tokens is is\ngoing to be mapped to a\n768 dimensional embedding Vector\nposition embedding allows the tokens to\nencode positional information because\ntheir position matters layer Norm is\ngoing to help stabilize training Dropout\nis going to act as a regularizer to\nprevent or mitigate overfitting then we\nhave the Transformer and this consists\nof six encoder Transformer blocks\nbecause this is Burt and with the\nencoder Transformer it's going to\nperform a series of attention operations\nin order to better encapsulate the\nmeaning of every single token and that's\nwhy the output of every single token is\ngoing to be 768 Dimensions better\nencapsulating meaning if you want to get\nan idea of exactly how every single\ncomponent works I highly recommend you\ntake a look at the Transformers from\nscratch playlist now we going to have\n384 tokens and each of those is going to\nbe mapped to a two dimensional output\nwhich indicates the probability of start\nindex and the probability that this\ntoken is the end index so that's the\nmodel that we're dealing with and we can\nactually start the training process\nfairly\neasily so we've loaded the data set we\nhave pre-processed the data we now have\ntrained the model and now let's actually\nlook at post-processing and prediction\nof the model itself so this code over\nhere is going to take a batch of\nexamples and generate some predictions\nnow this is going to be a list of 384\ntokens which indicates the probabilities\nof it each being the start index and\nanother 384 token list of it being the\nend index you could see and it also has\na batch size because we're passing 16\nexamples at a time but we really only\nneed the position like the true position\nfor every single column we can actually\njust take like the maximum value over\nhere so that means that for example this\nis going to be a list of 16 examples in\nthe batch so for the first example token\n46 is the start and then it ends at\ntoken 47 that's the answer then the\nsecond example the answer starts at\ntoken 57 and ends at token 58 for the\nthird example the answer starts at token\n78 and ends at token 81 and so on and so\nnow that we've done the load data set\nthe pre-processing the model training\nthe postprocessing we can now do some\nHardy evaluation because we had our\nevaluation ation set of about 10,000\nexamples and we could see how much there\nwas an exact match to the actual answers\ngenerated by the model and the answers\nthat are actually true and we can also\ngenerate some other evaluation metrics\nand once done you can push your\nfine-tune model to hugging face Hub quiz\ntime all right this is going to be a fun\none what is the primary advantage of\nusing disle Bert over Bert a disle Bert\nis larger and more\npowerful B distill Bert provides better\ninterpretability C distill Bert is\nfaster and has a smaller memory\nfootprint or D disal BT has a higher\ncapacity for model\ncomplexity comment your answer down\nbelow and let's have a discussion and if\nyou think I do deserve it please do\nconsider giving this video a like\nbecause it'll help me out a lot now\nthat's going to do for quiz time and\npass three of the explanation of the\nvideo but before we go let's generate a\n[Music]\nsummary transfer learning involves\npre-training a model on a general task\nthen further fine-tuning the model on a\nspecific task this way we don't need as\nmuch task specific data now Bert uses\ntransfer learning by pre-training on\nmass language modeling that is given a\nMK input determine the masks and next\nsentence prediction that is given two\nsentences determine if the second\nsentence logically follows the first and\nwe can use less data to fine-tune on\nother NLP tasks like question answering\nand that's all we have for today the\nlinks to the code and all the resources\nwill be provided in the description down\nbelow so do check it out if you want to\nknow more about how Bert works I highly\nsuggest you check out this video right\nover here and you can watch the rest of\nthe NLP playlist as well thank you all\nso much for watching if you do think I\ndeserve it please do give this video a\nlike thank you and I will see you in the\nnext one bye-bye\n",
  "words": [
    "greetings",
    "fellow",
    "learners",
    "begin",
    "titular",
    "tale",
    "transfer",
    "learning",
    "question",
    "activity",
    "skill",
    "easy",
    "pick",
    "necessarily",
    "anyone",
    "else",
    "would",
    "say",
    "teaching",
    "attribute",
    "lot",
    "public",
    "speaking",
    "since",
    "growing",
    "time",
    "10th",
    "grade",
    "speaking",
    "front",
    "thousands",
    "people",
    "hosting",
    "events",
    "school",
    "studying",
    "india",
    "eventually",
    "started",
    "hosting",
    "events",
    "still",
    "find",
    "channel",
    "till",
    "day",
    "public",
    "speaking",
    "definitely",
    "transferable",
    "skill",
    "used",
    "teaching",
    "youtube",
    "teaching",
    "came",
    "little",
    "bit",
    "quick",
    "please",
    "comment",
    "superpower",
    "skill",
    "would",
    "love",
    "know",
    "better",
    "video",
    "going",
    "divided",
    "three",
    "passes",
    "start",
    "overview",
    "transfer",
    "learning",
    "followed",
    "detailed",
    "example",
    "would",
    "exactly",
    "solve",
    "transfer",
    "learning",
    "problem",
    "going",
    "code",
    "thing",
    "pass",
    "three",
    "explanation",
    "stay",
    "tuned",
    "going",
    "learn",
    "lot",
    "neural",
    "network",
    "let",
    "say",
    "want",
    "train",
    "network",
    "take",
    "given",
    "question",
    "produce",
    "answer",
    "construct",
    "data",
    "set",
    "1",
    "million",
    "questions",
    "along",
    "answers",
    "training",
    "phase",
    "network",
    "pass",
    "question",
    "answer",
    "pairs",
    "model",
    "model",
    "updates",
    "parameters",
    "repeat",
    "process",
    "model",
    "eventually",
    "learns",
    "answer",
    "questions",
    "inference",
    "phase",
    "model",
    "take",
    "unseen",
    "question",
    "produce",
    "answer",
    "great",
    "let",
    "say",
    "want",
    "perform",
    "another",
    "related",
    "task",
    "translate",
    "given",
    "english",
    "sentence",
    "french",
    "sentence",
    "construct",
    "data",
    "set",
    "like",
    "let",
    "say",
    "1",
    "million",
    "english",
    "sentences",
    "french",
    "translations",
    "start",
    "untrained",
    "network",
    "training",
    "phase",
    "network",
    "pass",
    "english",
    "french",
    "pairs",
    "model",
    "update",
    "model",
    "parameters",
    "inference",
    "phase",
    "model",
    "take",
    "unseen",
    "english",
    "sentence",
    "produce",
    "french",
    "translation",
    "great",
    "one",
    "major",
    "pain",
    "point",
    "building",
    "data",
    "sets",
    "1",
    "million",
    "size",
    "difficult",
    "wanted",
    "solve",
    "problem",
    "say",
    "translating",
    "english",
    "spanish",
    "difficult",
    "would",
    "need",
    "collect",
    "1",
    "million",
    "examples",
    "scratch",
    "transfer",
    "learning",
    "helps",
    "mitigate",
    "problem",
    "let",
    "say",
    "want",
    "build",
    "english",
    "spanish",
    "translator",
    "first",
    "train",
    "model",
    "one",
    "problem",
    "like",
    "english",
    "french",
    "model",
    "problem",
    "want",
    "solve",
    "english",
    "spanish",
    "way",
    "knowledge",
    "transferred",
    "using",
    "model",
    "trained",
    "one",
    "problem",
    "starting",
    "point",
    "hence",
    "need",
    "much",
    "english",
    "spanish",
    "data",
    "model",
    "actually",
    "learn",
    "quiz",
    "music",
    "time",
    "paying",
    "attention",
    "let",
    "quiz",
    "find",
    "benefit",
    "transfer",
    "learning",
    "reduces",
    "need",
    "computational",
    "resources",
    "b",
    "overcomes",
    "data",
    "limitations",
    "c",
    "enhances",
    "model",
    "interpretability",
    "none",
    "please",
    "comment",
    "answer",
    "let",
    "discussion",
    "think",
    "deserve",
    "love",
    "learning",
    "please",
    "consider",
    "hitting",
    "like",
    "button",
    "help",
    "lot",
    "going",
    "pass",
    "one",
    "quiz",
    "time",
    "keep",
    "paying",
    "attention",
    "back",
    "quiz",
    "let",
    "illustrate",
    "transfer",
    "learning",
    "teaching",
    "model",
    "perform",
    "nlp",
    "task",
    "question",
    "answering",
    "simplest",
    "form",
    "question",
    "answering",
    "network",
    "given",
    "context",
    "question",
    "output",
    "answer",
    "extracted",
    "context",
    "output",
    "essentially",
    "going",
    "two",
    "numbers",
    "first",
    "number",
    "going",
    "index",
    "start",
    "position",
    "second",
    "number",
    "going",
    "index",
    "end",
    "position",
    "type",
    "network",
    "use",
    "bert",
    "transfer",
    "learning",
    "details",
    "architecture",
    "bert",
    "check",
    "video",
    "know",
    "bert",
    "stack",
    "encoder",
    "part",
    "transformer",
    "neural",
    "network",
    "trained",
    "two",
    "phases",
    "phase",
    "fine",
    "tuning",
    "phase",
    "let",
    "talk",
    "phase",
    "take",
    "dumb",
    "network",
    "train",
    "two",
    "problems",
    "mass",
    "language",
    "modeling",
    "next",
    "sentence",
    "prediction",
    "mass",
    "language",
    "modeling",
    "model",
    "take",
    "mask",
    "input",
    "determine",
    "masks",
    "next",
    "sentence",
    "prediction",
    "model",
    "take",
    "two",
    "sentences",
    "determine",
    "whether",
    "second",
    "sentence",
    "logically",
    "follows",
    "first",
    "trained",
    "two",
    "problems",
    "bert",
    "model",
    "set",
    "user",
    "usually",
    "need",
    "model",
    "models",
    "downloaded",
    "online",
    "fine",
    "tuned",
    "use",
    "case",
    "phase",
    "take",
    "network",
    "train",
    "question",
    "answering",
    "data",
    "set",
    "would",
    "question",
    "plus",
    "context",
    "concatenate",
    "together",
    "stream",
    "text",
    "broken",
    "individual",
    "units",
    "called",
    "token",
    "pass",
    "tokens",
    "bert",
    "code",
    "going",
    "use",
    "version",
    "bert",
    "called",
    "distill",
    "bert",
    "converts",
    "token",
    "768",
    "dimensional",
    "embedding",
    "say",
    "embedding",
    "basically",
    "vectors",
    "set",
    "768",
    "numbers",
    "represent",
    "meaning",
    "word",
    "384",
    "tokens",
    "mapped",
    "vector",
    "first",
    "number",
    "determine",
    "probability",
    "token",
    "start",
    "token",
    "answer",
    "second",
    "number",
    "determines",
    "probability",
    "token",
    "end",
    "token",
    "answer",
    "numbers",
    "0o",
    "one",
    "384",
    "vectors",
    "take",
    "maximum",
    "values",
    "across",
    "columns",
    "get",
    "start",
    "position",
    "end",
    "position",
    "little",
    "bit",
    "postprocessing",
    "handle",
    "edge",
    "cases",
    "overall",
    "big",
    "picture",
    "birch",
    "requires",
    "substantially",
    "less",
    "question",
    "answer",
    "data",
    "trained",
    "model",
    "scratch",
    "squeeze",
    "time",
    "time",
    "video",
    "paying",
    "attention",
    "let",
    "quiz",
    "find",
    "following",
    "architectures",
    "make",
    "use",
    "transfer",
    "learning",
    "bert",
    "b",
    "gpt",
    "c",
    "feed",
    "forward",
    "neural",
    "networks",
    "comment",
    "answer",
    "let",
    "discussion",
    "going",
    "quiz",
    "time",
    "keep",
    "paying",
    "attention",
    "back",
    "quiz",
    "pass",
    "going",
    "bert",
    "question",
    "answering",
    "data",
    "set",
    "notebook",
    "describe",
    "processes",
    "first",
    "loading",
    "training",
    "data",
    "going",
    "training",
    "data",
    "fed",
    "model",
    "loading",
    "training",
    "model",
    "postprocessing",
    "able",
    "interpret",
    "results",
    "model",
    "performance",
    "evaluation",
    "let",
    "go",
    "starting",
    "loading",
    "data",
    "set",
    "case",
    "going",
    "use",
    "squad",
    "version",
    "one",
    "means",
    "answers",
    "always",
    "present",
    "within",
    "context",
    "provided",
    "simplify",
    "process",
    "going",
    "use",
    "distilled",
    "bert",
    "batch",
    "size",
    "16",
    "means",
    "pass",
    "16",
    "examples",
    "network",
    "parallel",
    "get",
    "16",
    "results",
    "parallel",
    "data",
    "set",
    "training",
    "examples",
    "test",
    "examples",
    "shows",
    "need",
    "many",
    "examples",
    "actually",
    "model",
    "instead",
    "millions",
    "would",
    "potentially",
    "need",
    "training",
    "scratch",
    "record",
    "exactly",
    "training",
    "data",
    "looks",
    "important",
    "part",
    "context",
    "question",
    "answer",
    "answer",
    "see",
    "text",
    "answer",
    "respond",
    "corresponds",
    "text",
    "directly",
    "present",
    "within",
    "context",
    "consider",
    "unscriptural",
    "doctrines",
    "present",
    "right",
    "starting",
    "position",
    "also",
    "given",
    "position",
    "case",
    "step",
    "one",
    "loading",
    "data",
    "set",
    "complete",
    "want",
    "go",
    "step",
    "two",
    "actually",
    "training",
    "data",
    "first",
    "thing",
    "want",
    "concatenate",
    "question",
    "context",
    "break",
    "text",
    "individual",
    "tokens",
    "via",
    "auto",
    "tokenizer",
    "could",
    "see",
    "question",
    "context",
    "broken",
    "individual",
    "tokens",
    "padded",
    "using",
    "padding",
    "token",
    "required",
    "want",
    "fix",
    "size",
    "inputs",
    "mathematical",
    "inputs",
    "passed",
    "model",
    "given",
    "time",
    "also",
    "want",
    "fetch",
    "start",
    "position",
    "indicate",
    "index",
    "start",
    "answer",
    "index",
    "end",
    "answer",
    "already",
    "present",
    "context",
    "example",
    "33",
    "39",
    "fifth",
    "example",
    "right",
    "one",
    "right",
    "basically",
    "says",
    "think",
    "golden",
    "statue",
    "virgin",
    "mary",
    "answers",
    "question",
    "golden",
    "think",
    "33rd",
    "token",
    "whereas",
    "39th",
    "token",
    "go",
    "creating",
    "tokenized",
    "version",
    "plus",
    "padding",
    "plus",
    "know",
    "getting",
    "start",
    "end",
    "positions",
    "right",
    "answer",
    "well",
    "function",
    "right",
    "going",
    "called",
    "prepare",
    "training",
    "features",
    "able",
    "load",
    "data",
    "set",
    "able",
    "input",
    "data",
    "output",
    "data",
    "need",
    "load",
    "model",
    "architecture",
    "happens",
    "input",
    "tokens",
    "384",
    "tokens",
    "tokens",
    "going",
    "mapped",
    "768",
    "dimensional",
    "embedding",
    "vector",
    "position",
    "embedding",
    "allows",
    "tokens",
    "encode",
    "positional",
    "information",
    "position",
    "matters",
    "layer",
    "norm",
    "going",
    "help",
    "stabilize",
    "training",
    "dropout",
    "going",
    "act",
    "regularizer",
    "prevent",
    "mitigate",
    "overfitting",
    "transformer",
    "consists",
    "six",
    "encoder",
    "transformer",
    "blocks",
    "burt",
    "encoder",
    "transformer",
    "going",
    "perform",
    "series",
    "attention",
    "operations",
    "order",
    "better",
    "encapsulate",
    "meaning",
    "every",
    "single",
    "token",
    "output",
    "every",
    "single",
    "token",
    "going",
    "768",
    "dimensions",
    "better",
    "encapsulating",
    "meaning",
    "want",
    "get",
    "idea",
    "exactly",
    "every",
    "single",
    "component",
    "works",
    "highly",
    "recommend",
    "take",
    "look",
    "transformers",
    "scratch",
    "playlist",
    "going",
    "384",
    "tokens",
    "going",
    "mapped",
    "two",
    "dimensional",
    "output",
    "indicates",
    "probability",
    "start",
    "index",
    "probability",
    "token",
    "end",
    "index",
    "model",
    "dealing",
    "actually",
    "start",
    "training",
    "process",
    "fairly",
    "easily",
    "loaded",
    "data",
    "set",
    "data",
    "trained",
    "model",
    "let",
    "actually",
    "look",
    "prediction",
    "model",
    "code",
    "going",
    "take",
    "batch",
    "examples",
    "generate",
    "predictions",
    "going",
    "list",
    "384",
    "tokens",
    "indicates",
    "probabilities",
    "start",
    "index",
    "another",
    "384",
    "token",
    "list",
    "end",
    "index",
    "could",
    "see",
    "also",
    "batch",
    "size",
    "passing",
    "16",
    "examples",
    "time",
    "really",
    "need",
    "position",
    "like",
    "true",
    "position",
    "every",
    "single",
    "column",
    "actually",
    "take",
    "like",
    "maximum",
    "value",
    "means",
    "example",
    "going",
    "list",
    "16",
    "examples",
    "batch",
    "first",
    "example",
    "token",
    "46",
    "start",
    "ends",
    "token",
    "47",
    "answer",
    "second",
    "example",
    "answer",
    "starts",
    "token",
    "57",
    "ends",
    "token",
    "58",
    "third",
    "example",
    "answer",
    "starts",
    "token",
    "78",
    "ends",
    "token",
    "81",
    "done",
    "load",
    "data",
    "set",
    "model",
    "training",
    "postprocessing",
    "hardy",
    "evaluation",
    "evaluation",
    "ation",
    "set",
    "examples",
    "could",
    "see",
    "much",
    "exact",
    "match",
    "actual",
    "answers",
    "generated",
    "model",
    "answers",
    "actually",
    "true",
    "also",
    "generate",
    "evaluation",
    "metrics",
    "done",
    "push",
    "model",
    "hugging",
    "face",
    "hub",
    "quiz",
    "time",
    "right",
    "going",
    "fun",
    "one",
    "primary",
    "advantage",
    "using",
    "disle",
    "bert",
    "bert",
    "disle",
    "bert",
    "larger",
    "powerful",
    "b",
    "distill",
    "bert",
    "provides",
    "better",
    "interpretability",
    "c",
    "distill",
    "bert",
    "faster",
    "smaller",
    "memory",
    "footprint",
    "disal",
    "bt",
    "higher",
    "capacity",
    "model",
    "complexity",
    "comment",
    "answer",
    "let",
    "discussion",
    "think",
    "deserve",
    "please",
    "consider",
    "giving",
    "video",
    "like",
    "help",
    "lot",
    "going",
    "quiz",
    "time",
    "pass",
    "three",
    "explanation",
    "video",
    "go",
    "let",
    "generate",
    "music",
    "summary",
    "transfer",
    "learning",
    "involves",
    "model",
    "general",
    "task",
    "model",
    "specific",
    "task",
    "way",
    "need",
    "much",
    "task",
    "specific",
    "data",
    "bert",
    "uses",
    "transfer",
    "learning",
    "mass",
    "language",
    "modeling",
    "given",
    "mk",
    "input",
    "determine",
    "masks",
    "next",
    "sentence",
    "prediction",
    "given",
    "two",
    "sentences",
    "determine",
    "second",
    "sentence",
    "logically",
    "follows",
    "first",
    "use",
    "less",
    "data",
    "nlp",
    "tasks",
    "like",
    "question",
    "answering",
    "today",
    "links",
    "code",
    "resources",
    "provided",
    "description",
    "check",
    "want",
    "know",
    "bert",
    "works",
    "highly",
    "suggest",
    "check",
    "video",
    "right",
    "watch",
    "rest",
    "nlp",
    "playlist",
    "well",
    "thank",
    "much",
    "watching",
    "think",
    "deserve",
    "please",
    "give",
    "video",
    "like",
    "thank",
    "see",
    "next",
    "one"
  ],
  "keywords": [
    "transfer",
    "learning",
    "question",
    "skill",
    "would",
    "say",
    "teaching",
    "lot",
    "speaking",
    "time",
    "find",
    "please",
    "comment",
    "know",
    "better",
    "video",
    "going",
    "three",
    "start",
    "example",
    "exactly",
    "solve",
    "problem",
    "code",
    "pass",
    "neural",
    "network",
    "let",
    "want",
    "train",
    "take",
    "given",
    "produce",
    "answer",
    "data",
    "set",
    "1",
    "million",
    "answers",
    "training",
    "phase",
    "model",
    "process",
    "perform",
    "task",
    "english",
    "sentence",
    "french",
    "like",
    "sentences",
    "one",
    "size",
    "spanish",
    "need",
    "examples",
    "scratch",
    "first",
    "using",
    "trained",
    "starting",
    "much",
    "actually",
    "quiz",
    "paying",
    "attention",
    "b",
    "c",
    "discussion",
    "think",
    "deserve",
    "consider",
    "help",
    "nlp",
    "answering",
    "context",
    "output",
    "two",
    "numbers",
    "number",
    "index",
    "position",
    "second",
    "end",
    "use",
    "bert",
    "check",
    "encoder",
    "transformer",
    "mass",
    "language",
    "modeling",
    "next",
    "prediction",
    "input",
    "determine",
    "case",
    "plus",
    "text",
    "individual",
    "called",
    "token",
    "tokens",
    "version",
    "distill",
    "768",
    "dimensional",
    "embedding",
    "meaning",
    "384",
    "mapped",
    "probability",
    "get",
    "postprocessing",
    "loading",
    "able",
    "evaluation",
    "go",
    "means",
    "present",
    "batch",
    "16",
    "see",
    "right",
    "also",
    "could",
    "load",
    "every",
    "single",
    "generate",
    "list",
    "ends"
  ]
}