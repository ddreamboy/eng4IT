{
  "text": "hey guys in this video let's talk about\nEnsemble learning methods we'll cover\nthree common types of Ensemble begging\nboosting and stacking here are some\ninterview questions what is Ensemble\nlearning what are the examples of\nEnsemble learning what are boosting and\nbagging what are the advantages of\nbacking and boosting what are the\ndifferences between back-end boosting\nwhy boosting models are good explain\nstacking as you can tell being able to\nanswer these interview questions\nrequires you to have a clear\nunderstanding of different Ensemble\nlearning methods so in this video we'll\ngo over all of them and let's start with\nunderstanding what is Ensemble the idea\nbehind Ensemble method is pretty simple\nis that a group of weak Learners can\ncome together to form a strong learner\nand the strong learner will have better\npredictive performance than could be\nobtained from any of the base Learners\nalone if one base learner is erroneous\nit can be auto corrected by others so\nthe final model is typically less prone\nto overfeeding and more robust it's\nunlikely to be influenced by small\nchanges in the training data here's a\ndiagram showing the concept of Ensemble\nfor classification tasks we start with\nthe training set then we build\nclassification models based on this\ntraining set we have m models in total\nC1 C2 to CM and then each model has its\nown predictions and then we look at all\nthese predictions and we use voting to\nOutput the final prediction voting means\nthat we select the most frequent results\nacross all these predictions\nnow we know the general idea of Ensemble\nlet's look at one Ensemble learning\nmethod begging bagging is short for\nbootstrap aggregation it builds several\ninstances of an estimator on bootstrap\nsamples of the original training data\nand then aggregate their individual\npredictions to form a final prediction\nhere's a diagram showing how it works\nthe first step is to create bootstrap\nsamples simply with replacement from the\ntraining data here we have amp bootstrap\nsamples from T1 to TM the reason we\ncreate bootstrap samples is that we want\nto ensure each sample is independent\nfrom others as it does not depend on\nprevious student samples when creating\nbootstrap samples then we use a single\nlearning algorithm to build a model\nusing each bootstrap sample these models\ncan be built in parallel as you can see\nwe have C1 C2 to CM M classification\nmodels in total then we use mobile\nmodels to make predictions and the\npredictions are combined using voting or\naveraging this diagram shows backing for\nclassification tasks so it's using the\nvoting method voting means that the most\nfrequent result will be the final\nprediction and averaging for regression\ntasks means that we want to take the\naverage of all these predictions to be\nthe final prediction of the model an\nexample algorithm leveraging the back\ninvestor is random Forest here's how it\nworks random Forest combine a set of\ndecision trees to make predictions\nthe symmetries are good candidates for\nbagging because they can capture\ncomplicated interactions in the data and\nit grows sufficiently deep they have\nrelatively no bias because trees can be\nnoisy that benefit greatly from the\nvoting or average method to get the\nfinal prediction from a bias versus\nthree dollar point of view random virus\nstarts with low bias and high variance\nbecause its tree is fully grown and\ndecision trees tend to overfit and then\nthe random forests work towards reducing\nvariance by taking majority vote or\naveraging across trees all right now\nlet's move forward to boosting boosting\nimproves the prediction Power by\ntraining weak Learners sequentially each\ncompensating the weaknesses of its\npredecessors let me explain what this\nmeans both things start with a weak\nlearner gradually turned it in a strong\nlearner by letting future weak Learners\nfocus on correcting mistakes made by\nprevious learners for example in\nclassification tasks misclassified\nexamples get a higher weight than\nexamples that are classified correctly\nso future Learners focus more on\nexamples that previous Learners\nmisclassified so essentially we are\nmaking the weak learner more complicated\nand this process will help reduce the\nbias of the weak learner here's a\ndiagram showing how it works\nboth things start with a weak learner\nbetter than random gas you can see that\nin the first iteration some examples are\nclassified correctly and some are not in\nthe next iteration the weight of the\ndata are readjusted so that they can be\ncorrected\nboosting assigns higher weights to those\nthat were classified incorrectly and the\nlower weight to those that were\nclassified correctly\nthis sequential process of giving higher\nweights to misclassified predictions\ncontinues until a stopping Criterion is\nreached and the final prediction is the\nweighted result of all weak Learners one\nprobably example of boost investors is\ngradient boosted trees gradient boosted\ntrees trendy series sequentially it\noptimizes the residual loss of the tree\nby adding another tree it appears to art\nform backing on lots of problems and\nbecome the Preferred Choice from a\nbiased variance Trader point of view\ngradient boosted trees start with high\nbias and no balance because the first\nthree is shallow it's slightly better\nthan random gas and then they work\ntowards reducing bias by making the tree\nmore complicated now let's summarize the\ndifferences between these two Ensemble\nlearning methods backing and boosting in\nbagging individual Learners are trained\nindependently they can be built in\nparallel versus in boosting individual\nLearners are dependent on each other\nother remember that a new learner is\ncreated to correct mistakes of the\nprevious learner so the process is\nsequential from a bias variance\nperspective begging reduces variance of\nindividual Learners while boosting\nreduces bias by making individual\nLearners more complicated because of\nthis packing Masters work best with\nstrong and complex models for example\nfully developed decision trees while\nboosting methods usually work best with\nweak models for example shallow design\ntrees a good example of bagging is\nrandom forest and example of boosting is\na gradient boosted trees finally let's\nlook at stacking another Ensemble\nlearning method which is the last\nfrequently appears in interviews then\nback-end boosting the idea of stacking\nis to build a metal model that takes\noutput of Base Learners as inputs it\ncombines estimators to reduce their\nbiases it can be applied to\nclassification and regression tasks\nsimilar to backend boosting here's a\ndiagram of stacking and we can consider\nit as two level Ensemble method in the\nfirst level individual estimators are\ncreated based on the training data\nhere's a diagram showing how it works we\ncan consider stacking as two level\nEnsemble learning in the first level\nindividual estimators are created using\nthe training data and then a combiner\nestimator of meta learner is created to\nfit to the level 1 estimator predictions\nto make the final prediction for example\na static model for classification tasks\ncan look like this the individual\nclassifiers are random forest and\nsupport Vector machines and then we have\na meta learner or stacking classifier\nwhich is logistic regression to make the\nfinal prediction based on the output of\nindividual classifiers the stack method\nhas some pros and cons\nin practice a stacking predictor\npredicts as good as the best predictor\nof the base layer and sometimes\noutperforms it by combining the\ndifferent strengths of these predictors\nbut training a stacking predictor is\ncomputationally expensive\n",
  "words": [
    "hey",
    "guys",
    "video",
    "let",
    "talk",
    "ensemble",
    "learning",
    "methods",
    "cover",
    "three",
    "common",
    "types",
    "ensemble",
    "begging",
    "boosting",
    "stacking",
    "interview",
    "questions",
    "ensemble",
    "learning",
    "examples",
    "ensemble",
    "learning",
    "boosting",
    "bagging",
    "advantages",
    "backing",
    "boosting",
    "differences",
    "boosting",
    "boosting",
    "models",
    "good",
    "explain",
    "stacking",
    "tell",
    "able",
    "answer",
    "interview",
    "questions",
    "requires",
    "clear",
    "understanding",
    "different",
    "ensemble",
    "learning",
    "methods",
    "video",
    "go",
    "let",
    "start",
    "understanding",
    "ensemble",
    "idea",
    "behind",
    "ensemble",
    "method",
    "pretty",
    "simple",
    "group",
    "weak",
    "learners",
    "come",
    "together",
    "form",
    "strong",
    "learner",
    "strong",
    "learner",
    "better",
    "predictive",
    "performance",
    "could",
    "obtained",
    "base",
    "learners",
    "alone",
    "one",
    "base",
    "learner",
    "erroneous",
    "auto",
    "corrected",
    "others",
    "final",
    "model",
    "typically",
    "less",
    "prone",
    "overfeeding",
    "robust",
    "unlikely",
    "influenced",
    "small",
    "changes",
    "training",
    "data",
    "diagram",
    "showing",
    "concept",
    "ensemble",
    "classification",
    "tasks",
    "start",
    "training",
    "set",
    "build",
    "classification",
    "models",
    "based",
    "training",
    "set",
    "models",
    "total",
    "c1",
    "c2",
    "cm",
    "model",
    "predictions",
    "look",
    "predictions",
    "use",
    "voting",
    "output",
    "final",
    "prediction",
    "voting",
    "means",
    "select",
    "frequent",
    "results",
    "across",
    "predictions",
    "know",
    "general",
    "idea",
    "ensemble",
    "let",
    "look",
    "one",
    "ensemble",
    "learning",
    "method",
    "begging",
    "bagging",
    "short",
    "bootstrap",
    "aggregation",
    "builds",
    "several",
    "instances",
    "estimator",
    "bootstrap",
    "samples",
    "original",
    "training",
    "data",
    "aggregate",
    "individual",
    "predictions",
    "form",
    "final",
    "prediction",
    "diagram",
    "showing",
    "works",
    "first",
    "step",
    "create",
    "bootstrap",
    "samples",
    "simply",
    "replacement",
    "training",
    "data",
    "amp",
    "bootstrap",
    "samples",
    "t1",
    "tm",
    "reason",
    "create",
    "bootstrap",
    "samples",
    "want",
    "ensure",
    "sample",
    "independent",
    "others",
    "depend",
    "previous",
    "student",
    "samples",
    "creating",
    "bootstrap",
    "samples",
    "use",
    "single",
    "learning",
    "algorithm",
    "build",
    "model",
    "using",
    "bootstrap",
    "sample",
    "models",
    "built",
    "parallel",
    "see",
    "c1",
    "c2",
    "cm",
    "classification",
    "models",
    "total",
    "use",
    "mobile",
    "models",
    "make",
    "predictions",
    "predictions",
    "combined",
    "using",
    "voting",
    "averaging",
    "diagram",
    "shows",
    "backing",
    "classification",
    "tasks",
    "using",
    "voting",
    "method",
    "voting",
    "means",
    "frequent",
    "result",
    "final",
    "prediction",
    "averaging",
    "regression",
    "tasks",
    "means",
    "want",
    "take",
    "average",
    "predictions",
    "final",
    "prediction",
    "model",
    "example",
    "algorithm",
    "leveraging",
    "back",
    "investor",
    "random",
    "forest",
    "works",
    "random",
    "forest",
    "combine",
    "set",
    "decision",
    "trees",
    "make",
    "predictions",
    "symmetries",
    "good",
    "candidates",
    "bagging",
    "capture",
    "complicated",
    "interactions",
    "data",
    "grows",
    "sufficiently",
    "deep",
    "relatively",
    "bias",
    "trees",
    "noisy",
    "benefit",
    "greatly",
    "voting",
    "average",
    "method",
    "get",
    "final",
    "prediction",
    "bias",
    "versus",
    "three",
    "dollar",
    "point",
    "view",
    "random",
    "virus",
    "starts",
    "low",
    "bias",
    "high",
    "variance",
    "tree",
    "fully",
    "grown",
    "decision",
    "trees",
    "tend",
    "overfit",
    "random",
    "forests",
    "work",
    "towards",
    "reducing",
    "variance",
    "taking",
    "majority",
    "vote",
    "averaging",
    "across",
    "trees",
    "right",
    "let",
    "move",
    "forward",
    "boosting",
    "boosting",
    "improves",
    "prediction",
    "power",
    "training",
    "weak",
    "learners",
    "sequentially",
    "compensating",
    "weaknesses",
    "predecessors",
    "let",
    "explain",
    "means",
    "things",
    "start",
    "weak",
    "learner",
    "gradually",
    "turned",
    "strong",
    "learner",
    "letting",
    "future",
    "weak",
    "learners",
    "focus",
    "correcting",
    "mistakes",
    "made",
    "previous",
    "learners",
    "example",
    "classification",
    "tasks",
    "misclassified",
    "examples",
    "get",
    "higher",
    "weight",
    "examples",
    "classified",
    "correctly",
    "future",
    "learners",
    "focus",
    "examples",
    "previous",
    "learners",
    "misclassified",
    "essentially",
    "making",
    "weak",
    "learner",
    "complicated",
    "process",
    "help",
    "reduce",
    "bias",
    "weak",
    "learner",
    "diagram",
    "showing",
    "works",
    "things",
    "start",
    "weak",
    "learner",
    "better",
    "random",
    "gas",
    "see",
    "first",
    "iteration",
    "examples",
    "classified",
    "correctly",
    "next",
    "iteration",
    "weight",
    "data",
    "readjusted",
    "corrected",
    "boosting",
    "assigns",
    "higher",
    "weights",
    "classified",
    "incorrectly",
    "lower",
    "weight",
    "classified",
    "correctly",
    "sequential",
    "process",
    "giving",
    "higher",
    "weights",
    "misclassified",
    "predictions",
    "continues",
    "stopping",
    "criterion",
    "reached",
    "final",
    "prediction",
    "weighted",
    "result",
    "weak",
    "learners",
    "one",
    "probably",
    "example",
    "boost",
    "investors",
    "gradient",
    "boosted",
    "trees",
    "gradient",
    "boosted",
    "trees",
    "trendy",
    "series",
    "sequentially",
    "optimizes",
    "residual",
    "loss",
    "tree",
    "adding",
    "another",
    "tree",
    "appears",
    "art",
    "form",
    "backing",
    "lots",
    "problems",
    "become",
    "preferred",
    "choice",
    "biased",
    "variance",
    "trader",
    "point",
    "view",
    "gradient",
    "boosted",
    "trees",
    "start",
    "high",
    "bias",
    "balance",
    "first",
    "three",
    "shallow",
    "slightly",
    "better",
    "random",
    "gas",
    "work",
    "towards",
    "reducing",
    "bias",
    "making",
    "tree",
    "complicated",
    "let",
    "summarize",
    "differences",
    "two",
    "ensemble",
    "learning",
    "methods",
    "backing",
    "boosting",
    "bagging",
    "individual",
    "learners",
    "trained",
    "independently",
    "built",
    "parallel",
    "versus",
    "boosting",
    "individual",
    "learners",
    "dependent",
    "remember",
    "new",
    "learner",
    "created",
    "correct",
    "mistakes",
    "previous",
    "learner",
    "process",
    "sequential",
    "bias",
    "variance",
    "perspective",
    "begging",
    "reduces",
    "variance",
    "individual",
    "learners",
    "boosting",
    "reduces",
    "bias",
    "making",
    "individual",
    "learners",
    "complicated",
    "packing",
    "masters",
    "work",
    "best",
    "strong",
    "complex",
    "models",
    "example",
    "fully",
    "developed",
    "decision",
    "trees",
    "boosting",
    "methods",
    "usually",
    "work",
    "best",
    "weak",
    "models",
    "example",
    "shallow",
    "design",
    "trees",
    "good",
    "example",
    "bagging",
    "random",
    "forest",
    "example",
    "boosting",
    "gradient",
    "boosted",
    "trees",
    "finally",
    "let",
    "look",
    "stacking",
    "another",
    "ensemble",
    "learning",
    "method",
    "last",
    "frequently",
    "appears",
    "interviews",
    "boosting",
    "idea",
    "stacking",
    "build",
    "metal",
    "model",
    "takes",
    "output",
    "base",
    "learners",
    "inputs",
    "combines",
    "estimators",
    "reduce",
    "biases",
    "applied",
    "classification",
    "regression",
    "tasks",
    "similar",
    "backend",
    "boosting",
    "diagram",
    "stacking",
    "consider",
    "two",
    "level",
    "ensemble",
    "method",
    "first",
    "level",
    "individual",
    "estimators",
    "created",
    "based",
    "training",
    "data",
    "diagram",
    "showing",
    "works",
    "consider",
    "stacking",
    "two",
    "level",
    "ensemble",
    "learning",
    "first",
    "level",
    "individual",
    "estimators",
    "created",
    "using",
    "training",
    "data",
    "combiner",
    "estimator",
    "meta",
    "learner",
    "created",
    "fit",
    "level",
    "1",
    "estimator",
    "predictions",
    "make",
    "final",
    "prediction",
    "example",
    "static",
    "model",
    "classification",
    "tasks",
    "look",
    "like",
    "individual",
    "classifiers",
    "random",
    "forest",
    "support",
    "vector",
    "machines",
    "meta",
    "learner",
    "stacking",
    "classifier",
    "logistic",
    "regression",
    "make",
    "final",
    "prediction",
    "based",
    "output",
    "individual",
    "classifiers",
    "stack",
    "method",
    "pros",
    "cons",
    "practice",
    "stacking",
    "predictor",
    "predicts",
    "good",
    "best",
    "predictor",
    "base",
    "layer",
    "sometimes",
    "outperforms",
    "combining",
    "different",
    "strengths",
    "predictors",
    "training",
    "stacking",
    "predictor",
    "computationally",
    "expensive"
  ],
  "keywords": [
    "let",
    "ensemble",
    "learning",
    "methods",
    "three",
    "begging",
    "boosting",
    "stacking",
    "examples",
    "bagging",
    "backing",
    "models",
    "good",
    "start",
    "idea",
    "method",
    "weak",
    "learners",
    "form",
    "strong",
    "learner",
    "better",
    "base",
    "one",
    "final",
    "model",
    "training",
    "data",
    "diagram",
    "showing",
    "classification",
    "tasks",
    "set",
    "build",
    "based",
    "predictions",
    "look",
    "use",
    "voting",
    "output",
    "prediction",
    "means",
    "bootstrap",
    "estimator",
    "samples",
    "individual",
    "works",
    "first",
    "previous",
    "using",
    "make",
    "averaging",
    "regression",
    "example",
    "random",
    "forest",
    "decision",
    "trees",
    "complicated",
    "bias",
    "variance",
    "tree",
    "work",
    "misclassified",
    "higher",
    "weight",
    "classified",
    "correctly",
    "making",
    "process",
    "gradient",
    "boosted",
    "two",
    "created",
    "best",
    "estimators",
    "level",
    "predictor"
  ]
}