{
  "text": "Hey, I'm Andy from deep lizard.\nAnd in this course, we're going to learn how\nto use Kerris, and neural network API written\nin Python and integrated with TensorFlow.\nThroughout the course, each lesson will focus\non a specific deep learning concept, and show\nthe full implementation in code using the\nkeras API.\nWe'll be starting with the absolute basics\nof learning how to organize and pre processed\ndata, and then we'll move on to building and\ntraining our own artificial neural networks.\nAnd some of these networks will be built from\nscratch, and others will be pre trained and\nstate of the art models for which we'll fine\ntune to use on our own custom data sets.\nNow let's discuss the prerequisites needed\nto follow along with this course.\nFrom a knowledge standpoint, we'll give brief\nintroductions of each deep learning concept\nthat we are going to work with before we go\nthrough the code implementation.\nBut if you're an absolute beginner to deep\nlearning, then we first recommend you go through\nthe deep learning fundamentals scores on the\ntables or Comm.\nOr if you're super eager to jump into the\ncode, then you can simultaneously take this\ncourse and the deep learning fundamentals\ncourse, the deep learning fundamentals course\nwill give you all the knowledge you need to\nknow to get acquainted with major deep learning\nconcepts for which then you can come back\nand implement and code using the keras API.\nAnd this course, in regard to coding prerequisites,\njust some basic programming skills and some\nPython experience or all that's needed.\nOn peoples.com.\nYou can also find the deep learning learning\npath.\nSo you can see where this Kerris course falls\namidst all the deep lizard deep learning content.\nNow let's discuss the Course Resources.\nSo aside from the videos here on YouTube,\nyou will also have video and text resources\navailable on peoples are calm.\nAnd actually each episode has its own corresponding\nblog and a quiz available for you to take\nand test your own knowledge.\nAnd you can actually contribute your own quiz\nquestions as well.\nAnd you can see how to do that on the corresponding\nblog for each episode.\nAdditionally, all of the code resources used\nin this course, are regularly tested and maintained,\nincluding updates and bug fixes when needed.\nDownload access to code files used in this\ncourse, are available to members of the deep\nlizard hive mind.\nSo you can check out more about that on tables\nor calm as well.\nAlright, so now that we know what this course\nis about, and what resources we have available,\nalong with the prerequisites needed to get\nstarted, let's now talk a little bit more\nabout Kerris itself.\nKerris was developed with a focus on enabling\nfast user experimentation.\nSo this allows us to go from idea to implementation\nand very few steps.\nAside from this benefit, users often wonder\nwhy choose Kerris as the neural network API\nto learn or in general, which neural network\nAPI should they learn?\nOur general advice is to not commit yourself\njust to only learning one and sticking with\nthat one forever, we recommend to learn multiple\nneural network API's.\nAnd the idea is that once you have a fundamental\nunderstanding of the underlying concepts,\nthen the minor syntactical and implementation\ndifferences between the neural network API's\nshouldn't really be that hard to catch on\nto once you have at least one under your belt\nalready, especially for job prospects.\nKnowing more than one neural network API will\nshow your experience and allow you to compare\nand contrast the differences between API's\nand share your opinions for why you think\nthat certain API's may be better for certain\nproblems, and others for other problems.\nBeing able to demonstrate this will make you\na much more valuable candidate.\nNow, we previously touched on the fact that\nKerris is integrated with TensorFlow.\nSo let's discuss that more.\nNow.\nHistorically, Kerris was a high level neural\nnetwork API that you could configure to run\nagainst one of three separate lower level\nAPI's.\nAnd those lower level API's were TensorFlow\nviano and cntk.\nLater, though, Kerris became fully integrated\nwith the TensorFlow API, and is no longer\na separate library that you can choose to\nrun against one of the three back end engines\nthat we discussed previously.\nSo it's important to understand that cares\nis now completely integrated with the TensorFlow\nAPI.\nBut in this course, we are going to be focusing\non making use solely of that high level Kerris\nAPI without necessarily making much use of\nthe lower level TensorFlow API.\nNow, before we can start working with cares,\nthen we first have to obviously get it downloaded\nand installed onto our machines.\nAnd because Kerris is fully integrated with\nTensorFlow, we can do that by just installing\nTensorFlow Kerris will come completely packaged\nwith the TensorFlow installation.\nSo the installation procedure is as simple\nas running pip install TensorFlow from your\ncommand line, you might just want to check\nout the system requirements on tensor flows\nwebsite to make sure that your specific system\nwe The requirements needed for TensorFlow\nto install.\nAlright, so we have one last talking point\nbefore we can get into the actual meat of\nthe coding.\nAnd that is about GPU support.\nSo the first important thing to note is that\na GPU is not required to follow this course,\nif you're running your machine only on a CPU,\nthen that is totally fine for what we'll be\ndoing in the course.\nIf however, you do want to run your code on\na GPU, then you can do so pretty easily.\nAfter you get through the setup process for\nsetting up your GPU to work with TensorFlow,\nwe have a full guide on how to get the GPU\nsetup to work with TensorFlow on deep lizard\nComm.\nSo if you are interested in doing that, then\nhead over there to go through those steps.\nBut actually, I recommend just going through\nthe course with a CPU if you're not already\nset up with a GPU.\nAnd like I said, the all the code will work\ncompletely fine, run totally fine using only\na CPU.\nBut then after the fact, after you go through\nthe course successfully, then get the steps\nin order to work with the GPU if you have\none, and then run all the code that you have\nin place from earlier, run it on the GPU,\nthe second go round and just kind of look\nand see the kind of efficiency and speed ups\nthat you'll see in the code.\nAlright, so that's it for the Kerris introduction.\nNow we're finally ready to jump in to the\ncode.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the dibbles at hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow, let's move on to the next episode.\nHey, I may be from deep lizard.\nIn this episode, we'll learn how to prepare\nand process numerical data that will later\nuse to train our very first artificial neural\nnetwork.\nTo train any neural network and a supervised\nlearning task, we first need a data set of\nsamples along with the corresponding labels\nfor those samples.\nWhen referring to the word samples, we're\nsimply just talking about the underlying data\nset, where each data point in the set is referred\nto as a sample.\nIf we were to train a model to do sentiment\nanalysis on headlines from a media source,\nfor example, then the labels that correspond\nto each headline sample would be positive\nor negative.\nOr say that we were training an artificial\nneural network to identify images as cats\nor dogs, well, then that would mean that each\nimage would have a corresponding label of\ncat or dog.\nNote that in deep learning, you may also hear\nsamples referred to as inputs or input data.\nAnd you may hear labels referred to as targets\nor target data.\nWhen preparing a data set, we first need to\nunderstand the task for which the data will\nbe used.\nIn our example, we'll be using our data set\nto train an artificial neural network.\nSo once we understand this, then we can understand\nwhich format the data needs to be in, in order\nfor us to be able to pass the data to the\nnetwork.\nThe first type of neural network that we'll\nbe working with is called a sequential model\nfrom the cares API.\nAnd we'll discuss more details about the sequential\nmodel in a future episode.\nBut for now, we just need to understand what\ntype of data format the sequential model expects,\nso that we can prepare our dataset accordingly.\nThe sequential model receives data during\ntraining whenever we call the fit function\non it.\nAnd again, we're going to go into more details\nabout this function in the future.\nBut for now, let's check out what type of\ndata format the fit function expects.\nSo if we look at the fit documentation here,\non tensor flows website, we can see the first\ntwo parameters that the fit function expects\nare x and y.\nSo x is our input data, our samples, in other\nwords, so this function expects x our input\ndata to be in a NumPy array, a TensorFlow\ntensor, a dict, mapping, a TF dot data data\nset or a karass generator.\nSo if you're not familiar with all of these\ndata types, that's okay, because for our first\nexample, we are going to organize our data\nto be in a NumPy array.\nSo the first option here in the documentation,\nso this is for our input samples, but also\nthe Y parameter that is expected by the fit\nfunction is our data set that contains the\ncorresponding labels for our samples.\nSo the target data.\nNow the requirement for why is that it is\nformatted as one of the above formats that\nwe just discussed for x, but y needs to be\nin the same format as x.\nSo we can't have our samples contained in\na NumPy array, for example, and then have\ny Our target data or our labels for those\nsamples and the TensorFlow tensor.\nSo the format of x and y, both need to match.\nAnd we're going to be putting both of those\ninto NumPy arrays.\nAlright, so now we know the data format that\nthe model expects.\nBut there's another reason that we may want\nto transform or process our data.\nAnd that is to put it in a format that will\nmake it easier or more efficient for the model\nto learn from.\nAnd we can do that with data normalization\nor standardization techniques, data processing,\nand deep learning will vary greatly depending\non the type of data that we're working with.\nSo to start out, we are going to work with\na very simple numerical data set to train\nour model.\nAnd later, we'll get exposure to working with\ndifferent types of data as well.\nAlright, so now we're ready to prepare and\nprocess our first data set.\nSo we are in our Jupyter Notebook.\nAnd the first step is to import all the packages\nthat we'll be making use of including NumPy\nrandom and some modules from scikit learn.\nNext, we will create two list one called train\nsamples, one called train labels, and these\nlists will hold the corresponding samples\nand labels for our data set.\nNow about this data set, we are going to be\nworking with a very simple numerical data\nset.\nAnd so for this task, we're actually going\nto create the data ourselves.\nLater, we'll work with more practical examples\nand realistic ones where where we won't be\ncreating the data, but instead downloading\nit from some external source.\nBut for now, we're going to create this data\nourselves for which will train our first artificial\nneural network.\nSo as a motivation for this kind of dummy\ndata, we have this background story to give\nus an idea of what this data is all about.\nSo let's suppose that an experimental drug\nwas tested on individuals ranging from age\n13 to 100.\nIn a clinical trial, this trial had 2100 participants\ntotal, half of these participants were under\nthe age of 65.\nAnd half were 65 years or older.\nAnd the conclusions from this trial was that\naround 95% of the patients who were in the\nolder population, so 65 or older, the 95%\nof those patients experienced side effects.\nAnd around 95% of patients who were under\n65 years old, experienced no side effects.\nOkay, so this is a very simplistic data set.\nAnd so that is the background story.\nNow in this cell, here, we're going to go\nthrough the process of actually creating that\ndata set.\nSo what this is, is this first for loop is\ngoing to generate both the approximately 5%\nof younger individuals who did experience\nside effects, and the 5% of older individuals\nwho did not experience side effects.\nSo within this first for loop, we are first\ngenerating a random number or a random integer\nrather between 13 and 64.\nAnd that is constituting as a younger individual\nwho is under 65 years of age, or yet who's\nunder 65 years of age.\nAnd we are going to then append this number\nto the train samples list.\nAnd then we append a one to the train labels\nlist.\nNow a one is representing the fact that a\npatient did experience side effects.\nAnd a zero would represent a patient who did\nnot experience side effects.\nSo then similarly, we jumped down to the next\nline.\nAnd we are generating a random integer between\n65 in 100, to represent the older population.\nAnd we are doing that Remember, this is in\nour first for loop.\nSo it's only running 50 times.\nSo this is kind of the outlier group, these\n5% of older individuals who did not experience\nside effects.\nSo we then take that sample appended to the\ntrain samples list, and append a zero to the\ncorresponding train labels list.\nSince these patients were the older patients\nwho did not experience side effects.\nSo then if we jumped down to the next for\nloop, very similarly, we have pretty much\nthe same code here, except for this is the\nbulk of the group.\nAnd this for loop, we're running this for\nthe 95% of younger individuals who did not\nexperience side effects, as well as the 95%\nof older individuals who did experience side\neffects.\nSo generating a random number between 13 and\nsix before and then appending that number,\nrepresenting the age of the younger population\nto the train samples list, and appending the\nlabel of zero.\nSince these individuals did not experience\nside effects, we're appending zero to the\ntrain labels list.\nSimilarly, we do the same thing for the older\nindividuals from 65 to 100.\nExcept for since the majority of these guys\ndid experience side effects, we are appending\na one to the train The labels list.\nSo just to summarize, we have the samples\nlist that contains a bunch of integers ranging\nfrom 13 to 100.\nAnd then we have the train labels list that\nhas the labels that correspond to each of\nthese individuals with the ages 13 to 100.\nThe labels correspond to whether or not these\nindividuals experience side effects.\nSo a samples list containing ages, a labels\nlist containing zeros and ones representing\nside effects, or no side effects.\nAnd just to get a visualization of the samples,\nhere, we are printing out all of the samples\nin our list.\nAnd we see that these are just all integers,\nlike we'd expect, ranging from 13 to 100.\nAnd then correspondingly, if we run our train\nlabels list and print out all of the data\nthere, then we can see this list contains\na bunch of zeros and ones.\nAlright, so now the next step is to take these\nlists and then start processing them.\nSo we have our data generated.\nNow we need to process it to be in the format\nfor which we saw the fit function expects,\nand we discussed the fact that we are going\nto be passing this data as NumPy arrays to\nthe fit function.\nSo our next step is to go ahead and do that\ntransformation here, where we are taking the\ntrain labels list and making that now a NumPy\narray.\nSimilarly, doing the same thing with the train\nsamples list.\nAnd then we use the shuffle function to shuffle\nboth are trained labels and trained samples\nrespective to each other so that we can get\nrid of any imposed order from the data generation\nprocess.\nOkay, so now the data is in the NumPy array\nformat that is expected by the fit function.\nBut as mentioned earlier, there's another\nreason that we might want to do further processing\non the data.\nAnd that is to either normalize or standardize\nit so that we can get it in such a way that\nthe training of the neural network might become\nquicker or more efficient.\nAnd so that's what we're doing in this cell.\nNow we are using this min max scalar object\nto create a feature range ranging from zero\nto one, which we'll then use in this next\nline to rescale our data from the current\nscale of 13 to 100, down to a scale of zero\nto one, and then this reshaping that we're\ndoing here is just a formality because the\nfit transform function doesn't accept one\nD data by default.\nSince our data is one dimensional, we have\nto reshape it in this way to be able to pass\nit to the fit transform function.\nSo now if we print out the elements in our\nnew scaled train samples variable, which that's\nwhat we're calling our new scaled samples,\nthen we can print them out and see that now\nthe individual elements are no longer integers\nranging from 13 to 100.\nBut instead, we have these values ranging\nanywhere between zero and one.\nSo at this point, we have generated some raw\ndata, and then processed it to be in a NumPy\narray format that our model will expect.\nAnd then rescale the data to be on a scale\nbetween zero and one, and an upcoming episode,\nwe'll use this data to train our first artificial\nneural network, be sure to check out the blog\nand other resources available for this episode\non deep lizard calm, as well as the deep lizard\nhive mind where you can gain access to exclusive\nperks and rewards.\nThanks for contributing to collective intelligence.\nNow, let's move on to the next episode.\nHey, I'm Andy from Blizzard.\nAnd this episode will demonstrate how to create\nan artificial neural network using a sequential\nmodel from the keras API integrated within\ntensor flow.\nIn the last episode, we generated data from\nan imagined clinical trial.\nAnd now we'll create an artificial neural\nnetwork for which we can train on this data.\nAlright, so first things first, we need to\nimport all of the TensorFlow modules that\nwe'll be making use of to build our first\nmodel.\nAnd that includes everything that you see\nhere except for actually the last two of atom\nand categorical cross and vitry.\ncategorical cross entropy, rather, those two\nare going to be used when we train the model,\nnot when we build it.\nBut we're going ahead and bringing all the\nimports in now.\nAnd then next, if you are running this code\non a GPU, then you can run this cell which\nwill allow you to make sure that TensorFlow\nis correctly identifying your GPU as well\nas enable memory growth.\nAnd there's a few lines on the blog where\nyou can check out what exactly that means\nand why you might want to do this.\nBut if you are running a GPU then go ahead\nand run this cell.\nSo next, this is the The actual model that\nwe are building this is a sequential model.\nAnd that is kind of the most simplest type\nof model that you can build using Kerris or\nTensorFlow.\nAnd a sequential model can be described as\na linear stack of layers.\nSo if you look at how we're creating the model\nhere, that's exactly what it looks like.\nSo we are initializing the model as an instance\nof the sequential class.\nAnd we are passing in a list of layers here.\nNow, it's important to note that this first\ndense layer that we're looking at, that is\nactually the second layer overall.\nSo this is the first hidden layer.\nAnd that's because the input layer, we're\nnot explicitly defining using Kerris, the\ninput data is what creates the input layer\nitself.\nSo the way that the model knows what type\nof input data to expect, or the shape of the\ninput data, rather, is through this input\nshape parameter that we pass to our first\ndense layer.\nSo through this, the model understands the\nshape of the input data that it should expect.\nAnd then therefore it accepts that shape,\nhave input data, and then passes that data\nto the first hidden layer, which is this dense\nlayer here in our case, now, we are telling\nthis dense layer that we want it to have 16\nunits.\nThese units are also otherwise known as nodes\nor neurons.\nAnd the choice of 16 here is actually pretty\narbitrary.\nThis model overall is very simple.\nAnd with the arbitrary choice of nodes here,\nit's actually going to be pretty hard to create\na simple model, at least, that won't do a\ngood job at classifying this data, just given\nthe simplicity of the data itself.\nSo we understand that we're passing in, or\nthat we're specifying 16 units for this first\nhidden layer, we are specifying the input\nshape, so that the model knows the shape of\nthe input data to expect and then we are stating\nthat we want the relu activation function\nto follow this dense layer.\nNow, this input shape parameter is only specified\nfor our first hidden layer.\nSo after that, we have one more hidden, dense\nlayer.\nThis time, we are arbitrarily setting the\nnumber of units for this dense layer to be\n32.\nAnd again, we're following the layer by the\nactivation function value.\nAnd then lastly, we specify our last output\nlayer or our last layer, which is our output\nlayer.\nThis is another dense layer, this time with\nonly two units, and that is corresponding\nto the two possible output classes.\nEither a patient did experience side effects,\nor the patient did not experience side effects.\nAnd we're following this output layer with\nthe softmax function, which is just going\nto give us probabilities for each output class.\nSo between whether or not a patient experience\nside effects or not, we will have an output\nprobability for each class, letting us know\nwhich class is more probable for any given\npatient.\nAnd just in case, it's not clear, this dense\nlayer here is what we call a densely connected\nlayer or a fully connected layer, probably\nthe most well known type of layer and an artificial\nneural networks.\nNow in case you need any refresher on fully\nconnected layers, or activation functions,\nor anything else that we've discussed up to\nthis point, then just know that all of that\nis covered in the deep learning fundamentals\ncourse, if you need to go there and refresh\nyour memory on any of these topics here.\nAlright, so now we'll run this cell to create\nour model.\nAnd now we can use model dot summary to print\nout a visual summary of the architecture of\nthe model we just created.\nSo looking here, we can just see the visual\nrepresentation of the architecture that we\njust created in the cell above.\nAll right, so now we have just finished creating\nour very first neural network using this simple\nand intuitive sequential model type.\nIn the next episode, we will see how we can\nuse the data that we created last time to\ntrain this network.\nBe sure to check out the blog and other resources\navailable for this episode on depot's or.com\nas well as the tables at hive mind where you\ncan gain access to exclusive perks and rewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nIn this episode, we'll see how to train an\nartificial neural network using the keras\nAPI integrated with TensorFlow.\nIn previous episodes, we went through steps\nto generate data and also build an Artificial\nneural network.\nSo now we'll bring these two together to actually\ntrain the network on the data that we created\nand processed.\nAlright, so picking up where we were last\ntime in our Jupyter Notebook, make sure that\nyou still have all of your imports included\nand already ran so that we can continue where\nwe were before.\nSo first we have after building our model,\nwe are going to call this model compile function.\nAnd this just prepares the model for training.\nSo it gets everything in order that's needed\nbefore we can actually train the model.\nSo first, we are specifying to the compile\nfunction, what optimizer that we want to use,\nand we're choosing to use the very common\noptimizer atom with a learning rate of 0.0001.\nAnd next we specify the type of loss that\nwe need to use, which is in our case, we're\ngoing to use sparse categorical cross in but\nwe're going to use sparse categorical cross\nentropy.\nAnd then lastly, we specify what metrics we\nwant to see.\nSo this is just for the model performance,\nwhat we want to be able to judge our model\nby and we are specifying this list, which\njust includes accuracy, which is a very common\nway to be able to evaluate model performance.\nSo if we run this cell, alright, so the model\nhas been compiled and is ready for training.\nAnd training occurs whenever we call this\nfit function.\nNow recall earlier in the course, we actually\nlooked at the documentation for the split\nfunction, so that we knew how to process our\ninput data.\nSo to fit the first parameter that we're specifying\nis x here, which is our input data, which\nis currently stored in this scaled at train\nsamples variable, then y, which is our target\ndata, or our labels are labels are currently\nstored in the train labels variable.\nSo we are specifying that here.\nNext, we specify our batch size that we want\nto use for training.\nSo this is how many samples are included in\none batch to be passed and processed by the\nnetwork at one time.\nSo we're setting this to 10.\nAnd the number of epochs that we want to run,\nwe're setting this to 30.\nSo that means that the model is going to process\nor train on all of the data in the data set\n30 times before completing the total training\nprocess.\nNext, we're specifying this shuffle, shuffle\nparameter which we are setting to true.\nNow, by default, this is already set to true,\nbut I was just bringing it to your attention\nto show or to make you aware of the fact that\nthe data is being shuffled by default, when\nwe pass it to the network, which is a good\nthing, because we want any order that is inside\nof the dataset to be kind of erased before\nwe pass the data to the model so that the\nmodel is not necessarily learning anything\nabout the order of the data set.\nSo this is true by default.\nSo we don't necessarily have to specify that\nI was just letting you know.\nAnd actually, we'll see something about that\nin the next episode about why this is important\nregarding validation data, but we'll see that\ncoming up.\nThe last parameter that we specify here is\nverbose, which is just an option to allow\nus to see output from whenever we run this\nfit function.\nSo we can either set it to 01, or two, two\nis the most verbose level in terms of output\nmessages.\nSo we are setting that here so that we can\nget the highest level of output.\nSo now let's run this cell so that training\ncan begin.\nAll right, so training has just stopped.\nAnd we have run for 30 epochs.\nAnd if we look at the progress of the model,\nso we're starting out on our first epoch,\nour loss value is currently point six, eight\nand our accuracy is 50%.\nSo no better than chance.\nBut pretty quickly looking at the accuracy,\nwe can tell that it is steadily increasing\nall the way until we get to our last a POC,\nwhich we are yielding 94% accuracy.\nAnd our loss has also steadily decreased from\nthe point six five range to now being at point\ntwo seven.\nSo as you can see this model train very quickly,\nwith each epoch taking only under one second\nto run.\nAnd within 30 epochs, we are already at a\n94% accuracy right.\nNow although this is a very simple model,\nand we were training it on a very simple data,\nwe can see that without much effort at all,\nwe were able to yield pretty great results\nin a relatively quick manner of time as well.\nIn subsequent episodes, we'll demonstrate\nhow to work with more complex models as well\nas more complex data.\nBut for now, hopefully this example served\nthe purpose of encouraging you on how easy\nit is to get started with Kerris.\nBe sure to check out the blog and other resources\navailable for this episode on V bowser.com.\nAs well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nIn this episode, we'll demonstrate how we\ncan use tensor flows keras API to create a\nvalidation set on the fly during training.\nBefore we demonstrate how to build a validation\nset using Kerris, let's first talk about what\nexactly a validation set is.\nSo whenever we train a model, our hope is\nthat when we train it that we see good results\nfrom the training output, that we have low\nloss and high accuracy.\nBut we don't ever train a model just for the\nsake of training it, we want to take that\nmodel and hopefully be able to use it in some\nway on data that it wasn't necessarily exposed\nto during the training process.\nAnd although this new data is data that the\nmodel has never seen before, the hope is that\nthe model will be good enough to be able to\ngeneralize well on this new data and give\naccurate predictions for it, we can actually\nget an understanding of how well our model\nis generalizing by introducing a validation\nset during the training process to create\na validation set.\nBefore training begins, we can choose to take\na subset of the training set, and then separate\nit into a separate set labeled as validation\ndata.\nAnd then during the training process, the\nmodel will only train on the training data,\nand then we'll validate on the separated validation\ndata.\nSo what do we mean by validating?\nWell, essentially, if we have the addition\nof a validation set, then during training,\nthe model will be learning the features of\nthe training set, just as we've already seen.\nBut in addition, in each epoch, after the\nmodel has gone through the actual training\nprocess, it will take what it's learned from\nthe training data, and then validate by predicting\non the data in the validation set, using only\nwhat it's learned from the training data,\nthough.\nSo then during the training process, when\nwe look at the output of the accuracy and\nloss, not only will we be seeing that accuracy\nand loss computed for the training set, we'll\nalso see that computed on the validation set.\nIt's important to understand though, that\nthe model is only alerting on or training\non the training data.\nIt's not taking the validation set into account\nduring training.\nThe validation set is just for us to be able\nto see how well the model is able to predict\non data that it was not exposed to during\nthe training process.\nIn other words, it allows us to see how general\nour model is how well it's able to generalize\non data that is not included in the training\ndata.\nSo knowing this information will allow us\nto see if our model is running into the famous\noverfitting problem.\nSo overfitting occurs when the model has learned\nthe specific features of the training set\nreally well, but it's unable to generalize\non data it hasn't seen before.\nSo if while training, we see that the model\nis giving really good results for the training\nset, but less than good results for the validation\nset, then we can conclude that we have an\noverfitting problem, and then take the steps\nnecessary to combat that specific issue.\nIf you'd like to see the overfitting problem\ncovered in more detail, then there is an episode\nfor that in the deep learning fundamentals\ncourse.\nAlright, so now let's discuss how we can create\nand use a validation set with a karass sequential\nmodel, there's actually two ways that we can\ncreate and work with validation sets with\na sequential model.\nAnd the first way is to have a completely\nseparate validation set from the training\nset.\nAnd then to pass that validation set to the\nmodel in the fit function, there is a validation\ndata parameter.\nAnd so we can just set that equal to the structure\nthat is holding our validation data.\nAnd there's a write up in the corresponding\nblog for this episode that contains more details\nabout the format that that data needs to be\nin.\nBut we're going to actually only focus on\nthe second way of creating and using a validation\nset.\nThis step actually saves us a step because\nwe don't have to explicitly go through the\ncreation process, the validation set, instead,\nwe can get Kerris to create it for us.\nAlright, so we're back in our Jupyter Notebook\nright where we left off last time.\nAnd we're here on the model dot fit function.\nAnd recall, this is what we use last time\nto train our model.\nNow, I've already edited this cell to include\nthis new parameter, which is validation split.\nAnd what validation split does is it does\nwhat it sounds like it splits out a portion\nof the training set into a validation set.\nSo we just set this to a number between zero\nand one.\nSo just a fractional number to tell Kerris\nHow much of the training set we need to split\nout into the validation set.\nSo here I'm splitting out 10% of the training\nset.\nSo it's important to note that whenever we\ndo this, the validation set is completely\nheld out of the training set.\nSo the training samples that we remove from\nthe training set into validation set are no\nlonger contained within the training data\nany longer.\nSo using this approach, the validation set\nwill be created on the fly whenever we call\nthe fit function.\nNow, there's one other thing worth mentioning\nhere.\nAnd remember last time, I discussed this shuffle\nequals true parameter.\nAnd I said that by default, the training set\nis shuffled whenever we call fit.\nSo this shuffle equals true is already set\nby default.\nBut I was just bringing it up to let you know\nthat that the training set is being shuffled.\nSo that is a good thing, we want the training\nset to be shuffled.\nBut whenever we call validation split in this\nway, this split occurs before the training\nset is shuffled, meaning that if we created\nour training set and say, we put all of the\nsick patients first and then the non sick\npatients second, and then we say that we want\nto split off the last 10% of the training\ndata to be our validation data, it's going\nto take the last 10% of the training data.\nAnd therefore it could just take all of the\nthe second group that we put in the training\nset and not get any of the first group.\nSo I wanted to mention that because although\nthe training data is being shuffled with the\nfit function, if you haven't already shuffled\nyour training data before you pass it to fit,\nthen you also use the validation split parameter,\nit's important to know that your validation\nset is going to be the last X percent of your\ntraining set and therefore may not be shuffled\nand may yield some strange results because\nyou think that everything has been shuffled\nwhen really, it's only the training set has\nbeen shuffled after the validation set has\nbeen taken out.\nSo just keep that in mind the way that we\ncreated our training set.\nBefore this episode, we actually shuffled\nthe training data before it's ever passed\nto the fit function.\nSo in the future, whenever you're working\nwith data, it's a good idea to make sure that\nyour data is also shuffled beforehand, especially\nif you're going to be making use of the validation\nsplit parameter to create a validation set.\nAlright, so now we'll run this cell one more\ntime calling the fit function.\nBut this time, not only will we see loss and\naccuracy metrics for the training set, we'll\nalso see these metrics for the validation\nset.\nAlright, so the model has just finished running\nit's 30 epochs.\nAnd now we see both the loss and accuracy\non the left hand side, as well as the validation\nloss and validation accuracy on the right\nhand side.\nSo we can see, let's just look at the accuracy\nbetween the two.\nThey're both starting at around the same 50%\nMark, and going up gradually around the same\nrate.\nSo we just scroll all the way to our last\nepoch, we can see that the accuracy and validation\naccuracy are pretty similar with only 1% difference\nbetween the two.\nAnd yet the loss values are similar as well.\nSo we can see in this example that our model\nis not overfitting, it is actually performing\npretty well or just as well rather on the\nvalidation set as it is on the training set.\nSo our model is generalizing well.\nIf however, we saw that the opposite case\nwas true, and our validation accuracy was\nseriously lagging behind our training accuracy,\nthen we know that we have a overfitting problem\nand we would need to take steps to address\nthat issue.\nAlright, so we've now seen how to train the\nmodel how to validate the model, and how to\nmake use of both training and validation sets.\nIn the next episode, we're going to see how\nto make use of a third data set that test\ndata set.\nTo use the model for inference.\nBe sure to check out the blog and other resources\navailable for this episode on depot's or.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nIn this episode, we'll see how we can use\na neural network for inference to predict\non data from a test set using TensorFlow keras\nAPI.\nAs we touched on previously, whenever we train\na model, the hope is that we can then take\nthat model and use it on new data that it\nhas not seen before during training, and hopefully\nthe model is able to generalize well.\nAnd give us good results on this new data.\nSo as a simple example, suppose that we trained\na network to be able to identify images of\ncats or dogs.\nAnd so during the training process, of course,\nwe had a training set that, say, we downloaded\nfrom a website with 1000s of images of cats\nand dogs.\nSo the hope is later that if we wanted to,\nwe could maybe build a web app, for example.\nAnd we could have people from all over the\nworld, submit their dog and cat photos and\nhave our model, tell them with high accuracy,\nwhether or not their animal is a cat or a\ndog.\nSo I don't know why anyone would actually\nmake that web app. but you get the point.\nThe hope is that even though the images that\nare being sent in from people around the world\nhave their own cats and dogs, even though\nthose weren't included in the training set\nthat the model was originally trained on.\nHopefully, the models able to generalize well\nenough to understand from what it's learned\nabout dog and cat features, that it can predict\nthat Mandy's dog is actually a dog and not\na cat, for example, we call this process inference.\nSo the model takes what it learned during\ntraining, and then uses that knowledge to\ninfer things about data that it hasn't seen\nbefore.\nIn practice, we might hold out a subset of\nour training data to put in a set called the\ntest set.\nTypically, after the model has been trained\nand validated, we take the model and use it\nfor inference purposes against the test set,\njust as one additional step to the validation\nto make sure that the model is generalizing\nWell, before we deploy our model to production.\nSo at this point, the model that we've been\nworking with over the last few episodes has\nbeen trained and validated.\nAnd given the metrics that we saw during the\nvalidation process, we have a good idea that\nthe model is probably going to do a pretty\ngood job at inference on the test set as well.\nIn order to conclude that though, we first\nwould need to create a test set.\nSo we're going to do that now.\nAnd then after we create the test set, then\nwe'll use the model for inference on it.\nAlright, so we are back in our Jupyter Notebook.\nAnd now we're going to go through the process\nof creating the test set.\nAnd actually, if you just glance at this code\nhere, you can see that this whole process\nof setting up the the samples and labels list\nand then generating the data from the imagine\nclinical trial that we discussed in a previous\nepisode, we're then taking that generated\ndata and putting it into a NumPy array format,\nthen shuffling that data, and then scaling\nthe data to be on a scale from zero to one,\nrather than from the scale of 13 to 100.\nSo actually, that is the same exact process\nusing almost the same exact code, except for\nwe're working with our tests, labels, and\ntest samples, variables rather than train\nlabels and train samples.\nSo we're not going to go line by line through\nthis code.\nIf you need a refresher, go check out the\nearlier episode where we did the exact same\nprocess for the training set.\nThe important thing to take from this process,\nthough, is that the test set should be prepared\nand processed in the same format as the training\ndata was.\nSo we'll just go ahead and run the cells to\ntest and are to create and process the test\ndata.\nAnd now, we are going to use our model to\npredict on the test data.\nSo to obtain predictions from our model, we\ncall predict on the model that we created\nin the last couple of episodes.\nSo we are calling model dot predict.\nAnd we are first passing in this parameter\nx which we're setting equal to our scaled\ntest samples.\nSo that is what we created in just the line\nabove, where we scaled our test samples to\nbe on a scale from zero to one.\nSo this is the data that we want our model\nto predict on.\nThen we specify the batch size, and we are\nsetting the batch size equal to 10, which\nis the exact same batch size that we use for\nour training data whenever we train the model\nas well.\nAnd then the last parameter we're specifying\nis this verbose parameter, we're setting this\nequal to zero, because during predicting there\nis not any output from this function that\nwe actually care about seeing or that is going\nto be any use to us at the moment.\nSo we're setting that equal to zero to get\nno output.\nAlright, so then if we run this, so then our\nmodel predicts on all of the data in our test\nset.\nAnd if we want to have a visualization of\nwhat each of these predictions from the model\nlooks like for each sample, we can print them\nout here.\nSo looking at these predictions, the way that\nwe can interpret this is for each element\nwithin our test set.\nSo for each sample in our tests that we are\ngetting a probability that maps to either\nThe patient not experiencing a side effect,\nor the patient experiencing a side effect.\nSo for the first sample in our test set, this\nprediction says that the model is a 92, or\nis assigning a 92% probability to this patient,\nnot experiencing a side effect, and just a\naround 8% probability of the patient experiencing\na side effect.\nSo recall that we said, no side effect experience\nwas labeled as a zero, and a side effect experienced\nwas labeled as a one.\nSo that is how we know that this particular\nprobability maps to not having a side effect\nbecause it's in the zeroeth index.\nAnd this specific probability maps to having\na side effect because it is in the first index.\nSo if we're interested in seeing only the\nmost probable prediction for each sample in\nthe test set, then we can run this cell here,\nwhich is taking the predictions and getting\nthe index of the prediction with the highest\nprobability.\nAnd if we print that out, then we can see\nthat these are a little bit easier to interpret\nthan the previous output.\nSo we can see for the first sample that the\nprediction is zero, the second sample is a\none.\nAnd just to confirm, if we go back up here,\nwe can see that the first sample indeed has\nthe higher probability of a label of zero,\nmeaning no side effects.\nAnd the second sample has a higher probability\nof one meaning that the patient did experience\na side effect.\nSo from these prediction results, we're able\nto actually see the underlying predictions.\nBut we're not able to make much sense of them\nin terms of how well the model did add these\npredictions, because we didn't supply the\nlabels to the model during inference in the\nsame way that we do during training.\nThis is the nature of inference.\nA lot of times inference is occurring once\nthe model has been deployed to production.\nSo we don't necessarily have correct labels\nfor the data that the model is inferring from.\nIf we do have corresponding labels for our\ntest set, though, which in our case, we do,\nbecause we are the ones who generated the\ntest data, then we can visualize the prediction\nresults by plotting them to a confusion matrix.\nAnd that'll give us an overall idea at how\naccurate our model was at inference on the\ntest data.\nWe'll see exactly how that's done in the next\nepisode.\nBe sure to check out the blog and other resources\navailable for this episode on depot's or.com\nas well as the tables at hive mind where you\ncan gain access to exclusive perks and rewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nIn this episode, we'll demonstrate how to\nuse a confusion matrix to visualize prediction\nresults from a neural network during inference.\nIn the last episode, we showed how we could\nuse our train model for inference on data\ncontained in a test set.\nAlthough we have the labels for this test\nset, we don't pass them to the model during\ninference, and so we don't get any type of\naccuracy readings for how well the model does\non the test set.\nUsing a confusion matrix, we can visually\nobserve how well a model predicts on test\ndata.\nLet's jump right into the code to see exactly\nhow this is done.\nWe'll be using scikit Learn to create our\nconfusion matrix.\nSo the first thing we need to do is import\nthe necessary packages that we'll be making\nuse of next week create our confusion matrix\nby calling this confusion matrix function\nfrom scikit learn.\nAnd we pass in our test labels as the true\nlabels.\nAnd we pass in our predictions as the predictions\nand that the confusion matrix at specs and\nrecall this rounded predictions variable,\nas well as the test labels.\nThese were created in the last episode.\nSo rounded predictions recall was when we\nuse the arg max function to select only the\nmost probable predictions.\nSo now our predictions are in this format.\nAnd then our labels are zeros and ones that\ncorrespond to whether or not a patient had\nside effects or not.\nSo next we have this plot confusion matrix\nfunction.\nAnd this is directly copied from socket learns\nwebsite.\nThere's a link to the site on the corresponding\nblog where you can copy this exact function.\nBut this is just a function that socket learn\nhas created to be able to easily plot in our\nnotebook, the confusion matrix, which is going\nto be the actual visual output that we want\nto see.\nSo we just run this cell to define that function.\nAnd now we create this list that has the labels\nthat we will use on our computer.\nIn matrix, so we want, the labels have no\nside effects and had side effects.\nThose are the corresponding labels for our\ntest data, then we're going to call the plot\nconfusion matrix function that we just brought\nin and defined above from scikit learn.\nAnd to that we are going to pass in our confusion\nmatrix.\nAnd the classes for the confusion matrix which\nwe are specifying cm plot labels, which we\ndefined just right above.\nAnd lastly, just the title that is going to\nbe the title to display above the confusion\nmatrix.\nSo if we run this, then we actually get the\nconfusion matrix plot.\nAlright, so we have our predicted labels on\nthe x axis and our true labels on the y axis.\nSo the way we can read this is that we look\nand we see that our model predicted that a\npatient had no side effects 10 times when\nthe patient actually had a side effect.\nSo that's incorrect predictions.\nOn the flip side, though, the model predicted\nthat the patient had no side effects 196 times\nthat the patient indeed had no side effects.\nSo the this is the correct predictions.\nAnd actually, generally reading the confusion\nmatrix, looking at the top left to the bottom\nright diagonal, these squares here and blue\ngoing across this diagonal are the correct\npredictions.\nSo we can see total that the model predicted\n200 plus 196.\nSo 396 correct predictions out of a total\nof 420, I think, yes.\nSo all these numbers added up equal 420 396\nout of 420 predictions were correct.\nSo that gives us about a 94% accuracy rate\non our test set, which is equivalent to what\nwe were seeing for our validation, accuracy\nrate during training.\nSo as you can see, a confusion matrix is a\ngreat tool to use to be able to visualize\nhow well our model is doing edits predictions,\nand also be able to drill in a little bit\nfurther to see for which classes it might\nneed some work, be sure to check out the blog\nand other resources available for this episode\non the poser.com as well as the deep lizard\nhive mind where you can gain access to exclusive\nperks and rewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nAnd this episode will demonstrate the multiple\nways that we can save and load a karass sequential\nmodel.\nWe have a few different options when it comes\nto saving and loading a karass sequential\nmodel.\nAnd these all work a little bit differently\nfrom one another.\nSo we're going to go through all of the options.\nNow.\nWe've been working with a single model over\nthe last few episodes.\nAnd we're going to continue working with that\none.\nNow, I've printed out a summary of that model\nto refresh your memory about which one we've\nbeen working with.\nBut just make sure that in your Jupyter Notebook\nthat you have that model already created,\nbecause we're going to now show how we can\nsave that model.\nAlright, so the first way to save a model\nis just by calling the Save function on it.\nSo to save, we pass in a path for which we\nwant to save our model, and then the model\nname or the file name that we want to save\nour model under with the H five extension.\nSo this h5 file is going to be where the model\nis stored.\nAnd this code here is just a condition that\nI'm checking to see if the model is not already\nsaved to disk first, then save it because\nI don't want to continue saving the model\nover and over again, on my machine if it's\nalready been saved.\nSo that's what this this condition is about.\nBut this model dot save function is the first\nway that we can save the model.\nNow when we save using this way, it saves\nthe architecture of the model, allowing us\nto be able to recreate it with the same number\nof learnable parameters and layers and nodes\netc.\nIt also saves the weights of the model.\nSo if the models already been trained, then\nthe weights that it has learned and optimized\nfor are going to be in place within this saved\nmodel on disk.\nIt also saves the training configuration.\nSo things like our loss and our optimizer\nthat we set whenever we compile the model,\nand the state of the optimizer is also saved.\nSo that allows us if we are training the model,\nand then we stop and save the model to disk,\nthen we can later load that model again and\npick up training where we left off because\nthe state of the optimizer will be in that\nsaved state.\nSo this is the most comprehensive option when\nit comes to saving the model because it saves\neverything, the architecture, the learnable\nparameters and the state of the model.\nwhere it left off with training.\nSo if we want to load a model later that we\npreviously saved to disk, then we need to\nfirst import this load model function from\nTensorFlow Kerris dot models.\nAnd then we create a variable.\nIn this case, I'm calling it a new model,\nand then setting it to load model and then\npointing to where our saved model is on disk.\nSo then, if we run that, and then look at\na summary of our new model, we can see Indeed,\nit is an exact replica in terms of its architecture\nas the original model up here that we had\npreviously saved to disk.\nAlso, we can look at the weights of the new\nmodel, we didn't look at the weights ahead\nof time to be able to compare them directly.\nBut this is showing you that you can inspect\nthe weights to look and comparatively see\nthat the weights are actually the same as\nthe previous models weights.\nIf you were to have taken a look at those\nbeforehand.\nWe can also look at the optimizer just to\nshow you that although we never set an optimizer\nexplicitly for our new model, because we are\nloading it from the saved model, it does indeed\nuse the atom optimizer that we set a while\nback whenever we compiled our model for training.\nAlright, so that's it for the first saving\nand loading option.\nAnd again, that is the most comprehensive\noption to save and load everything about a\nparticular model.\nThe next option that we'll look at is using\nthis function called to JSON.\nSo we call model.to.\njson, if we only need to save the architecture\nof the model.\nSo we don't want to set its weights or the\ntraining configuration, then we can just save\nthe model architecture by saving it to a JSON\nstring.\nSo I'm using this example here, creating a\nvariable called JSON string, setting it equal\nto model.to.\njson.\nAnd remember, model is our original model\nthat we've been working with so far up to\nthis point.\nSo we call to JSON.\nAnd now if we print out this JSON string,\nthen we can see we get this string of details\nabout the model architecture.\nSo it's a sequential model.\nAnd then it's got the layers organized, with\nthe individual dense layers and all the details\nabout those specific layers from our original\nmodel.\nNow, if at a later point, we want to create\na new model with our older models, architecture,\nthen if we save it to a JSON string, then\nwe can import the model from JSON function\nfrom TensorFlow Kerris dot models.\nAnd now we're creating a new variable called\nmodel architecture.\nAnd we are loading in the JSON string using\nthe model from JSON function.\nSo now we have this new model, which I'm just\ncalling model architecture.\nAnd if we look at the summary of that, then\nagain, we can see that this is identical to\nthe summary of the original model.\nSo we have a new model in place now, but we\nonly have the architecture in place.\nSo you would have to retrain it to update\nits weights.\nAnd we would need to compile it to get an\noptimizer and our loss and everything like\nthat defined, this only creates the model\nfrom an architecture standpoint.\nBefore moving on to our third option, I just\nwanted to mention a brief point that we can\ngo through this same exact process, but using\na YAML string instead of a JSON string.\nSo the function to create a gamble string\nis just model.to a gamble instead of to JSON.\nAnd then the function to load a YAML string\nis model from a gamma model from a YAML instead\nof model from JSON.\nAlright, so our next option to save a model\nis actually just to save the weights of the\nmodel.\nSo if you only need to save the weights, and\nyou don't need to save the architecture, nor\nany of the training configurations, like the\noptimizer or loss, then we can save solely\nmodels weights by using the Save weights function.\nSo to do that, we just call model dot save\nweights.\nAnd this looks exactly the same as when we\ncalled model dot save, we're just passing\na path on disk to where to save our model\nalong with the file name ending with an H\nfive extension.\nSo I'm calling this my model weights dot h\nfive.\nAnd again, we have this condition here where\nI'm just checking if this H five file has\nalready been saved to disk, otherwise, I'm\nnot going to keep saving it over and over\nagain.\nNow, the thing with this is that when we save\nonly the weights, if we want to load them\nat a later time, then we don't have a model\nalready in place because we didn't save the\nmodel itself.\nWe only save the weights.\nSo to be able to bring in our weights to a\nnew model then we would then need to create\na second model at that point with the same\narchitecture.\nAnd then we could load the weights.\nAnd so that's what we're doing in this cell.\nI'm defining this model called model two.\nAnd it is the exact same model from an architecture\nstandpoint as the first model.\nSo if we run this, then at that point, we\nhave the option to load weights into this\nmodel.\nAnd the shape of these weights is going to\nhave to match the shape of what this model\narchitecture is essentially.\nSo we couldn't have a model with five layers\nhere to find, for example, and load these\nweights in because there wouldn't be a direct\nmapping of where these particular weights\nshould be loaded.\nSo that's why we have the same exact architecture\nhere as our original model.\nSo to load our weights, we call load weights.\nAnd we point to the place on disk where our\nweights are saved.\nAnd then we can call get weights on our new\nmodel, and see that our new model has been\npopulated with weights from our original model.\nAlright, so now you know all the ways that\nwe can say various aspects of a karass sequential\nmodel.\nBe sure to check out the blog and other resources\navailable for this episode on people's are.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow, let's move on to the next episode.\nHey, I may be from people's or, in this episode,\nwe'll go through all the necessary image preparation\nand processing steps needed to train our first\nconvolutional neural network.\nOur goal over the next few episodes will be\nto build and train a convolutional neural\nnetwork that can classify images as cats or\ndogs, the first thing that we need to do is\nget and prepare our data set for which we'll\nbe training our model.\nWe're going to work with the data set from\nthe kaggle cats versus dogs competition.\nAnd you can find a link to download the dataset\nin the corresponding blog for this episode\non people's are.com.\nSo we're mostly going to organize our data\non this programmatically.\nBut there's a couple of manual steps that\nwe'll go through first.\nSo after you've downloaded the data set from\nkaggle, you will have this zip folder here.\nAnd if we look inside, then this is the contents\nwe have a zipped train folder, and a zipped\ntest folder along with the sample submission\nCSV.\nSo we actually are not going to be working\nwith this, or this.\nSo you can delete the test that as well as\nthe CSV file, we're going to only be working\nwith this train zip.\nSo we want to extract the high level cats,\nor dogs versus cats at first, and then extract\nthe train zip.\nSo because that takes a while I went ahead\nand did that here.\nSo now I have just this train directory because\nI moved the test directory elsewhere and deleted\nthe CSV file.\nSo now I have this extracted train folder.\nAnd I go in here and I have a nested train\nfolder.\nAs you can see, I'm already in train once.\nNow I'm in train again.\nAnd in here we have all of these images of\ncats, and dogs.\nOkay, so that is the way that the data will\ncome downloaded.\nThe first step, or the next step that we need\nto do now is to come into here and grab all\nof these images.\nAnd we're going to Ctrl x or cut all of these\nand bring them up to the first train directory.\nSo we don't want this nested directory structure.\nInstead, we're going to place them directly\nwithin the first train directory.\nAlright, now all of our images have been copied\ninto our base train directory here.\nSo this nested train directory that the images\npreviously belong to is now empty.\nSo we can just go ahead and delete this one.\nAlright, so now our directory structure is\njust this dogs vs. Cat top directory within\nwe have a train directory.\nAnd then we have all of the images within\nour train directory.\nBoth of cats and dogs.\nThe last step is to just move the dogs versus\ncats directory that has all the data in it\nto be in the place on disk where you're going\nto be working.\nSo for me, I am in relative to my Jupyter\nNotebook, I am within a directory called data\nand I have placed dogs versus cats here.\nAlright, so that's it for the manual labor\nnow everything else that will do to organize\nthe data, and then later process the data\nwill be programmatically through code.\nAlright, so we're now here within our Jupyter\nNotebook.\nAnd First things first, we need to import\nall of the packages that we'll be making use\nof and all of the packages here are not just\nThis specific episode on processing the image\ndata.\nBut actually, these are all of the packages\nthat we'll be making use of over the next\nseveral episodes as we're working with cn\nns.\nAlright, so we'll get that taken care of.\nAnd now, just this cell here is making sure\nif we are using a GPU that TensorFlow is able\nto identify it correctly, and we are an enabling\nmemory growth on the GPU as well.\nIf you're not using a GPU, then no worries,\nas mentioned earlier, you are completely fine\nto follow this course with a CPU only.\nAlright, so now we're going to pick back up\nwith organizing our data on disk.\nSo assuming that we've gone through the first\nsteps from the beginning of this episode,\nwe're now going to organize the data and to\ntrain valid and test ders, which correspond\nto our training validation and test sets.\nSo this group here is going to first change\ndirectories into our dog vs. Cat directory.\nAnd then it's going to check to make sure\nthat the directory structure that we're about\nto make does not already exist.\nAnd if it doesn't, it's going to proceed with\nthe rest of this script.\nSo the first thing that it's doing as long\nas the directory structure is not already\nin place, is making the following directories.\nSo we already have a train directory.\nSo it's going to make a nested dog and cat\ndirectory within train.\nAnd then additionally is going to make a valid\ndirectory that contains dog and cat directories,\nand a test directory, which also contains\ndog and cat directories.\nNow, this particular data set contains 25,000\nimages of dogs and cats.\nAnd that is pretty much overkill for the tasks\nthat we will be using these images for.\nIn the upcoming episodes, we're actually only\ngoing to use a small subset of this data,\nyou're free to work with all the data if you'd\nlike.\nBut it would take a lot longer to train our\nnetworks and work with the data in general\nif we were using the entire set of it.\nSo we are going to be working with a subset\nconsisting of 1000 images in our training\nset 200 in our validation set, and 100 in\nour test set, and each of those sets are going\nto be split evenly among cats and dogs.\nSo that's exactly what this block of code\nhere is doing.\nIt's going into the images that are in our\ndogs vs cat directory, and moving 500 randomly\nmoving 500 cat images into our train cat directory\n500 dog images into our train dog directory.\nAnd then similarly doing the same thing for\nour valid, our valid our validation set, both\nfor cat and dogs and then our test set, both\nfor cat and dogs with just these quantities\ndiffering regarding the amounts that I stated\nearlier for each of the sets.\nAnd we're able to understand which images\nare cats and which are dogs based on the names\nof the files.\nSo if you saw earlier, the cat images actually\nhad the word cat in the file names and then\nthe dog images had the word dog in the file\nnames.\nSo that's how we're able to select dog images\nand cat images here with this script.\nAlright, so after this script runs, we can\npull up our file explorer and look at the\ndirectory structure and make sure it is what\nwe expect.\nSo we have our dogs vs cat directory within\nour data directory here.\nSo if we enter, then we have test train and\nvalid directories.\nInside test, we have cat that has cat images,\nand inside dog, we have dog images.\nIf we back out and go to train, we can see\nsimilarly.\nAnd if we go into valid we can see similarly.\nAnd you can select one of the folders and\nlook at the properties to to see how many\nfiles exist within the directory to make sure\nthat is it is the amount that we chose to\nput in from our script and that we didn't\naccidentally make any type of error.\nSo if we go back to the dogs vs cats the root\ndirectory here, you can see we have all of\nthese cat and dog images leftover.\nThese were the remaining 23,000 or so that\nwere left over after we moved our subset into\nour train valid and test directories.\nSo you're free to make use of these in any\nway that you want or delete them or move them\nto another location.\nAll of what we'll be working with are in these\nthree directories here.\nAlright, so at this point, we have obtained\nthe data and we have organized the data.\nNow it's time to move on to processing the\ndata.\nSo if we scroll down, first, we are just creating\nthese variables here where we have assigned\nour train valid and test paths.\nSo this is just pointing to the location on\ndisk where our different data sets reside.\nNow recall earlier in the course, we talked\nabout that whenever we train a model that\nwe need to put the data into a format that\nthe model expects.\nAnd we know that when we train a karass sequential\nmodel, that the model receives the data whenever\nwe call the fit function.\nSo we are going to put our images and do the\nformat of a karass generator.\nAnd we're doing that in this cell here.\nWe're creating these train valid and test\nbatches, and setting them equal to image data\ngenerator dot flow from directory, which is\ngoing to return a directory iterator.\nBasically, it's going to create batches of\ndata from the directories where our datasets\nreside.\nAnd these batches of data will be able to\nbe passed to the sequential model using the\nfit function.\nSo now let's look exactly at how we are defining\nthese variables.\nSo let's focus just on train batches for now.\nSo we're setting train batches equal to image\ndata generator dot flow from directory.\nBut first to image data generator, we are\nspecifying this pre processing function and\nsetting that equal to tf Kerris dot applications\nthat VGG 16 dot pre process input.\nSo I'm just going to tell you for now that\nthis is a function that is going to apply\nsome type of pre processing on the images\nbefore they get passed to the network that\nwe'll be using.\nAnd we're processing them in such a way that\nis equivalent to the way that a very popular\nmodel known as VGG 16, we're processing our\nimages in the same format as which images\nthat get passed to this VGG 16 model our process.\nAnd we're going to talk about more about this\nin a future episode.\nSo don't let it confuse you now just know\nthat this is causing some type of processing\nto occur on our images.\nAnd we'll talk more about it in a future episode\nand not stress on it now, because it's not\nnecessarily very important for us right at\nthis moment, the technical details of that\nat least.\nSo besides that, when we call flow from directory,\nthis is where we are passing in our actual\ndata and specifying how we want this data\nto be processed.\nSo we are setting directory equal to train\npath, which appear we defined the location\non disk where our training set was under the\ntrain PATH variable.\nAnd then we're setting target size equal to\n224 by 224.\nSo this is the height and width that we want\nthe cat and dog images to be resized to.\nSo if you're working with an image data set\nthat has images of varying sizes, or you just\nwant to scale them up or scale them down,\nthis is how you can specify that to happen.\nAnd this will resize all images in your data\nset to be of this height and width before\npassing them to our network.\nNow we are specifying our classes, which are\njust the classes for the potential labels\nof our data set.\nSo cat or dog, and we are setting our batch\nsize to 10.\nWe do the exact same thing for the validation\nset.\nAnd the test set.\nEverything is the exact same for both of them,\nexcept for where each of these sets live on\ndisk as being specified here under the directory\nparameter.\nAnd then the only other difference is here\nfor our test batches.\nWe are specifying this shuffle equals false\nparameter.\nNow, this is because whenever we use our test\nbatches later for inference to get our model\nto predict on images of cats and dogs after\ntraining and validation has been completed,\nwe're going to want to look at our prediction\nresults in a confusion matrix like we did\nin a previous video for a separate data set.\nAnd in order to do that, we need to be able\nto access the unsettled labels for our test\nset.\nSo that's why we set shuffle equals false\nfor only this set.\nFor both validation and training sets, we\ndo want the data sets to be shuffled.\nAlright, so we run this and we get the output\nof found 1000 images belonging to two classes.\nAnd that is corresponding to our train batches\nfound at 200 images belonging to two classes,\nwhich corresponds to valid valid batches,\nand then the 100 belonging to two classes\ncorresponding to our test batches.\nSo that is the output that you want to see\nfor yourself.\nThat's letting you know that it found the\nimages on disk that belong to both the cat\nand dog classes that you have specified here.\nSo if you are not getting this at this point,\nif you get found at zero images, then perhaps\nyou're pointing to the wrong place on disk.\nYou just need to make sure that it's able\nto find all the images that you set up previously.\nRight and here we are just verifying that\nthat is indeed the case.\nNow Next, we are going to just grab a single\nbatch of images and the corresponding labels\nfrom our train batches.\nAnd remember, our batch size is 10.\nSo this should be 10 images along with the\n10 corresponding labels.\nNext, we're introducing this function plot\nimages that we're going to use to plot the\nimages from our train batches that we just\nobtained above.\nAnd this function is directly from tensor\nflows website.\nSo check the link in the corresponding blog\nfor this episode on people's or.com.\nTo see, to be able to get to the tensor flows\nsite where exactly I pulled this off of.\nSo we will define this function here.\nAlright, so now we're just going to use this\nfunction to plot our images from our test\nbatches here.\nAnd we're going to print the corresponding\nlabels for those images.\nSo if we scroll down, we can see this is what\na batch of training data looks like.\nSo this might be a little bit different than\nwhat you expected, given the fact that it\nlooks like the color data has been a little\nbit distorted.\nAnd that's due to the pre processing function\nthat we called to pre process the images in\nsuch a way that in the same type of way that\nimages get pre processed for the famous VGG\n16 model.\nSo like I said, we're going to discuss in\ndetail what exactly that pre processing function\nis doing technically, as well as why we're\nusing it in a later video.\nBut for now, just know that it's skewing the\nRGB data in some way.\nSo we can still make out the fact that this\nis a cat.\nAnd this looks like a cat.\nThis is the dog, dog, dog, dog cat.\nYeah, so we can still kind of generally make\nout what these images are, but the color data\nis skewed.\nBut don't worry too much about the technical\ndetails behind that.\nFor right now, just know that this is what\nthe data looks like before we pass it to the\nmodel.\nAnd here are the corresponding labels for\nthe data.\nSo we have these one hot encoded vectors that\nrepresent either cat or dog.\nSo a one zero represents a cat, and a zero,\na one represents a dog.\nOkay, so I guess I was wrong earlier with\nthinking that this one was a dog.\nThis one is a cat, because as we can see,\nit maps to the 101 hot encoding.\nAnd if you don't know what I mean by one hot\nencoding, then check out the corresponding\nvideo for that in the deep learning fundamentals\ncourse on depot's or.com.\nBut yeah, we can see that 01 is the vector\nused to represent the label of a dog.\nSo this one is a dog.\nAnd the next two are dogs as well, this one\nand this one.\nNow, just a quick note about everything that\nwe've discussed up to this point, sometimes\nwe do not have the corresponding labels for\nour test set.\nSo in the examples that we've done so far,\nin this course, we've always had the corresponding\nlabels for our test set.\nBut in practice, a lot of times you may not\nhave those labels.\nAnd in fact, if we were to have used the downloaded\ntest directory that came from the kaggle download,\nthen we would see that that test directory\ndoes not have the images labeled with the\ncat or dog.\nSo in this case, we do have the test labels\nfor the cat and dog images since we pulled\nthem from the original training set from kaggle\nthat did have the corresponding labels.\nBut if you don't have access to the test labels,\nand you are wondering how to process your\ntest data accordingly, then check the blog\nfor this episode on dibbles comm I have a\nsection there that demonstrates what you need\nto do differently from what we showed in this\nvideo if you do not have access to the labels\nfor your test set.\nAlright, so now we have obtained our image\ndata, organized it on disk and processed it\naccordingly for our convolutional neural network.\nSo now in the next episode, we are going to\nget set up to start building and training\nour first CNN.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nAnd this episode will demonstrate how to build\na convolutional neural network and then train\nit on images of cats and dogs using tensor\nflows integrated Kerris API.\nWe'll be continuing to work with the cat and\ndog image data that we created in the last\nepisode.\nSo make sure you still have all of that in\nplace, as well as the imports that we brought\nin in the last episode as well as we'll be\nmaking use of those imports.\nFor the next several videos of working with\nCNN, so to create our first CNN, we will be\nmaking use of the Kerris sequential model.\nAnd recall, we introduced this model in an\nearlier episode when we were working with\njust plain simple numerical data.\nBut we'll continue to work with this model\nhere with our first CNN.\nSo the first layer to this model that we pass\nis a comma to D layer.\nSo this is just our standard convolutional\nlayer that will accept image data.\nAnd to this layer, we're arbitrarily setting\nthe filter value equal to 32.\nSo this first convolutional layer will have\n32 filters with a kernel size of three by\nthree.\nSo the choice of 32 is pretty arbitrary, but\nthe kernel size of three by three is a very\ncommon choice for image data.\nNow, this first column to D layer will be\nfollowed by the popular relu activation function.\nAnd we are specifying padding equals Same\nhere.\nAnd this just means that our images will have\nzero padding, padding the outside so that\nthe dimensionality of the images aren't reduced\nafter the convolution operations.\nSo lastly, for our first layer only, we specify\nthe input shape of the data.\nAnd recall we touched on this parameter previously,\nyou can think of this as kind of creating\nan implicit input layer for our model.\nThis comp 2d is actually our first hidden\nlayer, the input layer is made up of the input\ndata itself.\nAnd so we just need to tell the model the\nshape of the input data, which in our case\nis going to be 224.\nBy 224.\nRecall, we saw that we were setting that target\nsize parameter to do 24 by 224.\nWhenever we created our valid test and train\ndirectory iterators, we said that we wanted\nour images to be of this height and width.\nAnd then this three here is regarding the\ncolor channels.\nSince these.\nSince these images are an RGB format, we have\nthree color channels, so we specified this\ninput shape.\nSo then we follow our first convolutional\nlayer with a max pooling layer, where we're\nsetting our pool size to two by two and our\nstrides by two.\nAnd if you are familiar with Max pooling,\nthen you know this is going to cut our image\ndimensions in half.\nSo if you need to know more about max pooling,\nor you just need a refresher, same thing with\npadding here for zero padding, activation\nfunctions, anything like that, then be sure\nto check those episodes out and the corresponding\ndeep learning fundamentals course on depot's\nor.com.\nSo after this max pooling layer, we're then\nadding a another convolutional layer, that\nlooks pretty much exactly the same as the\nfirst one except for four, we're not including\nthe input shape parameter, since we only specify\nthat for our first hidden layer, and we are\nspecifying the filters to be 64 here instead\nof 32.\nSo 64 is again, an arbitrary choice, I just\nchose that number here.\nBut the general rule of increasing functions\nas you go until later layers of the network\nis common practice, we then follow this second\nconvolutional layer with another max pooling\nlayer identical to the first one, then we\nflatten all of this into a one dimensional\ntensor before passing it to our dense output\nlayer, which only has two nodes corresponding\nto cat and dog.\nAnd our output layer is being followed by\nthe softmax after activation function, which\nas you know, is going to give us probabilities\nfor each corresponding output from the model.\nAlright, so if we run this, we can then check\nout the summary of our model.\nAnd this is what we have exactly what we built,\nalong with some additional details about the\nlearnable parameters and the output shape\nof the network.\nSo these are things that are also covered\nin the in the deep learning fundamentals course\nas well, if you want to check out more information\nabout learnable parameters and such.\nSo now that our model is built, we can prepare\nit for training by calling model dot compile,\nwhich again, we have already used in a previous\nepisode, when we were just training on numerical\ndata.\nBut as you will see here, it looks pretty\nmuch the same as before, we are setting our\noptimizer equal to the atom optimizer with\na learning rate of 0.0001.\nAnd we are using categorical cross entropy.\nAnd we are looking at the accuracy as our\nmetrics to be able to judge the model performance.\nJust a quick note before moving on.\nWe are using categorical cross entropy.\nBut since we have just two outputs from our\nmodel, either cat or dog, then it is possible\nto instead use binary cross entropy.\nAnd if we did that, then we would need to\njust have a one single output node from our\nmodel instead of two And then rather than\nfollowing our output layer with the softmax\nactivation function, we would need to follow\nit with sigmoid.\nAnd both of these approaches using categorical\ncross entropy loss with the setup that we\nhave here, or using binary cross entropy loss\nwith the setup that I just described, work\nequally well, they're totally equivalent,\nand will yield the same results, categorical\ncross entropy, and using softmax on the app,\nas the activation function for the output\nlayer, or just a common approach for when\nyou have more than two classes.\nSo I like to continue using that approach,\neven when I only have two classes just because\nit's general.\nAnd it's the case that we're going to use\nwhenever there's more than two classes anyway,\nso I just like to stick with using the same\ntype of setup, even when we only have two\noutputs.\nAlright, so after compiling our model, we\ncan now train it using model dot fit, which\nwe should be very familiar with up to this\npoint.\nSo to the fit function, we are first specifying\nour training data, which is stored in train\nbatches.\nAnd then we specify our validation data, which\nis stored in valid batches.\nRecall, this is a different way of creating\nvalidation data, as we spoke about in an earlier\nepisode, we're not using validation split\nhere, because we've actually created a validation\nset separately ourselves before fitting the\nmodel.\nSo we are specifying that separated set here\nas the validation data parameter, then we\nare setting our epochs equal to 10.\nSo we're only going to train for 10 runs this\ntime and setting verbose, verbose equal to\ntwo so that we can see the most verbose output\nduring training.\nAnd one other thing to mention is that you\nwill see here that we are specifying x just\nas we have in the past, but we are not specifying\ny, which is our target data usually.\nAnd that's because when data is stored as\na generator, as we have here, the the generator\nitself actually contains the corresponding\nlabels.\nSo we do not need to specify them separately,\nwhenever we call fit, because they're actually\ncontained within the generator itself.\nSo let's go ahead and run this now.\nAlright, so the model just finished training.\nSo let's check out the results.\nBefore we do just if you get this warning,\nit appears from the research I've done to\nbe a bug within TensorFlow that's supposed\nto be fixed in a in the next release actually,\nis what I read.\nSo you can just safely ignore this warning,\nit has no impact on our training.\nBut if we scroll down and look at these results,\nthen we can see that by our 10th epoch, our\naccuracy on our training set has reached 100%.\nSo that is great.\nBut our validation accuracy is here at 69%.\nSo not so great, we definitely see that we\nhave some overfitting going on here.\nSo if this was a model that we really cared\nabout, and that we really wanted to use and\nbe able to deploy to production, then in this\nscenario, we will need to stop what we're\ndoing and combat this overfitting problem\nbefore going further.\nBut what we are going to do is that we'll\nbe seeing in an upcoming episode, how we can\nuse a pre trained model to perform really\nwell on this data.\nAnd that'll get us exposed to the concept\nof fine tuning.\nBefore we do that, though, we are going to\nsee in the next episode how this model holds\nup to inference at predicting on images in\nour test set.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I may be from deep lizard.\nAnd this episode will demonstrate how to use\na convolutional neural network for inference\nto predict on image data using tensor flows\nintegrated keras API.\nLast time, we built and trained our first\ncnn against cat and dog image data, and we\nsaw that the training results were great with\nthe model achieving 100% accuracy on the training\nset.\nHowever, it lagged behind by quite a good\nbit at only 70% accuracy on our validation\nset.\nSo that tells us that the model wasn't generalizing\nas well as we hoped.\nBut nonetheless, we are going to use our model\nnow for inference to predict on cat and dog\nimages in our test set.\nGiven the less than decent results that we\nsaw from the validation performance.\nOur expectation is that the model is not going\nto do so well on the test set either it's\nprobably going to perform at around the same\n70% rate.\nBut this is still going to give us exposure\nabout how we can use a CNN for inference using\nthe Kerris sequential API.\nAlright, so we are back in Jupyter Notebook\nand we need to make sure that we have all\nthe code in place from the last couple of\nepisodes as we will be continuing to make\nuse of both our model that we built last time,\nas well as our test data from whenever we\nprepared the data sets.\nSo the first thing we're going to do is get\na batch of test data from our test batches.\nAnd then we're going to plot that batch, we're\ngoing to plot the images specifically, and\nthen we're going to print out the corresponding\nlabels for those images.\nAnd we're using before we do that just a reminder,\nthis plot images function that we introduced\nin the last couple of episodes.\nAlright, so if we scroll down, we have our\ntest batches.\nRecall, we had the discussion about why the\nimage data looks the way that it does in terms\nof the color being skewed last time, but we\ncan see that just by looking even though we\nhave kind of distorted color we have, these\nare all actually cats here.\nAnd by looking at the corresponding label\nfor these images, we can see that they are\nall labeled with the one hot encoded vector\nof one zero, which we know is the label for\ncat.\nSo if you're wondering why we have all cats\nas our first 10 images in our first batch\nhere, that is because we're call whenever\nwe created the test set, we specified that\nwe did not want it to be shuffled.\nAnd that was so that we could do the following.\nIf we come to our next cell, and we run test\nbatches classes, then we can get a array that\nhas all of the corresponding labels for each\nimage in the test set.\nSo given that we have access to the ns shuffled\nlabels for the test set, that's why we don't\nwant to shuffle the test set directly, because\nwe want to be able to have this one one direct\nmapping from the uncoupled labels to the test\ndata set.\nAnd if we were to shuffle the test data set,\nevery time we generated a batch, then we wouldn't\nbe able to have the correct mapping between\nlabels and samples.\nSo we care about having the correct mapping,\nbecause later after we get our predictions\nfrom the model, we're going to want to plot\nour predictions to a confusion matrix.\nAnd so we want the corresponding labels that\nbelong to the samples in the test set.\nAlright, so next, we're actually going to\ngo ahead and obtain our predictions by calling\nmodel dot predict just as we have an earlier\nepisodes for other data sets.\nAnd to x, we are specifying our test batches,\nso all of our test data set, and we are choosing\nverbose to be zero to get no output whenever\nwe run our predictions.\nSo here, we are just printing out the arounded\npredictions from the model.\nSo the way that we can read this is first,\neach one of these arrays is a prediction for\na single sample.\nSo if we just look at the first one, this\nis the prediction for the first sample and\nthe test set.\nSo this one, wherever there is a one with\neach prediction is the one is the index for\nthe output class that had the highest probability\nfrom the model.\nSo in this case, we see that the zeroeth index\nhad the highest probability.\nSo we can just say that the label that the\nmodel predicted for this first sample was\na zero, because we see that there is a one\nhere in the zeroeth index.\nAnd if we look at the first element, or if\nwe look at the first label here, up here,\nfor our first test sample, it is indeed zero.\nSo we can eyeball that and see that the model\ndid accurately predict all the way down to\nhere, because that's the first 123456 and\n123456.\nOkay, so But then, whenever we see that the\nmodel predicted the first index to be the\nhighest probability, that means that the,\nthat the model predicted an output label of\na one, and so that corresponds to dog.\nSo it's hard for us to kind of draw an overall\nconclusion about the prediction accuracy for\nthis test set, just eyeballing the results\nlike this.\nBut if we scroll down, then we know that we\nhave the tool of a confusion matrix that we\ncan use to make visualizing these results\nmuch easier, like we've seen in previous episodes\nof this course already.\nSo we are going to do that.\nNow we're going to create a confusion matrix\nusing this confusion matrix function from\nscikit learn which we've already been introduced\nto and we are passing in our true labels using\ntest batches classes.\nRecall that we just touched on that a few\nminute ago.\nAnd for our predictive labels, we are passing\nin the predictions from our model, we are\ngetting the we're actually passing in the\nindex of each.\nWe're actually using arg max to pass in the\nindex of where the most probable prediction\nwas from our predictions list.\nSo this is something that we've already covered\nin previous episodes for why we do that.\nSo if we run that, we're now going to bring\nin this plot confusion matrix, which we've\ndiscussed is directly from psychic Lauren's\nwebsite, link to that is, and the corresponding\nblog for this episode on people's are calm.\nThis is just going to allow us to plot our\nconfusion matrix in a moment.\nAnd now if we look at the class indices, we\nsee that cat is first and dog is second.\nSo we just need to look at that so that we\nunderstand in which order, we should put our\nplot labels for our confusion matrix.\nAnd next we call plot confusion matrix and\npass in the confusion matrix itself, as well\nas the labels for the confusion matrix and\na title for the entire matrix.\nSo let's check that out.\nAlright, so from what we learned about how\nwe can easily interpret a confusion matrix,\nwe know that we can just look at this diagonal\nhere running from top left to bottom right,\nto see what the model predicted correctly.\nSo not that great, the model is definitely\noverfitting at this point.\nSo like I said, if this was a model that we\nwere really concerned about, then we would\ndefinitely want to combat that overfitting\nproblem.\nBut for now, we are going to move on to a\nnew model using a pre trained state of the\nart model called VGG 16.\nIn the next episode, so that we can see how\nwell that model does on classifying images\nof cats and dogs.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from be blizzard.\nAnd this episode will demonstrate how we can\nfine tune a pre train model to classify images\nusing tensor flows keras API.\nThe pre train model that we'll be working\nwith is called VGG 16.\nAnd this is the model that won the 2014 image\nnet competition.\nIn the image net competition, multiple teams\ncompete against each other to build a model\nthat best classifies images within the image\nnet library.\nAnd the image net library is made up of 1000s\nof images that belong to 1000 different classes\nusing Kerris will import this VGG 16 model,\nand then fine tune it to not classify on one\nof the 1000 categories for which it was originally\ntrained, but instead only on two categories,\ncat and dog.\nNote, however, that cats and dogs were included\nin the original image net library for which\nVGG 16 was trained on.\nAnd because of this, we won't have to do much\ntuning to change the model from classifying\nfrom 1000 classes to just the two cat and\ndog classes.\nSo the overall fine tuning that we'll do will\nbe very minimal.\nin later episodes, though, we'll do more involved\nfine tuning and use transfer learning to transfer\nwhat a model has learned on an original data\nset to completely new data and a new custom\ndata set that we'll be using later.\nTo understand fine tuning and transfer learning\nat a fundamental level, check out the corresponding\nepisode for fine tuning in the deep learning\nfundamentals course on V blizzard.com. before\nwe actually start building our model, let's\nquickly talk about the VGG 16 pre processing\nthat is done.\nNow recall we are here in our Jupyter Notebook.\nThis is from last time when we plotted a batch\nfrom our test data set.\nAnd a few episodes ago, we discussed the fact\nthat this color data was skewed as a result\nof the VGG 16 pre processing function that\nwe were calling.\nWell, now we're going to discuss what exactly\nthis pre processing does.\nSo as we can see, we can still make out the\nimages as these here being all cats, but it's\njust the data the color data itself that appears\nto be distorted.\nSo if we look at the paper that the authors\nof VGG 16 detailed the model in under the\narchitecture section, let's blow this up a\nbit, we can see that they state that the only\npre processing we do is subtract the mean\nRGB value computing on the trait computed\non the training set from each pixel.\nSo what does that mean?\nThat means that they computed the mean red\nvalue pixel for All of the training data.\nAnd then once I had that mean value across\neach image in the training set, they subtracted\nthat mean value from the red value, the red\npixel, each pixel in the image.\nAnd then they did the same thing for the green\nand blue pixel values as well.\nSo they found the green picks the Mean Green\npixel value among the entire training set.\nAnd then for each sample in the training set,\nevery green pixel, they subtracted that green\nvalue from it.\nSo and same thing for the blue, of course.\nSo that's what they did for the pre processing.\nSo given that, that's how VGG 16 was originally\ntrained.\nThat means now whenever new data is passed\nto the model, that it needs to be processed\nin the same exact way as the original training\nset.\nSo Kerris already has functions built in for\npopular models, like VGG 16, where they have\nthat pre processing in place that matches\nfor the corresponding model.\nSo that's why we were calling that model whenever\nwe process our cat and dog images earlier,\nso that we could go ahead and get those images\nprocessed in such a way that matched how the\noriginal training set was processed, when\nVGG was originally trained.\nAlright, so that is what that color distortion\nis all about.\nSo now let's jump back into the code.\nNow that we know what that's about, we have\nan understanding of the pre processing.\nNow, Let's now get back to actually building\nthe fine tuned model.\nSo the first thing that we need to do is download\nthe model.\nAnd when you call this for the first time,\nyou will need an internet connection because\nit's going to be downloading the model from\nthe internet.\nBut after this, subsequent calls to this function\nwill just be grabbing this model from the\ndownload a copy on your machine.\nAlright, so the model has been downloaded\nit now we are just running this summary.\nAnd we can see that this model is much more\ncomplex than what we have worked with up to\nthis point.\nSo total, there are almost 140 million parameters\nin this model.\nAnd on disk, it is over 500 megabytes.\nSo it is quite large.\nNow recall I said that this VGG 16 model originally\nwas predicting for 1000 different image net\nclasses.\nso here we can see our output layer of the\nVGG 16 model has a 1000 different outputs.\nSo our objective is going to simply be to\nchange this last output layer to predict only\ntwo output classes corresponding to cat and\ndog, along with a couple other details regarding\nthe fine tuning process that we'll get to\nin just a moment.\nSo now, we're actually going to just skip\nthese two cells here.\nThese are just for me to be able to make sure\nthat I've imported the model correctly.\nBut it is not relevant for any code here.\nIt's just checking that the trainable parameters.\nAnd non trainable parameters are what I expect\nthem to be after importing.\nBut this is not of concern at the moment for\nus.\nSo if we scroll down here, we're now going\nto build a new model called sequential.\nAlright, now before we run this code, we're\nactually just going to look at the type of\nmodel this is.\nSo this is returning a model called model.\nSo this is actually a model from the Kerris\nfunctional API.\nWe have been previously working with sequential\nmodels.\nSo we are in a later episode going to discuss\nthe functional API in depth, it's a bit more\ncomplicated and more sophisticated than the\nsequential model.\nFor now, since we're not ready to bring in\nthe functional model yet, we are going to\nconvert the original VGG 16 model into a sequential\nmodel.\nAnd we're going to do that by creating a new\nvariable called model here and setting this\nequal to an instance of sequential object.\nAnd we are going to then loop through every\nlayer and VGG 16.\nExcept for the last output layer, we're leaving\nthat out, we're going to loop through every\nlayer and then add each layer into our new\nsequential model.\nSo now we'll look at a summary of our new\nmodel.\nAnd by looking at this summary, if you take\nthe time to compare the previous summary to\nthis summary, what you will notice is that\nthey are exactly the same except for the last\nlayer has been not included in this new model.\nSo this layer here we have this fully connected\ntwo layer is what this is here, with the output\nshape of 4096. here if we scroll Back up,\nwe can see that this is this layer here.\nSo the predictions layer has not been included,\nbecause when we were iterating over our for\nloop, we went all the way up to the second\nlast layer, we did not include the last layer\nof VGG 16.\nAlright, so now let's scroll back down.\nAnd we're now going to iterate over all of\nthe layers within our new sequential model.\nAnd we are going to set each of these layers\nto not be trainable by setting layer dot trainable\nto false.\nAnd what this is going to do is going to freeze\nthe trainable parameters, or the weights and\nbiases from all the layers in the model so\nthat they're not going to be retrained.\nWhenever we go through the training process\nfor cats and dogs, because VGG 16 has already\nlearned the features of cats and dogs and\nits original training, we don't want it to\nhave to go through more training again it\nsince it's already learned those features.\nSo that's why we are freezing the weights\nhere, we're now going to add our own output\nlayer to this model.\nSo remember, we removed the previous output\nlayer that had 10 output or that had 1000\noutput classes rather.\nAnd now we are going to add our own output\nlayer that has only two output classes for\ncat and dog.\nSo we add that now since we have set all of\nthe previous layers to not be trainable, we\ncan see that actually only our last output\nlayer that's going to be the only trainable\nlayer in the entire model.\nAnd like I said before, that's because we\nalready know that VGG 16 has already learned\nthe features of cats and dogs during its original\ntraining.\nSo we only need to retrain this output layer\nto classify two output classes.\nSo now if we look at a new summary of our\nmodel, then we'll see that everything is the\nsame, except for now we have this new dense\nlayer as our output layer, which only has\ntwo classes, instead of 1000.\nFrom the original VGG 16 model, we can also\nsee that our model now only has 8000 trainable\nparameters, and those are all within our output\nlayer.\nAs I said that our output layer is our only\ntrainable layer.\nSo before actually, all of our layers were\ntrainable.\nIf we go take a look at our original VGG 16\nmodel, we see that we have 138 million total\nparameters, all of which are trainable, none\nof which are non trainable.\nSo if we didn't freeze those layers, then\nthey would be getting retrained during the\ntraining process for our cat and dog images.\nSo just to scroll back down again and check\nthis out.\nWe can see that now we still have quite a\nbit of learnable parameters or a total parameters\n134 million, but only 8000 of which are trainable.\nThe rest are non trainable.\nIn the next episode, we'll see how we can\ntrain this modified model on our images of\ncats and dogs.\nBe sure to check out the blog and other resources\navailable for this episode on the bowser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Mandy from Blizzard.\nIn this episode, we'll demonstrate how to\ntrain the fine tuned VGG 16 model that we\nbuilt last time on our own data set of cats\nand dogs.\nAlright, so we're jumping straight into the\ncode to train our model.\nBut of course, be sure that you already have\nthe code in place from last time as we will\nbe building on the code that we have already\nrun previously.\nSo, we are using our model here to first compile\nit to get it ready for training.\nThis model is the sequential model that we\nbuilt last time that is our fine tuned VGG\n16 model containing all the same layers with\nfrozen weights except for our last layer which\nwe have modified to output only two possible\noutputs.\nSo, we are compiling this model using the\natom optimizer as we have previously used\nwith the learning rate of 0.0001 and a the\nloss we are using again categorical cross\nentropy just like we have before and we are\nusing the accuracy as our only metric to judge\nmodel performance.\nSo, there is nothing new here with this call\nto compile.\nIt is pretty much exactly the same as what\nwe have seen for our previous models.\nNow we are going to train the model using\nmodel dot fit and we are passing in our training\ndata set which we have stored in train batches.\nWe are passing in our validation set Which\nwe have stored as valid batches.\nAnd we are only going to run this model for\nfive epochs.\nAnd we are setting the verbosity level to\ntwo so that we can see the most comprehensive\noutput from the model during training.\nSo let's see what happens.\nAlright, so training has just finished, and\nwe have some pretty outstanding results.\nSo just after five epochs on our training\ndata and validation data, we have an accuracy\non our training data of 99%.\nAnd validation accuracy, right on par at 98%.\nSo that's just after five epochs.\nAnd if we look at the first epoch, even our\nfirst epoch gives, it gives us a training\naccuracy of 85%, just starting out, and a\nvalidation accuracy of 93%.\nSo this isn't totally surprising, because\nremember, earlier in the course, we discussed\nhow VGG 16 had already been trained on images\nof cats and dogs from the image net library.\nSo it had already learned those features.\nNow, the flight training that we're doing\non the output layer, is just to train VGG\n16 to output only cat or dog at classes.\nAnd so it's really not surprising that it's\ndoing such a good job right off the bat in\nits first epoch, and even an even considerably\nbetter, and it's fifth epoch at 99% training\naccuracy.\nNow we're called the previous CNN that we\nbuilt from scratch ourselves the really simple\nconvolutional neural network, that model actually\ndid really well on the training data, reaching\n100% accuracy after a small amount of epochs\nas well.\nWhere we saw it lagging though was with the\nvalidation accuracy.\nSo it had a validation accuracy of around\n70%.\nHere we see that we are at 98%.\nSo the main recognizable difference between\nour very simple CNN and this VGG 16 fine tuned\nmodel is how well this model generalizes to\nour cat and dog data in the validation set,\nwhereas the model we built from scratch did\nnot generalize so well on data that was not\nincluded in the training set.\nIn the next episode, we're going to use this\nVGG 16 model for inference to predict on the\ncat and dog images in our test set.\nAnd given the accuracy that we are seeing\non the validation set here, we should expect\nto see some really good results on our test\nset as well.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nIn this episode, we'll use our fine tuned\nVGG 16 model for inference to predict on images\nin our test set.\nAll right, we are jumping right back into\nour Jupyter Notebook.\nAgain, making sure that all the code is in\nplace and has run it from the previous episodes\nas we will be building on the code that has\nbeen run there.\nSo first things first is that we are going\nto be using the model to get predictions from\nour test set.\nSo to do that we call model dot predict which\nwe have been exposed to in the past.\nRecall that this model here is our fine tuned\nVGG 16 model.\nAnd to predict we are passing our test set\nwhich we have stored in test batches.\nAnd we are setting our verbosity level to\nzero as we do not want to see any output any\noutput from our predictions.\nNow recall Previously, we talked about how\nwe did not shuffle the test set for our cat\nand dog image data set.\nAnd that is because we want to be able to\naccess the classes here and uncheck old order\nso that we can then pass in the uncheck old\nclasses that correspond to the old sorry,\nthe shuffled labels that correspond to the\nns shuffled test data, we want to be able\nto have those in a one to one mapping, where\nthe labels actually are the correct and shuffled\nlabels for the unsettled data samples, we\nwant to be able to pass those to our confusion\nmatrix.\nSo this is the same story as we saw whenever\nwe were using our CNN that we built from scratch\na few episodes back, we did the same process,\nwe're using the same dataset recall.\nAnd so now we are plotting this confusion\nmatrix in the exact same manner as before.\nSo this is actually the exact same line that\nwe used to plot the confusion matrix A few\nepisodes back when we plotted it on this same\nexact test set for the CN n that we built\nfrom scratch.\nAnd just a reminder from last time recall\nthat we looked at the class indices of the\ntest batches.\nSo that we could get the correct order of\nour cat and dog classes to use for our labels\nfor our confusion matrix.\nSo we are again doing the same thing here.\nAnd now we are calling the psychic learn plot\nconfusion matrix which you should have defined\nearlier in your notebook from a previous episode.\nAnd to plot confusion matrix we are passing\nin our confusion matrix, and our labels defined\njust above as well as a general title for\nthe confusion matrix itself.\nSo now, let's check out the plot so that we\ncan see how well our model did on these predictions.\nSo this is what the third or fourth time that\nwe've used a confusion matrix in this course\nso far, so you should be pretty normalized\nto how to read this data.\nSo recall, the quick and easy way is to look\nfrom the top left to the bottom right along\nthis diagonal, and we can get a quick overview\nof how well the model did.\nSo the model correctly predicted a dog for\n49 times for images that were truly dogs.\nAnd it correctly predicted a cat 47 times\nfor images that truly were cats.\nSo we can see that one time it predicted a\ncat when it was actually a dog.\nAnd three times it predicted a dog when images\nwere actually cats.\nSo overall, the model incorrectly predicted\nfour samples, so that gives us 96 out of 100.\nCorrect.\nOr let's see 96 correct predictions out of\n100 total predictions.\nSo that gives us an accuracy rate on our test\nset of 96%.\nNot surprising given what we saw in the last\nepisode for the high level of accuracy that\nour model had on the validation set.\nSo overall, this fine tuned VGG 16 model does\nreally well at generalizing on data that it\nhad not seen during training a lot better\nthan our original model for which we build\nfrom scratch.\nNow recall that we previously discussed that\nthe overall fine tuning approach that we took\nto this model was pretty minimal since cat\nand dog data was already included in the original\ntraining set for the original VGG 16 model.\nBut in upcoming episodes, we are going to\nbe doing more fine tuning, more fine tuning\nthan what we saw here for VGG 16.\nAs we will be fine tuning another well known\npre trained model, but this time for a completely\nnew data set that was not included in the\noriginal data set that it was trained on.\nSo stay tuned for that.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nAnd this episode we'll introduce mobile nets\na class of a lightweight deep convolutional\nneural networks that are much smaller and\nfaster and size than many of the mainstream\npopular models that are really well known.\nneural nets are a class of small, low power,\nlow latency models that can be used for things\nlike classification, detection, and other\nthings that cn ns are typically good for.\nAnd because of their small size, these models\nare considered great for mobile devices, hence\nthe name mobile nets.\nSo I have some stats taken down here.\nSo just to give a quick comparison, in regards\nto the size, the size of the full VGG 16 network\nthat we've worked with in the past few episodes\nis about 553 megabytes on disk.\nSo pretty large, generally speaking, the size\nof one of the currently largest mobile nets\nis only about 17 megabytes.\nSo that's a pretty huge difference, especially\nwhen you think about deploying a model to\nrun on a mobile app, for example, this vast\nsize difference is due to the number of parameters\nor weights and biases contained in the model.\nSo for example, let's see VGG 16, as we saw\npreviously has about 138 million total parameters.\nSo a lot.\nAnd the 17 megabytes mobile net that we talked\nabout, which was the largest mobile net currently\nhas only about 4.2 million parameters.\nSo that is much much smaller on a relative\nscale than VGG 16 with 138 million.\nAside from the size on disk being a consideration\nwhen it comes to comparing mobile nets to\nother larger models.\nWe also need to consider the memory as well.\nSo the more parameters that a model has The\nmore space in memory it will be taking up\nalso.\nSo while mobile nets are faster and smaller\nthan the competitors of these big hefty models\nlike VGG 16, there is a catch or a trade off,\nand that trade off is accuracy.\nSo, mobile nets are not as accurate as some\nof these big players like VGG 16, for example,\nbut don't let that discourage you.\nWhile it is true that mobile nets aren't as\naccurate as these resource heavy models like\nVGG 16, for example, the trade off is actually\npretty small, with only a relatively small\nreduction in accuracy.\nAnd in the corresponding blog for this episode,\nI have a link to a paper that goes more in\ndepth to this relatively small accuracy difference.\nIf you'd like to check that out further, let's\nnow see how we can work with mobile nets and\ncode with Kerris.\nAlright, so we are in our Jupyter Notebook.\nAnd the first thing we need to do is import\nall of the packages that we will be making\nuse of which these are not only for this video,\nbut for the next several videos where we will\nbe covering mobilenet.\nAnd as mentioned earlier in this course, a\nGPU is not required.\nBut if you're running a GPU, then you want\nto run this cell.\nThis is the same cell that we've seen a couple\nof times already in this course earlier, where\nwe are just making sure that TensorFlow can\nidentify our GPU if we're running one, and\nit is setting the memory growth to true if\nwe do have a GPU.\nSo again, don't worry, if you don't have a\nGPU, don't worry.\nBut if you do, then run this cell, similar\nto how we downloaded the VGG 16 model, when\nwe were working with it in previous episodes,\nwe take that same approach to download mobile\nnet here.\nSo we call TF dot cares applications dot mobile\nnet dot mobile net, and that the first time\nwe call it is going to download mobile net\nfrom the internet.\nSo you need an internet connection.\nBut subsequent calls to this are just going\nto be getting the model from a saved model\non disk and loading it into memory here.\nSo we are going to do that now and assign\nthat to this mobile variable.\nNow mobile net was originally trained on the\nimage net library, just like VGG 16.\nSo in a few minutes, we will be passing some\nimages to mobile net that I've saved on this.\nThey are not images from the image net library,\nbut they are images of some general things.\nAnd we're just going to get an idea about\nhow mobile net performs on these random images.\nBut first, in order to be able to pass these\nimages to mobile net, we're going to have\nto do a little bit of processing first.\nSo I've created this function called prepare\nimage.\nAnd what it does is it takes a file name,\nand it then inside the function, we have an\nimage path, which is pointing to the location\non disk, where I have these saved image files\nthat we're going to use to get predictions\nfrom mobile net.\nSo we have this image path defined to where\nthese images are saved, we then load the image\nby using the image path, and appending, the\nfile name that we pass in.\nSo say if I pass in image, one dot png here,\nthen we're going to take that file path, append\none PNG here, pass that to load image, and\npass this target size of 224 by 224.\nNow, this load image function is from the\nkeras API.\nSo what we are doing here is we are just taking\nthe image file, resizing it to be of size\n224 by 224, because that is the size of images\nthat mobile net expects, and then we just\ntake this image and transfer it to be in a\nformat of an array, then we expand the dimensions\nof this image, because that's going to put\nthe image in the shape that mobilenet expects.\nAnd then finally, we pass this new processed\nimage to our last function, which is TF Kerris,\nthat application stop mobile net, that pre\nprocess input.\nSo this is a similar function to what we saw\na few episodes back when we were working with\nVGG 16, it had its own pre process input.\nNow mobile net has its own pre process input\nfunction, which is processing images in a\nway that mobile net expects, so it's not the\nsame way as VGG 16.\nActually, it's just scaling all the RGB pixel\nvalues to be on a scale instead of from zero\nto 255, to be on a scale from minus one to\none.\nSo that's what this function is doing.\nSo overall, this entire function is just resizing\nthe image and putting it into an array format\nwith expanded dimensions and then mobile net,\nprocessing it and then returning this processed\nimage.\nOkay, so that's kind of a mouthful, but that's\nwhat we got to do two images before we pass\nthem to mobile net.\nAlright, so we will just Define that function.\nAnd now we are going to display our first\nimage called one dot png, from our mobile\nnet samples directory that I told you, I just\nset up with a few random images.\nAnd we're going to plot that to our Jupyter\nNotebook here.\nAnd what do you know, it's a lizard.\nSo that is our first image.\nNow, we are going to pass this image to our\nprepare image function that we defined right\nabove that we just finished talking about.\nSo we're going to pass that to the function\nto pre process the image accordingly, then\nwe are going to pass the pre processed image\nreturned by the function to our mobile net\nmodel, we're going to do that by calling predict\non the model, just like we've done in previous\nvideos, when we've called predict on models\nto use them for inference, then after we get\nthe prediction for this particular image,\nwe are then going to give this prediction\nto this image net utils dot decode predictions\nfunction.\nSo this is a function from cares, that is\njust going to return the top five predictions\nfrom the 1000 possible image net classes.\nAnd it's going to tell us the top five that\nmobile net is predicting for this image.\nSo let's run that and then print out those\nresults.\nAnd maybe you can have a better idea of what\nI mean once you see the printed output.\nSo we run this, and we have our output.\nSo these are the top five in order results\nfrom the imagenet classes that mobile net\nis predicting for this image.\nSo it's assigning a 58% probability to this\nimage of being a an American chameleon 28%\nprobability to green lizard 13% to a gamma.\nAnd then we have some small percentages here\nunder 1% for these other two types of lizards.\nSo it turns out, if you're not aware, and\nI don't know how you could not be aware of\nthis, because everyone should know this.\nBut this is an American chameleon.\nAnd I don't know I've always called these\nthings green and OLS.\nBut I looked it up.\nAnd they're also known as American chameleon.\nSo mobilenet got it right.\nSo yeah, it assigned at 58% probability that\nwas the highest, most probable class.\nNext was green lizard.\nSo I'd say that that is still really good\nfor that to be your second place.\nI don't know if green lizard is supposed to\nbe more general.\nBut and then a gamma, which is also a similar\nlooking lizard, if you didn't know.\nSo between these top three classes that it\npredicted, this is almost 100%.\nbetween the three of these, I would say mobile\nnet did a pretty good job on this prediction.\nSo let's move on to number two.\nAlright, so now we are going to plot our second\nimage.\nAnd this is a cup of well, I originally thought\nthat it was espresso, and then someone called\nit a cup of cappuccino.\nSo let's say I'm not sure I'm not a coffee\nconnoisseur, although I do like both espresso\nand cappuccinos.\nThis looks like it has some cream in it.\nSo hey, now we're going to go through the\nsame process of what we just did for the lizard\nimage where we are passing that image passing\nthis new image to our prepare image function,\nso that they undergoes all of the pre processing,\nthen we are going to pass the pre processed\nimage to the predict function for our mobile\nnet model.\nThen finally, we are going to get the top\nfive results from the predictions for this\nmodel relative in regards to the imagenet\nclasses.\nSo let's see.\nAll right, so according to mobile net, this\nis an espresso, not a cappuccino.\nSo I don't know.\nBut it predicts 99% probability of espresso\nas being the most probable class for this\nparticular image.\nAnd I'd say that that is pretty reasonable.\nSo let me know in the comments, what do you\nthink is this espresso or cappuccino?\nI don't know if mobile or if image net had\na cappuccino class.\nSo if it didn't, then I'd say that this is\npretty spot on.\nBut you can see that the other the other four\npredictions are all less than 1%.\nBut they are reasonable.\nI mean, the second one is cup, third eggnog,\nfourth coffee mug.\nFifth, wooden spoon gets a little bit weird,\nbut there is wood, there is a circular shape\ngoing on here.\nBut these are all under 1%.\nSo they're pretty negligible.\nI would say mobilenet did a pretty great job\nat giving a 99% probability to espresso for\nthis image.\nAll right, we have one more sample image.\nSo let's bring that in.\nAnd this is a strawberry or multiple strawberries\nif you call if you consider the background.\nSo same thing we are pre processing the strawberry\nimage.\nThen we are getting a prediction For a mobile\nnet for this image, and then we are getting\nthat top five results for the most probable\npredictions among the 1000 image net classes.\nAnd we see that mobile net with 99.999% probability\nclassifies this image as a strawberry correctly,\nso very well, and the rest are well, well\nunder 1%.\nBut they are all fruits.\nSo interesting.\nAnother really good prediction from mobile\nnet.\nSo even with the small reduction in accuracy\nthat we talked about, at the beginning of\nthis episode, you can probably tell from just\nthese three random samples that that reduction\nis maybe not even noticeable when you're just\ndoing tests like the ones that we just ran\nthrough.\nSo in upcoming episodes, we're actually going\nto be fine tuning this mobile net model to\nwork on a custom data set.\nAnd this custom data set is not one that was\nincluded in the original image net library,\nit's going to be a brand new data set, we're\ngoing to do more fine tuning than what we've\ndone in the past.\nSo stay tuned for that.\nBe sure to check out the blog and other resources\navailable for this episode on deep lizard\ncalm, as well as the deep lizard hive mind\nwhere you can gain access to exclusive perks\nand rewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Mandy from deep lizard.\nAnd this episode, we'll be preparing and processing\na custom data set that we'll use to fine tune\nmobilenet using tensor flows keras API.\nPreviously, we saw how well VGG 16 was able\nto classify and generalize on images of cats\nand dogs.\nBut we noted that VGG 16 was actually already\ntrained on cats and dogs, so it didn't take\na lot of fine tuning at all to get it to perform\nwell on our cat and dog dataset.\nNow, with mobile net, we'll be going through\nsome fine tuning steps as well.\nBut this time, we'll be working with a data\nset that is completely different from the\noriginal image net library for which mobile\nnet was originally trained on.\nThis new data set that we'll be working with\nis a data set of images of sign language digits,\nthere are 10 classes total for this dataset,\nranging from zero to nine, where each class\ncontains images of the particular sign for\nthat digit.\nThis data set is available on kaggle as grayscale\nimages, but it's also available on GitHub\nas RGB images.\nAnd for our particular task, we'll be using\nthe RGB images.\nSo check out the corresponding blog for this\nepisode on Peebles or.com, to get the link\nfor where you can download the data set yourself.\nSo after downloading the data set, the next\nstep is to organize the data on disk.\nAnd this will be a very similar procedure\nto what we saw for the cat and dog data set\nearlier in the course.\nSo once we have the download, this is what\nit looks like.\nIt is a zipped folder called sign language\ndigits data set master.\nAnd the first thing we want to do is extract\nall the contents of this directory.\nAnd when we do that, we can navigate inside\nuntil we get to the dataset directory.\nAnd inside here we have all of the classes.\nAnd each of these directories has the corresponding\nimages for this particular class.\nSo the what we want to do is we want to grab\nzero through nine.\nAnd we are going to Ctrl x or cut these directories.\nAnd we're going to navigate back to the root\ndirectory, which is here, we're going to place\nall of the directories zero through nine in\nthis route.\nAnd then we're going to get rid of everything\nelse by deleting.\nSo now, one last thing.\nSo I'm just going to delete this master.\nI don't necessarily like that.\nSo we have sign language digits data set.\nAnd directly within we have our nested directories\nconsisting of zero through nine, each of which\nhas our training data inside then the last\nstep is to move the sign language digits data\nset directory to where you're going to be\nworking.\nSo for me that is relative to my Jupyter Notebook,\nwhich lives inside of this deep learning with\ncares directory, I have a data directory and\nI have the sign language digits data set located\nhere.\nNow everything else that will do to organize\nand process the data will be done programmatically\nin code.\nSo we are in our Jupyter Notebook.\nMake sure that you do have the imports that\nwe brought in last time still in place because\nwe'll be making use of those Now, this is\njust the class breakdown to let you know how\nmany images are in each class.\nSo across the classes zero through nine, the\nthere are anywhere from 204 to 208 samples\nin each class.\nAnd then here I have just an explanation of\nhow your data should be structured up to this\npoint.\nNow the rest of the organization, we will\ndo programmatically with this script here.\nSo this script is going to organize the data\ninto train valid and test directories.\nSo recall right now, we just have all the\ndata located in the corresponding classes\nof zero through nine.\nBut the data is not broken up yet into the\nseparate data sets of train, test and validation.\nSo to do that, we are first changing directory\ninto our sign language digits, data set directory.\nAnd then we are checking to make sure that\nthe directory structure that we're about to\nsetup is not already in place on disk.\nAnd if it's not, then we make a train valid\nand test directory, right within sign language\ndigits dataset.\nSo next we are then iterating over all of\nthe directories within our sign language digits\ndataset directory.\nSo recall, those are directories labeled zero\nthrough nine.\nSo that's what we're doing in this for loop\nwith this range zero to 10.\nThat's going from directory zero to nine,\nand moving each of these directories into\nour train directory.\nAnd after that, we're then making two new\ndirectories, one inside of valid with the\ndirectory for whatever place we're at in the\nloop.\nSo if we are on run number zero, then we are\nmaking a directory called zero with invalid,\nand a directory called zero within test.\nAnd if we are on run number one, then we will\nbe creating a directory called one with invalid\nand one within test.\nSo we do this whole process of moving each\nclass in to train and then creating each class\ndirectory empty, within valid and test.\nSo at this point, let's suppose that we are\non the first run in this for loop.\nSo in this range, here we are following add\na number zero.\nSo here on this line, what we are doing is\nwe are sampling 30 samples from our train\nslash zero directory.\nBecause up here we created or we moved the\nclass directory, zero into train.\nAnd now we're going to sample 30 random samples\nfrom the zero directory within train, then,\nso we're calling these valid samples, because\nthese are going to be the samples that we\nmove into our validation set.\nAnd we do that next.\nSo for each of the 30 samples that we collected\nfrom the training set randomly in this line,\nwe're now going to be moving them from the\ntraining set to the validation, zero set to\nthe validation set in Class Zero.\nAnd then we do similarly the same thing for\nthe test samples, we randomly select five\nsamples from the train slash zero directory.\nAnd then we move those five samples from that\ntrain slash zero directory into the test zero\ndirectory.\nSo we just ran through that loop using the\nClass Zero as an example.\nBut that's going to happen for each class\nzero through nine.\nAnd just in case you have any issue with visualizing\nwhat that script does, if we go into sign\nlanguage, digits dataset, now let's check\nout how we have organized the data.\nSo recall we previously had classes zero through\nnine all listed directly here within the this\nroute folder.\nNow we have train valid and test directories.\nAnd within train, we moved the original zero\nthrough nine directories all into train.\nAnd then once that was done, then we sampled\n30 images from each of these classes and moved\nthem into the valid directory classes.\nAnd then similarly, we did the same thing\nfor the test directory.\nAnd then once we look in here, we can see\nthat the test directory has five samples for\nzero, we see that it has five samples for\none.\nIf we go check out the valid directories,\nlook at zero, it should have 30 zeros so See\nthat here 30.\nSo every valid directory has 30 samples, and\nthe training directory classes have not necessarily\nuniform samples because remember, we saw that\nthe number of samples in each class ranged\nanywhere from 204 to 209, I think.\nSo the number of images within each class\ndirectory for the training sample will differ\nslightly by maybe one or two images.\nBut the number of images in the classes for\nthe validation and test directories will be\nuniform since we did that programmatically\nwith our script here.\nSo checking this dataset out on disk, we can\nsee that this is exactly the same format,\nfor which we structured our cat and dog image\ndata set earlier in the course, now, we're\njust dealing with 10 classes instead of two.\nAnd when we downloaded our data, it was in\na slightly different organizational structure\nthan the cat and dog data that we previously\ndownloaded.\nAlright, so we have obtained the data, we've\norganized the data.\nNow the last step is to pre process the data.\nSo we do that first by starting out by defining\nwhere our train valid and test directories\nlive on disk.\nSo we supply those paths here.\nAnd now we are setting up our directory iterators,\nwhich we should be familiar with, at this\npoint.\nGiven this is the same format that we processed\nour cat and dog images whenever we build our\ncnn from scratch, and we use the fine tuned\nVGG 16 model.\nSo let's focus on this first variable train\nbatches.\nFirst, we are calling image data generator\ndot flow from directory which we can't quite\nsee here.\nBut we'll scroll into a minute to image data\ngenerator, we are passing this pre processing\nfunction, which in this case is the mobile\nnet pre processing function.\nRecall, we saw that already in the last episode.\nAnd there we discussed how this pre processes\nimages in such a way that it scales the image\ndata to be rather than on a scale from zero\nto 255, to instead be on a scale from minus\none to one.\nSo then on image data generator, we call flow\nfrom directory, which is blowing off the screen\nhere.\nAnd we set the directory equal to train path,\nwhich we have defined just above saying where\nour training data resides on disk, we are\nsetting the target size to 224 by 224, which\nrecall just resizes any training data to be\na height or to have a height of 224 and a\nwidth of 224, since that is the image size\nthat mobile net expects, and we are setting\nour batch size equal to 10.\nSo we've got the same deal for valid batches.\nAnd for test batches as well, everything exactly\nthe same except for the paths deferring to\nshow where the validation and test sets live\non disk.\nAnd we are familiar now that we specify shuffle\nequals false only for our test set so that\nwe can later appropriately plot our prediction\nresults to a confusion matrix.\nSo we run this cell, and we have output that\nwe have 17 112 images belonging to 10 classes.\nSo that corresponds to our training set 300\nimages to 10 classes for our validation set,\nand 50 images belonging to 10 classes for\nour test set.\nSo I have this cell with several assertions\nhere, which just assert that this output that\nwe find, or that we have right here is what\nwe expect.\nSo if you are not getting this, then it's\nperhaps because you are pointing to the wrong\nlocation on disk.\nSo a lot of times if you're pointing to the\nwrong location, then you'll probably get found\nzero images belonging to 10 classes.\nSo you just need to check your path to where\nyour data set resides if you get that.\nAlright, so now our data set has been processed.\nWe're now ready to move on to building and\nfine tuning our mobile net model for this\ndata set.\nBe sure to check out the blog and other resources\navailable for this episode on depot's or.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Mandy from Blizzard.\nIn this episode, we'll go through the process\nof fine tuning mobile net for a custom data\nset.\nAlright, so we are jumping right back into\nour Jupyter Notebook from last time.\nSo make sure your code is in place from then\nsince we will be building directly on that\nnow.\nSo first thing we're going to do is we are\ngoing to import mobile net just as we did\nin the first mobile net episode by calling\nTF Kerris, the applications that mobile net\ndot mobile net, remember, if this is your\nfirst time running this line, then you will\nneed an internet connection to download it\nfrom the internet.\nNow, let's just take a look at the model that\nwe downloaded.\nSo by calling model dot summary, we have this\noutput here, that is showing us all of these\nlovely layers included in mobile net.\nSo this is just to get a general idea of the\nmodel because we will be fine tuning it.\nSo the fine tuning process now is going to\nstart out with us getting all of the layers\nup to the sixth to last layer.\nSo if we scroll up and look at our output\n123456.\nSo we are going to get all of the layers up\nto this layer, and everything else is not\ngoing to be included.\nSo all of these layers are what we are going\nto keep and transfer into a new model, our\nnew fine tune model.\nAnd we are not going to include the last five\nlayers.\nAnd this is just a choice that I came to after\ndoing a little experimenting and testing,\nthe number of layers that you choose to include\nversus not include whenever you're fine tuning,\na model is going to come through experimentation\nand personal choice.\nSo for for us, we are getting everything from\nthis layer and above.\nAnd we are going to keep that in our new fine\ntuned model.\nSo let's scroll down.\nSo we're doing that by calling mobile dot\nlayers, passing in that six to last layer\nand output, then we are going to create a\nvariable called output.\nAnd we're going to set this equal to a dense\nlayer with 10 units.\nSo this is going to be our output layer.\nThat's why it's called output and 10 units\ndue to the nature of our classes, ranging\nzero through nine.\nAnd this, as per usual is going to be followed\nby a softmax activation function to give us\na probability distribution among those 10\noutputs.\nNow, this looks a little strange.\nSo we're calling this and then we're like\nputting this x variable next to it.\nSo what's this about?\nWell, the mobile net model is actually a functional\nmodel.\nSo this is from the functional API from Kerris,\nnot the sequential API.\nSo we kind of touched on this a little bit\nearlier, whenever we fine tuned VGG 16, we\nsaw that VGG 16 was also indeed a functional\nmodel.\nBut when we fine tuned it, we iterated over\neach of the layers and added them to a sequential\nmodel at that point, because we weren't ready\nto introduce the functional model yet.\nSo here, we are going to continue working\nwith a functional model type.\nSo that's why we are basically taking all\nof the layers here up to the sixth the last.\nAnd whenever we create this output layer,\nand then call this the previous layers stored\nin x here, that is the way that the functional\nmodel works, we're basically saying to this\noutput layer, pass all of the previous layers\nthat we have stored in X, up to this six to\nlast layer and mobile net.\nAnd then we can create the model using these\ntwo pieces x and output by saying by calling\nmodel, which is indeed a functional model\nwhen specified this way, and specifying inputs\nequals mobile dot input.\nSo this is taking the input from the original\nmobile net model, and outputs equals output.\nSo at this point, output is all of the mobile\nnet model up until the six the last layer\nplus this dense output layer.\nAlright, so let's run these two cells to create\nour new model.\nAlright, so our new models now been created.\nSo the next thing we're going to do is we're\ngoing to go through and freeze some layers.\nSo through some experimentation on my own,\nI have found that if we freeze, all except\nfor the last 23 layers, this appears to yield\nsome decent results.\nSo 23 is not a magic number here, play with\nthis yourself and let me know if you get better\nresults.\nBut basically, what we're doing here is we're\ngoing through all the layers in the model,\nand by default, they are all trainable.\nSo we're saying that we want only the last\n23 layers to be trainable.\nAll the layers except for the last one, a\nthree, make those not trainable.\nAnd just so that You understand, relatively\nspeaking, there are 88 total layers in the\noriginal mobile net model.\nAnd so we're saying that we don't want to\ntrain, or that we only want to train the last\n23 layers in our new model that we built just\nabove.\nRecall, this is much more than we earlier\ntrained with our fine tuned VGG 16 model where\nwe only train the output layer.\nSo let's go ahead and run that now.\nAnd now let's look at a summary of our new\nfine tuned model.\nSo here, if we just glance it looks basically\nthe same as what we saw from our original\nsummary.\nBut we will see here that now now our model\nends with this global average pooling 2d layer,\nwhich recall before was the sixth the last\nlayer, where I said that we would include\nthat layer and everything above it.\nSo all the layers below the global average\npooling layer that we previously saw and the\noriginal mobile net summary are now gone.\nAnd instead of an output layer with 1000 classes,\nwe now have an output layer with 10 classes\nfrom the or corresponding to the 10 potential\noutput classes that we have for our new sign\nlanguage digits dataset.\nIf we compare the total parameters, and how\nthey're split amongst trainable and non trainable\nparameters, in this model with the original\nmobile, mobile that model, then we will see\na difference there as well.\nAlright, so now this model has been built,\nwe are ready to train the model.\nSo the code here is nothing new.\nWe are compiling the model in the same exact\nfashion using the atom optimizer 0.00, with\nour little bit of luck, 0.0001 learning rate,\ncategorical categorical cross entropy loss\nand accuracy as our metric.\nSo this, we have probably seen 2 million times\nup to this point in this course.\nSo that's exactly the same.\nAdditionally, we have exactly the same fit\nfunction that we are running to train the\nmodel.\nSo we're passing in our train batches as our\ndata set, we are passing in validation batches\nas our validation data.\nAnd we are running this for 10 epochs.\nActually, we're going to go ahead and run\nthis for 30.\nI had 10 here just to save time earlier from\ntesting, but we're going to run this for 30\nepochs.\nAnd we are going to set verbose equal to two\nto get the most verbose output.\nNow, let's see what happens.\nAll right, so our model just finished training\nover 30 epochs.\nSo let's check out the results.\nAnd if you see this output, and you're wondering\nwhy the first output took 90 seconds, and\nthen we get or the first epoch took 90 seconds,\nand then we got it down to five seconds, just\na few later, it's because I realized that\nI was running on battery and not on my laptop\nbeing plugged in.\nSo once we plug the laptop in, it beefed up\nand started running much quicker.\nSo let's scroll down and look basically just\nlike how we ended here.\nSo we are at 100% accuracy on our training\nset, and 92% accuracy on our validation set.\nSo that is pretty freakin great considering\nthe fact that this is a completely new dataset,\nnot having images that were included in the\noriginal image net model.\nSo these are pretty good results.\nA little bit overfitting here since our validation\naccuracy is lower than our training accuracy.\nSo if we wanted to fix that, then we could\ntake some necessary steps to combat that overfitting\nissue.\nBut if we look earlier at the earlier epochs,\nto see what kind of story is being told here,\non our first epoch, our training accuracy\nactually starts out at 74% among 10 classes,\nso that is not bad for a starting point.\nAnd we quickly get to 100% on our training\naccuracy, just within four epochs.\nSo that's great.\nBut you can see that, at that point, we're\nonly at 81% accuracy for our validation set.\nSo we have a decent amount of overfitting\ngoing on earlier on here.\nAnd then, as we progress through the training\nprocess, the overfitting is becoming less\nand less of a problem.\nAnd you can see that we actually at this point,\nif we just look at the last eight epochs that\nhave run here, we've not even stalled out\nyet on our validation loss.\nIt's not stalled out in terms of decreasing\nand our value.\nOur validation accuracy has not stalled out\nin terms of increasing so perhaps just running\nmore epochs on this data will eradicate the\noverfitting problem.\nOtherwise, you can do some tuning yourself\nchanging some hyper parameters around do a\ndifferent structure of fine tuning on the\nmodel.\nSo freeze Morrow.\nLess than the last 23 layers for during the\nfine tuning process, or just experiment yourself.\nAnd if you come up with something that yields\nbetter results than this, then put it in the\ncomments and let us know.\nSo we have one last thing we want to do with\nour fine tuned mobile net model, and that\nis use it on our test set.\nSo we are familiar, you know the drill with\nthis procedure at this point, we have done\nit several times.\nSo we are now going to get predictions from\nthe model on our test set.\nAnd then we are going to plot those predictions\nto a confusion matrix.\nSo we are first going to get our true labels\nby calling test batches classes.\nWe're then going to gain predictions from\nthe model by calling model dot predict and\npassing in our test set stored in test batches\nhere, setting verbose equal to zero, because\nwe do not want to see any output from the\npredictions.\nAnd now we are creating our confusion matrix.\nUsing socket learns confusion matrix that\nwe imported earlier, we are setting our true\nlabels equal to the test labels that we defined\njust here above, we are setting our predicted\nlabels to the arg max of our predictions across\nx is one.\nAnd now we are going to check out our class\nindices of the test batches just to make sure\nthey are what we think they are.\nAnd they are of course, classes labeled zero\nthrough nine.\nSo we define our labels for our confusion\nmatrix here, accordingly.\nAnd then we call our plot confusion matrix\nthat we brought in earlier in the notebook\nand that we have used 17,000 Times up to this\npoint in this course, and we are passing in\nour confusion matrix for what to plot, we\nare passing in our labels that we want to\ncorrespond to our confusion matrix, and giving\nour confusion matrix, the very general title\nof confusion matrix, because hey, that's what\nit is.\nSo let's plot this.\nOh, no.\nSo plot confusion matrix is not defined?\nWell, it definitely is just somewhere in this\nnotebook, I must have skipped over here we\ngo.\nNope, here we are.\nAlright, so here's where plot confusion matrix\nis defined.\nLet's bring that in.\nNow it is defined and run back here.\nSo looking from the top left to the bottom\nright, and now we see that the model appears\nto have done pretty well.\nSo we have 10 classes total, with five samples\nper class.\nAnd we see that we have mostly we have all\nfours and fives across this diagonal, meaning\nthat most of the time the model predicted\ncorrectly.\nSo for example, for a nine, five times out\nof five, the model predicted an image was\na nine when it actually was for an eight.\nHowever, only four out of five times did the\nmodel correctly predict looks like one of\nthe times the model, let's see, predicted\na one when it should have been an eight.\nBut in total, we've got 12345 incorrect predictions\nout of 50 total.\nSo that gives us a 90% accuracy rate on our\ntest set, which is not surprising for us,\ngiven the accuracy that we saw right above\non our validation set.\nSo hopefully this series on a mobile net has\ngiven you further insight to how we can fine\ntune models for custom data set and use transfer\nlearning.\nTo use the information that a model gained\nfrom its original training set on a completely\nnew task in the future.\nBe sure to check out the blog and other resources\navailable for this episode on the poser.com\nas well as the deep lizard hive mind where\nyou can gain access to exclusive perks and\nrewards.\nThanks for contributing to collective intelligence.\nNow let's move on to the next episode.\nHey, I'm Andy from deep lizard.\nAnd in this episode, we're going to learn\nhow we can use data augmentation on images\nusing tensor flows.\nkeras API.\nData augmentation occurs when we create a\nnew data by making modifications to some existing\ndata.\nWe're going to explain the idea a little bit\nfurther before we jump into the code.\nBut if you want a more thorough explanation,\nthen be sure to check out the corresponding\nepisode in the deep learning fundamentals\ncourse on depot's or.com.\nFor the example that will demonstrate in just\na moment, the data we'll be working with is\nimage data.\nAnd so for image data specifically, data augmentation\nwould include things like flipping the image,\neither horizontally or vertically.\nIt could include rotating the image, it could\ninclude changing the color of the image, and\nso on.\nOne of the major reasons we want to use data\naugmentation is to simply just get access\nto more data.\nSo a lot of times that not having access to\nenough data is an issue that we can run into.\nAnd we can run into problems like overfitting\nif our training data set is too small.\nSo that is a major reason to use data augmentation\nis to just grow our training set, adding augmented\ndata to our training set can in turn reduce\noverfitting as well.\nAlright, so now let's jump into the code to\nsee how we can augment image data using Kerris.\nAlright, so the first thing that we need to\ndo, of course, is import all of the packages\nthat we will be making use of for this data\naugmentation.\nNext, we have this plot images function, which\nwe've introduced earlier in the course, this\nis directly from tensor flows website.\nAnd it just allows us to apply images to our\nJupyter Notebook.\nSo check out the corresponding blog for this\nepisode on deep lizard calm to get the link\nso that you can go copy this function yourself.\nAlright, so next we have this variable called\nGen, which is an image data generator.\nAnd recall, we've actually worked with image\ndata generators. earlier in the course, whenever\nwe create our train at test and valid batches\nthat we were using for training seeing it\nand with this, though, we are using image\ndata generator in a different way.\nHere, we are creating this generator that\nwe are specifying all of these parameters\nlike rotation, range, width, shift range,\nheights of range, sheer range, zoom range\nchannels of drains in horizontal flip.\nSo these are all options that allow us to\naugment our image data.\nSo you need to check the documentation on\ntensor flows website to get an idea of the\nunits for these parameters, because they're\nnot all the same.\nSo for example, rotation range here, I believe\nis measured in radians.\nWhereas like this width, shift range is measured\nas a percentage of the width of the image.\nSo these are all ways that we can augment\nimage data.\nSo this is going to be rotating the image\nby 10 radians, this is going to be shifting\nthe width of the image by 10%, the height\nby 10%.\nZooming in shifting the color channels, flipping\nthe image, so all sorts of things.\nSo just all different ways that we can augment\nimage data.\nSo there are other options to just be sure\nto check out the image data generator documentation\nif you want to see those options.\nBut for now, these are the ones that we'll\nbe working with.\nSo we store that in our Jen variable.\nNext, we are going to choose a random image\nfrom a dog directory that we had set up earlier\nin the course under the dogs versus cats dataset,\nwe're going to go into train into dog, then\nwe are going to choose a random image from\nthis directory.\nAnd then we're going to set this image path\naccordingly.\nSo we're just going to set this to point to\nwhatever that chosen image was on disk, then\nwe have this assertion here just to make sure\nthat that is indeed a valid file before we\nproceed with the remaining code.\nNow we're just going to plot this image to\nthe screen.\nAnd I'm not sure what this is going to be\nsince it is a random image from disk.\nSo that is a cute looking.\nI don't know.\nBeagle, Beagle, basset hound mix, I don't\nknow, what do you guys think.\nSo this is the random dog that was selected\nfrom from our dog trained directory.\nNow we are creating this new variable called\nall etre.\nAnd to our image data generator that we created\nearlier called Jin, we're calling this flow\nfunction and passing our image in to flow.\nAnd this is going to generate a batch of augmented\nimages from this single image.\nSo next, we are defining this og images variable,\nwhich is going to give us 10 samples of the\naugmented images created by og etre here.\nLastly, we are going to plot these images\nusing the plot images that we defined just\nabove.\nAll right, so let's zoom in a bit.\nAll right, we can see now that first let's\ntake a look at our original dog Alright, so\nthat is the original image.\nNow we can look and see that given those things\nlike rotation and width shift, and everything\nthat we defined earlier, whenever we defined\nwhenever we defined our image data generator\nthat has now been done to All of these images\nin one random way or another so we can see\nkind of what's happening here.\nSo for example, this particular image looks\nlike it has been shifted up some because we\ncan see that the head of the dog is being\ncut off a little bit.\nAnd this image, or let's see which way was\nthe dog originally facing, so its head is\nfacing to the right.\nSo yeah, so this image here has been flipped,\nthe dog is now facing to the left.\nAnd this image appears to be shifted down\nsome.\nAnd so some of these, like this one looks\nlike it's been rotated.\nSo we can get an idea just by looking at the\nimages, the types of data augmentation that\nhave been done to them.\nAnd we can see how this could be very helpful\nfor growing our dataset in general, because\nfor example, say that we have a bunch of images\nof dogs, but for whatever reason, they're\nall facing to the left.\nAnd we want to deploy our model to to be some\ngeneral model that will classify different\ndogs.\nBut the types of images that will be accepted\nby this model later might have dogs facing\nto the right as well.\nWell, maybe our model totally implodes on\nitself whenever it receives a dog facing to\nthe right.\nSo through data augmentation, given the fact\nthat it is very normal for dogs to face left\nor right, we could augment all of our dog\nimages to have the data or to have the dogs\nalso face in the right direction, as well\nas the left direction to have a more have\na more dynamic data set.\nNow, there is a note in the corresponding\nblog for this episode on people's are calm,\ngiving just the brief instruction for how\nto save these images to disk if you want to\nsave the images after you augment them, and\nthen add them back to your training set.\nSo check that out if you're interested in\ndoing that to actually grow your training\nset.\nRather than just plot the images here and\nyour Jupyter Notebook.\nBy the way, we are currently in Vietnam filming\nthis episode.\nIf you didn't know we also have a vlog channel\nwhere we document our travels and share a\nlittle bit more about ourselves.\nSo check that out at beetles or vlog on YouTube.\nAlso, be sure to check out the corresponding\nblog for this episode, along with other resources\navailable on the blizzard.com.\nAnd check out the people's hive mind where\nyou can gain exclusive access to perks and\nrewards.\nThanks for contributing to collective intelligence.\nI'll see you next time.\n",
  "words": [
    "hey",
    "andy",
    "deep",
    "lizard",
    "course",
    "going",
    "learn",
    "use",
    "kerris",
    "neural",
    "network",
    "api",
    "written",
    "python",
    "integrated",
    "tensorflow",
    "throughout",
    "course",
    "lesson",
    "focus",
    "specific",
    "deep",
    "learning",
    "concept",
    "show",
    "full",
    "implementation",
    "code",
    "using",
    "keras",
    "api",
    "starting",
    "absolute",
    "basics",
    "learning",
    "organize",
    "pre",
    "processed",
    "data",
    "move",
    "building",
    "training",
    "artificial",
    "neural",
    "networks",
    "networks",
    "built",
    "scratch",
    "others",
    "pre",
    "trained",
    "state",
    "art",
    "models",
    "fine",
    "tune",
    "use",
    "custom",
    "data",
    "sets",
    "let",
    "discuss",
    "prerequisites",
    "needed",
    "follow",
    "along",
    "course",
    "knowledge",
    "standpoint",
    "give",
    "brief",
    "introductions",
    "deep",
    "learning",
    "concept",
    "going",
    "work",
    "go",
    "code",
    "implementation",
    "absolute",
    "beginner",
    "deep",
    "learning",
    "first",
    "recommend",
    "go",
    "deep",
    "learning",
    "fundamentals",
    "scores",
    "tables",
    "comm",
    "super",
    "eager",
    "jump",
    "code",
    "simultaneously",
    "take",
    "course",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "give",
    "knowledge",
    "need",
    "know",
    "get",
    "acquainted",
    "major",
    "deep",
    "learning",
    "concepts",
    "come",
    "back",
    "implement",
    "code",
    "using",
    "keras",
    "api",
    "course",
    "regard",
    "coding",
    "prerequisites",
    "basic",
    "programming",
    "skills",
    "python",
    "experience",
    "needed",
    "also",
    "find",
    "deep",
    "learning",
    "learning",
    "path",
    "see",
    "kerris",
    "course",
    "falls",
    "amidst",
    "deep",
    "lizard",
    "deep",
    "learning",
    "content",
    "let",
    "discuss",
    "course",
    "resources",
    "aside",
    "videos",
    "youtube",
    "also",
    "video",
    "text",
    "resources",
    "available",
    "peoples",
    "calm",
    "actually",
    "episode",
    "corresponding",
    "blog",
    "quiz",
    "available",
    "take",
    "test",
    "knowledge",
    "actually",
    "contribute",
    "quiz",
    "questions",
    "well",
    "see",
    "corresponding",
    "blog",
    "episode",
    "additionally",
    "code",
    "resources",
    "used",
    "course",
    "regularly",
    "tested",
    "maintained",
    "including",
    "updates",
    "bug",
    "fixes",
    "needed",
    "download",
    "access",
    "code",
    "files",
    "used",
    "course",
    "available",
    "members",
    "deep",
    "lizard",
    "hive",
    "mind",
    "check",
    "tables",
    "calm",
    "well",
    "alright",
    "know",
    "course",
    "resources",
    "available",
    "along",
    "prerequisites",
    "needed",
    "get",
    "started",
    "let",
    "talk",
    "little",
    "bit",
    "kerris",
    "kerris",
    "developed",
    "focus",
    "enabling",
    "fast",
    "user",
    "experimentation",
    "allows",
    "us",
    "go",
    "idea",
    "implementation",
    "steps",
    "aside",
    "benefit",
    "users",
    "often",
    "wonder",
    "choose",
    "kerris",
    "neural",
    "network",
    "api",
    "learn",
    "general",
    "neural",
    "network",
    "api",
    "learn",
    "general",
    "advice",
    "commit",
    "learning",
    "one",
    "sticking",
    "one",
    "forever",
    "recommend",
    "learn",
    "multiple",
    "neural",
    "network",
    "api",
    "idea",
    "fundamental",
    "understanding",
    "underlying",
    "concepts",
    "minor",
    "syntactical",
    "implementation",
    "differences",
    "neural",
    "network",
    "api",
    "really",
    "hard",
    "catch",
    "least",
    "one",
    "belt",
    "already",
    "especially",
    "job",
    "prospects",
    "knowing",
    "one",
    "neural",
    "network",
    "api",
    "show",
    "experience",
    "allow",
    "compare",
    "contrast",
    "differences",
    "api",
    "share",
    "opinions",
    "think",
    "certain",
    "api",
    "may",
    "better",
    "certain",
    "problems",
    "others",
    "problems",
    "able",
    "demonstrate",
    "make",
    "much",
    "valuable",
    "candidate",
    "previously",
    "touched",
    "fact",
    "kerris",
    "integrated",
    "tensorflow",
    "let",
    "discuss",
    "historically",
    "kerris",
    "high",
    "level",
    "neural",
    "network",
    "api",
    "could",
    "configure",
    "run",
    "one",
    "three",
    "separate",
    "lower",
    "level",
    "api",
    "lower",
    "level",
    "api",
    "tensorflow",
    "viano",
    "cntk",
    "later",
    "though",
    "kerris",
    "became",
    "fully",
    "integrated",
    "tensorflow",
    "api",
    "longer",
    "separate",
    "library",
    "choose",
    "run",
    "one",
    "three",
    "back",
    "end",
    "engines",
    "discussed",
    "previously",
    "important",
    "understand",
    "cares",
    "completely",
    "integrated",
    "tensorflow",
    "api",
    "course",
    "going",
    "focusing",
    "making",
    "use",
    "solely",
    "high",
    "level",
    "kerris",
    "api",
    "without",
    "necessarily",
    "making",
    "much",
    "use",
    "lower",
    "level",
    "tensorflow",
    "api",
    "start",
    "working",
    "cares",
    "first",
    "obviously",
    "get",
    "downloaded",
    "installed",
    "onto",
    "machines",
    "kerris",
    "fully",
    "integrated",
    "tensorflow",
    "installing",
    "tensorflow",
    "kerris",
    "come",
    "completely",
    "packaged",
    "tensorflow",
    "installation",
    "installation",
    "procedure",
    "simple",
    "running",
    "pip",
    "install",
    "tensorflow",
    "command",
    "line",
    "might",
    "want",
    "check",
    "system",
    "requirements",
    "tensor",
    "flows",
    "website",
    "make",
    "sure",
    "specific",
    "system",
    "requirements",
    "needed",
    "tensorflow",
    "install",
    "alright",
    "one",
    "last",
    "talking",
    "point",
    "get",
    "actual",
    "meat",
    "coding",
    "gpu",
    "support",
    "first",
    "important",
    "thing",
    "note",
    "gpu",
    "required",
    "follow",
    "course",
    "running",
    "machine",
    "cpu",
    "totally",
    "fine",
    "course",
    "however",
    "want",
    "run",
    "code",
    "gpu",
    "pretty",
    "easily",
    "get",
    "setup",
    "process",
    "setting",
    "gpu",
    "work",
    "tensorflow",
    "full",
    "guide",
    "get",
    "gpu",
    "setup",
    "work",
    "tensorflow",
    "deep",
    "lizard",
    "comm",
    "interested",
    "head",
    "go",
    "steps",
    "actually",
    "recommend",
    "going",
    "course",
    "cpu",
    "already",
    "set",
    "gpu",
    "like",
    "said",
    "code",
    "work",
    "completely",
    "fine",
    "run",
    "totally",
    "fine",
    "using",
    "cpu",
    "fact",
    "go",
    "course",
    "successfully",
    "get",
    "steps",
    "order",
    "work",
    "gpu",
    "one",
    "run",
    "code",
    "place",
    "earlier",
    "run",
    "gpu",
    "second",
    "go",
    "round",
    "kind",
    "look",
    "see",
    "kind",
    "efficiency",
    "speed",
    "ups",
    "see",
    "code",
    "alright",
    "kerris",
    "introduction",
    "finally",
    "ready",
    "jump",
    "code",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "dibbles",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "may",
    "deep",
    "lizard",
    "episode",
    "learn",
    "prepare",
    "process",
    "numerical",
    "data",
    "later",
    "use",
    "train",
    "first",
    "artificial",
    "neural",
    "network",
    "train",
    "neural",
    "network",
    "supervised",
    "learning",
    "task",
    "first",
    "need",
    "data",
    "set",
    "samples",
    "along",
    "corresponding",
    "labels",
    "samples",
    "referring",
    "word",
    "samples",
    "simply",
    "talking",
    "underlying",
    "data",
    "set",
    "data",
    "point",
    "set",
    "referred",
    "sample",
    "train",
    "model",
    "sentiment",
    "analysis",
    "headlines",
    "media",
    "source",
    "example",
    "labels",
    "correspond",
    "headline",
    "sample",
    "would",
    "positive",
    "negative",
    "say",
    "training",
    "artificial",
    "neural",
    "network",
    "identify",
    "images",
    "cats",
    "dogs",
    "well",
    "would",
    "mean",
    "image",
    "would",
    "corresponding",
    "label",
    "cat",
    "dog",
    "note",
    "deep",
    "learning",
    "may",
    "also",
    "hear",
    "samples",
    "referred",
    "inputs",
    "input",
    "data",
    "may",
    "hear",
    "labels",
    "referred",
    "targets",
    "target",
    "data",
    "preparing",
    "data",
    "set",
    "first",
    "need",
    "understand",
    "task",
    "data",
    "used",
    "example",
    "using",
    "data",
    "set",
    "train",
    "artificial",
    "neural",
    "network",
    "understand",
    "understand",
    "format",
    "data",
    "needs",
    "order",
    "us",
    "able",
    "pass",
    "data",
    "network",
    "first",
    "type",
    "neural",
    "network",
    "working",
    "called",
    "sequential",
    "model",
    "cares",
    "api",
    "discuss",
    "details",
    "sequential",
    "model",
    "future",
    "episode",
    "need",
    "understand",
    "type",
    "data",
    "format",
    "sequential",
    "model",
    "expects",
    "prepare",
    "dataset",
    "accordingly",
    "sequential",
    "model",
    "receives",
    "data",
    "training",
    "whenever",
    "call",
    "fit",
    "function",
    "going",
    "go",
    "details",
    "function",
    "future",
    "let",
    "check",
    "type",
    "data",
    "format",
    "fit",
    "function",
    "expects",
    "look",
    "fit",
    "documentation",
    "tensor",
    "flows",
    "website",
    "see",
    "first",
    "two",
    "parameters",
    "fit",
    "function",
    "expects",
    "x",
    "x",
    "input",
    "data",
    "samples",
    "words",
    "function",
    "expects",
    "x",
    "input",
    "data",
    "numpy",
    "array",
    "tensorflow",
    "tensor",
    "dict",
    "mapping",
    "tf",
    "dot",
    "data",
    "data",
    "set",
    "karass",
    "generator",
    "familiar",
    "data",
    "types",
    "okay",
    "first",
    "example",
    "going",
    "organize",
    "data",
    "numpy",
    "array",
    "first",
    "option",
    "documentation",
    "input",
    "samples",
    "also",
    "parameter",
    "expected",
    "fit",
    "function",
    "data",
    "set",
    "contains",
    "corresponding",
    "labels",
    "samples",
    "target",
    "data",
    "requirement",
    "formatted",
    "one",
    "formats",
    "discussed",
    "x",
    "needs",
    "format",
    "ca",
    "samples",
    "contained",
    "numpy",
    "array",
    "example",
    "target",
    "data",
    "labels",
    "samples",
    "tensorflow",
    "tensor",
    "format",
    "x",
    "need",
    "match",
    "going",
    "putting",
    "numpy",
    "arrays",
    "alright",
    "know",
    "data",
    "format",
    "model",
    "expects",
    "another",
    "reason",
    "may",
    "want",
    "transform",
    "process",
    "data",
    "put",
    "format",
    "make",
    "easier",
    "efficient",
    "model",
    "learn",
    "data",
    "normalization",
    "standardization",
    "techniques",
    "data",
    "processing",
    "deep",
    "learning",
    "vary",
    "greatly",
    "depending",
    "type",
    "data",
    "working",
    "start",
    "going",
    "work",
    "simple",
    "numerical",
    "data",
    "set",
    "train",
    "model",
    "later",
    "get",
    "exposure",
    "working",
    "different",
    "types",
    "data",
    "well",
    "alright",
    "ready",
    "prepare",
    "process",
    "first",
    "data",
    "set",
    "jupyter",
    "notebook",
    "first",
    "step",
    "import",
    "packages",
    "making",
    "use",
    "including",
    "numpy",
    "random",
    "modules",
    "scikit",
    "learn",
    "next",
    "create",
    "two",
    "list",
    "one",
    "called",
    "train",
    "samples",
    "one",
    "called",
    "train",
    "labels",
    "lists",
    "hold",
    "corresponding",
    "samples",
    "labels",
    "data",
    "set",
    "data",
    "set",
    "going",
    "working",
    "simple",
    "numerical",
    "data",
    "set",
    "task",
    "actually",
    "going",
    "create",
    "data",
    "later",
    "work",
    "practical",
    "examples",
    "realistic",
    "ones",
    "wo",
    "creating",
    "data",
    "instead",
    "downloading",
    "external",
    "source",
    "going",
    "create",
    "data",
    "train",
    "first",
    "artificial",
    "neural",
    "network",
    "motivation",
    "kind",
    "dummy",
    "data",
    "background",
    "story",
    "give",
    "us",
    "idea",
    "data",
    "let",
    "suppose",
    "experimental",
    "drug",
    "tested",
    "individuals",
    "ranging",
    "age",
    "13",
    "clinical",
    "trial",
    "trial",
    "2100",
    "participants",
    "total",
    "half",
    "participants",
    "age",
    "half",
    "65",
    "years",
    "older",
    "conclusions",
    "trial",
    "around",
    "95",
    "patients",
    "older",
    "population",
    "65",
    "older",
    "95",
    "patients",
    "experienced",
    "side",
    "effects",
    "around",
    "95",
    "patients",
    "65",
    "years",
    "old",
    "experienced",
    "side",
    "effects",
    "okay",
    "simplistic",
    "data",
    "set",
    "background",
    "story",
    "cell",
    "going",
    "go",
    "process",
    "actually",
    "creating",
    "data",
    "set",
    "first",
    "loop",
    "going",
    "generate",
    "approximately",
    "5",
    "younger",
    "individuals",
    "experience",
    "side",
    "effects",
    "5",
    "older",
    "individuals",
    "experience",
    "side",
    "effects",
    "within",
    "first",
    "loop",
    "first",
    "generating",
    "random",
    "number",
    "random",
    "integer",
    "rather",
    "13",
    "constituting",
    "younger",
    "individual",
    "65",
    "years",
    "age",
    "yet",
    "65",
    "years",
    "age",
    "going",
    "append",
    "number",
    "train",
    "samples",
    "list",
    "append",
    "one",
    "train",
    "labels",
    "list",
    "one",
    "representing",
    "fact",
    "patient",
    "experience",
    "side",
    "effects",
    "zero",
    "would",
    "represent",
    "patient",
    "experience",
    "side",
    "effects",
    "similarly",
    "jumped",
    "next",
    "line",
    "generating",
    "random",
    "integer",
    "65",
    "100",
    "represent",
    "older",
    "population",
    "remember",
    "first",
    "loop",
    "running",
    "50",
    "times",
    "kind",
    "outlier",
    "group",
    "5",
    "older",
    "individuals",
    "experience",
    "side",
    "effects",
    "take",
    "sample",
    "appended",
    "train",
    "samples",
    "list",
    "append",
    "zero",
    "corresponding",
    "train",
    "labels",
    "list",
    "since",
    "patients",
    "older",
    "patients",
    "experience",
    "side",
    "effects",
    "jumped",
    "next",
    "loop",
    "similarly",
    "pretty",
    "much",
    "code",
    "except",
    "bulk",
    "group",
    "loop",
    "running",
    "95",
    "younger",
    "individuals",
    "experience",
    "side",
    "effects",
    "well",
    "95",
    "older",
    "individuals",
    "experience",
    "side",
    "effects",
    "generating",
    "random",
    "number",
    "13",
    "six",
    "appending",
    "number",
    "representing",
    "age",
    "younger",
    "population",
    "train",
    "samples",
    "list",
    "appending",
    "label",
    "zero",
    "since",
    "individuals",
    "experience",
    "side",
    "effects",
    "appending",
    "zero",
    "train",
    "labels",
    "list",
    "similarly",
    "thing",
    "older",
    "individuals",
    "65",
    "except",
    "since",
    "majority",
    "guys",
    "experience",
    "side",
    "effects",
    "appending",
    "one",
    "train",
    "labels",
    "list",
    "summarize",
    "samples",
    "list",
    "contains",
    "bunch",
    "integers",
    "ranging",
    "13",
    "train",
    "labels",
    "list",
    "labels",
    "correspond",
    "individuals",
    "ages",
    "13",
    "labels",
    "correspond",
    "whether",
    "individuals",
    "experience",
    "side",
    "effects",
    "samples",
    "list",
    "containing",
    "ages",
    "labels",
    "list",
    "containing",
    "zeros",
    "ones",
    "representing",
    "side",
    "effects",
    "side",
    "effects",
    "get",
    "visualization",
    "samples",
    "printing",
    "samples",
    "list",
    "see",
    "integers",
    "like",
    "expect",
    "ranging",
    "13",
    "correspondingly",
    "run",
    "train",
    "labels",
    "list",
    "print",
    "data",
    "see",
    "list",
    "contains",
    "bunch",
    "zeros",
    "ones",
    "alright",
    "next",
    "step",
    "take",
    "lists",
    "start",
    "processing",
    "data",
    "generated",
    "need",
    "process",
    "format",
    "saw",
    "fit",
    "function",
    "expects",
    "discussed",
    "fact",
    "going",
    "passing",
    "data",
    "numpy",
    "arrays",
    "fit",
    "function",
    "next",
    "step",
    "go",
    "ahead",
    "transformation",
    "taking",
    "train",
    "labels",
    "list",
    "making",
    "numpy",
    "array",
    "similarly",
    "thing",
    "train",
    "samples",
    "list",
    "use",
    "shuffle",
    "function",
    "shuffle",
    "trained",
    "labels",
    "trained",
    "samples",
    "respective",
    "get",
    "rid",
    "imposed",
    "order",
    "data",
    "generation",
    "process",
    "okay",
    "data",
    "numpy",
    "array",
    "format",
    "expected",
    "fit",
    "function",
    "mentioned",
    "earlier",
    "another",
    "reason",
    "might",
    "want",
    "processing",
    "data",
    "either",
    "normalize",
    "standardize",
    "get",
    "way",
    "training",
    "neural",
    "network",
    "might",
    "become",
    "quicker",
    "efficient",
    "cell",
    "using",
    "min",
    "max",
    "scalar",
    "object",
    "create",
    "feature",
    "range",
    "ranging",
    "zero",
    "one",
    "use",
    "next",
    "line",
    "rescale",
    "data",
    "current",
    "scale",
    "13",
    "100",
    "scale",
    "zero",
    "one",
    "reshaping",
    "formality",
    "fit",
    "transform",
    "function",
    "accept",
    "one",
    "data",
    "default",
    "since",
    "data",
    "one",
    "dimensional",
    "reshape",
    "way",
    "able",
    "pass",
    "fit",
    "transform",
    "function",
    "print",
    "elements",
    "new",
    "scaled",
    "train",
    "samples",
    "variable",
    "calling",
    "new",
    "scaled",
    "samples",
    "print",
    "see",
    "individual",
    "elements",
    "longer",
    "integers",
    "ranging",
    "13",
    "instead",
    "values",
    "ranging",
    "anywhere",
    "zero",
    "one",
    "point",
    "generated",
    "raw",
    "data",
    "processed",
    "numpy",
    "array",
    "format",
    "model",
    "expect",
    "rescale",
    "data",
    "scale",
    "zero",
    "one",
    "upcoming",
    "episode",
    "use",
    "data",
    "train",
    "first",
    "artificial",
    "neural",
    "network",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "deep",
    "lizard",
    "calm",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "blizzard",
    "episode",
    "demonstrate",
    "create",
    "artificial",
    "neural",
    "network",
    "using",
    "sequential",
    "model",
    "keras",
    "api",
    "integrated",
    "within",
    "tensor",
    "flow",
    "last",
    "episode",
    "generated",
    "data",
    "imagined",
    "clinical",
    "trial",
    "create",
    "artificial",
    "neural",
    "network",
    "train",
    "data",
    "alright",
    "first",
    "things",
    "first",
    "need",
    "import",
    "tensorflow",
    "modules",
    "making",
    "use",
    "build",
    "first",
    "model",
    "includes",
    "everything",
    "see",
    "except",
    "actually",
    "last",
    "two",
    "atom",
    "categorical",
    "cross",
    "vitry",
    "categorical",
    "cross",
    "entropy",
    "rather",
    "two",
    "going",
    "used",
    "train",
    "model",
    "build",
    "going",
    "ahead",
    "bringing",
    "imports",
    "next",
    "running",
    "code",
    "gpu",
    "run",
    "cell",
    "allow",
    "make",
    "sure",
    "tensorflow",
    "correctly",
    "identifying",
    "gpu",
    "well",
    "enable",
    "memory",
    "growth",
    "lines",
    "blog",
    "check",
    "exactly",
    "means",
    "might",
    "want",
    "running",
    "gpu",
    "go",
    "ahead",
    "run",
    "cell",
    "next",
    "actual",
    "model",
    "building",
    "sequential",
    "model",
    "kind",
    "simplest",
    "type",
    "model",
    "build",
    "using",
    "kerris",
    "tensorflow",
    "sequential",
    "model",
    "described",
    "linear",
    "stack",
    "layers",
    "look",
    "creating",
    "model",
    "exactly",
    "looks",
    "like",
    "initializing",
    "model",
    "instance",
    "sequential",
    "class",
    "passing",
    "list",
    "layers",
    "important",
    "note",
    "first",
    "dense",
    "layer",
    "looking",
    "actually",
    "second",
    "layer",
    "overall",
    "first",
    "hidden",
    "layer",
    "input",
    "layer",
    "explicitly",
    "defining",
    "using",
    "kerris",
    "input",
    "data",
    "creates",
    "input",
    "layer",
    "way",
    "model",
    "knows",
    "type",
    "input",
    "data",
    "expect",
    "shape",
    "input",
    "data",
    "rather",
    "input",
    "shape",
    "parameter",
    "pass",
    "first",
    "dense",
    "layer",
    "model",
    "understands",
    "shape",
    "input",
    "data",
    "expect",
    "therefore",
    "accepts",
    "shape",
    "input",
    "data",
    "passes",
    "data",
    "first",
    "hidden",
    "layer",
    "dense",
    "layer",
    "case",
    "telling",
    "dense",
    "layer",
    "want",
    "16",
    "units",
    "units",
    "also",
    "otherwise",
    "known",
    "nodes",
    "neurons",
    "choice",
    "16",
    "actually",
    "pretty",
    "arbitrary",
    "model",
    "overall",
    "simple",
    "arbitrary",
    "choice",
    "nodes",
    "actually",
    "going",
    "pretty",
    "hard",
    "create",
    "simple",
    "model",
    "least",
    "wo",
    "good",
    "job",
    "classifying",
    "data",
    "given",
    "simplicity",
    "data",
    "understand",
    "passing",
    "specifying",
    "16",
    "units",
    "first",
    "hidden",
    "layer",
    "specifying",
    "input",
    "shape",
    "model",
    "knows",
    "shape",
    "input",
    "data",
    "expect",
    "stating",
    "want",
    "relu",
    "activation",
    "function",
    "follow",
    "dense",
    "layer",
    "input",
    "shape",
    "parameter",
    "specified",
    "first",
    "hidden",
    "layer",
    "one",
    "hidden",
    "dense",
    "layer",
    "time",
    "arbitrarily",
    "setting",
    "number",
    "units",
    "dense",
    "layer",
    "following",
    "layer",
    "activation",
    "function",
    "value",
    "lastly",
    "specify",
    "last",
    "output",
    "layer",
    "last",
    "layer",
    "output",
    "layer",
    "another",
    "dense",
    "layer",
    "time",
    "two",
    "units",
    "corresponding",
    "two",
    "possible",
    "output",
    "classes",
    "either",
    "patient",
    "experience",
    "side",
    "effects",
    "patient",
    "experience",
    "side",
    "effects",
    "following",
    "output",
    "layer",
    "softmax",
    "function",
    "going",
    "give",
    "us",
    "probabilities",
    "output",
    "class",
    "whether",
    "patient",
    "experience",
    "side",
    "effects",
    "output",
    "probability",
    "class",
    "letting",
    "us",
    "know",
    "class",
    "probable",
    "given",
    "patient",
    "case",
    "clear",
    "dense",
    "layer",
    "call",
    "densely",
    "connected",
    "layer",
    "fully",
    "connected",
    "layer",
    "probably",
    "well",
    "known",
    "type",
    "layer",
    "artificial",
    "neural",
    "networks",
    "case",
    "need",
    "refresher",
    "fully",
    "connected",
    "layers",
    "activation",
    "functions",
    "anything",
    "else",
    "discussed",
    "point",
    "know",
    "covered",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "need",
    "go",
    "refresh",
    "memory",
    "topics",
    "alright",
    "run",
    "cell",
    "create",
    "model",
    "use",
    "model",
    "dot",
    "summary",
    "print",
    "visual",
    "summary",
    "architecture",
    "model",
    "created",
    "looking",
    "see",
    "visual",
    "representation",
    "architecture",
    "created",
    "cell",
    "right",
    "finished",
    "creating",
    "first",
    "neural",
    "network",
    "using",
    "simple",
    "intuitive",
    "sequential",
    "model",
    "type",
    "next",
    "episode",
    "see",
    "use",
    "data",
    "created",
    "last",
    "time",
    "train",
    "network",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "depot",
    "well",
    "tables",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "see",
    "train",
    "artificial",
    "neural",
    "network",
    "using",
    "keras",
    "api",
    "integrated",
    "tensorflow",
    "previous",
    "episodes",
    "went",
    "steps",
    "generate",
    "data",
    "also",
    "build",
    "artificial",
    "neural",
    "network",
    "bring",
    "two",
    "together",
    "actually",
    "train",
    "network",
    "data",
    "created",
    "processed",
    "alright",
    "picking",
    "last",
    "time",
    "jupyter",
    "notebook",
    "make",
    "sure",
    "still",
    "imports",
    "included",
    "already",
    "ran",
    "continue",
    "first",
    "building",
    "model",
    "going",
    "call",
    "model",
    "compile",
    "function",
    "prepares",
    "model",
    "training",
    "gets",
    "everything",
    "order",
    "needed",
    "actually",
    "train",
    "model",
    "first",
    "specifying",
    "compile",
    "function",
    "optimizer",
    "want",
    "use",
    "choosing",
    "use",
    "common",
    "optimizer",
    "atom",
    "learning",
    "rate",
    "next",
    "specify",
    "type",
    "loss",
    "need",
    "use",
    "case",
    "going",
    "use",
    "sparse",
    "categorical",
    "cross",
    "going",
    "use",
    "sparse",
    "categorical",
    "cross",
    "entropy",
    "lastly",
    "specify",
    "metrics",
    "want",
    "see",
    "model",
    "performance",
    "want",
    "able",
    "judge",
    "model",
    "specifying",
    "list",
    "includes",
    "accuracy",
    "common",
    "way",
    "able",
    "evaluate",
    "model",
    "performance",
    "run",
    "cell",
    "alright",
    "model",
    "compiled",
    "ready",
    "training",
    "training",
    "occurs",
    "whenever",
    "call",
    "fit",
    "function",
    "recall",
    "earlier",
    "course",
    "actually",
    "looked",
    "documentation",
    "split",
    "function",
    "knew",
    "process",
    "input",
    "data",
    "fit",
    "first",
    "parameter",
    "specifying",
    "x",
    "input",
    "data",
    "currently",
    "stored",
    "scaled",
    "train",
    "samples",
    "variable",
    "target",
    "data",
    "labels",
    "labels",
    "currently",
    "stored",
    "train",
    "labels",
    "variable",
    "specifying",
    "next",
    "specify",
    "batch",
    "size",
    "want",
    "use",
    "training",
    "many",
    "samples",
    "included",
    "one",
    "batch",
    "passed",
    "processed",
    "network",
    "one",
    "time",
    "setting",
    "number",
    "epochs",
    "want",
    "run",
    "setting",
    "means",
    "model",
    "going",
    "process",
    "train",
    "data",
    "data",
    "set",
    "30",
    "times",
    "completing",
    "total",
    "training",
    "process",
    "next",
    "specifying",
    "shuffle",
    "shuffle",
    "parameter",
    "setting",
    "true",
    "default",
    "already",
    "set",
    "true",
    "bringing",
    "attention",
    "show",
    "make",
    "aware",
    "fact",
    "data",
    "shuffled",
    "default",
    "pass",
    "network",
    "good",
    "thing",
    "want",
    "order",
    "inside",
    "dataset",
    "kind",
    "erased",
    "pass",
    "data",
    "model",
    "model",
    "necessarily",
    "learning",
    "anything",
    "order",
    "data",
    "set",
    "true",
    "default",
    "necessarily",
    "specify",
    "letting",
    "know",
    "actually",
    "see",
    "something",
    "next",
    "episode",
    "important",
    "regarding",
    "validation",
    "data",
    "see",
    "coming",
    "last",
    "parameter",
    "specify",
    "verbose",
    "option",
    "allow",
    "us",
    "see",
    "output",
    "whenever",
    "run",
    "fit",
    "function",
    "either",
    "set",
    "01",
    "two",
    "two",
    "verbose",
    "level",
    "terms",
    "output",
    "messages",
    "setting",
    "get",
    "highest",
    "level",
    "output",
    "let",
    "run",
    "cell",
    "training",
    "begin",
    "right",
    "training",
    "stopped",
    "run",
    "30",
    "epochs",
    "look",
    "progress",
    "model",
    "starting",
    "first",
    "epoch",
    "loss",
    "value",
    "currently",
    "point",
    "six",
    "eight",
    "accuracy",
    "50",
    "better",
    "chance",
    "pretty",
    "quickly",
    "looking",
    "accuracy",
    "tell",
    "steadily",
    "increasing",
    "way",
    "get",
    "last",
    "poc",
    "yielding",
    "94",
    "accuracy",
    "loss",
    "also",
    "steadily",
    "decreased",
    "point",
    "six",
    "five",
    "range",
    "point",
    "two",
    "seven",
    "see",
    "model",
    "train",
    "quickly",
    "epoch",
    "taking",
    "one",
    "second",
    "run",
    "within",
    "30",
    "epochs",
    "already",
    "94",
    "accuracy",
    "right",
    "although",
    "simple",
    "model",
    "training",
    "simple",
    "data",
    "see",
    "without",
    "much",
    "effort",
    "able",
    "yield",
    "pretty",
    "great",
    "results",
    "relatively",
    "quick",
    "manner",
    "time",
    "well",
    "subsequent",
    "episodes",
    "demonstrate",
    "work",
    "complex",
    "models",
    "well",
    "complex",
    "data",
    "hopefully",
    "example",
    "served",
    "purpose",
    "encouraging",
    "easy",
    "get",
    "started",
    "kerris",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "v",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "demonstrate",
    "use",
    "tensor",
    "flows",
    "keras",
    "api",
    "create",
    "validation",
    "set",
    "fly",
    "training",
    "demonstrate",
    "build",
    "validation",
    "set",
    "using",
    "kerris",
    "let",
    "first",
    "talk",
    "exactly",
    "validation",
    "set",
    "whenever",
    "train",
    "model",
    "hope",
    "train",
    "see",
    "good",
    "results",
    "training",
    "output",
    "low",
    "loss",
    "high",
    "accuracy",
    "ever",
    "train",
    "model",
    "sake",
    "training",
    "want",
    "take",
    "model",
    "hopefully",
    "able",
    "use",
    "way",
    "data",
    "necessarily",
    "exposed",
    "training",
    "process",
    "although",
    "new",
    "data",
    "data",
    "model",
    "never",
    "seen",
    "hope",
    "model",
    "good",
    "enough",
    "able",
    "generalize",
    "well",
    "new",
    "data",
    "give",
    "accurate",
    "predictions",
    "actually",
    "get",
    "understanding",
    "well",
    "model",
    "generalizing",
    "introducing",
    "validation",
    "set",
    "training",
    "process",
    "create",
    "validation",
    "set",
    "training",
    "begins",
    "choose",
    "take",
    "subset",
    "training",
    "set",
    "separate",
    "separate",
    "set",
    "labeled",
    "validation",
    "data",
    "training",
    "process",
    "model",
    "train",
    "training",
    "data",
    "validate",
    "separated",
    "validation",
    "data",
    "mean",
    "validating",
    "well",
    "essentially",
    "addition",
    "validation",
    "set",
    "training",
    "model",
    "learning",
    "features",
    "training",
    "set",
    "already",
    "seen",
    "addition",
    "epoch",
    "model",
    "gone",
    "actual",
    "training",
    "process",
    "take",
    "learned",
    "training",
    "data",
    "validate",
    "predicting",
    "data",
    "validation",
    "set",
    "using",
    "learned",
    "training",
    "data",
    "though",
    "training",
    "process",
    "look",
    "output",
    "accuracy",
    "loss",
    "seeing",
    "accuracy",
    "loss",
    "computed",
    "training",
    "set",
    "also",
    "see",
    "computed",
    "validation",
    "set",
    "important",
    "understand",
    "though",
    "model",
    "alerting",
    "training",
    "training",
    "data",
    "taking",
    "validation",
    "set",
    "account",
    "training",
    "validation",
    "set",
    "us",
    "able",
    "see",
    "well",
    "model",
    "able",
    "predict",
    "data",
    "exposed",
    "training",
    "process",
    "words",
    "allows",
    "us",
    "see",
    "general",
    "model",
    "well",
    "able",
    "generalize",
    "data",
    "included",
    "training",
    "data",
    "knowing",
    "information",
    "allow",
    "us",
    "see",
    "model",
    "running",
    "famous",
    "overfitting",
    "problem",
    "overfitting",
    "occurs",
    "model",
    "learned",
    "specific",
    "features",
    "training",
    "set",
    "really",
    "well",
    "unable",
    "generalize",
    "data",
    "seen",
    "training",
    "see",
    "model",
    "giving",
    "really",
    "good",
    "results",
    "training",
    "set",
    "less",
    "good",
    "results",
    "validation",
    "set",
    "conclude",
    "overfitting",
    "problem",
    "take",
    "steps",
    "necessary",
    "combat",
    "specific",
    "issue",
    "like",
    "see",
    "overfitting",
    "problem",
    "covered",
    "detail",
    "episode",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "alright",
    "let",
    "discuss",
    "create",
    "use",
    "validation",
    "set",
    "karass",
    "sequential",
    "model",
    "actually",
    "two",
    "ways",
    "create",
    "work",
    "validation",
    "sets",
    "sequential",
    "model",
    "first",
    "way",
    "completely",
    "separate",
    "validation",
    "set",
    "training",
    "set",
    "pass",
    "validation",
    "set",
    "model",
    "fit",
    "function",
    "validation",
    "data",
    "parameter",
    "set",
    "equal",
    "structure",
    "holding",
    "validation",
    "data",
    "write",
    "corresponding",
    "blog",
    "episode",
    "contains",
    "details",
    "format",
    "data",
    "needs",
    "going",
    "actually",
    "focus",
    "second",
    "way",
    "creating",
    "using",
    "validation",
    "set",
    "step",
    "actually",
    "saves",
    "us",
    "step",
    "explicitly",
    "go",
    "creation",
    "process",
    "validation",
    "set",
    "instead",
    "get",
    "kerris",
    "create",
    "us",
    "alright",
    "back",
    "jupyter",
    "notebook",
    "right",
    "left",
    "last",
    "time",
    "model",
    "dot",
    "fit",
    "function",
    "recall",
    "use",
    "last",
    "time",
    "train",
    "model",
    "already",
    "edited",
    "cell",
    "include",
    "new",
    "parameter",
    "validation",
    "split",
    "validation",
    "split",
    "sounds",
    "like",
    "splits",
    "portion",
    "training",
    "set",
    "validation",
    "set",
    "set",
    "number",
    "zero",
    "one",
    "fractional",
    "number",
    "tell",
    "kerris",
    "much",
    "training",
    "set",
    "need",
    "split",
    "validation",
    "set",
    "splitting",
    "10",
    "training",
    "set",
    "important",
    "note",
    "whenever",
    "validation",
    "set",
    "completely",
    "held",
    "training",
    "set",
    "training",
    "samples",
    "remove",
    "training",
    "set",
    "validation",
    "set",
    "longer",
    "contained",
    "within",
    "training",
    "data",
    "longer",
    "using",
    "approach",
    "validation",
    "set",
    "created",
    "fly",
    "whenever",
    "call",
    "fit",
    "function",
    "one",
    "thing",
    "worth",
    "mentioning",
    "remember",
    "last",
    "time",
    "discussed",
    "shuffle",
    "equals",
    "true",
    "parameter",
    "said",
    "default",
    "training",
    "set",
    "shuffled",
    "whenever",
    "call",
    "fit",
    "shuffle",
    "equals",
    "true",
    "already",
    "set",
    "default",
    "bringing",
    "let",
    "know",
    "training",
    "set",
    "shuffled",
    "good",
    "thing",
    "want",
    "training",
    "set",
    "shuffled",
    "whenever",
    "call",
    "validation",
    "split",
    "way",
    "split",
    "occurs",
    "training",
    "set",
    "shuffled",
    "meaning",
    "created",
    "training",
    "set",
    "say",
    "put",
    "sick",
    "patients",
    "first",
    "non",
    "sick",
    "patients",
    "second",
    "say",
    "want",
    "split",
    "last",
    "10",
    "training",
    "data",
    "validation",
    "data",
    "going",
    "take",
    "last",
    "10",
    "training",
    "data",
    "therefore",
    "could",
    "take",
    "second",
    "group",
    "put",
    "training",
    "set",
    "get",
    "first",
    "group",
    "wanted",
    "mention",
    "although",
    "training",
    "data",
    "shuffled",
    "fit",
    "function",
    "already",
    "shuffled",
    "training",
    "data",
    "pass",
    "fit",
    "also",
    "use",
    "validation",
    "split",
    "parameter",
    "important",
    "know",
    "validation",
    "set",
    "going",
    "last",
    "x",
    "percent",
    "training",
    "set",
    "therefore",
    "may",
    "shuffled",
    "may",
    "yield",
    "strange",
    "results",
    "think",
    "everything",
    "shuffled",
    "really",
    "training",
    "set",
    "shuffled",
    "validation",
    "set",
    "taken",
    "keep",
    "mind",
    "way",
    "created",
    "training",
    "set",
    "episode",
    "actually",
    "shuffled",
    "training",
    "data",
    "ever",
    "passed",
    "fit",
    "function",
    "future",
    "whenever",
    "working",
    "data",
    "good",
    "idea",
    "make",
    "sure",
    "data",
    "also",
    "shuffled",
    "beforehand",
    "especially",
    "going",
    "making",
    "use",
    "validation",
    "split",
    "parameter",
    "create",
    "validation",
    "set",
    "alright",
    "run",
    "cell",
    "one",
    "time",
    "calling",
    "fit",
    "function",
    "time",
    "see",
    "loss",
    "accuracy",
    "metrics",
    "training",
    "set",
    "also",
    "see",
    "metrics",
    "validation",
    "set",
    "alright",
    "model",
    "finished",
    "running",
    "30",
    "epochs",
    "see",
    "loss",
    "accuracy",
    "left",
    "hand",
    "side",
    "well",
    "validation",
    "loss",
    "validation",
    "accuracy",
    "right",
    "hand",
    "side",
    "see",
    "let",
    "look",
    "accuracy",
    "two",
    "starting",
    "around",
    "50",
    "mark",
    "going",
    "gradually",
    "around",
    "rate",
    "scroll",
    "way",
    "last",
    "epoch",
    "see",
    "accuracy",
    "validation",
    "accuracy",
    "pretty",
    "similar",
    "1",
    "difference",
    "two",
    "yet",
    "loss",
    "values",
    "similar",
    "well",
    "see",
    "example",
    "model",
    "overfitting",
    "actually",
    "performing",
    "pretty",
    "well",
    "well",
    "rather",
    "validation",
    "set",
    "training",
    "set",
    "model",
    "generalizing",
    "well",
    "however",
    "saw",
    "opposite",
    "case",
    "true",
    "validation",
    "accuracy",
    "seriously",
    "lagging",
    "behind",
    "training",
    "accuracy",
    "know",
    "overfitting",
    "problem",
    "would",
    "need",
    "take",
    "steps",
    "address",
    "issue",
    "alright",
    "seen",
    "train",
    "model",
    "validate",
    "model",
    "make",
    "use",
    "training",
    "validation",
    "sets",
    "next",
    "episode",
    "going",
    "see",
    "make",
    "use",
    "third",
    "data",
    "set",
    "test",
    "data",
    "set",
    "use",
    "model",
    "inference",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "depot",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "see",
    "use",
    "neural",
    "network",
    "inference",
    "predict",
    "data",
    "test",
    "set",
    "using",
    "tensorflow",
    "keras",
    "api",
    "touched",
    "previously",
    "whenever",
    "train",
    "model",
    "hope",
    "take",
    "model",
    "use",
    "new",
    "data",
    "seen",
    "training",
    "hopefully",
    "model",
    "able",
    "generalize",
    "well",
    "give",
    "us",
    "good",
    "results",
    "new",
    "data",
    "simple",
    "example",
    "suppose",
    "trained",
    "network",
    "able",
    "identify",
    "images",
    "cats",
    "dogs",
    "training",
    "process",
    "course",
    "training",
    "set",
    "say",
    "downloaded",
    "website",
    "1000s",
    "images",
    "cats",
    "dogs",
    "hope",
    "later",
    "wanted",
    "could",
    "maybe",
    "build",
    "web",
    "app",
    "example",
    "could",
    "people",
    "world",
    "submit",
    "dog",
    "cat",
    "photos",
    "model",
    "tell",
    "high",
    "accuracy",
    "whether",
    "animal",
    "cat",
    "dog",
    "know",
    "anyone",
    "would",
    "actually",
    "make",
    "web",
    "app",
    "get",
    "point",
    "hope",
    "even",
    "though",
    "images",
    "sent",
    "people",
    "around",
    "world",
    "cats",
    "dogs",
    "even",
    "though",
    "included",
    "training",
    "set",
    "model",
    "originally",
    "trained",
    "hopefully",
    "models",
    "able",
    "generalize",
    "well",
    "enough",
    "understand",
    "learned",
    "dog",
    "cat",
    "features",
    "predict",
    "mandy",
    "dog",
    "actually",
    "dog",
    "cat",
    "example",
    "call",
    "process",
    "inference",
    "model",
    "takes",
    "learned",
    "training",
    "uses",
    "knowledge",
    "infer",
    "things",
    "data",
    "seen",
    "practice",
    "might",
    "hold",
    "subset",
    "training",
    "data",
    "put",
    "set",
    "called",
    "test",
    "set",
    "typically",
    "model",
    "trained",
    "validated",
    "take",
    "model",
    "use",
    "inference",
    "purposes",
    "test",
    "set",
    "one",
    "additional",
    "step",
    "validation",
    "make",
    "sure",
    "model",
    "generalizing",
    "well",
    "deploy",
    "model",
    "production",
    "point",
    "model",
    "working",
    "last",
    "episodes",
    "trained",
    "validated",
    "given",
    "metrics",
    "saw",
    "validation",
    "process",
    "good",
    "idea",
    "model",
    "probably",
    "going",
    "pretty",
    "good",
    "job",
    "inference",
    "test",
    "set",
    "well",
    "order",
    "conclude",
    "though",
    "first",
    "would",
    "need",
    "create",
    "test",
    "set",
    "going",
    "create",
    "test",
    "set",
    "use",
    "model",
    "inference",
    "alright",
    "back",
    "jupyter",
    "notebook",
    "going",
    "go",
    "process",
    "creating",
    "test",
    "set",
    "actually",
    "glance",
    "code",
    "see",
    "whole",
    "process",
    "setting",
    "samples",
    "labels",
    "list",
    "generating",
    "data",
    "imagine",
    "clinical",
    "trial",
    "discussed",
    "previous",
    "episode",
    "taking",
    "generated",
    "data",
    "putting",
    "numpy",
    "array",
    "format",
    "shuffling",
    "data",
    "scaling",
    "data",
    "scale",
    "zero",
    "one",
    "rather",
    "scale",
    "13",
    "actually",
    "exact",
    "process",
    "using",
    "almost",
    "exact",
    "code",
    "except",
    "working",
    "tests",
    "labels",
    "test",
    "samples",
    "variables",
    "rather",
    "train",
    "labels",
    "train",
    "samples",
    "going",
    "go",
    "line",
    "line",
    "code",
    "need",
    "refresher",
    "go",
    "check",
    "earlier",
    "episode",
    "exact",
    "process",
    "training",
    "set",
    "important",
    "thing",
    "take",
    "process",
    "though",
    "test",
    "set",
    "prepared",
    "processed",
    "format",
    "training",
    "data",
    "go",
    "ahead",
    "run",
    "cells",
    "test",
    "create",
    "process",
    "test",
    "data",
    "going",
    "use",
    "model",
    "predict",
    "test",
    "data",
    "obtain",
    "predictions",
    "model",
    "call",
    "predict",
    "model",
    "created",
    "last",
    "couple",
    "episodes",
    "calling",
    "model",
    "dot",
    "predict",
    "first",
    "passing",
    "parameter",
    "x",
    "setting",
    "equal",
    "scaled",
    "test",
    "samples",
    "created",
    "line",
    "scaled",
    "test",
    "samples",
    "scale",
    "zero",
    "one",
    "data",
    "want",
    "model",
    "predict",
    "specify",
    "batch",
    "size",
    "setting",
    "batch",
    "size",
    "equal",
    "10",
    "exact",
    "batch",
    "size",
    "use",
    "training",
    "data",
    "whenever",
    "train",
    "model",
    "well",
    "last",
    "parameter",
    "specifying",
    "verbose",
    "parameter",
    "setting",
    "equal",
    "zero",
    "predicting",
    "output",
    "function",
    "actually",
    "care",
    "seeing",
    "going",
    "use",
    "us",
    "moment",
    "setting",
    "equal",
    "zero",
    "get",
    "output",
    "alright",
    "run",
    "model",
    "predicts",
    "data",
    "test",
    "set",
    "want",
    "visualization",
    "predictions",
    "model",
    "looks",
    "like",
    "sample",
    "print",
    "looking",
    "predictions",
    "way",
    "interpret",
    "element",
    "within",
    "test",
    "set",
    "sample",
    "tests",
    "getting",
    "probability",
    "maps",
    "either",
    "patient",
    "experiencing",
    "side",
    "effect",
    "patient",
    "experiencing",
    "side",
    "effect",
    "first",
    "sample",
    "test",
    "set",
    "prediction",
    "says",
    "model",
    "92",
    "assigning",
    "92",
    "probability",
    "patient",
    "experiencing",
    "side",
    "effect",
    "around",
    "8",
    "probability",
    "patient",
    "experiencing",
    "side",
    "effect",
    "recall",
    "said",
    "side",
    "effect",
    "experience",
    "labeled",
    "zero",
    "side",
    "effect",
    "experienced",
    "labeled",
    "one",
    "know",
    "particular",
    "probability",
    "maps",
    "side",
    "effect",
    "zeroeth",
    "index",
    "specific",
    "probability",
    "maps",
    "side",
    "effect",
    "first",
    "index",
    "interested",
    "seeing",
    "probable",
    "prediction",
    "sample",
    "test",
    "set",
    "run",
    "cell",
    "taking",
    "predictions",
    "getting",
    "index",
    "prediction",
    "highest",
    "probability",
    "print",
    "see",
    "little",
    "bit",
    "easier",
    "interpret",
    "previous",
    "output",
    "see",
    "first",
    "sample",
    "prediction",
    "zero",
    "second",
    "sample",
    "one",
    "confirm",
    "go",
    "back",
    "see",
    "first",
    "sample",
    "indeed",
    "higher",
    "probability",
    "label",
    "zero",
    "meaning",
    "side",
    "effects",
    "second",
    "sample",
    "higher",
    "probability",
    "one",
    "meaning",
    "patient",
    "experience",
    "side",
    "effect",
    "prediction",
    "results",
    "able",
    "actually",
    "see",
    "underlying",
    "predictions",
    "able",
    "make",
    "much",
    "sense",
    "terms",
    "well",
    "model",
    "add",
    "predictions",
    "supply",
    "labels",
    "model",
    "inference",
    "way",
    "training",
    "nature",
    "inference",
    "lot",
    "times",
    "inference",
    "occurring",
    "model",
    "deployed",
    "production",
    "necessarily",
    "correct",
    "labels",
    "data",
    "model",
    "inferring",
    "corresponding",
    "labels",
    "test",
    "set",
    "though",
    "case",
    "ones",
    "generated",
    "test",
    "data",
    "visualize",
    "prediction",
    "results",
    "plotting",
    "confusion",
    "matrix",
    "give",
    "us",
    "overall",
    "idea",
    "accurate",
    "model",
    "inference",
    "test",
    "data",
    "see",
    "exactly",
    "done",
    "next",
    "episode",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "depot",
    "well",
    "tables",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "demonstrate",
    "use",
    "confusion",
    "matrix",
    "visualize",
    "prediction",
    "results",
    "neural",
    "network",
    "inference",
    "last",
    "episode",
    "showed",
    "could",
    "use",
    "train",
    "model",
    "inference",
    "data",
    "contained",
    "test",
    "set",
    "although",
    "labels",
    "test",
    "set",
    "pass",
    "model",
    "inference",
    "get",
    "type",
    "accuracy",
    "readings",
    "well",
    "model",
    "test",
    "set",
    "using",
    "confusion",
    "matrix",
    "visually",
    "observe",
    "well",
    "model",
    "predicts",
    "test",
    "data",
    "let",
    "jump",
    "right",
    "code",
    "see",
    "exactly",
    "done",
    "using",
    "scikit",
    "learn",
    "create",
    "confusion",
    "matrix",
    "first",
    "thing",
    "need",
    "import",
    "necessary",
    "packages",
    "making",
    "use",
    "next",
    "week",
    "create",
    "confusion",
    "matrix",
    "calling",
    "confusion",
    "matrix",
    "function",
    "scikit",
    "learn",
    "pass",
    "test",
    "labels",
    "true",
    "labels",
    "pass",
    "predictions",
    "predictions",
    "confusion",
    "matrix",
    "specs",
    "recall",
    "rounded",
    "predictions",
    "variable",
    "well",
    "test",
    "labels",
    "created",
    "last",
    "episode",
    "rounded",
    "predictions",
    "recall",
    "use",
    "arg",
    "max",
    "function",
    "select",
    "probable",
    "predictions",
    "predictions",
    "format",
    "labels",
    "zeros",
    "ones",
    "correspond",
    "whether",
    "patient",
    "side",
    "effects",
    "next",
    "plot",
    "confusion",
    "matrix",
    "function",
    "directly",
    "copied",
    "socket",
    "learns",
    "website",
    "link",
    "site",
    "corresponding",
    "blog",
    "copy",
    "exact",
    "function",
    "function",
    "socket",
    "learn",
    "created",
    "able",
    "easily",
    "plot",
    "notebook",
    "confusion",
    "matrix",
    "going",
    "actual",
    "visual",
    "output",
    "want",
    "see",
    "run",
    "cell",
    "define",
    "function",
    "create",
    "list",
    "labels",
    "use",
    "computer",
    "matrix",
    "want",
    "labels",
    "side",
    "effects",
    "side",
    "effects",
    "corresponding",
    "labels",
    "test",
    "data",
    "going",
    "call",
    "plot",
    "confusion",
    "matrix",
    "function",
    "brought",
    "defined",
    "scikit",
    "learn",
    "going",
    "pass",
    "confusion",
    "matrix",
    "classes",
    "confusion",
    "matrix",
    "specifying",
    "cm",
    "plot",
    "labels",
    "defined",
    "right",
    "lastly",
    "title",
    "going",
    "title",
    "display",
    "confusion",
    "matrix",
    "run",
    "actually",
    "get",
    "confusion",
    "matrix",
    "plot",
    "alright",
    "predicted",
    "labels",
    "x",
    "axis",
    "true",
    "labels",
    "axis",
    "way",
    "read",
    "look",
    "see",
    "model",
    "predicted",
    "patient",
    "side",
    "effects",
    "10",
    "times",
    "patient",
    "actually",
    "side",
    "effect",
    "incorrect",
    "predictions",
    "flip",
    "side",
    "though",
    "model",
    "predicted",
    "patient",
    "side",
    "effects",
    "196",
    "times",
    "patient",
    "indeed",
    "side",
    "effects",
    "correct",
    "predictions",
    "actually",
    "generally",
    "reading",
    "confusion",
    "matrix",
    "looking",
    "top",
    "left",
    "bottom",
    "right",
    "diagonal",
    "squares",
    "blue",
    "going",
    "across",
    "diagonal",
    "correct",
    "predictions",
    "see",
    "total",
    "model",
    "predicted",
    "200",
    "plus",
    "396",
    "correct",
    "predictions",
    "total",
    "420",
    "think",
    "yes",
    "numbers",
    "added",
    "equal",
    "420",
    "396",
    "420",
    "predictions",
    "correct",
    "gives",
    "us",
    "94",
    "accuracy",
    "rate",
    "test",
    "set",
    "equivalent",
    "seeing",
    "validation",
    "accuracy",
    "rate",
    "training",
    "see",
    "confusion",
    "matrix",
    "great",
    "tool",
    "use",
    "able",
    "visualize",
    "well",
    "model",
    "edits",
    "predictions",
    "also",
    "able",
    "drill",
    "little",
    "bit",
    "see",
    "classes",
    "might",
    "need",
    "work",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "demonstrate",
    "multiple",
    "ways",
    "save",
    "load",
    "karass",
    "sequential",
    "model",
    "different",
    "options",
    "comes",
    "saving",
    "loading",
    "karass",
    "sequential",
    "model",
    "work",
    "little",
    "bit",
    "differently",
    "one",
    "another",
    "going",
    "go",
    "options",
    "working",
    "single",
    "model",
    "last",
    "episodes",
    "going",
    "continue",
    "working",
    "one",
    "printed",
    "summary",
    "model",
    "refresh",
    "memory",
    "one",
    "working",
    "make",
    "sure",
    "jupyter",
    "notebook",
    "model",
    "already",
    "created",
    "going",
    "show",
    "save",
    "model",
    "alright",
    "first",
    "way",
    "save",
    "model",
    "calling",
    "save",
    "function",
    "save",
    "pass",
    "path",
    "want",
    "save",
    "model",
    "model",
    "name",
    "file",
    "name",
    "want",
    "save",
    "model",
    "h",
    "five",
    "extension",
    "h5",
    "file",
    "going",
    "model",
    "stored",
    "code",
    "condition",
    "checking",
    "see",
    "model",
    "already",
    "saved",
    "disk",
    "first",
    "save",
    "want",
    "continue",
    "saving",
    "model",
    "machine",
    "already",
    "saved",
    "condition",
    "model",
    "dot",
    "save",
    "function",
    "first",
    "way",
    "save",
    "model",
    "save",
    "using",
    "way",
    "saves",
    "architecture",
    "model",
    "allowing",
    "us",
    "able",
    "recreate",
    "number",
    "learnable",
    "parameters",
    "layers",
    "nodes",
    "etc",
    "also",
    "saves",
    "weights",
    "model",
    "models",
    "already",
    "trained",
    "weights",
    "learned",
    "optimized",
    "going",
    "place",
    "within",
    "saved",
    "model",
    "disk",
    "also",
    "saves",
    "training",
    "configuration",
    "things",
    "like",
    "loss",
    "optimizer",
    "set",
    "whenever",
    "compile",
    "model",
    "state",
    "optimizer",
    "also",
    "saved",
    "allows",
    "us",
    "training",
    "model",
    "stop",
    "save",
    "model",
    "disk",
    "later",
    "load",
    "model",
    "pick",
    "training",
    "left",
    "state",
    "optimizer",
    "saved",
    "state",
    "comprehensive",
    "option",
    "comes",
    "saving",
    "model",
    "saves",
    "everything",
    "architecture",
    "learnable",
    "parameters",
    "state",
    "model",
    "left",
    "training",
    "want",
    "load",
    "model",
    "later",
    "previously",
    "saved",
    "disk",
    "need",
    "first",
    "import",
    "load",
    "model",
    "function",
    "tensorflow",
    "kerris",
    "dot",
    "models",
    "create",
    "variable",
    "case",
    "calling",
    "new",
    "model",
    "setting",
    "load",
    "model",
    "pointing",
    "saved",
    "model",
    "disk",
    "run",
    "look",
    "summary",
    "new",
    "model",
    "see",
    "indeed",
    "exact",
    "replica",
    "terms",
    "architecture",
    "original",
    "model",
    "previously",
    "saved",
    "disk",
    "also",
    "look",
    "weights",
    "new",
    "model",
    "look",
    "weights",
    "ahead",
    "time",
    "able",
    "compare",
    "directly",
    "showing",
    "inspect",
    "weights",
    "look",
    "comparatively",
    "see",
    "weights",
    "actually",
    "previous",
    "models",
    "weights",
    "taken",
    "look",
    "beforehand",
    "also",
    "look",
    "optimizer",
    "show",
    "although",
    "never",
    "set",
    "optimizer",
    "explicitly",
    "new",
    "model",
    "loading",
    "saved",
    "model",
    "indeed",
    "use",
    "atom",
    "optimizer",
    "set",
    "back",
    "whenever",
    "compiled",
    "model",
    "training",
    "alright",
    "first",
    "saving",
    "loading",
    "option",
    "comprehensive",
    "option",
    "save",
    "load",
    "everything",
    "particular",
    "model",
    "next",
    "option",
    "look",
    "using",
    "function",
    "called",
    "json",
    "call",
    "json",
    "need",
    "save",
    "architecture",
    "model",
    "want",
    "set",
    "weights",
    "training",
    "configuration",
    "save",
    "model",
    "architecture",
    "saving",
    "json",
    "string",
    "using",
    "example",
    "creating",
    "variable",
    "called",
    "json",
    "string",
    "setting",
    "equal",
    "json",
    "remember",
    "model",
    "original",
    "model",
    "working",
    "far",
    "point",
    "call",
    "json",
    "print",
    "json",
    "string",
    "see",
    "get",
    "string",
    "details",
    "model",
    "architecture",
    "sequential",
    "model",
    "got",
    "layers",
    "organized",
    "individual",
    "dense",
    "layers",
    "details",
    "specific",
    "layers",
    "original",
    "model",
    "later",
    "point",
    "want",
    "create",
    "new",
    "model",
    "older",
    "models",
    "architecture",
    "save",
    "json",
    "string",
    "import",
    "model",
    "json",
    "function",
    "tensorflow",
    "kerris",
    "dot",
    "models",
    "creating",
    "new",
    "variable",
    "called",
    "model",
    "architecture",
    "loading",
    "json",
    "string",
    "using",
    "model",
    "json",
    "function",
    "new",
    "model",
    "calling",
    "model",
    "architecture",
    "look",
    "summary",
    "see",
    "identical",
    "summary",
    "original",
    "model",
    "new",
    "model",
    "place",
    "architecture",
    "place",
    "would",
    "retrain",
    "update",
    "weights",
    "would",
    "need",
    "compile",
    "get",
    "optimizer",
    "loss",
    "everything",
    "like",
    "defined",
    "creates",
    "model",
    "architecture",
    "standpoint",
    "moving",
    "third",
    "option",
    "wanted",
    "mention",
    "brief",
    "point",
    "go",
    "exact",
    "process",
    "using",
    "yaml",
    "string",
    "instead",
    "json",
    "string",
    "function",
    "create",
    "gamble",
    "string",
    "gamble",
    "instead",
    "json",
    "function",
    "load",
    "yaml",
    "string",
    "model",
    "gamma",
    "model",
    "yaml",
    "instead",
    "model",
    "json",
    "alright",
    "next",
    "option",
    "save",
    "model",
    "actually",
    "save",
    "weights",
    "model",
    "need",
    "save",
    "weights",
    "need",
    "save",
    "architecture",
    "training",
    "configurations",
    "like",
    "optimizer",
    "loss",
    "save",
    "solely",
    "models",
    "weights",
    "using",
    "save",
    "weights",
    "function",
    "call",
    "model",
    "dot",
    "save",
    "weights",
    "looks",
    "exactly",
    "called",
    "model",
    "dot",
    "save",
    "passing",
    "path",
    "disk",
    "save",
    "model",
    "along",
    "file",
    "name",
    "ending",
    "h",
    "five",
    "extension",
    "calling",
    "model",
    "weights",
    "dot",
    "h",
    "five",
    "condition",
    "checking",
    "h",
    "five",
    "file",
    "already",
    "saved",
    "disk",
    "otherwise",
    "going",
    "keep",
    "saving",
    "thing",
    "save",
    "weights",
    "want",
    "load",
    "later",
    "time",
    "model",
    "already",
    "place",
    "save",
    "model",
    "save",
    "weights",
    "able",
    "bring",
    "weights",
    "new",
    "model",
    "would",
    "need",
    "create",
    "second",
    "model",
    "point",
    "architecture",
    "could",
    "load",
    "weights",
    "cell",
    "defining",
    "model",
    "called",
    "model",
    "two",
    "exact",
    "model",
    "architecture",
    "standpoint",
    "first",
    "model",
    "run",
    "point",
    "option",
    "load",
    "weights",
    "model",
    "shape",
    "weights",
    "going",
    "match",
    "shape",
    "model",
    "architecture",
    "essentially",
    "could",
    "model",
    "five",
    "layers",
    "find",
    "example",
    "load",
    "weights",
    "would",
    "direct",
    "mapping",
    "particular",
    "weights",
    "loaded",
    "exact",
    "architecture",
    "original",
    "model",
    "load",
    "weights",
    "call",
    "load",
    "weights",
    "point",
    "place",
    "disk",
    "weights",
    "saved",
    "call",
    "get",
    "weights",
    "new",
    "model",
    "see",
    "new",
    "model",
    "populated",
    "weights",
    "original",
    "model",
    "alright",
    "know",
    "ways",
    "say",
    "various",
    "aspects",
    "karass",
    "sequential",
    "model",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "people",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "may",
    "people",
    "episode",
    "go",
    "necessary",
    "image",
    "preparation",
    "processing",
    "steps",
    "needed",
    "train",
    "first",
    "convolutional",
    "neural",
    "network",
    "goal",
    "next",
    "episodes",
    "build",
    "train",
    "convolutional",
    "neural",
    "network",
    "classify",
    "images",
    "cats",
    "dogs",
    "first",
    "thing",
    "need",
    "get",
    "prepare",
    "data",
    "set",
    "training",
    "model",
    "going",
    "work",
    "data",
    "set",
    "kaggle",
    "cats",
    "versus",
    "dogs",
    "competition",
    "find",
    "link",
    "download",
    "dataset",
    "corresponding",
    "blog",
    "episode",
    "people",
    "mostly",
    "going",
    "organize",
    "data",
    "programmatically",
    "couple",
    "manual",
    "steps",
    "go",
    "first",
    "downloaded",
    "data",
    "set",
    "kaggle",
    "zip",
    "folder",
    "look",
    "inside",
    "contents",
    "zipped",
    "train",
    "folder",
    "zipped",
    "test",
    "folder",
    "along",
    "sample",
    "submission",
    "csv",
    "actually",
    "going",
    "working",
    "delete",
    "test",
    "well",
    "csv",
    "file",
    "going",
    "working",
    "train",
    "zip",
    "want",
    "extract",
    "high",
    "level",
    "cats",
    "dogs",
    "versus",
    "cats",
    "first",
    "extract",
    "train",
    "zip",
    "takes",
    "went",
    "ahead",
    "train",
    "directory",
    "moved",
    "test",
    "directory",
    "elsewhere",
    "deleted",
    "csv",
    "file",
    "extracted",
    "train",
    "folder",
    "go",
    "nested",
    "train",
    "folder",
    "see",
    "already",
    "train",
    "train",
    "images",
    "cats",
    "dogs",
    "okay",
    "way",
    "data",
    "come",
    "downloaded",
    "first",
    "step",
    "next",
    "step",
    "need",
    "come",
    "grab",
    "images",
    "going",
    "ctrl",
    "x",
    "cut",
    "bring",
    "first",
    "train",
    "directory",
    "want",
    "nested",
    "directory",
    "structure",
    "instead",
    "going",
    "place",
    "directly",
    "within",
    "first",
    "train",
    "directory",
    "alright",
    "images",
    "copied",
    "base",
    "train",
    "directory",
    "nested",
    "train",
    "directory",
    "images",
    "previously",
    "belong",
    "empty",
    "go",
    "ahead",
    "delete",
    "one",
    "alright",
    "directory",
    "structure",
    "dogs",
    "cat",
    "top",
    "directory",
    "within",
    "train",
    "directory",
    "images",
    "within",
    "train",
    "directory",
    "cats",
    "dogs",
    "last",
    "step",
    "move",
    "dogs",
    "versus",
    "cats",
    "directory",
    "data",
    "place",
    "disk",
    "going",
    "working",
    "relative",
    "jupyter",
    "notebook",
    "within",
    "directory",
    "called",
    "data",
    "placed",
    "dogs",
    "versus",
    "cats",
    "alright",
    "manual",
    "labor",
    "everything",
    "else",
    "organize",
    "data",
    "later",
    "process",
    "data",
    "programmatically",
    "code",
    "alright",
    "within",
    "jupyter",
    "notebook",
    "first",
    "things",
    "first",
    "need",
    "import",
    "packages",
    "making",
    "use",
    "packages",
    "specific",
    "episode",
    "processing",
    "image",
    "data",
    "actually",
    "packages",
    "making",
    "use",
    "next",
    "several",
    "episodes",
    "working",
    "cn",
    "ns",
    "alright",
    "get",
    "taken",
    "care",
    "cell",
    "making",
    "sure",
    "using",
    "gpu",
    "tensorflow",
    "able",
    "identify",
    "correctly",
    "enabling",
    "memory",
    "growth",
    "gpu",
    "well",
    "using",
    "gpu",
    "worries",
    "mentioned",
    "earlier",
    "completely",
    "fine",
    "follow",
    "course",
    "cpu",
    "alright",
    "going",
    "pick",
    "back",
    "organizing",
    "data",
    "disk",
    "assuming",
    "gone",
    "first",
    "steps",
    "beginning",
    "episode",
    "going",
    "organize",
    "data",
    "train",
    "valid",
    "test",
    "ders",
    "correspond",
    "training",
    "validation",
    "test",
    "sets",
    "group",
    "going",
    "first",
    "change",
    "directories",
    "dog",
    "cat",
    "directory",
    "going",
    "check",
    "make",
    "sure",
    "directory",
    "structure",
    "make",
    "already",
    "exist",
    "going",
    "proceed",
    "rest",
    "script",
    "first",
    "thing",
    "long",
    "directory",
    "structure",
    "already",
    "place",
    "making",
    "following",
    "directories",
    "already",
    "train",
    "directory",
    "going",
    "make",
    "nested",
    "dog",
    "cat",
    "directory",
    "within",
    "train",
    "additionally",
    "going",
    "make",
    "valid",
    "directory",
    "contains",
    "dog",
    "cat",
    "directories",
    "test",
    "directory",
    "also",
    "contains",
    "dog",
    "cat",
    "directories",
    "particular",
    "data",
    "set",
    "contains",
    "images",
    "dogs",
    "cats",
    "pretty",
    "much",
    "overkill",
    "tasks",
    "using",
    "images",
    "upcoming",
    "episodes",
    "actually",
    "going",
    "use",
    "small",
    "subset",
    "data",
    "free",
    "work",
    "data",
    "like",
    "would",
    "take",
    "lot",
    "longer",
    "train",
    "networks",
    "work",
    "data",
    "general",
    "using",
    "entire",
    "set",
    "going",
    "working",
    "subset",
    "consisting",
    "1000",
    "images",
    "training",
    "set",
    "200",
    "validation",
    "set",
    "100",
    "test",
    "set",
    "sets",
    "going",
    "split",
    "evenly",
    "among",
    "cats",
    "dogs",
    "exactly",
    "block",
    "code",
    "going",
    "images",
    "dogs",
    "vs",
    "cat",
    "directory",
    "moving",
    "500",
    "randomly",
    "moving",
    "500",
    "cat",
    "images",
    "train",
    "cat",
    "directory",
    "500",
    "dog",
    "images",
    "train",
    "dog",
    "directory",
    "similarly",
    "thing",
    "valid",
    "valid",
    "validation",
    "set",
    "cat",
    "dogs",
    "test",
    "set",
    "cat",
    "dogs",
    "quantities",
    "differing",
    "regarding",
    "amounts",
    "stated",
    "earlier",
    "sets",
    "able",
    "understand",
    "images",
    "cats",
    "dogs",
    "based",
    "names",
    "files",
    "saw",
    "earlier",
    "cat",
    "images",
    "actually",
    "word",
    "cat",
    "file",
    "names",
    "dog",
    "images",
    "word",
    "dog",
    "file",
    "names",
    "able",
    "select",
    "dog",
    "images",
    "cat",
    "images",
    "script",
    "alright",
    "script",
    "runs",
    "pull",
    "file",
    "explorer",
    "look",
    "directory",
    "structure",
    "make",
    "sure",
    "expect",
    "dogs",
    "vs",
    "cat",
    "directory",
    "within",
    "data",
    "directory",
    "enter",
    "test",
    "train",
    "valid",
    "directories",
    "inside",
    "test",
    "cat",
    "cat",
    "images",
    "inside",
    "dog",
    "dog",
    "images",
    "back",
    "go",
    "train",
    "see",
    "similarly",
    "go",
    "valid",
    "see",
    "similarly",
    "select",
    "one",
    "folders",
    "look",
    "properties",
    "see",
    "many",
    "files",
    "exist",
    "within",
    "directory",
    "make",
    "sure",
    "amount",
    "chose",
    "put",
    "script",
    "accidentally",
    "make",
    "type",
    "error",
    "go",
    "back",
    "dogs",
    "vs",
    "cats",
    "root",
    "directory",
    "see",
    "cat",
    "dog",
    "images",
    "leftover",
    "remaining",
    "left",
    "moved",
    "subset",
    "train",
    "valid",
    "test",
    "directories",
    "free",
    "make",
    "use",
    "way",
    "want",
    "delete",
    "move",
    "another",
    "location",
    "working",
    "three",
    "directories",
    "alright",
    "point",
    "obtained",
    "data",
    "organized",
    "data",
    "time",
    "move",
    "processing",
    "data",
    "scroll",
    "first",
    "creating",
    "variables",
    "assigned",
    "train",
    "valid",
    "test",
    "paths",
    "pointing",
    "location",
    "disk",
    "different",
    "data",
    "sets",
    "reside",
    "recall",
    "earlier",
    "course",
    "talked",
    "whenever",
    "train",
    "model",
    "need",
    "put",
    "data",
    "format",
    "model",
    "expects",
    "know",
    "train",
    "karass",
    "sequential",
    "model",
    "model",
    "receives",
    "data",
    "whenever",
    "call",
    "fit",
    "function",
    "going",
    "put",
    "images",
    "format",
    "karass",
    "generator",
    "cell",
    "creating",
    "train",
    "valid",
    "test",
    "batches",
    "setting",
    "equal",
    "image",
    "data",
    "generator",
    "dot",
    "flow",
    "directory",
    "going",
    "return",
    "directory",
    "iterator",
    "basically",
    "going",
    "create",
    "batches",
    "data",
    "directories",
    "datasets",
    "reside",
    "batches",
    "data",
    "able",
    "passed",
    "sequential",
    "model",
    "using",
    "fit",
    "function",
    "let",
    "look",
    "exactly",
    "defining",
    "variables",
    "let",
    "focus",
    "train",
    "batches",
    "setting",
    "train",
    "batches",
    "equal",
    "image",
    "data",
    "generator",
    "dot",
    "flow",
    "directory",
    "first",
    "image",
    "data",
    "generator",
    "specifying",
    "pre",
    "processing",
    "function",
    "setting",
    "equal",
    "tf",
    "kerris",
    "dot",
    "applications",
    "vgg",
    "16",
    "dot",
    "pre",
    "process",
    "input",
    "going",
    "tell",
    "function",
    "going",
    "apply",
    "type",
    "pre",
    "processing",
    "images",
    "get",
    "passed",
    "network",
    "using",
    "processing",
    "way",
    "equivalent",
    "way",
    "popular",
    "model",
    "known",
    "vgg",
    "16",
    "processing",
    "images",
    "format",
    "images",
    "get",
    "passed",
    "vgg",
    "16",
    "model",
    "process",
    "going",
    "talk",
    "future",
    "episode",
    "let",
    "confuse",
    "know",
    "causing",
    "type",
    "processing",
    "occur",
    "images",
    "talk",
    "future",
    "episode",
    "stress",
    "necessarily",
    "important",
    "us",
    "right",
    "moment",
    "technical",
    "details",
    "least",
    "besides",
    "call",
    "flow",
    "directory",
    "passing",
    "actual",
    "data",
    "specifying",
    "want",
    "data",
    "processed",
    "setting",
    "directory",
    "equal",
    "train",
    "path",
    "appear",
    "defined",
    "location",
    "disk",
    "training",
    "set",
    "train",
    "path",
    "variable",
    "setting",
    "target",
    "size",
    "equal",
    "224",
    "height",
    "width",
    "want",
    "cat",
    "dog",
    "images",
    "resized",
    "working",
    "image",
    "data",
    "set",
    "images",
    "varying",
    "sizes",
    "want",
    "scale",
    "scale",
    "specify",
    "happen",
    "resize",
    "images",
    "data",
    "set",
    "height",
    "width",
    "passing",
    "network",
    "specifying",
    "classes",
    "classes",
    "potential",
    "labels",
    "data",
    "set",
    "cat",
    "dog",
    "setting",
    "batch",
    "size",
    "exact",
    "thing",
    "validation",
    "set",
    "test",
    "set",
    "everything",
    "exact",
    "except",
    "sets",
    "live",
    "disk",
    "specified",
    "directory",
    "parameter",
    "difference",
    "test",
    "batches",
    "specifying",
    "shuffle",
    "equals",
    "false",
    "parameter",
    "whenever",
    "use",
    "test",
    "batches",
    "later",
    "inference",
    "get",
    "model",
    "predict",
    "images",
    "cats",
    "dogs",
    "training",
    "validation",
    "completed",
    "going",
    "want",
    "look",
    "prediction",
    "results",
    "confusion",
    "matrix",
    "like",
    "previous",
    "video",
    "separate",
    "data",
    "set",
    "order",
    "need",
    "able",
    "access",
    "unsettled",
    "labels",
    "test",
    "set",
    "set",
    "shuffle",
    "equals",
    "false",
    "set",
    "validation",
    "training",
    "sets",
    "want",
    "data",
    "sets",
    "shuffled",
    "alright",
    "run",
    "get",
    "output",
    "found",
    "1000",
    "images",
    "belonging",
    "two",
    "classes",
    "corresponding",
    "train",
    "batches",
    "found",
    "200",
    "images",
    "belonging",
    "two",
    "classes",
    "corresponds",
    "valid",
    "valid",
    "batches",
    "100",
    "belonging",
    "two",
    "classes",
    "corresponding",
    "test",
    "batches",
    "output",
    "want",
    "see",
    "letting",
    "know",
    "found",
    "images",
    "disk",
    "belong",
    "cat",
    "dog",
    "classes",
    "specified",
    "getting",
    "point",
    "get",
    "found",
    "zero",
    "images",
    "perhaps",
    "pointing",
    "wrong",
    "place",
    "disk",
    "need",
    "make",
    "sure",
    "able",
    "find",
    "images",
    "set",
    "previously",
    "right",
    "verifying",
    "indeed",
    "case",
    "next",
    "going",
    "grab",
    "single",
    "batch",
    "images",
    "corresponding",
    "labels",
    "train",
    "batches",
    "remember",
    "batch",
    "size",
    "10",
    "images",
    "along",
    "10",
    "corresponding",
    "labels",
    "next",
    "introducing",
    "function",
    "plot",
    "images",
    "going",
    "use",
    "plot",
    "images",
    "train",
    "batches",
    "obtained",
    "function",
    "directly",
    "tensor",
    "flows",
    "website",
    "check",
    "link",
    "corresponding",
    "blog",
    "episode",
    "people",
    "see",
    "able",
    "get",
    "tensor",
    "flows",
    "site",
    "exactly",
    "pulled",
    "define",
    "function",
    "alright",
    "going",
    "use",
    "function",
    "plot",
    "images",
    "test",
    "batches",
    "going",
    "print",
    "corresponding",
    "labels",
    "images",
    "scroll",
    "see",
    "batch",
    "training",
    "data",
    "looks",
    "like",
    "might",
    "little",
    "bit",
    "different",
    "expected",
    "given",
    "fact",
    "looks",
    "like",
    "color",
    "data",
    "little",
    "bit",
    "distorted",
    "due",
    "pre",
    "processing",
    "function",
    "called",
    "pre",
    "process",
    "images",
    "way",
    "type",
    "way",
    "images",
    "get",
    "pre",
    "processed",
    "famous",
    "vgg",
    "16",
    "model",
    "like",
    "said",
    "going",
    "discuss",
    "detail",
    "exactly",
    "pre",
    "processing",
    "function",
    "technically",
    "well",
    "using",
    "later",
    "video",
    "know",
    "skewing",
    "rgb",
    "data",
    "way",
    "still",
    "make",
    "fact",
    "cat",
    "looks",
    "like",
    "cat",
    "dog",
    "dog",
    "dog",
    "dog",
    "cat",
    "yeah",
    "still",
    "kind",
    "generally",
    "make",
    "images",
    "color",
    "data",
    "skewed",
    "worry",
    "much",
    "technical",
    "details",
    "behind",
    "right",
    "know",
    "data",
    "looks",
    "like",
    "pass",
    "model",
    "corresponding",
    "labels",
    "data",
    "one",
    "hot",
    "encoded",
    "vectors",
    "represent",
    "either",
    "cat",
    "dog",
    "one",
    "zero",
    "represents",
    "cat",
    "zero",
    "one",
    "represents",
    "dog",
    "okay",
    "guess",
    "wrong",
    "earlier",
    "thinking",
    "one",
    "dog",
    "one",
    "cat",
    "see",
    "maps",
    "101",
    "hot",
    "encoding",
    "know",
    "mean",
    "one",
    "hot",
    "encoding",
    "check",
    "corresponding",
    "video",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "depot",
    "yeah",
    "see",
    "01",
    "vector",
    "used",
    "represent",
    "label",
    "dog",
    "one",
    "dog",
    "next",
    "two",
    "dogs",
    "well",
    "one",
    "one",
    "quick",
    "note",
    "everything",
    "discussed",
    "point",
    "sometimes",
    "corresponding",
    "labels",
    "test",
    "set",
    "examples",
    "done",
    "far",
    "course",
    "always",
    "corresponding",
    "labels",
    "test",
    "set",
    "practice",
    "lot",
    "times",
    "may",
    "labels",
    "fact",
    "used",
    "downloaded",
    "test",
    "directory",
    "came",
    "kaggle",
    "download",
    "would",
    "see",
    "test",
    "directory",
    "images",
    "labeled",
    "cat",
    "dog",
    "case",
    "test",
    "labels",
    "cat",
    "dog",
    "images",
    "since",
    "pulled",
    "original",
    "training",
    "set",
    "kaggle",
    "corresponding",
    "labels",
    "access",
    "test",
    "labels",
    "wondering",
    "process",
    "test",
    "data",
    "accordingly",
    "check",
    "blog",
    "episode",
    "dibbles",
    "comm",
    "section",
    "demonstrates",
    "need",
    "differently",
    "showed",
    "video",
    "access",
    "labels",
    "test",
    "set",
    "alright",
    "obtained",
    "image",
    "data",
    "organized",
    "disk",
    "processed",
    "accordingly",
    "convolutional",
    "neural",
    "network",
    "next",
    "episode",
    "going",
    "get",
    "set",
    "start",
    "building",
    "training",
    "first",
    "cnn",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "demonstrate",
    "build",
    "convolutional",
    "neural",
    "network",
    "train",
    "images",
    "cats",
    "dogs",
    "using",
    "tensor",
    "flows",
    "integrated",
    "kerris",
    "api",
    "continuing",
    "work",
    "cat",
    "dog",
    "image",
    "data",
    "created",
    "last",
    "episode",
    "make",
    "sure",
    "still",
    "place",
    "well",
    "imports",
    "brought",
    "last",
    "episode",
    "well",
    "making",
    "use",
    "imports",
    "next",
    "several",
    "videos",
    "working",
    "cnn",
    "create",
    "first",
    "cnn",
    "making",
    "use",
    "kerris",
    "sequential",
    "model",
    "recall",
    "introduced",
    "model",
    "earlier",
    "episode",
    "working",
    "plain",
    "simple",
    "numerical",
    "data",
    "continue",
    "work",
    "model",
    "first",
    "cnn",
    "first",
    "layer",
    "model",
    "pass",
    "comma",
    "layer",
    "standard",
    "convolutional",
    "layer",
    "accept",
    "image",
    "data",
    "layer",
    "arbitrarily",
    "setting",
    "filter",
    "value",
    "equal",
    "first",
    "convolutional",
    "layer",
    "32",
    "filters",
    "kernel",
    "size",
    "three",
    "three",
    "choice",
    "32",
    "pretty",
    "arbitrary",
    "kernel",
    "size",
    "three",
    "three",
    "common",
    "choice",
    "image",
    "data",
    "first",
    "column",
    "layer",
    "followed",
    "popular",
    "relu",
    "activation",
    "function",
    "specifying",
    "padding",
    "equals",
    "means",
    "images",
    "zero",
    "padding",
    "padding",
    "outside",
    "dimensionality",
    "images",
    "reduced",
    "convolution",
    "operations",
    "lastly",
    "first",
    "layer",
    "specify",
    "input",
    "shape",
    "data",
    "recall",
    "touched",
    "parameter",
    "previously",
    "think",
    "kind",
    "creating",
    "implicit",
    "input",
    "layer",
    "model",
    "comp",
    "2d",
    "actually",
    "first",
    "hidden",
    "layer",
    "input",
    "layer",
    "made",
    "input",
    "data",
    "need",
    "tell",
    "model",
    "shape",
    "input",
    "data",
    "case",
    "going",
    "recall",
    "saw",
    "setting",
    "target",
    "size",
    "parameter",
    "24",
    "whenever",
    "created",
    "valid",
    "test",
    "train",
    "directory",
    "iterators",
    "said",
    "wanted",
    "images",
    "height",
    "width",
    "three",
    "regarding",
    "color",
    "channels",
    "since",
    "since",
    "images",
    "rgb",
    "format",
    "three",
    "color",
    "channels",
    "specified",
    "input",
    "shape",
    "follow",
    "first",
    "convolutional",
    "layer",
    "max",
    "pooling",
    "layer",
    "setting",
    "pool",
    "size",
    "two",
    "two",
    "strides",
    "two",
    "familiar",
    "max",
    "pooling",
    "know",
    "going",
    "cut",
    "image",
    "dimensions",
    "half",
    "need",
    "know",
    "max",
    "pooling",
    "need",
    "refresher",
    "thing",
    "padding",
    "zero",
    "padding",
    "activation",
    "functions",
    "anything",
    "like",
    "sure",
    "check",
    "episodes",
    "corresponding",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "depot",
    "max",
    "pooling",
    "layer",
    "adding",
    "another",
    "convolutional",
    "layer",
    "looks",
    "pretty",
    "much",
    "exactly",
    "first",
    "one",
    "except",
    "four",
    "including",
    "input",
    "shape",
    "parameter",
    "since",
    "specify",
    "first",
    "hidden",
    "layer",
    "specifying",
    "filters",
    "64",
    "instead",
    "64",
    "arbitrary",
    "choice",
    "chose",
    "number",
    "general",
    "rule",
    "increasing",
    "functions",
    "go",
    "later",
    "layers",
    "network",
    "common",
    "practice",
    "follow",
    "second",
    "convolutional",
    "layer",
    "another",
    "max",
    "pooling",
    "layer",
    "identical",
    "first",
    "one",
    "flatten",
    "one",
    "dimensional",
    "tensor",
    "passing",
    "dense",
    "output",
    "layer",
    "two",
    "nodes",
    "corresponding",
    "cat",
    "dog",
    "output",
    "layer",
    "followed",
    "softmax",
    "activation",
    "function",
    "know",
    "going",
    "give",
    "us",
    "probabilities",
    "corresponding",
    "output",
    "model",
    "alright",
    "run",
    "check",
    "summary",
    "model",
    "exactly",
    "built",
    "along",
    "additional",
    "details",
    "learnable",
    "parameters",
    "output",
    "shape",
    "network",
    "things",
    "also",
    "covered",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "well",
    "want",
    "check",
    "information",
    "learnable",
    "parameters",
    "model",
    "built",
    "prepare",
    "training",
    "calling",
    "model",
    "dot",
    "compile",
    "already",
    "used",
    "previous",
    "episode",
    "training",
    "numerical",
    "data",
    "see",
    "looks",
    "pretty",
    "much",
    "setting",
    "optimizer",
    "equal",
    "atom",
    "optimizer",
    "learning",
    "rate",
    "using",
    "categorical",
    "cross",
    "entropy",
    "looking",
    "accuracy",
    "metrics",
    "able",
    "judge",
    "model",
    "performance",
    "quick",
    "note",
    "moving",
    "using",
    "categorical",
    "cross",
    "entropy",
    "since",
    "two",
    "outputs",
    "model",
    "either",
    "cat",
    "dog",
    "possible",
    "instead",
    "use",
    "binary",
    "cross",
    "entropy",
    "would",
    "need",
    "one",
    "single",
    "output",
    "node",
    "model",
    "instead",
    "two",
    "rather",
    "following",
    "output",
    "layer",
    "softmax",
    "activation",
    "function",
    "would",
    "need",
    "follow",
    "sigmoid",
    "approaches",
    "using",
    "categorical",
    "cross",
    "entropy",
    "loss",
    "setup",
    "using",
    "binary",
    "cross",
    "entropy",
    "loss",
    "setup",
    "described",
    "work",
    "equally",
    "well",
    "totally",
    "equivalent",
    "yield",
    "results",
    "categorical",
    "cross",
    "entropy",
    "using",
    "softmax",
    "app",
    "activation",
    "function",
    "output",
    "layer",
    "common",
    "approach",
    "two",
    "classes",
    "like",
    "continue",
    "using",
    "approach",
    "even",
    "two",
    "classes",
    "general",
    "case",
    "going",
    "use",
    "whenever",
    "two",
    "classes",
    "anyway",
    "like",
    "stick",
    "using",
    "type",
    "setup",
    "even",
    "two",
    "outputs",
    "alright",
    "compiling",
    "model",
    "train",
    "using",
    "model",
    "dot",
    "fit",
    "familiar",
    "point",
    "fit",
    "function",
    "first",
    "specifying",
    "training",
    "data",
    "stored",
    "train",
    "batches",
    "specify",
    "validation",
    "data",
    "stored",
    "valid",
    "batches",
    "recall",
    "different",
    "way",
    "creating",
    "validation",
    "data",
    "spoke",
    "earlier",
    "episode",
    "using",
    "validation",
    "split",
    "actually",
    "created",
    "validation",
    "set",
    "separately",
    "fitting",
    "model",
    "specifying",
    "separated",
    "set",
    "validation",
    "data",
    "parameter",
    "setting",
    "epochs",
    "equal",
    "going",
    "train",
    "10",
    "runs",
    "time",
    "setting",
    "verbose",
    "verbose",
    "equal",
    "two",
    "see",
    "verbose",
    "output",
    "training",
    "one",
    "thing",
    "mention",
    "see",
    "specifying",
    "x",
    "past",
    "specifying",
    "target",
    "data",
    "usually",
    "data",
    "stored",
    "generator",
    "generator",
    "actually",
    "contains",
    "corresponding",
    "labels",
    "need",
    "specify",
    "separately",
    "whenever",
    "call",
    "fit",
    "actually",
    "contained",
    "within",
    "generator",
    "let",
    "go",
    "ahead",
    "run",
    "alright",
    "model",
    "finished",
    "training",
    "let",
    "check",
    "results",
    "get",
    "warning",
    "appears",
    "research",
    "done",
    "bug",
    "within",
    "tensorflow",
    "supposed",
    "fixed",
    "next",
    "release",
    "actually",
    "read",
    "safely",
    "ignore",
    "warning",
    "impact",
    "training",
    "scroll",
    "look",
    "results",
    "see",
    "10th",
    "epoch",
    "accuracy",
    "training",
    "set",
    "reached",
    "100",
    "great",
    "validation",
    "accuracy",
    "69",
    "great",
    "definitely",
    "see",
    "overfitting",
    "going",
    "model",
    "really",
    "cared",
    "really",
    "wanted",
    "use",
    "able",
    "deploy",
    "production",
    "scenario",
    "need",
    "stop",
    "combat",
    "overfitting",
    "problem",
    "going",
    "going",
    "seeing",
    "upcoming",
    "episode",
    "use",
    "pre",
    "trained",
    "model",
    "perform",
    "really",
    "well",
    "data",
    "get",
    "us",
    "exposed",
    "concept",
    "fine",
    "tuning",
    "though",
    "going",
    "see",
    "next",
    "episode",
    "model",
    "holds",
    "inference",
    "predicting",
    "images",
    "test",
    "set",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "may",
    "deep",
    "lizard",
    "episode",
    "demonstrate",
    "use",
    "convolutional",
    "neural",
    "network",
    "inference",
    "predict",
    "image",
    "data",
    "using",
    "tensor",
    "flows",
    "integrated",
    "keras",
    "api",
    "last",
    "time",
    "built",
    "trained",
    "first",
    "cnn",
    "cat",
    "dog",
    "image",
    "data",
    "saw",
    "training",
    "results",
    "great",
    "model",
    "achieving",
    "100",
    "accuracy",
    "training",
    "set",
    "however",
    "lagged",
    "behind",
    "quite",
    "good",
    "bit",
    "70",
    "accuracy",
    "validation",
    "set",
    "tells",
    "us",
    "model",
    "generalizing",
    "well",
    "hoped",
    "nonetheless",
    "going",
    "use",
    "model",
    "inference",
    "predict",
    "cat",
    "dog",
    "images",
    "test",
    "set",
    "given",
    "less",
    "decent",
    "results",
    "saw",
    "validation",
    "performance",
    "expectation",
    "model",
    "going",
    "well",
    "test",
    "set",
    "either",
    "probably",
    "going",
    "perform",
    "around",
    "70",
    "rate",
    "still",
    "going",
    "give",
    "us",
    "exposure",
    "use",
    "cnn",
    "inference",
    "using",
    "kerris",
    "sequential",
    "api",
    "alright",
    "back",
    "jupyter",
    "notebook",
    "need",
    "make",
    "sure",
    "code",
    "place",
    "last",
    "couple",
    "episodes",
    "continuing",
    "make",
    "use",
    "model",
    "built",
    "last",
    "time",
    "well",
    "test",
    "data",
    "whenever",
    "prepared",
    "data",
    "sets",
    "first",
    "thing",
    "going",
    "get",
    "batch",
    "test",
    "data",
    "test",
    "batches",
    "going",
    "plot",
    "batch",
    "going",
    "plot",
    "images",
    "specifically",
    "going",
    "print",
    "corresponding",
    "labels",
    "images",
    "using",
    "reminder",
    "plot",
    "images",
    "function",
    "introduced",
    "last",
    "couple",
    "episodes",
    "alright",
    "scroll",
    "test",
    "batches",
    "recall",
    "discussion",
    "image",
    "data",
    "looks",
    "way",
    "terms",
    "color",
    "skewed",
    "last",
    "time",
    "see",
    "looking",
    "even",
    "though",
    "kind",
    "distorted",
    "color",
    "actually",
    "cats",
    "looking",
    "corresponding",
    "label",
    "images",
    "see",
    "labeled",
    "one",
    "hot",
    "encoded",
    "vector",
    "one",
    "zero",
    "know",
    "label",
    "cat",
    "wondering",
    "cats",
    "first",
    "10",
    "images",
    "first",
    "batch",
    "call",
    "whenever",
    "created",
    "test",
    "set",
    "specified",
    "want",
    "shuffled",
    "could",
    "following",
    "come",
    "next",
    "cell",
    "run",
    "test",
    "batches",
    "classes",
    "get",
    "array",
    "corresponding",
    "labels",
    "image",
    "test",
    "set",
    "given",
    "access",
    "ns",
    "shuffled",
    "labels",
    "test",
    "set",
    "want",
    "shuffle",
    "test",
    "set",
    "directly",
    "want",
    "able",
    "one",
    "one",
    "direct",
    "mapping",
    "uncoupled",
    "labels",
    "test",
    "data",
    "set",
    "shuffle",
    "test",
    "data",
    "set",
    "every",
    "time",
    "generated",
    "batch",
    "would",
    "able",
    "correct",
    "mapping",
    "labels",
    "samples",
    "care",
    "correct",
    "mapping",
    "later",
    "get",
    "predictions",
    "model",
    "going",
    "want",
    "plot",
    "predictions",
    "confusion",
    "matrix",
    "want",
    "corresponding",
    "labels",
    "belong",
    "samples",
    "test",
    "set",
    "alright",
    "next",
    "actually",
    "going",
    "go",
    "ahead",
    "obtain",
    "predictions",
    "calling",
    "model",
    "dot",
    "predict",
    "earlier",
    "episodes",
    "data",
    "sets",
    "x",
    "specifying",
    "test",
    "batches",
    "test",
    "data",
    "set",
    "choosing",
    "verbose",
    "zero",
    "get",
    "output",
    "whenever",
    "run",
    "predictions",
    "printing",
    "arounded",
    "predictions",
    "model",
    "way",
    "read",
    "first",
    "one",
    "arrays",
    "prediction",
    "single",
    "sample",
    "look",
    "first",
    "one",
    "prediction",
    "first",
    "sample",
    "test",
    "set",
    "one",
    "wherever",
    "one",
    "prediction",
    "one",
    "index",
    "output",
    "class",
    "highest",
    "probability",
    "model",
    "case",
    "see",
    "zeroeth",
    "index",
    "highest",
    "probability",
    "say",
    "label",
    "model",
    "predicted",
    "first",
    "sample",
    "zero",
    "see",
    "one",
    "zeroeth",
    "index",
    "look",
    "first",
    "element",
    "look",
    "first",
    "label",
    "first",
    "test",
    "sample",
    "indeed",
    "zero",
    "eyeball",
    "see",
    "model",
    "accurately",
    "predict",
    "way",
    "first",
    "123456",
    "okay",
    "whenever",
    "see",
    "model",
    "predicted",
    "first",
    "index",
    "highest",
    "probability",
    "means",
    "model",
    "predicted",
    "output",
    "label",
    "one",
    "corresponds",
    "dog",
    "hard",
    "us",
    "kind",
    "draw",
    "overall",
    "conclusion",
    "prediction",
    "accuracy",
    "test",
    "set",
    "eyeballing",
    "results",
    "like",
    "scroll",
    "know",
    "tool",
    "confusion",
    "matrix",
    "use",
    "make",
    "visualizing",
    "results",
    "much",
    "easier",
    "like",
    "seen",
    "previous",
    "episodes",
    "course",
    "already",
    "going",
    "going",
    "create",
    "confusion",
    "matrix",
    "using",
    "confusion",
    "matrix",
    "function",
    "scikit",
    "learn",
    "already",
    "introduced",
    "passing",
    "true",
    "labels",
    "using",
    "test",
    "batches",
    "classes",
    "recall",
    "touched",
    "minute",
    "ago",
    "predictive",
    "labels",
    "passing",
    "predictions",
    "model",
    "getting",
    "actually",
    "passing",
    "index",
    "actually",
    "using",
    "arg",
    "max",
    "pass",
    "index",
    "probable",
    "prediction",
    "predictions",
    "list",
    "something",
    "already",
    "covered",
    "previous",
    "episodes",
    "run",
    "going",
    "bring",
    "plot",
    "confusion",
    "matrix",
    "discussed",
    "directly",
    "psychic",
    "lauren",
    "website",
    "link",
    "corresponding",
    "blog",
    "episode",
    "people",
    "calm",
    "going",
    "allow",
    "us",
    "plot",
    "confusion",
    "matrix",
    "moment",
    "look",
    "class",
    "indices",
    "see",
    "cat",
    "first",
    "dog",
    "second",
    "need",
    "look",
    "understand",
    "order",
    "put",
    "plot",
    "labels",
    "confusion",
    "matrix",
    "next",
    "call",
    "plot",
    "confusion",
    "matrix",
    "pass",
    "confusion",
    "matrix",
    "well",
    "labels",
    "confusion",
    "matrix",
    "title",
    "entire",
    "matrix",
    "let",
    "check",
    "alright",
    "learned",
    "easily",
    "interpret",
    "confusion",
    "matrix",
    "know",
    "look",
    "diagonal",
    "running",
    "top",
    "left",
    "bottom",
    "right",
    "see",
    "model",
    "predicted",
    "correctly",
    "great",
    "model",
    "definitely",
    "overfitting",
    "point",
    "like",
    "said",
    "model",
    "really",
    "concerned",
    "would",
    "definitely",
    "want",
    "combat",
    "overfitting",
    "problem",
    "going",
    "move",
    "new",
    "model",
    "using",
    "pre",
    "trained",
    "state",
    "art",
    "model",
    "called",
    "vgg",
    "next",
    "episode",
    "see",
    "well",
    "model",
    "classifying",
    "images",
    "cats",
    "dogs",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "blizzard",
    "episode",
    "demonstrate",
    "fine",
    "tune",
    "pre",
    "train",
    "model",
    "classify",
    "images",
    "using",
    "tensor",
    "flows",
    "keras",
    "api",
    "pre",
    "train",
    "model",
    "working",
    "called",
    "vgg",
    "model",
    "2014",
    "image",
    "net",
    "competition",
    "image",
    "net",
    "competition",
    "multiple",
    "teams",
    "compete",
    "build",
    "model",
    "best",
    "classifies",
    "images",
    "within",
    "image",
    "net",
    "library",
    "image",
    "net",
    "library",
    "made",
    "1000s",
    "images",
    "belong",
    "1000",
    "different",
    "classes",
    "using",
    "kerris",
    "import",
    "vgg",
    "16",
    "model",
    "fine",
    "tune",
    "classify",
    "one",
    "1000",
    "categories",
    "originally",
    "trained",
    "instead",
    "two",
    "categories",
    "cat",
    "dog",
    "note",
    "however",
    "cats",
    "dogs",
    "included",
    "original",
    "image",
    "net",
    "library",
    "vgg",
    "16",
    "trained",
    "wo",
    "much",
    "tuning",
    "change",
    "model",
    "classifying",
    "1000",
    "classes",
    "two",
    "cat",
    "dog",
    "classes",
    "overall",
    "fine",
    "tuning",
    "minimal",
    "later",
    "episodes",
    "though",
    "involved",
    "fine",
    "tuning",
    "use",
    "transfer",
    "learning",
    "transfer",
    "model",
    "learned",
    "original",
    "data",
    "set",
    "completely",
    "new",
    "data",
    "new",
    "custom",
    "data",
    "set",
    "using",
    "later",
    "understand",
    "fine",
    "tuning",
    "transfer",
    "learning",
    "fundamental",
    "level",
    "check",
    "corresponding",
    "episode",
    "fine",
    "tuning",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "v",
    "actually",
    "start",
    "building",
    "model",
    "let",
    "quickly",
    "talk",
    "vgg",
    "16",
    "pre",
    "processing",
    "done",
    "recall",
    "jupyter",
    "notebook",
    "last",
    "time",
    "plotted",
    "batch",
    "test",
    "data",
    "set",
    "episodes",
    "ago",
    "discussed",
    "fact",
    "color",
    "data",
    "skewed",
    "result",
    "vgg",
    "16",
    "pre",
    "processing",
    "function",
    "calling",
    "well",
    "going",
    "discuss",
    "exactly",
    "pre",
    "processing",
    "see",
    "still",
    "make",
    "images",
    "cats",
    "data",
    "color",
    "data",
    "appears",
    "distorted",
    "look",
    "paper",
    "authors",
    "vgg",
    "16",
    "detailed",
    "model",
    "architecture",
    "section",
    "let",
    "blow",
    "bit",
    "see",
    "state",
    "pre",
    "processing",
    "subtract",
    "mean",
    "rgb",
    "value",
    "computing",
    "trait",
    "computed",
    "training",
    "set",
    "pixel",
    "mean",
    "means",
    "computed",
    "mean",
    "red",
    "value",
    "pixel",
    "training",
    "data",
    "mean",
    "value",
    "across",
    "image",
    "training",
    "set",
    "subtracted",
    "mean",
    "value",
    "red",
    "value",
    "red",
    "pixel",
    "pixel",
    "image",
    "thing",
    "green",
    "blue",
    "pixel",
    "values",
    "well",
    "found",
    "green",
    "picks",
    "mean",
    "green",
    "pixel",
    "value",
    "among",
    "entire",
    "training",
    "set",
    "sample",
    "training",
    "set",
    "every",
    "green",
    "pixel",
    "subtracted",
    "green",
    "value",
    "thing",
    "blue",
    "course",
    "pre",
    "processing",
    "given",
    "vgg",
    "16",
    "originally",
    "trained",
    "means",
    "whenever",
    "new",
    "data",
    "passed",
    "model",
    "needs",
    "processed",
    "exact",
    "way",
    "original",
    "training",
    "set",
    "kerris",
    "already",
    "functions",
    "built",
    "popular",
    "models",
    "like",
    "vgg",
    "16",
    "pre",
    "processing",
    "place",
    "matches",
    "corresponding",
    "model",
    "calling",
    "model",
    "whenever",
    "process",
    "cat",
    "dog",
    "images",
    "earlier",
    "could",
    "go",
    "ahead",
    "get",
    "images",
    "processed",
    "way",
    "matched",
    "original",
    "training",
    "set",
    "processed",
    "vgg",
    "originally",
    "trained",
    "alright",
    "color",
    "distortion",
    "let",
    "jump",
    "back",
    "code",
    "know",
    "understanding",
    "pre",
    "processing",
    "let",
    "get",
    "back",
    "actually",
    "building",
    "fine",
    "tuned",
    "model",
    "first",
    "thing",
    "need",
    "download",
    "model",
    "call",
    "first",
    "time",
    "need",
    "internet",
    "connection",
    "going",
    "downloading",
    "model",
    "internet",
    "subsequent",
    "calls",
    "function",
    "grabbing",
    "model",
    "download",
    "copy",
    "machine",
    "alright",
    "model",
    "downloaded",
    "running",
    "summary",
    "see",
    "model",
    "much",
    "complex",
    "worked",
    "point",
    "total",
    "almost",
    "140",
    "million",
    "parameters",
    "model",
    "disk",
    "500",
    "megabytes",
    "quite",
    "large",
    "recall",
    "said",
    "vgg",
    "16",
    "model",
    "originally",
    "predicting",
    "1000",
    "different",
    "image",
    "net",
    "classes",
    "see",
    "output",
    "layer",
    "vgg",
    "16",
    "model",
    "1000",
    "different",
    "outputs",
    "objective",
    "going",
    "simply",
    "change",
    "last",
    "output",
    "layer",
    "predict",
    "two",
    "output",
    "classes",
    "corresponding",
    "cat",
    "dog",
    "along",
    "couple",
    "details",
    "regarding",
    "fine",
    "tuning",
    "process",
    "get",
    "moment",
    "actually",
    "going",
    "skip",
    "two",
    "cells",
    "able",
    "make",
    "sure",
    "imported",
    "model",
    "correctly",
    "relevant",
    "code",
    "checking",
    "trainable",
    "parameters",
    "non",
    "trainable",
    "parameters",
    "expect",
    "importing",
    "concern",
    "moment",
    "us",
    "scroll",
    "going",
    "build",
    "new",
    "model",
    "called",
    "sequential",
    "alright",
    "run",
    "code",
    "actually",
    "going",
    "look",
    "type",
    "model",
    "returning",
    "model",
    "called",
    "model",
    "actually",
    "model",
    "kerris",
    "functional",
    "api",
    "previously",
    "working",
    "sequential",
    "models",
    "later",
    "episode",
    "going",
    "discuss",
    "functional",
    "api",
    "depth",
    "bit",
    "complicated",
    "sophisticated",
    "sequential",
    "model",
    "since",
    "ready",
    "bring",
    "functional",
    "model",
    "yet",
    "going",
    "convert",
    "original",
    "vgg",
    "16",
    "model",
    "sequential",
    "model",
    "going",
    "creating",
    "new",
    "variable",
    "called",
    "model",
    "setting",
    "equal",
    "instance",
    "sequential",
    "object",
    "going",
    "loop",
    "every",
    "layer",
    "vgg",
    "except",
    "last",
    "output",
    "layer",
    "leaving",
    "going",
    "loop",
    "every",
    "layer",
    "add",
    "layer",
    "new",
    "sequential",
    "model",
    "look",
    "summary",
    "new",
    "model",
    "looking",
    "summary",
    "take",
    "time",
    "compare",
    "previous",
    "summary",
    "summary",
    "notice",
    "exactly",
    "except",
    "last",
    "layer",
    "included",
    "new",
    "model",
    "layer",
    "fully",
    "connected",
    "two",
    "layer",
    "output",
    "shape",
    "scroll",
    "back",
    "see",
    "layer",
    "predictions",
    "layer",
    "included",
    "iterating",
    "loop",
    "went",
    "way",
    "second",
    "last",
    "layer",
    "include",
    "last",
    "layer",
    "vgg",
    "alright",
    "let",
    "scroll",
    "back",
    "going",
    "iterate",
    "layers",
    "within",
    "new",
    "sequential",
    "model",
    "going",
    "set",
    "layers",
    "trainable",
    "setting",
    "layer",
    "dot",
    "trainable",
    "false",
    "going",
    "going",
    "freeze",
    "trainable",
    "parameters",
    "weights",
    "biases",
    "layers",
    "model",
    "going",
    "retrained",
    "whenever",
    "go",
    "training",
    "process",
    "cats",
    "dogs",
    "vgg",
    "16",
    "already",
    "learned",
    "features",
    "cats",
    "dogs",
    "original",
    "training",
    "want",
    "go",
    "training",
    "since",
    "already",
    "learned",
    "features",
    "freezing",
    "weights",
    "going",
    "add",
    "output",
    "layer",
    "model",
    "remember",
    "removed",
    "previous",
    "output",
    "layer",
    "10",
    "output",
    "1000",
    "output",
    "classes",
    "rather",
    "going",
    "add",
    "output",
    "layer",
    "two",
    "output",
    "classes",
    "cat",
    "dog",
    "add",
    "since",
    "set",
    "previous",
    "layers",
    "trainable",
    "see",
    "actually",
    "last",
    "output",
    "layer",
    "going",
    "trainable",
    "layer",
    "entire",
    "model",
    "like",
    "said",
    "already",
    "know",
    "vgg",
    "16",
    "already",
    "learned",
    "features",
    "cats",
    "dogs",
    "original",
    "training",
    "need",
    "retrain",
    "output",
    "layer",
    "classify",
    "two",
    "output",
    "classes",
    "look",
    "new",
    "summary",
    "model",
    "see",
    "everything",
    "except",
    "new",
    "dense",
    "layer",
    "output",
    "layer",
    "two",
    "classes",
    "instead",
    "original",
    "vgg",
    "16",
    "model",
    "also",
    "see",
    "model",
    "8000",
    "trainable",
    "parameters",
    "within",
    "output",
    "layer",
    "said",
    "output",
    "layer",
    "trainable",
    "layer",
    "actually",
    "layers",
    "trainable",
    "go",
    "take",
    "look",
    "original",
    "vgg",
    "16",
    "model",
    "see",
    "138",
    "million",
    "total",
    "parameters",
    "trainable",
    "none",
    "non",
    "trainable",
    "freeze",
    "layers",
    "would",
    "getting",
    "retrained",
    "training",
    "process",
    "cat",
    "dog",
    "images",
    "scroll",
    "back",
    "check",
    "see",
    "still",
    "quite",
    "bit",
    "learnable",
    "parameters",
    "total",
    "parameters",
    "134",
    "million",
    "8000",
    "trainable",
    "rest",
    "non",
    "trainable",
    "next",
    "episode",
    "see",
    "train",
    "modified",
    "model",
    "images",
    "cats",
    "dogs",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "mandy",
    "blizzard",
    "episode",
    "demonstrate",
    "train",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "built",
    "last",
    "time",
    "data",
    "set",
    "cats",
    "dogs",
    "alright",
    "jumping",
    "straight",
    "code",
    "train",
    "model",
    "course",
    "sure",
    "already",
    "code",
    "place",
    "last",
    "time",
    "building",
    "code",
    "already",
    "run",
    "previously",
    "using",
    "model",
    "first",
    "compile",
    "get",
    "ready",
    "training",
    "model",
    "sequential",
    "model",
    "built",
    "last",
    "time",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "containing",
    "layers",
    "frozen",
    "weights",
    "except",
    "last",
    "layer",
    "modified",
    "output",
    "two",
    "possible",
    "outputs",
    "compiling",
    "model",
    "using",
    "atom",
    "optimizer",
    "previously",
    "used",
    "learning",
    "rate",
    "loss",
    "using",
    "categorical",
    "cross",
    "entropy",
    "like",
    "using",
    "accuracy",
    "metric",
    "judge",
    "model",
    "performance",
    "nothing",
    "new",
    "call",
    "compile",
    "pretty",
    "much",
    "exactly",
    "seen",
    "previous",
    "models",
    "going",
    "train",
    "model",
    "using",
    "model",
    "dot",
    "fit",
    "passing",
    "training",
    "data",
    "set",
    "stored",
    "train",
    "batches",
    "passing",
    "validation",
    "set",
    "stored",
    "valid",
    "batches",
    "going",
    "run",
    "model",
    "five",
    "epochs",
    "setting",
    "verbosity",
    "level",
    "two",
    "see",
    "comprehensive",
    "output",
    "model",
    "training",
    "let",
    "see",
    "happens",
    "alright",
    "training",
    "finished",
    "pretty",
    "outstanding",
    "results",
    "five",
    "epochs",
    "training",
    "data",
    "validation",
    "data",
    "accuracy",
    "training",
    "data",
    "99",
    "validation",
    "accuracy",
    "right",
    "par",
    "98",
    "five",
    "epochs",
    "look",
    "first",
    "epoch",
    "even",
    "first",
    "epoch",
    "gives",
    "gives",
    "us",
    "training",
    "accuracy",
    "85",
    "starting",
    "validation",
    "accuracy",
    "93",
    "totally",
    "surprising",
    "remember",
    "earlier",
    "course",
    "discussed",
    "vgg",
    "16",
    "already",
    "trained",
    "images",
    "cats",
    "dogs",
    "image",
    "net",
    "library",
    "already",
    "learned",
    "features",
    "flight",
    "training",
    "output",
    "layer",
    "train",
    "vgg",
    "16",
    "output",
    "cat",
    "dog",
    "classes",
    "really",
    "surprising",
    "good",
    "job",
    "right",
    "bat",
    "first",
    "epoch",
    "even",
    "even",
    "considerably",
    "better",
    "fifth",
    "epoch",
    "99",
    "training",
    "accuracy",
    "called",
    "previous",
    "cnn",
    "built",
    "scratch",
    "really",
    "simple",
    "convolutional",
    "neural",
    "network",
    "model",
    "actually",
    "really",
    "well",
    "training",
    "data",
    "reaching",
    "100",
    "accuracy",
    "small",
    "amount",
    "epochs",
    "well",
    "saw",
    "lagging",
    "though",
    "validation",
    "accuracy",
    "validation",
    "accuracy",
    "around",
    "70",
    "see",
    "98",
    "main",
    "recognizable",
    "difference",
    "simple",
    "cnn",
    "vgg",
    "16",
    "fine",
    "tuned",
    "model",
    "well",
    "model",
    "generalizes",
    "cat",
    "dog",
    "data",
    "validation",
    "set",
    "whereas",
    "model",
    "built",
    "scratch",
    "generalize",
    "well",
    "data",
    "included",
    "training",
    "set",
    "next",
    "episode",
    "going",
    "use",
    "vgg",
    "16",
    "model",
    "inference",
    "predict",
    "cat",
    "dog",
    "images",
    "test",
    "set",
    "given",
    "accuracy",
    "seeing",
    "validation",
    "set",
    "expect",
    "see",
    "really",
    "good",
    "results",
    "test",
    "set",
    "well",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "use",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "inference",
    "predict",
    "images",
    "test",
    "set",
    "right",
    "jumping",
    "right",
    "back",
    "jupyter",
    "notebook",
    "making",
    "sure",
    "code",
    "place",
    "run",
    "previous",
    "episodes",
    "building",
    "code",
    "run",
    "first",
    "things",
    "first",
    "going",
    "using",
    "model",
    "get",
    "predictions",
    "test",
    "set",
    "call",
    "model",
    "dot",
    "predict",
    "exposed",
    "past",
    "recall",
    "model",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "predict",
    "passing",
    "test",
    "set",
    "stored",
    "test",
    "batches",
    "setting",
    "verbosity",
    "level",
    "zero",
    "want",
    "see",
    "output",
    "output",
    "predictions",
    "recall",
    "previously",
    "talked",
    "shuffle",
    "test",
    "set",
    "cat",
    "dog",
    "image",
    "data",
    "set",
    "want",
    "able",
    "access",
    "classes",
    "uncheck",
    "old",
    "order",
    "pass",
    "uncheck",
    "old",
    "classes",
    "correspond",
    "old",
    "sorry",
    "shuffled",
    "labels",
    "correspond",
    "ns",
    "shuffled",
    "test",
    "data",
    "want",
    "able",
    "one",
    "one",
    "mapping",
    "labels",
    "actually",
    "correct",
    "shuffled",
    "labels",
    "unsettled",
    "data",
    "samples",
    "want",
    "able",
    "pass",
    "confusion",
    "matrix",
    "story",
    "saw",
    "whenever",
    "using",
    "cnn",
    "built",
    "scratch",
    "episodes",
    "back",
    "process",
    "using",
    "dataset",
    "recall",
    "plotting",
    "confusion",
    "matrix",
    "exact",
    "manner",
    "actually",
    "exact",
    "line",
    "used",
    "plot",
    "confusion",
    "matrix",
    "episodes",
    "back",
    "plotted",
    "exact",
    "test",
    "set",
    "cn",
    "n",
    "built",
    "scratch",
    "reminder",
    "last",
    "time",
    "recall",
    "looked",
    "class",
    "indices",
    "test",
    "batches",
    "could",
    "get",
    "correct",
    "order",
    "cat",
    "dog",
    "classes",
    "use",
    "labels",
    "confusion",
    "matrix",
    "thing",
    "calling",
    "psychic",
    "learn",
    "plot",
    "confusion",
    "matrix",
    "defined",
    "earlier",
    "notebook",
    "previous",
    "episode",
    "plot",
    "confusion",
    "matrix",
    "passing",
    "confusion",
    "matrix",
    "labels",
    "defined",
    "well",
    "general",
    "title",
    "confusion",
    "matrix",
    "let",
    "check",
    "plot",
    "see",
    "well",
    "model",
    "predictions",
    "third",
    "fourth",
    "time",
    "used",
    "confusion",
    "matrix",
    "course",
    "far",
    "pretty",
    "normalized",
    "read",
    "data",
    "recall",
    "quick",
    "easy",
    "way",
    "look",
    "top",
    "left",
    "bottom",
    "right",
    "along",
    "diagonal",
    "get",
    "quick",
    "overview",
    "well",
    "model",
    "model",
    "correctly",
    "predicted",
    "dog",
    "49",
    "times",
    "images",
    "truly",
    "dogs",
    "correctly",
    "predicted",
    "cat",
    "47",
    "times",
    "images",
    "truly",
    "cats",
    "see",
    "one",
    "time",
    "predicted",
    "cat",
    "actually",
    "dog",
    "three",
    "times",
    "predicted",
    "dog",
    "images",
    "actually",
    "cats",
    "overall",
    "model",
    "incorrectly",
    "predicted",
    "four",
    "samples",
    "gives",
    "us",
    "96",
    "correct",
    "let",
    "see",
    "96",
    "correct",
    "predictions",
    "100",
    "total",
    "predictions",
    "gives",
    "us",
    "accuracy",
    "rate",
    "test",
    "set",
    "96",
    "surprising",
    "given",
    "saw",
    "last",
    "episode",
    "high",
    "level",
    "accuracy",
    "model",
    "validation",
    "set",
    "overall",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "really",
    "well",
    "generalizing",
    "data",
    "seen",
    "training",
    "lot",
    "better",
    "original",
    "model",
    "build",
    "scratch",
    "recall",
    "previously",
    "discussed",
    "overall",
    "fine",
    "tuning",
    "approach",
    "took",
    "model",
    "pretty",
    "minimal",
    "since",
    "cat",
    "dog",
    "data",
    "already",
    "included",
    "original",
    "training",
    "set",
    "original",
    "vgg",
    "16",
    "model",
    "upcoming",
    "episodes",
    "going",
    "fine",
    "tuning",
    "fine",
    "tuning",
    "saw",
    "vgg",
    "fine",
    "tuning",
    "another",
    "well",
    "known",
    "pre",
    "trained",
    "model",
    "time",
    "completely",
    "new",
    "data",
    "set",
    "included",
    "original",
    "data",
    "set",
    "trained",
    "stay",
    "tuned",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "introduce",
    "mobile",
    "nets",
    "class",
    "lightweight",
    "deep",
    "convolutional",
    "neural",
    "networks",
    "much",
    "smaller",
    "faster",
    "size",
    "many",
    "mainstream",
    "popular",
    "models",
    "really",
    "well",
    "known",
    "neural",
    "nets",
    "class",
    "small",
    "low",
    "power",
    "low",
    "latency",
    "models",
    "used",
    "things",
    "like",
    "classification",
    "detection",
    "things",
    "cn",
    "ns",
    "typically",
    "good",
    "small",
    "size",
    "models",
    "considered",
    "great",
    "mobile",
    "devices",
    "hence",
    "name",
    "mobile",
    "nets",
    "stats",
    "taken",
    "give",
    "quick",
    "comparison",
    "regards",
    "size",
    "size",
    "full",
    "vgg",
    "16",
    "network",
    "worked",
    "past",
    "episodes",
    "553",
    "megabytes",
    "disk",
    "pretty",
    "large",
    "generally",
    "speaking",
    "size",
    "one",
    "currently",
    "largest",
    "mobile",
    "nets",
    "17",
    "megabytes",
    "pretty",
    "huge",
    "difference",
    "especially",
    "think",
    "deploying",
    "model",
    "run",
    "mobile",
    "app",
    "example",
    "vast",
    "size",
    "difference",
    "due",
    "number",
    "parameters",
    "weights",
    "biases",
    "contained",
    "model",
    "example",
    "let",
    "see",
    "vgg",
    "16",
    "saw",
    "previously",
    "138",
    "million",
    "total",
    "parameters",
    "lot",
    "17",
    "megabytes",
    "mobile",
    "net",
    "talked",
    "largest",
    "mobile",
    "net",
    "currently",
    "million",
    "parameters",
    "much",
    "much",
    "smaller",
    "relative",
    "scale",
    "vgg",
    "16",
    "138",
    "million",
    "aside",
    "size",
    "disk",
    "consideration",
    "comes",
    "comparing",
    "mobile",
    "nets",
    "larger",
    "models",
    "also",
    "need",
    "consider",
    "memory",
    "well",
    "parameters",
    "model",
    "space",
    "memory",
    "taking",
    "also",
    "mobile",
    "nets",
    "faster",
    "smaller",
    "competitors",
    "big",
    "hefty",
    "models",
    "like",
    "vgg",
    "16",
    "catch",
    "trade",
    "trade",
    "accuracy",
    "mobile",
    "nets",
    "accurate",
    "big",
    "players",
    "like",
    "vgg",
    "16",
    "example",
    "let",
    "discourage",
    "true",
    "mobile",
    "nets",
    "accurate",
    "resource",
    "heavy",
    "models",
    "like",
    "vgg",
    "16",
    "example",
    "trade",
    "actually",
    "pretty",
    "small",
    "relatively",
    "small",
    "reduction",
    "accuracy",
    "corresponding",
    "blog",
    "episode",
    "link",
    "paper",
    "goes",
    "depth",
    "relatively",
    "small",
    "accuracy",
    "difference",
    "like",
    "check",
    "let",
    "see",
    "work",
    "mobile",
    "nets",
    "code",
    "kerris",
    "alright",
    "jupyter",
    "notebook",
    "first",
    "thing",
    "need",
    "import",
    "packages",
    "making",
    "use",
    "video",
    "next",
    "several",
    "videos",
    "covering",
    "mobilenet",
    "mentioned",
    "earlier",
    "course",
    "gpu",
    "required",
    "running",
    "gpu",
    "want",
    "run",
    "cell",
    "cell",
    "seen",
    "couple",
    "times",
    "already",
    "course",
    "earlier",
    "making",
    "sure",
    "tensorflow",
    "identify",
    "gpu",
    "running",
    "one",
    "setting",
    "memory",
    "growth",
    "true",
    "gpu",
    "worry",
    "gpu",
    "worry",
    "run",
    "cell",
    "similar",
    "downloaded",
    "vgg",
    "16",
    "model",
    "working",
    "previous",
    "episodes",
    "take",
    "approach",
    "download",
    "mobile",
    "net",
    "call",
    "tf",
    "dot",
    "cares",
    "applications",
    "dot",
    "mobile",
    "net",
    "dot",
    "mobile",
    "net",
    "first",
    "time",
    "call",
    "going",
    "download",
    "mobile",
    "net",
    "internet",
    "need",
    "internet",
    "connection",
    "subsequent",
    "calls",
    "going",
    "getting",
    "model",
    "saved",
    "model",
    "disk",
    "loading",
    "memory",
    "going",
    "assign",
    "mobile",
    "variable",
    "mobile",
    "net",
    "originally",
    "trained",
    "image",
    "net",
    "library",
    "like",
    "vgg",
    "minutes",
    "passing",
    "images",
    "mobile",
    "net",
    "saved",
    "images",
    "image",
    "net",
    "library",
    "images",
    "general",
    "things",
    "going",
    "get",
    "idea",
    "mobile",
    "net",
    "performs",
    "random",
    "images",
    "first",
    "order",
    "able",
    "pass",
    "images",
    "mobile",
    "net",
    "going",
    "little",
    "bit",
    "processing",
    "first",
    "created",
    "function",
    "called",
    "prepare",
    "image",
    "takes",
    "file",
    "name",
    "inside",
    "function",
    "image",
    "path",
    "pointing",
    "location",
    "disk",
    "saved",
    "image",
    "files",
    "going",
    "use",
    "get",
    "predictions",
    "mobile",
    "net",
    "image",
    "path",
    "defined",
    "images",
    "saved",
    "load",
    "image",
    "using",
    "image",
    "path",
    "appending",
    "file",
    "name",
    "pass",
    "say",
    "pass",
    "image",
    "one",
    "dot",
    "png",
    "going",
    "take",
    "file",
    "path",
    "append",
    "one",
    "png",
    "pass",
    "load",
    "image",
    "pass",
    "target",
    "size",
    "224",
    "load",
    "image",
    "function",
    "keras",
    "api",
    "taking",
    "image",
    "file",
    "resizing",
    "size",
    "224",
    "224",
    "size",
    "images",
    "mobile",
    "net",
    "expects",
    "take",
    "image",
    "transfer",
    "format",
    "array",
    "expand",
    "dimensions",
    "image",
    "going",
    "put",
    "image",
    "shape",
    "mobilenet",
    "expects",
    "finally",
    "pass",
    "new",
    "processed",
    "image",
    "last",
    "function",
    "tf",
    "kerris",
    "application",
    "stop",
    "mobile",
    "net",
    "pre",
    "process",
    "input",
    "similar",
    "function",
    "saw",
    "episodes",
    "back",
    "working",
    "vgg",
    "16",
    "pre",
    "process",
    "input",
    "mobile",
    "net",
    "pre",
    "process",
    "input",
    "function",
    "processing",
    "images",
    "way",
    "mobile",
    "net",
    "expects",
    "way",
    "vgg",
    "actually",
    "scaling",
    "rgb",
    "pixel",
    "values",
    "scale",
    "instead",
    "zero",
    "255",
    "scale",
    "minus",
    "one",
    "one",
    "function",
    "overall",
    "entire",
    "function",
    "resizing",
    "image",
    "putting",
    "array",
    "format",
    "expanded",
    "dimensions",
    "mobile",
    "net",
    "processing",
    "returning",
    "processed",
    "image",
    "okay",
    "kind",
    "mouthful",
    "got",
    "two",
    "images",
    "pass",
    "mobile",
    "net",
    "alright",
    "define",
    "function",
    "going",
    "display",
    "first",
    "image",
    "called",
    "one",
    "dot",
    "png",
    "mobile",
    "net",
    "samples",
    "directory",
    "told",
    "set",
    "random",
    "images",
    "going",
    "plot",
    "jupyter",
    "notebook",
    "know",
    "lizard",
    "first",
    "image",
    "going",
    "pass",
    "image",
    "prepare",
    "image",
    "function",
    "defined",
    "right",
    "finished",
    "talking",
    "going",
    "pass",
    "function",
    "pre",
    "process",
    "image",
    "accordingly",
    "going",
    "pass",
    "pre",
    "processed",
    "image",
    "returned",
    "function",
    "mobile",
    "net",
    "model",
    "going",
    "calling",
    "predict",
    "model",
    "like",
    "done",
    "previous",
    "videos",
    "called",
    "predict",
    "models",
    "use",
    "inference",
    "get",
    "prediction",
    "particular",
    "image",
    "going",
    "give",
    "prediction",
    "image",
    "net",
    "utils",
    "dot",
    "decode",
    "predictions",
    "function",
    "function",
    "cares",
    "going",
    "return",
    "top",
    "five",
    "predictions",
    "1000",
    "possible",
    "image",
    "net",
    "classes",
    "going",
    "tell",
    "us",
    "top",
    "five",
    "mobile",
    "net",
    "predicting",
    "image",
    "let",
    "run",
    "print",
    "results",
    "maybe",
    "better",
    "idea",
    "mean",
    "see",
    "printed",
    "output",
    "run",
    "output",
    "top",
    "five",
    "order",
    "results",
    "imagenet",
    "classes",
    "mobile",
    "net",
    "predicting",
    "image",
    "assigning",
    "58",
    "probability",
    "image",
    "american",
    "chameleon",
    "28",
    "probability",
    "green",
    "lizard",
    "13",
    "gamma",
    "small",
    "percentages",
    "1",
    "two",
    "types",
    "lizards",
    "turns",
    "aware",
    "know",
    "could",
    "aware",
    "everyone",
    "know",
    "american",
    "chameleon",
    "know",
    "always",
    "called",
    "things",
    "green",
    "ols",
    "looked",
    "also",
    "known",
    "american",
    "chameleon",
    "mobilenet",
    "got",
    "right",
    "yeah",
    "assigned",
    "58",
    "probability",
    "highest",
    "probable",
    "class",
    "next",
    "green",
    "lizard",
    "say",
    "still",
    "really",
    "good",
    "second",
    "place",
    "know",
    "green",
    "lizard",
    "supposed",
    "general",
    "gamma",
    "also",
    "similar",
    "looking",
    "lizard",
    "know",
    "top",
    "three",
    "classes",
    "predicted",
    "almost",
    "100",
    "three",
    "would",
    "say",
    "mobile",
    "net",
    "pretty",
    "good",
    "job",
    "prediction",
    "let",
    "move",
    "number",
    "two",
    "alright",
    "going",
    "plot",
    "second",
    "image",
    "cup",
    "well",
    "originally",
    "thought",
    "espresso",
    "someone",
    "called",
    "cup",
    "cappuccino",
    "let",
    "say",
    "sure",
    "coffee",
    "connoisseur",
    "although",
    "like",
    "espresso",
    "cappuccinos",
    "looks",
    "like",
    "cream",
    "hey",
    "going",
    "go",
    "process",
    "lizard",
    "image",
    "passing",
    "image",
    "passing",
    "new",
    "image",
    "prepare",
    "image",
    "function",
    "undergoes",
    "pre",
    "processing",
    "going",
    "pass",
    "pre",
    "processed",
    "image",
    "predict",
    "function",
    "mobile",
    "net",
    "model",
    "finally",
    "going",
    "get",
    "top",
    "five",
    "results",
    "predictions",
    "model",
    "relative",
    "regards",
    "imagenet",
    "classes",
    "let",
    "see",
    "right",
    "according",
    "mobile",
    "net",
    "espresso",
    "cappuccino",
    "know",
    "predicts",
    "99",
    "probability",
    "espresso",
    "probable",
    "class",
    "particular",
    "image",
    "say",
    "pretty",
    "reasonable",
    "let",
    "know",
    "comments",
    "think",
    "espresso",
    "cappuccino",
    "know",
    "mobile",
    "image",
    "net",
    "cappuccino",
    "class",
    "say",
    "pretty",
    "spot",
    "see",
    "four",
    "predictions",
    "less",
    "1",
    "reasonable",
    "mean",
    "second",
    "one",
    "cup",
    "third",
    "eggnog",
    "fourth",
    "coffee",
    "mug",
    "fifth",
    "wooden",
    "spoon",
    "gets",
    "little",
    "bit",
    "weird",
    "wood",
    "circular",
    "shape",
    "going",
    "1",
    "pretty",
    "negligible",
    "would",
    "say",
    "mobilenet",
    "pretty",
    "great",
    "job",
    "giving",
    "99",
    "probability",
    "espresso",
    "image",
    "right",
    "one",
    "sample",
    "image",
    "let",
    "bring",
    "strawberry",
    "multiple",
    "strawberries",
    "call",
    "consider",
    "background",
    "thing",
    "pre",
    "processing",
    "strawberry",
    "image",
    "getting",
    "prediction",
    "mobile",
    "net",
    "image",
    "getting",
    "top",
    "five",
    "results",
    "probable",
    "predictions",
    "among",
    "1000",
    "image",
    "net",
    "classes",
    "see",
    "mobile",
    "net",
    "probability",
    "classifies",
    "image",
    "strawberry",
    "correctly",
    "well",
    "rest",
    "well",
    "well",
    "1",
    "fruits",
    "interesting",
    "another",
    "really",
    "good",
    "prediction",
    "mobile",
    "net",
    "even",
    "small",
    "reduction",
    "accuracy",
    "talked",
    "beginning",
    "episode",
    "probably",
    "tell",
    "three",
    "random",
    "samples",
    "reduction",
    "maybe",
    "even",
    "noticeable",
    "tests",
    "like",
    "ones",
    "ran",
    "upcoming",
    "episodes",
    "actually",
    "going",
    "fine",
    "tuning",
    "mobile",
    "net",
    "model",
    "work",
    "custom",
    "data",
    "set",
    "custom",
    "data",
    "set",
    "one",
    "included",
    "original",
    "image",
    "net",
    "library",
    "going",
    "brand",
    "new",
    "data",
    "set",
    "going",
    "fine",
    "tuning",
    "done",
    "past",
    "stay",
    "tuned",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "deep",
    "lizard",
    "calm",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "mandy",
    "deep",
    "lizard",
    "episode",
    "preparing",
    "processing",
    "custom",
    "data",
    "set",
    "use",
    "fine",
    "tune",
    "mobilenet",
    "using",
    "tensor",
    "flows",
    "keras",
    "api",
    "previously",
    "saw",
    "well",
    "vgg",
    "16",
    "able",
    "classify",
    "generalize",
    "images",
    "cats",
    "dogs",
    "noted",
    "vgg",
    "16",
    "actually",
    "already",
    "trained",
    "cats",
    "dogs",
    "take",
    "lot",
    "fine",
    "tuning",
    "get",
    "perform",
    "well",
    "cat",
    "dog",
    "dataset",
    "mobile",
    "net",
    "going",
    "fine",
    "tuning",
    "steps",
    "well",
    "time",
    "working",
    "data",
    "set",
    "completely",
    "different",
    "original",
    "image",
    "net",
    "library",
    "mobile",
    "net",
    "originally",
    "trained",
    "new",
    "data",
    "set",
    "working",
    "data",
    "set",
    "images",
    "sign",
    "language",
    "digits",
    "10",
    "classes",
    "total",
    "dataset",
    "ranging",
    "zero",
    "nine",
    "class",
    "contains",
    "images",
    "particular",
    "sign",
    "digit",
    "data",
    "set",
    "available",
    "kaggle",
    "grayscale",
    "images",
    "also",
    "available",
    "github",
    "rgb",
    "images",
    "particular",
    "task",
    "using",
    "rgb",
    "images",
    "check",
    "corresponding",
    "blog",
    "episode",
    "peebles",
    "get",
    "link",
    "download",
    "data",
    "set",
    "downloading",
    "data",
    "set",
    "next",
    "step",
    "organize",
    "data",
    "disk",
    "similar",
    "procedure",
    "saw",
    "cat",
    "dog",
    "data",
    "set",
    "earlier",
    "course",
    "download",
    "looks",
    "like",
    "zipped",
    "folder",
    "called",
    "sign",
    "language",
    "digits",
    "data",
    "set",
    "master",
    "first",
    "thing",
    "want",
    "extract",
    "contents",
    "directory",
    "navigate",
    "inside",
    "get",
    "dataset",
    "directory",
    "inside",
    "classes",
    "directories",
    "corresponding",
    "images",
    "particular",
    "class",
    "want",
    "want",
    "grab",
    "zero",
    "nine",
    "going",
    "ctrl",
    "x",
    "cut",
    "directories",
    "going",
    "navigate",
    "back",
    "root",
    "directory",
    "going",
    "place",
    "directories",
    "zero",
    "nine",
    "route",
    "going",
    "get",
    "rid",
    "everything",
    "else",
    "deleting",
    "one",
    "last",
    "thing",
    "going",
    "delete",
    "master",
    "necessarily",
    "like",
    "sign",
    "language",
    "digits",
    "data",
    "set",
    "directly",
    "within",
    "nested",
    "directories",
    "consisting",
    "zero",
    "nine",
    "training",
    "data",
    "inside",
    "last",
    "step",
    "move",
    "sign",
    "language",
    "digits",
    "data",
    "set",
    "directory",
    "going",
    "working",
    "relative",
    "jupyter",
    "notebook",
    "lives",
    "inside",
    "deep",
    "learning",
    "cares",
    "directory",
    "data",
    "directory",
    "sign",
    "language",
    "digits",
    "data",
    "set",
    "located",
    "everything",
    "else",
    "organize",
    "process",
    "data",
    "done",
    "programmatically",
    "code",
    "jupyter",
    "notebook",
    "make",
    "sure",
    "imports",
    "brought",
    "last",
    "time",
    "still",
    "place",
    "making",
    "use",
    "class",
    "breakdown",
    "let",
    "know",
    "many",
    "images",
    "class",
    "across",
    "classes",
    "zero",
    "nine",
    "anywhere",
    "204",
    "208",
    "samples",
    "class",
    "explanation",
    "data",
    "structured",
    "point",
    "rest",
    "organization",
    "programmatically",
    "script",
    "script",
    "going",
    "organize",
    "data",
    "train",
    "valid",
    "test",
    "directories",
    "recall",
    "right",
    "data",
    "located",
    "corresponding",
    "classes",
    "zero",
    "nine",
    "data",
    "broken",
    "yet",
    "separate",
    "data",
    "sets",
    "train",
    "test",
    "validation",
    "first",
    "changing",
    "directory",
    "sign",
    "language",
    "digits",
    "data",
    "set",
    "directory",
    "checking",
    "make",
    "sure",
    "directory",
    "structure",
    "setup",
    "already",
    "place",
    "disk",
    "make",
    "train",
    "valid",
    "test",
    "directory",
    "right",
    "within",
    "sign",
    "language",
    "digits",
    "dataset",
    "next",
    "iterating",
    "directories",
    "within",
    "sign",
    "language",
    "digits",
    "dataset",
    "directory",
    "recall",
    "directories",
    "labeled",
    "zero",
    "nine",
    "loop",
    "range",
    "zero",
    "going",
    "directory",
    "zero",
    "nine",
    "moving",
    "directories",
    "train",
    "directory",
    "making",
    "two",
    "new",
    "directories",
    "one",
    "inside",
    "valid",
    "directory",
    "whatever",
    "place",
    "loop",
    "run",
    "number",
    "zero",
    "making",
    "directory",
    "called",
    "zero",
    "invalid",
    "directory",
    "called",
    "zero",
    "within",
    "test",
    "run",
    "number",
    "one",
    "creating",
    "directory",
    "called",
    "one",
    "invalid",
    "one",
    "within",
    "test",
    "whole",
    "process",
    "moving",
    "class",
    "train",
    "creating",
    "class",
    "directory",
    "empty",
    "within",
    "valid",
    "test",
    "point",
    "let",
    "suppose",
    "first",
    "run",
    "loop",
    "range",
    "following",
    "add",
    "number",
    "zero",
    "line",
    "sampling",
    "30",
    "samples",
    "train",
    "slash",
    "zero",
    "directory",
    "created",
    "moved",
    "class",
    "directory",
    "zero",
    "train",
    "going",
    "sample",
    "30",
    "random",
    "samples",
    "zero",
    "directory",
    "within",
    "train",
    "calling",
    "valid",
    "samples",
    "going",
    "samples",
    "move",
    "validation",
    "set",
    "next",
    "30",
    "samples",
    "collected",
    "training",
    "set",
    "randomly",
    "line",
    "going",
    "moving",
    "training",
    "set",
    "validation",
    "zero",
    "set",
    "validation",
    "set",
    "class",
    "zero",
    "similarly",
    "thing",
    "test",
    "samples",
    "randomly",
    "select",
    "five",
    "samples",
    "train",
    "slash",
    "zero",
    "directory",
    "move",
    "five",
    "samples",
    "train",
    "slash",
    "zero",
    "directory",
    "test",
    "zero",
    "directory",
    "ran",
    "loop",
    "using",
    "class",
    "zero",
    "example",
    "going",
    "happen",
    "class",
    "zero",
    "nine",
    "case",
    "issue",
    "visualizing",
    "script",
    "go",
    "sign",
    "language",
    "digits",
    "dataset",
    "let",
    "check",
    "organized",
    "data",
    "recall",
    "previously",
    "classes",
    "zero",
    "nine",
    "listed",
    "directly",
    "within",
    "route",
    "folder",
    "train",
    "valid",
    "test",
    "directories",
    "within",
    "train",
    "moved",
    "original",
    "zero",
    "nine",
    "directories",
    "train",
    "done",
    "sampled",
    "30",
    "images",
    "classes",
    "moved",
    "valid",
    "directory",
    "classes",
    "similarly",
    "thing",
    "test",
    "directory",
    "look",
    "see",
    "test",
    "directory",
    "five",
    "samples",
    "zero",
    "see",
    "five",
    "samples",
    "one",
    "go",
    "check",
    "valid",
    "directories",
    "look",
    "zero",
    "30",
    "zeros",
    "see",
    "every",
    "valid",
    "directory",
    "30",
    "samples",
    "training",
    "directory",
    "classes",
    "necessarily",
    "uniform",
    "samples",
    "remember",
    "saw",
    "number",
    "samples",
    "class",
    "ranged",
    "anywhere",
    "204",
    "209",
    "think",
    "number",
    "images",
    "within",
    "class",
    "directory",
    "training",
    "sample",
    "differ",
    "slightly",
    "maybe",
    "one",
    "two",
    "images",
    "number",
    "images",
    "classes",
    "validation",
    "test",
    "directories",
    "uniform",
    "since",
    "programmatically",
    "script",
    "checking",
    "dataset",
    "disk",
    "see",
    "exactly",
    "format",
    "structured",
    "cat",
    "dog",
    "image",
    "data",
    "set",
    "earlier",
    "course",
    "dealing",
    "10",
    "classes",
    "instead",
    "two",
    "downloaded",
    "data",
    "slightly",
    "different",
    "organizational",
    "structure",
    "cat",
    "dog",
    "data",
    "previously",
    "downloaded",
    "alright",
    "obtained",
    "data",
    "organized",
    "data",
    "last",
    "step",
    "pre",
    "process",
    "data",
    "first",
    "starting",
    "defining",
    "train",
    "valid",
    "test",
    "directories",
    "live",
    "disk",
    "supply",
    "paths",
    "setting",
    "directory",
    "iterators",
    "familiar",
    "point",
    "given",
    "format",
    "processed",
    "cat",
    "dog",
    "images",
    "whenever",
    "build",
    "cnn",
    "scratch",
    "use",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "let",
    "focus",
    "first",
    "variable",
    "train",
    "batches",
    "first",
    "calling",
    "image",
    "data",
    "generator",
    "dot",
    "flow",
    "directory",
    "ca",
    "quite",
    "see",
    "scroll",
    "minute",
    "image",
    "data",
    "generator",
    "passing",
    "pre",
    "processing",
    "function",
    "case",
    "mobile",
    "net",
    "pre",
    "processing",
    "function",
    "recall",
    "saw",
    "already",
    "last",
    "episode",
    "discussed",
    "pre",
    "processes",
    "images",
    "way",
    "scales",
    "image",
    "data",
    "rather",
    "scale",
    "zero",
    "255",
    "instead",
    "scale",
    "minus",
    "one",
    "one",
    "image",
    "data",
    "generator",
    "call",
    "flow",
    "directory",
    "blowing",
    "screen",
    "set",
    "directory",
    "equal",
    "train",
    "path",
    "defined",
    "saying",
    "training",
    "data",
    "resides",
    "disk",
    "setting",
    "target",
    "size",
    "224",
    "224",
    "recall",
    "resizes",
    "training",
    "data",
    "height",
    "height",
    "224",
    "width",
    "224",
    "since",
    "image",
    "size",
    "mobile",
    "net",
    "expects",
    "setting",
    "batch",
    "size",
    "equal",
    "got",
    "deal",
    "valid",
    "batches",
    "test",
    "batches",
    "well",
    "everything",
    "exactly",
    "except",
    "paths",
    "deferring",
    "show",
    "validation",
    "test",
    "sets",
    "live",
    "disk",
    "familiar",
    "specify",
    "shuffle",
    "equals",
    "false",
    "test",
    "set",
    "later",
    "appropriately",
    "plot",
    "prediction",
    "results",
    "confusion",
    "matrix",
    "run",
    "cell",
    "output",
    "17",
    "112",
    "images",
    "belonging",
    "10",
    "classes",
    "corresponds",
    "training",
    "set",
    "300",
    "images",
    "10",
    "classes",
    "validation",
    "set",
    "50",
    "images",
    "belonging",
    "10",
    "classes",
    "test",
    "set",
    "cell",
    "several",
    "assertions",
    "assert",
    "output",
    "find",
    "right",
    "expect",
    "getting",
    "perhaps",
    "pointing",
    "wrong",
    "location",
    "disk",
    "lot",
    "times",
    "pointing",
    "wrong",
    "location",
    "probably",
    "get",
    "found",
    "zero",
    "images",
    "belonging",
    "10",
    "classes",
    "need",
    "check",
    "path",
    "data",
    "set",
    "resides",
    "get",
    "alright",
    "data",
    "set",
    "processed",
    "ready",
    "move",
    "building",
    "fine",
    "tuning",
    "mobile",
    "net",
    "model",
    "data",
    "set",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "depot",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "mandy",
    "blizzard",
    "episode",
    "go",
    "process",
    "fine",
    "tuning",
    "mobile",
    "net",
    "custom",
    "data",
    "set",
    "alright",
    "jumping",
    "right",
    "back",
    "jupyter",
    "notebook",
    "last",
    "time",
    "make",
    "sure",
    "code",
    "place",
    "since",
    "building",
    "directly",
    "first",
    "thing",
    "going",
    "going",
    "import",
    "mobile",
    "net",
    "first",
    "mobile",
    "net",
    "episode",
    "calling",
    "tf",
    "kerris",
    "applications",
    "mobile",
    "net",
    "dot",
    "mobile",
    "net",
    "remember",
    "first",
    "time",
    "running",
    "line",
    "need",
    "internet",
    "connection",
    "download",
    "internet",
    "let",
    "take",
    "look",
    "model",
    "downloaded",
    "calling",
    "model",
    "dot",
    "summary",
    "output",
    "showing",
    "us",
    "lovely",
    "layers",
    "included",
    "mobile",
    "net",
    "get",
    "general",
    "idea",
    "model",
    "fine",
    "tuning",
    "fine",
    "tuning",
    "process",
    "going",
    "start",
    "us",
    "getting",
    "layers",
    "sixth",
    "last",
    "layer",
    "scroll",
    "look",
    "output",
    "going",
    "get",
    "layers",
    "layer",
    "everything",
    "else",
    "going",
    "included",
    "layers",
    "going",
    "keep",
    "transfer",
    "new",
    "model",
    "new",
    "fine",
    "tune",
    "model",
    "going",
    "include",
    "last",
    "five",
    "layers",
    "choice",
    "came",
    "little",
    "experimenting",
    "testing",
    "number",
    "layers",
    "choose",
    "include",
    "versus",
    "include",
    "whenever",
    "fine",
    "tuning",
    "model",
    "going",
    "come",
    "experimentation",
    "personal",
    "choice",
    "us",
    "getting",
    "everything",
    "layer",
    "going",
    "keep",
    "new",
    "fine",
    "tuned",
    "model",
    "let",
    "scroll",
    "calling",
    "mobile",
    "dot",
    "layers",
    "passing",
    "six",
    "last",
    "layer",
    "output",
    "going",
    "create",
    "variable",
    "called",
    "output",
    "going",
    "set",
    "equal",
    "dense",
    "layer",
    "10",
    "units",
    "going",
    "output",
    "layer",
    "called",
    "output",
    "10",
    "units",
    "due",
    "nature",
    "classes",
    "ranging",
    "zero",
    "nine",
    "per",
    "usual",
    "going",
    "followed",
    "softmax",
    "activation",
    "function",
    "give",
    "us",
    "probability",
    "distribution",
    "among",
    "10",
    "outputs",
    "looks",
    "little",
    "strange",
    "calling",
    "like",
    "putting",
    "x",
    "variable",
    "next",
    "well",
    "mobile",
    "net",
    "model",
    "actually",
    "functional",
    "model",
    "functional",
    "api",
    "kerris",
    "sequential",
    "api",
    "kind",
    "touched",
    "little",
    "bit",
    "earlier",
    "whenever",
    "fine",
    "tuned",
    "vgg",
    "16",
    "saw",
    "vgg",
    "16",
    "also",
    "indeed",
    "functional",
    "model",
    "fine",
    "tuned",
    "iterated",
    "layers",
    "added",
    "sequential",
    "model",
    "point",
    "ready",
    "introduce",
    "functional",
    "model",
    "yet",
    "going",
    "continue",
    "working",
    "functional",
    "model",
    "type",
    "basically",
    "taking",
    "layers",
    "sixth",
    "last",
    "whenever",
    "create",
    "output",
    "layer",
    "call",
    "previous",
    "layers",
    "stored",
    "x",
    "way",
    "functional",
    "model",
    "works",
    "basically",
    "saying",
    "output",
    "layer",
    "pass",
    "previous",
    "layers",
    "stored",
    "x",
    "six",
    "last",
    "layer",
    "mobile",
    "net",
    "create",
    "model",
    "using",
    "two",
    "pieces",
    "x",
    "output",
    "saying",
    "calling",
    "model",
    "indeed",
    "functional",
    "model",
    "specified",
    "way",
    "specifying",
    "inputs",
    "equals",
    "mobile",
    "dot",
    "input",
    "taking",
    "input",
    "original",
    "mobile",
    "net",
    "model",
    "outputs",
    "equals",
    "output",
    "point",
    "output",
    "mobile",
    "net",
    "model",
    "six",
    "last",
    "layer",
    "plus",
    "dense",
    "output",
    "layer",
    "alright",
    "let",
    "run",
    "two",
    "cells",
    "create",
    "new",
    "model",
    "alright",
    "new",
    "models",
    "created",
    "next",
    "thing",
    "going",
    "going",
    "go",
    "freeze",
    "layers",
    "experimentation",
    "found",
    "freeze",
    "except",
    "last",
    "23",
    "layers",
    "appears",
    "yield",
    "decent",
    "results",
    "23",
    "magic",
    "number",
    "play",
    "let",
    "know",
    "get",
    "better",
    "results",
    "basically",
    "going",
    "layers",
    "model",
    "default",
    "trainable",
    "saying",
    "want",
    "last",
    "23",
    "layers",
    "trainable",
    "layers",
    "except",
    "last",
    "one",
    "three",
    "make",
    "trainable",
    "understand",
    "relatively",
    "speaking",
    "88",
    "total",
    "layers",
    "original",
    "mobile",
    "net",
    "model",
    "saying",
    "want",
    "train",
    "want",
    "train",
    "last",
    "23",
    "layers",
    "new",
    "model",
    "built",
    "recall",
    "much",
    "earlier",
    "trained",
    "fine",
    "tuned",
    "vgg",
    "16",
    "model",
    "train",
    "output",
    "layer",
    "let",
    "go",
    "ahead",
    "run",
    "let",
    "look",
    "summary",
    "new",
    "fine",
    "tuned",
    "model",
    "glance",
    "looks",
    "basically",
    "saw",
    "original",
    "summary",
    "see",
    "model",
    "ends",
    "global",
    "average",
    "pooling",
    "2d",
    "layer",
    "recall",
    "sixth",
    "last",
    "layer",
    "said",
    "would",
    "include",
    "layer",
    "everything",
    "layers",
    "global",
    "average",
    "pooling",
    "layer",
    "previously",
    "saw",
    "original",
    "mobile",
    "net",
    "summary",
    "gone",
    "instead",
    "output",
    "layer",
    "1000",
    "classes",
    "output",
    "layer",
    "10",
    "classes",
    "corresponding",
    "10",
    "potential",
    "output",
    "classes",
    "new",
    "sign",
    "language",
    "digits",
    "dataset",
    "compare",
    "total",
    "parameters",
    "split",
    "amongst",
    "trainable",
    "non",
    "trainable",
    "parameters",
    "model",
    "original",
    "mobile",
    "mobile",
    "model",
    "see",
    "difference",
    "well",
    "alright",
    "model",
    "built",
    "ready",
    "train",
    "model",
    "code",
    "nothing",
    "new",
    "compiling",
    "model",
    "exact",
    "fashion",
    "using",
    "atom",
    "optimizer",
    "little",
    "bit",
    "luck",
    "learning",
    "rate",
    "categorical",
    "categorical",
    "cross",
    "entropy",
    "loss",
    "accuracy",
    "metric",
    "probably",
    "seen",
    "2",
    "million",
    "times",
    "point",
    "course",
    "exactly",
    "additionally",
    "exactly",
    "fit",
    "function",
    "running",
    "train",
    "model",
    "passing",
    "train",
    "batches",
    "data",
    "set",
    "passing",
    "validation",
    "batches",
    "validation",
    "data",
    "running",
    "10",
    "epochs",
    "actually",
    "going",
    "go",
    "ahead",
    "run",
    "10",
    "save",
    "time",
    "earlier",
    "testing",
    "going",
    "run",
    "30",
    "epochs",
    "going",
    "set",
    "verbose",
    "equal",
    "two",
    "get",
    "verbose",
    "output",
    "let",
    "see",
    "happens",
    "right",
    "model",
    "finished",
    "training",
    "30",
    "epochs",
    "let",
    "check",
    "results",
    "see",
    "output",
    "wondering",
    "first",
    "output",
    "took",
    "90",
    "seconds",
    "get",
    "first",
    "epoch",
    "took",
    "90",
    "seconds",
    "got",
    "five",
    "seconds",
    "later",
    "realized",
    "running",
    "battery",
    "laptop",
    "plugged",
    "plug",
    "laptop",
    "beefed",
    "started",
    "running",
    "much",
    "quicker",
    "let",
    "scroll",
    "look",
    "basically",
    "like",
    "ended",
    "100",
    "accuracy",
    "training",
    "set",
    "92",
    "accuracy",
    "validation",
    "set",
    "pretty",
    "freakin",
    "great",
    "considering",
    "fact",
    "completely",
    "new",
    "dataset",
    "images",
    "included",
    "original",
    "image",
    "net",
    "model",
    "pretty",
    "good",
    "results",
    "little",
    "bit",
    "overfitting",
    "since",
    "validation",
    "accuracy",
    "lower",
    "training",
    "accuracy",
    "wanted",
    "fix",
    "could",
    "take",
    "necessary",
    "steps",
    "combat",
    "overfitting",
    "issue",
    "look",
    "earlier",
    "earlier",
    "epochs",
    "see",
    "kind",
    "story",
    "told",
    "first",
    "epoch",
    "training",
    "accuracy",
    "actually",
    "starts",
    "74",
    "among",
    "10",
    "classes",
    "bad",
    "starting",
    "point",
    "quickly",
    "get",
    "100",
    "training",
    "accuracy",
    "within",
    "four",
    "epochs",
    "great",
    "see",
    "point",
    "81",
    "accuracy",
    "validation",
    "set",
    "decent",
    "amount",
    "overfitting",
    "going",
    "earlier",
    "progress",
    "training",
    "process",
    "overfitting",
    "becoming",
    "less",
    "less",
    "problem",
    "see",
    "actually",
    "point",
    "look",
    "last",
    "eight",
    "epochs",
    "run",
    "even",
    "stalled",
    "yet",
    "validation",
    "loss",
    "stalled",
    "terms",
    "decreasing",
    "value",
    "validation",
    "accuracy",
    "stalled",
    "terms",
    "increasing",
    "perhaps",
    "running",
    "epochs",
    "data",
    "eradicate",
    "overfitting",
    "problem",
    "otherwise",
    "tuning",
    "changing",
    "hyper",
    "parameters",
    "around",
    "different",
    "structure",
    "fine",
    "tuning",
    "model",
    "freeze",
    "morrow",
    "less",
    "last",
    "23",
    "layers",
    "fine",
    "tuning",
    "process",
    "experiment",
    "come",
    "something",
    "yields",
    "better",
    "results",
    "put",
    "comments",
    "let",
    "us",
    "know",
    "one",
    "last",
    "thing",
    "want",
    "fine",
    "tuned",
    "mobile",
    "net",
    "model",
    "use",
    "test",
    "set",
    "familiar",
    "know",
    "drill",
    "procedure",
    "point",
    "done",
    "several",
    "times",
    "going",
    "get",
    "predictions",
    "model",
    "test",
    "set",
    "going",
    "plot",
    "predictions",
    "confusion",
    "matrix",
    "first",
    "going",
    "get",
    "true",
    "labels",
    "calling",
    "test",
    "batches",
    "classes",
    "going",
    "gain",
    "predictions",
    "model",
    "calling",
    "model",
    "dot",
    "predict",
    "passing",
    "test",
    "set",
    "stored",
    "test",
    "batches",
    "setting",
    "verbose",
    "equal",
    "zero",
    "want",
    "see",
    "output",
    "predictions",
    "creating",
    "confusion",
    "matrix",
    "using",
    "socket",
    "learns",
    "confusion",
    "matrix",
    "imported",
    "earlier",
    "setting",
    "true",
    "labels",
    "equal",
    "test",
    "labels",
    "defined",
    "setting",
    "predicted",
    "labels",
    "arg",
    "max",
    "predictions",
    "across",
    "x",
    "one",
    "going",
    "check",
    "class",
    "indices",
    "test",
    "batches",
    "make",
    "sure",
    "think",
    "course",
    "classes",
    "labeled",
    "zero",
    "nine",
    "define",
    "labels",
    "confusion",
    "matrix",
    "accordingly",
    "call",
    "plot",
    "confusion",
    "matrix",
    "brought",
    "earlier",
    "notebook",
    "used",
    "times",
    "point",
    "course",
    "passing",
    "confusion",
    "matrix",
    "plot",
    "passing",
    "labels",
    "want",
    "correspond",
    "confusion",
    "matrix",
    "giving",
    "confusion",
    "matrix",
    "general",
    "title",
    "confusion",
    "matrix",
    "hey",
    "let",
    "plot",
    "oh",
    "plot",
    "confusion",
    "matrix",
    "defined",
    "well",
    "definitely",
    "somewhere",
    "notebook",
    "must",
    "skipped",
    "go",
    "nope",
    "alright",
    "plot",
    "confusion",
    "matrix",
    "defined",
    "let",
    "bring",
    "defined",
    "run",
    "back",
    "looking",
    "top",
    "left",
    "bottom",
    "right",
    "see",
    "model",
    "appears",
    "done",
    "pretty",
    "well",
    "10",
    "classes",
    "total",
    "five",
    "samples",
    "per",
    "class",
    "see",
    "mostly",
    "fours",
    "fives",
    "across",
    "diagonal",
    "meaning",
    "time",
    "model",
    "predicted",
    "correctly",
    "example",
    "nine",
    "five",
    "times",
    "five",
    "model",
    "predicted",
    "image",
    "nine",
    "actually",
    "eight",
    "however",
    "four",
    "five",
    "times",
    "model",
    "correctly",
    "predict",
    "looks",
    "like",
    "one",
    "times",
    "model",
    "let",
    "see",
    "predicted",
    "one",
    "eight",
    "total",
    "got",
    "12345",
    "incorrect",
    "predictions",
    "50",
    "total",
    "gives",
    "us",
    "90",
    "accuracy",
    "rate",
    "test",
    "set",
    "surprising",
    "us",
    "given",
    "accuracy",
    "saw",
    "right",
    "validation",
    "set",
    "hopefully",
    "series",
    "mobile",
    "net",
    "given",
    "insight",
    "fine",
    "tune",
    "models",
    "custom",
    "data",
    "set",
    "use",
    "transfer",
    "learning",
    "use",
    "information",
    "model",
    "gained",
    "original",
    "training",
    "set",
    "completely",
    "new",
    "task",
    "future",
    "sure",
    "check",
    "blog",
    "resources",
    "available",
    "episode",
    "well",
    "deep",
    "lizard",
    "hive",
    "mind",
    "gain",
    "access",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "let",
    "move",
    "next",
    "episode",
    "hey",
    "andy",
    "deep",
    "lizard",
    "episode",
    "going",
    "learn",
    "use",
    "data",
    "augmentation",
    "images",
    "using",
    "tensor",
    "flows",
    "keras",
    "api",
    "data",
    "augmentation",
    "occurs",
    "create",
    "new",
    "data",
    "making",
    "modifications",
    "existing",
    "data",
    "going",
    "explain",
    "idea",
    "little",
    "bit",
    "jump",
    "code",
    "want",
    "thorough",
    "explanation",
    "sure",
    "check",
    "corresponding",
    "episode",
    "deep",
    "learning",
    "fundamentals",
    "course",
    "depot",
    "example",
    "demonstrate",
    "moment",
    "data",
    "working",
    "image",
    "data",
    "image",
    "data",
    "specifically",
    "data",
    "augmentation",
    "would",
    "include",
    "things",
    "like",
    "flipping",
    "image",
    "either",
    "horizontally",
    "vertically",
    "could",
    "include",
    "rotating",
    "image",
    "could",
    "include",
    "changing",
    "color",
    "image",
    "one",
    "major",
    "reasons",
    "want",
    "use",
    "data",
    "augmentation",
    "simply",
    "get",
    "access",
    "data",
    "lot",
    "times",
    "access",
    "enough",
    "data",
    "issue",
    "run",
    "run",
    "problems",
    "like",
    "overfitting",
    "training",
    "data",
    "set",
    "small",
    "major",
    "reason",
    "use",
    "data",
    "augmentation",
    "grow",
    "training",
    "set",
    "adding",
    "augmented",
    "data",
    "training",
    "set",
    "turn",
    "reduce",
    "overfitting",
    "well",
    "alright",
    "let",
    "jump",
    "code",
    "see",
    "augment",
    "image",
    "data",
    "using",
    "kerris",
    "alright",
    "first",
    "thing",
    "need",
    "course",
    "import",
    "packages",
    "making",
    "use",
    "data",
    "augmentation",
    "next",
    "plot",
    "images",
    "function",
    "introduced",
    "earlier",
    "course",
    "directly",
    "tensor",
    "flows",
    "website",
    "allows",
    "us",
    "apply",
    "images",
    "jupyter",
    "notebook",
    "check",
    "corresponding",
    "blog",
    "episode",
    "deep",
    "lizard",
    "calm",
    "get",
    "link",
    "go",
    "copy",
    "function",
    "alright",
    "next",
    "variable",
    "called",
    "gen",
    "image",
    "data",
    "generator",
    "recall",
    "actually",
    "worked",
    "image",
    "data",
    "generators",
    "earlier",
    "course",
    "whenever",
    "create",
    "train",
    "test",
    "valid",
    "batches",
    "using",
    "training",
    "seeing",
    "though",
    "using",
    "image",
    "data",
    "generator",
    "different",
    "way",
    "creating",
    "generator",
    "specifying",
    "parameters",
    "like",
    "rotation",
    "range",
    "width",
    "shift",
    "range",
    "heights",
    "range",
    "sheer",
    "range",
    "zoom",
    "range",
    "channels",
    "drains",
    "horizontal",
    "flip",
    "options",
    "allow",
    "us",
    "augment",
    "image",
    "data",
    "need",
    "check",
    "documentation",
    "tensor",
    "flows",
    "website",
    "get",
    "idea",
    "units",
    "parameters",
    "example",
    "rotation",
    "range",
    "believe",
    "measured",
    "radians",
    "whereas",
    "like",
    "width",
    "shift",
    "range",
    "measured",
    "percentage",
    "width",
    "image",
    "ways",
    "augment",
    "image",
    "data",
    "going",
    "rotating",
    "image",
    "10",
    "radians",
    "going",
    "shifting",
    "width",
    "image",
    "10",
    "height",
    "10",
    "zooming",
    "shifting",
    "color",
    "channels",
    "flipping",
    "image",
    "sorts",
    "things",
    "different",
    "ways",
    "augment",
    "image",
    "data",
    "options",
    "sure",
    "check",
    "image",
    "data",
    "generator",
    "documentation",
    "want",
    "see",
    "options",
    "ones",
    "working",
    "store",
    "jen",
    "variable",
    "next",
    "going",
    "choose",
    "random",
    "image",
    "dog",
    "directory",
    "set",
    "earlier",
    "course",
    "dogs",
    "versus",
    "cats",
    "dataset",
    "going",
    "go",
    "train",
    "dog",
    "going",
    "choose",
    "random",
    "image",
    "directory",
    "going",
    "set",
    "image",
    "path",
    "accordingly",
    "going",
    "set",
    "point",
    "whatever",
    "chosen",
    "image",
    "disk",
    "assertion",
    "make",
    "sure",
    "indeed",
    "valid",
    "file",
    "proceed",
    "remaining",
    "code",
    "going",
    "plot",
    "image",
    "screen",
    "sure",
    "going",
    "since",
    "random",
    "image",
    "disk",
    "cute",
    "looking",
    "know",
    "beagle",
    "beagle",
    "basset",
    "hound",
    "mix",
    "know",
    "guys",
    "think",
    "random",
    "dog",
    "selected",
    "dog",
    "trained",
    "directory",
    "creating",
    "new",
    "variable",
    "called",
    "etre",
    "image",
    "data",
    "generator",
    "created",
    "earlier",
    "called",
    "jin",
    "calling",
    "flow",
    "function",
    "passing",
    "image",
    "flow",
    "going",
    "generate",
    "batch",
    "augmented",
    "images",
    "single",
    "image",
    "next",
    "defining",
    "og",
    "images",
    "variable",
    "going",
    "give",
    "us",
    "10",
    "samples",
    "augmented",
    "images",
    "created",
    "og",
    "etre",
    "lastly",
    "going",
    "plot",
    "images",
    "using",
    "plot",
    "images",
    "defined",
    "right",
    "let",
    "zoom",
    "bit",
    "right",
    "see",
    "first",
    "let",
    "take",
    "look",
    "original",
    "dog",
    "alright",
    "original",
    "image",
    "look",
    "see",
    "given",
    "things",
    "like",
    "rotation",
    "width",
    "shift",
    "everything",
    "defined",
    "earlier",
    "whenever",
    "defined",
    "whenever",
    "defined",
    "image",
    "data",
    "generator",
    "done",
    "images",
    "one",
    "random",
    "way",
    "another",
    "see",
    "kind",
    "happening",
    "example",
    "particular",
    "image",
    "looks",
    "like",
    "shifted",
    "see",
    "head",
    "dog",
    "cut",
    "little",
    "bit",
    "image",
    "let",
    "see",
    "way",
    "dog",
    "originally",
    "facing",
    "head",
    "facing",
    "right",
    "yeah",
    "image",
    "flipped",
    "dog",
    "facing",
    "left",
    "image",
    "appears",
    "shifted",
    "like",
    "one",
    "looks",
    "like",
    "rotated",
    "get",
    "idea",
    "looking",
    "images",
    "types",
    "data",
    "augmentation",
    "done",
    "see",
    "could",
    "helpful",
    "growing",
    "dataset",
    "general",
    "example",
    "say",
    "bunch",
    "images",
    "dogs",
    "whatever",
    "reason",
    "facing",
    "left",
    "want",
    "deploy",
    "model",
    "general",
    "model",
    "classify",
    "different",
    "dogs",
    "types",
    "images",
    "accepted",
    "model",
    "later",
    "might",
    "dogs",
    "facing",
    "right",
    "well",
    "well",
    "maybe",
    "model",
    "totally",
    "implodes",
    "whenever",
    "receives",
    "dog",
    "facing",
    "right",
    "data",
    "augmentation",
    "given",
    "fact",
    "normal",
    "dogs",
    "face",
    "left",
    "right",
    "could",
    "augment",
    "dog",
    "images",
    "data",
    "dogs",
    "also",
    "face",
    "right",
    "direction",
    "well",
    "left",
    "direction",
    "dynamic",
    "data",
    "set",
    "note",
    "corresponding",
    "blog",
    "episode",
    "people",
    "calm",
    "giving",
    "brief",
    "instruction",
    "save",
    "images",
    "disk",
    "want",
    "save",
    "images",
    "augment",
    "add",
    "back",
    "training",
    "set",
    "check",
    "interested",
    "actually",
    "grow",
    "training",
    "set",
    "rather",
    "plot",
    "images",
    "jupyter",
    "notebook",
    "way",
    "currently",
    "vietnam",
    "filming",
    "episode",
    "know",
    "also",
    "vlog",
    "channel",
    "document",
    "travels",
    "share",
    "little",
    "bit",
    "check",
    "beetles",
    "vlog",
    "youtube",
    "also",
    "sure",
    "check",
    "corresponding",
    "blog",
    "episode",
    "along",
    "resources",
    "available",
    "check",
    "people",
    "hive",
    "mind",
    "gain",
    "exclusive",
    "access",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "see",
    "next",
    "time"
  ],
  "keywords": [
    "hey",
    "andy",
    "deep",
    "lizard",
    "course",
    "going",
    "learn",
    "use",
    "kerris",
    "neural",
    "network",
    "api",
    "integrated",
    "tensorflow",
    "learning",
    "code",
    "using",
    "keras",
    "organize",
    "pre",
    "processed",
    "data",
    "move",
    "building",
    "training",
    "artificial",
    "built",
    "trained",
    "models",
    "fine",
    "sets",
    "let",
    "discuss",
    "along",
    "give",
    "work",
    "go",
    "first",
    "fundamentals",
    "take",
    "need",
    "know",
    "get",
    "back",
    "experience",
    "also",
    "path",
    "see",
    "resources",
    "available",
    "actually",
    "episode",
    "corresponding",
    "blog",
    "test",
    "well",
    "used",
    "download",
    "access",
    "hive",
    "mind",
    "check",
    "alright",
    "little",
    "bit",
    "us",
    "idea",
    "steps",
    "general",
    "one",
    "really",
    "already",
    "think",
    "may",
    "able",
    "demonstrate",
    "make",
    "much",
    "previously",
    "fact",
    "level",
    "could",
    "run",
    "three",
    "later",
    "though",
    "library",
    "discussed",
    "important",
    "understand",
    "completely",
    "making",
    "necessarily",
    "working",
    "downloaded",
    "simple",
    "running",
    "line",
    "might",
    "want",
    "tensor",
    "flows",
    "website",
    "sure",
    "last",
    "point",
    "gpu",
    "thing",
    "note",
    "pretty",
    "process",
    "setting",
    "set",
    "like",
    "said",
    "order",
    "place",
    "earlier",
    "second",
    "kind",
    "look",
    "ready",
    "gain",
    "exclusive",
    "perks",
    "rewards",
    "thanks",
    "contributing",
    "collective",
    "intelligence",
    "next",
    "prepare",
    "train",
    "samples",
    "labels",
    "sample",
    "model",
    "example",
    "correspond",
    "would",
    "say",
    "images",
    "cats",
    "dogs",
    "mean",
    "image",
    "label",
    "cat",
    "dog",
    "input",
    "target",
    "format",
    "pass",
    "type",
    "called",
    "sequential",
    "details",
    "expects",
    "dataset",
    "whenever",
    "call",
    "fit",
    "function",
    "two",
    "parameters",
    "x",
    "numpy",
    "array",
    "dot",
    "generator",
    "option",
    "parameter",
    "contains",
    "another",
    "put",
    "processing",
    "different",
    "jupyter",
    "notebook",
    "step",
    "import",
    "random",
    "create",
    "list",
    "creating",
    "instead",
    "individuals",
    "ranging",
    "13",
    "total",
    "older",
    "around",
    "side",
    "effects",
    "cell",
    "loop",
    "within",
    "number",
    "rather",
    "patient",
    "zero",
    "similarly",
    "100",
    "remember",
    "times",
    "since",
    "except",
    "expect",
    "print",
    "saw",
    "passing",
    "ahead",
    "taking",
    "shuffle",
    "either",
    "way",
    "max",
    "range",
    "scale",
    "new",
    "variable",
    "calling",
    "flow",
    "things",
    "build",
    "everything",
    "categorical",
    "cross",
    "entropy",
    "correctly",
    "memory",
    "exactly",
    "layers",
    "looks",
    "class",
    "dense",
    "layer",
    "looking",
    "overall",
    "shape",
    "case",
    "16",
    "units",
    "good",
    "given",
    "specifying",
    "activation",
    "time",
    "value",
    "specify",
    "output",
    "classes",
    "probability",
    "summary",
    "architecture",
    "created",
    "right",
    "previous",
    "episodes",
    "still",
    "included",
    "optimizer",
    "rate",
    "loss",
    "accuracy",
    "recall",
    "split",
    "stored",
    "batch",
    "size",
    "epochs",
    "30",
    "true",
    "shuffled",
    "inside",
    "validation",
    "verbose",
    "epoch",
    "five",
    "great",
    "results",
    "seen",
    "predictions",
    "learned",
    "predict",
    "overfitting",
    "problem",
    "equal",
    "structure",
    "left",
    "include",
    "10",
    "equals",
    "scroll",
    "inference",
    "people",
    "even",
    "originally",
    "exact",
    "getting",
    "effect",
    "prediction",
    "particular",
    "index",
    "indeed",
    "lot",
    "correct",
    "confusion",
    "matrix",
    "done",
    "plot",
    "directly",
    "defined",
    "predicted",
    "top",
    "save",
    "load",
    "file",
    "saved",
    "disk",
    "weights",
    "original",
    "json",
    "string",
    "convolutional",
    "directory",
    "valid",
    "directories",
    "script",
    "small",
    "1000",
    "batches",
    "vgg",
    "224",
    "width",
    "color",
    "cnn",
    "tuning",
    "net",
    "pixel",
    "green",
    "tuned",
    "trainable",
    "functional",
    "mobile",
    "nets",
    "sign",
    "language",
    "digits",
    "nine",
    "augmentation"
  ]
}