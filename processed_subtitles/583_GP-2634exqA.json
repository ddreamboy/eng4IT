{
  "text": "welcome to the new video in this video\nwe are going to see how to do the\npre-processing of data before building\nmachine learning model so before we fit\nthe data to the machine learning model\nwe need to pre-process the data in order\nto see whether there is any missing\nvalue whether there's any outliers\nwhether there is any duplicates value\npresent in the data or either there is\nany garbage values are present in the\ndata so we need to understand the data\nfirst before building any machine\nlearning model we need to check for the\nrelationship between the data how the\ndata is related to each other then only\nwe can build a best machine learning\nmodel so for that purpose we need to do\nthe pre-processing of data so in this\nvideo we're going to see step by step\nhow to pre-process the data before\nfitting the data into machine learning\nmodel so in this video we are going to\ncover the topics like the steps of\npre-processing of data so we start from\nimporting necessary libraries and then\nread the data set then we do the sanity\ncheck of the data and after that we do\nthe exploratory data analysis to\nunderstand the data better after that we\nwill do the missing value treatments of\ndata and after that we'll do outli\ntreatments then check for the duplicates\nand garbage value then do the treatment\nof the duplicates and garbage value\nafter that let's see what's the\nnormalization how to do normalization of\ndata and why needed then after that at\nlast encoding of categorical data to fit\nin the model so then let's see how to do\npre-processing of data step by step\nfirst I will import all the necessary\nLibrary so I will\nimport pandas as PD then import numpy\nLibrary as in\nP after that visualization libraries\nlike cbor\nand math plot\nLi\n[Music]\ninput math plot\nCP\nplts sorry math plotly P\nplot and as\nPLT so first I will import all the\nlibraries yeah my libraries have been\nimported then after that in step two I\nwill read the data to the data frame\nso so PD\ndo\nread so my data is in CSV so read CSV\nafter that I will copy the name of the\ndata so this this is data set is about\nlife expectancy data so I will read that\nand I will store it in a variable called\nDF so so I have read the data to DF as a\ndata frames so if I check for the head\nof the data so DF do\nhead so DF do head it will show the top\nfive so it have the information like\ncountry year status life expectancy\nadult mod ity infant alcohol percentage\nof expenditure htis pness portfolio like\nthat many columns are there in this data\nset so this is basically data set of the\nlife\nexpectancy by having all those column we\nhave to predict the life expectancy of\nthe data so in this video we are\ncovering the topic only the\npre-processing of data before building\nthe model what the step we need to be\ntaken so after that we will fit this\ndata to the model so I have read the\nhead then if I check for the tail that\nshows the\nto bottom five so DF\ndot so DF\nsorry so DF do tail it will show the\nbottom five record so this is bottom\nfive record so this is how after that I\nwill do as a step three we have to do\nthe s check in step three we will do the\nsanity check of the data so sanity check\nmeans we will do the sanity check like\nidentifying the missing value identify\nthe outliers is there any garbage values\nin the data is there any duplicates\nvalues in the data set so we will start\nthe S check by checking the shape of the\ndata shape means how many rows and\ncolumns we have so it's very simple DF\ndo shape if I do DF do shape it will\ngive the shape of data so this is my\nshape it means 2009 38 rows and 22\nColumns of data we have in my data data\nframe I have\n2938 rows and 22 columns I have so if I\ncheck for the information so DF do info\nif I give DF do info it will give the\ninformation like data types the column\ninformation it will give the overall\ninformation like the entries so here we\ncan see that it has\n2938 whatever You' seen in the shape\nsame information get by the information\nit has a\n2938 entries indexing from 0 to\n2937 so in total it has 22 columns so\nall the column information is there in\nrespect of the not null values and D\ntypes so here we can see that it has\nthe columns in total\n20 1 22 columns each this represent the\nnot null counts of the data so here we\ncan see that in total we have 2938 row\nso all here from here to here it have\nthe full values but in this part we can\nobserve some missing values are there so\nit also show the data types of each\ncolumn so the countryes in the data type\nof object and here is in the int form so\nthat like that all the column\ninformation is present so it will Al\nalso show here that in my data frame I\nhave the 16 column with the float data\ntype and four column with the int data\ntype and two columns is the object data\ntype that is object of country and the\nsecond one is the status so this is how\nI can do some sanity check then after\nthat in sanity check I will find the\nmissing value of my data frame I will\nfind is there any missing value so for\nfinding the missing value I need to\ncheck for DF do\neal so DF do eal do sum so before\nfitting into model I should mention I\nshould make sure that\nmy data is clean it should not have any\nmissing value so I need to pre-process\nthe data in that pre-processing I need\nto check for is there any missing value\nso if I do isal or sum it will give the\ncounts of missing value of each column\nso here I can find that in country year\nand status I don't have any missing\nvalue but in life expectancy and adult\nmodality we can find that the there is\n10 10 like that in the alcohol it have\n194 missing value and hepatitis B it\nhave 550 three missing value like that\nwe can find the count of missing value\nof each column if I want the percentage\nof missing value so while doing the\nmissing value treatment we need to\ndecide whether to do the missing value\ntreatment or to delete the column or to\ndelete the rows with missing value so we\nneed to decide based on many criteria\nbased on the number of missing value if\nthe number of missing value is very huge\nin any column we decided to delete that\ncolumn unless the Imp in or the filling\nwith the other value so first I need to\nfind\nthe uh the percentage of missing value\nso by the percentage I can get easy\ninferences so that I can decide whether\nto do missing value treatment or not so\nlet's I will do the percentage in the\ndifferent cell so I will first copy this\nso finding percentage is very easy crl C\nif I do this then crl V I will divide it\nby the the DF of shape I want one number\nDF dot shape in that the first so this\nwill give the counts of the overall\nvalue so I will multiply this to\n100 so it will give the percentage of\nmissing value so so here I can get that\nin this part it have zero this part it\nhave 3% of overall data compared to\noverall data it have the 3% missing\nvalue in this uh hyptis P it have 18%\nmissing value compared to overall data\nin the GDP the 15% so it's in the\nminority side don't have any 50% missing\nvalue If the percentage of the missing\nvalue is above 50% we decide to delete\nthat column so here is no problem there\nis no any missing value with the more\nthan 50% of the overall count so this is\nhow we can do the understand the missing\nvalue so after checking for missing\nvalue so we will do the treatment of\nmissing value after in the treatment of\nmissing Val in this sanity Che we will\njust understand whether we have the\nmissing value or not so I will do step\nby step don't do the missing value\ntreatment here because it will create\nsome confusion so after that after\nchecking for missing value I will check\nfor the duplicates is there any\nduplicates in my data set so while\nchecking for duplicate we need to have\nthe unique value so if we have the\nunique value in my data set then only I\ncan check for the duplicate otherwise\nthe duplicate checking is useless\nbecause\nsome besides unique\nvalue if I have the duplicates it may be\npossible if I had the unique value and\nthen also I have the duplicate that's\nthe problem so first I will check for\nthe your purpose so DF do\nduplicated if I do this DF do duplicated\ndo\nsum it will give the counts of duplicate\nso here we can see that is give that\nzero that means I have zero duplicates\nValu so this\nis the done with we are done with the\nmissing value number of missing value\nand also done with the number of\nduplicates after that we will check for\nthe garbage value we will find is there\nany garbage value in our data set so the\none thing about garbage value is if if\nthe garbage value in the present in the\nany of the column it is always be in the\nform of object data type so if uh for\nexample if if it is the ear if it is in\nthe form of int so is there any garbage\nVal if the garbage Val is present is\nthere any special character within this\ncolumn the data type will show it as\nobject is there any single garbage value\nit will show it as object so garbage\nvalue is always in the form of object so\nwe will check the garbage value in the\ncolumn where there is the data type of\nobject so we will do one for Loop\nfor count the unique values in the\nobject column then we will find whether\nthere is a garbage value or not so I\nwill use for Loop for I in DF dot\nselect D\ntypes I include only the object include\nis equal to objects because the garbage\nvalue will be in the object object\ncolumn so object do\ncolumns so I want this column after that\nI will do the value count of that so DF\ndo\nI the value\ncount value counts of this so I will\nprint\nthis so it will give the count of unique\nvalue so if there is any garbage value\nit will find that garbage value as a\nunique and it will print so I will def\ndefer\nthe result so I will print\nlike the separator I want it like 10\ntimes so if I print this let's see what\nwill we get so we get the garbage the\nvalues like for the country it have the\nwe will find here the Afghanistan Peru\nit don't have any garbage value so after\nthe status we have developing and so we\nwill find it over here that there is no\ngarbage value in this data set if there\nis any garbage value it will find it\nwill show it over here whether it's a\nstar or any special character it will\nshow it to here and it will give the\ncount so here I can found no garbage\nvalues so I need not to worry about the\ngarbage values so this is how we can\ncheck the garbage value we can check it\nby the value count as well as the unique\nvalue anyhow if to find find any garbage\nvalue we have to do impute or the we\nhave to change that garbage value to the\nany median mode anything you want so\nmake sure that our data set is free of\ngarbage value so after s it check we\nwill do data exploration data analysis\nto understand the data set the data in\nmy data set so to understand the data we\nwill do exploratory data analysis so how\nto do that so first start with the\ndescriptive statistics so in order to\nget the descriptive statistics of the\nnumerical column I just use DF to\ndescribe so if I use this code it will\ngive the descriptive statistics if I\nwant to\ntranspose I have to give T it will give\nthe information like this it give the\ncounts mean standard devation minimum 25\npercentile 50 percentile 75 percentile\nand maximum by this we understand about\nthe\nnumber how the data is distributed what\nis the standard deviation of the data\nlike that we will have the many\ninformation if you read all this we will\nget to know about how the data is\npresented how that is\ndistributed how the skilless of data\nevery information you will get you will\nget the basic understanding of the data\nhow it is\nso this is about descriptive of the\nnumerical column if I want the\ndescriptive statistics of\nthe the object column so I will use DF\ndo\ndescribe within bracket I have to use\ninclude object so if I use include\nobject it will give the descriptive\nstatistics of\nthe sorry\ndescribe dis DH DF sorry DF do describe\nand it will give the descriptive\nstatistic but in the different not as\nthe numerical column information it will\ngive the counts the number of values and\nthe unique values so in country column I\nhave the 193 unique value means in total\nI have 193 countries information in\nstatus I have only two\ntwo values like developing and developer\nso this is how we can inference from the\ndescriptive statistics of the object\ncolumn so after that I will do some\nexplor data analysis by understanding\nthe distribution of data to understand\nthe distribution of data visually we\nwill use histogram so I can use a his\nhistogram so for drawing histogram for\nthe each numerical column I will use\nobviously the for Loop for each column\nin DF dot select I will select only so\nthis is a simple code to select only\ndata types with\nthe sorry D\ntypes\ninclude is equal to sorry include number\nso I just want the column of the\nnumerical data type so include the\nnumber so if I use column over\nhere so column and after that I\nwill I want to print SNS that is cboard\nLibrary I use the flot the histogram CBO\nhistory\nflot CBO history flot within that data\nis my data frame and I will give the\nxaxis as each I means each column of\nnumerical data type and I will use PLT\ndo show to show\nevery chart so if I run this code it\nwill give the result like this so if I\ndon't want this red\ncolor warning I just use\nimport\nwarnings import warnings after that\nwarnings\nwarnings. filter\nwarnings within that just use\nignore so by this I don't get that red\ncolor warning so this is how I can get\nthe histogram for each numerical column\nfor the year I can see to that that is\nconstantly distributed here I can see\nthe left skilless of the data in life\nexpectancy of adult modity right\nskewness after that in the infant de\nthere is very minimal there is zero most\nof the values are zero after that in the\nalcohol like that for each numerical\ncolumn I have the\ndistribution chart so this is how I can\nget the distribution by this I can\nunderstand how the data is distributed\nin my data set so this is how I can get\nthe histogram so this is visually\nexplorating the data so\nin exploratory data analysis we will do\nhistogram to understand the distribution\nof data and we will also do the Box FL\nto identify the outlier in the datas to\nidentify whether there is any outlier in\nthe datas we will do the histogram the\nthe Box blot so I will copy the same\ncode and change a little bit so for the\nfilter so D types and here I use the Box\nflot just use the box flot for this and\nI want to show that so just use box flot\nso I get the box flot for each numerical\ncolumn so in the year column I don't\nhave any box plot so in the life\nexpectancy I have the box\nuh outliers in the lower side and in the\nadult mortality I have the outliers in\nthe upper side of the data so in the\ninfant I have the major outliers and in\nthe alcohol also I have the outliers\nthen\nthe percentage expenditure and many\ncolumn I have outlier I can see it by\nthe boxlot information so this is the\nuse of box blot by this way we can\nunderstand about the data I can\nunderstand about the distribution and I\ncan understand about the outlier of the\ndata so after that in order to I will do\nthe scatter flot so scatter flot is\nbasically for by analysis is it is used\nto get whether there is any relationship\nbetween the data or not is there any\nrelationship between the target variable\nand the independent variable so by\nfitting the model\nso we will cover it in the next video\nand I will explain just simply or here\nso in order to build the model there\nshould be relationship between the\ntarget variable and independent variable\nso here we in the explor data anal we\nwill just check whether there is any\npositive relationship or negative\nrelationship between the data so in this\ndata\nset for example in this data set here\nthe life expectancy is the dependent\nfactor all the other columns are the\nindependent Factor by using all the\nother Factor we will fit the model to\npredict the life expectance so here we\nhave to check the relationship between\nthat so I have to use a scatter flot to\ncheck the relationship so first I will\nget the column for the columns so\ncolumns so so I want to get the\nnumerical column first so D\ntypes DF do\nselect D types\ninclude is equal to number so this will\ngive this will give the column of the\nnumerical column only so I want to get\nThe Columns of that so I will use this\nThe Columns so I will get the columns of\nthat so by using this I will flot a\nscatter plot so for scatter plot I have\nto use\nthis like SNS so before that I have to\nuse a for Loop for getting the scatter\nflot for every column so for each\nelement\nin so I have to use a column so I will\nfirst select all the column then paste\nit over here then like life expectancy\nis my target variable so I will remove\nthis\nokay I will remove this and after that I\nput it over here then for each column in\nmy\ndata I want SNS means scatter\nflot so from cbor library scatter\nflot scatter Flo data is my data frame\nthen the X I will choose the each column\nthen as y I will use a\nfixed column that is my life expectancy\nthat is my target variable so it will\ncreate a scatter flot for this\ncombination like year and the life\nexpectancy adult mortality and life\nexpectancy infant deaths and life expect\nlike that it will create a scatter flot\nfor\neach combination so PLT do\nshow so let's see we will get the result\nor not yeah we get a scatter flot for\nthe each combination like life\nexpectancy and the year then we have the\nlife expectancy and the adult mortality\nit have the negative relationship then\nthis also showing some relationship over\nhere then it have the relationship like\nthis by this way we can find the\nrelationship we have to check the\nrelationship before before and after\noutl treatment so if we find the\nrelationship is better after the outl\ntreatment we will do the outl treatment\notherwise we don't do the outl treatment\nwe will do fit the model first before\nthe outl treatment then we will check\nfor the result of the model then we do\nthe outl treatment then again we build\nthe model like that the process will go\non so in this just see about the\nrelationship to understand the data so\nshowing some relationship so this is how\nthe interpret the scatter flot after\nthat correlation hit maps to understand\nthe correlation between the dat we will\nuse\nthe HTE M so DF\ndot if I use DF do\ncore that will give\nthe correlation Matrix\nso so here I have good string yeah I\nhave\nthe\nthat so I will select only the numeric\ncolumn do\nc so if I use this that here control V\nand Dot\ncore so let's see yeah we got the\ncorrelation Matrix of the numerical\ncolumn so it will give the correlation\nhere we can see the correlation between\nthe data by it ranges from 0 to 1 the\nabove 50 show the strong correlation of\nless than left than 50 show the some\ncorrelation between data so if I want to\nflot this in a chart I will use the heat\nmap to flot that so I will use SNS\ndo heat\nmap data so I will select this I will\nfirst store it in any variable so I will\nstore it in yes then I will give the\ndata as my yes then if I flot this let's\nsee what will we\nget yeah we got a heat map so if I want\nthe values in the hit map I have to use\na\nnot is equal to true if I use\nthat I will get the values in the hit\nmap so if I want to increase the size of\nthat I have to use the PIP plot.\nfigure fig\nsize figure size is equal to let's\ngive 15 comma\n15 so what the\n[Music]\nwrong lt. figure fix size sorry we have\nto give it in a\nbracket so if I give it in a bracket\nlet's\nsee yeah we got a heat map with the\nproper size in this I can understand the\ncorrelation between in this data set I\nhave the many column so it have the many\ninformation so by this I can get the\ncorrelation means here this color\nrepresent the high\ncorrelation and here we can find the\ncorrelation between each\ndata so here in this part it have the 8%\ncorrelation then the minus 24%\ncorrelation minus 12% correlation\nbetween alcohol and the infant death so\nthis is how I can get the inferences of\nthe correlation means the relationship\nbetween the data so we can go through\nthe each column information like this\ncolor I can find the correlation between\nthe data so if the correlation is good I\ncan assume that I can predict that my\nmodel will be the will give the better\nresult because there is a relationship\nbetween the data so this is the use of\nthe correlation matrix by using hit Maps\nso this is about exploratory data\nanalysis so till now we have covered\nlike the S Check in the sity check we\nwill do the shape we will check the\nshape of the data we will get the\ninformation of the data to understand\nthe columns information the data types\nand all we will get we will also check\nfor the missing value is there any\nmissing value by using DF do e null of\nsum then we will we will find the\npercentage of missing value then we'll\nfind the duplicates then we we saw about\nthe is there in garbage value by using\nthe value count after that we will do we\ndid exploratory data analysis by\ndescribe data by descriptive statistics\nof the numerical and\nthe object columns then we flot the data\nto understand the distribution we use\nhistogram then we understand the outlier\nin the data we will use box flot then to\nget the Rel relationship between that we\nuse scatter flot and the heat M heat map\nwith\nthe correlation Matrix so this is about\nthe sanity check and explorat data anal\nthen we\nwill come to the step five that is\nmissing value treatment in the sanity\ncheck we find there is a missing value\nin the data so in this step we\nwill treat the missing value we will\ntrat the missing value in the sense we\nwill fill it with the median mode or the\nwe will use some algorithm to fit it\nlike a imputer to fill the missing value\nso by this way we will get the data set\nwithout missing value so that is the\nnecessity before fitting the data into\nmodel we need\nto give the model without any missing\nvalue so that's why we will do the\nmissing values the decision of the\nmissing values depend on various Factor\nso let's figure out what it\nis then let's see how to do the missing\nvalue treatment so for missing value\ntreatment we have the option like we can\nfill it with the median mean or the mode\nof the data column also we have another\noption that is on module we have that is\nK imputer to fill the missing value so\nit work only for the numerical column so\nfor the categorical column we used to\nfill it with the mode so we have we can\nuse both we can use this median mode or\nmean method to inut the data or we can\nuse the K and inputter for the fill the\nmissing value of the numerical column so\nI will explain both in this video so\nfirst if I want to impute the missing so\nfirst I want to check for the missing\nvalue column so DF\ndo Isn Su we already check in the sanity\ncheck the missing value the count of\nmissing value so I will get it here\nitself so have some I have the missing\nvalue in the life expectancy and adult\nmortality so life expectancy and adult\nmortality have the missing value so life\nexpectance and mortality these column\nare like uh numerical but a discrete\ncolumn like life expectance is the full\nvalue if it is in\nthe continuous data we can fill it with\nthe\nmedian or the mean if it is in the\ndiscrete variable we have to put fill it\nwith the mode if it is a categorical\ncolumn with a missing value we have to\nfill it with the\nmode so let's see how to fill the\nmissing value so for that I need to get\nthe column so I will get the life\nexpectancy adult mortality so in this\nfor life expectancy we don't do the\nmissing value treatment because it's a\nTarget variable so while fitting the\nmodel while building the machine\nlearning model model for Target variable\nwe don't do any treatment like missing\nvalue treatment or the outl treatment we\ndon't do that because it's a Target\nvariable so it will become the\nartificial data if you do the any\ntreatment on the target variable so we\nwill skip this life expectancy for\nremaining all the columns we we will do\nthe outl treatment so while doing outl\ntreatment we have to decide whether to\nfill it with the median or the more so\nfor the categorical or the discrete\nvalues we need to fill it with the mode\nand for the numerical column we can fill\nit with the median or the mode that is\nwe need to decide based on the data the\ntype of data that's depends on defers to\ndata to data so that's a individual\nchoice so here I have to use first I\nwill show it in median mode mq2 then I\nwill use this can imputer to fill the\nmissing value so for some column I will\ndo the medium on mode so so I will need\nI need to select the column first so I\nhave to select the adult mortality first\nso I will write the for Loop so\nfor each column for each element in this\nso I have to create the list of column\nso first I will select this adult\nmorality so first I will select the\nnumerical column which which I have to\nfill it with the Medan so I will use the\nthe not infant the percentage ofte B the\nL the BMI so I can use the BMI body mass\nindex control C then I will use it over\nhere then I can use\nthe PO the number of Po let's check for\nthe let's check for it the type so for\npolio we have discrete or numeric so for\npolio I can see here it's\na yeah it's\nnumerical so I can fill it with the me\nor mode Medan Med or\nmean\nso then I will choose the income\ncomposition so this column contrl C and\nand here I fill\nwith so I have these three columns so I\nwill impute this with the\nmedian so resources I will import UT\nthis with the median\nso DF\ndot that\ncolumn DF of I means that\ncolumn and I will use the fill end to\nfill the value I have we have to will\nuse use the fill na so I will use the\nfill NA means is there any to fill it\nwith the we have to give the parameter\nas the which value we want to fill so I\nwant to fill it with the median of\nthat so I will use DF of I median so\nthis will give the median of this column\nso median and I want to make this change\nin the actual data so I will use in\nplace is equal to true so if I run this\ncode let's let let's see what will we\nget so if if now I check for the missing\nvalue in the life in\nthe which column we did BMI and Vol like\nthat if I do now we can see it over here\nin the income composition we have Zer\nnull value we just imputed and in BMI\nalso we have Zer null value we just\nimported imputed then in the polio like\nover here the polio we have zero Val we\njust imputed so so by this we impute the\ndata with the median for the numerical\nso if I want to impute the categorical\ndata so here I have the category only to\ncountry and the status I have zero null\nvalue no need to impute if I want to\nimpute if there is any missing value in\nthis category letter I have to includ\nwith the mode just in the place of the\nmedian first select the column in the\nplace of the median we have to use a\nmode and the first mode because the\ncolumn can have the more than one mode\nvalue so we have to use a mode of the\ncolumn then use the zero index so it\nwill impute the missing value with the\nmode of that column so that's how I can\nimpute with the missing Val\nso I have another\none this module I have to imput the\nmissing value that is from SK scalar if\nI use that I can do the missing value\ntreatment very easily without any\nconfusion and so first I need to import\nthat from Escalon from Escalon\ndo from escalar do\nimpute from escaler do impute\nimport k nni\ni means\nimputer so after that I have to\ninitialize to\ninut so inut is equal to K n n i sorry\niuter so if I initialize this so after\nthat I got one error imputer\nsorry\nyeah so after that I will use this to\nfill the missing value so after that\njust need to use the column so I will\nuse all the column so\nfor I\nin so I will use all the columns like\nfor I in DF\nof do\nselect D\ntypes include all the numerical column\nso I will use include\nnumber so include number and I want the\ncolumn of this so I will use\ncolumn so so after\nthat inut I have just imported inut so\nDF of I means that column is equal\nto\ninut dot we have to use\nfit\ntransform fit transform that column DF\nof I so so let's see what will we get\nfor R this I got one error the\nimpute fit transform DF of I I need to\ngive it in a\ntble braet so if I give\nthis fit\ntransform so fit transform that column\nso here we need to give it in a double\nsquare bracket so DF of I so if I impute\nthis let's see yeah that is imputed all\nthe numerical column with the so now I\ncheck for the null value I don't have\nany null value because I just imputed\nthat by using K NN imputer so how this\nwork is it will take the average of the\nnearest neighbor so K NN means the\nnearest neighbor it it will impute it\nwill take the average of the nearest the\nvalue and it will fill it that missing\nvalue with the nearest average of that\nnearest values so this is how the K\nimputer works so this is Al algorithm\nfor the filling the missing value so\nit's the best in the industry to fill\nthe missing value of the numerical\ncolumn so we can use this to fill the\nmissing value or if if the number of\nmissing value is less we can use also\nuse the median or the mode method we can\nalso use the K imputer from the escalar\nso anything is okay\nso so this is how we can deal with the\nmissing value so now I check for the\nmissing value of my data it will have no\nmissing value we imputed all the missing\nvalue of the categorical and numerical\nvariable so the missing value treatment\nis done after the missing value\ntreatment we have the step six of the\noutlier treatment so what is outlier so\noutlier is the any extreme value whether\nit's an upper side or lower side of the\nr that's called as outl so if it is if\noutlier is present in the data that's\nthat's not useful for the building a\nmodel so with outlier the model will\nwill won't give the best result so we\nwill do the outlier treatment we will\ncap the outliers with the whiskers or\nany values based on our decision so we\nwill fit the\nmodel after doing the outl treatment so\nin the pre-processing of data we need to\ndo the outlier treatment so for outlier\ntreatment we need need to decide based\non the our data we need to decide\nwhether to do outl treatment or not to\ndo outl treatment it's ented up to us to\nwhile building\nthe model so here in this video I will\nshow how to do the outl treatment so for\noutl treatment we first need to need to\nFirst need to Define one function to get\nthe wiers so after that so in the Box\nslot we analyzed that there is a outlier\nin the data so here we can see that like\nin the here we can see that there is a\noutl in the population there is the\noutliers in the thinness many column we\nhave the outliers in the above side and\nthe Lower Side so we can cap this outl\nto the upper whisker so this is our\nupper whisker so we can cap this all\nthis value to this upper whisker and if\nthere is any values below this lower\nwhisker we can cap that to the lower\nwhisker means we we replace these values\nwith the upper whisker value so if it is\n20 is more than upper whisker we fill it\nwith the like 70 like that we will do\nthe outlier treatment so let's see how\nto do the outlier treatment for the data\nso and while doing the outlier treatment\nwe need\nto keep it in mind that the outl\ntreatment is done only for the\ncontinuous numerical data it won't we\ndon't the outl treatment for the Target\nvariable we don't do the outl treatment\nfor the categorical and discrete\nvariable so we need to First figure it\nout which are the columns are the\nnumerical continuous numerical data for\nthat column we we will do the outl\ntreatment so for in this in this data\nfor example for the GDP it's a\ncontinuous data we can do the outl\ntreatment for the GDP then here we have\nthe more outl treatment so we can decide\nto not to do the outl\ntreatment like that we have to decide\nwhether to do the outl treatment or not\nso so for this video I will show how to\ndo the outl treatment so for total\nexpenditure I will do the outl treatment\nfor the polio I don't do the outlier\ntreatment as it is discrete show the\ndiscrete so for the BMI you don't have\nany outliers for\nthe isless it have the many outlier so I\ndon't do the outlier treatment for for\nthis like that I have to decide whether\nto do the outl treatment or not to do\nthe outl treatment but the outl\ntreatment is done only for the\ncontinuous numerical column so I will do\nthe outl treatment for the GDP and the\nschooling I don't do the outlet\ntreatment then for the income\ncomposition resource it have only one\nOutlet so no need to do the outl\ntreatment for that so for thinness 5 to\n9 years so first need to check for the\nhow the data is present in that column\nso for thinness it have the yeah\nfractional data so I can do the outl\ntreatment for thinness of 5 to 9 and\nlike that so I will select the three\ncolumn for do outl treatment so just to\nshow how to do the outl treatment so for\noutline treatment I have to first get\nthe upper RAR and lower rare for that I\nhave to Define\none function that is whisker function I\ncan give the name as whisker so in that\nI will will give the input input as\ncolumn so first I need to get the\nquartile so in order to find the upper\nscare lower whare I need to First need\nto get the quel so I will use the q1\nand Q3 that is 25 percentile and the 75\npercentile of the data so is equal to NP\ndot NP do percentile so this is a\nfunction percen function of\nthat Nary so np. percentile that column\nthen I want to get the 25 percentile and\nthe 75 percentile of this column so\nwhatever the column I give to\nthis function it will give the it will\ncalculate first the 25 percentile and\n75% and asign it to this after that I\nwill found the inter qual range so inter\nqual range means Q3 means 75\npercentile that is my Q3 minus q1 Q3\nminus q1 after that by after getting the\nIQR I can find the lower whare so the\nlower Vare is equal\nto Q\nq1 minus\n1.5\ninto\nIQR 1.5 into\nIQR then the upper\nVare the upper V sare is equal to\nQ3\nplus\n1.5\ninto IQR so after calculating this I\nwant to return this so return\nreturn so return\nlower risker first\nthen upper whisker so let's check\nwhether this execute oh sorry here I\nneed to give it that so\nreturn so if I check for the whisker\nwhether I get the upper whisker lowerer\nwhisker so whisker all first I need to\nget the column information DF\noff so I will get the columns of\neach columns\nso yeah I have the column so I will give\nthe GDP where is my GDP yeah this column\nif I give this\ncolumn so DF of this\ncolumn let's see what will we get we got\nan\nerror course model number have the new\npercentage so sorry percent okay okay\nokay percent n is missing here so I need\nto yeah it it will give the lower\nwhisker and the upper whisker of this\nGDP column so this is how I can get the\nlower whisker and the upper whisker of\nthis so after the lower RAR upper RAR I\nneed\nto fill it with that so this is I just\nuse it to check that whether it is\nworking or not so after after that I\nwill fill the lower RAR upper RAR of\nthis three column whichever I selected\nso for I in that column so first I will\nselect that\nGDP first one is\nGDP so control\nV then the total\nexpenditure contrl C and contrl V then I\nwill do the outlet treatment for\nthis\nthinness and this this column so contrl\nC and control V\nso let's take it over here so I have\nthis 1 2 3 four columns to do the\noutlier treatment so after that here I\nfirst need to get the lower whisker\nthat lower whisker comma upper whisker\nof that column so for getting lower\nwhare upper I already defined that\nvisker function so I will use that\nvisker function over here that and I\nwill use the DF\nof I over\nhere means that column then after that I\nwill use the fill that\nso I will use T of\nof I is equal\nto NP dot means nump dot I will use\nwhere to fill it so NP dot sorry NP do\nwhere NP do where DF of I means that\ncolumn check DF of I is lesser than low\nlower whisker fill it with lower whisker\notherwise keep it as it is so this code\ntells like that so if DF\ndo DF do\nI is equal to NP dot\nwhere DF of\nI is greater than upper\nwhisker fill it with upper whisker\notherwise keep it as it is so this is\nabout this code so if I run this code\nyeah\nthe lower whisker and upper whisker is\nkep the outl is capped with the lower\nand upper whisker so if I now check for\nthat box flot again so let's check for\nthe box flot so I will use this\ncolumns to check the box slot so control\nV so then\nSNS sorry\nSNS do\nboxplot I will directly use that DF\nof I then PLT\ndot\nshow pl. show let's see what will we get\nyeah we got a box slot with no outlier\nbefore we had the outliers now we don't\nhave outl because we did just did the\noutl treatment for this four column so\nthis is how we can do the outl treatment\nso the outl treatment decision is based\non the data it defers to data to data we\nneed to decide whether to do the outl\ntreatment or not so if I want to do the\noutl treatment we need to decide whether\nto cap to the upper whisker lower\nwhisker or we need to select in specific\nvalue to forfill that so it's based on\nthe data it depends to data to data so\nthis is how we can do the ideally do the\noutlier treatment by using that upper\nRAR and lower RAR so this is about\noutlier treatment so then we have the\nduplicates and garbage value so we\nalready find find in the S check we\ndon't have any duplicates value so we\ndon't have any garbage value also so we\ndo we skip this if I have any duplicate\nvalue we need to delete that column by\nusing drop duplicates to drop the\nduplicates value we have to use DF dot\nso here DF\ndot\ndrop\nduplicates DF do drop duplicates if any\nduplicates is there that will be deleted\nso in this data struct data frame we\ndon't have any duplicates so no need to\nworry about duplicates if you have the\nany garbage value we have to change it\nto the Medan or mode of that column so\nthat's how we will do the duplicat\ntreatment and the garbage treatment\ngarbage value treatment so so at last\nlet's see how to do encoding of data so\nencoding means for fitting the model\nfitting the data into model we need all\nthe columns in the form of numerical so\nin my data in my data we have some\ncolumns\nlike like here country and the status it\nis in the object data form so we need to\nconvert that object data into numerical\nthat is called as encoding so converting\nthe object into\nnumerical that is called as encoding so\nwe can do encoding of data in two ways\nthat is one hot encoding and label\nencoding one hard means we will create a\ndummies for the each category and we\nwill do tamies and we will also use the\nlabel encoding for some categorical data\nwhich can be the categorical variable\nwhich can be ordinal we can use the\nlabel encoding otherwise we will use\nthe tamies one H encoding so for\ncreating D is we have the easy technique\nusing the pandas pd. get Dumis we can\nuse\nthe use that module to create a Dumis so\nI have to use the pd. get to create a\ntmy variable so I will use like PD\ndot get\ndamis so pd. get damis within the\nparenthesis I have to give the data the\ndata is my data frame then I have to\ngive the column for which I need to\ncreate adamy the column is I have two\ncolumn like the country the first one is\ncountry then the second column is the\nstatus so\nstatus status then I have to use\ndrop\nfirst is equal\nto true it will delete or drop the first\nlevel so it will it won't give the any\nmulticolinearity problem if I drop the\nfirst level of the data so by using pd.\nget D of this data and I have to give\nthe column for which I need to create\ndumy in my present data I need to create\na dummies for the country and the status\nbecause that only\ntwo columns with the categorical\nfeatures so if I run this I have one\nproblem that is\nCallum sorry I'm missing columns so if I\ngive it has has created a dummy for each\ncountry like country ugu country like\nthat it will create a dummies for all\nthe country so this is how I can create\na dummy in a one single code that is pd.\nget tamies and I have to give the tamies\nso if I use the categorical ordinal\ncolumn I have to use the replace\nfunction to replace that the true or\nfalse we can use the label encoding by\nusing replace function I can give the\nlabel like 1 2 3 4 like that so in this\ndata I don't have any columns which can\nI do label encoding so I just did\na one hot encoding of this data by using\npd. G so this is how and I have to store\nit in a dumy another data so this will\ngive the the new data that is dummy so\nif I now check check for this dumy it\nwill give all the information in the\nform of numerical so I don't have any\ncategorical feature here so now my data\nis ready to fit in the model after this\nstage I can fit this data\ninto model whether it's regression model\nby using a scalar or stats model I can\nuse this data to fil the model so this\nis about this video in this video we\nlearned about many things like\npre-processing of data we learned about\nhow to import necessary libraries how to\nread a data set and how to do a sanity\ncheck to find missing value outlier\ngarbage value and how to do explorat\ndata analysis to understand the data\nbetter like uh descriptive statistics\nand we will also visualize the data for\nhistogram box flot scatter flot to\nunderstand the relationship and after\nthat we did the missing value treatment\nby mean media mode method and K nni\nimputer method then we did the outlier\ntreatment by upper risk and lower risk\nimpute then we did the duplicates and\ngarbage value treatment then we did the\nencoding of data so this is about this\nvideo in this video we learned about the\npre-processing of data so thank you for\nwatching stay tuned for the next\nvideo\n",
  "words": [
    "welcome",
    "new",
    "video",
    "video",
    "going",
    "see",
    "data",
    "building",
    "machine",
    "learning",
    "model",
    "fit",
    "data",
    "machine",
    "learning",
    "model",
    "need",
    "data",
    "order",
    "see",
    "whether",
    "missing",
    "value",
    "whether",
    "outliers",
    "whether",
    "duplicates",
    "value",
    "present",
    "data",
    "either",
    "garbage",
    "values",
    "present",
    "data",
    "need",
    "understand",
    "data",
    "first",
    "building",
    "machine",
    "learning",
    "model",
    "need",
    "check",
    "relationship",
    "data",
    "data",
    "related",
    "build",
    "best",
    "machine",
    "learning",
    "model",
    "purpose",
    "need",
    "data",
    "video",
    "going",
    "see",
    "step",
    "step",
    "data",
    "fitting",
    "data",
    "machine",
    "learning",
    "model",
    "video",
    "going",
    "cover",
    "topics",
    "like",
    "steps",
    "data",
    "start",
    "importing",
    "necessary",
    "libraries",
    "read",
    "data",
    "set",
    "sanity",
    "check",
    "data",
    "exploratory",
    "data",
    "analysis",
    "understand",
    "data",
    "better",
    "missing",
    "value",
    "treatments",
    "data",
    "outli",
    "treatments",
    "check",
    "duplicates",
    "garbage",
    "value",
    "treatment",
    "duplicates",
    "garbage",
    "value",
    "let",
    "see",
    "normalization",
    "normalization",
    "data",
    "needed",
    "last",
    "encoding",
    "categorical",
    "data",
    "fit",
    "model",
    "let",
    "see",
    "data",
    "step",
    "step",
    "first",
    "import",
    "necessary",
    "library",
    "import",
    "pandas",
    "pd",
    "import",
    "numpy",
    "library",
    "p",
    "visualization",
    "libraries",
    "like",
    "cbor",
    "math",
    "plot",
    "li",
    "music",
    "input",
    "math",
    "plot",
    "cp",
    "plts",
    "sorry",
    "math",
    "plotly",
    "p",
    "plot",
    "plt",
    "first",
    "import",
    "libraries",
    "yeah",
    "libraries",
    "imported",
    "step",
    "two",
    "read",
    "data",
    "data",
    "frame",
    "pd",
    "read",
    "data",
    "csv",
    "read",
    "csv",
    "copy",
    "name",
    "data",
    "data",
    "set",
    "life",
    "expectancy",
    "data",
    "read",
    "store",
    "variable",
    "called",
    "df",
    "read",
    "data",
    "df",
    "data",
    "frames",
    "check",
    "head",
    "data",
    "df",
    "head",
    "df",
    "head",
    "show",
    "top",
    "five",
    "information",
    "like",
    "country",
    "year",
    "status",
    "life",
    "expectancy",
    "adult",
    "mod",
    "ity",
    "infant",
    "alcohol",
    "percentage",
    "expenditure",
    "htis",
    "pness",
    "portfolio",
    "like",
    "many",
    "columns",
    "data",
    "set",
    "basically",
    "data",
    "set",
    "life",
    "expectancy",
    "column",
    "predict",
    "life",
    "expectancy",
    "data",
    "video",
    "covering",
    "topic",
    "data",
    "building",
    "model",
    "step",
    "need",
    "taken",
    "fit",
    "data",
    "model",
    "read",
    "head",
    "check",
    "tail",
    "shows",
    "bottom",
    "five",
    "df",
    "dot",
    "df",
    "sorry",
    "df",
    "tail",
    "show",
    "bottom",
    "five",
    "record",
    "bottom",
    "five",
    "record",
    "step",
    "three",
    "check",
    "step",
    "three",
    "sanity",
    "check",
    "data",
    "sanity",
    "check",
    "means",
    "sanity",
    "check",
    "like",
    "identifying",
    "missing",
    "value",
    "identify",
    "outliers",
    "garbage",
    "values",
    "data",
    "duplicates",
    "values",
    "data",
    "set",
    "start",
    "check",
    "checking",
    "shape",
    "data",
    "shape",
    "means",
    "many",
    "rows",
    "columns",
    "simple",
    "df",
    "shape",
    "df",
    "shape",
    "give",
    "shape",
    "data",
    "shape",
    "means",
    "2009",
    "38",
    "rows",
    "22",
    "columns",
    "data",
    "data",
    "data",
    "frame",
    "2938",
    "rows",
    "22",
    "columns",
    "check",
    "information",
    "df",
    "info",
    "give",
    "df",
    "info",
    "give",
    "information",
    "like",
    "data",
    "types",
    "column",
    "information",
    "give",
    "overall",
    "information",
    "like",
    "entries",
    "see",
    "2938",
    "whatever",
    "seen",
    "shape",
    "information",
    "get",
    "information",
    "2938",
    "entries",
    "indexing",
    "0",
    "2937",
    "total",
    "22",
    "columns",
    "column",
    "information",
    "respect",
    "null",
    "values",
    "types",
    "see",
    "columns",
    "total",
    "20",
    "1",
    "22",
    "columns",
    "represent",
    "null",
    "counts",
    "data",
    "see",
    "total",
    "2938",
    "row",
    "full",
    "values",
    "part",
    "observe",
    "missing",
    "values",
    "also",
    "show",
    "data",
    "types",
    "column",
    "countryes",
    "data",
    "type",
    "object",
    "int",
    "form",
    "like",
    "column",
    "information",
    "present",
    "al",
    "also",
    "show",
    "data",
    "frame",
    "16",
    "column",
    "float",
    "data",
    "type",
    "four",
    "column",
    "int",
    "data",
    "type",
    "two",
    "columns",
    "object",
    "data",
    "type",
    "object",
    "country",
    "second",
    "one",
    "status",
    "sanity",
    "check",
    "sanity",
    "check",
    "find",
    "missing",
    "value",
    "data",
    "frame",
    "find",
    "missing",
    "value",
    "finding",
    "missing",
    "value",
    "need",
    "check",
    "df",
    "eal",
    "df",
    "eal",
    "sum",
    "fitting",
    "model",
    "mention",
    "make",
    "sure",
    "data",
    "clean",
    "missing",
    "value",
    "need",
    "data",
    "need",
    "check",
    "missing",
    "value",
    "isal",
    "sum",
    "give",
    "counts",
    "missing",
    "value",
    "column",
    "find",
    "country",
    "year",
    "status",
    "missing",
    "value",
    "life",
    "expectancy",
    "adult",
    "modality",
    "find",
    "10",
    "10",
    "like",
    "alcohol",
    "194",
    "missing",
    "value",
    "hepatitis",
    "b",
    "550",
    "three",
    "missing",
    "value",
    "like",
    "find",
    "count",
    "missing",
    "value",
    "column",
    "want",
    "percentage",
    "missing",
    "value",
    "missing",
    "value",
    "treatment",
    "need",
    "decide",
    "whether",
    "missing",
    "value",
    "treatment",
    "delete",
    "column",
    "delete",
    "rows",
    "missing",
    "value",
    "need",
    "decide",
    "based",
    "many",
    "criteria",
    "based",
    "number",
    "missing",
    "value",
    "number",
    "missing",
    "value",
    "huge",
    "column",
    "decided",
    "delete",
    "column",
    "unless",
    "imp",
    "filling",
    "value",
    "first",
    "need",
    "find",
    "uh",
    "percentage",
    "missing",
    "value",
    "percentage",
    "get",
    "easy",
    "inferences",
    "decide",
    "whether",
    "missing",
    "value",
    "treatment",
    "let",
    "percentage",
    "different",
    "cell",
    "first",
    "copy",
    "finding",
    "percentage",
    "easy",
    "crl",
    "c",
    "crl",
    "v",
    "divide",
    "df",
    "shape",
    "want",
    "one",
    "number",
    "df",
    "dot",
    "shape",
    "first",
    "give",
    "counts",
    "overall",
    "value",
    "multiply",
    "100",
    "give",
    "percentage",
    "missing",
    "value",
    "get",
    "part",
    "zero",
    "part",
    "3",
    "overall",
    "data",
    "compared",
    "overall",
    "data",
    "3",
    "missing",
    "value",
    "uh",
    "hyptis",
    "p",
    "18",
    "missing",
    "value",
    "compared",
    "overall",
    "data",
    "gdp",
    "15",
    "minority",
    "side",
    "50",
    "missing",
    "value",
    "percentage",
    "missing",
    "value",
    "50",
    "decide",
    "delete",
    "column",
    "problem",
    "missing",
    "value",
    "50",
    "overall",
    "count",
    "understand",
    "missing",
    "value",
    "checking",
    "missing",
    "value",
    "treatment",
    "missing",
    "value",
    "treatment",
    "missing",
    "val",
    "sanity",
    "che",
    "understand",
    "whether",
    "missing",
    "value",
    "step",
    "step",
    "missing",
    "value",
    "treatment",
    "create",
    "confusion",
    "checking",
    "missing",
    "value",
    "check",
    "duplicates",
    "duplicates",
    "data",
    "set",
    "checking",
    "duplicate",
    "need",
    "unique",
    "value",
    "unique",
    "value",
    "data",
    "set",
    "check",
    "duplicate",
    "otherwise",
    "duplicate",
    "checking",
    "useless",
    "besides",
    "unique",
    "value",
    "duplicates",
    "may",
    "possible",
    "unique",
    "value",
    "also",
    "duplicate",
    "problem",
    "first",
    "check",
    "purpose",
    "df",
    "duplicated",
    "df",
    "duplicated",
    "sum",
    "give",
    "counts",
    "duplicate",
    "see",
    "give",
    "zero",
    "means",
    "zero",
    "duplicates",
    "valu",
    "done",
    "done",
    "missing",
    "value",
    "number",
    "missing",
    "value",
    "also",
    "done",
    "number",
    "duplicates",
    "check",
    "garbage",
    "value",
    "find",
    "garbage",
    "value",
    "data",
    "set",
    "one",
    "thing",
    "garbage",
    "value",
    "garbage",
    "value",
    "present",
    "column",
    "always",
    "form",
    "object",
    "data",
    "type",
    "uh",
    "example",
    "ear",
    "form",
    "int",
    "garbage",
    "val",
    "garbage",
    "val",
    "present",
    "special",
    "character",
    "within",
    "column",
    "data",
    "type",
    "show",
    "object",
    "single",
    "garbage",
    "value",
    "show",
    "object",
    "garbage",
    "value",
    "always",
    "form",
    "object",
    "check",
    "garbage",
    "value",
    "column",
    "data",
    "type",
    "object",
    "one",
    "loop",
    "count",
    "unique",
    "values",
    "object",
    "column",
    "find",
    "whether",
    "garbage",
    "value",
    "use",
    "loop",
    "df",
    "dot",
    "select",
    "types",
    "include",
    "object",
    "include",
    "equal",
    "objects",
    "garbage",
    "value",
    "object",
    "object",
    "column",
    "object",
    "columns",
    "want",
    "column",
    "value",
    "count",
    "df",
    "value",
    "count",
    "value",
    "counts",
    "print",
    "give",
    "count",
    "unique",
    "value",
    "garbage",
    "value",
    "find",
    "garbage",
    "value",
    "unique",
    "print",
    "def",
    "defer",
    "result",
    "print",
    "like",
    "separator",
    "want",
    "like",
    "10",
    "times",
    "print",
    "let",
    "see",
    "get",
    "get",
    "garbage",
    "values",
    "like",
    "country",
    "find",
    "afghanistan",
    "peru",
    "garbage",
    "value",
    "status",
    "developing",
    "find",
    "garbage",
    "value",
    "data",
    "set",
    "garbage",
    "value",
    "find",
    "show",
    "whether",
    "star",
    "special",
    "character",
    "show",
    "give",
    "count",
    "found",
    "garbage",
    "values",
    "need",
    "worry",
    "garbage",
    "values",
    "check",
    "garbage",
    "value",
    "check",
    "value",
    "count",
    "well",
    "unique",
    "value",
    "anyhow",
    "find",
    "find",
    "garbage",
    "value",
    "impute",
    "change",
    "garbage",
    "value",
    "median",
    "mode",
    "anything",
    "want",
    "make",
    "sure",
    "data",
    "set",
    "free",
    "garbage",
    "value",
    "check",
    "data",
    "exploration",
    "data",
    "analysis",
    "understand",
    "data",
    "set",
    "data",
    "data",
    "set",
    "understand",
    "data",
    "exploratory",
    "data",
    "analysis",
    "first",
    "start",
    "descriptive",
    "statistics",
    "order",
    "get",
    "descriptive",
    "statistics",
    "numerical",
    "column",
    "use",
    "df",
    "describe",
    "use",
    "code",
    "give",
    "descriptive",
    "statistics",
    "want",
    "transpose",
    "give",
    "give",
    "information",
    "like",
    "give",
    "counts",
    "mean",
    "standard",
    "devation",
    "minimum",
    "25",
    "percentile",
    "50",
    "percentile",
    "75",
    "percentile",
    "maximum",
    "understand",
    "number",
    "data",
    "distributed",
    "standard",
    "deviation",
    "data",
    "like",
    "many",
    "information",
    "read",
    "get",
    "know",
    "data",
    "presented",
    "distributed",
    "skilless",
    "data",
    "every",
    "information",
    "get",
    "get",
    "basic",
    "understanding",
    "data",
    "descriptive",
    "numerical",
    "column",
    "want",
    "descriptive",
    "statistics",
    "object",
    "column",
    "use",
    "df",
    "describe",
    "within",
    "bracket",
    "use",
    "include",
    "object",
    "use",
    "include",
    "object",
    "give",
    "descriptive",
    "statistics",
    "sorry",
    "describe",
    "dis",
    "dh",
    "df",
    "sorry",
    "df",
    "describe",
    "give",
    "descriptive",
    "statistic",
    "different",
    "numerical",
    "column",
    "information",
    "give",
    "counts",
    "number",
    "values",
    "unique",
    "values",
    "country",
    "column",
    "193",
    "unique",
    "value",
    "means",
    "total",
    "193",
    "countries",
    "information",
    "status",
    "two",
    "two",
    "values",
    "like",
    "developing",
    "developer",
    "inference",
    "descriptive",
    "statistics",
    "object",
    "column",
    "explor",
    "data",
    "analysis",
    "understanding",
    "distribution",
    "data",
    "understand",
    "distribution",
    "data",
    "visually",
    "use",
    "histogram",
    "use",
    "histogram",
    "drawing",
    "histogram",
    "numerical",
    "column",
    "use",
    "obviously",
    "loop",
    "column",
    "df",
    "dot",
    "select",
    "select",
    "simple",
    "code",
    "select",
    "data",
    "types",
    "sorry",
    "types",
    "include",
    "equal",
    "sorry",
    "include",
    "number",
    "want",
    "column",
    "numerical",
    "data",
    "type",
    "include",
    "number",
    "use",
    "column",
    "column",
    "want",
    "print",
    "sns",
    "cboard",
    "library",
    "use",
    "flot",
    "histogram",
    "cbo",
    "history",
    "flot",
    "cbo",
    "history",
    "flot",
    "within",
    "data",
    "data",
    "frame",
    "give",
    "xaxis",
    "means",
    "column",
    "numerical",
    "data",
    "type",
    "use",
    "plt",
    "show",
    "show",
    "every",
    "chart",
    "run",
    "code",
    "give",
    "result",
    "like",
    "want",
    "red",
    "color",
    "warning",
    "use",
    "import",
    "warnings",
    "import",
    "warnings",
    "warnings",
    "warnings",
    "filter",
    "warnings",
    "within",
    "use",
    "ignore",
    "get",
    "red",
    "color",
    "warning",
    "get",
    "histogram",
    "numerical",
    "column",
    "year",
    "see",
    "constantly",
    "distributed",
    "see",
    "left",
    "skilless",
    "data",
    "life",
    "expectancy",
    "adult",
    "modity",
    "right",
    "skewness",
    "infant",
    "de",
    "minimal",
    "zero",
    "values",
    "zero",
    "alcohol",
    "like",
    "numerical",
    "column",
    "distribution",
    "chart",
    "get",
    "distribution",
    "understand",
    "data",
    "distributed",
    "data",
    "set",
    "get",
    "histogram",
    "visually",
    "explorating",
    "data",
    "exploratory",
    "data",
    "analysis",
    "histogram",
    "understand",
    "distribution",
    "data",
    "also",
    "box",
    "fl",
    "identify",
    "outlier",
    "datas",
    "identify",
    "whether",
    "outlier",
    "datas",
    "histogram",
    "box",
    "blot",
    "copy",
    "code",
    "change",
    "little",
    "bit",
    "filter",
    "types",
    "use",
    "box",
    "flot",
    "use",
    "box",
    "flot",
    "want",
    "show",
    "use",
    "box",
    "flot",
    "get",
    "box",
    "flot",
    "numerical",
    "column",
    "year",
    "column",
    "box",
    "plot",
    "life",
    "expectancy",
    "box",
    "uh",
    "outliers",
    "lower",
    "side",
    "adult",
    "mortality",
    "outliers",
    "upper",
    "side",
    "data",
    "infant",
    "major",
    "outliers",
    "alcohol",
    "also",
    "outliers",
    "percentage",
    "expenditure",
    "many",
    "column",
    "outlier",
    "see",
    "boxlot",
    "information",
    "use",
    "box",
    "blot",
    "way",
    "understand",
    "data",
    "understand",
    "distribution",
    "understand",
    "outlier",
    "data",
    "order",
    "scatter",
    "flot",
    "scatter",
    "flot",
    "basically",
    "analysis",
    "used",
    "get",
    "whether",
    "relationship",
    "data",
    "relationship",
    "target",
    "variable",
    "independent",
    "variable",
    "fitting",
    "model",
    "cover",
    "next",
    "video",
    "explain",
    "simply",
    "order",
    "build",
    "model",
    "relationship",
    "target",
    "variable",
    "independent",
    "variable",
    "explor",
    "data",
    "anal",
    "check",
    "whether",
    "positive",
    "relationship",
    "negative",
    "relationship",
    "data",
    "data",
    "set",
    "example",
    "data",
    "set",
    "life",
    "expectancy",
    "dependent",
    "factor",
    "columns",
    "independent",
    "factor",
    "using",
    "factor",
    "fit",
    "model",
    "predict",
    "life",
    "expectance",
    "check",
    "relationship",
    "use",
    "scatter",
    "flot",
    "check",
    "relationship",
    "first",
    "get",
    "column",
    "columns",
    "columns",
    "want",
    "get",
    "numerical",
    "column",
    "first",
    "types",
    "df",
    "select",
    "types",
    "include",
    "equal",
    "number",
    "give",
    "give",
    "column",
    "numerical",
    "column",
    "want",
    "get",
    "columns",
    "use",
    "columns",
    "get",
    "columns",
    "using",
    "flot",
    "scatter",
    "plot",
    "scatter",
    "plot",
    "use",
    "like",
    "sns",
    "use",
    "loop",
    "getting",
    "scatter",
    "flot",
    "every",
    "column",
    "element",
    "use",
    "column",
    "first",
    "select",
    "column",
    "paste",
    "like",
    "life",
    "expectancy",
    "target",
    "variable",
    "remove",
    "okay",
    "remove",
    "put",
    "column",
    "data",
    "want",
    "sns",
    "means",
    "scatter",
    "flot",
    "cbor",
    "library",
    "scatter",
    "flot",
    "scatter",
    "flo",
    "data",
    "data",
    "frame",
    "x",
    "choose",
    "column",
    "use",
    "fixed",
    "column",
    "life",
    "expectancy",
    "target",
    "variable",
    "create",
    "scatter",
    "flot",
    "combination",
    "like",
    "year",
    "life",
    "expectancy",
    "adult",
    "mortality",
    "life",
    "expectancy",
    "infant",
    "deaths",
    "life",
    "expect",
    "like",
    "create",
    "scatter",
    "flot",
    "combination",
    "plt",
    "show",
    "let",
    "see",
    "get",
    "result",
    "yeah",
    "get",
    "scatter",
    "flot",
    "combination",
    "like",
    "life",
    "expectancy",
    "year",
    "life",
    "expectancy",
    "adult",
    "mortality",
    "negative",
    "relationship",
    "also",
    "showing",
    "relationship",
    "relationship",
    "like",
    "way",
    "find",
    "relationship",
    "check",
    "relationship",
    "outl",
    "treatment",
    "find",
    "relationship",
    "better",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "otherwise",
    "outl",
    "treatment",
    "fit",
    "model",
    "first",
    "outl",
    "treatment",
    "check",
    "result",
    "model",
    "outl",
    "treatment",
    "build",
    "model",
    "like",
    "process",
    "go",
    "see",
    "relationship",
    "understand",
    "data",
    "showing",
    "relationship",
    "interpret",
    "scatter",
    "flot",
    "correlation",
    "hit",
    "maps",
    "understand",
    "correlation",
    "dat",
    "use",
    "hte",
    "df",
    "dot",
    "use",
    "df",
    "core",
    "give",
    "correlation",
    "matrix",
    "good",
    "string",
    "yeah",
    "select",
    "numeric",
    "column",
    "c",
    "use",
    "control",
    "v",
    "dot",
    "core",
    "let",
    "see",
    "yeah",
    "got",
    "correlation",
    "matrix",
    "numerical",
    "column",
    "give",
    "correlation",
    "see",
    "correlation",
    "data",
    "ranges",
    "0",
    "1",
    "50",
    "show",
    "strong",
    "correlation",
    "less",
    "left",
    "50",
    "show",
    "correlation",
    "data",
    "want",
    "flot",
    "chart",
    "use",
    "heat",
    "map",
    "flot",
    "use",
    "sns",
    "heat",
    "map",
    "data",
    "select",
    "first",
    "store",
    "variable",
    "store",
    "yes",
    "give",
    "data",
    "yes",
    "flot",
    "let",
    "see",
    "get",
    "yeah",
    "got",
    "heat",
    "map",
    "want",
    "values",
    "hit",
    "map",
    "use",
    "equal",
    "true",
    "use",
    "get",
    "values",
    "hit",
    "map",
    "want",
    "increase",
    "size",
    "use",
    "pip",
    "plot",
    "figure",
    "fig",
    "size",
    "figure",
    "size",
    "equal",
    "let",
    "give",
    "15",
    "comma",
    "15",
    "music",
    "wrong",
    "figure",
    "fix",
    "size",
    "sorry",
    "give",
    "bracket",
    "give",
    "bracket",
    "let",
    "see",
    "yeah",
    "got",
    "heat",
    "map",
    "proper",
    "size",
    "understand",
    "correlation",
    "data",
    "set",
    "many",
    "column",
    "many",
    "information",
    "get",
    "correlation",
    "means",
    "color",
    "represent",
    "high",
    "correlation",
    "find",
    "correlation",
    "data",
    "part",
    "8",
    "correlation",
    "minus",
    "24",
    "correlation",
    "minus",
    "12",
    "correlation",
    "alcohol",
    "infant",
    "death",
    "get",
    "inferences",
    "correlation",
    "means",
    "relationship",
    "data",
    "go",
    "column",
    "information",
    "like",
    "color",
    "find",
    "correlation",
    "data",
    "correlation",
    "good",
    "assume",
    "predict",
    "model",
    "give",
    "better",
    "result",
    "relationship",
    "data",
    "use",
    "correlation",
    "matrix",
    "using",
    "hit",
    "maps",
    "exploratory",
    "data",
    "analysis",
    "till",
    "covered",
    "like",
    "check",
    "sity",
    "check",
    "shape",
    "check",
    "shape",
    "data",
    "get",
    "information",
    "data",
    "understand",
    "columns",
    "information",
    "data",
    "types",
    "get",
    "also",
    "check",
    "missing",
    "value",
    "missing",
    "value",
    "using",
    "df",
    "e",
    "null",
    "sum",
    "find",
    "percentage",
    "missing",
    "value",
    "find",
    "duplicates",
    "saw",
    "garbage",
    "value",
    "using",
    "value",
    "count",
    "exploratory",
    "data",
    "analysis",
    "describe",
    "data",
    "descriptive",
    "statistics",
    "numerical",
    "object",
    "columns",
    "flot",
    "data",
    "understand",
    "distribution",
    "use",
    "histogram",
    "understand",
    "outlier",
    "data",
    "use",
    "box",
    "flot",
    "get",
    "rel",
    "relationship",
    "use",
    "scatter",
    "flot",
    "heat",
    "heat",
    "map",
    "correlation",
    "matrix",
    "sanity",
    "check",
    "explorat",
    "data",
    "anal",
    "come",
    "step",
    "five",
    "missing",
    "value",
    "treatment",
    "sanity",
    "check",
    "find",
    "missing",
    "value",
    "data",
    "step",
    "treat",
    "missing",
    "value",
    "trat",
    "missing",
    "value",
    "sense",
    "fill",
    "median",
    "mode",
    "use",
    "algorithm",
    "fit",
    "like",
    "imputer",
    "fill",
    "missing",
    "value",
    "way",
    "get",
    "data",
    "set",
    "without",
    "missing",
    "value",
    "necessity",
    "fitting",
    "data",
    "model",
    "need",
    "give",
    "model",
    "without",
    "missing",
    "value",
    "missing",
    "values",
    "decision",
    "missing",
    "values",
    "depend",
    "various",
    "factor",
    "let",
    "figure",
    "let",
    "see",
    "missing",
    "value",
    "treatment",
    "missing",
    "value",
    "treatment",
    "option",
    "like",
    "fill",
    "median",
    "mean",
    "mode",
    "data",
    "column",
    "also",
    "another",
    "option",
    "module",
    "k",
    "imputer",
    "fill",
    "missing",
    "value",
    "work",
    "numerical",
    "column",
    "categorical",
    "column",
    "used",
    "fill",
    "mode",
    "use",
    "use",
    "median",
    "mode",
    "mean",
    "method",
    "inut",
    "data",
    "use",
    "k",
    "inputter",
    "fill",
    "missing",
    "value",
    "numerical",
    "column",
    "explain",
    "video",
    "first",
    "want",
    "impute",
    "missing",
    "first",
    "want",
    "check",
    "missing",
    "value",
    "column",
    "df",
    "su",
    "already",
    "check",
    "sanity",
    "check",
    "missing",
    "value",
    "count",
    "missing",
    "value",
    "get",
    "missing",
    "value",
    "life",
    "expectancy",
    "adult",
    "mortality",
    "life",
    "expectancy",
    "adult",
    "mortality",
    "missing",
    "value",
    "life",
    "expectance",
    "mortality",
    "column",
    "like",
    "uh",
    "numerical",
    "discrete",
    "column",
    "like",
    "life",
    "expectance",
    "full",
    "value",
    "continuous",
    "data",
    "fill",
    "median",
    "mean",
    "discrete",
    "variable",
    "put",
    "fill",
    "mode",
    "categorical",
    "column",
    "missing",
    "value",
    "fill",
    "mode",
    "let",
    "see",
    "fill",
    "missing",
    "value",
    "need",
    "get",
    "column",
    "get",
    "life",
    "expectancy",
    "adult",
    "mortality",
    "life",
    "expectancy",
    "missing",
    "value",
    "treatment",
    "target",
    "variable",
    "fitting",
    "model",
    "building",
    "machine",
    "learning",
    "model",
    "model",
    "target",
    "variable",
    "treatment",
    "like",
    "missing",
    "value",
    "treatment",
    "outl",
    "treatment",
    "target",
    "variable",
    "become",
    "artificial",
    "data",
    "treatment",
    "target",
    "variable",
    "skip",
    "life",
    "expectancy",
    "remaining",
    "columns",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "decide",
    "whether",
    "fill",
    "median",
    "categorical",
    "discrete",
    "values",
    "need",
    "fill",
    "mode",
    "numerical",
    "column",
    "fill",
    "median",
    "mode",
    "need",
    "decide",
    "based",
    "data",
    "type",
    "data",
    "depends",
    "defers",
    "data",
    "data",
    "individual",
    "choice",
    "use",
    "first",
    "show",
    "median",
    "mode",
    "mq2",
    "use",
    "imputer",
    "fill",
    "missing",
    "value",
    "column",
    "medium",
    "mode",
    "need",
    "need",
    "select",
    "column",
    "first",
    "select",
    "adult",
    "mortality",
    "first",
    "write",
    "loop",
    "column",
    "element",
    "create",
    "list",
    "column",
    "first",
    "select",
    "adult",
    "morality",
    "first",
    "select",
    "numerical",
    "column",
    "fill",
    "medan",
    "use",
    "infant",
    "percentage",
    "ofte",
    "b",
    "l",
    "bmi",
    "use",
    "bmi",
    "body",
    "mass",
    "index",
    "control",
    "c",
    "use",
    "use",
    "po",
    "number",
    "po",
    "let",
    "check",
    "let",
    "check",
    "type",
    "polio",
    "discrete",
    "numeric",
    "polio",
    "see",
    "yeah",
    "numerical",
    "fill",
    "mode",
    "medan",
    "med",
    "mean",
    "choose",
    "income",
    "composition",
    "column",
    "contrl",
    "c",
    "fill",
    "three",
    "columns",
    "impute",
    "median",
    "resources",
    "import",
    "ut",
    "median",
    "df",
    "dot",
    "column",
    "df",
    "means",
    "column",
    "use",
    "fill",
    "end",
    "fill",
    "value",
    "use",
    "use",
    "fill",
    "na",
    "use",
    "fill",
    "na",
    "means",
    "fill",
    "give",
    "parameter",
    "value",
    "want",
    "fill",
    "want",
    "fill",
    "median",
    "use",
    "df",
    "median",
    "give",
    "median",
    "column",
    "median",
    "want",
    "make",
    "change",
    "actual",
    "data",
    "use",
    "place",
    "equal",
    "true",
    "run",
    "code",
    "let",
    "let",
    "let",
    "see",
    "get",
    "check",
    "missing",
    "value",
    "life",
    "column",
    "bmi",
    "vol",
    "like",
    "see",
    "income",
    "composition",
    "zer",
    "null",
    "value",
    "imputed",
    "bmi",
    "also",
    "zer",
    "null",
    "value",
    "imported",
    "imputed",
    "polio",
    "like",
    "polio",
    "zero",
    "val",
    "imputed",
    "impute",
    "data",
    "median",
    "numerical",
    "want",
    "impute",
    "categorical",
    "data",
    "category",
    "country",
    "status",
    "zero",
    "null",
    "value",
    "need",
    "impute",
    "want",
    "impute",
    "missing",
    "value",
    "category",
    "letter",
    "includ",
    "mode",
    "place",
    "median",
    "first",
    "select",
    "column",
    "place",
    "median",
    "use",
    "mode",
    "first",
    "mode",
    "column",
    "one",
    "mode",
    "value",
    "use",
    "mode",
    "column",
    "use",
    "zero",
    "index",
    "impute",
    "missing",
    "value",
    "mode",
    "column",
    "impute",
    "missing",
    "val",
    "another",
    "one",
    "module",
    "imput",
    "missing",
    "value",
    "sk",
    "scalar",
    "use",
    "missing",
    "value",
    "treatment",
    "easily",
    "without",
    "confusion",
    "first",
    "need",
    "import",
    "escalon",
    "escalon",
    "escalar",
    "impute",
    "escaler",
    "impute",
    "import",
    "k",
    "nni",
    "means",
    "imputer",
    "initialize",
    "inut",
    "inut",
    "equal",
    "k",
    "n",
    "n",
    "sorry",
    "iuter",
    "initialize",
    "got",
    "one",
    "error",
    "imputer",
    "sorry",
    "yeah",
    "use",
    "fill",
    "missing",
    "value",
    "need",
    "use",
    "column",
    "use",
    "column",
    "use",
    "columns",
    "like",
    "df",
    "select",
    "types",
    "include",
    "numerical",
    "column",
    "use",
    "include",
    "number",
    "include",
    "number",
    "want",
    "column",
    "use",
    "column",
    "inut",
    "imported",
    "inut",
    "df",
    "means",
    "column",
    "equal",
    "inut",
    "dot",
    "use",
    "fit",
    "transform",
    "fit",
    "transform",
    "column",
    "df",
    "let",
    "see",
    "get",
    "r",
    "got",
    "one",
    "error",
    "impute",
    "fit",
    "transform",
    "df",
    "need",
    "give",
    "tble",
    "braet",
    "give",
    "fit",
    "transform",
    "fit",
    "transform",
    "column",
    "need",
    "give",
    "double",
    "square",
    "bracket",
    "df",
    "impute",
    "let",
    "see",
    "yeah",
    "imputed",
    "numerical",
    "column",
    "check",
    "null",
    "value",
    "null",
    "value",
    "imputed",
    "using",
    "k",
    "nn",
    "imputer",
    "work",
    "take",
    "average",
    "nearest",
    "neighbor",
    "k",
    "nn",
    "means",
    "nearest",
    "neighbor",
    "impute",
    "take",
    "average",
    "nearest",
    "value",
    "fill",
    "missing",
    "value",
    "nearest",
    "average",
    "nearest",
    "values",
    "k",
    "imputer",
    "works",
    "al",
    "algorithm",
    "filling",
    "missing",
    "value",
    "best",
    "industry",
    "fill",
    "missing",
    "value",
    "numerical",
    "column",
    "use",
    "fill",
    "missing",
    "value",
    "number",
    "missing",
    "value",
    "less",
    "use",
    "also",
    "use",
    "median",
    "mode",
    "method",
    "also",
    "use",
    "k",
    "imputer",
    "escalar",
    "anything",
    "okay",
    "deal",
    "missing",
    "value",
    "check",
    "missing",
    "value",
    "data",
    "missing",
    "value",
    "imputed",
    "missing",
    "value",
    "categorical",
    "numerical",
    "variable",
    "missing",
    "value",
    "treatment",
    "done",
    "missing",
    "value",
    "treatment",
    "step",
    "six",
    "outlier",
    "treatment",
    "outlier",
    "outlier",
    "extreme",
    "value",
    "whether",
    "upper",
    "side",
    "lower",
    "side",
    "r",
    "called",
    "outl",
    "outlier",
    "present",
    "data",
    "useful",
    "building",
    "model",
    "outlier",
    "model",
    "wo",
    "give",
    "best",
    "result",
    "outlier",
    "treatment",
    "cap",
    "outliers",
    "whiskers",
    "values",
    "based",
    "decision",
    "fit",
    "model",
    "outl",
    "treatment",
    "data",
    "need",
    "outlier",
    "treatment",
    "outlier",
    "treatment",
    "need",
    "need",
    "decide",
    "based",
    "data",
    "need",
    "decide",
    "whether",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "ented",
    "us",
    "building",
    "model",
    "video",
    "show",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "first",
    "need",
    "need",
    "first",
    "need",
    "define",
    "one",
    "function",
    "get",
    "wiers",
    "box",
    "slot",
    "analyzed",
    "outlier",
    "data",
    "see",
    "like",
    "see",
    "outl",
    "population",
    "outliers",
    "thinness",
    "many",
    "column",
    "outliers",
    "side",
    "lower",
    "side",
    "cap",
    "outl",
    "upper",
    "whisker",
    "upper",
    "whisker",
    "cap",
    "value",
    "upper",
    "whisker",
    "values",
    "lower",
    "whisker",
    "cap",
    "lower",
    "whisker",
    "means",
    "replace",
    "values",
    "upper",
    "whisker",
    "value",
    "20",
    "upper",
    "whisker",
    "fill",
    "like",
    "70",
    "like",
    "outlier",
    "treatment",
    "let",
    "see",
    "outlier",
    "treatment",
    "data",
    "outlier",
    "treatment",
    "need",
    "keep",
    "mind",
    "outl",
    "treatment",
    "done",
    "continuous",
    "numerical",
    "data",
    "wo",
    "outl",
    "treatment",
    "target",
    "variable",
    "outl",
    "treatment",
    "categorical",
    "discrete",
    "variable",
    "need",
    "first",
    "figure",
    "columns",
    "numerical",
    "continuous",
    "numerical",
    "data",
    "column",
    "outl",
    "treatment",
    "data",
    "example",
    "gdp",
    "continuous",
    "data",
    "outl",
    "treatment",
    "gdp",
    "outl",
    "treatment",
    "decide",
    "outl",
    "treatment",
    "like",
    "decide",
    "whether",
    "outl",
    "treatment",
    "video",
    "show",
    "outl",
    "treatment",
    "total",
    "expenditure",
    "outl",
    "treatment",
    "polio",
    "outlier",
    "treatment",
    "discrete",
    "show",
    "discrete",
    "bmi",
    "outliers",
    "isless",
    "many",
    "outlier",
    "outlier",
    "treatment",
    "like",
    "decide",
    "whether",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "done",
    "continuous",
    "numerical",
    "column",
    "outl",
    "treatment",
    "gdp",
    "schooling",
    "outlet",
    "treatment",
    "income",
    "composition",
    "resource",
    "one",
    "outlet",
    "need",
    "outl",
    "treatment",
    "thinness",
    "5",
    "9",
    "years",
    "first",
    "need",
    "check",
    "data",
    "present",
    "column",
    "thinness",
    "yeah",
    "fractional",
    "data",
    "outl",
    "treatment",
    "thinness",
    "5",
    "9",
    "like",
    "select",
    "three",
    "column",
    "outl",
    "treatment",
    "show",
    "outl",
    "treatment",
    "outline",
    "treatment",
    "first",
    "get",
    "upper",
    "rar",
    "lower",
    "rare",
    "define",
    "one",
    "function",
    "whisker",
    "function",
    "give",
    "name",
    "whisker",
    "give",
    "input",
    "input",
    "column",
    "first",
    "need",
    "get",
    "quartile",
    "order",
    "find",
    "upper",
    "scare",
    "lower",
    "whare",
    "need",
    "first",
    "need",
    "get",
    "quel",
    "use",
    "q1",
    "q3",
    "25",
    "percentile",
    "75",
    "percentile",
    "data",
    "equal",
    "np",
    "dot",
    "np",
    "percentile",
    "function",
    "percen",
    "function",
    "nary",
    "np",
    "percentile",
    "column",
    "want",
    "get",
    "25",
    "percentile",
    "75",
    "percentile",
    "column",
    "whatever",
    "column",
    "give",
    "function",
    "give",
    "calculate",
    "first",
    "25",
    "percentile",
    "75",
    "asign",
    "found",
    "inter",
    "qual",
    "range",
    "inter",
    "qual",
    "range",
    "means",
    "q3",
    "means",
    "75",
    "percentile",
    "q3",
    "minus",
    "q1",
    "q3",
    "minus",
    "q1",
    "getting",
    "iqr",
    "find",
    "lower",
    "whare",
    "lower",
    "vare",
    "equal",
    "q",
    "q1",
    "minus",
    "iqr",
    "iqr",
    "upper",
    "vare",
    "upper",
    "v",
    "sare",
    "equal",
    "q3",
    "plus",
    "iqr",
    "calculating",
    "want",
    "return",
    "return",
    "return",
    "return",
    "lower",
    "risker",
    "first",
    "upper",
    "whisker",
    "let",
    "check",
    "whether",
    "execute",
    "oh",
    "sorry",
    "need",
    "give",
    "return",
    "check",
    "whisker",
    "whether",
    "get",
    "upper",
    "whisker",
    "lowerer",
    "whisker",
    "whisker",
    "first",
    "need",
    "get",
    "column",
    "information",
    "df",
    "get",
    "columns",
    "columns",
    "yeah",
    "column",
    "give",
    "gdp",
    "gdp",
    "yeah",
    "column",
    "give",
    "column",
    "df",
    "column",
    "let",
    "see",
    "get",
    "got",
    "error",
    "course",
    "model",
    "number",
    "new",
    "percentage",
    "sorry",
    "percent",
    "okay",
    "okay",
    "okay",
    "percent",
    "n",
    "missing",
    "need",
    "yeah",
    "give",
    "lower",
    "whisker",
    "upper",
    "whisker",
    "gdp",
    "column",
    "get",
    "lower",
    "whisker",
    "upper",
    "whisker",
    "lower",
    "rar",
    "upper",
    "rar",
    "need",
    "fill",
    "use",
    "check",
    "whether",
    "working",
    "fill",
    "lower",
    "rar",
    "upper",
    "rar",
    "three",
    "column",
    "whichever",
    "selected",
    "column",
    "first",
    "select",
    "gdp",
    "first",
    "one",
    "gdp",
    "control",
    "v",
    "total",
    "expenditure",
    "contrl",
    "c",
    "contrl",
    "v",
    "outlet",
    "treatment",
    "thinness",
    "column",
    "contrl",
    "c",
    "control",
    "v",
    "let",
    "take",
    "1",
    "2",
    "3",
    "four",
    "columns",
    "outlier",
    "treatment",
    "first",
    "need",
    "get",
    "lower",
    "whisker",
    "lower",
    "whisker",
    "comma",
    "upper",
    "whisker",
    "column",
    "getting",
    "lower",
    "whare",
    "upper",
    "already",
    "defined",
    "visker",
    "function",
    "use",
    "visker",
    "function",
    "use",
    "df",
    "means",
    "column",
    "use",
    "fill",
    "use",
    "equal",
    "np",
    "dot",
    "means",
    "nump",
    "dot",
    "use",
    "fill",
    "np",
    "dot",
    "sorry",
    "np",
    "np",
    "df",
    "means",
    "column",
    "check",
    "df",
    "lesser",
    "low",
    "lower",
    "whisker",
    "fill",
    "lower",
    "whisker",
    "otherwise",
    "keep",
    "code",
    "tells",
    "like",
    "df",
    "df",
    "equal",
    "np",
    "dot",
    "df",
    "greater",
    "upper",
    "whisker",
    "fill",
    "upper",
    "whisker",
    "otherwise",
    "keep",
    "code",
    "run",
    "code",
    "yeah",
    "lower",
    "whisker",
    "upper",
    "whisker",
    "kep",
    "outl",
    "capped",
    "lower",
    "upper",
    "whisker",
    "check",
    "box",
    "flot",
    "let",
    "check",
    "box",
    "flot",
    "use",
    "columns",
    "check",
    "box",
    "slot",
    "control",
    "v",
    "sns",
    "sorry",
    "sns",
    "boxplot",
    "directly",
    "use",
    "df",
    "plt",
    "dot",
    "show",
    "pl",
    "show",
    "let",
    "see",
    "get",
    "yeah",
    "got",
    "box",
    "slot",
    "outlier",
    "outliers",
    "outl",
    "outl",
    "treatment",
    "four",
    "column",
    "outl",
    "treatment",
    "outl",
    "treatment",
    "decision",
    "based",
    "data",
    "defers",
    "data",
    "data",
    "need",
    "decide",
    "whether",
    "outl",
    "treatment",
    "want",
    "outl",
    "treatment",
    "need",
    "decide",
    "whether",
    "cap",
    "upper",
    "whisker",
    "lower",
    "whisker",
    "need",
    "select",
    "specific",
    "value",
    "forfill",
    "based",
    "data",
    "depends",
    "data",
    "data",
    "ideally",
    "outlier",
    "treatment",
    "using",
    "upper",
    "rar",
    "lower",
    "rar",
    "outlier",
    "treatment",
    "duplicates",
    "garbage",
    "value",
    "already",
    "find",
    "find",
    "check",
    "duplicates",
    "value",
    "garbage",
    "value",
    "also",
    "skip",
    "duplicate",
    "value",
    "need",
    "delete",
    "column",
    "using",
    "drop",
    "duplicates",
    "drop",
    "duplicates",
    "value",
    "use",
    "df",
    "dot",
    "df",
    "dot",
    "drop",
    "duplicates",
    "df",
    "drop",
    "duplicates",
    "duplicates",
    "deleted",
    "data",
    "struct",
    "data",
    "frame",
    "duplicates",
    "need",
    "worry",
    "duplicates",
    "garbage",
    "value",
    "change",
    "medan",
    "mode",
    "column",
    "duplicat",
    "treatment",
    "garbage",
    "treatment",
    "garbage",
    "value",
    "treatment",
    "last",
    "let",
    "see",
    "encoding",
    "data",
    "encoding",
    "means",
    "fitting",
    "model",
    "fitting",
    "data",
    "model",
    "need",
    "columns",
    "form",
    "numerical",
    "data",
    "data",
    "columns",
    "like",
    "like",
    "country",
    "status",
    "object",
    "data",
    "form",
    "need",
    "convert",
    "object",
    "data",
    "numerical",
    "called",
    "encoding",
    "converting",
    "object",
    "numerical",
    "called",
    "encoding",
    "encoding",
    "data",
    "two",
    "ways",
    "one",
    "hot",
    "encoding",
    "label",
    "encoding",
    "one",
    "hard",
    "means",
    "create",
    "dummies",
    "category",
    "tamies",
    "also",
    "use",
    "label",
    "encoding",
    "categorical",
    "data",
    "categorical",
    "variable",
    "ordinal",
    "use",
    "label",
    "encoding",
    "otherwise",
    "use",
    "tamies",
    "one",
    "h",
    "encoding",
    "creating",
    "easy",
    "technique",
    "using",
    "pandas",
    "pd",
    "get",
    "dumis",
    "use",
    "use",
    "module",
    "create",
    "dumis",
    "use",
    "pd",
    "get",
    "create",
    "tmy",
    "variable",
    "use",
    "like",
    "pd",
    "dot",
    "get",
    "damis",
    "pd",
    "get",
    "damis",
    "within",
    "parenthesis",
    "give",
    "data",
    "data",
    "data",
    "frame",
    "give",
    "column",
    "need",
    "create",
    "adamy",
    "column",
    "two",
    "column",
    "like",
    "country",
    "first",
    "one",
    "country",
    "second",
    "column",
    "status",
    "status",
    "status",
    "use",
    "drop",
    "first",
    "equal",
    "true",
    "delete",
    "drop",
    "first",
    "level",
    "wo",
    "give",
    "multicolinearity",
    "problem",
    "drop",
    "first",
    "level",
    "data",
    "using",
    "pd",
    "get",
    "data",
    "give",
    "column",
    "need",
    "create",
    "dumy",
    "present",
    "data",
    "need",
    "create",
    "dummies",
    "country",
    "status",
    "two",
    "columns",
    "categorical",
    "features",
    "run",
    "one",
    "problem",
    "callum",
    "sorry",
    "missing",
    "columns",
    "give",
    "created",
    "dummy",
    "country",
    "like",
    "country",
    "ugu",
    "country",
    "like",
    "create",
    "dummies",
    "country",
    "create",
    "dummy",
    "one",
    "single",
    "code",
    "pd",
    "get",
    "tamies",
    "give",
    "tamies",
    "use",
    "categorical",
    "ordinal",
    "column",
    "use",
    "replace",
    "function",
    "replace",
    "true",
    "false",
    "use",
    "label",
    "encoding",
    "using",
    "replace",
    "function",
    "give",
    "label",
    "like",
    "1",
    "2",
    "3",
    "4",
    "like",
    "data",
    "columns",
    "label",
    "encoding",
    "one",
    "hot",
    "encoding",
    "data",
    "using",
    "pd",
    "g",
    "store",
    "dumy",
    "another",
    "data",
    "give",
    "new",
    "data",
    "dummy",
    "check",
    "check",
    "dumy",
    "give",
    "information",
    "form",
    "numerical",
    "categorical",
    "feature",
    "data",
    "ready",
    "fit",
    "model",
    "stage",
    "fit",
    "data",
    "model",
    "whether",
    "regression",
    "model",
    "using",
    "scalar",
    "stats",
    "model",
    "use",
    "data",
    "fil",
    "model",
    "video",
    "video",
    "learned",
    "many",
    "things",
    "like",
    "data",
    "learned",
    "import",
    "necessary",
    "libraries",
    "read",
    "data",
    "set",
    "sanity",
    "check",
    "find",
    "missing",
    "value",
    "outlier",
    "garbage",
    "value",
    "explorat",
    "data",
    "analysis",
    "understand",
    "data",
    "better",
    "like",
    "uh",
    "descriptive",
    "statistics",
    "also",
    "visualize",
    "data",
    "histogram",
    "box",
    "flot",
    "scatter",
    "flot",
    "understand",
    "relationship",
    "missing",
    "value",
    "treatment",
    "mean",
    "media",
    "mode",
    "method",
    "k",
    "nni",
    "imputer",
    "method",
    "outlier",
    "treatment",
    "upper",
    "risk",
    "lower",
    "risk",
    "impute",
    "duplicates",
    "garbage",
    "value",
    "treatment",
    "encoding",
    "data",
    "video",
    "video",
    "learned",
    "data",
    "thank",
    "watching",
    "stay",
    "tuned",
    "next",
    "video"
  ],
  "keywords": [
    "video",
    "see",
    "data",
    "building",
    "machine",
    "learning",
    "model",
    "fit",
    "need",
    "whether",
    "missing",
    "value",
    "outliers",
    "duplicates",
    "present",
    "garbage",
    "values",
    "understand",
    "first",
    "check",
    "relationship",
    "step",
    "fitting",
    "like",
    "read",
    "set",
    "sanity",
    "analysis",
    "treatment",
    "let",
    "encoding",
    "categorical",
    "import",
    "pd",
    "plot",
    "sorry",
    "yeah",
    "two",
    "frame",
    "life",
    "expectancy",
    "variable",
    "df",
    "show",
    "information",
    "country",
    "year",
    "status",
    "adult",
    "infant",
    "percentage",
    "many",
    "columns",
    "column",
    "dot",
    "three",
    "means",
    "shape",
    "give",
    "types",
    "overall",
    "get",
    "total",
    "null",
    "counts",
    "also",
    "type",
    "object",
    "form",
    "one",
    "find",
    "count",
    "want",
    "decide",
    "delete",
    "based",
    "number",
    "uh",
    "c",
    "v",
    "zero",
    "gdp",
    "side",
    "50",
    "create",
    "duplicate",
    "unique",
    "done",
    "use",
    "select",
    "include",
    "equal",
    "result",
    "impute",
    "median",
    "mode",
    "descriptive",
    "statistics",
    "numerical",
    "code",
    "mean",
    "percentile",
    "distribution",
    "histogram",
    "sns",
    "flot",
    "box",
    "outlier",
    "lower",
    "mortality",
    "upper",
    "scatter",
    "target",
    "using",
    "outl",
    "correlation",
    "got",
    "heat",
    "map",
    "fill",
    "imputer",
    "k",
    "inut",
    "discrete",
    "imputed",
    "function",
    "whisker",
    "rar",
    "np",
    "drop",
    "label"
  ]
}