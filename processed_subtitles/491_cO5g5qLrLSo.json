{
  "text": "before reinforcement learning\nafter reinforcement learning\nwhat's happening guys my name is\nnicholas and in this video we're going\nto be going through a bit of a crash\ncourse on reinforcement learning\nnow if you've ever worked with deep\nlearning or machine learning before you\nknow the two key forms are supervised\nand unsupervised learning\nnow reinforcement learning is a little\nbit different to that because you tend\nto train\nin a live environment now there's a\nreally easy way to remember the core\nconcepts in reinforcement learning\nall you need to remember is area 51. now\nyou're probably thinking what the hell\ndoes area 51 have to do with\nreinforcement learning\nwell the area in area 51 stands for\naction reward environment and agent\nthese are the four key things you need\nin\nany reinforcement learning model now in\nthis video we're going to be covering\nall of those key concepts let's take a\ndeeper look as to what we're going to be\ngoing through so in this video we're\ngoing to cover\neverything you need to get started with\nreinforcement learning we're going to\nstart out by creating an environment\nusing open ai gym\nwe're then going to build a deep\nlearning model using tensorflow and\nkeras\nthis same model will then pass to\nkerasrl in order to train our\nreinforcement learning model\nusing policy-based learning now in terms\nof how we're going to be doing it we're\ngoing\nto be largely working within python and\nspecifically\nwe're going to be working inside of a\njupyter notebook we'll start out by\nbuilding our environment using open ai\ngym\nwe'll then build our deep learning model\nagain using tensorflow and keras\nand then once we've built that model\nwe're then going to train it using\nkerasrl\nwe'll then be able to take that same\nmodel save it down into memory and\nreload it for when we want to deploy it\ninto production\nready to get to it let's do it so\nthere's a couple of key things that we\nneed to do in order to build\nour deep reinforcement learning model so\nspecifically we need to first up\ninstall our dependencies then what we're\ngoing to do is build an environment with\nopen ai gym with just a couple of lines\nof code\nso this is going to allow us to see the\nenvironment that we're actually using\nreinforcement learning in later on\nthen we're going to build a deep\nlearning model with keras so we're\nspecifically going to be using the\nsequential api there\nand then what we're going to do is train\nthat keras model using keras\nreinforcement learning\nand last but not least we're going to\ndelete it all and reload that agent from\nmemory so this is going to allow you to\ndeploy it into production if you want to\nlater on\nso first up let's install our\ndependencies so what we're going to need\nhere\nis tensorflow keras kerasrl as well as\nopen ai\ngym\nso what we've done is we've installed\nour four key dependencies so we've used\npip\ninstall and specifically we've installed\ntensorflow 2.3.0\nwe've installed open ai gym so that's\njust gym\nwe've installed keras and we've also\ninstalled keras rl2\nso those are all our dependencies now\ndone and installed\nnow what we can actually go and do is\nset up a random environment with open ai\ngym\nnow open ai gym comes with a bunch of\npre-built\nenvironments that you can use to test\nout reinforcement learning on\nso if we actually head on over to\ngym.openai.com\nyou can see there's a bunch of random\nenvironments so\nhere we've got some algorithms we've got\natari games so if you wanted to build\natari\nor video game style reinforcement\nlearning engines you could\nwe're going to be working on these\nclassic control ones and specifically\nwe're going to be using cartpol and so\nthe whole idea behind carpol is that you\nwant to basically\nmove this cart down the bottom here in\norder to balance the pole\nup there so the whole idea is that for\neach step you take you get a point with\na maximum of\n200 points so ideally what we're going\nto see when we start off is with our\nrandom steps we're not going to get\nanywhere near 200 but\nonce we use deep learning and\nreinforcement learning we ideally should\nget a much closer to actually hitting\nour final result\nnow we've got two movements we can\neither go left or right so\nwhat we're going to see is when we\ncreate our environment we're going to\nhave two actions available either left\nor right\nif you work in different reinforcement\nlearning environments you might have a\ndifferent number of actions that you can\ntake so for example you might go up or\ndown left or right\nif you're working with other things so\nnow what we're going to do is\nset up this environment so you can work\nwith it within python so if we go back\nto our jupyter notebook\nlet's start setting that up so the first\nthing that we need to do is import our\ndependencies so\nin order to do that we're going to\nimport openai gym and we're also going\nto import the random library so we can\ntake a bunch of random steps\nso those are our two key dependencies\nimported so\nand this is specifically for our open ai\ngym so we've imported gym\nand we've also imported random now what\nwe can go and do is actually set up that\nenvironment\nso that's our environment set up so what\nwe went and did there is we used\nthe open ai gym library and specifically\nwe used\nthe make method to build our carpol\nenvironment so remember that was the\ncarpol environment that we saw here\nwe then extracted the states that we've\ngot so this is available through env\nwhich is our environment that we just\nset up\nobservation space dot shape so we're\ntaking a look at all the different\nstates that we've got available\nwithin our environment and we've also\nextracted the action so if you take a\nlook we're getting that from our action\nspace\nand we can see that we're going to have\na specific number of actions so if we\ntake a look at our states\nwe've basically got four states\navailable and if we take a look at our\nactions\nwe've got two actions so basically those\nare left or right moving\nour carpal left or right now what we can\nactually go and do is actually\nvisualize what it looks like when we're\ntaking random steps within our carpol\nenvironment\nso ideally what we'll see is that our\ncarpals just sort of moving randomly\nbecause we're taking random steps in\norder to get a specific score so\nremember with each step that we take\nwhere our carpol hasn't fully fallen\nover\nwe're going to get one point with a\nmaximum of 200 points so\nlet's build our random environment\nall right so we've written a bit of code\nthere now what we're actually going to\ndo is\nstart by breaking this down from here so\nthe first thing that we're going to do\nis render our environment so this is\ngoing to allow us to see our cut in\naction when it's moving left and right\nthen what we're doing is we're taking a\nrandom step so we're either\ngoing left or right so zero or one\nbasically represents one of those steps\nso we're just taking a random choice\nto see how that impacts our environment\nthen what we're doing is we're actually\napplying\nthat action to our environment and we're\ngetting a bunch of stuff as a result of\nthat\nso we're getting our state we're getting\nour reward we're getting whether or not\nwe've completed the game so whether or\nnot we've failed or whether or not we've\npassed\nand we're also getting a bunch of\ninformation then\nbased on our step we're going to get a\nreward so remember if we take a step in\nthe correct direction and we haven't\nfailed we get one point\nthis basically allows us to accumulate\nour entire reward\nnow if we fail or if we get to the end\nof the game then\ndone is going to be set to true so what\nwe're doing is we're continuously taking\nsteps until we're complete\nso we reset the entire environment up\nhere and then we're also printing out\nour final reward so ideally what we'll\nget is\nthe episode number as well as our score\nso\nlet's go on ahead and run that and see\nour episodes live and in action actually\nit looks like we've got a bug there\nepisode\nall right so you can see our carts\nmoving and it's moving randomly\nand you can see that our pole is sort of\nflailing about now what we're actually\nlogging out is the score each time so it\nlooks like\nwe're surpassing a specific threshold\nand we're failing so we're only getting\nup to a maximum of about\n38 so that's our maximum score now\nideally what we want to be able to get\nis\nall the way up to 200 and this is where\nreinforcement learning comes in\nso basically our deep learning model is\ngoing to learn the best action to take\nin that specific environment in order to\nmaximize our score\nnow this all starts with a deep learning\nmodel so let's go ahead and start\ncreating a deep learning model\nnow in order to do that we first up need\nto import some dependencies and these\nare largely going to be our tensorflow\nkeras dependencies\nso let's go ahead and import those\nso we've imported our dependencies so\nwe've specifically first up imported\nnumpy so this is going to allow us to\nwork with numpy arrays\nthen we've imported the sequential api\nso this is going to allow us build a\nsequential model with keras\nthen we've also imported two different\ntypes of layers so specifically we've\nimported\nour dense node as well as our flatten\nnode and last but not least we've\nimported the atom optimizer so that's\ngoing to be the optimizer that we use\nto train our deep learning model now\nwhat we can go and do is actually go and\nbuild that model so we're going to build\nthis\nwrapped inside of a function so we can\nreproduce this model whenever we need to\nso that's our build model function\ndefined so what we've basically gone and\ndone is created\na new function called build model and to\nthat we're going to pass two arguments\nso specifically our states\nso these were the states that we\nextracted from our environment up here\nand we're also going to pass through our\nactions so these are going to be the two\ndifferent actions that we've got in our\ncarpol environment\nin order to build our deep learning\nmodel we're first instantiating our\nsequential model then we're passing\nthrough the\nflatten node and specifically to that\nwe're going to be passing through\na flat node which contains our different\nstates so remember our four different\nstates that we had\nthen we're adding two dense nodes to\nstart building out our deep learning\nmodel with a relu activation function\nand last but not least our last dense\nnode has our actions so this is\nbasically going to mean\nthat we pass through our states at the\ntop and we\npass through our actions down the bottom\nso ideally what we should be able to do\nis\ntrain our model based on the states\ncoming through to determine the best\nactions\nto maximize our reward or our score that\nwe can see here\nso let's go ahead and create an instance\nof that model just by using that build\nmodel function\nand we can also visualize what the model\nlooks like using the model.summary\nfunction\nso you can see here that we're passing\nthrough our four different states\nwe've got 24 dense nodes 24 dense nodes\nso these are going to be our fully\nconnected layers within our neural\nnetwork\nand then last but not least we're going\nto be passing out our two different\nactions that we want to take within our\nenvironment now what we can go and do is\ntake this deep learning model and\nactually train it using keras rl\nso first up we need to import our keras\nrl dependencies so let's go ahead and do\nthat\nso those are our dependencies imported\nso we've imported\nthree key things here so we've imported\nout a deep\nqueue network agent so basically there's\na bunch of different agents within\nthe keras rl environment so you can see\nwe've got a dqm agent a naffa agent\nddpg sasa sem so all of these are\ndifferent agents that you can use to\ntrain\nyour reinforcement learning model we're\ngoing to be using dqn for this\nparticular video but\ntry testing out some of the others and\nsee how you go now what we\nalso have is a specific policy so within\nreinforcement learning you've got\ndifferent styles\nso you've got value-based reinforcement\nlearning and you've also got\npolicy-based reinforcement learning so\nin this case we're going to be using\npolicy-based reinforcement learning and\nthe specific policy that we're going to\nbe using\nis the boltzmann q policy which you can\nsee here\nnow the last thing that we've gone and\nimported is sequential memory so for\nour dqn agent we're going to need to\nmaintain some memory\nand the sequential memory class is what\nallows us to do that\nso now what we can go and do is set up\nour agent and again we're going to wrap\nthis inside of a function so we can\nreproduce it when we want to reload it\nfrom memory so let's go\nahead and build that function\nso that's our function defined now what\nwe've basically done is we've named our\nfunction build\nagent and to that we pass through our\nmodel so this is\nour deep learning model that we\nspecified up here and we're also passing\nthrough the different actions that we\ncan take so those were the two different\nactions\nleft or right that we had available\nwithin our environment\nthen we set up our policy we set up our\nmemory\nand we set up our dqn agent and to that\ndqn agent we actually pass through our\ndeep learning model\nand memory our policy as well as a\nnumber of\nother keyword arguments so then what we\ndo is we return that dqn\nagent so let's go on ahead and actually\nuse this dqn agent to actually now\ngo and train our reinforcement learning\nmodel so first up we want to start out\nby instantiating our dqm model\nthen we're going to compile it and then\nwe're going to go ahead and fit\nall right and there you go so you can\nsee that our dqn model is now starting\nto train\nso what we actually did is we\ninstantiated our or we used our build\nagent function to set up a new dqm model\nand that was that up here\nand we passed through our model as well\nas our actions\nwe then compiled it and we passed\nthrough our optimizer so this was that\natom optimizer that we imported right at\nthe start\nand we also passed through the metrics\nthat we want to track so in this case\nit's mean\nabsolute error then we use the fit\nfunction to kick off the training\nand to that we pass through our entire\nenvironment the number of steps we want\nto take\nwhether or not we want to visualize it\nso we'll take a look at that in a second\nand we also specified verbose as one so\nwe don't want full logging we want a\nlittle bit of logging\nnow what we can do is just let that go\nahead and train to take a couple of\nminutes and then\nwe should have a fully built\nreinforcement learning model\nfive minutes later sweet so that's our\nreinforcement\nlearning model now done dusted and\ntrained so all\nup it took about 256 seconds to go and\ntrain and you can see\nin our fourth interval that we're\naccumulating a reward of about 200\nnow what we can go and do is actually\nprint out and see what our total scores\nwere so remember when we started out up\nhere so just taking random steps we were\ngetting about a maximum score of about\n51\nbut that's not all that great\nconsidering that the total maximum score\nfor the game\nis about 200. so let's go and test this\nout and see what this\nactually looks like or how it's actually\nperforming so we can do that using\nthe dqn.test method so let's try that\nout\nall right so that's looking better\nalready so you can see in virtually\nevery single episode we're getting a\nscore of about 200\nand our mean is 200. so what we did\nthere in order to test that out\nis we accessed our dqn model and we use\nthe test method\nto that we pass through our actual\nenvironment the number of games that we\nwant to run so in this case\nthey're called episodes so we ran 100\ngames\nand whether or not we want to visualize\nit then what we did is we\noutputted our mean result now if we\nwanted to actually visualize what the\ndifference is we can do that as well\nand you can see our model is performing\nway better so you can see it's actually\nable\nto balance the pole a whole lot better\nthan what it was before when it was just\nrandomly sort of\nflailing about we can test that out\nagain so this time rather than doing\nfive episodes say we wanted to\num 15 for example so you can see that\nour model again\nit's performing way better than what it\nwas initially so\nit's actually able to reiterate itself\nand resort to balanced it and make sure\nthat that pole stays straight\nbrings a tear to my eye so good\nsweet so that's all done now what\nhappens if we actually wanted to go and\nsave this model away\nand use it later on say for example we\nwanted to go and deploy it into\nproduction\nwell what we can actually do is we can\nactually save the weights from our dqm\nmodel and then reload them later on and\nto try to test them out\nso we can do that using the save weights\nmethod\nfrom our dqm model so let's go ahead and\nsave our weights\nthen what we'll do is we'll blast away\nall of the stuff that we just created\nand we'll rebuild it by reloading our\nweights\nso we've now gone and saved our weight\nso if we actually take a look in our\nfolder you can see that we've gone\nand generated two different h5f files so\nthese basically allow us\nto save our reinforcement learning model\nweights\nnow if we wanted to go and rebuild our\nagent first up let's start by deleting\nour model deleting our environment and\ndeleting our dqn agent\nand then what we can do is rebuild it\nusing all the functions that we had and\nreload those weights to test it out so\nif we go and do that\nso you can see if we go and try to use\nour dqn.test method\nthere's nothing there because we've then\ngone and deleted it but what we can do\nis we can go and rebuild that\nenvironment and test it out so let's go\nand do that\nperfect so we've now gone and\nreinstantiated all of our models so we\nfirst up we built our environment\nwe extracted our actions and our states\njust like we did before\nthen we used our build model and our\nbuild agent\nfunctions to go and rebuild our deep\nlearning model and\nreinstantiate our dqn agent and then\nlast but not least we compiled it\nnow what we can do is actually reload\nour weights into our model and then test\nit out again so in order to do that we\ncan use the dqn\ndot load weights method so before up\nhere we use save weights now we can\nload our weights in order to re-test\nthis out\nand the file that we're going to pass to\nour load weights method is\nthe one that we exported out here so we\ncan copy that in and paste that here\nand now we've gone and reloaded our\nweights we can actually go\nand test out our environment again so\nideally what we should get is similar\nresults so\nagain you can see it's performing well\nit's performing just as well as what it\ndid\nbefore we deleted our weights and now we\nwent and reloaded them\nand that about wraps up this video so we\ncovered a bunch of stuff so specifically\nwe went and installed our dependencies\nwe then created a random environment\nusing open ai gym and we got about a\nmaximum score of about 51\nwe then built a deep learning model\nusing keras and then use keras rl to\ntrain that\nusing policy-based reinforcement\nlearning and then\nlast but not least we went and reloaded\nthat agent from memory so that allows\nyou to work with this\ninside of a production environment if\nyou want to go and deploy\nit and that about wraps it up thanks so\nmuch for tuning in guys hopefully you\nfound this video useful if you did be\nsure to give it a thumbs up hit\nsubscribe and tick that bell so you get\nnotified of when i release future videos\nif you do have any questions or need any\nhelp be sure to drop a mention in the\ncomments below\nand i'll get right back to you and all\nthe course materials\nincluding the github repository as well\nas links to documentation are available\nin the description below\nso you can get a kickstart and get up\nand running with your reinforcement\nlearning model\nthanks again for tuning in peace\n",
  "words": [
    "reinforcement",
    "learning",
    "reinforcement",
    "learning",
    "happening",
    "guys",
    "name",
    "nicholas",
    "video",
    "going",
    "going",
    "bit",
    "crash",
    "course",
    "reinforcement",
    "learning",
    "ever",
    "worked",
    "deep",
    "learning",
    "machine",
    "learning",
    "know",
    "two",
    "key",
    "forms",
    "supervised",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "little",
    "bit",
    "different",
    "tend",
    "train",
    "live",
    "environment",
    "really",
    "easy",
    "way",
    "remember",
    "core",
    "concepts",
    "reinforcement",
    "learning",
    "need",
    "remember",
    "area",
    "probably",
    "thinking",
    "hell",
    "area",
    "51",
    "reinforcement",
    "learning",
    "well",
    "area",
    "area",
    "51",
    "stands",
    "action",
    "reward",
    "environment",
    "agent",
    "four",
    "key",
    "things",
    "need",
    "reinforcement",
    "learning",
    "model",
    "video",
    "going",
    "covering",
    "key",
    "concepts",
    "let",
    "take",
    "deeper",
    "look",
    "going",
    "going",
    "video",
    "going",
    "cover",
    "everything",
    "need",
    "get",
    "started",
    "reinforcement",
    "learning",
    "going",
    "start",
    "creating",
    "environment",
    "using",
    "open",
    "ai",
    "gym",
    "going",
    "build",
    "deep",
    "learning",
    "model",
    "using",
    "tensorflow",
    "keras",
    "model",
    "pass",
    "kerasrl",
    "order",
    "train",
    "reinforcement",
    "learning",
    "model",
    "using",
    "learning",
    "terms",
    "going",
    "going",
    "largely",
    "working",
    "within",
    "python",
    "specifically",
    "going",
    "working",
    "inside",
    "jupyter",
    "notebook",
    "start",
    "building",
    "environment",
    "using",
    "open",
    "ai",
    "gym",
    "build",
    "deep",
    "learning",
    "model",
    "using",
    "tensorflow",
    "keras",
    "built",
    "model",
    "going",
    "train",
    "using",
    "kerasrl",
    "able",
    "take",
    "model",
    "save",
    "memory",
    "reload",
    "want",
    "deploy",
    "production",
    "ready",
    "get",
    "let",
    "couple",
    "key",
    "things",
    "need",
    "order",
    "build",
    "deep",
    "reinforcement",
    "learning",
    "model",
    "specifically",
    "need",
    "first",
    "install",
    "dependencies",
    "going",
    "build",
    "environment",
    "open",
    "ai",
    "gym",
    "couple",
    "lines",
    "code",
    "going",
    "allow",
    "us",
    "see",
    "environment",
    "actually",
    "using",
    "reinforcement",
    "learning",
    "later",
    "going",
    "build",
    "deep",
    "learning",
    "model",
    "keras",
    "specifically",
    "going",
    "using",
    "sequential",
    "api",
    "going",
    "train",
    "keras",
    "model",
    "using",
    "keras",
    "reinforcement",
    "learning",
    "last",
    "least",
    "going",
    "delete",
    "reload",
    "agent",
    "memory",
    "going",
    "allow",
    "deploy",
    "production",
    "want",
    "later",
    "first",
    "let",
    "install",
    "dependencies",
    "going",
    "need",
    "tensorflow",
    "keras",
    "kerasrl",
    "well",
    "open",
    "ai",
    "gym",
    "done",
    "installed",
    "four",
    "key",
    "dependencies",
    "used",
    "pip",
    "install",
    "specifically",
    "installed",
    "tensorflow",
    "installed",
    "open",
    "ai",
    "gym",
    "gym",
    "installed",
    "keras",
    "also",
    "installed",
    "keras",
    "rl2",
    "dependencies",
    "done",
    "installed",
    "actually",
    "go",
    "set",
    "random",
    "environment",
    "open",
    "ai",
    "gym",
    "open",
    "ai",
    "gym",
    "comes",
    "bunch",
    "environments",
    "use",
    "test",
    "reinforcement",
    "learning",
    "actually",
    "head",
    "see",
    "bunch",
    "random",
    "environments",
    "got",
    "algorithms",
    "got",
    "atari",
    "games",
    "wanted",
    "build",
    "atari",
    "video",
    "game",
    "style",
    "reinforcement",
    "learning",
    "engines",
    "could",
    "going",
    "working",
    "classic",
    "control",
    "ones",
    "specifically",
    "going",
    "using",
    "cartpol",
    "whole",
    "idea",
    "behind",
    "carpol",
    "want",
    "basically",
    "move",
    "cart",
    "bottom",
    "order",
    "balance",
    "pole",
    "whole",
    "idea",
    "step",
    "take",
    "get",
    "point",
    "maximum",
    "200",
    "points",
    "ideally",
    "going",
    "see",
    "start",
    "random",
    "steps",
    "going",
    "get",
    "anywhere",
    "near",
    "200",
    "use",
    "deep",
    "learning",
    "reinforcement",
    "learning",
    "ideally",
    "get",
    "much",
    "closer",
    "actually",
    "hitting",
    "final",
    "result",
    "got",
    "two",
    "movements",
    "either",
    "go",
    "left",
    "right",
    "going",
    "see",
    "create",
    "environment",
    "going",
    "two",
    "actions",
    "available",
    "either",
    "left",
    "right",
    "work",
    "different",
    "reinforcement",
    "learning",
    "environments",
    "might",
    "different",
    "number",
    "actions",
    "take",
    "example",
    "might",
    "go",
    "left",
    "right",
    "working",
    "things",
    "going",
    "set",
    "environment",
    "work",
    "within",
    "python",
    "go",
    "back",
    "jupyter",
    "notebook",
    "let",
    "start",
    "setting",
    "first",
    "thing",
    "need",
    "import",
    "dependencies",
    "order",
    "going",
    "import",
    "openai",
    "gym",
    "also",
    "going",
    "import",
    "random",
    "library",
    "take",
    "bunch",
    "random",
    "steps",
    "two",
    "key",
    "dependencies",
    "imported",
    "specifically",
    "open",
    "ai",
    "gym",
    "imported",
    "gym",
    "also",
    "imported",
    "random",
    "go",
    "actually",
    "set",
    "environment",
    "environment",
    "set",
    "went",
    "used",
    "open",
    "ai",
    "gym",
    "library",
    "specifically",
    "used",
    "make",
    "method",
    "build",
    "carpol",
    "environment",
    "remember",
    "carpol",
    "environment",
    "saw",
    "extracted",
    "states",
    "got",
    "available",
    "env",
    "environment",
    "set",
    "observation",
    "space",
    "dot",
    "shape",
    "taking",
    "look",
    "different",
    "states",
    "got",
    "available",
    "within",
    "environment",
    "also",
    "extracted",
    "action",
    "take",
    "look",
    "getting",
    "action",
    "space",
    "see",
    "going",
    "specific",
    "number",
    "actions",
    "take",
    "look",
    "states",
    "basically",
    "got",
    "four",
    "states",
    "available",
    "take",
    "look",
    "actions",
    "got",
    "two",
    "actions",
    "basically",
    "left",
    "right",
    "moving",
    "carpal",
    "left",
    "right",
    "actually",
    "go",
    "actually",
    "visualize",
    "looks",
    "like",
    "taking",
    "random",
    "steps",
    "within",
    "carpol",
    "environment",
    "ideally",
    "see",
    "carpals",
    "sort",
    "moving",
    "randomly",
    "taking",
    "random",
    "steps",
    "order",
    "get",
    "specific",
    "score",
    "remember",
    "step",
    "take",
    "carpol",
    "fully",
    "fallen",
    "going",
    "get",
    "one",
    "point",
    "maximum",
    "200",
    "points",
    "let",
    "build",
    "random",
    "environment",
    "right",
    "written",
    "bit",
    "code",
    "actually",
    "going",
    "start",
    "breaking",
    "first",
    "thing",
    "going",
    "render",
    "environment",
    "going",
    "allow",
    "us",
    "see",
    "cut",
    "action",
    "moving",
    "left",
    "right",
    "taking",
    "random",
    "step",
    "either",
    "going",
    "left",
    "right",
    "zero",
    "one",
    "basically",
    "represents",
    "one",
    "steps",
    "taking",
    "random",
    "choice",
    "see",
    "impacts",
    "environment",
    "actually",
    "applying",
    "action",
    "environment",
    "getting",
    "bunch",
    "stuff",
    "result",
    "getting",
    "state",
    "getting",
    "reward",
    "getting",
    "whether",
    "completed",
    "game",
    "whether",
    "failed",
    "whether",
    "passed",
    "also",
    "getting",
    "bunch",
    "information",
    "based",
    "step",
    "going",
    "get",
    "reward",
    "remember",
    "take",
    "step",
    "correct",
    "direction",
    "failed",
    "get",
    "one",
    "point",
    "basically",
    "allows",
    "us",
    "accumulate",
    "entire",
    "reward",
    "fail",
    "get",
    "end",
    "game",
    "done",
    "going",
    "set",
    "true",
    "continuously",
    "taking",
    "steps",
    "complete",
    "reset",
    "entire",
    "environment",
    "also",
    "printing",
    "final",
    "reward",
    "ideally",
    "get",
    "episode",
    "number",
    "well",
    "score",
    "let",
    "go",
    "ahead",
    "run",
    "see",
    "episodes",
    "live",
    "action",
    "actually",
    "looks",
    "like",
    "got",
    "bug",
    "episode",
    "right",
    "see",
    "carts",
    "moving",
    "moving",
    "randomly",
    "see",
    "pole",
    "sort",
    "flailing",
    "actually",
    "logging",
    "score",
    "time",
    "looks",
    "like",
    "surpassing",
    "specific",
    "threshold",
    "failing",
    "getting",
    "maximum",
    "38",
    "maximum",
    "score",
    "ideally",
    "want",
    "able",
    "get",
    "way",
    "200",
    "reinforcement",
    "learning",
    "comes",
    "basically",
    "deep",
    "learning",
    "model",
    "going",
    "learn",
    "best",
    "action",
    "take",
    "specific",
    "environment",
    "order",
    "maximize",
    "score",
    "starts",
    "deep",
    "learning",
    "model",
    "let",
    "go",
    "ahead",
    "start",
    "creating",
    "deep",
    "learning",
    "model",
    "order",
    "first",
    "need",
    "import",
    "dependencies",
    "largely",
    "going",
    "tensorflow",
    "keras",
    "dependencies",
    "let",
    "go",
    "ahead",
    "import",
    "imported",
    "dependencies",
    "specifically",
    "first",
    "imported",
    "numpy",
    "going",
    "allow",
    "us",
    "work",
    "numpy",
    "arrays",
    "imported",
    "sequential",
    "api",
    "going",
    "allow",
    "us",
    "build",
    "sequential",
    "model",
    "keras",
    "also",
    "imported",
    "two",
    "different",
    "types",
    "layers",
    "specifically",
    "imported",
    "dense",
    "node",
    "well",
    "flatten",
    "node",
    "last",
    "least",
    "imported",
    "atom",
    "optimizer",
    "going",
    "optimizer",
    "use",
    "train",
    "deep",
    "learning",
    "model",
    "go",
    "actually",
    "go",
    "build",
    "model",
    "going",
    "build",
    "wrapped",
    "inside",
    "function",
    "reproduce",
    "model",
    "whenever",
    "need",
    "build",
    "model",
    "function",
    "defined",
    "basically",
    "gone",
    "done",
    "created",
    "new",
    "function",
    "called",
    "build",
    "model",
    "going",
    "pass",
    "two",
    "arguments",
    "specifically",
    "states",
    "states",
    "extracted",
    "environment",
    "also",
    "going",
    "pass",
    "actions",
    "going",
    "two",
    "different",
    "actions",
    "got",
    "carpol",
    "environment",
    "order",
    "build",
    "deep",
    "learning",
    "model",
    "first",
    "instantiating",
    "sequential",
    "model",
    "passing",
    "flatten",
    "node",
    "specifically",
    "going",
    "passing",
    "flat",
    "node",
    "contains",
    "different",
    "states",
    "remember",
    "four",
    "different",
    "states",
    "adding",
    "two",
    "dense",
    "nodes",
    "start",
    "building",
    "deep",
    "learning",
    "model",
    "relu",
    "activation",
    "function",
    "last",
    "least",
    "last",
    "dense",
    "node",
    "actions",
    "basically",
    "going",
    "mean",
    "pass",
    "states",
    "top",
    "pass",
    "actions",
    "bottom",
    "ideally",
    "able",
    "train",
    "model",
    "based",
    "states",
    "coming",
    "determine",
    "best",
    "actions",
    "maximize",
    "reward",
    "score",
    "see",
    "let",
    "go",
    "ahead",
    "create",
    "instance",
    "model",
    "using",
    "build",
    "model",
    "function",
    "also",
    "visualize",
    "model",
    "looks",
    "like",
    "using",
    "function",
    "see",
    "passing",
    "four",
    "different",
    "states",
    "got",
    "24",
    "dense",
    "nodes",
    "24",
    "dense",
    "nodes",
    "going",
    "fully",
    "connected",
    "layers",
    "within",
    "neural",
    "network",
    "last",
    "least",
    "going",
    "passing",
    "two",
    "different",
    "actions",
    "want",
    "take",
    "within",
    "environment",
    "go",
    "take",
    "deep",
    "learning",
    "model",
    "actually",
    "train",
    "using",
    "keras",
    "rl",
    "first",
    "need",
    "import",
    "keras",
    "rl",
    "dependencies",
    "let",
    "go",
    "ahead",
    "dependencies",
    "imported",
    "imported",
    "three",
    "key",
    "things",
    "imported",
    "deep",
    "queue",
    "network",
    "agent",
    "basically",
    "bunch",
    "different",
    "agents",
    "within",
    "keras",
    "rl",
    "environment",
    "see",
    "got",
    "dqm",
    "agent",
    "naffa",
    "agent",
    "ddpg",
    "sasa",
    "sem",
    "different",
    "agents",
    "use",
    "train",
    "reinforcement",
    "learning",
    "model",
    "going",
    "using",
    "dqn",
    "particular",
    "video",
    "try",
    "testing",
    "others",
    "see",
    "go",
    "also",
    "specific",
    "policy",
    "within",
    "reinforcement",
    "learning",
    "got",
    "different",
    "styles",
    "got",
    "reinforcement",
    "learning",
    "also",
    "got",
    "reinforcement",
    "learning",
    "case",
    "going",
    "using",
    "reinforcement",
    "learning",
    "specific",
    "policy",
    "going",
    "using",
    "boltzmann",
    "q",
    "policy",
    "see",
    "last",
    "thing",
    "gone",
    "imported",
    "sequential",
    "memory",
    "dqn",
    "agent",
    "going",
    "need",
    "maintain",
    "memory",
    "sequential",
    "memory",
    "class",
    "allows",
    "us",
    "go",
    "set",
    "agent",
    "going",
    "wrap",
    "inside",
    "function",
    "reproduce",
    "want",
    "reload",
    "memory",
    "let",
    "go",
    "ahead",
    "build",
    "function",
    "function",
    "defined",
    "basically",
    "done",
    "named",
    "function",
    "build",
    "agent",
    "pass",
    "model",
    "deep",
    "learning",
    "model",
    "specified",
    "also",
    "passing",
    "different",
    "actions",
    "take",
    "two",
    "different",
    "actions",
    "left",
    "right",
    "available",
    "within",
    "environment",
    "set",
    "policy",
    "set",
    "memory",
    "set",
    "dqn",
    "agent",
    "dqn",
    "agent",
    "actually",
    "pass",
    "deep",
    "learning",
    "model",
    "memory",
    "policy",
    "well",
    "number",
    "keyword",
    "arguments",
    "return",
    "dqn",
    "agent",
    "let",
    "go",
    "ahead",
    "actually",
    "use",
    "dqn",
    "agent",
    "actually",
    "go",
    "train",
    "reinforcement",
    "learning",
    "model",
    "first",
    "want",
    "start",
    "instantiating",
    "dqm",
    "model",
    "going",
    "compile",
    "going",
    "go",
    "ahead",
    "fit",
    "right",
    "go",
    "see",
    "dqn",
    "model",
    "starting",
    "train",
    "actually",
    "instantiated",
    "used",
    "build",
    "agent",
    "function",
    "set",
    "new",
    "dqm",
    "model",
    "passed",
    "model",
    "well",
    "actions",
    "compiled",
    "passed",
    "optimizer",
    "atom",
    "optimizer",
    "imported",
    "right",
    "start",
    "also",
    "passed",
    "metrics",
    "want",
    "track",
    "case",
    "mean",
    "absolute",
    "error",
    "use",
    "fit",
    "function",
    "kick",
    "training",
    "pass",
    "entire",
    "environment",
    "number",
    "steps",
    "want",
    "take",
    "whether",
    "want",
    "visualize",
    "take",
    "look",
    "second",
    "also",
    "specified",
    "verbose",
    "one",
    "want",
    "full",
    "logging",
    "want",
    "little",
    "bit",
    "logging",
    "let",
    "go",
    "ahead",
    "train",
    "take",
    "couple",
    "minutes",
    "fully",
    "built",
    "reinforcement",
    "learning",
    "model",
    "five",
    "minutes",
    "later",
    "sweet",
    "reinforcement",
    "learning",
    "model",
    "done",
    "dusted",
    "trained",
    "took",
    "256",
    "seconds",
    "go",
    "train",
    "see",
    "fourth",
    "interval",
    "accumulating",
    "reward",
    "200",
    "go",
    "actually",
    "print",
    "see",
    "total",
    "scores",
    "remember",
    "started",
    "taking",
    "random",
    "steps",
    "getting",
    "maximum",
    "score",
    "51",
    "great",
    "considering",
    "total",
    "maximum",
    "score",
    "game",
    "let",
    "go",
    "test",
    "see",
    "actually",
    "looks",
    "like",
    "actually",
    "performing",
    "using",
    "method",
    "let",
    "try",
    "right",
    "looking",
    "better",
    "already",
    "see",
    "virtually",
    "every",
    "single",
    "episode",
    "getting",
    "score",
    "200",
    "mean",
    "order",
    "test",
    "accessed",
    "dqn",
    "model",
    "use",
    "test",
    "method",
    "pass",
    "actual",
    "environment",
    "number",
    "games",
    "want",
    "run",
    "case",
    "called",
    "episodes",
    "ran",
    "100",
    "games",
    "whether",
    "want",
    "visualize",
    "outputted",
    "mean",
    "result",
    "wanted",
    "actually",
    "visualize",
    "difference",
    "well",
    "see",
    "model",
    "performing",
    "way",
    "better",
    "see",
    "actually",
    "able",
    "balance",
    "pole",
    "whole",
    "lot",
    "better",
    "randomly",
    "sort",
    "flailing",
    "test",
    "time",
    "rather",
    "five",
    "episodes",
    "say",
    "wanted",
    "um",
    "15",
    "example",
    "see",
    "model",
    "performing",
    "way",
    "better",
    "initially",
    "actually",
    "able",
    "reiterate",
    "resort",
    "balanced",
    "make",
    "sure",
    "pole",
    "stays",
    "straight",
    "brings",
    "tear",
    "eye",
    "good",
    "sweet",
    "done",
    "happens",
    "actually",
    "wanted",
    "go",
    "save",
    "model",
    "away",
    "use",
    "later",
    "say",
    "example",
    "wanted",
    "go",
    "deploy",
    "production",
    "well",
    "actually",
    "actually",
    "save",
    "weights",
    "dqm",
    "model",
    "reload",
    "later",
    "try",
    "test",
    "using",
    "save",
    "weights",
    "method",
    "dqm",
    "model",
    "let",
    "go",
    "ahead",
    "save",
    "weights",
    "blast",
    "away",
    "stuff",
    "created",
    "rebuild",
    "reloading",
    "weights",
    "gone",
    "saved",
    "weight",
    "actually",
    "take",
    "look",
    "folder",
    "see",
    "gone",
    "generated",
    "two",
    "different",
    "h5f",
    "files",
    "basically",
    "allow",
    "us",
    "save",
    "reinforcement",
    "learning",
    "model",
    "weights",
    "wanted",
    "go",
    "rebuild",
    "agent",
    "first",
    "let",
    "start",
    "deleting",
    "model",
    "deleting",
    "environment",
    "deleting",
    "dqn",
    "agent",
    "rebuild",
    "using",
    "functions",
    "reload",
    "weights",
    "test",
    "go",
    "see",
    "go",
    "try",
    "use",
    "method",
    "nothing",
    "gone",
    "deleted",
    "go",
    "rebuild",
    "environment",
    "test",
    "let",
    "go",
    "perfect",
    "gone",
    "reinstantiated",
    "models",
    "first",
    "built",
    "environment",
    "extracted",
    "actions",
    "states",
    "like",
    "used",
    "build",
    "model",
    "build",
    "agent",
    "functions",
    "go",
    "rebuild",
    "deep",
    "learning",
    "model",
    "reinstantiate",
    "dqn",
    "agent",
    "last",
    "least",
    "compiled",
    "actually",
    "reload",
    "weights",
    "model",
    "test",
    "order",
    "use",
    "dqn",
    "dot",
    "load",
    "weights",
    "method",
    "use",
    "save",
    "weights",
    "load",
    "weights",
    "order",
    "file",
    "going",
    "pass",
    "load",
    "weights",
    "method",
    "one",
    "exported",
    "copy",
    "paste",
    "gone",
    "reloaded",
    "weights",
    "actually",
    "go",
    "test",
    "environment",
    "ideally",
    "get",
    "similar",
    "results",
    "see",
    "performing",
    "well",
    "performing",
    "well",
    "deleted",
    "weights",
    "went",
    "reloaded",
    "wraps",
    "video",
    "covered",
    "bunch",
    "stuff",
    "specifically",
    "went",
    "installed",
    "dependencies",
    "created",
    "random",
    "environment",
    "using",
    "open",
    "ai",
    "gym",
    "got",
    "maximum",
    "score",
    "51",
    "built",
    "deep",
    "learning",
    "model",
    "using",
    "keras",
    "use",
    "keras",
    "rl",
    "train",
    "using",
    "reinforcement",
    "learning",
    "last",
    "least",
    "went",
    "reloaded",
    "agent",
    "memory",
    "allows",
    "work",
    "inside",
    "production",
    "environment",
    "want",
    "go",
    "deploy",
    "wraps",
    "thanks",
    "much",
    "tuning",
    "guys",
    "hopefully",
    "found",
    "video",
    "useful",
    "sure",
    "give",
    "thumbs",
    "hit",
    "subscribe",
    "tick",
    "bell",
    "get",
    "notified",
    "release",
    "future",
    "videos",
    "questions",
    "need",
    "help",
    "sure",
    "drop",
    "mention",
    "comments",
    "get",
    "right",
    "back",
    "course",
    "materials",
    "including",
    "github",
    "repository",
    "well",
    "links",
    "documentation",
    "available",
    "description",
    "get",
    "kickstart",
    "get",
    "running",
    "reinforcement",
    "learning",
    "model",
    "thanks",
    "tuning",
    "peace"
  ],
  "keywords": [
    "reinforcement",
    "learning",
    "video",
    "going",
    "bit",
    "deep",
    "two",
    "key",
    "different",
    "train",
    "environment",
    "way",
    "remember",
    "need",
    "area",
    "51",
    "well",
    "action",
    "reward",
    "agent",
    "four",
    "things",
    "model",
    "let",
    "take",
    "look",
    "get",
    "start",
    "using",
    "open",
    "ai",
    "gym",
    "build",
    "tensorflow",
    "keras",
    "pass",
    "order",
    "working",
    "within",
    "specifically",
    "inside",
    "built",
    "able",
    "save",
    "memory",
    "reload",
    "want",
    "deploy",
    "production",
    "first",
    "dependencies",
    "allow",
    "us",
    "see",
    "actually",
    "later",
    "sequential",
    "last",
    "least",
    "done",
    "installed",
    "used",
    "also",
    "go",
    "set",
    "random",
    "bunch",
    "use",
    "test",
    "got",
    "wanted",
    "game",
    "carpol",
    "basically",
    "pole",
    "step",
    "maximum",
    "200",
    "ideally",
    "steps",
    "left",
    "right",
    "actions",
    "available",
    "work",
    "number",
    "import",
    "imported",
    "went",
    "method",
    "extracted",
    "states",
    "taking",
    "getting",
    "specific",
    "moving",
    "visualize",
    "looks",
    "like",
    "score",
    "one",
    "whether",
    "passed",
    "ahead",
    "dense",
    "node",
    "optimizer",
    "function",
    "gone",
    "passing",
    "mean",
    "rl",
    "dqm",
    "dqn",
    "try",
    "policy",
    "performing",
    "better",
    "weights",
    "rebuild"
  ]
}