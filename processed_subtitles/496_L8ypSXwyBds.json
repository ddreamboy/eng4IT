{
  "text": "patrick lober is a popular python\ninstructor and in this course he will\nteach you how to train an artificial\nintelligence to play a snake game using\nreinforcement learning hey guys today i\nhave a very exciting project for you we\nare going to build an ai that teaches\nitself how to play snake and we will\nbuild everything from scratch so we\nstart by creating the game with pygame\nand then we build an agent and a deep\nlearning algorithm with pie torch i will\nalso teach you the basics of\nreinforcement learning that we need to\nunderstand how all of this works so i\nthink this is going to be pretty cool\nand now before we start let me show you\nthe final project\nso i can start the script by saying\npython agents dot pi now this will start\ntraining our agent\nand here we see our game and then here i\nalso plot the scores and then the\naverage score\nand now let me also start a stopwatch so\nthat you can see that all of this is\nhappening live\nand now at this point our snake knows\nabsolutely nothing about the game it\nonly is aware of the environment and\ntries to make some more or less random\nmoves but with each move and especially\nwith each game it learns more and more\nand then knows how to play the game and\nit should get better and better\nso the first few games you won't see a\nlot of improvements but don't worry\nthat's absolutely normal i can tell you\nthat it takes around 80 to 100 games\nuntil our ai has a good game strategy\nand this will take around 10 minutes\nalso you don't need a gpu for this so\nall of this training can happen on this\ncpu that's totally fine\nokay so let me speed this up a little\nbit\n[Music]\n[Music]\nall right so now about 10 minutes have\npassed and we are at about game 90 i\nguess and now we can clearly see that\nour snake knows what it should do so\nit's more or less going straight for the\nfood and tries not to hit the boundaries\nso it's not perfect at this point but we\ncan see that it's getting better and\nbetter\nso we also see that the average score\nhere is increasing and now the per the\nbest score so far is\nand to be honest for me this is super\nexciting so if you imagine that at the\nbeginning our snake didn't know anything\nabout the game and now with a little bit\nof math behind the scenes it's clearly\nfollowing a strategy\nso this is just super cool don't you\nthink all right so let me speed this up\na little bit more\n[Music]\nall right so after 12 minutes our snake\nis getting better and better so i think\nyou can clearly see that our algorithm\nworks\nso now let me stop this and then let's\nstart with the theory\nso i will split the series into four\nparts in this first video we learn a\nlittle bit about the theory of\nreinforcement learning in the second\npart we implement the actual game or\nalso called the environment here with\npygame\nthen we implement the agent so i will\ntell you what this means in a second and\nin the last part we implement the actual\nmodel with pytorch\nso let's start with a little bit of\ntheory about reinforcement learning\nso this is the definition from wikipedia\nso reinforcement learning is an area of\nmachine learning concerned with how\nsoftware agents ought to take actions in\nan environment in order to maximize the\nnotion of cumulative reward so this\nmight sound a little bit complicated so\nin other words we can also say that\nreinforcement learning is teaching a\nsoftware agent how to behave in an\nenvironment by telling it how good it's\ndoing\nso what we should remember here is that\nwe have an a chance so that's basically\nour computer player then we have an\nenvironment so this is our game in this\ncase\nand then we give the agent a reward so\nwith this we tell it how good it's doing\nand then based on their reward it should\ntry to find the best next action\nso yeah that's reinforcement learning\nand to train the agent there are a lot\nof different approaches and not all of\nthem involve deep learning but in our\ncase we use deep learning and this is\nalso called steep q learning so this\napproach extends reinforcement learning\nby using a deep neural network to\npredict the actions and that's we're\ngoing to use in this tutorial\nall right so let me show you the rough\noverview of how i organized the code so\nas i said we're having four parts so in\nthe next part we implement the game with\npie game then we implement the agent and\nthen we implement the model with pie\ntorch\nso\nour game has to be assigned such that we\nhave a game loop and then with each\ngame loop we do a play step\nthat gets an action\nand then it does a step so it moves the\nsnake and then after the move it returns\nthe current reward and if we are game\nover or not and then also the current\nscore\nthen we have the agent and the agent\nbasically puts everything together so\nthat's why it must know about the game\nand it also knows about the model so we\nstore both both of them in our agent and\nthen we implement the training loop so\nthis is roughly what we have to do\nso\nbased on the game we have to calculate a\nstate\nand then based on the state we um\ncalculate the next action\nand this involves calling model predict\nand then with this new action\nwe do a next play step and then as i\nsaid we get a reward the game overstate\nand the score\nand now with this information we\ncalculate a new state\nand then we remember all of this so we\nstore the new state and the old state\nand the game over state and the score\nand with this we then train our model\nso for the model i call this linear q\nnet so this is not too complicated this\nis just a feed forward neural net\nwith a few linear layers and it needs to\nhave the these information so the new\nstate and the old state and then we can\ntrain the model and we can call model\npredict\nand then this gets us the next action so\nyeah this is\na rough overview how the code should\nlook like and now let's talk about some\nof those variables in more detail for\nexample the action or the state or the\nreward\nso let's start with the reward so that's\npretty easy\nso whenever our snake eats a food we\ngive it a plus 10 reward when we are\ngame over so when we die then we get -10\nand for everything else we just stay at\nzero so that's pretty simple\nthen we have the action so the action\ndetermines our next move\nso we could think that we have four\ndifferent actions so left right\nup and down\nbut if we design it like this then for\nexample what can happen is if we go\nright then we might take the action left\nand then we immediately die so this is\nbasically a 180 degree turn\nso we don't allow that so a better\napproach to design the action is to only\nuse\nthree different numbers\nand now this is dependent on the current\ndirection\nso\num 1 zero zero means we stay in the\ncurrent direction so we go straight so\nthis means if we go right then we stay\nright if we go left then we go left and\nso on\nthen if we have 0 1 0 this means we do a\nright turn and again this depends on the\ncurrent direction so if we go right and\ndo a right turn then we go\ndown next if we go down and do a right\nturn again then we go left and then\nagain we would go up\nso this is the right turn and the left\nturn is the other way around so if we go\nleft and do a left turn then we go down\nand so on so with this approach we\ncannot do a 180 degree turn and we also\nwe only have to predict three different\nstates so this will make it a little bit\neasier for our model\nso now we have the reward and the action\nthen we also need to calculate the state\nand the state means that we have to tell\nour snake some information about the\ngame that it knows about so it needs to\nknow about the environment\nand in this case our state has 11 values\nso it has the information if the danger\nis straight or if it's ahead if the\ndanger is right or if the danger is left\nthen it has um the current direction so\ndirection left right up and down\nand then it has the information if the\nfood is left or right or up or down\nand all of these are boolean values so\nlet me show you an actual example so in\nthis case\nif we are going right and our food is\nhere\nthen we see um danger straight right and\nleft none of this is true\nso\nfor example if our snake is over here at\nthis end and it's still going right\nthen danger straight would be a one\nso this again also depends on the\ncurrent direction for example if we move\nup at this corner here\nthen danger right would be a1\nthen for these directions only one of\nthem is one and the rest is always zero\nso in this case we have danger right set\nto one\nand then for this in our case our food\nis right of the snake and also down of\nthe snake so food right is one and food\ndown as one all right so now with the\nstate and the action we can design our\nmodel so this is just a feed forward\nneural net with an input layer a hidden\nlayer and an output layer and for the\ninput it gets the state so as i said we\nhave 11 different numbers in our state\n11 different boolean values zero or one\nso we need this size 11 at the beginning\nthen we can choose a hidden um size\nand for the output we need three outputs\nbecause then we predict the action so\nthis can be some numbers\nand these don't need to be probabilities\nso here we can have raw numbers and then\nwe simply choose the maximum so for\nexample if we\ntake 1 0 zero and if we go back then we\nsee this would be the action straight so\nkeep the current direction\nso yeah that's how our model looks like\nand of course now we have to train the\nmodel\nso for this let's talk a little bit\nabout this deep q learning\nso q\nstands for this is the q value and this\nstands for the quality of the action\nso this is what we want to improve so\neach actions should improve the quality\nof the snake\nso we start by initializing the q value\nso in this case we initialize our model\nwith some random parameters\nthen we choose an action by calling\nmodel predict state and we also\nsometimes choose a random move so we do\nthis especially at the beginning when we\ndon't know a lot about the game yet\nso\nand then later we have to do a trade-off\nwhen we don't want to do a random move\nanymore and only call model predict\nand this is also called a trade-off\nbetween exploration and exploitation\nso this will get clearer later when we\ndo the actual coding\nso then with this new action we perform\nthis action so we perform the next move\nand then we measure the reward and with\nthis information we can update our q\nvalue and then train the model and then\nwe repeat this step so this is an\niterative\ntraining loop so now to train the model\nas always we need to have some kind of\nloss function that we want to optimize\nor minimize so for the loss function we\nhave to look at a little bit of math and\nfor this i want to present you the\nso-called belmont equation\nso this might look scary so don't be\nscared here i will explain everything\nand actually it's not that difficult\nwhen we um understand this and then code\nthis later\nso\nwhat we want to do here we need to\nupdate the q value as i said here so\naccording to the belmont equation the\nnew q value is calculated like this so\nwe have the current q value\nplus the\nlearning rate and then we have the\nreward for taking that action at that\nstate\nplus a gamma parameter which is called\nthis count rate so don't worry about\nthis i will also show this later in the\ncode again\nand then we take the maximum expected\nfuture reward\ngiven the new state and all possible\nactions at that new state\nso\nyeah this looks scary but i will\nsimplify that for you and then it's\nactually not that difficult\nso the old q value is model predict with\nstate 0\nso if we go back at\nthis overview so the first time we say\nget state from the game this is our\nstate 0\nand then after we took this place step\nwe again measure or calculate the next\nstate so this is then our state one\nso with this information again our first\nqueue is\njust model predict with the old state\nand then the new queue is the reward\nplus our gamma value\ntimes the maximum value of the\nq state so again this is model predict\nbut this time we take state one\nand then with these two information our\nloss is simply\nthe q new minus q squared\nand yeah this is nothing else than the\nmean squared error so that's a very\nsimple error that we should already know\nabout and then this is what we must use\nin our optimization\nso yeah that's what we are going to use\nso we have to\nimplement all of these three classes\nand in the next video we start by\nimplementing the game\nin the last part i showed you all the\nnecessary theory that we need to know to\nget started with deep q learning\nand now we start implementing all of the\nparts so as i said we need to have a\ngame so the environment then we need an\nagent and we need a model\nso in this part we start by implementing\nthe game and we use pytorch for this\nso\nlet me actually start by creating a\nenvironment and we install all the\nnecessary dependencies that we need\nso in this case i use conda to manage\nthe environments and if you don't know\nhow to use conda then i have a tutorial\nfor you that i will link here\nbut yeah if you don't want to use connor\nyou can also just use a normal virtual\nand but i recommend to use a virtual and\nand now let's create a virtual and with\nconda create minus n and then give it a\nname for example pi game n and i also\nsay i want python equals 3.7\nall right so now this was created so now\nwe want to actuate it with conda\nactivate and then pie game n and hit\nenter and then we see the name of the\nenvironment in the front so this means\nthat we activated it successfully and\nnow we can start installing all what we\nneed so the first thing we want to\ninstall is pie game for our game so pip\ninstall pie game and hit enter\nso this is done the next thing we need\nis pytorch for our model later so for\nthis we can go to the official home page\nand on install\nand then here you can select your\noperating system so i use mac\nand i actually i want to say pip install\nand we don't need cuda support so only a\ncpu is fine\nand we don't need torch audio because we\ndon't work with audio files so we can\nonly grab this pip install torch torch\nvision\nand then paste it in here and hit enter\nand now this installs pytorch and all\nthe dependencies\nall right so this is done and then we\nneed two more things for plotting later\nso for this i say pip install\nmuch plot lip and we also want i python\nand then hit enter\nall right so this was successful as well\nand now we have everything we need so\nnow we can start implementing all the\ncodes and as a starting point i want to\ngrab the code from another tutorial that\ni did so you can find this on github and\nthen on my um account and then in the\nrepo python fun\nand here i actually have two snake games\nso and then we need this one snake pie\ngame\nand download this so you can do this and\ni already did this and have this here so\nif we open up the\neditor here i'm visuals using visual\nstudio code\nthen we can see we have exactly those\ntwo files\nand\nthen um the first thing i want to do is\ni want to run this file and test if this\nis actually working\nso right now this is just a normal\nsnake game that you have to control\nyourself so you have to\nuse the arrow keys so let's say python\nsnake game dot pi and then let's hope\nthat this is working so yeah so now i\ncan control the snake\nand i hope that i can eat the food yes\nand now if i hit the boundary then we\nare game over\nso this is working our environment is\nset up and now we can start implementing\nour code so we can change this so that\nwe can use this as a\nai controlled game\nso let me show you the overview from\nlast time so last time i told you that\nwe need a play step in our game and this\ngets an action and based on this action\nwe then take a move and then we must\nreturn a reward the game over state and\nthe current score\nso\nfirst let's\nwrite down all the things that we need\nto change here\nso first we want to have a reset\nfunction so after each\num game our agent should be able to\nreset the game and start with a new game\nthen we need to\nimplement the reward that our agent gets\nthen we need to change the play function\nso that it takes takes an action and\nthen um\nreturns a or\ncomputes the direction\nthen we also want to keep track of the\ncurrent frame or let's call this\ngame\niteration\nand for later we also need to have a\nchange in the if is collision function\nto check if this is a collision\nso\nfirst let's\nlet me go over this code quickly so what\nwe do here is we use pi game\nthen for the direction we use an enum\nthen for the point we use a named tuple\nand then here i created a class snake\ngame and here we initialize the things\nwe need for the game\nso here we initialize the game state for\nexample for the snake we use a\nlist with\nthree initial values\nand the head is always the front of this\nlist\nthen we keep track of the score and here\nwe have a helper function to place the\nfood\nand yeah and we already\nhave a function that is called play step\nand then if we go down to the very end\nso here we have our\ngame loop so while this is true we take\na game or a play step\nand we get the game over state and the\nscore so this place the function is the\nmost important one so here first we\nright now we grab the user input so the\nkey we press\nthen we calculate a move based on this\nkey\nand then we update our snake and check\nif we are game over and\nif we can continue we place the new food\nor check if we eat the food\nand we update our ui with this\nhelper function update ui then here we\nhave this helper function is collision\nwhere we check if we either hit the\nboundary or we run into ourself\nand then we also have this helper\nfunction move where we get the current\ndirection and then based on this\ndirection we simply um calculate\ncalculate the new position of the new\nhat\nso yeah that's all um\nwhat is done here and now let's change a\nfew things though so the first one i\nwant to change the class name to say\nsnake game ai to make it clear that this\nis a agent controlled game\nand now so the first thing we want is\nthe reset functionality so in here um i\nalready have this comment where we in it\nthe game state so now we want to\nrefactor all of this into a\nreset function so we create a new\nfunction define\nand then let's call this reset and it\nonly gets self and no other arguments\nand here we can grab all of this code\nand then simply paste it in here\nand in our\ninitializer we then call self dot reset\nso this is the first thing we need\nadditionally we want to keep track of\nthe um\ngame iteration or frame iteration so\nlet's call this self dot frame\niteration\nand in the beginning this is just zero\nthen this define place food can stay as\nit is and now we need to change the play\nstep function so first of all\nif we have a look at the overview\nhere i already told you that now we need\nto give this the action from the agent\nand we need to return a reward so let's\nstart by\num using this action parameter\nand here we grab the user input so\nactually right now we can get rid of\nthis so the only thing we still check if\nwe want to quit the game and now here um\nwe already\nhave this helper function where we move\nin the current direction\nso actually what we change here now this\nmove function doesn't get the direction\nfrom the user input so now here it gets\nthe action and then we have to determine\nthe new direction\nso we do this in a second but first\nlet's only change this and then here we\ncall the self.move\nwith the action\nand then we update the head then we\ncheck if we are game over or not and we\nactually now we also need the reward so\nwe simply say reward equals zero\nand let's go back to the slides from\nlast time so the reward is really simple\nwhenever we eat a food we say plus 10\nwhen we lose or when we die then we say\nour reward is -10 and for everything\nelse we just stay at zero\nso we initialize the reward with zero\nthen if we have a collision and game\nover then we say our\nreward equals to -10 and we want to\nreturn this as well so return the reward\ngame over and self.score\nand here we check only if we have a\ncollision so here i actually want to do\nanother\ncheck so if\nnothing happens for a long time so if\nour snake doesn't improve and doesn't\neat the food but also doesn't die then\nwe also want to check this\nand if this happens for a too long time\nthen we also break here\nso we can say or and then here we say if\nself dot frame iteration\nand if that this gets too large without\nanything happening then we um stop here\nso here i use this little formula if\nthis is greater than 100 times the\nlength of our snake so remember this is\na list\nthen we break so this is also like this\nthen it's dependent on the\nlength of the snake\nso the longer our snake is the more time\nit has\nso but then if it gets larger than this\nvalue then we break\nand of course we have to\nupdate the self.frame iteration and we\ncan simply do this here at the beginning\nso for each\nplay step we say self dot frame\niteration plus\nequals\n1\nand when we reset it then we reset it\nback to zero\nso this is here and then yeah if we stop\nwe have the reward -10\nthen here if our hat\nhits the food then we eat the food so\nour score increases and our reward is\nset to plus 10\nthen we place a new food and say\notherwise we remove the last part so we\nsimply move here\nthen this can stay as it is the update\nfunction and at the very end we also\nwant to return the reward then\nfor the is collision function we need a\nslight change so here i only check for\nself.head but later um to calculate the\nstate or the danger which i told you\nabout so if we\nhave a look at the\nstate so here we calculate the\ndanger so if we are for example if we\nare here at the corner then we have a\ndanger at the right\nso for this it might be handy if we\ndon't use self.head inside here\nbut if we give this function a point so\nthis gets the point argument and let's\nsay by default this is none\nand then here we simply ch check if the\npoint is\nnone then we set the point equals to\nself dot head\nso inside this where we call this with\nno argument it can stay as it is\nand then here of course we have to\nchange self.head to this is now our\npoint so here if we hit the\ncorner\npoint here and point here and point\nhere\nthen we have a collision and here if our\npoint is in the snake body then we also\nhave a collision\nand otherwise we don't have a collision\nall right so the update ui function can\nstay like this\nand now for the move function here we\nneed to change something so now we get a\naction and now based on this action we\nwant to determine the next move\nso if we go back to the slides so here\nwe designed the action like this so it\nhas three values\num one zero zero means we keep the\ncurrent direction and go straight 0 1 0\nmeans we do a right turn and 0 0 1 means\nwe do a left turn\nso this is dependent on the current\ndirection so if we go right and do a\nright turn then we go down next if we go\ndown and do a right turn then we go left\nnext and so on and left turn is the\nother way around\nso now um we want to determine the\ndirection based on the action so let's\nwrite a quick comment here we\nhave straight right turn or left turn so\nto get the next direction first i want\nto\ndefine all the possible directions in a\nclockwise order so we say clockwise\nequals and then a list and here we start\nwith direction dot right so\nhere remember for the direction we use\nthis enum class\nso it has to be one of those directions\nso\nour\nclockwise directions should start with\ndirection right then from this on the\nnext one is\ndirection dot down\nthen we have direction dot\nleft and as last thing we have direction\ndot up so right down left up this is\nclockwise and then to get the current\ndirection or the current index of the\ncurrent direction we say index equals\nand then we can say clockwise dot index\nand then the index of the self dot\ndirection so we are sure that this has\nto be in this array we because the self\ndirection\nmust be one of those enum values\nand then we check that different um\npossible states so these ones\nso for this we can use numpy and i guess\nwe have to import numpy first as np\nand then we can use it\nhere we can say if\nnumpy and then we use this function\narray equal and then here we put in the\naction and the array that we want to\ncompare\nso if this is equal to\none zero zero\nthen we go straight or we keep the\ncurrent directions so we simply say\nour new\ndirection\nequals and then clockwise\nof the\nindex and then remember the index is\njust the index of the current direction\nso here we basically have no\nchange then we say\nl if if our\narray if numpy array equal if the action\nequals to 0 one zero then we do a\nright turn\nso this means we go clockwise so if we\ngo right then the next direction would\nbe down if we go down then the next\ndirection would be left and if we go\nleft then the next\ndirection would be up so here we say\nindex\nequals or this is our next\nindex actually and here we say\nthis is the current index plus\n1 but then modulo 4 so this means if we\nare at the end up and then do the next\none if we have index\nso this is index 0 1 2 3 and then if we\nhave index\n4 modulo 4 is actually again index\nzero again so from this we do a turn and\nthen come back at the front again\nso this is our right turn so now this is\nthe\nnext index and now our new direction is\nclockwise of the\nnext\nindex\nand then otherwise we can simply use\nelse here and actually change this to an\nl if so now this is the last case so it\nhas to be here it has to be\nzero zero one\nand if this is the case then let's copy\nand paste this in here then our next\nindex is the current index\nminus one modulo four\nso this actually means we go\ncounter clockwise so we do a\nleft turn so if we start with right then\nthe next move would be up and then the\nnext would be\nleft and then the next would be down\nand then right again and so on so now\nthis is our new direction and then\nsimply we say self direction\nequals new\ndirection\nand then we go on so here we extract the\nhead\nand then here we have to check if self\ndot's direction now is right then we\nincrease the position of x\nand so on if we um have the left\ndirection then we decrease x\nand if we go down then we actually\nincrease y so for\nso the y starts at the top at zero and\nthen increases if we go down so if we go\ndown then we have to increase y and if\nwe go up then we have to decrease y so\nif self direction equals up\nthen y minus equals the block size and\nby the way the block size is just here a\nconstant value of 10 so that's how big\nour one block of the snake should be in\npixels\nso yeah this is everything we need here\nin the move function and now here\nwe don't need this anymore so\nthis is actually no longer working with\na user input so you can just\ndelete this and then later we control\nthis class from the agent\nand call this play step function\nso yeah for now this is all we need to\nimplement the game\nso i already talked about the theory of\ndeep q learning in the first part\nin the last part we implemented the pi\ngame so that we can use it in our\nagent controlled environment and\nnow we need the agent so let's start and\nso here um if you haven't watched the\nfirst two parts then i highly recommend\nto do so\nso this is the starting point from last\ntime and i actually want to make one\nmore change that i forgot\nso\nhere the is collision function should\nactually be public because then our\nagent should use it\nso just remove the underscore here and\nthen also remove it in this class itself\nwhen we call this\nso then we have our snake game and i\nalso want to rename this to just be game\nand now we create a new file agent dot\npi and then start implementing this\nso first here we import torch from pi\ntorch\nthen we import random because when later\nwe need this then we also need\nimport numpy snp\nand from our um implemented class we\nneed the snake game so we say from\ngame import snake\nlike snake game a i\nso i think that's what we call this\nclass snake game a\ni\nso yeah that's the right name then we\nalso hear at the beginning we defined\nthis enum for the direction and this\nnamed tuple for the point which has an x\nand a y attribute\nso we also want to import these two um\nthings so we import direction and we\nimport point and then we also say from\ncollections\nwe want to import deck so this is a data\nstructure where we want to store our\nmemories so\num if you don't know what a deck is then\ni will put a link in the description\nbelow\nso this is really handy in this case and\nyou will see why this is the case later\nand then here i want to define some\nparameters as constants so we have a\nmaximum memory of let's say 100\n000\nso we can store 100 000 items in this\nmemory\nthen we also want to to use a batch size\nthat you will see later and here i will\nset this to 1000 so you can play around\nwith these parameters\nand i also want a learning rate later\nand i want to set this to 0 0 1\nand yeah feel free to change this and\nthen we start creating our class agent\nand it gets of course an init function\nwith self and no other\narguments and then let's have a look at\nthe slides from the first part where i\nexplained the training\nso we want to create a training function\nwhere we do all of this so we need to\nget the state calculate the state\nwhere we are aware of the current\nenvironment then we need to calculate\nthe next move from the state\nand we need to\num then we want to update or do the next\nstep and call game.playstep and then\ncalculate the new state again then we\nwant to store everything in memory and\nthen we also want to train our model so\nwe need to store the game and the model\nin this class\nso first of all let me create the\nfunctions that we need first so we need\na function get state which gets self and\nthis this gets the game\nand then we calculate the state that i\nshowed you with these 11 different\nvariables then we want to have a\nfunction that we call remember\nremember and it has self and here we\nwant to put in the state then the action\nthen we want to remember the reward for\nthis action and we want to calculate or\nwe want to store the next state\nnext state\nand we also want to store done or bit or\nyou can also call this game over so this\nis the current game overstate then we\nneed two different functions to train\nand we call this defined train\non the long memory and it only needs\nself so i will explain this later\nand we also let's copy and paste this i\nalso have a function define train on\nshort memory so this is only with one\nstep\nyou will see this later\nthen we need a function and we call this\nget action to get the action based on\nthe state so it gets self and the state\nand first we only say pass\nand these are all the functions we need\ni guess and then i want to have a global\nfunction that i call simply train\nand here we say pass and then when we\nstart this module h and dot pi so we say\nif name\nunderscore equals equals\nmain then we simply call this train\nfunction and then we can start the\nscript by saying python agent dot pi\nlike i did in the very first tutorial so\nlet's start implementing the agent and\nthe training function so let's start\nwith the init function of the agent so\nhere what i want to store is first i\nwant to store some more parameters so\nself.number of games so i want to keep\ntrack of this so this is zero in the\nbeginning\nthen self.epsilon\nequals um zero in the beginning this is\na parameter to control the\nrandomness so you will see this later\nthen we also need self dot gamma equals\nzero\nso this is\nthis is the so-called this count rate\nwhich i briefly showed in the first\ntutorial i will explain this a little\nbit more in the next tutorial where we\nimplement the model and the actual deep\nq learning algorithm then we want to\nhave a memory so we say\nself.memory equals and for this we use\nthis stack and this can have a argument\nmax leng equals\nand here we say max memory\nand what then happens if we exceed this\nmemory then it will automatically remove\nelements from the left so then it will\ncall pop left for us and that's why this\ndeck is really handy here\nand then later here we also want to have\nour model and the trainer so i will\nleave this for a or s to do for the last\npart in the next video and now this is\nall for the init function and now we can\ngo back and now let's do the training\nfunction next so again let's have a look\nat these slides so we need\nthese\nfunctions in this order\nso\nlet's first let's write some comments\nof first let's create some lists to keep\ntrack of the scores so this is an empty\nlist in the beginning and this is used\nfor plotting later\nso then we also want to keep track of\nthe mean scores or average scores this\nis also an empty list in the beginning\nthen our total score equals zero in the\nbeginning\nour record our best score is zero in the\nbeginning\nthen we set up a agent so agent equals\nan agent\nand we also need to create a game so the\ngame is a snake game ai object\nand then here we create our training\nloop so we say while true so this should\nbasically run forever until we quit the\nscript\nand now here let's write some comments\nso we want to get\nget the old state or the current state\nso here let's say state old equals and\nthen we call agent dot\nget states and this gets the game so we\nalready\nset this up correctly we only have to\nimplement it then\nthen after this we want to get the move\nbased on this current state\nso we say the final move equals agent\ndot\nget\naction so we actually called this action\nand the action is based on the state\nthen with this move we want to perform\nthe move and then and get new state so\nfor this we say\nreward um\ndone and score\nequals and here we call game dot play\nstep from last time\nso\ni think\ngame dot play step with the action yes\ngame dot play step and this gets the\nfinal move\nand then we get the state\nold or the new now the new state state\nnew\nstate new equals agent and again gets\nstate now with the new game\nthen after that we want to train the\nshort memory of the agent so only for\none step\nso\nfor this we say agent agent dot train\nshort memory\nand this gets if we have a look here um\nactually uh this short memory should get\nsome parameters so exactly the same as\nwe put in the remember function so train\nshort memory gets all of those variables\nand then here when we call this now we\nshould get some hints strain\nor let's\nsave this file and then say agent dot\ntrain short memory and now we should get\nthe hints no we don't get this but\nactually we want to have the\nstate action reward next state and done\nso here let's do this so say let's say\nstate old then the action which was the\nfinal move then the reward\nthen the state new and adds last thing\nthe done or game over state variable so\nnow we have this then we want to\nremember all of these and store this in\nthe memory so we say agent dot remember\nand then here it gets the same\num variables so we want to store all of\nthis in our deck and\nthen this is all we need so now we check\nif\ndone or if game over then if this is\ntrue\nthen what we want to do is um\nwe want to let's write a comment train\nthe long memory and this is also called\nreplay memory or experience replay and\nthis is very important for our agent so\nnow it trains again on all the previous\nmoves and games that it played\nand this tremendously helps him to\nimprove itself\nand we also here want to plot the\nresults so first of all we want to reset\nthe game so we can simply do this by\nsaying game dot reset we already have\nthis function\nhere so this initializes the game state\nand resets everything so the score the\nsnake the frame iteration and places the\ninitial snake and the food\nso now we have this then we want to\nincrease agents dot number of games so\nthis\nplus equals one\nthen we want to say agent dot train long\nmemory and this doesn't need any\narguments then we want to check if we\nhave a new high score so if score\ngreater than the current record then we\nsay record equals our new score\nand we will also want to leave this as a\nto do here so here we want to say agent\ndot model\ndot save later when we have the model\nand so here in the here we want to store\nthis as self.model\nand now what we also want to do here um\nlet's print some information so print\nthe\ngame and then the current number and\nthen the score and the record\nso here let's say our game is agent dot\nn\ngames\nthen we also want to plot the or print\nthe score so this is just the score\nand we want to print the current record\nso the record equals record\nand then here we want to do some\nplotting so i will implement this in the\nnext tutorial so i will leave this s8 to\ndo\nso this is all for our training function\nso what i showed in the slides\nand now of course we have to implement\nthose functions\nso for the\nget\nstate function um let's go back to this\noverview\nand here as i said we store 11 values so\nif the danger is\nstraight right or left then the current\ndirection so only one of these is one\nand then the position of the food if\nit's left of the snake right of the\nsnake up or down of the snake\nso these are the 11 states and now let\nme actually copy and paste the code in\nhere so that i don't make any\nmistakes\nbut we will go over this\nso first let's grab the head from this\ngame so we can do this by calling game\ndot snake zero so this is a list and the\nfirst item is our head\nthen um let's create some points\nnext to this head in all directions that\nwe need to check if this hits the\nboundary and if this is a danger\nso for this we can use this named tuple\nso we can create a point\nwith this location but minus 20 so the\n20 is hard coded here so this is the\nnumber that i used for the block size\nso like this we create four points\naround the head then the current\ndirection is simply a boolean where we\ncheck if the current game direction\nequals to one of those\nso only one of those is one and the\nother one is\nzero or false\nand then\num we create this\narray or this list with this 11 um\nstates so\nhere we check\nthat if the danger is straight or ahead\nand this is dependent on the current\ndirection so if we are going right\nand the point right of us gives us a\ncollision\nthen we have a danger the same or or if\nwe go left and our\nleft point gets a collision then we also\nhave a danger here and so on so this is\ndangerous straight and then danger right\nmeans if we go up\nand the point right of us would give a\ncollision then we have a danger for a\nright turn basically\nand so on and the same for the left so\nthis might be a little bit tricky so i\nrecommend that you pause here and go\nover this logic for yourself again\nso yeah these only have give us three\nvalues in our state so far\nthen we have the move direction where\nonly one of them is true and the other\none is false\nand for the food location we simply\ncheck if food if game food x is\nsmaller than game head x then we have\nfood is left of us\nand the same way we check for right up\nand down and then we convert our list to\na numpy array and say the data type is\nin so this is a nice little trick to\nconvert this true or false booleans to a\nzero or one\nso yeah now this is the get state method\nnow let's move on to the remember\nfunction so here we want to remember all\nof this in our memory so this is a deck\nand this is very simple so here we say\nself dot memory and then the deck has\nalso the append function where we want\nto append all of this\nin this order so the state the action\nthe reward the next state and the game\nover state\nand as i said if this exceeds the\nmaximum memory\nthen pop left if\nmax\nmem\nmemory is\nreached and yeah this is the remember\nfunction\nthen let's start implementing the train\nlong and short memory functions so for\nthis so we actually we store a model and\na trainer in here so let's actually say\nself dot\nmodel equals let's say this is only none\nin the beginning and leave a to do\nand self dot trainer equals none in the\nbeginning and this is a to do\nso these are objects that we create in\nthe next tutorial\nand then here\nwe call this trainer to actually do the\noptimization\nso let's start here so for only one step\nwe say\nself.trainer and then this should get a\nfunction that we call let's call this\ntrain step\nand then it gets all of these variables\nso the state the action the reward the\nnext state and the game overstate and\nthis is all that we need to train it for\nonly one game step\nand we design this function um so that\nit takes either only one state like this\nbut it can also take a whole tensor or a\nnumpy array and then uses multiple as a\nso-called batch so let's do this here so\nfor this we take the variables from our\nmemory\nso here we want to grab a batch and so\nin the beginning we defined the batch\nsize is 1 000 so we want to grab 1 000\nsamples from our memory\nbut first we check if we um already have\na thousand samples in our memory\nso we say if lang and self dot memory if\nthis is\nsmaller\nthen the batch size then we simply\nor actually let's say if this is greater\nso if this is greater than we want to\nhave a random sample and say\nmini\nsample equals and then we want to get a\nrandom sample so we can use random dot\nsample so we already imported the random\nmodule\nrandom dot sample from self dot\nmemory and as a size it should have the\nbatch size\nso this will return a list of tuples and\nthis is because here i forgot one\nimportant thing so when we want to store\nthis and append this we want to append\nthis as only one element so only one\ntuple so we need extra parenthesis here\nso this is one tuple that we store\nand then here we get\nthe batch size number of tuples\nand otherwise else if we don't have uh\na thousand elements yet then we simply\ntake the whole memory so we say mini\nsample equals self dot\nmemory and\nthen we again want to call this training\nstep and for this so let's call this\nhere again self.trainer.trainstep\nbut here we have\nmultiple states so let's call this\nstates actions rewards next states and\ndone\nand right now so now we have it in\nthis format that we have one\ntuple after another\nand now we want to extract this from the\nmini sample and then put every states\ntogether every action together every\nreward to it together and so on and this\nis actually a really simple with python\nso we can say we want to extract the\nstates the actions\nthe rewards the next\nstates and the\nduns game overs\nand here we simply use the built in sip\nfunction and have to use one asterisk\nand then the mini sample argument\nso yeah check that for yourself if you\ndon't know how the sip function works\nbut again it now it puts every states\ntogether every actions and so on if this\nis too complicated for you then you can\nalso just do a for loop so you can\niterate over this mini sample and\nbasically say for\naction\nor for state\naction\nrewards\nnext state and done in one mini sample\nand then again you call\nthis here for only one for only one\nargument so yeah you can do it both ways\nbut actually i recommend to do it this\nway because then you have this as only\none argument and then you can do this\nfaster in pytorch all right so now we\nhave both the training functions now we\nonly need the get action function so\nhere in the beginning we want to do some\nrandom moves and this is also called a\ntrade-off between\nexploration and\nexploitation in deep learning so at some\npoint or in the beginning one we want to\nmake sure that we also make random moves\nand explore the environment but then the\nbetter our\nmodel or our agent gets the less random\nmoves we want to have and the more we\nwant to exploit our agent or our model\nso\nyeah this is what we want to do here so\nfor this we use this\nepsilon parameter that we\ninitialized in the beginning\nso for this let's implement this first\nso we say self dot epsilon equals and\nthis is dependent on the number of games\nso here i hard code this to 80 minus\nself dot number of games you can play\naround with this and then let's get the\nfinal move so in the beginning\nwe say zero zero zero and then one of\nthose now has to be true\nso here first let's check if random dot\nrent int and here between 0 and 200\nif this is smaller than self dot epsilon\nthen we take a random move so we say\nmove equals\nrandom dot rant ins and this must be\nbetween 0 and 2 so the 2 is actually\nincluded here and this will give us a\nrandom\nvalue 0 1 or 2 and now this index must\nbe set to one so we say final move of\nthis\nmove index equals one\nand yeah so so the more games we have\nthe smaller our epsilon will get\nand the smaller the epsilon will get\nthe less frequent this will be\nless than the epsilon\nand when this is even this can even\nbecome negative and then we don't longer\nhave a random move so again if this was\ntoo fast here then feel free to pause\nand think about this logic again\nso now we have that and otherwise else\nso here we actually here we want to do a\nmove that is based on our model so we\nwant to get a prediction prediction\nequals self dot model dot predict and it\nwants to predict the action based on one\nstate so we call the state zero\nand we get this here but we want to\nconvert this to a tensor so we say state\n0\nequals torch dot tensor and as an input\nit gets the state\nand we also give it a data type equals\nlet's use a torch dot float here then we\ncall self.model predict with the state\nthis will give us a prediction and this\ncan be a raw value so if we go\nback to this slide\nso this can be a raw value and then we\ntake the maximum of this and set this\nindex to a1\nso here we say our move equals and we\nget this by saying torch arc max and the\narc max of the prediction and this is a\ntensor again and to convert this to only\none number we can call the item and now\nthis is an integer and now again we can\nsay final move of the smooth index is\none\nand now we have this so now we return\nthe final move here\nreturn\nand yeah this is all we need so now we\nhave this and can save it like this\nand now we have all that we need for our\nagent class and now in the next one\nso what we must do here is implement the\nmodel and the trainer and then also the\nplotting\nso let's go back to the code and here i\nleft this essay to do so we need a model\nand a trainer\nso let's create a new file and let's\ncall this model dot pi\nand then here let's first import all the\nthings we need so we need import torch\nthen we want to import torch dot n n s n\nn\nthen we want to import torch dot optim s\noptim and also import torch\ndot n n dot functional s capital f\nand we also want to import o s to save\nour model\nand now we want to implement two classes\none for the model and one for the\ntrainer so let's create a class and\nlet's call this linear underscore\nqnet and this has to inherit from nn dot\nmodule\nmodule\nand by the way if you are not\ncomfortable with pytorch and want to\nlearn how to use this framework then i\nhave a beginner series here on this\ntutorial for free and i will put the\nlink in the description so this will\nteach you everything to need to get\nstarted with pytorch\nso\nright now let's start implementing this\nlinear qnet function so we need the init\nfunction define init and we need to have\nself\nand this gets an input size\ninput size a\nhidden size and an output size\nand then the first thing we want to do\nis to call this super initializer so we\ncall super in it\nand here um this is very simple so if we\nhave a look at the slides\nthen our models should just be a feed\nforward neural net with a input layer a\nhidden layer and an output layer\num feel free to extend this and improve\nthis but it works fine for this case and\nit's actually not that bad here\nso let's create two linear layers so\nlet's call this self.linear1\nequals nn.linear\nand this gets the input size as an input\nand then the hidden size as the output\nsize\nthen we have self.linear2\nequals\nnn.linear and now this gets the hidden\nsize as the input and the output size as\nthe output then as always in pi torch we\nhave to implement the forward function\nwith self and it gets x so the tensor\nand here what we want to do is first we\nwant to apply the linear layer and we\nalso use an actuation function here so\nagain if you don't know what this is\nthen check out my beginner tutorial\nseries there i explain all of this so we\nsay x and then we can call f dot\nreloose we use this directly from the\nfunctional module\nand here we say self dot linear one with\nour tensor x as the input so first we do\nthe linear layer then we apply the\nactuation function\nand then again we apply the second layer\nso we call self dot linear 2 with x\nand we don't need an actuation function\nhere at the end we can simply use the\nraw numbers and return x\nso this is our forward function\nthen let's also implement a helper\nfunction to save the model later so\nlet's call this self\nsafe and this gets the file name as an\ninput and i use a default here\nso we say model dot pth is simply the\nfile name and then the last time i think\ni already\ncalled this function um so not yet but\nnow we can comment this out so if we\nhave a new high score then we call agent\ndot model dot save and here let's create\na new\nfolder in here so let's say this is the\nmodel folder path equals and let's\ncreate a new folder\nin the current directory and call this\nmodel so dot slash model and then we\ncheck if this already exists so the file\nin this folder\nso we can say if not os dot path dot\nexists and then we say our\nmodel folder path\nthen we create this so we say os dot\nmakers and we want to make this model\nfolder path\nthen we create this final file name so\nwe say file\nname equals os\ndot path dot join and we want to join\nthe model folder path and the file name\nthat we use here as the input now this\nis the file name for saving and then we\nwant to\nsave this and we can do this with torch\ndot save and we want to save self dot\nstate\ndict so i also have a tutorial about\nsaving the model we only need to save\nthis state dictionary\nand then as a path we use this file name\nso now this is all we need for our\nlinear q net\nand now to do the actual training and\noptimization we also do this in a class\nthat i call q\ntrainer q trainer\nand now here what we want to do we want\nto implement a init function\nwhich gets self then it also gets the\nmodel then it should get a learning rate\nand it should get a gamma parameter\nand here we simply store everything\nself.lr equals lr self dot gamma equals\ngamma\nand we also store the model so we say\nself dot model equals model\nthen to do a pie charge optimization\nstep we need a optimizer so we can\ncreate this by calling self.optim\nor let's call this optimizer\nequals and we get this from the opt-in\nmodule and here you can choose one\noptimizer so i use the atom optimizer\nand we want to optimize model.parameters\nand this is a function\nand then it also needs the learning rate\nso lr equals self dot l r\nand now we also need a\ncriterion or a loss function so let's\ncall this self dot criterion\nequals and now if we go back to these\nslides at the very end\nwe learned in the first part that this\nis nothing else than the mean squared\nerror so that's very simple\nso we can create this here by saying\nself.criterion equals so this is nn.mse\nloss and now this is what we need in our\ninitializer and then we also need to\ndefine a\nwe call this train step function which\ngets self\nand then it needs to have all the stored\num parameters from last time\nso it needs to have or let's have a look\nat\nthis\nso here when we call this it needs the\nstate the final move the reward the new\nstates and done\nso let's copy and paste this in here and\nrename this slightly so this is just the\nstate\nthis is the action this is the reward\nso this is the new state this can\nuh let's call this\nnext state here and then done can stay\nas it is\nand for now let's simply do\npass here and before we implement this\nlet's go back to the agent and now set\nthis up so here we say from\nand we call this model and we want to\nimport the linear i think we call this\nlinear q\nnet and q\ntrainer\nand then here\nin the initializer we want to create an\ninstance of the model and of the trainer\nso self.model equals our linear qnet and\nnow this needs the input size the hidden\nsize and the output size\nso here i use 11 256 and three\nso remember if we have a look at the\nslides again\num\nthe first one is the size of the state\nso this is 11 values and the output must\nbe three because and we have three\ndifferent\num\nthree different numbers in our action\nand you can play around with this hidden\nsize but the other ones have to be\neleven and three\nso this is the model and the trainer\nequals the q trainer and this gets the\nmodel so self.model then it gets the\nlearning rate equals the learning rate\nwhich we specified here\nand we also pass on the gamma value so\ngamma equals self dot gamma and the\ngamma is the discount rate so i this has\nto be a value that is smaller than 1 and\nusually this is around 0.8 or 0.9 so in\nthis case let's set this to 0.9\nso you can play around with this as well\nbut keep in mind that it must be smaller\nthan one so now we have this and then i\nmade one error in the last tutorial so\nthis is very important that we fix this\nright now\nso here in the get\naction function i actually\ni called this self.model\npredict but actually pythog doesn't have\na predict function so this would be the\napi for tensorflow for example\nso in pi torch we simply call self.model\nlike this\nand then this will execute this forward\nfunction so this is actually the\nprediction then\nso yeah\nplease make sure to fix this\nokay so now we have everything and if we\nhave a look and go back then we see we\ncall this\nself.trainer train step\nwith only one parameter but also with\nmultiple ones so we want to make sure\nthat we can handle different sizes\nso now let's start implementing this\nfunction and now the first thing we want\nto do\nso right now this can be um either a\ntuple or a list or just a single value\nso let's convert this to a pi torch\ntensor so let me copy and paste this in\nhere so we do this for the states the\nnext state the action and reward\nand we can do this by calling\ntorch.tensor\nand then the variable and we specify the\ndata type to torch dot float\nand we don't have to do this for the\ndone or game over value because we don't\nneed this as a tensor and now we want to\nhandle\nmultiple\nsizes so we want to check if the length\nand then we can\ncheck\nstate dot shape\nand if this is one then we only have one\ndimension and then we want to reshape\nthis so right now we only have\nif this is the case then we only have\none number\nbut actually we want to have it in the\nform one and then the values so this is\nthe number\nof um batches so if this is already\nif this has\nalready multiple values then it's\nalready in the in the size n\nand x\nso then it's already correct\nand now here we want to append one\ndimension and we can do this with the\ntorch unsqueeze function so we can say\nstate equals states dot or sorry not\nstate but\ntorch dot un squeeze\nsqueeze and then the\nstates and we want to\nput it in dimension\nzero\nor axis zero so this means that it\nappends one dimension in the beginning\nand this is then just one then i also\nwanted to do this for the other um\ntensors so for the next state and for\naction and reward\nand the done value we also want to\nconvert this right now this is only a\nsingle value and we want to convert this\nto a tuple so we can do it like this so\nnow we have a\ndone so this is how you define a tuple\nwith only one value\nand now um we have it in the correct\nshape so now what we have to implement\nis um from last time or from the very\nfirst tutorial where i showed this\nbellman equation\nand then we simplified this so we have\nthe old queue where we simply call model\npredict with the old state\nand the new queue with this formula so\nlet's do this so\nfirst let's um write a comment here so\nas first thing we want to\nget the predicted\npredicted q values with the current\nstate\nand this is simply by doing let's call\nthis prediction equals self dot model\nand then we want to do this with\nstate 0 or we just call this state here\nand then for the second part we need\nthis formula the reward plus the gamma\nvalue times the maximum of again model\npredict with state one\nso first let's write this as a new uh\ncomment so the first thing is we want to\napply this formula reward plus\ngamma times and then\nthe next\npredicted\nq value\nand\nthen we want to have the\nmaximum so the maximum of this so\nmaximum\nand then um this is a little bit tricky\nso the maximum of this um\nsorry let's do it like this maximum of\nnext predicted q value so this is only\none value\nbut um if we do it like\nas first\num parameter the predictions this has\nactually this is the action this is\nactually\nthree different values\nso\nwhat we do to get the same here is we do\na clone of this\nand then we set the\nindex\nwith this action to the new q value so\nthis is let's call this q new like i\nshowed you in the formula\nand then we set the\nlet's call this predictions and then the\nindex of\nthe\narc max of the action we set this to our\nq\nnew value\nso again this might be tricky so again\nwe want to calculate the new q value\nwith this formula that i showed you but\nthen we need to have it in the same\nformat and for this we simply clone this\nso then we have three values again\nand two of the values are the same but\nthe value with the\naction so the action is for example\none zero zero\nso um the index of the one is then set\nto the new q value\nso this is what we want to do here\nso for this let's first let's create a\nclone target\nequals prediction dot clone so we can do\nthis with a pi torch tensor and then um\nwe want to iterate over our tensors and\napply this formula so for this we say\nfor\nindex in\nrange and then the length of the let's\ncall this done and here everything\nshould have the same size so\nthen this works so now we iterate over\nthis and then one thing that i didn't\nmention so far is that we only only want\nto do this only do this if not done\num otherwise we simply take the whole\nreward so we say q new equals reward of\nthe current\ncurrent index and now we check if we are\nnot done so we say if\nnot\ndone and the done is of the current\nindex\nthen we apply this formula so now we say\nq\nnew\nis actually um the reward so the reward\nof the current\nindex\nplus\nself dot\ngamma\nand then times torch dot\nmax the maximum value\nof the next prediction so here's self\ndot model of next\nstate of this\nindex\nso this is exactly what we have written\nhere and now we need to set the\ntarget of the maximum value of the\naction to this value\nso here we get the let's we call this\ntarget so the target of the current\nindex and then of the arc max of the\naction\nso for this we can again say torch dot\narc max of the of the action and we want\nto have this as a item so as a value and\nnot as a tensor and now this is our q\nnew value so this might be a little bit\ntricky to understand so i recommend that\nthat you pause here and go over this\nagain\nand now we have everything that we need\nso let's have a look at the slides again\nwe have our q and our q\nnew\nand then we apply the loss function so\nthe mean squared error and in pi torch\nso what we have to do here we can simply\nuse this optimizer and do a step\nand first we have to call this zero grad\nfunction to empty the gradient so this\nis just something that we have to\nremember in pi torch\nand then we calculate the loss by\ncalling\nself dot criterion\nand here we put in the\ntarget and the prediction\nso this is q new and q\nand then we call loss dot backward and\napply back propagation and then update\nour gradients and then we call\nself.optimizer.step\nand this is all that we need in this\ntraining step and now this is actually\nall that we need in this model file\nso now again let's go back to the agent\nand i guess we already set up the q\ntrainer\nand then when we train this we call this\ntrain step function\neither for only one of those parameters\nor for a whole batch and now this\nfunction can handle different sizes\nand now the only thing left to do here\nis to actually to plot the results\nso for this let's create a new file and\nlet's call this hell\nhelper dot pi\nand then here let me actually copy and\npaste this in here\nso this is just a simple function with\nmatplotlib and i python\nand yeah here we want to plot the scores\nso this is a list and we want to plot in\nthe plot the mean score\nso\nlet's create them so here in the agent\nwe say from\nhelper import the plot function\nand then down here in the training\nfunction so we already created an empty\nlist for the scores and for the mean\nscores\nand now after each um\ngame we want to append the score\nso let's remove the to do and implement\nthis\nso we say plot scores dot\nappends and then the current score\nthen let's calculate the new mean or\naverage score\nso for this let's say total score\nplus equals the score\nand then let's call this mean score\nequals the total score divided by the\nnumber of games so agent and games\nand then we append this to plot mean\nscores dot append\nthe mean score and then we simply call\nthe plot function with the plot scores\nand then the plot\nmean scores\nand now let's save this file and also\nlet's save this file and then let's try\nit out so in the terminal\nlet's call agent dot pi and let's cross\nfingers\nso\nwe have a syntax error in the model.pi\nfile\nso um\nhere we\nactually here we have two equal signs\nso let's fix this and save this and run\nit again\nand then we made another mistake name\nerror so here this is actually called\nprediction.clone so again let's save\nthis and run this\nand now it starts training without\ncrashing and it also plots\nso let's\nlet this run and see if this is\nimproving\n[Music]\nall right so as we can see the algorithm\nworks and the snake is getting better\nand better and the scores are getting\nhigher and higher and also the mean or\naverage score is getting higher so i\nforgot one\nimportant thing which i show you in a\nsecond\nbut for now um\nso the snake is not perfect and the main\nissues are that it traps itself\nsometimes and also sometimes it gets\nstuck in an endless loop sequence\nso this is something that you can\nimprove as a homework so yeah like this\nit now it trapped itself\nso yeah let me\nstop this actually and then show you\nwhat i forgot so in the game we can\nactually um set the speed\nso for the\nhuman controlled game when i want to\nplay this i set this to 20 but now i\nrecommend to set this to a larger number\nso that the training will be faster\nso for example you can use 40 here or\neven higher so i go with 40 and yeah i\nthink that's the whole code you can also\nfind this on github and yeah i hope you\nreally enjoyed this little series about\nreinforcement learning and if you\nenjoyed this then please hit the like\nbutton and consider subscribing to the\nchannel and then i hope to see you next\ntime bye\n",
  "words": [
    "patrick",
    "lober",
    "popular",
    "python",
    "instructor",
    "course",
    "teach",
    "train",
    "artificial",
    "intelligence",
    "play",
    "snake",
    "game",
    "using",
    "reinforcement",
    "learning",
    "hey",
    "guys",
    "today",
    "exciting",
    "project",
    "going",
    "build",
    "ai",
    "teaches",
    "play",
    "snake",
    "build",
    "everything",
    "scratch",
    "start",
    "creating",
    "game",
    "pygame",
    "build",
    "agent",
    "deep",
    "learning",
    "algorithm",
    "pie",
    "torch",
    "also",
    "teach",
    "basics",
    "reinforcement",
    "learning",
    "need",
    "understand",
    "works",
    "think",
    "going",
    "pretty",
    "cool",
    "start",
    "let",
    "show",
    "final",
    "project",
    "start",
    "script",
    "saying",
    "python",
    "agents",
    "dot",
    "pi",
    "start",
    "training",
    "agent",
    "see",
    "game",
    "also",
    "plot",
    "scores",
    "average",
    "score",
    "let",
    "also",
    "start",
    "stopwatch",
    "see",
    "happening",
    "live",
    "point",
    "snake",
    "knows",
    "absolutely",
    "nothing",
    "game",
    "aware",
    "environment",
    "tries",
    "make",
    "less",
    "random",
    "moves",
    "move",
    "especially",
    "game",
    "learns",
    "knows",
    "play",
    "game",
    "get",
    "better",
    "better",
    "first",
    "games",
    "wo",
    "see",
    "lot",
    "improvements",
    "worry",
    "absolutely",
    "normal",
    "tell",
    "takes",
    "around",
    "80",
    "100",
    "games",
    "ai",
    "good",
    "game",
    "strategy",
    "take",
    "around",
    "10",
    "minutes",
    "also",
    "need",
    "gpu",
    "training",
    "happen",
    "cpu",
    "totally",
    "fine",
    "okay",
    "let",
    "speed",
    "little",
    "bit",
    "music",
    "music",
    "right",
    "10",
    "minutes",
    "passed",
    "game",
    "90",
    "guess",
    "clearly",
    "see",
    "snake",
    "knows",
    "less",
    "going",
    "straight",
    "food",
    "tries",
    "hit",
    "boundaries",
    "perfect",
    "point",
    "see",
    "getting",
    "better",
    "better",
    "also",
    "see",
    "average",
    "score",
    "increasing",
    "per",
    "best",
    "score",
    "far",
    "honest",
    "super",
    "exciting",
    "imagine",
    "beginning",
    "snake",
    "know",
    "anything",
    "game",
    "little",
    "bit",
    "math",
    "behind",
    "scenes",
    "clearly",
    "following",
    "strategy",
    "super",
    "cool",
    "think",
    "right",
    "let",
    "speed",
    "little",
    "bit",
    "music",
    "right",
    "12",
    "minutes",
    "snake",
    "getting",
    "better",
    "better",
    "think",
    "clearly",
    "see",
    "algorithm",
    "works",
    "let",
    "stop",
    "let",
    "start",
    "theory",
    "split",
    "series",
    "four",
    "parts",
    "first",
    "video",
    "learn",
    "little",
    "bit",
    "theory",
    "reinforcement",
    "learning",
    "second",
    "part",
    "implement",
    "actual",
    "game",
    "also",
    "called",
    "environment",
    "pygame",
    "implement",
    "agent",
    "tell",
    "means",
    "second",
    "last",
    "part",
    "implement",
    "actual",
    "model",
    "pytorch",
    "let",
    "start",
    "little",
    "bit",
    "theory",
    "reinforcement",
    "learning",
    "definition",
    "wikipedia",
    "reinforcement",
    "learning",
    "area",
    "machine",
    "learning",
    "concerned",
    "software",
    "agents",
    "ought",
    "take",
    "actions",
    "environment",
    "order",
    "maximize",
    "notion",
    "cumulative",
    "reward",
    "might",
    "sound",
    "little",
    "bit",
    "complicated",
    "words",
    "also",
    "say",
    "reinforcement",
    "learning",
    "teaching",
    "software",
    "agent",
    "behave",
    "environment",
    "telling",
    "good",
    "remember",
    "chance",
    "basically",
    "computer",
    "player",
    "environment",
    "game",
    "case",
    "give",
    "agent",
    "reward",
    "tell",
    "good",
    "based",
    "reward",
    "try",
    "find",
    "best",
    "next",
    "action",
    "yeah",
    "reinforcement",
    "learning",
    "train",
    "agent",
    "lot",
    "different",
    "approaches",
    "involve",
    "deep",
    "learning",
    "case",
    "use",
    "deep",
    "learning",
    "also",
    "called",
    "steep",
    "q",
    "learning",
    "approach",
    "extends",
    "reinforcement",
    "learning",
    "using",
    "deep",
    "neural",
    "network",
    "predict",
    "actions",
    "going",
    "use",
    "tutorial",
    "right",
    "let",
    "show",
    "rough",
    "overview",
    "organized",
    "code",
    "said",
    "four",
    "parts",
    "next",
    "part",
    "implement",
    "game",
    "pie",
    "game",
    "implement",
    "agent",
    "implement",
    "model",
    "pie",
    "torch",
    "game",
    "assigned",
    "game",
    "loop",
    "game",
    "loop",
    "play",
    "step",
    "gets",
    "action",
    "step",
    "moves",
    "snake",
    "move",
    "returns",
    "current",
    "reward",
    "game",
    "also",
    "current",
    "score",
    "agent",
    "agent",
    "basically",
    "puts",
    "everything",
    "together",
    "must",
    "know",
    "game",
    "also",
    "knows",
    "model",
    "store",
    "agent",
    "implement",
    "training",
    "loop",
    "roughly",
    "based",
    "game",
    "calculate",
    "state",
    "based",
    "state",
    "um",
    "calculate",
    "next",
    "action",
    "involves",
    "calling",
    "model",
    "predict",
    "new",
    "action",
    "next",
    "play",
    "step",
    "said",
    "get",
    "reward",
    "game",
    "overstate",
    "score",
    "information",
    "calculate",
    "new",
    "state",
    "remember",
    "store",
    "new",
    "state",
    "old",
    "state",
    "game",
    "state",
    "score",
    "train",
    "model",
    "model",
    "call",
    "linear",
    "q",
    "net",
    "complicated",
    "feed",
    "forward",
    "neural",
    "net",
    "linear",
    "layers",
    "needs",
    "information",
    "new",
    "state",
    "old",
    "state",
    "train",
    "model",
    "call",
    "model",
    "predict",
    "gets",
    "us",
    "next",
    "action",
    "yeah",
    "rough",
    "overview",
    "code",
    "look",
    "like",
    "let",
    "talk",
    "variables",
    "detail",
    "example",
    "action",
    "state",
    "reward",
    "let",
    "start",
    "reward",
    "pretty",
    "easy",
    "whenever",
    "snake",
    "eats",
    "food",
    "give",
    "plus",
    "10",
    "reward",
    "game",
    "die",
    "get",
    "everything",
    "else",
    "stay",
    "zero",
    "pretty",
    "simple",
    "action",
    "action",
    "determines",
    "next",
    "move",
    "could",
    "think",
    "four",
    "different",
    "actions",
    "left",
    "right",
    "design",
    "like",
    "example",
    "happen",
    "go",
    "right",
    "might",
    "take",
    "action",
    "left",
    "immediately",
    "die",
    "basically",
    "180",
    "degree",
    "turn",
    "allow",
    "better",
    "approach",
    "design",
    "action",
    "use",
    "three",
    "different",
    "numbers",
    "dependent",
    "current",
    "direction",
    "um",
    "1",
    "zero",
    "zero",
    "means",
    "stay",
    "current",
    "direction",
    "go",
    "straight",
    "means",
    "go",
    "right",
    "stay",
    "right",
    "go",
    "left",
    "go",
    "left",
    "0",
    "1",
    "0",
    "means",
    "right",
    "turn",
    "depends",
    "current",
    "direction",
    "go",
    "right",
    "right",
    "turn",
    "go",
    "next",
    "go",
    "right",
    "turn",
    "go",
    "left",
    "would",
    "go",
    "right",
    "turn",
    "left",
    "turn",
    "way",
    "around",
    "go",
    "left",
    "left",
    "turn",
    "go",
    "approach",
    "180",
    "degree",
    "turn",
    "also",
    "predict",
    "three",
    "different",
    "states",
    "make",
    "little",
    "bit",
    "easier",
    "model",
    "reward",
    "action",
    "also",
    "need",
    "calculate",
    "state",
    "state",
    "means",
    "tell",
    "snake",
    "information",
    "game",
    "knows",
    "needs",
    "know",
    "environment",
    "case",
    "state",
    "11",
    "values",
    "information",
    "danger",
    "straight",
    "ahead",
    "danger",
    "right",
    "danger",
    "left",
    "um",
    "current",
    "direction",
    "direction",
    "left",
    "right",
    "information",
    "food",
    "left",
    "right",
    "boolean",
    "values",
    "let",
    "show",
    "actual",
    "example",
    "case",
    "going",
    "right",
    "food",
    "see",
    "um",
    "danger",
    "straight",
    "right",
    "left",
    "none",
    "true",
    "example",
    "snake",
    "end",
    "still",
    "going",
    "right",
    "danger",
    "straight",
    "would",
    "one",
    "also",
    "depends",
    "current",
    "direction",
    "example",
    "move",
    "corner",
    "danger",
    "right",
    "would",
    "a1",
    "directions",
    "one",
    "one",
    "rest",
    "always",
    "zero",
    "case",
    "danger",
    "right",
    "set",
    "one",
    "case",
    "food",
    "right",
    "snake",
    "also",
    "snake",
    "food",
    "right",
    "one",
    "food",
    "one",
    "right",
    "state",
    "action",
    "design",
    "model",
    "feed",
    "forward",
    "neural",
    "net",
    "input",
    "layer",
    "hidden",
    "layer",
    "output",
    "layer",
    "input",
    "gets",
    "state",
    "said",
    "11",
    "different",
    "numbers",
    "state",
    "11",
    "different",
    "boolean",
    "values",
    "zero",
    "one",
    "need",
    "size",
    "11",
    "beginning",
    "choose",
    "hidden",
    "um",
    "size",
    "output",
    "need",
    "three",
    "outputs",
    "predict",
    "action",
    "numbers",
    "need",
    "probabilities",
    "raw",
    "numbers",
    "simply",
    "choose",
    "maximum",
    "example",
    "take",
    "1",
    "0",
    "zero",
    "go",
    "back",
    "see",
    "would",
    "action",
    "straight",
    "keep",
    "current",
    "direction",
    "yeah",
    "model",
    "looks",
    "like",
    "course",
    "train",
    "model",
    "let",
    "talk",
    "little",
    "bit",
    "deep",
    "q",
    "learning",
    "q",
    "stands",
    "q",
    "value",
    "stands",
    "quality",
    "action",
    "want",
    "improve",
    "actions",
    "improve",
    "quality",
    "snake",
    "start",
    "initializing",
    "q",
    "value",
    "case",
    "initialize",
    "model",
    "random",
    "parameters",
    "choose",
    "action",
    "calling",
    "model",
    "predict",
    "state",
    "also",
    "sometimes",
    "choose",
    "random",
    "move",
    "especially",
    "beginning",
    "know",
    "lot",
    "game",
    "yet",
    "later",
    "want",
    "random",
    "move",
    "anymore",
    "call",
    "model",
    "predict",
    "also",
    "called",
    "exploration",
    "exploitation",
    "get",
    "clearer",
    "later",
    "actual",
    "coding",
    "new",
    "action",
    "perform",
    "action",
    "perform",
    "next",
    "move",
    "measure",
    "reward",
    "information",
    "update",
    "q",
    "value",
    "train",
    "model",
    "repeat",
    "step",
    "iterative",
    "training",
    "loop",
    "train",
    "model",
    "always",
    "need",
    "kind",
    "loss",
    "function",
    "want",
    "optimize",
    "minimize",
    "loss",
    "function",
    "look",
    "little",
    "bit",
    "math",
    "want",
    "present",
    "belmont",
    "equation",
    "might",
    "look",
    "scary",
    "scared",
    "explain",
    "everything",
    "actually",
    "difficult",
    "um",
    "understand",
    "code",
    "later",
    "want",
    "need",
    "update",
    "q",
    "value",
    "said",
    "according",
    "belmont",
    "equation",
    "new",
    "q",
    "value",
    "calculated",
    "like",
    "current",
    "q",
    "value",
    "plus",
    "learning",
    "rate",
    "reward",
    "taking",
    "action",
    "state",
    "plus",
    "gamma",
    "parameter",
    "called",
    "count",
    "rate",
    "worry",
    "also",
    "show",
    "later",
    "code",
    "take",
    "maximum",
    "expected",
    "future",
    "reward",
    "given",
    "new",
    "state",
    "possible",
    "actions",
    "new",
    "state",
    "yeah",
    "looks",
    "scary",
    "simplify",
    "actually",
    "difficult",
    "old",
    "q",
    "value",
    "model",
    "predict",
    "state",
    "0",
    "go",
    "back",
    "overview",
    "first",
    "time",
    "say",
    "get",
    "state",
    "game",
    "state",
    "0",
    "took",
    "place",
    "step",
    "measure",
    "calculate",
    "next",
    "state",
    "state",
    "one",
    "information",
    "first",
    "queue",
    "model",
    "predict",
    "old",
    "state",
    "new",
    "queue",
    "reward",
    "plus",
    "gamma",
    "value",
    "times",
    "maximum",
    "value",
    "q",
    "state",
    "model",
    "predict",
    "time",
    "take",
    "state",
    "one",
    "two",
    "information",
    "loss",
    "simply",
    "q",
    "new",
    "minus",
    "q",
    "squared",
    "yeah",
    "nothing",
    "else",
    "mean",
    "squared",
    "error",
    "simple",
    "error",
    "already",
    "know",
    "must",
    "use",
    "optimization",
    "yeah",
    "going",
    "use",
    "implement",
    "three",
    "classes",
    "next",
    "video",
    "start",
    "implementing",
    "game",
    "last",
    "part",
    "showed",
    "necessary",
    "theory",
    "need",
    "know",
    "get",
    "started",
    "deep",
    "q",
    "learning",
    "start",
    "implementing",
    "parts",
    "said",
    "need",
    "game",
    "environment",
    "need",
    "agent",
    "need",
    "model",
    "part",
    "start",
    "implementing",
    "game",
    "use",
    "pytorch",
    "let",
    "actually",
    "start",
    "creating",
    "environment",
    "install",
    "necessary",
    "dependencies",
    "need",
    "case",
    "use",
    "conda",
    "manage",
    "environments",
    "know",
    "use",
    "conda",
    "tutorial",
    "link",
    "yeah",
    "want",
    "use",
    "connor",
    "also",
    "use",
    "normal",
    "virtual",
    "recommend",
    "use",
    "virtual",
    "let",
    "create",
    "virtual",
    "conda",
    "create",
    "minus",
    "n",
    "give",
    "name",
    "example",
    "pi",
    "game",
    "n",
    "also",
    "say",
    "want",
    "python",
    "equals",
    "right",
    "created",
    "want",
    "actuate",
    "conda",
    "activate",
    "pie",
    "game",
    "n",
    "hit",
    "enter",
    "see",
    "name",
    "environment",
    "front",
    "means",
    "activated",
    "successfully",
    "start",
    "installing",
    "need",
    "first",
    "thing",
    "want",
    "install",
    "pie",
    "game",
    "game",
    "pip",
    "install",
    "pie",
    "game",
    "hit",
    "enter",
    "done",
    "next",
    "thing",
    "need",
    "pytorch",
    "model",
    "later",
    "go",
    "official",
    "home",
    "page",
    "install",
    "select",
    "operating",
    "system",
    "use",
    "mac",
    "actually",
    "want",
    "say",
    "pip",
    "install",
    "need",
    "cuda",
    "support",
    "cpu",
    "fine",
    "need",
    "torch",
    "audio",
    "work",
    "audio",
    "files",
    "grab",
    "pip",
    "install",
    "torch",
    "torch",
    "vision",
    "paste",
    "hit",
    "enter",
    "installs",
    "pytorch",
    "dependencies",
    "right",
    "done",
    "need",
    "two",
    "things",
    "plotting",
    "later",
    "say",
    "pip",
    "install",
    "much",
    "plot",
    "lip",
    "also",
    "want",
    "python",
    "hit",
    "enter",
    "right",
    "successful",
    "well",
    "everything",
    "need",
    "start",
    "implementing",
    "codes",
    "starting",
    "point",
    "want",
    "grab",
    "code",
    "another",
    "tutorial",
    "find",
    "github",
    "um",
    "account",
    "repo",
    "python",
    "fun",
    "actually",
    "two",
    "snake",
    "games",
    "need",
    "one",
    "snake",
    "pie",
    "game",
    "download",
    "already",
    "open",
    "editor",
    "visuals",
    "using",
    "visual",
    "studio",
    "code",
    "see",
    "exactly",
    "two",
    "files",
    "um",
    "first",
    "thing",
    "want",
    "want",
    "run",
    "file",
    "test",
    "actually",
    "working",
    "right",
    "normal",
    "snake",
    "game",
    "control",
    "use",
    "arrow",
    "keys",
    "let",
    "say",
    "python",
    "snake",
    "game",
    "dot",
    "pi",
    "let",
    "hope",
    "working",
    "yeah",
    "control",
    "snake",
    "hope",
    "eat",
    "food",
    "yes",
    "hit",
    "boundary",
    "game",
    "working",
    "environment",
    "set",
    "start",
    "implementing",
    "code",
    "change",
    "use",
    "ai",
    "controlled",
    "game",
    "let",
    "show",
    "overview",
    "last",
    "time",
    "last",
    "time",
    "told",
    "need",
    "play",
    "step",
    "game",
    "gets",
    "action",
    "based",
    "action",
    "take",
    "move",
    "must",
    "return",
    "reward",
    "game",
    "state",
    "current",
    "score",
    "first",
    "let",
    "write",
    "things",
    "need",
    "change",
    "first",
    "want",
    "reset",
    "function",
    "um",
    "game",
    "agent",
    "able",
    "reset",
    "game",
    "start",
    "new",
    "game",
    "need",
    "implement",
    "reward",
    "agent",
    "gets",
    "need",
    "change",
    "play",
    "function",
    "takes",
    "takes",
    "action",
    "um",
    "returns",
    "computes",
    "direction",
    "also",
    "want",
    "keep",
    "track",
    "current",
    "frame",
    "let",
    "call",
    "game",
    "iteration",
    "later",
    "also",
    "need",
    "change",
    "collision",
    "function",
    "check",
    "collision",
    "first",
    "let",
    "let",
    "go",
    "code",
    "quickly",
    "use",
    "pi",
    "game",
    "direction",
    "use",
    "enum",
    "point",
    "use",
    "named",
    "tuple",
    "created",
    "class",
    "snake",
    "game",
    "initialize",
    "things",
    "need",
    "game",
    "initialize",
    "game",
    "state",
    "example",
    "snake",
    "use",
    "list",
    "three",
    "initial",
    "values",
    "head",
    "always",
    "front",
    "list",
    "keep",
    "track",
    "score",
    "helper",
    "function",
    "place",
    "food",
    "yeah",
    "already",
    "function",
    "called",
    "play",
    "step",
    "go",
    "end",
    "game",
    "loop",
    "true",
    "take",
    "game",
    "play",
    "step",
    "get",
    "game",
    "state",
    "score",
    "place",
    "function",
    "important",
    "one",
    "first",
    "right",
    "grab",
    "user",
    "input",
    "key",
    "press",
    "calculate",
    "move",
    "based",
    "key",
    "update",
    "snake",
    "check",
    "game",
    "continue",
    "place",
    "new",
    "food",
    "check",
    "eat",
    "food",
    "update",
    "ui",
    "helper",
    "function",
    "update",
    "ui",
    "helper",
    "function",
    "collision",
    "check",
    "either",
    "hit",
    "boundary",
    "run",
    "ourself",
    "also",
    "helper",
    "function",
    "move",
    "get",
    "current",
    "direction",
    "based",
    "direction",
    "simply",
    "um",
    "calculate",
    "calculate",
    "new",
    "position",
    "new",
    "hat",
    "yeah",
    "um",
    "done",
    "let",
    "change",
    "things",
    "though",
    "first",
    "one",
    "want",
    "change",
    "class",
    "name",
    "say",
    "snake",
    "game",
    "ai",
    "make",
    "clear",
    "agent",
    "controlled",
    "game",
    "first",
    "thing",
    "want",
    "reset",
    "functionality",
    "um",
    "already",
    "comment",
    "game",
    "state",
    "want",
    "refactor",
    "reset",
    "function",
    "create",
    "new",
    "function",
    "define",
    "let",
    "call",
    "reset",
    "gets",
    "self",
    "arguments",
    "grab",
    "code",
    "simply",
    "paste",
    "initializer",
    "call",
    "self",
    "dot",
    "reset",
    "first",
    "thing",
    "need",
    "additionally",
    "want",
    "keep",
    "track",
    "um",
    "game",
    "iteration",
    "frame",
    "iteration",
    "let",
    "call",
    "self",
    "dot",
    "frame",
    "iteration",
    "beginning",
    "zero",
    "define",
    "place",
    "food",
    "stay",
    "need",
    "change",
    "play",
    "step",
    "function",
    "first",
    "look",
    "overview",
    "already",
    "told",
    "need",
    "give",
    "action",
    "agent",
    "need",
    "return",
    "reward",
    "let",
    "start",
    "um",
    "using",
    "action",
    "parameter",
    "grab",
    "user",
    "input",
    "actually",
    "right",
    "get",
    "rid",
    "thing",
    "still",
    "check",
    "want",
    "quit",
    "game",
    "um",
    "already",
    "helper",
    "function",
    "move",
    "current",
    "direction",
    "actually",
    "change",
    "move",
    "function",
    "get",
    "direction",
    "user",
    "input",
    "gets",
    "action",
    "determine",
    "new",
    "direction",
    "second",
    "first",
    "let",
    "change",
    "call",
    "action",
    "update",
    "head",
    "check",
    "game",
    "actually",
    "also",
    "need",
    "reward",
    "simply",
    "say",
    "reward",
    "equals",
    "zero",
    "let",
    "go",
    "back",
    "slides",
    "last",
    "time",
    "reward",
    "really",
    "simple",
    "whenever",
    "eat",
    "food",
    "say",
    "plus",
    "10",
    "lose",
    "die",
    "say",
    "reward",
    "everything",
    "else",
    "stay",
    "zero",
    "initialize",
    "reward",
    "zero",
    "collision",
    "game",
    "say",
    "reward",
    "equals",
    "want",
    "return",
    "well",
    "return",
    "reward",
    "game",
    "check",
    "collision",
    "actually",
    "want",
    "another",
    "check",
    "nothing",
    "happens",
    "long",
    "time",
    "snake",
    "improve",
    "eat",
    "food",
    "also",
    "die",
    "also",
    "want",
    "check",
    "happens",
    "long",
    "time",
    "also",
    "break",
    "say",
    "say",
    "self",
    "dot",
    "frame",
    "iteration",
    "gets",
    "large",
    "without",
    "anything",
    "happening",
    "um",
    "stop",
    "use",
    "little",
    "formula",
    "greater",
    "100",
    "times",
    "length",
    "snake",
    "remember",
    "list",
    "break",
    "also",
    "like",
    "dependent",
    "length",
    "snake",
    "longer",
    "snake",
    "time",
    "gets",
    "larger",
    "value",
    "break",
    "course",
    "update",
    "iteration",
    "simply",
    "beginning",
    "play",
    "step",
    "say",
    "self",
    "dot",
    "frame",
    "iteration",
    "plus",
    "equals",
    "1",
    "reset",
    "reset",
    "back",
    "zero",
    "yeah",
    "stop",
    "reward",
    "hat",
    "hits",
    "food",
    "eat",
    "food",
    "score",
    "increases",
    "reward",
    "set",
    "plus",
    "10",
    "place",
    "new",
    "food",
    "say",
    "otherwise",
    "remove",
    "last",
    "part",
    "simply",
    "move",
    "stay",
    "update",
    "function",
    "end",
    "also",
    "want",
    "return",
    "reward",
    "collision",
    "function",
    "need",
    "slight",
    "change",
    "check",
    "later",
    "um",
    "calculate",
    "state",
    "danger",
    "told",
    "look",
    "state",
    "calculate",
    "danger",
    "example",
    "corner",
    "danger",
    "right",
    "might",
    "handy",
    "use",
    "inside",
    "give",
    "function",
    "point",
    "gets",
    "point",
    "argument",
    "let",
    "say",
    "default",
    "none",
    "simply",
    "ch",
    "check",
    "point",
    "none",
    "set",
    "point",
    "equals",
    "self",
    "dot",
    "head",
    "inside",
    "call",
    "argument",
    "stay",
    "course",
    "change",
    "point",
    "hit",
    "corner",
    "point",
    "point",
    "point",
    "collision",
    "point",
    "snake",
    "body",
    "also",
    "collision",
    "otherwise",
    "collision",
    "right",
    "update",
    "ui",
    "function",
    "stay",
    "like",
    "move",
    "function",
    "need",
    "change",
    "something",
    "get",
    "action",
    "based",
    "action",
    "want",
    "determine",
    "next",
    "move",
    "go",
    "back",
    "slides",
    "designed",
    "action",
    "like",
    "three",
    "values",
    "um",
    "one",
    "zero",
    "zero",
    "means",
    "keep",
    "current",
    "direction",
    "go",
    "straight",
    "0",
    "1",
    "0",
    "means",
    "right",
    "turn",
    "0",
    "0",
    "1",
    "means",
    "left",
    "turn",
    "dependent",
    "current",
    "direction",
    "go",
    "right",
    "right",
    "turn",
    "go",
    "next",
    "go",
    "right",
    "turn",
    "go",
    "left",
    "next",
    "left",
    "turn",
    "way",
    "around",
    "um",
    "want",
    "determine",
    "direction",
    "based",
    "action",
    "let",
    "write",
    "quick",
    "comment",
    "straight",
    "right",
    "turn",
    "left",
    "turn",
    "get",
    "next",
    "direction",
    "first",
    "want",
    "define",
    "possible",
    "directions",
    "clockwise",
    "order",
    "say",
    "clockwise",
    "equals",
    "list",
    "start",
    "direction",
    "dot",
    "right",
    "remember",
    "direction",
    "use",
    "enum",
    "class",
    "one",
    "directions",
    "clockwise",
    "directions",
    "start",
    "direction",
    "right",
    "next",
    "one",
    "direction",
    "dot",
    "direction",
    "dot",
    "left",
    "last",
    "thing",
    "direction",
    "dot",
    "right",
    "left",
    "clockwise",
    "get",
    "current",
    "direction",
    "current",
    "index",
    "current",
    "direction",
    "say",
    "index",
    "equals",
    "say",
    "clockwise",
    "dot",
    "index",
    "index",
    "self",
    "dot",
    "direction",
    "sure",
    "array",
    "self",
    "direction",
    "must",
    "one",
    "enum",
    "values",
    "check",
    "different",
    "um",
    "possible",
    "states",
    "ones",
    "use",
    "numpy",
    "guess",
    "import",
    "numpy",
    "first",
    "np",
    "use",
    "say",
    "numpy",
    "use",
    "function",
    "array",
    "equal",
    "put",
    "action",
    "array",
    "want",
    "compare",
    "equal",
    "one",
    "zero",
    "zero",
    "go",
    "straight",
    "keep",
    "current",
    "directions",
    "simply",
    "say",
    "new",
    "direction",
    "equals",
    "clockwise",
    "index",
    "remember",
    "index",
    "index",
    "current",
    "direction",
    "basically",
    "change",
    "say",
    "l",
    "array",
    "numpy",
    "array",
    "equal",
    "action",
    "equals",
    "0",
    "one",
    "zero",
    "right",
    "turn",
    "means",
    "go",
    "clockwise",
    "go",
    "right",
    "next",
    "direction",
    "would",
    "go",
    "next",
    "direction",
    "would",
    "left",
    "go",
    "left",
    "next",
    "direction",
    "would",
    "say",
    "index",
    "equals",
    "next",
    "index",
    "actually",
    "say",
    "current",
    "index",
    "plus",
    "1",
    "modulo",
    "4",
    "means",
    "end",
    "next",
    "one",
    "index",
    "index",
    "0",
    "1",
    "2",
    "3",
    "index",
    "4",
    "modulo",
    "4",
    "actually",
    "index",
    "zero",
    "turn",
    "come",
    "back",
    "front",
    "right",
    "turn",
    "next",
    "index",
    "new",
    "direction",
    "clockwise",
    "next",
    "index",
    "otherwise",
    "simply",
    "use",
    "else",
    "actually",
    "change",
    "l",
    "last",
    "case",
    "zero",
    "zero",
    "one",
    "case",
    "let",
    "copy",
    "paste",
    "next",
    "index",
    "current",
    "index",
    "minus",
    "one",
    "modulo",
    "four",
    "actually",
    "means",
    "go",
    "counter",
    "clockwise",
    "left",
    "turn",
    "start",
    "right",
    "next",
    "move",
    "would",
    "next",
    "would",
    "left",
    "next",
    "would",
    "right",
    "new",
    "direction",
    "simply",
    "say",
    "self",
    "direction",
    "equals",
    "new",
    "direction",
    "go",
    "extract",
    "head",
    "check",
    "self",
    "dot",
    "direction",
    "right",
    "increase",
    "position",
    "x",
    "um",
    "left",
    "direction",
    "decrease",
    "x",
    "go",
    "actually",
    "increase",
    "starts",
    "top",
    "zero",
    "increases",
    "go",
    "go",
    "increase",
    "go",
    "decrease",
    "self",
    "direction",
    "equals",
    "minus",
    "equals",
    "block",
    "size",
    "way",
    "block",
    "size",
    "constant",
    "value",
    "10",
    "big",
    "one",
    "block",
    "snake",
    "pixels",
    "yeah",
    "everything",
    "need",
    "move",
    "function",
    "need",
    "anymore",
    "actually",
    "longer",
    "working",
    "user",
    "input",
    "delete",
    "later",
    "control",
    "class",
    "agent",
    "call",
    "play",
    "step",
    "function",
    "yeah",
    "need",
    "implement",
    "game",
    "already",
    "talked",
    "theory",
    "deep",
    "q",
    "learning",
    "first",
    "part",
    "last",
    "part",
    "implemented",
    "pi",
    "game",
    "use",
    "agent",
    "controlled",
    "environment",
    "need",
    "agent",
    "let",
    "start",
    "um",
    "watched",
    "first",
    "two",
    "parts",
    "highly",
    "recommend",
    "starting",
    "point",
    "last",
    "time",
    "actually",
    "want",
    "make",
    "one",
    "change",
    "forgot",
    "collision",
    "function",
    "actually",
    "public",
    "agent",
    "use",
    "remove",
    "underscore",
    "also",
    "remove",
    "class",
    "call",
    "snake",
    "game",
    "also",
    "want",
    "rename",
    "game",
    "create",
    "new",
    "file",
    "agent",
    "dot",
    "pi",
    "start",
    "implementing",
    "first",
    "import",
    "torch",
    "pi",
    "torch",
    "import",
    "random",
    "later",
    "need",
    "also",
    "need",
    "import",
    "numpy",
    "snp",
    "um",
    "implemented",
    "class",
    "need",
    "snake",
    "game",
    "say",
    "game",
    "import",
    "snake",
    "like",
    "snake",
    "game",
    "think",
    "call",
    "class",
    "snake",
    "game",
    "yeah",
    "right",
    "name",
    "also",
    "hear",
    "beginning",
    "defined",
    "enum",
    "direction",
    "named",
    "tuple",
    "point",
    "x",
    "attribute",
    "also",
    "want",
    "import",
    "two",
    "um",
    "things",
    "import",
    "direction",
    "import",
    "point",
    "also",
    "say",
    "collections",
    "want",
    "import",
    "deck",
    "data",
    "structure",
    "want",
    "store",
    "memories",
    "um",
    "know",
    "deck",
    "put",
    "link",
    "description",
    "really",
    "handy",
    "case",
    "see",
    "case",
    "later",
    "want",
    "define",
    "parameters",
    "constants",
    "maximum",
    "memory",
    "let",
    "say",
    "100",
    "000",
    "store",
    "100",
    "000",
    "items",
    "memory",
    "also",
    "want",
    "use",
    "batch",
    "size",
    "see",
    "later",
    "set",
    "1000",
    "play",
    "around",
    "parameters",
    "also",
    "want",
    "learning",
    "rate",
    "later",
    "want",
    "set",
    "0",
    "0",
    "1",
    "yeah",
    "feel",
    "free",
    "change",
    "start",
    "creating",
    "class",
    "agent",
    "gets",
    "course",
    "init",
    "function",
    "self",
    "arguments",
    "let",
    "look",
    "slides",
    "first",
    "part",
    "explained",
    "training",
    "want",
    "create",
    "training",
    "function",
    "need",
    "get",
    "state",
    "calculate",
    "state",
    "aware",
    "current",
    "environment",
    "need",
    "calculate",
    "next",
    "move",
    "state",
    "need",
    "um",
    "want",
    "update",
    "next",
    "step",
    "call",
    "calculate",
    "new",
    "state",
    "want",
    "store",
    "everything",
    "memory",
    "also",
    "want",
    "train",
    "model",
    "need",
    "store",
    "game",
    "model",
    "class",
    "first",
    "let",
    "create",
    "functions",
    "need",
    "first",
    "need",
    "function",
    "get",
    "state",
    "gets",
    "self",
    "gets",
    "game",
    "calculate",
    "state",
    "showed",
    "11",
    "different",
    "variables",
    "want",
    "function",
    "call",
    "remember",
    "remember",
    "self",
    "want",
    "put",
    "state",
    "action",
    "want",
    "remember",
    "reward",
    "action",
    "want",
    "calculate",
    "want",
    "store",
    "next",
    "state",
    "next",
    "state",
    "also",
    "want",
    "store",
    "done",
    "bit",
    "also",
    "call",
    "game",
    "current",
    "game",
    "overstate",
    "need",
    "two",
    "different",
    "functions",
    "train",
    "call",
    "defined",
    "train",
    "long",
    "memory",
    "needs",
    "self",
    "explain",
    "later",
    "also",
    "let",
    "copy",
    "paste",
    "also",
    "function",
    "define",
    "train",
    "short",
    "memory",
    "one",
    "step",
    "see",
    "later",
    "need",
    "function",
    "call",
    "get",
    "action",
    "get",
    "action",
    "based",
    "state",
    "gets",
    "self",
    "state",
    "first",
    "say",
    "pass",
    "functions",
    "need",
    "guess",
    "want",
    "global",
    "function",
    "call",
    "simply",
    "train",
    "say",
    "pass",
    "start",
    "module",
    "h",
    "dot",
    "pi",
    "say",
    "name",
    "underscore",
    "equals",
    "equals",
    "main",
    "simply",
    "call",
    "train",
    "function",
    "start",
    "script",
    "saying",
    "python",
    "agent",
    "dot",
    "pi",
    "like",
    "first",
    "tutorial",
    "let",
    "start",
    "implementing",
    "agent",
    "training",
    "function",
    "let",
    "start",
    "init",
    "function",
    "agent",
    "want",
    "store",
    "first",
    "want",
    "store",
    "parameters",
    "games",
    "want",
    "keep",
    "track",
    "zero",
    "beginning",
    "equals",
    "um",
    "zero",
    "beginning",
    "parameter",
    "control",
    "randomness",
    "see",
    "later",
    "also",
    "need",
    "self",
    "dot",
    "gamma",
    "equals",
    "zero",
    "count",
    "rate",
    "briefly",
    "showed",
    "first",
    "tutorial",
    "explain",
    "little",
    "bit",
    "next",
    "tutorial",
    "implement",
    "model",
    "actual",
    "deep",
    "q",
    "learning",
    "algorithm",
    "want",
    "memory",
    "say",
    "equals",
    "use",
    "stack",
    "argument",
    "max",
    "leng",
    "equals",
    "say",
    "max",
    "memory",
    "happens",
    "exceed",
    "memory",
    "automatically",
    "remove",
    "elements",
    "left",
    "call",
    "pop",
    "left",
    "us",
    "deck",
    "really",
    "handy",
    "later",
    "also",
    "want",
    "model",
    "trainer",
    "leave",
    "last",
    "part",
    "next",
    "video",
    "init",
    "function",
    "go",
    "back",
    "let",
    "training",
    "function",
    "next",
    "let",
    "look",
    "slides",
    "need",
    "functions",
    "order",
    "let",
    "first",
    "let",
    "write",
    "comments",
    "first",
    "let",
    "create",
    "lists",
    "keep",
    "track",
    "scores",
    "empty",
    "list",
    "beginning",
    "used",
    "plotting",
    "later",
    "also",
    "want",
    "keep",
    "track",
    "mean",
    "scores",
    "average",
    "scores",
    "also",
    "empty",
    "list",
    "beginning",
    "total",
    "score",
    "equals",
    "zero",
    "beginning",
    "record",
    "best",
    "score",
    "zero",
    "beginning",
    "set",
    "agent",
    "agent",
    "equals",
    "agent",
    "also",
    "need",
    "create",
    "game",
    "game",
    "snake",
    "game",
    "ai",
    "object",
    "create",
    "training",
    "loop",
    "say",
    "true",
    "basically",
    "run",
    "forever",
    "quit",
    "script",
    "let",
    "write",
    "comments",
    "want",
    "get",
    "get",
    "old",
    "state",
    "current",
    "state",
    "let",
    "say",
    "state",
    "old",
    "equals",
    "call",
    "agent",
    "dot",
    "get",
    "states",
    "gets",
    "game",
    "already",
    "set",
    "correctly",
    "implement",
    "want",
    "get",
    "move",
    "based",
    "current",
    "state",
    "say",
    "final",
    "move",
    "equals",
    "agent",
    "dot",
    "get",
    "action",
    "actually",
    "called",
    "action",
    "action",
    "based",
    "state",
    "move",
    "want",
    "perform",
    "move",
    "get",
    "new",
    "state",
    "say",
    "reward",
    "um",
    "done",
    "score",
    "equals",
    "call",
    "game",
    "dot",
    "play",
    "step",
    "last",
    "time",
    "think",
    "game",
    "dot",
    "play",
    "step",
    "action",
    "yes",
    "game",
    "dot",
    "play",
    "step",
    "gets",
    "final",
    "move",
    "get",
    "state",
    "old",
    "new",
    "new",
    "state",
    "state",
    "new",
    "state",
    "new",
    "equals",
    "agent",
    "gets",
    "state",
    "new",
    "game",
    "want",
    "train",
    "short",
    "memory",
    "agent",
    "one",
    "step",
    "say",
    "agent",
    "agent",
    "dot",
    "train",
    "short",
    "memory",
    "gets",
    "look",
    "um",
    "actually",
    "uh",
    "short",
    "memory",
    "get",
    "parameters",
    "exactly",
    "put",
    "remember",
    "function",
    "train",
    "short",
    "memory",
    "gets",
    "variables",
    "call",
    "get",
    "hints",
    "strain",
    "let",
    "save",
    "file",
    "say",
    "agent",
    "dot",
    "train",
    "short",
    "memory",
    "get",
    "hints",
    "get",
    "actually",
    "want",
    "state",
    "action",
    "reward",
    "next",
    "state",
    "done",
    "let",
    "say",
    "let",
    "say",
    "state",
    "old",
    "action",
    "final",
    "move",
    "reward",
    "state",
    "new",
    "adds",
    "last",
    "thing",
    "done",
    "game",
    "state",
    "variable",
    "want",
    "remember",
    "store",
    "memory",
    "say",
    "agent",
    "dot",
    "remember",
    "gets",
    "um",
    "variables",
    "want",
    "store",
    "deck",
    "need",
    "check",
    "done",
    "game",
    "true",
    "want",
    "um",
    "want",
    "let",
    "write",
    "comment",
    "train",
    "long",
    "memory",
    "also",
    "called",
    "replay",
    "memory",
    "experience",
    "replay",
    "important",
    "agent",
    "trains",
    "previous",
    "moves",
    "games",
    "played",
    "tremendously",
    "helps",
    "improve",
    "also",
    "want",
    "plot",
    "results",
    "first",
    "want",
    "reset",
    "game",
    "simply",
    "saying",
    "game",
    "dot",
    "reset",
    "already",
    "function",
    "initializes",
    "game",
    "state",
    "resets",
    "everything",
    "score",
    "snake",
    "frame",
    "iteration",
    "places",
    "initial",
    "snake",
    "food",
    "want",
    "increase",
    "agents",
    "dot",
    "number",
    "games",
    "plus",
    "equals",
    "one",
    "want",
    "say",
    "agent",
    "dot",
    "train",
    "long",
    "memory",
    "need",
    "arguments",
    "want",
    "check",
    "new",
    "high",
    "score",
    "score",
    "greater",
    "current",
    "record",
    "say",
    "record",
    "equals",
    "new",
    "score",
    "also",
    "want",
    "leave",
    "want",
    "say",
    "agent",
    "dot",
    "model",
    "dot",
    "save",
    "later",
    "model",
    "want",
    "store",
    "also",
    "want",
    "um",
    "let",
    "print",
    "information",
    "print",
    "game",
    "current",
    "number",
    "score",
    "record",
    "let",
    "say",
    "game",
    "agent",
    "dot",
    "n",
    "games",
    "also",
    "want",
    "plot",
    "print",
    "score",
    "score",
    "want",
    "print",
    "current",
    "record",
    "record",
    "equals",
    "record",
    "want",
    "plotting",
    "implement",
    "next",
    "tutorial",
    "leave",
    "s8",
    "training",
    "function",
    "showed",
    "slides",
    "course",
    "implement",
    "functions",
    "get",
    "state",
    "function",
    "um",
    "let",
    "go",
    "back",
    "overview",
    "said",
    "store",
    "11",
    "values",
    "danger",
    "straight",
    "right",
    "left",
    "current",
    "direction",
    "one",
    "one",
    "position",
    "food",
    "left",
    "snake",
    "right",
    "snake",
    "snake",
    "11",
    "states",
    "let",
    "actually",
    "copy",
    "paste",
    "code",
    "make",
    "mistakes",
    "go",
    "first",
    "let",
    "grab",
    "head",
    "game",
    "calling",
    "game",
    "dot",
    "snake",
    "zero",
    "list",
    "first",
    "item",
    "head",
    "um",
    "let",
    "create",
    "points",
    "next",
    "head",
    "directions",
    "need",
    "check",
    "hits",
    "boundary",
    "danger",
    "use",
    "named",
    "tuple",
    "create",
    "point",
    "location",
    "minus",
    "20",
    "20",
    "hard",
    "coded",
    "number",
    "used",
    "block",
    "size",
    "like",
    "create",
    "four",
    "points",
    "around",
    "head",
    "current",
    "direction",
    "simply",
    "boolean",
    "check",
    "current",
    "game",
    "direction",
    "equals",
    "one",
    "one",
    "one",
    "one",
    "zero",
    "false",
    "um",
    "create",
    "array",
    "list",
    "11",
    "um",
    "states",
    "check",
    "danger",
    "straight",
    "ahead",
    "dependent",
    "current",
    "direction",
    "going",
    "right",
    "point",
    "right",
    "us",
    "gives",
    "us",
    "collision",
    "danger",
    "go",
    "left",
    "left",
    "point",
    "gets",
    "collision",
    "also",
    "danger",
    "dangerous",
    "straight",
    "danger",
    "right",
    "means",
    "go",
    "point",
    "right",
    "us",
    "would",
    "give",
    "collision",
    "danger",
    "right",
    "turn",
    "basically",
    "left",
    "might",
    "little",
    "bit",
    "tricky",
    "recommend",
    "pause",
    "go",
    "logic",
    "yeah",
    "give",
    "us",
    "three",
    "values",
    "state",
    "far",
    "move",
    "direction",
    "one",
    "true",
    "one",
    "false",
    "food",
    "location",
    "simply",
    "check",
    "food",
    "game",
    "food",
    "x",
    "smaller",
    "game",
    "head",
    "x",
    "food",
    "left",
    "us",
    "way",
    "check",
    "right",
    "convert",
    "list",
    "numpy",
    "array",
    "say",
    "data",
    "type",
    "nice",
    "little",
    "trick",
    "convert",
    "true",
    "false",
    "booleans",
    "zero",
    "one",
    "yeah",
    "get",
    "state",
    "method",
    "let",
    "move",
    "remember",
    "function",
    "want",
    "remember",
    "memory",
    "deck",
    "simple",
    "say",
    "self",
    "dot",
    "memory",
    "deck",
    "also",
    "append",
    "function",
    "want",
    "append",
    "order",
    "state",
    "action",
    "reward",
    "next",
    "state",
    "game",
    "state",
    "said",
    "exceeds",
    "maximum",
    "memory",
    "pop",
    "left",
    "max",
    "mem",
    "memory",
    "reached",
    "yeah",
    "remember",
    "function",
    "let",
    "start",
    "implementing",
    "train",
    "long",
    "short",
    "memory",
    "functions",
    "actually",
    "store",
    "model",
    "trainer",
    "let",
    "actually",
    "say",
    "self",
    "dot",
    "model",
    "equals",
    "let",
    "say",
    "none",
    "beginning",
    "leave",
    "self",
    "dot",
    "trainer",
    "equals",
    "none",
    "beginning",
    "objects",
    "create",
    "next",
    "tutorial",
    "call",
    "trainer",
    "actually",
    "optimization",
    "let",
    "start",
    "one",
    "step",
    "say",
    "get",
    "function",
    "call",
    "let",
    "call",
    "train",
    "step",
    "gets",
    "variables",
    "state",
    "action",
    "reward",
    "next",
    "state",
    "game",
    "overstate",
    "need",
    "train",
    "one",
    "game",
    "step",
    "design",
    "function",
    "um",
    "takes",
    "either",
    "one",
    "state",
    "like",
    "also",
    "take",
    "whole",
    "tensor",
    "numpy",
    "array",
    "uses",
    "multiple",
    "batch",
    "let",
    "take",
    "variables",
    "memory",
    "want",
    "grab",
    "batch",
    "beginning",
    "defined",
    "batch",
    "size",
    "1",
    "000",
    "want",
    "grab",
    "1",
    "000",
    "samples",
    "memory",
    "first",
    "check",
    "um",
    "already",
    "thousand",
    "samples",
    "memory",
    "say",
    "lang",
    "self",
    "dot",
    "memory",
    "smaller",
    "batch",
    "size",
    "simply",
    "actually",
    "let",
    "say",
    "greater",
    "greater",
    "want",
    "random",
    "sample",
    "say",
    "mini",
    "sample",
    "equals",
    "want",
    "get",
    "random",
    "sample",
    "use",
    "random",
    "dot",
    "sample",
    "already",
    "imported",
    "random",
    "module",
    "random",
    "dot",
    "sample",
    "self",
    "dot",
    "memory",
    "size",
    "batch",
    "size",
    "return",
    "list",
    "tuples",
    "forgot",
    "one",
    "important",
    "thing",
    "want",
    "store",
    "append",
    "want",
    "append",
    "one",
    "element",
    "one",
    "tuple",
    "need",
    "extra",
    "parenthesis",
    "one",
    "tuple",
    "store",
    "get",
    "batch",
    "size",
    "number",
    "tuples",
    "otherwise",
    "else",
    "uh",
    "thousand",
    "elements",
    "yet",
    "simply",
    "take",
    "whole",
    "memory",
    "say",
    "mini",
    "sample",
    "equals",
    "self",
    "dot",
    "memory",
    "want",
    "call",
    "training",
    "step",
    "let",
    "call",
    "multiple",
    "states",
    "let",
    "call",
    "states",
    "actions",
    "rewards",
    "next",
    "states",
    "done",
    "right",
    "format",
    "one",
    "tuple",
    "another",
    "want",
    "extract",
    "mini",
    "sample",
    "put",
    "every",
    "states",
    "together",
    "every",
    "action",
    "together",
    "every",
    "reward",
    "together",
    "actually",
    "really",
    "simple",
    "python",
    "say",
    "want",
    "extract",
    "states",
    "actions",
    "rewards",
    "next",
    "states",
    "duns",
    "game",
    "overs",
    "simply",
    "use",
    "built",
    "sip",
    "function",
    "use",
    "one",
    "asterisk",
    "mini",
    "sample",
    "argument",
    "yeah",
    "check",
    "know",
    "sip",
    "function",
    "works",
    "puts",
    "every",
    "states",
    "together",
    "every",
    "actions",
    "complicated",
    "also",
    "loop",
    "iterate",
    "mini",
    "sample",
    "basically",
    "say",
    "action",
    "state",
    "action",
    "rewards",
    "next",
    "state",
    "done",
    "one",
    "mini",
    "sample",
    "call",
    "one",
    "one",
    "argument",
    "yeah",
    "ways",
    "actually",
    "recommend",
    "way",
    "one",
    "argument",
    "faster",
    "pytorch",
    "right",
    "training",
    "functions",
    "need",
    "get",
    "action",
    "function",
    "beginning",
    "want",
    "random",
    "moves",
    "also",
    "called",
    "exploration",
    "exploitation",
    "deep",
    "learning",
    "point",
    "beginning",
    "one",
    "want",
    "make",
    "sure",
    "also",
    "make",
    "random",
    "moves",
    "explore",
    "environment",
    "better",
    "model",
    "agent",
    "gets",
    "less",
    "random",
    "moves",
    "want",
    "want",
    "exploit",
    "agent",
    "model",
    "yeah",
    "want",
    "use",
    "epsilon",
    "parameter",
    "initialized",
    "beginning",
    "let",
    "implement",
    "first",
    "say",
    "self",
    "dot",
    "epsilon",
    "equals",
    "dependent",
    "number",
    "games",
    "hard",
    "code",
    "80",
    "minus",
    "self",
    "dot",
    "number",
    "games",
    "play",
    "around",
    "let",
    "get",
    "final",
    "move",
    "beginning",
    "say",
    "zero",
    "zero",
    "zero",
    "one",
    "true",
    "first",
    "let",
    "check",
    "random",
    "dot",
    "rent",
    "int",
    "0",
    "200",
    "smaller",
    "self",
    "dot",
    "epsilon",
    "take",
    "random",
    "move",
    "say",
    "move",
    "equals",
    "random",
    "dot",
    "rant",
    "ins",
    "must",
    "0",
    "2",
    "2",
    "actually",
    "included",
    "give",
    "us",
    "random",
    "value",
    "0",
    "1",
    "2",
    "index",
    "must",
    "set",
    "one",
    "say",
    "final",
    "move",
    "move",
    "index",
    "equals",
    "one",
    "yeah",
    "games",
    "smaller",
    "epsilon",
    "get",
    "smaller",
    "epsilon",
    "get",
    "less",
    "frequent",
    "less",
    "epsilon",
    "even",
    "even",
    "become",
    "negative",
    "longer",
    "random",
    "move",
    "fast",
    "feel",
    "free",
    "pause",
    "think",
    "logic",
    "otherwise",
    "else",
    "actually",
    "want",
    "move",
    "based",
    "model",
    "want",
    "get",
    "prediction",
    "prediction",
    "equals",
    "self",
    "dot",
    "model",
    "dot",
    "predict",
    "wants",
    "predict",
    "action",
    "based",
    "one",
    "state",
    "call",
    "state",
    "zero",
    "get",
    "want",
    "convert",
    "tensor",
    "say",
    "state",
    "0",
    "equals",
    "torch",
    "dot",
    "tensor",
    "input",
    "gets",
    "state",
    "also",
    "give",
    "data",
    "type",
    "equals",
    "let",
    "use",
    "torch",
    "dot",
    "float",
    "call",
    "predict",
    "state",
    "give",
    "us",
    "prediction",
    "raw",
    "value",
    "go",
    "back",
    "slide",
    "raw",
    "value",
    "take",
    "maximum",
    "set",
    "index",
    "a1",
    "say",
    "move",
    "equals",
    "get",
    "saying",
    "torch",
    "arc",
    "max",
    "arc",
    "max",
    "prediction",
    "tensor",
    "convert",
    "one",
    "number",
    "call",
    "item",
    "integer",
    "say",
    "final",
    "move",
    "smooth",
    "index",
    "one",
    "return",
    "final",
    "move",
    "return",
    "yeah",
    "need",
    "save",
    "like",
    "need",
    "agent",
    "class",
    "next",
    "one",
    "must",
    "implement",
    "model",
    "trainer",
    "also",
    "plotting",
    "let",
    "go",
    "back",
    "code",
    "left",
    "essay",
    "need",
    "model",
    "trainer",
    "let",
    "create",
    "new",
    "file",
    "let",
    "call",
    "model",
    "dot",
    "pi",
    "let",
    "first",
    "import",
    "things",
    "need",
    "need",
    "import",
    "torch",
    "want",
    "import",
    "torch",
    "dot",
    "n",
    "n",
    "n",
    "n",
    "want",
    "import",
    "torch",
    "dot",
    "optim",
    "optim",
    "also",
    "import",
    "torch",
    "dot",
    "n",
    "n",
    "dot",
    "functional",
    "capital",
    "f",
    "also",
    "want",
    "import",
    "save",
    "model",
    "want",
    "implement",
    "two",
    "classes",
    "one",
    "model",
    "one",
    "trainer",
    "let",
    "create",
    "class",
    "let",
    "call",
    "linear",
    "underscore",
    "qnet",
    "inherit",
    "nn",
    "dot",
    "module",
    "module",
    "way",
    "comfortable",
    "pytorch",
    "want",
    "learn",
    "use",
    "framework",
    "beginner",
    "series",
    "tutorial",
    "free",
    "put",
    "link",
    "description",
    "teach",
    "everything",
    "need",
    "get",
    "started",
    "pytorch",
    "right",
    "let",
    "start",
    "implementing",
    "linear",
    "qnet",
    "function",
    "need",
    "init",
    "function",
    "define",
    "init",
    "need",
    "self",
    "gets",
    "input",
    "size",
    "input",
    "size",
    "hidden",
    "size",
    "output",
    "size",
    "first",
    "thing",
    "want",
    "call",
    "super",
    "initializer",
    "call",
    "super",
    "um",
    "simple",
    "look",
    "slides",
    "models",
    "feed",
    "forward",
    "neural",
    "net",
    "input",
    "layer",
    "hidden",
    "layer",
    "output",
    "layer",
    "um",
    "feel",
    "free",
    "extend",
    "improve",
    "works",
    "fine",
    "case",
    "actually",
    "bad",
    "let",
    "create",
    "two",
    "linear",
    "layers",
    "let",
    "call",
    "equals",
    "gets",
    "input",
    "size",
    "input",
    "hidden",
    "size",
    "output",
    "size",
    "equals",
    "gets",
    "hidden",
    "size",
    "input",
    "output",
    "size",
    "output",
    "always",
    "pi",
    "torch",
    "implement",
    "forward",
    "function",
    "self",
    "gets",
    "x",
    "tensor",
    "want",
    "first",
    "want",
    "apply",
    "linear",
    "layer",
    "also",
    "use",
    "actuation",
    "function",
    "know",
    "check",
    "beginner",
    "tutorial",
    "series",
    "explain",
    "say",
    "x",
    "call",
    "f",
    "dot",
    "reloose",
    "use",
    "directly",
    "functional",
    "module",
    "say",
    "self",
    "dot",
    "linear",
    "one",
    "tensor",
    "x",
    "input",
    "first",
    "linear",
    "layer",
    "apply",
    "actuation",
    "function",
    "apply",
    "second",
    "layer",
    "call",
    "self",
    "dot",
    "linear",
    "2",
    "x",
    "need",
    "actuation",
    "function",
    "end",
    "simply",
    "use",
    "raw",
    "numbers",
    "return",
    "x",
    "forward",
    "function",
    "let",
    "also",
    "implement",
    "helper",
    "function",
    "save",
    "model",
    "later",
    "let",
    "call",
    "self",
    "safe",
    "gets",
    "file",
    "name",
    "input",
    "use",
    "default",
    "say",
    "model",
    "dot",
    "pth",
    "simply",
    "file",
    "name",
    "last",
    "time",
    "think",
    "already",
    "called",
    "function",
    "um",
    "yet",
    "comment",
    "new",
    "high",
    "score",
    "call",
    "agent",
    "dot",
    "model",
    "dot",
    "save",
    "let",
    "create",
    "new",
    "folder",
    "let",
    "say",
    "model",
    "folder",
    "path",
    "equals",
    "let",
    "create",
    "new",
    "folder",
    "current",
    "directory",
    "call",
    "model",
    "dot",
    "slash",
    "model",
    "check",
    "already",
    "exists",
    "file",
    "folder",
    "say",
    "os",
    "dot",
    "path",
    "dot",
    "exists",
    "say",
    "model",
    "folder",
    "path",
    "create",
    "say",
    "os",
    "dot",
    "makers",
    "want",
    "make",
    "model",
    "folder",
    "path",
    "create",
    "final",
    "file",
    "name",
    "say",
    "file",
    "name",
    "equals",
    "os",
    "dot",
    "path",
    "dot",
    "join",
    "want",
    "join",
    "model",
    "folder",
    "path",
    "file",
    "name",
    "use",
    "input",
    "file",
    "name",
    "saving",
    "want",
    "save",
    "torch",
    "dot",
    "save",
    "want",
    "save",
    "self",
    "dot",
    "state",
    "dict",
    "also",
    "tutorial",
    "saving",
    "model",
    "need",
    "save",
    "state",
    "dictionary",
    "path",
    "use",
    "file",
    "name",
    "need",
    "linear",
    "q",
    "net",
    "actual",
    "training",
    "optimization",
    "also",
    "class",
    "call",
    "q",
    "trainer",
    "q",
    "trainer",
    "want",
    "want",
    "implement",
    "init",
    "function",
    "gets",
    "self",
    "also",
    "gets",
    "model",
    "get",
    "learning",
    "rate",
    "get",
    "gamma",
    "parameter",
    "simply",
    "store",
    "everything",
    "equals",
    "lr",
    "self",
    "dot",
    "gamma",
    "equals",
    "gamma",
    "also",
    "store",
    "model",
    "say",
    "self",
    "dot",
    "model",
    "equals",
    "model",
    "pie",
    "charge",
    "optimization",
    "step",
    "need",
    "optimizer",
    "create",
    "calling",
    "let",
    "call",
    "optimizer",
    "equals",
    "get",
    "module",
    "choose",
    "one",
    "optimizer",
    "use",
    "atom",
    "optimizer",
    "want",
    "optimize",
    "function",
    "also",
    "needs",
    "learning",
    "rate",
    "lr",
    "equals",
    "self",
    "dot",
    "l",
    "r",
    "also",
    "need",
    "criterion",
    "loss",
    "function",
    "let",
    "call",
    "self",
    "dot",
    "criterion",
    "equals",
    "go",
    "back",
    "slides",
    "end",
    "learned",
    "first",
    "part",
    "nothing",
    "else",
    "mean",
    "squared",
    "error",
    "simple",
    "create",
    "saying",
    "equals",
    "loss",
    "need",
    "initializer",
    "also",
    "need",
    "define",
    "call",
    "train",
    "step",
    "function",
    "gets",
    "self",
    "needs",
    "stored",
    "um",
    "parameters",
    "last",
    "time",
    "needs",
    "let",
    "look",
    "call",
    "needs",
    "state",
    "final",
    "move",
    "reward",
    "new",
    "states",
    "done",
    "let",
    "copy",
    "paste",
    "rename",
    "slightly",
    "state",
    "action",
    "reward",
    "new",
    "state",
    "uh",
    "let",
    "call",
    "next",
    "state",
    "done",
    "stay",
    "let",
    "simply",
    "pass",
    "implement",
    "let",
    "go",
    "back",
    "agent",
    "set",
    "say",
    "call",
    "model",
    "want",
    "import",
    "linear",
    "think",
    "call",
    "linear",
    "q",
    "net",
    "q",
    "trainer",
    "initializer",
    "want",
    "create",
    "instance",
    "model",
    "trainer",
    "equals",
    "linear",
    "qnet",
    "needs",
    "input",
    "size",
    "hidden",
    "size",
    "output",
    "size",
    "use",
    "11",
    "256",
    "three",
    "remember",
    "look",
    "slides",
    "um",
    "first",
    "one",
    "size",
    "state",
    "11",
    "values",
    "output",
    "must",
    "three",
    "three",
    "different",
    "um",
    "three",
    "different",
    "numbers",
    "action",
    "play",
    "around",
    "hidden",
    "size",
    "ones",
    "eleven",
    "three",
    "model",
    "trainer",
    "equals",
    "q",
    "trainer",
    "gets",
    "model",
    "gets",
    "learning",
    "rate",
    "equals",
    "learning",
    "rate",
    "specified",
    "also",
    "pass",
    "gamma",
    "value",
    "gamma",
    "equals",
    "self",
    "dot",
    "gamma",
    "gamma",
    "discount",
    "rate",
    "value",
    "smaller",
    "1",
    "usually",
    "around",
    "case",
    "let",
    "set",
    "play",
    "around",
    "well",
    "keep",
    "mind",
    "must",
    "smaller",
    "one",
    "made",
    "one",
    "error",
    "last",
    "tutorial",
    "important",
    "fix",
    "right",
    "get",
    "action",
    "function",
    "actually",
    "called",
    "predict",
    "actually",
    "pythog",
    "predict",
    "function",
    "would",
    "api",
    "tensorflow",
    "example",
    "pi",
    "torch",
    "simply",
    "call",
    "like",
    "execute",
    "forward",
    "function",
    "actually",
    "prediction",
    "yeah",
    "please",
    "make",
    "sure",
    "fix",
    "okay",
    "everything",
    "look",
    "go",
    "back",
    "see",
    "call",
    "train",
    "step",
    "one",
    "parameter",
    "also",
    "multiple",
    "ones",
    "want",
    "make",
    "sure",
    "handle",
    "different",
    "sizes",
    "let",
    "start",
    "implementing",
    "function",
    "first",
    "thing",
    "want",
    "right",
    "um",
    "either",
    "tuple",
    "list",
    "single",
    "value",
    "let",
    "convert",
    "pi",
    "torch",
    "tensor",
    "let",
    "copy",
    "paste",
    "states",
    "next",
    "state",
    "action",
    "reward",
    "calling",
    "variable",
    "specify",
    "data",
    "type",
    "torch",
    "dot",
    "float",
    "done",
    "game",
    "value",
    "need",
    "tensor",
    "want",
    "handle",
    "multiple",
    "sizes",
    "want",
    "check",
    "length",
    "check",
    "state",
    "dot",
    "shape",
    "one",
    "one",
    "dimension",
    "want",
    "reshape",
    "right",
    "case",
    "one",
    "number",
    "actually",
    "want",
    "form",
    "one",
    "values",
    "number",
    "um",
    "batches",
    "already",
    "already",
    "multiple",
    "values",
    "already",
    "size",
    "n",
    "x",
    "already",
    "correct",
    "want",
    "append",
    "one",
    "dimension",
    "torch",
    "unsqueeze",
    "function",
    "say",
    "state",
    "equals",
    "states",
    "dot",
    "sorry",
    "state",
    "torch",
    "dot",
    "un",
    "squeeze",
    "squeeze",
    "states",
    "want",
    "put",
    "dimension",
    "zero",
    "axis",
    "zero",
    "means",
    "appends",
    "one",
    "dimension",
    "beginning",
    "one",
    "also",
    "wanted",
    "um",
    "tensors",
    "next",
    "state",
    "action",
    "reward",
    "done",
    "value",
    "also",
    "want",
    "convert",
    "right",
    "single",
    "value",
    "want",
    "convert",
    "tuple",
    "like",
    "done",
    "define",
    "tuple",
    "one",
    "value",
    "um",
    "correct",
    "shape",
    "implement",
    "um",
    "last",
    "time",
    "first",
    "tutorial",
    "showed",
    "bellman",
    "equation",
    "simplified",
    "old",
    "queue",
    "simply",
    "call",
    "model",
    "predict",
    "old",
    "state",
    "new",
    "queue",
    "formula",
    "let",
    "first",
    "let",
    "um",
    "write",
    "comment",
    "first",
    "thing",
    "want",
    "get",
    "predicted",
    "predicted",
    "q",
    "values",
    "current",
    "state",
    "simply",
    "let",
    "call",
    "prediction",
    "equals",
    "self",
    "dot",
    "model",
    "want",
    "state",
    "0",
    "call",
    "state",
    "second",
    "part",
    "need",
    "formula",
    "reward",
    "plus",
    "gamma",
    "value",
    "times",
    "maximum",
    "model",
    "predict",
    "state",
    "one",
    "first",
    "let",
    "write",
    "new",
    "uh",
    "comment",
    "first",
    "thing",
    "want",
    "apply",
    "formula",
    "reward",
    "plus",
    "gamma",
    "times",
    "next",
    "predicted",
    "q",
    "value",
    "want",
    "maximum",
    "maximum",
    "maximum",
    "um",
    "little",
    "bit",
    "tricky",
    "maximum",
    "um",
    "sorry",
    "let",
    "like",
    "maximum",
    "next",
    "predicted",
    "q",
    "value",
    "one",
    "value",
    "um",
    "like",
    "first",
    "um",
    "parameter",
    "predictions",
    "actually",
    "action",
    "actually",
    "three",
    "different",
    "values",
    "get",
    "clone",
    "set",
    "index",
    "action",
    "new",
    "q",
    "value",
    "let",
    "call",
    "q",
    "new",
    "like",
    "showed",
    "formula",
    "set",
    "let",
    "call",
    "predictions",
    "index",
    "arc",
    "max",
    "action",
    "set",
    "q",
    "new",
    "value",
    "might",
    "tricky",
    "want",
    "calculate",
    "new",
    "q",
    "value",
    "formula",
    "showed",
    "need",
    "format",
    "simply",
    "clone",
    "three",
    "values",
    "two",
    "values",
    "value",
    "action",
    "action",
    "example",
    "one",
    "zero",
    "zero",
    "um",
    "index",
    "one",
    "set",
    "new",
    "q",
    "value",
    "want",
    "let",
    "first",
    "let",
    "create",
    "clone",
    "target",
    "equals",
    "prediction",
    "dot",
    "clone",
    "pi",
    "torch",
    "tensor",
    "um",
    "want",
    "iterate",
    "tensors",
    "apply",
    "formula",
    "say",
    "index",
    "range",
    "length",
    "let",
    "call",
    "done",
    "everything",
    "size",
    "works",
    "iterate",
    "one",
    "thing",
    "mention",
    "far",
    "want",
    "done",
    "um",
    "otherwise",
    "simply",
    "take",
    "whole",
    "reward",
    "say",
    "q",
    "new",
    "equals",
    "reward",
    "current",
    "current",
    "index",
    "check",
    "done",
    "say",
    "done",
    "done",
    "current",
    "index",
    "apply",
    "formula",
    "say",
    "q",
    "new",
    "actually",
    "um",
    "reward",
    "reward",
    "current",
    "index",
    "plus",
    "self",
    "dot",
    "gamma",
    "times",
    "torch",
    "dot",
    "max",
    "maximum",
    "value",
    "next",
    "prediction",
    "self",
    "dot",
    "model",
    "next",
    "state",
    "index",
    "exactly",
    "written",
    "need",
    "set",
    "target",
    "maximum",
    "value",
    "action",
    "value",
    "get",
    "let",
    "call",
    "target",
    "target",
    "current",
    "index",
    "arc",
    "max",
    "action",
    "say",
    "torch",
    "dot",
    "arc",
    "max",
    "action",
    "want",
    "item",
    "value",
    "tensor",
    "q",
    "new",
    "value",
    "might",
    "little",
    "bit",
    "tricky",
    "understand",
    "recommend",
    "pause",
    "go",
    "everything",
    "need",
    "let",
    "look",
    "slides",
    "q",
    "q",
    "new",
    "apply",
    "loss",
    "function",
    "mean",
    "squared",
    "error",
    "pi",
    "torch",
    "simply",
    "use",
    "optimizer",
    "step",
    "first",
    "call",
    "zero",
    "grad",
    "function",
    "empty",
    "gradient",
    "something",
    "remember",
    "pi",
    "torch",
    "calculate",
    "loss",
    "calling",
    "self",
    "dot",
    "criterion",
    "put",
    "target",
    "prediction",
    "q",
    "new",
    "q",
    "call",
    "loss",
    "dot",
    "backward",
    "apply",
    "back",
    "propagation",
    "update",
    "gradients",
    "call",
    "need",
    "training",
    "step",
    "actually",
    "need",
    "model",
    "file",
    "let",
    "go",
    "back",
    "agent",
    "guess",
    "already",
    "set",
    "q",
    "trainer",
    "train",
    "call",
    "train",
    "step",
    "function",
    "either",
    "one",
    "parameters",
    "whole",
    "batch",
    "function",
    "handle",
    "different",
    "sizes",
    "thing",
    "left",
    "actually",
    "plot",
    "results",
    "let",
    "create",
    "new",
    "file",
    "let",
    "call",
    "hell",
    "helper",
    "dot",
    "pi",
    "let",
    "actually",
    "copy",
    "paste",
    "simple",
    "function",
    "matplotlib",
    "python",
    "yeah",
    "want",
    "plot",
    "scores",
    "list",
    "want",
    "plot",
    "plot",
    "mean",
    "score",
    "let",
    "create",
    "agent",
    "say",
    "helper",
    "import",
    "plot",
    "function",
    "training",
    "function",
    "already",
    "created",
    "empty",
    "list",
    "scores",
    "mean",
    "scores",
    "um",
    "game",
    "want",
    "append",
    "score",
    "let",
    "remove",
    "implement",
    "say",
    "plot",
    "scores",
    "dot",
    "appends",
    "current",
    "score",
    "let",
    "calculate",
    "new",
    "mean",
    "average",
    "score",
    "let",
    "say",
    "total",
    "score",
    "plus",
    "equals",
    "score",
    "let",
    "call",
    "mean",
    "score",
    "equals",
    "total",
    "score",
    "divided",
    "number",
    "games",
    "agent",
    "games",
    "append",
    "plot",
    "mean",
    "scores",
    "dot",
    "append",
    "mean",
    "score",
    "simply",
    "call",
    "plot",
    "function",
    "plot",
    "scores",
    "plot",
    "mean",
    "scores",
    "let",
    "save",
    "file",
    "also",
    "let",
    "save",
    "file",
    "let",
    "try",
    "terminal",
    "let",
    "call",
    "agent",
    "dot",
    "pi",
    "let",
    "cross",
    "fingers",
    "syntax",
    "error",
    "file",
    "um",
    "actually",
    "two",
    "equal",
    "signs",
    "let",
    "fix",
    "save",
    "run",
    "made",
    "another",
    "mistake",
    "name",
    "error",
    "actually",
    "called",
    "let",
    "save",
    "run",
    "starts",
    "training",
    "without",
    "crashing",
    "also",
    "plots",
    "let",
    "let",
    "run",
    "see",
    "improving",
    "music",
    "right",
    "see",
    "algorithm",
    "works",
    "snake",
    "getting",
    "better",
    "better",
    "scores",
    "getting",
    "higher",
    "higher",
    "also",
    "mean",
    "average",
    "score",
    "getting",
    "higher",
    "forgot",
    "one",
    "important",
    "thing",
    "show",
    "second",
    "um",
    "snake",
    "perfect",
    "main",
    "issues",
    "traps",
    "sometimes",
    "also",
    "sometimes",
    "gets",
    "stuck",
    "endless",
    "loop",
    "sequence",
    "something",
    "improve",
    "homework",
    "yeah",
    "like",
    "trapped",
    "yeah",
    "let",
    "stop",
    "actually",
    "show",
    "forgot",
    "game",
    "actually",
    "um",
    "set",
    "speed",
    "human",
    "controlled",
    "game",
    "want",
    "play",
    "set",
    "20",
    "recommend",
    "set",
    "larger",
    "number",
    "training",
    "faster",
    "example",
    "use",
    "40",
    "even",
    "higher",
    "go",
    "40",
    "yeah",
    "think",
    "whole",
    "code",
    "also",
    "find",
    "github",
    "yeah",
    "hope",
    "really",
    "enjoyed",
    "little",
    "series",
    "reinforcement",
    "learning",
    "enjoyed",
    "please",
    "hit",
    "like",
    "button",
    "consider",
    "subscribing",
    "channel",
    "hope",
    "see",
    "next",
    "time",
    "bye"
  ],
  "keywords": [
    "python",
    "train",
    "play",
    "snake",
    "game",
    "reinforcement",
    "learning",
    "going",
    "everything",
    "start",
    "agent",
    "deep",
    "pie",
    "torch",
    "also",
    "need",
    "think",
    "let",
    "show",
    "final",
    "dot",
    "pi",
    "training",
    "see",
    "plot",
    "scores",
    "score",
    "point",
    "environment",
    "make",
    "random",
    "move",
    "get",
    "better",
    "first",
    "games",
    "around",
    "take",
    "little",
    "bit",
    "right",
    "straight",
    "food",
    "hit",
    "beginning",
    "know",
    "part",
    "implement",
    "called",
    "means",
    "last",
    "model",
    "pytorch",
    "actions",
    "reward",
    "might",
    "say",
    "remember",
    "basically",
    "case",
    "give",
    "based",
    "next",
    "action",
    "yeah",
    "different",
    "use",
    "q",
    "predict",
    "tutorial",
    "code",
    "said",
    "loop",
    "step",
    "gets",
    "current",
    "must",
    "store",
    "calculate",
    "state",
    "um",
    "new",
    "information",
    "old",
    "call",
    "linear",
    "needs",
    "us",
    "look",
    "like",
    "example",
    "plus",
    "else",
    "stay",
    "zero",
    "simple",
    "left",
    "go",
    "turn",
    "three",
    "direction",
    "1",
    "0",
    "would",
    "states",
    "11",
    "values",
    "danger",
    "true",
    "one",
    "set",
    "input",
    "layer",
    "hidden",
    "output",
    "size",
    "simply",
    "maximum",
    "back",
    "keep",
    "value",
    "want",
    "parameters",
    "later",
    "update",
    "loss",
    "function",
    "actually",
    "rate",
    "gamma",
    "parameter",
    "time",
    "two",
    "mean",
    "error",
    "already",
    "implementing",
    "showed",
    "install",
    "create",
    "n",
    "name",
    "equals",
    "thing",
    "done",
    "grab",
    "paste",
    "file",
    "change",
    "return",
    "write",
    "reset",
    "iteration",
    "collision",
    "check",
    "tuple",
    "class",
    "list",
    "head",
    "helper",
    "define",
    "self",
    "slides",
    "formula",
    "clockwise",
    "index",
    "array",
    "numpy",
    "import",
    "put",
    "x",
    "memory",
    "batch",
    "functions",
    "short",
    "max",
    "trainer",
    "record",
    "save",
    "number",
    "smaller",
    "convert",
    "append",
    "tensor",
    "sample",
    "prediction",
    "apply",
    "folder",
    "path"
  ]
}