{
  "text": "Hello World! It's Siraj\nand let's make our own language translator using TensorFlow\nToday there are about 6,800 different languages spoken across the world\nand in an increasingly globalised world\nnearly every culture has interactions with every other culture in some way\nthat means there are an incalculable number of translation requirements every second of every day across the world\nTranslating is no easy task\nA language isn't just a collection of words and of rules of grammar and syntax\nit's also a vast inter-connecting system of cultural references and connotations\nand this reflects a centuries old problem of\ntwo cultures wanting to communicate but are blocked by a language barrier\nour translation systems are fast improving though, so whether it be an idea or a story or a quest\neach new advancement means one less message will be lost in translation.\nDuring the Second World war, the British government was hard at work\ntrying to decrypt the morse coded radio communications that Nazi Germany used to send\nmessages sucurely, known as Enigma.\nThey decided to hire a man named Alan Turing to help in their effort\nand when the American government learnt of their translation effort\nthey were inspired to try it themselves, post war.\nSpecifically because they needed a way to keep up with Russian scientific publications.\nThe first public demo of a machine translation system, translated 250 words between Russian and English in 1954.\nIt was dictionary based so it would attempt to match the source language to the target language\nword for word\nThe results were poor since it didn't capture syntactic structure.\nThe second generation of systems used Interlingua\nthat means they changed a source language to a special intermediary language\nwith specific rules encoded into it\nthen from that generated the target language.\nThis proved to be more efficient, but this approach was soon overshadowed by the rise of statistical translation in the early 90s\nPrimarily from engineers at IBM\nInnovation at IBM\nWatch this\nA popular approach was to break the source text down into segments\nthen compare them to an aligned bi-lingual corpus\nusing statistical evidence and probabilities to choose the most likely translation.\nNowadays the most used statistical translation system in the world is Google Translate\nand with good reason.\nGoogle uses deep learning to translate from a given language to another with state of the art results\nSo how do they do this?\nLet's recreate their results in TensorFlow to find out\nThe dataset we'll be using to train our language translation model\nis a corpus of transcribed TED talks\nIt's got both the English version and the French version\nand our goal will be to create a model that can translate from one to the other after training.\nWe'll be using TensorFlow's built in data_utils class\nto help us pre-process our data set and we'll start by defining our vocab size\nwhich is the number of words we want to train on from our dataset.\nWe'll set it to 40k for each which is a small portion of the data\nthen we'll use the data_utils class to read the data from the data directory.\nGiving it our desired vocab size and it will return the formatted and tokenised words in both languages\nWe'll then initialise TensorFlow placeholders for our encoder and decoder inputs\nBoth will be integer tensors that represent discrete values\nthey will be embedded into a dense representation later\nWe'll feed our vocabulary word to the encoder and the encoded representation that's learnt to the decoder.\nNow we can build our model\nGoogle published a paper more recently discussing a system they integrated into their translation service\ncalled Neural Machine Translation.\nIt's an encoder decoder model inspired by similar work from other papers on topics like text summarisation.\nSo whereas as before Google Translate would translate from language A to English to language B\nwith this new NMT architecture, it can translate directly from one language to the other\nIt doesn't memorise phrase to phrase translations\ninstead it encodes the semantics of the sentence.\nThis encoding is generalised so it can even translate between a language pair like Japanese and Korean\nthat it hasn't explicitly seen before.\nSo I guess we can use a LSTM recurrent network to encode a sentence in language A\nthe RNN spits out a hidden state 's' which represents the vectorised contents of the sentence.\nWe can then feed 's' to the decoder which will generate the translated sentence in language B, word by word.\nSounds easy enough right?\nWRONG!\nThere is a drawback to this architecture, it has limited memory\nthat hidden state 's' of the LSTM is where we're trying to cram the whole sentence we want to translate\n's' is usually only a few hundred floating point numbers long\nThe more we try to force our sentence into this fixed dimensionality vector\nthe more lossy our neural net is forced to be.\nWe could increase the hidden size of the LSTM after all they're supposed to remember long term dependencies\nbut what happens is as we increase  the hidden size 'h' of the LSTM\nthe training time increases exponentially.\nSo to solve this we're going to bring 'Attention' into the mix\nIf I was translating a long sentence, I'd probably glance back at the source sentence a couple times\nto make sure I was capturing all the details.\nI'd iteratively pay attention to the relevant parts of the source sentence\nWe can let neural nets do the same by letting them store and refer to previous outputs of the LSTM\nThis increases the  storage of our model without changing the functionality of the LSTM\nSo the idea is once we have LSTM outputs from the encoder stored\nwe can query each output asking how relevant they are to the computation happening in the encoder\nEach encoder output gets a relevancy score which we can convert to a probability score\nby applying a softmax activation to it.\nThen we extract a context vector which is a weighted summation to the encoder outputs\ndepending on how relevant they are.\nMemory ain't enough, pay attention\nMemory ain't enough, pay attention (In Hindi)\nMemory ain't enough, pay attention (In German)\nMemory ain't enough, pay attention (In Spanish)\nMemory ain't enough, pay attention\nWe build our model using TensorFlow's built in embedding attention sequence to sequence function\ngiving it our encoder and decoder inputs as well as a few hyper parameters we define like\nthe number of layers.\nIt builds a model that is just like the one we discussed\nTensorFlow has several built in models like this that we can drop into our code easily\nSo normally this alone would be fine and we could run this and the results would be decent\nbut they added another improvement to their model that requires\nMORE CODE\nA 100 GPUs\nand a WEEK OF TRAINING\nSeriously that's what it took\nwe won't implement it all programatically but let's dive into it conceptually\nIf the outputs don't have sufficient context then they won't be able to give a good answer\nwe need to include info about future words, so that the encoder output is determined by the words on the left and the right.\nWe humans would definitely use this kind of full context to determine the meaning of a word we see in a sentence.\nThe way they did this is tho use a bi-directional encoder\nso it's two RNNs.\nOne that goes forward over the sentence\nand the other goes backwards.\nSo for each word it concatenates the vector outputs\nwhich produces a vector with context from both sides.\nand they added a lot of layers to their model.\nThe encoder has one bi-directional RNN layer and seven uni-direciotnal RNN layers\nThe decoder has eight uni-directional RNN layers\nThe more layers the longer the training times\nso that's why we use a single bi-directional layer\nif all the layers were bi-directional\nthe whole layer would have to finish before layer dependencies could start computing\nBut by using uni-directional layers, computation is going to be more parallel.\nWe'll initialise our TensorFlow section, then our model inside of it\nLet's see some results after training.\nFirst I'll give it this phrase\nLooks good\nand now another phrase\nDOPE!\nWhile it's not perfect and we still have a way to go\nwe're definitely getting closer to having a universal translation model.\nBreaking it down\nEncoder-Decoder architectures are for state-of-the-art performance in machine translation\nby storing the previous outputs of the LSTM cells\nwe can judge the relevancy of each to decide which to use\nvia an attention mechanism.\nAnd by using a bi-directional RNN, the context of both past and future words is used to create an accurate encoder output vector.\nThe coding challenge winner from last week is Ryan Lee\nThis was very impressive, he created a recipe summariser by scraping a 125,000 recipes from the web\nand documented it all beautifully with installation steps so you can reproduce the results yourself.\nWIZARD OF THE WEEK!\nand the runner up is Sarah Collins\nher code converts scientific papers to text and prioritises them by topic.\nThis weeks coding challenge is to create a simple translation system using an encoder-decoder model.\nAll the details are in the readme, post your github link in the comments and\nI'll announce the winner next week.\nPlease subscribe for more programming videos\nCheck out this related video\nand for now I've got to get a better GPU\nSo, thanks for watching!\n",
  "words": [
    "hello",
    "world",
    "siraj",
    "let",
    "make",
    "language",
    "translator",
    "using",
    "tensorflow",
    "today",
    "different",
    "languages",
    "spoken",
    "across",
    "world",
    "increasingly",
    "globalised",
    "world",
    "nearly",
    "every",
    "culture",
    "interactions",
    "every",
    "culture",
    "way",
    "means",
    "incalculable",
    "number",
    "translation",
    "requirements",
    "every",
    "second",
    "every",
    "day",
    "across",
    "world",
    "translating",
    "easy",
    "task",
    "language",
    "collection",
    "words",
    "rules",
    "grammar",
    "syntax",
    "also",
    "vast",
    "system",
    "cultural",
    "references",
    "connotations",
    "reflects",
    "centuries",
    "old",
    "problem",
    "two",
    "cultures",
    "wanting",
    "communicate",
    "blocked",
    "language",
    "barrier",
    "translation",
    "systems",
    "fast",
    "improving",
    "though",
    "whether",
    "idea",
    "story",
    "quest",
    "new",
    "advancement",
    "means",
    "one",
    "less",
    "message",
    "lost",
    "translation",
    "second",
    "world",
    "war",
    "british",
    "government",
    "hard",
    "work",
    "trying",
    "decrypt",
    "morse",
    "coded",
    "radio",
    "communications",
    "nazi",
    "germany",
    "used",
    "send",
    "messages",
    "sucurely",
    "known",
    "enigma",
    "decided",
    "hire",
    "man",
    "named",
    "alan",
    "turing",
    "help",
    "effort",
    "american",
    "government",
    "learnt",
    "translation",
    "effort",
    "inspired",
    "try",
    "post",
    "war",
    "specifically",
    "needed",
    "way",
    "keep",
    "russian",
    "scientific",
    "publications",
    "first",
    "public",
    "demo",
    "machine",
    "translation",
    "system",
    "translated",
    "250",
    "words",
    "russian",
    "english",
    "dictionary",
    "based",
    "would",
    "attempt",
    "match",
    "source",
    "language",
    "target",
    "language",
    "word",
    "word",
    "results",
    "poor",
    "since",
    "capture",
    "syntactic",
    "structure",
    "second",
    "generation",
    "systems",
    "used",
    "interlingua",
    "means",
    "changed",
    "source",
    "language",
    "special",
    "intermediary",
    "language",
    "specific",
    "rules",
    "encoded",
    "generated",
    "target",
    "language",
    "proved",
    "efficient",
    "approach",
    "soon",
    "overshadowed",
    "rise",
    "statistical",
    "translation",
    "early",
    "90s",
    "primarily",
    "engineers",
    "ibm",
    "innovation",
    "ibm",
    "watch",
    "popular",
    "approach",
    "break",
    "source",
    "text",
    "segments",
    "compare",
    "aligned",
    "corpus",
    "using",
    "statistical",
    "evidence",
    "probabilities",
    "choose",
    "likely",
    "translation",
    "nowadays",
    "used",
    "statistical",
    "translation",
    "system",
    "world",
    "google",
    "translate",
    "good",
    "reason",
    "google",
    "uses",
    "deep",
    "learning",
    "translate",
    "given",
    "language",
    "another",
    "state",
    "art",
    "results",
    "let",
    "recreate",
    "results",
    "tensorflow",
    "find",
    "dataset",
    "using",
    "train",
    "language",
    "translation",
    "model",
    "corpus",
    "transcribed",
    "ted",
    "talks",
    "got",
    "english",
    "version",
    "french",
    "version",
    "goal",
    "create",
    "model",
    "translate",
    "one",
    "training",
    "using",
    "tensorflow",
    "built",
    "class",
    "help",
    "us",
    "data",
    "set",
    "start",
    "defining",
    "vocab",
    "size",
    "number",
    "words",
    "want",
    "train",
    "dataset",
    "set",
    "40k",
    "small",
    "portion",
    "data",
    "use",
    "class",
    "read",
    "data",
    "data",
    "directory",
    "giving",
    "desired",
    "vocab",
    "size",
    "return",
    "formatted",
    "tokenised",
    "words",
    "languages",
    "initialise",
    "tensorflow",
    "placeholders",
    "encoder",
    "decoder",
    "inputs",
    "integer",
    "tensors",
    "represent",
    "discrete",
    "values",
    "embedded",
    "dense",
    "representation",
    "later",
    "feed",
    "vocabulary",
    "word",
    "encoder",
    "encoded",
    "representation",
    "learnt",
    "decoder",
    "build",
    "model",
    "google",
    "published",
    "paper",
    "recently",
    "discussing",
    "system",
    "integrated",
    "translation",
    "service",
    "called",
    "neural",
    "machine",
    "translation",
    "encoder",
    "decoder",
    "model",
    "inspired",
    "similar",
    "work",
    "papers",
    "topics",
    "like",
    "text",
    "summarisation",
    "whereas",
    "google",
    "translate",
    "would",
    "translate",
    "language",
    "english",
    "language",
    "b",
    "new",
    "nmt",
    "architecture",
    "translate",
    "directly",
    "one",
    "language",
    "memorise",
    "phrase",
    "phrase",
    "translations",
    "instead",
    "encodes",
    "semantics",
    "sentence",
    "encoding",
    "generalised",
    "even",
    "translate",
    "language",
    "pair",
    "like",
    "japanese",
    "korean",
    "explicitly",
    "seen",
    "guess",
    "use",
    "lstm",
    "recurrent",
    "network",
    "encode",
    "sentence",
    "language",
    "rnn",
    "spits",
    "hidden",
    "state",
    "represents",
    "vectorised",
    "contents",
    "sentence",
    "feed",
    "decoder",
    "generate",
    "translated",
    "sentence",
    "language",
    "b",
    "word",
    "word",
    "sounds",
    "easy",
    "enough",
    "right",
    "wrong",
    "drawback",
    "architecture",
    "limited",
    "memory",
    "hidden",
    "state",
    "lstm",
    "trying",
    "cram",
    "whole",
    "sentence",
    "want",
    "translate",
    "usually",
    "hundred",
    "floating",
    "point",
    "numbers",
    "long",
    "try",
    "force",
    "sentence",
    "fixed",
    "dimensionality",
    "vector",
    "lossy",
    "neural",
    "net",
    "forced",
    "could",
    "increase",
    "hidden",
    "size",
    "lstm",
    "supposed",
    "remember",
    "long",
    "term",
    "dependencies",
    "happens",
    "increase",
    "hidden",
    "size",
    "h",
    "lstm",
    "training",
    "time",
    "increases",
    "exponentially",
    "solve",
    "going",
    "bring",
    "mix",
    "translating",
    "long",
    "sentence",
    "probably",
    "glance",
    "back",
    "source",
    "sentence",
    "couple",
    "times",
    "make",
    "sure",
    "capturing",
    "details",
    "iteratively",
    "pay",
    "attention",
    "relevant",
    "parts",
    "source",
    "sentence",
    "let",
    "neural",
    "nets",
    "letting",
    "store",
    "refer",
    "previous",
    "outputs",
    "lstm",
    "increases",
    "storage",
    "model",
    "without",
    "changing",
    "functionality",
    "lstm",
    "idea",
    "lstm",
    "outputs",
    "encoder",
    "stored",
    "query",
    "output",
    "asking",
    "relevant",
    "computation",
    "happening",
    "encoder",
    "encoder",
    "output",
    "gets",
    "relevancy",
    "score",
    "convert",
    "probability",
    "score",
    "applying",
    "softmax",
    "activation",
    "extract",
    "context",
    "vector",
    "weighted",
    "summation",
    "encoder",
    "outputs",
    "depending",
    "relevant",
    "memory",
    "ai",
    "enough",
    "pay",
    "attention",
    "memory",
    "ai",
    "enough",
    "pay",
    "attention",
    "hindi",
    "memory",
    "ai",
    "enough",
    "pay",
    "attention",
    "german",
    "memory",
    "ai",
    "enough",
    "pay",
    "attention",
    "spanish",
    "memory",
    "ai",
    "enough",
    "pay",
    "attention",
    "build",
    "model",
    "using",
    "tensorflow",
    "built",
    "embedding",
    "attention",
    "sequence",
    "sequence",
    "function",
    "giving",
    "encoder",
    "decoder",
    "inputs",
    "well",
    "hyper",
    "parameters",
    "define",
    "like",
    "number",
    "layers",
    "builds",
    "model",
    "like",
    "one",
    "discussed",
    "tensorflow",
    "several",
    "built",
    "models",
    "like",
    "drop",
    "code",
    "easily",
    "normally",
    "alone",
    "would",
    "fine",
    "could",
    "run",
    "results",
    "would",
    "decent",
    "added",
    "another",
    "improvement",
    "model",
    "requires",
    "code",
    "100",
    "gpus",
    "week",
    "training",
    "seriously",
    "took",
    "wo",
    "implement",
    "programatically",
    "let",
    "dive",
    "conceptually",
    "outputs",
    "sufficient",
    "context",
    "wo",
    "able",
    "give",
    "good",
    "answer",
    "need",
    "include",
    "info",
    "future",
    "words",
    "encoder",
    "output",
    "determined",
    "words",
    "left",
    "right",
    "humans",
    "would",
    "definitely",
    "use",
    "kind",
    "full",
    "context",
    "determine",
    "meaning",
    "word",
    "see",
    "sentence",
    "way",
    "tho",
    "use",
    "encoder",
    "two",
    "rnns",
    "one",
    "goes",
    "forward",
    "sentence",
    "goes",
    "backwards",
    "word",
    "concatenates",
    "vector",
    "outputs",
    "produces",
    "vector",
    "context",
    "sides",
    "added",
    "lot",
    "layers",
    "model",
    "encoder",
    "one",
    "rnn",
    "layer",
    "seven",
    "rnn",
    "layers",
    "decoder",
    "eight",
    "rnn",
    "layers",
    "layers",
    "longer",
    "training",
    "times",
    "use",
    "single",
    "layer",
    "layers",
    "whole",
    "layer",
    "would",
    "finish",
    "layer",
    "dependencies",
    "could",
    "start",
    "computing",
    "using",
    "layers",
    "computation",
    "going",
    "parallel",
    "initialise",
    "tensorflow",
    "section",
    "model",
    "inside",
    "let",
    "see",
    "results",
    "training",
    "first",
    "give",
    "phrase",
    "looks",
    "good",
    "another",
    "phrase",
    "dope",
    "perfect",
    "still",
    "way",
    "go",
    "definitely",
    "getting",
    "closer",
    "universal",
    "translation",
    "model",
    "breaking",
    "architectures",
    "performance",
    "machine",
    "translation",
    "storing",
    "previous",
    "outputs",
    "lstm",
    "cells",
    "judge",
    "relevancy",
    "decide",
    "use",
    "via",
    "attention",
    "mechanism",
    "using",
    "rnn",
    "context",
    "past",
    "future",
    "words",
    "used",
    "create",
    "accurate",
    "encoder",
    "output",
    "vector",
    "coding",
    "challenge",
    "winner",
    "last",
    "week",
    "ryan",
    "lee",
    "impressive",
    "created",
    "recipe",
    "summariser",
    "scraping",
    "recipes",
    "web",
    "documented",
    "beautifully",
    "installation",
    "steps",
    "reproduce",
    "results",
    "wizard",
    "week",
    "runner",
    "sarah",
    "collins",
    "code",
    "converts",
    "scientific",
    "papers",
    "text",
    "prioritises",
    "topic",
    "weeks",
    "coding",
    "challenge",
    "create",
    "simple",
    "translation",
    "system",
    "using",
    "model",
    "details",
    "readme",
    "post",
    "github",
    "link",
    "comments",
    "announce",
    "winner",
    "next",
    "week",
    "please",
    "subscribe",
    "programming",
    "videos",
    "check",
    "related",
    "video",
    "got",
    "get",
    "better",
    "gpu",
    "thanks",
    "watching"
  ],
  "keywords": [
    "world",
    "let",
    "make",
    "language",
    "using",
    "tensorflow",
    "languages",
    "across",
    "every",
    "culture",
    "way",
    "means",
    "number",
    "translation",
    "second",
    "translating",
    "easy",
    "words",
    "rules",
    "system",
    "two",
    "systems",
    "idea",
    "new",
    "one",
    "war",
    "government",
    "work",
    "trying",
    "used",
    "help",
    "effort",
    "learnt",
    "inspired",
    "try",
    "post",
    "russian",
    "scientific",
    "first",
    "machine",
    "translated",
    "english",
    "would",
    "source",
    "target",
    "word",
    "results",
    "encoded",
    "approach",
    "statistical",
    "ibm",
    "text",
    "corpus",
    "google",
    "translate",
    "good",
    "another",
    "state",
    "dataset",
    "train",
    "model",
    "got",
    "version",
    "create",
    "training",
    "built",
    "class",
    "data",
    "set",
    "start",
    "vocab",
    "size",
    "want",
    "use",
    "giving",
    "initialise",
    "encoder",
    "decoder",
    "inputs",
    "representation",
    "feed",
    "build",
    "neural",
    "papers",
    "like",
    "b",
    "architecture",
    "phrase",
    "sentence",
    "lstm",
    "rnn",
    "hidden",
    "enough",
    "right",
    "memory",
    "whole",
    "long",
    "vector",
    "could",
    "increase",
    "dependencies",
    "increases",
    "going",
    "times",
    "details",
    "pay",
    "attention",
    "relevant",
    "previous",
    "outputs",
    "output",
    "computation",
    "relevancy",
    "score",
    "context",
    "ai",
    "sequence",
    "layers",
    "code",
    "added",
    "week",
    "wo",
    "give",
    "future",
    "definitely",
    "see",
    "goes",
    "layer",
    "coding",
    "challenge",
    "winner"
  ]
}