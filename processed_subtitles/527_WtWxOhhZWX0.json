{
  "text": "in this lesson you will be learning the\nconcept of ensemble learning\nby the end of this lesson you will be\nable to\nexplain ensemble learning evaluate the\nperformance of boosting models\noverview ensemble technique combines\nindividual models together to improve\nthe stability and predictive power of\nthe model\nthis approach permits higher predictive\nperformance compared to a single model\nensemble finds ways to combine multiple\nmachine learning models into one\npredictive model in order to decrease\nvariance using bagging decrease bias\nusing boosting or improve predictions\nusing stacking let's understand the\nideology behind ensembl learning as you\nhave seen before certain models do well\nin modeling one aspect of the data while\nothers do well in modeling another\ninstead of learning a single complex\nmodel learn several simple models and\ncombine their output to produce the\nfinal decision\nin ensemble learning the combined\nstrength of the models offsets\nindividual model variances and biases\nensemble learning will provide a\ncomposite prediction where the final\naccuracy is better than the accuracy of\nindividual models\nensemble methods can be divided into two\ngroups sequential ensemble methods and\nparallel ensemble methods in sequential\nensemble methods base learners are\ngenerated consecutively that is add a\nboost\nthe basic motivation of sequential\nmethods is to use the dependence between\nthe base learners\nby weighing previously mislabeled\nexamples with higher weight\nthe overall performance of a model can\nbe boosted\nparallel ensemble methods are applied\nwherever the base learners are generated\nin parallel for example random forest\nsince the errors are often reduced\ndramatically by averaging the basic\nmotivation of parallel methods is to use\nindependence between the base learners\nhere in the given diagram we are using\nparallel ensemble method where the\ndifferent models work best in one or\nother aspect of data\nthey are combined in the ensemble model\nwhere the best aspect of each of the\nmodels is considered upon the input data\nand then ensemble learning model comes\nup with the predictions\nensemble model is the application of\nmultiple models in order to obtain\nbetter performances from a single model\nperformance depends on two major factors\nrobustness and accuracy ensemble models\nincorporate the predictions from all the\nbase learners this provides robustness\nensemble models deliver accurate\npredictions and have improved\nperformances\nensemble learning methods part a\nyou can create an ensemble by combining\nall weak learners once weak models are\nproperly combined we can obtain more\naccurate and or robust models\nyou can also create an ensemble of\nwell-chosen strong and diverse models\nby combining data from numerous modeling\napproaches ensemble models gain more\naccuracy and robustness than a\nfine-tuned single model can gain\nmodel averaging is an approach to\nensemble learning where each ensemble\nmember contributes an equal amount to\nthe final prediction\nin the case of regression the ensemble\nprediction is calculated as the average\nof the member predictions\nin the case of predicting a class label\nthe prediction is calculated as the mode\nof the member predictions\nin the case of predicting class\nprobability the prediction can be\ncalculated as the arg max of the summed\nprobabilities for each class label\na limitation of this approach is that\neach model has an equal contribution to\nthe final prediction made by the\nensemble\nthere is a requirement that all ensemble\nmembers have skills compared to random\nchance although some models are known to\nperform much better or much worse than\nother models\na weighted average ensemble is an\nextension of a model averaging ensemble\nwhere the contribution of each member to\nthe final prediction\nis weighted by the performance of the\nmodel\nthe model weights are small positive\nvalues and the sum of all weights equals\none allowing the weights to indicate the\npercentage of trust or expected\nperformance from each model\nnow let's take a look at what bagging is\nthe idea behind bagging is to combine\nthe results of multiple models to get a\ngeneralized result from a single model\nbagging or bootstrap aggregation reduces\nvariance of an estimate by taking the\nmean of multiple estimates\nthere are three steps to perform bagging\nstep one is to create randomly sampled\ndata sets of the original training data\nbootstrapping\nstep two is to build and fit several\nclassifiers to each of these diverse\ncopies\nstep three is to take the average of the\npredictions to make a final overall\nprediction\nensemble learning methods part b\nlet's understand how random forest is an\nensemble learning model\nrandom forest is a good example of\nensemble machine learning method random\nforests as one can guess combine various\ndecision trees to produce a more\ngeneralized model by reducing the\nnotorious overfitting tendency of\ndecision trees\nrandom forests are utilized to produce\ndecorated decision trees\nit creates random subsets of the\nfeatures\nsmaller trees are built using these\nsubsets creating tree diversity\nto overcome overfitting diverse sets of\ndecision trees are required\nboosting is a sequential process where\nthe errors of the previous model is\ncorrected by each subsequent model\nlet's assume an example of different\ntypes of body pain and their diagnosis\nheadache stomach pain and leg pain will\nhave a different reason\nlet's assume that x causes headache then\nx will not cause leg pain because why\ncauses leg pain\nthese two variables are not the reason\nfor stomach pain stomach pain will be\ncaused by variable z depending on the\nresults of weighted opinions the doctor\nwill suggest the diagnosis sequentially\nlet's understand the way boosting works\nin the steps flow step one is to train a\nclassifier h1 that best classifies the\ndata with respect to accuracy step two\nis to identify regions where h1 produces\nerrors and weighs to them and produces a\nh2 classifier step three is to aggregate\nit those samples for which h1 gives a\ndifferent result from h2 and produces h3\nclassifier\nrepeat step 2 for a new classifier\nlet's understand what adaboost is and\nhow it works let's first understand\nboosting boosting is a technique of\nchanging weak learners into strong\nlearners in boosting each new tree as a\nfit on a modified version of the\noriginal data set\nadaboost is one of the first boosting\nalgorithms to be adapted in solving\npractices adaboost helps you mix\nmultiple weak classifiers into one\nstrong classifier\nlet's understand this through a scenario\nconsider a scenario where there are plus\nand minus symbols indicating the data\npoints we will try to understand\nadaboost workflow\nworking of add-a-boost\nso the first step is to assign equal\nweights to each data point and apply a\ndecision stump to classify them as a\nplus or minus\nthe most suited and most common\nalgorithm used with adaboost is a\ndecision tree with one level these trees\nare short and solely contain one\ndecision for classification they are\noften called decision stumps\nas you can see in the given graph\ndecision stump d1 has generated a\nvertical plane at the left side to\nclassify the plus\na decision stump is a decision tree it\nuses only a single attribute for\nsplitting for distinct attributes this\ntypically means that the tree consists\nonly of a single interior node\nnow apply higher weights to the\nincorrectly predicted three plus as\nshown in the given graph and add another\ndecision stump\nyou can see that the size of three\nincorrectly predicted plus is bigger\nthan the rest of the data points\nin the second step the second decision\nstump d2 will try to predict them\ncorrectly so as you can see d2 has\nclassified three misclassified plus\ncorrectly\nd2 has also caused misclassification\nerror to three minus in the next step d3\nadds higher weights to 3 minus a\nhorizontal line is generated to classify\nplus and minus based on higher weight of\nthe misclassified observation\nin the final step d1 d2 and d3 are\ncombined to form a strong prediction\nthat has complex rules as compared to\nthe individual weak learners\nat a boost algorithm and flowchart\nnow since you know the workflow of\nadaboost classifier let's understand the\nattaboost algorithm a weak classifier is\nprepared on the training data using the\nweighted samples\nonly binary classification problems are\nsupported\nso every decision stump makes one\ndecision on one input variable and\noutputs a plus\n1.0 or minus 1.0 value for the first or\nsecond class value\nthe misclassification rate is calculated\nas error equals\ncorrect minus n\ndivided by n\nin the given equation error is the\nmisclassification rate correct is the\nnumber of training instances predicted\nby the model and n is the total number\nof training instances so let's\nunderstand the steps involved in this\nalgorithm\nstep one initially data points is\nweighted equally with weight w i equals\n1 over n where n is the number of\nsamples step 2 a classifier h1 picked up\nthat classifies the data with minimal\nerror rate\nstep 3 the weighted factor alpha is\ndependent on errors e t\nstep four weight after time t is given\nas this formula\nwhere z is the normalizing factor\nh1x\nyx is the sign of the current output\nlet's understand the flow of adaboost\nalgorithm it follows the following steps\none initially adaboost selects a\ntraining subset randomly\ntwo it iteratively trains the adaboost\nmachine learning model by selecting the\ntraining set based on the accurate\nprediction of the last training\nthree it assigns the higher weight to\nwrongly classified observations so that\nin the next iteration these observations\nwill get a high probability for\nclassification\nfour\nalso it assigns weight to the trained\nclassifier in each iteration according\nto the accuracy of the classifier\nthe more accurate classifier will get\nhigher weight\nfive this process iterates until the\ncomplete training data fits without any\nerror or until it reaches the specified\nmaximum number of estimators\ngradient boosting\nnow let's understand gradient boosting\ngradient boosting trains several models\nin a very gradual additive and\nsequential manner the major difference\nbetween adaboost and a gradient boosting\nalgorithm is how the two algorithms\nidentify the shortcomings of weak\nlearners that is decision trees gbm\nminimizes the loss function mse of a\nmodel by adding weak learners using a\ngradient descent procedure\ngradient boosting involves three\nelements\none a loss function to be optimized two\na weak learner to make predictions\nthree an additive model to add weak\nlearners to minimize the loss function\nlet's understand the gbm mechanism\ngradient boosting trains many models\nsequentially each new model gradually\nminimizes the loss function of the whole\nsystem using the gradient descent method\nso in y equals ax plus b plus e e needs\nspecial attention because it is an error\nterm\nthe learning procedure consecutively\nfits new models to produce an additional\naccurate estimate of the response\nvariable the principal idea behind this\nalgorithm is to construct new base\nlearners that might be maximally\ncorrelated with negative gradient of the\nloss function associated with the whole\nensemble\nso let's understand the working gradient\nboosting\ngbm predicts the residuals or errors of\nthe prior models and then sums them to\nmake the final prediction\nstep 2 one week learner is added at a\ntime and existing weak learners in the\nmodel are left unchanged step 3 gbm\nrepetitively leverages the patterns in\nresiduals and strengthens a model with\nweak predictions\nstep four modeling is stopped when they\ndo not have any pattern that can be\nmodeled\nlet's look at the steps involved in gbm\nalgorithm\nstep one fit a simple regression or\nclassification model\nstep 2 calculate error residuals that is\nactual value minus predicted value\nstep 3 fit a new model on error\nresiduals as target variable with the\nsame input variables step 4 add the\npredicted residuals to the previous\npredictions\nstep 5 fit another model on residuals\nand repeat steps 2 and 5 until the model\nis over fit or the sum of the residuals\nbecome constant\nxgboost\nlet's understand extreme gradient\nboosting it is a library for developing\nquick and superior gradient boosting\ntree models xg boost is one of the\nimplementations of the gradient boosting\nconcept but what makes it unique is that\nit uses a more regularized model\nformulation to control overfitting which\ngives it better performance thus it\nhelps to reduce overfitting\nextreme gradient boosting is a custom\ntree based algorithm used for\nclassification regression and ranking\nwith custom loss function\nit has interfaces for python and r that\ncan be executed on yarn xg boost is\nextensively used in ml competitions as\nit is almost 10 times faster than other\ngradient boosting techniques\nlet's understand the xgboost library\nfeatures xgboost library tools are built\nfor the sole purpose of model\nperformance and computational speed\nlet's look at system features followed\nby algorithm features we will finally\ntake a look at the model features of\nxgboost\nsystem features are\nparallelization\nparallelization it produces tree\nconstruction using all cpu cores while\ntraining distributed computing it uses a\ncluster of machines in order to train\nvery large models cache optimization\ndata structure involved in xg boost\nmakes the best use of hardware\nalgorithm features are\nsparse aware\nalgorithm behind xg boost provides\nautomatic handling of missing data\nvalues\nblock structure it supports the\nparallelization of tree construction\ncontinued training\nits algorithm provides continued\ntraining in order to boost an already\nfitted model on new data\nmodel features are gradient boosting it\nuses gradient boosting machine algorithm\nincluding learning rate\nstochastic grading boosting\nit does sub sampling at the row column\nand column per split levels\nregularized gradient boosting it does\nregularization using both l1 and l2\nregularization\nxg boost parameters part a\nlet's understand the xg boost parameters\ngeneral parameters consist of a number\nof threats that help to prepare data\ntask parameters correspond to the\nobjective that helps to build a model\nevaluation metrics help to predict and\nvisualize the predicted values booster\nparameters consist of a step size that\nevaluates the prediction performance\nmetrics and roc curve and regularization\nthat compares the prediction performance\nagainst the logistic regression model\nlet's understand the general parameters\nof xgboost general parameters guide the\noverall functioning of xg boost so let's\nunderstand what these general parameters\nare\nn thread is used to define the number of\nparallel threads we are going to use if\nno value is entered the algorithm\nautomatically detects the number of\ncores and runs a thread on all the cores\nbooster parameters guide the individual\ntree or regression at each step there\nare two types of boosters which booster\ncan you use gb tree uses tree based\nmodel\ngp linear uses linear function\nusually a tree booster outperforms the\nlinear booster\nsilent default equals zero is used to\nprint the running messages if the silent\nparameter is set to one no running\nmessages will be printed hence keep it\nzero as the messages might help in\nunderstanding the model\nlet's look at the booster parameters\nbooster parameters give the individual\nbooster tree regression at each step\nlet's look at the parameters for tree\nbooster first eta defines the step size\nshrinkage used in update to prevent\noverfitting\nafter each boosting step you can\ndirectly get the weights of new features\nand eta shrinks the feature weights to\nmake the boosting process more\nconservative\neta range is between 0 and 1 and the\ndefault value is\n0.3 gamma defines the minimal loss\nreduction required to make a further\npartition on a leaf node of the tree\nthe algorithm will be more conservative\nwhen the gamma is larger\nits value ranges from zero to infinity\nand the default value is zero\nmax depth is used to define the maximum\ndepth of a tree\nits value ranges from one to infinity\nand the default value is six\nmin child weight is used to define the\nminimum sum of instance weight needed in\na child if the tree partition step\nresults in a leaf node with the sum of\ninstance weight less than min child\nweight then the building process will\ngive up further partitioning\nin linear regression tasks this simply\ncorresponds to the minimum number of\ninstances needed in each node\nthe algorithm will be more conservative\nif the min child weight is larger it\nranges from zero to infinity and the\ndefault value is one\nxg boost parameters part b\nmax delta step is used to set the\nmaximum delta step allowed in each\ntree's weight estimation if the value is\nset to zero it means there is no\nconstraint\nif it is set to a positive value it can\nhelp make the update step more\nconservative\nusually this parameter is not needed but\nit might help in logistic regression\nwhen the class is extremely imbalanced\nits value ranges from zero to infinity\nand the default value is zero\nsubsample is used to calculate the\nsubsample ratio of the training instance\nits value ranges between zero and one\nand the default value is one call sample\nby tree is the subsample of ratio of\ncolumns when constructing each tree\nsub sampling occurs once for every tree\nconstructed its value ranges between\nzero and one and the default value is\none\nnow let's understand the parameters for\nlinear booster\nlambda defines the ridge regression\nregularization term on weights\nincreasing this value will make the\nmodel more conservative the default\nvalue is zero\nalpha defines the lasso regression\nregularization term on weights\nincreasing this value will make the\nmodel more conservative\nthe default value is zero\nlambda bias defines the ridge regression\nregularization term on bias\nincreasing this value will make the\nmodel more conservative the default\nvalue is zero\nnow let's look at the task parameters\ntask parameters guide the optimization\nobjective to be calculated at each step\nthe objective options are binary\nlogistic defines logistic regression for\nbinary classification whose output is\nprobability not class\nmulti-soft max defines the multi-class\nclassification using softmax objective\nevaluation metrics for validation data\na default metric will be assigned\naccording to the objective rmse for\nregression error for classification and\nmean average precision for ranking the\nevaluation metric options are rmse\nlog loss error auc\nmirror\nmlo gloss\ndemo pima indians diabetes\nproblem statement the objective of the\ndata set is to diagnostically predict\nwhether or not a patient has diabetes\nbased on certain diagnostic measurements\nincluding in the data set\nall patients are females of at least 21\nyears of age and of pima indian heritage\nthe data set consists of several medical\npredictor variables and one target\nvariable outcome\npredictor variables include the number\nof pregnancies the patients have had\ntheir bmi\ninsulin level age and so on\nlet us import the required libraries\nincluding pandas and model selection\nfrom sklearn also we will import\nadaboost classifier from\nsklearn.ensemble to further optimize our\nmodel\nimport pandas import sklearn import\nmodel underscore selection\nfrom\nsklearn.ensemble import add a boost\nclassifier\nnow load the data set with a pandas data\nframe\nextract the values from the columns in\nthe form of an array\nset the random seed value as seven and\nnumber of trees as thirty\nwe will build classifiers using adaboost\nand xg boost let's create the adaboost\nmodel using\nscikit learn\nattaboost uses decision tree classifier\nas the default classifier\npass the model within the cross\nvalidation score function to evaluate\nthe results using the cross validation\ntechnique\nconstruct the model now by splitting the\ntrain test indices into 10 consecutive\nfolds\nagain evaluate the model such that each\nfold gets used once as a validation\nwhile the remaining nine folds form the\ntraining set\nprint the results\nadaboost gives an accuracy of 76\nsimilarly we apply the xgboost algorithm\nimport the respective modules namely svm\nand xgb classifier from sklearn import\nsvm\nfrom xgboost import xgb classifier\ninitialize the xgb classifier under the\nname clf\nset the seed value a 7 and number of\ntrees is 30.\nconstruct the xgb classifier using the\nk-folds technique\nsuch that number of folds equals 10.\nevaluate the model using the cross\nvalidation score similar to the way we\ndid for add a boost classifier\nagain the accuracy is around 77 percent\nwe have shown only two classifiers you\ncan use different classifiers and\ncompare the results\nalso check results after applying\nten-fold cross-validation\ngreat we have seen how to apply boosting\nalgorithms to classification let's\nquickly recap the steps we covered one\nbuild models using adaboost and xg boost\nto compare the accuracy of classifiers\nmodel selection\nwhen do you need to evaluate the model\nperformance\nmodels can be evaluated based on\nmeasures of performance model evaluation\naims to estimate the generalization\naccuracy of a model on future data that\nis unseen out of sample data\nso model evaluation techniques are used\nto evaluate the model's performance when\nthere is a chance of evaluating new data\nthat is unseen or out of sample\nlet's now look at how to access model\nperformance train test split or cross\nvalidation split are the techniques used\nto evaluate the model performance train\ntest split divides the training data set\ninto two sets and uses the first\ntraining set to train the model and\ntests the model using the second set\ncross validation split splits the data\nset in two sets namely train and test\nand then calculates the accuracy of each\nset and finds the average of the results\nlet's look at how train test split works\nso we have a given data sets that we\nneed to split into training data and\ntesting data\nsplit the data randomly using negative\nand positive symbol\nnow when you have training data randomly\nselected from the given data set the\nremaining data falls under the test data\nmove this data into the hypothesis space\nmake sure that your test set meets the\ntwo subsequent conditions given\nthe test set must be large enough to\nyield statistically meaningful results\nthe test set must have different\ncharacteristics than the training set\nnow verify the result by matching the\ntrain data with the test data\nnow test the training data individually\nwith respect to the test data present in\nthe hypothesis space\nwherever the data that has the same\nsymbol matches it provides accurate\nresult prediction given here is 9 13\nwhich shows the matching prediction\ncommon splitting strategies\nso let's understand what percentage of\ndata should be used for training and for\ntesting so we can use either of the two\ncommon splitting strategies to split the\ndata\nk-fold cross-validation or leave one out\nlet's first understand how k-fold\ncross-validation works the procedure\ncontains a single parameter referred to\nas k and points to the number of groups\nthat a given data sample is to be split\ninto as such the procedure is usually\nreferred to as k-fold cross validation\nwhen a particular value for k is chosen\nit's going to be employed in place of k\nwithin the relevancy of the model\nso how does the leave one out technique\nwork\nthis approach leaves one data point out\nof the training data that is if there\nare n data points in the original sample\nthen n1 samples are used to train the\nmodel and one point is used as the\nvalidation set\nall the combinations are created in this\nway the original sample can be separated\nand the error is averaged for all trials\nin order to provide overall\neffectiveness\nnow let's understand the difference\nbetween train test split and cross\nvalidation advantages of cross\nvalidation one it is a better estimator\nof out of sample accuracy two it\nprovides a more efficient use of data\nsince each data is used for training and\ntesting\nadvantages of test train split\none runs k times faster than k fold\ntwo simpler than k fold so it would be\neasier to analyze the testing errors\ndemo cross validation\nproblem statement a few learners have\nimplemented random forest classifier on\nthe iris data but better accuracy can be\nachieved by using the cross validation\nsampling technique\nin this analysis we will generate the\nrandom forest using the cross validation\nsplitting technique\nlet us import the inbuilt iris data set\nfrom the\nsklearn.datasets library\nfrom\nsklearn.dataset import load underscore\niris\nload the dataset with a data frame\nextract the input data\nsimilarly extract the target values\nk-fold cross-validation\nuse the k-fold cross-validation\ntechnique to split the data set into k\nconsecutive folds without shuffling by\ndefault here we will split the data set\ninto five consecutive groups from\nsklearn.model underscore selection\nimport k-fold\nusing a for loop we will now iterate the\nindividual train test sets within the\ninput data\nlet's now split the data set into 10\nconsecutive folds\nusing a for loop we will now iterate the\nindividual train test sets within the\noutput data\nlet's initialize the random forest\nclassifier from\nsklearn.ensemble import random forest\nclassifier\nimport the cross validation score\nlibrary from sklearn and get the model\nevaluated for accuracy by using the\ncross validation score parameter\nfrom sklearn.model underscore selection\nimport cross underscore val underscore\nscore\nthe accuracy of the classifier is ninety\nfive\ngreat we have seen how to apply cross\nvalidation to split the data set let's\nquickly recap the steps we covered one\nshuffle and split the data set randomly\ninto k groups\ntwo summarize the skill of the model\nusing model evaluation scores\nthis brings us to the end of ensemble\nlearning you are now able to explain\nensemble learning evaluate the\nperformance of boosting models\nhi there if you like this video\nsubscribe to the simply learn youtube\nchannel and click here to watch similar\nvideos turn it up and get certified\nclick here\n",
  "words": [
    "lesson",
    "learning",
    "concept",
    "ensemble",
    "learning",
    "end",
    "lesson",
    "able",
    "explain",
    "ensemble",
    "learning",
    "evaluate",
    "performance",
    "boosting",
    "models",
    "overview",
    "ensemble",
    "technique",
    "combines",
    "individual",
    "models",
    "together",
    "improve",
    "stability",
    "predictive",
    "power",
    "model",
    "approach",
    "permits",
    "higher",
    "predictive",
    "performance",
    "compared",
    "single",
    "model",
    "ensemble",
    "finds",
    "ways",
    "combine",
    "multiple",
    "machine",
    "learning",
    "models",
    "one",
    "predictive",
    "model",
    "order",
    "decrease",
    "variance",
    "using",
    "bagging",
    "decrease",
    "bias",
    "using",
    "boosting",
    "improve",
    "predictions",
    "using",
    "stacking",
    "let",
    "understand",
    "ideology",
    "behind",
    "ensembl",
    "learning",
    "seen",
    "certain",
    "models",
    "well",
    "modeling",
    "one",
    "aspect",
    "data",
    "others",
    "well",
    "modeling",
    "another",
    "instead",
    "learning",
    "single",
    "complex",
    "model",
    "learn",
    "several",
    "simple",
    "models",
    "combine",
    "output",
    "produce",
    "final",
    "decision",
    "ensemble",
    "learning",
    "combined",
    "strength",
    "models",
    "offsets",
    "individual",
    "model",
    "variances",
    "biases",
    "ensemble",
    "learning",
    "provide",
    "composite",
    "prediction",
    "final",
    "accuracy",
    "better",
    "accuracy",
    "individual",
    "models",
    "ensemble",
    "methods",
    "divided",
    "two",
    "groups",
    "sequential",
    "ensemble",
    "methods",
    "parallel",
    "ensemble",
    "methods",
    "sequential",
    "ensemble",
    "methods",
    "base",
    "learners",
    "generated",
    "consecutively",
    "add",
    "boost",
    "basic",
    "motivation",
    "sequential",
    "methods",
    "use",
    "dependence",
    "base",
    "learners",
    "weighing",
    "previously",
    "mislabeled",
    "examples",
    "higher",
    "weight",
    "overall",
    "performance",
    "model",
    "boosted",
    "parallel",
    "ensemble",
    "methods",
    "applied",
    "wherever",
    "base",
    "learners",
    "generated",
    "parallel",
    "example",
    "random",
    "forest",
    "since",
    "errors",
    "often",
    "reduced",
    "dramatically",
    "averaging",
    "basic",
    "motivation",
    "parallel",
    "methods",
    "use",
    "independence",
    "base",
    "learners",
    "given",
    "diagram",
    "using",
    "parallel",
    "ensemble",
    "method",
    "different",
    "models",
    "work",
    "best",
    "one",
    "aspect",
    "data",
    "combined",
    "ensemble",
    "model",
    "best",
    "aspect",
    "models",
    "considered",
    "upon",
    "input",
    "data",
    "ensemble",
    "learning",
    "model",
    "comes",
    "predictions",
    "ensemble",
    "model",
    "application",
    "multiple",
    "models",
    "order",
    "obtain",
    "better",
    "performances",
    "single",
    "model",
    "performance",
    "depends",
    "two",
    "major",
    "factors",
    "robustness",
    "accuracy",
    "ensemble",
    "models",
    "incorporate",
    "predictions",
    "base",
    "learners",
    "provides",
    "robustness",
    "ensemble",
    "models",
    "deliver",
    "accurate",
    "predictions",
    "improved",
    "performances",
    "ensemble",
    "learning",
    "methods",
    "part",
    "create",
    "ensemble",
    "combining",
    "weak",
    "learners",
    "weak",
    "models",
    "properly",
    "combined",
    "obtain",
    "accurate",
    "robust",
    "models",
    "also",
    "create",
    "ensemble",
    "strong",
    "diverse",
    "models",
    "combining",
    "data",
    "numerous",
    "modeling",
    "approaches",
    "ensemble",
    "models",
    "gain",
    "accuracy",
    "robustness",
    "single",
    "model",
    "gain",
    "model",
    "averaging",
    "approach",
    "ensemble",
    "learning",
    "ensemble",
    "member",
    "contributes",
    "equal",
    "amount",
    "final",
    "prediction",
    "case",
    "regression",
    "ensemble",
    "prediction",
    "calculated",
    "average",
    "member",
    "predictions",
    "case",
    "predicting",
    "class",
    "label",
    "prediction",
    "calculated",
    "mode",
    "member",
    "predictions",
    "case",
    "predicting",
    "class",
    "probability",
    "prediction",
    "calculated",
    "arg",
    "max",
    "summed",
    "probabilities",
    "class",
    "label",
    "limitation",
    "approach",
    "model",
    "equal",
    "contribution",
    "final",
    "prediction",
    "made",
    "ensemble",
    "requirement",
    "ensemble",
    "members",
    "skills",
    "compared",
    "random",
    "chance",
    "although",
    "models",
    "known",
    "perform",
    "much",
    "better",
    "much",
    "worse",
    "models",
    "weighted",
    "average",
    "ensemble",
    "extension",
    "model",
    "averaging",
    "ensemble",
    "contribution",
    "member",
    "final",
    "prediction",
    "weighted",
    "performance",
    "model",
    "model",
    "weights",
    "small",
    "positive",
    "values",
    "sum",
    "weights",
    "equals",
    "one",
    "allowing",
    "weights",
    "indicate",
    "percentage",
    "trust",
    "expected",
    "performance",
    "model",
    "let",
    "take",
    "look",
    "bagging",
    "idea",
    "behind",
    "bagging",
    "combine",
    "results",
    "multiple",
    "models",
    "get",
    "generalized",
    "result",
    "single",
    "model",
    "bagging",
    "bootstrap",
    "aggregation",
    "reduces",
    "variance",
    "estimate",
    "taking",
    "mean",
    "multiple",
    "estimates",
    "three",
    "steps",
    "perform",
    "bagging",
    "step",
    "one",
    "create",
    "randomly",
    "sampled",
    "data",
    "sets",
    "original",
    "training",
    "data",
    "bootstrapping",
    "step",
    "two",
    "build",
    "fit",
    "several",
    "classifiers",
    "diverse",
    "copies",
    "step",
    "three",
    "take",
    "average",
    "predictions",
    "make",
    "final",
    "overall",
    "prediction",
    "ensemble",
    "learning",
    "methods",
    "part",
    "b",
    "let",
    "understand",
    "random",
    "forest",
    "ensemble",
    "learning",
    "model",
    "random",
    "forest",
    "good",
    "example",
    "ensemble",
    "machine",
    "learning",
    "method",
    "random",
    "forests",
    "one",
    "guess",
    "combine",
    "various",
    "decision",
    "trees",
    "produce",
    "generalized",
    "model",
    "reducing",
    "notorious",
    "overfitting",
    "tendency",
    "decision",
    "trees",
    "random",
    "forests",
    "utilized",
    "produce",
    "decorated",
    "decision",
    "trees",
    "creates",
    "random",
    "subsets",
    "features",
    "smaller",
    "trees",
    "built",
    "using",
    "subsets",
    "creating",
    "tree",
    "diversity",
    "overcome",
    "overfitting",
    "diverse",
    "sets",
    "decision",
    "trees",
    "required",
    "boosting",
    "sequential",
    "process",
    "errors",
    "previous",
    "model",
    "corrected",
    "subsequent",
    "model",
    "let",
    "assume",
    "example",
    "different",
    "types",
    "body",
    "pain",
    "diagnosis",
    "headache",
    "stomach",
    "pain",
    "leg",
    "pain",
    "different",
    "reason",
    "let",
    "assume",
    "x",
    "causes",
    "headache",
    "x",
    "cause",
    "leg",
    "pain",
    "causes",
    "leg",
    "pain",
    "two",
    "variables",
    "reason",
    "stomach",
    "pain",
    "stomach",
    "pain",
    "caused",
    "variable",
    "z",
    "depending",
    "results",
    "weighted",
    "opinions",
    "doctor",
    "suggest",
    "diagnosis",
    "sequentially",
    "let",
    "understand",
    "way",
    "boosting",
    "works",
    "steps",
    "flow",
    "step",
    "one",
    "train",
    "classifier",
    "h1",
    "best",
    "classifies",
    "data",
    "respect",
    "accuracy",
    "step",
    "two",
    "identify",
    "regions",
    "h1",
    "produces",
    "errors",
    "weighs",
    "produces",
    "h2",
    "classifier",
    "step",
    "three",
    "aggregate",
    "samples",
    "h1",
    "gives",
    "different",
    "result",
    "h2",
    "produces",
    "h3",
    "classifier",
    "repeat",
    "step",
    "2",
    "new",
    "classifier",
    "let",
    "understand",
    "adaboost",
    "works",
    "let",
    "first",
    "understand",
    "boosting",
    "boosting",
    "technique",
    "changing",
    "weak",
    "learners",
    "strong",
    "learners",
    "boosting",
    "new",
    "tree",
    "fit",
    "modified",
    "version",
    "original",
    "data",
    "set",
    "adaboost",
    "one",
    "first",
    "boosting",
    "algorithms",
    "adapted",
    "solving",
    "practices",
    "adaboost",
    "helps",
    "mix",
    "multiple",
    "weak",
    "classifiers",
    "one",
    "strong",
    "classifier",
    "let",
    "understand",
    "scenario",
    "consider",
    "scenario",
    "plus",
    "minus",
    "symbols",
    "indicating",
    "data",
    "points",
    "try",
    "understand",
    "adaboost",
    "workflow",
    "working",
    "first",
    "step",
    "assign",
    "equal",
    "weights",
    "data",
    "point",
    "apply",
    "decision",
    "stump",
    "classify",
    "plus",
    "minus",
    "suited",
    "common",
    "algorithm",
    "used",
    "adaboost",
    "decision",
    "tree",
    "one",
    "level",
    "trees",
    "short",
    "solely",
    "contain",
    "one",
    "decision",
    "classification",
    "often",
    "called",
    "decision",
    "stumps",
    "see",
    "given",
    "graph",
    "decision",
    "stump",
    "d1",
    "generated",
    "vertical",
    "plane",
    "left",
    "side",
    "classify",
    "plus",
    "decision",
    "stump",
    "decision",
    "tree",
    "uses",
    "single",
    "attribute",
    "splitting",
    "distinct",
    "attributes",
    "typically",
    "means",
    "tree",
    "consists",
    "single",
    "interior",
    "node",
    "apply",
    "higher",
    "weights",
    "incorrectly",
    "predicted",
    "three",
    "plus",
    "shown",
    "given",
    "graph",
    "add",
    "another",
    "decision",
    "stump",
    "see",
    "size",
    "three",
    "incorrectly",
    "predicted",
    "plus",
    "bigger",
    "rest",
    "data",
    "points",
    "second",
    "step",
    "second",
    "decision",
    "stump",
    "d2",
    "try",
    "predict",
    "correctly",
    "see",
    "d2",
    "classified",
    "three",
    "misclassified",
    "plus",
    "correctly",
    "d2",
    "also",
    "caused",
    "misclassification",
    "error",
    "three",
    "minus",
    "next",
    "step",
    "d3",
    "adds",
    "higher",
    "weights",
    "3",
    "minus",
    "horizontal",
    "line",
    "generated",
    "classify",
    "plus",
    "minus",
    "based",
    "higher",
    "weight",
    "misclassified",
    "observation",
    "final",
    "step",
    "d1",
    "d2",
    "d3",
    "combined",
    "form",
    "strong",
    "prediction",
    "complex",
    "rules",
    "compared",
    "individual",
    "weak",
    "learners",
    "boost",
    "algorithm",
    "flowchart",
    "since",
    "know",
    "workflow",
    "adaboost",
    "classifier",
    "let",
    "understand",
    "attaboost",
    "algorithm",
    "weak",
    "classifier",
    "prepared",
    "training",
    "data",
    "using",
    "weighted",
    "samples",
    "binary",
    "classification",
    "problems",
    "supported",
    "every",
    "decision",
    "stump",
    "makes",
    "one",
    "decision",
    "one",
    "input",
    "variable",
    "outputs",
    "plus",
    "minus",
    "value",
    "first",
    "second",
    "class",
    "value",
    "misclassification",
    "rate",
    "calculated",
    "error",
    "equals",
    "correct",
    "minus",
    "n",
    "divided",
    "n",
    "given",
    "equation",
    "error",
    "misclassification",
    "rate",
    "correct",
    "number",
    "training",
    "instances",
    "predicted",
    "model",
    "n",
    "total",
    "number",
    "training",
    "instances",
    "let",
    "understand",
    "steps",
    "involved",
    "algorithm",
    "step",
    "one",
    "initially",
    "data",
    "points",
    "weighted",
    "equally",
    "weight",
    "w",
    "equals",
    "1",
    "n",
    "n",
    "number",
    "samples",
    "step",
    "2",
    "classifier",
    "h1",
    "picked",
    "classifies",
    "data",
    "minimal",
    "error",
    "rate",
    "step",
    "3",
    "weighted",
    "factor",
    "alpha",
    "dependent",
    "errors",
    "e",
    "step",
    "four",
    "weight",
    "time",
    "given",
    "formula",
    "z",
    "normalizing",
    "factor",
    "h1x",
    "yx",
    "sign",
    "current",
    "output",
    "let",
    "understand",
    "flow",
    "adaboost",
    "algorithm",
    "follows",
    "following",
    "steps",
    "one",
    "initially",
    "adaboost",
    "selects",
    "training",
    "subset",
    "randomly",
    "two",
    "iteratively",
    "trains",
    "adaboost",
    "machine",
    "learning",
    "model",
    "selecting",
    "training",
    "set",
    "based",
    "accurate",
    "prediction",
    "last",
    "training",
    "three",
    "assigns",
    "higher",
    "weight",
    "wrongly",
    "classified",
    "observations",
    "next",
    "iteration",
    "observations",
    "get",
    "high",
    "probability",
    "classification",
    "four",
    "also",
    "assigns",
    "weight",
    "trained",
    "classifier",
    "iteration",
    "according",
    "accuracy",
    "classifier",
    "accurate",
    "classifier",
    "get",
    "higher",
    "weight",
    "five",
    "process",
    "iterates",
    "complete",
    "training",
    "data",
    "fits",
    "without",
    "error",
    "reaches",
    "specified",
    "maximum",
    "number",
    "estimators",
    "gradient",
    "boosting",
    "let",
    "understand",
    "gradient",
    "boosting",
    "gradient",
    "boosting",
    "trains",
    "several",
    "models",
    "gradual",
    "additive",
    "sequential",
    "manner",
    "major",
    "difference",
    "adaboost",
    "gradient",
    "boosting",
    "algorithm",
    "two",
    "algorithms",
    "identify",
    "shortcomings",
    "weak",
    "learners",
    "decision",
    "trees",
    "gbm",
    "minimizes",
    "loss",
    "function",
    "mse",
    "model",
    "adding",
    "weak",
    "learners",
    "using",
    "gradient",
    "descent",
    "procedure",
    "gradient",
    "boosting",
    "involves",
    "three",
    "elements",
    "one",
    "loss",
    "function",
    "optimized",
    "two",
    "weak",
    "learner",
    "make",
    "predictions",
    "three",
    "additive",
    "model",
    "add",
    "weak",
    "learners",
    "minimize",
    "loss",
    "function",
    "let",
    "understand",
    "gbm",
    "mechanism",
    "gradient",
    "boosting",
    "trains",
    "many",
    "models",
    "sequentially",
    "new",
    "model",
    "gradually",
    "minimizes",
    "loss",
    "function",
    "whole",
    "system",
    "using",
    "gradient",
    "descent",
    "method",
    "equals",
    "ax",
    "plus",
    "b",
    "plus",
    "e",
    "e",
    "needs",
    "special",
    "attention",
    "error",
    "term",
    "learning",
    "procedure",
    "consecutively",
    "fits",
    "new",
    "models",
    "produce",
    "additional",
    "accurate",
    "estimate",
    "response",
    "variable",
    "principal",
    "idea",
    "behind",
    "algorithm",
    "construct",
    "new",
    "base",
    "learners",
    "might",
    "maximally",
    "correlated",
    "negative",
    "gradient",
    "loss",
    "function",
    "associated",
    "whole",
    "ensemble",
    "let",
    "understand",
    "working",
    "gradient",
    "boosting",
    "gbm",
    "predicts",
    "residuals",
    "errors",
    "prior",
    "models",
    "sums",
    "make",
    "final",
    "prediction",
    "step",
    "2",
    "one",
    "week",
    "learner",
    "added",
    "time",
    "existing",
    "weak",
    "learners",
    "model",
    "left",
    "unchanged",
    "step",
    "3",
    "gbm",
    "repetitively",
    "leverages",
    "patterns",
    "residuals",
    "strengthens",
    "model",
    "weak",
    "predictions",
    "step",
    "four",
    "modeling",
    "stopped",
    "pattern",
    "modeled",
    "let",
    "look",
    "steps",
    "involved",
    "gbm",
    "algorithm",
    "step",
    "one",
    "fit",
    "simple",
    "regression",
    "classification",
    "model",
    "step",
    "2",
    "calculate",
    "error",
    "residuals",
    "actual",
    "value",
    "minus",
    "predicted",
    "value",
    "step",
    "3",
    "fit",
    "new",
    "model",
    "error",
    "residuals",
    "target",
    "variable",
    "input",
    "variables",
    "step",
    "4",
    "add",
    "predicted",
    "residuals",
    "previous",
    "predictions",
    "step",
    "5",
    "fit",
    "another",
    "model",
    "residuals",
    "repeat",
    "steps",
    "2",
    "5",
    "model",
    "fit",
    "sum",
    "residuals",
    "become",
    "constant",
    "xgboost",
    "let",
    "understand",
    "extreme",
    "gradient",
    "boosting",
    "library",
    "developing",
    "quick",
    "superior",
    "gradient",
    "boosting",
    "tree",
    "models",
    "xg",
    "boost",
    "one",
    "implementations",
    "gradient",
    "boosting",
    "concept",
    "makes",
    "unique",
    "uses",
    "regularized",
    "model",
    "formulation",
    "control",
    "overfitting",
    "gives",
    "better",
    "performance",
    "thus",
    "helps",
    "reduce",
    "overfitting",
    "extreme",
    "gradient",
    "boosting",
    "custom",
    "tree",
    "based",
    "algorithm",
    "used",
    "classification",
    "regression",
    "ranking",
    "custom",
    "loss",
    "function",
    "interfaces",
    "python",
    "r",
    "executed",
    "yarn",
    "xg",
    "boost",
    "extensively",
    "used",
    "ml",
    "competitions",
    "almost",
    "10",
    "times",
    "faster",
    "gradient",
    "boosting",
    "techniques",
    "let",
    "understand",
    "xgboost",
    "library",
    "features",
    "xgboost",
    "library",
    "tools",
    "built",
    "sole",
    "purpose",
    "model",
    "performance",
    "computational",
    "speed",
    "let",
    "look",
    "system",
    "features",
    "followed",
    "algorithm",
    "features",
    "finally",
    "take",
    "look",
    "model",
    "features",
    "xgboost",
    "system",
    "features",
    "parallelization",
    "parallelization",
    "produces",
    "tree",
    "construction",
    "using",
    "cpu",
    "cores",
    "training",
    "distributed",
    "computing",
    "uses",
    "cluster",
    "machines",
    "order",
    "train",
    "large",
    "models",
    "cache",
    "optimization",
    "data",
    "structure",
    "involved",
    "xg",
    "boost",
    "makes",
    "best",
    "use",
    "hardware",
    "algorithm",
    "features",
    "sparse",
    "aware",
    "algorithm",
    "behind",
    "xg",
    "boost",
    "provides",
    "automatic",
    "handling",
    "missing",
    "data",
    "values",
    "block",
    "structure",
    "supports",
    "parallelization",
    "tree",
    "construction",
    "continued",
    "training",
    "algorithm",
    "provides",
    "continued",
    "training",
    "order",
    "boost",
    "already",
    "fitted",
    "model",
    "new",
    "data",
    "model",
    "features",
    "gradient",
    "boosting",
    "uses",
    "gradient",
    "boosting",
    "machine",
    "algorithm",
    "including",
    "learning",
    "rate",
    "stochastic",
    "grading",
    "boosting",
    "sub",
    "sampling",
    "row",
    "column",
    "column",
    "per",
    "split",
    "levels",
    "regularized",
    "gradient",
    "boosting",
    "regularization",
    "using",
    "l1",
    "l2",
    "regularization",
    "xg",
    "boost",
    "parameters",
    "part",
    "let",
    "understand",
    "xg",
    "boost",
    "parameters",
    "general",
    "parameters",
    "consist",
    "number",
    "threats",
    "help",
    "prepare",
    "data",
    "task",
    "parameters",
    "correspond",
    "objective",
    "helps",
    "build",
    "model",
    "evaluation",
    "metrics",
    "help",
    "predict",
    "visualize",
    "predicted",
    "values",
    "booster",
    "parameters",
    "consist",
    "step",
    "size",
    "evaluates",
    "prediction",
    "performance",
    "metrics",
    "roc",
    "curve",
    "regularization",
    "compares",
    "prediction",
    "performance",
    "logistic",
    "regression",
    "model",
    "let",
    "understand",
    "general",
    "parameters",
    "xgboost",
    "general",
    "parameters",
    "guide",
    "overall",
    "functioning",
    "xg",
    "boost",
    "let",
    "understand",
    "general",
    "parameters",
    "n",
    "thread",
    "used",
    "define",
    "number",
    "parallel",
    "threads",
    "going",
    "use",
    "value",
    "entered",
    "algorithm",
    "automatically",
    "detects",
    "number",
    "cores",
    "runs",
    "thread",
    "cores",
    "booster",
    "parameters",
    "guide",
    "individual",
    "tree",
    "regression",
    "step",
    "two",
    "types",
    "boosters",
    "booster",
    "use",
    "gb",
    "tree",
    "uses",
    "tree",
    "based",
    "model",
    "gp",
    "linear",
    "uses",
    "linear",
    "function",
    "usually",
    "tree",
    "booster",
    "outperforms",
    "linear",
    "booster",
    "silent",
    "default",
    "equals",
    "zero",
    "used",
    "print",
    "running",
    "messages",
    "silent",
    "parameter",
    "set",
    "one",
    "running",
    "messages",
    "printed",
    "hence",
    "keep",
    "zero",
    "messages",
    "might",
    "help",
    "understanding",
    "model",
    "let",
    "look",
    "booster",
    "parameters",
    "booster",
    "parameters",
    "give",
    "individual",
    "booster",
    "tree",
    "regression",
    "step",
    "let",
    "look",
    "parameters",
    "tree",
    "booster",
    "first",
    "eta",
    "defines",
    "step",
    "size",
    "shrinkage",
    "used",
    "update",
    "prevent",
    "overfitting",
    "boosting",
    "step",
    "directly",
    "get",
    "weights",
    "new",
    "features",
    "eta",
    "shrinks",
    "feature",
    "weights",
    "make",
    "boosting",
    "process",
    "conservative",
    "eta",
    "range",
    "0",
    "1",
    "default",
    "value",
    "gamma",
    "defines",
    "minimal",
    "loss",
    "reduction",
    "required",
    "make",
    "partition",
    "leaf",
    "node",
    "tree",
    "algorithm",
    "conservative",
    "gamma",
    "larger",
    "value",
    "ranges",
    "zero",
    "infinity",
    "default",
    "value",
    "zero",
    "max",
    "depth",
    "used",
    "define",
    "maximum",
    "depth",
    "tree",
    "value",
    "ranges",
    "one",
    "infinity",
    "default",
    "value",
    "six",
    "min",
    "child",
    "weight",
    "used",
    "define",
    "minimum",
    "sum",
    "instance",
    "weight",
    "needed",
    "child",
    "tree",
    "partition",
    "step",
    "results",
    "leaf",
    "node",
    "sum",
    "instance",
    "weight",
    "less",
    "min",
    "child",
    "weight",
    "building",
    "process",
    "give",
    "partitioning",
    "linear",
    "regression",
    "tasks",
    "simply",
    "corresponds",
    "minimum",
    "number",
    "instances",
    "needed",
    "node",
    "algorithm",
    "conservative",
    "min",
    "child",
    "weight",
    "larger",
    "ranges",
    "zero",
    "infinity",
    "default",
    "value",
    "one",
    "xg",
    "boost",
    "parameters",
    "part",
    "b",
    "max",
    "delta",
    "step",
    "used",
    "set",
    "maximum",
    "delta",
    "step",
    "allowed",
    "tree",
    "weight",
    "estimation",
    "value",
    "set",
    "zero",
    "means",
    "constraint",
    "set",
    "positive",
    "value",
    "help",
    "make",
    "update",
    "step",
    "conservative",
    "usually",
    "parameter",
    "needed",
    "might",
    "help",
    "logistic",
    "regression",
    "class",
    "extremely",
    "imbalanced",
    "value",
    "ranges",
    "zero",
    "infinity",
    "default",
    "value",
    "zero",
    "subsample",
    "used",
    "calculate",
    "subsample",
    "ratio",
    "training",
    "instance",
    "value",
    "ranges",
    "zero",
    "one",
    "default",
    "value",
    "one",
    "call",
    "sample",
    "tree",
    "subsample",
    "ratio",
    "columns",
    "constructing",
    "tree",
    "sub",
    "sampling",
    "occurs",
    "every",
    "tree",
    "constructed",
    "value",
    "ranges",
    "zero",
    "one",
    "default",
    "value",
    "one",
    "let",
    "understand",
    "parameters",
    "linear",
    "booster",
    "lambda",
    "defines",
    "ridge",
    "regression",
    "regularization",
    "term",
    "weights",
    "increasing",
    "value",
    "make",
    "model",
    "conservative",
    "default",
    "value",
    "zero",
    "alpha",
    "defines",
    "lasso",
    "regression",
    "regularization",
    "term",
    "weights",
    "increasing",
    "value",
    "make",
    "model",
    "conservative",
    "default",
    "value",
    "zero",
    "lambda",
    "bias",
    "defines",
    "ridge",
    "regression",
    "regularization",
    "term",
    "bias",
    "increasing",
    "value",
    "make",
    "model",
    "conservative",
    "default",
    "value",
    "zero",
    "let",
    "look",
    "task",
    "parameters",
    "task",
    "parameters",
    "guide",
    "optimization",
    "objective",
    "calculated",
    "step",
    "objective",
    "options",
    "binary",
    "logistic",
    "defines",
    "logistic",
    "regression",
    "binary",
    "classification",
    "whose",
    "output",
    "probability",
    "class",
    "max",
    "defines",
    "classification",
    "using",
    "softmax",
    "objective",
    "evaluation",
    "metrics",
    "validation",
    "data",
    "default",
    "metric",
    "assigned",
    "according",
    "objective",
    "rmse",
    "regression",
    "error",
    "classification",
    "mean",
    "average",
    "precision",
    "ranking",
    "evaluation",
    "metric",
    "options",
    "rmse",
    "log",
    "loss",
    "error",
    "auc",
    "mirror",
    "mlo",
    "gloss",
    "demo",
    "pima",
    "indians",
    "diabetes",
    "problem",
    "statement",
    "objective",
    "data",
    "set",
    "diagnostically",
    "predict",
    "whether",
    "patient",
    "diabetes",
    "based",
    "certain",
    "diagnostic",
    "measurements",
    "including",
    "data",
    "set",
    "patients",
    "females",
    "least",
    "21",
    "years",
    "age",
    "pima",
    "indian",
    "heritage",
    "data",
    "set",
    "consists",
    "several",
    "medical",
    "predictor",
    "variables",
    "one",
    "target",
    "variable",
    "outcome",
    "predictor",
    "variables",
    "include",
    "number",
    "pregnancies",
    "patients",
    "bmi",
    "insulin",
    "level",
    "age",
    "let",
    "us",
    "import",
    "required",
    "libraries",
    "including",
    "pandas",
    "model",
    "selection",
    "sklearn",
    "also",
    "import",
    "adaboost",
    "classifier",
    "optimize",
    "model",
    "import",
    "pandas",
    "import",
    "sklearn",
    "import",
    "model",
    "underscore",
    "selection",
    "import",
    "add",
    "boost",
    "classifier",
    "load",
    "data",
    "set",
    "pandas",
    "data",
    "frame",
    "extract",
    "values",
    "columns",
    "form",
    "array",
    "set",
    "random",
    "seed",
    "value",
    "seven",
    "number",
    "trees",
    "thirty",
    "build",
    "classifiers",
    "using",
    "adaboost",
    "xg",
    "boost",
    "let",
    "create",
    "adaboost",
    "model",
    "using",
    "scikit",
    "learn",
    "attaboost",
    "uses",
    "decision",
    "tree",
    "classifier",
    "default",
    "classifier",
    "pass",
    "model",
    "within",
    "cross",
    "validation",
    "score",
    "function",
    "evaluate",
    "results",
    "using",
    "cross",
    "validation",
    "technique",
    "construct",
    "model",
    "splitting",
    "train",
    "test",
    "indices",
    "10",
    "consecutive",
    "folds",
    "evaluate",
    "model",
    "fold",
    "gets",
    "used",
    "validation",
    "remaining",
    "nine",
    "folds",
    "form",
    "training",
    "set",
    "print",
    "results",
    "adaboost",
    "gives",
    "accuracy",
    "76",
    "similarly",
    "apply",
    "xgboost",
    "algorithm",
    "import",
    "respective",
    "modules",
    "namely",
    "svm",
    "xgb",
    "classifier",
    "sklearn",
    "import",
    "svm",
    "xgboost",
    "import",
    "xgb",
    "classifier",
    "initialize",
    "xgb",
    "classifier",
    "name",
    "clf",
    "set",
    "seed",
    "value",
    "7",
    "number",
    "trees",
    "construct",
    "xgb",
    "classifier",
    "using",
    "technique",
    "number",
    "folds",
    "equals",
    "evaluate",
    "model",
    "using",
    "cross",
    "validation",
    "score",
    "similar",
    "way",
    "add",
    "boost",
    "classifier",
    "accuracy",
    "around",
    "77",
    "percent",
    "shown",
    "two",
    "classifiers",
    "use",
    "different",
    "classifiers",
    "compare",
    "results",
    "also",
    "check",
    "results",
    "applying",
    "great",
    "seen",
    "apply",
    "boosting",
    "algorithms",
    "classification",
    "let",
    "quickly",
    "recap",
    "steps",
    "covered",
    "one",
    "build",
    "models",
    "using",
    "adaboost",
    "xg",
    "boost",
    "compare",
    "accuracy",
    "classifiers",
    "model",
    "selection",
    "need",
    "evaluate",
    "model",
    "performance",
    "models",
    "evaluated",
    "based",
    "measures",
    "performance",
    "model",
    "evaluation",
    "aims",
    "estimate",
    "generalization",
    "accuracy",
    "model",
    "future",
    "data",
    "unseen",
    "sample",
    "data",
    "model",
    "evaluation",
    "techniques",
    "used",
    "evaluate",
    "model",
    "performance",
    "chance",
    "evaluating",
    "new",
    "data",
    "unseen",
    "sample",
    "let",
    "look",
    "access",
    "model",
    "performance",
    "train",
    "test",
    "split",
    "cross",
    "validation",
    "split",
    "techniques",
    "used",
    "evaluate",
    "model",
    "performance",
    "train",
    "test",
    "split",
    "divides",
    "training",
    "data",
    "set",
    "two",
    "sets",
    "uses",
    "first",
    "training",
    "set",
    "train",
    "model",
    "tests",
    "model",
    "using",
    "second",
    "set",
    "cross",
    "validation",
    "split",
    "splits",
    "data",
    "set",
    "two",
    "sets",
    "namely",
    "train",
    "test",
    "calculates",
    "accuracy",
    "set",
    "finds",
    "average",
    "results",
    "let",
    "look",
    "train",
    "test",
    "split",
    "works",
    "given",
    "data",
    "sets",
    "need",
    "split",
    "training",
    "data",
    "testing",
    "data",
    "split",
    "data",
    "randomly",
    "using",
    "negative",
    "positive",
    "symbol",
    "training",
    "data",
    "randomly",
    "selected",
    "given",
    "data",
    "set",
    "remaining",
    "data",
    "falls",
    "test",
    "data",
    "move",
    "data",
    "hypothesis",
    "space",
    "make",
    "sure",
    "test",
    "set",
    "meets",
    "two",
    "subsequent",
    "conditions",
    "given",
    "test",
    "set",
    "must",
    "large",
    "enough",
    "yield",
    "statistically",
    "meaningful",
    "results",
    "test",
    "set",
    "must",
    "different",
    "characteristics",
    "training",
    "set",
    "verify",
    "result",
    "matching",
    "train",
    "data",
    "test",
    "data",
    "test",
    "training",
    "data",
    "individually",
    "respect",
    "test",
    "data",
    "present",
    "hypothesis",
    "space",
    "wherever",
    "data",
    "symbol",
    "matches",
    "provides",
    "accurate",
    "result",
    "prediction",
    "given",
    "9",
    "13",
    "shows",
    "matching",
    "prediction",
    "common",
    "splitting",
    "strategies",
    "let",
    "understand",
    "percentage",
    "data",
    "used",
    "training",
    "testing",
    "use",
    "either",
    "two",
    "common",
    "splitting",
    "strategies",
    "split",
    "data",
    "leave",
    "one",
    "let",
    "first",
    "understand",
    "works",
    "procedure",
    "contains",
    "single",
    "parameter",
    "referred",
    "k",
    "points",
    "number",
    "groups",
    "given",
    "data",
    "sample",
    "split",
    "procedure",
    "usually",
    "referred",
    "cross",
    "validation",
    "particular",
    "value",
    "k",
    "chosen",
    "going",
    "employed",
    "place",
    "k",
    "within",
    "relevancy",
    "model",
    "leave",
    "one",
    "technique",
    "work",
    "approach",
    "leaves",
    "one",
    "data",
    "point",
    "training",
    "data",
    "n",
    "data",
    "points",
    "original",
    "sample",
    "n1",
    "samples",
    "used",
    "train",
    "model",
    "one",
    "point",
    "used",
    "validation",
    "set",
    "combinations",
    "created",
    "way",
    "original",
    "sample",
    "separated",
    "error",
    "averaged",
    "trials",
    "order",
    "provide",
    "overall",
    "effectiveness",
    "let",
    "understand",
    "difference",
    "train",
    "test",
    "split",
    "cross",
    "validation",
    "advantages",
    "cross",
    "validation",
    "one",
    "better",
    "estimator",
    "sample",
    "accuracy",
    "two",
    "provides",
    "efficient",
    "use",
    "data",
    "since",
    "data",
    "used",
    "training",
    "testing",
    "advantages",
    "test",
    "train",
    "split",
    "one",
    "runs",
    "k",
    "times",
    "faster",
    "k",
    "fold",
    "two",
    "simpler",
    "k",
    "fold",
    "would",
    "easier",
    "analyze",
    "testing",
    "errors",
    "demo",
    "cross",
    "validation",
    "problem",
    "statement",
    "learners",
    "implemented",
    "random",
    "forest",
    "classifier",
    "iris",
    "data",
    "better",
    "accuracy",
    "achieved",
    "using",
    "cross",
    "validation",
    "sampling",
    "technique",
    "analysis",
    "generate",
    "random",
    "forest",
    "using",
    "cross",
    "validation",
    "splitting",
    "technique",
    "let",
    "us",
    "import",
    "inbuilt",
    "iris",
    "data",
    "set",
    "library",
    "import",
    "load",
    "underscore",
    "iris",
    "load",
    "dataset",
    "data",
    "frame",
    "extract",
    "input",
    "data",
    "similarly",
    "extract",
    "target",
    "values",
    "use",
    "technique",
    "split",
    "data",
    "set",
    "k",
    "consecutive",
    "folds",
    "without",
    "shuffling",
    "default",
    "split",
    "data",
    "set",
    "five",
    "consecutive",
    "groups",
    "underscore",
    "selection",
    "import",
    "using",
    "loop",
    "iterate",
    "individual",
    "train",
    "test",
    "sets",
    "within",
    "input",
    "data",
    "let",
    "split",
    "data",
    "set",
    "10",
    "consecutive",
    "folds",
    "using",
    "loop",
    "iterate",
    "individual",
    "train",
    "test",
    "sets",
    "within",
    "output",
    "data",
    "let",
    "initialize",
    "random",
    "forest",
    "classifier",
    "import",
    "random",
    "forest",
    "classifier",
    "import",
    "cross",
    "validation",
    "score",
    "library",
    "sklearn",
    "get",
    "model",
    "evaluated",
    "accuracy",
    "using",
    "cross",
    "validation",
    "score",
    "parameter",
    "underscore",
    "selection",
    "import",
    "cross",
    "underscore",
    "val",
    "underscore",
    "score",
    "accuracy",
    "classifier",
    "ninety",
    "five",
    "great",
    "seen",
    "apply",
    "cross",
    "validation",
    "split",
    "data",
    "set",
    "let",
    "quickly",
    "recap",
    "steps",
    "covered",
    "one",
    "shuffle",
    "split",
    "data",
    "set",
    "randomly",
    "k",
    "groups",
    "two",
    "summarize",
    "skill",
    "model",
    "using",
    "model",
    "evaluation",
    "scores",
    "brings",
    "us",
    "end",
    "ensemble",
    "learning",
    "able",
    "explain",
    "ensemble",
    "learning",
    "evaluate",
    "performance",
    "boosting",
    "models",
    "hi",
    "like",
    "video",
    "subscribe",
    "simply",
    "learn",
    "youtube",
    "channel",
    "click",
    "watch",
    "similar",
    "videos",
    "turn",
    "get",
    "certified",
    "click"
  ],
  "keywords": [
    "learning",
    "ensemble",
    "evaluate",
    "performance",
    "boosting",
    "models",
    "technique",
    "individual",
    "model",
    "approach",
    "higher",
    "single",
    "combine",
    "multiple",
    "machine",
    "one",
    "order",
    "using",
    "bagging",
    "predictions",
    "let",
    "understand",
    "behind",
    "modeling",
    "data",
    "several",
    "output",
    "produce",
    "final",
    "decision",
    "combined",
    "prediction",
    "accuracy",
    "better",
    "methods",
    "two",
    "groups",
    "sequential",
    "parallel",
    "base",
    "learners",
    "generated",
    "add",
    "boost",
    "use",
    "weight",
    "overall",
    "random",
    "forest",
    "errors",
    "given",
    "different",
    "best",
    "input",
    "provides",
    "accurate",
    "part",
    "create",
    "weak",
    "also",
    "strong",
    "member",
    "regression",
    "calculated",
    "average",
    "class",
    "max",
    "weighted",
    "weights",
    "values",
    "sum",
    "equals",
    "look",
    "results",
    "get",
    "result",
    "three",
    "steps",
    "step",
    "randomly",
    "sets",
    "original",
    "training",
    "build",
    "fit",
    "classifiers",
    "make",
    "trees",
    "overfitting",
    "features",
    "tree",
    "process",
    "pain",
    "variables",
    "variable",
    "works",
    "train",
    "classifier",
    "h1",
    "produces",
    "samples",
    "2",
    "new",
    "adaboost",
    "first",
    "set",
    "plus",
    "minus",
    "points",
    "apply",
    "stump",
    "algorithm",
    "used",
    "classification",
    "uses",
    "splitting",
    "node",
    "predicted",
    "second",
    "d2",
    "error",
    "3",
    "based",
    "value",
    "rate",
    "n",
    "number",
    "gradient",
    "gbm",
    "loss",
    "function",
    "procedure",
    "term",
    "residuals",
    "xgboost",
    "library",
    "xg",
    "split",
    "regularization",
    "parameters",
    "general",
    "help",
    "objective",
    "evaluation",
    "booster",
    "logistic",
    "linear",
    "default",
    "zero",
    "parameter",
    "defines",
    "conservative",
    "ranges",
    "infinity",
    "child",
    "sample",
    "validation",
    "import",
    "selection",
    "sklearn",
    "underscore",
    "within",
    "cross",
    "score",
    "test",
    "consecutive",
    "folds",
    "xgb",
    "testing",
    "k"
  ]
}