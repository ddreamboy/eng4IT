{
  "text": "big data analytics helps organizations\nharness their data and use it to\nidentify new opportunities that in turn\nleads to smarter business moves more\nefficient operations higher profits and\nhappier customers understanding this we\nhave come for this tutorial on big data\nanalytics now before we go ahead with\nthe session I'd like to inform you guys\nthat we have launched a completely free\nplatform called as great Learning\nAcademy where you have access to free\ncourses such as AI cloud and digital\nmarketing you can check out the details\nin the description\nbelow now let's have a quick glance at\nthe agenda we start off by understanding\nthe concepts of big data and Hadoop then\nwe will learn about hyve and then\nfinally we will learn about spark so\ntoday we will be covering um Big Data\nwhat exactly is Big Data and then we\nwill look at something called Hadoop\necosystem and Hadoop architecture\nbasically so these things are the\nfoundation for all your uh machine\nlearning statistics spark anything that\nyou are learning the foundation is this\nso today we will be understanding a big\ndata and Hadoop ecosystem we'll ensure\nthat we go through all the basics and\nall so my name is Ragu Raman you can\ncall me Ragu uh I have been in the IT\nindustry for about 12 years as of now\nand I have been working on Big Data\nplatforms for for about 7 years 7 to 8\nyears uh from 2011 onwards I started on\nBig Data I trained people on Hadoop\nspark you know the Big Data uh typical\ntools that we use uh I train a lot of\npeople from corporate backgrounds I'm a\nregular trainer at Flipkart GE EMC and\nand many other companies the definition\nof Big Data if you Google you will get\nfirst thing you can just go to Google\nand say what is Big Data you will get\ntons of Articles PDFs and presentations\nthat is not what I'm going to teach you\nright what I'm going to teach you is\nthat let's take a practical use case\nright let's take a practical use case\nand try to understand how big data\nactually makes sense right so first of\nall big data is related to it so you\nneed some level of it knowledge to be\nfair right this has something to do with\nit uh I will share my personal\nexperience okay so when I started\nworking in 20072 2008 and all okay I was\nworking uh with a company in Bangalore\nand we had created an application a\nsales application so basically what we\ndo if you install this application there\nwas a retail company uh the customers\nwill input all their sales data in that\nyou know so they will say that today\nthis much quantity item sold know sold\nand sold and sold and this application\nwill capture all the data okay and store\nit now that was a very simple world we\ndid not had mobile phones we did not had\nFacebook WhatsApp nothing was there 2005\nright and we were using an rdbms system\nto store this you know what is an\nrdbms okay at least you good at that\nright so this is your culprit we are not\ngoing to use this kind Big Data okay so\nrdbms is what we were using to\ntraditionally store the data right and\nwhat is an rdbms system it has a row\ncolumn format you have a table you\ninsert the data now if somebody is\nteaching you rdbms they will say that\nwell rdbms is great can store everything\nyou know that's all fine but unless your\nstart data starts increasing right so\noriginally rdbms can store everything I\ndon't have any I'm I'm not against rdbms\nor something even for transactional\nmanagement we use rdbms there is no\ndoubt about it MySQL Oracle we were\nusing MySQL actually so whatever the\ncustomer is inserting this MySQL will\ncapture it end of the year you do some\nprocessing you will understand how many\nitems were sold everybody is happy happy\nthis is what was happening in 20 05 fast\nforward to\n2011 I working with IC Bank IC Bank want\nto migrate to Big Data platform so 2011\nwe run a project in which we migrated uh\nthe entire ICC Bank into Big Data\nplatform okay so I was part of that\nproject I was not doing everything in\nthat project so what is the problem with\nicci bank right so that is a question\nwhy ICA Bank want to go to Big Data\nright now this example will also help\nyou to understand the the IT world right\nso for people who are not coming from\nthe IT background they might need to\nunderstand how things are working right\nso we we went to IC Bank we spoke with\nthem we said hey what what do you want\nthey said you know what we are facing a\nlot of problems we want to go to Big\nData we want to go to Hadoop we'll see\nwhat is Hadoop later so we asked what is\na big deal what is your problem so there\nwere a lot of problems which they were\nfacing one of the problem is that if you\nlook at this rdbms system right\nyour typical rdbms system what is rdbms\nsystems doing they will store the data\nin the form of a table right now if your\ntable size is very very small it's not a\nbig deal even if you look at my SQL or\nif you go to Enterprise scale Oracle and\nall you know they can store gigabytes\nand sometimes up to terabytes of data\nbut as the size becomes big so let's say\nI ask you okay you want you have to\nstore the data in Oracle the data is\nkeep on increasing every week you are\nnot ready to migrate anywhere Oracle if\nyou talk only about Oracle they have\nstorage separate and dbms engine\nseparate I don't know how many of you\nhave worked extensively with Oracle in\nOracle what I can do I can purchase\nOracle and I can add storage boxes as\nmany as I\nwant you know so these boxes will be in\nthe network so my database engine will\nbe connecting to them and storing the\ndata whatever data I'm giving so you can\nask argue saying that I can increase the\nstorage capacity and then Oracle can\nstore as much data as you want there is\nno doubt about it right but there is\nalso limitation for the storage one\nproblem second problem is that as this\nbox increases the storage in Oracle\nincreases you have to divide your data\nyou have to partition your data so let's\nsay for example you know uh you are\nworking with Oracle okay so let's say\nyou working with Oracle and let's say we\nare looking at U iPhone\nsales iPhone right so you are selling\niPhone let's say you're working with\napple right so one of the example is\nthat if you are creating tables to store\niPhone data typically if you look at the\nschema and all what we will do is that\nthere will be one table okay there will\nbe one table wherein you will be having\nproperties like uh you know how many\nitems were purchased at what rate they\nwere purchased how how many people\nbought so on and so forth so this will\nbe your items table so in the items\ntable you will have all the data related\nto the iPhones you were selling right\nyou will also have another table where\nthere will be the user\ndata you will also have the user data\nuser data means which customer bought\nfrom where he bought all this data this\nis called\nnormalization you know what is\nnormalization in rdbm systems in order\nto avoid duplication of data what we do\nis that we normalize the data we always\nkeep the data in different different\ntables and whenever you want to get the\nfull data what you\ndo joints right so if I want to get both\nthe data from this I have to do\nsomething called a join query I I fire a\njoin query and all the data from here\nwill come and join and produce me the\noutput this is good if you're having\nsmall amount of data what if you have\nterabytes of data here and here my join\nquery will fail\nright or the join query has to wait now\nanother point that people say is that if\nyour rdbms is becoming bigger and bigger\nwhy don't you partition your data that\nis possible I can have like four boxes\nmy one table okay if this is my table\nright it can be logically partitioned\nand kept on four machines if it is\nbecoming bigger and bigger okay this is\npossible there is something called\nlogical partitioning look at the iPhone\ndata I can say all the data where people\nfrom India are buying the iPhone go to\nthis machine that's called logical\npartitioning based on the country column\nso here I have all the India data where\npeople are buying iPhone here I have the\nUS data where people are buying the\niPhone but the problem is here I'll have\nthe Burma data also India data will be\nlet's say 2 terabyte China data will be\n3 terabyte this will be KBS actually so\nthe problem is that if a table is\nbecoming so big okay I can cluster it I\ncan take four machines in my BMS or data\nwarehousing architecture and I can say\npartition the data because if a table is\nso big if I fire a query the query will\ntake a lot of time so what I do I say\nthat okay don't store the Full Table\ndivide the T table or partition the\ntable to partition I will say that you\nhave to use some Logic for partitioning\nI cannot physically divide the data\nthat's not possible in rdbms I cannot\nsay that take a table and cut it into\nfour doesn't work so I have to logically\npartition so I have to say that there is\na country column how many countries are\nthere in the world maximum 155 right I'm\nnot much of a geography person but I\nthink somewhere around 155 right or at\nleast people who buy iPhone 155\ncountries are there so I will say take\nthe country column so everybody who is\nbuying an iPhone will have a country\ncolumn So based on the country column it\nwill partition your data right now I\nhave four boxes this box has India data\nus data Burma data and something data\nagain same story if you're running the\njoin query this box has to process all\nthe India data for join query this box\nwill complete very fast this guy has to\nwait so again you're so the problem is\nthat traditionally people were using\nrdbm systems and data warehouses to\nstore the data now what is a drawback\none of the drawback is that as the size\nof the data increases your processing\nwill become very very slow right you\ncan't do it now uh the problem with\nclasses in Big Data is that there are\nlot of Technologies so when you start\ntalking about them why don't you\ndenormalize the\ndata why don't you denormalize the data\nfor example all the transaction data\nuser data everything I'll keep in one\ntable is that\npossible H one duplicates second what is\nthe maximum number of columns you can\nhave in a table rdbms table th000 Oracle\nsays th000 what if my data has more than\nthousand columns if I keep all the data\nI can't do so that is the reason they\ndon't provide the denormalized solutions\nif you look at rdbms systems they will\nalways say normalize your data so that\nit will be represented in different\ndifferent tables and whenever you need\nthe answer you do a join and you get the\nresult second drawback this is only\nstructured data meaning anything which\nyou can represent in row column I can\ndump here like an Excel sheet data right\ncan I dump images and audio files in\nthis\ntable yes or no can we store\nunstructured data in\nrbms\nyeah yes you can there is clob blob and\nall objects right but can it process\nunstructured data not possible so in the\nmodern day if you look at the modern day\nwhat is our problem so if you look at a\ntypical uh uh Bank like so let's first\ncomplete this part so one of the\ndrawbacks of rdbms is that as the size\nof the data increases it cannot cope up\nwith processing one of the problem right\nsecond problem is the\nschema meaning anything which you can\nrepresent in a row column format I can\ndump in an rdbms anything apart from\nthat I have to figure out a solution so\nwhat is the actual solution if I have to\nstore images and retrieve them in real\ntime how does Flipkart stores the data I\nwas working with Flipkart for some time\nflip cart has around 10 million products\neach product has 10 images that is 1\nbillion images so when you click on a\niPhone x picture on flip cart flip cart\nwill say that come after 10 minutes or\nimmediately\ndisplay have you ever thought how it is\nhappening good question right how it is\nhappening right so do you think they're\ndumping in an rdbms and picking it from\nthere 1 billion\nimages no no SQL\ndatabases there is a whole Arena of\ndatabases called No SQL fortunately or\nunfortunately it is not in my course so\nI don't have to teach right but if you\nare going to so your uh end goal is to\nbecome data scientist and I think that\nis why you're learning this course right\nyou're learning spark and other things\nright so I would suggest you to Google\nand learn a little bit about something\ncalled No SQL databases if you have\nheard about m ODB Cassandra hspace Neo\n4J vmot these are all popular vendors of\nnosql database so nosql databases will\nhelp you to store and retrieve\nunstructured data at any speed you want\nso what Amazon amazon.com actually does\nAmazon has something called Dynamo DB\nDynamo DB is an nosql database so in\nDynamo DB I don't have a row column I\nhave key value pairs so what they do the\nkey will be the product name\nsay you clicking on iPhone x iPhone x is\nthe key the value will be all the data\nyou have so the moment you click boom\nyour page is there so now SQL databases\nare really faster also it helps you to\nstore unstructured\ndata how are you booking a cab using\nOla right I go out I didn't get a cab\nYesterday by the way in Chennai but if I\ntry to get a cab using Ola right I open\nup Ola how many customers Ola has\nmillions in India and all of them are\nbooking at the same time right do you\nthink all your requests are taken by an\nrdbms and search in the table and give\nyou a cab does it work can an rdbms\nsystem handle 30 million 40 million\nconcurrent sessions on this amount of\ndata calculating route mongod DB no SQL\ndatabase Ola runs on mongodb I was\nworking with Ola for one of their design\nproblems actually so I didn't design\nanything but they told me that they are\nusing mongodb so the qu the problem is\nthe real the the new world that we see\ntoday does not fit our old\ndescription when I went for my first job\ninterview in\n2006 my technical guy who interviewed me\nasked what is your strongest skill I\nsaid Java he said you're selected\nbecause Java is like if you know Java\nyou will get a job without an interview\nnow if I say Java he will not allow me\nto sit in front actually because nobody\nI mean Java is still good but the new\nworld demands new skills actually that\nis my whole idea so the second the\ndrawback of your rdbms is scalability is\nthere okay third is\nprice price is a problem right I mean\nobviously if you calculate the cost if\nyou look at Oracle and all they are very\nvery costly\nSolutions IC Bank was paying around 30\ncrores only in support fees to Oracle 30\ncrores not even an year I think 3 months\nthey were paying 30 cres 40 cres only\nfor support for Oracle because it is an\nEnterprise grade product right you have\nto pay money there is no other way all\nright now if you switch to nosql\nplatforms they are all open source you\ndon't have to pay this much amount of\nmoney but that's not our discussion I'm\njust saying so all this problems means\ntraditionally storing the data on an\nrdbms and retrieving is not good for the\nmodern day in the modern day what is\nhappening is that every company want to\ncollect and analyze the data be banking\nindustry e-commerce industry or any\nindustry IND that you want they want to\ncollect and analyze the data so to give\nyou an example if you visit Amazon.com\nor flipcart.com what is happening they\nstart tracking from which IP address you\nare uh visiting the website how long you\nare clicking on a page which items\nyou're adding to cart which images\nyou're clicking which offers you are\nclicking all this data is analyzed to\nunderstand how a customer will\nbehave that is how they do marketing\nright so the more you start visiting the\ne-commerce companies you start seeing\nrecommendations and all the more the\nmore you visit Amazon they will start\nrecommending products to you asking you\nto buy so they have to collect all this\ndata so when I was working with Flipkart\nflipcart said that they collect data\nfrom the apps that you're using from the\nwebsite that you're having even from\nsocial media Flipkart is very active in\nsocial media like Facebook Twitter all\nthese places they are active so if\nanybody tweets about flip cart or visit\ntheir Facebook page all this data is\nbeing\ncollected now this so This is actually\ncalled a big data so the technical\ndefinition of big data is this huge\namount of data which you cannot process\nusing your traditional methods that is\ncalled Big Data right so why that is of\ninterest to us we are going to figure\nout how to store this data huge amount\nof data right and how to process this\ndata so using the traditional techniques\nyou will not be able to store it or\nprocess it that is not possible at all\nso we will be seeing how you can use\nHadoop as a solution to to store all\nthis data and process all this data\nright so interrupt me anytime if if you\nwant to ask anything I'll keep on\ntalking yeah so when I say no SQL\ndatabases are there it does not mean you\nrbms is gone so many people ask this\nquestion okay rdbms is the only solution\nfor transaction management so even if\nyou're buying something from flip cart\nwho will uh complete your transaction\nOracle probably or any transactional\nsystem for transactional management and\nwhy is that because rdbms systems are\nthe only ones which can ensure the asset\nproperties right if a transaction goes\nor not goes you bought something from\nFlipkart tomorrow Flipkart cannot come\nback to you and say that I don't really\nthink I don't know whether the\ntransaction happened or not probably you\ngot the money it's not possible right\nthen you will start complaining against\nflip cart right so flip cart comes to\nyou and say that you know what we are\nusing Cassandra as per Cassandra your\ntransaction is we don't know we'll come\nback to in another one week is that\npossible no so transaction management\nvery good question uh is still handled\nby\nrdbms technically it is called polyl\npersistence so I'm not a person who use\na lot of technical words but polyl\npersistence that means uh uh coexistence\nof uh traditional databases and you know\nthe other ones even if you look at\nAmazon or any company you know they will\nhave their transactional data and a\nbunch of no sqls to handle other type of\nuh things okay polyl persistence well\nyou don't have to really I mean I'm not\na person who really use a lot of uh uh\nwhat do you\nsay polyglot\npersistence persistence is e or a I\ndon't know even my English skills are at\nstake when I'm training people\npersistence is e I think right what do\nyou think take a vote for that\nprobably no no no no yeah so that's what\nwe have not come to Hadoop so far uh I\nwill explain Hadoop is not a database\nso teaching big data in 16 hours itself\nis a challenge actually because it's a v\ntopic actually in the world of Big Data\nthere is a separate category of systems\ncalled nosql databases and they are\ndatabases so what is a database does\nrealtime uh you know queries right and\nin that area we have mongodb Cassandra\nhspace and these guys now these guys are\nused in real time if you want to fetch\nthe data say for example you are opening\nan e-commerce website probably it is\ngiven by a nosql database right or\nmillions of people are booking flights\nthrough to clear trip.com and clear trip\nhas to identify you know how many people\nare booking in a particular route so\nthat it can give a better offer this is\nprobably done by reading realtime data\nfrom a nosql system our area is not\nnosql nosql is a DB actually it's a\ndatabase our area is not uh not SQL we\nwill come to Hado right now so this is\nthat technical term for coexistence of\nDBS and no SQL and all so now let's\nunderstand um so we establish the idea\nthat rdbms is not good for us at least\nin the world of big data but don't say\nthat rdbms is not good because\ntransaction management is still handled\nby rdbms so you cannot say that get rid\nof rdbms right now there is a very\nimportant term even though it is not\ndirectly related to your syllabus but I\nthink it makes a lot of sense how many\nof you are aware of this term\nraise your hand I will raise oh lot of\nETL guys good to know that so you know\nwhat is an ETL right and some people do\nnot know what is an ETL that's fine so\nETL stands for uh extract transform and\nload so what is the idea here is that\nsee if if I'm working uh with a company\nright the company will have rdbm systems\nright so these are all rdbms systems so\nprobably this guy is using MySQL\nand he's also using\nOracle right a lot of rdbm systems are\nbeing used by a company right now these\nare all serving the customers so if you\ntake the example of ICI Bank all their\ncore banking\ndata is an\noracle all their core banking data is an\noracle so if you're making an online\ntransaction the data hits Oracle all\ntheir CRM data is in my SQL what do you\nmean by CRM customer relationship right\nyour name address blah blah blah so they\nhave these two type of data now is this\nthe only type of data that they have no\nIC bank had Al also acquired a lot of\nother companies when we were working\nwith them so their subsidiary companies\nwere giving them data in the form of XML\nfiles XML right now IC Bank also had a\nFacebook page okay from where they were\ngetting around 10,000 people visiting\nevery day the Facebook page and they\nused to click on some item so they have\nto do this cross selling upselling if a\nuser goes to Facebook page and say that\nI really like your loan take more loan\nright so that is your cross selling and\nupselling right so what they do they\ncollect all this data from social media\nthat comes in the format of what\nJson JavaScript object notation Json key\nvalue pairs basically so this is where\ntheir core banking data this is where\ntheir CRM data this is from their\npartner companies this is from the\nsocial media data they are getting okay\nand they had customer care this one the\nsocial media data they want to do\nanalytics so every day 10,000 people are\nvisiting their web page they want to\nknow a person clicked on let's say a\nlawn offer and how much time he stayed\non the page and what age group he\nbelongs to so that they can recommend\nproducts recommendations actually a lot\nof recommend\nso social media data was being used they\ndid not reveal the all the things that\nthey are doing because that were\ninternal but basically using the social\nmedia data they were doing\nrecommendations so they will download\nall this data so if a customer is\nclicking on the Facebook page they will\nget the access of this guy if you are\nclicking on IC bank's Facebook page they\nwill know your age your occupation\neverything you shared in Facebook that\nis by default given to me in most of the\ncases you can restrict the information\nbut most of the public customers they\nwill share this so by downloading this\ndata today they will understand 10,000\npeople clicked and out of them 5,000\nwere youngsters and they clicked on this\nlaan that means the trend is towards a\nparticular bike laan if we give an offer\non bike laan youngsters may actually buy\nit so this is analytics right so all the\nsocial media data they can use in\nmultiple ways actually so this was\ngetting in the Json format actually and\nthis type of data was there they also\nwere collecting their customer care log\nfiles so that is again flat file it was\ncoming in a format of flat\nfile flat file means text file so\ncustomer care log actually means the\nchat logs that they were having also the\ncall logs they don't record they don't\nget the actual call but how long the\ncall happened and what you pressed so\nwhen you call customer you press right\nthis this this huh that is actually\ncaptured and then they want to figure\nout how many people are called on what\ncategory how much time they spend right\nso if you have clicked on a complaint\nbutton and you talk for 1 hour chances\nare very high you are a bad customer\nright I mean whenever you're scolling\nprobably right so they want to figure it\nout right so that is the call record\nthat was coming in flat file this format\nright and at that point in time IC Bank\nalso wanted to offer a solution so\npreviously what they used to do uh their\nagents were there people who go to your\nhouse and make you buy loan and all so\nthey used to go to every place so this\nagent will get a list and he will go to\nthat place and he will go to your house\nand say that sir please take a car loan\nyou will say no he will say bye he will\nget money he is getting salary right so\nthis technique was not working because\nif I get a list okay one customer will\nbe in uh here this is what place what\nno thisr right one guy will be INR\nanother guy will be in some other remote\nplace in Chennai so by by the time he\ntravels from here to that place day will\nbe over so he is able to cover only two\ncustomers so what ICA bank had in mind\nthey will track his location he has a\ndevice okay and give him customers in\nreal time so if he's in kanja he'll get\nall the customers nearby him so this guy\nhas to go right he cannot say I can't go\nso he will the customer application it\nwas a tablet will track this guy and\nwherever this guy goes the nearby\ncustomers will be provided to him so he\ncan cover more customers right and this\nhas a lot of impact because why it was\npractical because sometimes the\ncustomers will call and say that please\ncome I'll give you a loan so what you do\nfrom here you might travel to there is a\nEuropean city Paris in Chennai yeah so\nyou'll go to Paris right and because the\ncustomer called right so you go to Paris\nand meet the customer now you don't know\nhow many customers are around Paris that\nthis application will track and send to\nyou so these guys were collecting that\ndata also from all the tabs and that was\nuh quite some big amount of data they\nwere collecting how many customers they\nmet and whether they closed it or not\nclosed okay that was coming in I think\nsome other format so some format I write\nI forgot actually how it was coming so\nthis is the whole data they have right\nand some more data was there and this\ndata is lying on different different\nsystems it's not in one place the core\nbanking data is is an oracle some other\ndata is in some other place right and if\nyou want to analyze this data one thing\nis that I cannot go to Oracle and say\nthat I want to analyze your data why\nbecause this Oracle DB is already busy\nserving\ncustomers correct this Oracle DB is the\ncore banking database so if you're\nmaking a transaction Oracle is handling\nthat right so I cannot go to Oracle and\nsay that let me run some analytics on\nyour data doesn't work that way because\nthen Oracle will slow down I can't do\nthat so I have to take all this data\nfrom all these places and dump it into a\ncentral place so that is what is you\ncall as your data warehouse\nright that is called a how many of you\nknow what is a data warehouse so a data\nwarehouse is nothing but a database only\nin one way it understands the language\nof SQL but the major difference is that\nit is not facing the customer\nright so you will do something called\nETL okay so there are ETL tools which\ncan get the data from all these\nplaces right and this guy will dump it\ninto a data\nwarehouse right so data warehouse is\nalso called as your olap online\nanalytical platform so on and so forth\nand what is the purpose of this guy so\nthis guy will have all the data from\nhere okay and you can transform the data\nas well so that is why it is called ETL\nextract transform and load you don't\njust take the data and dump it here then\nit is useless right I can read the data\nfrom a flat file and give some structure\nand then dump it here like a proper\ntable format so now all my data is here\nfrom this right and I can run business\nintelligence here bi tools Tableau right\nif you have heard about micro strategy\nTableau what I can do I can connect a\nvisualization tool here and it will show\nme in charts and diagrams what is\nhappening I mean this is traditional\nEnterprise architecture this is how\npeople used to work forever I mean every\ncompany you go this will be there by\ndefault they will have these these\ndesperate uh discrete data sources and\nETL tools so if you have heard about\nInformatica Informatica is one of the uh\nbiggest ETL tool Informatica Talent uh\npentaho ssis so these are all ETL tools\nso these tools what they do they will\nconnect with here and take all the data\nand apply some schema and push it to a\ndata warehouse now the data will be here\nso now all the data is in one\nplace so if I'm Chandra Kar I wish I\nwould be I can go here Chandra Kar is\nwho IC Banks CEO right who is at place\nso I can go here so Chandra Kar may not\nknow SQL right you can't EXP expect that\nright if you're a CEO of the company I\ncan't ask you do you know Java I'll be\nfired next moment right so but the CEO\nwant to know what's happening right by\ngetting all this data only you will\nunderstand what's happening in the\ncompany but you don't have time to sit\nand learn all this language or anything\nso if I'm Chandra Kar I'll go here I'll\nconnect my Tableau and I'll ask my\nTableau show me the last month's uh\ntransaction this quarter this product\nthis area show me it will show a nice\nbar chart\nI'm\nhappy now why this is very very\nimportant if you make any mistake in any\nof this this entire stuff will be\nchanged right and what she sees will be\ndifferent\nright she has no idea how this is\nhappening right only you know how this\nis happening right so your ETL tool\nshould run proper the data should be\ncaptured and dumped here and then\nanalyzed so for CEOs and CTO they use\nvisualization tools to connect with a\ndata warehouse then analyze the data so\nall the data is inside this data\nwarehouse right and this is not\naccessible by Public public has no\naccess to this only internal company has\naccess like maybe CEOs of IC bank they\ncan go here and visualize the data from\nhere others public does not have any\naccess to here right and then they\nvisualize the data they make a decision\nthey may not make a decision that is up\nto them right in the whole setup there\nis no Hadoop so far and this is how IC a\nbank was running so far now what was the\nwhat were the challenges they faced one\nof the challenge was that this ETL tool\nokay I don't know how many of you have\nrun on ATL tools okay they typically run\non a single\nmachine okay so they get installed on a\nserver so there is a limit as to how\nmuch this guy can pull to a data\nwarehouse right how much amount of data\nthis guy pull from here to a data\nwarehouse there is a limitation actually\nyou can't have terabytes of data being\npulled Point number one point number two\nnone of this is real\ntime none of this is real time I go to\nTanish store Tanish is a jeweler stuff I\ngo to a Tanish store I buy something\nfrom there for 20,000 rupees I use IC\ncredit card I should immediately get a\nmessage saying that again you shop 20,\n20% discount that is real time I can't\ndo it\nhere because all this is separate data\nand these ETL jobs run at the night so\ntoday morning if I'm shopping tomorrow\nI'll get the message 20% off by the time\nI'm out of T shop right I may not use it\nat all so IC bank's first problem was\nthat since the data was increasing you\nknow they were not able to run ETL\nproperly second problem none of this is\nreal time they wanted to push customized\noffers their requirement was that\nwhenever any transaction is made they\nwant to capture it analyze it right and\nbased on that they have to recommend\nsomething to the\nuser right say for example you're using\ncredit card or debit card or even you're\nclicking on some link immediately\nsomething should happen now that is not\nso easy right because your data might be\nsomewhere in here Oracle and I cannot\ntrack each and every transaction here\nbecause this is my transactional DB I\ncan't I can't say that track each and\neverything from here right that's not\npossible third problem this data\nwarehouse\nthis data warehouse is a very costly\nAffair I have worked with a company\ncalled teradata they are the leader in\ndata warehousing\nMarket I had a US client uh Terra they\nwere using a product called Terra data\nfor data warehouse they were paying\naround $5 million every year only for\nthis setup now it is good you can\nanalyze the data and all that's okay but\nyou're paying a lot of money actually so\ndata warehousing is actually a very\ncostly Affair it's not so so cheap if\nyou want to implement but companies like\nIC bank they must use it right so they\ncan't say that we will not use it so so\nthey wanted a cheaper solution they\ndon't want all their data in this data\nwarehouse they want an alternate\nsolution should be real time cheaper\nscalable right and anything they want to\nintegrate you know any any type of data\nthat comes they wanted to integrate and\nthat is how they came to us and asked us\nlike can we migrate to some big data\nsolution and we recommended Hadoop and\nthen the migration took a lot of time\nthat is a different case altogether but\nif you talk about Hadoop Hadoop actually\ncomes\nhere in a way I can say it is yeah\nsorry right\nright\nexactly right exactly\nright\nexactly not possible not\npossible exactly so here you have your n\ndimensional cubes and other things right\nin the data warehousing you will have\nyour what you call um uh schema what\nschema you call Star schema snowflake\nschema right and so this is actually\nolap is actually not designed for Speed\nthat is true it is only analytical it is\nbatch job actually right so with this\narchitecture I can never give a realtime\nsolution\nexactly exactly so a lot of people\nargued saying that you can actually beat\nit if you do parallel processing there\nis something called MPP\nright there is something called MPP MPP\nmeans massive parallel processing so a\nlot of people came saying that so these\nsystems are actually Terra data and all\nare really faster actually I mean not\nlike rdbms but they're faster okay so a\nlot of people came up and saying said\nthat okay if your data warehouse is very\nslow why don't you do parallel\nprocessing okay you install it on four\nfive machines and you can do par that is\npossible but still the same problem\nexist you cannot physically divide your\ntables only logical division is possible\nlike I said country based you can divide\nor month based you can divide end of the\nday if you want to do a join operation\nor something same questions will happen\nso I can never propose this as a\nrealtime solution so that is my Crux of\nwhat I'm trying to tell you and this is\nhow uh even ICA Bank was running and\ncost was another problem they had a lot\nof cost involved so and obviously since\nthey wanted to collect a lot of\nunstructured data they wanted to go to a\nbetter solution okay and that is how\nHadoop came into the picture\nnot really mostly here you have\nstructure data mostly structure data\nonly they are used as the final place\nwhere you get the data you know from\nthere somebody can visualize and in here\nyou very rarely you modify the data you\ndon't modify the data because this is\nthe place where the final data is\navailable know from where you can\nconnect and visualize the data that is\nall you do or probably you can run a\nquery and get the output that is all you\ndo it is analysis only you're not doing\nany transaction transaction management\nall you're doing here\nright now so now the point is that now\nlet's discuss a little bit about Hado so\nwhat I thought is like today probably uh\nwe can cover the architecture and little\nbit of\ntheory so that uh tomorrow so if you\nlook at your syllabus that I'm supposed\nto teach U we have something called\nHadoop and Hadoop architecture that is\nanyway required right and then you have\nmap reduce now there is a catch here map\nreduce is the default programming\nframework on top of Hado so if anybody\nwant to analyze the data on top of Hado\nthey use something called map reduce\nPoint number one now map reduce came\nlong back okay and today there are lot\nof ways to analyze the data not only map\nruce now as per your syllabus the\ncurriculum says teach map ruce and then\nsome Concepts in map R and I will be\ndoing that but one other thing is that\nif you want to learn map ruce ideally\nyou should be an expert in Java because\nmap ruce programs are written in Java or\nany other programming language so you\ncan write in Python and other things but\nnatively people write map ruce programs\nin Java now I know that not all of you\nare familiar with Java and comfortable\nwith Java so if I talk a lot about Java\nit may not make much sense out of you\nbut then why you are learning map reduce\nis to help you in spark so next you will\nbe learning something called spark right\n16 hours of spark content will be there\nspark also uses similar Concepts as map\nruce so even if even though you're not\nable to understand each and every line\nof code that I'm writing that's okay but\nyou need to understand the logic how it\nis working because Java programs not\neverybody will be able to follow and\nunderstand and Deb so that is not\nexpected also from you but if we discuss\nmap reduce I write a map program and we\nrun a map reduce program you should be\nable to understand what is happening\ninside that right logically not like\nsyntax wise line by line wise right so\nonce we complete Hadoop architecture\nlike tomorrow I will be discussing map\nruce Theory and map ruce programs will\nbe running and then in the next 8 Hour\nsyllabus we will be completing bit of\nmap reduce and going to something called\nHive P Hive so Hive is your data\nwarehouse in in hadu so that is more of\nSQL side so you can understand that\npoint yeah somebody had a question sorry\nsomeone asked a question in the middle I\njust said I will complete who\nasked nobody\nasked okay I mean so um just rub\nthis right now what I'm going to do I'm\njust going to delete this part for the\ntime being okay I'll just remove this\npart for the time being\nyeah H that's what so now what is\nhappening the traditional data\nwarehouses are there like the data and\nall but they are not so popular and in\nthe market like people used to say if\nyou cannot convince what you\ndo you confuse\npeople and it is very easy to do that\nright because why am I saying this\nbecause I see a lot of people getting\nconfused because this so when Hadoop\ncame Hadoop became a very popular\nsolution for data warehousing right and\nsince Hadoop is open source and you can\nread the mckeny report it will be only 1\nby4 of cost of a traditional data\nwarehouse so what will happen everybody\nwill use Hadoop obviously why should I\npay extra money but then what is a\ndrawback the traditional data\nwarehousing companies they cannot sell\nright so if if today you go to the\nwebsite of Informatica or even Terra\ndata they will say that we are selling\nuh Terra data plus\nHadoop previously you had to give me $10\nmillion now you give me 9.5 take it\nthat's useless actually because why\nshould you buy Terra dat but they are\nsaying because teradata has a name in\nthe market that is a different thing\nright so teradata will say that buy our\ndata warehouse we will also give you\nHado that was cheaper right previously\nyou paid us 10 million now pay 9.5\nyou're like okay great solution I'm\ngetting Hado also you don't actually\nneed to use ter dat\nyou can build a custom solution but the\nproblem couple of problems are that one\nuh the name of the company like Terra\ndata and support also from them second\nit takes a lot of time to migrate to\nthese things one fine morning you cannot\ngo and say that okay from Tomorrow\nonwards we are using only hadu doesn't\nwork like that right in a company you\nneed time to do migrations and processes\nand all so people stick with their\ntraditional tools so what I have been\nseeing uh in some of the companies where\nI went for Consulting they have\ncompletely migrated to Big Data\nplatforms like hadu and they keep Terra\ndata and all in a very less level like\nthe hot data we call right hot data cold\ndata warm data we have so the\nimmediately accessible hot data will be\nthere in Terra data and all and you have\nthe cold warm data which you don't need\nto immediately analyze you will dump it\ninto Hadoop or something and keep it so\nyou can reduce the overall cost in this\narchitecture\nright so now speaking of Hadoop uh our\narchitecture is here still I just\nchanged little bit okay I'll just write\nit\nhere uh the question is when all this\nhappened right so lot of people so I\nhandled a lot of trainings in Hadoop and\nSpark so a lot of people ask me that\nokay so we are learning a completely new\ntechnology called Hadoop and I will say\nthat you are wrong because Hadoop came\nin\n2005 so almost 13 years now so it's\nalmost like very old uh but for for\npeople it is still very new for those\nwho do not understand Hado so uh in 2005\nthis framework called Hadoop was created\nokay long back right uh and there were\ntwo guys called duck cutting and Mike\nCilla who created this framework they\ntook the original idea from Google so\nGoogle was already doing a lot of\ndistributed computing and Google was uh\ndealing with a lot of Big Data right so\nthey published couple of white papers\nand this duck cutting and my Cilla they\ntook the idea from Google and then\ncreated a framework called Hadoop and\nlater they gave it to Apache as an open\nsource project so Hadoop is a open\nsource project actually which means it\nis with Apache right now you can\ndownload it for free install it for free\nyou don't have to pay any money but if\nsomething is really free you have\ndrawbacks also right correct for example\nuh you have Android phones I have an\niPhone I used to use Android phones but\nyou know the difference right the\ndifference is not show off that is not\nthe difference the real difference is\nthat Android phone might get a lot of\nbugs which may not come in an Apple\nphone if you have used both you will\nunderstand the problem is Android is\nopen source right and open source means\nevery company will modify according to\nthe requirement as Samsung phone will\nhave their own Android right so\naccordingly a lot of bugs may come the\nplatform may not be so stable as it is\nif it is an Apple phone only Apple\nproduces it right and if there is a bug\nalso if I call Apple or somebody call\nApple they will fix it because there's\nonly one version of it available and\nthey know what bucks may come may not\ncome so the same story here originally\nit was Apache Hado and even still you\ncan go to the Apache website and\ndownload Hado but soon people realize\nthat if they are using Apache Hado it is\nopen source uh but if something does not\nwork something crashes nobody's there to\nfix it because it is open source and\naccordingly a company called Cloudera\nKing there is a company\ncalled clera you might have probably\nheard clera is the first company which\ncreated uh uh a commercial distribution\nfor Hadoop\nand related tools so today you can go to\nCloud era and ask them to give give you\nHado what is Advantage you will be\ngetting the same Hadoop almost similar\nwhat you find in Apache website but if\nyou install it and if something is not\nworking they will fix it you have a\nsupport since you paying money you get\nsupport just like a vendor right so one\nof the companies which are which is very\npopular in the world of big data is\ncloud era so they Stell Hadoop spark\neverything as a bundle okay even your\nlab actually has cloud data distribution\nthe lab we are going to use actually\nruns a cloud data distribution uh and I\ncan also use the this thing right if\ninternet is there because I just want to\nshow you this so if you just go here and\nsimply type not now probably you can do\nit later simply type Cloud era you can\nsee Cloud era's website\nright and uh Cloud era came in 200\neight so today if you're buying any big\ndata platform the leading vendor is\ncloud guard so they will give you\neverything packaged in one one product\nlike Hadoop will be there no SQL hbas\nwill be there other related tools will\nbe there data science tools will be\nthere so commercially they are actually\nthe number one provider cloud data right\nand somewhere in 2012 we will discuss\nHadoop in- depth okay I'm just giving\nyou some idea we'll discuss the\narchitecture and all there is also one\nmore company called hoton\nworks hoton works came in somewhere in\n2012 okay and honb is also the next\nmajor competitor to Cloud era the major\ndifference is that honb sells original\nhadu Apache hadu they don't modify it\nclera sells a modified version which\nthey say it is better so\nthey no no so it is actually Apache open\nsource license so what Apache says is\nthat anybody can modify it and sell it\nonly thing they should also provide a\nfree version and they do it so if you go\nto Cloud era you can download the\nCloud's product for free you don't have\nto pay any money but you will not get\nsupport so what people do they will buy\nonly with support right so if I want to\nsell Hadoop I can start a company I can\ndownload from Apache modify the source\ncode I can sell it to you guys but two\nthings one I should give a free show\noffer also no support second thing that\nis the story with every open source\nright you have ubu Linux you have Red\nHat Linux what is the difference Red\nHat yes not open source whatever their\nproduct because hoton works is open\nsource hadu and you can download from\ntheir product cloudas is not 100% open\nsource it's a custom product but you can\ndownload it for free from their website\nand install it on as many machines as\nyou want but you will not get technical\nsupport So ideally people will not do it\nright because you need technical support\nso you will pay money and get it\nactually right but these are the\ncompanies the hoton works and clera as\nthe leading vendors in the Big Data\nWorld they also provide certification so\nif you want to write a certification\nexam probably after the spark class you\ncan try that uh claer provides\ncertification exams in the Big Data\nworld you can write it and get there is\nalso one more company called mppar the\nthree are there there are many companies\nactually the most uh common ones are\nthree there is also somebody called\nmapar apach doesn't do it right it's\nopen source Community it's not a company\nit is like you and me so I can edit for\nfree you can also edit for free but you\nwill not so if something crashes what do\nyou do you post a question in the\ncommunity hey yesterday my old servers\nwere crashed I'm fired from the company\nplease help me to fix it and you don't\nwant to do it in Enterprise world right\nso the real thing is\nto be very fair with you why still\npeople are trusting all these companies\nMicrosoft or apple or Oracle they get\nsupport right something doesn't work you\ncannot be able to fix right you have to\ncall them and make them fix so mappa is\nthe third company which is sort of like\npopular this company if my information\nis correct was created by a couple of\npeople who quit clouder and hoton works\nIndians only right and they found this\ncompany called mapar mapar is very\npopular these days actually no very very\npopular vendor\nthe highest paid Edition is also\nmap cost wise they are bigger than cler\nand hotworks cloud and hotw license is\ncheaper than mapar map's license cost is\nvery high actually map's architecture is\nalso very different not like traditional\nHadoop architecture right so for that\nreason uh like I went to GE I went to\nFlipkart they are all using either hot\nworks or Cloud era they don't use map\nmuch\nthey also downloaded the open source\nHado modified it and selling it same\nsame story but they made a lot of change\nto the architecture of Hadoop actually\napart from this IBM has their own flavor\nit's called Big insights Microsoft has\ntheir own flavor that's called HD inside\nH HD inside is there Amazon has every\ncompany has an addition of their own uh\nHadoop and B data platform but most\npopular ones are claa hoton works and\nmappa these guys are the most popular\nones actually\n[Music]\nso huh so for a developer there is not\nmuch difference for example if you're\nwriting a map reduce program it'll run\nthe same way on all three platforms\nthere is not much of a difference the\ndifference actually is that hoton workor\nis totally open source they sell the\nsame what Apache gives Cloud modifies it\na bit and they say that with the\nmodification Hado is more cable that is\ntheir claim we don't know it's all\nmarketing gimmick it is same like buying\nan Android Samsung mobile phone and HTC\nmobile phone what is the difference end\nof the day internally it is Android but\nSamsung will say that our Android is\nbetter more uh bug free or something HTC\nwill say that this phone will never hang\nso only HTC or Samsung know what they\nhave done inside the phone but end of\nthe day if I download at say WhatsApp\nit'll run on both the phones right\nbecause it is Android so same thing only\nit's just a commercial uh distribution\nsame story with Linux you can buy from\nRed Hat there is also one more provider\nright\nsus Red Hat Linux sus Linux internally\neverything is same if I type an LS\ncommand it'll work both ways kernel\nmight be slightly different but the end\nof the day they are paying open source\noriginal Linux only so same story here\nuh but considering the platforms Cloud\nseems to be more more popular because\nthey got a head start 2008 they started\nright long back they started hoton works\nis also gaining a lot of popularity\nthese days I've have been seeing\nthat so we are using Cloud era in our\nlab we have a cloud era distribution\nokay so but we have not we have not\ntalked about hadu right so we have been\ntalking a lot about the company but when\ndid we start the session I didn't check\n2:30 so when do you have brakes and all\nI think 3:30 right\nnow see I don't have any control on that\nbrakes are controlled by\nuh her right I don't have any control\nshe has to approve I don't have any\ncontrol trust\nme uh see she is talking about schedules\nand all I don't know any of this okay I\ncan teach I can stop you say stop I'll\nstop okay by the way if you don't like\nmy class you can work out also but you\nwill not do it because you paid money\nright if it was a free class half of\nthem could have walked out but since you\npaid money you will stay I know\nthat\nokay yeah sure please ask ask as many\nquestions as you\nwant yes\ncorrect no no map reduce nobody is using\neven in the Enterprise map redu is gone\nspark is that is what I'm saying in your\ncurriculum there is map ruce and why you\nare learning map ruce two reasons one is\nmigration a lot of companies will\nalready have a lot of map ruce programs\nand they want to go to spark so if you\nwant to go you should know what is map\nruce second reason even for learning\nspark the concepts of map ruce are\nimportant spark also was built on the\nbasic idea of map ruce so how map\nproduce Works how the program runs these\nbasic concepts if you can understand\nthat will help you to learn spark in a\nbetter way that is the reason we are\nlearning spark uh map ruce actually in\nthe Enterprise totally gone there is\nnothing called map\nproduce no spark uses python in your\ncourse uh that is the reason I'm not\nteaching spark so initially uh Great\nLakes asked me can you teach spark I\nsaid yes and then there was a problem\nbecause I teach spark using\nScala huh I a Scala developer you you\nare you are learning spark using python\nspark can be programmed in four\nlanguages Scala python R and Java four\nlanguages are there and you learn with\npython I'm not a python guy so that's\nthe reason I'm not teaching spark I\nactually like to teach spark because\nmost of my trainings these days are on\nspark actually but I know python also I\ncan teach but not immediately I said\nwe'll pick it up later probably and and\nteach that\nsorry Scala similar to Java\nno not like uh similar to Java 8 if you\nknow Java 8 glorified verion of java ah\nit's a glorified version of java\nactually Java 8 was developed based on\nScala so Scala is another programming\nlanguage it's a very nice language\nactually uh but I don't know people do\nnot really like it okay so like it in\nthe sense it's not as popular as Java\nright Java is very popular but Scala is\nnot so popular but it's a it's a very uh\ninteresting programming language\nI used Scala a lot in my programs and\ntrainings and all anyway so any other\nquestions before we go to Hadoop and\narchitecture and\nall not many\nquestions so if there are no questions\nideally either people understood\neverything or they understood nothing\nright\nso\ndat yeah you can consider but it is not\njust a data warehouse that is a\ndifference so uh you will see soon uh\nwhat is a difference because if I'm\nbuying a traditional data warehouse it\ncan do only one thing it can become a\ndata warehouse I can't do anything else\nHadoop is not like that it can become a\ndata warehouse and much more and we will\nsee\nthat no they still so your architecture\nis\nhuh see now the situation is very\ncomplicated actually because if you are\nstarting a new company and ideally you\ndon't have much money what you\ndo you go to Cloud right so every\nsolution is offered in the cloud and\ntypically uh companies will go for this\nHadoop and all are available in Cloud\nalso so they will go for cloud\ndistributions and so always you cannot\nsay that Hadoop is a 100% replacement of\ndata warehouse okay it can work as a\ndata warehouse but performance\nconsiderations are there so sometimes\nyou might need some reports within\nmillisecond second 2 second which\nprobably Hadoop may not be able to do\nHado is a batch processing system it's\nnot real time so if I have terra data or\nwhat is that EXA data Neta know these\nguys are really fast right MPP so what\ncompanies do they keep the hot data\nthere probably my company has 100 terab\ndata in that 2 terab Data I need every\nday very fast I'll keep it in teradata\n98 terab I'll dump it in Haru I need it\nonce in a while and even if it takes\nsome time it is fine okay same story\nwith the cloud so most of the\norganizations are now deploying\neverything over Amazon web services so\nAmazon offers fully managed Hadoop\nSolutions on the cloud like you can go\nto Amazon ask it give me a Hadoop\ncluster in 10 minutes you will have\nfully functional Hadoop solution\neverything installed up and running\nservers everything automation right so\nthen they also give you TR data\nwarehouses they have something called\nwhat is the data warehouse red shift\nthere is a solution called red shift red\nshift is a online cloud-based data\nwarehouse so Amazon offers red shift you\nif you want H data to be stalled so like\nI said everything cannot be given in one\nsolution you need cheaper you need the\nsame speed you know everything you have\nto package in one solution it may not be\npossible right so like someone was\nsuggesting right you can compromise\nspeed either right or uh realtime access\nright so same like that so had do is a\nsolution most of the people adopt but\nnot everything can be guaranteed at the\nsame\ntime okay so that takes us to the\nquestion what is Hadoop and why this is\nso popular or what is the architecture\nof Hadoop it's fairly easy to understand\nactually because the idea is very\nsimple I have a single machine\nhow much amount of data can I store in a\nsingle\nmachine what do you think I I'll give\nyou a server how much will you\nstore based on\nwhat okay so and that is based on\nwhat yeah so storage capacity how do you\ndecide if I give you a\nserver how many hard disk or how much\nspace you can dump\nyeah so I'll give you a server how do\nyou decide how many hard disk I can add\nin\nthat motherboard right motherboard is\nwho decides so the motherboard will say\nI'll support 10 hard disk you can have\n10 hard disk so the idea is very simple\nso people initially thought I can use\none machine and when that machine was\nnot sufficient what they\ndid no no\nno no no no no no distributor Computing\ndid not came so early\nright ah they added dis they reached the\ndis so one machine is over what do you\ndo no no I don't want to\nArchive super Computing uh no I want\ncheaper I can't sell my\nhouse now another machine is what I'm\ngoing to talk but in between something\ncame\nexternal storage Nas and all right\nnetwork storage probably you guys are\nnot much from Storage background there\nwas something called network attached\nstorage very very popular have you heard\nabout EMC and all company called EMC\nsquare they survived only because of\nthese things so you have something\ncalled Network attach storage Nas box\nyou can buy a\nbox install the Box anywhere and you can\ndump the data there so the system can\ndump and process from there that is\ncalled Nas so that will support more\nstorage actually but it is not within\nthe\nmachine when Nas was not enough came\nsand storage area network sand are even\npopular these days okay so a full room\nfull of hard disk you get a fiber\nchannel connection so if I can dump all\nthe hard dis there I can store as much\nas I want but the problem is even if I'm\nusing a\nsand this is my sand\nthis this give me unlimited storage okay\nproblem is it gives me only storage no\nprocessing if I want to process the data\ncan one server process all the data in a\nsand no not possible right so processing\ncannot happen storage can happen in a\nsand and Sands are very costly clumsy to\nmanage you need separate people to\nmanage and all so people got fed up with\nsand also so Nas was there San was there\nso and that is where you started\nthinking about distributed Computing\ndistributed computing is not a new idea\nit was already there okay so in Hadoop\nwhat we are doing is very very simple\nrather than one machine you take a bunch\nof machines okay so I will say the bunch\nis just for sake of argument four okay\nso I take four\nmachines 1\ntwo\n3\n4 these four machines are\nlet's say Linux machines to be fair they\ncan be Windows machines but uh the\nsupport for Windows is very limited in\nthe open source community so typically\nwe recommend the boxes to be Unix or\nLinux so I take a bunch of boxes say\nfour boxes for servers right and I can\ndownload and install this Cloudera or\nhotworks any of their Hadoop on this\nwhile installing it will ask me which\nmachine is Master which machine is slave\nthe idea is one machine has to be Master\nthe rest of them can be\nslaves so in this architecture I come up\nwith an idea one guy is master and three\nguys are slaves fine so we have not\ndiscussed what is the master doing what\nis the slave doing that's later okay but\nyou install Hado in a setup that when\nthere is one master and let's say three\nslaves right now the idea here is that\nif I want to store anything okay Hadoop\nwill help me to store among these three\nboxes right so I can use the story space\nof all three boxes so if each of them\nare having a 2 tbte hard disk I will get\na total capacity of 6\nterabyte in my Hadoop cluster this is\ncalled a Hadoop cluster a cluster means\na group of computers where Hadoop is\nin yeah so Hadoop is a framework\nactually it is not a software so one\nconfusion people have is that is Hadoop\na software no it is a plan\nplatform okay and I will show you the\ndifferent different components we have\nin the Hadoop platform so if you install\nHadoop let's say we don't know anything\nabout hadu but let's say we install it\nin a four system setup while installing\nit will ask you to select one as Master\nremaining as slave so I have one master\nand three slaves right and what Hado can\ndo it can take the storage space of all\nthese three machines and project it as a\nsingle SLE 6 tbte storage box right so\nthe beauty of this architecture is that\nlet's say you start dumping the data and\nthe data gets stored here and let's say\nyour 6 terab is over I can add machines\non the fly from three I can expand to\n30,000 without shutting down the\ncluster meaning your storage problem is\nsolved you don't have to ever worry\nabout storage in a Hadoop cluster Hadoop\nallows you resizing without any downtime\nso I can add more machines or I can\nremove machines up to me to decide\nright I I'll tell you how the data is\nstored I will I will come to so as of\nnow we have not discussed storage and\nprocessing I'm just giving you a 10,000\nfoot overview like what is happening in\na Hado cluster so one of the advantage\nis that you can start with like three\nmachines and expand to 30 300 or 3,000\nmachines\nyeah like\nis yes you can also add when you have\nyour data here and you analyzing the\ndata okay I can keep on adding machines\nwithout shutting down\nthem you can remove but if you're\nremoving it'll have a problem for\nexample if the program is running here\nif I remove it the program might crash\nbecause this machine is needed that is\nnot the idea if you have one more\nmachine here where nothing is running I\ncan remove it gracefully nothing will\nhappen so you are getting something\ncalled scaling out this architecture is\ncalled scaling out meaning you keep on\nadding more machines so that uh you know\nuh you can get unlimited scalability now\nI will tell you how storage happens so\nfar I did not discuss how storage\nhappens yeah why is it not\npossible which\none this sizing yeah it is not possible\nsqls are designed in that fashion if I\nwant to modify anything I have to first\nstop reading and writing from my\ndatabase then only I can alter my table\nor increase anything or decrease\nanything probably I'll be able to add a\nmachine extra but it doesn't uh make any\nsense in a SQL World actually and in a\nSQL world I cannot come up with a 30,000\nserver setup that's impossible cost\nFirst of all scalability like I said the\nmajor difference here is that in the SQL\nword logical division of data you cannot\nphysically divide the data so when\nyou're logically dividing the data like\nI said country India 2 CR people and if\none machine is taking that it'll have\nmore data another will less data I don't\nhave any way to manage it here that that\nwill be handled I'll show you how that\nis happening so the B so this is called\na Hadoop cluster so the technical term\nfor this is a cluster cluster is nothing\nbut a group of machines right so that is\navailable in a Hadoop cluster now\ntypically uh if you go to companies and\ncheck with them like I was with this 20\n2 bar7 company called 25 bar7 they are a\ncall center company and they run Hardo\nclusters for analytics and all they had\na 50 machine cluster 50 node cluster\neach node has around 256 GB\nRAM and 10 tab\nstorage so 50 into 10 is\nwhat sorry not 10 what am I saying 100\nterab storage so so 50 into 100 is\nwhat 5 petabyte of storage they had and\n256 * 50 that is a ram capacity of the\ncluster that is a very minimal Hardo\ncluster cheap Hardo cluster so typically\nyou'll have 500 knots th000 knots Hardo\nclusters because the amount of data\nyou're storing is in petabytes th000\nterabyte is one petabyte actually so\nmost of the companies they will have\nhuge amount of data to dump in this so\nthe size of the cluster will be in that\nthat uh uh level actually and Hadoop is\ndistributed meaning if you install it it\nis not getting installed in one machine\nit is installed in all the machines and\nthe components will talk so it runs in a\ncluster not in a single machine or two\nmachines actually right so so the basic\nidea in storage is that you can add more\nmachines and get more storage\nright no no no no I got your question\nwhat if the master goes down right I'll\ncome back to that now the slave will be\nacting only as a slave actually\nyeah so good question another thing is\nthat let me ask you this question so I'm\nasking you to build a Hado cluster with\n50 machines take an example 50 slave\nmachines there is 50 servers do you\nreally think it is a cheaper\nsolution in terms of\nHardware like you buy 50 servers or 500\nservers isn't that really a cheaper\nsolution what do you think\nyeah so let them\nsay\nhuh correct now my point is if let's say\nyou're buying 500 servers each server\nwill cost you some money right so if I\nsay 500 times that money right it is\nreally a huge amount of investment for\nme that is the\nreason we call this machines commodity\nHardware what do you mean by commodity\nHardware\nWare commodity Hardware means any\nmachine without any label you will never\nbuy a Dell server to build a Hadoop\ncluster you will bu build it with cheap\nthrowaway\nservers right you're not going to buy a\nVolvo bus you're going to buy a TM what\nis that Tamil Nadu I'm not blaming Tamil\nNadu don't take it regionally okay even\nKerala whatever you're not going to buy\na Volvo bus to accommodate your\npassengers you're buying a bus you know\nwhich can break down at any point in\ntime see you want to carry all your\ncitizens will you be buying only Volvo\nbuses no right if you look at most of\nthe buses that are here what is your\nidea Citizens need to travel from one\npoint to another point and the bus\nshould be there that's all you are not\ninvesting crores in buying Volvo buses\neverywhere right you buy just some bus\nsame idea here in a Hardo cluster\ntypical Hardo cluster the machines that\nwe use are called commodity Hardware\ncommodity Hardware means assembled\nservers so you can get it actually so\nyou're not actually buying an IBM server\nbecause I was also working as a design\narchitect for one of the Hadoop solution\nuh there we got servers for around\n35,000 rupees 40,000 rupees 40,000\nrupees you cannot buy a server if it is\nan IBM server you'll pay 2 lakh three\nlakh so this was like somebody will\nassemble and give you the only thing it\nwill work it will crash\nalso it will definitely crash 100% so\nthen you will be thinking if I'm having\na 500 not Hadoop cluster if my slaves\nare crashing what is the reliability\nthere I will answer that question just\njust par the question for the time being\nno no no server is just a big storage\nbox right storage and\nprocessing yeah so server is what a\nserver is so this is your desktop normal\ndesktops right so normally you have\nsomething called desktop at home and all\nserver is something which can serve\nmultiple clients in one way it is a\nbigger box with bigger storage and\nprocessing so if I want to buy a server\nI have multiple option one is that I go\nand tell IBM or D give buy me a server\nso they will give me a branded server so\nthey will charge a lot of money they\nwill also give me support and all but I\nhave to pay two lakh three lakh for a\nserver so normally when I say server\nthis is what people buy but in a Hadoop\ncluster you never do that you buy\ncheapest servers possible because you\ncan afford to have failures in a Hardo\ncluster I'll show you how failures will\nbe handled in Hardo cluster but you are\nnot inves invting a lot of money on the\nhardware that is idea because otherwise\nthe solution is not feasible if I'm\nbuying 500 IBM servers I can take the\nsame money and give it to teradata so as\ngood as same thing right so I don't need\nsupport or anything I just need some\nnormal machines and Hadoop also does not\nsay what kind of a machine you need you\ncan even build a Hadoop cluster using\ndesktops if you want performance will be\nless but it'll beautifully work or\nlaptops or anything that you want it\ndoesn't say that right so that is the uh\nso I'll keep this diagram because I need\nthis diagram again that is I'm not\nremoving it actually\nyeah H right\ncorrect right so new businesses might\ntake Hado more because after five years\nI may not buy Terra data right if this\nis a situation I might buy a Hadoop\nsolution probably\nI'll tell you what is the biggest\nchallenge in my experience in my\nConsulting experience the biggest\nchallenge is\nknowledge see all the companies want a\nbetter solution only right no company\nwant to unnecessarily pay money but one\nthing is that if it is a Terra dat\nsolution in the market if you go there\nare a lot of people who understand data\nwarehousing data warehousing is a new\nNot A New Concept if I go to the market\nI can get n number of people who know\nwhat is data warehousing and they can\neasily install a Terra data for me I\njust have to pay money but building a\nteam of Hadoop expertise is not easy\nthat is why you're sitting here right if\nI go to a company also if all of them\nhave to understand how Hadoop is working\nand Hadoop is not like click click\ninstall there are a lot of things you\nhave to configure and tune so people\ndon't want to spend that much of a risk\nso one reason is that uh the knowledge\ncurve we call right the learning curve\nis a bit High you know you can't learn\nit in a day and say that okay I'll\nImplement had do it's not possible right\nso most of the companies are stuck in\nthis uh uh\nposition I'll tell you a real story\nsince you asked this question so when I\nwent to one of the companies for\ntraining okay the company said we want\nto learn hadu Basics I said okay no big\ndeal I'll teach you hadu Basics I went\nto the company I started the training\nthey started asking me like a name Note\nBlock size which are all related to hadu\nI said hey I'm teaching you Hado Basics\nyou asking me all these questions they\nsaid we already know Hadoop then why am\nI here they said their company got a\nproject related to hadu they implemented\nHado they're working on Hado for two\nyears okay they learned everything on\ntheir own now they want some training to\nclarify everything is\ncorrect no it it's actually something\nreally happening you check with any of\nyour friends who are in any company who\nare working in Big Data they will tell\nthe same story because the manager will\nsay okay tomorrow your big data project\nwill start who those who are here can\njoin half of them will run away okay\nhalf of them will stay okay and these\nguys have to learn on their own I'm able\nto share this experience because I go to\nmany companies and most of them are\nself- Learners there are no resources or\ntraining programs like this to learn\nthey will sit and learn on their own and\nand and then learn over a period of time\nso that is another problem because if I\nwant to learn Oracle I can get official\ntrainings a lot and I have lot of people\nwho know oracle they can teach me right\nso the learning curve actually makes a\nproblem in these cases actually so I\nthink you guys are better because you\nwill know all this right if you're\njoining in a company you can say that\nhey I am aware of all these things\nprobably you can get an immediate\nproject anyway you are learning to get a\njob I believe right end of the day your\nidea is to get a job\nright I I'll show you I'll show you the\narchitecture so now what you need to\nunderstand so since we were looking at\nthe high level definitions only let's\ndive\nbit deeper Hadoop has three major\ncomponents there is something called\nhdfs there is something called\nYan I can share the slides and PDF if\nyou want you can make a note that's all\nit's not\nmandatory third is called map\nreduce so when you download and install\nHadoop only Hadoop these three things\nwill come by default hdfs Yann and map\nproduce these three components will be\nuh installed by default and you don't\nhave have to do anything they will get\ninstalled right in this this guy is the\nstorage this guy is handling storage\nokay this guy is Resource\nManagement this is the resource\nmanager this is\nprocessing the pen is not\nliting so hdfs is the guy who is\nresponsible for storage to say that this\nis a software piece that manage huh\ninside so if you're if you're installing\nhadu it'll have three components\nactually you can see them actually hdfs\nyou can click and see I'll show you okay\nso hdfs is the hdfs stands for Hadoop\ndistributed file system it's a file\nsystem actually it is called Hadoop\ndistributed file system I'll write it\nhere\nprobably Hadoop\ndistributed file system that is\nhdfs now from your course point of view\nuh hdfs is important because this talks\nlike how the uh data is stored yarn is\nnot that much important but we will\ncover yarn map reduce we will learn for\nsure so map reduce I'm pushing for\ntomorrow so the processing part we will\ndo tomorrow we are not going to learn\nhow to process the data today okay\nbecause if I teach everything also end\nof the day you have to go back and\nrecollect all these things right you\nwill not get it so this part I'll push\nfor\ntomorrow and how many of you are from\njava\nbackground oh some of us are here at\nleast okay so because map is actually in\nJava okay so we will look at the code\nand many people are not from the Java\nbackground also so and like I said in\nthe industry it will never happen that\nyou have to write a map ruce code\nbecause map Ru is gone from the industry\nmigrations only happen so even if you're\nable to understand the logic that's more\nthan sufficient\nactually so first let's look at this guy\ncalled\nhdfs right what hdfs is doing so I will\nprobably push yarn and map ruce for\ntomorrow because once I complete hdfs I\njust want to show you something called\nHadoop ecosystem okay which will I have\nto draw here so we'll complete hdf a\nstorage part Yan is a simple piece map R\ninway tomorrow we will be covering right\nso this is the guy who is handling the\nstorage and lot of you are asking how do\nyou store the data in hero right okay is\nthis\npen this is gone actually we have extra\npen not writing\nactually this one\nthese are used pens I think also is it\nred and green you'll get\nblack this is okay I think\nright so let's take a hypothetical\nsituation where you have installed\nHadoop in one master okay and in my\nexample we have six slave\nmachines now this picture will uh pose a\nlot of questions usually okay and that's\nokay\nso there is one master and six slave\nmachines and we are talking only about\nstorage no processing the master will\nhave a process called name\nnode the Master machine will be running\na process called a name node when you\ninstall it will start running this\nprocess called name node okay the slaves\nwill be running a process called Data\nnode data node so I'll call it as data\nNote 1\n2\n3\n4 five and\nsix so technically we will say that\nthere is one name node and six data\nnodes I mean that is how you say in in a\nHado cluster right so name node is the\nstorage master and data node is the\nstorage slave okay so that thus far we\nhave understood now the the next thing\nis that let's say you want to store a\nfile in a Hadoop cluster Hadoop does not\ncare what you're\nstoring meaning you can store any format\ncan be XML Json images video Hadoop\ndoesn't really care what format of data\nyou are storing while processing you\nhave to make sense of the data so that\nis in map ruce if I stored a text file I\nhave to write it in map ruce that read\nthis text file and do whatever I want to\ndo the hdfs file system does not really\nuh bother about what type of data you're\nstoring second point and probably the\nmost important point is that once you\nstore any file you cannot edit\nit modifications are not possible you\nstore it delete it that's\nit in hdfs modification is\nimpossible and now you have to\nunderstand these facts really well\nbecause you get confused later right so\nhow will I edit it is not possible in\nhdfs as it is so why you may be asking\nwhy it is not possible because hdfs is\ndesigned as a file system for handling\nhuge amount of data terabytes and\npetabytes amount of data right and if I\nwant to modify or edit you know uh row\nby row like in a transaction that is\nhappening you know it is not possible\nbecause if you look at a hard disk right\nthere is something called seek time if a\nhard disk want to fetch a record there\nis something called seek time right and\nif I'm having one terabyte file I want\nto edit the 100th line it's not possible\nfor my hard disk to seek and get the\ndata in that if you're because you're\nusing all commodity machines here they\nare not even faster I'm not using solid\nstraight drives or anything very\ncommodity uh level Hardware I'm using\nhere so hadoop's idea is that you want\nto read files sequentially there is no\nRandom Access there is only sequential\naccess so if I store a file I can read\nthe entire file and process it I cannot\nsay needle in a Haack\nproblem needle in a Haack problem\nmeaning if you have a 1 million row\ntable you cannot say take the 100 line\nand edit not possible in Hado at least\nin Hado it is impossible so either you\ncan read the whole data and process it\nthat's all you can do and the result can\nbe stored obviously if you process the\ndata you get some result that can be\nstored but on the file system editing is\nnot possible you can't\nedit not possible so\nso H but directly update is not possible\nI cannot open a file and update\nsay huh that is appending is possible\nappending is always possible I have a\nfile I want to append the data that's\nokay because you're just adding in the\nend but I'm just reading the 100 throw\nand edit that is not possible now you\nmay be wondering what if I want to do it\nthere is a solution you can do it you\nhave MPP engines on top of hu which can\ndo it I'll show you how to do it I mean\nit is possible but considering only hdfs\nideally if you're storing a file and you\nyou got some modifications in the file\nyou delete it and restore the file\nmeaning you don't edit it actually\nright can we take the Del yeah you can\ndelete you can delete the whole file\ndelete is possible I'm saying that\nminute edits are not possible because\nHadoop is considered to be a system\nwhere you are doing analysis of the data\nit is not transactional system where\nwill your minute edits and all come O TP\nsystems if I'm having an rdbms I want to\ndo insert update insert update it's not\nan rdbms right if you're rdbms world it\nis possible I don't have any objection\nbut here it is not possible by\ndefault huh\nexactly not costly\nexactly correct and also we are not\ndoing any transactional it's not a\nreal-time transactional DB right so I\ndon't want to minutely edit also but\nworst case if I want to do it it is\npossible so we have mppp engin on which\ncan do it actually\nexactly so if I'm storing a one TB file\nif I want to process the file probably I\nwant to process only the 100 line I have\nto load the one TV into RAM hdfs alone\nwith hdfs alone so then the business use\ncases will come so that is where you\nneed to make a decision I will teach I\nwill train you on that meaning if if my\nif I'm writing a group by query or an\naggregate query right probably I need\nthe whole data I have a very big table\nI'm writing an aggregate query right\nwhich will say group buy and Order and\nsomething like that it makes sense I\nload the full file because I don't know\nwhere is a data whole data need to be\nscanned probably but if I'm doing a\nselect something something where name\nequal to something name equal to\nraguraman only one line need to be\nprocessed for processing only one line\nwhy should I load the whole data not\nrequired for that you have an option\nthat's called MPP engine I'll show you\nhow to do that a massive parallel\nprocessing engines on Hado so you have\ntools like a hawk La uh Impala so there\nare some guys who can do that actually\nand I will show you how they work on\nthat no no no they are dumping in data\nwarehouse right from there they don't\nwant to update transaction data update\nhappens here right this is where the\ntransactions are happening correct so\nthis is where customers are placing\ntheir order and and this data you don't\nwant to change this data was created by\ncustomer I just want to get the data\nright probably I want to modify and get\nit so once it is here I don't want to\nmake any changes I just want to analyze\nthe data we will see that anyway now\nlet's take a hypothetical situation\nhuh no I'll tell you what is a role okay\nso what is going to happen let's say\nthat you want to\nstore a file I got a new marker by the\nway right so let's say you want to store\na file of\nsize I don't\nknow 192 MB\nyou have a file text file imagine and\nyou want to store it the size of the\nfile is we'll take a break at 4:30 okay\nit's 410 actually so probably we'll take\nit we'll take it 4:30 anyway so the file\nsize is 192 megabytes you want to store\nit in a Hado cluster now there are a lot\nof things which actually matters to your\nrealtime experience here we are just\nlearning the basics because in a real\ncompany if you want to connect with a\nHadoop cluster the Hadoop cluster will\nbe somewhere else so if you sitting here\nthe Hadoop cluster will be separate you\nwill be separate right so you will get\nsomething called a Hadoop client package\nokay so one way is that you get\nsomething called a Hadoop client using\nthat you can access from your laptop\nsecond is that you can log into a node\nthere is something called Gateway\nmachine this is\ncalled Gateway machine so if I'm sitting\nhere this is me very nice right so I can\njust connect to this machine from this\nmachine I can connect to the Hado\ncluster you don't directly get inside\nthe Hado cluster that's my point for\nsecurity reasons and many other reasons\nso companies do two thing either they\nwill install something called a Hado\nplan even that is very rare what we used\nto do there will be a Linux machine\ncalled Gateway node you will get a\nusername and password you type it you\nconnect to this machine this machine\nwill already know how to reach here you\ndon't have to worry you issue the\ncommands here it'll run\nhere got it right but whatever way so\nlet's say you want to push this 192 file\nfrom your machine you connected to the\nGateway machine you said okay upload the\ndata what is going to happen is that\nfirst thing which is going to happen is\nthat your name node machine this is a\nmaster this guy will tell you something\ncalled block size there is something\ncalled block size this block size can be\nconfigured for every Hadoop cluster when\nyou are installing Hadoop so I am\nassuming that the block size for this\ncluster is 64\nmegabytes so what is block size block\nsize uh tell you block size will tell\nyou what is the maximum size of data\nthat you can store meaning if you're\nstoring a 192 MV file what will happen\nit will divide that into three blocks\nsame thing happens on your laptop if\nyou're storing a a a song on your laptop\nwhat happens will the song get stored as\nit\nis what do you\nthink okay you store a song MP3 song\nright on your laptop what do you think\nthe hard disk will actually take the\nsong as it is or it will divide\nit it will divide it it will not store\nas it is I think Linux uses 4 kilobyte\nof 4 KB block size Linux Windows also\nuses similar any file system will chop\nyour data and store same thing happens\nin Hadoop also so I I'm assuming that\nthis Hadoop cluster has a block size of\n64 MB so this name node will get back to\nyour client package and say that divide\nyour file into three or whatever you\nwant because the block size is 64 MB so\nwhat is going to happen this file will\nget divided into three\nblocks you don't have to do this this\nwill happen behind the scenes like you\ndon't have to say that what is your\nblock size it will happen automatically\nwho will do Hado Cent Hado client\npackage gate this Gateway has Hado Cent\ninstalled that is what so this G Hado\nclient on the Gateway will already\ncommunicate with name node name node\nwill say that boss in my cluster the\nblock size is 64 MB you manage yourself\nso the client will divide your data into\nblocks of 64 6464 total is 128 so now\nyou have three blocks B1 B2 and B3 you\nhave three blocks now of the data right\nand then again you go back to the name\nnode you ask the name node I have three\nblocks okay now tell me where to store\nand the name node what it does it has\ncommunication with all the data nodes it\nknows where how much space is available\nit will tell you that do one thing store\nthe first block probably here just an\nexample store the second block probably\nhere B2 store the third block here\nB3 most of the cases the name node will\ngive you different different machines it\nwill never allow you to store everything\nin one machine so you divide and\ndistribute that is how you're storing\nthe data so now your data is divided\ninto three blocks and they are stored in\nthree different data nodes so this is\nlike six in reality you have 50 500\nmachines so three blocks is easy right\nbut sometimes a couple of blocks may end\nup in one machine also depends on the\nstorage but ideally\nit gets stored like this now you may be\nwondering why the block\nsizes it's not random it has name node\nhas an idea like how much storage spaces\navailable on each data node so it will\ncome up with the free machines okay\nwithin that it just picks randomly three\nof them and say that you dump it there\nwhat yes that has to do with processing\nand I will tell you so if you think\nlogically now if I want to process this\ndata okay three machines can process\nthis data right if I store all the three\nblocks on one machine one machine has to\nprocess three blocks right that makes a\ndifference right so ideally you divide\nand distribute the data most of the\ncases you don't have to do it name not\nwill do it for you\nautomatically this is all inside a lan\nthis is all inside a landan all these\nmachines are communicating with each\nother the data nodes all send a hardbeat\nto the name node saying that they are\nalive also so that the name note can\ndetect how many of them are alive or\ndown or anything\nbut yeah so but now we have a problem\nwhat is a\nproblem if this machine\ncrashes because this is our bmtc bus\nright it's not a Volvo bus right and it\ncan crash at any point in time if this\ncrashes my block one is gone I can't get\nmy data so by default in Hadoop there is\na replication of three which means each\nblock gets replicated three times to\nother machines I'll just draw this then\ncome to your questions give me a moment\nB2 again B2 and I'm just drawing\nrandomly I mean just taking that these\nare the machines B3 probably from\nhere B3 so if you look at each block\neach block is replicated three times to\ndifferent different machines so now now\nlet's say this machine crashes this B2\nis still available here and here I can\nrecover it from here and now what\nhappens is that the name node will store\nthe metadata meaning the name node will\nwrite here there is a file called\nabc.txt it is divided into three blocks\nblock one is available on where\nthree where is it 3\n2 6 6 so B1 3 2 6 B2\nwhere where is\nB2 huh one\nB3 yeah so this is your metadata meaning\nif tomorrow you want to read the file\nyou have no clue where is the file you\njust go to the name node not you the\nHadoop client will connect with the name\nnode and the name node will pass this\ninformation you go to B1 B2 B3 sorry you\ngo to data Note 3 42 you will get it if\nany of them are down it will give the\nalternative one so the purpose of your\nname node is to actually store the\nmetadata or the index and if the name\nnode\ncrashes you cannot access the cluster\nthat is for sure because if the name\nnode is gone then everything will be\ngone to prevent that in most of the\nHadoop clusters you will have an active\nname node and a p uh standby name node\nmeaning there'll be two name nodes these\ntwo guys will be in constant\ncommunication so if the active name not\ncrashes the standby will take over I\nwill talk a little bit more about this\nlater how this active standby is working\nbut you don't have to worry if the\nactive name not crashes standby will\nimmediately take over so now coming to\nthe\nquestions yeah\nplease\ncorrect now three blocks are there the\nreplication huh\nthree sorry sorry\n[Music]\nrepeting very rare all the three notes\ngoing down at the same time is very very\nrare even though we say commodity they\nare not really commodity these days I\nmean even though we say that had because\nthe storage and server cost is very\ncheap these days so all the three\nmachines at the same time going down is\nvery very uh rare but what what is the\nother possibility King JN fired a\nmissile right from North Korea what will\nyou\ndo exactly nobody will be there but\nlet's say King joh fired only to Chennai\nokay and so in Chennai the Hado cluster\nis gone right so what you do you are not\nthere to run the query but the company\nshould survive\nright Disaster Recovery will be there so\ntypically we set up a Dr disaster\nrecovery\nand the disaster recovery to set up that\nuh there is a company called van disco\ndisco disco dance van disco very\ninteresting company so van disco is one\nof the popular companies who set up\ndisaster recovery for hadu but you don't\ndo Disaster Recovery like in traditional\nsystems meaning if I have a 100 not hadu\ncluster I won't set up a 100 not backup\ncluster right that is waste of money\nvery rare king Jong will fire a missile\nright if he fires God Saves right so\nwhat I will do in even in the Hadoop\ncluster you have to classify the data\nyou are storing 100 terab data probably\nnot all 100 terabyte is so important\nright is it no a lot of data is like\narchiv and all so in the 100 terab data\nI will identify probably 10 terabyte\nwhich is very\ncritical and I can periodically back up\nthat data to my Dr Center I don't set up\na 100 node cluster in NOA to back up\ncluster from here so we use a tool\ncalled distributed copy disc CP in hadu\nthat can synchronize these two clusters\nprimary backup dist CP it's called\ndistributed copy I'm not lying so some\npeople say I'm making up\nstories I'll show\nyou um yeah so very excellent question\nI'll just come to that just give me a\nmoment so go to Van\ndisco very interesting company right I\nlike the name who named the company I\ndon't know van disco so van disco is not\nonly a Hado company by the way they\nprovide replication\nSolutions uh Solutions\nbackup Disaster\nRecovery if I go to Disaster\nRecovery why your internet is very very\nslow\nright see it is not even opening up the\npage yeah Disaster\nRecovery H 100 per Hardo availability\nright see this is what I'm talking\nabout we used vanis in one of our\nprojects that is why I know usually\npeople do not be aware of these things\nso I was working as a Hadoop\nadministrator also for some time that is\nwhy I know this because this is these\nare all like real times things\nokay sure\nsorry DP is the\ntool want\nto yeah it is same thing yeah but\nmanaging this on your own is very\ndifficult actually DP can be run I know\nthat even cloud and hot work supports\nbut there are lot of things you have to\nconsider in running D CP first of all\nresource utilization will be a problem\nbecause you keep on reading and writing\nthe data second if it fails you don't\nknow how to manage DP can fail at many\npoints point in time so if you are\nimplementing vandas Co they use this CP\nonly again but they have their own ways\nto handle it actually I mean they'll\nensure the replication happens properly\nyou don't have to bother about it now\nI'm not suggesting that you must use it\nor something if you want dis you can do\nit on your own even claer provides their\nown some tool for disaster recovery for\nHadoop actually but we are here right so\nmy question why the block size is 64 MB\nnot\n4kb block size can be 4kb right\nright maybe I don't know I'm just\nasking seek time seek time if I'm\nstoring in 4kb 4kb I can't read how do\nyou read one\nTB you want huh exactly now this I\nshowed you because my math is very weak\n64 * 3 is 198 I know that that is why I\nshowed this example in real production\ncluster the block size is 128 MB not 64\nI don't know what is three times 128 MB\nto be honest okay so real production\ncluster block size is 128 MB it is\nconfigurable they use 64 also there is a\nparameter I'll show you where you can\nchange block size okay Hadoop the\ndefault uh block size is 128 MB you\nshould also know also understand one\nmore thing there are three major\nreleases of Hadoop Hadoop 1 2 3\nHadoop 1 is the older release and\nnobody's using it these days so you\ndon't have to worry about Hadoop 1 it is\nnot available in production right now\n2013 was the last year when this was\nused so no longer used Hadoop 2 is what\nyou are using I am using we are using\nthis is the current edition Hado 3\nreleased in 2017 December 15 like 3\nmonths back okay now let me ask you a\nquestion if you are storing 100 terab\ndata in a Hardo cluster how much storage\nyou\nneed is that\ntrue but only replication considering\n300 right into three is it good or\nbad bad what is your thoughts I mean see\ntrainings are always interaction right I\nmight know a lot of things I might not\nknow a lot of things you know right\nright so you can so what is your thought\non\nthat can fine two can be fine I don't\nI'm just asking I mean it's not a yes or\nno question I'm just asking what do you\nthink so there are multiple approaches\none thing is that some companies this\nreplication Factor can be changed by the\nway it's not hardcoded some companies\nsay two\nright so you get two copy of data also\nwhile copying the data you can mention\nthe replication Factor very interesting\nthing I'm copying a file one TB file\nthis file I can afford to lose I can say\ncopy with replication\none but it had led into a debate in the\nHadoop world so a lot of people said\nthat this is very bad actually because\nthree times replication means you're\nlosing a lot of space that is why in\nHadoop 3 there is no\nreplication very interesting point I was\njust beta testing Hado 3 even I don't\nknow completely in Hado 3 we use a\ntechnique called eraser encoding it is\nsimilar to raid RAID 10 Z parity how\nmany of you are aware of it parity based\nraid okay there is something called raid\nnot the Akay Kumar movie something\nbetter okay redundant array of\nindependent discs that's called raid\nraid is a fall tolerance technique okay\nso based on raid they have formulated a\nnew strategy in haduk 3 where you don't\nreplicate the data there is another\ntechnique called eraser encoding if you\nenable that you can still recover the\ndata if it fails they say it will use\nonly 30% of additional storage meaning\nif you're storing 100 GB you need 130 GB\nbetter right much much better you don't\nneed 300 GB but Hadoop 3 will not make\nit to production very soon so don't\nworry right still we are on Hadoop\n2 who needs T\nbreak yeah t break so you want the\nquestion or you want the\nbreak\nhuh oh so you guys are too interested\nactually I thought you will all sleep in\nthe class we should get the Hado class\nin the morning and other class in the\nevening right no no no don't do\nthat\nwhy\nwhy that is more stat I actually wanted\nto learn statistics it's good right I\ncan probably come and sit in the morning\nprobably now statistics if is after\nyou'll definitely sleep 100% is\nguaranteed I'm not blaming the trainer\nokay don't take it that way statistic\nitself is a subject which is a bit\nboring actually right\ntoday no no tomorrow again second half I\nam coming talking\nSTS is it so first half what you\nlearned r r programming okay okay got it\nI have question huh\nTex file I can\nunderstand\nindividually\nyeah I know picture\nfct I will give you a solution so the\nprocessing has to be handled uh by the\nframework so we have something called\nmap reduce in map produce you have a\nJava class there is a abstract class in\nJava I mean so for those who do not know\nJava there is a there is a class there\nis a a technique where you tell what\ntype of a file you dealing with and\nthere are only limited types you can\nread text uh XML uh sequence file key\nvalue so some limited number of types of\nfiles are there and how do you so this\nis a very good question how do you\nhandle unstructured\ndata so it is also very confusing\nconcept many people believe that in the\nworld of Big Data you are always playing\nwith with uh videos and uh images\nactually no see many people believe\nevery day you'll go to office and run\nsome facial recognition like you seen a\nmission impossible movie and yes yeah I\ngot big data no you don't do that\nactually you can analyze images and\nvideos there is no it's very difficult\nactually to do analyzing video and image\ndata is very difficult because you can't\ndirectly analyze it you have to convert\nit into binary once it becomes binary in\nHadoop you have a format called sequence\nfile you to RIT a sequence file and\nwrite a program to analyze it it's\nactually very difficult job to analyze\nthese kind of files and not everybody\nneed to analyze\nit let me give you a realtime example I\nwas working with 25 by7 right the\ncustomer care company what is their\nprimary business customer care in\nCustomer Care people\ncall right do you think they will listen\nto what customer is calling\nyes or no so you are running a customer\ncare company not your company you are\ngiving support to other\ncompanies and you are running a call\ncenter in a call center customers will\ncall right do you actually listen to the\nwhat they are talking let's say take\nidea for example how many calls idea\nwill be getting in customer care do you\nreally think they analyze your\ncalls two things one it is legally not\npossible I think I'm not 100% sure maybe\nit is possible they say that be calling\nit your call be\nrecorded fine fine that is just for the\nquality accent and all that is not doing\nbig data analytics for example if you\nwere angry customer you spoke for 1 hour\nthat call is recorded the person who\ntook the call will go to the manager and\nthe next meeting manager will Analyze\nThat call and say that this was your\nmistakes I'm saying do you actually\nanalyze big data analytics on these kind\nof things not really right you will\nanalyze the metadata\nnot the actual data how many people\ncalled what was the duration what they\npress this is the data not the actual\naudio data because you can do it I'm not\nsaying it is impossible but then you\nhave to so how do you understand what\nhe's\nspeaking you need voice recognition\nright and it may fail it may not fail\nyou can you are not 100% assure if it is\na Us customer or something we can\napproximately match with an Indian\ncustomer how do you identify all this\nit's not possible possible right he may\nspeak any language any accent right so\npractically these things are not\npossible I'm saying you can analyze\nunstructured data there is no doubt\nabout it and companies like Facebook and\nall do Facebook actually analyze video\ndata they have to for some reasons that\nthey have to do right and there they\nhave written complicated algorithms to\nread and convert to Binary and all but\nif you are going to work in a regular\nproject you will all see structure data\nCSV files Json data\nnormal text data but if you're storing\nalso so you need a class to read these\ntype of files that's what the question\nwas\nright yeah\nsorry go\nhuh oh uh that's an excellent question I\nalso missed another question from this\nside that was same question no\nyeah so uh I think we'll take a break\nbecause I need more time to explain\nanyway when I say Hadoop it is this we\nknow there is hdfs Yan or map ruce but\nthat is not only Hadoop you have lot of\nother tools on top of that so you need\nto have at least a rough idea as to what\nare the tools we have in the Hado\necosystem the whole stack right so I\nwill talk about it once we complete hdfs\ndiscussion I will go to the ecosystem\nstack because in the ecosystem stack you\nwill have a lot of questions how this\nwork how that at least a basic idea\nbecause I'll take the same ICC scenario\nand I'll I'll tell you how we mve then\nyou will get a better idea probably\nright uh yeah so we were on this\nquestion\nside long I'm sorry I didn't notice you\nuh that's an yeah that's an excellent\nquestion so the question was that if one\nmachine is totally gone if I plug an\nadditional machine will Hadoop\nautomatically take care of copying the\ndata no the Pro the problem the point is\nthat if you that's called commissioning\nso if you add a new node here or let's\nsay we are simply adding a new node\nlet's say the existing setup is that if\nI add a new node this node will be empty\nso you have to do something called\nbalancer there is a utility called\nbalancer I will run balancer\nso that Hadoop will analyze and\nunderstand that this is underutilized\nthis is over utilized it will move some\nblocks here if you don't run balancer it\nwill keep it as a new machine but the\ndata will not be balanced so we run\nsomething called balancer to manage that\nthere was another excellent question\nthis replication how does it work so\nreplication it works in a very\ninteresting way because if you're\nwriting the data the first block goes\nhere B1 right now this guy will tell\nthis guy to get a\ncopy and this guy will get a copy and\nthen this guy will tell this guy to get\na copy so third copy is got finally this\nwill send an acknowledgement here here\nand you will get an acknowledgement then\nonly a block is written so it is called\npipeline right you are not pushing\neverything at the same time as a\npipeline it writes and gets an\nacknowledgement that was one\nquestion uh\nyeah sorry\nyeah so this will be uh communicated\nwhen you are doing a Hadoop copy\noperation you will talk to the name node\nand the name node will give you a list\nof data nodes a chain of data nodes\nwhich should get the replica primary and\nreplica this information will be pass to\nthem what is\nMaxim maximum storage uh excellent\nquestion because the name not actually\nstores metadata this metadata is in the\nram when the name node is running the\nmetadata is being served from the ram\nactually it also persists in the hard\ndisk so that in case it crashes it gets\nuh for 1 trillion files 64 GB is the ram\nrequired so you should actually\ncalculate the Ram size not the hard disk\nsize uh one trillion file a file size\ncan be anything I'm not talking about\nfile size because for a single file 256\nKB of metadata is\ncreated one file means how many blocks\nare there it's replica that is the\nmetadata right so 64 GB can handle 1\ntrillion files in hard cluster so\naccordingly you to increase the Ram size\nhard dis size is fine you don't need\nmuch hard\nsize kb per\nfile\nexactly no no no you can increase the\nram of that machine right and the name\nnode is not a commodity Hardware very\nimportant point the name node is a\ncostly machine you don't want a that's a\nvol bus right because you don't want\nthat to crash\nunnecessarily\nhas exactly so if let's say this machine\nis down very good question if this\nmachine is down this B3 and B1 is gone\nautomatically it will create one more B\nand B1 somewhere and if this machine\ncomes back online it will delete them\nbecause it has to always maintain a copy\nof three always the replica should be\nthree should not be four or two so that\nis automatically handled you don't have\nto worry it'll delete and recreate if it\nwants\nwhile consider\ndat\nname it will take it will take data\nlocality near near by considerations\nwill be there in that how the Met will\nbe okay so I will come to that just a\nmoment metad data I have to talk about\nit yeah one more the first thing you\ntold Lo SI is\n64 that was because 128 is actual okay\nlet it\nwhat how as I understand this hdfs isct\nsystem on top of\nLem correct\nis going to\nthe so when you are installing hdfs even\nthough internally it is Linux file\nsystem it'll talk to the Linux file\nsystem and the 64 MV will be further\nchop down the 64 MB is on Hado\ninternally on Linux it is 4K\nbut this entire 64 MB will be one point\nyou don't it will not fragment it you're\ngetting my point right if I'm simply\ndumping 64 MB on Linux it'll divide into\n4 KB and fragment it in many many places\nwill here it will be continuous so read\nand WR are\nfast uh any other questions you\nhave ah so balancer has to be ideally\nrun by the administrator Hadoop\nadministrator you have to manually do it\nand there is a command called Hado\nbalancer you can run it from the GI also\nyou can run it uh I will show you so\ntomorrow if you guys can remind me about\nI'll show you all this I may not be able\nto run a balancer but I can show you\nwhere it is I will forget right so\ntomorrow I'll show you the Hado cluster\nI can show where are these things\nactually so the H balancer is part of\nyour Hadoop installation hdfs hdfs so\nyou can say run balancer but normally we\nwill run balancer only during off PE\ncovers like during the daytime if you\nrun it it has to move the blocks and\nyour processing and all will will get\ninterrupted W interrupted but it'll be\nslow so let's say night midnight 12:00 I\nrun the balancer I make sure that the\nblocks are evenly\ndistributed huh so this block size that\nis also a very good question uh this\nblock size is just a logic okay so my\nquestion is that if I have a 200 MB file\nhow many blocks will be\nthere\ntwo uh no let's take 64 MB uh and 200 MB\nfile what will be the size of fourth\nblock so how many of you say that the\nfourth block is 64 MB I will say\nyes how how many of you say fourth block\nis 12 Mb not 8 MB\nright very\nless it is 8 MB it's not 128 MB the\nblock size is valid only when the file\nsize is more than the block\nsize any file you divide ultimately you\nwill end up in a 10 MB or 8 MB so had\nwill keep it as it is otherwise if it is\nstoring in a 128 MB piece remaining data\nis unnecessarily so it is a logic\nactually it is a logic even the physical\ndivision happens here okay logically it\nwill keep a block of 12 Mb only because\nyou have only 12 Mb data left there got\nit it's not like physically I'm marking\na space and filling only this much that\ndoesn't happen like\nthat\nsaid to contact\nsystem when you are storing only it\ncontacts right if I'm storing a file of\nsay 200 MB it knows four blocks need to\nbe created right fourth block size it\nwill say that 12 Mb only I need because\nI have only 12 Mb data so this 12 M also\nwill be sequence exactly it will be in\nsequence\nonly otherwise remaining space is wasted\nright and every file will have some\nwasted space so why are you wasting that\nmuch space if I store one trillion file\nhow much will be wasted can you think\nthink lot of file size right that is not\nrequired actually so it is a logic\ninternally built in to hdf is that the\nblock size is valid as long as the file\nsize is bigger than the block size once\nyou chop chop chop and last 10 MB that\nis 10 MB only it's not 128 MB or 64 MB\nright it will increase exactly so if you\nappend that block size will increase so\nthat's a good question so in this 200 MB\nfile if I'm appending the data right it\nwill increase it will provide a room for\nthat actually because the last block was\nonly 8 MB now you keep appending the\ndata that block will become a 128 MB it\nwill keep on\naable no in Linux what it does is that\nwhen it is creating a block if a block\nsize is small it will say that gives\nsome room when you're creating the next\nblock don't keep it immediately next to\nthat the hard disk is so big that it\nshould not immediately create it there\nright most of the cases happen will\nhappen very rarely probably it may not\nwork there very rarely okay it is almost\nimpossible in most of the cases you will\nget that data\nthere yeah so that is for processing I\ntold you I will come back to that so if\nI'm putting all the file one big file\nhere if I want to process it this\nmachine only can process my file right\nif I'm splitting it now three machines\ncan parall process my data right my\nprocessing is faster so if I'm and the\nsame time you will not run all the\nprocessing another misconcept that\npeople have is that in a Hadoop cluster\nif you have 100 TB data all the 100 De\ndevelopers at the same time will say run\nno it doesn't work like that are you\ngetting my\npoint you have a lot of data in a hard\ncluster you wrote a program I wrote a\nprogram he wrote a program we won't come\nto office exactly 10:00 and say run the\nprogram same time doesn't work like that\nyou may have a lot of data people would\nhave written programs but you schedule\nthe program so that is where scheduling\ncomes I was about to talk about it so if\nyou're working in a Hadoop cluster\nresources are very important right\nbecause each machine has processor then\nhard disk and network right and each\nmachine will be processing your files\nwhatever you have written so we have a\ntool called Uzi Apache Uzi Uzi is a tool\nactually that comes along with Hadoop\nwhen you install as a tool so in Uzi you\nwill be scheduling your program you\nwrote a spark program analyze 100 TB\ndata if you immediately submit the\nprogram probably the program will not\nwork because already somebody's\nanalyzing the data where do you have uh\nspace to fit all these things right so\nwhat you do you write it in Uzi and you\ncontact your Hado pmin say that I want\nto analyze 100 TB data when can I run\nthis he will say tomorrow 11:00 okay\nsubmit most of these analysis is batch\nnot real time real time is a different\ncase but bat jobs you schedule it bya\nUzi also you have something called Q in\nHadoop for example you are working as a\ndeveloper I'm a tester can we have same\naccess in the cluster of of course\ntesting cluster will be different but\nlet's say I'm a researcher you are a\ndeveloper he's a data scientist can we\nhave the same access on the Hado cluster\nno probably you are a developer you need\nmore resources okay probably I need less\nresources so what we do we create\nsomething called Q okay there is\nsomething called Q we create in those\nque we allocate resources if you're\ncoming from the developer team you will\nget 30% of the resources if you're\ncoming from other team you get 40\npercentage right otherwise I can utilize\nall the resource right if I write a\nprogram I can make the cluster only for\nmine that's not possible in a Hado\ncluster so management is entirely\ndifferent how do you run these jobs are\nall different but to just answer the\nquestion why you are dividing is that\nyou can use parallelism otherwise a\nsingle machine has to process everything\nright quota size is different ESS Q\nright Q means multiple teams will be\nthere each team can get a que so when\nyou submit a program you will submit to\na que you will not direct ly submit to\nthe cluster so let's say uh just for an\nexample take a in terms of\nprocessing queuing in terms of resources\nsay for example in your haduk cluster\nyou\nhave huh you have 10 data nodes each\ndata node has let's say 10 GB Ram right\nso total you have 100 GB Ram in the\ncluster right H and let's say 10 data\nnod each has four core processor so so\nyou get what 40 core\nprocessor right now if if somebody want\nthey can get all these resources in\ntheir program if he is getting nobody\nelse can run a program in the cluster\nshould not be like that so what we do we\ncreate four qes let's say or two cues in\nthis que I will say 60% of resource\nshould be there here I'll say 30% should\nbe there here I'll say 10 per will be\nthere so this will be the developer CU\nthey will get 60% of the resources that\n60 GB RAM and what is that 30 UH 60\npercentage right 24 24 core processor so\nthey will never get more than 24 so\nmultiple ways of scheduling is there in\nthe normal scheduling they will get\nmaximum 60% only or if the other Q is\nempty they can claim the resources from\nthere no no Q is actually a Java class\nyou can create something called\nschedulers in Hado there is something\ncalled capacity Eder Fair scheduler and\nall so the scheduler is called a que so\nif I'm creating something called Fair\nscheduler within Fair scheduler it will\nallow me to create cues so I can say\ncreate three cues and whenever somebody\nis submitting a program they have to\nmention the Q name also otherwise the\nprogram will not\nrun\nokay\n[Music]\nokay correct but the data might be\ndifferent in no no this is not talking\nabout the data data they will get their\ndata I'm talking about RAM and\nprocessing power okay data is available\nokay so the location of R from H so if\nI'm running my program we will come to\nmap ruce tomorrow this exp y h so if I'm\nrunning my program I need a ram and\nprocessor to run my program right how do\nI run my program I need RAM and\nprocessing power that the cluster should\ngive me\nright so I can limit People based on Q\nonly this much resource you can get that\nis possible that is called scheduling\nin can be out depend upon theu ah so\nwhen you submit to a queue within inside\nthe queue you can write a policy the\ndefault policy is fifo first in first\nout right apart from fifo you can also\nhave priority ities and other things you\ncan write your own policy whatever at a\ntime in the cluster only one job is run\nno no at a time three jobs can run these\nthree qes can people can submit right\nthree qes three jobs at time three or\nmore than for example this Q has let's\nsay uh 60 GB Ram right I need only 20 GB\nRam you can also use 20 right three\npeople depending on how much you want to\nprocess huh how much resource you want\nthat has to be I'll tell you how to\ncalculate that all this\nnone of this is important but these are\nall admin activities none of this I\ndiscussed is actually related to your\ncourse but this may all make sense when\nyou start working see learning is one\nthing okay if you if we just want to\nlearn we'll just learn map produce and\nrun some programs and go home but when\nyou actually start working uh you know\nthese these things are actually used in\nreal life want to know h no no no don't\nworry scheduling and all Uzi nothing\ncomes in your uh lab part in the lab\npart actually what is there hdfs is\nthere you will load the data read the\ndata map reduce is there hi is there\nthat's all none of this actually comes\nbut I can show you at least from the\ncloud manager UI this uh this thing what\nis that balancer and where is a name\nnode where is a data node these things\nwe can see since just we want to look at\nit so are we clear about\nhdfs I think at least we can say that uh\non a on a basic level we are clear\nprobably not everything but yeah\nI didn't get records\nmeans huh so here it is text file or\nflat file any file you can select so by\ndefault like I said if you're storing\nsomething on hdfs hdfs does not care\nwhat is your format you store an image\nit store this image jpg you have to make\nsense of it while reading the data it it\nis not like rdbms so that is one thing\nit is not like rdbms at all because it\nhdfs is a file system so if you dumb say\nfor example on my Windows machine can I\nstore an MP3 file can I open a notepad\nwhy don't know how to pass exactly I can\nstore an MP3 file in Hado if I want to\nprocess I should write a logic to\nprocess it actually\nthat is for only queries map reduces not\nquery map reduces programming language\nqueries you're talking about hi so hi is\nyour data warehouse on hadu hi will\naccept normal SQL queries whatever\nSQL on the data yeah so you have to\ncreate a table and all from the\ndata exactly so Hive is like a you will\nyour data will be in htfs you have to\ncreate a table give a schema then it\nlooks like an ADM table then you can run\nyour\nqueries no it's not an rbms at all\nactually the data is running\nL\nexactly it can be a CSV it can be a text\nfile regular files it can save\nyeah huh I will show you in Hive how to\nstore and process the data\nright yes in Hive you have something\ncalled called indexing but it is not as\nefficient because uh indexing will\nreally work well if your data can be\nmanaged by the rdbms system or the data\nwarehousing system if I'm indexing the\ndata in hi the data is actually managed\nby\nhdfs right so I can get some performance\nimprovements but not like in your rdbms\nnever compare hadu with rdbms you can at\nthe most you can compare with a data\nwarehouse because it is not\ntransactional right it's equal to a data\nWarehouse I can\nsay I didn't get you you're asking if I\ncopy a file can\nI no you don't create a record that's\nwhat I'm saying you are again talking\ntransactional right you don't create a\nrecord see this is a big dat platform so\nyou dump terabytes of file you are not\ncreating a record very rarely you edit\nthat's what I'm saying this needle in a\nHaack problem right so very rarely you\nsay that I want to create a new record\nyou will be adding files continuous\nstreams that will be huge in size you\ncan afford to lose you can afford to\nlose the data I will talk more about\nthat in the ecosystem side\nokay huh\nH so a lot of companies actually run\nthis using virtual machines it is\npossible but performance we cannot\nalways uh depend on VMS I mean VMS are\nultimately what uh they don't use uh if\nit is a bare metal virtual machine you\ncan guarantee uh so there are two types\nof VMS right you something called bare\nmetal like a hyperv and all there you\ncan get good performance or VMware if it\nis not a bare metal no uh performance\nwill be compromised because again you\nhave to go through one layer then talk\nto the storage and\nall it can be configured so I will just\ntalk about something called Hadoop\necosystem and then we will discuss\n[Music]\nfurther located correct\nis uh no it's all a Java call actually\nthese are all so the Hadoop is actually\nwritten in Java so client is also in\nJava everything is a Java method\nactually so you are just invoking a Java\nobject there is a FS read object which\nyou call and this object will get all\nthis metadata and give it to the client\nand the client will keep it in memory\nand then go and read one by one from\nwherever it want okay so I will just\nexplain this ecosystem and come back to\nyour questions further I know there are\nlot of questions and it is good but we\nshould finish a couple of topics more as\nwell right so uh I'm just taking this IC\nuse case so you guys will understand\nbetter so traditionally we do something\ncalled ETL to bring the data right and\nnow what we have we have this hadu so\nI'm just writing here\nhdfs so this is your hdfs file\nsystem this is hdfs and in the world of\nHado we very rarely do\nETL why because ETL means you are\nreading the data transforming the data\nand dumping the data if you're getting\nbig data transforming it on the Fly is\nvery very difficult actually so in the\nworld of Hadoop what we do is something\ncalled elt instead of ETL you will do\nsomething called elt that is extract\nload and transform so first thing is\nextraction how do you get the dat into a\nHadoop system so there are multiple\ntools first of all if you are having\nstructured data say you have data in\nOracle you have data in MySQL so this is\nall structured data which understands\nSQL there is a tool called\nscoop there is a tool called\nscoop scoop can get this data to your\nhdfs huh\nscoop it is an ETL tool you can't call\nit as an ETL tool scoop is used to bring\nthe data from any SQL system to Hadoop\ntransfer the\ndata\nexter no it is part of Hadoop ecosystem\nso what is going to happen when you say\nthat you want Hadoop from claer claer\nwill give you Hadoop on top of that it\nwill give you all these tools which I'll\nbe explaining now so scoop is part of\nyour Hadoop\necosystem and scoop is used to bring\ndata from SQL stores it can take from\nlet's say Oracle it can take from MySQL\nbut it cannot take from flat file Json\nnothing nothing only SQL only places\nwhere SQL\nunderstood so from ICA use case we got\nall the data from CRM through scoop to\nhad okay so scoop is an Apache tool it\nis a very light white\ntool we faced a problem there I wanted\nto discuss that let me ask ask you this\nquestion this is IC Banks core banking\nOracle database do you really think they\nwill allow you to touch it then how do\nyou get the\ndata\nmeaning who will copy they will should\nbe\ncopy\nreplication any tools or or how do you\nreplicate I mean\nexcellent CDC change data capture so we\nFace the same problem there is Golden\nGate Oracle has a solution called Golden\nGate so CDC change data capture meaning\nthey will not allow you to touch it\nright so they will have some replica\nsolution so where they will write it\ninto a replica DB it's actually a log\nfile from there you to capture and get\nit they won't allow you to directly\ntouch their Oracle DB by anyway it's\ncalled C CDC change data\ncapture no that will have only the Delta\nso what they will do they will give you\nthe original data backup data they have\nthe CDC will capture only the changes in\nthe\ndatabase got it and then you have to\nfrom CDC you can write a Java code and\nthen get the updates dumbed into here\nthat is how you get\nit okay CDC will discuss later because I\nneed spend half an hour here to explain\njust understand the changes can can be\ncaptured because it is anyway not\nrelated to your uh know Big Data\nplatforms this is just a business\nscenario yes yes yes yes of course they\ngoing to VI all these exactly exactly so\nwe will not discuss CDC in detail\nbecause it'll take another half an hour\nI don't want to spend half an hour it is\nnot related to your syllabus anyway CDC\nand even I didn't do CDC because there\nwill be teams who will do that even I\nwas not aware of it it's a technique ah\nit's a change data capture is a\ntechnique so whatever changes are\nhappening here it'll capture and dump\nsomewhere from there you can read\nit database exactly from there you can\nread it so I was just saying that we use\nCDC to get the data so Golden Gate is a\nproduct that Oracle uses for CDC anyway\nwe use scoop to get from here and here\nright now comes the place where we are\nhaving a lot of debate actually okay uh\nnow the real question is that okay I\nknow how to get the data from here\nthrough scool right what about the other\ndata right now if you want to get the\nother type of data there are multiple\nways you can get okay one of the most\npopular used tool is called\nFlume Apache\nFlume Flume is a point too delivery tool\nwhat Flume does it takes the data from\nSource give to the destination The\nSource can be anywhere and normally\nFlume is used for unstructured data say\nfor example uh in a folder you're\ngetting all the Json data or XML data or\nflat files I can create something called\na flume agent and this guy will keep on\npolling that folder Whenever there is a\nnew data this guy will read it and it\nwill send it to hdfs this is what Flume\ndoes now the real interesting thing is\nthat Flume can do two things one thing\nis that it can simply read all this data\nlet's say this is in a folder\nokay it can simply read this data and\nsay send it to\nHado this is fine but icsi Bank also had\nanother requirement so this is simply\nyou're collecting the data and dumping\ninto Hado they don't want to do that\nokay so they wanted they had a\nrequirement in which what used to happen\nwas that this Flume was capturing some\ndata okay some data was getting\ngenerated in the form of Json here so in\nthe social media if somebody is clicking\non one of their products they wanted to\nsend an\nSMS real time so this Json data will\nkeep on coming you will keep on getting\nit and Flume can directly dump into hdfs\nbut that is useless for me because I\nwant to analyze the data whenever the\ndata is getting generated that is where\nthis guy comes into picture\nKafka Kafka so there is a difference\nbetween Flume and Kafka okay you have to\nunderstand Flume is like a point to\npoint delivery you give me the data I\nwill dump it there that is all Flume\ndoes it doesn't do anything and Flume\ndoes not store the data anywhere meaning\nit'll take the data temporarily it may\nstore and then immediately push the data\nKafka is a message Q anybody aware of\nmessage cues so Kafka is your message\nqueue in the Big Data world meaning what\nthis guy will do this guy will be\nrunning in its own cluster no Hadoop\ncluster this guy will run on it own\ncluster like 30 machine 40 machine this\nguy Flume can send the data to this guy\nKafka and Kafka is a cluster so it can\nstore the data by default it can store\nfor seven days by default okay anybody\ncan go to Kafka and get the data publish\nsubscribe system we call it the\nadvantage is it will store the data so\nthat multiple people can ask it flu is\npoint to point if flu gets the data\ndumps the data\nhere yeah so here what we used to do we\ndidn't do this okay we get we got the\ndata to Kafka from Kafka I can push it\nto hdfs one way so I'm getting the\noriginal copy here okay now in ICA bank\nso the next question is that how do you\nanalyze data in real\ntime so somebody's clicking on Facebook\nthe data is coming as XML right and\nimmediately when the data comes flu will\nread it push it to Kafka so now the data\nhas reached still Kafka but I want to\nanalyze the data in real time that is\nwhere your spark comes into picture\nthere is something called spark\nstreaming in spark you will learn this I\nthink I'm not sure okay there is\nsomething called spark\nstreaming so spark streaming is a\nutility which analyzes real-time\ndata now fum can read any type of data I\ndidn't get\nsorry point to point\nso it just takes from here D to\nKafka is for contct the SQL Server\ndatabase and no no no flu will not work\nwith SQL Server only scoop will work any\nSQL data scoop any other type of data\nFlume now Kafka can also directly get\nthis data now this is a bit confusing I\nknow okay because this is very new to\nyou first of all but Kafka can also\ndirectly get the XML file but but the\nadvantage is that Flume is pull based\npull based means if I install Flume\nwithout disturbing the setup I can pull\nthe data Kafka cannot pull the data you\nhave to install a tool here and modify\nthe existing\nsetup Flume is pull based meaning if you\ninstall plume here it can start reading\nthe data automatically I don't have to\nmodify anything if I install Kafka\ndirectly Kafka can also get the XML data\nit can read the XML data but I need\nsomething called a Kafka producer that\nhas to be installed here on the ICS\nSource system that is not possible right\nso since it is an existing setup I can't\ndirectly get through Kafka I get through\nFlume Flume will give to Kafka Kafka\nwill send one copy to hdfs because I\nneed to keep the original data second\ncopy it will send to something called\nspark streaming spark streaming is a\nlibrary available in spark where it can\ndo real time processing the moment you\ngive the data it'll process it real time\nso somebody clicked on that immediately\nthe data is here spark streaming will\nprocess it and say send an SMS or you\ncan take an action whatever you want now\nwhat are the other tools which can do\nreal time apart from spark streaming you\nhave something called fling you have\nsomething called\nstorm these are all realtime processing\ntools you might have to make a note of\nthat\nyeah sorry sorry yeah\nyeah correct so Kafka can be Facebook FL\ncan be WhatsApp because what WhatsApp is\npoint to point right one to one and\nFacebook is like distributed right\nanybody WhatsApp also they will keep it\nright I don't\nknow oh okay okay okay that that context\nyeah yeah correct correct good I didn't\nunderstand that point see so that's very\neasy so in WhatsApp it is push right flu\nis like WhatsApp okay once flu gets the\ndata it'll push it push it to somewhere\nit will not store the data Kafka means\nit'll get the data and store it for you\nas long as you consume it so in in in ic\nBank this was the architecture we got it\nthrough Flume send it to Kafka here one\ncopy will go to spark streaming so this\nguy will do realtime processing and do\nsomething okay one copy will go to hdfs\nso the data will remain in Kafka as long\nas you want you can configure by default\nit is 7 10 days you can also mention the\nsize of the data I want to keep only 10\nGB data 20 GB data there is something\ncalled topics in Kafka where you can\nmention how long the data should stay\nhere it's a cluster right we store the\ndata it can but it can do only one then\nit cannot send here right you're getting\nmy point Flume can send here then this\nwill not happen it is point to point\ndelivery I need a copy here I need to\nsend there also at the same time also\nprob tomorrow I want to build one more\napplication okay which will do something\non this data and that application I have\na new application I build tomorrow that\ncan also get the data from Kafka Kafka\nis like if you give data to Kafka\nanybody can take it from me I will store\nit for you got it Flume is like point to\npoint got it Kafka will store it so that\nanybody can come and get the data 10\napplications can read from\nKafka yeah\nsorry slow slow very slow hard dis to\nread I can directly push in real time\nright so before you get out of tanic\nshop I want to send an SMS\nright after processing no this will be\none golden copy will go here the\noriginal Facebook clicks and all golden\ncopy will go here that tomorrow you can\nanalyze now you clicked on a page\nInsurance page in Facebook I want to\nimmediately contact you so it has to be\nprocess processed and understood and\nthen some decision has to be ven so I'm\nsending one copy here one copy here here\nthe processing will happen here simply\nthe original data will be stored so\nafter data be deleted discard ah it will\nbe discarded because original data is\nanywhere here you can store it\nalso yes golden copy will be here it's\ncalled Golden\ncopy no no no map reduce cannot do real\ntime it is batch yeah\nyeah because how do you read from Oracle\nhow do you read from Oracle no no no no\nFL and Kafka cannot only scoop can read\nfrom uh SQL based stores now Kafka is a\nvery interesting thing I teach a three\ndays course only for\nKafka three days it's not enough\nactually to Kafka because it's such a\nbig topic actually because Kafka can be\na message\nCU elk C stack right no no elk stack\nwill not come here in this example uh we\nwe didn't directly use elk stack we were\nusing that's what I was telling in the\nbreak to somebody what we were doing is\nthat we used to store the data in Hadoop\nto process we were using something\ncalled Splunk there is a tool called\nSplunk Splunk can actually replace your\nelk stack it does elk both in one tool\nso that is for visualization right you\nhave some log files machine learning you\ncan just apply and visualize we were\nusing some something called Splunk so\nthis is the big pipeline I'm talking\nabout you can also build it using elk if\nyou want I'm just\nsaying elk is a stack actually elastic\nsearch log stash kibana it's a probably\nhe can explain\nright ah so who was working on elk\nsomebody was yeah so you can explain\nprobably what is\nElk lightweight we can inst\nbut probably not for 100 TB 1,000\nTB so let's say you're getting a lot of\nlog files you want to make sense out of\nit right from different different places\nlet's say I'm collecting all the logs\nfrom this network H I want to make sense\noff of it so I need a tool which can\neasily collect it clean it and then\ndisplay in a dashboard that is Elk what\nit is doing there is a tool which is\nvery similar that's called Splunk that\nthat is not in the big day in the Hadoop\necosystem it is not there that is a\nthird party tool\nactually open source\nyeah\nexactly no no no no that's part SQL only\nscoop exactly so that's a good question\nso when scoop is getting the data right\nscope will ideally push only to here\nhdfs okay but I can say scoop to push to\nKafka also that is also possible once\nthe data is available in scoop okay I\ncan say give it to kafa so either it can\nbe read for live processing or scoop can\npush to something called Hive we have a\nhive here which is a data warehouse from\nthere I can analyze in live if I\nwant but normally this transactional\ndata is not much use for realtime\nprocessing you are doing it based on\nthis Facebook clicks and all based on\nthis transaction I can get it now just\nto answer his question um that's a good\nquestion actually uh so uh can you uh\ngive me a suggestion so a customer is\nmaking a\ntransaction uh a customer is making a\ntransaction using ICC credit card he\nmade transaction for\n50,000 I have to immediately send him an\noffer so if he's making a transaction\nhere I made a mistake here I want to\nshow that if he made a transaction here\n50,000 okay where will be the data in\nOracle how do you capture the data from\nhere scoop you will say scoop we were\nusing Flume because your CDC will be\nproducing a log file the log will be\ncaptured by Flume not scoop so here\nscoop will not come into picture here\nscoop will come because I'm directly\nreading from CRM my CRM is in MySQL\nMySQL is original table and I can read\ndirectly but in the Oracle case is I was\nnot directly reading the table if I want\nto read a table scope but we were\nreading the CDC change data capture that\nwas a log file that was being collected\nby Flume actually so you can actually\nmake it so this is the answer if you\nwant to make real time okay you need\nsome CDC system which can push the data\nto flum\nactually huh\nexactly\nexactly real time uh I have worked on a\nbanking use case where so I'm talking\nabout this side spark streaming Flink\nstorm so spark streaming is what you\nwill be learning Flink and storm are\nother Frameworks which can do realtime\ndata processing so you will be wondering\nwhy realtime data processing is so\ncritical all are three all three are\nreal time you may use any of them fling\nis Apache storm is also Apache storm is\nalmost out of the market okay uh spark\nstreaming and Flink are popular right\nnow we have worked on a use case with\nCity\nBank you swipe your credit card right\nhow do you know it's a fraud or not\nyou're are swiping your credit card I\nwant to find out whether it is a fraud\ntransaction or a real\ntransaction correct\nexactly no even you don't have to be a\nfraud let's say I use a credit card\nregularly if I One Fine Day if I\nsuddenly pay in US dollars or something\nI'll get a call from customer care did\nyou really make it that means imagine\nHDFC Bank how many credit cards will be\nin circulation and how many people will\nbe transacting every day do you think\nsomebody is sitting and monitoring every\nswipe spark streaming you are collecting\nall the data all the credit card\ntransactions will get dumped okay they\nwon't come in a relational form they\nwill come in some flat file or something\nyour Flume will catch it the moment it\ncomes it'll push it through Kafka or\ndirectly to spark streaming or Flink or\nstorm and there I could have written a\nlogic Okay match this transaction if\nthis guy's limit is something or his\nlast uh transaction made places outside\nIndia or something trigger a mail to\ncustomer care that is how you're getting\nthis Emi calls also when you make a big\ntransaction immediately they will call\nand say do you want to convert Emi right\nthese guys are responsible for that the\nstock market stop ah same stock market\nvery big example yeah same thing because\nall real time right because tomorrow if\nI get a message for today's stop loss\ndoesn't make any sense right now since\nyou're interested in this I think in\nyour Spar course streaming is not there\nvery bad\nokay but I can give you some example\nbecause what is the difference the storm\ncan process one event also meaning One\ncredit card transaction storm can\nprocess ideally One credit card\ntransaction it will process minute spark\nstreaming can process only Bunch it\ncannot process one meaning it will\ncollect probably 2 seconds worth all the\ntransaction and predict I mean so that's\ncalled micro batching it is creating a\nbatch and then from that it is\nprocessing real time a near real time\nexactly 100% real time is only possible\nwith storm and fling also does the same\nI think I know there are lot of\ninformation I'm passing and lot of you\nare actually almost like dead by hearing\nthese terms and all but I can't really\nhelp it because I'm not making it up you\ncan Google for that okay and and\nprobably now it may not make sense if\nyou're listening to me but probably\nlater when you start working if somebody\ntalks about spark streaming and a use\ncase you will think ah okay okay oh this\nis what he at that so IC bank's use case\nwas that if somebody is clicking you\nthey wrote a spark streaming code okay\nnow I have a question for\nyou no no this Securities we were not\nhandling I'm not 100% sure how it is\nworking our use case was only limited to\nthis right but now I have a question\nokay uh the question is very much valid\nyou are making a credit card transaction\nand Spark streaming catches it spark\nstreaming catches it but that is the\nrealtime\ntransaction how does it know that this\nis a fraud based on the previous\ntransaction fine yeah the bat data is\nthere so but how it is working machine\nlearning\nthey could have built a machine learning\nModel H profiling all your past within\nhard\nonly SP Spar spark\nml\nneeds yeah they already have historical\ndata all these customers transactions\nhere these are all on top of hdfs only\nthis is running on top of hdfs only this\nis part of HD\nhuh when you install\nspark exactly Bas on your last so spark\nis getting installed on top of Hadoop\nonly hdfs is the storage for spark so if\nI'm making a credit card transaction\ntoday all my previous transactions are\nalready here they could have already\nbuilt a machine learning model and if\nyou simply feed this new transaction it\ncan predict otherwise how do you compare\nwith batch data I cannot run a batch job\nit'll take a lot of time right all the\ncustomer data so they will run a MLB\nit's called MLB in Spar machine learning\nmodel will be running that will easily\ncompare and say it's a fraud if you want\nto compare the batch\notherwise see now I'm just making a\ntransaction right it has to compare with\nmy previous transactions to understand\nwhether I'm a\nfraud so like he said I was traveling\nonly within Chennai for the last 10\nyears uh today I'm swiping in New\nYork h so New York swipe if you want to\nunderstand is an anomaly you should\ncompare it with previous swipe right now\neach time you cannot go and compare for\neach customer so they build a machine\nlearning model based on all this data\nthat can if you feed a data it can\npredict you know whether he will be a\nfraud or\nnot no no uh Kafka okay so Kafka will\nnot send spark will get because it is a\npublished subscribe system it is called\nspark streaming is an application which\nwill PLL a Kafka topic and get it from\nKafka it getk pulling the DAT from\nfine but you said on HTS\ncorrect no this is your permanent\nstorage Spar can read from here also\nfrom here Spar can read from a variety\nof\nsources never for theod of time huh like\none week by default so this is your real\ntime data yeah that's right I I'm just\nreading it to get my current information\nall my original information is going\nhere only right the golden copy is here\nso if tomorrow I want to run a spark job\nyesterday's data I can read from\nhdfs using the\nhtfs huh so this original data will get\nfrom here current data and there will be\na machine learning model which is\nalready built based on this data it will\nnot read the data the model will could\nhave already read and learned that is so\nyou guys have to teach me this you are\nlearning machine learning right not\nme you have to tell me how it is working\nright how would I I am a data Eng\nwhat is you machine learning right your\ntopic is machine learning\nright no no no no I'm saying in this\nhdfs you have all the transactions made\nby all the customers for last one year\ncredit card rep Ah that's a repository\nyou feed it to Sparks machine learning\nalgorithm it will build a model which if\nyou give any new data it can predict\nwhether it is fraud or not it not have\nto go here I didn't\nlast that you have to adjust in the\nmachine learning that's what I'm\nsaying's rate is available in the huh\nandk will me what my l\nandit no it will not directly compare\nand get this I'm talking only about\nfraud detection so your use case is your\nthis thing right uh stock market price\nvariation ah recommendation\nrecommendation\nhisorical price hisorical purchase is\nthere in today's price is there in so\nI'm comparing One Price what I bought\nprice that's okay so my point is this\nhistorical data will be huge it will be\nin terabytes right so you could have\nalready created machine learning model\nby feeding this data which if you give\nany new data can predict whether it is\nfraud or not that is happening in real\ntime I cannot say that once if somebody\nswipes a credit card I will tell you\nwhether it is fraud tomorrow doesn't\nmake any sense right I have to do it\nimmediately now the flip side is that\nall this requires resources nothing is\ngoing to work if you don't have\nresources so you need to give enough\nresources for all these things to work\nmachine learning and all are very\nresource hungry CP power ram power right\nso this cluster should be capable of\ndoing this anyway for the time being\nlet's move on from Kafka right so we are\njust taking a use case right now I'll go\nhere once your data is in hdfs if you\nwant to do batch processing the default\nframework is map reduce\nMr what is map reduce we will see\ntomorrow but I have my data I want to do\nbatch processing I don't care how much\ntime it takes one day can take I just\nwant to process the data I can use\nsomething called map reduce right you\nalso have one more guy called\nPig this is uh almost gone from the\nmarket pig pig the animal\npig pig is a scripting tool on hadu but\nnot very popular now because of spark so\nnobody is actually so even if you're not\naware of pig is perfectly fine I think\nokay you don't have to worry and then\nyou have somebody called Hive this is\nthere in your\nsyllabus so Hive I will teach in the\nnext month but basically what Hive\nallows you to do is to write SQL on top\nof hadu but the drawback is you say\ncreate table and select count star from\nthe table hit the query it will convert\nyour query into a map reduce program so\nit's a translator it is not a native SQL\nengine yeah if I'm writing an oracle\nquery Oracle will execute it for me if I\nwrite a hi query map reduce will execute\nit for me\nso you will understand when we speak\nmore about Hive don't worry\nso huh c c C++ exactly so map reduce\nitself is very slow because this is\nbatch processing in hi you write a SQL\nquery and hi will not execute your query\nit will write a map reduce program for\nyou even more\nslower to spar hi to spark can be\nconfigured yeah so but the problem is hi\nwas the original tool it was was built\nin 2005 by Facebook okay and at that\npoint it was very popular because there\nwas no other tool right so people were\nwriting High queries even though it is\nslow it is equal right see which is\nbetter you can suffer some slowness or\nyou have to learn Java you can suffer\nSQL\nright ah right so\nETL 15 minutes it is fine yeah so\nFacebook developers thought that instead\nof learning Java right we will learn we\nwill stick with SQL ETL jobs we will run\nthey created this tool called Hive so if\nI'm a hive developer I don't have to\nlearn anything it's SQL I say create\ntable create table join but once I say\njoin it'll write a map ruce for me then\nthe map produce will run I may have to\nwait for in fact in Flipkart there was a\nhive query which took around 12 hours to\nrun today they will fire tomorrow\nthey'll get the result 12 hours was the\ntypical runtime a huge amount of data\nobviously but then they find it and all\nbut that's a different story but so lot\nof time batch batch processing so these\nare all batch oops these are all batch\nprocessing systems built on top of Hado\nright and then you have spark you will\nlearn spark later I don't want to\nconfuse you a lot so spark is\nthere and this guy is inside spark okay\neven though I wrote it here this guy is\ninside spark spark is an inmemory\nexecution Engine spark is again\nclassified as a batch processing system\nbut very very fast than map reduce only\ndifference is that spark programs are\nfaster than map reduce\nprograms near real time not real time so\nspark streaming is actually real time I\nwould say near real time not like 100%\nreal\ntime there'll be a buffer always like 1\nsecond 2 second you can Define that and\nit'll process like that I cannot have\nmicroc real time that's not possible\nbasally ah so yeah so Mr and pig are\nobsolete Hive is not obsolete because\nHive is your data warehouse where you\ncreate tables and store the data so this\nguy will remain but now what is going to\nhappen spark will connect with\nHive and Spark can\nwrite yeah so here you can write a SQL\nokay spark has a spark SQL Library it\ncan read tables from hive and run on top\nof that right so spark will talk start\ntalking to Hive so Hive is not really\nobsolete hi will be there for some time\nwe can say no no hdfs storage is hdfs\nonly Mr there is pig is gone no there is\nnothing pig is\nalmost pig is a scripting to so Pig also\nwill convert your query to map\nruce\nobsolute Spark spark only not new\nversion spark has practically replaced\nPig\nactually so this is your batch\nprocessing Frameworks right now some\nother guys came and they said that so\nthat was a typical question that asked\nduring the training so if I want to run\nokay again there is a confusion if I\nwant to run a very big group by query\nlet's say I want to read the Full Table\nscan and run a query Hive is great even\nspark SQL with Hive is great because\nthey will read the whole data and\nprocess it but if I have minute queries\nlet's say select uh something from the\ntable where name equal to\nraguraman only one row it needs to\nprocess for that why should I load the\nfull data and process it is useless but\nHadoop says load everything in\nsequentially and then process that is\nonly possible that is where you have\nthis MPP engines Impala there is one guy\ncalled Impala there is another guy guy\ncalled Hawk\nHawk and\nLP there is something called\npresto presto there is also something\ncalled uh what is\nthat the\ndrill\nPhoenix these are all MPP engines just\njust be aware of the name you don't have\nto be an expert on like there is no\nchance where your manager will ask okay\nname the top five what you say MPP\nengines or you fired nothing is going to\nhappen like that any one of them will be\nused actually Impala is claa specific\nclaa created Impala so it will be mostly\navailable only on cloud era what Impala\ndoes is very simple it has its own\nexecution engine you fire a SQL query it\nwill run it for you no map\nreduce right so what is going to happen\nfor these queries like where name equal\nto raguraman you can fire an Impala\nquery it will scan only that\ndata right so it can connect with hdfs\nand pick only the data because it\nremembers the metadata where is what\ndata available it can ah columnar it can\npick that data and query but then you\nwill be wondering why can't we use this\ninstead of\nHive if and these guys are very fast so\nif these guys are too fast then why\ndon't I use them instead of Hive they\ndon't have\nreliability they are all inmemory\nqueries if an Impala query is running in\nmy hard cluster if any one of the\nmachine crashes my query will be gone if\na high query is running even 10 machine\ncrash query will complete it is map\nreduced buil-in redundancy is there map\nreduce will never fail or spark will\nnever\nfail\nyeah\ncorrect no no no no it is loading the\nwhole data into memory then only\nanalysis happens\nright going\nto name something\ncorrect no no no before so when you say\nSelect Staff from this table first that\ntable data will be loaded into the ram\nthen only it reads from there right so\ninitially the whole data is\nloaded exactly that's the problem but if\nI'm using Impala okay it has its own\nmetadata it knows where is the data it\nloads only that and queries will be\nfaster so if I run an Impala query\nconsidering a hi query Impala queries\nare much much faster but no fall\ntolerance so for this very fast queries\nlike your queries can take like 2\nminutes 3 minutes time you can afford to\nlose a query useing Pala but if I'm\nhaving a ETL job where I don't want to\nwant the job to fail run it using Hive\nor spark SQL because these guys will\nnever fail\nonly it is distributed it can run on N\nnumber of\nmachines okay Sy gets down it query will\nbe gone because there is no I will show\nyou tomorrow in app reduce we will\npersist the data intermediate results\nthat is how it is fall tolerant Impala\nis all in memory so if a machine crashes\nthat data is gone what was in the ram\nquery will\nabot so all MPP engines are like this I\nthink in the traditional World also in\nthis the difference is Impala is on\ncloud era Hawk and LP are Hots so there\nis a competition Impala was originally\ncreated by Cloud era so they promote it\nif you're in a cloud era platform you\nwill see Impala if you're coton Works\nplatform you will never see Impala they\nwill promote haulk they will say Hawk is\nthe best engine okay and LP is the new\nversion of Hawk Presto drill and all are\nsimilar ones but not by default\navailable this guy is very very\ninteresting so there is something called\nhbase I will add it here somebody was\nasking me what is hbase hbase is your\ndatabase of hadu no SQL database\nrealtime database real real time because\nhbas will get installed on\nhdfs that is your storage right but it\ncan do random reads and writes not like\nentire block so normally when you are\nreading the data from hdfs you have to\nread the whole block right but hbas has\nan API so if you install hbas on top of\nHadoop it can do random reads and wrs\nindividual records it can read and write\nand process because it is a nosql\ndatabase of hadu the default no SQL\ndatabase but the drawback is hbas has\nits own language so if somebody has to\nwork with hbas you have to learn the\nlanguage it will not allow SQL queries\nphix is SQL on a\nspace Phenix and hbas is\nlike Roti and paner butter masala deadly\ncombination I mean very deadly\ncombination because you writing SQL on\ntop of\nnon-sql right very deadly so so they say\nthat in another two years it will not be\nslow is really fast because hbas itself\nis fast SQL there is a conversion okay\nbut that's already built in any SQL\nquery there are some limitations but\nmost of the\nSQL no no no it will not fail so finix\nwill get all the metadata and already\nall the queries are converted whatever\nquery SQL you are writing it knows what\nis equivalent hbas query it has so it\nhas just push the query so you just\nwrite a SQL it'll run on hbas and give\nyou the result and it can do things like\nsecondary indexes and all it can index\nyour data apart from HB it's very fast\nactually now the real news is that in\nanother two years what is predicted is\nthat this combination the drawback is\nPhoenix is still in Alpha Beta Phenix is\nnot production ready once Phenix becomes\nproduction ready this is oil\nTP it's oil TP that means Hado will\nprovide you olap and O\nTP huh\nit it will support in future not now so\nthey are trying to make it acid support\nso that this entire combination become\nan oil TP system like that right so then\nwhat will happen you will get\ntransaction Management in Hadoop so far\nit is not there another two years down\nthe line they say that so Phenix is\ngetting integrated with hbas originally\nhbas was only there the drawback is it\ndoesn't understand SQL so Phoenix\nstarted Phoenix is a layer on top of\nthat you can write SQL so in another two\nyears they are developing this as as a\nsystem for transaction management so\nyou'll get true transaction Management\nin\nHadoop uh so yeah so if you talk about\nthe clera and all if if I'm going to\nclera and say that I want your product\nthey will give you all this in one\npackage but if you go to Apache and\ndownload you get only hdfs map ruce and\nYan only Hado all this you have to\nseparately install so commercial\ndistributions will package it and you\nwill not get everything okay for example\nHawk will not be there this Presto drill\nPhenix you will get hbas Hive Pig map\nreduce spark Impala so these things you\nwill get you have a doubt I'll show you\nthat uh in the cloud\npage yes there is something called spark\nSQL where it runs SQL query it is really\nfast\nactually that's what the tables you will\nbe creating in high Spar doesn't have\nstorage right so whatever tables you\ncreate there uh it can connect and run\nso I'll say cloud CDH\nthe HB has its own processing engine its\nown processing engine so this is cloudas\nproduct why I'm showing you this to tell\nyou that I'm not blabbering okay you may\nbe thinking that this guy is talking\nabout a lot of tools do they really\nexist in the world look at\nhere look at here spark High Pig map\nruce spark Impala uh yarn right what you\nhave hdfs hbas Kafka Flume\nscoop this is a real product from clera\nyou get go to claa this is what you get\nsome of them I have not even explained\nbecause I don't know kite I don't know\nwhat it is to be very honest Kudo is a\nnew file system relational even I'm not\naware of kudu I don't know what it is\nbut these are like new entries you even\nif you don't know them you will work\nwithout any problem and for security you\nhave something called Sentry so a lot of\npeople are wor worried about security on\nHadoop right so there is a service\ncalled Sentry which will handle like\naccess management authorization\nauthentication all this will be handled\nby Sentry we covered these things right\nthese things we covered this we covered\nthis we covered uh something extra I\nhave wrote I mean that is okay I mean\nhere this part right this prestor drill\nPhenix and all is not there but I just\nwrote\nthem one thing which is not present\nthere but it comes is Uzi\nthis I told you right this is the\nworkflow scheduler like if you want to\nschedule a workflow and all uh you can\ndo\nthat I think we can do one thing uh we\nhave half an hour left and you are like\nalmost tortured I'll show you a cloud\ncluster how about that that'll be better\nthan talking a lot so I keep on talking\nall this right so I'll show you a cloud\ncluster uh we will not do the handson\ntoday uh in the cluster we are using\nusing for the lab we have only readon\naccess from the admin side which means\nyou cannot change the replication factor\nor block size or anything these are all\ndone by admins we can upload the data\nand then say that run the program that\nwill work but that we will do tomorrow\nbut let's look at the cluster right so\nso far we are talking cluster cluster\nand we are not seeing any cluster so far\nso I\nthink cloud hoton works and uh are these\nthree are getting used actually both\nboth all the three companies are sort of\nfamous I can\nsay right okay so do you have this in\nyour LMS like your lab and\nall just check\nonce\nyeah so um because some of the things\nare a bit confusing so I have to tell\nyou we have to use use web console web\nconsole will connect with the uh Linux\nfile system where Hadoop is installed we\nwill do that later Cloud era manager is\nthe admin interface of the cluster so I\nwill connect with it but we will not be\nable to modify but we can see stuff so\nif I go to Cloud\nera no no Cloud era manager is a utility\nthat comes with Cloud era to administer\nthe cluster like you want to add a\nservice remove a service add a machine\nremove a machine so the administrator if\nyou are a Hadoop administrator you have\nto do all these things add users remove\nusers permissions everything can be done\nby Cloud manager and why I did not speak\nabout this because we will not be doing\nanything using this it's an admin\nutility but to look at the cluster this\nguy is good so let me show you that\ncloud\nmanager now you have to install it so if\nI'm running in a company I have to\npurchase let's say 20 machines\nphysically yeah and then I have to there\nis a process of installing Hado this\ncluster already has installed it is a\nrunning cluster if you're creating from\nscratch you have to purchase let's say\n20 machines and download something\ncalled Cloud manager and run it and it\nwill help you to set up the cluster it\nwill tell that which is your master\nwhich is your slave what is your block\nsize what is like if I don't have I\nuseal cloud also this is physical this\nis not even physical this cluster is\nactually running on AWS Amazon web\nservices huh\npractice you will have uh access to this\nright I mean if you want to practice\nthis setup is already available to you\nright giving some bad request try to Lo\nuh let me try to log in okay just\nprobably you don't have access I\ndon't yeah so just pay attention uh uh\nso this is your Cloudera\ncluster okay this is just an admin GUI\nokay to the cluster and once you login\nthis is your dashboard so you are\nlooking at a Hado cluster first point\nyou have to understand second Point what\nis there in the Hado cluster these are\nall the services installed see\nhdfs why am I talking like this I can\nsee it here right okay hdfs Hive Hue I\nwill talk about it Impala Kafka Uzi\nsolar Spark yarn zookeeper all these\nservices are running in the cluster you\nsee a green right this is just to get a\nquick overview of the cluster like what\nis running in the cluster fine how many\nmachines are there in the cluster you\nclick on this host\nmenu can you\nsee can you see these machines these are\nall machines one1 right so how many\nmachines are there I think there are\nnine machinein see good health n nine\nmachines are there in the cluster and\nyou can also see their configuration if\nI scroll\nhere can you say there see their RAM and\nhard\ndisk what is the physical memory that is\nthe ram the first two machines are first\nfour machines are 8 GB Ram next one is\n16 GB Ram these are their hard disk can\nyou see and you can see how much storage\nis full and all everything right see\nhere so this guy is having 32 GB RAM and\n18.8 is currently used some process will\nbe running this guy also had a 249 GB\nhard disk 600 GB hard disk 50gb hard\ndisk you can all see from a single\nwindow it's very easy so it is not as\ncomplicated as you thought by the way\nright so this is just the host menu\nright uh you can\njust uh from cloud manager this is the\nhomepage click on host host you will see\nthis\nright uh now if you look at hdfs this is\nhdfs click on hdfs\nokay you will see this hdfs menu this is\nall related to hdfs in hdfs what we\nlearned replication block size so where\ndo you configure this there is something\ncalled\nconfiguration and do you want to change\nthe see it is saying read only I cannot\nchange it but do you want to see the\nreplication factor of this cluster DFS\nsorry DFS\ndo what is the application factor three\nclick there you can change it to two or\n10 or whatever you want if you are an\nadmin you are not an admin do you want\nto see the block size DFS do block\nsize can you see so I can click here and\nchange if I'm a hard admin and it will\nbe applicable to everybody immediately\nand all uh somebody was asking about\nbalancer right where is balancer I can\nsay actions okay uh the problem is that\nsome of these things I will not be able\nto run\nwhere\ninstances H this is balancer where it is\nrunning uh but I can run a command huh\nsorry it's not it's not green it is not\nrunning if it is running this process\nwill come as green\nactually uh then chart commands\nconfiguration um so there is a lot of\nconfiguration okay so that is one thing\nwe saw and and\nuh host menu we saw already right so so\nfar we discussed only hdfs and here is\nKafka if I go to Kafka me just go to\nKafka okay Kafka says Kafka broker three\nthat means three machines are running\nKafka I told you Kafka will be running\nin a cluster since this is a lab setup\nwe are running Kafka inside Hadoop only\nideally we run it separately because it\nneeds storage spaces to hold your data\nbut here Kafka Brokers are three we are\nrunning inside here\nonly because this is only cluster we\nhave I mean these machines if I go to\nthese machines I will show you if you go\nto this host menu you can expand a host\nand see what it is doing for example if\nI expand this\nguy see what is it it is your name node\nright so and also I can go to hdfs and\nsee that for example I go to\nhdfs and\ninstances what do you see\nhere active standby active standby two\nmachines two name notes I told you right\nactive and standby they are running\nright so the same machines are running\nKafka here it's not a separate cluster\nby the way that's what I meant by that\nuh I don't have really admin access\notherwise something could have been done\nI don't want to mess up the cluster also\nby the way as so some of the menus are\nactually disabled we can't run them\nsee exactly ideally that should not be\nthe case because then it is you not\nutilizing the proper space right so\nideally you will set up a separate\nbroker and that will download the data\nactually based in\nus they are all running on AWS\nactually I don't know I mean I don't\nhave access to that can be anywhere\ndoesn't matter aw you can create cluster\nanywhere I I don't have any idea as to\nwho created this cluster and all it's a\nthird party they just give access to us\neven I have only read only access like\nyou you guys have now uh this is to just\nbrush up for tomorrow tomorrow we'll be\nactually doing it but I want to clarify\none more thing uh if you go to this big\ndata Cloud lab this cloud data manager\nyou don't really need because that's the\nadmin interface you're not going to do\nany admin activity the important thing\nthat you need is web console so if you\nclick on web\nconsole I just clicked on that for some\nreason it is not opening up let me just\nsee do you guys see\nanything why am I not getting login you\ncan log in but if I have to show I have\nto log in\nright so where is\nmoini she's not\nhere yeah you can log in that is that\nwill connect you to one of the machines\nin the cluster and you will see Linux\nyou can just log on to one of the\nmachine that is what you see there and\nI'm am not able to log\non you're getting the Linux\nshell take some time no I'm taking\nlong long time and there is a there is a\nproduct called Hue this is very\nimportant there is a product called Hue\nthis Hue is a third party\napplication okay but it is very very\npopular why do you need Hue is that if\nyou're a developer and you want to\nupload your files to Hadoop you can use\nHue one of the use cases for example I\ncan click this I'll click Hue and last\nthe username and password if I sign in\nokay I'll just sign in if I click on\nthis hdfs browser see\nthere this is my Hadoop these are all\nthe files in my\nHadoop so hu is like a web UI to your\nHadoop hdfs file system so these are all\nthe files and folders if you connect you\nwill not see anything here because you\nare having a fresh account one more very\nimportant point I want to tell you is\nthat in Hadoop you will have a home\ndirectory user SL something I have GL\nfaculty you will have your own directory\ncan you try this Hue thing on your PC\njust click on this Hue type username\npassword see whether you can see this\nthing are you able to see\n[Music]\nanother point is that sometimes the\ncluster will become very slow because it\nis not designed to accept request from\n36 people right if all of us try to log\nin it'll say timed out and other\nthings\nno what username and password you're\ntyping empty that's fine because in your\nHado Home D\nthere are no files I have already run\nsomething so that is why these files are\nthere so we will create it tomorrow\ndon't worry we will create it using\nusing the same data notes individually\nfor our own file man huh the same\ncluster so different different users are\ncreated in the hard cluster so if you\nlog in you will get a home directory\nlike in Linux in Linux what happens if\nyou log on you will get a home directory\nso this is my home directory in hdfs and\nsince I already created some files I can\nsee them you may not be able to see\nbecause it is a fresh uh home directory\nfor you tomorrow I will show you how to\nupload the file run a sample map ruce\nprogram explain the program and all we\nwill do that tomorrow don't\nworry so there was some unanswered\nquestions I said I will pick it up later\nI forgot also anybody had any questions\nI\npushed did I tell you I will show\nsomething and I forgot also some of the\nthings\nhuh\ncorrect no it will be uh in a non-c\ncommodity Hardware like a real server\nhuh the problem is I'm not able to\nconnect with the web console see it is\nnot giving me option to\nconnect uh I need to connect to that to\nshow you something\nhu\nKafka Impala is running\nhere yeah uh okay I missed the concept I\nwanted to talk about it it's a very\nimportant concept right uh do you know\nwhat is a rack\nyou know what is a\nrack server\nrack rack is like a rack right how do\nyou explain what is a rack even I don't\nknow how to\nexplain Dev H servers and\nall because from the point of view of\nHadoop this might be important sorry I\nwill explain this one and then pick up\nyour questions images.google.com\nso this is for people who do not know\nserver rack\ndo you know what is a rack\nnow this is a rack it's a housing where\nyou push all the servers in all the high\nfive movies you will see somebody\nsitting something typing something in a\nvery big data center and all right so\nthat's a rack why racks are important\nthey are important for one uh reason\nbecause can I erase this this\ndiagram this one\neven I don't know what I\nwrote that's\nfine so why racks are important is\nbecause of one particular reason\nsee let's say you have three racks\nyou are having three racks okay and each\nrack has a data node not one there are\nmany so let's say two data notes are\nthere so uh in reality when you are\ninstalling Hadoop in a cluster this is\nhow you do right because in a data\ncenter you will have racks and you will\nhave servers the servers are called your\ndata nodes right so I have six data\nnodes in three racks now there is\nsomething called rack awareness in\nHadoop meaning if let's say you are\ncopying a block what you can do this\nfirst copy will go\nhere the second copy will go here\nimagine and the third copy will go here\nmeaning\nif you enable this feature called rack\nawareness Hadoop will ensure that the\nblocks are spread across multiple racks\nwhy yeah a rack can fail rack is a point\nof failure because normally you will\nsupply this power and network everything\nthrough through rack so if a rack fails\nuh you know if I'm keeping all the three\nblocks in the same rack if that rack\nfails then my blocks are are gone so\nHado can if you enable rack awareness\nyou have to tell Hado which data not is\nin which rack if you do that it can\nplace them like this but why don't it\nkeep like all three in three different\nracks why the uh rest of the two copies\nare in the same\nrack yeah but same location means these\nare all in the same data center right\nhuh so this inter rack bandwidth is very\nimportant rack to rack the bandwidth is\nvery important so uh this is just\nreplication right you will be running a\nlot of programs which will use this\nbandwidth so to save that bandwidth what\nit does is that the next replication\nwill happen within the rack server to\nserver but this is enough now even if\none rack and all the three racks going\ndown is very remote possibility right\nand my point was I can show you that\nhere so when I saw that if you go to all\nhost can you see\nthis so in the rack one four are there\nrack two two are there rack three two\nare there uh and default rack two are\nthere default means I don't know they\nhave kept something here but 422 so this\nis actually the\ndistribution it is actually available in\nyour cloud data\nmanager Sy no no no no no each rack is\nnot a system what you going\nto not one many will be in one rack\nright see\nhere all the Masters will be in a\nseparate rack see if you look at this\npicture right in this machine some 30\ndata notes will be\nthere each slot you'll have one server\nthey are all rack server so server is\nnot like this it is like this like a\npizza you get a pizza how does the p\nlooks like that is how you a server will\nlook like not like\nthis there is also that kind of server\nTower servers are there but these\nservers typically we use are these kind\nof servers I had a picture actually uh\nYahoo Hado cluster at\nYahoo\nHado cluster at\nYahoo\nH so the biggest Hado cluster on Earth\nis with Yahoo\nactually Yahoo holds a record for\nthat they have 42,000\nmachines and still they went bankrupt\nsold it to some useless\ncompany so the picture is not visible\nactually but you got the idea this\nis no that's a good question so can I\nhave multiple name nodes yes the concept\nis called Federation meaning ideally if\nyou cross more than 5,000 data nodes you\nneed one more name node like 42,000\nmachines I cannot have a single name\nnode to handle because the amount of\nmetadata will become slow that is a\nproblem so ideally Yahoo has done some\nR&D they said that if you cross more\nthan 5,000 data nodes you need one more\ndata node that's called name node that's\ncalled Federation Federation means multi\nmultiple active name notes like you can\nhave 10 name notes all will be working\nthey'll share the load actually right so\nlike that Federation is called\nit yeah\nsorry all in one rack actually you can\nput inside the same data center only you\nneed all of them right because all the\ndata notes are in one landan so the name\nnote also should be inside that only but\nthey will handle the load actually you\nknow so some data nodes will be not to\nthis name node and all they can handle\nload sorry just a\nmoment file server can store but it\ncannot process right that's the use case\nHadoop is not only for storing whatever\ndata you store the data can be processed\nthen and there by the machine\ncorrect\ncorrect huh so there we are not using\nHadoop right I we discussed it in the\nbeginning of the class now SQL databases\nif you don't want processing if your use\ncase is such that I just want to dump a\nbunch of images let's say 1 million\nimage just I just want to process and\ndisplay in an e-commerce website I don't\nuse Hadoop Hadoop is not used for that\nthere you have no SQL databases like\nDynamo DB or mongod DB\nright SharePoint if it is a smaller\nsetup SharePoint you can\nuse which\none\nhastac uh Haack is built on top of this\non top of this yeah Facebooks right\ncorrect correct so it is all business\nuse case the question is that what do I\nwant to do with the data there are other\ncompanies who generate terabytes of data\nthey just archive the\ndata I just generate data I don't want\nto analyze I just archive the data who\ncares so it is up to me to decide what\nshould I do with the data right\nfor huh I was working with GE very\ninteresting use case GE right the\ncompany called GE I have a training next\nweek there so they have an aviation\nDepartment G is building this aircraft\nengine you know that flight engine 80%\nof the commercial aircraft engines are\nfrom GE and what they do so most of the\nplanes are flying with G engine there is\nalso Rolls-Royce and one more company so\nall these aircraft engines are having\nsensors so when the flight is flying\nthey capture the sensor data like\npressure and blah blah blah and they\nanalyze it to predict when the flight\nwill crash or turbulence will hit or\nsomething like that whether you're going\nto die or something so the idea is that\nbut so I said okay good deal you're\ndoing this they said no it's not a good\ndeal the problem is if a flight is\nflying from let's say Dubai to\nBangalore there will be two\nengines it takes 6 hours each engine\nwill generate one TB\ndata how many flight are there in total\nand how many flights fly every day can\nyou imagine the amount of data they\ndon't analyze how can you analyze I mean\nif I'm giving you let's say th000\nterabyte every hour or every minute you\ncan't analyze it so they take a subset\nof that they have an algorithm which\ntakes a subset of the data and analyze\nrest they delete and dump because it is\nnot humanly possible also you don't need\nthat much amount every sensor data need\nnot be analyzed these very interesting\nbecause they also have locomotive\nengines train train all the Europe and\nus they run trains and these trains are\nhaving sensors so they get also that\nit's very difficult to work with G\nbecause huge amount of data they also\ndon't know what to do with the data\nright and they have no clue how to say\nmake sense of the data locomotive data\nis huge actually so the trains will\ngenerate a lot of data actually so they\nget it uh and then they take it and\nclean it and just get only a part of the\ndata and they analyze it in Hadoop you\nalso have certain file format\nfor example AO you may not have heard\nabout it AO is a file format we use only\nin Hadoop Park orc these are all used to\ncompress and store your data in hadu you\ncan also have compression techniques\nbecause sometimes storing the data as it\nis is not good why don't you compress it\nbut what is a drawback of\ncompression you have to decompress you\nneed a lot of processing power if you\nwant to decompress it so AO is a\nserialization format it doesn't compress\nthe data par and all will compress the\ndata column and push the data so I will\nshow you in Hive when we discuss Hive\nwhen you create a hive table you can say\nthat the data has to be compressed it\nwill compress and store the data\nactually that is\npossible so my point is not everybody\nwant to analyze all the data in the\nworld that is what so humanly not think\nabout Facebook how much data they'll be\ngetting probably they're not analyzing\nthe whole data as subset of interesting\ndata that is all they will be\ndoing so any questions we'll wind up at\n6:30 I think but before that we can pick\nup\nquestions\nhuh yes spark is memory intensive\nideally spark requires Ram so if you\ngive Ram it will perform better if you\ndon't give Ram it'll use hard disk I\nmean rest of the space use hard disk\nperformance will be slightly\ndegraded so for streaming and we need\nlot of ram the problem is uh football\nWorld Cup is going to happen right yes\nright\nwhen this year only right so let's say\nyou want to write a program which will\ndownload all the tweets and find out the\nmost favorite player for football World\nCup possible right so I can easily\nstream the data from Twitter the problem\nis I want to find out\nuh you know the most popular player\nevery five minute imagine probably not\nfive minute you want to do so I want to\nget five minute for data hold it in my\nmemory and process what if five minute\ndata is so huge I cannot hold it in\nmemory then it'll be\nslow yes I'm just giving an IM imaginary\nsituation where if five minutes for data\nis in terabytes you can't hold it\nactually not only tter any data that you\nhave then you might want to persist into\ndisk some of the data then processing\nwill become slow it is not real time\nthen so when you are running spark\nstreaming and all you know your RAM\nshould be available otherwise it will\nbecome very\nslow today's match you will predict\ntomorrow right so things like that can\nhappen so that's not really good\nactually so a lot of considerations are\nthere where you have to use all these\nthings so I think we will wind up what\ndo you\nthink yeah so it's almost 6:30 and also\nthat you to is SL yes so I will be\nsharing slides also books so uh there\nthere are a couple of good books written\non Hado which will give you a lot of\nidea about Basics like how hdfs is\nworking or all these things actually\nmuch we need to understand to\nbasically um so I would say whatever I'm\ntraining here is enough but go through\nthe book also say for example hdfs you\ndon't have to to be an expert on high\navailability see there is an active name\nnode and standby name node if the active\nphase standby will take over how that\nhappens there is a process so probably\nthat topic even if you don't know it is\nnot a big deal because that's an admin\ntopic if you are a Hadoop admin you\nshould know how that should work but\nsince you are a developer you don't have\nto go in depth into those topics\nactually so map reduce And Hive should\nbe more than enough I think and you are\nbuilding this awareness to learn\nspark the big basic idea is that you're\nbuilding this so you can look at spark\nso I will say concentrate on spark so\nlet's talk about Hive right which you\npeople might be mostly interested and\nI'm not so usually I'm not a slides\nperson but for Hive I need some slides\nthis has around 60 slides I will not run\nall of them don't worry I'll share it\nwith you you can go through that I'll\nrun couple of them to get an idea right\nand so that otherwise everything I have\nto draw and explain it will be difficult\nfor Hive right so Hive is actually very\nsimple uh in 2005 when Hadoop came most\nof the major vendors started using haduk\nlike Facebook Yahoo and all uh but they\nall faced one common problem the problem\nwas that uh Yahoo started using Hadoop\nuh but Yahoo had a lot of developers who\ndid not know Java so the problem was\nthat they had to write map ruce programs\nand uh skills set was the problem you\ncan't say that everybody should learn\nJava and start writing code so Yahoo\nfound it very troubling same time\nFacebook also started using Hadoop and\nthey also had the same problem Facebook\nhad mostly SQL developers and they were\nnot comfortable with Java and way back\nin 2005 and 6 we had only map reduced\nthere was nothing else on Hadoop okay\nplain map reduce nothing else uh so the\nHadoop ecosystem we saw in the last\nclass none of that was available so\neverybody was confused so Yahoo and\nFacebook started two projects same time\nuh so Yahoo created something called a\npig the creature pig animal\npig seriously it's called\nPig pig. apache.org\nsee that's a pig real pig creature so\nYahoo created this tool called Pig it's\na scripting language I teach a course\ncalled a pig on Hadoop only on Pig it's\na very interesting language scripting\nlanguage uh Advantage is very simple you\nlearn this language uh and then you\nwhatever analysis you want you write in\nthat language it's a script so very\nsmall like four five lines Max or 10\nlines and once you write your pig script\nyou say run it will convert your pig\nscript into map ruce program so you\ndon't have to learn map ruce you learned\nPig and somewhere in 2010 and 11 there\nwas a heavy job inflow for pig\ndevelopers 80% of Hadoop job was handled\nby Pig in 2010\n80% it was such a successful language\nthat everybody was dying for pig I have\ntaken only Pig trainings people don't\nbecause that was the effect of pig\nactually right because it's a very easy\nlanguage scripting language you just\nlearn it and everything will happen\nbehind the scenes you want to do a join\noperation you say join a comma B that's\nit even SQL queries are more complicated\nin join you have to say a dot b dot here\nnothing take a b join a comma B done\njoining done simple inner join I'm\nsaying so people started using Pig\nextensively but at the same time your\nFacebook developed Hive hive's idea is\ndifferent you write sequel okay you\ncreate your databases your tables and\nall the blah blah blah you write SQL and\nhi Will convert your SQL query into map\nreduce same concept so two companies\ndeveloped one Pig and one Hive they\ndidn't know that they were doing the\nsame uh thing\nultimately but then what happened\nsomewhere around\n201101 Pig started losing the attraction\nbecause spark came so when spark came\nand became predominantly useful spark is\nalso very short and like a script you\ncan write in Python and all very easy to\nwrite so spark and pig are almost\nsimilar so people thought like if I'm\nlearning something very short and\ninteresting and very fast why should I\nlearn Pig I can learn p uh spark itself\nright so pig is no longer used by\nanybody very rarely pig is gone almost\nHive is not gone because it is SQL SQL\nwill be there till the world exist right\nno questions asked right no questions\nSQL is SQL so you can't get rid of SQL\nright how do you get get rid of SQL what\nwhat if all the ATL developers and you\nknow there's a huge Community to back\nSQL not like Pig I actually like Pig\nmore but there is nobody to back Pig\nright there is a lot of people who back\nSQL right and and that is not the only\nreason hi is a beautiful data warehouse\nit is treated as a data warehouse of\nHado meaning you can create tables and\nuh you can query the data so most of the\nvisualization tools and all uh you know\ncan use Hive in the back end to fetch\nthe Rell and all so Hive became very\npopular and now what is happening Hive\nis there uh plus spark is using Hive in\nspark you have a module called spark SQL\nthat uses SQL language uh now that guy\ncan process itself it can also talk to\nHive so whatever data you have in Hive\nspark can also read so Hive is\npredominantly used these days and hi\nwill always be very uh popular uh it may\nbe I think one of the most popular\necosystem tool is Hive so far in the\nindustry because it's been 10 years 12\nyears and still uh uh Community is very\nmuch uh so the the um command wise it is\nvery simple because it is SQL and I\ndon't think I have to teach you SQL\nthat's not the idea behind the session\nyou will already know SQL but we will\njust see what is happening behind the\nscenes if you write a SQL query uh and\nsome of the advanced concepts Hy like\npartitioning bucketing indexing uh these\nkind of things and from the industry uh\nexperience there is one major thing uh\nthere are two major vendors hoton works\nand claa right now hoton works uses\noriginal Apache Hado that is their\npropaganda so what Hive is also Apache\nproduct it's open source right and\nwhatever major changes are there in hive\nare predominant in hoton works not in\nclaa claa is actually not so good at hi\nwe are using a claer lab that's bad\nactually I mean I'm not blaming the lab\nbut claa is actually a bit bad in hiide\nI'll tell you the reason\nbecause I I teach at hoton works that is\nthe reason I'm able to tell uh in hoton\nWorks they have a project called\nT some weird people from Delhi created\nthis project T is a Hindi word there is\na weird movie by that name also right\nhave you heard t t t speed in\nHindi\nhuh huh so T is a uh how do you say it's\nnot a replacement the is a project\ncreated by Indians then they gave it to\nApache it became open source th is\nfaster map ruce so what these guys did\nnormally in map ruce what happens you\nread the data right and uh then you do\nmapping and then you store the output in\nhard disk again Shuffle you read the\ndata do the shuffle store the output\nback in hard disk again reducer will\nread from hard disk so some weird guys\nthought why don't we do all this in in\nmemory we don't want to use hard disk\nread the data once do all the map reduce\nShuffle and then push the result so your\nmap reduce becomes 10 times faster that\nis T so it's a dag a directed a cyclic\ngraph concept and th was given to Apache\nwhat henbs does if you write a hi query\nthey will convert it into a taze job not\nmap reduce job so hi queries are very\nfaster on hoton Works in a claa cluster\nclaa doesn't support the because the is\ncontributed by hoton hoton BS they are\ncompetitors right so if you are working\non a claa cluster you will never see th\nin your life they won't allow so the\nproblem is that if I'm writing a hi\nquery on cloud era it's very slow\nbecause it'll convert to map reduce run\nas a map reduce program takes a lot of\ntime if I'm writing the same hi query on\nhoton works it's at least five times\nfaster because it converts to T job by\ndefault it is T and T is superb you\ncan't compare right so that is one\ndrawback I mean so when you start\nworking in the industry so these days\nmostly people are moving to hoton works\nfor majority of reasons but the stake is\nat Cloud around Li number of of users\nare more for cloud era but over a period\nof time probably people will shift to\nhoton work so these are good to know\ninformation so then you will be thinking\nthat from claa there is no answer to\nthis no they have\nImpala there is a product called Impala\nApache Impala Impala's contribution is\nmostly from claa Impala does the same\nthing what does in memory fast real time\nqueries so I'll show you Impala we have\nImpala in the cloud cluster I'll show\nyou hot also if you want to so T itself\nas a project is not really uh uh uh yeah\nnot really uh how do I say important\nright now since spark most of these\nthings are gone including T because\nspark is also in memory T is also in\nmemory so when spark came T relevance is\nnot there but the only relevance is that\nif you're running a hi query it converts\nit into a t job so it's faster actually\non hoton Works platforms so I will show\nyou a hoton cluster probably tomorrow\nokay I have an access I'll show you the\nT job running you'll see the difference\nactually so SP but there are a lot of\ncompanies who run traditional hi only hi\nlike they don't want to use Spar they\njust want to use hi for them going to a\ncloud era may be a bit challenging\nbecause hoton works by default give you\nfree T right so T is fast also T is fall\ntolerant Impala is not I'll discuss\nprobably tomorrow probably too early to\ndiscuss but uh there are SQL engines are\ntoo many actually in this category and\nthere is a tight competition between all\nof them and it's all marketing gimmick\nyou go to Cloud they will say that\nImpala is the best in the world never\nlisten to hoton Works hoton work will\nsay that and hoton works recently Hive\nrecently announced something called\nLP Apache Hive LP is realtime queries\nusing Hive Hive is normally batch\nprocessing because it is converting into\nmap reduce right but recently Apache\nannounced that how you can support\nrealtime queries that's called\nLP um the the uh uh real abbreviation I\ndon't know Long Live and process that is\nnot the real one but we used to say long\nlive and process it is still in beta not\nso but the problem LP is available only\non hoton Works no Cloud era open source\nright because it is a contribution of\nOpen Source Hive which is available only\non hoton cloud cannot get it so they are\nstill with Impala Impala is the only\nsolution they have so when you talk\nabout H there are a lot of things but\nyou should be aware of the industry\nwhat's happening right so these are the\nreasons why people are shifting towards\nhot and work some of the major things I\nsee compared to that doesn't mean cloud\nis bad okay it's not like they are also\nhaving great support their technical\nsupport is excellent and even with the\nImpala they do a lot of tricks actually\ntoo so if you have a problem they'll\ngive a solution but uh a lot of people\nare preferring this open source\ninitiatives hoton works and all are open\nsource but the problem is I told you\nright I was with G they use LP none of\nthe queries work most of the time it is\nopen source right if it doesn't work you\ncan't blame Cloud will fix it because it\nis propietary like iPhone and Android\nright so open source you get lot of\nfeatures but it it will work or not work\nyou cannot guarantee that's one thing\nthey but they will blame internally on\nApache they say that we'll give you\ntechnical support within the limits so\noriginally the product is by Apache so\nhoton Works cannot fix directly because\nit's an open source Community most of\nthe things work I mean I'm not saying\nthat everything doesn't work but open\nsource has its own drawback sometimes\nright uh but I'm seeing a lot of people\nmigrating to hoton Works actually these\ndays\nright so in Hive what you need to\nunderstand is these things it provides\nSQL like interface on\nnow very important Point Hive does not\nhave\nstorage it uses the data in Hadoop for\nexample you copy a CSV file into hdfs\nand what I can do I can go to hve say\ncreate a table and give the schema then\nI say load this CSV to the table what is\ngoing to happen the CSV will be in\nHadoop it is on hdfs it is as blocks and\nreplicas and all but in hi you'll just\nproject a structure\nso you get a table once you get a table\nyou can do any SQL most of the SQL right\nso the idea is to project a structure to\nthe data that is already available in\nHado and hi can create tables from a\nvariety of formats structure data even\nlog files unstructured data uh Json\nfiles different types of files can be\nread by Hive and put it in a table\nformat right so that's the importance of\nHive uh so it's treated as the Hadoop\ndata warehouse system so the data is in\nhdfs and this is just giving a table\nlike structure on top of the data hi\nwill just maintain the schema only\nbecause the data is already in Hado\nright so you don't have to load the data\neven though you say load the data the\ndata will just become uh in Hadoop I'll\nshow you how to load the data I mean how\nto create a tables and all yeah so if\nyou want to run a query you can directly\nsay write your SQL say select star from\nthe table but the problem is it will hit\nhi hi will read the query and it will\nconvert it into a map reduce program\nJava base create a jar file so that will\ntake some time then when it runs it's a\nmap ruce job and the funniest part is\nHadoop never understands what is hyp for\nHadoop it's a map reduce program you're\ngetting the point right from the\nhadoop's point of view there is nothing\ncalled hi it understands only map reduce\nso whether you write a map reduce or hi\nwrite a map reduce it's the same thing\nright if you want you can write a map\nreduce program to query structure data\nit's very difficult that's why you're\nusing hve it'll translate and create a\njar file push it to the cluster and the\ncluster will run the map reduce regular\nprogram show you the output you can save\nit in a table or or anything you can do\nso you can use all regular SQL\nstatements you can say insert into a\ntable from this table so the result will\nbe if it is matching with the SCH will\ninsert the same way the only thing is\nprojecting a structure had Hadoop only\nhdfs only everything is on hdfs it is\nslow Hive is expected to be slow so that\nis one thing hi is considered to be a\nbat processing engine so it is not a\ndatabase another common problem is that\npeople may not be able to differentiate\nbetween a database and a data warehouse\nthey're different right a database is O\nTP you call transactional real time\nright where you want a millisecond speed\nright subse speed that's a database and\nyou always have insert update all these\nkind of operation a data warehouse is\ndifferent a data warehouse is a place\nwhere all the cleaned data comes and\nlands right from there your reporting\ntools can connect and visualize the data\nright so Hive is a data warehouse where\nyou store your final data so let's say\nyou have lot of tables you ran queries\nand you got the final output you st\nstore it in height\nso that your visualization tools\ncan huh store it in hdfs but we say\nstore it in hi but it's in hdfs only\nmeaning if I open Hive and say select\nstar I will see a table if I go to the\nlocation open the file I see a text file\nsame thing so the text file will be\nthere if I open and say select star same\ntext file it will show me in a nice row\ncolumn format and another thing is that\nhi's language is actually called hql hi\nquery language it's not\nSQL but it is based on SQL 92 syntax\nmost of your queries will work okay but\nit's technically called hql hi query\nlanguage right it's it's up to us to\ndecide I mean um so the idea is to\nfamiliar with familiarize with h then\ndepending on our business use case we\ncan write any SQL queries you can write\ncomplex queries simple queries you need\nto have some idea about SQL queries like\nwhat is a join operation what is a group\nby query to understand what is happening\neven a person without knowing SQL can\nlearn High I mean by looking at a query\nyou can understand what is the intention\nof the query\nright Hive is not a DB Hive queries take\nminutes even for small data sets and\ncan't be compared to databases like\nOracle Hive does not provide realtime\nqueries so that is what the original\nHive which came was converting into map\nreduce so it was very very slow then\ncame th so what hoton works did they\nstarted using High plus th which is a\nbit more faster right claer cannot use\nth so claer still runs Hive into map\nreduce and their answer to that is\nImpala Impala is a inmemory execution\nengine but Impala will work only on\ncloud era so I will show you Impala and\nsame query will be very very fast there\nis no difference apart from that I mean\nsyntax and everything is same so uh uh\ndata warehousing in the sense like\nusually it is used as a storage engine\nrather than so what is a data warehouse\na data warehouse is where you store all\nyour massive data right structure data\nand the intention of Hive is also same\nand all the bi tools can connect with hi\nso if you're having something like\nTableau for example Tableau is a\nvisualization tool so I want to\nvisualize terabytes of data from Hadoop\nI can connect Tableau with Hadoop and\nTableau will be firing SQL queries using\nhi and the data can be visualized so\nthat's a typical application of a olap\nsystem so it's an olab system online\nanalytical processing it's not an olp\nsystem so you can't compare it like\nOracle because that's like real time uh\nbut again hi cannot\nreplace a proper data warehouse okay so\nif you're talking about something like\nTerra dat for example so they are like\nhow do I say the expensive and fast\nreliable data\nwarehouses understand hi is built on top\nof Hadoop only so it has all the\ndrawbacks of Hadoop right so most of the\norganizations what they do these days is\nthat they will categorize their data H\ndata cold data uh the hold data will go\nto something like Tera data you know\nwhere they need to immediately fish the\ndata run faster queries and the cold\ndata which it's okay you can take some\n10 minutes to query we'll go to Hive\nHive supports insert statements now it\nwas not there before insert statements\nare already there but the problem is\nthat each insert statement will fire a\nmap Produce job so you do a bulk load\nthat's better so in in Hive normally\nwhen you load the data you do a bulk\nload you don't insert it and the latest\neditions of Hy support all the cred\noperations create read update delete\neverything is supported uh but again if\nyou want to get the speed you should use\nLP that's what I'm saying real time\nqueries LP and LP as of now cloud data\ndoesn't support so I can say if you are\non a cloud era platform inserts and\nupdates will be very slow not like terra\nterra and inserts are very fast but the\nquestion is that in a data warehouse\nusually you don't have to do an insert\nvery rare right because you will do bulk\nloading of the data or your ETL tools\nwill be dumping the data there they will\ncollect data from the rdbms and then\ndump it into there so very rarely you\nmodify the data in a data warehouse so\nthe use case is like that for he yes all\nthe cred operations are supported even\ninsert update delete but it'll be slow\non hotworks platform you have acid\nsupport enabl transaction management but\nit is not as reliable as a database okay\nuh because you can never replace an\nrdbms system with something like he but\nyou have transaction management\nsupported that is with LP LP is the\nfeature which gives High realtime\nperformance but LP also requires\nresources it cannot so it requires more\nresources to execute but you get all the\nasset properties transaction management\neverything and this is very important to\nunderstand\nso there is a command line interface for\nhype it's called Hive shell you can open\nthe hive shell and start typing your\nquery say create database is create\ntable and then type the query so that is\none way to work with hyp uh but people\nmay or may not use it because if you are\nyou're in the production and all you\nmight have some client right so some\nwhat type of clients you use like to\nconnect with uh DBS and data warehouses\nso SEC server or Todd or something know\nsomething like that you might either you\ncan use that so one thing is that you\ncan use use a CLI and hi is very vast\nactually so I'm getting confused when I\ntalk this CLI there is two clis the\noriginal CLI is called Hive and you\nsimply type Hive it'll open you do all\nthe blah blah\nblah the new CLI is called bline it's\ncalled beline okay and beline is a\nproper jdbc client so through Bine also\nyou can open the CLI or you can simply\nsay hi it will open the command line for\nyou you can do all the create a\nstatements and all there is a web GUI\nbut it is very rarely used Hive is just\na client it's not a jdbc client so when\nI open the CLI and say Hive jdbc you\nhave drivers right to connect so when\nyou want to connect from an application\nright to a database you need a driver\nthat's called a jdbc or odbc drivers are\nthere that's one way to connect uh the\nso the CLI original Hive CLI is called\nHive client that's not a jdbc client you\njust open it it'll directly hit Hive and\nthere is it'll directly hit the hive and\nyou can type all the queries and all the\nB line is a proper uh jdbc client which\nmeans you can install it in some other\nmachine from there you can connect you\ncan give the server address port number\nand it will hit right and it is not\nwritten here there is something called H\nserver this whole part is called H\nserver this part for some reason in the\ndiagram it is not written but this whole\npart is called hi server because if you\nare connecting with a data warehouse\nthere should be a server component which\nwill accept your queries you know\nwhatever you're typing right so that is\nthis whole part so whether you come\nthrough CLI and there is a web GUI or\njdbc you all hit the hi server\nright and H server is the component\nwhich will accept your query so once you\nwrite a query there is a planner parser\nOptimizer it'll optimize your query and\nconvert a map reduce program and push it\ninto the Hadoop\ncluster in hi server itself you have H\nserver one and hi server two okay so the\nold one is called hi server one which\nnobody is using now you don't have to\nbother uh the latest one is called hi\nserver 2 okay that is every everybody is\nusing so H server 2 is responsible for\nmanaging all your connection so if you\nopen a connection with h it'll directly\nhit this H server to and from there\nwhatever query you write this guy has a\nyou know compiler Optimizer executor\nit'll get the query compile it optimize\nit push it as a map reduce program right\nI think I can show you that from here if\nyou go to Cloud manager should be here\nit is very difficult to talk about Hive\nrather than showing something because\ntalking is fine but you know if if I\nshow something probably people will\nunderstand more so if I go to hi\nconfiguration uh where is H\nserver H see H server 2 so this is\ncalled H server\n2 the the default Hive server is hi\nserver 2 because there was a old H\nserver one which nobody's using now\nbecause of performance issues and all so\nright now whether it is HBS or cloud or\nany platform the server component of hi\nis called hi server 2 okay so either you\ncome through a jdbc client or the\ncommand line you are all hitting H\nserver 2 okay from there it will uh\ncompile your query and optimize your\nquery and push it and there are tons of\noptimization techniques in Hive to to uh\nyou know read your query\nand better it and all probably we will\ndiscuss that tomorrow some of the\noptimization\ntechniques another very important point\nin this slide is this metast\nstore this thing meaning if I open a a\nhive session and I say create a table if\nI exit it should not go right because\nthe table should persist whatever you're\ncreating so that is where the meta store\ncomes into picture that that is where\nall the metadata will be\nstored and in production setup what we\ndo is that we will create a myql server\nand in the MySQL server meta store will\nbe configured so all the metadata of\nyour table like the schema access rights\neverything will be in the meta store\nright uh and U so you have to ensure\nthat the metast store service is running\notherwise hi will not start because it\nneeds to talk to metast store this meta\nstore is very important because even in\nspark spark will talk to this meta store\nso if I create a hive table I can read\nit in spark because it can talk to the\nsame meta store and get the data from\nthere all the metadata will be there in\nthe meta store okay meta store is the\nmetadata metast store is a service and\nthat will push the metadata so you have\nto give a destination in the so here it\nwill be there I'll show you where the\nmetast store is I don't know where they\nhave configured here if I go here to hi\nconfiguration\nright hi meta store server\nthread ah database type is my SQL right\nuh\nyes this is where it is\nsee on this machine they have configured\nMySQL and that is where the metadata\nwill be pushed and we don't have any use\nof the metadata hi will read and write\nit that's fine okay and it uses it to\npersist all the table information and\nDBU create and everything right we we\ndon't have much of a use for that but it\nis there and in hi you have something\ncalled a cost based Optimizer CBO\nCBO H I don't know why it is\ndisabled enable cost based optimiz\ndisabled it is disabled for some reason\nI don't know so this is called CBO cost\nbased Optimizer you are aware of rdbms\nright if you write a very complicated\nquery it will generate multiple query\nplans how to execute this query this\nOptimizer so what Hive does if you\nenable it if you write a very\ncomplicated query it will generate\nmultiple plans and based on the cost of\neach plan it will select the best plan\nso that is the cost based Optimizer it\nis disabled I don't know why it is\ndisabled so usually uh you can enable\nthis and this is how optimization can be\ndone in Hive right so this is one thing\nCBO we will look into other things also\num\nokay all the configurations are here to\nstart hi you simply say hi that's\nit you simply type hi it will start the\nhive\nshell may get some warning that's\nfine and you will easily see a warning\nsaying that hi CLI is deprecated and\nmigration to beine is recommended that's\nokay so what it actually means is that\nthis is the original hi\nCLI and I told you that beline is the\nnew client so they are saying that use\nbeline whenever possible so that's fine\nI mean to learn Hive I think the best\nway is this CLI they have added some\nmore commands in the bline section\nactually because the old high CLI\nsupport is uh getting lower every day uh\nso but for learning purpose and it's all\nthe same whether you use the CLI or\nbline I'll show you how to use the bline\nalso um so this is the H shell what you\nopen right and it's very easy to get\nstarted with hi all you can do is that\nyou can start by creating a database you\nsay\ncreate database so first thing is that\nwe create something called a database\nthat is the highest level of abstraction\nso I say create\ndatabase I don't know\nsomething May\n19th so um I just created a database so\nthe command is create database and the\ndatabase name so I'm just calling it as\nMay\n19th and if I do a show databases\nyou can see lot of DBS are there people\nhave created right see\nhere so these are all the databases that\ngot created inside Hive and minus main\n19 right you could have created\nsomething else right whatever I mean you\nremember and you have to say\nuse to switch the DB you have to say use\nMay\n19 these are regular commands if you\naware of SQL right H there is a way to\nshow that it will display the DB name\nSet uh\nhi there is a property where it we can\nenable it to see the DB I'll search for\nit you can even print the column headers\nso normally when you say uh select star\nor something it won't show the column\nheaders\nthe column\nname you have columns and the names\nright it won't show but you can make it\nshow there is a command to print\nthat so first we'll make sure\neverybody's on the same\npage\nhi\nprint current\nDB set High CLI print current\nDB so this is how you can uh print your\nDB you simply say set High CLI print\ncurrent DB equal to true and then it\nwill display the DB there can you try\nthis everybody is able to log on to hi\nand start and all anybody having problem\nin Python you don't have right no you\nhave\nindentation indentation is very\ncomplicated actually you miss one\nindentation nothing will work in Python\nI know that a\nlot very difficult people who are coming\nfrom the Java world and you are familiar\nwith curly bracer here everything is you\nstart a control structure there is an\nindentation right but the editor will\ntake care of it I think you're using\nsome editor the that will be in the\nmetast store only the definition right\nmetast store it will have it all that's\nwhat when you normally start H I said\nthe use may9 right that the DB it won't\nshow me in which DB I am in so now it is\ndisplaying it here right see this it\nwill show in bracket what is your\ncurrent DB uh so it will display\nwhichever data datase you are in if you\nchange it will show\nright and another important point is\nthat probably towards a little bit\ntowards admin site there is a file\ncalled hi site\nXML hiy site. XML and that file has\naround thousand\nproperties and the whole Hive is\ncontrolled by that file so these\nproperties are actually from that file\nmeaning either you can do like this you\ncan open a session and say set High blah\nblah blah true but if you exit and relog\nin it will not print the DP again you\nhave to type this or you can ask your\nadmin change in Hite XML uh this\nproperty so that for everybody who log\nin the DB will be visible so that file\nhas lot of properties actually I'll try\nto find that for\nyou problem is that I was with Hon works\nfor the past 10 15 days I'm finding it\nbit difficult to read Cloud era but\nit'll be\nhere\nconfiguration ah or do one thing no it's\neasy uh what you can do all of you can\ndo open one more command prompt uh web\nconsole and just\ncopy uh um if you go to\nCD\nEtc\nHive\ncon uh yes if you navigate to this\nlocation this is common for cloud hoton\nWorks everybody ITC Hive con folder uh\nyou will have a file called Hive hund\nsite. XML this file controls end to end\nof hi all the properties right and if I\nopen it you can see all the\nproperties H see hi metast store URI\nthis is the metast store URI remember so\nwhere the connection is there then some\nother things are there autoc convert\njoin uh how do you search in VI\neditor this one right then I can type\nright uh what did we set now High\nCLI oh no hi\ndot it says pattern not found it should\nbe there right I mean I'm just wondering\nor what if I search DB saying pattern\nnot from DB should be there let me just\nsearch for\nHive probably I don't know uh see all\nthis um what you say properties are\nthere in this file probably some of them\nare uh so another thing is that you are\naccessing this file locally I mean from\nthis machine if I go to Cloud manager\nconfiguration I'll say\nDB should be there somewhere I think\nsome of them are uh actually not visible\nto us but all these properties are read\nfrom Hite XML all these things will be\nin Hite XML actually so that is how you\ncan print the uh current DB you are in\nwell that's not our intention what is\nOur intention so let me show you the\ndata that we have so if you go to this\nfolder right you will have two files now\nonce you start loading data to\nHive it is very complicated even though\nthe queries are very simple people try\nto get it is like watching a Christopher\nnoan movie you know you won't understand\nwhat happened once you finish the movie\nalso you won't understand what happened\nso the point is I will write first then\nprobably we will do it okay so you will\ncreate a table that everybody knows this\nyou will do in hi you will say create\ntable blah blah blah it will create a\ntable fine then next step is that you\nwant to load the data into the table the\ndata can be loaded from lock file system\nor\nhdfs it's a bit\ntricky so if you create a table and you\nwant to put some data right the data can\nbe loaded from Hadoop or from your Linux\nfile\nsystem right okay and I will show you\nwhat will happen how to do both this and\nwhat is going to happen if you do both\nthis what will happen to your data and\nall these things right so we'll do step\nby step otherwise people get confused a\nlot in in this stage it's actually very\neasy but uh end of the day people get\nconfused a lot where is my data what\nhappened to my data sometimes you will\nget local also so what happens these ETL\ntools and out sometimes they dump it\ninto some local machine you can load you\ncan say hi take it from my Linux dump it\ninto so it is actually copying from\nLinux to Hado but there is a way to do\nit that's what I wanted to show so let's\nlook at the data right we'll also do\nsome small analysis uh can you open this\nfile\nbut I need a better editor\nactually wordpad will do\nright uh that file is already available\nuh in your uh data\nset let me download this okay real quick\ncan you see this file transaction. txt\nTX\nns1 it will be here you will have a code\nand data folder inside that hi inside\nthat I think uh it's it is a txt file\nright because for me the extension is\nnot visible\nuh just give me one moment I'll just\ninstall this notepad++\nso last class you are watching narcos or\nwhat the ads are coming\nright what happened in the morning\num so what we will do is that first we\nwill copy the file to the local file\nsystem that is Linux and then we'll\ncreate a hive table we say load the data\nto The Hive table and we will see what\nis happening in that process then so\nthere are two files actually another\nfile I'll copy to hdfs from there I'll\ngo to another table and show you what is\nhappening so okay we'll try without this\nI mean you have notepad anyway right so\nthat's fine I just wanted to show you\nthis yeah so this is the transaction\ndata what we have here and if you look\nat this data right uh I'll just uh take\none line and explain what it is so you\ncan understand right so if you take this\nline so the first uh column is the\ntransaction ID that's nothing but 18\nthen then you have the transaction date\nuh then you have the customer ID so\n4244 is customer ID okay and then you\nhave the amount spent\n$88 then you have the category of items\nhe bought team sports and what actually\nhe bought Baseball City of Salt Lake uh\nstate of Utah and credit so it's a\ntypical sports store Transaction what\ncategory what item what amount credit\ndebit you know and you have n number of\nlines like that it's okay that's okay no\nproblem so this is one data that we have\nit's a transactional data and we want to\ncreate a hyp table load this data so\nthat's one thing second data that you\nhave there is a file called a cust can\nyou see\nthis and I want to open this\nwith uh word\npad yeah so this is the c customer data\nso First Column will be customer ID\nfirst name last name age and profession\nChristina Chung age 55 is a pilot like\nthat right so you have a store customer\ndata you have a transaction data right\nand what we want to figure out is that\nwe want to figure out the total amount\nspent age wise by the customers for\nexample I want to know in my store what\nis the total total amount spent by\npeople age between 20 to 30 and 30 to 40\nand 40 to 50 so that is the analysis we\nwant to do so in one data set we have\nall the\ntransaction like how much amount they're\nspending and all second um uh data set\nwe have the person's name and age and\nall so we'll do a join and then we will\nuh do a case statement and figure out\nthat's what we are going to do so the\nSQL analysis is not the intention the\nintention is to show you what is\nhappening happening behind the scene and\nand how hi is dealing this right okay so\nnow what I want you to do is uh first we\nwill do uh only the uh local so we will\ncopy the data to the local file system\nfrom there we will analyze it so I want\nyou to copy these two to our uh local\nsystem so how do you do it you have to\nopen the FTP stuff right so I'll just\nsay FTP\nlogin and hopefully it might work for\nme upload\nfiles so upload these two files one is\ncalled cust CD another one is called\ntxns One\nopen yeah so it works\nhere okay I need to to open one\nmore\nokay huh luckily for me it is available\nyou have to figure out so so use FTP and\ncopy the files to Linux not to Hadoop\ndon't upload to\nHadoop I'll just wait if uh you need\nsome\ntime yeah but uh we don't have to change\nthe extension yeah so uh if it is a text\nfile format it can access any text file\nonly thing you have to mention the D\nlimiter comma or what ever it is so hi\ncan handle text CSV Json XML variety par\nso there are multiple file formats it\ncan handle so by default it'll read all\nthe text but if you having Json and all\nthere is so are you guys familiar with\nsomething called serd do you have you\nheard about serd no serd stands for\nserializer\ndeserializer okay uh it's a very common\npractice so what will happen is that\nlet's say you have a Json file you know\nwhat is Json right key and value huh\nlike key value pair so if I have a Json\nfile I can ask H to create a table from\nthat but the problem is that Json is\nsemi structure data it's not proper\ncomma separated or something and how\nwill say I don't know what you're\ntalking about I can't find a schema so\nto do that we can use something called a\nJson serd you have to download it it's a\njar file I don't know whether the latest\nH has it if not you have to download a\njar file which is called a Json serd\nOkay add it into Hive and then you say\ncreate a table using the SD so using\nthis serd it will parse the Json and key\nwill become the column value will become\nthe data like that so serdes are used to\nuh what do I say read a semi-structured\nunstructured data I will show you an\nexample of unstructured data where you\ncan use something called a reject serd\nregular Expressions you can use to to\nthe data so hi Supports number of series\nactually to read different types of data\nJson I will see if I have a data show so\nthat is again an open source uh projects\nmore most of them so internally nothing\nhappens they will read the key and the\nvalue if it is a Json serd and it will\ngive that structure like a table to Hive\nso if Hive looks at it hi will not see\nas key value pair I will say a column\nheader and a value it'll just apply the\nsame thing so it is reading the semi\nstructure data and giving a structure to\nthat it is a parser so we have parsers\nright it's a parser\nactually so this thing are you able to\ndo are you able to upload\neverybody and and find this file you\nhave a file called commands can you see\nwhether it is there\nuh just commands in the high folder you\nwill have a file called\ncommands uh no no no this is not uh uh\nthis is the commands we have to type or\ncopy paste so we don't have to upload\nit so this is in the rug for me this\ndata is in the folder called Ragu okay\nthat is where the data\nresides um let me close\nthis now we can actually create a table\nso let me just copy paste this um you\ncan so don't do a create database\nbecause the first two steps we have\nalready done we created a database and\nthen we said use it okay and the first\nthing you need to do is this command\nI'll explain it so just copy this create\na table just go here\nand paste it and add a semicolon at the\nend and hit\nenter you have to add a semicolon\notherwise it will not work and hit\nenter yeah so this is the typical uh\ncreate table command in SQL it says\ncreate a table and then the table name\nis transaction records or whatever name\nyou want you can give and within the\nbrackets you are mentioning the schema\nof the data so hi support\nsimple and complex data types so those\nwho are aware of data types so you have\nsimple data types like integer string\nand all it also supports complex data\ntypes like map strs and all right so\nthey are all supported but here we are\njust starting with a very simple so I'm\njust saying even there is a date data\ntype uh and and the important Point here\nis not the schema schema I'm mentioning\nyou even have a data type for date I'm\nnot using it I'm saying string but hi\nsupports date handling and all uh so\nafter that the most important Point here\nis row format D limited Fields\nterminated by comma what that means hi\nis expecting the data in row by row and\nthe D limiter is comma which means you\ncan load only\ncomma uh separated files ideally right\nI'll tell you what will happen if I do\nsomething else let's say I'm not using\ncomma what will happen I'll show you but\nright now we are simply Crea creting a\ntable there is a record reader for Hive\nso that will read no we don't have to\nDefine okay so now this table is created\nright so all of you are created Hive can\nbe accessed through Hue you have a hue\nright that I'll show you that there you\nhave auto complete mostly but the hard\nway if you're working with h there is no\nauto complete you have to type\neverything\nmanually now now is the important thing\nhow do you load the data right and there\nare lot of things I want to talk about\nabout loading the data first of all\nidentify where is your data right so in\nmy\ncase the data is in this\nlocation so just make sure you\nunderstand the location of the data in\nmy case this is the location I'll just\ncopy this and the data is txns 1.txt and\nI will\nsay load data local input\npath into\ntable so this is the\ncommand uh the path I found here I went\nto uh this location I did a\nPWD see this is where I have the data\nright\nit is on\nLinux it is on\nLinux if and it is very easy if you make\nany mistake it will say path cannot be\nfound it will not load if there is a\ncorrect path loading the data right it's\nvery easy to identify whether the\ncommand is correct or not in my case see\nit says loading the data so that means\nit is working fine for me are you also\ngetting the same thing so make sure it's\na local path okay so people get confused\nit's is a local\npath yeah because here we have not done\nthe analysis we just loaded the data and\nhere you can write SQL query you don't\nhave we'll be writing SQL you don't have\nto write the Java program\nand not at all you don't have to know\nJava jar file it will create a jar file\nactually if you mention a folder it will\nload all the files in the folder so just\nmention the exact file he mentioned the\nfolder name so there were two files he\nloaded everything\nso here since it is a data warehouse it\njust happens two files but from where to\ntret that will be another problem right\neven I don't know this should be the\nfile size in bytes I\nthink uh roughly it is 4 MB right should\nbe 4 MB the size of the file\nactually now this this part is fine\nactually I mean you are just loading the\ndata and if you want to check you can\nsimply do a select star\nfrom txn records you can do a\nlimit five can you try this do a select\nstar from the table do a limit five\ndon't simply run a select St there is\naround 50,000 records I think so do a\nlimit I want to see the top five rows\nright so that's the command this is to\nmake sure the data is loaded now you can\nuh upload the data from local as well as\nHado so I'm just showing how to do this\nI'll show you from Hadoop also both\npossible h no select star mean select\neverything from the table I want to show\nsee the Full Table Right limit five\nmeans top five rows I want to\nsee now if you actually want to see the\nquery right you can write any query\ndoesn't matter but I can simply say\nselect count\nstar from\nso this select count star is a query and\nas you can see the moment I run the\nquery it is firing a map Produce job see\nso the query will fire a map reduce jaw\nbecause it is converting it into and see\nmap zero reduce\nzero and this is this cluster is really\ngood the the what I'm telling you is\nthat we have around 10 nodes here in the\ncluster and only I think around 20\npeople are using it very less data\nthat's why it is very fast actually it\nis very very slow this is this is\nunexpected if if Hive is fast there is\nsome problem you are to troubleshoot so\nthis means select star select star means\nshow me the full table limit five I want\nto see the top five rows you just\nverifying the data is loaded no the top\nfive rows because it has 50,000 rows if\nI simply do select star it will display\n50,000 I don't want I just want to see\nno no select will not fire a map\nproducer because it's a simple read it\ndoesn't need to do any calculation so\nwhen you do a select star uh it won't\nfire a map reduce job it will simply\nshow you the\noutput so just let me know are you able\nto do till this you have to do a select\ncount star it should fire a map reduce\njob and you should see the\noutput uh one more thing I want you to\ndo in that file I have given to you\nright this commands file uh it might be\nslightly wrong not wrong ah don't copy\npaste that's what I'm saying that I'll\nwrite it here because I use it for a\ndifferent session also so there uh if I\nopen this here it will say load data in\npath see it is saying load data in path\nit is actually load data local in paath\nright so don't follow the same thing and\nyou can correct it also whatever you're\nrunning here just correct it there so\nthat it is uh reflected right otherwise\ntomorrow you try and it may not work the\nway you expect because it's the map\nreduce uh output I mean it is firing a\nmap ruce job right so map ruce logs are\njust being displayed that's what you can\nsuppress it there is a way to suppress\nit but normally if you submit a jar file\nmap reduce it displays the same thing\nMPP reducer that's one finally you will\nsee the result also it is a default\nnature actually in hi because even\nthough it is a hi query it is not a Hy\nquery it's a map reduce program that's\nwhy it shows map and this is good for\ntroubleshooting sometimes it will show\nyou uh errors and warnings and all when\nit is running I'll show you how to\nenable column but right now oh you doing\na select Star right if you want column\nnames\nset hi. CLI do print\ndo header equal to\nTrue okay now if I do\nthis this is countstar right it'll call\ndefinitely because it it is some\nanalysis it has to call for it only the\nselect star will not call because it is\njust reading the data\nso are you able to load the data at\nleast run the basic query then that's\nokay he's asking can you see the Java\ncode high is\nwriting no I I have not found it so far\nhow to do that no that that is internal\nto Hy I mean how it is converting that\ninto a Java Pro actually we have all\nthis in map reduce for example how to\nwrite a join in map ruce is there select\nstatement doesn't normal select star is\nsimply showing right it doesn't convert\nto map where condition internally there\nshould be some logic again to everything\nis key value pair end of the day ah\neverything is so another drawback is\nthat a hive is good but you will not\nknow what is happening behind the\nscene like end of the day how the code\nis written you will not know you will\nsee the result but that's enough if you\nknow SQL you can just fire the query\nright Hado into Hive huh in Hadoop to\nHive and Hadoop because it is in Hadoop\nand it is visible in Hive anyway so that\nnow the important point is I don't\nactually want to show you running the\nquery that's so once you now you have\nthis table and you can write all the SQL\nquery that is not my intention the real\nquestion is where is the data first\nquestion right so you loaded the data\nand uh from local you said that means\nfrom Linux so first thing you have to\nunderstand is that data must be in\nHadoop so when you did a load data it\ncould have gone to Hadoop that's for\nsure but where in Hado right so to\nunderstand\nthis what you need to do um you have a\ndescribe\ncommand so let's say\ndescribe and txn\nRecords so this is a very common command\nif you do a describe on the table name\nit will show you the columns and data\ntypes and all and if you want you can\nalso\nsay\ndescribe formatted and the table\nname so describe formatted will give you\nmore details about your table that you\ncreated and if I hit enter there is a\nvery interesting thing that you can see\nthe thing is it says table type is\nmanaged\ntable so any table that you create in h\neither it can be a managed table or\nsomething called external table by\ndefault everything is manage table I'll\ntell you what is what you mean by manage\ntable okay but you can clearly say uh\nsee it says it's a manage table Point\nnumber one point number two it displays\nsomething called\nlocation right it says location and it\nsays user Hive Warehouse so I want you\nto do one thing just go to your Hue file\nbrowser can you go to this\nthing if you go to this Hue just go to\nthis user\nfolder there's a folder called user can\nyou go\nhere oh I think there are a lot of users\nactually so n number of users are there\nokay so do one thing user\nslash\nHive so if you simply type it here user\nSL hiive you will land on this page so\nwhat will happen is that when you\ninstall Hive there is something called\nWarehouse folder hi will ask you where\nto create it and normally in all Hado\nclusters it will be user hi inside that\nthere will be a folder called Warehouse\nokay and if you open this folder you can\nsee all the DBS people have created and\nyou can open your DB my database was uh\nMay 19 so this is my database May 19\nopen that and there is a folder called\ntxn records this is nothing but your\ntable you open this you have the\ndata so try to find out your data by\nyourself so my point is uh what is going\nto happen whenever you're creating a DB\nin Hadoop this folder is there user High\nWarehouse inside that will create a\nfolder with your DB name when you create\na table a folder with a table name and\nwhen you say load the data it is simply\ncopying the data\nhere are you able to reach this point I\ninan this place you can also do\nsomething interesting I can simply go\nhere and delete the\ndata I deleted the data from from here\nright and if I do a select star\nnow uh sorry it it fired the query\nbecause the table is there it say zero\nbecause the data is not there right let\nit\nrun yeah so or if I do a select\nstar there is nothing so I did a select\nstar from the table and there is no data\nbecause I deleted it from here\nright and you can manually copy the data\nhere if you don't want to type low data\nlocal in path I can just copy this\ntransaction here to this\nlocation huh that is a schema but you\ndon't have a data so now I manually\nuploaded the data here and if I go back\nhere I should be able to see the\ntable so this is where your data goes if\nyou're copying if you're saying low data\nlocal in paath the data goes into this\nuser High Warehouse DB folder table\nfolder that is where it DS okay so are\nyou able to see this folder I think\nyes and now you are now I am going to\nconfuse you okay so if you understood\nthis much I'm going to confuse you uh so\nnow what you did you loaded the data\nfrom local file system and I have one\nmore data right so there is one more\nfile called um uh customer right I want\nto upload this from Hadoop so what I'm\ngoing to do you can also do along with\nme I'll go to H okay and uh in my home\nfolder\nRagu okay I'll just upload the data here\nstep number\none so where is the cust file customer\nfile copy that into Hado in your home f\nfolder wherever you\nwant\nuh this is a bit confusing I\nmean so remember the location where you\nhave copied that in had\nfine uploaded and now I want to copy\nthat into customer table but first we\nhave to create a customer table right we\ndon't have a customer table so I can go\nhere and there is a command here create\ntable customer see find sales based on\nage group below that there is a command\njust copy this\ncommand come back\nhere and just say paste\nit and it should create a customer\ntable fine\nso now you can load the data from Linux\nyou already know how to do that okay but\nI don't want to do that I want to load\nfrom Hadoop how do I do it I'll say load\ndata inpath I don't say local I say\ninpath I'll\nsay uh\nRagu\nslash cousins\ninto table\ncustomer try this only difference is\nthat you are not using local you say\nload data in path and give the Hadoop\nlocation now I want you to do one thing\nwhere did you upload your data in hdfs\nthis cust file uh can you check the data\nthere it will not be there if you can\nsee there is a problem it will move from\nthere ah just see once uh see in my case\nit was in it was in Ragu if I refresh\nhere it'll be\ngone because uh the tables we are\ncreating is called managed table so when\nwe did a describe it showed it is a\nmanaged table what is a managed table a\nmanage table is a table where high will\nmanage your data you don't have any\ncontrol so in Hadoop it has already\nselected a location where it will dump\nall the file so even though you say load\nthe data from this Hadoop location will\ncut it from there and dump it into that\nso you can go back to that user High\nWarehouse folder there your data will be\nthere for\nsure you don't have any control as to\nwhere the data will be how will control\nit try whether you can see the data\nthere in user Hive Warehouse house this\nlocation no no no never it's just a\nprojection ah it's just a pointer that's\nall it's just a projection the metadata\nwill get updated that's what is\nhappening because when I say move from\nthis folder to that folder rather than\nmoving that name node will update the\nmetadata it will point from here to here\nthat's all it is happening there it is\nthere I mean other other place it will\nbe gone where you have already uploaded\nthe data will be gone this will not\nhappen if you're Lo from local it will\nnot delete from local okay so that's a\nbit confusing but that is how it works I\nmean people will think hey where is the\ndata what happened where was my file\nright because it is managing the data so\nit will not allow you to keep the data\nit has to be in the user highhouse\nthere so that's one thing and that's\ncalled a manage table what we are\ncreating is called a manage table and\nnow ideally if you have done everything\ncorrect like I am doing if you do a show\ntables you should see two\ntables\nand uh you should also have data in both\nthe tables can you verify do show tables\nand if you do a select star you should\nhave data in both the\ntables because then only you can do\nfurther any analysis what you want right\nonce you H have the data but here we can\nuse SQL you are not familiar with SQL I\nguess SQL is the most powerful language\nin the world for analysis there is\nnothing which can beat SQL even though I\ndon't know much of SQL I'm not a SQL guy\nso you might have to write complex\nqueries but end of the day that is not\nthe idea you should know what's\nhappening queries anybody can write\nmaybe you don't know how even I don't\nknow how to write some queries so I'll\ntake help or somebody will write the\nquery but if you fire a query what is\nhappening where it is running that is\nI'll tell you a very interesting story I\nhave a friend of mine uh he is working\nin Cape Germany so I went to train in\nCape Germany so he was in my training\nteam he is my friend he was also sitting\nin the class I knew that okay so I was\nasking the participants how many of you\nare using Hadoop or hi or and most of\nthem said no they are all ETL\nguys the point is they were running\nhoton works and hoton works has a tool\ncalled Hawk H awq Hawk uh Hawk is like\nHive only but with some modification\nlike Hawk has become LP now to be fair\npreviously they used to call it as Hawk\nokay so Hawk is actually a connection to\nHive only end of the day so these people\nwere using hawkk for past two years but\nthey didn't know that they were using\nHadoop because for a SQL Developer it\ndoesn't really matter you write a SQL\nquery and do you really care where it is\nhitting or who is processing your query\nyou don't really care your your idea is\nto run the query right Hawk as such is\nLP so uh Hawk is an improvement on hi\nand now the project is discontinued it\nhas become that LP real time acid\nqueries on Hy previously it was known as\nHawk so I went like 3 years back that\ntime they were running Hulk and they\ndidn't know that it was Hado so most of\ntheir queries were on Hado\nbut that's what I'm saying if you are a\nSQL Developer mostly you don't really\ncare where the query is running you care\nwhere the query is actually running or\nnot there's a problem or there's a\nperformance issue that is what you look\ninto so that's what I'm saying so I was\nasking them do you actually use hadu\nthey said no we have never used hadu\nthey were actually running queries for 2\nyears\nalmost okay so once you have the data\nwhat we want to do is uh see I want to\ndo it in a different different way you\ncan do it in many ways first thing I'm\ngoing to do is that I'm going to create\na table so you can get the commands from\nhere it's called out\none um so I'm creating a table called\nout one if you look at the schema of\nthis table you will understand what I'm\ndoing it's a join table meaning I want\nto do a join and I want to store the\nresult in this right so usually you\ndon't want to ideally push it into table\nbut you can do it also so I'm creating\nthis empty table to store my join\nresults so I'll be doing a join\noperation and this table will hold the\nresult and how do you do a join query\nit's very very simple uh so if you know\nSQL I think this will be very easy for\nyou you will do a insert overr right\nstatement table and it's a typical inner\njoint right you will say a do customer\nnumber first name age and profession\nthat is from uh one table B do amount\nand product that's from another table uh\nfrom customer a join transaction records\nB on the joining colum is customer\nnumber so for those who do not\nunderstand this we are merging two\ntables so this will produce a result but\nwe want to store it somewhere right so\nwe created an empty table called out one\nso there it'll push it so this is a\ntypical syntax of a join so that a means\none I mean from a here is customer table\nB here is transaction records table so\nfrom a table I want columns customer\nnumber first name age and profession B I\nwant amount and product and whenever you\nwant to do a join there should be a\ncommon column then only join makes sense\nso the common column here is customer\nnumber this one is the common column\ncustomer number and this is called a\nsimple inner join you have outer joints\nleft outer right outer full outer all\nthis everything will work no now it is\ninsert over right now the table has no\ndata it'll simply dump it if you already\nhad some data it will delete everything\nand load it you also have an append\nwhere you can simply\nhappend okay for SQL uh guys I have a\nquestion just I thought I'll bug you\nwith questions what do you mean by uh\nschema on\nright sorry SCH scha on ah right do you\nknow what this is if yes what it is your\nrdbms and typical systems are called\nschema on right meaning if you create a\ntable let's say the table has six\ncolumns if you try to insert seven\ncolumn data what will\nhappen it will say violated it you\ncannot insert seven columns to six\ncolumn table that is because it is\ncalled schema on right it will validate\nthe schema while you are writing the\ndata Hive is schema on read not write\nmeaning I can create a hive table and\nupload an MP3 file it'll properly upload\nno complaints I can upload a movie\nproperly uploaded no complaints because\nit will not validate what you are\nuploading right you will say load the\ndata it will simply copy that and dump\nit in the folder but when you query the\ndata it'll validate there it will show\nan error saying that okay uh and why it\nis is because why Highway schema on read\nis for faster uploads because otherwise\nif it is validating everything it will\ntake time to load the data it's a data\nwarehouse right I think most of the data\nwarehouses are schema on read I don't\nknow much about other things but hi is\ndefinitely schema and read right so\nschema and write is typically on your\ntraditional\nsystems so run your join query see\nwhether the join happens\nah it's faster so impire will be more\nfaster I think if this is like\nthis now if hi is faster there is some\nproblem actually actually we have to\ntroubleshoot ideally we do it select so\nnow verify the data aels you do a select\nstar\nfrom out\none limit 5 so you should see the join\nuh output here and as you can see you\nare having these details from the\ncustomer data like camerone 59 actor and\nthese details from the other other table\nright like the amount and the product um\nthese things are from the transaction\ntable right you should see the join\nresult so now you have joined the table\nuh join two tables\nactually and now what I will do uh\nlogically speaking what I will do is\nthat I will use something called a case\nstatement in SQL so those who are aware\nit's easy for others what is a case\nstatement very simple I will say that\nlook at the age column okay if age is\nbetween 20 and 30 categorize the\ncustomer into something 30 40 do\nsomething so you because you want to\ngroup them right then only you can sum\nthe transactions so in SQL you can\neasily do that using something called a\ncase statement okay that's what I'm\ngoing to do so but to store the output\nprobably I'll create a table I'm just\ncreating intermediate\ntables so create this table called out\ntwo so this table is called out two and\nif you look at the schema of this table\nthere is a last column called\nlevel this column does not exist before\nbecause that is the place where the case\nwill push the result right so that level\ncolumn will have low medium based on the\nage of the customer I'll push the data\nthere\nokay and how do you write a case\nstatement this is one way to do\nit so I will say that select everything\nand I will open a case here when age is\nless than 30 I'll mark the customer low\nage is 30 to 50 middle more than 50 old\nelse others so this is how the\nclassification will happen and that will\nadd to the last\ncolumn of our\ntable now by default simple case T it\nwill go to the last column only if\nyou're writing a simple case it will\nhave the last column added where the uh\nconditions will be added whatever\ncondition you're mentioning so here uh\nI'm just saying that only low low medium\nhigh right three three categories I'm\nmentioning so see this is the case\nstatement right and Hive is full of SQL\nI can't really help it I mean there is\nno other way to write in Hive right\nwithout using SQL I cannot write\nanything in Hive right so don't worry if\nyou don't understand SQL but get an idea\nlike this is what will happen if I do it\nuh because high is extensive L used I\nmean most popular tool I can\nsay uh so that means we are in the last\npart now uh we have created this table\ncalled uh uh out to and if I do\na select star from out\nto limit 5 see now if you look at the\nlast column every customer is classified\nold middle right and now we will do\nsomething called a group by query we'll\njust group them based on Lower middle\nsum all the this thing and you will get\nthe final output this is just to show\nyou how Hive is running okay no other\num and to do that I will create a table\ncalled out three and I want you to tell\nme the query let's see how many of you\ncan tell so this is the third table I\ncreated okay uh Group by by um uh you\nknow this what is that column level\nyeah so I mean I'm just asking so what\nwe are doing here is we are simply\nselecting the data grouping it and\ncounting it and now the final table if\nyou do a select star you will see the\nfinal\nresult select star\nfrom out three so now you can see Low\nMiddle old and total amount spend so who\nis spending\nmore not low I think this is bigger\nright older\nI think older people are spending more\nactually I don't\nknow maybe\nright\nhuh based on what you are selling retail\nstore right Sports\nitems probably older people are actually\nselling uh buying more these things uh\nnow quite interestingly if you don't\nwant to open a shell and type all this\nstuff you can go to Hue there is a query\neditor and there is hive\nsame thing but you can go to this\nhi and in here you can select your\nDB yeah so it is a default database go\nback\ncancel um my database is May 17th sorry\nMay\n19th so I'm in May time and here you can\ndo the same\nthing uh\nuh\nselect so now it will do auto complete\nand other kind of things a little bit\nright just say\nPlay See it'll if you just hit play\nit'll do that\nso uh this tool called Hue is very um uh\nyou know useful actually because\num huh huh that is one thing apart from\nthat uh let's say I take a query\num so we have a table right out to\nright\nselect star\nfrom out to let's say\n50 something like this right and if I\nrun\nthis the query runs and it gives you\noptions to visualize\nsee you can visualize the\ndata I mean right this is hug\nproperty no in my knowledge no there is\na website called a GE hue.com um but\nthey are available on all the cloud by\ndefault has hue if you buy Cloud era you\nget Hue hoton Works doesn't have Hue you\nhave to separately install if you want\nhoton Works has this ambari UI is very\ngood ambari is their admin tool that has\na file manager same like this you can\nclick and upload the file that's very\ngood actually uh who is third party in\nin my\nknowledge ambar is from Apache is just\nusing it even on Apache Hado you can\ninstall ambari for so if you're having a\nplain Apache do you want to administer\nit you can install ambari but popularly\non hoton\nWorks uh because nobody uses much of\nApache hadu you are either hoton works\nor Cloud era right mapar has the worst\nUI it's called mppar control\ncenter worst I mean I'll show you the UI\nof mapa I had a mppar cluster I lost\nit\nmaper control\nsystem I don't know maybe they have\nimproved no they have not\nimproved mapar UI is called mapar\ncontrol system M MCS okay um\nyeah this is how it looks like see how\nweird I mean this is their like Cloud\nmanager you see this is their UI very\nDreadful phones and you can't find\nanything in this I mean the worst UI\never created probably I mean comparing\nit with Cloud manager and all cloud is\nway better the UI fonts everything this\nmap control system their\nUI uh anyway I wanted to discuss one\nmore thing so so far we have done manage\ntables I want to talk about external\ntable and that is again a bit confusing\nokay uh we'll finish external tables and\nwind up probably okay you had been\nthrough a tough session actually so just\ndo one thing uh I want you to do one\nsmall thing um I want you to create a\ntable\nokay so I'm copying the command from\nhere and I want to change it so just\nopen one more\nnotepad open\nsorry new\nright so this command is for creating\nthe uh transactional table regular\ncommand I just want to change it I will\nsay\ncreate external table and I will say\ntransaction records uncore\nEXT okay and then I will\nsay\nlocation I'll explain what I'm doing\nokay first let me try\nthis uh\ncontrl\nC my\ndata so I'll just copy this and see\nwhether the command is working then you\ncan also\ntry yes it works so uh you want me to\nincrease the font let me just\nincrease font\nuh amount double is there just only one\nthing so what is the difference here you\nare creating something called external\ntable first thing you can note down is\nthat you say create external table so\nthere is a keyword called external you\nare using Point number one point number\ntwo after all the schema and all I say\nlocation in location I say user GL\nfaculty that's my home folder my data\nwhat is going to happen if you run this\ncommand it'll create a table that's for\nsure it'll also create that folder for\nyou my data in Hadoop I don't have that\nfolder but it'll create that folder for\nme let me show you I I'll tell you why\nit is required if I go to my home\ndirectory my home\ndirectory\nuh where is it yeah here it is my data\ncan you see\nso this table is this folder is created\nby my table creation command I didn't\ncreate it okay now what is the idea of\nan external table when you create an\nexternal table you have the control as\nto where is the data so right now there\nis a folder called my data right and all\nI need to do is open this folder okay\nand copy my transaction files here\nsorry just copy this\nfile here I'm just just uploading the\nfile and if I\nsay select star from\ndxn records uncore\nEXT limit file I have the data so the\ndifference here is that what is the\ndifference between manage table and\nexternal table in manage table you don't\nhave any control as to where is the data\nyou just say load the data and hi will\nalways keep it in user High Warehouse\nblah blah blah in external table you say\nI want a location where I will keep my\ndata so here I mentioned a folder and I\ncan upload all my data here and that\nwill appear in my\ntable more than this so this is one\nexample where we actually use external\ntable is that customer will say that\nthey will have some data and uh we will\nask where is your data they will say my\ndata is in S3 Amazon S3 you can simply\nsay create an external table point to\nAmazon S3 location all the data will\nappear in the table the data will be\nthere I'm saying you can see that in\nyour table so any external location you\ncan keep your data and you can access in\nthe\ntable that's called an external\ntable try this if you\nwant create an external table and upload\nthe data see whether you can find it\nbecause all the times you may not have\ncontrol on the data you cannot say that\nall the time I will copy the data to my\nplace sometimes you might want to create\na table and point the data point your uh\ntable to a folder actually that is\npossible from my local system I uploaded\nto that\nfolder so once you upload the data to\nthis folder it will appear in the table\nmy data\nfolder the data has to be in this folder\nthen you can see in the table because\nyou are saying the location is\nthere see the business use case is\ndifferent I'll show\nyou I'll show you the business use case\nokay so what is going to happen is that\nlet's say you are having some DBS so\nhere I have an oracle\nokay here I have some my\nSQL Etc right and what you do is that so\nyou have some databases right and you\nlet's say run scoop you know what is\nscoop Right Scoop is your ETL\ntool you say hey scoop do one thing\nconnect with these DBS every day at\n12:00 p.m. 12:00 a.m. midnight so I can\nschedule a job everyday midnight what\nscoop will do scoop will connect with\nthese two DBS and it'll run the ETL job\nit'll pull the data from here and once\nscoop gets the data scoop has to send it\nsomewhere so I have my Hadoop cluster\nhere\nright there is my Hadoop cluster right\nwhat will do scoop will dump this data\ninto a folder so there is a folder\ncalled I don't know\nRagu so SC scoop will keep on pushing\nthe data to this folder called ra\nnow I can create an external table okay\nand point that location to\nRagu right so what will happen when I do\na select s automatically the data that\nscoop is dumping here will appear in my\ntable are you able to understand what\nI'm\nsaying so typically in an ETL leg what\nis happening is that every day you'll be\nrunning ETL jobs to bring data from\nmultiple places and all this data will\nget dumped into some folder\nnow somebody cannot go there and say low\ndata local in paath rather than doing\nthat you create an external table point\nto that folder so the moment the file\nlands it is appearing in your table very\nsimple that is actual use case of\nexternal\ntable Yeah Yeah scoop cannot do\nscheduling Uzi has to do now if you are\nasking scoop can directly dump it into\nhi tables also it is possible scoop can\ntake the data from Oracle dump it into a\nhiy table that's possible but many jobs\nare there where you get data from\nmultiple sources say you're a flume job\nif you're running Flume agent it may be\ncollecting log files all the log files\nwill dump it into some folder now I can\ncreate a simple external table where I\ncan use some serd to read the log and I\nsimply do a select star all the data\nwill appear in my table otherwise I have\nto say low data local in paath every\ntime to dump it into this table right so\nthe actual use of external table is that\nyou know you can simply point to any\nfolder and all the files will appear in\nyour table so Hive doesn't care what is\nInsider you have to make sure that the\nschema matches everything and uh some\npeople ask this what if I have a 20\ncolumn\ndata in the folder and you have a 10\ncolumn H table the first 10 columns will\ncome it'll truncate from there ideally\nokay because it just fits wherever it is\npossible and then it will remove it so\nall the files will be appended in the\ntable and shown like five files wherever\nyour records it cannot match or cannot\nunderstand it say null values I don't\nknow what it is or data types\nmismatch only five columns will come\nit'll try to fit but there is no\nguarantee right so ideally that is not a\nuse case because in a data\nwarehouse you are not supposed to do it\nactually you are not supposed to\nexperiment in a data warehouse the data\nshould be clean that is why you have\nthis cleaning pre-processing data\nFactory operations and all right then\nonly you take the data to a data\nwarehouse\nfinally um and another important point\nabout external table is this so if I do\na show tables\nnow I have this table called uh\ntransaction records and this is a manage\ntable this is my original table I say\ndrop drop\ntable\ndxn records I dro the manage table can\nyou tell me what will happen to the data\nwill it retain the data or data will be\ngone data will be gone\nsure no data will retain data will\nremain okay I'm just asking so I just\ndropped a manage table you know what is\na manage table I'm asking table is gone\nfor sure the file I manually uploaded\nright right will it be\nthere yes no 50/50 we'll\ncheck\nuser where is\nit hi\nWarehouse May\n19 do you see a uh folder called\ntransaction\nrecords the data is gone\nso manage table has this drawback I\nwon't call it as a drawback if so this\nis very common it happened once in a\nproject where I was working and that is\nhow I came to know I mean normally when\nyou work you don't learn these things\nyou learn by mistake something somebody\ndoes so we had a table very huge table\nwhich was shared among our project two\nthree projects were sharing this table\nand they had full rights that point in\ntime one of the engineer he said drop\nthe table\nand then it's gone so the pro so he said\ndrop the table accidentally he didn't\nintentionally do it but the problem is\nthe table is gone but they had some\nrecovery mechanism at that point in time\nin Hado in h there is no way to recover\nBut ultimately it's a delete from Hado\nso they had some recovery trash\nmechanism recycle mechanism and based on\nthat they recovered somehow okay uh that\nis why when you create a table which you\nto share always\ncreate an external table drop\ntable I can just use the up Arrow right\nwhat is the table name transaction\nrecords uncore EXT so I'm dropping the\nexternal table table is gone and I'll go\nto\nuser ah I can just go to home folder\nright I'll go here and we had a folder\ncalled what was the folder\nmy data the data\nremains because Hive does not have any\ncontrol on your external data you have\ncontrol you uploaded it will not delete\nso this is easy you can recreate the\ntable data will not be lost right so if\nyou want to share the data share expose\nthe table always create external tables\nmanage tables are very rare actually for\nlearning purpose we it's easy to do\nmanage table and in the operation side\nthey are all the same the queries and\neverything is same manage table there is\nno difference but always remember create\nexternal tables if you want to share\nthis okay uh this PPT has more data I\nrequest you to go through that\nalso the official website of Hive very\nvery\nimportant hive. apache.org\num and it has tons of information okay\nlike I I don't know how much it has lot\nof things actually uh you can start with\ngetting started guide on hive to\nunderstand what is\nHive\nyeah I forgot how to connect using Bine\n[Music]\num so I'll I'll show you tomorrow how to\nrun b line because I just have to see by\ndefault high server 2 runs in port\nnumber 10,000 so when you're starting b\nline you have to say jdbc connect to\n10,000 port number and then it will\nconnect with the high server 2 I will\nshow you how to do that tomorrow\nanyway I forgot so please go through\nthis hi official website it has tons and\ntons of information um lot of things are\nthere there is a high Wiki page which we\njust saw\nright and and uh just uh look at these\npoints we already\ndiscussed uh query execution via Apache\nT Apache spark or map\nreduce oh probably last bit I can show\nyou\nthe but I forgot my username and\npassword it is not here right I'll show\nyou tomorrow the uh anyway query\nexecution via T map reduce or spark how\nyou can use spark as an execution engine\nbut people don't prefer it because from\nspark they fire queries the other way\nthat's more easy you you can start spark\nSQL and say fire it but how you can\nactually use spark as an execution\nengine it is possible right and this is\nsubsecond query retrieval via Hi\nl only hoton works LP this is what I was\ntalking about LP LP enables uh asset\ntransactions faster queries everything\nthat you can think about a normal rdbms\nI mean within few but that again\nrequires resources I mean even though it\nsays very fast you it will take lot of\nresources actually uh so when you are\nrunning th th itself is in memory so\nanything which is in memory requires\nresources in memory means Ram it'll use\nonly Ram so Tas itself is fast because\nof in memory so you your cluster needs\nto have a lot of RAM for LP and all\notherwise it will be very slow ah\nprocedure so that is hpl SQL is nothing\nthat is a combin that's called hi query\nlanguage hql only you can write your own\ncustom uh functions and all inside H hi\nsupports something called UDF user\ndefined function you can write so that\nis the say it as\nhsql as a fancy way of saying that only\nright I would feel vice\nversa uh in the sense yeah see uh if you\ngood in Java map reduces your choice but\nmap reduces extinct extinct right then\nyou go to spark spark uses Java but that\nis Java 8\nfunctional Java not Java 7 so a lot of\npeople have trouble there um but even in\nSpar Community the most trusted\npreferred is python python and Scala\nthen only Java comes so Java is\nsupported in spark 100 percentage but\ndevelopers are very less people want\neither python or Scala because it's easy\nto write actually right um I would say\nsome knowledge on SQL is or highways\nvery important whatever you're doing\nright um I will I will tell you this in\nspark um in spark if you write your code\nusing Python and SQL the SQL is much\nmore\noptimized because if I'm writing\nanything using SQL I have something\ncalled\nschema right which means I can\nunderstand my\ndata what is schema if I have schema I\ncan understand my data so if I write a u\njoin and filter I can push the fil\nfilter first then do a join are you able\nto\nunderstand H in SQL I write a query by\ndefault it does right by default in ssql\nif you write a group by join then filter\nwhat will happen filter will come\nfirst because why should you load all\nthe data then finally you do a filter\nfirst you filter then do it that's\ncalled optimization that is not possible\nif you write a\nlanguage in spark we will see when we\nwrite spark code it is not optimized\nbecause spark is not having a very tight\nschema but spark SQL has so that's what\nI'm saying SQL always has a preference\nbecause of its optimization that is not\npossible in other languages like even in\nmap ruce you write you have no idea how\nis the data underlying right you're not\nmentioning any schema it has to load the\nfull data to do analysis any map reduce\nprogram you write the whole file need to\nbe loaded you cannot say that select\nonly this column and analyze not\npossible Right first all everything has\nto be loaded\nso always the structur languages are\nhaving a preference on this Java is good\nI'm not saying Java is bad but I won't\nconsider it in the further Scala\nprobably you can relate with Scala Scala\nis very similar to Java if you know Java\nScala you can very easily learn Scala\nspark is a good combination very good\ncombination uh what else you have\nquestions we'll wind up in 5 minutes uh\nafter your questions so tomorrow I'll be\ncovering uh something called U uh\npartitioning bucketing indexing and hi\nwhich are a bit Advanced topics actually\nokay but very much needed actually to\nunderstand what is hi because they are\nthe highlights of Hive if somebody want\nto learn Hive these are the points or if\nsomebody ask you about hi these are what\nthey ask nobody nobody will ask how do\nyou do a select star query right that\nnobody's going to ask they will ask how\ndo you partition your data so at least\nif you can understand the logic of it\nthat will be more than sufficient right\nand we will look at this NASA data you\nhave a NASA data see NASA so tomorrow we\nwill analyze NASA data using hi so I can\nshow show you the serd there Rejects and\nsdis and all I can explain in this NASA\ndata so make sure uh this data sets are\nthere with\nyou uh anything else let me know if you\nlook at this case study that we have\nprepared so this is about the NASA data\nanalyzing the NASA web server Logs with\nApache Hive and this will be helpful\nbecause we will see the same data in\nspark in the spark class also this data\nwill be the same data so if you\nunderstand the data now we can analyze\nit using spark as well so I guess you\nare aware of this server logs contain\nlots of information from web servers\nright and this case study will show you\nhow to derive insights from web server\nlogs so you are going to look at some\nlog files which are generated by web\nserver and if you have ever worked with\nweb servers um so let's say you type uh\ngoogle.com and then download something\nall these request will be hitting the\nweb server so you will get get request\nuh put request then HTTP status code so\nthat is how your log files will look\nlike okay I will show you the log file\nand you can actually download the data\nfor free from this URL we have already\ndownloaded and uploaded the data you\ndon't have to worry but you can it's\npublic data from NASA and if I scroll\ndown look at here before moving to the\nactivity please go through the HTTP\nresponse codes at this link right so are\nyou aware of something called HTTP\nresponse\ncodes yeah if not you can just go\nthrough this URL so for every request\nthere will be a code Associated right so\nif you're browsing and hitting a web\nserver the server will respond with a\ncode so if it is 20 it is okay anything\nstarting with two is Success 2 not1 is\ncreated 22 is accepted 204 is no content\nand all uh 30 is redirection that\nwebsite will redirect you to some other\nplace right and client errors are\n4 and 500 is server error so sometimes\nyou try to open a website and you will\nsay 501 server not found or like that so\nanything starting with five is uh\ninternal server error right um so\nGateway timeout service is unavailable\nuh something something like that you can\nsee so if you want you can just go\nthrough this list right\nand understanding the data so the data\nset contain 2 months worth of all HTTP\nrequest to the NASA Kennedy Space Center\nserver in Florida so this data is\ncollected from the NASA web server and 2\nmonths worth all the requests are there\nand I think around 1 million lines are\nthere in the data it's pretty huge data\nactually somewhere around 1 million\nlines the log files are stored in the\nApache common log format so that's what\na lot of people ask me\nuh when you say text file it is not with\na txt extension normally you say text\nfile that is something with txt but\nthere are lot of other data which is\ncalled text file it is\nalways not mandatory that you you will\nhave a txt extension right so this\nApache common log format it is a text\nfile but you cannot open it in notepad\nright you can open it in wordpad or any\nother thing and it will show you\nproperly like text Data I'll show you\nthe\ndata and now let's look at the data so\nif I go here there is uh NASA access\nlogs and this is the data I'll say open\nwith\nand I'll say word\npad say\nokay uh so this is how the data looks\nlike and it's not really pretty I mean\nin the sense like it's not really nice\nuh it's not\nstructured um and what are the fields we\nare interested in in this right so this\nis log file and if you look at here the\nPDF right uh we have the host identity\nuser identity time request status size\nif I scroll down the first field is the\nhost making the request okay so where is\nthe host making the request this is the\nhost this guy in24 do inet something so\nthis is the uh fully qualified domain\nname or the host name host making the\nrequest then what we have next two are\nthe user identity from remote and local\nmachine these are unavailable so next\ntwo fields are not used they are dashes\nthat's fine and then we have the time\nstamp in day month uh hour so year this\nformat so here we can see\nthis so this is your time stamp right\nthe time when it is making the\nrequest time Z on so this is this minus\n400 is your time zone what you see here\nis the time zone okay and then there is\nthe a request send to the server so here\nyou can see get it's a get request okay\nand somebody is trying to get shutle\nmission status news some news something\nokay so that's a get request we have and\nthen there is an HTTP Replay code uh\nHTTP Replay code is 200 it's a success\nokay and then you have the size of the\nfile received so probably in bytes 1839\nso this is the data h so you can see\nthat a lot of people are uh sending\nrequests to the NASA web server and\nthat's how the data looks like now what\nwe want to do we want to create a hive\ntable to load this data and then we want\nto write some queries and the size of\nthis file is 167 megabytes so it's a big\nfile actually not very small and we will\ntest now how the cluster will behave\nbecause if you start writing the query\nall of us write the query we'll see how\nefficient the cluster is anyway right um\nand so the next question is that how do\nI actually load this data into hi\nbecause this data does not have any\nstructure to do that we are using the\nserd or the serializer deserializer now\na little bit uh uh information to you I\nalso want you to give you an\nassignment so if you simply go to Hive\nand say Sir\nD yeah you can see here this is called a\nserd so what is a serd serd is a sh for\nserializer deserializer like I said if\nyou are having data which is not really\nstructured like this log file or or Json\nfiles or anything you want to read and\nwrite you can use this parser called\nserd right and hi supports a lot of\nbuil-in\nSES so these are the buil-in SES we have\nand we are going to use this guy\nRex you are aware of uh\nRex regular expression right so that\nmeans you can write a regular expression\nhigh will par this parse it and using\nthis s it'll read it there is an orc s\nAO these things we'll look at\nlater and then custom SES you can write\nyour own series if you want that's also\npossible right um we will look at this\nlater let's open this uh\nrexer yeah so it says uh if you want to\ncreate a table and use the serd you have\nto say row format serd remember\npreviously we used to say raw format\ndelimited Fields now I say raw format\nserd and then I will say this is the\nsdy okay and then you say with Seri\nproperties and you will say input\nregular expression whatever regular\nexpression you have you have to put\nthere and that should actually be able\nto get the data whatever rejects you are\nwriting and then it should give the\nstructure to the data this is the idea\nright um and this is very important\nstored as text\nfile I mean I think yesterday I didn't\ndiscuss it when you create a hive table\nby default this option is there called\nstored as text file which means whatever\ndata you are loading into the hi table\nit will store it as a text file regular\ntext file be it a CSV file or any file\nso yesterday we loaded some customer\ndata transaction data and they were\ngetting loaded as it is okay now you\nhave other options you can say stored as\norc par and all that is where the\nImprovement comes and I will talk about\nit what is the difference in when you\nsay text file or when you say orc file\nwhere is the difference but as of now\nwe'll say is stored as text file we are\nnot\nbothered It Is by default text file ah\nso that is why yesterday we were not\nmentioning it it is by default text file\nnow the data set is available in this\nlocation uh can you see here forward SLG\ndata and\nuh\nhuh one moment uh what was the name\naccess\nright yeah yeah so this is the file it's\ncalled access there is no extension and\nit is available in a common folder\ncalled GL data so first let's do one\nthing let's start Hive anyway we need to\nstart\nhi so start\nHive and let's create a table so we will\nuse the DB yesterday we created I'll say\nuse may9 that was the\nDB right so you can switch to whichever\nDB you\ncreated and this is the command to\ncreate the table you can copy paste this\nfrom the PDF okay say copy and I'll\nexplain what it\nis so you will say create a table if not\nexist NASA log and the fields we are\ninterested are\nhost um identity user identity time\nrequest status size and then you'll say\nrow format serd we want to use the\nreject serd with SD properties you can\ninput the regular expression so this is\na regular expression we are using and in\nthe output we will just say that U so\nthat's part of serd you will say that\nstore it with equal uh spaces right and\nthen you say stored as text file so this\nis where you're saying that store it as\ntext file so this is your regular\nexpression huh\nuh you can go to the\nPDF and if you cannot copy like this\nright click say select\ntool there's a select tool once you\nselect it you can left click and copy\nlike\nthis so you have to have some idea about\nregular Expressions if you want to write\nit from scratch but that's\nokay yeah sure uh um I'll show you from\nconsole\num yeah this is the command I just said\nhi then use the database and then just\ncopy pasted the table creation Command\nright now we have not loaded the data\nyou don't have to load the data that\noutput format string it is simply saying\nthat there are eight columns right and\nwith space uh each column with space uh\nit's a bit difficult to explain\nuh it actually reads and separates all\nthe data with how many spaces you have\nand it is looking at digits and\nstrings uh and then extracting the\ninteresting Fields it's a bit difficult\nto explain the Rex\nactually so if you are writing a Rex\nfrom scratch uh you have to validate it\nso this was validated even I don't\nremember how I validate I wrote it long\nback actually yeah so it's a create\ntable\nyeah normal table only difference here\nis that in the table properties we are\nsaying that uh whatever data that comes\nto this table apply the serd the regular\nexpression otherwise it cannot recognize\nthe data inside right so right now this\nis the Apache common log format right\nand Apache common log format has a fixed\nuh schema not schema the way the log\nfiles are generated so you can write it\nin multiple ways this is one way of\nwriting it so in hi if you look they\nwill simply say that this is the Rex we\nwant here I'll show\nyou huh so here it is a input reject see\nthe output reject is not stored so here\nyou can see the input reject right are\nyou looking\nhere I mean this was written like if you\navoid the output string also it will\nwork but we are just ensuring that\neverything looks string I mean just we\nare manually saying that it is string\nthat's all I think it is not updated H's\ndocumentation is not updated no it is\nnot\nupdated let me\nsee\num so where is\nit more about Rex 30 can be found here\nso they have given two links actually\nprobably it is here let me just check so\nwhat happen happens is let me just see\nif I can find it\nfirst H\nsee in in open- Source communities what\nhappens is that let's say you are\nlooking at Hive or spark or anything\nsomebody will create the product so\nlet's say I created Hive and I will give\nit to everybody everybody will start\nworking and then will what will happen\nsomebody will say that something is\nbroken so I may not add it in my\ndocumentation I'll add it in a J jira is\nyour issue tracking right it's a jira\nlink in jira link it clearly says that\nthere is input Rejects and output format\nstring you should use so somebody added\na uh yes I will work on adding a serd\nhow to and some example is a new\ndirectory so somebody said that uh this\nis not properly formatted so in the J it\nis added actually it is not an official\ndocumentation originally when they\ncreated probably that property is not\nthere I'm saying then somebody could\nhave complained and but they have\nmentioned the link here right you can\nsee that it is here if you look at here\nit says more about can be found here I\ndon't know why they're not updating the\ndocumentation but it is there anyway so\none problem is this if you want to\nsearch documentation you may not find\neverything in the official documentation\nsometimes so I always look at jira\nbecause any issues they fix will be in\njira and also not everything not every\ntime things will not work the way you\nare expecting so you have to Google gole\nsaying that high plus jira not able to\nread this file some some issue you are\ncoming across and there'll be a j ticket\nfor that always right so once we now you\nhave to load the data so let's continue\nwith that and where is the location the\nlocation is here GL data right so how do\nyou load you say\nload\ndata wait wait\nwait wait wait wait if I do load data in\npath what will\nhappen I say load data in path what is\ngoing to happen we can create an\nexternal table right that's better I\ndidn't think about it because normally\nthe participants will have the data in\nother classes and they will upload\nthemselves so what do you want you want\nto create an external table everybody\nand point to the data that will do okay\nso then you have to modify this so how\ndo you modify\nthis if you want an external\ntable you will say\ncreate external\ntable let's drop the original table okay\ndo a drop first do a drop drop table\nwhat is the\nname NASA log so drop\nit and now we say create external table\nNASA law these are all fine and the only\ndifference is location\nSLG\ndata um wait wait wait\nwait external table if you're creating\nit should point to a\nfolder this will load everything from GL\ndata we don't want that\nright in G data you have lot of folders\nright so I will move it I'll see if I\ncan create a folder in G data let's see\nnew directory I call it as\nNASA yeah and I'll move it inside here\nright that should make\nsense see you have to take care of\neverything not this right this one right\naccess action move\nto GL data\nNASA\nright there is no NASA folder yeah there\nis move fine so ideally now if I check\nNASA it should be\nhere H now it should work otherwise it\nwill load every file from the GL data\nfolder we don't want that\nright so what should be your\nlocation this should be the location\nright and let me see if it\nworks uh no you can use small letter or\ncapital\nletter so the table creation will work I\nknow that my question is what about the\nquery select star\nfrom works so you have to try this and\nuh let me know\nit works but all of us are accessing the\nsame data\nokay all of us are in the same\ndata so make sure you can see see\nproperly\nstructured now I already created a table\nfirst right and I dropped it because uh\nhow do you load data to that table we\nhave a shared location so it is better\nto create an external table to point\nthere so I dropped it and recreated the\ntable so here it is\nlocation and also here is a location\nthis also should be there this external\nyou can type small letter or capital\nletter folder name should be Cas\nsensitive I\nthink and and do a select star and let\nme know whether you able to see the data\nso there are two approaches not every\ndata can be made structured also there\nis one problem problem so Hive is a\nstructured tool that means somehow you\nshould give structure then only it will\nwork there are some type of data like\nfor\nexample there is a format called\nAO AV so previously when you get AO data\nbecause that's like unstructured you are\nnot able to do anything but now there is\nan AO reader they have created so there\nare still some type of data where you\ncannot give a structure one point second\npoint\nhow this works in organizations is that\nlike I said there'll be a data inje team\nright so I'm just giving you the example\nof GE in GE what happens see ge's\nbusiness is so enormous that nobody has\nany clue what is happening there G is\none of the biggest firms in the world\nactually right and their size of data is\nenormous what they are getting so uh\nthey have Financial applications okay\nthese Financial applications will\ngenerate data okay and how do you get\nthe data they will have apis they will\npull the data in Json format so it comes\nin Json format and sensor data they get\nsensor data uh so they have this\nlocomotive engines train engines in\nEurope G has created so these train\nengines whenever they are running will\nproduce sensor data they collect it and\nthat data will come through Kafka it\nwill come in a AO format through Kafka\nto Hadoop ultimate land on Hadoop but\nthe data is sensor data so it's like\nevents sensor data will come in events\nformat like whenever there is an event\nit will generate a data it's text file\nonly but the format that they get is\ncalled AO so the point is that data when\nit lands on Hadoop nobody can give any\nstructure to that\ndata nobody on Earth can give so G has a\ndata inje team their job is to take the\ndata give some structure and give to the\nBig Data team you're getting my point\nright because if you directly give it to\nBig Data team they have no clue what to\ndo with the data so this there will be a\nstaging area where all this data lands\nokay and they had written some Java code\ncustom Java code what that Java code\nwill do it will read the AO remove some\nunwanted fails give some structure send\nit to next side so so that is how they\nare injecting the data I'm saying so row\ndata most of the times you may not be\nable to directly handle it\nand Second Use case is if you are a big\ndata developer or like us we don't want\ndata in the original format but if you\nare a machine learning guy you want the\ndata in original format are you getting\nthe difference like if I'm a data\nscientist I don't want anybody to touch\nmy data I want the original data with\nall the nonsense then only I can build\nmy machine learning models because they\nwant all the features they don't want to\nmodify the data but if I'm a big data\nguy like like this I am happy with\nstructure I can lose some data I just\nneed the structured format I will work\nso these two teams will be there so the\nmachine learning teams will be original\ndata they'll be collecting and then they\nwill have their own algorithms to Crunch\nthe data Kafka is used because their\ndata is coming from\nEurope uh streaming data they don't\nstream live they stream once in a day or\nsomething I mean they collect all the\nsensor data huh it's an Amazon the\nentire setup is an Amazon so there is\nsome sensor which uh uh in Amazon you\nhave a solution right who is working on\nAmazon sensor data you have a solution\nAmazon what you\ncall Kinesis Kinesis right Kinesis is\nyour sensor platform in Amazon so\nKinesis will collect all this data and\nfrom Kinesis will push to\nKafka because somewhere it has to store\nright the data there are very less\nreal-time decisions they want to do on\nthat data because there is train data\nand and only time when they're analyzing\nthat data is if the train is not working\nlike it crashed the train engine got\ncrashed otherwise they don't want to\nanalyze it in real time 99% of the time\nthe trains are running so this data is\nuseless for them so when the train\nengine got crashed or something happened\nthat time they want to look at the data\nlast 24 hour what caused the crash so\nthey are not doing because the size of\nthe data is in\npetabytes so analyzing it is also not\npossible for them so they collect it\nreal time and batch it and send it once\nin a day or something and it will go\nthrough Amazon it will land in Kafka why\nKafka because from Kafka two three teams\nwill collect it one team is using the\ndata for something else distribute it\nand then one copy will be stored in Hado\nlike original data copy anybody want\nthey can get it that is how their\narchitecture is right now G's isn't\nevolv in an architecture they are just\nmoving to Hadoop now it's not a stable\narchitecture so probably after a couple\nof years they'll find another way to\nbetter this things and all right now\nthey're not doing real time on the uh uh\nthis train data they also have flight\ndata\nflight this aircraft engines are all G\nright commercial aircraft all are G\n80% and all these engines generate same\nsensor data they collect it but again\nthat's all not real time it's very rare\nyou get a problem and that time only\nthey want to\nanalyze so uh I hope you are able to\nreach here right till this\npoint ah so this is the place where your\ndata is there right if you go to this\nlocation I have copied the data there\nnow if you look at the PDF what it says\nload the data select star now these are\nsome of the use cases I wrote I mean you\ncan also write your own use cases so\nfind the top end points that received\nserver side error\nmeaning which are the uh how many uh\nendpoints have received uh server side\nerror right I'm just asking like there\nis 501 uh 5 these are all servers side\nerror and how many times it has occurred\nso how do you figure it out there are\nmultiple ways to do it but one of the\nway is this you can say\nselect status count of request from the\nuh what you say NASA log table Group by\nstatus and you can say having status reg\nregular expression extract so here I'm\nsaying that uh anything starting with 5\nin the status column because 5 is server\nside order by status descending limit\nfile so that's the query so I just want\nto know all the errors starting with 5\nand count them how many are there that's\nwhat I'm writing now when you are using\nhaving it has to be double uh operator\nassignment in having close in hi when\nyou're using having close it has to be\nassignment something if you say equal to\nsomething equal to something it will\nbecome a equal to one it will become a\nbecome ones so here I'm just saying just\ncomparing that comparison operator is\ndouble equal\nhuh no in having only I'm saying we\nclose and all it will be\nnormal in having you will say so\nsomewhere there was a documentation on\nthis I'll share it with you okay can you\ntry this query and see now it is taking\ntime 24 seconds only for the ver anyway\nthis will take some time can you guys\ntry the query it'll be slow\nso when you mention the commands it has\nto be either full small letters or full\ncapital letters he said capital L\nlocation and it's a bug I know this\nbecause because otherwise it should\nthrow an error it won't throw an error\nuh and this is a very common thing if\nyou create external tables you mention\nlocation um even if that location does\nnot exist it may not throw error and you\nwill try querying nothing will come up\nyou will think what is a problem space\nis fine but ideally this location\nkeyword sometimes it has a problem so\nthat is what I did it started working\nbut you're getting or maybe maybe that\nitself is not a problem I dropped it and\nrecreated probably it started working\nnot sure so how much time it is taking\nfor you to run the query 75\nseconds uh there is also one more Point\num which I forgot\nyeah so\nanyway so somebody was asking about this\nreference on regular expression here you\nhave a hi language manual it is in the\nPDF\nand which uh resource is requested most\nfrequently by The Host this is a good\nquestion right because which page is uh\nrequested the most and I don't think\nit's a very difficult query but still\nwe'll run it just to\nsee so uh now if you look at here it\nsays select uh request comma count as\nrequest count from NASA log group By\nRequest order by descending limit 30 you\nare looking at the top 30 requests\nactually\nright you can skip header and all but I\ndon't think you can skip the data part I\ndon't think you can skip columns no\nI I don't think you can filter because\nultimately it's on Hadoop so the data\nremains on hadu right so that's one\nproblem because if it is an\nhdfs uh then there is no way you can\nfilter anyway locally I have to see\nwhether he is asking while loading the\ndata can I filter the data like I don't\nwant all the 10 columns I want only you\nyou have skip header that option is\nthere table Properties by the way there\nis something called table\nproperties um if you just go to Google\nsay\nhi create table TBL\nproperties so in creation of the table\nthere is something called table\nproperties where is it\nhuh so table properties right so here\nyou can mention um uh the uh you know\nSkip header and all let me just check no\nAuto compaction mapper\nmemory Auto Purge external\ntrue you have skip uh header is there\nthat's for sure but I don't think you\ncan skip other columns that is not\npossible I think anyway I will just see\nif I can get back to you on that\nokay and there are some more queries I\nhave written you can try that like\ndisplay the top 10 host who made maximum\nrequest and all uh you can try that find\nthe total count of different response\nCotes returned by the server so just try\nthese queries let me know if they are\nnot working\nI have not actually tried everything it\nshould work\nokay so now uh talking uh about Impala\nlittle bit right probably not in depth\nbut you should know a little bit of\nImpala because it is heavily used in\nindustry imala if it is in the cloud\nside we are on cloud side if it is hot\nand work side everything is hi but in\nthe clera side we have something called\nImpala right so a small intro not much\nbut and I will show you an Impala query\nalso so what is going to\nhappen if you're having data nodes right\nso this is a data\nnode this is another\none and this is another one\nuh you can install something called\nImpala right and if you using a cloud\ndistribution Impala will be by default\ninstalled you don't have to worry and\nImpala is its own SQL engine it does not\ndepend on hi or anything right so when\nyou install Impala what will happen\nthere is a demon called Impala\nD Impala D It's called Impala demon it\nwill start running on every data\nnote like something like Oracle I mean\nI'm just saying right so this Impala D\nwill be running on all the notes first\nof all right and Impala has a shell from\nwhere you can fire the query it does not\ndepend on H and if you fire a query what\nis going to happen is that uh let's say\nit hits this machine so the query will\nbe accepted by one of the machine so\nlet's say this guy right and this guy\nwill look at the query now Impala can\ntalk to hive's metast\nstore which means whatever tables you\nare creating in Hive Impala can see so\nall the tables you create you can query\nusing Impala so and whatever tables you\ncreate in Impala hi can also see because\nthey share the same meta store so that\nis one advantage so and on top of that\nanother thing is that when you write a\nhi query it will convert that into map\nreduce so hi is not bothered about how\nthe query is running\nbecause it is a responsibility of map\nreduce to run the query Impala will not\nconvert to map reduce Impala has this\nImpala demon which will run the query\nfor you it is like a rdbms I can say not\nan rdbms I'm saying like if you're\nfiring a Oracle query who will run the\nquery Oracle will run the query same\nlike that if I fire the query this\nImpala will get the query and it is the\nresponsibility of this Impala Dem s to\nexecute the query so they will have the\nuh hi\nmetadata\nokay they will also have the block\nmetadata\nmeaning if you create an Impala table\nlike a hive table and you say load the\ndata right now the data is actually in\nHadoop and the data is in blocks and\nreplicas now who has the metadata about\nthis name node these guys will copy that\nto themselves because they should know\nwhere is the data Impala is not going to\ndepend on anything to fire the query it\nwill run itself right so that is why the\nqueries are very fast and when the query\nhits Impala it runs in memory so there\nis a drawback so let's say the data was\nhere you have a block here you have a\nblock here what will\nhappen you will have\nRam this block data will come here and\nthis block data will come here\nRAM and this guy will simply coordinate\nthe query because the data is here and\nhere right so this guy will coordinate\nthe query and it will run the query in\nmemory the drawback is if let's say this\nmachine crashes the entire query will be\nabouted it is not fall tolerant but High\nqueries are fall tolerant because map\nredu is fall tolerant there is no way\nmap reduce can fail right but imp\nqueries are not fall tolerant so if the\none of the machine because it is a\ndistributed query not one machine is\nrunning the query probably 10 machines\nare running the query and if any one of\nthe machine\ncrashes the query will be abot it but it\nwill not convert to map reduce or do\nanything so your queries are super fast\nactually directly hit hdfs query the\ndata and give you the output whatever\nyou want so it's faster but this\nreliability uh issue is there with\nImpala so what we do if you're are\nhaving a cloud cluster if you having ETL\njobs we never use Impala because in ETL\njobs behind the scenes it'll be running\nhi query and probably the query will\ntake 3 hours to run 4 hours to run\nprobably 5 hours that's fine even if\nduring that ATL job a machine crashes I\nI'm safe because it's a hi query my map\nreduce will take care of it but if I run\nthat ETL query using Impala let's say\nthe query will be faster 2 hours\nprobably it will take but after 1 hour\nif one of the machine crashes my Impala\nhas to restart the whole\nquery it'll throw error but again your\nETL job has to start from beginning you\nlost one hour right so actually it needs\nHigh meta store for the metadata Impala\nmust use hive's meta store so Hive\nshould be installed so that is another\nAdvantage U hi will remain because lot\nof tools requires Hive even spark SQL\nspark SQL normally when you configure\nspark you will tell spark SQL use H\nmetastore so that H metast store is used\nby everybody so all the tables you\ncreate all will be in one place right\nand if the table is there you can either\nquery using Hive query using Impala\nthat's up to you to decide but Impala\nalso has some drawbacks this NASA table\nwe created you can't query using Impala\nit is this Rex is not\nsupported so this table is created using\nRex SD that is only for hi I can see\nthis table in Impala but I won't be able\nto query I think if my memory serves me\nwell but most of the regular tables you\ncan query without any problem both of\nthem can access at the same time because\nmeta store is a shared\nplace what is meta store it is your\nMySQL it's a shared Place anybody can\naccess not only high and Impala 100 of\nother people\naccess it need not run you have to\ninstall hi set up a meta\nstore I'm saying whatever table you\ncreate in Impala it stores in highest\nmeta\nstore so each data node will have an\nImpaler demon and each demon will have a\ncopy of house meta store block data so\nand what happens if you fire a query any\ndemon can pick it up that's called a\ncoordinator meaning if I write a Impala\nquery there are hundreds of data nodes\nso any data node running Impala can get\nthe query and that node is called a\ncoordinator for that query so in this\nexample this is the coordinator because\nthis guy accepted the query but this\ndoes not have the data that's fine okay\nand once it gets the query it does a\nlocal lookup so it's very fast it has\nthe metadata it understand immediately\nhere is the data here is the data it\njust splits the query and streams to\nhere and here these run the query\ncollect the result to display to\nyou so ideally a lot of machines will be\nthere where you are having data but the\nmetadata lookup is very very\nfast there is no frequency you to\nmanually refresh it it will not\nautomatically copy meaning if I create a\nhi table it will not appear in Impala I\nhave to refresh the metadata there is an\noption in Impala where you can say\nrefresh so if you feel that there is\nsome new table you say refresh then only\nthe metadata will come a block metadata\nso that will come once your table\nmetadata is created say I'm creating a\ntable H once I create a table I load the\ndata then only the metadata of that data\nwill come it will not store the whole\nHadoop\nmetadata your Hadoop cluster has let's\nsay th000 terab\nfile okay Impala will not store th000\nterab metadata that is useless\nright you saying this Hado cluster has\nlet's say th000\nfiles okay in which you created an\nImpala\ntable okay you say Impala table you\nloaded file\none it will have the metadata of only\nthis file why should it need all the\nmetadata the query will hit only this\nfile\nright ah only related to that table I'm\nsaying when you create a table you will\nsay load the data what data so in our\nexample we loaded the NASA data so the\nblock information of that NASA file it\nwill\nremember ah from hi or if you're\ndirectly loading into it it will\nremember itself it won't remember the\nblock information of other\nfiles in Hado you have a lot of files\nright not all the files are in\ntable are you getting my point what I'm\nsaying it'll remember only the metadata\nof the files associated with the table\notherwise why it should remember all the\nfiles manual refreshment is required I\nmean block it may remember I'm saying\nthe table properties it will not\nautomatically refresh so you have to\nmanually refresh every time so usually\nwe won't do like that usually even\nthough Impala and H can communicate we\ndon't do it because hi has its\nfunctionality Impala has its\nfunctionality meaning whatever tables\nyou have in Hive normally they will be\nused for reliability like ETL jobs and\nall and that hi will handle Impala will\nyou you will create Separate Tables you\nwill say do the same thing load data\nlocal in paath blah blah blah and that\nwill be handled by Impala only like\nSeparate Tables they can talk to each\nother but what I'm saying if you want\nthe up to-date information you should\nsay refresh then only you can get it and\nthere are two three types of refresh I\ndon't remember completely there's a\ncomplete refresh which will redo and\nload the total metadata from high meta\nstore blocks everything there's a\npartial refresh you can do it in Hue so\nthese things are available in h I'm not\njust saying that I mean you will be\nwondering so if you go to query editors\nyou can see Impala right so here is\nImpala so probably the table you created\nright now should come in Impala that's\nthe logic right so if you go back where\nis the May 17 May 19 this is my table\nmy DB right see customer NASA log these\nare in\nImpala and if I do a select star from\nNASA log I don't think you can see it I\ndoubt five I doubt you will be able to\nsee\nit maybe I don't remember\nexactly huh no\nuh fail to load metadata uh fail to load\nmetadata invalid storage description\nImpala does not support this table type\nreason SD Library not\nsupported it won't support that SD\nthat's why you are not able to see\nthat no no it won't that's what I'm\nsaying Impala tables if Impala is\nquaring your data it's not fall tolerant\nso what the ETL will do it will query\nand get the data who will query that's\nthe question if high is squaring it is\nfall tolerant so here you can see the\nNASA log thing is not supported but uh\nif I look at any other table so let's\nsay\nselect count\nstar from let's say out one right if I\ndo this ideally it should\nwork for some reason imp is a bit slow\nin this this uh hi is faster in this\ncase it is not really like this actually\nit's very\nfast there is no map reduce it directly\nhits and gives a result the same query\nif you run you will see map reduce job\nstarting map reduce nothing ah so we\nwill uh and there is one more thing this\nrefresh manually has to do normally we\ndo it manually uh you can write a batch\njob to refresh so I don't know whether\nthe refresh is configured here or not\nfor the Impala because this NASA lock\ntable is available we just created it\nright now but it is available in Impala\nso probably and this is the option to\nrefresh uh can you see this refresh\nthree options will come clear cache\nperform incremental metadata update this\nwill sync missing tables in hi which\nmeans if you have a new table in hi that\nis not here it'll do it and then there\nis invalidate all metadata and rebuild\nit this can both be both resource and\ntime iny when you say invalidate it will\ndelete the whole metadata and from\nscratch it will build that'll take a lot\nof time so these are the refresh options\nin Impala that you have so try to create\na table see whether you can find it in\nImpala I don't know so we'll create some\nsimple table\nright which table you want to\ncreate maybe I'll just change this and\nrun it as it is right\nso I just created and if I go to\nImpala May\n19th where is\nit no I don't have it so if I say\nrefresh uh let's say perform incremental\nrefresh H now it came transaction record\nso ideally we do this manually because\nimp cannot automatically identify you\ncreated a table in high if you do it it\nwill come\nIDE huh so Impala also stores a lot of\nmetadata uh and actual data recent\nqueries Sol in the cache so if you want\nto clear it you can say clear cach the\nreent queries you ran it'll store it in\nthe huh in the cash it will store so\nsometimes the cash will become big like\nyou're running a lot of queries so that\nwill affect the performance so you can\nsay clear the cash yeah ideally it\nshould keep it in the cash hi does not\nuse Impala hi uses only map ruce no I\ndon't think that is possible probably\nadmin or somebody can do it from a\ndeveloper side I have never uh I mean\nwhat is a use case I don't know I mean\nmasking table masking is there um I\ndon't know if it is possible I have\nnever tried huh so table masking you can\ndo so that has to be done by the admins\nso you can say that mask the table it\nwill not be visible to any other process\nlike Impala or spark or anything if you\nwant H but usually we won't do like that\nusually we restrict the access based on\nuser so the user so if I am a guy okay\nso all the hi tables will be there in\nImpala let's say but if I a guy who is a\ndeveloper and my access will be that I\ncan see only these tables in Hive or\nthese tables in Impala so behind the\nscenes everywhere you will have tables\nbut user level access you can control\nwho can see what tables otherwise\neverybody will be able to see all the\ntables right I don't think directly\nthere is an option to stop them\ncommunicating ah hoton Works Hado\ndoesn't not have Impala uh they have uh\nthis thing Hive is there and by default\nhi plus th the is the execution engine\nuh then LP LP is their uh highlight\nactually LP is much reliable much faster\nthan Impala to be honest right so I'll\nshow you a little maybe not now I'll see\na\nsession so this part is clear right Hi\nand hi plus Impala at least Basics a h\nhas an automatic way of identifying the\ndata but that will work only if you have\na proper delimiter like comma or space\nin Hue you can create a table there it\nwill identify if you have space or comma\nor something uh I don't think geospatial\ndata means what is the structure so impa\nI mean this much only you need to know\nas of now I mean I'm just giving it as\nan option normally in Hive classes we\ndon't teach Impala at all the s why hi\nis hi Impala is\nImpala H not subject like it has become\nlike so people don't learn it separately\nin the project because the queries and\nall are\nsame the SQL you write everything is\nsame there is no difference at all only\nthing is the use case is slightly\ndifferent uh no only it has its and it\nis contributed by claer Impala is an\nopen source project but mostly uh the\ncontribution is from claa so it is like\na propery product so if any issues comes\nclaer will support a lot in Impala Point\nnumber one and so another problem is\nthat if you go to other platforms you\nwon't see imala probably that is the\nreason people don't care much about\nImpala right so if I go to hoton works I\nwill not see Impala at all\nright if you want security there is\nsomething called Sentry service Sentry\nyou have to set up Sentry I don't think\nwe have Sentry here so in clouder I'm\nsaying clera you have something called\nSentry Sentry\nclera so this is called uh Sentry here\nyou can set up all the uh you know user\nlevel aess access service level access\neverything ah centry again Sentry is\nopen source but so here you have users\ngroups uh and uh access see\nauthorization privileges model for\nhighand Impala so the problem is uh\ncentry is again Apache but only Cloud\nuses you go to Hoten works you have\nknock and range two tools one is called\nRanger and\nnox again they are open source but only\non hotworks so one problem is a platform\nwise there are some slight differences\nIan it happens so there are slight\ndifferences Cloud uses this Sentry and\nin Sentry you can configure who should\naccess which table and and all the\nauthorizations the admin has to set I\ndon't think Sentry is installed in our\ncluster which means anybody can see\nanything even hi for hi you have\nauthentication username password you you\nwill create ah here that is not there I\nmean as of now we are just saying hi it\nwill enter\nright H Karo is by default I'm saying\nright now what you're doing you are\ngoing you logging into one of the\nmachine in the cluster it doesn't work\nlike\nthat are you getting my point so I think\nwe discussed this in the first class so\nthis is your Cloud lab right\nthis is your Cloud lab and you have an\nedge node so you log on here from here\nyou connect to the lab and any security\nany service you need you have to enable\nhere and as of now nothing is enabled so\nwhen you just start high you just hit\nthe cluster and anybody can see\nanybody's table or anything that you\ncreate Sentry is not enabled I think and\nthat even when you're working you won't\nhave access to Sentry and all it's\ntotally the admin side they only can\ndecide who should you can request them\nyou may be not able to see who has\naccess yes yes so they will be in the\ncluster but once you hit the edge not\nthat will be applied to you like all the\npolicies whatever you're having\notherwise you can't restrict the users\nright from coming\nin uh hon Works has Ranger\nRanger hoton\nWorks Ranger is\nsimilar I also like the hoton works\ndocumentation a lot it is much more\nprecise and clear you read you'll\nunderstand what they're talking cloud\ndocumentation very weird nobody will\nunderstand what they mean look at H\ndocumentation very easy to read and\nunderstand see comprehensive security\nfor Enterprise hard what Ranger does how\nRanger works right everything you have\nhere h\nso Apache Ranger offers centralized\nsecurity for all these things hi so if\nyou go to Hive uh they will show you uh\nwhat Hive does see there is even a\nYouTube presentation very nice\ndocumentation they have the best\ndocumentation actually hoton\nworks see LP live long and process is\nwhat I said I think there is another\nexplanation it's not actually live long\nand\nprocess uh something else is there aut\njust say it is live long and process I\ndon't think it is live long and\nprocess so hi details are here and some\noptimization techniques are here okay H\nhoton document is the best if you want\nto learn only thing if you're in a cloud\nproduction cluster some of the things\nmay not be applicable since it is\ntotally open source like they have their\nown ways right so if you are using a\ncloud era cluster in production you\nshould always ref for cloud data\ndocumentation because not everything\nwill be same some difference will be\nthere right uh I\nhope that clarifies uh Impala so we're\ntalking about\nImpala um now I have to talk about\nsomething called partitioning in Hive\nokay so how many of you are actually\naware of partitioning some of you might\nbe aware of this Con concept yeah for\nthose who are already know this probably\nit's a refresher right see the idea is\nvery simple so can you tell me so your\nmanager asked you to create a sales\ntable okay and U you are creating a DB\ncalled retail and a table called sales\nso where will be the location let's say\nmanage table uh it will be\nuser\nhyve then what\nwarehouse and the DB is\nwhat\nretail and the table is what\nsales this is the location right for the\nmanage table and let's imagine every\nmonth you are getting the data month end\nso in January you got the data so it\nbecame what jan. CSV you said load data\nlocal blah blah blah this is where the\ndata will go you know this already and\nthe data got uploaded here everybody's\nhappy no problem the problem is as the\nnext month and next month C happens\nright what is going to happen in your\nsales folder in February what will\nhappen February data will get uploaded\nit's in the same folder by the way and\nagain what will\nhappen March\nright and\nApril and May and so on so the problem\nis if this is the model you are\nfollowing after after let's say couple\nof years if you look at the folder you\nwill have 10 20 files inside the same\nfolder now the real problem is let's say\nyou are writing a query something like\nthis\nselect star\nfrom sales\nright\nwhere let's say month equal\nto\nFeb let's say you're writing this query\nthe problem is Hive has no clue where is\nyour data it has to scan all the files\nin this folder to give you the result\nthe result is here that you know H\ndoesn't know when you write a query it\nwill scan all the folders in the sales\nfolder so if you are having let's say\n100 files Hive has to scan all the 100\nfiles before producing the\nresult the drawback of this is that your\nqueries will be very very\nslow so in order to avoid this problem\nif this is the problem you're facing you\ncan do something called\npartitioning partitioning of table okay\nand in partitioning you have two things\nthere is something called Static\npartitioning and dynamic\npartitioning static and dynamic you\nhave two type of partitioning you can do\nokay and I will show you that with the\nhelp of an example that will be better\nbut what is the idea in partitioning\nfirst you have to identify the column or\ncolumns based on that you want to do\npartitioning so I'm assuming that this\nis my table structure and most of my\nqueries are based on month equal to\nsomething right so what I can do in hi I\ncan say that okay create a partition\ntable where the partition column is\nmonth and what hi will do if you provide\nthe data it will automatically identify\nand\ncreate folders like this and we'll place\nthe data like\nthis so when you partition based on\nmonth okay there are multiple ways to do\nthis I will show you technically what is\nhappening behind the scenes is that it\nwill identify the month column and as\nmany months you have those many sub\nfolders will be created and it will move\nthe data now we know that there are only\n12 months in an year so if this is the\ndata model you will first partition\nbased on year then month so that it will\ncreate a folder called 2016 within that\n12 subfolders then 2017 within that 12\nso but if this is the model if you write\nthis query it will only hit\nhere will skip all these other\nsubfolders so your queries will be\nnaturally faster this depends on what is\nthe columns you are having so let's say\nI'm saying here the column name is Jan\nso what is the next column you have a\nweek one week two like that you\nhave I mean this depends on the\ndata now in this data I don't have date\nI have a month column I have a day\ncolumn I have a year column now\npartitioning is not a must it's not like\neverybody should do partitioning there\nare many conditions where you can not do\npartitioning so like you said if you are\nhaving date you can't partition directly\nbecause if I have a date column uh and\nlet's say you are looking at last 10e\ndata how many dates will be there\n3,650 if I say Partition by date it will\ncreate 3,650 folders that's useless\nthere is another way to tackle it I'll\ntell you but you don't do that so\npartitioning is applicable if the\ncardinality is very less something\ncalled cardinality right how many unique\nvalues you have in the column like in\nthis case or let's say you are getting\ndata from different countries in the\nworld fine maximum you have 150 I don't\nknow country country wise you can say\npartition but yeah like you said or\nanother Cas is that you are having\ntransactions and I want to divide the\ndata based on transaction ID that's not\npossible because every transaction has a\nunique transaction ID it starts creating\nthose many partitions that's not\npossible so partitioning should be used\nonly when your cardinality is within\ncontrol like this right now you're\npartitioning only on the Jan colum I'm\ngiving him a different use case let's\nsay you getting the data from different\ndifferent countries it also depends on\nwhat query you are running for example\nlet's say this is the partition data I\nwrite a query select star from sales\nwhere country equal to India it is a\nworst query I can write because my data\nis partitioned on month and I write a\nquery based on country it will have no\nimpact it's actually worse so if you\nknow that most of your queries are based\non country and month you Partition by\ncountry and month I mean whichever way\nit is possible probably first Partition\nby month then country or country then\nmonth you have to figure out the\ncardinality so that and also if you are\nending up creating lot of partitions\nthat's not a good idea for Hado because\nhad does not like a lot of subfolders\nand sub files right so and and drawback\nI think I discussed this in the first\nclass this this partitioning if you do\nin\ntraditional uh databases it is sometimes\nnot\neffective why\nbecause we had a use case where we had a\nOracle cluster like this know four\nmachines are\nthere okay and then you are saying let's\nsay you are looking at the iPhone sales\ndata you're selling iPhone iPhone sales\ndata and you want to partition the data\nbased on Country number of sales in\nwhich country right so I will say that\nPartition by country but that's a\nlogical partitioning you are not\nphysically dividing the data the problem\nis it will create countries like India\nChina us and all right so probably us\npartition will be here this much will be\nthere H and probably what this will be\nwhat uh I don't know give me some weird\ncountry\nCongo you're getting the problem you're\nlogically partitioning you saying that\ndivide the data based on Country since\nin US lot of people buy iPhone what is\ngoing to happen us partition will be\nvery big this will be your us partition\nright I'm talking about Oracle okay and\nthis will be your Congo partition and in\nOracle if you fire a query what will\nhappen the machine is responsible for\nthey work together so the US partition\nqueries will be very slow because this\nguy has to CH the Congo partition will\nbe very\nfast right in Hadoop also what happens\nyou will do the same partitioning okay\nyou will say that you have us data India\ndata and all it create sub folders but\nthe advantage is that even though this\nfile will be 3tb it'll be on 100\nmachines\nblocks in Oracle this will be one\nmachine are you able to understand what\nI'm saying\nso if you are storing the data in Oracle\nOracle has no idea of blocks and\nreplicas so us partition data will be 3\nTB 3 TB will be one machine it won't\ndivide this if this is in Hadoop 3 TB\nwill never be in one\nmachine blocks will get divided right so\nmy query will be faster actually so the\npartitioning is much more effective in\nHado I'm saying right so which means\npartitioning in traditional databases\nand are not so effective it is effective\nup to an extent but you have to ensure\nall the partitions are having equal\namount of data so part partitioning is\nlogical in every case even in Hadoop it\nis logical in Oracle it is logical in\nOracle I'm saying that all the\npartitions of us should be in one\nmachine there is no way I can further\ndivide this\ndata you are getting my point right\nbecause it is not Hado or something it's\njust a DB so all the US data will be in\none machine so that machine will have\nlot of data related to us but in Hadoop\nif these are four data nodes you will\nnever have 3 terab on one data node will\nalways divide and spread are distributed\nso my performance is better in\npartitioning right so these are some of\nthe things we understand when we work\nright uh so so that is why some of the\ncompanies will say don't do partitioning\nmuch ah sharding is there sharding is\ndividing a table existing table um\nsharding mostly we do in nosql Oracle\nafter LG support sharding\nbut it is not extensively useful\nactually in in nosql databases you can\ndo something called table sharding table\nsharding is like physical division you\ntake a table and say take 10,000 10,000\nrows just divide like three or four and\nthen dump it\nokay but that is not highly possible in\nrdbms because in rdbms you have\nnormalization it is physically any\npartition will physically divide the\ndata but can you say that I want to\ndivide this table into 2 GB 2GB 2 GB\nthat's not possible right ah that is\nlogical U physical division means like\nhe's saying sharding I take a table the\ntable has say th000 rows I can say\ndivide it into five 200 200 200 rows\nkeep in five machines that's not\npossible Right in normal rdbms that's\ncalled physical division you are not\ngiving any condition you are saying take\nthe Full Table split it into five 100\n100 rows keep it in five machines that's\nnot possible impossible in rdbms in\nnosql it is possible because it is\ndenormalized in nosql everything is\ndenormalized so I can divide my data and\npush it in whichever way I want nobody\ncares it's already distributed in hi or\ntraditional rdbms physical division is\nnot possible you logically divide you\nsay look at this column and if you have\n100 values divide into 100 200 to 200\nyou don't have any control you can't say\nthis much Division I want or this size I\nwant there is there is no physical\ncontrol logically you're dividing the\ndata right there is no way for you to\nunderstand after March again there is a\neffect hi has no clue right it keeps on\nsearching all uh you know data so\ncreating this either you can manually\ncreate this folders or you can so\nnormally we don't do like that we run a\ncommand and create it\nactually so it is better to do this\nrather than talking about this is just a\nbasic idea I want you to find out the\ndata set and then start work working on\nit so there is partitioning can you see\nthis\nfolder\npartitioning and there is a word file if\nyou want I mean this has the\ncommands but I have been facing one\nissue with this word file which\nis when you copy paste sometimes the\ncommands will not work we will see\nwhether it will work here okay uh that\nthat code right what you call single\ncode it will not identify so we will see\nwhether it works or\nnot so we have not discussed what is\nstatic partitioning or dynamic\npartitioning and I will talk about this\nfirst\nthing um let's create a table and then\nwe can start talking about\nit so do you have the DB yes right okay\nthe DB is here and I will say create\ntable just copy this command okay\nnow static partitioning is a bit\nconfusing but we can\nunderstand so I'm creating a table\ncalled user one as you can see if you\nlook at the schema it has first name\nlast name and ID only three columns and\nthen I will say partitioned by country\ncomma region this means these are my\npartition columns I want to divide first\nby country then by region fine but there\nis a catch here what is the catch what\nis the data you're going to load if you\nlook at the data this is the data user\ninfo one and if you open this data can\nyou tell me how many fields are there in\nthis\ndata three which are uh first name uh\nlast name and uh ID right and if you\nlook at the table I\ncreated it is matching here\nright then I'm saying Partition by\ncountry comma region right but there is\nno country or region\nhere right this is static partitioning\nso static partitioning means you are\ngetting the data you know from the where\nthe data is coming but the data will not\nhave that columns meaning I know that\nprobably this data is coming from\ncountry equal to US state equal to\nCalifornia right and while uploading\nthis data I should mention it the data\ndoes not have any columns for country or\nregion or anything right if I upload the\ndata you will understand so do one thing\nuh copy this to Hado this user info one\ncan you do\nit uh go to\nhdfs there is\nuh I'll just copy to Ragu\nuser info\none in fact copy all so that we can save\ntime right I mean these are also\nrequired uh yeah so then you have to use\nFTP if you're copying to local you have\nto use FTP better copy to H it's faster\nright\nanything and I have a question to you so\nright now in Hadoop will it be already\ncreated the country and region folders\nwhat do you\nthink so partitioning means it is\ncreating\nsubfolders right I'm saying create a\npartition table country and uh region\nwill it actually create a sub folder\nyeah so no nothing will be created as of\nnow and then what you need to do is um\nwhere is a word pad\nso I will copy\nthis I don't want to type a lot but you\nhave to change it okay don't type it as\nit is\nbecause\num what is a location Ragu for me file\nname is different I\nthink user info 1.txt\nright is there a txt extension\nyeah so now what I'm saying I'm saying\nthat load the data and this is my data\ninto the table and while loading the\ndata I'm mentioning the details I'm\nsaying there is a country New Zealand\nand region\ncubc so what will happen it'll create a\nfolder called country New Zealand\nanother folder called region cubec\nwithin that this file will go can you\nverify that in the warehouse folder if\nyou have loaded it it should be\navailable in the warehouse folder so\nI'll go to\nHue uh I'll go to\nhdfs and where is your U data user hi\nright user\nHive\nWarehouse minus\nMay so many DBS are there people\nstarted May 19\nDB uh here is the user one can you see\ncountry equal to New Zealand there is a\nfolder in Hadoop called country equal to\nNew Zealand within that there is region\nequal to cuc within that you have your\ndata try try yourself and see whether\nyou can see it this this will be there\nin your project and assignments so\npartitioning is very\nimportant it's in the word file\nright oh okay so it's wrong right sorry\nsorry I'll just show here\nso remember this point static\npartitioning should be used like we had\na use case Okay where we were getting uh\nuser data from multiple countries okay\none of our clients's application the\nuser data was coming from multiple\ncountries but the data will not have a\ncountry column so we used to manually\ncreate that's called Static you will say\nthat put this data in us now we have to\ndo something called Dynamic partitioning\nthat I will show you\nthat so make sure you are uh able to log\nto hi and always do use your DB some of\nyou are using default DB and when you\ntype the command you say already the\ntable exist because lot of you are\nactually using default DB\nso if you uh meanwhile if you look at\nthis data right\nhi uh\npartitioning can you tell me what is the\ndifference between this data and the\nprevious one you loaded exactly it has\nthe city and uh state or\nwhatever by the way what is Togo and\nWest Bengal doing together whatever okay\nso have some City or region yeah so this\ndata has the country and city uh column\nso the best way to do partitioning here\nis you create a partition table you say\nthere is a partition by country and\nstate or whatever column and then you\ntell hi I'll give you the data you\ndecide yourself I'm not going to divide\nthe data you look at the column I I\nunderstand how many countries are there\nor how many states are there accordingly\ncreate them and put them that is called\ndynamic how many H but it's only like 20\nlines of data 20 or 20 those many will\nbe created ideally huh and I think uh\nthe first part it starts with Christmas\nIsland there is no repetition right but\nthere is one thing you have to\nunderstand in Dynamic partitioning\nDynamic partitioning is by default\ndisabled in Hive because uh somebody can\ncome and say that okay create a dynamic\npartition table use transaction ID\ncolumn then what will happen millions of\npartitions will get created and it will\nscrew everything so by default even in\nproduction clusters Dynamic partitioning\nwill be disabled so first you to enable\nit first thing second thing is that uh\nyou cannot upload the data directly to\nDynamic partition table what you need to\ndo first you upload the data to a\ntemporary table or some table okay from\nthat table you insert to partition\ntable meaning previously what we used to\ndo we created a static partition table\nthen we said load data in part it it\nwent we are able to do that because we\nknow which partition where it is going\nright now we don't know hi has to decide\nso what I'll be doing I will create a\ntable first normal table I will upload\nthis data no partitioning nothing then\nI'll create a partition table and from\nthe original table I will say insert to\nthe other table so while doing that it\nwill divide and send that is the only\nway to do uh Dynamic partition in in hi\nokay we'll do that practically so you\nwill understand you can't directly load\nit has to be in a table from there you\nhave to do insert but in this example\nit's not really useful because you're\nhaving all unique\nvalues like normally you should have\nlike five records for one country or\nsomething like that but here everything\nis unique values we are having but\nthat's the\nidea um let me see if I have another\nfile okay because partitions in high\njust this land property\ncase\nstudy one moment\nokay\nload\nlist uh I will do one\nthing once we finish this particular\nexample on Dynamic partitioning we will\ndo something called bucketing also and\nonce we finish that I will give you this\nassignment this landed property analysis\nyou can do it by yourself I mean now\nit's easy commands are already there and\nthat will help you to reinforce the idea\non partitioning this has partitioning\nalso just try that once I finish okay uh\nso for the time being for dynamic\npartitioning what we will do we will\ncreate a table called user 2 you can see\nwhat I'm doing from here same thing I\ncopy paste\num and this is the partitioning table\nright look at here I'm saying that\npartitioned by country and region so\nthis table is the one which will have my\ndata partitioned okay then what I do\nI'll create a table called user three\nthat's a regular\ntable uh first name last name\ncountry region first name last name ah\ncountry region right\nso do we have only four\ncolumns where is the\ndata user\ninfo three\nright now we have first name last name\nID so your schema should have five\ncolumns right then only it will work I\nmean the original table so this table\ncalled user three will hold your\noriginal data so what you will do first\nyou will load the data here how do you\nload load data in path because already\nit is in Hadoop where is in Hadoop ru/\nwhat's the file\nname\nuser\ninfo 3.txt right is that the\nname into table\nuser\nthree so do till this load the data to\nuh user 3\ntable Yeah so this user 3 table will\nhold your original data no partitioning\nnothing you'll just have your data from\nthere I will insert to the partition\ntable I cannot directly upload the data\nto partition table in Dynamic\npartitioning so I should copy the data\nto one original table from there I have\nto insert\nH in static you are mentioning which\ncountry which uh State and all manually\nso you can upload the data say create a\nfolder like like and then dump the file\nas it is here the file has to be divided\nbased on column values\ndynamically hi has to decide so you have\nto load the data to one table from there\nyou say insert to the partition table\nand hi Will dynamically partition\nit based on these two columns country\nand\nregion here I have created right country\nand\nregion yes first all the countries\nwithin country regions so right now we\nhave around 20 25 lines of data only\nthat much will be there partition column\nshould be outside the schema when you\ndefine a partition table so these are\nthe partition columns whether they exist\nin the data or not exist in the data\nthey have to be outside the schema you a\npartition by there you mention the\nschema so it will it will not be here\nit'll be here no no it's not required\nonly here because the total schema will\nbe including the partition columns so\nit'll\nunderstand okay now I want you to do one\nthing if you look at the word file right\nyou see these things can you see\nthis right so these are the properties\nyou need to enable for dynamic\npartitioning so the first one is very\neasy if you read you will understand you\nare saying that hey hi enable Dynamic\npartitioning\nset Hive exit Dynamic partition true\nwhich means by default it is false by\ndefault Dynamic partitions are not\nsupported so you're saying that yes I\nwant Dynamic partitions but what about\nthis let me copy\nthat so even if you enable Dynamic\npartitioning by default in high what\nhappens if you create a partition table\nat least one static partition should be\nthere so it's a bit complicat to\nunderstand so there are lot of layers of\nsecurity but everything you can override\nalso so in hiive first they say I will\nnot allow you to create Dynamic\npartition at all only static is allowed\nso you say set High exic Dynamic\npartition true which means now I can\ncreate Dynamic partition but then they\nare thinking if you are allowing you to\ncreate Dynamic partition what if again\nyou try creating thousand partition\n10,000 partition so there is a strict\nmode now we are in non-strict what will\nthe strict mode tell you is that even if\nyou are in Dynamic partitioning okay you\nhave to first create one static\npartition in the table rest can be d h\none one column you static you do rest\neverything has to be dynamic you can\nagain turn it off so many people have a\nconfusion huh\neach partition table so right now our\npartition is based on what uh country\nwhat is that country and region right so\nwhat they saying country you manually\nmention while uploading country\npartition you manually mention Region\nHigh will\ndecide in strict mode will not add it\nwill manage internally but it is saying\nthat manually you mention at least one\nstatic partition need to be created it\nis just a safety measure even in\nproduction setups we disable it because\nwho want to do it you are you have three\ncolumns and one column you have to\nstatically mention means nobody is able\nto allow you to do that right but\nsometimes it is very useful I'll tell\nyou because you are getting the data\nwhere there is no country\ncolumn okay so you want to add a country\npartition that will be static you have\nsome other data let's say I don't know\nstate or something that already is there\nso that can be dynamic so in that case\nyou don't have to turn it off because\nyou will mention country static Al there\nis no other way rest of them it will be\ndynamically creating it so we are\nturning it off because it's a developer\nenvironment it's fine we'll create as\nmany partitions as we want huh normally\nthese are all true I mean uh like\nDynamic partition is false which means\nyou cannot create any Dynamic\npartitioning this will be strict by\ndefault so what we do is that uh\nyesterday somebody asked me even I\nforgot to discuss it you can create\nscripts hi script like SQL script you\ncreate SQL script right do SQL file same\nway I can copy all these commands in a\ntext file save it as hql I say run it'll\nrun as a script that's you won't type\neverything right who do who does that so\nin the script while doing you will first\nadd these things you know set this this\nthis and just run it that's how usually\nwe do\nit it can be SQL also SQL or SQL only\ntwo it can be technically it can be\ntechnically I have I not tried anything\napart from that normally we try either\nSQL or\nhql I don't think if you add any other\nwill it work if you have a SQL script it\nwork it'll work hi I have not tried\nbecause all the scripts we have is hql\nby default people say as hql only I have\nnot tried it might\nwork but I'm saying so and these are\nspecific to your session so H's language\nis called hql hi query language okay and\nthis is not a script we are not writing\nscripts this is this is the\nshell ah this is the query script is\ndifferent what I'm saying all these\ncommands you can write in a text file\nnotepad and you can say run the notepad\nit will execute one by one you don't\nhave to copy paste each time hql it is\nhigh query language\nso huh so that's what I'm said yesterday\nthe language of Hive is called hql which\nis derived from SQL so 95% is similar\nthere are some uh very rare uh know\ncommands which only Hive has SQL doesn't\nhave I think it is SQL 2002 dialect what\nit is using SQL 2002 there are ANC\nformats right for SQL SQL 92 SQL 2002\nSQL 2004 Hive is built on SQL 2002 if I\nremember SQL format I'm saying this is\nsyntax right do you find any syntax\ndifferent from your regular SQL to tax H\npartitioning close is different right\nthat's what I'm saying so if you're\ndoing partitioning in SQL it's not this\ncommand slight difference is there so\nthese two properties I think you\nunderstood what is the enabling Dynamic\npartitioning and the mode is non-strict\nnow again for controlling this is hi xic\nmaximum Dynamic partition 100 which\nmeans you are restricting maximum number\nof dynamic partition is\n100 then this property is not actually\nuh correct the property is correct but\nyou should find a different value for\nexample uh let's write it as 100 what\nthis means is that so when let's say you\nare creating Dynamic partitioning let's\nsay you have a very large file and let's\nsay in the column you have 150 values\ncountries imagine so 150 partitions will\nbe created each partition will be\ncreated by one\nreducer it it's map reduce job end of\nthe day so this creation of partitioning\ntable will fire a map reduce job end of\nthe day so one reducer will work to\ncreate one partition so if you have 150\npartitions 150 reducers will work here\nyou can control it this Dynamic\npartition per node 100 means maximum\nnumber of reducers will be 100\nused so if you want you can control the\nnumber of reducers okay the advantage\nand disadvantage is different uh meaning\nsometimes what is going to happen you\nmay not really need 150 reducers to do\nthe job correct so if you have 150\npartitions if your data is very less you\ncan achieve it using 50 reducers maybe\nso you say limited to 50 only 50\nreducers will be launched\nso this you are saying so these values\nare wrong I'm saying wrong in the sense\nyou're saying maximum number of\npartition is 100 that's fine okay\nmaximum reducers are 100 so ideally here\n100 reducers will run if you have 100\npartition\nbut mappers will depend on your input\nsplit right again so like what data you\nare getting so that you don't have any\ncontrol anyway reducers only you have\ncontrol my point is if you're having a\nvery small file\nokay and probably 100 partitions are\nthere you don't need 100 reducers\nprobably 10 reducers can do the job so\nyou can control this property and I\ndidn't make up these properties it's\navailable in hi okay so people will be\nthinking that okay from where did you\nget all these properties it's properly\ndocumented I'll give you the document\nfor\nthis you can copy any of these\nproperties go to Google Just paste it\nand high language manual will come\nuh not the first one okay so this is\nhoton ver I mean the original Hive\nlanguage manual will come if you open\nthis oh just search\nfor here you have all the\nproperties uh um where is\nit hi EXC Dynamic partition default\nvalue false okay uh whether or not to\nallow Dynamic partition hi exit Dynamic\npartition mode default value strict in\nstrict mode the user must specify at\nleast one static partition right High\nvexx maximum Dynamic partition default\nvalue th000 High vexx maximum Dynamic\npartition per node maximum number of to\ncreate an each mapper reducer node that\nmeans how many reducers will be used\ntotal right and you also have uh other\nproperties for hi we will look into that\nlater\nanyway so if you come here so we have\nenabled all these features right and now\nwhat you should do is\nthat where is a word file here you have\nto Simply say insert so this command you\nwill say insert the data from the table\nto the partition table this is how you\ndo\nit and this will fire a map reduce job\nyou can see and you can see on the\nscreen it will create partitions look at\nmy screen or you can look at your screen\nalso whichever you can see the\npartitions loading loading loading I'll\nshow you because Dynamic right so see\ncountry null region null right now see\ncan you see country Nigeria region poosi\nwhatever om man see all all these\npartitions are loaded dynamically and\nverify that in Hadoop verify whether\nthis is created in Hadoop right if you\ngo\nhere um what is the table table\nname so if I go to user two see how\nKorea came I I think it didn't come it\nsays just Korea North Korea we had right\nin courtes and\nall yeah I disabled we are in non-strict\nmode and any country you take there'll\nbe a region any region you take there'll\nbe a part file because it is output of a\nreducer that is why that z0 comes\nreducers will run\nright huh uh I think I don't know how\nmany reducers ran for this this that's\nanother thing so this is zero right and\nif I go to something\nelse yeah so that means those many\nreducers ran\nright because everything is zero so\notherwise\nuh 0 1 2 if it is same I'll just check\nuh we can actually see the statistics\nhere if you go\nto not\nworkflow job\nbrowser there is a job\nbrowser and this is the job we ran did I\nshow you this no I think I showed you\nfirst class if I go\nhere there is a task\nit's not displaying\neverything sewing stage\none I'll see anyway so can you are you\nable to run this I said yes I think yes\nright can you see the partitions I don't\nknow how many reducers were called we'll\nsee that later but are you able to\ncreate this just look at your hdfs and\nsee whether you can find all the\npartitions uh because of the input data\nright so our data need to be either\nchanged I think because all other data\nis like regular so here if you give like\nthis or you have to clean this I mean um\nso usually what we do is that I mean if\nthere are only limited records you can\nuse either pig or Spar to clean the data\nyou can easily clean it you can say that\nfind codes and then replace so First\nColumn name that you give the partition\nwill be the top level partition within\nthat it'll create those many sub\npartitions\nactually but rest of the partitions are\nloading right there is no problem I\nthink how do you search so there is no\nway to search for uh uh rejections from\nhere you have to manually verify because\nhere it says country equal to Korea\nregion equal to North that's all it\ndidn't reject it actually it created\nKorea then the region is North actually\nthat is incorrect okay as per so before\nloading you have to validate the data\nthat is only way so we do that there are\nvalidators which can do field validation\nand whether all the fields are like this\nand exception handling this you have to\ndo before you do it this data set has an\nexception that's fine but usually you\nhave to validate now uh so this is\nusually how you do Dynamic partitioning\nand one more thing is that when you're\ndoing Dynamic partitioning um there are\nperformance problems sometimes like\ncreating too many Dynamic partitions is\nnot advised even Hadoop will not allow\nbecause name node has to handle all the\nmetadata for the subfolders and the\nfiles like you are having 150 countries\nand each country has States so if you\nsay divide by country and state\nthousands of folders will be created so\nthat will increase the metadata on name\nnode so in one way it is good your\nqueries will be faster but other way so\nthere is no limit as to how many you can\ncreate partitions but we say that do it\nwith\ncaution no uh only thing is that your\nname note performance will be affected\nright like thousands of partitions are\nnot a big problem but so many huge\nnumber of partitions are there that will\nbecome a problem actually uh it's not\ntable size so usually when you use\npartitioning is that if your queries are\nrunning slow like and the reason is that\ninside the same folder you're having\nmultiple files and it has to scan all\nthe files and there are some cases where\nyou cannot do partitioning you may not\nbe able to find out a logic to do\npartitioning probably there is no common\ncolumn or something then I can't do\npartitioning there is no other way there\nit's not like everybody must do\npartitioning there is no rule like that\nright now somebody asked a question\nright so this will lead us to getting\nbut uh what if I'm\nhaving let's say you are getting data\nfrom the country uh side so you decided\nto partition so it becomes country your\npartition column and there are\n150\ncountries look at Amazon look at Dell\nfor example Dell has presence in 150\ncountries okay and so customers are\nbuying products of Dell and they have\nsome data which is country wise okay so\nwhat they decided they decided to\npartition based on Country fine no\nproblem now after this what happened is\nthat so every time when customer is\npurchasing a product there is a\ntransaction ID right and they want to\nwrite queries like this select\nsomething\nokay\nwhere country equal\nto India okay comma transaction ID equal\nto 1 2 3\n4 now the problem is it's already\npartition based on the country so that\npart is saved because it will look at\nonly India but within India you have\nmillions of transactions so this\ntransaction file will be huge it has to\nscan all probably there are multiple\ntransaction files okay and trans and you\ncannot partition based on transaction ID\nbecause every transaction is unique\nright so what you do that's the question\nand I don't know whether this is\navailable in rdbms and all we do\nsomething called bucketing\nbucket uh you have something called hash\npartitioning right can you tell me what\nis Hash\npartitioning in rdbms I'm\nasking rdbms you have who knows about\nhash partitioning what is Hash\npartitioning because this bucketing is\nvery similar to Hash partitioning in Rd\nBMS side I have not done hash\npartitioning so what we do is that see\nthere is a transaction ID column right\nthis is your\ntransaction ID column and here what are\nthe values you can have let's say 1 2 3\n4 right like this transaction ID and\nlet's say 1\nmillion this is the data you have now\nyou want to divide this data based on\ntransaction ID and you cannot do\npartitioning you say bucket it when you\ndo bucketing it can be done only on one\ncolumn you cannot say two column\nbucketing not possible and I say I want\nto create let's say 10 buckets so you\nalways mention the number of buckets you\nwant that is all you do what is going to\nhappen within each country partition\nit'll create 10 files not folders it'll\ndivide this data based on an internal\nhashing logic okay and create 10 buckets\n10 files within each country folder so\nwhen this query hits it will calculate\nthe hash of this okay and find out which\nfile is having it'll hit only there so\nto easily understand this is not the\nactual idea but 5 6 7 8 9 10 11 12 these\nare the transaction idas now I want to\ndivide them into 10 files so there is a\ninternal logic which hi uses I'm not\ngoing to tell the same logic but when I\nsay I want 10 buckets 10 files will be\ncreated fine so I say 1 2 3 four five\nthese are the files\nokay and let's say we take a very simple\nlogic you want to find the modulo 10 you\nknow modulo 10 right what is the Modo 10\nof one one so this will go to this\nbucket this will also go\nwhere are you able to understand\nthis will go here this will go here well\nit is not doing like this I'm saying ah\nsimilar so there is an internal hashing\nlogic and then when you write this query\nit'll look at four it'll go to Fourth\nbucket that's a logic not this logic it\ndoesn't do model of 10 okay it does some\nother complicated logic but it will\ninternally divide we don't have to\nbother it will internally divide now\nmany people have a misconception that\nyou should always use party in and\nbucketing together no I mean it's a\nmisconception people who are already\nworking I have seen a lot of people\ndiscussing no but in most of the\nexamples when you search you want to\nlearn partitioning people will say that\nI will first create partition then\ninside that bucketing that's a practice\nbut you can independently create\nbucketing without partitioning I can\nsimply take a table so I want to bucket\nit that's all I don't want any\npartitioning or anything so what will\nhappen in the same folder it will create\nthose many files 10 files\nah physical F we can see that bucketing\nwe can see practically we can see if you\ncan do partitioning don't do bucketing\nbecause that's the most obvious thing\nwhere you cannot do partitioning you\nenable bucketing uh for example India is\nthere within that you don't have any\nchoice and there is no ideal way to\ndecide how many buckets are required I\nmean so next question is how many\nbuckets 10 100 uh so ideally you should\ntake care that the file size should be\nsomewhere near to the block size don't\ncreate like 1 MB size\nbucket so if you are having let's say\nthe data is 10 GB within a partition\nokay you say create 1,000 buckets you\nknow it will become very very small\ndon't do that because that will create\nsmaller files so ideally take a block\nsize calculation how many buckets you\nneed you have to decide some logic you\nhave to use for that actually if data\nbecomes big that's not a problem data\nshould not become small that's the\nproblem like bucket size should not be 1\nMB then if it will create too many small\nfiles in Hado right Hado doesn't like it\nso if the bucket size is 500 MB fine it\ncan become 1 GB it will still divide\ninto blocks that's fine we are okay with\nit not not the block size I'm saying\nbigger than the block size if it is\nsmaller than the block size Hadoop\nalways doesn't prefer that because\nsmaller files are not very well handled\nin Hadoop okay ah join operations these\nbuckets are excellent for join\noperations there is something called\nbucket join so H has lot of advanced\nconcepts for the time being you you may\nnot be aware of that okay so what will\nhappen is that I have a table\nhere okay and this is my column and this\nis\nbucketed H so this means this is divided\ninto uh let's say five files 1 2 3 4\nfive I have another table that also has\nthe same column that is also bucketed\npossible right and this is also having\neither same number of buckets or\nmultiple then you can do a real fast\nbucket joint there is something called\nbucket joint because since the data is\ndivided into equal equal partitions the\njoints will be really fast actually when\nyou bucket it so when you write a joint\nquery you will say use bucketed joint\nthere is a command you can use I mean\nthere can be mismatch the when you do\njoin operations there should be a common\ncolumn join column and usually they will\nhave similar uh entries right that is\nthe idea you are doing join otherwise\nwhat is the use of a join\nso ah so this is like you are having two\ntables and you want to do a join\noperation normally you can do a join no\ndoubt but if the join column common\ncolumn is bucketed okay then there is\nsomething called bucket joint you can\nuse which will be faster high will\ninternally make it faster since it is\nbucketed the join operation will become\nfaster uh hashing so and there is\nmultiple types of bucket joints actually\nin hi so it's very interesting uh it's\nbit Advanced concept but these things\nare possible\nin no no that's anyway just an so it's\nvery rare that you may get a chance on\nthis but people are aware of these\nthings like bucket joints you can do\nright so we can do bucketing practically\nthat'll be better right rather than\ntalking so why don't you look at another\ndata set\nso can you open this bucketing\ndata how do you open with what Excel\nright see this is a very interesting\ncase study because here you can\nappreciate more uh partitioning because\nwe have similar things see you are\nhaving street right and the street is\nhaving almost unique values some some\nunique at least city is Sacramento zip\ncode you have state you have California\nuh bedroom bathroom Square fet so this\nis this thing real estate data and type\nis residential condo blah blah blah\nprice some price right so and how many\nlines of data you have 986 lines of data\nyou have now Our intention is to do\nbucketing but we will do partitioning\nalso just to see anyway we can do that\nso what I want you to do is first upload\nthis data to\nhow do you\nupload the file name is realore state.\nCSV that's the file name okay\nand I want you to open this word pad\nwhich will be here bucketing\nHive yeah so what we are going to do\nfirst we will create a\ntable\nokay and this is a normal table\nokay I'll say create table R\nState that's called real estate and this\nhas the schema regular\nschema no partitioning no\nbucketing uh terminated by comma regular\nstuff no\nchanges and we will load the data how do\nyou load the data\nload\ndata in path\nso I guess you will be able to load the\ndata so this if you create\nwhat you can do is load the data and you\nhave to enable bucketing so again\nbucketing is disabled so just say set\nHive enforce bucketing\ntrue so the command hive. enforce do\nbucketing equal to True will enable\nbucketing\nright and look at this command this is\nis probably the most important\ncommand create\ntable uh you are calling the table\nbucket underscore table and I have the\nschema Partition by City clustered by\nStreet into four buckets what do you\nmean by this Partition by city so City\nyou have common entries right I think\nSacrament or something so I'll say that\npartition column is City then I'll say\nclustered by street so Street column\ninto four buckets so it will create uh\nthose many City partitions each\npartition will have four uh this thing\nwhat do you call buckets but it is not\nmandatory another thing is that if a\ncity has only one line of data only one\nbucket will be there if it has more data\nthan only the four buckets will make\nsense some of the Cities will have only\none street let's say it'll create only\none bucket in that case um and now just\nuh load it and see whether you can see\nthe buckets I want you to\ntry disabled yeah even bucketing is\ndisabled and now I want all of you to\ntry yourself uh I'm just doing an insert\nonce the insert is complete go back to\nyour user High Warehouse folder you\nshould be able to find partitions and\nbuckets\nso I will I will even show you my\nbuckets I mean like we just want to see\nthe buckets so if I\nuser hive\nsorry Hive\nWarehouse uh minus may9\nDB May 19\nDB what is the table we haveck bucket\ntable if I open bucket table these are\nthe city if I open a city there are four\nbuckets but again uh bucketing also\ndepend on distribution of data see here\nthe last two buckets are not having any\ndata but if you're having only one line\nuh or just two lines like then only one\nor two buckets will be created high is\nvery intelligent like very less data it\nwon't create four buckets here I think\nyou're having more more like some four\n10 lines I don't know how many you have\nbased on that it'll create but the last\ntwo buckets are empty in this example so\nideally you should have more data this\nis happening because we are having very\nless data right some thousand lines we\nhave actually but number of mappers will\nbe there right H either mapper or\nreducer I said reducer so I'll show you\nhere in that property it will be visible\nit can be either a mapper or a\nreducer now uh when we created the\npartition uh he is saying I'm not able\nto see any reducer jobs only only mapper\njobs Ran So see\nhere maximum number of dynamic\npartitions allowed to be created in each\nmapper SL reducer not so it can be a\nmapper job or a reducer job if you're\nhaving less amount of data it will be a\nmapper only will run but sometimes what\nhappens you will have huge amount of\ndata and the mappers will first move the\ndata then the reducers will aggregate in\ndifferent buckets or\npartitions now he's saying he is uh not\nable to see reducers in the job I'm\nsaying it can be either mapper or\nreducer in the when firing a partition\nquery it depends on resource allocation\nif the cluster is having resources it\nwill allocate I got only one mapper mine\nis actually only one mapper no reducer\nat all yeah it can be either a mapper or\nreducer it doesn't matter actually to be\nhonest uh what is going to happen\nis when you created that partition table\nright when you do an insert what is\nhappening map ruce job so and what is\nthat map ruce job doing that is my\nquestion it has to segregate the data\nthat is what it is doing right it is not\ndoing any other logic it is just\nsegregating the data based on your\npartition or bucket you know so right\nnow you did so did you check this\npartitioning with bucketing is it also\nnot firing any\nreducer you saying that the recent one\nright with bucketing so you are getting\nreducer zero mapper is there how many\none mapper yeah even I got one mapper so\nsometimes what happens if the cluster\nhas enough resources it will fire\nmappers and reducers so that it can\ncomplete really fast so mappers will\nfirst probably segregate based on\npartition and reducer will just\ncalculate the hash and dump it but if\nthe cluster is not having enough\nresources this job will be very slow\nonly mappers will be launched they have\nto complete the whole thing but there is\nno intelligence in it I'm saying this\njust a way of the cluster managing your\npartitioning process huh the logic is\nwritten but you cannot really say it has\nto be done by a reducer or a\nmapper right sometimes it'll be achieved\nonly using a mapper sometimes it'll\nShuffle and fire a reducer also yes in\nmy case it was done there was only one\nmapper in my map reducer in his case I\nthink four reducers were launched so\nprobably there was some Shuffle\noperation depends on where is your data\nalso another thing is that when there is\na reducer there is shuffling of\ndata data shuffles and then goes to a\nreducer so I don't know probably in his\ncase some Shuffle operation happened\nthat is why the reducer kicked in anyway\nit is not really important I'm saying I\nmean while inserting into a partition\ntable it really doesn't matter whether a\nmapper is doing it or reducer is doing\nit it does not have any impact on the\ndeveloper side because it is just a uh\nfinding by column and then move moving\nthe data that is all you're doing but in\na query and all it is very important\nbecause if you write a proper query how\nmany mappers reducers get involved\nthat's very important actually Hive can\nalso store the result as text file so\nlet's say you write a query and the\noutput is normally uh shown on the\nscreen like you write a query it will\nshow there you can say save it on hdfs\nthe output whatever query output the\ncommand\nis uh insert all overwrite\ndirectory there is a command called\ninsert overwrite directory then you\nwrite your query what will happen is\nthat whatever is the output of the query\nit will save as a text file on Hadoop if\nyou want to save it right so normally\nyou want to see but sometimes you want\nto say if you want you can do that also\ndirectly it is insert override\ndirectory if I remember because we\nrarely save it as text file okay\nHive in\ninsert overwrite\ndirectory huh so this is the one insert\nover right\ndirectory uh but I have to see\nit is there you can try this command\ninsert override directory and then you\ngive uh a path and then you say select\nstar so let's try that I will\nsay directory\num what we will\nsay uh where is the\nspelling oh\ndirect insert override directory I will\nsay\nru/\nABC count\nstar uh give me a table name that we\ncreated just now NASA log right\nthere is a NASA log\nright so this means it will create a\nfolder in Hadoop in Ragu ABC there the\nresult will be stored query result if\nyou want to store it somewhere you can\ndo that you can try this out see so when\nyou're creating a hyp table it store it\nas a text file by default whatever data\nyou're loading into a Hy table it is\navailable in Hadoop and that will be in\na text format like CSV or or any text\nformat\nand over the period of years there has\nbeen lot of compression techniques to\ncompress the data so while creating a\ntable you can mention how to store the\ndata now I have some very good hoton\nworks links I will share it with\nyou um is there a\nway okay I can anyway you're not going\nto see right I'll just open my mail\nbecause I just shared some links I\nwanted to show you\nI think in your assignment this is there\norc\nright correct and that is very easy I\nmean you so storing the data as or is\nvery very easy you just need to\nunderstand what is\nhappening yeah very rare uh customers\nare there in hot mail\nright I never quit hot\nmail so here is it so there are some\nhoton Works links which I found very\ninteresting\nokay so orc is a format so this is hot\nworks link but uh so orc files in hdp\nbetter compression better performance so\nwhen creating a hive table last line you\ncan add store as orc only one thing you\nneed to do just say store as orc and\nwhat is going to happen whatever data\nyou're loading into that table high will\ncompress it orc stands for for optimized\nrow column format it's a compression\ntechnique what is Advantage First\nAdvantage your data is compressed well\nthat's not a great Advantage maybe\nsecond advantage and the most important\nAdvantage is that orc has\nindexing there is a technique called\nindexing right now in Hive You by\ndefault has indexing but that is very\npoor if you enable indexing you will not\nget any performance what orc does it\nwill compress your data and whatever\ndata is in the compressed uh file it\nwill create an index inside that so when\nyou hit queries columnar queries and all\nyour queries will be really fast if the\ntable is having orc property column\nindexing regular column indexing you\nhave right in rdbms and all what is\nindexing in order to faster the queries\nyou will create sort of like a how do I\nsay pointer to the original data orc\nwill create an index on row and column\nas well there is row indexing and column\nindexing it'll create on everything\nactually and it is very very efficient\napart from orc if you want you can\ncreate normal indexing in hyp there is\nsomething called uh bit map index and\ncompact index but they are very\ninefficient actually because even if you\ncreate an index in Hive uh you are not\nactually dealing with name node or hdfs\nyou don't know where is the data but if\nif in orc what is happening is that you\nare asking Hive to manage the data\ncompress data so it will create its own\nindex for the the whole data and store\nit now I'll share this link but if you\nscroll down one of the things that they\nsay here is this so these are so if you\nhave a 585 GB file in Hive if you store\nit as text file this is the size there\nis a format called RC file it will\nbecome 55 there is another format called\npar become 221 if it is O it is 131 GB\nthat is the level of compression you are\ngetting so par and RC are other\ncompression formats uh RC is not very U\ncommon now because orc is optimization\non RC RC is raw columnar orc is\noptimized ra columnar so RC nobody's\nusing par some people use so park is\nalso another uh compression technique\nbut Park does not have indexing or\nanything it'll simply compress your data\nthat's all orc has this advantage of uh\nyou know indexing within that right so\nthat's one advantage\nuh AO is a serialization\nformat uh how do I say so let's say you\nare sending a lot of data you can send\nthe data along with the schema normally\nwhere will you mention the schema in a\ntable now AO is a format which can apply\nschema even to text files without a\ntable or anything while sending the data\nyou can have a headr around schema so it\nis very common in Hadoop systems that\nyou use AO format okay and if you scroll\ndown um so this is how orc will store\nthe data this is one thing you need to\nunderstand\nso it will create index on let's say\nfirst 10,000 rows you can mention how\nmany rows you need to create index there\nis a row and column index so here I'm\nsaying that 10,000 rows I want index so\nwhat it'll do first 10,000 rows it'll\ncreate an index and store here next\n10,000 rows it create an index and store\nhere so if somebody's quing it can\nunderstand which uh you know uh group of\nrows it has to push and it can even skip\nthese uh 10,000 rows in one shot let's\nsay you're writing a filter condition\nsince it knows what is inside here it\ncan simply skip this entire 10,000 row\nand scan here so it's very fast if you\ndo it in orc other formats do not have\ndo not have this\nability and there is something called\nvectorization in hi it's an advanced\nproperty\nvectorization will allow you to read\nthousands of rows in one shot normally\nwhen you reading the data it reads one\nrow second row third row if you enable\nvectorization with orc orc can read\nthousands of rows in one shot so\nvectorization and all will work only on\norc orc is the only format which\nsupports vectorization also this batch\nindexing this properties are only\navailable and when you go to hoton Works\nuh I told you that you can have acid\nproperties is in height you can enable\nrealtime queries the condition is that\nthe table should be orc then only acid\nwill work so for all these reasons\npeople prefer orc wherever possible but\nwhat can be one drawback these are all\nadvantages but what can be one\ndrawback exactly it has to\ndecompress meaning uh your CPU Cycles\nmight be required anything which is\ncompressed you have to decompress\nso compress is okay you're storing it\nand that's okay but when you're querying\nit it cannot it has to uncompress the\ndata right so CPU Cycles are required\nbut still people are preferring it uh\nbecause they are okay to spend some\nmoney on CPUs but still it is very\nfaster and gives you all these uh uh\nproperties so just go through this link\nuh one link is this okay\nand stride is this this is one stride\nthis 10,000 rows is called one\nstride that that one block of rows is\ncalled one stride actually okay uh and\nthis is the uh speed of queries of 1\nterab data so this is hi 1.0 this is hi\n1.1 that's fine and then hi plus\nvectorized query High plus PPD PPD is\npredicate push down you know what is is\npredicate push\ndown like sending your filter first\nright so if you write a query like\nthis so let's say you are saying that\njoin the data and then filter the\ndata this is bad right you are saying\njoin the data then filter the data first\nyou should filter the data then join so\nby default in rdbms and all it will\nfirst bring the filter together it will\noptimize in Hive you have to manually do\nthis that is called PPD predicate push\ndown so if you enable PPD along with orc\nthis filter will come first then join\nwill come so it will push the predicate\nbasically that's the idea orc yeah so\nand I can show you this I mean I'm not\njust uh blabbering if you look at uh\nyour uh Cloud era um this\nthing uh Cloud manager right so where is\nthe cloud lab\nyou look at the cloud era manager you\ncan see whether these properties are\nenabled probably not all of them are\nenabled that's one thing let's check\nI'll go here I'll go to hi and I will go\nto\nconfiguration and I don't know if I say\nPPD uh no it is not showing\nah enable vectorization\noptimization okay it is enabled so this\nvectorization is the property by which\nuh if you are storing the data as orc it\ncan read thousands of rows in one shot\nif you enable vectorization it can read\nthousands of rows in one go and then\ngive it to you so it makes so\nvectorization is enabled PPD I have not\nseen I don't know whether it is enabled\nor uh something if I search for PPD it\nis not predicate push down I don't see\nanything predicate no it is not there\nprobably that property itself might not\nbe here it is not available no\nultimately when you are reading it will\nnot get bulk it will get row by row that\nis a default nature it is very slow\nactually in hi when you're reading the\ndata it's in the row format right so\nwhen you're creating the table you are\nsaying row format Del limited and let's\nsay your query result requires let's say\noutput is 10,000 rows it will not push\n10,000 rows it will say Row one row two\nRow three like that it will come it's\nactually very slow in original hi if you\nenable vectorization it can read minimum\nthousand is there in vectorization\nminimum th rows you can mention whatever\nyou want those many will come in one\nshot so it's very fast actually while\ngetting the result and vectorization\nrequires orc no other format will work\nwith vectorization there is a reader for\nvectorization which is available in orc\nonly as of now I don't think any other\nformat supports vectorization right so\nnow the question is that how do you\nenable this orc\nright optimized row column\nformat\nso huh so these things are very very\nimportant okay let me see if I can show\nyou from another\none so these are some of the things\nwhich you can go\nthrough so when you create a table all\nyou need to do is that you say stored as\norc last line you add stored as orc and\nit will become orc and it will take the\ndefault values you can even disable\ncompression some people do it so what if\nyou don't have enough CPU power you want\nto use indexing and all but I don't want\nto compress the data you can say\ncompression none it will not compress\nthe data possible uh and so these are\nthe properties you can mention in orc\nyou can mention the compression the\ndefault algorithm is called zip it's\nvery efficient you can either say none Z\nor Snappy these are the three options so\nzib and snappy are compression\nalgorithms if you say none\norc table will be created no compression\nand then you have orc compression\nsize number of bytes in each compression\nchunk so the more you put the more\ncompression will happen in this case it\nis 256 KB by default orc stripe size\nnumber of bytes in each stripe 256 MB so\neach I told you right 10,000 rows like\nthat so the size of that is 256 MB you\ncan say that in my table probably your\ntable has 1 TB data divide that into 256\n256 like that and row index stri 10,000\nfor every 10,000 row an index will be\ncreated so this you can increase or\ndecrease compression size uh is the\nnumber of bytes in each compression\nchunk so that means uh when it is\ncompressing uh it creates something\ncalled chunks inside so what should be\nthe size of each chunk minimum size\nactually so here by default it is 256 KB\ndepends on the data as well so we leave\nthese things to default we don't change\nthe compression size and all because uh\nthe uh algorithm Z or Snappy or none you\ncan change and the stripe size is how it\nis dividing the data like here the\nstripe size is 256 MB 256 MB will be the\nsize of your one Stripe Right and in\neach each stripe it will get 10,000 rows\nand create an index so probably in one\nstripe it will create like five or six\nindexes depending on how many rows you\nare having one you're mentioning the\nsize and within that how many rows one\nindex need to be\ncreated it's okay so sometimes I mean\nthis these are the properties just read\nyou will understand anyway okay and you\ndon't have to mention this also so\nnormally when you create orc table you\njust say stored as orc because that is\nbetter orc will take care of the\ncompression itself these are the values\nyou can adjust if you want to tune them\nif you simply say store as or that's\nalso fine yes so if you say compression\nnone it will not compress the data it\nwill index it it will definitely index\nit but no compression will be there now\nthe default is text file because for so\nlong people are using text file I don't\nknow orc is an evolving standard okay uh\nand orc has different different versions\nand all with every High release they\nwill release a new standard for orc and\nall text file is the standard format and\nyou don't want to force anybody to use a\nparticular standard right this is better\nso if you want you can use it that's the\nonly thing you can't read an orc file\nyou can see that you cannot read it\nbecause it is like compressed and only\nhi can read it Hive can only create and\nread it it will look like uh uh like a\nzip file only but you won't be able to\nopen and see what is in side only hi or\norc readers can read it actually like\nyou want to set up a small setup or like\nin a production you want to do\nit production so obviously you need\nmachines and you may not be setting up\nonly Hadoop And Hive nobody people will\nbe having the complete package actually\nwhen you say Hadoop And Hive that's only\ntwo tools right so rather than setting\nonly Hadoop and Hy so it's very easy you\nhave to have machines then you have to\ngo to one of the vendor either cloud or\nhoton Works get their distribution\ninstall it if you are using Apache\nHadoop you have to first download the\noriginal\nSource extract it there are four files\ncore site XML hdf site XML Yan site XML\nmapsite XML in each of these files XML\nfiles you have to manually type the\nproperties probably take a year to\ncomplete almost and you will not succeed\neven I will not succeed because it's\nvery difficult nobody does that we used\nto do it I used to set up Apache\nclusters but very very complicated it is\nbecause everything is manual there is no\nautomation everything and there is no\nuse for that also only use is that\nprobably you want open source Apache\nthat is hoton Works The hoton Works\nEdition and Apache Edition are exactly\nsame no difference at all yeah even\nclaer and hoton works there is a free\nedition okay so since all these\ncompanies are selling Open Source\nProducts you can go to Pon works or\nCloud era and download their product\ninstall them on as many machines as you\nwant the problem is you will not get\ntechnical\nsupport in Cloud era you have an Edition\ncalled Express Edition okay what is our\ncluster our cluster should be Express\nEdition\nright you can check it so this is cloud\nera\nright uh somewhere it will be\nthere I don't think maybe I'll be able\nto see the\naddition because you don't have access\nto everything\nright huh see claer Express this is free\nso we are running on eight notes or 10\nnotes right this cluster it's absolutely\nfree you don't have to pay a single\npenny to Cloud but if something happens\nthey won't help you like technical\nsupport is not there the commercial\nEdition is called Cloudera Enterprise so\nwhen you are installing the cluster it\nwill ask you do you want Express do you\nwant Enterprise if you say Enterprise\nthey will ask please browse and upload\nyour license key then only the\ninstallation will proceed actually so if\nyou want to test or try something any of\nthis there are people who run on\nhundreds of notes and uh this Edition uh\nExpress Edition no the hon workor\nsandbox will not run in multiple notes\nit's only single machine setup that\nmanually you have to install uh I'm not\nquite sure whether it will work because\nthe sandbox is designed for one machine\nonly it is is not designed for\ndistributed like it is running only it\ncannot be expanded even if you connect\nboth of them they won't work because\nthey are working as independently\nsandbox is a virtual machine which you\ncan download and install your PC if you\nstart it uh it will have everything hadu\nP spark everything just to learn like a\nsingle machine setup production setup is\ntotally different installing and all so\nthat comes in the admin side that is why\nI'm not talking much about it you may\nnot be able to understand if I say\nbecause there are a lot of other things\napart from just Hadoop or something you\nhave to set up a lot of repositories\nthen machines and mirroring and racking\nand then the setup is same you will have\nlet's say 100 machines one will become\nmaster and in every machine you will use\neither Cloud manager or any tool to\ndownload Hadoop and\ndistribute so then you have to first\nwhat you need to do you have to download\nthis tool called Cloud manager download\nit on one machine then start Cloud era\nmanager it will ask you how many\nmachines you have you say four machines\nit will ask you which one you want\nMaster you say this machine it will\ndownload install do everything for you\nso Cloud manager you have to install on\none machine and then you start it it'll\nimmediately ask you how many machines\nyou have you give the IP it'll detect\nthen ask which one is your name node\nwhich one is the data no which one is so\nyou select this this this it will\ndownload Hado distribute it install it\nset it up and give you the cluster that\nis how this cluster is created actually\nno no there is no limitation you can\nhave thousands of machines but people\ndon't do it actually I mean if it is a\nproduction setup they go for Enterprise\nlicensing uh but even the express\nEdition is pretty stable and the express\nEdition uh does not have some features\nof the Enterprise Edition like uh you\nhave something called rolling restarts\nand rolling upgrades for example let's\nsay you want to restart a machine in a\ncluster now usually when you restart it\nit'll have tons of problems because your\nname will detect this is down so\nsometimes in the Hadoop cluster you\nmight want to restart all the machines\nyou applied some patches or something\nand sometimes you want to upgrade the\nHadoop version you are running 2.6 you\nwant to go to 2.7 upgrade will happen\nbut after that every machine should\nrestart so uh if I do this in a\nproduction cluster is very difficult so\nin the express Edition there is no way\nevery machine will go down in Ender\npress Edition you have something called\nrolling restart which means one by one\nit will restart machines without\naffecting the cluster end of the day\nevery machine could have restarted but\nyour services will not be affected and\nyou also have mirroring backups Advan\nbackup mirroring solutions that are all\navailable only in Enterprise this\naddition will not have all the features\nwill be there like spark or whatever\ntools you want everything will be there\ncommissioning you can Commission on the\nFly you can add two machines nothing\nwill happen two machines will get added\nimmediately they start become part of\nyour cluster it's called commissioning\nuh I don't know if I can show you here I\nwill not be able to add them but you go\nto this host menu and there is something\ncalled Commission State it is commission\n11 now that means there are 11 machines\nrunning now I can't add because I don't\nhave any uh permission but there will be\nan option called add a machine here\ncommission and so right now it says\ncommissioned 11 H so as of now\neverything is commissioned I don't have\nany admin rights otherwise we can just\nsay click add it will add on the flight\nno problem and Spark requires some level\nof understanding before we actually go\nto the hands on side we can do the hands\non in spark for sure we'll be doing a\nlot of hands on but if I directly start\nwith okay write a program you'll not\nunderstand what we are doing right so\nthat is why some slides are there I'm\nnot really a slides person but still\nsome topics we have to understand only\nyou can go to spark\nright um so first thing you need to\nunderstand is what is spark and from\nwhere we got spark right so why people\nare so excited about spark so uh you\nyour previous session trainer right uh\nwhat is his\nname uh he was asking me that can I\nlearn spark I really want to learn spark\nThat's The Power of spark I'm not saying\nmean even I want to learn machine\nlearning I'm not saying that way uh but\nhe want to leverage spark so he was\nasking me some doubts like uh can I get\nstarted with spark I want to do machine\nlearning on spark uh because everybody's\nexcited about spark so and all my\ntrainings these days are on spark most\nof majority of trainings are on spark uh\nso there is a lot of excitement about\nthis tool and why is it why spark is so\npopular or is it some magic or something\nwhat spark is doing right so uh it's\nvery easy to understand why it is so\npopular uh somewhere in\n2009 okay we had uh a project called amp\nlab in the University of\nBerkeley there is a research project\ncalled amplab so this amplab project is\neven live today it's a research project\nin uh University of Berkeley so there\npeople were trying to create a new\ncluster manager do you remember\nYan yeah so Yan is what your resource\nmanager in Hadoop and Hado version 2\ncame in\n2012 what you're seeing today so Yan\ncame in\n2012 right Yan came with Hadoop we are\ntalking about 2009 so in 2009 there is\nno Yan we are we were running Hadoop one\nold Hadoop version and there was no good\ncluster managers so some folks at the\nUniversity of Berkeley they were trying\nto create a cluster manager very similar\nto yarn okay at that time there is no\nyarn okay and they call this project as\nmesos\nmesos mesos is the project name they\nhave given for this project okay and\nthese guys were doing some R&D and at\nlast they created the cluster manager\nlike very similar to Yan mesos is there\neven now it's an Apache project very\nsimilar to yarn so why yarn became so\npopular because yarn is coming by fall\nwith Hadoop if you download Hadoop you\nwill get yarn if I want to use use mesos\nI have to install it separately but\nHadoop will run on top of mesos also\nthere the only difference will be that\nyour yarn layer will be handled by mesos\nokay so mesos is a very famous project\neven today some of the folks use it\nactually not everybody use it and mesos\nwas created in 2009 in amplab so they\ncreated mesos and they said that it's a\ngreat cluster manager we're going to use\nit blah blah blah right now in order to\ntest the power of\nmesos okay first they ran some map ruce\nprograms of course map ruce was what we\nhad originally and it was running fine\nso then they thought why don't we just\nuse uh a different programming framework\nand they created something called spark\nso spark was created actually to test\nmesos meaning mesos is your cluster\nmanager and they created a new uh\nprogramming framework called spark so\nyou can write a program using spark\nit'll run on mesos and the only\ndifference was that spark was completely\ninmemory execution like it'll use most\nof your RAM so if you have a cluster\nmanager and if there is a lot of ram\ngetting used you can actually test right\nwhether the cluster manager is good or\nbad so they created this spark uh\nprogramming framework wherein if you\nwrite a spark program it will use\nmajority of your RAM for faster\nprocessing and they tested mesos and\nmesos became a success everybody was\nhappy happy people never thought much\nabout spark they just created as a sub\nproject okay but after an year like 2010\nor 11 people thought that hey this spark\nis good because if you write a program\nin spark first thing people noticed is\nthat it is running faster compared with\nmap reduce I'll tell you why it is\nrunning faster so people thought that\nwhy don't we develop spark a bit more if\npeople can write a spark program and if\nit is running faster should be great\nright and the development started and\nsomewhere in\n2012 uh I believe spark became an Apache\nproject so they contributed to Apache\nand said that you know we have created a\nnew product it's an make it open\nsource and this is a framework like map\nreduce if you write a spark program\ninstead of a map reduce program it will\nrun faster than map reduce Apache\nthought okay let's try that it became an\nApache top top level project okay from\nthere everything changed from 2012 so\nactually spark is quite new because uh\nit became widely popular somewhere\naround 2015 only and I started using\nSpark from 2016 only this is me the\nworld\nright so I'm like good right because it\nbecame popular somewhere 15 16 I started\nusing spark okay and even in 2016 there\nwere very less trainings in spark\nbecause people were like excited but\nnobody knew what is Park so we started\nusing in 2016 all of a sudden end of\nlike December 2016 it became the most\nactive Apache project in the history and\nin 2018 17 18 now this is the uh biggest\ncontributed Apache project so in a span\nof two three years it gained a lot of\npopularity right and now spark is like I\nsaid the most popular Apache project so\nfar so Apache has around 300 100 plus\nprojects in that the number one is spark\nas of now right also originally there\nwas a spark version zero then we got a\nspark version one now we are in spark\nversion\n2.3 this is the latest version and many\ncompanies run something called spark\n1.6 it very stable a very stable version\nthat is a reason they run it probably I\ndon't know right after 1.6 you have 2.0\nthere is no\n1.7 after 1.6 the next spark version is\nMajor release 2.0 okay and we are on 2.3\nright now if you go to the spark website\nit will say we are on 2.3 currently 2.3\nis not pretty stable because it's a\nlatest version so not so stable uh but\n2.1 2.2 they are very very stable\nactually and we are learning spark two\nwe are not learning spark one you are\nnot missing anything because in spark\nyou have some improvements and all the\nfeatures of one are available in two so\nthat part is covered and uh in most of\nthe trainings they will request you\nplease start with Spar two uh I mean\ndon't start with spark one meaning it's\nit's it's a it's not so old but people\nwant to start with spark 2 so the major\nversion right now is two or 2.3 I can\nsay that we are using and something else\nhappened so what actually made spark\nmore popular is that in 2012 these guys\nthe founders of spark there is two guys\nthey gave to Apache and said that take\nspark and go but they were not so happy\nso what these guys did\nparall they found a company called Data\nbricks data bricks so the founders of\nspark they gave it to Apache because it\nbecame open source obviously but they\nalso want to make money right so it is\nopen source you can't make money so if\nif you give something to Apache it you\nare actually contributing to the world\nprobably like for the good scenario if I\ndon't give it to Apache like Microsoft\nWindows Microsoft Windows is not\navailable with Apache right so it's only\nmoneymaking and so you are not getting\nany chance to modify anything or add\nyour contributions or anything if it is\nnot in Apache the only contributions for\nuh commercial versions are done by the\ncompany so the growth will be limited\nvery simple the growth will be limited\nlike you will have your own developers\nwho can modify and all but others will\nnot be allowed so when it went to Apache\nnow what happened is that Apache has a\nspark version Apache spark that anybody\ncan download that is 100% free open\nsource again the spk is available with\nclera hoton Works mapar sure that we\nknow so we are using a cloud data\ncluster that has spark installed so that\nis again same spark only we can use it\nbut these guys in 2012 found this data\nbricks okay company and their sole\npurpose is to sell spark nothing else\nbut this company is so special because\nit is created by the founders of spark\nthe people who actually wrote the sour\ncode found this company so if I want\nonly spark I can probably go to these\nguys because they are the best but\npeople don't go here people go to cloud\nor Hoten BS because chances are very\nrare you want only spark you want other\ntool sour like scoop you want Flume you\nwant now data bricks has nothing to do\nwith scoop or Flume they only Spark\nright now uh one of the participant\nasked this question I'll get back to\nthat in a moment but still spark is an\nindependent project which means it does\nnot require\nHadoop but mostly you will see it on top\nof\nHadoop okay spark is actually an\nindependent project which means I can\ndownload spark I have spark running on\nmy Windows 10 laptop beautifully Works\ndoesn't require anything right NTFS F\nsystem it will work okay so but people\ndon't do it because the point is um like\nright now we have a migration happening\nin GE so I was with GE last week before\ncoming here so there we have a migration\nhappen G is migrating to spark spark and\nHado right so G already has a Hadoop\ncluster so why should they install spark\nseparately right so they are already\nhaving a Hadoop cluster so they will\ninstall spark on top of Hadoop as an\necosystem tool so it will start\nleveraging hdfs for store storage and\nthen processing and all so most of the\ntimes you will see spark on top of\nHadoop but that does not mean it will\nrun only on top ofu you can run it\nanywhere practically and I will show you\nin the slides wherever you can run\nhowever you can run so on and so forth\nuh so data bricks is popular in that\ncase and it is available Al everywhere\nright so I think we'll uh run some\nslides to get you an idea and uh\ndisclaimer these slides are from data\nbre so some of them will be their\nmarketing slides uh the best resource to\nlearn spark okay so a couple of things\nwhich I know I can teach you right uh a\nlot of people ask like uh give me the\nbest book to learn spark and they\nsuggest one book then they will never\ncome back to me so the best book to\nlearn spark is something called spark\nthe definitive guide don't buy it don't\nread\nit just make a\nnote spark the definitive guide this is\nthe best book don't even touch it\nwhy I'm saying this okay you can\nprobably buy I don't mind this is\nwritten by uh Z mahara and the the\nfounder of spark the guy who wrote The\nSource Code wrote the book and the book\nis pretty much like the source code you\nwon't understand anything even I don't\nunderstand half of the book but very\ngood so buy it and keep it so people ask\nand you can show this is a spark right\nuh very difficult to understand but it\nhas like accurate information because\nthe guy who wrot spark is writing this\nbook right so if you read about rdd he\nwill go to any level to explain what it\nis so it's a bit difficult to understand\nprobably don't read it immediately once\nyou complete the spark class and do some\nasignment then you start with this book\nyou it'll get give you a very good idea\nto read There are some other books\nrelated to spark available I forgot the\nname\nbut huh learning spark right that is\ngood learning spark is easy this is very\ntough actually okay this is and this is\nversion\ntwo these guys also wrote the same spark\ndefinitely got for spark version one\nwhich was very easy actually this is a\nbit tough I don't know so this is one\nplace where you can second is the\ndocumentation from Apache and datab\nBricks this is where I learn\nactually so you open Apache\ndocumentation you open datab bricks\ndocumentation they will give you a lot\nof ideas I mean Basics at least if you\nwant to learn right\nuh okay so that's learning spark so book\nyou can\nkeep I think it is not available in\nIndia did anybody try buying\nit is there it was not there before it\nis\nthere flip card spark the definitive\nAmazon\nalso ah this is the mat zaharia and Bill\nChambers these are the people who wrote\nspark okay okay um so it's good I mean\nI'm not against this but try if you want\nyou can get a how much it is in flip\ncart is costly\nright PDF you can download ah so this is\npaper Gap back\n2018 actually right so latest uh Edition\nit is\nright um and basic operation and u a lot\nof things are there Hadoop there are two\nbooks actually maybe there are new books\nbetter one is Hadoop the definitive\nguide then um there is one more book\nright for\nHadoop one is Hadoop the definitive\nguide this is Park the definitive guide\nyou also have for Hadoop Hadoop the\ndefinitive guide uh then there is one\nmore good book for Hadoop um Hadoop in\naction I think that's also a good book\nokay hadu books are bit more easy to\nunderstand uh in the sense like this spk\nbook is more more oriented on the sour\ncode side like how exactly stuff work\nhadu books are a bit more\neasy H this is the slide which you need\nto understand if you want to learn spark\nright uh and this slide is very\nimportant also if you really want to\nlearn spark why it is important so\nsomewhere in 2004 before Hadoop came\nthat's what the year there uh and with\nHadoop you got something called map\nreduce you know what is map reduce I\ndon't have to teach right till 2015 I\ncan say you can see 2007 till 2015 what\nhappened was that one of the problems\nwith Hadoop was that you have map ruce\nfine no problem but what map ruce does\nis batch processing it is slow and it\ndoes batch processing so people wanted\nto try different different different\ntype of workloads in Hadoop so some of\nthem wanted to write SQL queries on top\nof Hadoop that is where you got Hive uh\nthen we got Impala then we got Presto\nnow here you have drill uh and and many\ntools so around 10 plus different SQL\ntools are there which can explore the\ndata in Hado and some other people\nwanted uh you know machine learning to\nbe done on top of hadu so if you in the\nearlier versions before spark when\npeople want to do machine learning uh\nthey use something called mahoot a\nmahoot h here you can see mahoot\nsomewhere here this is your uh machine\nlearning library but what is the problem\nmachine learning is what okay you guys\ncan teach me right machine learning is\nall about\nwhat you all\nsitting fine it is about\niteration what do you mean by\niteration like you are taking the same\ndata and iterating over and over again\nright one of the things which map reduce\ncannot do is iteration because if you\nwrite a map reduce program the mapper\nwill run producer will run it will push\nthe data to hard disk again the second\niteration again you have to read it so\nwhen Maho was doing machine learning the\nproblem was that it was very very slow\nbecause internally it uses map ruce on\ntop of Hadoop so mahud came but this\nbecame your machine learning uh tool\nokay and so Interactive we covered for\ngraph processing we got graph lag pral\nso these are the tools which you use for\ngraph processing on Hado traditional\nHado no spark or nothing then people\nwanted to do a streaming data analysis\nrealtime data that is where you got\nstorm right so you see the problem right\nover the period of 8 to 10 years people\nstarted developing different different\ntools to handle different different\nworkload so if if today I go to Hadoop\nsystem and if I'm very new to Hadoop and\nif I ask somebody what tool I should\nlearn then he will be confused because\nall these things you have to learn Pig\nHive Maho everything to start working\nwith Hadoop another problem is if let's\nsay one fine day you start learning\nstorm storm is realtime processing you\nhave to first learn Storm's API Storm's\nlanguage then you have to install it on\ntop of hadu you should learn the\nintegration then only it starts working\nso learning all these different tools\nactually became a problem okay and what\nhappened in 2014 when spark came the\nbiggest USB of spark is that it is a\nunified processing engine what these uh\n50 plus tools on Hadoop are doing spark\nalone can do that is the real uh\nadvantage of spark now many people\nconfus here because if you go to anybody\nand ask like why somebody should learn\nspark they will say because it is faster\nnow speed is a byproduct of spark it is\nfaster I know but the real reason why\npeople are migrating to spark is that\nall these tools whatever they are doing\nspark alone can do so you don't have to\nlearn like hundreds of tools you learn\none tool one language and you can do\neverything but you need to have some\nbasic idea about what you're going to do\nfor example if you want to do machine\nlearning spark has something called MLB\nmachine learning library that will allow\nyou to do machine learning but you\nshould know what is machine learning\nthen only you can do otherwise you\ncannot explore it but I don't have to\ninstall 10 tools or 20 tools spark alone\ncan do it so the primary reason why\npeople are going for spark is this\nGeneral unified engine in my experience\nthe second reason is speed obviously it\nis faster I will tell you why it is\nfaster how it is faster and all the\nthird reason is ease of programming like\nyou know python now so you can get\nstarted with python and Spark the\nlanguage is Python and if you want to\nwrite SQL queries or streaming or\nmachine learning for everything you can\nuse python in spark that is not the case\nin the traditional World Pig uses a\ndifferent language storm uses a\ndifferent language so every tool here\nactually use a different\nlanguage even for graph processing in\nspark you can use Python normal python\nso ease of programming is another thing\nbecause I have struggled a lot because I\nhave worked couple of these tools I have\nworked as part of the project and then\nthe major problem is that you will take\nsome three four months to learn the tool\nfirst then when you come back that tool\nwill be obsolete they say go and learn\nsome other tool so with spark that Gap\nis gone so that is why every company\nwant to migrate to spark now he is\nasking um in in spark so there is\nsomething called streaming we will come\nto that streaming data means you want to\nprocess real time data say I gave you an\nexample right so let's say uh credit\ncard fraud detection I swipe my credit\ncard and that has to be immediately\ncaptured by some system and it has to\nsay whether it is fraud or not this is\nreal time right now to implement this\none solution I have is spark streaming\nin spark I can't do it okay but spark\ndoes something called micro batching\nmeaning it is not able to capture a\nsingle swipe as it is it will collect\none second worth data\nso I cannot go be below one second one\nsecond worth data but if I'm using\nsomething like storm it can even pick\none swipe in that microc second I say\nthere is a swipe but I'm saying that is\nnot so important like one second worth\ndelay you can suffer at least still you\ncan manage with spark most of the cases\nbut yes spark supports streaming so in\nthis and this pral giraffe these guys\nare gone the graph processing is totally\ngone we have sparkk for that most of the\nMaho is gone I told you right T\nT is there uh soon will be gone almost\nsoon will be gone because stas is middle\nof map produce and Spark what T does it\nmakes your map reduce faster that is\nwhat T does basically so spark makes it\nmore faster so it is in the middle so\nalmost it'll be gone in couple of years\nhoton works is the company who's\npromoting T so probably they will keep\nit because they want market share right\nthat is a reason okay let's see what is\nspark right so in spark you have\nscheduling monitoring and distributing I\nwill show you this\npractically and I like this uh diagram a\nlot because this will show you the spark\narchitecture in a very nice way actually\nso how does it look to you like if you\nlook at this diagram what do you think\nit\nis everybody saying it is solar system\nso the point is at the core you have\nspark so when you download spark you get\nsomething called Spark spark core okay\nand that is the lowest layer of\nabstraction in spark where you can start\nprogramming directly and I will show you\nwhat to do there and you see there is a\nram and hard disk picture which came up\nwhich means spark can use both RAM and\nhard disk and I'll tell you why this is\nvery important later usually other\nsystems can also use and these are the\nfour languages currently supported in\nspark code for programming so this is\nScala this this icon is Scala this is\npython R and\nJava as of now in 230 only four\nlanguages are supported so you should\neither learn any of this and you already\nknow python so it makes sense you can\ncontinue with\npython I teach spark with Scala that is\nalso a very uh the source code of spark\nis actually written in\nScala that is the reason some of the\nfolks want to learn Scala and do it but\nnow uh so earlier in spark 1.6 version\nand all if you write a python code it\nwas\nslower there is one thing you have to\nunderstand if you are in spark 1.6 and\nif you write your spark code in Python\nit will be slow and Scala code will be\ntwice\nfaster it was like that because they\ndidn't do any\noptimization Java also slow slower than\nuh Scala fastest one was Scala at that\npoint in time\ncorrect correct still it will be a bit\nslow because they didn't implement the\ncertain libraries that was related to\nJava to support the uh you know\noptimizations in Java so this is only in\n1.6 and 1.6 is very rare in Spar two\nwhat they have written they have written\nan abstraction layer so that you write\nthe code in any of these languages your\nprogram will run at the same speed so it\ndoesn't matter you're writing python or\nScala or Java everything will be same so\nas of now we don't have to worry about\nit SQL is also there but SQL is not a\nlanguage as such I mean you can also\nwrite queries and all using SQL okay go\nfor one more\norbit yeah so these are the so you will\nbe thinking that if spark is able to do\neverything right how it is able to do it\nso these are the libraries available in\nspark meaning when you download spark\nyou get something called spark fine and\nthat is like map reduce you can write\nbut maybe I'm not a core programmer I\nknow only SQL I want to write SQL you\nuse something called spark SQL in spark\nSQL very much like Hive and all you can\ncreate tables and there's something\ncalled Data frame you can create them\nand query the data and the good news is\nthey integrated spark SQL with hive by\ndefault so what will happen in a Hadoop\ncluster if you install spark okay and if\nI open spark SQL I can read the data\nfrom my hive table so previously when\nyou are running queries in Hive it is\nvery very slow now the same tables you\ncan read in Spar SQL and then query so\nthese days Hive is just used as a\nstorage most of the processing will be\ndone by your spark SQL because it is\nfaster ideally right uh another library\nis ml this is what I was saying your\nmachine learning libraries so so\nwhatever you are learning you can\nImplement here I don't think deep\nlearning and all are supported because\nfor deep learning and all you will be\nusing tensor flow do you have deep\nlearning in\nsyllabus yeah huh now by default it not\ncome I'm saying you have to download and\nimport it separately H third party but\nyour regression and all like normal\nstuff right they are all supported by\ndefault so what algorithms you learned\nso\nfar which one only regression\nso no na buys and decision trees and\nblah blah blah okay H so they are all\ncoming in ml anyway and graphic is the\nframework where you can represent your\ndata using a graph have you ever done\ngraph processing no right I have done\nokay because why this is\nimportant if you have studied uh\nengineering computer science you could\nhave learned graph\nTheory right I mean I have learned I\ndon't know was there in your syllabus\nthere is something called graph Theory\nso there you have some stuff like you\nhave edges and relations you know you to\nDefine like this\nright so this will be\nme this will be you this will be U one\nand and then you write relations between\nthem like I like you probably you don't\nlike me so you like someone else like\nthis so I mean you have property graphs\nwhere you have an edges okay which\nrepresents some property and then you\nhave relations between them okay and\nthen you can query the structure so for\nsome type of uh we partially work with a\nsocial media company in one of our\nprojects not like Facebook and all they\nwere a startup actually there their\nentire data was like this even Facebook\nright if you download the data from\nFacebook it's very difficult to do it\nokay you can't directly do it but there\nis a way to get the data data means all\nthe publicly available data they will\ngive you in the graph API format the\nformat in which the data comes is graph\nit doesn't come in Json or XML it's\ngraph API format because Facebook is\nactually using this graph structure to\nstore not The Spar graph I think they\nhave their own some system but the\nrepresentation is in the form of a graph\nactually so many times this is required\nthis graph representation of data and\nSpar can do this Graphics Library which\ncan represent your data and then stream\nstreaming data streaming data is where I\nwas saying you have real time data\ncoming in and you want to process it\nreal time and then make a decision based\non that yeah so some of the queries you\nknow uh if you write like this it'll be\nmuch more efficient the multi-\nrelational queries say for example this\nguy like this guy this gu so if you want\nto find out patterns and relations\nrepresentation using graph is easy in\nsome cases let me give you another\nexample this airport data we had if\nyou're having Airport ports and flight\ndata right so let's say I represent\nairports like this\nBangalore Chennai I don't know USA\nsomewhere right and if I represent all\nthe flights which are going in you\nknow I can do this using SQL also but\nthe graph queries will be much much more\nfaster for me because this directory\nstructure right the graph structure is\nable it is easy to Traverse in the graph\nAPI so probably between two airports you\nhave like thousands of flights currently\nflying so you want to query them and\ntrack something so always for airport\ndata use graph type of a system rather\nthan SQL will work but it will be slower\nif the data is really high even spark\nSQL will be slower so this airport\nmanagement right how many flights are\nflying in real time and so on and so\nforth they use this graph API a lot so\nonly for certain use cases where you are\nhaving let's say two edges and lot of\nrelations between them social media is\nanother example right so I want to find\nout what you like in Facebook let's say\nso I want I have to do a lot of\ncomparison how many friends you have and\nwhat your friends actually like do you\nactually like that so traversing these\nqueries in SQL is very difficult in\ngraph I can easily Traverse I can just\nsay that these these these notes find me\nthese type of relation easily tell me so\nsocial media companies do use a lot of\nGraphics actually even Twitter use\nTwitter how many people are you\nfollowing and how many were followed by\nyou so this relations right these are\nall learned from graph apis in\nTwitter we will not go to graph so and\nand there is a no SQL this is different\nokay there is a nosql database called\nneo4j\nneo4j it's a nosql database they store\ndata in graph format so I mean that is a\ndifferent use case that is for Real Time\nqueries and all but this company\nactually uses only graph to store the\ndata okay Graphics is different that\ncomes part of spark NE 4J is a nosql\ndatabase I had worked in a small project\nwhere they were using\nneo4j I didn't learn it actually but\nI've seen that people are actually using\nit okay what do you most some of you\nmight understand but some of you may not\nunderstand so we went one uh Circle\nfurther right so we are in one more\norbit okay now let me ask you what do\nyou make out of\nthis who is\nthis no is there anything special about\nthis picture fine I mean\nuh uh so what is this\nthis Flamingo okay what is the flamingo\ndoing huh waiting for who be be creative\nright so what is a flamingo doing\nstanding on one huh yeah you very\ncreative exactly this is called\nStandalone mode of\nspark I didn't create this slide it is\nData slide so if you have to blame blame\nthem exactly Flamingo is standing in one\nleg right so you have to be more\ncreative okay for learning spot so these\nare different modes in which you can run\nspark basically okay so don't get\nconfused with the name so the top one\nthis single PC that is called\nlocal local mode means you are running\nit locally on one machine okay you\ncannot expand to more than one machine\nand you use only for development and\ntesting purpose and that's\nunderstood this mod is mesos M this is\nicon of mesos so originally spark was\ncreated on top of mesos I told you right\nso still if you want you can install a\nmesos cluster right and then install\nspark on top of that it will run no\nproblem right and this is stand\nStandalone mode so what is Standalone\nmode I want to install spark in a\ncluster but I don't have yarn I don't\nhave mesos I don't have anything then\nspark give you its own cluster manager\nthat's called\nStandalone but this is very rare because\nideally it'll be on yarn and this is\nyarn spark on yarn this is yarn so\nideally in most cases what you see is\nyarn this blue ball I mean that is yarn\nuh because most of the organizations\nwill already have a Hadoop cluster and\nyarn and it just makes sense to\nintegrate spark on top of that another\nvery important Point lot of people get\nconfused that is why I'm writing\nit spark\nhas no\nstorage there is nothing called storage\nin spark because it is an execution\nengine like map reduce is map reduce\nhaving any storage no it reads the data\nfrom Hado process it then probably store\nit back in Hado right similar to that\nspark also has no storage there is no\nstorage component in spark it is just an\nexecution engine which means you should\nprovide the data somewhere so if you're\nrunning spark on Hadoop the data is on\nhdfs if you're running spark on let's\nsay your PC your PC file system has the\ndata Standalone or mesos whichever up to\nyou to decide okay if you're interested\nsome extra knowledge spark can also run\non\nkubernetes\nkubernetes spark on kubernetes is I\nthink production ready now you know what\nis kubernetes so okay for those who\ndon't know probably these things might\nbe useful in future because kubernetes\nare actually running somewhere so you\nhave something called Docker right there\nis something called Docker this is a\nvery old tool very old like four or five\nyears I think I don't know what Docker\nallows you to do you know what is a\nvirtual machine right you know what is a\nVM so in your laptop if you create a VM\nwhat will happen let's say I install a\nWindows VM the problem is that this VM\nwill use a lot of resources right\noperating system you to install\neverything so if my laptop has 8 GB Ram\nI need to give 4GB Ram only for the VM\nwhat Docker does it allows you to create\nsomething called a container\nokay meaning on one laptop I can run\nlike 10 Docker containers one Linux One\nwindows it will use the libraries that\nare already available on my base\noperating system so it will not install\na full operating system that's what I'm\nsaying so instead of giving 4 GB RAM to\na VM I can give 1 GB to a Docker or 512\nMB to a Docker and if it is on Linux it\nis much much good performance so Docker\nbecame a big hit because if you are a\nprogrammer you wrote a Java program you\nwant to test it you want to test it on\nApple you want to test it on Linux\nWindows how do you do all of this right\nso you can just launch Docker containers\non your PC itself like 10 of them run\nyour code see whether it is working very\neasy right kubernetes is the next level\nof Docker where you have something\ncalled container orchestration in a data\ncenter you are able to run multiple\ncontainers like Docker containers you\ncan manage all of them using\nkuber if I'm correct it is is a Google\nproject okay so it is a data center\norchestration so Docker is like you are\nrunning it on one machine probably 10\nmachines match so here once you install\nceretti it become a data center manager\nactually so you can give like hundreds\nand thousands of servers to kuber and\nsay that I want these many containers\nDocker like containers it will launch\nthem and manage them for you so kuber\ncan become one of this instead of Y or\nmesos or this thing kuber can become\nyour resource manager I can install\nspark and then I can say that I want to\nrun spark program if I say hit Ender\nit'll go to kuber it will launch\ncontainers for me to run the spark\ncode Docker\ncontainers yeah yeah cloud cloud\neverything is uh available on cloud\ncloud everything supports so in Cloud\nyou have multiple options either you can\ndirectly run say I go to Amazon I create\nsome machines myself or I can use these\nservices like\nEMR\nEMR elastic map reduce I can go to\nAmazon and say that hey Amazon I I need\nyou to create a h Spar cluster for me\nsay 10 machines 100 machines in 5\nminutes It'll create and give it to me\nand then I can run all my workload but\nthese are like disposable clusters once\nyou complete your job you have to delete\nthem otherwise you're paying money\ncontinuously right\nso uh so U kubernetes is on the cloud\nalso I mean it is local data center as\nwell as Cloud so it is available\neverywhere I don't know I have not\nextensively gone to kubernetes but uh I\nthink you can search because uh it is\nsomething which is coming up in a big\nway I think Google is\nthe Ku kubernetes kubernetes whatever\nyou call\nso this is production grade container\norchestration so the problem with your\nDocker and all is that it is very\ndifficult to manage the containers with\nkuet you can manage them\nlike ah it is a cloud only I don't know\nthe exact architecture of kuber like\nwhether they launch their own container\nor Docker container but basically so it\nis all coming to an abstraction now\npreviously everything was very clear for\nexample you buy a\nserver and you you can see the hard disk\nright then you install the operating\nsystem now what kubernetes says that\ngive me a data center you don't worry\ndata center group of servers give me a\ngroup of servers like a data Senter and\nI will launch as many resources as you\nwant you just tell me how many\ncontainers you want I will ensure from\nwhere I need to launch how I need to\nlaunch so like ah same like that so like\nan abstraction it is getting right so\nbut uh probably so it say ah kubernetes\nbuilt up 15 years of EXP experience of\nrunning production workloads at Google\nyeah so originally this came from Google\nGoogle was already running this not in\nthis name some other name so they now\neven Hadoop came from Google right so\nsame like that they are creating this\nkuber\nnetti I think spark on kuber is\navailable that is why I was speaking\nabout\nthis spark on\nkubernetes yeah very much available\nSpark 2.0 you must have a running kuber\ncluster blah blah blah ah do ah it is\nDocker only okay Docker images see so\ninternally it is using Docker only\nnothing\nelse blah blah\nblah yeah so it is supported so it is\nnot in the slide that's why I'm saying\nin the slide you don't see kuber slide\nis a bit\nolder and if you pay attention to the\nslide I can show you one small change\nyou have to tell me what is a change\nokay if you pay attention\ndid you see there is some change in the\nslide what is a\nchange no no somebody's on top of this\nright that is\nZookeeper zookeeper oh you didn't say\nI'll show you once more see it's gone\nnow it'll come up you saw that so what\nit technically means is that all these\nthree modes whether you are running on\nStandalone or mesos or yarn can be\nhighly available using zookeeper\nzookeeper supports your high\navailability if you like spark if you're\nrunning if one machine crashes or or uh\nyou know if all these states are\nmaintained by zookeeper so zookeeper is\nintegrated with\nspark zookeeper is another Hadoop\necosystem tool why it is used is for\ncommunication between the machines\nactually to put it very simply like in a\nvery large cluster if one machine goes\ndown then how do you know\nno it's not like a lot balancer um how\ncan I\nexplain so I'll give you a simple\nexample otherwise zookeeper might become\nanother big problem\nright and zookeeper is an admin thing\nmostly so you don't have to worry about\nit too much but\nstill do you remember I told you you can\nhave a active name not a standby name\nnode right so in a Hadoop\ncluster you have two name notes\nright this is\nactive this\nstandby correct now what is the idea\nonly only one should be working only\nactive if the active\ncrashes this question who will tell this\nthat's a question let's say I'm\nconnecting to the cluster how do I know\nwho is active I can't connect to two\nmachine right so this guy will have an\nIP address this guy will have an IP\naddress one way that I can keep on\npinging who is active\ncurrently how do I know who is\nactive who will tell me right there is\nnobody to tell so one way is that I can\nkeep on pinging both the machines to see\nwho is alive who is not alive so like\nthat there are many situations in which\nyou have to understand which machine is\nalive or not alive so with zookeeper\nwhat happens is very\nsimple it is a service\nso this active name node will register\nwith Zookeeper okay standby will also\nregister with zookeeper now you just ask\nzookeper who is active it will tell this\nis active if this guy crashes okay this\nguy will inform Zer that I'm the new\nactive you again Ask zookeeper zookeeper\nwill tell you so it's a coordination\nmechanism zookeeper is used for\ncoordination and high availability what\nthis slide means is that spark support\nzookeeper so for talking between the\nmachines and management and all it can\nuse zookeeper if possible if available\nright so this this is the icon of\nZookeeper it it didn't just come this is\nthe actual icon of\nZookeeper you are asking uh I know\nbecause normally people are not much\naware of\nZookeeper no no it is it is for um all\nthe services actually let me show\nyou so zookeeper is for any service to\ncoordinate so one example I gave you is\nthat how do you know name no is working\nideally this will not automatically come\nup the Zookeeper has to inform this guy\nto come up if you look at the\narchitecture okay if you implement High\navailability in Hado normally what\nhappens if I don't have zeper if this\nguy goes down I have to write a command\nto make this up that is actually a\nwastage of time right it should\nautomatically come up so these guys will\nbe connected with zookeeper and there'll\nbe a keep alive message sent so if ah\nheartbeat so if this guy goes down\nzookeeper will know that this guy is\ngone after let's say a second or two\nthere is a timer you can configure it\nwill ask this guy come up okay and\nupdate the metadata here so basically\nthis guy holds metadata who is doing\nwhat in the cluster otherwise you see\nthere are lot of services in a hard\ncluster where there is active and\nstandby not only name node resource\nmanager your yarn resource manager is an\nactive and standby if you take hbase\nthere is a service called now SQL hbas\nhas a master and standby so if I go to a\nHado cluster if I start asking everybody\nwho is the master who is the slave it's\nvery difficult so I go to zuker and this\nguy will have all this knowledge even\nfor for spark you have a active and\nstandby Master okay if you're installing\nspark independently and that can be\ncoordinated with zookeeper that is what\nthe slide is\nsaying so zookeeper by default will be\navailable in most of the\nClusters we again went\nback now now what so this orbit is\nover spark can read from almost any file\nsystem and that's by\nhdfs uh I don't know what is this Cube\nby the\nway this I don't know this is S3 Amazon\nthis is local file system that is some\nother file system maybe any file system\nbasically uh so it's supports a lot of\nfile systems even which we are not aware\nof right local file systems\nit can read from all these things any\nnosql database any rdbms that is I think\na very cool feature because I will show\nyou if you have a MySQL DB Spar can\ndirectly read from the table or if you\nhave mongod DB or any nosql database it\ncan read from the table and process the\ndata output probably it can store back\nit into uh that table\nright it also supports Hadoop input\nformats\nspark streaming can work with fluman\nKafka meaning you are doing realtime\nstreaming right so the question is that\nhow do you get the\ndata so this is always a challenge\nbecause what happens is that normally\nyou will have a spark cluster\nrunning so let's say this is a spark\ncluster Hado cluster only where you have\nspark\ninstalled so let's say you have a spark\ncluster running I want to get Twitter\ndata so actually spark can directly get\nthe Twitter data there is no problem it\ncan directly come to the cluster and you\ncan process it but the problem is if one\nof the machine who is receiving the this\ndata crashes you will lose the data for\nsome\ntime right in\nspark so what you can do in this\narchitecture is that for high\navailability you can either use Flume or\nKafka so you can ask these guys to get\nthe data for\nyou Flume or\nso Flume is pointto point delivery of\ndata like I can configure a flume agent\nwe will see in the webinar which can get\nthe data from Twitter to here Kafka is a\nmessage CU so you can get the data from\nhere so even if your machines are not\nworking data will come here right you\nare not losing the data so for that\nreliability you are using a flume and\nKafka if you have a spark cluster right\nso this is a spark cluster and I'm\ntalking only about spark streaming okay\nnot like normal processing when you\nconfigure spark streaming one of the\nmachine one of the machine will start\nworking as something called a\nreceiver okay so what is this machine's\njob to get the data that's all it cares\nand it's a normal data node now the\ndrawback of spark streaming not the\ndrawback I can say if this machine\ncrashes if this machine crashes okay\nyour stream will be lost\nof course it will switch to another\nmachine but it may take some time so\nthat let's say 5 seconds or 10 seconds\nyou losing the data right you're not\nstoring it anywhere correct so if I want\nto avoid this if this machine goes down\nthen my stream will be gone if I want to\navoid this what I can do I can say Kafka\nhey get the data in Kafka okay from\nthere I'll get to spark even if this\nmachine crashes another machine will\ncome up the data will be available here\nso spark streaming if you want\nreliability you have to use Kafka or\nFlume direct stream you may not have\nreliability okay\nso if we are covering spark streaming\nprobably in that architecture side I\nwill speak the distributions we already\nknow and these are some statistics and\nuh I think you should be aware of it\ndevelopers from 500 plus companies you\nhave 3,000 developers and etc etc\nright uh this is a slide some of you are\nasking will some of the tools go extinct\nyes your hive now nobody's running a lot\nof Hive queries everything is spark SQL\nqueries these days and Maho for machine\nlearning everybody has migrated to spark\nM storm most of them are in spark\nstreaming now uh and this slide just\ncompares the difference ways in you can\nrun spark for example instead of map\nreduce now most of us are using spark\nthe the uh resource manager can be Yan\nor\nmesos doesn't matter any of this and the\nstorage level in Hadoop is hdfs this\ntachon is not there now tachon was a\nstorage manager like in 200 N long back\nwhen when originally mesos came the\nstorage was handled by a system called\nton now this is renamed as alusio aluk\nor alusio this project is still there\nthere is a storage layer like hdfs only\ndistributed storage you can get so one\ncommon problem in spark is that you will\ntry to bring the data from different\nplaces like spark will be running on\nHadoop but maybe some of your data is in\nHadoop some of them is in rdbms some of\nthem is in Cassandra so if you use\nnormal hdfs it's fine if you're using\nthis tachon or now it is known as alusio\nit has a caching layer so it can speed\nup your processing by caching the data\nthat is only Advantage not extensively\nused because setting it up and all are a\nmess actually okay I have seen it once\nthat time it was called ton only not\nalusio I think they renamed it if my if\nmy memory serves me well ton is alusio\nnow spelling I have to look\ninto alusio alio or alio whatever you\ncall I don't know I don't\nknow uh alio formerly T\nyou open source memory speed virtual\ndistributed\nstorage I don't know how many\nTechnologies are there so many are there\nactually this is like so let's say you\nhave tons of data and you need a storage\nlayer in between caching layer to make\nit faster then you use alio that is only\nuse case otherwise everybody is using\nhdfs only nor normal hdfs\nin SSD SSD caching it will do so it is\ncostly or it is costly but faster but\nonly use case is that if you're\nprocessing a huge amount of data like uh\nwe had like Bank of America Bank of\nAmerica they had like tons and tons of\ndata to process like terabytes of data\nand it keeps on coming so storing them\non hdfs first time read will be very\nslow anyway will be very slow so they\npush directly to your this thing alusio\nbecause that's an SSD caching layer from\nthere they read it so read is faster\nactually that is only use case you are\nseeing for aluo and this is another use\ncase of it right you have on premise\nstorage and cloud storage and you're\ngetting the data it comes to an alosio\nlayer from there you can start your\nComputing so it is a storage layer\nabstraction you can store data in\nmultiple places okay and alio will get\nit in one place from there we can start\nprocessing in so it is like bringing the\ndata into a caching layer and then\nprocessing it\nokay don't worry if you don't know alio\nalso it's perfectly fine it's not like\nan mandatory component or\nsomething okay now can you tell me what\nis a drawback of map reduce if there is\nany drawback what do you think so spark\nis replacing map reduce right so what is\na drawback of map ruce that you\ns yeah so mapper output you purchase\nthen output you persist then of course\nreducer output you have to finally\npersist right so that is why map redu is\nvery very slow so that is what is in the\nslide actually written so and if I have\nto do an iterative processing right I\nread the data and 10 times I have to\nprocess it then I have to schedule the\njobs 10 map ruce jobs I have to run\nbecause it is very difficult to change\nthem together intermediate read and\nwrite always will be there right uh that\nis where Spark\nbecomes very different\nbecause and this slide just says that\nyou can use Uzi to schedule all these\njobs nothing else spark does something\ncalled inmemory\nprocessing and this is very confusing\nfor many people the first thing you need\nto understand is that inmemory\nprocessing means it will use Ram if\navailable doesn't mean that you always\nneed to give Ram if Ram is provided so\nso let's say you want to process 10gb\nfile and if your cluster has 10gb free\nRam it will read it into the RAM and it\nwill do all the calculation only the\nfinal output it will push it into your\nhard disk intermediate results are not\nstored onto hard disk second point is\nthat it will if what if Ram is not\navailable then it will start using hard\ndisk also step by step it'll read it'll\nread whatever data that fits into the\nRAM process it then again it'll read it\nand process it like that it has to go\nthere is no other way right if you don't\nhave Ram still it is faster than M\nproduce okay because from ground up they\nhave designed the code of uh spark they\nhave not modified map ruce code or\nsomething to create spark uh and that is\nwhy here it says 10 to 100 times faster\nthan map reduce typical map reduce\nright so in memory processing we will\nsee uh how it is faster and all\nah that end is Cassandra that I that is\nCassandra it just demonstrate that you\ncan store the result in Cassandra from\nanywhere you can read and store\nalso uh these are the Distributors and\napplications which uses Spark so of\ncourse data brick is the major\ndistributor and hoton works Cloud era\nall these guys are having uh you know\nspark as of now and these all\napplications can use spark for\nprocessing so mostly bi tools and all\nright visualization tools so previously\nif I'm using something like uh where is\nit uh pentah okay to visualize my data\nwhat it will do it'll fire a query to my\nhad cluster hi will run the query and\nvisualize it and now spark SQL will run\nthe query so it's much much faster so\nall the bi tools and ETL tools now use\nspark for moving the data and processing\nthe data and so on and so forth uh this\nslide is a bit old this is the 100 tbte\nuh sort competition in\n2014 so every year there is a sorting\ncompetition that will happen even you\ncan participate if you want so the uh\nthe thing is that they will give you one\nterabyte of data okay uh you have to\nsort it okay and the data they will\nalready give you the format and all\nbased on how you have to sort it is it\ninteger sorting or quick sort or what\nsort they will tell you they already\ngive you the data also sample data and\nyou to write an algorithm to sort can be\na Java program any program you can write\nand whoever sorts the data fastest will\nwin that is the uh conclusion and in\n2014 when this ran uh spark did it in uh\n23 minutes Hadoop in s map reduce in 72\nminutes but look at the cluster size\nspark was running on 206 machines map\nrce was in 2,100 machines that is a\ndifference 1 by 10th of machine still it\nis faster because lot of ram was\navailable everything was in memory so\nfasting faster sorting will happen and\neven for one petabyte they did a sorting\nand again spark became the winner there\nokay so you are having these notebooks\nyou know what's a notebook\nright ah in Python you use right so you\nknow what it is luckily I don't have to\nteach that so I mean the code is\nactually written in a notebook okay uh\nbut we will use the shell also bit\nbecause I don't like notebooks\npersonally\nokay do you know what is a driver these\nare some things you require otherwise\nyou cannot understand spark that's\nwhy what is a driver you know not device\ndriver\nokay which drives the program that's\nactually correct right driver is the guy\nwhich so when you're writing a spark\nprogram the program will have something\ncalled a driver okay and this is the\nmaster of the\nprogram driver is the master of the\nprogram and then you have something\ncalled\nexecutor this is the\nslave so let's say I wrote a spark\nprogram in the spark program definitely\nthere will be a driver and an Executor\nwithout that the spark program cannot\nrun I mean these are logical Concepts\nI'm saying okay now when you want to run\nthat program okay one way you can run is\nthat you can say run it\nlocally I can say that hey I wrote a\nspark program okay run the program local\nwhen I say local what will happen is\nthat a jvm will be\ncreated like a jvm okay both my driver\nexecutor everything will be inside\nthis so this is the local mode of spark\nJava virtual\nmachine\nH still a container will be created\nsimilar to this I'm jvm in a general I'm\nsaying okay so a container will be\nallocated so this is like a yarn\ncontainer H so a container will be\nallocated in that both your driver and\nexecutor will run so the local mode is\nnot very efficient because you will get\nonly one container and everything is\nrunning inside that right so if you're\nsubmitting the Spar code actually when\nyou say you you have a python code right\nit will read your logic and it convert\nand run it inside a j end of the day\neverything is inside a jvm only without\nthat it cannot run even map reduce\nprograms we wrote in Java right we can\nwrite python code in map ruce so if I'm\nwriting a python map ruce program if I\nrun I will say uh submit the program\nthen I have to mention some jar files\nand those jar files will be in Hadoop\nthey will read your python code and\nconvert that into a format which will\nrun inside a jvm and execute inside a\njvm only because end of the day it is\nwritten in Java or if it is spark it is\nWR Scala right so everything has to be\ninside a jvu so the local mode the uh\nproblem is you get only one container\ninside that the driver and executor\neverything will run okay so this is only\ngood for testing purpose right right so\nyou just want to test a spark program or\nyou want to learn spark you will\nnormally say that hey Spark Run in the\nlocal mode I write some code it just\nruns if you are in a cluster this is\nwhere things becomes\nmore interesting and this is a bit\nconfusing also okay so this is my Hadoop\ncluster\nright and I have four data nodes\nimagine I\nhave four data nodes imagine I created a\nspark program now the first question is\nthat when you running the spark program\nwhat are you analyzing okay I'm\nanalyzing a file what is the size of the\nfile right so let's imagine the file is\nin Hadoop okay just a simple use case uh\nthe file is here here and here just an\nexample and somewhere the total size of\nthe\nfile is let's say\n10gb so I have a 10gb file in Hado that\nthat may be in blocks and all so that's\nunderstood and I want to process it\nusing spark now and that is in a cluster\nright so when I submit my program to a\ncluster I have to ask Yann for\nexecutors how many executors you want\nand what is their\ncapacity meaning I can either say he hey\ny give me one executor huh and in that\nexecutor 20 GB Ram I want no problem\nwhat will happen an Executor will come\nhere this is your\nexecutor it's a container nothing but a\ncontainer and this has let's say 20 GB\nRam okay and some four processor core or\nsomething and then your entire Spar code\nwill get executed inside this but\nnormally people will not do this this is\nlike one machine is executing everything\nso possibly what I will do is that I\nwill ask Yan hey Yan give me let's say\nfour executors just an example I'm\nsaying give me four executors so Yan\nwill give me four\nexecutors right and each executor I tell\ny give me 5gb each ex this is 5gb this\nis 5gb this is 5gb this\n5gb ram ram in memory so total 20 but\n5gb right and so imagine there is and\nthen there is a driver right so this is\nExecutor imagine you have five nodes in\nthis Hado cluster so here my driver\ndriver is\nrunning okay so what will happen when\nyou submit the program in one of the\nmachine the driver will start\nrunning and in the program you have\nasked Yan give me four executors and\neach executor I want 5gb Ram two\nprocessor core blah blah blah so Yan\nwill launch 1 2 3 4 here and whatever\ncode you have written this driver will\npush into all four machines at the same\ntime and each machine will process your\ncode this is how in a cluster spark is\nprocessing but this is not so easy as it\nlooks like because you should know how\nmuch memory you need for processing\nright and how many executors you want\nwill ra will Yan actually give you those\nI cannot say that give me 100 executors\neach with 100 GB Ram Yan will say that I\ndon't have capacity I can't give you so\nnormally wherever I have went for\nConsulting and all uh if you write a\nspark program you will discuss with your\nadmin okay I want to write a spark\nprogram because in spar Mark if you want\nto get the full performance it should be\nin memory you need maximum RAM and that\nis a challenge in every Hado cluster so\nyou will go to your Hadoop admin or\nspark admin and say that I have written\na program I need 100 GB Ram in the\ncluster so he will tell you in your\nprogram you ask for let's say 10\nexecutor 10 GB each something like that\nright that you will configure as the\nnumber of executor how much mam you want\nand then you submit your program so Yann\nwill launch these things and the driver\nwill be running on a separate machine\nright and whatever logic you have\nwritten so driver will read and start\npushing to all executors and the output\nnormally comes back to the driver\nwhatever output you have in the driver\nyou can write a logic either store the\noutput on Hadoop or store it in\nCassandra or wherever you want that is\npossible and this driver normally will\nbe your appm this is your app master\nMaster remember App Master from\nYan that is your driver so a lot of\npeople ask what if this machine goes\ndown my driver machine goes down\nobviously if your driver machine goes\ndown then the processing will be\ndisturbed now spark program will not\ncrash okay uh in Yan you can configure\napplication Master restart timer so\nrestart so if this machine goes down it\nwill restart your am on another machine\napplication Master on another machine\nso that it can continue processing this\napplication Master is in constant touch\nwith your resource manager ah so there\nis an ID application ID where it will\nnot tell what you have processed uh or\nwhat execution is going on currently it\nknows those things so your resource\nmanager will launch one more application\nmaster that will become your spark\ndriver get the code it starts running\nfrom\nthere so the so the driver is the master\npart of your program and these execute\nuors are where you push your code so\ndepending on your cluster you can ask\nfor the number of executors and how much\nmemory you want for each executor that\nis how it actually runs and there is\nsomething called application Master\nrestart timer you have to configure in\nYann there you can say three five so\nit'll try to restart it the application\nMaster on the same machine sometimes\nwhat happens this machine will not crash\nyour application Master will crash the\ndriver process will crash so it'll\nrestart it on the same machine can be\ndue to many reason maybe the resource\nnot available so you can configure the\ntimer and if it doesn't work then it go\nto another machine and say start from\nthere uh you need to understand\nsomething called R\nDD okay so that is why rdd fundamentals\nit is written so rdds are the basic\nbuilding blocks of spark so first thing\nyou need to understand is that in spark\nyour data is represented as something\ncalled\nrdd right any data that you have if you\nwant to process in spark the first step\nyou do is that you create something\ncalled an rdd rdd stands for resent\ndistributed data set it is like a\nvariable or a pointer you can say right\nso these slides will give you some idea\nabout rdd I will also practically show\nwhat is an rdd so just uh look at this\npicture this picture is very uh very\ngood in understanding uh how spark is\nworking right so I have four blocks of\ndata in Hadoop so that is what is\nrepresented there hdfs right so you know\nfour blocks of data is there right and I\nwant to process them now that is one\nfile even though it is divided into four\nblocks it is one single file I want to\nprocess and what is this data it's some\ntext Data imagine each block has like\nthis error then a Tim stamp and a\nmessage again warning some time stamp\nand a message some log file imagine\nright four blocks of data so we are\nassuming they are on four data nodes and\nyou want to process them now from here\nonwards it is going to confuse you okay\nso I'm saying in advance you will get\nconfused okay but don't worry so where\nis your original data in hard disk right\nas blocks in hdfs so that you have to\nkeep it in your mind now I have already\nwritten a program imagine to process the\ndata right in the program the first step\nI need to do is to create an\nrdd okay so I have to say that create an\nrdd an rdd is like a variable you can\nsay or a pointer so here the name of my\nrdd is called log lines\nrdd you can call call it Ragu if you\nwant I just say create he spark create\nsomething called log Lines rdd by\nreading the data from this file you can\ngive the location where is the file and\nif I run this what is going to happen\nand imagine in this case I have asked\nfor four\nexecutors in my spark program I said I\nwant four executors for some reason so\nwhat will happen in each data node one\none executor will be launched in ideal\nsituation\nright and I have one one block on each\ndata node and I say hey Spark create an\nrdd called log lines rdd for me and if I\nhit enter all this data will be copied\ninto the Ram or the main memory because\nrdds represent your data in\nmemory assuming Ram is available ideal\ncondition so if I redraw this picture\nthis is actually uh data bricks picture\nright but if I redraw this picture now\ndon't think that always the file is in\nHadoop right in in my typical example I\nhave the file in Hadoop but there is one\nsmall thing here let's say you\nhave uh six Nots in Hado cluster 1 2\n3\n4 5 6 that is the same architecture and\nblock one 2 3\n4 now the big question\nyou want four executors which machines\nwill launch the executor who is\nlaunching the\nexecutor yan yan has no data locality\nawareness right Yan doesn't know where\nis your data right in map reduce data\nlocality is there because your blocks\nare residing here let's say your blocks\nare here right so these are the four\nblocks I write a map ruce program what\nis going to happen in my map ruce\nframework it is written that okay I need\nto have four\nmappers one more very important point\nyou need to remember in map reduce if\nI'm processing the same data four jvms\nwill get\nlaunched correct one here one here one\nhere one here and this will copy here\nthis will copy here this will copy here\nthis will copy here that is how your\nmapper runs because can you process four\nblocks using one\nmapper not possible you will have number\nof mappers equal to number of blocks or\nnumber of input splits we say right\nideally you can say number of mapers\nequal to one but then what is going to\nhappen this block will process this will\ngo then next block will come and it will\nprocess all no no that's not possible\nbecause the size of this mapper this\ncontainer is like 1 GB or or the default\nyarn container size is there this is\nyour yarn container in map reduce so\nwhen yarn is launching containers for\nyour maper\nideally how it launches is that it will\nlaunch one one container for each block\nright and your block size is fixed 128\nMB right so in yarn settings you will\nsay that my container size is 1 GB I\nwant 1 GB container right and what will\nhappen 1 GB 1GB 1GB 1GB four containers\nwill be used it is very rare that\nmanually you mention number of mappers\nyou can't ideally do that because that\nwill affect your performance and also if\nyou're launching one container only you\ncan't resize it so this is like a 1GB\ncontainer so there is a property called\nset Spar do Dynamic execution enable or\ndisabled that is to Yan so you are\ntelling Yan I want eight executors but\ndon't kill everybody I may use it or may\nnot use it so it'll keep it for you if\nyou uh uh enable it that is where it\nwill kill you are saying that if it is\nnot getting used just kill them I don't\nwant them so if your data size is small\nit'll kill them but coming back to rdds\nand our discussion I'm saying that in\nthat example you have four blocks of\ndata so that is these four blocks my\nHardo cluster is let's say six KN and\nyou want to read this data and process\nit so what you say you say I want to\ncreate an rdd the name of the rdd is\ncalled log lines rdd or whatever rdd is\njust like a pointer okay and when you do\nthat when that code runs what is going\nto happen is that you ask for four\nexecutors also and let's assume the\nexecutors are launched here so there is\nan executor here okay there is one more\nguy here one more guy here and one more\nguy here so these blocks will be copied\nto here so now where is your data your\ndata is residing inside the ram of four\nmachines or four executors this is\ncalled an rdd because otherwise how do\nyou call it is your data you need some\nway to mention like like a variable so\nthat is called your RTD so now once the\ndata is available in the ram let's say\nnow the data is there there you you have\na representation to your data so this\nentire log file is now called log lines\nrdd also in spark there is something\ncalled\npartitions meaning right now your data\nis lying as four blocks so we say that\nthis is a four partition rdd so normally\nwhen you're reading from Hado you will\nhave blocks each block will become a\npartition what if you're not reading\nfrom Hado let's say I'm reading from my\nlocal PC I can mention how many\npartitions I need for the data say on my\nWindows laptop okay I have a uh 1GB file\nif I simply read it it will come as a\nsingle 1GB partition okay if I want I\ncan say that hey from my this is in my\nWindows laptop okay I can say that hey\nSpark read this 1 GB but create four\npartitions for me on this what is\nAdvantage each partition can go to a an\nExecutor an Executor will process a\npartition not a\nfile so always your data has to be\npartitioned the more partitions you have\nthe more parallelism you will get no\nthat's just an example anyway in the\npersonal laptop I cannot launch multiple\nexecutors I'm saying you're reading from\na personal laptop and dumping into a\nHadoop cluster imagine it's not from a\nHadoop file system or let's say you're\nreading from\nCassandra a typical example Cassandra is\nan SQL database Cassandra doesn't have\nblocks or anything blocks are on Hadoop\nso if I get the data from Cassandra I\nread a table I will get 1 million rows\nright 1 million rows I cannot give it to\nan\nExecutor right then the processing will\nbe very slow so what I will do I have to\nhave an idea about my data let's say 1\nmillion data is 1 GB or something I will\nsay that when I'm creating an rdd I can\nsay that take this data from Cassandra\nand divide it into four parties\neach partition will go to an executive\nah an Executor can manage more than one\npartition also minimum one partition it\nshould get depending so let's say I have\nan Executor with 20 GB memory H so let's\nsay a partition size say 1 GB it can\nmanage 20 partitions data locality is\nnot 100% guaranteed when you launch an\nExecutor 100% is not guaranteed so is\nthis clear because you have to figure\nout the number of\nexecutors okay first thing that depends\non what is the size of your data so in\nHadoop normally this is not required\nbecause in Hadoop what happens by\ndefault a block is a\npartition right but again that will make\nsense because what is the block size in\nHadoop 128 MB 128 MB is the block size\nright so I have a file with uh that is\ndivided into 10\nblocks so total size of the file is\nthis MB that is\n1.2 GB I want to process 1.2 GB file now\nthe question is that how many executors\nI need that depends on your cluster\nconfiguration I can even process this in\na single executor I can ask hey Yan give\nme one executor 2 GB Ram or 3 GB Ram\njust a safe side I'm saying so Yan will\nlaunch one executor let's say 3gb Ram\nokay and this has eight uh 10\npartitions this file has 10 partitions\n10 blogs all 10 partitions will come in\nthat single what is say container get\nprocessed ah copy original data is in\nhdfs anyway it's happen in the same ahuh\nso in this case I'm asking for one\ncontainer so this executor right so this\nexecutor you're launching okay so you\ncan ask how many executors you\nwant he's not understanding what I'm\nsaying so this is my my hard\ncluster that's what it is not very\neffective I'm\nsaying you can do it it is not very\neffective so the more partitions you\nhave more more processing speed you will\nget the same example if you take so I\nhave four partitions right four\nexecutors I launched and each is getting\nloaded into one partition this will be\nfaster I can also launch a single\nexecutor here copy all this four here\nit may not be so fast and another very\nimportant point so these are all related\nto Resource Management but you should\nalso understand this stuff\nright an Executor is a\ncontainer right a jvm One processor core\nis required to manage one\ncontainer meaning if this is a dual core\nmachine how many maximum executors you\ncan launch\ntwo if you ask for eight executors what\nwill happen doesn't\nwork so an Executor is a jvm to manage a\njvm jvm has RAM and CPU Ram you can say\nI want 8 GB Ram 10 GB Ram fine but\nwithin that to manage ideally for a\nsingle jvm one processor core will be\nallocated minimum One processor core\nright but again the question if I have\n16 GB Ram I mean 16 GB RAM for a\ncontainer that will have lot of\npartitions inside that maybe one CPU\ncore is enough to run all of them\ndepends on what is the processing power\nof your uh machine but you have to keep\nall these things when you're launching a\ncluster performance side I'm saying so\nto put it short ideally you don't have\nto do these many things when you're\nlaunching a spark program you should\nknow the size of your data and how it\nshould be chunked correct so the idea is\nif the number of partitions are more so\nlet's say I have a file whatever the\nsize May X size is X right if the number\nof partitions are more that means the\nnumber of splits are more ideally so I'm\nsaying that I want 10 partitions so I\nget 10 partitions now if each partition\ncan be inside an Executor 10 executors\ncan process that data so parallelism you\nget but if it is less let's say I want\nonly two partitions then they are in two\nexecutors then you know two executors so\nideally your this is less what you say\nprocessing so the more partitions you\nhave the more processing power you can\nget so uh if you are loading let's say\ntext file right so if I'm loading a text\nfile I can say that create an rdd number\nof partition five so what it'll equally\nsplit that text file into five and then\ngive it to one one executor but the\npoint is you need five executors to\nprocess it so if you go to Yan Yan will\nsay I don't have enough res I'll give\nyou only two executor so two executor\nwill get five partition one guy will get\ntwo another will three so and each\nexecutor is having one CPU core right so\nit'll may be a bit slow so if you have\nenough resources parallelism Works\nactually better so usually what we do in\ntheuh huh I will I will redraw this\npicture so things will become clear to\nyou I think I just draw it in one manner\nonly so my point is I just delete this\nokay give me one moment no number of\npartitions so that's what I'm saying if\nyou have a file this is your file right\nthis is your\nfile and this file is divided into 10\nblocks in Hado okay so I have\nwhat I I won't exactly write it but\nlet's say 1 2 3 4 5 6 up to 10 blocks\nyou have okay now to answer your\nquestion right when I read the data into\nspark it'll automatically do something\ncalled partitioning and each block will\nbecome one\nPartition by default and you cannot\nchange it okay by default it is one and\nmost of the cases we keep it but what\nyou can do so so I get this is one block\nthis is one block so all this is red so\nthis is my partition one partition two\npartition three right this is what\npartition four and I have partition\n10\nright huh similar to input splits so you\nhave 10 partition now you want to\nprocess it that is where the speed and\nall matters right to process it what you\nneed you need RAM you need CPU power\nright so where this partition will go\nthis is hard disk blocks are\nwhere this is hard disk blocks are in\nyour hard disk when I say I create a\npartition partition is in Ram it won't\njust sit in the ram it has to be inside\nan\nExecutor or a jvm right that is where\nthe data should come then the processing\nshould start right so then then you can\ndecide I have 10 partitions okay how\nmany executors I need can I ask for 10\nexecutors yes you can do you can say I\nwant 10 executors okay so you will get\nwhat executor 1 and here you will get\nexecutor 10 and each uh partition will\ngo to an Executor partitions always go\nto execut so this P1 will go to this\nexecutor now this executor I have asked\nfor 1 GB RAM for this executor I also\nhave one processor core that means this\npartition can be uh managed by this\nexecutor same your P10 will go here okay\nthis executor also have one\nprocessor and 1 GB Ram but the point is\nyour yarn should allow you to give you\n10 uh this these things the same thing I\ncan do in another way I ask\nfor only two\nexecutors example okay I ask for two\nexecutors this executor has\n5gb this executor also have 5gb this has\none processor core this has one\nprocessor core and P1 P2 P3 up to P5\nwill come here P6 p7 P10 will come here\nnow if you write a program so let's say\nI wrote a program I say filter the data\nH so my driver is here right this is\nwhat my driver in my driver I said\nfilter the data H where is the data in\nPartition so this filter is your logic\nthis logic will be pushed to this\nexecutor and this filter will apply here\nhere here here here who will apply this\nprocessor and RAM together right so One\nprocessor core I have using that one\nprocessor core can I uh uh parallely run\nthis probably I have multi-threading and\nall I can parall process this much\namount of data good otherwise one by one\nit will process it take take some time\ndepends on yes your processor core you\ncan increase you can even say I want\nfour cores for the container when you're\nlaunching this uh executor I said One\nCore by default right I can also say hey\nYan give me my executor each executor\nrequires four\ncore but then the problem is if this is\na quad core data node only one executor\ncan be launched because you already took\nfour course if you ask one more you will\nnot get\nright if this is a quad core data node\nif you launch one executor you cannot\nlaunch another executor already you\ntaken four cores the same system the\nsame system there is multi-threading so\nI think uh by default it can handle only\ntwo partitions Max multi- threading\nmeans two threads right so in production\nhow we do it is that once we design our\napplication we look at the data from\nwhere the data is coming will it already\ncome partition or you have to partition\nbecause if you're reading from Hado it\nalready is partition like blocks and all\nbut if you're reading from Cassandra\nCassandra doesn't by default partition\nthe data so you get like one big chunk\nso while reading we decide should we\npartition yes we partition how many 10\nso 10 means what will be the size this\nmuch so how many executors you need so\nthen you go to your admin and say that\ncan I launch a spark job with this many\nwill I be getting he will say yes you\nwill be getting then you launch\nthem\nnow no no no no it is a nosql\ndatabase out of this but you can read\nfrom there and you have connectors in\nspark where you can read from Cassandra\nso are you able to follow what I'm\nsaying I mean some of you are like he is\ntalking Greek and Latin right there is\nalso a way where you launch the job\ndefault so if you don't want to mention\nall this stuffff you just say run my job\nso Yan will figure out the best way to\nlaunch probably it may not be the\nfastest way but it'll just execute your\nprogram that is also possible so now\nyou're thinking that this guy is trying\nto fool us because he's saying you can\nask for three I will practically show\nyou three executors running by launching\nyou can actually do this it's not like\nsome Theory stuff when you're launching\na spark shell you can go to Y say num\nexecutor executor memory driver memory\nall you can ask and it'll give you those\nand you can see them okay I got three\nexecutor or four executor or whatever\nyou want\nright now if you have understood this\nmuch I'm going to again confuse you\nbecause if you ask for an Executor right\nyou say I want an\nExecutor and how much memory you want\nexecutor you say I want 10\nGB by default you get only 90% of\nit okay because 10 percentage is\nallocated for system\ncalls it is a container it has to access\nsystem calls and respond so you say 10\nGBS 10 GB is full not you it will not\ngive you full 10gb 10 percentage system\nwill take so what you\nget 9 GB in this you will not get in\nthis you get only 60 percentage so\nultimately some 54 percentage of the ram\nonly you will get because it is a jvm\nright there is garbage collection jvm\nmanagement all that requires memory so\nin reality if you look at a spark\ncluster if you ask for 10 GB container\nyou get around 5.8 GB for rdd remaining\nall system will\ntake please keep it in your mind because\nthis is an interview question so you\nwill go and say 100 GB Ram I get they'll\nsay no it will not boss ah so by default\nif you ask for 10gb 10% system calls\nwill\ntake H container has to communicate with\nyour operating system yes right and yarn\nso for these things it will reserve some\nmemory right so from 10 GB you became\n9gb in this 9gb your jvm has to manage\nit's garbage collection and\ncommunication uh all this it will take\nanother I think uh I don't know the\nexact number 30 percentage or something\nit will take it will drill down to Total\nI think 54 or 56 percentage of this 10gb\nonly your rdd memory you will get to\nactually fit the party\nwhich means in 10 GB you get roughly\naround 6 GB to fit the partition so\ndon't think that you get a jvm 10 size\nyou can fit 10 GB of wor partition no it\ndoesn't work like that so these are\nactually internal to spark but when we\nrun we will understand I mean in\nproduction when we run because then your\ncalculation will be wrong right if you\ndon't understand this your calculation\nwill be wrong okay so this part\npartitions and uh parallelism so now if\nyou look at the the picture does it make\nmore sense this\npicture so you have four blocks you ask\nfor four containers four partitions\nideal case each is loaded and that's\ncalled your rdd right now the real\nquestion how do you write a spark\nprogram right basically that is what you\nwant to do apart from rdd or RAM and all\nonce you get the data you should analyze\nthe data so how do I analyze the data in\nin Python did you uh learn something\ncalled a higher order function function\nyou know right so normally when you\nwrite a python function you will say\nfunction then function name blah blah\nblah let's how Def and all then you\nwrite a function and you will reuse the\nfunction why are you creating a function\nyou can call it anytime there is\nsomething called Anonymous function or\ndisposable function let's say I want to\ncreate a function I will use it only\nonce I don't want it anymore so you\ndon't have to really give a name for the\nfunction or defin you can create it on\nthe fight that is called Anonymous\nfunction\nit's called Anonymous function\nAnonymous okay in in uh spark\nprogramming in spark programming what we\ndo is that we have something called\nhigher order function there is something\ncalled higher\norder function what is higher order\nfunction let's say I have a function\ncalled\nABC I can pass another function to this\nfunction that's called called a higher\norder\nfunction meaning this ABC is a function\nand normally you will pass some\nparameter or something some value right\nyou will say a is this B is this that is\nwhat you s but I can have a function and\nI can pass another function to this\nfunction that's called a higher order\nfunction so we will be passing Anonymous\nfunctions here this Lambda is an\nanonymous anonymous function I will show\nyou the code it will become better but\nin sparkk basically what we do is that\nonce you create an rdd now you have the\ndata ready your data now I want to\nprocess the data how do you process the\ndata you have something called\nTransformations so it is actually there\nin a spark website it's also uh good\nthat you can look at spark official\nwebsite how do you go spark. apache.org\nhey why it is not showing\nyeah so spark. apache.org is the\nofficial website of spark and if you go\nto documentation it will say latest\nrelease is\n2.3.0 right and these are the older\nversions and all if you click on here\nyou can see all the spark versions you\ncan see that\n163 was the last spark one version now\nwe are on 230 this is our version I mean\nlatest version and if you go to\ndocumentation let's say latest release\nimagine latest release okay if you\nscroll down okay here you can see rdd\nprogramming guide see and if you click\non this okay and scroll down a\nbit this we will come back you can see\nresel distributor data set or rdd this\nis what we created right so we just\ncreated an rdd at least theoretically so\nonce you create an rdd I will show you\nhow to create it okay you have rdd\noperations so now my data is available\nas an rdd what can I do with the rdd\nright so that is where you can start\nwriting your functions okay Anonymous\nfunction uh and yeah these are the\nTransformations this this is what you\nneed to understand so these are all\ntransformations you can use in spark map\nfilter flap map there are many actually\nso if I want to filter my data I will\njust call this filter okay if I call\nfilter it will ask me what do you want\nme to filter within this bracket I will\nwrite my expression to filter that is\nhow you filter your data map is another\nit is like four each okay uh map I will\ncall map will ask me like what do you\nwant me to do so you'll write an\nexpression within map what map has to\nperform so these are all higher order\nfunctions map filter flat map these are\nall higher order functions actually so\nyou do something called\ntransformations in one rdd if you apply\nany of these functions it will create a\nnew rdd that's called transformation\nthat is how you analyze your data let's\nsay I want to filter my data I will call\nthe filter transformation and rdds are\nimmutable very important Point once you\ncreate an rdd you cannot change it you\ncan only create another one by applying\nsome logic you can never edit an rdd\nthey are immutable\nright so if I go to my\nPPT yeah so we have created our log\nlines rdd fine till this we have\nunderstood and then what I did probably\nI'm interested only in error messages\nfrom this rdd so you you know you see\nthere a lot of data info warning error I\nwant only error messages to filter so\nwhat I can do I can call the filter\ntransformation\nokay so I can call a filter\ntransformation and then I can say that\nhey Spark Okay match only error lines\nand give it to me and I'll show you how\nto light the logic this will produce\nanother rdd and I can call it as errors\nrdd this is the steps in which you write\na spark program first you create your\nrdd and now I want to do a filter I will\ncall a filter and it will you know\nchange whatever I mean it will filter\nonly error messages and that I will will\nstore it as another\nrdd now somebody was asking me what will\nhappen to the memory right so it'll\ndelete this\nrdd I mean if there is not enough memory\nso let's say this rdd fit into the\nmemory and then you call this filter\naction it will filter whatever is\nrequired and this will be gone this is\nnot required now because you have this\nright because next processing will start\nfrom this step\nright I will I will show you what you\ncan do okay originally let's assume in\nthe normal use case you call function it\ncreates another rdd and this rdd is gone\nnow now this is your current data um so\nif you have enabled in Yar Dynamic\nallocation then this will be gone I told\nyou right this is actually a partition\nand there is a executor running here now\nthis guy will be idle because it doesn't\nknow what to do it cannot predict that\nthere will be no data right so that exe\nso that is one more problem huh now your\nproblem is you have four executors the\nsecond executor has no data to process\nbecause it filtered all the error there\nis no error now this guy will be sitting\nidle so one data node will be there one\nexecutor will be there that has no data\nit'll simply sit idle so that becomes a\nproblem right how can I solve that\nproblem it's actually very easy in spark\nthere is a transformation called\ncoilies coilies is a very common\ntransformation and why do you call coil\nis you pass a number it will reduce the\nnumber of partitions you can resite SI\nit because and you have to calculate\nthis but in this example assume you know\nthat see now if you look at the data\nsecond partition is empty third\npartition has only one line that's also\nnot so so I'm assuming that I want only\ntwo so I can tell spark take this rdd\napply coilies keep all the data but\ndelete two partition and two jvm I want\nonly two so it will just bring it into\ntwo partitions now for example just an\nexample if you are having one terab of\ndata right and you take a good sample of\nthat data uh let's say 10 GB or 100 GB\nand you run it once so then you can\nunderstand you know uh after the filter\nokay you can call a collect action so\nlet's say after the filter I want to see\nthe data I can do it so I know that\noriginally I loaded let's say I don't\nknow 1 GB data or 10 GB data after the\nfilter when I get the data it's only 5gb\nthat means half is reduced so I can\ncalculate okay if I'm loading one\nterabyte of data I don't have to manage\nthis many jvm so after this step I\nshould call a coilies I can do that now\nthe most important Point surprising to\nis that let's say you wrote a spar code\nin which you wrote these three lines\ncreate an rdd filter the rdd you say\ncoilies and you say run nothing will\nhappen this is the surprise actually you\nwrote a spar code where you wrote all\nthese three lines okay read from hdfs\nFilter error message then you set coil\nis and you submit the job to the cluster\nnothing will happen in the cluster\nbecause all of this are lazy there is\nsomething called lazy execution meaning\nspark will not start execution unless\nyou call something called an action\nthese are transformations meaning you're\nchanging the data but you are never\nsaying that show me the output right you\nare saying repartition the data filter\nthe data okay so what where are you\nsaying show me the output you're not\ndoing that so unless you call something\ncalled an action where you say that give\nme the output nothing will work to do\nthat you can call an action called\ncollect collect is a most common action\nin spark when you call collect okay you\nare telling spark that I want to see the\nfinal output that's it so collect will\nlook at this cleaned rdd so you are\nasking give me the output of this\ncleaned rdd and Spark will understand if\nI want cleaned rdd I should have errors\nrdd if I want error hard I should read\nfirst it'll go to your hard disk read\nthe data do all this stuff and then show\nthis is output right show this on your\nscreen and then the entire pipeline is\nemptied spark never keeps your data in\nmemory once the processing is over okay\nyou got the result on your screen right\nthis you so everything is gone no rdd\nnothing is there no container no\npartition nothing will be there\neverything is gone so only for that\nSplit Second all these things happens\nnow I if I want to know whether I should\nrerun or repartition what I should do is\nthat I should go back here okay I will\ndo a save of this error you can also do\na save method on an rdd when you say\nsave it will save as a file a text file\nso I will say save as an action on this\nfilter rdd and then I have to check what\nis the size right my original data is 10\nGB after the filter it is 5gb so to\nprocess 5gb why I'm launching these many\nthings so I can rewrite my code in that\nway so collect will simply display the\noutput on the screen there is also an\naction called save as text file okay I'm\nnot uh just talking I'm showing stuff\ncan you see actions now his question was\nwhat if I don't want to do it so he had\na very good question so he is asking so\nthe problem with this is if I run The\nSpar code in a split second everything\nwill happen and then I see the output on\nmy screen no rdd nothing is there in the\nmemory because uh the this is called dag\ndirector a cyclic graph spark will\ncreate a dag a graph of execution and it\nperform step one step two step three\nstep four how a result and then\neverything is gone again you run again\nit to load that's a different thing uh\nnow so he had a question I will come to\nthat so I will give so collect is an\naction where you return all the elements\nin the driver program meaning you see\nthe output you can also say save as text\nfile there is an action uh that will\nsave see save as text file if if I say\nsave as text file for an rdd whatever\ndata is there in the rdd it will save as\na text file so I can see that in Hadoop\nor wherever you are storing it right now\nis this the only action available no I\nhave save as Cassandra table or or many\nother ways to save the data ideally text\nfile save we do to just to see the data\nokay uh in the initial uh phase of\nreading the data even map reduce or\nspark everything will be same because it\nis blocks you are reading no difference\nit makes once the data is available in\nmemory then the difference comes because\nhere you see we did a filter then what's\nthat coilies and then let's say\nsomething else also we did for doing all\nthis the data is still in the memory you\nnot writing anything there is no\nintermediate result you're are pushing\nto hard disk once you call collect you\ncan write 100 functions and then call\ncollect all those manipulation will\nhappen finally the collect only will\ndisplay the output that is a speed h no\nvery good question only one action can\nbe called it once very good question\nyeah so there are many actions or let's\nsay collect and save as text file\ncollect is one action I cannot say\ncollect and save as text file no so\nfirst it collect will run again if I say\nsave as text file again the whole\noperation has to start so you will be\nwondering is there a way I can improve\nit I will show you okay so probably that\nis his question so um here I will show\nyou this so once you\nexecute execute the dash dag is nothing\nbut directed a cyclic graph it is a\nfancy way of saying spark creates all\nthe steps to be executed in a graph\nformat and I will practically show you\nthe dag when we run spark job you can\nsee that you can see partitions you can\nsee the dag you can see how many\nexecutors it is launching everything is\nvisible it's not just Theory Theory I'm\nseeing but unless I speak about it and\njust show then you will say how this\ncame you know how the partitions\nactually came you'll be confused right\nand the driver collects the data so this\nis the full step if you want to look at\nit once more uh no if you wanted a\nfilter you to write it in mapper reduce\nis only aggregation so either you can\nsay sum or multiply or or it's an\naggregation function but because you get\none key and all the values in the\nreducer so what do you want to do with\nall these values you can't filter them\nso you can aggregate them or do whatever\nso if I have to write a filter I write\nit in the mapper side here I can so here\nthe final St stage is the collect so\nbefore the collect I can do whatever I\nwant I can filter and here the advantage\nis it's not just key value pair you can\nalso work with key value pairs in spark\nso I can create a key value pair and\nsimilar to map produce I can operate I\ncan reduce and I can do all these things\nuh yeah he was asking a question I think\nI will answer that because if you look\nat the picture I have a very good\npicture to explain this just see this\npicture same stuff I'm doing there is\ndog lines rdd errors rdd cleaned rdd and\nfinally I'm calling an action called\ncount count is another action so if you\nsay count everything will work from the\nbeginning because it has to count right\nso count action called and what happened\nit show me the result as five because\nthere are five lines in the rdd no\nsurprises now I also want to call one\nmore action on the rdd called save to\nCassandra the problem is if I call this\naction pipeline is empty spark has to\nstart from Reading From the Block\ncreating the original rdd then filtering\nthen errors all then I can save to\nCassandra okay now one more thing then\nafter I'm doing one more filter I want\nonly messages one from this data see\nmessage one message one message one I\njust get only message one and then I say\ncollect so I just want to see this but\nagain what will happen it will start\nfrom the top so if you look at this\nthree actions you can see all three\nactions depend on this rdd everything is\nstarting from here right that is where\nyou can cash an rdd it is possible to\nCache an rdd so when I say caching\nwhatever data you have in that rdd along\nwith rdd will be cached and then where\ncan you cache you can cach in Ram you\ncan cach in hard disk and you can also\nsay Ram plus hard disk like wherever you\nwant so there are multiple options to\ncach it\nand once you cash an rdd let's say you\nsay count it will count then you say\nsave to Cassandra it will start from\nhere because this rdd data is already\ncached and don't think if you're caching\nyou're caching for one month once your\nspark program is over caching is deleted\nso this is caching is valid only till\nyour program completes all the action\nonce all the actions are over you exit\neverything is gone then you are not so I\nwill show you tomorrow when you start a\nspark spark application uh if I a\ndeveloper I will create something called\nspark context object there's an object I\ncreate and that represents my program\nand ideally when my program finishes\nI'll kill that object if I kill that\neverything is gone even caching will be\ngone because that caching is valid only\nto speed up my existing program I'm not\ncaching it for a program I'll write next\nmonth that is not possible during that\nexecution time I want to speed it up see\nI I create this clean AIO whatever and I\nsay count it will count and delete all\nthis thing from memory right so it is\nnot in memory during the execution is in\nmemory after the execution is over\nnothing is in memory ah then you should\nkeep the intermediate data in memory\nright you should tell spark I will call\nan action show me the output but this\nrdd whatever data you have don't delete\nfrom your pipeline keep it because I I I\nwant to reuse it for some other action\nthen it keep that's called caching\nH so there is a command called UNP\npersist UNP persist is an action it's\nnot a transformation if you say UNP\npersist it will delete it from cach I'll\nshow you how to cach tomorrow it's very\neasy and otherwise if you kill your Spar\ncontext object like your session is gone\nit'll automatically deleted so basically\nso you may be thinking so how do I start\nprogramming with spark right I want to\nwrite a spark program so first thing you\nshould learn how to create an rdd if you\nknow how to create an rdd then you can\nstart with uh these simple simple\nTransformations map filter now there is\nalso a flip side of all of this uh the\nflip side is initially when Spar came\ninto the industry everybody was mad\nabout rdds okay transformation so I'll\nwrite a map filter people were literally\ndying on writing these things okay but\nsoon there was a problem with this see\nthe problem is if you write your code\nusing these Transformations and actions\nthere is is no way sparkk can optimize\nyour uh code why am I saying this is\nthat you say filter okay in the filter\nyou are saying that filter only error\nmessages right unless spark runs it it\ndoesn't know what you're talking about\nand this these things do not have a very\nstrict\nschema rdds are not having very strict\nschema so when you want to process\nstructure data let's say like CSV file\nokay I want to read a CSV file and I\nwant control I want schema everything\nrdd is not a good way to process it you\ncan process it but spark internally will\nnot be able to optimize your code that\nis where Spark SQL comes into picture or\ndata frames we call it so spark has a\nmodule called spark SQL and Spark SQL is\nmuch more powerful and optimized than\ncore\nspark so when you write spark SQL you\nshould know how to create a table and\nquery using SQL it's end off a SQL only\nso it's is easy for to program and more\nthan that it is much more optimized than\ncore rdds because you are using all\nLambda and Spark has no way to\nunderstand what Lambda you are using\nunless it runs it right I wrote a Lambda\ncode some weird code spark has no way to\nunderstand what is the meaning of this\nLambda unless it see the data but if it\nis a table I can say filter on this\ncolumn spark knows what is that column\nwhat is that data type where it is right\nit can avoid that column while loading\nthe data to optimize I'm saying right so\nif I write a SQL query it has a filter\nGroup by then join and something spark\nand read it apply the schema and\nunderstand what you talking about I will\nI will talk about in the data frame\nclass I will talk about it so any\nquestions on Cor spark like so far we\nhave discussed if anything is not clear\njust ask me okay so don't think okay so\nthese are some of the common\nTransformations map flat map filter\nthese are the name of the transformation\nand these are some of the actions that\nwe have right yeah so um anyway we will\nget started with the core hands on\ntomorrow but uh we can look at some of\nthe things so when you have a cluster so\nI'm just connected to the cluster right\nnow right spark gives you a shell to\nwork like the hive shell if you remember\nso you started high and started typing\nso very similar to that spark gives you\nsomething called a spark shell\nH and the spark shell is available in\nPython Scala and R there is no Java\nshell Java as such does not have a shell\nfunctionality you have to write using ID\nor something right now if you want your\nuh spark shell in Scala we are going to\ndo python but I want the Scala shell I\ncan simply say spark\nshell if I simply type spark hyph shell\nuh oh I\nthink why spark shell is not\nworking I think I have to export a path\nto get started spark shell okay so let\nme try this let's try the python shell\nppar 2 this should\nwork H\n99 so this is Java 8 as of now Java 8 is\nsupported okay I think in the cluster\nthey have disabled Scala shell access\nmaybe I have to export some\nconfiguration to do that that's why I'm\nnot able to open the scalar shell but if\nyou want the python shell you type Pi\nspark two that's the command why P spark\ntwo because we are in spark two this\ncluster has spark one and Spark two\ninstalled you can try this we will just\nuh you know some do some Basics not like\nadvanced stuff but\nI don't know why it's taking this much\ntime usually the shell starts very\nfast the shell is not starting let me\njust\ncheck yeah so started so very simple so\nthis is your python shell okay and what\nit says when starting the shell it says\ntype help and blah blah blah uh logging\nlevel for spark R and all it says Spark\nversion is\n2.2.0 so on the cloudex lab we running\n2.2.0 using python version 275 that's\nokay okay spark session available as\nspark I will talk about this later later\nwhat is this spark session okay but it\njust says spark session available as\nspark okay and this is your spark shell\nso now from here you can start creating\nrdds and then writing Transformations\nand everything that you want okay\nnow I will just see if I can show you\nthis\nso if you type this command P spark\nsimply type P spark okay dash dash\nhelp sorry sorry P spark 2 right py\nspark 2 hyphen hyphen\nhelp yeah so I want to show you these\noptions okay just have a look at\nthis so I'm just saying that I want to\nstart a spark shell okay and I need some\nhelp in starting that right so what it\nsays what are the options you can have\nso one is Master URL so this we don't\nhave to worry as of now okay if I scroll\ndown I can show you can you see this\ndriver\nmemory so when you start spark shell\nokay a driver will be created okay and\nthen you can ask for how many executors\nyou want right how much memory you want\nfor the\ndriver right one question and then here\nyou see what is this executor memory I\ntold you right you can ask for how many\nexecutors you want and how much memory\nso by default I think it is 1 GB it is\nwritten here default is 1 GB okay but\nyou can ask for executor memory right\nand if I scroll down\nuh see here executor core how many\nprocessor cores you want for an Executor\nright and see here driver core how many\nprocessor core you want for the driver\nnumber of executor default two so if you\nstarting the spark shell like this\ninteractive shell you will get two\nexecutors one driver each executor has 1\nGB Ram default setting but using these\narguments I can say uh how many\nexecutors I want and what is their\nmemory and so on and so\nforth now another important Point let me\ntry this okay just give me one moment I\nwill just do a p spark\ntwo\nokay\nand let me let me open my\nmail because uh the spark also has a UI\nand that UI\nwas sent to me in my mail because for\nevery cluster it is\ndifferent sign\nin\nuh just check this\nthis is a yarn mode I think okay let me\njust check\nokay so one of the URL I think it is not\nworking probably it will work now I\ndon't\nknow yeah so can you guys see this uh GL\nfaculty great learning this is me okay\nand it says 55 seconds before I started\nas uh shell this is actually showing you\nthe history server that's what I'm\nwondering okay 55 seconds before it\nstarted let me just go to the\napplication\nso you have to actually see this URL but\nit is not\nshowing okay maybe I\nthink it's giving the old UI\nlet me exit from here\nokay I\nrefresh so what you're looking at here\nis this history server URL and this is\nnot really useful to us because once you\nstart a spark shell and then you exit it\nit will display here you can see here P\nspark shell it was launched by me 1.8\nminute I exit from here and then you can\nsee here but this is not really useful\nto us because uh this will come only in\nthe history server that means once you\nexit from spark okay so there is a real\nUI that port number is not working\ncurrently when you are running the spark\nit will show there but I can show you\nsomething if so this session we I\nstopped okay I didn't do anything I\nstopped this session and if I go to this\napplication this is the spark UI\nso here you can see the jobs and there\nare no jobs and if I expand this can you\nsee executor one added and there is\nExecutor two added because default you\nare having two executors uh and here you\ncan see\nexecutors and there was one driver and\ntwo executors so there was one driver\nactive and two executors we got as per\nthis right um and one more important\npoint is that\nwhen you simply say p spark\n2 if you simply say Pi spark 2 spark\nwill start in the local\nmode that means you are not talking to\nyarn or anything if you want to talk to\nyarn you have to say hyphen hyphen\nMaster yarn this is the option you have\nto use so whenever you are starting the\nspark shell otherwise if you start in\nthe local mode what will happen driver\nexecutor everything will be in a single\njvm and also the resources will be\nallocated by your operating system not\nby Yan in the local mode Yan has no role\nin your local mode and that is good for\nlearning if you simply want to learn say\nspy spark 2 and then you can just start\nwriting your code whatever ideally when\nyou're are launching the spark shell you\nhave to say py spark 2 and then you have\nto say hyphen hyphen Master Yan meaning\nuh I want to register with Yan and then\nlaunch Spar so what this request will go\nto Yan and Yan will allocate containers\nto you now let's try to do one thing\nlet's try to launch it with our own\nparameters right so what were the what\nwere the options we have P\nspark to das Dash\nhelp so let's look at what are the\noptions let's say we want more executor\nI want three executor probably right so\nwhat is the option num executor hyph\nhyund num executors I will say three and\nhyphen hyphen\nexecute course course let it be there I\nwant memory more so where is memory\nhyphen hyphen executor memory right\ndefault is 1 GB I want probably 2 GB\nsomething like that right so what I can\ndo I can simply go to my\nshell P spark 2 I can say master is Yar\nand I can say number\nof executors probably four I don't know\nand then what is that\nexecutor\nmemory why this is there okay executor\nmemory I need 2G 2GB\nhitender\nuh by spark does not support any\napplication option why is saying\nthat probably we are asking for\nmore let's\nsee let's see whether we can get three\nexecutors okay so first we will try that\nthen we will see whether we can get more\nmemory or something so right now I'm\nsaying that hey launch a spark shell the\nmaster is\ny\nsorry H not meem right\nmemory right yeah wrong command correct\nso now I asked for three executors and\nit started and we can't see it here\nunless we exit so let me exit from here\nso it will go to the history server\nokay I'll refresh\nhere this is what we started\nright can you see executor one added two\nadded three added okay can you see so\nthree executors were given to me and you\ncan actually go here\nexecutors can you see this is one driver\nthese are executors right you get\nexecutors and here you\nsee the memory right so how much memory\ndid you\nask\nhuh I think one default we selected\nright one GB we selected actually so so\nfor rdd storage right now you are\ngetting what\n384\n.1 uh megabyte so how this rule actually\nworks like I told around roughly 50% of\nthe memory will be taken and remaining\nit will give it to you you will actually\nget around 600 MB in this but it'll\ndisplay only this much because you are\nnot loading any data as of now it is\nsimply running the executor is simply\nrunning you don't have any anything\ninside this right\nright for the\nrdd that's what you don't have any data\nas such right when we load something we\ncan actually see the memory it will\noccupy how much uh amount of rdd you\nhave it will\noccupi huh so 384 right now it has\nallocated but there is no data inside\nthat zero B is saying that but when you\nactually load let's say 500 data it will\nshow 500 MB actually occupied in this\nbecause up to maximum 600 MB I think you\nwill get because 40% anyway will be gone\nfor your system calls and\njvm right so up to 600 500 and something\nuh 50 or something you will see here\nokay so right now you can see only less\nmemory because we are not actually doing\nanything with here so that is why it is\nsimply displaying uh uh you know this\nmuch and uh I exited from here so what\nif we try to change\nthis number of executors four hyphen\nhyphen executor memory\nright memory 2G there is a threshold\nvalue I I think uh if it is more than 1\nGB let's try this there is a threshold\nvalue what is this ah it says an error\noccurred while calling Javas Spar\ncontext required executor memory is\nabove maximum threshold for this cluster\nso the cluster uh uh administrator has\nsaid that you can ask only maximum this\nmuch so what they\nsay\n248 + 384 MB is above so uh it is saying\nthat Max to Max you can ask only 248 2\nGB of memory but here what will happen\nwhen you launch an Executor is that\ninitially it will allocate a 384 MB okay\nthen it'll keep a 2GB as you know what\nyou say excess to be allocated so it is\nsaying that the total memories above the\ncluster administrator what he has said\nyou cannot launch so we can try 1.5g or\nsomething like that I think uh 1.5 gab\nshould work ideally let me check if that\nwill work okay I'll\nexit\nexit and we'll try this tomorrow don't\nworry okay I just wanted to check\nwhether the cluster is running fine we\nwill try this together tomorrow don't\nworry okay so how do you mention um so\nsee here size must be specified as bytes\nkilobytes megabytes gigabytes terabytes\nright so I think you cannot say\n1.5 G that will not work right so how do\nyou mention uh megabytes M right\ndid I say g\nh so what did I\ntype I typed G only right then why it\ndidn't\nwork M should\nwork M should work ideally right 1,500\nM it should be whole number I think you\ncannot say 1 Point 5G like you can say\n1G or 2G okay so now it launched and I\nwill exit so but uh now what I'm showing\nyou here is not correct because I'm\nshowing you from the history\nserver like I'm starting spark shell\nthen exit from there then I'm showing\nyou that this is the UI that is not\ncorrect you you should see live right if\nyou're if you're running the job you\nshould see live that UI is not working I\nwas asking for that and I don't know why\nso the jobs will display anything which\nis running why you are not seeing\nanything here because we don't have any\nrdd or any action or if you have\nsomething you will see here okay and\nstages I will talk the storage will show\nyou if you have cached an rdd if you\ncach an rdd it will come here in the\nstorage section environment is where it\nwill show the Java libraries and all\nexecutors will show how many executors\nand driver that you have added jobs jobs\nis where if you run a job it will come\nlike you said this thing do collect it\nwill display here like when you run an\nactual job it will display here the dag\nand\nall stages it will show you in the job\nhow many stages are there and how those\nstages are created we will see that\ntomorrow anyway okay then you have\nstorage and then you have environment\nand executors right so environment will\nshow you the different libraries and all\nwhich are part of spark so if you go\nhere now you can't actually mess with me\nwhat if I want to see this from the\nshell right so I'm just\nsaying uh P spark I'm starting it again\nokay and I don't have a GUI access now\nbut there is a command what\nhappened what it is saying hm spelling\nmistake ah M\nsorry so there was a command SC do\nunderscore\ncon there is a weird command which can\nshow you all this from the shell so now\nyou are seeing this in the GUI what if I\ndon't have a GUI so I can do an sc\ndoore do get\nall perfect\nso I remember the command I'm happy okay\nso there is a weird command nobody\nremembers this command actually SC doore\nokay uh but the output is weird but you\ncan see something from the output what\nis that you can\nsee can you see number of executor\nsomewhere it will be here is driver\nmemory right this is driver memory and\nyou can say master is\nyarn somewhere number ah see here\nexecutor memory how much it has\nallocated 1,500 you can also see number\nof\nexecuted where do you\nsee it's very difficult to read from\nthis I can copy paste that's\nbetter is not even allowing me to\ncopy H see here when I'm copying I'm\ngetting\nso this command is like a million dollar\ncommand I'm quite sure nobody will know\nthis command even I came to know very\nrecently how can you see The Spar\nconfigs from the CLI SC doore conon get\nall it's a shortcut actually I'll show\nyou the command this is the command\nplease make a note of this because\nsometimes we will start working in a\ncluster where they will not give you GUI\nthen how do you know how many executors\nyou launched or what happened you know\nyou have to look at this way this way so\nthis will give you an idea about the\nnumber of executors everything launched\nlive live you're seeing like now you\nlaunched now these many are there\nscore con. getet all now the good news\nis we will try something in the Shell\nthe bad news is you will actually try\nmost of the things like this Jupiter\nnotebook so how do you create an rdd\nfrom p park import SC blah blah blah uh\nme just all output clear\nokay so kdd cup data rad data\nparaly this brings us to the end of this\nbig data analytics tutorial now before\nyou guys sign off I'd like to inform\nthat we have launched a completely free\nplatform called as great Learning\nAcademy where you have access to free\ncourses such as AI cloud and digital\nmarketing so guys thank you very much\nfor attending the session and have a\ngreat learning\nahead e\n",
  "words": [
    "big",
    "data",
    "analytics",
    "helps",
    "organizations",
    "harness",
    "data",
    "use",
    "identify",
    "new",
    "opportunities",
    "turn",
    "leads",
    "smarter",
    "business",
    "moves",
    "efficient",
    "operations",
    "higher",
    "profits",
    "happier",
    "customers",
    "understanding",
    "come",
    "tutorial",
    "big",
    "data",
    "analytics",
    "go",
    "ahead",
    "session",
    "like",
    "inform",
    "guys",
    "launched",
    "completely",
    "free",
    "platform",
    "called",
    "great",
    "learning",
    "academy",
    "access",
    "free",
    "courses",
    "ai",
    "cloud",
    "digital",
    "marketing",
    "check",
    "details",
    "description",
    "let",
    "quick",
    "glance",
    "agenda",
    "start",
    "understanding",
    "concepts",
    "big",
    "data",
    "hadoop",
    "learn",
    "hyve",
    "finally",
    "learn",
    "spark",
    "today",
    "covering",
    "um",
    "big",
    "data",
    "exactly",
    "big",
    "data",
    "look",
    "something",
    "called",
    "hadoop",
    "ecosystem",
    "hadoop",
    "architecture",
    "basically",
    "things",
    "foundation",
    "uh",
    "machine",
    "learning",
    "statistics",
    "spark",
    "anything",
    "learning",
    "foundation",
    "today",
    "understanding",
    "big",
    "data",
    "hadoop",
    "ecosystem",
    "ensure",
    "go",
    "basics",
    "name",
    "ragu",
    "raman",
    "call",
    "ragu",
    "uh",
    "industry",
    "12",
    "years",
    "working",
    "big",
    "data",
    "platforms",
    "7",
    "years",
    "7",
    "8",
    "years",
    "uh",
    "2011",
    "onwards",
    "started",
    "big",
    "data",
    "trained",
    "people",
    "hadoop",
    "spark",
    "know",
    "big",
    "data",
    "uh",
    "typical",
    "tools",
    "use",
    "uh",
    "train",
    "lot",
    "people",
    "corporate",
    "backgrounds",
    "regular",
    "trainer",
    "flipkart",
    "ge",
    "emc",
    "many",
    "companies",
    "definition",
    "big",
    "data",
    "google",
    "get",
    "first",
    "thing",
    "go",
    "google",
    "say",
    "big",
    "data",
    "get",
    "tons",
    "articles",
    "pdfs",
    "presentations",
    "going",
    "teach",
    "right",
    "going",
    "teach",
    "let",
    "take",
    "practical",
    "use",
    "case",
    "right",
    "let",
    "take",
    "practical",
    "use",
    "case",
    "try",
    "understand",
    "big",
    "data",
    "actually",
    "makes",
    "sense",
    "right",
    "first",
    "big",
    "data",
    "related",
    "need",
    "level",
    "knowledge",
    "fair",
    "right",
    "something",
    "uh",
    "share",
    "personal",
    "experience",
    "okay",
    "started",
    "working",
    "20072",
    "2008",
    "okay",
    "working",
    "uh",
    "company",
    "bangalore",
    "created",
    "application",
    "sales",
    "application",
    "basically",
    "install",
    "application",
    "retail",
    "company",
    "uh",
    "customers",
    "input",
    "sales",
    "data",
    "know",
    "say",
    "today",
    "much",
    "quantity",
    "item",
    "sold",
    "know",
    "sold",
    "sold",
    "sold",
    "application",
    "capture",
    "data",
    "okay",
    "store",
    "simple",
    "world",
    "mobile",
    "phones",
    "facebook",
    "whatsapp",
    "nothing",
    "2005",
    "right",
    "using",
    "rdbms",
    "system",
    "store",
    "know",
    "rdbms",
    "okay",
    "least",
    "good",
    "right",
    "culprit",
    "going",
    "use",
    "kind",
    "big",
    "data",
    "okay",
    "rdbms",
    "using",
    "traditionally",
    "store",
    "data",
    "right",
    "rdbms",
    "system",
    "row",
    "column",
    "format",
    "table",
    "insert",
    "data",
    "somebody",
    "teaching",
    "rdbms",
    "say",
    "well",
    "rdbms",
    "great",
    "store",
    "everything",
    "know",
    "fine",
    "unless",
    "start",
    "data",
    "starts",
    "increasing",
    "right",
    "originally",
    "rdbms",
    "store",
    "everything",
    "rdbms",
    "something",
    "even",
    "transactional",
    "management",
    "use",
    "rdbms",
    "doubt",
    "mysql",
    "oracle",
    "using",
    "mysql",
    "actually",
    "whatever",
    "customer",
    "inserting",
    "mysql",
    "capture",
    "end",
    "year",
    "processing",
    "understand",
    "many",
    "items",
    "sold",
    "everybody",
    "happy",
    "happy",
    "happening",
    "20",
    "05",
    "fast",
    "forward",
    "2011",
    "working",
    "ic",
    "bank",
    "ic",
    "bank",
    "want",
    "migrate",
    "big",
    "data",
    "platform",
    "2011",
    "run",
    "project",
    "migrated",
    "uh",
    "entire",
    "icc",
    "bank",
    "big",
    "data",
    "platform",
    "okay",
    "part",
    "project",
    "everything",
    "project",
    "problem",
    "icci",
    "bank",
    "right",
    "question",
    "ica",
    "bank",
    "want",
    "go",
    "big",
    "data",
    "right",
    "example",
    "also",
    "help",
    "understand",
    "world",
    "right",
    "people",
    "coming",
    "background",
    "might",
    "need",
    "understand",
    "things",
    "working",
    "right",
    "went",
    "ic",
    "bank",
    "spoke",
    "said",
    "hey",
    "want",
    "said",
    "know",
    "facing",
    "lot",
    "problems",
    "want",
    "go",
    "big",
    "data",
    "want",
    "go",
    "hadoop",
    "see",
    "hadoop",
    "later",
    "asked",
    "big",
    "deal",
    "problem",
    "lot",
    "problems",
    "facing",
    "one",
    "problem",
    "look",
    "rdbms",
    "system",
    "right",
    "typical",
    "rdbms",
    "system",
    "rdbms",
    "systems",
    "store",
    "data",
    "form",
    "table",
    "right",
    "table",
    "size",
    "small",
    "big",
    "deal",
    "even",
    "look",
    "sql",
    "go",
    "enterprise",
    "scale",
    "oracle",
    "know",
    "store",
    "gigabytes",
    "sometimes",
    "terabytes",
    "data",
    "size",
    "becomes",
    "big",
    "let",
    "say",
    "ask",
    "okay",
    "want",
    "store",
    "data",
    "oracle",
    "data",
    "keep",
    "increasing",
    "every",
    "week",
    "ready",
    "migrate",
    "anywhere",
    "oracle",
    "talk",
    "oracle",
    "storage",
    "separate",
    "dbms",
    "engine",
    "separate",
    "know",
    "many",
    "worked",
    "extensively",
    "oracle",
    "oracle",
    "purchase",
    "oracle",
    "add",
    "storage",
    "boxes",
    "many",
    "want",
    "know",
    "boxes",
    "network",
    "database",
    "engine",
    "connecting",
    "storing",
    "data",
    "whatever",
    "data",
    "giving",
    "ask",
    "argue",
    "saying",
    "increase",
    "storage",
    "capacity",
    "oracle",
    "store",
    "much",
    "data",
    "want",
    "doubt",
    "right",
    "also",
    "limitation",
    "storage",
    "one",
    "problem",
    "second",
    "problem",
    "box",
    "increases",
    "storage",
    "oracle",
    "increases",
    "divide",
    "data",
    "partition",
    "data",
    "let",
    "say",
    "example",
    "know",
    "uh",
    "working",
    "oracle",
    "okay",
    "let",
    "say",
    "working",
    "oracle",
    "let",
    "say",
    "looking",
    "u",
    "iphone",
    "sales",
    "iphone",
    "right",
    "selling",
    "iphone",
    "let",
    "say",
    "working",
    "apple",
    "right",
    "one",
    "example",
    "creating",
    "tables",
    "store",
    "iphone",
    "data",
    "typically",
    "look",
    "schema",
    "one",
    "table",
    "okay",
    "one",
    "table",
    "wherein",
    "properties",
    "like",
    "uh",
    "know",
    "many",
    "items",
    "purchased",
    "rate",
    "purchased",
    "many",
    "people",
    "bought",
    "forth",
    "items",
    "table",
    "items",
    "table",
    "data",
    "related",
    "iphones",
    "selling",
    "right",
    "also",
    "another",
    "table",
    "user",
    "data",
    "also",
    "user",
    "data",
    "user",
    "data",
    "means",
    "customer",
    "bought",
    "bought",
    "data",
    "called",
    "normalization",
    "know",
    "normalization",
    "rdbm",
    "systems",
    "order",
    "avoid",
    "duplication",
    "data",
    "normalize",
    "data",
    "always",
    "keep",
    "data",
    "different",
    "different",
    "tables",
    "whenever",
    "want",
    "get",
    "full",
    "data",
    "joints",
    "right",
    "want",
    "get",
    "data",
    "something",
    "called",
    "join",
    "query",
    "fire",
    "join",
    "query",
    "data",
    "come",
    "join",
    "produce",
    "output",
    "good",
    "small",
    "amount",
    "data",
    "terabytes",
    "data",
    "join",
    "query",
    "fail",
    "right",
    "join",
    "query",
    "wait",
    "another",
    "point",
    "people",
    "say",
    "rdbms",
    "becoming",
    "bigger",
    "bigger",
    "partition",
    "data",
    "possible",
    "like",
    "four",
    "boxes",
    "one",
    "table",
    "okay",
    "table",
    "right",
    "logically",
    "partitioned",
    "kept",
    "four",
    "machines",
    "becoming",
    "bigger",
    "bigger",
    "okay",
    "possible",
    "something",
    "called",
    "logical",
    "partitioning",
    "look",
    "iphone",
    "data",
    "say",
    "data",
    "people",
    "india",
    "buying",
    "iphone",
    "go",
    "machine",
    "called",
    "logical",
    "partitioning",
    "based",
    "country",
    "column",
    "india",
    "data",
    "people",
    "buying",
    "iphone",
    "us",
    "data",
    "people",
    "buying",
    "iphone",
    "problem",
    "burma",
    "data",
    "also",
    "india",
    "data",
    "let",
    "say",
    "2",
    "terabyte",
    "china",
    "data",
    "3",
    "terabyte",
    "kbs",
    "actually",
    "problem",
    "table",
    "becoming",
    "big",
    "okay",
    "cluster",
    "take",
    "four",
    "machines",
    "bms",
    "data",
    "warehousing",
    "architecture",
    "say",
    "partition",
    "data",
    "table",
    "big",
    "fire",
    "query",
    "query",
    "take",
    "lot",
    "time",
    "say",
    "okay",
    "store",
    "full",
    "table",
    "divide",
    "table",
    "partition",
    "table",
    "partition",
    "say",
    "use",
    "logic",
    "partitioning",
    "physically",
    "divide",
    "data",
    "possible",
    "rdbms",
    "say",
    "take",
    "table",
    "cut",
    "four",
    "work",
    "logically",
    "partition",
    "say",
    "country",
    "column",
    "many",
    "countries",
    "world",
    "maximum",
    "155",
    "right",
    "much",
    "geography",
    "person",
    "think",
    "somewhere",
    "around",
    "155",
    "right",
    "least",
    "people",
    "buy",
    "iphone",
    "155",
    "countries",
    "say",
    "take",
    "country",
    "column",
    "everybody",
    "buying",
    "iphone",
    "country",
    "column",
    "based",
    "country",
    "column",
    "partition",
    "data",
    "right",
    "four",
    "boxes",
    "box",
    "india",
    "data",
    "us",
    "data",
    "burma",
    "data",
    "something",
    "data",
    "story",
    "running",
    "join",
    "query",
    "box",
    "process",
    "india",
    "data",
    "join",
    "query",
    "box",
    "complete",
    "fast",
    "guy",
    "wait",
    "problem",
    "traditionally",
    "people",
    "using",
    "rdbm",
    "systems",
    "data",
    "warehouses",
    "store",
    "data",
    "drawback",
    "one",
    "drawback",
    "size",
    "data",
    "increases",
    "processing",
    "become",
    "slow",
    "right",
    "ca",
    "uh",
    "problem",
    "classes",
    "big",
    "data",
    "lot",
    "technologies",
    "start",
    "talking",
    "denormalize",
    "data",
    "denormalize",
    "data",
    "example",
    "transaction",
    "data",
    "user",
    "data",
    "everything",
    "keep",
    "one",
    "table",
    "possible",
    "h",
    "one",
    "duplicates",
    "second",
    "maximum",
    "number",
    "columns",
    "table",
    "rdbms",
    "table",
    "th000",
    "oracle",
    "says",
    "th000",
    "data",
    "thousand",
    "columns",
    "keep",
    "data",
    "ca",
    "reason",
    "provide",
    "denormalized",
    "solutions",
    "look",
    "rdbms",
    "systems",
    "always",
    "say",
    "normalize",
    "data",
    "represented",
    "different",
    "different",
    "tables",
    "whenever",
    "need",
    "answer",
    "join",
    "get",
    "result",
    "second",
    "drawback",
    "structured",
    "data",
    "meaning",
    "anything",
    "represent",
    "row",
    "column",
    "dump",
    "like",
    "excel",
    "sheet",
    "data",
    "right",
    "dump",
    "images",
    "audio",
    "files",
    "table",
    "yes",
    "store",
    "unstructured",
    "data",
    "rbms",
    "yeah",
    "yes",
    "clob",
    "blob",
    "objects",
    "right",
    "process",
    "unstructured",
    "data",
    "possible",
    "modern",
    "day",
    "look",
    "modern",
    "day",
    "problem",
    "look",
    "typical",
    "uh",
    "uh",
    "bank",
    "like",
    "let",
    "first",
    "complete",
    "part",
    "one",
    "drawbacks",
    "rdbms",
    "size",
    "data",
    "increases",
    "cope",
    "processing",
    "one",
    "problem",
    "right",
    "second",
    "problem",
    "schema",
    "meaning",
    "anything",
    "represent",
    "row",
    "column",
    "format",
    "dump",
    "rdbms",
    "anything",
    "apart",
    "figure",
    "solution",
    "actual",
    "solution",
    "store",
    "images",
    "retrieve",
    "real",
    "time",
    "flipkart",
    "stores",
    "data",
    "working",
    "flipkart",
    "time",
    "flip",
    "cart",
    "around",
    "10",
    "million",
    "products",
    "product",
    "10",
    "images",
    "1",
    "billion",
    "images",
    "click",
    "iphone",
    "x",
    "picture",
    "flip",
    "cart",
    "flip",
    "cart",
    "say",
    "come",
    "10",
    "minutes",
    "immediately",
    "display",
    "ever",
    "thought",
    "happening",
    "good",
    "question",
    "right",
    "happening",
    "right",
    "think",
    "dumping",
    "rdbms",
    "picking",
    "1",
    "billion",
    "images",
    "sql",
    "databases",
    "whole",
    "arena",
    "databases",
    "called",
    "sql",
    "fortunately",
    "unfortunately",
    "course",
    "teach",
    "right",
    "going",
    "uh",
    "end",
    "goal",
    "become",
    "data",
    "scientist",
    "think",
    "learning",
    "course",
    "right",
    "learning",
    "spark",
    "things",
    "right",
    "would",
    "suggest",
    "google",
    "learn",
    "little",
    "bit",
    "something",
    "called",
    "sql",
    "databases",
    "heard",
    "odb",
    "cassandra",
    "hspace",
    "neo",
    "4j",
    "vmot",
    "popular",
    "vendors",
    "nosql",
    "database",
    "nosql",
    "databases",
    "help",
    "store",
    "retrieve",
    "unstructured",
    "data",
    "speed",
    "want",
    "amazon",
    "actually",
    "amazon",
    "something",
    "called",
    "dynamo",
    "db",
    "dynamo",
    "db",
    "nosql",
    "database",
    "dynamo",
    "db",
    "row",
    "column",
    "key",
    "value",
    "pairs",
    "key",
    "product",
    "name",
    "say",
    "clicking",
    "iphone",
    "x",
    "iphone",
    "x",
    "key",
    "value",
    "data",
    "moment",
    "click",
    "boom",
    "page",
    "sql",
    "databases",
    "really",
    "faster",
    "also",
    "helps",
    "store",
    "unstructured",
    "data",
    "booking",
    "cab",
    "using",
    "ola",
    "right",
    "go",
    "get",
    "cab",
    "yesterday",
    "way",
    "chennai",
    "try",
    "get",
    "cab",
    "using",
    "ola",
    "right",
    "open",
    "ola",
    "many",
    "customers",
    "ola",
    "millions",
    "india",
    "booking",
    "time",
    "right",
    "think",
    "requests",
    "taken",
    "rdbms",
    "search",
    "table",
    "give",
    "cab",
    "work",
    "rdbms",
    "system",
    "handle",
    "30",
    "million",
    "40",
    "million",
    "concurrent",
    "sessions",
    "amount",
    "data",
    "calculating",
    "route",
    "mongod",
    "db",
    "sql",
    "database",
    "ola",
    "runs",
    "mongodb",
    "working",
    "ola",
    "one",
    "design",
    "problems",
    "actually",
    "design",
    "anything",
    "told",
    "using",
    "mongodb",
    "qu",
    "problem",
    "real",
    "new",
    "world",
    "see",
    "today",
    "fit",
    "old",
    "description",
    "went",
    "first",
    "job",
    "interview",
    "2006",
    "technical",
    "guy",
    "interviewed",
    "asked",
    "strongest",
    "skill",
    "said",
    "java",
    "said",
    "selected",
    "java",
    "like",
    "know",
    "java",
    "get",
    "job",
    "without",
    "interview",
    "say",
    "java",
    "allow",
    "sit",
    "front",
    "actually",
    "nobody",
    "mean",
    "java",
    "still",
    "good",
    "new",
    "world",
    "demands",
    "new",
    "skills",
    "actually",
    "whole",
    "idea",
    "second",
    "drawback",
    "rdbms",
    "scalability",
    "okay",
    "third",
    "price",
    "price",
    "problem",
    "right",
    "mean",
    "obviously",
    "calculate",
    "cost",
    "look",
    "oracle",
    "costly",
    "solutions",
    "ic",
    "bank",
    "paying",
    "around",
    "30",
    "crores",
    "support",
    "fees",
    "oracle",
    "30",
    "crores",
    "even",
    "year",
    "think",
    "3",
    "months",
    "paying",
    "30",
    "cres",
    "40",
    "cres",
    "support",
    "oracle",
    "enterprise",
    "grade",
    "product",
    "right",
    "pay",
    "money",
    "way",
    "right",
    "switch",
    "nosql",
    "platforms",
    "open",
    "source",
    "pay",
    "much",
    "amount",
    "money",
    "discussion",
    "saying",
    "problems",
    "means",
    "traditionally",
    "storing",
    "data",
    "rdbms",
    "retrieving",
    "good",
    "modern",
    "day",
    "modern",
    "day",
    "happening",
    "every",
    "company",
    "want",
    "collect",
    "analyze",
    "data",
    "banking",
    "industry",
    "industry",
    "industry",
    "ind",
    "want",
    "want",
    "collect",
    "analyze",
    "data",
    "give",
    "example",
    "visit",
    "happening",
    "start",
    "tracking",
    "ip",
    "address",
    "uh",
    "visiting",
    "website",
    "long",
    "clicking",
    "page",
    "items",
    "adding",
    "cart",
    "images",
    "clicking",
    "offers",
    "clicking",
    "data",
    "analyzed",
    "understand",
    "customer",
    "behave",
    "marketing",
    "right",
    "start",
    "visiting",
    "companies",
    "start",
    "seeing",
    "recommendations",
    "visit",
    "amazon",
    "start",
    "recommending",
    "products",
    "asking",
    "buy",
    "collect",
    "data",
    "working",
    "flipkart",
    "flipcart",
    "said",
    "collect",
    "data",
    "apps",
    "using",
    "website",
    "even",
    "social",
    "media",
    "flipkart",
    "active",
    "social",
    "media",
    "like",
    "facebook",
    "twitter",
    "places",
    "active",
    "anybody",
    "tweets",
    "flip",
    "cart",
    "visit",
    "facebook",
    "page",
    "data",
    "collected",
    "actually",
    "called",
    "big",
    "data",
    "technical",
    "definition",
    "big",
    "data",
    "huge",
    "amount",
    "data",
    "process",
    "using",
    "traditional",
    "methods",
    "called",
    "big",
    "data",
    "right",
    "interest",
    "us",
    "going",
    "figure",
    "store",
    "data",
    "huge",
    "amount",
    "data",
    "right",
    "process",
    "data",
    "using",
    "traditional",
    "techniques",
    "able",
    "store",
    "process",
    "possible",
    "seeing",
    "use",
    "hadoop",
    "solution",
    "store",
    "data",
    "process",
    "data",
    "right",
    "interrupt",
    "anytime",
    "want",
    "ask",
    "anything",
    "keep",
    "talking",
    "yeah",
    "say",
    "sql",
    "databases",
    "mean",
    "rbms",
    "gone",
    "many",
    "people",
    "ask",
    "question",
    "okay",
    "rdbms",
    "solution",
    "transaction",
    "management",
    "even",
    "buying",
    "something",
    "flip",
    "cart",
    "uh",
    "complete",
    "transaction",
    "oracle",
    "probably",
    "transactional",
    "system",
    "transactional",
    "management",
    "rdbms",
    "systems",
    "ones",
    "ensure",
    "asset",
    "properties",
    "right",
    "transaction",
    "goes",
    "goes",
    "bought",
    "something",
    "flipkart",
    "tomorrow",
    "flipkart",
    "come",
    "back",
    "say",
    "really",
    "think",
    "know",
    "whether",
    "transaction",
    "happened",
    "probably",
    "got",
    "money",
    "possible",
    "right",
    "start",
    "complaining",
    "flip",
    "cart",
    "right",
    "flip",
    "cart",
    "comes",
    "say",
    "know",
    "using",
    "cassandra",
    "per",
    "cassandra",
    "transaction",
    "know",
    "come",
    "back",
    "another",
    "one",
    "week",
    "possible",
    "transaction",
    "management",
    "good",
    "question",
    "uh",
    "still",
    "handled",
    "rdbms",
    "technically",
    "called",
    "polyl",
    "persistence",
    "person",
    "use",
    "lot",
    "technical",
    "words",
    "polyl",
    "persistence",
    "means",
    "uh",
    "uh",
    "coexistence",
    "uh",
    "traditional",
    "databases",
    "know",
    "ones",
    "even",
    "look",
    "amazon",
    "company",
    "know",
    "transactional",
    "data",
    "bunch",
    "sqls",
    "handle",
    "type",
    "uh",
    "things",
    "okay",
    "polyl",
    "persistence",
    "well",
    "really",
    "mean",
    "person",
    "really",
    "use",
    "lot",
    "uh",
    "uh",
    "say",
    "polyglot",
    "persistence",
    "persistence",
    "e",
    "know",
    "even",
    "english",
    "skills",
    "stake",
    "training",
    "people",
    "persistence",
    "e",
    "think",
    "right",
    "think",
    "take",
    "vote",
    "probably",
    "yeah",
    "come",
    "hadoop",
    "far",
    "uh",
    "explain",
    "hadoop",
    "database",
    "teaching",
    "big",
    "data",
    "16",
    "hours",
    "challenge",
    "actually",
    "v",
    "topic",
    "actually",
    "world",
    "big",
    "data",
    "separate",
    "category",
    "systems",
    "called",
    "nosql",
    "databases",
    "databases",
    "database",
    "realtime",
    "uh",
    "know",
    "queries",
    "right",
    "area",
    "mongodb",
    "cassandra",
    "hspace",
    "guys",
    "guys",
    "used",
    "real",
    "time",
    "want",
    "fetch",
    "data",
    "say",
    "example",
    "opening",
    "website",
    "probably",
    "given",
    "nosql",
    "database",
    "right",
    "millions",
    "people",
    "booking",
    "flights",
    "clear",
    "clear",
    "trip",
    "identify",
    "know",
    "many",
    "people",
    "booking",
    "particular",
    "route",
    "give",
    "better",
    "offer",
    "probably",
    "done",
    "reading",
    "realtime",
    "data",
    "nosql",
    "system",
    "area",
    "nosql",
    "nosql",
    "db",
    "actually",
    "database",
    "area",
    "uh",
    "sql",
    "come",
    "hado",
    "right",
    "technical",
    "term",
    "coexistence",
    "dbs",
    "sql",
    "let",
    "understand",
    "um",
    "establish",
    "idea",
    "rdbms",
    "good",
    "us",
    "least",
    "world",
    "big",
    "data",
    "say",
    "rdbms",
    "good",
    "transaction",
    "management",
    "still",
    "handled",
    "rdbms",
    "say",
    "get",
    "rid",
    "rdbms",
    "right",
    "important",
    "term",
    "even",
    "though",
    "directly",
    "related",
    "syllabus",
    "think",
    "makes",
    "lot",
    "sense",
    "many",
    "aware",
    "term",
    "raise",
    "hand",
    "raise",
    "oh",
    "lot",
    "etl",
    "guys",
    "good",
    "know",
    "know",
    "etl",
    "right",
    "people",
    "know",
    "etl",
    "fine",
    "etl",
    "stands",
    "uh",
    "extract",
    "transform",
    "load",
    "idea",
    "see",
    "working",
    "uh",
    "company",
    "right",
    "company",
    "rdbm",
    "systems",
    "right",
    "rdbms",
    "systems",
    "probably",
    "guy",
    "using",
    "mysql",
    "also",
    "using",
    "oracle",
    "right",
    "lot",
    "rdbm",
    "systems",
    "used",
    "company",
    "right",
    "serving",
    "customers",
    "take",
    "example",
    "ici",
    "bank",
    "core",
    "banking",
    "data",
    "oracle",
    "core",
    "banking",
    "data",
    "oracle",
    "making",
    "online",
    "transaction",
    "data",
    "hits",
    "oracle",
    "crm",
    "data",
    "sql",
    "mean",
    "crm",
    "customer",
    "relationship",
    "right",
    "name",
    "address",
    "blah",
    "blah",
    "blah",
    "two",
    "type",
    "data",
    "type",
    "data",
    "ic",
    "bank",
    "al",
    "also",
    "acquired",
    "lot",
    "companies",
    "working",
    "subsidiary",
    "companies",
    "giving",
    "data",
    "form",
    "xml",
    "files",
    "xml",
    "right",
    "ic",
    "bank",
    "also",
    "facebook",
    "page",
    "okay",
    "getting",
    "around",
    "people",
    "visiting",
    "every",
    "day",
    "facebook",
    "page",
    "used",
    "click",
    "item",
    "cross",
    "selling",
    "upselling",
    "user",
    "goes",
    "facebook",
    "page",
    "say",
    "really",
    "like",
    "loan",
    "take",
    "loan",
    "right",
    "cross",
    "selling",
    "upselling",
    "right",
    "collect",
    "data",
    "social",
    "media",
    "comes",
    "format",
    "json",
    "javascript",
    "object",
    "notation",
    "json",
    "key",
    "value",
    "pairs",
    "basically",
    "core",
    "banking",
    "data",
    "crm",
    "data",
    "partner",
    "companies",
    "social",
    "media",
    "data",
    "getting",
    "okay",
    "customer",
    "care",
    "one",
    "social",
    "media",
    "data",
    "want",
    "analytics",
    "every",
    "day",
    "people",
    "visiting",
    "web",
    "page",
    "want",
    "know",
    "person",
    "clicked",
    "let",
    "say",
    "lawn",
    "offer",
    "much",
    "time",
    "stayed",
    "page",
    "age",
    "group",
    "belongs",
    "recommend",
    "products",
    "recommendations",
    "actually",
    "lot",
    "recommend",
    "social",
    "media",
    "data",
    "used",
    "reveal",
    "things",
    "internal",
    "basically",
    "using",
    "social",
    "media",
    "data",
    "recommendations",
    "download",
    "data",
    "customer",
    "clicking",
    "facebook",
    "page",
    "get",
    "access",
    "guy",
    "clicking",
    "ic",
    "bank",
    "facebook",
    "page",
    "know",
    "age",
    "occupation",
    "everything",
    "shared",
    "facebook",
    "default",
    "given",
    "cases",
    "restrict",
    "information",
    "public",
    "customers",
    "share",
    "downloading",
    "data",
    "today",
    "understand",
    "people",
    "clicked",
    "youngsters",
    "clicked",
    "laan",
    "means",
    "trend",
    "towards",
    "particular",
    "bike",
    "laan",
    "give",
    "offer",
    "bike",
    "laan",
    "youngsters",
    "may",
    "actually",
    "buy",
    "analytics",
    "right",
    "social",
    "media",
    "data",
    "use",
    "multiple",
    "ways",
    "actually",
    "getting",
    "json",
    "format",
    "actually",
    "type",
    "data",
    "also",
    "collecting",
    "customer",
    "care",
    "log",
    "files",
    "flat",
    "file",
    "coming",
    "format",
    "flat",
    "file",
    "flat",
    "file",
    "means",
    "text",
    "file",
    "customer",
    "care",
    "log",
    "actually",
    "means",
    "chat",
    "logs",
    "also",
    "call",
    "logs",
    "record",
    "get",
    "actual",
    "call",
    "long",
    "call",
    "happened",
    "pressed",
    "call",
    "customer",
    "press",
    "right",
    "huh",
    "actually",
    "captured",
    "want",
    "figure",
    "many",
    "people",
    "called",
    "category",
    "much",
    "time",
    "spend",
    "right",
    "clicked",
    "complaint",
    "button",
    "talk",
    "1",
    "hour",
    "chances",
    "high",
    "bad",
    "customer",
    "right",
    "mean",
    "whenever",
    "scolling",
    "probably",
    "right",
    "want",
    "figure",
    "right",
    "call",
    "record",
    "coming",
    "flat",
    "file",
    "format",
    "right",
    "point",
    "time",
    "ic",
    "bank",
    "also",
    "wanted",
    "offer",
    "solution",
    "previously",
    "used",
    "uh",
    "agents",
    "people",
    "go",
    "house",
    "make",
    "buy",
    "loan",
    "used",
    "go",
    "every",
    "place",
    "agent",
    "get",
    "list",
    "go",
    "place",
    "go",
    "house",
    "say",
    "sir",
    "please",
    "take",
    "car",
    "loan",
    "say",
    "say",
    "bye",
    "get",
    "money",
    "getting",
    "salary",
    "right",
    "technique",
    "working",
    "get",
    "list",
    "okay",
    "one",
    "customer",
    "uh",
    "place",
    "thisr",
    "right",
    "one",
    "guy",
    "inr",
    "another",
    "guy",
    "remote",
    "place",
    "chennai",
    "time",
    "travels",
    "place",
    "day",
    "able",
    "cover",
    "two",
    "customers",
    "ica",
    "bank",
    "mind",
    "track",
    "location",
    "device",
    "okay",
    "give",
    "customers",
    "real",
    "time",
    "kanja",
    "get",
    "customers",
    "nearby",
    "guy",
    "go",
    "right",
    "say",
    "ca",
    "go",
    "customer",
    "application",
    "tablet",
    "track",
    "guy",
    "wherever",
    "guy",
    "goes",
    "nearby",
    "customers",
    "provided",
    "cover",
    "customers",
    "right",
    "lot",
    "impact",
    "practical",
    "sometimes",
    "customers",
    "call",
    "say",
    "please",
    "come",
    "give",
    "loan",
    "might",
    "travel",
    "european",
    "city",
    "paris",
    "chennai",
    "yeah",
    "go",
    "paris",
    "right",
    "customer",
    "called",
    "right",
    "go",
    "paris",
    "meet",
    "customer",
    "know",
    "many",
    "customers",
    "around",
    "paris",
    "application",
    "track",
    "send",
    "guys",
    "collecting",
    "data",
    "also",
    "tabs",
    "uh",
    "quite",
    "big",
    "amount",
    "data",
    "collecting",
    "many",
    "customers",
    "met",
    "whether",
    "closed",
    "closed",
    "okay",
    "coming",
    "think",
    "format",
    "format",
    "write",
    "forgot",
    "actually",
    "coming",
    "whole",
    "data",
    "right",
    "data",
    "data",
    "lying",
    "different",
    "different",
    "systems",
    "one",
    "place",
    "core",
    "banking",
    "data",
    "oracle",
    "data",
    "place",
    "right",
    "want",
    "analyze",
    "data",
    "one",
    "thing",
    "go",
    "oracle",
    "say",
    "want",
    "analyze",
    "data",
    "oracle",
    "db",
    "already",
    "busy",
    "serving",
    "customers",
    "correct",
    "oracle",
    "db",
    "core",
    "banking",
    "database",
    "making",
    "transaction",
    "oracle",
    "handling",
    "right",
    "go",
    "oracle",
    "say",
    "let",
    "run",
    "analytics",
    "data",
    "work",
    "way",
    "oracle",
    "slow",
    "ca",
    "take",
    "data",
    "places",
    "dump",
    "central",
    "place",
    "call",
    "data",
    "warehouse",
    "right",
    "called",
    "many",
    "know",
    "data",
    "warehouse",
    "data",
    "warehouse",
    "nothing",
    "database",
    "one",
    "way",
    "understands",
    "language",
    "sql",
    "major",
    "difference",
    "facing",
    "customer",
    "right",
    "something",
    "called",
    "etl",
    "okay",
    "etl",
    "tools",
    "get",
    "data",
    "places",
    "right",
    "guy",
    "dump",
    "data",
    "warehouse",
    "right",
    "data",
    "warehouse",
    "also",
    "called",
    "olap",
    "online",
    "analytical",
    "platform",
    "forth",
    "purpose",
    "guy",
    "guy",
    "data",
    "okay",
    "transform",
    "data",
    "well",
    "called",
    "etl",
    "extract",
    "transform",
    "load",
    "take",
    "data",
    "dump",
    "useless",
    "right",
    "read",
    "data",
    "flat",
    "file",
    "give",
    "structure",
    "dump",
    "like",
    "proper",
    "table",
    "format",
    "data",
    "right",
    "run",
    "business",
    "intelligence",
    "bi",
    "tools",
    "tableau",
    "right",
    "heard",
    "micro",
    "strategy",
    "tableau",
    "connect",
    "visualization",
    "tool",
    "show",
    "charts",
    "diagrams",
    "happening",
    "mean",
    "traditional",
    "enterprise",
    "architecture",
    "people",
    "used",
    "work",
    "forever",
    "mean",
    "every",
    "company",
    "go",
    "default",
    "desperate",
    "uh",
    "discrete",
    "data",
    "sources",
    "etl",
    "tools",
    "heard",
    "informatica",
    "informatica",
    "one",
    "uh",
    "biggest",
    "etl",
    "tool",
    "informatica",
    "talent",
    "uh",
    "pentaho",
    "ssis",
    "etl",
    "tools",
    "tools",
    "connect",
    "take",
    "data",
    "apply",
    "schema",
    "push",
    "data",
    "warehouse",
    "data",
    "data",
    "one",
    "place",
    "chandra",
    "kar",
    "wish",
    "would",
    "go",
    "chandra",
    "kar",
    "ic",
    "banks",
    "ceo",
    "right",
    "place",
    "go",
    "chandra",
    "kar",
    "may",
    "know",
    "sql",
    "right",
    "ca",
    "exp",
    "expect",
    "right",
    "ceo",
    "company",
    "ca",
    "ask",
    "know",
    "java",
    "fired",
    "next",
    "moment",
    "right",
    "ceo",
    "want",
    "know",
    "happening",
    "right",
    "getting",
    "data",
    "understand",
    "happening",
    "company",
    "time",
    "sit",
    "learn",
    "language",
    "anything",
    "chandra",
    "kar",
    "go",
    "connect",
    "tableau",
    "ask",
    "tableau",
    "show",
    "last",
    "month",
    "uh",
    "transaction",
    "quarter",
    "product",
    "area",
    "show",
    "show",
    "nice",
    "bar",
    "chart",
    "happy",
    "important",
    "make",
    "mistake",
    "entire",
    "stuff",
    "changed",
    "right",
    "sees",
    "different",
    "right",
    "idea",
    "happening",
    "right",
    "know",
    "happening",
    "right",
    "etl",
    "tool",
    "run",
    "proper",
    "data",
    "captured",
    "dumped",
    "analyzed",
    "ceos",
    "cto",
    "use",
    "visualization",
    "tools",
    "connect",
    "data",
    "warehouse",
    "analyze",
    "data",
    "data",
    "inside",
    "data",
    "warehouse",
    "right",
    "accessible",
    "public",
    "public",
    "access",
    "internal",
    "company",
    "access",
    "like",
    "maybe",
    "ceos",
    "ic",
    "bank",
    "go",
    "visualize",
    "data",
    "others",
    "public",
    "access",
    "right",
    "visualize",
    "data",
    "make",
    "decision",
    "may",
    "make",
    "decision",
    "right",
    "whole",
    "setup",
    "hadoop",
    "far",
    "ic",
    "bank",
    "running",
    "far",
    "challenges",
    "faced",
    "one",
    "challenge",
    "etl",
    "tool",
    "okay",
    "know",
    "many",
    "run",
    "atl",
    "tools",
    "okay",
    "typically",
    "run",
    "single",
    "machine",
    "okay",
    "get",
    "installed",
    "server",
    "limit",
    "much",
    "guy",
    "pull",
    "data",
    "warehouse",
    "right",
    "much",
    "amount",
    "data",
    "guy",
    "pull",
    "data",
    "warehouse",
    "limitation",
    "actually",
    "ca",
    "terabytes",
    "data",
    "pulled",
    "point",
    "number",
    "one",
    "point",
    "number",
    "two",
    "none",
    "real",
    "time",
    "none",
    "real",
    "time",
    "go",
    "tanish",
    "store",
    "tanish",
    "jeweler",
    "stuff",
    "go",
    "tanish",
    "store",
    "buy",
    "something",
    "rupees",
    "use",
    "ic",
    "credit",
    "card",
    "immediately",
    "get",
    "message",
    "saying",
    "shop",
    "20",
    "20",
    "discount",
    "real",
    "time",
    "ca",
    "separate",
    "data",
    "etl",
    "jobs",
    "run",
    "night",
    "today",
    "morning",
    "shopping",
    "tomorrow",
    "get",
    "message",
    "20",
    "time",
    "shop",
    "right",
    "may",
    "use",
    "ic",
    "bank",
    "first",
    "problem",
    "since",
    "data",
    "increasing",
    "know",
    "able",
    "run",
    "etl",
    "properly",
    "second",
    "problem",
    "none",
    "real",
    "time",
    "wanted",
    "push",
    "customized",
    "offers",
    "requirement",
    "whenever",
    "transaction",
    "made",
    "want",
    "capture",
    "analyze",
    "right",
    "based",
    "recommend",
    "something",
    "user",
    "right",
    "say",
    "example",
    "using",
    "credit",
    "card",
    "debit",
    "card",
    "even",
    "clicking",
    "link",
    "immediately",
    "something",
    "happen",
    "easy",
    "right",
    "data",
    "might",
    "somewhere",
    "oracle",
    "track",
    "every",
    "transaction",
    "transactional",
    "db",
    "ca",
    "ca",
    "say",
    "track",
    "everything",
    "right",
    "possible",
    "third",
    "problem",
    "data",
    "warehouse",
    "data",
    "warehouse",
    "costly",
    "affair",
    "worked",
    "company",
    "called",
    "teradata",
    "leader",
    "data",
    "warehousing",
    "market",
    "us",
    "client",
    "uh",
    "terra",
    "using",
    "product",
    "called",
    "terra",
    "data",
    "data",
    "warehouse",
    "paying",
    "around",
    "5",
    "million",
    "every",
    "year",
    "setup",
    "good",
    "analyze",
    "data",
    "okay",
    "paying",
    "lot",
    "money",
    "actually",
    "data",
    "warehousing",
    "actually",
    "costly",
    "affair",
    "cheap",
    "want",
    "implement",
    "companies",
    "like",
    "ic",
    "bank",
    "must",
    "use",
    "right",
    "ca",
    "say",
    "use",
    "wanted",
    "cheaper",
    "solution",
    "want",
    "data",
    "data",
    "warehouse",
    "want",
    "alternate",
    "solution",
    "real",
    "time",
    "cheaper",
    "scalable",
    "right",
    "anything",
    "want",
    "integrate",
    "know",
    "type",
    "data",
    "comes",
    "wanted",
    "integrate",
    "came",
    "us",
    "asked",
    "us",
    "like",
    "migrate",
    "big",
    "data",
    "solution",
    "recommended",
    "hadoop",
    "migration",
    "took",
    "lot",
    "time",
    "different",
    "case",
    "altogether",
    "talk",
    "hadoop",
    "hadoop",
    "actually",
    "comes",
    "way",
    "say",
    "yeah",
    "sorry",
    "right",
    "right",
    "exactly",
    "right",
    "exactly",
    "right",
    "exactly",
    "possible",
    "possible",
    "exactly",
    "n",
    "dimensional",
    "cubes",
    "things",
    "right",
    "data",
    "warehousing",
    "call",
    "um",
    "uh",
    "schema",
    "schema",
    "call",
    "star",
    "schema",
    "snowflake",
    "schema",
    "right",
    "actually",
    "olap",
    "actually",
    "designed",
    "speed",
    "true",
    "analytical",
    "batch",
    "job",
    "actually",
    "right",
    "architecture",
    "never",
    "give",
    "realtime",
    "solution",
    "exactly",
    "exactly",
    "lot",
    "people",
    "argued",
    "saying",
    "actually",
    "beat",
    "parallel",
    "processing",
    "something",
    "called",
    "mpp",
    "right",
    "something",
    "called",
    "mpp",
    "mpp",
    "means",
    "massive",
    "parallel",
    "processing",
    "lot",
    "people",
    "came",
    "saying",
    "systems",
    "actually",
    "terra",
    "data",
    "really",
    "faster",
    "actually",
    "mean",
    "like",
    "rdbms",
    "faster",
    "okay",
    "lot",
    "people",
    "came",
    "saying",
    "said",
    "okay",
    "data",
    "warehouse",
    "slow",
    "parallel",
    "processing",
    "okay",
    "install",
    "four",
    "five",
    "machines",
    "par",
    "possible",
    "still",
    "problem",
    "exist",
    "physically",
    "divide",
    "tables",
    "logical",
    "division",
    "possible",
    "like",
    "said",
    "country",
    "based",
    "divide",
    "month",
    "based",
    "divide",
    "end",
    "day",
    "want",
    "join",
    "operation",
    "something",
    "questions",
    "happen",
    "never",
    "propose",
    "realtime",
    "solution",
    "crux",
    "trying",
    "tell",
    "uh",
    "even",
    "ica",
    "bank",
    "running",
    "cost",
    "another",
    "problem",
    "lot",
    "cost",
    "involved",
    "obviously",
    "since",
    "wanted",
    "collect",
    "lot",
    "unstructured",
    "data",
    "wanted",
    "go",
    "better",
    "solution",
    "okay",
    "hadoop",
    "came",
    "picture",
    "really",
    "mostly",
    "structure",
    "data",
    "mostly",
    "structure",
    "data",
    "used",
    "final",
    "place",
    "get",
    "data",
    "know",
    "somebody",
    "visualize",
    "rarely",
    "modify",
    "data",
    "modify",
    "data",
    "place",
    "final",
    "data",
    "available",
    "know",
    "connect",
    "visualize",
    "data",
    "probably",
    "run",
    "query",
    "get",
    "output",
    "analysis",
    "transaction",
    "transaction",
    "management",
    "right",
    "point",
    "let",
    "discuss",
    "little",
    "bit",
    "hado",
    "thought",
    "like",
    "today",
    "probably",
    "uh",
    "cover",
    "architecture",
    "little",
    "bit",
    "theory",
    "uh",
    "tomorrow",
    "look",
    "syllabus",
    "supposed",
    "teach",
    "u",
    "something",
    "called",
    "hadoop",
    "hadoop",
    "architecture",
    "anyway",
    "required",
    "right",
    "map",
    "reduce",
    "catch",
    "map",
    "reduce",
    "default",
    "programming",
    "framework",
    "top",
    "hado",
    "anybody",
    "want",
    "analyze",
    "data",
    "top",
    "hado",
    "use",
    "something",
    "called",
    "map",
    "reduce",
    "point",
    "number",
    "one",
    "map",
    "reduce",
    "came",
    "long",
    "back",
    "okay",
    "today",
    "lot",
    "ways",
    "analyze",
    "data",
    "map",
    "ruce",
    "per",
    "syllabus",
    "curriculum",
    "says",
    "teach",
    "map",
    "ruce",
    "concepts",
    "map",
    "r",
    "one",
    "thing",
    "want",
    "learn",
    "map",
    "ruce",
    "ideally",
    "expert",
    "java",
    "map",
    "ruce",
    "programs",
    "written",
    "java",
    "programming",
    "language",
    "write",
    "python",
    "things",
    "natively",
    "people",
    "write",
    "map",
    "ruce",
    "programs",
    "java",
    "know",
    "familiar",
    "java",
    "comfortable",
    "java",
    "talk",
    "lot",
    "java",
    "may",
    "make",
    "much",
    "sense",
    "learning",
    "map",
    "reduce",
    "help",
    "spark",
    "next",
    "learning",
    "something",
    "called",
    "spark",
    "right",
    "16",
    "hours",
    "spark",
    "content",
    "spark",
    "also",
    "uses",
    "similar",
    "concepts",
    "map",
    "ruce",
    "even",
    "even",
    "though",
    "able",
    "understand",
    "every",
    "line",
    "code",
    "writing",
    "okay",
    "need",
    "understand",
    "logic",
    "working",
    "java",
    "programs",
    "everybody",
    "able",
    "follow",
    "understand",
    "deb",
    "expected",
    "also",
    "discuss",
    "map",
    "reduce",
    "write",
    "map",
    "program",
    "run",
    "map",
    "reduce",
    "program",
    "able",
    "understand",
    "happening",
    "inside",
    "right",
    "logically",
    "like",
    "syntax",
    "wise",
    "line",
    "line",
    "wise",
    "right",
    "complete",
    "hadoop",
    "architecture",
    "like",
    "tomorrow",
    "discussing",
    "map",
    "ruce",
    "theory",
    "map",
    "ruce",
    "programs",
    "running",
    "next",
    "8",
    "hour",
    "syllabus",
    "completing",
    "bit",
    "map",
    "reduce",
    "going",
    "something",
    "called",
    "hive",
    "p",
    "hive",
    "hive",
    "data",
    "warehouse",
    "hadu",
    "sql",
    "side",
    "understand",
    "point",
    "yeah",
    "somebody",
    "question",
    "sorry",
    "someone",
    "asked",
    "question",
    "middle",
    "said",
    "complete",
    "asked",
    "nobody",
    "asked",
    "okay",
    "mean",
    "um",
    "rub",
    "right",
    "going",
    "going",
    "delete",
    "part",
    "time",
    "okay",
    "remove",
    "part",
    "time",
    "yeah",
    "h",
    "happening",
    "traditional",
    "data",
    "warehouses",
    "like",
    "data",
    "popular",
    "market",
    "like",
    "people",
    "used",
    "say",
    "convince",
    "confuse",
    "people",
    "easy",
    "right",
    "saying",
    "see",
    "lot",
    "people",
    "getting",
    "confused",
    "hadoop",
    "came",
    "hadoop",
    "became",
    "popular",
    "solution",
    "data",
    "warehousing",
    "right",
    "since",
    "hadoop",
    "open",
    "source",
    "read",
    "mckeny",
    "report",
    "1",
    "by4",
    "cost",
    "traditional",
    "data",
    "warehouse",
    "happen",
    "everybody",
    "use",
    "hadoop",
    "obviously",
    "pay",
    "extra",
    "money",
    "drawback",
    "traditional",
    "data",
    "warehousing",
    "companies",
    "sell",
    "right",
    "today",
    "go",
    "website",
    "informatica",
    "even",
    "terra",
    "data",
    "say",
    "selling",
    "uh",
    "terra",
    "data",
    "plus",
    "hadoop",
    "previously",
    "give",
    "10",
    "million",
    "give",
    "take",
    "useless",
    "actually",
    "buy",
    "terra",
    "dat",
    "saying",
    "teradata",
    "name",
    "market",
    "different",
    "thing",
    "right",
    "teradata",
    "say",
    "buy",
    "data",
    "warehouse",
    "also",
    "give",
    "hado",
    "cheaper",
    "right",
    "previously",
    "paid",
    "us",
    "10",
    "million",
    "pay",
    "like",
    "okay",
    "great",
    "solution",
    "getting",
    "hado",
    "also",
    "actually",
    "need",
    "use",
    "ter",
    "dat",
    "build",
    "custom",
    "solution",
    "problem",
    "couple",
    "problems",
    "one",
    "uh",
    "name",
    "company",
    "like",
    "terra",
    "data",
    "support",
    "also",
    "second",
    "takes",
    "lot",
    "time",
    "migrate",
    "things",
    "one",
    "fine",
    "morning",
    "go",
    "say",
    "okay",
    "tomorrow",
    "onwards",
    "using",
    "hadu",
    "work",
    "like",
    "right",
    "company",
    "need",
    "time",
    "migrations",
    "processes",
    "people",
    "stick",
    "traditional",
    "tools",
    "seeing",
    "uh",
    "companies",
    "went",
    "consulting",
    "completely",
    "migrated",
    "big",
    "data",
    "platforms",
    "like",
    "hadu",
    "keep",
    "terra",
    "data",
    "less",
    "level",
    "like",
    "hot",
    "data",
    "call",
    "right",
    "hot",
    "data",
    "cold",
    "data",
    "warm",
    "data",
    "immediately",
    "accessible",
    "hot",
    "data",
    "terra",
    "data",
    "cold",
    "warm",
    "data",
    "need",
    "immediately",
    "analyze",
    "dump",
    "hadoop",
    "something",
    "keep",
    "reduce",
    "overall",
    "cost",
    "architecture",
    "right",
    "speaking",
    "hadoop",
    "uh",
    "architecture",
    "still",
    "changed",
    "little",
    "bit",
    "okay",
    "write",
    "uh",
    "question",
    "happened",
    "right",
    "lot",
    "people",
    "handled",
    "lot",
    "trainings",
    "hadoop",
    "spark",
    "lot",
    "people",
    "ask",
    "okay",
    "learning",
    "completely",
    "new",
    "technology",
    "called",
    "hadoop",
    "say",
    "wrong",
    "hadoop",
    "came",
    "2005",
    "almost",
    "13",
    "years",
    "almost",
    "like",
    "old",
    "uh",
    "people",
    "still",
    "new",
    "understand",
    "hado",
    "uh",
    "2005",
    "framework",
    "called",
    "hadoop",
    "created",
    "okay",
    "long",
    "back",
    "right",
    "uh",
    "two",
    "guys",
    "called",
    "duck",
    "cutting",
    "mike",
    "cilla",
    "created",
    "framework",
    "took",
    "original",
    "idea",
    "google",
    "google",
    "already",
    "lot",
    "distributed",
    "computing",
    "google",
    "uh",
    "dealing",
    "lot",
    "big",
    "data",
    "right",
    "published",
    "couple",
    "white",
    "papers",
    "duck",
    "cutting",
    "cilla",
    "took",
    "idea",
    "google",
    "created",
    "framework",
    "called",
    "hadoop",
    "later",
    "gave",
    "apache",
    "open",
    "source",
    "project",
    "hadoop",
    "open",
    "source",
    "project",
    "actually",
    "means",
    "apache",
    "right",
    "download",
    "free",
    "install",
    "free",
    "pay",
    "money",
    "something",
    "really",
    "free",
    "drawbacks",
    "also",
    "right",
    "correct",
    "example",
    "uh",
    "android",
    "phones",
    "iphone",
    "used",
    "use",
    "android",
    "phones",
    "know",
    "difference",
    "right",
    "difference",
    "show",
    "difference",
    "real",
    "difference",
    "android",
    "phone",
    "might",
    "get",
    "lot",
    "bugs",
    "may",
    "come",
    "apple",
    "phone",
    "used",
    "understand",
    "problem",
    "android",
    "open",
    "source",
    "right",
    "open",
    "source",
    "means",
    "every",
    "company",
    "modify",
    "according",
    "requirement",
    "samsung",
    "phone",
    "android",
    "right",
    "accordingly",
    "lot",
    "bugs",
    "may",
    "come",
    "platform",
    "may",
    "stable",
    "apple",
    "phone",
    "apple",
    "produces",
    "right",
    "bug",
    "also",
    "call",
    "apple",
    "somebody",
    "call",
    "apple",
    "fix",
    "one",
    "version",
    "available",
    "know",
    "bucks",
    "may",
    "come",
    "may",
    "come",
    "story",
    "originally",
    "apache",
    "hado",
    "even",
    "still",
    "go",
    "apache",
    "website",
    "download",
    "hado",
    "soon",
    "people",
    "realize",
    "using",
    "apache",
    "hado",
    "open",
    "source",
    "uh",
    "something",
    "work",
    "something",
    "crashes",
    "nobody",
    "fix",
    "open",
    "source",
    "accordingly",
    "company",
    "called",
    "cloudera",
    "king",
    "company",
    "called",
    "clera",
    "might",
    "probably",
    "heard",
    "clera",
    "first",
    "company",
    "created",
    "uh",
    "uh",
    "commercial",
    "distribution",
    "hadoop",
    "related",
    "tools",
    "today",
    "go",
    "cloud",
    "era",
    "ask",
    "give",
    "give",
    "hado",
    "advantage",
    "getting",
    "hadoop",
    "almost",
    "similar",
    "find",
    "apache",
    "website",
    "install",
    "something",
    "working",
    "fix",
    "support",
    "since",
    "paying",
    "money",
    "get",
    "support",
    "like",
    "vendor",
    "right",
    "one",
    "companies",
    "popular",
    "world",
    "big",
    "data",
    "cloud",
    "era",
    "stell",
    "hadoop",
    "spark",
    "everything",
    "bundle",
    "okay",
    "even",
    "lab",
    "actually",
    "cloud",
    "data",
    "distribution",
    "lab",
    "going",
    "use",
    "actually",
    "runs",
    "cloud",
    "data",
    "distribution",
    "uh",
    "also",
    "use",
    "thing",
    "right",
    "internet",
    "want",
    "show",
    "go",
    "simply",
    "type",
    "probably",
    "later",
    "simply",
    "type",
    "cloud",
    "era",
    "see",
    "cloud",
    "era",
    "website",
    "right",
    "uh",
    "cloud",
    "era",
    "came",
    "200",
    "eight",
    "today",
    "buying",
    "big",
    "data",
    "platform",
    "leading",
    "vendor",
    "cloud",
    "guard",
    "give",
    "everything",
    "packaged",
    "one",
    "one",
    "product",
    "like",
    "hadoop",
    "sql",
    "hbas",
    "related",
    "tools",
    "data",
    "science",
    "tools",
    "commercially",
    "actually",
    "number",
    "one",
    "provider",
    "cloud",
    "data",
    "right",
    "somewhere",
    "2012",
    "discuss",
    "hadoop",
    "depth",
    "okay",
    "giving",
    "idea",
    "discuss",
    "architecture",
    "also",
    "one",
    "company",
    "called",
    "hoton",
    "works",
    "hoton",
    "works",
    "came",
    "somewhere",
    "2012",
    "okay",
    "honb",
    "also",
    "next",
    "major",
    "competitor",
    "cloud",
    "era",
    "major",
    "difference",
    "honb",
    "sells",
    "original",
    "hadu",
    "apache",
    "hadu",
    "modify",
    "clera",
    "sells",
    "modified",
    "version",
    "say",
    "better",
    "actually",
    "apache",
    "open",
    "source",
    "license",
    "apache",
    "says",
    "anybody",
    "modify",
    "sell",
    "thing",
    "also",
    "provide",
    "free",
    "version",
    "go",
    "cloud",
    "era",
    "download",
    "cloud",
    "product",
    "free",
    "pay",
    "money",
    "get",
    "support",
    "people",
    "buy",
    "support",
    "right",
    "want",
    "sell",
    "hadoop",
    "start",
    "company",
    "download",
    "apache",
    "modify",
    "source",
    "code",
    "sell",
    "guys",
    "two",
    "things",
    "one",
    "give",
    "free",
    "show",
    "offer",
    "also",
    "support",
    "second",
    "thing",
    "story",
    "every",
    "open",
    "source",
    "right",
    "ubu",
    "linux",
    "red",
    "hat",
    "linux",
    "difference",
    "red",
    "hat",
    "yes",
    "open",
    "source",
    "whatever",
    "product",
    "hoton",
    "works",
    "open",
    "source",
    "hadu",
    "download",
    "product",
    "cloudas",
    "100",
    "open",
    "source",
    "custom",
    "product",
    "download",
    "free",
    "website",
    "install",
    "many",
    "machines",
    "want",
    "get",
    "technical",
    "support",
    "ideally",
    "people",
    "right",
    "need",
    "technical",
    "support",
    "pay",
    "money",
    "get",
    "actually",
    "right",
    "companies",
    "hoton",
    "works",
    "clera",
    "leading",
    "vendors",
    "big",
    "data",
    "world",
    "also",
    "provide",
    "certification",
    "want",
    "write",
    "certification",
    "exam",
    "probably",
    "spark",
    "class",
    "try",
    "uh",
    "claer",
    "provides",
    "certification",
    "exams",
    "big",
    "data",
    "world",
    "write",
    "get",
    "also",
    "one",
    "company",
    "called",
    "mppar",
    "three",
    "many",
    "companies",
    "actually",
    "uh",
    "common",
    "ones",
    "three",
    "also",
    "somebody",
    "called",
    "mapar",
    "apach",
    "right",
    "open",
    "source",
    "community",
    "company",
    "like",
    "edit",
    "free",
    "also",
    "edit",
    "free",
    "something",
    "crashes",
    "post",
    "question",
    "community",
    "hey",
    "yesterday",
    "old",
    "servers",
    "crashed",
    "fired",
    "company",
    "please",
    "help",
    "fix",
    "want",
    "enterprise",
    "world",
    "right",
    "real",
    "thing",
    "fair",
    "still",
    "people",
    "trusting",
    "companies",
    "microsoft",
    "apple",
    "oracle",
    "get",
    "support",
    "right",
    "something",
    "work",
    "able",
    "fix",
    "right",
    "call",
    "make",
    "fix",
    "mappa",
    "third",
    "company",
    "sort",
    "like",
    "popular",
    "company",
    "information",
    "correct",
    "created",
    "couple",
    "people",
    "quit",
    "clouder",
    "hoton",
    "works",
    "indians",
    "right",
    "found",
    "company",
    "called",
    "mapar",
    "mapar",
    "popular",
    "days",
    "actually",
    "popular",
    "vendor",
    "highest",
    "paid",
    "edition",
    "also",
    "map",
    "cost",
    "wise",
    "bigger",
    "cler",
    "hotworks",
    "cloud",
    "hotw",
    "license",
    "cheaper",
    "mapar",
    "map",
    "license",
    "cost",
    "high",
    "actually",
    "map",
    "architecture",
    "also",
    "different",
    "like",
    "traditional",
    "hadoop",
    "architecture",
    "right",
    "reason",
    "uh",
    "like",
    "went",
    "ge",
    "went",
    "flipkart",
    "using",
    "either",
    "hot",
    "works",
    "cloud",
    "era",
    "use",
    "map",
    "much",
    "also",
    "downloaded",
    "open",
    "source",
    "hado",
    "modified",
    "selling",
    "story",
    "made",
    "lot",
    "change",
    "architecture",
    "hadoop",
    "actually",
    "apart",
    "ibm",
    "flavor",
    "called",
    "big",
    "insights",
    "microsoft",
    "flavor",
    "called",
    "hd",
    "inside",
    "h",
    "hd",
    "inside",
    "amazon",
    "every",
    "company",
    "addition",
    "uh",
    "hadoop",
    "b",
    "data",
    "platform",
    "popular",
    "ones",
    "claa",
    "hoton",
    "works",
    "mappa",
    "guys",
    "popular",
    "ones",
    "actually",
    "music",
    "huh",
    "developer",
    "much",
    "difference",
    "example",
    "writing",
    "map",
    "reduce",
    "program",
    "run",
    "way",
    "three",
    "platforms",
    "much",
    "difference",
    "difference",
    "actually",
    "hoton",
    "workor",
    "totally",
    "open",
    "source",
    "sell",
    "apache",
    "gives",
    "cloud",
    "modifies",
    "bit",
    "say",
    "modification",
    "hado",
    "cable",
    "claim",
    "know",
    "marketing",
    "gimmick",
    "like",
    "buying",
    "android",
    "samsung",
    "mobile",
    "phone",
    "htc",
    "mobile",
    "phone",
    "difference",
    "end",
    "day",
    "internally",
    "android",
    "samsung",
    "say",
    "android",
    "better",
    "uh",
    "bug",
    "free",
    "something",
    "htc",
    "say",
    "phone",
    "never",
    "hang",
    "htc",
    "samsung",
    "know",
    "done",
    "inside",
    "phone",
    "end",
    "day",
    "download",
    "say",
    "whatsapp",
    "run",
    "phones",
    "right",
    "android",
    "thing",
    "commercial",
    "uh",
    "distribution",
    "story",
    "linux",
    "buy",
    "red",
    "hat",
    "also",
    "one",
    "provider",
    "right",
    "sus",
    "red",
    "hat",
    "linux",
    "sus",
    "linux",
    "internally",
    "everything",
    "type",
    "ls",
    "command",
    "work",
    "ways",
    "kernel",
    "might",
    "slightly",
    "different",
    "end",
    "day",
    "paying",
    "open",
    "source",
    "original",
    "linux",
    "story",
    "uh",
    "considering",
    "platforms",
    "cloud",
    "seems",
    "popular",
    "got",
    "head",
    "start",
    "2008",
    "started",
    "right",
    "long",
    "back",
    "started",
    "hoton",
    "works",
    "also",
    "gaining",
    "lot",
    "popularity",
    "days",
    "seeing",
    "using",
    "cloud",
    "era",
    "lab",
    "cloud",
    "era",
    "distribution",
    "okay",
    "talked",
    "hadu",
    "right",
    "talking",
    "lot",
    "company",
    "start",
    "session",
    "check",
    "brakes",
    "think",
    "right",
    "see",
    "control",
    "brakes",
    "controlled",
    "uh",
    "right",
    "control",
    "approve",
    "control",
    "trust",
    "uh",
    "see",
    "talking",
    "schedules",
    "know",
    "okay",
    "teach",
    "stop",
    "say",
    "stop",
    "stop",
    "okay",
    "way",
    "like",
    "class",
    "work",
    "also",
    "paid",
    "money",
    "right",
    "free",
    "class",
    "half",
    "could",
    "walked",
    "since",
    "paid",
    "money",
    "stay",
    "know",
    "okay",
    "yeah",
    "sure",
    "please",
    "ask",
    "ask",
    "many",
    "questions",
    "want",
    "yes",
    "correct",
    "map",
    "reduce",
    "nobody",
    "using",
    "even",
    "enterprise",
    "map",
    "redu",
    "gone",
    "spark",
    "saying",
    "curriculum",
    "map",
    "ruce",
    "learning",
    "map",
    "ruce",
    "two",
    "reasons",
    "one",
    "migration",
    "lot",
    "companies",
    "already",
    "lot",
    "map",
    "ruce",
    "programs",
    "want",
    "go",
    "spark",
    "want",
    "go",
    "know",
    "map",
    "ruce",
    "second",
    "reason",
    "even",
    "learning",
    "spark",
    "concepts",
    "map",
    "ruce",
    "important",
    "spark",
    "also",
    "built",
    "basic",
    "idea",
    "map",
    "ruce",
    "map",
    "produce",
    "works",
    "program",
    "runs",
    "basic",
    "concepts",
    "understand",
    "help",
    "learn",
    "spark",
    "better",
    "way",
    "reason",
    "learning",
    "spark",
    "uh",
    "map",
    "ruce",
    "actually",
    "enterprise",
    "totally",
    "gone",
    "nothing",
    "called",
    "map",
    "produce",
    "spark",
    "uses",
    "python",
    "course",
    "uh",
    "reason",
    "teaching",
    "spark",
    "initially",
    "uh",
    "great",
    "lakes",
    "asked",
    "teach",
    "spark",
    "said",
    "yes",
    "problem",
    "teach",
    "spark",
    "using",
    "scala",
    "huh",
    "scala",
    "developer",
    "learning",
    "spark",
    "using",
    "python",
    "spark",
    "programmed",
    "four",
    "languages",
    "scala",
    "python",
    "r",
    "java",
    "four",
    "languages",
    "learn",
    "python",
    "python",
    "guy",
    "reason",
    "teaching",
    "spark",
    "actually",
    "like",
    "teach",
    "spark",
    "trainings",
    "days",
    "spark",
    "actually",
    "know",
    "python",
    "also",
    "teach",
    "immediately",
    "said",
    "pick",
    "later",
    "probably",
    "teach",
    "sorry",
    "scala",
    "similar",
    "java",
    "like",
    "uh",
    "similar",
    "java",
    "8",
    "know",
    "java",
    "8",
    "glorified",
    "verion",
    "java",
    "ah",
    "glorified",
    "version",
    "java",
    "actually",
    "java",
    "8",
    "developed",
    "based",
    "scala",
    "scala",
    "another",
    "programming",
    "language",
    "nice",
    "language",
    "actually",
    "uh",
    "know",
    "people",
    "really",
    "like",
    "okay",
    "like",
    "sense",
    "popular",
    "java",
    "right",
    "java",
    "popular",
    "scala",
    "popular",
    "uh",
    "interesting",
    "programming",
    "language",
    "used",
    "scala",
    "lot",
    "programs",
    "trainings",
    "anyway",
    "questions",
    "go",
    "hadoop",
    "architecture",
    "many",
    "questions",
    "questions",
    "ideally",
    "either",
    "people",
    "understood",
    "everything",
    "understood",
    "nothing",
    "right",
    "dat",
    "yeah",
    "consider",
    "data",
    "warehouse",
    "difference",
    "uh",
    "see",
    "soon",
    "uh",
    "difference",
    "buying",
    "traditional",
    "data",
    "warehouse",
    "one",
    "thing",
    "become",
    "data",
    "warehouse",
    "ca",
    "anything",
    "else",
    "hadoop",
    "like",
    "become",
    "data",
    "warehouse",
    "much",
    "see",
    "still",
    "architecture",
    "huh",
    "see",
    "situation",
    "complicated",
    "actually",
    "starting",
    "new",
    "company",
    "ideally",
    "much",
    "money",
    "go",
    "cloud",
    "right",
    "every",
    "solution",
    "offered",
    "cloud",
    "typically",
    "uh",
    "companies",
    "go",
    "hadoop",
    "available",
    "cloud",
    "also",
    "go",
    "cloud",
    "distributions",
    "always",
    "say",
    "hadoop",
    "100",
    "replacement",
    "data",
    "warehouse",
    "okay",
    "work",
    "data",
    "warehouse",
    "performance",
    "considerations",
    "sometimes",
    "might",
    "need",
    "reports",
    "within",
    "millisecond",
    "second",
    "2",
    "second",
    "probably",
    "hadoop",
    "may",
    "able",
    "hado",
    "batch",
    "processing",
    "system",
    "real",
    "time",
    "terra",
    "data",
    "exa",
    "data",
    "neta",
    "know",
    "guys",
    "really",
    "fast",
    "right",
    "mpp",
    "companies",
    "keep",
    "hot",
    "data",
    "probably",
    "company",
    "100",
    "terab",
    "data",
    "2",
    "terab",
    "data",
    "need",
    "every",
    "day",
    "fast",
    "keep",
    "teradata",
    "98",
    "terab",
    "dump",
    "haru",
    "need",
    "even",
    "takes",
    "time",
    "fine",
    "okay",
    "story",
    "cloud",
    "organizations",
    "deploying",
    "everything",
    "amazon",
    "web",
    "services",
    "amazon",
    "offers",
    "fully",
    "managed",
    "hadoop",
    "solutions",
    "cloud",
    "like",
    "go",
    "amazon",
    "ask",
    "give",
    "hadoop",
    "cluster",
    "10",
    "minutes",
    "fully",
    "functional",
    "hadoop",
    "solution",
    "everything",
    "installed",
    "running",
    "servers",
    "everything",
    "automation",
    "right",
    "also",
    "give",
    "tr",
    "data",
    "warehouses",
    "something",
    "called",
    "data",
    "warehouse",
    "red",
    "shift",
    "solution",
    "called",
    "red",
    "shift",
    "red",
    "shift",
    "online",
    "data",
    "warehouse",
    "amazon",
    "offers",
    "red",
    "shift",
    "want",
    "h",
    "data",
    "stalled",
    "like",
    "said",
    "everything",
    "given",
    "one",
    "solution",
    "need",
    "cheaper",
    "need",
    "speed",
    "know",
    "everything",
    "package",
    "one",
    "solution",
    "may",
    "possible",
    "right",
    "like",
    "someone",
    "suggesting",
    "right",
    "compromise",
    "speed",
    "either",
    "right",
    "uh",
    "realtime",
    "access",
    "right",
    "like",
    "solution",
    "people",
    "adopt",
    "everything",
    "guaranteed",
    "time",
    "okay",
    "takes",
    "us",
    "question",
    "hadoop",
    "popular",
    "architecture",
    "hadoop",
    "fairly",
    "easy",
    "understand",
    "actually",
    "idea",
    "simple",
    "single",
    "machine",
    "much",
    "amount",
    "data",
    "store",
    "single",
    "machine",
    "think",
    "give",
    "server",
    "much",
    "store",
    "based",
    "okay",
    "based",
    "yeah",
    "storage",
    "capacity",
    "decide",
    "give",
    "server",
    "many",
    "hard",
    "disk",
    "much",
    "space",
    "dump",
    "yeah",
    "give",
    "server",
    "decide",
    "many",
    "hard",
    "disk",
    "add",
    "motherboard",
    "right",
    "motherboard",
    "decides",
    "motherboard",
    "say",
    "support",
    "10",
    "hard",
    "disk",
    "10",
    "hard",
    "disk",
    "idea",
    "simple",
    "people",
    "initially",
    "thought",
    "use",
    "one",
    "machine",
    "machine",
    "sufficient",
    "distributor",
    "computing",
    "came",
    "early",
    "right",
    "ah",
    "added",
    "dis",
    "reached",
    "dis",
    "one",
    "machine",
    "want",
    "archive",
    "super",
    "computing",
    "uh",
    "want",
    "cheaper",
    "ca",
    "sell",
    "house",
    "another",
    "machine",
    "going",
    "talk",
    "something",
    "came",
    "external",
    "storage",
    "nas",
    "right",
    "network",
    "storage",
    "probably",
    "guys",
    "much",
    "storage",
    "background",
    "something",
    "called",
    "network",
    "attached",
    "storage",
    "popular",
    "heard",
    "emc",
    "company",
    "called",
    "emc",
    "square",
    "survived",
    "things",
    "something",
    "called",
    "network",
    "attach",
    "storage",
    "nas",
    "box",
    "buy",
    "box",
    "install",
    "box",
    "anywhere",
    "dump",
    "data",
    "system",
    "dump",
    "process",
    "called",
    "nas",
    "support",
    "storage",
    "actually",
    "within",
    "machine",
    "nas",
    "enough",
    "came",
    "sand",
    "storage",
    "area",
    "network",
    "sand",
    "even",
    "popular",
    "days",
    "okay",
    "full",
    "room",
    "full",
    "hard",
    "disk",
    "get",
    "fiber",
    "channel",
    "connection",
    "dump",
    "hard",
    "dis",
    "store",
    "much",
    "want",
    "problem",
    "even",
    "using",
    "sand",
    "sand",
    "give",
    "unlimited",
    "storage",
    "okay",
    "problem",
    "gives",
    "storage",
    "processing",
    "want",
    "process",
    "data",
    "one",
    "server",
    "process",
    "data",
    "sand",
    "possible",
    "right",
    "processing",
    "happen",
    "storage",
    "happen",
    "sand",
    "sands",
    "costly",
    "clumsy",
    "manage",
    "need",
    "separate",
    "people",
    "manage",
    "people",
    "got",
    "fed",
    "sand",
    "also",
    "nas",
    "san",
    "started",
    "thinking",
    "distributed",
    "computing",
    "distributed",
    "computing",
    "new",
    "idea",
    "already",
    "okay",
    "hadoop",
    "simple",
    "rather",
    "one",
    "machine",
    "take",
    "bunch",
    "machines",
    "okay",
    "say",
    "bunch",
    "sake",
    "argument",
    "four",
    "okay",
    "take",
    "four",
    "machines",
    "1",
    "two",
    "3",
    "4",
    "four",
    "machines",
    "let",
    "say",
    "linux",
    "machines",
    "fair",
    "windows",
    "machines",
    "uh",
    "support",
    "windows",
    "limited",
    "open",
    "source",
    "community",
    "typically",
    "recommend",
    "boxes",
    "unix",
    "linux",
    "take",
    "bunch",
    "boxes",
    "say",
    "four",
    "boxes",
    "servers",
    "right",
    "download",
    "install",
    "cloudera",
    "hotworks",
    "hadoop",
    "installing",
    "ask",
    "machine",
    "master",
    "machine",
    "slave",
    "idea",
    "one",
    "machine",
    "master",
    "rest",
    "slaves",
    "architecture",
    "come",
    "idea",
    "one",
    "guy",
    "master",
    "three",
    "guys",
    "slaves",
    "fine",
    "discussed",
    "master",
    "slave",
    "later",
    "okay",
    "install",
    "hado",
    "setup",
    "one",
    "master",
    "let",
    "say",
    "three",
    "slaves",
    "right",
    "idea",
    "want",
    "store",
    "anything",
    "okay",
    "hadoop",
    "help",
    "store",
    "among",
    "three",
    "boxes",
    "right",
    "use",
    "story",
    "space",
    "three",
    "boxes",
    "2",
    "tbte",
    "hard",
    "disk",
    "get",
    "total",
    "capacity",
    "6",
    "terabyte",
    "hadoop",
    "cluster",
    "called",
    "hadoop",
    "cluster",
    "cluster",
    "means",
    "group",
    "computers",
    "hadoop",
    "yeah",
    "hadoop",
    "framework",
    "actually",
    "software",
    "one",
    "confusion",
    "people",
    "hadoop",
    "software",
    "plan",
    "platform",
    "okay",
    "show",
    "different",
    "different",
    "components",
    "hadoop",
    "platform",
    "install",
    "hadoop",
    "let",
    "say",
    "know",
    "anything",
    "hadu",
    "let",
    "say",
    "install",
    "four",
    "system",
    "setup",
    "installing",
    "ask",
    "select",
    "one",
    "master",
    "remaining",
    "slave",
    "one",
    "master",
    "three",
    "slaves",
    "right",
    "hado",
    "take",
    "storage",
    "space",
    "three",
    "machines",
    "project",
    "single",
    "sle",
    "6",
    "tbte",
    "storage",
    "box",
    "right",
    "beauty",
    "architecture",
    "let",
    "say",
    "start",
    "dumping",
    "data",
    "data",
    "gets",
    "stored",
    "let",
    "say",
    "6",
    "terab",
    "add",
    "machines",
    "fly",
    "three",
    "expand",
    "without",
    "shutting",
    "cluster",
    "meaning",
    "storage",
    "problem",
    "solved",
    "ever",
    "worry",
    "storage",
    "hadoop",
    "cluster",
    "hadoop",
    "allows",
    "resizing",
    "without",
    "downtime",
    "add",
    "machines",
    "remove",
    "machines",
    "decide",
    "right",
    "tell",
    "data",
    "stored",
    "come",
    "discussed",
    "storage",
    "processing",
    "giving",
    "foot",
    "overview",
    "like",
    "happening",
    "hado",
    "cluster",
    "one",
    "advantage",
    "start",
    "like",
    "three",
    "machines",
    "expand",
    "30",
    "300",
    "machines",
    "yeah",
    "like",
    "yes",
    "also",
    "add",
    "data",
    "analyzing",
    "data",
    "okay",
    "keep",
    "adding",
    "machines",
    "without",
    "shutting",
    "remove",
    "removing",
    "problem",
    "example",
    "program",
    "running",
    "remove",
    "program",
    "might",
    "crash",
    "machine",
    "needed",
    "idea",
    "one",
    "machine",
    "nothing",
    "running",
    "remove",
    "gracefully",
    "nothing",
    "happen",
    "getting",
    "something",
    "called",
    "scaling",
    "architecture",
    "called",
    "scaling",
    "meaning",
    "keep",
    "adding",
    "machines",
    "uh",
    "know",
    "uh",
    "get",
    "unlimited",
    "scalability",
    "tell",
    "storage",
    "happens",
    "far",
    "discuss",
    "storage",
    "happens",
    "yeah",
    "possible",
    "one",
    "sizing",
    "yeah",
    "possible",
    "sqls",
    "designed",
    "fashion",
    "want",
    "modify",
    "anything",
    "first",
    "stop",
    "reading",
    "writing",
    "database",
    "alter",
    "table",
    "increase",
    "anything",
    "decrease",
    "anything",
    "probably",
    "able",
    "add",
    "machine",
    "extra",
    "uh",
    "make",
    "sense",
    "sql",
    "world",
    "actually",
    "sql",
    "world",
    "come",
    "server",
    "setup",
    "impossible",
    "cost",
    "first",
    "scalability",
    "like",
    "said",
    "major",
    "difference",
    "sql",
    "word",
    "logical",
    "division",
    "data",
    "physically",
    "divide",
    "data",
    "logically",
    "dividing",
    "data",
    "like",
    "said",
    "country",
    "india",
    "2",
    "cr",
    "people",
    "one",
    "machine",
    "taking",
    "data",
    "another",
    "less",
    "data",
    "way",
    "manage",
    "handled",
    "show",
    "happening",
    "b",
    "called",
    "hadoop",
    "cluster",
    "technical",
    "term",
    "cluster",
    "cluster",
    "nothing",
    "group",
    "machines",
    "right",
    "available",
    "hadoop",
    "cluster",
    "typically",
    "uh",
    "go",
    "companies",
    "check",
    "like",
    "20",
    "2",
    "bar7",
    "company",
    "called",
    "25",
    "bar7",
    "call",
    "center",
    "company",
    "run",
    "hardo",
    "clusters",
    "analytics",
    "50",
    "machine",
    "cluster",
    "50",
    "node",
    "cluster",
    "node",
    "around",
    "256",
    "gb",
    "ram",
    "10",
    "tab",
    "storage",
    "50",
    "10",
    "sorry",
    "10",
    "saying",
    "100",
    "terab",
    "storage",
    "50",
    "100",
    "5",
    "petabyte",
    "storage",
    "256",
    "50",
    "ram",
    "capacity",
    "cluster",
    "minimal",
    "hardo",
    "cluster",
    "cheap",
    "hardo",
    "cluster",
    "typically",
    "500",
    "knots",
    "th000",
    "knots",
    "hardo",
    "clusters",
    "amount",
    "data",
    "storing",
    "petabytes",
    "th000",
    "terabyte",
    "one",
    "petabyte",
    "actually",
    "companies",
    "huge",
    "amount",
    "data",
    "dump",
    "size",
    "cluster",
    "uh",
    "uh",
    "level",
    "actually",
    "hadoop",
    "distributed",
    "meaning",
    "install",
    "getting",
    "installed",
    "one",
    "machine",
    "installed",
    "machines",
    "components",
    "talk",
    "runs",
    "cluster",
    "single",
    "machine",
    "two",
    "machines",
    "actually",
    "right",
    "basic",
    "idea",
    "storage",
    "add",
    "machines",
    "get",
    "storage",
    "right",
    "got",
    "question",
    "master",
    "goes",
    "right",
    "come",
    "back",
    "slave",
    "acting",
    "slave",
    "actually",
    "yeah",
    "good",
    "question",
    "another",
    "thing",
    "let",
    "ask",
    "question",
    "asking",
    "build",
    "hado",
    "cluster",
    "50",
    "machines",
    "take",
    "example",
    "50",
    "slave",
    "machines",
    "50",
    "servers",
    "really",
    "think",
    "cheaper",
    "solution",
    "terms",
    "hardware",
    "like",
    "buy",
    "50",
    "servers",
    "500",
    "servers",
    "really",
    "cheaper",
    "solution",
    "think",
    "yeah",
    "let",
    "say",
    "huh",
    "correct",
    "point",
    "let",
    "say",
    "buying",
    "500",
    "servers",
    "server",
    "cost",
    "money",
    "right",
    "say",
    "500",
    "times",
    "money",
    "right",
    "really",
    "huge",
    "amount",
    "investment",
    "reason",
    "call",
    "machines",
    "commodity",
    "hardware",
    "mean",
    "commodity",
    "hardware",
    "ware",
    "commodity",
    "hardware",
    "means",
    "machine",
    "without",
    "label",
    "never",
    "buy",
    "dell",
    "server",
    "build",
    "hadoop",
    "cluster",
    "bu",
    "build",
    "cheap",
    "throwaway",
    "servers",
    "right",
    "going",
    "buy",
    "volvo",
    "bus",
    "going",
    "buy",
    "tm",
    "tamil",
    "nadu",
    "blaming",
    "tamil",
    "nadu",
    "take",
    "regionally",
    "okay",
    "even",
    "kerala",
    "whatever",
    "going",
    "buy",
    "volvo",
    "bus",
    "accommodate",
    "passengers",
    "buying",
    "bus",
    "know",
    "break",
    "point",
    "time",
    "see",
    "want",
    "carry",
    "citizens",
    "buying",
    "volvo",
    "buses",
    "right",
    "look",
    "buses",
    "idea",
    "citizens",
    "need",
    "travel",
    "one",
    "point",
    "another",
    "point",
    "bus",
    "investing",
    "crores",
    "buying",
    "volvo",
    "buses",
    "everywhere",
    "right",
    "buy",
    "bus",
    "idea",
    "hardo",
    "cluster",
    "typical",
    "hardo",
    "cluster",
    "machines",
    "use",
    "called",
    "commodity",
    "hardware",
    "commodity",
    "hardware",
    "means",
    "assembled",
    "servers",
    "get",
    "actually",
    "actually",
    "buying",
    "ibm",
    "server",
    "also",
    "working",
    "design",
    "architect",
    "one",
    "hadoop",
    "solution",
    "uh",
    "got",
    "servers",
    "around",
    "rupees",
    "rupees",
    "rupees",
    "buy",
    "server",
    "ibm",
    "server",
    "pay",
    "2",
    "lakh",
    "three",
    "lakh",
    "like",
    "somebody",
    "assemble",
    "give",
    "thing",
    "work",
    "crash",
    "also",
    "definitely",
    "crash",
    "100",
    "thinking",
    "500",
    "hadoop",
    "cluster",
    "slaves",
    "crashing",
    "reliability",
    "answer",
    "question",
    "par",
    "question",
    "time",
    "server",
    "big",
    "storage",
    "box",
    "right",
    "storage",
    "processing",
    "yeah",
    "server",
    "server",
    "desktop",
    "normal",
    "desktops",
    "right",
    "normally",
    "something",
    "called",
    "desktop",
    "home",
    "server",
    "something",
    "serve",
    "multiple",
    "clients",
    "one",
    "way",
    "bigger",
    "box",
    "bigger",
    "storage",
    "processing",
    "want",
    "buy",
    "server",
    "multiple",
    "option",
    "one",
    "go",
    "tell",
    "ibm",
    "give",
    "buy",
    "server",
    "give",
    "branded",
    "server",
    "charge",
    "lot",
    "money",
    "also",
    "give",
    "support",
    "pay",
    "two",
    "lakh",
    "three",
    "lakh",
    "server",
    "normally",
    "say",
    "server",
    "people",
    "buy",
    "hadoop",
    "cluster",
    "never",
    "buy",
    "cheapest",
    "servers",
    "possible",
    "afford",
    "failures",
    "hardo",
    "cluster",
    "show",
    "failures",
    "handled",
    "hardo",
    "cluster",
    "inves",
    "invting",
    "lot",
    "money",
    "hardware",
    "idea",
    "otherwise",
    "solution",
    "feasible",
    "buying",
    "500",
    "ibm",
    "servers",
    "take",
    "money",
    "give",
    "teradata",
    "good",
    "thing",
    "right",
    "need",
    "support",
    "anything",
    "need",
    "normal",
    "machines",
    "hadoop",
    "also",
    "say",
    "kind",
    "machine",
    "need",
    "even",
    "build",
    "hadoop",
    "cluster",
    "using",
    "desktops",
    "want",
    "performance",
    "less",
    "beautifully",
    "work",
    "laptops",
    "anything",
    "want",
    "say",
    "right",
    "uh",
    "keep",
    "diagram",
    "need",
    "diagram",
    "removing",
    "actually",
    "yeah",
    "h",
    "right",
    "correct",
    "right",
    "new",
    "businesses",
    "might",
    "take",
    "hado",
    "five",
    "years",
    "may",
    "buy",
    "terra",
    "data",
    "right",
    "situation",
    "might",
    "buy",
    "hadoop",
    "solution",
    "probably",
    "tell",
    "biggest",
    "challenge",
    "experience",
    "consulting",
    "experience",
    "biggest",
    "challenge",
    "knowledge",
    "see",
    "companies",
    "want",
    "better",
    "solution",
    "right",
    "company",
    "want",
    "unnecessarily",
    "pay",
    "money",
    "one",
    "thing",
    "terra",
    "dat",
    "solution",
    "market",
    "go",
    "lot",
    "people",
    "understand",
    "data",
    "warehousing",
    "data",
    "warehousing",
    "new",
    "new",
    "concept",
    "go",
    "market",
    "get",
    "n",
    "number",
    "people",
    "know",
    "data",
    "warehousing",
    "easily",
    "install",
    "terra",
    "data",
    "pay",
    "money",
    "building",
    "team",
    "hadoop",
    "expertise",
    "easy",
    "sitting",
    "right",
    "go",
    "company",
    "also",
    "understand",
    "hadoop",
    "working",
    "hadoop",
    "like",
    "click",
    "click",
    "install",
    "lot",
    "things",
    "configure",
    "tune",
    "people",
    "want",
    "spend",
    "much",
    "risk",
    "one",
    "reason",
    "uh",
    "knowledge",
    "curve",
    "call",
    "right",
    "learning",
    "curve",
    "bit",
    "high",
    "know",
    "ca",
    "learn",
    "day",
    "say",
    "okay",
    "implement",
    "possible",
    "right",
    "companies",
    "stuck",
    "uh",
    "uh",
    "position",
    "tell",
    "real",
    "story",
    "since",
    "asked",
    "question",
    "went",
    "one",
    "companies",
    "training",
    "okay",
    "company",
    "said",
    "want",
    "learn",
    "hadu",
    "basics",
    "said",
    "okay",
    "big",
    "deal",
    "teach",
    "hadu",
    "basics",
    "went",
    "company",
    "started",
    "training",
    "started",
    "asking",
    "like",
    "name",
    "note",
    "block",
    "size",
    "related",
    "hadu",
    "said",
    "hey",
    "teaching",
    "hado",
    "basics",
    "asking",
    "questions",
    "said",
    "already",
    "know",
    "hadoop",
    "said",
    "company",
    "got",
    "project",
    "related",
    "hadu",
    "implemented",
    "hado",
    "working",
    "hado",
    "two",
    "years",
    "okay",
    "learned",
    "everything",
    "want",
    "training",
    "clarify",
    "everything",
    "correct",
    "actually",
    "something",
    "really",
    "happening",
    "check",
    "friends",
    "company",
    "working",
    "big",
    "data",
    "tell",
    "story",
    "manager",
    "say",
    "okay",
    "tomorrow",
    "big",
    "data",
    "project",
    "start",
    "join",
    "half",
    "run",
    "away",
    "okay",
    "half",
    "stay",
    "okay",
    "guys",
    "learn",
    "able",
    "share",
    "experience",
    "go",
    "many",
    "companies",
    "learners",
    "resources",
    "training",
    "programs",
    "like",
    "learn",
    "sit",
    "learn",
    "learn",
    "period",
    "time",
    "another",
    "problem",
    "want",
    "learn",
    "oracle",
    "get",
    "official",
    "trainings",
    "lot",
    "lot",
    "people",
    "know",
    "oracle",
    "teach",
    "right",
    "learning",
    "curve",
    "actually",
    "makes",
    "problem",
    "cases",
    "actually",
    "think",
    "guys",
    "better",
    "know",
    "right",
    "joining",
    "company",
    "say",
    "hey",
    "aware",
    "things",
    "probably",
    "get",
    "immediate",
    "project",
    "anyway",
    "learning",
    "get",
    "job",
    "believe",
    "right",
    "end",
    "day",
    "idea",
    "get",
    "job",
    "right",
    "show",
    "show",
    "architecture",
    "need",
    "understand",
    "since",
    "looking",
    "high",
    "level",
    "definitions",
    "let",
    "dive",
    "bit",
    "deeper",
    "hadoop",
    "three",
    "major",
    "components",
    "something",
    "called",
    "hdfs",
    "something",
    "called",
    "yan",
    "share",
    "slides",
    "pdf",
    "want",
    "make",
    "note",
    "mandatory",
    "third",
    "called",
    "map",
    "reduce",
    "download",
    "install",
    "hadoop",
    "hadoop",
    "three",
    "things",
    "come",
    "default",
    "hdfs",
    "yann",
    "map",
    "produce",
    "three",
    "components",
    "uh",
    "installed",
    "default",
    "anything",
    "get",
    "installed",
    "right",
    "guy",
    "storage",
    "guy",
    "handling",
    "storage",
    "okay",
    "guy",
    "resource",
    "management",
    "resource",
    "manager",
    "processing",
    "pen",
    "liting",
    "hdfs",
    "guy",
    "responsible",
    "storage",
    "say",
    "software",
    "piece",
    "manage",
    "huh",
    "inside",
    "installing",
    "hadu",
    "three",
    "components",
    "actually",
    "see",
    "actually",
    "hdfs",
    "click",
    "see",
    "show",
    "okay",
    "hdfs",
    "hdfs",
    "stands",
    "hadoop",
    "distributed",
    "file",
    "system",
    "file",
    "system",
    "actually",
    "called",
    "hadoop",
    "distributed",
    "file",
    "system",
    "write",
    "probably",
    "hadoop",
    "distributed",
    "file",
    "system",
    "hdfs",
    "course",
    "point",
    "view",
    "uh",
    "hdfs",
    "important",
    "talks",
    "like",
    "uh",
    "data",
    "stored",
    "yarn",
    "much",
    "important",
    "cover",
    "yarn",
    "map",
    "reduce",
    "learn",
    "sure",
    "map",
    "reduce",
    "pushing",
    "tomorrow",
    "processing",
    "part",
    "tomorrow",
    "going",
    "learn",
    "process",
    "data",
    "today",
    "okay",
    "teach",
    "everything",
    "also",
    "end",
    "day",
    "go",
    "back",
    "recollect",
    "things",
    "right",
    "get",
    "part",
    "push",
    "tomorrow",
    "many",
    "java",
    "background",
    "oh",
    "us",
    "least",
    "okay",
    "map",
    "actually",
    "java",
    "okay",
    "look",
    "code",
    "many",
    "people",
    "java",
    "background",
    "also",
    "like",
    "said",
    "industry",
    "never",
    "happen",
    "write",
    "map",
    "ruce",
    "code",
    "map",
    "ru",
    "gone",
    "industry",
    "migrations",
    "happen",
    "even",
    "able",
    "understand",
    "logic",
    "sufficient",
    "actually",
    "first",
    "let",
    "look",
    "guy",
    "called",
    "hdfs",
    "right",
    "hdfs",
    "probably",
    "push",
    "yarn",
    "map",
    "ruce",
    "tomorrow",
    "complete",
    "hdfs",
    "want",
    "show",
    "something",
    "called",
    "hadoop",
    "ecosystem",
    "okay",
    "draw",
    "complete",
    "hdf",
    "storage",
    "part",
    "yan",
    "simple",
    "piece",
    "map",
    "r",
    "inway",
    "tomorrow",
    "covering",
    "right",
    "guy",
    "handling",
    "storage",
    "lot",
    "asking",
    "store",
    "data",
    "hero",
    "right",
    "okay",
    "pen",
    "gone",
    "actually",
    "extra",
    "pen",
    "writing",
    "actually",
    "one",
    "used",
    "pens",
    "think",
    "also",
    "red",
    "green",
    "get",
    "black",
    "okay",
    "think",
    "right",
    "let",
    "take",
    "hypothetical",
    "situation",
    "installed",
    "hadoop",
    "one",
    "master",
    "okay",
    "example",
    "six",
    "slave",
    "machines",
    "picture",
    "uh",
    "pose",
    "lot",
    "questions",
    "usually",
    "okay",
    "okay",
    "one",
    "master",
    "six",
    "slave",
    "machines",
    "talking",
    "storage",
    "processing",
    "master",
    "process",
    "called",
    "name",
    "node",
    "master",
    "machine",
    "running",
    "process",
    "called",
    "name",
    "node",
    "install",
    "start",
    "running",
    "process",
    "called",
    "name",
    "node",
    "okay",
    "slaves",
    "running",
    "process",
    "called",
    "data",
    "node",
    "data",
    "node",
    "call",
    "data",
    "note",
    "1",
    "2",
    "3",
    "4",
    "five",
    "six",
    "technically",
    "say",
    "one",
    "name",
    "node",
    "six",
    "data",
    "nodes",
    "mean",
    "say",
    "hado",
    "cluster",
    "right",
    "name",
    "node",
    "storage",
    "master",
    "data",
    "node",
    "storage",
    "slave",
    "okay",
    "thus",
    "far",
    "understood",
    "next",
    "thing",
    "let",
    "say",
    "want",
    "store",
    "file",
    "hadoop",
    "cluster",
    "hadoop",
    "care",
    "storing",
    "meaning",
    "store",
    "format",
    "xml",
    "json",
    "images",
    "video",
    "hadoop",
    "really",
    "care",
    "format",
    "data",
    "storing",
    "processing",
    "make",
    "sense",
    "data",
    "map",
    "ruce",
    "stored",
    "text",
    "file",
    "write",
    "map",
    "ruce",
    "read",
    "text",
    "file",
    "whatever",
    "want",
    "hdfs",
    "file",
    "system",
    "really",
    "uh",
    "bother",
    "type",
    "data",
    "storing",
    "second",
    "point",
    "probably",
    "important",
    "point",
    "store",
    "file",
    "edit",
    "modifications",
    "possible",
    "store",
    "delete",
    "hdfs",
    "modification",
    "impossible",
    "understand",
    "facts",
    "really",
    "well",
    "get",
    "confused",
    "later",
    "right",
    "edit",
    "possible",
    "hdfs",
    "may",
    "asking",
    "possible",
    "hdfs",
    "designed",
    "file",
    "system",
    "handling",
    "huge",
    "amount",
    "data",
    "terabytes",
    "petabytes",
    "amount",
    "data",
    "right",
    "want",
    "modify",
    "edit",
    "know",
    "uh",
    "row",
    "row",
    "like",
    "transaction",
    "happening",
    "know",
    "possible",
    "look",
    "hard",
    "disk",
    "right",
    "something",
    "called",
    "seek",
    "time",
    "hard",
    "disk",
    "want",
    "fetch",
    "record",
    "something",
    "called",
    "seek",
    "time",
    "right",
    "one",
    "terabyte",
    "file",
    "want",
    "edit",
    "100th",
    "line",
    "possible",
    "hard",
    "disk",
    "seek",
    "get",
    "data",
    "using",
    "commodity",
    "machines",
    "even",
    "faster",
    "using",
    "solid",
    "straight",
    "drives",
    "anything",
    "commodity",
    "uh",
    "level",
    "hardware",
    "using",
    "hadoop",
    "idea",
    "want",
    "read",
    "files",
    "sequentially",
    "random",
    "access",
    "sequential",
    "access",
    "store",
    "file",
    "read",
    "entire",
    "file",
    "process",
    "say",
    "needle",
    "haack",
    "problem",
    "needle",
    "haack",
    "problem",
    "meaning",
    "1",
    "million",
    "row",
    "table",
    "say",
    "take",
    "100",
    "line",
    "edit",
    "possible",
    "hado",
    "least",
    "hado",
    "impossible",
    "either",
    "read",
    "whole",
    "data",
    "process",
    "result",
    "stored",
    "obviously",
    "process",
    "data",
    "get",
    "result",
    "stored",
    "file",
    "system",
    "editing",
    "possible",
    "ca",
    "edit",
    "possible",
    "h",
    "directly",
    "update",
    "possible",
    "open",
    "file",
    "update",
    "say",
    "huh",
    "appending",
    "possible",
    "appending",
    "always",
    "possible",
    "file",
    "want",
    "append",
    "data",
    "okay",
    "adding",
    "end",
    "reading",
    "100",
    "throw",
    "edit",
    "possible",
    "may",
    "wondering",
    "want",
    "solution",
    "mpp",
    "engines",
    "top",
    "hu",
    "show",
    "mean",
    "possible",
    "considering",
    "hdfs",
    "ideally",
    "storing",
    "file",
    "got",
    "modifications",
    "file",
    "delete",
    "restore",
    "file",
    "meaning",
    "edit",
    "actually",
    "right",
    "take",
    "del",
    "yeah",
    "delete",
    "delete",
    "whole",
    "file",
    "delete",
    "possible",
    "saying",
    "minute",
    "edits",
    "possible",
    "hadoop",
    "considered",
    "system",
    "analysis",
    "data",
    "transactional",
    "system",
    "minute",
    "edits",
    "come",
    "tp",
    "systems",
    "rdbms",
    "want",
    "insert",
    "update",
    "insert",
    "update",
    "rdbms",
    "right",
    "rdbms",
    "world",
    "possible",
    "objection",
    "possible",
    "default",
    "huh",
    "exactly",
    "costly",
    "exactly",
    "correct",
    "also",
    "transactional",
    "transactional",
    "db",
    "right",
    "want",
    "minutely",
    "edit",
    "also",
    "worst",
    "case",
    "want",
    "possible",
    "mppp",
    "engin",
    "actually",
    "exactly",
    "storing",
    "one",
    "tb",
    "file",
    "want",
    "process",
    "file",
    "probably",
    "want",
    "process",
    "100",
    "line",
    "load",
    "one",
    "tv",
    "ram",
    "hdfs",
    "alone",
    "hdfs",
    "alone",
    "business",
    "use",
    "cases",
    "come",
    "need",
    "make",
    "decision",
    "teach",
    "train",
    "meaning",
    "writing",
    "group",
    "query",
    "aggregate",
    "query",
    "right",
    "probably",
    "need",
    "whole",
    "data",
    "big",
    "table",
    "writing",
    "aggregate",
    "query",
    "right",
    "say",
    "group",
    "buy",
    "order",
    "something",
    "like",
    "makes",
    "sense",
    "load",
    "full",
    "file",
    "know",
    "data",
    "whole",
    "data",
    "need",
    "scanned",
    "probably",
    "select",
    "something",
    "something",
    "name",
    "equal",
    "something",
    "name",
    "equal",
    "raguraman",
    "one",
    "line",
    "need",
    "processed",
    "processing",
    "one",
    "line",
    "load",
    "whole",
    "data",
    "required",
    "option",
    "called",
    "mpp",
    "engine",
    "show",
    "massive",
    "parallel",
    "processing",
    "engines",
    "hado",
    "tools",
    "like",
    "hawk",
    "la",
    "uh",
    "impala",
    "guys",
    "actually",
    "show",
    "work",
    "dumping",
    "data",
    "warehouse",
    "right",
    "want",
    "update",
    "transaction",
    "data",
    "update",
    "happens",
    "right",
    "transactions",
    "happening",
    "correct",
    "customers",
    "placing",
    "order",
    "data",
    "want",
    "change",
    "data",
    "created",
    "customer",
    "want",
    "get",
    "data",
    "right",
    "probably",
    "want",
    "modify",
    "get",
    "want",
    "make",
    "changes",
    "want",
    "analyze",
    "data",
    "see",
    "anyway",
    "let",
    "take",
    "hypothetical",
    "situation",
    "huh",
    "tell",
    "role",
    "okay",
    "going",
    "happen",
    "let",
    "say",
    "want",
    "store",
    "file",
    "got",
    "new",
    "marker",
    "way",
    "right",
    "let",
    "say",
    "want",
    "store",
    "file",
    "size",
    "know",
    "192",
    "mb",
    "file",
    "text",
    "file",
    "imagine",
    "want",
    "store",
    "size",
    "file",
    "take",
    "break",
    "okay",
    "410",
    "actually",
    "probably",
    "take",
    "take",
    "anyway",
    "file",
    "size",
    "192",
    "megabytes",
    "want",
    "store",
    "hado",
    "cluster",
    "lot",
    "things",
    "actually",
    "matters",
    "realtime",
    "experience",
    "learning",
    "basics",
    "real",
    "company",
    "want",
    "connect",
    "hadoop",
    "cluster",
    "hadoop",
    "cluster",
    "somewhere",
    "else",
    "sitting",
    "hadoop",
    "cluster",
    "separate",
    "separate",
    "right",
    "get",
    "something",
    "called",
    "hadoop",
    "client",
    "package",
    "okay",
    "one",
    "way",
    "get",
    "something",
    "called",
    "hadoop",
    "client",
    "using",
    "access",
    "laptop",
    "second",
    "log",
    "node",
    "something",
    "called",
    "gateway",
    "machine",
    "called",
    "gateway",
    "machine",
    "sitting",
    "nice",
    "right",
    "connect",
    "machine",
    "machine",
    "connect",
    "hado",
    "cluster",
    "directly",
    "get",
    "inside",
    "hado",
    "cluster",
    "point",
    "security",
    "reasons",
    "many",
    "reasons",
    "companies",
    "two",
    "thing",
    "either",
    "install",
    "something",
    "called",
    "hado",
    "plan",
    "even",
    "rare",
    "used",
    "linux",
    "machine",
    "called",
    "gateway",
    "node",
    "get",
    "username",
    "password",
    "type",
    "connect",
    "machine",
    "machine",
    "already",
    "know",
    "reach",
    "worry",
    "issue",
    "commands",
    "run",
    "got",
    "right",
    "whatever",
    "way",
    "let",
    "say",
    "want",
    "push",
    "192",
    "file",
    "machine",
    "connected",
    "gateway",
    "machine",
    "said",
    "okay",
    "upload",
    "data",
    "going",
    "happen",
    "first",
    "thing",
    "going",
    "happen",
    "name",
    "node",
    "machine",
    "master",
    "guy",
    "tell",
    "something",
    "called",
    "block",
    "size",
    "something",
    "called",
    "block",
    "size",
    "block",
    "size",
    "configured",
    "every",
    "hadoop",
    "cluster",
    "installing",
    "hadoop",
    "assuming",
    "block",
    "size",
    "cluster",
    "64",
    "megabytes",
    "block",
    "size",
    "block",
    "size",
    "uh",
    "tell",
    "block",
    "size",
    "tell",
    "maximum",
    "size",
    "data",
    "store",
    "meaning",
    "storing",
    "192",
    "mv",
    "file",
    "happen",
    "divide",
    "three",
    "blocks",
    "thing",
    "happens",
    "laptop",
    "storing",
    "song",
    "laptop",
    "happens",
    "song",
    "get",
    "stored",
    "think",
    "okay",
    "store",
    "song",
    "mp3",
    "song",
    "right",
    "laptop",
    "think",
    "hard",
    "disk",
    "actually",
    "take",
    "song",
    "divide",
    "divide",
    "store",
    "think",
    "linux",
    "uses",
    "4",
    "kilobyte",
    "4",
    "kb",
    "block",
    "size",
    "linux",
    "windows",
    "also",
    "uses",
    "similar",
    "file",
    "system",
    "chop",
    "data",
    "store",
    "thing",
    "happens",
    "hadoop",
    "also",
    "assuming",
    "hadoop",
    "cluster",
    "block",
    "size",
    "64",
    "mb",
    "name",
    "node",
    "get",
    "back",
    "client",
    "package",
    "say",
    "divide",
    "file",
    "three",
    "whatever",
    "want",
    "block",
    "size",
    "64",
    "mb",
    "going",
    "happen",
    "file",
    "get",
    "divided",
    "three",
    "blocks",
    "happen",
    "behind",
    "scenes",
    "like",
    "say",
    "block",
    "size",
    "happen",
    "automatically",
    "hado",
    "cent",
    "hado",
    "client",
    "package",
    "gate",
    "gateway",
    "hado",
    "cent",
    "installed",
    "g",
    "hado",
    "client",
    "gateway",
    "already",
    "communicate",
    "name",
    "node",
    "name",
    "node",
    "say",
    "boss",
    "cluster",
    "block",
    "size",
    "64",
    "mb",
    "manage",
    "client",
    "divide",
    "data",
    "blocks",
    "64",
    "6464",
    "total",
    "128",
    "three",
    "blocks",
    "b1",
    "b2",
    "b3",
    "three",
    "blocks",
    "data",
    "right",
    "go",
    "back",
    "name",
    "node",
    "ask",
    "name",
    "node",
    "three",
    "blocks",
    "okay",
    "tell",
    "store",
    "name",
    "node",
    "communication",
    "data",
    "nodes",
    "knows",
    "much",
    "space",
    "available",
    "tell",
    "one",
    "thing",
    "store",
    "first",
    "block",
    "probably",
    "example",
    "store",
    "second",
    "block",
    "probably",
    "b2",
    "store",
    "third",
    "block",
    "b3",
    "cases",
    "name",
    "node",
    "give",
    "different",
    "different",
    "machines",
    "never",
    "allow",
    "store",
    "everything",
    "one",
    "machine",
    "divide",
    "distribute",
    "storing",
    "data",
    "data",
    "divided",
    "three",
    "blocks",
    "stored",
    "three",
    "different",
    "data",
    "nodes",
    "like",
    "six",
    "reality",
    "50",
    "500",
    "machines",
    "three",
    "blocks",
    "easy",
    "right",
    "sometimes",
    "couple",
    "blocks",
    "may",
    "end",
    "one",
    "machine",
    "also",
    "depends",
    "storage",
    "ideally",
    "gets",
    "stored",
    "like",
    "may",
    "wondering",
    "block",
    "sizes",
    "random",
    "name",
    "node",
    "idea",
    "like",
    "much",
    "storage",
    "spaces",
    "available",
    "data",
    "node",
    "come",
    "free",
    "machines",
    "okay",
    "within",
    "picks",
    "randomly",
    "three",
    "say",
    "dump",
    "yes",
    "processing",
    "tell",
    "think",
    "logically",
    "want",
    "process",
    "data",
    "okay",
    "three",
    "machines",
    "process",
    "data",
    "right",
    "store",
    "three",
    "blocks",
    "one",
    "machine",
    "one",
    "machine",
    "process",
    "three",
    "blocks",
    "right",
    "makes",
    "difference",
    "right",
    "ideally",
    "divide",
    "distribute",
    "data",
    "cases",
    "name",
    "automatically",
    "inside",
    "lan",
    "inside",
    "landan",
    "machines",
    "communicating",
    "data",
    "nodes",
    "send",
    "hardbeat",
    "name",
    "node",
    "saying",
    "alive",
    "also",
    "name",
    "note",
    "detect",
    "many",
    "alive",
    "anything",
    "yeah",
    "problem",
    "problem",
    "machine",
    "crashes",
    "bmtc",
    "bus",
    "right",
    "volvo",
    "bus",
    "right",
    "crash",
    "point",
    "time",
    "crashes",
    "block",
    "one",
    "gone",
    "ca",
    "get",
    "data",
    "default",
    "hadoop",
    "replication",
    "three",
    "means",
    "block",
    "gets",
    "replicated",
    "three",
    "times",
    "machines",
    "draw",
    "come",
    "questions",
    "give",
    "moment",
    "b2",
    "b2",
    "drawing",
    "randomly",
    "mean",
    "taking",
    "machines",
    "b3",
    "probably",
    "b3",
    "look",
    "block",
    "block",
    "replicated",
    "three",
    "times",
    "different",
    "different",
    "machines",
    "let",
    "say",
    "machine",
    "crashes",
    "b2",
    "still",
    "available",
    "recover",
    "happens",
    "name",
    "node",
    "store",
    "metadata",
    "meaning",
    "name",
    "node",
    "write",
    "file",
    "called",
    "divided",
    "three",
    "blocks",
    "block",
    "one",
    "available",
    "three",
    "3",
    "2",
    "6",
    "6",
    "b1",
    "3",
    "2",
    "6",
    "b2",
    "b2",
    "huh",
    "one",
    "b3",
    "yeah",
    "metadata",
    "meaning",
    "tomorrow",
    "want",
    "read",
    "file",
    "clue",
    "file",
    "go",
    "name",
    "node",
    "hadoop",
    "client",
    "connect",
    "name",
    "node",
    "name",
    "node",
    "pass",
    "information",
    "go",
    "b1",
    "b2",
    "b3",
    "sorry",
    "go",
    "data",
    "note",
    "3",
    "42",
    "get",
    "give",
    "alternative",
    "one",
    "purpose",
    "name",
    "node",
    "actually",
    "store",
    "metadata",
    "index",
    "name",
    "node",
    "crashes",
    "access",
    "cluster",
    "sure",
    "name",
    "node",
    "gone",
    "everything",
    "gone",
    "prevent",
    "hadoop",
    "clusters",
    "active",
    "name",
    "node",
    "p",
    "uh",
    "standby",
    "name",
    "node",
    "meaning",
    "two",
    "name",
    "nodes",
    "two",
    "guys",
    "constant",
    "communication",
    "active",
    "name",
    "crashes",
    "standby",
    "take",
    "talk",
    "little",
    "bit",
    "later",
    "active",
    "standby",
    "working",
    "worry",
    "active",
    "name",
    "crashes",
    "standby",
    "immediately",
    "take",
    "coming",
    "questions",
    "yeah",
    "please",
    "correct",
    "three",
    "blocks",
    "replication",
    "huh",
    "three",
    "sorry",
    "sorry",
    "music",
    "repeting",
    "rare",
    "three",
    "notes",
    "going",
    "time",
    "rare",
    "even",
    "though",
    "say",
    "commodity",
    "really",
    "commodity",
    "days",
    "mean",
    "even",
    "though",
    "say",
    "storage",
    "server",
    "cost",
    "cheap",
    "days",
    "three",
    "machines",
    "time",
    "going",
    "uh",
    "rare",
    "possibility",
    "king",
    "jn",
    "fired",
    "missile",
    "right",
    "north",
    "korea",
    "exactly",
    "nobody",
    "let",
    "say",
    "king",
    "joh",
    "fired",
    "chennai",
    "okay",
    "chennai",
    "hado",
    "cluster",
    "gone",
    "right",
    "run",
    "query",
    "company",
    "survive",
    "right",
    "disaster",
    "recovery",
    "typically",
    "set",
    "dr",
    "disaster",
    "recovery",
    "disaster",
    "recovery",
    "set",
    "uh",
    "company",
    "called",
    "van",
    "disco",
    "disco",
    "disco",
    "dance",
    "van",
    "disco",
    "interesting",
    "company",
    "van",
    "disco",
    "one",
    "popular",
    "companies",
    "set",
    "disaster",
    "recovery",
    "hadu",
    "disaster",
    "recovery",
    "like",
    "traditional",
    "systems",
    "meaning",
    "100",
    "hadu",
    "cluster",
    "wo",
    "set",
    "100",
    "backup",
    "cluster",
    "right",
    "waste",
    "money",
    "rare",
    "king",
    "jong",
    "fire",
    "missile",
    "right",
    "fires",
    "god",
    "saves",
    "right",
    "even",
    "hadoop",
    "cluster",
    "classify",
    "data",
    "storing",
    "100",
    "terab",
    "data",
    "probably",
    "100",
    "terabyte",
    "important",
    "right",
    "lot",
    "data",
    "like",
    "archiv",
    "100",
    "terab",
    "data",
    "identify",
    "probably",
    "10",
    "terabyte",
    "critical",
    "periodically",
    "back",
    "data",
    "dr",
    "center",
    "set",
    "100",
    "node",
    "cluster",
    "noa",
    "back",
    "cluster",
    "use",
    "tool",
    "called",
    "distributed",
    "copy",
    "disc",
    "cp",
    "hadu",
    "synchronize",
    "two",
    "clusters",
    "primary",
    "backup",
    "dist",
    "cp",
    "called",
    "distributed",
    "copy",
    "lying",
    "people",
    "say",
    "making",
    "stories",
    "show",
    "um",
    "yeah",
    "excellent",
    "question",
    "come",
    "give",
    "moment",
    "go",
    "van",
    "disco",
    "interesting",
    "company",
    "right",
    "like",
    "name",
    "named",
    "company",
    "know",
    "van",
    "disco",
    "van",
    "disco",
    "hado",
    "company",
    "way",
    "provide",
    "replication",
    "solutions",
    "uh",
    "solutions",
    "backup",
    "disaster",
    "recovery",
    "go",
    "disaster",
    "recovery",
    "internet",
    "slow",
    "right",
    "see",
    "even",
    "opening",
    "page",
    "yeah",
    "disaster",
    "recovery",
    "h",
    "100",
    "per",
    "hardo",
    "availability",
    "right",
    "see",
    "talking",
    "used",
    "vanis",
    "one",
    "projects",
    "know",
    "usually",
    "people",
    "aware",
    "things",
    "working",
    "hadoop",
    "administrator",
    "also",
    "time",
    "know",
    "like",
    "real",
    "times",
    "things",
    "okay",
    "sure",
    "sorry",
    "dp",
    "tool",
    "want",
    "yeah",
    "thing",
    "yeah",
    "managing",
    "difficult",
    "actually",
    "dp",
    "run",
    "know",
    "even",
    "cloud",
    "hot",
    "work",
    "supports",
    "lot",
    "things",
    "consider",
    "running",
    "cp",
    "first",
    "resource",
    "utilization",
    "problem",
    "keep",
    "reading",
    "writing",
    "data",
    "second",
    "fails",
    "know",
    "manage",
    "dp",
    "fail",
    "many",
    "points",
    "point",
    "time",
    "implementing",
    "vandas",
    "co",
    "use",
    "cp",
    "ways",
    "handle",
    "actually",
    "mean",
    "ensure",
    "replication",
    "happens",
    "properly",
    "bother",
    "suggesting",
    "must",
    "use",
    "something",
    "want",
    "dis",
    "even",
    "claer",
    "provides",
    "tool",
    "disaster",
    "recovery",
    "hadoop",
    "actually",
    "right",
    "question",
    "block",
    "size",
    "64",
    "mb",
    "4kb",
    "block",
    "size",
    "4kb",
    "right",
    "right",
    "maybe",
    "know",
    "asking",
    "seek",
    "time",
    "seek",
    "time",
    "storing",
    "4kb",
    "4kb",
    "ca",
    "read",
    "read",
    "one",
    "tb",
    "want",
    "huh",
    "exactly",
    "showed",
    "math",
    "weak",
    "64",
    "3",
    "198",
    "know",
    "showed",
    "example",
    "real",
    "production",
    "cluster",
    "block",
    "size",
    "128",
    "mb",
    "64",
    "know",
    "three",
    "times",
    "128",
    "mb",
    "honest",
    "okay",
    "real",
    "production",
    "cluster",
    "block",
    "size",
    "128",
    "mb",
    "configurable",
    "use",
    "64",
    "also",
    "parameter",
    "show",
    "change",
    "block",
    "size",
    "okay",
    "hadoop",
    "default",
    "uh",
    "block",
    "size",
    "128",
    "mb",
    "also",
    "know",
    "also",
    "understand",
    "one",
    "thing",
    "three",
    "major",
    "releases",
    "hadoop",
    "hadoop",
    "1",
    "2",
    "3",
    "hadoop",
    "1",
    "older",
    "release",
    "nobody",
    "using",
    "days",
    "worry",
    "hadoop",
    "1",
    "available",
    "production",
    "right",
    "2013",
    "last",
    "year",
    "used",
    "longer",
    "used",
    "hadoop",
    "2",
    "using",
    "using",
    "using",
    "current",
    "edition",
    "hado",
    "3",
    "released",
    "2017",
    "december",
    "15",
    "like",
    "3",
    "months",
    "back",
    "okay",
    "let",
    "ask",
    "question",
    "storing",
    "100",
    "terab",
    "data",
    "hardo",
    "cluster",
    "much",
    "storage",
    "need",
    "true",
    "replication",
    "considering",
    "300",
    "right",
    "three",
    "good",
    "bad",
    "bad",
    "thoughts",
    "mean",
    "see",
    "trainings",
    "always",
    "interaction",
    "right",
    "might",
    "know",
    "lot",
    "things",
    "might",
    "know",
    "lot",
    "things",
    "know",
    "right",
    "right",
    "thought",
    "fine",
    "two",
    "fine",
    "asking",
    "mean",
    "yes",
    "question",
    "asking",
    "think",
    "multiple",
    "approaches",
    "one",
    "thing",
    "companies",
    "replication",
    "factor",
    "changed",
    "way",
    "hardcoded",
    "companies",
    "say",
    "two",
    "right",
    "get",
    "two",
    "copy",
    "data",
    "also",
    "copying",
    "data",
    "mention",
    "replication",
    "factor",
    "interesting",
    "thing",
    "copying",
    "file",
    "one",
    "tb",
    "file",
    "file",
    "afford",
    "lose",
    "say",
    "copy",
    "replication",
    "one",
    "led",
    "debate",
    "hadoop",
    "world",
    "lot",
    "people",
    "said",
    "bad",
    "actually",
    "three",
    "times",
    "replication",
    "means",
    "losing",
    "lot",
    "space",
    "hadoop",
    "3",
    "replication",
    "interesting",
    "point",
    "beta",
    "testing",
    "hado",
    "3",
    "even",
    "know",
    "completely",
    "hado",
    "3",
    "use",
    "technique",
    "called",
    "eraser",
    "encoding",
    "similar",
    "raid",
    "raid",
    "10",
    "z",
    "parity",
    "many",
    "aware",
    "parity",
    "based",
    "raid",
    "okay",
    "something",
    "called",
    "raid",
    "akay",
    "kumar",
    "movie",
    "something",
    "better",
    "okay",
    "redundant",
    "array",
    "independent",
    "discs",
    "called",
    "raid",
    "raid",
    "fall",
    "tolerance",
    "technique",
    "okay",
    "based",
    "raid",
    "formulated",
    "new",
    "strategy",
    "haduk",
    "3",
    "replicate",
    "data",
    "another",
    "technique",
    "called",
    "eraser",
    "encoding",
    "enable",
    "still",
    "recover",
    "data",
    "fails",
    "say",
    "use",
    "30",
    "additional",
    "storage",
    "meaning",
    "storing",
    "100",
    "gb",
    "need",
    "130",
    "gb",
    "better",
    "right",
    "much",
    "much",
    "better",
    "need",
    "300",
    "gb",
    "hadoop",
    "3",
    "make",
    "production",
    "soon",
    "worry",
    "right",
    "still",
    "hadoop",
    "2",
    "needs",
    "break",
    "yeah",
    "break",
    "want",
    "question",
    "want",
    "break",
    "huh",
    "oh",
    "guys",
    "interested",
    "actually",
    "thought",
    "sleep",
    "class",
    "get",
    "hado",
    "class",
    "morning",
    "class",
    "evening",
    "right",
    "stat",
    "actually",
    "wanted",
    "learn",
    "statistics",
    "good",
    "right",
    "probably",
    "come",
    "sit",
    "morning",
    "probably",
    "statistics",
    "definitely",
    "sleep",
    "100",
    "guaranteed",
    "blaming",
    "trainer",
    "okay",
    "take",
    "way",
    "statistic",
    "subject",
    "bit",
    "boring",
    "actually",
    "right",
    "today",
    "tomorrow",
    "second",
    "half",
    "coming",
    "talking",
    "sts",
    "first",
    "half",
    "learned",
    "r",
    "r",
    "programming",
    "okay",
    "okay",
    "got",
    "question",
    "huh",
    "tex",
    "file",
    "understand",
    "individually",
    "yeah",
    "know",
    "picture",
    "fct",
    "give",
    "solution",
    "processing",
    "handled",
    "uh",
    "framework",
    "something",
    "called",
    "map",
    "reduce",
    "map",
    "produce",
    "java",
    "class",
    "abstract",
    "class",
    "java",
    "mean",
    "know",
    "java",
    "class",
    "technique",
    "tell",
    "type",
    "file",
    "dealing",
    "limited",
    "types",
    "read",
    "text",
    "uh",
    "xml",
    "uh",
    "sequence",
    "file",
    "key",
    "value",
    "limited",
    "number",
    "types",
    "files",
    "good",
    "question",
    "handle",
    "unstructured",
    "data",
    "also",
    "confusing",
    "concept",
    "many",
    "people",
    "believe",
    "world",
    "big",
    "data",
    "always",
    "playing",
    "uh",
    "videos",
    "uh",
    "images",
    "actually",
    "see",
    "many",
    "people",
    "believe",
    "every",
    "day",
    "go",
    "office",
    "run",
    "facial",
    "recognition",
    "like",
    "seen",
    "mission",
    "impossible",
    "movie",
    "yes",
    "yeah",
    "got",
    "big",
    "data",
    "actually",
    "analyze",
    "images",
    "videos",
    "difficult",
    "actually",
    "analyzing",
    "video",
    "image",
    "data",
    "difficult",
    "ca",
    "directly",
    "analyze",
    "convert",
    "binary",
    "becomes",
    "binary",
    "hadoop",
    "format",
    "called",
    "sequence",
    "file",
    "rit",
    "sequence",
    "file",
    "write",
    "program",
    "analyze",
    "actually",
    "difficult",
    "job",
    "analyze",
    "kind",
    "files",
    "everybody",
    "need",
    "analyze",
    "let",
    "give",
    "realtime",
    "example",
    "working",
    "25",
    "by7",
    "right",
    "customer",
    "care",
    "company",
    "primary",
    "business",
    "customer",
    "care",
    "customer",
    "care",
    "people",
    "call",
    "right",
    "think",
    "listen",
    "customer",
    "calling",
    "yes",
    "running",
    "customer",
    "care",
    "company",
    "company",
    "giving",
    "support",
    "companies",
    "running",
    "call",
    "center",
    "call",
    "center",
    "customers",
    "call",
    "right",
    "actually",
    "listen",
    "talking",
    "let",
    "say",
    "take",
    "idea",
    "example",
    "many",
    "calls",
    "idea",
    "getting",
    "customer",
    "care",
    "really",
    "think",
    "analyze",
    "calls",
    "two",
    "things",
    "one",
    "legally",
    "possible",
    "think",
    "100",
    "sure",
    "maybe",
    "possible",
    "say",
    "calling",
    "call",
    "recorded",
    "fine",
    "fine",
    "quality",
    "accent",
    "big",
    "data",
    "analytics",
    "example",
    "angry",
    "customer",
    "spoke",
    "1",
    "hour",
    "call",
    "recorded",
    "person",
    "took",
    "call",
    "go",
    "manager",
    "next",
    "meeting",
    "manager",
    "analyze",
    "call",
    "say",
    "mistakes",
    "saying",
    "actually",
    "analyze",
    "big",
    "data",
    "analytics",
    "kind",
    "things",
    "really",
    "right",
    "analyze",
    "metadata",
    "actual",
    "data",
    "many",
    "people",
    "called",
    "duration",
    "press",
    "data",
    "actual",
    "audio",
    "data",
    "saying",
    "impossible",
    "understand",
    "speaking",
    "need",
    "voice",
    "recognition",
    "right",
    "may",
    "fail",
    "may",
    "fail",
    "100",
    "assure",
    "us",
    "customer",
    "something",
    "approximately",
    "match",
    "indian",
    "customer",
    "identify",
    "possible",
    "possible",
    "right",
    "may",
    "speak",
    "language",
    "accent",
    "right",
    "practically",
    "things",
    "possible",
    "saying",
    "analyze",
    "unstructured",
    "data",
    "doubt",
    "companies",
    "like",
    "facebook",
    "facebook",
    "actually",
    "analyze",
    "video",
    "data",
    "reasons",
    "right",
    "written",
    "complicated",
    "algorithms",
    "read",
    "convert",
    "binary",
    "going",
    "work",
    "regular",
    "project",
    "see",
    "structure",
    "data",
    "csv",
    "files",
    "json",
    "data",
    "normal",
    "text",
    "data",
    "storing",
    "also",
    "need",
    "class",
    "read",
    "type",
    "files",
    "question",
    "right",
    "yeah",
    "sorry",
    "go",
    "huh",
    "oh",
    "uh",
    "excellent",
    "question",
    "also",
    "missed",
    "another",
    "question",
    "side",
    "question",
    "yeah",
    "uh",
    "think",
    "take",
    "break",
    "need",
    "time",
    "explain",
    "anyway",
    "say",
    "hadoop",
    "know",
    "hdfs",
    "yan",
    "map",
    "ruce",
    "hadoop",
    "lot",
    "tools",
    "top",
    "need",
    "least",
    "rough",
    "idea",
    "tools",
    "hado",
    "ecosystem",
    "whole",
    "stack",
    "right",
    "talk",
    "complete",
    "hdfs",
    "discussion",
    "go",
    "ecosystem",
    "stack",
    "ecosystem",
    "stack",
    "lot",
    "questions",
    "work",
    "least",
    "basic",
    "idea",
    "take",
    "icc",
    "scenario",
    "tell",
    "mve",
    "get",
    "better",
    "idea",
    "probably",
    "right",
    "uh",
    "yeah",
    "question",
    "side",
    "long",
    "sorry",
    "notice",
    "uh",
    "yeah",
    "excellent",
    "question",
    "question",
    "one",
    "machine",
    "totally",
    "gone",
    "plug",
    "additional",
    "machine",
    "hadoop",
    "automatically",
    "take",
    "care",
    "copying",
    "data",
    "pro",
    "problem",
    "point",
    "called",
    "commissioning",
    "add",
    "new",
    "node",
    "let",
    "say",
    "simply",
    "adding",
    "new",
    "node",
    "let",
    "say",
    "existing",
    "setup",
    "add",
    "new",
    "node",
    "node",
    "empty",
    "something",
    "called",
    "balancer",
    "utility",
    "called",
    "balancer",
    "run",
    "balancer",
    "hadoop",
    "analyze",
    "understand",
    "underutilized",
    "utilized",
    "move",
    "blocks",
    "run",
    "balancer",
    "keep",
    "new",
    "machine",
    "data",
    "balanced",
    "run",
    "something",
    "called",
    "balancer",
    "manage",
    "another",
    "excellent",
    "question",
    "replication",
    "work",
    "replication",
    "works",
    "interesting",
    "way",
    "writing",
    "data",
    "first",
    "block",
    "goes",
    "b1",
    "right",
    "guy",
    "tell",
    "guy",
    "get",
    "copy",
    "guy",
    "get",
    "copy",
    "guy",
    "tell",
    "guy",
    "get",
    "copy",
    "third",
    "copy",
    "got",
    "finally",
    "send",
    "acknowledgement",
    "get",
    "acknowledgement",
    "block",
    "written",
    "called",
    "pipeline",
    "right",
    "pushing",
    "everything",
    "time",
    "pipeline",
    "writes",
    "gets",
    "acknowledgement",
    "one",
    "question",
    "uh",
    "yeah",
    "sorry",
    "yeah",
    "uh",
    "communicated",
    "hadoop",
    "copy",
    "operation",
    "talk",
    "name",
    "node",
    "name",
    "node",
    "give",
    "list",
    "data",
    "nodes",
    "chain",
    "data",
    "nodes",
    "get",
    "replica",
    "primary",
    "replica",
    "information",
    "pass",
    "maxim",
    "maximum",
    "storage",
    "uh",
    "excellent",
    "question",
    "name",
    "actually",
    "stores",
    "metadata",
    "metadata",
    "ram",
    "name",
    "node",
    "running",
    "metadata",
    "served",
    "ram",
    "actually",
    "also",
    "persists",
    "hard",
    "disk",
    "case",
    "crashes",
    "gets",
    "uh",
    "1",
    "trillion",
    "files",
    "64",
    "gb",
    "ram",
    "required",
    "actually",
    "calculate",
    "ram",
    "size",
    "hard",
    "disk",
    "size",
    "uh",
    "one",
    "trillion",
    "file",
    "file",
    "size",
    "anything",
    "talking",
    "file",
    "size",
    "single",
    "file",
    "256",
    "kb",
    "metadata",
    "created",
    "one",
    "file",
    "means",
    "many",
    "blocks",
    "replica",
    "metadata",
    "right",
    "64",
    "gb",
    "handle",
    "1",
    "trillion",
    "files",
    "hard",
    "cluster",
    "accordingly",
    "increase",
    "ram",
    "size",
    "hard",
    "dis",
    "size",
    "fine",
    "need",
    "much",
    "hard",
    "size",
    "kb",
    "per",
    "file",
    "exactly",
    "increase",
    "ram",
    "machine",
    "right",
    "name",
    "node",
    "commodity",
    "hardware",
    "important",
    "point",
    "name",
    "node",
    "costly",
    "machine",
    "want",
    "vol",
    "bus",
    "right",
    "want",
    "crash",
    "unnecessarily",
    "exactly",
    "let",
    "say",
    "machine",
    "good",
    "question",
    "machine",
    "b3",
    "b1",
    "gone",
    "automatically",
    "create",
    "one",
    "b",
    "b1",
    "somewhere",
    "machine",
    "comes",
    "back",
    "online",
    "delete",
    "always",
    "maintain",
    "copy",
    "three",
    "always",
    "replica",
    "three",
    "four",
    "two",
    "automatically",
    "handled",
    "worry",
    "delete",
    "recreate",
    "wants",
    "consider",
    "dat",
    "name",
    "take",
    "take",
    "data",
    "locality",
    "near",
    "near",
    "considerations",
    "met",
    "okay",
    "come",
    "moment",
    "metad",
    "data",
    "talk",
    "yeah",
    "one",
    "first",
    "thing",
    "told",
    "lo",
    "si",
    "64",
    "128",
    "actual",
    "okay",
    "let",
    "understand",
    "hdfs",
    "isct",
    "system",
    "top",
    "lem",
    "correct",
    "going",
    "installing",
    "hdfs",
    "even",
    "though",
    "internally",
    "linux",
    "file",
    "system",
    "talk",
    "linux",
    "file",
    "system",
    "64",
    "mv",
    "chop",
    "64",
    "mb",
    "hado",
    "internally",
    "linux",
    "4k",
    "entire",
    "64",
    "mb",
    "one",
    "point",
    "fragment",
    "getting",
    "point",
    "right",
    "simply",
    "dumping",
    "64",
    "mb",
    "linux",
    "divide",
    "4",
    "kb",
    "fragment",
    "many",
    "many",
    "places",
    "continuous",
    "read",
    "wr",
    "fast",
    "uh",
    "questions",
    "ah",
    "balancer",
    "ideally",
    "run",
    "administrator",
    "hadoop",
    "administrator",
    "manually",
    "command",
    "called",
    "hado",
    "balancer",
    "run",
    "gi",
    "also",
    "run",
    "uh",
    "show",
    "tomorrow",
    "guys",
    "remind",
    "show",
    "may",
    "able",
    "run",
    "balancer",
    "show",
    "forget",
    "right",
    "tomorrow",
    "show",
    "hado",
    "cluster",
    "show",
    "things",
    "actually",
    "h",
    "balancer",
    "part",
    "hadoop",
    "installation",
    "hdfs",
    "hdfs",
    "say",
    "run",
    "balancer",
    "normally",
    "run",
    "balancer",
    "pe",
    "covers",
    "like",
    "daytime",
    "run",
    "move",
    "blocks",
    "processing",
    "get",
    "interrupted",
    "w",
    "interrupted",
    "slow",
    "let",
    "say",
    "night",
    "midnight",
    "run",
    "balancer",
    "make",
    "sure",
    "blocks",
    "evenly",
    "distributed",
    "huh",
    "block",
    "size",
    "also",
    "good",
    "question",
    "uh",
    "block",
    "size",
    "logic",
    "okay",
    "question",
    "200",
    "mb",
    "file",
    "many",
    "blocks",
    "two",
    "uh",
    "let",
    "take",
    "64",
    "mb",
    "uh",
    "200",
    "mb",
    "file",
    "size",
    "fourth",
    "block",
    "many",
    "say",
    "fourth",
    "block",
    "64",
    "mb",
    "say",
    "yes",
    "many",
    "say",
    "fourth",
    "block",
    "12",
    "mb",
    "8",
    "mb",
    "right",
    "less",
    "8",
    "mb",
    "128",
    "mb",
    "block",
    "size",
    "valid",
    "file",
    "size",
    "block",
    "size",
    "file",
    "divide",
    "ultimately",
    "end",
    "10",
    "mb",
    "8",
    "mb",
    "keep",
    "otherwise",
    "storing",
    "128",
    "mb",
    "piece",
    "remaining",
    "data",
    "unnecessarily",
    "logic",
    "actually",
    "logic",
    "even",
    "physical",
    "division",
    "happens",
    "okay",
    "logically",
    "keep",
    "block",
    "12",
    "mb",
    "12",
    "mb",
    "data",
    "left",
    "got",
    "like",
    "physically",
    "marking",
    "space",
    "filling",
    "much",
    "happen",
    "like",
    "said",
    "contact",
    "system",
    "storing",
    "contacts",
    "right",
    "storing",
    "file",
    "say",
    "200",
    "mb",
    "knows",
    "four",
    "blocks",
    "need",
    "created",
    "right",
    "fourth",
    "block",
    "size",
    "say",
    "12",
    "mb",
    "need",
    "12",
    "mb",
    "data",
    "12",
    "also",
    "sequence",
    "exactly",
    "sequence",
    "otherwise",
    "remaining",
    "space",
    "wasted",
    "right",
    "every",
    "file",
    "wasted",
    "space",
    "wasting",
    "much",
    "space",
    "store",
    "one",
    "trillion",
    "file",
    "much",
    "wasted",
    "think",
    "think",
    "lot",
    "file",
    "size",
    "right",
    "required",
    "actually",
    "logic",
    "internally",
    "built",
    "hdf",
    "block",
    "size",
    "valid",
    "long",
    "file",
    "size",
    "bigger",
    "block",
    "size",
    "chop",
    "chop",
    "chop",
    "last",
    "10",
    "mb",
    "10",
    "mb",
    "128",
    "mb",
    "64",
    "mb",
    "right",
    "increase",
    "exactly",
    "append",
    "block",
    "size",
    "increase",
    "good",
    "question",
    "200",
    "mb",
    "file",
    "appending",
    "data",
    "right",
    "increase",
    "provide",
    "room",
    "actually",
    "last",
    "block",
    "8",
    "mb",
    "keep",
    "appending",
    "data",
    "block",
    "become",
    "128",
    "mb",
    "keep",
    "aable",
    "linux",
    "creating",
    "block",
    "block",
    "size",
    "small",
    "say",
    "gives",
    "room",
    "creating",
    "next",
    "block",
    "keep",
    "immediately",
    "next",
    "hard",
    "disk",
    "big",
    "immediately",
    "create",
    "right",
    "cases",
    "happen",
    "happen",
    "rarely",
    "probably",
    "may",
    "work",
    "rarely",
    "okay",
    "almost",
    "impossible",
    "cases",
    "get",
    "data",
    "yeah",
    "processing",
    "told",
    "come",
    "back",
    "putting",
    "file",
    "one",
    "big",
    "file",
    "want",
    "process",
    "machine",
    "process",
    "file",
    "right",
    "splitting",
    "three",
    "machines",
    "parall",
    "process",
    "data",
    "right",
    "processing",
    "faster",
    "time",
    "run",
    "processing",
    "another",
    "misconcept",
    "people",
    "hadoop",
    "cluster",
    "100",
    "tb",
    "data",
    "100",
    "de",
    "developers",
    "time",
    "say",
    "run",
    "work",
    "like",
    "getting",
    "point",
    "lot",
    "data",
    "hard",
    "cluster",
    "wrote",
    "program",
    "wrote",
    "program",
    "wrote",
    "program",
    "wo",
    "come",
    "office",
    "exactly",
    "say",
    "run",
    "program",
    "time",
    "work",
    "like",
    "may",
    "lot",
    "data",
    "people",
    "would",
    "written",
    "programs",
    "schedule",
    "program",
    "scheduling",
    "comes",
    "talk",
    "working",
    "hadoop",
    "cluster",
    "resources",
    "important",
    "right",
    "machine",
    "processor",
    "hard",
    "disk",
    "network",
    "right",
    "machine",
    "processing",
    "files",
    "whatever",
    "written",
    "tool",
    "called",
    "uzi",
    "apache",
    "uzi",
    "uzi",
    "tool",
    "actually",
    "comes",
    "along",
    "hadoop",
    "install",
    "tool",
    "uzi",
    "scheduling",
    "program",
    "wrote",
    "spark",
    "program",
    "analyze",
    "100",
    "tb",
    "data",
    "immediately",
    "submit",
    "program",
    "probably",
    "program",
    "work",
    "already",
    "somebody",
    "analyzing",
    "data",
    "uh",
    "space",
    "fit",
    "things",
    "right",
    "write",
    "uzi",
    "contact",
    "hado",
    "pmin",
    "say",
    "want",
    "analyze",
    "100",
    "tb",
    "data",
    "run",
    "say",
    "tomorrow",
    "okay",
    "submit",
    "analysis",
    "batch",
    "real",
    "time",
    "real",
    "time",
    "different",
    "case",
    "bat",
    "jobs",
    "schedule",
    "bya",
    "uzi",
    "also",
    "something",
    "called",
    "q",
    "hadoop",
    "example",
    "working",
    "developer",
    "tester",
    "access",
    "cluster",
    "course",
    "testing",
    "cluster",
    "different",
    "let",
    "say",
    "researcher",
    "developer",
    "data",
    "scientist",
    "access",
    "hado",
    "cluster",
    "probably",
    "developer",
    "need",
    "resources",
    "okay",
    "probably",
    "need",
    "less",
    "resources",
    "create",
    "something",
    "called",
    "q",
    "okay",
    "something",
    "called",
    "q",
    "create",
    "que",
    "allocate",
    "resources",
    "coming",
    "developer",
    "team",
    "get",
    "30",
    "resources",
    "coming",
    "team",
    "get",
    "40",
    "percentage",
    "right",
    "otherwise",
    "utilize",
    "resource",
    "right",
    "write",
    "program",
    "make",
    "cluster",
    "mine",
    "possible",
    "hado",
    "cluster",
    "management",
    "entirely",
    "different",
    "run",
    "jobs",
    "different",
    "answer",
    "question",
    "dividing",
    "use",
    "parallelism",
    "otherwise",
    "single",
    "machine",
    "process",
    "everything",
    "right",
    "quota",
    "size",
    "different",
    "ess",
    "q",
    "right",
    "q",
    "means",
    "multiple",
    "teams",
    "team",
    "get",
    "que",
    "submit",
    "program",
    "submit",
    "que",
    "direct",
    "ly",
    "submit",
    "cluster",
    "let",
    "say",
    "uh",
    "example",
    "take",
    "terms",
    "processing",
    "queuing",
    "terms",
    "resources",
    "say",
    "example",
    "haduk",
    "cluster",
    "huh",
    "10",
    "data",
    "nodes",
    "data",
    "node",
    "let",
    "say",
    "10",
    "gb",
    "ram",
    "right",
    "total",
    "100",
    "gb",
    "ram",
    "cluster",
    "right",
    "h",
    "let",
    "say",
    "10",
    "data",
    "nod",
    "four",
    "core",
    "processor",
    "get",
    "40",
    "core",
    "processor",
    "right",
    "somebody",
    "want",
    "get",
    "resources",
    "program",
    "getting",
    "nobody",
    "else",
    "run",
    "program",
    "cluster",
    "like",
    "create",
    "four",
    "qes",
    "let",
    "say",
    "two",
    "cues",
    "que",
    "say",
    "60",
    "resource",
    "say",
    "30",
    "say",
    "10",
    "per",
    "developer",
    "cu",
    "get",
    "60",
    "resources",
    "60",
    "gb",
    "ram",
    "30",
    "uh",
    "60",
    "percentage",
    "right",
    "24",
    "24",
    "core",
    "processor",
    "never",
    "get",
    "24",
    "multiple",
    "ways",
    "scheduling",
    "normal",
    "scheduling",
    "get",
    "maximum",
    "60",
    "q",
    "empty",
    "claim",
    "resources",
    "q",
    "actually",
    "java",
    "class",
    "create",
    "something",
    "called",
    "schedulers",
    "hado",
    "something",
    "called",
    "capacity",
    "eder",
    "fair",
    "scheduler",
    "scheduler",
    "called",
    "que",
    "creating",
    "something",
    "called",
    "fair",
    "scheduler",
    "within",
    "fair",
    "scheduler",
    "allow",
    "create",
    "cues",
    "say",
    "create",
    "three",
    "cues",
    "whenever",
    "somebody",
    "submitting",
    "program",
    "mention",
    "q",
    "name",
    "also",
    "otherwise",
    "program",
    "run",
    "okay",
    "music",
    "okay",
    "correct",
    "data",
    "might",
    "different",
    "talking",
    "data",
    "data",
    "get",
    "data",
    "talking",
    "ram",
    "processing",
    "power",
    "okay",
    "data",
    "available",
    "okay",
    "location",
    "r",
    "h",
    "running",
    "program",
    "come",
    "map",
    "ruce",
    "tomorrow",
    "exp",
    "h",
    "running",
    "program",
    "need",
    "ram",
    "processor",
    "run",
    "program",
    "right",
    "run",
    "program",
    "need",
    "ram",
    "processing",
    "power",
    "cluster",
    "give",
    "right",
    "limit",
    "people",
    "based",
    "q",
    "much",
    "resource",
    "get",
    "possible",
    "called",
    "scheduling",
    "depend",
    "upon",
    "theu",
    "ah",
    "submit",
    "queue",
    "within",
    "inside",
    "queue",
    "write",
    "policy",
    "default",
    "policy",
    "fifo",
    "first",
    "first",
    "right",
    "apart",
    "fifo",
    "also",
    "priority",
    "ities",
    "things",
    "write",
    "policy",
    "whatever",
    "time",
    "cluster",
    "one",
    "job",
    "run",
    "time",
    "three",
    "jobs",
    "run",
    "three",
    "qes",
    "people",
    "submit",
    "right",
    "three",
    "qes",
    "three",
    "jobs",
    "time",
    "three",
    "example",
    "q",
    "let",
    "say",
    "uh",
    "60",
    "gb",
    "ram",
    "right",
    "need",
    "20",
    "gb",
    "ram",
    "also",
    "use",
    "20",
    "right",
    "three",
    "people",
    "depending",
    "much",
    "want",
    "process",
    "huh",
    "much",
    "resource",
    "want",
    "tell",
    "calculate",
    "none",
    "important",
    "admin",
    "activities",
    "none",
    "discussed",
    "actually",
    "related",
    "course",
    "may",
    "make",
    "sense",
    "start",
    "working",
    "see",
    "learning",
    "one",
    "thing",
    "okay",
    "want",
    "learn",
    "learn",
    "map",
    "produce",
    "run",
    "programs",
    "go",
    "home",
    "actually",
    "start",
    "working",
    "uh",
    "know",
    "things",
    "actually",
    "used",
    "real",
    "life",
    "want",
    "know",
    "h",
    "worry",
    "scheduling",
    "uzi",
    "nothing",
    "comes",
    "uh",
    "lab",
    "part",
    "lab",
    "part",
    "actually",
    "hdfs",
    "load",
    "data",
    "read",
    "data",
    "map",
    "reduce",
    "hi",
    "none",
    "actually",
    "comes",
    "show",
    "least",
    "cloud",
    "manager",
    "ui",
    "uh",
    "thing",
    "balancer",
    "name",
    "node",
    "data",
    "node",
    "things",
    "see",
    "since",
    "want",
    "look",
    "clear",
    "hdfs",
    "think",
    "least",
    "say",
    "uh",
    "basic",
    "level",
    "clear",
    "probably",
    "everything",
    "yeah",
    "get",
    "records",
    "means",
    "huh",
    "text",
    "file",
    "flat",
    "file",
    "file",
    "select",
    "default",
    "like",
    "said",
    "storing",
    "something",
    "hdfs",
    "hdfs",
    "care",
    "format",
    "store",
    "image",
    "store",
    "image",
    "jpg",
    "make",
    "sense",
    "reading",
    "data",
    "like",
    "rdbms",
    "one",
    "thing",
    "like",
    "rdbms",
    "hdfs",
    "file",
    "system",
    "dumb",
    "say",
    "example",
    "windows",
    "machine",
    "store",
    "mp3",
    "file",
    "open",
    "notepad",
    "know",
    "pass",
    "exactly",
    "store",
    "mp3",
    "file",
    "hado",
    "want",
    "process",
    "write",
    "logic",
    "process",
    "actually",
    "queries",
    "map",
    "reduces",
    "query",
    "map",
    "reduces",
    "programming",
    "language",
    "queries",
    "talking",
    "hi",
    "hi",
    "data",
    "warehouse",
    "hadu",
    "hi",
    "accept",
    "normal",
    "sql",
    "queries",
    "whatever",
    "sql",
    "data",
    "yeah",
    "create",
    "table",
    "data",
    "exactly",
    "hive",
    "like",
    "data",
    "htfs",
    "create",
    "table",
    "give",
    "schema",
    "looks",
    "like",
    "adm",
    "table",
    "run",
    "queries",
    "rbms",
    "actually",
    "data",
    "running",
    "l",
    "exactly",
    "csv",
    "text",
    "file",
    "regular",
    "files",
    "save",
    "yeah",
    "huh",
    "show",
    "hive",
    "store",
    "process",
    "data",
    "right",
    "yes",
    "hive",
    "something",
    "called",
    "called",
    "indexing",
    "efficient",
    "uh",
    "indexing",
    "really",
    "work",
    "well",
    "data",
    "managed",
    "rdbms",
    "system",
    "data",
    "warehousing",
    "system",
    "indexing",
    "data",
    "hi",
    "data",
    "actually",
    "managed",
    "hdfs",
    "right",
    "get",
    "performance",
    "improvements",
    "like",
    "rdbms",
    "never",
    "compare",
    "hadu",
    "rdbms",
    "compare",
    "data",
    "warehouse",
    "transactional",
    "right",
    "equal",
    "data",
    "warehouse",
    "say",
    "get",
    "asking",
    "copy",
    "file",
    "create",
    "record",
    "saying",
    "talking",
    "transactional",
    "right",
    "create",
    "record",
    "see",
    "big",
    "dat",
    "platform",
    "dump",
    "terabytes",
    "file",
    "creating",
    "record",
    "rarely",
    "edit",
    "saying",
    "needle",
    "haack",
    "problem",
    "right",
    "rarely",
    "say",
    "want",
    "create",
    "new",
    "record",
    "adding",
    "files",
    "continuous",
    "streams",
    "huge",
    "size",
    "afford",
    "lose",
    "afford",
    "lose",
    "data",
    "talk",
    "ecosystem",
    "side",
    "okay",
    "huh",
    "h",
    "lot",
    "companies",
    "actually",
    "run",
    "using",
    "virtual",
    "machines",
    "possible",
    "performance",
    "always",
    "uh",
    "depend",
    "vms",
    "mean",
    "vms",
    "ultimately",
    "uh",
    "use",
    "uh",
    "bare",
    "metal",
    "virtual",
    "machine",
    "guarantee",
    "uh",
    "two",
    "types",
    "vms",
    "right",
    "something",
    "called",
    "bare",
    "metal",
    "like",
    "hyperv",
    "get",
    "good",
    "performance",
    "vmware",
    "bare",
    "metal",
    "uh",
    "performance",
    "compromised",
    "go",
    "one",
    "layer",
    "talk",
    "storage",
    "configured",
    "talk",
    "something",
    "called",
    "hadoop",
    "ecosystem",
    "discuss",
    "music",
    "located",
    "correct",
    "uh",
    "java",
    "call",
    "actually",
    "hadoop",
    "actually",
    "written",
    "java",
    "client",
    "also",
    "java",
    "everything",
    "java",
    "method",
    "actually",
    "invoking",
    "java",
    "object",
    "fs",
    "read",
    "object",
    "call",
    "object",
    "get",
    "metadata",
    "give",
    "client",
    "client",
    "keep",
    "memory",
    "go",
    "read",
    "one",
    "one",
    "wherever",
    "want",
    "okay",
    "explain",
    "ecosystem",
    "come",
    "back",
    "questions",
    "know",
    "lot",
    "questions",
    "good",
    "finish",
    "couple",
    "topics",
    "well",
    "right",
    "uh",
    "taking",
    "ic",
    "use",
    "case",
    "guys",
    "understand",
    "better",
    "traditionally",
    "something",
    "called",
    "etl",
    "bring",
    "data",
    "right",
    "hadu",
    "writing",
    "hdfs",
    "hdfs",
    "file",
    "system",
    "hdfs",
    "world",
    "hado",
    "rarely",
    "etl",
    "etl",
    "means",
    "reading",
    "data",
    "transforming",
    "data",
    "dumping",
    "data",
    "getting",
    "big",
    "data",
    "transforming",
    "fly",
    "difficult",
    "actually",
    "world",
    "hadoop",
    "something",
    "called",
    "elt",
    "instead",
    "etl",
    "something",
    "called",
    "elt",
    "extract",
    "load",
    "transform",
    "first",
    "thing",
    "extraction",
    "get",
    "dat",
    "hadoop",
    "system",
    "multiple",
    "tools",
    "first",
    "structured",
    "data",
    "say",
    "data",
    "oracle",
    "data",
    "mysql",
    "structured",
    "data",
    "understands",
    "sql",
    "tool",
    "called",
    "scoop",
    "tool",
    "called",
    "scoop",
    "scoop",
    "get",
    "data",
    "hdfs",
    "huh",
    "scoop",
    "etl",
    "tool",
    "ca",
    "call",
    "etl",
    "tool",
    "scoop",
    "used",
    "bring",
    "data",
    "sql",
    "system",
    "hadoop",
    "transfer",
    "data",
    "exter",
    "part",
    "hadoop",
    "ecosystem",
    "going",
    "happen",
    "say",
    "want",
    "hadoop",
    "claer",
    "claer",
    "give",
    "hadoop",
    "top",
    "give",
    "tools",
    "explaining",
    "scoop",
    "part",
    "hadoop",
    "ecosystem",
    "scoop",
    "used",
    "bring",
    "data",
    "sql",
    "stores",
    "take",
    "let",
    "say",
    "oracle",
    "take",
    "mysql",
    "take",
    "flat",
    "file",
    "json",
    "nothing",
    "nothing",
    "sql",
    "places",
    "sql",
    "understood",
    "ica",
    "use",
    "case",
    "got",
    "data",
    "crm",
    "scoop",
    "okay",
    "scoop",
    "apache",
    "tool",
    "light",
    "white",
    "tool",
    "faced",
    "problem",
    "wanted",
    "discuss",
    "let",
    "ask",
    "ask",
    "question",
    "ic",
    "banks",
    "core",
    "banking",
    "oracle",
    "database",
    "really",
    "think",
    "allow",
    "touch",
    "get",
    "data",
    "meaning",
    "copy",
    "copy",
    "replication",
    "tools",
    "replicate",
    "mean",
    "excellent",
    "cdc",
    "change",
    "data",
    "capture",
    "face",
    "problem",
    "golden",
    "gate",
    "oracle",
    "solution",
    "called",
    "golden",
    "gate",
    "cdc",
    "change",
    "data",
    "capture",
    "meaning",
    "allow",
    "touch",
    "right",
    "replica",
    "solution",
    "write",
    "replica",
    "db",
    "actually",
    "log",
    "file",
    "capture",
    "get",
    "wo",
    "allow",
    "directly",
    "touch",
    "oracle",
    "db",
    "anyway",
    "called",
    "c",
    "cdc",
    "change",
    "data",
    "capture",
    "delta",
    "give",
    "original",
    "data",
    "backup",
    "data",
    "cdc",
    "capture",
    "changes",
    "database",
    "got",
    "cdc",
    "write",
    "java",
    "code",
    "get",
    "updates",
    "dumbed",
    "get",
    "okay",
    "cdc",
    "discuss",
    "later",
    "need",
    "spend",
    "half",
    "hour",
    "explain",
    "understand",
    "changes",
    "captured",
    "anyway",
    "related",
    "uh",
    "know",
    "big",
    "data",
    "platforms",
    "business",
    "scenario",
    "yes",
    "yes",
    "yes",
    "yes",
    "course",
    "going",
    "vi",
    "exactly",
    "exactly",
    "discuss",
    "cdc",
    "detail",
    "take",
    "another",
    "half",
    "hour",
    "want",
    "spend",
    "half",
    "hour",
    "related",
    "syllabus",
    "anyway",
    "cdc",
    "even",
    "cdc",
    "teams",
    "even",
    "aware",
    "technique",
    "ah",
    "change",
    "data",
    "capture",
    "technique",
    "whatever",
    "changes",
    "happening",
    "capture",
    "dump",
    "somewhere",
    "read",
    "database",
    "exactly",
    "read",
    "saying",
    "use",
    "cdc",
    "get",
    "data",
    "golden",
    "gate",
    "product",
    "oracle",
    "uses",
    "cdc",
    "anyway",
    "use",
    "scoop",
    "get",
    "right",
    "comes",
    "place",
    "lot",
    "debate",
    "actually",
    "okay",
    "uh",
    "real",
    "question",
    "okay",
    "know",
    "get",
    "data",
    "scool",
    "right",
    "data",
    "right",
    "want",
    "get",
    "type",
    "data",
    "multiple",
    "ways",
    "get",
    "okay",
    "one",
    "popular",
    "used",
    "tool",
    "called",
    "flume",
    "apache",
    "flume",
    "flume",
    "point",
    "delivery",
    "tool",
    "flume",
    "takes",
    "data",
    "source",
    "give",
    "destination",
    "source",
    "anywhere",
    "normally",
    "flume",
    "used",
    "unstructured",
    "data",
    "say",
    "example",
    "uh",
    "folder",
    "getting",
    "json",
    "data",
    "xml",
    "data",
    "flat",
    "files",
    "create",
    "something",
    "called",
    "flume",
    "agent",
    "guy",
    "keep",
    "polling",
    "folder",
    "whenever",
    "new",
    "data",
    "guy",
    "read",
    "send",
    "hdfs",
    "flume",
    "real",
    "interesting",
    "thing",
    "flume",
    "two",
    "things",
    "one",
    "thing",
    "simply",
    "read",
    "data",
    "let",
    "say",
    "folder",
    "okay",
    "simply",
    "read",
    "data",
    "say",
    "send",
    "hado",
    "fine",
    "icsi",
    "bank",
    "also",
    "another",
    "requirement",
    "simply",
    "collecting",
    "data",
    "dumping",
    "hado",
    "want",
    "okay",
    "wanted",
    "requirement",
    "used",
    "happen",
    "flume",
    "capturing",
    "data",
    "okay",
    "data",
    "getting",
    "generated",
    "form",
    "json",
    "social",
    "media",
    "somebody",
    "clicking",
    "one",
    "products",
    "wanted",
    "send",
    "sms",
    "real",
    "time",
    "json",
    "data",
    "keep",
    "coming",
    "keep",
    "getting",
    "flume",
    "directly",
    "dump",
    "hdfs",
    "useless",
    "want",
    "analyze",
    "data",
    "whenever",
    "data",
    "getting",
    "generated",
    "guy",
    "comes",
    "picture",
    "kafka",
    "kafka",
    "difference",
    "flume",
    "kafka",
    "okay",
    "understand",
    "flume",
    "like",
    "point",
    "point",
    "delivery",
    "give",
    "data",
    "dump",
    "flume",
    "anything",
    "flume",
    "store",
    "data",
    "anywhere",
    "meaning",
    "take",
    "data",
    "temporarily",
    "may",
    "store",
    "immediately",
    "push",
    "data",
    "kafka",
    "message",
    "q",
    "anybody",
    "aware",
    "message",
    "cues",
    "kafka",
    "message",
    "queue",
    "big",
    "data",
    "world",
    "meaning",
    "guy",
    "guy",
    "running",
    "cluster",
    "hadoop",
    "cluster",
    "guy",
    "run",
    "cluster",
    "like",
    "30",
    "machine",
    "40",
    "machine",
    "guy",
    "flume",
    "send",
    "data",
    "guy",
    "kafka",
    "kafka",
    "cluster",
    "store",
    "data",
    "default",
    "store",
    "seven",
    "days",
    "default",
    "okay",
    "anybody",
    "go",
    "kafka",
    "get",
    "data",
    "publish",
    "subscribe",
    "system",
    "call",
    "advantage",
    "store",
    "data",
    "multiple",
    "people",
    "ask",
    "flu",
    "point",
    "point",
    "flu",
    "gets",
    "data",
    "dumps",
    "data",
    "yeah",
    "used",
    "okay",
    "get",
    "got",
    "data",
    "kafka",
    "kafka",
    "push",
    "hdfs",
    "one",
    "way",
    "getting",
    "original",
    "copy",
    "okay",
    "ica",
    "bank",
    "next",
    "question",
    "analyze",
    "data",
    "real",
    "time",
    "somebody",
    "clicking",
    "facebook",
    "data",
    "coming",
    "xml",
    "right",
    "immediately",
    "data",
    "comes",
    "flu",
    "read",
    "push",
    "kafka",
    "data",
    "reached",
    "still",
    "kafka",
    "want",
    "analyze",
    "data",
    "real",
    "time",
    "spark",
    "comes",
    "picture",
    "something",
    "called",
    "spark",
    "streaming",
    "spark",
    "learn",
    "think",
    "sure",
    "okay",
    "something",
    "called",
    "spark",
    "streaming",
    "spark",
    "streaming",
    "utility",
    "analyzes",
    "data",
    "fum",
    "read",
    "type",
    "data",
    "get",
    "sorry",
    "point",
    "point",
    "takes",
    "kafka",
    "contct",
    "sql",
    "server",
    "database",
    "flu",
    "work",
    "sql",
    "server",
    "scoop",
    "work",
    "sql",
    "data",
    "scoop",
    "type",
    "data",
    "flume",
    "kafka",
    "also",
    "directly",
    "get",
    "data",
    "bit",
    "confusing",
    "know",
    "okay",
    "new",
    "first",
    "kafka",
    "also",
    "directly",
    "get",
    "xml",
    "file",
    "advantage",
    "flume",
    "pull",
    "based",
    "pull",
    "based",
    "means",
    "install",
    "flume",
    "without",
    "disturbing",
    "setup",
    "pull",
    "data",
    "kafka",
    "pull",
    "data",
    "install",
    "tool",
    "modify",
    "existing",
    "setup",
    "flume",
    "pull",
    "based",
    "meaning",
    "install",
    "plume",
    "start",
    "reading",
    "data",
    "automatically",
    "modify",
    "anything",
    "install",
    "kafka",
    "directly",
    "kafka",
    "also",
    "get",
    "xml",
    "data",
    "read",
    "xml",
    "data",
    "need",
    "something",
    "called",
    "kafka",
    "producer",
    "installed",
    "ics",
    "source",
    "system",
    "possible",
    "right",
    "since",
    "existing",
    "setup",
    "ca",
    "directly",
    "get",
    "kafka",
    "get",
    "flume",
    "flume",
    "give",
    "kafka",
    "kafka",
    "send",
    "one",
    "copy",
    "hdfs",
    "need",
    "keep",
    "original",
    "data",
    "second",
    "copy",
    "send",
    "something",
    "called",
    "spark",
    "streaming",
    "spark",
    "streaming",
    "library",
    "available",
    "spark",
    "real",
    "time",
    "processing",
    "moment",
    "give",
    "data",
    "process",
    "real",
    "time",
    "somebody",
    "clicked",
    "immediately",
    "data",
    "spark",
    "streaming",
    "process",
    "say",
    "send",
    "sms",
    "take",
    "action",
    "whatever",
    "want",
    "tools",
    "real",
    "time",
    "apart",
    "spark",
    "streaming",
    "something",
    "called",
    "fling",
    "something",
    "called",
    "storm",
    "realtime",
    "processing",
    "tools",
    "might",
    "make",
    "note",
    "yeah",
    "sorry",
    "sorry",
    "yeah",
    "yeah",
    "correct",
    "kafka",
    "facebook",
    "fl",
    "whatsapp",
    "whatsapp",
    "point",
    "point",
    "right",
    "one",
    "one",
    "facebook",
    "like",
    "distributed",
    "right",
    "anybody",
    "whatsapp",
    "also",
    "keep",
    "right",
    "know",
    "oh",
    "okay",
    "okay",
    "okay",
    "context",
    "yeah",
    "yeah",
    "correct",
    "correct",
    "good",
    "understand",
    "point",
    "see",
    "easy",
    "whatsapp",
    "push",
    "right",
    "flu",
    "like",
    "whatsapp",
    "okay",
    "flu",
    "gets",
    "data",
    "push",
    "push",
    "somewhere",
    "store",
    "data",
    "kafka",
    "means",
    "get",
    "data",
    "store",
    "long",
    "consume",
    "ic",
    "bank",
    "architecture",
    "got",
    "flume",
    "send",
    "kafka",
    "one",
    "copy",
    "go",
    "spark",
    "streaming",
    "guy",
    "realtime",
    "processing",
    "something",
    "okay",
    "one",
    "copy",
    "go",
    "hdfs",
    "data",
    "remain",
    "kafka",
    "long",
    "want",
    "configure",
    "default",
    "7",
    "10",
    "days",
    "also",
    "mention",
    "size",
    "data",
    "want",
    "keep",
    "10",
    "gb",
    "data",
    "20",
    "gb",
    "data",
    "something",
    "called",
    "topics",
    "kafka",
    "mention",
    "long",
    "data",
    "stay",
    "cluster",
    "right",
    "store",
    "data",
    "one",
    "send",
    "right",
    "getting",
    "point",
    "flume",
    "send",
    "happen",
    "point",
    "point",
    "delivery",
    "need",
    "copy",
    "need",
    "send",
    "also",
    "time",
    "also",
    "prob",
    "tomorrow",
    "want",
    "build",
    "one",
    "application",
    "okay",
    "something",
    "data",
    "application",
    "new",
    "application",
    "build",
    "tomorrow",
    "also",
    "get",
    "data",
    "kafka",
    "kafka",
    "like",
    "give",
    "data",
    "kafka",
    "anybody",
    "take",
    "store",
    "got",
    "flume",
    "like",
    "point",
    "point",
    "got",
    "kafka",
    "store",
    "anybody",
    "come",
    "get",
    "data",
    "10",
    "applications",
    "read",
    "kafka",
    "yeah",
    "sorry",
    "slow",
    "slow",
    "slow",
    "hard",
    "dis",
    "read",
    "directly",
    "push",
    "real",
    "time",
    "right",
    "get",
    "tanic",
    "shop",
    "want",
    "send",
    "sms",
    "right",
    "processing",
    "one",
    "golden",
    "copy",
    "go",
    "original",
    "facebook",
    "clicks",
    "golden",
    "copy",
    "go",
    "tomorrow",
    "analyze",
    "clicked",
    "page",
    "insurance",
    "page",
    "facebook",
    "want",
    "immediately",
    "contact",
    "process",
    "processed",
    "understood",
    "decision",
    "ven",
    "sending",
    "one",
    "copy",
    "one",
    "copy",
    "processing",
    "happen",
    "simply",
    "original",
    "data",
    "stored",
    "data",
    "deleted",
    "discard",
    "ah",
    "discarded",
    "original",
    "data",
    "anywhere",
    "store",
    "also",
    "yes",
    "golden",
    "copy",
    "called",
    "golden",
    "copy",
    "map",
    "reduce",
    "real",
    "time",
    "batch",
    "yeah",
    "yeah",
    "read",
    "oracle",
    "read",
    "oracle",
    "fl",
    "kafka",
    "scoop",
    "read",
    "uh",
    "sql",
    "based",
    "stores",
    "kafka",
    "interesting",
    "thing",
    "teach",
    "three",
    "days",
    "course",
    "kafka",
    "three",
    "days",
    "enough",
    "actually",
    "kafka",
    "big",
    "topic",
    "actually",
    "kafka",
    "message",
    "cu",
    "elk",
    "c",
    "stack",
    "right",
    "elk",
    "stack",
    "come",
    "example",
    "uh",
    "directly",
    "use",
    "elk",
    "stack",
    "using",
    "telling",
    "break",
    "somebody",
    "used",
    "store",
    "data",
    "hadoop",
    "process",
    "using",
    "something",
    "called",
    "splunk",
    "tool",
    "called",
    "splunk",
    "splunk",
    "actually",
    "replace",
    "elk",
    "stack",
    "elk",
    "one",
    "tool",
    "visualization",
    "right",
    "log",
    "files",
    "machine",
    "learning",
    "apply",
    "visualize",
    "using",
    "something",
    "called",
    "splunk",
    "big",
    "pipeline",
    "talking",
    "also",
    "build",
    "using",
    "elk",
    "want",
    "saying",
    "elk",
    "stack",
    "actually",
    "elastic",
    "search",
    "log",
    "stash",
    "kibana",
    "probably",
    "explain",
    "right",
    "ah",
    "working",
    "elk",
    "somebody",
    "yeah",
    "explain",
    "probably",
    "elk",
    "lightweight",
    "inst",
    "probably",
    "100",
    "tb",
    "tb",
    "let",
    "say",
    "getting",
    "lot",
    "log",
    "files",
    "want",
    "make",
    "sense",
    "right",
    "different",
    "different",
    "places",
    "let",
    "say",
    "collecting",
    "logs",
    "network",
    "h",
    "want",
    "make",
    "sense",
    "need",
    "tool",
    "easily",
    "collect",
    "clean",
    "display",
    "dashboard",
    "elk",
    "tool",
    "similar",
    "called",
    "splunk",
    "big",
    "day",
    "hadoop",
    "ecosystem",
    "third",
    "party",
    "tool",
    "actually",
    "open",
    "source",
    "yeah",
    "exactly",
    "part",
    "sql",
    "scoop",
    "exactly",
    "good",
    "question",
    "scoop",
    "getting",
    "data",
    "right",
    "scope",
    "ideally",
    "push",
    "hdfs",
    "okay",
    "say",
    "scoop",
    "push",
    "kafka",
    "also",
    "also",
    "possible",
    "data",
    "available",
    "scoop",
    "okay",
    "say",
    "give",
    "kafa",
    "either",
    "read",
    "live",
    "processing",
    "scoop",
    "push",
    "something",
    "called",
    "hive",
    "hive",
    "data",
    "warehouse",
    "analyze",
    "live",
    "want",
    "normally",
    "transactional",
    "data",
    "much",
    "use",
    "realtime",
    "processing",
    "based",
    "facebook",
    "clicks",
    "based",
    "transaction",
    "get",
    "answer",
    "question",
    "um",
    "good",
    "question",
    "actually",
    "uh",
    "uh",
    "uh",
    "give",
    "suggestion",
    "customer",
    "making",
    "transaction",
    "uh",
    "customer",
    "making",
    "transaction",
    "using",
    "icc",
    "credit",
    "card",
    "made",
    "transaction",
    "immediately",
    "send",
    "offer",
    "making",
    "transaction",
    "made",
    "mistake",
    "want",
    "show",
    "made",
    "transaction",
    "okay",
    "data",
    "oracle",
    "capture",
    "data",
    "scoop",
    "say",
    "scoop",
    "using",
    "flume",
    "cdc",
    "producing",
    "log",
    "file",
    "log",
    "captured",
    "flume",
    "scoop",
    "scoop",
    "come",
    "picture",
    "scoop",
    "come",
    "directly",
    "reading",
    "crm",
    "crm",
    "mysql",
    "mysql",
    "original",
    "table",
    "read",
    "directly",
    "oracle",
    "case",
    "directly",
    "reading",
    "table",
    "want",
    "read",
    "table",
    "scope",
    "reading",
    "cdc",
    "change",
    "data",
    "capture",
    "log",
    "file",
    "collected",
    "flume",
    "actually",
    "actually",
    "make",
    "answer",
    "want",
    "make",
    "real",
    "time",
    "okay",
    "need",
    "cdc",
    "system",
    "push",
    "data",
    "flum",
    "actually",
    "huh",
    "exactly",
    "exactly",
    "real",
    "time",
    "uh",
    "worked",
    "banking",
    "use",
    "case",
    "talking",
    "side",
    "spark",
    "streaming",
    "flink",
    "storm",
    "spark",
    "streaming",
    "learning",
    "flink",
    "storm",
    "frameworks",
    "realtime",
    "data",
    "processing",
    "wondering",
    "realtime",
    "data",
    "processing",
    "critical",
    "three",
    "three",
    "real",
    "time",
    "may",
    "use",
    "fling",
    "apache",
    "storm",
    "also",
    "apache",
    "storm",
    "almost",
    "market",
    "okay",
    "uh",
    "spark",
    "streaming",
    "flink",
    "popular",
    "right",
    "worked",
    "use",
    "case",
    "city",
    "bank",
    "swipe",
    "credit",
    "card",
    "right",
    "know",
    "fraud",
    "swiping",
    "credit",
    "card",
    "want",
    "find",
    "whether",
    "fraud",
    "transaction",
    "real",
    "transaction",
    "correct",
    "exactly",
    "even",
    "fraud",
    "let",
    "say",
    "use",
    "credit",
    "card",
    "regularly",
    "one",
    "fine",
    "day",
    "suddenly",
    "pay",
    "us",
    "dollars",
    "something",
    "get",
    "call",
    "customer",
    "care",
    "really",
    "make",
    "means",
    "imagine",
    "hdfc",
    "bank",
    "many",
    "credit",
    "cards",
    "circulation",
    "many",
    "people",
    "transacting",
    "every",
    "day",
    "think",
    "somebody",
    "sitting",
    "monitoring",
    "every",
    "swipe",
    "spark",
    "streaming",
    "collecting",
    "data",
    "credit",
    "card",
    "transactions",
    "get",
    "dumped",
    "okay",
    "wo",
    "come",
    "relational",
    "form",
    "come",
    "flat",
    "file",
    "something",
    "flume",
    "catch",
    "moment",
    "comes",
    "push",
    "kafka",
    "directly",
    "spark",
    "streaming",
    "flink",
    "storm",
    "could",
    "written",
    "logic",
    "okay",
    "match",
    "transaction",
    "guy",
    "limit",
    "something",
    "last",
    "uh",
    "transaction",
    "made",
    "places",
    "outside",
    "india",
    "something",
    "trigger",
    "mail",
    "customer",
    "care",
    "getting",
    "emi",
    "calls",
    "also",
    "make",
    "big",
    "transaction",
    "immediately",
    "call",
    "say",
    "want",
    "convert",
    "emi",
    "right",
    "guys",
    "responsible",
    "stock",
    "market",
    "stop",
    "ah",
    "stock",
    "market",
    "big",
    "example",
    "yeah",
    "thing",
    "real",
    "time",
    "right",
    "tomorrow",
    "get",
    "message",
    "today",
    "stop",
    "loss",
    "make",
    "sense",
    "right",
    "since",
    "interested",
    "think",
    "spar",
    "course",
    "streaming",
    "bad",
    "okay",
    "give",
    "example",
    "difference",
    "storm",
    "process",
    "one",
    "event",
    "also",
    "meaning",
    "one",
    "credit",
    "card",
    "transaction",
    "storm",
    "process",
    "ideally",
    "one",
    "credit",
    "card",
    "transaction",
    "process",
    "minute",
    "spark",
    "streaming",
    "process",
    "bunch",
    "process",
    "one",
    "meaning",
    "collect",
    "probably",
    "2",
    "seconds",
    "worth",
    "transaction",
    "predict",
    "mean",
    "called",
    "micro",
    "batching",
    "creating",
    "batch",
    "processing",
    "real",
    "time",
    "near",
    "real",
    "time",
    "exactly",
    "100",
    "real",
    "time",
    "possible",
    "storm",
    "fling",
    "also",
    "think",
    "know",
    "lot",
    "information",
    "passing",
    "lot",
    "actually",
    "almost",
    "like",
    "dead",
    "hearing",
    "terms",
    "ca",
    "really",
    "help",
    "making",
    "google",
    "okay",
    "probably",
    "may",
    "make",
    "sense",
    "listening",
    "probably",
    "later",
    "start",
    "working",
    "somebody",
    "talks",
    "spark",
    "streaming",
    "use",
    "case",
    "think",
    "ah",
    "okay",
    "okay",
    "oh",
    "ic",
    "bank",
    "use",
    "case",
    "somebody",
    "clicking",
    "wrote",
    "spark",
    "streaming",
    "code",
    "okay",
    "question",
    "securities",
    "handling",
    "100",
    "sure",
    "working",
    "use",
    "case",
    "limited",
    "right",
    "question",
    "okay",
    "uh",
    "question",
    "much",
    "valid",
    "making",
    "credit",
    "card",
    "transaction",
    "spark",
    "streaming",
    "catches",
    "spark",
    "streaming",
    "catches",
    "realtime",
    "transaction",
    "know",
    "fraud",
    "based",
    "previous",
    "transaction",
    "fine",
    "yeah",
    "bat",
    "data",
    "working",
    "machine",
    "learning",
    "could",
    "built",
    "machine",
    "learning",
    "model",
    "h",
    "profiling",
    "past",
    "within",
    "hard",
    "sp",
    "spar",
    "spark",
    "ml",
    "needs",
    "yeah",
    "already",
    "historical",
    "data",
    "customers",
    "transactions",
    "top",
    "hdfs",
    "running",
    "top",
    "hdfs",
    "part",
    "hd",
    "huh",
    "install",
    "spark",
    "exactly",
    "bas",
    "last",
    "spark",
    "getting",
    "installed",
    "top",
    "hadoop",
    "hdfs",
    "storage",
    "spark",
    "making",
    "credit",
    "card",
    "transaction",
    "today",
    "previous",
    "transactions",
    "already",
    "could",
    "already",
    "built",
    "machine",
    "learning",
    "model",
    "simply",
    "feed",
    "new",
    "transaction",
    "predict",
    "otherwise",
    "compare",
    "batch",
    "data",
    "run",
    "batch",
    "job",
    "take",
    "lot",
    "time",
    "right",
    "customer",
    "data",
    "run",
    "mlb",
    "called",
    "mlb",
    "spar",
    "machine",
    "learning",
    "model",
    "running",
    "easily",
    "compare",
    "say",
    "fraud",
    "want",
    "compare",
    "batch",
    "otherwise",
    "see",
    "making",
    "transaction",
    "right",
    "compare",
    "previous",
    "transactions",
    "understand",
    "whether",
    "fraud",
    "like",
    "said",
    "traveling",
    "within",
    "chennai",
    "last",
    "10",
    "years",
    "uh",
    "today",
    "swiping",
    "new",
    "york",
    "h",
    "new",
    "york",
    "swipe",
    "want",
    "understand",
    "anomaly",
    "compare",
    "previous",
    "swipe",
    "right",
    "time",
    "go",
    "compare",
    "customer",
    "build",
    "machine",
    "learning",
    "model",
    "based",
    "data",
    "feed",
    "data",
    "predict",
    "know",
    "whether",
    "fraud",
    "uh",
    "kafka",
    "okay",
    "kafka",
    "send",
    "spark",
    "get",
    "published",
    "subscribe",
    "system",
    "called",
    "spark",
    "streaming",
    "application",
    "pll",
    "kafka",
    "topic",
    "get",
    "kafka",
    "getk",
    "pulling",
    "dat",
    "fine",
    "said",
    "hts",
    "correct",
    "permanent",
    "storage",
    "spar",
    "read",
    "also",
    "spar",
    "read",
    "variety",
    "sources",
    "never",
    "theod",
    "time",
    "huh",
    "like",
    "one",
    "week",
    "default",
    "real",
    "time",
    "data",
    "yeah",
    "right",
    "reading",
    "get",
    "current",
    "information",
    "original",
    "information",
    "going",
    "right",
    "golden",
    "copy",
    "tomorrow",
    "want",
    "run",
    "spark",
    "job",
    "yesterday",
    "data",
    "read",
    "hdfs",
    "using",
    "htfs",
    "huh",
    "original",
    "data",
    "get",
    "current",
    "data",
    "machine",
    "learning",
    "model",
    "already",
    "built",
    "based",
    "data",
    "read",
    "data",
    "model",
    "could",
    "already",
    "read",
    "learned",
    "guys",
    "teach",
    "learning",
    "machine",
    "learning",
    "right",
    "tell",
    "working",
    "right",
    "would",
    "data",
    "eng",
    "machine",
    "learning",
    "right",
    "topic",
    "machine",
    "learning",
    "right",
    "saying",
    "hdfs",
    "transactions",
    "made",
    "customers",
    "last",
    "one",
    "year",
    "credit",
    "card",
    "rep",
    "ah",
    "repository",
    "feed",
    "sparks",
    "machine",
    "learning",
    "algorithm",
    "build",
    "model",
    "give",
    "new",
    "data",
    "predict",
    "whether",
    "fraud",
    "go",
    "last",
    "adjust",
    "machine",
    "learning",
    "saying",
    "rate",
    "available",
    "huh",
    "andk",
    "l",
    "andit",
    "directly",
    "compare",
    "get",
    "talking",
    "fraud",
    "detection",
    "use",
    "case",
    "thing",
    "right",
    "uh",
    "stock",
    "market",
    "price",
    "variation",
    "ah",
    "recommendation",
    "recommendation",
    "hisorical",
    "price",
    "hisorical",
    "purchase",
    "today",
    "price",
    "comparing",
    "one",
    "price",
    "bought",
    "price",
    "okay",
    "point",
    "historical",
    "data",
    "huge",
    "terabytes",
    "right",
    "could",
    "already",
    "created",
    "machine",
    "learning",
    "model",
    "feeding",
    "data",
    "give",
    "new",
    "data",
    "predict",
    "whether",
    "fraud",
    "happening",
    "real",
    "time",
    "say",
    "somebody",
    "swipes",
    "credit",
    "card",
    "tell",
    "whether",
    "fraud",
    "tomorrow",
    "make",
    "sense",
    "right",
    "immediately",
    "flip",
    "side",
    "requires",
    "resources",
    "nothing",
    "going",
    "work",
    "resources",
    "need",
    "give",
    "enough",
    "resources",
    "things",
    "work",
    "machine",
    "learning",
    "resource",
    "hungry",
    "cp",
    "power",
    "ram",
    "power",
    "right",
    "cluster",
    "capable",
    "anyway",
    "time",
    "let",
    "move",
    "kafka",
    "right",
    "taking",
    "use",
    "case",
    "right",
    "go",
    "data",
    "hdfs",
    "want",
    "batch",
    "processing",
    "default",
    "framework",
    "map",
    "reduce",
    "mr",
    "map",
    "reduce",
    "see",
    "tomorrow",
    "data",
    "want",
    "batch",
    "processing",
    "care",
    "much",
    "time",
    "takes",
    "one",
    "day",
    "take",
    "want",
    "process",
    "data",
    "use",
    "something",
    "called",
    "map",
    "reduce",
    "right",
    "also",
    "one",
    "guy",
    "called",
    "pig",
    "uh",
    "almost",
    "gone",
    "market",
    "pig",
    "pig",
    "animal",
    "pig",
    "pig",
    "scripting",
    "tool",
    "hadu",
    "popular",
    "spark",
    "nobody",
    "actually",
    "even",
    "aware",
    "pig",
    "perfectly",
    "fine",
    "think",
    "okay",
    "worry",
    "somebody",
    "called",
    "hive",
    "syllabus",
    "hive",
    "teach",
    "next",
    "month",
    "basically",
    "hive",
    "allows",
    "write",
    "sql",
    "top",
    "hadu",
    "drawback",
    "say",
    "create",
    "table",
    "select",
    "count",
    "star",
    "table",
    "hit",
    "query",
    "convert",
    "query",
    "map",
    "reduce",
    "program",
    "translator",
    "native",
    "sql",
    "engine",
    "yeah",
    "writing",
    "oracle",
    "query",
    "oracle",
    "execute",
    "write",
    "hi",
    "query",
    "map",
    "reduce",
    "execute",
    "understand",
    "speak",
    "hive",
    "worry",
    "huh",
    "c",
    "c",
    "exactly",
    "map",
    "reduce",
    "slow",
    "batch",
    "processing",
    "hi",
    "write",
    "sql",
    "query",
    "hi",
    "execute",
    "query",
    "write",
    "map",
    "reduce",
    "program",
    "even",
    "slower",
    "spar",
    "hi",
    "spark",
    "configured",
    "yeah",
    "problem",
    "hi",
    "original",
    "tool",
    "built",
    "2005",
    "facebook",
    "okay",
    "point",
    "popular",
    "tool",
    "right",
    "people",
    "writing",
    "high",
    "queries",
    "even",
    "though",
    "slow",
    "equal",
    "right",
    "see",
    "better",
    "suffer",
    "slowness",
    "learn",
    "java",
    "suffer",
    "sql",
    "right",
    "ah",
    "right",
    "etl",
    "15",
    "minutes",
    "fine",
    "yeah",
    "facebook",
    "developers",
    "thought",
    "instead",
    "learning",
    "java",
    "right",
    "learn",
    "stick",
    "sql",
    "etl",
    "jobs",
    "run",
    "created",
    "tool",
    "called",
    "hive",
    "hive",
    "developer",
    "learn",
    "anything",
    "sql",
    "say",
    "create",
    "table",
    "create",
    "table",
    "join",
    "say",
    "join",
    "write",
    "map",
    "ruce",
    "map",
    "produce",
    "run",
    "may",
    "wait",
    "fact",
    "flipkart",
    "hive",
    "query",
    "took",
    "around",
    "12",
    "hours",
    "run",
    "today",
    "fire",
    "tomorrow",
    "get",
    "result",
    "12",
    "hours",
    "typical",
    "runtime",
    "huge",
    "amount",
    "data",
    "obviously",
    "find",
    "different",
    "story",
    "lot",
    "time",
    "batch",
    "batch",
    "processing",
    "batch",
    "oops",
    "batch",
    "processing",
    "systems",
    "built",
    "top",
    "hado",
    "right",
    "spark",
    "learn",
    "spark",
    "later",
    "want",
    "confuse",
    "lot",
    "spark",
    "guy",
    "inside",
    "spark",
    "okay",
    "even",
    "though",
    "wrote",
    "guy",
    "inside",
    "spark",
    "spark",
    "inmemory",
    "execution",
    "engine",
    "spark",
    "classified",
    "batch",
    "processing",
    "system",
    "fast",
    "map",
    "reduce",
    "difference",
    "spark",
    "programs",
    "faster",
    "map",
    "reduce",
    "programs",
    "near",
    "real",
    "time",
    "real",
    "time",
    "spark",
    "streaming",
    "actually",
    "real",
    "time",
    "would",
    "say",
    "near",
    "real",
    "time",
    "like",
    "100",
    "real",
    "time",
    "buffer",
    "always",
    "like",
    "1",
    "second",
    "2",
    "second",
    "define",
    "process",
    "like",
    "microc",
    "real",
    "time",
    "possible",
    "basally",
    "ah",
    "yeah",
    "mr",
    "pig",
    "obsolete",
    "hive",
    "obsolete",
    "hive",
    "data",
    "warehouse",
    "create",
    "tables",
    "store",
    "data",
    "guy",
    "remain",
    "going",
    "happen",
    "spark",
    "connect",
    "hive",
    "spark",
    "write",
    "yeah",
    "write",
    "sql",
    "okay",
    "spark",
    "spark",
    "sql",
    "library",
    "read",
    "tables",
    "hive",
    "run",
    "top",
    "right",
    "spark",
    "talk",
    "start",
    "talking",
    "hive",
    "hive",
    "really",
    "obsolete",
    "hi",
    "time",
    "say",
    "hdfs",
    "storage",
    "hdfs",
    "mr",
    "pig",
    "gone",
    "nothing",
    "pig",
    "almost",
    "pig",
    "scripting",
    "pig",
    "also",
    "convert",
    "query",
    "map",
    "ruce",
    "obsolute",
    "spark",
    "spark",
    "new",
    "version",
    "spark",
    "practically",
    "replaced",
    "pig",
    "actually",
    "batch",
    "processing",
    "frameworks",
    "right",
    "guys",
    "came",
    "said",
    "typical",
    "question",
    "asked",
    "training",
    "want",
    "run",
    "okay",
    "confusion",
    "want",
    "run",
    "big",
    "group",
    "query",
    "let",
    "say",
    "want",
    "read",
    "full",
    "table",
    "scan",
    "run",
    "query",
    "hive",
    "great",
    "even",
    "spark",
    "sql",
    "hive",
    "great",
    "read",
    "whole",
    "data",
    "process",
    "minute",
    "queries",
    "let",
    "say",
    "select",
    "uh",
    "something",
    "table",
    "name",
    "equal",
    "raguraman",
    "one",
    "row",
    "needs",
    "process",
    "load",
    "full",
    "data",
    "process",
    "useless",
    "hadoop",
    "says",
    "load",
    "everything",
    "sequentially",
    "process",
    "possible",
    "mpp",
    "engines",
    "impala",
    "one",
    "guy",
    "called",
    "impala",
    "another",
    "guy",
    "guy",
    "called",
    "hawk",
    "hawk",
    "lp",
    "something",
    "called",
    "presto",
    "presto",
    "also",
    "something",
    "called",
    "uh",
    "drill",
    "phoenix",
    "mpp",
    "engines",
    "aware",
    "name",
    "expert",
    "like",
    "chance",
    "manager",
    "ask",
    "okay",
    "name",
    "top",
    "five",
    "say",
    "mpp",
    "engines",
    "fired",
    "nothing",
    "going",
    "happen",
    "like",
    "one",
    "used",
    "actually",
    "impala",
    "claa",
    "specific",
    "claa",
    "created",
    "impala",
    "mostly",
    "available",
    "cloud",
    "era",
    "impala",
    "simple",
    "execution",
    "engine",
    "fire",
    "sql",
    "query",
    "run",
    "map",
    "reduce",
    "right",
    "going",
    "happen",
    "queries",
    "like",
    "name",
    "equal",
    "raguraman",
    "fire",
    "impala",
    "query",
    "scan",
    "data",
    "right",
    "connect",
    "hdfs",
    "pick",
    "data",
    "remembers",
    "metadata",
    "data",
    "available",
    "ah",
    "columnar",
    "pick",
    "data",
    "query",
    "wondering",
    "ca",
    "use",
    "instead",
    "hive",
    "guys",
    "fast",
    "guys",
    "fast",
    "use",
    "instead",
    "hive",
    "reliability",
    "inmemory",
    "queries",
    "impala",
    "query",
    "running",
    "hard",
    "cluster",
    "one",
    "machine",
    "crashes",
    "query",
    "gone",
    "high",
    "query",
    "running",
    "even",
    "10",
    "machine",
    "crash",
    "query",
    "complete",
    "map",
    "reduced",
    "redundancy",
    "map",
    "reduce",
    "never",
    "fail",
    "spark",
    "never",
    "fail",
    "yeah",
    "correct",
    "loading",
    "whole",
    "data",
    "memory",
    "analysis",
    "happens",
    "right",
    "going",
    "name",
    "something",
    "correct",
    "say",
    "select",
    "staff",
    "table",
    "first",
    "table",
    "data",
    "loaded",
    "ram",
    "reads",
    "right",
    "initially",
    "whole",
    "data",
    "loaded",
    "exactly",
    "problem",
    "using",
    "impala",
    "okay",
    "metadata",
    "knows",
    "data",
    "loads",
    "queries",
    "faster",
    "run",
    "impala",
    "query",
    "considering",
    "hi",
    "query",
    "impala",
    "queries",
    "much",
    "much",
    "faster",
    "fall",
    "tolerance",
    "fast",
    "queries",
    "like",
    "queries",
    "take",
    "like",
    "2",
    "minutes",
    "3",
    "minutes",
    "time",
    "afford",
    "lose",
    "query",
    "useing",
    "pala",
    "etl",
    "job",
    "want",
    "want",
    "job",
    "fail",
    "run",
    "using",
    "hive",
    "spark",
    "sql",
    "guys",
    "never",
    "fail",
    "distributed",
    "run",
    "n",
    "number",
    "machines",
    "okay",
    "sy",
    "gets",
    "query",
    "gone",
    "show",
    "tomorrow",
    "app",
    "reduce",
    "persist",
    "data",
    "intermediate",
    "results",
    "fall",
    "tolerant",
    "impala",
    "memory",
    "machine",
    "crashes",
    "data",
    "gone",
    "ram",
    "query",
    "abot",
    "mpp",
    "engines",
    "like",
    "think",
    "traditional",
    "world",
    "also",
    "difference",
    "impala",
    "cloud",
    "era",
    "hawk",
    "lp",
    "hots",
    "competition",
    "impala",
    "originally",
    "created",
    "cloud",
    "era",
    "promote",
    "cloud",
    "era",
    "platform",
    "see",
    "impala",
    "coton",
    "works",
    "platform",
    "never",
    "see",
    "impala",
    "promote",
    "haulk",
    "say",
    "hawk",
    "best",
    "engine",
    "okay",
    "lp",
    "new",
    "version",
    "hawk",
    "presto",
    "drill",
    "similar",
    "ones",
    "default",
    "available",
    "guy",
    "interesting",
    "something",
    "called",
    "hbase",
    "add",
    "somebody",
    "asking",
    "hbase",
    "hbase",
    "database",
    "hadu",
    "sql",
    "database",
    "realtime",
    "database",
    "real",
    "real",
    "time",
    "hbas",
    "get",
    "installed",
    "hdfs",
    "storage",
    "right",
    "random",
    "reads",
    "writes",
    "like",
    "entire",
    "block",
    "normally",
    "reading",
    "data",
    "hdfs",
    "read",
    "whole",
    "block",
    "right",
    "hbas",
    "api",
    "install",
    "hbas",
    "top",
    "hadoop",
    "random",
    "reads",
    "wrs",
    "individual",
    "records",
    "read",
    "write",
    "process",
    "nosql",
    "database",
    "hadu",
    "default",
    "sql",
    "database",
    "drawback",
    "hbas",
    "language",
    "somebody",
    "work",
    "hbas",
    "learn",
    "language",
    "allow",
    "sql",
    "queries",
    "phix",
    "sql",
    "space",
    "phenix",
    "hbas",
    "like",
    "roti",
    "paner",
    "butter",
    "masala",
    "deadly",
    "combination",
    "mean",
    "deadly",
    "combination",
    "writing",
    "sql",
    "top",
    "right",
    "deadly",
    "say",
    "another",
    "two",
    "years",
    "slow",
    "really",
    "fast",
    "hbas",
    "fast",
    "sql",
    "conversion",
    "okay",
    "already",
    "built",
    "sql",
    "query",
    "limitations",
    "sql",
    "fail",
    "finix",
    "get",
    "metadata",
    "already",
    "queries",
    "converted",
    "whatever",
    "query",
    "sql",
    "writing",
    "knows",
    "equivalent",
    "hbas",
    "query",
    "push",
    "query",
    "write",
    "sql",
    "run",
    "hbas",
    "give",
    "result",
    "things",
    "like",
    "secondary",
    "indexes",
    "index",
    "data",
    "apart",
    "hb",
    "fast",
    "actually",
    "real",
    "news",
    "another",
    "two",
    "years",
    "predicted",
    "combination",
    "drawback",
    "phoenix",
    "still",
    "alpha",
    "beta",
    "phenix",
    "production",
    "ready",
    "phenix",
    "becomes",
    "production",
    "ready",
    "oil",
    "tp",
    "oil",
    "tp",
    "means",
    "hado",
    "provide",
    "olap",
    "tp",
    "huh",
    "support",
    "future",
    "trying",
    "make",
    "acid",
    "support",
    "entire",
    "combination",
    "become",
    "oil",
    "tp",
    "system",
    "like",
    "right",
    "happen",
    "get",
    "transaction",
    "management",
    "hadoop",
    "far",
    "another",
    "two",
    "years",
    "line",
    "say",
    "phenix",
    "getting",
    "integrated",
    "hbas",
    "originally",
    "hbas",
    "drawback",
    "understand",
    "sql",
    "phoenix",
    "started",
    "phoenix",
    "layer",
    "top",
    "write",
    "sql",
    "another",
    "two",
    "years",
    "developing",
    "system",
    "transaction",
    "management",
    "get",
    "true",
    "transaction",
    "management",
    "hadoop",
    "uh",
    "yeah",
    "talk",
    "clera",
    "going",
    "clera",
    "say",
    "want",
    "product",
    "give",
    "one",
    "package",
    "go",
    "apache",
    "download",
    "get",
    "hdfs",
    "map",
    "ruce",
    "yan",
    "hado",
    "separately",
    "install",
    "commercial",
    "distributions",
    "package",
    "get",
    "everything",
    "okay",
    "example",
    "hawk",
    "presto",
    "drill",
    "phenix",
    "get",
    "hbas",
    "hive",
    "pig",
    "map",
    "reduce",
    "spark",
    "impala",
    "things",
    "get",
    "doubt",
    "show",
    "uh",
    "cloud",
    "page",
    "yes",
    "something",
    "called",
    "spark",
    "sql",
    "runs",
    "sql",
    "query",
    "really",
    "fast",
    "actually",
    "tables",
    "creating",
    "high",
    "spar",
    "storage",
    "right",
    "whatever",
    "tables",
    "create",
    "uh",
    "connect",
    "run",
    "say",
    "cloud",
    "cdh",
    "hb",
    "processing",
    "engine",
    "processing",
    "engine",
    "cloudas",
    "product",
    "showing",
    "tell",
    "blabbering",
    "okay",
    "may",
    "thinking",
    "guy",
    "talking",
    "lot",
    "tools",
    "really",
    "exist",
    "world",
    "look",
    "look",
    "spark",
    "high",
    "pig",
    "map",
    "ruce",
    "spark",
    "impala",
    "uh",
    "yarn",
    "right",
    "hdfs",
    "hbas",
    "kafka",
    "flume",
    "scoop",
    "real",
    "product",
    "clera",
    "get",
    "go",
    "claa",
    "get",
    "even",
    "explained",
    "know",
    "kite",
    "know",
    "honest",
    "kudo",
    "new",
    "file",
    "system",
    "relational",
    "even",
    "aware",
    "kudu",
    "know",
    "like",
    "new",
    "entries",
    "even",
    "know",
    "work",
    "without",
    "problem",
    "security",
    "something",
    "called",
    "sentry",
    "lot",
    "people",
    "wor",
    "worried",
    "security",
    "hadoop",
    "right",
    "service",
    "called",
    "sentry",
    "handle",
    "like",
    "access",
    "management",
    "authorization",
    "authentication",
    "handled",
    "sentry",
    "covered",
    "things",
    "right",
    "things",
    "covered",
    "covered",
    "covered",
    "uh",
    "something",
    "extra",
    "wrote",
    "mean",
    "okay",
    "mean",
    "part",
    "right",
    "prestor",
    "drill",
    "phenix",
    "wrote",
    "one",
    "thing",
    "present",
    "comes",
    "uzi",
    "told",
    "right",
    "workflow",
    "scheduler",
    "like",
    "want",
    "schedule",
    "workflow",
    "uh",
    "think",
    "one",
    "thing",
    "uh",
    "half",
    "hour",
    "left",
    "like",
    "almost",
    "tortured",
    "show",
    "cloud",
    "cluster",
    "better",
    "talking",
    "lot",
    "keep",
    "talking",
    "right",
    "show",
    "cloud",
    "cluster",
    "uh",
    "handson",
    "today",
    "uh",
    "cluster",
    "using",
    "using",
    "lab",
    "readon",
    "access",
    "admin",
    "side",
    "means",
    "change",
    "replication",
    "factor",
    "block",
    "size",
    "anything",
    "done",
    "admins",
    "upload",
    "data",
    "say",
    "run",
    "program",
    "work",
    "tomorrow",
    "let",
    "look",
    "cluster",
    "right",
    "far",
    "talking",
    "cluster",
    "cluster",
    "seeing",
    "cluster",
    "far",
    "think",
    "cloud",
    "hoton",
    "works",
    "uh",
    "three",
    "getting",
    "used",
    "actually",
    "three",
    "companies",
    "sort",
    "famous",
    "say",
    "right",
    "okay",
    "lms",
    "like",
    "lab",
    "check",
    "yeah",
    "um",
    "things",
    "bit",
    "confusing",
    "tell",
    "use",
    "use",
    "web",
    "console",
    "web",
    "console",
    "connect",
    "uh",
    "linux",
    "file",
    "system",
    "hadoop",
    "installed",
    "later",
    "cloud",
    "era",
    "manager",
    "admin",
    "interface",
    "cluster",
    "connect",
    "able",
    "modify",
    "see",
    "stuff",
    "go",
    "cloud",
    "era",
    "cloud",
    "era",
    "manager",
    "utility",
    "comes",
    "cloud",
    "era",
    "administer",
    "cluster",
    "like",
    "want",
    "add",
    "service",
    "remove",
    "service",
    "add",
    "machine",
    "remove",
    "machine",
    "administrator",
    "hadoop",
    "administrator",
    "things",
    "add",
    "users",
    "remove",
    "users",
    "permissions",
    "everything",
    "done",
    "cloud",
    "manager",
    "speak",
    "anything",
    "using",
    "admin",
    "utility",
    "look",
    "cluster",
    "guy",
    "good",
    "let",
    "show",
    "cloud",
    "manager",
    "install",
    "running",
    "company",
    "purchase",
    "let",
    "say",
    "20",
    "machines",
    "physically",
    "yeah",
    "process",
    "installing",
    "hado",
    "cluster",
    "already",
    "installed",
    "running",
    "cluster",
    "creating",
    "scratch",
    "purchase",
    "let",
    "say",
    "20",
    "machines",
    "download",
    "something",
    "called",
    "cloud",
    "manager",
    "run",
    "help",
    "set",
    "cluster",
    "tell",
    "master",
    "slave",
    "block",
    "size",
    "like",
    "useal",
    "cloud",
    "also",
    "physical",
    "even",
    "physical",
    "cluster",
    "actually",
    "running",
    "aws",
    "amazon",
    "web",
    "services",
    "huh",
    "practice",
    "uh",
    "access",
    "right",
    "mean",
    "want",
    "practice",
    "setup",
    "already",
    "available",
    "right",
    "giving",
    "bad",
    "request",
    "try",
    "lo",
    "uh",
    "let",
    "try",
    "log",
    "okay",
    "probably",
    "access",
    "yeah",
    "pay",
    "attention",
    "uh",
    "uh",
    "cloudera",
    "cluster",
    "okay",
    "admin",
    "gui",
    "okay",
    "cluster",
    "login",
    "dashboard",
    "looking",
    "hado",
    "cluster",
    "first",
    "point",
    "understand",
    "second",
    "point",
    "hado",
    "cluster",
    "services",
    "installed",
    "see",
    "hdfs",
    "talking",
    "like",
    "see",
    "right",
    "okay",
    "hdfs",
    "hive",
    "hue",
    "talk",
    "impala",
    "kafka",
    "uzi",
    "solar",
    "spark",
    "yarn",
    "zookeeper",
    "services",
    "running",
    "cluster",
    "see",
    "green",
    "right",
    "get",
    "quick",
    "overview",
    "cluster",
    "like",
    "running",
    "cluster",
    "fine",
    "many",
    "machines",
    "cluster",
    "click",
    "host",
    "menu",
    "see",
    "see",
    "machines",
    "machines",
    "one1",
    "right",
    "many",
    "machines",
    "think",
    "nine",
    "machinein",
    "see",
    "good",
    "health",
    "n",
    "nine",
    "machines",
    "cluster",
    "also",
    "see",
    "configuration",
    "scroll",
    "say",
    "see",
    "ram",
    "hard",
    "disk",
    "physical",
    "memory",
    "ram",
    "first",
    "two",
    "machines",
    "first",
    "four",
    "machines",
    "8",
    "gb",
    "ram",
    "next",
    "one",
    "16",
    "gb",
    "ram",
    "hard",
    "disk",
    "see",
    "see",
    "much",
    "storage",
    "full",
    "everything",
    "right",
    "see",
    "guy",
    "32",
    "gb",
    "ram",
    "currently",
    "used",
    "process",
    "running",
    "guy",
    "also",
    "249",
    "gb",
    "hard",
    "disk",
    "600",
    "gb",
    "hard",
    "disk",
    "50gb",
    "hard",
    "disk",
    "see",
    "single",
    "window",
    "easy",
    "complicated",
    "thought",
    "way",
    "right",
    "host",
    "menu",
    "right",
    "uh",
    "uh",
    "cloud",
    "manager",
    "homepage",
    "click",
    "host",
    "host",
    "see",
    "right",
    "uh",
    "look",
    "hdfs",
    "hdfs",
    "click",
    "hdfs",
    "okay",
    "see",
    "hdfs",
    "menu",
    "related",
    "hdfs",
    "hdfs",
    "learned",
    "replication",
    "block",
    "size",
    "configure",
    "something",
    "called",
    "configuration",
    "want",
    "change",
    "see",
    "saying",
    "read",
    "change",
    "want",
    "see",
    "replication",
    "factor",
    "cluster",
    "dfs",
    "sorry",
    "dfs",
    "application",
    "factor",
    "three",
    "click",
    "change",
    "two",
    "10",
    "whatever",
    "want",
    "admin",
    "admin",
    "want",
    "see",
    "block",
    "size",
    "dfs",
    "block",
    "size",
    "see",
    "click",
    "change",
    "hard",
    "admin",
    "applicable",
    "everybody",
    "immediately",
    "uh",
    "somebody",
    "asking",
    "balancer",
    "right",
    "balancer",
    "say",
    "actions",
    "okay",
    "uh",
    "problem",
    "things",
    "able",
    "run",
    "instances",
    "h",
    "balancer",
    "running",
    "uh",
    "run",
    "command",
    "huh",
    "sorry",
    "green",
    "running",
    "running",
    "process",
    "come",
    "green",
    "actually",
    "uh",
    "chart",
    "commands",
    "configuration",
    "um",
    "lot",
    "configuration",
    "okay",
    "one",
    "thing",
    "saw",
    "uh",
    "host",
    "menu",
    "saw",
    "already",
    "right",
    "far",
    "discussed",
    "hdfs",
    "kafka",
    "go",
    "kafka",
    "go",
    "kafka",
    "okay",
    "kafka",
    "says",
    "kafka",
    "broker",
    "three",
    "means",
    "three",
    "machines",
    "running",
    "kafka",
    "told",
    "kafka",
    "running",
    "cluster",
    "since",
    "lab",
    "setup",
    "running",
    "kafka",
    "inside",
    "hadoop",
    "ideally",
    "run",
    "separately",
    "needs",
    "storage",
    "spaces",
    "hold",
    "data",
    "kafka",
    "brokers",
    "three",
    "running",
    "inside",
    "cluster",
    "mean",
    "machines",
    "go",
    "machines",
    "show",
    "go",
    "host",
    "menu",
    "expand",
    "host",
    "see",
    "example",
    "expand",
    "guy",
    "see",
    "name",
    "node",
    "right",
    "also",
    "go",
    "hdfs",
    "see",
    "example",
    "go",
    "hdfs",
    "instances",
    "see",
    "active",
    "standby",
    "active",
    "standby",
    "two",
    "machines",
    "two",
    "name",
    "notes",
    "told",
    "right",
    "active",
    "standby",
    "running",
    "right",
    "machines",
    "running",
    "kafka",
    "separate",
    "cluster",
    "way",
    "meant",
    "uh",
    "really",
    "admin",
    "access",
    "otherwise",
    "something",
    "could",
    "done",
    "want",
    "mess",
    "cluster",
    "also",
    "way",
    "menus",
    "actually",
    "disabled",
    "ca",
    "run",
    "see",
    "exactly",
    "ideally",
    "case",
    "utilizing",
    "proper",
    "space",
    "right",
    "ideally",
    "set",
    "separate",
    "broker",
    "download",
    "data",
    "actually",
    "based",
    "us",
    "running",
    "aws",
    "actually",
    "know",
    "mean",
    "access",
    "anywhere",
    "matter",
    "aw",
    "create",
    "cluster",
    "anywhere",
    "idea",
    "created",
    "cluster",
    "third",
    "party",
    "give",
    "access",
    "us",
    "even",
    "read",
    "access",
    "like",
    "guys",
    "uh",
    "brush",
    "tomorrow",
    "tomorrow",
    "actually",
    "want",
    "clarify",
    "one",
    "thing",
    "uh",
    "go",
    "big",
    "data",
    "cloud",
    "lab",
    "cloud",
    "data",
    "manager",
    "really",
    "need",
    "admin",
    "interface",
    "going",
    "admin",
    "activity",
    "important",
    "thing",
    "need",
    "web",
    "console",
    "click",
    "web",
    "console",
    "clicked",
    "reason",
    "opening",
    "let",
    "see",
    "guys",
    "see",
    "anything",
    "getting",
    "login",
    "log",
    "show",
    "log",
    "right",
    "moini",
    "yeah",
    "log",
    "connect",
    "one",
    "machines",
    "cluster",
    "see",
    "linux",
    "log",
    "one",
    "machine",
    "see",
    "able",
    "log",
    "getting",
    "linux",
    "shell",
    "take",
    "time",
    "taking",
    "long",
    "long",
    "time",
    "product",
    "called",
    "hue",
    "important",
    "product",
    "called",
    "hue",
    "hue",
    "third",
    "party",
    "application",
    "okay",
    "popular",
    "need",
    "hue",
    "developer",
    "want",
    "upload",
    "files",
    "hadoop",
    "use",
    "hue",
    "one",
    "use",
    "cases",
    "example",
    "click",
    "click",
    "hue",
    "last",
    "username",
    "password",
    "sign",
    "okay",
    "sign",
    "click",
    "hdfs",
    "browser",
    "see",
    "hadoop",
    "files",
    "hadoop",
    "hu",
    "like",
    "web",
    "ui",
    "hadoop",
    "hdfs",
    "file",
    "system",
    "files",
    "folders",
    "connect",
    "see",
    "anything",
    "fresh",
    "account",
    "one",
    "important",
    "point",
    "want",
    "tell",
    "hadoop",
    "home",
    "directory",
    "user",
    "sl",
    "something",
    "gl",
    "faculty",
    "directory",
    "try",
    "hue",
    "thing",
    "pc",
    "click",
    "hue",
    "type",
    "username",
    "password",
    "see",
    "whether",
    "see",
    "thing",
    "able",
    "see",
    "music",
    "another",
    "point",
    "sometimes",
    "cluster",
    "become",
    "slow",
    "designed",
    "accept",
    "request",
    "36",
    "people",
    "right",
    "us",
    "try",
    "log",
    "say",
    "timed",
    "things",
    "username",
    "password",
    "typing",
    "empty",
    "fine",
    "hado",
    "home",
    "files",
    "already",
    "run",
    "something",
    "files",
    "create",
    "tomorrow",
    "worry",
    "create",
    "using",
    "using",
    "data",
    "notes",
    "individually",
    "file",
    "man",
    "huh",
    "cluster",
    "different",
    "different",
    "users",
    "created",
    "hard",
    "cluster",
    "log",
    "get",
    "home",
    "directory",
    "like",
    "linux",
    "linux",
    "happens",
    "log",
    "get",
    "home",
    "directory",
    "home",
    "directory",
    "hdfs",
    "since",
    "already",
    "created",
    "files",
    "see",
    "may",
    "able",
    "see",
    "fresh",
    "uh",
    "home",
    "directory",
    "tomorrow",
    "show",
    "upload",
    "file",
    "run",
    "sample",
    "map",
    "ruce",
    "program",
    "explain",
    "program",
    "tomorrow",
    "worry",
    "unanswered",
    "questions",
    "said",
    "pick",
    "later",
    "forgot",
    "also",
    "anybody",
    "questions",
    "pushed",
    "tell",
    "show",
    "something",
    "forgot",
    "also",
    "things",
    "huh",
    "correct",
    "uh",
    "commodity",
    "hardware",
    "like",
    "real",
    "server",
    "huh",
    "problem",
    "able",
    "connect",
    "web",
    "console",
    "see",
    "giving",
    "option",
    "connect",
    "uh",
    "need",
    "connect",
    "show",
    "something",
    "hu",
    "kafka",
    "impala",
    "running",
    "yeah",
    "uh",
    "okay",
    "missed",
    "concept",
    "wanted",
    "talk",
    "important",
    "concept",
    "right",
    "uh",
    "know",
    "rack",
    "know",
    "rack",
    "server",
    "rack",
    "rack",
    "like",
    "rack",
    "right",
    "explain",
    "rack",
    "even",
    "know",
    "explain",
    "dev",
    "h",
    "servers",
    "point",
    "view",
    "hadoop",
    "might",
    "important",
    "sorry",
    "explain",
    "one",
    "pick",
    "questions",
    "people",
    "know",
    "server",
    "rack",
    "know",
    "rack",
    "rack",
    "housing",
    "push",
    "servers",
    "high",
    "five",
    "movies",
    "see",
    "somebody",
    "sitting",
    "something",
    "typing",
    "something",
    "big",
    "data",
    "center",
    "right",
    "rack",
    "racks",
    "important",
    "important",
    "one",
    "uh",
    "reason",
    "erase",
    "diagram",
    "one",
    "even",
    "know",
    "wrote",
    "fine",
    "racks",
    "important",
    "one",
    "particular",
    "reason",
    "see",
    "let",
    "say",
    "three",
    "racks",
    "three",
    "racks",
    "okay",
    "rack",
    "data",
    "node",
    "one",
    "many",
    "let",
    "say",
    "two",
    "data",
    "notes",
    "uh",
    "reality",
    "installing",
    "hadoop",
    "cluster",
    "right",
    "data",
    "center",
    "racks",
    "servers",
    "servers",
    "called",
    "data",
    "nodes",
    "right",
    "six",
    "data",
    "nodes",
    "three",
    "racks",
    "something",
    "called",
    "rack",
    "awareness",
    "hadoop",
    "meaning",
    "let",
    "say",
    "copying",
    "block",
    "first",
    "copy",
    "go",
    "second",
    "copy",
    "go",
    "imagine",
    "third",
    "copy",
    "go",
    "meaning",
    "enable",
    "feature",
    "called",
    "rack",
    "awareness",
    "hadoop",
    "ensure",
    "blocks",
    "spread",
    "across",
    "multiple",
    "racks",
    "yeah",
    "rack",
    "fail",
    "rack",
    "point",
    "failure",
    "normally",
    "supply",
    "power",
    "network",
    "everything",
    "rack",
    "rack",
    "fails",
    "uh",
    "know",
    "keeping",
    "three",
    "blocks",
    "rack",
    "rack",
    "fails",
    "blocks",
    "gone",
    "hado",
    "enable",
    "rack",
    "awareness",
    "tell",
    "hado",
    "data",
    "rack",
    "place",
    "like",
    "keep",
    "like",
    "three",
    "three",
    "different",
    "racks",
    "uh",
    "rest",
    "two",
    "copies",
    "rack",
    "yeah",
    "location",
    "means",
    "data",
    "center",
    "right",
    "huh",
    "inter",
    "rack",
    "bandwidth",
    "important",
    "rack",
    "rack",
    "bandwidth",
    "important",
    "uh",
    "replication",
    "right",
    "running",
    "lot",
    "programs",
    "use",
    "bandwidth",
    "save",
    "bandwidth",
    "next",
    "replication",
    "happen",
    "within",
    "rack",
    "server",
    "server",
    "enough",
    "even",
    "one",
    "rack",
    "three",
    "racks",
    "going",
    "remote",
    "possibility",
    "right",
    "point",
    "show",
    "saw",
    "go",
    "host",
    "see",
    "rack",
    "one",
    "four",
    "rack",
    "two",
    "two",
    "rack",
    "three",
    "two",
    "uh",
    "default",
    "rack",
    "two",
    "default",
    "means",
    "know",
    "kept",
    "something",
    "422",
    "actually",
    "distribution",
    "actually",
    "available",
    "cloud",
    "data",
    "manager",
    "sy",
    "rack",
    "system",
    "going",
    "one",
    "many",
    "one",
    "rack",
    "right",
    "see",
    "masters",
    "separate",
    "rack",
    "see",
    "look",
    "picture",
    "right",
    "machine",
    "30",
    "data",
    "notes",
    "slot",
    "one",
    "server",
    "rack",
    "server",
    "server",
    "like",
    "like",
    "like",
    "pizza",
    "get",
    "pizza",
    "p",
    "looks",
    "like",
    "server",
    "look",
    "like",
    "like",
    "also",
    "kind",
    "server",
    "tower",
    "servers",
    "servers",
    "typically",
    "use",
    "kind",
    "servers",
    "picture",
    "actually",
    "uh",
    "yahoo",
    "hado",
    "cluster",
    "yahoo",
    "hado",
    "cluster",
    "yahoo",
    "h",
    "biggest",
    "hado",
    "cluster",
    "earth",
    "yahoo",
    "actually",
    "yahoo",
    "holds",
    "record",
    "machines",
    "still",
    "went",
    "bankrupt",
    "sold",
    "useless",
    "company",
    "picture",
    "visible",
    "actually",
    "got",
    "idea",
    "good",
    "question",
    "multiple",
    "name",
    "nodes",
    "yes",
    "concept",
    "called",
    "federation",
    "meaning",
    "ideally",
    "cross",
    "data",
    "nodes",
    "need",
    "one",
    "name",
    "node",
    "like",
    "machines",
    "single",
    "name",
    "node",
    "handle",
    "amount",
    "metadata",
    "become",
    "slow",
    "problem",
    "ideally",
    "yahoo",
    "done",
    "r",
    "said",
    "cross",
    "data",
    "nodes",
    "need",
    "one",
    "data",
    "node",
    "called",
    "name",
    "node",
    "called",
    "federation",
    "federation",
    "means",
    "multi",
    "multiple",
    "active",
    "name",
    "notes",
    "like",
    "10",
    "name",
    "notes",
    "working",
    "share",
    "load",
    "actually",
    "right",
    "like",
    "federation",
    "called",
    "yeah",
    "sorry",
    "one",
    "rack",
    "actually",
    "put",
    "inside",
    "data",
    "center",
    "need",
    "right",
    "data",
    "notes",
    "one",
    "landan",
    "name",
    "note",
    "also",
    "inside",
    "handle",
    "load",
    "actually",
    "know",
    "data",
    "nodes",
    "name",
    "node",
    "handle",
    "load",
    "sorry",
    "moment",
    "file",
    "server",
    "store",
    "process",
    "right",
    "use",
    "case",
    "hadoop",
    "storing",
    "whatever",
    "data",
    "store",
    "data",
    "processed",
    "machine",
    "correct",
    "correct",
    "huh",
    "using",
    "hadoop",
    "right",
    "discussed",
    "beginning",
    "class",
    "sql",
    "databases",
    "want",
    "processing",
    "use",
    "case",
    "want",
    "dump",
    "bunch",
    "images",
    "let",
    "say",
    "1",
    "million",
    "image",
    "want",
    "process",
    "display",
    "website",
    "use",
    "hadoop",
    "hadoop",
    "used",
    "sql",
    "databases",
    "like",
    "dynamo",
    "db",
    "mongod",
    "db",
    "right",
    "sharepoint",
    "smaller",
    "setup",
    "sharepoint",
    "use",
    "one",
    "hastac",
    "uh",
    "haack",
    "built",
    "top",
    "top",
    "yeah",
    "facebooks",
    "right",
    "correct",
    "correct",
    "business",
    "use",
    "case",
    "question",
    "want",
    "data",
    "companies",
    "generate",
    "terabytes",
    "data",
    "archive",
    "data",
    "generate",
    "data",
    "want",
    "analyze",
    "archive",
    "data",
    "cares",
    "decide",
    "data",
    "right",
    "huh",
    "working",
    "ge",
    "interesting",
    "use",
    "case",
    "ge",
    "right",
    "company",
    "called",
    "ge",
    "training",
    "next",
    "week",
    "aviation",
    "department",
    "g",
    "building",
    "aircraft",
    "engine",
    "know",
    "flight",
    "engine",
    "80",
    "commercial",
    "aircraft",
    "engines",
    "ge",
    "planes",
    "flying",
    "g",
    "engine",
    "also",
    "one",
    "company",
    "aircraft",
    "engines",
    "sensors",
    "flight",
    "flying",
    "capture",
    "sensor",
    "data",
    "like",
    "pressure",
    "blah",
    "blah",
    "blah",
    "analyze",
    "predict",
    "flight",
    "crash",
    "turbulence",
    "hit",
    "something",
    "like",
    "whether",
    "going",
    "die",
    "something",
    "idea",
    "said",
    "okay",
    "good",
    "deal",
    "said",
    "good",
    "deal",
    "problem",
    "flight",
    "flying",
    "let",
    "say",
    "dubai",
    "bangalore",
    "two",
    "engines",
    "takes",
    "6",
    "hours",
    "engine",
    "generate",
    "one",
    "tb",
    "data",
    "many",
    "flight",
    "total",
    "many",
    "flights",
    "fly",
    "every",
    "day",
    "imagine",
    "amount",
    "data",
    "analyze",
    "analyze",
    "mean",
    "giving",
    "let",
    "say",
    "th000",
    "terabyte",
    "every",
    "hour",
    "every",
    "minute",
    "ca",
    "analyze",
    "take",
    "subset",
    "algorithm",
    "takes",
    "subset",
    "data",
    "analyze",
    "rest",
    "delete",
    "dump",
    "humanly",
    "possible",
    "also",
    "need",
    "much",
    "amount",
    "every",
    "sensor",
    "data",
    "need",
    "analyzed",
    "interesting",
    "also",
    "locomotive",
    "engines",
    "train",
    "train",
    "europe",
    "us",
    "run",
    "trains",
    "trains",
    "sensors",
    "get",
    "also",
    "difficult",
    "work",
    "g",
    "huge",
    "amount",
    "data",
    "also",
    "know",
    "data",
    "right",
    "clue",
    "say",
    "make",
    "sense",
    "data",
    "locomotive",
    "data",
    "huge",
    "actually",
    "trains",
    "generate",
    "lot",
    "data",
    "actually",
    "get",
    "uh",
    "take",
    "clean",
    "get",
    "part",
    "data",
    "analyze",
    "hadoop",
    "also",
    "certain",
    "file",
    "format",
    "example",
    "ao",
    "may",
    "heard",
    "ao",
    "file",
    "format",
    "use",
    "hadoop",
    "park",
    "orc",
    "used",
    "compress",
    "store",
    "data",
    "hadu",
    "also",
    "compression",
    "techniques",
    "sometimes",
    "storing",
    "data",
    "good",
    "compress",
    "drawback",
    "compression",
    "decompress",
    "need",
    "lot",
    "processing",
    "power",
    "want",
    "decompress",
    "ao",
    "serialization",
    "format",
    "compress",
    "data",
    "par",
    "compress",
    "data",
    "column",
    "push",
    "data",
    "show",
    "hive",
    "discuss",
    "hive",
    "create",
    "hive",
    "table",
    "say",
    "data",
    "compressed",
    "compress",
    "store",
    "data",
    "actually",
    "possible",
    "point",
    "everybody",
    "want",
    "analyze",
    "data",
    "world",
    "humanly",
    "think",
    "facebook",
    "much",
    "data",
    "getting",
    "probably",
    "analyzing",
    "whole",
    "data",
    "subset",
    "interesting",
    "data",
    "questions",
    "wind",
    "think",
    "pick",
    "questions",
    "huh",
    "yes",
    "spark",
    "memory",
    "intensive",
    "ideally",
    "spark",
    "requires",
    "ram",
    "give",
    "ram",
    "perform",
    "better",
    "give",
    "ram",
    "use",
    "hard",
    "disk",
    "mean",
    "rest",
    "space",
    "use",
    "hard",
    "disk",
    "performance",
    "slightly",
    "degraded",
    "streaming",
    "need",
    "lot",
    "ram",
    "problem",
    "uh",
    "football",
    "world",
    "cup",
    "going",
    "happen",
    "right",
    "yes",
    "right",
    "year",
    "right",
    "let",
    "say",
    "want",
    "write",
    "program",
    "download",
    "tweets",
    "find",
    "favorite",
    "player",
    "football",
    "world",
    "cup",
    "possible",
    "right",
    "easily",
    "stream",
    "data",
    "twitter",
    "problem",
    "want",
    "find",
    "uh",
    "know",
    "popular",
    "player",
    "every",
    "five",
    "minute",
    "imagine",
    "probably",
    "five",
    "minute",
    "want",
    "want",
    "get",
    "five",
    "minute",
    "data",
    "hold",
    "memory",
    "process",
    "five",
    "minute",
    "data",
    "huge",
    "hold",
    "memory",
    "slow",
    "yes",
    "giving",
    "im",
    "imaginary",
    "situation",
    "five",
    "minutes",
    "data",
    "terabytes",
    "ca",
    "hold",
    "actually",
    "tter",
    "data",
    "might",
    "want",
    "persist",
    "disk",
    "data",
    "processing",
    "become",
    "slow",
    "real",
    "time",
    "running",
    "spark",
    "streaming",
    "know",
    "ram",
    "available",
    "otherwise",
    "become",
    "slow",
    "today",
    "match",
    "predict",
    "tomorrow",
    "right",
    "things",
    "like",
    "happen",
    "really",
    "good",
    "actually",
    "lot",
    "considerations",
    "use",
    "things",
    "think",
    "wind",
    "think",
    "yeah",
    "almost",
    "also",
    "sl",
    "yes",
    "sharing",
    "slides",
    "also",
    "books",
    "uh",
    "couple",
    "good",
    "books",
    "written",
    "hado",
    "give",
    "lot",
    "idea",
    "basics",
    "like",
    "hdfs",
    "working",
    "things",
    "actually",
    "much",
    "need",
    "understand",
    "basically",
    "um",
    "would",
    "say",
    "whatever",
    "training",
    "enough",
    "go",
    "book",
    "also",
    "say",
    "example",
    "hdfs",
    "expert",
    "high",
    "availability",
    "see",
    "active",
    "name",
    "node",
    "standby",
    "name",
    "node",
    "active",
    "phase",
    "standby",
    "take",
    "happens",
    "process",
    "probably",
    "topic",
    "even",
    "know",
    "big",
    "deal",
    "admin",
    "topic",
    "hadoop",
    "admin",
    "know",
    "work",
    "since",
    "developer",
    "go",
    "depth",
    "topics",
    "actually",
    "map",
    "reduce",
    "hive",
    "enough",
    "think",
    "building",
    "awareness",
    "learn",
    "spark",
    "big",
    "basic",
    "idea",
    "building",
    "look",
    "spark",
    "say",
    "concentrate",
    "spark",
    "let",
    "talk",
    "hive",
    "right",
    "people",
    "might",
    "mostly",
    "interested",
    "usually",
    "slides",
    "person",
    "hive",
    "need",
    "slides",
    "around",
    "60",
    "slides",
    "run",
    "worry",
    "share",
    "go",
    "run",
    "couple",
    "get",
    "idea",
    "right",
    "otherwise",
    "everything",
    "draw",
    "explain",
    "difficult",
    "hive",
    "right",
    "hive",
    "actually",
    "simple",
    "uh",
    "2005",
    "hadoop",
    "came",
    "major",
    "vendors",
    "started",
    "using",
    "haduk",
    "like",
    "facebook",
    "yahoo",
    "uh",
    "faced",
    "one",
    "common",
    "problem",
    "problem",
    "uh",
    "yahoo",
    "started",
    "using",
    "hadoop",
    "uh",
    "yahoo",
    "lot",
    "developers",
    "know",
    "java",
    "problem",
    "write",
    "map",
    "ruce",
    "programs",
    "uh",
    "skills",
    "set",
    "problem",
    "ca",
    "say",
    "everybody",
    "learn",
    "java",
    "start",
    "writing",
    "code",
    "yahoo",
    "found",
    "troubling",
    "time",
    "facebook",
    "also",
    "started",
    "using",
    "hadoop",
    "also",
    "problem",
    "facebook",
    "mostly",
    "sql",
    "developers",
    "comfortable",
    "java",
    "way",
    "back",
    "2005",
    "6",
    "map",
    "reduced",
    "nothing",
    "else",
    "hadoop",
    "okay",
    "plain",
    "map",
    "reduce",
    "nothing",
    "else",
    "uh",
    "hadoop",
    "ecosystem",
    "saw",
    "last",
    "class",
    "none",
    "available",
    "everybody",
    "confused",
    "yahoo",
    "facebook",
    "started",
    "two",
    "projects",
    "time",
    "uh",
    "yahoo",
    "created",
    "something",
    "called",
    "pig",
    "creature",
    "pig",
    "animal",
    "pig",
    "seriously",
    "called",
    "pig",
    "pig",
    "see",
    "pig",
    "real",
    "pig",
    "creature",
    "yahoo",
    "created",
    "tool",
    "called",
    "pig",
    "scripting",
    "language",
    "teach",
    "course",
    "called",
    "pig",
    "hadoop",
    "pig",
    "interesting",
    "language",
    "scripting",
    "language",
    "uh",
    "advantage",
    "simple",
    "learn",
    "language",
    "uh",
    "whatever",
    "analysis",
    "want",
    "write",
    "language",
    "script",
    "small",
    "like",
    "four",
    "five",
    "lines",
    "max",
    "10",
    "lines",
    "write",
    "pig",
    "script",
    "say",
    "run",
    "convert",
    "pig",
    "script",
    "map",
    "ruce",
    "program",
    "learn",
    "map",
    "ruce",
    "learned",
    "pig",
    "somewhere",
    "2010",
    "11",
    "heavy",
    "job",
    "inflow",
    "pig",
    "developers",
    "80",
    "hadoop",
    "job",
    "handled",
    "pig",
    "2010",
    "80",
    "successful",
    "language",
    "everybody",
    "dying",
    "pig",
    "taken",
    "pig",
    "trainings",
    "people",
    "effect",
    "pig",
    "actually",
    "right",
    "easy",
    "language",
    "scripting",
    "language",
    "learn",
    "everything",
    "happen",
    "behind",
    "scenes",
    "want",
    "join",
    "operation",
    "say",
    "join",
    "comma",
    "b",
    "even",
    "sql",
    "queries",
    "complicated",
    "join",
    "say",
    "dot",
    "b",
    "dot",
    "nothing",
    "take",
    "b",
    "join",
    "comma",
    "b",
    "done",
    "joining",
    "done",
    "simple",
    "inner",
    "join",
    "saying",
    "people",
    "started",
    "using",
    "pig",
    "extensively",
    "time",
    "facebook",
    "developed",
    "hive",
    "hive",
    "idea",
    "different",
    "write",
    "sequel",
    "okay",
    "create",
    "databases",
    "tables",
    "blah",
    "blah",
    "blah",
    "write",
    "sql",
    "hi",
    "convert",
    "sql",
    "query",
    "map",
    "reduce",
    "concept",
    "two",
    "companies",
    "developed",
    "one",
    "pig",
    "one",
    "hive",
    "know",
    "uh",
    "thing",
    "ultimately",
    "happened",
    "somewhere",
    "around",
    "201101",
    "pig",
    "started",
    "losing",
    "attraction",
    "spark",
    "came",
    "spark",
    "came",
    "became",
    "predominantly",
    "useful",
    "spark",
    "also",
    "short",
    "like",
    "script",
    "write",
    "python",
    "easy",
    "write",
    "spark",
    "pig",
    "almost",
    "similar",
    "people",
    "thought",
    "like",
    "learning",
    "something",
    "short",
    "interesting",
    "fast",
    "learn",
    "pig",
    "learn",
    "p",
    "uh",
    "spark",
    "right",
    "pig",
    "longer",
    "used",
    "anybody",
    "rarely",
    "pig",
    "gone",
    "almost",
    "hive",
    "gone",
    "sql",
    "sql",
    "till",
    "world",
    "exist",
    "right",
    "questions",
    "asked",
    "right",
    "questions",
    "sql",
    "sql",
    "ca",
    "get",
    "rid",
    "sql",
    "right",
    "get",
    "get",
    "rid",
    "sql",
    "atl",
    "developers",
    "know",
    "huge",
    "community",
    "back",
    "sql",
    "like",
    "pig",
    "actually",
    "like",
    "pig",
    "nobody",
    "back",
    "pig",
    "right",
    "lot",
    "people",
    "back",
    "sql",
    "right",
    "reason",
    "hi",
    "beautiful",
    "data",
    "warehouse",
    "treated",
    "data",
    "warehouse",
    "hado",
    "meaning",
    "create",
    "tables",
    "uh",
    "query",
    "data",
    "visualization",
    "tools",
    "uh",
    "know",
    "use",
    "hive",
    "back",
    "end",
    "fetch",
    "rell",
    "hive",
    "became",
    "popular",
    "happening",
    "hive",
    "uh",
    "plus",
    "spark",
    "using",
    "hive",
    "spark",
    "module",
    "called",
    "spark",
    "sql",
    "uses",
    "sql",
    "language",
    "uh",
    "guy",
    "process",
    "also",
    "talk",
    "hive",
    "whatever",
    "data",
    "hive",
    "spark",
    "also",
    "read",
    "hive",
    "predominantly",
    "used",
    "days",
    "hi",
    "always",
    "uh",
    "popular",
    "uh",
    "may",
    "think",
    "one",
    "popular",
    "ecosystem",
    "tool",
    "hive",
    "far",
    "industry",
    "10",
    "years",
    "12",
    "years",
    "still",
    "uh",
    "uh",
    "community",
    "much",
    "uh",
    "um",
    "command",
    "wise",
    "simple",
    "sql",
    "think",
    "teach",
    "sql",
    "idea",
    "behind",
    "session",
    "already",
    "know",
    "sql",
    "see",
    "happening",
    "behind",
    "scenes",
    "write",
    "sql",
    "query",
    "uh",
    "advanced",
    "concepts",
    "hy",
    "like",
    "partitioning",
    "bucketing",
    "indexing",
    "uh",
    "kind",
    "things",
    "industry",
    "uh",
    "experience",
    "one",
    "major",
    "thing",
    "uh",
    "two",
    "major",
    "vendors",
    "hoton",
    "works",
    "claa",
    "right",
    "hoton",
    "works",
    "uses",
    "original",
    "apache",
    "hado",
    "propaganda",
    "hive",
    "also",
    "apache",
    "product",
    "open",
    "source",
    "right",
    "whatever",
    "major",
    "changes",
    "hive",
    "predominant",
    "hoton",
    "works",
    "claa",
    "claa",
    "actually",
    "good",
    "hi",
    "using",
    "claer",
    "lab",
    "bad",
    "actually",
    "mean",
    "blaming",
    "lab",
    "claa",
    "actually",
    "bit",
    "bad",
    "hiide",
    "tell",
    "reason",
    "teach",
    "hoton",
    "works",
    "reason",
    "able",
    "tell",
    "uh",
    "hoton",
    "works",
    "project",
    "called",
    "weird",
    "people",
    "delhi",
    "created",
    "project",
    "hindi",
    "word",
    "weird",
    "movie",
    "name",
    "also",
    "right",
    "heard",
    "speed",
    "hindi",
    "huh",
    "huh",
    "uh",
    "say",
    "replacement",
    "project",
    "created",
    "indians",
    "gave",
    "apache",
    "became",
    "open",
    "source",
    "th",
    "faster",
    "map",
    "ruce",
    "guys",
    "normally",
    "map",
    "ruce",
    "happens",
    "read",
    "data",
    "right",
    "uh",
    "mapping",
    "store",
    "output",
    "hard",
    "disk",
    "shuffle",
    "read",
    "data",
    "shuffle",
    "store",
    "output",
    "back",
    "hard",
    "disk",
    "reducer",
    "read",
    "hard",
    "disk",
    "weird",
    "guys",
    "thought",
    "memory",
    "want",
    "use",
    "hard",
    "disk",
    "read",
    "data",
    "map",
    "reduce",
    "shuffle",
    "push",
    "result",
    "map",
    "reduce",
    "becomes",
    "10",
    "times",
    "faster",
    "dag",
    "directed",
    "cyclic",
    "graph",
    "concept",
    "th",
    "given",
    "apache",
    "henbs",
    "write",
    "hi",
    "query",
    "convert",
    "taze",
    "job",
    "map",
    "reduce",
    "job",
    "hi",
    "queries",
    "faster",
    "hoton",
    "works",
    "claa",
    "cluster",
    "claa",
    "support",
    "contributed",
    "hoton",
    "hoton",
    "bs",
    "competitors",
    "right",
    "working",
    "claa",
    "cluster",
    "never",
    "see",
    "th",
    "life",
    "wo",
    "allow",
    "problem",
    "writing",
    "hi",
    "query",
    "cloud",
    "era",
    "slow",
    "convert",
    "map",
    "reduce",
    "run",
    "map",
    "reduce",
    "program",
    "takes",
    "lot",
    "time",
    "writing",
    "hi",
    "query",
    "hoton",
    "works",
    "least",
    "five",
    "times",
    "faster",
    "converts",
    "job",
    "default",
    "superb",
    "ca",
    "compare",
    "right",
    "one",
    "drawback",
    "mean",
    "start",
    "working",
    "industry",
    "days",
    "mostly",
    "people",
    "moving",
    "hoton",
    "works",
    "majority",
    "reasons",
    "stake",
    "cloud",
    "around",
    "li",
    "number",
    "users",
    "cloud",
    "era",
    "period",
    "time",
    "probably",
    "people",
    "shift",
    "hoton",
    "work",
    "good",
    "know",
    "information",
    "thinking",
    "claa",
    "answer",
    "impala",
    "product",
    "called",
    "impala",
    "apache",
    "impala",
    "impala",
    "contribution",
    "mostly",
    "claa",
    "impala",
    "thing",
    "memory",
    "fast",
    "real",
    "time",
    "queries",
    "show",
    "impala",
    "impala",
    "cloud",
    "cluster",
    "show",
    "hot",
    "also",
    "want",
    "project",
    "really",
    "uh",
    "uh",
    "uh",
    "yeah",
    "really",
    "uh",
    "say",
    "important",
    "right",
    "since",
    "spark",
    "things",
    "gone",
    "including",
    "spark",
    "also",
    "memory",
    "also",
    "memory",
    "spark",
    "came",
    "relevance",
    "relevance",
    "running",
    "hi",
    "query",
    "converts",
    "job",
    "faster",
    "actually",
    "hoton",
    "works",
    "platforms",
    "show",
    "hoton",
    "cluster",
    "probably",
    "tomorrow",
    "okay",
    "access",
    "show",
    "job",
    "running",
    "see",
    "difference",
    "actually",
    "sp",
    "lot",
    "companies",
    "run",
    "traditional",
    "hi",
    "hi",
    "like",
    "want",
    "use",
    "spar",
    "want",
    "use",
    "hi",
    "going",
    "cloud",
    "era",
    "may",
    "bit",
    "challenging",
    "hoton",
    "works",
    "default",
    "give",
    "free",
    "right",
    "fast",
    "also",
    "fall",
    "tolerant",
    "impala",
    "discuss",
    "probably",
    "tomorrow",
    "probably",
    "early",
    "discuss",
    "uh",
    "sql",
    "engines",
    "many",
    "actually",
    "category",
    "tight",
    "competition",
    "marketing",
    "gimmick",
    "go",
    "cloud",
    "say",
    "impala",
    "best",
    "world",
    "never",
    "listen",
    "hoton",
    "works",
    "hoton",
    "work",
    "say",
    "hoton",
    "works",
    "recently",
    "hive",
    "recently",
    "announced",
    "something",
    "called",
    "lp",
    "apache",
    "hive",
    "lp",
    "realtime",
    "queries",
    "using",
    "hive",
    "hive",
    "normally",
    "batch",
    "processing",
    "converting",
    "map",
    "reduce",
    "right",
    "recently",
    "apache",
    "announced",
    "support",
    "realtime",
    "queries",
    "called",
    "lp",
    "um",
    "uh",
    "uh",
    "real",
    "abbreviation",
    "know",
    "long",
    "live",
    "process",
    "real",
    "one",
    "used",
    "say",
    "long",
    "live",
    "process",
    "still",
    "beta",
    "problem",
    "lp",
    "available",
    "hoton",
    "works",
    "cloud",
    "era",
    "open",
    "source",
    "right",
    "contribution",
    "open",
    "source",
    "hive",
    "available",
    "hoton",
    "cloud",
    "get",
    "still",
    "impala",
    "impala",
    "solution",
    "talk",
    "h",
    "lot",
    "things",
    "aware",
    "industry",
    "happening",
    "right",
    "reasons",
    "people",
    "shifting",
    "towards",
    "hot",
    "work",
    "major",
    "things",
    "see",
    "compared",
    "mean",
    "cloud",
    "bad",
    "okay",
    "like",
    "also",
    "great",
    "support",
    "technical",
    "support",
    "excellent",
    "even",
    "impala",
    "lot",
    "tricks",
    "actually",
    "problem",
    "give",
    "solution",
    "uh",
    "lot",
    "people",
    "preferring",
    "open",
    "source",
    "initiatives",
    "hoton",
    "works",
    "open",
    "source",
    "problem",
    "told",
    "right",
    "g",
    "use",
    "lp",
    "none",
    "queries",
    "work",
    "time",
    "open",
    "source",
    "right",
    "work",
    "ca",
    "blame",
    "cloud",
    "fix",
    "propietary",
    "like",
    "iphone",
    "android",
    "right",
    "open",
    "source",
    "get",
    "lot",
    "features",
    "work",
    "work",
    "guarantee",
    "one",
    "thing",
    "blame",
    "internally",
    "apache",
    "say",
    "give",
    "technical",
    "support",
    "within",
    "limits",
    "originally",
    "product",
    "apache",
    "hoton",
    "works",
    "fix",
    "directly",
    "open",
    "source",
    "community",
    "things",
    "work",
    "mean",
    "saying",
    "everything",
    "work",
    "open",
    "source",
    "drawback",
    "sometimes",
    "right",
    "uh",
    "seeing",
    "lot",
    "people",
    "migrating",
    "hoton",
    "works",
    "actually",
    "days",
    "right",
    "hive",
    "need",
    "understand",
    "things",
    "provides",
    "sql",
    "like",
    "interface",
    "important",
    "point",
    "hive",
    "storage",
    "uses",
    "data",
    "hadoop",
    "example",
    "copy",
    "csv",
    "file",
    "hdfs",
    "go",
    "hve",
    "say",
    "create",
    "table",
    "give",
    "schema",
    "say",
    "load",
    "csv",
    "table",
    "going",
    "happen",
    "csv",
    "hadoop",
    "hdfs",
    "blocks",
    "replicas",
    "hi",
    "project",
    "structure",
    "get",
    "table",
    "get",
    "table",
    "sql",
    "sql",
    "right",
    "idea",
    "project",
    "structure",
    "data",
    "already",
    "available",
    "hado",
    "hi",
    "create",
    "tables",
    "variety",
    "formats",
    "structure",
    "data",
    "even",
    "log",
    "files",
    "unstructured",
    "data",
    "uh",
    "json",
    "files",
    "different",
    "types",
    "files",
    "read",
    "hive",
    "put",
    "table",
    "format",
    "right",
    "importance",
    "hive",
    "uh",
    "treated",
    "hadoop",
    "data",
    "warehouse",
    "system",
    "data",
    "hdfs",
    "giving",
    "table",
    "like",
    "structure",
    "top",
    "data",
    "hi",
    "maintain",
    "schema",
    "data",
    "already",
    "hado",
    "right",
    "load",
    "data",
    "even",
    "though",
    "say",
    "load",
    "data",
    "data",
    "become",
    "uh",
    "hadoop",
    "show",
    "load",
    "data",
    "mean",
    "create",
    "tables",
    "yeah",
    "want",
    "run",
    "query",
    "directly",
    "say",
    "write",
    "sql",
    "say",
    "select",
    "star",
    "table",
    "problem",
    "hit",
    "hi",
    "hi",
    "read",
    "query",
    "convert",
    "map",
    "reduce",
    "program",
    "java",
    "base",
    "create",
    "jar",
    "file",
    "take",
    "time",
    "runs",
    "map",
    "ruce",
    "job",
    "funniest",
    "part",
    "hadoop",
    "never",
    "understands",
    "hyp",
    "hadoop",
    "map",
    "reduce",
    "program",
    "getting",
    "point",
    "right",
    "hadoop",
    "point",
    "view",
    "nothing",
    "called",
    "hi",
    "understands",
    "map",
    "reduce",
    "whether",
    "write",
    "map",
    "reduce",
    "hi",
    "write",
    "map",
    "reduce",
    "thing",
    "right",
    "want",
    "write",
    "map",
    "reduce",
    "program",
    "query",
    "structure",
    "data",
    "difficult",
    "using",
    "hve",
    "translate",
    "create",
    "jar",
    "file",
    "push",
    "cluster",
    "cluster",
    "run",
    "map",
    "reduce",
    "regular",
    "program",
    "show",
    "output",
    "save",
    "table",
    "anything",
    "use",
    "regular",
    "sql",
    "statements",
    "say",
    "insert",
    "table",
    "table",
    "result",
    "matching",
    "sch",
    "insert",
    "way",
    "thing",
    "projecting",
    "structure",
    "hadoop",
    "hdfs",
    "everything",
    "hdfs",
    "slow",
    "hive",
    "expected",
    "slow",
    "one",
    "thing",
    "hi",
    "considered",
    "bat",
    "processing",
    "engine",
    "database",
    "another",
    "common",
    "problem",
    "people",
    "may",
    "able",
    "differentiate",
    "database",
    "data",
    "warehouse",
    "different",
    "right",
    "database",
    "tp",
    "call",
    "transactional",
    "real",
    "time",
    "right",
    "want",
    "millisecond",
    "speed",
    "right",
    "subse",
    "speed",
    "database",
    "always",
    "insert",
    "update",
    "kind",
    "operation",
    "data",
    "warehouse",
    "different",
    "data",
    "warehouse",
    "place",
    "cleaned",
    "data",
    "comes",
    "lands",
    "right",
    "reporting",
    "tools",
    "connect",
    "visualize",
    "data",
    "right",
    "hive",
    "data",
    "warehouse",
    "store",
    "final",
    "data",
    "let",
    "say",
    "lot",
    "tables",
    "ran",
    "queries",
    "got",
    "final",
    "output",
    "st",
    "store",
    "height",
    "visualization",
    "tools",
    "huh",
    "store",
    "hdfs",
    "say",
    "store",
    "hi",
    "hdfs",
    "meaning",
    "open",
    "hive",
    "say",
    "select",
    "star",
    "see",
    "table",
    "go",
    "location",
    "open",
    "file",
    "see",
    "text",
    "file",
    "thing",
    "text",
    "file",
    "open",
    "say",
    "select",
    "star",
    "text",
    "file",
    "show",
    "nice",
    "row",
    "column",
    "format",
    "another",
    "thing",
    "hi",
    "language",
    "actually",
    "called",
    "hql",
    "hi",
    "query",
    "language",
    "sql",
    "based",
    "sql",
    "92",
    "syntax",
    "queries",
    "work",
    "okay",
    "technically",
    "called",
    "hql",
    "hi",
    "query",
    "language",
    "right",
    "us",
    "decide",
    "mean",
    "um",
    "idea",
    "familiar",
    "familiarize",
    "h",
    "depending",
    "business",
    "use",
    "case",
    "write",
    "sql",
    "queries",
    "write",
    "complex",
    "queries",
    "simple",
    "queries",
    "need",
    "idea",
    "sql",
    "queries",
    "like",
    "join",
    "operation",
    "group",
    "query",
    "understand",
    "happening",
    "even",
    "person",
    "without",
    "knowing",
    "sql",
    "learn",
    "high",
    "mean",
    "looking",
    "query",
    "understand",
    "intention",
    "query",
    "right",
    "hive",
    "db",
    "hive",
    "queries",
    "take",
    "minutes",
    "even",
    "small",
    "data",
    "sets",
    "ca",
    "compared",
    "databases",
    "like",
    "oracle",
    "hive",
    "provide",
    "realtime",
    "queries",
    "original",
    "hive",
    "came",
    "converting",
    "map",
    "reduce",
    "slow",
    "came",
    "th",
    "hoton",
    "works",
    "started",
    "using",
    "high",
    "plus",
    "th",
    "bit",
    "faster",
    "right",
    "claer",
    "use",
    "th",
    "claer",
    "still",
    "runs",
    "hive",
    "map",
    "reduce",
    "answer",
    "impala",
    "impala",
    "inmemory",
    "execution",
    "engine",
    "impala",
    "work",
    "cloud",
    "era",
    "show",
    "impala",
    "query",
    "fast",
    "difference",
    "apart",
    "mean",
    "syntax",
    "everything",
    "uh",
    "uh",
    "data",
    "warehousing",
    "sense",
    "like",
    "usually",
    "used",
    "storage",
    "engine",
    "rather",
    "data",
    "warehouse",
    "data",
    "warehouse",
    "store",
    "massive",
    "data",
    "right",
    "structure",
    "data",
    "intention",
    "hive",
    "also",
    "bi",
    "tools",
    "connect",
    "hi",
    "something",
    "like",
    "tableau",
    "example",
    "tableau",
    "visualization",
    "tool",
    "want",
    "visualize",
    "terabytes",
    "data",
    "hadoop",
    "connect",
    "tableau",
    "hadoop",
    "tableau",
    "firing",
    "sql",
    "queries",
    "using",
    "hi",
    "data",
    "visualized",
    "typical",
    "application",
    "olap",
    "system",
    "olab",
    "system",
    "online",
    "analytical",
    "processing",
    "olp",
    "system",
    "ca",
    "compare",
    "like",
    "oracle",
    "like",
    "real",
    "time",
    "uh",
    "hi",
    "replace",
    "proper",
    "data",
    "warehouse",
    "okay",
    "talking",
    "something",
    "like",
    "terra",
    "dat",
    "example",
    "like",
    "say",
    "expensive",
    "fast",
    "reliable",
    "data",
    "warehouses",
    "understand",
    "hi",
    "built",
    "top",
    "hadoop",
    "drawbacks",
    "hadoop",
    "right",
    "organizations",
    "days",
    "categorize",
    "data",
    "h",
    "data",
    "cold",
    "data",
    "uh",
    "hold",
    "data",
    "go",
    "something",
    "like",
    "tera",
    "data",
    "know",
    "need",
    "immediately",
    "fish",
    "data",
    "run",
    "faster",
    "queries",
    "cold",
    "data",
    "okay",
    "take",
    "10",
    "minutes",
    "query",
    "go",
    "hive",
    "hive",
    "supports",
    "insert",
    "statements",
    "insert",
    "statements",
    "already",
    "problem",
    "insert",
    "statement",
    "fire",
    "map",
    "produce",
    "job",
    "bulk",
    "load",
    "better",
    "hive",
    "normally",
    "load",
    "data",
    "bulk",
    "load",
    "insert",
    "latest",
    "editions",
    "hy",
    "support",
    "cred",
    "operations",
    "create",
    "read",
    "update",
    "delete",
    "everything",
    "supported",
    "uh",
    "want",
    "get",
    "speed",
    "use",
    "lp",
    "saying",
    "real",
    "time",
    "queries",
    "lp",
    "lp",
    "cloud",
    "data",
    "support",
    "say",
    "cloud",
    "era",
    "platform",
    "inserts",
    "updates",
    "slow",
    "like",
    "terra",
    "terra",
    "inserts",
    "fast",
    "question",
    "data",
    "warehouse",
    "usually",
    "insert",
    "rare",
    "right",
    "bulk",
    "loading",
    "data",
    "etl",
    "tools",
    "dumping",
    "data",
    "collect",
    "data",
    "rdbms",
    "dump",
    "rarely",
    "modify",
    "data",
    "data",
    "warehouse",
    "use",
    "case",
    "like",
    "yes",
    "cred",
    "operations",
    "supported",
    "even",
    "insert",
    "update",
    "delete",
    "slow",
    "hotworks",
    "platform",
    "acid",
    "support",
    "enabl",
    "transaction",
    "management",
    "reliable",
    "database",
    "okay",
    "uh",
    "never",
    "replace",
    "rdbms",
    "system",
    "something",
    "like",
    "transaction",
    "management",
    "supported",
    "lp",
    "lp",
    "feature",
    "gives",
    "high",
    "realtime",
    "performance",
    "lp",
    "also",
    "requires",
    "resources",
    "requires",
    "resources",
    "execute",
    "get",
    "asset",
    "properties",
    "transaction",
    "management",
    "everything",
    "important",
    "understand",
    "command",
    "line",
    "interface",
    "hype",
    "called",
    "hive",
    "shell",
    "open",
    "hive",
    "shell",
    "start",
    "typing",
    "query",
    "say",
    "create",
    "database",
    "create",
    "table",
    "type",
    "query",
    "one",
    "way",
    "work",
    "hyp",
    "uh",
    "people",
    "may",
    "may",
    "use",
    "production",
    "might",
    "client",
    "right",
    "type",
    "clients",
    "use",
    "like",
    "connect",
    "uh",
    "dbs",
    "data",
    "warehouses",
    "sec",
    "server",
    "todd",
    "something",
    "know",
    "something",
    "like",
    "might",
    "either",
    "use",
    "one",
    "thing",
    "use",
    "use",
    "cli",
    "hi",
    "vast",
    "actually",
    "getting",
    "confused",
    "talk",
    "cli",
    "two",
    "clis",
    "original",
    "cli",
    "called",
    "hive",
    "simply",
    "type",
    "hive",
    "open",
    "blah",
    "blah",
    "blah",
    "new",
    "cli",
    "called",
    "bline",
    "called",
    "beline",
    "okay",
    "beline",
    "proper",
    "jdbc",
    "client",
    "bine",
    "also",
    "open",
    "cli",
    "simply",
    "say",
    "hi",
    "open",
    "command",
    "line",
    "create",
    "statements",
    "web",
    "gui",
    "rarely",
    "used",
    "hive",
    "client",
    "jdbc",
    "client",
    "open",
    "cli",
    "say",
    "hive",
    "jdbc",
    "drivers",
    "right",
    "connect",
    "want",
    "connect",
    "application",
    "right",
    "database",
    "need",
    "driver",
    "called",
    "jdbc",
    "odbc",
    "drivers",
    "one",
    "way",
    "connect",
    "uh",
    "cli",
    "original",
    "hive",
    "cli",
    "called",
    "hive",
    "client",
    "jdbc",
    "client",
    "open",
    "directly",
    "hit",
    "hive",
    "directly",
    "hit",
    "hive",
    "type",
    "queries",
    "b",
    "line",
    "proper",
    "uh",
    "jdbc",
    "client",
    "means",
    "install",
    "machine",
    "connect",
    "give",
    "server",
    "address",
    "port",
    "number",
    "hit",
    "right",
    "written",
    "something",
    "called",
    "h",
    "server",
    "whole",
    "part",
    "called",
    "h",
    "server",
    "part",
    "reason",
    "diagram",
    "written",
    "whole",
    "part",
    "called",
    "hi",
    "server",
    "connecting",
    "data",
    "warehouse",
    "server",
    "component",
    "accept",
    "queries",
    "know",
    "whatever",
    "typing",
    "right",
    "whole",
    "part",
    "whether",
    "come",
    "cli",
    "web",
    "gui",
    "jdbc",
    "hit",
    "hi",
    "server",
    "right",
    "h",
    "server",
    "component",
    "accept",
    "query",
    "write",
    "query",
    "planner",
    "parser",
    "optimizer",
    "optimize",
    "query",
    "convert",
    "map",
    "reduce",
    "program",
    "push",
    "hadoop",
    "cluster",
    "hi",
    "server",
    "h",
    "server",
    "one",
    "hi",
    "server",
    "two",
    "okay",
    "old",
    "one",
    "called",
    "hi",
    "server",
    "one",
    "nobody",
    "using",
    "bother",
    "uh",
    "latest",
    "one",
    "called",
    "hi",
    "server",
    "2",
    "okay",
    "every",
    "everybody",
    "using",
    "h",
    "server",
    "2",
    "responsible",
    "managing",
    "connection",
    "open",
    "connection",
    "h",
    "directly",
    "hit",
    "h",
    "server",
    "whatever",
    "query",
    "write",
    "guy",
    "know",
    "compiler",
    "optimizer",
    "executor",
    "get",
    "query",
    "compile",
    "optimize",
    "push",
    "map",
    "reduce",
    "program",
    "right",
    "think",
    "show",
    "go",
    "cloud",
    "manager",
    "difficult",
    "talk",
    "hive",
    "rather",
    "showing",
    "something",
    "talking",
    "fine",
    "know",
    "show",
    "something",
    "probably",
    "people",
    "understand",
    "go",
    "hi",
    "configuration",
    "uh",
    "h",
    "server",
    "h",
    "see",
    "h",
    "server",
    "2",
    "called",
    "h",
    "server",
    "2",
    "default",
    "hive",
    "server",
    "hi",
    "server",
    "2",
    "old",
    "h",
    "server",
    "one",
    "nobody",
    "using",
    "performance",
    "issues",
    "right",
    "whether",
    "hbs",
    "cloud",
    "platform",
    "server",
    "component",
    "hi",
    "called",
    "hi",
    "server",
    "2",
    "okay",
    "either",
    "come",
    "jdbc",
    "client",
    "command",
    "line",
    "hitting",
    "h",
    "server",
    "2",
    "okay",
    "uh",
    "compile",
    "query",
    "optimize",
    "query",
    "push",
    "tons",
    "optimization",
    "techniques",
    "hive",
    "uh",
    "know",
    "read",
    "query",
    "better",
    "probably",
    "discuss",
    "tomorrow",
    "optimization",
    "techniques",
    "another",
    "important",
    "point",
    "slide",
    "metast",
    "store",
    "thing",
    "meaning",
    "open",
    "hive",
    "session",
    "say",
    "create",
    "table",
    "exit",
    "go",
    "right",
    "table",
    "persist",
    "whatever",
    "creating",
    "meta",
    "store",
    "comes",
    "picture",
    "metadata",
    "stored",
    "production",
    "setup",
    "create",
    "myql",
    "server",
    "mysql",
    "server",
    "meta",
    "store",
    "configured",
    "metadata",
    "table",
    "like",
    "schema",
    "access",
    "rights",
    "everything",
    "meta",
    "store",
    "right",
    "uh",
    "u",
    "ensure",
    "metast",
    "store",
    "service",
    "running",
    "otherwise",
    "hi",
    "start",
    "needs",
    "talk",
    "metast",
    "store",
    "meta",
    "store",
    "important",
    "even",
    "spark",
    "spark",
    "talk",
    "meta",
    "store",
    "create",
    "hive",
    "table",
    "read",
    "spark",
    "talk",
    "meta",
    "store",
    "get",
    "data",
    "metadata",
    "meta",
    "store",
    "okay",
    "meta",
    "store",
    "metadata",
    "metast",
    "store",
    "service",
    "push",
    "metadata",
    "give",
    "destination",
    "show",
    "metast",
    "store",
    "know",
    "configured",
    "go",
    "hi",
    "configuration",
    "right",
    "hi",
    "meta",
    "store",
    "server",
    "thread",
    "ah",
    "database",
    "type",
    "sql",
    "right",
    "uh",
    "yes",
    "see",
    "machine",
    "configured",
    "mysql",
    "metadata",
    "pushed",
    "use",
    "metadata",
    "hi",
    "read",
    "write",
    "fine",
    "okay",
    "uses",
    "persist",
    "table",
    "information",
    "dbu",
    "create",
    "everything",
    "right",
    "much",
    "use",
    "hi",
    "something",
    "called",
    "cost",
    "based",
    "optimizer",
    "cbo",
    "cbo",
    "h",
    "know",
    "disabled",
    "enable",
    "cost",
    "based",
    "optimiz",
    "disabled",
    "disabled",
    "reason",
    "know",
    "called",
    "cbo",
    "cost",
    "based",
    "optimizer",
    "aware",
    "rdbms",
    "right",
    "write",
    "complicated",
    "query",
    "generate",
    "multiple",
    "query",
    "plans",
    "execute",
    "query",
    "optimizer",
    "hive",
    "enable",
    "write",
    "complicated",
    "query",
    "generate",
    "multiple",
    "plans",
    "based",
    "cost",
    "plan",
    "select",
    "best",
    "plan",
    "cost",
    "based",
    "optimizer",
    "disabled",
    "know",
    "disabled",
    "usually",
    "uh",
    "enable",
    "optimization",
    "done",
    "hive",
    "right",
    "one",
    "thing",
    "cbo",
    "look",
    "things",
    "also",
    "um",
    "okay",
    "configurations",
    "start",
    "hi",
    "simply",
    "say",
    "hi",
    "simply",
    "type",
    "hi",
    "start",
    "hive",
    "shell",
    "may",
    "get",
    "warning",
    "fine",
    "easily",
    "see",
    "warning",
    "saying",
    "hi",
    "cli",
    "deprecated",
    "migration",
    "beine",
    "recommended",
    "okay",
    "actually",
    "means",
    "original",
    "hi",
    "cli",
    "told",
    "beline",
    "new",
    "client",
    "saying",
    "use",
    "beline",
    "whenever",
    "possible",
    "fine",
    "mean",
    "learn",
    "hive",
    "think",
    "best",
    "way",
    "cli",
    "added",
    "commands",
    "bline",
    "section",
    "actually",
    "old",
    "high",
    "cli",
    "support",
    "uh",
    "getting",
    "lower",
    "every",
    "day",
    "uh",
    "learning",
    "purpose",
    "whether",
    "use",
    "cli",
    "bline",
    "show",
    "use",
    "bline",
    "also",
    "um",
    "h",
    "shell",
    "open",
    "right",
    "easy",
    "get",
    "started",
    "hi",
    "start",
    "creating",
    "database",
    "say",
    "create",
    "database",
    "first",
    "thing",
    "create",
    "something",
    "called",
    "database",
    "highest",
    "level",
    "abstraction",
    "say",
    "create",
    "database",
    "know",
    "something",
    "may",
    "19th",
    "um",
    "created",
    "database",
    "command",
    "create",
    "database",
    "database",
    "name",
    "calling",
    "may",
    "19th",
    "show",
    "databases",
    "see",
    "lot",
    "dbs",
    "people",
    "created",
    "right",
    "see",
    "databases",
    "got",
    "created",
    "inside",
    "hive",
    "minus",
    "main",
    "19",
    "right",
    "could",
    "created",
    "something",
    "else",
    "right",
    "whatever",
    "mean",
    "remember",
    "say",
    "use",
    "switch",
    "db",
    "say",
    "use",
    "may",
    "19",
    "regular",
    "commands",
    "aware",
    "sql",
    "right",
    "h",
    "way",
    "show",
    "display",
    "db",
    "name",
    "set",
    "uh",
    "hi",
    "property",
    "enable",
    "see",
    "db",
    "search",
    "even",
    "print",
    "column",
    "headers",
    "normally",
    "say",
    "uh",
    "select",
    "star",
    "something",
    "wo",
    "show",
    "column",
    "headers",
    "column",
    "name",
    "columns",
    "names",
    "right",
    "wo",
    "show",
    "make",
    "show",
    "command",
    "print",
    "first",
    "make",
    "sure",
    "everybody",
    "page",
    "hi",
    "print",
    "current",
    "db",
    "set",
    "high",
    "cli",
    "print",
    "current",
    "db",
    "uh",
    "print",
    "db",
    "simply",
    "say",
    "set",
    "high",
    "cli",
    "print",
    "current",
    "db",
    "equal",
    "true",
    "display",
    "db",
    "try",
    "everybody",
    "able",
    "log",
    "hi",
    "start",
    "anybody",
    "problem",
    "python",
    "right",
    "indentation",
    "indentation",
    "complicated",
    "actually",
    "miss",
    "one",
    "indentation",
    "nothing",
    "work",
    "python",
    "know",
    "lot",
    "difficult",
    "people",
    "coming",
    "java",
    "world",
    "familiar",
    "curly",
    "bracer",
    "everything",
    "start",
    "control",
    "structure",
    "indentation",
    "right",
    "editor",
    "take",
    "care",
    "think",
    "using",
    "editor",
    "metast",
    "store",
    "definition",
    "right",
    "metast",
    "store",
    "normally",
    "start",
    "h",
    "said",
    "use",
    "may9",
    "right",
    "db",
    "wo",
    "show",
    "db",
    "displaying",
    "right",
    "see",
    "show",
    "bracket",
    "current",
    "db",
    "uh",
    "display",
    "whichever",
    "data",
    "datase",
    "change",
    "show",
    "right",
    "another",
    "important",
    "point",
    "probably",
    "towards",
    "little",
    "bit",
    "towards",
    "admin",
    "site",
    "file",
    "called",
    "hi",
    "site",
    "xml",
    "hiy",
    "site",
    "xml",
    "file",
    "around",
    "thousand",
    "properties",
    "whole",
    "hive",
    "controlled",
    "file",
    "properties",
    "actually",
    "file",
    "meaning",
    "either",
    "like",
    "open",
    "session",
    "say",
    "set",
    "high",
    "blah",
    "blah",
    "blah",
    "true",
    "exit",
    "relog",
    "print",
    "dp",
    "type",
    "ask",
    "admin",
    "change",
    "hite",
    "xml",
    "uh",
    "property",
    "everybody",
    "log",
    "db",
    "visible",
    "file",
    "lot",
    "properties",
    "actually",
    "try",
    "find",
    "problem",
    "hon",
    "works",
    "past",
    "10",
    "15",
    "days",
    "finding",
    "bit",
    "difficult",
    "read",
    "cloud",
    "era",
    "configuration",
    "ah",
    "one",
    "thing",
    "easy",
    "uh",
    "open",
    "one",
    "command",
    "prompt",
    "uh",
    "web",
    "console",
    "copy",
    "uh",
    "um",
    "go",
    "cd",
    "etc",
    "hive",
    "con",
    "uh",
    "yes",
    "navigate",
    "location",
    "common",
    "cloud",
    "hoton",
    "works",
    "everybody",
    "itc",
    "hive",
    "con",
    "folder",
    "uh",
    "file",
    "called",
    "hive",
    "hund",
    "site",
    "xml",
    "file",
    "controls",
    "end",
    "end",
    "hi",
    "properties",
    "right",
    "open",
    "see",
    "properties",
    "h",
    "see",
    "hi",
    "metast",
    "store",
    "uri",
    "metast",
    "store",
    "uri",
    "remember",
    "connection",
    "things",
    "autoc",
    "convert",
    "join",
    "uh",
    "search",
    "vi",
    "editor",
    "one",
    "right",
    "type",
    "right",
    "uh",
    "set",
    "high",
    "cli",
    "oh",
    "hi",
    "dot",
    "says",
    "pattern",
    "found",
    "right",
    "mean",
    "wondering",
    "search",
    "db",
    "saying",
    "pattern",
    "db",
    "let",
    "search",
    "hive",
    "probably",
    "know",
    "uh",
    "see",
    "um",
    "say",
    "properties",
    "file",
    "probably",
    "uh",
    "another",
    "thing",
    "accessing",
    "file",
    "locally",
    "mean",
    "machine",
    "go",
    "cloud",
    "manager",
    "configuration",
    "say",
    "db",
    "somewhere",
    "think",
    "uh",
    "actually",
    "visible",
    "us",
    "properties",
    "read",
    "hite",
    "xml",
    "things",
    "hite",
    "xml",
    "actually",
    "print",
    "uh",
    "current",
    "db",
    "well",
    "intention",
    "intention",
    "let",
    "show",
    "data",
    "go",
    "folder",
    "right",
    "two",
    "files",
    "start",
    "loading",
    "data",
    "hive",
    "complicated",
    "even",
    "though",
    "queries",
    "simple",
    "people",
    "try",
    "get",
    "like",
    "watching",
    "christopher",
    "noan",
    "movie",
    "know",
    "wo",
    "understand",
    "happened",
    "finish",
    "movie",
    "also",
    "wo",
    "understand",
    "happened",
    "point",
    "write",
    "first",
    "probably",
    "okay",
    "create",
    "table",
    "everybody",
    "knows",
    "hi",
    "say",
    "create",
    "table",
    "blah",
    "blah",
    "blah",
    "create",
    "table",
    "fine",
    "next",
    "step",
    "want",
    "load",
    "data",
    "table",
    "data",
    "loaded",
    "lock",
    "file",
    "system",
    "hdfs",
    "bit",
    "tricky",
    "create",
    "table",
    "want",
    "put",
    "data",
    "right",
    "data",
    "loaded",
    "hadoop",
    "linux",
    "file",
    "system",
    "right",
    "okay",
    "show",
    "happen",
    "going",
    "happen",
    "happen",
    "data",
    "things",
    "right",
    "step",
    "step",
    "otherwise",
    "people",
    "get",
    "confused",
    "lot",
    "stage",
    "actually",
    "easy",
    "uh",
    "end",
    "day",
    "people",
    "get",
    "confused",
    "lot",
    "data",
    "happened",
    "data",
    "sometimes",
    "get",
    "local",
    "also",
    "happens",
    "etl",
    "tools",
    "sometimes",
    "dump",
    "local",
    "machine",
    "load",
    "say",
    "hi",
    "take",
    "linux",
    "dump",
    "actually",
    "copying",
    "linux",
    "hado",
    "way",
    "wanted",
    "show",
    "let",
    "look",
    "data",
    "right",
    "also",
    "small",
    "analysis",
    "uh",
    "open",
    "file",
    "need",
    "better",
    "editor",
    "actually",
    "wordpad",
    "right",
    "uh",
    "file",
    "already",
    "available",
    "uh",
    "uh",
    "data",
    "set",
    "let",
    "download",
    "okay",
    "real",
    "quick",
    "see",
    "file",
    "transaction",
    "txt",
    "tx",
    "ns1",
    "code",
    "data",
    "folder",
    "inside",
    "hi",
    "inside",
    "think",
    "uh",
    "txt",
    "file",
    "right",
    "extension",
    "visible",
    "uh",
    "give",
    "one",
    "moment",
    "install",
    "last",
    "class",
    "watching",
    "narcos",
    "ads",
    "coming",
    "right",
    "happened",
    "morning",
    "um",
    "first",
    "copy",
    "file",
    "local",
    "file",
    "system",
    "linux",
    "create",
    "hive",
    "table",
    "say",
    "load",
    "data",
    "hive",
    "table",
    "see",
    "happening",
    "process",
    "two",
    "files",
    "actually",
    "another",
    "file",
    "copy",
    "hdfs",
    "go",
    "another",
    "table",
    "show",
    "happening",
    "okay",
    "try",
    "without",
    "mean",
    "notepad",
    "anyway",
    "right",
    "fine",
    "wanted",
    "show",
    "yeah",
    "transaction",
    "data",
    "look",
    "data",
    "right",
    "uh",
    "uh",
    "take",
    "one",
    "line",
    "explain",
    "understand",
    "right",
    "take",
    "line",
    "first",
    "uh",
    "column",
    "transaction",
    "id",
    "nothing",
    "18",
    "transaction",
    "date",
    "uh",
    "customer",
    "id",
    "4244",
    "customer",
    "id",
    "okay",
    "amount",
    "spent",
    "88",
    "category",
    "items",
    "bought",
    "team",
    "sports",
    "actually",
    "bought",
    "baseball",
    "city",
    "salt",
    "lake",
    "uh",
    "state",
    "utah",
    "credit",
    "typical",
    "sports",
    "store",
    "transaction",
    "category",
    "item",
    "amount",
    "credit",
    "debit",
    "know",
    "n",
    "number",
    "lines",
    "like",
    "okay",
    "okay",
    "problem",
    "one",
    "data",
    "transactional",
    "data",
    "want",
    "create",
    "hyp",
    "table",
    "load",
    "data",
    "one",
    "thing",
    "second",
    "data",
    "file",
    "called",
    "cust",
    "see",
    "want",
    "open",
    "uh",
    "word",
    "pad",
    "yeah",
    "c",
    "customer",
    "data",
    "first",
    "column",
    "customer",
    "id",
    "first",
    "name",
    "last",
    "name",
    "age",
    "profession",
    "christina",
    "chung",
    "age",
    "55",
    "pilot",
    "like",
    "right",
    "store",
    "customer",
    "data",
    "transaction",
    "data",
    "right",
    "want",
    "figure",
    "want",
    "figure",
    "total",
    "amount",
    "spent",
    "age",
    "wise",
    "customers",
    "example",
    "want",
    "know",
    "store",
    "total",
    "total",
    "amount",
    "spent",
    "people",
    "age",
    "20",
    "30",
    "30",
    "40",
    "40",
    "50",
    "analysis",
    "want",
    "one",
    "data",
    "set",
    "transaction",
    "like",
    "much",
    "amount",
    "spending",
    "second",
    "um",
    "uh",
    "data",
    "set",
    "person",
    "name",
    "age",
    "join",
    "uh",
    "case",
    "statement",
    "figure",
    "going",
    "sql",
    "analysis",
    "intention",
    "intention",
    "show",
    "happening",
    "happening",
    "behind",
    "scene",
    "hi",
    "dealing",
    "right",
    "okay",
    "want",
    "uh",
    "first",
    "uh",
    "uh",
    "local",
    "copy",
    "data",
    "local",
    "file",
    "system",
    "analyze",
    "want",
    "copy",
    "two",
    "uh",
    "local",
    "system",
    "open",
    "ftp",
    "stuff",
    "right",
    "say",
    "ftp",
    "login",
    "hopefully",
    "might",
    "work",
    "upload",
    "files",
    "upload",
    "two",
    "files",
    "one",
    "called",
    "cust",
    "cd",
    "another",
    "one",
    "called",
    "txns",
    "one",
    "open",
    "yeah",
    "works",
    "okay",
    "need",
    "open",
    "one",
    "okay",
    "huh",
    "luckily",
    "available",
    "figure",
    "use",
    "ftp",
    "copy",
    "files",
    "linux",
    "hadoop",
    "upload",
    "hadoop",
    "wait",
    "uh",
    "need",
    "time",
    "yeah",
    "uh",
    "change",
    "extension",
    "yeah",
    "uh",
    "text",
    "file",
    "format",
    "access",
    "text",
    "file",
    "thing",
    "mention",
    "limiter",
    "comma",
    "ever",
    "hi",
    "handle",
    "text",
    "csv",
    "json",
    "xml",
    "variety",
    "par",
    "multiple",
    "file",
    "formats",
    "handle",
    "default",
    "read",
    "text",
    "json",
    "guys",
    "familiar",
    "something",
    "called",
    "serd",
    "heard",
    "serd",
    "serd",
    "stands",
    "serializer",
    "deserializer",
    "okay",
    "uh",
    "common",
    "practice",
    "happen",
    "let",
    "say",
    "json",
    "file",
    "know",
    "json",
    "right",
    "key",
    "value",
    "huh",
    "like",
    "key",
    "value",
    "pair",
    "json",
    "file",
    "ask",
    "h",
    "create",
    "table",
    "problem",
    "json",
    "semi",
    "structure",
    "data",
    "proper",
    "comma",
    "separated",
    "something",
    "say",
    "know",
    "talking",
    "ca",
    "find",
    "schema",
    "use",
    "something",
    "called",
    "json",
    "serd",
    "download",
    "jar",
    "file",
    "know",
    "whether",
    "latest",
    "h",
    "download",
    "jar",
    "file",
    "called",
    "json",
    "serd",
    "okay",
    "add",
    "hive",
    "say",
    "create",
    "table",
    "using",
    "sd",
    "using",
    "serd",
    "parse",
    "json",
    "key",
    "become",
    "column",
    "value",
    "become",
    "data",
    "like",
    "serdes",
    "used",
    "uh",
    "say",
    "read",
    "unstructured",
    "data",
    "show",
    "example",
    "unstructured",
    "data",
    "use",
    "something",
    "called",
    "reject",
    "serd",
    "regular",
    "expressions",
    "use",
    "data",
    "hi",
    "supports",
    "number",
    "series",
    "actually",
    "read",
    "different",
    "types",
    "data",
    "json",
    "see",
    "data",
    "show",
    "open",
    "source",
    "uh",
    "projects",
    "internally",
    "nothing",
    "happens",
    "read",
    "key",
    "value",
    "json",
    "serd",
    "give",
    "structure",
    "like",
    "table",
    "hive",
    "hive",
    "looks",
    "hi",
    "see",
    "key",
    "value",
    "pair",
    "say",
    "column",
    "header",
    "value",
    "apply",
    "thing",
    "reading",
    "semi",
    "structure",
    "data",
    "giving",
    "structure",
    "parser",
    "parsers",
    "right",
    "parser",
    "actually",
    "thing",
    "able",
    "able",
    "upload",
    "everybody",
    "find",
    "file",
    "file",
    "called",
    "commands",
    "see",
    "whether",
    "uh",
    "commands",
    "high",
    "folder",
    "file",
    "called",
    "commands",
    "uh",
    "uh",
    "uh",
    "commands",
    "type",
    "copy",
    "paste",
    "upload",
    "rug",
    "data",
    "folder",
    "called",
    "ragu",
    "okay",
    "data",
    "resides",
    "um",
    "let",
    "close",
    "actually",
    "create",
    "table",
    "let",
    "copy",
    "paste",
    "um",
    "create",
    "database",
    "first",
    "two",
    "steps",
    "already",
    "done",
    "created",
    "database",
    "said",
    "use",
    "okay",
    "first",
    "thing",
    "need",
    "command",
    "explain",
    "copy",
    "create",
    "table",
    "go",
    "paste",
    "add",
    "semicolon",
    "end",
    "hit",
    "enter",
    "add",
    "semicolon",
    "otherwise",
    "work",
    "hit",
    "enter",
    "yeah",
    "typical",
    "uh",
    "create",
    "table",
    "command",
    "sql",
    "says",
    "create",
    "table",
    "table",
    "name",
    "transaction",
    "records",
    "whatever",
    "name",
    "want",
    "give",
    "within",
    "brackets",
    "mentioning",
    "schema",
    "data",
    "hi",
    "support",
    "simple",
    "complex",
    "data",
    "types",
    "aware",
    "data",
    "types",
    "simple",
    "data",
    "types",
    "like",
    "integer",
    "string",
    "also",
    "supports",
    "complex",
    "data",
    "types",
    "like",
    "map",
    "strs",
    "right",
    "supported",
    "starting",
    "simple",
    "saying",
    "even",
    "date",
    "data",
    "type",
    "uh",
    "important",
    "point",
    "schema",
    "schema",
    "mentioning",
    "even",
    "data",
    "type",
    "date",
    "using",
    "saying",
    "string",
    "hi",
    "supports",
    "date",
    "handling",
    "uh",
    "important",
    "point",
    "row",
    "format",
    "limited",
    "fields",
    "terminated",
    "comma",
    "means",
    "hi",
    "expecting",
    "data",
    "row",
    "row",
    "limiter",
    "comma",
    "means",
    "load",
    "comma",
    "uh",
    "separated",
    "files",
    "ideally",
    "right",
    "tell",
    "happen",
    "something",
    "else",
    "let",
    "say",
    "using",
    "comma",
    "happen",
    "show",
    "right",
    "simply",
    "crea",
    "creting",
    "table",
    "record",
    "reader",
    "hive",
    "read",
    "define",
    "okay",
    "table",
    "created",
    "right",
    "created",
    "hive",
    "accessed",
    "hue",
    "hue",
    "right",
    "show",
    "auto",
    "complete",
    "mostly",
    "hard",
    "way",
    "working",
    "h",
    "auto",
    "complete",
    "type",
    "everything",
    "manually",
    "important",
    "thing",
    "load",
    "data",
    "right",
    "lot",
    "things",
    "want",
    "talk",
    "loading",
    "data",
    "first",
    "identify",
    "data",
    "right",
    "case",
    "data",
    "location",
    "make",
    "sure",
    "understand",
    "location",
    "data",
    "case",
    "location",
    "copy",
    "data",
    "txns",
    "say",
    "load",
    "data",
    "local",
    "input",
    "path",
    "table",
    "command",
    "uh",
    "path",
    "found",
    "went",
    "uh",
    "location",
    "pwd",
    "see",
    "data",
    "right",
    "linux",
    "linux",
    "easy",
    "make",
    "mistake",
    "say",
    "path",
    "found",
    "load",
    "correct",
    "path",
    "loading",
    "data",
    "right",
    "easy",
    "identify",
    "whether",
    "command",
    "correct",
    "case",
    "see",
    "says",
    "loading",
    "data",
    "means",
    "working",
    "fine",
    "also",
    "getting",
    "thing",
    "make",
    "sure",
    "local",
    "path",
    "okay",
    "people",
    "get",
    "confused",
    "local",
    "path",
    "yeah",
    "done",
    "analysis",
    "loaded",
    "data",
    "write",
    "sql",
    "query",
    "writing",
    "sql",
    "write",
    "java",
    "program",
    "know",
    "java",
    "jar",
    "file",
    "create",
    "jar",
    "file",
    "actually",
    "mention",
    "folder",
    "load",
    "files",
    "folder",
    "mention",
    "exact",
    "file",
    "mentioned",
    "folder",
    "name",
    "two",
    "files",
    "loaded",
    "everything",
    "since",
    "data",
    "warehouse",
    "happens",
    "two",
    "files",
    "tret",
    "another",
    "problem",
    "right",
    "even",
    "know",
    "file",
    "size",
    "bytes",
    "think",
    "uh",
    "roughly",
    "4",
    "mb",
    "right",
    "4",
    "mb",
    "size",
    "file",
    "actually",
    "part",
    "fine",
    "actually",
    "mean",
    "loading",
    "data",
    "want",
    "check",
    "simply",
    "select",
    "star",
    "txn",
    "records",
    "limit",
    "five",
    "try",
    "select",
    "star",
    "table",
    "limit",
    "five",
    "simply",
    "run",
    "select",
    "st",
    "around",
    "records",
    "think",
    "limit",
    "want",
    "see",
    "top",
    "five",
    "rows",
    "right",
    "command",
    "make",
    "sure",
    "data",
    "loaded",
    "uh",
    "upload",
    "data",
    "local",
    "well",
    "hado",
    "showing",
    "show",
    "hadoop",
    "also",
    "possible",
    "h",
    "select",
    "star",
    "mean",
    "select",
    "everything",
    "table",
    "want",
    "show",
    "see",
    "full",
    "table",
    "right",
    "limit",
    "five",
    "means",
    "top",
    "five",
    "rows",
    "want",
    "see",
    "actually",
    "want",
    "see",
    "query",
    "right",
    "write",
    "query",
    "matter",
    "simply",
    "say",
    "select",
    "count",
    "star",
    "select",
    "count",
    "star",
    "query",
    "see",
    "moment",
    "run",
    "query",
    "firing",
    "map",
    "produce",
    "job",
    "see",
    "query",
    "fire",
    "map",
    "reduce",
    "jaw",
    "converting",
    "see",
    "map",
    "zero",
    "reduce",
    "zero",
    "cluster",
    "really",
    "good",
    "telling",
    "around",
    "10",
    "nodes",
    "cluster",
    "think",
    "around",
    "20",
    "people",
    "using",
    "less",
    "data",
    "fast",
    "actually",
    "slow",
    "unexpected",
    "hive",
    "fast",
    "problem",
    "troubleshoot",
    "means",
    "select",
    "star",
    "select",
    "star",
    "means",
    "show",
    "full",
    "table",
    "limit",
    "five",
    "want",
    "see",
    "top",
    "five",
    "rows",
    "verifying",
    "data",
    "loaded",
    "top",
    "five",
    "rows",
    "rows",
    "simply",
    "select",
    "star",
    "display",
    "want",
    "want",
    "see",
    "select",
    "fire",
    "map",
    "producer",
    "simple",
    "read",
    "need",
    "calculation",
    "select",
    "star",
    "uh",
    "wo",
    "fire",
    "map",
    "reduce",
    "job",
    "simply",
    "show",
    "output",
    "let",
    "know",
    "able",
    "till",
    "select",
    "count",
    "star",
    "fire",
    "map",
    "reduce",
    "job",
    "see",
    "output",
    "uh",
    "one",
    "thing",
    "want",
    "file",
    "given",
    "right",
    "commands",
    "file",
    "uh",
    "might",
    "slightly",
    "wrong",
    "wrong",
    "ah",
    "copy",
    "paste",
    "saying",
    "write",
    "use",
    "different",
    "session",
    "also",
    "uh",
    "open",
    "say",
    "load",
    "data",
    "path",
    "see",
    "saying",
    "load",
    "data",
    "path",
    "actually",
    "load",
    "data",
    "local",
    "paath",
    "right",
    "follow",
    "thing",
    "correct",
    "also",
    "whatever",
    "running",
    "correct",
    "uh",
    "reflected",
    "right",
    "otherwise",
    "tomorrow",
    "try",
    "may",
    "work",
    "way",
    "expect",
    "map",
    "reduce",
    "uh",
    "output",
    "mean",
    "firing",
    "map",
    "ruce",
    "job",
    "right",
    "map",
    "ruce",
    "logs",
    "displayed",
    "suppress",
    "way",
    "suppress",
    "normally",
    "submit",
    "jar",
    "file",
    "map",
    "reduce",
    "displays",
    "thing",
    "mpp",
    "reducer",
    "one",
    "finally",
    "see",
    "result",
    "also",
    "default",
    "nature",
    "actually",
    "hi",
    "even",
    "though",
    "hi",
    "query",
    "hy",
    "query",
    "map",
    "reduce",
    "program",
    "shows",
    "map",
    "good",
    "troubleshooting",
    "sometimes",
    "show",
    "uh",
    "errors",
    "warnings",
    "running",
    "show",
    "enable",
    "column",
    "right",
    "oh",
    "select",
    "star",
    "right",
    "want",
    "column",
    "names",
    "set",
    "hi",
    "cli",
    "print",
    "header",
    "equal",
    "true",
    "okay",
    "countstar",
    "right",
    "call",
    "definitely",
    "analysis",
    "call",
    "select",
    "star",
    "call",
    "reading",
    "data",
    "able",
    "load",
    "data",
    "least",
    "run",
    "basic",
    "query",
    "okay",
    "asking",
    "see",
    "java",
    "code",
    "high",
    "writing",
    "found",
    "far",
    "internal",
    "hy",
    "mean",
    "converting",
    "java",
    "pro",
    "actually",
    "map",
    "reduce",
    "example",
    "write",
    "join",
    "map",
    "ruce",
    "select",
    "statement",
    "normal",
    "select",
    "star",
    "simply",
    "showing",
    "right",
    "convert",
    "map",
    "condition",
    "internally",
    "logic",
    "everything",
    "key",
    "value",
    "pair",
    "end",
    "day",
    "ah",
    "everything",
    "another",
    "drawback",
    "hive",
    "good",
    "know",
    "happening",
    "behind",
    "scene",
    "like",
    "end",
    "day",
    "code",
    "written",
    "know",
    "see",
    "result",
    "enough",
    "know",
    "sql",
    "fire",
    "query",
    "right",
    "hado",
    "hive",
    "huh",
    "hadoop",
    "hive",
    "hadoop",
    "hadoop",
    "visible",
    "hive",
    "anyway",
    "important",
    "point",
    "actually",
    "want",
    "show",
    "running",
    "query",
    "table",
    "write",
    "sql",
    "query",
    "intention",
    "real",
    "question",
    "data",
    "first",
    "question",
    "right",
    "loaded",
    "data",
    "uh",
    "local",
    "said",
    "means",
    "linux",
    "first",
    "thing",
    "understand",
    "data",
    "must",
    "hadoop",
    "load",
    "data",
    "could",
    "gone",
    "hadoop",
    "sure",
    "hado",
    "right",
    "understand",
    "need",
    "um",
    "describe",
    "command",
    "let",
    "say",
    "describe",
    "txn",
    "records",
    "common",
    "command",
    "describe",
    "table",
    "name",
    "show",
    "columns",
    "data",
    "types",
    "want",
    "also",
    "say",
    "describe",
    "formatted",
    "table",
    "name",
    "describe",
    "formatted",
    "give",
    "details",
    "table",
    "created",
    "hit",
    "enter",
    "interesting",
    "thing",
    "see",
    "thing",
    "says",
    "table",
    "type",
    "managed",
    "table",
    "table",
    "create",
    "h",
    "either",
    "managed",
    "table",
    "something",
    "called",
    "external",
    "table",
    "default",
    "everything",
    "manage",
    "table",
    "tell",
    "mean",
    "manage",
    "table",
    "okay",
    "clearly",
    "say",
    "uh",
    "see",
    "says",
    "manage",
    "table",
    "point",
    "number",
    "one",
    "point",
    "number",
    "two",
    "displays",
    "something",
    "called",
    "location",
    "right",
    "says",
    "location",
    "says",
    "user",
    "hive",
    "warehouse",
    "want",
    "one",
    "thing",
    "go",
    "hue",
    "file",
    "browser",
    "go",
    "thing",
    "go",
    "hue",
    "go",
    "user",
    "folder",
    "folder",
    "called",
    "user",
    "go",
    "oh",
    "think",
    "lot",
    "users",
    "actually",
    "n",
    "number",
    "users",
    "okay",
    "one",
    "thing",
    "user",
    "slash",
    "hive",
    "simply",
    "type",
    "user",
    "sl",
    "hiive",
    "land",
    "page",
    "happen",
    "install",
    "hive",
    "something",
    "called",
    "warehouse",
    "folder",
    "hi",
    "ask",
    "create",
    "normally",
    "hado",
    "clusters",
    "user",
    "hi",
    "inside",
    "folder",
    "called",
    "warehouse",
    "okay",
    "open",
    "folder",
    "see",
    "dbs",
    "people",
    "created",
    "open",
    "db",
    "database",
    "uh",
    "may",
    "19",
    "database",
    "may",
    "19",
    "open",
    "folder",
    "called",
    "txn",
    "records",
    "nothing",
    "table",
    "open",
    "data",
    "try",
    "find",
    "data",
    "point",
    "uh",
    "going",
    "happen",
    "whenever",
    "creating",
    "db",
    "hadoop",
    "folder",
    "user",
    "high",
    "warehouse",
    "inside",
    "create",
    "folder",
    "db",
    "name",
    "create",
    "table",
    "folder",
    "table",
    "name",
    "say",
    "load",
    "data",
    "simply",
    "copying",
    "data",
    "able",
    "reach",
    "point",
    "inan",
    "place",
    "also",
    "something",
    "interesting",
    "simply",
    "go",
    "delete",
    "data",
    "deleted",
    "data",
    "right",
    "select",
    "star",
    "uh",
    "sorry",
    "fired",
    "query",
    "table",
    "say",
    "zero",
    "data",
    "right",
    "let",
    "run",
    "yeah",
    "select",
    "star",
    "nothing",
    "select",
    "star",
    "table",
    "data",
    "deleted",
    "right",
    "manually",
    "copy",
    "data",
    "want",
    "type",
    "low",
    "data",
    "local",
    "path",
    "copy",
    "transaction",
    "location",
    "huh",
    "schema",
    "data",
    "manually",
    "uploaded",
    "data",
    "go",
    "back",
    "able",
    "see",
    "table",
    "data",
    "goes",
    "copying",
    "saying",
    "low",
    "data",
    "local",
    "paath",
    "data",
    "goes",
    "user",
    "high",
    "warehouse",
    "db",
    "folder",
    "table",
    "folder",
    "ds",
    "okay",
    "able",
    "see",
    "folder",
    "think",
    "yes",
    "going",
    "confuse",
    "okay",
    "understood",
    "much",
    "going",
    "confuse",
    "uh",
    "loaded",
    "data",
    "local",
    "file",
    "system",
    "one",
    "data",
    "right",
    "one",
    "file",
    "called",
    "um",
    "uh",
    "customer",
    "right",
    "want",
    "upload",
    "hadoop",
    "going",
    "also",
    "along",
    "go",
    "h",
    "okay",
    "uh",
    "home",
    "folder",
    "ragu",
    "okay",
    "upload",
    "data",
    "step",
    "number",
    "one",
    "cust",
    "file",
    "customer",
    "file",
    "copy",
    "hado",
    "home",
    "f",
    "folder",
    "wherever",
    "want",
    "uh",
    "bit",
    "confusing",
    "mean",
    "remember",
    "location",
    "copied",
    "fine",
    "uploaded",
    "want",
    "copy",
    "customer",
    "table",
    "first",
    "create",
    "customer",
    "table",
    "right",
    "customer",
    "table",
    "go",
    "command",
    "create",
    "table",
    "customer",
    "see",
    "find",
    "sales",
    "based",
    "age",
    "group",
    "command",
    "copy",
    "command",
    "come",
    "back",
    "say",
    "paste",
    "create",
    "customer",
    "table",
    "fine",
    "load",
    "data",
    "linux",
    "already",
    "know",
    "okay",
    "want",
    "want",
    "load",
    "hadoop",
    "say",
    "load",
    "data",
    "inpath",
    "say",
    "local",
    "say",
    "inpath",
    "say",
    "uh",
    "ragu",
    "slash",
    "cousins",
    "table",
    "customer",
    "try",
    "difference",
    "using",
    "local",
    "say",
    "load",
    "data",
    "path",
    "give",
    "hadoop",
    "location",
    "want",
    "one",
    "thing",
    "upload",
    "data",
    "hdfs",
    "cust",
    "file",
    "uh",
    "check",
    "data",
    "see",
    "problem",
    "move",
    "ah",
    "see",
    "uh",
    "see",
    "case",
    "ragu",
    "refresh",
    "gone",
    "uh",
    "tables",
    "creating",
    "called",
    "managed",
    "table",
    "describe",
    "showed",
    "managed",
    "table",
    "managed",
    "table",
    "manage",
    "table",
    "table",
    "high",
    "manage",
    "data",
    "control",
    "hadoop",
    "already",
    "selected",
    "location",
    "dump",
    "file",
    "even",
    "though",
    "say",
    "load",
    "data",
    "hadoop",
    "location",
    "cut",
    "dump",
    "go",
    "back",
    "user",
    "high",
    "warehouse",
    "folder",
    "data",
    "sure",
    "control",
    "data",
    "control",
    "try",
    "whether",
    "see",
    "data",
    "user",
    "hive",
    "warehouse",
    "house",
    "location",
    "never",
    "projection",
    "ah",
    "pointer",
    "projection",
    "metadata",
    "get",
    "updated",
    "happening",
    "say",
    "move",
    "folder",
    "folder",
    "rather",
    "moving",
    "name",
    "node",
    "update",
    "metadata",
    "point",
    "happening",
    "mean",
    "place",
    "gone",
    "already",
    "uploaded",
    "data",
    "gone",
    "happen",
    "lo",
    "local",
    "delete",
    "local",
    "okay",
    "bit",
    "confusing",
    "works",
    "mean",
    "people",
    "think",
    "hey",
    "data",
    "happened",
    "file",
    "right",
    "managing",
    "data",
    "allow",
    "keep",
    "data",
    "user",
    "highhouse",
    "one",
    "thing",
    "called",
    "manage",
    "table",
    "creating",
    "called",
    "manage",
    "table",
    "ideally",
    "done",
    "everything",
    "correct",
    "like",
    "show",
    "tables",
    "see",
    "two",
    "tables",
    "uh",
    "also",
    "data",
    "tables",
    "verify",
    "show",
    "tables",
    "select",
    "star",
    "data",
    "tables",
    "analysis",
    "want",
    "right",
    "h",
    "data",
    "use",
    "sql",
    "familiar",
    "sql",
    "guess",
    "sql",
    "powerful",
    "language",
    "world",
    "analysis",
    "nothing",
    "beat",
    "sql",
    "even",
    "though",
    "know",
    "much",
    "sql",
    "sql",
    "guy",
    "might",
    "write",
    "complex",
    "queries",
    "end",
    "day",
    "idea",
    "know",
    "happening",
    "queries",
    "anybody",
    "write",
    "maybe",
    "know",
    "even",
    "know",
    "write",
    "queries",
    "take",
    "help",
    "somebody",
    "write",
    "query",
    "fire",
    "query",
    "happening",
    "running",
    "tell",
    "interesting",
    "story",
    "friend",
    "mine",
    "uh",
    "working",
    "cape",
    "germany",
    "went",
    "train",
    "cape",
    "germany",
    "training",
    "team",
    "friend",
    "also",
    "sitting",
    "class",
    "knew",
    "okay",
    "asking",
    "participants",
    "many",
    "using",
    "hadoop",
    "hi",
    "said",
    "etl",
    "guys",
    "point",
    "running",
    "hoton",
    "works",
    "hoton",
    "works",
    "tool",
    "called",
    "hawk",
    "h",
    "awq",
    "hawk",
    "uh",
    "hawk",
    "like",
    "hive",
    "modification",
    "like",
    "hawk",
    "become",
    "lp",
    "fair",
    "previously",
    "used",
    "call",
    "hawk",
    "okay",
    "hawk",
    "actually",
    "connection",
    "hive",
    "end",
    "day",
    "people",
    "using",
    "hawkk",
    "past",
    "two",
    "years",
    "know",
    "using",
    "hadoop",
    "sql",
    "developer",
    "really",
    "matter",
    "write",
    "sql",
    "query",
    "really",
    "care",
    "hitting",
    "processing",
    "query",
    "really",
    "care",
    "idea",
    "run",
    "query",
    "right",
    "hawk",
    "lp",
    "uh",
    "hawk",
    "improvement",
    "hi",
    "project",
    "discontinued",
    "become",
    "lp",
    "real",
    "time",
    "acid",
    "queries",
    "hy",
    "previously",
    "known",
    "hawk",
    "went",
    "like",
    "3",
    "years",
    "back",
    "time",
    "running",
    "hulk",
    "know",
    "hado",
    "queries",
    "hado",
    "saying",
    "sql",
    "developer",
    "mostly",
    "really",
    "care",
    "query",
    "running",
    "care",
    "query",
    "actually",
    "running",
    "problem",
    "performance",
    "issue",
    "look",
    "saying",
    "asking",
    "actually",
    "use",
    "hadu",
    "said",
    "never",
    "used",
    "hadu",
    "actually",
    "running",
    "queries",
    "2",
    "years",
    "almost",
    "okay",
    "data",
    "want",
    "uh",
    "see",
    "want",
    "different",
    "different",
    "way",
    "many",
    "ways",
    "first",
    "thing",
    "going",
    "going",
    "create",
    "table",
    "get",
    "commands",
    "called",
    "one",
    "um",
    "creating",
    "table",
    "called",
    "one",
    "look",
    "schema",
    "table",
    "understand",
    "join",
    "table",
    "meaning",
    "want",
    "join",
    "want",
    "store",
    "result",
    "right",
    "usually",
    "want",
    "ideally",
    "push",
    "table",
    "also",
    "creating",
    "empty",
    "table",
    "store",
    "join",
    "results",
    "join",
    "operation",
    "table",
    "hold",
    "result",
    "join",
    "query",
    "simple",
    "uh",
    "know",
    "sql",
    "think",
    "easy",
    "insert",
    "overr",
    "right",
    "statement",
    "table",
    "typical",
    "inner",
    "joint",
    "right",
    "say",
    "customer",
    "number",
    "first",
    "name",
    "age",
    "profession",
    "uh",
    "one",
    "table",
    "b",
    "amount",
    "product",
    "another",
    "table",
    "uh",
    "customer",
    "join",
    "transaction",
    "records",
    "b",
    "joining",
    "colum",
    "customer",
    "number",
    "understand",
    "merging",
    "two",
    "tables",
    "produce",
    "result",
    "want",
    "store",
    "somewhere",
    "right",
    "created",
    "empty",
    "table",
    "called",
    "one",
    "push",
    "typical",
    "syntax",
    "join",
    "means",
    "one",
    "mean",
    "customer",
    "table",
    "b",
    "transaction",
    "records",
    "table",
    "table",
    "want",
    "columns",
    "customer",
    "number",
    "first",
    "name",
    "age",
    "profession",
    "b",
    "want",
    "amount",
    "product",
    "whenever",
    "want",
    "join",
    "common",
    "column",
    "join",
    "makes",
    "sense",
    "common",
    "column",
    "customer",
    "number",
    "one",
    "common",
    "column",
    "customer",
    "number",
    "called",
    "simple",
    "inner",
    "join",
    "outer",
    "joints",
    "left",
    "outer",
    "right",
    "outer",
    "full",
    "outer",
    "everything",
    "work",
    "insert",
    "right",
    "table",
    "data",
    "simply",
    "dump",
    "already",
    "data",
    "delete",
    "everything",
    "load",
    "also",
    "append",
    "simply",
    "happend",
    "okay",
    "sql",
    "uh",
    "guys",
    "question",
    "thought",
    "bug",
    "questions",
    "mean",
    "uh",
    "schema",
    "right",
    "sorry",
    "sch",
    "scha",
    "ah",
    "right",
    "know",
    "yes",
    "rdbms",
    "typical",
    "systems",
    "called",
    "schema",
    "right",
    "meaning",
    "create",
    "table",
    "let",
    "say",
    "table",
    "six",
    "columns",
    "try",
    "insert",
    "seven",
    "column",
    "data",
    "happen",
    "say",
    "violated",
    "insert",
    "seven",
    "columns",
    "six",
    "column",
    "table",
    "called",
    "schema",
    "right",
    "validate",
    "schema",
    "writing",
    "data",
    "hive",
    "schema",
    "read",
    "write",
    "meaning",
    "create",
    "hive",
    "table",
    "upload",
    "mp3",
    "file",
    "properly",
    "upload",
    "complaints",
    "upload",
    "movie",
    "properly",
    "uploaded",
    "complaints",
    "validate",
    "uploading",
    "right",
    "say",
    "load",
    "data",
    "simply",
    "copy",
    "dump",
    "folder",
    "query",
    "data",
    "validate",
    "show",
    "error",
    "saying",
    "okay",
    "uh",
    "highway",
    "schema",
    "read",
    "faster",
    "uploads",
    "otherwise",
    "validating",
    "everything",
    "take",
    "time",
    "load",
    "data",
    "data",
    "warehouse",
    "right",
    "think",
    "data",
    "warehouses",
    "schema",
    "read",
    "know",
    "much",
    "things",
    "hi",
    "definitely",
    "schema",
    "read",
    "right",
    "schema",
    "write",
    "typically",
    "traditional",
    "systems",
    "run",
    "join",
    "query",
    "see",
    "whether",
    "join",
    "happens",
    "ah",
    "faster",
    "impire",
    "faster",
    "think",
    "like",
    "hi",
    "faster",
    "problem",
    "actually",
    "actually",
    "troubleshoot",
    "ideally",
    "select",
    "verify",
    "data",
    "aels",
    "select",
    "star",
    "one",
    "limit",
    "5",
    "see",
    "join",
    "uh",
    "output",
    "see",
    "details",
    "customer",
    "data",
    "like",
    "camerone",
    "59",
    "actor",
    "details",
    "table",
    "right",
    "like",
    "amount",
    "product",
    "um",
    "things",
    "transaction",
    "table",
    "right",
    "see",
    "join",
    "result",
    "joined",
    "table",
    "uh",
    "join",
    "two",
    "tables",
    "actually",
    "uh",
    "logically",
    "speaking",
    "use",
    "something",
    "called",
    "case",
    "statement",
    "sql",
    "aware",
    "easy",
    "others",
    "case",
    "statement",
    "simple",
    "say",
    "look",
    "age",
    "column",
    "okay",
    "age",
    "20",
    "30",
    "categorize",
    "customer",
    "something",
    "30",
    "40",
    "something",
    "want",
    "group",
    "right",
    "sum",
    "transactions",
    "sql",
    "easily",
    "using",
    "something",
    "called",
    "case",
    "statement",
    "okay",
    "going",
    "store",
    "output",
    "probably",
    "create",
    "table",
    "creating",
    "intermediate",
    "tables",
    "create",
    "table",
    "called",
    "two",
    "table",
    "called",
    "two",
    "look",
    "schema",
    "table",
    "last",
    "column",
    "called",
    "level",
    "column",
    "exist",
    "place",
    "case",
    "push",
    "result",
    "right",
    "level",
    "column",
    "low",
    "medium",
    "based",
    "age",
    "customer",
    "push",
    "data",
    "okay",
    "write",
    "case",
    "statement",
    "one",
    "way",
    "say",
    "select",
    "everything",
    "open",
    "case",
    "age",
    "less",
    "30",
    "mark",
    "customer",
    "low",
    "age",
    "30",
    "50",
    "middle",
    "50",
    "old",
    "else",
    "others",
    "classification",
    "happen",
    "add",
    "last",
    "column",
    "table",
    "default",
    "simple",
    "case",
    "go",
    "last",
    "column",
    "writing",
    "simple",
    "case",
    "last",
    "column",
    "added",
    "uh",
    "conditions",
    "added",
    "whatever",
    "condition",
    "mentioning",
    "uh",
    "saying",
    "low",
    "low",
    "medium",
    "high",
    "right",
    "three",
    "three",
    "categories",
    "mentioning",
    "see",
    "case",
    "statement",
    "right",
    "hive",
    "full",
    "sql",
    "ca",
    "really",
    "help",
    "mean",
    "way",
    "write",
    "hive",
    "right",
    "without",
    "using",
    "sql",
    "write",
    "anything",
    "hive",
    "right",
    "worry",
    "understand",
    "sql",
    "get",
    "idea",
    "like",
    "happen",
    "uh",
    "high",
    "extensive",
    "l",
    "used",
    "mean",
    "popular",
    "tool",
    "say",
    "uh",
    "means",
    "last",
    "part",
    "uh",
    "created",
    "table",
    "called",
    "uh",
    "uh",
    "select",
    "star",
    "limit",
    "5",
    "see",
    "look",
    "last",
    "column",
    "every",
    "customer",
    "classified",
    "old",
    "middle",
    "right",
    "something",
    "called",
    "group",
    "query",
    "group",
    "based",
    "lower",
    "middle",
    "sum",
    "thing",
    "get",
    "final",
    "output",
    "show",
    "hive",
    "running",
    "okay",
    "um",
    "create",
    "table",
    "called",
    "three",
    "want",
    "tell",
    "query",
    "let",
    "see",
    "many",
    "tell",
    "third",
    "table",
    "created",
    "okay",
    "uh",
    "group",
    "um",
    "uh",
    "know",
    "column",
    "level",
    "yeah",
    "mean",
    "asking",
    "simply",
    "selecting",
    "data",
    "grouping",
    "counting",
    "final",
    "table",
    "select",
    "star",
    "see",
    "final",
    "result",
    "select",
    "star",
    "three",
    "see",
    "low",
    "middle",
    "old",
    "total",
    "amount",
    "spend",
    "spending",
    "low",
    "think",
    "bigger",
    "right",
    "older",
    "think",
    "older",
    "people",
    "spending",
    "actually",
    "know",
    "maybe",
    "right",
    "huh",
    "based",
    "selling",
    "retail",
    "store",
    "right",
    "sports",
    "items",
    "probably",
    "older",
    "people",
    "actually",
    "selling",
    "uh",
    "buying",
    "things",
    "uh",
    "quite",
    "interestingly",
    "want",
    "open",
    "shell",
    "type",
    "stuff",
    "go",
    "hue",
    "query",
    "editor",
    "hive",
    "thing",
    "go",
    "hi",
    "select",
    "db",
    "yeah",
    "default",
    "database",
    "go",
    "back",
    "cancel",
    "um",
    "database",
    "may",
    "17th",
    "sorry",
    "may",
    "19th",
    "may",
    "time",
    "thing",
    "uh",
    "uh",
    "select",
    "auto",
    "complete",
    "kind",
    "things",
    "little",
    "bit",
    "right",
    "say",
    "play",
    "see",
    "hit",
    "play",
    "uh",
    "tool",
    "called",
    "hue",
    "um",
    "uh",
    "know",
    "useful",
    "actually",
    "um",
    "huh",
    "huh",
    "one",
    "thing",
    "apart",
    "uh",
    "let",
    "say",
    "take",
    "query",
    "um",
    "table",
    "right",
    "right",
    "select",
    "star",
    "let",
    "say",
    "50",
    "something",
    "like",
    "right",
    "run",
    "query",
    "runs",
    "gives",
    "options",
    "visualize",
    "see",
    "visualize",
    "data",
    "mean",
    "right",
    "hug",
    "property",
    "knowledge",
    "website",
    "called",
    "ge",
    "um",
    "available",
    "cloud",
    "default",
    "hue",
    "buy",
    "cloud",
    "era",
    "get",
    "hue",
    "hoton",
    "works",
    "hue",
    "separately",
    "install",
    "want",
    "hoton",
    "works",
    "ambari",
    "ui",
    "good",
    "ambari",
    "admin",
    "tool",
    "file",
    "manager",
    "like",
    "click",
    "upload",
    "file",
    "good",
    "actually",
    "uh",
    "third",
    "party",
    "knowledge",
    "ambar",
    "apache",
    "using",
    "even",
    "apache",
    "hado",
    "install",
    "ambari",
    "plain",
    "apache",
    "want",
    "administer",
    "install",
    "ambari",
    "popularly",
    "hoton",
    "works",
    "uh",
    "nobody",
    "uses",
    "much",
    "apache",
    "hadu",
    "either",
    "hoton",
    "works",
    "cloud",
    "era",
    "right",
    "mapar",
    "worst",
    "ui",
    "called",
    "mppar",
    "control",
    "center",
    "worst",
    "mean",
    "show",
    "ui",
    "mapa",
    "mppar",
    "cluster",
    "lost",
    "maper",
    "control",
    "system",
    "know",
    "maybe",
    "improved",
    "improved",
    "mapar",
    "ui",
    "called",
    "mapar",
    "control",
    "system",
    "mcs",
    "okay",
    "um",
    "yeah",
    "looks",
    "like",
    "see",
    "weird",
    "mean",
    "like",
    "cloud",
    "manager",
    "see",
    "ui",
    "dreadful",
    "phones",
    "ca",
    "find",
    "anything",
    "mean",
    "worst",
    "ui",
    "ever",
    "created",
    "probably",
    "mean",
    "comparing",
    "cloud",
    "manager",
    "cloud",
    "way",
    "better",
    "ui",
    "fonts",
    "everything",
    "map",
    "control",
    "system",
    "ui",
    "uh",
    "anyway",
    "wanted",
    "discuss",
    "one",
    "thing",
    "far",
    "done",
    "manage",
    "tables",
    "want",
    "talk",
    "external",
    "table",
    "bit",
    "confusing",
    "okay",
    "uh",
    "finish",
    "external",
    "tables",
    "wind",
    "probably",
    "okay",
    "tough",
    "session",
    "actually",
    "one",
    "thing",
    "uh",
    "want",
    "one",
    "small",
    "thing",
    "um",
    "want",
    "create",
    "table",
    "okay",
    "copying",
    "command",
    "want",
    "change",
    "open",
    "one",
    "notepad",
    "open",
    "sorry",
    "new",
    "right",
    "command",
    "creating",
    "uh",
    "transactional",
    "table",
    "regular",
    "command",
    "want",
    "change",
    "say",
    "create",
    "external",
    "table",
    "say",
    "transaction",
    "records",
    "uncore",
    "ext",
    "okay",
    "say",
    "location",
    "explain",
    "okay",
    "first",
    "let",
    "try",
    "uh",
    "contrl",
    "c",
    "data",
    "copy",
    "see",
    "whether",
    "command",
    "working",
    "also",
    "try",
    "yes",
    "works",
    "uh",
    "want",
    "increase",
    "font",
    "let",
    "increase",
    "font",
    "uh",
    "amount",
    "double",
    "one",
    "thing",
    "difference",
    "creating",
    "something",
    "called",
    "external",
    "table",
    "first",
    "thing",
    "note",
    "say",
    "create",
    "external",
    "table",
    "keyword",
    "called",
    "external",
    "using",
    "point",
    "number",
    "one",
    "point",
    "number",
    "two",
    "schema",
    "say",
    "location",
    "location",
    "say",
    "user",
    "gl",
    "faculty",
    "home",
    "folder",
    "data",
    "going",
    "happen",
    "run",
    "command",
    "create",
    "table",
    "sure",
    "also",
    "create",
    "folder",
    "data",
    "hadoop",
    "folder",
    "create",
    "folder",
    "let",
    "show",
    "tell",
    "required",
    "go",
    "home",
    "directory",
    "home",
    "directory",
    "uh",
    "yeah",
    "data",
    "see",
    "table",
    "folder",
    "created",
    "table",
    "creation",
    "command",
    "create",
    "okay",
    "idea",
    "external",
    "table",
    "create",
    "external",
    "table",
    "control",
    "data",
    "right",
    "folder",
    "called",
    "data",
    "right",
    "need",
    "open",
    "folder",
    "okay",
    "copy",
    "transaction",
    "files",
    "sorry",
    "copy",
    "file",
    "uploading",
    "file",
    "say",
    "select",
    "star",
    "dxn",
    "records",
    "uncore",
    "ext",
    "limit",
    "file",
    "data",
    "difference",
    "difference",
    "manage",
    "table",
    "external",
    "table",
    "manage",
    "table",
    "control",
    "data",
    "say",
    "load",
    "data",
    "hi",
    "always",
    "keep",
    "user",
    "high",
    "warehouse",
    "blah",
    "blah",
    "blah",
    "external",
    "table",
    "say",
    "want",
    "location",
    "keep",
    "data",
    "mentioned",
    "folder",
    "upload",
    "data",
    "appear",
    "table",
    "one",
    "example",
    "actually",
    "use",
    "external",
    "table",
    "customer",
    "say",
    "data",
    "uh",
    "ask",
    "data",
    "say",
    "data",
    "s3",
    "amazon",
    "s3",
    "simply",
    "say",
    "create",
    "external",
    "table",
    "point",
    "amazon",
    "s3",
    "location",
    "data",
    "appear",
    "table",
    "data",
    "saying",
    "see",
    "table",
    "external",
    "location",
    "keep",
    "data",
    "access",
    "table",
    "called",
    "external",
    "table",
    "try",
    "want",
    "create",
    "external",
    "table",
    "upload",
    "data",
    "see",
    "whether",
    "find",
    "times",
    "may",
    "control",
    "data",
    "say",
    "time",
    "copy",
    "data",
    "place",
    "sometimes",
    "might",
    "want",
    "create",
    "table",
    "point",
    "data",
    "point",
    "uh",
    "table",
    "folder",
    "actually",
    "possible",
    "local",
    "system",
    "uploaded",
    "folder",
    "upload",
    "data",
    "folder",
    "appear",
    "table",
    "data",
    "folder",
    "data",
    "folder",
    "see",
    "table",
    "saying",
    "location",
    "see",
    "business",
    "use",
    "case",
    "different",
    "show",
    "show",
    "business",
    "use",
    "case",
    "okay",
    "going",
    "happen",
    "let",
    "say",
    "dbs",
    "oracle",
    "okay",
    "sql",
    "etc",
    "right",
    "databases",
    "right",
    "let",
    "say",
    "run",
    "scoop",
    "know",
    "scoop",
    "right",
    "scoop",
    "etl",
    "tool",
    "say",
    "hey",
    "scoop",
    "one",
    "thing",
    "connect",
    "dbs",
    "every",
    "day",
    "midnight",
    "schedule",
    "job",
    "everyday",
    "midnight",
    "scoop",
    "scoop",
    "connect",
    "two",
    "dbs",
    "run",
    "etl",
    "job",
    "pull",
    "data",
    "scoop",
    "gets",
    "data",
    "scoop",
    "send",
    "somewhere",
    "hadoop",
    "cluster",
    "right",
    "hadoop",
    "cluster",
    "right",
    "scoop",
    "dump",
    "data",
    "folder",
    "folder",
    "called",
    "know",
    "ragu",
    "sc",
    "scoop",
    "keep",
    "pushing",
    "data",
    "folder",
    "called",
    "ra",
    "create",
    "external",
    "table",
    "okay",
    "point",
    "location",
    "ragu",
    "right",
    "happen",
    "select",
    "automatically",
    "data",
    "scoop",
    "dumping",
    "appear",
    "table",
    "able",
    "understand",
    "saying",
    "typically",
    "etl",
    "leg",
    "happening",
    "every",
    "day",
    "running",
    "etl",
    "jobs",
    "bring",
    "data",
    "multiple",
    "places",
    "data",
    "get",
    "dumped",
    "folder",
    "somebody",
    "go",
    "say",
    "low",
    "data",
    "local",
    "paath",
    "rather",
    "create",
    "external",
    "table",
    "point",
    "folder",
    "moment",
    "file",
    "lands",
    "appearing",
    "table",
    "simple",
    "actual",
    "use",
    "case",
    "external",
    "table",
    "yeah",
    "yeah",
    "scoop",
    "scheduling",
    "uzi",
    "asking",
    "scoop",
    "directly",
    "dump",
    "hi",
    "tables",
    "also",
    "possible",
    "scoop",
    "take",
    "data",
    "oracle",
    "dump",
    "hiy",
    "table",
    "possible",
    "many",
    "jobs",
    "get",
    "data",
    "multiple",
    "sources",
    "say",
    "flume",
    "job",
    "running",
    "flume",
    "agent",
    "may",
    "collecting",
    "log",
    "files",
    "log",
    "files",
    "dump",
    "folder",
    "create",
    "simple",
    "external",
    "table",
    "use",
    "serd",
    "read",
    "log",
    "simply",
    "select",
    "star",
    "data",
    "appear",
    "table",
    "otherwise",
    "say",
    "low",
    "data",
    "local",
    "paath",
    "every",
    "time",
    "dump",
    "table",
    "right",
    "actual",
    "use",
    "external",
    "table",
    "know",
    "simply",
    "point",
    "folder",
    "files",
    "appear",
    "table",
    "hive",
    "care",
    "insider",
    "make",
    "sure",
    "schema",
    "matches",
    "everything",
    "uh",
    "people",
    "ask",
    "20",
    "column",
    "data",
    "folder",
    "10",
    "column",
    "h",
    "table",
    "first",
    "10",
    "columns",
    "come",
    "truncate",
    "ideally",
    "okay",
    "fits",
    "wherever",
    "possible",
    "remove",
    "files",
    "appended",
    "table",
    "shown",
    "like",
    "five",
    "files",
    "wherever",
    "records",
    "match",
    "understand",
    "say",
    "null",
    "values",
    "know",
    "data",
    "types",
    "mismatch",
    "five",
    "columns",
    "come",
    "try",
    "fit",
    "guarantee",
    "right",
    "ideally",
    "use",
    "case",
    "data",
    "warehouse",
    "supposed",
    "actually",
    "supposed",
    "experiment",
    "data",
    "warehouse",
    "data",
    "clean",
    "cleaning",
    "data",
    "factory",
    "operations",
    "right",
    "take",
    "data",
    "data",
    "warehouse",
    "finally",
    "um",
    "another",
    "important",
    "point",
    "external",
    "table",
    "show",
    "tables",
    "table",
    "called",
    "uh",
    "transaction",
    "records",
    "manage",
    "table",
    "original",
    "table",
    "say",
    "drop",
    "drop",
    "table",
    "dxn",
    "records",
    "dro",
    "manage",
    "table",
    "tell",
    "happen",
    "data",
    "retain",
    "data",
    "data",
    "gone",
    "data",
    "gone",
    "sure",
    "data",
    "retain",
    "data",
    "remain",
    "okay",
    "asking",
    "dropped",
    "manage",
    "table",
    "know",
    "manage",
    "table",
    "asking",
    "table",
    "gone",
    "sure",
    "file",
    "manually",
    "uploaded",
    "right",
    "right",
    "yes",
    "check",
    "user",
    "hi",
    "warehouse",
    "may",
    "19",
    "see",
    "uh",
    "folder",
    "called",
    "transaction",
    "records",
    "data",
    "gone",
    "manage",
    "table",
    "drawback",
    "wo",
    "call",
    "drawback",
    "common",
    "happened",
    "project",
    "working",
    "came",
    "know",
    "mean",
    "normally",
    "work",
    "learn",
    "things",
    "learn",
    "mistake",
    "something",
    "somebody",
    "table",
    "huge",
    "table",
    "shared",
    "among",
    "project",
    "two",
    "three",
    "projects",
    "sharing",
    "table",
    "full",
    "rights",
    "point",
    "time",
    "one",
    "engineer",
    "said",
    "drop",
    "table",
    "gone",
    "pro",
    "said",
    "drop",
    "table",
    "accidentally",
    "intentionally",
    "problem",
    "table",
    "gone",
    "recovery",
    "mechanism",
    "point",
    "time",
    "hado",
    "h",
    "way",
    "recover",
    "ultimately",
    "delete",
    "hado",
    "recovery",
    "trash",
    "mechanism",
    "recycle",
    "mechanism",
    "based",
    "recovered",
    "somehow",
    "okay",
    "uh",
    "create",
    "table",
    "share",
    "always",
    "create",
    "external",
    "table",
    "drop",
    "table",
    "use",
    "arrow",
    "right",
    "table",
    "name",
    "transaction",
    "records",
    "uncore",
    "ext",
    "dropping",
    "external",
    "table",
    "table",
    "gone",
    "go",
    "user",
    "ah",
    "go",
    "home",
    "folder",
    "right",
    "go",
    "folder",
    "called",
    "folder",
    "data",
    "data",
    "remains",
    "hive",
    "control",
    "external",
    "data",
    "control",
    "uploaded",
    "delete",
    "easy",
    "recreate",
    "table",
    "data",
    "lost",
    "right",
    "want",
    "share",
    "data",
    "share",
    "expose",
    "table",
    "always",
    "create",
    "external",
    "tables",
    "manage",
    "tables",
    "rare",
    "actually",
    "learning",
    "purpose",
    "easy",
    "manage",
    "table",
    "operation",
    "side",
    "queries",
    "everything",
    "manage",
    "table",
    "difference",
    "always",
    "remember",
    "create",
    "external",
    "tables",
    "want",
    "share",
    "okay",
    "uh",
    "ppt",
    "data",
    "request",
    "go",
    "also",
    "official",
    "website",
    "hive",
    "important",
    "hive",
    "um",
    "tons",
    "information",
    "okay",
    "like",
    "know",
    "much",
    "lot",
    "things",
    "actually",
    "uh",
    "start",
    "getting",
    "started",
    "guide",
    "hive",
    "understand",
    "hive",
    "yeah",
    "forgot",
    "connect",
    "using",
    "bine",
    "music",
    "um",
    "show",
    "tomorrow",
    "run",
    "b",
    "line",
    "see",
    "default",
    "high",
    "server",
    "2",
    "runs",
    "port",
    "number",
    "starting",
    "b",
    "line",
    "say",
    "jdbc",
    "connect",
    "port",
    "number",
    "connect",
    "high",
    "server",
    "2",
    "show",
    "tomorrow",
    "anyway",
    "forgot",
    "please",
    "go",
    "hi",
    "official",
    "website",
    "tons",
    "tons",
    "information",
    "um",
    "lot",
    "things",
    "high",
    "wiki",
    "page",
    "saw",
    "right",
    "uh",
    "uh",
    "look",
    "points",
    "already",
    "discussed",
    "uh",
    "query",
    "execution",
    "via",
    "apache",
    "apache",
    "spark",
    "map",
    "reduce",
    "oh",
    "probably",
    "last",
    "bit",
    "show",
    "forgot",
    "username",
    "password",
    "right",
    "show",
    "tomorrow",
    "uh",
    "anyway",
    "query",
    "execution",
    "via",
    "map",
    "reduce",
    "spark",
    "use",
    "spark",
    "execution",
    "engine",
    "people",
    "prefer",
    "spark",
    "fire",
    "queries",
    "way",
    "easy",
    "start",
    "spark",
    "sql",
    "say",
    "fire",
    "actually",
    "use",
    "spark",
    "execution",
    "engine",
    "possible",
    "right",
    "subsecond",
    "query",
    "retrieval",
    "via",
    "hi",
    "l",
    "hoton",
    "works",
    "lp",
    "talking",
    "lp",
    "lp",
    "enables",
    "uh",
    "asset",
    "transactions",
    "faster",
    "queries",
    "everything",
    "think",
    "normal",
    "rdbms",
    "mean",
    "within",
    "requires",
    "resources",
    "mean",
    "even",
    "though",
    "says",
    "fast",
    "take",
    "lot",
    "resources",
    "actually",
    "uh",
    "running",
    "th",
    "th",
    "memory",
    "anything",
    "memory",
    "requires",
    "resources",
    "memory",
    "means",
    "ram",
    "use",
    "ram",
    "tas",
    "fast",
    "memory",
    "cluster",
    "needs",
    "lot",
    "ram",
    "lp",
    "otherwise",
    "slow",
    "ah",
    "procedure",
    "hpl",
    "sql",
    "nothing",
    "combin",
    "called",
    "hi",
    "query",
    "language",
    "hql",
    "write",
    "custom",
    "uh",
    "functions",
    "inside",
    "h",
    "hi",
    "supports",
    "something",
    "called",
    "udf",
    "user",
    "defined",
    "function",
    "write",
    "say",
    "hsql",
    "fancy",
    "way",
    "saying",
    "right",
    "would",
    "feel",
    "vice",
    "versa",
    "uh",
    "sense",
    "yeah",
    "see",
    "uh",
    "good",
    "java",
    "map",
    "reduces",
    "choice",
    "map",
    "reduces",
    "extinct",
    "extinct",
    "right",
    "go",
    "spark",
    "spark",
    "uses",
    "java",
    "java",
    "8",
    "functional",
    "java",
    "java",
    "7",
    "lot",
    "people",
    "trouble",
    "um",
    "even",
    "spar",
    "community",
    "trusted",
    "preferred",
    "python",
    "python",
    "scala",
    "java",
    "comes",
    "java",
    "supported",
    "spark",
    "100",
    "percentage",
    "developers",
    "less",
    "people",
    "want",
    "either",
    "python",
    "scala",
    "easy",
    "write",
    "actually",
    "right",
    "um",
    "would",
    "say",
    "knowledge",
    "sql",
    "highways",
    "important",
    "whatever",
    "right",
    "um",
    "tell",
    "spark",
    "um",
    "spark",
    "write",
    "code",
    "using",
    "python",
    "sql",
    "sql",
    "much",
    "optimized",
    "writing",
    "anything",
    "using",
    "sql",
    "something",
    "called",
    "schema",
    "right",
    "means",
    "understand",
    "data",
    "schema",
    "schema",
    "understand",
    "data",
    "write",
    "u",
    "join",
    "filter",
    "push",
    "fil",
    "filter",
    "first",
    "join",
    "able",
    "understand",
    "h",
    "sql",
    "write",
    "query",
    "default",
    "right",
    "default",
    "ssql",
    "write",
    "group",
    "join",
    "filter",
    "happen",
    "filter",
    "come",
    "first",
    "load",
    "data",
    "finally",
    "filter",
    "first",
    "filter",
    "called",
    "optimization",
    "possible",
    "write",
    "language",
    "spark",
    "see",
    "write",
    "spark",
    "code",
    "optimized",
    "spark",
    "tight",
    "schema",
    "spark",
    "sql",
    "saying",
    "sql",
    "always",
    "preference",
    "optimization",
    "possible",
    "languages",
    "like",
    "even",
    "map",
    "ruce",
    "write",
    "idea",
    "data",
    "underlying",
    "right",
    "mentioning",
    "schema",
    "load",
    "full",
    "data",
    "analysis",
    "map",
    "reduce",
    "program",
    "write",
    "whole",
    "file",
    "need",
    "loaded",
    "say",
    "select",
    "column",
    "analyze",
    "possible",
    "right",
    "first",
    "everything",
    "loaded",
    "always",
    "structur",
    "languages",
    "preference",
    "java",
    "good",
    "saying",
    "java",
    "bad",
    "wo",
    "consider",
    "scala",
    "probably",
    "relate",
    "scala",
    "scala",
    "similar",
    "java",
    "know",
    "java",
    "scala",
    "easily",
    "learn",
    "scala",
    "spark",
    "good",
    "combination",
    "good",
    "combination",
    "uh",
    "else",
    "questions",
    "wind",
    "5",
    "minutes",
    "uh",
    "questions",
    "tomorrow",
    "covering",
    "uh",
    "something",
    "called",
    "u",
    "uh",
    "partitioning",
    "bucketing",
    "indexing",
    "hi",
    "bit",
    "advanced",
    "topics",
    "actually",
    "okay",
    "much",
    "needed",
    "actually",
    "understand",
    "hi",
    "highlights",
    "hive",
    "somebody",
    "want",
    "learn",
    "hive",
    "points",
    "somebody",
    "ask",
    "hi",
    "ask",
    "nobody",
    "nobody",
    "ask",
    "select",
    "star",
    "query",
    "right",
    "nobody",
    "going",
    "ask",
    "ask",
    "partition",
    "data",
    "least",
    "understand",
    "logic",
    "sufficient",
    "right",
    "look",
    "nasa",
    "data",
    "nasa",
    "data",
    "see",
    "nasa",
    "tomorrow",
    "analyze",
    "nasa",
    "data",
    "using",
    "hi",
    "show",
    "show",
    "serd",
    "rejects",
    "sdis",
    "explain",
    "nasa",
    "data",
    "make",
    "sure",
    "uh",
    "data",
    "sets",
    "uh",
    "anything",
    "else",
    "let",
    "know",
    "look",
    "case",
    "study",
    "prepared",
    "nasa",
    "data",
    "analyzing",
    "nasa",
    "web",
    "server",
    "logs",
    "apache",
    "hive",
    "helpful",
    "see",
    "data",
    "spark",
    "spark",
    "class",
    "also",
    "data",
    "data",
    "understand",
    "data",
    "analyze",
    "using",
    "spark",
    "well",
    "guess",
    "aware",
    "server",
    "logs",
    "contain",
    "lots",
    "information",
    "web",
    "servers",
    "right",
    "case",
    "study",
    "show",
    "derive",
    "insights",
    "web",
    "server",
    "logs",
    "going",
    "look",
    "log",
    "files",
    "generated",
    "web",
    "server",
    "ever",
    "worked",
    "web",
    "servers",
    "um",
    "let",
    "say",
    "type",
    "uh",
    "download",
    "something",
    "request",
    "hitting",
    "web",
    "server",
    "get",
    "get",
    "request",
    "uh",
    "put",
    "request",
    "http",
    "status",
    "code",
    "log",
    "files",
    "look",
    "like",
    "okay",
    "show",
    "log",
    "file",
    "actually",
    "download",
    "data",
    "free",
    "url",
    "already",
    "downloaded",
    "uploaded",
    "data",
    "worry",
    "public",
    "data",
    "nasa",
    "scroll",
    "look",
    "moving",
    "activity",
    "please",
    "go",
    "http",
    "response",
    "codes",
    "link",
    "right",
    "aware",
    "something",
    "called",
    "http",
    "response",
    "codes",
    "yeah",
    "go",
    "url",
    "every",
    "request",
    "code",
    "associated",
    "right",
    "browsing",
    "hitting",
    "web",
    "server",
    "server",
    "respond",
    "code",
    "20",
    "okay",
    "anything",
    "starting",
    "two",
    "success",
    "2",
    "not1",
    "created",
    "22",
    "accepted",
    "204",
    "content",
    "uh",
    "30",
    "redirection",
    "website",
    "redirect",
    "place",
    "right",
    "client",
    "errors",
    "4",
    "500",
    "server",
    "error",
    "sometimes",
    "try",
    "open",
    "website",
    "say",
    "501",
    "server",
    "found",
    "like",
    "anything",
    "starting",
    "five",
    "uh",
    "internal",
    "server",
    "error",
    "right",
    "um",
    "gateway",
    "timeout",
    "service",
    "unavailable",
    "uh",
    "something",
    "something",
    "like",
    "see",
    "want",
    "go",
    "list",
    "right",
    "understanding",
    "data",
    "data",
    "set",
    "contain",
    "2",
    "months",
    "worth",
    "http",
    "request",
    "nasa",
    "kennedy",
    "space",
    "center",
    "server",
    "florida",
    "data",
    "collected",
    "nasa",
    "web",
    "server",
    "2",
    "months",
    "worth",
    "requests",
    "think",
    "around",
    "1",
    "million",
    "lines",
    "data",
    "pretty",
    "huge",
    "data",
    "actually",
    "somewhere",
    "around",
    "1",
    "million",
    "lines",
    "log",
    "files",
    "stored",
    "apache",
    "common",
    "log",
    "format",
    "lot",
    "people",
    "ask",
    "uh",
    "say",
    "text",
    "file",
    "txt",
    "extension",
    "normally",
    "say",
    "text",
    "file",
    "something",
    "txt",
    "lot",
    "data",
    "called",
    "text",
    "file",
    "always",
    "mandatory",
    "txt",
    "extension",
    "right",
    "apache",
    "common",
    "log",
    "format",
    "text",
    "file",
    "open",
    "notepad",
    "right",
    "open",
    "wordpad",
    "thing",
    "show",
    "properly",
    "like",
    "text",
    "data",
    "show",
    "data",
    "let",
    "look",
    "data",
    "go",
    "uh",
    "nasa",
    "access",
    "logs",
    "data",
    "say",
    "open",
    "say",
    "word",
    "pad",
    "say",
    "okay",
    "uh",
    "data",
    "looks",
    "like",
    "really",
    "pretty",
    "mean",
    "sense",
    "like",
    "really",
    "nice",
    "uh",
    "structured",
    "um",
    "fields",
    "interested",
    "right",
    "log",
    "file",
    "look",
    "pdf",
    "right",
    "uh",
    "host",
    "identity",
    "user",
    "identity",
    "time",
    "request",
    "status",
    "size",
    "scroll",
    "first",
    "field",
    "host",
    "making",
    "request",
    "okay",
    "host",
    "making",
    "request",
    "host",
    "guy",
    "in24",
    "inet",
    "something",
    "uh",
    "fully",
    "qualified",
    "domain",
    "name",
    "host",
    "name",
    "host",
    "making",
    "request",
    "next",
    "two",
    "user",
    "identity",
    "remote",
    "local",
    "machine",
    "unavailable",
    "next",
    "two",
    "fields",
    "used",
    "dashes",
    "fine",
    "time",
    "stamp",
    "day",
    "month",
    "uh",
    "hour",
    "year",
    "format",
    "see",
    "time",
    "stamp",
    "right",
    "time",
    "making",
    "request",
    "time",
    "z",
    "minus",
    "400",
    "time",
    "zone",
    "see",
    "time",
    "zone",
    "okay",
    "request",
    "send",
    "server",
    "see",
    "get",
    "get",
    "request",
    "okay",
    "somebody",
    "trying",
    "get",
    "shutle",
    "mission",
    "status",
    "news",
    "news",
    "something",
    "okay",
    "get",
    "request",
    "http",
    "replay",
    "code",
    "uh",
    "http",
    "replay",
    "code",
    "200",
    "success",
    "okay",
    "size",
    "file",
    "received",
    "probably",
    "bytes",
    "1839",
    "data",
    "h",
    "see",
    "lot",
    "people",
    "uh",
    "sending",
    "requests",
    "nasa",
    "web",
    "server",
    "data",
    "looks",
    "like",
    "want",
    "want",
    "create",
    "hive",
    "table",
    "load",
    "data",
    "want",
    "write",
    "queries",
    "size",
    "file",
    "167",
    "megabytes",
    "big",
    "file",
    "actually",
    "small",
    "test",
    "cluster",
    "behave",
    "start",
    "writing",
    "query",
    "us",
    "write",
    "query",
    "see",
    "efficient",
    "cluster",
    "anyway",
    "right",
    "um",
    "next",
    "question",
    "actually",
    "load",
    "data",
    "hi",
    "data",
    "structure",
    "using",
    "serd",
    "serializer",
    "deserializer",
    "little",
    "bit",
    "uh",
    "uh",
    "information",
    "also",
    "want",
    "give",
    "assignment",
    "simply",
    "go",
    "hive",
    "say",
    "sir",
    "yeah",
    "see",
    "called",
    "serd",
    "serd",
    "serd",
    "sh",
    "serializer",
    "deserializer",
    "like",
    "said",
    "data",
    "really",
    "structured",
    "like",
    "log",
    "file",
    "json",
    "files",
    "anything",
    "want",
    "read",
    "write",
    "use",
    "parser",
    "called",
    "serd",
    "right",
    "hi",
    "supports",
    "lot",
    "ses",
    "ses",
    "going",
    "use",
    "guy",
    "rex",
    "aware",
    "uh",
    "rex",
    "regular",
    "expression",
    "right",
    "means",
    "write",
    "regular",
    "expression",
    "high",
    "par",
    "parse",
    "using",
    "read",
    "orc",
    "ao",
    "things",
    "look",
    "later",
    "custom",
    "ses",
    "write",
    "series",
    "want",
    "also",
    "possible",
    "right",
    "um",
    "look",
    "later",
    "let",
    "open",
    "uh",
    "rexer",
    "yeah",
    "says",
    "uh",
    "want",
    "create",
    "table",
    "use",
    "serd",
    "say",
    "row",
    "format",
    "serd",
    "remember",
    "previously",
    "used",
    "say",
    "raw",
    "format",
    "delimited",
    "fields",
    "say",
    "raw",
    "format",
    "serd",
    "say",
    "sdy",
    "okay",
    "say",
    "seri",
    "properties",
    "say",
    "input",
    "regular",
    "expression",
    "whatever",
    "regular",
    "expression",
    "put",
    "actually",
    "able",
    "get",
    "data",
    "whatever",
    "rejects",
    "writing",
    "give",
    "structure",
    "data",
    "idea",
    "right",
    "um",
    "important",
    "stored",
    "text",
    "file",
    "mean",
    "think",
    "yesterday",
    "discuss",
    "create",
    "hive",
    "table",
    "default",
    "option",
    "called",
    "stored",
    "text",
    "file",
    "means",
    "whatever",
    "data",
    "loading",
    "hi",
    "table",
    "store",
    "text",
    "file",
    "regular",
    "text",
    "file",
    "csv",
    "file",
    "file",
    "yesterday",
    "loaded",
    "customer",
    "data",
    "transaction",
    "data",
    "getting",
    "loaded",
    "okay",
    "options",
    "say",
    "stored",
    "orc",
    "par",
    "improvement",
    "comes",
    "talk",
    "difference",
    "say",
    "text",
    "file",
    "say",
    "orc",
    "file",
    "difference",
    "say",
    "stored",
    "text",
    "file",
    "bothered",
    "default",
    "text",
    "file",
    "ah",
    "yesterday",
    "mentioning",
    "default",
    "text",
    "file",
    "data",
    "set",
    "available",
    "location",
    "uh",
    "see",
    "forward",
    "slg",
    "data",
    "uh",
    "huh",
    "one",
    "moment",
    "uh",
    "name",
    "access",
    "right",
    "yeah",
    "yeah",
    "file",
    "called",
    "access",
    "extension",
    "available",
    "common",
    "folder",
    "called",
    "gl",
    "data",
    "first",
    "let",
    "one",
    "thing",
    "let",
    "start",
    "hive",
    "anyway",
    "need",
    "start",
    "hi",
    "start",
    "hive",
    "let",
    "create",
    "table",
    "use",
    "db",
    "yesterday",
    "created",
    "say",
    "use",
    "may9",
    "db",
    "right",
    "switch",
    "whichever",
    "db",
    "created",
    "command",
    "create",
    "table",
    "copy",
    "paste",
    "pdf",
    "okay",
    "say",
    "copy",
    "explain",
    "say",
    "create",
    "table",
    "exist",
    "nasa",
    "log",
    "fields",
    "interested",
    "host",
    "um",
    "identity",
    "user",
    "identity",
    "time",
    "request",
    "status",
    "size",
    "say",
    "row",
    "format",
    "serd",
    "want",
    "use",
    "reject",
    "serd",
    "sd",
    "properties",
    "input",
    "regular",
    "expression",
    "regular",
    "expression",
    "using",
    "output",
    "say",
    "u",
    "part",
    "serd",
    "say",
    "store",
    "equal",
    "uh",
    "spaces",
    "right",
    "say",
    "stored",
    "text",
    "file",
    "saying",
    "store",
    "text",
    "file",
    "regular",
    "expression",
    "huh",
    "uh",
    "go",
    "pdf",
    "copy",
    "like",
    "right",
    "click",
    "say",
    "select",
    "tool",
    "select",
    "tool",
    "select",
    "left",
    "click",
    "copy",
    "like",
    "idea",
    "regular",
    "expressions",
    "want",
    "write",
    "scratch",
    "okay",
    "yeah",
    "sure",
    "uh",
    "um",
    "show",
    "console",
    "um",
    "yeah",
    "command",
    "said",
    "hi",
    "use",
    "database",
    "copy",
    "pasted",
    "table",
    "creation",
    "command",
    "right",
    "loaded",
    "data",
    "load",
    "data",
    "output",
    "format",
    "string",
    "simply",
    "saying",
    "eight",
    "columns",
    "right",
    "space",
    "uh",
    "column",
    "space",
    "uh",
    "bit",
    "difficult",
    "explain",
    "uh",
    "actually",
    "reads",
    "separates",
    "data",
    "many",
    "spaces",
    "looking",
    "digits",
    "strings",
    "uh",
    "extracting",
    "interesting",
    "fields",
    "bit",
    "difficult",
    "explain",
    "rex",
    "actually",
    "writing",
    "rex",
    "scratch",
    "uh",
    "validate",
    "validated",
    "even",
    "remember",
    "validate",
    "wrote",
    "long",
    "back",
    "actually",
    "yeah",
    "create",
    "table",
    "yeah",
    "normal",
    "table",
    "difference",
    "table",
    "properties",
    "saying",
    "uh",
    "whatever",
    "data",
    "comes",
    "table",
    "apply",
    "serd",
    "regular",
    "expression",
    "otherwise",
    "recognize",
    "data",
    "inside",
    "right",
    "right",
    "apache",
    "common",
    "log",
    "format",
    "right",
    "apache",
    "common",
    "log",
    "format",
    "fixed",
    "uh",
    "schema",
    "schema",
    "way",
    "log",
    "files",
    "generated",
    "write",
    "multiple",
    "ways",
    "one",
    "way",
    "writing",
    "hi",
    "look",
    "simply",
    "say",
    "rex",
    "want",
    "show",
    "huh",
    "input",
    "reject",
    "see",
    "output",
    "reject",
    "stored",
    "see",
    "input",
    "reject",
    "right",
    "looking",
    "mean",
    "written",
    "like",
    "avoid",
    "output",
    "string",
    "also",
    "work",
    "ensuring",
    "everything",
    "looks",
    "string",
    "mean",
    "manually",
    "saying",
    "string",
    "think",
    "updated",
    "h",
    "documentation",
    "updated",
    "updated",
    "let",
    "see",
    "um",
    "rex",
    "30",
    "found",
    "given",
    "two",
    "links",
    "actually",
    "probably",
    "let",
    "check",
    "happen",
    "happens",
    "let",
    "see",
    "find",
    "first",
    "h",
    "see",
    "source",
    "communities",
    "happens",
    "let",
    "say",
    "looking",
    "hive",
    "spark",
    "anything",
    "somebody",
    "create",
    "product",
    "let",
    "say",
    "created",
    "hive",
    "give",
    "everybody",
    "everybody",
    "start",
    "working",
    "happen",
    "somebody",
    "say",
    "something",
    "broken",
    "may",
    "add",
    "documentation",
    "add",
    "j",
    "jira",
    "issue",
    "tracking",
    "right",
    "jira",
    "link",
    "jira",
    "link",
    "clearly",
    "says",
    "input",
    "rejects",
    "output",
    "format",
    "string",
    "use",
    "somebody",
    "added",
    "uh",
    "yes",
    "work",
    "adding",
    "serd",
    "example",
    "new",
    "directory",
    "somebody",
    "said",
    "uh",
    "properly",
    "formatted",
    "j",
    "added",
    "actually",
    "official",
    "documentation",
    "originally",
    "created",
    "probably",
    "property",
    "saying",
    "somebody",
    "could",
    "complained",
    "mentioned",
    "link",
    "right",
    "see",
    "look",
    "says",
    "found",
    "know",
    "updating",
    "documentation",
    "anyway",
    "one",
    "problem",
    "want",
    "search",
    "documentation",
    "may",
    "find",
    "everything",
    "official",
    "documentation",
    "sometimes",
    "always",
    "look",
    "jira",
    "issues",
    "fix",
    "jira",
    "also",
    "everything",
    "every",
    "time",
    "things",
    "work",
    "way",
    "expecting",
    "google",
    "gole",
    "saying",
    "high",
    "plus",
    "jira",
    "able",
    "read",
    "file",
    "issue",
    "coming",
    "across",
    "j",
    "ticket",
    "always",
    "right",
    "load",
    "data",
    "let",
    "continue",
    "location",
    "location",
    "gl",
    "data",
    "right",
    "load",
    "say",
    "load",
    "data",
    "wait",
    "wait",
    "wait",
    "wait",
    "wait",
    "wait",
    "load",
    "data",
    "path",
    "happen",
    "say",
    "load",
    "data",
    "path",
    "going",
    "happen",
    "create",
    "external",
    "table",
    "right",
    "better",
    "think",
    "normally",
    "participants",
    "data",
    "classes",
    "upload",
    "want",
    "want",
    "create",
    "external",
    "table",
    "everybody",
    "point",
    "data",
    "okay",
    "modify",
    "modify",
    "want",
    "external",
    "table",
    "say",
    "create",
    "external",
    "table",
    "let",
    "drop",
    "original",
    "table",
    "okay",
    "drop",
    "first",
    "drop",
    "drop",
    "table",
    "name",
    "nasa",
    "log",
    "drop",
    "say",
    "create",
    "external",
    "table",
    "nasa",
    "law",
    "fine",
    "difference",
    "location",
    "slg",
    "data",
    "um",
    "wait",
    "wait",
    "wait",
    "wait",
    "external",
    "table",
    "creating",
    "point",
    "folder",
    "load",
    "everything",
    "gl",
    "data",
    "want",
    "right",
    "g",
    "data",
    "lot",
    "folders",
    "right",
    "move",
    "see",
    "create",
    "folder",
    "g",
    "data",
    "let",
    "see",
    "new",
    "directory",
    "call",
    "nasa",
    "yeah",
    "move",
    "inside",
    "right",
    "make",
    "sense",
    "see",
    "take",
    "care",
    "everything",
    "right",
    "one",
    "right",
    "access",
    "action",
    "move",
    "gl",
    "data",
    "nasa",
    "right",
    "nasa",
    "folder",
    "yeah",
    "move",
    "fine",
    "ideally",
    "check",
    "nasa",
    "h",
    "work",
    "otherwise",
    "load",
    "every",
    "file",
    "gl",
    "data",
    "folder",
    "want",
    "right",
    "location",
    "location",
    "right",
    "let",
    "see",
    "works",
    "uh",
    "use",
    "small",
    "letter",
    "capital",
    "letter",
    "table",
    "creation",
    "work",
    "know",
    "question",
    "query",
    "select",
    "star",
    "works",
    "try",
    "uh",
    "let",
    "know",
    "works",
    "us",
    "accessing",
    "data",
    "okay",
    "us",
    "data",
    "make",
    "sure",
    "see",
    "see",
    "properly",
    "structured",
    "already",
    "created",
    "table",
    "first",
    "right",
    "dropped",
    "uh",
    "load",
    "data",
    "table",
    "shared",
    "location",
    "better",
    "create",
    "external",
    "table",
    "point",
    "dropped",
    "recreated",
    "table",
    "location",
    "also",
    "location",
    "also",
    "external",
    "type",
    "small",
    "letter",
    "capital",
    "letter",
    "folder",
    "name",
    "cas",
    "sensitive",
    "think",
    "select",
    "star",
    "let",
    "know",
    "whether",
    "able",
    "see",
    "data",
    "two",
    "approaches",
    "every",
    "data",
    "made",
    "structured",
    "also",
    "one",
    "problem",
    "problem",
    "hive",
    "structured",
    "tool",
    "means",
    "somehow",
    "give",
    "structure",
    "work",
    "type",
    "data",
    "like",
    "example",
    "format",
    "called",
    "ao",
    "av",
    "previously",
    "get",
    "ao",
    "data",
    "like",
    "unstructured",
    "able",
    "anything",
    "ao",
    "reader",
    "created",
    "still",
    "type",
    "data",
    "give",
    "structure",
    "one",
    "point",
    "second",
    "point",
    "works",
    "organizations",
    "like",
    "said",
    "data",
    "inje",
    "team",
    "right",
    "giving",
    "example",
    "ge",
    "ge",
    "happens",
    "see",
    "ge",
    "business",
    "enormous",
    "nobody",
    "clue",
    "happening",
    "g",
    "one",
    "biggest",
    "firms",
    "world",
    "actually",
    "right",
    "size",
    "data",
    "enormous",
    "getting",
    "uh",
    "financial",
    "applications",
    "okay",
    "financial",
    "applications",
    "generate",
    "data",
    "okay",
    "get",
    "data",
    "apis",
    "pull",
    "data",
    "json",
    "format",
    "comes",
    "json",
    "format",
    "sensor",
    "data",
    "get",
    "sensor",
    "data",
    "uh",
    "locomotive",
    "engines",
    "train",
    "engines",
    "europe",
    "g",
    "created",
    "train",
    "engines",
    "whenever",
    "running",
    "produce",
    "sensor",
    "data",
    "collect",
    "data",
    "come",
    "kafka",
    "come",
    "ao",
    "format",
    "kafka",
    "hadoop",
    "ultimate",
    "land",
    "hadoop",
    "data",
    "sensor",
    "data",
    "like",
    "events",
    "sensor",
    "data",
    "come",
    "events",
    "format",
    "like",
    "whenever",
    "event",
    "generate",
    "data",
    "text",
    "file",
    "format",
    "get",
    "called",
    "ao",
    "point",
    "data",
    "lands",
    "hadoop",
    "nobody",
    "give",
    "structure",
    "data",
    "nobody",
    "earth",
    "give",
    "g",
    "data",
    "inje",
    "team",
    "job",
    "take",
    "data",
    "give",
    "structure",
    "give",
    "big",
    "data",
    "team",
    "getting",
    "point",
    "right",
    "directly",
    "give",
    "big",
    "data",
    "team",
    "clue",
    "data",
    "staging",
    "area",
    "data",
    "lands",
    "okay",
    "written",
    "java",
    "code",
    "custom",
    "java",
    "code",
    "java",
    "code",
    "read",
    "ao",
    "remove",
    "unwanted",
    "fails",
    "give",
    "structure",
    "send",
    "next",
    "side",
    "injecting",
    "data",
    "saying",
    "row",
    "data",
    "times",
    "may",
    "able",
    "directly",
    "handle",
    "second",
    "use",
    "case",
    "big",
    "data",
    "developer",
    "like",
    "us",
    "want",
    "data",
    "original",
    "format",
    "machine",
    "learning",
    "guy",
    "want",
    "data",
    "original",
    "format",
    "getting",
    "difference",
    "like",
    "data",
    "scientist",
    "want",
    "anybody",
    "touch",
    "data",
    "want",
    "original",
    "data",
    "nonsense",
    "build",
    "machine",
    "learning",
    "models",
    "want",
    "features",
    "want",
    "modify",
    "data",
    "big",
    "data",
    "guy",
    "like",
    "like",
    "happy",
    "structure",
    "lose",
    "data",
    "need",
    "structured",
    "format",
    "work",
    "two",
    "teams",
    "machine",
    "learning",
    "teams",
    "original",
    "data",
    "collecting",
    "algorithms",
    "crunch",
    "data",
    "kafka",
    "used",
    "data",
    "coming",
    "europe",
    "uh",
    "streaming",
    "data",
    "stream",
    "live",
    "stream",
    "day",
    "something",
    "mean",
    "collect",
    "sensor",
    "data",
    "huh",
    "amazon",
    "entire",
    "setup",
    "amazon",
    "sensor",
    "uh",
    "uh",
    "amazon",
    "solution",
    "right",
    "working",
    "amazon",
    "sensor",
    "data",
    "solution",
    "amazon",
    "call",
    "kinesis",
    "kinesis",
    "right",
    "kinesis",
    "sensor",
    "platform",
    "amazon",
    "kinesis",
    "collect",
    "data",
    "kinesis",
    "push",
    "kafka",
    "somewhere",
    "store",
    "right",
    "data",
    "less",
    "decisions",
    "want",
    "data",
    "train",
    "data",
    "time",
    "analyzing",
    "data",
    "train",
    "working",
    "like",
    "crashed",
    "train",
    "engine",
    "got",
    "crashed",
    "otherwise",
    "want",
    "analyze",
    "real",
    "time",
    "99",
    "time",
    "trains",
    "running",
    "data",
    "useless",
    "train",
    "engine",
    "got",
    "crashed",
    "something",
    "happened",
    "time",
    "want",
    "look",
    "data",
    "last",
    "24",
    "hour",
    "caused",
    "crash",
    "size",
    "data",
    "petabytes",
    "analyzing",
    "also",
    "possible",
    "collect",
    "real",
    "time",
    "batch",
    "send",
    "day",
    "something",
    "go",
    "amazon",
    "land",
    "kafka",
    "kafka",
    "kafka",
    "two",
    "three",
    "teams",
    "collect",
    "one",
    "team",
    "using",
    "data",
    "something",
    "else",
    "distribute",
    "one",
    "copy",
    "stored",
    "hado",
    "like",
    "original",
    "data",
    "copy",
    "anybody",
    "want",
    "get",
    "architecture",
    "right",
    "g",
    "evolv",
    "architecture",
    "moving",
    "hadoop",
    "stable",
    "architecture",
    "probably",
    "couple",
    "years",
    "find",
    "another",
    "way",
    "better",
    "things",
    "right",
    "real",
    "time",
    "uh",
    "uh",
    "train",
    "data",
    "also",
    "flight",
    "data",
    "flight",
    "aircraft",
    "engines",
    "g",
    "right",
    "commercial",
    "aircraft",
    "g",
    "80",
    "engines",
    "generate",
    "sensor",
    "data",
    "collect",
    "real",
    "time",
    "rare",
    "get",
    "problem",
    "time",
    "want",
    "analyze",
    "uh",
    "hope",
    "able",
    "reach",
    "right",
    "till",
    "point",
    "ah",
    "place",
    "data",
    "right",
    "go",
    "location",
    "copied",
    "data",
    "look",
    "pdf",
    "says",
    "load",
    "data",
    "select",
    "star",
    "use",
    "cases",
    "wrote",
    "mean",
    "also",
    "write",
    "use",
    "cases",
    "find",
    "top",
    "end",
    "points",
    "received",
    "server",
    "side",
    "error",
    "meaning",
    "uh",
    "many",
    "uh",
    "endpoints",
    "received",
    "uh",
    "server",
    "side",
    "error",
    "right",
    "asking",
    "like",
    "501",
    "uh",
    "5",
    "servers",
    "side",
    "error",
    "many",
    "times",
    "occurred",
    "figure",
    "multiple",
    "ways",
    "one",
    "way",
    "say",
    "select",
    "status",
    "count",
    "request",
    "uh",
    "say",
    "nasa",
    "log",
    "table",
    "group",
    "status",
    "say",
    "status",
    "reg",
    "regular",
    "expression",
    "extract",
    "saying",
    "uh",
    "anything",
    "starting",
    "5",
    "status",
    "column",
    "5",
    "server",
    "side",
    "order",
    "status",
    "descending",
    "limit",
    "file",
    "query",
    "want",
    "know",
    "errors",
    "starting",
    "5",
    "count",
    "many",
    "writing",
    "using",
    "double",
    "uh",
    "operator",
    "assignment",
    "close",
    "hi",
    "using",
    "close",
    "assignment",
    "something",
    "say",
    "equal",
    "something",
    "equal",
    "something",
    "become",
    "equal",
    "one",
    "become",
    "become",
    "ones",
    "saying",
    "comparing",
    "comparison",
    "operator",
    "double",
    "equal",
    "huh",
    "saying",
    "close",
    "normal",
    "say",
    "somewhere",
    "documentation",
    "share",
    "okay",
    "try",
    "query",
    "see",
    "taking",
    "time",
    "24",
    "seconds",
    "ver",
    "anyway",
    "take",
    "time",
    "guys",
    "try",
    "query",
    "slow",
    "mention",
    "commands",
    "either",
    "full",
    "small",
    "letters",
    "full",
    "capital",
    "letters",
    "said",
    "capital",
    "l",
    "location",
    "bug",
    "know",
    "otherwise",
    "throw",
    "error",
    "wo",
    "throw",
    "error",
    "uh",
    "common",
    "thing",
    "create",
    "external",
    "tables",
    "mention",
    "location",
    "um",
    "even",
    "location",
    "exist",
    "may",
    "throw",
    "error",
    "try",
    "querying",
    "nothing",
    "come",
    "think",
    "problem",
    "space",
    "fine",
    "ideally",
    "location",
    "keyword",
    "sometimes",
    "problem",
    "started",
    "working",
    "getting",
    "maybe",
    "maybe",
    "problem",
    "dropped",
    "recreated",
    "probably",
    "started",
    "working",
    "sure",
    "much",
    "time",
    "taking",
    "run",
    "query",
    "75",
    "seconds",
    "uh",
    "also",
    "one",
    "point",
    "um",
    "forgot",
    "yeah",
    "anyway",
    "somebody",
    "asking",
    "reference",
    "regular",
    "expression",
    "hi",
    "language",
    "manual",
    "pdf",
    "uh",
    "resource",
    "requested",
    "frequently",
    "host",
    "good",
    "question",
    "right",
    "page",
    "uh",
    "requested",
    "think",
    "difficult",
    "query",
    "still",
    "run",
    "see",
    "uh",
    "look",
    "says",
    "select",
    "uh",
    "request",
    "comma",
    "count",
    "request",
    "count",
    "nasa",
    "log",
    "group",
    "request",
    "order",
    "descending",
    "limit",
    "30",
    "looking",
    "top",
    "30",
    "requests",
    "actually",
    "right",
    "skip",
    "header",
    "think",
    "skip",
    "data",
    "part",
    "think",
    "skip",
    "columns",
    "think",
    "filter",
    "ultimately",
    "hadoop",
    "data",
    "remains",
    "hadu",
    "right",
    "one",
    "problem",
    "hdfs",
    "uh",
    "way",
    "filter",
    "anyway",
    "locally",
    "see",
    "whether",
    "asking",
    "loading",
    "data",
    "filter",
    "data",
    "like",
    "want",
    "10",
    "columns",
    "want",
    "skip",
    "header",
    "option",
    "table",
    "properties",
    "way",
    "something",
    "called",
    "table",
    "properties",
    "um",
    "go",
    "google",
    "say",
    "hi",
    "create",
    "table",
    "tbl",
    "properties",
    "creation",
    "table",
    "something",
    "called",
    "table",
    "properties",
    "huh",
    "table",
    "properties",
    "right",
    "mention",
    "um",
    "uh",
    "uh",
    "know",
    "skip",
    "header",
    "let",
    "check",
    "auto",
    "compaction",
    "mapper",
    "memory",
    "auto",
    "purge",
    "external",
    "true",
    "skip",
    "uh",
    "header",
    "sure",
    "think",
    "skip",
    "columns",
    "possible",
    "think",
    "anyway",
    "see",
    "get",
    "back",
    "okay",
    "queries",
    "written",
    "try",
    "like",
    "display",
    "top",
    "10",
    "host",
    "made",
    "maximum",
    "request",
    "uh",
    "try",
    "find",
    "total",
    "count",
    "different",
    "response",
    "cotes",
    "returned",
    "server",
    "try",
    "queries",
    "let",
    "know",
    "working",
    "actually",
    "tried",
    "everything",
    "work",
    "okay",
    "uh",
    "talking",
    "uh",
    "impala",
    "little",
    "bit",
    "right",
    "probably",
    "depth",
    "know",
    "little",
    "bit",
    "impala",
    "heavily",
    "used",
    "industry",
    "imala",
    "cloud",
    "side",
    "cloud",
    "side",
    "hot",
    "work",
    "side",
    "everything",
    "hi",
    "clera",
    "side",
    "something",
    "called",
    "impala",
    "right",
    "small",
    "intro",
    "much",
    "show",
    "impala",
    "query",
    "also",
    "going",
    "happen",
    "data",
    "nodes",
    "right",
    "data",
    "node",
    "another",
    "one",
    "another",
    "one",
    "uh",
    "install",
    "something",
    "called",
    "impala",
    "right",
    "using",
    "cloud",
    "distribution",
    "impala",
    "default",
    "installed",
    "worry",
    "impala",
    "sql",
    "engine",
    "depend",
    "hi",
    "anything",
    "right",
    "install",
    "impala",
    "happen",
    "demon",
    "called",
    "impala",
    "impala",
    "called",
    "impala",
    "demon",
    "start",
    "running",
    "every",
    "data",
    "note",
    "like",
    "something",
    "like",
    "oracle",
    "mean",
    "saying",
    "right",
    "impala",
    "running",
    "notes",
    "first",
    "right",
    "impala",
    "shell",
    "fire",
    "query",
    "depend",
    "h",
    "fire",
    "query",
    "going",
    "happen",
    "uh",
    "let",
    "say",
    "hits",
    "machine",
    "query",
    "accepted",
    "one",
    "machine",
    "let",
    "say",
    "guy",
    "right",
    "guy",
    "look",
    "query",
    "impala",
    "talk",
    "hive",
    "metast",
    "store",
    "means",
    "whatever",
    "tables",
    "creating",
    "hive",
    "impala",
    "see",
    "tables",
    "create",
    "query",
    "using",
    "impala",
    "whatever",
    "tables",
    "create",
    "impala",
    "hi",
    "also",
    "see",
    "share",
    "meta",
    "store",
    "one",
    "advantage",
    "top",
    "another",
    "thing",
    "write",
    "hi",
    "query",
    "convert",
    "map",
    "reduce",
    "hi",
    "bothered",
    "query",
    "running",
    "responsibility",
    "map",
    "reduce",
    "run",
    "query",
    "impala",
    "convert",
    "map",
    "reduce",
    "impala",
    "impala",
    "demon",
    "run",
    "query",
    "like",
    "rdbms",
    "say",
    "rdbms",
    "saying",
    "like",
    "firing",
    "oracle",
    "query",
    "run",
    "query",
    "oracle",
    "run",
    "query",
    "like",
    "fire",
    "query",
    "impala",
    "get",
    "query",
    "responsibility",
    "impala",
    "dem",
    "execute",
    "query",
    "uh",
    "hi",
    "metadata",
    "okay",
    "also",
    "block",
    "metadata",
    "meaning",
    "create",
    "impala",
    "table",
    "like",
    "hive",
    "table",
    "say",
    "load",
    "data",
    "right",
    "data",
    "actually",
    "hadoop",
    "data",
    "blocks",
    "replicas",
    "metadata",
    "name",
    "node",
    "guys",
    "copy",
    "know",
    "data",
    "impala",
    "going",
    "depend",
    "anything",
    "fire",
    "query",
    "run",
    "right",
    "queries",
    "fast",
    "query",
    "hits",
    "impala",
    "runs",
    "memory",
    "drawback",
    "let",
    "say",
    "data",
    "block",
    "block",
    "happen",
    "ram",
    "block",
    "data",
    "come",
    "block",
    "data",
    "come",
    "ram",
    "guy",
    "simply",
    "coordinate",
    "query",
    "data",
    "right",
    "guy",
    "coordinate",
    "query",
    "run",
    "query",
    "memory",
    "drawback",
    "let",
    "say",
    "machine",
    "crashes",
    "entire",
    "query",
    "abouted",
    "fall",
    "tolerant",
    "high",
    "queries",
    "fall",
    "tolerant",
    "map",
    "redu",
    "fall",
    "tolerant",
    "way",
    "map",
    "reduce",
    "fail",
    "right",
    "imp",
    "queries",
    "fall",
    "tolerant",
    "one",
    "machine",
    "distributed",
    "query",
    "one",
    "machine",
    "running",
    "query",
    "probably",
    "10",
    "machines",
    "running",
    "query",
    "one",
    "machine",
    "crashes",
    "query",
    "abot",
    "convert",
    "map",
    "reduce",
    "anything",
    "queries",
    "super",
    "fast",
    "actually",
    "directly",
    "hit",
    "hdfs",
    "query",
    "data",
    "give",
    "output",
    "whatever",
    "want",
    "faster",
    "reliability",
    "uh",
    "issue",
    "impala",
    "cloud",
    "cluster",
    "etl",
    "jobs",
    "never",
    "use",
    "impala",
    "etl",
    "jobs",
    "behind",
    "scenes",
    "running",
    "hi",
    "query",
    "probably",
    "query",
    "take",
    "3",
    "hours",
    "run",
    "4",
    "hours",
    "run",
    "probably",
    "5",
    "hours",
    "fine",
    "even",
    "atl",
    "job",
    "machine",
    "crashes",
    "safe",
    "hi",
    "query",
    "map",
    "reduce",
    "take",
    "care",
    "run",
    "etl",
    "query",
    "using",
    "impala",
    "let",
    "say",
    "query",
    "faster",
    "2",
    "hours",
    "probably",
    "take",
    "1",
    "hour",
    "one",
    "machine",
    "crashes",
    "impala",
    "restart",
    "whole",
    "query",
    "throw",
    "error",
    "etl",
    "job",
    "start",
    "beginning",
    "lost",
    "one",
    "hour",
    "right",
    "actually",
    "needs",
    "high",
    "meta",
    "store",
    "metadata",
    "impala",
    "must",
    "use",
    "hive",
    "meta",
    "store",
    "hive",
    "installed",
    "another",
    "advantage",
    "u",
    "hi",
    "remain",
    "lot",
    "tools",
    "requires",
    "hive",
    "even",
    "spark",
    "sql",
    "spark",
    "sql",
    "normally",
    "configure",
    "spark",
    "tell",
    "spark",
    "sql",
    "use",
    "h",
    "metastore",
    "h",
    "metast",
    "store",
    "used",
    "everybody",
    "tables",
    "create",
    "one",
    "place",
    "right",
    "table",
    "either",
    "query",
    "using",
    "hive",
    "query",
    "using",
    "impala",
    "decide",
    "impala",
    "also",
    "drawbacks",
    "nasa",
    "table",
    "created",
    "ca",
    "query",
    "using",
    "impala",
    "rex",
    "supported",
    "table",
    "created",
    "using",
    "rex",
    "sd",
    "hi",
    "see",
    "table",
    "impala",
    "wo",
    "able",
    "query",
    "think",
    "memory",
    "serves",
    "well",
    "regular",
    "tables",
    "query",
    "without",
    "problem",
    "access",
    "time",
    "meta",
    "store",
    "shared",
    "place",
    "meta",
    "store",
    "mysql",
    "shared",
    "place",
    "anybody",
    "access",
    "high",
    "impala",
    "100",
    "people",
    "access",
    "need",
    "run",
    "install",
    "hi",
    "set",
    "meta",
    "store",
    "saying",
    "whatever",
    "table",
    "create",
    "impala",
    "stores",
    "highest",
    "meta",
    "store",
    "data",
    "node",
    "impaler",
    "demon",
    "demon",
    "copy",
    "house",
    "meta",
    "store",
    "block",
    "data",
    "happens",
    "fire",
    "query",
    "demon",
    "pick",
    "called",
    "coordinator",
    "meaning",
    "write",
    "impala",
    "query",
    "hundreds",
    "data",
    "nodes",
    "data",
    "node",
    "running",
    "impala",
    "get",
    "query",
    "node",
    "called",
    "coordinator",
    "query",
    "example",
    "coordinator",
    "guy",
    "accepted",
    "query",
    "data",
    "fine",
    "okay",
    "gets",
    "query",
    "local",
    "lookup",
    "fast",
    "metadata",
    "understand",
    "immediately",
    "data",
    "data",
    "splits",
    "query",
    "streams",
    "run",
    "query",
    "collect",
    "result",
    "display",
    "ideally",
    "lot",
    "machines",
    "data",
    "metadata",
    "lookup",
    "fast",
    "frequency",
    "manually",
    "refresh",
    "automatically",
    "copy",
    "meaning",
    "create",
    "hi",
    "table",
    "appear",
    "impala",
    "refresh",
    "metadata",
    "option",
    "impala",
    "say",
    "refresh",
    "feel",
    "new",
    "table",
    "say",
    "refresh",
    "metadata",
    "come",
    "block",
    "metadata",
    "come",
    "table",
    "metadata",
    "created",
    "say",
    "creating",
    "table",
    "h",
    "create",
    "table",
    "load",
    "data",
    "metadata",
    "data",
    "come",
    "store",
    "whole",
    "hadoop",
    "metadata",
    "hadoop",
    "cluster",
    "let",
    "say",
    "th000",
    "terab",
    "file",
    "okay",
    "impala",
    "store",
    "th000",
    "terab",
    "metadata",
    "useless",
    "right",
    "saying",
    "hado",
    "cluster",
    "let",
    "say",
    "th000",
    "files",
    "okay",
    "created",
    "impala",
    "table",
    "okay",
    "say",
    "impala",
    "table",
    "loaded",
    "file",
    "one",
    "metadata",
    "file",
    "need",
    "metadata",
    "query",
    "hit",
    "file",
    "right",
    "ah",
    "related",
    "table",
    "saying",
    "create",
    "table",
    "say",
    "load",
    "data",
    "data",
    "example",
    "loaded",
    "nasa",
    "data",
    "block",
    "information",
    "nasa",
    "file",
    "remember",
    "ah",
    "hi",
    "directly",
    "loading",
    "remember",
    "wo",
    "remember",
    "block",
    "information",
    "files",
    "hado",
    "lot",
    "files",
    "right",
    "files",
    "table",
    "getting",
    "point",
    "saying",
    "remember",
    "metadata",
    "files",
    "associated",
    "table",
    "otherwise",
    "remember",
    "files",
    "manual",
    "refreshment",
    "required",
    "mean",
    "block",
    "may",
    "remember",
    "saying",
    "table",
    "properties",
    "automatically",
    "refresh",
    "manually",
    "refresh",
    "every",
    "time",
    "usually",
    "wo",
    "like",
    "usually",
    "even",
    "though",
    "impala",
    "h",
    "communicate",
    "hi",
    "functionality",
    "impala",
    "functionality",
    "meaning",
    "whatever",
    "tables",
    "hive",
    "normally",
    "used",
    "reliability",
    "like",
    "etl",
    "jobs",
    "hi",
    "handle",
    "impala",
    "create",
    "separate",
    "tables",
    "say",
    "thing",
    "load",
    "data",
    "local",
    "paath",
    "blah",
    "blah",
    "blah",
    "handled",
    "impala",
    "like",
    "separate",
    "tables",
    "talk",
    "saying",
    "want",
    "information",
    "say",
    "refresh",
    "get",
    "two",
    "three",
    "types",
    "refresh",
    "remember",
    "completely",
    "complete",
    "refresh",
    "redo",
    "load",
    "total",
    "metadata",
    "high",
    "meta",
    "store",
    "blocks",
    "everything",
    "partial",
    "refresh",
    "hue",
    "things",
    "available",
    "h",
    "saying",
    "mean",
    "wondering",
    "go",
    "query",
    "editors",
    "see",
    "impala",
    "right",
    "impala",
    "probably",
    "table",
    "created",
    "right",
    "come",
    "impala",
    "logic",
    "right",
    "go",
    "back",
    "may",
    "17",
    "may",
    "19",
    "table",
    "db",
    "right",
    "see",
    "customer",
    "nasa",
    "log",
    "impala",
    "select",
    "star",
    "nasa",
    "log",
    "think",
    "see",
    "doubt",
    "five",
    "doubt",
    "able",
    "see",
    "maybe",
    "remember",
    "exactly",
    "huh",
    "uh",
    "fail",
    "load",
    "metadata",
    "uh",
    "fail",
    "load",
    "metadata",
    "invalid",
    "storage",
    "description",
    "impala",
    "support",
    "table",
    "type",
    "reason",
    "sd",
    "library",
    "supported",
    "wo",
    "support",
    "sd",
    "able",
    "see",
    "wo",
    "saying",
    "impala",
    "tables",
    "impala",
    "quaring",
    "data",
    "fall",
    "tolerant",
    "etl",
    "query",
    "get",
    "data",
    "query",
    "question",
    "high",
    "squaring",
    "fall",
    "tolerant",
    "see",
    "nasa",
    "log",
    "thing",
    "supported",
    "uh",
    "look",
    "table",
    "let",
    "say",
    "select",
    "count",
    "star",
    "let",
    "say",
    "one",
    "right",
    "ideally",
    "work",
    "reason",
    "imp",
    "bit",
    "slow",
    "uh",
    "hi",
    "faster",
    "case",
    "really",
    "like",
    "actually",
    "fast",
    "map",
    "reduce",
    "directly",
    "hits",
    "gives",
    "result",
    "query",
    "run",
    "see",
    "map",
    "reduce",
    "job",
    "starting",
    "map",
    "reduce",
    "nothing",
    "ah",
    "uh",
    "one",
    "thing",
    "refresh",
    "manually",
    "normally",
    "manually",
    "uh",
    "write",
    "batch",
    "job",
    "refresh",
    "know",
    "whether",
    "refresh",
    "configured",
    "impala",
    "nasa",
    "lock",
    "table",
    "available",
    "created",
    "right",
    "available",
    "impala",
    "probably",
    "option",
    "refresh",
    "uh",
    "see",
    "refresh",
    "three",
    "options",
    "come",
    "clear",
    "cache",
    "perform",
    "incremental",
    "metadata",
    "update",
    "sync",
    "missing",
    "tables",
    "hi",
    "means",
    "new",
    "table",
    "hi",
    "invalidate",
    "metadata",
    "rebuild",
    "resource",
    "time",
    "iny",
    "say",
    "invalidate",
    "delete",
    "whole",
    "metadata",
    "scratch",
    "build",
    "take",
    "lot",
    "time",
    "refresh",
    "options",
    "impala",
    "try",
    "create",
    "table",
    "see",
    "whether",
    "find",
    "impala",
    "know",
    "create",
    "simple",
    "table",
    "right",
    "table",
    "want",
    "create",
    "maybe",
    "change",
    "run",
    "right",
    "created",
    "go",
    "impala",
    "may",
    "19th",
    "say",
    "refresh",
    "uh",
    "let",
    "say",
    "perform",
    "incremental",
    "refresh",
    "h",
    "came",
    "transaction",
    "record",
    "ideally",
    "manually",
    "imp",
    "automatically",
    "identify",
    "created",
    "table",
    "high",
    "come",
    "ide",
    "huh",
    "impala",
    "also",
    "stores",
    "lot",
    "metadata",
    "uh",
    "actual",
    "data",
    "recent",
    "queries",
    "sol",
    "cache",
    "want",
    "clear",
    "say",
    "clear",
    "cach",
    "reent",
    "queries",
    "ran",
    "store",
    "huh",
    "cash",
    "store",
    "sometimes",
    "cash",
    "become",
    "big",
    "like",
    "running",
    "lot",
    "queries",
    "affect",
    "performance",
    "say",
    "clear",
    "cash",
    "yeah",
    "ideally",
    "keep",
    "cash",
    "hi",
    "use",
    "impala",
    "hi",
    "uses",
    "map",
    "ruce",
    "think",
    "possible",
    "probably",
    "admin",
    "somebody",
    "developer",
    "side",
    "never",
    "uh",
    "mean",
    "use",
    "case",
    "know",
    "mean",
    "masking",
    "table",
    "masking",
    "um",
    "know",
    "possible",
    "never",
    "tried",
    "huh",
    "table",
    "masking",
    "done",
    "admins",
    "say",
    "mask",
    "table",
    "visible",
    "process",
    "like",
    "impala",
    "spark",
    "anything",
    "want",
    "h",
    "usually",
    "wo",
    "like",
    "usually",
    "restrict",
    "access",
    "based",
    "user",
    "user",
    "guy",
    "okay",
    "hi",
    "tables",
    "impala",
    "let",
    "say",
    "guy",
    "developer",
    "access",
    "see",
    "tables",
    "hive",
    "tables",
    "impala",
    "behind",
    "scenes",
    "everywhere",
    "tables",
    "user",
    "level",
    "access",
    "control",
    "see",
    "tables",
    "otherwise",
    "everybody",
    "able",
    "see",
    "tables",
    "right",
    "think",
    "directly",
    "option",
    "stop",
    "communicating",
    "ah",
    "hoton",
    "works",
    "hado",
    "impala",
    "uh",
    "uh",
    "thing",
    "hive",
    "default",
    "hi",
    "plus",
    "th",
    "execution",
    "engine",
    "uh",
    "lp",
    "lp",
    "uh",
    "highlight",
    "actually",
    "lp",
    "much",
    "reliable",
    "much",
    "faster",
    "impala",
    "honest",
    "right",
    "show",
    "little",
    "maybe",
    "see",
    "session",
    "part",
    "clear",
    "right",
    "hi",
    "hi",
    "plus",
    "impala",
    "least",
    "basics",
    "h",
    "automatic",
    "way",
    "identifying",
    "data",
    "work",
    "proper",
    "delimiter",
    "like",
    "comma",
    "space",
    "hue",
    "create",
    "table",
    "identify",
    "space",
    "comma",
    "something",
    "uh",
    "think",
    "geospatial",
    "data",
    "means",
    "structure",
    "impa",
    "mean",
    "much",
    "need",
    "know",
    "mean",
    "giving",
    "option",
    "normally",
    "hive",
    "classes",
    "teach",
    "impala",
    "hi",
    "hi",
    "impala",
    "impala",
    "h",
    "subject",
    "like",
    "become",
    "like",
    "people",
    "learn",
    "separately",
    "project",
    "queries",
    "sql",
    "write",
    "everything",
    "difference",
    "thing",
    "use",
    "case",
    "slightly",
    "different",
    "uh",
    "contributed",
    "claer",
    "impala",
    "open",
    "source",
    "project",
    "mostly",
    "uh",
    "contribution",
    "claa",
    "like",
    "propery",
    "product",
    "issues",
    "comes",
    "claer",
    "support",
    "lot",
    "impala",
    "point",
    "number",
    "one",
    "another",
    "problem",
    "go",
    "platforms",
    "wo",
    "see",
    "imala",
    "probably",
    "reason",
    "people",
    "care",
    "much",
    "impala",
    "right",
    "go",
    "hoton",
    "works",
    "see",
    "impala",
    "right",
    "want",
    "security",
    "something",
    "called",
    "sentry",
    "service",
    "sentry",
    "set",
    "sentry",
    "think",
    "sentry",
    "clouder",
    "saying",
    "clera",
    "something",
    "called",
    "sentry",
    "sentry",
    "clera",
    "called",
    "uh",
    "sentry",
    "set",
    "uh",
    "know",
    "user",
    "level",
    "aess",
    "access",
    "service",
    "level",
    "access",
    "everything",
    "ah",
    "centry",
    "sentry",
    "open",
    "source",
    "users",
    "groups",
    "uh",
    "uh",
    "access",
    "see",
    "authorization",
    "privileges",
    "model",
    "highand",
    "impala",
    "problem",
    "uh",
    "centry",
    "apache",
    "cloud",
    "uses",
    "go",
    "hoten",
    "works",
    "knock",
    "range",
    "two",
    "tools",
    "one",
    "called",
    "ranger",
    "nox",
    "open",
    "source",
    "hotworks",
    "one",
    "problem",
    "platform",
    "wise",
    "slight",
    "differences",
    "ian",
    "happens",
    "slight",
    "differences",
    "cloud",
    "uses",
    "sentry",
    "sentry",
    "configure",
    "access",
    "table",
    "authorizations",
    "admin",
    "set",
    "think",
    "sentry",
    "installed",
    "cluster",
    "means",
    "anybody",
    "see",
    "anything",
    "even",
    "hi",
    "hi",
    "authentication",
    "username",
    "password",
    "create",
    "ah",
    "mean",
    "saying",
    "hi",
    "enter",
    "right",
    "h",
    "karo",
    "default",
    "saying",
    "right",
    "going",
    "logging",
    "one",
    "machine",
    "cluster",
    "work",
    "like",
    "getting",
    "point",
    "think",
    "discussed",
    "first",
    "class",
    "cloud",
    "lab",
    "right",
    "cloud",
    "lab",
    "edge",
    "node",
    "log",
    "connect",
    "lab",
    "security",
    "service",
    "need",
    "enable",
    "nothing",
    "enabled",
    "start",
    "high",
    "hit",
    "cluster",
    "anybody",
    "see",
    "anybody",
    "table",
    "anything",
    "create",
    "sentry",
    "enabled",
    "think",
    "even",
    "working",
    "wo",
    "access",
    "sentry",
    "totally",
    "admin",
    "side",
    "decide",
    "request",
    "may",
    "able",
    "see",
    "access",
    "yes",
    "yes",
    "cluster",
    "hit",
    "edge",
    "applied",
    "like",
    "policies",
    "whatever",
    "otherwise",
    "ca",
    "restrict",
    "users",
    "right",
    "coming",
    "uh",
    "hon",
    "works",
    "ranger",
    "ranger",
    "hoton",
    "works",
    "ranger",
    "similar",
    "also",
    "like",
    "hoton",
    "works",
    "documentation",
    "lot",
    "much",
    "precise",
    "clear",
    "read",
    "understand",
    "talking",
    "cloud",
    "documentation",
    "weird",
    "nobody",
    "understand",
    "mean",
    "look",
    "h",
    "documentation",
    "easy",
    "read",
    "understand",
    "see",
    "comprehensive",
    "security",
    "enterprise",
    "hard",
    "ranger",
    "ranger",
    "works",
    "right",
    "everything",
    "h",
    "apache",
    "ranger",
    "offers",
    "centralized",
    "security",
    "things",
    "hi",
    "go",
    "hive",
    "uh",
    "show",
    "uh",
    "hive",
    "see",
    "even",
    "youtube",
    "presentation",
    "nice",
    "documentation",
    "best",
    "documentation",
    "actually",
    "hoton",
    "works",
    "see",
    "lp",
    "live",
    "long",
    "process",
    "said",
    "think",
    "another",
    "explanation",
    "actually",
    "live",
    "long",
    "process",
    "uh",
    "something",
    "else",
    "aut",
    "say",
    "live",
    "long",
    "process",
    "think",
    "live",
    "long",
    "process",
    "hi",
    "details",
    "optimization",
    "techniques",
    "okay",
    "h",
    "hoton",
    "document",
    "best",
    "want",
    "learn",
    "thing",
    "cloud",
    "production",
    "cluster",
    "things",
    "may",
    "applicable",
    "since",
    "totally",
    "open",
    "source",
    "like",
    "ways",
    "right",
    "using",
    "cloud",
    "era",
    "cluster",
    "production",
    "always",
    "ref",
    "cloud",
    "data",
    "documentation",
    "everything",
    "difference",
    "right",
    "uh",
    "hope",
    "clarifies",
    "uh",
    "impala",
    "talking",
    "impala",
    "um",
    "talk",
    "something",
    "called",
    "partitioning",
    "hive",
    "okay",
    "many",
    "actually",
    "aware",
    "partitioning",
    "might",
    "aware",
    "con",
    "concept",
    "yeah",
    "already",
    "know",
    "probably",
    "refresher",
    "right",
    "see",
    "idea",
    "simple",
    "tell",
    "manager",
    "asked",
    "create",
    "sales",
    "table",
    "okay",
    "u",
    "creating",
    "db",
    "called",
    "retail",
    "table",
    "called",
    "sales",
    "location",
    "let",
    "say",
    "manage",
    "table",
    "uh",
    "user",
    "hyve",
    "warehouse",
    "db",
    "retail",
    "table",
    "sales",
    "location",
    "right",
    "manage",
    "table",
    "let",
    "imagine",
    "every",
    "month",
    "getting",
    "data",
    "month",
    "end",
    "january",
    "got",
    "data",
    "became",
    "csv",
    "said",
    "load",
    "data",
    "local",
    "blah",
    "blah",
    "blah",
    "data",
    "go",
    "know",
    "already",
    "data",
    "got",
    "uploaded",
    "everybody",
    "happy",
    "problem",
    "problem",
    "next",
    "month",
    "next",
    "month",
    "c",
    "happens",
    "right",
    "going",
    "happen",
    "sales",
    "folder",
    "february",
    "happen",
    "february",
    "data",
    "get",
    "uploaded",
    "folder",
    "way",
    "happen",
    "march",
    "right",
    "april",
    "may",
    "problem",
    "model",
    "following",
    "let",
    "say",
    "couple",
    "years",
    "look",
    "folder",
    "10",
    "20",
    "files",
    "inside",
    "folder",
    "real",
    "problem",
    "let",
    "say",
    "writing",
    "query",
    "something",
    "like",
    "select",
    "star",
    "sales",
    "right",
    "let",
    "say",
    "month",
    "equal",
    "feb",
    "let",
    "say",
    "writing",
    "query",
    "problem",
    "hive",
    "clue",
    "data",
    "scan",
    "files",
    "folder",
    "give",
    "result",
    "result",
    "know",
    "h",
    "know",
    "write",
    "query",
    "scan",
    "folders",
    "sales",
    "folder",
    "let",
    "say",
    "100",
    "files",
    "hive",
    "scan",
    "100",
    "files",
    "producing",
    "result",
    "drawback",
    "queries",
    "slow",
    "order",
    "avoid",
    "problem",
    "problem",
    "facing",
    "something",
    "called",
    "partitioning",
    "partitioning",
    "table",
    "okay",
    "partitioning",
    "two",
    "things",
    "something",
    "called",
    "static",
    "partitioning",
    "dynamic",
    "partitioning",
    "static",
    "dynamic",
    "two",
    "type",
    "partitioning",
    "okay",
    "show",
    "help",
    "example",
    "better",
    "idea",
    "partitioning",
    "first",
    "identify",
    "column",
    "columns",
    "based",
    "want",
    "partitioning",
    "assuming",
    "table",
    "structure",
    "queries",
    "based",
    "month",
    "equal",
    "something",
    "right",
    "hi",
    "say",
    "okay",
    "create",
    "partition",
    "table",
    "partition",
    "column",
    "month",
    "hi",
    "provide",
    "data",
    "automatically",
    "identify",
    "create",
    "folders",
    "like",
    "place",
    "data",
    "like",
    "partition",
    "based",
    "month",
    "okay",
    "multiple",
    "ways",
    "show",
    "technically",
    "happening",
    "behind",
    "scenes",
    "identify",
    "month",
    "column",
    "many",
    "months",
    "many",
    "sub",
    "folders",
    "created",
    "move",
    "data",
    "know",
    "12",
    "months",
    "year",
    "data",
    "model",
    "first",
    "partition",
    "based",
    "year",
    "month",
    "create",
    "folder",
    "called",
    "2016",
    "within",
    "12",
    "subfolders",
    "2017",
    "within",
    "12",
    "model",
    "write",
    "query",
    "hit",
    "skip",
    "subfolders",
    "queries",
    "naturally",
    "faster",
    "depends",
    "columns",
    "let",
    "say",
    "saying",
    "column",
    "name",
    "jan",
    "next",
    "column",
    "week",
    "one",
    "week",
    "two",
    "like",
    "mean",
    "depends",
    "data",
    "data",
    "date",
    "month",
    "column",
    "day",
    "column",
    "year",
    "column",
    "partitioning",
    "must",
    "like",
    "everybody",
    "partitioning",
    "many",
    "conditions",
    "partitioning",
    "like",
    "said",
    "date",
    "ca",
    "partition",
    "directly",
    "date",
    "column",
    "uh",
    "let",
    "say",
    "looking",
    "last",
    "10e",
    "data",
    "many",
    "dates",
    "say",
    "partition",
    "date",
    "create",
    "folders",
    "useless",
    "another",
    "way",
    "tackle",
    "tell",
    "partitioning",
    "applicable",
    "cardinality",
    "less",
    "something",
    "called",
    "cardinality",
    "right",
    "many",
    "unique",
    "values",
    "column",
    "like",
    "case",
    "let",
    "say",
    "getting",
    "data",
    "different",
    "countries",
    "world",
    "fine",
    "maximum",
    "150",
    "know",
    "country",
    "country",
    "wise",
    "say",
    "partition",
    "yeah",
    "like",
    "said",
    "another",
    "cas",
    "transactions",
    "want",
    "divide",
    "data",
    "based",
    "transaction",
    "id",
    "possible",
    "every",
    "transaction",
    "unique",
    "transaction",
    "id",
    "starts",
    "creating",
    "many",
    "partitions",
    "possible",
    "partitioning",
    "used",
    "cardinality",
    "within",
    "control",
    "like",
    "right",
    "partitioning",
    "jan",
    "colum",
    "giving",
    "different",
    "use",
    "case",
    "let",
    "say",
    "getting",
    "data",
    "different",
    "different",
    "countries",
    "also",
    "depends",
    "query",
    "running",
    "example",
    "let",
    "say",
    "partition",
    "data",
    "write",
    "query",
    "select",
    "star",
    "sales",
    "country",
    "equal",
    "india",
    "worst",
    "query",
    "write",
    "data",
    "partitioned",
    "month",
    "write",
    "query",
    "based",
    "country",
    "impact",
    "actually",
    "worse",
    "know",
    "queries",
    "based",
    "country",
    "month",
    "partition",
    "country",
    "month",
    "mean",
    "whichever",
    "way",
    "possible",
    "probably",
    "first",
    "partition",
    "month",
    "country",
    "country",
    "month",
    "figure",
    "cardinality",
    "also",
    "ending",
    "creating",
    "lot",
    "partitions",
    "good",
    "idea",
    "hado",
    "like",
    "lot",
    "subfolders",
    "sub",
    "files",
    "right",
    "drawback",
    "think",
    "discussed",
    "first",
    "class",
    "partitioning",
    "traditional",
    "uh",
    "databases",
    "sometimes",
    "effective",
    "use",
    "case",
    "oracle",
    "cluster",
    "like",
    "know",
    "four",
    "machines",
    "okay",
    "saying",
    "let",
    "say",
    "looking",
    "iphone",
    "sales",
    "data",
    "selling",
    "iphone",
    "iphone",
    "sales",
    "data",
    "want",
    "partition",
    "data",
    "based",
    "country",
    "number",
    "sales",
    "country",
    "right",
    "say",
    "partition",
    "country",
    "logical",
    "partitioning",
    "physically",
    "dividing",
    "data",
    "problem",
    "create",
    "countries",
    "like",
    "india",
    "china",
    "us",
    "right",
    "probably",
    "us",
    "partition",
    "much",
    "h",
    "probably",
    "uh",
    "know",
    "give",
    "weird",
    "country",
    "congo",
    "getting",
    "problem",
    "logically",
    "partitioning",
    "saying",
    "divide",
    "data",
    "based",
    "country",
    "since",
    "us",
    "lot",
    "people",
    "buy",
    "iphone",
    "going",
    "happen",
    "us",
    "partition",
    "big",
    "us",
    "partition",
    "right",
    "talking",
    "oracle",
    "okay",
    "congo",
    "partition",
    "oracle",
    "fire",
    "query",
    "happen",
    "machine",
    "responsible",
    "work",
    "together",
    "us",
    "partition",
    "queries",
    "slow",
    "guy",
    "ch",
    "congo",
    "partition",
    "fast",
    "right",
    "hadoop",
    "also",
    "happens",
    "partitioning",
    "okay",
    "say",
    "us",
    "data",
    "india",
    "data",
    "create",
    "sub",
    "folders",
    "advantage",
    "even",
    "though",
    "file",
    "3tb",
    "100",
    "machines",
    "blocks",
    "oracle",
    "one",
    "machine",
    "able",
    "understand",
    "saying",
    "storing",
    "data",
    "oracle",
    "oracle",
    "idea",
    "blocks",
    "replicas",
    "us",
    "partition",
    "data",
    "3",
    "tb",
    "3",
    "tb",
    "one",
    "machine",
    "wo",
    "divide",
    "hadoop",
    "3",
    "tb",
    "never",
    "one",
    "machine",
    "blocks",
    "get",
    "divided",
    "right",
    "query",
    "faster",
    "actually",
    "partitioning",
    "much",
    "effective",
    "hado",
    "saying",
    "right",
    "means",
    "partitioning",
    "traditional",
    "databases",
    "effective",
    "effective",
    "extent",
    "ensure",
    "partitions",
    "equal",
    "amount",
    "data",
    "part",
    "partitioning",
    "logical",
    "every",
    "case",
    "even",
    "hadoop",
    "logical",
    "oracle",
    "logical",
    "oracle",
    "saying",
    "partitions",
    "us",
    "one",
    "machine",
    "way",
    "divide",
    "data",
    "getting",
    "point",
    "right",
    "hado",
    "something",
    "db",
    "us",
    "data",
    "one",
    "machine",
    "machine",
    "lot",
    "data",
    "related",
    "us",
    "hadoop",
    "four",
    "data",
    "nodes",
    "never",
    "3",
    "terab",
    "one",
    "data",
    "node",
    "always",
    "divide",
    "spread",
    "distributed",
    "performance",
    "better",
    "partitioning",
    "right",
    "things",
    "understand",
    "work",
    "right",
    "uh",
    "companies",
    "say",
    "partitioning",
    "much",
    "ah",
    "sharding",
    "sharding",
    "dividing",
    "table",
    "existing",
    "table",
    "um",
    "sharding",
    "mostly",
    "nosql",
    "oracle",
    "lg",
    "support",
    "sharding",
    "extensively",
    "useful",
    "actually",
    "nosql",
    "databases",
    "something",
    "called",
    "table",
    "sharding",
    "table",
    "sharding",
    "like",
    "physical",
    "division",
    "take",
    "table",
    "say",
    "take",
    "rows",
    "divide",
    "like",
    "three",
    "four",
    "dump",
    "okay",
    "highly",
    "possible",
    "rdbms",
    "rdbms",
    "normalization",
    "physically",
    "partition",
    "physically",
    "divide",
    "data",
    "say",
    "want",
    "divide",
    "table",
    "2",
    "gb",
    "2gb",
    "2",
    "gb",
    "possible",
    "right",
    "ah",
    "logical",
    "u",
    "physical",
    "division",
    "means",
    "like",
    "saying",
    "sharding",
    "take",
    "table",
    "table",
    "say",
    "th000",
    "rows",
    "say",
    "divide",
    "five",
    "200",
    "200",
    "200",
    "rows",
    "keep",
    "five",
    "machines",
    "possible",
    "right",
    "normal",
    "rdbms",
    "called",
    "physical",
    "division",
    "giving",
    "condition",
    "saying",
    "take",
    "full",
    "table",
    "split",
    "five",
    "100",
    "100",
    "rows",
    "keep",
    "five",
    "machines",
    "possible",
    "impossible",
    "rdbms",
    "nosql",
    "possible",
    "denormalized",
    "nosql",
    "everything",
    "denormalized",
    "divide",
    "data",
    "push",
    "whichever",
    "way",
    "want",
    "nobody",
    "cares",
    "already",
    "distributed",
    "hi",
    "traditional",
    "rdbms",
    "physical",
    "division",
    "possible",
    "logically",
    "divide",
    "say",
    "look",
    "column",
    "100",
    "values",
    "divide",
    "100",
    "200",
    "200",
    "control",
    "ca",
    "say",
    "much",
    "division",
    "want",
    "size",
    "want",
    "physical",
    "control",
    "logically",
    "dividing",
    "data",
    "right",
    "way",
    "understand",
    "march",
    "effect",
    "hi",
    "clue",
    "right",
    "keeps",
    "searching",
    "uh",
    "know",
    "data",
    "creating",
    "either",
    "manually",
    "create",
    "folders",
    "normally",
    "like",
    "run",
    "command",
    "create",
    "actually",
    "better",
    "rather",
    "talking",
    "basic",
    "idea",
    "want",
    "find",
    "data",
    "set",
    "start",
    "work",
    "working",
    "partitioning",
    "see",
    "folder",
    "partitioning",
    "word",
    "file",
    "want",
    "mean",
    "commands",
    "facing",
    "one",
    "issue",
    "word",
    "file",
    "copy",
    "paste",
    "sometimes",
    "commands",
    "work",
    "see",
    "whether",
    "work",
    "okay",
    "uh",
    "code",
    "right",
    "call",
    "single",
    "code",
    "identify",
    "see",
    "whether",
    "works",
    "discussed",
    "static",
    "partitioning",
    "dynamic",
    "partitioning",
    "talk",
    "first",
    "thing",
    "um",
    "let",
    "create",
    "table",
    "start",
    "talking",
    "db",
    "yes",
    "right",
    "okay",
    "db",
    "say",
    "create",
    "table",
    "copy",
    "command",
    "okay",
    "static",
    "partitioning",
    "bit",
    "confusing",
    "understand",
    "creating",
    "table",
    "called",
    "user",
    "one",
    "see",
    "look",
    "schema",
    "first",
    "name",
    "last",
    "name",
    "id",
    "three",
    "columns",
    "say",
    "partitioned",
    "country",
    "comma",
    "region",
    "means",
    "partition",
    "columns",
    "want",
    "divide",
    "first",
    "country",
    "region",
    "fine",
    "catch",
    "catch",
    "data",
    "going",
    "load",
    "look",
    "data",
    "data",
    "user",
    "info",
    "one",
    "open",
    "data",
    "tell",
    "many",
    "fields",
    "data",
    "three",
    "uh",
    "first",
    "name",
    "uh",
    "last",
    "name",
    "uh",
    "id",
    "right",
    "look",
    "table",
    "created",
    "matching",
    "right",
    "saying",
    "partition",
    "country",
    "comma",
    "region",
    "right",
    "country",
    "region",
    "right",
    "static",
    "partitioning",
    "static",
    "partitioning",
    "means",
    "getting",
    "data",
    "know",
    "data",
    "coming",
    "data",
    "columns",
    "meaning",
    "know",
    "probably",
    "data",
    "coming",
    "country",
    "equal",
    "us",
    "state",
    "equal",
    "california",
    "right",
    "uploading",
    "data",
    "mention",
    "data",
    "columns",
    "country",
    "region",
    "anything",
    "right",
    "upload",
    "data",
    "understand",
    "one",
    "thing",
    "uh",
    "copy",
    "hado",
    "user",
    "info",
    "one",
    "uh",
    "go",
    "hdfs",
    "uh",
    "copy",
    "ragu",
    "user",
    "info",
    "one",
    "fact",
    "copy",
    "save",
    "time",
    "right",
    "mean",
    "also",
    "required",
    "uh",
    "yeah",
    "use",
    "ftp",
    "copying",
    "local",
    "use",
    "ftp",
    "better",
    "copy",
    "h",
    "faster",
    "right",
    "anything",
    "question",
    "right",
    "hadoop",
    "already",
    "created",
    "country",
    "region",
    "folders",
    "think",
    "partitioning",
    "means",
    "creating",
    "subfolders",
    "right",
    "saying",
    "create",
    "partition",
    "table",
    "country",
    "uh",
    "region",
    "actually",
    "create",
    "sub",
    "folder",
    "yeah",
    "nothing",
    "created",
    "need",
    "um",
    "word",
    "pad",
    "copy",
    "want",
    "type",
    "lot",
    "change",
    "okay",
    "type",
    "um",
    "location",
    "ragu",
    "file",
    "name",
    "different",
    "think",
    "user",
    "info",
    "right",
    "txt",
    "extension",
    "yeah",
    "saying",
    "saying",
    "load",
    "data",
    "data",
    "table",
    "loading",
    "data",
    "mentioning",
    "details",
    "saying",
    "country",
    "new",
    "zealand",
    "region",
    "cubc",
    "happen",
    "create",
    "folder",
    "called",
    "country",
    "new",
    "zealand",
    "another",
    "folder",
    "called",
    "region",
    "cubec",
    "within",
    "file",
    "go",
    "verify",
    "warehouse",
    "folder",
    "loaded",
    "available",
    "warehouse",
    "folder",
    "go",
    "hue",
    "uh",
    "go",
    "hdfs",
    "u",
    "data",
    "user",
    "hi",
    "right",
    "user",
    "hive",
    "warehouse",
    "minus",
    "may",
    "many",
    "dbs",
    "people",
    "started",
    "may",
    "19",
    "db",
    "uh",
    "user",
    "one",
    "see",
    "country",
    "equal",
    "new",
    "zealand",
    "folder",
    "hadoop",
    "called",
    "country",
    "equal",
    "new",
    "zealand",
    "within",
    "region",
    "equal",
    "cuc",
    "within",
    "data",
    "try",
    "try",
    "see",
    "whether",
    "see",
    "project",
    "assignments",
    "partitioning",
    "important",
    "word",
    "file",
    "right",
    "oh",
    "okay",
    "wrong",
    "right",
    "sorry",
    "sorry",
    "show",
    "remember",
    "point",
    "static",
    "partitioning",
    "used",
    "like",
    "use",
    "case",
    "okay",
    "getting",
    "uh",
    "user",
    "data",
    "multiple",
    "countries",
    "okay",
    "one",
    "clients",
    "application",
    "user",
    "data",
    "coming",
    "multiple",
    "countries",
    "data",
    "country",
    "column",
    "used",
    "manually",
    "create",
    "called",
    "static",
    "say",
    "put",
    "data",
    "us",
    "something",
    "called",
    "dynamic",
    "partitioning",
    "show",
    "make",
    "sure",
    "uh",
    "able",
    "log",
    "hi",
    "always",
    "use",
    "db",
    "using",
    "default",
    "db",
    "type",
    "command",
    "say",
    "already",
    "table",
    "exist",
    "lot",
    "actually",
    "using",
    "default",
    "db",
    "uh",
    "meanwhile",
    "look",
    "data",
    "right",
    "hi",
    "uh",
    "partitioning",
    "tell",
    "difference",
    "data",
    "previous",
    "one",
    "loaded",
    "exactly",
    "city",
    "uh",
    "state",
    "whatever",
    "way",
    "togo",
    "west",
    "bengal",
    "together",
    "whatever",
    "okay",
    "city",
    "region",
    "yeah",
    "data",
    "country",
    "city",
    "uh",
    "column",
    "best",
    "way",
    "partitioning",
    "create",
    "partition",
    "table",
    "say",
    "partition",
    "country",
    "state",
    "whatever",
    "column",
    "tell",
    "hi",
    "give",
    "data",
    "decide",
    "going",
    "divide",
    "data",
    "look",
    "column",
    "understand",
    "many",
    "countries",
    "many",
    "states",
    "accordingly",
    "create",
    "put",
    "called",
    "dynamic",
    "many",
    "h",
    "like",
    "20",
    "lines",
    "data",
    "20",
    "20",
    "many",
    "created",
    "ideally",
    "huh",
    "think",
    "uh",
    "first",
    "part",
    "starts",
    "christmas",
    "island",
    "repetition",
    "right",
    "one",
    "thing",
    "understand",
    "dynamic",
    "partitioning",
    "dynamic",
    "partitioning",
    "default",
    "disabled",
    "hive",
    "uh",
    "somebody",
    "come",
    "say",
    "okay",
    "create",
    "dynamic",
    "partition",
    "table",
    "use",
    "transaction",
    "id",
    "column",
    "happen",
    "millions",
    "partitions",
    "get",
    "created",
    "screw",
    "everything",
    "default",
    "even",
    "production",
    "clusters",
    "dynamic",
    "partitioning",
    "disabled",
    "first",
    "enable",
    "first",
    "thing",
    "second",
    "thing",
    "uh",
    "upload",
    "data",
    "directly",
    "dynamic",
    "partition",
    "table",
    "need",
    "first",
    "upload",
    "data",
    "temporary",
    "table",
    "table",
    "okay",
    "table",
    "insert",
    "partition",
    "table",
    "meaning",
    "previously",
    "used",
    "created",
    "static",
    "partition",
    "table",
    "said",
    "load",
    "data",
    "part",
    "went",
    "able",
    "know",
    "partition",
    "going",
    "right",
    "know",
    "hi",
    "decide",
    "create",
    "table",
    "first",
    "normal",
    "table",
    "upload",
    "data",
    "partitioning",
    "nothing",
    "create",
    "partition",
    "table",
    "original",
    "table",
    "say",
    "insert",
    "table",
    "divide",
    "send",
    "way",
    "uh",
    "dynamic",
    "partition",
    "hi",
    "okay",
    "practically",
    "understand",
    "ca",
    "directly",
    "load",
    "table",
    "insert",
    "example",
    "really",
    "useful",
    "unique",
    "values",
    "like",
    "normally",
    "like",
    "five",
    "records",
    "one",
    "country",
    "something",
    "like",
    "everything",
    "unique",
    "values",
    "idea",
    "um",
    "let",
    "see",
    "another",
    "file",
    "okay",
    "partitions",
    "high",
    "land",
    "property",
    "case",
    "study",
    "one",
    "moment",
    "okay",
    "load",
    "list",
    "uh",
    "one",
    "thing",
    "finish",
    "particular",
    "example",
    "dynamic",
    "partitioning",
    "something",
    "called",
    "bucketing",
    "also",
    "finish",
    "give",
    "assignment",
    "landed",
    "property",
    "analysis",
    "mean",
    "easy",
    "commands",
    "already",
    "help",
    "reinforce",
    "idea",
    "partitioning",
    "partitioning",
    "also",
    "try",
    "finish",
    "okay",
    "uh",
    "time",
    "dynamic",
    "partitioning",
    "create",
    "table",
    "called",
    "user",
    "2",
    "see",
    "thing",
    "copy",
    "paste",
    "um",
    "partitioning",
    "table",
    "right",
    "look",
    "saying",
    "partitioned",
    "country",
    "region",
    "table",
    "one",
    "data",
    "partitioned",
    "okay",
    "create",
    "table",
    "called",
    "user",
    "three",
    "regular",
    "table",
    "uh",
    "first",
    "name",
    "last",
    "name",
    "country",
    "region",
    "first",
    "name",
    "last",
    "name",
    "ah",
    "country",
    "region",
    "right",
    "four",
    "columns",
    "data",
    "user",
    "info",
    "three",
    "right",
    "first",
    "name",
    "last",
    "name",
    "id",
    "schema",
    "five",
    "columns",
    "right",
    "work",
    "mean",
    "original",
    "table",
    "table",
    "called",
    "user",
    "three",
    "hold",
    "original",
    "data",
    "first",
    "load",
    "data",
    "load",
    "load",
    "data",
    "path",
    "already",
    "hadoop",
    "hadoop",
    "file",
    "name",
    "user",
    "info",
    "right",
    "name",
    "table",
    "user",
    "three",
    "till",
    "load",
    "data",
    "uh",
    "user",
    "3",
    "table",
    "yeah",
    "user",
    "3",
    "table",
    "hold",
    "original",
    "data",
    "partitioning",
    "nothing",
    "data",
    "insert",
    "partition",
    "table",
    "directly",
    "upload",
    "data",
    "partition",
    "table",
    "dynamic",
    "partitioning",
    "copy",
    "data",
    "one",
    "original",
    "table",
    "insert",
    "h",
    "static",
    "mentioning",
    "country",
    "uh",
    "state",
    "manually",
    "upload",
    "data",
    "say",
    "create",
    "folder",
    "like",
    "like",
    "dump",
    "file",
    "file",
    "divided",
    "based",
    "column",
    "values",
    "dynamically",
    "hi",
    "decide",
    "load",
    "data",
    "one",
    "table",
    "say",
    "insert",
    "partition",
    "table",
    "hi",
    "dynamically",
    "partition",
    "based",
    "two",
    "columns",
    "country",
    "region",
    "created",
    "right",
    "country",
    "region",
    "yes",
    "first",
    "countries",
    "within",
    "country",
    "regions",
    "right",
    "around",
    "20",
    "25",
    "lines",
    "data",
    "much",
    "partition",
    "column",
    "outside",
    "schema",
    "define",
    "partition",
    "table",
    "partition",
    "columns",
    "whether",
    "exist",
    "data",
    "exist",
    "data",
    "outside",
    "schema",
    "partition",
    "mention",
    "schema",
    "required",
    "total",
    "schema",
    "including",
    "partition",
    "columns",
    "understand",
    "okay",
    "want",
    "one",
    "thing",
    "look",
    "word",
    "file",
    "right",
    "see",
    "things",
    "see",
    "right",
    "properties",
    "need",
    "enable",
    "dynamic",
    "partitioning",
    "first",
    "one",
    "easy",
    "read",
    "understand",
    "saying",
    "hey",
    "hi",
    "enable",
    "dynamic",
    "partitioning",
    "set",
    "hive",
    "exit",
    "dynamic",
    "partition",
    "true",
    "means",
    "default",
    "false",
    "default",
    "dynamic",
    "partitions",
    "supported",
    "saying",
    "yes",
    "want",
    "dynamic",
    "partitions",
    "let",
    "copy",
    "even",
    "enable",
    "dynamic",
    "partitioning",
    "default",
    "high",
    "happens",
    "create",
    "partition",
    "table",
    "least",
    "one",
    "static",
    "partition",
    "bit",
    "complicat",
    "understand",
    "lot",
    "layers",
    "security",
    "everything",
    "override",
    "also",
    "hiive",
    "first",
    "say",
    "allow",
    "create",
    "dynamic",
    "partition",
    "static",
    "allowed",
    "say",
    "set",
    "high",
    "exic",
    "dynamic",
    "partition",
    "true",
    "means",
    "create",
    "dynamic",
    "partition",
    "thinking",
    "allowing",
    "create",
    "dynamic",
    "partition",
    "try",
    "creating",
    "thousand",
    "partition",
    "partition",
    "strict",
    "mode",
    "strict",
    "mode",
    "tell",
    "even",
    "dynamic",
    "partitioning",
    "okay",
    "first",
    "create",
    "one",
    "static",
    "partition",
    "table",
    "rest",
    "h",
    "one",
    "one",
    "column",
    "static",
    "rest",
    "everything",
    "dynamic",
    "turn",
    "many",
    "people",
    "confusion",
    "huh",
    "partition",
    "table",
    "right",
    "partition",
    "based",
    "uh",
    "country",
    "country",
    "region",
    "right",
    "saying",
    "country",
    "manually",
    "mention",
    "uploading",
    "country",
    "partition",
    "manually",
    "mention",
    "region",
    "high",
    "decide",
    "strict",
    "mode",
    "add",
    "manage",
    "internally",
    "saying",
    "manually",
    "mention",
    "least",
    "one",
    "static",
    "partition",
    "need",
    "created",
    "safety",
    "measure",
    "even",
    "production",
    "setups",
    "disable",
    "want",
    "three",
    "columns",
    "one",
    "column",
    "statically",
    "mention",
    "means",
    "nobody",
    "able",
    "allow",
    "right",
    "sometimes",
    "useful",
    "tell",
    "getting",
    "data",
    "country",
    "column",
    "okay",
    "want",
    "add",
    "country",
    "partition",
    "static",
    "data",
    "let",
    "say",
    "know",
    "state",
    "something",
    "already",
    "dynamic",
    "case",
    "turn",
    "mention",
    "country",
    "static",
    "al",
    "way",
    "rest",
    "dynamically",
    "creating",
    "turning",
    "developer",
    "environment",
    "fine",
    "create",
    "many",
    "partitions",
    "want",
    "huh",
    "normally",
    "true",
    "mean",
    "uh",
    "like",
    "dynamic",
    "partition",
    "false",
    "means",
    "create",
    "dynamic",
    "partitioning",
    "strict",
    "default",
    "uh",
    "yesterday",
    "somebody",
    "asked",
    "even",
    "forgot",
    "discuss",
    "create",
    "scripts",
    "hi",
    "script",
    "like",
    "sql",
    "script",
    "create",
    "sql",
    "script",
    "right",
    "sql",
    "file",
    "way",
    "copy",
    "commands",
    "text",
    "file",
    "save",
    "hql",
    "say",
    "run",
    "run",
    "script",
    "wo",
    "type",
    "everything",
    "right",
    "script",
    "first",
    "add",
    "things",
    "know",
    "set",
    "run",
    "usually",
    "sql",
    "also",
    "sql",
    "sql",
    "two",
    "technically",
    "technically",
    "tried",
    "anything",
    "apart",
    "normally",
    "try",
    "either",
    "sql",
    "hql",
    "think",
    "add",
    "work",
    "sql",
    "script",
    "work",
    "work",
    "hi",
    "tried",
    "scripts",
    "hql",
    "default",
    "people",
    "say",
    "hql",
    "tried",
    "might",
    "work",
    "saying",
    "specific",
    "session",
    "h",
    "language",
    "called",
    "hql",
    "hi",
    "query",
    "language",
    "okay",
    "script",
    "writing",
    "scripts",
    "shell",
    "ah",
    "query",
    "script",
    "different",
    "saying",
    "commands",
    "write",
    "text",
    "file",
    "notepad",
    "say",
    "run",
    "notepad",
    "execute",
    "one",
    "one",
    "copy",
    "paste",
    "time",
    "hql",
    "high",
    "query",
    "language",
    "huh",
    "said",
    "yesterday",
    "language",
    "hive",
    "called",
    "hql",
    "derived",
    "sql",
    "95",
    "similar",
    "uh",
    "rare",
    "uh",
    "know",
    "commands",
    "hive",
    "sql",
    "think",
    "sql",
    "2002",
    "dialect",
    "using",
    "sql",
    "2002",
    "anc",
    "formats",
    "right",
    "sql",
    "sql",
    "92",
    "sql",
    "2002",
    "sql",
    "2004",
    "hive",
    "built",
    "sql",
    "2002",
    "remember",
    "sql",
    "format",
    "saying",
    "syntax",
    "right",
    "find",
    "syntax",
    "different",
    "regular",
    "sql",
    "tax",
    "h",
    "partitioning",
    "close",
    "different",
    "right",
    "saying",
    "partitioning",
    "sql",
    "command",
    "slight",
    "difference",
    "two",
    "properties",
    "think",
    "understood",
    "enabling",
    "dynamic",
    "partitioning",
    "mode",
    "controlling",
    "hi",
    "xic",
    "maximum",
    "dynamic",
    "partition",
    "100",
    "means",
    "restricting",
    "maximum",
    "number",
    "dynamic",
    "partition",
    "100",
    "property",
    "actually",
    "uh",
    "correct",
    "property",
    "correct",
    "find",
    "different",
    "value",
    "example",
    "uh",
    "let",
    "write",
    "100",
    "means",
    "let",
    "say",
    "creating",
    "dynamic",
    "partitioning",
    "let",
    "say",
    "large",
    "file",
    "let",
    "say",
    "column",
    "150",
    "values",
    "countries",
    "imagine",
    "150",
    "partitions",
    "created",
    "partition",
    "created",
    "one",
    "reducer",
    "map",
    "reduce",
    "job",
    "end",
    "day",
    "creation",
    "partitioning",
    "table",
    "fire",
    "map",
    "reduce",
    "job",
    "end",
    "day",
    "one",
    "reducer",
    "work",
    "create",
    "one",
    "partition",
    "150",
    "partitions",
    "150",
    "reducers",
    "work",
    "control",
    "dynamic",
    "partition",
    "per",
    "node",
    "100",
    "means",
    "maximum",
    "number",
    "reducers",
    "100",
    "used",
    "want",
    "control",
    "number",
    "reducers",
    "okay",
    "advantage",
    "disadvantage",
    "different",
    "uh",
    "meaning",
    "sometimes",
    "going",
    "happen",
    "may",
    "really",
    "need",
    "150",
    "reducers",
    "job",
    "correct",
    "150",
    "partitions",
    "data",
    "less",
    "achieve",
    "using",
    "50",
    "reducers",
    "maybe",
    "say",
    "limited",
    "50",
    "50",
    "reducers",
    "launched",
    "saying",
    "values",
    "wrong",
    "saying",
    "wrong",
    "sense",
    "saying",
    "maximum",
    "number",
    "partition",
    "100",
    "fine",
    "okay",
    "maximum",
    "reducers",
    "100",
    "ideally",
    "100",
    "reducers",
    "run",
    "100",
    "partition",
    "mappers",
    "depend",
    "input",
    "split",
    "right",
    "like",
    "data",
    "getting",
    "control",
    "anyway",
    "reducers",
    "control",
    "point",
    "small",
    "file",
    "okay",
    "probably",
    "100",
    "partitions",
    "need",
    "100",
    "reducers",
    "probably",
    "10",
    "reducers",
    "job",
    "control",
    "property",
    "make",
    "properties",
    "available",
    "hi",
    "okay",
    "people",
    "thinking",
    "okay",
    "get",
    "properties",
    "properly",
    "documented",
    "give",
    "document",
    "copy",
    "properties",
    "go",
    "google",
    "paste",
    "high",
    "language",
    "manual",
    "come",
    "uh",
    "first",
    "one",
    "okay",
    "hoton",
    "ver",
    "mean",
    "original",
    "hive",
    "language",
    "manual",
    "come",
    "open",
    "oh",
    "search",
    "properties",
    "uh",
    "um",
    "hi",
    "exc",
    "dynamic",
    "partition",
    "default",
    "value",
    "false",
    "okay",
    "uh",
    "whether",
    "allow",
    "dynamic",
    "partition",
    "hi",
    "exit",
    "dynamic",
    "partition",
    "mode",
    "default",
    "value",
    "strict",
    "strict",
    "mode",
    "user",
    "must",
    "specify",
    "least",
    "one",
    "static",
    "partition",
    "right",
    "high",
    "vexx",
    "maximum",
    "dynamic",
    "partition",
    "default",
    "value",
    "th000",
    "high",
    "vexx",
    "maximum",
    "dynamic",
    "partition",
    "per",
    "node",
    "maximum",
    "number",
    "create",
    "mapper",
    "reducer",
    "node",
    "means",
    "many",
    "reducers",
    "used",
    "total",
    "right",
    "also",
    "uh",
    "properties",
    "hi",
    "look",
    "later",
    "anyway",
    "come",
    "enabled",
    "features",
    "right",
    "word",
    "file",
    "simply",
    "say",
    "insert",
    "command",
    "say",
    "insert",
    "data",
    "table",
    "partition",
    "table",
    "fire",
    "map",
    "reduce",
    "job",
    "see",
    "see",
    "screen",
    "create",
    "partitions",
    "look",
    "screen",
    "look",
    "screen",
    "also",
    "whichever",
    "see",
    "partitions",
    "loading",
    "loading",
    "loading",
    "show",
    "dynamic",
    "right",
    "see",
    "country",
    "null",
    "region",
    "null",
    "right",
    "see",
    "see",
    "country",
    "nigeria",
    "region",
    "poosi",
    "whatever",
    "om",
    "man",
    "see",
    "partitions",
    "loaded",
    "dynamically",
    "verify",
    "hadoop",
    "verify",
    "whether",
    "created",
    "hadoop",
    "right",
    "go",
    "um",
    "table",
    "table",
    "name",
    "go",
    "user",
    "two",
    "see",
    "korea",
    "came",
    "think",
    "come",
    "says",
    "korea",
    "north",
    "korea",
    "right",
    "courtes",
    "yeah",
    "disabled",
    "mode",
    "country",
    "take",
    "region",
    "region",
    "take",
    "part",
    "file",
    "output",
    "reducer",
    "z0",
    "comes",
    "reducers",
    "run",
    "right",
    "huh",
    "uh",
    "think",
    "know",
    "many",
    "reducers",
    "ran",
    "another",
    "thing",
    "zero",
    "right",
    "go",
    "something",
    "else",
    "yeah",
    "means",
    "many",
    "reducers",
    "ran",
    "right",
    "everything",
    "zero",
    "otherwise",
    "uh",
    "0",
    "1",
    "2",
    "check",
    "uh",
    "actually",
    "see",
    "statistics",
    "go",
    "workflow",
    "job",
    "browser",
    "job",
    "browser",
    "job",
    "ran",
    "show",
    "think",
    "showed",
    "first",
    "class",
    "go",
    "task",
    "displaying",
    "everything",
    "sewing",
    "stage",
    "one",
    "see",
    "anyway",
    "able",
    "run",
    "said",
    "yes",
    "think",
    "yes",
    "right",
    "see",
    "partitions",
    "know",
    "many",
    "reducers",
    "called",
    "see",
    "later",
    "able",
    "create",
    "look",
    "hdfs",
    "see",
    "whether",
    "find",
    "partitions",
    "uh",
    "input",
    "data",
    "right",
    "data",
    "need",
    "either",
    "changed",
    "think",
    "data",
    "like",
    "regular",
    "give",
    "like",
    "clean",
    "mean",
    "um",
    "usually",
    "mean",
    "limited",
    "records",
    "use",
    "either",
    "pig",
    "spar",
    "clean",
    "data",
    "easily",
    "clean",
    "say",
    "find",
    "codes",
    "replace",
    "first",
    "column",
    "name",
    "give",
    "partition",
    "top",
    "level",
    "partition",
    "within",
    "create",
    "many",
    "sub",
    "partitions",
    "actually",
    "rest",
    "partitions",
    "loading",
    "right",
    "problem",
    "think",
    "search",
    "way",
    "search",
    "uh",
    "uh",
    "rejections",
    "manually",
    "verify",
    "says",
    "country",
    "equal",
    "korea",
    "region",
    "equal",
    "north",
    "reject",
    "actually",
    "created",
    "korea",
    "region",
    "north",
    "actually",
    "incorrect",
    "okay",
    "per",
    "loading",
    "validate",
    "data",
    "way",
    "validators",
    "field",
    "validation",
    "whether",
    "fields",
    "like",
    "exception",
    "handling",
    "data",
    "set",
    "exception",
    "fine",
    "usually",
    "validate",
    "uh",
    "usually",
    "dynamic",
    "partitioning",
    "one",
    "thing",
    "dynamic",
    "partitioning",
    "um",
    "performance",
    "problems",
    "sometimes",
    "like",
    "creating",
    "many",
    "dynamic",
    "partitions",
    "advised",
    "even",
    "hadoop",
    "allow",
    "name",
    "node",
    "handle",
    "metadata",
    "subfolders",
    "files",
    "like",
    "150",
    "countries",
    "country",
    "states",
    "say",
    "divide",
    "country",
    "state",
    "thousands",
    "folders",
    "created",
    "increase",
    "metadata",
    "name",
    "node",
    "one",
    "way",
    "good",
    "queries",
    "faster",
    "way",
    "limit",
    "many",
    "create",
    "partitions",
    "say",
    "caution",
    "uh",
    "thing",
    "name",
    "note",
    "performance",
    "affected",
    "right",
    "like",
    "thousands",
    "partitions",
    "big",
    "problem",
    "many",
    "huge",
    "number",
    "partitions",
    "become",
    "problem",
    "actually",
    "uh",
    "table",
    "size",
    "usually",
    "use",
    "partitioning",
    "queries",
    "running",
    "slow",
    "like",
    "reason",
    "inside",
    "folder",
    "multiple",
    "files",
    "scan",
    "files",
    "cases",
    "partitioning",
    "may",
    "able",
    "find",
    "logic",
    "partitioning",
    "probably",
    "common",
    "column",
    "something",
    "ca",
    "partitioning",
    "way",
    "like",
    "everybody",
    "must",
    "partitioning",
    "rule",
    "like",
    "right",
    "somebody",
    "asked",
    "question",
    "right",
    "lead",
    "us",
    "getting",
    "uh",
    "let",
    "say",
    "getting",
    "data",
    "country",
    "uh",
    "side",
    "decided",
    "partition",
    "becomes",
    "country",
    "partition",
    "column",
    "150",
    "countries",
    "look",
    "amazon",
    "look",
    "dell",
    "example",
    "dell",
    "presence",
    "150",
    "countries",
    "okay",
    "customers",
    "buying",
    "products",
    "dell",
    "data",
    "country",
    "wise",
    "okay",
    "decided",
    "decided",
    "partition",
    "based",
    "country",
    "fine",
    "problem",
    "happened",
    "every",
    "time",
    "customer",
    "purchasing",
    "product",
    "transaction",
    "id",
    "right",
    "want",
    "write",
    "queries",
    "like",
    "select",
    "something",
    "okay",
    "country",
    "equal",
    "india",
    "okay",
    "comma",
    "transaction",
    "id",
    "equal",
    "1",
    "2",
    "3",
    "4",
    "problem",
    "already",
    "partition",
    "based",
    "country",
    "part",
    "saved",
    "look",
    "india",
    "within",
    "india",
    "millions",
    "transactions",
    "transaction",
    "file",
    "huge",
    "scan",
    "probably",
    "multiple",
    "transaction",
    "files",
    "okay",
    "trans",
    "partition",
    "based",
    "transaction",
    "id",
    "every",
    "transaction",
    "unique",
    "right",
    "question",
    "know",
    "whether",
    "available",
    "rdbms",
    "something",
    "called",
    "bucketing",
    "bucket",
    "uh",
    "something",
    "called",
    "hash",
    "partitioning",
    "right",
    "tell",
    "hash",
    "partitioning",
    "rdbms",
    "asking",
    "rdbms",
    "knows",
    "hash",
    "partitioning",
    "hash",
    "partitioning",
    "bucketing",
    "similar",
    "hash",
    "partitioning",
    "rd",
    "bms",
    "side",
    "done",
    "hash",
    "partitioning",
    "see",
    "transaction",
    "id",
    "column",
    "right",
    "transaction",
    "id",
    "column",
    "values",
    "let",
    "say",
    "1",
    "2",
    "3",
    "4",
    "right",
    "like",
    "transaction",
    "id",
    "let",
    "say",
    "1",
    "million",
    "data",
    "want",
    "divide",
    "data",
    "based",
    "transaction",
    "id",
    "partitioning",
    "say",
    "bucket",
    "bucketing",
    "done",
    "one",
    "column",
    "say",
    "two",
    "column",
    "bucketing",
    "possible",
    "say",
    "want",
    "create",
    "let",
    "say",
    "10",
    "buckets",
    "always",
    "mention",
    "number",
    "buckets",
    "want",
    "going",
    "happen",
    "within",
    "country",
    "partition",
    "create",
    "10",
    "files",
    "folders",
    "divide",
    "data",
    "based",
    "internal",
    "hashing",
    "logic",
    "okay",
    "create",
    "10",
    "buckets",
    "10",
    "files",
    "within",
    "country",
    "folder",
    "query",
    "hits",
    "calculate",
    "hash",
    "okay",
    "find",
    "file",
    "hit",
    "easily",
    "understand",
    "actual",
    "idea",
    "5",
    "6",
    "7",
    "8",
    "9",
    "10",
    "11",
    "12",
    "transaction",
    "idas",
    "want",
    "divide",
    "10",
    "files",
    "internal",
    "logic",
    "hi",
    "uses",
    "going",
    "tell",
    "logic",
    "say",
    "want",
    "10",
    "buckets",
    "10",
    "files",
    "created",
    "fine",
    "say",
    "1",
    "2",
    "3",
    "four",
    "five",
    "files",
    "okay",
    "let",
    "say",
    "take",
    "simple",
    "logic",
    "want",
    "find",
    "modulo",
    "10",
    "know",
    "modulo",
    "10",
    "right",
    "modo",
    "10",
    "one",
    "one",
    "go",
    "bucket",
    "also",
    "go",
    "able",
    "understand",
    "go",
    "go",
    "well",
    "like",
    "saying",
    "ah",
    "similar",
    "internal",
    "hashing",
    "logic",
    "write",
    "query",
    "look",
    "four",
    "go",
    "fourth",
    "bucket",
    "logic",
    "logic",
    "model",
    "10",
    "okay",
    "complicated",
    "logic",
    "internally",
    "divide",
    "bother",
    "internally",
    "divide",
    "many",
    "people",
    "misconception",
    "always",
    "use",
    "party",
    "bucketing",
    "together",
    "mean",
    "misconception",
    "people",
    "already",
    "working",
    "seen",
    "lot",
    "people",
    "discussing",
    "examples",
    "search",
    "want",
    "learn",
    "partitioning",
    "people",
    "say",
    "first",
    "create",
    "partition",
    "inside",
    "bucketing",
    "practice",
    "independently",
    "create",
    "bucketing",
    "without",
    "partitioning",
    "simply",
    "take",
    "table",
    "want",
    "bucket",
    "want",
    "partitioning",
    "anything",
    "happen",
    "folder",
    "create",
    "many",
    "files",
    "10",
    "files",
    "ah",
    "physical",
    "f",
    "see",
    "bucketing",
    "see",
    "practically",
    "see",
    "partitioning",
    "bucketing",
    "obvious",
    "thing",
    "partitioning",
    "enable",
    "bucketing",
    "uh",
    "example",
    "india",
    "within",
    "choice",
    "ideal",
    "way",
    "decide",
    "many",
    "buckets",
    "required",
    "mean",
    "next",
    "question",
    "many",
    "buckets",
    "10",
    "100",
    "uh",
    "ideally",
    "take",
    "care",
    "file",
    "size",
    "somewhere",
    "near",
    "block",
    "size",
    "create",
    "like",
    "1",
    "mb",
    "size",
    "bucket",
    "let",
    "say",
    "data",
    "10",
    "gb",
    "within",
    "partition",
    "okay",
    "say",
    "create",
    "buckets",
    "know",
    "become",
    "small",
    "create",
    "smaller",
    "files",
    "ideally",
    "take",
    "block",
    "size",
    "calculation",
    "many",
    "buckets",
    "need",
    "decide",
    "logic",
    "use",
    "actually",
    "data",
    "becomes",
    "big",
    "problem",
    "data",
    "become",
    "small",
    "problem",
    "like",
    "bucket",
    "size",
    "1",
    "mb",
    "create",
    "many",
    "small",
    "files",
    "hado",
    "right",
    "hado",
    "like",
    "bucket",
    "size",
    "500",
    "mb",
    "fine",
    "become",
    "1",
    "gb",
    "still",
    "divide",
    "blocks",
    "fine",
    "okay",
    "block",
    "size",
    "saying",
    "bigger",
    "block",
    "size",
    "smaller",
    "block",
    "size",
    "hadoop",
    "always",
    "prefer",
    "smaller",
    "files",
    "well",
    "handled",
    "hadoop",
    "okay",
    "ah",
    "join",
    "operations",
    "buckets",
    "excellent",
    "join",
    "operations",
    "something",
    "called",
    "bucket",
    "join",
    "h",
    "lot",
    "advanced",
    "concepts",
    "time",
    "may",
    "aware",
    "okay",
    "happen",
    "table",
    "okay",
    "column",
    "bucketed",
    "h",
    "means",
    "divided",
    "uh",
    "let",
    "say",
    "five",
    "files",
    "1",
    "2",
    "3",
    "4",
    "five",
    "another",
    "table",
    "also",
    "column",
    "also",
    "bucketed",
    "possible",
    "right",
    "also",
    "either",
    "number",
    "buckets",
    "multiple",
    "real",
    "fast",
    "bucket",
    "joint",
    "something",
    "called",
    "bucket",
    "joint",
    "since",
    "data",
    "divided",
    "equal",
    "equal",
    "partitions",
    "joints",
    "really",
    "fast",
    "actually",
    "bucket",
    "write",
    "joint",
    "query",
    "say",
    "use",
    "bucketed",
    "joint",
    "command",
    "use",
    "mean",
    "mismatch",
    "join",
    "operations",
    "common",
    "column",
    "join",
    "column",
    "usually",
    "similar",
    "uh",
    "entries",
    "right",
    "idea",
    "join",
    "otherwise",
    "use",
    "join",
    "ah",
    "like",
    "two",
    "tables",
    "want",
    "join",
    "operation",
    "normally",
    "join",
    "doubt",
    "join",
    "column",
    "common",
    "column",
    "bucketed",
    "okay",
    "something",
    "called",
    "bucket",
    "joint",
    "use",
    "faster",
    "high",
    "internally",
    "make",
    "faster",
    "since",
    "bucketed",
    "join",
    "operation",
    "become",
    "faster",
    "uh",
    "hashing",
    "multiple",
    "types",
    "bucket",
    "joints",
    "actually",
    "hi",
    "interesting",
    "uh",
    "bit",
    "advanced",
    "concept",
    "things",
    "possible",
    "anyway",
    "rare",
    "may",
    "get",
    "chance",
    "people",
    "aware",
    "things",
    "like",
    "bucket",
    "joints",
    "right",
    "bucketing",
    "practically",
    "better",
    "right",
    "rather",
    "talking",
    "look",
    "another",
    "data",
    "set",
    "open",
    "bucketing",
    "data",
    "open",
    "excel",
    "right",
    "see",
    "interesting",
    "case",
    "study",
    "appreciate",
    "uh",
    "partitioning",
    "similar",
    "things",
    "see",
    "street",
    "right",
    "street",
    "almost",
    "unique",
    "values",
    "unique",
    "least",
    "city",
    "sacramento",
    "zip",
    "code",
    "state",
    "california",
    "uh",
    "bedroom",
    "bathroom",
    "square",
    "fet",
    "thing",
    "real",
    "estate",
    "data",
    "type",
    "residential",
    "condo",
    "blah",
    "blah",
    "blah",
    "price",
    "price",
    "right",
    "many",
    "lines",
    "data",
    "986",
    "lines",
    "data",
    "intention",
    "bucketing",
    "partitioning",
    "also",
    "see",
    "anyway",
    "want",
    "first",
    "upload",
    "data",
    "upload",
    "file",
    "name",
    "realore",
    "state",
    "csv",
    "file",
    "name",
    "okay",
    "want",
    "open",
    "word",
    "pad",
    "bucketing",
    "hive",
    "yeah",
    "going",
    "first",
    "create",
    "table",
    "okay",
    "normal",
    "table",
    "okay",
    "say",
    "create",
    "table",
    "r",
    "state",
    "called",
    "real",
    "estate",
    "schema",
    "regular",
    "schema",
    "partitioning",
    "bucketing",
    "uh",
    "terminated",
    "comma",
    "regular",
    "stuff",
    "changes",
    "load",
    "data",
    "load",
    "data",
    "load",
    "data",
    "path",
    "guess",
    "able",
    "load",
    "data",
    "create",
    "load",
    "data",
    "enable",
    "bucketing",
    "bucketing",
    "disabled",
    "say",
    "set",
    "hive",
    "enforce",
    "bucketing",
    "true",
    "command",
    "hive",
    "enforce",
    "bucketing",
    "equal",
    "true",
    "enable",
    "bucketing",
    "right",
    "look",
    "command",
    "probably",
    "important",
    "command",
    "create",
    "table",
    "uh",
    "calling",
    "table",
    "bucket",
    "underscore",
    "table",
    "schema",
    "partition",
    "city",
    "clustered",
    "street",
    "four",
    "buckets",
    "mean",
    "partition",
    "city",
    "city",
    "common",
    "entries",
    "right",
    "think",
    "sacrament",
    "something",
    "say",
    "partition",
    "column",
    "city",
    "say",
    "clustered",
    "street",
    "street",
    "column",
    "four",
    "buckets",
    "create",
    "uh",
    "many",
    "city",
    "partitions",
    "partition",
    "four",
    "uh",
    "thing",
    "call",
    "buckets",
    "mandatory",
    "another",
    "thing",
    "city",
    "one",
    "line",
    "data",
    "one",
    "bucket",
    "data",
    "four",
    "buckets",
    "make",
    "sense",
    "cities",
    "one",
    "street",
    "let",
    "say",
    "create",
    "one",
    "bucket",
    "case",
    "um",
    "uh",
    "load",
    "see",
    "whether",
    "see",
    "buckets",
    "want",
    "try",
    "disabled",
    "yeah",
    "even",
    "bucketing",
    "disabled",
    "want",
    "try",
    "uh",
    "insert",
    "insert",
    "complete",
    "go",
    "back",
    "user",
    "high",
    "warehouse",
    "folder",
    "able",
    "find",
    "partitions",
    "buckets",
    "even",
    "show",
    "buckets",
    "mean",
    "like",
    "want",
    "see",
    "buckets",
    "user",
    "hive",
    "sorry",
    "hive",
    "warehouse",
    "uh",
    "minus",
    "may9",
    "db",
    "may",
    "19",
    "db",
    "table",
    "haveck",
    "bucket",
    "table",
    "open",
    "bucket",
    "table",
    "city",
    "open",
    "city",
    "four",
    "buckets",
    "uh",
    "bucketing",
    "also",
    "depend",
    "distribution",
    "data",
    "see",
    "last",
    "two",
    "buckets",
    "data",
    "one",
    "line",
    "uh",
    "two",
    "lines",
    "like",
    "one",
    "two",
    "buckets",
    "created",
    "high",
    "intelligent",
    "like",
    "less",
    "data",
    "wo",
    "create",
    "four",
    "buckets",
    "think",
    "like",
    "four",
    "10",
    "lines",
    "know",
    "many",
    "based",
    "create",
    "last",
    "two",
    "buckets",
    "empty",
    "example",
    "ideally",
    "data",
    "happening",
    "less",
    "data",
    "right",
    "thousand",
    "lines",
    "actually",
    "number",
    "mappers",
    "right",
    "h",
    "either",
    "mapper",
    "reducer",
    "said",
    "reducer",
    "show",
    "property",
    "visible",
    "either",
    "mapper",
    "reducer",
    "uh",
    "created",
    "partition",
    "uh",
    "saying",
    "able",
    "see",
    "reducer",
    "jobs",
    "mapper",
    "jobs",
    "ran",
    "see",
    "maximum",
    "number",
    "dynamic",
    "partitions",
    "allowed",
    "created",
    "mapper",
    "sl",
    "reducer",
    "mapper",
    "job",
    "reducer",
    "job",
    "less",
    "amount",
    "data",
    "mapper",
    "run",
    "sometimes",
    "happens",
    "huge",
    "amount",
    "data",
    "mappers",
    "first",
    "move",
    "data",
    "reducers",
    "aggregate",
    "different",
    "buckets",
    "partitions",
    "saying",
    "uh",
    "able",
    "see",
    "reducers",
    "job",
    "saying",
    "either",
    "mapper",
    "reducer",
    "firing",
    "partition",
    "query",
    "depends",
    "resource",
    "allocation",
    "cluster",
    "resources",
    "allocate",
    "got",
    "one",
    "mapper",
    "mine",
    "actually",
    "one",
    "mapper",
    "reducer",
    "yeah",
    "either",
    "mapper",
    "reducer",
    "matter",
    "actually",
    "honest",
    "uh",
    "going",
    "happen",
    "created",
    "partition",
    "table",
    "right",
    "insert",
    "happening",
    "map",
    "ruce",
    "job",
    "map",
    "ruce",
    "job",
    "question",
    "segregate",
    "data",
    "right",
    "logic",
    "segregating",
    "data",
    "based",
    "partition",
    "bucket",
    "know",
    "right",
    "check",
    "partitioning",
    "bucketing",
    "also",
    "firing",
    "reducer",
    "saying",
    "recent",
    "one",
    "right",
    "bucketing",
    "getting",
    "reducer",
    "zero",
    "mapper",
    "many",
    "one",
    "mapper",
    "yeah",
    "even",
    "got",
    "one",
    "mapper",
    "sometimes",
    "happens",
    "cluster",
    "enough",
    "resources",
    "fire",
    "mappers",
    "reducers",
    "complete",
    "really",
    "fast",
    "mappers",
    "first",
    "probably",
    "segregate",
    "based",
    "partition",
    "reducer",
    "calculate",
    "hash",
    "dump",
    "cluster",
    "enough",
    "resources",
    "job",
    "slow",
    "mappers",
    "launched",
    "complete",
    "whole",
    "thing",
    "intelligence",
    "saying",
    "way",
    "cluster",
    "managing",
    "partitioning",
    "process",
    "huh",
    "logic",
    "written",
    "really",
    "say",
    "done",
    "reducer",
    "mapper",
    "right",
    "sometimes",
    "achieved",
    "using",
    "mapper",
    "sometimes",
    "shuffle",
    "fire",
    "reducer",
    "also",
    "yes",
    "case",
    "done",
    "one",
    "mapper",
    "map",
    "reducer",
    "case",
    "think",
    "four",
    "reducers",
    "launched",
    "probably",
    "shuffle",
    "operation",
    "depends",
    "data",
    "also",
    "another",
    "thing",
    "reducer",
    "shuffling",
    "data",
    "data",
    "shuffles",
    "goes",
    "reducer",
    "know",
    "probably",
    "case",
    "shuffle",
    "operation",
    "happened",
    "reducer",
    "kicked",
    "anyway",
    "really",
    "important",
    "saying",
    "mean",
    "inserting",
    "partition",
    "table",
    "really",
    "matter",
    "whether",
    "mapper",
    "reducer",
    "impact",
    "developer",
    "side",
    "uh",
    "finding",
    "column",
    "move",
    "moving",
    "data",
    "query",
    "important",
    "write",
    "proper",
    "query",
    "many",
    "mappers",
    "reducers",
    "get",
    "involved",
    "important",
    "actually",
    "hive",
    "also",
    "store",
    "result",
    "text",
    "file",
    "let",
    "say",
    "write",
    "query",
    "output",
    "normally",
    "uh",
    "shown",
    "screen",
    "like",
    "write",
    "query",
    "show",
    "say",
    "save",
    "hdfs",
    "output",
    "whatever",
    "query",
    "output",
    "command",
    "uh",
    "insert",
    "overwrite",
    "directory",
    "command",
    "called",
    "insert",
    "overwrite",
    "directory",
    "write",
    "query",
    "happen",
    "whatever",
    "output",
    "query",
    "save",
    "text",
    "file",
    "hadoop",
    "want",
    "save",
    "right",
    "normally",
    "want",
    "see",
    "sometimes",
    "want",
    "say",
    "want",
    "also",
    "directly",
    "insert",
    "override",
    "directory",
    "remember",
    "rarely",
    "save",
    "text",
    "file",
    "okay",
    "hive",
    "insert",
    "overwrite",
    "directory",
    "huh",
    "one",
    "insert",
    "right",
    "directory",
    "uh",
    "see",
    "try",
    "command",
    "insert",
    "override",
    "directory",
    "give",
    "uh",
    "path",
    "say",
    "select",
    "star",
    "let",
    "try",
    "say",
    "directory",
    "um",
    "say",
    "uh",
    "spelling",
    "oh",
    "direct",
    "insert",
    "override",
    "directory",
    "say",
    "abc",
    "count",
    "star",
    "uh",
    "give",
    "table",
    "name",
    "created",
    "nasa",
    "log",
    "right",
    "nasa",
    "log",
    "right",
    "means",
    "create",
    "folder",
    "hadoop",
    "ragu",
    "abc",
    "result",
    "stored",
    "query",
    "result",
    "want",
    "store",
    "somewhere",
    "try",
    "see",
    "creating",
    "hyp",
    "table",
    "store",
    "text",
    "file",
    "default",
    "whatever",
    "data",
    "loading",
    "hy",
    "table",
    "available",
    "hadoop",
    "text",
    "format",
    "like",
    "csv",
    "text",
    "format",
    "period",
    "years",
    "lot",
    "compression",
    "techniques",
    "compress",
    "data",
    "creating",
    "table",
    "mention",
    "store",
    "data",
    "good",
    "hoton",
    "works",
    "links",
    "share",
    "um",
    "way",
    "okay",
    "anyway",
    "going",
    "see",
    "right",
    "open",
    "mail",
    "shared",
    "links",
    "wanted",
    "show",
    "think",
    "assignment",
    "orc",
    "right",
    "correct",
    "easy",
    "mean",
    "storing",
    "data",
    "easy",
    "need",
    "understand",
    "happening",
    "yeah",
    "rare",
    "uh",
    "customers",
    "hot",
    "mail",
    "right",
    "never",
    "quit",
    "hot",
    "mail",
    "hoton",
    "works",
    "links",
    "found",
    "interesting",
    "okay",
    "orc",
    "format",
    "hot",
    "works",
    "link",
    "uh",
    "orc",
    "files",
    "hdp",
    "better",
    "compression",
    "better",
    "performance",
    "creating",
    "hive",
    "table",
    "last",
    "line",
    "add",
    "store",
    "orc",
    "one",
    "thing",
    "need",
    "say",
    "store",
    "orc",
    "going",
    "happen",
    "whatever",
    "data",
    "loading",
    "table",
    "high",
    "compress",
    "orc",
    "stands",
    "optimized",
    "row",
    "column",
    "format",
    "compression",
    "technique",
    "advantage",
    "first",
    "advantage",
    "data",
    "compressed",
    "well",
    "great",
    "advantage",
    "maybe",
    "second",
    "advantage",
    "important",
    "advantage",
    "orc",
    "indexing",
    "technique",
    "called",
    "indexing",
    "right",
    "hive",
    "default",
    "indexing",
    "poor",
    "enable",
    "indexing",
    "get",
    "performance",
    "orc",
    "compress",
    "data",
    "whatever",
    "data",
    "compressed",
    "uh",
    "file",
    "create",
    "index",
    "inside",
    "hit",
    "queries",
    "columnar",
    "queries",
    "queries",
    "really",
    "fast",
    "table",
    "orc",
    "property",
    "column",
    "indexing",
    "regular",
    "column",
    "indexing",
    "right",
    "rdbms",
    "indexing",
    "order",
    "faster",
    "queries",
    "create",
    "sort",
    "like",
    "say",
    "pointer",
    "original",
    "data",
    "orc",
    "create",
    "index",
    "row",
    "column",
    "well",
    "row",
    "indexing",
    "column",
    "indexing",
    "create",
    "everything",
    "actually",
    "efficient",
    "apart",
    "orc",
    "want",
    "create",
    "normal",
    "indexing",
    "hyp",
    "something",
    "called",
    "uh",
    "bit",
    "map",
    "index",
    "compact",
    "index",
    "inefficient",
    "actually",
    "even",
    "create",
    "index",
    "hive",
    "uh",
    "actually",
    "dealing",
    "name",
    "node",
    "hdfs",
    "know",
    "data",
    "orc",
    "happening",
    "asking",
    "hive",
    "manage",
    "data",
    "compress",
    "data",
    "create",
    "index",
    "whole",
    "data",
    "store",
    "share",
    "link",
    "scroll",
    "one",
    "things",
    "say",
    "585",
    "gb",
    "file",
    "hive",
    "store",
    "text",
    "file",
    "size",
    "format",
    "called",
    "rc",
    "file",
    "become",
    "55",
    "another",
    "format",
    "called",
    "par",
    "become",
    "221",
    "131",
    "gb",
    "level",
    "compression",
    "getting",
    "par",
    "rc",
    "compression",
    "formats",
    "uh",
    "rc",
    "u",
    "common",
    "orc",
    "optimization",
    "rc",
    "rc",
    "raw",
    "columnar",
    "orc",
    "optimized",
    "ra",
    "columnar",
    "rc",
    "nobody",
    "using",
    "par",
    "people",
    "use",
    "park",
    "also",
    "another",
    "uh",
    "compression",
    "technique",
    "park",
    "indexing",
    "anything",
    "simply",
    "compress",
    "data",
    "orc",
    "advantage",
    "uh",
    "know",
    "indexing",
    "within",
    "right",
    "one",
    "advantage",
    "uh",
    "ao",
    "serialization",
    "format",
    "uh",
    "say",
    "let",
    "say",
    "sending",
    "lot",
    "data",
    "send",
    "data",
    "along",
    "schema",
    "normally",
    "mention",
    "schema",
    "table",
    "ao",
    "format",
    "apply",
    "schema",
    "even",
    "text",
    "files",
    "without",
    "table",
    "anything",
    "sending",
    "data",
    "headr",
    "around",
    "schema",
    "common",
    "hadoop",
    "systems",
    "use",
    "ao",
    "format",
    "okay",
    "scroll",
    "um",
    "orc",
    "store",
    "data",
    "one",
    "thing",
    "need",
    "understand",
    "create",
    "index",
    "let",
    "say",
    "first",
    "rows",
    "mention",
    "many",
    "rows",
    "need",
    "create",
    "index",
    "row",
    "column",
    "index",
    "saying",
    "rows",
    "want",
    "index",
    "first",
    "rows",
    "create",
    "index",
    "store",
    "next",
    "rows",
    "create",
    "index",
    "store",
    "somebody",
    "quing",
    "understand",
    "uh",
    "know",
    "uh",
    "group",
    "rows",
    "push",
    "even",
    "skip",
    "uh",
    "rows",
    "one",
    "shot",
    "let",
    "say",
    "writing",
    "filter",
    "condition",
    "since",
    "knows",
    "inside",
    "simply",
    "skip",
    "entire",
    "row",
    "scan",
    "fast",
    "orc",
    "formats",
    "ability",
    "something",
    "called",
    "vectorization",
    "hi",
    "advanced",
    "property",
    "vectorization",
    "allow",
    "read",
    "thousands",
    "rows",
    "one",
    "shot",
    "normally",
    "reading",
    "data",
    "reads",
    "one",
    "row",
    "second",
    "row",
    "third",
    "row",
    "enable",
    "vectorization",
    "orc",
    "orc",
    "read",
    "thousands",
    "rows",
    "one",
    "shot",
    "vectorization",
    "work",
    "orc",
    "orc",
    "format",
    "supports",
    "vectorization",
    "also",
    "batch",
    "indexing",
    "properties",
    "available",
    "go",
    "hoton",
    "works",
    "uh",
    "told",
    "acid",
    "properties",
    "height",
    "enable",
    "realtime",
    "queries",
    "condition",
    "table",
    "orc",
    "acid",
    "work",
    "reasons",
    "people",
    "prefer",
    "orc",
    "wherever",
    "possible",
    "one",
    "drawback",
    "advantages",
    "one",
    "drawback",
    "exactly",
    "decompress",
    "meaning",
    "uh",
    "cpu",
    "cycles",
    "might",
    "required",
    "anything",
    "compressed",
    "decompress",
    "compress",
    "okay",
    "storing",
    "okay",
    "querying",
    "uncompress",
    "data",
    "right",
    "cpu",
    "cycles",
    "required",
    "still",
    "people",
    "preferring",
    "uh",
    "okay",
    "spend",
    "money",
    "cpus",
    "still",
    "faster",
    "gives",
    "uh",
    "uh",
    "properties",
    "go",
    "link",
    "uh",
    "one",
    "link",
    "okay",
    "stride",
    "one",
    "stride",
    "rows",
    "called",
    "one",
    "stride",
    "one",
    "block",
    "rows",
    "called",
    "one",
    "stride",
    "actually",
    "okay",
    "uh",
    "uh",
    "speed",
    "queries",
    "1",
    "terab",
    "data",
    "hi",
    "hi",
    "fine",
    "hi",
    "plus",
    "vectorized",
    "query",
    "high",
    "plus",
    "ppd",
    "ppd",
    "predicate",
    "push",
    "know",
    "predicate",
    "push",
    "like",
    "sending",
    "filter",
    "first",
    "right",
    "write",
    "query",
    "like",
    "let",
    "say",
    "saying",
    "join",
    "data",
    "filter",
    "data",
    "bad",
    "right",
    "saying",
    "join",
    "data",
    "filter",
    "data",
    "first",
    "filter",
    "data",
    "join",
    "default",
    "rdbms",
    "first",
    "bring",
    "filter",
    "together",
    "optimize",
    "hive",
    "manually",
    "called",
    "ppd",
    "predicate",
    "push",
    "enable",
    "ppd",
    "along",
    "orc",
    "filter",
    "come",
    "first",
    "join",
    "come",
    "push",
    "predicate",
    "basically",
    "idea",
    "orc",
    "yeah",
    "show",
    "mean",
    "uh",
    "blabbering",
    "look",
    "uh",
    "uh",
    "cloud",
    "era",
    "um",
    "thing",
    "uh",
    "cloud",
    "manager",
    "right",
    "cloud",
    "lab",
    "look",
    "cloud",
    "era",
    "manager",
    "see",
    "whether",
    "properties",
    "enabled",
    "probably",
    "enabled",
    "one",
    "thing",
    "let",
    "check",
    "go",
    "go",
    "hi",
    "go",
    "configuration",
    "know",
    "say",
    "ppd",
    "uh",
    "showing",
    "ah",
    "enable",
    "vectorization",
    "optimization",
    "okay",
    "enabled",
    "vectorization",
    "property",
    "uh",
    "storing",
    "data",
    "orc",
    "read",
    "thousands",
    "rows",
    "one",
    "shot",
    "enable",
    "vectorization",
    "read",
    "thousands",
    "rows",
    "one",
    "go",
    "give",
    "makes",
    "vectorization",
    "enabled",
    "ppd",
    "seen",
    "know",
    "whether",
    "enabled",
    "uh",
    "something",
    "search",
    "ppd",
    "predicate",
    "push",
    "see",
    "anything",
    "predicate",
    "probably",
    "property",
    "might",
    "available",
    "ultimately",
    "reading",
    "get",
    "bulk",
    "get",
    "row",
    "row",
    "default",
    "nature",
    "slow",
    "actually",
    "hi",
    "reading",
    "data",
    "row",
    "format",
    "right",
    "creating",
    "table",
    "saying",
    "row",
    "format",
    "del",
    "limited",
    "let",
    "say",
    "query",
    "result",
    "requires",
    "let",
    "say",
    "output",
    "rows",
    "push",
    "rows",
    "say",
    "row",
    "one",
    "row",
    "two",
    "row",
    "three",
    "like",
    "come",
    "actually",
    "slow",
    "original",
    "hi",
    "enable",
    "vectorization",
    "read",
    "minimum",
    "thousand",
    "vectorization",
    "minimum",
    "th",
    "rows",
    "mention",
    "whatever",
    "want",
    "many",
    "come",
    "one",
    "shot",
    "fast",
    "actually",
    "getting",
    "result",
    "vectorization",
    "requires",
    "orc",
    "format",
    "work",
    "vectorization",
    "reader",
    "vectorization",
    "available",
    "orc",
    "think",
    "format",
    "supports",
    "vectorization",
    "right",
    "question",
    "enable",
    "orc",
    "right",
    "optimized",
    "row",
    "column",
    "format",
    "huh",
    "things",
    "important",
    "okay",
    "let",
    "see",
    "show",
    "another",
    "one",
    "things",
    "go",
    "create",
    "table",
    "need",
    "say",
    "stored",
    "orc",
    "last",
    "line",
    "add",
    "stored",
    "orc",
    "become",
    "orc",
    "take",
    "default",
    "values",
    "even",
    "disable",
    "compression",
    "people",
    "enough",
    "cpu",
    "power",
    "want",
    "use",
    "indexing",
    "want",
    "compress",
    "data",
    "say",
    "compression",
    "none",
    "compress",
    "data",
    "possible",
    "uh",
    "properties",
    "mention",
    "orc",
    "mention",
    "compression",
    "default",
    "algorithm",
    "called",
    "zip",
    "efficient",
    "either",
    "say",
    "none",
    "z",
    "snappy",
    "three",
    "options",
    "zib",
    "snappy",
    "compression",
    "algorithms",
    "say",
    "none",
    "orc",
    "table",
    "created",
    "compression",
    "orc",
    "compression",
    "size",
    "number",
    "bytes",
    "compression",
    "chunk",
    "put",
    "compression",
    "happen",
    "case",
    "256",
    "kb",
    "default",
    "orc",
    "stripe",
    "size",
    "number",
    "bytes",
    "stripe",
    "256",
    "mb",
    "told",
    "right",
    "rows",
    "like",
    "size",
    "256",
    "mb",
    "say",
    "table",
    "probably",
    "table",
    "1",
    "tb",
    "data",
    "divide",
    "256",
    "256",
    "like",
    "row",
    "index",
    "stri",
    "every",
    "row",
    "index",
    "created",
    "increase",
    "decrease",
    "compression",
    "size",
    "uh",
    "number",
    "bytes",
    "compression",
    "chunk",
    "means",
    "uh",
    "compressing",
    "uh",
    "creates",
    "something",
    "called",
    "chunks",
    "inside",
    "size",
    "chunk",
    "minimum",
    "size",
    "actually",
    "default",
    "256",
    "kb",
    "depends",
    "data",
    "well",
    "leave",
    "things",
    "default",
    "change",
    "compression",
    "size",
    "uh",
    "uh",
    "algorithm",
    "z",
    "snappy",
    "none",
    "change",
    "stripe",
    "size",
    "dividing",
    "data",
    "like",
    "stripe",
    "size",
    "256",
    "mb",
    "256",
    "mb",
    "size",
    "one",
    "stripe",
    "right",
    "stripe",
    "get",
    "rows",
    "create",
    "index",
    "probably",
    "one",
    "stripe",
    "create",
    "like",
    "five",
    "six",
    "indexes",
    "depending",
    "many",
    "rows",
    "one",
    "mentioning",
    "size",
    "within",
    "many",
    "rows",
    "one",
    "index",
    "need",
    "created",
    "okay",
    "sometimes",
    "mean",
    "properties",
    "read",
    "understand",
    "anyway",
    "okay",
    "mention",
    "also",
    "normally",
    "create",
    "orc",
    "table",
    "say",
    "stored",
    "orc",
    "better",
    "orc",
    "take",
    "care",
    "compression",
    "values",
    "adjust",
    "want",
    "tune",
    "simply",
    "say",
    "store",
    "also",
    "fine",
    "yes",
    "say",
    "compression",
    "none",
    "compress",
    "data",
    "index",
    "definitely",
    "index",
    "compression",
    "default",
    "text",
    "file",
    "long",
    "people",
    "using",
    "text",
    "file",
    "know",
    "orc",
    "evolving",
    "standard",
    "okay",
    "uh",
    "orc",
    "different",
    "different",
    "versions",
    "every",
    "high",
    "release",
    "release",
    "new",
    "standard",
    "orc",
    "text",
    "file",
    "standard",
    "format",
    "want",
    "force",
    "anybody",
    "use",
    "particular",
    "standard",
    "right",
    "better",
    "want",
    "use",
    "thing",
    "ca",
    "read",
    "orc",
    "file",
    "see",
    "read",
    "like",
    "compressed",
    "hi",
    "read",
    "hive",
    "create",
    "read",
    "look",
    "like",
    "uh",
    "uh",
    "like",
    "zip",
    "file",
    "wo",
    "able",
    "open",
    "see",
    "side",
    "hi",
    "orc",
    "readers",
    "read",
    "actually",
    "like",
    "want",
    "set",
    "small",
    "setup",
    "like",
    "production",
    "want",
    "production",
    "obviously",
    "need",
    "machines",
    "may",
    "setting",
    "hadoop",
    "hive",
    "nobody",
    "people",
    "complete",
    "package",
    "actually",
    "say",
    "hadoop",
    "hive",
    "two",
    "tools",
    "right",
    "rather",
    "setting",
    "hadoop",
    "hy",
    "easy",
    "machines",
    "go",
    "one",
    "vendor",
    "either",
    "cloud",
    "hoton",
    "works",
    "get",
    "distribution",
    "install",
    "using",
    "apache",
    "hadoop",
    "first",
    "download",
    "original",
    "source",
    "extract",
    "four",
    "files",
    "core",
    "site",
    "xml",
    "hdf",
    "site",
    "xml",
    "yan",
    "site",
    "xml",
    "mapsite",
    "xml",
    "files",
    "xml",
    "files",
    "manually",
    "type",
    "properties",
    "probably",
    "take",
    "year",
    "complete",
    "almost",
    "succeed",
    "even",
    "succeed",
    "difficult",
    "nobody",
    "used",
    "used",
    "set",
    "apache",
    "clusters",
    "complicated",
    "everything",
    "manual",
    "automation",
    "everything",
    "use",
    "also",
    "use",
    "probably",
    "want",
    "open",
    "source",
    "apache",
    "hoton",
    "works",
    "hoton",
    "works",
    "edition",
    "apache",
    "edition",
    "exactly",
    "difference",
    "yeah",
    "even",
    "claer",
    "hoton",
    "works",
    "free",
    "edition",
    "okay",
    "since",
    "companies",
    "selling",
    "open",
    "source",
    "products",
    "go",
    "pon",
    "works",
    "cloud",
    "era",
    "download",
    "product",
    "install",
    "many",
    "machines",
    "want",
    "problem",
    "get",
    "technical",
    "support",
    "cloud",
    "era",
    "edition",
    "called",
    "express",
    "edition",
    "okay",
    "cluster",
    "cluster",
    "express",
    "edition",
    "right",
    "check",
    "cloud",
    "era",
    "right",
    "uh",
    "somewhere",
    "think",
    "maybe",
    "able",
    "see",
    "addition",
    "access",
    "everything",
    "right",
    "huh",
    "see",
    "claer",
    "express",
    "free",
    "running",
    "eight",
    "notes",
    "10",
    "notes",
    "right",
    "cluster",
    "absolutely",
    "free",
    "pay",
    "single",
    "penny",
    "cloud",
    "something",
    "happens",
    "wo",
    "help",
    "like",
    "technical",
    "support",
    "commercial",
    "edition",
    "called",
    "cloudera",
    "enterprise",
    "installing",
    "cluster",
    "ask",
    "want",
    "express",
    "want",
    "enterprise",
    "say",
    "enterprise",
    "ask",
    "please",
    "browse",
    "upload",
    "license",
    "key",
    "installation",
    "proceed",
    "actually",
    "want",
    "test",
    "try",
    "something",
    "people",
    "run",
    "hundreds",
    "notes",
    "uh",
    "edition",
    "uh",
    "express",
    "edition",
    "hon",
    "workor",
    "sandbox",
    "run",
    "multiple",
    "notes",
    "single",
    "machine",
    "setup",
    "manually",
    "install",
    "uh",
    "quite",
    "sure",
    "whether",
    "work",
    "sandbox",
    "designed",
    "one",
    "machine",
    "designed",
    "distributed",
    "like",
    "running",
    "expanded",
    "even",
    "connect",
    "wo",
    "work",
    "working",
    "independently",
    "sandbox",
    "virtual",
    "machine",
    "download",
    "install",
    "pc",
    "start",
    "uh",
    "everything",
    "hadu",
    "p",
    "spark",
    "everything",
    "learn",
    "like",
    "single",
    "machine",
    "setup",
    "production",
    "setup",
    "totally",
    "different",
    "installing",
    "comes",
    "admin",
    "side",
    "talking",
    "much",
    "may",
    "able",
    "understand",
    "say",
    "lot",
    "things",
    "apart",
    "hadoop",
    "something",
    "set",
    "lot",
    "repositories",
    "machines",
    "mirroring",
    "racking",
    "setup",
    "let",
    "say",
    "100",
    "machines",
    "one",
    "become",
    "master",
    "every",
    "machine",
    "use",
    "either",
    "cloud",
    "manager",
    "tool",
    "download",
    "hadoop",
    "distribute",
    "first",
    "need",
    "download",
    "tool",
    "called",
    "cloud",
    "manager",
    "download",
    "one",
    "machine",
    "start",
    "cloud",
    "era",
    "manager",
    "ask",
    "many",
    "machines",
    "say",
    "four",
    "machines",
    "ask",
    "one",
    "want",
    "master",
    "say",
    "machine",
    "download",
    "install",
    "everything",
    "cloud",
    "manager",
    "install",
    "one",
    "machine",
    "start",
    "immediately",
    "ask",
    "many",
    "machines",
    "give",
    "ip",
    "detect",
    "ask",
    "one",
    "name",
    "node",
    "one",
    "data",
    "one",
    "select",
    "download",
    "hado",
    "distribute",
    "install",
    "set",
    "give",
    "cluster",
    "cluster",
    "created",
    "actually",
    "limitation",
    "thousands",
    "machines",
    "people",
    "actually",
    "mean",
    "production",
    "setup",
    "go",
    "enterprise",
    "licensing",
    "uh",
    "even",
    "express",
    "edition",
    "pretty",
    "stable",
    "express",
    "edition",
    "uh",
    "features",
    "enterprise",
    "edition",
    "like",
    "uh",
    "something",
    "called",
    "rolling",
    "restarts",
    "rolling",
    "upgrades",
    "example",
    "let",
    "say",
    "want",
    "restart",
    "machine",
    "cluster",
    "usually",
    "restart",
    "tons",
    "problems",
    "name",
    "detect",
    "sometimes",
    "hadoop",
    "cluster",
    "might",
    "want",
    "restart",
    "machines",
    "applied",
    "patches",
    "something",
    "sometimes",
    "want",
    "upgrade",
    "hadoop",
    "version",
    "running",
    "want",
    "go",
    "upgrade",
    "happen",
    "every",
    "machine",
    "restart",
    "uh",
    "production",
    "cluster",
    "difficult",
    "express",
    "edition",
    "way",
    "every",
    "machine",
    "go",
    "ender",
    "press",
    "edition",
    "something",
    "called",
    "rolling",
    "restart",
    "means",
    "one",
    "one",
    "restart",
    "machines",
    "without",
    "affecting",
    "cluster",
    "end",
    "day",
    "every",
    "machine",
    "could",
    "restarted",
    "services",
    "affected",
    "also",
    "mirroring",
    "backups",
    "advan",
    "backup",
    "mirroring",
    "solutions",
    "available",
    "enterprise",
    "addition",
    "features",
    "like",
    "spark",
    "whatever",
    "tools",
    "want",
    "everything",
    "commissioning",
    "commission",
    "fly",
    "add",
    "two",
    "machines",
    "nothing",
    "happen",
    "two",
    "machines",
    "get",
    "added",
    "immediately",
    "start",
    "become",
    "part",
    "cluster",
    "called",
    "commissioning",
    "uh",
    "know",
    "show",
    "able",
    "add",
    "go",
    "host",
    "menu",
    "something",
    "called",
    "commission",
    "state",
    "commission",
    "11",
    "means",
    "11",
    "machines",
    "running",
    "ca",
    "add",
    "uh",
    "permission",
    "option",
    "called",
    "add",
    "machine",
    "commission",
    "right",
    "says",
    "commissioned",
    "11",
    "h",
    "everything",
    "commissioned",
    "admin",
    "rights",
    "otherwise",
    "say",
    "click",
    "add",
    "add",
    "flight",
    "problem",
    "spark",
    "requires",
    "level",
    "understanding",
    "actually",
    "go",
    "hands",
    "side",
    "hands",
    "spark",
    "sure",
    "lot",
    "hands",
    "directly",
    "start",
    "okay",
    "write",
    "program",
    "understand",
    "right",
    "slides",
    "really",
    "slides",
    "person",
    "still",
    "topics",
    "understand",
    "go",
    "spark",
    "right",
    "um",
    "first",
    "thing",
    "need",
    "understand",
    "spark",
    "got",
    "spark",
    "right",
    "people",
    "excited",
    "spark",
    "uh",
    "previous",
    "session",
    "trainer",
    "right",
    "uh",
    "name",
    "uh",
    "asking",
    "learn",
    "spark",
    "really",
    "want",
    "learn",
    "spark",
    "power",
    "spark",
    "saying",
    "mean",
    "even",
    "want",
    "learn",
    "machine",
    "learning",
    "saying",
    "way",
    "uh",
    "want",
    "leverage",
    "spark",
    "asking",
    "doubts",
    "like",
    "uh",
    "get",
    "started",
    "spark",
    "want",
    "machine",
    "learning",
    "spark",
    "uh",
    "everybody",
    "excited",
    "spark",
    "trainings",
    "days",
    "spark",
    "majority",
    "trainings",
    "spark",
    "uh",
    "lot",
    "excitement",
    "tool",
    "spark",
    "popular",
    "magic",
    "something",
    "spark",
    "right",
    "uh",
    "easy",
    "understand",
    "popular",
    "uh",
    "somewhere",
    "2009",
    "okay",
    "uh",
    "project",
    "called",
    "amp",
    "lab",
    "university",
    "berkeley",
    "research",
    "project",
    "called",
    "amplab",
    "amplab",
    "project",
    "even",
    "live",
    "today",
    "research",
    "project",
    "uh",
    "university",
    "berkeley",
    "people",
    "trying",
    "create",
    "new",
    "cluster",
    "manager",
    "remember",
    "yan",
    "yeah",
    "yan",
    "resource",
    "manager",
    "hadoop",
    "hado",
    "version",
    "2",
    "came",
    "2012",
    "seeing",
    "today",
    "yan",
    "came",
    "2012",
    "right",
    "yan",
    "came",
    "hadoop",
    "talking",
    "2009",
    "2009",
    "yan",
    "running",
    "hadoop",
    "one",
    "old",
    "hadoop",
    "version",
    "good",
    "cluster",
    "managers",
    "folks",
    "university",
    "berkeley",
    "trying",
    "create",
    "cluster",
    "manager",
    "similar",
    "yarn",
    "okay",
    "time",
    "yarn",
    "okay",
    "call",
    "project",
    "mesos",
    "mesos",
    "mesos",
    "project",
    "name",
    "given",
    "project",
    "okay",
    "guys",
    "r",
    "last",
    "created",
    "cluster",
    "manager",
    "like",
    "similar",
    "yan",
    "mesos",
    "even",
    "apache",
    "project",
    "similar",
    "yarn",
    "yarn",
    "became",
    "popular",
    "yarn",
    "coming",
    "fall",
    "hadoop",
    "download",
    "hadoop",
    "get",
    "yarn",
    "want",
    "use",
    "use",
    "mesos",
    "install",
    "separately",
    "hadoop",
    "run",
    "top",
    "mesos",
    "also",
    "difference",
    "yarn",
    "layer",
    "handled",
    "mesos",
    "okay",
    "mesos",
    "famous",
    "project",
    "even",
    "today",
    "folks",
    "use",
    "actually",
    "everybody",
    "use",
    "mesos",
    "created",
    "2009",
    "amplab",
    "created",
    "mesos",
    "said",
    "great",
    "cluster",
    "manager",
    "going",
    "use",
    "blah",
    "blah",
    "blah",
    "right",
    "order",
    "test",
    "power",
    "mesos",
    "okay",
    "first",
    "ran",
    "map",
    "ruce",
    "programs",
    "course",
    "map",
    "ruce",
    "originally",
    "running",
    "fine",
    "thought",
    "use",
    "uh",
    "different",
    "programming",
    "framework",
    "created",
    "something",
    "called",
    "spark",
    "spark",
    "created",
    "actually",
    "test",
    "mesos",
    "meaning",
    "mesos",
    "cluster",
    "manager",
    "created",
    "new",
    "uh",
    "programming",
    "framework",
    "called",
    "spark",
    "write",
    "program",
    "using",
    "spark",
    "run",
    "mesos",
    "difference",
    "spark",
    "completely",
    "inmemory",
    "execution",
    "like",
    "use",
    "ram",
    "cluster",
    "manager",
    "lot",
    "ram",
    "getting",
    "used",
    "actually",
    "test",
    "right",
    "whether",
    "cluster",
    "manager",
    "good",
    "bad",
    "created",
    "spark",
    "uh",
    "programming",
    "framework",
    "wherein",
    "write",
    "spark",
    "program",
    "use",
    "majority",
    "ram",
    "faster",
    "processing",
    "tested",
    "mesos",
    "mesos",
    "became",
    "success",
    "everybody",
    "happy",
    "happy",
    "people",
    "never",
    "thought",
    "much",
    "spark",
    "created",
    "sub",
    "project",
    "okay",
    "year",
    "like",
    "2010",
    "11",
    "people",
    "thought",
    "hey",
    "spark",
    "good",
    "write",
    "program",
    "spark",
    "first",
    "thing",
    "people",
    "noticed",
    "running",
    "faster",
    "compared",
    "map",
    "reduce",
    "tell",
    "running",
    "faster",
    "people",
    "thought",
    "develop",
    "spark",
    "bit",
    "people",
    "write",
    "spark",
    "program",
    "running",
    "faster",
    "great",
    "right",
    "development",
    "started",
    "somewhere",
    "2012",
    "uh",
    "believe",
    "spark",
    "became",
    "apache",
    "project",
    "contributed",
    "apache",
    "said",
    "know",
    "created",
    "new",
    "product",
    "make",
    "open",
    "source",
    "framework",
    "like",
    "map",
    "reduce",
    "write",
    "spark",
    "program",
    "instead",
    "map",
    "reduce",
    "program",
    "run",
    "faster",
    "map",
    "reduce",
    "apache",
    "thought",
    "okay",
    "let",
    "try",
    "became",
    "apache",
    "top",
    "top",
    "level",
    "project",
    "okay",
    "everything",
    "changed",
    "2012",
    "actually",
    "spark",
    "quite",
    "new",
    "uh",
    "became",
    "widely",
    "popular",
    "somewhere",
    "around",
    "2015",
    "started",
    "using",
    "spark",
    "2016",
    "world",
    "right",
    "like",
    "good",
    "right",
    "became",
    "popular",
    "somewhere",
    "15",
    "16",
    "started",
    "using",
    "spark",
    "okay",
    "even",
    "2016",
    "less",
    "trainings",
    "spark",
    "people",
    "like",
    "excited",
    "nobody",
    "knew",
    "park",
    "started",
    "using",
    "2016",
    "sudden",
    "end",
    "like",
    "december",
    "2016",
    "became",
    "active",
    "apache",
    "project",
    "history",
    "2018",
    "17",
    "18",
    "uh",
    "biggest",
    "contributed",
    "apache",
    "project",
    "span",
    "two",
    "three",
    "years",
    "gained",
    "lot",
    "popularity",
    "right",
    "spark",
    "like",
    "said",
    "popular",
    "apache",
    "project",
    "far",
    "apache",
    "around",
    "300",
    "100",
    "plus",
    "projects",
    "number",
    "one",
    "spark",
    "right",
    "also",
    "originally",
    "spark",
    "version",
    "zero",
    "got",
    "spark",
    "version",
    "one",
    "spark",
    "version",
    "latest",
    "version",
    "many",
    "companies",
    "run",
    "something",
    "called",
    "spark",
    "stable",
    "stable",
    "version",
    "reason",
    "run",
    "probably",
    "know",
    "right",
    "next",
    "spark",
    "version",
    "major",
    "release",
    "okay",
    "right",
    "go",
    "spark",
    "website",
    "say",
    "currently",
    "pretty",
    "stable",
    "latest",
    "version",
    "stable",
    "uh",
    "stable",
    "actually",
    "learning",
    "spark",
    "two",
    "learning",
    "spark",
    "one",
    "missing",
    "anything",
    "spark",
    "improvements",
    "features",
    "one",
    "available",
    "two",
    "part",
    "covered",
    "uh",
    "trainings",
    "request",
    "please",
    "start",
    "spar",
    "two",
    "uh",
    "mean",
    "start",
    "spark",
    "one",
    "meaning",
    "old",
    "people",
    "want",
    "start",
    "spark",
    "2",
    "major",
    "version",
    "right",
    "two",
    "say",
    "using",
    "something",
    "else",
    "happened",
    "actually",
    "made",
    "spark",
    "popular",
    "2012",
    "guys",
    "founders",
    "spark",
    "two",
    "guys",
    "gave",
    "apache",
    "said",
    "take",
    "spark",
    "go",
    "happy",
    "guys",
    "parall",
    "found",
    "company",
    "called",
    "data",
    "bricks",
    "data",
    "bricks",
    "founders",
    "spark",
    "gave",
    "apache",
    "became",
    "open",
    "source",
    "obviously",
    "also",
    "want",
    "make",
    "money",
    "right",
    "open",
    "source",
    "ca",
    "make",
    "money",
    "give",
    "something",
    "apache",
    "actually",
    "contributing",
    "world",
    "probably",
    "like",
    "good",
    "scenario",
    "give",
    "apache",
    "like",
    "microsoft",
    "windows",
    "microsoft",
    "windows",
    "available",
    "apache",
    "right",
    "moneymaking",
    "getting",
    "chance",
    "modify",
    "anything",
    "add",
    "contributions",
    "anything",
    "apache",
    "contributions",
    "uh",
    "commercial",
    "versions",
    "done",
    "company",
    "growth",
    "limited",
    "simple",
    "growth",
    "limited",
    "like",
    "developers",
    "modify",
    "others",
    "allowed",
    "went",
    "apache",
    "happened",
    "apache",
    "spark",
    "version",
    "apache",
    "spark",
    "anybody",
    "download",
    "100",
    "free",
    "open",
    "source",
    "spk",
    "available",
    "clera",
    "hoton",
    "works",
    "mapar",
    "sure",
    "know",
    "using",
    "cloud",
    "data",
    "cluster",
    "spark",
    "installed",
    "spark",
    "use",
    "guys",
    "2012",
    "found",
    "data",
    "bricks",
    "okay",
    "company",
    "sole",
    "purpose",
    "sell",
    "spark",
    "nothing",
    "else",
    "company",
    "special",
    "created",
    "founders",
    "spark",
    "people",
    "actually",
    "wrote",
    "sour",
    "code",
    "found",
    "company",
    "want",
    "spark",
    "probably",
    "go",
    "guys",
    "best",
    "people",
    "go",
    "people",
    "go",
    "cloud",
    "hoten",
    "bs",
    "chances",
    "rare",
    "want",
    "spark",
    "want",
    "tool",
    "sour",
    "like",
    "scoop",
    "want",
    "flume",
    "want",
    "data",
    "bricks",
    "nothing",
    "scoop",
    "flume",
    "spark",
    "right",
    "uh",
    "one",
    "participant",
    "asked",
    "question",
    "get",
    "back",
    "moment",
    "still",
    "spark",
    "independent",
    "project",
    "means",
    "require",
    "hadoop",
    "mostly",
    "see",
    "top",
    "hadoop",
    "okay",
    "spark",
    "actually",
    "independent",
    "project",
    "means",
    "download",
    "spark",
    "spark",
    "running",
    "windows",
    "10",
    "laptop",
    "beautifully",
    "works",
    "require",
    "anything",
    "right",
    "ntfs",
    "f",
    "system",
    "work",
    "okay",
    "people",
    "point",
    "um",
    "like",
    "right",
    "migration",
    "happening",
    "ge",
    "ge",
    "last",
    "week",
    "coming",
    "migration",
    "happen",
    "g",
    "migrating",
    "spark",
    "spark",
    "hado",
    "right",
    "g",
    "already",
    "hadoop",
    "cluster",
    "install",
    "spark",
    "separately",
    "right",
    "already",
    "hadoop",
    "cluster",
    "install",
    "spark",
    "top",
    "hadoop",
    "ecosystem",
    "tool",
    "start",
    "leveraging",
    "hdfs",
    "store",
    "storage",
    "processing",
    "times",
    "see",
    "spark",
    "top",
    "hadoop",
    "mean",
    "run",
    "top",
    "ofu",
    "run",
    "anywhere",
    "practically",
    "show",
    "slides",
    "wherever",
    "run",
    "however",
    "run",
    "forth",
    "uh",
    "data",
    "bricks",
    "popular",
    "case",
    "available",
    "al",
    "everywhere",
    "right",
    "think",
    "uh",
    "run",
    "slides",
    "get",
    "idea",
    "uh",
    "disclaimer",
    "slides",
    "data",
    "bre",
    "marketing",
    "slides",
    "uh",
    "best",
    "resource",
    "learn",
    "spark",
    "okay",
    "couple",
    "things",
    "know",
    "teach",
    "right",
    "uh",
    "lot",
    "people",
    "ask",
    "like",
    "uh",
    "give",
    "best",
    "book",
    "learn",
    "spark",
    "suggest",
    "one",
    "book",
    "never",
    "come",
    "back",
    "best",
    "book",
    "learn",
    "spark",
    "something",
    "called",
    "spark",
    "definitive",
    "guide",
    "buy",
    "read",
    "make",
    "note",
    "spark",
    "definitive",
    "guide",
    "best",
    "book",
    "even",
    "touch",
    "saying",
    "okay",
    "probably",
    "buy",
    "mind",
    "written",
    "uh",
    "z",
    "mahara",
    "founder",
    "spark",
    "guy",
    "wrote",
    "source",
    "code",
    "wrote",
    "book",
    "book",
    "pretty",
    "much",
    "like",
    "source",
    "code",
    "wo",
    "understand",
    "anything",
    "even",
    "understand",
    "half",
    "book",
    "good",
    "buy",
    "keep",
    "people",
    "ask",
    "show",
    "spark",
    "right",
    "uh",
    "difficult",
    "understand",
    "like",
    "accurate",
    "information",
    "guy",
    "wrot",
    "spark",
    "writing",
    "book",
    "right",
    "read",
    "rdd",
    "go",
    "level",
    "explain",
    "bit",
    "difficult",
    "understand",
    "probably",
    "read",
    "immediately",
    "complete",
    "spark",
    "class",
    "asignment",
    "start",
    "book",
    "get",
    "give",
    "good",
    "idea",
    "read",
    "books",
    "related",
    "spark",
    "available",
    "forgot",
    "name",
    "huh",
    "learning",
    "spark",
    "right",
    "good",
    "learning",
    "spark",
    "easy",
    "tough",
    "actually",
    "okay",
    "version",
    "two",
    "guys",
    "also",
    "wrote",
    "spark",
    "definitely",
    "got",
    "spark",
    "version",
    "one",
    "easy",
    "actually",
    "bit",
    "tough",
    "know",
    "one",
    "place",
    "second",
    "documentation",
    "apache",
    "datab",
    "bricks",
    "learn",
    "actually",
    "open",
    "apache",
    "documentation",
    "open",
    "datab",
    "bricks",
    "documentation",
    "give",
    "lot",
    "ideas",
    "mean",
    "basics",
    "least",
    "want",
    "learn",
    "right",
    "uh",
    "okay",
    "learning",
    "spark",
    "book",
    "keep",
    "think",
    "available",
    "india",
    "anybody",
    "try",
    "buying",
    "flip",
    "card",
    "spark",
    "definitive",
    "amazon",
    "also",
    "ah",
    "mat",
    "zaharia",
    "bill",
    "chambers",
    "people",
    "wrote",
    "spark",
    "okay",
    "okay",
    "um",
    "good",
    "mean",
    "try",
    "want",
    "get",
    "much",
    "flip",
    "cart",
    "costly",
    "right",
    "pdf",
    "download",
    "ah",
    "paper",
    "gap",
    "back",
    "2018",
    "actually",
    "right",
    "latest",
    "uh",
    "edition",
    "right",
    "um",
    "basic",
    "operation",
    "u",
    "lot",
    "things",
    "hadoop",
    "two",
    "books",
    "actually",
    "maybe",
    "new",
    "books",
    "better",
    "one",
    "hadoop",
    "definitive",
    "guide",
    "um",
    "one",
    "book",
    "right",
    "hadoop",
    "one",
    "hadoop",
    "definitive",
    "guide",
    "park",
    "definitive",
    "guide",
    "also",
    "hadoop",
    "hadoop",
    "definitive",
    "guide",
    "uh",
    "one",
    "good",
    "book",
    "hadoop",
    "um",
    "hadoop",
    "action",
    "think",
    "also",
    "good",
    "book",
    "okay",
    "hadu",
    "books",
    "bit",
    "easy",
    "understand",
    "uh",
    "sense",
    "like",
    "spk",
    "book",
    "oriented",
    "sour",
    "code",
    "side",
    "like",
    "exactly",
    "stuff",
    "work",
    "hadu",
    "books",
    "bit",
    "easy",
    "h",
    "slide",
    "need",
    "understand",
    "want",
    "learn",
    "spark",
    "right",
    "uh",
    "slide",
    "important",
    "also",
    "really",
    "want",
    "learn",
    "spark",
    "important",
    "somewhere",
    "2004",
    "hadoop",
    "came",
    "year",
    "uh",
    "hadoop",
    "got",
    "something",
    "called",
    "map",
    "reduce",
    "know",
    "map",
    "reduce",
    "teach",
    "right",
    "till",
    "2015",
    "say",
    "see",
    "2007",
    "till",
    "2015",
    "happened",
    "one",
    "problems",
    "hadoop",
    "map",
    "ruce",
    "fine",
    "problem",
    "map",
    "ruce",
    "batch",
    "processing",
    "slow",
    "batch",
    "processing",
    "people",
    "wanted",
    "try",
    "different",
    "different",
    "different",
    "type",
    "workloads",
    "hadoop",
    "wanted",
    "write",
    "sql",
    "queries",
    "top",
    "hadoop",
    "got",
    "hive",
    "uh",
    "got",
    "impala",
    "got",
    "presto",
    "drill",
    "uh",
    "many",
    "tools",
    "around",
    "10",
    "plus",
    "different",
    "sql",
    "tools",
    "explore",
    "data",
    "hado",
    "people",
    "wanted",
    "uh",
    "know",
    "machine",
    "learning",
    "done",
    "top",
    "hadu",
    "earlier",
    "versions",
    "spark",
    "people",
    "want",
    "machine",
    "learning",
    "uh",
    "use",
    "something",
    "called",
    "mahoot",
    "mahoot",
    "h",
    "see",
    "mahoot",
    "somewhere",
    "uh",
    "machine",
    "learning",
    "library",
    "problem",
    "machine",
    "learning",
    "okay",
    "guys",
    "teach",
    "right",
    "machine",
    "learning",
    "sitting",
    "fine",
    "iteration",
    "mean",
    "iteration",
    "like",
    "taking",
    "data",
    "iterating",
    "right",
    "one",
    "things",
    "map",
    "reduce",
    "iteration",
    "write",
    "map",
    "reduce",
    "program",
    "mapper",
    "run",
    "producer",
    "run",
    "push",
    "data",
    "hard",
    "disk",
    "second",
    "iteration",
    "read",
    "maho",
    "machine",
    "learning",
    "problem",
    "slow",
    "internally",
    "uses",
    "map",
    "ruce",
    "top",
    "hadoop",
    "mahud",
    "came",
    "became",
    "machine",
    "learning",
    "uh",
    "tool",
    "okay",
    "interactive",
    "covered",
    "graph",
    "processing",
    "got",
    "graph",
    "lag",
    "pral",
    "tools",
    "use",
    "graph",
    "processing",
    "hado",
    "traditional",
    "hado",
    "spark",
    "nothing",
    "people",
    "wanted",
    "streaming",
    "data",
    "analysis",
    "realtime",
    "data",
    "got",
    "storm",
    "right",
    "see",
    "problem",
    "right",
    "period",
    "8",
    "10",
    "years",
    "people",
    "started",
    "developing",
    "different",
    "different",
    "tools",
    "handle",
    "different",
    "different",
    "workload",
    "today",
    "go",
    "hadoop",
    "system",
    "new",
    "hadoop",
    "ask",
    "somebody",
    "tool",
    "learn",
    "confused",
    "things",
    "learn",
    "pig",
    "hive",
    "maho",
    "everything",
    "start",
    "working",
    "hadoop",
    "another",
    "problem",
    "let",
    "say",
    "one",
    "fine",
    "day",
    "start",
    "learning",
    "storm",
    "storm",
    "realtime",
    "processing",
    "first",
    "learn",
    "storm",
    "api",
    "storm",
    "language",
    "install",
    "top",
    "hadu",
    "learn",
    "integration",
    "starts",
    "working",
    "learning",
    "different",
    "tools",
    "actually",
    "became",
    "problem",
    "okay",
    "happened",
    "2014",
    "spark",
    "came",
    "biggest",
    "usb",
    "spark",
    "unified",
    "processing",
    "engine",
    "uh",
    "50",
    "plus",
    "tools",
    "hadoop",
    "spark",
    "alone",
    "real",
    "uh",
    "advantage",
    "spark",
    "many",
    "people",
    "confus",
    "go",
    "anybody",
    "ask",
    "like",
    "somebody",
    "learn",
    "spark",
    "say",
    "faster",
    "speed",
    "byproduct",
    "spark",
    "faster",
    "know",
    "real",
    "reason",
    "people",
    "migrating",
    "spark",
    "tools",
    "whatever",
    "spark",
    "alone",
    "learn",
    "like",
    "hundreds",
    "tools",
    "learn",
    "one",
    "tool",
    "one",
    "language",
    "everything",
    "need",
    "basic",
    "idea",
    "going",
    "example",
    "want",
    "machine",
    "learning",
    "spark",
    "something",
    "called",
    "mlb",
    "machine",
    "learning",
    "library",
    "allow",
    "machine",
    "learning",
    "know",
    "machine",
    "learning",
    "otherwise",
    "explore",
    "install",
    "10",
    "tools",
    "20",
    "tools",
    "spark",
    "alone",
    "primary",
    "reason",
    "people",
    "going",
    "spark",
    "general",
    "unified",
    "engine",
    "experience",
    "second",
    "reason",
    "speed",
    "obviously",
    "faster",
    "tell",
    "faster",
    "faster",
    "third",
    "reason",
    "ease",
    "programming",
    "like",
    "know",
    "python",
    "get",
    "started",
    "python",
    "spark",
    "language",
    "python",
    "want",
    "write",
    "sql",
    "queries",
    "streaming",
    "machine",
    "learning",
    "everything",
    "use",
    "python",
    "spark",
    "case",
    "traditional",
    "world",
    "pig",
    "uses",
    "different",
    "language",
    "storm",
    "uses",
    "different",
    "language",
    "every",
    "tool",
    "actually",
    "use",
    "different",
    "language",
    "even",
    "graph",
    "processing",
    "spark",
    "use",
    "python",
    "normal",
    "python",
    "ease",
    "programming",
    "another",
    "thing",
    "struggled",
    "lot",
    "worked",
    "couple",
    "tools",
    "worked",
    "part",
    "project",
    "major",
    "problem",
    "take",
    "three",
    "four",
    "months",
    "learn",
    "tool",
    "first",
    "come",
    "back",
    "tool",
    "obsolete",
    "say",
    "go",
    "learn",
    "tool",
    "spark",
    "gap",
    "gone",
    "every",
    "company",
    "want",
    "migrate",
    "spark",
    "asking",
    "um",
    "spark",
    "something",
    "called",
    "streaming",
    "come",
    "streaming",
    "data",
    "means",
    "want",
    "process",
    "real",
    "time",
    "data",
    "say",
    "gave",
    "example",
    "right",
    "let",
    "say",
    "uh",
    "credit",
    "card",
    "fraud",
    "detection",
    "swipe",
    "credit",
    "card",
    "immediately",
    "captured",
    "system",
    "say",
    "whether",
    "fraud",
    "real",
    "time",
    "right",
    "implement",
    "one",
    "solution",
    "spark",
    "streaming",
    "spark",
    "ca",
    "okay",
    "spark",
    "something",
    "called",
    "micro",
    "batching",
    "meaning",
    "able",
    "capture",
    "single",
    "swipe",
    "collect",
    "one",
    "second",
    "worth",
    "data",
    "go",
    "one",
    "second",
    "one",
    "second",
    "worth",
    "data",
    "using",
    "something",
    "like",
    "storm",
    "even",
    "pick",
    "one",
    "swipe",
    "microc",
    "second",
    "say",
    "swipe",
    "saying",
    "important",
    "like",
    "one",
    "second",
    "worth",
    "delay",
    "suffer",
    "least",
    "still",
    "manage",
    "spark",
    "cases",
    "yes",
    "spark",
    "supports",
    "streaming",
    "pral",
    "giraffe",
    "guys",
    "gone",
    "graph",
    "processing",
    "totally",
    "gone",
    "sparkk",
    "maho",
    "gone",
    "told",
    "right",
    "uh",
    "soon",
    "gone",
    "almost",
    "soon",
    "gone",
    "stas",
    "middle",
    "map",
    "produce",
    "spark",
    "makes",
    "map",
    "reduce",
    "faster",
    "basically",
    "spark",
    "makes",
    "faster",
    "middle",
    "almost",
    "gone",
    "couple",
    "years",
    "hoton",
    "works",
    "company",
    "promoting",
    "probably",
    "keep",
    "want",
    "market",
    "share",
    "right",
    "reason",
    "okay",
    "let",
    "see",
    "spark",
    "right",
    "spark",
    "scheduling",
    "monitoring",
    "distributing",
    "show",
    "practically",
    "like",
    "uh",
    "diagram",
    "lot",
    "show",
    "spark",
    "architecture",
    "nice",
    "way",
    "actually",
    "look",
    "like",
    "look",
    "diagram",
    "think",
    "everybody",
    "saying",
    "solar",
    "system",
    "point",
    "core",
    "spark",
    "download",
    "spark",
    "get",
    "something",
    "called",
    "spark",
    "spark",
    "core",
    "okay",
    "lowest",
    "layer",
    "abstraction",
    "spark",
    "start",
    "programming",
    "directly",
    "show",
    "see",
    "ram",
    "hard",
    "disk",
    "picture",
    "came",
    "means",
    "spark",
    "use",
    "ram",
    "hard",
    "disk",
    "tell",
    "important",
    "later",
    "usually",
    "systems",
    "also",
    "use",
    "four",
    "languages",
    "currently",
    "supported",
    "spark",
    "code",
    "programming",
    "scala",
    "icon",
    "scala",
    "python",
    "r",
    "java",
    "230",
    "four",
    "languages",
    "supported",
    "either",
    "learn",
    "already",
    "know",
    "python",
    "makes",
    "sense",
    "continue",
    "python",
    "teach",
    "spark",
    "scala",
    "also",
    "uh",
    "source",
    "code",
    "spark",
    "actually",
    "written",
    "scala",
    "reason",
    "folks",
    "want",
    "learn",
    "scala",
    "uh",
    "earlier",
    "spark",
    "version",
    "write",
    "python",
    "code",
    "slower",
    "one",
    "thing",
    "understand",
    "spark",
    "write",
    "spark",
    "code",
    "python",
    "slow",
    "scala",
    "code",
    "twice",
    "faster",
    "like",
    "optimization",
    "java",
    "also",
    "slow",
    "slower",
    "uh",
    "scala",
    "fastest",
    "one",
    "scala",
    "point",
    "time",
    "correct",
    "correct",
    "still",
    "bit",
    "slow",
    "implement",
    "certain",
    "libraries",
    "related",
    "java",
    "support",
    "uh",
    "know",
    "optimizations",
    "java",
    "rare",
    "spar",
    "two",
    "written",
    "written",
    "abstraction",
    "layer",
    "write",
    "code",
    "languages",
    "program",
    "run",
    "speed",
    "matter",
    "writing",
    "python",
    "scala",
    "java",
    "everything",
    "worry",
    "sql",
    "also",
    "sql",
    "language",
    "mean",
    "also",
    "write",
    "queries",
    "using",
    "sql",
    "okay",
    "go",
    "one",
    "orbit",
    "yeah",
    "thinking",
    "spark",
    "able",
    "everything",
    "right",
    "able",
    "libraries",
    "available",
    "spark",
    "meaning",
    "download",
    "spark",
    "get",
    "something",
    "called",
    "spark",
    "fine",
    "like",
    "map",
    "reduce",
    "write",
    "maybe",
    "core",
    "programmer",
    "know",
    "sql",
    "want",
    "write",
    "sql",
    "use",
    "something",
    "called",
    "spark",
    "sql",
    "spark",
    "sql",
    "much",
    "like",
    "hive",
    "create",
    "tables",
    "something",
    "called",
    "data",
    "frame",
    "create",
    "query",
    "data",
    "good",
    "news",
    "integrated",
    "spark",
    "sql",
    "hive",
    "default",
    "happen",
    "hadoop",
    "cluster",
    "install",
    "spark",
    "okay",
    "open",
    "spark",
    "sql",
    "read",
    "data",
    "hive",
    "table",
    "previously",
    "running",
    "queries",
    "hive",
    "slow",
    "tables",
    "read",
    "spar",
    "sql",
    "query",
    "days",
    "hive",
    "used",
    "storage",
    "processing",
    "done",
    "spark",
    "sql",
    "faster",
    "ideally",
    "right",
    "uh",
    "another",
    "library",
    "ml",
    "saying",
    "machine",
    "learning",
    "libraries",
    "whatever",
    "learning",
    "implement",
    "think",
    "deep",
    "learning",
    "supported",
    "deep",
    "learning",
    "using",
    "tensor",
    "flow",
    "deep",
    "learning",
    "syllabus",
    "yeah",
    "huh",
    "default",
    "come",
    "saying",
    "download",
    "import",
    "separately",
    "h",
    "third",
    "party",
    "regression",
    "like",
    "normal",
    "stuff",
    "right",
    "supported",
    "default",
    "algorithms",
    "learned",
    "far",
    "one",
    "regression",
    "na",
    "buys",
    "decision",
    "trees",
    "blah",
    "blah",
    "blah",
    "okay",
    "h",
    "coming",
    "ml",
    "anyway",
    "graphic",
    "framework",
    "represent",
    "data",
    "using",
    "graph",
    "ever",
    "done",
    "graph",
    "processing",
    "right",
    "done",
    "okay",
    "important",
    "studied",
    "uh",
    "engineering",
    "computer",
    "science",
    "could",
    "learned",
    "graph",
    "theory",
    "right",
    "mean",
    "learned",
    "know",
    "syllabus",
    "something",
    "called",
    "graph",
    "theory",
    "stuff",
    "like",
    "edges",
    "relations",
    "know",
    "define",
    "like",
    "right",
    "u",
    "one",
    "write",
    "relations",
    "like",
    "like",
    "probably",
    "like",
    "like",
    "someone",
    "else",
    "like",
    "mean",
    "property",
    "graphs",
    "edges",
    "okay",
    "represents",
    "property",
    "relations",
    "okay",
    "query",
    "structure",
    "type",
    "uh",
    "partially",
    "work",
    "social",
    "media",
    "company",
    "one",
    "projects",
    "like",
    "facebook",
    "startup",
    "actually",
    "entire",
    "data",
    "like",
    "even",
    "facebook",
    "right",
    "download",
    "data",
    "facebook",
    "difficult",
    "okay",
    "ca",
    "directly",
    "way",
    "get",
    "data",
    "data",
    "means",
    "publicly",
    "available",
    "data",
    "give",
    "graph",
    "api",
    "format",
    "format",
    "data",
    "comes",
    "graph",
    "come",
    "json",
    "xml",
    "graph",
    "api",
    "format",
    "facebook",
    "actually",
    "using",
    "graph",
    "structure",
    "store",
    "spar",
    "graph",
    "think",
    "system",
    "representation",
    "form",
    "graph",
    "actually",
    "many",
    "times",
    "required",
    "graph",
    "representation",
    "data",
    "spar",
    "graphics",
    "library",
    "represent",
    "data",
    "stream",
    "streaming",
    "data",
    "streaming",
    "data",
    "saying",
    "real",
    "time",
    "data",
    "coming",
    "want",
    "process",
    "real",
    "time",
    "make",
    "decision",
    "based",
    "yeah",
    "queries",
    "know",
    "uh",
    "write",
    "like",
    "much",
    "efficient",
    "relational",
    "queries",
    "say",
    "example",
    "guy",
    "like",
    "guy",
    "gu",
    "want",
    "find",
    "patterns",
    "relations",
    "representation",
    "using",
    "graph",
    "easy",
    "cases",
    "let",
    "give",
    "another",
    "example",
    "airport",
    "data",
    "airport",
    "ports",
    "flight",
    "data",
    "right",
    "let",
    "say",
    "represent",
    "airports",
    "like",
    "bangalore",
    "chennai",
    "know",
    "usa",
    "somewhere",
    "right",
    "represent",
    "flights",
    "going",
    "know",
    "using",
    "sql",
    "also",
    "graph",
    "queries",
    "much",
    "much",
    "faster",
    "directory",
    "structure",
    "right",
    "graph",
    "structure",
    "able",
    "easy",
    "traverse",
    "graph",
    "api",
    "probably",
    "two",
    "airports",
    "like",
    "thousands",
    "flights",
    "currently",
    "flying",
    "want",
    "query",
    "track",
    "something",
    "always",
    "airport",
    "data",
    "use",
    "graph",
    "type",
    "system",
    "rather",
    "sql",
    "work",
    "slower",
    "data",
    "really",
    "high",
    "even",
    "spark",
    "sql",
    "slower",
    "airport",
    "management",
    "right",
    "many",
    "flights",
    "flying",
    "real",
    "time",
    "forth",
    "use",
    "graph",
    "api",
    "lot",
    "certain",
    "use",
    "cases",
    "let",
    "say",
    "two",
    "edges",
    "lot",
    "relations",
    "social",
    "media",
    "another",
    "example",
    "right",
    "want",
    "find",
    "like",
    "facebook",
    "let",
    "say",
    "want",
    "lot",
    "comparison",
    "many",
    "friends",
    "friends",
    "actually",
    "like",
    "actually",
    "like",
    "traversing",
    "queries",
    "sql",
    "difficult",
    "graph",
    "easily",
    "traverse",
    "say",
    "notes",
    "find",
    "type",
    "relation",
    "easily",
    "tell",
    "social",
    "media",
    "companies",
    "use",
    "lot",
    "graphics",
    "actually",
    "even",
    "twitter",
    "use",
    "twitter",
    "many",
    "people",
    "following",
    "many",
    "followed",
    "relations",
    "right",
    "learned",
    "graph",
    "apis",
    "twitter",
    "go",
    "graph",
    "sql",
    "different",
    "okay",
    "nosql",
    "database",
    "called",
    "neo4j",
    "neo4j",
    "nosql",
    "database",
    "store",
    "data",
    "graph",
    "format",
    "mean",
    "different",
    "use",
    "case",
    "real",
    "time",
    "queries",
    "company",
    "actually",
    "uses",
    "graph",
    "store",
    "data",
    "okay",
    "graphics",
    "different",
    "comes",
    "part",
    "spark",
    "ne",
    "4j",
    "nosql",
    "database",
    "worked",
    "small",
    "project",
    "using",
    "neo4j",
    "learn",
    "actually",
    "seen",
    "people",
    "actually",
    "using",
    "okay",
    "might",
    "understand",
    "may",
    "understand",
    "went",
    "one",
    "uh",
    "circle",
    "right",
    "one",
    "orbit",
    "okay",
    "let",
    "ask",
    "make",
    "anything",
    "special",
    "picture",
    "fine",
    "mean",
    "uh",
    "uh",
    "flamingo",
    "okay",
    "flamingo",
    "huh",
    "waiting",
    "creative",
    "right",
    "flamingo",
    "standing",
    "one",
    "huh",
    "yeah",
    "creative",
    "exactly",
    "called",
    "standalone",
    "mode",
    "spark",
    "create",
    "slide",
    "data",
    "slide",
    "blame",
    "blame",
    "exactly",
    "flamingo",
    "standing",
    "one",
    "leg",
    "right",
    "creative",
    "okay",
    "learning",
    "spot",
    "different",
    "modes",
    "run",
    "spark",
    "basically",
    "okay",
    "get",
    "confused",
    "name",
    "top",
    "one",
    "single",
    "pc",
    "called",
    "local",
    "local",
    "mode",
    "means",
    "running",
    "locally",
    "one",
    "machine",
    "okay",
    "expand",
    "one",
    "machine",
    "use",
    "development",
    "testing",
    "purpose",
    "understood",
    "mod",
    "mesos",
    "icon",
    "mesos",
    "originally",
    "spark",
    "created",
    "top",
    "mesos",
    "told",
    "right",
    "still",
    "want",
    "install",
    "mesos",
    "cluster",
    "right",
    "install",
    "spark",
    "top",
    "run",
    "problem",
    "right",
    "stand",
    "standalone",
    "mode",
    "standalone",
    "mode",
    "want",
    "install",
    "spark",
    "cluster",
    "yarn",
    "mesos",
    "anything",
    "spark",
    "give",
    "cluster",
    "manager",
    "called",
    "standalone",
    "rare",
    "ideally",
    "yarn",
    "yarn",
    "spark",
    "yarn",
    "yarn",
    "ideally",
    "cases",
    "see",
    "yarn",
    "blue",
    "ball",
    "mean",
    "yarn",
    "uh",
    "organizations",
    "already",
    "hadoop",
    "cluster",
    "yarn",
    "makes",
    "sense",
    "integrate",
    "spark",
    "top",
    "another",
    "important",
    "point",
    "lot",
    "people",
    "get",
    "confused",
    "writing",
    "spark",
    "storage",
    "nothing",
    "called",
    "storage",
    "spark",
    "execution",
    "engine",
    "like",
    "map",
    "reduce",
    "map",
    "reduce",
    "storage",
    "reads",
    "data",
    "hado",
    "process",
    "probably",
    "store",
    "back",
    "hado",
    "right",
    "similar",
    "spark",
    "also",
    "storage",
    "storage",
    "component",
    "spark",
    "execution",
    "engine",
    "means",
    "provide",
    "data",
    "somewhere",
    "running",
    "spark",
    "hadoop",
    "data",
    "hdfs",
    "running",
    "spark",
    "let",
    "say",
    "pc",
    "pc",
    "file",
    "system",
    "data",
    "standalone",
    "mesos",
    "whichever",
    "decide",
    "okay",
    "interested",
    "extra",
    "knowledge",
    "spark",
    "also",
    "run",
    "kubernetes",
    "kubernetes",
    "spark",
    "kubernetes",
    "think",
    "production",
    "ready",
    "know",
    "kubernetes",
    "okay",
    "know",
    "probably",
    "things",
    "might",
    "useful",
    "future",
    "kubernetes",
    "actually",
    "running",
    "somewhere",
    "something",
    "called",
    "docker",
    "right",
    "something",
    "called",
    "docker",
    "old",
    "tool",
    "old",
    "like",
    "four",
    "five",
    "years",
    "think",
    "know",
    "docker",
    "allows",
    "know",
    "virtual",
    "machine",
    "right",
    "know",
    "vm",
    "laptop",
    "create",
    "vm",
    "happen",
    "let",
    "say",
    "install",
    "windows",
    "vm",
    "problem",
    "vm",
    "use",
    "lot",
    "resources",
    "right",
    "operating",
    "system",
    "install",
    "everything",
    "laptop",
    "8",
    "gb",
    "ram",
    "need",
    "give",
    "4gb",
    "ram",
    "vm",
    "docker",
    "allows",
    "create",
    "something",
    "called",
    "container",
    "okay",
    "meaning",
    "one",
    "laptop",
    "run",
    "like",
    "10",
    "docker",
    "containers",
    "one",
    "linux",
    "one",
    "windows",
    "use",
    "libraries",
    "already",
    "available",
    "base",
    "operating",
    "system",
    "install",
    "full",
    "operating",
    "system",
    "saying",
    "instead",
    "giving",
    "4",
    "gb",
    "ram",
    "vm",
    "give",
    "1",
    "gb",
    "docker",
    "512",
    "mb",
    "docker",
    "linux",
    "much",
    "much",
    "good",
    "performance",
    "docker",
    "became",
    "big",
    "hit",
    "programmer",
    "wrote",
    "java",
    "program",
    "want",
    "test",
    "want",
    "test",
    "apple",
    "want",
    "test",
    "linux",
    "windows",
    "right",
    "launch",
    "docker",
    "containers",
    "pc",
    "like",
    "10",
    "run",
    "code",
    "see",
    "whether",
    "working",
    "easy",
    "right",
    "kubernetes",
    "next",
    "level",
    "docker",
    "something",
    "called",
    "container",
    "orchestration",
    "data",
    "center",
    "able",
    "run",
    "multiple",
    "containers",
    "like",
    "docker",
    "containers",
    "manage",
    "using",
    "kuber",
    "correct",
    "google",
    "project",
    "okay",
    "data",
    "center",
    "orchestration",
    "docker",
    "like",
    "running",
    "one",
    "machine",
    "probably",
    "10",
    "machines",
    "match",
    "install",
    "ceretti",
    "become",
    "data",
    "center",
    "manager",
    "actually",
    "give",
    "like",
    "hundreds",
    "thousands",
    "servers",
    "kuber",
    "say",
    "want",
    "many",
    "containers",
    "docker",
    "like",
    "containers",
    "launch",
    "manage",
    "kuber",
    "become",
    "one",
    "instead",
    "mesos",
    "thing",
    "kuber",
    "become",
    "resource",
    "manager",
    "install",
    "spark",
    "say",
    "want",
    "run",
    "spark",
    "program",
    "say",
    "hit",
    "ender",
    "go",
    "kuber",
    "launch",
    "containers",
    "run",
    "spark",
    "code",
    "docker",
    "containers",
    "yeah",
    "yeah",
    "cloud",
    "cloud",
    "everything",
    "uh",
    "available",
    "cloud",
    "cloud",
    "everything",
    "supports",
    "cloud",
    "multiple",
    "options",
    "either",
    "directly",
    "run",
    "say",
    "go",
    "amazon",
    "create",
    "machines",
    "use",
    "services",
    "like",
    "emr",
    "emr",
    "elastic",
    "map",
    "reduce",
    "go",
    "amazon",
    "say",
    "hey",
    "amazon",
    "need",
    "create",
    "h",
    "spar",
    "cluster",
    "say",
    "10",
    "machines",
    "100",
    "machines",
    "5",
    "minutes",
    "create",
    "give",
    "run",
    "workload",
    "like",
    "disposable",
    "clusters",
    "complete",
    "job",
    "delete",
    "otherwise",
    "paying",
    "money",
    "continuously",
    "right",
    "uh",
    "u",
    "kubernetes",
    "cloud",
    "also",
    "mean",
    "local",
    "data",
    "center",
    "well",
    "cloud",
    "available",
    "everywhere",
    "know",
    "extensively",
    "gone",
    "kubernetes",
    "uh",
    "think",
    "search",
    "uh",
    "something",
    "coming",
    "big",
    "way",
    "think",
    "google",
    "ku",
    "kubernetes",
    "kubernetes",
    "whatever",
    "call",
    "production",
    "grade",
    "container",
    "orchestration",
    "problem",
    "docker",
    "difficult",
    "manage",
    "containers",
    "kuet",
    "manage",
    "like",
    "ah",
    "cloud",
    "know",
    "exact",
    "architecture",
    "kuber",
    "like",
    "whether",
    "launch",
    "container",
    "docker",
    "container",
    "basically",
    "coming",
    "abstraction",
    "previously",
    "everything",
    "clear",
    "example",
    "buy",
    "server",
    "see",
    "hard",
    "disk",
    "right",
    "install",
    "operating",
    "system",
    "kubernetes",
    "says",
    "give",
    "data",
    "center",
    "worry",
    "data",
    "center",
    "group",
    "servers",
    "give",
    "group",
    "servers",
    "like",
    "data",
    "senter",
    "launch",
    "many",
    "resources",
    "want",
    "tell",
    "many",
    "containers",
    "want",
    "ensure",
    "need",
    "launch",
    "need",
    "launch",
    "like",
    "ah",
    "like",
    "like",
    "abstraction",
    "getting",
    "right",
    "uh",
    "probably",
    "say",
    "ah",
    "kubernetes",
    "built",
    "15",
    "years",
    "exp",
    "experience",
    "running",
    "production",
    "workloads",
    "google",
    "yeah",
    "originally",
    "came",
    "google",
    "google",
    "already",
    "running",
    "name",
    "name",
    "even",
    "hadoop",
    "came",
    "google",
    "right",
    "like",
    "creating",
    "kuber",
    "netti",
    "think",
    "spark",
    "kuber",
    "available",
    "speaking",
    "spark",
    "kubernetes",
    "yeah",
    "much",
    "available",
    "spark",
    "must",
    "running",
    "kuber",
    "cluster",
    "blah",
    "blah",
    "blah",
    "ah",
    "ah",
    "docker",
    "okay",
    "docker",
    "images",
    "see",
    "internally",
    "using",
    "docker",
    "nothing",
    "else",
    "blah",
    "blah",
    "blah",
    "yeah",
    "supported",
    "slide",
    "saying",
    "slide",
    "see",
    "kuber",
    "slide",
    "bit",
    "older",
    "pay",
    "attention",
    "slide",
    "show",
    "one",
    "small",
    "change",
    "tell",
    "change",
    "okay",
    "pay",
    "attention",
    "see",
    "change",
    "slide",
    "change",
    "somebody",
    "top",
    "right",
    "zookeeper",
    "zookeeper",
    "oh",
    "say",
    "show",
    "see",
    "gone",
    "come",
    "saw",
    "technically",
    "means",
    "three",
    "modes",
    "whether",
    "running",
    "standalone",
    "mesos",
    "yarn",
    "highly",
    "available",
    "using",
    "zookeeper",
    "zookeeper",
    "supports",
    "high",
    "availability",
    "like",
    "spark",
    "running",
    "one",
    "machine",
    "crashes",
    "uh",
    "know",
    "states",
    "maintained",
    "zookeeper",
    "zookeeper",
    "integrated",
    "spark",
    "zookeeper",
    "another",
    "hadoop",
    "ecosystem",
    "tool",
    "used",
    "communication",
    "machines",
    "actually",
    "put",
    "simply",
    "like",
    "large",
    "cluster",
    "one",
    "machine",
    "goes",
    "know",
    "like",
    "lot",
    "balancer",
    "um",
    "explain",
    "give",
    "simple",
    "example",
    "otherwise",
    "zookeeper",
    "might",
    "become",
    "another",
    "big",
    "problem",
    "right",
    "zookeeper",
    "admin",
    "thing",
    "mostly",
    "worry",
    "much",
    "still",
    "remember",
    "told",
    "active",
    "name",
    "standby",
    "name",
    "node",
    "right",
    "hadoop",
    "cluster",
    "two",
    "name",
    "notes",
    "right",
    "active",
    "standby",
    "correct",
    "idea",
    "one",
    "working",
    "active",
    "active",
    "crashes",
    "question",
    "tell",
    "question",
    "let",
    "say",
    "connecting",
    "cluster",
    "know",
    "active",
    "ca",
    "connect",
    "two",
    "machine",
    "right",
    "guy",
    "ip",
    "address",
    "guy",
    "ip",
    "address",
    "one",
    "way",
    "keep",
    "pinging",
    "active",
    "currently",
    "know",
    "active",
    "tell",
    "right",
    "nobody",
    "tell",
    "one",
    "way",
    "keep",
    "pinging",
    "machines",
    "see",
    "alive",
    "alive",
    "like",
    "many",
    "situations",
    "understand",
    "machine",
    "alive",
    "alive",
    "zookeeper",
    "happens",
    "simple",
    "service",
    "active",
    "name",
    "node",
    "register",
    "zookeeper",
    "okay",
    "standby",
    "also",
    "register",
    "zookeeper",
    "ask",
    "zookeper",
    "active",
    "tell",
    "active",
    "guy",
    "crashes",
    "okay",
    "guy",
    "inform",
    "zer",
    "new",
    "active",
    "ask",
    "zookeeper",
    "zookeeper",
    "tell",
    "coordination",
    "mechanism",
    "zookeeper",
    "used",
    "coordination",
    "high",
    "availability",
    "slide",
    "means",
    "spark",
    "support",
    "zookeeper",
    "talking",
    "machines",
    "management",
    "use",
    "zookeeper",
    "possible",
    "available",
    "right",
    "icon",
    "zookeeper",
    "come",
    "actual",
    "icon",
    "zookeeper",
    "asking",
    "uh",
    "know",
    "normally",
    "people",
    "much",
    "aware",
    "zookeeper",
    "um",
    "services",
    "actually",
    "let",
    "show",
    "zookeeper",
    "service",
    "coordinate",
    "one",
    "example",
    "gave",
    "know",
    "name",
    "working",
    "ideally",
    "automatically",
    "come",
    "zookeeper",
    "inform",
    "guy",
    "come",
    "look",
    "architecture",
    "okay",
    "implement",
    "high",
    "availability",
    "hado",
    "normally",
    "happens",
    "zeper",
    "guy",
    "goes",
    "write",
    "command",
    "make",
    "actually",
    "wastage",
    "time",
    "right",
    "automatically",
    "come",
    "guys",
    "connected",
    "zookeeper",
    "keep",
    "alive",
    "message",
    "sent",
    "ah",
    "heartbeat",
    "guy",
    "goes",
    "zookeeper",
    "know",
    "guy",
    "gone",
    "let",
    "say",
    "second",
    "two",
    "timer",
    "configure",
    "ask",
    "guy",
    "come",
    "okay",
    "update",
    "metadata",
    "basically",
    "guy",
    "holds",
    "metadata",
    "cluster",
    "otherwise",
    "see",
    "lot",
    "services",
    "hard",
    "cluster",
    "active",
    "standby",
    "name",
    "node",
    "resource",
    "manager",
    "yarn",
    "resource",
    "manager",
    "active",
    "standby",
    "take",
    "hbase",
    "service",
    "called",
    "sql",
    "hbas",
    "master",
    "standby",
    "go",
    "hado",
    "cluster",
    "start",
    "asking",
    "everybody",
    "master",
    "slave",
    "difficult",
    "go",
    "zuker",
    "guy",
    "knowledge",
    "even",
    "spark",
    "active",
    "standby",
    "master",
    "okay",
    "installing",
    "spark",
    "independently",
    "coordinated",
    "zookeeper",
    "slide",
    "saying",
    "zookeeper",
    "default",
    "available",
    "clusters",
    "went",
    "back",
    "orbit",
    "spark",
    "read",
    "almost",
    "file",
    "system",
    "hdfs",
    "uh",
    "know",
    "cube",
    "way",
    "know",
    "s3",
    "amazon",
    "local",
    "file",
    "system",
    "file",
    "system",
    "maybe",
    "file",
    "system",
    "basically",
    "uh",
    "supports",
    "lot",
    "file",
    "systems",
    "even",
    "aware",
    "right",
    "local",
    "file",
    "systems",
    "read",
    "things",
    "nosql",
    "database",
    "rdbms",
    "think",
    "cool",
    "feature",
    "show",
    "mysql",
    "db",
    "spar",
    "directly",
    "read",
    "table",
    "mongod",
    "db",
    "nosql",
    "database",
    "read",
    "table",
    "process",
    "data",
    "output",
    "probably",
    "store",
    "back",
    "uh",
    "table",
    "right",
    "also",
    "supports",
    "hadoop",
    "input",
    "formats",
    "spark",
    "streaming",
    "work",
    "fluman",
    "kafka",
    "meaning",
    "realtime",
    "streaming",
    "right",
    "question",
    "get",
    "data",
    "always",
    "challenge",
    "happens",
    "normally",
    "spark",
    "cluster",
    "running",
    "let",
    "say",
    "spark",
    "cluster",
    "hado",
    "cluster",
    "spark",
    "installed",
    "let",
    "say",
    "spark",
    "cluster",
    "running",
    "want",
    "get",
    "twitter",
    "data",
    "actually",
    "spark",
    "directly",
    "get",
    "twitter",
    "data",
    "problem",
    "directly",
    "come",
    "cluster",
    "process",
    "problem",
    "one",
    "machine",
    "receiving",
    "data",
    "crashes",
    "lose",
    "data",
    "time",
    "right",
    "spark",
    "architecture",
    "high",
    "availability",
    "either",
    "use",
    "flume",
    "kafka",
    "ask",
    "guys",
    "get",
    "data",
    "flume",
    "flume",
    "pointto",
    "point",
    "delivery",
    "data",
    "like",
    "configure",
    "flume",
    "agent",
    "see",
    "webinar",
    "get",
    "data",
    "twitter",
    "kafka",
    "message",
    "cu",
    "get",
    "data",
    "even",
    "machines",
    "working",
    "data",
    "come",
    "right",
    "losing",
    "data",
    "reliability",
    "using",
    "flume",
    "kafka",
    "spark",
    "cluster",
    "right",
    "spark",
    "cluster",
    "talking",
    "spark",
    "streaming",
    "okay",
    "like",
    "normal",
    "processing",
    "configure",
    "spark",
    "streaming",
    "one",
    "machine",
    "one",
    "machine",
    "start",
    "working",
    "something",
    "called",
    "receiver",
    "okay",
    "machine",
    "job",
    "get",
    "data",
    "cares",
    "normal",
    "data",
    "node",
    "drawback",
    "spark",
    "streaming",
    "drawback",
    "say",
    "machine",
    "crashes",
    "machine",
    "crashes",
    "okay",
    "stream",
    "lost",
    "course",
    "switch",
    "another",
    "machine",
    "may",
    "take",
    "time",
    "let",
    "say",
    "5",
    "seconds",
    "10",
    "seconds",
    "losing",
    "data",
    "right",
    "storing",
    "anywhere",
    "correct",
    "want",
    "avoid",
    "machine",
    "goes",
    "stream",
    "gone",
    "want",
    "avoid",
    "say",
    "kafka",
    "hey",
    "get",
    "data",
    "kafka",
    "okay",
    "get",
    "spark",
    "even",
    "machine",
    "crashes",
    "another",
    "machine",
    "come",
    "data",
    "available",
    "spark",
    "streaming",
    "want",
    "reliability",
    "use",
    "kafka",
    "flume",
    "direct",
    "stream",
    "may",
    "reliability",
    "okay",
    "covering",
    "spark",
    "streaming",
    "probably",
    "architecture",
    "side",
    "speak",
    "distributions",
    "already",
    "know",
    "statistics",
    "uh",
    "think",
    "aware",
    "developers",
    "500",
    "plus",
    "companies",
    "developers",
    "etc",
    "etc",
    "right",
    "uh",
    "slide",
    "asking",
    "tools",
    "go",
    "extinct",
    "yes",
    "hive",
    "nobody",
    "running",
    "lot",
    "hive",
    "queries",
    "everything",
    "spark",
    "sql",
    "queries",
    "days",
    "maho",
    "machine",
    "learning",
    "everybody",
    "migrated",
    "spark",
    "storm",
    "spark",
    "streaming",
    "uh",
    "slide",
    "compares",
    "difference",
    "ways",
    "run",
    "spark",
    "example",
    "instead",
    "map",
    "reduce",
    "us",
    "using",
    "spark",
    "uh",
    "resource",
    "manager",
    "yan",
    "mesos",
    "matter",
    "storage",
    "level",
    "hadoop",
    "hdfs",
    "tachon",
    "tachon",
    "storage",
    "manager",
    "like",
    "200",
    "n",
    "long",
    "back",
    "originally",
    "mesos",
    "came",
    "storage",
    "handled",
    "system",
    "called",
    "ton",
    "renamed",
    "alusio",
    "aluk",
    "alusio",
    "project",
    "still",
    "storage",
    "layer",
    "like",
    "hdfs",
    "distributed",
    "storage",
    "get",
    "one",
    "common",
    "problem",
    "spark",
    "try",
    "bring",
    "data",
    "different",
    "places",
    "like",
    "spark",
    "running",
    "hadoop",
    "maybe",
    "data",
    "hadoop",
    "rdbms",
    "cassandra",
    "use",
    "normal",
    "hdfs",
    "fine",
    "using",
    "tachon",
    "known",
    "alusio",
    "caching",
    "layer",
    "speed",
    "processing",
    "caching",
    "data",
    "advantage",
    "extensively",
    "used",
    "setting",
    "mess",
    "actually",
    "okay",
    "seen",
    "time",
    "called",
    "ton",
    "alusio",
    "think",
    "renamed",
    "memory",
    "serves",
    "well",
    "ton",
    "alusio",
    "spelling",
    "look",
    "alusio",
    "alio",
    "alio",
    "whatever",
    "call",
    "know",
    "know",
    "uh",
    "alio",
    "formerly",
    "open",
    "source",
    "memory",
    "speed",
    "virtual",
    "distributed",
    "storage",
    "know",
    "many",
    "technologies",
    "many",
    "actually",
    "like",
    "let",
    "say",
    "tons",
    "data",
    "need",
    "storage",
    "layer",
    "caching",
    "layer",
    "make",
    "faster",
    "use",
    "alio",
    "use",
    "case",
    "otherwise",
    "everybody",
    "using",
    "hdfs",
    "normal",
    "hdfs",
    "ssd",
    "ssd",
    "caching",
    "costly",
    "costly",
    "faster",
    "use",
    "case",
    "processing",
    "huge",
    "amount",
    "data",
    "like",
    "uh",
    "like",
    "bank",
    "america",
    "bank",
    "america",
    "like",
    "tons",
    "tons",
    "data",
    "process",
    "like",
    "terabytes",
    "data",
    "keeps",
    "coming",
    "storing",
    "hdfs",
    "first",
    "time",
    "read",
    "slow",
    "anyway",
    "slow",
    "push",
    "directly",
    "thing",
    "alusio",
    "ssd",
    "caching",
    "layer",
    "read",
    "read",
    "faster",
    "actually",
    "use",
    "case",
    "seeing",
    "aluo",
    "another",
    "use",
    "case",
    "right",
    "premise",
    "storage",
    "cloud",
    "storage",
    "getting",
    "data",
    "comes",
    "alosio",
    "layer",
    "start",
    "computing",
    "storage",
    "layer",
    "abstraction",
    "store",
    "data",
    "multiple",
    "places",
    "okay",
    "alio",
    "get",
    "one",
    "place",
    "start",
    "processing",
    "like",
    "bringing",
    "data",
    "caching",
    "layer",
    "processing",
    "okay",
    "worry",
    "know",
    "alio",
    "also",
    "perfectly",
    "fine",
    "like",
    "mandatory",
    "component",
    "something",
    "okay",
    "tell",
    "drawback",
    "map",
    "reduce",
    "drawback",
    "think",
    "spark",
    "replacing",
    "map",
    "reduce",
    "right",
    "drawback",
    "map",
    "ruce",
    "yeah",
    "mapper",
    "output",
    "purchase",
    "output",
    "persist",
    "course",
    "reducer",
    "output",
    "finally",
    "persist",
    "right",
    "map",
    "redu",
    "slow",
    "slide",
    "actually",
    "written",
    "iterative",
    "processing",
    "right",
    "read",
    "data",
    "10",
    "times",
    "process",
    "schedule",
    "jobs",
    "10",
    "map",
    "ruce",
    "jobs",
    "run",
    "difficult",
    "change",
    "together",
    "intermediate",
    "read",
    "write",
    "always",
    "right",
    "uh",
    "spark",
    "becomes",
    "different",
    "slide",
    "says",
    "use",
    "uzi",
    "schedule",
    "jobs",
    "nothing",
    "else",
    "spark",
    "something",
    "called",
    "inmemory",
    "processing",
    "confusing",
    "many",
    "people",
    "first",
    "thing",
    "need",
    "understand",
    "inmemory",
    "processing",
    "means",
    "use",
    "ram",
    "available",
    "mean",
    "always",
    "need",
    "give",
    "ram",
    "ram",
    "provided",
    "let",
    "say",
    "want",
    "process",
    "10gb",
    "file",
    "cluster",
    "10gb",
    "free",
    "ram",
    "read",
    "ram",
    "calculation",
    "final",
    "output",
    "push",
    "hard",
    "disk",
    "intermediate",
    "results",
    "stored",
    "onto",
    "hard",
    "disk",
    "second",
    "point",
    "ram",
    "available",
    "start",
    "using",
    "hard",
    "disk",
    "also",
    "step",
    "step",
    "read",
    "read",
    "whatever",
    "data",
    "fits",
    "ram",
    "process",
    "read",
    "process",
    "like",
    "go",
    "way",
    "right",
    "ram",
    "still",
    "faster",
    "produce",
    "okay",
    "ground",
    "designed",
    "code",
    "uh",
    "spark",
    "modified",
    "map",
    "ruce",
    "code",
    "something",
    "create",
    "spark",
    "uh",
    "says",
    "10",
    "100",
    "times",
    "faster",
    "map",
    "reduce",
    "typical",
    "map",
    "reduce",
    "right",
    "memory",
    "processing",
    "see",
    "uh",
    "faster",
    "ah",
    "end",
    "cassandra",
    "cassandra",
    "demonstrate",
    "store",
    "result",
    "cassandra",
    "anywhere",
    "read",
    "store",
    "also",
    "uh",
    "distributors",
    "applications",
    "uses",
    "spark",
    "course",
    "data",
    "brick",
    "major",
    "distributor",
    "hoton",
    "works",
    "cloud",
    "era",
    "guys",
    "uh",
    "know",
    "spark",
    "applications",
    "use",
    "spark",
    "processing",
    "mostly",
    "bi",
    "tools",
    "right",
    "visualization",
    "tools",
    "previously",
    "using",
    "something",
    "like",
    "uh",
    "uh",
    "pentah",
    "okay",
    "visualize",
    "data",
    "fire",
    "query",
    "cluster",
    "hi",
    "run",
    "query",
    "visualize",
    "spark",
    "sql",
    "run",
    "query",
    "much",
    "much",
    "faster",
    "bi",
    "tools",
    "etl",
    "tools",
    "use",
    "spark",
    "moving",
    "data",
    "processing",
    "data",
    "forth",
    "uh",
    "slide",
    "bit",
    "old",
    "100",
    "tbte",
    "uh",
    "sort",
    "competition",
    "2014",
    "every",
    "year",
    "sorting",
    "competition",
    "happen",
    "even",
    "participate",
    "want",
    "uh",
    "thing",
    "give",
    "one",
    "terabyte",
    "data",
    "okay",
    "uh",
    "sort",
    "okay",
    "data",
    "already",
    "give",
    "format",
    "based",
    "sort",
    "integer",
    "sorting",
    "quick",
    "sort",
    "sort",
    "tell",
    "already",
    "give",
    "data",
    "also",
    "sample",
    "data",
    "write",
    "algorithm",
    "sort",
    "java",
    "program",
    "program",
    "write",
    "whoever",
    "sorts",
    "data",
    "fastest",
    "win",
    "uh",
    "conclusion",
    "2014",
    "ran",
    "uh",
    "spark",
    "uh",
    "23",
    "minutes",
    "hadoop",
    "map",
    "reduce",
    "72",
    "minutes",
    "look",
    "cluster",
    "size",
    "spark",
    "running",
    "206",
    "machines",
    "map",
    "rce",
    "machines",
    "difference",
    "1",
    "10th",
    "machine",
    "still",
    "faster",
    "lot",
    "ram",
    "available",
    "everything",
    "memory",
    "fasting",
    "faster",
    "sorting",
    "happen",
    "even",
    "one",
    "petabyte",
    "sorting",
    "spark",
    "became",
    "winner",
    "okay",
    "notebooks",
    "know",
    "notebook",
    "right",
    "ah",
    "python",
    "use",
    "right",
    "know",
    "luckily",
    "teach",
    "mean",
    "code",
    "actually",
    "written",
    "notebook",
    "okay",
    "uh",
    "use",
    "shell",
    "also",
    "bit",
    "like",
    "notebooks",
    "personally",
    "okay",
    "know",
    "driver",
    "things",
    "require",
    "otherwise",
    "understand",
    "spark",
    "driver",
    "know",
    "device",
    "driver",
    "okay",
    "drives",
    "program",
    "actually",
    "correct",
    "right",
    "driver",
    "guy",
    "writing",
    "spark",
    "program",
    "program",
    "something",
    "called",
    "driver",
    "okay",
    "master",
    "program",
    "driver",
    "master",
    "program",
    "something",
    "called",
    "executor",
    "slave",
    "let",
    "say",
    "wrote",
    "spark",
    "program",
    "spark",
    "program",
    "definitely",
    "driver",
    "executor",
    "without",
    "spark",
    "program",
    "run",
    "mean",
    "logical",
    "concepts",
    "saying",
    "okay",
    "want",
    "run",
    "program",
    "okay",
    "one",
    "way",
    "run",
    "say",
    "run",
    "locally",
    "say",
    "hey",
    "wrote",
    "spark",
    "program",
    "okay",
    "run",
    "program",
    "local",
    "say",
    "local",
    "happen",
    "jvm",
    "created",
    "like",
    "jvm",
    "okay",
    "driver",
    "executor",
    "everything",
    "inside",
    "local",
    "mode",
    "spark",
    "java",
    "virtual",
    "machine",
    "h",
    "still",
    "container",
    "created",
    "similar",
    "jvm",
    "general",
    "saying",
    "okay",
    "container",
    "allocated",
    "like",
    "yarn",
    "container",
    "h",
    "container",
    "allocated",
    "driver",
    "executor",
    "run",
    "local",
    "mode",
    "efficient",
    "get",
    "one",
    "container",
    "everything",
    "running",
    "inside",
    "right",
    "submitting",
    "spar",
    "code",
    "actually",
    "say",
    "python",
    "code",
    "right",
    "read",
    "logic",
    "convert",
    "run",
    "inside",
    "j",
    "end",
    "day",
    "everything",
    "inside",
    "jvm",
    "without",
    "run",
    "even",
    "map",
    "reduce",
    "programs",
    "wrote",
    "java",
    "right",
    "write",
    "python",
    "code",
    "map",
    "ruce",
    "writing",
    "python",
    "map",
    "ruce",
    "program",
    "run",
    "say",
    "uh",
    "submit",
    "program",
    "mention",
    "jar",
    "files",
    "jar",
    "files",
    "hadoop",
    "read",
    "python",
    "code",
    "convert",
    "format",
    "run",
    "inside",
    "jvm",
    "execute",
    "inside",
    "jvm",
    "end",
    "day",
    "written",
    "java",
    "spark",
    "wr",
    "scala",
    "right",
    "everything",
    "inside",
    "jvu",
    "local",
    "mode",
    "uh",
    "problem",
    "get",
    "one",
    "container",
    "inside",
    "driver",
    "executor",
    "everything",
    "run",
    "okay",
    "good",
    "testing",
    "purpose",
    "right",
    "right",
    "want",
    "test",
    "spark",
    "program",
    "want",
    "learn",
    "spark",
    "normally",
    "say",
    "hey",
    "spark",
    "run",
    "local",
    "mode",
    "write",
    "code",
    "runs",
    "cluster",
    "things",
    "becomes",
    "interesting",
    "bit",
    "confusing",
    "also",
    "okay",
    "hadoop",
    "cluster",
    "right",
    "four",
    "data",
    "nodes",
    "imagine",
    "four",
    "data",
    "nodes",
    "imagine",
    "created",
    "spark",
    "program",
    "first",
    "question",
    "running",
    "spark",
    "program",
    "analyzing",
    "okay",
    "analyzing",
    "file",
    "size",
    "file",
    "right",
    "let",
    "imagine",
    "file",
    "hadoop",
    "okay",
    "simple",
    "use",
    "case",
    "uh",
    "file",
    "example",
    "somewhere",
    "total",
    "size",
    "file",
    "let",
    "say",
    "10gb",
    "10gb",
    "file",
    "hado",
    "may",
    "blocks",
    "understood",
    "want",
    "process",
    "using",
    "spark",
    "cluster",
    "right",
    "submit",
    "program",
    "cluster",
    "ask",
    "yann",
    "executors",
    "many",
    "executors",
    "want",
    "capacity",
    "meaning",
    "either",
    "say",
    "hey",
    "give",
    "one",
    "executor",
    "huh",
    "executor",
    "20",
    "gb",
    "ram",
    "want",
    "problem",
    "happen",
    "executor",
    "come",
    "executor",
    "container",
    "nothing",
    "container",
    "let",
    "say",
    "20",
    "gb",
    "ram",
    "okay",
    "four",
    "processor",
    "core",
    "something",
    "entire",
    "spar",
    "code",
    "get",
    "executed",
    "inside",
    "normally",
    "people",
    "like",
    "one",
    "machine",
    "executing",
    "everything",
    "possibly",
    "ask",
    "yan",
    "hey",
    "yan",
    "give",
    "let",
    "say",
    "four",
    "executors",
    "example",
    "saying",
    "give",
    "four",
    "executors",
    "yan",
    "give",
    "four",
    "executors",
    "right",
    "executor",
    "tell",
    "give",
    "5gb",
    "ex",
    "5gb",
    "5gb",
    "5gb",
    "5gb",
    "ram",
    "ram",
    "memory",
    "total",
    "20",
    "5gb",
    "right",
    "imagine",
    "driver",
    "right",
    "executor",
    "imagine",
    "five",
    "nodes",
    "hado",
    "cluster",
    "driver",
    "driver",
    "running",
    "okay",
    "happen",
    "submit",
    "program",
    "one",
    "machine",
    "driver",
    "start",
    "running",
    "program",
    "asked",
    "yan",
    "give",
    "four",
    "executors",
    "executor",
    "want",
    "5gb",
    "ram",
    "two",
    "processor",
    "core",
    "blah",
    "blah",
    "blah",
    "yan",
    "launch",
    "1",
    "2",
    "3",
    "4",
    "whatever",
    "code",
    "written",
    "driver",
    "push",
    "four",
    "machines",
    "time",
    "machine",
    "process",
    "code",
    "cluster",
    "spark",
    "processing",
    "easy",
    "looks",
    "like",
    "know",
    "much",
    "memory",
    "need",
    "processing",
    "right",
    "many",
    "executors",
    "want",
    "ra",
    "yan",
    "actually",
    "give",
    "say",
    "give",
    "100",
    "executors",
    "100",
    "gb",
    "ram",
    "yan",
    "say",
    "capacity",
    "ca",
    "give",
    "normally",
    "wherever",
    "went",
    "consulting",
    "uh",
    "write",
    "spark",
    "program",
    "discuss",
    "admin",
    "okay",
    "want",
    "write",
    "spark",
    "program",
    "spar",
    "mark",
    "want",
    "get",
    "full",
    "performance",
    "memory",
    "need",
    "maximum",
    "ram",
    "challenge",
    "every",
    "hado",
    "cluster",
    "go",
    "hadoop",
    "admin",
    "spark",
    "admin",
    "say",
    "written",
    "program",
    "need",
    "100",
    "gb",
    "ram",
    "cluster",
    "tell",
    "program",
    "ask",
    "let",
    "say",
    "10",
    "executor",
    "10",
    "gb",
    "something",
    "like",
    "right",
    "configure",
    "number",
    "executor",
    "much",
    "mam",
    "want",
    "submit",
    "program",
    "yann",
    "launch",
    "things",
    "driver",
    "running",
    "separate",
    "machine",
    "right",
    "whatever",
    "logic",
    "written",
    "driver",
    "read",
    "start",
    "pushing",
    "executors",
    "output",
    "normally",
    "comes",
    "back",
    "driver",
    "whatever",
    "output",
    "driver",
    "write",
    "logic",
    "either",
    "store",
    "output",
    "hadoop",
    "store",
    "cassandra",
    "wherever",
    "want",
    "possible",
    "driver",
    "normally",
    "appm",
    "app",
    "master",
    "master",
    "remember",
    "app",
    "master",
    "yan",
    "driver",
    "lot",
    "people",
    "ask",
    "machine",
    "goes",
    "driver",
    "machine",
    "goes",
    "obviously",
    "driver",
    "machine",
    "goes",
    "processing",
    "disturbed",
    "spark",
    "program",
    "crash",
    "okay",
    "uh",
    "yan",
    "configure",
    "application",
    "master",
    "restart",
    "timer",
    "restart",
    "machine",
    "goes",
    "restart",
    "another",
    "machine",
    "application",
    "master",
    "another",
    "machine",
    "continue",
    "processing",
    "application",
    "master",
    "constant",
    "touch",
    "resource",
    "manager",
    "ah",
    "id",
    "application",
    "id",
    "tell",
    "processed",
    "uh",
    "execution",
    "going",
    "currently",
    "knows",
    "things",
    "resource",
    "manager",
    "launch",
    "one",
    "application",
    "master",
    "become",
    "spark",
    "driver",
    "get",
    "code",
    "starts",
    "running",
    "driver",
    "master",
    "part",
    "program",
    "execute",
    "uors",
    "push",
    "code",
    "depending",
    "cluster",
    "ask",
    "number",
    "executors",
    "much",
    "memory",
    "want",
    "executor",
    "actually",
    "runs",
    "something",
    "called",
    "application",
    "master",
    "restart",
    "timer",
    "configure",
    "yann",
    "say",
    "three",
    "five",
    "try",
    "restart",
    "application",
    "master",
    "machine",
    "sometimes",
    "happens",
    "machine",
    "crash",
    "application",
    "master",
    "crash",
    "driver",
    "process",
    "crash",
    "restart",
    "machine",
    "due",
    "many",
    "reason",
    "maybe",
    "resource",
    "available",
    "configure",
    "timer",
    "work",
    "go",
    "another",
    "machine",
    "say",
    "start",
    "uh",
    "need",
    "understand",
    "something",
    "called",
    "r",
    "dd",
    "okay",
    "rdd",
    "fundamentals",
    "written",
    "rdds",
    "basic",
    "building",
    "blocks",
    "spark",
    "first",
    "thing",
    "need",
    "understand",
    "spark",
    "data",
    "represented",
    "something",
    "called",
    "rdd",
    "right",
    "data",
    "want",
    "process",
    "spark",
    "first",
    "step",
    "create",
    "something",
    "called",
    "rdd",
    "rdd",
    "stands",
    "resent",
    "distributed",
    "data",
    "set",
    "like",
    "variable",
    "pointer",
    "say",
    "right",
    "slides",
    "give",
    "idea",
    "rdd",
    "also",
    "practically",
    "show",
    "rdd",
    "uh",
    "look",
    "picture",
    "picture",
    "uh",
    "good",
    "understanding",
    "uh",
    "spark",
    "working",
    "right",
    "four",
    "blocks",
    "data",
    "hadoop",
    "represented",
    "hdfs",
    "right",
    "know",
    "four",
    "blocks",
    "data",
    "right",
    "want",
    "process",
    "one",
    "file",
    "even",
    "though",
    "divided",
    "four",
    "blocks",
    "one",
    "single",
    "file",
    "want",
    "process",
    "data",
    "text",
    "data",
    "imagine",
    "block",
    "like",
    "error",
    "tim",
    "stamp",
    "message",
    "warning",
    "time",
    "stamp",
    "message",
    "log",
    "file",
    "imagine",
    "right",
    "four",
    "blocks",
    "data",
    "assuming",
    "four",
    "data",
    "nodes",
    "want",
    "process",
    "onwards",
    "going",
    "confuse",
    "okay",
    "saying",
    "advance",
    "get",
    "confused",
    "okay",
    "worry",
    "original",
    "data",
    "hard",
    "disk",
    "right",
    "blocks",
    "hdfs",
    "keep",
    "mind",
    "already",
    "written",
    "program",
    "imagine",
    "process",
    "data",
    "right",
    "program",
    "first",
    "step",
    "need",
    "create",
    "rdd",
    "okay",
    "say",
    "create",
    "rdd",
    "rdd",
    "like",
    "variable",
    "say",
    "pointer",
    "name",
    "rdd",
    "called",
    "log",
    "lines",
    "rdd",
    "call",
    "call",
    "ragu",
    "want",
    "say",
    "create",
    "spark",
    "create",
    "something",
    "called",
    "log",
    "lines",
    "rdd",
    "reading",
    "data",
    "file",
    "give",
    "location",
    "file",
    "run",
    "going",
    "happen",
    "imagine",
    "case",
    "asked",
    "four",
    "executors",
    "spark",
    "program",
    "said",
    "want",
    "four",
    "executors",
    "reason",
    "happen",
    "data",
    "node",
    "one",
    "one",
    "executor",
    "launched",
    "ideal",
    "situation",
    "right",
    "one",
    "one",
    "block",
    "data",
    "node",
    "say",
    "hey",
    "spark",
    "create",
    "rdd",
    "called",
    "log",
    "lines",
    "rdd",
    "hit",
    "enter",
    "data",
    "copied",
    "ram",
    "main",
    "memory",
    "rdds",
    "represent",
    "data",
    "memory",
    "assuming",
    "ram",
    "available",
    "ideal",
    "condition",
    "redraw",
    "picture",
    "actually",
    "uh",
    "data",
    "bricks",
    "picture",
    "right",
    "redraw",
    "picture",
    "think",
    "always",
    "file",
    "hadoop",
    "right",
    "typical",
    "example",
    "file",
    "hadoop",
    "one",
    "small",
    "thing",
    "let",
    "say",
    "uh",
    "six",
    "nots",
    "hado",
    "cluster",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "architecture",
    "block",
    "one",
    "2",
    "3",
    "4",
    "big",
    "question",
    "want",
    "four",
    "executors",
    "machines",
    "launch",
    "executor",
    "launching",
    "executor",
    "yan",
    "yan",
    "data",
    "locality",
    "awareness",
    "right",
    "yan",
    "know",
    "data",
    "right",
    "map",
    "reduce",
    "data",
    "locality",
    "blocks",
    "residing",
    "let",
    "say",
    "blocks",
    "right",
    "four",
    "blocks",
    "write",
    "map",
    "ruce",
    "program",
    "going",
    "happen",
    "map",
    "ruce",
    "framework",
    "written",
    "okay",
    "need",
    "four",
    "mappers",
    "one",
    "important",
    "point",
    "need",
    "remember",
    "map",
    "reduce",
    "processing",
    "data",
    "four",
    "jvms",
    "get",
    "launched",
    "correct",
    "one",
    "one",
    "one",
    "one",
    "copy",
    "copy",
    "copy",
    "copy",
    "mapper",
    "runs",
    "process",
    "four",
    "blocks",
    "using",
    "one",
    "mapper",
    "possible",
    "number",
    "mappers",
    "equal",
    "number",
    "blocks",
    "number",
    "input",
    "splits",
    "say",
    "right",
    "ideally",
    "say",
    "number",
    "mapers",
    "equal",
    "one",
    "going",
    "happen",
    "block",
    "process",
    "go",
    "next",
    "block",
    "come",
    "process",
    "possible",
    "size",
    "mapper",
    "container",
    "like",
    "1",
    "gb",
    "default",
    "yarn",
    "container",
    "size",
    "yarn",
    "container",
    "map",
    "reduce",
    "yarn",
    "launching",
    "containers",
    "maper",
    "ideally",
    "launches",
    "launch",
    "one",
    "one",
    "container",
    "block",
    "right",
    "block",
    "size",
    "fixed",
    "128",
    "mb",
    "right",
    "yarn",
    "settings",
    "say",
    "container",
    "size",
    "1",
    "gb",
    "want",
    "1",
    "gb",
    "container",
    "right",
    "happen",
    "1",
    "gb",
    "1gb",
    "1gb",
    "1gb",
    "four",
    "containers",
    "used",
    "rare",
    "manually",
    "mention",
    "number",
    "mappers",
    "ca",
    "ideally",
    "affect",
    "performance",
    "also",
    "launching",
    "one",
    "container",
    "ca",
    "resize",
    "like",
    "1gb",
    "container",
    "property",
    "called",
    "set",
    "spar",
    "dynamic",
    "execution",
    "enable",
    "disabled",
    "yan",
    "telling",
    "yan",
    "want",
    "eight",
    "executors",
    "kill",
    "everybody",
    "may",
    "use",
    "may",
    "use",
    "keep",
    "uh",
    "uh",
    "enable",
    "kill",
    "saying",
    "getting",
    "used",
    "kill",
    "want",
    "data",
    "size",
    "small",
    "kill",
    "coming",
    "back",
    "rdds",
    "discussion",
    "saying",
    "example",
    "four",
    "blocks",
    "data",
    "four",
    "blocks",
    "hardo",
    "cluster",
    "let",
    "say",
    "six",
    "kn",
    "want",
    "read",
    "data",
    "process",
    "say",
    "say",
    "want",
    "create",
    "rdd",
    "name",
    "rdd",
    "called",
    "log",
    "lines",
    "rdd",
    "whatever",
    "rdd",
    "like",
    "pointer",
    "okay",
    "code",
    "runs",
    "going",
    "happen",
    "ask",
    "four",
    "executors",
    "also",
    "let",
    "assume",
    "executors",
    "launched",
    "executor",
    "okay",
    "one",
    "guy",
    "one",
    "guy",
    "one",
    "guy",
    "blocks",
    "copied",
    "data",
    "data",
    "residing",
    "inside",
    "ram",
    "four",
    "machines",
    "four",
    "executors",
    "called",
    "rdd",
    "otherwise",
    "call",
    "data",
    "need",
    "way",
    "mention",
    "like",
    "like",
    "variable",
    "called",
    "rtd",
    "data",
    "available",
    "ram",
    "let",
    "say",
    "data",
    "representation",
    "data",
    "entire",
    "log",
    "file",
    "called",
    "log",
    "lines",
    "rdd",
    "also",
    "spark",
    "something",
    "called",
    "partitions",
    "meaning",
    "right",
    "data",
    "lying",
    "four",
    "blocks",
    "say",
    "four",
    "partition",
    "rdd",
    "normally",
    "reading",
    "hado",
    "blocks",
    "block",
    "become",
    "partition",
    "reading",
    "hado",
    "let",
    "say",
    "reading",
    "local",
    "pc",
    "mention",
    "many",
    "partitions",
    "need",
    "data",
    "say",
    "windows",
    "laptop",
    "okay",
    "uh",
    "1gb",
    "file",
    "simply",
    "read",
    "come",
    "single",
    "1gb",
    "partition",
    "okay",
    "want",
    "say",
    "hey",
    "windows",
    "laptop",
    "okay",
    "say",
    "hey",
    "spark",
    "read",
    "1",
    "gb",
    "create",
    "four",
    "partitions",
    "advantage",
    "partition",
    "go",
    "executor",
    "executor",
    "process",
    "partition",
    "file",
    "always",
    "data",
    "partitioned",
    "partitions",
    "parallelism",
    "get",
    "example",
    "anyway",
    "personal",
    "laptop",
    "launch",
    "multiple",
    "executors",
    "saying",
    "reading",
    "personal",
    "laptop",
    "dumping",
    "hadoop",
    "cluster",
    "imagine",
    "hadoop",
    "file",
    "system",
    "let",
    "say",
    "reading",
    "cassandra",
    "typical",
    "example",
    "cassandra",
    "sql",
    "database",
    "cassandra",
    "blocks",
    "anything",
    "blocks",
    "hadoop",
    "get",
    "data",
    "cassandra",
    "read",
    "table",
    "get",
    "1",
    "million",
    "rows",
    "right",
    "1",
    "million",
    "rows",
    "give",
    "executor",
    "right",
    "processing",
    "slow",
    "idea",
    "data",
    "let",
    "say",
    "1",
    "million",
    "data",
    "1",
    "gb",
    "something",
    "say",
    "creating",
    "rdd",
    "say",
    "take",
    "data",
    "cassandra",
    "divide",
    "four",
    "parties",
    "partition",
    "go",
    "executive",
    "ah",
    "executor",
    "manage",
    "one",
    "partition",
    "also",
    "minimum",
    "one",
    "partition",
    "get",
    "depending",
    "let",
    "say",
    "executor",
    "20",
    "gb",
    "memory",
    "h",
    "let",
    "say",
    "partition",
    "size",
    "say",
    "1",
    "gb",
    "manage",
    "20",
    "partitions",
    "data",
    "locality",
    "100",
    "guaranteed",
    "launch",
    "executor",
    "100",
    "guaranteed",
    "clear",
    "figure",
    "number",
    "executors",
    "okay",
    "first",
    "thing",
    "depends",
    "size",
    "data",
    "hadoop",
    "normally",
    "required",
    "hadoop",
    "happens",
    "default",
    "block",
    "partition",
    "right",
    "make",
    "sense",
    "block",
    "size",
    "hadoop",
    "128",
    "mb",
    "128",
    "mb",
    "block",
    "size",
    "right",
    "file",
    "uh",
    "divided",
    "10",
    "blocks",
    "total",
    "size",
    "file",
    "mb",
    "gb",
    "want",
    "process",
    "gb",
    "file",
    "question",
    "many",
    "executors",
    "need",
    "depends",
    "cluster",
    "configuration",
    "even",
    "process",
    "single",
    "executor",
    "ask",
    "hey",
    "yan",
    "give",
    "one",
    "executor",
    "2",
    "gb",
    "ram",
    "3",
    "gb",
    "ram",
    "safe",
    "side",
    "saying",
    "yan",
    "launch",
    "one",
    "executor",
    "let",
    "say",
    "3gb",
    "ram",
    "okay",
    "eight",
    "uh",
    "10",
    "partitions",
    "file",
    "10",
    "partitions",
    "10",
    "blogs",
    "10",
    "partitions",
    "come",
    "single",
    "say",
    "container",
    "get",
    "processed",
    "ah",
    "copy",
    "original",
    "data",
    "hdfs",
    "anyway",
    "happen",
    "ahuh",
    "case",
    "asking",
    "one",
    "container",
    "executor",
    "right",
    "executor",
    "launching",
    "okay",
    "ask",
    "many",
    "executors",
    "want",
    "understanding",
    "saying",
    "hard",
    "cluster",
    "effective",
    "saying",
    "effective",
    "partitions",
    "processing",
    "speed",
    "get",
    "example",
    "take",
    "four",
    "partitions",
    "right",
    "four",
    "executors",
    "launched",
    "getting",
    "loaded",
    "one",
    "partition",
    "faster",
    "also",
    "launch",
    "single",
    "executor",
    "copy",
    "four",
    "may",
    "fast",
    "another",
    "important",
    "point",
    "related",
    "resource",
    "management",
    "also",
    "understand",
    "stuff",
    "right",
    "executor",
    "container",
    "right",
    "jvm",
    "one",
    "processor",
    "core",
    "required",
    "manage",
    "one",
    "container",
    "meaning",
    "dual",
    "core",
    "machine",
    "many",
    "maximum",
    "executors",
    "launch",
    "two",
    "ask",
    "eight",
    "executors",
    "happen",
    "work",
    "executor",
    "jvm",
    "manage",
    "jvm",
    "jvm",
    "ram",
    "cpu",
    "ram",
    "say",
    "want",
    "8",
    "gb",
    "ram",
    "10",
    "gb",
    "ram",
    "fine",
    "within",
    "manage",
    "ideally",
    "single",
    "jvm",
    "one",
    "processor",
    "core",
    "allocated",
    "minimum",
    "one",
    "processor",
    "core",
    "right",
    "question",
    "16",
    "gb",
    "ram",
    "mean",
    "16",
    "gb",
    "ram",
    "container",
    "lot",
    "partitions",
    "inside",
    "maybe",
    "one",
    "cpu",
    "core",
    "enough",
    "run",
    "depends",
    "processing",
    "power",
    "uh",
    "machine",
    "keep",
    "things",
    "launching",
    "cluster",
    "performance",
    "side",
    "saying",
    "put",
    "short",
    "ideally",
    "many",
    "things",
    "launching",
    "spark",
    "program",
    "know",
    "size",
    "data",
    "chunked",
    "correct",
    "idea",
    "number",
    "partitions",
    "let",
    "say",
    "file",
    "whatever",
    "size",
    "may",
    "x",
    "size",
    "x",
    "right",
    "number",
    "partitions",
    "means",
    "number",
    "splits",
    "ideally",
    "saying",
    "want",
    "10",
    "partitions",
    "get",
    "10",
    "partitions",
    "partition",
    "inside",
    "executor",
    "10",
    "executors",
    "process",
    "data",
    "parallelism",
    "get",
    "less",
    "let",
    "say",
    "want",
    "two",
    "partitions",
    "two",
    "executors",
    "know",
    "two",
    "executors",
    "ideally",
    "less",
    "say",
    "processing",
    "partitions",
    "processing",
    "power",
    "get",
    "uh",
    "loading",
    "let",
    "say",
    "text",
    "file",
    "right",
    "loading",
    "text",
    "file",
    "say",
    "create",
    "rdd",
    "number",
    "partition",
    "five",
    "equally",
    "split",
    "text",
    "file",
    "five",
    "give",
    "one",
    "one",
    "executor",
    "point",
    "need",
    "five",
    "executors",
    "process",
    "go",
    "yan",
    "yan",
    "say",
    "enough",
    "res",
    "give",
    "two",
    "executor",
    "two",
    "executor",
    "get",
    "five",
    "partition",
    "one",
    "guy",
    "get",
    "two",
    "another",
    "three",
    "executor",
    "one",
    "cpu",
    "core",
    "right",
    "may",
    "bit",
    "slow",
    "enough",
    "resources",
    "parallelism",
    "works",
    "actually",
    "better",
    "usually",
    "theuh",
    "huh",
    "redraw",
    "picture",
    "things",
    "become",
    "clear",
    "think",
    "draw",
    "one",
    "manner",
    "point",
    "delete",
    "okay",
    "give",
    "one",
    "moment",
    "number",
    "partitions",
    "saying",
    "file",
    "file",
    "right",
    "file",
    "file",
    "divided",
    "10",
    "blocks",
    "hado",
    "okay",
    "wo",
    "exactly",
    "write",
    "let",
    "say",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "10",
    "blocks",
    "okay",
    "answer",
    "question",
    "right",
    "read",
    "data",
    "spark",
    "automatically",
    "something",
    "called",
    "partitioning",
    "block",
    "become",
    "one",
    "partition",
    "default",
    "change",
    "okay",
    "default",
    "one",
    "cases",
    "keep",
    "get",
    "one",
    "block",
    "one",
    "block",
    "red",
    "partition",
    "one",
    "partition",
    "two",
    "partition",
    "three",
    "right",
    "partition",
    "four",
    "partition",
    "10",
    "right",
    "huh",
    "similar",
    "input",
    "splits",
    "10",
    "partition",
    "want",
    "process",
    "speed",
    "matters",
    "right",
    "process",
    "need",
    "need",
    "ram",
    "need",
    "cpu",
    "power",
    "right",
    "partition",
    "go",
    "hard",
    "disk",
    "blocks",
    "hard",
    "disk",
    "blocks",
    "hard",
    "disk",
    "say",
    "create",
    "partition",
    "partition",
    "ram",
    "wo",
    "sit",
    "ram",
    "inside",
    "executor",
    "jvm",
    "right",
    "data",
    "come",
    "processing",
    "start",
    "right",
    "decide",
    "10",
    "partitions",
    "okay",
    "many",
    "executors",
    "need",
    "ask",
    "10",
    "executors",
    "yes",
    "say",
    "want",
    "10",
    "executors",
    "okay",
    "get",
    "executor",
    "1",
    "get",
    "executor",
    "10",
    "uh",
    "partition",
    "go",
    "executor",
    "partitions",
    "always",
    "go",
    "execut",
    "p1",
    "go",
    "executor",
    "executor",
    "asked",
    "1",
    "gb",
    "ram",
    "executor",
    "also",
    "one",
    "processor",
    "core",
    "means",
    "partition",
    "uh",
    "managed",
    "executor",
    "p10",
    "go",
    "okay",
    "executor",
    "also",
    "one",
    "processor",
    "1",
    "gb",
    "ram",
    "point",
    "yarn",
    "allow",
    "give",
    "10",
    "uh",
    "things",
    "thing",
    "another",
    "way",
    "ask",
    "two",
    "executors",
    "example",
    "okay",
    "ask",
    "two",
    "executors",
    "executor",
    "5gb",
    "executor",
    "also",
    "5gb",
    "one",
    "processor",
    "core",
    "one",
    "processor",
    "core",
    "p1",
    "p2",
    "p3",
    "p5",
    "come",
    "p6",
    "p7",
    "p10",
    "come",
    "write",
    "program",
    "let",
    "say",
    "wrote",
    "program",
    "say",
    "filter",
    "data",
    "h",
    "driver",
    "right",
    "driver",
    "driver",
    "said",
    "filter",
    "data",
    "h",
    "data",
    "partition",
    "filter",
    "logic",
    "logic",
    "pushed",
    "executor",
    "filter",
    "apply",
    "apply",
    "processor",
    "ram",
    "together",
    "right",
    "one",
    "processor",
    "core",
    "using",
    "one",
    "processor",
    "core",
    "uh",
    "uh",
    "parallely",
    "run",
    "probably",
    "parall",
    "process",
    "much",
    "amount",
    "data",
    "good",
    "otherwise",
    "one",
    "one",
    "process",
    "take",
    "take",
    "time",
    "depends",
    "yes",
    "processor",
    "core",
    "increase",
    "even",
    "say",
    "want",
    "four",
    "cores",
    "container",
    "launching",
    "uh",
    "executor",
    "said",
    "one",
    "core",
    "default",
    "right",
    "also",
    "say",
    "hey",
    "yan",
    "give",
    "executor",
    "executor",
    "requires",
    "four",
    "core",
    "problem",
    "quad",
    "core",
    "data",
    "node",
    "one",
    "executor",
    "launched",
    "already",
    "took",
    "four",
    "course",
    "ask",
    "one",
    "get",
    "right",
    "quad",
    "core",
    "data",
    "node",
    "launch",
    "one",
    "executor",
    "launch",
    "another",
    "executor",
    "already",
    "taken",
    "four",
    "cores",
    "system",
    "system",
    "think",
    "uh",
    "default",
    "handle",
    "two",
    "partitions",
    "max",
    "threading",
    "means",
    "two",
    "threads",
    "right",
    "production",
    "design",
    "application",
    "look",
    "data",
    "data",
    "coming",
    "already",
    "come",
    "partition",
    "partition",
    "reading",
    "hado",
    "already",
    "partition",
    "like",
    "blocks",
    "reading",
    "cassandra",
    "cassandra",
    "default",
    "partition",
    "data",
    "get",
    "like",
    "one",
    "big",
    "chunk",
    "reading",
    "decide",
    "partition",
    "yes",
    "partition",
    "many",
    "10",
    "10",
    "means",
    "size",
    "much",
    "many",
    "executors",
    "need",
    "go",
    "admin",
    "say",
    "launch",
    "spark",
    "job",
    "many",
    "getting",
    "say",
    "yes",
    "getting",
    "launch",
    "nosql",
    "database",
    "read",
    "connectors",
    "spark",
    "read",
    "cassandra",
    "able",
    "follow",
    "saying",
    "mean",
    "like",
    "talking",
    "greek",
    "latin",
    "right",
    "also",
    "way",
    "launch",
    "job",
    "default",
    "want",
    "mention",
    "stuffff",
    "say",
    "run",
    "job",
    "yan",
    "figure",
    "best",
    "way",
    "launch",
    "probably",
    "may",
    "fastest",
    "way",
    "execute",
    "program",
    "also",
    "possible",
    "thinking",
    "guy",
    "trying",
    "fool",
    "us",
    "saying",
    "ask",
    "three",
    "practically",
    "show",
    "three",
    "executors",
    "running",
    "launching",
    "actually",
    "like",
    "theory",
    "stuff",
    "launching",
    "spark",
    "shell",
    "go",
    "say",
    "num",
    "executor",
    "executor",
    "memory",
    "driver",
    "memory",
    "ask",
    "give",
    "see",
    "okay",
    "got",
    "three",
    "executor",
    "four",
    "executor",
    "whatever",
    "want",
    "right",
    "understood",
    "much",
    "going",
    "confuse",
    "ask",
    "executor",
    "right",
    "say",
    "want",
    "executor",
    "much",
    "memory",
    "want",
    "executor",
    "say",
    "want",
    "10",
    "gb",
    "default",
    "get",
    "90",
    "okay",
    "10",
    "percentage",
    "allocated",
    "system",
    "calls",
    "container",
    "access",
    "system",
    "calls",
    "respond",
    "say",
    "10",
    "gbs",
    "10",
    "gb",
    "full",
    "give",
    "full",
    "10gb",
    "10",
    "percentage",
    "system",
    "take",
    "get",
    "9",
    "gb",
    "get",
    "get",
    "60",
    "percentage",
    "ultimately",
    "54",
    "percentage",
    "ram",
    "get",
    "jvm",
    "right",
    "garbage",
    "collection",
    "jvm",
    "management",
    "requires",
    "memory",
    "reality",
    "look",
    "spark",
    "cluster",
    "ask",
    "10",
    "gb",
    "container",
    "get",
    "around",
    "gb",
    "rdd",
    "remaining",
    "system",
    "take",
    "please",
    "keep",
    "mind",
    "interview",
    "question",
    "go",
    "say",
    "100",
    "gb",
    "ram",
    "get",
    "say",
    "boss",
    "ah",
    "default",
    "ask",
    "10gb",
    "10",
    "system",
    "calls",
    "take",
    "h",
    "container",
    "communicate",
    "operating",
    "system",
    "yes",
    "right",
    "yarn",
    "things",
    "reserve",
    "memory",
    "right",
    "10",
    "gb",
    "became",
    "9gb",
    "9gb",
    "jvm",
    "manage",
    "garbage",
    "collection",
    "communication",
    "uh",
    "take",
    "another",
    "think",
    "uh",
    "know",
    "exact",
    "number",
    "30",
    "percentage",
    "something",
    "take",
    "drill",
    "total",
    "think",
    "54",
    "56",
    "percentage",
    "10gb",
    "rdd",
    "memory",
    "get",
    "actually",
    "fit",
    "party",
    "means",
    "10",
    "gb",
    "get",
    "roughly",
    "around",
    "6",
    "gb",
    "fit",
    "partition",
    "think",
    "get",
    "jvm",
    "10",
    "size",
    "fit",
    "10",
    "gb",
    "wor",
    "partition",
    "work",
    "like",
    "actually",
    "internal",
    "spark",
    "run",
    "understand",
    "mean",
    "production",
    "run",
    "calculation",
    "wrong",
    "right",
    "understand",
    "calculation",
    "wrong",
    "okay",
    "part",
    "partitions",
    "uh",
    "parallelism",
    "look",
    "picture",
    "make",
    "sense",
    "picture",
    "four",
    "blocks",
    "ask",
    "four",
    "containers",
    "four",
    "partitions",
    "ideal",
    "case",
    "loaded",
    "called",
    "rdd",
    "right",
    "real",
    "question",
    "write",
    "spark",
    "program",
    "right",
    "basically",
    "want",
    "apart",
    "rdd",
    "ram",
    "get",
    "data",
    "analyze",
    "data",
    "analyze",
    "data",
    "python",
    "uh",
    "learn",
    "something",
    "called",
    "higher",
    "order",
    "function",
    "function",
    "know",
    "right",
    "normally",
    "write",
    "python",
    "function",
    "say",
    "function",
    "function",
    "name",
    "blah",
    "blah",
    "blah",
    "let",
    "def",
    "write",
    "function",
    "reuse",
    "function",
    "creating",
    "function",
    "call",
    "anytime",
    "something",
    "called",
    "anonymous",
    "function",
    "disposable",
    "function",
    "let",
    "say",
    "want",
    "create",
    "function",
    "use",
    "want",
    "anymore",
    "really",
    "give",
    "name",
    "function",
    "defin",
    "create",
    "fight",
    "called",
    "anonymous",
    "function",
    "called",
    "anonymous",
    "function",
    "anonymous",
    "okay",
    "uh",
    "spark",
    "programming",
    "spark",
    "programming",
    "something",
    "called",
    "higher",
    "order",
    "function",
    "something",
    "called",
    "higher",
    "order",
    "function",
    "higher",
    "order",
    "function",
    "let",
    "say",
    "function",
    "called",
    "abc",
    "pass",
    "another",
    "function",
    "function",
    "called",
    "called",
    "higher",
    "order",
    "function",
    "meaning",
    "abc",
    "function",
    "normally",
    "pass",
    "parameter",
    "something",
    "value",
    "right",
    "say",
    "b",
    "function",
    "pass",
    "another",
    "function",
    "function",
    "called",
    "higher",
    "order",
    "function",
    "passing",
    "anonymous",
    "functions",
    "lambda",
    "anonymous",
    "anonymous",
    "function",
    "show",
    "code",
    "become",
    "better",
    "sparkk",
    "basically",
    "create",
    "rdd",
    "data",
    "ready",
    "data",
    "want",
    "process",
    "data",
    "process",
    "data",
    "something",
    "called",
    "transformations",
    "actually",
    "spark",
    "website",
    "also",
    "uh",
    "good",
    "look",
    "spark",
    "official",
    "website",
    "go",
    "spark",
    "hey",
    "showing",
    "yeah",
    "spark",
    "official",
    "website",
    "spark",
    "go",
    "documentation",
    "say",
    "latest",
    "release",
    "right",
    "older",
    "versions",
    "click",
    "see",
    "spark",
    "versions",
    "see",
    "163",
    "last",
    "spark",
    "one",
    "version",
    "230",
    "version",
    "mean",
    "latest",
    "version",
    "go",
    "documentation",
    "let",
    "say",
    "latest",
    "release",
    "imagine",
    "latest",
    "release",
    "okay",
    "scroll",
    "okay",
    "see",
    "rdd",
    "programming",
    "guide",
    "see",
    "click",
    "okay",
    "scroll",
    "bit",
    "come",
    "back",
    "see",
    "resel",
    "distributor",
    "data",
    "set",
    "rdd",
    "created",
    "right",
    "created",
    "rdd",
    "least",
    "theoretically",
    "create",
    "rdd",
    "show",
    "create",
    "okay",
    "rdd",
    "operations",
    "data",
    "available",
    "rdd",
    "rdd",
    "right",
    "start",
    "writing",
    "functions",
    "okay",
    "anonymous",
    "function",
    "uh",
    "yeah",
    "transformations",
    "need",
    "understand",
    "transformations",
    "use",
    "spark",
    "map",
    "filter",
    "flap",
    "map",
    "many",
    "actually",
    "want",
    "filter",
    "data",
    "call",
    "filter",
    "okay",
    "call",
    "filter",
    "ask",
    "want",
    "filter",
    "within",
    "bracket",
    "write",
    "expression",
    "filter",
    "filter",
    "data",
    "map",
    "another",
    "like",
    "four",
    "okay",
    "uh",
    "map",
    "call",
    "map",
    "ask",
    "like",
    "want",
    "write",
    "expression",
    "within",
    "map",
    "map",
    "perform",
    "higher",
    "order",
    "functions",
    "map",
    "filter",
    "flat",
    "map",
    "higher",
    "order",
    "functions",
    "actually",
    "something",
    "called",
    "transformations",
    "one",
    "rdd",
    "apply",
    "functions",
    "create",
    "new",
    "rdd",
    "called",
    "transformation",
    "analyze",
    "data",
    "let",
    "say",
    "want",
    "filter",
    "data",
    "call",
    "filter",
    "transformation",
    "rdds",
    "immutable",
    "important",
    "point",
    "create",
    "rdd",
    "change",
    "create",
    "another",
    "one",
    "applying",
    "logic",
    "never",
    "edit",
    "rdd",
    "immutable",
    "right",
    "go",
    "ppt",
    "yeah",
    "created",
    "log",
    "lines",
    "rdd",
    "fine",
    "till",
    "understood",
    "probably",
    "interested",
    "error",
    "messages",
    "rdd",
    "know",
    "see",
    "lot",
    "data",
    "info",
    "warning",
    "error",
    "want",
    "error",
    "messages",
    "filter",
    "call",
    "filter",
    "transformation",
    "okay",
    "call",
    "filter",
    "transformation",
    "say",
    "hey",
    "spark",
    "okay",
    "match",
    "error",
    "lines",
    "give",
    "show",
    "light",
    "logic",
    "produce",
    "another",
    "rdd",
    "call",
    "errors",
    "rdd",
    "steps",
    "write",
    "spark",
    "program",
    "first",
    "create",
    "rdd",
    "want",
    "filter",
    "call",
    "filter",
    "know",
    "change",
    "whatever",
    "mean",
    "filter",
    "error",
    "messages",
    "store",
    "another",
    "rdd",
    "somebody",
    "asking",
    "happen",
    "memory",
    "right",
    "delete",
    "rdd",
    "mean",
    "enough",
    "memory",
    "let",
    "say",
    "rdd",
    "fit",
    "memory",
    "call",
    "filter",
    "action",
    "filter",
    "whatever",
    "required",
    "gone",
    "required",
    "right",
    "next",
    "processing",
    "start",
    "step",
    "right",
    "show",
    "okay",
    "originally",
    "let",
    "assume",
    "normal",
    "use",
    "case",
    "call",
    "function",
    "creates",
    "another",
    "rdd",
    "rdd",
    "gone",
    "current",
    "data",
    "um",
    "enabled",
    "yar",
    "dynamic",
    "allocation",
    "gone",
    "told",
    "right",
    "actually",
    "partition",
    "executor",
    "running",
    "guy",
    "idle",
    "know",
    "predict",
    "data",
    "right",
    "exe",
    "one",
    "problem",
    "huh",
    "problem",
    "four",
    "executors",
    "second",
    "executor",
    "data",
    "process",
    "filtered",
    "error",
    "error",
    "guy",
    "sitting",
    "idle",
    "one",
    "data",
    "node",
    "one",
    "executor",
    "data",
    "simply",
    "sit",
    "idle",
    "becomes",
    "problem",
    "right",
    "solve",
    "problem",
    "actually",
    "easy",
    "spark",
    "transformation",
    "called",
    "coilies",
    "coilies",
    "common",
    "transformation",
    "call",
    "coil",
    "pass",
    "number",
    "reduce",
    "number",
    "partitions",
    "resite",
    "si",
    "calculate",
    "example",
    "assume",
    "know",
    "see",
    "look",
    "data",
    "second",
    "partition",
    "empty",
    "third",
    "partition",
    "one",
    "line",
    "also",
    "assuming",
    "want",
    "two",
    "tell",
    "spark",
    "take",
    "rdd",
    "apply",
    "coilies",
    "keep",
    "data",
    "delete",
    "two",
    "partition",
    "two",
    "jvm",
    "want",
    "two",
    "bring",
    "two",
    "partitions",
    "example",
    "example",
    "one",
    "terab",
    "data",
    "right",
    "take",
    "good",
    "sample",
    "data",
    "uh",
    "let",
    "say",
    "10",
    "gb",
    "100",
    "gb",
    "run",
    "understand",
    "know",
    "uh",
    "filter",
    "okay",
    "call",
    "collect",
    "action",
    "let",
    "say",
    "filter",
    "want",
    "see",
    "data",
    "know",
    "originally",
    "loaded",
    "let",
    "say",
    "know",
    "1",
    "gb",
    "data",
    "10",
    "gb",
    "data",
    "filter",
    "get",
    "data",
    "5gb",
    "means",
    "half",
    "reduced",
    "calculate",
    "okay",
    "loading",
    "one",
    "terabyte",
    "data",
    "manage",
    "many",
    "jvm",
    "step",
    "call",
    "coilies",
    "important",
    "point",
    "surprising",
    "let",
    "say",
    "wrote",
    "spar",
    "code",
    "wrote",
    "three",
    "lines",
    "create",
    "rdd",
    "filter",
    "rdd",
    "say",
    "coilies",
    "say",
    "run",
    "nothing",
    "happen",
    "surprise",
    "actually",
    "wrote",
    "spar",
    "code",
    "wrote",
    "three",
    "lines",
    "okay",
    "read",
    "hdfs",
    "filter",
    "error",
    "message",
    "set",
    "coil",
    "submit",
    "job",
    "cluster",
    "nothing",
    "happen",
    "cluster",
    "lazy",
    "something",
    "called",
    "lazy",
    "execution",
    "meaning",
    "spark",
    "start",
    "execution",
    "unless",
    "call",
    "something",
    "called",
    "action",
    "transformations",
    "meaning",
    "changing",
    "data",
    "never",
    "saying",
    "show",
    "output",
    "right",
    "saying",
    "repartition",
    "data",
    "filter",
    "data",
    "okay",
    "saying",
    "show",
    "output",
    "unless",
    "call",
    "something",
    "called",
    "action",
    "say",
    "give",
    "output",
    "nothing",
    "work",
    "call",
    "action",
    "called",
    "collect",
    "collect",
    "common",
    "action",
    "spark",
    "call",
    "collect",
    "okay",
    "telling",
    "spark",
    "want",
    "see",
    "final",
    "output",
    "collect",
    "look",
    "cleaned",
    "rdd",
    "asking",
    "give",
    "output",
    "cleaned",
    "rdd",
    "spark",
    "understand",
    "want",
    "cleaned",
    "rdd",
    "errors",
    "rdd",
    "want",
    "error",
    "hard",
    "read",
    "first",
    "go",
    "hard",
    "disk",
    "read",
    "data",
    "stuff",
    "show",
    "output",
    "right",
    "show",
    "screen",
    "entire",
    "pipeline",
    "emptied",
    "spark",
    "never",
    "keeps",
    "data",
    "memory",
    "processing",
    "okay",
    "got",
    "result",
    "screen",
    "right",
    "everything",
    "gone",
    "rdd",
    "nothing",
    "container",
    "partition",
    "nothing",
    "everything",
    "gone",
    "split",
    "second",
    "things",
    "happens",
    "want",
    "know",
    "whether",
    "rerun",
    "repartition",
    "go",
    "back",
    "okay",
    "save",
    "error",
    "also",
    "save",
    "method",
    "rdd",
    "say",
    "save",
    "save",
    "file",
    "text",
    "file",
    "say",
    "save",
    "action",
    "filter",
    "rdd",
    "check",
    "size",
    "right",
    "original",
    "data",
    "10",
    "gb",
    "filter",
    "5gb",
    "process",
    "5gb",
    "launching",
    "many",
    "things",
    "rewrite",
    "code",
    "way",
    "collect",
    "simply",
    "display",
    "output",
    "screen",
    "also",
    "action",
    "called",
    "save",
    "text",
    "file",
    "okay",
    "uh",
    "talking",
    "showing",
    "stuff",
    "see",
    "actions",
    "question",
    "want",
    "good",
    "question",
    "asking",
    "problem",
    "run",
    "spar",
    "code",
    "split",
    "second",
    "everything",
    "happen",
    "see",
    "output",
    "screen",
    "rdd",
    "nothing",
    "memory",
    "uh",
    "called",
    "dag",
    "director",
    "cyclic",
    "graph",
    "spark",
    "create",
    "dag",
    "graph",
    "execution",
    "perform",
    "step",
    "one",
    "step",
    "two",
    "step",
    "three",
    "step",
    "four",
    "result",
    "everything",
    "gone",
    "run",
    "load",
    "different",
    "thing",
    "uh",
    "question",
    "come",
    "give",
    "collect",
    "action",
    "return",
    "elements",
    "driver",
    "program",
    "meaning",
    "see",
    "output",
    "also",
    "say",
    "save",
    "text",
    "file",
    "action",
    "uh",
    "save",
    "see",
    "save",
    "text",
    "file",
    "say",
    "save",
    "text",
    "file",
    "rdd",
    "whatever",
    "data",
    "rdd",
    "save",
    "text",
    "file",
    "see",
    "hadoop",
    "wherever",
    "storing",
    "right",
    "action",
    "available",
    "save",
    "cassandra",
    "table",
    "many",
    "ways",
    "save",
    "data",
    "ideally",
    "text",
    "file",
    "save",
    "see",
    "data",
    "okay",
    "uh",
    "initial",
    "uh",
    "phase",
    "reading",
    "data",
    "even",
    "map",
    "reduce",
    "spark",
    "everything",
    "blocks",
    "reading",
    "difference",
    "makes",
    "data",
    "available",
    "memory",
    "difference",
    "comes",
    "see",
    "filter",
    "coilies",
    "let",
    "say",
    "something",
    "else",
    "also",
    "data",
    "still",
    "memory",
    "writing",
    "anything",
    "intermediate",
    "result",
    "pushing",
    "hard",
    "disk",
    "call",
    "collect",
    "write",
    "100",
    "functions",
    "call",
    "collect",
    "manipulation",
    "happen",
    "finally",
    "collect",
    "display",
    "output",
    "speed",
    "h",
    "good",
    "question",
    "one",
    "action",
    "called",
    "good",
    "question",
    "yeah",
    "many",
    "actions",
    "let",
    "say",
    "collect",
    "save",
    "text",
    "file",
    "collect",
    "one",
    "action",
    "say",
    "collect",
    "save",
    "text",
    "file",
    "first",
    "collect",
    "run",
    "say",
    "save",
    "text",
    "file",
    "whole",
    "operation",
    "start",
    "wondering",
    "way",
    "improve",
    "show",
    "okay",
    "probably",
    "question",
    "um",
    "show",
    "execute",
    "execute",
    "dash",
    "dag",
    "nothing",
    "directed",
    "cyclic",
    "graph",
    "fancy",
    "way",
    "saying",
    "spark",
    "creates",
    "steps",
    "executed",
    "graph",
    "format",
    "practically",
    "show",
    "dag",
    "run",
    "spark",
    "job",
    "see",
    "see",
    "partitions",
    "see",
    "dag",
    "see",
    "many",
    "executors",
    "launching",
    "everything",
    "visible",
    "theory",
    "theory",
    "seeing",
    "unless",
    "speak",
    "show",
    "say",
    "came",
    "know",
    "partitions",
    "actually",
    "came",
    "confused",
    "right",
    "driver",
    "collects",
    "data",
    "full",
    "step",
    "want",
    "look",
    "uh",
    "wanted",
    "filter",
    "write",
    "mapper",
    "reduce",
    "aggregation",
    "either",
    "say",
    "sum",
    "multiply",
    "aggregation",
    "function",
    "get",
    "one",
    "key",
    "values",
    "reducer",
    "want",
    "values",
    "ca",
    "filter",
    "aggregate",
    "whatever",
    "write",
    "filter",
    "write",
    "mapper",
    "side",
    "final",
    "st",
    "stage",
    "collect",
    "collect",
    "whatever",
    "want",
    "filter",
    "advantage",
    "key",
    "value",
    "pair",
    "also",
    "work",
    "key",
    "value",
    "pairs",
    "spark",
    "create",
    "key",
    "value",
    "pair",
    "similar",
    "map",
    "produce",
    "operate",
    "reduce",
    "things",
    "uh",
    "yeah",
    "asking",
    "question",
    "think",
    "answer",
    "look",
    "picture",
    "good",
    "picture",
    "explain",
    "see",
    "picture",
    "stuff",
    "dog",
    "lines",
    "rdd",
    "errors",
    "rdd",
    "cleaned",
    "rdd",
    "finally",
    "calling",
    "action",
    "called",
    "count",
    "count",
    "another",
    "action",
    "say",
    "count",
    "everything",
    "work",
    "beginning",
    "count",
    "right",
    "count",
    "action",
    "called",
    "happened",
    "show",
    "result",
    "five",
    "five",
    "lines",
    "rdd",
    "surprises",
    "also",
    "want",
    "call",
    "one",
    "action",
    "rdd",
    "called",
    "save",
    "cassandra",
    "problem",
    "call",
    "action",
    "pipeline",
    "empty",
    "spark",
    "start",
    "reading",
    "block",
    "creating",
    "original",
    "rdd",
    "filtering",
    "errors",
    "save",
    "cassandra",
    "okay",
    "one",
    "thing",
    "one",
    "filter",
    "want",
    "messages",
    "one",
    "data",
    "see",
    "message",
    "one",
    "message",
    "one",
    "message",
    "one",
    "get",
    "message",
    "one",
    "say",
    "collect",
    "want",
    "see",
    "happen",
    "start",
    "top",
    "look",
    "three",
    "actions",
    "see",
    "three",
    "actions",
    "depend",
    "rdd",
    "everything",
    "starting",
    "right",
    "cash",
    "rdd",
    "possible",
    "cache",
    "rdd",
    "say",
    "caching",
    "whatever",
    "data",
    "rdd",
    "along",
    "rdd",
    "cached",
    "cache",
    "cach",
    "ram",
    "cach",
    "hard",
    "disk",
    "also",
    "say",
    "ram",
    "plus",
    "hard",
    "disk",
    "like",
    "wherever",
    "want",
    "multiple",
    "options",
    "cach",
    "cash",
    "rdd",
    "let",
    "say",
    "say",
    "count",
    "count",
    "say",
    "save",
    "cassandra",
    "start",
    "rdd",
    "data",
    "already",
    "cached",
    "think",
    "caching",
    "caching",
    "one",
    "month",
    "spark",
    "program",
    "caching",
    "deleted",
    "caching",
    "valid",
    "till",
    "program",
    "completes",
    "action",
    "actions",
    "exit",
    "everything",
    "gone",
    "show",
    "tomorrow",
    "start",
    "spark",
    "spark",
    "application",
    "uh",
    "developer",
    "create",
    "something",
    "called",
    "spark",
    "context",
    "object",
    "object",
    "create",
    "represents",
    "program",
    "ideally",
    "program",
    "finishes",
    "kill",
    "object",
    "kill",
    "everything",
    "gone",
    "even",
    "caching",
    "gone",
    "caching",
    "valid",
    "speed",
    "existing",
    "program",
    "caching",
    "program",
    "write",
    "next",
    "month",
    "possible",
    "execution",
    "time",
    "want",
    "speed",
    "see",
    "create",
    "clean",
    "aio",
    "whatever",
    "say",
    "count",
    "count",
    "delete",
    "thing",
    "memory",
    "right",
    "memory",
    "execution",
    "memory",
    "execution",
    "nothing",
    "memory",
    "ah",
    "keep",
    "intermediate",
    "data",
    "memory",
    "right",
    "tell",
    "spark",
    "call",
    "action",
    "show",
    "output",
    "rdd",
    "whatever",
    "data",
    "delete",
    "pipeline",
    "keep",
    "want",
    "reuse",
    "action",
    "keep",
    "called",
    "caching",
    "h",
    "command",
    "called",
    "unp",
    "persist",
    "unp",
    "persist",
    "action",
    "transformation",
    "say",
    "unp",
    "persist",
    "delete",
    "cach",
    "show",
    "cach",
    "tomorrow",
    "easy",
    "otherwise",
    "kill",
    "spar",
    "context",
    "object",
    "like",
    "session",
    "gone",
    "automatically",
    "deleted",
    "basically",
    "may",
    "thinking",
    "start",
    "programming",
    "spark",
    "right",
    "want",
    "write",
    "spark",
    "program",
    "first",
    "thing",
    "learn",
    "create",
    "rdd",
    "know",
    "create",
    "rdd",
    "start",
    "uh",
    "simple",
    "simple",
    "transformations",
    "map",
    "filter",
    "also",
    "flip",
    "side",
    "uh",
    "flip",
    "side",
    "initially",
    "spar",
    "came",
    "industry",
    "everybody",
    "mad",
    "rdds",
    "okay",
    "transformation",
    "write",
    "map",
    "filter",
    "people",
    "literally",
    "dying",
    "writing",
    "things",
    "okay",
    "soon",
    "problem",
    "see",
    "problem",
    "write",
    "code",
    "using",
    "transformations",
    "actions",
    "way",
    "sparkk",
    "optimize",
    "uh",
    "code",
    "saying",
    "say",
    "filter",
    "okay",
    "filter",
    "saying",
    "filter",
    "error",
    "messages",
    "right",
    "unless",
    "spark",
    "runs",
    "know",
    "talking",
    "things",
    "strict",
    "schema",
    "rdds",
    "strict",
    "schema",
    "want",
    "process",
    "structure",
    "data",
    "let",
    "say",
    "like",
    "csv",
    "file",
    "okay",
    "want",
    "read",
    "csv",
    "file",
    "want",
    "control",
    "want",
    "schema",
    "everything",
    "rdd",
    "good",
    "way",
    "process",
    "process",
    "spark",
    "internally",
    "able",
    "optimize",
    "code",
    "spark",
    "sql",
    "comes",
    "picture",
    "data",
    "frames",
    "call",
    "spark",
    "module",
    "called",
    "spark",
    "sql",
    "spark",
    "sql",
    "much",
    "powerful",
    "optimized",
    "core",
    "spark",
    "write",
    "spark",
    "sql",
    "know",
    "create",
    "table",
    "query",
    "using",
    "sql",
    "end",
    "sql",
    "easy",
    "program",
    "much",
    "optimized",
    "core",
    "rdds",
    "using",
    "lambda",
    "spark",
    "way",
    "understand",
    "lambda",
    "using",
    "unless",
    "runs",
    "right",
    "wrote",
    "lambda",
    "code",
    "weird",
    "code",
    "spark",
    "way",
    "understand",
    "meaning",
    "lambda",
    "unless",
    "see",
    "data",
    "table",
    "say",
    "filter",
    "column",
    "spark",
    "knows",
    "column",
    "data",
    "type",
    "right",
    "avoid",
    "column",
    "loading",
    "data",
    "optimize",
    "saying",
    "right",
    "write",
    "sql",
    "query",
    "filter",
    "group",
    "join",
    "something",
    "spark",
    "read",
    "apply",
    "schema",
    "understand",
    "talking",
    "talk",
    "data",
    "frame",
    "class",
    "talk",
    "questions",
    "cor",
    "spark",
    "like",
    "far",
    "discussed",
    "anything",
    "clear",
    "ask",
    "okay",
    "think",
    "okay",
    "common",
    "transformations",
    "map",
    "flat",
    "map",
    "filter",
    "name",
    "transformation",
    "actions",
    "right",
    "yeah",
    "um",
    "anyway",
    "get",
    "started",
    "core",
    "hands",
    "tomorrow",
    "uh",
    "look",
    "things",
    "cluster",
    "connected",
    "cluster",
    "right",
    "right",
    "spark",
    "gives",
    "shell",
    "work",
    "like",
    "hive",
    "shell",
    "remember",
    "started",
    "high",
    "started",
    "typing",
    "similar",
    "spark",
    "gives",
    "something",
    "called",
    "spark",
    "shell",
    "h",
    "spark",
    "shell",
    "available",
    "python",
    "scala",
    "r",
    "java",
    "shell",
    "java",
    "shell",
    "functionality",
    "write",
    "using",
    "id",
    "something",
    "right",
    "want",
    "uh",
    "spark",
    "shell",
    "scala",
    "going",
    "python",
    "want",
    "scala",
    "shell",
    "simply",
    "say",
    "spark",
    "shell",
    "simply",
    "type",
    "spark",
    "hyph",
    "shell",
    "uh",
    "oh",
    "think",
    "spark",
    "shell",
    "working",
    "think",
    "export",
    "path",
    "get",
    "started",
    "spark",
    "shell",
    "okay",
    "let",
    "try",
    "let",
    "try",
    "python",
    "shell",
    "ppar",
    "2",
    "work",
    "h",
    "99",
    "java",
    "8",
    "java",
    "8",
    "supported",
    "okay",
    "think",
    "cluster",
    "disabled",
    "scala",
    "shell",
    "access",
    "maybe",
    "export",
    "configuration",
    "able",
    "open",
    "scalar",
    "shell",
    "want",
    "python",
    "shell",
    "type",
    "pi",
    "spark",
    "two",
    "command",
    "p",
    "spark",
    "two",
    "spark",
    "two",
    "cluster",
    "spark",
    "one",
    "spark",
    "two",
    "installed",
    "try",
    "uh",
    "know",
    "basics",
    "like",
    "advanced",
    "stuff",
    "know",
    "taking",
    "much",
    "time",
    "usually",
    "shell",
    "starts",
    "fast",
    "shell",
    "starting",
    "let",
    "check",
    "yeah",
    "started",
    "simple",
    "python",
    "shell",
    "okay",
    "says",
    "starting",
    "shell",
    "says",
    "type",
    "help",
    "blah",
    "blah",
    "blah",
    "uh",
    "logging",
    "level",
    "spark",
    "r",
    "says",
    "spark",
    "version",
    "cloudex",
    "lab",
    "running",
    "using",
    "python",
    "version",
    "275",
    "okay",
    "okay",
    "spark",
    "session",
    "available",
    "spark",
    "talk",
    "later",
    "later",
    "spark",
    "session",
    "okay",
    "says",
    "spark",
    "session",
    "available",
    "spark",
    "okay",
    "spark",
    "shell",
    "start",
    "creating",
    "rdds",
    "writing",
    "transformations",
    "everything",
    "want",
    "okay",
    "see",
    "show",
    "type",
    "command",
    "p",
    "spark",
    "simply",
    "type",
    "p",
    "spark",
    "okay",
    "dash",
    "dash",
    "help",
    "sorry",
    "sorry",
    "p",
    "spark",
    "2",
    "right",
    "py",
    "spark",
    "2",
    "hyphen",
    "hyphen",
    "help",
    "yeah",
    "want",
    "show",
    "options",
    "okay",
    "look",
    "saying",
    "want",
    "start",
    "spark",
    "shell",
    "okay",
    "need",
    "help",
    "starting",
    "right",
    "says",
    "options",
    "one",
    "master",
    "url",
    "worry",
    "okay",
    "scroll",
    "show",
    "see",
    "driver",
    "memory",
    "start",
    "spark",
    "shell",
    "okay",
    "driver",
    "created",
    "okay",
    "ask",
    "many",
    "executors",
    "want",
    "right",
    "much",
    "memory",
    "want",
    "driver",
    "right",
    "one",
    "question",
    "see",
    "executor",
    "memory",
    "told",
    "right",
    "ask",
    "many",
    "executors",
    "want",
    "much",
    "memory",
    "default",
    "think",
    "1",
    "gb",
    "written",
    "default",
    "1",
    "gb",
    "okay",
    "ask",
    "executor",
    "memory",
    "right",
    "scroll",
    "uh",
    "see",
    "executor",
    "core",
    "many",
    "processor",
    "cores",
    "want",
    "executor",
    "right",
    "see",
    "driver",
    "core",
    "many",
    "processor",
    "core",
    "want",
    "driver",
    "number",
    "executor",
    "default",
    "two",
    "starting",
    "spark",
    "shell",
    "like",
    "interactive",
    "shell",
    "get",
    "two",
    "executors",
    "one",
    "driver",
    "executor",
    "1",
    "gb",
    "ram",
    "default",
    "setting",
    "using",
    "arguments",
    "say",
    "uh",
    "many",
    "executors",
    "want",
    "memory",
    "forth",
    "another",
    "important",
    "point",
    "let",
    "try",
    "okay",
    "give",
    "one",
    "moment",
    "p",
    "spark",
    "two",
    "okay",
    "let",
    "let",
    "open",
    "mail",
    "uh",
    "spark",
    "also",
    "ui",
    "ui",
    "sent",
    "mail",
    "every",
    "cluster",
    "different",
    "sign",
    "uh",
    "check",
    "yarn",
    "mode",
    "think",
    "okay",
    "let",
    "check",
    "okay",
    "one",
    "url",
    "think",
    "working",
    "probably",
    "work",
    "know",
    "yeah",
    "guys",
    "see",
    "uh",
    "gl",
    "faculty",
    "great",
    "learning",
    "okay",
    "says",
    "55",
    "seconds",
    "started",
    "uh",
    "shell",
    "actually",
    "showing",
    "history",
    "server",
    "wondering",
    "okay",
    "55",
    "seconds",
    "started",
    "let",
    "go",
    "application",
    "actually",
    "see",
    "url",
    "showing",
    "okay",
    "maybe",
    "think",
    "giving",
    "old",
    "ui",
    "let",
    "exit",
    "okay",
    "refresh",
    "looking",
    "history",
    "server",
    "url",
    "really",
    "useful",
    "us",
    "start",
    "spark",
    "shell",
    "exit",
    "display",
    "see",
    "p",
    "spark",
    "shell",
    "launched",
    "minute",
    "exit",
    "see",
    "really",
    "useful",
    "us",
    "uh",
    "come",
    "history",
    "server",
    "means",
    "exit",
    "spark",
    "okay",
    "real",
    "ui",
    "port",
    "number",
    "working",
    "currently",
    "running",
    "spark",
    "show",
    "show",
    "something",
    "session",
    "stopped",
    "okay",
    "anything",
    "stopped",
    "session",
    "go",
    "application",
    "spark",
    "ui",
    "see",
    "jobs",
    "jobs",
    "expand",
    "see",
    "executor",
    "one",
    "added",
    "executor",
    "two",
    "added",
    "default",
    "two",
    "executors",
    "uh",
    "see",
    "executors",
    "one",
    "driver",
    "two",
    "executors",
    "one",
    "driver",
    "active",
    "two",
    "executors",
    "got",
    "per",
    "right",
    "um",
    "one",
    "important",
    "point",
    "simply",
    "say",
    "p",
    "spark",
    "2",
    "simply",
    "say",
    "pi",
    "spark",
    "2",
    "spark",
    "start",
    "local",
    "mode",
    "means",
    "talking",
    "yarn",
    "anything",
    "want",
    "talk",
    "yarn",
    "say",
    "hyphen",
    "hyphen",
    "master",
    "yarn",
    "option",
    "use",
    "whenever",
    "starting",
    "spark",
    "shell",
    "otherwise",
    "start",
    "local",
    "mode",
    "happen",
    "driver",
    "executor",
    "everything",
    "single",
    "jvm",
    "also",
    "resources",
    "allocated",
    "operating",
    "system",
    "yan",
    "local",
    "mode",
    "yan",
    "role",
    "local",
    "mode",
    "good",
    "learning",
    "simply",
    "want",
    "learn",
    "say",
    "spy",
    "spark",
    "2",
    "start",
    "writing",
    "code",
    "whatever",
    "ideally",
    "launching",
    "spark",
    "shell",
    "say",
    "py",
    "spark",
    "2",
    "say",
    "hyphen",
    "hyphen",
    "master",
    "yan",
    "meaning",
    "uh",
    "want",
    "register",
    "yan",
    "launch",
    "spar",
    "request",
    "go",
    "yan",
    "yan",
    "allocate",
    "containers",
    "let",
    "try",
    "one",
    "thing",
    "let",
    "try",
    "launch",
    "parameters",
    "right",
    "options",
    "p",
    "spark",
    "das",
    "dash",
    "help",
    "let",
    "look",
    "options",
    "let",
    "say",
    "want",
    "executor",
    "want",
    "three",
    "executor",
    "probably",
    "right",
    "option",
    "num",
    "executor",
    "hyph",
    "hyund",
    "num",
    "executors",
    "say",
    "three",
    "hyphen",
    "hyphen",
    "execute",
    "course",
    "course",
    "let",
    "want",
    "memory",
    "memory",
    "hyphen",
    "hyphen",
    "executor",
    "memory",
    "right",
    "default",
    "1",
    "gb",
    "want",
    "probably",
    "2",
    "gb",
    "something",
    "like",
    "right",
    "simply",
    "go",
    "shell",
    "p",
    "spark",
    "2",
    "say",
    "master",
    "yar",
    "say",
    "number",
    "executors",
    "probably",
    "four",
    "know",
    "executor",
    "memory",
    "okay",
    "executor",
    "memory",
    "need",
    "2g",
    "2gb",
    "hitender",
    "uh",
    "spark",
    "support",
    "application",
    "option",
    "saying",
    "probably",
    "asking",
    "let",
    "see",
    "let",
    "see",
    "whether",
    "get",
    "three",
    "executors",
    "okay",
    "first",
    "try",
    "see",
    "whether",
    "get",
    "memory",
    "something",
    "right",
    "saying",
    "hey",
    "launch",
    "spark",
    "shell",
    "master",
    "sorry",
    "h",
    "meem",
    "right",
    "memory",
    "right",
    "yeah",
    "wrong",
    "command",
    "correct",
    "asked",
    "three",
    "executors",
    "started",
    "ca",
    "see",
    "unless",
    "exit",
    "let",
    "exit",
    "go",
    "history",
    "server",
    "okay",
    "refresh",
    "started",
    "right",
    "see",
    "executor",
    "one",
    "added",
    "two",
    "added",
    "three",
    "added",
    "okay",
    "see",
    "three",
    "executors",
    "given",
    "actually",
    "go",
    "executors",
    "see",
    "one",
    "driver",
    "executors",
    "right",
    "get",
    "executors",
    "see",
    "memory",
    "right",
    "much",
    "memory",
    "ask",
    "huh",
    "think",
    "one",
    "default",
    "selected",
    "right",
    "one",
    "gb",
    "selected",
    "actually",
    "rdd",
    "storage",
    "right",
    "getting",
    "384",
    "uh",
    "megabyte",
    "rule",
    "actually",
    "works",
    "like",
    "told",
    "around",
    "roughly",
    "50",
    "memory",
    "taken",
    "remaining",
    "give",
    "actually",
    "get",
    "around",
    "600",
    "mb",
    "display",
    "much",
    "loading",
    "data",
    "simply",
    "running",
    "executor",
    "simply",
    "running",
    "anything",
    "inside",
    "right",
    "right",
    "rdd",
    "data",
    "right",
    "load",
    "something",
    "actually",
    "see",
    "memory",
    "occupy",
    "much",
    "uh",
    "amount",
    "rdd",
    "occupi",
    "huh",
    "384",
    "right",
    "allocated",
    "data",
    "inside",
    "zero",
    "b",
    "saying",
    "actually",
    "load",
    "let",
    "say",
    "500",
    "data",
    "show",
    "500",
    "mb",
    "actually",
    "occupied",
    "maximum",
    "600",
    "mb",
    "think",
    "get",
    "40",
    "anyway",
    "gone",
    "system",
    "calls",
    "jvm",
    "right",
    "600",
    "500",
    "something",
    "uh",
    "50",
    "something",
    "see",
    "okay",
    "right",
    "see",
    "less",
    "memory",
    "actually",
    "anything",
    "simply",
    "displaying",
    "uh",
    "uh",
    "know",
    "much",
    "uh",
    "exited",
    "try",
    "change",
    "number",
    "executors",
    "four",
    "hyphen",
    "hyphen",
    "executor",
    "memory",
    "right",
    "memory",
    "2g",
    "threshold",
    "value",
    "think",
    "uh",
    "1",
    "gb",
    "let",
    "try",
    "threshold",
    "value",
    "ah",
    "says",
    "error",
    "occurred",
    "calling",
    "javas",
    "spar",
    "context",
    "required",
    "executor",
    "memory",
    "maximum",
    "threshold",
    "cluster",
    "cluster",
    "uh",
    "uh",
    "administrator",
    "said",
    "ask",
    "maximum",
    "much",
    "say",
    "248",
    "384",
    "mb",
    "uh",
    "saying",
    "max",
    "max",
    "ask",
    "248",
    "2",
    "gb",
    "memory",
    "happen",
    "launch",
    "executor",
    "initially",
    "allocate",
    "384",
    "mb",
    "okay",
    "keep",
    "2gb",
    "know",
    "say",
    "excess",
    "allocated",
    "saying",
    "total",
    "memories",
    "cluster",
    "administrator",
    "said",
    "launch",
    "try",
    "something",
    "like",
    "think",
    "uh",
    "gab",
    "work",
    "ideally",
    "let",
    "check",
    "work",
    "okay",
    "exit",
    "exit",
    "try",
    "tomorrow",
    "worry",
    "okay",
    "wanted",
    "check",
    "whether",
    "cluster",
    "running",
    "fine",
    "try",
    "together",
    "tomorrow",
    "worry",
    "okay",
    "mention",
    "um",
    "see",
    "size",
    "must",
    "specified",
    "bytes",
    "kilobytes",
    "megabytes",
    "gigabytes",
    "terabytes",
    "right",
    "think",
    "say",
    "g",
    "work",
    "right",
    "mention",
    "uh",
    "megabytes",
    "right",
    "say",
    "g",
    "h",
    "type",
    "typed",
    "g",
    "right",
    "work",
    "work",
    "work",
    "ideally",
    "right",
    "whole",
    "number",
    "think",
    "say",
    "1",
    "point",
    "5g",
    "like",
    "say",
    "1g",
    "2g",
    "okay",
    "launched",
    "exit",
    "uh",
    "showing",
    "correct",
    "showing",
    "history",
    "server",
    "like",
    "starting",
    "spark",
    "shell",
    "exit",
    "showing",
    "ui",
    "correct",
    "see",
    "live",
    "right",
    "running",
    "job",
    "see",
    "live",
    "ui",
    "working",
    "asking",
    "know",
    "jobs",
    "display",
    "anything",
    "running",
    "seeing",
    "anything",
    "rdd",
    "action",
    "something",
    "see",
    "okay",
    "stages",
    "talk",
    "storage",
    "show",
    "cached",
    "rdd",
    "cach",
    "rdd",
    "come",
    "storage",
    "section",
    "environment",
    "show",
    "java",
    "libraries",
    "executors",
    "show",
    "many",
    "executors",
    "driver",
    "added",
    "jobs",
    "jobs",
    "run",
    "job",
    "come",
    "like",
    "said",
    "thing",
    "collect",
    "display",
    "like",
    "run",
    "actual",
    "job",
    "display",
    "dag",
    "stages",
    "show",
    "job",
    "many",
    "stages",
    "stages",
    "created",
    "see",
    "tomorrow",
    "anyway",
    "okay",
    "storage",
    "environment",
    "executors",
    "right",
    "environment",
    "show",
    "different",
    "libraries",
    "part",
    "spark",
    "go",
    "ca",
    "actually",
    "mess",
    "want",
    "see",
    "shell",
    "right",
    "saying",
    "uh",
    "p",
    "spark",
    "starting",
    "okay",
    "gui",
    "access",
    "command",
    "happened",
    "saying",
    "hm",
    "spelling",
    "mistake",
    "ah",
    "sorry",
    "command",
    "sc",
    "underscore",
    "con",
    "weird",
    "command",
    "show",
    "shell",
    "seeing",
    "gui",
    "gui",
    "sc",
    "doore",
    "get",
    "perfect",
    "remember",
    "command",
    "happy",
    "okay",
    "weird",
    "command",
    "nobody",
    "remembers",
    "command",
    "actually",
    "sc",
    "doore",
    "okay",
    "uh",
    "output",
    "weird",
    "see",
    "something",
    "output",
    "see",
    "see",
    "number",
    "executor",
    "somewhere",
    "driver",
    "memory",
    "right",
    "driver",
    "memory",
    "say",
    "master",
    "yarn",
    "somewhere",
    "number",
    "ah",
    "see",
    "executor",
    "memory",
    "much",
    "allocated",
    "also",
    "see",
    "number",
    "executed",
    "see",
    "difficult",
    "read",
    "copy",
    "paste",
    "better",
    "even",
    "allowing",
    "copy",
    "h",
    "see",
    "copying",
    "getting",
    "command",
    "like",
    "million",
    "dollar",
    "command",
    "quite",
    "sure",
    "nobody",
    "know",
    "command",
    "even",
    "came",
    "know",
    "recently",
    "see",
    "spar",
    "configs",
    "cli",
    "sc",
    "doore",
    "conon",
    "get",
    "shortcut",
    "actually",
    "show",
    "command",
    "command",
    "please",
    "make",
    "note",
    "sometimes",
    "start",
    "working",
    "cluster",
    "give",
    "gui",
    "know",
    "many",
    "executors",
    "launched",
    "happened",
    "know",
    "look",
    "way",
    "way",
    "give",
    "idea",
    "number",
    "executors",
    "everything",
    "launched",
    "live",
    "live",
    "seeing",
    "like",
    "launched",
    "many",
    "score",
    "con",
    "getet",
    "good",
    "news",
    "try",
    "something",
    "shell",
    "bad",
    "news",
    "actually",
    "try",
    "things",
    "like",
    "jupiter",
    "notebook",
    "create",
    "rdd",
    "p",
    "park",
    "import",
    "sc",
    "blah",
    "blah",
    "blah",
    "uh",
    "output",
    "clear",
    "okay",
    "kdd",
    "cup",
    "data",
    "rad",
    "data",
    "paraly",
    "brings",
    "us",
    "end",
    "big",
    "data",
    "analytics",
    "tutorial",
    "guys",
    "sign",
    "like",
    "inform",
    "launched",
    "completely",
    "free",
    "platform",
    "called",
    "great",
    "learning",
    "academy",
    "access",
    "free",
    "courses",
    "ai",
    "cloud",
    "digital",
    "marketing",
    "guys",
    "thank",
    "much",
    "attending",
    "session",
    "great",
    "learning",
    "ahead",
    "e"
  ],
  "keywords": [
    "big",
    "data",
    "use",
    "new",
    "customers",
    "come",
    "go",
    "session",
    "like",
    "guys",
    "launched",
    "free",
    "platform",
    "called",
    "great",
    "learning",
    "access",
    "cloud",
    "check",
    "let",
    "start",
    "hadoop",
    "learn",
    "spark",
    "today",
    "um",
    "exactly",
    "look",
    "something",
    "ecosystem",
    "architecture",
    "basically",
    "things",
    "uh",
    "machine",
    "anything",
    "name",
    "call",
    "12",
    "years",
    "working",
    "8",
    "started",
    "people",
    "know",
    "typical",
    "tools",
    "lot",
    "regular",
    "many",
    "companies",
    "google",
    "get",
    "first",
    "thing",
    "say",
    "going",
    "teach",
    "right",
    "take",
    "case",
    "try",
    "understand",
    "actually",
    "sense",
    "related",
    "need",
    "level",
    "share",
    "okay",
    "company",
    "created",
    "application",
    "sales",
    "install",
    "much",
    "capture",
    "store",
    "simple",
    "world",
    "facebook",
    "nothing",
    "using",
    "rdbms",
    "system",
    "least",
    "good",
    "row",
    "column",
    "format",
    "table",
    "insert",
    "somebody",
    "well",
    "everything",
    "fine",
    "originally",
    "even",
    "transactional",
    "management",
    "oracle",
    "whatever",
    "customer",
    "end",
    "year",
    "processing",
    "everybody",
    "happening",
    "20",
    "fast",
    "ic",
    "bank",
    "want",
    "run",
    "project",
    "entire",
    "part",
    "problem",
    "question",
    "example",
    "also",
    "help",
    "coming",
    "might",
    "went",
    "said",
    "hey",
    "see",
    "later",
    "asked",
    "one",
    "systems",
    "size",
    "small",
    "sql",
    "enterprise",
    "sometimes",
    "ask",
    "keep",
    "every",
    "talk",
    "storage",
    "separate",
    "engine",
    "add",
    "database",
    "storing",
    "giving",
    "saying",
    "second",
    "divide",
    "partition",
    "u",
    "iphone",
    "creating",
    "tables",
    "schema",
    "properties",
    "another",
    "user",
    "means",
    "order",
    "always",
    "different",
    "whenever",
    "full",
    "join",
    "query",
    "fire",
    "produce",
    "output",
    "amount",
    "fail",
    "wait",
    "point",
    "possible",
    "four",
    "machines",
    "partitioning",
    "india",
    "buying",
    "based",
    "country",
    "us",
    "2",
    "3",
    "cluster",
    "time",
    "logic",
    "work",
    "countries",
    "maximum",
    "think",
    "somewhere",
    "around",
    "buy",
    "running",
    "process",
    "complete",
    "guy",
    "drawback",
    "become",
    "slow",
    "ca",
    "talking",
    "transaction",
    "h",
    "number",
    "columns",
    "says",
    "reason",
    "result",
    "meaning",
    "dump",
    "files",
    "yes",
    "yeah",
    "day",
    "solution",
    "real",
    "10",
    "million",
    "product",
    "1",
    "click",
    "picture",
    "immediately",
    "display",
    "thought",
    "databases",
    "whole",
    "course",
    "bit",
    "cassandra",
    "popular",
    "nosql",
    "speed",
    "amazon",
    "db",
    "key",
    "value",
    "moment",
    "page",
    "really",
    "faster",
    "way",
    "open",
    "search",
    "give",
    "handle",
    "30",
    "runs",
    "told",
    "old",
    "job",
    "java",
    "without",
    "allow",
    "nobody",
    "mean",
    "still",
    "idea",
    "third",
    "cost",
    "support",
    "pay",
    "money",
    "source",
    "collect",
    "analyze",
    "website",
    "long",
    "asking",
    "active",
    "anybody",
    "huge",
    "traditional",
    "able",
    "gone",
    "probably",
    "goes",
    "tomorrow",
    "back",
    "whether",
    "happened",
    "got",
    "comes",
    "handled",
    "type",
    "far",
    "explain",
    "realtime",
    "queries",
    "used",
    "clear",
    "better",
    "done",
    "reading",
    "hado",
    "important",
    "though",
    "directly",
    "aware",
    "oh",
    "etl",
    "load",
    "core",
    "making",
    "blah",
    "two",
    "xml",
    "getting",
    "json",
    "care",
    "web",
    "age",
    "group",
    "download",
    "default",
    "cases",
    "information",
    "may",
    "multiple",
    "ways",
    "log",
    "file",
    "text",
    "huh",
    "high",
    "bad",
    "wanted",
    "make",
    "place",
    "location",
    "city",
    "send",
    "write",
    "already",
    "correct",
    "warehouse",
    "language",
    "major",
    "difference",
    "read",
    "structure",
    "connect",
    "tool",
    "show",
    "push",
    "next",
    "last",
    "month",
    "stuff",
    "inside",
    "maybe",
    "setup",
    "single",
    "installed",
    "server",
    "limit",
    "none",
    "credit",
    "card",
    "message",
    "jobs",
    "since",
    "happen",
    "easy",
    "client",
    "terra",
    "5",
    "came",
    "sorry",
    "star",
    "batch",
    "never",
    "five",
    "operation",
    "questions",
    "tell",
    "mostly",
    "modify",
    "available",
    "analysis",
    "discuss",
    "anyway",
    "required",
    "map",
    "reduce",
    "programming",
    "framework",
    "top",
    "ruce",
    "r",
    "ideally",
    "programs",
    "written",
    "python",
    "uses",
    "similar",
    "line",
    "code",
    "writing",
    "program",
    "hive",
    "p",
    "hadu",
    "side",
    "delete",
    "became",
    "plus",
    "less",
    "almost",
    "original",
    "distributed",
    "apache",
    "version",
    "crashes",
    "era",
    "advantage",
    "find",
    "lab",
    "simply",
    "hbas",
    "hoton",
    "works",
    "linux",
    "100",
    "class",
    "three",
    "common",
    "edit",
    "servers",
    "found",
    "days",
    "edition",
    "either",
    "change",
    "b",
    "claa",
    "developer",
    "internally",
    "command",
    "control",
    "sure",
    "scala",
    "ah",
    "interesting",
    "else",
    "starting",
    "performance",
    "within",
    "terab",
    "decide",
    "hard",
    "disk",
    "space",
    "added",
    "external",
    "enough",
    "manage",
    "4",
    "master",
    "total",
    "select",
    "stored",
    "worry",
    "happens",
    "center",
    "50",
    "node",
    "gb",
    "ram",
    "500",
    "times",
    "normal",
    "normally",
    "home",
    "option",
    "otherwise",
    "block",
    "manager",
    "resources",
    "hdfs",
    "yan",
    "resource",
    "yarn",
    "usually",
    "nodes",
    "engines",
    "tb",
    "equal",
    "hawk",
    "impala",
    "mb",
    "imagine",
    "rare",
    "commands",
    "upload",
    "64",
    "blocks",
    "automatically",
    "g",
    "128",
    "replication",
    "metadata",
    "index",
    "standby",
    "notes",
    "set",
    "wo",
    "copy",
    "difficult",
    "supports",
    "production",
    "mention",
    "enable",
    "types",
    "convert",
    "balancer",
    "create",
    "manually",
    "wrote",
    "processor",
    "submit",
    "admin",
    "hi",
    "ui",
    "records",
    "save",
    "indexing",
    "layer",
    "memory",
    "scoop",
    "cdc",
    "flume",
    "folder",
    "kafka",
    "streaming",
    "action",
    "storm",
    "live",
    "fraud",
    "spar",
    "model",
    "pig",
    "count",
    "hit",
    "execute",
    "execution",
    "lp",
    "loading",
    "loaded",
    "best",
    "sentry",
    "request",
    "hue",
    "zookeeper",
    "host",
    "disabled",
    "shell",
    "directory",
    "rack",
    "yahoo",
    "ao",
    "orc",
    "compress",
    "compression",
    "book",
    "lines",
    "comma",
    "bucketing",
    "reducer",
    "graph",
    "supported",
    "cli",
    "driver",
    "executor",
    "slide",
    "exit",
    "meta",
    "remember",
    "property",
    "step",
    "local",
    "id",
    "serd",
    "path",
    "rows",
    "refresh",
    "error",
    "values",
    "function",
    "filter",
    "nasa",
    "documentation",
    "mapper",
    "restart",
    "static",
    "dynamic",
    "partitions",
    "region",
    "mode",
    "reducers",
    "bucket",
    "buckets",
    "vectorization",
    "mesos",
    "rdd",
    "kubernetes",
    "docker",
    "container",
    "containers",
    "launch",
    "caching",
    "jvm",
    "executors"
  ]
}