{
  "text": "hi everyone i'm patrick from the\nassembly ai team and in this video we\nlearn about reinforcement learning in\nthe previous two videos we already\ncovered supervised and unsupervised\nlearning and now reinforcement learning\nis the third area in the field of\nmachine learning so today you will learn\nabout the definition of reinforcement\nlearning of states actions and rewards\nand then we dive into q-learning and\ndeep q-learning with neural networks\nthis area has gotten a lot of popularity\nin recent years especially with video\ngames so maybe you have seen how an ai\nlearns to play snake or chess or the\nbreakout game but now you're wondering\nhow this works so the idea behind\nreinforcement learning is that a\nso-called software agent will learn from\nthe environment by interacting with it\nand then receiving rewards for\nperforming actions and then the agent\ntries to improve its behavior so\nessentially it teaches itself how to get\nbetter this idea is inspired from our\nnatural experiences\nimagine you're a child and you see a\nfireplace for the first time you like\nthat it's warm it's positive so you get\na positive reward but then you reach out\nwith your hand and try to touch it and\nnow it's too warm so it hurts so you get\na negative reward or a punishment so to\nsay but now you might have understood\nthis and learned that fire can be a good\nthing but that you should be careful and\nnot get too close and this is exactly\nhow reinforcement learning works it's\nthe computational approach of learning\nfrom actions in an environment through\nrewards and punishments one specific\nimplementation of this approach is the\nq-learning algorithm it's a value-based\napproach based on a so-called queue\ntable the q table calculates the maximum\nexpected future reward for each action\nat each state and with this information\nwe can then choose the action with the\nhighest reward let's look at a concrete\nexample to make this more clear let's\nsay we want to teach an ai how to play\nthe snake game in this game the snake\ntries to reach and eat the food without\nhitting the wall or itself we can list\nthe actions and states in a queue table\nthe columns will be the four possible\nactions the snake can do turning left\nright up and down and the state can be\nthe current direction so also left right\nup and down these are the rows but of\ncourse we can add more states to\ndescribe the current situation for\nexample we can describe the location of\nthe food and at the states food is left\nof the snake right up or down we could\nalso do the same thing with the walls\nand describe the danger but for\nsimplicity i leave this out here but you\nsee the more states we add here the more\ninformation we have about the\nenvironment but also the more complex\nour system will get okay so now we have\nall rows and columns and now the value\nof each cell will be the maximum\nexpected future reward for that given\nstate and action we call this the q\nvalue so far so good but how do we\ncalculate this q value here's the\ninteresting part we do not implement\nthis q value calculation in a fixed way\ninstead we improve this q table in an\niterative approach this is basically our\ntraining or learning process the q\nlearning algorithm works like this first\nwe initialize all q values for example\nwith a 0 then we choose an action a in\nthe current state s this is based on the\ncurrent best q value we perform this\naction and observe the outcome so we get\na new state we also measure their reward\nafter this action and then we update q\nwith an update formula that is called\nthe bellman equation and then we repeat\nsteps 2 to 5 until the learning no\nlonger improves and we get a nice q\ntable in the end now a few questions may\nappear first how can we choose the best\naction in the beginning when all our\nvalues are zero this is where the\nexploration versus exploitation\ntrade-off comes into play in the\nbeginning we choose the action randomly\nso that our agent can explore the\nenvironment but the more training steps\nwe get the more we reduce this random\nexploration and use exploitation instead\nso we make use of the information we\nhave this trade-off is controlled in the\ncalculations by a parameter that is\nusually called the epsilon parameter now\nthe next question is how the rewards are\nmeasured this is actually up to us so we\ncan come up with a good reward system\nfor the game in case of the snake game\nfor example we can give a reward of 10\npoints if the snake eats an apple and a\nreward of -10 points if the snake dies\nand zero for every other normal move now\nwith all these elements we can inspect\nthe bellman equation the idea here is to\nupdate our q value like this the new q\nvalue is calculated by the current q\nvalue plus a learning rate times a\nreward plus a discount rate times the\nhighest q value between possible actions\nfrom the new state and then minus the\ncurrent q value the discount rate is a\nvalue between 0 and 1 and determines how\nmuch the agent cares about rewards in\nthe distant future relative to those in\nthe immediate future so now we have\neverything we need and coming back to\nour iterative learning approach we can\nnow come up with a good q table by using\nthis q learning algorithm now deep q\nlearning takes the q learning idea and\ntakes it one step further instead of\nusing a q table we use a neural network\nthat takes a state and approximates the\nq values for each action based on that\nstate and we do this because using a\nclassic q table is not very scalable it\nmight work for a simple game but let's\nimagine a more complex game with dozens\nof possible actions and game states then\nthe q table will soon get far too\ncomplex and cannot be solved efficiently\nanymore so now we use a deep neural\nnetwork that gets the state as input and\nproduces different q values for each\naction and then again we can choose the\naction with the highest q value the\nlearning process is still the same with\nthis iterative update approach but\ninstead of updating the queue table here\nwe update the weights in the neural\nnetwork so that the outputs get better\nand this is how deep q learning works if\nyou're interested to see a concrete\ncoding tutorial with deep q learning let\nus know in the comments and then we can\ntry to create a future video about this\nalright i hope i could give you a good\nintroduction to reinforcement learning\nif you enjoyed the video then please\nleave us a thumbs up and consider\nsubscribing to our channel for more\ncontent like this also if you want to\ntry assembly ai for free then grab your\nfree api token using the link in the\ndescription below and then i hope to see\nyou in the next video bye\n",
  "words": [
    "hi",
    "everyone",
    "patrick",
    "assembly",
    "ai",
    "team",
    "video",
    "learn",
    "reinforcement",
    "learning",
    "previous",
    "two",
    "videos",
    "already",
    "covered",
    "supervised",
    "unsupervised",
    "learning",
    "reinforcement",
    "learning",
    "third",
    "area",
    "field",
    "machine",
    "learning",
    "today",
    "learn",
    "definition",
    "reinforcement",
    "learning",
    "states",
    "actions",
    "rewards",
    "dive",
    "deep",
    "neural",
    "networks",
    "area",
    "gotten",
    "lot",
    "popularity",
    "recent",
    "years",
    "especially",
    "video",
    "games",
    "maybe",
    "seen",
    "ai",
    "learns",
    "play",
    "snake",
    "chess",
    "breakout",
    "game",
    "wondering",
    "works",
    "idea",
    "behind",
    "reinforcement",
    "learning",
    "software",
    "agent",
    "learn",
    "environment",
    "interacting",
    "receiving",
    "rewards",
    "performing",
    "actions",
    "agent",
    "tries",
    "improve",
    "behavior",
    "essentially",
    "teaches",
    "get",
    "better",
    "idea",
    "inspired",
    "natural",
    "experiences",
    "imagine",
    "child",
    "see",
    "fireplace",
    "first",
    "time",
    "like",
    "warm",
    "positive",
    "get",
    "positive",
    "reward",
    "reach",
    "hand",
    "try",
    "touch",
    "warm",
    "hurts",
    "get",
    "negative",
    "reward",
    "punishment",
    "say",
    "might",
    "understood",
    "learned",
    "fire",
    "good",
    "thing",
    "careful",
    "get",
    "close",
    "exactly",
    "reinforcement",
    "learning",
    "works",
    "computational",
    "approach",
    "learning",
    "actions",
    "environment",
    "rewards",
    "punishments",
    "one",
    "specific",
    "implementation",
    "approach",
    "algorithm",
    "approach",
    "based",
    "queue",
    "table",
    "q",
    "table",
    "calculates",
    "maximum",
    "expected",
    "future",
    "reward",
    "action",
    "state",
    "information",
    "choose",
    "action",
    "highest",
    "reward",
    "let",
    "look",
    "concrete",
    "example",
    "make",
    "clear",
    "let",
    "say",
    "want",
    "teach",
    "ai",
    "play",
    "snake",
    "game",
    "game",
    "snake",
    "tries",
    "reach",
    "eat",
    "food",
    "without",
    "hitting",
    "wall",
    "list",
    "actions",
    "states",
    "queue",
    "table",
    "columns",
    "four",
    "possible",
    "actions",
    "snake",
    "turning",
    "left",
    "right",
    "state",
    "current",
    "direction",
    "also",
    "left",
    "right",
    "rows",
    "course",
    "add",
    "states",
    "describe",
    "current",
    "situation",
    "example",
    "describe",
    "location",
    "food",
    "states",
    "food",
    "left",
    "snake",
    "right",
    "could",
    "also",
    "thing",
    "walls",
    "describe",
    "danger",
    "simplicity",
    "leave",
    "see",
    "states",
    "add",
    "information",
    "environment",
    "also",
    "complex",
    "system",
    "get",
    "okay",
    "rows",
    "columns",
    "value",
    "cell",
    "maximum",
    "expected",
    "future",
    "reward",
    "given",
    "state",
    "action",
    "call",
    "q",
    "value",
    "far",
    "good",
    "calculate",
    "q",
    "value",
    "interesting",
    "part",
    "implement",
    "q",
    "value",
    "calculation",
    "fixed",
    "way",
    "instead",
    "improve",
    "q",
    "table",
    "iterative",
    "approach",
    "basically",
    "training",
    "learning",
    "process",
    "q",
    "learning",
    "algorithm",
    "works",
    "like",
    "first",
    "initialize",
    "q",
    "values",
    "example",
    "0",
    "choose",
    "action",
    "current",
    "state",
    "based",
    "current",
    "best",
    "q",
    "value",
    "perform",
    "action",
    "observe",
    "outcome",
    "get",
    "new",
    "state",
    "also",
    "measure",
    "reward",
    "action",
    "update",
    "q",
    "update",
    "formula",
    "called",
    "bellman",
    "equation",
    "repeat",
    "steps",
    "2",
    "5",
    "learning",
    "longer",
    "improves",
    "get",
    "nice",
    "q",
    "table",
    "end",
    "questions",
    "may",
    "appear",
    "first",
    "choose",
    "best",
    "action",
    "beginning",
    "values",
    "zero",
    "exploration",
    "versus",
    "exploitation",
    "comes",
    "play",
    "beginning",
    "choose",
    "action",
    "randomly",
    "agent",
    "explore",
    "environment",
    "training",
    "steps",
    "get",
    "reduce",
    "random",
    "exploration",
    "use",
    "exploitation",
    "instead",
    "make",
    "use",
    "information",
    "controlled",
    "calculations",
    "parameter",
    "usually",
    "called",
    "epsilon",
    "parameter",
    "next",
    "question",
    "rewards",
    "measured",
    "actually",
    "us",
    "come",
    "good",
    "reward",
    "system",
    "game",
    "case",
    "snake",
    "game",
    "example",
    "give",
    "reward",
    "10",
    "points",
    "snake",
    "eats",
    "apple",
    "reward",
    "points",
    "snake",
    "dies",
    "zero",
    "every",
    "normal",
    "move",
    "elements",
    "inspect",
    "bellman",
    "equation",
    "idea",
    "update",
    "q",
    "value",
    "like",
    "new",
    "q",
    "value",
    "calculated",
    "current",
    "q",
    "value",
    "plus",
    "learning",
    "rate",
    "times",
    "reward",
    "plus",
    "discount",
    "rate",
    "times",
    "highest",
    "q",
    "value",
    "possible",
    "actions",
    "new",
    "state",
    "minus",
    "current",
    "q",
    "value",
    "discount",
    "rate",
    "value",
    "0",
    "1",
    "determines",
    "much",
    "agent",
    "cares",
    "rewards",
    "distant",
    "future",
    "relative",
    "immediate",
    "future",
    "everything",
    "need",
    "coming",
    "back",
    "iterative",
    "learning",
    "approach",
    "come",
    "good",
    "q",
    "table",
    "using",
    "q",
    "learning",
    "algorithm",
    "deep",
    "q",
    "learning",
    "takes",
    "q",
    "learning",
    "idea",
    "takes",
    "one",
    "step",
    "instead",
    "using",
    "q",
    "table",
    "use",
    "neural",
    "network",
    "takes",
    "state",
    "approximates",
    "q",
    "values",
    "action",
    "based",
    "state",
    "using",
    "classic",
    "q",
    "table",
    "scalable",
    "might",
    "work",
    "simple",
    "game",
    "let",
    "imagine",
    "complex",
    "game",
    "dozens",
    "possible",
    "actions",
    "game",
    "states",
    "q",
    "table",
    "soon",
    "get",
    "far",
    "complex",
    "solved",
    "efficiently",
    "anymore",
    "use",
    "deep",
    "neural",
    "network",
    "gets",
    "state",
    "input",
    "produces",
    "different",
    "q",
    "values",
    "action",
    "choose",
    "action",
    "highest",
    "q",
    "value",
    "learning",
    "process",
    "still",
    "iterative",
    "update",
    "approach",
    "instead",
    "updating",
    "queue",
    "table",
    "update",
    "weights",
    "neural",
    "network",
    "outputs",
    "get",
    "better",
    "deep",
    "q",
    "learning",
    "works",
    "interested",
    "see",
    "concrete",
    "coding",
    "tutorial",
    "deep",
    "q",
    "learning",
    "let",
    "us",
    "know",
    "comments",
    "try",
    "create",
    "future",
    "video",
    "alright",
    "hope",
    "could",
    "give",
    "good",
    "introduction",
    "reinforcement",
    "learning",
    "enjoyed",
    "video",
    "please",
    "leave",
    "us",
    "thumbs",
    "consider",
    "subscribing",
    "channel",
    "content",
    "like",
    "also",
    "want",
    "try",
    "assembly",
    "ai",
    "free",
    "grab",
    "free",
    "api",
    "token",
    "using",
    "link",
    "description",
    "hope",
    "see",
    "next",
    "video",
    "bye"
  ],
  "keywords": [
    "ai",
    "video",
    "learn",
    "reinforcement",
    "learning",
    "states",
    "actions",
    "rewards",
    "deep",
    "neural",
    "play",
    "snake",
    "game",
    "works",
    "idea",
    "agent",
    "environment",
    "get",
    "see",
    "first",
    "like",
    "reward",
    "try",
    "good",
    "approach",
    "algorithm",
    "based",
    "queue",
    "table",
    "q",
    "future",
    "action",
    "state",
    "information",
    "choose",
    "highest",
    "let",
    "example",
    "food",
    "possible",
    "left",
    "right",
    "current",
    "also",
    "describe",
    "complex",
    "value",
    "instead",
    "iterative",
    "values",
    "new",
    "update",
    "use",
    "us",
    "rate",
    "using",
    "takes",
    "network"
  ]
}