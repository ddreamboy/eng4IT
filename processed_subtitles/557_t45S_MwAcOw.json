{
  "text": "hi I'm sanjana Reddy a machine learning\nengineer at Google's Advanced Solutions\nlab\nthere's been a lot of excitement around\ngenerative Ai and all the new\nadvancements including new vertex AI\nfeatures that are coming up such as gen\nAI Studio model Garden genei API\nour objective in this short session is\nto give you a solid footing on some of\nthe underlying Concepts that make all\nthe Gen AI magic possible\ntoday I'm going to talk about\nTransformer models and the Bert model\nlanguage modeling has evolved over the\nyears\nthe recent breakthroughs in the past 10\nyears include the usage of neural\nnetworks to represent text such as word\nto whack and engrams in 2013.\nin 2014 the development of sequence to\nsequence models such as rnns and lstms\nhelped improve the performance of ml\nmodels on NLP tasks such as translation\nand text classification\nin 2015 the excitement came with\nattention mechanisms and the models\nbuilt based on it such as Transformers\nand the bird model\nin this presentation we'll focus on\nTransformers\nTransformers is based on a 2017 paper\nnamed attention is all you need\nalthough all the models before\nTransformers were able to represent\nverbs as vectors these vectors did not\ncontain the context\nand the usage of words changes based on\nthe context for example Bank in\nRiverbank versus Bank in bank robber\nmight have the same Vector\nrepresentation before attention\nmechanisms came about a Transformer is\nan encoder decoder model that uses the\nattention mechanism\nit can take advantage of parallelization\nand also process a large amount of data\nat the same time because of its model\narchitecture\nattention mechanism helps improve the\nperformance of machine translation\napplications\nTransformer models were built using\nattention mechanisms at the core\na Transformer model consists of encoder\nand decoder\nthe encoder encodes the input sequence\nand passes it to the decoder and the\ndecoder\ndecodes a representation for irrelevant\ntask\nthe encoding component is a stack of\nencoders of the same number the research\npaper that introduced Transformers\nStacks 6 encoders on top of each other\nsix is not a magical number it's just a\nhyper parameter\nthe encoders are all identical in\nstructure but with different weights\neach encoder can be broken down into two\nsub-layers\nthe first layer is called the\nself-attention\nthe input of the encoder first flows\nthrough a self-attention layer which\nhelps the encoder look at relevant parts\nof the words as it encodes a center word\nin the input sentence\nand the second layer is called a feed\nforward layer the output of the\nself-attention layer is fed to the feed\nforward neural network\nthe exact same feed forward neural\nnetwork is independently applied to each\nposition\nthe decoder has both the self-attention\nand the feed forward layer but between\nthem is the encoder decoder attention\nlayer that helps the decoder focus on\nrelevant parts of the input sentence\nafter embedding the words in the input\nsequence each of the embedding Vector\nflows through the two layers of the\nencoder\nthe word at each position passes through\na self-attention process then it passes\nthrough a feed-forward neural network\nthe exact same network with each Vector\nflowing through it separately\ndependencies exist between these paths\nin the self attention layer\nhowever the feed forward layer does not\nhave these dependencies and therefore\nvarious paths can be executed in\nparallel while they flow through the\nfeed forward layer\nin the self-attention layer the input\nembedding is broken up into query key\nand value vectors\nthese vectors are computed using weights\nthat the Transformer learns during the\ntraining process\nall of these computations happen in\nparallel in the model in the form of\nMatrix computations\nonce we have the query key and value\nvectors the next step is to multiply\neach value vector by the softmax score\nin preparation to sum them up the\nintuition here is to keep intact the\nvalues of the words you want to focus on\nand leave out irrelevant words by\nmultiplying them by tiny numbers like\n0.001 for example\nnext we have to sum up the weighted\nvalue vectors\nwhich produces the output of the\nself-attention layer at this position\nfor the first word you can send along\nthe resulting Vector to the feedforward\nneural network\nto sum up this process of getting the\nfinal embeddings these are the steps\nthat we take\nwe start with the natural language\nsentence\nembed each word in the sentence\nafter that we perform multi-headed\nattention eight times in this case and\nmultiply this embedded word with the\nrespective weighted matrices\nwe then calculate the attention using\nthe resulting qkv matrices\nfinally we concatenate the matrices to\nproduce the output Matrix which is the\nsame Dimension as the final Matrix that\nthis layer initially got\nthere's multiple variations of\nTransformers out there now\nsome use both the encoder and the\ndecoder component from the original\narchitecture some use only the encoder\nand some use only the decoder\na popular encoder only architecture is\nBert\nBert is one of the trained Transformer\nmodels Bert stands for bi-directional\nencoder representations from\nTransformers and was developed by Google\nin 2018.\nsince then multiple variations of bird\nhave been built today Bert Powers Google\nsearch\nyou can see how different the results\nprovided by Bert are for the same search\nquery before and after\nit was trained in two variations one\nmodel contains bird base which had 12\nstack of Transformers with approximately\n110 million parameters and the other\nbird large with 24 layers of\nTransformers with about 340 million\nparameters\nthe bird model is powerful because it\ncan handle long input context\nit was trained on the entire Wikipedia\nCorpus and Books Corpus\nthe bird model was trained for 1 million\nsteps Bert is trained on different tasks\nwhich means it has multi-task objective\nthis makes Bert very powerful\nbecause of the kind of tasks it was\ntrained on it works at both a sentence\nlevel and at a token level\nthese are the two different versions of\nBert that were originally released one\nis bird base which had 12 layers whereas\nbird large had 24 layers and compared to\nthe original Transformer which had six\nlayers\nthe way that bird works is that it was\ntrained on two different tasks task one\nis called a masked language model where\nthe sentences are masked and the model\nis trained to predict the masked words\nif you were to train bird from scratch\nyou would have to mask a certain\npercentage of the words in your Corpus\nthe recommended percentage for masking\nis 15 percent\nthe masking percentage achieves a\nbalance between too little and too much\nmasking do little masking makes the\ntraining process extremely expensive and\ntoo much masking removes the contacts\nthat the model requires\nthe second task is to predict the next\nsentence\nfor example the model is given two sets\nof sentences Bert aims to learn the\nrelationships between sentences and\npredict the next sentence given the\nfirst one\nfor example sentence a could be a man\nwent to the store and sentence B is he\nbought a gallon of milk\nbird is responsible for classifying if\nsentence B is the next sentence after\nsentence a\nthis is a binary classification task\nthis helps Bert perform at a sentence\nlevel\nin order to train Bert you need to feed\nthree different kinds of embeddings to\nthe model\nfor the input sentence you get three\ndifferent embeddings token segment and\nposition embeddings\nthe token embeddings is a representation\nof each token as an embedding in the\ninput sentence\nthe words are transformed into Vector\nrepresentations of certain dimensions\nbird can solve NLP tasks that involve\ntext classification as well\nan example is to classify whether two\nsentences say my dog is cute and he\nlikes playing are semantically similar\nthe pairs of input texts are simply\nconcatenated and fed into the model how\ndoes bird distinguish the inputs in a\ngiven pair the answer is to use segment\nembeddings\nthere is a special token represented by\nSCP that separates the two different\nsplits of the sentence\nanother problem is to learn the order of\nthe words in the sentence\nas you know bird consists of a stack of\nTransformers Bert is designed to process\ninput sequences up to a length of 512.\nthe order of the input sequence is\nincorporated into the position\nembeddings this allows Bert to learn a\nvector representation for each position\nvert can be used for different\nDownstream tasks although Bert was\ntrained on mass language modeling and\nsingle sentence classification it can be\nused for popular NLP tasks like single\nsentence classification sentence pair\nclassification question answering and\nsingle sentence tagging tasks\nthank you for listening\n",
  "words": [
    "hi",
    "sanjana",
    "reddy",
    "machine",
    "learning",
    "engineer",
    "google",
    "advanced",
    "solutions",
    "lab",
    "lot",
    "excitement",
    "around",
    "generative",
    "ai",
    "new",
    "advancements",
    "including",
    "new",
    "vertex",
    "ai",
    "features",
    "coming",
    "gen",
    "ai",
    "studio",
    "model",
    "garden",
    "genei",
    "api",
    "objective",
    "short",
    "session",
    "give",
    "solid",
    "footing",
    "underlying",
    "concepts",
    "make",
    "gen",
    "ai",
    "magic",
    "possible",
    "today",
    "going",
    "talk",
    "transformer",
    "models",
    "bert",
    "model",
    "language",
    "modeling",
    "evolved",
    "years",
    "recent",
    "breakthroughs",
    "past",
    "10",
    "years",
    "include",
    "usage",
    "neural",
    "networks",
    "represent",
    "text",
    "word",
    "whack",
    "engrams",
    "2014",
    "development",
    "sequence",
    "sequence",
    "models",
    "rnns",
    "lstms",
    "helped",
    "improve",
    "performance",
    "ml",
    "models",
    "nlp",
    "tasks",
    "translation",
    "text",
    "classification",
    "2015",
    "excitement",
    "came",
    "attention",
    "mechanisms",
    "models",
    "built",
    "based",
    "transformers",
    "bird",
    "model",
    "presentation",
    "focus",
    "transformers",
    "transformers",
    "based",
    "2017",
    "paper",
    "named",
    "attention",
    "need",
    "although",
    "models",
    "transformers",
    "able",
    "represent",
    "verbs",
    "vectors",
    "vectors",
    "contain",
    "context",
    "usage",
    "words",
    "changes",
    "based",
    "context",
    "example",
    "bank",
    "riverbank",
    "versus",
    "bank",
    "bank",
    "robber",
    "might",
    "vector",
    "representation",
    "attention",
    "mechanisms",
    "came",
    "transformer",
    "encoder",
    "decoder",
    "model",
    "uses",
    "attention",
    "mechanism",
    "take",
    "advantage",
    "parallelization",
    "also",
    "process",
    "large",
    "amount",
    "data",
    "time",
    "model",
    "architecture",
    "attention",
    "mechanism",
    "helps",
    "improve",
    "performance",
    "machine",
    "translation",
    "applications",
    "transformer",
    "models",
    "built",
    "using",
    "attention",
    "mechanisms",
    "core",
    "transformer",
    "model",
    "consists",
    "encoder",
    "decoder",
    "encoder",
    "encodes",
    "input",
    "sequence",
    "passes",
    "decoder",
    "decoder",
    "decodes",
    "representation",
    "irrelevant",
    "task",
    "encoding",
    "component",
    "stack",
    "encoders",
    "number",
    "research",
    "paper",
    "introduced",
    "transformers",
    "stacks",
    "6",
    "encoders",
    "top",
    "six",
    "magical",
    "number",
    "hyper",
    "parameter",
    "encoders",
    "identical",
    "structure",
    "different",
    "weights",
    "encoder",
    "broken",
    "two",
    "first",
    "layer",
    "called",
    "input",
    "encoder",
    "first",
    "flows",
    "layer",
    "helps",
    "encoder",
    "look",
    "relevant",
    "parts",
    "words",
    "encodes",
    "center",
    "word",
    "input",
    "sentence",
    "second",
    "layer",
    "called",
    "feed",
    "forward",
    "layer",
    "output",
    "layer",
    "fed",
    "feed",
    "forward",
    "neural",
    "network",
    "exact",
    "feed",
    "forward",
    "neural",
    "network",
    "independently",
    "applied",
    "position",
    "decoder",
    "feed",
    "forward",
    "layer",
    "encoder",
    "decoder",
    "attention",
    "layer",
    "helps",
    "decoder",
    "focus",
    "relevant",
    "parts",
    "input",
    "sentence",
    "embedding",
    "words",
    "input",
    "sequence",
    "embedding",
    "vector",
    "flows",
    "two",
    "layers",
    "encoder",
    "word",
    "position",
    "passes",
    "process",
    "passes",
    "neural",
    "network",
    "exact",
    "network",
    "vector",
    "flowing",
    "separately",
    "dependencies",
    "exist",
    "paths",
    "self",
    "attention",
    "layer",
    "however",
    "feed",
    "forward",
    "layer",
    "dependencies",
    "therefore",
    "various",
    "paths",
    "executed",
    "parallel",
    "flow",
    "feed",
    "forward",
    "layer",
    "layer",
    "input",
    "embedding",
    "broken",
    "query",
    "key",
    "value",
    "vectors",
    "vectors",
    "computed",
    "using",
    "weights",
    "transformer",
    "learns",
    "training",
    "process",
    "computations",
    "happen",
    "parallel",
    "model",
    "form",
    "matrix",
    "computations",
    "query",
    "key",
    "value",
    "vectors",
    "next",
    "step",
    "multiply",
    "value",
    "vector",
    "softmax",
    "score",
    "preparation",
    "sum",
    "intuition",
    "keep",
    "intact",
    "values",
    "words",
    "want",
    "focus",
    "leave",
    "irrelevant",
    "words",
    "multiplying",
    "tiny",
    "numbers",
    "like",
    "example",
    "next",
    "sum",
    "weighted",
    "value",
    "vectors",
    "produces",
    "output",
    "layer",
    "position",
    "first",
    "word",
    "send",
    "along",
    "resulting",
    "vector",
    "feedforward",
    "neural",
    "network",
    "sum",
    "process",
    "getting",
    "final",
    "embeddings",
    "steps",
    "take",
    "start",
    "natural",
    "language",
    "sentence",
    "embed",
    "word",
    "sentence",
    "perform",
    "attention",
    "eight",
    "times",
    "case",
    "multiply",
    "embedded",
    "word",
    "respective",
    "weighted",
    "matrices",
    "calculate",
    "attention",
    "using",
    "resulting",
    "qkv",
    "matrices",
    "finally",
    "concatenate",
    "matrices",
    "produce",
    "output",
    "matrix",
    "dimension",
    "final",
    "matrix",
    "layer",
    "initially",
    "got",
    "multiple",
    "variations",
    "transformers",
    "use",
    "encoder",
    "decoder",
    "component",
    "original",
    "architecture",
    "use",
    "encoder",
    "use",
    "decoder",
    "popular",
    "encoder",
    "architecture",
    "bert",
    "bert",
    "one",
    "trained",
    "transformer",
    "models",
    "bert",
    "stands",
    "encoder",
    "representations",
    "transformers",
    "developed",
    "google",
    "since",
    "multiple",
    "variations",
    "bird",
    "built",
    "today",
    "bert",
    "powers",
    "google",
    "search",
    "see",
    "different",
    "results",
    "provided",
    "bert",
    "search",
    "query",
    "trained",
    "two",
    "variations",
    "one",
    "model",
    "contains",
    "bird",
    "base",
    "12",
    "stack",
    "transformers",
    "approximately",
    "110",
    "million",
    "parameters",
    "bird",
    "large",
    "24",
    "layers",
    "transformers",
    "340",
    "million",
    "parameters",
    "bird",
    "model",
    "powerful",
    "handle",
    "long",
    "input",
    "context",
    "trained",
    "entire",
    "wikipedia",
    "corpus",
    "books",
    "corpus",
    "bird",
    "model",
    "trained",
    "1",
    "million",
    "steps",
    "bert",
    "trained",
    "different",
    "tasks",
    "means",
    "objective",
    "makes",
    "bert",
    "powerful",
    "kind",
    "tasks",
    "trained",
    "works",
    "sentence",
    "level",
    "token",
    "level",
    "two",
    "different",
    "versions",
    "bert",
    "originally",
    "released",
    "one",
    "bird",
    "base",
    "12",
    "layers",
    "whereas",
    "bird",
    "large",
    "24",
    "layers",
    "compared",
    "original",
    "transformer",
    "six",
    "layers",
    "way",
    "bird",
    "works",
    "trained",
    "two",
    "different",
    "tasks",
    "task",
    "one",
    "called",
    "masked",
    "language",
    "model",
    "sentences",
    "masked",
    "model",
    "trained",
    "predict",
    "masked",
    "words",
    "train",
    "bird",
    "scratch",
    "would",
    "mask",
    "certain",
    "percentage",
    "words",
    "corpus",
    "recommended",
    "percentage",
    "masking",
    "15",
    "percent",
    "masking",
    "percentage",
    "achieves",
    "balance",
    "little",
    "much",
    "masking",
    "little",
    "masking",
    "makes",
    "training",
    "process",
    "extremely",
    "expensive",
    "much",
    "masking",
    "removes",
    "contacts",
    "model",
    "requires",
    "second",
    "task",
    "predict",
    "next",
    "sentence",
    "example",
    "model",
    "given",
    "two",
    "sets",
    "sentences",
    "bert",
    "aims",
    "learn",
    "relationships",
    "sentences",
    "predict",
    "next",
    "sentence",
    "given",
    "first",
    "one",
    "example",
    "sentence",
    "could",
    "man",
    "went",
    "store",
    "sentence",
    "b",
    "bought",
    "gallon",
    "milk",
    "bird",
    "responsible",
    "classifying",
    "sentence",
    "b",
    "next",
    "sentence",
    "sentence",
    "binary",
    "classification",
    "task",
    "helps",
    "bert",
    "perform",
    "sentence",
    "level",
    "order",
    "train",
    "bert",
    "need",
    "feed",
    "three",
    "different",
    "kinds",
    "embeddings",
    "model",
    "input",
    "sentence",
    "get",
    "three",
    "different",
    "embeddings",
    "token",
    "segment",
    "position",
    "embeddings",
    "token",
    "embeddings",
    "representation",
    "token",
    "embedding",
    "input",
    "sentence",
    "words",
    "transformed",
    "vector",
    "representations",
    "certain",
    "dimensions",
    "bird",
    "solve",
    "nlp",
    "tasks",
    "involve",
    "text",
    "classification",
    "well",
    "example",
    "classify",
    "whether",
    "two",
    "sentences",
    "say",
    "dog",
    "cute",
    "likes",
    "playing",
    "semantically",
    "similar",
    "pairs",
    "input",
    "texts",
    "simply",
    "concatenated",
    "fed",
    "model",
    "bird",
    "distinguish",
    "inputs",
    "given",
    "pair",
    "answer",
    "use",
    "segment",
    "embeddings",
    "special",
    "token",
    "represented",
    "scp",
    "separates",
    "two",
    "different",
    "splits",
    "sentence",
    "another",
    "problem",
    "learn",
    "order",
    "words",
    "sentence",
    "know",
    "bird",
    "consists",
    "stack",
    "transformers",
    "bert",
    "designed",
    "process",
    "input",
    "sequences",
    "length",
    "order",
    "input",
    "sequence",
    "incorporated",
    "position",
    "embeddings",
    "allows",
    "bert",
    "learn",
    "vector",
    "representation",
    "position",
    "vert",
    "used",
    "different",
    "downstream",
    "tasks",
    "although",
    "bert",
    "trained",
    "mass",
    "language",
    "modeling",
    "single",
    "sentence",
    "classification",
    "used",
    "popular",
    "nlp",
    "tasks",
    "like",
    "single",
    "sentence",
    "classification",
    "sentence",
    "pair",
    "classification",
    "question",
    "answering",
    "single",
    "sentence",
    "tagging",
    "tasks",
    "thank",
    "listening"
  ],
  "keywords": [
    "google",
    "ai",
    "model",
    "transformer",
    "models",
    "bert",
    "language",
    "neural",
    "text",
    "word",
    "sequence",
    "nlp",
    "tasks",
    "classification",
    "attention",
    "mechanisms",
    "built",
    "based",
    "transformers",
    "bird",
    "focus",
    "vectors",
    "context",
    "words",
    "example",
    "bank",
    "vector",
    "representation",
    "encoder",
    "decoder",
    "process",
    "large",
    "architecture",
    "helps",
    "using",
    "input",
    "passes",
    "task",
    "stack",
    "encoders",
    "different",
    "two",
    "first",
    "layer",
    "called",
    "sentence",
    "feed",
    "forward",
    "output",
    "network",
    "position",
    "embedding",
    "layers",
    "query",
    "value",
    "matrix",
    "next",
    "sum",
    "embeddings",
    "matrices",
    "variations",
    "use",
    "one",
    "trained",
    "million",
    "corpus",
    "level",
    "token",
    "masked",
    "sentences",
    "predict",
    "percentage",
    "masking",
    "given",
    "learn",
    "order",
    "single"
  ]
}