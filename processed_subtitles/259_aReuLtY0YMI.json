{
  "text": "let's rewind to the days before the\nworld turned digital\nback then miniscule amounts of data were\ngenerated at a relatively sluggish pace\nall the data was mostly documents and in\nthe form of rows and columns\nstoring or processing this data wasn't\nmuch trouble as a single storage unit\nand processor combination would do the\njob but as years passed by the internet\ntook the world by storm giving rise to\ntons of data generated in a multitude of\nforms and formats every microsecond\nsemi-structured and unstructured data\nwas available now in the form of emails\nimages audio and video to name a few\nall this data became collectively known\nas big data\nalthough fascinating it became nearly\nimpossible to handle this big data and a\nstorage unit processor combination was\nobviously not enough\nso what was the solution\nmultiple storage units and processors\nwere undoubtedly the need of the hour\nthis concept was incorporated in the\nframework of hadoop that could store and\nprocess vast amounts of any data\nefficiently using a cluster of commodity\nhardware hadoop consisted of three\ncomponents that were specifically\ndesigned to work on big data in order to\ncapitalize on data the first step is\nstoring it the first component of hadoop\nis its storage unit the hadoop\ndistributed file system or hdfs\nstoring massive data on one computer is\nunfeasible hence data is distributed\namongst many computers and stored in\nblocks\nso if you have 600 megabytes of data to\nbe stored hdfs splits the data into\nmultiple blocks of data that are then\nstored on several data nodes in the\ncluster\n128 megabytes is the default size of\neach block\nhence 600 megabytes will be split into\nfour blocks a b c and d of 128 megabytes\neach\nand the remaining 88 megabytes in the\nlast block e\nso now you might be wondering what if\none data node crashes\ndo we lose that specific piece of data\nwell no that's the beauty of hdfs\nhdfs makes copies of the data and stores\nit across multiple systems\nfor example when block a is created it\nis replicated with a replication factor\nof 3 and stored on different data nodes\nthis is termed the replication method\nby doing so data is not lost at any cost\neven if one data node crashes making\nhdfs fault tolerant after storing the\ndata successfully it needs to be\nprocessed\nthis is where the second component of\nhadoop mapreduce comes into play\nin the traditional data processing\nmethod entire data would be processed on\na single machine having a single\nprocessor this consumed time and was\ninefficient especially when processing\nlarge volumes of a variety of data to\novercome this mapreduce splits data into\nparts and processes each of them\nseparately on different data nodes\nthe individual results are then\naggregated to give the final output\nlet's try to count the number of\noccurrences of words taking this example\nfirst the input is split into five\nseparate parts based on full stops\nthe next step is the mapper phase where\nthe occurrence of each word is counted\nand allocated a number\nafter that depending on the words\nsimilar words are shuffled sorted and\ngrouped following which in the reducer\nphase all the grouped words are given a\ncount\nfinally the output is displayed by\naggregating the results all this is done\nby writing a simple program\nsimilarly mapreduce processes each part\nof big data individually and then sums\nthe result at the end\nthis improves load balancing and saves a\nconsiderable amount of time\nnow that we have our mapreduce job ready\nit is time for us to run it on the\nhadoop cluster\nthis is done with the help of a set of\nresources such as ram network bandwidth\nand cpu\nmultiple jobs are run on hadoop\nsimultaneously and each of them needs\nsome resources to complete the task\nsuccessfully\nto efficiently manage these resources we\nhave the third component of hadoop which\nis yarn\nyet another resource negotiator or yarn\nconsists of a resource manager node\nmanager application master and\ncontainers the resource manager assigns\nresources node managers handle the nodes\nand monitor the resource usage in the\nnode the containers hold a collection of\nphysical resources\nsuppose we want to process the mapreduce\njob we had created\nfirst the application master requests\nthe container from the node manager\nonce the node manager gets the resources\nit sends them to the resource manager\nthis way yarn processes job requests and\nmanages cluster resources in hadoop\nin addition to these components hadoop\nalso has various big data tools and\nframeworks dedicated to managing\nprocessing and analyzing data\nthe hadoop ecosystem comprises several\nother components like hive pig apache\nspark flume and scoop to name a few\nthe hadoop ecosystem works together on\nbig data management\nso here's a question for you\nwhat is the advantage of the 3x\nreplication schema in hdfs\na supports parallel processing b faster\ndata analysis\nc ensures fault tolerance\nd\nmanages cluster resources\ngive it a thought and leave your answers\nin the comment section below three lucky\nwinners will receive amazon gift\nvouchers hadoop has proved to be a game\nchanger for businesses from startups and\nbig giants like facebook ibm ebay and\namazon there are several applications of\nhadoop like data warehousing\nrecommendation systems fraud detection\nand so on\nwe hope you enjoyed this video if you\ndid a thumbs up would be really\nappreciated here's your reminder to\nsubscribe to our channel and click on\nthe bell icon for more on the latest\ntechnologies and trends thank you for\nwatching and stay tuned for more from\nsimplylearn\nyou\n",
  "words": [
    "let",
    "rewind",
    "days",
    "world",
    "turned",
    "digital",
    "back",
    "miniscule",
    "amounts",
    "data",
    "generated",
    "relatively",
    "sluggish",
    "pace",
    "data",
    "mostly",
    "documents",
    "form",
    "rows",
    "columns",
    "storing",
    "processing",
    "data",
    "much",
    "trouble",
    "single",
    "storage",
    "unit",
    "processor",
    "combination",
    "would",
    "job",
    "years",
    "passed",
    "internet",
    "took",
    "world",
    "storm",
    "giving",
    "rise",
    "tons",
    "data",
    "generated",
    "multitude",
    "forms",
    "formats",
    "every",
    "microsecond",
    "unstructured",
    "data",
    "available",
    "form",
    "emails",
    "images",
    "audio",
    "video",
    "name",
    "data",
    "became",
    "collectively",
    "known",
    "big",
    "data",
    "although",
    "fascinating",
    "became",
    "nearly",
    "impossible",
    "handle",
    "big",
    "data",
    "storage",
    "unit",
    "processor",
    "combination",
    "obviously",
    "enough",
    "solution",
    "multiple",
    "storage",
    "units",
    "processors",
    "undoubtedly",
    "need",
    "hour",
    "concept",
    "incorporated",
    "framework",
    "hadoop",
    "could",
    "store",
    "process",
    "vast",
    "amounts",
    "data",
    "efficiently",
    "using",
    "cluster",
    "commodity",
    "hardware",
    "hadoop",
    "consisted",
    "three",
    "components",
    "specifically",
    "designed",
    "work",
    "big",
    "data",
    "order",
    "capitalize",
    "data",
    "first",
    "step",
    "storing",
    "first",
    "component",
    "hadoop",
    "storage",
    "unit",
    "hadoop",
    "distributed",
    "file",
    "system",
    "hdfs",
    "storing",
    "massive",
    "data",
    "one",
    "computer",
    "unfeasible",
    "hence",
    "data",
    "distributed",
    "amongst",
    "many",
    "computers",
    "stored",
    "blocks",
    "600",
    "megabytes",
    "data",
    "stored",
    "hdfs",
    "splits",
    "data",
    "multiple",
    "blocks",
    "data",
    "stored",
    "several",
    "data",
    "nodes",
    "cluster",
    "128",
    "megabytes",
    "default",
    "size",
    "block",
    "hence",
    "600",
    "megabytes",
    "split",
    "four",
    "blocks",
    "b",
    "c",
    "128",
    "megabytes",
    "remaining",
    "88",
    "megabytes",
    "last",
    "block",
    "e",
    "might",
    "wondering",
    "one",
    "data",
    "node",
    "crashes",
    "lose",
    "specific",
    "piece",
    "data",
    "well",
    "beauty",
    "hdfs",
    "hdfs",
    "makes",
    "copies",
    "data",
    "stores",
    "across",
    "multiple",
    "systems",
    "example",
    "block",
    "created",
    "replicated",
    "replication",
    "factor",
    "3",
    "stored",
    "different",
    "data",
    "nodes",
    "termed",
    "replication",
    "method",
    "data",
    "lost",
    "cost",
    "even",
    "one",
    "data",
    "node",
    "crashes",
    "making",
    "hdfs",
    "fault",
    "tolerant",
    "storing",
    "data",
    "successfully",
    "needs",
    "processed",
    "second",
    "component",
    "hadoop",
    "mapreduce",
    "comes",
    "play",
    "traditional",
    "data",
    "processing",
    "method",
    "entire",
    "data",
    "would",
    "processed",
    "single",
    "machine",
    "single",
    "processor",
    "consumed",
    "time",
    "inefficient",
    "especially",
    "processing",
    "large",
    "volumes",
    "variety",
    "data",
    "overcome",
    "mapreduce",
    "splits",
    "data",
    "parts",
    "processes",
    "separately",
    "different",
    "data",
    "nodes",
    "individual",
    "results",
    "aggregated",
    "give",
    "final",
    "output",
    "let",
    "try",
    "count",
    "number",
    "occurrences",
    "words",
    "taking",
    "example",
    "first",
    "input",
    "split",
    "five",
    "separate",
    "parts",
    "based",
    "full",
    "stops",
    "next",
    "step",
    "mapper",
    "phase",
    "occurrence",
    "word",
    "counted",
    "allocated",
    "number",
    "depending",
    "words",
    "similar",
    "words",
    "shuffled",
    "sorted",
    "grouped",
    "following",
    "reducer",
    "phase",
    "grouped",
    "words",
    "given",
    "count",
    "finally",
    "output",
    "displayed",
    "aggregating",
    "results",
    "done",
    "writing",
    "simple",
    "program",
    "similarly",
    "mapreduce",
    "processes",
    "part",
    "big",
    "data",
    "individually",
    "sums",
    "result",
    "end",
    "improves",
    "load",
    "balancing",
    "saves",
    "considerable",
    "amount",
    "time",
    "mapreduce",
    "job",
    "ready",
    "time",
    "us",
    "run",
    "hadoop",
    "cluster",
    "done",
    "help",
    "set",
    "resources",
    "ram",
    "network",
    "bandwidth",
    "cpu",
    "multiple",
    "jobs",
    "run",
    "hadoop",
    "simultaneously",
    "needs",
    "resources",
    "complete",
    "task",
    "successfully",
    "efficiently",
    "manage",
    "resources",
    "third",
    "component",
    "hadoop",
    "yarn",
    "yet",
    "another",
    "resource",
    "negotiator",
    "yarn",
    "consists",
    "resource",
    "manager",
    "node",
    "manager",
    "application",
    "master",
    "containers",
    "resource",
    "manager",
    "assigns",
    "resources",
    "node",
    "managers",
    "handle",
    "nodes",
    "monitor",
    "resource",
    "usage",
    "node",
    "containers",
    "hold",
    "collection",
    "physical",
    "resources",
    "suppose",
    "want",
    "process",
    "mapreduce",
    "job",
    "created",
    "first",
    "application",
    "master",
    "requests",
    "container",
    "node",
    "manager",
    "node",
    "manager",
    "gets",
    "resources",
    "sends",
    "resource",
    "manager",
    "way",
    "yarn",
    "processes",
    "job",
    "requests",
    "manages",
    "cluster",
    "resources",
    "hadoop",
    "addition",
    "components",
    "hadoop",
    "also",
    "various",
    "big",
    "data",
    "tools",
    "frameworks",
    "dedicated",
    "managing",
    "processing",
    "analyzing",
    "data",
    "hadoop",
    "ecosystem",
    "comprises",
    "several",
    "components",
    "like",
    "hive",
    "pig",
    "apache",
    "spark",
    "flume",
    "scoop",
    "name",
    "hadoop",
    "ecosystem",
    "works",
    "together",
    "big",
    "data",
    "management",
    "question",
    "advantage",
    "3x",
    "replication",
    "schema",
    "hdfs",
    "supports",
    "parallel",
    "processing",
    "b",
    "faster",
    "data",
    "analysis",
    "c",
    "ensures",
    "fault",
    "tolerance",
    "manages",
    "cluster",
    "resources",
    "give",
    "thought",
    "leave",
    "answers",
    "comment",
    "section",
    "three",
    "lucky",
    "winners",
    "receive",
    "amazon",
    "gift",
    "vouchers",
    "hadoop",
    "proved",
    "game",
    "changer",
    "businesses",
    "startups",
    "big",
    "giants",
    "like",
    "facebook",
    "ibm",
    "ebay",
    "amazon",
    "several",
    "applications",
    "hadoop",
    "like",
    "data",
    "warehousing",
    "recommendation",
    "systems",
    "fraud",
    "detection",
    "hope",
    "enjoyed",
    "video",
    "thumbs",
    "would",
    "really",
    "appreciated",
    "reminder",
    "subscribe",
    "channel",
    "click",
    "bell",
    "icon",
    "latest",
    "technologies",
    "trends",
    "thank",
    "watching",
    "stay",
    "tuned",
    "simplylearn"
  ],
  "keywords": [
    "let",
    "world",
    "amounts",
    "data",
    "generated",
    "form",
    "storing",
    "processing",
    "single",
    "storage",
    "unit",
    "processor",
    "combination",
    "would",
    "job",
    "video",
    "name",
    "became",
    "big",
    "handle",
    "multiple",
    "hadoop",
    "process",
    "efficiently",
    "cluster",
    "three",
    "components",
    "first",
    "step",
    "component",
    "distributed",
    "hdfs",
    "one",
    "hence",
    "stored",
    "blocks",
    "600",
    "megabytes",
    "splits",
    "several",
    "nodes",
    "128",
    "block",
    "split",
    "b",
    "c",
    "node",
    "crashes",
    "systems",
    "example",
    "created",
    "replication",
    "different",
    "method",
    "fault",
    "successfully",
    "needs",
    "processed",
    "mapreduce",
    "time",
    "parts",
    "processes",
    "results",
    "give",
    "output",
    "count",
    "number",
    "words",
    "phase",
    "grouped",
    "done",
    "run",
    "resources",
    "yarn",
    "resource",
    "manager",
    "application",
    "master",
    "containers",
    "requests",
    "manages",
    "ecosystem",
    "like",
    "amazon"
  ]
}