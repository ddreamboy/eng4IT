{
  "text": "[Music]\nwelcome back to deep learning and you\ncan see I have a couple of upgrades so\nwe have a much better recording quality\nnow and I hope you can also see that I\nfinally fix the sound problem so you\nshould be able to hear me much better\nright now and we are back to a new\nsession we want to talk about a couple\nof exciting topics so let's see what\nI've got for you so today I want to\nstart discussing different architectures\nand in particular in the first couple of\nvideos I want to talk a bit about the\nearly architectures the things that\nwe've seen in the very early days of\ndeep learning and we will follow them by\nlooking into deeper models in later\nvideos and in the end we want to talk\nabout learning architectures instead of\nwhat humans might need just dozens of\nexamples these things will need millions\na lot of what we'll see in the next\ncouple of slides and videos have of\ncourse being developed for image\nrecognition and object detection tasks\nand in particular two data sets are very\nimportant for these kinds of tarsus this\nis the image net data set which you find\nin reference 11 it has something like a\nthousand classes more than 14 million\nimages and subsets have been used for\nthe image net large-scale visual\nrecognition challenges it contains\nnatural images of varying size so a lot\nof these images have actually been\ndownloaded from the internet there's\nalso a smaller data sets if you don't\nwant to train with like millions of\nimages right away so there's also very\nimportant the size our data sets so for\n10 and sy for 100 which is 10 and 100\nclasses and there we only have\n50k training and 10k testing images the\nimages have reduced size 32 by 32 in\norder to very quickly be able to explore\ndifferent architectures pros and cons\nand if you have these smaller data sets\nthen it also doesn't take so long for\ntraining so this is also a very common\ndata set if you want to evaluate your\narchitecture okay so based on these\ndifferent data sets we then want to go\nahead and look into the early\narchitectures and I think one of the\nmost important ones is lunette which was\npublished in 1998 in reference number 9\nand you can see this is essentially the\nconvolutional neural network as we have\nbeen discussing so far it has been used\nfor example for letter recognition we\nhave the convolutional layers that we\nhave trainable kernel stem pooling\nanother set of convolutional layers and\nanother pooling operation and then\ntowards the end we are going into fully\nconnected layers where we then gradually\nreduce and in the very end we have the\noutput layer that corresponds to the\nnumber of classes we have been doing\nthis for millennia\nso this is a very typical CN n type of\narchitecture and this kind of approach\nhas been used in many papers has\ninspired a lot of work we have for every\narchitecture here key features and you\ncan see here most of the bullets are in\ngray that means that most of these\nfeatures did not survive but of course\nwhat survived here was convolution for\nspatial features this is the main idea\nthat is still prevalent all the other\nthings like subsampling using average\npooling it still used a non-linearity\nthe tongue and super bowl eCos so it's a\nnot so deep model right then it had\nsparse connectivity between s 2 and C 3\nlayers as you see here in the figure\nso also not that common anymore the\nmulti-layer perceptrons final classifier\nsought\nsomething that we see no longer because\nit has been removed for for example of\nfully convolutional networks which is a\nmuch better approach and also the\nsequence of convolution pooling and\nnon-linearity is kind of fixed and today\nwe will do that in a much better way but\nof course this architecture is\nfundamental for many of the further\ndevelopments and I think it's really\nimportant that we are also listing it\nhere the next milestone that I want to\ntalk about in this video is Alex net you\nfind the typical image by the way you\nwill find exactly this image also in the\noriginal publication so Alex net is\nconsisting of those two branches that\nyou see here and you can see that even\nin the original publication the top\nbranch is is cut in half so it's a kind\nof artifact that you find in many\nrepresentations of Alex net when they\nrefer to this figure so they figure is\ncut into parts but it's not that severe\nbecause those two parts are essentially\nidentical and one of the reasons why it\nwas split into two sub networks you\ncould say is because Alex net has been\nimplemented on graphical processing\nunits so this is implemented on GPUs and\nit actually was already multi-gpu so the\ntwo branches that you see on the top\nthey have been implemented on two\ndifferent graphics processing units and\nthey could also be trained and then\nsynchronized using the software so the\nGPU is of course a feature that is still\nvery prevalent\nyou know everybody today in deep\nlearning is very much relying on graphic\nprocessing units as we've seen in\nnumerous occasions in this lecture it\nhad essentially eight layers so it's not\nsuch a deep network\nit had overlapping max pooling with a\nstride of two size of three and it\nintroduced the Ray Lewis non-linearity\nwhich is also very very commonly used\ntoday so also very important feature and\nof course it is the winner of the 2012\nimagenet challenge which essentially cut\ndown the error rate into half so it's\nreally one of the milestones towards the\nbreakthrough of CNN's cool AI what else\ndo we have to combat overfitting in this\narchitecture already used drop out with\na probability of 0.5 in the first two\nfully connected layers and deduced data\naugmentation so there were random\ntransformations and random intensity\nvariations another key feature was that\nit has been employing mini-batch\nstochastic gradient descent with\nmomentum 0.9 and an l2 weight decay with\na parameter setting of 5 times 10 to the\nminus 5 and it was using a rather simple\nweight initialization just using a\nnormal distribution and a small standard\ndeviation which we have seen much better\napproaches already in the previous\nvideos what else is important well we've\nseen that the separation is a historical\nreason the GPUs at the time but too\nsmall to host the entire network so it\nwas split on two CPUs another key paper\nI would say is the networking Network\npaper where they essentially introduced\none-by-one filters this was originally\ndescribed as a network in network but\neffectively we know it today as one by\none convolutions because they\nessentially introduced fully connected\nlayers over the channels and we use this\nrecipe now a lot if you want to compress\nchannels and we fully connect over the\nchannel dimension so this is very nice\nbecause we've seen already that this is\nequivalent to a fully connected layer\nand we can now integrate fully connected\nlayers in terms of one by one\nconvolution and this enables us this\nvery nice concept of the fully\nconvolutional Network\nso it has very few parameters shared\nacross all the activations and then it\nintroduces this global spatial average\npooling as the last layer and this is\nessentially the birth of fully\nconvolutional neural networks\nanother very important architecture is\nthe vgg network the visual geometry\ngroup of the University of Oxford and\nthey introduced very small kernel sizes\nin each convolution and the network is\nalso very common because it's available\nfor download so there's pre trained\nmodels available and you can see that\nthe key feature that they have in this\nnetwork is that they essentially reduce\nthe spatial dimension and they increase\nthe channel they're mentioned so step by\nstep and this has this gradual\ntransformation from spatial domain into\na let's say for the classifier important\ninterpretation domain so we can see the\nspecialty dimension goes down at the\nsame time we go up with the channel\ndimension and this allows us to\ngradually convert from images and color\nimages towards meaning so I think the\nsmall kernel sizes are the key feature\nthat are still used it was typically\nused in 16 and 19 layers with max\npooling between some of the layers of\nstraight to size to learning procedure\nwas very similar to Aleks net but turn\nout to be hard to train and practice you\nneeded pre-training with shallower\nnetworks in order to construct this so\nthe network is not so great in terms of\nperformance has a lot of parameters but\nit's pre trained and it's available and\ntherefore this has also caused the\ncommunity to adopt this quite widely so\nyou can see also when you work with open\nsource and accessible software this is\nalso a key feature that is important for\nus in order to develop further concept\nbecause parameters can be shared\ntraining models can be shared source\ncode can be shared and this is why I\nthink this is a very important instance\nof the net\nto find in the deep learning landscape\nanother key network that we already seen\nat quite some occasions in this lecture\nis Google in it and here we have the\ninception we've one version that you\nfind in reference and 14 I think the\nmain points that I want to highlight\nhere is that they had very good ideas in\norder to save computations by using a\ncouple of tricks so they developed these\nnetworks with embedded hardware in mind\nand it also just features 1.5 billion\nmultiply add operations in the inference\ntime so this is pretty cool but what I\nfind even cooler\nare these inception blocks so in total\nit had 22 layers and the global average\npooling as a final layer but these\ninception modules are really nice and we\nwill look at them in a little more\ndetail on the next couple of slides\nbecause they essentially allow you to\nlet the network decide whether it wants\nto pour or whether it wants to convolved\nwhich is pretty cool and another trick\nthat is really nice are these auxilary\nclassifiers that they used in earlier\nlayers in order to stabilize the\ngradient so the idea is that you plug in\nyour loss in to some of the more early\nlayers where you already try to figure\nout a preliminary classification and\nthis helps relieve of building deeper\nmodels because you can bring in the loss\nat a rather early stage and you know the\ndeeper you go into the network the more\nyou go to the earlier layers the more\nlikely it is that you get a vanishing\ngradient and with these auxilary\nclassifiers you can prevent it to some\nextent and it's also quite useful if you\nfor example want to figure out how many\nof those inceptions modules do you\nreally need then you can work with those\naxillary classifiers so that's really a\nvery interesting content so let's talk a\nbit about those inception modules and by\nthe way the inception modules are of\ncourse something that has survived for\nquite some time and it's still being\nused in\nany of the state-of-the-art deep\nlearning models so there's different\nbranches through this networks there's\nlike only a one by one convolution a one\nby one convolution followed by a three\nby three convolution or one by one\nconvolution followed by a five by five\nconvolution or max pooling followed by\none by one convolution so all of these\nbranches go in parallel and then you\nconcatenate the output of the branches\nand offer it to the next layer so\nessentially this allows then the network\nto decide which of the branches it\ntrusts in the next layer and this way it\ncan somehow determine whether it wants\nto pool or whether it wants to convulse\nso you can essentially think about this\nas an automatic routing that is\ndetermined during the training also\ninteresting is that the one by one\nfilters serve as a kind of bottleneck\nlayer so you can use that in order to\ncompress the channels from the previous\nlayers and then you can compress and\nthen convolve still there's a lot of\ncomputations if you were to implement it\nexactly this way so the idea is then\nthat they use this bottleneck layer in\norder to essentially compress the\ncorrelations between different feature\nMaps\nso the idea is that you have this one by\none filters and what you do is instead\nof running let's say 256 input feature\nmaps and 256 output feature maps were\nfree by free convolution this would\nalready mean that you have something\nlike 600,000 multiply add operations so\ninstead you use this bottleneck ideas so\nyou compress the channels from 256 but\nby one by one convolution to 64 then you\ndo on the 64 channels the three by three\nconvolution and then you uncompress\nessentially from the 64 channels again\nto 256 and this saves a lot of\ncomputations so in total you need\napproximately\nseventy thousand multiplied operations\nhere and if you look at the original\n600,000 multiply add operations then you\ncan see that we already saved a lot of\ncomputer also because I'm lazy okay so\nthese are essentially classical deep\nlearning architectures and we want to\ntalk about more sophisticated ones in\nthe second part and there I want to show\nyou how to go even deeper both deeper\nlayers and how you can do that\nefficiently with for example other\nversions of the inception module so\nthank you very much for listening and\nhope to see you in the next video\ngoodbye\n[Music]\n",
  "words": [
    "music",
    "welcome",
    "back",
    "deep",
    "learning",
    "see",
    "couple",
    "upgrades",
    "much",
    "better",
    "recording",
    "quality",
    "hope",
    "also",
    "see",
    "finally",
    "fix",
    "sound",
    "problem",
    "able",
    "hear",
    "much",
    "better",
    "right",
    "back",
    "new",
    "session",
    "want",
    "talk",
    "couple",
    "exciting",
    "topics",
    "let",
    "see",
    "got",
    "today",
    "want",
    "start",
    "discussing",
    "different",
    "architectures",
    "particular",
    "first",
    "couple",
    "videos",
    "want",
    "talk",
    "bit",
    "early",
    "architectures",
    "things",
    "seen",
    "early",
    "days",
    "deep",
    "learning",
    "follow",
    "looking",
    "deeper",
    "models",
    "later",
    "videos",
    "end",
    "want",
    "talk",
    "learning",
    "architectures",
    "instead",
    "humans",
    "might",
    "need",
    "dozens",
    "examples",
    "things",
    "need",
    "millions",
    "lot",
    "see",
    "next",
    "couple",
    "slides",
    "videos",
    "course",
    "developed",
    "image",
    "recognition",
    "object",
    "detection",
    "tasks",
    "particular",
    "two",
    "data",
    "sets",
    "important",
    "kinds",
    "tarsus",
    "image",
    "net",
    "data",
    "set",
    "find",
    "reference",
    "11",
    "something",
    "like",
    "thousand",
    "classes",
    "14",
    "million",
    "images",
    "subsets",
    "used",
    "image",
    "net",
    "visual",
    "recognition",
    "challenges",
    "contains",
    "natural",
    "images",
    "varying",
    "size",
    "lot",
    "images",
    "actually",
    "downloaded",
    "internet",
    "also",
    "smaller",
    "data",
    "sets",
    "want",
    "train",
    "like",
    "millions",
    "images",
    "right",
    "away",
    "also",
    "important",
    "size",
    "data",
    "sets",
    "10",
    "sy",
    "100",
    "10",
    "100",
    "classes",
    "50k",
    "training",
    "10k",
    "testing",
    "images",
    "images",
    "reduced",
    "size",
    "32",
    "32",
    "order",
    "quickly",
    "able",
    "explore",
    "different",
    "architectures",
    "pros",
    "cons",
    "smaller",
    "data",
    "sets",
    "also",
    "take",
    "long",
    "training",
    "also",
    "common",
    "data",
    "set",
    "want",
    "evaluate",
    "architecture",
    "okay",
    "based",
    "different",
    "data",
    "sets",
    "want",
    "go",
    "ahead",
    "look",
    "early",
    "architectures",
    "think",
    "one",
    "important",
    "ones",
    "lunette",
    "published",
    "1998",
    "reference",
    "number",
    "9",
    "see",
    "essentially",
    "convolutional",
    "neural",
    "network",
    "discussing",
    "far",
    "used",
    "example",
    "letter",
    "recognition",
    "convolutional",
    "layers",
    "trainable",
    "kernel",
    "stem",
    "pooling",
    "another",
    "set",
    "convolutional",
    "layers",
    "another",
    "pooling",
    "operation",
    "towards",
    "end",
    "going",
    "fully",
    "connected",
    "layers",
    "gradually",
    "reduce",
    "end",
    "output",
    "layer",
    "corresponds",
    "number",
    "classes",
    "millennia",
    "typical",
    "cn",
    "n",
    "type",
    "architecture",
    "kind",
    "approach",
    "used",
    "many",
    "papers",
    "inspired",
    "lot",
    "work",
    "every",
    "architecture",
    "key",
    "features",
    "see",
    "bullets",
    "gray",
    "means",
    "features",
    "survive",
    "course",
    "survived",
    "convolution",
    "spatial",
    "features",
    "main",
    "idea",
    "still",
    "prevalent",
    "things",
    "like",
    "subsampling",
    "using",
    "average",
    "pooling",
    "still",
    "used",
    "tongue",
    "super",
    "bowl",
    "ecos",
    "deep",
    "model",
    "right",
    "sparse",
    "connectivity",
    "2",
    "c",
    "3",
    "layers",
    "see",
    "figure",
    "also",
    "common",
    "anymore",
    "perceptrons",
    "final",
    "classifier",
    "sought",
    "something",
    "see",
    "longer",
    "removed",
    "example",
    "fully",
    "convolutional",
    "networks",
    "much",
    "better",
    "approach",
    "also",
    "sequence",
    "convolution",
    "pooling",
    "kind",
    "fixed",
    "today",
    "much",
    "better",
    "way",
    "course",
    "architecture",
    "fundamental",
    "many",
    "developments",
    "think",
    "really",
    "important",
    "also",
    "listing",
    "next",
    "milestone",
    "want",
    "talk",
    "video",
    "alex",
    "net",
    "find",
    "typical",
    "image",
    "way",
    "find",
    "exactly",
    "image",
    "also",
    "original",
    "publication",
    "alex",
    "net",
    "consisting",
    "two",
    "branches",
    "see",
    "see",
    "even",
    "original",
    "publication",
    "top",
    "branch",
    "cut",
    "half",
    "kind",
    "artifact",
    "find",
    "many",
    "representations",
    "alex",
    "net",
    "refer",
    "figure",
    "figure",
    "cut",
    "parts",
    "severe",
    "two",
    "parts",
    "essentially",
    "identical",
    "one",
    "reasons",
    "split",
    "two",
    "sub",
    "networks",
    "could",
    "say",
    "alex",
    "net",
    "implemented",
    "graphical",
    "processing",
    "units",
    "implemented",
    "gpus",
    "actually",
    "already",
    "two",
    "branches",
    "see",
    "top",
    "implemented",
    "two",
    "different",
    "graphics",
    "processing",
    "units",
    "could",
    "also",
    "trained",
    "synchronized",
    "using",
    "software",
    "gpu",
    "course",
    "feature",
    "still",
    "prevalent",
    "know",
    "everybody",
    "today",
    "deep",
    "learning",
    "much",
    "relying",
    "graphic",
    "processing",
    "units",
    "seen",
    "numerous",
    "occasions",
    "lecture",
    "essentially",
    "eight",
    "layers",
    "deep",
    "network",
    "overlapping",
    "max",
    "pooling",
    "stride",
    "two",
    "size",
    "three",
    "introduced",
    "ray",
    "lewis",
    "also",
    "commonly",
    "used",
    "today",
    "also",
    "important",
    "feature",
    "course",
    "winner",
    "2012",
    "imagenet",
    "challenge",
    "essentially",
    "cut",
    "error",
    "rate",
    "half",
    "really",
    "one",
    "milestones",
    "towards",
    "breakthrough",
    "cnn",
    "cool",
    "ai",
    "else",
    "combat",
    "overfitting",
    "architecture",
    "already",
    "used",
    "drop",
    "probability",
    "first",
    "two",
    "fully",
    "connected",
    "layers",
    "deduced",
    "data",
    "augmentation",
    "random",
    "transformations",
    "random",
    "intensity",
    "variations",
    "another",
    "key",
    "feature",
    "employing",
    "stochastic",
    "gradient",
    "descent",
    "momentum",
    "l2",
    "weight",
    "decay",
    "parameter",
    "setting",
    "5",
    "times",
    "10",
    "minus",
    "5",
    "using",
    "rather",
    "simple",
    "weight",
    "initialization",
    "using",
    "normal",
    "distribution",
    "small",
    "standard",
    "deviation",
    "seen",
    "much",
    "better",
    "approaches",
    "already",
    "previous",
    "videos",
    "else",
    "important",
    "well",
    "seen",
    "separation",
    "historical",
    "reason",
    "gpus",
    "time",
    "small",
    "host",
    "entire",
    "network",
    "split",
    "two",
    "cpus",
    "another",
    "key",
    "paper",
    "would",
    "say",
    "networking",
    "network",
    "paper",
    "essentially",
    "introduced",
    "filters",
    "originally",
    "described",
    "network",
    "network",
    "effectively",
    "know",
    "today",
    "one",
    "one",
    "convolutions",
    "essentially",
    "introduced",
    "fully",
    "connected",
    "layers",
    "channels",
    "use",
    "recipe",
    "lot",
    "want",
    "compress",
    "channels",
    "fully",
    "connect",
    "channel",
    "dimension",
    "nice",
    "seen",
    "already",
    "equivalent",
    "fully",
    "connected",
    "layer",
    "integrate",
    "fully",
    "connected",
    "layers",
    "terms",
    "one",
    "one",
    "convolution",
    "enables",
    "us",
    "nice",
    "concept",
    "fully",
    "convolutional",
    "network",
    "parameters",
    "shared",
    "across",
    "activations",
    "introduces",
    "global",
    "spatial",
    "average",
    "pooling",
    "last",
    "layer",
    "essentially",
    "birth",
    "fully",
    "convolutional",
    "neural",
    "networks",
    "another",
    "important",
    "architecture",
    "vgg",
    "network",
    "visual",
    "geometry",
    "group",
    "university",
    "oxford",
    "introduced",
    "small",
    "kernel",
    "sizes",
    "convolution",
    "network",
    "also",
    "common",
    "available",
    "download",
    "pre",
    "trained",
    "models",
    "available",
    "see",
    "key",
    "feature",
    "network",
    "essentially",
    "reduce",
    "spatial",
    "dimension",
    "increase",
    "channel",
    "mentioned",
    "step",
    "step",
    "gradual",
    "transformation",
    "spatial",
    "domain",
    "let",
    "say",
    "classifier",
    "important",
    "interpretation",
    "domain",
    "see",
    "specialty",
    "dimension",
    "goes",
    "time",
    "go",
    "channel",
    "dimension",
    "allows",
    "us",
    "gradually",
    "convert",
    "images",
    "color",
    "images",
    "towards",
    "meaning",
    "think",
    "small",
    "kernel",
    "sizes",
    "key",
    "feature",
    "still",
    "used",
    "typically",
    "used",
    "16",
    "19",
    "layers",
    "max",
    "pooling",
    "layers",
    "straight",
    "size",
    "learning",
    "procedure",
    "similar",
    "aleks",
    "net",
    "turn",
    "hard",
    "train",
    "practice",
    "needed",
    "shallower",
    "networks",
    "order",
    "construct",
    "network",
    "great",
    "terms",
    "performance",
    "lot",
    "parameters",
    "pre",
    "trained",
    "available",
    "therefore",
    "also",
    "caused",
    "community",
    "adopt",
    "quite",
    "widely",
    "see",
    "also",
    "work",
    "open",
    "source",
    "accessible",
    "software",
    "also",
    "key",
    "feature",
    "important",
    "us",
    "order",
    "develop",
    "concept",
    "parameters",
    "shared",
    "training",
    "models",
    "shared",
    "source",
    "code",
    "shared",
    "think",
    "important",
    "instance",
    "net",
    "find",
    "deep",
    "learning",
    "landscape",
    "another",
    "key",
    "network",
    "already",
    "seen",
    "quite",
    "occasions",
    "lecture",
    "google",
    "inception",
    "one",
    "version",
    "find",
    "reference",
    "14",
    "think",
    "main",
    "points",
    "want",
    "highlight",
    "good",
    "ideas",
    "order",
    "save",
    "computations",
    "using",
    "couple",
    "tricks",
    "developed",
    "networks",
    "embedded",
    "hardware",
    "mind",
    "also",
    "features",
    "billion",
    "multiply",
    "add",
    "operations",
    "inference",
    "time",
    "pretty",
    "cool",
    "find",
    "even",
    "cooler",
    "inception",
    "blocks",
    "total",
    "22",
    "layers",
    "global",
    "average",
    "pooling",
    "final",
    "layer",
    "inception",
    "modules",
    "really",
    "nice",
    "look",
    "little",
    "detail",
    "next",
    "couple",
    "slides",
    "essentially",
    "allow",
    "let",
    "network",
    "decide",
    "whether",
    "wants",
    "pour",
    "whether",
    "wants",
    "convolved",
    "pretty",
    "cool",
    "another",
    "trick",
    "really",
    "nice",
    "auxilary",
    "classifiers",
    "used",
    "earlier",
    "layers",
    "order",
    "stabilize",
    "gradient",
    "idea",
    "plug",
    "loss",
    "early",
    "layers",
    "already",
    "try",
    "figure",
    "preliminary",
    "classification",
    "helps",
    "relieve",
    "building",
    "deeper",
    "models",
    "bring",
    "loss",
    "rather",
    "early",
    "stage",
    "know",
    "deeper",
    "go",
    "network",
    "go",
    "earlier",
    "layers",
    "likely",
    "get",
    "vanishing",
    "gradient",
    "auxilary",
    "classifiers",
    "prevent",
    "extent",
    "also",
    "quite",
    "useful",
    "example",
    "want",
    "figure",
    "many",
    "inceptions",
    "modules",
    "really",
    "need",
    "work",
    "axillary",
    "classifiers",
    "really",
    "interesting",
    "content",
    "let",
    "talk",
    "bit",
    "inception",
    "modules",
    "way",
    "inception",
    "modules",
    "course",
    "something",
    "survived",
    "quite",
    "time",
    "still",
    "used",
    "deep",
    "learning",
    "models",
    "different",
    "branches",
    "networks",
    "like",
    "one",
    "one",
    "convolution",
    "one",
    "one",
    "convolution",
    "followed",
    "three",
    "three",
    "convolution",
    "one",
    "one",
    "convolution",
    "followed",
    "five",
    "five",
    "convolution",
    "max",
    "pooling",
    "followed",
    "one",
    "one",
    "convolution",
    "branches",
    "go",
    "parallel",
    "concatenate",
    "output",
    "branches",
    "offer",
    "next",
    "layer",
    "essentially",
    "allows",
    "network",
    "decide",
    "branches",
    "trusts",
    "next",
    "layer",
    "way",
    "somehow",
    "determine",
    "whether",
    "wants",
    "pool",
    "whether",
    "wants",
    "convulse",
    "essentially",
    "think",
    "automatic",
    "routing",
    "determined",
    "training",
    "also",
    "interesting",
    "one",
    "one",
    "filters",
    "serve",
    "kind",
    "bottleneck",
    "layer",
    "use",
    "order",
    "compress",
    "channels",
    "previous",
    "layers",
    "compress",
    "convolve",
    "still",
    "lot",
    "computations",
    "implement",
    "exactly",
    "way",
    "idea",
    "use",
    "bottleneck",
    "layer",
    "order",
    "essentially",
    "compress",
    "correlations",
    "different",
    "feature",
    "maps",
    "idea",
    "one",
    "one",
    "filters",
    "instead",
    "running",
    "let",
    "say",
    "256",
    "input",
    "feature",
    "maps",
    "256",
    "output",
    "feature",
    "maps",
    "free",
    "free",
    "convolution",
    "would",
    "already",
    "mean",
    "something",
    "like",
    "multiply",
    "add",
    "operations",
    "instead",
    "use",
    "bottleneck",
    "ideas",
    "compress",
    "channels",
    "256",
    "one",
    "one",
    "convolution",
    "64",
    "64",
    "channels",
    "three",
    "three",
    "convolution",
    "uncompress",
    "essentially",
    "64",
    "channels",
    "256",
    "saves",
    "lot",
    "computations",
    "total",
    "need",
    "approximately",
    "seventy",
    "thousand",
    "multiplied",
    "operations",
    "look",
    "original",
    "multiply",
    "add",
    "operations",
    "see",
    "already",
    "saved",
    "lot",
    "computer",
    "also",
    "lazy",
    "okay",
    "essentially",
    "classical",
    "deep",
    "learning",
    "architectures",
    "want",
    "talk",
    "sophisticated",
    "ones",
    "second",
    "part",
    "want",
    "show",
    "go",
    "even",
    "deeper",
    "deeper",
    "layers",
    "efficiently",
    "example",
    "versions",
    "inception",
    "module",
    "thank",
    "much",
    "listening",
    "hope",
    "see",
    "next",
    "video",
    "goodbye",
    "music"
  ],
  "keywords": [
    "deep",
    "learning",
    "see",
    "couple",
    "much",
    "better",
    "also",
    "right",
    "want",
    "talk",
    "let",
    "today",
    "different",
    "architectures",
    "videos",
    "early",
    "things",
    "seen",
    "deeper",
    "models",
    "end",
    "instead",
    "need",
    "lot",
    "next",
    "course",
    "image",
    "recognition",
    "two",
    "data",
    "sets",
    "important",
    "net",
    "set",
    "find",
    "reference",
    "something",
    "like",
    "classes",
    "images",
    "used",
    "size",
    "10",
    "training",
    "order",
    "common",
    "architecture",
    "go",
    "look",
    "think",
    "one",
    "essentially",
    "convolutional",
    "network",
    "example",
    "layers",
    "kernel",
    "pooling",
    "another",
    "towards",
    "fully",
    "connected",
    "output",
    "layer",
    "kind",
    "many",
    "work",
    "key",
    "features",
    "convolution",
    "spatial",
    "idea",
    "still",
    "using",
    "average",
    "figure",
    "networks",
    "way",
    "really",
    "alex",
    "original",
    "branches",
    "even",
    "cut",
    "say",
    "implemented",
    "processing",
    "units",
    "already",
    "trained",
    "feature",
    "know",
    "max",
    "three",
    "introduced",
    "cool",
    "gradient",
    "small",
    "time",
    "filters",
    "channels",
    "use",
    "compress",
    "channel",
    "dimension",
    "nice",
    "us",
    "parameters",
    "shared",
    "available",
    "quite",
    "inception",
    "computations",
    "multiply",
    "add",
    "operations",
    "modules",
    "whether",
    "wants",
    "classifiers",
    "followed",
    "bottleneck",
    "maps",
    "256",
    "64"
  ]
}