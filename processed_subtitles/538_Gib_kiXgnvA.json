{
  "text": "hello people from the future welcome to\nnormalise nerd in this video I'm gonna\nexplain the gang yes the famous\ngenerative adversarial networks I know\nthat this is one of those topics if you\ndon't approach it properly then this\nmight feel really intimidating but trust\nme by the end of this video you will\nfeel very comfortable with gangs now I\nput a lot of effort in making these\nvideos so if you like my content please\nsubscribe and hit the bell icon let's\nget started okay the first thing that\nyou need to know is gang is not a single\nmodel it's a combination of two models\nthe first one is a generative model\ncalled G and the second one is a\ndiscriminative model called D now what\nthe hell are discriminative and\ngenerative models well in machine\nlearning we have two main methods for\nbuilding predictive models the most\nfamous one is the discriminative method\nwell in this case the model learns the\nconditional probability of the target\nvariable given the input variable most\ncommon examples are logistic regression\nlinear regression etc on the other hand\nin a generative model the model learns\nthe joint probability distribution of\nthe input variable and the output\nvariable if the model wants to predict\nsomething then it uses Bayes theorem and\nit computes the conditional probability\nof the target variable given the input\nvariable the most common example is the\nnaive Bayes model\nthe biggest advantage of generative\nmodels over discriminative models is\nthat we can use generative model to make\nnew instances of data because in this\ncase we are learning the distribution\nfunction of the data itself which is\nsimply not possible using a\ndiscriminator in our Gantz we are using\nthis generative model to produce new\ndata points that is we are producing\nfake data points using our generator and\nwe are using this discriminator to tell\nif a given data point is an original one\nor it has been produced by our generator\nnow these two models work in an\nadversarial setup that\nmeans they compete with each other and\neventually both of them gets better and\nbetter in their job let me show you the\nstructure of this thing okay so here's\nthe high-level view of our gang this G\nand D are nothing but multi-layered\nneural networks and this theta G and\ntheta D are just the weights okay we are\nusing neural networks here because they\ncan approximate any function we know\nthat from the universal approximation\ntheorem now look at here suppose this is\nour distribution function of the\noriginal data now in reality we can't\nreally draw that or even mathematically\ncompute that because we input images we\nmean put voices we input videos and they\nare higher dimensional complex data so\nthis is only for mathematical analysis\nokay and look at here this is a noise\ndistribution and you can see that this\nis just the normal distribution I am\ntaking and I am gonna sample randomly\nsome data from this distribution and\nwe'll feed that to our generator will to\nget something from the generator\nwe must input something right and we are\ninputting here noise that means this Z\ncontains no information and after\npassing this Z to our model generator it\nwill produce something called G of Z now\nlook at that I have described the\ndistribution of the G of Z with the same\nX that I have written for the earlier\ndata well I am doing this because the\ndomain of our original data is same as\nthe range of G of Z this is important\nbecause we are trying to replicate our\noriginal data so just remember the short\nforms when I say P data this represents\nthe probability distribution of our\noriginal data\nwhen I say PZ it represents the\ndistribution of the noise and when I say\nPG it represents the distribution\nfunction for the output of our generator\nand we are going to pass reconstructed\ndata and original data to our\ndiscriminator and this will provide us a\nsingle number and the single\nwill tell the probability of the input\nbelonging to the original data so you\ncan see this discriminator is just a\nsimple binary classifier and for the\ntraining purpose when you are putting\nthe original data to the discriminator\nwe will say Y is equal to 1 and when we\nare going to pass reconstructed data we\nwill say the level is 0 and the D will\ntry to maximize the chances of\npredicting correct classes but G will\ntry to fool D so we can say that G and D\nare playing a two-player minimax game\nwhat the hell is that\nwell a minimax game is just a two-player\ngame like tic-tac-toe where we can\ninterpret the objective as one player is\ntrying to maximize its probability of\nwinning what the other player is trying\nto minimize the probability of winning\nof the first player okay now we are\nsaying about maximizing and minimizing\nbut what should they maximize or\nminimize we need a mathematical\nexpression right and that's called as\nvalue function let me show you the value\nfunction for this minimax game here well\nthis is the value function for gaen and\nhere mean and Max simply represents that\nG wants to minimize this expression and\nD wants to maximize this expression now\nI know that at first this might feel\ngibberish but if you look here closely\nyou will find that this expression is\nsurprisingly similar to the binary cross\nCentauri function and if you are feeling\nlike that then you are absolutely\ncorrect\nlet me show you why so this is our\nordinary binary cross interval function\nfor a moment just ignore the negative\nsign and the summation so this is just\nthe binary kazantip function for one\ninput right why is the ground truth that\nis the label and Y hat is just the\nprediction of the model when y is equal\nto 1 that is when we are passing the\noriginal data the wipe read is equal to\nD of X so if you just replace these\nthings in the formula you will get lost\nto Ln of D of X now when we are giving\nthe\ndata as our input the wipe red will be d\nof G of Z because obviously first we\nhave passed the noise to our generator\nand it has produced something and then\nwe are giving the produced fake data to\nthe model B and if you replace these\nthings in the function you will get Ln\nof 1 minus D of G of Z now let's combine\nthem so I have just added them together\nand we get this does it look similar to\nthe value function yes but here we are\nmissing the capital E's at the front\nwell they are just expectations\nunderstand that this expression is valid\nfor only one data point but we have to\ndo this for the entire training data set\nright and to represent that\nmathematically we need to use\nexpectation well expectation is just the\naverage value of some experiment if you\nperform this experiment a large number\nof times\nsuppose you are playing a game where you\nneed to roll a die and your score is the\nnumber on the upper face so if you play\nthis game for a really long time then\nthe expected score is 3.5 the formula is\nvery simple you just need to add all the\npossible outcomes multiplied with their\nprobability so it's kind of a weighted\nmean so let's apply the expectation on\nthis equation and look at here that we\nare adding all the scores with their\nprobability same thing goes for here but\nthis is only true for a discrete\ndistribution if we assume that our P\ndata and PZ are actually continuous\ndistribution then the integral sign will\nreplace the summation and we have to\nplace the DX and DZ accordingly and this\nwhole thing is written in the short form\nas e ok so now you know the value\nfunction for Gann does it look\nintimidating now I don't think so now\nI'm gonna tell you how we optimize this\nfunction in practice well this is our\nbig training loop and just like every\nother neural network we have to optimize\nthe loss function using some stochastic\nprocess\nI am using here the stochastic gradient\ndescent okay so first we enter our big\ntraining loop and we fix the learning of\nG and then we are entering the inner\nloop for B well this loop will continue\nfor K steps okay and in this loop first\nwe take m data points from the original\ndistribution and M data points from the\nfake data okay and then we update the\nparameters of our discriminator by\ngradient ascent why because remember\nthat our discriminator is trying to\nmaximize the value function so after we\nhave performed K updates of D we get out\nof this loop and we fix the learning of\nD now we are going to train our\ngenerator for this case we take only M\nfake data samples and update the\nparameters of our generator by gradient\ndescent why because remembers generator\nis trying to minimize the value function\nnow you might ask why I haven't taken\nthis portion in the update step of\ngenerator well look closely does this\nexpression contains any term\ncorresponding to the generator no so the\npartial derivative of this term with\nrespect to theta GU will be zero that's\nwhy we are taking only this portion one\nimportant thing you should note that for\nevery key updates of the discriminator\nwe are updating the generator once okay\nif you have understood this video so far\nthen you know what is the value function\nfor Gann and how we optimize this in\npractice but if you are like me and want\nto know what is the guarantee that our\ngenerator will surely replicate the\noriginal distribution then take a deep\nbreath and continue watching okay just\nto be clear we want to prove that PG\nwill converge to P data if our generator\nis able to find the global minimum for\nthe value function in other words we\nwant to show that PG is equal to P data\nat the global minimum of the value\nfunction okay this is a two-step process\nfirst of all we are fixing the G\nwe wanna see for which value of the\ndiscriminator the value function is\nmaximum look here that I have replaced G\nof Z with X well we can do this because\nthe domain of both of them are same now\nif you differentiate this then you will\nsee that the maximum value of this\nexpression will occur if the d of x\nattains this expression P data over P\ndata plus P G well obviously one can\ndifferentiate that and attain this\nexpression but let us look into it ibly\nso we can represent our value function\nlike this formula a ln x plus b ln 1\nminus x and we want to find the value of\nx for which this expression is maximum\nso if I take B is equal to 0.6 and a is\nequal to 0.45 then you will see that the\ngraph looks something like this and the\nMaxima occurs at point 4 to 9 which is\nnothing but a Upon A plus B now let's\nfix the BX as this and replace that into\nour value function so after fixing D and\nsubstituting that in the value function\nwe get this and after a little\nmodification we are getting this long\nexpression and here\nmimsey just represents that G will try\nto minimize this thing now understand\nwhat we want to do here\nwell we want to prove that probability\ndistribution of generator will be\nexactly same as the probability\ndistribution of the data so it makes\nsense to talk about some of the methods\nto measure the difference between two\ndistributions and one of the most famous\nmethods are G is divergence that is\nJenson Shannon divergence now if you\nlook at the formula for J's divergence\nthen it looks surprisingly close to this\nlong expression isn't it just for a\nrefresher this e here just represents\nthe expectation in the first portion to\nfind the expectation of this value we\nare using the probabilities from the\nfirst distribution but in the second\nportion we are using the probabilities\nfrom the second distribution okay now\nlet's see if we can somehow get to the J\nis divergence from this thing okay so\nafter the little modification we are\ngetting this so what have we done here\nwe have just multiplied two in these two\nlogarithms and for this we need to\nsubtract two times the Ln two here all\nright and if we look closely here then\nthis whole portion is actually equals to\ntwo times the J is divergence of P data\nand PG and obviously we have the\nnegative two Ln two here so G wants to\nminimize this what is the minimum value\nof this expression\nwell the J is divergence between any two\ndistribution cannot be negative the\nminimum it can get is zero and it will\nattend zero only when p1 is equal to p2\nthat is if P data is equal to P G then\nonly this term will be zero and the\nwhole expression will attain its minimum\nthat is minus 2 Ln 2 so voila now you we\nhave proved that add the global minimum\nof our value function the P G will be\nexactly same as P data and our generator\nis actually trying to attain that state\nnow let me show you how G achieves that\nstate that is different phases of\ntraining so at the beginning neither the\ndiscriminator nor the generator knows\nwhat they are doing so the P G is not\nreplicating the P data and the\nclassifier discriminator is not\nclassifying as well after updating the\ntheta D that is when the discriminator\nhas learned something so the classifier\nwill be better so now the discriminator\ncan actually distinguish between the\nreal data and the fake data now after\nthe generator has learned something look\nat that the distribution P G\nis now closer to the P data and the\ndiscriminator is trying to predict the\ntrue level of the data points but it is\nnot performing as well now at the end\nwhen the generator has attained the\nminimum of the value function then it\nhas successfully replicated the\ndistribution function of the data point\nso now PG is indistinguishable from P\ndata so now it is impossible for the\ndiscriminator to tell which data point\nis an original one and which data point\nis a generated one so the discriminator\nwill output 0.5 for every input and that\nis what we want to achieve well this is\na very simplistic view of the gaen in\nreality training the Gann is really hard\nthe goal of this video was to make you\nunderstand\nGantz I hope you are now very\ncomfortable with the concept of ganz and\nif you have understood everything that I\nhave talked about in this video then do\ncongratulate yourself because now you\nknow the math behind one of the finest\ninventions in AI I hope you have liked\nthis video please share this video and\nsubscribe to my channel\nstay safe and thanks for watching\n[Music]\n",
  "words": [
    "hello",
    "people",
    "future",
    "welcome",
    "normalise",
    "nerd",
    "video",
    "gon",
    "na",
    "explain",
    "gang",
    "yes",
    "famous",
    "generative",
    "adversarial",
    "networks",
    "know",
    "one",
    "topics",
    "approach",
    "properly",
    "might",
    "feel",
    "really",
    "intimidating",
    "trust",
    "end",
    "video",
    "feel",
    "comfortable",
    "gangs",
    "put",
    "lot",
    "effort",
    "making",
    "videos",
    "like",
    "content",
    "please",
    "subscribe",
    "hit",
    "bell",
    "icon",
    "let",
    "get",
    "started",
    "okay",
    "first",
    "thing",
    "need",
    "know",
    "gang",
    "single",
    "model",
    "combination",
    "two",
    "models",
    "first",
    "one",
    "generative",
    "model",
    "called",
    "g",
    "second",
    "one",
    "discriminative",
    "model",
    "called",
    "hell",
    "discriminative",
    "generative",
    "models",
    "well",
    "machine",
    "learning",
    "two",
    "main",
    "methods",
    "building",
    "predictive",
    "models",
    "famous",
    "one",
    "discriminative",
    "method",
    "well",
    "case",
    "model",
    "learns",
    "conditional",
    "probability",
    "target",
    "variable",
    "given",
    "input",
    "variable",
    "common",
    "examples",
    "logistic",
    "regression",
    "linear",
    "regression",
    "etc",
    "hand",
    "generative",
    "model",
    "model",
    "learns",
    "joint",
    "probability",
    "distribution",
    "input",
    "variable",
    "output",
    "variable",
    "model",
    "wants",
    "predict",
    "something",
    "uses",
    "bayes",
    "theorem",
    "computes",
    "conditional",
    "probability",
    "target",
    "variable",
    "given",
    "input",
    "variable",
    "common",
    "example",
    "naive",
    "bayes",
    "model",
    "biggest",
    "advantage",
    "generative",
    "models",
    "discriminative",
    "models",
    "use",
    "generative",
    "model",
    "make",
    "new",
    "instances",
    "data",
    "case",
    "learning",
    "distribution",
    "function",
    "data",
    "simply",
    "possible",
    "using",
    "discriminator",
    "gantz",
    "using",
    "generative",
    "model",
    "produce",
    "new",
    "data",
    "points",
    "producing",
    "fake",
    "data",
    "points",
    "using",
    "generator",
    "using",
    "discriminator",
    "tell",
    "given",
    "data",
    "point",
    "original",
    "one",
    "produced",
    "generator",
    "two",
    "models",
    "work",
    "adversarial",
    "setup",
    "means",
    "compete",
    "eventually",
    "gets",
    "better",
    "better",
    "job",
    "let",
    "show",
    "structure",
    "thing",
    "okay",
    "view",
    "gang",
    "g",
    "nothing",
    "neural",
    "networks",
    "theta",
    "g",
    "theta",
    "weights",
    "okay",
    "using",
    "neural",
    "networks",
    "approximate",
    "function",
    "know",
    "universal",
    "approximation",
    "theorem",
    "look",
    "suppose",
    "distribution",
    "function",
    "original",
    "data",
    "reality",
    "ca",
    "really",
    "draw",
    "even",
    "mathematically",
    "compute",
    "input",
    "images",
    "mean",
    "put",
    "voices",
    "input",
    "videos",
    "higher",
    "dimensional",
    "complex",
    "data",
    "mathematical",
    "analysis",
    "okay",
    "look",
    "noise",
    "distribution",
    "see",
    "normal",
    "distribution",
    "taking",
    "gon",
    "na",
    "sample",
    "randomly",
    "data",
    "distribution",
    "feed",
    "generator",
    "get",
    "something",
    "generator",
    "must",
    "input",
    "something",
    "right",
    "inputting",
    "noise",
    "means",
    "z",
    "contains",
    "information",
    "passing",
    "z",
    "model",
    "generator",
    "produce",
    "something",
    "called",
    "g",
    "z",
    "look",
    "described",
    "distribution",
    "g",
    "z",
    "x",
    "written",
    "earlier",
    "data",
    "well",
    "domain",
    "original",
    "data",
    "range",
    "g",
    "z",
    "important",
    "trying",
    "replicate",
    "original",
    "data",
    "remember",
    "short",
    "forms",
    "say",
    "p",
    "data",
    "represents",
    "probability",
    "distribution",
    "original",
    "data",
    "say",
    "pz",
    "represents",
    "distribution",
    "noise",
    "say",
    "pg",
    "represents",
    "distribution",
    "function",
    "output",
    "generator",
    "going",
    "pass",
    "reconstructed",
    "data",
    "original",
    "data",
    "discriminator",
    "provide",
    "us",
    "single",
    "number",
    "single",
    "tell",
    "probability",
    "input",
    "belonging",
    "original",
    "data",
    "see",
    "discriminator",
    "simple",
    "binary",
    "classifier",
    "training",
    "purpose",
    "putting",
    "original",
    "data",
    "discriminator",
    "say",
    "equal",
    "1",
    "going",
    "pass",
    "reconstructed",
    "data",
    "say",
    "level",
    "0",
    "try",
    "maximize",
    "chances",
    "predicting",
    "correct",
    "classes",
    "g",
    "try",
    "fool",
    "say",
    "g",
    "playing",
    "minimax",
    "game",
    "hell",
    "well",
    "minimax",
    "game",
    "game",
    "like",
    "interpret",
    "objective",
    "one",
    "player",
    "trying",
    "maximize",
    "probability",
    "winning",
    "player",
    "trying",
    "minimize",
    "probability",
    "winning",
    "first",
    "player",
    "okay",
    "saying",
    "maximizing",
    "minimizing",
    "maximize",
    "minimize",
    "need",
    "mathematical",
    "expression",
    "right",
    "called",
    "value",
    "function",
    "let",
    "show",
    "value",
    "function",
    "minimax",
    "game",
    "well",
    "value",
    "function",
    "gaen",
    "mean",
    "max",
    "simply",
    "represents",
    "g",
    "wants",
    "minimize",
    "expression",
    "wants",
    "maximize",
    "expression",
    "know",
    "first",
    "might",
    "feel",
    "gibberish",
    "look",
    "closely",
    "find",
    "expression",
    "surprisingly",
    "similar",
    "binary",
    "cross",
    "centauri",
    "function",
    "feeling",
    "like",
    "absolutely",
    "correct",
    "let",
    "show",
    "ordinary",
    "binary",
    "cross",
    "interval",
    "function",
    "moment",
    "ignore",
    "negative",
    "sign",
    "summation",
    "binary",
    "kazantip",
    "function",
    "one",
    "input",
    "right",
    "ground",
    "truth",
    "label",
    "hat",
    "prediction",
    "model",
    "equal",
    "1",
    "passing",
    "original",
    "data",
    "wipe",
    "read",
    "equal",
    "x",
    "replace",
    "things",
    "formula",
    "get",
    "lost",
    "ln",
    "x",
    "giving",
    "data",
    "input",
    "wipe",
    "red",
    "g",
    "z",
    "obviously",
    "first",
    "passed",
    "noise",
    "generator",
    "produced",
    "something",
    "giving",
    "produced",
    "fake",
    "data",
    "model",
    "b",
    "replace",
    "things",
    "function",
    "get",
    "ln",
    "1",
    "minus",
    "g",
    "z",
    "let",
    "combine",
    "added",
    "together",
    "get",
    "look",
    "similar",
    "value",
    "function",
    "yes",
    "missing",
    "capital",
    "e",
    "front",
    "well",
    "expectations",
    "understand",
    "expression",
    "valid",
    "one",
    "data",
    "point",
    "entire",
    "training",
    "data",
    "set",
    "right",
    "represent",
    "mathematically",
    "need",
    "use",
    "expectation",
    "well",
    "expectation",
    "average",
    "value",
    "experiment",
    "perform",
    "experiment",
    "large",
    "number",
    "times",
    "suppose",
    "playing",
    "game",
    "need",
    "roll",
    "die",
    "score",
    "number",
    "upper",
    "face",
    "play",
    "game",
    "really",
    "long",
    "time",
    "expected",
    "score",
    "formula",
    "simple",
    "need",
    "add",
    "possible",
    "outcomes",
    "multiplied",
    "probability",
    "kind",
    "weighted",
    "mean",
    "let",
    "apply",
    "expectation",
    "equation",
    "look",
    "adding",
    "scores",
    "probability",
    "thing",
    "goes",
    "true",
    "discrete",
    "distribution",
    "assume",
    "p",
    "data",
    "pz",
    "actually",
    "continuous",
    "distribution",
    "integral",
    "sign",
    "replace",
    "summation",
    "place",
    "dx",
    "dz",
    "accordingly",
    "whole",
    "thing",
    "written",
    "short",
    "form",
    "e",
    "ok",
    "know",
    "value",
    "function",
    "gann",
    "look",
    "intimidating",
    "think",
    "gon",
    "na",
    "tell",
    "optimize",
    "function",
    "practice",
    "well",
    "big",
    "training",
    "loop",
    "like",
    "every",
    "neural",
    "network",
    "optimize",
    "loss",
    "function",
    "using",
    "stochastic",
    "process",
    "using",
    "stochastic",
    "gradient",
    "descent",
    "okay",
    "first",
    "enter",
    "big",
    "training",
    "loop",
    "fix",
    "learning",
    "g",
    "entering",
    "inner",
    "loop",
    "b",
    "well",
    "loop",
    "continue",
    "k",
    "steps",
    "okay",
    "loop",
    "first",
    "take",
    "data",
    "points",
    "original",
    "distribution",
    "data",
    "points",
    "fake",
    "data",
    "okay",
    "update",
    "parameters",
    "discriminator",
    "gradient",
    "ascent",
    "remember",
    "discriminator",
    "trying",
    "maximize",
    "value",
    "function",
    "performed",
    "k",
    "updates",
    "get",
    "loop",
    "fix",
    "learning",
    "going",
    "train",
    "generator",
    "case",
    "take",
    "fake",
    "data",
    "samples",
    "update",
    "parameters",
    "generator",
    "gradient",
    "descent",
    "remembers",
    "generator",
    "trying",
    "minimize",
    "value",
    "function",
    "might",
    "ask",
    "taken",
    "portion",
    "update",
    "step",
    "generator",
    "well",
    "look",
    "closely",
    "expression",
    "contains",
    "term",
    "corresponding",
    "generator",
    "partial",
    "derivative",
    "term",
    "respect",
    "theta",
    "gu",
    "zero",
    "taking",
    "portion",
    "one",
    "important",
    "thing",
    "note",
    "every",
    "key",
    "updates",
    "discriminator",
    "updating",
    "generator",
    "okay",
    "understood",
    "video",
    "far",
    "know",
    "value",
    "function",
    "gann",
    "optimize",
    "practice",
    "like",
    "want",
    "know",
    "guarantee",
    "generator",
    "surely",
    "replicate",
    "original",
    "distribution",
    "take",
    "deep",
    "breath",
    "continue",
    "watching",
    "okay",
    "clear",
    "want",
    "prove",
    "pg",
    "converge",
    "p",
    "data",
    "generator",
    "able",
    "find",
    "global",
    "minimum",
    "value",
    "function",
    "words",
    "want",
    "show",
    "pg",
    "equal",
    "p",
    "data",
    "global",
    "minimum",
    "value",
    "function",
    "okay",
    "process",
    "first",
    "fixing",
    "g",
    "wan",
    "na",
    "see",
    "value",
    "discriminator",
    "value",
    "function",
    "maximum",
    "look",
    "replaced",
    "g",
    "z",
    "x",
    "well",
    "domain",
    "differentiate",
    "see",
    "maximum",
    "value",
    "expression",
    "occur",
    "x",
    "attains",
    "expression",
    "p",
    "data",
    "p",
    "data",
    "plus",
    "p",
    "g",
    "well",
    "obviously",
    "one",
    "differentiate",
    "attain",
    "expression",
    "let",
    "us",
    "look",
    "ibly",
    "represent",
    "value",
    "function",
    "like",
    "formula",
    "ln",
    "x",
    "plus",
    "b",
    "ln",
    "1",
    "minus",
    "x",
    "want",
    "find",
    "value",
    "x",
    "expression",
    "maximum",
    "take",
    "b",
    "equal",
    "equal",
    "see",
    "graph",
    "looks",
    "something",
    "like",
    "maxima",
    "occurs",
    "point",
    "4",
    "9",
    "nothing",
    "upon",
    "plus",
    "b",
    "let",
    "fix",
    "bx",
    "replace",
    "value",
    "function",
    "fixing",
    "substituting",
    "value",
    "function",
    "get",
    "little",
    "modification",
    "getting",
    "long",
    "expression",
    "mimsey",
    "represents",
    "g",
    "try",
    "minimize",
    "thing",
    "understand",
    "want",
    "well",
    "want",
    "prove",
    "probability",
    "distribution",
    "generator",
    "exactly",
    "probability",
    "distribution",
    "data",
    "makes",
    "sense",
    "talk",
    "methods",
    "measure",
    "difference",
    "two",
    "distributions",
    "one",
    "famous",
    "methods",
    "g",
    "divergence",
    "jenson",
    "shannon",
    "divergence",
    "look",
    "formula",
    "j",
    "divergence",
    "looks",
    "surprisingly",
    "close",
    "long",
    "expression",
    "refresher",
    "e",
    "represents",
    "expectation",
    "first",
    "portion",
    "find",
    "expectation",
    "value",
    "using",
    "probabilities",
    "first",
    "distribution",
    "second",
    "portion",
    "using",
    "probabilities",
    "second",
    "distribution",
    "okay",
    "let",
    "see",
    "somehow",
    "get",
    "j",
    "divergence",
    "thing",
    "okay",
    "little",
    "modification",
    "getting",
    "done",
    "multiplied",
    "two",
    "two",
    "logarithms",
    "need",
    "subtract",
    "two",
    "times",
    "ln",
    "two",
    "right",
    "look",
    "closely",
    "whole",
    "portion",
    "actually",
    "equals",
    "two",
    "times",
    "j",
    "divergence",
    "p",
    "data",
    "pg",
    "obviously",
    "negative",
    "two",
    "ln",
    "two",
    "g",
    "wants",
    "minimize",
    "minimum",
    "value",
    "expression",
    "well",
    "j",
    "divergence",
    "two",
    "distribution",
    "negative",
    "minimum",
    "get",
    "zero",
    "attend",
    "zero",
    "p1",
    "equal",
    "p2",
    "p",
    "data",
    "equal",
    "p",
    "g",
    "term",
    "zero",
    "whole",
    "expression",
    "attain",
    "minimum",
    "minus",
    "2",
    "ln",
    "2",
    "voila",
    "proved",
    "add",
    "global",
    "minimum",
    "value",
    "function",
    "p",
    "g",
    "exactly",
    "p",
    "data",
    "generator",
    "actually",
    "trying",
    "attain",
    "state",
    "let",
    "show",
    "g",
    "achieves",
    "state",
    "different",
    "phases",
    "training",
    "beginning",
    "neither",
    "discriminator",
    "generator",
    "knows",
    "p",
    "g",
    "replicating",
    "p",
    "data",
    "classifier",
    "discriminator",
    "classifying",
    "well",
    "updating",
    "theta",
    "discriminator",
    "learned",
    "something",
    "classifier",
    "better",
    "discriminator",
    "actually",
    "distinguish",
    "real",
    "data",
    "fake",
    "data",
    "generator",
    "learned",
    "something",
    "look",
    "distribution",
    "p",
    "g",
    "closer",
    "p",
    "data",
    "discriminator",
    "trying",
    "predict",
    "true",
    "level",
    "data",
    "points",
    "performing",
    "well",
    "end",
    "generator",
    "attained",
    "minimum",
    "value",
    "function",
    "successfully",
    "replicated",
    "distribution",
    "function",
    "data",
    "point",
    "pg",
    "indistinguishable",
    "p",
    "data",
    "impossible",
    "discriminator",
    "tell",
    "data",
    "point",
    "original",
    "one",
    "data",
    "point",
    "generated",
    "one",
    "discriminator",
    "output",
    "every",
    "input",
    "want",
    "achieve",
    "well",
    "simplistic",
    "view",
    "gaen",
    "reality",
    "training",
    "gann",
    "really",
    "hard",
    "goal",
    "video",
    "make",
    "understand",
    "gantz",
    "hope",
    "comfortable",
    "concept",
    "ganz",
    "understood",
    "everything",
    "talked",
    "video",
    "congratulate",
    "know",
    "math",
    "behind",
    "one",
    "finest",
    "inventions",
    "ai",
    "hope",
    "liked",
    "video",
    "please",
    "share",
    "video",
    "subscribe",
    "channel",
    "stay",
    "safe",
    "thanks",
    "watching",
    "music"
  ],
  "keywords": [
    "video",
    "gon",
    "na",
    "gang",
    "famous",
    "generative",
    "networks",
    "know",
    "one",
    "might",
    "feel",
    "really",
    "like",
    "let",
    "get",
    "okay",
    "first",
    "thing",
    "need",
    "single",
    "model",
    "two",
    "models",
    "called",
    "g",
    "second",
    "discriminative",
    "well",
    "learning",
    "methods",
    "case",
    "probability",
    "variable",
    "given",
    "input",
    "distribution",
    "output",
    "wants",
    "something",
    "data",
    "function",
    "using",
    "discriminator",
    "points",
    "fake",
    "generator",
    "tell",
    "point",
    "original",
    "produced",
    "better",
    "show",
    "neural",
    "theta",
    "look",
    "mean",
    "noise",
    "see",
    "right",
    "z",
    "x",
    "trying",
    "say",
    "p",
    "represents",
    "pg",
    "going",
    "number",
    "binary",
    "classifier",
    "training",
    "equal",
    "1",
    "try",
    "maximize",
    "minimax",
    "game",
    "player",
    "minimize",
    "expression",
    "value",
    "closely",
    "find",
    "negative",
    "replace",
    "formula",
    "ln",
    "obviously",
    "b",
    "minus",
    "e",
    "understand",
    "expectation",
    "times",
    "long",
    "actually",
    "whole",
    "gann",
    "optimize",
    "loop",
    "every",
    "gradient",
    "fix",
    "take",
    "update",
    "portion",
    "term",
    "zero",
    "want",
    "global",
    "minimum",
    "maximum",
    "plus",
    "attain",
    "divergence",
    "j"
  ]
}